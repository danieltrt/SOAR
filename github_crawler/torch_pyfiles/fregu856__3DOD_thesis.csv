file_path,api_count,code
Extended-Frustum-PointNet/datasets_img.py,34,"b'# camera-ready\n\nimport sys\nsys.path.append(""/root/3DOD_thesis/utils"")\nfrom kittiloader import LabelLoader2D3D, calibread, LabelLoader2D3D_sequence # (this needs to be imported before torch, because cv2 needs to be imported before torch for some reason)\n\nimport torch\nimport torch.utils.data\n\nimport os\nimport pickle\nimport numpy as np\nimport math\nimport cv2\n\ndef wrapToPi(a):\n    return (a + np.pi) % (2*np.pi) - np.pi\n\ndef getBinNumber4(angle):\n    # angle is assumed to be in [-pi, pi[\n\n    if (angle >= -np.pi/4) and (angle < np.pi/4):\n        bin_number = 0\n    elif (angle >= np.pi/4) and (angle < 3*np.pi/4):\n        bin_number = 1\n    elif ((angle >= 3*np.pi/4) and (angle < np.pi)) or ((angle >= -np.pi) and (angle < -3*np.pi/4)):\n        bin_number = 2\n    elif (angle >= -3*np.pi/4) and (angle < -np.pi/4):\n        bin_number = 3\n    else:\n        raise Exception(""getBinNumber4: angle is not in [-pi, pi["")\n\n    return bin_number\n\ndef getBinNumber(angle, NH):\n    if NH == 4:\n        return getBinNumber4(angle)\n    else:\n        raise Exception(""getBinNumber: NH is not 4"")\n\ndef getBinCenter(bin_number, NH):\n    if NH == 4:\n        bin_center = wrapToPi(bin_number*(np.pi/2))\n    else:\n        raise Exception(""getBinCenter: NH is not 4"")\n\n    return bin_center\n\ndef getBinCenters(bin_numbers, NH):\n    # bin_number has shape (m, n)\n\n    if NH == 4:\n        bin_centers = wrapToPi(bin_numbers*(np.pi/2))\n    else:\n        raise Exception(""getBinCenters: NH is not 4"")\n\n    return bin_centers\n\nclass DatasetFrustumPointNetImgAugmentation(torch.utils.data.Dataset):\n    def __init__(self, kitti_data_path, kitti_meta_path, type, NH):\n        self.img_dir = kitti_data_path + ""/object/training/image_2/""\n        self.label_dir = kitti_data_path + ""/object/training/label_2/""\n        self.calib_dir = kitti_data_path + ""/object/training/calib/""\n        self.lidar_dir = kitti_data_path + ""/object/training/velodyne/""\n\n        self.NH = NH\n\n        with open(kitti_meta_path + ""/%s_img_ids.pkl"" % type, ""rb"") as file: # (needed for python3)\n            img_ids = pickle.load(file)\n\n        with open(kitti_meta_path + ""/kitti_train_mean_car_size.pkl"", ""rb"") as file: # (needed for python3)\n            self.mean_car_size = pickle.load(file)\n\n        with open(kitti_meta_path + ""/kitti_centered_frustum_mean_xyz.pkl"", ""rb"") as file: # (needed for python3)\n            self.centered_frustum_mean_xyz = pickle.load(file)\n            self.centered_frustum_mean_xyz = self.centered_frustum_mean_xyz.astype(np.float32)\n\n        self.examples = []\n        for img_id in img_ids:\n            labels = LabelLoader2D3D(img_id, self.label_dir, "".txt"", self.calib_dir, "".txt"")\n            for label in labels:\n                label_2d = label[""label_2D""]\n                if label_2d[""truncated""] < 0.5 and label_2d[""class""] == ""Car"":\n                    label[""img_id""] = img_id\n                    self.examples.append(label)\n\n        self.num_examples = len(self.examples)\n\n    def __getitem__(self, index):\n        example = self.examples[index]\n\n        img_id = example[""img_id""]\n\n        lidar_path = self.lidar_dir + img_id + "".bin""\n        point_cloud = np.fromfile(lidar_path, dtype=np.float32).reshape(-1, 4)\n        orig_point_cloud = point_cloud\n\n        # remove points that are located behind the camera:\n        point_cloud = point_cloud[point_cloud[:, 0] > 0, :]\n        # remove points that are located too far away from the camera:\n        point_cloud = point_cloud[point_cloud[:, 0] < 80, :]\n\n        calib = calibread(self.calib_dir + img_id + "".txt"")\n        P2 = calib[""P2""]\n        Tr_velo_to_cam_orig = calib[""Tr_velo_to_cam""]\n        R0_rect_orig = calib[""R0_rect""]\n        #\n        R0_rect = np.eye(4)\n        R0_rect[0:3, 0:3] = R0_rect_orig\n        #\n        Tr_velo_to_cam = np.eye(4)\n        Tr_velo_to_cam[0:3, :] = Tr_velo_to_cam_orig\n\n        point_cloud_xyz = point_cloud[:, 0:3]\n        point_cloud_xyz_hom = np.ones((point_cloud.shape[0], 4))\n        point_cloud_xyz_hom[:, 0:3] = point_cloud[:, 0:3] # (point_cloud_xyz_hom has shape (num_points, 4))\n\n        # project the points onto the image plane (homogeneous coords):\n        img_points_hom = np.dot(P2, np.dot(R0_rect, np.dot(Tr_velo_to_cam, point_cloud_xyz_hom.T))).T # (point_cloud_xyz_hom.T has shape (4, num_points))\n        # normalize:\n        img_points = np.zeros((img_points_hom.shape[0], 2))\n        img_points[:, 0] = img_points_hom[:, 0]/img_points_hom[:, 2]\n        img_points[:, 1] = img_points_hom[:, 1]/img_points_hom[:, 2]\n\n        # transform the points into (rectified) camera coordinates:\n        point_cloud_xyz_camera_hom = np.dot(R0_rect, np.dot(Tr_velo_to_cam, point_cloud_xyz_hom.T)).T # (point_cloud_xyz_hom.T has shape (4, num_points))\n        # normalize:\n        point_cloud_xyz_camera = np.zeros((point_cloud_xyz_camera_hom.shape[0], 3))\n        point_cloud_xyz_camera[:, 0] = point_cloud_xyz_camera_hom[:, 0]/point_cloud_xyz_camera_hom[:, 3]\n        point_cloud_xyz_camera[:, 1] = point_cloud_xyz_camera_hom[:, 1]/point_cloud_xyz_camera_hom[:, 3]\n        point_cloud_xyz_camera[:, 2] = point_cloud_xyz_camera_hom[:, 2]/point_cloud_xyz_camera_hom[:, 3]\n\n        point_cloud_camera = point_cloud\n        point_cloud_camera[:, 0:3] = point_cloud_xyz_camera\n\n        label_2D = example[""label_2D""]\n        label_3D = example[""label_3D""]\n\n        bbox = label_2D[""poly""]\n\n        # img = cv2.imread(self.img_dir + img_id + "".png"", -1)\n        # img_with_bboxes = draw_2d_polys(img, [label_2D])\n        # cv2.imwrite(""test.png"", img_with_bboxes)\n\n        ########################################################################\n        # frustum:\n        ########################################################################\n        u_min = bbox[0, 0] # (left)\n        u_max = bbox[1, 0] # (rigth)\n        v_min = bbox[0, 1] # (top)\n        v_max = bbox[2, 1] # (bottom)\n\n        ########################################################################\n        # # # # augment the 2Dbbox:\n        ########################################################################\n        w = u_max - u_min\n        h = v_max - v_min\n        u_center = u_min + w/2.0\n        v_center = v_min + h/2.0\n\n        # translate the center by random distances sampled from\n        # uniform[-0.1w, 0.1w] and uniform[-0.1h, 0.1h] in u,v directions:\n        u_center = u_center + np.random.uniform(low=-0.1*w, high=0.1*w)\n        v_center = v_center + np.random.uniform(low=-0.1*h, high=0.1*h)\n\n        # randomly scale w and h by factor sampled from uniform[0.9, 1.1]:\n        w = w*np.random.uniform(low=0.9, high=1.1)\n        h = h*np.random.uniform(low=0.9, high=1.1)\n\n        u_min = u_center - w/2.0\n        u_max = u_center + w/2.0\n        v_min = v_center - h/2.0\n        v_max = v_center + h/2.0\n\n        row_mask = np.logical_and(\n                    np.logical_and(img_points[:, 0] >= u_min,\n                                   img_points[:, 0] <= u_max),\n                    np.logical_and(img_points[:, 1] >= v_min,\n                                   img_points[:, 1] <= v_max))\n\n        frustum_point_cloud_xyz = point_cloud_xyz[row_mask, :] # (needed only for visualization)\n        frustum_point_cloud = point_cloud[row_mask, :]\n        frustum_point_cloud_xyz_camera = point_cloud_xyz_camera[row_mask, :]\n        frustum_point_cloud_camera = point_cloud_camera[row_mask, :]\n\n        if frustum_point_cloud.shape[0] == 0:\n            print (img_id)\n            print (frustum_point_cloud.shape)\n            return self.__getitem__(0)\n\n        # randomly sample 1024 points in the frustum point cloud:\n        if frustum_point_cloud.shape[0] < 1024:\n            row_idx = np.random.choice(frustum_point_cloud.shape[0], 1024, replace=True)\n        else:\n            row_idx = np.random.choice(frustum_point_cloud.shape[0], 1024, replace=False)\n        frustum_point_cloud_xyz = frustum_point_cloud_xyz[row_idx, :]\n        frustum_point_cloud = frustum_point_cloud[row_idx, :]\n        frustum_point_cloud_xyz_camera = frustum_point_cloud_xyz_camera[row_idx, :]\n        frustum_point_cloud_camera = frustum_point_cloud_camera[row_idx, :]\n        # (the frustum point cloud now has exactly 1024 points)\n\n        ########################################################################\n        # get the input 2dbbox img crop and resize to 224 x 224:\n        ########################################################################\n        img_path = self.img_dir + img_id + "".png""\n        img = cv2.imread(img_path, -1)\n\n        bbox_2d_img = img[int(np.max([0, v_min])):int(v_max), int(np.max([0, u_min])):int(u_max)]\n        bbox_2d_img = cv2.resize(bbox_2d_img, (224, 224))\n\n        # cv2.imshow(""test"", bbox_2d_img)\n        # cv2.waitKey(0)\n\n        ########################################################################\n        # InstanceSeg ground truth:\n        ########################################################################\n        points = label_3D[""points""]\n\n        y_max = points[0, 1]\n        y_min = points[4, 1]\n\n        # D, A, B are consecutive corners of the rectangle (projection of the 3D bbox) in the x,z plane\n        # the vectors AB and AD are orthogonal\n        # a point P = (x, z) lies within the rectangle in the x,z plane iff:\n        # (0 < AP dot AB < AB dot AB) && (0 < AP dot AD < AD dot AD)\n        # (https://math.stackexchange.com/a/190373)\n\n        A = np.array([points[0, 0], points[0, 2]])\n        B = np.array([points[1, 0], points[1, 2]])\n        D = np.array([points[3, 0], points[3, 2]])\n\n        AB = B - A\n        AD = D - A\n        AB_dot_AB = np.dot(AB, AB)\n        AD_dot_AD = np.dot(AD, AD)\n\n        P = np.zeros((frustum_point_cloud_xyz_camera.shape[0], 2))\n        P[:, 0] = frustum_point_cloud_xyz_camera[:, 0]\n        P[:, 1] = frustum_point_cloud_xyz_camera[:, 2]\n\n        AP = P - A\n        AP_dot_AB = np.dot(AP, AB)\n        AP_dot_AD = np.dot(AP, AD)\n\n        row_mask = np.logical_and(\n                    np.logical_and(frustum_point_cloud_xyz_camera[:, 1] >= y_min, frustum_point_cloud_xyz_camera[:, 1] <= y_max),\n                    np.logical_and(np.logical_and(AP_dot_AB >= 0, AP_dot_AB <= AB_dot_AB),\n                                   np.logical_and(AP_dot_AD >= 0, AP_dot_AD <= AD_dot_AD)))\n\n        row_mask_gt = row_mask\n\n        gt_point_cloud_xyz_camera = frustum_point_cloud_xyz_camera[row_mask, :] # (needed only for visualization)\n\n        label_InstanceSeg = np.zeros((frustum_point_cloud.shape[0],), dtype=np.int64)\n        label_InstanceSeg[row_mask] = 1 # (0: point is NOT part of the objet, 1: point is part of the object)\n\n        ########################################################################\n        # visualization of frustum and InstanceSeg ground truth:\n        ########################################################################\n        # import sys\n        # sys.path.append(""/home/fregu856/exjobb/Open3D/build/lib"")\n        # from py3d import *\n        #\n        # # visualize the frustum points with ground truth as colored points:\n        # frustum_pcd_camera = PointCloud()\n        # frustum_pcd_camera.points = Vector3dVector(frustum_point_cloud_xyz_camera)\n        # frustum_pcd_camera.paint_uniform_color([0.25, 0.25, 0.25])\n        # gt_pcd_camera = PointCloud()\n        # gt_pcd_camera.points = Vector3dVector(gt_point_cloud_xyz_camera)\n        # gt_pcd_camera.paint_uniform_color([0, 0, 1])\n        # draw_geometries([gt_pcd_camera, frustum_pcd_camera])\n        #\n        # # visualize the frustum points and gt points as differently colored points in the full point cloud:\n        # frustum_pcd_camera.paint_uniform_color([1, 0, 0])\n        # pcd_camera = PointCloud()\n        # pcd_camera.points = Vector3dVector(point_cloud_camera[:, 0:3])\n        # pcd_camera.paint_uniform_color([0.25, 0.25, 0.25])\n        # draw_geometries([gt_pcd_camera, frustum_pcd_camera, pcd_camera])\n\n        ########################################################################\n        # normalize frustum point cloud:\n        ########################################################################\n        # get the 2dbbox center img point in hom. coords:\n        u_center = u_min + (u_max - u_min)/2.0\n        v_center = v_min + (v_max - v_min)/2.0\n        center_img_point_hom = np.array([u_center, v_center, 1])\n\n        # (more than one 3D point is projected onto the center image point, i.e,\n        # the linear system of equations is under-determined and has inf number\n        # of solutions. By using the pseudo-inverse, we obtain the least-norm sol)\n\n        # get a point (the least-norm sol.) that projects onto the center image point, in hom. coords:\n        P2_pseudo_inverse = np.linalg.pinv(P2) # (shape: (4, 3)) (P2 has shape (3, 4))\n        point_hom = np.dot(P2_pseudo_inverse, center_img_point_hom)\n\n        # hom --> normal coords:\n        point = np.array(([point_hom[0]/point_hom[3], point_hom[1]/point_hom[3], point_hom[2]/point_hom[3]]))\n\n        # if the point is behind the camera, switch to the mirror point in front of the camera:\n        if point[2] < 0:\n            point[0] = -point[0]\n            point[2] = -point[2]\n\n        # compute the angle of the point in the x-z plane: ((rectified) camera coords)\n        frustum_angle = np.arctan2(point[0], point[2]) # (np.arctan2(x, z)) # (frustum_angle = 0: frustum is centered)\n\n        # rotation_matrix to rotate points frustum_angle around the y axis (counter-clockwise):\n        frustum_R = np.asarray([[np.cos(frustum_angle), 0, -np.sin(frustum_angle)],\n                           [0, 1, 0],\n                           [np.sin(frustum_angle), 0, np.cos(frustum_angle)]],\n                           dtype=\'float32\')\n\n        # rotate the frustum point cloud to center it:\n        centered_frustum_point_cloud_xyz_camera = np.dot(frustum_R, frustum_point_cloud_xyz_camera.T).T\n\n        # subtract the centered frustum train xyz mean:\n        centered_frustum_point_cloud_xyz_camera -= self.centered_frustum_mean_xyz\n\n        centered_frustum_point_cloud_camera = frustum_point_cloud_camera\n        centered_frustum_point_cloud_camera[:, 0:3] = centered_frustum_point_cloud_xyz_camera\n\n        # # # # # # # # # # debug visualizations START:\n        # import sys\n        # sys.path.append(""/home/fregu856/exjobb/Open3D/build/lib"")\n        # from py3d import *\n        #\n        # frustum_pcd_camera = PointCloud()\n        # frustum_pcd_camera.points = Vector3dVector(frustum_point_cloud_xyz_camera)\n        # frustum_pcd_camera.paint_uniform_color([0.25, 0.25, 0.25])\n        #\n        # centered_frustum_pcd_camera = PointCloud()\n        # centered_frustum_pcd_camera.points = Vector3dVector(centered_frustum_point_cloud_xyz_camera)\n        # centered_frustum_pcd_camera.paint_uniform_color([1, 0, 0])\n        #\n        # uncentered_frustum_point_cloud_xyz_camera = np.dot(np.linalg.inv(frustum_R), centered_frustum_point_cloud_xyz_camera.T).T\n        # uncentered_frustum_pcd_camera = PointCloud()\n        # uncentered_frustum_pcd_camera.points = Vector3dVector(uncentered_frustum_point_cloud_xyz_camera)\n        # uncentered_frustum_pcd_camera.paint_uniform_color([0, 1, 0])\n        # draw_geometries([centered_frustum_pcd_camera, uncentered_frustum_pcd_camera, frustum_pcd_camera])\n        # # # # # # # # # # debug visualizations END:\n\n        ########################################################################\n        # randomly shift the frustum point cloud in the z direction:\n        ########################################################################\n        z_shift = np.random.uniform(low=-20, high=20)\n        centered_frustum_point_cloud_camera[:, 2] -= z_shift\n\n        ########################################################################\n        # flip the frustum point cloud in the x-z plane with 0.5 prob:\n        ########################################################################\n        # # # # # # # # # # debug visualizations START:\n        # import sys\n        # sys.path.append(""/home/fregu856/exjobb/Open3D/build/lib"")\n        # from py3d import *\n        # pcd = PointCloud()\n        # pcd.points = Vector3dVector(centered_frustum_point_cloud_camera[:, 0:3])\n        # pcd.paint_uniform_color([0.25, 0.25, 0.25])\n        # draw_geometries([pcd])\n        # # # # # # # # # # debug visualizations END:\n\n        # get 0 or 1 with equal probability (indicating if the frustum should be flipped or not):\n        flip = np.random.randint(low=0, high=2)\n\n        # flip the frustum point cloud if flip == 1 (set all x values to -values):\n        centered_frustum_point_cloud_camera[:, 0] = flip*(-centered_frustum_point_cloud_camera[:, 0]) + (1-flip)*centered_frustum_point_cloud_camera[:, 0]\n\n        # # # # # # # # # # debug visualizations START:\n        # pcd.points = Vector3dVector(centered_frustum_point_cloud_camera[:, 0:3])\n        # pcd.paint_uniform_color([0.25, 0.25, 0.25])\n        # draw_geometries([pcd])\n        # # # # # # # # # # debug visualizations END:\n\n        ########################################################################\n        # flip the 2dbbox img crop if flip == 1:\n        ########################################################################\n        if flip == 1:\n            bbox_2d_img = cv2.flip(bbox_2d_img, 1)\n            # cv2.imshow(""test"", bbox_2d_img)\n            # cv2.waitKey(0)\n\n        ########################################################################\n        # TNet ground truth:\n        ########################################################################\n        label_TNet = np.dot(frustum_R, label_3D[""center""]) - self.centered_frustum_mean_xyz\n\n        # flip the label if flip == 1 (set the x value to -value):\n        label_TNet[0] = flip*(-label_TNet[0]) + (1-flip)*label_TNet[0]\n\n        # adjust for the random shift:\n        label_TNet[2] -= z_shift\n\n        ########################################################################\n        # BboxNet ground truth:\n        ########################################################################\n        centered_r_y = wrapToPi(label_3D[\'r_y\'] - frustum_angle)\n        # flip the angle if flip == 1:\n        if flip == 1:\n            centered_r_y = wrapToPi(np.pi - centered_r_y)\n\n        bin_number = getBinNumber(centered_r_y, NH=self.NH)\n        bin_center = getBinCenter(bin_number, NH=self.NH)\n        residual = wrapToPi(centered_r_y - bin_center)\n\n        label_BboxNet = np.zeros((11, ), dtype=np.float32) # ([x, y, z, h, w, l, r_y_bin_number, r_y_residual, r_y_bin_number_neighbor,r_y_residual_neighbor, h_mean, w_mean, l_mean])\n\n        label_BboxNet[0:3] = np.dot(frustum_R, label_3D[""center""]) - self.centered_frustum_mean_xyz\n        # flip the label if flip == 1 (set the x value to -value):\n        label_BboxNet[0] = flip*(-label_BboxNet[0]) + (1-flip)*label_BboxNet[0]\n        # adjust for the random shift:\n        label_BboxNet[2] -= z_shift\n\n        label_BboxNet[3] = label_3D[\'h\']\n        label_BboxNet[4] = label_3D[\'w\']\n        label_BboxNet[5] = label_3D[\'l\']\n        label_BboxNet[6] = bin_number\n        label_BboxNet[7] = residual\n        label_BboxNet[8:] = self.mean_car_size\n\n        ########################################################################\n        # corner loss ground truth:\n        ########################################################################\n        Rmat = np.asarray([[math.cos(residual), 0, math.sin(residual)],\n                           [0, 1, 0],\n                           [-math.sin(residual), 0, math.cos(residual)]],\n                           dtype=\'float32\')\n\n        center = label_BboxNet[0:3]\n        l = label_3D[\'l\']\n        w = label_3D[\'w\']\n        h = label_3D[\'h\']\n        p0 = center + np.dot(Rmat, np.asarray([l/2.0, 0, w/2.0], dtype=\'float32\').flatten())\n        p1 = center + np.dot(Rmat, np.asarray([-l/2.0, 0, w/2.0], dtype=\'float32\').flatten())\n        p2 = center + np.dot(Rmat, np.asarray([-l/2.0, 0, -w/2.0], dtype=\'float32\').flatten())\n        p3 = center + np.dot(Rmat, np.asarray([l/2.0, 0, -w/2.0], dtype=\'float32\').flatten())\n        p4 = center + np.dot(Rmat, np.asarray([l/2.0, -h, w/2.0], dtype=\'float32\').flatten())\n        p5 = center + np.dot(Rmat, np.asarray([-l/2.0, -h, w/2.0], dtype=\'float32\').flatten())\n        p6 = center + np.dot(Rmat, np.asarray([-l/2.0, -h, -w/2.0], dtype=\'float32\').flatten())\n        p7 = center + np.dot(Rmat, np.asarray([l/2.0, -h, -w/2.0], dtype=\'float32\').flatten())\n        label_corner = np.array([p0, p1, p2, p3, p4, p5, p6, p7])\n        label_corner_flipped = np.array([p2, p3, p0, p1, p6, p7, p4, p5])\n\n        ########################################################################\n        # normalize the 2dbbox img crop:\n        ########################################################################\n        bbox_2d_img = bbox_2d_img/255.0\n        bbox_2d_img = bbox_2d_img - np.array([0.485, 0.456, 0.406])\n        bbox_2d_img = bbox_2d_img/np.array([0.229, 0.224, 0.225]) # (shape: (H, W, 3))\n        bbox_2d_img = np.transpose(bbox_2d_img, (2, 0, 1)) # (shape: (3, H, W))\n        bbox_2d_img = bbox_2d_img.astype(np.float32)\n        ########################################################################\n\n        centered_frustum_point_cloud_camera = torch.from_numpy(centered_frustum_point_cloud_camera) # (shape: (1024, 4))\n        bbox_2d_img = torch.from_numpy(bbox_2d_img) # (shape: (3, H, W) = (3, 224, 224))\n        label_InstanceSeg = torch.from_numpy(label_InstanceSeg) # (shape: (1024, ))\n        label_TNet = torch.from_numpy(label_TNet) # (shape: (3, ))\n        label_BboxNet = torch.from_numpy(label_BboxNet) # (shape: (11, ))\n        label_corner = torch.from_numpy(label_corner) # (shape: (8, 3))\n        label_corner_flipped = torch.from_numpy(label_corner_flipped) # (shape: (8, 3))\n\n        return (centered_frustum_point_cloud_camera, bbox_2d_img, label_InstanceSeg, label_TNet, label_BboxNet, label_corner, label_corner_flipped)\n\n    def __len__(self):\n        return self.num_examples\n\n# test = DatasetFrustumPointNetImgAugmentation(""/home/fregu856/exjobb/data/kitti"", ""/home/fregu856/exjobb/data/kitti/meta"", type=""train"", NH=4)\n# for i in range(10):\n#     _ = test.__getitem__(i)\n\nclass EvalDatasetFrustumPointNetImg(torch.utils.data.Dataset):\n    def __init__(self, kitti_data_path, kitti_meta_path, type, NH):\n        self.img_dir = kitti_data_path + ""/object/training/image_2/""\n        self.label_dir = kitti_data_path + ""/object/training/label_2/""\n        self.calib_dir = kitti_data_path + ""/object/training/calib/""\n        self.lidar_dir = kitti_data_path + ""/object/training/velodyne/""\n\n        self.NH = NH\n\n        with open(kitti_meta_path + ""/%s_img_ids.pkl"" % type, ""rb"") as file: # (needed for python3)\n            img_ids = pickle.load(file)\n\n        with open(kitti_meta_path + ""/kitti_train_mean_car_size.pkl"", ""rb"") as file: # (needed for python3)\n            self.mean_car_size = pickle.load(file)\n\n        with open(kitti_meta_path + ""/kitti_centered_frustum_mean_xyz.pkl"", ""rb"") as file: # (needed for python3)\n            self.centered_frustum_mean_xyz = pickle.load(file)\n            self.centered_frustum_mean_xyz = self.centered_frustum_mean_xyz.astype(np.float32)\n\n        self.examples = []\n        for img_id in img_ids:\n            labels = LabelLoader2D3D(img_id, self.label_dir, "".txt"", self.calib_dir, "".txt"")\n            for label in labels:\n                label_2d = label[""label_2D""]\n                if label_2d[""truncated""] < 0.5 and label_2d[""class""] == ""Car"":\n                    label[""img_id""] = img_id\n                    self.examples.append(label)\n\n        self.num_examples = len(self.examples)\n\n    def __getitem__(self, index):\n        example = self.examples[index]\n\n        img_id = example[""img_id""]\n\n        lidar_path = self.lidar_dir + img_id + "".bin""\n        point_cloud = np.fromfile(lidar_path, dtype=np.float32).reshape(-1, 4)\n        orig_point_cloud = point_cloud\n\n        # remove points that are located behind the camera:\n        point_cloud = point_cloud[point_cloud[:, 0] > 0, :]\n        # remove points that are located too far away from the camera:\n        point_cloud = point_cloud[point_cloud[:, 0] < 80, :]\n\n        calib = calibread(self.calib_dir + img_id + "".txt"")\n        P2 = calib[""P2""]\n        Tr_velo_to_cam_orig = calib[""Tr_velo_to_cam""]\n        R0_rect_orig = calib[""R0_rect""]\n        #\n        R0_rect = np.eye(4)\n        R0_rect[0:3, 0:3] = R0_rect_orig\n        #\n        Tr_velo_to_cam = np.eye(4)\n        Tr_velo_to_cam[0:3, :] = Tr_velo_to_cam_orig\n\n        point_cloud_xyz = point_cloud[:, 0:3]\n        point_cloud_xyz_hom = np.ones((point_cloud.shape[0], 4))\n        point_cloud_xyz_hom[:, 0:3] = point_cloud[:, 0:3] # (point_cloud_xyz_hom has shape (num_points, 4))\n\n        # project the points onto the image plane (homogeneous coords):\n        img_points_hom = np.dot(P2, np.dot(R0_rect, np.dot(Tr_velo_to_cam, point_cloud_xyz_hom.T))).T # (point_cloud_xyz_hom.T has shape (4, num_points))\n        # normalize:\n        img_points = np.zeros((img_points_hom.shape[0], 2))\n        img_points[:, 0] = img_points_hom[:, 0]/img_points_hom[:, 2]\n        img_points[:, 1] = img_points_hom[:, 1]/img_points_hom[:, 2]\n\n        # transform the points into (rectified) camera coordinates:\n        point_cloud_xyz_camera_hom = np.dot(R0_rect, np.dot(Tr_velo_to_cam, point_cloud_xyz_hom.T)).T # (point_cloud_xyz_hom.T has shape (4, num_points))\n        # normalize:\n        point_cloud_xyz_camera = np.zeros((point_cloud_xyz_camera_hom.shape[0], 3))\n        point_cloud_xyz_camera[:, 0] = point_cloud_xyz_camera_hom[:, 0]/point_cloud_xyz_camera_hom[:, 3]\n        point_cloud_xyz_camera[:, 1] = point_cloud_xyz_camera_hom[:, 1]/point_cloud_xyz_camera_hom[:, 3]\n        point_cloud_xyz_camera[:, 2] = point_cloud_xyz_camera_hom[:, 2]/point_cloud_xyz_camera_hom[:, 3]\n\n        point_cloud_camera = point_cloud\n        point_cloud_camera[:, 0:3] = point_cloud_xyz_camera\n\n        label_2D = example[""label_2D""]\n        label_3D = example[""label_3D""]\n\n        bbox = label_2D[""poly""]\n\n        # img = cv2.imread(self.img_dir + img_id + "".png"", -1)\n        # img_with_bboxes = draw_2d_polys(img, [label_2D])\n        # cv2.imwrite(""test.png"", img_with_bboxes)\n\n        ########################################################################\n        # frustum:\n        ########################################################################\n        u_min = bbox[0, 0] # (left)\n        u_max = bbox[1, 0] # (rigth)\n        v_min = bbox[0, 1] # (top)\n        v_max = bbox[2, 1] # (bottom)\n\n        u_min_expanded = u_min #- (u_max-u_min)*0.05\n        u_max_expanded = u_max #+ (u_max-u_min)*0.05\n        v_min_expanded = v_min #- (v_max-v_min)*0.05\n        v_max_expanded = v_max #+ (v_max-v_min)*0.05\n        input_2Dbbox = np.array([u_min_expanded, u_max_expanded, v_min_expanded, v_max_expanded])\n\n        row_mask = np.logical_and(\n                    np.logical_and(img_points[:, 0] >= u_min_expanded,\n                                   img_points[:, 0] <= u_max_expanded),\n                    np.logical_and(img_points[:, 1] >= v_min_expanded,\n                                   img_points[:, 1] <= v_max_expanded))\n\n        frustum_point_cloud_xyz = point_cloud_xyz[row_mask, :] # (needed only for visualization)\n        frustum_point_cloud = point_cloud[row_mask, :]\n        frustum_point_cloud_xyz_camera = point_cloud_xyz_camera[row_mask, :]\n        frustum_point_cloud_camera = point_cloud_camera[row_mask, :]\n\n        if frustum_point_cloud.shape[0] == 0:\n            print (img_id)\n            print (frustum_point_cloud.shape)\n            return self.__getitem__(0)\n\n        # randomly sample 1024 points in the frustum point cloud:\n        if frustum_point_cloud.shape[0] < 1024:\n            row_idx = np.random.choice(frustum_point_cloud.shape[0], 1024, replace=True)\n        else:\n            row_idx = np.random.choice(frustum_point_cloud.shape[0], 1024, replace=False)\n        frustum_point_cloud_xyz = frustum_point_cloud_xyz[row_idx, :]\n        frustum_point_cloud = frustum_point_cloud[row_idx, :]\n        frustum_point_cloud_xyz_camera = frustum_point_cloud_xyz_camera[row_idx, :]\n        frustum_point_cloud_camera = frustum_point_cloud_camera[row_idx, :]\n        # (the frustum point cloud now has exactly 1024 points)\n\n        ########################################################################\n        # get the input 2dbbox img crop and resize to 224 x 224:\n        ########################################################################\n        img_path = self.img_dir + img_id + "".png""\n        img = cv2.imread(img_path, -1)\n\n        bbox_2d_img = img[int(np.max([0, v_min])):int(v_max), int(np.max([0, u_min])):int(u_max)]\n        bbox_2d_img = cv2.resize(bbox_2d_img, (224, 224))\n\n        # cv2.imshow(""test"", bbox_2d_img)\n        # cv2.waitKey(0)\n\n        ########################################################################\n        # InstanceSeg ground truth:\n        ########################################################################\n        points = label_3D[""points""]\n\n        y_max = points[0, 1]\n        y_min = points[4, 1]\n\n        # D, A, B are consecutive corners of the rectangle (projection of the 3D bbox) in the x,z plane\n        # the vectors AB and AD are orthogonal\n        # a point P = (x, z) lies within the rectangle in the x,z plane iff:\n        # (0 < AP dot AB < AB dot AB) && (0 < AP dot AD < AD dot AD)\n        # (https://math.stackexchange.com/a/190373)\n\n        A = np.array([points[0, 0], points[0, 2]])\n        B = np.array([points[1, 0], points[1, 2]])\n        D = np.array([points[3, 0], points[3, 2]])\n\n        AB = B - A\n        AD = D - A\n        AB_dot_AB = np.dot(AB, AB)\n        AD_dot_AD = np.dot(AD, AD)\n\n        P = np.zeros((frustum_point_cloud_xyz_camera.shape[0], 2))\n        P[:, 0] = frustum_point_cloud_xyz_camera[:, 0]\n        P[:, 1] = frustum_point_cloud_xyz_camera[:, 2]\n\n        AP = P - A\n        AP_dot_AB = np.dot(AP, AB)\n        AP_dot_AD = np.dot(AP, AD)\n\n        row_mask = np.logical_and(\n                    np.logical_and(frustum_point_cloud_xyz_camera[:, 1] >= y_min, frustum_point_cloud_xyz_camera[:, 1] <= y_max),\n                    np.logical_and(np.logical_and(AP_dot_AB >= 0, AP_dot_AB <= AB_dot_AB),\n                                   np.logical_and(AP_dot_AD >= 0, AP_dot_AD <= AD_dot_AD)))\n\n        gt_point_cloud_xyz_camera = frustum_point_cloud_xyz_camera[row_mask, :] # (needed only for visualization)\n\n        label_InstanceSeg = np.zeros((frustum_point_cloud.shape[0],), dtype=np.int64)\n        label_InstanceSeg[row_mask] = 1 # (0: point is NOT part of the objet, 1: point is part of the object)\n\n        ########################################################################\n        # visualization of frustum and InstanceSeg ground truth:\n        ########################################################################\n        # import sys\n        # sys.path.append(""/home/fregu856/exjobb/Open3D/build/lib"")\n        # from py3d import *\n        #\n        # # visualize the frustum points with ground truth as colored points:\n        # frustum_pcd_camera = PointCloud()\n        # frustum_pcd_camera.points = Vector3dVector(frustum_point_cloud_xyz_camera)\n        # frustum_pcd_camera.paint_uniform_color([0.25, 0.25, 0.25])\n        # gt_pcd_camera = PointCloud()\n        # gt_pcd_camera.points = Vector3dVector(gt_point_cloud_xyz_camera)\n        # gt_pcd_camera.paint_uniform_color([0, 0, 1])\n        # draw_geometries([gt_pcd_camera, frustum_pcd_camera])\n\n        # # visualize the frustum points and gt points as differently colored points in the full point cloud:\n        # frustum_pcd_camera.paint_uniform_color([1, 0, 0])\n        # pcd_camera = PointCloud()\n        # pcd_camera.points = Vector3dVector(orig_point_cloud_camera[:, 0:3])\n        # pcd_camera.paint_uniform_color([0.25, 0.25, 0.25])\n        # draw_geometries([gt_pcd_camera, frustum_pcd_camera, pcd_camera])\n\n        ########################################################################\n        # normalize frustum point cloud:\n        ########################################################################\n        # get the 2dbbox center img point in hom. coords:\n        u_center = u_min_expanded + (u_max_expanded - u_min_expanded)/2.0\n        v_center = v_min_expanded + (v_max_expanded - v_min_expanded)/2.0\n        center_img_point_hom = np.array([u_center, v_center, 1])\n\n        # (more than one 3D point is projected onto the center image point, i.e,\n        # the linear system of equations is under-determined and has inf number\n        # of solutions. By using the pseudo-inverse, we obtain the least-norm sol)\n\n        # get a point (the least-norm sol.) that projects onto the center image point, in hom. coords:\n        P2_pseudo_inverse = np.linalg.pinv(P2) # (shape: (4, 3)) (P2 has shape (3, 4))\n        point_hom = np.dot(P2_pseudo_inverse, center_img_point_hom)\n\n        # hom --> normal coords:\n        point = np.array(([point_hom[0]/point_hom[3], point_hom[1]/point_hom[3], point_hom[2]/point_hom[3]]))\n\n        # if the point is behind the camera, switch to the mirror point in front of the camera:\n        if point[2] < 0:\n            point[0] = -point[0]\n            point[2] = -point[2]\n\n        # compute the angle of the point in the x-z plane: ((rectified) camera coords)\n        frustum_angle = np.arctan2(point[0], point[2]) # (np.arctan2(x, z)) # (frustum_angle = 0: frustum is centered)\n\n        # rotation_matrix to rotate points frustum_angle around the y axis (counter-clockwise):\n        frustum_R = np.asarray([[np.cos(frustum_angle), 0, -np.sin(frustum_angle)],\n                           [0, 1, 0],\n                           [np.sin(frustum_angle), 0, np.cos(frustum_angle)]],\n                           dtype=\'float32\')\n\n        # rotate the frustum point cloud to center it:\n        centered_frustum_point_cloud_xyz_camera = np.dot(frustum_R, frustum_point_cloud_xyz_camera.T).T\n\n        # subtract the centered frustum train xyz mean:\n        centered_frustum_point_cloud_xyz_camera -= self.centered_frustum_mean_xyz\n\n        centered_frustum_point_cloud_camera = frustum_point_cloud_camera\n        centered_frustum_point_cloud_camera[:, 0:3] = centered_frustum_point_cloud_xyz_camera\n\n        # # # # # # # # # # debug visualizations START:\n        # import sys\n        # sys.path.append(""/home/fregu856/exjobb/Open3D/build/lib"")\n        # from py3d import *\n        #\n        # frustum_pcd_camera = PointCloud()\n        # frustum_pcd_camera.points = Vector3dVector(frustum_point_cloud_xyz_camera)\n        # frustum_pcd_camera.paint_uniform_color([0.25, 0.25, 0.25])\n        #\n        # centered_frustum_pcd_camera = PointCloud()\n        # centered_frustum_pcd_camera.points = Vector3dVector(centered_frustum_point_cloud_xyz_camera)\n        # centered_frustum_pcd_camera.paint_uniform_color([1, 0, 0])\n        # draw_geometries([centered_frustum_pcd_camera, frustum_pcd_camera])\n        # # # # # # # # # # debug visualizations END:\n\n        ########################################################################\n        # TNet ground truth:\n        ########################################################################\n        label_TNet = np.dot(frustum_R, label_3D[""center""]) - self.centered_frustum_mean_xyz\n\n        # # # # # # # # # # debug visualizations START:\n        # import sys\n        # sys.path.append(""/home/fregu856/exjobb/Open3D/build/lib"")\n        # from py3d import *\n        #\n        # # visualize InstanceSeg GT and 3dbbox center GT:\n        # frustum_pcd_camera = PointCloud()\n        # frustum_pcd_camera.points = Vector3dVector(frustum_point_cloud_xyz_camera)\n        # frustum_pcd_camera.paint_uniform_color([0.25, 0.25, 0.25cd\n        # gt_pcd_camera = PointCloud()\n        # gt_pcd_camera.points = Vector3dVector(gt_point_cloud_xyz_camera)\n        # gt_pcd_camera.paint_uniform_color([0, 0, 1])\n        # center_gt_pcd_camera = PointCloud()\n        # center_gt_pcd_camera.points = Vector3dVector(np.array([label_TNet]))\n        # center_gt_pcd_camera.paint_uniform_color([0, 1, 0])\n        # draw_geometries([center_gt_pcd_camera, gt_pcd_camera, frustum_pcd_camera])\n        # # # # # # # # # # debug visualizations END:\n\n        ########################################################################\n        # BboxNet ground truth:\n        ########################################################################\n        centered_r_y = wrapToPi(label_3D[\'r_y\'] - frustum_angle)\n        bin_number = getBinNumber(centered_r_y, NH=self.NH)\n        bin_center = getBinCenter(bin_number, NH=self.NH)\n        residual = wrapToPi(centered_r_y - bin_center)\n\n        label_BboxNet = np.zeros((11, ), dtype=np.float32) # ([x, y, z, h, w, l, r_y_bin_number, r_y_residual, r_y_bin_number_neighbor,r_y_residual_neighbor, h_mean, w_mean, l_mean])\n\n        label_BboxNet[0:3] = np.dot(frustum_R, label_3D[""center""]) - self.centered_frustum_mean_xyz\n        label_BboxNet[3] = label_3D[\'h\']\n        label_BboxNet[4] = label_3D[\'w\']\n        label_BboxNet[5] = label_3D[\'l\']\n        label_BboxNet[6] = bin_number\n        label_BboxNet[7] = residual\n        label_BboxNet[8:] = self.mean_car_size\n\n        ########################################################################\n        # corner loss ground truth:\n        ########################################################################\n        Rmat = np.asarray([[math.cos(residual), 0, math.sin(residual)],\n                           [0, 1, 0],\n                           [-math.sin(residual), 0, math.cos(residual)]],\n                           dtype=\'float32\')\n\n        center = label_BboxNet[0:3]\n        l = label_3D[\'l\']\n        w = label_3D[\'w\']\n        h = label_3D[\'h\']\n        p0 = center + np.dot(Rmat, np.asarray([l/2.0, 0, w/2.0], dtype=\'float32\').flatten())\n        p1 = center + np.dot(Rmat, np.asarray([-l/2.0, 0, w/2.0], dtype=\'float32\').flatten())\n        p2 = center + np.dot(Rmat, np.asarray([-l/2.0, 0, -w/2.0], dtype=\'float32\').flatten())\n        p3 = center + np.dot(Rmat, np.asarray([l/2.0, 0, -w/2.0], dtype=\'float32\').flatten())\n        p4 = center + np.dot(Rmat, np.asarray([l/2.0, -h, w/2.0], dtype=\'float32\').flatten())\n        p5 = center + np.dot(Rmat, np.asarray([-l/2.0, -h, w/2.0], dtype=\'float32\').flatten())\n        p6 = center + np.dot(Rmat, np.asarray([-l/2.0, -h, -w/2.0], dtype=\'float32\').flatten())\n        p7 = center + np.dot(Rmat, np.asarray([l/2.0, -h, -w/2.0], dtype=\'float32\').flatten())\n        label_corner = np.array([p0, p1, p2, p3, p4, p5, p6, p7])\n        label_corner_flipped = np.array([p2, p3, p0, p1, p6, p7, p4, p5])\n\n        ########################################################################\n        # normalize the 2dbbox img crop:\n        ########################################################################\n        bbox_2d_img = bbox_2d_img/255.0\n        bbox_2d_img = bbox_2d_img - np.array([0.485, 0.456, 0.406])\n        bbox_2d_img = bbox_2d_img/np.array([0.229, 0.224, 0.225]) # (shape: (H, W, 3))\n        bbox_2d_img = np.transpose(bbox_2d_img, (2, 0, 1)) # (shape: (3, H, W))\n        bbox_2d_img = bbox_2d_img.astype(np.float32)\n        ########################################################################\n\n        centered_frustum_point_cloud_camera = torch.from_numpy(centered_frustum_point_cloud_camera) # (shape: (1024, 4))\n        bbox_2d_img = torch.from_numpy(bbox_2d_img) # (shape: (3, H, W) = (3, 224, 224))\n        label_InstanceSeg = torch.from_numpy(label_InstanceSeg) # (shape: (1024, ))\n        label_TNet = torch.from_numpy(label_TNet) # (shape: (3, ))\n        label_BboxNet = torch.from_numpy(label_BboxNet) # (shape: (11, ))\n        label_corner = torch.from_numpy(label_corner) # (shape: (8, 3))\n        label_corner_flipped = torch.from_numpy(label_corner_flipped) # (shape: (8, 3))\n\n        return (centered_frustum_point_cloud_camera, bbox_2d_img, label_InstanceSeg, label_TNet, label_BboxNet, label_corner, label_corner_flipped, img_id, input_2Dbbox, frustum_R, frustum_angle, self.centered_frustum_mean_xyz)\n\n    def __len__(self):\n        return self.num_examples\n\nclass EvalSequenceDatasetFrustumPointNetImg(torch.utils.data.Dataset):\n    def __init__(self, kitti_data_path, kitti_meta_path, NH, sequence):\n        self.img_dir = kitti_data_path + ""/tracking/training/image_02/"" + sequence + ""/""\n        self.lidar_dir = kitti_data_path + ""/tracking/training/velodyne/"" + sequence + ""/""\n        self.label_path = kitti_data_path + ""/tracking/training/label_02/"" + sequence + "".txt""\n        self.calib_path = kitti_meta_path + ""/tracking/training/calib/"" + sequence + "".txt"" # NOTE! NOTE! the data format for the calib files was sliightly different for tracking, so I manually modifed the 20 files and saved them in the kitti_meta folder\n\n        self.NH = NH\n\n        with open(kitti_meta_path + ""/kitti_train_mean_car_size.pkl"", ""rb"") as file: # (needed for python3)\n            self.mean_car_size = pickle.load(file)\n\n        with open(kitti_meta_path + ""/kitti_centered_frustum_mean_xyz.pkl"", ""rb"") as file: # (needed for python3)\n            self.centered_frustum_mean_xyz = pickle.load(file)\n            self.centered_frustum_mean_xyz = self.centered_frustum_mean_xyz.astype(np.float32)\n\n        img_ids = []\n        img_names = os.listdir(self.img_dir)\n        for img_name in img_names:\n            img_id = img_name.split("".png"")[0]\n            img_ids.append(img_id)\n\n        self.examples = []\n        for img_id in img_ids:\n            if img_id.lstrip(\'0\') == \'\':\n                img_id_float = 0.0\n            else:\n                img_id_float = float(img_id.lstrip(\'0\'))\n\n            labels = LabelLoader2D3D_sequence(img_id, img_id_float, self.label_path, self.calib_path)\n\n            for label in labels:\n                label_2d = label[""label_2D""]\n                if label_2d[""truncated""] < 0.5 and label_2d[""class""] == ""Car"":\n                    label[""img_id""] = img_id\n                    self.examples.append(label)\n\n        self.num_examples = len(self.examples)\n\n    def __getitem__(self, index):\n        example = self.examples[index]\n\n        img_id = example[""img_id""]\n\n        lidar_path = self.lidar_dir + img_id + "".bin""\n        point_cloud = np.fromfile(lidar_path, dtype=np.float32).reshape(-1, 4)\n        orig_point_cloud = point_cloud\n\n        # remove points that are located behind the camera: # TODO! should we allow some points behind the camera when we actually have amodal 2D bboxes??\n        point_cloud = point_cloud[point_cloud[:, 0] > 0, :]\n        # remove points that are located too far away from the camera:\n        point_cloud = point_cloud[point_cloud[:, 0] < 80, :]\n\n        calib = calibread(self.calib_path)\n        P2 = calib[""P2""]\n        Tr_velo_to_cam_orig = calib[""Tr_velo_to_cam""]\n        R0_rect_orig = calib[""R0_rect""]\n        #\n        R0_rect = np.eye(4)\n        R0_rect[0:3, 0:3] = R0_rect_orig\n        #\n        Tr_velo_to_cam = np.eye(4)\n        Tr_velo_to_cam[0:3, :] = Tr_velo_to_cam_orig\n\n        point_cloud_xyz = point_cloud[:, 0:3]\n        point_cloud_xyz_hom = np.ones((point_cloud.shape[0], 4))\n        point_cloud_xyz_hom[:, 0:3] = point_cloud[:, 0:3] # (point_cloud_xyz_hom has shape (num_points, 4))\n\n        # project the points onto the image plane (homogeneous coords):\n        img_points_hom = np.dot(P2, np.dot(R0_rect, np.dot(Tr_velo_to_cam, point_cloud_xyz_hom.T))).T # (point_cloud_xyz_hom.T has shape (4, num_points))\n        # normalize:\n        img_points = np.zeros((img_points_hom.shape[0], 2))\n        img_points[:, 0] = img_points_hom[:, 0]/img_points_hom[:, 2]\n        img_points[:, 1] = img_points_hom[:, 1]/img_points_hom[:, 2]\n\n        # transform the points into (rectified) camera coordinates:\n        point_cloud_xyz_camera_hom = np.dot(R0_rect, np.dot(Tr_velo_to_cam, point_cloud_xyz_hom.T)).T # (point_cloud_xyz_hom.T has shape (4, num_points))\n        # normalize:\n        point_cloud_xyz_camera = np.zeros((point_cloud_xyz_camera_hom.shape[0], 3))\n        point_cloud_xyz_camera[:, 0] = point_cloud_xyz_camera_hom[:, 0]/point_cloud_xyz_camera_hom[:, 3]\n        point_cloud_xyz_camera[:, 1] = point_cloud_xyz_camera_hom[:, 1]/point_cloud_xyz_camera_hom[:, 3]\n        point_cloud_xyz_camera[:, 2] = point_cloud_xyz_camera_hom[:, 2]/point_cloud_xyz_camera_hom[:, 3]\n\n        point_cloud_camera = point_cloud\n        point_cloud_camera[:, 0:3] = point_cloud_xyz_camera\n\n        label_2D = example[""label_2D""]\n        label_3D = example[""label_3D""]\n\n        bbox = label_2D[""poly""]\n\n        # img = cv2.imread(self.img_dir + img_id + "".png"", -1)\n        # img_with_bboxes = draw_2d_polys(img, [label_2D])\n        # cv2.imwrite(""test.png"", img_with_bboxes)\n\n        ########################################################################\n        # frustum:\n        ########################################################################\n        u_min = bbox[0, 0] # (left)\n        u_max = bbox[1, 0] # (rigth)\n        v_min = bbox[0, 1] # (top)\n        v_max = bbox[2, 1] # (bottom)\n\n        u_min_expanded = u_min #- (u_max-u_min)*0.05\n        u_max_expanded = u_max #+ (u_max-u_min)*0.05\n        v_min_expanded = v_min #- (v_max-v_min)*0.05\n        v_max_expanded = v_max #+ (v_max-v_min)*0.05\n        input_2Dbbox = np.array([u_min_expanded, u_max_expanded, v_min_expanded, v_max_expanded])\n\n        row_mask = np.logical_and(\n                    np.logical_and(img_points[:, 0] >= u_min_expanded,\n                                   img_points[:, 0] <= u_max_expanded),\n                    np.logical_and(img_points[:, 1] >= v_min_expanded,\n                                   img_points[:, 1] <= v_max_expanded))\n\n        frustum_point_cloud_xyz = point_cloud_xyz[row_mask, :] # (needed only for visualization)\n        frustum_point_cloud = point_cloud[row_mask, :]\n        frustum_point_cloud_xyz_camera = point_cloud_xyz_camera[row_mask, :]\n        frustum_point_cloud_camera = point_cloud_camera[row_mask, :]\n\n        if frustum_point_cloud.shape[0] == 0:\n            print (img_id)\n            print (frustum_point_cloud.shape)\n            return self.__getitem__(0)\n\n        # randomly sample 1024 points in the frustum point cloud:\n        if frustum_point_cloud.shape[0] < 1024:\n            row_idx = np.random.choice(frustum_point_cloud.shape[0], 1024, replace=True)\n        else:\n            row_idx = np.random.choice(frustum_point_cloud.shape[0], 1024, replace=False)\n        frustum_point_cloud_xyz = frustum_point_cloud_xyz[row_idx, :]\n        frustum_point_cloud = frustum_point_cloud[row_idx, :]\n        frustum_point_cloud_xyz_camera = frustum_point_cloud_xyz_camera[row_idx, :]\n        frustum_point_cloud_camera = frustum_point_cloud_camera[row_idx, :]\n        # (the frustum point cloud now has exactly 1024 points)\n\n        ########################################################################\n        # get the input 2dbbox img crop and resize to 224 x 224:\n        ########################################################################\n        img_path = self.img_dir + img_id + "".png""\n        img = cv2.imread(img_path, -1)\n\n        bbox_2d_img = img[int(np.max([0, v_min])):int(v_max), int(np.max([0, u_min])):int(u_max)]\n        bbox_2d_img = cv2.resize(bbox_2d_img, (224, 224))\n\n        # cv2.imshow(""test"", bbox_2d_img)\n        # cv2.waitKey(0)\n\n        ########################################################################\n        # InstanceSeg ground truth:\n        ########################################################################\n        points = label_3D[""points""]\n\n        y_max = points[0, 1]\n        y_min = points[4, 1]\n\n        # D, A, B are consecutive corners of the rectangle (projection of the 3D bbox) in the x,z plane\n        # the vectors AB and AD are orthogonal\n        # a point P = (x, z) lies within the rectangle in the x,z plane iff:\n        # (0 < AP dot AB < AB dot AB) && (0 < AP dot AD < AD dot AD)\n        # (https://math.stackexchange.com/a/190373)\n\n        A = np.array([points[0, 0], points[0, 2]])\n        B = np.array([points[1, 0], points[1, 2]])\n        D = np.array([points[3, 0], points[3, 2]])\n\n        AB = B - A\n        AD = D - A\n        AB_dot_AB = np.dot(AB, AB)\n        AD_dot_AD = np.dot(AD, AD)\n\n        P = np.zeros((frustum_point_cloud_xyz_camera.shape[0], 2))\n        P[:, 0] = frustum_point_cloud_xyz_camera[:, 0]\n        P[:, 1] = frustum_point_cloud_xyz_camera[:, 2]\n\n        AP = P - A\n        AP_dot_AB = np.dot(AP, AB)\n        AP_dot_AD = np.dot(AP, AD)\n\n        row_mask = np.logical_and(\n                    np.logical_and(frustum_point_cloud_xyz_camera[:, 1] >= y_min, frustum_point_cloud_xyz_camera[:, 1] <= y_max),\n                    np.logical_and(np.logical_and(AP_dot_AB >= 0, AP_dot_AB <= AB_dot_AB),\n                                   np.logical_and(AP_dot_AD >= 0, AP_dot_AD <= AD_dot_AD)))\n\n        gt_point_cloud_xyz_camera = frustum_point_cloud_xyz_camera[row_mask, :] # (needed only for visualization)\n\n        label_InstanceSeg = np.zeros((frustum_point_cloud.shape[0],), dtype=np.int64)\n        label_InstanceSeg[row_mask] = 1 # (0: point is NOT part of the objet, 1: point is part of the object)\n\n        ########################################################################\n        # visualization of frustum and InstanceSeg ground truth:\n        ########################################################################\n        # import sys\n        # sys.path.append(""/home/fregu856/exjobb/Open3D/build/lib"")\n        # from py3d import *\n        #\n        # # visualize the frustum points with ground truth as colored points:\n        # frustum_pcd_camera = PointCloud()\n        # frustum_pcd_camera.points = Vector3dVector(frustum_point_cloud_xyz_camera)\n        # frustum_pcd_camera.paint_uniform_color([0.25, 0.25, 0.25])\n        # gt_pcd_camera = PointCloud()\n        # gt_pcd_camera.points = Vector3dVector(gt_point_cloud_xyz_camera)\n        # gt_pcd_camera.paint_uniform_color([0, 0, 1])\n        # draw_geometries([gt_pcd_camera, frustum_pcd_camera])\n\n        # # visualize the frustum points and gt points as differently colored points in the full point cloud:\n        # frustum_pcd_camera.paint_uniform_color([1, 0, 0])\n        # pcd_camera = PointCloud()\n        # pcd_camera.points = Vector3dVector(orig_point_cloud_camera[:, 0:3])\n        # pcd_camera.paint_uniform_color([0.25, 0.25, 0.25])\n        # draw_geometries([gt_pcd_camera, frustum_pcd_camera, pcd_camera])\n\n        ########################################################################\n        # normalize frustum point cloud:\n        ########################################################################\n        # get the 2dbbox center img point in hom. coords:\n        u_center = u_min_expanded + (u_max_expanded - u_min_expanded)/2.0\n        v_center = v_min_expanded + (v_max_expanded - v_min_expanded)/2.0\n        center_img_point_hom = np.array([u_center, v_center, 1])\n\n        # (more than one 3D point is projected onto the center image point, i.e,\n        # the linear system of equations is under-determined and has inf number\n        # of solutions. By using the pseudo-inverse, we obtain the least-norm sol)\n\n        # get a point (the least-norm sol.) that projects onto the center image point, in hom. coords:\n        P2_pseudo_inverse = np.linalg.pinv(P2) # (shape: (4, 3)) (P2 has shape (3, 4))\n        point_hom = np.dot(P2_pseudo_inverse, center_img_point_hom)\n\n        # hom --> normal coords:\n        point = np.array(([point_hom[0]/point_hom[3], point_hom[1]/point_hom[3], point_hom[2]/point_hom[3]]))\n\n        # if the point is behind the camera, switch to the mirror point in front of the camera:\n        if point[2] < 0:\n            point[0] = -point[0]\n            point[2] = -point[2]\n\n        # compute the angle of the point in the x-z plane: ((rectified) camera coords)\n        frustum_angle = np.arctan2(point[0], point[2]) # (np.arctan2(x, z)) # (frustum_angle = 0: frustum is centered)\n\n        # rotation_matrix to rotate points frustum_angle around the y axis (counter-clockwise):\n        frustum_R = np.asarray([[np.cos(frustum_angle), 0, -np.sin(frustum_angle)],\n                           [0, 1, 0],\n                           [np.sin(frustum_angle), 0, np.cos(frustum_angle)]],\n                           dtype=\'float32\')\n\n        # rotate the frustum point cloud to center it:\n        centered_frustum_point_cloud_xyz_camera = np.dot(frustum_R, frustum_point_cloud_xyz_camera.T).T\n\n        # subtract the centered frustum train xyz mean:\n        centered_frustum_point_cloud_xyz_camera -= self.centered_frustum_mean_xyz\n\n        centered_frustum_point_cloud_camera = frustum_point_cloud_camera\n        centered_frustum_point_cloud_camera[:, 0:3] = centered_frustum_point_cloud_xyz_camera\n\n        # # # # # # # # # # debug visualizations START:\n        # import sys\n        # sys.path.append(""/home/fregu856/exjobb/Open3D/build/lib"")\n        # from py3d import *\n        #\n        # frustum_pcd_camera = PointCloud()\n        # frustum_pcd_camera.points = Vector3dVector(frustum_point_cloud_xyz_camera)\n        # frustum_pcd_camera.paint_uniform_color([0.25, 0.25, 0.25])\n        #\n        # centered_frustum_pcd_camera = PointCloud()\n        # centered_frustum_pcd_camera.points = Vector3dVector(centered_frustum_point_cloud_xyz_camera)\n        # centered_frustum_pcd_camera.paint_uniform_color([1, 0, 0])\n        # draw_geometries([centered_frustum_pcd_camera, frustum_pcd_camera])\n        # # # # # # # # # # debug visualizations END:\n\n        ########################################################################\n        # TNet ground truth:\n        ########################################################################\n        label_TNet = np.dot(frustum_R, label_3D[""center""]) - self.centered_frustum_mean_xyz\n\n        # # # # # # # # # # debug visualizations START:\n        # import sys\n        # sys.path.append(""/home/fregu856/exjobb/Open3D/build/lib"")\n        # from py3d import *\n        #\n        # # visualize InstanceSeg GT and 3dbbox center GT:\n        # frustum_pcd_camera = PointCloud()\n        # frustum_pcd_camera.points = Vector3dVector(frustum_point_cloud_xyz_camera)\n        # frustum_pcd_camera.paint_uniform_color([0.25, 0.25, 0.25cd\n        # gt_pcd_camera = PointCloud()\n        # gt_pcd_camera.points = Vector3dVector(gt_point_cloud_xyz_camera)\n        # gt_pcd_camera.paint_uniform_color([0, 0, 1])\n        # center_gt_pcd_camera = PointCloud()\n        # center_gt_pcd_camera.points = Vector3dVector(np.array([label_TNet]))\n        # center_gt_pcd_camera.paint_uniform_color([0, 1, 0])\n        # draw_geometries([center_gt_pcd_camera, gt_pcd_camera, frustum_pcd_camera])\n        # # # # # # # # # # debug visualizations END:\n\n        ########################################################################\n        # BboxNet ground truth:\n        ########################################################################\n        centered_r_y = wrapToPi(label_3D[\'r_y\'] - frustum_angle)\n        bin_number = getBinNumber(centered_r_y, NH=self.NH)\n        bin_center = getBinCenter(bin_number, NH=self.NH)\n        residual = wrapToPi(centered_r_y - bin_center)\n\n        label_BboxNet = np.zeros((11, ), dtype=np.float32) # ([x, y, z, h, w, l, r_y_bin_number, r_y_residual, r_y_bin_number_neighbor,r_y_residual_neighbor, h_mean, w_mean, l_mean])\n\n        label_BboxNet[0:3] = np.dot(frustum_R, label_3D[""center""]) - self.centered_frustum_mean_xyz\n        label_BboxNet[3] = label_3D[\'h\']\n        label_BboxNet[4] = label_3D[\'w\']\n        label_BboxNet[5] = label_3D[\'l\']\n        label_BboxNet[6] = bin_number\n        label_BboxNet[7] = residual\n        label_BboxNet[8:] = self.mean_car_size\n\n        ########################################################################\n        # corner loss ground truth:\n        ########################################################################\n        Rmat = np.asarray([[math.cos(residual), 0, math.sin(residual)],\n                           [0, 1, 0],\n                           [-math.sin(residual), 0, math.cos(residual)]],\n                           dtype=\'float32\')\n\n        center = label_BboxNet[0:3]\n        l = label_3D[\'l\']\n        w = label_3D[\'w\']\n        h = label_3D[\'h\']\n        p0 = center + np.dot(Rmat, np.asarray([l/2.0, 0, w/2.0], dtype=\'float32\').flatten())\n        p1 = center + np.dot(Rmat, np.asarray([-l/2.0, 0, w/2.0], dtype=\'float32\').flatten())\n        p2 = center + np.dot(Rmat, np.asarray([-l/2.0, 0, -w/2.0], dtype=\'float32\').flatten())\n        p3 = center + np.dot(Rmat, np.asarray([l/2.0, 0, -w/2.0], dtype=\'float32\').flatten())\n        p4 = center + np.dot(Rmat, np.asarray([l/2.0, -h, w/2.0], dtype=\'float32\').flatten())\n        p5 = center + np.dot(Rmat, np.asarray([-l/2.0, -h, w/2.0], dtype=\'float32\').flatten())\n        p6 = center + np.dot(Rmat, np.asarray([-l/2.0, -h, -w/2.0], dtype=\'float32\').flatten())\n        p7 = center + np.dot(Rmat, np.asarray([l/2.0, -h, -w/2.0], dtype=\'float32\').flatten())\n        label_corner = np.array([p0, p1, p2, p3, p4, p5, p6, p7])\n        label_corner_flipped = np.array([p2, p3, p0, p1, p6, p7, p4, p5])\n\n        ########################################################################\n        # normalize the 2dbbox img crop:\n        ########################################################################\n        bbox_2d_img = bbox_2d_img/255.0\n        bbox_2d_img = bbox_2d_img - np.array([0.485, 0.456, 0.406])\n        bbox_2d_img = bbox_2d_img/np.array([0.229, 0.224, 0.225]) # (shape: (H, W, 3))\n        bbox_2d_img = np.transpose(bbox_2d_img, (2, 0, 1)) # (shape: (3, H, W))\n        bbox_2d_img = bbox_2d_img.astype(np.float32)\n        ########################################################################\n\n        centered_frustum_point_cloud_camera = torch.from_numpy(centered_frustum_point_cloud_camera) # (shape: (1024, 4))\n        bbox_2d_img = torch.from_numpy(bbox_2d_img) # (shape: (3, H, W) = (3, 224, 224))\n        label_InstanceSeg = torch.from_numpy(label_InstanceSeg) # (shape: (1024, ))\n        label_TNet = torch.from_numpy(label_TNet) # (shape: (3, ))\n        label_BboxNet = torch.from_numpy(label_BboxNet) # (shape: (11, ))\n        label_corner = torch.from_numpy(label_corner) # (shape: (8, 3))\n        label_corner_flipped = torch.from_numpy(label_corner_flipped) # (shape: (8, 3))\n\n        return (centered_frustum_point_cloud_camera, bbox_2d_img, label_InstanceSeg, label_TNet, label_BboxNet, label_corner, label_corner_flipped, img_id, input_2Dbbox, frustum_R, frustum_angle, self.centered_frustum_mean_xyz)\n\n    def __len__(self):\n        return self.num_examples\n\nclass DatasetKittiTestSequence(torch.utils.data.Dataset):\n    def __init__(self, kitti_data_path, kitti_meta_path, NH, sequence):\n        self.img_dir = kitti_data_path + ""/tracking/testing/image_02/"" + sequence + ""/""\n        self.lidar_dir = kitti_data_path + ""/tracking/testing/velodyne/"" + sequence + ""/""\n        self.calib_path = kitti_meta_path + ""/tracking/testing/calib/"" + sequence + "".txt"" # NOTE! NOTE! the data format for the calib files was sliightly different for tracking, so I manually modifed the 28 files and saved them in the kitti_meta folder\n        self.detections_2d_path = kitti_meta_path + ""/tracking/testing/2d_detections/"" + sequence + ""/inferResult_1.txt""\n\n        self.NH = NH\n\n        with open(kitti_meta_path + ""/kitti_train_mean_car_size.pkl"", ""rb"") as file: # (needed for python3)\n            self.mean_car_size = pickle.load(file)\n\n        with open(kitti_meta_path + ""/kitti_centered_frustum_mean_xyz.pkl"", ""rb"") as file: # (needed for python3)\n            self.centered_frustum_mean_xyz = pickle.load(file)\n            self.centered_frustum_mean_xyz = self.centered_frustum_mean_xyz.astype(np.float32)\n\n        self.examples = []\n        with open(self.detections_2d_path) as file:\n            # line format: img_id, img_height, img_width, class, u_min, v_min, u_max, v_max, confidence score, distance estimate\n            for line in file:\n                values = line.split()\n                object_class = float(values[3])\n                if object_class == 1: # (1: Car)\n                    img_id = values[0]\n                    u_min = float(values[4])\n                    v_min = float(values[5])\n                    u_max = float(values[6])\n                    v_max = float(values[7])\n                    score_2d = float(values[8])\n\n                    detection_2d = {}\n                    detection_2d[""u_min""] = u_min\n                    detection_2d[""v_min""] = v_min\n                    detection_2d[""u_max""] = u_max\n                    detection_2d[""v_max""] = v_max\n                    detection_2d[""score_2d""] = score_2d\n                    detection_2d[""img_id""] = img_id\n\n                    self.examples.append(detection_2d)\n\n        self.num_examples = len(self.examples)\n\n    def __getitem__(self, index):\n        example = self.examples[index]\n\n        img_id = example[""img_id""]\n\n        lidar_path = self.lidar_dir + img_id + "".bin""\n        point_cloud = np.fromfile(lidar_path, dtype=np.float32).reshape(-1, 4)\n        orig_point_cloud = point_cloud\n\n        # remove points that are located behind the camera:\n        point_cloud = point_cloud[point_cloud[:, 0] > -5, :]\n        # remove points that are located too far away from the camera:\n        point_cloud = point_cloud[point_cloud[:, 0] < 80, :]\n\n        calib = calibread(self.calib_path)\n        P2 = calib[""P2""]\n        Tr_velo_to_cam_orig = calib[""Tr_velo_to_cam""]\n        R0_rect_orig = calib[""R0_rect""]\n        #\n        R0_rect = np.eye(4)\n        R0_rect[0:3, 0:3] = R0_rect_orig\n        #\n        Tr_velo_to_cam = np.eye(4)\n        Tr_velo_to_cam[0:3, :] = Tr_velo_to_cam_orig\n\n        point_cloud_xyz = point_cloud[:, 0:3]\n        point_cloud_xyz_hom = np.ones((point_cloud.shape[0], 4))\n        point_cloud_xyz_hom[:, 0:3] = point_cloud[:, 0:3] # (point_cloud_xyz_hom has shape (num_points, 4))\n\n        # project the points onto the image plane (homogeneous coords):\n        img_points_hom = np.dot(P2, np.dot(R0_rect, np.dot(Tr_velo_to_cam, point_cloud_xyz_hom.T))).T # (point_cloud_xyz_hom.T has shape (4, num_points))\n        # normalize:\n        img_points = np.zeros((img_points_hom.shape[0], 2))\n        img_points[:, 0] = img_points_hom[:, 0]/img_points_hom[:, 2]\n        img_points[:, 1] = img_points_hom[:, 1]/img_points_hom[:, 2]\n\n        # transform the points into (rectified) camera coordinates:\n        point_cloud_xyz_camera_hom = np.dot(R0_rect, np.dot(Tr_velo_to_cam, point_cloud_xyz_hom.T)).T # (point_cloud_xyz_hom.T has shape (4, num_points))\n        # normalize:\n        point_cloud_xyz_camera = np.zeros((point_cloud_xyz_camera_hom.shape[0], 3))\n        point_cloud_xyz_camera[:, 0] = point_cloud_xyz_camera_hom[:, 0]/point_cloud_xyz_camera_hom[:, 3]\n        point_cloud_xyz_camera[:, 1] = point_cloud_xyz_camera_hom[:, 1]/point_cloud_xyz_camera_hom[:, 3]\n        point_cloud_xyz_camera[:, 2] = point_cloud_xyz_camera_hom[:, 2]/point_cloud_xyz_camera_hom[:, 3]\n\n        point_cloud_camera = point_cloud\n        point_cloud_camera[:, 0:3] = point_cloud_xyz_camera\n\n        ########################################################################\n        # frustum:\n        ########################################################################\n        u_min = example[""u_min""] # (left)\n        u_max = example[""u_max""] # (rigth)\n        v_min = example[""v_min""] # (top)\n        v_max = example[""v_max""] # (bottom)\n\n        input_2Dbbox = np.array([u_min, u_max, v_min, v_max])\n\n        row_mask = np.logical_and(\n                    np.logical_and(img_points[:, 0] >= u_min,\n                                   img_points[:, 0] <= u_max),\n                    np.logical_and(img_points[:, 1] >= v_min,\n                                   img_points[:, 1] <= v_max))\n\n        frustum_point_cloud_xyz = point_cloud_xyz[row_mask, :] # (needed only for visualization)\n        frustum_point_cloud = point_cloud[row_mask, :]\n        frustum_point_cloud_xyz_camera = point_cloud_xyz_camera[row_mask, :]\n        frustum_point_cloud_camera = point_cloud_camera[row_mask, :]\n\n        empty_frustum_flag = 0\n        if frustum_point_cloud.shape[0] == 0:\n            empty_frustum_flag = 1\n            frustum_point_cloud = np.zeros((1024, 4), dtype=np.float32)\n            frustum_point_cloud_xyz = np.zeros((1024, 3), dtype=np.float32)\n            frustum_point_cloud_camera = np.zeros((1024, 4), dtype=np.float32)\n            frustum_point_cloud_xyz_camera = np.zeros((1024, 3), dtype=np.float32)\n\n        # randomly sample 1024 points in the frustum point cloud:\n        if frustum_point_cloud.shape[0] < 1024:\n            row_idx = np.random.choice(frustum_point_cloud.shape[0], 1024, replace=True)\n        else:\n            row_idx = np.random.choice(frustum_point_cloud.shape[0], 1024, replace=False)\n        frustum_point_cloud_xyz = frustum_point_cloud_xyz[row_idx, :]\n        frustum_point_cloud = frustum_point_cloud[row_idx, :]\n        frustum_point_cloud_xyz_camera = frustum_point_cloud_xyz_camera[row_idx, :]\n        frustum_point_cloud_camera = frustum_point_cloud_camera[row_idx, :]\n        # (the frustum point cloud now has exactly 1024 points)\n\n        ########################################################################\n        # get the input 2dbbox img crop and resize to 224 x 224:\n        ########################################################################\n        img_path = self.img_dir + img_id + "".png""\n        img = cv2.imread(img_path, -1)\n\n        bbox_2d_img = img[int(np.max([0, v_min])):int(v_max), int(np.max([0, u_min])):int(u_max)]\n        bbox_2d_img = cv2.resize(bbox_2d_img, (224, 224))\n\n        # cv2.imshow(""test"", bbox_2d_img)\n        # cv2.waitKey(0)\n\n        ########################################################################\n        # normalize frustum point cloud:\n        ########################################################################\n        # get the 2dbbox center img point in hom. coords:\n        u_center = u_min + (u_max - u_min)/2.0\n        v_center = v_min + (v_max - v_min)/2.0\n        center_img_point_hom = np.array([u_center, v_center, 1])\n\n        # (more than one 3D point is projected onto the center image point, i.e,\n        # the linear system of equations is under-determined and has inf number\n        # of solutions. By using the pseudo-inverse, we obtain the least-norm sol)\n\n        # get a point (the least-norm sol.) that projects onto the center image point, in hom. coords:\n        P2_pseudo_inverse = np.linalg.pinv(P2) # (shape: (4, 3)) (P2 has shape (3, 4))\n        point_hom = np.dot(P2_pseudo_inverse, center_img_point_hom)\n\n        # hom --> normal coords:\n        point = np.array(([point_hom[0]/point_hom[3], point_hom[1]/point_hom[3], point_hom[2]/point_hom[3]]))\n\n        # if the point is behind the camera, switch to the mirror point in front of the camera:\n        if point[2] < 0:\n            point[0] = -point[0]\n            point[2] = -point[2]\n\n        # compute the angle of the point in the x-z plane: ((rectified) camera coords)\n        frustum_angle = np.arctan2(point[0], point[2]) # (np.arctan2(x, z)) # (frustum_angle = 0: frustum is centered)\n\n        # rotation_matrix to rotate points frustum_angle around the y axis (counter-clockwise):\n        frustum_R = np.asarray([[np.cos(frustum_angle), 0, -np.sin(frustum_angle)],\n                           [0, 1, 0],\n                           [np.sin(frustum_angle), 0, np.cos(frustum_angle)]],\n                           dtype=\'float32\')\n\n        # rotate the frustum point cloud to center it:\n        centered_frustum_point_cloud_xyz_camera = np.dot(frustum_R, frustum_point_cloud_xyz_camera.T).T\n\n        # subtract the centered frustum train xyz mean:\n        centered_frustum_point_cloud_xyz_camera -= self.centered_frustum_mean_xyz\n\n        centered_frustum_point_cloud_camera = frustum_point_cloud_camera\n        centered_frustum_point_cloud_camera[:, 0:3] = centered_frustum_point_cloud_xyz_camera\n\n        # # # # # # # # # # debug visualizations START:\n        # import sys\n        # sys.path.append(""/home/fregu856/exjobb/Open3D/build/lib"")\n        # from py3d import *\n        # frustum_pcd_camera = PointCloud()\n        # frustum_pcd_camera.points = Vector3dVector(frustum_point_cloud_xyz_camera)\n        # frustum_pcd_camera.paint_uniform_color([0.25, 0.25, 0.25])\n        # centered_frustum_pcd_camera = PointCloud()\n        # centered_frustum_pcd_camera.points = Vector3dVector(centered_frustum_point_cloud_xyz_camera)\n        # centered_frustum_pcd_camera.paint_uniform_color([1, 0, 0])\n        # draw_geometries([centered_frustum_pcd_camera, frustum_pcd_camera])\n        # # # # # # # # # # debug visualizations END:\n\n        ########################################################################\n        # normalize the 2dbbox img crop:\n        ########################################################################\n        bbox_2d_img = bbox_2d_img/255.0\n        bbox_2d_img = bbox_2d_img - np.array([0.485, 0.456, 0.406])\n        bbox_2d_img = bbox_2d_img/np.array([0.229, 0.224, 0.225]) # (shape: (H, W, 3))\n        bbox_2d_img = np.transpose(bbox_2d_img, (2, 0, 1)) # (shape: (3, H, W))\n        bbox_2d_img = bbox_2d_img.astype(np.float32)\n\n        centered_frustum_point_cloud_camera = torch.from_numpy(centered_frustum_point_cloud_camera) # (shape: (1024, 4))\n        bbox_2d_img = torch.from_numpy(bbox_2d_img) # (shape: (3, H, W) = (3, 224, 224))\n\n        return (centered_frustum_point_cloud_camera, bbox_2d_img, img_id, input_2Dbbox, frustum_R, frustum_angle, empty_frustum_flag, self.centered_frustum_mean_xyz, self.mean_car_size)\n\n    def __len__(self):\n        return self.num_examples\n\nclass DatasetKittiTest(torch.utils.data.Dataset):\n    def __init__(self, kitti_data_path, kitti_meta_path, NH):\n        self.img_dir = kitti_data_path + ""/object/testing/image_2/""\n        self.calib_dir = kitti_data_path + ""/object/testing/calib/""\n        self.lidar_dir = kitti_data_path + ""/object/testing/velodyne/""\n        self.detections_2d_dir = kitti_meta_path + ""/object/testing/2d_detections/""\n\n        self.NH = NH\n\n        with open(kitti_meta_path + ""/kitti_train_mean_car_size.pkl"", ""rb"") as file: # (needed for python3)\n            self.mean_car_size = pickle.load(file)\n\n        with open(kitti_meta_path + ""/kitti_centered_frustum_mean_xyz.pkl"", ""rb"") as file: # (needed for python3)\n            self.centered_frustum_mean_xyz = pickle.load(file)\n            self.centered_frustum_mean_xyz = self.centered_frustum_mean_xyz.astype(np.float32)\n\n        img_ids = []\n        img_names = os.listdir(self.img_dir)\n        for img_name in img_names:\n            img_id = img_name.split("".png"")[0]\n            img_ids.append(img_id)\n\n        self.examples = []\n        for img_id in img_ids:\n            detections_file_path = self.detections_2d_dir + img_id + "".txt""\n            with open(detections_file_path) as file:\n                # line format: img_id, img_height, img_width, class, u_min, v_min, u_max, v_max, confidence score, distance estimate\n                for line in file:\n                    values = line.split()\n                    object_class = float(values[3])\n                    if object_class == 1: # (1: Car)\n                        u_min = float(values[4])\n                        v_min = float(values[5])\n                        u_max = float(values[6])\n                        v_max = float(values[7])\n                        score_2d = float(values[8])\n\n                        detection_2d = {}\n                        detection_2d[""u_min""] = u_min\n                        detection_2d[""v_min""] = v_min\n                        detection_2d[""u_max""] = u_max\n                        detection_2d[""v_max""] = v_max\n                        detection_2d[""score_2d""] = score_2d\n                        detection_2d[""img_id""] = img_id\n\n                        self.examples.append(detection_2d)\n\n        self.num_examples = len(self.examples)\n\n    def __getitem__(self, index):\n        example = self.examples[index]\n\n        img_id = example[""img_id""]\n\n        lidar_path = self.lidar_dir + img_id + "".bin""\n        point_cloud = np.fromfile(lidar_path, dtype=np.float32).reshape(-1, 4)\n        orig_point_cloud = point_cloud\n\n        # remove points that are located behind the camera:\n        point_cloud = point_cloud[point_cloud[:, 0] > -5, :]\n        # remove points that are located too far away from the camera:\n        point_cloud = point_cloud[point_cloud[:, 0] < 80, :]\n\n        calib = calibread(self.calib_dir + img_id + "".txt"")\n        P2 = calib[""P2""]\n        Tr_velo_to_cam_orig = calib[""Tr_velo_to_cam""]\n        R0_rect_orig = calib[""R0_rect""]\n        #\n        R0_rect = np.eye(4)\n        R0_rect[0:3, 0:3] = R0_rect_orig\n        #\n        Tr_velo_to_cam = np.eye(4)\n        Tr_velo_to_cam[0:3, :] = Tr_velo_to_cam_orig\n\n        point_cloud_xyz = point_cloud[:, 0:3]\n        point_cloud_xyz_hom = np.ones((point_cloud.shape[0], 4))\n        point_cloud_xyz_hom[:, 0:3] = point_cloud[:, 0:3] # (point_cloud_xyz_hom has shape (num_points, 4))\n\n        # project the points onto the image plane (homogeneous coords):\n        img_points_hom = np.dot(P2, np.dot(R0_rect, np.dot(Tr_velo_to_cam, point_cloud_xyz_hom.T))).T # (point_cloud_xyz_hom.T has shape (4, num_points))\n        # normalize:\n        img_points = np.zeros((img_points_hom.shape[0], 2))\n        img_points[:, 0] = img_points_hom[:, 0]/img_points_hom[:, 2]\n        img_points[:, 1] = img_points_hom[:, 1]/img_points_hom[:, 2]\n\n        # transform the points into (rectified) camera coordinates:\n        point_cloud_xyz_camera_hom = np.dot(R0_rect, np.dot(Tr_velo_to_cam, point_cloud_xyz_hom.T)).T # (point_cloud_xyz_hom.T has shape (4, num_points))\n        # normalize:\n        point_cloud_xyz_camera = np.zeros((point_cloud_xyz_camera_hom.shape[0], 3))\n        point_cloud_xyz_camera[:, 0] = point_cloud_xyz_camera_hom[:, 0]/point_cloud_xyz_camera_hom[:, 3]\n        point_cloud_xyz_camera[:, 1] = point_cloud_xyz_camera_hom[:, 1]/point_cloud_xyz_camera_hom[:, 3]\n        point_cloud_xyz_camera[:, 2] = point_cloud_xyz_camera_hom[:, 2]/point_cloud_xyz_camera_hom[:, 3]\n\n        point_cloud_camera = point_cloud\n        point_cloud_camera[:, 0:3] = point_cloud_xyz_camera\n\n        ########################################################################\n        # frustum:\n        ########################################################################\n        u_min = example[""u_min""] # (left)\n        u_max = example[""u_max""] # (rigth)\n        v_min = example[""v_min""] # (top)\n        v_max = example[""v_max""] # (bottom)\n\n        score_2d = example[""score_2d""]\n\n        input_2Dbbox = np.array([u_min, u_max, v_min, v_max])\n\n        row_mask = np.logical_and(\n                    np.logical_and(img_points[:, 0] >= u_min,\n                                   img_points[:, 0] <= u_max),\n                    np.logical_and(img_points[:, 1] >= v_min,\n                                   img_points[:, 1] <= v_max))\n\n        frustum_point_cloud_xyz = point_cloud_xyz[row_mask, :] # (needed only for visualization)\n        frustum_point_cloud = point_cloud[row_mask, :]\n        frustum_point_cloud_xyz_camera = point_cloud_xyz_camera[row_mask, :]\n        frustum_point_cloud_camera = point_cloud_camera[row_mask, :]\n\n        empty_frustum_flag = 0\n        if frustum_point_cloud.shape[0] == 0:\n            empty_frustum_flag = 1\n            frustum_point_cloud = np.zeros((1024, 4), dtype=np.float32)\n            frustum_point_cloud_xyz = np.zeros((1024, 3), dtype=np.float32)\n            frustum_point_cloud_camera = np.zeros((1024, 4), dtype=np.float32)\n            frustum_point_cloud_xyz_camera = np.zeros((1024, 3), dtype=np.float32)\n\n        # randomly sample 1024 points in the frustum point cloud:\n        if frustum_point_cloud.shape[0] < 1024:\n            row_idx = np.random.choice(frustum_point_cloud.shape[0], 1024, replace=True)\n        else:\n            row_idx = np.random.choice(frustum_point_cloud.shape[0], 1024, replace=False)\n        frustum_point_cloud_xyz = frustum_point_cloud_xyz[row_idx, :]\n        frustum_point_cloud = frustum_point_cloud[row_idx, :]\n        frustum_point_cloud_xyz_camera = frustum_point_cloud_xyz_camera[row_idx, :]\n        frustum_point_cloud_camera = frustum_point_cloud_camera[row_idx, :]\n        # (the frustum point cloud now has exactly 1024 points)\n\n        ########################################################################\n        # get the input 2dbbox img crop and resize to 224 x 224:\n        ########################################################################\n        img_path = self.img_dir + img_id + "".png""\n        img = cv2.imread(img_path, -1)\n\n        bbox_2d_img = img[int(np.max([0, v_min])):int(v_max), int(np.max([0, u_min])):int(u_max)]\n        bbox_2d_img = cv2.resize(bbox_2d_img, (224, 224))\n\n        ########################################################################\n        # normalize frustum point cloud:\n        ########################################################################\n        # get the 2dbbox center img point in hom. coords:\n        u_center = u_min + (u_max - u_min)/2.0\n        v_center = v_min + (v_max - v_min)/2.0\n        center_img_point_hom = np.array([u_center, v_center, 1])\n\n        # (more than one 3D point is projected onto the center image point, i.e,\n        # the linear system of equations is under-determined and has inf number\n        # of solutions. By using the pseudo-inverse, we obtain the least-norm sol)\n\n        # get a point (the least-norm sol.) that projects onto the center image point, in hom. coords:\n        P2_pseudo_inverse = np.linalg.pinv(P2) # (shape: (4, 3)) (P2 has shape (3, 4))\n        point_hom = np.dot(P2_pseudo_inverse, center_img_point_hom)\n\n        # hom --> normal coords:\n        point = np.array(([point_hom[0]/point_hom[3], point_hom[1]/point_hom[3], point_hom[2]/point_hom[3]]))\n\n        # if the point is behind the camera, switch to the mirror point in front of the camera:\n        if point[2] < 0:\n            point[0] = -point[0]\n            point[2] = -point[2]\n\n        # compute the angle of the point in the x-z plane: ((rectified) camera coords)\n        frustum_angle = np.arctan2(point[0], point[2]) # (np.arctan2(x, z)) # (frustum_angle = 0: frustum is centered)\n\n        # rotation_matrix to rotate points frustum_angle around the y axis (counter-clockwise):\n        frustum_R = np.asarray([[np.cos(frustum_angle), 0, -np.sin(frustum_angle)],\n                           [0, 1, 0],\n                           [np.sin(frustum_angle), 0, np.cos(frustum_angle)]],\n                           dtype=\'float32\')\n\n        # rotate the frustum point cloud to center it:\n        centered_frustum_point_cloud_xyz_camera = np.dot(frustum_R, frustum_point_cloud_xyz_camera.T).T\n\n        # subtract the centered frustum train xyz mean:\n        centered_frustum_point_cloud_xyz_camera -= self.centered_frustum_mean_xyz\n\n        centered_frustum_point_cloud_camera = frustum_point_cloud_camera\n        centered_frustum_point_cloud_camera[:, 0:3] = centered_frustum_point_cloud_xyz_camera\n\n        # # # # # # # # # # debug visualizations START:\n        # import sys\n        # sys.path.append(""/home/fregu856/exjobb/Open3D/build/lib"")\n        # from py3d import *\n        # frustum_pcd_camera = PointCloud()\n        # frustum_pcd_camera.points = Vector3dVector(frustum_point_cloud_xyz_camera)\n        # frustum_pcd_camera.paint_uniform_color([0.25, 0.25, 0.25])\n        # centered_frustum_pcd_camera = PointCloud()\n        # centered_frustum_pcd_camera.points = Vector3dVector(centered_frustum_point_cloud_xyz_camera)\n        # centered_frustum_pcd_camera.paint_uniform_color([1, 0, 0])\n        # draw_geometries([centered_frustum_pcd_camera, frustum_pcd_camera])\n        # # # # # # # # # # debug visualizations START:\n\n        ########################################################################\n        # normalize the 2dbbox img crop:\n        ########################################################################\n        bbox_2d_img = bbox_2d_img/255.0\n        bbox_2d_img = bbox_2d_img - np.array([0.485, 0.456, 0.406])\n        bbox_2d_img = bbox_2d_img/np.array([0.229, 0.224, 0.225]) # (shape: (H, W, 3))\n        bbox_2d_img = np.transpose(bbox_2d_img, (2, 0, 1)) # (shape: (3, H, W))\n        bbox_2d_img = bbox_2d_img.astype(np.float32)\n\n        centered_frustum_point_cloud_camera = torch.from_numpy(centered_frustum_point_cloud_camera) # (shape: (1024, 3))\n        bbox_2d_img = torch.from_numpy(bbox_2d_img) # (shape: (3, H, W) = (3, 224, 224))\n\n        return (centered_frustum_point_cloud_camera, bbox_2d_img, img_id, input_2Dbbox, frustum_R, frustum_angle, empty_frustum_flag, self.centered_frustum_mean_xyz, self.mean_car_size, score_2d)\n\n    def __len__(self):\n        return self.num_examples\n\nclass DatasetKittiVal2ddetections(torch.utils.data.Dataset):\n    def __init__(self, kitti_data_path, kitti_meta_path, NH):\n        self.img_dir = kitti_data_path + ""/object/training/image_2/""\n        self.calib_dir = kitti_data_path + ""/object/training/calib/""\n        self.lidar_dir = kitti_data_path + ""/object/training/velodyne/""\n        self.detections_2d_path = kitti_meta_path + ""/rgb_detection_val.txt""\n\n        self.NH = NH\n\n        with open(kitti_meta_path + ""/val_img_ids.pkl"", ""rb"") as file: # (needed for python3)\n            img_ids = pickle.load(file)\n\n        with open(kitti_meta_path + ""/kitti_train_mean_car_size.pkl"", ""rb"") as file: # (needed for python3)\n            self.mean_car_size = pickle.load(file)\n\n        with open(kitti_meta_path + ""/kitti_centered_frustum_mean_xyz.pkl"", ""rb"") as file: # (needed for python3)\n            self.centered_frustum_mean_xyz = pickle.load(file)\n            self.centered_frustum_mean_xyz = self.centered_frustum_mean_xyz.astype(np.float32)\n\n        self.examples = []\n        with open(self.detections_2d_path) as file:\n            # line format: /home/rqi/Data/KITTI/object/training/image_2/***img_id***.png, class, conf_score, u_min, v_min, u_max, v_max\n            for line in file:\n                values = line.split()\n                object_class = float(values[1])\n                if object_class == 2: # (2: Car)\n                    score_2d = float(values[2])\n                    u_min = float(values[3])\n                    v_min = float(values[4])\n                    u_max = float(values[5])\n                    v_max = float(values[6])\n\n                    img_id = values[0].split(""image_2/"")[1]\n                    img_id = img_id.split(""."")[0]\n\n                    detection_2d = {}\n                    detection_2d[""u_min""] = u_min\n                    detection_2d[""v_min""] = v_min\n                    detection_2d[""u_max""] = u_max\n                    detection_2d[""v_max""] = v_max\n                    detection_2d[""score_2d""] = score_2d\n                    detection_2d[""img_id""] = img_id\n\n                    self.examples.append(detection_2d)\n\n        self.num_examples = len(self.examples)\n\n    def __getitem__(self, index):\n        example = self.examples[index]\n\n        img_id = example[""img_id""]\n\n        lidar_path = self.lidar_dir + img_id + "".bin""\n        point_cloud = np.fromfile(lidar_path, dtype=np.float32).reshape(-1, 4)\n        orig_point_cloud = point_cloud\n\n        # remove points that are located behind the camera:\n        point_cloud = point_cloud[point_cloud[:, 0] > -5, :]\n        # remove points that are located too far away from the camera:\n        point_cloud = point_cloud[point_cloud[:, 0] < 80, :]\n\n        calib = calibread(self.calib_dir + img_id + "".txt"")\n        P2 = calib[""P2""]\n        Tr_velo_to_cam_orig = calib[""Tr_velo_to_cam""]\n        R0_rect_orig = calib[""R0_rect""]\n        #\n        R0_rect = np.eye(4)\n        R0_rect[0:3, 0:3] = R0_rect_orig\n        #\n        Tr_velo_to_cam = np.eye(4)\n        Tr_velo_to_cam[0:3, :] = Tr_velo_to_cam_orig\n\n        point_cloud_xyz = point_cloud[:, 0:3]\n        point_cloud_xyz_hom = np.ones((point_cloud.shape[0], 4))\n        point_cloud_xyz_hom[:, 0:3] = point_cloud[:, 0:3] # (point_cloud_xyz_hom has shape (num_points, 4))\n\n        # project the points onto the image plane (homogeneous coords):\n        img_points_hom = np.dot(P2, np.dot(R0_rect, np.dot(Tr_velo_to_cam, point_cloud_xyz_hom.T))).T # (point_cloud_xyz_hom.T has shape (4, num_points))\n        # normalize:\n        img_points = np.zeros((img_points_hom.shape[0], 2))\n        img_points[:, 0] = img_points_hom[:, 0]/img_points_hom[:, 2]\n        img_points[:, 1] = img_points_hom[:, 1]/img_points_hom[:, 2]\n\n        # transform the points into (rectified) camera coordinates:\n        point_cloud_xyz_camera_hom = np.dot(R0_rect, np.dot(Tr_velo_to_cam, point_cloud_xyz_hom.T)).T # (point_cloud_xyz_hom.T has shape (4, num_points))\n        # normalize:\n        point_cloud_xyz_camera = np.zeros((point_cloud_xyz_camera_hom.shape[0], 3))\n        point_cloud_xyz_camera[:, 0] = point_cloud_xyz_camera_hom[:, 0]/point_cloud_xyz_camera_hom[:, 3]\n        point_cloud_xyz_camera[:, 1] = point_cloud_xyz_camera_hom[:, 1]/point_cloud_xyz_camera_hom[:, 3]\n        point_cloud_xyz_camera[:, 2] = point_cloud_xyz_camera_hom[:, 2]/point_cloud_xyz_camera_hom[:, 3]\n\n        point_cloud_camera = point_cloud\n        point_cloud_camera[:, 0:3] = point_cloud_xyz_camera\n\n        ########################################################################\n        # frustum:\n        ########################################################################\n        u_min = example[""u_min""] # (left)\n        u_max = example[""u_max""] # (rigth)\n        v_min = example[""v_min""] # (top)\n        v_max = example[""v_max""] # (bottom)\n\n        score_2d = example[""score_2d""]\n\n        input_2Dbbox = np.array([u_min, u_max, v_min, v_max])\n\n        row_mask = np.logical_and(\n                    np.logical_and(img_points[:, 0] >= u_min,\n                                   img_points[:, 0] <= u_max),\n                    np.logical_and(img_points[:, 1] >= v_min,\n                                   img_points[:, 1] <= v_max))\n\n        frustum_point_cloud_xyz = point_cloud_xyz[row_mask, :] # (needed only for visualization)\n        frustum_point_cloud = point_cloud[row_mask, :]\n        frustum_point_cloud_xyz_camera = point_cloud_xyz_camera[row_mask, :]\n        frustum_point_cloud_camera = point_cloud_camera[row_mask, :]\n\n        empty_frustum_flag = 0\n        if frustum_point_cloud.shape[0] == 0:\n            empty_frustum_flag = 1\n            frustum_point_cloud = np.zeros((1024, 4), dtype=np.float32)\n            frustum_point_cloud_xyz = np.zeros((1024, 3), dtype=np.float32)\n            frustum_point_cloud_camera = np.zeros((1024, 4), dtype=np.float32)\n            frustum_point_cloud_xyz_camera = np.zeros((1024, 3), dtype=np.float32)\n\n        # randomly sample 1024 points in the frustum point cloud:\n        if frustum_point_cloud.shape[0] < 1024:\n            row_idx = np.random.choice(frustum_point_cloud.shape[0], 1024, replace=True)\n        else:\n            row_idx = np.random.choice(frustum_point_cloud.shape[0], 1024, replace=False)\n        frustum_point_cloud_xyz = frustum_point_cloud_xyz[row_idx, :]\n        frustum_point_cloud = frustum_point_cloud[row_idx, :]\n        frustum_point_cloud_xyz_camera = frustum_point_cloud_xyz_camera[row_idx, :]\n        frustum_point_cloud_camera = frustum_point_cloud_camera[row_idx, :]\n        # (the frustum point cloud now has exactly 1024 points)\n\n        ########################################################################\n        # get the input 2dbbox img crop and resize to 224 x 224:\n        ########################################################################\n        img_path = self.img_dir + img_id + "".png""\n        img = cv2.imread(img_path, -1)\n\n        bbox_2d_img = img[int(np.max([0, v_min])):int(v_max), int(np.max([0, u_min])):int(u_max)]\n        bbox_2d_img = cv2.resize(bbox_2d_img, (224, 224))\n\n        ########################################################################\n        # normalize frustum point cloud:\n        ########################################################################\n        # get the 2dbbox center img point in hom. coords:\n        u_center = u_min + (u_max - u_min)/2.0\n        v_center = v_min + (v_max - v_min)/2.0\n        center_img_point_hom = np.array([u_center, v_center, 1])\n\n        # (more than one 3D point is projected onto the center image point, i.e,\n        # the linear system of equations is under-determined and has inf number\n        # of solutions. By using the pseudo-inverse, we obtain the least-norm sol)\n\n        # get a point (the least-norm sol.) that projects onto the center image point, in hom. coords:\n        P2_pseudo_inverse = np.linalg.pinv(P2) # (shape: (4, 3)) (P2 has shape (3, 4))\n        point_hom = np.dot(P2_pseudo_inverse, center_img_point_hom)\n\n        # hom --> normal coords:\n        point = np.array(([point_hom[0]/point_hom[3], point_hom[1]/point_hom[3], point_hom[2]/point_hom[3]]))\n\n        # if the point is behind the camera, switch to the mirror point in front of the camera:\n        if point[2] < 0:\n            point[0] = -point[0]\n            point[2] = -point[2]\n\n        # compute the angle of the point in the x-z plane: ((rectified) camera coords)\n        frustum_angle = np.arctan2(point[0], point[2]) # (np.arctan2(x, z)) # (frustum_angle = 0: frustum is centered)\n\n        # rotation_matrix to rotate points frustum_angle around the y axis (counter-clockwise):\n        frustum_R = np.asarray([[np.cos(frustum_angle), 0, -np.sin(frustum_angle)],\n                           [0, 1, 0],\n                           [np.sin(frustum_angle), 0, np.cos(frustum_angle)]],\n                           dtype=\'float32\')\n\n        # rotate the frustum point cloud to center it:\n        centered_frustum_point_cloud_xyz_camera = np.dot(frustum_R, frustum_point_cloud_xyz_camera.T).T\n\n        # subtract the centered frustum train xyz mean:\n        centered_frustum_point_cloud_xyz_camera -= self.centered_frustum_mean_xyz\n\n        centered_frustum_point_cloud_camera = frustum_point_cloud_camera\n        centered_frustum_point_cloud_camera[:, 0:3] = centered_frustum_point_cloud_xyz_camera\n\n        # # # # # # # # # # debug visualizations START:\n        # import sys\n        # sys.path.append(""/home/fregu856/exjobb/Open3D/build/lib"")\n        # from py3d import *\n        # frustum_pcd_camera = PointCloud()\n        # frustum_pcd_camera.points = Vector3dVector(frustum_point_cloud_xyz_camera)\n        # frustum_pcd_camera.paint_uniform_color([0.25, 0.25, 0.25])\n        # centered_frustum_pcd_camera = PointCloud()\n        # centered_frustum_pcd_camera.points = Vector3dVector(centered_frustum_point_cloud_xyz_camera)\n        # centered_frustum_pcd_camera.paint_uniform_color([1, 0, 0])\n        # draw_geometries([centered_frustum_pcd_camera, frustum_pcd_camera])\n        # # # # # # # # # # debug visualizations END:\n\n        ########################################################################\n        # normalize the 2dbbox img crop:\n        ########################################################################\n        bbox_2d_img = bbox_2d_img/255.0\n        bbox_2d_img = bbox_2d_img - np.array([0.485, 0.456, 0.406])\n        bbox_2d_img = bbox_2d_img/np.array([0.229, 0.224, 0.225]) # (shape: (H, W, 3))\n        bbox_2d_img = np.transpose(bbox_2d_img, (2, 0, 1)) # (shape: (3, H, W))\n        bbox_2d_img = bbox_2d_img.astype(np.float32)\n\n        centered_frustum_point_cloud_camera = torch.from_numpy(centered_frustum_point_cloud_camera) # (shape: (1024, 3))\n        bbox_2d_img = torch.from_numpy(bbox_2d_img) # (shape: (3, H, W) = (3, 224, 224))\n\n        return (centered_frustum_point_cloud_camera, bbox_2d_img, img_id, input_2Dbbox, frustum_R, frustum_angle, empty_frustum_flag, self.centered_frustum_mean_xyz, self.mean_car_size, score_2d)\n\n    def __len__(self):\n        return self.num_examples\n'"
Extended-Frustum-PointNet/eval_frustum_pointnet_img_test.py,8,"b'# camera-ready\n\nfrom datasets_img import DatasetKittiTest, wrapToPi, getBinCenter # (this needs to be imported before torch, because cv2 needs to be imported before torch for some reason)\nfrom frustum_pointnet_img import FrustumPointNetImg\n\nimport torch\nimport torch.utils.data\nimport torch.nn as nn\nfrom torch.autograd import Variable\nimport torch.optim as optim\nimport torch.nn.functional as F\n\nimport numpy as np\nimport pickle\n\nbatch_size = 8\n\nnetwork = FrustumPointNetImg(""Extended-Frustum-PointNet_eval_test"", project_dir=""/root/3DOD_thesis"")\nnetwork.load_state_dict(torch.load(""/root/3DOD_thesis/pretrained_models/model_38_2_epoch_400.pth""))\nnetwork = network.cuda()\n\nnetwork.eval() # (set in evaluation mode, this affects BatchNorm and dropout)\n\nNH = network.BboxNet_network.NH\n\nval_dataset = DatasetKittiTest(kitti_data_path=""/root/3DOD_thesis/data/kitti"",\n                               kitti_meta_path=""/root/3DOD_thesis/data/kitti/meta"",\n                               NH=NH)\n\nnum_val_batches = int(len(val_dataset)/batch_size)\n\nval_loader = torch.utils.data.DataLoader(dataset=val_dataset,\n                                         batch_size=batch_size, shuffle=False,\n                                         num_workers=4)\neval_dict = {}\nfor step, (frustum_point_clouds, bbox_2d_imgs, img_ids, input_2Dbboxes, frustum_Rs, frustum_angles, empty_frustum_flags, centered_frustum_mean_xyz, mean_car_size, scores_2d) in enumerate(val_loader):\n    if step % 100 == 0:\n        print (""step: %d/%d"" % (step+1, num_val_batches))\n\n    with torch.no_grad(): # (corresponds to setting volatile=True in all variables, this is done during inference to reduce memory consumption)\n        bbox_2d_imgs = Variable(bbox_2d_imgs) # (shape: (batch_size, 3, H, W))\n        frustum_point_clouds = Variable(frustum_point_clouds) # (shape: (batch_size, num_points, 4))\n        frustum_point_clouds = frustum_point_clouds.transpose(2, 1) # (shape: (batch_size, 4, num_points))\n\n        frustum_point_clouds = frustum_point_clouds.cuda()\n        bbox_2d_imgs = bbox_2d_imgs.cuda()\n\n        outputs = network(frustum_point_clouds, bbox_2d_imgs)\n        outputs_InstanceSeg = outputs[0] # (shape: (batch_size, num_points, 2))\n        outputs_TNet = outputs[1] # (shape: (batch_size, 3))\n        outputs_BboxNet = outputs[2] # (shape: (batch_size, 3 + 3 + 2*NH))\n        seg_point_clouds_mean = outputs[3] # (shape: (batch_size, 3))\n        dont_care_mask = outputs[4] # (shape: (batch_size, ))\n\n        ############################################################################\n        # save data for visualization:\n        ############################################################################\n        centered_frustum_mean_xyz = centered_frustum_mean_xyz[0].numpy()\n        mean_car_size = mean_car_size[0].numpy()\n        for i in range(outputs_InstanceSeg.size()[0]):\n            dont_care_mask_value = dont_care_mask[i]\n            empty_frustum_flag = empty_frustum_flags[i]\n\n            # don\'t care about predicted 3Dbboxes that corresponds to empty\n            # point clouds outputted by InstanceSeg, or empty input frustums:\n            if dont_care_mask_value == 1 and empty_frustum_flag == 0:\n                pred_InstanceSeg = outputs_InstanceSeg[i].data.cpu().numpy() # (shape: (num_points, 2))\n                frustum_point_cloud = frustum_point_clouds[i].transpose(1, 0).data.cpu().numpy() # (shape: (num_points, 4))\n                seg_point_cloud_mean = seg_point_clouds_mean[i].data.cpu().numpy() # (shape: (3, ))\n                img_id = img_ids[i]\n                frustum_R = frustum_Rs[i] # (shape: (3, 3))\n                frustum_angle = frustum_angles[i]\n                input_2Dbbox = input_2Dbboxes[i]\n                score_2d = scores_2d[i]\n\n                unshifted_frustum_point_cloud_xyz = frustum_point_cloud[:, 0:3] + centered_frustum_mean_xyz\n                decentered_frustum_point_cloud_xyz = np.dot(np.linalg.inv(frustum_R), unshifted_frustum_point_cloud_xyz.T).T\n                frustum_point_cloud[:, 0:3] = decentered_frustum_point_cloud_xyz\n\n                row_mask = pred_InstanceSeg[:, 1] > pred_InstanceSeg[:, 0]\n                pred_seg_point_cloud = frustum_point_cloud[row_mask, :]\n\n                pred_center_TNet = np.dot(np.linalg.inv(frustum_R), outputs_TNet[i].data.cpu().numpy() + centered_frustum_mean_xyz + seg_point_cloud_mean) # (shape: (3, )) # NOTE!\n                centroid = seg_point_cloud_mean\n\n                pred_center_BboxNet = np.dot(np.linalg.inv(frustum_R), outputs_BboxNet[i][0:3].data.cpu().numpy() + centered_frustum_mean_xyz + seg_point_cloud_mean + outputs_TNet[i].data.cpu().numpy()) # (shape: (3, )) # NOTE!\n\n                pred_h = outputs_BboxNet[i][3].data.cpu().numpy() + mean_car_size[0]\n                pred_w = outputs_BboxNet[i][4].data.cpu().numpy() + mean_car_size[1]\n                pred_l = outputs_BboxNet[i][5].data.cpu().numpy() + mean_car_size[2]\n\n                pred_bin_scores = outputs_BboxNet[i][6:(6+4)].data.cpu().numpy() # (shape (NH=8, ))\n                pred_residuals = outputs_BboxNet[i][(6+4):].data.cpu().numpy() # (shape (NH=8, ))\n                pred_bin_number = np.argmax(pred_bin_scores)\n                pred_bin_center = getBinCenter(pred_bin_number, NH=NH)\n                pred_residual = pred_residuals[pred_bin_number]\n                pred_centered_r_y = pred_bin_center + pred_residual\n                pred_r_y = wrapToPi(pred_centered_r_y + frustum_angle) # NOTE!\n\n                pred_r_y = pred_r_y.data.cpu().numpy()\n                score_2d = score_2d.data.cpu().numpy()\n                input_2Dbbox = input_2Dbbox.data.cpu().numpy()\n\n                if img_id not in eval_dict:\n                    eval_dict[img_id] = []\n\n                bbox_dict = {}\n                bbox_dict[""pred_center_TNet""] = pred_center_TNet\n                bbox_dict[""pred_center_BboxNet""] = pred_center_BboxNet\n                bbox_dict[""centroid""] = centroid\n                bbox_dict[""pred_h""] = pred_h\n                bbox_dict[""pred_w""] = pred_w\n                bbox_dict[""pred_l""] = pred_l\n                bbox_dict[""pred_r_y""] = pred_r_y\n                bbox_dict[""input_2Dbbox""] = input_2Dbbox\n                bbox_dict[""score_2d""] = score_2d\n\n                eval_dict[img_id].append(bbox_dict)\n\nwith open(""%s/eval_dict_test.pkl"" % (network.model_dir), ""wb"") as file:\n    pickle.dump(eval_dict, file, protocol=2) # (protocol=2 is needed to be able to open this file with python2)\n'"
Extended-Frustum-PointNet/eval_frustum_pointnet_img_test_seq.py,8,"b'# camera-ready\n\nfrom datasets_img import DatasetKittiTestSequence, wrapToPi, getBinCenter # (this needs to be imported before torch, because cv2 needs to be imported before torch for some reason)\nfrom frustum_pointnet_img import FrustumPointNetImg\n\nimport torch\nimport torch.utils.data\nimport torch.nn as nn\nfrom torch.autograd import Variable\nimport torch.optim as optim\nimport torch.nn.functional as F\n\nimport numpy as np\nimport pickle\n\nbatch_size = 8\n\nnetwork = FrustumPointNetImg(""Extended-Frustum-PointNet_eval_test_seq"", project_dir=""/root/3DOD_thesis"")\nnetwork.load_state_dict(torch.load(""/root/3DOD_thesis/pretrained_models/model_38_2_epoch_400.pth""))\nnetwork = network.cuda()\n\nnetwork.eval() # (set in evaluation mode, this affects BatchNorm and dropout)\n\nNH = network.BboxNet_network.NH\n\nfor sequence in [""0000"", ""0001"", ""0002"", ""0003"", ""0004"", ""0005"", ""0006"", ""0007"", ""0008"", ""0009"", ""0010"", ""0011"", ""0012"", ""0013"", ""0014"", ""0015"", ""0016"", ""0017"", ""0018"", ""0027""]:\n    print (sequence)\n\n    test_dataset = DatasetKittiTestSequence(kitti_data_path=""/root/3DOD_thesis/data/kitti"",\n                                            kitti_meta_path=""/root/3DOD_thesis/data/kitti/meta"",\n                                            NH=NH, sequence=sequence)\n\n    num_test_batches = int(len(test_dataset)/batch_size)\n\n    test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n                                              batch_size=batch_size, shuffle=False,\n                                              num_workers=4)\n    eval_dict = {}\n    for step, (frustum_point_clouds, bbox_2d_imgs, img_ids, input_2Dbboxes, frustum_Rs, frustum_angles, empty_frustum_flags, centered_frustum_mean_xyz, mean_car_size) in enumerate(test_loader):\n        if step % 100 == 0:\n            print (""step: %d/%d"" % (step+1, num_test_batches))\n\n        with torch.no_grad(): # (corresponds to setting volatile=True in all variables, this is done during inference to reduce memory consumption)\n            bbox_2d_imgs = Variable(bbox_2d_imgs) # (shape: (batch_size, 3, H, W))\n            frustum_point_clouds = Variable(frustum_point_clouds) # (shape: (batch_size, num_points, 4))\n            frustum_point_clouds = frustum_point_clouds.transpose(2, 1) # (shape: (batch_size, 4, num_points))\n\n            frustum_point_clouds = frustum_point_clouds.cuda()\n            bbox_2d_imgs = bbox_2d_imgs.cuda()\n\n            outputs = network(frustum_point_clouds, bbox_2d_imgs)\n            outputs_InstanceSeg = outputs[0] # (shape: (batch_size, num_points, 2))\n            outputs_TNet = outputs[1] # (shape: (batch_size, 3))\n            outputs_BboxNet = outputs[2] # (shape: (batch_size, 3 + 3 + 2*NH))\n            seg_point_clouds_mean = outputs[3] # (shape: (batch_size, 3))\n            dont_care_mask = outputs[4] # (shape: (batch_size, ))\n\n            ############################################################################\n            # save data for visualization:\n            ############################################################################\n            centered_frustum_mean_xyz = centered_frustum_mean_xyz[0].numpy()\n            mean_car_size = mean_car_size[0].numpy()\n            for i in range(outputs_InstanceSeg.size()[0]):\n                dont_care_mask_value = dont_care_mask[i]\n                empty_frustum_flag = empty_frustum_flags[i]\n\n                # don\'t care about predicted 3Dbboxes that corresponds to empty\n                # point clouds outputted by InstanceSeg, or empty input frustums:\n                if dont_care_mask_value == 1 and empty_frustum_flag == 0:\n                    pred_InstanceSeg = outputs_InstanceSeg[i].data.cpu().numpy() # (shape: (num_points, 2))\n                    frustum_point_cloud = frustum_point_clouds[i].transpose(1, 0).data.cpu().numpy() # (shape: (num_points, 4))\n                    seg_point_cloud_mean = seg_point_clouds_mean[i].data.cpu().numpy() # (shape: (3, ))\n                    img_id = img_ids[i]\n                    #if img_id in [""000000"", ""000001"", ""000002"", ""000003"", ""000004"", ""000005"", ""000006"", ""000007"", ""000008"", ""000009"", ""000010"", ""000011"", ""000012"", ""000013"", ""000014"", ""000015"", ""000016"", ""000017"", ""000018"", ""000019"", ""000020"", ""000021"", ""000022"", ""000023"", ""000024"", ""000025"", ""000026"", ""000027"", ""000028"", ""000029"", ""000030""]:\n                    input_2Dbbox = input_2Dbboxes[i] # (shape: (4, ))\n                    frustum_R = frustum_Rs[i] # (shape: (3, 3))\n                    frustum_angle = frustum_angles[i]\n\n                    unshifted_frustum_point_cloud_xyz = frustum_point_cloud[:, 0:3] + centered_frustum_mean_xyz\n                    decentered_frustum_point_cloud_xyz = np.dot(np.linalg.inv(frustum_R), unshifted_frustum_point_cloud_xyz.T).T\n                    frustum_point_cloud[:, 0:3] = decentered_frustum_point_cloud_xyz\n\n                    row_mask = pred_InstanceSeg[:, 1] > pred_InstanceSeg[:, 0]\n                    pred_seg_point_cloud = frustum_point_cloud[row_mask, :]\n\n                    pred_center_TNet = np.dot(np.linalg.inv(frustum_R), outputs_TNet[i].data.cpu().numpy() + centered_frustum_mean_xyz + seg_point_cloud_mean) # (shape: (3, )) # NOTE!\n                    centroid = seg_point_cloud_mean\n\n                    pred_center_BboxNet = np.dot(np.linalg.inv(frustum_R), outputs_BboxNet[i][0:3].data.cpu().numpy() + centered_frustum_mean_xyz + seg_point_cloud_mean + outputs_TNet[i].data.cpu().numpy()) # (shape: (3, )) # NOTE!\n\n                    pred_h = outputs_BboxNet[i][3].data.cpu().numpy() + mean_car_size[0]\n                    pred_w = outputs_BboxNet[i][4].data.cpu().numpy() + mean_car_size[1]\n                    pred_l = outputs_BboxNet[i][5].data.cpu().numpy() + mean_car_size[2]\n\n                    pred_bin_scores = outputs_BboxNet[i][6:(6+4)].data.cpu().numpy() # (shape (NH=8, ))\n                    pred_residuals = outputs_BboxNet[i][(6+4):].data.cpu().numpy() # (shape (NH=8, ))\n                    pred_bin_number = np.argmax(pred_bin_scores)\n                    pred_bin_center = getBinCenter(pred_bin_number, NH=NH)\n                    pred_residual = pred_residuals[pred_bin_number]\n                    pred_centered_r_y = pred_bin_center + pred_residual\n                    pred_r_y = wrapToPi(pred_centered_r_y + frustum_angle) # NOTE!\n\n                    pred_r_y = pred_r_y.data.cpu().numpy()\n                    input_2Dbbox = input_2Dbbox.data.cpu().numpy()\n\n                    if img_id not in eval_dict:\n                        eval_dict[img_id] = []\n\n                    bbox_dict = {}\n                    # bbox_dict[""frustum_point_cloud""] = frustum_point_cloud\n                    # bbox_dict[""pred_seg_point_cloud""] = pred_seg_point_cloud\n                    bbox_dict[""pred_center_TNet""] = pred_center_TNet\n                    bbox_dict[""pred_center_BboxNet""] = pred_center_BboxNet\n                    bbox_dict[""centroid""] = centroid\n                    bbox_dict[""pred_h""] = pred_h\n                    bbox_dict[""pred_w""] = pred_w\n                    bbox_dict[""pred_l""] = pred_l\n                    bbox_dict[""pred_r_y""] = pred_r_y\n                    bbox_dict[""input_2Dbbox""] = input_2Dbbox\n\n                    eval_dict[img_id].append(bbox_dict)\n\n    with open(""%s/eval_dict_test_seq_%s.pkl"" % (network.model_dir, sequence), ""wb"") as file:\n        pickle.dump(eval_dict, file, protocol=2) # (protocol=2 is needed to be able to open this file with python2)\n'"
Extended-Frustum-PointNet/eval_frustum_pointnet_img_val.py,39,"b'# camera-ready\n\nfrom datasets_img import EvalDatasetFrustumPointNetImg, wrapToPi, getBinCenter# (this needs to be imported before torch, because cv2 needs to be imported before torch for some reason)\nfrom frustum_pointnet_img import FrustumPointNetImg\n\nimport torch\nimport torch.utils.data\nimport torch.nn as nn\nfrom torch.autograd import Variable\nimport torch.optim as optim\nimport torch.nn.functional as F\n\nimport numpy as np\nimport pickle\nimport matplotlib\nmatplotlib.use(""Agg"")\nimport matplotlib.pyplot as plt\n\nbatch_size = 8\n\nnetwork = FrustumPointNetImg(""Extended-Frustum-PointNet_eval_val"", project_dir=""/root/3DOD_thesis"")\nnetwork.load_state_dict(torch.load(""/root/3DOD_thesis/pretrained_models/model_38_2_epoch_400.pth""))\nnetwork = network.cuda()\n\nNH = network.BboxNet_network.NH\n\nval_dataset = EvalDatasetFrustumPointNetImg(kitti_data_path=""/root/3DOD_thesis/data/kitti"",\n                                            kitti_meta_path=""/root/3DOD_thesis/data/kitti/meta"",\n                                            type=""val"", NH=NH)\n\nnum_val_batches = int(len(val_dataset)/batch_size)\n\nval_loader = torch.utils.data.DataLoader(dataset=val_dataset,\n                                         batch_size=batch_size, shuffle=False,\n                                         num_workers=4)\n\nregression_loss_func = nn.SmoothL1Loss()\n\nnetwork.eval() # (set in evaluation mode, this affects BatchNorm, dropout etc.)\nbatch_losses = []\nbatch_losses_InstanceSeg = []\nbatch_losses_TNet = []\nbatch_losses_BboxNet = []\nbatch_losses_BboxNet_center = []\nbatch_losses_BboxNet_size = []\nbatch_losses_BboxNet_heading_regr = []\nbatch_losses_BboxNet_heading_class = []\nbatch_losses_BboxNet_heading_class_weighted = []\nbatch_losses_corner = []\nbatch_accuracies = []\nbatch_precisions = []\nbatch_recalls = []\nbatch_f1s = []\nbatch_accuracies_heading_class = []\neval_dict = {}\nfor step, (frustum_point_clouds, bbox_2d_imgs, labels_InstanceSeg, labels_TNet, labels_BboxNet, labels_corner, labels_corner_flipped, img_ids, input_2Dbboxes, frustum_Rs, frustum_angles, centered_frustum_mean_xyz) in enumerate(val_loader):\n    with torch.no_grad(): # (corresponds to setting volatile=True in all variables, this is done during inference to reduce memory consumption)\n        frustum_point_clouds = Variable(frustum_point_clouds) # (shape: (batch_size, num_points, 4))\n        bbox_2d_imgs = Variable(bbox_2d_imgs) # (shape: (batch_size, 3, H, W))\n        labels_InstanceSeg = Variable(labels_InstanceSeg) # (shape: (batch_size, num_points))\n        labels_TNet = Variable(labels_TNet) # (shape: (batch_size, 3))\n        labels_BboxNet = Variable(labels_BboxNet) # (shape:(batch_size, 11))\n        labels_corner = Variable(labels_corner) # (shape: (batch_size, 8, 3))\n        labels_corner_flipped = Variable(labels_corner_flipped) # (shape: (batch_size, 8, 3))\n\n        frustum_point_clouds = frustum_point_clouds.transpose(2, 1) # (shape: (batch_size, 4, num_points))\n\n        frustum_point_clouds = frustum_point_clouds.cuda()\n        bbox_2d_imgs = bbox_2d_imgs.cuda()\n        labels_InstanceSeg = labels_InstanceSeg.cuda()\n        labels_TNet = labels_TNet.cuda()\n        labels_BboxNet = labels_BboxNet.cuda()\n        labels_corner = labels_corner.cuda()\n        labels_corner_flipped = labels_corner_flipped.cuda()\n\n        outputs = network(frustum_point_clouds, bbox_2d_imgs)\n        outputs_InstanceSeg = outputs[0] # (shape: (batch_size, num_points, 2))\n        outputs_TNet = outputs[1] # (shape: (batch_size, 3))\n        outputs_BboxNet = outputs[2] # (shape: (batch_size, 3 + 3 + 2*NH))\n        seg_point_clouds_mean = outputs[3] # (shape: (batch_size, 3))\n        dont_care_mask = outputs[4] # (shape: (batch_size, ))\n\n        ############################################################################\n        # save data for visualization:\n        ############################################################################\n        centered_frustum_mean_xyz = centered_frustum_mean_xyz[0].numpy()\n        for i in range(outputs_InstanceSeg.size()[0]):\n            dont_care_mask_value = dont_care_mask[i]\n\n            # don\'t care about predicted 3Dbboxes that corresponds to empty point clouds outputted by InstanceSeg:\n            if dont_care_mask_value == 1:\n                pred_InstanceSeg = outputs_InstanceSeg[i].data.cpu().numpy() # (shape: (num_points, 2))\n                frustum_point_cloud = frustum_point_clouds[i].transpose(1, 0).data.cpu().numpy() # (shape: (num_points, 4))\n                label_InstanceSeg = labels_InstanceSeg[i].data.cpu().numpy() # (shape: (num_points, ))\n                seg_point_cloud_mean = seg_point_clouds_mean[i].data.cpu().numpy() # (shape: (3, ))\n                img_id = img_ids[i]\n                #if img_id in [""000006"", ""000007"", ""000008"", ""000009"", ""000010"", ""000011"", ""000012"", ""000013"", ""000014"", ""000015"", ""000016"", ""000017"", ""000018"", ""000019"", ""000020"", ""000021""]:\n                input_2Dbbox = input_2Dbboxes[i] # (shape: (4, ))\n                frustum_R = frustum_Rs[i] # (shape: (3, 3))\n                frustum_angle = frustum_angles[i]\n\n                unshifted_frustum_point_cloud_xyz = frustum_point_cloud[:, 0:3] + centered_frustum_mean_xyz\n                decentered_frustum_point_cloud_xyz = np.dot(np.linalg.inv(frustum_R), unshifted_frustum_point_cloud_xyz.T).T\n                frustum_point_cloud[:, 0:3] = decentered_frustum_point_cloud_xyz\n\n                row_mask = pred_InstanceSeg[:, 1] > pred_InstanceSeg[:, 0]\n                pred_seg_point_cloud = frustum_point_cloud[row_mask, :]\n\n                row_mask = label_InstanceSeg == 1\n                gt_seg_point_cloud = frustum_point_cloud[row_mask, :]\n\n                pred_center_TNet = np.dot(np.linalg.inv(frustum_R), outputs_TNet[i].data.cpu().numpy() + centered_frustum_mean_xyz + seg_point_cloud_mean) # (shape: (3, )) # NOTE!\n                gt_center = np.dot(np.linalg.inv(frustum_R), labels_TNet[i].data.cpu().numpy() + centered_frustum_mean_xyz) # NOTE!\n                centroid = seg_point_cloud_mean\n\n                pred_center_BboxNet = np.dot(np.linalg.inv(frustum_R), outputs_BboxNet[i][0:3].data.cpu().numpy() + centered_frustum_mean_xyz + seg_point_cloud_mean + outputs_TNet[i].data.cpu().numpy()) # (shape: (3, )) # NOTE!\n\n                pred_h = outputs_BboxNet[i][3].data.cpu().numpy() + labels_BboxNet[i][8].data.cpu().numpy()\n                pred_w = outputs_BboxNet[i][4].data.cpu().numpy() + labels_BboxNet[i][9].data.cpu().numpy()\n                pred_l = outputs_BboxNet[i][5].data.cpu().numpy() + labels_BboxNet[i][10].data.cpu().numpy()\n\n                pred_bin_scores = outputs_BboxNet[i][6:(6+4)].data.cpu().numpy() # (shape (NH=8, ))\n                pred_residuals = outputs_BboxNet[i][(6+4):].data.cpu().numpy() # (shape (NH=8, ))\n                pred_bin_number = np.argmax(pred_bin_scores)\n                pred_bin_center = getBinCenter(pred_bin_number, NH=NH)\n                pred_residual = pred_residuals[pred_bin_number]\n                pred_centered_r_y = pred_bin_center + pred_residual\n                pred_r_y = wrapToPi(pred_centered_r_y + frustum_angle) # NOTE!\n\n                gt_h = labels_BboxNet[i][3].data.cpu().numpy()\n                gt_w = labels_BboxNet[i][4].data.cpu().numpy()\n                gt_l = labels_BboxNet[i][5].data.cpu().numpy()\n\n                gt_bin_number = labels_BboxNet[i][6].data.cpu().numpy()\n                gt_bin_center = getBinCenter(gt_bin_number, NH=NH)\n                gt_residual = labels_BboxNet[i][7].data.cpu().numpy()\n                gt_centered_r_y = gt_bin_center + gt_residual\n                gt_r_y = wrapToPi(gt_centered_r_y + frustum_angle) # NOTE!\n\n                pred_r_y = pred_r_y.data.cpu().numpy()\n                gt_r_y = gt_r_y.data.cpu().numpy()\n                input_2Dbbox = input_2Dbbox.data.cpu().numpy()\n\n                if img_id not in eval_dict:\n                    eval_dict[img_id] = []\n\n                bbox_dict = {}\n                # # # # uncomment this if you want to visualize the frustum or the segmentation (e.g., if you want to run visualization/visualize_eval_val_extra.py):\n                # bbox_dict[""frustum_point_cloud""] = frustum_point_cloud\n                # bbox_dict[""pred_seg_point_cloud""] = pred_seg_point_cloud\n                # bbox_dict[""gt_seg_point_cloud""] = gt_seg_point_cloud\n                # # # #\n                bbox_dict[""pred_center_TNet""] = pred_center_TNet\n                bbox_dict[""pred_center_BboxNet""] = pred_center_BboxNet\n                bbox_dict[""gt_center""] = gt_center\n                bbox_dict[""centroid""] = centroid\n                bbox_dict[""pred_h""] = pred_h\n                bbox_dict[""pred_w""] = pred_w\n                bbox_dict[""pred_l""] = pred_l\n                bbox_dict[""pred_r_y""] = pred_r_y\n                bbox_dict[""gt_h""] = gt_h\n                bbox_dict[""gt_w""] = gt_w\n                bbox_dict[""gt_l""] = gt_l\n                bbox_dict[""gt_r_y""] = gt_r_y\n                bbox_dict[""input_2Dbbox""] = input_2Dbbox\n\n                eval_dict[img_id].append(bbox_dict)\n\n        ########################################################################\n        # compute precision, recall etc. for the InstanceSeg:\n        ########################################################################\n        preds = outputs_InstanceSeg.data.cpu().numpy() # (shape: (batch_size, num_points, 2))\n        preds = np.argmax(preds, 2) # (shape: (batch_size, num_points))\n\n        labels_InstanceSeg_np = labels_InstanceSeg.data.cpu().numpy() # (shape: (batch_size, num_points))\n\n        accuracy = np.count_nonzero(preds == labels_InstanceSeg_np)/(preds.shape[0]*preds.shape[1])\n        if np.count_nonzero(preds == 1) > 0:\n            precision = np.count_nonzero(np.logical_and(preds == labels_InstanceSeg_np, preds == 1))/np.count_nonzero(preds == 1) # (TP/(TP + FP))\n        else:\n            precision = -1\n        if np.count_nonzero(labels_InstanceSeg_np == 1) > 0:\n            recall = np.count_nonzero(np.logical_and(preds == labels_InstanceSeg_np, preds == 1))/np.count_nonzero(labels_InstanceSeg_np == 1) # (TP/(TP + FN))\n        else:\n            recall = -1\n        if recall + precision > 0:\n            f1 = 2*recall*precision/(recall + precision)\n        else:\n            f1 = -1\n\n        batch_accuracies.append(accuracy)\n        if precision != -1:\n            batch_precisions.append(precision)\n        if recall != -1:\n            batch_recalls.append(recall)\n        if f1 != -1:\n            batch_f1s.append(f1)\n\n        ########################################################################\n        # compute accuracy for the heading classification:\n        ########################################################################\n        pred_bin_scores = outputs_BboxNet[:, 6:(6+NH)].data.cpu().numpy() # (shape: (batch_size, NH))\n        pred_bin_numbers = np.argmax(pred_bin_scores, 1) # (shape: (batch_size, ))\n        gt_bin_numbers = labels_BboxNet[:, 6].data.cpu().numpy() # (shape: (batch_size, ))\n\n        accuracy_heading_class = np.count_nonzero(pred_bin_numbers == gt_bin_numbers)/(pred_bin_numbers.shape[0])\n        batch_accuracies_heading_class.append(accuracy_heading_class)\n\n        ########################################################################\n        # compute the InstanceSeg loss:\n        ########################################################################\n        outputs_InstanceSeg = outputs_InstanceSeg.view(-1, 2) # (shape (batch_size*num_points, 2))\n        labels_InstanceSeg = labels_InstanceSeg.view(-1, 1) # (shape: (batch_size*num_points, 1))\n        labels_InstanceSeg = labels_InstanceSeg[:, 0] # (shape: (batch_size*num_points, ))\n        loss_InstanceSeg = F.nll_loss(outputs_InstanceSeg, labels_InstanceSeg)\n        loss_InstanceSeg_value = loss_InstanceSeg.data.cpu().numpy()\n        batch_losses_InstanceSeg.append(loss_InstanceSeg_value)\n\n        ########################################################################\n        # compute the TNet loss:\n        ########################################################################\n        # mask entries corresponding to empty seg point clouds (select only the entries which we care about):\n        outputs_TNet = outputs_TNet[dont_care_mask, :] # (shape: (batch_size*, 3))\n        labels_TNet = labels_TNet[dont_care_mask, :] # (shape: (batch_size*, 3))\n        seg_point_clouds_mean = seg_point_clouds_mean[dont_care_mask, :] # (shape: (batch_size*, 3))\n\n        # shift the GT to the seg point clouds local coords:\n        labels_TNet = labels_TNet - seg_point_clouds_mean\n\n        # compute the Huber (smooth L1) loss:\n        if outputs_TNet.size()[0] == 0:\n            loss_TNet = Variable(torch.from_numpy(np.zeros((1, ), dtype=np.float32))).cuda()\n        else:\n            loss_TNet = regression_loss_func(outputs_TNet, labels_TNet)\n\n        loss_TNet_value = loss_TNet.data.cpu().numpy()\n        batch_losses_TNet.append(loss_TNet_value)\n\n        ########################################################################\n        # compute the BboxNet loss:\n        ########################################################################\n        # mask entries corresponding to empty seg point clouds (select only the entries which we care about):\n        outputs_BboxNet = outputs_BboxNet[dont_care_mask, :] # (shape: (batch_size*, 3 + 3 + 2*NH))\n        labels_BboxNet = labels_BboxNet[dont_care_mask, :]# (shape: (batch_size*, 11))\n\n        if outputs_BboxNet.size()[0] == 0:\n            loss_BboxNet = Variable(torch.from_numpy(np.zeros((1, ), dtype=np.float32))).cuda()\n            loss_BboxNet_size = Variable(torch.from_numpy(np.zeros((1, ), dtype=np.float32))).cuda()\n            loss_BboxNet_center = Variable(torch.from_numpy(np.zeros((1, ), dtype=np.float32))).cuda()\n            loss_BboxNet_heading_class = Variable(torch.from_numpy(np.zeros((1, ), dtype=np.float32))).cuda()\n            loss_BboxNet_heading_regr = Variable(torch.from_numpy(np.zeros((1, ), dtype=np.float32))).cuda()\n        else:\n            # compute the BboxNet center loss:\n            labels_BboxNet_center = labels_BboxNet[:, 0:3] # (shape: (batch_size*, 3))\n            # # shift the center GT to local coords:\n            labels_BboxNet_center = Variable(labels_BboxNet_center.data - seg_point_clouds_mean.data - outputs_TNet.data).cuda() # (outputs_TNet is a variable outputted by model, so it requires grads, which cant be passed as target to the loss function)\n            # # compute the Huber (smooth L1) loss:\n            outputs_BboxNet_center = outputs_BboxNet[:, 0:3]\n            loss_BboxNet_center = regression_loss_func(outputs_BboxNet_center, labels_BboxNet_center)\n\n            # compute the BboxNet size loss:\n            labels_BboxNet_size = labels_BboxNet[:, 3:6] # (shape: (batch_size*, 3))\n            # # subtract the mean car size in train:\n            labels_BboxNet_size = labels_BboxNet_size - labels_BboxNet[:, 8:]\n            # # compute the Huber (smooth L1) loss:\n            loss_BboxNet_size = regression_loss_func(outputs_BboxNet[:, 3:6], labels_BboxNet_size)\n\n            # compute the BboxNet heading loss\n            # # compute the classification loss:\n            labels_BboxNet_heading_class = Variable(labels_BboxNet[:, 6].data.type(torch.LongTensor)).cuda() # (shape: (batch_size*, ))\n            outputs_BboxNet_heading_class = outputs_BboxNet[:, 6:(6+NH)] # (shape: (batch_size*, NH))\n            loss_BboxNet_heading_class = F.nll_loss(F.log_softmax(outputs_BboxNet_heading_class, dim=1), labels_BboxNet_heading_class)\n            # # compute the regression loss:\n            # # # # get the GT residual for the GT bin:\n            labels_BboxNet_heading_regr = labels_BboxNet[:, 7] # (shape: (batch_size*, ))\n            # # # # get the pred residual for all bins:\n            outputs_BboxNet_heading_regr_all = outputs_BboxNet[:, (6+NH):] # (shape: (batch_size*, 8))\n            # # # # get the pred residual for the GT bin:\n            outputs_BboxNet_heading_regr = outputs_BboxNet_heading_regr_all.gather(1, labels_BboxNet_heading_class.view(-1, 1)) # (shape: (batch_size*, 1))\n            outputs_BboxNet_heading_regr = outputs_BboxNet_heading_regr[:, 0] # (shape: (batch_size*, )\n            # # # # compute the loss:\n            loss_BboxNet_heading_regr = regression_loss_func(outputs_BboxNet_heading_regr, labels_BboxNet_heading_regr)\n            # # compute the total BBoxNet heading loss:\n            loss_BboxNet_heading = loss_BboxNet_heading_class + 10*loss_BboxNet_heading_regr\n\n            # compute the BboxNet total loss:\n            loss_BboxNet = loss_BboxNet_center + loss_BboxNet_size + loss_BboxNet_heading\n\n        loss_BboxNet_value = loss_BboxNet.data.cpu().numpy()\n        batch_losses_BboxNet.append(loss_BboxNet_value)\n\n        loss_BboxNet_size_value = loss_BboxNet_size.data.cpu().numpy()\n        batch_losses_BboxNet_size.append(loss_BboxNet_size_value)\n\n        loss_BboxNet_center_value = loss_BboxNet_center.data.cpu().numpy()\n        batch_losses_BboxNet_center.append(loss_BboxNet_center_value)\n\n        loss_BboxNet_heading_class_value = loss_BboxNet_heading_class.data.cpu().numpy()\n        batch_losses_BboxNet_heading_class.append(loss_BboxNet_heading_class_value)\n\n        loss_BboxNet_heading_regr_value = loss_BboxNet_heading_regr.data.cpu().numpy()\n        batch_losses_BboxNet_heading_regr.append(loss_BboxNet_heading_regr_value)\n\n        ########################################################################\n        # compute the corner loss:\n        ########################################################################\n        # mask entries corresponding to empty seg point clouds (select only the entries which we care about):\n        labels_corner = labels_corner[dont_care_mask]# (shape: (batch_size*, 8, 3))\n        labels_corner_flipped = labels_corner_flipped[dont_care_mask]# (shape: (batch_size*, 8, 3))\n\n        if outputs_BboxNet.size()[0] == 0:\n            loss_corner = Variable(torch.from_numpy(np.zeros((1, ), dtype=np.float32))).cuda()\n        else:\n            outputs_BboxNet_center = outputs_BboxNet[:, 0:3] # (shape: (batch_size*, 3))\n            # shift to the same coords used in the labels for the corner loss:\n            pred_center = outputs_BboxNet_center + seg_point_clouds_mean + outputs_TNet # (shape: (batch_size*, 3))\n            pred_center_unsqeezed = pred_center.unsqueeze(2) # (shape: (batch_size, 3, 1))\n\n            # shift the outputted size to the same ""coords"" used in the labels for the corner loss:\n            outputs_BboxNet_size = outputs_BboxNet[:, 3:6] + labels_BboxNet[:, 8:] # (shape: (batch_size*, 3))\n\n            pred_h = outputs_BboxNet_size[:, 0] # (shape: (batch_size*, ))\n            pred_w = outputs_BboxNet_size[:, 1] # (shape: (batch_size*, ))\n            pred_l = outputs_BboxNet_size[:, 2] # (shape: (batch_size*, ))\n\n            # get the pred residuals for the GT bins:\n            pred_residuals = outputs_BboxNet_heading_regr # (shape: (batch_size*, ))\n\n            Rmat = Variable(torch.zeros(pred_h.size()[0], 3, 3), requires_grad=True).cuda() # (shape: (batch_size*, 3, 3))\n            Rmat[:, 0, 0] = torch.cos(pred_residuals)\n            Rmat[:, 0, 2] = torch.sin(pred_residuals)\n            Rmat[:, 1, 1] = 1\n            Rmat[:, 2, 0] = -torch.sin(pred_residuals)\n            Rmat[:, 2, 2] = torch.cos(pred_residuals)\n\n            p0_orig = Variable(torch.zeros(pred_h.size()[0], 3, 1), requires_grad=True).cuda() # (shape: (batch_size*, 3, 1))\n            p0_orig[:, 0, 0] = pred_l/2.0\n            p0_orig[:, 2, 0] = pred_w/2.0\n\n            p1_orig = Variable(torch.zeros(pred_h.size()[0], 3, 1), requires_grad=True).cuda() # (shape: (batch_size*, 3, 1))\n            p1_orig[:, 0, 0] = -pred_l/2.0\n            p1_orig[:, 2, 0] = pred_w/2.0\n\n            p2_orig = Variable(torch.zeros(pred_h.size()[0], 3, 1), requires_grad=True).cuda() # (shape: (batch_size*, 3, 1))\n            p2_orig[:, 0, 0] = -pred_l/2.0\n            p2_orig[:, 2, 0] = -pred_w/2.0\n\n            p3_orig = Variable(torch.zeros(pred_h.size()[0], 3, 1), requires_grad=True).cuda() # (shape: (batch_size*, 3, 1))\n            p3_orig[:, 0, 0] = pred_l/2.0\n            p3_orig[:, 2, 0] = -pred_w/2.0\n\n            p4_orig = Variable(torch.zeros(pred_h.size()[0], 3, 1), requires_grad=True).cuda() # (shape: (batch_size*, 3, 1))\n            p4_orig[:, 0, 0] = pred_l/2.0\n            p4_orig[:, 1, 0] = -pred_h\n            p4_orig[:, 2, 0] = pred_w/2.0\n\n            p5_orig = Variable(torch.zeros(pred_h.size()[0], 3, 1), requires_grad=True).cuda() # (shape: (batch_size*, 3, 1))\n            p5_orig[:, 0, 0] = -pred_l/2.0\n            p5_orig[:, 1, 0] = -pred_h\n            p5_orig[:, 2, 0] = pred_w/2.0\n\n            p6_orig = Variable(torch.zeros(pred_h.size()[0], 3, 1), requires_grad=True).cuda() # (shape: (batch_size*, 3, 1))\n            p6_orig[:, 0, 0] = -pred_l/2.0\n            p6_orig[:, 1, 0] = -pred_h\n            p6_orig[:, 2, 0] = -pred_w/2.0\n\n            p7_orig = Variable(torch.zeros(pred_h.size()[0], 3, 1), requires_grad=True).cuda() # (shape: (batch_size*, 3, 1))\n            p7_orig[:, 0, 0] = pred_l/2.0\n            p7_orig[:, 1, 0] = -pred_h\n            p7_orig[:, 2, 0] = -pred_w/2.0\n\n            pred_p0_unsqeezed = pred_center_unsqeezed + torch.bmm(Rmat, p0_orig) # (shape: (batch_size*, 3, 1))\n            pred_p0 = pred_p0_unsqeezed[:, :, 0] # (shape: (batch_size*, 3))\n            pred_p1_unsqeezed = pred_center_unsqeezed + torch.bmm(Rmat, p1_orig) # (shape: (batch_size*, 3, 1))\n            pred_p1 = pred_p1_unsqeezed[:, :, 0] # (shape: (batch_size*, 3))\n            pred_p2_unsqeezed = pred_center_unsqeezed + torch.bmm(Rmat, p2_orig) # (shape: (batch_size*, 3, 1))\n            pred_p2 = pred_p2_unsqeezed[:, :, 0] # (shape: (batch_size*, 3))\n            pred_p3_unsqeezed = pred_center_unsqeezed + torch.bmm(Rmat, p3_orig) # (shape: (batch_size*, 3, 1))\n            pred_p3 = pred_p3_unsqeezed[:, :, 0] # (shape: (batch_size*, 3))\n            pred_p4_unsqeezed = pred_center_unsqeezed + torch.bmm(Rmat, p4_orig) # (shape: (batch_size*, 3, 1))\n            pred_p4 = pred_p4_unsqeezed[:, :, 0] # (shape: (batch_size*, 3))\n            pred_p5_unsqeezed = pred_center_unsqeezed + torch.bmm(Rmat, p5_orig) # (shape: (batch_size*, 3, 1))\n            pred_p5 = pred_p5_unsqeezed[:, :, 0] # (shape: (batch_size*, 3))\n            pred_p6_unsqeezed = pred_center_unsqeezed + torch.bmm(Rmat, p6_orig) # (shape: (batch_size*, 3, 1))\n            pred_p6 = pred_p6_unsqeezed[:, :, 0] # (shape: (batch_size*, 3))\n            pred_p7_unsqeezed = pred_center_unsqeezed + torch.bmm(Rmat, p7_orig) # (shape: (batch_size*, 3, 1))\n            pred_p7 = pred_p7_unsqeezed[:, :, 0] # (shape: (batch_size*, 3))\n\n            outputs_corner = Variable(torch.zeros(pred_h.size()[0], 8, 3), requires_grad=True).cuda() # (shape: (batch_size*, 8, 3))\n            outputs_corner[:, 0] = pred_p0\n            outputs_corner[:, 1] = pred_p1\n            outputs_corner[:, 2] = pred_p2\n            outputs_corner[:, 3] = pred_p3\n            outputs_corner[:, 4] = pred_p4\n            outputs_corner[:, 5] = pred_p5\n            outputs_corner[:, 6] = pred_p6\n            outputs_corner[:, 7] = pred_p7\n\n            loss_corner_unflipped = regression_loss_func(outputs_corner, labels_corner)\n            loss_corner_flipped = regression_loss_func(outputs_corner, labels_corner_flipped)\n\n            loss_corner = torch.min(loss_corner_unflipped, loss_corner_flipped)\n\n        loss_corner_value = loss_corner.data.cpu().numpy()\n        batch_losses_corner.append(loss_corner_value)\n\n        ########################################################################\n        # compute the total loss:\n        ########################################################################\n        lambda_value = 1\n        gamma_value = 10\n        loss = loss_InstanceSeg + lambda_value*(loss_TNet + loss_BboxNet + gamma_value*loss_corner)\n        loss_value = loss.data.cpu().numpy()\n        batch_losses.append(loss_value)\n\n# compute the val epoch loss:\nepoch_loss = np.mean(batch_losses)\nprint (""validation loss: %g"" % epoch_loss)\n# compute the val epoch TNet loss:\nepoch_loss = np.mean(batch_losses_TNet)\nprint (""validation TNet loss: %g"" % epoch_loss)\n# compute the val epoch InstanceSeg loss:\nepoch_loss = np.mean(batch_losses_InstanceSeg)\nprint (""validation InstanceSeg loss: %g"" % epoch_loss)\n# compute the val epoch BboxNet loss:\nepoch_loss = np.mean(batch_losses_BboxNet)\nprint (""validation BboxNet loss: %g"" % epoch_loss)\n# compute the val epoch BboxNet size loss:\nepoch_loss = np.mean(batch_losses_BboxNet_size)\nprint (""validation BboxNet size loss: %g"" % epoch_loss)\n# compute the val epoch BboxNet center loss:\nepoch_loss = np.mean(batch_losses_BboxNet_center)\nprint (""validation BboxNet center loss: %g"" % epoch_loss)\n# compute the val epoch BboxNet heading class loss:\nepoch_loss = np.mean(batch_losses_BboxNet_heading_class)\nprint (""validation BboxNet heading class loss: %g"" % epoch_loss)\n# compute the val epoch BboxNet heading regr loss:\nepoch_loss = np.mean(batch_losses_BboxNet_heading_regr)\nprint (""validation BboxNet heading regr loss: %g"" % epoch_loss)\n# compute the val epoch heading class accuracy:\nepoch_accuracy = np.mean(batch_accuracies_heading_class)\nprint (""validation heading class accuracy: %g"" % epoch_accuracy)\n# compute the val epoch corner loss:\nepoch_loss = np.mean(batch_losses_corner)\nprint (""validation corner loss: %g"" % epoch_loss)\n# compute the val epoch accuracy:\nepoch_accuracy = np.mean(batch_accuracies)\nprint (""validation accuracy: %g"" % epoch_accuracy)\n# compute the val epoch precision:\nepoch_precision = np.mean(batch_precisions)\nprint (""validation precision: %g"" % epoch_precision)\n# compute the val epoch recall:\nepoch_recall = np.mean(batch_recalls)\nprint (""validation recall: %g"" % epoch_recall)\n# compute the val epoch f1:\nepoch_f1 = np.mean(batch_f1s)\nprint (""validation f1: %g"" % epoch_f1)\n\nwith open(""%s/eval_dict_val.pkl"" % network.model_dir, ""wb"") as file:\n    pickle.dump(eval_dict, file, protocol=2) # (protocol=2 is needed to be able to open this file with python2)\n'"
Extended-Frustum-PointNet/eval_frustum_pointnet_img_val_2ddetections.py,8,"b'# camera-ready\n\nfrom datasets_img import DatasetKittiVal2ddetections, wrapToPi, getBinCenter # (this needs to be imported before torch, because cv2 needs to be imported before torch for some reason)\nfrom frustum_pointnet_img import FrustumPointNetImg\n\nimport torch\nimport torch.utils.data\nimport torch.nn as nn\nfrom torch.autograd import Variable\nimport torch.optim as optim\nimport torch.nn.functional as F\n\nimport numpy as np\nimport pickle\n\nbatch_size = 8\n\nnetwork = FrustumPointNetImg(""Extended-Frustum-PointNet_eval_val_2ddetections"", project_dir=""/root/3DOD_thesis"")\nnetwork.load_state_dict(torch.load(""/root/3DOD_thesis/pretrained_models/model_38_2_epoch_400.pth""))\nnetwork = network.cuda()\n\nnetwork.eval() # (set in evaluation mode, this affects BatchNorm and dropout)\n\nNH = network.BboxNet_network.NH\n\nval_dataset = DatasetKittiVal2ddetections(kitti_data_path=""/root/3DOD_thesis/data/kitti"",\n                                          kitti_meta_path=""/root/3DOD_thesis/data/kitti/meta"",\n                                          NH=NH)\n\nnum_val_batches = int(len(val_dataset)/batch_size)\n\nval_loader = torch.utils.data.DataLoader(dataset=val_dataset,\n                                         batch_size=batch_size, shuffle=False,\n                                         num_workers=4)\neval_dict = {}\nfor step, (frustum_point_clouds, bbox_2d_imgs, img_ids, input_2Dbboxes, frustum_Rs, frustum_angles, empty_frustum_flags, centered_frustum_mean_xyz, mean_car_size, scores_2d) in enumerate(val_loader):\n    if step % 100 == 0:\n        print (""step: %d/%d"" % (step+1, num_val_batches))\n\n    with torch.no_grad(): # (corresponds to setting volatile=True in all variables, this is done during inference to reduce memory consumption)\n        bbox_2d_imgs = Variable(bbox_2d_imgs) # (shape: (batch_size, 3, H, W))\n        frustum_point_clouds = Variable(frustum_point_clouds) # (shape: (batch_size, num_points, 4))\n        frustum_point_clouds = frustum_point_clouds.transpose(2, 1) # (shape: (batch_size, 4, num_points))\n\n        frustum_point_clouds = frustum_point_clouds.cuda()\n        bbox_2d_imgs = bbox_2d_imgs.cuda()\n\n        outputs = network(frustum_point_clouds, bbox_2d_imgs)\n        outputs_InstanceSeg = outputs[0] # (shape: (batch_size, num_points, 2))\n        outputs_TNet = outputs[1] # (shape: (batch_size, 3))\n        outputs_BboxNet = outputs[2] # (shape: (batch_size, 3 + 3 + 2*NH))\n        seg_point_clouds_mean = outputs[3] # (shape: (batch_size, 3))\n        dont_care_mask = outputs[4] # (shape: (batch_size, ))\n\n        ############################################################################\n        # save data for visualization:\n        ############################################################################\n        centered_frustum_mean_xyz = centered_frustum_mean_xyz[0].numpy()\n        mean_car_size = mean_car_size[0].numpy()\n        for i in range(outputs_InstanceSeg.size()[0]):\n            dont_care_mask_value = dont_care_mask[i]\n            empty_frustum_flag = empty_frustum_flags[i]\n\n            # don\'t care about predicted 3Dbboxes that corresponds to empty\n            # point clouds outputted by InstanceSeg, or empty input frustums:\n            if dont_care_mask_value == 1 and empty_frustum_flag == 0:\n                pred_InstanceSeg = outputs_InstanceSeg[i].data.cpu().numpy() # (shape: (num_points, 2))\n                frustum_point_cloud = frustum_point_clouds[i].transpose(1, 0).data.cpu().numpy() # (shape: (num_points, 4))\n                seg_point_cloud_mean = seg_point_clouds_mean[i].data.cpu().numpy() # (shape: (3, ))\n                img_id = img_ids[i]\n                frustum_R = frustum_Rs[i] # (shape: (3, 3))\n                frustum_angle = frustum_angles[i]\n                input_2Dbbox = input_2Dbboxes[i]\n                score_2d = scores_2d[i]\n\n                unshifted_frustum_point_cloud_xyz = frustum_point_cloud[:, 0:3] + centered_frustum_mean_xyz\n                decentered_frustum_point_cloud_xyz = np.dot(np.linalg.inv(frustum_R), unshifted_frustum_point_cloud_xyz.T).T\n                frustum_point_cloud[:, 0:3] = decentered_frustum_point_cloud_xyz\n\n                row_mask = pred_InstanceSeg[:, 1] > pred_InstanceSeg[:, 0]\n                pred_seg_point_cloud = frustum_point_cloud[row_mask, :]\n\n                pred_center_TNet = np.dot(np.linalg.inv(frustum_R), outputs_TNet[i].data.cpu().numpy() + centered_frustum_mean_xyz + seg_point_cloud_mean) # (shape: (3, )) # NOTE!\n                centroid = seg_point_cloud_mean\n\n                pred_center_BboxNet = np.dot(np.linalg.inv(frustum_R), outputs_BboxNet[i][0:3].data.cpu().numpy() + centered_frustum_mean_xyz + seg_point_cloud_mean + outputs_TNet[i].data.cpu().numpy()) # (shape: (3, )) # NOTE!\n\n                pred_h = outputs_BboxNet[i][3].data.cpu().numpy() + mean_car_size[0]\n                pred_w = outputs_BboxNet[i][4].data.cpu().numpy() + mean_car_size[1]\n                pred_l = outputs_BboxNet[i][5].data.cpu().numpy() + mean_car_size[2]\n\n                pred_bin_scores = outputs_BboxNet[i][6:(6+4)].data.cpu().numpy() # (shape (NH=8, ))\n                pred_residuals = outputs_BboxNet[i][(6+4):].data.cpu().numpy() # (shape (NH=8, ))\n                pred_bin_number = np.argmax(pred_bin_scores)\n                pred_bin_center = getBinCenter(pred_bin_number, NH=NH)\n                pred_residual = pred_residuals[pred_bin_number]\n                pred_centered_r_y = pred_bin_center + pred_residual\n                pred_r_y = wrapToPi(pred_centered_r_y + frustum_angle) # NOTE!\n\n                pred_r_y = pred_r_y.data.cpu().numpy()\n                input_2Dbbox = input_2Dbbox.data.cpu().numpy()\n                score_2d = score_2d.cpu().numpy()\n\n                if img_id not in eval_dict:\n                    eval_dict[img_id] = []\n\n                bbox_dict = {}\n                bbox_dict[""pred_center_TNet""] = pred_center_TNet\n                bbox_dict[""pred_center_BboxNet""] = pred_center_BboxNet\n                bbox_dict[""centroid""] = centroid\n                bbox_dict[""pred_h""] = pred_h\n                bbox_dict[""pred_w""] = pred_w\n                bbox_dict[""pred_l""] = pred_l\n                bbox_dict[""pred_r_y""] = pred_r_y\n                bbox_dict[""input_2Dbbox""] = input_2Dbbox\n                bbox_dict[""score_2d""] = score_2d\n\n                eval_dict[img_id].append(bbox_dict)\n\nwith open(""%s/eval_dict_val_2ddetections.pkl"" % (network.model_dir), ""wb"") as file:\n    pickle.dump(eval_dict, file, protocol=2) # (protocol=2 is needed to be able to open this file with python2)\n'"
Extended-Frustum-PointNet/eval_frustum_pointnet_img_val_seq.py,39,"b'# camera-ready\n\nfrom datasets_img import EvalSequenceDatasetFrustumPointNetImg, wrapToPi, getBinCenter # (this needs to be imported before torch, because cv2 needs to be imported before torch for some reason)\nfrom frustum_pointnet_img import FrustumPointNetImg\n\nimport torch\nimport torch.utils.data\nimport torch.nn as nn\nfrom torch.autograd import Variable\nimport torch.optim as optim\nimport torch.nn.functional as F\n\nimport numpy as np\nimport pickle\nimport matplotlib\nmatplotlib.use(""Agg"")\nimport matplotlib.pyplot as plt\n\nsequence = ""0004""\n\nbatch_size = 8\n\nnetwork = FrustumPointNetImg(""Extended-Frustum-PointNet_eval_val_seq"", project_dir=""/root/3DOD_thesis"")\nnetwork.load_state_dict(torch.load(""/root/3DOD_thesis/pretrained_models/model_38_2_epoch_400.pth""))\nnetwork = network.cuda()\n\nNH = network.BboxNet_network.NH\n\nval_dataset = EvalSequenceDatasetFrustumPointNetImg(kitti_data_path=""/root/3DOD_thesis/data/kitti"",\n                                                    kitti_meta_path=""/root/3DOD_thesis/data/kitti/meta"",\n                                                    NH=NH, sequence=sequence)\n\nnum_val_batches = int(len(val_dataset)/batch_size)\n\nval_loader = torch.utils.data.DataLoader(dataset=val_dataset,\n                                         batch_size=batch_size, shuffle=False,\n                                         num_workers=4)\n\nregression_loss_func = nn.SmoothL1Loss()\n\nnetwork.eval() # (set in evaluation mode, this affects BatchNorm, dropout etc.)\nbatch_losses = []\nbatch_losses_InstanceSeg = []\nbatch_losses_TNet = []\nbatch_losses_BboxNet = []\nbatch_losses_BboxNet_center = []\nbatch_losses_BboxNet_size = []\nbatch_losses_BboxNet_heading_regr = []\nbatch_losses_BboxNet_heading_class = []\nbatch_losses_BboxNet_heading_class_weighted = []\nbatch_losses_corner = []\nbatch_accuracies = []\nbatch_precisions = []\nbatch_recalls = []\nbatch_f1s = []\nbatch_accuracies_heading_class = []\neval_dict = {}\nfor step, (frustum_point_clouds, bbox_2d_imgs, labels_InstanceSeg, labels_TNet, labels_BboxNet, labels_corner, labels_corner_flipped, img_ids, input_2Dbboxes, frustum_Rs, frustum_angles, centered_frustum_mean_xyz) in enumerate(val_loader):\n    with torch.no_grad(): # (corresponds to setting volatile=True in all variables, this is done during inference to reduce memory consumption)\n        frustum_point_clouds = Variable(frustum_point_clouds) # (shape: (batch_size, num_points, 4))\n        bbox_2d_imgs = Variable(bbox_2d_imgs) # (shape: (batch_size, 3, H, W))\n        labels_InstanceSeg = Variable(labels_InstanceSeg) # (shape: (batch_size, num_points))\n        labels_TNet = Variable(labels_TNet) # (shape: (batch_size, 3))\n        labels_BboxNet = Variable(labels_BboxNet) # (shape:(batch_size, 11))\n        labels_corner = Variable(labels_corner) # (shape: (batch_size, 8, 3))\n        labels_corner_flipped = Variable(labels_corner_flipped) # (shape: (batch_size, 8, 3))\n\n        frustum_point_clouds = frustum_point_clouds.transpose(2, 1) # (shape: (batch_size, 4, num_points))\n\n        frustum_point_clouds = frustum_point_clouds.cuda()\n        bbox_2d_imgs = bbox_2d_imgs.cuda()\n        labels_InstanceSeg = labels_InstanceSeg.cuda()\n        labels_TNet = labels_TNet.cuda()\n        labels_BboxNet = labels_BboxNet.cuda()\n        labels_corner = labels_corner.cuda()\n        labels_corner_flipped = labels_corner_flipped.cuda()\n\n        outputs = network(frustum_point_clouds, bbox_2d_imgs)\n        outputs_InstanceSeg = outputs[0] # (shape: (batch_size, num_points, 2))\n        outputs_TNet = outputs[1] # (shape: (batch_size, 3))\n        outputs_BboxNet = outputs[2] # (shape: (batch_size, 3 + 3 + 2*NH))\n        seg_point_clouds_mean = outputs[3] # (shape: (batch_size, 3))\n        dont_care_mask = outputs[4] # (shape: (batch_size, ))\n\n        ############################################################################\n        # save data for visualization:\n        ############################################################################\n        centered_frustum_mean_xyz = centered_frustum_mean_xyz[0].numpy()\n        for i in range(outputs_InstanceSeg.size()[0]):\n            dont_care_mask_value = dont_care_mask[i]\n\n            # don\'t care about predicted 3Dbboxes that corresponds to empty point clouds outputted by InstanceSeg:\n            if dont_care_mask_value == 1:\n                pred_InstanceSeg = outputs_InstanceSeg[i].data.cpu().numpy() # (shape: (num_points, 2))\n                frustum_point_cloud = frustum_point_clouds[i].transpose(1, 0).data.cpu().numpy() # (shape: (num_points, 4))\n                label_InstanceSeg = labels_InstanceSeg[i].data.cpu().numpy() # (shape: (num_points, ))\n                seg_point_cloud_mean = seg_point_clouds_mean[i].data.cpu().numpy() # (shape: (3, ))\n                img_id = img_ids[i]\n                #if img_id in [""000006"", ""000007"", ""000008"", ""000009"", ""000010"", ""000011"", ""000012"", ""000013"", ""000014"", ""000015"", ""000016"", ""000017"", ""000018"", ""000019"", ""000020"", ""000021""]:\n                input_2Dbbox = input_2Dbboxes[i] # (shape: (4, ))\n                frustum_R = frustum_Rs[i] # (shape: (3, 3))\n                frustum_angle = frustum_angles[i]\n\n                unshifted_frustum_point_cloud_xyz = frustum_point_cloud[:, 0:3] + centered_frustum_mean_xyz\n                decentered_frustum_point_cloud_xyz = np.dot(np.linalg.inv(frustum_R), unshifted_frustum_point_cloud_xyz.T).T\n                frustum_point_cloud[:, 0:3] = decentered_frustum_point_cloud_xyz\n\n                row_mask = pred_InstanceSeg[:, 1] > pred_InstanceSeg[:, 0]\n                pred_seg_point_cloud = frustum_point_cloud[row_mask, :]\n\n                row_mask = label_InstanceSeg == 1\n                gt_seg_point_cloud = frustum_point_cloud[row_mask, :]\n\n                pred_center_TNet = np.dot(np.linalg.inv(frustum_R), outputs_TNet[i].data.cpu().numpy() + centered_frustum_mean_xyz + seg_point_cloud_mean) # (shape: (3, )) # NOTE!\n                gt_center = np.dot(np.linalg.inv(frustum_R), labels_TNet[i].data.cpu().numpy() + centered_frustum_mean_xyz) # NOTE!\n                centroid = seg_point_cloud_mean\n\n                pred_center_BboxNet = np.dot(np.linalg.inv(frustum_R), outputs_BboxNet[i][0:3].data.cpu().numpy() + centered_frustum_mean_xyz + seg_point_cloud_mean + outputs_TNet[i].data.cpu().numpy()) # (shape: (3, )) # NOTE!\n\n                pred_h = outputs_BboxNet[i][3].data.cpu().numpy() + labels_BboxNet[i][8].data.cpu().numpy()\n                pred_w = outputs_BboxNet[i][4].data.cpu().numpy() + labels_BboxNet[i][9].data.cpu().numpy()\n                pred_l = outputs_BboxNet[i][5].data.cpu().numpy() + labels_BboxNet[i][10].data.cpu().numpy()\n\n                pred_bin_scores = outputs_BboxNet[i][6:(6+4)].data.cpu().numpy() # (shape (NH=8, ))\n                pred_residuals = outputs_BboxNet[i][(6+4):].data.cpu().numpy() # (shape (NH=8, ))\n                pred_bin_number = np.argmax(pred_bin_scores)\n                pred_bin_center = getBinCenter(pred_bin_number, NH=NH)\n                pred_residual = pred_residuals[pred_bin_number]\n                pred_centered_r_y = pred_bin_center + pred_residual\n                pred_r_y = wrapToPi(pred_centered_r_y + frustum_angle) # NOTE!\n\n                gt_h = labels_BboxNet[i][3].data.cpu().numpy()\n                gt_w = labels_BboxNet[i][4].data.cpu().numpy()\n                gt_l = labels_BboxNet[i][5].data.cpu().numpy()\n\n                gt_bin_number = labels_BboxNet[i][6].data.cpu().numpy()\n                gt_bin_center = getBinCenter(gt_bin_number, NH=NH)\n                gt_residual = labels_BboxNet[i][7].data.cpu().numpy()\n                gt_centered_r_y = gt_bin_center + gt_residual\n                gt_r_y = wrapToPi(gt_centered_r_y + frustum_angle) # NOTE!\n\n                pred_r_y = pred_r_y.data.cpu().numpy()\n                gt_r_y = gt_r_y.data.cpu().numpy()\n                input_2Dbbox = input_2Dbbox.data.cpu().numpy()\n\n                if img_id not in eval_dict:\n                    eval_dict[img_id] = []\n\n                bbox_dict = {}\n                # bbox_dict[""frustum_point_cloud""] = frustum_point_cloud\n                # bbox_dict[""pred_seg_point_cloud""] = pred_seg_point_cloud\n                # bbox_dict[""gt_seg_point_cloud""] = gt_seg_point_cloud\n                bbox_dict[""pred_center_TNet""] = pred_center_TNet\n                bbox_dict[""pred_center_BboxNet""] = pred_center_BboxNet\n                bbox_dict[""gt_center""] = gt_center\n                bbox_dict[""centroid""] = centroid\n                bbox_dict[""pred_h""] = pred_h\n                bbox_dict[""pred_w""] = pred_w\n                bbox_dict[""pred_l""] = pred_l\n                bbox_dict[""pred_r_y""] = pred_r_y\n                bbox_dict[""gt_h""] = gt_h\n                bbox_dict[""gt_w""] = gt_w\n                bbox_dict[""gt_l""] = gt_l\n                bbox_dict[""gt_r_y""] = gt_r_y\n                bbox_dict[""input_2Dbbox""] = input_2Dbbox\n\n                eval_dict[img_id].append(bbox_dict)\n\n        ########################################################################\n        # compute precision, recall etc. for the InstanceSeg:\n        ########################################################################\n        preds = outputs_InstanceSeg.data.cpu().numpy() # (shape: (batch_size, num_points, 2))\n        preds = np.argmax(preds, 2) # (shape: (batch_size, num_points))\n\n        labels_InstanceSeg_np = labels_InstanceSeg.data.cpu().numpy() # (shape: (batch_size, num_points))\n\n        accuracy = np.count_nonzero(preds == labels_InstanceSeg_np)/(preds.shape[0]*preds.shape[1])\n        if np.count_nonzero(preds == 1) > 0:\n            precision = np.count_nonzero(np.logical_and(preds == labels_InstanceSeg_np, preds == 1))/np.count_nonzero(preds == 1) # (TP/(TP + FP))\n        else:\n            precision = -1\n        if np.count_nonzero(labels_InstanceSeg_np == 1) > 0:\n            recall = np.count_nonzero(np.logical_and(preds == labels_InstanceSeg_np, preds == 1))/np.count_nonzero(labels_InstanceSeg_np == 1) # (TP/(TP + FN))\n        else:\n            recall = -1\n        if recall + precision > 0:\n            f1 = 2*recall*precision/(recall + precision)\n        else:\n            f1 = -1\n\n        batch_accuracies.append(accuracy)\n        if precision != -1:\n            batch_precisions.append(precision)\n        if recall != -1:\n            batch_recalls.append(recall)\n        if f1 != -1:\n            batch_f1s.append(f1)\n\n        ########################################################################\n        # compute accuracy for the heading classification:\n        ########################################################################\n        pred_bin_scores = outputs_BboxNet[:, 6:(6+NH)].data.cpu().numpy() # (shape: (batch_size, NH))\n        pred_bin_numbers = np.argmax(pred_bin_scores, 1) # (shape: (batch_size, ))\n        gt_bin_numbers = labels_BboxNet[:, 6].data.cpu().numpy() # (shape: (batch_size, ))\n\n        accuracy_heading_class = np.count_nonzero(pred_bin_numbers == gt_bin_numbers)/(pred_bin_numbers.shape[0])\n        batch_accuracies_heading_class.append(accuracy_heading_class)\n\n        ########################################################################\n        # compute the InstanceSeg loss:\n        ########################################################################\n        outputs_InstanceSeg = outputs_InstanceSeg.view(-1, 2) # (shape (batch_size*num_points, 2))\n        labels_InstanceSeg = labels_InstanceSeg.view(-1, 1) # (shape: (batch_size*num_points, 1))\n        labels_InstanceSeg = labels_InstanceSeg[:, 0] # (shape: (batch_size*num_points, ))\n        loss_InstanceSeg = F.nll_loss(outputs_InstanceSeg, labels_InstanceSeg)\n        loss_InstanceSeg_value = loss_InstanceSeg.data.cpu().numpy()\n        batch_losses_InstanceSeg.append(loss_InstanceSeg_value)\n\n        ########################################################################\n        # compute the TNet loss:\n        ########################################################################\n        # mask entries corresponding to empty seg point clouds (select only the entries which we care about):\n        outputs_TNet = outputs_TNet[dont_care_mask, :] # (shape: (batch_size*, 3))\n        labels_TNet = labels_TNet[dont_care_mask, :] # (shape: (batch_size*, 3))\n        seg_point_clouds_mean = seg_point_clouds_mean[dont_care_mask, :] # (shape: (batch_size*, 3))\n\n        # shift the GT to the seg point clouds local coords:\n        labels_TNet = labels_TNet - seg_point_clouds_mean\n\n        # compute the Huber (smooth L1) loss:\n        if outputs_TNet.size()[0] == 0:\n            loss_TNet = Variable(torch.from_numpy(np.zeros((1, ), dtype=np.float32))).cuda()\n        else:\n            loss_TNet = regression_loss_func(outputs_TNet, labels_TNet)\n\n        loss_TNet_value = loss_TNet.data.cpu().numpy()\n        batch_losses_TNet.append(loss_TNet_value)\n\n        ########################################################################\n        # compute the BboxNet loss:\n        ########################################################################\n        # mask entries corresponding to empty seg point clouds (select only the entries which we care about):\n        outputs_BboxNet = outputs_BboxNet[dont_care_mask, :] # (shape: (batch_size*, 3 + 3 + 2*NH))\n        labels_BboxNet = labels_BboxNet[dont_care_mask, :]# (shape: (batch_size*, 11))\n\n        if outputs_BboxNet.size()[0] == 0:\n            loss_BboxNet = Variable(torch.from_numpy(np.zeros((1, ), dtype=np.float32))).cuda()\n            loss_BboxNet_size = Variable(torch.from_numpy(np.zeros((1, ), dtype=np.float32))).cuda()\n            loss_BboxNet_center = Variable(torch.from_numpy(np.zeros((1, ), dtype=np.float32))).cuda()\n            loss_BboxNet_heading_class = Variable(torch.from_numpy(np.zeros((1, ), dtype=np.float32))).cuda()\n            loss_BboxNet_heading_regr = Variable(torch.from_numpy(np.zeros((1, ), dtype=np.float32))).cuda()\n        else:\n            # compute the BboxNet center loss:\n            labels_BboxNet_center = labels_BboxNet[:, 0:3] # (shape: (batch_size*, 3))\n            # # shift the center GT to local coords:\n            labels_BboxNet_center = Variable(labels_BboxNet_center.data - seg_point_clouds_mean.data - outputs_TNet.data).cuda() # (outputs_TNet is a variable outputted by model, so it requires grads, which cant be passed as target to the loss function)\n            # # compute the Huber (smooth L1) loss:\n            outputs_BboxNet_center = outputs_BboxNet[:, 0:3]\n            loss_BboxNet_center = regression_loss_func(outputs_BboxNet_center, labels_BboxNet_center)\n\n            # compute the BboxNet size loss:\n            labels_BboxNet_size = labels_BboxNet[:, 3:6] # (shape: (batch_size*, 3))\n            # # subtract the mean car size in train:\n            labels_BboxNet_size = labels_BboxNet_size - labels_BboxNet[:, 8:]\n            # # compute the Huber (smooth L1) loss:\n            loss_BboxNet_size = regression_loss_func(outputs_BboxNet[:, 3:6], labels_BboxNet_size)\n\n            # compute the BboxNet heading loss\n            # # compute the classification loss:\n            labels_BboxNet_heading_class = Variable(labels_BboxNet[:, 6].data.type(torch.LongTensor)).cuda() # (shape: (batch_size*, ))\n            outputs_BboxNet_heading_class = outputs_BboxNet[:, 6:(6+NH)] # (shape: (batch_size*, NH))\n            loss_BboxNet_heading_class = F.nll_loss(F.log_softmax(outputs_BboxNet_heading_class, dim=1), labels_BboxNet_heading_class)\n            # # compute the regression loss:\n            # # # # get the GT residual for the GT bin:\n            labels_BboxNet_heading_regr = labels_BboxNet[:, 7] # (shape: (batch_size*, ))\n            # # # # get the pred residual for all bins:\n            outputs_BboxNet_heading_regr_all = outputs_BboxNet[:, (6+NH):] # (shape: (batch_size*, 8))\n            # # # # get the pred residual for the GT bin:\n            outputs_BboxNet_heading_regr = outputs_BboxNet_heading_regr_all.gather(1, labels_BboxNet_heading_class.view(-1, 1)) # (shape: (batch_size*, 1))\n            outputs_BboxNet_heading_regr = outputs_BboxNet_heading_regr[:, 0] # (shape: (batch_size*, )\n            # # # # compute the loss:\n            loss_BboxNet_heading_regr = regression_loss_func(outputs_BboxNet_heading_regr, labels_BboxNet_heading_regr)\n            # # compute the total BBoxNet heading loss:\n            loss_BboxNet_heading = loss_BboxNet_heading_class + 10*loss_BboxNet_heading_regr\n\n            # compute the BboxNet total loss:\n            loss_BboxNet = loss_BboxNet_center + loss_BboxNet_size + loss_BboxNet_heading\n\n        loss_BboxNet_value = loss_BboxNet.data.cpu().numpy()\n        batch_losses_BboxNet.append(loss_BboxNet_value)\n\n        loss_BboxNet_size_value = loss_BboxNet_size.data.cpu().numpy()\n        batch_losses_BboxNet_size.append(loss_BboxNet_size_value)\n\n        loss_BboxNet_center_value = loss_BboxNet_center.data.cpu().numpy()\n        batch_losses_BboxNet_center.append(loss_BboxNet_center_value)\n\n        loss_BboxNet_heading_class_value = loss_BboxNet_heading_class.data.cpu().numpy()\n        batch_losses_BboxNet_heading_class.append(loss_BboxNet_heading_class_value)\n\n        loss_BboxNet_heading_regr_value = loss_BboxNet_heading_regr.data.cpu().numpy()\n        batch_losses_BboxNet_heading_regr.append(loss_BboxNet_heading_regr_value)\n\n        ########################################################################\n        # compute the corner loss:\n        ########################################################################\n        # mask entries corresponding to empty seg point clouds (select only the entries which we care about):\n        labels_corner = labels_corner[dont_care_mask]# (shape: (batch_size*, 8, 3))\n        labels_corner_flipped = labels_corner_flipped[dont_care_mask]# (shape: (batch_size*, 8, 3))\n\n        if outputs_BboxNet.size()[0] == 0:\n            loss_corner = Variable(torch.from_numpy(np.zeros((1, ), dtype=np.float32))).cuda()\n        else:\n            outputs_BboxNet_center = outputs_BboxNet[:, 0:3] # (shape: (batch_size*, 3))\n            # shift to the same coords used in the labels for the corner loss:\n            pred_center = outputs_BboxNet_center + seg_point_clouds_mean + outputs_TNet # (shape: (batch_size*, 3))\n            pred_center_unsqeezed = pred_center.unsqueeze(2) # (shape: (batch_size, 3, 1))\n\n            # shift the outputted size to the same ""coords"" used in the labels for the corner loss:\n            outputs_BboxNet_size = outputs_BboxNet[:, 3:6] + labels_BboxNet[:, 8:] # (shape: (batch_size*, 3))\n\n            pred_h = outputs_BboxNet_size[:, 0] # (shape: (batch_size*, ))\n            pred_w = outputs_BboxNet_size[:, 1] # (shape: (batch_size*, ))\n            pred_l = outputs_BboxNet_size[:, 2] # (shape: (batch_size*, ))\n\n            # get the pred residuals for the GT bins:\n            pred_residuals = outputs_BboxNet_heading_regr # (shape: (batch_size*, ))\n\n            Rmat = Variable(torch.zeros(pred_h.size()[0], 3, 3), requires_grad=True).cuda() # (shape: (batch_size*, 3, 3))\n            Rmat[:, 0, 0] = torch.cos(pred_residuals)\n            Rmat[:, 0, 2] = torch.sin(pred_residuals)\n            Rmat[:, 1, 1] = 1\n            Rmat[:, 2, 0] = -torch.sin(pred_residuals)\n            Rmat[:, 2, 2] = torch.cos(pred_residuals)\n\n            p0_orig = Variable(torch.zeros(pred_h.size()[0], 3, 1), requires_grad=True).cuda() # (shape: (batch_size*, 3, 1))\n            p0_orig[:, 0, 0] = pred_l/2.0\n            p0_orig[:, 2, 0] = pred_w/2.0\n\n            p1_orig = Variable(torch.zeros(pred_h.size()[0], 3, 1), requires_grad=True).cuda() # (shape: (batch_size*, 3, 1))\n            p1_orig[:, 0, 0] = -pred_l/2.0\n            p1_orig[:, 2, 0] = pred_w/2.0\n\n            p2_orig = Variable(torch.zeros(pred_h.size()[0], 3, 1), requires_grad=True).cuda() # (shape: (batch_size*, 3, 1))\n            p2_orig[:, 0, 0] = -pred_l/2.0\n            p2_orig[:, 2, 0] = -pred_w/2.0\n\n            p3_orig = Variable(torch.zeros(pred_h.size()[0], 3, 1), requires_grad=True).cuda() # (shape: (batch_size*, 3, 1))\n            p3_orig[:, 0, 0] = pred_l/2.0\n            p3_orig[:, 2, 0] = -pred_w/2.0\n\n            p4_orig = Variable(torch.zeros(pred_h.size()[0], 3, 1), requires_grad=True).cuda() # (shape: (batch_size*, 3, 1))\n            p4_orig[:, 0, 0] = pred_l/2.0\n            p4_orig[:, 1, 0] = -pred_h\n            p4_orig[:, 2, 0] = pred_w/2.0\n\n            p5_orig = Variable(torch.zeros(pred_h.size()[0], 3, 1), requires_grad=True).cuda() # (shape: (batch_size*, 3, 1))\n            p5_orig[:, 0, 0] = -pred_l/2.0\n            p5_orig[:, 1, 0] = -pred_h\n            p5_orig[:, 2, 0] = pred_w/2.0\n\n            p6_orig = Variable(torch.zeros(pred_h.size()[0], 3, 1), requires_grad=True).cuda() # (shape: (batch_size*, 3, 1))\n            p6_orig[:, 0, 0] = -pred_l/2.0\n            p6_orig[:, 1, 0] = -pred_h\n            p6_orig[:, 2, 0] = -pred_w/2.0\n\n            p7_orig = Variable(torch.zeros(pred_h.size()[0], 3, 1), requires_grad=True).cuda() # (shape: (batch_size*, 3, 1))\n            p7_orig[:, 0, 0] = pred_l/2.0\n            p7_orig[:, 1, 0] = -pred_h\n            p7_orig[:, 2, 0] = -pred_w/2.0\n\n            pred_p0_unsqeezed = pred_center_unsqeezed + torch.bmm(Rmat, p0_orig) # (shape: (batch_size*, 3, 1))\n            pred_p0 = pred_p0_unsqeezed[:, :, 0] # (shape: (batch_size*, 3))\n            pred_p1_unsqeezed = pred_center_unsqeezed + torch.bmm(Rmat, p1_orig) # (shape: (batch_size*, 3, 1))\n            pred_p1 = pred_p1_unsqeezed[:, :, 0] # (shape: (batch_size*, 3))\n            pred_p2_unsqeezed = pred_center_unsqeezed + torch.bmm(Rmat, p2_orig) # (shape: (batch_size*, 3, 1))\n            pred_p2 = pred_p2_unsqeezed[:, :, 0] # (shape: (batch_size*, 3))\n            pred_p3_unsqeezed = pred_center_unsqeezed + torch.bmm(Rmat, p3_orig) # (shape: (batch_size*, 3, 1))\n            pred_p3 = pred_p3_unsqeezed[:, :, 0] # (shape: (batch_size*, 3))\n            pred_p4_unsqeezed = pred_center_unsqeezed + torch.bmm(Rmat, p4_orig) # (shape: (batch_size*, 3, 1))\n            pred_p4 = pred_p4_unsqeezed[:, :, 0] # (shape: (batch_size*, 3))\n            pred_p5_unsqeezed = pred_center_unsqeezed + torch.bmm(Rmat, p5_orig) # (shape: (batch_size*, 3, 1))\n            pred_p5 = pred_p5_unsqeezed[:, :, 0] # (shape: (batch_size*, 3))\n            pred_p6_unsqeezed = pred_center_unsqeezed + torch.bmm(Rmat, p6_orig) # (shape: (batch_size*, 3, 1))\n            pred_p6 = pred_p6_unsqeezed[:, :, 0] # (shape: (batch_size*, 3))\n            pred_p7_unsqeezed = pred_center_unsqeezed + torch.bmm(Rmat, p7_orig) # (shape: (batch_size*, 3, 1))\n            pred_p7 = pred_p7_unsqeezed[:, :, 0] # (shape: (batch_size*, 3))\n\n            outputs_corner = Variable(torch.zeros(pred_h.size()[0], 8, 3), requires_grad=True).cuda() # (shape: (batch_size*, 8, 3))\n            outputs_corner[:, 0] = pred_p0\n            outputs_corner[:, 1] = pred_p1\n            outputs_corner[:, 2] = pred_p2\n            outputs_corner[:, 3] = pred_p3\n            outputs_corner[:, 4] = pred_p4\n            outputs_corner[:, 5] = pred_p5\n            outputs_corner[:, 6] = pred_p6\n            outputs_corner[:, 7] = pred_p7\n\n            loss_corner_unflipped = regression_loss_func(outputs_corner, labels_corner)\n            loss_corner_flipped = regression_loss_func(outputs_corner, labels_corner_flipped)\n\n            loss_corner = torch.min(loss_corner_unflipped, loss_corner_flipped)\n\n        loss_corner_value = loss_corner.data.cpu().numpy()\n        batch_losses_corner.append(loss_corner_value)\n\n        ########################################################################\n        # compute the total loss:\n        ########################################################################\n        lambda_value = 1\n        gamma_value = 10\n        loss = loss_InstanceSeg + lambda_value*(loss_TNet + loss_BboxNet + gamma_value*loss_corner)\n        loss_value = loss.data.cpu().numpy()\n        batch_losses.append(loss_value)\n\n# compute the val epoch loss:\nepoch_loss = np.mean(batch_losses)\nprint (""validation loss: %g"" % epoch_loss)\n# compute the val epoch TNet loss:\nepoch_loss = np.mean(batch_losses_TNet)\nprint (""validation TNet loss: %g"" % epoch_loss)\n# compute the val epoch InstanceSeg loss:\nepoch_loss = np.mean(batch_losses_InstanceSeg)\nprint (""validation InstanceSeg loss: %g"" % epoch_loss)\n# compute the val epoch BboxNet loss:\nepoch_loss = np.mean(batch_losses_BboxNet)\nprint (""validation BboxNet loss: %g"" % epoch_loss)\n# compute the val epoch BboxNet size loss:\nepoch_loss = np.mean(batch_losses_BboxNet_size)\nprint (""validation BboxNet size loss: %g"" % epoch_loss)\n# compute the val epoch BboxNet center loss:\nepoch_loss = np.mean(batch_losses_BboxNet_center)\nprint (""validation BboxNet center loss: %g"" % epoch_loss)\n# compute the val epoch BboxNet heading class loss:\nepoch_loss = np.mean(batch_losses_BboxNet_heading_class)\nprint (""validation BboxNet heading class loss: %g"" % epoch_loss)\n# compute the val epoch BboxNet heading regr loss:\nepoch_loss = np.mean(batch_losses_BboxNet_heading_regr)\nprint (""validation BboxNet heading regr loss: %g"" % epoch_loss)\n# compute the val epoch heading class accuracy:\nepoch_accuracy = np.mean(batch_accuracies_heading_class)\nprint (""validation heading class accuracy: %g"" % epoch_accuracy)\n# compute the val epoch corner loss:\nepoch_loss = np.mean(batch_losses_corner)\nprint (""validation corner loss: %g"" % epoch_loss)\n# compute the val epoch accuracy:\nepoch_accuracy = np.mean(batch_accuracies)\nprint (""validation accuracy: %g"" % epoch_accuracy)\n# compute the val epoch precision:\nepoch_precision = np.mean(batch_precisions)\nprint (""validation precision: %g"" % epoch_precision)\n# compute the val epoch recall:\nepoch_recall = np.mean(batch_recalls)\nprint (""validation recall: %g"" % epoch_recall)\n# compute the val epoch f1:\nepoch_f1 = np.mean(batch_f1s)\nprint (""validation f1: %g"" % epoch_f1)\n\nwith open(""%s/eval_dict_val_seq_%s.pkl"" % (network.model_dir, sequence), ""wb"") as file:\n    pickle.dump(eval_dict, file, protocol=2) # (protocol=2 is needed to be able to open this file with python2)\n'"
Extended-Frustum-PointNet/frustum_pointnet_img.py,14,"b'# camera-ready\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nimport torchvision.models as models\n\nimport os\nimport numpy as np\n\nclass InstanceSeg(nn.Module):\n    def __init__(self, num_points=1024):\n        super(InstanceSeg, self).__init__()\n\n        self.num_points = num_points\n\n        self.conv1 = nn.Conv1d(4, 64, 1)\n        self.conv2 = nn.Conv1d(64, 64, 1)\n        self.conv3 = nn.Conv1d(64, 64, 1)\n        self.conv4 = nn.Conv1d(64, 128, 1)\n        self.conv5 = nn.Conv1d(128, 1024, 1)\n        self.conv6 = nn.Conv1d(1088, 512, 1)\n        self.conv7 = nn.Conv1d(512, 256, 1)\n        self.conv8 = nn.Conv1d(256, 128, 1)\n        self.conv9 = nn.Conv1d(128, 128, 1)\n        self.conv10 = nn.Conv1d(128, 2, 1)\n        self.max_pool = nn.MaxPool1d(num_points)\n\n    def forward(self, x):\n        batch_size = x.size()[0] # (x has shape (batch_size, 4, num_points))\n\n        out = F.relu(self.conv1(x)) # (shape: (batch_size, 64, num_points))\n        out = F.relu(self.conv2(out)) # (shape: (batch_size, 64, num_points))\n        point_features = out\n\n        out = F.relu(self.conv3(out)) # (shape: (batch_size, 64, num_points))\n        out = F.relu(self.conv4(out)) # (shape: (batch_size, 128, num_points))\n        out = F.relu(self.conv5(out)) # (shape: (batch_size, 1024, num_points))\n        global_feature = self.max_pool(out) # (shape: (batch_size, 1024, 1))\n\n        global_feature_repeated = global_feature.repeat(1, 1, self.num_points) # (shape: (batch_size, 1024, num_points))\n        out = torch.cat([global_feature_repeated, point_features], 1) # (shape: (batch_size, 1024+64=1088, num_points))\n\n        out = F.relu(self.conv6(out)) # (shape: (batch_size, 512, num_points))\n        out = F.relu(self.conv7(out)) # (shape: (batch_size, 256, num_points))\n        out = F.relu(self.conv8(out)) # (shape: (batch_size, 128, num_points))\n        out = F.relu(self.conv9(out)) # (shape: (batch_size, 128, num_points))\n\n        out = self.conv10(out) # (shape: (batch_size, 2, num_points))\n\n        out = out.transpose(2,1).contiguous() # (shape: (batch_size, num_points, 2))\n        out = F.log_softmax(out.view(-1, 2), dim=1) # (shape: (batch_size*num_points, 2))\n        out = out.view(batch_size, self.num_points, 2) # (shape: (batch_size, num_points, 2))\n\n        return out\n\nclass TNet(nn.Module):\n    def __init__(self, num_seg_points=512):\n        super(TNet, self).__init__()\n\n        self.num_seg_points = num_seg_points\n\n        self.conv1 = nn.Conv1d(3, 128, 1)\n        self.conv2 = nn.Conv1d(128, 256, 1)\n        self.conv3 = nn.Conv1d(256, 512, 1)\n        self.max_pool = nn.MaxPool1d(num_seg_points)\n        self.fc1 = nn.Linear(512, 256)\n        self.fc2 = nn.Linear(256, 128)\n        self.fc3 = nn.Linear(128, 3)\n\n    def forward(self, x):\n        # (x has shape (batch_size, 3, num_seg_points))\n\n        out = F.relu(self.conv1(x)) # (shape: (batch_size, 128, num_seg_points))\n        out = F.relu(self.conv2(out)) # (shape: (batch_size, 256, num_seg_points))\n        out = F.relu(self.conv3(out)) # (shape: (batch_size, 512, num_seg_points))\n        out = self.max_pool(out) # (shape: (batch_size, 512, 1))\n        out = out.view(-1, 512) # (shape: (batch_size, 512))\n\n        out = F.relu(self.fc1(out)) # (shape: (batch_size, 256))\n        out = F.relu(self.fc2(out)) # (shape: (batch_size, 128))\n\n        out = self.fc3(out) # (shape: (batch_size, 3))\n\n        return out\n\nclass BboxNet(nn.Module):\n    def __init__(self, num_seg_points=512):\n        super(BboxNet, self).__init__()\n\n        self.NH = 4 # (number of angle bins)\n\n        self.num_seg_points = num_seg_points\n\n        self.conv1 = nn.Conv1d(3, 128, 1)\n        self.conv2 = nn.Conv1d(128, 128, 1)\n        self.conv3 = nn.Conv1d(128, 256, 1)\n        self.conv4 = nn.Conv1d(256, 512, 1)\n        self.max_pool = nn.MaxPool1d(num_seg_points)\n\n        self.fc_pcd1 = nn.Linear(512, 512)\n        self.fc_pcd2 = nn.Linear(512, 256)\n        self.fc_pcd3 = nn.Linear(256, 3)\n\n        self.fc_pcd_img1 = nn.Linear(1024, 512)\n        self.fc_pcd_img2 = nn.Linear(512, 256)\n        self.fc_pcd_img3 = nn.Linear(256, 3 + 2*self.NH)\n\n        resnet34 = models.resnet34()\n        # load pretrained model:\n        resnet34.load_state_dict(torch.load(""/root/3DOD_thesis/pretrained_models/resnet/resnet34-333f7ec4.pth""))\n        # remove fully connected layer:\n        self.resnet34 = nn.Sequential(*list(resnet34.children())[:-2])\n\n        self.avg_pool = nn.AvgPool2d(kernel_size=7)\n\n    def forward(self, pcd, img):\n        # (pcd has shape (batch_size, 3, num_seg_points))\n        # (img has shape (batch_size, 3, H, W))\n\n        out_pcd = F.relu(self.conv1(pcd)) # (shape: (batch_size, 128, num_seg_points))\n        out_pcd = F.relu(self.conv2(out_pcd)) # (shape: (batch_size, 128, num_seg_points))\n        out_pcd = F.relu(self.conv3(out_pcd)) # (shape: (batch_size, 256, num_seg_points))\n        out_pcd = F.relu(self.conv4(out_pcd)) # (shape: (batch_size, 512, num_seg_points))\n        out_pcd = self.max_pool(out_pcd) # (shape: (batch_size, 512, 1))\n        out_pcd = out_pcd.view(-1, 512) # (shape: (batch_size, 512))\n\n        out_img = self.resnet34(img) # (shape: (batch_size, 512, 7, 7))\n        out_img = self.avg_pool(out_img) # (shape: (batch_size, 512, 1, 1))\n        out_img = out_img.view(-1, 512) # (shape: (batch_size, 512))\n\n        out_pcd_img = torch.cat([out_pcd, out_img], 1) # (shape: (batch_size, 1024))\n\n        out_pcd_img = F.relu(self.fc_pcd_img1(out_pcd_img)) # (shape: (batch_size, 512))\n        out_pcd_img = F.relu(self.fc_pcd_img2(out_pcd_img)) # (shape: (batch_size, 256))\n        out_pcd_img = self.fc_pcd_img3(out_pcd_img) # (shape: (batch_size, 3 + 2*NH))\n\n        out_pcd = F.relu(self.fc_pcd1(out_pcd)) # (shape: (batch_size, 512))\n        out_pcd = F.relu(self.fc_pcd2(out_pcd)) # (shape: (batch_size, 256))\n        out_pcd = self.fc_pcd3(out_pcd) # (shape: (batch_size, 3))\n\n        out = torch.cat([out_pcd, out_pcd_img], 1) # (shape: (batch_size, 3 + 3 + 2*NH))\n\n        return out\n\nclass FrustumPointNetImg(nn.Module):\n    def __init__(self, model_id, project_dir, num_points=1024):\n        super(FrustumPointNetImg, self).__init__()\n\n        self.num_points = num_points\n        self.model_id = model_id\n        self.project_dir = project_dir\n        self.create_model_dirs()\n\n        self.InstanceSeg_network = InstanceSeg()\n        self.TNet_network = TNet()\n        self.BboxNet_network = BboxNet()\n\n    def forward(self, pcd, img):\n        batch_size = pcd.size()[0] # (pcd has shape (batch_size, 4, num_points))\n\n        # (img has shape (batch_size, 3, H, W))\n\n        out_InstanceSeg = self.InstanceSeg_network(pcd) # (shape (batch_size, num_points, 2))\n\n        ########################################################################\n        point_clouds = pcd.transpose(2, 1)[:, :, 0:3].data.cpu().numpy() # (shape: (batch_size, num_points, 3))\n        seg_scores = out_InstanceSeg.data.cpu().numpy() # (shape: (batch_size, num_points_ 2))\n\n        seg_point_clouds = np.zeros((0, 512, 3), dtype=np.float32) # (shape: (batch_size, 512=num_seg_points, 3))\n        out_dont_care_mask = torch.ones((batch_size, )) # (shape: (batch_size, ))\n        out_dont_care_mask = out_dont_care_mask.type(torch.ByteTensor).cuda() # (NOTE! ByteTensor is needed for this to act as a selction mask)\n        for i in range(seg_scores.shape[0]):\n            ex_seg_scores = seg_scores[i] # (shape: (num_points, 2))\n            ex_point_cloud = point_clouds[i] # (shape: (num_points, 3))\n\n            row_mask = ex_seg_scores[:, 1] > ex_seg_scores[:, 0]\n            ex_seg_point_cloud = ex_point_cloud[row_mask, :]\n\n            if ex_seg_point_cloud.shape[0] == 0: # (if the segmented point cloud is empty:)\n                ex_seg_point_cloud = np.zeros((512, 3), dtype=np.float32)\n                out_dont_care_mask[i] = 0\n\n            # randomly sample 512 points in ex_seg_point_cloud:\n            if ex_seg_point_cloud.shape[0] < 512:\n                row_idx = np.random.choice(ex_seg_point_cloud.shape[0], 512, replace=True)\n            else:\n                row_idx = np.random.choice(ex_seg_point_cloud.shape[0], 512, replace=False)\n            ex_seg_point_cloud = ex_seg_point_cloud[row_idx, :]\n\n            seg_point_clouds = np.concatenate((seg_point_clouds, [ex_seg_point_cloud]), axis=0)\n\n        # subtract the point cloud centroid from each seg_point_cloud (transform to local coords):\n        seg_point_clouds_mean = np.mean(seg_point_clouds, axis=1) # (shape: (batch_size, 3)) (seg_point_clouds has shape (batch_size, num_seg_points, 3))\n        out_seg_point_clouds_mean = Variable(torch.from_numpy(seg_point_clouds_mean)).cuda()\n        seg_point_clouds_mean = np.expand_dims(seg_point_clouds_mean, axis=1) # (shape: (batch_size, 1, 3))\n        seg_point_clouds = seg_point_clouds - seg_point_clouds_mean\n\n        seg_point_clouds = Variable(torch.from_numpy(seg_point_clouds)).cuda() # (shape: (batch_size, num_seg_points, 3))\n        seg_point_clouds = seg_point_clouds.transpose(2, 1) # (shape: (batch_size, 3, num_seg_points))\n        ########################################################################\n\n        out_TNet = self.TNet_network(seg_point_clouds) # (shape: (batch_size, 3))\n\n        # subtract the TNet predicted translation from each seg_point_cloud (transfrom to local coords):\n        seg_point_clouds = seg_point_clouds - out_TNet.unsqueeze(2).repeat(1, 1, seg_point_clouds.size()[2]) # (out_TNet.unsqueeze(2).repeat(1, 1, seg_point_clouds.size()[2]) has shape: (batch_size, 3, num_seg_points))\n\n        out_BboxNet = self.BboxNet_network(seg_point_clouds, img) # (shape: (batch_size, 3 + 3 + 1))\n\n        # (out_InstanceSeg has shape: (batch_size, num_points, 2))\n        # (out_seg_point_clouds_mean has shape: (batch_size, 3))\n        # (out_dont_care_mask has shape: (batch_size, ))\n\n        return (out_InstanceSeg, out_TNet, out_BboxNet, out_seg_point_clouds_mean, out_dont_care_mask)\n\n    def create_model_dirs(self):\n        self.logs_dir = self.project_dir + ""/training_logs""\n        self.model_dir = self.logs_dir + ""/model_%s"" % self.model_id\n        self.checkpoints_dir = self.model_dir + ""/checkpoints""\n        if not os.path.exists(self.logs_dir):\n            os.makedirs(self.logs_dir)\n        if not os.path.exists(self.model_dir):\n            os.makedirs(self.model_dir)\n            os.makedirs(self.checkpoints_dir)\n\n# from torch.autograd import Variable\n# x = Variable(torch.randn(32, 3, 512))\n# network = BboxNet()\n# out = network(x)\n#\n# x = Variable(torch.randn(32, 4, 1024)).cuda()\n# network = FrustumPointNet(""frustum_pointnet_test"", ""/staging/frexgus/frustum_pointnet"")\n# network = network.cuda()\n# out = network(x)\n'"
Extended-Frustum-PointNet/train_frustum_pointnet_img.py,73,"b'# camera-ready\n\nfrom datasets_img import DatasetFrustumPointNetImgAugmentation, EvalDatasetFrustumPointNetImg, getBinCenters, wrapToPi # (this needs to be imported before torch, because cv2 needs to be imported before torch for some reason)\nfrom frustum_pointnet_img import FrustumPointNetImg\n\nimport torch\nimport torch.utils.data\nimport torch.nn as nn\nfrom torch.autograd import Variable\nimport torch.optim as optim\nimport torch.nn.functional as F\n\nimport numpy as np\nimport pickle\nimport matplotlib\nmatplotlib.use(""Agg"")\nimport matplotlib.pyplot as plt\n\n# NOTE! NOTE! change this to not overwrite all log data when you train the model:\nmodel_id = ""Extended-Frustum-PointNet_1""\n\nnum_epochs = 700\nbatch_size = 8\nlearning_rate = 0.001\n\nnetwork = FrustumPointNetImg(model_id, project_dir=""/root/3DOD_thesis"")\nnetwork = network.cuda()\n\nNH = network.BboxNet_network.NH\n\ntrain_dataset = DatasetFrustumPointNetImgAugmentation(kitti_data_path=""/root/3DOD_thesis/data/kitti"",\n                                                      kitti_meta_path=""/root/3DOD_thesis/data/kitti/meta"",\n                                                      type=""train"", NH=NH)\nval_dataset = EvalDatasetFrustumPointNetImg(kitti_data_path=""/root/3DOD_thesis/data/kitti"",\n                                            kitti_meta_path=""/root/3DOD_thesis/data/kitti/meta"",\n                                            type=""val"", NH=NH)\n\nnum_train_batches = int(len(train_dataset)/batch_size)\nnum_val_batches = int(len(val_dataset)/batch_size)\n\nprint (""num_train_batches:"", num_train_batches)\nprint (""num_val_batches:"", num_val_batches)\n\ntrain_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n                                           batch_size=batch_size, shuffle=True,\n                                           num_workers=4)\nval_loader = torch.utils.data.DataLoader(dataset=val_dataset,\n                                         batch_size=batch_size, shuffle=False,\n                                         num_workers=4)\n\nregression_loss_func = nn.SmoothL1Loss()\n\noptimizer = torch.optim.Adam(network.parameters(), lr=learning_rate)\n\nepoch_losses_train = []\nepoch_losses_InstanceSeg_train = []\nepoch_losses_TNet_train = []\nepoch_losses_BboxNet_train = []\nepoch_losses_BboxNet_size_train = []\nepoch_losses_BboxNet_center_train = []\nepoch_losses_BboxNet_heading_class_train = []\nepoch_losses_BboxNet_heading_regr_train = []\nepoch_accuracies_heading_class_train = []\nepoch_losses_corner_train = []\nepoch_accuracies_train = []\nepoch_precisions_train = []\nepoch_recalls_train = []\nepoch_f1s_train = []\nepoch_losses_val = []\nepoch_losses_InstanceSeg_val = []\nepoch_losses_TNet_val = []\nepoch_losses_BboxNet_val = []\nepoch_losses_BboxNet_size_val = []\nepoch_losses_BboxNet_center_val = []\nepoch_losses_BboxNet_heading_class_val = []\nepoch_losses_BboxNet_heading_regr_val = []\nepoch_accuracies_heading_class_val = []\nepoch_losses_corner_val = []\nepoch_accuracies_val = []\nepoch_precisions_val = []\nepoch_recalls_val = []\nepoch_f1s_val = []\nfor epoch in range(num_epochs):\n    print (""###########################"")\n    print (""######## NEW EPOCH ########"")\n    print (""###########################"")\n    print (""epoch: %d/%d"" % (epoch+1, num_epochs))\n\n    if epoch % 100 == 0 and epoch > 0:\n        learning_rate = learning_rate/2\n        optimizer = torch.optim.Adam(network.parameters(), lr=learning_rate)\n\n    print (""learning_rate:"")\n    print (learning_rate)\n\n    ################################################################################\n    # train:\n    ################################################################################\n    network.train() # (set in training mode, this affects BatchNorm and dropout)\n    batch_losses = []\n    batch_losses_InstanceSeg = []\n    batch_losses_TNet = []\n    batch_losses_BboxNet = []\n    batch_losses_BboxNet_center = []\n    batch_losses_BboxNet_size = []\n    batch_losses_BboxNet_heading_regr = []\n    batch_losses_BboxNet_heading_class = []\n    batch_losses_corner = []\n    batch_accuracies = []\n    batch_precisions = []\n    batch_recalls = []\n    batch_f1s = []\n    batch_accuracies_heading_class = []\n    for step, (frustum_point_clouds, bbox_2d_imgs, labels_InstanceSeg, labels_TNet, labels_BboxNet, labels_corner, labels_corner_flipped) in enumerate(train_loader):\n        frustum_point_clouds = Variable(frustum_point_clouds) # (shape: (batch_size, num_points, 4))\n        bbox_2d_imgs = Variable(bbox_2d_imgs) # (shape: (batch_size, 3, H, W))\n        labels_InstanceSeg = Variable(labels_InstanceSeg) # (shape: (batch_size, num_points))\n        labels_TNet = Variable(labels_TNet) # (shape: (batch_size, 3))\n        labels_BboxNet = Variable(labels_BboxNet) # (shape:(batch_size, 11))\n        labels_corner = Variable(labels_corner) # (shape: (batch_size, 8, 3))\n        labels_corner_flipped = Variable(labels_corner_flipped) # (shape: (batch_size, 8, 3))\n\n        frustum_point_clouds = frustum_point_clouds.transpose(2, 1) # (shape: (batch_size, 4, num_points))\n\n        frustum_point_clouds = frustum_point_clouds.cuda()\n        bbox_2d_imgs = bbox_2d_imgs.cuda()\n        labels_InstanceSeg = labels_InstanceSeg.cuda()\n        labels_TNet = labels_TNet.cuda()\n        labels_BboxNet = labels_BboxNet.cuda()\n        labels_corner = labels_corner.cuda()\n        labels_corner_flipped = labels_corner_flipped.cuda()\n\n        outputs = network(frustum_point_clouds, bbox_2d_imgs)\n        outputs_InstanceSeg = outputs[0] # (shape: (batch_size, num_points, 2))\n        outputs_TNet = outputs[1] # (shape: (batch_size, 3))\n        outputs_BboxNet = outputs[2] # (shape: (batch_size, 3 + 3 + 2*NH))\n        seg_point_clouds_mean = outputs[3] # (shape: (batch_size, 3))\n        dont_care_mask = outputs[4] # (shape: (batch_size, ))\n\n        ########################################################################\n        # compute precision, recall etc. for the InstanceSeg:\n        ########################################################################\n        preds = outputs_InstanceSeg.data.cpu().numpy() # (shape: (batch_size, num_points, 2))\n        preds = np.argmax(preds, 2) # (shape: (batch_size, num_points))\n\n        labels_InstanceSeg_np = labels_InstanceSeg.data.cpu().numpy() # (shape: (batch_size, num_points))\n\n        accuracy = np.count_nonzero(preds == labels_InstanceSeg_np)/(preds.shape[0]*preds.shape[1])\n        if np.count_nonzero(preds == 1) > 0:\n            precision = np.count_nonzero(np.logical_and(preds == labels_InstanceSeg_np, preds == 1))/np.count_nonzero(preds == 1) # (TP/(TP + FP))\n        else:\n            precision = -1\n        if np.count_nonzero(labels_InstanceSeg_np == 1) > 0:\n            recall = np.count_nonzero(np.logical_and(preds == labels_InstanceSeg_np, preds == 1))/np.count_nonzero(labels_InstanceSeg_np == 1) # (TP/(TP + FN))\n        else:\n            recall = -1\n        if recall + precision > 0:\n            f1 = 2*recall*precision/(recall + precision)\n        else:\n            f1 = -1\n\n        batch_accuracies.append(accuracy)\n        if precision != -1:\n            batch_precisions.append(precision)\n        if recall != -1:\n            batch_recalls.append(recall)\n        if f1 != -1:\n            batch_f1s.append(f1)\n\n        ########################################################################\n        # compute accuracy for the heading classification:\n        ########################################################################\n        pred_bin_scores = outputs_BboxNet[:, 6:(6+NH)].data.cpu().numpy() # (shape: (batch_size, NH))\n        pred_bin_numbers = np.argmax(pred_bin_scores, 1) # (shape: (batch_size, ))\n        gt_bin_numbers = labels_BboxNet[:, 6].data.cpu().numpy() # (shape: (batch_size, ))\n\n        accuracy_heading_class = np.count_nonzero(pred_bin_numbers == gt_bin_numbers)/(pred_bin_numbers.shape[0])\n        batch_accuracies_heading_class.append(accuracy_heading_class)\n\n        ########################################################################\n        # compute the InstanceSeg loss:\n        ########################################################################\n        outputs_InstanceSeg = outputs_InstanceSeg.view(-1, 2) # (shape (batch_size*num_points, 2))\n        labels_InstanceSeg = labels_InstanceSeg.view(-1, 1) # (shape: (batch_size*num_points, 1))\n        labels_InstanceSeg = labels_InstanceSeg[:, 0] # (shape: (batch_size*num_points, ))\n        loss_InstanceSeg = F.nll_loss(outputs_InstanceSeg, labels_InstanceSeg)\n        loss_InstanceSeg_value = loss_InstanceSeg.data.cpu().numpy()\n        batch_losses_InstanceSeg.append(loss_InstanceSeg_value)\n\n        ########################################################################\n        # compute the TNet loss:\n        ########################################################################\n        # mask entries corresponding to empty seg point clouds (select only the entries which we care about):\n        outputs_TNet = outputs_TNet[dont_care_mask, :] # (shape: (batch_size*, 3))\n        labels_TNet = labels_TNet[dont_care_mask, :] # (shape: (batch_size*, 3))\n        seg_point_clouds_mean = seg_point_clouds_mean[dont_care_mask, :] # (shape: (batch_size*, 3))\n\n        # shift the GT to the seg point clouds local coords:\n        labels_TNet = labels_TNet - seg_point_clouds_mean\n\n        # compute the Huber (smooth L1) loss:\n        if outputs_TNet.size()[0] == 0:\n            loss_TNet = Variable(torch.from_numpy(np.zeros((1, ), dtype=np.float32))).cuda()\n        else:\n            loss_TNet = regression_loss_func(outputs_TNet, labels_TNet)\n\n        loss_TNet_value = loss_TNet.data.cpu().numpy()\n        batch_losses_TNet.append(loss_TNet_value)\n\n        ########################################################################\n        # compute the BboxNet loss:\n        ########################################################################\n        # mask entries corresponding to empty seg point clouds (select only the entries which we care about):\n        outputs_BboxNet = outputs_BboxNet[dont_care_mask, :] # (shape: (batch_size*, 3 + 3 + 2*NH))\n        labels_BboxNet = labels_BboxNet[dont_care_mask, :]# (shape: (batch_size*, 11))\n\n        if outputs_BboxNet.size()[0] == 0:\n            loss_BboxNet = Variable(torch.from_numpy(np.zeros((1, ), dtype=np.float32))).cuda()\n            loss_BboxNet_size = Variable(torch.from_numpy(np.zeros((1, ), dtype=np.float32))).cuda()\n            loss_BboxNet_center = Variable(torch.from_numpy(np.zeros((1, ), dtype=np.float32))).cuda()\n            loss_BboxNet_heading_class = Variable(torch.from_numpy(np.zeros((1, ), dtype=np.float32))).cuda()\n            loss_BboxNet_heading_regr = Variable(torch.from_numpy(np.zeros((1, ), dtype=np.float32))).cuda()\n        else:\n            # compute the BboxNet center loss:\n            labels_BboxNet_center = labels_BboxNet[:, 0:3] # (shape: (batch_size*, 3))\n            # # shift the center GT to local coords:\n            labels_BboxNet_center = Variable(labels_BboxNet_center.data - seg_point_clouds_mean.data - outputs_TNet.data).cuda() # (outputs_TNet is a variable outputted by model, so it requires grads, which cant be passed as target to the loss function)\n            # # compute the Huber (smooth L1) loss:\n            outputs_BboxNet_center = outputs_BboxNet[:, 0:3]\n            loss_BboxNet_center = regression_loss_func(outputs_BboxNet_center, labels_BboxNet_center)\n\n            # compute the BboxNet size loss:\n            labels_BboxNet_size = labels_BboxNet[:, 3:6] # (shape: (batch_size*, 3))\n            # # subtract the mean car size in train:\n            labels_BboxNet_size = labels_BboxNet_size - labels_BboxNet[:, 8:]\n            # # compute the Huber (smooth L1) loss:\n            loss_BboxNet_size = regression_loss_func(outputs_BboxNet[:, 3:6], labels_BboxNet_size)\n\n            # compute the BboxNet heading loss\n            # # compute the classification loss:\n            labels_BboxNet_heading_class = Variable(labels_BboxNet[:, 6].data.type(torch.LongTensor)).cuda() # (shape: (batch_size*, ))\n            outputs_BboxNet_heading_class = outputs_BboxNet[:, 6:(6+NH)] # (shape: (batch_size*, NH))\n            loss_BboxNet_heading_class = F.nll_loss(F.log_softmax(outputs_BboxNet_heading_class, dim=1), labels_BboxNet_heading_class)\n            # # compute the regression loss:\n            # # # # get the GT residual for the GT bin:\n            labels_BboxNet_heading_regr = labels_BboxNet[:, 7] # (shape: (batch_size*, ))\n            # # # # get the pred residual for all bins:\n            outputs_BboxNet_heading_regr_all = outputs_BboxNet[:, (6+NH):] # (shape: (batch_size*, 8))\n            # # # # get the pred residual for the GT bin:\n            outputs_BboxNet_heading_regr = outputs_BboxNet_heading_regr_all.gather(1, labels_BboxNet_heading_class.view(-1, 1)) # (shape: (batch_size*, 1))\n            outputs_BboxNet_heading_regr = outputs_BboxNet_heading_regr[:, 0] # (shape: (batch_size*, )\n            # # # # compute the loss:\n            loss_BboxNet_heading_regr = regression_loss_func(outputs_BboxNet_heading_regr, labels_BboxNet_heading_regr)\n            # # compute the total BBoxNet heading loss:\n            loss_BboxNet_heading = loss_BboxNet_heading_class + 10*loss_BboxNet_heading_regr\n\n            # compute the BboxNet total loss:\n            loss_BboxNet = loss_BboxNet_center + loss_BboxNet_size + loss_BboxNet_heading\n\n        loss_BboxNet_value = loss_BboxNet.data.cpu().numpy()\n        batch_losses_BboxNet.append(loss_BboxNet_value)\n\n        loss_BboxNet_size_value = loss_BboxNet_size.data.cpu().numpy()\n        batch_losses_BboxNet_size.append(loss_BboxNet_size_value)\n\n        loss_BboxNet_center_value = loss_BboxNet_center.data.cpu().numpy()\n        batch_losses_BboxNet_center.append(loss_BboxNet_center_value)\n\n        loss_BboxNet_heading_class_value = loss_BboxNet_heading_class.data.cpu().numpy()\n        batch_losses_BboxNet_heading_class.append(loss_BboxNet_heading_class_value)\n\n        loss_BboxNet_heading_regr_value = loss_BboxNet_heading_regr.data.cpu().numpy()\n        batch_losses_BboxNet_heading_regr.append(loss_BboxNet_heading_regr_value)\n\n        ########################################################################\n        # compute the corner loss:\n        ########################################################################\n        # mask entries corresponding to empty seg point clouds (select only the entries which we care about):\n        labels_corner = labels_corner[dont_care_mask]# (shape: (batch_size*, 8, 3))\n        labels_corner_flipped = labels_corner_flipped[dont_care_mask]# (shape: (batch_size*, 8, 3))\n\n        if outputs_BboxNet.size()[0] == 0:\n            loss_corner = Variable(torch.from_numpy(np.zeros((1, ), dtype=np.float32))).cuda()\n        else:\n            outputs_BboxNet_center = outputs_BboxNet[:, 0:3] # (shape: (batch_size*, 3))\n            # shift to the same coords used in the labels for the corner loss:\n            pred_center = outputs_BboxNet_center + seg_point_clouds_mean + outputs_TNet # (shape: (batch_size*, 3))\n            pred_center_unsqeezed = pred_center.unsqueeze(2) # (shape: (batch_size, 3, 1))\n\n            # shift the outputted size to the same ""coords"" used in the labels for the corner loss:\n            outputs_BboxNet_size = outputs_BboxNet[:, 3:6] + labels_BboxNet[:, 8:] # (shape: (batch_size*, 3))\n\n            pred_h = outputs_BboxNet_size[:, 0] # (shape: (batch_size*, ))\n            pred_w = outputs_BboxNet_size[:, 1] # (shape: (batch_size*, ))\n            pred_l = outputs_BboxNet_size[:, 2] # (shape: (batch_size*, ))\n\n            # get the pred residuals for the GT bins:\n            pred_residuals = outputs_BboxNet_heading_regr # (shape: (batch_size*, ))\n\n            Rmat = Variable(torch.zeros(pred_h.size()[0], 3, 3), requires_grad=True).cuda() # (shape: (batch_size*, 3, 3))\n            Rmat[:, 0, 0] = torch.cos(pred_residuals)\n            Rmat[:, 0, 2] = torch.sin(pred_residuals)\n            Rmat[:, 1, 1] = 1\n            Rmat[:, 2, 0] = -torch.sin(pred_residuals)\n            Rmat[:, 2, 2] = torch.cos(pred_residuals)\n\n            p0_orig = Variable(torch.zeros(pred_h.size()[0], 3, 1), requires_grad=True).cuda() # (shape: (batch_size*, 3, 1))\n            p0_orig[:, 0, 0] = pred_l/2.0\n            p0_orig[:, 2, 0] = pred_w/2.0\n\n            p1_orig = Variable(torch.zeros(pred_h.size()[0], 3, 1), requires_grad=True).cuda() # (shape: (batch_size*, 3, 1))\n            p1_orig[:, 0, 0] = -pred_l/2.0\n            p1_orig[:, 2, 0] = pred_w/2.0\n\n            p2_orig = Variable(torch.zeros(pred_h.size()[0], 3, 1), requires_grad=True).cuda() # (shape: (batch_size*, 3, 1))\n            p2_orig[:, 0, 0] = -pred_l/2.0\n            p2_orig[:, 2, 0] = -pred_w/2.0\n\n            p3_orig = Variable(torch.zeros(pred_h.size()[0], 3, 1), requires_grad=True).cuda() # (shape: (batch_size*, 3, 1))\n            p3_orig[:, 0, 0] = pred_l/2.0\n            p3_orig[:, 2, 0] = -pred_w/2.0\n\n            p4_orig = Variable(torch.zeros(pred_h.size()[0], 3, 1), requires_grad=True).cuda() # (shape: (batch_size*, 3, 1))\n            p4_orig[:, 0, 0] = pred_l/2.0\n            p4_orig[:, 1, 0] = -pred_h\n            p4_orig[:, 2, 0] = pred_w/2.0\n\n            p5_orig = Variable(torch.zeros(pred_h.size()[0], 3, 1), requires_grad=True).cuda() # (shape: (batch_size*, 3, 1))\n            p5_orig[:, 0, 0] = -pred_l/2.0\n            p5_orig[:, 1, 0] = -pred_h\n            p5_orig[:, 2, 0] = pred_w/2.0\n\n            p6_orig = Variable(torch.zeros(pred_h.size()[0], 3, 1), requires_grad=True).cuda() # (shape: (batch_size*, 3, 1))\n            p6_orig[:, 0, 0] = -pred_l/2.0\n            p6_orig[:, 1, 0] = -pred_h\n            p6_orig[:, 2, 0] = -pred_w/2.0\n\n            p7_orig = Variable(torch.zeros(pred_h.size()[0], 3, 1), requires_grad=True).cuda() # (shape: (batch_size*, 3, 1))\n            p7_orig[:, 0, 0] = pred_l/2.0\n            p7_orig[:, 1, 0] = -pred_h\n            p7_orig[:, 2, 0] = -pred_w/2.0\n\n            pred_p0_unsqeezed = pred_center_unsqeezed + torch.bmm(Rmat, p0_orig) # (shape: (batch_size*, 3, 1))\n            pred_p0 = pred_p0_unsqeezed[:, :, 0] # (shape: (batch_size*, 3))\n            pred_p1_unsqeezed = pred_center_unsqeezed + torch.bmm(Rmat, p1_orig) # (shape: (batch_size*, 3, 1))\n            pred_p1 = pred_p1_unsqeezed[:, :, 0] # (shape: (batch_size*, 3))\n            pred_p2_unsqeezed = pred_center_unsqeezed + torch.bmm(Rmat, p2_orig) # (shape: (batch_size*, 3, 1))\n            pred_p2 = pred_p2_unsqeezed[:, :, 0] # (shape: (batch_size*, 3))\n            pred_p3_unsqeezed = pred_center_unsqeezed + torch.bmm(Rmat, p3_orig) # (shape: (batch_size*, 3, 1))\n            pred_p3 = pred_p3_unsqeezed[:, :, 0] # (shape: (batch_size*, 3))\n            pred_p4_unsqeezed = pred_center_unsqeezed + torch.bmm(Rmat, p4_orig) # (shape: (batch_size*, 3, 1))\n            pred_p4 = pred_p4_unsqeezed[:, :, 0] # (shape: (batch_size*, 3))\n            pred_p5_unsqeezed = pred_center_unsqeezed + torch.bmm(Rmat, p5_orig) # (shape: (batch_size*, 3, 1))\n            pred_p5 = pred_p5_unsqeezed[:, :, 0] # (shape: (batch_size*, 3))\n            pred_p6_unsqeezed = pred_center_unsqeezed + torch.bmm(Rmat, p6_orig) # (shape: (batch_size*, 3, 1))\n            pred_p6 = pred_p6_unsqeezed[:, :, 0] # (shape: (batch_size*, 3))\n            pred_p7_unsqeezed = pred_center_unsqeezed + torch.bmm(Rmat, p7_orig) # (shape: (batch_size*, 3, 1))\n            pred_p7 = pred_p7_unsqeezed[:, :, 0] # (shape: (batch_size*, 3))\n\n            outputs_corner = Variable(torch.zeros(pred_h.size()[0], 8, 3), requires_grad=True).cuda() # (shape: (batch_size*, 8, 3))\n            outputs_corner[:, 0] = pred_p0\n            outputs_corner[:, 1] = pred_p1\n            outputs_corner[:, 2] = pred_p2\n            outputs_corner[:, 3] = pred_p3\n            outputs_corner[:, 4] = pred_p4\n            outputs_corner[:, 5] = pred_p5\n            outputs_corner[:, 6] = pred_p6\n            outputs_corner[:, 7] = pred_p7\n\n            loss_corner_unflipped = regression_loss_func(outputs_corner, labels_corner)\n            loss_corner_flipped = regression_loss_func(outputs_corner, labels_corner_flipped)\n\n            loss_corner = torch.min(loss_corner_unflipped, loss_corner_flipped)\n\n        loss_corner_value = loss_corner.data.cpu().numpy()\n        batch_losses_corner.append(loss_corner_value)\n\n        ########################################################################\n        # compute the total loss:\n        ########################################################################\n        lambda_value = 1\n        gamma_value = 10\n        loss = loss_InstanceSeg + lambda_value*(loss_TNet + loss_BboxNet + gamma_value*loss_corner)\n        loss_value = loss.data.cpu().numpy()\n        batch_losses.append(loss_value)\n\n        ########################################################################\n        # optimization step:\n        ########################################################################\n        optimizer.zero_grad() # (reset gradients)\n        loss.backward() # (compute gradients)\n        optimizer.step() # (perform optimization step)\n\n    # compute the train epoch loss:\n    epoch_loss = np.mean(batch_losses)\n    # save the train epoch loss:\n    epoch_losses_train.append(epoch_loss)\n    # save the train epoch loss to disk:\n    with open(""%s/epoch_losses_train.pkl"" % network.model_dir, ""wb"") as file:\n        pickle.dump(epoch_losses_train, file)\n    print (""training loss: %g"" % epoch_loss)\n    # plot the training loss vs epoch and save to disk:\n    plt.figure(1)\n    plt.plot(epoch_losses_train, ""k^"")\n    plt.plot(epoch_losses_train, ""k"")\n    plt.ylabel(""loss"")\n    plt.xlabel(""epoch"")\n    plt.title(""training loss per epoch"")\n    plt.savefig(""%s/epoch_losses_train.png"" % network.model_dir)\n    plt.close(1)\n\n    # compute the train epoch TNet loss:\n    epoch_loss = np.mean(batch_losses_TNet)\n    # save the train epoch loss:\n    epoch_losses_TNet_train.append(epoch_loss)\n    # save the train epoch TNet loss to disk:\n    with open(""%s/epoch_losses_TNet_train.pkl"" % network.model_dir, ""wb"") as file:\n        pickle.dump(epoch_losses_TNet_train, file)\n    print (""training TNet loss: %g"" % epoch_loss)\n    # plot the training TNet loss vs epoch and save to disk:\n    plt.figure(1)\n    plt.plot(epoch_losses_TNet_train, ""k^"")\n    plt.plot(epoch_losses_TNet_train, ""k"")\n    plt.ylabel(""loss"")\n    plt.xlabel(""epoch"")\n    plt.title(""training TNet loss per epoch"")\n    plt.savefig(""%s/epoch_losses_TNet_train.png"" % network.model_dir)\n    plt.close(1)\n\n    # compute the train epoch InstanceSeg loss:\n    epoch_loss = np.mean(batch_losses_InstanceSeg)\n    # save the train epoch loss:\n    epoch_losses_InstanceSeg_train.append(epoch_loss)\n    # save the train epoch InstanceSeg loss to disk:\n    with open(""%s/epoch_losses_InstanceSeg_train.pkl"" % network.model_dir, ""wb"") as file:\n        pickle.dump(epoch_losses_InstanceSeg_train, file)\n    print (""training InstanceSeg loss: %g"" % epoch_loss)\n    # plot the training InstanceSeg loss vs epoch and save to disk:\n    plt.figure(1)\n    plt.plot(epoch_losses_InstanceSeg_train, ""k^"")\n    plt.plot(epoch_losses_InstanceSeg_train, ""k"")\n    plt.ylabel(""loss"")\n    plt.xlabel(""epoch"")\n    plt.title(""training InstanceSeg loss per epoch"")\n    plt.savefig(""%s/epoch_losses_InstanceSeg_train.png"" % network.model_dir)\n    plt.close(1)\n\n    # compute the train epoch BboxNet loss:\n    epoch_loss = np.mean(batch_losses_BboxNet)\n    # save the train epoch loss:\n    epoch_losses_BboxNet_train.append(epoch_loss)\n    # save the train epoch BboxNet loss to disk:\n    with open(""%s/epoch_losses_BboxNet_train.pkl"" % network.model_dir, ""wb"") as file:\n        pickle.dump(epoch_losses_BboxNet_train, file)\n    print (""training BboxNet loss: %g"" % epoch_loss)\n    # plot the training BboxNet loss vs epoch and save to disk:\n    plt.figure(1)\n    plt.plot(epoch_losses_BboxNet_train, ""k^"")\n    plt.plot(epoch_losses_BboxNet_train, ""k"")\n    plt.ylabel(""loss"")\n    plt.xlabel(""epoch"")\n    plt.title(""training BboxNet loss per epoch"")\n    plt.savefig(""%s/epoch_losses_BboxNet_train.png"" % network.model_dir)\n    plt.close(1)\n\n    # compute the train epoch BboxNet size loss:\n    epoch_loss = np.mean(batch_losses_BboxNet_size)\n    # save the train epoch loss:\n    epoch_losses_BboxNet_size_train.append(epoch_loss)\n    # save the train epoch BboxNet size loss to disk:\n    with open(""%s/epoch_losses_BboxNet_size_train.pkl"" % network.model_dir, ""wb"") as file:\n        pickle.dump(epoch_losses_BboxNet_size_train, file)\n    print (""training BboxNet size loss: %g"" % epoch_loss)\n    # plot the training BboxNet size loss vs epoch and save to disk:\n    plt.figure(1)\n    plt.plot(epoch_losses_BboxNet_size_train, ""k^"")\n    plt.plot(epoch_losses_BboxNet_size_train, ""k"")\n    plt.ylabel(""loss"")\n    plt.xlabel(""epoch"")\n    plt.title(""training BboxNet size loss per epoch"")\n    plt.savefig(""%s/epoch_losses_BboxNet_size_train.png"" % network.model_dir)\n    plt.close(1)\n\n    # compute the train epoch BboxNet center loss:\n    epoch_loss = np.mean(batch_losses_BboxNet_center)\n    # save the train epoch loss:\n    epoch_losses_BboxNet_center_train.append(epoch_loss)\n    # save the train epoch BboxNet center loss to disk:\n    with open(""%s/epoch_losses_BboxNet_center_train.pkl"" % network.model_dir, ""wb"") as file:\n        pickle.dump(epoch_losses_BboxNet_center_train, file)\n    print (""training BboxNet center loss: %g"" % epoch_loss)\n    # plot the training BboxNet center loss vs epoch and save to disk:\n    plt.figure(1)\n    plt.plot(epoch_losses_BboxNet_center_train, ""k^"")\n    plt.plot(epoch_losses_BboxNet_center_train, ""k"")\n    plt.ylabel(""loss"")\n    plt.xlabel(""epoch"")\n    plt.title(""training BboxNet center loss per epoch"")\n    plt.savefig(""%s/epoch_losses_BboxNet_center_train.png"" % network.model_dir)\n    plt.close(1)\n\n    # compute the train epoch BboxNet heading class loss:\n    epoch_loss = np.mean(batch_losses_BboxNet_heading_class)\n    # save the train epoch loss:\n    epoch_losses_BboxNet_heading_class_train.append(epoch_loss)\n    # save the train epoch BboxNet heading class loss to disk:\n    with open(""%s/epoch_losses_BboxNet_heading_class_train.pkl"" % network.model_dir, ""wb"") as file:\n        pickle.dump(epoch_losses_BboxNet_heading_class_train, file)\n    print (""training BboxNet heading class loss: %g"" % epoch_loss)\n    # plot the training BboxNet heading class loss vs epoch and save to disk:\n    plt.figure(1)\n    plt.plot(epoch_losses_BboxNet_heading_class_train, ""k^"")\n    plt.plot(epoch_losses_BboxNet_heading_class_train, ""k"")\n    plt.ylabel(""loss"")\n    plt.xlabel(""epoch"")\n    plt.title(""training BboxNet heading class loss per epoch"")\n    plt.savefig(""%s/epoch_losses_BboxNet_heading_class_train.png"" % network.model_dir)\n    plt.close(1)\n\n    # compute the train epoch BboxNet heading regr loss:\n    epoch_loss = np.mean(batch_losses_BboxNet_heading_regr)\n    # save the train epoch loss:\n    epoch_losses_BboxNet_heading_regr_train.append(epoch_loss)\n    # save the train epoch BboxNet heading regr loss to disk:\n    with open(""%s/epoch_losses_BboxNet_heading_regr_train.pkl"" % network.model_dir, ""wb"") as file:\n        pickle.dump(epoch_losses_BboxNet_heading_regr_train, file)\n    print (""training BboxNet heading regr loss: %g"" % epoch_loss)\n    # plot the training BboxNet heading regr loss vs epoch and save to disk:\n    plt.figure(1)\n    plt.plot(epoch_losses_BboxNet_heading_regr_train, ""k^"")\n    plt.plot(epoch_losses_BboxNet_heading_regr_train, ""k"")\n    plt.ylabel(""loss"")\n    plt.xlabel(""epoch"")\n    plt.title(""training BboxNet heading regr loss per epoch"")\n    plt.savefig(""%s/epoch_losses_BboxNet_heading_regr_train.png"" % network.model_dir)\n    plt.close(1)\n\n    # compute the train epoch heading class accuracy:\n    epoch_accuracy = np.mean(batch_accuracies_heading_class)\n    # save the train epoch heading class accuracy:\n    epoch_accuracies_heading_class_train.append(epoch_accuracy)\n    # save the train epoch heading class accuracy to disk:\n    with open(""%s/epoch_accuracies_heading_class_train.pkl"" % network.model_dir, ""wb"") as file:\n        pickle.dump(epoch_accuracies_heading_class_train, file)\n    print (""training heading class accuracy: %g"" % epoch_accuracy)\n    # plot the training heading class accuracy vs epoch and save to disk:\n    plt.figure(1)\n    plt.plot(epoch_accuracies_heading_class_train, ""k^"")\n    plt.plot(epoch_accuracies_heading_class_train, ""k"")\n    plt.ylabel(""accuracy"")\n    plt.xlabel(""epoch"")\n    plt.title(""training heading class accuracy per epoch"")\n    plt.savefig(""%s/epoch_accuracies_heading_class_train.png"" % network.model_dir)\n    plt.close(1)\n\n    # compute the train epoch corner loss:\n    epoch_loss = np.mean(batch_losses_corner)\n    # save the train epoch corner loss:\n    epoch_losses_corner_train.append(epoch_loss)\n    # save the train epoch corner loss to disk:\n    with open(""%s/epoch_losses_corner_train.pkl"" % network.model_dir, ""wb"") as file:\n        pickle.dump(epoch_losses_corner_train, file)\n    print (""training corner loss: %g"" % epoch_loss)\n    # plot the training corner loss vs epoch and save to disk:\n    plt.figure(1)\n    plt.plot(epoch_losses_corner_train, ""k^"")\n    plt.plot(epoch_losses_corner_train, ""k"")\n    plt.ylabel(""loss"")\n    plt.xlabel(""epoch"")\n    plt.title(""training corner loss per epoch"")\n    plt.savefig(""%s/epoch_losses_corner_train.png"" % network.model_dir)\n    plt.close(1)\n\n    # compute the train epoch accuracy:\n    epoch_accuracy = np.mean(batch_accuracies)\n    # save the train epoch accuracy:\n    epoch_accuracies_train.append(epoch_accuracy)\n    # save the train epoch accuracy to disk:\n    with open(""%s/epoch_accuracies_train.pkl"" % network.model_dir, ""wb"") as file:\n        pickle.dump(epoch_accuracies_train, file)\n    print (""training accuracy: %g"" % epoch_accuracy)\n    # plot the training accuracy vs epoch and save to disk:\n    plt.figure(1)\n    plt.plot(epoch_accuracies_train, ""k^"")\n    plt.plot(epoch_accuracies_train, ""k"")\n    plt.ylabel(""accuracy"")\n    plt.xlabel(""epoch"")\n    plt.title(""training accuracy per epoch"")\n    plt.savefig(""%s/epoch_accuracies_train.png"" % network.model_dir)\n    plt.close(1)\n\n    # compute the train epoch precision:\n    epoch_precision = np.mean(batch_precisions)\n    # save the train epoch precision:\n    epoch_precisions_train.append(epoch_precision)\n    # save the train epoch precision to disk:\n    with open(""%s/epoch_precisions_train.pkl"" % network.model_dir, ""wb"") as file:\n        pickle.dump(epoch_precisions_train, file)\n    print (""training precision: %g"" % epoch_precision)\n    # plot the training precision vs epoch and save to disk:\n    plt.figure(1)\n    plt.plot(epoch_precisions_train, ""k^"")\n    plt.plot(epoch_precisions_train, ""k"")\n    plt.ylabel(""precision"")\n    plt.xlabel(""epoch"")\n    plt.title(""training precision per epoch"")\n    plt.savefig(""%s/epoch_precisions_train.png"" % network.model_dir)\n    plt.close(1)\n\n    # compute the train epoch recall:\n    epoch_recall = np.mean(batch_recalls)\n    # save the train epoch recall:\n    epoch_recalls_train.append(epoch_recall)\n    # save the train epoch recall to disk:\n    with open(""%s/epoch_recalls_train.pkl"" % network.model_dir, ""wb"") as file:\n        pickle.dump(epoch_recalls_train, file)\n    print (""training recall: %g"" % epoch_recall)\n    # plot the training recall vs epoch and save to disk:\n    plt.figure(1)\n    plt.plot(epoch_recalls_train, ""k^"")\n    plt.plot(epoch_recalls_train, ""k"")\n    plt.ylabel(""recall"")\n    plt.xlabel(""epoch"")\n    plt.title(""training recall per epoch"")\n    plt.savefig(""%s/epoch_recalls_train.png"" % network.model_dir)\n    plt.close(1)\n\n    # compute the train epoch f1:\n    epoch_f1 = np.mean(batch_f1s)\n    # save the train epoch f1:\n    epoch_f1s_train.append(epoch_f1)\n    # save the train epoch f1 to disk:\n    with open(""%s/epoch_f1s_train.pkl"" % network.model_dir, ""wb"") as file:\n        pickle.dump(epoch_f1s_train, file)\n    print (""training f1: %g"" % epoch_f1)\n    # plot the training f1 vs epoch and save to disk:\n    plt.figure(1)\n    plt.plot(epoch_f1s_train, ""k^"")\n    plt.plot(epoch_f1s_train, ""k"")\n    plt.ylabel(""f1"")\n    plt.xlabel(""epoch"")\n    plt.title(""training f1 per epoch"")\n    plt.savefig(""%s/epoch_f1s_train.png"" % network.model_dir)\n    plt.close(1)\n\n    print (""####"")\n\n    ################################################################################\n    # val:\n    ################################################################################\n    network.eval() # (set in evaluation mode, this affects BatchNorm and dropout)\n    batch_losses = []\n    batch_losses_InstanceSeg = []\n    batch_losses_TNet = []\n    batch_losses_BboxNet = []\n    batch_losses_BboxNet_center = []\n    batch_losses_BboxNet_size = []\n    batch_losses_BboxNet_heading_regr = []\n    batch_losses_BboxNet_heading_class = []\n    batch_losses_corner = []\n    batch_accuracies = []\n    batch_precisions = []\n    batch_recalls = []\n    batch_f1s = []\n    batch_accuracies_heading_class = []\n    for step, (frustum_point_clouds, bbox_2d_imgs, labels_InstanceSeg, labels_TNet, labels_BboxNet, labels_corner, labels_corner_flipped, img_ids, input_2Dbboxes, frustum_Rs, frustum_angles, centered_frustum_mean_xyz) in enumerate(val_loader):\n        with torch.no_grad(): # (corresponds to setting volatile=True in all variables, this is done during inference to reduce memory consumption)\n            frustum_point_clouds = Variable(frustum_point_clouds) # (shape: (batch_size, num_points, 4))\n            bbox_2d_imgs = Variable(bbox_2d_imgs) # (shape: (batch_size, 3, H, W))\n            labels_InstanceSeg = Variable(labels_InstanceSeg) # (shape: (batch_size, num_points))\n            labels_TNet = Variable(labels_TNet) # (shape: (batch_size, 3))\n            labels_BboxNet = Variable(labels_BboxNet) # (shape:(batch_size, 11))\n            labels_corner = Variable(labels_corner) # (shape: (batch_size, 8, 3))\n            labels_corner_flipped = Variable(labels_corner_flipped) # (shape: (batch_size, 8, 3))\n\n            frustum_point_clouds = frustum_point_clouds.transpose(2, 1) # (shape: (batch_size, 4, num_points))\n\n            frustum_point_clouds = frustum_point_clouds.cuda()\n            bbox_2d_imgs = bbox_2d_imgs.cuda()\n            labels_InstanceSeg = labels_InstanceSeg.cuda()\n            labels_TNet = labels_TNet.cuda()\n            labels_BboxNet = labels_BboxNet.cuda()\n            labels_corner = labels_corner.cuda()\n            labels_corner_flipped = labels_corner_flipped.cuda()\n\n            outputs = network(frustum_point_clouds, bbox_2d_imgs)\n            outputs_InstanceSeg = outputs[0] # (shape: (batch_size, num_points, 2))\n            outputs_TNet = outputs[1] # (shape: (batch_size, 3))\n            outputs_BboxNet = outputs[2] # (shape: (batch_size, 3 + 3 + 2*NH))\n            seg_point_clouds_mean = outputs[3] # (shape: (batch_size, 3))\n            dont_care_mask = outputs[4] # (shape: (batch_size, ))\n\n            ########################################################################\n            # compute precision, recall etc. for the InstanceSeg:\n            ########################################################################\n            preds = outputs_InstanceSeg.data.cpu().numpy() # (shape: (batch_size, num_points, 2))\n            preds = np.argmax(preds, 2) # (shape: (batch_size, num_points))\n\n            labels_InstanceSeg_np = labels_InstanceSeg.data.cpu().numpy() # (shape: (batch_size, num_points))\n\n            accuracy = np.count_nonzero(preds == labels_InstanceSeg_np)/(preds.shape[0]*preds.shape[1])\n            if np.count_nonzero(preds == 1) > 0:\n                precision = np.count_nonzero(np.logical_and(preds == labels_InstanceSeg_np, preds == 1))/np.count_nonzero(preds == 1) # (TP/(TP + FP))\n            else:\n                precision = -1\n            if np.count_nonzero(labels_InstanceSeg_np == 1) > 0:\n                recall = np.count_nonzero(np.logical_and(preds == labels_InstanceSeg_np, preds == 1))/np.count_nonzero(labels_InstanceSeg_np == 1) # (TP/(TP + FN))\n            else:\n                recall = -1\n            if recall + precision > 0:\n                f1 = 2*recall*precision/(recall + precision)\n            else:\n                f1 = -1\n\n            batch_accuracies.append(accuracy)\n            if precision != -1:\n                batch_precisions.append(precision)\n            if recall != -1:\n                batch_recalls.append(recall)\n            if f1 != -1:\n                batch_f1s.append(f1)\n\n            ########################################################################\n            # compute accuracy for the heading classification:\n            ########################################################################\n            pred_bin_scores = outputs_BboxNet[:, 6:(6+NH)].data.cpu().numpy() # (shape: (batch_size, NH))\n            pred_bin_numbers = np.argmax(pred_bin_scores, 1) # (shape: (batch_size, ))\n            gt_bin_numbers = labels_BboxNet[:, 6].data.cpu().numpy() # (shape: (batch_size, ))\n\n            accuracy_heading_class = np.count_nonzero(pred_bin_numbers == gt_bin_numbers)/(pred_bin_numbers.shape[0])\n            batch_accuracies_heading_class.append(accuracy_heading_class)\n\n            ########################################################################\n            # compute the InstanceSeg loss:\n            ########################################################################\n            outputs_InstanceSeg = outputs_InstanceSeg.view(-1, 2) # (shape (batch_size*num_points, 2))\n            labels_InstanceSeg = labels_InstanceSeg.view(-1, 1) # (shape: (batch_size*num_points, 1))\n            labels_InstanceSeg = labels_InstanceSeg[:, 0] # (shape: (batch_size*num_points, ))\n            loss_InstanceSeg = F.nll_loss(outputs_InstanceSeg, labels_InstanceSeg)\n            loss_InstanceSeg_value = loss_InstanceSeg.data.cpu().numpy()\n            batch_losses_InstanceSeg.append(loss_InstanceSeg_value)\n\n            ########################################################################\n            # compute the TNet loss:\n            ########################################################################\n            # mask entries corresponding to empty seg point clouds (select only the entries which we care about):\n            outputs_TNet = outputs_TNet[dont_care_mask, :] # (shape: (batch_size*, 3))\n            labels_TNet = labels_TNet[dont_care_mask, :] # (shape: (batch_size*, 3))\n            seg_point_clouds_mean = seg_point_clouds_mean[dont_care_mask, :] # (shape: (batch_size*, 3))\n\n            # shift the GT to the seg point clouds local coords:\n            labels_TNet = labels_TNet - seg_point_clouds_mean\n\n            # compute the Huber (smooth L1) loss:\n            if outputs_TNet.size()[0] == 0:\n                loss_TNet = Variable(torch.from_numpy(np.zeros((1, ), dtype=np.float32))).cuda()\n            else:\n                loss_TNet = regression_loss_func(outputs_TNet, labels_TNet)\n\n            loss_TNet_value = loss_TNet.data.cpu().numpy()\n            batch_losses_TNet.append(loss_TNet_value)\n\n            ########################################################################\n            # compute the BboxNet loss:\n            ########################################################################\n            # mask entries corresponding to empty seg point clouds (select only the entries which we care about):\n            outputs_BboxNet = outputs_BboxNet[dont_care_mask, :] # (shape: (batch_size*, 3 + 3 + 2*NH))\n            labels_BboxNet = labels_BboxNet[dont_care_mask, :]# (shape: (batch_size*, 11))\n\n            if outputs_BboxNet.size()[0] == 0:\n                loss_BboxNet = Variable(torch.from_numpy(np.zeros((1, ), dtype=np.float32))).cuda()\n                loss_BboxNet_size = Variable(torch.from_numpy(np.zeros((1, ), dtype=np.float32))).cuda()\n                loss_BboxNet_center = Variable(torch.from_numpy(np.zeros((1, ), dtype=np.float32))).cuda()\n                loss_BboxNet_heading_class = Variable(torch.from_numpy(np.zeros((1, ), dtype=np.float32))).cuda()\n                loss_BboxNet_heading_regr = Variable(torch.from_numpy(np.zeros((1, ), dtype=np.float32))).cuda()\n            else:\n                # compute the BboxNet center loss:\n                labels_BboxNet_center = labels_BboxNet[:, 0:3] # (shape: (batch_size*, 3))\n                # # shift the center GT to local coords:\n                labels_BboxNet_center = Variable(labels_BboxNet_center.data - seg_point_clouds_mean.data - outputs_TNet.data).cuda() # (outputs_TNet is a variable outputted by model, so it requires grads, which cant be passed as target to the loss function)\n                # # compute the Huber (smooth L1) loss:\n                outputs_BboxNet_center = outputs_BboxNet[:, 0:3]\n                loss_BboxNet_center = regression_loss_func(outputs_BboxNet_center, labels_BboxNet_center)\n\n                # compute the BboxNet size loss:\n                labels_BboxNet_size = labels_BboxNet[:, 3:6] # (shape: (batch_size*, 3))\n                # # subtract the mean car size in train:\n                labels_BboxNet_size = labels_BboxNet_size - labels_BboxNet[:, 8:]\n                # # compute the Huber (smooth L1) loss:\n                loss_BboxNet_size = regression_loss_func(outputs_BboxNet[:, 3:6], labels_BboxNet_size)\n\n                # compute the BboxNet heading loss\n                # # compute the classification loss:\n                labels_BboxNet_heading_class = Variable(labels_BboxNet[:, 6].data.type(torch.LongTensor)).cuda() # (shape: (batch_size*, ))\n                outputs_BboxNet_heading_class = outputs_BboxNet[:, 6:(6+NH)] # (shape: (batch_size*, NH))\n                loss_BboxNet_heading_class = F.nll_loss(F.log_softmax(outputs_BboxNet_heading_class, dim=1), labels_BboxNet_heading_class)\n                # # compute the regression loss:\n                # # # # get the GT residual for the GT bin:\n                labels_BboxNet_heading_regr = labels_BboxNet[:, 7] # (shape: (batch_size*, ))\n                # # # # get the pred residual for all bins:\n                outputs_BboxNet_heading_regr_all = outputs_BboxNet[:, (6+NH):] # (shape: (batch_size*, 8))\n                # # # # get the pred residual for the GT bin:\n                outputs_BboxNet_heading_regr = outputs_BboxNet_heading_regr_all.gather(1, labels_BboxNet_heading_class.view(-1, 1)) # (shape: (batch_size*, 1))\n                outputs_BboxNet_heading_regr = outputs_BboxNet_heading_regr[:, 0] # (shape: (batch_size*, )\n                # # # # compute the loss:\n                loss_BboxNet_heading_regr = regression_loss_func(outputs_BboxNet_heading_regr, labels_BboxNet_heading_regr)\n                # # compute the total BBoxNet heading loss:\n                loss_BboxNet_heading = loss_BboxNet_heading_class + 10*loss_BboxNet_heading_regr\n\n                # compute the BboxNet total loss:\n                loss_BboxNet = loss_BboxNet_center + loss_BboxNet_size + loss_BboxNet_heading\n\n            loss_BboxNet_value = loss_BboxNet.data.cpu().numpy()\n            batch_losses_BboxNet.append(loss_BboxNet_value)\n\n            loss_BboxNet_size_value = loss_BboxNet_size.data.cpu().numpy()\n            batch_losses_BboxNet_size.append(loss_BboxNet_size_value)\n\n            loss_BboxNet_center_value = loss_BboxNet_center.data.cpu().numpy()\n            batch_losses_BboxNet_center.append(loss_BboxNet_center_value)\n\n            loss_BboxNet_heading_class_value = loss_BboxNet_heading_class.data.cpu().numpy()\n            batch_losses_BboxNet_heading_class.append(loss_BboxNet_heading_class_value)\n\n            loss_BboxNet_heading_regr_value = loss_BboxNet_heading_regr.data.cpu().numpy()\n            batch_losses_BboxNet_heading_regr.append(loss_BboxNet_heading_regr_value)\n\n            ########################################################################\n            # compute the corner loss:\n            ########################################################################\n            # mask entries corresponding to empty seg point clouds (select only the entries which we care about):\n            labels_corner = labels_corner[dont_care_mask]# (shape: (batch_size*, 8, 3))\n            labels_corner_flipped = labels_corner_flipped[dont_care_mask]# (shape: (batch_size*, 8, 3))\n\n            if outputs_BboxNet.size()[0] == 0:\n                loss_corner = Variable(torch.from_numpy(np.zeros((1, ), dtype=np.float32))).cuda()\n            else:\n                outputs_BboxNet_center = outputs_BboxNet[:, 0:3] # (shape: (batch_size*, 3))\n                # shift to the same coords used in the labels for the corner loss:\n                pred_center = outputs_BboxNet_center + seg_point_clouds_mean + outputs_TNet # (shape: (batch_size*, 3))\n                pred_center_unsqeezed = pred_center.unsqueeze(2) # (shape: (batch_size, 3, 1))\n\n                # shift the outputted size to the same ""coords"" used in the labels for the corner loss:\n                outputs_BboxNet_size = outputs_BboxNet[:, 3:6] + labels_BboxNet[:, 8:] # (shape: (batch_size*, 3))\n\n                pred_h = outputs_BboxNet_size[:, 0] # (shape: (batch_size*, ))\n                pred_w = outputs_BboxNet_size[:, 1] # (shape: (batch_size*, ))\n                pred_l = outputs_BboxNet_size[:, 2] # (shape: (batch_size*, ))\n\n                # get the pred residuals for the GT bins:\n                pred_residuals = outputs_BboxNet_heading_regr # (shape: (batch_size*, ))\n\n                Rmat = Variable(torch.zeros(pred_h.size()[0], 3, 3), requires_grad=True).cuda() # (shape: (batch_size*, 3, 3))\n                Rmat[:, 0, 0] = torch.cos(pred_residuals)\n                Rmat[:, 0, 2] = torch.sin(pred_residuals)\n                Rmat[:, 1, 1] = 1\n                Rmat[:, 2, 0] = -torch.sin(pred_residuals)\n                Rmat[:, 2, 2] = torch.cos(pred_residuals)\n\n                p0_orig = Variable(torch.zeros(pred_h.size()[0], 3, 1), requires_grad=True).cuda() # (shape: (batch_size*, 3, 1))\n                p0_orig[:, 0, 0] = pred_l/2.0\n                p0_orig[:, 2, 0] = pred_w/2.0\n\n                p1_orig = Variable(torch.zeros(pred_h.size()[0], 3, 1), requires_grad=True).cuda() # (shape: (batch_size*, 3, 1))\n                p1_orig[:, 0, 0] = -pred_l/2.0\n                p1_orig[:, 2, 0] = pred_w/2.0\n\n                p2_orig = Variable(torch.zeros(pred_h.size()[0], 3, 1), requires_grad=True).cuda() # (shape: (batch_size*, 3, 1))\n                p2_orig[:, 0, 0] = -pred_l/2.0\n                p2_orig[:, 2, 0] = -pred_w/2.0\n\n                p3_orig = Variable(torch.zeros(pred_h.size()[0], 3, 1), requires_grad=True).cuda() # (shape: (batch_size*, 3, 1))\n                p3_orig[:, 0, 0] = pred_l/2.0\n                p3_orig[:, 2, 0] = -pred_w/2.0\n\n                p4_orig = Variable(torch.zeros(pred_h.size()[0], 3, 1), requires_grad=True).cuda() # (shape: (batch_size*, 3, 1))\n                p4_orig[:, 0, 0] = pred_l/2.0\n                p4_orig[:, 1, 0] = -pred_h\n                p4_orig[:, 2, 0] = pred_w/2.0\n\n                p5_orig = Variable(torch.zeros(pred_h.size()[0], 3, 1), requires_grad=True).cuda() # (shape: (batch_size*, 3, 1))\n                p5_orig[:, 0, 0] = -pred_l/2.0\n                p5_orig[:, 1, 0] = -pred_h\n                p5_orig[:, 2, 0] = pred_w/2.0\n\n                p6_orig = Variable(torch.zeros(pred_h.size()[0], 3, 1), requires_grad=True).cuda() # (shape: (batch_size*, 3, 1))\n                p6_orig[:, 0, 0] = -pred_l/2.0\n                p6_orig[:, 1, 0] = -pred_h\n                p6_orig[:, 2, 0] = -pred_w/2.0\n\n                p7_orig = Variable(torch.zeros(pred_h.size()[0], 3, 1), requires_grad=True).cuda() # (shape: (batch_size*, 3, 1))\n                p7_orig[:, 0, 0] = pred_l/2.0\n                p7_orig[:, 1, 0] = -pred_h\n                p7_orig[:, 2, 0] = -pred_w/2.0\n\n                pred_p0_unsqeezed = pred_center_unsqeezed + torch.bmm(Rmat, p0_orig) # (shape: (batch_size*, 3, 1))\n                pred_p0 = pred_p0_unsqeezed[:, :, 0] # (shape: (batch_size*, 3))\n                pred_p1_unsqeezed = pred_center_unsqeezed + torch.bmm(Rmat, p1_orig) # (shape: (batch_size*, 3, 1))\n                pred_p1 = pred_p1_unsqeezed[:, :, 0] # (shape: (batch_size*, 3))\n                pred_p2_unsqeezed = pred_center_unsqeezed + torch.bmm(Rmat, p2_orig) # (shape: (batch_size*, 3, 1))\n                pred_p2 = pred_p2_unsqeezed[:, :, 0] # (shape: (batch_size*, 3))\n                pred_p3_unsqeezed = pred_center_unsqeezed + torch.bmm(Rmat, p3_orig) # (shape: (batch_size*, 3, 1))\n                pred_p3 = pred_p3_unsqeezed[:, :, 0] # (shape: (batch_size*, 3))\n                pred_p4_unsqeezed = pred_center_unsqeezed + torch.bmm(Rmat, p4_orig) # (shape: (batch_size*, 3, 1))\n                pred_p4 = pred_p4_unsqeezed[:, :, 0] # (shape: (batch_size*, 3))\n                pred_p5_unsqeezed = pred_center_unsqeezed + torch.bmm(Rmat, p5_orig) # (shape: (batch_size*, 3, 1))\n                pred_p5 = pred_p5_unsqeezed[:, :, 0] # (shape: (batch_size*, 3))\n                pred_p6_unsqeezed = pred_center_unsqeezed + torch.bmm(Rmat, p6_orig) # (shape: (batch_size*, 3, 1))\n                pred_p6 = pred_p6_unsqeezed[:, :, 0] # (shape: (batch_size*, 3))\n                pred_p7_unsqeezed = pred_center_unsqeezed + torch.bmm(Rmat, p7_orig) # (shape: (batch_size*, 3, 1))\n                pred_p7 = pred_p7_unsqeezed[:, :, 0] # (shape: (batch_size*, 3))\n\n                outputs_corner = Variable(torch.zeros(pred_h.size()[0], 8, 3), requires_grad=True).cuda() # (shape: (batch_size*, 8, 3))\n                outputs_corner[:, 0] = pred_p0\n                outputs_corner[:, 1] = pred_p1\n                outputs_corner[:, 2] = pred_p2\n                outputs_corner[:, 3] = pred_p3\n                outputs_corner[:, 4] = pred_p4\n                outputs_corner[:, 5] = pred_p5\n                outputs_corner[:, 6] = pred_p6\n                outputs_corner[:, 7] = pred_p7\n\n                loss_corner_unflipped = regression_loss_func(outputs_corner, labels_corner)\n                loss_corner_flipped = regression_loss_func(outputs_corner, labels_corner_flipped)\n\n                loss_corner = torch.min(loss_corner_unflipped, loss_corner_flipped)\n\n            loss_corner_value = loss_corner.data.cpu().numpy()\n            batch_losses_corner.append(loss_corner_value)\n\n            ########################################################################\n            # compute the total loss:\n            ########################################################################\n            lambda_value = 1\n            gamma_value = 10\n            loss = loss_InstanceSeg + lambda_value*(loss_TNet + loss_BboxNet + gamma_value*loss_corner)\n            loss_value = loss.data.cpu().numpy()\n            batch_losses.append(loss_value)\n\n    # compute the val epoch loss:\n    epoch_loss = np.mean(batch_losses)\n    # save the val epoch loss:\n    epoch_losses_val.append(epoch_loss)\n    # save the val epoch loss to disk:\n    with open(""%s/epoch_losses_val.pkl"" % network.model_dir, ""wb"") as file:\n        pickle.dump(epoch_losses_val, file)\n    print (""validation loss: %g"" % epoch_loss)\n    # plot the val loss vs epoch and save to disk:\n    plt.figure(1)\n    plt.plot(epoch_losses_val, ""k^"")\n    plt.plot(epoch_losses_val, ""k"")\n    plt.ylabel(""loss"")\n    plt.xlabel(""epoch"")\n    plt.title(""validation loss per epoch"")\n    plt.savefig(""%s/epoch_losses_val.png"" % network.model_dir)\n    plt.close(1)\n\n    # compute the val epoch TNet loss:\n    epoch_loss = np.mean(batch_losses_TNet)\n    # save the val epoch loss:\n    epoch_losses_TNet_val.append(epoch_loss)\n    # save the val epoch TNet loss to disk:\n    with open(""%s/epoch_losses_TNet_val.pkl"" % network.model_dir, ""wb"") as file:\n        pickle.dump(epoch_losses_TNet_val, file)\n    print (""validation TNet loss: %g"" % epoch_loss)\n    # plot the validation TNet loss vs epoch and save to disk:\n    plt.figure(1)\n    plt.plot(epoch_losses_TNet_val, ""k^"")\n    plt.plot(epoch_losses_TNet_val, ""k"")\n    plt.ylabel(""loss"")\n    plt.xlabel(""epoch"")\n    plt.title(""validation TNet loss per epoch"")\n    plt.savefig(""%s/epoch_losses_TNet_val.png"" % network.model_dir)\n    plt.close(1)\n\n    # compute the val epoch InstanceSeg loss:\n    epoch_loss = np.mean(batch_losses_InstanceSeg)\n    # save the val epoch loss:\n    epoch_losses_InstanceSeg_val.append(epoch_loss)\n    # save the val epoch InstanceSeg loss to disk:\n    with open(""%s/epoch_losses_InstanceSeg_val.pkl"" % network.model_dir, ""wb"") as file:\n        pickle.dump(epoch_losses_InstanceSeg_val, file)\n    print (""validation InstanceSeg loss: %g"" % epoch_loss)\n    # plot the validation InstanceSeg loss vs epoch and save to disk:\n    plt.figure(1)\n    plt.plot(epoch_losses_InstanceSeg_val, ""k^"")\n    plt.plot(epoch_losses_InstanceSeg_val, ""k"")\n    plt.ylabel(""loss"")\n    plt.xlabel(""epoch"")\n    plt.title(""validation InstanceSeg loss per epoch"")\n    plt.savefig(""%s/epoch_losses_InstanceSeg_val.png"" % network.model_dir)\n    plt.close(1)\n\n    # compute the val epoch BboxNet loss:\n    epoch_loss = np.mean(batch_losses_BboxNet)\n    # save the val epoch loss:\n    epoch_losses_BboxNet_val.append(epoch_loss)\n    # save the val epoch BboxNet loss to disk:\n    with open(""%s/epoch_losses_BboxNet_val.pkl"" % network.model_dir, ""wb"") as file:\n        pickle.dump(epoch_losses_BboxNet_val, file)\n    print (""validation BboxNet loss: %g"" % epoch_loss)\n    # plot the validation BboxNet loss vs epoch and save to disk:\n    plt.figure(1)\n    plt.plot(epoch_losses_BboxNet_val, ""k^"")\n    plt.plot(epoch_losses_BboxNet_val, ""k"")\n    plt.ylabel(""loss"")\n    plt.xlabel(""epoch"")\n    plt.title(""validation BboxNet loss per epoch"")\n    plt.savefig(""%s/epoch_losses_BboxNet_val.png"" % network.model_dir)\n    plt.close(1)\n\n    # compute the val epoch BboxNet size loss:\n    epoch_loss = np.mean(batch_losses_BboxNet_size)\n    # save the val epoch loss:\n    epoch_losses_BboxNet_size_val.append(epoch_loss)\n    # save the val epoch BboxNet size loss to disk:\n    with open(""%s/epoch_losses_BboxNet_size_val.pkl"" % network.model_dir, ""wb"") as file:\n        pickle.dump(epoch_losses_BboxNet_size_val, file)\n    print (""validation BboxNet size loss: %g"" % epoch_loss)\n    # plot the validation BboxNet size loss vs epoch and save to disk:\n    plt.figure(1)\n    plt.plot(epoch_losses_BboxNet_size_val, ""k^"")\n    plt.plot(epoch_losses_BboxNet_size_val, ""k"")\n    plt.ylabel(""loss"")\n    plt.xlabel(""epoch"")\n    plt.title(""validation BboxNet size loss per epoch"")\n    plt.savefig(""%s/epoch_losses_BboxNet_size_val.png"" % network.model_dir)\n    plt.close(1)\n\n    # compute the val epoch BboxNet center loss:\n    epoch_loss = np.mean(batch_losses_BboxNet_center)\n    # save the val epoch loss:\n    epoch_losses_BboxNet_center_val.append(epoch_loss)\n    # save the val epoch BboxNet center loss to disk:\n    with open(""%s/epoch_losses_BboxNet_center_val.pkl"" % network.model_dir, ""wb"") as file:\n        pickle.dump(epoch_losses_BboxNet_center_val, file)\n    print (""validation BboxNet center loss: %g"" % epoch_loss)\n    # plot the validation BboxNet center loss vs epoch and save to disk:\n    plt.figure(1)\n    plt.plot(epoch_losses_BboxNet_center_val, ""k^"")\n    plt.plot(epoch_losses_BboxNet_center_val, ""k"")\n    plt.ylabel(""loss"")\n    plt.xlabel(""epoch"")\n    plt.title(""validation BboxNet center loss per epoch"")\n    plt.savefig(""%s/epoch_losses_BboxNet_center_val.png"" % network.model_dir)\n    plt.close(1)\n\n    # compute the val epoch BboxNet heading class loss:\n    epoch_loss = np.mean(batch_losses_BboxNet_heading_class)\n    # save the val epoch loss:\n    epoch_losses_BboxNet_heading_class_val.append(epoch_loss)\n    # save the val epoch BboxNet heading class loss to disk:\n    with open(""%s/epoch_losses_BboxNet_heading_class_val.pkl"" % network.model_dir, ""wb"") as file:\n        pickle.dump(epoch_losses_BboxNet_heading_class_val, file)\n    print (""validation BboxNet heading class loss: %g"" % epoch_loss)\n    # plot the validation BboxNet heading class loss vs epoch and save to disk:\n    plt.figure(1)\n    plt.plot(epoch_losses_BboxNet_heading_class_val, ""k^"")\n    plt.plot(epoch_losses_BboxNet_heading_class_val, ""k"")\n    plt.ylabel(""loss"")\n    plt.xlabel(""epoch"")\n    plt.title(""validation BboxNet heading class loss per epoch"")\n    plt.savefig(""%s/epoch_losses_BboxNet_heading_class_val.png"" % network.model_dir)\n    plt.close(1)\n\n    # compute the val epoch BboxNet heading regr loss:\n    epoch_loss = np.mean(batch_losses_BboxNet_heading_regr)\n    # save the val epoch loss:\n    epoch_losses_BboxNet_heading_regr_val.append(epoch_loss)\n    # save the val epoch BboxNet heading regr loss to disk:\n    with open(""%s/epoch_losses_BboxNet_heading_regr_val.pkl"" % network.model_dir, ""wb"") as file:\n        pickle.dump(epoch_losses_BboxNet_heading_regr_val, file)\n    print (""validation BboxNet heading regr loss: %g"" % epoch_loss)\n    # plot the validation BboxNet heading regr loss vs epoch and save to disk:\n    plt.figure(1)\n    plt.plot(epoch_losses_BboxNet_heading_regr_val, ""k^"")\n    plt.plot(epoch_losses_BboxNet_heading_regr_val, ""k"")\n    plt.ylabel(""loss"")\n    plt.xlabel(""epoch"")\n    plt.title(""validation BboxNet heading regr loss per epoch"")\n    plt.savefig(""%s/epoch_losses_BboxNet_heading_regr_val.png"" % network.model_dir)\n    plt.close(1)\n\n    # compute the val epoch heading class accuracy:\n    epoch_accuracy = np.mean(batch_accuracies_heading_class)\n    # save the val epoch heading class accuracy:\n    epoch_accuracies_heading_class_val.append(epoch_accuracy)\n    # save the val epoch heading class accuracy to disk:\n    with open(""%s/epoch_accuracies_heading_class_val.pkl"" % network.model_dir, ""wb"") as file:\n        pickle.dump(epoch_accuracies_heading_class_val, file)\n    print (""validation heading class accuracy: %g"" % epoch_accuracy)\n    # plot the validation heading class accuracy vs epoch and save to disk:\n    plt.figure(1)\n    plt.plot(epoch_accuracies_heading_class_val, ""k^"")\n    plt.plot(epoch_accuracies_heading_class_val, ""k"")\n    plt.ylabel(""accuracy"")\n    plt.xlabel(""epoch"")\n    plt.title(""validation heading class accuracy per epoch"")\n    plt.savefig(""%s/epoch_accuracies_heading_class_val.png"" % network.model_dir)\n    plt.close(1)\n\n    # compute the val epoch corner loss:\n    epoch_loss = np.mean(batch_losses_corner)\n    # save the val epoch corner loss:\n    epoch_losses_corner_val.append(epoch_loss)\n    # save the val epoch corner loss to disk:\n    with open(""%s/epoch_losses_corner_val.pkl"" % network.model_dir, ""wb"") as file:\n        pickle.dump(epoch_losses_corner_val, file)\n    print (""validation corner loss: %g"" % epoch_loss)\n    # plot the validation corner loss vs epoch and save to disk:\n    plt.figure(1)\n    plt.plot(epoch_losses_corner_val, ""k^"")\n    plt.plot(epoch_losses_corner_val, ""k"")\n    plt.ylabel(""loss"")\n    plt.xlabel(""epoch"")\n    plt.title(""validation corner loss per epoch"")\n    plt.savefig(""%s/epoch_losses_corner_val.png"" % network.model_dir)\n    plt.close(1)\n\n    # compute the val epoch accuracy:\n    epoch_accuracy = np.mean(batch_accuracies)\n    # save the val epoch accuracy:\n    epoch_accuracies_val.append(epoch_accuracy)\n    # save the val epoch accuracy to disk:\n    with open(""%s/epoch_accuracies_val.pkl"" % network.model_dir, ""wb"") as file:\n        pickle.dump(epoch_accuracies_val, file)\n    print (""validation accuracy: %g"" % epoch_accuracy)\n    # plot the validation accuracy vs epoch and save to disk:\n    plt.figure(1)\n    plt.plot(epoch_accuracies_val, ""k^"")\n    plt.plot(epoch_accuracies_val, ""k"")\n    plt.ylabel(""accuracy"")\n    plt.xlabel(""epoch"")\n    plt.title(""validation accuracy per epoch"")\n    plt.savefig(""%s/epoch_accuracies_val.png"" % network.model_dir)\n    plt.close(1)\n\n    # compute the val epoch precision:\n    epoch_precision = np.mean(batch_precisions)\n    # save the val epoch precision:\n    epoch_precisions_val.append(epoch_precision)\n    # save the val epoch precision to disk:\n    with open(""%s/epoch_precisions_val.pkl"" % network.model_dir, ""wb"") as file:\n        pickle.dump(epoch_precisions_val, file)\n    print (""validation precision: %g"" % epoch_precision)\n    # plot the validation precision vs epoch and save to disk:\n    plt.figure(1)\n    plt.plot(epoch_precisions_val, ""k^"")\n    plt.plot(epoch_precisions_val, ""k"")\n    plt.ylabel(""precision"")\n    plt.xlabel(""epoch"")\n    plt.title(""validation precision per epoch"")\n    plt.savefig(""%s/epoch_precisions_val.png"" % network.model_dir)\n    plt.close(1)\n\n    # compute the val epoch recall:\n    epoch_recall = np.mean(batch_recalls)\n    # save the val epoch recall:\n    epoch_recalls_val.append(epoch_recall)\n    # save the val epoch recall to disk:\n    with open(""%s/epoch_recalls_val.pkl"" % network.model_dir, ""wb"") as file:\n        pickle.dump(epoch_recalls_val, file)\n    print (""validation recall: %g"" % epoch_recall)\n    # plot the validation recall vs epoch and save to disk:\n    plt.figure(1)\n    plt.plot(epoch_recalls_val, ""k^"")\n    plt.plot(epoch_recalls_val, ""k"")\n    plt.ylabel(""recall"")\n    plt.xlabel(""epoch"")\n    plt.title(""validation recall per epoch"")\n    plt.savefig(""%s/epoch_recalls_val.png"" % network.model_dir)\n    plt.close(1)\n\n    # compute the val epoch f1:\n    epoch_f1 = np.mean(batch_f1s)\n    # save the val epoch f1:\n    epoch_f1s_val.append(epoch_f1)\n    # save the val epoch f1 to disk:\n    with open(""%s/epoch_f1s_val.pkl"" % network.model_dir, ""wb"") as file:\n        pickle.dump(epoch_f1s_val, file)\n    print (""validation f1: %g"" % epoch_f1)\n    # plot the validation f1 vs epoch and save to disk:\n    plt.figure(1)\n    plt.plot(epoch_f1s_val, ""k^"")\n    plt.plot(epoch_f1s_val, ""k"")\n    plt.ylabel(""f1"")\n    plt.xlabel(""epoch"")\n    plt.title(""validation f1 per epoch"")\n    plt.savefig(""%s/epoch_f1s_val.png"" % network.model_dir)\n    plt.close(1)\n\n    # save the model weights to disk:\n    checkpoint_path = network.checkpoints_dir + ""/model_"" + model_id +""_epoch_"" + str(epoch+1) + "".pth""\n    torch.save(network.state_dict(), checkpoint_path)\n'"
Frustum-PointNet/datasets.py,29,"b'# camera-ready\n\nimport sys\nsys.path.append(""/root/3DOD_thesis/utils"")\nfrom kittiloader import LabelLoader2D3D, calibread, LabelLoader2D3D_sequence # (this needs to be imported before torch, because cv2 needs to be imported before torch for some reason)\n\nimport torch\nimport torch.utils.data\n\nimport os\nimport pickle\nimport numpy as np\nimport math\n\n# # # # # # # # # # debug visualizations START:\n# import sys\n# sys.path.append(""/home/fregu856/exjobb/Open3D/build/lib"")\n# from py3d import *\n#\n# def draw_geometries_dark_background(geometries):\n#     vis = Visualizer()\n#     vis.create_window()\n#     opt = vis.get_render_option()\n#     opt.background_color = np.asarray([0, 0, 0])\n#     for geometry in geometries:\n#         vis.add_geometry(geometry)\n#     vis.run()\n#     vis.destroy_window()\n# # # # # # # # # # debug visualizations END:\n\ndef wrapToPi(a):\n    return (a + np.pi) % (2*np.pi) - np.pi\n\ndef getBinNumber4(angle):\n    # angle is assumed to be in [-pi, pi[\n\n    if (angle >= -np.pi/4) and (angle < np.pi/4):\n        bin_number = 0\n    elif (angle >= np.pi/4) and (angle < 3*np.pi/4):\n        bin_number = 1\n    elif ((angle >= 3*np.pi/4) and (angle < np.pi)) or ((angle >= -np.pi) and (angle < -3*np.pi/4)):\n        bin_number = 2\n    elif (angle >= -3*np.pi/4) and (angle < -np.pi/4):\n        bin_number = 3\n    else:\n        raise Exception(""getBinNumber4: angle is not in [-pi, pi["")\n\n    return bin_number\n\ndef getBinNumber(angle, NH):\n    if NH == 4:\n        return getBinNumber4(angle)\n    else:\n        raise Exception(""getBinNumber: NH is not 4"")\n\ndef getBinCenter(bin_number, NH):\n    if NH == 4:\n        bin_center = wrapToPi(bin_number*(np.pi/2))\n    else:\n        raise Exception(""getBinCenter: NH is not 4"")\n\n    return bin_center\n\ndef getBinCenters(bin_numbers, NH):\n    # bin_number has shape (m, n)\n\n    if NH == 4:\n        bin_centers = wrapToPi(bin_numbers*(np.pi/2))\n    else:\n        raise Exception(""getBinCenters: NH is not 4"")\n\n    return bin_centers\n\nclass DatasetFrustumPointNetAugmentation(torch.utils.data.Dataset):\n    def __init__(self, kitti_data_path, kitti_meta_path, type, NH):\n        self.img_dir = kitti_data_path + ""/object/training/image_2/""\n        self.label_dir = kitti_data_path + ""/object/training/label_2/""\n        self.calib_dir = kitti_data_path + ""/object/training/calib/""\n        self.lidar_dir = kitti_data_path + ""/object/training/velodyne/""\n\n        self.NH = NH\n\n        with open(kitti_meta_path + ""/%s_img_ids.pkl"" % type, ""rb"") as file: # (needed for python3)\n            img_ids = pickle.load(file)\n\n        with open(kitti_meta_path + ""/kitti_train_mean_car_size.pkl"", ""rb"") as file: # (needed for python3)\n            self.mean_car_size = pickle.load(file)\n\n        with open(kitti_meta_path + ""/kitti_centered_frustum_mean_xyz.pkl"", ""rb"") as file: # (needed for python3)\n            self.centered_frustum_mean_xyz = pickle.load(file)\n            self.centered_frustum_mean_xyz = self.centered_frustum_mean_xyz.astype(np.float32)\n\n        self.examples = []\n        for img_id in img_ids:\n            labels = LabelLoader2D3D(img_id, self.label_dir, "".txt"", self.calib_dir, "".txt"")\n            for label in labels:\n                label_2d = label[""label_2D""]\n                if label_2d[""truncated""] < 0.5 and label_2d[""class""] == ""Car"":\n                    label[""img_id""] = img_id\n                    self.examples.append(label)\n\n        self.num_examples = len(self.examples)\n\n    def __getitem__(self, index):\n        example = self.examples[index]\n\n        img_id = example[""img_id""]\n        #print(img_id)\n\n        lidar_path = self.lidar_dir + img_id + "".bin""\n        point_cloud = np.fromfile(lidar_path, dtype=np.float32).reshape(-1, 4)\n        orig_point_cloud = point_cloud\n\n        # remove points that are located behind the camera:\n        point_cloud = point_cloud[point_cloud[:, 0] > 0, :]\n        # remove points that are located too far away from the camera:\n        point_cloud = point_cloud[point_cloud[:, 0] < 80, :]\n\n        # # # # # debug visualization:\n        # pcd = PointCloud()\n        # pcd.points = Vector3dVector(point_cloud[:, 0:3])\n        # pcd.paint_uniform_color([0.65, 0.65, 0.65])\n        # draw_geometries_dark_background([pcd])\n        # # # # #\n\n        calib = calibread(self.calib_dir + img_id + "".txt"")\n        P2 = calib[""P2""]\n        Tr_velo_to_cam_orig = calib[""Tr_velo_to_cam""]\n        R0_rect_orig = calib[""R0_rect""]\n        #\n        R0_rect = np.eye(4)\n        R0_rect[0:3, 0:3] = R0_rect_orig\n        #\n        Tr_velo_to_cam = np.eye(4)\n        Tr_velo_to_cam[0:3, :] = Tr_velo_to_cam_orig\n\n        point_cloud_xyz = point_cloud[:, 0:3]\n        point_cloud_xyz_hom = np.ones((point_cloud.shape[0], 4))\n        point_cloud_xyz_hom[:, 0:3] = point_cloud[:, 0:3] # (point_cloud_xyz_hom has shape (num_points, 4))\n\n        # project the points onto the image plane (homogeneous coords):\n        img_points_hom = np.dot(P2, np.dot(R0_rect, np.dot(Tr_velo_to_cam, point_cloud_xyz_hom.T))).T # (point_cloud_xyz_hom.T has shape (4, num_points))\n        # normalize:\n        img_points = np.zeros((img_points_hom.shape[0], 2))\n        img_points[:, 0] = img_points_hom[:, 0]/img_points_hom[:, 2]\n        img_points[:, 1] = img_points_hom[:, 1]/img_points_hom[:, 2]\n\n        # transform the points into (rectified) camera coordinates:\n        point_cloud_xyz_camera_hom = np.dot(R0_rect, np.dot(Tr_velo_to_cam, point_cloud_xyz_hom.T)).T # (point_cloud_xyz_hom.T has shape (4, num_points))\n        # normalize:\n        point_cloud_xyz_camera = np.zeros((point_cloud_xyz_camera_hom.shape[0], 3))\n        point_cloud_xyz_camera[:, 0] = point_cloud_xyz_camera_hom[:, 0]/point_cloud_xyz_camera_hom[:, 3]\n        point_cloud_xyz_camera[:, 1] = point_cloud_xyz_camera_hom[:, 1]/point_cloud_xyz_camera_hom[:, 3]\n        point_cloud_xyz_camera[:, 2] = point_cloud_xyz_camera_hom[:, 2]/point_cloud_xyz_camera_hom[:, 3]\n\n        point_cloud_camera = point_cloud\n        point_cloud_camera[:, 0:3] = point_cloud_xyz_camera\n\n        label_2D = example[""label_2D""]\n        label_3D = example[""label_3D""]\n\n        bbox = label_2D[""poly""]\n\n        # img = cv2.imread(self.img_dir + img_id + "".png"", -1)\n        # img_with_bboxes = draw_2d_polys(img, [label_2D])\n        # cv2.imwrite(""test.png"", img_with_bboxes)\n\n        ########################################################################\n        # frustum:\n        ########################################################################\n        u_min = bbox[0, 0] # (left)\n        u_max = bbox[1, 0] # (rigth)\n        v_min = bbox[0, 1] # (top)\n        v_max = bbox[2, 1] # (bottom)\n\n        ########################################################################\n        # # # # augment the 2Dbbox START:\n        ########################################################################\n        w = u_max - u_min\n        h = v_max - v_min\n        u_center = u_min + w/2.0\n        v_center = v_min + h/2.0\n\n        # translate the center by random distances sampled from\n        # uniform[-0.1w, 0.1w] and uniform[-0.1h, 0.1h] in u,v directions:\n        u_center = u_center + np.random.uniform(low=-0.1*w, high=0.1*w)\n        v_center = v_center + np.random.uniform(low=-0.1*h, high=0.1*h)\n\n        # randomly scale w and h by factor sampled from uniform[0.9, 1.1]:\n        w = w*np.random.uniform(low=0.9, high=1.1)\n        h = h*np.random.uniform(low=0.9, high=1.1)\n\n        u_min = u_center - w/2.0\n        u_max = u_center + w/2.0\n        v_min = v_center - h/2.0\n        v_max = v_center + h/2.0\n        ########################################################################\n        # # # # augment the 2Dbbox END:\n        ########################################################################\n\n        row_mask = np.logical_and(\n                    np.logical_and(img_points[:, 0] >= u_min,\n                                   img_points[:, 0] <= u_max),\n                    np.logical_and(img_points[:, 1] >= v_min,\n                                   img_points[:, 1] <= v_max))\n\n        frustum_point_cloud_xyz = point_cloud_xyz[row_mask, :] # (needed only for visualization)\n        frustum_point_cloud = point_cloud[row_mask, :]\n        frustum_point_cloud_xyz_camera = point_cloud_xyz_camera[row_mask, :]\n        frustum_point_cloud_camera = point_cloud_camera[row_mask, :]\n\n        if frustum_point_cloud.shape[0] == 0:\n            print (img_id)\n            print (frustum_point_cloud.shape)\n            return self.__getitem__(0)\n\n        # randomly sample 1024 points in the frustum point cloud:\n        if frustum_point_cloud.shape[0] < 1024:\n            row_idx = np.random.choice(frustum_point_cloud.shape[0], 1024, replace=True)\n        else:\n            row_idx = np.random.choice(frustum_point_cloud.shape[0], 1024, replace=False)\n        frustum_point_cloud_xyz = frustum_point_cloud_xyz[row_idx, :]\n        frustum_point_cloud = frustum_point_cloud[row_idx, :]\n        frustum_point_cloud_xyz_camera = frustum_point_cloud_xyz_camera[row_idx, :]\n        frustum_point_cloud_camera = frustum_point_cloud_camera[row_idx, :]\n        # (the frustum point cloud now has exactly 1024 points)\n\n        ########################################################################\n        # InstanceSeg ground truth:\n        ########################################################################\n        points = label_3D[""points""]\n\n        y_max = points[0, 1]\n        y_min = points[4, 1]\n\n        # D, A, B are consecutive corners of the rectangle (projection of the 3D bbox) in the x,z plane\n        # the vectors AB and AD are orthogonal\n        # a point P = (x, z) lies within the rectangle in the x,z plane iff:\n        # (0 < AP dot AB < AB dot AB) && (0 < AP dot AD < AD dot AD)\n        # (https://math.stackexchange.com/a/190373)\n\n        A = np.array([points[0, 0], points[0, 2]])\n        B = np.array([points[1, 0], points[1, 2]])\n        D = np.array([points[3, 0], points[3, 2]])\n\n        AB = B - A\n        AD = D - A\n        AB_dot_AB = np.dot(AB, AB)\n        AD_dot_AD = np.dot(AD, AD)\n\n        P = np.zeros((frustum_point_cloud_xyz_camera.shape[0], 2))\n        P[:, 0] = frustum_point_cloud_xyz_camera[:, 0]\n        P[:, 1] = frustum_point_cloud_xyz_camera[:, 2]\n\n        AP = P - A\n        AP_dot_AB = np.dot(AP, AB)\n        AP_dot_AD = np.dot(AP, AD)\n\n        row_mask = np.logical_and(\n                    np.logical_and(frustum_point_cloud_xyz_camera[:, 1] >= y_min, frustum_point_cloud_xyz_camera[:, 1] <= y_max),\n                    np.logical_and(np.logical_and(AP_dot_AB >= 0, AP_dot_AB <= AB_dot_AB),\n                                   np.logical_and(AP_dot_AD >= 0, AP_dot_AD <= AD_dot_AD)))\n\n        row_mask_gt = row_mask\n\n        gt_point_cloud_xyz_camera = frustum_point_cloud_xyz_camera[row_mask, :] # (needed only for visualization)\n\n        label_InstanceSeg = np.zeros((frustum_point_cloud.shape[0],), dtype=np.int64)\n        label_InstanceSeg[row_mask] = 1 # (0: point is NOT part of the objet, 1: point is part of the object)\n\n        ########################################################################\n        # visualization of frustum and InstanceSeg ground truth:\n        ########################################################################\n        # import sys\n        # sys.path.append(""/home/fregu856/exjobb/Open3D/build/lib"")\n        # from py3d import *\n        #\n        # # visualize the frustum points with ground truth as colored points:\n        # frustum_pcd_camera = PointCloud()\n        # frustum_pcd_camera.points = Vector3dVector(frustum_point_cloud_xyz_camera)\n        # frustum_pcd_camera.paint_uniform_color([0.25, 0.25, 0.25])\n        # gt_pcd_camera = PointCloud()\n        # gt_pcd_camera.points = Vector3dVector(gt_point_cloud_xyz_camera)\n        # gt_pcd_camera.paint_uniform_color([0, 0, 1])\n        # draw_geometries([gt_pcd_camera, frustum_pcd_camera])\n        #\n        # # visualize the frustum points and gt points as differently colored points in the full point cloud:\n        # frustum_pcd_camera.paint_uniform_color([1, 0, 0])\n        # pcd_camera = PointCloud()\n        # pcd_camera.points = Vector3dVector(point_cloud_camera[:, 0:3])\n        # pcd_camera.paint_uniform_color([0.25, 0.25, 0.25])\n        # draw_geometries([gt_pcd_camera, frustum_pcd_camera, pcd_camera])\n\n        ########################################################################\n        # normalize frustum point cloud:\n        ########################################################################\n        # get the 2dbbox center img point in hom. coords:\n        u_center = u_min + (u_max - u_min)/2.0\n        v_center = v_min + (v_max - v_min)/2.0\n        center_img_point_hom = np.array([u_center, v_center, 1])\n\n        # (more than one 3D point is projected onto the center image point, i.e,\n        # the linear system of equations is under-determined and has inf number\n        # of solutions. By using the pseudo-inverse, we obtain the least-norm sol)\n\n        # get a point (the least-norm sol.) that projects onto the center image point, in hom. coords:\n        P2_pseudo_inverse = np.linalg.pinv(P2) # (shape: (4, 3)) (P2 has shape (3, 4))\n        point_hom = np.dot(P2_pseudo_inverse, center_img_point_hom)\n\n        # hom --> normal coords:\n        point = np.array(([point_hom[0]/point_hom[3], point_hom[1]/point_hom[3], point_hom[2]/point_hom[3]]))\n\n        # if the point is behind the camera, switch to the mirror point in front of the camera:\n        if point[2] < 0:\n            point[0] = -point[0]\n            point[2] = -point[2]\n\n        # compute the angle of the point in the x-z plane: ((rectified) camera coords)\n        frustum_angle = np.arctan2(point[0], point[2]) # (np.arctan2(x, z)) # (frustum_angle = 0: frustum is centered)\n\n        # rotation_matrix to rotate points frustum_angle around the y axis (counter-clockwise):\n        frustum_R = np.asarray([[np.cos(frustum_angle), 0, -np.sin(frustum_angle)],\n                           [0, 1, 0],\n                           [np.sin(frustum_angle), 0, np.cos(frustum_angle)]],\n                           dtype=\'float32\')\n\n        # rotate the frustum point cloud to center it:\n        centered_frustum_point_cloud_xyz_camera = np.dot(frustum_R, frustum_point_cloud_xyz_camera.T).T\n\n        # subtract the centered frustum train xyz mean:\n        centered_frustum_point_cloud_xyz_camera -= self.centered_frustum_mean_xyz\n\n        centered_frustum_point_cloud_camera = frustum_point_cloud_camera\n        centered_frustum_point_cloud_camera[:, 0:3] = centered_frustum_point_cloud_xyz_camera\n\n        # # # # # # # # # # debug visualizations START:\n        # import sys\n        # sys.path.append(""/home/fregu856/exjobb/Open3D/build/lib"")\n        # from py3d import *\n        #\n        # frustum_pcd_camera = PointCloud()\n        # frustum_pcd_camera.points = Vector3dVector(frustum_point_cloud_xyz_camera)\n        # frustum_pcd_camera.paint_uniform_color([0.25, 0.25, 0.25])\n        #\n        # centered_frustum_pcd_camera = PointCloud()\n        # centered_frustum_pcd_camera.points = Vector3dVector(centered_frustum_point_cloud_xyz_camera)\n        # centered_frustum_pcd_camera.paint_uniform_color([1, 0, 0])\n        #\n        # uncentered_frustum_point_cloud_xyz_camera = np.dot(np.linalg.inv(frustum_R), centered_frustum_point_cloud_xyz_camera.T).T\n        # uncentered_frustum_pcd_camera = PointCloud()\n        # uncentered_frustum_pcd_camera.points = Vector3dVector(uncentered_frustum_point_cloud_xyz_camera)\n        # uncentered_frustum_pcd_camera.paint_uniform_color([0, 1, 0])\n        # draw_geometries([centered_frustum_pcd_camera, uncentered_frustum_pcd_camera, frustum_pcd_camera])\n        # # # # # # # # # # debug visualizations END:\n\n        ########################################################################\n        # randomly shift the frustum point cloud in the z direction:\n        ########################################################################\n        z_shift = np.random.uniform(low=-20, high=20)\n        centered_frustum_point_cloud_camera[:, 2] -= z_shift\n\n        ########################################################################\n        # flip the frustum point cloud in the x-z plane with 0.5 prob:\n        ########################################################################\n        # # # # # # # # # # debug visualizations START:\n        # import sys\n        # sys.path.append(""/home/fregu856/exjobb/Open3D/build/lib"")\n        # from py3d import *\n        # pcd = PointCloud()\n        # pcd.points = Vector3dVector(centered_frustum_point_cloud_camera[:, 0:3])\n        # pcd.paint_uniform_color([0.25, 0.25, 0.25])\n        # draw_geometries([pcd])\n        # # # # # # # # # # debug visualizations END:\n\n        # get 0 or 1 with equal probability (indicating if the frustum should be flipped or not):\n        flip = np.random.randint(low=0, high=2)\n\n        # flip the frustum point cloud if flip == 1 (set all x values to -values):\n        centered_frustum_point_cloud_camera[:, 0] = flip*(-centered_frustum_point_cloud_camera[:, 0]) + (1-flip)*centered_frustum_point_cloud_camera[:, 0]\n\n        # # # # # # # # # # debug visualizations START:\n        # pcd.points = Vector3dVector(centered_frustum_point_cloud_camera[:, 0:3])\n        # pcd.paint_uniform_color([0.25, 0.25, 0.25])\n        # draw_geometries([pcd])\n        # # # # # # # # # # debug visualizations END:\n\n        ########################################################################\n        # TNet ground truth:\n        ########################################################################\n        label_TNet = np.dot(frustum_R, label_3D[""center""]) - self.centered_frustum_mean_xyz\n\n        # flip the label if flip == 1 (set the x value to -value):\n        label_TNet[0] = flip*(-label_TNet[0]) + (1-flip)*label_TNet[0]\n\n        # adjust for the random shift:\n        label_TNet[2] -= z_shift\n\n        ########################################################################\n        # randomly rotate the GT points in the frustum point cloud around the\n        # 3Dbbox center with an angle in uniform[-pi/10, pi/10]:\n        ########################################################################\n        # (the 3Dbbox center is NOT affected, only the heading angle)\n\n        centered_frustum_point_cloud_xyz_camera = centered_frustum_point_cloud_camera[:, 0:3]\n        gt_seg_point_cloud = centered_frustum_point_cloud_xyz_camera[row_mask_gt, :]\n\n        random_angle = np.random.uniform(low=-np.pi/10, high=np.pi/10)\n        R = np.asarray([[np.cos(random_angle), 0, -np.sin(random_angle)],\n                           [0, 1, 0],\n                           [np.sin(random_angle), 0, np.cos(random_angle)]],\n                           dtype=\'float32\')\n\n        # rotate the GT points random_angle counter-clockwise around the 3Dbbox center:\n        gt_seg_point_cloud_at_origo = gt_seg_point_cloud - label_TNet\n        rotated_gt_seg_point_cloud = np.dot(R, gt_seg_point_cloud_at_origo.T).T + label_TNet\n\n        # insert the rotated GT points back into the frustum point cloud:\n        rot_centered_frustum_point_cloud_xyz_camera = np.copy(centered_frustum_point_cloud_xyz_camera)\n        rot_centered_frustum_point_cloud_xyz_camera[row_mask_gt, :] = rotated_gt_seg_point_cloud\n\n        # # # # # # # # # # debug visualizations START:\n        # import sys\n        # sys.path.append(""/home/fregu856/exjobb/Open3D/build/lib"")\n        # from py3d import *\n        # frustum_pcd = PointCloud()\n        # frustum_pcd.points = Vector3dVector(centered_frustum_point_cloud_xyz_camera)\n        # frustum_pcd.paint_uniform_color([0.25, 0.25, 0.25])\n        # gt_seg_pcd = PointCloud()\n        # gt_seg_pcd.points = Vector3dVector(gt_seg_point_cloud)\n        # gt_seg_pcd.paint_uniform_color([0, 0, 1])\n        # draw_geometries([gt_seg_pcd, frustum_pcd])\n        # rot_frustum_pcd = PointCloud()\n        # rot_frustum_pcd.points = Vector3dVector(rot_centered_frustum_point_cloud_xyz_camera)\n        # rot_frustum_pcd.paint_uniform_color([0.25, 0.25, 0.25])\n        # rotated_gt_seg_pcd = PointCloud()\n        # rotated_gt_seg_pcd.points = Vector3dVector(rotated_gt_seg_point_cloud)\n        # rotated_gt_seg_pcd.paint_uniform_color([1, 0, 0])\n        # draw_geometries([rotated_gt_seg_pcd, rot_frustum_pcd])\n        # draw_geometries([gt_seg_pcd, rotated_gt_seg_pcd, frustum_pcd])\n        # # # # # # # # # # debug visualizations END:\n\n        centered_frustum_point_cloud_camera[:, 0:3] = rot_centered_frustum_point_cloud_xyz_camera\n\n        ########################################################################\n        # BboxNet ground truth:\n        ########################################################################\n        centered_r_y = wrapToPi(label_3D[\'r_y\'] - frustum_angle)\n        # flip the angle if flip == 1:\n        if flip == 1:\n            centered_r_y = wrapToPi(np.pi - centered_r_y)\n        # adjust for random rotation:\n        centered_r_y = wrapToPi(centered_r_y - random_angle)\n\n        bin_number = getBinNumber(centered_r_y, NH=self.NH)\n        bin_center = getBinCenter(bin_number, NH=self.NH)\n        residual = wrapToPi(centered_r_y - bin_center)\n\n        label_BboxNet = np.zeros((11, ), dtype=np.float32) # ([x, y, z, h, w, l, r_y_bin_number, r_y_residual, r_y_bin_number_neighbor,r_y_residual_neighbor, h_mean, w_mean, l_mean])\n\n        label_BboxNet[0:3] = np.dot(frustum_R, label_3D[""center""]) - self.centered_frustum_mean_xyz\n        # flip the label if flip == 1 (set the x value to -value):\n        label_BboxNet[0] = flip*(-label_BboxNet[0]) + (1-flip)*label_BboxNet[0]\n        # adjust for the random shift:\n        label_BboxNet[2] -= z_shift\n\n        label_BboxNet[3] = label_3D[\'h\']\n        label_BboxNet[4] = label_3D[\'w\']\n        label_BboxNet[5] = label_3D[\'l\']\n        label_BboxNet[6] = bin_number\n        label_BboxNet[7] = residual\n        label_BboxNet[8:] = self.mean_car_size\n\n        ########################################################################\n        # corner loss ground truth:\n        ########################################################################\n        Rmat = np.asarray([[math.cos(residual), 0, math.sin(residual)],\n                           [0, 1, 0],\n                           [-math.sin(residual), 0, math.cos(residual)]],\n                           dtype=\'float32\')\n\n        center = label_BboxNet[0:3]\n        l = label_3D[\'l\']\n        w = label_3D[\'w\']\n        h = label_3D[\'h\']\n        p0 = center + np.dot(Rmat, np.asarray([l/2.0, 0, w/2.0], dtype=\'float32\').flatten())\n        p1 = center + np.dot(Rmat, np.asarray([-l/2.0, 0, w/2.0], dtype=\'float32\').flatten())\n        p2 = center + np.dot(Rmat, np.asarray([-l/2.0, 0, -w/2.0], dtype=\'float32\').flatten())\n        p3 = center + np.dot(Rmat, np.asarray([l/2.0, 0, -w/2.0], dtype=\'float32\').flatten())\n        p4 = center + np.dot(Rmat, np.asarray([l/2.0, -h, w/2.0], dtype=\'float32\').flatten())\n        p5 = center + np.dot(Rmat, np.asarray([-l/2.0, -h, w/2.0], dtype=\'float32\').flatten())\n        p6 = center + np.dot(Rmat, np.asarray([-l/2.0, -h, -w/2.0], dtype=\'float32\').flatten())\n        p7 = center + np.dot(Rmat, np.asarray([l/2.0, -h, -w/2.0], dtype=\'float32\').flatten())\n        label_corner = np.array([p0, p1, p2, p3, p4, p5, p6, p7])\n        label_corner_flipped = np.array([p2, p3, p0, p1, p6, p7, p4, p5])\n\n        ########################################################################\n\n        centered_frustum_point_cloud_camera = torch.from_numpy(centered_frustum_point_cloud_camera) # (shape: (1024, 4))\n        label_InstanceSeg = torch.from_numpy(label_InstanceSeg) # (shape: (1024, ))\n        label_TNet = torch.from_numpy(label_TNet) # (shape: (3, ))\n        label_BboxNet = torch.from_numpy(label_BboxNet) # (shape: (11, ))\n        label_corner = torch.from_numpy(label_corner) # (shape: (8, 3))\n        label_corner_flipped = torch.from_numpy(label_corner_flipped) # (shape: (8, 3))\n\n        return (centered_frustum_point_cloud_camera, label_InstanceSeg, label_TNet, label_BboxNet, label_corner, label_corner_flipped)\n\n    def __len__(self):\n        return self.num_examples\n\n# test = DatasetFrustumPointNetAugmentation(""/home/fregu856/exjobb/data/kitti"", ""/home/fregu856/exjobb/data/kitti/meta"", type=""train"", NH=4)\n# for i in range(60):\n#     _ = test.__getitem__(i)\n\nclass EvalDatasetFrustumPointNet(torch.utils.data.Dataset):\n    def __init__(self, kitti_data_path, kitti_meta_path, type, NH):\n        self.img_dir = kitti_data_path + ""/object/training/image_2/""\n        self.label_dir = kitti_data_path + ""/object/training/label_2/""\n        self.calib_dir = kitti_data_path + ""/object/training/calib/""\n        self.lidar_dir = kitti_data_path + ""/object/training/velodyne/""\n\n        self.NH = NH\n\n        with open(kitti_meta_path + ""/%s_img_ids.pkl"" % type, ""rb"") as file: # (needed for python3)\n            img_ids = pickle.load(file)\n\n        with open(kitti_meta_path + ""/kitti_train_mean_car_size.pkl"", ""rb"") as file: # (needed for python3)\n            self.mean_car_size = pickle.load(file)\n\n        with open(kitti_meta_path + ""/kitti_centered_frustum_mean_xyz.pkl"", ""rb"") as file: # (needed for python3)\n            self.centered_frustum_mean_xyz = pickle.load(file)\n            self.centered_frustum_mean_xyz = self.centered_frustum_mean_xyz.astype(np.float32)\n\n        self.examples = []\n        for img_id in img_ids:\n            labels = LabelLoader2D3D(img_id, self.label_dir, "".txt"", self.calib_dir, "".txt"")\n            for label in labels:\n                label_2d = label[""label_2D""]\n                if label_2d[""truncated""] < 0.5 and label_2d[""class""] == ""Car"":\n                    label[""img_id""] = img_id\n                    self.examples.append(label)\n\n        self.num_examples = len(self.examples)\n\n    def __getitem__(self, index):\n        example = self.examples[index]\n\n        img_id = example[""img_id""]\n        #print(img_id)\n\n        lidar_path = self.lidar_dir + img_id + "".bin""\n        point_cloud = np.fromfile(lidar_path, dtype=np.float32).reshape(-1, 4)\n        orig_point_cloud = point_cloud\n\n        # remove points that are located behind the camera:\n        point_cloud = point_cloud[point_cloud[:, 0] > 0, :]\n        # remove points that are located too far away from the camera:\n        point_cloud = point_cloud[point_cloud[:, 0] < 80, :]\n\n        calib = calibread(self.calib_dir + img_id + "".txt"")\n        P2 = calib[""P2""]\n        Tr_velo_to_cam_orig = calib[""Tr_velo_to_cam""]\n        R0_rect_orig = calib[""R0_rect""]\n        #\n        R0_rect = np.eye(4)\n        R0_rect[0:3, 0:3] = R0_rect_orig\n        #\n        Tr_velo_to_cam = np.eye(4)\n        Tr_velo_to_cam[0:3, :] = Tr_velo_to_cam_orig\n\n        point_cloud_xyz = point_cloud[:, 0:3]\n        point_cloud_xyz_hom = np.ones((point_cloud.shape[0], 4))\n        point_cloud_xyz_hom[:, 0:3] = point_cloud[:, 0:3] # (point_cloud_xyz_hom has shape (num_points, 4))\n\n        # project the points onto the image plane (homogeneous coords):\n        img_points_hom = np.dot(P2, np.dot(R0_rect, np.dot(Tr_velo_to_cam, point_cloud_xyz_hom.T))).T # (point_cloud_xyz_hom.T has shape (4, num_points))\n        # normalize:\n        img_points = np.zeros((img_points_hom.shape[0], 2))\n        img_points[:, 0] = img_points_hom[:, 0]/img_points_hom[:, 2]\n        img_points[:, 1] = img_points_hom[:, 1]/img_points_hom[:, 2]\n\n        # transform the points into (rectified) camera coordinates:\n        point_cloud_xyz_camera_hom = np.dot(R0_rect, np.dot(Tr_velo_to_cam, point_cloud_xyz_hom.T)).T # (point_cloud_xyz_hom.T has shape (4, num_points))\n        # normalize:\n        point_cloud_xyz_camera = np.zeros((point_cloud_xyz_camera_hom.shape[0], 3))\n        point_cloud_xyz_camera[:, 0] = point_cloud_xyz_camera_hom[:, 0]/point_cloud_xyz_camera_hom[:, 3]\n        point_cloud_xyz_camera[:, 1] = point_cloud_xyz_camera_hom[:, 1]/point_cloud_xyz_camera_hom[:, 3]\n        point_cloud_xyz_camera[:, 2] = point_cloud_xyz_camera_hom[:, 2]/point_cloud_xyz_camera_hom[:, 3]\n\n        point_cloud_camera = point_cloud\n        point_cloud_camera[:, 0:3] = point_cloud_xyz_camera\n\n        label_2D = example[""label_2D""]\n        label_3D = example[""label_3D""]\n\n        bbox = label_2D[""poly""]\n\n        # img = cv2.imread(self.img_dir + img_id + "".png"", -1)\n        # img_with_bboxes = draw_2d_polys(img, [label_2D])\n        # cv2.imwrite(""test.png"", img_with_bboxes)\n\n        ########################################################################\n        # frustum:\n        ########################################################################\n        u_min = bbox[0, 0] # (left)\n        u_max = bbox[1, 0] # (rigth)\n        v_min = bbox[0, 1] # (top)\n        v_max = bbox[2, 1] # (bottom)\n\n        # expand the 2D bbox slightly:\n        u_min_expanded = u_min #- (u_max-u_min)*0.05\n        u_max_expanded = u_max #+ (u_max-u_min)*0.05\n        v_min_expanded = v_min #- (v_max-v_min)*0.05\n        v_max_expanded = v_max #+ (v_max-v_min)*0.05\n        input_2Dbbox = np.array([u_min_expanded, u_max_expanded, v_min_expanded, v_max_expanded])\n\n        row_mask = np.logical_and(\n                    np.logical_and(img_points[:, 0] >= u_min_expanded,\n                                   img_points[:, 0] <= u_max_expanded),\n                    np.logical_and(img_points[:, 1] >= v_min_expanded,\n                                   img_points[:, 1] <= v_max_expanded))\n\n        frustum_point_cloud_xyz = point_cloud_xyz[row_mask, :] # (needed only for visualization)\n        frustum_point_cloud = point_cloud[row_mask, :]\n        frustum_point_cloud_xyz_camera = point_cloud_xyz_camera[row_mask, :]\n        frustum_point_cloud_camera = point_cloud_camera[row_mask, :]\n\n        if frustum_point_cloud.shape[0] == 0:\n            print (img_id)\n            print (frustum_point_cloud.shape)\n            return self.__getitem__(0)\n\n        # randomly sample 1024 points in the frustum point cloud:\n        if frustum_point_cloud.shape[0] < 1024:\n            row_idx = np.random.choice(frustum_point_cloud.shape[0], 1024, replace=True)\n        else:\n            row_idx = np.random.choice(frustum_point_cloud.shape[0], 1024, replace=False)\n        frustum_point_cloud_xyz = frustum_point_cloud_xyz[row_idx, :]\n        frustum_point_cloud = frustum_point_cloud[row_idx, :]\n        frustum_point_cloud_xyz_camera = frustum_point_cloud_xyz_camera[row_idx, :]\n        frustum_point_cloud_camera = frustum_point_cloud_camera[row_idx, :]\n        # (the frustum point cloud now has exactly 1024 points)\n\n        ########################################################################\n        # InstanceSeg ground truth:\n        ########################################################################\n        points = label_3D[""points""]\n\n        y_max = points[0, 1]\n        y_min = points[4, 1]\n\n        # D, A, B are consecutive corners of the rectangle (projection of the 3D bbox) in the x,z plane\n        # the vectors AB and AD are orthogonal\n        # a point P = (x, z) lies within the rectangle in the x,z plane iff:\n        # (0 < AP dot AB < AB dot AB) && (0 < AP dot AD < AD dot AD)\n        # (https://math.stackexchange.com/a/190373)\n\n        A = np.array([points[0, 0], points[0, 2]])\n        B = np.array([points[1, 0], points[1, 2]])\n        D = np.array([points[3, 0], points[3, 2]])\n\n        AB = B - A\n        AD = D - A\n        AB_dot_AB = np.dot(AB, AB)\n        AD_dot_AD = np.dot(AD, AD)\n\n        P = np.zeros((frustum_point_cloud_xyz_camera.shape[0], 2))\n        P[:, 0] = frustum_point_cloud_xyz_camera[:, 0]\n        P[:, 1] = frustum_point_cloud_xyz_camera[:, 2]\n\n        AP = P - A\n        AP_dot_AB = np.dot(AP, AB)\n        AP_dot_AD = np.dot(AP, AD)\n\n        row_mask = np.logical_and(\n                    np.logical_and(frustum_point_cloud_xyz_camera[:, 1] >= y_min, frustum_point_cloud_xyz_camera[:, 1] <= y_max),\n                    np.logical_and(np.logical_and(AP_dot_AB >= 0, AP_dot_AB <= AB_dot_AB),\n                                   np.logical_and(AP_dot_AD >= 0, AP_dot_AD <= AD_dot_AD)))\n\n        gt_point_cloud_xyz_camera = frustum_point_cloud_xyz_camera[row_mask, :] # (needed only for visualization)\n\n        label_InstanceSeg = np.zeros((frustum_point_cloud.shape[0],), dtype=np.int64)\n        label_InstanceSeg[row_mask] = 1 # (0: point is NOT part of the objet, 1: point is part of the object)\n\n        ########################################################################\n        # visualization of frustum and InstanceSeg ground truth:\n        ########################################################################\n        # # # # # # # # # # debug visualizations START:\n        # import sys\n        # sys.path.append(""/home/fregu856/exjobb/Open3D/build/lib"")\n        # from py3d import *\n        #\n        # # visualize the frustum points with ground truth as colored points:\n        # frustum_pcd_camera = PointCloud()\n        # frustum_pcd_camera.points = Vector3dVector(frustum_point_cloud_xyz_camera)\n        # frustum_pcd_camera.paint_uniform_color([0.25, 0.25, 0.25])\n        # gt_pcd_camera = PointCloud()\n        # gt_pcd_camera.points = Vector3dVector(gt_point_cloud_xyz_camera)\n        # gt_pcd_camera.paint_uniform_color([0, 0, 1])\n        # draw_geometries([gt_pcd_camera, frustum_pcd_camera])\n\n        # # visualize the frustum points and gt points as differently colored points in the full point cloud:\n        # frustum_pcd_camera.paint_uniform_color([1, 0, 0])\n        # pcd_camera = PointCloud()\n        # pcd_camera.points = Vector3dVector(orig_point_cloud_camera[:, 0:3])\n        # pcd_camera.paint_uniform_color([0.25, 0.25, 0.25])\n        # draw_geometries([gt_pcd_camera, frustum_pcd_camera, pcd_camera])\n        # # # # # # # # # # debug visualizations END:\n\n        ########################################################################\n        # normalize frustum point cloud:\n        ########################################################################\n        # get the 2dbbox center img point in hom. coords:\n        u_center = u_min_expanded + (u_max_expanded - u_min_expanded)/2.0\n        v_center = v_min_expanded + (v_max_expanded - v_min_expanded)/2.0\n        center_img_point_hom = np.array([u_center, v_center, 1])\n\n        # (more than one 3D point is projected onto the center image point, i.e,\n        # the linear system of equations is under-determined and has inf number\n        # of solutions. By using the pseudo-inverse, we obtain the least-norm sol)\n\n        # get a point (the least-norm sol.) that projects onto the center image point, in hom. coords:\n        P2_pseudo_inverse = np.linalg.pinv(P2) # (shape: (4, 3)) (P2 has shape (3, 4))\n        point_hom = np.dot(P2_pseudo_inverse, center_img_point_hom)\n\n        # hom --> normal coords:\n        point = np.array(([point_hom[0]/point_hom[3], point_hom[1]/point_hom[3], point_hom[2]/point_hom[3]]))\n\n        # if the point is behind the camera, switch to the mirror point in front of the camera:\n        if point[2] < 0:\n            point[0] = -point[0]\n            point[2] = -point[2]\n\n        # compute the angle of the point in the x-z plane: ((rectified) camera coords)\n        frustum_angle = np.arctan2(point[0], point[2]) # (np.arctan2(x, z)) # (frustum_angle = 0: frustum is centered)\n\n        # rotation_matrix to rotate points frustum_angle around the y axis (counter-clockwise):\n        frustum_R = np.asarray([[np.cos(frustum_angle), 0, -np.sin(frustum_angle)],\n                           [0, 1, 0],\n                           [np.sin(frustum_angle), 0, np.cos(frustum_angle)]],\n                           dtype=\'float32\')\n\n        # rotate the frustum point cloud to center it:\n        centered_frustum_point_cloud_xyz_camera = np.dot(frustum_R, frustum_point_cloud_xyz_camera.T).T\n\n        # subtract the centered frustum train xyz mean:\n        centered_frustum_point_cloud_xyz_camera -= self.centered_frustum_mean_xyz\n\n        centered_frustum_point_cloud_camera = frustum_point_cloud_camera\n        centered_frustum_point_cloud_camera[:, 0:3] = centered_frustum_point_cloud_xyz_camera\n\n        # # # # # # # # # # debug visualizations START:\n        # import sys\n        # sys.path.append(""/home/fregu856/exjobb/Open3D/build/lib"")\n        # from py3d import *\n        #\n        # frustum_pcd_camera = PointCloud()\n        # frustum_pcd_camera.points = Vector3dVector(frustum_point_cloud_xyz_camera)\n        # frustum_pcd_camera.paint_uniform_color([0.25, 0.25, 0.25])\n        #\n        # centered_frustum_pcd_camera = PointCloud()\n        # centered_frustum_pcd_camera.points = Vector3dVector(centered_frustum_point_cloud_xyz_camera)\n        # centered_frustum_pcd_camera.paint_uniform_color([1, 0, 0])\n        # draw_geometries([centered_frustum_pcd_camera, frustum_pcd_camera])\n        # # # # # # # # # # debug visualizations END:\n\n        ########################################################################\n        # TNet ground truth:\n        ########################################################################\n        label_TNet = np.dot(frustum_R, label_3D[""center""]) - self.centered_frustum_mean_xyz\n\n        # # # # # # # # # # debug visualizations START:\n        # import sys\n        # sys.path.append(""/home/fregu856/exjobb/Open3D/build/lib"")\n        # from py3d import *\n        #\n        # # visualize InstanceSeg GT and 3dbbox center GT:\n        # frustum_pcd_camera = PointCloud()\n        # frustum_pcd_camera.points = Vector3dVector(frustum_point_cloud_xyz_camera)\n        # frustum_pcd_camera.paint_uniform_color([0.25, 0.25, 0.25cd\n        # gt_pcd_camera = PointCloud()\n        # gt_pcd_camera.points = Vector3dVector(gt_point_cloud_xyz_camera)\n        # gt_pcd_camera.paint_uniform_color([0, 0, 1])\n        # center_gt_pcd_camera = PointCloud()\n        # center_gt_pcd_camera.points = Vector3dVector(np.array([label_TNet]))\n        # center_gt_pcd_camera.paint_uniform_color([0, 1, 0])\n        # draw_geometries([center_gt_pcd_camera, gt_pcd_camera, frustum_pcd_camera])\n        # # # # # # # # # # debug visualizations END:\n\n        ########################################################################\n        # BboxNet ground truth:\n        ########################################################################\n        centered_r_y = wrapToPi(label_3D[\'r_y\'] - frustum_angle)\n        bin_number = getBinNumber(centered_r_y, NH=self.NH)\n        bin_center = getBinCenter(bin_number, NH=self.NH)\n        residual = wrapToPi(centered_r_y - bin_center)\n\n        label_BboxNet = np.zeros((11, ), dtype=np.float32) # ([x, y, z, h, w, l, r_y_bin_number, r_y_residual, r_y_bin_number_neighbor,r_y_residual_neighbor, h_mean, w_mean, l_mean])\n\n        label_BboxNet[0:3] = np.dot(frustum_R, label_3D[""center""]) - self.centered_frustum_mean_xyz\n        label_BboxNet[3] = label_3D[\'h\']\n        label_BboxNet[4] = label_3D[\'w\']\n        label_BboxNet[5] = label_3D[\'l\']\n        label_BboxNet[6] = bin_number\n        label_BboxNet[7] = residual\n        label_BboxNet[8:] = self.mean_car_size\n\n        ########################################################################\n        # corner loss ground truth:\n        ########################################################################\n        Rmat = np.asarray([[math.cos(residual), 0, math.sin(residual)],\n                           [0, 1, 0],\n                           [-math.sin(residual), 0, math.cos(residual)]],\n                           dtype=\'float32\')\n\n        center = label_BboxNet[0:3]\n        l = label_3D[\'l\']\n        w = label_3D[\'w\']\n        h = label_3D[\'h\']\n        p0 = center + np.dot(Rmat, np.asarray([l/2.0, 0, w/2.0], dtype=\'float32\').flatten())\n        p1 = center + np.dot(Rmat, np.asarray([-l/2.0, 0, w/2.0], dtype=\'float32\').flatten())\n        p2 = center + np.dot(Rmat, np.asarray([-l/2.0, 0, -w/2.0], dtype=\'float32\').flatten())\n        p3 = center + np.dot(Rmat, np.asarray([l/2.0, 0, -w/2.0], dtype=\'float32\').flatten())\n        p4 = center + np.dot(Rmat, np.asarray([l/2.0, -h, w/2.0], dtype=\'float32\').flatten())\n        p5 = center + np.dot(Rmat, np.asarray([-l/2.0, -h, w/2.0], dtype=\'float32\').flatten())\n        p6 = center + np.dot(Rmat, np.asarray([-l/2.0, -h, -w/2.0], dtype=\'float32\').flatten())\n        p7 = center + np.dot(Rmat, np.asarray([l/2.0, -h, -w/2.0], dtype=\'float32\').flatten())\n        label_corner = np.array([p0, p1, p2, p3, p4, p5, p6, p7])\n        label_corner_flipped = np.array([p2, p3, p0, p1, p6, p7, p4, p5])\n\n        ########################################################################\n\n        centered_frustum_point_cloud_camera = torch.from_numpy(centered_frustum_point_cloud_camera) # (shape: (1024, 4))\n        label_InstanceSeg = torch.from_numpy(label_InstanceSeg) # (shape: (1024, ))\n        label_TNet = torch.from_numpy(label_TNet) # (shape: (3, ))\n        label_BboxNet = torch.from_numpy(label_BboxNet) # (shape: (11, ))\n        label_corner = torch.from_numpy(label_corner) # (shape: (8, 3))\n        label_corner_flipped = torch.from_numpy(label_corner_flipped) # (shape: (8, 3))\n\n        return (centered_frustum_point_cloud_camera, label_InstanceSeg, label_TNet, label_BboxNet, label_corner, label_corner_flipped, img_id, input_2Dbbox, frustum_R, frustum_angle, self.centered_frustum_mean_xyz)\n\n    def __len__(self):\n        return self.num_examples\n\nclass EvalSequenceDatasetFrustumPointNet(torch.utils.data.Dataset):\n    def __init__(self, kitti_data_path, kitti_meta_path, NH, sequence=""0000""):\n        self.img_dir = kitti_data_path + ""/tracking/training/image_02/"" + sequence + ""/""\n        self.lidar_dir = kitti_data_path + ""/tracking/training/velodyne/"" + sequence + ""/""\n        self.label_path = kitti_data_path + ""/tracking/training/label_02/"" + sequence + "".txt""\n        self.calib_path = kitti_meta_path + ""/tracking/training/calib/"" + sequence + "".txt"" # NOTE! NOTE! the data format for the calib files was sliightly different for tracking, so I manually modifed the 20 files and saved them in the kitti_meta folder\n\n        self.NH = NH\n\n        with open(kitti_meta_path + ""/kitti_train_mean_car_size.pkl"", ""rb"") as file: # (needed for python3)\n            self.mean_car_size = pickle.load(file)\n\n        with open(kitti_meta_path + ""/kitti_centered_frustum_mean_xyz.pkl"", ""rb"") as file: # (needed for python3)\n            self.centered_frustum_mean_xyz = pickle.load(file)\n            self.centered_frustum_mean_xyz = self.centered_frustum_mean_xyz.astype(np.float32)\n\n        img_ids = []\n        img_names = os.listdir(self.img_dir)\n        for img_name in img_names:\n            img_id = img_name.split("".png"")[0]\n            img_ids.append(img_id)\n\n        self.examples = []\n        for img_id in img_ids:\n            if img_id.lstrip(\'0\') == \'\':\n                img_id_float = 0.0\n            else:\n                img_id_float = float(img_id.lstrip(\'0\'))\n\n            labels = LabelLoader2D3D_sequence(img_id, img_id_float, self.label_path, self.calib_path)\n\n            for label in labels:\n                label_2d = label[""label_2D""]\n                if label_2d[""truncated""] < 0.5 and label_2d[""class""] == ""Car"":\n                    label[""img_id""] = img_id\n                    self.examples.append(label)\n\n        self.num_examples = len(self.examples)\n\n    def __getitem__(self, index):\n        example = self.examples[index]\n\n        img_id = example[""img_id""]\n        #print(img_id)\n\n        lidar_path = self.lidar_dir + img_id + "".bin""\n        point_cloud = np.fromfile(lidar_path, dtype=np.float32).reshape(-1, 4)\n        orig_point_cloud = point_cloud\n\n        # remove points that are located behind the camera:\n        point_cloud = point_cloud[point_cloud[:, 0] > 0, :]\n        # remove points that are located too far away from the camera:\n        point_cloud = point_cloud[point_cloud[:, 0] < 80, :]\n\n        calib = calibread(self.calib_path)\n        P2 = calib[""P2""]\n        Tr_velo_to_cam_orig = calib[""Tr_velo_to_cam""]\n        R0_rect_orig = calib[""R0_rect""]\n        #\n        R0_rect = np.eye(4)\n        R0_rect[0:3, 0:3] = R0_rect_orig\n        #\n        Tr_velo_to_cam = np.eye(4)\n        Tr_velo_to_cam[0:3, :] = Tr_velo_to_cam_orig\n\n        point_cloud_xyz = point_cloud[:, 0:3]\n        point_cloud_xyz_hom = np.ones((point_cloud.shape[0], 4))\n        point_cloud_xyz_hom[:, 0:3] = point_cloud[:, 0:3] # (point_cloud_xyz_hom has shape (num_points, 4))\n\n        # project the points onto the image plane (homogeneous coords):\n        img_points_hom = np.dot(P2, np.dot(R0_rect, np.dot(Tr_velo_to_cam, point_cloud_xyz_hom.T))).T # (point_cloud_xyz_hom.T has shape (4, num_points))\n        # normalize:\n        img_points = np.zeros((img_points_hom.shape[0], 2))\n        img_points[:, 0] = img_points_hom[:, 0]/img_points_hom[:, 2]\n        img_points[:, 1] = img_points_hom[:, 1]/img_points_hom[:, 2]\n\n        # transform the points into (rectified) camera coordinates:\n        point_cloud_xyz_camera_hom = np.dot(R0_rect, np.dot(Tr_velo_to_cam, point_cloud_xyz_hom.T)).T # (point_cloud_xyz_hom.T has shape (4, num_points))\n        # normalize:\n        point_cloud_xyz_camera = np.zeros((point_cloud_xyz_camera_hom.shape[0], 3))\n        point_cloud_xyz_camera[:, 0] = point_cloud_xyz_camera_hom[:, 0]/point_cloud_xyz_camera_hom[:, 3]\n        point_cloud_xyz_camera[:, 1] = point_cloud_xyz_camera_hom[:, 1]/point_cloud_xyz_camera_hom[:, 3]\n        point_cloud_xyz_camera[:, 2] = point_cloud_xyz_camera_hom[:, 2]/point_cloud_xyz_camera_hom[:, 3]\n\n        point_cloud_camera = point_cloud\n        point_cloud_camera[:, 0:3] = point_cloud_xyz_camera\n\n        label_2D = example[""label_2D""]\n        label_3D = example[""label_3D""]\n\n        bbox = label_2D[""poly""]\n\n        # img = cv2.imread(self.img_dir + img_id + "".png"", -1)\n        # img_with_bboxes = draw_2d_polys(img, [label_2D])\n        # cv2.imwrite(""test.png"", img_with_bboxes)\n\n        ########################################################################\n        # frustum:\n        ########################################################################\n        u_min = bbox[0, 0] # (left)\n        u_max = bbox[1, 0] # (rigth)\n        v_min = bbox[0, 1] # (top)\n        v_max = bbox[2, 1] # (bottom)\n\n        # expand the 2D bbox slightly:\n        u_min_expanded = u_min #- (u_max-u_min)*0.05\n        u_max_expanded = u_max #+ (u_max-u_min)*0.05\n        v_min_expanded = v_min #- (v_max-v_min)*0.05\n        v_max_expanded = v_max #+ (v_max-v_min)*0.05\n        input_2Dbbox = np.array([u_min_expanded, u_max_expanded, v_min_expanded, v_max_expanded])\n\n        row_mask = np.logical_and(\n                    np.logical_and(img_points[:, 0] >= u_min_expanded,\n                                   img_points[:, 0] <= u_max_expanded),\n                    np.logical_and(img_points[:, 1] >= v_min_expanded,\n                                   img_points[:, 1] <= v_max_expanded))\n\n        frustum_point_cloud_xyz = point_cloud_xyz[row_mask, :] # (needed only for visualization)\n        frustum_point_cloud = point_cloud[row_mask, :]\n        frustum_point_cloud_xyz_camera = point_cloud_xyz_camera[row_mask, :]\n        frustum_point_cloud_camera = point_cloud_camera[row_mask, :]\n\n        if frustum_point_cloud.shape[0] == 0:\n            print (img_id)\n            print (frustum_point_cloud.shape)\n            return self.__getitem__(0)\n\n        # randomly sample 1024 points in the frustum point cloud:\n        if frustum_point_cloud.shape[0] < 1024:\n            row_idx = np.random.choice(frustum_point_cloud.shape[0], 1024, replace=True)\n        else:\n            row_idx = np.random.choice(frustum_point_cloud.shape[0], 1024, replace=False)\n        frustum_point_cloud_xyz = frustum_point_cloud_xyz[row_idx, :]\n        frustum_point_cloud = frustum_point_cloud[row_idx, :]\n        frustum_point_cloud_xyz_camera = frustum_point_cloud_xyz_camera[row_idx, :]\n        frustum_point_cloud_camera = frustum_point_cloud_camera[row_idx, :]\n        # (the frustum point cloud now has exactly 1024 points)\n\n        ########################################################################\n        # InstanceSeg ground truth:\n        ########################################################################\n        points = label_3D[""points""]\n\n        y_max = points[0, 1]\n        y_min = points[4, 1]\n\n        # D, A, B are consecutive corners of the rectangle (projection of the 3D bbox) in the x,z plane\n        # the vectors AB and AD are orthogonal\n        # a point P = (x, z) lies within the rectangle in the x,z plane iff:\n        # (0 < AP dot AB < AB dot AB) && (0 < AP dot AD < AD dot AD)\n        # (https://math.stackexchange.com/a/190373)\n\n        A = np.array([points[0, 0], points[0, 2]])\n        B = np.array([points[1, 0], points[1, 2]])\n        D = np.array([points[3, 0], points[3, 2]])\n\n        AB = B - A\n        AD = D - A\n        AB_dot_AB = np.dot(AB, AB)\n        AD_dot_AD = np.dot(AD, AD)\n\n        P = np.zeros((frustum_point_cloud_xyz_camera.shape[0], 2))\n        P[:, 0] = frustum_point_cloud_xyz_camera[:, 0]\n        P[:, 1] = frustum_point_cloud_xyz_camera[:, 2]\n\n        AP = P - A\n        AP_dot_AB = np.dot(AP, AB)\n        AP_dot_AD = np.dot(AP, AD)\n\n        row_mask = np.logical_and(\n                    np.logical_and(frustum_point_cloud_xyz_camera[:, 1] >= y_min, frustum_point_cloud_xyz_camera[:, 1] <= y_max),\n                    np.logical_and(np.logical_and(AP_dot_AB >= 0, AP_dot_AB <= AB_dot_AB),\n                                   np.logical_and(AP_dot_AD >= 0, AP_dot_AD <= AD_dot_AD)))\n\n        gt_point_cloud_xyz_camera = frustum_point_cloud_xyz_camera[row_mask, :] # (needed only for visualization)\n\n        label_InstanceSeg = np.zeros((frustum_point_cloud.shape[0],), dtype=np.int64)\n        label_InstanceSeg[row_mask] = 1 # (0: point is NOT part of the objet, 1: point is part of the object)\n\n        ########################################################################\n        # visualization of frustum and InstanceSeg ground truth:\n        ########################################################################\n        # # # # # # # # # # debug visualizations START:\n        # import sys\n        # sys.path.append(""/home/fregu856/exjobb/Open3D/build/lib"")\n        # from py3d import *\n        #\n        # # visualize the frustum points with ground truth as colored points:\n        # frustum_pcd_camera = PointCloud()\n        # frustum_pcd_camera.points = Vector3dVector(frustum_point_cloud_xyz_camera)\n        # frustum_pcd_camera.paint_uniform_color([0.25, 0.25, 0.25])\n        # gt_pcd_camera = PointCloud()\n        # gt_pcd_camera.points = Vector3dVector(gt_point_cloud_xyz_camera)\n        # gt_pcd_camera.paint_uniform_color([0, 0, 1])\n        # draw_geometries([gt_pcd_camera, frustum_pcd_camera])\n\n        # # visualize the frustum points and gt points as differently colored points in the full point cloud:\n        # frustum_pcd_camera.paint_uniform_color([1, 0, 0])\n        # pcd_camera = PointCloud()\n        # pcd_camera.points = Vector3dVector(orig_point_cloud_camera[:, 0:3])\n        # pcd_camera.paint_uniform_color([0.25, 0.25, 0.25])\n        # draw_geometries([gt_pcd_camera, frustum_pcd_camera, pcd_camera])\n        # # # # # # # # # # debug visualizations END:\n\n        ########################################################################\n        # normalize frustum point cloud:\n        ########################################################################\n        # get the 2dbbox center img point in hom. coords:\n        u_center = u_min_expanded + (u_max_expanded - u_min_expanded)/2.0\n        v_center = v_min_expanded + (v_max_expanded - v_min_expanded)/2.0\n        center_img_point_hom = np.array([u_center, v_center, 1])\n\n        # (more than one 3D point is projected onto the center image point, i.e,\n        # the linear system of equations is under-determined and has inf number\n        # of solutions. By using the pseudo-inverse, we obtain the least-norm sol)\n\n        # get a point (the least-norm sol.) that projects onto the center image point, in hom. coords:\n        P2_pseudo_inverse = np.linalg.pinv(P2) # (shape: (4, 3)) (P2 has shape (3, 4))\n        point_hom = np.dot(P2_pseudo_inverse, center_img_point_hom)\n\n        # hom --> normal coords:\n        point = np.array(([point_hom[0]/point_hom[3], point_hom[1]/point_hom[3], point_hom[2]/point_hom[3]]))\n\n        # if the point is behind the camera, switch to the mirror point in front of the camera:\n        if point[2] < 0:\n            point[0] = -point[0]\n            point[2] = -point[2]\n\n        # compute the angle of the point in the x-z plane: ((rectified) camera coords)\n        frustum_angle = np.arctan2(point[0], point[2]) # (np.arctan2(x, z)) # (frustum_angle = 0: frustum is centered)\n\n        # rotation_matrix to rotate points frustum_angle around the y axis (counter-clockwise):\n        frustum_R = np.asarray([[np.cos(frustum_angle), 0, -np.sin(frustum_angle)],\n                           [0, 1, 0],\n                           [np.sin(frustum_angle), 0, np.cos(frustum_angle)]],\n                           dtype=\'float32\')\n\n        # rotate the frustum point cloud to center it:\n        centered_frustum_point_cloud_xyz_camera = np.dot(frustum_R, frustum_point_cloud_xyz_camera.T).T\n\n        # subtract the centered frustum train xyz mean:\n        centered_frustum_point_cloud_xyz_camera -= self.centered_frustum_mean_xyz\n\n        centered_frustum_point_cloud_camera = frustum_point_cloud_camera\n        centered_frustum_point_cloud_camera[:, 0:3] = centered_frustum_point_cloud_xyz_camera\n\n        # # # # # # # # # # debug visualizations START:\n        # import sys\n        # sys.path.append(""/home/fregu856/exjobb/Open3D/build/lib"")\n        # from py3d import *\n        #\n        # frustum_pcd_camera = PointCloud()\n        # frustum_pcd_camera.points = Vector3dVector(frustum_point_cloud_xyz_camera)\n        # frustum_pcd_camera.paint_uniform_color([0.25, 0.25, 0.25])\n        #\n        # centered_frustum_pcd_camera = PointCloud()\n        # centered_frustum_pcd_camera.points = Vector3dVector(centered_frustum_point_cloud_xyz_camera)\n        # centered_frustum_pcd_camera.paint_uniform_color([1, 0, 0])\n        # draw_geometries([centered_frustum_pcd_camera, frustum_pcd_camera])\n        # # # # # # # # # # debug visualizations END:\n\n        ########################################################################\n        # TNet ground truth:\n        ########################################################################\n        label_TNet = np.dot(frustum_R, label_3D[""center""]) - self.centered_frustum_mean_xyz\n\n        # # # # # # # # # # debug visualizations START:\n        # import sys\n        # sys.path.append(""/home/fregu856/exjobb/Open3D/build/lib"")\n        # from py3d import *\n        #\n        # # visualize InstanceSeg GT and 3dbbox center GT:\n        # frustum_pcd_camera = PointCloud()\n        # frustum_pcd_camera.points = Vector3dVector(frustum_point_cloud_xyz_camera)\n        # frustum_pcd_camera.paint_uniform_color([0.25, 0.25, 0.25cd\n        # gt_pcd_camera = PointCloud()\n        # gt_pcd_camera.points = Vector3dVector(gt_point_cloud_xyz_camera)\n        # gt_pcd_camera.paint_uniform_color([0, 0, 1])\n        # center_gt_pcd_camera = PointCloud()\n        # center_gt_pcd_camera.points = Vector3dVector(np.array([label_TNet]))\n        # center_gt_pcd_camera.paint_uniform_color([0, 1, 0])\n        # draw_geometries([center_gt_pcd_camera, gt_pcd_camera, frustum_pcd_camera])\n        # # # # # # # # # # debug visualizations END:\n\n        ########################################################################\n        # BboxNet ground truth:\n        ########################################################################\n        centered_r_y = wrapToPi(label_3D[\'r_y\'] - frustum_angle)\n        bin_number = getBinNumber(centered_r_y, NH=self.NH)\n        bin_center = getBinCenter(bin_number, NH=self.NH)\n        residual = wrapToPi(centered_r_y - bin_center)\n\n        label_BboxNet = np.zeros((11, ), dtype=np.float32) # ([x, y, z, h, w, l, r_y_bin_number, r_y_residual, r_y_bin_number_neighbor,r_y_residual_neighbor, h_mean, w_mean, l_mean])\n\n        label_BboxNet[0:3] = np.dot(frustum_R, label_3D[""center""]) - self.centered_frustum_mean_xyz\n        label_BboxNet[3] = label_3D[\'h\']\n        label_BboxNet[4] = label_3D[\'w\']\n        label_BboxNet[5] = label_3D[\'l\']\n        label_BboxNet[6] = bin_number\n        label_BboxNet[7] = residual\n        label_BboxNet[8:] = self.mean_car_size\n\n        ########################################################################\n        # corner loss ground truth:\n        ########################################################################\n        Rmat = np.asarray([[math.cos(residual), 0, math.sin(residual)],\n                           [0, 1, 0],\n                           [-math.sin(residual), 0, math.cos(residual)]],\n                           dtype=\'float32\')\n\n        center = label_BboxNet[0:3]\n        l = label_3D[\'l\']\n        w = label_3D[\'w\']\n        h = label_3D[\'h\']\n        p0 = center + np.dot(Rmat, np.asarray([l/2.0, 0, w/2.0], dtype=\'float32\').flatten())\n        p1 = center + np.dot(Rmat, np.asarray([-l/2.0, 0, w/2.0], dtype=\'float32\').flatten())\n        p2 = center + np.dot(Rmat, np.asarray([-l/2.0, 0, -w/2.0], dtype=\'float32\').flatten())\n        p3 = center + np.dot(Rmat, np.asarray([l/2.0, 0, -w/2.0], dtype=\'float32\').flatten())\n        p4 = center + np.dot(Rmat, np.asarray([l/2.0, -h, w/2.0], dtype=\'float32\').flatten())\n        p5 = center + np.dot(Rmat, np.asarray([-l/2.0, -h, w/2.0], dtype=\'float32\').flatten())\n        p6 = center + np.dot(Rmat, np.asarray([-l/2.0, -h, -w/2.0], dtype=\'float32\').flatten())\n        p7 = center + np.dot(Rmat, np.asarray([l/2.0, -h, -w/2.0], dtype=\'float32\').flatten())\n        label_corner = np.array([p0, p1, p2, p3, p4, p5, p6, p7])\n        label_corner_flipped = np.array([p2, p3, p0, p1, p6, p7, p4, p5])\n\n        ########################################################################\n\n        centered_frustum_point_cloud_camera = torch.from_numpy(centered_frustum_point_cloud_camera) # (shape: (1024, 4))\n        label_InstanceSeg = torch.from_numpy(label_InstanceSeg) # (shape: (1024, ))\n        label_TNet = torch.from_numpy(label_TNet) # (shape: (3, ))\n        label_BboxNet = torch.from_numpy(label_BboxNet) # (shape: (11, ))\n        label_corner = torch.from_numpy(label_corner) # (shape: (8, 3))\n        label_corner_flipped = torch.from_numpy(label_corner_flipped) # (shape: (8, 3))\n\n        return (centered_frustum_point_cloud_camera, label_InstanceSeg, label_TNet, label_BboxNet, label_corner, label_corner_flipped, img_id, input_2Dbbox, frustum_R, frustum_angle, self.centered_frustum_mean_xyz)\n\n    def __len__(self):\n        return self.num_examples\n\nclass DatasetKittiTest(torch.utils.data.Dataset):\n    def __init__(self, kitti_data_path, kitti_meta_path, NH):\n        self.img_dir = kitti_data_path + ""/object/testing/image_2/""\n        self.calib_dir = kitti_data_path + ""/object/testing/calib/""\n        self.lidar_dir = kitti_data_path + ""/object/testing/velodyne/""\n        self.detections_2d_dir = kitti_meta_path + ""/object/testing/2d_detections/""\n\n        self.NH = NH\n\n        with open(kitti_meta_path + ""/kitti_train_mean_car_size.pkl"", ""rb"") as file: # (needed for python3)\n            self.mean_car_size = pickle.load(file)\n\n        with open(kitti_meta_path + ""/kitti_centered_frustum_mean_xyz.pkl"", ""rb"") as file: # (needed for python3)\n            self.centered_frustum_mean_xyz = pickle.load(file)\n            self.centered_frustum_mean_xyz = self.centered_frustum_mean_xyz.astype(np.float32)\n\n        img_ids = []\n        img_names = os.listdir(self.img_dir)\n        for img_name in img_names:\n            img_id = img_name.split("".png"")[0]\n            img_ids.append(img_id)\n\n        self.examples = []\n        for img_id in img_ids:\n            detections_file_path = self.detections_2d_dir + img_id + "".txt""\n            with open(detections_file_path) as file:\n                # line format: img_id, img_height, img_width, class, u_min, v_min, u_max, v_max, confidence score, distance estimate\n                for line in file:\n                    values = line.split()\n                    object_class = float(values[3])\n                    if object_class == 1: # (1: Car)\n                        u_min = float(values[4])\n                        v_min = float(values[5])\n                        u_max = float(values[6])\n                        v_max = float(values[7])\n                        score_2d = float(values[8])\n\n                        detection_2d = {}\n                        detection_2d[""u_min""] = u_min\n                        detection_2d[""v_min""] = v_min\n                        detection_2d[""u_max""] = u_max\n                        detection_2d[""v_max""] = v_max\n                        detection_2d[""score_2d""] = score_2d\n                        detection_2d[""img_id""] = img_id\n\n                        self.examples.append(detection_2d)\n\n        self.num_examples = len(self.examples)\n\n    def __getitem__(self, index):\n        example = self.examples[index]\n\n        img_id = example[""img_id""]\n\n        lidar_path = self.lidar_dir + img_id + "".bin""\n        point_cloud = np.fromfile(lidar_path, dtype=np.float32).reshape(-1, 4)\n        orig_point_cloud = point_cloud\n\n        # remove points that are located behind the camera:\n        point_cloud = point_cloud[point_cloud[:, 0] > -5, :]\n        # remove points that are located too far away from the camera:\n        point_cloud = point_cloud[point_cloud[:, 0] < 80, :]\n\n        calib = calibread(self.calib_dir + img_id + "".txt"")\n        P2 = calib[""P2""]\n        Tr_velo_to_cam_orig = calib[""Tr_velo_to_cam""]\n        R0_rect_orig = calib[""R0_rect""]\n        #\n        R0_rect = np.eye(4)\n        R0_rect[0:3, 0:3] = R0_rect_orig\n        #\n        Tr_velo_to_cam = np.eye(4)\n        Tr_velo_to_cam[0:3, :] = Tr_velo_to_cam_orig\n\n        point_cloud_xyz = point_cloud[:, 0:3]\n        point_cloud_xyz_hom = np.ones((point_cloud.shape[0], 4))\n        point_cloud_xyz_hom[:, 0:3] = point_cloud[:, 0:3] # (point_cloud_xyz_hom has shape (num_points, 4))\n\n        # project the points onto the image plane (homogeneous coords):\n        img_points_hom = np.dot(P2, np.dot(R0_rect, np.dot(Tr_velo_to_cam, point_cloud_xyz_hom.T))).T # (point_cloud_xyz_hom.T has shape (4, num_points))\n        # normalize:\n        img_points = np.zeros((img_points_hom.shape[0], 2))\n        img_points[:, 0] = img_points_hom[:, 0]/img_points_hom[:, 2]\n        img_points[:, 1] = img_points_hom[:, 1]/img_points_hom[:, 2]\n\n        # transform the points into (rectified) camera coordinates:\n        point_cloud_xyz_camera_hom = np.dot(R0_rect, np.dot(Tr_velo_to_cam, point_cloud_xyz_hom.T)).T # (point_cloud_xyz_hom.T has shape (4, num_points))\n        # normalize:\n        point_cloud_xyz_camera = np.zeros((point_cloud_xyz_camera_hom.shape[0], 3))\n        point_cloud_xyz_camera[:, 0] = point_cloud_xyz_camera_hom[:, 0]/point_cloud_xyz_camera_hom[:, 3]\n        point_cloud_xyz_camera[:, 1] = point_cloud_xyz_camera_hom[:, 1]/point_cloud_xyz_camera_hom[:, 3]\n        point_cloud_xyz_camera[:, 2] = point_cloud_xyz_camera_hom[:, 2]/point_cloud_xyz_camera_hom[:, 3]\n\n        point_cloud_camera = point_cloud\n        point_cloud_camera[:, 0:3] = point_cloud_xyz_camera\n\n        ########################################################################\n        # frustum:\n        ########################################################################\n        u_min = example[""u_min""] # (left)\n        u_max = example[""u_max""] # (rigth)\n        v_min = example[""v_min""] # (top)\n        v_max = example[""v_max""] # (bottom)\n\n        score_2d = example[""score_2d""]\n\n        input_2Dbbox = np.array([u_min, u_max, v_min, v_max])\n\n        row_mask = np.logical_and(\n                    np.logical_and(img_points[:, 0] >= u_min,\n                                   img_points[:, 0] <= u_max),\n                    np.logical_and(img_points[:, 1] >= v_min,\n                                   img_points[:, 1] <= v_max))\n\n        frustum_point_cloud_xyz = point_cloud_xyz[row_mask, :] # (needed only for visualization)\n        frustum_point_cloud = point_cloud[row_mask, :]\n        frustum_point_cloud_xyz_camera = point_cloud_xyz_camera[row_mask, :]\n        frustum_point_cloud_camera = point_cloud_camera[row_mask, :]\n\n        empty_frustum_flag = 0\n        if frustum_point_cloud.shape[0] == 0:\n            empty_frustum_flag = 1\n            frustum_point_cloud = np.zeros((1024, 4), dtype=np.float32)\n            frustum_point_cloud_xyz = np.zeros((1024, 3), dtype=np.float32)\n            frustum_point_cloud_camera = np.zeros((1024, 4), dtype=np.float32)\n            frustum_point_cloud_xyz_camera = np.zeros((1024, 3), dtype=np.float32)\n\n        # randomly sample 1024 points in the frustum point cloud:\n        if frustum_point_cloud.shape[0] < 1024:\n            row_idx = np.random.choice(frustum_point_cloud.shape[0], 1024, replace=True)\n        else:\n            row_idx = np.random.choice(frustum_point_cloud.shape[0], 1024, replace=False)\n        frustum_point_cloud_xyz = frustum_point_cloud_xyz[row_idx, :]\n        frustum_point_cloud = frustum_point_cloud[row_idx, :]\n        frustum_point_cloud_xyz_camera = frustum_point_cloud_xyz_camera[row_idx, :]\n        frustum_point_cloud_camera = frustum_point_cloud_camera[row_idx, :]\n        # (the frustum point cloud now has exactly 1024 points)\n\n        ########################################################################\n        # normalize frustum point cloud:\n        ########################################################################\n        # get the 2dbbox center img point in hom. coords:\n        u_center = u_min + (u_max - u_min)/2.0\n        v_center = v_min + (v_max - v_min)/2.0\n        center_img_point_hom = np.array([u_center, v_center, 1])\n\n        # (more than one 3D point is projected onto the center image point, i.e,\n        # the linear system of equations is under-determined and has inf number\n        # of solutions. By using the pseudo-inverse, we obtain the least-norm sol)\n\n        # get a point (the least-norm sol.) that projects onto the center image point, in hom. coords:\n        P2_pseudo_inverse = np.linalg.pinv(P2) # (shape: (4, 3)) (P2 has shape (3, 4))\n        point_hom = np.dot(P2_pseudo_inverse, center_img_point_hom)\n\n        # hom --> normal coords:\n        point = np.array(([point_hom[0]/point_hom[3], point_hom[1]/point_hom[3], point_hom[2]/point_hom[3]]))\n\n        # if the point is behind the camera, switch to the mirror point in front of the camera:\n        if point[2] < 0:\n            point[0] = -point[0]\n            point[2] = -point[2]\n\n        # compute the angle of the point in the x-z plane: ((rectified) camera coords)\n        frustum_angle = np.arctan2(point[0], point[2]) # (np.arctan2(x, z)) # (frustum_angle = 0: frustum is centered)\n\n        # rotation_matrix to rotate points frustum_angle around the y axis (counter-clockwise):\n        frustum_R = np.asarray([[np.cos(frustum_angle), 0, -np.sin(frustum_angle)],\n                           [0, 1, 0],\n                           [np.sin(frustum_angle), 0, np.cos(frustum_angle)]],\n                           dtype=\'float32\')\n\n        # rotate the frustum point cloud to center it:\n        centered_frustum_point_cloud_xyz_camera = np.dot(frustum_R, frustum_point_cloud_xyz_camera.T).T\n\n        # subtract the centered frustum train xyz mean:\n        centered_frustum_point_cloud_xyz_camera -= self.centered_frustum_mean_xyz\n\n        centered_frustum_point_cloud_camera = frustum_point_cloud_camera\n        centered_frustum_point_cloud_camera[:, 0:3] = centered_frustum_point_cloud_xyz_camera\n\n        # # # # # # # # # # debug visualizations START:\n        # import sys\n        # sys.path.append(""/home/fregu856/exjobb/Open3D/build/lib"")\n        # from py3d import *\n        # frustum_pcd_camera = PointCloud()\n        # frustum_pcd_camera.points = Vector3dVector(frustum_point_cloud_xyz_camera)\n        # frustum_pcd_camera.paint_uniform_color([0.25, 0.25, 0.25])\n        # centered_frustum_pcd_camera = PointCloud()\n        # centered_frustum_pcd_camera.points = Vector3dVector(centered_frustum_point_cloud_xyz_camera)\n        # centered_frustum_pcd_camera.paint_uniform_color([1, 0, 0])\n        # draw_geometries([centered_frustum_pcd_camera, frustum_pcd_camera])\n        # # # # # # # # # # debug visualizations END:\n\n        centered_frustum_point_cloud_camera = torch.from_numpy(centered_frustum_point_cloud_camera) # (shape: (1024, 4))\n\n        return (centered_frustum_point_cloud_camera, img_id, input_2Dbbox, frustum_R, frustum_angle, empty_frustum_flag, self.centered_frustum_mean_xyz, self.mean_car_size, score_2d)\n\n    def __len__(self):\n        return self.num_examples\n\n# test = DatasetKittiTest(""/home/fregu856/exjobb/data/kitti"", ""/home/fregu856/exjobb/data/kitti/meta"", NH=4)\n# # for i in range(10):\n# #     print(test.__getitem__(i))\n#\n# test_loader = torch.utils.data.DataLoader(dataset=test,\n#                                           batch_size=2, shuffle=False,\n#                                           num_workers=16)\n# for step, (frustum_point_clouds, img_ids, input_2Dbboxes, frustum_Rs, frustum_angles, empty_frustum_flags, centered_frustum_mean_xyz, mean_car_size) in enumerate(test_loader):\n#     print (mean_car_size)\n\nclass DatasetKittiTestSequence(torch.utils.data.Dataset):\n    def __init__(self, kitti_data_path, kitti_meta_path, NH, sequence):\n        self.img_dir = kitti_data_path + ""/tracking/testing/image_02/"" + sequence + ""/""\n        self.lidar_dir = kitti_data_path + ""/tracking/testing/velodyne/"" + sequence + ""/""\n        self.calib_path = kitti_meta_path + ""/tracking/testing/calib/"" + sequence + "".txt"" # NOTE! NOTE! the data format for the calib files was sliightly different for tracking, so I manually modifed the 28 files and saved them in the kitti_meta folder\n        self.detections_2d_path = kitti_meta_path + ""/tracking/testing/2d_detections/"" + sequence + ""/inferResult_1.txt""\n\n        self.NH = NH\n\n        with open(kitti_meta_path + ""/kitti_train_mean_car_size.pkl"", ""rb"") as file: # (needed for python3)\n            self.mean_car_size = pickle.load(file)\n\n        with open(kitti_meta_path + ""/kitti_centered_frustum_mean_xyz.pkl"", ""rb"") as file: # (needed for python3)\n            self.centered_frustum_mean_xyz = pickle.load(file)\n            self.centered_frustum_mean_xyz = self.centered_frustum_mean_xyz.astype(np.float32)\n\n        self.examples = []\n        with open(self.detections_2d_path) as file:\n            # line format: img_id, img_height, img_width, class, u_min, v_min, u_max, v_max, confidence score, distance estimate\n            for line in file:\n                values = line.split()\n                object_class = float(values[3])\n                if object_class == 1: # (1: Car)\n                    img_id = values[0]\n                    u_min = float(values[4])\n                    v_min = float(values[5])\n                    u_max = float(values[6])\n                    v_max = float(values[7])\n                    score_2d = float(values[8])\n\n                    detection_2d = {}\n                    detection_2d[""u_min""] = u_min\n                    detection_2d[""v_min""] = v_min\n                    detection_2d[""u_max""] = u_max\n                    detection_2d[""v_max""] = v_max\n                    detection_2d[""score_2d""] = score_2d\n                    detection_2d[""img_id""] = img_id\n\n                    self.examples.append(detection_2d)\n\n        self.num_examples = len(self.examples)\n\n    def __getitem__(self, index):\n        example = self.examples[index]\n\n        img_id = example[""img_id""]\n\n        lidar_path = self.lidar_dir + img_id + "".bin""\n        point_cloud = np.fromfile(lidar_path, dtype=np.float32).reshape(-1, 4)\n        orig_point_cloud = point_cloud\n\n        # remove points that are located behind the camera:\n        point_cloud = point_cloud[point_cloud[:, 0] > -5, :]\n        # remove points that are located too far away from the camera:\n        point_cloud = point_cloud[point_cloud[:, 0] < 80, :]\n\n        calib = calibread(self.calib_path)\n        P2 = calib[""P2""]\n        Tr_velo_to_cam_orig = calib[""Tr_velo_to_cam""]\n        R0_rect_orig = calib[""R0_rect""]\n        #\n        R0_rect = np.eye(4)\n        R0_rect[0:3, 0:3] = R0_rect_orig\n        #\n        Tr_velo_to_cam = np.eye(4)\n        Tr_velo_to_cam[0:3, :] = Tr_velo_to_cam_orig\n\n        point_cloud_xyz = point_cloud[:, 0:3]\n        point_cloud_xyz_hom = np.ones((point_cloud.shape[0], 4))\n        point_cloud_xyz_hom[:, 0:3] = point_cloud[:, 0:3] # (point_cloud_xyz_hom has shape (num_points, 4))\n\n        # project the points onto the image plane (homogeneous coords):\n        img_points_hom = np.dot(P2, np.dot(R0_rect, np.dot(Tr_velo_to_cam, point_cloud_xyz_hom.T))).T # (point_cloud_xyz_hom.T has shape (4, num_points))\n        # normalize:\n        img_points = np.zeros((img_points_hom.shape[0], 2))\n        img_points[:, 0] = img_points_hom[:, 0]/img_points_hom[:, 2]\n        img_points[:, 1] = img_points_hom[:, 1]/img_points_hom[:, 2]\n\n        # transform the points into (rectified) camera coordinates:\n        point_cloud_xyz_camera_hom = np.dot(R0_rect, np.dot(Tr_velo_to_cam, point_cloud_xyz_hom.T)).T # (point_cloud_xyz_hom.T has shape (4, num_points))\n        # normalize:\n        point_cloud_xyz_camera = np.zeros((point_cloud_xyz_camera_hom.shape[0], 3))\n        point_cloud_xyz_camera[:, 0] = point_cloud_xyz_camera_hom[:, 0]/point_cloud_xyz_camera_hom[:, 3]\n        point_cloud_xyz_camera[:, 1] = point_cloud_xyz_camera_hom[:, 1]/point_cloud_xyz_camera_hom[:, 3]\n        point_cloud_xyz_camera[:, 2] = point_cloud_xyz_camera_hom[:, 2]/point_cloud_xyz_camera_hom[:, 3]\n\n        point_cloud_camera = point_cloud\n        point_cloud_camera[:, 0:3] = point_cloud_xyz_camera\n\n        ########################################################################\n        # frustum:\n        ########################################################################\n        u_min = example[""u_min""] # (left)\n        u_max = example[""u_max""] # (rigth)\n        v_min = example[""v_min""] # (top)\n        v_max = example[""v_max""] # (bottom)\n\n        input_2Dbbox = np.array([u_min, u_max, v_min, v_max])\n\n        row_mask = np.logical_and(\n                    np.logical_and(img_points[:, 0] >= u_min,\n                                   img_points[:, 0] <= u_max),\n                    np.logical_and(img_points[:, 1] >= v_min,\n                                   img_points[:, 1] <= v_max))\n\n        frustum_point_cloud_xyz = point_cloud_xyz[row_mask, :] # (needed only for visualization)\n        frustum_point_cloud = point_cloud[row_mask, :]\n        frustum_point_cloud_xyz_camera = point_cloud_xyz_camera[row_mask, :]\n        frustum_point_cloud_camera = point_cloud_camera[row_mask, :]\n\n        empty_frustum_flag = 0\n        if frustum_point_cloud.shape[0] == 0:\n            empty_frustum_flag = 1\n            frustum_point_cloud = np.zeros((1024, 4), dtype=np.float32)\n            frustum_point_cloud_xyz = np.zeros((1024, 3), dtype=np.float32)\n            frustum_point_cloud_camera = np.zeros((1024, 4), dtype=np.float32)\n            frustum_point_cloud_xyz_camera = np.zeros((1024, 3), dtype=np.float32)\n\n        # randomly sample 1024 points in the frustum point cloud:\n        if frustum_point_cloud.shape[0] < 1024:\n            row_idx = np.random.choice(frustum_point_cloud.shape[0], 1024, replace=True)\n        else:\n            row_idx = np.random.choice(frustum_point_cloud.shape[0], 1024, replace=False)\n        frustum_point_cloud_xyz = frustum_point_cloud_xyz[row_idx, :]\n        frustum_point_cloud = frustum_point_cloud[row_idx, :]\n        frustum_point_cloud_xyz_camera = frustum_point_cloud_xyz_camera[row_idx, :]\n        frustum_point_cloud_camera = frustum_point_cloud_camera[row_idx, :]\n        # (the frustum point cloud now has exactly 1024 points)\n\n        ########################################################################\n        # normalize frustum point cloud:\n        ########################################################################\n        # get the 2dbbox center img point in hom. coords:\n        u_center = u_min + (u_max - u_min)/2.0\n        v_center = v_min + (v_max - v_min)/2.0\n        center_img_point_hom = np.array([u_center, v_center, 1])\n\n        # (more than one 3D point is projected onto the center image point, i.e,\n        # the linear system of equations is under-determined and has inf number\n        # of solutions. By using the pseudo-inverse, we obtain the least-norm sol)\n\n        # get a point (the least-norm sol.) that projects onto the center image point, in hom. coords:\n        P2_pseudo_inverse = np.linalg.pinv(P2) # (shape: (4, 3)) (P2 has shape (3, 4))\n        point_hom = np.dot(P2_pseudo_inverse, center_img_point_hom)\n\n        # hom --> normal coords:\n        point = np.array(([point_hom[0]/point_hom[3], point_hom[1]/point_hom[3], point_hom[2]/point_hom[3]]))\n\n        # if the point is behind the camera, switch to the mirror point in front of the camera:\n        if point[2] < 0:\n            point[0] = -point[0]\n            point[2] = -point[2]\n\n        # compute the angle of the point in the x-z plane: ((rectified) camera coords)\n        frustum_angle = np.arctan2(point[0], point[2]) # (np.arctan2(x, z)) # (frustum_angle = 0: frustum is centered)\n\n        # rotation_matrix to rotate points frustum_angle around the y axis (counter-clockwise):\n        frustum_R = np.asarray([[np.cos(frustum_angle), 0, -np.sin(frustum_angle)],\n                           [0, 1, 0],\n                           [np.sin(frustum_angle), 0, np.cos(frustum_angle)]],\n                           dtype=\'float32\')\n\n        # rotate the frustum point cloud to center it:\n        centered_frustum_point_cloud_xyz_camera = np.dot(frustum_R, frustum_point_cloud_xyz_camera.T).T\n\n        # subtract the centered frustum train xyz mean:\n        centered_frustum_point_cloud_xyz_camera -= self.centered_frustum_mean_xyz\n\n        centered_frustum_point_cloud_camera = frustum_point_cloud_camera\n        centered_frustum_point_cloud_camera[:, 0:3] = centered_frustum_point_cloud_xyz_camera\n\n        # # # # # # # # # # debug visualizations START:\n        # import sys\n        # sys.path.append(""/home/fregu856/exjobb/Open3D/build/lib"")\n        # from py3d import *\n        # frustum_pcd_camera = PointCloud()\n        # frustum_pcd_camera.points = Vector3dVector(frustum_point_cloud_xyz_camera)\n        # frustum_pcd_camera.paint_uniform_color([0.25, 0.25, 0.25])\n        # centered_frustum_pcd_camera = PointCloud()\n        # centered_frustum_pcd_camera.points = Vector3dVector(centered_frustum_point_cloud_xyz_camera)\n        # centered_frustum_pcd_camera.paint_uniform_color([1, 0, 0])\n        # draw_geometries([centered_frustum_pcd_camera, frustum_pcd_camera])\n        # # # # # # # # # # debug visualizations END:\n\n        centered_frustum_point_cloud_camera = torch.from_numpy(centered_frustum_point_cloud_camera) # (shape: (1024, 4))\n\n        return (centered_frustum_point_cloud_camera, img_id, input_2Dbbox, frustum_R, frustum_angle, empty_frustum_flag, self.centered_frustum_mean_xyz, self.mean_car_size)\n\n    def __len__(self):\n        return self.num_examples\n\nclass DatasetKittiVal2ddetections(torch.utils.data.Dataset):\n    def __init__(self, kitti_data_path, kitti_meta_path, NH):\n        self.img_dir = kitti_data_path + ""/object/training/image_2/""\n        self.calib_dir = kitti_data_path + ""/object/training/calib/""\n        self.lidar_dir = kitti_data_path + ""/object/training/velodyne/""\n        self.detections_2d_path = kitti_meta_path + ""/rgb_detection_val.txt""\n\n        self.NH = NH\n\n        with open(kitti_meta_path + ""/kitti_train_mean_car_size.pkl"", ""rb"") as file: # (needed for python3)\n            self.mean_car_size = pickle.load(file)\n\n        with open(kitti_meta_path + ""/kitti_centered_frustum_mean_xyz.pkl"", ""rb"") as file: # (needed for python3)\n            self.centered_frustum_mean_xyz = pickle.load(file)\n            self.centered_frustum_mean_xyz = self.centered_frustum_mean_xyz.astype(np.float32)\n\n        self.examples = []\n        with open(self.detections_2d_path) as file:\n            # line format: /home/rqi/Data/KITTI/object/training/image_2/***img_id***.png, class, conf_score, u_min, v_min, u_max, v_max\n            for line in file:\n                values = line.split()\n                object_class = float(values[1])\n                if object_class == 2: # (2: Car)\n                    score_2d = float(values[2])\n                    u_min = float(values[3])\n                    v_min = float(values[4])\n                    u_max = float(values[5])\n                    v_max = float(values[6])\n\n                    img_id = values[0].split(""image_2/"")[1]\n                    img_id = img_id.split(""."")[0]\n\n                    detection_2d = {}\n                    detection_2d[""u_min""] = u_min\n                    detection_2d[""v_min""] = v_min\n                    detection_2d[""u_max""] = u_max\n                    detection_2d[""v_max""] = v_max\n                    detection_2d[""score_2d""] = score_2d\n                    detection_2d[""img_id""] = img_id\n\n                    self.examples.append(detection_2d)\n\n        self.num_examples = len(self.examples)\n\n    def __getitem__(self, index):\n        example = self.examples[index]\n\n        img_id = example[""img_id""]\n\n        lidar_path = self.lidar_dir + img_id + "".bin""\n        point_cloud = np.fromfile(lidar_path, dtype=np.float32).reshape(-1, 4)\n        orig_point_cloud = point_cloud\n\n        # remove points that are located behind the camera:\n        point_cloud = point_cloud[point_cloud[:, 0] > -5, :]\n        # remove points that are located too far away from the camera:\n        point_cloud = point_cloud[point_cloud[:, 0] < 80, :]\n\n        calib = calibread(self.calib_dir + img_id + "".txt"")\n        P2 = calib[""P2""]\n        Tr_velo_to_cam_orig = calib[""Tr_velo_to_cam""]\n        R0_rect_orig = calib[""R0_rect""]\n        #\n        R0_rect = np.eye(4)\n        R0_rect[0:3, 0:3] = R0_rect_orig\n        #\n        Tr_velo_to_cam = np.eye(4)\n        Tr_velo_to_cam[0:3, :] = Tr_velo_to_cam_orig\n\n        point_cloud_xyz = point_cloud[:, 0:3]\n        point_cloud_xyz_hom = np.ones((point_cloud.shape[0], 4))\n        point_cloud_xyz_hom[:, 0:3] = point_cloud[:, 0:3] # (point_cloud_xyz_hom has shape (num_points, 4))\n\n        # project the points onto the image plane (homogeneous coords):\n        img_points_hom = np.dot(P2, np.dot(R0_rect, np.dot(Tr_velo_to_cam, point_cloud_xyz_hom.T))).T # (point_cloud_xyz_hom.T has shape (4, num_points))\n        # normalize:\n        img_points = np.zeros((img_points_hom.shape[0], 2))\n        img_points[:, 0] = img_points_hom[:, 0]/img_points_hom[:, 2]\n        img_points[:, 1] = img_points_hom[:, 1]/img_points_hom[:, 2]\n\n        # transform the points into (rectified) camera coordinates:\n        point_cloud_xyz_camera_hom = np.dot(R0_rect, np.dot(Tr_velo_to_cam, point_cloud_xyz_hom.T)).T # (point_cloud_xyz_hom.T has shape (4, num_points))\n        # normalize:\n        point_cloud_xyz_camera = np.zeros((point_cloud_xyz_camera_hom.shape[0], 3))\n        point_cloud_xyz_camera[:, 0] = point_cloud_xyz_camera_hom[:, 0]/point_cloud_xyz_camera_hom[:, 3]\n        point_cloud_xyz_camera[:, 1] = point_cloud_xyz_camera_hom[:, 1]/point_cloud_xyz_camera_hom[:, 3]\n        point_cloud_xyz_camera[:, 2] = point_cloud_xyz_camera_hom[:, 2]/point_cloud_xyz_camera_hom[:, 3]\n\n        point_cloud_camera = point_cloud\n        point_cloud_camera[:, 0:3] = point_cloud_xyz_camera\n\n        ########################################################################\n        # frustum:\n        ########################################################################\n        u_min = example[""u_min""] # (left)\n        u_max = example[""u_max""] # (rigth)\n        v_min = example[""v_min""] # (top)\n        v_max = example[""v_max""] # (bottom)\n\n        score_2d = example[""score_2d""]\n\n        input_2Dbbox = np.array([u_min, u_max, v_min, v_max])\n\n        row_mask = np.logical_and(\n                    np.logical_and(img_points[:, 0] >= u_min,\n                                   img_points[:, 0] <= u_max),\n                    np.logical_and(img_points[:, 1] >= v_min,\n                                   img_points[:, 1] <= v_max))\n\n        frustum_point_cloud_xyz = point_cloud_xyz[row_mask, :] # (needed only for visualization)\n        frustum_point_cloud = point_cloud[row_mask, :]\n        frustum_point_cloud_xyz_camera = point_cloud_xyz_camera[row_mask, :]\n        frustum_point_cloud_camera = point_cloud_camera[row_mask, :]\n\n        empty_frustum_flag = 0\n        if frustum_point_cloud.shape[0] == 0:\n            empty_frustum_flag = 1\n            frustum_point_cloud = np.zeros((1024, 4), dtype=np.float32)\n            frustum_point_cloud_xyz = np.zeros((1024, 3), dtype=np.float32)\n            frustum_point_cloud_camera = np.zeros((1024, 4), dtype=np.float32)\n            frustum_point_cloud_xyz_camera = np.zeros((1024, 3), dtype=np.float32)\n\n        # randomly sample 1024 points in the frustum point cloud:\n        if frustum_point_cloud.shape[0] < 1024:\n            row_idx = np.random.choice(frustum_point_cloud.shape[0], 1024, replace=True)\n        else:\n            row_idx = np.random.choice(frustum_point_cloud.shape[0], 1024, replace=False)\n        frustum_point_cloud_xyz = frustum_point_cloud_xyz[row_idx, :]\n        frustum_point_cloud = frustum_point_cloud[row_idx, :]\n        frustum_point_cloud_xyz_camera = frustum_point_cloud_xyz_camera[row_idx, :]\n        frustum_point_cloud_camera = frustum_point_cloud_camera[row_idx, :]\n        # (the frustum point cloud now has exactly 1024 points)\n\n        ########################################################################\n        # normalize frustum point cloud:\n        ########################################################################\n        # get the 2dbbox center img point in hom. coords:\n        u_center = u_min + (u_max - u_min)/2.0\n        v_center = v_min + (v_max - v_min)/2.0\n        center_img_point_hom = np.array([u_center, v_center, 1])\n\n        # (more than one 3D point is projected onto the center image point, i.e,\n        # the linear system of equations is under-determined and has inf number\n        # of solutions. By using the pseudo-inverse, we obtain the least-norm sol)\n\n        # get a point (the least-norm sol.) that projects onto the center image point, in hom. coords:\n        P2_pseudo_inverse = np.linalg.pinv(P2) # (shape: (4, 3)) (P2 has shape (3, 4))\n        point_hom = np.dot(P2_pseudo_inverse, center_img_point_hom)\n\n        # hom --> normal coords:\n        point = np.array(([point_hom[0]/point_hom[3], point_hom[1]/point_hom[3], point_hom[2]/point_hom[3]]))\n\n        # if the point is behind the camera, switch to the mirror point in front of the camera:\n        if point[2] < 0:\n            point[0] = -point[0]\n            point[2] = -point[2]\n\n        # compute the angle of the point in the x-z plane: ((rectified) camera coords)\n        frustum_angle = np.arctan2(point[0], point[2]) # (np.arctan2(x, z)) # (frustum_angle = 0: frustum is centered)\n\n        # rotation_matrix to rotate points frustum_angle around the y axis (counter-clockwise):\n        frustum_R = np.asarray([[np.cos(frustum_angle), 0, -np.sin(frustum_angle)],\n                           [0, 1, 0],\n                           [np.sin(frustum_angle), 0, np.cos(frustum_angle)]],\n                           dtype=\'float32\')\n\n        # rotate the frustum point cloud to center it:\n        centered_frustum_point_cloud_xyz_camera = np.dot(frustum_R, frustum_point_cloud_xyz_camera.T).T\n\n        # subtract the centered frustum train xyz mean:\n        centered_frustum_point_cloud_xyz_camera -= self.centered_frustum_mean_xyz\n\n        centered_frustum_point_cloud_camera = frustum_point_cloud_camera\n        centered_frustum_point_cloud_camera[:, 0:3] = centered_frustum_point_cloud_xyz_camera\n\n        # # # # # # # # # # debug visualizations START:\n        # import sys\n        # sys.path.append(""/home/fregu856/exjobb/Open3D/build/lib"")\n        # from py3d import *\n        # frustum_pcd_camera = PointCloud()\n        # frustum_pcd_camera.points = Vector3dVector(frustum_point_cloud_xyz_camera)\n        # frustum_pcd_camera.paint_uniform_color([0.25, 0.25, 0.25])\n        # centered_frustum_pcd_camera = PointCloud()\n        # centered_frustum_pcd_camera.points = Vector3dVector(centered_frustum_point_cloud_xyz_camera)\n        # centered_frustum_pcd_camera.paint_uniform_color([1, 0, 0])\n        # draw_geometries([centered_frustum_pcd_camera, frustum_pcd_camera])\n        # # # # # # # # # # debug visualizations END:\n\n        centered_frustum_point_cloud_camera = torch.from_numpy(centered_frustum_point_cloud_camera) # (shape: (1024, 4))\n\n        return (centered_frustum_point_cloud_camera, img_id, input_2Dbbox, frustum_R, frustum_angle, empty_frustum_flag, self.centered_frustum_mean_xyz, self.mean_car_size, score_2d)\n\n    def __len__(self):\n        return self.num_examples\n\n# ###############################################################################\n# # compute the mean car size (h, w, l) in the train set:\n# ###############################################################################\n# with open(""/root/3DOD_thesis/data/kitti/meta/train_img_ids.pkl"", ""rb"") as file:\n#     img_ids = pickle.load(file)\n#\n# label_dir = ""/root/3DOD_thesis/data/kitti/object/training/label_2/""\n# calib_dir = ""/root/3DOD_thesis/data/kitti/object/training/calib/""\n#\n# hs = []\n# ws = []\n# ls = []\n# for img_id in img_ids:\n#     labels = LabelLoader2D3D(img_id, label_dir, "".txt"", calib_dir, "".txt"")\n#     for label in labels:\n#         label_3d = label[""label_3D""]\n#         if label_3d[""class""] == ""Car"":\n#             h = label_3d[\'h\']\n#             hs.append(h)\n#             w = label_3d[\'w\']\n#             ws.append(w)\n#             l = label_3d[\'l\']\n#             ls.append(l)\n#\n# h_mean = np.mean(np.array(hs))\n# w_mean = np.mean(np.array(ws))\n# l_mean = np.mean(np.array(ls))\n# train_mean_car_size = np.array([h_mean, w_mean, l_mean])\n# #with open(""/root/3DOD_thesis/data/kitti/meta/train_mean_car_size.pkl"", ""wb"") as file:\n# #    pickle.dump(train_mean_car_size, file, protocol=2) # (protocol=2 is needed to be able to open this file with python2))\n#\n# print (train_mean_car_size)\n# ###############################################################################\n'"
Frustum-PointNet/eval_frustum_pointnet_test.py,8,"b'# camera-ready\n\nfrom datasets import DatasetKittiTest, wrapToPi, getBinCenter # (this needs to be imported before torch, because cv2 needs to be imported before torch for some reason)\nfrom frustum_pointnet import FrustumPointNet\n\nimport torch\nimport torch.utils.data\nimport torch.nn as nn\nfrom torch.autograd import Variable\nimport torch.optim as optim\nimport torch.nn.functional as F\n\nimport numpy as np\nimport pickle\n\nbatch_size = 32\n\nnetwork = FrustumPointNet(""Frustum-PointNet_eval_test"", project_dir=""/root/3DOD_thesis"")\nnetwork.load_state_dict(torch.load(""/root/3DOD_thesis/pretrained_models/model_37_2_epoch_400.pth""))\nnetwork = network.cuda()\n\nNH = network.BboxNet_network.NH\n\nval_dataset = DatasetKittiTest(kitti_data_path=""/root/3DOD_thesis/data/kitti"",\n                               kitti_meta_path=""/root/3DOD_thesis/data/kitti/meta"",\n                               NH=NH)\n\nnum_val_batches = int(len(val_dataset)/batch_size)\n\nval_loader = torch.utils.data.DataLoader(dataset=val_dataset,\n                                          batch_size=batch_size, shuffle=False,\n                                          num_workers=16)\n\nnetwork.eval() # (set in evaluation mode, this affects BatchNorm and dropout)\neval_dict = {}\nfor step, (frustum_point_clouds, img_ids, input_2Dbboxes, frustum_Rs, frustum_angles, empty_frustum_flags, centered_frustum_mean_xyz, mean_car_size, scores_2d) in enumerate(val_loader):\n    if step % 100 == 0:\n        print (""step: %d/%d"" % (step+1, num_val_batches))\n\n    with torch.no_grad(): # (corresponds to setting volatile=True in all variables, this is done during inference to reduce memory consumption)\n        frustum_point_clouds = Variable(frustum_point_clouds) # (shape: (batch_size, num_points, 4))\n        frustum_point_clouds = frustum_point_clouds.transpose(2, 1) # (shape: (batch_size, 4, num_points))\n\n        frustum_point_clouds = frustum_point_clouds.cuda()\n\n        outputs = network(frustum_point_clouds)\n\n        outputs_InstanceSeg = outputs[0] # (shape: (batch_size, num_points, 2))\n        outputs_TNet = outputs[1] # (shape: (batch_size, 3))\n        outputs_BboxNet = outputs[2] # (shape: (batch_size, 3 + 3 + 2*NH))\n        seg_point_clouds_mean = outputs[3] # (shape: (batch_size, 3))\n        dont_care_mask = outputs[4] # (shape: (batch_size, ))\n\n        ############################################################################\n        # save data for visualization:\n        ############################################################################\n        centered_frustum_mean_xyz = centered_frustum_mean_xyz[0].numpy()\n        mean_car_size = mean_car_size[0].numpy()\n        for i in range(outputs_InstanceSeg.size()[0]):\n            dont_care_mask_value = dont_care_mask[i]\n            empty_frustum_flag = empty_frustum_flags[i]\n\n            # don\'t care about predicted 3Dbboxes that corresponds to empty\n            # point clouds outputted by InstanceSeg, or empty input frustums:\n            if dont_care_mask_value == 1 and empty_frustum_flag == 0:\n                pred_InstanceSeg = outputs_InstanceSeg[i].data.cpu().numpy() # (shape: (num_points, 2))\n                frustum_point_cloud = frustum_point_clouds[i].transpose(1, 0).data.cpu().numpy() # (shape: (num_points, 4))\n                seg_point_cloud_mean = seg_point_clouds_mean[i].data.cpu().numpy() # (shape: (3, ))\n                img_id = img_ids[i]\n                input_2Dbbox = input_2Dbboxes[i] # (shape: (4, ))\n                frustum_R = frustum_Rs[i] # (shape: (3, 3))\n                frustum_angle = frustum_angles[i]\n                score_2d = scores_2d[i]\n\n                unshifted_frustum_point_cloud_xyz = frustum_point_cloud[:, 0:3] + centered_frustum_mean_xyz\n                decentered_frustum_point_cloud_xyz = np.dot(np.linalg.inv(frustum_R), unshifted_frustum_point_cloud_xyz.T).T\n                frustum_point_cloud[:, 0:3] = decentered_frustum_point_cloud_xyz\n\n                row_mask = pred_InstanceSeg[:, 1] > pred_InstanceSeg[:, 0]\n                pred_seg_point_cloud = frustum_point_cloud[row_mask, :]\n\n                pred_center_TNet = np.dot(np.linalg.inv(frustum_R), outputs_TNet[i].data.cpu().numpy() + centered_frustum_mean_xyz + seg_point_cloud_mean) # (shape: (3, )) # NOTE!\n                centroid = seg_point_cloud_mean\n\n                pred_center_BboxNet = np.dot(np.linalg.inv(frustum_R), outputs_BboxNet[i][0:3].data.cpu().numpy() + centered_frustum_mean_xyz + seg_point_cloud_mean + outputs_TNet[i].data.cpu().numpy()) # (shape: (3, )) # NOTE!\n\n                pred_h = outputs_BboxNet[i][3].data.cpu().numpy() + mean_car_size[0]\n                pred_w = outputs_BboxNet[i][4].data.cpu().numpy() + mean_car_size[1]\n                pred_l = outputs_BboxNet[i][5].data.cpu().numpy() + mean_car_size[2]\n\n                pred_bin_scores = outputs_BboxNet[i][6:(6+4)].data.cpu().numpy() # (shape (NH=8, ))\n                pred_residuals = outputs_BboxNet[i][(6+4):].data.cpu().numpy() # (shape (NH=8, ))\n                pred_bin_number = np.argmax(pred_bin_scores)\n                pred_bin_center = getBinCenter(pred_bin_number, NH=NH)\n                pred_residual = pred_residuals[pred_bin_number]\n                pred_centered_r_y = pred_bin_center + pred_residual\n                pred_r_y = wrapToPi(pred_centered_r_y + frustum_angle) # NOTE!\n\n                pred_r_y = pred_r_y.data.cpu().numpy()\n                score_2d = score_2d.data.cpu().numpy()\n                input_2Dbbox = input_2Dbbox.data.cpu().numpy()\n\n                if img_id not in eval_dict:\n                    eval_dict[img_id] = []\n\n                bbox_dict = {}\n                # # # # uncomment this if you want to visualize the frustum or the segmentation:\n                # bbox_dict[""frustum_point_cloud""] = frustum_point_cloud\n                # bbox_dict[""pred_seg_point_cloud""] = pred_seg_point_cloud\n                # # # #\n                bbox_dict[""pred_center_TNet""] = pred_center_TNet\n                bbox_dict[""pred_center_BboxNet""] = pred_center_BboxNet\n                bbox_dict[""centroid""] = centroid\n                bbox_dict[""pred_h""] = pred_h\n                bbox_dict[""pred_w""] = pred_w\n                bbox_dict[""pred_l""] = pred_l\n                bbox_dict[""pred_r_y""] = pred_r_y\n                bbox_dict[""input_2Dbbox""] = input_2Dbbox\n                bbox_dict[""score_2d""] = score_2d\n\n                eval_dict[img_id].append(bbox_dict)\n\nwith open(""%s/eval_dict_test.pkl"" % network.model_dir, ""wb"") as file:\n    pickle.dump(eval_dict, file, protocol=2) # (protocol=2 is needed to be able to open this file with python2)\n'"
Frustum-PointNet/eval_frustum_pointnet_test_seq.py,8,"b'# camera-ready\n\nfrom datasets import DatasetKittiTestSequence, wrapToPi, getBinCenter # (this needs to be imported before torch, because cv2 needs to be imported before torch for some reason)\nfrom frustum_pointnet import FrustumPointNet\n\nimport torch\nimport torch.utils.data\nimport torch.nn as nn\nfrom torch.autograd import Variable\nimport torch.optim as optim\nimport torch.nn.functional as F\n\nimport numpy as np\nimport pickle\n\nbatch_size = 32\n\nnetwork = FrustumPointNet(""Frustum-PointNet_eval_test_seq"", project_dir=""/root/3DOD_thesis"")\nnetwork.load_state_dict(torch.load(""/root/3DOD_thesis/pretrained_models/model_37_2_epoch_400.pth""))\nnetwork = network.cuda()\n\nnetwork.eval() # (set in evaluation mode, this affects BatchNorm and dropout)\n\nNH = network.BboxNet_network.NH\n\nfor sequence in [""0000"", ""0001"", ""0002"", ""0003"", ""0004"", ""0005"", ""0006"", ""0007"", ""0008"", ""0009"", ""0010"", ""0011"", ""0012"", ""0013"", ""0014"", ""0015"", ""0016"", ""0017"", ""0018"", ""0027""]:\n    print (sequence)\n\n    test_dataset = DatasetKittiTestSequence(kitti_data_path=""/root/3DOD_thesis/data/kitti"",\n                                            kitti_meta_path=""/root/3DOD_thesis/data/kitti/meta"",\n                                            NH=NH, sequence=sequence)\n\n    num_test_batches = int(len(test_dataset)/batch_size)\n\n    test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n                                              batch_size=batch_size, shuffle=False,\n                                              num_workers=16)\n\n    eval_dict = {}\n    for step, (frustum_point_clouds, img_ids, input_2Dbboxes, frustum_Rs, frustum_angles, empty_frustum_flags, centered_frustum_mean_xyz, mean_car_size) in enumerate(test_loader):\n        if step % 100 == 0:\n            print (""step: %d/%d"" % (step+1, num_test_batches))\n\n        with torch.no_grad(): # (corresponds to setting volatile=True in all variables, this is done during inference to reduce memory consumption)\n            frustum_point_clouds = Variable(frustum_point_clouds) # (shape: (batch_size, num_points, 4))\n            frustum_point_clouds = frustum_point_clouds.transpose(2, 1) # (shape: (batch_size, 4, num_points))\n\n            frustum_point_clouds = frustum_point_clouds.cuda()\n\n            outputs = network(frustum_point_clouds)\n            outputs_InstanceSeg = outputs[0] # (shape: (batch_size, num_points, 2))\n            outputs_TNet = outputs[1] # (shape: (batch_size, 3))\n            outputs_BboxNet = outputs[2] # (shape: (batch_size, 3 + 3 + 2*NH))\n            seg_point_clouds_mean = outputs[3] # (shape: (batch_size, 3))\n            dont_care_mask = outputs[4] # (shape: (batch_size, ))\n\n            ############################################################################\n            # save data for visualization:\n            ############################################################################\n            centered_frustum_mean_xyz = centered_frustum_mean_xyz[0].numpy()\n            mean_car_size = mean_car_size[0].numpy()\n            for i in range(outputs_InstanceSeg.size()[0]):\n                dont_care_mask_value = dont_care_mask[i]\n                empty_frustum_flag = empty_frustum_flags[i]\n\n                # don\'t care about predicted 3Dbboxes that corresponds to empty\n                # point clouds outputted by InstanceSeg, or empty input frustums:\n                if dont_care_mask_value == 1 and empty_frustum_flag == 0:\n                    pred_InstanceSeg = outputs_InstanceSeg[i].data.cpu().numpy() # (shape: (num_points, 2))\n                    frustum_point_cloud = frustum_point_clouds[i].transpose(1, 0).data.cpu().numpy() # (shape: (num_points, 4))\n                    seg_point_cloud_mean = seg_point_clouds_mean[i].data.cpu().numpy() # (shape: (3, ))\n                    img_id = img_ids[i]\n                    #if img_id in [""000000"", ""000001"", ""000002"", ""000003"", ""000004"", ""000005"", ""000006"", ""000007"", ""000008"", ""000009"", ""000010"", ""000011"", ""000012"", ""000013"", ""000014"", ""000015"", ""000016"", ""000017"", ""000018"", ""000019"", ""000020"", ""000021"", ""000022"", ""000023"", ""000024"", ""000025"", ""000026"", ""000027"", ""000028"", ""000029"", ""000030""]:\n                    input_2Dbbox = input_2Dbboxes[i] # (shape: (4, ))\n                    frustum_R = frustum_Rs[i] # (shape: (3, 3))\n                    frustum_angle = frustum_angles[i]\n\n                    unshifted_frustum_point_cloud_xyz = frustum_point_cloud[:, 0:3] + centered_frustum_mean_xyz\n                    decentered_frustum_point_cloud_xyz = np.dot(np.linalg.inv(frustum_R), unshifted_frustum_point_cloud_xyz.T).T\n                    frustum_point_cloud[:, 0:3] = decentered_frustum_point_cloud_xyz\n\n                    row_mask = pred_InstanceSeg[:, 1] > pred_InstanceSeg[:, 0]\n                    pred_seg_point_cloud = frustum_point_cloud[row_mask, :]\n\n                    pred_center_TNet = np.dot(np.linalg.inv(frustum_R), outputs_TNet[i].data.cpu().numpy() + centered_frustum_mean_xyz + seg_point_cloud_mean) # (shape: (3, )) # NOTE!\n                    centroid = seg_point_cloud_mean\n\n                    pred_center_BboxNet = np.dot(np.linalg.inv(frustum_R), outputs_BboxNet[i][0:3].data.cpu().numpy() + centered_frustum_mean_xyz + seg_point_cloud_mean + outputs_TNet[i].data.cpu().numpy()) # (shape: (3, )) # NOTE!\n\n                    pred_h = outputs_BboxNet[i][3].data.cpu().numpy() + mean_car_size[0]\n                    pred_w = outputs_BboxNet[i][4].data.cpu().numpy() + mean_car_size[1]\n                    pred_l = outputs_BboxNet[i][5].data.cpu().numpy() + mean_car_size[2]\n\n                    pred_bin_scores = outputs_BboxNet[i][6:(6+4)].data.cpu().numpy() # (shape (NH=8, ))\n                    pred_residuals = outputs_BboxNet[i][(6+4):].data.cpu().numpy() # (shape (NH=8, ))\n                    pred_bin_number = np.argmax(pred_bin_scores)\n                    pred_bin_center = getBinCenter(pred_bin_number, NH=NH)\n                    pred_residual = pred_residuals[pred_bin_number]\n                    pred_centered_r_y = pred_bin_center + pred_residual\n                    pred_r_y = wrapToPi(pred_centered_r_y + frustum_angle) # NOTE!\n\n                    pred_r_y = pred_r_y.data.cpu().numpy()\n                    input_2Dbbox = input_2Dbbox.data.cpu().numpy()\n\n                    if img_id not in eval_dict:\n                        eval_dict[img_id] = []\n\n                    bbox_dict = {}\n                    # # # # uncomment this if you want to visualize the frustum or the segmentation:\n                    # bbox_dict[""frustum_point_cloud""] = frustum_point_cloud\n                    # bbox_dict[""pred_seg_point_cloud""] = pred_seg_point_cloud\n                    # # # #\n                    bbox_dict[""pred_center_TNet""] = pred_center_TNet\n                    bbox_dict[""pred_center_BboxNet""] = pred_center_BboxNet\n                    bbox_dict[""centroid""] = centroid\n                    bbox_dict[""pred_h""] = pred_h\n                    bbox_dict[""pred_w""] = pred_w\n                    bbox_dict[""pred_l""] = pred_l\n                    bbox_dict[""pred_r_y""] = pred_r_y\n                    bbox_dict[""input_2Dbbox""] = input_2Dbbox\n\n                    eval_dict[img_id].append(bbox_dict)\n\n    with open(""%s/eval_dict_test_seq_%s.pkl"" % (network.model_dir, sequence), ""wb"") as file:\n        pickle.dump(eval_dict, file, protocol=2) # (protocol=2 is needed to be able to open this file with python2)\n'"
Frustum-PointNet/eval_frustum_pointnet_val.py,39,"b'# camera-ready\n\nfrom datasets import EvalDatasetFrustumPointNet, wrapToPi, getBinCenter # (this needs to be imported before torch, because cv2 needs to be imported before torch for some reason)\nfrom frustum_pointnet import FrustumPointNet\n\nimport torch\nimport torch.utils.data\nimport torch.nn as nn\nfrom torch.autograd import Variable\nimport torch.optim as optim\nimport torch.nn.functional as F\n\nimport numpy as np\nimport pickle\nimport matplotlib\nmatplotlib.use(""Agg"")\nimport matplotlib.pyplot as plt\n\nbatch_size = 32\n\nnetwork = FrustumPointNet(""Frustum-PointNet_eval_val"", project_dir=""/root/3DOD_thesis"")\nnetwork.load_state_dict(torch.load(""/root/3DOD_thesis/pretrained_models/model_37_2_epoch_400.pth""))\nnetwork = network.cuda()\n\nNH = network.BboxNet_network.NH\n\nval_dataset = EvalDatasetFrustumPointNet(kitti_data_path=""/root/3DOD_thesis/data/kitti"",\n                                         kitti_meta_path=""/root/3DOD_thesis/data/kitti/meta"",\n                                         type=""val"", NH=NH)\n\nnum_val_batches = int(len(val_dataset)/batch_size)\n\nval_loader = torch.utils.data.DataLoader(dataset=val_dataset,\n                                         batch_size=batch_size, shuffle=False,\n                                         num_workers=16)\n\nregression_loss_func = nn.SmoothL1Loss()\n\nnetwork.eval() # (set in evaluation mode, this affects BatchNorm, dropout etc.)\nbatch_losses = []\nbatch_losses_InstanceSeg = []\nbatch_losses_TNet = []\nbatch_losses_BboxNet = []\nbatch_losses_BboxNet_center = []\nbatch_losses_BboxNet_size = []\nbatch_losses_BboxNet_heading_regr = []\nbatch_losses_BboxNet_heading_class = []\nbatch_losses_BboxNet_heading_class_weighted = []\nbatch_losses_corner = []\nbatch_accuracies = []\nbatch_precisions = []\nbatch_recalls = []\nbatch_f1s = []\nbatch_accuracies_heading_class = []\neval_dict = {}\nfor step, (frustum_point_clouds, labels_InstanceSeg, labels_TNet, labels_BboxNet, labels_corner, labels_corner_flipped, img_ids, input_2Dbboxes, frustum_Rs, frustum_angles, centered_frustum_mean_xyz) in enumerate(val_loader):\n    with torch.no_grad(): # (corresponds to setting volatile=True in all variables, this is done during inference to reduce memory consumption)\n        frustum_point_clouds = Variable(frustum_point_clouds) # (shape: (batch_size, num_points, 4))\n        labels_InstanceSeg = Variable(labels_InstanceSeg) # (shape: (batch_size, num_points))\n        labels_TNet = Variable(labels_TNet) # (shape: (batch_size, 3))\n        labels_BboxNet = Variable(labels_BboxNet) # (shape:(batch_size, 11))\n        labels_corner = Variable(labels_corner) # (shape: (batch_size, 8, 3))\n        labels_corner_flipped = Variable(labels_corner_flipped) # (shape: (batch_size, 8, 3))\n\n        frustum_point_clouds = frustum_point_clouds.transpose(2, 1) # (shape: (batch_size, 4, num_points))\n\n        frustum_point_clouds = frustum_point_clouds.cuda()\n        labels_InstanceSeg = labels_InstanceSeg.cuda()\n        labels_TNet = labels_TNet.cuda()\n        labels_BboxNet = labels_BboxNet.cuda()\n        labels_corner = labels_corner.cuda()\n        labels_corner_flipped = labels_corner_flipped.cuda()\n\n        outputs = network(frustum_point_clouds)\n        outputs_InstanceSeg = outputs[0] # (shape: (batch_size, num_points, 2))\n        outputs_TNet = outputs[1] # (shape: (batch_size, 3))\n        outputs_BboxNet = outputs[2] # (shape: (batch_size, 3 + 3 + 2*NH))\n        seg_point_clouds_mean = outputs[3] # (shape: (batch_size, 3))\n        dont_care_mask = outputs[4] # (shape: (batch_size, ))\n\n        ############################################################################\n        # save data for visualization:\n        ############################################################################\n        centered_frustum_mean_xyz = centered_frustum_mean_xyz[0].numpy()\n        for i in range(outputs_InstanceSeg.size()[0]):\n            dont_care_mask_value = dont_care_mask[i]\n\n            # don\'t care about predicted 3Dbboxes that corresponds to empty point clouds outputted by InstanceSeg:\n            if dont_care_mask_value == 1:\n                pred_InstanceSeg = outputs_InstanceSeg[i].data.cpu().numpy() # (shape: (num_points, 2))\n                frustum_point_cloud = frustum_point_clouds[i].transpose(1, 0).data.cpu().numpy() # (shape: (num_points, 4))\n                label_InstanceSeg = labels_InstanceSeg[i].data.cpu().numpy() # (shape: (num_points, ))\n                seg_point_cloud_mean = seg_point_clouds_mean[i].data.cpu().numpy() # (shape: (3, ))\n                img_id = img_ids[i]\n                #if img_id in [""000006"", ""000007"", ""000008"", ""000009"", ""000010"", ""000011"", ""000012"", ""000013"", ""000014"", ""000015"", ""000016"", ""000017"", ""000018"", ""000019"", ""000020"", ""000021""]:\n                input_2Dbbox = input_2Dbboxes[i] # (shape: (4, ))\n                frustum_R = frustum_Rs[i] # (shape: (3, 3))\n                frustum_angle = frustum_angles[i]\n\n                unshifted_frustum_point_cloud_xyz = frustum_point_cloud[:, 0:3] + centered_frustum_mean_xyz\n                decentered_frustum_point_cloud_xyz = np.dot(np.linalg.inv(frustum_R), unshifted_frustum_point_cloud_xyz.T).T\n                frustum_point_cloud[:, 0:3] = decentered_frustum_point_cloud_xyz\n\n                row_mask = pred_InstanceSeg[:, 1] > pred_InstanceSeg[:, 0]\n                pred_seg_point_cloud = frustum_point_cloud[row_mask, :]\n\n                row_mask = label_InstanceSeg == 1\n                gt_seg_point_cloud = frustum_point_cloud[row_mask, :]\n\n                pred_center_TNet = np.dot(np.linalg.inv(frustum_R), outputs_TNet[i].data.cpu().numpy() + centered_frustum_mean_xyz + seg_point_cloud_mean) # (shape: (3, )) # NOTE!\n                gt_center = np.dot(np.linalg.inv(frustum_R), labels_TNet[i].data.cpu().numpy() + centered_frustum_mean_xyz) # NOTE!\n                centroid = seg_point_cloud_mean\n\n                pred_center_BboxNet = np.dot(np.linalg.inv(frustum_R), outputs_BboxNet[i][0:3].data.cpu().numpy() + centered_frustum_mean_xyz + seg_point_cloud_mean + outputs_TNet[i].data.cpu().numpy()) # (shape: (3, )) # NOTE!\n\n                pred_h = outputs_BboxNet[i][3].data.cpu().numpy() + labels_BboxNet[i][8].data.cpu().numpy()\n                pred_w = outputs_BboxNet[i][4].data.cpu().numpy() + labels_BboxNet[i][9].data.cpu().numpy()\n                pred_l = outputs_BboxNet[i][5].data.cpu().numpy() + labels_BboxNet[i][10].data.cpu().numpy()\n\n                pred_bin_scores = outputs_BboxNet[i][6:(6+4)].data.cpu().numpy() # (shape (NH=8, ))\n                pred_residuals = outputs_BboxNet[i][(6+4):].data.cpu().numpy() # (shape (NH=8, ))\n                pred_bin_number = np.argmax(pred_bin_scores)\n                pred_bin_center = getBinCenter(pred_bin_number, NH=NH)\n                pred_residual = pred_residuals[pred_bin_number]\n                pred_centered_r_y = pred_bin_center + pred_residual\n                pred_r_y = wrapToPi(pred_centered_r_y + frustum_angle) # NOTE!\n\n                gt_h = labels_BboxNet[i][3].data.cpu().numpy()\n                gt_w = labels_BboxNet[i][4].data.cpu().numpy()\n                gt_l = labels_BboxNet[i][5].data.cpu().numpy()\n\n                gt_bin_number = labels_BboxNet[i][6].data.cpu().numpy()\n                gt_bin_center = getBinCenter(gt_bin_number, NH=NH)\n                gt_residual = labels_BboxNet[i][7].data.cpu().numpy()\n                gt_centered_r_y = gt_bin_center + gt_residual\n                gt_r_y = wrapToPi(gt_centered_r_y + frustum_angle) # NOTE!\n\n                pred_r_y = pred_r_y.data.cpu().numpy()\n                gt_r_y = gt_r_y.data.cpu().numpy()\n                input_2Dbbox = input_2Dbbox.data.cpu().numpy()\n\n                if img_id not in eval_dict:\n                    eval_dict[img_id] = []\n\n                bbox_dict = {}\n                # # # # uncomment this if you want to visualize the frustum or the segmentation (e.g., if you want to run visualization/visualize_eval_val_extra.py):\n                # bbox_dict[""frustum_point_cloud""] = frustum_point_cloud\n                # bbox_dict[""pred_seg_point_cloud""] = pred_seg_point_cloud\n                # bbox_dict[""gt_seg_point_cloud""] = gt_seg_point_cloud\n                # # # #\n                bbox_dict[""pred_center_TNet""] = pred_center_TNet\n                bbox_dict[""pred_center_BboxNet""] = pred_center_BboxNet\n                bbox_dict[""gt_center""] = gt_center\n                bbox_dict[""centroid""] = centroid\n                bbox_dict[""pred_h""] = pred_h\n                bbox_dict[""pred_w""] = pred_w\n                bbox_dict[""pred_l""] = pred_l\n                bbox_dict[""pred_r_y""] = pred_r_y\n                bbox_dict[""gt_h""] = gt_h\n                bbox_dict[""gt_w""] = gt_w\n                bbox_dict[""gt_l""] = gt_l\n                bbox_dict[""gt_r_y""] = gt_r_y\n                bbox_dict[""input_2Dbbox""] = input_2Dbbox\n\n                eval_dict[img_id].append(bbox_dict)\n\n        ########################################################################\n        # compute precision, recall etc. for the InstanceSeg:\n        ########################################################################\n        preds = outputs_InstanceSeg.data.cpu().numpy() # (shape: (batch_size, num_points, 2))\n        preds = np.argmax(preds, 2) # (shape: (batch_size, num_points))\n\n        labels_InstanceSeg_np = labels_InstanceSeg.data.cpu().numpy() # (shape: (batch_size, num_points))\n\n        accuracy = np.count_nonzero(preds == labels_InstanceSeg_np)/(preds.shape[0]*preds.shape[1])\n        if np.count_nonzero(preds == 1) > 0:\n            precision = np.count_nonzero(np.logical_and(preds == labels_InstanceSeg_np, preds == 1))/np.count_nonzero(preds == 1) # (TP/(TP + FP))\n        else:\n            precision = -1\n        if np.count_nonzero(labels_InstanceSeg_np == 1) > 0:\n            recall = np.count_nonzero(np.logical_and(preds == labels_InstanceSeg_np, preds == 1))/np.count_nonzero(labels_InstanceSeg_np == 1) # (TP/(TP + FN))\n        else:\n            recall = -1\n        if recall + precision > 0:\n            f1 = 2*recall*precision/(recall + precision)\n        else:\n            f1 = -1\n\n        batch_accuracies.append(accuracy)\n        if precision != -1:\n            batch_precisions.append(precision)\n        if recall != -1:\n            batch_recalls.append(recall)\n        if f1 != -1:\n            batch_f1s.append(f1)\n\n        ########################################################################\n        # compute accuracy for the heading classification:\n        ########################################################################\n        pred_bin_scores = outputs_BboxNet[:, 6:(6+NH)].data.cpu().numpy() # (shape: (batch_size, NH))\n        pred_bin_numbers = np.argmax(pred_bin_scores, 1) # (shape: (batch_size, ))\n        gt_bin_numbers = labels_BboxNet[:, 6].data.cpu().numpy() # (shape: (batch_size, ))\n\n        accuracy_heading_class = np.count_nonzero(pred_bin_numbers == gt_bin_numbers)/(pred_bin_numbers.shape[0])\n        batch_accuracies_heading_class.append(accuracy_heading_class)\n\n        ########################################################################\n        # compute the InstanceSeg loss:\n        ########################################################################\n        outputs_InstanceSeg = outputs_InstanceSeg.view(-1, 2) # (shape (batch_size*num_points, 2))\n        labels_InstanceSeg = labels_InstanceSeg.view(-1, 1) # (shape: (batch_size*num_points, 1))\n        labels_InstanceSeg = labels_InstanceSeg[:, 0] # (shape: (batch_size*num_points, ))\n        loss_InstanceSeg = F.nll_loss(outputs_InstanceSeg, labels_InstanceSeg)\n        loss_InstanceSeg_value = loss_InstanceSeg.data.cpu().numpy()\n        batch_losses_InstanceSeg.append(loss_InstanceSeg_value)\n\n        ########################################################################\n        # compute the TNet loss:\n        ########################################################################\n        # mask entries corresponding to empty seg point clouds (select only the entries which we care about):\n        outputs_TNet = outputs_TNet[dont_care_mask, :] # (shape: (batch_size*, 3))\n        labels_TNet = labels_TNet[dont_care_mask, :] # (shape: (batch_size*, 3))\n        seg_point_clouds_mean = seg_point_clouds_mean[dont_care_mask, :] # (shape: (batch_size*, 3))\n\n        # shift the GT to the seg point clouds local coords:\n        labels_TNet = labels_TNet - seg_point_clouds_mean\n\n        # compute the Huber (smooth L1) loss:\n        if outputs_TNet.size()[0] == 0:\n            loss_TNet = Variable(torch.from_numpy(np.zeros((1, ), dtype=np.float32))).cuda()\n        else:\n            loss_TNet = regression_loss_func(outputs_TNet, labels_TNet)\n\n        loss_TNet_value = loss_TNet.data.cpu().numpy()\n        batch_losses_TNet.append(loss_TNet_value)\n\n        ########################################################################\n        # compute the BboxNet loss:\n        ########################################################################\n        # mask entries corresponding to empty seg point clouds (select only the entries which we care about):\n        outputs_BboxNet = outputs_BboxNet[dont_care_mask, :] # (shape: (batch_size*, 3 + 3 + 2*NH))\n        labels_BboxNet = labels_BboxNet[dont_care_mask, :]# (shape: (batch_size*, 11))\n\n        if outputs_BboxNet.size()[0] == 0:\n            loss_BboxNet = Variable(torch.from_numpy(np.zeros((1, ), dtype=np.float32))).cuda()\n            loss_BboxNet_size = Variable(torch.from_numpy(np.zeros((1, ), dtype=np.float32))).cuda()\n            loss_BboxNet_center = Variable(torch.from_numpy(np.zeros((1, ), dtype=np.float32))).cuda()\n            loss_BboxNet_heading_class = Variable(torch.from_numpy(np.zeros((1, ), dtype=np.float32))).cuda()\n            loss_BboxNet_heading_regr = Variable(torch.from_numpy(np.zeros((1, ), dtype=np.float32))).cuda()\n        else:\n            # compute the BboxNet center loss:\n            labels_BboxNet_center = labels_BboxNet[:, 0:3] # (shape: (batch_size*, 3))\n            # # shift the center GT to local coords:\n            labels_BboxNet_center = Variable(labels_BboxNet_center.data - seg_point_clouds_mean.data - outputs_TNet.data).cuda() # (outputs_TNet is a variable outputted by model, so it requires grads, which cant be passed as target to the loss function)\n            # # compute the Huber (smooth L1) loss:\n            outputs_BboxNet_center = outputs_BboxNet[:, 0:3]\n            loss_BboxNet_center = regression_loss_func(outputs_BboxNet_center, labels_BboxNet_center)\n\n            # compute the BboxNet size loss:\n            labels_BboxNet_size = labels_BboxNet[:, 3:6] # (shape: (batch_size*, 3))\n            # # subtract the mean car size in train:\n            labels_BboxNet_size = labels_BboxNet_size - labels_BboxNet[:, 8:]\n            # # compute the Huber (smooth L1) loss:\n            loss_BboxNet_size = regression_loss_func(outputs_BboxNet[:, 3:6], labels_BboxNet_size)\n\n            # compute the BboxNet heading loss\n            # # compute the classification loss:\n            labels_BboxNet_heading_class = Variable(labels_BboxNet[:, 6].data.type(torch.LongTensor)).cuda() # (shape: (batch_size*, ))\n            outputs_BboxNet_heading_class = outputs_BboxNet[:, 6:(6+NH)] # (shape: (batch_size*, NH))\n            loss_BboxNet_heading_class = F.nll_loss(F.log_softmax(outputs_BboxNet_heading_class, dim=1), labels_BboxNet_heading_class)\n            # # compute the regression loss:\n            # # # # get the GT residual for the GT bin:\n            labels_BboxNet_heading_regr = labels_BboxNet[:, 7] # (shape: (batch_size*, ))\n            # # # # get the pred residual for all bins:\n            outputs_BboxNet_heading_regr_all = outputs_BboxNet[:, (6+NH):] # (shape: (batch_size*, 8))\n            # # # # get the pred residual for the GT bin:\n            outputs_BboxNet_heading_regr = outputs_BboxNet_heading_regr_all.gather(1, labels_BboxNet_heading_class.view(-1, 1)) # (shape: (batch_size*, 1))\n            outputs_BboxNet_heading_regr = outputs_BboxNet_heading_regr[:, 0] # (shape: (batch_size*, )\n            # # # # compute the loss:\n            loss_BboxNet_heading_regr = regression_loss_func(outputs_BboxNet_heading_regr, labels_BboxNet_heading_regr)\n            # # compute the total BBoxNet heading loss:\n            loss_BboxNet_heading = loss_BboxNet_heading_class + 10*loss_BboxNet_heading_regr\n\n            # compute the BboxNet total loss:\n            loss_BboxNet = loss_BboxNet_center + loss_BboxNet_size + loss_BboxNet_heading\n\n        loss_BboxNet_value = loss_BboxNet.data.cpu().numpy()\n        batch_losses_BboxNet.append(loss_BboxNet_value)\n\n        loss_BboxNet_size_value = loss_BboxNet_size.data.cpu().numpy()\n        batch_losses_BboxNet_size.append(loss_BboxNet_size_value)\n\n        loss_BboxNet_center_value = loss_BboxNet_center.data.cpu().numpy()\n        batch_losses_BboxNet_center.append(loss_BboxNet_center_value)\n\n        loss_BboxNet_heading_class_value = loss_BboxNet_heading_class.data.cpu().numpy()\n        batch_losses_BboxNet_heading_class.append(loss_BboxNet_heading_class_value)\n\n        loss_BboxNet_heading_regr_value = loss_BboxNet_heading_regr.data.cpu().numpy()\n        batch_losses_BboxNet_heading_regr.append(loss_BboxNet_heading_regr_value)\n\n        ########################################################################\n        # compute the corner loss:\n        ########################################################################\n        # mask entries corresponding to empty seg point clouds (select only the entries which we care about):\n        labels_corner = labels_corner[dont_care_mask]# (shape: (batch_size*, 8, 3))\n        labels_corner_flipped = labels_corner_flipped[dont_care_mask]# (shape: (batch_size*, 8, 3))\n\n        if outputs_BboxNet.size()[0] == 0:\n            loss_corner = Variable(torch.from_numpy(np.zeros((1, ), dtype=np.float32))).cuda()\n        else:\n            outputs_BboxNet_center = outputs_BboxNet[:, 0:3] # (shape: (batch_size*, 3))\n            # shift to the same coords used in the labels for the corner loss:\n            pred_center = outputs_BboxNet_center + seg_point_clouds_mean + outputs_TNet # (shape: (batch_size*, 3))\n            pred_center_unsqeezed = pred_center.unsqueeze(2) # (shape: (batch_size, 3, 1))\n\n            # shift the outputted size to the same ""coords"" used in the labels for the corner loss:\n            outputs_BboxNet_size = outputs_BboxNet[:, 3:6] + labels_BboxNet[:, 8:] # (shape: (batch_size*, 3))\n\n            pred_h = outputs_BboxNet_size[:, 0] # (shape: (batch_size*, ))\n            pred_w = outputs_BboxNet_size[:, 1] # (shape: (batch_size*, ))\n            pred_l = outputs_BboxNet_size[:, 2] # (shape: (batch_size*, ))\n\n            # get the pred residuals for the GT bins:\n            pred_residuals = outputs_BboxNet_heading_regr # (shape: (batch_size*, ))\n\n            Rmat = Variable(torch.zeros(pred_h.size()[0], 3, 3), requires_grad=True).cuda() # (shape: (batch_size*, 3, 3))\n            Rmat[:, 0, 0] = torch.cos(pred_residuals)\n            Rmat[:, 0, 2] = torch.sin(pred_residuals)\n            Rmat[:, 1, 1] = 1\n            Rmat[:, 2, 0] = -torch.sin(pred_residuals)\n            Rmat[:, 2, 2] = torch.cos(pred_residuals)\n\n            p0_orig = Variable(torch.zeros(pred_h.size()[0], 3, 1), requires_grad=True).cuda() # (shape: (batch_size*, 3, 1))\n            p0_orig[:, 0, 0] = pred_l/2.0\n            p0_orig[:, 2, 0] = pred_w/2.0\n\n            p1_orig = Variable(torch.zeros(pred_h.size()[0], 3, 1), requires_grad=True).cuda() # (shape: (batch_size*, 3, 1))\n            p1_orig[:, 0, 0] = -pred_l/2.0\n            p1_orig[:, 2, 0] = pred_w/2.0\n\n            p2_orig = Variable(torch.zeros(pred_h.size()[0], 3, 1), requires_grad=True).cuda() # (shape: (batch_size*, 3, 1))\n            p2_orig[:, 0, 0] = -pred_l/2.0\n            p2_orig[:, 2, 0] = -pred_w/2.0\n\n            p3_orig = Variable(torch.zeros(pred_h.size()[0], 3, 1), requires_grad=True).cuda() # (shape: (batch_size*, 3, 1))\n            p3_orig[:, 0, 0] = pred_l/2.0\n            p3_orig[:, 2, 0] = -pred_w/2.0\n\n            p4_orig = Variable(torch.zeros(pred_h.size()[0], 3, 1), requires_grad=True).cuda() # (shape: (batch_size*, 3, 1))\n            p4_orig[:, 0, 0] = pred_l/2.0\n            p4_orig[:, 1, 0] = -pred_h\n            p4_orig[:, 2, 0] = pred_w/2.0\n\n            p5_orig = Variable(torch.zeros(pred_h.size()[0], 3, 1), requires_grad=True).cuda() # (shape: (batch_size*, 3, 1))\n            p5_orig[:, 0, 0] = -pred_l/2.0\n            p5_orig[:, 1, 0] = -pred_h\n            p5_orig[:, 2, 0] = pred_w/2.0\n\n            p6_orig = Variable(torch.zeros(pred_h.size()[0], 3, 1), requires_grad=True).cuda() # (shape: (batch_size*, 3, 1))\n            p6_orig[:, 0, 0] = -pred_l/2.0\n            p6_orig[:, 1, 0] = -pred_h\n            p6_orig[:, 2, 0] = -pred_w/2.0\n\n            p7_orig = Variable(torch.zeros(pred_h.size()[0], 3, 1), requires_grad=True).cuda() # (shape: (batch_size*, 3, 1))\n            p7_orig[:, 0, 0] = pred_l/2.0\n            p7_orig[:, 1, 0] = -pred_h\n            p7_orig[:, 2, 0] = -pred_w/2.0\n\n            pred_p0_unsqeezed = pred_center_unsqeezed + torch.bmm(Rmat, p0_orig) # (shape: (batch_size*, 3, 1))\n            pred_p0 = pred_p0_unsqeezed[:, :, 0] # (shape: (batch_size*, 3))\n            pred_p1_unsqeezed = pred_center_unsqeezed + torch.bmm(Rmat, p1_orig) # (shape: (batch_size*, 3, 1))\n            pred_p1 = pred_p1_unsqeezed[:, :, 0] # (shape: (batch_size*, 3))\n            pred_p2_unsqeezed = pred_center_unsqeezed + torch.bmm(Rmat, p2_orig) # (shape: (batch_size*, 3, 1))\n            pred_p2 = pred_p2_unsqeezed[:, :, 0] # (shape: (batch_size*, 3))\n            pred_p3_unsqeezed = pred_center_unsqeezed + torch.bmm(Rmat, p3_orig) # (shape: (batch_size*, 3, 1))\n            pred_p3 = pred_p3_unsqeezed[:, :, 0] # (shape: (batch_size*, 3))\n            pred_p4_unsqeezed = pred_center_unsqeezed + torch.bmm(Rmat, p4_orig) # (shape: (batch_size*, 3, 1))\n            pred_p4 = pred_p4_unsqeezed[:, :, 0] # (shape: (batch_size*, 3))\n            pred_p5_unsqeezed = pred_center_unsqeezed + torch.bmm(Rmat, p5_orig) # (shape: (batch_size*, 3, 1))\n            pred_p5 = pred_p5_unsqeezed[:, :, 0] # (shape: (batch_size*, 3))\n            pred_p6_unsqeezed = pred_center_unsqeezed + torch.bmm(Rmat, p6_orig) # (shape: (batch_size*, 3, 1))\n            pred_p6 = pred_p6_unsqeezed[:, :, 0] # (shape: (batch_size*, 3))\n            pred_p7_unsqeezed = pred_center_unsqeezed + torch.bmm(Rmat, p7_orig) # (shape: (batch_size*, 3, 1))\n            pred_p7 = pred_p7_unsqeezed[:, :, 0] # (shape: (batch_size*, 3))\n\n            outputs_corner = Variable(torch.zeros(pred_h.size()[0], 8, 3), requires_grad=True).cuda() # (shape: (batch_size*, 8, 3))\n            outputs_corner[:, 0] = pred_p0\n            outputs_corner[:, 1] = pred_p1\n            outputs_corner[:, 2] = pred_p2\n            outputs_corner[:, 3] = pred_p3\n            outputs_corner[:, 4] = pred_p4\n            outputs_corner[:, 5] = pred_p5\n            outputs_corner[:, 6] = pred_p6\n            outputs_corner[:, 7] = pred_p7\n\n            loss_corner_unflipped = regression_loss_func(outputs_corner, labels_corner)\n            loss_corner_flipped = regression_loss_func(outputs_corner, labels_corner_flipped)\n\n            loss_corner = torch.min(loss_corner_unflipped, loss_corner_flipped)\n\n        loss_corner_value = loss_corner.data.cpu().numpy()\n        batch_losses_corner.append(loss_corner_value)\n\n        ########################################################################\n        # compute the total loss:\n        ########################################################################\n        lambda_value = 1\n        gamma_value = 10\n        loss = loss_InstanceSeg + lambda_value*(loss_TNet + loss_BboxNet + gamma_value*loss_corner)\n        loss_value = loss.data.cpu().numpy()\n        batch_losses.append(loss_value)\n\n# compute the val epoch loss:\nepoch_loss = np.mean(batch_losses)\nprint (""validation loss: %g"" % epoch_loss)\n# compute the val epoch TNet loss:\nepoch_loss = np.mean(batch_losses_TNet)\nprint (""validation TNet loss: %g"" % epoch_loss)\n# compute the val epoch InstanceSeg loss:\nepoch_loss = np.mean(batch_losses_InstanceSeg)\nprint (""validation InstanceSeg loss: %g"" % epoch_loss)\n# compute the val epoch BboxNet loss:\nepoch_loss = np.mean(batch_losses_BboxNet)\nprint (""validation BboxNet loss: %g"" % epoch_loss)\n# compute the val epoch BboxNet size loss:\nepoch_loss = np.mean(batch_losses_BboxNet_size)\nprint (""validation BboxNet size loss: %g"" % epoch_loss)\n# compute the val epoch BboxNet center loss:\nepoch_loss = np.mean(batch_losses_BboxNet_center)\nprint (""validation BboxNet center loss: %g"" % epoch_loss)\n# compute the val epoch BboxNet heading class loss:\nepoch_loss = np.mean(batch_losses_BboxNet_heading_class)\nprint (""validation BboxNet heading class loss: %g"" % epoch_loss)\n# compute the val epoch BboxNet heading regr loss:\nepoch_loss = np.mean(batch_losses_BboxNet_heading_regr)\nprint (""validation BboxNet heading regr loss: %g"" % epoch_loss)\n# compute the val epoch heading class accuracy:\nepoch_accuracy = np.mean(batch_accuracies_heading_class)\nprint (""validation heading class accuracy: %g"" % epoch_accuracy)\n# compute the val epoch corner loss:\nepoch_loss = np.mean(batch_losses_corner)\nprint (""validation corner loss: %g"" % epoch_loss)\n# compute the val epoch accuracy:\nepoch_accuracy = np.mean(batch_accuracies)\nprint (""validation accuracy: %g"" % epoch_accuracy)\n# compute the val epoch precision:\nepoch_precision = np.mean(batch_precisions)\nprint (""validation precision: %g"" % epoch_precision)\n# compute the val epoch recall:\nepoch_recall = np.mean(batch_recalls)\nprint (""validation recall: %g"" % epoch_recall)\n# compute the val epoch f1:\nepoch_f1 = np.mean(batch_f1s)\nprint (""validation f1: %g"" % epoch_f1)\n\nwith open(""%s/eval_dict_val.pkl"" % network.model_dir, ""wb"") as file:\n    pickle.dump(eval_dict, file, protocol=2) # (protocol=2 is needed to be able to open this file with python2)\n'"
Frustum-PointNet/eval_frustum_pointnet_val_2ddetections.py,8,"b'# camera-ready\n\nfrom datasets import DatasetKittiVal2ddetections, wrapToPi, getBinCenter # (this needs to be imported before torch, because cv2 needs to be imported before torch for some reason)\nfrom frustum_pointnet import FrustumPointNet\n\nimport torch\nimport torch.utils.data\nimport torch.nn as nn\nfrom torch.autograd import Variable\nimport torch.optim as optim\nimport torch.nn.functional as F\n\nimport numpy as np\nimport pickle\n\nbatch_size = 32\n\nnetwork = FrustumPointNet(""Frustum-PointNet_eval_val_2ddetections"", project_dir=""/root/3DOD_thesis"")\nnetwork.load_state_dict(torch.load(""/root/3DOD_thesis/pretrained_models/model_37_2_epoch_400.pth""))\nnetwork = network.cuda()\n\nNH = network.BboxNet_network.NH\n\nval_dataset = DatasetKittiVal2ddetections(kitti_data_path=""/root/3DOD_thesis/data/kitti"",\n                                          kitti_meta_path=""/root/3DOD_thesis/data/kitti/meta"",\n                                          NH=NH)\n\nnum_val_batches = int(len(val_dataset)/batch_size)\n\nval_loader = torch.utils.data.DataLoader(dataset=val_dataset,\n                                          batch_size=batch_size, shuffle=False,\n                                          num_workers=16)\n\nnetwork.eval() # (set in evaluation mode, this affects BatchNorm and dropout)\neval_dict = {}\nfor step, (frustum_point_clouds, img_ids, input_2Dbboxes, frustum_Rs, frustum_angles, empty_frustum_flags, centered_frustum_mean_xyz, mean_car_size, scores_2d) in enumerate(val_loader):\n    if step % 100 == 0:\n        print (""step: %d/%d"" % (step+1, num_val_batches))\n\n    with torch.no_grad(): # (corresponds to setting volatile=True in all variables, this is done during inference to reduce memory consumption)\n        frustum_point_clouds = Variable(frustum_point_clouds) # (shape: (batch_size, num_points, 4))\n        frustum_point_clouds = frustum_point_clouds.transpose(2, 1) # (shape: (batch_size, 4, num_points))\n\n        frustum_point_clouds = frustum_point_clouds.cuda()\n\n        outputs = network(frustum_point_clouds)\n        outputs_InstanceSeg = outputs[0] # (shape: (batch_size, num_points, 2))\n        outputs_TNet = outputs[1] # (shape: (batch_size, 3))\n        outputs_BboxNet = outputs[2] # (shape: (batch_size, 3 + 3 + 2*NH))\n        seg_point_clouds_mean = outputs[3] # (shape: (batch_size, 3))\n        dont_care_mask = outputs[4] # (shape: (batch_size, ))\n\n        ############################################################################\n        # save data for visualization:\n        ############################################################################\n        centered_frustum_mean_xyz = centered_frustum_mean_xyz[0].numpy()\n        mean_car_size = mean_car_size[0].numpy()\n        for i in range(outputs_InstanceSeg.size()[0]):\n            dont_care_mask_value = dont_care_mask[i]\n            empty_frustum_flag = empty_frustum_flags[i]\n\n            # don\'t care about predicted 3Dbboxes that corresponds to empty\n            # point clouds outputted by InstanceSeg, or empty input frustums:\n            if dont_care_mask_value == 1 and empty_frustum_flag == 0:\n                pred_InstanceSeg = outputs_InstanceSeg[i].data.cpu().numpy() # (shape: (num_points, 2))\n                frustum_point_cloud = frustum_point_clouds[i].transpose(1, 0).data.cpu().numpy() # (shape: (num_points, 4))\n                seg_point_cloud_mean = seg_point_clouds_mean[i].data.cpu().numpy() # (shape: (3, ))\n                img_id = img_ids[i]\n                input_2Dbbox = input_2Dbboxes[i] # (shape: (4, ))\n                frustum_R = frustum_Rs[i] # (shape: (3, 3))\n                frustum_angle = frustum_angles[i]\n                score_2d = scores_2d[i]\n\n                unshifted_frustum_point_cloud_xyz = frustum_point_cloud[:, 0:3] + centered_frustum_mean_xyz\n                decentered_frustum_point_cloud_xyz = np.dot(np.linalg.inv(frustum_R), unshifted_frustum_point_cloud_xyz.T).T\n                frustum_point_cloud[:, 0:3] = decentered_frustum_point_cloud_xyz\n\n                row_mask = pred_InstanceSeg[:, 1] > pred_InstanceSeg[:, 0]\n                pred_seg_point_cloud = frustum_point_cloud[row_mask, :]\n\n                pred_center_TNet = np.dot(np.linalg.inv(frustum_R), outputs_TNet[i].data.cpu().numpy() + centered_frustum_mean_xyz + seg_point_cloud_mean) # (shape: (3, )) # NOTE!\n                centroid = seg_point_cloud_mean\n\n                pred_center_BboxNet = np.dot(np.linalg.inv(frustum_R), outputs_BboxNet[i][0:3].data.cpu().numpy() + centered_frustum_mean_xyz + seg_point_cloud_mean + outputs_TNet[i].data.cpu().numpy()) # (shape: (3, )) # NOTE!\n\n                pred_h = outputs_BboxNet[i][3].data.cpu().numpy() + mean_car_size[0]\n                pred_w = outputs_BboxNet[i][4].data.cpu().numpy() + mean_car_size[1]\n                pred_l = outputs_BboxNet[i][5].data.cpu().numpy() + mean_car_size[2]\n\n                pred_bin_scores = outputs_BboxNet[i][6:(6+4)].data.cpu().numpy() # (shape (NH=8, ))\n                pred_residuals = outputs_BboxNet[i][(6+4):].data.cpu().numpy() # (shape (NH=8, ))\n                pred_bin_number = np.argmax(pred_bin_scores)\n                pred_bin_center = getBinCenter(pred_bin_number, NH=NH)\n                pred_residual = pred_residuals[pred_bin_number]\n                pred_centered_r_y = pred_bin_center + pred_residual\n                pred_r_y = wrapToPi(pred_centered_r_y + frustum_angle) # NOTE!\n\n                pred_r_y = pred_r_y.data.cpu().numpy()\n                input_2Dbbox = input_2Dbbox.data.cpu().numpy()\n                score_2d = score_2d.cpu().numpy()\n\n                if img_id not in eval_dict:\n                    eval_dict[img_id] = []\n\n                bbox_dict = {}\n                bbox_dict[""pred_center_TNet""] = pred_center_TNet\n                bbox_dict[""pred_center_BboxNet""] = pred_center_BboxNet\n                bbox_dict[""centroid""] = centroid\n                bbox_dict[""pred_h""] = pred_h\n                bbox_dict[""pred_w""] = pred_w\n                bbox_dict[""pred_l""] = pred_l\n                bbox_dict[""pred_r_y""] = pred_r_y\n                bbox_dict[""input_2Dbbox""] = input_2Dbbox\n                bbox_dict[""score_2d""] = score_2d\n\n                eval_dict[img_id].append(bbox_dict)\n\nwith open(""%s/eval_dict_val_2ddetections.pkl"" % network.model_dir, ""wb"") as file:\n    pickle.dump(eval_dict, file, protocol=2) # (protocol=2 is needed to be able to open this file with python2)\n'"
Frustum-PointNet/eval_frustum_pointnet_val_seq.py,39,"b'# camera-ready\n\nfrom datasets import EvalSequenceDatasetFrustumPointNet, wrapToPi, getBinCenter # (this needs to be imported before torch, because cv2 needs to be imported before torch for some reason)\nfrom frustum_pointnet import FrustumPointNet\n\nimport torch\nimport torch.utils.data\nimport torch.nn as nn\nfrom torch.autograd import Variable\nimport torch.optim as optim\nimport torch.nn.functional as F\n\nimport numpy as np\nimport pickle\nimport matplotlib\nmatplotlib.use(""Agg"")\nimport matplotlib.pyplot as plt\n\nsequence = ""0004""\n\nbatch_size = 32\n\nnetwork = FrustumPointNet(""Frustum-PointNet_eval_val_seq"", project_dir=""/root/3DOD_thesis"")\nnetwork.load_state_dict(torch.load(""/root/3DOD_thesis/pretrained_models/model_37_2_epoch_400.pth""))\nnetwork = network.cuda()\n\nNH = network.BboxNet_network.NH\n\nval_dataset = EvalSequenceDatasetFrustumPointNet(kitti_data_path=""/root/3DOD_thesis/data/kitti"",\n                                                 kitti_meta_path=""/root/3DOD_thesis/data/kitti/meta"",\n                                                 NH=NH, sequence=sequence)\n\nnum_val_batches = int(len(val_dataset)/batch_size)\n\nval_loader = torch.utils.data.DataLoader(dataset=val_dataset,\n                                         batch_size=batch_size, shuffle=False,\n                                         num_workers=16)\n\nregression_loss_func = nn.SmoothL1Loss()\n\nnetwork.eval() # (set in evaluation mode, this affects BatchNorm, dropout etc.)\nbatch_losses = []\nbatch_losses_InstanceSeg = []\nbatch_losses_TNet = []\nbatch_losses_BboxNet = []\nbatch_losses_BboxNet_center = []\nbatch_losses_BboxNet_size = []\nbatch_losses_BboxNet_heading_regr = []\nbatch_losses_BboxNet_heading_class = []\nbatch_losses_BboxNet_heading_class_weighted = []\nbatch_losses_corner = []\nbatch_accuracies = []\nbatch_precisions = []\nbatch_recalls = []\nbatch_f1s = []\nbatch_accuracies_heading_class = []\neval_dict = {}\nfor step, (frustum_point_clouds, labels_InstanceSeg, labels_TNet, labels_BboxNet, labels_corner, labels_corner_flipped, img_ids, input_2Dbboxes, frustum_Rs, frustum_angles, centered_frustum_mean_xyz) in enumerate(val_loader):\n    with torch.no_grad(): # (corresponds to setting volatile=True in all variables, this is done during inference to reduce memory consumption)\n        frustum_point_clouds = Variable(frustum_point_clouds) # (shape: (batch_size, num_points, 4))\n        labels_InstanceSeg = Variable(labels_InstanceSeg) # (shape: (batch_size, num_points))\n        labels_TNet = Variable(labels_TNet) # (shape: (batch_size, 3))\n        labels_BboxNet = Variable(labels_BboxNet) # (shape:(batch_size, 11))\n        labels_corner = Variable(labels_corner) # (shape: (batch_size, 8, 3))\n        labels_corner_flipped = Variable(labels_corner_flipped) # (shape: (batch_size, 8, 3))\n\n        frustum_point_clouds = frustum_point_clouds.transpose(2, 1) # (shape: (batch_size, 4, num_points))\n\n        frustum_point_clouds = frustum_point_clouds.cuda()\n        labels_InstanceSeg = labels_InstanceSeg.cuda()\n        labels_TNet = labels_TNet.cuda()\n        labels_BboxNet = labels_BboxNet.cuda()\n        labels_corner = labels_corner.cuda()\n        labels_corner_flipped = labels_corner_flipped.cuda()\n\n        outputs = network(frustum_point_clouds)\n        outputs_InstanceSeg = outputs[0] # (shape: (batch_size, num_points, 2))\n        outputs_TNet = outputs[1] # (shape: (batch_size, 3))\n        outputs_BboxNet = outputs[2] # (shape: (batch_size, 3 + 3 + 2*NH))\n        seg_point_clouds_mean = outputs[3] # (shape: (batch_size, 3))\n        dont_care_mask = outputs[4] # (shape: (batch_size, ))\n\n        ############################################################################\n        # save data for visualization:\n        ############################################################################\n        centered_frustum_mean_xyz = centered_frustum_mean_xyz[0].numpy()\n        for i in range(outputs_InstanceSeg.size()[0]):\n            dont_care_mask_value = dont_care_mask[i]\n\n            # don\'t care about predicted 3Dbboxes that corresponds to empty point clouds outputted by InstanceSeg:\n            if dont_care_mask_value == 1:\n                pred_InstanceSeg = outputs_InstanceSeg[i].data.cpu().numpy() # (shape: (num_points, 2))\n                frustum_point_cloud = frustum_point_clouds[i].transpose(1, 0).data.cpu().numpy() # (shape: (num_points, 4))\n                label_InstanceSeg = labels_InstanceSeg[i].data.cpu().numpy() # (shape: (num_points, ))\n                seg_point_cloud_mean = seg_point_clouds_mean[i].data.cpu().numpy() # (shape: (3, ))\n                img_id = img_ids[i]\n                #if img_id in [""000006"", ""000007"", ""000008"", ""000009"", ""000010"", ""000011"", ""000012"", ""000013"", ""000014"", ""000015"", ""000016"", ""000017"", ""000018"", ""000019"", ""000020"", ""000021""]:\n                input_2Dbbox = input_2Dbboxes[i] # (shape: (4, ))\n                frustum_R = frustum_Rs[i] # (shape: (3, 3))\n                frustum_angle = frustum_angles[i]\n\n                unshifted_frustum_point_cloud_xyz = frustum_point_cloud[:, 0:3] + centered_frustum_mean_xyz\n                decentered_frustum_point_cloud_xyz = np.dot(np.linalg.inv(frustum_R), unshifted_frustum_point_cloud_xyz.T).T\n                frustum_point_cloud[:, 0:3] = decentered_frustum_point_cloud_xyz\n\n                row_mask = pred_InstanceSeg[:, 1] > pred_InstanceSeg[:, 0]\n                pred_seg_point_cloud = frustum_point_cloud[row_mask, :]\n\n                row_mask = label_InstanceSeg == 1\n                gt_seg_point_cloud = frustum_point_cloud[row_mask, :]\n\n                pred_center_TNet = np.dot(np.linalg.inv(frustum_R), outputs_TNet[i].data.cpu().numpy() + centered_frustum_mean_xyz + seg_point_cloud_mean) # (shape: (3, )) # NOTE!\n                gt_center = np.dot(np.linalg.inv(frustum_R), labels_TNet[i].data.cpu().numpy() + centered_frustum_mean_xyz) # NOTE!\n                centroid = seg_point_cloud_mean\n\n                pred_center_BboxNet = np.dot(np.linalg.inv(frustum_R), outputs_BboxNet[i][0:3].data.cpu().numpy() + centered_frustum_mean_xyz + seg_point_cloud_mean + outputs_TNet[i].data.cpu().numpy()) # (shape: (3, )) # NOTE!\n\n                pred_h = outputs_BboxNet[i][3].data.cpu().numpy() + labels_BboxNet[i][8].data.cpu().numpy()\n                pred_w = outputs_BboxNet[i][4].data.cpu().numpy() + labels_BboxNet[i][9].data.cpu().numpy()\n                pred_l = outputs_BboxNet[i][5].data.cpu().numpy() + labels_BboxNet[i][10].data.cpu().numpy()\n\n                pred_bin_scores = outputs_BboxNet[i][6:(6+4)].data.cpu().numpy() # (shape (NH=8, ))\n                pred_residuals = outputs_BboxNet[i][(6+4):].data.cpu().numpy() # (shape (NH=8, ))\n                pred_bin_number = np.argmax(pred_bin_scores)\n                pred_bin_center = getBinCenter(pred_bin_number, NH=NH)\n                pred_residual = pred_residuals[pred_bin_number]\n                pred_centered_r_y = pred_bin_center + pred_residual\n                pred_r_y = wrapToPi(pred_centered_r_y + frustum_angle) # NOTE!\n\n                gt_h = labels_BboxNet[i][3].data.cpu().numpy()\n                gt_w = labels_BboxNet[i][4].data.cpu().numpy()\n                gt_l = labels_BboxNet[i][5].data.cpu().numpy()\n\n                gt_bin_number = labels_BboxNet[i][6].data.cpu().numpy()\n                gt_bin_center = getBinCenter(gt_bin_number, NH=NH)\n                gt_residual = labels_BboxNet[i][7].data.cpu().numpy()\n                gt_centered_r_y = gt_bin_center + gt_residual\n                gt_r_y = wrapToPi(gt_centered_r_y + frustum_angle) # NOTE!\n\n                pred_r_y = pred_r_y.data.cpu().numpy()\n                gt_r_y = gt_r_y.data.cpu().numpy()\n                input_2Dbbox = input_2Dbbox.data.cpu().numpy()\n\n                if img_id not in eval_dict:\n                    eval_dict[img_id] = []\n\n                bbox_dict = {}\n                # # # # uncomment this if you want to visualize the frustum or the segmentation:\n                # bbox_dict[""frustum_point_cloud""] = frustum_point_cloud\n                # bbox_dict[""pred_seg_point_cloud""] = pred_seg_point_cloud\n                # bbox_dict[""gt_seg_point_cloud""] = gt_seg_point_cloud\n                # # # #\n                bbox_dict[""pred_center_TNet""] = pred_center_TNet\n                bbox_dict[""pred_center_BboxNet""] = pred_center_BboxNet\n                bbox_dict[""gt_center""] = gt_center\n                bbox_dict[""centroid""] = centroid\n                bbox_dict[""pred_h""] = pred_h\n                bbox_dict[""pred_w""] = pred_w\n                bbox_dict[""pred_l""] = pred_l\n                bbox_dict[""pred_r_y""] = pred_r_y\n                bbox_dict[""gt_h""] = gt_h\n                bbox_dict[""gt_w""] = gt_w\n                bbox_dict[""gt_l""] = gt_l\n                bbox_dict[""gt_r_y""] = gt_r_y\n                bbox_dict[""input_2Dbbox""] = input_2Dbbox\n\n                eval_dict[img_id].append(bbox_dict)\n\n        ########################################################################\n        # compute precision, recall etc. for the InstanceSeg:\n        ########################################################################\n        preds = outputs_InstanceSeg.data.cpu().numpy() # (shape: (batch_size, num_points, 2))\n        preds = np.argmax(preds, 2) # (shape: (batch_size, num_points))\n\n        labels_InstanceSeg_np = labels_InstanceSeg.data.cpu().numpy() # (shape: (batch_size, num_points))\n\n        accuracy = np.count_nonzero(preds == labels_InstanceSeg_np)/(preds.shape[0]*preds.shape[1])\n        if np.count_nonzero(preds == 1) > 0:\n            precision = np.count_nonzero(np.logical_and(preds == labels_InstanceSeg_np, preds == 1))/np.count_nonzero(preds == 1) # (TP/(TP + FP))\n        else:\n            precision = -1\n        if np.count_nonzero(labels_InstanceSeg_np == 1) > 0:\n            recall = np.count_nonzero(np.logical_and(preds == labels_InstanceSeg_np, preds == 1))/np.count_nonzero(labels_InstanceSeg_np == 1) # (TP/(TP + FN))\n        else:\n            recall = -1\n        if recall + precision > 0:\n            f1 = 2*recall*precision/(recall + precision)\n        else:\n            f1 = -1\n\n        batch_accuracies.append(accuracy)\n        if precision != -1:\n            batch_precisions.append(precision)\n        if recall != -1:\n            batch_recalls.append(recall)\n        if f1 != -1:\n            batch_f1s.append(f1)\n\n        ########################################################################\n        # compute accuracy for the heading classification:\n        ########################################################################\n        pred_bin_scores = outputs_BboxNet[:, 6:(6+NH)].data.cpu().numpy() # (shape: (batch_size, NH))\n        pred_bin_numbers = np.argmax(pred_bin_scores, 1) # (shape: (batch_size, ))\n        gt_bin_numbers = labels_BboxNet[:, 6].data.cpu().numpy() # (shape: (batch_size, ))\n\n        accuracy_heading_class = np.count_nonzero(pred_bin_numbers == gt_bin_numbers)/(pred_bin_numbers.shape[0])\n        batch_accuracies_heading_class.append(accuracy_heading_class)\n\n        ########################################################################\n        # compute the InstanceSeg loss:\n        ########################################################################\n        outputs_InstanceSeg = outputs_InstanceSeg.view(-1, 2) # (shape (batch_size*num_points, 2))\n        labels_InstanceSeg = labels_InstanceSeg.view(-1, 1) # (shape: (batch_size*num_points, 1))\n        labels_InstanceSeg = labels_InstanceSeg[:, 0] # (shape: (batch_size*num_points, ))\n        loss_InstanceSeg = F.nll_loss(outputs_InstanceSeg, labels_InstanceSeg)\n        loss_InstanceSeg_value = loss_InstanceSeg.data.cpu().numpy()\n        batch_losses_InstanceSeg.append(loss_InstanceSeg_value)\n\n        ########################################################################\n        # compute the TNet loss:\n        ########################################################################\n        # mask entries corresponding to empty seg point clouds (select only the entries which we care about):\n        outputs_TNet = outputs_TNet[dont_care_mask, :] # (shape: (batch_size*, 3))\n        labels_TNet = labels_TNet[dont_care_mask, :] # (shape: (batch_size*, 3))\n        seg_point_clouds_mean = seg_point_clouds_mean[dont_care_mask, :] # (shape: (batch_size*, 3))\n\n        # shift the GT to the seg point clouds local coords:\n        labels_TNet = labels_TNet - seg_point_clouds_mean\n\n        # compute the Huber (smooth L1) loss:\n        if outputs_TNet.size()[0] == 0:\n            loss_TNet = Variable(torch.from_numpy(np.zeros((1, ), dtype=np.float32))).cuda()\n        else:\n            loss_TNet = regression_loss_func(outputs_TNet, labels_TNet)\n\n        loss_TNet_value = loss_TNet.data.cpu().numpy()\n        batch_losses_TNet.append(loss_TNet_value)\n\n        ########################################################################\n        # compute the BboxNet loss:\n        ########################################################################\n        # mask entries corresponding to empty seg point clouds (select only the entries which we care about):\n        outputs_BboxNet = outputs_BboxNet[dont_care_mask, :] # (shape: (batch_size*, 3 + 3 + 2*NH))\n        labels_BboxNet = labels_BboxNet[dont_care_mask, :]# (shape: (batch_size*, 11))\n\n        if outputs_BboxNet.size()[0] == 0:\n            loss_BboxNet = Variable(torch.from_numpy(np.zeros((1, ), dtype=np.float32))).cuda()\n            loss_BboxNet_size = Variable(torch.from_numpy(np.zeros((1, ), dtype=np.float32))).cuda()\n            loss_BboxNet_center = Variable(torch.from_numpy(np.zeros((1, ), dtype=np.float32))).cuda()\n            loss_BboxNet_heading_class = Variable(torch.from_numpy(np.zeros((1, ), dtype=np.float32))).cuda()\n            loss_BboxNet_heading_regr = Variable(torch.from_numpy(np.zeros((1, ), dtype=np.float32))).cuda()\n            ####\n        else:\n            # compute the BboxNet center loss:\n            labels_BboxNet_center = labels_BboxNet[:, 0:3] # (shape: (batch_size*, 3))\n            # # shift the center GT to local coords:\n            labels_BboxNet_center = Variable(labels_BboxNet_center.data - seg_point_clouds_mean.data - outputs_TNet.data).cuda() # (outputs_TNet is a variable outputted by model, so it requires grads, which cant be passed as target to the loss function)\n            # # compute the Huber (smooth L1) loss:\n            outputs_BboxNet_center = outputs_BboxNet[:, 0:3]\n            loss_BboxNet_center = regression_loss_func(outputs_BboxNet_center, labels_BboxNet_center)\n\n            # compute the BboxNet size loss:\n            labels_BboxNet_size = labels_BboxNet[:, 3:6] # (shape: (batch_size*, 3))\n            # # subtract the mean car size in train:\n            labels_BboxNet_size = labels_BboxNet_size - labels_BboxNet[:, 8:]\n            # # compute the Huber (smooth L1) loss:\n            loss_BboxNet_size = regression_loss_func(outputs_BboxNet[:, 3:6], labels_BboxNet_size)\n\n            # compute the BboxNet heading loss\n            # # compute the classification loss:\n            labels_BboxNet_heading_class = Variable(labels_BboxNet[:, 6].data.type(torch.LongTensor)).cuda() # (shape: (batch_size*, ))\n            outputs_BboxNet_heading_class = outputs_BboxNet[:, 6:(6+NH)] # (shape: (batch_size*, NH))\n            loss_BboxNet_heading_class = F.nll_loss(F.log_softmax(outputs_BboxNet_heading_class, dim=1), labels_BboxNet_heading_class)\n            # # compute the regression loss:\n            # # # # get the GT residual for the GT bin:\n            labels_BboxNet_heading_regr = labels_BboxNet[:, 7] # (shape: (batch_size*, ))\n            # # # # get the pred residual for all bins:\n            outputs_BboxNet_heading_regr_all = outputs_BboxNet[:, (6+NH):] # (shape: (batch_size*, 8))\n            # # # # get the pred residual for the GT bin:\n            outputs_BboxNet_heading_regr = outputs_BboxNet_heading_regr_all.gather(1, labels_BboxNet_heading_class.view(-1, 1)) # (shape: (batch_size*, 1))\n            outputs_BboxNet_heading_regr = outputs_BboxNet_heading_regr[:, 0] # (shape: (batch_size*, )\n            # # # # compute the loss:\n            loss_BboxNet_heading_regr = regression_loss_func(outputs_BboxNet_heading_regr, labels_BboxNet_heading_regr)\n            # # compute the total BBoxNet heading loss:\n            loss_BboxNet_heading = loss_BboxNet_heading_class + 10*loss_BboxNet_heading_regr\n\n            # compute the BboxNet total loss:\n            loss_BboxNet = loss_BboxNet_center + loss_BboxNet_size + loss_BboxNet_heading\n\n        loss_BboxNet_value = loss_BboxNet.data.cpu().numpy()\n        batch_losses_BboxNet.append(loss_BboxNet_value)\n\n        loss_BboxNet_size_value = loss_BboxNet_size.data.cpu().numpy()\n        batch_losses_BboxNet_size.append(loss_BboxNet_size_value)\n\n        loss_BboxNet_center_value = loss_BboxNet_center.data.cpu().numpy()\n        batch_losses_BboxNet_center.append(loss_BboxNet_center_value)\n\n        loss_BboxNet_heading_class_value = loss_BboxNet_heading_class.data.cpu().numpy()\n        batch_losses_BboxNet_heading_class.append(loss_BboxNet_heading_class_value)\n\n        loss_BboxNet_heading_regr_value = loss_BboxNet_heading_regr.data.cpu().numpy()\n        batch_losses_BboxNet_heading_regr.append(loss_BboxNet_heading_regr_value)\n\n        ########################################################################\n        # compute the corner loss:\n        ########################################################################\n        # mask entries corresponding to empty seg point clouds (select only the entries which we care about):\n        labels_corner = labels_corner[dont_care_mask]# (shape: (batch_size*, 8, 3))\n        labels_corner_flipped = labels_corner_flipped[dont_care_mask]# (shape: (batch_size*, 8, 3))\n\n        if outputs_BboxNet.size()[0] == 0:\n            loss_corner = Variable(torch.from_numpy(np.zeros((1, ), dtype=np.float32))).cuda()\n        else:\n            outputs_BboxNet_center = outputs_BboxNet[:, 0:3] # (shape: (batch_size*, 3))\n            # shift to the same coords used in the labels for the corner loss:\n            pred_center = outputs_BboxNet_center + seg_point_clouds_mean + outputs_TNet # (shape: (batch_size*, 3))\n            pred_center_unsqeezed = pred_center.unsqueeze(2) # (shape: (batch_size, 3, 1))\n\n            # shift the outputted size to the same ""coords"" used in the labels for the corner loss:\n            outputs_BboxNet_size = outputs_BboxNet[:, 3:6] + labels_BboxNet[:, 8:] # (shape: (batch_size*, 3))\n\n            pred_h = outputs_BboxNet_size[:, 0] # (shape: (batch_size*, ))\n            pred_w = outputs_BboxNet_size[:, 1] # (shape: (batch_size*, ))\n            pred_l = outputs_BboxNet_size[:, 2] # (shape: (batch_size*, ))\n\n            # get the pred residuals for the GT bins:\n            pred_residuals = outputs_BboxNet_heading_regr # (shape: (batch_size*, ))\n\n            Rmat = Variable(torch.zeros(pred_h.size()[0], 3, 3), requires_grad=True).cuda() # (shape: (batch_size*, 3, 3))\n            Rmat[:, 0, 0] = torch.cos(pred_residuals)\n            Rmat[:, 0, 2] = torch.sin(pred_residuals)\n            Rmat[:, 1, 1] = 1\n            Rmat[:, 2, 0] = -torch.sin(pred_residuals)\n            Rmat[:, 2, 2] = torch.cos(pred_residuals)\n\n            p0_orig = Variable(torch.zeros(pred_h.size()[0], 3, 1), requires_grad=True).cuda() # (shape: (batch_size*, 3, 1))\n            p0_orig[:, 0, 0] = pred_l/2.0\n            p0_orig[:, 2, 0] = pred_w/2.0\n\n            p1_orig = Variable(torch.zeros(pred_h.size()[0], 3, 1), requires_grad=True).cuda() # (shape: (batch_size*, 3, 1))\n            p1_orig[:, 0, 0] = -pred_l/2.0\n            p1_orig[:, 2, 0] = pred_w/2.0\n\n            p2_orig = Variable(torch.zeros(pred_h.size()[0], 3, 1), requires_grad=True).cuda() # (shape: (batch_size*, 3, 1))\n            p2_orig[:, 0, 0] = -pred_l/2.0\n            p2_orig[:, 2, 0] = -pred_w/2.0\n\n            p3_orig = Variable(torch.zeros(pred_h.size()[0], 3, 1), requires_grad=True).cuda() # (shape: (batch_size*, 3, 1))\n            p3_orig[:, 0, 0] = pred_l/2.0\n            p3_orig[:, 2, 0] = -pred_w/2.0\n\n            p4_orig = Variable(torch.zeros(pred_h.size()[0], 3, 1), requires_grad=True).cuda() # (shape: (batch_size*, 3, 1))\n            p4_orig[:, 0, 0] = pred_l/2.0\n            p4_orig[:, 1, 0] = -pred_h\n            p4_orig[:, 2, 0] = pred_w/2.0\n\n            p5_orig = Variable(torch.zeros(pred_h.size()[0], 3, 1), requires_grad=True).cuda() # (shape: (batch_size*, 3, 1))\n            p5_orig[:, 0, 0] = -pred_l/2.0\n            p5_orig[:, 1, 0] = -pred_h\n            p5_orig[:, 2, 0] = pred_w/2.0\n\n            p6_orig = Variable(torch.zeros(pred_h.size()[0], 3, 1), requires_grad=True).cuda() # (shape: (batch_size*, 3, 1))\n            p6_orig[:, 0, 0] = -pred_l/2.0\n            p6_orig[:, 1, 0] = -pred_h\n            p6_orig[:, 2, 0] = -pred_w/2.0\n\n            p7_orig = Variable(torch.zeros(pred_h.size()[0], 3, 1), requires_grad=True).cuda() # (shape: (batch_size*, 3, 1))\n            p7_orig[:, 0, 0] = pred_l/2.0\n            p7_orig[:, 1, 0] = -pred_h\n            p7_orig[:, 2, 0] = -pred_w/2.0\n\n            pred_p0_unsqeezed = pred_center_unsqeezed + torch.bmm(Rmat, p0_orig) # (shape: (batch_size*, 3, 1))\n            pred_p0 = pred_p0_unsqeezed[:, :, 0] # (shape: (batch_size*, 3))\n            pred_p1_unsqeezed = pred_center_unsqeezed + torch.bmm(Rmat, p1_orig) # (shape: (batch_size*, 3, 1))\n            pred_p1 = pred_p1_unsqeezed[:, :, 0] # (shape: (batch_size*, 3))\n            pred_p2_unsqeezed = pred_center_unsqeezed + torch.bmm(Rmat, p2_orig) # (shape: (batch_size*, 3, 1))\n            pred_p2 = pred_p2_unsqeezed[:, :, 0] # (shape: (batch_size*, 3))\n            pred_p3_unsqeezed = pred_center_unsqeezed + torch.bmm(Rmat, p3_orig) # (shape: (batch_size*, 3, 1))\n            pred_p3 = pred_p3_unsqeezed[:, :, 0] # (shape: (batch_size*, 3))\n            pred_p4_unsqeezed = pred_center_unsqeezed + torch.bmm(Rmat, p4_orig) # (shape: (batch_size*, 3, 1))\n            pred_p4 = pred_p4_unsqeezed[:, :, 0] # (shape: (batch_size*, 3))\n            pred_p5_unsqeezed = pred_center_unsqeezed + torch.bmm(Rmat, p5_orig) # (shape: (batch_size*, 3, 1))\n            pred_p5 = pred_p5_unsqeezed[:, :, 0] # (shape: (batch_size*, 3))\n            pred_p6_unsqeezed = pred_center_unsqeezed + torch.bmm(Rmat, p6_orig) # (shape: (batch_size*, 3, 1))\n            pred_p6 = pred_p6_unsqeezed[:, :, 0] # (shape: (batch_size*, 3))\n            pred_p7_unsqeezed = pred_center_unsqeezed + torch.bmm(Rmat, p7_orig) # (shape: (batch_size*, 3, 1))\n            pred_p7 = pred_p7_unsqeezed[:, :, 0] # (shape: (batch_size*, 3))\n\n            outputs_corner = Variable(torch.zeros(pred_h.size()[0], 8, 3), requires_grad=True).cuda() # (shape: (batch_size*, 8, 3))\n            outputs_corner[:, 0] = pred_p0\n            outputs_corner[:, 1] = pred_p1\n            outputs_corner[:, 2] = pred_p2\n            outputs_corner[:, 3] = pred_p3\n            outputs_corner[:, 4] = pred_p4\n            outputs_corner[:, 5] = pred_p5\n            outputs_corner[:, 6] = pred_p6\n            outputs_corner[:, 7] = pred_p7\n\n            loss_corner_unflipped = regression_loss_func(outputs_corner, labels_corner)\n            loss_corner_flipped = regression_loss_func(outputs_corner, labels_corner_flipped)\n\n            loss_corner = torch.min(loss_corner_unflipped, loss_corner_flipped)\n\n        loss_corner_value = loss_corner.data.cpu().numpy()\n        batch_losses_corner.append(loss_corner_value)\n\n        ########################################################################\n        # compute the total loss:\n        ########################################################################\n        lambda_value = 1\n        gamma_value = 10\n        loss = loss_InstanceSeg + lambda_value*(loss_TNet + loss_BboxNet + gamma_value*loss_corner)\n        loss_value = loss.data.cpu().numpy()\n        batch_losses.append(loss_value)\n\n# compute the val epoch loss:\nepoch_loss = np.mean(batch_losses)\nprint (""validation loss: %g"" % epoch_loss)\n# compute the val epoch TNet loss:\nepoch_loss = np.mean(batch_losses_TNet)\nprint (""validation TNet loss: %g"" % epoch_loss)\n# compute the val epoch InstanceSeg loss:\nepoch_loss = np.mean(batch_losses_InstanceSeg)\nprint (""validation InstanceSeg loss: %g"" % epoch_loss)\n# compute the val epoch BboxNet loss:\nepoch_loss = np.mean(batch_losses_BboxNet)\nprint (""validation BboxNet loss: %g"" % epoch_loss)\n# compute the val epoch BboxNet size loss:\nepoch_loss = np.mean(batch_losses_BboxNet_size)\nprint (""validation BboxNet size loss: %g"" % epoch_loss)\n# compute the val epoch BboxNet center loss:\nepoch_loss = np.mean(batch_losses_BboxNet_center)\nprint (""validation BboxNet center loss: %g"" % epoch_loss)\n# compute the val epoch BboxNet heading class loss:\nepoch_loss = np.mean(batch_losses_BboxNet_heading_class)\nprint (""validation BboxNet heading class loss: %g"" % epoch_loss)\n# compute the val epoch BboxNet heading regr loss:\nepoch_loss = np.mean(batch_losses_BboxNet_heading_regr)\nprint (""validation BboxNet heading regr loss: %g"" % epoch_loss)\n# compute the val epoch heading class accuracy:\nepoch_accuracy = np.mean(batch_accuracies_heading_class)\nprint (""validation heading class accuracy: %g"" % epoch_accuracy)\n# compute the val epoch corner loss:\nepoch_loss = np.mean(batch_losses_corner)\nprint (""validation corner loss: %g"" % epoch_loss)\n# compute the val epoch accuracy:\nepoch_accuracy = np.mean(batch_accuracies)\nprint (""validation accuracy: %g"" % epoch_accuracy)\n# compute the val epoch precision:\nepoch_precision = np.mean(batch_precisions)\nprint (""validation precision: %g"" % epoch_precision)\n# compute the val epoch recall:\nepoch_recall = np.mean(batch_recalls)\nprint (""validation recall: %g"" % epoch_recall)\n# compute the val epoch f1:\nepoch_f1 = np.mean(batch_f1s)\nprint (""validation f1: %g"" % epoch_f1)\n\nwith open(""%s/eval_dict_val_seq_%s.pkl"" % (network.model_dir, sequence), ""wb"") as file:\n    pickle.dump(eval_dict, file, protocol=2) # (protocol=2 is needed to be able to open this file with python2)\n'"
Frustum-PointNet/frustum_pointnet.py,11,"b'# camera-ready\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\n\nimport os\nimport numpy as np\n\nclass InstanceSeg(nn.Module):\n    def __init__(self, num_points=1024):\n        super(InstanceSeg, self).__init__()\n\n        self.num_points = num_points\n\n        self.conv1 = nn.Conv1d(4, 64, 1)\n        self.conv2 = nn.Conv1d(64, 64, 1)\n        self.conv3 = nn.Conv1d(64, 64, 1)\n        self.conv4 = nn.Conv1d(64, 128, 1)\n        self.conv5 = nn.Conv1d(128, 1024, 1)\n        self.conv6 = nn.Conv1d(1088, 512, 1)\n        self.conv7 = nn.Conv1d(512, 256, 1)\n        self.conv8 = nn.Conv1d(256, 128, 1)\n        self.conv9 = nn.Conv1d(128, 128, 1)\n        self.conv10 = nn.Conv1d(128, 2, 1)\n        self.max_pool = nn.MaxPool1d(num_points)\n\n    def forward(self, x):\n        batch_size = x.size()[0] # (x has shape (batch_size, 4, num_points))\n\n        out = F.relu(self.conv1(x)) # (shape: (batch_size, 64, num_points))\n        out = F.relu(self.conv2(out)) # (shape: (batch_size, 64, num_points))\n        point_features = out\n\n        out = F.relu(self.conv3(out)) # (shape: (batch_size, 64, num_points))\n        out = F.relu(self.conv4(out)) # (shape: (batch_size, 128, num_points))\n        out = F.relu(self.conv5(out)) # (shape: (batch_size, 1024, num_points))\n        global_feature = self.max_pool(out) # (shape: (batch_size, 1024, 1))\n\n        global_feature_repeated = global_feature.repeat(1, 1, self.num_points) # (shape: (batch_size, 1024, num_points))\n        out = torch.cat([global_feature_repeated, point_features], 1) # (shape: (batch_size, 1024+64=1088, num_points))\n\n        out = F.relu(self.conv6(out)) # (shape: (batch_size, 512, num_points))\n        out = F.relu(self.conv7(out)) # (shape: (batch_size, 256, num_points))\n        out = F.relu(self.conv8(out)) # (shape: (batch_size, 128, num_points))\n        out = F.relu(self.conv9(out)) # (shape: (batch_size, 128, num_points))\n\n        out = self.conv10(out) # (shape: (batch_size, 2, num_points))\n\n        out = out.transpose(2,1).contiguous() # (shape: (batch_size, num_points, 2))\n        out = F.log_softmax(out.view(-1, 2), dim=1) # (shape: (batch_size*num_points, 2))\n        out = out.view(batch_size, self.num_points, 2) # (shape: (batch_size, num_points, 2))\n\n        return out\n\nclass TNet(nn.Module):\n    def __init__(self, num_seg_points=512):\n        super(TNet, self).__init__()\n\n        self.num_seg_points = num_seg_points\n\n        self.conv1 = nn.Conv1d(3, 128, 1)\n        self.conv2 = nn.Conv1d(128, 256, 1)\n        self.conv3 = nn.Conv1d(256, 512, 1)\n        self.max_pool = nn.MaxPool1d(num_seg_points)\n        self.fc1 = nn.Linear(512, 256)\n        self.fc2 = nn.Linear(256, 128)\n        self.fc3 = nn.Linear(128, 3)\n\n    def forward(self, x):\n        # (x has shape (batch_size, 3, num_seg_points))\n\n        out = F.relu(self.conv1(x)) # (shape: (batch_size, 128, num_seg_points))\n        out = F.relu(self.conv2(out)) # (shape: (batch_size, 256, num_seg_points))\n        out = F.relu(self.conv3(out)) # (shape: (batch_size, 512, num_seg_points))\n        out = self.max_pool(out) # (shape: (batch_size, 512, 1))\n        out = out.view(-1, 512) # (shape: (batch_size, 512))\n\n        out = F.relu(self.fc1(out)) # (shape: (batch_size, 256))\n        out = F.relu(self.fc2(out)) # (shape: (batch_size, 128))\n\n        out = self.fc3(out) # (shape: (batch_size, 3))\n\n        return out\n\nclass BboxNet(nn.Module):\n    def __init__(self, num_seg_points=512):\n        super(BboxNet, self).__init__()\n\n        self.NH = 4 # (number of angle bins)\n\n        self.num_seg_points = num_seg_points\n\n        self.conv1 = nn.Conv1d(3, 128, 1)\n        self.conv2 = nn.Conv1d(128, 128, 1)\n        self.conv3 = nn.Conv1d(128, 256, 1)\n        self.conv4 = nn.Conv1d(256, 512, 1)\n        self.max_pool = nn.MaxPool1d(num_seg_points)\n        self.fc1 = nn.Linear(512, 512)\n        self.fc2 = nn.Linear(512, 256)\n        self.fc3 = nn.Linear(256, 3 + 3 + 2*self.NH)\n\n    def forward(self, x):\n        # (x has shape (batch_size, 3, num_seg_points))\n\n        out = F.relu(self.conv1(x)) # (shape: (batch_size, 128, num_seg_points))\n        out = F.relu(self.conv2(out)) # (shape: (batch_size, 128, num_seg_points))\n        out = F.relu(self.conv3(out)) # (shape: (batch_size, 256, num_seg_points))\n        out = F.relu(self.conv4(out)) # (shape: (batch_size, 512, num_seg_points))\n        out = self.max_pool(out) # (shape: (batch_size, 512, 1))\n        out = out.view(-1, 512) # (shape: (batch_size, 512))\n\n        out = F.relu(self.fc1(out)) # (shape: (batch_size, 512))\n        out = F.relu(self.fc2(out)) # (shape: (batch_size, 256))\n\n        out = self.fc3(out) # (shape: (batch_size, 3 + 3 + 2*NH))\n\n        return out\n\nclass FrustumPointNet(nn.Module):\n    def __init__(self, model_id, project_dir, num_points=1024):\n        super(FrustumPointNet, self).__init__()\n\n        self.num_points = num_points\n        self.model_id = model_id\n        self.project_dir = project_dir\n        self.create_model_dirs()\n\n        self.InstanceSeg_network = InstanceSeg()\n        self.TNet_network = TNet()\n        self.BboxNet_network = BboxNet()\n\n    def forward(self, x):\n        batch_size = x.size()[0] # (x has shape (batch_size, 4, num_points))\n\n        out_InstanceSeg = self.InstanceSeg_network(x) # (shape (batch_size, num_points, 2))\n\n        ########################################################################\n        point_clouds = x.transpose(2, 1)[:, :, 0:3].data.cpu().numpy() # (shape: (batch_size, num_points, 3))\n        seg_scores = out_InstanceSeg.data.cpu().numpy() # (shape: (batch_size, num_points_ 2))\n\n        seg_point_clouds = np.zeros((0, 512, 3), dtype=np.float32) # (shape: (batch_size, 512=num_seg_points, 3))\n        out_dont_care_mask = torch.ones((batch_size, )) # (shape: (batch_size, ))\n        out_dont_care_mask = out_dont_care_mask.type(torch.ByteTensor).cuda() # (NOTE! ByteTensor is needed for this to act as a selction mask)\n        for i in range(seg_scores.shape[0]):\n            ex_seg_scores = seg_scores[i] # (shape: (num_points, 2))\n            ex_point_cloud = point_clouds[i] # (shape: (num_points, 3))\n\n            row_mask = ex_seg_scores[:, 1] > ex_seg_scores[:, 0]\n            ex_seg_point_cloud = ex_point_cloud[row_mask, :]\n\n            if ex_seg_point_cloud.shape[0] == 0: # (if the segmented point cloud is empty:)\n                ex_seg_point_cloud = np.zeros((512, 3), dtype=np.float32)\n                out_dont_care_mask[i] = 0\n\n            # randomly sample 512 points in ex_seg_point_cloud:\n            if ex_seg_point_cloud.shape[0] < 512:\n                row_idx = np.random.choice(ex_seg_point_cloud.shape[0], 512, replace=True)\n            else:\n                row_idx = np.random.choice(ex_seg_point_cloud.shape[0], 512, replace=False)\n            ex_seg_point_cloud = ex_seg_point_cloud[row_idx, :]\n\n            seg_point_clouds = np.concatenate((seg_point_clouds, [ex_seg_point_cloud]), axis=0)\n\n        # subtract the point cloud centroid from each seg_point_cloud (transform to local coords):\n        seg_point_clouds_mean = np.mean(seg_point_clouds, axis=1) # (shape: (batch_size, 3)) (seg_point_clouds has shape (batch_size, num_seg_points, 3))\n        out_seg_point_clouds_mean = Variable(torch.from_numpy(seg_point_clouds_mean)).cuda()\n        seg_point_clouds_mean = np.expand_dims(seg_point_clouds_mean, axis=1) # (shape: (batch_size, 1, 3))\n        seg_point_clouds = seg_point_clouds - seg_point_clouds_mean\n\n        seg_point_clouds = Variable(torch.from_numpy(seg_point_clouds)).cuda() # (shape: (batch_size, num_seg_points, 3))\n        seg_point_clouds = seg_point_clouds.transpose(2, 1) # (shape: (batch_size, 3, num_seg_points))\n        ########################################################################\n\n        out_TNet = self.TNet_network(seg_point_clouds) # (shape: (batch_size, 3))\n\n        # subtract the TNet predicted translation from each seg_point_cloud (transfrom to local coords):\n        seg_point_clouds = seg_point_clouds - out_TNet.unsqueeze(2).repeat(1, 1, seg_point_clouds.size()[2]) # (out_TNet.unsqueeze(2).repeat(1, 1, seg_point_clouds.size()[2]) has shape: (batch_size, 3, num_seg_points))\n\n        out_BboxNet = self.BboxNet_network(seg_point_clouds) # (shape: (batch_size, 3 + 3 + 2*NH))\n\n        # (out_InstanceSeg has shape: (batch_size, num_points, 2))\n        # (out_seg_point_clouds_mean has shape: (batch_size, 3))\n        # (out_dont_care_mask has shape: (batch_size, ))\n\n        return (out_InstanceSeg, out_TNet, out_BboxNet, out_seg_point_clouds_mean, out_dont_care_mask)\n\n    def create_model_dirs(self):\n        self.logs_dir = self.project_dir + ""/training_logs""\n        self.model_dir = self.logs_dir + ""/model_%s"" % self.model_id\n        self.checkpoints_dir = self.model_dir + ""/checkpoints""\n        if not os.path.exists(self.logs_dir):\n            os.makedirs(self.logs_dir)\n        if not os.path.exists(self.model_dir):\n            os.makedirs(self.model_dir)\n            os.makedirs(self.checkpoints_dir)\n\n# from torch.autograd import Variable\n# x = Variable(torch.randn(32, 3, 512))\n# network = BboxNet()\n# out = network(x)\n\n# x = Variable(torch.randn(32, 4, 1024)).cuda()\n# network = FrustumPointNet(""frustum_pointnet_test"", ""/staging/frexgus/frustum_pointnet"")\n# network = network.cuda()\n# out = network(x)\n'"
Frustum-PointNet/train_frustum_pointnet.py,73,"b'# camera-ready\n\nfrom datasets import DatasetFrustumPointNetAugmentation, EvalDatasetFrustumPointNet, getBinCenters, wrapToPi # (this needs to be imported before torch, because cv2 needs to be imported before torch for some reason)\nfrom frustum_pointnet import FrustumPointNet\n\nimport torch\nimport torch.utils.data\nimport torch.nn as nn\nfrom torch.autograd import Variable\nimport torch.optim as optim\nimport torch.nn.functional as F\n\nimport numpy as np\nimport pickle\nimport matplotlib\nmatplotlib.use(""Agg"")\nimport matplotlib.pyplot as plt\n\n# NOTE! NOTE! change this to not overwrite all log data when you train the model:\nmodel_id = ""Frustum-PointNet_1""\n\nnum_epochs = 700\nbatch_size = 32\nlearning_rate = 0.001\n\nnetwork = FrustumPointNet(model_id, project_dir=""/root/3DOD_thesis"")\nnetwork = network.cuda()\n\nNH = network.BboxNet_network.NH\n\ntrain_dataset = DatasetFrustumPointNetAugmentation(kitti_data_path=""/root/3DOD_thesis/data/kitti"",\n                                                   kitti_meta_path=""/root/3DOD_thesis/data/kitti/meta"",\n                                                   type=""train"", NH=NH)\nval_dataset = EvalDatasetFrustumPointNet(kitti_data_path=""/root/3DOD_thesis/data/kitti"",\n                                         kitti_meta_path=""/root/3DOD_thesis/data/kitti/meta"",\n                                         type=""val"", NH=NH)\n\nnum_train_batches = int(len(train_dataset)/batch_size)\nnum_val_batches = int(len(val_dataset)/batch_size)\n\nprint (""num_train_batches:"", num_train_batches)\nprint (""num_val_batches:"", num_val_batches)\n\ntrain_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n                                           batch_size=batch_size, shuffle=True,\n                                           num_workers=16)\nval_loader = torch.utils.data.DataLoader(dataset=val_dataset,\n                                         batch_size=batch_size, shuffle=False,\n                                         num_workers=16)\n\nregression_loss_func = nn.SmoothL1Loss()\n\noptimizer = torch.optim.Adam(network.parameters(), lr=learning_rate)\n\nepoch_losses_train = []\nepoch_losses_InstanceSeg_train = []\nepoch_losses_TNet_train = []\nepoch_losses_BboxNet_train = []\nepoch_losses_BboxNet_size_train = []\nepoch_losses_BboxNet_center_train = []\nepoch_losses_BboxNet_heading_class_train = []\nepoch_losses_BboxNet_heading_regr_train = []\nepoch_accuracies_heading_class_train = []\nepoch_losses_corner_train = []\nepoch_accuracies_train = []\nepoch_precisions_train = []\nepoch_recalls_train = []\nepoch_f1s_train = []\nepoch_losses_val = []\nepoch_losses_InstanceSeg_val = []\nepoch_losses_TNet_val = []\nepoch_losses_BboxNet_val = []\nepoch_losses_BboxNet_size_val = []\nepoch_losses_BboxNet_center_val = []\nepoch_losses_BboxNet_heading_class_val = []\nepoch_losses_BboxNet_heading_regr_val = []\nepoch_accuracies_heading_class_val = []\nepoch_losses_corner_val = []\nepoch_accuracies_val = []\nepoch_precisions_val = []\nepoch_recalls_val = []\nepoch_f1s_val = []\nfor epoch in range(num_epochs):\n    print (""###########################"")\n    print (""######## NEW EPOCH ########"")\n    print (""###########################"")\n    print (""epoch: %d/%d"" % (epoch+1, num_epochs))\n\n    if epoch % 100 == 0 and epoch > 0:\n        learning_rate = learning_rate/2\n        optimizer = torch.optim.Adam(network.parameters(), lr=learning_rate)\n\n    print (""learning_rate:"")\n    print (learning_rate)\n\n    ################################################################################\n    # train:\n    ################################################################################\n    network.train() # (set in training mode, this affects BatchNorm and dropout)\n    batch_losses = []\n    batch_losses_InstanceSeg = []\n    batch_losses_TNet = []\n    batch_losses_BboxNet = []\n    batch_losses_BboxNet_center = []\n    batch_losses_BboxNet_size = []\n    batch_losses_BboxNet_heading_regr = []\n    batch_losses_BboxNet_heading_class = []\n    batch_losses_corner = []\n    batch_accuracies = []\n    batch_precisions = []\n    batch_recalls = []\n    batch_f1s = []\n    batch_accuracies_heading_class = []\n    for step, (frustum_point_clouds, labels_InstanceSeg, labels_TNet, labels_BboxNet, labels_corner, labels_corner_flipped) in enumerate(train_loader):\n        frustum_point_clouds = Variable(frustum_point_clouds) # (shape: (batch_size, num_points, 4))\n        labels_InstanceSeg = Variable(labels_InstanceSeg) # (shape: (batch_size, num_points))\n        labels_TNet = Variable(labels_TNet) # (shape: (batch_size, 3))\n        labels_BboxNet = Variable(labels_BboxNet) # (shape:(batch_size, 11))\n        labels_corner = Variable(labels_corner) # (shape: (batch_size, 8, 3))\n        labels_corner_flipped = Variable(labels_corner_flipped) # (shape: (batch_size, 8, 3))\n\n        frustum_point_clouds = frustum_point_clouds.transpose(2, 1) # (shape: (batch_size, 4, num_points))\n\n        frustum_point_clouds = frustum_point_clouds.cuda()\n        labels_InstanceSeg = labels_InstanceSeg.cuda()\n        labels_TNet = labels_TNet.cuda()\n        labels_BboxNet = labels_BboxNet.cuda()\n        labels_corner = labels_corner.cuda()\n        labels_corner_flipped = labels_corner_flipped.cuda()\n\n        outputs = network(frustum_point_clouds)\n        outputs_InstanceSeg = outputs[0] # (shape: (batch_size, num_points, 2))\n        outputs_TNet = outputs[1] # (shape: (batch_size, 3))\n        outputs_BboxNet = outputs[2] # (shape: (batch_size, 3 + 3 + 2*NH))\n        seg_point_clouds_mean = outputs[3] # (shape: (batch_size, 3))\n        dont_care_mask = outputs[4] # (shape: (batch_size, ))\n\n        ########################################################################\n        # compute precision, recall etc. for the InstanceSeg:\n        ########################################################################\n        preds = outputs_InstanceSeg.data.cpu().numpy() # (shape: (batch_size, num_points, 2))\n        preds = np.argmax(preds, 2) # (shape: (batch_size, num_points))\n\n        labels_InstanceSeg_np = labels_InstanceSeg.data.cpu().numpy() # (shape: (batch_size, num_points))\n\n        accuracy = np.count_nonzero(preds == labels_InstanceSeg_np)/(preds.shape[0]*preds.shape[1])\n        if np.count_nonzero(preds == 1) > 0:\n            precision = np.count_nonzero(np.logical_and(preds == labels_InstanceSeg_np, preds == 1))/np.count_nonzero(preds == 1) # (TP/(TP + FP))\n        else:\n            precision = -1\n        if np.count_nonzero(labels_InstanceSeg_np == 1) > 0:\n            recall = np.count_nonzero(np.logical_and(preds == labels_InstanceSeg_np, preds == 1))/np.count_nonzero(labels_InstanceSeg_np == 1) # (TP/(TP + FN))\n        else:\n            recall = -1\n        if recall + precision > 0:\n            f1 = 2*recall*precision/(recall + precision)\n        else:\n            f1 = -1\n\n        batch_accuracies.append(accuracy)\n        if precision != -1:\n            batch_precisions.append(precision)\n        if recall != -1:\n            batch_recalls.append(recall)\n        if f1 != -1:\n            batch_f1s.append(f1)\n\n        ########################################################################\n        # compute accuracy for the heading classification:\n        ########################################################################\n        pred_bin_scores = outputs_BboxNet[:, 6:(6+NH)].data.cpu().numpy() # (shape: (batch_size, NH))\n        pred_bin_numbers = np.argmax(pred_bin_scores, 1) # (shape: (batch_size, ))\n        gt_bin_numbers = labels_BboxNet[:, 6].data.cpu().numpy() # (shape: (batch_size, ))\n\n        accuracy_heading_class = np.count_nonzero(pred_bin_numbers == gt_bin_numbers)/(pred_bin_numbers.shape[0])\n        batch_accuracies_heading_class.append(accuracy_heading_class)\n\n        ########################################################################\n        # compute the InstanceSeg loss:\n        ########################################################################\n        outputs_InstanceSeg = outputs_InstanceSeg.view(-1, 2) # (shape (batch_size*num_points, 2))\n        labels_InstanceSeg = labels_InstanceSeg.view(-1, 1) # (shape: (batch_size*num_points, 1))\n        labels_InstanceSeg = labels_InstanceSeg[:, 0] # (shape: (batch_size*num_points, ))\n        loss_InstanceSeg = F.nll_loss(outputs_InstanceSeg, labels_InstanceSeg)\n        loss_InstanceSeg_value = loss_InstanceSeg.data.cpu().numpy()\n        batch_losses_InstanceSeg.append(loss_InstanceSeg_value)\n\n        ########################################################################\n        # compute the TNet loss:\n        ########################################################################\n        # mask entries corresponding to empty seg point clouds (select only the entries which we care about):\n        outputs_TNet = outputs_TNet[dont_care_mask, :] # (shape: (batch_size*, 3))\n        labels_TNet = labels_TNet[dont_care_mask, :] # (shape: (batch_size*, 3))\n        seg_point_clouds_mean = seg_point_clouds_mean[dont_care_mask, :] # (shape: (batch_size*, 3))\n\n        # shift the GT to the seg point clouds local coords:\n        labels_TNet = labels_TNet - seg_point_clouds_mean\n\n        # compute the Huber (smooth L1) loss:\n        if outputs_TNet.size()[0] == 0:\n            loss_TNet = Variable(torch.from_numpy(np.zeros((1, ), dtype=np.float32))).cuda()\n        else:\n            loss_TNet = regression_loss_func(outputs_TNet, labels_TNet)\n\n        loss_TNet_value = loss_TNet.data.cpu().numpy()\n        batch_losses_TNet.append(loss_TNet_value)\n\n        ########################################################################\n        # compute the BboxNet loss:\n        ########################################################################\n        # mask entries corresponding to empty seg point clouds (select only the entries which we care about):\n        outputs_BboxNet = outputs_BboxNet[dont_care_mask, :] # (shape: (batch_size*, 3 + 3 + 2*NH))\n        labels_BboxNet = labels_BboxNet[dont_care_mask, :]# (shape: (batch_size*, 11))\n\n        if outputs_BboxNet.size()[0] == 0:\n            loss_BboxNet = Variable(torch.from_numpy(np.zeros((1, ), dtype=np.float32))).cuda()\n            loss_BboxNet_size = Variable(torch.from_numpy(np.zeros((1, ), dtype=np.float32))).cuda()\n            loss_BboxNet_center = Variable(torch.from_numpy(np.zeros((1, ), dtype=np.float32))).cuda()\n            loss_BboxNet_heading_class = Variable(torch.from_numpy(np.zeros((1, ), dtype=np.float32))).cuda()\n            loss_BboxNet_heading_regr = Variable(torch.from_numpy(np.zeros((1, ), dtype=np.float32))).cuda()\n            ####\n        else:\n            # compute the BboxNet center loss:\n            labels_BboxNet_center = labels_BboxNet[:, 0:3] # (shape: (batch_size*, 3))\n            # # shift the center GT to local coords:\n            labels_BboxNet_center = Variable(labels_BboxNet_center.data - seg_point_clouds_mean.data - outputs_TNet.data).cuda() # (outputs_TNet is a variable outputted by model, so it requires grads, which cant be passed as target to the loss function)\n            # # compute the Huber (smooth L1) loss:\n            outputs_BboxNet_center = outputs_BboxNet[:, 0:3]\n            loss_BboxNet_center = regression_loss_func(outputs_BboxNet_center, labels_BboxNet_center)\n\n            # compute the BboxNet size loss:\n            labels_BboxNet_size = labels_BboxNet[:, 3:6] # (shape: (batch_size*, 3))\n            # # subtract the mean car size in train:\n            labels_BboxNet_size = labels_BboxNet_size - labels_BboxNet[:, 8:]\n            # # compute the Huber (smooth L1) loss:\n            loss_BboxNet_size = regression_loss_func(outputs_BboxNet[:, 3:6], labels_BboxNet_size)\n\n            # compute the BboxNet heading loss\n            # # compute the classification loss:\n            labels_BboxNet_heading_class = Variable(labels_BboxNet[:, 6].data.type(torch.LongTensor)).cuda() # (shape: (batch_size*, ))\n            outputs_BboxNet_heading_class = outputs_BboxNet[:, 6:(6+NH)] # (shape: (batch_size*, NH))\n            loss_BboxNet_heading_class = F.nll_loss(F.log_softmax(outputs_BboxNet_heading_class, dim=1), labels_BboxNet_heading_class)\n            # # compute the regression loss:\n            # # # # get the GT residual for the GT bin:\n            labels_BboxNet_heading_regr = labels_BboxNet[:, 7] # (shape: (batch_size*, ))\n            # # # # get the pred residual for all bins:\n            outputs_BboxNet_heading_regr_all = outputs_BboxNet[:, (6+NH):] # (shape: (batch_size*, 8))\n            # # # # get the pred residual for the GT bin:\n            outputs_BboxNet_heading_regr = outputs_BboxNet_heading_regr_all.gather(1, labels_BboxNet_heading_class.view(-1, 1)) # (shape: (batch_size*, 1))\n            outputs_BboxNet_heading_regr = outputs_BboxNet_heading_regr[:, 0] # (shape: (batch_size*, )\n            # # # # compute the loss:\n            loss_BboxNet_heading_regr = regression_loss_func(outputs_BboxNet_heading_regr, labels_BboxNet_heading_regr)\n            # # compute the total BBoxNet heading loss:\n            loss_BboxNet_heading = loss_BboxNet_heading_class + 10*loss_BboxNet_heading_regr\n\n            # compute the BboxNet total loss:\n            loss_BboxNet = loss_BboxNet_center + loss_BboxNet_size + loss_BboxNet_heading\n\n        loss_BboxNet_value = loss_BboxNet.data.cpu().numpy()\n        batch_losses_BboxNet.append(loss_BboxNet_value)\n\n        loss_BboxNet_size_value = loss_BboxNet_size.data.cpu().numpy()\n        batch_losses_BboxNet_size.append(loss_BboxNet_size_value)\n\n        loss_BboxNet_center_value = loss_BboxNet_center.data.cpu().numpy()\n        batch_losses_BboxNet_center.append(loss_BboxNet_center_value)\n\n        loss_BboxNet_heading_class_value = loss_BboxNet_heading_class.data.cpu().numpy()\n        batch_losses_BboxNet_heading_class.append(loss_BboxNet_heading_class_value)\n\n        loss_BboxNet_heading_regr_value = loss_BboxNet_heading_regr.data.cpu().numpy()\n        batch_losses_BboxNet_heading_regr.append(loss_BboxNet_heading_regr_value)\n\n        ########################################################################\n        # compute the corner loss:\n        ########################################################################\n        # mask entries corresponding to empty seg point clouds (select only the entries which we care about):\n        labels_corner = labels_corner[dont_care_mask]# (shape: (batch_size*, 8, 3))\n        labels_corner_flipped = labels_corner_flipped[dont_care_mask]# (shape: (batch_size*, 8, 3))\n\n        if outputs_BboxNet.size()[0] == 0:\n            loss_corner = Variable(torch.from_numpy(np.zeros((1, ), dtype=np.float32))).cuda()\n        else:\n            outputs_BboxNet_center = outputs_BboxNet[:, 0:3] # (shape: (batch_size*, 3))\n            # shift to the same coords used in the labels for the corner loss:\n            pred_center = outputs_BboxNet_center + seg_point_clouds_mean + outputs_TNet # (shape: (batch_size*, 3))\n            pred_center_unsqeezed = pred_center.unsqueeze(2) # (shape: (batch_size, 3, 1))\n\n            # shift the outputted size to the same ""coords"" used in the labels for the corner loss:\n            outputs_BboxNet_size = outputs_BboxNet[:, 3:6] + labels_BboxNet[:, 8:] # (shape: (batch_size*, 3))\n\n            pred_h = outputs_BboxNet_size[:, 0] # (shape: (batch_size*, ))\n            pred_w = outputs_BboxNet_size[:, 1] # (shape: (batch_size*, ))\n            pred_l = outputs_BboxNet_size[:, 2] # (shape: (batch_size*, ))\n\n            # get the pred residuals for the GT bins:\n            pred_residuals = outputs_BboxNet_heading_regr # (shape: (batch_size*, ))\n\n            Rmat = Variable(torch.zeros(pred_h.size()[0], 3, 3), requires_grad=True).cuda() # (shape: (batch_size*, 3, 3))\n            Rmat[:, 0, 0] = torch.cos(pred_residuals)\n            Rmat[:, 0, 2] = torch.sin(pred_residuals)\n            Rmat[:, 1, 1] = 1\n            Rmat[:, 2, 0] = -torch.sin(pred_residuals)\n            Rmat[:, 2, 2] = torch.cos(pred_residuals)\n\n            p0_orig = Variable(torch.zeros(pred_h.size()[0], 3, 1), requires_grad=True).cuda() # (shape: (batch_size*, 3, 1))\n            p0_orig[:, 0, 0] = pred_l/2.0\n            p0_orig[:, 2, 0] = pred_w/2.0\n\n            p1_orig = Variable(torch.zeros(pred_h.size()[0], 3, 1), requires_grad=True).cuda() # (shape: (batch_size*, 3, 1))\n            p1_orig[:, 0, 0] = -pred_l/2.0\n            p1_orig[:, 2, 0] = pred_w/2.0\n\n            p2_orig = Variable(torch.zeros(pred_h.size()[0], 3, 1), requires_grad=True).cuda() # (shape: (batch_size*, 3, 1))\n            p2_orig[:, 0, 0] = -pred_l/2.0\n            p2_orig[:, 2, 0] = -pred_w/2.0\n\n            p3_orig = Variable(torch.zeros(pred_h.size()[0], 3, 1), requires_grad=True).cuda() # (shape: (batch_size*, 3, 1))\n            p3_orig[:, 0, 0] = pred_l/2.0\n            p3_orig[:, 2, 0] = -pred_w/2.0\n\n            p4_orig = Variable(torch.zeros(pred_h.size()[0], 3, 1), requires_grad=True).cuda() # (shape: (batch_size*, 3, 1))\n            p4_orig[:, 0, 0] = pred_l/2.0\n            p4_orig[:, 1, 0] = -pred_h\n            p4_orig[:, 2, 0] = pred_w/2.0\n\n            p5_orig = Variable(torch.zeros(pred_h.size()[0], 3, 1), requires_grad=True).cuda() # (shape: (batch_size*, 3, 1))\n            p5_orig[:, 0, 0] = -pred_l/2.0\n            p5_orig[:, 1, 0] = -pred_h\n            p5_orig[:, 2, 0] = pred_w/2.0\n\n            p6_orig = Variable(torch.zeros(pred_h.size()[0], 3, 1), requires_grad=True).cuda() # (shape: (batch_size*, 3, 1))\n            p6_orig[:, 0, 0] = -pred_l/2.0\n            p6_orig[:, 1, 0] = -pred_h\n            p6_orig[:, 2, 0] = -pred_w/2.0\n\n            p7_orig = Variable(torch.zeros(pred_h.size()[0], 3, 1), requires_grad=True).cuda() # (shape: (batch_size*, 3, 1))\n            p7_orig[:, 0, 0] = pred_l/2.0\n            p7_orig[:, 1, 0] = -pred_h\n            p7_orig[:, 2, 0] = -pred_w/2.0\n\n            pred_p0_unsqeezed = pred_center_unsqeezed + torch.bmm(Rmat, p0_orig) # (shape: (batch_size*, 3, 1))\n            pred_p0 = pred_p0_unsqeezed[:, :, 0] # (shape: (batch_size*, 3))\n            pred_p1_unsqeezed = pred_center_unsqeezed + torch.bmm(Rmat, p1_orig) # (shape: (batch_size*, 3, 1))\n            pred_p1 = pred_p1_unsqeezed[:, :, 0] # (shape: (batch_size*, 3))\n            pred_p2_unsqeezed = pred_center_unsqeezed + torch.bmm(Rmat, p2_orig) # (shape: (batch_size*, 3, 1))\n            pred_p2 = pred_p2_unsqeezed[:, :, 0] # (shape: (batch_size*, 3))\n            pred_p3_unsqeezed = pred_center_unsqeezed + torch.bmm(Rmat, p3_orig) # (shape: (batch_size*, 3, 1))\n            pred_p3 = pred_p3_unsqeezed[:, :, 0] # (shape: (batch_size*, 3))\n            pred_p4_unsqeezed = pred_center_unsqeezed + torch.bmm(Rmat, p4_orig) # (shape: (batch_size*, 3, 1))\n            pred_p4 = pred_p4_unsqeezed[:, :, 0] # (shape: (batch_size*, 3))\n            pred_p5_unsqeezed = pred_center_unsqeezed + torch.bmm(Rmat, p5_orig) # (shape: (batch_size*, 3, 1))\n            pred_p5 = pred_p5_unsqeezed[:, :, 0] # (shape: (batch_size*, 3))\n            pred_p6_unsqeezed = pred_center_unsqeezed + torch.bmm(Rmat, p6_orig) # (shape: (batch_size*, 3, 1))\n            pred_p6 = pred_p6_unsqeezed[:, :, 0] # (shape: (batch_size*, 3))\n            pred_p7_unsqeezed = pred_center_unsqeezed + torch.bmm(Rmat, p7_orig) # (shape: (batch_size*, 3, 1))\n            pred_p7 = pred_p7_unsqeezed[:, :, 0] # (shape: (batch_size*, 3))\n\n            outputs_corner = Variable(torch.zeros(pred_h.size()[0], 8, 3), requires_grad=True).cuda() # (shape: (batch_size*, 8, 3))\n            outputs_corner[:, 0] = pred_p0\n            outputs_corner[:, 1] = pred_p1\n            outputs_corner[:, 2] = pred_p2\n            outputs_corner[:, 3] = pred_p3\n            outputs_corner[:, 4] = pred_p4\n            outputs_corner[:, 5] = pred_p5\n            outputs_corner[:, 6] = pred_p6\n            outputs_corner[:, 7] = pred_p7\n\n            loss_corner_unflipped = regression_loss_func(outputs_corner, labels_corner)\n            loss_corner_flipped = regression_loss_func(outputs_corner, labels_corner_flipped)\n\n            loss_corner = torch.min(loss_corner_unflipped, loss_corner_flipped)\n\n        loss_corner_value = loss_corner.data.cpu().numpy()\n        batch_losses_corner.append(loss_corner_value)\n\n        ########################################################################\n        # compute the total loss:\n        ########################################################################\n        lambda_value = 1\n        gamma_value = 10\n        loss = loss_InstanceSeg + lambda_value*(loss_TNet + loss_BboxNet + gamma_value*loss_corner)\n        loss_value = loss.data.cpu().numpy()\n        batch_losses.append(loss_value)\n\n        ########################################################################\n        # optimization step:\n        ########################################################################\n        optimizer.zero_grad() # (reset gradients)\n        loss.backward() # (compute gradients)\n        optimizer.step() # (perform optimization step)\n\n    # compute the train epoch loss:\n    epoch_loss = np.mean(batch_losses)\n    # save the train epoch loss:\n    epoch_losses_train.append(epoch_loss)\n    # save the train epoch loss to disk:\n    with open(""%s/epoch_losses_train.pkl"" % network.model_dir, ""wb"") as file:\n        pickle.dump(epoch_losses_train, file)\n    print (""training loss: %g"" % epoch_loss)\n    # plot the training loss vs epoch and save to disk:\n    plt.figure(1)\n    plt.plot(epoch_losses_train, ""k^"")\n    plt.plot(epoch_losses_train, ""k"")\n    plt.ylabel(""loss"")\n    plt.xlabel(""epoch"")\n    plt.title(""training loss per epoch"")\n    plt.savefig(""%s/epoch_losses_train.png"" % network.model_dir)\n    plt.close(1)\n\n    # compute the train epoch TNet loss:\n    epoch_loss = np.mean(batch_losses_TNet)\n    # save the train epoch loss:\n    epoch_losses_TNet_train.append(epoch_loss)\n    # save the train epoch TNet loss to disk:\n    with open(""%s/epoch_losses_TNet_train.pkl"" % network.model_dir, ""wb"") as file:\n        pickle.dump(epoch_losses_TNet_train, file)\n    print (""training TNet loss: %g"" % epoch_loss)\n    # plot the training TNet loss vs epoch and save to disk:\n    plt.figure(1)\n    plt.plot(epoch_losses_TNet_train, ""k^"")\n    plt.plot(epoch_losses_TNet_train, ""k"")\n    plt.ylabel(""loss"")\n    plt.xlabel(""epoch"")\n    plt.title(""training TNet loss per epoch"")\n    plt.savefig(""%s/epoch_losses_TNet_train.png"" % network.model_dir)\n    plt.close(1)\n\n    # compute the train epoch InstanceSeg loss:\n    epoch_loss = np.mean(batch_losses_InstanceSeg)\n    # save the train epoch loss:\n    epoch_losses_InstanceSeg_train.append(epoch_loss)\n    # save the train epoch InstanceSeg loss to disk:\n    with open(""%s/epoch_losses_InstanceSeg_train.pkl"" % network.model_dir, ""wb"") as file:\n        pickle.dump(epoch_losses_InstanceSeg_train, file)\n    print (""training InstanceSeg loss: %g"" % epoch_loss)\n    # plot the training InstanceSeg loss vs epoch and save to disk:\n    plt.figure(1)\n    plt.plot(epoch_losses_InstanceSeg_train, ""k^"")\n    plt.plot(epoch_losses_InstanceSeg_train, ""k"")\n    plt.ylabel(""loss"")\n    plt.xlabel(""epoch"")\n    plt.title(""training InstanceSeg loss per epoch"")\n    plt.savefig(""%s/epoch_losses_InstanceSeg_train.png"" % network.model_dir)\n    plt.close(1)\n\n    # compute the train epoch BboxNet loss:\n    epoch_loss = np.mean(batch_losses_BboxNet)\n    # save the train epoch loss:\n    epoch_losses_BboxNet_train.append(epoch_loss)\n    # save the train epoch BboxNet loss to disk:\n    with open(""%s/epoch_losses_BboxNet_train.pkl"" % network.model_dir, ""wb"") as file:\n        pickle.dump(epoch_losses_BboxNet_train, file)\n    print (""training BboxNet loss: %g"" % epoch_loss)\n    # plot the training BboxNet loss vs epoch and save to disk:\n    plt.figure(1)\n    plt.plot(epoch_losses_BboxNet_train, ""k^"")\n    plt.plot(epoch_losses_BboxNet_train, ""k"")\n    plt.ylabel(""loss"")\n    plt.xlabel(""epoch"")\n    plt.title(""training BboxNet loss per epoch"")\n    plt.savefig(""%s/epoch_losses_BboxNet_train.png"" % network.model_dir)\n    plt.close(1)\n\n    # compute the train epoch BboxNet size loss:\n    epoch_loss = np.mean(batch_losses_BboxNet_size)\n    # save the train epoch loss:\n    epoch_losses_BboxNet_size_train.append(epoch_loss)\n    # save the train epoch BboxNet size loss to disk:\n    with open(""%s/epoch_losses_BboxNet_size_train.pkl"" % network.model_dir, ""wb"") as file:\n        pickle.dump(epoch_losses_BboxNet_size_train, file)\n    print (""training BboxNet size loss: %g"" % epoch_loss)\n    # plot the training BboxNet size loss vs epoch and save to disk:\n    plt.figure(1)\n    plt.plot(epoch_losses_BboxNet_size_train, ""k^"")\n    plt.plot(epoch_losses_BboxNet_size_train, ""k"")\n    plt.ylabel(""loss"")\n    plt.xlabel(""epoch"")\n    plt.title(""training BboxNet size loss per epoch"")\n    plt.savefig(""%s/epoch_losses_BboxNet_size_train.png"" % network.model_dir)\n    plt.close(1)\n\n    # compute the train epoch BboxNet center loss:\n    epoch_loss = np.mean(batch_losses_BboxNet_center)\n    # save the train epoch loss:\n    epoch_losses_BboxNet_center_train.append(epoch_loss)\n    # save the train epoch BboxNet center loss to disk:\n    with open(""%s/epoch_losses_BboxNet_center_train.pkl"" % network.model_dir, ""wb"") as file:\n        pickle.dump(epoch_losses_BboxNet_center_train, file)\n    print (""training BboxNet center loss: %g"" % epoch_loss)\n    # plot the training BboxNet center loss vs epoch and save to disk:\n    plt.figure(1)\n    plt.plot(epoch_losses_BboxNet_center_train, ""k^"")\n    plt.plot(epoch_losses_BboxNet_center_train, ""k"")\n    plt.ylabel(""loss"")\n    plt.xlabel(""epoch"")\n    plt.title(""training BboxNet center loss per epoch"")\n    plt.savefig(""%s/epoch_losses_BboxNet_center_train.png"" % network.model_dir)\n    plt.close(1)\n\n    # compute the train epoch BboxNet heading class loss:\n    epoch_loss = np.mean(batch_losses_BboxNet_heading_class)\n    # save the train epoch loss:\n    epoch_losses_BboxNet_heading_class_train.append(epoch_loss)\n    # save the train epoch BboxNet heading class loss to disk:\n    with open(""%s/epoch_losses_BboxNet_heading_class_train.pkl"" % network.model_dir, ""wb"") as file:\n        pickle.dump(epoch_losses_BboxNet_heading_class_train, file)\n    print (""training BboxNet heading class loss: %g"" % epoch_loss)\n    # plot the training BboxNet heading class loss vs epoch and save to disk:\n    plt.figure(1)\n    plt.plot(epoch_losses_BboxNet_heading_class_train, ""k^"")\n    plt.plot(epoch_losses_BboxNet_heading_class_train, ""k"")\n    plt.ylabel(""loss"")\n    plt.xlabel(""epoch"")\n    plt.title(""training BboxNet heading class loss per epoch"")\n    plt.savefig(""%s/epoch_losses_BboxNet_heading_class_train.png"" % network.model_dir)\n    plt.close(1)\n\n    # compute the train epoch BboxNet heading regr loss:\n    epoch_loss = np.mean(batch_losses_BboxNet_heading_regr)\n    # save the train epoch loss:\n    epoch_losses_BboxNet_heading_regr_train.append(epoch_loss)\n    # save the train epoch BboxNet heading regr loss to disk:\n    with open(""%s/epoch_losses_BboxNet_heading_regr_train.pkl"" % network.model_dir, ""wb"") as file:\n        pickle.dump(epoch_losses_BboxNet_heading_regr_train, file)\n    print (""training BboxNet heading regr loss: %g"" % epoch_loss)\n    # plot the training BboxNet heading regr loss vs epoch and save to disk:\n    plt.figure(1)\n    plt.plot(epoch_losses_BboxNet_heading_regr_train, ""k^"")\n    plt.plot(epoch_losses_BboxNet_heading_regr_train, ""k"")\n    plt.ylabel(""loss"")\n    plt.xlabel(""epoch"")\n    plt.title(""training BboxNet heading regr loss per epoch"")\n    plt.savefig(""%s/epoch_losses_BboxNet_heading_regr_train.png"" % network.model_dir)\n    plt.close(1)\n\n    # compute the train epoch heading class accuracy:\n    epoch_accuracy = np.mean(batch_accuracies_heading_class)\n    # save the train epoch heading class accuracy:\n    epoch_accuracies_heading_class_train.append(epoch_accuracy)\n    # save the train epoch heading class accuracy to disk:\n    with open(""%s/epoch_accuracies_heading_class_train.pkl"" % network.model_dir, ""wb"") as file:\n        pickle.dump(epoch_accuracies_heading_class_train, file)\n    print (""training heading class accuracy: %g"" % epoch_accuracy)\n    # plot the training heading class accuracy vs epoch and save to disk:\n    plt.figure(1)\n    plt.plot(epoch_accuracies_heading_class_train, ""k^"")\n    plt.plot(epoch_accuracies_heading_class_train, ""k"")\n    plt.ylabel(""accuracy"")\n    plt.xlabel(""epoch"")\n    plt.title(""training heading class accuracy per epoch"")\n    plt.savefig(""%s/epoch_accuracies_heading_class_train.png"" % network.model_dir)\n    plt.close(1)\n\n    # compute the train epoch corner loss:\n    epoch_loss = np.mean(batch_losses_corner)\n    # save the train epoch corner loss:\n    epoch_losses_corner_train.append(epoch_loss)\n    # save the train epoch corner loss to disk:\n    with open(""%s/epoch_losses_corner_train.pkl"" % network.model_dir, ""wb"") as file:\n        pickle.dump(epoch_losses_corner_train, file)\n    print (""training corner loss: %g"" % epoch_loss)\n    # plot the training corner loss vs epoch and save to disk:\n    plt.figure(1)\n    plt.plot(epoch_losses_corner_train, ""k^"")\n    plt.plot(epoch_losses_corner_train, ""k"")\n    plt.ylabel(""loss"")\n    plt.xlabel(""epoch"")\n    plt.title(""training corner loss per epoch"")\n    plt.savefig(""%s/epoch_losses_corner_train.png"" % network.model_dir)\n    plt.close(1)\n\n    # compute the train epoch accuracy:\n    epoch_accuracy = np.mean(batch_accuracies)\n    # save the train epoch accuracy:\n    epoch_accuracies_train.append(epoch_accuracy)\n    # save the train epoch accuracy to disk:\n    with open(""%s/epoch_accuracies_train.pkl"" % network.model_dir, ""wb"") as file:\n        pickle.dump(epoch_accuracies_train, file)\n    print (""training accuracy: %g"" % epoch_accuracy)\n    # plot the training accuracy vs epoch and save to disk:\n    plt.figure(1)\n    plt.plot(epoch_accuracies_train, ""k^"")\n    plt.plot(epoch_accuracies_train, ""k"")\n    plt.ylabel(""accuracy"")\n    plt.xlabel(""epoch"")\n    plt.title(""training accuracy per epoch"")\n    plt.savefig(""%s/epoch_accuracies_train.png"" % network.model_dir)\n    plt.close(1)\n\n    # compute the train epoch precision:\n    epoch_precision = np.mean(batch_precisions)\n    # save the train epoch precision:\n    epoch_precisions_train.append(epoch_precision)\n    # save the train epoch precision to disk:\n    with open(""%s/epoch_precisions_train.pkl"" % network.model_dir, ""wb"") as file:\n        pickle.dump(epoch_precisions_train, file)\n    print (""training precision: %g"" % epoch_precision)\n    # plot the training precision vs epoch and save to disk:\n    plt.figure(1)\n    plt.plot(epoch_precisions_train, ""k^"")\n    plt.plot(epoch_precisions_train, ""k"")\n    plt.ylabel(""precision"")\n    plt.xlabel(""epoch"")\n    plt.title(""training precision per epoch"")\n    plt.savefig(""%s/epoch_precisions_train.png"" % network.model_dir)\n    plt.close(1)\n\n    # compute the train epoch recall:\n    epoch_recall = np.mean(batch_recalls)\n    # save the train epoch recall:\n    epoch_recalls_train.append(epoch_recall)\n    # save the train epoch recall to disk:\n    with open(""%s/epoch_recalls_train.pkl"" % network.model_dir, ""wb"") as file:\n        pickle.dump(epoch_recalls_train, file)\n    print (""training recall: %g"" % epoch_recall)\n    # plot the training recall vs epoch and save to disk:\n    plt.figure(1)\n    plt.plot(epoch_recalls_train, ""k^"")\n    plt.plot(epoch_recalls_train, ""k"")\n    plt.ylabel(""recall"")\n    plt.xlabel(""epoch"")\n    plt.title(""training recall per epoch"")\n    plt.savefig(""%s/epoch_recalls_train.png"" % network.model_dir)\n    plt.close(1)\n\n    # compute the train epoch f1:\n    epoch_f1 = np.mean(batch_f1s)\n    # save the train epoch f1:\n    epoch_f1s_train.append(epoch_f1)\n    # save the train epoch f1 to disk:\n    with open(""%s/epoch_f1s_train.pkl"" % network.model_dir, ""wb"") as file:\n        pickle.dump(epoch_f1s_train, file)\n    print (""training f1: %g"" % epoch_f1)\n    # plot the training f1 vs epoch and save to disk:\n    plt.figure(1)\n    plt.plot(epoch_f1s_train, ""k^"")\n    plt.plot(epoch_f1s_train, ""k"")\n    plt.ylabel(""f1"")\n    plt.xlabel(""epoch"")\n    plt.title(""training f1 per epoch"")\n    plt.savefig(""%s/epoch_f1s_train.png"" % network.model_dir)\n    plt.close(1)\n\n    print (""####"")\n\n    ############################################################################\n    # val:\n    ############################################################################\n    network.eval() # (set in evaluation mode, this affects BatchNorm and dropout)\n    batch_losses = []\n    batch_losses_InstanceSeg = []\n    batch_losses_TNet = []\n    batch_losses_BboxNet = []\n    batch_losses_BboxNet_center = []\n    batch_losses_BboxNet_size = []\n    batch_losses_BboxNet_heading_regr = []\n    batch_losses_BboxNet_heading_class = []\n    batch_losses_corner = []\n    batch_accuracies = []\n    batch_precisions = []\n    batch_recalls = []\n    batch_f1s = []\n    batch_accuracies_heading_class = []\n    for step, (frustum_point_clouds, labels_InstanceSeg, labels_TNet, labels_BboxNet, labels_corner, labels_corner_flipped, img_ids, input_2Dbboxes, frustum_Rs, frustum_angles, centered_frustum_mean_xyz) in enumerate(val_loader):\n        with torch.no_grad(): # (corresponds to setting volatile=True in all variables, this is done during inference to reduce memory consumption)\n            frustum_point_clouds = Variable(frustum_point_clouds) # (shape: (batch_size, num_points, 4))\n            labels_InstanceSeg = Variable(labels_InstanceSeg) # (shape: (batch_size, num_points))\n            labels_TNet = Variable(labels_TNet) # (shape: (batch_size, 3))\n            labels_BboxNet = Variable(labels_BboxNet) # (shape:(batch_size, 11))\n            labels_corner = Variable(labels_corner) # (shape: (batch_size, 8, 3))\n            labels_corner_flipped = Variable(labels_corner_flipped) # (shape: (batch_size, 8, 3))\n\n            frustum_point_clouds = frustum_point_clouds.transpose(2, 1) # (shape: (batch_size, 4, num_points))\n\n            frustum_point_clouds = frustum_point_clouds.cuda()\n            labels_InstanceSeg = labels_InstanceSeg.cuda()\n            labels_TNet = labels_TNet.cuda()\n            labels_BboxNet = labels_BboxNet.cuda()\n            labels_corner = labels_corner.cuda()\n            labels_corner_flipped = labels_corner_flipped.cuda()\n\n            outputs = network(frustum_point_clouds)\n            outputs_InstanceSeg = outputs[0] # (shape: (batch_size, num_points, 2))\n            outputs_TNet = outputs[1] # (shape: (batch_size, 3))\n            outputs_BboxNet = outputs[2] # (shape: (batch_size, 3 + 3 + 2*NH))\n            seg_point_clouds_mean = outputs[3] # (shape: (batch_size, 3))\n            dont_care_mask = outputs[4] # (shape: (batch_size, ))\n\n            ########################################################################\n            # compute precision, recall etc. for the InstanceSeg:\n            ########################################################################\n            preds = outputs_InstanceSeg.data.cpu().numpy() # (shape: (batch_size, num_points, 2))\n            preds = np.argmax(preds, 2) # (shape: (batch_size, num_points))\n\n            labels_InstanceSeg_np = labels_InstanceSeg.data.cpu().numpy() # (shape: (batch_size, num_points))\n\n            accuracy = np.count_nonzero(preds == labels_InstanceSeg_np)/(preds.shape[0]*preds.shape[1])\n            if np.count_nonzero(preds == 1) > 0:\n                precision = np.count_nonzero(np.logical_and(preds == labels_InstanceSeg_np, preds == 1))/np.count_nonzero(preds == 1) # (TP/(TP + FP))\n            else:\n                precision = -1\n            if np.count_nonzero(labels_InstanceSeg_np == 1) > 0:\n                recall = np.count_nonzero(np.logical_and(preds == labels_InstanceSeg_np, preds == 1))/np.count_nonzero(labels_InstanceSeg_np == 1) # (TP/(TP + FN))\n            else:\n                recall = -1\n            if recall + precision > 0:\n                f1 = 2*recall*precision/(recall + precision)\n            else:\n                f1 = -1\n\n            batch_accuracies.append(accuracy)\n            if precision != -1:\n                batch_precisions.append(precision)\n            if recall != -1:\n                batch_recalls.append(recall)\n            if f1 != -1:\n                batch_f1s.append(f1)\n\n            ########################################################################\n            # compute accuracy for the heading classification:\n            ########################################################################\n            pred_bin_scores = outputs_BboxNet[:, 6:(6+NH)].data.cpu().numpy() # (shape: (batch_size, NH))\n            pred_bin_numbers = np.argmax(pred_bin_scores, 1) # (shape: (batch_size, ))\n            gt_bin_numbers = labels_BboxNet[:, 6].data.cpu().numpy() # (shape: (batch_size, ))\n\n            accuracy_heading_class = np.count_nonzero(pred_bin_numbers == gt_bin_numbers)/(pred_bin_numbers.shape[0])\n            batch_accuracies_heading_class.append(accuracy_heading_class)\n\n            ########################################################################\n            # compute the InstanceSeg loss:\n            ########################################################################\n            outputs_InstanceSeg = outputs_InstanceSeg.view(-1, 2) # (shape (batch_size*num_points, 2))\n            labels_InstanceSeg = labels_InstanceSeg.view(-1, 1) # (shape: (batch_size*num_points, 1))\n            labels_InstanceSeg = labels_InstanceSeg[:, 0] # (shape: (batch_size*num_points, ))\n            loss_InstanceSeg = F.nll_loss(outputs_InstanceSeg, labels_InstanceSeg)\n            loss_InstanceSeg_value = loss_InstanceSeg.data.cpu().numpy()\n            batch_losses_InstanceSeg.append(loss_InstanceSeg_value)\n\n            ########################################################################\n            # compute the TNet loss:\n            ########################################################################\n            # mask entries corresponding to empty seg point clouds (select only the entries which we care about):\n            outputs_TNet = outputs_TNet[dont_care_mask, :] # (shape: (batch_size*, 3))\n            labels_TNet = labels_TNet[dont_care_mask, :] # (shape: (batch_size*, 3))\n            seg_point_clouds_mean = seg_point_clouds_mean[dont_care_mask, :] # (shape: (batch_size*, 3))\n\n            # shift the GT to the seg point clouds local coords:\n            labels_TNet = labels_TNet - seg_point_clouds_mean\n\n            # compute the Huber (smooth L1) loss:\n            if outputs_TNet.size()[0] == 0:\n                loss_TNet = Variable(torch.from_numpy(np.zeros((1, ), dtype=np.float32))).cuda()\n            else:\n                loss_TNet = regression_loss_func(outputs_TNet, labels_TNet)\n\n            loss_TNet_value = loss_TNet.data.cpu().numpy()\n            batch_losses_TNet.append(loss_TNet_value)\n\n            ########################################################################\n            # compute the BboxNet loss:\n            ########################################################################\n            # mask entries corresponding to empty seg point clouds (select only the entries which we care about):\n            outputs_BboxNet = outputs_BboxNet[dont_care_mask, :] # (shape: (batch_size*, 3 + 3 + 2*NH))\n            labels_BboxNet = labels_BboxNet[dont_care_mask, :]# (shape: (batch_size*, 11))\n\n            if outputs_BboxNet.size()[0] == 0:\n                loss_BboxNet = Variable(torch.from_numpy(np.zeros((1, ), dtype=np.float32))).cuda()\n                loss_BboxNet_size = Variable(torch.from_numpy(np.zeros((1, ), dtype=np.float32))).cuda()\n                loss_BboxNet_center = Variable(torch.from_numpy(np.zeros((1, ), dtype=np.float32))).cuda()\n                loss_BboxNet_heading_class = Variable(torch.from_numpy(np.zeros((1, ), dtype=np.float32))).cuda()\n                loss_BboxNet_heading_regr = Variable(torch.from_numpy(np.zeros((1, ), dtype=np.float32))).cuda()\n                ####\n            else:\n                # compute the BboxNet center loss:\n                labels_BboxNet_center = labels_BboxNet[:, 0:3] # (shape: (batch_size*, 3))\n                # # shift the center GT to local coords:\n                labels_BboxNet_center = Variable(labels_BboxNet_center.data - seg_point_clouds_mean.data - outputs_TNet.data).cuda() # (outputs_TNet is a variable outputted by model, so it requires grads, which cant be passed as target to the loss function)\n                # # compute the Huber (smooth L1) loss:\n                outputs_BboxNet_center = outputs_BboxNet[:, 0:3]\n                loss_BboxNet_center = regression_loss_func(outputs_BboxNet_center, labels_BboxNet_center)\n\n                # compute the BboxNet size loss:\n                labels_BboxNet_size = labels_BboxNet[:, 3:6] # (shape: (batch_size*, 3))\n                # # subtract the mean car size in train:\n                labels_BboxNet_size = labels_BboxNet_size - labels_BboxNet[:, 8:]\n                # # compute the Huber (smooth L1) loss:\n                loss_BboxNet_size = regression_loss_func(outputs_BboxNet[:, 3:6], labels_BboxNet_size)\n\n                # compute the BboxNet heading loss\n                # # compute the classification loss:\n                labels_BboxNet_heading_class = Variable(labels_BboxNet[:, 6].data.type(torch.LongTensor)).cuda() # (shape: (batch_size*, ))\n                outputs_BboxNet_heading_class = outputs_BboxNet[:, 6:(6+NH)] # (shape: (batch_size*, NH))\n                loss_BboxNet_heading_class = F.nll_loss(F.log_softmax(outputs_BboxNet_heading_class, dim=1), labels_BboxNet_heading_class)\n                # # compute the regression loss:\n                # # # # get the GT residual for the GT bin:\n                labels_BboxNet_heading_regr = labels_BboxNet[:, 7] # (shape: (batch_size*, ))\n                # # # # get the pred residual for all bins:\n                outputs_BboxNet_heading_regr_all = outputs_BboxNet[:, (6+NH):] # (shape: (batch_size*, 8))\n                # # # # get the pred residual for the GT bin:\n                outputs_BboxNet_heading_regr = outputs_BboxNet_heading_regr_all.gather(1, labels_BboxNet_heading_class.view(-1, 1)) # (shape: (batch_size*, 1))\n                outputs_BboxNet_heading_regr = outputs_BboxNet_heading_regr[:, 0] # (shape: (batch_size*, )\n                # # # # compute the loss:\n                loss_BboxNet_heading_regr = regression_loss_func(outputs_BboxNet_heading_regr, labels_BboxNet_heading_regr)\n                # # compute the total BBoxNet heading loss:\n                loss_BboxNet_heading = loss_BboxNet_heading_class + 10*loss_BboxNet_heading_regr\n\n                # compute the BboxNet total loss:\n                loss_BboxNet = loss_BboxNet_center + loss_BboxNet_size + loss_BboxNet_heading\n\n            loss_BboxNet_value = loss_BboxNet.data.cpu().numpy()\n            batch_losses_BboxNet.append(loss_BboxNet_value)\n\n            loss_BboxNet_size_value = loss_BboxNet_size.data.cpu().numpy()\n            batch_losses_BboxNet_size.append(loss_BboxNet_size_value)\n\n            loss_BboxNet_center_value = loss_BboxNet_center.data.cpu().numpy()\n            batch_losses_BboxNet_center.append(loss_BboxNet_center_value)\n\n            loss_BboxNet_heading_class_value = loss_BboxNet_heading_class.data.cpu().numpy()\n            batch_losses_BboxNet_heading_class.append(loss_BboxNet_heading_class_value)\n\n            loss_BboxNet_heading_regr_value = loss_BboxNet_heading_regr.data.cpu().numpy()\n            batch_losses_BboxNet_heading_regr.append(loss_BboxNet_heading_regr_value)\n\n            ########################################################################\n            # compute the corner loss:\n            ########################################################################\n            # mask entries corresponding to empty seg point clouds (select only the entries which we care about):\n            labels_corner = labels_corner[dont_care_mask]# (shape: (batch_size*, 8, 3))\n            labels_corner_flipped = labels_corner_flipped[dont_care_mask]# (shape: (batch_size*, 8, 3))\n\n            if outputs_BboxNet.size()[0] == 0:\n                loss_corner = Variable(torch.from_numpy(np.zeros((1, ), dtype=np.float32))).cuda()\n            else:\n                outputs_BboxNet_center = outputs_BboxNet[:, 0:3] # (shape: (batch_size*, 3))\n                # shift to the same coords used in the labels for the corner loss:\n                pred_center = outputs_BboxNet_center + seg_point_clouds_mean + outputs_TNet # (shape: (batch_size*, 3))\n                pred_center_unsqeezed = pred_center.unsqueeze(2) # (shape: (batch_size, 3, 1))\n\n                # shift the outputted size to the same ""coords"" used in the labels for the corner loss:\n                outputs_BboxNet_size = outputs_BboxNet[:, 3:6] + labels_BboxNet[:, 8:] # (shape: (batch_size*, 3))\n\n                pred_h = outputs_BboxNet_size[:, 0] # (shape: (batch_size*, ))\n                pred_w = outputs_BboxNet_size[:, 1] # (shape: (batch_size*, ))\n                pred_l = outputs_BboxNet_size[:, 2] # (shape: (batch_size*, ))\n\n                # get the pred residuals for the GT bins:\n                pred_residuals = outputs_BboxNet_heading_regr # (shape: (batch_size*, ))\n\n                Rmat = Variable(torch.zeros(pred_h.size()[0], 3, 3), requires_grad=True).cuda() # (shape: (batch_size*, 3, 3))\n                Rmat[:, 0, 0] = torch.cos(pred_residuals)\n                Rmat[:, 0, 2] = torch.sin(pred_residuals)\n                Rmat[:, 1, 1] = 1\n                Rmat[:, 2, 0] = -torch.sin(pred_residuals)\n                Rmat[:, 2, 2] = torch.cos(pred_residuals)\n\n                p0_orig = Variable(torch.zeros(pred_h.size()[0], 3, 1), requires_grad=True).cuda() # (shape: (batch_size*, 3, 1))\n                p0_orig[:, 0, 0] = pred_l/2.0\n                p0_orig[:, 2, 0] = pred_w/2.0\n\n                p1_orig = Variable(torch.zeros(pred_h.size()[0], 3, 1), requires_grad=True).cuda() # (shape: (batch_size*, 3, 1))\n                p1_orig[:, 0, 0] = -pred_l/2.0\n                p1_orig[:, 2, 0] = pred_w/2.0\n\n                p2_orig = Variable(torch.zeros(pred_h.size()[0], 3, 1), requires_grad=True).cuda() # (shape: (batch_size*, 3, 1))\n                p2_orig[:, 0, 0] = -pred_l/2.0\n                p2_orig[:, 2, 0] = -pred_w/2.0\n\n                p3_orig = Variable(torch.zeros(pred_h.size()[0], 3, 1), requires_grad=True).cuda() # (shape: (batch_size*, 3, 1))\n                p3_orig[:, 0, 0] = pred_l/2.0\n                p3_orig[:, 2, 0] = -pred_w/2.0\n\n                p4_orig = Variable(torch.zeros(pred_h.size()[0], 3, 1), requires_grad=True).cuda() # (shape: (batch_size*, 3, 1))\n                p4_orig[:, 0, 0] = pred_l/2.0\n                p4_orig[:, 1, 0] = -pred_h\n                p4_orig[:, 2, 0] = pred_w/2.0\n\n                p5_orig = Variable(torch.zeros(pred_h.size()[0], 3, 1), requires_grad=True).cuda() # (shape: (batch_size*, 3, 1))\n                p5_orig[:, 0, 0] = -pred_l/2.0\n                p5_orig[:, 1, 0] = -pred_h\n                p5_orig[:, 2, 0] = pred_w/2.0\n\n                p6_orig = Variable(torch.zeros(pred_h.size()[0], 3, 1), requires_grad=True).cuda() # (shape: (batch_size*, 3, 1))\n                p6_orig[:, 0, 0] = -pred_l/2.0\n                p6_orig[:, 1, 0] = -pred_h\n                p6_orig[:, 2, 0] = -pred_w/2.0\n\n                p7_orig = Variable(torch.zeros(pred_h.size()[0], 3, 1), requires_grad=True).cuda() # (shape: (batch_size*, 3, 1))\n                p7_orig[:, 0, 0] = pred_l/2.0\n                p7_orig[:, 1, 0] = -pred_h\n                p7_orig[:, 2, 0] = -pred_w/2.0\n\n                pred_p0_unsqeezed = pred_center_unsqeezed + torch.bmm(Rmat, p0_orig) # (shape: (batch_size*, 3, 1))\n                pred_p0 = pred_p0_unsqeezed[:, :, 0] # (shape: (batch_size*, 3))\n                pred_p1_unsqeezed = pred_center_unsqeezed + torch.bmm(Rmat, p1_orig) # (shape: (batch_size*, 3, 1))\n                pred_p1 = pred_p1_unsqeezed[:, :, 0] # (shape: (batch_size*, 3))\n                pred_p2_unsqeezed = pred_center_unsqeezed + torch.bmm(Rmat, p2_orig) # (shape: (batch_size*, 3, 1))\n                pred_p2 = pred_p2_unsqeezed[:, :, 0] # (shape: (batch_size*, 3))\n                pred_p3_unsqeezed = pred_center_unsqeezed + torch.bmm(Rmat, p3_orig) # (shape: (batch_size*, 3, 1))\n                pred_p3 = pred_p3_unsqeezed[:, :, 0] # (shape: (batch_size*, 3))\n                pred_p4_unsqeezed = pred_center_unsqeezed + torch.bmm(Rmat, p4_orig) # (shape: (batch_size*, 3, 1))\n                pred_p4 = pred_p4_unsqeezed[:, :, 0] # (shape: (batch_size*, 3))\n                pred_p5_unsqeezed = pred_center_unsqeezed + torch.bmm(Rmat, p5_orig) # (shape: (batch_size*, 3, 1))\n                pred_p5 = pred_p5_unsqeezed[:, :, 0] # (shape: (batch_size*, 3))\n                pred_p6_unsqeezed = pred_center_unsqeezed + torch.bmm(Rmat, p6_orig) # (shape: (batch_size*, 3, 1))\n                pred_p6 = pred_p6_unsqeezed[:, :, 0] # (shape: (batch_size*, 3))\n                pred_p7_unsqeezed = pred_center_unsqeezed + torch.bmm(Rmat, p7_orig) # (shape: (batch_size*, 3, 1))\n                pred_p7 = pred_p7_unsqeezed[:, :, 0] # (shape: (batch_size*, 3))\n\n                outputs_corner = Variable(torch.zeros(pred_h.size()[0], 8, 3), requires_grad=True).cuda() # (shape: (batch_size*, 8, 3))\n                outputs_corner[:, 0] = pred_p0\n                outputs_corner[:, 1] = pred_p1\n                outputs_corner[:, 2] = pred_p2\n                outputs_corner[:, 3] = pred_p3\n                outputs_corner[:, 4] = pred_p4\n                outputs_corner[:, 5] = pred_p5\n                outputs_corner[:, 6] = pred_p6\n                outputs_corner[:, 7] = pred_p7\n\n                loss_corner_unflipped = regression_loss_func(outputs_corner, labels_corner)\n                loss_corner_flipped = regression_loss_func(outputs_corner, labels_corner_flipped)\n\n                loss_corner = torch.min(loss_corner_unflipped, loss_corner_flipped)\n\n            loss_corner_value = loss_corner.data.cpu().numpy()\n            batch_losses_corner.append(loss_corner_value)\n\n            ########################################################################\n            # compute the total loss:\n            ########################################################################\n            lambda_value = 1\n            gamma_value = 10\n            loss = loss_InstanceSeg + lambda_value*(loss_TNet + loss_BboxNet + gamma_value*loss_corner)\n            loss_value = loss.data.cpu().numpy()\n            batch_losses.append(loss_value)\n\n    # compute the val epoch loss:\n    epoch_loss = np.mean(batch_losses)\n    # save the val epoch loss:\n    epoch_losses_val.append(epoch_loss)\n    # save the val epoch loss to disk:\n    with open(""%s/epoch_losses_val.pkl"" % network.model_dir, ""wb"") as file:\n        pickle.dump(epoch_losses_val, file)\n    print (""validation loss: %g"" % epoch_loss)\n    # plot the val loss vs epoch and save to disk:\n    plt.figure(1)\n    plt.plot(epoch_losses_val, ""k^"")\n    plt.plot(epoch_losses_val, ""k"")\n    plt.ylabel(""loss"")\n    plt.xlabel(""epoch"")\n    plt.title(""validation loss per epoch"")\n    plt.savefig(""%s/epoch_losses_val.png"" % network.model_dir)\n    plt.close(1)\n\n    # compute the val epoch TNet loss:\n    epoch_loss = np.mean(batch_losses_TNet)\n    # save the val epoch loss:\n    epoch_losses_TNet_val.append(epoch_loss)\n    # save the val epoch TNet loss to disk:\n    with open(""%s/epoch_losses_TNet_val.pkl"" % network.model_dir, ""wb"") as file:\n        pickle.dump(epoch_losses_TNet_val, file)\n    print (""validation TNet loss: %g"" % epoch_loss)\n    # plot the validation TNet loss vs epoch and save to disk:\n    plt.figure(1)\n    plt.plot(epoch_losses_TNet_val, ""k^"")\n    plt.plot(epoch_losses_TNet_val, ""k"")\n    plt.ylabel(""loss"")\n    plt.xlabel(""epoch"")\n    plt.title(""validation TNet loss per epoch"")\n    plt.savefig(""%s/epoch_losses_TNet_val.png"" % network.model_dir)\n    plt.close(1)\n\n    # compute the val epoch InstanceSeg loss:\n    epoch_loss = np.mean(batch_losses_InstanceSeg)\n    # save the val epoch loss:\n    epoch_losses_InstanceSeg_val.append(epoch_loss)\n    # save the val epoch InstanceSeg loss to disk:\n    with open(""%s/epoch_losses_InstanceSeg_val.pkl"" % network.model_dir, ""wb"") as file:\n        pickle.dump(epoch_losses_InstanceSeg_val, file)\n    print (""validation InstanceSeg loss: %g"" % epoch_loss)\n    # plot the validation InstanceSeg loss vs epoch and save to disk:\n    plt.figure(1)\n    plt.plot(epoch_losses_InstanceSeg_val, ""k^"")\n    plt.plot(epoch_losses_InstanceSeg_val, ""k"")\n    plt.ylabel(""loss"")\n    plt.xlabel(""epoch"")\n    plt.title(""validation InstanceSeg loss per epoch"")\n    plt.savefig(""%s/epoch_losses_InstanceSeg_val.png"" % network.model_dir)\n    plt.close(1)\n\n    # compute the val epoch BboxNet loss:\n    epoch_loss = np.mean(batch_losses_BboxNet)\n    # save the val epoch loss:\n    epoch_losses_BboxNet_val.append(epoch_loss)\n    # save the val epoch BboxNet loss to disk:\n    with open(""%s/epoch_losses_BboxNet_val.pkl"" % network.model_dir, ""wb"") as file:\n        pickle.dump(epoch_losses_BboxNet_val, file)\n    print (""validation BboxNet loss: %g"" % epoch_loss)\n    # plot the validation BboxNet loss vs epoch and save to disk:\n    plt.figure(1)\n    plt.plot(epoch_losses_BboxNet_val, ""k^"")\n    plt.plot(epoch_losses_BboxNet_val, ""k"")\n    plt.ylabel(""loss"")\n    plt.xlabel(""epoch"")\n    plt.title(""validation BboxNet loss per epoch"")\n    plt.savefig(""%s/epoch_losses_BboxNet_val.png"" % network.model_dir)\n    plt.close(1)\n\n    # compute the val epoch BboxNet size loss:\n    epoch_loss = np.mean(batch_losses_BboxNet_size)\n    # save the val epoch loss:\n    epoch_losses_BboxNet_size_val.append(epoch_loss)\n    # save the val epoch BboxNet size loss to disk:\n    with open(""%s/epoch_losses_BboxNet_size_val.pkl"" % network.model_dir, ""wb"") as file:\n        pickle.dump(epoch_losses_BboxNet_size_val, file)\n    print (""validation BboxNet size loss: %g"" % epoch_loss)\n    # plot the validation BboxNet size loss vs epoch and save to disk:\n    plt.figure(1)\n    plt.plot(epoch_losses_BboxNet_size_val, ""k^"")\n    plt.plot(epoch_losses_BboxNet_size_val, ""k"")\n    plt.ylabel(""loss"")\n    plt.xlabel(""epoch"")\n    plt.title(""validation BboxNet size loss per epoch"")\n    plt.savefig(""%s/epoch_losses_BboxNet_size_val.png"" % network.model_dir)\n    plt.close(1)\n\n    # compute the val epoch BboxNet center loss:\n    epoch_loss = np.mean(batch_losses_BboxNet_center)\n    # save the val epoch loss:\n    epoch_losses_BboxNet_center_val.append(epoch_loss)\n    # save the val epoch BboxNet center loss to disk:\n    with open(""%s/epoch_losses_BboxNet_center_val.pkl"" % network.model_dir, ""wb"") as file:\n        pickle.dump(epoch_losses_BboxNet_center_val, file)\n    print (""validation BboxNet center loss: %g"" % epoch_loss)\n    # plot the validation BboxNet center loss vs epoch and save to disk:\n    plt.figure(1)\n    plt.plot(epoch_losses_BboxNet_center_val, ""k^"")\n    plt.plot(epoch_losses_BboxNet_center_val, ""k"")\n    plt.ylabel(""loss"")\n    plt.xlabel(""epoch"")\n    plt.title(""validation BboxNet center loss per epoch"")\n    plt.savefig(""%s/epoch_losses_BboxNet_center_val.png"" % network.model_dir)\n    plt.close(1)\n\n    # compute the val epoch BboxNet heading class loss:\n    epoch_loss = np.mean(batch_losses_BboxNet_heading_class)\n    # save the val epoch loss:\n    epoch_losses_BboxNet_heading_class_val.append(epoch_loss)\n    # save the val epoch BboxNet heading class loss to disk:\n    with open(""%s/epoch_losses_BboxNet_heading_class_val.pkl"" % network.model_dir, ""wb"") as file:\n        pickle.dump(epoch_losses_BboxNet_heading_class_val, file)\n    print (""validation BboxNet heading class loss: %g"" % epoch_loss)\n    # plot the validation BboxNet heading class loss vs epoch and save to disk:\n    plt.figure(1)\n    plt.plot(epoch_losses_BboxNet_heading_class_val, ""k^"")\n    plt.plot(epoch_losses_BboxNet_heading_class_val, ""k"")\n    plt.ylabel(""loss"")\n    plt.xlabel(""epoch"")\n    plt.title(""validation BboxNet heading class loss per epoch"")\n    plt.savefig(""%s/epoch_losses_BboxNet_heading_class_val.png"" % network.model_dir)\n    plt.close(1)\n\n    # compute the val epoch BboxNet heading regr loss:\n    epoch_loss = np.mean(batch_losses_BboxNet_heading_regr)\n    # save the val epoch loss:\n    epoch_losses_BboxNet_heading_regr_val.append(epoch_loss)\n    # save the val epoch BboxNet heading regr loss to disk:\n    with open(""%s/epoch_losses_BboxNet_heading_regr_val.pkl"" % network.model_dir, ""wb"") as file:\n        pickle.dump(epoch_losses_BboxNet_heading_regr_val, file)\n    print (""validation BboxNet heading regr loss: %g"" % epoch_loss)\n    # plot the validation BboxNet heading regr loss vs epoch and save to disk:\n    plt.figure(1)\n    plt.plot(epoch_losses_BboxNet_heading_regr_val, ""k^"")\n    plt.plot(epoch_losses_BboxNet_heading_regr_val, ""k"")\n    plt.ylabel(""loss"")\n    plt.xlabel(""epoch"")\n    plt.title(""validation BboxNet heading regr loss per epoch"")\n    plt.savefig(""%s/epoch_losses_BboxNet_heading_regr_val.png"" % network.model_dir)\n    plt.close(1)\n\n    # compute the val epoch heading class accuracy:\n    epoch_accuracy = np.mean(batch_accuracies_heading_class)\n    # save the val epoch heading class accuracy:\n    epoch_accuracies_heading_class_val.append(epoch_accuracy)\n    # save the val epoch heading class accuracy to disk:\n    with open(""%s/epoch_accuracies_heading_class_val.pkl"" % network.model_dir, ""wb"") as file:\n        pickle.dump(epoch_accuracies_heading_class_val, file)\n    print (""validation heading class accuracy: %g"" % epoch_accuracy)\n    # plot the validation heading class accuracy vs epoch and save to disk:\n    plt.figure(1)\n    plt.plot(epoch_accuracies_heading_class_val, ""k^"")\n    plt.plot(epoch_accuracies_heading_class_val, ""k"")\n    plt.ylabel(""accuracy"")\n    plt.xlabel(""epoch"")\n    plt.title(""validation heading class accuracy per epoch"")\n    plt.savefig(""%s/epoch_accuracies_heading_class_val.png"" % network.model_dir)\n    plt.close(1)\n\n    # compute the val epoch corner loss:\n    epoch_loss = np.mean(batch_losses_corner)\n    # save the val epoch corner loss:\n    epoch_losses_corner_val.append(epoch_loss)\n    # save the val epoch corner loss to disk:\n    with open(""%s/epoch_losses_corner_val.pkl"" % network.model_dir, ""wb"") as file:\n        pickle.dump(epoch_losses_corner_val, file)\n    print (""validation corner loss: %g"" % epoch_loss)\n    # plot the validation corner loss vs epoch and save to disk:\n    plt.figure(1)\n    plt.plot(epoch_losses_corner_val, ""k^"")\n    plt.plot(epoch_losses_corner_val, ""k"")\n    plt.ylabel(""loss"")\n    plt.xlabel(""epoch"")\n    plt.title(""validation corner loss per epoch"")\n    plt.savefig(""%s/epoch_losses_corner_val.png"" % network.model_dir)\n    plt.close(1)\n\n    # compute the val epoch accuracy:\n    epoch_accuracy = np.mean(batch_accuracies)\n    # save the val epoch accuracy:\n    epoch_accuracies_val.append(epoch_accuracy)\n    # save the val epoch accuracy to disk:\n    with open(""%s/epoch_accuracies_val.pkl"" % network.model_dir, ""wb"") as file:\n        pickle.dump(epoch_accuracies_val, file)\n    print (""validation accuracy: %g"" % epoch_accuracy)\n    # plot the validation accuracy vs epoch and save to disk:\n    plt.figure(1)\n    plt.plot(epoch_accuracies_val, ""k^"")\n    plt.plot(epoch_accuracies_val, ""k"")\n    plt.ylabel(""accuracy"")\n    plt.xlabel(""epoch"")\n    plt.title(""validation accuracy per epoch"")\n    plt.savefig(""%s/epoch_accuracies_val.png"" % network.model_dir)\n    plt.close(1)\n\n    # compute the val epoch precision:\n    epoch_precision = np.mean(batch_precisions)\n    # save the val epoch precision:\n    epoch_precisions_val.append(epoch_precision)\n    # save the val epoch precision to disk:\n    with open(""%s/epoch_precisions_val.pkl"" % network.model_dir, ""wb"") as file:\n        pickle.dump(epoch_precisions_val, file)\n    print (""validation precision: %g"" % epoch_precision)\n    # plot the validation precision vs epoch and save to disk:\n    plt.figure(1)\n    plt.plot(epoch_precisions_val, ""k^"")\n    plt.plot(epoch_precisions_val, ""k"")\n    plt.ylabel(""precision"")\n    plt.xlabel(""epoch"")\n    plt.title(""validation precision per epoch"")\n    plt.savefig(""%s/epoch_precisions_val.png"" % network.model_dir)\n    plt.close(1)\n\n    # compute the val epoch recall:\n    epoch_recall = np.mean(batch_recalls)\n    # save the val epoch recall:\n    epoch_recalls_val.append(epoch_recall)\n    # save the val epoch recall to disk:\n    with open(""%s/epoch_recalls_val.pkl"" % network.model_dir, ""wb"") as file:\n        pickle.dump(epoch_recalls_val, file)\n    print (""validation recall: %g"" % epoch_recall)\n    # plot the validation recall vs epoch and save to disk:\n    plt.figure(1)\n    plt.plot(epoch_recalls_val, ""k^"")\n    plt.plot(epoch_recalls_val, ""k"")\n    plt.ylabel(""recall"")\n    plt.xlabel(""epoch"")\n    plt.title(""validation recall per epoch"")\n    plt.savefig(""%s/epoch_recalls_val.png"" % network.model_dir)\n    plt.close(1)\n\n    # compute the val epoch f1:\n    epoch_f1 = np.mean(batch_f1s)\n    # save the val epoch f1:\n    epoch_f1s_val.append(epoch_f1)\n    # save the val epoch f1 to disk:\n    with open(""%s/epoch_f1s_val.pkl"" % network.model_dir, ""wb"") as file:\n        pickle.dump(epoch_f1s_val, file)\n    print (""validation f1: %g"" % epoch_f1)\n    # plot the validation f1 vs epoch and save to disk:\n    plt.figure(1)\n    plt.plot(epoch_f1s_val, ""k^"")\n    plt.plot(epoch_f1s_val, ""k"")\n    plt.ylabel(""f1"")\n    plt.xlabel(""epoch"")\n    plt.title(""validation f1 per epoch"")\n    plt.savefig(""%s/epoch_f1s_val.png"" % network.model_dir)\n    plt.close(1)\n\n    # save the model weights to disk:\n    checkpoint_path = network.checkpoints_dir + ""/model_"" + model_id +""_epoch_"" + str(epoch+1) + "".pth""\n    torch.save(network.state_dict(), checkpoint_path)\n'"
Image-Only/datasets_imgnet.py,22,"b'# camera-ready\n\nimport sys\nsys.path.append(""/root/3DOD_thesis/utils"")\nfrom kittiloader import LabelLoader2D3D, calibread, LabelLoader2D3D_sequence # (this needs to be imported before torch, because cv2 needs to be imported before torch for some reason)\n\nimport torch\nimport torch.utils.data\n\nimport os\nimport pickle\nimport numpy as np\nimport math\nimport cv2\nfrom scipy.optimize import least_squares\n\ndef wrapToPi(a):\n    return (a + np.pi) % (2*np.pi) - np.pi\n\ndef get_keypoints(center, h, w, l, r_y, P2_mat):\n    Rmat = np.asarray([[math.cos(r_y), 0, math.sin(r_y)],\n                       [0, 1, 0],\n                       [-math.sin(r_y), 0, math.cos(r_y)]],\n                       dtype=\'float32\')\n\n    # get the keypoints in 3d camera coords:\n    p0 = center + np.dot(Rmat, np.asarray([l/2.0, 0, w/2.0], dtype=\'float32\').flatten())\n    p1 = center + np.dot(Rmat, np.asarray([-l/2.0, 0, w/2.0], dtype=\'float32\').flatten())\n    p2 = center + np.dot(Rmat, np.asarray([-l/2.0, 0, -w/2.0], dtype=\'float32\').flatten())\n    p3 = center + np.dot(Rmat, np.asarray([l/2.0, 0, -w/2.0], dtype=\'float32\').flatten())\n    p4 = center + np.dot(Rmat, np.asarray([l/2.0, -h, w/2.0], dtype=\'float32\').flatten())\n    p5 = center + np.dot(Rmat, np.asarray([-l/2.0, -h, w/2.0], dtype=\'float32\').flatten())\n    p6 = center + np.dot(Rmat, np.asarray([-l/2.0, -h, -w/2.0], dtype=\'float32\').flatten())\n    p7 = center + np.dot(Rmat, np.asarray([l/2.0, -h, -w/2.0], dtype=\'float32\').flatten())\n    keypoints_3d = np.array([p0, p1, p2, p3, p4, p5, p6, p7]) # (shape: (8, 3))\n\n    # convert to homogeneous coords:\n    keypoints_3d_hom = np.ones((8, 4), dtype=np.float32) # (shape: (8, 4))\n    keypoints_3d_hom[:, 0:3] = keypoints_3d\n\n    # project onto the image plane:\n    keypoints_hom = np.dot(P2_mat, keypoints_3d_hom.T).T # (shape: (8, 3))\n    # normalize:\n    keypoints = np.zeros((8, 2), dtype=np.float32)\n    keypoints[:, 0] = keypoints_hom[:, 0]/keypoints_hom[:, 2]\n    keypoints[:, 1] = keypoints_hom[:, 1]/keypoints_hom[:, 2]\n\n    return keypoints # (shape: (8, 2))\n\ndef draw_3dbbox_from_keypoints(img, keypoints):\n    img = np.copy(img)\n\n    color = [190, 0, 255] # (BGR)\n    front_color = [255, 230, 0] # (BGR)\n    lines = [[0, 3, 7, 4, 0], [1, 2, 6, 5, 1], [0, 1], [2, 3], [6, 7], [4, 5]] # (0 -> 3 -> 7 -> 4 -> 0, 1 -> 2 -> 6 -> 5 -> 1, etc.)\n    colors = [front_color, color, color, color, color, color]\n\n    for n, line in enumerate(lines):\n        bg = colors[n]\n\n        cv2.polylines(img, np.int32([keypoints[line]]), False, bg, lineType=cv2.LINE_AA, thickness=2)\n\n    return img\n\nclass BoxRegressor(object):\n    # NOTE! based on code provided by Eskil J\xc3\xb6rgensen\n\n    def __init__(self, camera_matrix, pred_size, pred_keypoints, pred_distance):\n        super(BoxRegressor, self).__init__()\n\n        self.P = camera_matrix\n        self.P_pseudo_inverse = np.linalg.pinv(self.P)\n        self.pred_keypoints = pred_keypoints # (shape: (8, 2))\n        self.pred_size = pred_size\n        self.pred_distance = pred_distance\n\n    def _residuals(self, params):\n        [h, w, l, x, y, z, rot_y] = params\n\n        projected_keypoints = get_keypoints(np.array([x, y, z]), h, w, l, rot_y, self.P) # (shape: (8, 2))\n\n        resids_keypoints = projected_keypoints - self.pred_keypoints # (shape: (8, 2))\n        resids_keypoints = resids_keypoints.flatten() # (shape: (16 ,))\n\n        resids_size_regularization = np.array([h - self.pred_size[0],\n                                               w - self.pred_size[1],\n                                               l - self.pred_size[2]]) # (shape: (3, ))\n\n        resids_distance_regularization = np.array([np.linalg.norm(params[3:6]) - self.pred_distance]) # (shape: (1, ))\n\n        resids = np.append(resids_keypoints, 100*resids_size_regularization) # (shape: (19, ))\n        resids = np.append(resids, 10*resids_distance_regularization) # (shape: (20, ))\n\n        return resids\n\n    def _initial_guess(self):\n        h, w, l = self.pred_size\n\n        img_keypoints_center_hom = [np.mean(self.pred_keypoints[:, 0]), np.mean(self.pred_keypoints[:, 1]), 1]\n        l0 = np.dot(self.P_pseudo_inverse, img_keypoints_center_hom)\n        l0 = l0[:3]/l0[3]\n\n        # if the point is behind the camera, switch to the mirror point in front of the camera:\n        if l0[2] < 0:\n            l0[0] = -l0[0]\n            l0[2] = -l0[2]\n\n        [x0, y0, z0] = (l0/np.linalg.norm(l0))*self.pred_distance\n\n        rot_y = -np.pi/2\n\n        return [h, w, l, x0, y0, z0, rot_y]\n\n    def solve(self):\n        x0 = self._initial_guess()\n\n        ls_results = []\n        costs = []\n        for rot_y in [-2, -1, 0, 1]:\n            x0[6] = rot_y*np.pi/2\n\n            ls_result = least_squares(self._residuals, x0, jac=""3-point"")\n            ls_results.append(ls_result)\n            costs.append(ls_result.cost)\n\n        self.result = ls_results[np.argmin(costs)]\n        params = self.result.x\n\n        return params\n\nclass DatasetImgNetAugmentation(torch.utils.data.Dataset):\n    def __init__(self, kitti_data_path, kitti_meta_path, type):\n        self.img_dir = kitti_data_path + ""/object/training/image_2/""\n        self.label_dir = kitti_data_path + ""/object/training/label_2/""\n        self.calib_dir = kitti_data_path + ""/object/training/calib/""\n        self.lidar_dir = kitti_data_path + ""/object/training/velodyne/""\n\n        with open(kitti_meta_path + ""/%s_img_ids_random.pkl"" % type, ""rb"") as file: # (needed for python3)\n            img_ids = pickle.load(file)\n\n        with open(kitti_meta_path + ""/kitti_train_mean_car_size.pkl"", ""rb"") as file: # (needed for python3)\n            self.mean_car_size = pickle.load(file)\n            self.mean_car_size = self.mean_car_size.astype(np.float32)\n\n        with open(kitti_meta_path + ""/kitti_train_mean_distance.pkl"", ""rb"") as file: # (needed for python3)\n            self.mean_distance = pickle.load(file)\n            self.mean_distance = self.mean_distance.astype(np.float32)\n            self.mean_distance = self.mean_distance[0]\n\n        self.examples = []\n        for img_id in img_ids:\n            labels = LabelLoader2D3D(img_id, self.label_dir, "".txt"", self.calib_dir, "".txt"")\n            for label in labels:\n                label_2d = label[""label_2D""]\n                if label_2d[""truncated""] < 0.5 and label_2d[""class""] == ""Car"":\n                    label[""img_id""] = img_id\n                    self.examples.append(label)\n\n        self.num_examples = len(self.examples)\n\n    def __getitem__(self, index):\n        example = self.examples[index]\n\n        img_id = example[""img_id""]\n\n        label_2D = example[""label_2D""]\n        label_3D = example[""label_3D""]\n\n        bbox = label_2D[""poly""]\n\n        u_min = bbox[0, 0] # (left)\n        u_max = bbox[1, 0] # (rigth)\n        v_min = bbox[0, 1] # (top)\n        v_max = bbox[2, 1] # (bottom)\n\n        ########################################################################\n        # augment the 2Dbbox:\n        ########################################################################\n        w = u_max - u_min\n        h = v_max - v_min\n        u_center = u_min + w/2.0\n        v_center = v_min + h/2.0\n\n        # translate the center by random distances sampled from\n        # uniform[-0.1w, 0.1w] and uniform[-0.1h, 0.1h] in u,v directions:\n        u_center = u_center + np.random.uniform(low=-0.1*w, high=0.1*w)\n        v_center = v_center + np.random.uniform(low=-0.1*h, high=0.1*h)\n\n        # randomly scale w and h by factor sampled from uniform[0.9, 1.1]:\n        w = w*np.random.uniform(low=0.9, high=1.1)\n        h = h*np.random.uniform(low=0.9, high=1.1)\n\n        u_min = u_center - w/2.0\n        u_max = u_center + w/2.0\n        v_min = v_center - h/2.0\n        v_max = v_center + h/2.0\n\n        ########################################################################\n        # get the input 2dbbox img crop and resize to 224 x 224:\n        ########################################################################\n        img_path = self.img_dir + img_id + "".png""\n        img = cv2.imread(img_path, -1)\n\n        bbox_2d_img = img[int(np.max([0, v_min])):int(v_max), int(np.max([0, u_min])):int(u_max)]\n        bbox_2d_img = cv2.resize(bbox_2d_img, (224, 224))\n\n        # # # # # debug visualization:\n        # cv2.imshow(""test"", bbox_2d_img)\n        # cv2.waitKey(0)\n        # # # # #\n\n        ########################################################################\n        # flip the 2dbbox img crop with 0.5 probability:\n        ########################################################################\n        # get 0 or 1 with equal probability (indicating if the frustum should be flipped or not):\n        flip = np.random.randint(low=0, high=2)\n\n        if flip == 1:\n            bbox_2d_img = cv2.flip(bbox_2d_img, 1)\n            # cv2.imshow(""test"", bbox_2d_img)\n            # cv2.waitKey(0)\n\n        ########################################################################\n        # normalize the 2dbbox img crop:\n        ########################################################################\n        bbox_2d_img = bbox_2d_img/255.0\n        bbox_2d_img = bbox_2d_img - np.array([0.485, 0.456, 0.406])\n        bbox_2d_img = bbox_2d_img/np.array([0.229, 0.224, 0.225]) # (shape: (H, W, 3))\n        bbox_2d_img = np.transpose(bbox_2d_img, (2, 0, 1)) # (shape: (3, H, W))\n        bbox_2d_img = bbox_2d_img.astype(np.float32)\n\n        ########################################################################\n        # size ground truth:\n        ########################################################################\n        label_size = np.zeros((3, ), dtype=np.float32) # ([h, w, l])\n        label_size[0] = label_3D[\'h\']\n        label_size[1] = label_3D[\'w\']\n        label_size[2] = label_3D[\'l\']\n        label_size = label_size - self.mean_car_size\n\n        ########################################################################\n        # keypoints ground truth:\n        ########################################################################\n        label_keypoints = get_keypoints(label_3D[""center""], label_3D[\'h\'],\n                                        label_3D[\'w\'], label_3D[\'l\'],\n                                        label_3D[\'r_y\'], label_3D[\'P0_mat\']) # (shape (8, 2))\n\n        if flip == 1:\n            img = cv2.imread(self.img_dir + img_id + "".png"", -1)\n            img_w = img.shape[1]\n\n            u_center = img_w - u_center\n            label_keypoints[:, 0] = img_w - label_keypoints[:, 0]\n\n            # swap the keypoints to adjust for the flipping:\n            # # swap keypoint 7 and 4:\n            temp = np.copy(label_keypoints[7, :])\n            label_keypoints[7, :]= label_keypoints[4, :]\n            label_keypoints[4, :] = temp\n            # # swap keypoint 3 and 0:\n            temp = np.copy(label_keypoints[3, :])\n            label_keypoints[3, :]= label_keypoints[0, :]\n            label_keypoints[0, :] = temp\n            # # swap keypoint 6 and 5:\n            temp = np.copy(label_keypoints[6, :])\n            label_keypoints[6, :]= label_keypoints[5, :]\n            label_keypoints[5, :] = temp\n            # # swap keypoint 2 and 1:\n            temp = np.copy(label_keypoints[2, :])\n            label_keypoints[2, :]= label_keypoints[1, :]\n            label_keypoints[1, :] = temp\n\n        # # # # # # debug visualization:\n        # img = cv2.imread(self.img_dir + img_id + "".png"", -1)\n        # if flip == 1:\n        #     img = cv2.flip(img, 1)\n        # img_with_gt_3dbbox = draw_3dbbox_from_keypoints(img, label_keypoints)\n        # cv2.imshow(""test"", img_with_gt_3dbbox)\n        # cv2.waitKey(0)\n        # # # # # #\n\n        label_keypoints = label_keypoints - np.array([u_center, v_center])\n        label_keypoints = label_keypoints/np.array([w, h])\n\n        # # # # # # debug visualization:\n        # img = cv2.imread(self.img_dir + img_id + "".png"", -1)\n        # if flip == 1:\n        #     img = cv2.flip(img, 1)\n        # keypoints_restored = label_keypoints*np.array([w, h]) + np.array([u_center, v_center])\n        # img_with_gt_3dbbox = draw_3dbbox_from_keypoints(img, keypoints_restored)\n        # cv2.imshow(""test"", img_with_gt_3dbbox)\n        # cv2.waitKey(0)\n        # # # # # #\n\n        label_keypoints = label_keypoints.flatten() # (shape: (2*8 = 16, )) (np.resize(label_keypoints, (8, 2)) to restore)\n        label_keypoints = label_keypoints.astype(np.float32)\n\n        ########################################################################\n        # distance ground truth:\n        ########################################################################\n        label_distance = np.array([np.linalg.norm(label_3D[""center""])], dtype=np.float32)\n\n        label_distance = label_distance - self.mean_distance\n\n        ########################################################################\n        # convert numpy -> torch:\n        ########################################################################\n        bbox_2d_img = torch.from_numpy(bbox_2d_img) # (shape: (3, H, W) = (3, 224, 224))\n        label_size = torch.from_numpy(label_size) # (shape: (3, ))\n        label_keypoints = torch.from_numpy(label_keypoints) # (shape: (2*8 = 16, ))\n        label_distance = torch.from_numpy(label_distance) # (shape: (1, ))\n\n        return (bbox_2d_img, label_size, label_keypoints, label_distance)\n\n    def __len__(self):\n        return self.num_examples\n\n# test = DatasetImgNetAugmentation(""/home/fregu856/exjobb/data/kitti"", ""/home/fregu856/exjobb/data/kitti/meta"", type=""train"")\n# for i in range(15):\n#     _ = test.__getitem__(i)\n\nclass DatasetImgNetEval(torch.utils.data.Dataset):\n    def __init__(self, kitti_data_path, kitti_meta_path, type):\n        self.img_dir = kitti_data_path + ""/object/training/image_2/""\n        self.label_dir = kitti_data_path + ""/object/training/label_2/""\n        self.calib_dir = kitti_data_path + ""/object/training/calib/""\n        self.lidar_dir = kitti_data_path + ""/object/training/velodyne/""\n\n        with open(kitti_meta_path + ""/%s_img_ids.pkl"" % type, ""rb"") as file: # (needed for python3)\n            img_ids = pickle.load(file)\n\n        with open(kitti_meta_path + ""/kitti_train_mean_car_size.pkl"", ""rb"") as file: # (needed for python3)\n            self.mean_car_size = pickle.load(file)\n            self.mean_car_size = self.mean_car_size.astype(np.float32)\n\n        with open(kitti_meta_path + ""/kitti_train_mean_distance.pkl"", ""rb"") as file: # (needed for python3)\n            self.mean_distance = pickle.load(file)\n            self.mean_distance = self.mean_distance.astype(np.float32)\n            self.mean_distance = self.mean_distance[0]\n\n        self.examples = []\n        for img_id in img_ids:\n            labels = LabelLoader2D3D(img_id, self.label_dir, "".txt"", self.calib_dir, "".txt"")\n            for label in labels:\n                label_2d = label[""label_2D""]\n                if label_2d[""truncated""] < 0.5 and label_2d[""class""] == ""Car"":\n                    label[""img_id""] = img_id\n                    self.examples.append(label)\n\n        self.num_examples = len(self.examples)\n\n    def __getitem__(self, index):\n        example = self.examples[index]\n\n        img_id = example[""img_id""]\n\n        label_2D = example[""label_2D""]\n        label_3D = example[""label_3D""]\n\n        bbox = label_2D[""poly""]\n\n        u_min = bbox[0, 0] # (left)\n        u_max = bbox[1, 0] # (rigth)\n        v_min = bbox[0, 1] # (top)\n        v_max = bbox[2, 1] # (bottom)\n\n        w = u_max - u_min\n        h = v_max - v_min\n        u_center = u_min + w/2.0\n        v_center = v_min + h/2.0\n\n        ########################################################################\n        # get the input 2dbbox img crop and resize to 224 x 224:\n        ########################################################################\n        img_path = self.img_dir + img_id + "".png""\n        img = cv2.imread(img_path, -1)\n\n        bbox_2d_img = img[int(np.max([0, v_min])):int(v_max), int(np.max([0, u_min])):int(u_max)]\n        bbox_2d_img = cv2.resize(bbox_2d_img, (224, 224))\n\n        # # # # # debug visualization:\n        # cv2.imshow(""test"", bbox_2d_img)\n        # cv2.waitKey(0)\n        # # # # #\n\n        ########################################################################\n        # normalize the 2dbbox img crop:\n        ########################################################################\n        bbox_2d_img = bbox_2d_img/255.0\n        bbox_2d_img = bbox_2d_img - np.array([0.485, 0.456, 0.406])\n        bbox_2d_img = bbox_2d_img/np.array([0.229, 0.224, 0.225]) # (shape: (H, W, 3))\n        bbox_2d_img = np.transpose(bbox_2d_img, (2, 0, 1)) # (shape: (3, H, W))\n        bbox_2d_img = bbox_2d_img.astype(np.float32)\n\n        ########################################################################\n        # size ground truth:\n        ########################################################################\n        label_size = np.zeros((3, ), dtype=np.float32) # ([h, w, l])\n        label_size[0] = label_3D[\'h\']\n        label_size[1] = label_3D[\'w\']\n        label_size[2] = label_3D[\'l\']\n        label_size = label_size - self.mean_car_size\n\n        ########################################################################\n        # keypoints ground truth:\n        ########################################################################\n        label_keypoints = get_keypoints(label_3D[""center""], label_3D[\'h\'],\n                                        label_3D[\'w\'], label_3D[\'l\'],\n                                        label_3D[\'r_y\'], label_3D[\'P0_mat\']) # (shape (8, 2))\n\n        # # # # # # debug visualization:\n        # img = cv2.imread(self.img_dir + img_id + "".png"", -1)\n        # img_with_gt_3dbbox = draw_3dbbox_from_keypoints(img, label_keypoints)\n        # cv2.imshow(""test"", img_with_gt_3dbbox)\n        # cv2.waitKey(0)\n        # # # # # #\n\n        label_keypoints = label_keypoints - np.array([u_center, v_center])\n        label_keypoints = label_keypoints/np.array([w, h])\n\n        # # # # # # debug visualization:\n        # img = cv2.imread(self.img_dir + img_id + "".png"", -1)\n        # keypoints_restored = label_keypoints*np.array([img_w, img_h]) + np.array([u_center, v_center])\n        # img_with_gt_3dbbox = draw_3dbbox_from_keypoints(img, keypoints_restored)\n        # cv2.imshow(""test"", img_with_gt_3dbbox)\n        # cv2.waitKey(0)\n        # # # # # #\n\n        label_keypoints = label_keypoints.flatten() # (shape: (2*8 = 16, )) (np.resize(label_keypoints, (8, 2)) to restore)\n        label_keypoints = label_keypoints.astype(np.float32)\n\n        ########################################################################\n        # distance ground truth:\n        ########################################################################\n        label_distance = np.array([np.linalg.norm(label_3D[""center""])], dtype=np.float32)\n\n        label_distance = label_distance - self.mean_distance\n\n        ########################################################################\n        # convert numpy -> torch:\n        ########################################################################\n        bbox_2d_img = torch.from_numpy(bbox_2d_img) # (shape: (3, H, W) = (3, 224, 224))\n        label_size = torch.from_numpy(label_size) # (shape: (3, ))\n        label_keypoints = torch.from_numpy(label_keypoints) # (shape: (2*8 = 16, ))\n        label_distance = torch.from_numpy(label_distance) # (shape: (1, ))\n\n        camera_matrix = label_3D[""P0_mat""]\n        gt_center = label_3D[""center""]\n        gt_center = gt_center.astype(np.float32)\n        gt_r_y = np.float32(label_3D[""r_y""])\n\n        return (bbox_2d_img, label_size, label_keypoints, label_distance, img_id, self.mean_car_size, w, h, u_center, v_center, camera_matrix, gt_center, gt_r_y, self.mean_distance)\n\n    def __len__(self):\n        return self.num_examples\n\n# test = DatasetImgNetEval(""/home/fregu856/exjobb/data/kitti"", ""/home/fregu856/exjobb/data/kitti/meta"", type=""train"")\n# for i in range(15):\n#     _ = test.__getitem__(i)\n\nclass DatasetImgNetEvalValSeq(torch.utils.data.Dataset):\n    def __init__(self, kitti_data_path, kitti_meta_path, sequence=""0000""):\n        self.img_dir = kitti_data_path + ""/tracking/training/image_02/"" + sequence + ""/""\n        self.lidar_dir = kitti_data_path + ""/tracking/training/velodyne/"" + sequence + ""/""\n        self.label_path = kitti_data_path + ""/tracking/training/label_02/"" + sequence + "".txt""\n        self.calib_path = kitti_meta_path + ""/tracking/training/calib/"" + sequence + "".txt"" # NOTE! NOTE! the data format for the calib files was sliightly different for tracking, so I manually modifed the 20 files and saved them in the kitti_meta folder\n\n        with open(kitti_meta_path + ""/kitti_train_mean_car_size.pkl"", ""rb"") as file: # (needed for python3)\n            self.mean_car_size = pickle.load(file)\n            self.mean_car_size = self.mean_car_size.astype(np.float32)\n\n        with open(kitti_meta_path + ""/kitti_train_mean_distance.pkl"", ""rb"") as file: # (needed for python3)\n            self.mean_distance = pickle.load(file)\n            self.mean_distance = self.mean_distance.astype(np.float32)\n            self.mean_distance = self.mean_distance[0]\n\n        img_ids = []\n        img_names = os.listdir(self.img_dir)\n        for img_name in img_names:\n            img_id = img_name.split("".png"")[0]\n            img_ids.append(img_id)\n\n        self.examples = []\n        for img_id in img_ids:\n            if img_id.lstrip(\'0\') == \'\':\n                img_id_float = 0.0\n            else:\n                img_id_float = float(img_id.lstrip(\'0\'))\n\n            labels = LabelLoader2D3D_sequence(img_id, img_id_float, self.label_path, self.calib_path)\n\n            for label in labels:\n                label_2d = label[""label_2D""]\n                if label_2d[""truncated""] < 0.5 and label_2d[""class""] == ""Car"":\n                    label[""img_id""] = img_id\n                    self.examples.append(label)\n\n        self.num_examples = len(self.examples)\n\n    def __getitem__(self, index):\n        example = self.examples[index]\n\n        img_id = example[""img_id""]\n\n        label_2D = example[""label_2D""]\n        label_3D = example[""label_3D""]\n\n        bbox = label_2D[""poly""]\n\n        u_min = bbox[0, 0] # (left)\n        u_max = bbox[1, 0] # (rigth)\n        v_min = bbox[0, 1] # (top)\n        v_max = bbox[2, 1] # (bottom)\n\n        w = u_max - u_min\n        h = v_max - v_min\n        u_center = u_min + w/2.0\n        v_center = v_min + h/2.0\n\n        ########################################################################\n        # get the input 2dbbox img crop and resize to 224 x 224:\n        ########################################################################\n        img_path = self.img_dir + img_id + "".png""\n        img = cv2.imread(img_path, -1)\n\n        bbox_2d_img = img[int(np.max([0, v_min])):int(v_max), int(np.max([0, u_min])):int(u_max)]\n        bbox_2d_img = cv2.resize(bbox_2d_img, (224, 224))\n\n        # # # # # debug visualization:\n        #cv2.imshow(""test"", bbox_2d_img)\n        #cv2.waitKey(0)\n        # # # # #\n\n        ########################################################################\n        # normalize the 2dbbox img crop:\n        ########################################################################\n        bbox_2d_img = bbox_2d_img/255.0\n        bbox_2d_img = bbox_2d_img - np.array([0.485, 0.456, 0.406])\n        bbox_2d_img = bbox_2d_img/np.array([0.229, 0.224, 0.225]) # (shape: (H, W, 3))\n        bbox_2d_img = np.transpose(bbox_2d_img, (2, 0, 1)) # (shape: (3, H, W))\n        bbox_2d_img = bbox_2d_img.astype(np.float32)\n\n        ########################################################################\n        # size ground truth:\n        ########################################################################\n        label_size = np.zeros((3, ), dtype=np.float32) # ([h, w, l])\n        label_size[0] = label_3D[\'h\']\n        label_size[1] = label_3D[\'w\']\n        label_size[2] = label_3D[\'l\']\n        label_size = label_size - self.mean_car_size\n\n        ########################################################################\n        # keypoints ground truth:\n        ########################################################################\n        label_keypoints = get_keypoints(label_3D[""center""], label_3D[\'h\'],\n                                        label_3D[\'w\'], label_3D[\'l\'],\n                                        label_3D[\'r_y\'], label_3D[\'P0_mat\']) # (shape (8, 2))\n\n        # # # # # # debug visualization:\n        # img = cv2.imread(self.img_dir + img_id + "".png"", -1)\n        # img_with_gt_3dbbox = draw_3dbbox_from_keypoints(img, label_keypoints)\n        # cv2.imshow(""test"", img_with_gt_3dbbox)\n        # cv2.waitKey(0)\n        # # # # # #\n\n        label_keypoints = label_keypoints - np.array([u_center, v_center])\n        label_keypoints = label_keypoints/np.array([w, h])\n\n        # # # # # # debug visualization:\n        # img = cv2.imread(self.img_dir + img_id + "".png"", -1)\n        # keypoints_restored = label_keypoints*np.array([img_w, img_h]) + np.array([u_center, v_center])\n        # img_with_gt_3dbbox = draw_3dbbox_from_keypoints(img, keypoints_restored)\n        # cv2.imshow(""test"", img_with_gt_3dbbox)\n        # cv2.waitKey(0)\n        # # # # # #\n\n        label_keypoints = label_keypoints.flatten() # (shape: (2*8 = 16, )) (np.resize(label_keypoints, (8, 2)) to restore)\n        label_keypoints = label_keypoints.astype(np.float32)\n\n        ########################################################################\n        # distance ground truth:\n        ########################################################################\n        label_distance = np.array([np.linalg.norm(label_3D[""center""])], dtype=np.float32)\n\n        label_distance = label_distance - self.mean_distance\n\n        ########################################################################\n        # convert numpy -> torch:\n        ########################################################################\n        bbox_2d_img = torch.from_numpy(bbox_2d_img) # (shape: (3, H, W) = (3, 224, 224))\n        label_size = torch.from_numpy(label_size) # (shape: (3, ))\n        label_keypoints = torch.from_numpy(label_keypoints) # (shape: (2*8 = 16, ))\n        label_distance = torch.from_numpy(label_distance) # (shape: (1, ))\n\n        camera_matrix = label_3D[""P0_mat""]\n        gt_center = label_3D[""center""]\n        gt_center = gt_center.astype(np.float32)\n        gt_r_y = np.float32(label_3D[""r_y""])\n\n        return (bbox_2d_img, label_size, label_keypoints, label_distance, img_id, self.mean_car_size, w, h, u_center, v_center, camera_matrix, gt_center, gt_r_y, self.mean_distance)\n\n    def __len__(self):\n        return self.num_examples\n\n# test = DatasetImgNetEvalValSeq(""/home/fregu856/exjobb/data/kitti"", ""/home/fregu856/exjobb/data/kitti/meta"", train_dataset=""kitti"", sequence=""0004"")\n# for i in range(15):\n#     _ = test.__getitem__(i)\n\nclass DatasetImgNetEvalTestSeq(torch.utils.data.Dataset):\n    def __init__(self, kitti_data_path, kitti_meta_path, sequence=""0000""):\n        self.img_dir = kitti_data_path + ""/tracking/testing/image_02/"" + sequence + ""/""\n        self.lidar_dir = kitti_data_path + ""/tracking/testing/velodyne/"" + sequence + ""/""\n        self.calib_path = kitti_meta_path + ""/tracking/testing/calib/"" + sequence + "".txt"" # NOTE! NOTE! the data format for the calib files was sliightly different for tracking, so I manually modifed the 28 files and saved them in the kitti_meta folder\n        self.detections_2d_path = kitti_meta_path + ""/tracking/testing/2d_detections/"" + sequence + ""/inferResult_1.txt""\n\n        with open(kitti_meta_path + ""/kitti_train_mean_car_size.pkl"", ""rb"") as file: # (needed for python3)\n            self.mean_car_size = pickle.load(file)\n            self.mean_car_size = self.mean_car_size.astype(np.float32)\n\n        with open(kitti_meta_path + ""/kitti_train_mean_distance.pkl"", ""rb"") as file: # (needed for python3)\n            self.mean_distance = pickle.load(file)\n            self.mean_distance = self.mean_distance.astype(np.float32)\n            self.mean_distance = self.mean_distance[0]\n\n        self.examples = []\n        with open(self.detections_2d_path) as file:\n            # line format: img_id, img_height, img_width, class, u_min, v_min, u_max, v_max, confidence score, distance estimate\n            for line in file:\n                values = line.split()\n                object_class = float(values[3])\n                if object_class == 1: # (1: Car)\n                    img_id = values[0]\n                    u_min = float(values[4])\n                    v_min = float(values[5])\n                    u_max = float(values[6])\n                    v_max = float(values[7])\n                    score_2d = float(values[8])\n\n                    detection_2d = {}\n                    detection_2d[""u_min""] = u_min\n                    detection_2d[""v_min""] = v_min\n                    detection_2d[""u_max""] = u_max\n                    detection_2d[""v_max""] = v_max\n                    detection_2d[""score_2d""] = score_2d\n                    detection_2d[""img_id""] = img_id\n\n                    self.examples.append(detection_2d)\n\n        self.num_examples = len(self.examples)\n\n    def __getitem__(self, index):\n        example = self.examples[index]\n\n        img_id = example[""img_id""]\n\n        calib = calibread(self.calib_path)\n        camera_matrix = calib[\'P2\']\n\n        u_min = example[""u_min""] # (left)\n        u_max = example[""u_max""] # (rigth)\n        v_min = example[""v_min""] # (top)\n        v_max = example[""v_max""] # (bottom)\n\n        input_2Dbbox = np.array([u_min, u_max, v_min, v_max])\n\n        w = u_max - u_min\n        h = v_max - v_min\n        u_center = u_min + w/2.0\n        v_center = v_min + h/2.0\n\n        ########################################################################\n        # get the input 2dbbox img crop and resize to 224 x 224:\n        ########################################################################\n        img_path = self.img_dir + img_id + "".png""\n        img = cv2.imread(img_path, -1)\n\n        bbox_2d_img = img[int(np.max([0, v_min])):int(v_max), int(np.max([0, u_min])):int(u_max)]\n        bbox_2d_img = cv2.resize(bbox_2d_img, (224, 224))\n\n        # # # # # debug visualization:\n        #cv2.imshow(""test"", bbox_2d_img)\n        #cv2.waitKey(0)\n        # # # # #\n\n        ########################################################################\n        # normalize the 2dbbox img crop:\n        ########################################################################\n        bbox_2d_img = bbox_2d_img/255.0\n        bbox_2d_img = bbox_2d_img - np.array([0.485, 0.456, 0.406])\n        bbox_2d_img = bbox_2d_img/np.array([0.229, 0.224, 0.225]) # (shape: (H, W, 3))\n        bbox_2d_img = np.transpose(bbox_2d_img, (2, 0, 1)) # (shape: (3, H, W))\n        bbox_2d_img = bbox_2d_img.astype(np.float32)\n\n        ########################################################################\n        # convert numpy -> torch:\n        ########################################################################\n        bbox_2d_img = torch.from_numpy(bbox_2d_img) # (shape: (3, H, W) = (3, 224, 224))\n\n        return (bbox_2d_img, img_id, self.mean_car_size, w, h, u_center, v_center, input_2Dbbox, camera_matrix, self.mean_distance)\n\n    def __len__(self):\n        return self.num_examples\n\nclass DatasetKittiTest(torch.utils.data.Dataset):\n    def __init__(self, kitti_data_path, kitti_meta_path):\n        self.img_dir = kitti_data_path + ""/object/testing/image_2/""\n        self.calib_dir = kitti_data_path + ""/object/testing/calib/""\n        self.lidar_dir = kitti_data_path + ""/object/testing/velodyne/""\n        self.detections_2d_dir = kitti_meta_path + ""/object/testing/2d_detections/""\n\n        with open(kitti_meta_path + ""/kitti_train_mean_car_size.pkl"", ""rb"") as file: # (needed for python3)\n            self.mean_car_size = pickle.load(file)\n            self.mean_car_size = self.mean_car_size.astype(np.float32)\n\n        with open(kitti_meta_path + ""/kitti_train_mean_distance.pkl"", ""rb"") as file: # (needed for python3)\n            self.mean_distance = pickle.load(file)\n            self.mean_distance = self.mean_distance.astype(np.float32)\n            self.mean_distance = self.mean_distance[0]\n\n        img_ids = []\n        img_names = os.listdir(self.img_dir)\n        for img_name in img_names:\n            img_id = img_name.split("".png"")[0]\n            img_ids.append(img_id)\n\n        self.examples = []\n        for img_id in img_ids:\n            detections_file_path = self.detections_2d_dir + img_id + "".txt""\n            with open(detections_file_path) as file:\n                # line format: img_id, img_height, img_width, class, u_min, v_min, u_max, v_max, confidence score, distance estimate\n                for line in file:\n                    values = line.split()\n                    object_class = float(values[3])\n                    if object_class == 1: # (1: Car)\n                        u_min = float(values[4])\n                        v_min = float(values[5])\n                        u_max = float(values[6])\n                        v_max = float(values[7])\n                        score_2d = float(values[8])\n\n                        detection_2d = {}\n                        detection_2d[""u_min""] = u_min\n                        detection_2d[""v_min""] = v_min\n                        detection_2d[""u_max""] = u_max\n                        detection_2d[""v_max""] = v_max\n                        detection_2d[""score_2d""] = score_2d\n                        detection_2d[""img_id""] = img_id\n\n                        self.examples.append(detection_2d)\n\n        self.num_examples = len(self.examples)\n\n    def __getitem__(self, index):\n        example = self.examples[index]\n\n        img_id = example[""img_id""]\n\n        calib_path = self.calib_dir + img_id + "".txt""\n        calib = calibread(calib_path)\n        camera_matrix = calib[\'P2\']\n\n        u_min = example[""u_min""] # (left)\n        u_max = example[""u_max""] # (rigth)\n        v_min = example[""v_min""] # (top)\n        v_max = example[""v_max""] # (bottom)\n\n        score_2d = example[""score_2d""]\n\n        input_2Dbbox = np.array([u_min, u_max, v_min, v_max])\n\n        w = u_max - u_min\n        h = v_max - v_min\n        u_center = u_min + w/2.0\n        v_center = v_min + h/2.0\n\n        ########################################################################\n        # get the input 2dbbox img crop and resize to 224 x 224:\n        ########################################################################\n        img_path = self.img_dir + img_id + "".png""\n        img = cv2.imread(img_path, -1)\n\n        bbox_2d_img = img[int(np.max([0, v_min])):int(v_max), int(np.max([0, u_min])):int(u_max)]\n        bbox_2d_img = cv2.resize(bbox_2d_img, (224, 224))\n\n        # # # # # debug visualization:\n        #cv2.imshow(""test"", bbox_2d_img)\n        #cv2.waitKey(0)\n        # # # # #\n\n        ########################################################################\n        # normalize the 2dbbox img crop:\n        ########################################################################\n        bbox_2d_img = bbox_2d_img/255.0\n        bbox_2d_img = bbox_2d_img - np.array([0.485, 0.456, 0.406])\n        bbox_2d_img = bbox_2d_img/np.array([0.229, 0.224, 0.225]) # (shape: (H, W, 3))\n        bbox_2d_img = np.transpose(bbox_2d_img, (2, 0, 1)) # (shape: (3, H, W))\n        bbox_2d_img = bbox_2d_img.astype(np.float32)\n\n        ########################################################################\n        # convert numpy -> torch:\n        ########################################################################\n        bbox_2d_img = torch.from_numpy(bbox_2d_img) # (shape: (3, H, W) = (3, 224, 224))\n\n        return (bbox_2d_img, img_id, self.mean_car_size, w, h, u_center, v_center, input_2Dbbox, camera_matrix, self.mean_distance, score_2d)\n\n    def __len__(self):\n        return self.num_examples\n\nclass DatasetImgNetVal2ddetections(torch.utils.data.Dataset):\n    def __init__(self, kitti_data_path, kitti_meta_path):\n        self.img_dir = kitti_data_path + ""/object/training/image_2/""\n        self.calib_dir = kitti_data_path + ""/object/training/calib/""\n        self.lidar_dir = kitti_data_path + ""/object/training/velodyne/""\n        self.detections_2d_path = kitti_meta_path + ""/rgb_detection_val.txt""\n\n        with open(kitti_meta_path + ""/kitti_train_mean_car_size.pkl"", ""rb"") as file: # (needed for python3)\n            self.mean_car_size = pickle.load(file)\n            self.mean_car_size = self.mean_car_size.astype(np.float32)\n\n        with open(kitti_meta_path + ""/kitti_train_mean_distance.pkl"", ""rb"") as file: # (needed for python3)\n            self.mean_distance = pickle.load(file)\n            self.mean_distance = self.mean_distance.astype(np.float32)\n            self.mean_distance = self.mean_distance[0]\n\n        self.examples = []\n        with open(self.detections_2d_path) as file:\n            # line format: /home/rqi/Data/KITTI/object/training/image_2/***img_id***.png, class, conf_score, u_min, v_min, u_max, v_max\n            for line in file:\n                values = line.split()\n                object_class = float(values[1])\n                if object_class == 2: # (2: Car)\n                    score_2d = float(values[2])\n                    u_min = float(values[3])\n                    v_min = float(values[4])\n                    u_max = float(values[5])\n                    v_max = float(values[6])\n\n                    img_id = values[0].split(""image_2/"")[1]\n                    img_id = img_id.split(""."")[0]\n\n                    detection_2d = {}\n                    detection_2d[""u_min""] = u_min\n                    detection_2d[""v_min""] = v_min\n                    detection_2d[""u_max""] = u_max\n                    detection_2d[""v_max""] = v_max\n                    detection_2d[""score_2d""] = score_2d\n                    detection_2d[""img_id""] = img_id\n\n                    self.examples.append(detection_2d)\n\n        self.num_examples = len(self.examples)\n\n    def __getitem__(self, index):\n        example = self.examples[index]\n\n        img_id = example[""img_id""]\n\n        calib_path = self.calib_dir + img_id + "".txt""\n        calib = calibread(calib_path)\n        camera_matrix = calib[\'P2\']\n\n        u_min = example[""u_min""] # (left)\n        u_max = example[""u_max""] # (rigth)\n        v_min = example[""v_min""] # (top)\n        v_max = example[""v_max""] # (bottom)\n\n        score_2d = example[""score_2d""]\n\n        input_2Dbbox = np.array([u_min, u_max, v_min, v_max])\n\n        w = u_max - u_min\n        h = v_max - v_min\n        u_center = u_min + w/2.0\n        v_center = v_min + h/2.0\n\n        ########################################################################\n        # get the input 2dbbox img crop and resize to 224 x 224:\n        ########################################################################\n        img_path = self.img_dir + img_id + "".png""\n        img = cv2.imread(img_path, -1)\n\n        bbox_2d_img = img[int(np.max([0, v_min])):int(v_max), int(np.max([0, u_min])):int(u_max)]\n        bbox_2d_img = cv2.resize(bbox_2d_img, (224, 224))\n\n        # # # # # debug visualization:\n        #cv2.imshow(""test"", bbox_2d_img)\n        #cv2.waitKey(0)\n        # # # # #\n\n        ########################################################################\n        # normalize the 2dbbox img crop:\n        ########################################################################\n        bbox_2d_img = bbox_2d_img/255.0\n        bbox_2d_img = bbox_2d_img - np.array([0.485, 0.456, 0.406])\n        bbox_2d_img = bbox_2d_img/np.array([0.229, 0.224, 0.225]) # (shape: (H, W, 3))\n        bbox_2d_img = np.transpose(bbox_2d_img, (2, 0, 1)) # (shape: (3, H, W))\n        bbox_2d_img = bbox_2d_img.astype(np.float32)\n\n        ########################################################################\n        # convert numpy -> torch:\n        ########################################################################\n        bbox_2d_img = torch.from_numpy(bbox_2d_img) # (shape: (3, H, W) = (3, 224, 224))\n\n        return (bbox_2d_img, img_id, self.mean_car_size, w, h, u_center, v_center, input_2Dbbox, camera_matrix, self.mean_distance, score_2d)\n\n    def __len__(self):\n        return self.num_examples\n\n# ###############################################################################\n# # compute mean_distance in the KITTI train set:\n# ###############################################################################\n# from kittiloader import LabelLoader2D3D\n#\n# import pickle\n# import numpy as np\n#\n# with open(""/staging/frexgus/kitti/meta/train_img_ids.pkl"", ""rb"") as file:\n#     img_ids = pickle.load(file)\n#\n# label_dir = ""/datasets/kitti/object/training/label_2/""\n# calib_dir = ""/datasets/kitti/object/training/calib/""\n#\n# distances = np.array([])\n# for img_id in img_ids:\n#     labels = LabelLoader2D3D(img_id, label_dir, "".txt"", calib_dir, "".txt"")\n#     for label in labels:\n#         label_3d = label[""label_3D""]\n#         if label_3d[""class""] == ""Car"":\n#             distance = np.linalg.norm(label_3d[""center""])\n#\n#             distances = np.append(distances, distance)\n#\n# print (distances)\n# print (distances.shape)\n#\n# mean_distance = np.mean(distances)\n# mean_distance = np.array([mean_distance])\n#\n# print (mean_distance)\n#\n# with open(""/staging/frexgus/kitti/meta/kitti_train_mean_distance.pkl"", ""wb"") as file:\n#    pickle.dump(mean_distance, file, protocol=2) # (protocol=2 is needed to be able to open this file with python2))\n'"
Image-Only/eval_imgnet_test.py,8,"b'# camera-ready\n\nfrom datasets_imgnet import DatasetKittiTest, BoxRegressor # (this needs to be imported before torch, because cv2 needs to be imported before torch for some reason)\nfrom datasets_imgnet import wrapToPi\nfrom imgnet import ImgNet\n\nimport torch\nimport torch.utils.data\nimport torch.nn as nn\nfrom torch.autograd import Variable\nimport torch.optim as optim\nimport torch.nn.functional as F\n\nimport numpy as np\nimport pickle\nimport matplotlib\nmatplotlib.use(""Agg"")\nimport matplotlib.pyplot as plt\nimport cv2\n\nbatch_size = 8\n\nnetwork = ImgNet(""Image-Only_eval_test"", project_dir=""/root/3DOD_thesis"")\nnetwork.load_state_dict(torch.load(""/root/3DOD_thesis/pretrained_models/model_10_2_epoch_400.pth""))\nnetwork = network.cuda()\n\nnetwork.eval() # (set in evaluation mode, this affects BatchNorm and dropout)\n\nval_dataset = DatasetKittiTest(kitti_data_path=""/root/3DOD_thesis/data/kitti"",\n                               kitti_meta_path=""/root/3DOD_thesis/data/kitti/meta"")\n\nnum_val_batches = int(len(val_dataset)/batch_size)\n\nval_loader = torch.utils.data.DataLoader(dataset=val_dataset,\n                                         batch_size=batch_size, shuffle=False,\n                                         num_workers=4)\n\neval_dict = {}\nfor step, (bbox_2d_imgs, img_ids, mean_car_size, ws, hs, u_centers, v_centers, input_2Dbboxes, camera_matrices, mean_distance, scores_2d) in enumerate(val_loader):\n    if step % 100 == 0:\n        print (""step: %d/%d"" % (step+1, num_val_batches))\n\n    with torch.no_grad(): # (corresponds to setting volatile=True in all variables, this is done during inference to reduce memory consumption)\n        bbox_2d_imgs = Variable(bbox_2d_imgs) # (shape: (batch_size, 3, H, W) = (batch_size, 3, 224, 224))\n\n        bbox_2d_imgs = bbox_2d_imgs.cuda()\n\n        outputs = network(bbox_2d_imgs) # (shape: (batch_size, 2*8 + 3 = 19))\n        outputs_keypoints = outputs[:, 0:16] # (shape: (batch_size, 2*8))\n        outputs_size = outputs[:, 16:19] # (shape: (batch_size, 3)\n        outputs_distance = outputs[:, 19] # (shape: (batch_size, )\n\n        ############################################################################\n        # save data for visualization:\n        ############################################################################\n        mean_car_size = Variable(mean_car_size).cuda()\n        outputs_size = outputs_size + mean_car_size # NOTE NOTE\n        mean_distance = Variable(mean_distance).cuda()\n        outputs_distance = outputs_distance + mean_distance # NOTE NOTE\n        for i in range(outputs.size()[0]):\n            output_keypoints = outputs_keypoints[i].data.cpu().numpy()\n            output_size = outputs_size[i].data.cpu().numpy()\n            output_distance = outputs_distance[i].data.cpu().numpy()\n            w = ws[i]\n            h = hs[i]\n            u_center = u_centers[i]\n            v_center = v_centers[i]\n            img_id = img_ids[i]\n            camera_matrix = camera_matrices[i]\n            input_2Dbbox = input_2Dbboxes[i]\n            score_2d = scores_2d[i]\n\n            output_keypoints = np.resize(output_keypoints, (8, 2))\n            output_keypoints = output_keypoints*np.array([w, h]) + np.array([u_center, v_center])\n\n            box_regressor = BoxRegressor(camera_matrix=camera_matrix,\n                                         pred_size=output_size,\n                                         pred_keypoints=output_keypoints,\n                                         pred_distance=output_distance)\n\n            pred_params = box_regressor.solve()\n            pred_h, pred_w, pred_l, pred_x, pred_y, pred_z, pred_r_y  = pred_params\n            pred_r_y = wrapToPi(pred_r_y)\n\n            score_2d = score_2d.data.cpu().numpy()\n            input_2Dbbox = input_2Dbbox.data.cpu().numpy()\n\n            if img_id not in eval_dict:\n                eval_dict[img_id] = []\n\n            bbox_dict = {}\n            bbox_dict[""pred_center_BboxNet""] = np.array([pred_x, pred_y, pred_z])\n            bbox_dict[""pred_h""] = pred_h\n            bbox_dict[""pred_w""] = pred_w\n            bbox_dict[""pred_l""] = pred_l\n            bbox_dict[""pred_r_y""] = pred_r_y\n            bbox_dict[""input_2Dbbox""] = input_2Dbbox\n            bbox_dict[""score_2d""] = score_2d\n\n            eval_dict[img_id].append(bbox_dict)\n\nwith open(""%s/eval_dict_test.pkl"" % network.model_dir, ""wb"") as file:\n    pickle.dump(eval_dict, file, protocol=2) # (protocol=2 is needed to be able to open this file with python2)\n'"
Image-Only/eval_imgnet_test_seq.py,8,"b'# camera-ready\n\nfrom datasets_imgnet import DatasetImgNetEvalTestSeq, BoxRegressor # (this needs to be imported before torch, because cv2 needs to be imported before torch for some reason)\nfrom datasets_imgnet import wrapToPi\nfrom imgnet import ImgNet\n\nimport torch\nimport torch.utils.data\nimport torch.nn as nn\nfrom torch.autograd import Variable\nimport torch.optim as optim\nimport torch.nn.functional as F\n\nimport numpy as np\nimport pickle\nimport matplotlib\nmatplotlib.use(""Agg"")\nimport matplotlib.pyplot as plt\nimport cv2\n\nbatch_size = 8\n\nnetwork = ImgNet(""Image-Only_eval_test_seq"", project_dir=""/root/3DOD_thesis"")\nnetwork.load_state_dict(torch.load(""/root/3DOD_thesis/pretrained_models/model_10_2_epoch_400.pth""))\nnetwork = network.cuda()\n\nnetwork.eval() # (set in evaluation mode, this affects BatchNorm and dropout)\n\nfor sequence in [""0000"", ""0001"", ""0002"", ""0003"", ""0004"", ""0005"", ""0006"", ""0007"", ""0008"", ""0009"", ""0010"", ""0011"", ""0012"", ""0013"", ""0014"", ""0015"", ""0016"", ""0017"", ""0018"", ""0027""]:\n    print (sequence)\n\n    test_dataset = DatasetImgNetEvalTestSeq(kitti_data_path=""/root/3DOD_thesis/data/kitti"",\n                                            kitti_meta_path=""/root/3DOD_thesis/data/kitti/meta"",\n                                            sequence=sequence)\n\n    test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n                                              batch_size=batch_size, shuffle=False,\n                                              num_workers=4)\n\n    eval_dict = {}\n    for step, (bbox_2d_imgs, img_ids, mean_car_size, ws, hs, u_centers, v_centers, input_2Dbboxes, camera_matrices, mean_distance) in enumerate(test_loader):\n        with torch.no_grad(): # (corresponds to setting volatile=True in all variables, this is done during inference to reduce memory consumption)\n            bbox_2d_imgs = Variable(bbox_2d_imgs) # (shape: (batch_size, 3, H, W) = (batch_size, 3, 224, 224))\n\n            bbox_2d_imgs = bbox_2d_imgs.cuda()\n\n            outputs = network(bbox_2d_imgs) # (shape: (batch_size, 2*8 + 3 = 19))\n            outputs_keypoints = outputs[:, 0:16] # (shape: (batch_size, 2*8))\n            outputs_size = outputs[:, 16:19] # (shape: (batch_size, 3)\n            outputs_distance = outputs[:, 19] # (shape: (batch_size, )\n\n            ############################################################################\n            # save data for visualization:\n            ############################################################################\n            mean_car_size = Variable(mean_car_size).cuda()\n            outputs_size = outputs_size + mean_car_size # NOTE NOTE\n            mean_distance = Variable(mean_distance).cuda()\n            outputs_distance = outputs_distance + mean_distance # NOTE NOTE\n            for i in range(outputs.size()[0]):\n                output_keypoints = outputs_keypoints[i].data.cpu().numpy()\n                output_size = outputs_size[i].data.cpu().numpy()\n                output_distance = outputs_distance[i].data.cpu().numpy()\n                w = ws[i]\n                h = hs[i]\n                u_center = u_centers[i]\n                v_center = v_centers[i]\n                img_id = img_ids[i]\n                camera_matrix = camera_matrices[i]\n                input_2Dbbox = input_2Dbboxes[i]\n\n                output_keypoints = np.resize(output_keypoints, (8, 2))\n                output_keypoints = output_keypoints*np.array([w, h]) + np.array([u_center, v_center])\n\n                box_regressor = BoxRegressor(camera_matrix=camera_matrix,\n                                             pred_size=output_size,\n                                             pred_keypoints=output_keypoints,\n                                             pred_distance=output_distance)\n\n                pred_params = box_regressor.solve()\n                pred_h, pred_w, pred_l, pred_x, pred_y, pred_z, pred_r_y  = pred_params\n                pred_r_y = wrapToPi(pred_r_y)\n\n                input_2Dbbox = input_2Dbbox.data.cpu().numpy()\n\n                if img_id not in eval_dict:\n                    eval_dict[img_id] = []\n\n                bbox_dict = {}\n                bbox_dict[""pred_center_BboxNet""] = np.array([pred_x, pred_y, pred_z])\n                bbox_dict[""pred_h""] = pred_h\n                bbox_dict[""pred_w""] = pred_w\n                bbox_dict[""pred_l""] = pred_l\n                bbox_dict[""pred_r_y""] = pred_r_y\n                bbox_dict[""input_2Dbbox""] = input_2Dbbox\n\n                eval_dict[img_id].append(bbox_dict)\n\n    with open(""%s/eval_dict_test_seq_%s.pkl"" % (network.model_dir, sequence), ""wb"") as file:\n        pickle.dump(eval_dict, file, protocol=2) # (protocol=2 is needed to be able to open this file with python2)\n'"
Image-Only/eval_imgnet_val.py,12,"b'# camera-ready\n\nfrom datasets_imgnet import DatasetImgNetAugmentation, DatasetImgNetEval, BoxRegressor # (this needs to be imported before torch, because cv2 needs to be imported before torch for some reason)\nfrom datasets_imgnet import wrapToPi\nfrom imgnet import ImgNet\n\nimport torch\nimport torch.utils.data\nimport torch.nn as nn\nfrom torch.autograd import Variable\nimport torch.optim as optim\nimport torch.nn.functional as F\n\nimport numpy as np\nimport pickle\nimport matplotlib\nmatplotlib.use(""Agg"")\nimport matplotlib.pyplot as plt\nimport cv2\n\ndef draw_3dbbox_from_keypoints(img, keypoints):\n    img = np.copy(img)\n\n    color = [190, 0, 255] # (BGR)\n    front_color = [255, 230, 0] # (BGR)\n    lines = [[0, 3, 7, 4, 0], [1, 2, 6, 5, 1], [0, 1], [2, 3], [6, 7], [4, 5]] # (0 -> 3 -> 7 -> 4 -> 0, 1 -> 2 -> 6 -> 5 -> 1, etc.)\n    colors = [front_color, color, color, color, color, color]\n\n    for n, line in enumerate(lines):\n        bg = colors[n]\n\n        cv2.polylines(img, np.int32([keypoints[line]]), False, bg, lineType=cv2.LINE_AA, thickness=2)\n\n    return img\n\nbatch_size = 8\n\nnetwork = ImgNet(""Image-Only_eval_val"", project_dir=""/root/3DOD_thesis"")\nnetwork.load_state_dict(torch.load(""/root/3DOD_thesis/pretrained_models/model_10_2_epoch_400.pth""))\nnetwork = network.cuda()\n\nval_dataset = DatasetImgNetEval(kitti_data_path=""/root/3DOD_thesis/data/kitti"",\n                                kitti_meta_path=""/root/3DOD_thesis/data/kitti/meta"",\n                                type=""val"")\n\nnum_val_batches = int(len(val_dataset)/batch_size)\n\nprint (""num_val_batches:"", num_val_batches)\n\nval_loader = torch.utils.data.DataLoader(dataset=val_dataset,\n                                         batch_size=batch_size, shuffle=False,\n                                         num_workers=4)\n\nregression_loss_func = nn.SmoothL1Loss()\n\nnetwork.eval() # (set in evaluation mode, this affects BatchNorm and dropout)\nbatch_losses = []\nbatch_losses_size = []\nbatch_losses_keypoints = []\nbatch_losses_distance = []\nbatch_losses_3d_center = []\nbatch_losses_3d_size = []\nbatch_losses_3d_r_y = []\nbatch_losses_3d_distance = []\neval_dict = {}\nfor step, (bbox_2d_imgs, labels_size, labels_keypoints, labels_distance, img_ids, mean_car_size, ws, hs, u_centers, v_centers, camera_matrices, labels_center, labels_r_y, mean_distance) in enumerate(val_loader):\n    with torch.no_grad(): # (corresponds to setting volatile=True in all variables, this is done during inference to reduce memory consumption)\n        if step % 100 == 0:\n            print (""step %d/%d"" % (step+1, num_val_batches))\n\n        bbox_2d_imgs = Variable(bbox_2d_imgs) # (shape: (batch_size, 3, H, W) = (batch_size, 3, 224, 224))\n        labels_size = Variable(labels_size) # (shape: (batch_size, 3))\n        labels_keypoints = Variable(labels_keypoints) # (shape: (batch_size, 2*8))\n        labels_distance = Variable(labels_distance) # (shape: (batch_size, 1))\n        labels_distance = labels_distance.view(-1) # (shape: (batch_size, ))\n\n        bbox_2d_imgs = bbox_2d_imgs.cuda()\n        labels_size = labels_size.cuda()\n        labels_keypoints = labels_keypoints.cuda()\n        labels_distance = labels_distance.cuda()\n\n        outputs = network(bbox_2d_imgs) # (shape: (batch_size, 2*8 + 3 = 19))\n        outputs_keypoints = outputs[:, 0:16] # (shape: (batch_size, 2*8))\n        outputs_size = outputs[:, 16:19] # (shape: (batch_size, 3)\n        outputs_distance = outputs[:, 19] # (shape: (batch_size, )\n\n        ########################################################################\n        # compute the size loss:\n        ########################################################################\n        loss_size = regression_loss_func(outputs_size, labels_size)\n\n        loss_size_value = loss_size.data.cpu().numpy()\n        batch_losses_size.append(loss_size_value)\n\n        ########################################################################\n        # compute the keypoints loss:\n        ########################################################################\n        loss_keypoints = regression_loss_func(outputs_keypoints, labels_keypoints)\n\n        loss_keypoints_value = loss_keypoints.data.cpu().numpy()\n        batch_losses_keypoints.append(loss_keypoints_value)\n\n        ########################################################################\n        # compute the distance loss:\n        ########################################################################\n        loss_distance = regression_loss_func(outputs_distance, labels_distance)\n\n        loss_distance_value = loss_distance.data.cpu().numpy()\n        batch_losses_distance.append(loss_distance_value)\n\n        ########################################################################\n        # compute the total loss:\n        ########################################################################\n        loss = loss_size + 10*loss_keypoints + 0.01*loss_distance\n        loss_value = loss.data.cpu().numpy()\n        batch_losses.append(loss_value)\n\n        ########################################################################\n        mean_car_size = Variable(mean_car_size).cuda()\n        mean_distance = Variable(mean_distance).cuda()\n        outputs_size = outputs_size + mean_car_size # NOTE NOTE\n        labels_size = labels_size + mean_car_size # NOTE NOTE\n        outputs_distance = outputs_distance + mean_distance # NOTE NOTE\n        preds_3d_size = torch.zeros((outputs.size()[0], 3))\n        preds_3d_center = torch.zeros((outputs.size()[0], 3))\n        preds_3d_r_y = torch.zeros((outputs.size()[0], ))\n        preds_3d_distance = torch.zeros((outputs.size()[0], ))\n        for i in range(outputs.size()[0]):\n            output_keypoints = outputs_keypoints[i].data.cpu().numpy()\n            output_size = outputs_size[i].data.cpu().numpy()\n            output_distance = outputs_distance[i].data.cpu().numpy()\n            w = ws[i]\n            h = hs[i]\n            u_center = u_centers[i]\n            v_center = v_centers[i]\n            img_id = img_ids[i]\n            camera_matrix = camera_matrices[i]\n            gt_center = labels_center[i]\n            gt_size = labels_size[i]\n            gt_r_y = labels_r_y[i]\n\n            #if img_id in [""000006"", ""000007"", ""000008"", ""000009"", ""000010"", ""000011"", ""000012"", ""000013"", ""000014"", ""000015"", ""000016"", ""000017"", ""000018"", ""000019"", ""000020"", ""000021""]:\n            output_keypoints = np.resize(output_keypoints, (8, 2))\n            output_keypoints = output_keypoints*np.array([w, h]) + np.array([u_center, v_center])\n\n            box_regressor = BoxRegressor(camera_matrix=camera_matrix,\n                                         pred_size=output_size,\n                                         pred_keypoints=output_keypoints,\n                                         pred_distance=output_distance)\n\n            pred_params = box_regressor.solve()\n            pred_h, pred_w, pred_l, pred_x, pred_y, pred_z, pred_r_y  = pred_params\n            pred_r_y = wrapToPi(pred_r_y)\n\n            preds_3d_size[i, 0] = pred_h\n            preds_3d_size[i, 1] = pred_w\n            preds_3d_size[i, 2] = pred_l\n\n            preds_3d_center[i, 0] = pred_x\n            preds_3d_center[i, 1] = pred_y\n            preds_3d_center[i, 2] = pred_z\n\n            preds_3d_r_y[i] = pred_r_y\n\n            preds_3d_distance[i] = np.linalg.norm(np.array([pred_x, pred_y, pred_z]))\n\n            gt_r_y = gt_r_y.data.cpu().numpy()\n\n            if img_id not in eval_dict:\n                eval_dict[img_id] = []\n\n            bbox_dict = {}\n            bbox_dict[""pred_center_BboxNet""] = np.array([pred_x, pred_y, pred_z])\n            bbox_dict[""pred_h""] = pred_h\n            bbox_dict[""pred_w""] = pred_w\n            bbox_dict[""pred_l""] = pred_l\n            bbox_dict[""pred_r_y""] = pred_r_y\n            bbox_dict[""gt_center""] = gt_center.numpy()\n            bbox_dict[""gt_h""] = gt_size[0].data.cpu().numpy()\n            bbox_dict[""gt_w""] = gt_size[1].data.cpu().numpy()\n            bbox_dict[""gt_l""] = gt_size[2].data.cpu().numpy()\n            bbox_dict[""gt_r_y""] = gt_r_y\n\n            eval_dict[img_id].append(bbox_dict)\n\n        preds_3d_size = Variable(preds_3d_size).cuda()\n        preds_3d_size = preds_3d_size - mean_car_size # NOTE NOTE\n\n        labels_size = labels_size - mean_car_size # NOTE NOTE\n\n        preds_3d_distance = Variable(preds_3d_distance).cuda()\n        preds_3d_distance = preds_3d_distance - mean_distance # NOTE NOTE\n\n        preds_3d_center = Variable(preds_3d_center).cuda()\n        preds_3d_r_y = Variable(preds_3d_r_y).cuda()\n\n        labels_center = Variable(labels_center).cuda()\n        labels_r_y = Variable(labels_r_y).cuda()\n\n        loss_3d_size = regression_loss_func(preds_3d_size, labels_size)\n        loss_3d_size_value = loss_3d_size.data.cpu().numpy()\n        batch_losses_3d_size.append(loss_3d_size_value)\n\n        loss_3d_center = regression_loss_func(preds_3d_center, labels_center)\n        loss_3d_center_value = loss_3d_center.data.cpu().numpy()\n        batch_losses_3d_center.append(loss_3d_center_value)\n\n        loss_3d_r_y = regression_loss_func(preds_3d_r_y, labels_r_y)\n        loss_3d_r_y_value = loss_3d_r_y.data.cpu().numpy()\n        batch_losses_3d_r_y.append(loss_3d_r_y_value)\n\n        loss_3d_distance = regression_loss_func(preds_3d_distance, labels_distance)\n        loss_3d_distance_value = loss_3d_distance.data.cpu().numpy()\n        batch_losses_3d_distance.append(loss_3d_distance_value)\n\nepoch_loss = np.mean(batch_losses)\nprint (""val loss: %g"" % epoch_loss)\n\nepoch_loss = np.mean(batch_losses_size)\nprint (""val size loss: %g"" % epoch_loss)\n\nepoch_loss = np.mean(batch_losses_keypoints)\nprint (""val keypoints loss: %g"" % epoch_loss)\n\nepoch_loss = np.mean(batch_losses_distance)\nprint (""val distance loss: %g"" % epoch_loss)\n\nepoch_loss = np.mean(batch_losses_3d_size)\nprint (""val 3d size loss: %g"" % epoch_loss)\n\nepoch_loss = np.mean(batch_losses_3d_center)\nprint (""val 3d center loss: %g"" % epoch_loss)\n\nepoch_loss = np.mean(batch_losses_3d_r_y)\nprint (""val 3d r_y loss: %g"" % epoch_loss)\n\nepoch_loss = np.mean(batch_losses_3d_distance)\nprint (""val 3d distance loss: %g"" % epoch_loss)\n\nwith open(""%s/eval_dict_val.pkl"" % network.model_dir, ""wb"") as file:\n    pickle.dump(eval_dict, file, protocol=2) # (protocol=2 is needed to be able to open this file with python2)\n'"
Image-Only/eval_imgnet_val_2ddetections.py,8,"b'# camera-ready\n\nfrom datasets_imgnet import DatasetImgNetVal2ddetections, BoxRegressor # (this needs to be imported before torch, because cv2 needs to be imported before torch for some reason)\nfrom datasets_imgnet import wrapToPi\nfrom imgnet import ImgNet\n\nimport torch\nimport torch.utils.data\nimport torch.nn as nn\nfrom torch.autograd import Variable\nimport torch.optim as optim\nimport torch.nn.functional as F\n\nimport numpy as np\nimport pickle\nimport matplotlib\nmatplotlib.use(""Agg"")\nimport matplotlib.pyplot as plt\nimport cv2\n\nbatch_size = 8\n\nnetwork = ImgNet(""Image-Only_eval_val_2ddetections"", project_dir=""/root/3DOD_thesis"")\nnetwork.load_state_dict(torch.load(""/root/3DOD_thesis/pretrained_models/model_10_2_epoch_400.pth""))\nnetwork = network.cuda()\n\nnetwork.eval() # (set in evaluation mode, this affects BatchNorm and dropout)\n\nval_dataset = DatasetImgNetVal2ddetections(kitti_data_path=""/root/3DOD_thesis/data/kitti"",\n                                           kitti_meta_path=""/root/3DOD_thesis/data/kitti/meta"")\n\nnum_val_batches = int(len(val_dataset)/batch_size)\n\nval_loader = torch.utils.data.DataLoader(dataset=val_dataset,\n                                         batch_size=batch_size, shuffle=False,\n                                         num_workers=4)\n\neval_dict = {}\nfor step, (bbox_2d_imgs, img_ids, mean_car_size, ws, hs, u_centers, v_centers, input_2Dbboxes, camera_matrices, mean_distance, scores_2d) in enumerate(val_loader):\n    if step % 100 == 0:\n        print (""step: %d/%d"" % (step+1, num_val_batches))\n\n    with torch.no_grad(): # (corresponds to setting volatile=True in all variables, this is done during inference to reduce memory consumption)\n        bbox_2d_imgs = Variable(bbox_2d_imgs) # (shape: (batch_size, 3, H, W) = (batch_size, 3, 224, 224))\n\n        bbox_2d_imgs = bbox_2d_imgs.cuda()\n\n        outputs = network(bbox_2d_imgs) # (shape: (batch_size, 2*8 + 3 = 19))\n        outputs_keypoints = outputs[:, 0:16] # (shape: (batch_size, 2*8))\n        outputs_size = outputs[:, 16:19] # (shape: (batch_size, 3)\n        outputs_distance = outputs[:, 19] # (shape: (batch_size, )\n\n        ############################################################################\n        # save data for visualization:\n        ############################################################################\n        mean_car_size = Variable(mean_car_size).cuda()\n        outputs_size = outputs_size + mean_car_size # NOTE NOTE\n        mean_distance = Variable(mean_distance).cuda()\n        outputs_distance = outputs_distance + mean_distance # NOTE NOTE\n        for i in range(outputs.size()[0]):\n            output_keypoints = outputs_keypoints[i].data.cpu().numpy()\n            output_size = outputs_size[i].data.cpu().numpy()\n            output_distance = outputs_distance[i].data.cpu().numpy()\n            w = ws[i]\n            h = hs[i]\n            u_center = u_centers[i]\n            v_center = v_centers[i]\n            img_id = img_ids[i]\n            camera_matrix = camera_matrices[i]\n            input_2Dbbox = input_2Dbboxes[i]\n            score_2d = scores_2d[i]\n\n            output_keypoints = np.resize(output_keypoints, (8, 2))\n            output_keypoints = output_keypoints*np.array([w, h]) + np.array([u_center, v_center])\n\n            box_regressor = BoxRegressor(camera_matrix=camera_matrix,\n                                         pred_size=output_size,\n                                         pred_keypoints=output_keypoints,\n                                         pred_distance=output_distance)\n\n            pred_params = box_regressor.solve()\n            pred_h, pred_w, pred_l, pred_x, pred_y, pred_z, pred_r_y  = pred_params\n            pred_r_y = wrapToPi(pred_r_y)\n\n            input_2Dbbox = input_2Dbbox.data.cpu().numpy()\n            score_2d = score_2d.cpu().numpy()\n\n            if img_id not in eval_dict:\n                eval_dict[img_id] = []\n\n            bbox_dict = {}\n            bbox_dict[""pred_center_BboxNet""] = np.array([pred_x, pred_y, pred_z])\n            bbox_dict[""pred_h""] = pred_h\n            bbox_dict[""pred_w""] = pred_w\n            bbox_dict[""pred_l""] = pred_l\n            bbox_dict[""pred_r_y""] = pred_r_y\n            bbox_dict[""input_2Dbbox""] = input_2Dbbox\n            bbox_dict[""score_2d""] = score_2d\n\n            eval_dict[img_id].append(bbox_dict)\n\nwith open(""%s/eval_dict_val_2ddetections.pkl"" % network.model_dir, ""wb"") as file:\n    pickle.dump(eval_dict, file, protocol=2) # (protocol=2 is needed to be able to open this file with python2)\n'"
Image-Only/eval_imgnet_val_seq.py,12,"b'# camera-ready\n\nfrom datasets_imgnet import DatasetImgNetEvalValSeq, BoxRegressor # (this needs to be imported before torch, because cv2 needs to be imported before torch for some reason)\nfrom datasets_imgnet import wrapToPi\nfrom imgnet import ImgNet\n\nimport torch\nimport torch.utils.data\nimport torch.nn as nn\nfrom torch.autograd import Variable\nimport torch.optim as optim\nimport torch.nn.functional as F\n\nimport numpy as np\nimport pickle\nimport matplotlib\nmatplotlib.use(""Agg"")\nimport matplotlib.pyplot as plt\nimport cv2\n\nsequence = ""0004""\n\nbatch_size = 8\n\nnetwork = ImgNet(""Image-Only_eval_val_seq"", project_dir=""/root/3DOD_thesis"")\nnetwork.load_state_dict(torch.load(""/root/3DOD_thesis/pretrained_models/model_10_2_epoch_400.pth""))\nnetwork = network.cuda()\n\nval_dataset = DatasetImgNetEvalValSeq(kitti_data_path=""/root/3DOD_thesis/data/kitti"",\n                                     kitti_meta_path=""/root/3DOD_thesis/data/kitti/meta"",\n                                     sequence=sequence)\n\nnum_val_batches = int(len(val_dataset)/batch_size)\n\nprint (""num_val_batches:"", num_val_batches)\n\nval_loader = torch.utils.data.DataLoader(dataset=val_dataset,\n                                         batch_size=batch_size, shuffle=False,\n                                         num_workers=4)\n\nregression_loss_func = nn.SmoothL1Loss()\n\nnetwork.eval() # (set in evaluation mode, this affects BatchNorm and dropout)\nbatch_losses = []\nbatch_losses_size = []\nbatch_losses_keypoints = []\nbatch_losses_distance = []\nbatch_losses_3d_center = []\nbatch_losses_3d_size = []\nbatch_losses_3d_r_y = []\nbatch_losses_3d_distance = []\neval_dict = {}\nfor step, (bbox_2d_imgs, labels_size, labels_keypoints, labels_distance, img_ids, mean_car_size, ws, hs, u_centers, v_centers, camera_matrices, labels_center, labels_r_y, mean_distance) in enumerate(val_loader):\n    with torch.no_grad(): # (corresponds to setting volatile=True in all variables, this is done during inference to reduce memory consumption)\n        print (""step %d/%d"" % (step+1, num_val_batches))\n\n        bbox_2d_imgs = Variable(bbox_2d_imgs) # (shape: (batch_size, 3, H, W) = (batch_size, 3, 224, 224))\n        labels_size = Variable(labels_size) # (shape: (batch_size, 3))\n        labels_keypoints = Variable(labels_keypoints) # (shape: (batch_size, 2*8))\n        labels_distance = Variable(labels_distance) # (shape: (batch_size, 1))\n        labels_distance = labels_distance.view(-1) # (shape: (batch_size, ))\n\n        bbox_2d_imgs = bbox_2d_imgs.cuda()\n        labels_size = labels_size.cuda()\n        labels_keypoints = labels_keypoints.cuda()\n        labels_distance = labels_distance.cuda()\n\n        outputs = network(bbox_2d_imgs) # (shape: (batch_size, 2*8 + 3 = 19))\n        outputs_keypoints = outputs[:, 0:16] # (shape: (batch_size, 2*8))\n        outputs_size = outputs[:, 16:19] # (shape: (batch_size, 3)\n        outputs_distance = outputs[:, 19] # (shape: (batch_size, )\n\n        ########################################################################\n        # compute the size loss:\n        ########################################################################\n        loss_size = regression_loss_func(outputs_size, labels_size)\n\n        loss_size_value = loss_size.data.cpu().numpy()\n        batch_losses_size.append(loss_size_value)\n\n        ########################################################################\n        # compute the keypoints loss:\n        ########################################################################\n        loss_keypoints = regression_loss_func(outputs_keypoints, labels_keypoints)\n\n        loss_keypoints_value = loss_keypoints.data.cpu().numpy()\n        batch_losses_keypoints.append(loss_keypoints_value)\n\n        ########################################################################\n        # compute the distance loss:\n        ########################################################################\n        loss_distance = regression_loss_func(outputs_distance, labels_distance)\n\n        loss_distance_value = loss_distance.data.cpu().numpy()\n        batch_losses_distance.append(loss_distance_value)\n\n        ########################################################################\n        # compute the total loss:\n        ########################################################################\n        loss = loss_size + 10*loss_keypoints + 0.01*loss_distance\n        loss_value = loss.data.cpu().numpy()\n        batch_losses.append(loss_value)\n\n        ########################################################################\n        mean_car_size = Variable(mean_car_size).cuda()\n        mean_distance = Variable(mean_distance).cuda()\n        outputs_size = outputs_size + mean_car_size # NOTE NOTE\n        labels_size = labels_size + mean_car_size # NOTE NOTE\n        outputs_distance = outputs_distance + mean_distance # NOTE NOTE\n        preds_3d_size = torch.zeros((outputs.size()[0], 3))\n        preds_3d_center = torch.zeros((outputs.size()[0], 3))\n        preds_3d_r_y = torch.zeros((outputs.size()[0], ))\n        preds_3d_distance = torch.zeros((outputs.size()[0], ))\n        for i in range(outputs.size()[0]):\n            output_keypoints = outputs_keypoints[i].data.cpu().numpy()\n            output_size = outputs_size[i].data.cpu().numpy()\n            output_distance = outputs_distance[i].data.cpu().numpy()\n            w = ws[i]\n            h = hs[i]\n            u_center = u_centers[i]\n            v_center = v_centers[i]\n            img_id = img_ids[i]\n            camera_matrix = camera_matrices[i]\n            gt_center = labels_center[i]\n            gt_size = labels_size[i]\n            gt_r_y = labels_r_y[i]\n\n            #if img_id in [""000006"", ""000007"", ""000008"", ""000009"", ""000010"", ""000011"", ""000012"", ""000013"", ""000014"", ""000015"", ""000016"", ""000017"", ""000018"", ""000019"", ""000020"", ""000021""]:\n            output_keypoints = np.resize(output_keypoints, (8, 2))\n            output_keypoints = output_keypoints*np.array([w, h]) + np.array([u_center, v_center])\n\n            box_regressor = BoxRegressor(camera_matrix=camera_matrix,\n                                         pred_size=output_size,\n                                         pred_keypoints=output_keypoints,\n                                         pred_distance=output_distance)\n\n            pred_params = box_regressor.solve()\n            pred_h, pred_w, pred_l, pred_x, pred_y, pred_z, pred_r_y  = pred_params\n            pred_r_y = wrapToPi(pred_r_y)\n\n            preds_3d_size[i, 0] = pred_h\n            preds_3d_size[i, 1] = pred_w\n            preds_3d_size[i, 2] = pred_l\n\n            preds_3d_center[i, 0] = pred_x\n            preds_3d_center[i, 1] = pred_y\n            preds_3d_center[i, 2] = pred_z\n\n            preds_3d_r_y[i] = pred_r_y\n\n            preds_3d_distance[i] = np.linalg.norm(np.array([pred_x, pred_y, pred_z]))\n\n            gt_r_y = gt_r_y.data.cpu().numpy()\n\n            if img_id not in eval_dict:\n                eval_dict[img_id] = []\n\n            bbox_dict = {}\n            bbox_dict[""pred_center_BboxNet""] = np.array([pred_x, pred_y, pred_z])\n            bbox_dict[""pred_h""] = pred_h\n            bbox_dict[""pred_w""] = pred_w\n            bbox_dict[""pred_l""] = pred_l\n            bbox_dict[""pred_r_y""] = pred_r_y\n            bbox_dict[""gt_center""] = gt_center.numpy()\n            bbox_dict[""gt_h""] = gt_size[0].data.cpu().numpy()\n            bbox_dict[""gt_w""] = gt_size[1].data.cpu().numpy()\n            bbox_dict[""gt_l""] = gt_size[2].data.cpu().numpy()\n            bbox_dict[""gt_r_y""] = gt_r_y\n\n            eval_dict[img_id].append(bbox_dict)\n\n        preds_3d_size = Variable(preds_3d_size).cuda()\n        preds_3d_size = preds_3d_size - mean_car_size # NOTE NOTE\n\n        labels_size = labels_size - mean_car_size # NOTE NOTE\n\n        preds_3d_distance = Variable(preds_3d_distance).cuda()\n        preds_3d_distance = preds_3d_distance - mean_distance # NOTE NOTE\n\n        preds_3d_center = Variable(preds_3d_center).cuda()\n        preds_3d_r_y = Variable(preds_3d_r_y).cuda()\n\n        labels_center = Variable(labels_center).cuda()\n        labels_r_y = Variable(labels_r_y).cuda()\n\n        loss_3d_size = regression_loss_func(preds_3d_size, labels_size)\n        loss_3d_size_value = loss_3d_size.data.cpu().numpy()\n        batch_losses_3d_size.append(loss_3d_size_value)\n\n        loss_3d_center = regression_loss_func(preds_3d_center, labels_center)\n        loss_3d_center_value = loss_3d_center.data.cpu().numpy()\n        batch_losses_3d_center.append(loss_3d_center_value)\n\n        loss_3d_r_y = regression_loss_func(preds_3d_r_y, labels_r_y)\n        loss_3d_r_y_value = loss_3d_r_y.data.cpu().numpy()\n        batch_losses_3d_r_y.append(loss_3d_r_y_value)\n\n        loss_3d_distance = regression_loss_func(preds_3d_distance, labels_distance)\n        loss_3d_distance_value = loss_3d_distance.data.cpu().numpy()\n        batch_losses_3d_distance.append(loss_3d_distance_value)\n\nepoch_loss = np.mean(batch_losses)\nprint (""val loss: %g"" % epoch_loss)\n\nepoch_loss = np.mean(batch_losses_size)\nprint (""val size loss: %g"" % epoch_loss)\n\nepoch_loss = np.mean(batch_losses_keypoints)\nprint (""val keypoints loss: %g"" % epoch_loss)\n\nepoch_loss = np.mean(batch_losses_distance)\nprint (""val distance loss: %g"" % epoch_loss)\n\nepoch_loss = np.mean(batch_losses_3d_size)\nprint (""val 3d size loss: %g"" % epoch_loss)\n\nepoch_loss = np.mean(batch_losses_3d_center)\nprint (""val 3d center loss: %g"" % epoch_loss)\n\nepoch_loss = np.mean(batch_losses_3d_r_y)\nprint (""val 3d r_y loss: %g"" % epoch_loss)\n\nepoch_loss = np.mean(batch_losses_3d_distance)\nprint (""val 3d distance loss: %g"" % epoch_loss)\n\nwith open(""%s/eval_dict_val_seq_%s.pkl"" % (network.model_dir, sequence), ""wb"") as file:\n    pickle.dump(eval_dict, file, protocol=2) # (protocol=2 is needed to be able to open this file with python2)\n'"
Image-Only/imgnet.py,5,"b'# camera-ready\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nimport torchvision.models as models\n\nimport os\n\nclass ImgNet(nn.Module):\n    def __init__(self, model_id, project_dir):\n        super(ImgNet, self).__init__()\n\n        self.model_id = model_id\n        self.project_dir = project_dir\n        self.create_model_dirs()\n\n        resnet34 = models.resnet34()\n        # load pretrained model:\n        resnet34.load_state_dict(torch.load(""/root/3DOD_thesis/pretrained_models/resnet/resnet34-333f7ec4.pth""))\n        # remove fully connected layer:\n        self.resnet34 = nn.Sequential(*list(resnet34.children())[:-2])\n\n        self.avg_pool = nn.AvgPool2d(kernel_size=7)\n\n        self.fc1 = nn.Linear(512, 256)\n        self.fc2 = nn.Linear(256, 128)\n        self.fc3 = nn.Linear(128, 2*8 + 3 + 1)\n\n    def forward(self, img):\n        # (img has shape (batch_size, 3, H, W))\n\n        out = self.resnet34(img) # (shape: (batch_size, 512, 7, 7))\n        out = self.avg_pool(out) # (shape: (batch_size, 512, 1, 1))\n        out = out.view(-1, 512) # (shape: (batch_size, 512))\n\n        out = F.relu(self.fc1(out)) # (shape: (batch_size, 256))\n        out = F.relu(self.fc2(out)) # (shape: (batch_size, 128)))\n\n        out = self.fc3(out) # (shape: (batch_size, 2*8 + 3 + 1 = 20))\n\n        return out\n\n    def create_model_dirs(self):\n        self.logs_dir = self.project_dir + ""/training_logs""\n        self.model_dir = self.logs_dir + ""/model_%s"" % self.model_id\n        self.checkpoints_dir = self.model_dir + ""/checkpoints""\n        if not os.path.exists(self.logs_dir):\n            os.makedirs(self.logs_dir)\n        if not os.path.exists(self.model_dir):\n            os.makedirs(self.model_dir)\n            os.makedirs(self.checkpoints_dir)\n\n# x = Variable(torch.randn(32, 3, 224, 224)).cuda()\n# network = ImgNet(""ImgNet_test"", ""/staging/frexgus/imgnet"")\n# network = network.cuda()\n# out = network(x)\n'"
Image-Only/train_imgnet.py,11,"b'# camera-ready\n\nfrom datasets_imgnet import DatasetImgNetAugmentation, DatasetImgNetEval # (this needs to be imported before torch, because cv2 needs to be imported before torch for some reason)\nfrom imgnet import ImgNet\n\nimport torch\nimport torch.utils.data\nimport torch.nn as nn\nfrom torch.autograd import Variable\nimport torch.optim as optim\nimport torch.nn.functional as F\n\nimport numpy as np\nimport pickle\nimport matplotlib\nmatplotlib.use(""Agg"")\nimport matplotlib.pyplot as plt\nimport cv2\n\ndef draw_3dbbox_from_keypoints(img, keypoints):\n    img = np.copy(img)\n\n    color = [190, 0, 255] # (BGR)\n    front_color = [255, 230, 0] # (BGR)\n    lines = [[0, 3, 7, 4, 0], [1, 2, 6, 5, 1], [0, 1], [2, 3], [6, 7], [4, 5]] # (0 -> 3 -> 7 -> 4 -> 0, 1 -> 2 -> 6 -> 5 -> 1, etc.)\n    colors = [front_color, color, color, color, color, color]\n\n    for n, line in enumerate(lines):\n        bg = colors[n]\n\n        cv2.polylines(img, np.int32([keypoints[line]]), False, bg, lineType=cv2.LINE_AA, thickness=2)\n\n    return img\n\n# NOTE! NOTE! change this to not overwrite all log data when you train the model:\nmodel_id = ""Image-Only_1""\n\nnum_epochs = 1000\nbatch_size = 8\nlearning_rate = 0.001\n\nnetwork = ImgNet(model_id, project_dir=""/root/3DOD_thesis"")\nnetwork = network.cuda()\n\ntrain_dataset = DatasetImgNetAugmentation(kitti_data_path=""/root/3DOD_thesis/data/kitti"",\n                                          kitti_meta_path=""/root/3DOD_thesis/data/kitti/meta"",\n                                          type=""train"")\nval_dataset = DatasetImgNetEval(kitti_data_path=""/root/3DOD_thesis/data/kitti"",\n                                kitti_meta_path=""/root/3DOD_thesis/data/kitti/meta"",\n                                type=""val"")\n\nnum_train_batches = int(len(train_dataset)/batch_size)\nnum_val_batches = int(len(val_dataset)/batch_size)\n\nprint (""num_train_batches:"", num_train_batches)\nprint (""num_val_batches:"", num_val_batches)\n\ntrain_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n                                           batch_size=batch_size, shuffle=True,\n                                           num_workers=4)\nval_loader = torch.utils.data.DataLoader(dataset=val_dataset,\n                                         batch_size=batch_size, shuffle=False,\n                                         num_workers=4)\n\nregression_loss_func = nn.SmoothL1Loss()\n\noptimizer = torch.optim.Adam(network.parameters(), lr=learning_rate)\n\nepoch_losses_train = []\nepoch_losses_size_train = []\nepoch_losses_keypoints_train = []\nepoch_losses_distance_train = []\nepoch_losses_val = []\nepoch_losses_size_val = []\nepoch_losses_keypoints_val = []\nepoch_losses_distance_val = []\nfor epoch in range(num_epochs):\n    print (""###########################"")\n    print (""######## NEW EPOCH ########"")\n    print (""###########################"")\n    print (""epoch: %d/%d"" % (epoch+1, num_epochs))\n\n    if epoch % 100 == 0 and epoch > 0:\n        learning_rate = learning_rate/2\n        optimizer = torch.optim.Adam(network.parameters(), lr=learning_rate)\n\n    print (""learning_rate:"")\n    print (learning_rate)\n\n    ################################################################################\n    # train:\n    ################################################################################\n    network.train() # (set in training mode, this affects BatchNorm and dropout)\n    batch_losses = []\n    batch_losses_size = []\n    batch_losses_keypoints = []\n    batch_losses_distance = []\n    for step, (bbox_2d_imgs, labels_size, labels_keypoints, labels_distance) in enumerate(train_loader):\n        bbox_2d_imgs = Variable(bbox_2d_imgs) # (shape: (batch_size, 3, H, W) = (batch_size, 3, 224, 224))\n        labels_size = Variable(labels_size) # (shape: (batch_size, 3))\n        labels_keypoints = Variable(labels_keypoints) # (shape: (batch_size, 2*8))\n        labels_distance = Variable(labels_distance) # (shape: (batch_size, 1))\n        labels_distance = labels_distance.view(-1) # (shape: (batch_size, ))\n\n        bbox_2d_imgs = bbox_2d_imgs.cuda()\n        labels_size = labels_size.cuda()\n        labels_keypoints = labels_keypoints.cuda()\n        labels_distance = labels_distance.cuda()\n\n        outputs = network(bbox_2d_imgs) # (shape: (batch_size, 2*8 + 3 +1 = 20))\n        outputs_keypoints = outputs[:, 0:16] # (shape: (batch_size, 2*8))\n        outputs_size = outputs[:, 16:19] # (shape: (batch_size, 3)\n        outputs_distance = outputs[:, 19] # (shape: (batch_size, )\n\n        ########################################################################\n        # compute the size loss:\n        ########################################################################\n        loss_size = regression_loss_func(outputs_size, labels_size)\n\n        loss_size_value = loss_size.data.cpu().numpy()\n        batch_losses_size.append(loss_size_value)\n\n        ########################################################################\n        # compute the keypoints loss:\n        ########################################################################\n        loss_keypoints = regression_loss_func(outputs_keypoints, labels_keypoints)\n\n        loss_keypoints_value = loss_keypoints.data.cpu().numpy()\n        batch_losses_keypoints.append(loss_keypoints_value)\n\n        ########################################################################\n        # compute the distance loss:\n        ########################################################################\n        loss_distance = regression_loss_func(outputs_distance, labels_distance)\n\n        loss_distance_value = loss_distance.data.cpu().numpy()\n        batch_losses_distance.append(loss_distance_value)\n\n        ########################################################################\n        # compute the total loss:\n        ########################################################################\n        loss = loss_size + 10*loss_keypoints + 0.01*loss_distance\n        loss_value = loss.data.cpu().numpy()\n        batch_losses.append(loss_value)\n\n        ########################################################################\n        # optimization step:\n        ########################################################################\n        optimizer.zero_grad() # (reset gradients)\n        loss.backward() # (compute gradients)\n        optimizer.step() # (perform optimization step)\n\n    epoch_loss = np.mean(batch_losses)\n    epoch_losses_train.append(epoch_loss)\n    with open(""%s/epoch_losses_train.pkl"" % network.model_dir, ""wb"") as file:\n        pickle.dump(epoch_losses_train, file)\n    print (""train loss: %g"" % epoch_loss)\n    plt.figure(1)\n    plt.plot(epoch_losses_train, ""k^"")\n    plt.plot(epoch_losses_train, ""k"")\n    plt.ylabel(""loss"")\n    plt.xlabel(""epoch"")\n    plt.title(""train loss per epoch"")\n    plt.savefig(""%s/epoch_losses_train.png"" % network.model_dir)\n    plt.close(1)\n\n    epoch_loss = np.mean(batch_losses_size)\n    epoch_losses_size_train.append(epoch_loss)\n    with open(""%s/epoch_losses_size_train.pkl"" % network.model_dir, ""wb"") as file:\n        pickle.dump(epoch_losses_size_train, file)\n    print (""train size loss: %g"" % epoch_loss)\n    plt.figure(1)\n    plt.plot(epoch_losses_size_train, ""k^"")\n    plt.plot(epoch_losses_size_train, ""k"")\n    plt.ylabel(""loss"")\n    plt.xlabel(""epoch"")\n    plt.title(""train size loss per epoch"")\n    plt.savefig(""%s/epoch_losses_size_train.png"" % network.model_dir)\n    plt.close(1)\n\n    epoch_loss = np.mean(batch_losses_keypoints)\n    epoch_losses_keypoints_train.append(epoch_loss)\n    with open(""%s/epoch_losses_keypoints_train.pkl"" % network.model_dir, ""wb"") as file:\n        pickle.dump(epoch_losses_keypoints_train, file)\n    print (""train keypoints loss: %g"" % epoch_loss)\n    plt.figure(1)\n    plt.plot(epoch_losses_keypoints_train, ""k^"")\n    plt.plot(epoch_losses_keypoints_train, ""k"")\n    plt.ylabel(""loss"")\n    plt.xlabel(""epoch"")\n    plt.title(""train keypoints loss per epoch"")\n    plt.savefig(""%s/epoch_losses_keypoints_train.png"" % network.model_dir)\n    plt.close(1)\n\n    epoch_loss = np.mean(batch_losses_distance)\n    epoch_losses_distance_train.append(epoch_loss)\n    with open(""%s/epoch_losses_distance_train.pkl"" % network.model_dir, ""wb"") as file:\n        pickle.dump(epoch_losses_distance_train, file)\n    print (""train distance loss: %g"" % epoch_loss)\n    plt.figure(1)\n    plt.plot(epoch_losses_distance_train, ""k^"")\n    plt.plot(epoch_losses_distance_train, ""k"")\n    plt.ylabel(""loss"")\n    plt.xlabel(""epoch"")\n    plt.title(""train distance loss per epoch"")\n    plt.savefig(""%s/epoch_losses_distance_train.png"" % network.model_dir)\n    plt.close(1)\n\n    print (""####"")\n\n    ################################################################################\n    # val:\n    ################################################################################\n    network.eval() # (set in evaluation mode, this affects BatchNorm and dropout)\n    batch_losses = []\n    batch_losses_size = []\n    batch_losses_keypoints = []\n    batch_losses_distance = []\n    for step, (bbox_2d_imgs, labels_size, labels_keypoints, labels_distance, img_ids, mean_car_size, ws, hs, u_centers, v_centers, camera_matrices, gt_centers, gt_r_ys, mean_distance) in enumerate(val_loader):\n        with torch.no_grad(): # (corresponds to setting volatile=True in all variables, this is done during inference to reduce memory consumption)\n            bbox_2d_imgs = Variable(bbox_2d_imgs) # (shape: (batch_size, 3, H, W) = (batch_size, 3, 224, 224))\n            labels_size = Variable(labels_size) # (shape: (batch_size, 3))\n            labels_keypoints = Variable(labels_keypoints) # (shape: (batch_size, 2*8))\n            labels_distance = Variable(labels_distance) # (shape: (batch_size, 1))\n            labels_distance = labels_distance.view(-1) # (shape: (batch_size, ))\n\n            bbox_2d_imgs = bbox_2d_imgs.cuda()\n            labels_size = labels_size.cuda()\n            labels_keypoints = labels_keypoints.cuda()\n            labels_distance = labels_distance.cuda()\n\n            outputs = network(bbox_2d_imgs) # (shape: (batch_size, 2*8 + 3 + 1 = 20))\n            outputs_keypoints = outputs[:, 0:16] # (shape: (batch_size, 2*8))\n            outputs_size = outputs[:, 16:19] # (shape: (batch_size, 3)\n            outputs_distance = outputs[:, 19] # (shape: (batch_size, )\n\n            ########################################################################\n            # compute the size loss:\n            ########################################################################\n            loss_size = regression_loss_func(outputs_size, labels_size)\n\n            loss_size_value = loss_size.data.cpu().numpy()\n            batch_losses_size.append(loss_size_value)\n\n            ########################################################################\n            # compute the keypoints loss:\n            ########################################################################\n            loss_keypoints = regression_loss_func(outputs_keypoints, labels_keypoints)\n\n            loss_keypoints_value = loss_keypoints.data.cpu().numpy()\n            batch_losses_keypoints.append(loss_keypoints_value)\n\n            ########################################################################\n            # compute the distance loss:\n            ########################################################################\n            loss_distance = regression_loss_func(outputs_distance, labels_distance)\n\n            loss_distance_value = loss_distance.data.cpu().numpy()\n            batch_losses_distance.append(loss_distance_value)\n\n            ########################################################################\n            # compute the total loss:\n            ########################################################################\n            loss = loss_size + 10*loss_keypoints + 0.01*loss_distance\n            loss_value = loss.data.cpu().numpy()\n            batch_losses.append(loss_value)\n\n            # if step == 1:\n            #     output_keypoints = outputs_keypoints[0]\n            #     label_keypoints = labels_keypoints[0]\n            #     w = ws[0]\n            #     h = hs[0]\n            #     u_center = u_centers[0]\n            #     v_center = v_centers[0]\n            #     img_id = img_ids[0]\n            #\n            #     img = cv2.imread(val_dataset.img_dir + img_id + "".png"", -1)\n            #\n            #     label_keypoints = np.resize(label_keypoints, (8, 2))\n            #     label_keypoints = label_keypoints*np.array([w, h]) + np.array([u_center, v_center])\n            #     img_with_gt_3dbbox = draw_3dbbox_from_keypoints(img, label_keypoints)\n            #     cv2.imwrite(""%s/img_with_gt_3dbbox_epoch_%d.png"" % (network.model_dir, epoch+1), img_with_gt_3dbbox)\n            #\n            #     output_keypoints = np.resize(output_keypoints, (8, 2))\n            #     output_keypoints = output_keypoints*np.array([w, h]) + np.array([u_center, v_center])\n            #     img_with_pred_3dbbox = draw_3dbbox_from_keypoints(img, output_keypoints)\n            #     cv2.imwrite(""%s/img_with_pred_3dbbox_epoch_%d.png"" % (network.model_dir, epoch+1), img_with_pred_3dbbox)\n\n    epoch_loss = np.mean(batch_losses)\n    epoch_losses_val.append(epoch_loss)\n    with open(""%s/epoch_losses_val.pkl"" % network.model_dir, ""wb"") as file:\n        pickle.dump(epoch_losses_val, file)\n    print (""val loss: %g"" % epoch_loss)\n    plt.figure(1)\n    plt.plot(epoch_losses_val, ""k^"")\n    plt.plot(epoch_losses_val, ""k"")\n    plt.ylabel(""loss"")\n    plt.xlabel(""epoch"")\n    plt.title(""val loss per epoch"")\n    plt.savefig(""%s/epoch_losses_val.png"" % network.model_dir)\n    plt.close(1)\n\n    epoch_loss = np.mean(batch_losses_size)\n    epoch_losses_size_val.append(epoch_loss)\n    with open(""%s/epoch_losses_size_val.pkl"" % network.model_dir, ""wb"") as file:\n        pickle.dump(epoch_losses_size_val, file)\n    print (""val size loss: %g"" % epoch_loss)\n    plt.figure(1)\n    plt.plot(epoch_losses_size_val, ""k^"")\n    plt.plot(epoch_losses_size_val, ""k"")\n    plt.ylabel(""loss"")\n    plt.xlabel(""epoch"")\n    plt.title(""val size loss per epoch"")\n    plt.savefig(""%s/epoch_losses_size_val.png"" % network.model_dir)\n    plt.close(1)\n\n    epoch_loss = np.mean(batch_losses_keypoints)\n    epoch_losses_keypoints_val.append(epoch_loss)\n    with open(""%s/epoch_losses_keypoints_val.pkl"" % network.model_dir, ""wb"") as file:\n        pickle.dump(epoch_losses_keypoints_val, file)\n    print (""val keypoints loss: %g"" % epoch_loss)\n    plt.figure(1)\n    plt.plot(epoch_losses_keypoints_val, ""k^"")\n    plt.plot(epoch_losses_keypoints_val, ""k"")\n    plt.ylabel(""loss"")\n    plt.xlabel(""epoch"")\n    plt.title(""val keypoints loss per epoch"")\n    plt.savefig(""%s/epoch_losses_keypoints_val.png"" % network.model_dir)\n    plt.close(1)\n\n    epoch_loss = np.mean(batch_losses_distance)\n    epoch_losses_distance_val.append(epoch_loss)\n    with open(""%s/epoch_losses_distance_val.pkl"" % network.model_dir, ""wb"") as file:\n        pickle.dump(epoch_losses_distance_val, file)\n    print (""val distance loss: %g"" % epoch_loss)\n    plt.figure(1)\n    plt.plot(epoch_losses_distance_val, ""k^"")\n    plt.plot(epoch_losses_distance_val, ""k"")\n    plt.ylabel(""loss"")\n    plt.xlabel(""epoch"")\n    plt.title(""val distance loss per epoch"")\n    plt.savefig(""%s/epoch_losses_distance_val.png"" % network.model_dir)\n    plt.close(1)\n\n    # save the model weights to disk:\n    checkpoint_path = network.checkpoints_dir + ""/model_"" + model_id +""_epoch_"" + str(epoch+1) + "".pth""\n    torch.save(network.state_dict(), checkpoint_path)\n'"
eval_kitti/create_link.py,0,"b'#!/usr/bin/env python\n\nimport sys\nimport os\nimport numpy as np\n\nif len(sys.argv)<2:\n    print \'Usage: parser.py results_folder database\'\n\nresult_sha = sys.argv[1]\ndatabase = sys.argv[2]\nLFRCNN = os.path.join(os.path.expanduser(\'~\'), \'lsi-faster-rcnn\')\noutput_folder = os.path.join(\'output\', sys.argv[1], \'kitti_\'+database)\n\ni = 0\ndirs = os.listdir(os.path.join(LFRCNN,output_folder))\nfor dir in dirs:\n    print i, dir\n    i+=1\n\nvar = raw_input(""Please select. "")\n\niter_folder = os.path.join(LFRCNN, output_folder, dirs[int(var)])\n\ni = 0\ndirs = os.listdir(iter_folder)\nfor dir in dirs:\n    print i, dir\n    i+=1\n\nvar = raw_input(""Please select. "")\n\ntxts_folder = os.path.join(iter_folder, dirs[int(var)])\n\nprint txts_folder\n\nvar = raw_input(""Write name. "")\n\ndst_folder = os.path.join(\'results\',var)\ndst = os.path.join(dst_folder,\'data\')\n\nif not os.path.exists(dst_folder):\n    os.makedirs(dst_folder)\n\nos.symlink(txts_folder, dst)\nprint \'Created link from\', txts_folder, \'to\', os.path.join(\'results\',var,\'data\')\n'"
eval_kitti/parser.py,0,"b""#!/usr/bin/env python\n\nimport sys\nimport os\nimport numpy as np\n\n# classes = ['car', 'pedestrian', 'cyclist', 'van', 'truck', 'person_sitting', 'tram']\nclasses = ['car']\ndifficulties = ['easy', 'moderate', 'hard']\nparams = ['detection', 'detection_ground', 'detection_3d']\n\nif len(sys.argv)<2:\n    print 'Usage: parser.py results_folder'\n\nresult_sha = sys.argv[1]\ntxt_dir = os.path.join('build','results', result_sha)\n\nfor class_name in classes:\n    for param in params:\n        txt_name = os.path.join(txt_dir, 'stats_' + class_name + '_' + param + '.txt')\n\n        if not os.path.isfile(txt_name):\n            print txt_name, 'not found'\n            continue\n\n        cont = np.loadtxt(txt_name)\n\n        for idx, difficulty in enumerate(difficulties):\n            sum = 0;\n            for i in xrange(0, 41, 4): # NOTE! had to change from xrange(0, 40, 4)\n                sum += cont[idx][i]\n\n            average = sum/11.0\n            print class_name, difficulty, param, average\n\n        print '----------------'\n\n    print '================='\n"""
evaluation/create_txt_files_test.py,0,"b'# camera-ready\n\nimport pickle\nimport numpy as np\nimport os\nimport math\n\nimport sys\nsys.path.append(""/home/fregu856/3DOD_thesis/utils"") # NOTE! you\'ll have to adapt this for your file structure\nfrom kittiloader import calibread\n\ndef ProjectTo2Dbbox(center, h, w, l, r_y, P2):\n    # input: 3Dbbox in (rectified) camera coords\n\n    Rmat = np.asarray([[math.cos(r_y), 0, math.sin(r_y)],\n                       [0, 1, 0],\n                       [-math.sin(r_y), 0, math.cos(r_y)]],\n                       dtype=\'float32\')\n\n    p0 = center + np.dot(Rmat, np.asarray([l/2.0, 0, w/2.0], dtype=\'float32\').flatten())\n    p1 = center + np.dot(Rmat, np.asarray([-l/2.0, 0, w/2.0], dtype=\'float32\').flatten())\n    p2 = center + np.dot(Rmat, np.asarray([-l/2.0, 0, -w/2.0], dtype=\'float32\').flatten())\n    p3 = center + np.dot(Rmat, np.asarray([l/2.0, 0, -w/2.0], dtype=\'float32\').flatten())\n    p4 = center + np.dot(Rmat, np.asarray([l/2.0, -h, w/2.0], dtype=\'float32\').flatten())\n    p5 = center + np.dot(Rmat, np.asarray([-l/2.0, -h, w/2.0], dtype=\'float32\').flatten())\n    p6 = center + np.dot(Rmat, np.asarray([-l/2.0, -h, -w/2.0], dtype=\'float32\').flatten())\n    p7 = center + np.dot(Rmat, np.asarray([l/2.0, -h, -w/2.0], dtype=\'float32\').flatten())\n\n    points = np.array([p0, p1, p2, p3, p4, p5, p6, p7])\n\n    points_hom = np.ones((points.shape[0], 4)) # (shape: (8, 4))\n    points_hom[:, 0:3] = points\n\n    # project the points onto the image plane (homogeneous coords):\n    img_points_hom = np.dot(P2, points_hom.T).T # (shape: (8, 3)) (points_hom.T has shape (4, 8))\n    # normalize:\n    img_points = np.zeros((img_points_hom.shape[0], 2)) # (shape: (8, 2))\n    img_points[:, 0] = img_points_hom[:, 0]/img_points_hom[:, 2]\n    img_points[:, 1] = img_points_hom[:, 1]/img_points_hom[:, 2]\n\n    u_min = np.min(img_points[:, 0])\n    v_min = np.min(img_points[:, 1])\n    u_max = np.max(img_points[:, 0])\n    v_max = np.max(img_points[:, 1])\n\n    left = int(u_min)\n    top = int(v_min)\n    right = int(u_max)\n    bottom = int(v_max)\n\n    projected_2Dbbox = [left, top, right, bottom]\n\n    return projected_2Dbbox\n\nexperiment_name = ""test_Frustum-PointNet_1"" # NOTE change this for every new experiment\n\nproject_dir = ""/home/fregu856/3DOD_thesis/"" # NOTE! you\'ll have to adapt this for your file structure\ndata_dir = project_dir + ""data/kitti/object/testing/""\ncalib_dir = data_dir + ""calib/""\n\neval_kitti_dir = project_dir + ""/eval_kitti/""\nresults_dir = eval_kitti_dir + ""build/results/""\n\nexperiment_results_dir = results_dir + experiment_name + ""/""\nresults_data_dir = experiment_results_dir + ""data/""\nif os.path.exists(experiment_results_dir):\n    raise Exception(""That experiment name already exists!"")\nelse:\n    os.makedirs(experiment_results_dir)\n    os.makedirs(results_data_dir)\n\nimg_ids = []\nimg_names = os.listdir(calib_dir)\nfor img_name in img_names:\n    img_id = img_name.split("".txt"")[0]\n    img_ids.append(img_id)\n\n# NOTE! here you can choose what model\'s output you want to compute metrics for\n# Frustum-PointNet:\nwith open(""/home/fregu856/3DOD_thesis/training_logs/model_Frustum-PointNet_eval_test/eval_dict_test.pkl"", ""rb"") as file: # NOTE! you\'ll have to adapt this for your file structure\n    eval_dict = pickle.load(file)\n#################################\n# # Extended-Frustum-PointNet:\n# with open(""/home/fregu856/3DOD_thesis/training_logs/model_Extended-Frustum-PointNet_eval_test/eval_dict_test.pkl"", ""rb"") as file: # NOTE! you\'ll have to adapt this for your file structure\n#     eval_dict = pickle.load(file)\n# ##################################\n# # Image-Only:\n# with open(""/home/fregu856/3DOD_thesis/training_logs/model_Image-Only_eval_test/eval_dict_test.pkl"", ""rb"") as file: # NOTE! you\'ll have to adapt this for your file structure\n#     eval_dict = pickle.load(file)\n\nfor img_id in img_ids:\n    print img_id\n\n    img_label_file_path = results_data_dir + img_id + "".txt""\n    with open(img_label_file_path, ""w"") as img_label_file:\n\n        if img_id in eval_dict: # (if any predicted bboxes for the image:) (otherwise, just create an empty file)\n            calib = calibread(calib_dir + img_id + "".txt"")\n            P2 = calib[""P2""]\n\n            bbox_dicts = eval_dict[img_id]\n            for bbox_dict in bbox_dicts:\n                pred_center_BboxNet = bbox_dict[""pred_center_BboxNet""]\n                pred_x = pred_center_BboxNet[0]\n                pred_y = pred_center_BboxNet[1]\n                pred_z = pred_center_BboxNet[2]\n                pred_h = bbox_dict[""pred_h""]\n                pred_w = bbox_dict[""pred_w""]\n                pred_l = bbox_dict[""pred_l""]\n                pred_r_y = bbox_dict[""pred_r_y""]\n\n                projected_2Dbbox = ProjectTo2Dbbox(pred_center_BboxNet, pred_h, pred_w, pred_l, pred_r_y, P2)\n                left = projected_2Dbbox[0]\n                top = projected_2Dbbox[1]\n                right = projected_2Dbbox[2]\n                bottom = projected_2Dbbox[3]\n\n                score = bbox_dict[""score_2d""]\n\n                # (type, truncated, occluded, alpha, left, top, right, bottom, h, w, l, x, y, z, ry, score)\n                img_label_file.write(""Car -1 -1 -10 %d %d %d %d %.2f %.2f %.2f %.2f %.2f %.2f %.2f %.2f\\n"" % (left, top, right, bottom, pred_h, pred_w, pred_l, pred_x, pred_y, pred_z, pred_r_y, score))\n'"
evaluation/create_txt_files_val.py,0,"b'# camera-ready\n\nimport pickle\nimport numpy as np\nimport os\nimport math\n\nimport sys\nsys.path.append(""/home/fregu856/3DOD_thesis/utils"") # NOTE! you\'ll have to adapt this for your file structure\nfrom kittiloader import calibread\n\ndef ProjectTo2Dbbox(center, h, w, l, r_y, P2):\n    # input: 3Dbbox in (rectified) camera coords\n\n    Rmat = np.asarray([[math.cos(r_y), 0, math.sin(r_y)],\n                       [0, 1, 0],\n                       [-math.sin(r_y), 0, math.cos(r_y)]],\n                       dtype=\'float32\')\n\n    p0 = center + np.dot(Rmat, np.asarray([l/2.0, 0, w/2.0], dtype=\'float32\').flatten())\n    p1 = center + np.dot(Rmat, np.asarray([-l/2.0, 0, w/2.0], dtype=\'float32\').flatten())\n    p2 = center + np.dot(Rmat, np.asarray([-l/2.0, 0, -w/2.0], dtype=\'float32\').flatten())\n    p3 = center + np.dot(Rmat, np.asarray([l/2.0, 0, -w/2.0], dtype=\'float32\').flatten())\n    p4 = center + np.dot(Rmat, np.asarray([l/2.0, -h, w/2.0], dtype=\'float32\').flatten())\n    p5 = center + np.dot(Rmat, np.asarray([-l/2.0, -h, w/2.0], dtype=\'float32\').flatten())\n    p6 = center + np.dot(Rmat, np.asarray([-l/2.0, -h, -w/2.0], dtype=\'float32\').flatten())\n    p7 = center + np.dot(Rmat, np.asarray([l/2.0, -h, -w/2.0], dtype=\'float32\').flatten())\n\n    points = np.array([p0, p1, p2, p3, p4, p5, p6, p7])\n\n    points_hom = np.ones((points.shape[0], 4)) # (shape: (8, 4))\n    points_hom[:, 0:3] = points\n\n    # project the points onto the image plane (homogeneous coords):\n    img_points_hom = np.dot(P2, points_hom.T).T # (shape: (8, 3)) (points_hom.T has shape (4, 8))\n    # normalize:\n    img_points = np.zeros((img_points_hom.shape[0], 2)) # (shape: (8, 2))\n    img_points[:, 0] = img_points_hom[:, 0]/img_points_hom[:, 2]\n    img_points[:, 1] = img_points_hom[:, 1]/img_points_hom[:, 2]\n\n    u_min = np.min(img_points[:, 0])\n    v_min = np.min(img_points[:, 1])\n    u_max = np.max(img_points[:, 0])\n    v_max = np.max(img_points[:, 1])\n\n    left = int(u_min)\n    top = int(v_min)\n    right = int(u_max)\n    bottom = int(v_max)\n\n    projected_2Dbbox = [left, top, right, bottom]\n\n    return projected_2Dbbox\n\nexperiment_name = ""val_Frustum-PointNet_1"" # NOTE change this for every new experiment\n\nproject_dir = ""/home/fregu856/3DOD_thesis/"" # NOTE! you\'ll have to adapt this for your file structure\ndata_dir = project_dir + ""data/kitti/object/training/""\ncalib_dir = data_dir + ""calib/""\n\neval_kitti_dir = project_dir + ""/eval_kitti/""\nresults_dir = eval_kitti_dir + ""build/results/""\n\nexperiment_results_dir = results_dir + experiment_name + ""/""\nresults_data_dir = experiment_results_dir + ""data/""\nif os.path.exists(experiment_results_dir):\n    raise Exception(""That experiment name already exists!"")\nelse:\n    os.makedirs(experiment_results_dir)\n    os.makedirs(results_data_dir)\n\ntraining_img_ids = []\nimg_names = os.listdir(calib_dir)\nfor img_name in img_names:\n    img_id = img_name.split("".txt"")[0]\n    training_img_ids.append(img_id)\n\n# NOTE! here you can choose what model\'s output you want to compute metrics for\n# Frustum-PointNet:\nwith open(""/home/fregu856/3DOD_thesis/training_logs/model_Frustum-PointNet_eval_val/eval_dict_val.pkl"", ""rb"") as file: # NOTE! you\'ll have to adapt this for your file structure\n    eval_dict = pickle.load(file)\n#################################\n# # Extended-Frustum-PointNet:\n# with open(""/home/fregu856/3DOD_thesis/training_logs/model_Extended-Frustum-PointNet_eval_val/eval_dict_val.pkl"", ""rb"") as file: # NOTE! you\'ll have to adapt this for your file structure\n#     eval_dict = pickle.load(file)\n# ##################################\n# # Image-Only:\n# with open(""/home/fregu856/3DOD_thesis/training_logs/model_Image-Only_eval_val/eval_dict_val.pkl"", ""rb"") as file: # NOTE! you\'ll have to adapt this for your file structure\n#     eval_dict = pickle.load(file)\n\nfor img_id in training_img_ids:\n    print img_id\n\n    img_label_file_path = results_data_dir + img_id + "".txt""\n    with open(img_label_file_path, ""w"") as img_label_file:\n\n        if img_id in eval_dict: # (if any predicted bboxes for the image:) (otherwise, just create an empty file)\n            calib = calibread(calib_dir + img_id + "".txt"")\n            P2 = calib[""P2""]\n\n            bbox_dicts = eval_dict[img_id]\n            for bbox_dict in bbox_dicts:\n                pred_center_BboxNet = bbox_dict[""pred_center_BboxNet""]\n                pred_x = pred_center_BboxNet[0]\n                pred_y = pred_center_BboxNet[1]\n                pred_z = pred_center_BboxNet[2]\n                pred_h = bbox_dict[""pred_h""]\n                pred_w = bbox_dict[""pred_w""]\n                pred_l = bbox_dict[""pred_l""]\n                pred_r_y = bbox_dict[""pred_r_y""]\n\n                projected_2Dbbox = ProjectTo2Dbbox(pred_center_BboxNet, pred_h, pred_w, pred_l, pred_r_y, P2)\n                left = projected_2Dbbox[0]\n                top = projected_2Dbbox[1]\n                right = projected_2Dbbox[2]\n                bottom = projected_2Dbbox[3]\n\n                score = 1.0\n\n                # (type, truncated, occluded, alpha, left, top, right, bottom, h, w, l, x, y, z, ry, score)\n                img_label_file.write(""Car -1 -1 -10 %d %d %d %d %.2f %.2f %.2f %.2f %.2f %.2f %.2f %.2f\\n"" % (left, top, right, bottom, pred_h, pred_w, pred_l, pred_x, pred_y, pred_z, pred_r_y, score))\n'"
evaluation/create_txt_files_val_2ddetections.py,0,"b'# camera-ready\n\nimport pickle\nimport numpy as np\nimport os\nimport math\n\nimport sys\nsys.path.append(""/home/fregu856/3DOD_thesis/utils"") # NOTE! you\'ll have to adapt this for your file structure\nfrom kittiloader import calibread\n\ndef ProjectTo2Dbbox(center, h, w, l, r_y, P2):\n    # input: 3Dbbox in (rectified) camera coords\n\n    Rmat = np.asarray([[math.cos(r_y), 0, math.sin(r_y)],\n                       [0, 1, 0],\n                       [-math.sin(r_y), 0, math.cos(r_y)]],\n                       dtype=\'float32\')\n\n    p0 = center + np.dot(Rmat, np.asarray([l/2.0, 0, w/2.0], dtype=\'float32\').flatten())\n    p1 = center + np.dot(Rmat, np.asarray([-l/2.0, 0, w/2.0], dtype=\'float32\').flatten())\n    p2 = center + np.dot(Rmat, np.asarray([-l/2.0, 0, -w/2.0], dtype=\'float32\').flatten())\n    p3 = center + np.dot(Rmat, np.asarray([l/2.0, 0, -w/2.0], dtype=\'float32\').flatten())\n    p4 = center + np.dot(Rmat, np.asarray([l/2.0, -h, w/2.0], dtype=\'float32\').flatten())\n    p5 = center + np.dot(Rmat, np.asarray([-l/2.0, -h, w/2.0], dtype=\'float32\').flatten())\n    p6 = center + np.dot(Rmat, np.asarray([-l/2.0, -h, -w/2.0], dtype=\'float32\').flatten())\n    p7 = center + np.dot(Rmat, np.asarray([l/2.0, -h, -w/2.0], dtype=\'float32\').flatten())\n\n    points = np.array([p0, p1, p2, p3, p4, p5, p6, p7])\n\n    points_hom = np.ones((points.shape[0], 4)) # (shape: (8, 4))\n    points_hom[:, 0:3] = points\n\n    # project the points onto the image plane (homogeneous coords):\n    img_points_hom = np.dot(P2, points_hom.T).T # (shape: (8, 3)) (points_hom.T has shape (4, 8))\n    # normalize:\n    img_points = np.zeros((img_points_hom.shape[0], 2)) # (shape: (8, 2))\n    img_points[:, 0] = img_points_hom[:, 0]/img_points_hom[:, 2]\n    img_points[:, 1] = img_points_hom[:, 1]/img_points_hom[:, 2]\n\n    u_min = np.min(img_points[:, 0])\n    v_min = np.min(img_points[:, 1])\n    u_max = np.max(img_points[:, 0])\n    v_max = np.max(img_points[:, 1])\n\n    left = int(u_min)\n    top = int(v_min)\n    right = int(u_max)\n    bottom = int(v_max)\n\n    projected_2Dbbox = [left, top, right, bottom]\n\n    return projected_2Dbbox\n\nexperiment_name = ""val_2ddetections_Frustum-PointNet_1"" # NOTE change this for every new experiment\n\nproject_dir = ""/home/fregu856/3DOD_thesis/"" # NOTE! you\'ll have to adapt this for your file structure\ndata_dir = project_dir + ""data/kitti/object/training/""\ncalib_dir = data_dir + ""calib/""\n\neval_kitti_dir = project_dir + ""/eval_kitti/""\nresults_dir = eval_kitti_dir + ""build/results/""\n\nexperiment_results_dir = results_dir + experiment_name + ""/""\nresults_data_dir = experiment_results_dir + ""data/""\nif os.path.exists(experiment_results_dir):\n    raise Exception(""That experiment name already exists!"")\nelse:\n    os.makedirs(experiment_results_dir)\n    os.makedirs(results_data_dir)\n\ntraining_img_ids = []\nimg_names = os.listdir(calib_dir)\nfor img_name in img_names:\n    img_id = img_name.split("".txt"")[0]\n    training_img_ids.append(img_id)\n\n# NOTE! here you can choose what model\'s output you want to compute metrics for\n# Frustum-PointNet:\nwith open(""/home/fregu856/3DOD_thesis/training_logs/model_Frustum-PointNet_eval_val_2ddetections/eval_dict_val_2ddetections.pkl"", ""rb"") as file: # NOTE! you\'ll have to adapt this for your file structure\n    eval_dict = pickle.load(file)\n#################################\n# # Extended-Frustum-PointNet:\n# with open(""/home/fregu856/3DOD_thesis/training_logs/model_Extended-Frustum-PointNet_eval_val_2ddetections/eval_dict_val_2ddetections.pkl"", ""rb"") as file: # NOTE! you\'ll have to adapt this for your file structure\n#     eval_dict = pickle.load(file)\n# ##################################\n# # Image-Only:\n# with open(""/home/fregu856/3DOD_thesis/training_logs/model_Image-Only_eval_val_2ddetections/eval_dict_val_2ddetections.pkl"", ""rb"") as file: # NOTE! you\'ll have to adapt this for your file structure\n#     eval_dict = pickle.load(file)\n\nfor img_id in training_img_ids:\n    print img_id\n\n    img_label_file_path = results_data_dir + img_id + "".txt""\n    with open(img_label_file_path, ""w"") as img_label_file:\n\n        if img_id in eval_dict: # (if any predicted bboxes for the image:) (otherwise, just create an empty file)\n            calib = calibread(calib_dir + img_id + "".txt"")\n            P2 = calib[""P2""]\n\n            bbox_dicts = eval_dict[img_id]\n            for bbox_dict in bbox_dicts:\n                pred_center_BboxNet = bbox_dict[""pred_center_BboxNet""]\n                pred_x = pred_center_BboxNet[0]\n                pred_y = pred_center_BboxNet[1]\n                pred_z = pred_center_BboxNet[2]\n                pred_h = bbox_dict[""pred_h""]\n                pred_w = bbox_dict[""pred_w""]\n                pred_l = bbox_dict[""pred_l""]\n                pred_r_y = bbox_dict[""pred_r_y""]\n\n                projected_2Dbbox = ProjectTo2Dbbox(pred_center_BboxNet, pred_h, pred_w, pred_l, pred_r_y, P2)\n                left = projected_2Dbbox[0]\n                top = projected_2Dbbox[1]\n                right = projected_2Dbbox[2]\n                bottom = projected_2Dbbox[3]\n\n                score = bbox_dict[""score_2d""]\n\n                # (type, truncated, occluded, alpha, left, top, right, bottom, h, w, l, x, y, z, ry, score)\n                img_label_file.write(""Car -1 -1 -10 %d %d %d %d %.2f %.2f %.2f %.2f %.2f %.2f %.2f %.2f\\n"" % (left, top, right, bottom, pred_h, pred_w, pred_l, pred_x, pred_y, pred_z, pred_r_y, score))\n'"
utils/kittiloader.py,0,"b'# camera-ready\n\nimport numpy as np\nimport cv2\nimport math\n\n################################################################################\n# imported functions:\n################################################################################\ndef calibread(file_path):\n    out = dict()\n    for line in open(file_path, \'r\'):\n        line = line.strip()\n        if line == \'\' or line[0] == \'#\':\n            continue\n        val = line.split(\':\')\n        assert len(val) == 2, \'Wrong file format, only one : per line!\'\n        key_name = val[0].strip()\n        val = np.asarray(val[-1].strip().split(\' \'), dtype=\'f8\')\n        assert len(val) in [12, 9], ""Wrong file format, wrong number of numbers!""\n        if len(val) == 12:\n            out[key_name] = val.reshape(3, 4)\n        elif len(val) == 9:\n            out[key_name] = val.reshape(3, 3)\n    return out\n\ndef LabelLoader2D3D(file_id, path, ext, calib_path, calib_ext):\n    labels = labelread(path + ""/"" + file_id + ext)\n    calib = calibread(calib_path + ""/"" + file_id + calib_ext)\n    polys = list()\n    for bbox in labels:\n        poly = dict()\n\n        poly2d = dict()\n        poly2d[\'class\'] = bbox[\'type\']\n        poly2d[\'truncated\'] = bbox[\'truncated\']\n        poly2d[\'poly\'] = np.array([[bbox[\'bbox\'][\'left\'], bbox[\'bbox\'][\'top\']],\n                                 [bbox[\'bbox\'][\'right\'], bbox[\'bbox\'][\'top\']],\n                                 [bbox[\'bbox\'][\'right\'], bbox[\'bbox\'][\'bottom\']],\n                                 [bbox[\'bbox\'][\'left\'], bbox[\'bbox\'][\'bottom\']]],\n                                dtype=\'int32\')\n        poly[""label_2D""] = poly2d\n\n        poly3d = dict()\n        poly3d[\'class\'] = bbox[\'type\']\n        location = np.asarray([bbox[\'location\'][\'x\'],\n                               bbox[\'location\'][\'y\'],\n                               bbox[\'location\'][\'z\']], dtype=\'float32\')\n        r_y = bbox[\'rotation_y\']\n        Rmat = np.asarray([[math.cos(r_y), 0, math.sin(r_y)], [0, 1, 0],\n                           [-math.sin(r_y), 0, math.cos(r_y)]],\n                          dtype=\'float32\')\n        length = bbox[\'dimensions\'][\'length\']\n        width = bbox[\'dimensions\'][\'width\']\n        height = bbox[\'dimensions\'][\'height\']\n        p0 = np.dot(Rmat, np.asarray(\n            [length / 2.0, 0, width / 2.0], dtype=\'float32\'))\n        p1 = np.dot(Rmat, np.asarray(\n            [-length / 2.0, 0, width / 2.0], dtype=\'float32\'))\n        p2 = np.dot(Rmat, np.asarray(\n            [-length / 2.0, 0, -width / 2.0], dtype=\'float32\'))\n        p3 = np.dot(Rmat, np.asarray(\n            [length / 2.0, 0, -width / 2.0], dtype=\'float32\'))\n        p4 = np.dot(Rmat, np.asarray(\n            [length / 2.0, -height, width / 2.0], dtype=\'float32\'))\n        p5 = np.dot(Rmat, np.asarray(\n            [-length / 2.0, -height, width / 2.0], dtype=\'float32\'))\n        p6 = np.dot(Rmat, np.asarray(\n            [-length / 2.0, -height, -width / 2.0], dtype=\'float32\'))\n        p7 = np.dot(Rmat, np.asarray(\n            [length / 2.0, -height, -width / 2.0], dtype=\'float32\'))\n        poly3d[\'points\'] = np.array(location + [p0, p1, p2, p3, p4, p5, p6, p7])\n        poly3d[\'lines\'] = [[0, 3, 7, 4, 0], [1, 2, 6, 5, 1],\n                         [0, 1], [2, 3], [6, 7], [4, 5]]\n        poly3d[\'colors\'] = [[255, 0, 0], [0, 0, 255], [\n            255, 0, 0], [255, 0, 0], [255, 0, 0], [255, 0, 0]]\n        poly3d[\'P0_mat\'] = calib[\'P2\']\n        poly3d[\'center\'] = location\n        poly3d[\'l\'] = length\n        poly3d[\'w\'] = width\n        poly3d[\'h\'] = height\n        poly3d[\'r_y\'] = r_y\n        poly[""label_3D""] = poly3d\n\n        polys.append(poly)\n    return polys\n\ndef LabelLoader2D3D_sequence(img_id, img_id_float, label_path, calib_path):\n    labels = labelread_sequence(label_path)\n\n    img_id_labels = []\n    for label in labels:\n        if label[""frame""] == img_id_float:\n            img_id_labels.append(label)\n\n    calib = calibread(calib_path)\n    polys = list()\n    for bbox in img_id_labels:\n        poly = dict()\n\n        poly2d = dict()\n        poly2d[\'class\'] = bbox[\'type\']\n        poly2d[\'truncated\'] = bbox[\'truncated\']\n        poly2d[\'occluded\'] = bbox[\'occluded\']\n        poly2d[\'poly\'] = np.array([[bbox[\'bbox\'][\'left\'], bbox[\'bbox\'][\'top\']],\n                                 [bbox[\'bbox\'][\'right\'], bbox[\'bbox\'][\'top\']],\n                                 [bbox[\'bbox\'][\'right\'], bbox[\'bbox\'][\'bottom\']],\n                                 [bbox[\'bbox\'][\'left\'], bbox[\'bbox\'][\'bottom\']]],\n                                dtype=\'int32\')\n        poly[""label_2D""] = poly2d\n\n        poly3d = dict()\n        poly3d[\'class\'] = bbox[\'type\']\n        location = np.asarray([bbox[\'location\'][\'x\'],\n                               bbox[\'location\'][\'y\'],\n                               bbox[\'location\'][\'z\']], dtype=\'float32\')\n        r_y = bbox[\'rotation_y\']\n        Rmat = np.asarray([[math.cos(r_y), 0, math.sin(r_y)], [0, 1, 0],\n                           [-math.sin(r_y), 0, math.cos(r_y)]],\n                          dtype=\'float32\')\n        length = bbox[\'dimensions\'][\'length\']\n        width = bbox[\'dimensions\'][\'width\']\n        height = bbox[\'dimensions\'][\'height\']\n        p0 = np.dot(Rmat, np.asarray(\n            [length / 2.0, 0, width / 2.0], dtype=\'float32\'))\n        p1 = np.dot(Rmat, np.asarray(\n            [-length / 2.0, 0, width / 2.0], dtype=\'float32\'))\n        p2 = np.dot(Rmat, np.asarray(\n            [-length / 2.0, 0, -width / 2.0], dtype=\'float32\'))\n        p3 = np.dot(Rmat, np.asarray(\n            [length / 2.0, 0, -width / 2.0], dtype=\'float32\'))\n        p4 = np.dot(Rmat, np.asarray(\n            [length / 2.0, -height, width / 2.0], dtype=\'float32\'))\n        p5 = np.dot(Rmat, np.asarray(\n            [-length / 2.0, -height, width / 2.0], dtype=\'float32\'))\n        p6 = np.dot(Rmat, np.asarray(\n            [-length / 2.0, -height, -width / 2.0], dtype=\'float32\'))\n        p7 = np.dot(Rmat, np.asarray(\n            [length / 2.0, -height, -width / 2.0], dtype=\'float32\'))\n        poly3d[\'points\'] = np.array(location + [p0, p1, p2, p3, p4, p5, p6, p7])\n        poly3d[\'lines\'] = [[0, 3, 7, 4, 0], [1, 2, 6, 5, 1],\n                         [0, 1], [2, 3], [6, 7], [4, 5]]\n        poly3d[\'colors\'] = [[255, 0, 0], [0, 0, 255], [\n            255, 0, 0], [255, 0, 0], [255, 0, 0], [255, 0, 0]]\n        poly3d[\'P0_mat\'] = calib[\'P2\']\n        poly3d[\'center\'] = location\n        poly3d[\'l\'] = length\n        poly3d[\'w\'] = width\n        poly3d[\'h\'] = height\n        poly3d[\'r_y\'] = r_y\n        poly[""label_3D""] = poly3d\n\n        polys.append(poly)\n    return polys\n\ndef labelread(file_path):\n    bbox = (\'bbox\', [\'left\', \'top\', \'right\', \'bottom\'])\n    dimensions = (\'dimensions\', [\'height\', \'width\', \'length\'])\n    location = (\'location\', [\'x\', \'y\', \'z\'])\n    keys = [\'type\', \'truncated\', \'occluded\', \'alpha\', bbox,\n            dimensions, location, \'rotation_y\', \'score\']\n    labels = list()\n    for line in open(file_path, \'r\'):\n        vals = line.split()\n        l, _ = vals_to_dict(vals, keys)\n        labels.append(l)\n    return labels\n\n################################################################################\n# helper functions:\n################################################################################\ndef labelread_sequence(file_path):\n    bbox = (\'bbox\', [\'left\', \'top\', \'right\', \'bottom\'])\n    dimensions = (\'dimensions\', [\'height\', \'width\', \'length\'])\n    location = (\'location\', [\'x\', \'y\', \'z\'])\n    keys = [\'frame\', \'track_id\', \'type\', \'truncated\', \'occluded\', \'alpha\', bbox,\n            dimensions, location, \'rotation_y\', \'score\']\n    labels = list()\n    for line in open(file_path, \'r\'):\n        vals = line.split()\n        l, _ = vals_to_dict(vals, keys)\n        labels.append(l)\n    return labels\n\ndef vals_to_dict(vals, keys, vals_n=0):\n    out = dict()\n    for key in keys:\n        if isinstance(key, str):\n            try:\n                val = float(vals[vals_n])\n            except:\n                val = vals[vals_n]\n            data = val\n            key_name = key\n            vals_n += 1\n        else:\n            data, vals_n = vals_to_dict(vals, key[1], vals_n)\n            key_name = key[0]\n        out[key_name] = data\n        if vals_n >= len(vals):\n            break\n    return out, vals_n\n'"
utils/random_code.py,6,"b'# camera-ready\n\n# this file contains code snippets which I have found (more or less) useful at\n# some point during the project. Probably nothing interesting to see here.\n\n\n\n\n\n\n\n# import pickle\n# import numpy as np\n#\n# with open(""/home/fregu856/exjobb/training_logs/imgnet/model_7/epoch_losses_val.pkl"", ""rb"") as file:\n#     val_loss = pickle.load(file)\n#\n# with open(""/home/fregu856/exjobb/training_logs/imgnet/model_7/epoch_losses_distance_val.pkl"", ""rb"") as file:\n#     val_distance_loss = pickle.load(file)\n#\n# with open(""/home/fregu856/exjobb/training_logs/imgnet/model_7/epoch_losses_keypoints_val.pkl"", ""rb"") as file:\n#     val_keypoints_loss = pickle.load(file)\n#\n# with open(""/home/fregu856/exjobb/training_logs/imgnet/model_7/epoch_losses_size_val.pkl"", ""rb"") as file:\n#     val_size_loss = pickle.load(file)\n#\n# print (""val loss min:"", np.argmin(np.array(val_loss)), np.min(np.array(val_loss)))\n#\n# print (""val keypoints loss min:"", np.argmin(np.array(val_keypoints_loss)), np.min(np.array(val_keypoints_loss)))\n#\n# print (""val distance loss min:"", np.argmin(np.array(val_distance_loss)), np.min(np.array(val_distance_loss)))\n#\n# print (""val size loss min:"", np.argmin(np.array(val_size_loss)), np.min(np.array(val_size_loss)))\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n# import numpy as np\n# import pickle\n# import random\n#\n# with open(""/home/fregu856/exjobb/data/kitti/meta/train_img_ids.pkl"", ""rb"") as file:\n#     old_train_img_ids = pickle.load(file)\n#\n# with open(""/home/fregu856/exjobb/data/kitti/meta/val_img_ids.pkl"", ""rb"") as file:\n#     old_val_img_ids = pickle.load(file)\n#\n# print (""old_train_img_ids:"", len(old_train_img_ids))\n# print (""old_val_img_ids:"", len(old_val_img_ids))\n#\n# img_ids = old_train_img_ids + old_val_img_ids\n# random.shuffle(img_ids)\n# random.shuffle(img_ids)\n# random.shuffle(img_ids)\n# random.shuffle(img_ids)\n#\n# no_of_imgs = len(img_ids)\n# train_img_ids = img_ids[:int(no_of_imgs*0.9)]\n# val_img_ids = img_ids[-int(no_of_imgs*0.1):]\n#\n# print (no_of_imgs)\n# print (""train:"", len(train_img_ids))\n# print (""val:"", len(val_img_ids))\n#\n# with open(""/home/fregu856/exjobb/data/kitti/meta/train_img_ids_random.pkl"", ""wb"") as file:\n#     pickle.dump(train_img_ids, file, protocol=2) # (protocol=2 is needed to be able to open this file with python2)\n#\n# with open(""/home/fregu856/exjobb/data/kitti/meta/val_img_ids_random.pkl"", ""wb"") as file:\n#     pickle.dump(val_img_ids, file, protocol=2) # (protocol=2 is needed to be able to open this file with python2)\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n# import pickle\n# import matplotlib\n# matplotlib.use(""Agg"")\n# import matplotlib.pyplot as plt\n#\n# import numpy as np\n#\n# model_id = ""40""\n#\n# with open(""/home/fregu856/exjobb/training_logs/frustum_pointnet/model_"" + model_id + ""/epoch_losses_TNet_val.pkl"", ""rb"") as file:\n#     TNet_val = pickle.load(file)\n#\n# with open(""/home/fregu856/exjobb/training_logs/frustum_pointnet/model_"" + model_id + ""/epoch_losses_BboxNet_size_val.pkl"", ""rb"") as file:\n#     size_val = pickle.load(file)\n#\n# with open(""/home/fregu856/exjobb/training_logs/frustum_pointnet/model_"" + model_id + ""/epoch_losses_BboxNet_center_val.pkl"", ""rb"") as file:\n#     center_val = pickle.load(file)\n#\n# with open(""/home/fregu856/exjobb/training_logs/frustum_pointnet/model_"" + model_id + ""/epoch_accuracies_heading_class_val.pkl"", ""rb"") as file:\n#     data_acc = pickle.load(file)\n#\n# with open(""/home/fregu856/exjobb/training_logs/frustum_pointnet/model_"" + model_id + ""/epoch_losses_val.pkl"", ""rb"") as file:\n#     data_loss = pickle.load(file)\n#\n# # for index, value in enumerate(data_acc):\n# #     print(index, value)\n# #\n# # for index, value in enumerate(data_loss):\n# #     print(index, value)\n# #\n# # for index, value in enumerate(size_val):\n# #     print(index, value)\n#\n# plt.figure(1)\n# plt.plot(data_acc[50:], ""k^"")\n# plt.plot(data_acc[50:], ""k"")\n# plt.ylabel(""accuracy"")\n# plt.xlabel(""epoch"")\n# plt.title(""training heading class accuracy per epoch"")\n# plt.savefig(""test.png"")\n# plt.close(1)\n#\n# plt.figure(1)\n# plt.plot(data_loss[50:], ""k^"")\n# plt.plot(data_loss[50:], ""k"")\n# plt.ylabel(""accuracy"")\n# plt.xlabel(""epoch"")\n# plt.title(""training heading class accuracy per epoch"")\n# plt.savefig(""test2.png"")\n# plt.close(1)\n#\n# print (""heading class accuracy max:"", np.argmax(np.array(data_acc)), np.max(np.array(data_acc)))\n#\n# print (""val loss min:"", np.argmin(np.array(data_loss)),  np.min(np.array(data_loss)))\n#\n# print (""TNet val loss min:"", np.argmin(np.array(TNet_val)), np.min(np.array(TNet_val)))\n#\n# print (""center val loss min:"", np.argmin(np.array(center_val)), np.min(np.array(center_val)))\n#\n# print (""size val loss min:"", np.argmin(np.array(size_val)), np.min(np.array(size_val)))\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n# import pickle\n# import matplotlib\n# matplotlib.use(""Agg"")\n# import matplotlib.pyplot as plt\n#\n# import numpy as np\n#\n# model_id = ""10_2""\n#\n# with open(""/home/fregu856/exjobb/training_logs/imgnet/model_"" + model_id + ""/epoch_losses_val.pkl"", ""rb"") as file:\n#     val_loss = pickle.load(file)\n#\n# print (""val loss min:"", np.argmin(np.array(val_loss)),  np.min(np.array(val_loss)))\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n# import numpy as np\n# import pickle\n#\n# with open(""/home/fregu856/exjobb/data/kitti/meta/val_img_ids_random.pkl"", ""rb"") as file: # (needed for python3)\n#     val_img_ids = pickle.load(file)\n#\n# print len(val_img_ids)\n#\n# with open(""/home/fregu856/exjobb/code/eval_kitti/build/lists/val_random.txt"", ""w"") as txt_file:\n#     for img_id in val_img_ids:\n#         txt_file.write(img_id + ""\\n"")\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n# ################################################################################\n# # split SYN into train/val and save to disk:\n# ################################################################################\n# # NOTE NOTE needs to be run with python3\n#\n# import numpy as np\n# import random\n# import pickle\n#\n# # SYN has 25000 images, ids: {1, 2, 3,..., 25000}\n#\n# img_ids = list(np.arange(1,25000+1))\n#\n# random.shuffle(img_ids)\n# random.shuffle(img_ids)\n# random.shuffle(img_ids)\n#\n# no_of_imgs = len(img_ids)\n# train_img_ids = img_ids[:int(no_of_imgs*0.8)]\n# val_img_ids = img_ids[-int(no_of_imgs*0.2):]\n#\n# with open(""/home/fregu856/exjobb/data/7dlabs/meta/train_img_ids.pkl"", ""wb"") as file:\n#     pickle.dump(train_img_ids, file, protocol=2) # (protocol=2 is needed to be able to open this file with python2)\n#\n# with open(""/home/fregu856/exjobb/data/7dlabs/meta/val_img_ids.pkl"", ""wb"") as file:\n#     pickle.dump(val_img_ids, file, protocol=2) # (protocol=2 is needed to be able to open this file with python2)\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n# ################################################################################\n# # compute mean of all centered/rotated input frustum point clouds in train:\n# ################################################################################\n# from datasets import DatasetFrustumPointNet # (this needs to be imported before torch, because cv2 needs to be imported before torch for some reason)\n# from frustum_pointnet import FrustumPointNet\n#\n# import torch\n# import torch.utils.data\n# import torch.nn as nn\n# from torch.autograd import Variable\n# import torch.optim as optim\n# import torch.nn.functional as F\n#\n# import numpy as np\n# import pickle\n# import matplotlib\n# matplotlib.use(""Agg"")\n# import matplotlib.pyplot as plt\n#\n# batch_size = 32\n#\n# train_dataset = DatasetFrustumPointNet(kitti_data_path=""/datasets/kitti"",\n#                                        kitti_meta_path=""/staging/frexgus/kitti/meta"",\n#                                        type=""train"", NH=4)\n#\n# train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n#                                            batch_size=batch_size, shuffle=True,\n#                                            num_workers=16)\n#\n# mean_xyz_list = []\n# for step, (centered_frustum_point_clouds_camera, labels_InstanceSeg, labels_TNet, labels_BboxNet, labels_corner, labels_corner_flipped) in enumerate(train_loader):\n#\n#     frustum_point_clouds = centered_frustum_point_clouds_camera.numpy()\n#     frustum_point_clouds_xyz = frustum_point_clouds[:, :, 0:3]\n#\n#     mean_xyz = np.mean(frustum_point_clouds_xyz, 1) # (shape: (batch_size, 3))\n#     mean_xyz = np.mean(mean_xyz, 0) # (shape: (3, ))\n#\n#     mean_xyz_list.append(mean_xyz)\n#\n# num_means = len(mean_xyz_list)\n#\n# means = np.zeros((num_means, 3))\n# for i in range(num_means):\n#     means[i] = mean_xyz_list[i]\n#\n# mean_xyz = np.mean(means, 0)\n#\n# print (mean_xyz)\n#\n# with open(""/staging/frexgus/kitti/meta/centered_frustum_mean_xyz.pkl"", ""wb"") as file:\n#   pickle.dump(mean_xyz, file, protocol=2) # (protocol=2 is needed to be able to open this file with python2))\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n# import pickle\n# import numpy\n#\n# with open(""/home/fregu856/exjobb/data/7dlabs/meta/kitti_train_mean_car_size.pkl"", ""rb"") as file:\n#     data = pickle.load(file)\n#\n# print (data)\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n# import os\n# import numpy as np\n# import cPickle\n#\n# train_file_path = ""/home/fregu856/exjobb/data/kitti/meta/train.txt""\n# val_file_path = ""/home/fregu856/exjobb/data/kitti/meta/val.txt""\n#\n# train_img_ids = []\n# with open(train_file_path) as train_file:\n#     for line in train_file:\n#         img_id = line.strip()\n#         train_img_ids.append(img_id)\n#\n# val_img_ids = []\n# with open(val_file_path) as val_file:\n#     for line in val_file:\n#         img_id = line.strip()\n#         val_img_ids.append(img_id)\n#\n# cPickle.dump(train_img_ids, open(""/home/fregu856/exjobb/data/kitti/meta/train_img_ids.pkl"", ""w""))\n# cPickle.dump(val_img_ids, open(""/home/fregu856/exjobb/data/kitti/meta/val_img_ids.pkl"", ""w""))\n#\n# # train_img_ids = cPickle.load(open(""/home/fregu856/exjobb/data/kitti/meta/train_img_ids.pkl""))\n# # val_img_ids = cPickle.load(open(""/home/fregu856/exjobb/data/kitti/meta/val_img_ids.pkl""))\n# #\n# # print len(train_img_ids)\n# # print len(val_img_ids)\n'"
visualization/visualize_eval_test.py,0,"b'# camera-ready\n\nimport pickle\nimport numpy as np\nimport math\nimport cv2\n\nimport sys\nsys.path.append(""/home/fregu856/3DOD_thesis/utils"") # NOTE! you\'ll have to adapt this for your file structure\nfrom kittiloader import LabelLoader2D3D, calibread\n\ndef create3Dbbox(center, h, w, l, r_y, type=""pred""):\n    if type == ""pred"":\n        color = [1, 0.75, 0] # (normalized RGB)\n        front_color = [1, 0, 0] # (normalized RGB)\n    else: # (if type == ""gt"":)\n        color = [1, 0, 0.75] # (normalized RGB)\n        front_color = [0, 0.9, 1] # (normalized RGB)\n\n    Rmat = np.asarray([[math.cos(r_y), 0, math.sin(r_y)],\n                       [0, 1, 0],\n                       [-math.sin(r_y), 0, math.cos(r_y)]],\n                       dtype=\'float32\')\n\n    Rmat_90 = np.asarray([[math.cos(r_y+np.pi/2), 0, math.sin(r_y+np.pi/2)],\n                          [0, 1, 0],\n                          [-math.sin(r_y+np.pi/2), 0, math.cos(r_y+np.pi/2)]],\n                          dtype=\'float32\')\n\n    Rmat_90_x = np.asarray([[1, 0, 0],\n                            [0, math.cos(np.pi/2), math.sin(np.pi/2)],\n                            [0, -math.sin(np.pi/2), math.cos(np.pi/2)]],\n                            dtype=\'float32\')\n\n    p0 = center + np.dot(Rmat, np.asarray([l/2.0, 0, w/2.0], dtype=\'float32\').flatten())\n    p1 = center + np.dot(Rmat, np.asarray([-l/2.0, 0, w/2.0], dtype=\'float32\').flatten())\n    p2 = center + np.dot(Rmat, np.asarray([-l/2.0, 0, -w/2.0], dtype=\'float32\').flatten())\n    p3 = center + np.dot(Rmat, np.asarray([l/2.0, 0, -w/2.0], dtype=\'float32\').flatten())\n    p4 = center + np.dot(Rmat, np.asarray([l/2.0, -h, w/2.0], dtype=\'float32\').flatten())\n    p5 = center + np.dot(Rmat, np.asarray([-l/2.0, -h, w/2.0], dtype=\'float32\').flatten())\n    p6 = center + np.dot(Rmat, np.asarray([-l/2.0, -h, -w/2.0], dtype=\'float32\').flatten())\n    p7 = center + np.dot(Rmat, np.asarray([l/2.0, -h, -w/2.0], dtype=\'float32\').flatten())\n\n    p0_3 = center + np.dot(Rmat, np.asarray([l/2.0, 0, 0], dtype=\'float32\').flatten())\n    p1_2 = center + np.dot(Rmat, np.asarray([-l/2.0, 0, 0], dtype=\'float32\').flatten())\n    p4_7 = center + np.dot(Rmat, np.asarray([l/2.0, -h, 0], dtype=\'float32\').flatten())\n    p5_6 = center + np.dot(Rmat, np.asarray([-l/2.0, -h, 0], dtype=\'float32\').flatten())\n    p0_1 = center + np.dot(Rmat, np.asarray([0, 0, w/2.0], dtype=\'float32\').flatten())\n    p3_2 = center + np.dot(Rmat, np.asarray([0, 0, -w/2.0], dtype=\'float32\').flatten())\n    p4_5 = center + np.dot(Rmat, np.asarray([0, -h, w/2.0], dtype=\'float32\').flatten())\n    p7_6 = center + np.dot(Rmat, np.asarray([0, -h, -w/2.0], dtype=\'float32\').flatten())\n    p0_4 = center + np.dot(Rmat, np.asarray([l/2.0, -h/2.0, w/2.0], dtype=\'float32\').flatten())\n    p3_7 = center + np.dot(Rmat, np.asarray([l/2.0, -h/2.0, -w/2.0], dtype=\'float32\').flatten())\n    p1_5 = center + np.dot(Rmat, np.asarray([-l/2.0, -h/2.0, w/2.0], dtype=\'float32\').flatten())\n    p2_6 = center + np.dot(Rmat, np.asarray([-l/2.0, -h/2.0, -w/2.0], dtype=\'float32\').flatten())\n    p0_1_3_2 = center\n\n    length_0_3 = np.linalg.norm(p0 - p3)\n    cylinder_0_3 = create_mesh_cylinder(radius=0.025, height=length_0_3)\n    cylinder_0_3.compute_vertex_normals()\n    transform_0_3 = np.eye(4)\n    transform_0_3[0:3, 0:3] = Rmat\n    transform_0_3[0:3, 3] = p0_3\n    cylinder_0_3.transform(transform_0_3)\n    cylinder_0_3.paint_uniform_color(front_color)\n\n    length_1_2 = np.linalg.norm(p1 - p2)\n    cylinder_1_2 = create_mesh_cylinder(radius=0.025, height=length_1_2)\n    cylinder_1_2.compute_vertex_normals()\n    transform_1_2 = np.eye(4)\n    transform_1_2[0:3, 0:3] = Rmat\n    transform_1_2[0:3, 3] = p1_2\n    cylinder_1_2.transform(transform_1_2)\n    cylinder_1_2.paint_uniform_color(color)\n\n    length_4_7 = np.linalg.norm(p4 - p7)\n    cylinder_4_7 = create_mesh_cylinder(radius=0.025, height=length_4_7)\n    cylinder_4_7.compute_vertex_normals()\n    transform_4_7 = np.eye(4)\n    transform_4_7[0:3, 0:3] = Rmat\n    transform_4_7[0:3, 3] = p4_7\n    cylinder_4_7.transform(transform_4_7)\n    cylinder_4_7.paint_uniform_color(front_color)\n\n    length_5_6 = np.linalg.norm(p5 - p6)\n    cylinder_5_6 = create_mesh_cylinder(radius=0.025, height=length_5_6)\n    cylinder_5_6.compute_vertex_normals()\n    transform_5_6 = np.eye(4)\n    transform_5_6[0:3, 0:3] = Rmat\n    transform_5_6[0:3, 3] = p5_6\n    cylinder_5_6.transform(transform_5_6)\n    cylinder_5_6.paint_uniform_color(color)\n\n    # #\n\n    length_0_1 = np.linalg.norm(p0 - p1)\n    cylinder_0_1 = create_mesh_cylinder(radius=0.025, height=length_0_1)\n    cylinder_0_1.compute_vertex_normals()\n    transform_0_1 = np.eye(4)\n    transform_0_1[0:3, 0:3] = Rmat_90\n    transform_0_1[0:3, 3] = p0_1\n    cylinder_0_1.transform(transform_0_1)\n    cylinder_0_1.paint_uniform_color(color)\n\n    length_3_2 = np.linalg.norm(p3 - p2)\n    cylinder_3_2 = create_mesh_cylinder(radius=0.025, height=length_3_2)\n    cylinder_3_2.compute_vertex_normals()\n    transform_3_2 = np.eye(4)\n    transform_3_2[0:3, 0:3] = Rmat_90\n    transform_3_2[0:3, 3] = p3_2\n    cylinder_3_2.transform(transform_3_2)\n    cylinder_3_2.paint_uniform_color(color)\n\n    length_4_5 = np.linalg.norm(p4 - p5)\n    cylinder_4_5 = create_mesh_cylinder(radius=0.025, height=length_4_5)\n    cylinder_4_5.compute_vertex_normals()\n    transform_4_5 = np.eye(4)\n    transform_4_5[0:3, 0:3] = Rmat_90\n    transform_4_5[0:3, 3] = p4_5\n    cylinder_4_5.transform(transform_4_5)\n    cylinder_4_5.paint_uniform_color(color)\n\n    length_7_6 = np.linalg.norm(p7 - p6)\n    cylinder_7_6 = create_mesh_cylinder(radius=0.025, height=length_7_6)\n    cylinder_7_6.compute_vertex_normals()\n    transform_7_6 = np.eye(4)\n    transform_7_6[0:3, 0:3] = Rmat_90\n    transform_7_6[0:3, 3] = p7_6\n    cylinder_7_6.transform(transform_7_6)\n    cylinder_7_6.paint_uniform_color(color)\n\n    # #\n\n    length_0_4 = np.linalg.norm(p0 - p4)\n    cylinder_0_4 = create_mesh_cylinder(radius=0.025, height=length_0_4)\n    cylinder_0_4.compute_vertex_normals()\n    transform_0_4 = np.eye(4)\n    transform_0_4[0:3, 0:3] = np.dot(Rmat, Rmat_90_x)\n    transform_0_4[0:3, 3] = p0_4\n    cylinder_0_4.transform(transform_0_4)\n    cylinder_0_4.paint_uniform_color(front_color)\n\n    length_3_7 = np.linalg.norm(p3 - p7)\n    cylinder_3_7 = create_mesh_cylinder(radius=0.025, height=length_3_7)\n    cylinder_3_7.compute_vertex_normals()\n    transform_3_7 = np.eye(4)\n    transform_3_7[0:3, 0:3] = np.dot(Rmat, Rmat_90_x)\n    transform_3_7[0:3, 3] = p3_7\n    cylinder_3_7.transform(transform_3_7)\n    cylinder_3_7.paint_uniform_color(front_color)\n\n    length_1_5 = np.linalg.norm(p1 - p5)\n    cylinder_1_5 = create_mesh_cylinder(radius=0.025, height=length_1_5)\n    cylinder_1_5.compute_vertex_normals()\n    transform_1_5 = np.eye(4)\n    transform_1_5[0:3, 0:3] = np.dot(Rmat, Rmat_90_x)\n    transform_1_5[0:3, 3] = p1_5\n    cylinder_1_5.transform(transform_1_5)\n    cylinder_1_5.paint_uniform_color(color)\n\n    length_2_6 = np.linalg.norm(p2 - p6)\n    cylinder_2_6 = create_mesh_cylinder(radius=0.025, height=length_2_6)\n    cylinder_2_6.compute_vertex_normals()\n    transform_2_6 = np.eye(4)\n    transform_2_6[0:3, 0:3] = np.dot(Rmat, Rmat_90_x)\n    transform_2_6[0:3, 3] = p2_6\n    cylinder_2_6.transform(transform_2_6)\n    cylinder_2_6.paint_uniform_color(color)\n\n    # #\n\n    length_0_1_3_2 = np.linalg.norm(p0_1 - p3_2)\n    cylinder_0_1_3_2 = create_mesh_cylinder(radius=0.025, height=length_0_1_3_2)\n    cylinder_0_1_3_2.compute_vertex_normals()\n    transform_0_1_3_2 = np.eye(4)\n    transform_0_1_3_2[0:3, 0:3] = Rmat\n    transform_0_1_3_2[0:3, 3] = p0_1_3_2\n    cylinder_0_1_3_2.transform(transform_0_1_3_2)\n    cylinder_0_1_3_2.paint_uniform_color(color)\n\n    return [cylinder_0_1_3_2, cylinder_0_3, cylinder_1_2, cylinder_4_7, cylinder_5_6, cylinder_0_1, cylinder_3_2, cylinder_4_5, cylinder_7_6, cylinder_0_4, cylinder_3_7, cylinder_1_5, cylinder_2_6]\n\ndef create3Dbbox_poly(center, h, w, l, r_y, P2_mat, type=""pred""):\n    if type == ""pred"":\n        color = [0, 190, 255] # (BGR)\n        front_color = [0, 0, 255] # (BGR)\n    else: # (if type == ""gt"":)\n        color = [190, 0, 255] # (BGR)\n        front_color = [255, 230, 0] # (BGR)\n\n    poly = {}\n\n    Rmat = np.asarray([[math.cos(r_y), 0, math.sin(r_y)],\n                       [0, 1, 0],\n                       [-math.sin(r_y), 0, math.cos(r_y)]],\n                       dtype=\'float32\')\n\n    p0 = center + np.dot(Rmat, np.asarray([l/2.0, 0, w/2.0], dtype=\'float32\').flatten())\n    p1 = center + np.dot(Rmat, np.asarray([-l/2.0, 0, w/2.0], dtype=\'float32\').flatten())\n    p2 = center + np.dot(Rmat, np.asarray([-l/2.0, 0, -w/2.0], dtype=\'float32\').flatten())\n    p3 = center + np.dot(Rmat, np.asarray([l/2.0, 0, -w/2.0], dtype=\'float32\').flatten())\n    p4 = center + np.dot(Rmat, np.asarray([l/2.0, -h, w/2.0], dtype=\'float32\').flatten())\n    p5 = center + np.dot(Rmat, np.asarray([-l/2.0, -h, w/2.0], dtype=\'float32\').flatten())\n    p6 = center + np.dot(Rmat, np.asarray([-l/2.0, -h, -w/2.0], dtype=\'float32\').flatten())\n    p7 = center + np.dot(Rmat, np.asarray([l/2.0, -h, -w/2.0], dtype=\'float32\').flatten())\n\n    poly[\'points\'] = np.array([p0, p1, p2, p3, p4, p5, p6, p7])\n    poly[\'lines\'] = [[0, 3, 7, 4, 0], [1, 2, 6, 5, 1], [0, 1], [2, 3], [6, 7], [4, 5]] # (0 -> 3 -> 7 -> 4 -> 0, 1 -> 2 -> 6 -> 5 -> 1, etc.)\n    poly[\'colors\'] = [front_color, color, color, color, color, color]\n    poly[\'P0_mat\'] = P2_mat\n\n    return poly\n\ndef create2Dbbox_poly(bbox2D):\n    u_min = bbox2D[0] # (left)\n    u_max = bbox2D[1] # (rigth)\n    v_min = bbox2D[2] # (top)\n    v_max = bbox2D[3] # (bottom)\n\n    poly = {}\n    poly[\'poly\'] = np.array([[u_min, v_min], [u_max, v_min],\n                             [u_max, v_max], [u_min, v_max]], dtype=\'int32\')\n\n    return poly\n\ndef draw_2d_polys(img, polys):\n    img = np.copy(img)\n    for poly in polys:\n        if \'color\' in poly:\n            bg = poly[\'color\']\n        else:\n            bg = np.array([0, 255, 0], dtype=\'float64\')\n\n        cv2.polylines(img, np.int32([poly[\'poly\']]), True, bg, lineType=cv2.LINE_AA, thickness=2)\n\n    return img\n\ndef draw_3d_polys(img, polys):\n    img = np.copy(img)\n    for poly in polys:\n        for n, line in enumerate(poly[\'lines\']):\n            if \'colors\' in poly:\n                bg = poly[\'colors\'][n]\n            else:\n                bg = np.array([255, 0, 0], dtype=\'float64\')\n\n            p3d = np.vstack((poly[\'points\'][line].T, np.ones((1, poly[\'points\'][line].shape[0]))))\n            p2d = np.dot(poly[\'P0_mat\'], p3d)\n\n            for m, p in enumerate(p2d[2, :]):\n                p2d[:, m] = p2d[:, m]/p\n\n            cv2.polylines(img, np.int32([p2d[:2, :].T]), False, bg, lineType=cv2.LINE_AA, thickness=2)\n\n    return img\n\ndef draw_geometries_dark_background(geometries):\n    vis = Visualizer()\n    vis.create_window()\n    opt = vis.get_render_option()\n    opt.background_color = np.asarray([0, 0, 0])\n    for geometry in geometries:\n        vis.add_geometry(geometry)\n    vis.run()\n    vis.destroy_window()\n\nsys.path.append(""/home/fregu856/3DOD_thesis/Open3D/build/lib"") # NOTE! you\'ll have to adapt this for your file structure\nfrom py3d import *\n\nproject_dir = ""/home/fregu856/3DOD_thesis/"" # NOTE! you\'ll have to adapt this for your file structure\ndata_dir = project_dir + ""data/kitti/object/testing/""\nimg_dir = data_dir + ""image_2/""\ncalib_dir = data_dir + ""calib/""\nlidar_dir = data_dir + ""velodyne/""\n\nimg_height = 375\nimg_width = 1242\n\n# NOTE! here you can choose what model\'s output you want to visualize\n# Frustum-PointNet:\nwith open(""/home/fregu856/3DOD_thesis/training_logs/model_Frustum-PointNet_eval_test/eval_dict_test.pkl"", ""rb"") as file: # NOTE! you\'ll have to adapt this for your file structure\n    eval_dict = pickle.load(file)\n#################################\n# # Extended-Frustum-PointNet:\n# with open(""/home/fregu856/3DOD_thesis/training_logs/model_Extended-Frustum-PointNet_eval_test/eval_dict_test.pkl"", ""rb"") as file: # NOTE! you\'ll have to adapt this for your file structure\n#     eval_dict = pickle.load(file)\n# ##################################\n# # Image-Only:\n# with open(""/home/fregu856/3DOD_thesis/training_logs/model_Image-Only_eval_test/eval_dict_test.pkl"", ""rb"") as file: # NOTE! you\'ll have to adapt this for your file structure\n#     eval_dict = pickle.load(file)\n\nfor img_id in eval_dict:\n    # NOTE! remove this if statement in case you have access to the complete KITTI dataset\n    if img_id in [""000000"", ""000001"", ""000002"", ""000003"", ""000004"", ""000005"", ""000006"", ""000007"", ""000008"", ""000009"", ""000010""]:\n        print img_id\n\n        bbox_dicts = eval_dict[img_id]\n\n        img = cv2.imread(img_dir + img_id + "".png"", -1)\n\n        lidar_path = lidar_dir + img_id + "".bin""\n        point_cloud = np.fromfile(lidar_path, dtype=np.float32).reshape(-1, 4)\n\n        # remove points that are located behind the camera:\n        point_cloud = point_cloud[point_cloud[:, 0] > -2.5, :]\n\n        calib = calibread(calib_dir + img_id + "".txt"")\n        P2 = calib[""P2""]\n        Tr_velo_to_cam_orig = calib[""Tr_velo_to_cam""]\n        R0_rect_orig = calib[""R0_rect""]\n        #\n        R0_rect = np.eye(4)\n        R0_rect[0:3, 0:3] = R0_rect_orig\n        #\n        Tr_velo_to_cam = np.eye(4)\n        Tr_velo_to_cam[0:3, :] = Tr_velo_to_cam_orig\n\n        point_cloud_xyz = point_cloud[:, 0:3]\n        point_cloud_xyz_hom = np.ones((point_cloud.shape[0], 4))\n        point_cloud_xyz_hom[:, 0:3] = point_cloud[:, 0:3] # (point_cloud_xyz_hom has shape (num_points, 4))\n\n        # transform the points into (rectified) camera coordinates:\n        point_cloud_xyz_camera_hom = np.dot(R0_rect, np.dot(Tr_velo_to_cam, point_cloud_xyz_hom.T)).T # (point_cloud_xyz_hom.T has shape (4, num_points))\n        # normalize:\n        point_cloud_xyz_camera = np.zeros((point_cloud_xyz_camera_hom.shape[0], 3))\n        point_cloud_xyz_camera[:, 0] = point_cloud_xyz_camera_hom[:, 0]/point_cloud_xyz_camera_hom[:, 3]\n        point_cloud_xyz_camera[:, 1] = point_cloud_xyz_camera_hom[:, 1]/point_cloud_xyz_camera_hom[:, 3]\n        point_cloud_xyz_camera[:, 2] = point_cloud_xyz_camera_hom[:, 2]/point_cloud_xyz_camera_hom[:, 3]\n\n        pcd = PointCloud()\n        pcd.points = Vector3dVector(point_cloud_xyz_camera)\n        pcd.paint_uniform_color([0.65, 0.65, 0.65])\n\n        pred_bboxes = []\n        pred_bbox_polys = []\n        input_2Dbbox_polys = []\n        pred_seg_pcds = []\n        for bbox_dict in bbox_dicts:\n            # frustum_point_cloud = bbox_dict[""frustum_point_cloud""]\n            # pred_seg_point_cloud = bbox_dict[""pred_seg_point_cloud""]\n            input_2Dbbox = bbox_dict[""input_2Dbbox""]\n            pred_center_BboxNet = bbox_dict[""pred_center_BboxNet""]\n            pred_h = bbox_dict[""pred_h""]\n            pred_w = bbox_dict[""pred_w""]\n            pred_l = bbox_dict[""pred_l""]\n            pred_r_y = bbox_dict[""pred_r_y""]\n\n            # pred_seg_pcd = PointCloud()\n            # pred_seg_pcd.points = Vector3dVector(pred_seg_point_cloud[:, 0:3])\n            # color_value = np.random.uniform(low=0.0, high=1.0)\n            # pred_seg_pcd.paint_uniform_color([color_value, 1-color_value, np.random.uniform(low=0.0, high=1.0)])\n            # pred_seg_pcds.append(pred_seg_pcd)\n\n            input_2Dbbox_poly = create2Dbbox_poly(input_2Dbbox)\n            input_2Dbbox_polys.append(input_2Dbbox_poly)\n\n            pred_bbox_poly = create3Dbbox_poly(pred_center_BboxNet, pred_h, pred_w, pred_l, pred_r_y, P2, type=""pred"")\n            pred_bbox_polys.append(pred_bbox_poly)\n\n            pred_bbox = create3Dbbox(pred_center_BboxNet, pred_h, pred_w, pred_l, pred_r_y, type=""pred"")\n            pred_bboxes += pred_bbox\n\n        img_with_input_2Dbboxes = draw_2d_polys(img, input_2Dbbox_polys)\n        img_with_input_2Dbboxes = cv2.resize(img_with_input_2Dbboxes, (img_width, img_height))\n\n        img_with_pred_bboxes = draw_3d_polys(img, pred_bbox_polys)\n        img_with_pred_bboxes = cv2.resize(img_with_pred_bboxes, (img_width, img_height))\n\n        combined_img = np.zeros((2*img_height, img_width, 3), dtype=np.uint8)\n        combined_img[0:img_height] = img_with_input_2Dbboxes\n        combined_img[img_height:] = img_with_pred_bboxes\n        cv2.imwrite(""visualization_eval_test.png"", combined_img)\n\n        draw_geometries_dark_background(pred_seg_pcds + pred_bboxes + [pcd])\n'"
visualization/visualize_eval_test_seq.py,0,"b'# camera-ready\n\nimport pickle\nimport numpy as np\nimport math\nimport cv2\nimport os\n\nimport sys\nsys.path.append(""/home/fregu856/3DOD_thesis/Open3D/build/lib"") # NOTE! you\'ll have to adapt this for your file structure\nfrom py3d import *\n\nsys.path.append(""/home/fregu856/3DOD_thesis/utils"") # NOTE! you\'ll have to adapt this for your file structure\nfrom kittiloader import LabelLoader2D3D, calibread\n\ndef create3Dbbox(center, h, w, l, r_y, type=""pred""):\n    if type == ""pred"":\n        color = [1, 0.75, 0] # (normalized RGB)\n        front_color = [1, 0, 0] # (normalized RGB)\n    else: # (if type == ""gt"":)\n        color = [1, 0, 0.75] # (normalized RGB)\n        front_color = [0, 0.9, 1] # (normalized RGB)\n\n    Rmat = np.asarray([[math.cos(r_y), 0, math.sin(r_y)],\n                       [0, 1, 0],\n                       [-math.sin(r_y), 0, math.cos(r_y)]],\n                       dtype=\'float32\')\n\n    Rmat_90 = np.asarray([[math.cos(r_y+np.pi/2), 0, math.sin(r_y+np.pi/2)],\n                          [0, 1, 0],\n                          [-math.sin(r_y+np.pi/2), 0, math.cos(r_y+np.pi/2)]],\n                          dtype=\'float32\')\n\n    Rmat_90_x = np.asarray([[1, 0, 0],\n                            [0, math.cos(np.pi/2), math.sin(np.pi/2)],\n                            [0, -math.sin(np.pi/2), math.cos(np.pi/2)]],\n                            dtype=\'float32\')\n\n    p0 = center + np.dot(Rmat, np.asarray([l/2.0, 0, w/2.0], dtype=\'float32\').flatten())\n    p1 = center + np.dot(Rmat, np.asarray([-l/2.0, 0, w/2.0], dtype=\'float32\').flatten())\n    p2 = center + np.dot(Rmat, np.asarray([-l/2.0, 0, -w/2.0], dtype=\'float32\').flatten())\n    p3 = center + np.dot(Rmat, np.asarray([l/2.0, 0, -w/2.0], dtype=\'float32\').flatten())\n    p4 = center + np.dot(Rmat, np.asarray([l/2.0, -h, w/2.0], dtype=\'float32\').flatten())\n    p5 = center + np.dot(Rmat, np.asarray([-l/2.0, -h, w/2.0], dtype=\'float32\').flatten())\n    p6 = center + np.dot(Rmat, np.asarray([-l/2.0, -h, -w/2.0], dtype=\'float32\').flatten())\n    p7 = center + np.dot(Rmat, np.asarray([l/2.0, -h, -w/2.0], dtype=\'float32\').flatten())\n\n    p0_3 = center + np.dot(Rmat, np.asarray([l/2.0, 0, 0], dtype=\'float32\').flatten())\n    p1_2 = center + np.dot(Rmat, np.asarray([-l/2.0, 0, 0], dtype=\'float32\').flatten())\n    p4_7 = center + np.dot(Rmat, np.asarray([l/2.0, -h, 0], dtype=\'float32\').flatten())\n    p5_6 = center + np.dot(Rmat, np.asarray([-l/2.0, -h, 0], dtype=\'float32\').flatten())\n    p0_1 = center + np.dot(Rmat, np.asarray([0, 0, w/2.0], dtype=\'float32\').flatten())\n    p3_2 = center + np.dot(Rmat, np.asarray([0, 0, -w/2.0], dtype=\'float32\').flatten())\n    p4_5 = center + np.dot(Rmat, np.asarray([0, -h, w/2.0], dtype=\'float32\').flatten())\n    p7_6 = center + np.dot(Rmat, np.asarray([0, -h, -w/2.0], dtype=\'float32\').flatten())\n    p0_4 = center + np.dot(Rmat, np.asarray([l/2.0, -h/2.0, w/2.0], dtype=\'float32\').flatten())\n    p3_7 = center + np.dot(Rmat, np.asarray([l/2.0, -h/2.0, -w/2.0], dtype=\'float32\').flatten())\n    p1_5 = center + np.dot(Rmat, np.asarray([-l/2.0, -h/2.0, w/2.0], dtype=\'float32\').flatten())\n    p2_6 = center + np.dot(Rmat, np.asarray([-l/2.0, -h/2.0, -w/2.0], dtype=\'float32\').flatten())\n    p0_1_3_2 = center\n\n    length_0_3 = np.linalg.norm(p0 - p3)\n    cylinder_0_3 = create_mesh_cylinder(radius=0.035, height=length_0_3)\n    cylinder_0_3.compute_vertex_normals()\n    transform_0_3 = np.eye(4)\n    transform_0_3[0:3, 0:3] = Rmat\n    transform_0_3[0:3, 3] = p0_3\n    cylinder_0_3.transform(transform_0_3)\n    cylinder_0_3.paint_uniform_color(front_color)\n\n    length_1_2 = np.linalg.norm(p1 - p2)\n    cylinder_1_2 = create_mesh_cylinder(radius=0.035, height=length_1_2)\n    cylinder_1_2.compute_vertex_normals()\n    transform_1_2 = np.eye(4)\n    transform_1_2[0:3, 0:3] = Rmat\n    transform_1_2[0:3, 3] = p1_2\n    cylinder_1_2.transform(transform_1_2)\n    cylinder_1_2.paint_uniform_color(color)\n\n    length_4_7 = np.linalg.norm(p4 - p7)\n    cylinder_4_7 = create_mesh_cylinder(radius=0.035, height=length_4_7)\n    cylinder_4_7.compute_vertex_normals()\n    transform_4_7 = np.eye(4)\n    transform_4_7[0:3, 0:3] = Rmat\n    transform_4_7[0:3, 3] = p4_7\n    cylinder_4_7.transform(transform_4_7)\n    cylinder_4_7.paint_uniform_color(front_color)\n\n    length_5_6 = np.linalg.norm(p5 - p6)\n    cylinder_5_6 = create_mesh_cylinder(radius=0.035, height=length_5_6)\n    cylinder_5_6.compute_vertex_normals()\n    transform_5_6 = np.eye(4)\n    transform_5_6[0:3, 0:3] = Rmat\n    transform_5_6[0:3, 3] = p5_6\n    cylinder_5_6.transform(transform_5_6)\n    cylinder_5_6.paint_uniform_color(color)\n\n    # #\n\n    length_0_1 = np.linalg.norm(p0 - p1)\n    cylinder_0_1 = create_mesh_cylinder(radius=0.035, height=length_0_1)\n    cylinder_0_1.compute_vertex_normals()\n    transform_0_1 = np.eye(4)\n    transform_0_1[0:3, 0:3] = Rmat_90\n    transform_0_1[0:3, 3] = p0_1\n    cylinder_0_1.transform(transform_0_1)\n    cylinder_0_1.paint_uniform_color(color)\n\n    length_3_2 = np.linalg.norm(p3 - p2)\n    cylinder_3_2 = create_mesh_cylinder(radius=0.035, height=length_3_2)\n    cylinder_3_2.compute_vertex_normals()\n    transform_3_2 = np.eye(4)\n    transform_3_2[0:3, 0:3] = Rmat_90\n    transform_3_2[0:3, 3] = p3_2\n    cylinder_3_2.transform(transform_3_2)\n    cylinder_3_2.paint_uniform_color(color)\n\n    length_4_5 = np.linalg.norm(p4 - p5)\n    cylinder_4_5 = create_mesh_cylinder(radius=0.035, height=length_4_5)\n    cylinder_4_5.compute_vertex_normals()\n    transform_4_5 = np.eye(4)\n    transform_4_5[0:3, 0:3] = Rmat_90\n    transform_4_5[0:3, 3] = p4_5\n    cylinder_4_5.transform(transform_4_5)\n    cylinder_4_5.paint_uniform_color(color)\n\n    length_7_6 = np.linalg.norm(p7 - p6)\n    cylinder_7_6 = create_mesh_cylinder(radius=0.035, height=length_7_6)\n    cylinder_7_6.compute_vertex_normals()\n    transform_7_6 = np.eye(4)\n    transform_7_6[0:3, 0:3] = Rmat_90\n    transform_7_6[0:3, 3] = p7_6\n    cylinder_7_6.transform(transform_7_6)\n    cylinder_7_6.paint_uniform_color(color)\n\n    # #\n\n    length_0_4 = np.linalg.norm(p0 - p4)\n    cylinder_0_4 = create_mesh_cylinder(radius=0.035, height=length_0_4)\n    cylinder_0_4.compute_vertex_normals()\n    transform_0_4 = np.eye(4)\n    transform_0_4[0:3, 0:3] = np.dot(Rmat, Rmat_90_x)\n    transform_0_4[0:3, 3] = p0_4\n    cylinder_0_4.transform(transform_0_4)\n    cylinder_0_4.paint_uniform_color(front_color)\n\n    length_3_7 = np.linalg.norm(p3 - p7)\n    cylinder_3_7 = create_mesh_cylinder(radius=0.035, height=length_3_7)\n    cylinder_3_7.compute_vertex_normals()\n    transform_3_7 = np.eye(4)\n    transform_3_7[0:3, 0:3] = np.dot(Rmat, Rmat_90_x)\n    transform_3_7[0:3, 3] = p3_7\n    cylinder_3_7.transform(transform_3_7)\n    cylinder_3_7.paint_uniform_color(front_color)\n\n    length_1_5 = np.linalg.norm(p1 - p5)\n    cylinder_1_5 = create_mesh_cylinder(radius=0.035, height=length_1_5)\n    cylinder_1_5.compute_vertex_normals()\n    transform_1_5 = np.eye(4)\n    transform_1_5[0:3, 0:3] = np.dot(Rmat, Rmat_90_x)\n    transform_1_5[0:3, 3] = p1_5\n    cylinder_1_5.transform(transform_1_5)\n    cylinder_1_5.paint_uniform_color(color)\n\n    length_2_6 = np.linalg.norm(p2 - p6)\n    cylinder_2_6 = create_mesh_cylinder(radius=0.035, height=length_2_6)\n    cylinder_2_6.compute_vertex_normals()\n    transform_2_6 = np.eye(4)\n    transform_2_6[0:3, 0:3] = np.dot(Rmat, Rmat_90_x)\n    transform_2_6[0:3, 3] = p2_6\n    cylinder_2_6.transform(transform_2_6)\n    cylinder_2_6.paint_uniform_color(color)\n\n    # #\n\n    length_0_1_3_2 = np.linalg.norm(p0_1 - p3_2)\n    cylinder_0_1_3_2 = create_mesh_cylinder(radius=0.035, height=length_0_1_3_2)\n    cylinder_0_1_3_2.compute_vertex_normals()\n    transform_0_1_3_2 = np.eye(4)\n    transform_0_1_3_2[0:3, 0:3] = Rmat\n    transform_0_1_3_2[0:3, 3] = p0_1_3_2\n    cylinder_0_1_3_2.transform(transform_0_1_3_2)\n    cylinder_0_1_3_2.paint_uniform_color(color)\n\n    return [cylinder_0_1_3_2, cylinder_0_3, cylinder_1_2, cylinder_4_7, cylinder_5_6, cylinder_0_1, cylinder_3_2, cylinder_4_5, cylinder_7_6, cylinder_0_4, cylinder_3_7, cylinder_1_5, cylinder_2_6]\n\ndef create3Dbbox_poly(center, h, w, l, r_y, P2_mat, type=""pred""):\n    if type == ""pred"":\n        color = [0, 190, 255] # (BGR)\n        front_color = [0, 0, 255] # (BGR)\n    else: # (if type == ""gt"":)\n        color = [190, 0, 255] # (BGR)\n        front_color = [255, 230, 0] # (BGR)\n\n    poly = {}\n\n    Rmat = np.asarray([[math.cos(r_y), 0, math.sin(r_y)],\n                       [0, 1, 0],\n                       [-math.sin(r_y), 0, math.cos(r_y)]],\n                       dtype=\'float32\')\n\n    p0 = center + np.dot(Rmat, np.asarray([l/2.0, 0, w/2.0], dtype=\'float32\').flatten())\n    p1 = center + np.dot(Rmat, np.asarray([-l/2.0, 0, w/2.0], dtype=\'float32\').flatten())\n    p2 = center + np.dot(Rmat, np.asarray([-l/2.0, 0, -w/2.0], dtype=\'float32\').flatten())\n    p3 = center + np.dot(Rmat, np.asarray([l/2.0, 0, -w/2.0], dtype=\'float32\').flatten())\n    p4 = center + np.dot(Rmat, np.asarray([l/2.0, -h, w/2.0], dtype=\'float32\').flatten())\n    p5 = center + np.dot(Rmat, np.asarray([-l/2.0, -h, w/2.0], dtype=\'float32\').flatten())\n    p6 = center + np.dot(Rmat, np.asarray([-l/2.0, -h, -w/2.0], dtype=\'float32\').flatten())\n    p7 = center + np.dot(Rmat, np.asarray([l/2.0, -h, -w/2.0], dtype=\'float32\').flatten())\n\n    poly[\'points\'] = np.array([p0, p1, p2, p3, p4, p5, p6, p7])\n    poly[\'lines\'] = [[0, 3, 7, 4, 0], [1, 2, 6, 5, 1], [0, 1], [2, 3], [6, 7], [4, 5]] # (0 -> 3 -> 7 -> 4 -> 0, 1 -> 2 -> 6 -> 5 -> 1, etc.)\n    poly[\'colors\'] = [front_color, color, color, color, color, color]\n    poly[\'P0_mat\'] = P2_mat\n\n    return poly\n\ndef create2Dbbox_poly(bbox2D):\n    u_min = bbox2D[0] # (left)\n    u_max = bbox2D[1] # (rigth)\n    v_min = bbox2D[2] # (top)\n    v_max = bbox2D[3] # (bottom)\n\n    poly = {}\n    poly[\'poly\'] = np.array([[u_min, v_min], [u_max, v_min],\n                             [u_max, v_max], [u_min, v_max]], dtype=\'int32\')\n\n    return poly\n\ndef draw_2d_polys_no_text(img, polys):\n    img = np.copy(img)\n    for poly in polys:\n        if \'color\' in poly:\n            bg = poly[\'color\']\n        else:\n            bg = np.array([0, 255, 0], dtype=\'float64\')\n\n        cv2.polylines(img, np.int32([poly[\'poly\']]), True, bg, lineType=cv2.LINE_AA, thickness=2)\n\n    return img\n\ndef draw_3d_polys(img, polys):\n    img = np.copy(img)\n    for poly in polys:\n        for n, line in enumerate(poly[\'lines\']):\n            if \'colors\' in poly:\n                bg = poly[\'colors\'][n]\n            else:\n                bg = np.array([255, 0, 0], dtype=\'float64\')\n\n            p3d = np.vstack((poly[\'points\'][line].T, np.ones((1, poly[\'points\'][line].shape[0]))))\n            p2d = np.dot(poly[\'P0_mat\'], p3d)\n\n            for m, p in enumerate(p2d[2, :]):\n                p2d[:, m] = p2d[:, m]/p\n\n            cv2.polylines(img, np.int32([p2d[:2, :].T]), False, bg, lineType=cv2.LINE_AA, thickness=2)\n\n    return img\n\nfor sequence in [""0001"", ""0002"", ""0007"", ""0011""]:\n    print (sequence)\n\n    project_dir = ""/home/fregu856/3DOD_thesis/"" # NOTE! you\'ll have to adapt this for your file structure\n    data_dir = project_dir + ""data/kitti/tracking/testing/""\n    img_dir = data_dir + ""image_02/"" + sequence + ""/""\n    calib_path = project_dir + ""data/kitti/meta/tracking/testing/calib/"" + sequence + "".txt"" # NOTE! kitti/meta\n    lidar_dir = data_dir + ""velodyne/"" + sequence + ""/""\n\n    calib = calibread(calib_path)\n    P2 = calib[""P2""]\n    Tr_velo_to_cam_orig = calib[""Tr_velo_to_cam""]\n    R0_rect_orig = calib[""R0_rect""]\n\n    R0_rect = np.eye(4)\n    R0_rect[0:3, 0:3] = R0_rect_orig\n\n    Tr_velo_to_cam = np.eye(4)\n    Tr_velo_to_cam[0:3, :] = Tr_velo_to_cam_orig\n\n    # NOTE! here you can choose what model\'s output you want to visualize\n    # Frustum-PointNet:\n    with open(""/home/fregu856/3DOD_thesis/training_logs/model_Frustum-PointNet_eval_test_seq/eval_dict_test_seq_%s.pkl"" % sequence, ""rb"") as file: # NOTE! you\'ll have to adapt this for your file structure\n        eval_dict = pickle.load(file)\n    #################################\n    # # Extended-Frustum-PointNet:\n    # with open(""/home/fregu856/3DOD_thesis/training_logs/model_Extended-Frustum-PointNet_eval_test_seq/eval_dict_test_seq_%s.pkl"" % sequence, ""rb"") as file: # NOTE! you\'ll have to adapt this for your file structure\n    #     eval_dict = pickle.load(file)\n    # #################################\n    # # Image-Only:\n    # with open(""/home/fregu856/3DOD_thesis/training_logs/model_Image-Only_eval_test_seq/eval_dict_test_seq_%s.pkl"" % sequence, ""rb"") as file: # NOTE! you\'ll have to adapt this for your file structure\n    #     eval_dict = pickle.load(file)\n\n    img_data_dict = {}\n    for img_id in eval_dict:\n        data_dict = {}\n\n        bbox_dicts = eval_dict[img_id]\n\n        pred_bboxes = []\n        pred_bbox_polys = []\n        input_2Dbbox_polys = []\n        pred_seg_pcds = []\n        for bbox_dict in bbox_dicts:\n            # frustum_point_cloud = bbox_dict[""frustum_point_cloud""]\n            # pred_seg_point_cloud = bbox_dict[""pred_seg_point_cloud""]\n            input_2Dbbox = bbox_dict[""input_2Dbbox""]\n            pred_center_BboxNet = bbox_dict[""pred_center_BboxNet""]\n            pred_h = bbox_dict[""pred_h""]\n            pred_w = bbox_dict[""pred_w""]\n            pred_l = bbox_dict[""pred_l""]\n            pred_r_y = bbox_dict[""pred_r_y""]\n\n            # pred_seg_pcd = PointCloud()\n            # pred_seg_pcd.points = Vector3dVector(pred_seg_point_cloud[:, 0:3])\n            # pred_seg_pcd.paint_uniform_color([1, 0, 0])\n            # pred_seg_pcds.append(pred_seg_pcd)\n\n            input_2Dbbox_poly = create2Dbbox_poly(input_2Dbbox)\n            input_2Dbbox_polys.append(input_2Dbbox_poly)\n\n            pred_bbox_poly = create3Dbbox_poly(pred_center_BboxNet, pred_h, pred_w, pred_l, pred_r_y, P2, type=""pred"")\n            pred_bbox_polys.append(pred_bbox_poly)\n\n            pred_bbox = create3Dbbox(pred_center_BboxNet, pred_h, pred_w, pred_l, pred_r_y, type=""pred"")\n            pred_bboxes += pred_bbox\n\n        data_dict[""pred_bboxes""] = pred_bboxes\n        data_dict[""pred_bbox_polys""] = pred_bbox_polys\n        data_dict[""input_2Dbbox_polys""] = input_2Dbbox_polys\n        data_dict[""pred_seg_pcds""] = pred_seg_pcds\n\n        img_data_dict[img_id] = data_dict\n\n    sorted_img_ids = []\n    img_names = sorted(os.listdir(img_dir))\n    for img_name in img_names:\n        img_id = img_name.split("".png"")[0]\n        sorted_img_ids.append(img_id)\n\n    img_height = 375\n    img_width = 1242\n\n    small_img_height = 187\n    small_img_width = 620\n\n    # ################################################################################\n    # # create a video of images (no bboxes):\n    # ################################################################################\n    # out = cv2.VideoWriter(""eval_test_seq_%s_img.avi"" % sequence, cv2.VideoWriter_fourcc(*\'H264\'), 12, (img_width, img_height), True)\n    #\n    # for img_id in sorted_img_ids:\n    #     print img_id\n    #\n    #     img = cv2.imread(img_dir + img_id + "".png"", -1)\n    #\n    #     img = cv2.resize(img, (img_width, img_height)) # (the image MUST have the size specified in VideoWriter)\n    #\n    #     out.write(img)\n\n    # ################################################################################\n    # # create a video of images with pred:\n    # ################################################################################\n    # out = cv2.VideoWriter(""eval_test_seq_%s_img_pred.avi"" % sequence, cv2.VideoWriter_fourcc(*\'H264\'), 12, (img_width, img_height), True)\n    #\n    # for img_id in sorted_img_ids:\n    #     print img_id\n    #\n    #     img = cv2.imread(img_dir + img_id + "".png"", -1)\n    #\n    #     img_with_pred_bboxes = img\n    #\n    #     if img_id in img_data_dict:\n    #         data_dict = img_data_dict[img_id]\n    #         pred_bbox_polys = data_dict[""pred_bbox_polys""]\n    #\n    #         img_with_pred_bboxes = draw_3d_polys(img, pred_bbox_polys)\n    #\n    #     img_with_pred_bboxes = cv2.resize(img_with_pred_bboxes, (img_width, img_height)) # (the image MUST have the size specified in VideoWriter)\n    #\n    #     out.write(img_with_pred_bboxes)\n\n    # ################################################################################\n    # # create a video of images with input 2Dbboxes on top of pred 3Dbboxes:\n    # ################################################################################\n    # out = cv2.VideoWriter(""eval_test_seq_%s_img_input_pred.avi"" % sequence, cv2.VideoWriter_fourcc(*\'H264\'), 12, (img_width, 2*img_height), True)\n    #\n    # for img_id in sorted_img_ids:\n    #     print img_id\n    #\n    #     img = cv2.imread(img_dir + img_id + "".png"", -1)\n    #\n    #     img_with_input_2Dbboxes = img\n    #     img_with_pred_bboxes = img\n    #\n    #     if img_id in img_data_dict:\n    #         data_dict = img_data_dict[img_id]\n    #         input_2Dbbox_polys = data_dict[""input_2Dbbox_polys""]\n    #         pred_bbox_polys = data_dict[""pred_bbox_polys""]\n    #\n    #         img_with_input_2Dbboxes = draw_2d_polys_no_text(img, input_2Dbbox_polys)\n    #         img_with_pred_bboxes = draw_3d_polys(img, pred_bbox_polys)\n    #\n    #     img_with_input_2Dbboxes = cv2.resize(img_with_input_2Dbboxes, (img_width, img_height)) # (the image MUST have the size specified in VideoWriter)\n    #     img_with_pred_bboxes = cv2.resize(img_with_pred_bboxes, (img_width, img_height)) # (the image MUST have the size specified in VideoWriter)\n    #\n    #     combined_img = np.zeros((2*img_height, img_width, 3), dtype=np.uint8)\n    #     combined_img[0:img_height] = img_with_input_2Dbboxes\n    #     combined_img[img_height:] = img_with_pred_bboxes\n    #\n    #     out.write(combined_img)\n\n    class ImgCreatorLiDAR:\n        def __init__(self):\n            self.counter = 0\n            self.trajectory = read_pinhole_camera_trajectory(""/home/fregu856/3DOD_thesis/visualization/camera_trajectory.json"") # NOTE! you\'ll have to adapt this for your file structure\n\n        def move_forward(self, vis):\n            # this function is called within the Visualizer::run() loop.\n            # the run loop calls the function, then re-renders the image.\n\n            if self.counter < 2: # (the counter is for making sure the camera view has been changed before the img is captured)\n                # set the camera view:\n                ctr = vis.get_view_control()\n                ctr.convert_from_pinhole_camera_parameters(self.trajectory.intrinsic, self.trajectory.extrinsic[0])\n\n                self.counter += 1\n            else:\n                # capture an image:\n                img = vis.capture_screen_float_buffer()\n                img = 255*np.asarray(img)\n                img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n                img = img.astype(np.uint8)\n                self.lidar_img = img\n\n                # close the window:\n                vis.destroy_window()\n\n                self.counter = 0\n\n            return False\n\n        def create_img(self, geometries):\n            vis = Visualizer()\n            vis.create_window()\n            opt = vis.get_render_option()\n            opt.background_color = np.asarray([0, 0, 0])\n            for geometry in geometries:\n                vis.add_geometry(geometry)\n            vis.register_animation_callback(self.move_forward)\n            vis.run()\n\n            return self.lidar_img\n\n    # ################################################################################\n    # # create a video of LiDAR (no bboxes):\n    # ################################################################################\n    # out_lidar = cv2.VideoWriter(""eval_test_seq_%s_lidar.avi"" % sequence, cv2.VideoWriter_fourcc(*\'H264\'), 12, (1920, 1080), True)\n    #\n    # lidar_img_creator = ImgCreatorLiDAR()\n    # for img_id in sorted_img_ids:\n    #     print img_id\n    #\n    #     lidar_path = lidar_dir + img_id + "".bin""\n    #     point_cloud = np.fromfile(lidar_path, dtype=np.float32).reshape(-1, 4)\n    #\n    #     # remove points that are located behind the camera:\n    #     point_cloud = point_cloud[point_cloud[:, 0] > -2.5, :]\n    #\n    #     point_cloud_xyz = point_cloud[:, 0:3]\n    #     point_cloud_xyz_hom = np.ones((point_cloud.shape[0], 4))\n    #     point_cloud_xyz_hom[:, 0:3] = point_cloud[:, 0:3] # (point_cloud_xyz_hom has shape (num_points, 4))\n    #\n    #     # transform the points into (rectified) camera coordinates:\n    #     point_cloud_xyz_camera_hom = np.dot(R0_rect, np.dot(Tr_velo_to_cam, point_cloud_xyz_hom.T)).T # (point_cloud_xyz_hom.T has shape (4, num_points))\n    #     # normalize:\n    #     point_cloud_xyz_camera = np.zeros((point_cloud_xyz_camera_hom.shape[0], 3))\n    #     point_cloud_xyz_camera[:, 0] = point_cloud_xyz_camera_hom[:, 0]/point_cloud_xyz_camera_hom[:, 3]\n    #     point_cloud_xyz_camera[:, 1] = point_cloud_xyz_camera_hom[:, 1]/point_cloud_xyz_camera_hom[:, 3]\n    #     point_cloud_xyz_camera[:, 2] = point_cloud_xyz_camera_hom[:, 2]/point_cloud_xyz_camera_hom[:, 3]\n    #\n    #     pcd = PointCloud()\n    #     pcd.points = Vector3dVector(point_cloud_xyz_camera)\n    #     pcd.paint_uniform_color([0.65, 0.65, 0.65])\n    #\n    #     img = lidar_img_creator.create_img([pcd])\n    #     out_lidar.write(img)\n\n    # ################################################################################\n    # # create a video of LiDAR with pred:\n    # ################################################################################\n    # out_lidar_pred = cv2.VideoWriter(""eval_test_seq_%s_lidar_pred.avi"" % sequence, cv2.VideoWriter_fourcc(*\'H264\'), 12, (1920, 1080), True)\n    #\n    # lidar_img_creator = ImgCreatorLiDAR()\n    # for img_id in sorted_img_ids:\n    #     print img_id\n    #\n    #     lidar_path = lidar_dir + img_id + "".bin""\n    #     point_cloud = np.fromfile(lidar_path, dtype=np.float32).reshape(-1, 4)\n    #\n    #     # remove points that are located behind the camera:\n    #     point_cloud = point_cloud[point_cloud[:, 0] > -2.5, :]\n    #\n    #     point_cloud_xyz = point_cloud[:, 0:3]\n    #     point_cloud_xyz_hom = np.ones((point_cloud.shape[0], 4))\n    #     point_cloud_xyz_hom[:, 0:3] = point_cloud[:, 0:3] # (point_cloud_xyz_hom has shape (num_points, 4))\n    #\n    #     # transform the points into (rectified) camera coordinates:\n    #     point_cloud_xyz_camera_hom = np.dot(R0_rect, np.dot(Tr_velo_to_cam, point_cloud_xyz_hom.T)).T # (point_cloud_xyz_hom.T has shape (4, num_points))\n    #     # normalize:\n    #     point_cloud_xyz_camera = np.zeros((point_cloud_xyz_camera_hom.shape[0], 3))\n    #     point_cloud_xyz_camera[:, 0] = point_cloud_xyz_camera_hom[:, 0]/point_cloud_xyz_camera_hom[:, 3]\n    #     point_cloud_xyz_camera[:, 1] = point_cloud_xyz_camera_hom[:, 1]/point_cloud_xyz_camera_hom[:, 3]\n    #     point_cloud_xyz_camera[:, 2] = point_cloud_xyz_camera_hom[:, 2]/point_cloud_xyz_camera_hom[:, 3]\n    #\n    #     pcd = PointCloud()\n    #     pcd.points = Vector3dVector(point_cloud_xyz_camera)\n    #     pcd.paint_uniform_color([0.65, 0.65, 0.65])\n    #\n    #     pred_bboxes = []\n    #\n    #     if img_id in img_data_dict:\n    #         data_dict = img_data_dict[img_id]\n    #         pred_bboxes = data_dict[""pred_bboxes""]\n    #\n    #     img = lidar_img_creator.create_img(pred_bboxes + [pcd])\n    #     out_lidar_pred.write(img)\n\n    # ################################################################################\n    # # create a video of image and LiDAR (no bboxes):\n    # ################################################################################\n    # out_lidar_img = cv2.VideoWriter(""eval_test_seq_%s_lidar_img.avi"" % sequence, cv2.VideoWriter_fourcc(*\'H264\'), 12, (1920, 1080), True)\n    #\n    # lidar_img_creator = ImgCreatorLiDAR()\n    # for img_id in sorted_img_ids:\n    #     print img_id\n    #\n    #     img = cv2.imread(img_dir + img_id + "".png"", -1)\n    #     small_img = cv2.resize(img, (small_img_width, small_img_height))\n    #\n    #     lidar_path = lidar_dir + img_id + "".bin""\n    #     point_cloud = np.fromfile(lidar_path, dtype=np.float32).reshape(-1, 4)\n    #\n    #     # remove points that are located behind the camera:\n    #     point_cloud = point_cloud[point_cloud[:, 0] > -2.5, :]\n    #\n    #     point_cloud_xyz = point_cloud[:, 0:3]\n    #     point_cloud_xyz_hom = np.ones((point_cloud.shape[0], 4))\n    #     point_cloud_xyz_hom[:, 0:3] = point_cloud[:, 0:3] # (point_cloud_xyz_hom has shape (num_points, 4))\n    #\n    #     # transform the points into (rectified) camera coordinates:\n    #     point_cloud_xyz_camera_hom = np.dot(R0_rect, np.dot(Tr_velo_to_cam, point_cloud_xyz_hom.T)).T # (point_cloud_xyz_hom.T has shape (4, num_points))\n    #     # normalize:\n    #     point_cloud_xyz_camera = np.zeros((point_cloud_xyz_camera_hom.shape[0], 3))\n    #     point_cloud_xyz_camera[:, 0] = point_cloud_xyz_camera_hom[:, 0]/point_cloud_xyz_camera_hom[:, 3]\n    #     point_cloud_xyz_camera[:, 1] = point_cloud_xyz_camera_hom[:, 1]/point_cloud_xyz_camera_hom[:, 3]\n    #     point_cloud_xyz_camera[:, 2] = point_cloud_xyz_camera_hom[:, 2]/point_cloud_xyz_camera_hom[:, 3]\n    #\n    #     pcd = PointCloud()\n    #     pcd.points = Vector3dVector(point_cloud_xyz_camera)\n    #     pcd.paint_uniform_color([0.65, 0.65, 0.65])\n    #\n    #     img_lidar = lidar_img_creator.create_img([pcd])\n    #\n    #     combined_img = img_lidar\n    #     combined_img[-small_img_height:, ((1920/2)-(small_img_width/2)):((1920/2)+(small_img_width/2))] = small_img\n    #\n    #     out_lidar_img.write(combined_img)\n\n    # ################################################################################\n    # # create a video of image and LiDAR with pred:\n    # ################################################################################\n    # out_lidar_img_pred = cv2.VideoWriter(""eval_test_seq_%s_lidar_img_pred.avi"" % sequence, cv2.VideoWriter_fourcc(*\'H264\'), 12, (1920, 1080), True)\n    #\n    # lidar_img_creator = ImgCreatorLiDAR()\n    # for img_id in sorted_img_ids:\n    #     print img_id\n    #\n    #     img = cv2.imread(img_dir + img_id + "".png"", -1)\n    #     img_with_pred_bboxes = img\n    #     if img_id in img_data_dict:\n    #         data_dict = img_data_dict[img_id]\n    #         pred_bbox_polys = data_dict[""pred_bbox_polys""]\n    #         img_with_pred_bboxes = draw_3d_polys(img, pred_bbox_polys)\n    #     small_img_with_pred_bboxes = cv2.resize(img_with_pred_bboxes, (small_img_width, small_img_height))\n    #\n    #     lidar_path = lidar_dir + img_id + "".bin""\n    #     point_cloud = np.fromfile(lidar_path, dtype=np.float32).reshape(-1, 4)\n    #\n    #     # remove points that are located behind the camera:\n    #     point_cloud = point_cloud[point_cloud[:, 0] > -2.5, :]\n    #\n    #     point_cloud_xyz = point_cloud[:, 0:3]\n    #     point_cloud_xyz_hom = np.ones((point_cloud.shape[0], 4))\n    #     point_cloud_xyz_hom[:, 0:3] = point_cloud[:, 0:3] # (point_cloud_xyz_hom has shape (num_points, 4))\n    #\n    #     # transform the points into (rectified) camera coordinates:\n    #     point_cloud_xyz_camera_hom = np.dot(R0_rect, np.dot(Tr_velo_to_cam, point_cloud_xyz_hom.T)).T # (point_cloud_xyz_hom.T has shape (4, num_points))\n    #     # normalize:\n    #     point_cloud_xyz_camera = np.zeros((point_cloud_xyz_camera_hom.shape[0], 3))\n    #     point_cloud_xyz_camera[:, 0] = point_cloud_xyz_camera_hom[:, 0]/point_cloud_xyz_camera_hom[:, 3]\n    #     point_cloud_xyz_camera[:, 1] = point_cloud_xyz_camera_hom[:, 1]/point_cloud_xyz_camera_hom[:, 3]\n    #     point_cloud_xyz_camera[:, 2] = point_cloud_xyz_camera_hom[:, 2]/point_cloud_xyz_camera_hom[:, 3]\n    #\n    #     pcd = PointCloud()\n    #     pcd.points = Vector3dVector(point_cloud_xyz_camera)\n    #     pcd.paint_uniform_color([0.65, 0.65, 0.65])\n    #\n    #     pred_bboxes = []\n    #\n    #     if img_id in img_data_dict:\n    #         data_dict = img_data_dict[img_id]\n    #         pred_bboxes = data_dict[""pred_bboxes""]\n    #\n    #     img_lidar = lidar_img_creator.create_img(pred_bboxes + [pcd])\n    #\n    #     combined_img = img_lidar\n    #     combined_img[-small_img_height:, ((1920/2)-(small_img_width/2)):((1920/2)+(small_img_width/2))] = small_img_with_pred_bboxes\n    #\n    #     out_lidar_img_pred.write(combined_img)\n\n    ################################################################################\n    # create a video of image and LiDAR with input 2Dbboxes and pred 3Dbboxes:\n    ################################################################################\n    out_lidar_img_pred = cv2.VideoWriter(""eval_test_seq_%s_lidar_img_input_pred.avi"" % sequence, cv2.VideoWriter_fourcc(*\'H264\'), 12, (1920, 1080), True)\n\n    lidar_img_creator = ImgCreatorLiDAR()\n    for img_id in sorted_img_ids:\n        print img_id\n\n        img = cv2.imread(img_dir + img_id + "".png"", -1)\n\n        img_with_input_2Dbboxes = img\n        img_with_pred_bboxes = img\n        if img_id in img_data_dict:\n            data_dict = img_data_dict[img_id]\n            input_2Dbbox_polys = data_dict[""input_2Dbbox_polys""]\n            pred_bbox_polys = data_dict[""pred_bbox_polys""]\n\n            img_with_input_2Dbboxes = draw_2d_polys_no_text(img, input_2Dbbox_polys)\n            img_with_pred_bboxes = draw_3d_polys(img, pred_bbox_polys)\n\n        small_img_with_input_2Dbboxes = cv2.resize(img_with_input_2Dbboxes, (small_img_width, small_img_height))\n        small_img_with_pred_bboxes = cv2.resize(img_with_pred_bboxes, (small_img_width, small_img_height))\n\n        lidar_path = lidar_dir + img_id + "".bin""\n        point_cloud = np.fromfile(lidar_path, dtype=np.float32).reshape(-1, 4)\n\n        # remove points that are located behind the camera:\n        point_cloud = point_cloud[point_cloud[:, 0] > -2.5, :]\n\n        point_cloud_xyz = point_cloud[:, 0:3]\n        point_cloud_xyz_hom = np.ones((point_cloud.shape[0], 4))\n        point_cloud_xyz_hom[:, 0:3] = point_cloud[:, 0:3] # (point_cloud_xyz_hom has shape (num_points, 4))\n\n        # transform the points into (rectified) camera coordinates:\n        point_cloud_xyz_camera_hom = np.dot(R0_rect, np.dot(Tr_velo_to_cam, point_cloud_xyz_hom.T)).T # (point_cloud_xyz_hom.T has shape (4, num_points))\n        # normalize:\n        point_cloud_xyz_camera = np.zeros((point_cloud_xyz_camera_hom.shape[0], 3))\n        point_cloud_xyz_camera[:, 0] = point_cloud_xyz_camera_hom[:, 0]/point_cloud_xyz_camera_hom[:, 3]\n        point_cloud_xyz_camera[:, 1] = point_cloud_xyz_camera_hom[:, 1]/point_cloud_xyz_camera_hom[:, 3]\n        point_cloud_xyz_camera[:, 2] = point_cloud_xyz_camera_hom[:, 2]/point_cloud_xyz_camera_hom[:, 3]\n\n        pcd = PointCloud()\n        pcd.points = Vector3dVector(point_cloud_xyz_camera)\n        pcd.paint_uniform_color([0.65, 0.65, 0.65])\n\n        pred_bboxes = []\n\n        if img_id in img_data_dict:\n            data_dict = img_data_dict[img_id]\n            pred_bboxes = data_dict[""pred_bboxes""]\n\n        img_lidar = lidar_img_creator.create_img(pred_bboxes + [pcd])\n\n        combined_img = img_lidar\n        combined_img[-small_img_height:, ((1920/2)-small_img_width-5):((1920/2)-5)] = small_img_with_input_2Dbboxes\n        combined_img[-small_img_height:, ((1920/2)+5):((1920/2)+5+small_img_width)] = small_img_with_pred_bboxes\n\n        out_lidar_img_pred.write(combined_img)\n'"
visualization/visualize_eval_val.py,0,"b'# camera-ready\n\nimport pickle\nimport numpy as np\nimport math\nimport cv2\n\nimport sys\nsys.path.append(""/home/fregu856/3DOD_thesis/utils"") # NOTE! you\'ll have to adapt this for your file structure\nfrom kittiloader import calibread\n\ndef create3Dbbox(center, h, w, l, r_y, type=""pred""):\n    if type == ""pred"":\n        color = [1, 0.75, 0] # (normalized RGB)\n        front_color = [1, 0, 0] # (normalized RGB)\n    else: # (if type == ""gt"":)\n        color = [1, 0, 0.75] # (normalized RGB)\n        front_color = [0, 0.9, 1] # (normalized RGB)\n\n    Rmat = np.asarray([[math.cos(r_y), 0, math.sin(r_y)],\n                       [0, 1, 0],\n                       [-math.sin(r_y), 0, math.cos(r_y)]],\n                       dtype=\'float32\')\n\n    Rmat_90 = np.asarray([[math.cos(r_y+np.pi/2), 0, math.sin(r_y+np.pi/2)],\n                          [0, 1, 0],\n                          [-math.sin(r_y+np.pi/2), 0, math.cos(r_y+np.pi/2)]],\n                          dtype=\'float32\')\n\n    Rmat_90_x = np.asarray([[1, 0, 0],\n                            [0, math.cos(np.pi/2), math.sin(np.pi/2)],\n                            [0, -math.sin(np.pi/2), math.cos(np.pi/2)]],\n                            dtype=\'float32\')\n\n    p0 = center + np.dot(Rmat, np.asarray([l/2.0, 0, w/2.0], dtype=\'float32\').flatten())\n    p1 = center + np.dot(Rmat, np.asarray([-l/2.0, 0, w/2.0], dtype=\'float32\').flatten())\n    p2 = center + np.dot(Rmat, np.asarray([-l/2.0, 0, -w/2.0], dtype=\'float32\').flatten())\n    p3 = center + np.dot(Rmat, np.asarray([l/2.0, 0, -w/2.0], dtype=\'float32\').flatten())\n    p4 = center + np.dot(Rmat, np.asarray([l/2.0, -h, w/2.0], dtype=\'float32\').flatten())\n    p5 = center + np.dot(Rmat, np.asarray([-l/2.0, -h, w/2.0], dtype=\'float32\').flatten())\n    p6 = center + np.dot(Rmat, np.asarray([-l/2.0, -h, -w/2.0], dtype=\'float32\').flatten())\n    p7 = center + np.dot(Rmat, np.asarray([l/2.0, -h, -w/2.0], dtype=\'float32\').flatten())\n\n    p0_3 = center + np.dot(Rmat, np.asarray([l/2.0, 0, 0], dtype=\'float32\').flatten())\n    p1_2 = center + np.dot(Rmat, np.asarray([-l/2.0, 0, 0], dtype=\'float32\').flatten())\n    p4_7 = center + np.dot(Rmat, np.asarray([l/2.0, -h, 0], dtype=\'float32\').flatten())\n    p5_6 = center + np.dot(Rmat, np.asarray([-l/2.0, -h, 0], dtype=\'float32\').flatten())\n    p0_1 = center + np.dot(Rmat, np.asarray([0, 0, w/2.0], dtype=\'float32\').flatten())\n    p3_2 = center + np.dot(Rmat, np.asarray([0, 0, -w/2.0], dtype=\'float32\').flatten())\n    p4_5 = center + np.dot(Rmat, np.asarray([0, -h, w/2.0], dtype=\'float32\').flatten())\n    p7_6 = center + np.dot(Rmat, np.asarray([0, -h, -w/2.0], dtype=\'float32\').flatten())\n    p0_4 = center + np.dot(Rmat, np.asarray([l/2.0, -h/2.0, w/2.0], dtype=\'float32\').flatten())\n    p3_7 = center + np.dot(Rmat, np.asarray([l/2.0, -h/2.0, -w/2.0], dtype=\'float32\').flatten())\n    p1_5 = center + np.dot(Rmat, np.asarray([-l/2.0, -h/2.0, w/2.0], dtype=\'float32\').flatten())\n    p2_6 = center + np.dot(Rmat, np.asarray([-l/2.0, -h/2.0, -w/2.0], dtype=\'float32\').flatten())\n    p0_1_3_2 = center\n\n    length_0_3 = np.linalg.norm(p0 - p3)\n    cylinder_0_3 = create_mesh_cylinder(radius=0.025, height=length_0_3)\n    cylinder_0_3.compute_vertex_normals()\n    transform_0_3 = np.eye(4)\n    transform_0_3[0:3, 0:3] = Rmat\n    transform_0_3[0:3, 3] = p0_3\n    cylinder_0_3.transform(transform_0_3)\n    cylinder_0_3.paint_uniform_color(front_color)\n\n    length_1_2 = np.linalg.norm(p1 - p2)\n    cylinder_1_2 = create_mesh_cylinder(radius=0.025, height=length_1_2)\n    cylinder_1_2.compute_vertex_normals()\n    transform_1_2 = np.eye(4)\n    transform_1_2[0:3, 0:3] = Rmat\n    transform_1_2[0:3, 3] = p1_2\n    cylinder_1_2.transform(transform_1_2)\n    cylinder_1_2.paint_uniform_color(color)\n\n    length_4_7 = np.linalg.norm(p4 - p7)\n    cylinder_4_7 = create_mesh_cylinder(radius=0.025, height=length_4_7)\n    cylinder_4_7.compute_vertex_normals()\n    transform_4_7 = np.eye(4)\n    transform_4_7[0:3, 0:3] = Rmat\n    transform_4_7[0:3, 3] = p4_7\n    cylinder_4_7.transform(transform_4_7)\n    cylinder_4_7.paint_uniform_color(front_color)\n\n    length_5_6 = np.linalg.norm(p5 - p6)\n    cylinder_5_6 = create_mesh_cylinder(radius=0.025, height=length_5_6)\n    cylinder_5_6.compute_vertex_normals()\n    transform_5_6 = np.eye(4)\n    transform_5_6[0:3, 0:3] = Rmat\n    transform_5_6[0:3, 3] = p5_6\n    cylinder_5_6.transform(transform_5_6)\n    cylinder_5_6.paint_uniform_color(color)\n\n    # #\n\n    length_0_1 = np.linalg.norm(p0 - p1)\n    cylinder_0_1 = create_mesh_cylinder(radius=0.025, height=length_0_1)\n    cylinder_0_1.compute_vertex_normals()\n    transform_0_1 = np.eye(4)\n    transform_0_1[0:3, 0:3] = Rmat_90\n    transform_0_1[0:3, 3] = p0_1\n    cylinder_0_1.transform(transform_0_1)\n    cylinder_0_1.paint_uniform_color(color)\n\n    length_3_2 = np.linalg.norm(p3 - p2)\n    cylinder_3_2 = create_mesh_cylinder(radius=0.025, height=length_3_2)\n    cylinder_3_2.compute_vertex_normals()\n    transform_3_2 = np.eye(4)\n    transform_3_2[0:3, 0:3] = Rmat_90\n    transform_3_2[0:3, 3] = p3_2\n    cylinder_3_2.transform(transform_3_2)\n    cylinder_3_2.paint_uniform_color(color)\n\n    length_4_5 = np.linalg.norm(p4 - p5)\n    cylinder_4_5 = create_mesh_cylinder(radius=0.025, height=length_4_5)\n    cylinder_4_5.compute_vertex_normals()\n    transform_4_5 = np.eye(4)\n    transform_4_5[0:3, 0:3] = Rmat_90\n    transform_4_5[0:3, 3] = p4_5\n    cylinder_4_5.transform(transform_4_5)\n    cylinder_4_5.paint_uniform_color(color)\n\n    length_7_6 = np.linalg.norm(p7 - p6)\n    cylinder_7_6 = create_mesh_cylinder(radius=0.025, height=length_7_6)\n    cylinder_7_6.compute_vertex_normals()\n    transform_7_6 = np.eye(4)\n    transform_7_6[0:3, 0:3] = Rmat_90\n    transform_7_6[0:3, 3] = p7_6\n    cylinder_7_6.transform(transform_7_6)\n    cylinder_7_6.paint_uniform_color(color)\n\n    # #\n\n    length_0_4 = np.linalg.norm(p0 - p4)\n    cylinder_0_4 = create_mesh_cylinder(radius=0.025, height=length_0_4)\n    cylinder_0_4.compute_vertex_normals()\n    transform_0_4 = np.eye(4)\n    transform_0_4[0:3, 0:3] = np.dot(Rmat, Rmat_90_x)\n    transform_0_4[0:3, 3] = p0_4\n    cylinder_0_4.transform(transform_0_4)\n    cylinder_0_4.paint_uniform_color(front_color)\n\n    length_3_7 = np.linalg.norm(p3 - p7)\n    cylinder_3_7 = create_mesh_cylinder(radius=0.025, height=length_3_7)\n    cylinder_3_7.compute_vertex_normals()\n    transform_3_7 = np.eye(4)\n    transform_3_7[0:3, 0:3] = np.dot(Rmat, Rmat_90_x)\n    transform_3_7[0:3, 3] = p3_7\n    cylinder_3_7.transform(transform_3_7)\n    cylinder_3_7.paint_uniform_color(front_color)\n\n    length_1_5 = np.linalg.norm(p1 - p5)\n    cylinder_1_5 = create_mesh_cylinder(radius=0.025, height=length_1_5)\n    cylinder_1_5.compute_vertex_normals()\n    transform_1_5 = np.eye(4)\n    transform_1_5[0:3, 0:3] = np.dot(Rmat, Rmat_90_x)\n    transform_1_5[0:3, 3] = p1_5\n    cylinder_1_5.transform(transform_1_5)\n    cylinder_1_5.paint_uniform_color(color)\n\n    length_2_6 = np.linalg.norm(p2 - p6)\n    cylinder_2_6 = create_mesh_cylinder(radius=0.025, height=length_2_6)\n    cylinder_2_6.compute_vertex_normals()\n    transform_2_6 = np.eye(4)\n    transform_2_6[0:3, 0:3] = np.dot(Rmat, Rmat_90_x)\n    transform_2_6[0:3, 3] = p2_6\n    cylinder_2_6.transform(transform_2_6)\n    cylinder_2_6.paint_uniform_color(color)\n\n    # #\n\n    length_0_1_3_2 = np.linalg.norm(p0_1 - p3_2)\n    cylinder_0_1_3_2 = create_mesh_cylinder(radius=0.025, height=length_0_1_3_2)\n    cylinder_0_1_3_2.compute_vertex_normals()\n    transform_0_1_3_2 = np.eye(4)\n    transform_0_1_3_2[0:3, 0:3] = Rmat\n    transform_0_1_3_2[0:3, 3] = p0_1_3_2\n    cylinder_0_1_3_2.transform(transform_0_1_3_2)\n    cylinder_0_1_3_2.paint_uniform_color(color)\n\n    return [cylinder_0_1_3_2, cylinder_0_3, cylinder_1_2, cylinder_4_7, cylinder_5_6, cylinder_0_1, cylinder_3_2, cylinder_4_5, cylinder_7_6, cylinder_0_4, cylinder_3_7, cylinder_1_5, cylinder_2_6]\n\ndef create3Dbbox_poly(center, h, w, l, r_y, P2_mat, type=""pred""):\n    if type == ""pred"":\n        color = [0, 190, 255] # (BGR)\n        front_color = [0, 0, 255] # (BGR)\n    else: # (if type == ""gt"":)\n        color = [190, 0, 255] # (BGR)\n        front_color = [255, 230, 0] # (BGR)\n\n    poly = {}\n\n    Rmat = np.asarray([[math.cos(r_y), 0, math.sin(r_y)],\n                       [0, 1, 0],\n                       [-math.sin(r_y), 0, math.cos(r_y)]],\n                       dtype=\'float32\')\n\n    p0 = center + np.dot(Rmat, np.asarray([l/2.0, 0, w/2.0], dtype=\'float32\').flatten())\n    p1 = center + np.dot(Rmat, np.asarray([-l/2.0, 0, w/2.0], dtype=\'float32\').flatten())\n    p2 = center + np.dot(Rmat, np.asarray([-l/2.0, 0, -w/2.0], dtype=\'float32\').flatten())\n    p3 = center + np.dot(Rmat, np.asarray([l/2.0, 0, -w/2.0], dtype=\'float32\').flatten())\n    p4 = center + np.dot(Rmat, np.asarray([l/2.0, -h, w/2.0], dtype=\'float32\').flatten())\n    p5 = center + np.dot(Rmat, np.asarray([-l/2.0, -h, w/2.0], dtype=\'float32\').flatten())\n    p6 = center + np.dot(Rmat, np.asarray([-l/2.0, -h, -w/2.0], dtype=\'float32\').flatten())\n    p7 = center + np.dot(Rmat, np.asarray([l/2.0, -h, -w/2.0], dtype=\'float32\').flatten())\n\n    poly[\'points\'] = np.array([p0, p1, p2, p3, p4, p5, p6, p7])\n    poly[\'lines\'] = [[0, 3, 7, 4, 0], [1, 2, 6, 5, 1], [0, 1], [2, 3], [6, 7], [4, 5]] # (0 -> 3 -> 7 -> 4 -> 0, 1 -> 2 -> 6 -> 5 -> 1, etc.)\n    poly[\'colors\'] = [front_color, color, color, color, color, color]\n    poly[\'P0_mat\'] = P2_mat\n\n    return poly\n\ndef draw_3d_polys(img, polys):\n    img = np.copy(img)\n    for poly in polys:\n        for n, line in enumerate(poly[\'lines\']):\n            if \'colors\' in poly:\n                bg = poly[\'colors\'][n]\n            else:\n                bg = np.array([255, 0, 0], dtype=\'float64\')\n\n            p3d = np.vstack((poly[\'points\'][line].T, np.ones((1, poly[\'points\'][line].shape[0]))))\n            p2d = np.dot(poly[\'P0_mat\'], p3d)\n\n            for m, p in enumerate(p2d[2, :]):\n                p2d[:, m] = p2d[:, m]/p\n\n            cv2.polylines(img, np.int32([p2d[:2, :].T]), False, bg, lineType=cv2.LINE_AA, thickness=2)\n\n    return img\n\ndef draw_geometries_dark_background(geometries):\n    vis = Visualizer()\n    vis.create_window()\n    opt = vis.get_render_option()\n    opt.background_color = np.asarray([0, 0, 0])\n    for geometry in geometries:\n        vis.add_geometry(geometry)\n    vis.run()\n    vis.destroy_window()\n\nsys.path.append(""/home/fregu856/3DOD_thesis/Open3D/build/lib"") # NOTE! you\'ll have to adapt this for your file structure\nfrom py3d import *\n\nproject_dir = ""/home/fregu856/3DOD_thesis/"" # NOTE! you\'ll have to adapt this for your file structure\ndata_dir = project_dir + ""data/kitti/object/training/""\nimg_dir = data_dir + ""image_2/""\nlabel_dir = data_dir + ""label_2/""\ncalib_dir = data_dir + ""calib/""\nlidar_dir = data_dir + ""velodyne/""\n\n# NOTE! here you can choose what model\'s output you want to visualize\n# Frustum-PointNet:\nwith open(""/home/fregu856/3DOD_thesis/training_logs/model_Frustum-PointNet_eval_val/eval_dict_val.pkl"", ""rb"") as file: # NOTE! you\'ll have to adapt this for your file structure\n    eval_dict = pickle.load(file)\n#################################\n# # Extended-Frustum-PointNet:\n# with open(""/home/fregu856/3DOD_thesis/training_logs/model_Extended-Frustum-PointNet_eval_val/eval_dict_val.pkl"", ""rb"") as file: # NOTE! you\'ll have to adapt this for your file structure\n#     eval_dict = pickle.load(file)\n# ##################################\n# # Image-Only:\n# with open(""/home/fregu856/3DOD_thesis/training_logs/model_Image-Only_eval_val/eval_dict_val.pkl"", ""rb"") as file: # NOTE! you\'ll have to adapt this for your file structure\n#     eval_dict = pickle.load(file)\n\nfor img_id in eval_dict:\n    # NOTE! remove this if statement in case you have access to the complete KITTI dataset\n    if img_id in [""000006"", ""000007"", ""000008"", ""000009"", ""000010"", ""000011"", ""000012"", ""000013"", ""000014"", ""000015"", ""000016""]:\n        print img_id\n\n        bbox_dicts = eval_dict[img_id]\n\n        img = cv2.imread(img_dir + img_id + "".png"", -1)\n\n        lidar_path = lidar_dir + img_id + "".bin""\n        point_cloud = np.fromfile(lidar_path, dtype=np.float32).reshape(-1, 4)\n\n        # remove points that are located behind the camera:\n        point_cloud = point_cloud[point_cloud[:, 0] > -2.5, :]\n\n        calib = calibread(calib_dir + img_id + "".txt"")\n        P2 = calib[""P2""]\n        Tr_velo_to_cam_orig = calib[""Tr_velo_to_cam""]\n        R0_rect_orig = calib[""R0_rect""]\n        #\n        R0_rect = np.eye(4)\n        R0_rect[0:3, 0:3] = R0_rect_orig\n        #\n        Tr_velo_to_cam = np.eye(4)\n        Tr_velo_to_cam[0:3, :] = Tr_velo_to_cam_orig\n\n        point_cloud_xyz = point_cloud[:, 0:3]\n        point_cloud_xyz_hom = np.ones((point_cloud.shape[0], 4))\n        point_cloud_xyz_hom[:, 0:3] = point_cloud[:, 0:3] # (point_cloud_xyz_hom has shape (num_points, 4))\n\n        # transform the points into (rectified) camera coordinates:\n        point_cloud_xyz_camera_hom = np.dot(R0_rect, np.dot(Tr_velo_to_cam, point_cloud_xyz_hom.T)).T # (point_cloud_xyz_hom.T has shape (4, num_points))\n        # normalize:\n        point_cloud_xyz_camera = np.zeros((point_cloud_xyz_camera_hom.shape[0], 3))\n        point_cloud_xyz_camera[:, 0] = point_cloud_xyz_camera_hom[:, 0]/point_cloud_xyz_camera_hom[:, 3]\n        point_cloud_xyz_camera[:, 1] = point_cloud_xyz_camera_hom[:, 1]/point_cloud_xyz_camera_hom[:, 3]\n        point_cloud_xyz_camera[:, 2] = point_cloud_xyz_camera_hom[:, 2]/point_cloud_xyz_camera_hom[:, 3]\n\n        pcd = PointCloud()\n        pcd.points = Vector3dVector(point_cloud_xyz_camera)\n        pcd.paint_uniform_color([0.65, 0.65, 0.65])\n\n        gt_bboxes = []\n        pred_bboxes = []\n        gt_bbox_polys = []\n        pred_bbox_polys = []\n        for bbox_dict in bbox_dicts:\n            pred_center_BboxNet = bbox_dict[""pred_center_BboxNet""]\n            gt_center = bbox_dict[""gt_center""]\n            pred_h = bbox_dict[""pred_h""]\n            pred_w = bbox_dict[""pred_w""]\n            pred_l = bbox_dict[""pred_l""]\n            pred_r_y = bbox_dict[""pred_r_y""]\n            gt_h = bbox_dict[""gt_h""]\n            gt_w = bbox_dict[""gt_w""]\n            gt_l = bbox_dict[""gt_l""]\n            gt_r_y = bbox_dict[""gt_r_y""]\n\n            gt_bbox = create3Dbbox(gt_center, gt_h, gt_w, gt_l, gt_r_y, type=""gt"")\n            gt_bboxes += gt_bbox\n\n            pred_bbox = create3Dbbox(pred_center_BboxNet, pred_h, pred_w, pred_l, pred_r_y, type=""pred"")\n            pred_bboxes += pred_bbox\n\n            gt_bbox_poly = create3Dbbox_poly(gt_center, gt_h, gt_w, gt_l, gt_r_y, P2, type=""gt"")\n            gt_bbox_polys.append(gt_bbox_poly)\n\n            pred_bbox_poly = create3Dbbox_poly(pred_center_BboxNet, pred_h, pred_w, pred_l, pred_r_y, P2, type=""pred"")\n            pred_bbox_polys.append(pred_bbox_poly)\n\n        img_with_gt_bboxes = draw_3d_polys(img, gt_bbox_polys)\n        cv2.imwrite(""img_with_gt_bboxes.png"", img_with_gt_bboxes)\n\n        img_with_pred_bboxes = draw_3d_polys(img, pred_bbox_polys)\n        cv2.imwrite(""img_with_pred_bboxes.png"", img_with_pred_bboxes)\n\n        draw_geometries_dark_background(gt_bboxes + [pcd])\n\n        draw_geometries_dark_background(pred_bboxes + [pcd])\n\n        draw_geometries_dark_background(pred_bboxes + gt_bboxes + [pcd])\n'"
visualization/visualize_eval_val_extra.py,0,"b'# camera-ready\n\nimport pickle\nimport numpy as np\nimport math\nimport cv2\n\nimport sys\nsys.path.append(""/home/fregu856/3DOD_thesis/utils"") # NOTE! you\'ll have to adapt this for your file structure\nfrom kittiloader import LabelLoader2D3D, calibread\n\ndef create3Dbbox(center, h, w, l, r_y, type=""pred""):\n    if type == ""pred"":\n        color = [1, 0.75, 0] # (normalized RGB)\n        front_color = [1, 0, 0] # (normalized RGB)\n    else: # (if type == ""gt"":)\n        color = [1, 0, 0.75] # (normalized RGB)\n        front_color = [0, 0.9, 1] # (normalized RGB)\n\n    Rmat = np.asarray([[math.cos(r_y), 0, math.sin(r_y)],\n                       [0, 1, 0],\n                       [-math.sin(r_y), 0, math.cos(r_y)]],\n                       dtype=\'float32\')\n\n    Rmat_90 = np.asarray([[math.cos(r_y+np.pi/2), 0, math.sin(r_y+np.pi/2)],\n                          [0, 1, 0],\n                          [-math.sin(r_y+np.pi/2), 0, math.cos(r_y+np.pi/2)]],\n                          dtype=\'float32\')\n\n    Rmat_90_x = np.asarray([[1, 0, 0],\n                            [0, math.cos(np.pi/2), math.sin(np.pi/2)],\n                            [0, -math.sin(np.pi/2), math.cos(np.pi/2)]],\n                            dtype=\'float32\')\n\n    p0 = center + np.dot(Rmat, np.asarray([l/2.0, 0, w/2.0], dtype=\'float32\').flatten())\n    p1 = center + np.dot(Rmat, np.asarray([-l/2.0, 0, w/2.0], dtype=\'float32\').flatten())\n    p2 = center + np.dot(Rmat, np.asarray([-l/2.0, 0, -w/2.0], dtype=\'float32\').flatten())\n    p3 = center + np.dot(Rmat, np.asarray([l/2.0, 0, -w/2.0], dtype=\'float32\').flatten())\n    p4 = center + np.dot(Rmat, np.asarray([l/2.0, -h, w/2.0], dtype=\'float32\').flatten())\n    p5 = center + np.dot(Rmat, np.asarray([-l/2.0, -h, w/2.0], dtype=\'float32\').flatten())\n    p6 = center + np.dot(Rmat, np.asarray([-l/2.0, -h, -w/2.0], dtype=\'float32\').flatten())\n    p7 = center + np.dot(Rmat, np.asarray([l/2.0, -h, -w/2.0], dtype=\'float32\').flatten())\n\n    p0_3 = center + np.dot(Rmat, np.asarray([l/2.0, 0, 0], dtype=\'float32\').flatten())\n    p1_2 = center + np.dot(Rmat, np.asarray([-l/2.0, 0, 0], dtype=\'float32\').flatten())\n    p4_7 = center + np.dot(Rmat, np.asarray([l/2.0, -h, 0], dtype=\'float32\').flatten())\n    p5_6 = center + np.dot(Rmat, np.asarray([-l/2.0, -h, 0], dtype=\'float32\').flatten())\n    p0_1 = center + np.dot(Rmat, np.asarray([0, 0, w/2.0], dtype=\'float32\').flatten())\n    p3_2 = center + np.dot(Rmat, np.asarray([0, 0, -w/2.0], dtype=\'float32\').flatten())\n    p4_5 = center + np.dot(Rmat, np.asarray([0, -h, w/2.0], dtype=\'float32\').flatten())\n    p7_6 = center + np.dot(Rmat, np.asarray([0, -h, -w/2.0], dtype=\'float32\').flatten())\n    p0_4 = center + np.dot(Rmat, np.asarray([l/2.0, -h/2.0, w/2.0], dtype=\'float32\').flatten())\n    p3_7 = center + np.dot(Rmat, np.asarray([l/2.0, -h/2.0, -w/2.0], dtype=\'float32\').flatten())\n    p1_5 = center + np.dot(Rmat, np.asarray([-l/2.0, -h/2.0, w/2.0], dtype=\'float32\').flatten())\n    p2_6 = center + np.dot(Rmat, np.asarray([-l/2.0, -h/2.0, -w/2.0], dtype=\'float32\').flatten())\n    p0_1_3_2 = center\n\n    length_0_3 = np.linalg.norm(p0 - p3)\n    cylinder_0_3 = create_mesh_cylinder(radius=0.025, height=length_0_3)\n    cylinder_0_3.compute_vertex_normals()\n    transform_0_3 = np.eye(4)\n    transform_0_3[0:3, 0:3] = Rmat\n    transform_0_3[0:3, 3] = p0_3\n    cylinder_0_3.transform(transform_0_3)\n    cylinder_0_3.paint_uniform_color(front_color)\n\n    length_1_2 = np.linalg.norm(p1 - p2)\n    cylinder_1_2 = create_mesh_cylinder(radius=0.025, height=length_1_2)\n    cylinder_1_2.compute_vertex_normals()\n    transform_1_2 = np.eye(4)\n    transform_1_2[0:3, 0:3] = Rmat\n    transform_1_2[0:3, 3] = p1_2\n    cylinder_1_2.transform(transform_1_2)\n    cylinder_1_2.paint_uniform_color(color)\n\n    length_4_7 = np.linalg.norm(p4 - p7)\n    cylinder_4_7 = create_mesh_cylinder(radius=0.025, height=length_4_7)\n    cylinder_4_7.compute_vertex_normals()\n    transform_4_7 = np.eye(4)\n    transform_4_7[0:3, 0:3] = Rmat\n    transform_4_7[0:3, 3] = p4_7\n    cylinder_4_7.transform(transform_4_7)\n    cylinder_4_7.paint_uniform_color(front_color)\n\n    length_5_6 = np.linalg.norm(p5 - p6)\n    cylinder_5_6 = create_mesh_cylinder(radius=0.025, height=length_5_6)\n    cylinder_5_6.compute_vertex_normals()\n    transform_5_6 = np.eye(4)\n    transform_5_6[0:3, 0:3] = Rmat\n    transform_5_6[0:3, 3] = p5_6\n    cylinder_5_6.transform(transform_5_6)\n    cylinder_5_6.paint_uniform_color(color)\n\n    # #\n\n    length_0_1 = np.linalg.norm(p0 - p1)\n    cylinder_0_1 = create_mesh_cylinder(radius=0.025, height=length_0_1)\n    cylinder_0_1.compute_vertex_normals()\n    transform_0_1 = np.eye(4)\n    transform_0_1[0:3, 0:3] = Rmat_90\n    transform_0_1[0:3, 3] = p0_1\n    cylinder_0_1.transform(transform_0_1)\n    cylinder_0_1.paint_uniform_color(color)\n\n    length_3_2 = np.linalg.norm(p3 - p2)\n    cylinder_3_2 = create_mesh_cylinder(radius=0.025, height=length_3_2)\n    cylinder_3_2.compute_vertex_normals()\n    transform_3_2 = np.eye(4)\n    transform_3_2[0:3, 0:3] = Rmat_90\n    transform_3_2[0:3, 3] = p3_2\n    cylinder_3_2.transform(transform_3_2)\n    cylinder_3_2.paint_uniform_color(color)\n\n    length_4_5 = np.linalg.norm(p4 - p5)\n    cylinder_4_5 = create_mesh_cylinder(radius=0.025, height=length_4_5)\n    cylinder_4_5.compute_vertex_normals()\n    transform_4_5 = np.eye(4)\n    transform_4_5[0:3, 0:3] = Rmat_90\n    transform_4_5[0:3, 3] = p4_5\n    cylinder_4_5.transform(transform_4_5)\n    cylinder_4_5.paint_uniform_color(color)\n\n    length_7_6 = np.linalg.norm(p7 - p6)\n    cylinder_7_6 = create_mesh_cylinder(radius=0.025, height=length_7_6)\n    cylinder_7_6.compute_vertex_normals()\n    transform_7_6 = np.eye(4)\n    transform_7_6[0:3, 0:3] = Rmat_90\n    transform_7_6[0:3, 3] = p7_6\n    cylinder_7_6.transform(transform_7_6)\n    cylinder_7_6.paint_uniform_color(color)\n\n    # #\n\n    length_0_4 = np.linalg.norm(p0 - p4)\n    cylinder_0_4 = create_mesh_cylinder(radius=0.025, height=length_0_4)\n    cylinder_0_4.compute_vertex_normals()\n    transform_0_4 = np.eye(4)\n    transform_0_4[0:3, 0:3] = np.dot(Rmat, Rmat_90_x)\n    transform_0_4[0:3, 3] = p0_4\n    cylinder_0_4.transform(transform_0_4)\n    cylinder_0_4.paint_uniform_color(front_color)\n\n    length_3_7 = np.linalg.norm(p3 - p7)\n    cylinder_3_7 = create_mesh_cylinder(radius=0.025, height=length_3_7)\n    cylinder_3_7.compute_vertex_normals()\n    transform_3_7 = np.eye(4)\n    transform_3_7[0:3, 0:3] = np.dot(Rmat, Rmat_90_x)\n    transform_3_7[0:3, 3] = p3_7\n    cylinder_3_7.transform(transform_3_7)\n    cylinder_3_7.paint_uniform_color(front_color)\n\n    length_1_5 = np.linalg.norm(p1 - p5)\n    cylinder_1_5 = create_mesh_cylinder(radius=0.025, height=length_1_5)\n    cylinder_1_5.compute_vertex_normals()\n    transform_1_5 = np.eye(4)\n    transform_1_5[0:3, 0:3] = np.dot(Rmat, Rmat_90_x)\n    transform_1_5[0:3, 3] = p1_5\n    cylinder_1_5.transform(transform_1_5)\n    cylinder_1_5.paint_uniform_color(color)\n\n    length_2_6 = np.linalg.norm(p2 - p6)\n    cylinder_2_6 = create_mesh_cylinder(radius=0.025, height=length_2_6)\n    cylinder_2_6.compute_vertex_normals()\n    transform_2_6 = np.eye(4)\n    transform_2_6[0:3, 0:3] = np.dot(Rmat, Rmat_90_x)\n    transform_2_6[0:3, 3] = p2_6\n    cylinder_2_6.transform(transform_2_6)\n    cylinder_2_6.paint_uniform_color(color)\n\n    # #\n\n    length_0_1_3_2 = np.linalg.norm(p0_1 - p3_2)\n    cylinder_0_1_3_2 = create_mesh_cylinder(radius=0.025, height=length_0_1_3_2)\n    cylinder_0_1_3_2.compute_vertex_normals()\n    transform_0_1_3_2 = np.eye(4)\n    transform_0_1_3_2[0:3, 0:3] = Rmat\n    transform_0_1_3_2[0:3, 3] = p0_1_3_2\n    cylinder_0_1_3_2.transform(transform_0_1_3_2)\n    cylinder_0_1_3_2.paint_uniform_color(color)\n\n    return [cylinder_0_1_3_2, cylinder_0_3, cylinder_1_2, cylinder_4_7, cylinder_5_6, cylinder_0_1, cylinder_3_2, cylinder_4_5, cylinder_7_6, cylinder_0_4, cylinder_3_7, cylinder_1_5, cylinder_2_6]\n\ndef create3Dbbox_poly(center, h, w, l, r_y, P2_mat, type=""pred""):\n    if type == ""pred"":\n        color = [0, 190, 255] # (BGR)\n        front_color = [0, 0, 255] # (BGR)\n    else: # (if type == ""gt"":)\n        color = [190, 0, 255] # (BGR)\n        front_color = [255, 230, 0] # (BGR)\n\n    poly = {}\n\n    Rmat = np.asarray([[math.cos(r_y), 0, math.sin(r_y)],\n                       [0, 1, 0],\n                       [-math.sin(r_y), 0, math.cos(r_y)]],\n                       dtype=\'float32\')\n\n    p0 = center + np.dot(Rmat, np.asarray([l/2.0, 0, w/2.0], dtype=\'float32\').flatten())\n    p1 = center + np.dot(Rmat, np.asarray([-l/2.0, 0, w/2.0], dtype=\'float32\').flatten())\n    p2 = center + np.dot(Rmat, np.asarray([-l/2.0, 0, -w/2.0], dtype=\'float32\').flatten())\n    p3 = center + np.dot(Rmat, np.asarray([l/2.0, 0, -w/2.0], dtype=\'float32\').flatten())\n    p4 = center + np.dot(Rmat, np.asarray([l/2.0, -h, w/2.0], dtype=\'float32\').flatten())\n    p5 = center + np.dot(Rmat, np.asarray([-l/2.0, -h, w/2.0], dtype=\'float32\').flatten())\n    p6 = center + np.dot(Rmat, np.asarray([-l/2.0, -h, -w/2.0], dtype=\'float32\').flatten())\n    p7 = center + np.dot(Rmat, np.asarray([l/2.0, -h, -w/2.0], dtype=\'float32\').flatten())\n\n    poly[\'points\'] = np.array([p0, p1, p2, p3, p4, p5, p6, p7])\n    poly[\'lines\'] = [[0, 3, 7, 4, 0], [1, 2, 6, 5, 1], [0, 1], [2, 3], [6, 7], [4, 5]] # (0 -> 3 -> 7 -> 4 -> 0, 1 -> 2 -> 6 -> 5 -> 1, etc.)\n    poly[\'colors\'] = [front_color, color, color, color, color, color]\n    poly[\'P0_mat\'] = P2_mat\n\n    return poly\n\ndef create2Dbbox_poly(bbox2D):\n    u_min = bbox2D[0] # (left)\n    u_max = bbox2D[1] # (rigth)\n    v_min = bbox2D[2] # (top)\n    v_max = bbox2D[3] # (bottom)\n\n    poly = {}\n    poly[\'poly\'] = np.array([[u_min, v_min], [u_max, v_min],\n                             [u_max, v_max], [u_min, v_max]], dtype=\'int32\')\n\n    return poly\n\ndef draw_3d_polys(img, polys):\n    img = np.copy(img)\n    for poly in polys:\n        for n, line in enumerate(poly[\'lines\']):\n            if \'colors\' in poly:\n                bg = poly[\'colors\'][n]\n            else:\n                bg = np.array([255, 0, 0], dtype=\'float64\')\n\n            p3d = np.vstack((poly[\'points\'][line].T, np.ones((1, poly[\'points\'][line].shape[0]))))\n            p2d = np.dot(poly[\'P0_mat\'], p3d)\n\n            for m, p in enumerate(p2d[2, :]):\n                p2d[:, m] = p2d[:, m]/p\n\n            cv2.polylines(img, np.int32([p2d[:2, :].T]), False, bg, lineType=cv2.LINE_AA, thickness=2)\n\n    return img\n\ndef draw_2d_polys_no_text(img, polys):\n    img = np.copy(img)\n    for poly in polys:\n        if \'color\' in poly:\n            bg = poly[\'color\']\n        else:\n            bg = np.array([255, 0, 0], dtype=\'float64\')\n\n        cv2.polylines(img, np.int32([poly[\'poly\']]), True, bg, lineType=cv2.LINE_AA, thickness=2)\n\n    return img\n\ndef draw_geometries_dark_background(geometries):\n    vis = Visualizer()\n    vis.create_window()\n    opt = vis.get_render_option()\n    opt.background_color = np.asarray([0, 0, 0])\n    for geometry in geometries:\n        vis.add_geometry(geometry)\n    vis.run()\n    vis.destroy_window()\n\n\nsys.path.append(""/home/fregu856/3DOD_thesis/Open3D/build/lib"") # NOTE! you\'ll have to adapt this for your file structure\nfrom py3d import *\n\nproject_dir = ""/home/fregu856/3DOD_thesis/"" # NOTE! you\'ll have to adapt this for your file structure\ndata_dir = project_dir + ""data/kitti/object/training/""\nimg_dir = data_dir + ""image_2/""\nlabel_dir = data_dir + ""label_2/""\ncalib_dir = data_dir + ""calib/""\nlidar_dir = data_dir + ""velodyne/""\n\n# NOTE! here you can choose what model\'s output you want to visualize\n# Frustum-PointNet:\nwith open(""/home/fregu856/3DOD_thesis/training_logs/model_Frustum-PointNet_eval_val/eval_dict_val.pkl"", ""rb"") as file: # NOTE! you\'ll have to adapt this for your file structure\n    eval_dict = pickle.load(file)\n#################################\n# # Extended-Frustum-PointNet:\n# with open(""/home/fregu856/3DOD_thesis/training_logs/model_Extended-Frustum-PointNet_eval_val/eval_dict_val.pkl"", ""rb"") as file: # NOTE! you\'ll have to adapt this for your file structure\n#     eval_dict = pickle.load(file)\n\nfor img_id in eval_dict:\n    # NOTE! remove this if statement in case you have access to the complete KITTI dataset\n    if img_id in [""000006"", ""000007"", ""000008"", ""000009"", ""000010"", ""000011"", ""000012"", ""000013"", ""000014"", ""000015"", ""000016""]:\n        print img_id\n\n        bbox_dicts = eval_dict[img_id]\n\n        img = cv2.imread(img_dir + img_id + "".png"", -1)\n\n        lidar_path = lidar_dir + img_id + "".bin""\n        point_cloud = np.fromfile(lidar_path, dtype=np.float32).reshape(-1, 4)\n\n        # remove points that are located behind the camera:\n        point_cloud = point_cloud[point_cloud[:, 0] > -2.5, :]\n\n        calib = calibread(calib_dir + img_id + "".txt"")\n        P2 = calib[""P2""]\n        Tr_velo_to_cam_orig = calib[""Tr_velo_to_cam""]\n        R0_rect_orig = calib[""R0_rect""]\n        #\n        R0_rect = np.eye(4)\n        R0_rect[0:3, 0:3] = R0_rect_orig\n        #\n        Tr_velo_to_cam = np.eye(4)\n        Tr_velo_to_cam[0:3, :] = Tr_velo_to_cam_orig\n\n        point_cloud_xyz = point_cloud[:, 0:3]\n        point_cloud_xyz_hom = np.ones((point_cloud.shape[0], 4))\n        point_cloud_xyz_hom[:, 0:3] = point_cloud[:, 0:3] # (point_cloud_xyz_hom has shape (num_points, 4))\n\n        # transform the points into (rectified) camera coordinates:\n        point_cloud_xyz_camera_hom = np.dot(R0_rect, np.dot(Tr_velo_to_cam, point_cloud_xyz_hom.T)).T # (point_cloud_xyz_hom.T has shape (4, num_points))\n        # normalize:\n        point_cloud_xyz_camera = np.zeros((point_cloud_xyz_camera_hom.shape[0], 3))\n        point_cloud_xyz_camera[:, 0] = point_cloud_xyz_camera_hom[:, 0]/point_cloud_xyz_camera_hom[:, 3]\n        point_cloud_xyz_camera[:, 1] = point_cloud_xyz_camera_hom[:, 1]/point_cloud_xyz_camera_hom[:, 3]\n        point_cloud_xyz_camera[:, 2] = point_cloud_xyz_camera_hom[:, 2]/point_cloud_xyz_camera_hom[:, 3]\n\n        pcd = PointCloud()\n        pcd.points = Vector3dVector(point_cloud_xyz_camera)\n        pcd.paint_uniform_color([0.65, 0.65, 0.65])\n\n        for bbox_dict in bbox_dicts:\n            frustum_point_cloud = bbox_dict[""frustum_point_cloud""]\n            pred_seg_point_cloud = bbox_dict[""pred_seg_point_cloud""]\n            gt_seg_point_cloud = bbox_dict[""gt_seg_point_cloud""]\n            pred_center_TNet = bbox_dict[""pred_center_TNet""]\n            pred_center_BboxNet = bbox_dict[""pred_center_BboxNet""]\n            gt_center = bbox_dict[""gt_center""]\n            centroid = bbox_dict[""centroid""]\n            pred_h = bbox_dict[""pred_h""]\n            pred_w = bbox_dict[""pred_w""]\n            pred_l = bbox_dict[""pred_l""]\n            pred_r_y = bbox_dict[""pred_r_y""]\n            gt_h = bbox_dict[""gt_h""]\n            gt_w = bbox_dict[""gt_w""]\n            gt_l = bbox_dict[""gt_l""]\n            gt_r_y = bbox_dict[""gt_r_y""]\n            input_2Dbbox = bbox_dict[""input_2Dbbox""]\n\n            input_2Dbbox_poly = create2Dbbox_poly(input_2Dbbox)\n            img_with_input_2Dbbox = draw_2d_polys_no_text(img, [input_2Dbbox_poly])\n            cv2.imwrite(""img_with_input_2Dbbox.png"", img_with_input_2Dbbox)\n\n            frustum_pcd = PointCloud()\n            frustum_pcd.points = Vector3dVector(frustum_point_cloud[:, 0:3])\n            frustum_pcd.paint_uniform_color([1, 0, 0])\n\n            gt_seg_pcd = PointCloud()\n            gt_seg_pcd.points = Vector3dVector(gt_seg_point_cloud[:, 0:3])\n            gt_seg_pcd.paint_uniform_color([0, 1, 0])\n\n            pred_seg_pcd = PointCloud()\n            pred_seg_pcd.points = Vector3dVector(pred_seg_point_cloud[:, 0:3])\n            pred_seg_pcd.paint_uniform_color([0, 0, 1])\n\n            gt_bbox = create3Dbbox(gt_center, gt_h, gt_w, gt_l, gt_r_y, type=""gt"")\n\n            pred_bbox = create3Dbbox(pred_center_BboxNet, pred_h, pred_w, pred_l, pred_r_y, type=""pred"")\n\n            gt_bbox_poly = create3Dbbox_poly(gt_center, gt_h, gt_w, gt_l, gt_r_y, P2, type=""gt"")\n            img_with_gt_bbox = draw_3d_polys(img, [gt_bbox_poly])\n            cv2.imwrite(""img_with_gt_bbox.png"", img_with_gt_bbox)\n\n            pred_bbox_poly = create3Dbbox_poly(pred_center_BboxNet, pred_h, pred_w, pred_l, pred_r_y, P2, type=""pred"")\n            img_with_pred_bbox = draw_3d_polys(img, [pred_bbox_poly])\n            cv2.imwrite(""img_with_pred_bbox.png"", img_with_pred_bbox)\n\n            draw_geometries_dark_background([frustum_pcd, pcd])\n\n            draw_geometries_dark_background(gt_bbox + [gt_seg_pcd, frustum_pcd, pcd])\n\n            draw_geometries_dark_background(pred_bbox + [pred_seg_pcd, frustum_pcd, pcd])\n\n            draw_geometries_dark_background(gt_bbox + pred_bbox + [pcd])\n'"
visualization/visualize_eval_val_seq.py,0,"b'# camera-ready\n\nimport pickle\nimport numpy as np\nimport math\nimport cv2\nimport os\n\nimport sys\nsys.path.append(""/home/fregu856/3DOD_thesis/Open3D/build/lib"") # NOTE! you\'ll have to adapt this for your file structure\nfrom py3d import *\n\nsys.path.append(""/home/fregu856/3DOD_thesis/utils"") # NOTE! you\'ll have to adapt this for your file structure\nfrom kittiloader import LabelLoader2D3D, calibread\n\ndef create3Dbbox(center, h, w, l, r_y, type=""pred""):\n    if type == ""pred"":\n        color = [1, 0.75, 0] # (normalized RGB)\n        front_color = [1, 0, 0] # (normalized RGB)\n    else: # (if type == ""gt"":)\n        color = [1, 0, 0.75] # (normalized RGB)\n        front_color = [0, 0.9, 1] # (normalized RGB)\n\n    Rmat = np.asarray([[math.cos(r_y), 0, math.sin(r_y)],\n                       [0, 1, 0],\n                       [-math.sin(r_y), 0, math.cos(r_y)]],\n                       dtype=\'float32\')\n\n    Rmat_90 = np.asarray([[math.cos(r_y+np.pi/2), 0, math.sin(r_y+np.pi/2)],\n                          [0, 1, 0],\n                          [-math.sin(r_y+np.pi/2), 0, math.cos(r_y+np.pi/2)]],\n                          dtype=\'float32\')\n\n    Rmat_90_x = np.asarray([[1, 0, 0],\n                            [0, math.cos(np.pi/2), math.sin(np.pi/2)],\n                            [0, -math.sin(np.pi/2), math.cos(np.pi/2)]],\n                            dtype=\'float32\')\n\n    p0 = center + np.dot(Rmat, np.asarray([l/2.0, 0, w/2.0], dtype=\'float32\').flatten())\n    p1 = center + np.dot(Rmat, np.asarray([-l/2.0, 0, w/2.0], dtype=\'float32\').flatten())\n    p2 = center + np.dot(Rmat, np.asarray([-l/2.0, 0, -w/2.0], dtype=\'float32\').flatten())\n    p3 = center + np.dot(Rmat, np.asarray([l/2.0, 0, -w/2.0], dtype=\'float32\').flatten())\n    p4 = center + np.dot(Rmat, np.asarray([l/2.0, -h, w/2.0], dtype=\'float32\').flatten())\n    p5 = center + np.dot(Rmat, np.asarray([-l/2.0, -h, w/2.0], dtype=\'float32\').flatten())\n    p6 = center + np.dot(Rmat, np.asarray([-l/2.0, -h, -w/2.0], dtype=\'float32\').flatten())\n    p7 = center + np.dot(Rmat, np.asarray([l/2.0, -h, -w/2.0], dtype=\'float32\').flatten())\n\n    p0_3 = center + np.dot(Rmat, np.asarray([l/2.0, 0, 0], dtype=\'float32\').flatten())\n    p1_2 = center + np.dot(Rmat, np.asarray([-l/2.0, 0, 0], dtype=\'float32\').flatten())\n    p4_7 = center + np.dot(Rmat, np.asarray([l/2.0, -h, 0], dtype=\'float32\').flatten())\n    p5_6 = center + np.dot(Rmat, np.asarray([-l/2.0, -h, 0], dtype=\'float32\').flatten())\n    p0_1 = center + np.dot(Rmat, np.asarray([0, 0, w/2.0], dtype=\'float32\').flatten())\n    p3_2 = center + np.dot(Rmat, np.asarray([0, 0, -w/2.0], dtype=\'float32\').flatten())\n    p4_5 = center + np.dot(Rmat, np.asarray([0, -h, w/2.0], dtype=\'float32\').flatten())\n    p7_6 = center + np.dot(Rmat, np.asarray([0, -h, -w/2.0], dtype=\'float32\').flatten())\n    p0_4 = center + np.dot(Rmat, np.asarray([l/2.0, -h/2.0, w/2.0], dtype=\'float32\').flatten())\n    p3_7 = center + np.dot(Rmat, np.asarray([l/2.0, -h/2.0, -w/2.0], dtype=\'float32\').flatten())\n    p1_5 = center + np.dot(Rmat, np.asarray([-l/2.0, -h/2.0, w/2.0], dtype=\'float32\').flatten())\n    p2_6 = center + np.dot(Rmat, np.asarray([-l/2.0, -h/2.0, -w/2.0], dtype=\'float32\').flatten())\n    p0_1_3_2 = center\n\n    length_0_3 = np.linalg.norm(p0 - p3)\n    cylinder_0_3 = create_mesh_cylinder(radius=0.035, height=length_0_3)\n    cylinder_0_3.compute_vertex_normals()\n    transform_0_3 = np.eye(4)\n    transform_0_3[0:3, 0:3] = Rmat\n    transform_0_3[0:3, 3] = p0_3\n    cylinder_0_3.transform(transform_0_3)\n    cylinder_0_3.paint_uniform_color(front_color)\n\n    length_1_2 = np.linalg.norm(p1 - p2)\n    cylinder_1_2 = create_mesh_cylinder(radius=0.035, height=length_1_2)\n    cylinder_1_2.compute_vertex_normals()\n    transform_1_2 = np.eye(4)\n    transform_1_2[0:3, 0:3] = Rmat\n    transform_1_2[0:3, 3] = p1_2\n    cylinder_1_2.transform(transform_1_2)\n    cylinder_1_2.paint_uniform_color(color)\n\n    length_4_7 = np.linalg.norm(p4 - p7)\n    cylinder_4_7 = create_mesh_cylinder(radius=0.035, height=length_4_7)\n    cylinder_4_7.compute_vertex_normals()\n    transform_4_7 = np.eye(4)\n    transform_4_7[0:3, 0:3] = Rmat\n    transform_4_7[0:3, 3] = p4_7\n    cylinder_4_7.transform(transform_4_7)\n    cylinder_4_7.paint_uniform_color(front_color)\n\n    length_5_6 = np.linalg.norm(p5 - p6)\n    cylinder_5_6 = create_mesh_cylinder(radius=0.035, height=length_5_6)\n    cylinder_5_6.compute_vertex_normals()\n    transform_5_6 = np.eye(4)\n    transform_5_6[0:3, 0:3] = Rmat\n    transform_5_6[0:3, 3] = p5_6\n    cylinder_5_6.transform(transform_5_6)\n    cylinder_5_6.paint_uniform_color(color)\n\n    # #\n\n    length_0_1 = np.linalg.norm(p0 - p1)\n    cylinder_0_1 = create_mesh_cylinder(radius=0.035, height=length_0_1)\n    cylinder_0_1.compute_vertex_normals()\n    transform_0_1 = np.eye(4)\n    transform_0_1[0:3, 0:3] = Rmat_90\n    transform_0_1[0:3, 3] = p0_1\n    cylinder_0_1.transform(transform_0_1)\n    cylinder_0_1.paint_uniform_color(color)\n\n    length_3_2 = np.linalg.norm(p3 - p2)\n    cylinder_3_2 = create_mesh_cylinder(radius=0.035, height=length_3_2)\n    cylinder_3_2.compute_vertex_normals()\n    transform_3_2 = np.eye(4)\n    transform_3_2[0:3, 0:3] = Rmat_90\n    transform_3_2[0:3, 3] = p3_2\n    cylinder_3_2.transform(transform_3_2)\n    cylinder_3_2.paint_uniform_color(color)\n\n    length_4_5 = np.linalg.norm(p4 - p5)\n    cylinder_4_5 = create_mesh_cylinder(radius=0.035, height=length_4_5)\n    cylinder_4_5.compute_vertex_normals()\n    transform_4_5 = np.eye(4)\n    transform_4_5[0:3, 0:3] = Rmat_90\n    transform_4_5[0:3, 3] = p4_5\n    cylinder_4_5.transform(transform_4_5)\n    cylinder_4_5.paint_uniform_color(color)\n\n    length_7_6 = np.linalg.norm(p7 - p6)\n    cylinder_7_6 = create_mesh_cylinder(radius=0.035, height=length_7_6)\n    cylinder_7_6.compute_vertex_normals()\n    transform_7_6 = np.eye(4)\n    transform_7_6[0:3, 0:3] = Rmat_90\n    transform_7_6[0:3, 3] = p7_6\n    cylinder_7_6.transform(transform_7_6)\n    cylinder_7_6.paint_uniform_color(color)\n\n    # #\n\n    length_0_4 = np.linalg.norm(p0 - p4)\n    cylinder_0_4 = create_mesh_cylinder(radius=0.035, height=length_0_4)\n    cylinder_0_4.compute_vertex_normals()\n    transform_0_4 = np.eye(4)\n    transform_0_4[0:3, 0:3] = np.dot(Rmat, Rmat_90_x)\n    transform_0_4[0:3, 3] = p0_4\n    cylinder_0_4.transform(transform_0_4)\n    cylinder_0_4.paint_uniform_color(front_color)\n\n    length_3_7 = np.linalg.norm(p3 - p7)\n    cylinder_3_7 = create_mesh_cylinder(radius=0.035, height=length_3_7)\n    cylinder_3_7.compute_vertex_normals()\n    transform_3_7 = np.eye(4)\n    transform_3_7[0:3, 0:3] = np.dot(Rmat, Rmat_90_x)\n    transform_3_7[0:3, 3] = p3_7\n    cylinder_3_7.transform(transform_3_7)\n    cylinder_3_7.paint_uniform_color(front_color)\n\n    length_1_5 = np.linalg.norm(p1 - p5)\n    cylinder_1_5 = create_mesh_cylinder(radius=0.035, height=length_1_5)\n    cylinder_1_5.compute_vertex_normals()\n    transform_1_5 = np.eye(4)\n    transform_1_5[0:3, 0:3] = np.dot(Rmat, Rmat_90_x)\n    transform_1_5[0:3, 3] = p1_5\n    cylinder_1_5.transform(transform_1_5)\n    cylinder_1_5.paint_uniform_color(color)\n\n    length_2_6 = np.linalg.norm(p2 - p6)\n    cylinder_2_6 = create_mesh_cylinder(radius=0.035, height=length_2_6)\n    cylinder_2_6.compute_vertex_normals()\n    transform_2_6 = np.eye(4)\n    transform_2_6[0:3, 0:3] = np.dot(Rmat, Rmat_90_x)\n    transform_2_6[0:3, 3] = p2_6\n    cylinder_2_6.transform(transform_2_6)\n    cylinder_2_6.paint_uniform_color(color)\n\n    # #\n\n    length_0_1_3_2 = np.linalg.norm(p0_1 - p3_2)\n    cylinder_0_1_3_2 = create_mesh_cylinder(radius=0.035, height=length_0_1_3_2)\n    cylinder_0_1_3_2.compute_vertex_normals()\n    transform_0_1_3_2 = np.eye(4)\n    transform_0_1_3_2[0:3, 0:3] = Rmat\n    transform_0_1_3_2[0:3, 3] = p0_1_3_2\n    cylinder_0_1_3_2.transform(transform_0_1_3_2)\n    cylinder_0_1_3_2.paint_uniform_color(color)\n\n    return [cylinder_0_1_3_2, cylinder_0_3, cylinder_1_2, cylinder_4_7, cylinder_5_6, cylinder_0_1, cylinder_3_2, cylinder_4_5, cylinder_7_6, cylinder_0_4, cylinder_3_7, cylinder_1_5, cylinder_2_6]\n\ndef create3Dbbox_poly(center, h, w, l, r_y, P2_mat, type=""pred""):\n    if type == ""pred"":\n        color = [0, 190, 255] # (BGR)\n        front_color = [0, 0, 255] # (BGR)\n    else: # (if type == ""gt"":)\n        color = [190, 0, 255] # (BGR)\n        front_color = [255, 230, 0] # (BGR)\n\n    poly = {}\n\n    Rmat = np.asarray([[math.cos(r_y), 0, math.sin(r_y)],\n                       [0, 1, 0],\n                       [-math.sin(r_y), 0, math.cos(r_y)]],\n                       dtype=\'float32\')\n\n    p0 = center + np.dot(Rmat, np.asarray([l/2.0, 0, w/2.0], dtype=\'float32\').flatten())\n    p1 = center + np.dot(Rmat, np.asarray([-l/2.0, 0, w/2.0], dtype=\'float32\').flatten())\n    p2 = center + np.dot(Rmat, np.asarray([-l/2.0, 0, -w/2.0], dtype=\'float32\').flatten())\n    p3 = center + np.dot(Rmat, np.asarray([l/2.0, 0, -w/2.0], dtype=\'float32\').flatten())\n    p4 = center + np.dot(Rmat, np.asarray([l/2.0, -h, w/2.0], dtype=\'float32\').flatten())\n    p5 = center + np.dot(Rmat, np.asarray([-l/2.0, -h, w/2.0], dtype=\'float32\').flatten())\n    p6 = center + np.dot(Rmat, np.asarray([-l/2.0, -h, -w/2.0], dtype=\'float32\').flatten())\n    p7 = center + np.dot(Rmat, np.asarray([l/2.0, -h, -w/2.0], dtype=\'float32\').flatten())\n\n    poly[\'points\'] = np.array([p0, p1, p2, p3, p4, p5, p6, p7])\n    poly[\'lines\'] = [[0, 3, 7, 4, 0], [1, 2, 6, 5, 1], [0, 1], [2, 3], [6, 7], [4, 5]] # (0 -> 3 -> 7 -> 4 -> 0, 1 -> 2 -> 6 -> 5 -> 1, etc.)\n    poly[\'colors\'] = [front_color, color, color, color, color, color]\n    poly[\'P0_mat\'] = P2_mat\n\n    return poly\n\ndef draw_3d_polys(img, polys):\n    img = np.copy(img)\n    for poly in polys:\n        for n, line in enumerate(poly[\'lines\']):\n            if \'colors\' in poly:\n                bg = poly[\'colors\'][n]\n            else:\n                bg = np.array([255, 0, 0], dtype=\'float64\')\n\n            p3d = np.vstack((poly[\'points\'][line].T, np.ones((1, poly[\'points\'][line].shape[0]))))\n            p2d = np.dot(poly[\'P0_mat\'], p3d)\n\n            for m, p in enumerate(p2d[2, :]):\n                p2d[:, m] = p2d[:, m]/p\n\n            cv2.polylines(img, np.int32([p2d[:2, :].T]), False, bg, lineType=cv2.LINE_AA, thickness=2)\n\n    return img\n\nsequence = ""0004"" # NOTE! change this to visualize a different sequence\n\nproject_dir = ""/home/fregu856/3DOD_thesis/"" # NOTE! you\'ll have to adapt this for your file structure\ndata_dir = project_dir + ""data/kitti/tracking/training/""\nimg_dir = data_dir + ""image_02/"" + sequence + ""/""\ncalib_path = project_dir + ""data/kitti/meta/tracking/training/calib/"" + sequence + "".txt"" # NOTE! kitti/meta\nlidar_dir = data_dir + ""velodyne/"" + sequence + ""/""\n\ncalib = calibread(calib_path)\nP2 = calib[""P2""]\nTr_velo_to_cam_orig = calib[""Tr_velo_to_cam""]\nR0_rect_orig = calib[""R0_rect""]\n\nR0_rect = np.eye(4)\nR0_rect[0:3, 0:3] = R0_rect_orig\n\nTr_velo_to_cam = np.eye(4)\nTr_velo_to_cam[0:3, :] = Tr_velo_to_cam_orig\n\n# NOTE! here you can choose what model\'s output you want to visualize\n# Frustum-PointNet:\nwith open(""/home/fregu856/3DOD_thesis/training_logs/model_Frustum-PointNet_eval_val_seq/eval_dict_val_seq_%s.pkl"" % sequence, ""rb"") as file: # NOTE! you\'ll have to adapt this for your file structure\n    eval_dict = pickle.load(file)\n#################################\n# # Extended-Frustum-PointNet:\n# with open(""/home/fregu856/3DOD_thesis/training_logs/model_Extended-Frustum-PointNet_eval_val_seq/eval_dict_val_seq_%s.pkl"" % sequence, ""rb"") as file: # NOTE! you\'ll have to adapt this for your file structure\n#     eval_dict = pickle.load(file)\n##################################\n# # Image-Only:\n# with open(""/home/fregu856/3DOD_thesis/training_logs/model_Image-Only_eval_val_seq/eval_dict_val_seq_%s.pkl"" % sequence, ""rb"") as file: # NOTE! you\'ll have to adapt this for your file structure\n#     eval_dict = pickle.load(file)\n\nimg_data_dict = {}\nfor img_id in eval_dict:\n    data_dict = {}\n\n    bbox_dicts = eval_dict[img_id]\n\n    gt_bboxes = []\n    pred_bboxes = []\n    gt_bbox_polys = []\n    pred_bbox_polys = []\n    for bbox_dict in bbox_dicts:\n        pred_center_BboxNet = bbox_dict[""pred_center_BboxNet""]\n        gt_center = bbox_dict[""gt_center""]\n        pred_h = bbox_dict[""pred_h""]\n        pred_w = bbox_dict[""pred_w""]\n        pred_l = bbox_dict[""pred_l""]\n        pred_r_y = bbox_dict[""pred_r_y""]\n        gt_h = bbox_dict[""gt_h""]\n        gt_w = bbox_dict[""gt_w""]\n        gt_l = bbox_dict[""gt_l""]\n        gt_r_y = bbox_dict[""gt_r_y""]\n\n        gt_bbox = create3Dbbox(gt_center, gt_h, gt_w, gt_l, gt_r_y, type=""gt"")\n        gt_bboxes += gt_bbox\n\n        pred_bbox = create3Dbbox(pred_center_BboxNet, pred_h, pred_w, pred_l, pred_r_y, type=""pred"")\n        pred_bboxes += pred_bbox\n\n        gt_bbox_poly = create3Dbbox_poly(gt_center, gt_h, gt_w, gt_l, gt_r_y, P2, type=""gt"")\n        gt_bbox_polys.append(gt_bbox_poly)\n\n        pred_bbox_poly = create3Dbbox_poly(pred_center_BboxNet, pred_h, pred_w, pred_l, pred_r_y, P2, type=""pred"")\n        pred_bbox_polys.append(pred_bbox_poly)\n\n    data_dict[""gt_bboxes""] = gt_bboxes\n    data_dict[""pred_bboxes""] = pred_bboxes\n    data_dict[""gt_bbox_polys""] = gt_bbox_polys\n    data_dict[""pred_bbox_polys""] = pred_bbox_polys\n\n    img_data_dict[img_id] = data_dict\n\nsorted_img_ids = []\nimg_names = sorted(os.listdir(img_dir))\nfor img_name in img_names:\n    img_id = img_name.split("".png"")[0]\n    sorted_img_ids.append(img_id)\n\nimg_height = 375\nimg_width = 1242\n\nsmall_img_height = 187\nsmall_img_width = 620\n\n# ################################################################################\n# # create a video of images (no bboxes):\n# ################################################################################\n# out = cv2.VideoWriter(""eval_val_seq_%s_img.avi"" % sequence, cv2.VideoWriter_fourcc(*\'H264\'), 12, (img_width, img_height), True)\n#\n# for img_id in sorted_img_ids:\n#     print img_id\n#\n#     img = cv2.imread(img_dir + img_id + "".png"", -1)\n#\n#     img = cv2.resize(img, (img_width, img_height)) # (the image MUST have the size specified in VideoWriter)\n#\n#     out.write(img)\n\n# ################################################################################\n# # create a video of images with GT:\n# ################################################################################\n# out = cv2.VideoWriter(""eval_val_seq_%s_img_GT.avi"" % sequence, cv2.VideoWriter_fourcc(*\'H264\'), 12, (img_width, img_height), True)\n#\n# for img_id in sorted_img_ids:\n#     print img_id\n#\n#     img = cv2.imread(img_dir + img_id + "".png"", -1)\n#\n#     img_with_gt_bboxes = img\n#\n#     if img_id in img_data_dict:\n#         data_dict = img_data_dict[img_id]\n#         gt_bbox_polys = data_dict[""gt_bbox_polys""]\n#\n#         img_with_gt_bboxes = draw_3d_polys(img, gt_bbox_polys)\n#\n#     img_with_gt_bboxes = cv2.resize(img_with_gt_bboxes, (img_width, img_height)) # (the image MUST have the size specified in VideoWriter)\n#\n#     out.write(img_with_gt_bboxes)\n\n# ################################################################################\n# # create a video of images with pred:\n# ################################################################################\n# out = cv2.VideoWriter(""eval_val_seq_%s_img_pred.avi"" % sequence, cv2.VideoWriter_fourcc(*\'H264\'), 12, (img_width, img_height), True)\n#\n# for img_id in sorted_img_ids:\n#     print img_id\n#\n#     img = cv2.imread(img_dir + img_id + "".png"", -1)\n#\n#     img_with_pred_bboxes = img\n#\n#     if img_id in img_data_dict:\n#         data_dict = img_data_dict[img_id]\n#         pred_bbox_polys = data_dict[""pred_bbox_polys""]\n#\n#         img_with_pred_bboxes = draw_3d_polys(img, pred_bbox_polys)\n#\n#     img_with_pred_bboxes = cv2.resize(img_with_pred_bboxes, (img_width, img_height)) # (the image MUST have the size specified in VideoWriter)\n#\n#     out.write(img_with_pred_bboxes)\n\n# ################################################################################\n# # create a video of images with GT on top of pred:\n# ################################################################################\n# out = cv2.VideoWriter(""eval_val_seq_%s_img_GT_pred.avi"" % sequence, cv2.VideoWriter_fourcc(*\'H264\'), 12, (img_width, 2*img_height), True)\n#\n# for img_id in sorted_img_ids:\n#     print img_id\n#\n#     img = cv2.imread(img_dir + img_id + "".png"", -1)\n#\n#     img_with_gt_bboxes = img\n#     img_with_pred_bboxes = img\n#\n#     if img_id in img_data_dict:\n#         data_dict = img_data_dict[img_id]\n#         gt_bbox_polys = data_dict[""gt_bbox_polys""]\n#         pred_bbox_polys = data_dict[""pred_bbox_polys""]\n#\n#         img_with_gt_bboxes = draw_3d_polys(img, gt_bbox_polys)\n#         img_with_pred_bboxes = draw_3d_polys(img, pred_bbox_polys)\n#\n#     img_with_gt_bboxes = cv2.resize(img_with_gt_bboxes, (img_width, img_height)) # (the image MUST have the size specified in VideoWriter)\n#     img_with_pred_bboxes = cv2.resize(img_with_pred_bboxes, (img_width, img_height)) # (the image MUST have the size specified in VideoWriter)\n#\n#     combined_img = np.zeros((2*img_height, img_width, 3), dtype=np.uint8)\n#     combined_img[0:img_height] = img_with_gt_bboxes\n#     combined_img[img_height:] = img_with_pred_bboxes\n#\n#     out.write(combined_img)\n\nclass ImgCreatorLiDAR:\n    def __init__(self):\n        self.counter = 0\n        self.trajectory = read_pinhole_camera_trajectory(""/home/fregu856/3DOD_thesis/visualization/camera_trajectory.json"") # NOTE! you\'ll have to adapt this for your file structure\n\n    def move_forward(self, vis):\n        # this function is called within the Visualizer::run() loop.\n        # the run loop calls the function, then re-renders the image.\n\n        if self.counter < 2: # (the counter is for making sure the camera view has been changed before the img is captured)\n            # set the camera view:\n            ctr = vis.get_view_control()\n            ctr.convert_from_pinhole_camera_parameters(self.trajectory.intrinsic, self.trajectory.extrinsic[0])\n\n            self.counter += 1\n        else:\n            # capture an image:\n            img = vis.capture_screen_float_buffer()\n            img = 255*np.asarray(img)\n            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n            img = img.astype(np.uint8)\n            self.lidar_img = img\n\n            # close the window:\n            vis.destroy_window()\n\n            self.counter = 0\n\n        return False\n\n    def create_img(self, geometries):\n        vis = Visualizer()\n        vis.create_window()\n        opt = vis.get_render_option()\n        opt.background_color = np.asarray([0, 0, 0])\n        for geometry in geometries:\n            vis.add_geometry(geometry)\n        vis.register_animation_callback(self.move_forward)\n        vis.run()\n\n        return self.lidar_img\n\n# ################################################################################\n# # create a video of LiDAR (no bboxes):\n# ################################################################################\n# out_lidar = cv2.VideoWriter(""eval_val_seq_%s_lidar.avi"" % sequence, cv2.VideoWriter_fourcc(*\'H264\'), 12, (1920, 1080), True)\n#\n# lidar_img_creator = ImgCreatorLiDAR()\n# for img_id in sorted_img_ids:\n#     print img_id\n#\n#     lidar_path = lidar_dir + img_id + "".bin""\n#     point_cloud = np.fromfile(lidar_path, dtype=np.float32).reshape(-1, 4)\n#\n#     # remove points that are located behind the camera:\n#     point_cloud = point_cloud[point_cloud[:, 0] > -2.5, :]\n#\n#     point_cloud_xyz = point_cloud[:, 0:3]\n#     point_cloud_xyz_hom = np.ones((point_cloud.shape[0], 4))\n#     point_cloud_xyz_hom[:, 0:3] = point_cloud[:, 0:3] # (point_cloud_xyz_hom has shape (num_points, 4))\n#\n#     # transform the points into (rectified) camera coordinates:\n#     point_cloud_xyz_camera_hom = np.dot(R0_rect, np.dot(Tr_velo_to_cam, point_cloud_xyz_hom.T)).T # (point_cloud_xyz_hom.T has shape (4, num_points))\n#     # normalize:\n#     point_cloud_xyz_camera = np.zeros((point_cloud_xyz_camera_hom.shape[0], 3))\n#     point_cloud_xyz_camera[:, 0] = point_cloud_xyz_camera_hom[:, 0]/point_cloud_xyz_camera_hom[:, 3]\n#     point_cloud_xyz_camera[:, 1] = point_cloud_xyz_camera_hom[:, 1]/point_cloud_xyz_camera_hom[:, 3]\n#     point_cloud_xyz_camera[:, 2] = point_cloud_xyz_camera_hom[:, 2]/point_cloud_xyz_camera_hom[:, 3]\n#\n#     pcd = PointCloud()\n#     pcd.points = Vector3dVector(point_cloud_xyz_camera)\n#     pcd.paint_uniform_color([0.65, 0.65, 0.65])\n#\n#     img = lidar_img_creator.create_img([pcd])\n#     out_lidar.write(img)\n\n# ################################################################################\n# # create a video of LiDAR with GT:\n# ################################################################################\n# out_lidar_GT = cv2.VideoWriter(""eval_val_seq_%s_lidar_GT.avi"" % sequence, cv2.VideoWriter_fourcc(*\'H264\'), 12, (1920, 1080), True)\n#\n# lidar_img_creator = ImgCreatorLiDAR()\n# for img_id in sorted_img_ids:\n#     print img_id\n#\n#     lidar_path = lidar_dir + img_id + "".bin""\n#     point_cloud = np.fromfile(lidar_path, dtype=np.float32).reshape(-1, 4)\n#\n#     # remove points that are located behind the camera:\n#     point_cloud = point_cloud[point_cloud[:, 0] > -2.5, :]\n#\n#     point_cloud_xyz = point_cloud[:, 0:3]\n#     point_cloud_xyz_hom = np.ones((point_cloud.shape[0], 4))\n#     point_cloud_xyz_hom[:, 0:3] = point_cloud[:, 0:3] # (point_cloud_xyz_hom has shape (num_points, 4))\n#\n#     # transform the points into (rectified) camera coordinates:\n#     point_cloud_xyz_camera_hom = np.dot(R0_rect, np.dot(Tr_velo_to_cam, point_cloud_xyz_hom.T)).T # (point_cloud_xyz_hom.T has shape (4, num_points))\n#     # normalize:\n#     point_cloud_xyz_camera = np.zeros((point_cloud_xyz_camera_hom.shape[0], 3))\n#     point_cloud_xyz_camera[:, 0] = point_cloud_xyz_camera_hom[:, 0]/point_cloud_xyz_camera_hom[:, 3]\n#     point_cloud_xyz_camera[:, 1] = point_cloud_xyz_camera_hom[:, 1]/point_cloud_xyz_camera_hom[:, 3]\n#     point_cloud_xyz_camera[:, 2] = point_cloud_xyz_camera_hom[:, 2]/point_cloud_xyz_camera_hom[:, 3]\n#\n#     pcd = PointCloud()\n#     pcd.points = Vector3dVector(point_cloud_xyz_camera)\n#     pcd.paint_uniform_color([0.65, 0.65, 0.65])\n#\n#     gt_bboxes = []\n#\n#     if img_id in img_data_dict:\n#         data_dict = img_data_dict[img_id]\n#         gt_bboxes = data_dict[""gt_bboxes""]\n#\n#     img = lidar_img_creator.create_img(gt_bboxes + [pcd])\n#     out_lidar_GT.write(img)\n\n# ################################################################################\n# # create a video of LiDAR with pred:\n# ################################################################################\n# out_lidar_pred = cv2.VideoWriter(""eval_val_seq_%s_lidar_pred.avi"" % sequence, cv2.VideoWriter_fourcc(*\'H264\'), 12, (1920, 1080), True)\n#\n# lidar_img_creator = ImgCreatorLiDAR()\n# for img_id in sorted_img_ids:\n#     print img_id\n#\n#     lidar_path = lidar_dir + img_id + "".bin""\n#     point_cloud = np.fromfile(lidar_path, dtype=np.float32).reshape(-1, 4)\n#\n#     # remove points that are located behind the camera:\n#     point_cloud = point_cloud[point_cloud[:, 0] > -2.5, :]\n#\n#     point_cloud_xyz = point_cloud[:, 0:3]\n#     point_cloud_xyz_hom = np.ones((point_cloud.shape[0], 4))\n#     point_cloud_xyz_hom[:, 0:3] = point_cloud[:, 0:3] # (point_cloud_xyz_hom has shape (num_points, 4))\n#\n#     # transform the points into (rectified) camera coordinates:\n#     point_cloud_xyz_camera_hom = np.dot(R0_rect, np.dot(Tr_velo_to_cam, point_cloud_xyz_hom.T)).T # (point_cloud_xyz_hom.T has shape (4, num_points))\n#     # normalize:\n#     point_cloud_xyz_camera = np.zeros((point_cloud_xyz_camera_hom.shape[0], 3))\n#     point_cloud_xyz_camera[:, 0] = point_cloud_xyz_camera_hom[:, 0]/point_cloud_xyz_camera_hom[:, 3]\n#     point_cloud_xyz_camera[:, 1] = point_cloud_xyz_camera_hom[:, 1]/point_cloud_xyz_camera_hom[:, 3]\n#     point_cloud_xyz_camera[:, 2] = point_cloud_xyz_camera_hom[:, 2]/point_cloud_xyz_camera_hom[:, 3]\n#\n#     pcd = PointCloud()\n#     pcd.points = Vector3dVector(point_cloud_xyz_camera)\n#     pcd.paint_uniform_color([0.65, 0.65, 0.65])\n#\n#     pred_bboxes = []\n#\n#     if img_id in img_data_dict:\n#         data_dict = img_data_dict[img_id]\n#         pred_bboxes = data_dict[""pred_bboxes""]\n#\n#     img = lidar_img_creator.create_img(pred_bboxes + [pcd])\n#     out_lidar_pred.write(img)\n\n# ################################################################################\n# # create a video of LiDAR with GT and pred:\n# ################################################################################\n# out_lidar_GT_pred = cv2.VideoWriter(""eval_val_seq_%s_lidar_GT_pred.avi"" % sequence, cv2.VideoWriter_fourcc(*\'H264\'), 12, (1920, 1080), True)\n#\n# lidar_img_creator = ImgCreatorLiDAR()\n# for img_id in sorted_img_ids:\n#     print img_id\n#\n#     lidar_path = lidar_dir + img_id + "".bin""\n#     point_cloud = np.fromfile(lidar_path, dtype=np.float32).reshape(-1, 4)\n#\n#     # remove points that are located behind the camera:\n#     point_cloud = point_cloud[point_cloud[:, 0] > -2.5, :]\n#\n#     point_cloud_xyz = point_cloud[:, 0:3]\n#     point_cloud_xyz_hom = np.ones((point_cloud.shape[0], 4))\n#     point_cloud_xyz_hom[:, 0:3] = point_cloud[:, 0:3] # (point_cloud_xyz_hom has shape (num_points, 4))\n#\n#     # transform the points into (rectified) camera coordinates:\n#     point_cloud_xyz_camera_hom = np.dot(R0_rect, np.dot(Tr_velo_to_cam, point_cloud_xyz_hom.T)).T # (point_cloud_xyz_hom.T has shape (4, num_points))\n#     # normalize:\n#     point_cloud_xyz_camera = np.zeros((point_cloud_xyz_camera_hom.shape[0], 3))\n#     point_cloud_xyz_camera[:, 0] = point_cloud_xyz_camera_hom[:, 0]/point_cloud_xyz_camera_hom[:, 3]\n#     point_cloud_xyz_camera[:, 1] = point_cloud_xyz_camera_hom[:, 1]/point_cloud_xyz_camera_hom[:, 3]\n#     point_cloud_xyz_camera[:, 2] = point_cloud_xyz_camera_hom[:, 2]/point_cloud_xyz_camera_hom[:, 3]\n#\n#     pcd = PointCloud()\n#     pcd.points = Vector3dVector(point_cloud_xyz_camera)\n#     pcd.paint_uniform_color([0.65, 0.65, 0.65])\n#\n#     gt_bboxes = []\n#     pred_bboxes = []\n#\n#     if img_id in img_data_dict:\n#         data_dict = img_data_dict[img_id]\n#         gt_bboxes = data_dict[""gt_bboxes""]\n#         pred_bboxes = data_dict[""pred_bboxes""]\n#\n#     img = lidar_img_creator.create_img(gt_bboxes + pred_bboxes + [pcd])\n#     out_lidar_GT_pred.write(img)\n\n# ################################################################################\n# # create a video of image and LiDAR (no bboxes):\n# ################################################################################\n# out_lidar_img = cv2.VideoWriter(""eval_val_seq_%s_lidar_img.avi"" % sequence, cv2.VideoWriter_fourcc(*\'H264\'), 12, (1920, 1080), True)\n#\n# lidar_img_creator = ImgCreatorLiDAR()\n# for img_id in sorted_img_ids:\n#     print img_id\n#\n#     img = cv2.imread(img_dir + img_id + "".png"", -1)\n#     small_img = cv2.resize(img, (small_img_width, small_img_height))\n#\n#     lidar_path = lidar_dir + img_id + "".bin""\n#     point_cloud = np.fromfile(lidar_path, dtype=np.float32).reshape(-1, 4)\n#\n#     # remove points that are located behind the camera:\n#     point_cloud = point_cloud[point_cloud[:, 0] > -2.5, :]\n#\n#     point_cloud_xyz = point_cloud[:, 0:3]\n#     point_cloud_xyz_hom = np.ones((point_cloud.shape[0], 4))\n#     point_cloud_xyz_hom[:, 0:3] = point_cloud[:, 0:3] # (point_cloud_xyz_hom has shape (num_points, 4))\n#\n#     # transform the points into (rectified) camera coordinates:\n#     point_cloud_xyz_camera_hom = np.dot(R0_rect, np.dot(Tr_velo_to_cam, point_cloud_xyz_hom.T)).T # (point_cloud_xyz_hom.T has shape (4, num_points))\n#     # normalize:\n#     point_cloud_xyz_camera = np.zeros((point_cloud_xyz_camera_hom.shape[0], 3))\n#     point_cloud_xyz_camera[:, 0] = point_cloud_xyz_camera_hom[:, 0]/point_cloud_xyz_camera_hom[:, 3]\n#     point_cloud_xyz_camera[:, 1] = point_cloud_xyz_camera_hom[:, 1]/point_cloud_xyz_camera_hom[:, 3]\n#     point_cloud_xyz_camera[:, 2] = point_cloud_xyz_camera_hom[:, 2]/point_cloud_xyz_camera_hom[:, 3]\n#\n#     pcd = PointCloud()\n#     pcd.points = Vector3dVector(point_cloud_xyz_camera)\n#     pcd.paint_uniform_color([0.65, 0.65, 0.65])\n#\n#     img_lidar = lidar_img_creator.create_img([pcd])\n#\n#     combined_img = img_lidar\n#     combined_img[-small_img_height:, ((1920/2)-(small_img_width/2)):((1920/2)+(small_img_width/2))] = small_img\n#\n#     out_lidar_img.write(combined_img)\n\n# ################################################################################\n# # create a video of image and LiDAR with GT:\n# ################################################################################\n# out_lidar_img_GT = cv2.VideoWriter(""eval_val_seq_%s_lidar_img_GT.avi"" % sequence, cv2.VideoWriter_fourcc(*\'H264\'), 12, (1920, 1080), True)\n#\n# lidar_img_creator = ImgCreatorLiDAR()\n# for img_id in sorted_img_ids:\n#     print img_id\n#\n#     img = cv2.imread(img_dir + img_id + "".png"", -1)\n#     img_with_gt_bboxes = img\n#     if img_id in img_data_dict:\n#         data_dict = img_data_dict[img_id]\n#         gt_bbox_polys = data_dict[""gt_bbox_polys""]\n#         img_with_gt_bboxes = draw_3d_polys(img, gt_bbox_polys)\n#     small_img_with_gt_bboxes = cv2.resize(img_with_gt_bboxes, (small_img_width, small_img_height))\n#\n#     lidar_path = lidar_dir + img_id + "".bin""\n#     point_cloud = np.fromfile(lidar_path, dtype=np.float32).reshape(-1, 4)\n#\n#     # remove points that are located behind the camera:\n#     point_cloud = point_cloud[point_cloud[:, 0] > -2.5, :]\n#\n#     point_cloud_xyz = point_cloud[:, 0:3]\n#     point_cloud_xyz_hom = np.ones((point_cloud.shape[0], 4))\n#     point_cloud_xyz_hom[:, 0:3] = point_cloud[:, 0:3] # (point_cloud_xyz_hom has shape (num_points, 4))\n#\n#     # transform the points into (rectified) camera coordinates:\n#     point_cloud_xyz_camera_hom = np.dot(R0_rect, np.dot(Tr_velo_to_cam, point_cloud_xyz_hom.T)).T # (point_cloud_xyz_hom.T has shape (4, num_points))\n#     # normalize:\n#     point_cloud_xyz_camera = np.zeros((point_cloud_xyz_camera_hom.shape[0], 3))\n#     point_cloud_xyz_camera[:, 0] = point_cloud_xyz_camera_hom[:, 0]/point_cloud_xyz_camera_hom[:, 3]\n#     point_cloud_xyz_camera[:, 1] = point_cloud_xyz_camera_hom[:, 1]/point_cloud_xyz_camera_hom[:, 3]\n#     point_cloud_xyz_camera[:, 2] = point_cloud_xyz_camera_hom[:, 2]/point_cloud_xyz_camera_hom[:, 3]\n#\n#     pcd = PointCloud()\n#     pcd.points = Vector3dVector(point_cloud_xyz_camera)\n#     pcd.paint_uniform_color([0.65, 0.65, 0.65])\n#\n#     gt_bboxes = []\n#\n#     if img_id in img_data_dict:\n#         data_dict = img_data_dict[img_id]\n#         gt_bboxes = data_dict[""gt_bboxes""]\n#\n#     img_lidar = lidar_img_creator.create_img(gt_bboxes + [pcd])\n#\n#     combined_img = img_lidar\n#     combined_img[-small_img_height:, ((1920/2)-(small_img_width/2)):((1920/2)+(small_img_width/2))] = small_img_with_gt_bboxes\n#\n#     out_lidar_img_GT.write(combined_img)\n\n# ################################################################################\n# # create a video of image and LiDAR with pred:\n# ################################################################################\n# out_lidar_img_pred = cv2.VideoWriter(""eval_val_seq_%s_lidar_img_pred.avi"" % sequence, cv2.VideoWriter_fourcc(*\'H264\'), 12, (1920, 1080), True)\n#\n# lidar_img_creator = ImgCreatorLiDAR()\n# for img_id in sorted_img_ids:\n#     print img_id\n#\n#     img = cv2.imread(img_dir + img_id + "".png"", -1)\n#     img_with_pred_bboxes = img\n#     if img_id in img_data_dict:\n#         data_dict = img_data_dict[img_id]\n#         pred_bbox_polys = data_dict[""pred_bbox_polys""]\n#         img_with_pred_bboxes = draw_3d_polys(img, pred_bbox_polys)\n#     small_img_with_pred_bboxes = cv2.resize(img_with_pred_bboxes, (small_img_width, small_img_height))\n#\n#     lidar_path = lidar_dir + img_id + "".bin""\n#     point_cloud = np.fromfile(lidar_path, dtype=np.float32).reshape(-1, 4)\n#\n#     # remove points that are located behind the camera:\n#     point_cloud = point_cloud[point_cloud[:, 0] > -2.5, :]\n#\n#     point_cloud_xyz = point_cloud[:, 0:3]\n#     point_cloud_xyz_hom = np.ones((point_cloud.shape[0], 4))\n#     point_cloud_xyz_hom[:, 0:3] = point_cloud[:, 0:3] # (point_cloud_xyz_hom has shape (num_points, 4))\n#\n#     # transform the points into (rectified) camera coordinates:\n#     point_cloud_xyz_camera_hom = np.dot(R0_rect, np.dot(Tr_velo_to_cam, point_cloud_xyz_hom.T)).T # (point_cloud_xyz_hom.T has shape (4, num_points))\n#     # normalize:\n#     point_cloud_xyz_camera = np.zeros((point_cloud_xyz_camera_hom.shape[0], 3))\n#     point_cloud_xyz_camera[:, 0] = point_cloud_xyz_camera_hom[:, 0]/point_cloud_xyz_camera_hom[:, 3]\n#     point_cloud_xyz_camera[:, 1] = point_cloud_xyz_camera_hom[:, 1]/point_cloud_xyz_camera_hom[:, 3]\n#     point_cloud_xyz_camera[:, 2] = point_cloud_xyz_camera_hom[:, 2]/point_cloud_xyz_camera_hom[:, 3]\n#\n#     pcd = PointCloud()\n#     pcd.points = Vector3dVector(point_cloud_xyz_camera)\n#     pcd.paint_uniform_color([0.65, 0.65, 0.65])\n#\n#     pred_bboxes = []\n#\n#     if img_id in img_data_dict:\n#         data_dict = img_data_dict[img_id]\n#         pred_bboxes = data_dict[""pred_bboxes""]\n#\n#     img_lidar = lidar_img_creator.create_img(pred_bboxes + [pcd])\n#\n#     combined_img = img_lidar\n#     combined_img[-small_img_height:, ((1920/2)-(small_img_width/2)):((1920/2)+(small_img_width/2))] = small_img_with_pred_bboxes\n#\n#     out_lidar_img_pred.write(combined_img)\n\n################################################################################\n# create a video of image and LiDAR with GT and pred:\n################################################################################\nout_lidar_img_GT_pred = cv2.VideoWriter(""eval_val_seq_%s_lidar_img_GT_pred.avi"" % sequence, cv2.VideoWriter_fourcc(*\'H264\'), 12, (1920, 1080), True)\n\nlidar_img_creator = ImgCreatorLiDAR()\nfor img_id in sorted_img_ids:\n    print img_id\n\n    img = cv2.imread(img_dir + img_id + "".png"", -1)\n    img_with_bboxes = img\n    if img_id in img_data_dict:\n        data_dict = img_data_dict[img_id]\n        gt_bbox_polys = data_dict[""gt_bbox_polys""]\n        pred_bbox_polys = data_dict[""pred_bbox_polys""]\n        img_with_bboxes = draw_3d_polys(img_with_bboxes, gt_bbox_polys)\n        img_with_bboxes = draw_3d_polys(img_with_bboxes, pred_bbox_polys)\n    small_img_with_bboxes = cv2.resize(img_with_bboxes, (small_img_width, small_img_height))\n\n    lidar_path = lidar_dir + img_id + "".bin""\n    point_cloud = np.fromfile(lidar_path, dtype=np.float32).reshape(-1, 4)\n\n    # remove points that are located behind the camera:\n    point_cloud = point_cloud[point_cloud[:, 0] > -2.5, :]\n\n    point_cloud_xyz = point_cloud[:, 0:3]\n    point_cloud_xyz_hom = np.ones((point_cloud.shape[0], 4))\n    point_cloud_xyz_hom[:, 0:3] = point_cloud[:, 0:3] # (point_cloud_xyz_hom has shape (num_points, 4))\n\n    # transform the points into (rectified) camera coordinates:\n    point_cloud_xyz_camera_hom = np.dot(R0_rect, np.dot(Tr_velo_to_cam, point_cloud_xyz_hom.T)).T # (point_cloud_xyz_hom.T has shape (4, num_points))\n    # normalize:\n    point_cloud_xyz_camera = np.zeros((point_cloud_xyz_camera_hom.shape[0], 3))\n    point_cloud_xyz_camera[:, 0] = point_cloud_xyz_camera_hom[:, 0]/point_cloud_xyz_camera_hom[:, 3]\n    point_cloud_xyz_camera[:, 1] = point_cloud_xyz_camera_hom[:, 1]/point_cloud_xyz_camera_hom[:, 3]\n    point_cloud_xyz_camera[:, 2] = point_cloud_xyz_camera_hom[:, 2]/point_cloud_xyz_camera_hom[:, 3]\n\n    pcd = PointCloud()\n    pcd.points = Vector3dVector(point_cloud_xyz_camera)\n    pcd.paint_uniform_color([0.65, 0.65, 0.65])\n\n    gt_bboxes = []\n    pred_bboxes = []\n\n    if img_id in img_data_dict:\n        data_dict = img_data_dict[img_id]\n        gt_bboxes = data_dict[""gt_bboxes""]\n        pred_bboxes = data_dict[""pred_bboxes""]\n\n    img_lidar = lidar_img_creator.create_img(gt_bboxes + pred_bboxes + [pcd])\n\n    combined_img = img_lidar\n    combined_img[-small_img_height:, ((1920/2)-(small_img_width/2)):((1920/2)+(small_img_width/2))] = small_img_with_bboxes\n\n    out_lidar_img_GT_pred.write(combined_img)\n'"
visualization/visualize_lidar.py,0,"b'# camera-ready\n\nimport os\nimport numpy as np\n\nimport sys\nsys.path.append(""/home/fregu856/3DOD_thesis/Open3D/build/lib"") # NOTE! you\'ll have to adapt this for your file structure\nfrom py3d import *\n\ndef draw_geometries_dark_background(geometries):\n    vis = Visualizer()\n    vis.create_window()\n    opt = vis.get_render_option()\n    opt.background_color = np.asarray([0, 0, 0])\n    for geometry in geometries:\n        vis.add_geometry(geometry)\n    vis.run()\n    vis.destroy_window()\n\nproject_dir = ""/home/fregu856/3DOD_thesis/"" # NOTE! you\'ll have to adapt this for your file structure\ndata_dir = project_dir + ""data/kitti/object/training/""\nimg_dir = data_dir + ""image_2/""\nlidar_dir = data_dir + ""velodyne/""\n\nimg_paths = []\nlabel_paths = []\nlidar_paths = []\nimg_names = os.listdir(img_dir)\nfor step, img_name in enumerate(img_names):\n    img_id = img_name.split("".png"")[0]\n\n    img_path = img_dir + img_name\n    img_paths.append(img_path)\n\n    lidar_path = lidar_dir + img_id + "".bin""\n    lidar_paths.append(lidar_path)\n\nfor lidar_path in lidar_paths:\n    point_cloud = np.fromfile(lidar_path, dtype=np.float32).reshape(-1, 4)\n\n    print lidar_path\n    print point_cloud.shape\n\n    point_cloud_xyz = point_cloud[:, 0:3]\n\n    pcd = PointCloud()\n    pcd.points = Vector3dVector(point_cloud_xyz)\n    pcd.paint_uniform_color([0.65, 0.65, 0.65])\n\n    draw_geometries_dark_background([pcd])\n'"
