file_path,api_count,code
datasets.py,11,"b'# camera-ready\n\nimport torch\nimport torch.utils.data\n\nimport numpy as np\nimport cv2\nimport os\n\ntrain_dirs = [""jena/"", ""zurich/"", ""weimar/"", ""ulm/"", ""tubingen/"", ""stuttgart/"",\n              ""strasbourg/"", ""monchengladbach/"", ""krefeld/"", ""hanover/"",\n              ""hamburg/"", ""erfurt/"", ""dusseldorf/"", ""darmstadt/"", ""cologne/"",\n              ""bremen/"", ""bochum/"", ""aachen/""]\nval_dirs = [""frankfurt/"", ""munster/"", ""lindau/""]\ntest_dirs = [""berlin"", ""bielefeld"", ""bonn"", ""leverkusen"", ""mainz"", ""munich""]\n\nclass DatasetTrain(torch.utils.data.Dataset):\n    def __init__(self, cityscapes_data_path, cityscapes_meta_path):\n        self.img_dir = cityscapes_data_path + ""/leftImg8bit/train/""\n        self.label_dir = cityscapes_meta_path + ""/label_imgs/""\n\n        self.img_h = 1024\n        self.img_w = 2048\n\n        self.new_img_h = 512\n        self.new_img_w = 1024\n\n        self.examples = []\n        for train_dir in train_dirs:\n            train_img_dir_path = self.img_dir + train_dir\n\n            file_names = os.listdir(train_img_dir_path)\n            for file_name in file_names:\n                img_id = file_name.split(""_leftImg8bit.png"")[0]\n\n                img_path = train_img_dir_path + file_name\n\n                label_img_path = self.label_dir + img_id + "".png""\n\n                example = {}\n                example[""img_path""] = img_path\n                example[""label_img_path""] = label_img_path\n                example[""img_id""] = img_id\n                self.examples.append(example)\n\n        self.num_examples = len(self.examples)\n\n    def __getitem__(self, index):\n        example = self.examples[index]\n\n        img_path = example[""img_path""]\n        img = cv2.imread(img_path, -1) # (shape: (1024, 2048, 3))\n        # resize img without interpolation (want the image to still match\n        # label_img, which we resize below):\n        img = cv2.resize(img, (self.new_img_w, self.new_img_h),\n                         interpolation=cv2.INTER_NEAREST) # (shape: (512, 1024, 3))\n\n        label_img_path = example[""label_img_path""]\n        label_img = cv2.imread(label_img_path, -1) # (shape: (1024, 2048))\n        # resize label_img without interpolation (want the resulting image to\n        # still only contain pixel values corresponding to an object class):\n        label_img = cv2.resize(label_img, (self.new_img_w, self.new_img_h),\n                               interpolation=cv2.INTER_NEAREST) # (shape: (512, 1024))\n\n        # flip the img and the label with 0.5 probability:\n        flip = np.random.randint(low=0, high=2)\n        if flip == 1:\n            img = cv2.flip(img, 1)\n            label_img = cv2.flip(label_img, 1)\n\n        ########################################################################\n        # randomly scale the img and the label:\n        ########################################################################\n        scale = np.random.uniform(low=0.7, high=2.0)\n        new_img_h = int(scale*self.new_img_h)\n        new_img_w = int(scale*self.new_img_w)\n\n        # resize img without interpolation (want the image to still match\n        # label_img, which we resize below):\n        img = cv2.resize(img, (new_img_w, new_img_h),\n                         interpolation=cv2.INTER_NEAREST) # (shape: (new_img_h, new_img_w, 3))\n\n        # resize label_img without interpolation (want the resulting image to\n        # still only contain pixel values corresponding to an object class):\n        label_img = cv2.resize(label_img, (new_img_w, new_img_h),\n                               interpolation=cv2.INTER_NEAREST) # (shape: (new_img_h, new_img_w))\n        ########################################################################\n\n        # # # # # # # # debug visualization START\n        # print (scale)\n        # print (new_img_h)\n        # print (new_img_w)\n        #\n        # cv2.imshow(""test"", img)\n        # cv2.waitKey(0)\n        #\n        # cv2.imshow(""test"", label_img)\n        # cv2.waitKey(0)\n        # # # # # # # # debug visualization END\n\n        ########################################################################\n        # select a 256x256 random crop from the img and label:\n        ########################################################################\n        start_x = np.random.randint(low=0, high=(new_img_w - 256))\n        end_x = start_x + 256\n        start_y = np.random.randint(low=0, high=(new_img_h - 256))\n        end_y = start_y + 256\n\n        img = img[start_y:end_y, start_x:end_x] # (shape: (256, 256, 3))\n        label_img = label_img[start_y:end_y, start_x:end_x] # (shape: (256, 256))\n        ########################################################################\n\n        # # # # # # # # debug visualization START\n        # print (img.shape)\n        # print (label_img.shape)\n        #\n        # cv2.imshow(""test"", img)\n        # cv2.waitKey(0)\n        #\n        # cv2.imshow(""test"", label_img)\n        # cv2.waitKey(0)\n        # # # # # # # # debug visualization END\n\n        # normalize the img (with the mean and std for the pretrained ResNet):\n        img = img/255.0\n        img = img - np.array([0.485, 0.456, 0.406])\n        img = img/np.array([0.229, 0.224, 0.225]) # (shape: (256, 256, 3))\n        img = np.transpose(img, (2, 0, 1)) # (shape: (3, 256, 256))\n        img = img.astype(np.float32)\n\n        # convert numpy -> torch:\n        img = torch.from_numpy(img) # (shape: (3, 256, 256))\n        label_img = torch.from_numpy(label_img) # (shape: (256, 256))\n\n        return (img, label_img)\n\n    def __len__(self):\n        return self.num_examples\n\nclass DatasetVal(torch.utils.data.Dataset):\n    def __init__(self, cityscapes_data_path, cityscapes_meta_path):\n        self.img_dir = cityscapes_data_path + ""/leftImg8bit/val/""\n        self.label_dir = cityscapes_meta_path + ""/label_imgs/""\n\n        self.img_h = 1024\n        self.img_w = 2048\n\n        self.new_img_h = 512\n        self.new_img_w = 1024\n\n        self.examples = []\n        for val_dir in val_dirs:\n            val_img_dir_path = self.img_dir + val_dir\n\n            file_names = os.listdir(val_img_dir_path)\n            for file_name in file_names:\n                img_id = file_name.split(""_leftImg8bit.png"")[0]\n\n                img_path = val_img_dir_path + file_name\n\n                label_img_path = self.label_dir + img_id + "".png""\n                label_img = cv2.imread(label_img_path, -1) # (shape: (1024, 2048))\n\n                example = {}\n                example[""img_path""] = img_path\n                example[""label_img_path""] = label_img_path\n                example[""img_id""] = img_id\n                self.examples.append(example)\n\n        self.num_examples = len(self.examples)\n\n    def __getitem__(self, index):\n        example = self.examples[index]\n\n        img_id = example[""img_id""]\n\n        img_path = example[""img_path""]\n        img = cv2.imread(img_path, -1) # (shape: (1024, 2048, 3))\n        # resize img without interpolation (want the image to still match\n        # label_img, which we resize below):\n        img = cv2.resize(img, (self.new_img_w, self.new_img_h),\n                         interpolation=cv2.INTER_NEAREST) # (shape: (512, 1024, 3))\n\n        label_img_path = example[""label_img_path""]\n        label_img = cv2.imread(label_img_path, -1) # (shape: (1024, 2048))\n        # resize label_img without interpolation (want the resulting image to\n        # still only contain pixel values corresponding to an object class):\n        label_img = cv2.resize(label_img, (self.new_img_w, self.new_img_h),\n                               interpolation=cv2.INTER_NEAREST) # (shape: (512, 1024))\n\n        # # # # # # # # debug visualization START\n        # cv2.imshow(""test"", img)\n        # cv2.waitKey(0)\n        #\n        # cv2.imshow(""test"", label_img)\n        # cv2.waitKey(0)\n        # # # # # # # # debug visualization END\n\n        # normalize the img (with the mean and std for the pretrained ResNet):\n        img = img/255.0\n        img = img - np.array([0.485, 0.456, 0.406])\n        img = img/np.array([0.229, 0.224, 0.225]) # (shape: (512, 1024, 3))\n        img = np.transpose(img, (2, 0, 1)) # (shape: (3, 512, 1024))\n        img = img.astype(np.float32)\n\n        # convert numpy -> torch:\n        img = torch.from_numpy(img) # (shape: (3, 512, 1024))\n        label_img = torch.from_numpy(label_img) # (shape: (512, 1024))\n\n        return (img, label_img, img_id)\n\n    def __len__(self):\n        return self.num_examples\n\nclass DatasetSeq(torch.utils.data.Dataset):\n    def __init__(self, cityscapes_data_path, cityscapes_meta_path, sequence):\n        self.img_dir = cityscapes_data_path + ""/leftImg8bit/demoVideo/stuttgart_"" + sequence + ""/""\n\n        self.img_h = 1024\n        self.img_w = 2048\n\n        self.new_img_h = 512\n        self.new_img_w = 1024\n\n        self.examples = []\n\n        file_names = os.listdir(self.img_dir)\n        for file_name in file_names:\n            img_id = file_name.split(""_leftImg8bit.png"")[0]\n\n            img_path = self.img_dir + file_name\n\n            example = {}\n            example[""img_path""] = img_path\n            example[""img_id""] = img_id\n            self.examples.append(example)\n\n        self.num_examples = len(self.examples)\n\n    def __getitem__(self, index):\n        example = self.examples[index]\n\n        img_id = example[""img_id""]\n\n        img_path = example[""img_path""]\n        img = cv2.imread(img_path, -1) # (shape: (1024, 2048, 3))\n        # resize img without interpolation:\n        img = cv2.resize(img, (self.new_img_w, self.new_img_h),\n                         interpolation=cv2.INTER_NEAREST) # (shape: (512, 1024, 3))\n\n        # normalize the img (with the mean and std for the pretrained ResNet):\n        img = img/255.0\n        img = img - np.array([0.485, 0.456, 0.406])\n        img = img/np.array([0.229, 0.224, 0.225]) # (shape: (512, 1024, 3))\n        img = np.transpose(img, (2, 0, 1)) # (shape: (3, 512, 1024))\n        img = img.astype(np.float32)\n\n        # convert numpy -> torch:\n        img = torch.from_numpy(img) # (shape: (3, 512, 1024))\n\n        return (img, img_id)\n\n    def __len__(self):\n        return self.num_examples\n\nclass DatasetThnSeq(torch.utils.data.Dataset):\n    def __init__(self, thn_data_path):\n        self.img_dir = thn_data_path + ""/""\n\n        self.examples = []\n\n        file_names = os.listdir(self.img_dir)\n        for file_name in file_names:\n            img_id = file_name.split("".png"")[0]\n\n            img_path = self.img_dir + file_name\n\n            example = {}\n            example[""img_path""] = img_path\n            example[""img_id""] = img_id\n            self.examples.append(example)\n\n        self.num_examples = len(self.examples)\n\n    def __getitem__(self, index):\n        example = self.examples[index]\n\n        img_id = example[""img_id""]\n\n        img_path = example[""img_path""]\n        img = cv2.imread(img_path, -1) # (shape: (512, 1024, 3))\n\n        # normalize the img (with mean and std for the pretrained ResNet):\n        img = img/255.0\n        img = img - np.array([0.485, 0.456, 0.406])\n        img = img/np.array([0.229, 0.224, 0.225]) # (shape: (512, 1024, 3))\n        img = np.transpose(img, (2, 0, 1)) # (shape: (3, 512, 1024))\n        img = img.astype(np.float32)\n\n        # convert numpy -> torch:\n        img = torch.from_numpy(img) # (shape: (3, 512, 1024))\n\n        return (img, img_id)\n\n    def __len__(self):\n        return self.num_examples\n'"
train.py,14,"b'# camera-ready\n\nimport sys\n\nfrom datasets import DatasetTrain, DatasetVal # (this needs to be imported before torch, because cv2 needs to be imported before torch for some reason)\n\nsys.path.append(""/root/deeplabv3/model"")\nfrom deeplabv3 import DeepLabV3\n\nsys.path.append(""/root/deeplabv3/utils"")\nfrom utils import add_weight_decay\n\nimport torch\nimport torch.utils.data\nimport torch.nn as nn\nfrom torch.autograd import Variable\nimport torch.optim as optim\nimport torch.nn.functional as F\n\nimport numpy as np\nimport pickle\nimport matplotlib\nmatplotlib.use(""Agg"")\nimport matplotlib.pyplot as plt\nimport cv2\n\nimport time\n\n# NOTE! NOTE! change this to not overwrite all log data when you train the model:\nmodel_id = ""1""\n\nnum_epochs = 1000\nbatch_size = 3\nlearning_rate = 0.0001\n\nnetwork = DeepLabV3(model_id, project_dir=""/root/deeplabv3"").cuda()\n\ntrain_dataset = DatasetTrain(cityscapes_data_path=""/root/deeplabv3/data/cityscapes"",\n                             cityscapes_meta_path=""/root/deeplabv3/data/cityscapes/meta"")\nval_dataset = DatasetVal(cityscapes_data_path=""/root/deeplabv3/data/cityscapes"",\n                         cityscapes_meta_path=""/root/deeplabv3/data/cityscapes/meta"")\n\nnum_train_batches = int(len(train_dataset)/batch_size)\nnum_val_batches = int(len(val_dataset)/batch_size)\nprint (""num_train_batches:"", num_train_batches)\nprint (""num_val_batches:"", num_val_batches)\n\ntrain_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n                                           batch_size=batch_size, shuffle=True,\n                                           num_workers=1)\nval_loader = torch.utils.data.DataLoader(dataset=val_dataset,\n                                         batch_size=batch_size, shuffle=False,\n                                         num_workers=1)\n\nparams = add_weight_decay(network, l2_value=0.0001)\noptimizer = torch.optim.Adam(params, lr=learning_rate)\n\nwith open(""/root/deeplabv3/data/cityscapes/meta/class_weights.pkl"", ""rb"") as file: # (needed for python3)\n    class_weights = np.array(pickle.load(file))\nclass_weights = torch.from_numpy(class_weights)\nclass_weights = Variable(class_weights.type(torch.FloatTensor)).cuda()\n\n# loss function\nloss_fn = nn.CrossEntropyLoss(weight=class_weights)\n\nepoch_losses_train = []\nepoch_losses_val = []\nfor epoch in range(num_epochs):\n    print (""###########################"")\n    print (""######## NEW EPOCH ########"")\n    print (""###########################"")\n    print (""epoch: %d/%d"" % (epoch+1, num_epochs))\n\n    ############################################################################\n    # train:\n    ############################################################################\n    network.train() # (set in training mode, this affects BatchNorm and dropout)\n    batch_losses = []\n    for step, (imgs, label_imgs) in enumerate(train_loader):\n        #current_time = time.time()\n\n        imgs = Variable(imgs).cuda() # (shape: (batch_size, 3, img_h, img_w))\n        label_imgs = Variable(label_imgs.type(torch.LongTensor)).cuda() # (shape: (batch_size, img_h, img_w))\n\n        outputs = network(imgs) # (shape: (batch_size, num_classes, img_h, img_w))\n\n        # compute the loss:\n        loss = loss_fn(outputs, label_imgs)\n        loss_value = loss.data.cpu().numpy()\n        batch_losses.append(loss_value)\n\n        # optimization step:\n        optimizer.zero_grad() # (reset gradients)\n        loss.backward() # (compute gradients)\n        optimizer.step() # (perform optimization step)\n\n        #print (time.time() - current_time)\n\n    epoch_loss = np.mean(batch_losses)\n    epoch_losses_train.append(epoch_loss)\n    with open(""%s/epoch_losses_train.pkl"" % network.model_dir, ""wb"") as file:\n        pickle.dump(epoch_losses_train, file)\n    print (""train loss: %g"" % epoch_loss)\n    plt.figure(1)\n    plt.plot(epoch_losses_train, ""k^"")\n    plt.plot(epoch_losses_train, ""k"")\n    plt.ylabel(""loss"")\n    plt.xlabel(""epoch"")\n    plt.title(""train loss per epoch"")\n    plt.savefig(""%s/epoch_losses_train.png"" % network.model_dir)\n    plt.close(1)\n\n    print (""####"")\n\n    ############################################################################\n    # val:\n    ############################################################################\n    network.eval() # (set in evaluation mode, this affects BatchNorm and dropout)\n    batch_losses = []\n    for step, (imgs, label_imgs, img_ids) in enumerate(val_loader):\n        with torch.no_grad(): # (corresponds to setting volatile=True in all variables, this is done during inference to reduce memory consumption)\n            imgs = Variable(imgs).cuda() # (shape: (batch_size, 3, img_h, img_w))\n            label_imgs = Variable(label_imgs.type(torch.LongTensor)).cuda() # (shape: (batch_size, img_h, img_w))\n\n            outputs = network(imgs) # (shape: (batch_size, num_classes, img_h, img_w))\n\n            # compute the loss:\n            loss = loss_fn(outputs, label_imgs)\n            loss_value = loss.data.cpu().numpy()\n            batch_losses.append(loss_value)\n\n    epoch_loss = np.mean(batch_losses)\n    epoch_losses_val.append(epoch_loss)\n    with open(""%s/epoch_losses_val.pkl"" % network.model_dir, ""wb"") as file:\n        pickle.dump(epoch_losses_val, file)\n    print (""val loss: %g"" % epoch_loss)\n    plt.figure(1)\n    plt.plot(epoch_losses_val, ""k^"")\n    plt.plot(epoch_losses_val, ""k"")\n    plt.ylabel(""loss"")\n    plt.xlabel(""epoch"")\n    plt.title(""val loss per epoch"")\n    plt.savefig(""%s/epoch_losses_val.png"" % network.model_dir)\n    plt.close(1)\n\n    # save the model weights to disk:\n    checkpoint_path = network.checkpoints_dir + ""/model_"" + model_id +""_epoch_"" + str(epoch+1) + "".pth""\n    torch.save(network.state_dict(), checkpoint_path)\n'"
evaluation/eval_on_val.py,11,"b'# camera-ready\n\nimport sys\n\nsys.path.append(""/root/deeplabv3"")\nfrom datasets import DatasetVal # (this needs to be imported before torch, because cv2 needs to be imported before torch for some reason)\n\nsys.path.append(""/root/deeplabv3/model"")\nfrom deeplabv3 import DeepLabV3\n\nsys.path.append(""/root/deeplabv3/utils"")\nfrom utils import label_img_to_color\n\nimport torch\nimport torch.utils.data\nimport torch.nn as nn\nfrom torch.autograd import Variable\nimport torch.optim as optim\nimport torch.nn.functional as F\n\nimport numpy as np\nimport pickle\nimport matplotlib\nmatplotlib.use(""Agg"")\nimport matplotlib.pyplot as plt\nimport cv2\n\nbatch_size = 2\n\nnetwork = DeepLabV3(""eval_val"", project_dir=""/root/deeplabv3"").cuda()\nnetwork.load_state_dict(torch.load(""/root/deeplabv3/pretrained_models/model_13_2_2_2_epoch_580.pth""))\n\nval_dataset = DatasetVal(cityscapes_data_path=""/root/deeplabv3/data/cityscapes"",\n                         cityscapes_meta_path=""/root/deeplabv3/data/cityscapes/meta"")\n\nnum_val_batches = int(len(val_dataset)/batch_size)\nprint (""num_val_batches:"", num_val_batches)\n\nval_loader = torch.utils.data.DataLoader(dataset=val_dataset,\n                                         batch_size=batch_size, shuffle=False,\n                                         num_workers=1)\n\nwith open(""/root/deeplabv3/data/cityscapes/meta/class_weights.pkl"", ""rb"") as file: # (needed for python3)\n    class_weights = np.array(pickle.load(file))\nclass_weights = torch.from_numpy(class_weights)\nclass_weights = Variable(class_weights.type(torch.FloatTensor)).cuda()\n\n# loss function\nloss_fn = nn.CrossEntropyLoss(weight=class_weights)\n\nnetwork.eval() # (set in evaluation mode, this affects BatchNorm and dropout)\nbatch_losses = []\nfor step, (imgs, label_imgs, img_ids) in enumerate(val_loader):\n    with torch.no_grad(): # (corresponds to setting volatile=True in all variables, this is done during inference to reduce memory consumption)\n        imgs = Variable(imgs).cuda() # (shape: (batch_size, 3, img_h, img_w))\n        label_imgs = Variable(label_imgs.type(torch.LongTensor)).cuda() # (shape: (batch_size, img_h, img_w))\n\n        outputs = network(imgs) # (shape: (batch_size, num_classes, img_h, img_w))\n\n        # compute the loss:\n        loss = loss_fn(outputs, label_imgs)\n        loss_value = loss.data.cpu().numpy()\n        batch_losses.append(loss_value)\n\n        ########################################################################\n        # save data for visualization:\n        ########################################################################\n        outputs = outputs.data.cpu().numpy() # (shape: (batch_size, num_classes, img_h, img_w))\n        pred_label_imgs = np.argmax(outputs, axis=1) # (shape: (batch_size, img_h, img_w))\n        pred_label_imgs = pred_label_imgs.astype(np.uint8)\n\n        for i in range(pred_label_imgs.shape[0]):\n            if i == 0:\n                pred_label_img = pred_label_imgs[i] # (shape: (img_h, img_w))\n                img_id = img_ids[i]\n                img = imgs[i] # (shape: (3, img_h, img_w))\n\n                img = img.data.cpu().numpy()\n                img = np.transpose(img, (1, 2, 0)) # (shape: (img_h, img_w, 3))\n                img = img*np.array([0.229, 0.224, 0.225])\n                img = img + np.array([0.485, 0.456, 0.406])\n                img = img*255.0\n                img = img.astype(np.uint8)\n\n                pred_label_img_color = label_img_to_color(pred_label_img)\n                overlayed_img = 0.35*img + 0.65*pred_label_img_color\n                overlayed_img = overlayed_img.astype(np.uint8)\n\n                cv2.imwrite(network.model_dir + ""/"" + img_id + ""_overlayed.png"", overlayed_img)\n\nval_loss = np.mean(batch_losses)\nprint (""val loss: %g"" % val_loss)\n'"
evaluation/eval_on_val_for_metrics.py,11,"b'# camera-ready\n\nimport sys\n\nsys.path.append(""/root/deeplabv3"")\nfrom datasets import DatasetVal # (this needs to be imported before torch, because cv2 needs to be imported before torch for some reason)\n\nsys.path.append(""/root/deeplabv3/model"")\nfrom deeplabv3 import DeepLabV3\n\nsys.path.append(""/root/deeplabv3/utils"")\nfrom utils import label_img_to_color\n\nimport torch\nimport torch.utils.data\nimport torch.nn as nn\nfrom torch.autograd import Variable\nimport torch.optim as optim\nimport torch.nn.functional as F\n\nimport numpy as np\nimport pickle\nimport matplotlib\nmatplotlib.use(""Agg"")\nimport matplotlib.pyplot as plt\nimport cv2\n\ntrainId_to_id = {\n    0: 7,\n    1: 8,\n    2: 11,\n    3: 12,\n    4: 13,\n    5: 17,\n    6: 19,\n    7: 20,\n    8: 21,\n    9: 22,\n    10: 23,\n    11: 24,\n    12: 25,\n    13: 26,\n    14: 27,\n    15: 28,\n    16: 31,\n    17: 32,\n    18: 33,\n    19: 0\n}\ntrainId_to_id_map_func = np.vectorize(trainId_to_id.get)\n\nbatch_size = 2\n\nnetwork = DeepLabV3(""eval_val_for_metrics"", project_dir=""/root/deeplabv3"").cuda()\nnetwork.load_state_dict(torch.load(""/root/deeplabv3/pretrained_models/model_13_2_2_2_epoch_580.pth""))\n\nval_dataset = DatasetVal(cityscapes_data_path=""/root/deeplabv3/data/cityscapes"",\n                         cityscapes_meta_path=""/root/deeplabv3/data/cityscapes/meta"")\n\nnum_val_batches = int(len(val_dataset)/batch_size)\nprint (""num_val_batches:"", num_val_batches)\n\nval_loader = torch.utils.data.DataLoader(dataset=val_dataset,\n                                         batch_size=batch_size, shuffle=False,\n                                         num_workers=1)\n\nwith open(""/root/deeplabv3/data/cityscapes/meta/class_weights.pkl"", ""rb"") as file: # (needed for python3)\n    class_weights = np.array(pickle.load(file))\nclass_weights = torch.from_numpy(class_weights)\nclass_weights = Variable(class_weights.type(torch.FloatTensor)).cuda()\n\n# loss function\nloss_fn = nn.CrossEntropyLoss(weight=class_weights)\n\nnetwork.eval() # (set in evaluation mode, this affects BatchNorm and dropout)\nbatch_losses = []\nfor step, (imgs, label_imgs, img_ids) in enumerate(val_loader):\n    with torch.no_grad(): # (corresponds to setting volatile=True in all variables, this is done during inference to reduce memory consumption)\n        imgs = Variable(imgs).cuda() # (shape: (batch_size, 3, img_h, img_w))\n        label_imgs = Variable(label_imgs.type(torch.LongTensor)).cuda() # (shape: (batch_size, img_h, img_w))\n\n        outputs = network(imgs) # (shape: (batch_size, num_classes, img_h, img_w))\n\n        # compute the loss:\n        loss = loss_fn(outputs, label_imgs)\n        loss_value = loss.data.cpu().numpy()\n        batch_losses.append(loss_value)\n\n        ########################################################################\n        # save data for visualization:\n        ########################################################################\n        outputs = F.upsample(outputs, size=(1024, 2048), mode=""bilinear"") # (shape: (batch_size, num_classes, 1024, 2048))\n\n        outputs = outputs.data.cpu().numpy() # (shape: (batch_size, num_classes, 1024, 2048))\n        pred_label_imgs = np.argmax(outputs, axis=1) # (shape: (batch_size, 1024, 2048))\n        pred_label_imgs = pred_label_imgs.astype(np.uint8)\n\n        for i in range(pred_label_imgs.shape[0]):\n            pred_label_img = pred_label_imgs[i] # (shape: (1024, 2048))\n            img_id = img_ids[i]\n\n            # convert pred_label_img from trainId to id pixel values:\n            pred_label_img = trainId_to_id_map_func(pred_label_img) # (shape: (1024, 2048))\n            pred_label_img = pred_label_img.astype(np.uint8)\n\n            cv2.imwrite(network.model_dir + ""/"" + img_id + ""_pred_label_img.png"", pred_label_img)\n\nval_loss = np.mean(batch_losses)\nprint (""val loss: %g"" % val_loss)\n'"
model/aspp.py,4,"b'# camera-ready\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass ASPP(nn.Module):\n    def __init__(self, num_classes):\n        super(ASPP, self).__init__()\n\n        self.conv_1x1_1 = nn.Conv2d(512, 256, kernel_size=1)\n        self.bn_conv_1x1_1 = nn.BatchNorm2d(256)\n\n        self.conv_3x3_1 = nn.Conv2d(512, 256, kernel_size=3, stride=1, padding=6, dilation=6)\n        self.bn_conv_3x3_1 = nn.BatchNorm2d(256)\n\n        self.conv_3x3_2 = nn.Conv2d(512, 256, kernel_size=3, stride=1, padding=12, dilation=12)\n        self.bn_conv_3x3_2 = nn.BatchNorm2d(256)\n\n        self.conv_3x3_3 = nn.Conv2d(512, 256, kernel_size=3, stride=1, padding=18, dilation=18)\n        self.bn_conv_3x3_3 = nn.BatchNorm2d(256)\n\n        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n\n        self.conv_1x1_2 = nn.Conv2d(512, 256, kernel_size=1)\n        self.bn_conv_1x1_2 = nn.BatchNorm2d(256)\n\n        self.conv_1x1_3 = nn.Conv2d(1280, 256, kernel_size=1) # (1280 = 5*256)\n        self.bn_conv_1x1_3 = nn.BatchNorm2d(256)\n\n        self.conv_1x1_4 = nn.Conv2d(256, num_classes, kernel_size=1)\n\n    def forward(self, feature_map):\n        # (feature_map has shape (batch_size, 512, h/16, w/16)) (assuming self.resnet is ResNet18_OS16 or ResNet34_OS16. If self.resnet instead is ResNet18_OS8 or ResNet34_OS8, it will be (batch_size, 512, h/8, w/8))\n\n        feature_map_h = feature_map.size()[2] # (== h/16)\n        feature_map_w = feature_map.size()[3] # (== w/16)\n\n        out_1x1 = F.relu(self.bn_conv_1x1_1(self.conv_1x1_1(feature_map))) # (shape: (batch_size, 256, h/16, w/16))\n        out_3x3_1 = F.relu(self.bn_conv_3x3_1(self.conv_3x3_1(feature_map))) # (shape: (batch_size, 256, h/16, w/16))\n        out_3x3_2 = F.relu(self.bn_conv_3x3_2(self.conv_3x3_2(feature_map))) # (shape: (batch_size, 256, h/16, w/16))\n        out_3x3_3 = F.relu(self.bn_conv_3x3_3(self.conv_3x3_3(feature_map))) # (shape: (batch_size, 256, h/16, w/16))\n\n        out_img = self.avg_pool(feature_map) # (shape: (batch_size, 512, 1, 1))\n        out_img = F.relu(self.bn_conv_1x1_2(self.conv_1x1_2(out_img))) # (shape: (batch_size, 256, 1, 1))\n        out_img = F.upsample(out_img, size=(feature_map_h, feature_map_w), mode=""bilinear"") # (shape: (batch_size, 256, h/16, w/16))\n\n        out = torch.cat([out_1x1, out_3x3_1, out_3x3_2, out_3x3_3, out_img], 1) # (shape: (batch_size, 1280, h/16, w/16))\n        out = F.relu(self.bn_conv_1x1_3(self.conv_1x1_3(out))) # (shape: (batch_size, 256, h/16, w/16))\n        out = self.conv_1x1_4(out) # (shape: (batch_size, num_classes, h/16, w/16))\n\n        return out\n\nclass ASPP_Bottleneck(nn.Module):\n    def __init__(self, num_classes):\n        super(ASPP_Bottleneck, self).__init__()\n\n        self.conv_1x1_1 = nn.Conv2d(4*512, 256, kernel_size=1)\n        self.bn_conv_1x1_1 = nn.BatchNorm2d(256)\n\n        self.conv_3x3_1 = nn.Conv2d(4*512, 256, kernel_size=3, stride=1, padding=6, dilation=6)\n        self.bn_conv_3x3_1 = nn.BatchNorm2d(256)\n\n        self.conv_3x3_2 = nn.Conv2d(4*512, 256, kernel_size=3, stride=1, padding=12, dilation=12)\n        self.bn_conv_3x3_2 = nn.BatchNorm2d(256)\n\n        self.conv_3x3_3 = nn.Conv2d(4*512, 256, kernel_size=3, stride=1, padding=18, dilation=18)\n        self.bn_conv_3x3_3 = nn.BatchNorm2d(256)\n\n        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n\n        self.conv_1x1_2 = nn.Conv2d(4*512, 256, kernel_size=1)\n        self.bn_conv_1x1_2 = nn.BatchNorm2d(256)\n\n        self.conv_1x1_3 = nn.Conv2d(1280, 256, kernel_size=1) # (1280 = 5*256)\n        self.bn_conv_1x1_3 = nn.BatchNorm2d(256)\n\n        self.conv_1x1_4 = nn.Conv2d(256, num_classes, kernel_size=1)\n\n    def forward(self, feature_map):\n        # (feature_map has shape (batch_size, 4*512, h/16, w/16))\n\n        feature_map_h = feature_map.size()[2] # (== h/16)\n        feature_map_w = feature_map.size()[3] # (== w/16)\n\n        out_1x1 = F.relu(self.bn_conv_1x1_1(self.conv_1x1_1(feature_map))) # (shape: (batch_size, 256, h/16, w/16))\n        out_3x3_1 = F.relu(self.bn_conv_3x3_1(self.conv_3x3_1(feature_map))) # (shape: (batch_size, 256, h/16, w/16))\n        out_3x3_2 = F.relu(self.bn_conv_3x3_2(self.conv_3x3_2(feature_map))) # (shape: (batch_size, 256, h/16, w/16))\n        out_3x3_3 = F.relu(self.bn_conv_3x3_3(self.conv_3x3_3(feature_map))) # (shape: (batch_size, 256, h/16, w/16))\n\n        out_img = self.avg_pool(feature_map) # (shape: (batch_size, 512, 1, 1))\n        out_img = F.relu(self.bn_conv_1x1_2(self.conv_1x1_2(out_img))) # (shape: (batch_size, 256, 1, 1))\n        out_img = F.upsample(out_img, size=(feature_map_h, feature_map_w), mode=""bilinear"") # (shape: (batch_size, 256, h/16, w/16))\n\n        out = torch.cat([out_1x1, out_3x3_1, out_3x3_2, out_3x3_3, out_img], 1) # (shape: (batch_size, 1280, h/16, w/16))\n        out = F.relu(self.bn_conv_1x1_3(self.conv_1x1_3(out))) # (shape: (batch_size, 256, h/16, w/16))\n        out = self.conv_1x1_4(out) # (shape: (batch_size, num_classes, h/16, w/16))\n\n        return out\n'"
model/deeplabv3.py,2,"b'# camera-ready\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nimport os\n\nfrom resnet import ResNet18_OS16, ResNet34_OS16, ResNet50_OS16, ResNet101_OS16, ResNet152_OS16, ResNet18_OS8, ResNet34_OS8\nfrom aspp import ASPP, ASPP_Bottleneck\n\nclass DeepLabV3(nn.Module):\n    def __init__(self, model_id, project_dir):\n        super(DeepLabV3, self).__init__()\n\n        self.num_classes = 20\n\n        self.model_id = model_id\n        self.project_dir = project_dir\n        self.create_model_dirs()\n\n        self.resnet = ResNet18_OS8() # NOTE! specify the type of ResNet here\n        self.aspp = ASPP(num_classes=self.num_classes) # NOTE! if you use ResNet50-152, set self.aspp = ASPP_Bottleneck(num_classes=self.num_classes) instead\n\n    def forward(self, x):\n        # (x has shape (batch_size, 3, h, w))\n\n        h = x.size()[2]\n        w = x.size()[3]\n\n        feature_map = self.resnet(x) # (shape: (batch_size, 512, h/16, w/16)) (assuming self.resnet is ResNet18_OS16 or ResNet34_OS16. If self.resnet is ResNet18_OS8 or ResNet34_OS8, it will be (batch_size, 512, h/8, w/8). If self.resnet is ResNet50-152, it will be (batch_size, 4*512, h/16, w/16))\n\n        output = self.aspp(feature_map) # (shape: (batch_size, num_classes, h/16, w/16))\n\n        output = F.upsample(output, size=(h, w), mode=""bilinear"") # (shape: (batch_size, num_classes, h, w))\n\n        return output\n\n    def create_model_dirs(self):\n        self.logs_dir = self.project_dir + ""/training_logs""\n        self.model_dir = self.logs_dir + ""/model_%s"" % self.model_id\n        self.checkpoints_dir = self.model_dir + ""/checkpoints""\n        if not os.path.exists(self.logs_dir):\n            os.makedirs(self.logs_dir)\n        if not os.path.exists(self.model_dir):\n            os.makedirs(self.model_dir)\n            os.makedirs(self.checkpoints_dir)\n'"
model/resnet.py,9,"b'# camera-ready\n\n# NOTE! OS: output stride, the ratio of input image resolution to final output resolution (OS16: output size is (img_h/16, img_w/16)) (OS8: output size is (img_h/8, img_w/8))\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision.models as models\n\ndef make_layer(block, in_channels, channels, num_blocks, stride=1, dilation=1):\n    strides = [stride] + [1]*(num_blocks - 1) # (stride == 2, num_blocks == 4 --> strides == [2, 1, 1, 1])\n\n    blocks = []\n    for stride in strides:\n        blocks.append(block(in_channels=in_channels, channels=channels, stride=stride, dilation=dilation))\n        in_channels = block.expansion*channels\n\n    layer = nn.Sequential(*blocks) # (*blocks: call with unpacked list entires as arguments)\n\n    return layer\n\nclass BasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self, in_channels, channels, stride=1, dilation=1):\n        super(BasicBlock, self).__init__()\n\n        out_channels = self.expansion*channels\n\n        self.conv1 = nn.Conv2d(in_channels, channels, kernel_size=3, stride=stride, padding=dilation, dilation=dilation, bias=False)\n        self.bn1 = nn.BatchNorm2d(channels)\n\n        self.conv2 = nn.Conv2d(channels, channels, kernel_size=3, stride=1, padding=dilation, dilation=dilation, bias=False)\n        self.bn2 = nn.BatchNorm2d(channels)\n\n        if (stride != 1) or (in_channels != out_channels):\n            conv = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False)\n            bn = nn.BatchNorm2d(out_channels)\n            self.downsample = nn.Sequential(conv, bn)\n        else:\n            self.downsample = nn.Sequential()\n\n    def forward(self, x):\n        # (x has shape: (batch_size, in_channels, h, w))\n\n        out = F.relu(self.bn1(self.conv1(x))) # (shape: (batch_size, channels, h, w) if stride == 1, (batch_size, channels, h/2, w/2) if stride == 2)\n        out = self.bn2(self.conv2(out)) # (shape: (batch_size, channels, h, w) if stride == 1, (batch_size, channels, h/2, w/2) if stride == 2)\n\n        out = out + self.downsample(x) # (shape: (batch_size, channels, h, w) if stride == 1, (batch_size, channels, h/2, w/2) if stride == 2)\n\n        out = F.relu(out) # (shape: (batch_size, channels, h, w) if stride == 1, (batch_size, channels, h/2, w/2) if stride == 2)\n\n        return out\n\nclass Bottleneck(nn.Module):\n    expansion = 4\n\n    def __init__(self, in_channels, channels, stride=1, dilation=1):\n        super(Bottleneck, self).__init__()\n\n        out_channels = self.expansion*channels\n\n        self.conv1 = nn.Conv2d(in_channels, channels, kernel_size=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(channels)\n\n        self.conv2 = nn.Conv2d(channels, channels, kernel_size=3, stride=stride, padding=dilation, dilation=dilation, bias=False)\n        self.bn2 = nn.BatchNorm2d(channels)\n\n        self.conv3 = nn.Conv2d(channels, out_channels, kernel_size=1, bias=False)\n        self.bn3 = nn.BatchNorm2d(out_channels)\n\n        if (stride != 1) or (in_channels != out_channels):\n            conv = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False)\n            bn = nn.BatchNorm2d(out_channels)\n            self.downsample = nn.Sequential(conv, bn)\n        else:\n            self.downsample = nn.Sequential()\n\n    def forward(self, x):\n        # (x has shape: (batch_size, in_channels, h, w))\n\n        out = F.relu(self.bn1(self.conv1(x))) # (shape: (batch_size, channels, h, w))\n        out = F.relu(self.bn2(self.conv2(out))) # (shape: (batch_size, channels, h, w) if stride == 1, (batch_size, channels, h/2, w/2) if stride == 2)\n        out = self.bn3(self.conv3(out)) # (shape: (batch_size, out_channels, h, w) if stride == 1, (batch_size, out_channels, h/2, w/2) if stride == 2)\n\n        out = out + self.downsample(x) # (shape: (batch_size, out_channels, h, w) if stride == 1, (batch_size, out_channels, h/2, w/2) if stride == 2)\n\n        out = F.relu(out) # (shape: (batch_size, out_channels, h, w) if stride == 1, (batch_size, out_channels, h/2, w/2) if stride == 2)\n\n        return out\n\nclass ResNet_Bottleneck_OS16(nn.Module):\n    def __init__(self, num_layers):\n        super(ResNet_Bottleneck_OS16, self).__init__()\n\n        if num_layers == 50:\n            resnet = models.resnet50()\n            # load pretrained model:\n            resnet.load_state_dict(torch.load(""/root/deeplabv3/pretrained_models/resnet/resnet50-19c8e357.pth""))\n            # remove fully connected layer, avg pool and layer5:\n            self.resnet = nn.Sequential(*list(resnet.children())[:-3])\n\n            print (""pretrained resnet, 50"")\n        elif num_layers == 101:\n            resnet = models.resnet101()\n            # load pretrained model:\n            resnet.load_state_dict(torch.load(""/root/deeplabv3/pretrained_models/resnet/resnet101-5d3b4d8f.pth""))\n            # remove fully connected layer, avg pool and layer5:\n            self.resnet = nn.Sequential(*list(resnet.children())[:-3])\n\n            print (""pretrained resnet, 101"")\n        elif num_layers == 152:\n            resnet = models.resnet152()\n            # load pretrained model:\n            resnet.load_state_dict(torch.load(""/root/deeplabv3/pretrained_models/resnet/resnet152-b121ed2d.pth""))\n            # remove fully connected layer, avg pool and layer5:\n            self.resnet = nn.Sequential(*list(resnet.children())[:-3])\n\n            print (""pretrained resnet, 152"")\n        else:\n            raise Exception(""num_layers must be in {50, 101, 152}!"")\n\n        self.layer5 = make_layer(Bottleneck, in_channels=4*256, channels=512, num_blocks=3, stride=1, dilation=2)\n\n    def forward(self, x):\n        # (x has shape (batch_size, 3, h, w))\n\n        # pass x through (parts of) the pretrained ResNet:\n        c4 = self.resnet(x) # (shape: (batch_size, 4*256, h/16, w/16)) (it\'s called c4 since 16 == 2^4)\n\n        output = self.layer5(c4) # (shape: (batch_size, 4*512, h/16, w/16))\n\n        return output\n\nclass ResNet_BasicBlock_OS16(nn.Module):\n    def __init__(self, num_layers):\n        super(ResNet_BasicBlock_OS16, self).__init__()\n\n        if num_layers == 18:\n            resnet = models.resnet18()\n            # load pretrained model:\n            resnet.load_state_dict(torch.load(""/root/deeplabv3/pretrained_models/resnet/resnet18-5c106cde.pth""))\n            # remove fully connected layer, avg pool and layer5:\n            self.resnet = nn.Sequential(*list(resnet.children())[:-3])\n\n            num_blocks = 2\n            print (""pretrained resnet, 18"")\n        elif num_layers == 34:\n            resnet = models.resnet34()\n            # load pretrained model:\n            resnet.load_state_dict(torch.load(""/root/deeplabv3/pretrained_models/resnet/resnet34-333f7ec4.pth""))\n            # remove fully connected layer, avg pool and layer5:\n            self.resnet = nn.Sequential(*list(resnet.children())[:-3])\n\n            num_blocks = 3\n            print (""pretrained resnet, 34"")\n        else:\n            raise Exception(""num_layers must be in {18, 34}!"")\n\n        self.layer5 = make_layer(BasicBlock, in_channels=256, channels=512, num_blocks=num_blocks, stride=1, dilation=2)\n\n    def forward(self, x):\n        # (x has shape (batch_size, 3, h, w))\n\n        # pass x through (parts of) the pretrained ResNet:\n        c4 = self.resnet(x) # (shape: (batch_size, 256, h/16, w/16)) (it\'s called c4 since 16 == 2^4)\n\n        output = self.layer5(c4) # (shape: (batch_size, 512, h/16, w/16))\n\n        return output\n\nclass ResNet_BasicBlock_OS8(nn.Module):\n    def __init__(self, num_layers):\n        super(ResNet_BasicBlock_OS8, self).__init__()\n\n        if num_layers == 18:\n            resnet = models.resnet18()\n            # load pretrained model:\n            resnet.load_state_dict(torch.load(""/root/deeplabv3/pretrained_models/resnet/resnet18-5c106cde.pth""))\n            # remove fully connected layer, avg pool, layer4 and layer5:\n            self.resnet = nn.Sequential(*list(resnet.children())[:-4])\n\n            num_blocks_layer_4 = 2\n            num_blocks_layer_5 = 2\n            print (""pretrained resnet, 18"")\n        elif num_layers == 34:\n            resnet = models.resnet34()\n            # load pretrained model:\n            resnet.load_state_dict(torch.load(""/root/deeplabv3/pretrained_models/resnet/resnet34-333f7ec4.pth""))\n            # remove fully connected layer, avg pool, layer4 and layer5:\n            self.resnet = nn.Sequential(*list(resnet.children())[:-4])\n\n            num_blocks_layer_4 = 6\n            num_blocks_layer_5 = 3\n            print (""pretrained resnet, 34"")\n        else:\n            raise Exception(""num_layers must be in {18, 34}!"")\n\n        self.layer4 = make_layer(BasicBlock, in_channels=128, channels=256, num_blocks=num_blocks_layer_4, stride=1, dilation=2)\n\n        self.layer5 = make_layer(BasicBlock, in_channels=256, channels=512, num_blocks=num_blocks_layer_5, stride=1, dilation=4)\n\n    def forward(self, x):\n        # (x has shape (batch_size, 3, h, w))\n\n        # pass x through (parts of) the pretrained ResNet:\n        c3 = self.resnet(x) # (shape: (batch_size, 128, h/8, w/8)) (it\'s called c3 since 8 == 2^3)\n\n        output = self.layer4(c3) # (shape: (batch_size, 256, h/8, w/8))\n        output = self.layer5(output) # (shape: (batch_size, 512, h/8, w/8))\n\n        return output\n\ndef ResNet18_OS16():\n    return ResNet_BasicBlock_OS16(num_layers=18)\n\ndef ResNet34_OS16():\n    return ResNet_BasicBlock_OS16(num_layers=34)\n\ndef ResNet50_OS16():\n    return ResNet_Bottleneck_OS16(num_layers=50)\n\ndef ResNet101_OS16():\n    return ResNet_Bottleneck_OS16(num_layers=101)\n\ndef ResNet152_OS16():\n    return ResNet_Bottleneck_OS16(num_layers=152)\n\ndef ResNet18_OS8():\n    return ResNet_BasicBlock_OS8(num_layers=18)\n\ndef ResNet34_OS8():\n    return ResNet_BasicBlock_OS8(num_layers=34)\n'"
utils/preprocess_data.py,0,"b'# camera-ready\n\nimport pickle\nimport numpy as np\nimport cv2\nimport os\nfrom collections import namedtuple\n\n# (NOTE! this is taken from the official Cityscapes scripts:)\nLabel = namedtuple( \'Label\' , [\n\n    \'name\'        , # The identifier of this label, e.g. \'car\', \'person\', ... .\n                    # We use them to uniquely name a class\n\n    \'id\'          , # An integer ID that is associated with this label.\n                    # The IDs are used to represent the label in ground truth images\n                    # An ID of -1 means that this label does not have an ID and thus\n                    # is ignored when creating ground truth images (e.g. license plate).\n                    # Do not modify these IDs, since exactly these IDs are expected by the\n                    # evaluation server.\n\n    \'trainId\'     , # Feel free to modify these IDs as suitable for your method. Then create\n                    # ground truth images with train IDs, using the tools provided in the\n                    # \'preparation\' folder. However, make sure to validate or submit results\n                    # to our evaluation server using the regular IDs above!\n                    # For trainIds, multiple labels might have the same ID. Then, these labels\n                    # are mapped to the same class in the ground truth images. For the inverse\n                    # mapping, we use the label that is defined first in the list below.\n                    # For example, mapping all void-type classes to the same ID in training,\n                    # might make sense for some approaches.\n                    # Max value is 255!\n\n    \'category\'    , # The name of the category that this label belongs to\n\n    \'categoryId\'  , # The ID of this category. Used to create ground truth images\n                    # on category level.\n\n    \'hasInstances\', # Whether this label distinguishes between single instances or not\n\n    \'ignoreInEval\', # Whether pixels having this class as ground truth label are ignored\n                    # during evaluations or not\n\n    \'color\'       , # The color of this label\n    ] )\n\n# (NOTE! this is taken from the official Cityscapes scripts:)\nlabels = [\n    #       name                     id    trainId   category            catId     hasInstances   ignoreInEval   color\n    Label(  \'unlabeled\'            ,  0 ,      19 , \'void\'            , 0       , False        , True         , (  0,  0,  0) ),\n    Label(  \'ego vehicle\'          ,  1 ,      19 , \'void\'            , 0       , False        , True         , (  0,  0,  0) ),\n    Label(  \'rectification border\' ,  2 ,      19 , \'void\'            , 0       , False        , True         , (  0,  0,  0) ),\n    Label(  \'out of roi\'           ,  3 ,      19 , \'void\'            , 0       , False        , True         , (  0,  0,  0) ),\n    Label(  \'static\'               ,  4 ,      19 , \'void\'            , 0       , False        , True         , (  0,  0,  0) ),\n    Label(  \'dynamic\'              ,  5 ,      19 , \'void\'            , 0       , False        , True         , (111, 74,  0) ),\n    Label(  \'ground\'               ,  6 ,      19 , \'void\'            , 0       , False        , True         , ( 81,  0, 81) ),\n    Label(  \'road\'                 ,  7 ,        0 , \'flat\'            , 1       , False        , False        , (128, 64,128) ),\n    Label(  \'sidewalk\'             ,  8 ,        1 , \'flat\'            , 1       , False        , False        , (244, 35,232) ),\n    Label(  \'parking\'              ,  9 ,      19 , \'flat\'            , 1       , False        , True         , (250,170,160) ),\n    Label(  \'rail track\'           , 10 ,      19 , \'flat\'            , 1       , False        , True         , (230,150,140) ),\n    Label(  \'building\'             , 11 ,        2 , \'construction\'    , 2       , False        , False        , ( 70, 70, 70) ),\n    Label(  \'wall\'                 , 12 ,        3 , \'construction\'    , 2       , False        , False        , (102,102,156) ),\n    Label(  \'fence\'                , 13 ,        4 , \'construction\'    , 2       , False        , False        , (190,153,153) ),\n    Label(  \'guard rail\'           , 14 ,      19 , \'construction\'    , 2       , False        , True         , (180,165,180) ),\n    Label(  \'bridge\'               , 15 ,      19 , \'construction\'    , 2       , False        , True         , (150,100,100) ),\n    Label(  \'tunnel\'               , 16 ,      19 , \'construction\'    , 2       , False        , True         , (150,120, 90) ),\n    Label(  \'pole\'                 , 17 ,        5 , \'object\'          , 3       , False        , False        , (153,153,153) ),\n    Label(  \'polegroup\'            , 18 ,      19 , \'object\'          , 3       , False        , True         , (153,153,153) ),\n    Label(  \'traffic light\'        , 19 ,        6 , \'object\'          , 3       , False        , False        , (250,170, 30) ),\n    Label(  \'traffic sign\'         , 20 ,        7 , \'object\'          , 3       , False        , False        , (220,220,  0) ),\n    Label(  \'vegetation\'           , 21 ,        8 , \'nature\'          , 4       , False        , False        , (107,142, 35) ),\n    Label(  \'terrain\'              , 22 ,        9 , \'nature\'          , 4       , False        , False        , (152,251,152) ),\n    Label(  \'sky\'                  , 23 ,       10 , \'sky\'             , 5       , False        , False        , ( 70,130,180) ),\n    Label(  \'person\'               , 24 ,       11 , \'human\'           , 6       , True         , False        , (220, 20, 60) ),\n    Label(  \'rider\'                , 25 ,       12 , \'human\'           , 6       , True         , False        , (255,  0,  0) ),\n    Label(  \'car\'                  , 26 ,       13 , \'vehicle\'         , 7       , True         , False        , (  0,  0,142) ),\n    Label(  \'truck\'                , 27 ,       14 , \'vehicle\'         , 7       , True         , False        , (  0,  0, 70) ),\n    Label(  \'bus\'                  , 28 ,       15 , \'vehicle\'         , 7       , True         , False        , (  0, 60,100) ),\n    Label(  \'caravan\'              , 29 ,      19 , \'vehicle\'         , 7       , True         , True         , (  0,  0, 90) ),\n    Label(  \'trailer\'              , 30 ,      19 , \'vehicle\'         , 7       , True         , True         , (  0,  0,110) ),\n    Label(  \'train\'                , 31 ,       16 , \'vehicle\'         , 7       , True         , False        , (  0, 80,100) ),\n    Label(  \'motorcycle\'           , 32 ,       17 , \'vehicle\'         , 7       , True         , False        , (  0,  0,230) ),\n    Label(  \'bicycle\'              , 33 ,       18 , \'vehicle\'         , 7       , True         , False        , (119, 11, 32) ),\n    Label(  \'license plate\'        , -1 ,       19 , \'vehicle\'         , 7       , False        , True         , (  0,  0,142) ),\n]\n\n# create a function which maps id to trainId:\nid_to_trainId = {label.id: label.trainId for label in labels}\nid_to_trainId_map_func = np.vectorize(id_to_trainId.get)\n\ntrain_dirs = [""jena/"", ""zurich/"", ""weimar/"", ""ulm/"", ""tubingen/"", ""stuttgart/"",\n              ""strasbourg/"", ""monchengladbach/"", ""krefeld/"", ""hanover/"",\n              ""hamburg/"", ""erfurt/"", ""dusseldorf/"", ""darmstadt/"", ""cologne/"",\n              ""bremen/"", ""bochum/"", ""aachen/""]\nval_dirs = [""frankfurt/"", ""munster/"", ""lindau/""]\ntest_dirs = [""berlin"", ""bielefeld"", ""bonn"", ""leverkusen"", ""mainz"", ""munich""]\n\ncityscapes_data_path = ""/root/deeplabv3/data/cityscapes""\ncityscapes_meta_path = ""/root/deeplabv3/data/cityscapes/meta""\n\nif not os.path.exists(cityscapes_meta_path):\n    os.makedirs(cityscapes_meta_path)\nif not os.path.exists(cityscapes_meta_path + ""/label_imgs""):\n    os.makedirs(cityscapes_meta_path + ""/label_imgs"")\n\n################################################################################\n# convert all labels to label imgs with trainId pixel values (and save to disk):\n################################################################################\ntrain_label_img_paths = []\n\nimg_dir = cityscapes_data_path + ""/leftImg8bit/train/""\nlabel_dir = cityscapes_data_path + ""/gtFine/train/""\nfor train_dir in train_dirs:\n    print (train_dir)\n\n    train_img_dir_path = img_dir + train_dir\n    train_label_dir_path = label_dir + train_dir\n\n    file_names = os.listdir(train_img_dir_path)\n    for file_name in file_names:\n        img_id = file_name.split(""_leftImg8bit.png"")[0]\n\n        gtFine_img_path = train_label_dir_path + img_id + ""_gtFine_labelIds.png""\n        gtFine_img = cv2.imread(gtFine_img_path, -1) # (shape: (1024, 2048))\n\n        # convert gtFine_img from id to trainId pixel values:\n        label_img = id_to_trainId_map_func(gtFine_img) # (shape: (1024, 2048))\n        label_img = label_img.astype(np.uint8)\n\n        cv2.imwrite(cityscapes_meta_path + ""/label_imgs/"" + img_id + "".png"", label_img)\n        train_label_img_paths.append(cityscapes_meta_path + ""/label_imgs/"" + img_id + "".png"")\n\nimg_dir = cityscapes_data_path + ""/leftImg8bit/val/""\nlabel_dir = cityscapes_data_path + ""/gtFine/val/""\nfor val_dir in val_dirs:\n    print (val_dir)\n\n    val_img_dir_path = img_dir + val_dir\n    val_label_dir_path = label_dir + val_dir\n\n    file_names = os.listdir(val_img_dir_path)\n    for file_name in file_names:\n        img_id = file_name.split(""_leftImg8bit.png"")[0]\n\n        gtFine_img_path = val_label_dir_path + img_id + ""_gtFine_labelIds.png""\n        gtFine_img = cv2.imread(gtFine_img_path, -1) # (shape: (1024, 2048))\n\n        # convert gtFine_img from id to trainId pixel values:\n        label_img = id_to_trainId_map_func(gtFine_img) # (shape: (1024, 2048))\n        label_img = label_img.astype(np.uint8)\n\n        cv2.imwrite(cityscapes_meta_path + ""/label_imgs/"" + img_id + "".png"", label_img)\n\n################################################################################\n# compute the class weigths:\n################################################################################\nprint (""computing class weights"")\n\nnum_classes = 20\n\ntrainId_to_count = {}\nfor trainId in range(num_classes):\n    trainId_to_count[trainId] = 0\n\n# get the total number of pixels in all train label_imgs that are of each object class:\nfor step, label_img_path in enumerate(train_label_img_paths):\n    if step % 100 == 0:\n        print (step)\n\n    label_img = cv2.imread(label_img_path, -1)\n\n    for trainId in range(num_classes):\n        # count how many pixels in label_img which are of object class trainId:\n        trainId_mask = np.equal(label_img, trainId)\n        trainId_count = np.sum(trainId_mask)\n\n        # add to the total count:\n        trainId_to_count[trainId] += trainId_count\n\n# compute the class weights according to the ENet paper:\nclass_weights = []\ntotal_count = sum(trainId_to_count.values())\nfor trainId, count in trainId_to_count.items():\n    trainId_prob = float(count)/float(total_count)\n    trainId_weight = 1/np.log(1.02 + trainId_prob)\n    class_weights.append(trainId_weight)\n\nprint (class_weights)\n\nwith open(cityscapes_meta_path + ""/class_weights.pkl"", ""wb"") as file:\n    pickle.dump(class_weights, file, protocol=2) # (protocol=2 is needed to be able to open this file with python2)\n'"
utils/random_code.py,0,"b'# camera-ready\n\n# this file contains code snippets which I have found (more or less) useful at\n# some point during the project. Probably nothing interesting to see here.\n\nimport pickle\nimport matplotlib\nmatplotlib.use(""Agg"")\nimport matplotlib.pyplot as plt\n\nimport numpy as np\n\nmodel_id = ""13_2_2_2""\n\nwith open(""/home/fregu856/exjobb/training_logs/multitask/model_"" + model_id + ""/epoch_losses_train.pkl"", ""rb"") as file:\n    train_loss = pickle.load(file)\n\nwith open(""/home/fregu856/exjobb/training_logs/multitask/model_"" + model_id + ""/epoch_losses_val.pkl"", ""rb"") as file:\n    val_loss = pickle.load(file)\n\nprint (""train loss min:"", np.argmin(np.array(train_loss)), np.min(np.array(train_loss)))\n\nprint (""val loss min:"", np.argmin(np.array(val_loss)), np.min(np.array(val_loss)))\n'"
utils/utils.py,1,"b'# camera-ready\n\nimport torch\nimport torch.nn as nn\n\nimport numpy as np\n\ndef add_weight_decay(net, l2_value, skip_list=()):\n    # https://raberrytv.wordpress.com/2017/10/29/pytorch-weight-decay-made-easy/\n\n    decay, no_decay = [], []\n    for name, param in net.named_parameters():\n        if not param.requires_grad:\n            continue # frozen weights\n        if len(param.shape) == 1 or name.endswith("".bias"") or name in skip_list:\n            no_decay.append(param)\n        else:\n            decay.append(param)\n\n    return [{\'params\': no_decay, \'weight_decay\': 0.0}, {\'params\': decay, \'weight_decay\': l2_value}]\n\n# function for colorizing a label image:\ndef label_img_to_color(img):\n    label_to_color = {\n        0: [128, 64,128],\n        1: [244, 35,232],\n        2: [ 70, 70, 70],\n        3: [102,102,156],\n        4: [190,153,153],\n        5: [153,153,153],\n        6: [250,170, 30],\n        7: [220,220,  0],\n        8: [107,142, 35],\n        9: [152,251,152],\n        10: [ 70,130,180],\n        11: [220, 20, 60],\n        12: [255,  0,  0],\n        13: [  0,  0,142],\n        14: [  0,  0, 70],\n        15: [  0, 60,100],\n        16: [  0, 80,100],\n        17: [  0,  0,230],\n        18: [119, 11, 32],\n        19: [81,  0, 81]\n        }\n\n    img_height, img_width = img.shape\n\n    img_color = np.zeros((img_height, img_width, 3))\n    for row in range(img_height):\n        for col in range(img_width):\n            label = img[row, col]\n\n            img_color[row, col] = np.array(label_to_color[label])\n\n    return img_color\n'"
visualization/run_on_seq.py,8,"b'# camera-ready\n\nimport sys\n\nsys.path.append(""/root/deeplabv3"")\nfrom datasets import DatasetSeq # (this needs to be imported before torch, because cv2 needs to be imported before torch for some reason)\n\nsys.path.append(""/root/deeplabv3/model"")\nfrom deeplabv3 import DeepLabV3\n\nsys.path.append(""/root/deeplabv3/utils"")\nfrom utils import label_img_to_color\n\nimport torch\nimport torch.utils.data\nimport torch.nn as nn\nfrom torch.autograd import Variable\nimport torch.optim as optim\nimport torch.nn.functional as F\n\nimport numpy as np\nimport pickle\nimport matplotlib\nmatplotlib.use(""Agg"")\nimport matplotlib.pyplot as plt\nimport cv2\n\nimport os\n\nbatch_size = 2\n\nnetwork = DeepLabV3(""eval_seq"", project_dir=""/root/deeplabv3"").cuda()\nnetwork.load_state_dict(torch.load(""/root/deeplabv3/pretrained_models/model_13_2_2_2_epoch_580.pth""))\n\nfor sequence in [""00"", ""01"", ""02""]:\n    print (sequence)\n\n    val_dataset = DatasetSeq(cityscapes_data_path=""/root/deeplabv3/data/cityscapes"",\n                             cityscapes_meta_path=""/root/deeplabv3/data/cityscapes/meta"",\n                             sequence=sequence)\n\n    num_val_batches = int(len(val_dataset)/batch_size)\n    print (""num_val_batches:"", num_val_batches)\n\n    val_loader = torch.utils.data.DataLoader(dataset=val_dataset,\n                                             batch_size=batch_size, shuffle=False,\n                                             num_workers=1)\n\n    network.eval() # (set in evaluation mode, this affects BatchNorm and dropout)\n    unsorted_img_ids = []\n    for step, (imgs, img_ids) in enumerate(val_loader):\n        with torch.no_grad(): # (corresponds to setting volatile=True in all variables, this is done during inference to reduce memory consumption)\n            imgs = Variable(imgs).cuda() # (shape: (batch_size, 3, img_h, img_w))\n\n            outputs = network(imgs) # (shape: (batch_size, num_classes, img_h, img_w))\n\n            ####################################################################\n            # save data for visualization:\n            ####################################################################\n            outputs = outputs.data.cpu().numpy() # (shape: (batch_size, num_classes, img_h, img_w))\n            pred_label_imgs = np.argmax(outputs, axis=1) # (shape: (batch_size, img_h, img_w))\n            pred_label_imgs = pred_label_imgs.astype(np.uint8)\n\n            for i in range(pred_label_imgs.shape[0]):\n                pred_label_img = pred_label_imgs[i] # (shape: (img_h, img_w))\n                img_id = img_ids[i]\n                img = imgs[i] # (shape: (3, img_h, img_w))\n\n                img = img.data.cpu().numpy()\n                img = np.transpose(img, (1, 2, 0)) # (shape: (img_h, img_w, 3))\n                img = img*np.array([0.229, 0.224, 0.225])\n                img = img + np.array([0.485, 0.456, 0.406])\n                img = img*255.0\n                img = img.astype(np.uint8)\n\n                pred_label_img_color = label_img_to_color(pred_label_img)\n                overlayed_img = 0.35*img + 0.65*pred_label_img_color\n                overlayed_img = overlayed_img.astype(np.uint8)\n\n                img_h = overlayed_img.shape[0]\n                img_w = overlayed_img.shape[1]\n\n                cv2.imwrite(network.model_dir + ""/"" + img_id + "".png"", img)\n                cv2.imwrite(network.model_dir + ""/"" + img_id + ""_pred.png"", pred_label_img_color)\n                cv2.imwrite(network.model_dir + ""/"" + img_id + ""_overlayed.png"", overlayed_img)\n\n                unsorted_img_ids.append(img_id)\n\n    ############################################################################\n    # create visualization video:\n    ############################################################################\n    out = cv2.VideoWriter(""%s/stuttgart_%s_combined.avi"" % (network.model_dir, sequence), cv2.VideoWriter_fourcc(*""MJPG""), 20, (2*img_w, 2*img_h))\n    sorted_img_ids = sorted(unsorted_img_ids)\n    for img_id in sorted_img_ids:\n        img = cv2.imread(network.model_dir + ""/"" + img_id + "".png"", -1)\n        pred_img = cv2.imread(network.model_dir + ""/"" + img_id + ""_pred.png"", -1)\n        overlayed_img = cv2.imread(network.model_dir + ""/"" + img_id + ""_overlayed.png"", -1)\n\n        combined_img = np.zeros((2*img_h, 2*img_w, 3), dtype=np.uint8)\n\n        combined_img[0:img_h, 0:img_w] = img\n        combined_img[0:img_h, img_w:(2*img_w)] = pred_img\n        combined_img[img_h:(2*img_h), (int(img_w/2)):(img_w + int(img_w/2))] = overlayed_img\n\n        out.write(combined_img)\n\n    out.release()\n'"
visualization/run_on_thn_seq.py,8,"b'# camera-ready\n\nimport sys\n\nsys.path.append(""/root/deeplabv3"")\nfrom datasets import DatasetThnSeq # (this needs to be imported before torch, because cv2 needs to be imported before torch for some reason)\n\nsys.path.append(""/root/deeplabv3/model"")\nfrom deeplabv3 import DeepLabV3\n\nsys.path.append(""/root/deeplabv3/utils"")\nfrom utils import label_img_to_color\n\nimport torch\nimport torch.utils.data\nimport torch.nn as nn\nfrom torch.autograd import Variable\nimport torch.optim as optim\nimport torch.nn.functional as F\n\nimport numpy as np\nimport pickle\nimport matplotlib\nmatplotlib.use(""Agg"")\nimport matplotlib.pyplot as plt\nimport cv2\n\nbatch_size = 2\n\nnetwork = DeepLabV3(""eval_seq_thn"", project_dir=""/root/deeplabv3"").cuda()\nnetwork.load_state_dict(torch.load(""/root/deeplabv3/pretrained_models/model_13_2_2_2_epoch_580.pth""))\n\nval_dataset = DatasetThnSeq(thn_data_path=""/root/deeplabv3/data/thn"")\n\nnum_val_batches = int(len(val_dataset)/batch_size)\nprint (""num_val_batches:"", num_val_batches)\n\nval_loader = torch.utils.data.DataLoader(dataset=val_dataset,\n                                         batch_size=batch_size, shuffle=False,\n                                         num_workers=1)\n\nnetwork.eval() # (set in evaluation mode, this affects BatchNorm and dropout)\nunsorted_img_ids = []\nfor step, (imgs, img_ids) in enumerate(val_loader):\n    with torch.no_grad(): # (corresponds to setting volatile=True in all variables, this is done during inference to reduce memory consumption)\n        imgs = Variable(imgs).cuda() # (shape: (batch_size, 3, img_h, img_w))\n\n        outputs = network(imgs) # (shape: (batch_size, num_classes, img_h, img_w))\n\n        ########################################################################\n        # save data for visualization:\n        ########################################################################\n        outputs = outputs.data.cpu().numpy() # (shape: (batch_size, num_classes, img_h, img_w))\n        pred_label_imgs = np.argmax(outputs, axis=1) # (shape: (batch_size, img_h, img_w))\n        pred_label_imgs = pred_label_imgs.astype(np.uint8)\n\n        for i in range(pred_label_imgs.shape[0]):\n            pred_label_img = pred_label_imgs[i] # (shape: (img_h, img_w))\n            img_id = img_ids[i]\n            img = imgs[i] # (shape: (3, img_h, img_w))\n\n            img = img.data.cpu().numpy()\n            img = np.transpose(img, (1, 2, 0)) # (shape: (img_h, img_w, 3))\n            img = img*np.array([0.229, 0.224, 0.225])\n            img = img + np.array([0.485, 0.456, 0.406])\n            img = img*255.0\n            img = img.astype(np.uint8)\n\n            pred_label_img_color = label_img_to_color(pred_label_img)\n            overlayed_img = 0.35*img + 0.65*pred_label_img_color\n            overlayed_img = overlayed_img.astype(np.uint8)\n\n            img_h = overlayed_img.shape[0]\n            img_w = overlayed_img.shape[1]\n\n            # TODO! do this using network.model_dir instead\n            cv2.imwrite(network.model_dir + ""/"" + img_id + "".png"", img)\n            cv2.imwrite(network.model_dir + ""/"" + img_id + ""_pred.png"", pred_label_img_color)\n            cv2.imwrite(network.model_dir + ""/"" + img_id + ""_overlayed.png"", overlayed_img)\n\n            unsorted_img_ids.append(img_id)\n\n################################################################################\n# create visualization video:\n################################################################################\nout = cv2.VideoWriter(""%s/thn_combined.avi"" % network.model_dir, cv2.VideoWriter_fourcc(*""MJPG""), 12, (2*img_w, 2*img_h))\nsorted_img_ids = sorted(unsorted_img_ids)\nfor img_id in sorted_img_ids:\n    img = cv2.imread(network.model_dir + ""/"" + img_id + "".png"", -1)\n    pred_img = cv2.imread(network.model_dir + ""/"" + img_id + ""_pred.png"", -1)\n    overlayed_img = cv2.imread(network.model_dir + ""/"" + img_id + ""_overlayed.png"", -1)\n\n    combined_img = np.zeros((2*img_h, 2*img_w, 3), dtype=np.uint8)\n\n    combined_img[0:img_h, 0:img_w] = img\n    combined_img[0:img_h, img_w:(2*img_w)] = pred_img\n    combined_img[img_h:(2*img_h), (int(img_w/2)):(img_w + int(img_w/2))] = overlayed_img\n\n    out.write(combined_img)\n\nout.release()\n'"
