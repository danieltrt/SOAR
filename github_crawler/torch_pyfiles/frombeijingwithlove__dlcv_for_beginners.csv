file_path,api_count,code
chap10/kaoya_shuizhurou_roc_auc.py,0,"b'import matplotlib.pyplot as plt\nfrom sklearn.metrics import confusion_matrix, roc_curve, auc\n\nresult_filepath = \'val_results.txt\'\n\n# the index of ky & szr are 0 and 2, respectively\nis_ky = []\npred_ky = []\nis_szr = []\npred_szr = []\nky_scores = []\nszr_scores = []\nwith open(result_filepath, \'r\') as f:\n    lines = f.readlines()\n    for line in lines:\n        tokens = line.split()\n        true_label = int(tokens[1])\n        pred_label = int(tokens[2])\n        ky_prob = float(tokens[3])\n        szr_prob = float(tokens[5])\n\n        is_ky.append(1 if true_label == 0 else 0)\n        pred_ky.append(1 if pred_label == 0 else 0)\n        ky_scores.append(ky_prob)\n\n        is_szr.append(1 if true_label == 2 else 0)\n        szr_scores.append(szr_prob)\n\nky_cnf_mat = confusion_matrix(is_ky, pred_ky, labels=[1, 0])\nprint(ky_cnf_mat)\n\nky_fpr, ky_tpr, ky_ths = roc_curve(is_ky, ky_scores)\nky_auc = auc(ky_fpr, ky_tpr)\n\nszr_fpr, szr_tpr, szr_ths = roc_curve(is_szr, szr_scores)\nszr_auc = auc(szr_fpr, szr_tpr)\n\nplt.plot(ky_fpr, ky_tpr, \'k--\', lw=2,\n         label=\'Kao Ya ROC curve (auc = {:.2f})\'.format(ky_auc))\nplt.plot(szr_fpr, szr_tpr, \'b-.\', lw=2,\n         label=\'Shui Zhu Rou ROC curve (auc = {:.2f})\'.format(szr_auc))\nplt.plot([0, 1], [0, 1], \'k\', lw=1)\nplt.plot([0, 0, 1], [0, 1, 1], \'k:\', lw=2)\nplt.xlim([-0.02, 1.0])\nplt.ylim([0.0, 1.02])\nplt.xlabel(\'False Positive Rate\', fontsize=16)\nplt.ylabel(\'True Positive Rate\', fontsize=16)\nplt.title(\'Receiver operating characteristic example\')\nplt.legend(loc=""lower right"")\nplt.show()\n'"
chap10/make_confusion_matrix.py,0,"b'import itertools\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import confusion_matrix\n\ndef plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title=\'Confusion matrix\',\n                          cmap=plt.cm.Blues):\n\n    plt.imshow(cm, interpolation=\'nearest\', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    if normalize:\n        cm = cm.astype(\'float\') / cm.sum(axis=1)[:, np.newaxis]\n        print(""Normalized confusion matrix"")\n    else:\n        print(\'Confusion matrix, without normalization\')\n\n    print(cm)\n\n    thresh = cm.max() / 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, cm[i, j],\n                 horizontalalignment=""center"",\n                 color=""white"" if cm[i, j] > thresh else ""black"")\n\n    plt.tight_layout()\n    plt.ylabel(\'True label\')\n    plt.xlabel(\'Predicted label\')\n\nresult_filepath = \'val_results.txt\'\n\ntrue_labels = []\npred_labels = []\nn_correct = 0\nwith open(result_filepath, \'r\') as f:\n    lines = f.readlines()\n    for line in lines:\n        tokens = line.split()\n        true_label = int(tokens[1])\n        pred_label = int(tokens[2])\n        true_labels.append(true_label)\n        pred_labels.append(pred_label)\n        n_correct += 1 if true_label == pred_label else 0\n\nprint(\'Accuracy = {:.2f}%\'.format(float(n_correct)/float(len(true_labels))*100))\ncnf_mat = confusion_matrix(true_labels, pred_labels)\nfoods = [\'kaoya\', \'yangrouchuan\', \'shuizhurou\', \'jitang\', \'maxiao\', \'miantiao\', \'baozi\']\nplot_confusion_matrix(cnf_mat, classes=foods)\nplt.show()\n'"
chap10/recognize_food.py,0,"b""import sys\nimport numpy as np\nsys.path.append('/path/to/caffe/python')\nimport caffe\n\nWEIGHTS_FILE = 'food_resnet-10_iter_10000.caffemodel'\nDEPLOY_FILE = 'food_resnet_10_cvgj_deploy.prototxt'\n\n#caffe.set_mode_cpu()\nnet = caffe.Net(DEPLOY_FILE, WEIGHTS_FILE, caffe.TEST)\n\ntransformer = caffe.io.Transformer({'data': net.blobs['data'].data.shape})\ntransformer.set_transpose('data', (2, 0, 1))\ntransformer.set_raw_scale('data', 255)\ntransformer.set_channel_swap('data', (2, 1, 0))\n\nimage_list = sys.argv[1]\nresult_list = '{}_results.txt'.format(image_list[:image_list.rfind('.')])\n\nfoods = open('/path/to/keywords.txt', 'rb').read().split()\nwith open(image_list, 'r') as f, open(result_list, 'w') as f_ret:\n    for line in f.readlines():\n        filepath, label = line.split()\n        label = int(label)\n        image = caffe.io.load_image(filepath)\n        transformed_image = transformer.preprocess('data', image)\n        net.blobs['data'].data[...] = transformed_image\n\n        output = net.forward()\n        probs = output['prob'][0]\n        pred = np.argmax(probs)\n\n        print('{}, predicted: {}, true: {}'.format(filepath, foods[pred], foods[label]))\n        result_line = '{} {} {} {}\\n'.format(filepath, label, pred, ' '.join([str(x) for x in probs]))\n        f_ret.write(result_line)\n"""
chap10/sort_kaoya_by_pred_prob.py,0,"b'from operator import itemgetter\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import precision_recall_curve, average_precision_score\n\nresult_filepath = \'val_results.txt\'\n\nky_probs = []\nwith open(result_filepath, \'r\') as f:\n    lines = f.readlines()\n    for line in lines:\n        tokens = line.split()\n        true_label = int(tokens[1])\n        is_ky = 1 if true_label == 0 else 0\n        ky_prob = float(tokens[3])\n        ky_probs.append([is_ky, ky_prob])\n\nky_probs_sorted = np.array(sorted(ky_probs, key=itemgetter(1), reverse=True))\nfor is_ky, ky_prob in ky_probs_sorted:\n    print(\'{:.0f} {:.6f}\'.format(is_ky, ky_prob))\n\nlabels = ky_probs_sorted[:, 0]\nprobs = ky_probs_sorted[:, 1]\n\nprecision, recall, ths = precision_recall_curve(labels, probs)\nap = average_precision_score(labels, probs)\n\nplt.figure(\'Kao Ya Precision-Recall Curve\')\nplt.plot(recall, precision, \'k\', lw=2, label=\'Kao Ya\')\nplt.xlabel(\'Recall\', fontsize=16)\nplt.ylabel(\'Precision\', fontsize=16)\nplt.ylim([0.0, 1.05])\nplt.xlim([0.0, 1.0])\nplt.title(\'Precision-Recall Curve: Average Precision={:.4f}\'.format(ap))\nplt.legend(loc=""lower left"")\nplt.show()\n\n'"
chap10/visualize_activation.py,0,"b""import sys\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport cv2\nsys.path.append('/path/to/caffe/python')\nimport caffe\n\nWEIGHTS_FILE = 'food_resnet-10_iter_10000.caffemodel'\nDEPLOY_FILE = 'food_resnet_10_cvgj_deploy.prototxt'\nFEATURE_MAPS = 'layer_512_1_sum'\nFC_LAYER = 'fc_food'\n\n#caffe.set_mode_cpu()\nnet = caffe.Net(DEPLOY_FILE, WEIGHTS_FILE, caffe.TEST)\n\ntransformer = caffe.io.Transformer({'data': net.blobs['data'].data.shape})\ntransformer.set_transpose('data', (2, 0, 1))\ntransformer.set_raw_scale('data', 255)\ntransformer.set_channel_swap('data', (2, 1, 0))\n\nimage_list = sys.argv[1]\n\ncmap = plt.get_cmap('jet')\nwith open(image_list, 'r') as f:\n    for line in f.readlines():\n        filepath = line.split()[0]\n        image = caffe.io.load_image(filepath)\n        # uncomment the following 2 lines to forward with\n        # original image size and corresponding activation maps\n        #transformer.inputs['data'] = (1, 3, image.shape[0], image.shape[1])\n        #net.blobs['data'].reshape(1, 3, image.shape[0], image.shape[1])\n        transformed_image = transformer.preprocess('data', image)\n        net.blobs['data'].data[...] = transformed_image\n\n        output = net.forward()\n        pred = np.argmax(output['prob'][0])\n\n        feature_maps = net.blobs[FEATURE_MAPS].data[0]\n        fc_params = net.params[FC_LAYER]\n        fc_w = fc_params[0].data[pred]\n        #fc_b = fc_params[1].data[pred]\n\n        activation_map = np.zeros_like(feature_maps[0])\n        for feature_map, w in zip(feature_maps, fc_w):\n            activation_map += feature_map * w\n        #activation_map += fc_b\n\n        # Visualize as\n        # left: original image\n        # middle: activation map\n        # right: original image overlaid with activation map in 'jet' colormap\n        image = np.round(image*255).astype(np.uint8)\n        h, w = image.shape[:2]\n        activation_map = cv2.resize(activation_map, (w, h), interpolation=cv2.INTER_CUBIC)\n        activation_map -= activation_map.min()\n        activation_map /= activation_map.max()\n        activation_color_map = np.round(cmap(activation_map)[:, :, :3]*255).astype(np.uint8)\n        activation_map = np.stack(np.round([activation_map*255]*3).astype(np.uint8))\n        activation_map = activation_map.transpose(1, 2, 0)\n        overlay_img = image/2 + activation_color_map/2\n        vis_img = np.hstack([image, activation_map, overlay_img])\n        vis_img = cv2.cvtColor(vis_img, cv2.COLOR_RGB2BGR)\n\n        cv2.imshow('Activation Map Visualization', vis_img)\n        cv2.waitKey()\n"""
chap12/gen_pairwise_imglist.py,0,"b""import os\nimport random\nimport re\n\ntrain_dir = 'mnist/train'\nval_dir = 'mnist/val'\nn_train = 100000\nn_val = 10000\n\npattern = re.compile('\\d+_(\\d)\\.jpg')\n\nfor img_dir, n_pairs in zip([train_dir, val_dir], [n_train, n_val]):\n    imglist = os.listdir(img_dir)\n    n_samples = len(imglist)\n    dataset = img_dir[img_dir.rfind(os.sep)+1:]\n    with open('{}.txt'.format(dataset), 'w') as f, \\\n            open('{}_p.txt'.format(dataset), 'w') as f_p:\n        for i in range(n_pairs):\n            filename = imglist[random.randint(0, n_samples-1)]\n            digit = pattern.findall(filename)[0]\n            filepath = os.sep.join([img_dir, filename])\n\n            filename_p = imglist[random.randint(0, n_samples-1)]\n            digit_p = pattern.findall(filename_p)[0]\n            filepath_p = os.sep.join([img_dir, filename_p])\n\n            label = 1 if digit == digit_p else 0\n\n            f.write('{} {}\\n'.format(filepath, label))\n            f_p.write('{} {}\\n'.format(filepath_p, label))\n"""
chap12/visualize_result.py,0,"b""import os\nimport sys\nsys.path.append('/path/to/caffe/python')\nimport re\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.manifold import TSNE\nimport cv2\nimport caffe\n\nWEIGHTS_FILE = 'mnist_siamese_iter_20000.caffemodel'\nDEPLOY_FILE = 'mnist_siamese.prototxt'\nIMG_DIR = 'mnist/test'\nMEAN = 128\nSCALE = 0.00390625\n\ncaffe.set_mode_gpu()\ncaffe.set_device(0)\nnet = caffe.Net(DEPLOY_FILE, WEIGHTS_FILE, caffe.TEST)\n\npattern = re.compile('\\d+_(\\d)\\.jpg')\n\nimage_list = os.listdir(IMG_DIR)\nn_imgs = len(image_list)\n\nnet.blobs['data'].reshape(n_imgs, 1, 28, 28)\n\nlabels = []\nfor i, filename in enumerate(image_list):\n    digit = int(pattern.findall(filename)[0])\n    labels.append(digit)\n    filepath = os.sep.join([IMG_DIR, filename])\n    image = cv2.imread(filepath, cv2.IMREAD_GRAYSCALE).astype(np.float) - MEAN\n    image *= SCALE\n    net.blobs['data'].data[i, ...] = image\n\nlabels = np.array(labels)\n\noutput = net.forward()\nfeat = output['feat']\n\ncolors = ['#ff0000', '#ffff00', '#00ff00', '#00ffff', '#0000ff',\n          '#ff00ff', '#990000', '#999900', '#009900', '#009999']\nlegend = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n\nplt.figure('feat')\nfor i in range(10):\n    plt.plot(feat[labels==i,0].flatten(),\n             feat[labels==i,1].flatten(),\n             '.', c=colors[i])\nplt.legend(legend)\n\nplt.figure('ip2')\nip2_feat = net.blobs['ip2'].data\nmodel = TSNE(n_components=2)\n\nip2_vis_feat = model.fit_transform(ip2_feat)\nfor i in range(10):\n    plt.plot(ip2_vis_feat[labels==i,0].flatten(),\n             ip2_vis_feat[labels==i,1].flatten(),\n             '.', c=colors[i])\nplt.legend(legend)\n\nplt.show()\n"""
chap5/bar_n_pie_chart.py,0,"b""import numpy as np\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\n\nmpl.rcParams['axes.titlesize'] = 20\nmpl.rcParams['xtick.labelsize'] = 16\nmpl.rcParams['ytick.labelsize'] = 16\nmpl.rcParams['axes.labelsize'] = 16\nmpl.rcParams['xtick.major.size'] = 0\nmpl.rcParams['ytick.major.size'] = 0\n\nspeed_map = {\n    'dog': (48, '#7199cf'),\n    'cat': (45, '#4fc4aa'),\n    'cheetah': (120, '#e1a7a2')\n}\n\nfig = plt.figure('Bar chart & Pie chart')\n\nax = fig.add_subplot(121)\nax.set_title('Running speed - bar chart')\n\nxticks = np.arange(3)\n\nbar_width = 0.5\n\nanimals = speed_map.keys()\nspeeds = [x[0] for x in speed_map.values()]\ncolors = [x[1] for x in speed_map.values()]\nbars = ax.bar(xticks, speeds, width=bar_width, edgecolor='none')\n\nax.set_ylabel('Speed(km/h)')\nax.set_xticks(xticks+bar_width/2)\nax.set_xticklabels(animals)\nax.set_xlim([bar_width/2-0.5, 3-bar_width/2])\nax.set_ylim([0, 125])\n\nfor bar, color in zip(bars, colors):\n    bar.set_color(color)\n\nax = fig.add_subplot(122)\nax.set_title('Running speed - pie chart')\n\nlabels = ['{}\\n{} km/h'.format(a, s) for a, s in zip(animals, speeds)]\n\nax.pie(speeds, labels=labels, colors=colors)\n\nplt.axis('equal')\nplt.show()\n"""
chap5/fit_data.py,0,"b""import numpy as np\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\n\nmpl.rcParams['xtick.labelsize'] = 24\nmpl.rcParams['ytick.labelsize'] = 24\n\nnp.random.seed(42)\n\nx = np.linspace(0, 5, 100)\ny = 2*np.sin(x) + 0.3*x**2\ny_data = y + np.random.normal(scale=0.3, size=100)\n\nplt.figure('data')\nplt.plot(x, y_data, '.')\n\nplt.figure('model')\nplt.plot(x, y)\n\nplt.figure('data & model')\nplt.plot(x, y, 'k', lw=3)\nplt.scatter(x, y_data)\n\nplt.show()\n"""
chap5/scatter_3d.py,0,"b""import matplotlib.pyplot as plt\nimport numpy as np\n\nfrom mpl_toolkits.mplot3d import Axes3D\n\nnp.random.seed(42)\n\nn_samples = 500\ndim = 3\n\nsamples = np.random.multivariate_normal(\n    np.zeros(dim),\n    np.eye(dim),\n    n_samples\n)\n\nfor i in range(samples.shape[0]):\n    r = np.power(np.random.random(), 1.0/3.0)\n    samples[i] *= r / np.linalg.norm(samples[i])\n\nupper_samples = []\nlower_samples = []\nfor x, y, z in samples:\n    if z > 3*x + 2*y - 1:\n        upper_samples.append((x, y, z))\n    else:\n        lower_samples.append((x, y, z))\n\nfig = plt.figure('3D scatter plot')\nax = fig.add_subplot(111, projection='3d')\n\nuppers = np.array(upper_samples)\nlowers = np.array(lower_samples)\n\nax.scatter(uppers[:, 0], uppers[:, 1], uppers[:, 2], c='r', marker='o')\nax.scatter(lowers[:, 0], lowers[:, 1], lowers[:, 2], c='g', marker='^')\n\nplt.show()\n"""
chap5/surface_3d.py,0,"b""import matplotlib.pyplot as plt\nimport numpy as np\n\nfrom mpl_toolkits.mplot3d import Axes3D     \n\nnp.random.seed(42)\n\nn_grids = 51            \nc = n_grids / 2         \nnf = 2                  \n\nx = np.linspace(0, 1, n_grids)\ny = np.linspace(0, 1, n_grids)\nX, Y = np.meshgrid(x, y)\n\nspectrum = np.zeros((n_grids, n_grids), dtype=np.complex)\nnoise = [np.complex(x, y) for x, y in np.random.uniform(-1,1,((2*nf+1)**2/2, 2))]\nnoisy_block = np.concatenate((noise, [0j], np.conjugate(noise[::-1])))\n\nspectrum[c-nf:c+nf+1, c-nf:c+nf+1] = noisy_block.reshape((2*nf+1, 2*nf+1))\nZ = np.real(np.fft.ifft2(np.fft.ifftshift(spectrum)))\n\nfig = plt.figure('3D surface & wire')\n\nax = fig.add_subplot(1, 2, 1, projection='3d')\nax.plot_surface(X, Y, Z, alpha=0.7, cmap='jet', rstride=1, cstride=1, lw=0)\n\nax = fig.add_subplot(1, 2, 2, projection='3d')\nax.plot_wireframe(X, Y, Z, rstride=3, cstride=3, lw=0.5)\n\nplt.show()\n"""
chap5/three_doors.py,0,"b""import numpy.random as random\n\nrandom.seed(42)\n\nn_tests = 10000\n\nwinning_doors = random.randint(0, 3, n_tests)\nchange_mind_wins = 0\ninsist_wins = 0\n\nfor winning_door in winning_doors:\n\n    first_try = random.randint(0, 3)\n    remaining_choices = [i for i in range(3) if i != first_try]\n    wrong_choices = [i for i in range(3) if i != winning_door]\n\n    if first_try in wrong_choices:\n        wrong_choices.remove(first_try)\n    \n    screened_out = random.choice(wrong_choices)\n    remaining_choices.remove(screened_out)\n    \n    changed_mind_try = remaining_choices[0]\n\n    change_mind_wins += 1 if changed_mind_try == winning_door else 0\n    insist_wins += 1 if first_try == winning_door else 0\n\nprint(\n    'You win {1} out of {0} tests if you changed your mind\\n'\n    'You win {2} out of {0} tests if you insist on the initial choice'.format(\n        n_tests, change_mind_wins, insist_wins\n        )\n)\n"""
chap7/gen_data.py,0,"b""import pickle\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\ndef cos_curve(x):\n    return 0.25*np.sin(2*x*np.pi+0.5*np.pi) + 0.5\n\nnp.random.seed(123)\nsamples = []\nlabels = []\n\nsample_density = 50\nfor i in range(sample_density):\n    x1, x2 = np.random.random(2)\n    bound = cos_curve(x1)\n    if bound - 0.1 < x2 <= bound + 0.1:\n        continue\n    else:\n        samples.append((x1, x2))\n        if x2 > bound:\n            labels.append(1)\n        else:\n            labels.append(0)\n\nwith open('data.pkl', 'wb') as f:\n    pickle.dump((samples, labels), f)\n\nfor i, sample in enumerate(samples):\n    plt.plot(sample[0], sample[1],\n             'o' if labels[i] else '^',\n             mec='r' if labels[i] else 'b',\n             mfc='none',\n             markersize=10)\n\nx1 = np.linspace(0, 1)\nplt.plot(x1, cos_curve(x1), 'k--')\nplt.show()\n"""
chap9/gen_hdf5.py,0,"b""import sys\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport h5py\n\nIMAGE_SIZE = (100, 100)\nMEAN_VALUE = 128\n\nfilename = sys.argv[1]\nsetname, ext = filename.split('.')\n\nwith open(filename, 'r') as f:\n    lines = f.readlines()\n\nnp.random.shuffle(lines)\n\nsample_size = len(lines)\nimgs = np.zeros((sample_size, 1,) + IMAGE_SIZE, dtype=np.float32)\nfreqs = np.zeros((sample_size, 2), dtype=np.float32)\n\nh5_filename = '{}.h5'.format(setname)\nwith h5py.File(h5_filename, 'w') as h:\n    for i, line in enumerate(lines):\n        image_name, fx, fy = line[:-1].split()\n        img = plt.imread(image_name)[:, :, 0].astype(np.float32)\n        img = img.reshape((1, )+img.shape)\n        img -= MEAN_VALUE\n        imgs[i] = img\n        freqs[i] = [float(fx), float(fy)]\n        if (i+1) % 1000 == 0:\n            print('Processed {} images!'.format(i+1))\n    h.create_dataset('data', data=imgs)\n    h.create_dataset('freq', data=freqs)\n\nwith open('{}_h5.txt'.format(setname), 'w') as f:\n    f.write(h5_filename)\n"""
chap9/gen_label.py,0,"b""import os\n\nfilename2score = lambda x: x[:x.rfind('.')].split('_')[-2:]\n\nfilenames = os.listdir('samples')\n\nwith open('train.txt', 'w') as f_train_txt:\n    for filename in filenames[:50000]:\n        fx, fy = filename2score(filename)\n        line = 'samples/{} {} {}\\n'.format(filename, fx, fy)\n        f_train_txt.write(line)\n\nwith open('val.txt', 'w') as f_val_txt:\n    for filename in filenames[50000:60000]:\n        fx, fy = filename2score(filename)\n        line = 'samples/{} {} {}\\n'.format(filename, fx, fy)\n        f_val_txt.write(line)\n\nwith open('test.txt', 'w') as f_test_txt:\n    for filename in filenames[60000:]:\n        line = 'samples/{}\\n'.format(filename)\n        f_test_txt.write(line)\n"""
chap9/make_noises.py,0,"b""import os\nimport sys\nimport datetime\nimport cv2\n\nfrom multiprocessing import Process, cpu_count\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nH_IMG, W_IMG = 100, 100\nSAMPLE_SIZE = 70000\nSAMPLES_DIR = 'samples'\n\ndef make_noise(index):\n    h = np.random.randint(1, H_IMG)\n    w = np.random.randint(1, W_IMG)\n    noise = np.random.random((h, w))\n    noisy_img = cv2.resize(noise, (H_IMG, W_IMG), interpolation=cv2.INTER_CUBIC)\n    fx = float(w) / float(W_IMG)\n    fy = float(h) / float(H_IMG)\n    filename = '{}/{:0>5d}_{}_{}.jpg'.format(SAMPLES_DIR, index, fx, fy)\n    plt.imsave(filename, noisy_img, cmap='gray')\n\ndef make_noises(i0, i1):\n    np.random.seed(datetime.datetime.now().microsecond)\n    for i in xrange(i0, i1):\n        make_noise(i)\n    print('Noises from {} to {} are made!'.format(i0+1, i1))\n    sys.stdout.flush()\n\ndef main():\n    cmd = 'mkdir -p {}'.format(SAMPLES_DIR)\n    os.system(cmd)\n    n_procs = cpu_count()\n\n    print('Making noises with {} processes ...'.format(n_procs))\n    length = float(SAMPLE_SIZE)/float(n_procs)\n    indices = [int(round(i * length)) for i in range(n_procs + 1)]\n    processes = [Process(target=make_noises, args=(indices[i], indices[i+1])) for i in range(n_procs)]\n\n    for p in processes:\n        p.start()\n    \n    for p in processes:\n        p.join()\n\n    print('Done!')\n\nif __name__ == '__main__':\n    main()\n"""
chap9/predict.py,0,"b""import sys\nimport numpy as np\nsys.path.append('/path/to/caffe/python')\nimport caffe\n\nWEIGHTS_FILE = 'freq_regression_iter_10000.caffemodel'\nDEPLOY_FILE = 'deploy.prototxt'\nMEAN_VALUE = 128\n\n#caffe.set_mode_cpu()\nnet = caffe.Net(DEPLOY_FILE, WEIGHTS_FILE, caffe.TEST)\n\ntransformer = caffe.io.Transformer({'data': net.blobs['data'].data.shape})\ntransformer.set_transpose('data', (2,0,1))\ntransformer.set_mean('data', np.array([MEAN_VALUE]))\ntransformer.set_raw_scale('data', 255)\n\nimage_list = sys.argv[1]\n\nbatch_size = net.blobs['data'].data.shape[0]\nwith open(image_list, 'r') as f:\n    i = 0\n    filenames = []\n    for line in f.readlines():\n        filename = line[:-1]\n        filenames.append(filename)\n        image = caffe.io.load_image(filename, False)\n        transformed_image = transformer.preprocess('data', image)\n        net.blobs['data'].data[i, ...] = transformed_image\n        i += 1\n\n        if i == batch_size:\n            output = net.forward()\n            freqs = output['pred']\n\n            for filename, (fx, fy) in zip(filenames, freqs):\n                print('Predicted frequencies for {} is {:.2f} and {:.2f}'.format(filename, fx, fy))\n\n            i = 0\n            filenames = []\n"""
chap9/visualize_conv1_kernels.py,0,"b""import sys\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport cv2\nsys.path.append('/path/to/caffe/python')\nimport caffe\n\nZOOM_IN_SIZE = 50\nPAD_SIZE = 4\n\nWEIGHTS_FILE = 'freq_regression_iter_10000.caffemodel'\nDEPLOY_FILE = 'deploy.prototxt'\n\nnet = caffe.Net(DEPLOY_FILE, WEIGHTS_FILE, caffe.TEST)\nkernels = net.params['conv1'][0].data\n\nkernels -= kernels.min()\nkernels /= kernels.max()\n\nzoomed_in_kernels = []\nfor kernel in kernels:\n    zoomed_in_kernels.append(cv2.resize(kernel[0], (ZOOM_IN_SIZE, ZOOM_IN_SIZE), interpolation=cv2.INTER_NEAREST))\n\n# plot 12*8 squares kernels\nhalf_pad = PAD_SIZE / 2\npadded_size = ZOOM_IN_SIZE+PAD_SIZE\npadding = ((0, 0), (half_pad, half_pad), (half_pad, half_pad))\n\npadded_kernels = np.pad(zoomed_in_kernels, padding, 'constant', constant_values=1)\npadded_kernels = padded_kernels.reshape(8, 12, padded_size, padded_size).transpose(0, 2, 1, 3)\nkernels_img = padded_kernels.reshape((8*padded_size, 12*padded_size))[half_pad:-half_pad, half_pad: -half_pad]\n\nplt.imshow(kernels_img, cmap='gray', interpolation='nearest')\nplt.axis('off')\n\nplt.show()\n"""
chap10/data/collect_data.py,0,"b'import os\nimport re\nimport urllib\nfrom multiprocessing import Process\n\nSUPPORTED_FORMATS = [\'jpg\', \'png\', \'jpeg\']\nURL_TEMPLATE = r\'http://image.b***u.com/search/flip?tn=b***uimage&ie=utf-8&word={keyword}&pn={index}\'\n\ndef download_images_from_b***u(dir_name, keyword, start_index, end_index):\n    index = start_index\n    while index < end_index:\n        url = URL_TEMPLATE.format(keyword=keyword, index=index)\n        try:\n            html_text = urllib.urlopen(url).read().decode(\'utf-8\', \'ignore\')\n            image_urls = re.findall(r\'""objURL"":""(.*?)""\', html_text)\n            if not image_urls:\n                print(\'Cannot retrieve anymore image urls \\nStopping ...\'.format(url))\n                break\n        except IOError as e:\n            print(e)\n            print(\'Cannot open {}. \\nStopping ...\'.format(url))\n            break\n\n        downloaded_urls = []\n        for url in image_urls:\n            filename = url.split(\'/\')[-1]\n            ext = filename[filename.rfind(\'.\')+1:]\n            if ext.lower() not in SUPPORTED_FORMATS:\n                index += 1\n                continue\n            filename = \'{}/{:0>6d}.{}\'.format(dir_name, index, ext)\n            cmd = \'wget ""{}"" -t 3 -T 5 -O {}\'.format(url, filename)\n            os.system(cmd)\n            \n            if os.path.exists(filename) and os.path.getsize(filename) > 1024:\n                index_url = \'{:0>6d},{}\'.format(index, url)\n                downloaded_urls.append(index_url)\n            else:\n                os.system(\'rm {}\'.format(filename))\n\n            index += 1\n            if index >= end_index:\n                break\n\n        with open(\'{}_urls.txt\'.format(dir_name), \'a\') as furls:\n            urls_text = \'{}\\n\'.format(\'\\n\'.join(downloaded_urls))\n            if len(urls_text) > 11:\n                furls.write(urls_text)\n\ndef download_images(keywords, num_per_kw, procs_per_kw):\n    args_list = []\n    for class_id, keyword in enumerate(keywords):\n        dir_name = \'{:0>3d}\'.format(class_id)\n        os.system(\'mkdir -p {}\'.format(dir_name))\n        num_per_proc = int(round(float(num_per_kw/procs_per_kw)))\n        for i in range(procs_per_kw):\n            start_index = i * num_per_proc\n            end_index = start_index + num_per_proc - 1\n            args_list.append((dir_name, keyword, start_index, end_index))\n\n    processes = [Process(target=download_images_from_b***u, args=x) for x in args_list]\n\n    print(\'Starting to download images with {} processes ...\'.format(len(processes)))\n\n    for p in processes:\n        p.start()\n\n    for p in processes:\n        p.join()\n\n    print(\'Done!\')\n\nif __name__ == ""__main__"":\n    with open(\'keywords.txt\', \'rb\') as f:\n        foods = f.read().split()\n    download_images(foods, 2000, 3)\n'"
chap10/data/downscale.py,0,"b""import os\r\nimport cv2\r\nimport sys\r\n\r\ninput_path = sys.argv[1].rstrip(os.sep)\r\ntarget_short_edge = int(sys.argv[2])\r\n\r\nfor root, dirs, files in os.walk(input_path):\r\n    print('scanning {} ...'.format(root))\r\n    for filename in files:\r\n        filepath = os.sep.join([root, filename])\r\n\r\n        img = cv2.imread(filepath)\r\n        h, w = img.shape[:2]\r\n        short_edge = min(w, h)\r\n\r\n        if short_edge > target_short_edge:\r\n            scale = float(target_short_edge) / float(short_edge)\r\n            new_w = int(round(w*scale))\r\n            new_h = int(round(h*scale))\r\n            print('Down sampling {} from {}x{} to {}x{} ...'.format(\r\n                filepath, w, h, new_w, new_h\r\n            ))\r\n            img = cv2.resize(img, (new_w, new_h))\r\n            cv2.imwrite(filepath, img)\r\n\r\nprint('Done!')\r\n"""
chap10/data/food_augmentation.py,0,"b""import os\n\nn_total = 3000\n\nclass_dirs = os.listdir('train')\n\nfor class_dir in class_dirs:\n    src_path = 'train/{}'.format(class_dir)\n    n_samples = len(os.listdir(src_path))\n    n_aug = n_total - n_samples\n    cmd = 'python run_augmentation.py {} temp {}'.format(src_path, n_aug)\n    os.system(cmd)\n    cmd = 'mv temp/* {}'.format(src_path)\n    os.system(cmd)\n\nos.system('rm -r temp')\n"""
chap10/data/gen_label_list.py,0,"b""import os\nimport sys\n\ndataset = sys.argv[1].rstrip(os.sep)\n\nclass_dirs = os.listdir(dataset)\n\nwith open('{}.txt'.format(dataset), 'w') as f:\n    for class_dir in class_dirs:\n        class_path = os.sep.join([dataset, class_dir])\n        label = int(class_dir)\n        lines = ['{}/{} {}'.format(class_path, x, label) for x in os.listdir(class_path)]\n        f.write('\\n'.join(lines) + '\\n')\n"""
chap10/data/remove_dups_from_list.py,0,"b""import os\nimport sys\n\ndup_list = sys.argv[1]\n\nwith open(dup_list, 'r') as f:\n    lines = f.readlines()\n    for line in lines:\n        dups = line.split()\n        print('Removing duplicates of {}'.format(dups[0]))\n        for dup in dups[1:]:\n            cmd = 'rm {}'.format(dup)\n            os.system(cmd)\n"""
chap10/data/remove_invalid_images.py,0,"b""import os\nimport sys\nimport cv2\nfrom collect_data import SUPPORTED_FORMATS\n\ninput_path = sys.argv[1]\n\nfor root, dirs, files in os.walk(input_path):\n    for filename in files:\n        ext = filename[filename.rfind('.')+1:].lower()\n        if ext not in SUPPORTED_FORMATS:\n            continue\n        filepath = os.sep.join([root, filename])\n        if cv2.imread(filepath) is None:\n            os.system('rm {}'.format(filepath))\n            print('{} is not a valid image file. Deleted!'.format(filepath))\n"""
chap10/data/sample_val.py,0,"b""import os\nimport random\n\nN = 300\n\nos.system('mkdir -p val')\nclass_dirs = os.listdir('train')\n\nfor class_dir in class_dirs:\n    os.system('mkdir -p val/{}'.format(class_dir))\n    root = 'train/{}'.format(class_dir)\n    print('Sampling validation set with {} images from {} ...'.format(N, root))\n    filenames = os.listdir(root)\n    random.shuffle(filenames)\n    val_filenames = filenames[:N]\n    for filename in val_filenames:\n        src_filepath = os.sep.join([root, filename])\n        dst_filepath = os.sep.join(['val', class_dir, filename])\n        cmd = 'mv {} {}'.format(src_filepath, dst_filepath)\n        os.system(cmd)\n"""
chap6/bbox_labeling/bbox_labeling.py,0,"b""import os\nimport cv2\nfrom tkFileDialog import askdirectory\nfrom tkMessageBox import askyesno\n\nWINDOW_NAME = 'Simple Bounding Box Labeling Tool'\nFPS = 24\nSUPPOTED_FORMATS = ['jpg', 'jpeg', 'png']\nDEFAULT_COLOR = {'Object': (255, 0, 0)}\nCOLOR_GRAY = (192, 192, 192)\nBAR_HEIGHT = 16\n\nKEY_UP = 65362\nKEY_DOWN = 65364\nKEY_LEFT = 65361\nKEY_RIGHT = 65363\nKEY_ESC = 27\nKEY_DELETE = 65535\nKEY_EMPTY = 0\n\nget_bbox_name = '{}.bbox'.format\n\n\nclass SimpleBBoxLabeling:\n\n    def __init__(self, data_dir, fps=FPS, window_name=None):\n        self._data_dir = data_dir\n        self.fps = fps\n        self.window_name = window_name if window_name else WINDOW_NAME\n\n        self._pt0 = None\n        self._pt1 = None\n        self._drawing = False\n        self._cur_label = None\n        self._bboxes = []\n\n        label_path = '{}.labels'.format(self._data_dir)\n        self.label_colors = DEFAULT_COLOR if not os.path.exists(label_path) else self.load_labels(label_path)\n\n        imagefiles = [x for x in os.listdir(self._data_dir) if x[x.rfind('.') + 1:].lower() in SUPPOTED_FORMATS]\n        labeled = [x for x in imagefiles if os.path.exists(get_bbox_name(x))]\n        to_be_labeled = [x for x in imagefiles if x not in labeled]\n\n        self._filelist = labeled + to_be_labeled\n        self._index = len(labeled)\n        if self._index > len(self._filelist) - 1:\n            self._index = len(self._filelist) - 1\n\n    def _mouse_ops(self, event, x, y, flags, param):\n\n        if event == cv2.EVENT_LBUTTONDOWN:\n            self._drawing = True\n            self._pt0 = (x, y)\n\n        elif event == cv2.EVENT_LBUTTONUP:\n            self._drawing = False\n            self._pt1 = (x, y)\n            self._bboxes.append((self._cur_label, (self._pt0, self._pt1)))\n\n        elif event == cv2.EVENT_MOUSEMOVE:\n            self._pt1 = (x, y)\n\n        elif event == cv2.EVENT_RBUTTONUP:\n            if self._bboxes:\n                self._bboxes.pop()\n\n    def _clean_bbox(self):\n        self._pt0 = None\n        self._pt1 = None\n        self._drawing = False\n        self._bboxes = []\n\n    def _draw_bbox(self, img):\n\n        h, w = img.shape[:2]\n        canvas = cv2.copyMakeBorder(img, 0, BAR_HEIGHT, 0, 0, cv2.BORDER_CONSTANT, value=COLOR_GRAY)\n\n        label_msg = '{}: {}, {}'.format(self._cur_label, self._pt0, self._pt1) \\\n            if self._drawing \\\n            else 'Current label: {}'.format(self._cur_label)\n        msg = '{}/{}: {} | {}'.format(self._index + 1, len(self._filelist), self._filelist[self._index], label_msg)\n\n        cv2.putText(canvas, msg, (1, h+12),\n                    cv2.FONT_HERSHEY_SIMPLEX,\n                    0.5, (0, 0, 0), 1)\n        for label, (bpt0, bpt1) in self._bboxes:\n            label_color = self.label_colors[label] if label in self.label_colors else COLOR_GRAY\n            cv2.rectangle(canvas, bpt0, bpt1, label_color, thickness=2)\n            cv2.putText(canvas, label, (bpt0[0]+3, bpt0[1]+15),\n                        cv2.FONT_HERSHEY_SIMPLEX,\n                        0.5, label_color, 2)\n        if self._drawing:\n            label_color = self.label_colors[self._cur_label] if self._cur_label in self.label_colors else COLOR_GRAY\n            if self._pt1[0] >= self._pt0[0] and self._pt1[1] >= self._pt0[1]:\n                cv2.rectangle(canvas, self._pt0, self._pt1, label_color, thickness=2)\n            cv2.putText(canvas, self._cur_label, (self._pt0[0] + 3, self._pt0[1] + 15),\n                        cv2.FONT_HERSHEY_SIMPLEX,\n                        0.5, label_color, 2)\n        return canvas\n\n    @staticmethod\n    def export_bbox(filepath, bboxes):\n        if bboxes:\n            with open(filepath, 'w') as f:\n                for bbox in bboxes:\n                    line = repr(bbox) + '\\n'\n                    f.write(line)\n        elif os.path.exists(filepath):\n            os.remove(filepath)\n\n    @staticmethod\n    def load_bbox(filepath):\n        bboxes = []\n        with open(filepath, 'r') as f:\n            line = f.readline().rstrip()\n            while line:\n                bboxes.append(eval(line))\n                line = f.readline().rstrip()\n        return bboxes\n\n    @staticmethod\n    def load_labels(filepath):\n        label_colors = {}\n        with open(filepath, 'r') as f:\n            line = f.readline().rstrip()\n            while line:\n                label, color = eval(line)\n                label_colors[label] = color\n                line = f.readline().rstrip()\n        return label_colors\n\n    @staticmethod\n    def load_sample(filepath):\n        img = cv2.imread(filepath)\n        bbox_filepath = get_bbox_name(filepath)\n        bboxes = []\n        if os.path.exists(bbox_filepath):\n            bboxes = SimpleBBoxLabeling.load_bbox(bbox_filepath)\n        return img, bboxes\n\n    def _export_n_clean_bbox(self):\n        bbox_filepath = os.sep.join([self._data_dir, get_bbox_name(self._filelist[self._index])])\n        self.export_bbox(bbox_filepath, self._bboxes)\n        self._clean_bbox()\n\n    def _delete_current_sample(self):\n        filename = self._filelist[self._index]\n        filepath = os.sep.join([self._data_dir, filename])\n        if os.path.exists(filepath):\n            os.remove(filepath)\n        filepath = get_bbox_name(filepath)\n        if os.path.exists(filepath):\n            os.remove(filepath)\n        self._filelist.pop(self._index)\n        print('{} is deleted!'.format(filename))\n\n    def start(self):\n\n        last_filename = ''\n        label_index = 0\n        labels = self.label_colors.keys()\n        n_labels = len(labels)\n\n        cv2.namedWindow(self.window_name)\n        cv2.setMouseCallback(self.window_name, self._mouse_ops)\n        key = KEY_EMPTY\n        delay = int(1000 / FPS)\n\n        while key != KEY_ESC:\n\n            if key == KEY_UP:\n                if label_index == 0:\n                    pass\n                else:\n                    label_index -= 1\n\n            elif key == KEY_DOWN:\n                if label_index == n_labels - 1:\n                    pass\n                else:\n                    label_index += 1\n\n            elif key == KEY_LEFT:\n                if self._index > 0:\n                    self._export_n_clean_bbox()\n\n                self._index -= 1\n                if self._index < 0:\n                    self._index = 0\n\n            elif key == KEY_RIGHT:\n                if self._index < len(self._filelist) - 1:\n                    self._export_n_clean_bbox()\n\n                self._index += 1\n                if self._index > len(self._filelist) - 1:\n                    self._index = len(self._filelist) - 1\n\n            elif key == KEY_DELETE:\n                if askyesno('Delete Sample', 'Are you sure?'):\n                    self._delete_current_sample()\n                    key = KEY_EMPTY\n                    continue\n\n            filename = self._filelist[self._index]\n            if filename != last_filename:\n                filepath = os.sep.join([self._data_dir, filename])\n                img, self._bboxes = self.load_sample(filepath)\n\n            self._cur_label = labels[label_index]\n\n            canvas = self._draw_bbox(img)\n            cv2.imshow(self.window_name, canvas)\n            key = cv2.waitKey(delay)\n\n            last_filename = filename\n\n        print('Finished!')\n\n        cv2.destroyAllWindows()\n        self.export_bbox(os.sep.join([self._data_dir, get_bbox_name(filename)]), self._bboxes)\n\n        print('Labels updated!')\n\nif __name__ == '__main__':\n    dir_with_images = askdirectory(title='Where are the images?')\n    labeling_task = SimpleBBoxLabeling(dir_with_images)\n    labeling_task.start()\n"""
chap6/bbox_labeling/detection_anno_bbox2voc.py,0,"b""import os\nimport sys\nimport xml.etree.ElementTree as ET\n#import xml.dom.minidom as minidom\nimport cv2\nfrom bbox_labeling import SimpleBBoxLabeling\n\ninput_dir = sys.argv[1].rstrip(os.sep)\n\nbbox_filenames = [x for x in os.listdir(input_dir) if x.endswith('.bbox')]\n\nfor bbox_filename in bbox_filenames:\n    bbox_filepath = os.sep.join([input_dir, bbox_filename])\n    jpg_filepath = bbox_filepath[:-5]\n    if not os.path.exists(jpg_filepath):\n        print('Something is wrong with {}!'.format(bbox_filepath))\n        break\n\n    root = ET.Element('annotation')\n\n    filename = ET.SubElement(root, 'filename')\n    jpg_filename = jpg_filepath.split(os.sep)[-1]\n    filename.text = jpg_filename\n\n    img = cv2.imread(jpg_filepath)\n    h, w, c = img.shape\n    size = ET.SubElement(root, 'size')\n    width = ET.SubElement(size, 'width')\n    width.text = str(w)\n    height = ET.SubElement(size, 'height')\n    height.text = str(h)\n    depth = ET.SubElement(size, 'depth')\n    depth.text = str(c)\n\n    bboxes = SimpleBBoxLabeling.load_bbox(bbox_filepath)\n    for obj_name, coord in bboxes:\n        obj = ET.SubElement(root, 'object')\n        name = ET.SubElement(obj, 'name')\n        name.text = obj_name\n        bndbox = ET.SubElement(obj, 'bndbox')\n        xmin = ET.SubElement(bndbox, 'xmin')\n        xmax = ET.SubElement(bndbox, 'xmax')\n        ymin = ET.SubElement(bndbox, 'ymin')\n        ymax = ET.SubElement(bndbox, 'ymax')\n        (left, top), (right, bottom) = coord\n        xmin.text = str(left)\n        xmax.text = str(right)\n        ymin.text = str(top)\n        ymax.text = str(bottom)\n\n    xml_filepath = jpg_filepath[:jpg_filepath.rfind('.')] + '.xml'\n    with open(xml_filepath, 'w') as f:\n        anno_xmlstr = ET.tostring(root)\n\n        # In case a nicely formatted xml is needed\n        # uncomment the following 2 lines and minidom import\n        #anno_xml = minidom.parseString(anno_xmlstr)\n        #anno_xmlstr = anno_xml.toprettyxml()\n        f.write(anno_xmlstr)\n"""
chap6/data_augmentation/image_augmentation.py,0,"b'import numpy as np\r\nimport cv2\r\n\r\ncrop_image = lambda img, x0, y0, w, h: img[y0:y0+h, x0:x0+w]\r\n\r\ndef random_crop(img, area_ratio, hw_vari):\r\n    h, w = img.shape[:2]\r\n    hw_delta = np.random.uniform(-hw_vari, hw_vari)\r\n    hw_mult = 1 + hw_delta\r\n    w_crop = int(round(w*np.sqrt(area_ratio*hw_mult)))\r\n    if w_crop > w - 2:\r\n        w_crop = w - 2\r\n    h_crop = int(round(h*np.sqrt(area_ratio/hw_mult)))\r\n    if h_crop > h - 2:\r\n        h_crop = h - 2\r\n    x0 = np.random.randint(0, w-w_crop-1)\r\n    y0 = np.random.randint(0, h-h_crop-1)\r\n    return crop_image(img, x0, y0, w_crop, h_crop)\r\n\r\ndef rotate_image(img, angle, crop):\r\n    h, w = img.shape[:2]\r\n    angle %= 360\r\n    M_rotate = cv2.getRotationMatrix2D((w/2, h/2), angle, 1)\r\n    img_rotated = cv2.warpAffine(img, M_rotate, (w, h))\r\n\r\n    if crop:\r\n        angle_crop = angle % 180\r\n        if angle_crop > 90:\r\n            angle_crop = 180 - angle_crop\r\n        theta = angle_crop * np.pi / 180.0\r\n        hw_ratio = float(h) / float(w)\r\n        tan_theta = np.tan(theta)\r\n        numerator = np.cos(theta) + np.sin(theta) * tan_theta\r\n        r = hw_ratio if h > w else 1 / hw_ratio\r\n        denominator = r * tan_theta + 1\r\n        crop_mult = numerator / denominator\r\n        w_crop = int(round(crop_mult*w))\r\n        h_crop = int(round(crop_mult*h))\r\n        x0 = int((w-w_crop)/2)\r\n        y0 = int((h-h_crop)/2)\r\n\r\n        img_rotated = crop_image(img_rotated, x0, y0, w_crop, h_crop)\r\n\r\n    return img_rotated\r\n\r\ndef random_rotate(img, angle_vari, p_crop):\r\n    angle = np.random.uniform(-angle_vari, angle_vari)\r\n    crop = False if np.random.random() > p_crop else True\r\n    return rotate_image(img, angle, crop)\r\n\r\ndef hsv_transform(img, hue_delta, sat_mult, val_mult):\r\n    img_hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV).astype(np.float)\r\n    img_hsv[:, :, 0] = (img_hsv[:, :, 0] + hue_delta) % 180\r\n    img_hsv[:, :, 1] *= sat_mult\r\n    img_hsv[:, :, 2] *= val_mult\r\n    img_hsv[img_hsv > 255] = 255\r\n    return cv2.cvtColor(np.round(img_hsv).astype(np.uint8), cv2.COLOR_HSV2BGR)\r\n\r\ndef random_hsv_transform(img, hue_vari, sat_vari, val_vari):\r\n    hue_delta = np.random.randint(-hue_vari, hue_vari)\r\n    sat_mult = 1 + np.random.uniform(-sat_vari, sat_vari)\r\n    val_mult = 1 + np.random.uniform(-val_vari, val_vari)\r\n    return hsv_transform(img, hue_delta, sat_mult, val_mult)\r\n\r\ndef gamma_transform(img, gamma):\r\n    gamma_table = [np.power(x / 255.0, gamma) * 255.0 for x in range(256)]\r\n    gamma_table = np.round(np.array(gamma_table)).astype(np.uint8)\r\n    return cv2.LUT(img, gamma_table)\r\n\r\ndef random_gamma_transform(img, gamma_vari):\r\n    log_gamma_vari = np.log(gamma_vari)\r\n    alpha = np.random.uniform(-log_gamma_vari, log_gamma_vari)\r\n    gamma = np.exp(alpha)\r\n    return gamma_transform(img, gamma)\r\n\r\n\r\n'"
chap6/data_augmentation/run_augmentation.py,0,"b""import os\r\nimport argparse\r\nimport random\r\nimport math\r\nfrom multiprocessing import Process, cpu_count\r\n\r\nimport cv2\r\n\r\nimport image_augmentation as ia\r\n\r\ndef parse_args():\r\n    parser = argparse.ArgumentParser(\r\n        description='A Simple Image Data Augmentation Tool',\r\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter)\r\n\r\n    parser.add_argument('input_dir',\r\n                        help='Directory containing images')\r\n    parser.add_argument('output_dir',\r\n                        help='Directory for augmented images')\r\n    parser.add_argument('num',\r\n                        help='Number of images to be augmented',\r\n                        type=int)\r\n\r\n    parser.add_argument('--num_procs',\r\n                        help='Number of processes for paralleled augmentation',\r\n                        type=int, default=cpu_count())\r\n\r\n    parser.add_argument('--p_mirror',\r\n                        help='Ratio to mirror an image',\r\n                        type=float, default=0.5)\r\n\r\n    parser.add_argument('--p_crop',\r\n                        help='Ratio to randomly crop an image',\r\n                        type=float, default=1.0)\r\n    parser.add_argument('--crop_size',\r\n                        help='The ratio of cropped image size to original image size, in area',\r\n                        type=float, default=0.8)\r\n    parser.add_argument('--crop_hw_vari',\r\n                        help='Variation of h/w ratio',\r\n                        type=float, default=0.1)\r\n\r\n    parser.add_argument('--p_rotate',\r\n                        help='Ratio to randomly rotate an image',\r\n                        type=float, default=1.0)\r\n    parser.add_argument('--p_rotate_crop',\r\n                        help='Ratio to crop out the empty part in a rotated image',\r\n                        type=float, default=1.0)\r\n    parser.add_argument('--rotate_angle_vari',\r\n                        help='Variation range of rotate angle',\r\n                        type=float, default=10.0)\r\n\r\n    parser.add_argument('--p_hsv',\r\n                        help='Ratio to randomly change gamma of an image',\r\n                        type=float, default=1.0)\r\n    parser.add_argument('--hue_vari',\r\n                        help='Variation of hue',\r\n                        type=int, default=10)\r\n    parser.add_argument('--sat_vari',\r\n                        help='Variation of saturation',\r\n                        type=float, default=0.1)\r\n    parser.add_argument('--val_vari',\r\n                        help='Variation of value',\r\n                        type=float, default=0.1)\r\n\r\n    parser.add_argument('--p_gamma',\r\n                        help='Ratio to randomly change gamma of an image',\r\n                        type=float, default=1.0)\r\n    parser.add_argument('--gamma_vari',\r\n                        help='Variation of gamma',\r\n                        type=float, default=2.0)\r\n\r\n    args = parser.parse_args()\r\n    args.input_dir = args.input_dir.rstrip('/')\r\n    args.output_dir = args.output_dir.rstrip('/')\r\n\r\n    return args\r\n\r\ndef generate_image_list(args):\r\n    filenames = os.listdir(args.input_dir)\r\n    num_imgs = len(filenames)\r\n\r\n    num_ave_aug = int(math.floor(args.num/num_imgs))\r\n    rem = args.num - num_ave_aug*num_imgs\r\n    lucky_seq = [True]*rem + [False]*(num_imgs-rem)\r\n    random.shuffle(lucky_seq)\r\n\r\n    img_list = [\r\n        (os.sep.join([args.input_dir, filename]), num_ave_aug+1 if lucky else num_ave_aug)\r\n        for filename, lucky in zip(filenames, lucky_seq)\r\n    ]\r\n\r\n    random.shuffle(img_list)  # in case the file size are not uniformly distributed\r\n\r\n    length = float(num_imgs) / float(args.num_procs)\r\n    indices = [int(round(i * length)) for i in range(args.num_procs + 1)]\r\n    return [img_list[indices[i]:indices[i + 1]] for i in range(args.num_procs)]\r\n\r\ndef augment_images(filelist, args):\r\n    for filepath, n in filelist:\r\n        img = cv2.imread(filepath)\r\n        filename = filepath.split(os.sep)[-1]\r\n        dot_pos = filename.rfind('.')\r\n        imgname = filename[:dot_pos]\r\n        ext = filename[dot_pos:]\r\n\r\n        print('Augmenting {} ...'.format(filename))\r\n        for i in range(n):\r\n            img_varied = img.copy()\r\n            varied_imgname = '{}_{:0>3d}_'.format(imgname, i)\r\n            if random.random() < args.p_mirror:\r\n                img_varied = cv2.flip(img_varied, 1)\r\n                varied_imgname += 'm'\r\n            if random.random() < args.p_crop:\r\n                img_varied = ia.random_crop(\r\n                    img_varied,\r\n                    args.crop_size,\r\n                    args.crop_hw_vari)\r\n                varied_imgname += 'c'\r\n            if random.random() < args.p_rotate:\r\n                img_varied = ia.random_rotate(\r\n                    img_varied,\r\n                    args.rotate_angle_vari,\r\n                    args.p_rotate_crop)\r\n                varied_imgname += 'r'\r\n            if random.random() < args.p_hsv:\r\n                img_varied = ia.random_hsv_transform(\r\n                    img_varied,\r\n                    args.hue_vari,\r\n                    args.sat_vari,\r\n                    args.val_vari)\r\n                varied_imgname += 'h'\r\n            if random.random() < args.p_gamma:\r\n                img_varied = ia.random_gamma_transform(\r\n                    img_varied,\r\n                    args.gamma_vari)\r\n                varied_imgname += 'g'\r\n            output_filepath = os.sep.join([\r\n                args.output_dir,\r\n                '{}{}'.format(varied_imgname, ext)])\r\n            cv2.imwrite(output_filepath, img_varied)\r\n\r\ndef main():\r\n    args = parse_args()\r\n    params_str = str(args)[10:-1]\r\n\r\n    if not os.path.exists(args.output_dir):\r\n        os.mkdir(args.output_dir)\r\n\r\n    print('Starting image data augmentation for {}\\n'\r\n          'with\\n{}\\n'.format(args.input_dir, params_str))\r\n\r\n    sublists = generate_image_list(args)\r\n    processes = [Process(target=augment_images, args=(x, args, )) for x in sublists]\r\n\r\n    for p in processes:\r\n        p.start()\r\n\r\n    for p in processes:\r\n        p.join()\r\n\r\n    print('\\nDone!')\r\n\r\nif __name__ == '__main__':\r\n    main()\r\n"""
chap6/data_augmentation/run_augmentation_pool_map.py,0,"b""import os\r\nimport argparse\r\nimport random\r\nimport math\r\nfrom multiprocessing import cpu_count, Pool\r\nfrom functools import partial\r\n\r\nimport cv2\r\n\r\nimport image_augmentation as ia\r\n\r\ndef parse_args():\r\n    parser = argparse.ArgumentParser(\r\n        description='A Simple Image Data Augmentation Tool',\r\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter)\r\n\r\n    parser.add_argument('input_dir',\r\n                        help='Directory containing images')\r\n    parser.add_argument('output_dir',\r\n                        help='Directory for augmented images')\r\n    parser.add_argument('num',\r\n                        help='Number of images to be augmented',\r\n                        type=int)\r\n\r\n    parser.add_argument('--num_procs',\r\n                        help='Number of processes for paralleled augmentation',\r\n                        type=int, default=cpu_count())\r\n\r\n    parser.add_argument('--p_mirror',\r\n                        help='Ratio to mirror an image',\r\n                        type=float, default=0.5)\r\n\r\n    parser.add_argument('--p_crop',\r\n                        help='Ratio to randomly crop an image',\r\n                        type=float, default=1.0)\r\n    parser.add_argument('--crop_size',\r\n                        help='The ratio of cropped image size to original image size, in area',\r\n                        type=float, default=0.8)\r\n    parser.add_argument('--crop_hw_vari',\r\n                        help='Variation of h/w ratio',\r\n                        type=float, default=0.1)\r\n\r\n    parser.add_argument('--p_rotate',\r\n                        help='Ratio to randomly rotate an image',\r\n                        type=float, default=1.0)\r\n    parser.add_argument('--p_rotate_crop',\r\n                        help='Ratio to crop out the empty part in a rotated image',\r\n                        type=float, default=1.0)\r\n    parser.add_argument('--rotate_angle_vari',\r\n                        help='Variation range of rotate angle',\r\n                        type=float, default=10.0)\r\n\r\n    parser.add_argument('--p_hsv',\r\n                        help='Ratio to randomly change gamma of an image',\r\n                        type=float, default=1.0)\r\n    parser.add_argument('--hue_vari',\r\n                        help='Variation of hue',\r\n                        type=int, default=10)\r\n    parser.add_argument('--sat_vari',\r\n                        help='Variation of saturation',\r\n                        type=float, default=0.1)\r\n    parser.add_argument('--val_vari',\r\n                        help='Variation of value',\r\n                        type=float, default=0.1)\r\n\r\n    parser.add_argument('--p_gamma',\r\n                        help='Ratio to randomly change gamma of an image',\r\n                        type=float, default=1.0)\r\n    parser.add_argument('--gamma_vari',\r\n                        help='Variation of gamma',\r\n                        type=float, default=2.0)\r\n\r\n    args = parser.parse_args()\r\n    args.input_dir = args.input_dir.rstrip('/')\r\n    args.output_dir = args.output_dir.rstrip('/')\r\n\r\n    return args\r\n\r\ndef generate_image_list(args):\r\n    filenames = os.listdir(args.input_dir)\r\n    num_imgs = len(filenames)\r\n\r\n    num_ave_aug = int(math.floor(args.num/num_imgs))\r\n    rem = args.num - num_ave_aug*num_imgs\r\n    lucky_seq = [True]*rem + [False]*(num_imgs-rem)\r\n    random.shuffle(lucky_seq)\r\n\r\n    img_list = [\r\n        (os.sep.join([args.input_dir, filename]), num_ave_aug+1 if lucky else num_ave_aug)\r\n        for filename, lucky in zip(filenames, lucky_seq)\r\n    ]\r\n\r\n    random.shuffle(img_list)  # in case the file size are not uniformly distributed\r\n    return img_list\r\n\r\ndef augment_image(image_num_pair, args):\r\n    filepath, n = image_num_pair\r\n    img = cv2.imread(filepath)\r\n    filename = filepath.split(os.sep)[-1]\r\n    dot_pos = filename.rfind('.')\r\n    imgname = filename[:dot_pos]\r\n    ext = filename[dot_pos:]\r\n\r\n    print('Augmenting {} ...'.format(filename))\r\n    for i in range(n):\r\n        img_varied = img.copy()\r\n        varied_imgname = '{}_{:0>3d}_'.format(imgname, i)\r\n        if random.random() < args.p_mirror:\r\n            img_varied = cv2.flip(img_varied, 1)\r\n            varied_imgname += 'm'\r\n        if random.random() < args.p_crop:\r\n            img_varied = ia.random_crop(\r\n                img_varied,\r\n                args.crop_size,\r\n                args.crop_hw_vari)\r\n            varied_imgname += 'c'\r\n        if random.random() < args.p_rotate:\r\n            img_varied = ia.random_rotate(\r\n                img_varied,\r\n                args.rotate_angle_vari,\r\n                args.p_rotate_crop)\r\n            varied_imgname += 'r'\r\n        if random.random() < args.p_hsv:\r\n            img_varied = ia.random_hsv_transform(\r\n                img_varied,\r\n                args.hue_vari,\r\n                args.sat_vari,\r\n                args.val_vari)\r\n            varied_imgname += 'h'\r\n        if random.random() < args.p_gamma:\r\n            img_varied = ia.random_gamma_transform(\r\n                img_varied,\r\n                args.gamma_vari)\r\n            varied_imgname += 'g'\r\n        output_filepath = os.sep.join([\r\n            args.output_dir,\r\n            '{}{}'.format(varied_imgname, ext)])\r\n        cv2.imwrite(output_filepath, img_varied)\r\n\r\ndef main():\r\n    args = parse_args()\r\n    params_str = str(args)[10:-1]\r\n\r\n    if not os.path.exists(args.output_dir):\r\n        os.mkdir(args.output_dir)\r\n\r\n    print('Starting image data augmentation for {}\\n'\r\n          'with\\n{}\\n'.format(args.input_dir, params_str))\r\n\r\n    image_list = generate_image_list(args)\r\n    aug_img = partial(augment_image, args=args)\r\n    pool = Pool(args.num_procs)\r\n    pool.map(aug_img, image_list)\r\n\r\n    print('\\nDone!')\r\n\r\nif __name__ == '__main__':\r\n    main()\r\n"""
chap7/caffe/gen_hdf5.py,0,"b""import pickle\nimport numpy as np\nimport h5py\n\nwith open('../data.pkl', 'rb') as f:\n    samples, labels = pickle.load(f)\nsample_size = len(labels)\n\nsamples = np.array(samples).reshape((sample_size, 2))\nlabels = np.array(labels).reshape((sample_size, 1))\n\nh5_filename = 'data.h5'\nwith h5py.File(h5_filename, 'w') as h:\n    h.create_dataset('data', data=samples)\n    h.create_dataset('label', data=labels)\n\nwith open('data_h5.txt', 'w') as f:\n    f.write(h5_filename)\n"""
chap7/caffe/simple_mlp_test.py,0,"b""import sys\nimport pickle\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\nsys.path.append('/path/to/caffe/python')\nimport caffe\n\nnet = caffe.Net('test.prototxt', 'simple_mlp_iter_2000.caffemodel', caffe.TEST)\n\n# load original data\nwith open('../data.pkl', 'rb') as f:\n    samples, labels = pickle.load(f)\nsamples = np.array(samples)\nlabels = np.array(labels)\n\n# Visualize result\nX = np.arange(0, 1.05, 0.05)\nY = np.arange(0, 1.05, 0.05)\nX, Y = np.meshgrid(X, Y)\n\n# Plot the surface of probability\ngrids = np.array([[X[i][j], Y[i][j]] for i in range(X.shape[0]) for j in range(X.shape[1])])\ngrid_probs = []\nfor grid in grids:\n    net.blobs['data'].data[...] = grid.reshape((1, 2))[...]\n    output = net.forward()\n    grid_probs.append(output['prob'][0][1])\n\ngrid_probs = np.array(grid_probs).reshape(X.shape)\n\nfig = plt.figure('Sample Surface')\nax = fig.gca(projection='3d')\n\nax.plot_surface(X, Y, grid_probs, alpha=0.15, color='k', rstride=2, cstride=2, lw=0.5)\n\n# Plot the predicted probability of samples\nsamples0 = samples[labels==0]\nsamples0_probs = []\nfor sample in samples0:\n    net.blobs['data'].data[...] = sample.reshape((1, 2))[...]\n    output = net.forward()\n    samples0_probs.append(output['prob'][0][1])\n\nsamples1 = samples[labels==1]\nsamples1_probs = []\nfor sample in samples1:\n    net.blobs['data'].data[...] = sample.reshape((1, 2))[...]\n    output = net.forward()\n    samples1_probs.append(output['prob'][0][1])\n\nax.scatter(samples0[:, 0], samples0[:, 1], samples0_probs, c='b', marker='^', s=50)\nax.scatter(samples1[:, 0], samples1[:, 1], samples1_probs, c='r', marker='o', s=50)\n\nplt.show()\n"""
chap7/caffe/simple_mlp_train.py,0,"b""import sys\nimport numpy as np\n\nsys.path.append('/path/to/caffe/python')\nimport caffe\n\nsolver = caffe.SGDSolver('solver.prototxt')\nsolver.solve()\n\nnet = solver.net\nnet.blobs['data'] = np.array([[0.5, 0.5]])\noutput = net.forward()\nprint(output)\n"""
chap7/mxnet/simple_mlp.py,0,"b""import pickle\nimport logging\nimport numpy as np\nimport mxnet as mx\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\n\n# Define the network\ndata = mx.sym.Variable('data')\nfc1 = mx.sym.FullyConnected(data=data, name='fc1', num_hidden=2)\nsigmoid1 = mx.sym.Activation(data=fc1, name='sigmoid1', act_type='sigmoid')\nfc2 = mx.sym.FullyConnected(data=sigmoid1, name='fc2', num_hidden=2)\nmlp = mx.sym.SoftmaxOutput(data=fc2, name='softmax')\n\nshape = {'data': (2,)}\nmlp_dot = mx.viz.plot_network(symbol=mlp, shape=shape)\nmlp_dot.render('simple_mlp.gv', view=True)\n\n# Load data & train the model\nwith open('../data.pkl', 'rb') as f:\n    samples, labels = pickle.load(f)\n\nlogging.getLogger().setLevel(logging.DEBUG)\n\nbatch_size = len(labels)\nsamples = np.array(samples)\nlabels = np.array(labels)\n\ntrain_iter = mx.io.NDArrayIter(samples, labels, batch_size)\n\nmodel = mx.model.FeedForward.create(\n    symbol=mlp,\n    X=train_iter,\n    num_epoch=1000,\n    learning_rate=0.1,\n    momentum=0.99)\n\n'''\n# Alternative interface to train the model\nmodel = mx.model.FeedForward(\n    symbol=mlp,\n    num_epoch=1000,\n    learning_rate=0.1,\n    momentum=0.99)\nmodel.fit(X=train_iter)\n'''\n\nprint(model.predict(mx.nd.array([[0.5, 0.5]])))\n\n# Visualize result\nX = np.arange(0, 1.05, 0.05)\nY = np.arange(0, 1.05, 0.05)\nX, Y = np.meshgrid(X, Y)\n\ngrids = mx.nd.array([[X[i][j], Y[i][j]] for i in range(X.shape[0]) for j in range(X.shape[1])])\ngrid_probs = model.predict(grids)[:, 1].reshape(X.shape)\n\nfig = plt.figure('Sample Surface')\nax = fig.gca(projection='3d')\n\nax.plot_surface(X, Y, grid_probs, alpha=0.15, color='k', rstride=2, cstride=2, lw=0.5)\nsamples0 = samples[labels==0]\nsamples0_probs = model.predict(samples0)[:, 1]\nsamples1 = samples[labels==1]\nsamples1_probs = model.predict(samples1)[:, 1]\n\nax.scatter(samples0[:, 0], samples0[:, 1], samples0_probs, c='b', marker='^', s=50)\nax.scatter(samples1[:, 0], samples1[:, 1], samples1_probs, c='r', marker='o', s=50)\n\nplt.show()\n"""
chap8/caffe/recognize_digit.py,0,"b""import sys\nsys.path.append('/path/to/caffe/python')\nimport numpy as np\nimport cv2\nimport caffe\n\nMEAN = 128\nSCALE = 0.00390625\n\nimglist = sys.argv[1]\n\ncaffe.set_mode_gpu()\ncaffe.set_device(0)\nnet = caffe.Net('lenet.prototxt', 'mnist_lenet_iter_36000.caffemodel', caffe.TEST)\nnet.blobs['data'].reshape(1, 1, 28, 28)\n\nwith open(imglist, 'r') as f:\n    line = f.readline()\n    while line:\n        imgpath, label = line.split()\n        line = f.readline()\n        image = cv2.imread(imgpath, cv2.IMREAD_GRAYSCALE).astype(np.float) - MEAN\n        image *= SCALE\n        net.blobs['data'].data[...] = image\n        output = net.forward()\n        pred_label = np.argmax(output['prob'][0])\n        print('Predicted digit for {} is {}'.format(imgpath, pred_label))\n"""
chap8/data/convert_mnist.py,0,"b""import os\r\nimport pickle, gzip\r\nfrom matplotlib import pyplot\r\n\r\n# Load the dataset\r\nprint('Loading data from mnist.pkl.gz ...')\r\nwith gzip.open('mnist.pkl.gz', 'rb') as f:\r\n    train_set, valid_set, test_set = pickle.load(f)\r\n\r\nimgs_dir = 'mnist'\r\nos.system('mkdir -p {}'.format(imgs_dir))\r\ndatasets = {'train': train_set, 'val': valid_set, 'test': test_set}\r\nfor dataname, dataset in datasets.items():\r\n    print('Converting {} dataset ...'.format(dataname))\r\n    data_dir = os.sep.join([imgs_dir, dataname])\r\n    os.system('mkdir -p {}'.format(data_dir))\r\n    for i, (img, label) in enumerate(zip(*dataset)):\r\n        filename = '{:0>6d}_{}.jpg'.format(i, label)\r\n        filepath = os.sep.join([data_dir, filename])\r\n        img = img.reshape((28, 28))\r\n        pyplot.imsave(filepath, img, cmap='gray')\r\n        if (i+1) % 10000 == 0:\r\n            print('{} images converted!'.format(i+1))\r\n\r\n"""
chap8/data/gen_caffe_imglist.py,0,"b""import os\nimport sys\n\ninput_path = sys.argv[1].rstrip(os.sep)\noutput_path = sys.argv[2]\n\nfilenames = os.listdir(input_path)\n\nwith open(output_path, 'w') as f:\n    for filename in filenames:\n        filepath = os.sep.join([input_path, filename])\n        label = filename[:filename.rfind('.')].split('_')[1]\n        line = '{} {}\\n'.format(filepath, label)\n        f.write(line)\n\n"""
chap8/data/gen_mxnet_imglist.py,0,"b""import os\nimport sys\n\ninput_path = sys.argv[1].rstrip(os.sep)\noutput_path = sys.argv[2]\n\nfilenames = os.listdir(input_path)\n\nwith open(output_path, 'w') as f:\n    for i, filename in enumerate(filenames):\n        filepath = os.sep.join([input_path, filename])\n        label = filename[:filename.rfind('.')].split('_')[1]\n        line = '{}\\t{}\\t{}\\n'.format(i, label, filepath)\n        f.write(line)\n\n"""
chap8/mxnet/benchmark_model.py,0,"b'import time\nimport mxnet as mx\n\nbenchmark_dataiter = mx.io.ImageRecordIter(\n    path_imgrec=""../data/test.rec"",\n    data_shape=(1, 28, 28),\n    batch_size=64,\n    mean_r=128,\n    scale=0.00390625,\n)\n\nmod = mx.mod.Module.load(\'mnist_lenet\', 35, context=mx.gpu(2))\nmod.bind(\n    data_shapes=benchmark_dataiter.provide_data, \n    label_shapes=benchmark_dataiter.provide_label, \n    for_training=False)\n\nstart = time.time()\nfor i, batch in enumerate(benchmark_dataiter):\n    mod.forward(batch)\ntime_elapsed = time.time() - start\nmsg = \'{} batches iterated!\\nAverage forward time per batch: {:.6f} ms\'\nprint(msg.format(i+1, 1000*time_elapsed/float(i)))\n'"
chap8/mxnet/recognize_digit.py,0,"b""import sys\nimport os\nimport cv2\nfrom collections import namedtuple\nBatch = namedtuple('Batch', ['data'])\nimport numpy as np\nimport mxnet as mx\n\ninput_path = sys.argv[1].rstrip(os.sep)\n\nmod = mx.mod.Module.load('mnist_lenet', 35, context=mx.gpu(2))\nmod.bind(\n    data_shapes=[('data', (1, 1, 28, 28))], \n    for_training=False)\n\nfilenames = os.listdir(input_path)\nfor filename in filenames:\n    filepath = os.sep.join([input_path, filename])\n    img = cv2.imread(filepath, cv2.IMREAD_GRAYSCALE)\n    img = (img.astype(np.float)-128) * 0.00390625\n    img = img.reshape((1, 1)+img.shape)\n    mod.forward(Batch([mx.nd.array(img)]))\n    prob = mod.get_outputs()[0].asnumpy()\n    prob = np.squeeze(prob)\n    pred_label = np.argmax(prob)\n    print('Predicted digit for {} is {}'.format(filepath, pred_label))\n"""
chap8/mxnet/score_model.py,0,"b'import mxnet as mx\n\ntest_dataiter = mx.io.ImageRecordIter(\n    path_imgrec=""../data/test.rec"",\n    data_shape=(1, 28, 28),\n    batch_size=100,\n    mean_r=128,\n    scale=0.00390625,\n)\n\nmod = mx.mod.Module.load(\'mnist_lenet\', 35, context=mx.gpu(2))\nmod.bind(\n    data_shapes=test_dataiter.provide_data, \n    label_shapes=test_dataiter.provide_label, \n    for_training=False)\n\n\'\'\'\n# in case we need to continue to train from epoch 35\nmod.fit(...,\n        arg_params=arg_params,\n        aux_params=aux_params,\n        begin_epoch=35)\n\'\'\'\n\nmetric = mx.metric.create(\'acc\')\nmod.score(test_dataiter, metric)\n\nfor name, val in metric.get_name_value():\n    print(\'{}={:.2f}%\'.format(name, val*100))\n'"
chap8/mxnet/train_lenet5.py,0,"b'import mxnet as mx\nimport logging\n\n# data & preprocessing\ndata = mx.symbol.Variable(\'data\')\n\n# 1st conv\nconv1 = mx.symbol.Convolution(data=data, kernel=(5, 5), num_filter=20)\npool1 = mx.symbol.Pooling(data=conv1, pool_type=""max"",\n                          kernel=(2, 2), stride=(2, 2))\n# 2nd conv\nconv2 = mx.symbol.Convolution(data=pool1, kernel=(5, 5), num_filter=50)\npool2 = mx.symbol.Pooling(data=conv2, pool_type=""max"",\n                          kernel=(2, 2), stride=(2, 2))\n# 1st fc & relu\nflatten = mx.symbol.Flatten(data=pool2)\nfc1 = mx.symbol.FullyConnected(data=flatten, num_hidden=500)\nrelu1 = mx.symbol.Activation(data=fc1, act_type=""relu"")\n\n# 2nd fc\nfc2 = mx.symbol.FullyConnected(data=relu1, num_hidden=10)\n# loss\nlenet5 = mx.symbol.SoftmaxOutput(data=fc2, name=\'softmax\')\n\ntrain_dataiter = mx.io.ImageRecordIter(\n    path_imgrec=""../data/train.rec"",\n    data_shape=(1, 28, 28),\n    batch_size=50,\n    mean_r=128,\n    scale=0.00390625,\n    rand_crop=True,\n    min_crop_size=26,\n    max_crop_size=28,\n    max_rotate_angle=15,\n    fill_value=0\n)\n\nval_dataiter = mx.io.ImageRecordIter(\n    path_imgrec=""../data/val.rec"",\n    data_shape=(1, 28, 28),\n    batch_size=100,\n    mean_r=128,\n    scale=0.00390625,\n)\n\nlogging.getLogger().setLevel(logging.DEBUG)\nfh = logging.FileHandler(\'train_mnist_lenet.log\')\nlogging.getLogger().addHandler(fh)\n\nlr_scheduler = mx.lr_scheduler.FactorScheduler(1000, factor=0.95)\noptimizer_params = {\n    \'learning_rate\': 0.01,\n    \'momentum\': 0.9,\n    \'wd\': 0.0005,\n    \'lr_scheduler\': lr_scheduler\n}\ncheckpoint = mx.callback.do_checkpoint(\'mnist_lenet\', period=5)\n\nmod = mx.mod.Module(lenet5, context=mx.gpu(2))\nmod.fit(train_dataiter,\n        eval_data=val_dataiter,\n        optimizer_params=optimizer_params,\n        num_epoch=36,\n        epoch_end_callback=checkpoint)\n'"
random_bonus/adversarial_example_caffe/adversarial_example_demo.py,0,"b""import sys\nfrom operator import itemgetter\nimport numpy\nfrom matplotlib import pyplot\nsys.path.append('/path/to/caffe/python')\nimport caffe\n\n\ndef make_n_test_adversarial_example(\n        img, net, transformer, epsilon,\n        data_blob='data', prob_blob='prob',\n        label_index=None, top_k=5):\n\n    # Load image & forward\n    transformed_img = transformer.preprocess(data_blob, img)\n    net.blobs[data_blob].data[0] = transformed_img\n    net.forward()\n    probs = [x for x in enumerate(net.blobs[prob_blob].data.flatten())]\n    num_classes = len(probs)\n    sorted_probs = sorted(probs, key=itemgetter(1), reverse=True)\n    top_preds = sorted_probs[:top_k]\n    pred = sorted_probs[0][0]\n\n    # if label_index is set,\n    # generate a adversarial example toward the label,\n    # else\n    # reduce the probability of predicted label\n    net.blobs[prob_blob].diff[...] = 0\n    if type(label_index) is int and 0 <= label_index < num_classes:\n        net.blobs[prob_blob].diff[0][label_index] = 1.\n    else:\n        net.blobs[prob_blob].diff[0][pred] = -1.\n\n    # generate attack image with fast gradient sign method\n    diffs = net.backward()\n    diff_sign_mat = numpy.sign(diffs[data_blob])\n    adversarial_noise = epsilon * diff_sign_mat\n\n    # clip exceeded values\n    attack_hwc = transformer.deprocess(data_blob, transformed_img + adversarial_noise[0])\n    attack_hwc[attack_hwc > 1] = 1.\n    attack_hwc[attack_hwc < 0] = 0.\n    attack_img = transformer.preprocess(data_blob, attack_hwc)\n\n    net.blobs[data_blob].data[0] = attack_img\n    net.forward()\n    probs = [x for x in enumerate(net.blobs[prob_blob].data.flatten())]\n    sorted_probs = sorted(probs, key=itemgetter(1), reverse=True)\n    top_attacked_preds = sorted_probs[:top_k]\n\n    return attack_hwc, top_preds, top_attacked_preds\n\n\ndef visualize_attack(title, original_img, attack_img, original_preds, attacked_preds, labels):\n    pred = original_preds[0][0]\n    attacked_pred = attacked_preds[0][0]\n    k = len(original_preds)\n    fig_name = '{}: {} to {}'.format(title, labels[pred], labels[attacked_pred])\n\n    pyplot.figure(fig_name)\n    for img, plt0, plt1, preds in [\n        (original_img, 231, 234, original_preds),\n        (attack_img, 233, 236, attacked_preds)\n    ]:\n        pyplot.subplot(plt0)\n        pyplot.axis('off')\n        pyplot.imshow(img)\n        ax = pyplot.subplot(plt1)\n        pyplot.axis('off')\n        ax.set_xlim([0, 2])\n        bars = ax.barh(range(k-1, -1, -1), [x[1] for x in preds])\n        for i, bar in enumerate(bars):\n            x_loc = bar.get_x() + bar.get_width()\n            y_loc = k - i - 1\n            label = labels[preds[i][0]]\n            ax.text(x_loc, y_loc, '{}: {:.2f}%'.format(label, preds[i][1]*100))\n\n    pyplot.subplot(232)\n    pyplot.axis('off')\n    noise = attack_img - original_img\n    pyplot.imshow(255 * noise)\n\n\nif __name__ == '__main__':\n    # path to test image\n    image_path = sys.argv[1]\n\n    # model to attack\n    model_definition = 'squeezenet-v1.0-deploy-with-force-backward.prototxt'\n    model_weights = 'squeezenet_v1.0.caffemodel'\n    channel_means = numpy.array([104., 117., 123.])\n\n    # initialize net\n    net = caffe.Net(model_definition, model_weights, caffe.TEST)\n    n_channels, height, width = net.blobs['data'].shape[-3:]\n    net.blobs['data'].reshape(1, n_channels, height, width)\n\n    # initialize transformer\n    transformer = caffe.io.Transformer({'data': net.blobs['data'].data.shape})\n    transformer.set_transpose('data', (2, 0, 1))\n    transformer.set_mean('data', channel_means)\n    transformer.set_raw_scale('data', 255)\n    transformer.set_channel_swap('data', (2, 1, 0))\n\n    # load labels from imagenet synset words\n    with open('synset_words.txt', 'r') as f:\n        labels = [x.rstrip()[x.find(' '):].split(',')[0] for x in f.readlines()]\n\n    # load image\n    img = caffe.io.load_image(image_path)\n\n    examples = [\n        (None, 1.0),    # make adversarial example to reduce the predicted probability\n        (296, 1.0),     # make adversarial example toward ice bear(296)\n        (9, 1.0),       # make adversarial example toward ostrich(9)\n        (9, 2.0),       # make adversarial example toward ostrich(9) with stronger noise\n        (9, 6.0),       # make adversarial example toward ostrich(9) with very strong noise\n        (9, 18.0),      # make adversarial example toward ostrich(9) with too strong noise\n        (752, 1.0),     # make adversarial example toward racket(752)\n        (752, 2.0),     # make adversarial example toward racket(752) with stronger noise\n        (752, 6.0),     # make adversarial example toward racket(752) with very strong noise\n        (752, 18.0),    # make adversarial example toward racket(752) with too strong noise\n    ]\n\n    for i, (label_index, epsilon) in enumerate(examples):\n        attack_img, original_preds, attacked_preds = \\\n            make_n_test_adversarial_example(img, net, transformer, epsilon, label_index=label_index)\n        visualize_attack('example{}'.format(i), img, attack_img, original_preds, attacked_preds, labels)\n\n    # try to make adversarial example toward racket(752) with epsilon=0.1, iterate 10 times\n    attack_img, original_preds, attacked_preds = \\\n        make_n_test_adversarial_example(img, net, transformer, 0.1, label_index=752)\n    for i in range(9):\n        attack_img, _, attacked_preds = \\\n            make_n_test_adversarial_example(attack_img, net, transformer, 0.1, label_index=752)\n    visualize_attack('racket_iterative'.format(i), img, attack_img, original_preds, attacked_preds, labels)\n\n    pyplot.show()\n"""
random_bonus/gan_n_cgan_2d_example/argparser.py,1,"b""import os\nimport argparse\nimport torch.optim as optim\n\nOPTIMIZERS = {\n    'adadelta': optim.Adadelta,\n    'adam': optim.Adam,\n    'rmsprop': optim.RMSprop,\n    'sgd': optim.SGD\n}\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser(\n        description='A Simple Demo of Generative Adversarial Networks with 2D Samples',\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n\n    parser.add_argument('input_path',\n                        help='Image or directory containing images to define distribution')\n\n    parser.add_argument('--z_dim',\n                        help='Dimensionality of latent space',\n                        type=int, default=2)\n    parser.add_argument('--iterations',\n                        help='Num of training iterations',\n                        type=int, default=2000)\n    parser.add_argument('--batch_size',\n                        help='Batch size of each kind',\n                        type=int, default=2000)\n    parser.add_argument('--optimizer',\n                        help='Optimizer: Adadelta/Adam/RMSprop/SGD',\n                        type=str, default='Adadelta')\n    parser.add_argument('--d_lr',\n                        help='Learning rate of discriminator, for Adadelta it is the base learning rate',\n                        type=float, default=1)\n    parser.add_argument('--g_lr',\n                        help='Learning rate of generator, for Adadelta it is the base learning rate',\n                        type=float, default=1)\n    parser.add_argument('--d_steps',\n                        help='Steps of discriminators in each iteration',\n                        type=int, default=3)\n    parser.add_argument('--g_steps',\n                        help='Steps of generator in each iteration',\n                        type=int, default=1)\n    parser.add_argument('--d_hidden_size',\n                        help='Num of hidden units in discriminator',\n                        type=int, default=100)\n    parser.add_argument('--g_hidden_size',\n                        help='Num of hidden units in generator',\n                        type=int, default=50)\n    parser.add_argument('--display_interval',\n                        help='Interval of iterations to display/export images',\n                        type=int, default=10)\n    parser.add_argument('--no_display',\n                        help='Show plots during training', action='store_true')\n    parser.add_argument('--export',\n                        help='Export images', action='store_true')\n    parser.add_argument('--cpu',\n                        help='Set to CPU mode', action='store_true')\n\n    args = parser.parse_args()\n    args.input_path = args.input_path.rstrip(os.sep)\n    args.optimizer = OPTIMIZERS[args.optimizer.lower()]\n\n    return args\n"""
random_bonus/gan_n_cgan_2d_example/cgan_demo.py,14,"b""#!/usr/bin/env python\n# Conditional Generative Adversarial Networks (GAN) example with 2D samples in PyTorch.\nimport os\nimport numpy\nfrom skimage import io\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\nfrom sampler import generate_lut, sample_2d\nfrom visualizer import CGANDemoVisualizer\nfrom argparser import parse_args\nfrom networks import SimpleMLP\n\nDIMENSION = 2\n\nargs = parse_args()\ncuda = False if args.cpu else True\nbs = args.batch_size\nz_dim = args.z_dim\n\nimage_paths = [os.sep.join([args.input_path, x]) for x in os.listdir(args.input_path)]\ndensity_imgs = [io.imread(x, True) for x in image_paths]\nluts_2d = [generate_lut(x) for x in density_imgs]\n# Sampling based on visual density, a too small batch size may result in failure with conditions\npix_sums = [numpy.sum(x) for x in density_imgs]\ntotal_pix_sums = numpy.sum(pix_sums)\nc_indices = [0] + [int(sum(pix_sums[:i+1])/total_pix_sums*bs+0.5) for i in range(len(pix_sums)-1)] + [bs]\n\nc_dim = len(luts_2d)    # Dimensionality of condition labels <--> number of images\n\nvisualizer = CGANDemoVisualizer('Conditional GAN 2D Example Visualization of {}'.format(args.input_path))\n\ngenerator = SimpleMLP(input_size=z_dim+c_dim, hidden_size=args.g_hidden_size, output_size=DIMENSION)\ndiscriminator = SimpleMLP(input_size=DIMENSION+c_dim, hidden_size=args.d_hidden_size, output_size=1)\n\nif cuda:\n    generator.cuda()\n    discriminator.cuda()\ncriterion = nn.BCELoss()\n\nd_optimizer = args.optimizer(discriminator.parameters(), lr=args.d_lr)\ng_optimizer = args.optimizer(generator.parameters(), lr=args.d_lr)\n\ny = numpy.zeros((bs, c_dim))\nfor i in range(c_dim):\n    y[c_indices[i]:c_indices[i + 1], i] = 1  # conditional labels, one-hot encoding\ny = Variable(torch.Tensor(y))\nif cuda:\n    y = y.cuda()\n\nfor train_iter in range(args.iterations):\n    for d_index in range(args.d_steps):\n        # 1. Train D on real+fake\n        discriminator.zero_grad()\n\n        #  1A: Train D on real samples with conditions\n        real_samples = numpy.zeros((bs, DIMENSION))\n        for i in range(c_dim):\n            real_samples[c_indices[i]:c_indices[i+1], :] = sample_2d(luts_2d[i], c_indices[i+1]-c_indices[i])\n\n        # first c dimensions is the condition inputs, the last 2 dimensions are samples\n        real_samples = Variable(torch.Tensor(real_samples))\n        if cuda:\n            real_samples = real_samples.cuda()\n        d_real_data = torch.cat([y, real_samples], 1)\n        if cuda:\n            d_real_data = d_real_data.cuda()\n        d_real_decision = discriminator(d_real_data)\n        labels = Variable(torch.ones(bs))\n        if cuda:\n            labels = labels.cuda()\n        d_real_loss = criterion(d_real_decision, labels)  # ones = true\n\n        #  1B: Train D on fake\n        latent_samples = Variable(torch.randn(bs, z_dim))\n        if cuda:\n            latent_samples = latent_samples.cuda()\n        # first c dimensions is the condition inputs, the last z_dim dimensions are latent samples\n        d_gen_input = torch.cat([y, latent_samples], 1)\n        d_fake_data = generator(d_gen_input).detach()  # detach to avoid training G on these labels\n        conditional_d_fake_data = torch.cat([y, d_fake_data], 1)\n        if cuda:\n            conditional_d_fake_data = conditional_d_fake_data.cuda()\n        d_fake_decision = discriminator(conditional_d_fake_data)\n        labels = Variable(torch.zeros(bs))\n        if cuda:\n            labels = labels.cuda()\n        d_fake_loss = criterion(d_fake_decision, labels)  # zeros = fake\n\n        d_loss = d_real_loss + d_fake_loss\n        d_loss.backward()\n\n        d_optimizer.step()     # Only optimizes D's parameters; changes based on stored gradients from backward()\n\n    for g_index in range(args.g_steps):\n        # 2. Train G on D's response (but DO NOT train D on these labels)\n        generator.zero_grad()\n\n        latent_samples = Variable(torch.randn(bs, z_dim))\n        if cuda:\n            latent_samples = latent_samples.cuda()\n        g_gen_input = torch.cat([y, latent_samples], 1)\n        g_fake_data = generator(g_gen_input)\n        conditional_g_fake_data = torch.cat([y, g_fake_data], 1)\n        g_fake_decision = discriminator(conditional_g_fake_data)\n        labels = Variable(torch.ones(bs))\n        if cuda:\n            labels = labels.cuda()\n        g_loss = criterion(g_fake_decision, labels)  # we want to fool, so pretend it's all genuine\n\n        g_loss.backward()\n        g_optimizer.step()  # Only optimizes G's parameters\n\n    if train_iter % args.display_interval == 0:\n        loss_d_real = d_real_loss.data.cpu().numpy()[0] if cuda else d_real_loss.data.numpy()[0]\n        loss_d_fake = d_fake_loss.data.cpu().numpy()[0] if cuda else d_fake_loss.data.numpy()[0]\n        loss_g = g_loss.data.cpu().numpy()[0] if cuda else g_loss.data.numpy()[0]\n\n        msg = 'Iteration {}: D_loss(real/fake): {:.6g}/{:.6g} G_loss: {:.6g}'.format(train_iter, loss_d_real, loss_d_fake, loss_g)\n        print(msg)\n\n        real_samples_with_y = d_real_data.data.cpu().numpy() if cuda else d_real_data.data.numpy()\n        gen_samples_with_y = conditional_g_fake_data.data.cpu().numpy() if cuda else conditional_g_fake_data.data.numpy()\n        if args.no_display:\n            visualizer.draw(real_samples_with_y, gen_samples_with_y, msg, show=False)\n        else:\n            visualizer.draw(real_samples_with_y, gen_samples_with_y, msg)\n\n        if args.export:\n            filename = args.input_path.split(os.sep)[-1]\n            output_dir = 'cgan_training_{}'.format(filename)\n            os.system('mkdir -p {}'.format(output_dir))\n            export_filepath = os.sep.join([output_dir, 'iter_{:0>6d}.png'.format(train_iter)])\n            visualizer.savefig(export_filepath)\n\nif not args.no_display:\n    visualizer.show()\n"""
random_bonus/gan_n_cgan_2d_example/gan_demo.py,8,"b""#!/usr/bin/env python\n# Generative Adversarial Networks (GAN) example with 2D samples in PyTorch.\nimport os\nfrom skimage import io\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\nfrom sampler import generate_lut, sample_2d\nfrom visualizer import GANDemoVisualizer\nfrom argparser import parse_args\nfrom networks import SimpleMLP\n\nDIMENSION = 2\n\nargs = parse_args()\ncuda = False if args.cpu else True\nbs = args.batch_size\nz_dim = args.z_dim\n\ndensity_img = io.imread(args.input_path, True)\nlut_2d = generate_lut(density_img)\n\nvisualizer = GANDemoVisualizer('GAN 2D Example Visualization of {}'.format(args.input_path))\n\ngenerator = SimpleMLP(input_size=z_dim, hidden_size=args.g_hidden_size, output_size=DIMENSION)\ndiscriminator = SimpleMLP(input_size=DIMENSION, hidden_size=args.d_hidden_size, output_size=1)\n\nif cuda:\n    generator.cuda()\n    discriminator.cuda()\ncriterion = nn.BCELoss()\n\nd_optimizer = args.optimizer(discriminator.parameters(), lr=args.d_lr)\ng_optimizer = args.optimizer(generator.parameters(), lr=args.d_lr)\n\nfor train_iter in range(args.iterations):\n    for d_index in range(args.d_steps):\n        # 1. Train D on real+fake\n        discriminator.zero_grad()\n\n        #  1A: Train D on real\n        real_samples = sample_2d(lut_2d, bs)\n        d_real_data = Variable(torch.Tensor(real_samples))\n        if cuda:\n            d_real_data = d_real_data.cuda()\n        d_real_decision = discriminator(d_real_data)\n        labels = Variable(torch.ones(bs))\n        if cuda:\n            labels = labels.cuda()\n        d_real_loss = criterion(d_real_decision, labels)  # ones = true\n\n        #  1B: Train D on fake\n        latent_samples = torch.randn(bs, z_dim)\n        d_gen_input = Variable(latent_samples)\n        if cuda:\n            d_gen_input = d_gen_input.cuda()\n        d_fake_data = generator(d_gen_input).detach()  # detach to avoid training G on these labels\n        d_fake_decision = discriminator(d_fake_data)\n        labels = Variable(torch.zeros(bs))\n        if cuda:\n            labels = labels.cuda()\n        d_fake_loss = criterion(d_fake_decision, labels)  # zeros = fake\n\n        d_loss = d_real_loss + d_fake_loss\n        d_loss.backward()\n\n        d_optimizer.step()     # Only optimizes D's parameters; changes based on stored gradients from backward()\n\n    for g_index in range(args.g_steps):\n        # 2. Train G on D's response (but DO NOT train D on these labels)\n        generator.zero_grad()\n\n        latent_samples = torch.randn(bs, z_dim)\n        g_gen_input = Variable(latent_samples)\n        if cuda:\n            g_gen_input = g_gen_input.cuda()\n        g_fake_data = generator(g_gen_input)\n        g_fake_decision = discriminator(g_fake_data)\n        labels = Variable(torch.ones(bs))\n        if cuda:\n            labels = labels.cuda()\n        g_loss = criterion(g_fake_decision, labels)  # we want to fool, so pretend it's all genuine\n\n        g_loss.backward()\n        g_optimizer.step()  # Only optimizes G's parameters\n\n    if train_iter % args.display_interval == 0:\n        loss_d_real = d_real_loss.data.cpu().numpy()[0] if cuda else d_real_loss.data.numpy()[0]\n        loss_d_fake = d_fake_loss.data.cpu().numpy()[0] if cuda else d_fake_loss.data.numpy()[0]\n        loss_g = g_loss.data.cpu().numpy()[0] if cuda else g_loss.data.numpy()[0]\n\n        msg = 'Iteration {}: D_loss(real/fake): {:.6g}/{:.6g} G_loss: {:.6g}'.format(train_iter, loss_d_real, loss_d_fake, loss_g)\n        print(msg)\n\n        gen_samples = g_fake_data.data.cpu().numpy() if cuda else g_fake_data.data.numpy()\n\n        if args.no_display:\n            visualizer.draw(real_samples, gen_samples, msg, show=False)\n        else:\n            visualizer.draw(real_samples, gen_samples, msg)\n\n        if args.export:\n            filename = args.input_path.split(os.sep)[-1]\n            output_dir = 'gan_training_{}'.format(filename[:filename.rfind('.')])\n            os.system('mkdir -p {}'.format(output_dir))\n            export_filepath = os.sep.join([output_dir, 'iter_{:0>6d}.png'.format(train_iter)])\n            visualizer.savefig(export_filepath)\n\nif not args.no_display:\n    visualizer.show()\n"""
random_bonus/gan_n_cgan_2d_example/networks.py,2,"b'import torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass SimpleMLP(nn.Module):\n    def __init__(self, input_size, hidden_size, output_size):\n        super(SimpleMLP, self).__init__()\n        self.map1 = nn.Linear(input_size, hidden_size)\n        self.map2 = nn.Linear(hidden_size, output_size)\n\n    def forward(self, x):\n        x = F.leaky_relu(self.map1(x), 0.1)\n        return F.sigmoid(self.map2(x))\n\nclass DeepMLP(nn.Module):\n    def __init__(self, input_size, hidden_size, output_size):\n        super(DeepMLP, self).__init__()\n        self.map1 = nn.Linear(input_size, hidden_size)\n        self.map2 = nn.Linear(hidden_size, hidden_size)\n        self.map3 = nn.Linear(hidden_size, output_size)\n\n    def forward(self, x):\n        x = F.leaky_relu(self.map1(x), 0.1)\n        x = F.leaky_relu(self.map2(x), 0.1)\n        return F.sigmoid(self.map3(x))\n'"
random_bonus/gan_n_cgan_2d_example/sampler.py,0,"b'from functools import partial\nimport numpy\nfrom skimage import transform\n\nEPS = 1e-66\nRESOLUTION = 0.001\nnum_grids = int(1/RESOLUTION+0.5)\n\ndef generate_lut(img):\n    """"""\n    linear approximation of CDF & marginal\n    :param density_img:\n    :return: lut_y, lut_x\n    """"""\n    density_img = transform.resize(img, (num_grids, num_grids))\n    x_accumlation = numpy.sum(density_img, axis=1)\n    sum_xy = numpy.sum(x_accumlation)\n    y_cdf_of_accumulated_x = [[0., 0.]]\n    accumulated = 0\n    for ir, i in enumerate(range(num_grids-1, -1, -1)):\n        accumulated += x_accumlation[i]\n        if accumulated == 0:\n            y_cdf_of_accumulated_x[0][0] = float(ir+1)/float(num_grids)\n        elif EPS < accumulated < sum_xy - EPS:\n            y_cdf_of_accumulated_x.append([float(ir+1)/float(num_grids), accumulated/sum_xy])\n        else:\n            break\n    y_cdf_of_accumulated_x.append([float(ir+1)/float(num_grids), 1.])\n    y_cdf_of_accumulated_x = numpy.array(y_cdf_of_accumulated_x)\n\n    x_cdfs = []\n    for j in range(num_grids):\n        x_freq = density_img[num_grids-j-1]\n        sum_x = numpy.sum(x_freq)\n        x_cdf = [[0., 0.]]\n        accumulated = 0\n        for i in range(num_grids):\n            accumulated += x_freq[i]\n            if accumulated == 0:\n                x_cdf[0][0] = float(i+1) / float(num_grids)\n            elif EPS < accumulated < sum_xy - EPS:\n                x_cdf.append([float(i+1)/float(num_grids), accumulated/sum_x])\n            else:\n                break\n        x_cdf.append([float(i+1)/float(num_grids), 1.])\n        if accumulated > EPS:\n            x_cdf = numpy.array(x_cdf)\n            x_cdfs.append(x_cdf)\n        else:\n            x_cdfs.append(None)\n\n    y_lut = partial(numpy.interp, xp=y_cdf_of_accumulated_x[:, 1], fp=y_cdf_of_accumulated_x[:, 0])\n    x_luts = [partial(numpy.interp, xp=x_cdfs[i][:, 1], fp=x_cdfs[i][:, 0]) if x_cdfs[i] is not None else None for i in range(num_grids)]\n\n    return y_lut, x_luts\n\ndef sample_2d(lut, N):\n    y_lut, x_luts = lut\n    u_rv = numpy.random.random((N, 2))\n    samples = numpy.zeros(u_rv.shape)\n    for i, (x, y) in enumerate(u_rv):\n        ys = y_lut(y)\n        x_bin = int(ys/RESOLUTION)\n        xs = x_luts[x_bin](x)\n        samples[i][0] = xs\n        samples[i][1] = ys\n\n    return samples\n\nif __name__ == \'__main__\':\n    from skimage import io\n    density_img = io.imread(\'inputs/random.jpg\', True)\n    lut_2d = generate_lut(density_img)\n    samples = sample_2d(lut_2d, 10000)\n\n    from matplotlib import pyplot\n    fig, (ax0, ax1) = pyplot.subplots(ncols=2, figsize=(9, 4))\n    fig.canvas.set_window_title(\'Test 2D Sampling\')\n    ax0.imshow(density_img, cmap=\'gray\')\n    ax0.xaxis.set_major_locator(pyplot.NullLocator())\n    ax0.yaxis.set_major_locator(pyplot.NullLocator())\n\n    ax1.axis(\'equal\')\n    ax1.axis([0, 1, 0, 1])\n    ax1.plot(samples[:, 0], samples[:, 1], \'k,\')\n    pyplot.show()\n'"
random_bonus/gan_n_cgan_2d_example/visualizer.py,0,"b""from itertools import cycle\nimport numpy\nfrom matplotlib import pyplot\nfrom skimage import filters\n\n\nclass GANDemoVisualizer:\n\n    def __init__(self, title, l_kde=100, bw_kde=5):\n        self.title = title\n        self.l_kde = l_kde\n        self.resolution = 1. / self.l_kde\n        self.bw_kde_ = bw_kde\n        self.fig, self.axes = pyplot.subplots(ncols=3, figsize=(13.5, 4))\n        self.fig.canvas.set_window_title(self.title)\n\n    def draw(self, real_samples, gen_samples, msg=None, cmap='hot', pause_time=0.05, max_sample_size=500, show=True):\n        if msg:\n            self.fig.suptitle(msg)\n        ax0, ax1, ax2 = self.axes\n\n        self.draw_samples(ax0, 'real and generated samples', real_samples, gen_samples, max_sample_size)\n        self.draw_density_estimation(ax1, 'density: real samples', real_samples, cmap)\n        self.draw_density_estimation(ax2, 'density: generated samples', gen_samples, cmap)\n\n        if show:\n            pyplot.draw()\n            pyplot.pause(pause_time)\n\n    @staticmethod\n    def draw_samples(axis, title, real_samples, generated_samples, max_sample_size):\n        axis.clear()\n        axis.set_xlabel(title)\n        axis.plot(generated_samples[:max_sample_size, 0], generated_samples[:max_sample_size, 1], '.')\n        axis.plot(real_samples[:max_sample_size, 0], real_samples[:max_sample_size, 1], 'kx')\n        axis.axis('equal')\n        axis.axis([0, 1, 0, 1])\n\n    def draw_density_estimation(self, axis, title, samples, cmap):\n        axis.clear()\n        axis.set_xlabel(title)\n        density_estimation = numpy.zeros((self.l_kde, self.l_kde))\n        for x, y in samples:\n            if 0 < x < 1 and 0 < y < 1:\n                density_estimation[int((1-y) / self.resolution)][int(x / self.resolution)] += 1\n        density_estimation = filters.gaussian(density_estimation, self.bw_kde_)\n        axis.imshow(density_estimation, cmap=cmap)\n        axis.xaxis.set_major_locator(pyplot.NullLocator())\n        axis.yaxis.set_major_locator(pyplot.NullLocator())\n\n    def savefig(self, filepath):\n        self.fig.savefig(filepath)\n\n    @staticmethod\n    def show():\n        pyplot.show()\n\n\nclass CGANDemoVisualizer(GANDemoVisualizer):\n\n    def __init__(self, title, l_kde=100, bw_kde=5):\n        GANDemoVisualizer.__init__(self, title, l_kde, bw_kde)\n\n    def draw(self, real_samples, gen_samples, msg=None, cmap='hot', pause_time=0.05, max_sample_size=500, show=True):\n        if msg:\n            self.fig.suptitle(msg)\n        ax0, ax1, ax2 = self.axes\n\n        self.draw_samples(ax0, 'real and generated samples', real_samples, gen_samples, max_sample_size)\n        self.draw_density_estimation(ax1, 'density: real samples', real_samples[:, -2:], cmap)\n        self.draw_density_estimation(ax2, 'density: generated samples', gen_samples[:, -2:], cmap)\n\n        if show:\n            pyplot.draw()\n            pyplot.pause(pause_time)\n\n    def draw_samples(self, axis, title, real_samples, generated_samples, max_sample_size):\n        axis.clear()\n        axis.set_xlabel(title)\n        g_samples = numpy.copy(generated_samples)\n        r_samples = numpy.copy(real_samples)\n        numpy.random.shuffle(g_samples)\n        numpy.random.shuffle(r_samples)\n        g_samples = g_samples[:max_sample_size, :]\n        r_samples = r_samples[:max_sample_size, :]\n        color_iter = cycle('bgrcmy')\n        for i in range(g_samples.shape[1]-2):\n            c = next(color_iter)\n            samples = g_samples[g_samples[:, i] > 0, :][:, -2:]\n            axis.plot(samples[:, 0], samples[:, 1], c+'.', markersize=5)\n            samples = r_samples[r_samples[:, i] > 0, :][:, -2:]\n            axis.plot(samples[:, 0], samples[:, 1], c+'x', markersize=5)\n        axis.axis('equal')\n        axis.axis([0, 1, 0, 1])\n\n    def savefig(self, filepath):\n        self.fig.savefig(filepath)\n\n    @staticmethod\n    def show():\n        pyplot.show()\n"""
random_bonus/generate_mosaic_for_porno_images/crop_n_resize.py,0,"b""import os\r\nimport cv2\r\nimport sys\r\n\r\nfolders = sys.argv[1:-1]\r\nlength = int(sys.argv[-1])\r\n\r\ncnt = 0\r\nfor folder in folders:\r\n    print('scanning {} ...'.format(folder))\r\n    folder = folder.rstrip('/')\r\n    files = os.listdir(folder)\r\n\r\n    for img_file in files:\r\n        filepath = '{}/{}'.format(folder, img_file)\r\n        try:\r\n            img = cv2.imread(filepath)\r\n            h, w, c = img.shape\r\n        except:\r\n            print('problematic file:', filepath)\r\n            continue\r\n\r\n        if img is None:\r\n            print('problematic file:', filepath)\r\n            continue\r\n        elif h == length and w == length:\r\n            continue\r\n        else:\r\n            if h > w:\r\n                dl = int((h-w)/2)\r\n                if dl > 0:\r\n                    img = img[dl:-dl, ...]\r\n            else:\r\n                dl = int((w-h)/2)\r\n                if dl > 0:\r\n                    img = img[:, dl:-dl, ...]\r\n            img = cv2.resize(img, (length, length))\r\n            cv2.imwrite(filepath, img)\r\n\r\n        cnt += 1\r\n        if cnt % 100 == 0:\r\n            print('{} images processed!'.format(cnt))\r\n\r\nprint('Done!')\r\n\r\n"""
random_bonus/generate_mosaic_for_porno_images/gen_mosaic.py,0,"b""import sys\nimport os\nimport numpy as np\nimport cv2\nsys.path.append('/path/to/caffe/python')\nimport caffe\n\nWEIGHTS_FILE = 'open_nsfw/nsfw_model/resnet_50_1by2_nsfw.caffemodel'\nDEPLOY_FILE = 'deploy_global_pooling.prototxt'\nFEATURE_MAPS = 'eltwise_stage3_block2'\nFC_LAYER = 'fc_nsfw'\n\nSHORT_EDGE = 320\nMOSAIC_RANGE = [5, 15]\n\n#caffe.set_mode_cpu()\nnet = caffe.Net(DEPLOY_FILE, WEIGHTS_FILE, caffe.TEST)\ninput_dir = sys.argv[1]\noutput_dir = sys.argv[2]\nos.system('mkdir -p {}'.format(output_dir))\n\nporno = 1\nmask_th = 0.5\n\nfilenames = os.listdir(input_dir)\nfor i, filename in enumerate(filenames):\n    filepath = os.sep.join([input_dir, filename])\n\n    image = cv2.imread(filepath)[:, :, :3]\n    height, width = image.shape[:2]\n\n    short_edge_image = min(image.shape[:2])\n    scale_ratio = float(SHORT_EDGE) / float(short_edge_image)\n    if scale_ratio < 1:\n        transformed_image = cv2.resize(image, (0, 0), fx=scale_ratio, fy=scale_ratio)\n    else:\n        transformed_image = np.copy(image)\n    transformed_image = transformed_image.astype(np.float32)\n    transformed_image -= np.array([104., 117., 123.])\n    transformed_image = np.transpose(transformed_image, (2, 0, 1))\n\n    net.blobs['data'].reshape(1, 3, transformed_image.shape[1], transformed_image.shape[2])\n    net.blobs['data'].data[...] = transformed_image\n\n    mosaic_size = np.random.random_integers(MOSAIC_RANGE[0], MOSAIC_RANGE[1]+1, 1)\n    scale_mosaic = 1 / float(mosaic_size)\n    mosaic_image = cv2.resize(image, (0, 0), fx=scale_mosaic, fy=scale_mosaic)\n    mosaic_image = cv2.resize(mosaic_image, (width, height), interpolation=cv2.INTER_NEAREST)\n\n    net.forward()\n    feature_maps = net.blobs[FEATURE_MAPS].data[0]\n    fc_params = net.params[FC_LAYER]\n    fc_w = fc_params[0].data[porno]\n\n    activation_map = np.zeros_like(feature_maps[0])\n    for feature_map, w in zip(feature_maps, fc_w):\n        activation_map += feature_map * w\n\n    activation_map = cv2.resize(activation_map, (width, height), interpolation=cv2.INTER_CUBIC)\n    activation_map -= activation_map.min()\n    activation_map /= activation_map.max()\n    mask = np.zeros(activation_map.shape)\n    mask[activation_map > mask_th] = 1\n    image_with_mosaic = np.copy(image)\n    image_with_mosaic[mask > mask_th] = mosaic_image[mask > mask_th]\n\n    output_filepath = os.sep.join([output_dir, filename])\n    cv2.imwrite(output_filepath, image_with_mosaic)\n\n    if (i+1) % 100 == 0:\n        print('{} images processed!'.format(i+1))\n        \n    # uncomment the following for visualization\n    #vis_img = np.hstack([image, image_with_mosaic])\n    #cv2.imshow('Mosaic Visualization', vis_img)\n    #cv2.waitKey()\n\nprint('Done!')\n\n"""
random_bonus/great-circle-interp/distance-experiment.py,0,"b""import numpy\nfrom matplotlib import pyplot\n\n\ndef dist_o2l(p1, p2):\n    # distance from origin to the line defined by (p1, p2)\n    p12 = p2 - p1\n    u12 = p12 / numpy.linalg.norm(p12)\n    l_pp = numpy.dot(-p1, u12)\n    pp = l_pp*u12 + p1\n    return numpy.linalg.norm(pp)\n\ndim = 100\nN = 100000\n\nrvs = []\ndists2l = []\nfor i in range(N):\n    u = numpy.random.randn(dim)\n    v = numpy.random.randn(dim)\n    rvs.extend([u, v])\n    dists2l.append(dist_o2l(u, v))\n\ndists = [numpy.linalg.norm(x) for x in rvs]\n\nprint('Distances to samples, mean: {}, std: {}'.format(numpy.mean(dists), numpy.std(dists)))\nprint('Distances to lines, mean: {}, std: {}'.format(numpy.mean(dists2l), numpy.std(dists2l)))\n\nfig, (ax0, ax1) = pyplot.subplots(ncols=2, figsize=(11, 5))\nax0.hist(dists, 100, normed=1, color='g')\nax1.hist(dists2l, 100, normed=1, color='b')\npyplot.show()\n"""
random_bonus/great-circle-interp/latent-walk-great-circle.py,4,"b'from __future__ import print_function\nimport argparse\nimport os\nimport numpy\nfrom scipy.stats import chi\nimport torch.utils.data\nfrom torch.autograd import Variable\nfrom networks import NetG\nfrom PIL import Image\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\'--nz\', type=int, default=100, help=\'size of the latent z vector\')\nparser.add_argument(\'--niter\', type=int, default=10, help=\'how many paths\')\nparser.add_argument(\'--n_steps\', type=int, default=23, help=\'steps to walk\')\nparser.add_argument(\'--ngf\', type=int, default=64)\nparser.add_argument(\'--ngpu\', type=int, default=1, help=\'number of GPUs to use\')\nparser.add_argument(\'--netG\', default=\'netG_epoch_49.pth\', help=""trained params for G"")\n\nopt = parser.parse_args()\noutput_dir = \'gcircle-walk\'\nos.system(\'mkdir -p {}\'.format(output_dir))\nprint(opt)\n\nngpu = int(opt.ngpu)\nnz = int(opt.nz)\nngf = int(opt.ngf)\nnc = 3\n\nnetG = NetG(ngf, nz, nc, ngpu)\nnetG.load_state_dict(torch.load(opt.netG, map_location=lambda storage, loc: storage))\nnetG.eval()\nprint(netG)\n\nfor j in range(opt.niter):\n    # step 1\n    r = chi.rvs(df=100)\n\n    # step 2\n    u = numpy.random.normal(0, 1, nz)\n    w = numpy.random.normal(0, 1, nz)\n    u /= numpy.linalg.norm(u)\n    w /= numpy.linalg.norm(w)\n\n    v = w - numpy.dot(u, w) * u\n    v /= numpy.linalg.norm(v)\n\n    ndimgs = []\n    for i in range(opt.n_steps):\n        t = float(i) / float(opt.n_steps)\n        # step 3\n        z = numpy.cos(t * 2 * numpy.pi) * u + numpy.sin(t * 2 * numpy.pi) * v\n        z *= r\n\n        noise_t = z.reshape((1, nz, 1, 1))\n        noise_t = torch.FloatTensor(noise_t)\n        noisev = Variable(noise_t)\n        fake = netG(noisev)\n        timg = fake[0]\n        timg = timg.data\n\n        timg.add_(1).div_(2)\n        ndimg = timg.mul(255).clamp(0, 255).byte().permute(1, 2, 0).numpy()\n        ndimgs.append(ndimg)\n\n    print(\'exporting {} ...\'.format(j))\n    ndimg = numpy.hstack(ndimgs)\n\n    im = Image.fromarray(ndimg)\n    filename = os.sep.join([output_dir, \'gc-{:0>6d}.png\'.format(j)])\n    im.save(filename)\n'"
random_bonus/great-circle-interp/latent-walk-slerp-vs-lerp.py,5,"b'from __future__ import print_function\nimport argparse\nimport numpy\nimport torch.utils.data\nfrom torch.autograd import Variable\nimport networks\nfrom PIL import Image\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\'--nz\', type=int, default=100, help=\'size of the latent z vector\')\nparser.add_argument(\'--n_samples\', type=int, default=10, help=\'how many images\')\nparser.add_argument(\'--n_steps\', type=int, default=11, help=\'steps for interpolation\')\nparser.add_argument(\'--ngf\', type=int, default=64)\nparser.add_argument(\'--ngpu\', type=int, default=1, help=\'number of GPUs to use\')\nparser.add_argument(\'--netG\', default=\'netG_epoch_49.pth\', help=""path to netG"")\n\nopt = parser.parse_args()\nprint(opt)\n\nngpu = int(opt.ngpu)\nnz = int(opt.nz)\nngf = int(opt.ngf)\nnc = 3\n\nnetG = networks.NetG(ngf, nz, nc, ngpu)\nnetG.eval()\nnetG.load_state_dict(torch.load(opt.netG, map_location=lambda storage, loc: storage))\nprint(netG)\n\nn_steps = opt.n_steps\nfor epoch in range(opt.n_samples):\n    u = numpy.random.randn(nz)\n    v = numpy.random.randn(nz)\n    lu = numpy.linalg.norm(u)\n    lv = numpy.linalg.norm(v)\n    theta = numpy.arccos(numpy.dot(u, v)/lu/lv)\n\n    ndimgs_slerp = []\n    ndimgs_lerp = []\n    for i in range(n_steps+1):\n        t = float(i) / float(n_steps)\n\n        # slerp\n        z_slerp = numpy.sin((1 - t) * theta) / numpy.sin(theta) * u + numpy.sin(t * theta) / numpy.sin(theta) * v\n\n        noise_t = z_slerp.reshape((1, nz, 1, 1))\n        noise_t = torch.FloatTensor(noise_t)\n        noisev = Variable(noise_t)\n        fake = netG(noisev)\n        timg = fake[0]\n        timg = timg.data\n\n        timg.add_(1).div_(2)\n        ndimg = timg.mul(255).clamp(0, 255).byte().permute(1, 2, 0).numpy()\n        ndimgs_slerp.append(ndimg)\n\n        # lerp\n        z_lerp = (1 - t) * u + t * v\n\n        noise_t = z_lerp.reshape((1, nz, 1, 1))\n        noise_t = torch.FloatTensor(noise_t)\n        noisev = Variable(noise_t)\n        fake = netG(noisev)\n        timg = fake[0]\n        timg = timg.data\n\n        timg.add_(1).div_(2)\n        ndimg = timg.mul(255).clamp(0, 255).byte().permute(1, 2, 0).numpy()\n        ndimgs_lerp.append(ndimg)\n\n    print(\'exporting {} ...\'.format(epoch))\n\n    # export slerp result\n    ndimg = numpy.hstack(ndimgs_slerp)\n    im = Image.fromarray(ndimg)\n    im.save(\'e{:0>3d}-slerp.png\'.format(epoch))\n\n    # export lerp result\n    ndimg = numpy.hstack(ndimgs_lerp)\n    im = Image.fromarray(ndimg)\n    im.save(\'e{:0>3d}-lerp.png\'.format(epoch))\n'"
random_bonus/great-circle-interp/networks.py,5,"b'import torch\nimport torch.nn as nn\nimport torch.nn.parallel\nimport torch.backends.cudnn as cudnn\n\n\nclass NetG(nn.Module):\n    def __init__(self, ngf, nz, nc, ngpu):\n        super(NetG, self).__init__()\n        self.ngpu = ngpu\n        self.main = nn.Sequential(\n            # input is Z, going into a convolution\n            nn.ConvTranspose2d(nz, ngf * 8, 4, 1, 0, bias=False),\n            nn.BatchNorm2d(ngf * 8),\n            nn.ReLU(True),\n            # state size. (ngf*8) x 4 x 4\n            nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ngf * 4),\n            nn.ReLU(True),\n            # state size. (ngf*4) x 8 x 8\n            nn.ConvTranspose2d(ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ngf * 2),\n            nn.ReLU(True),\n            # state size. (ngf*2) x 16 x 16\n            nn.ConvTranspose2d(ngf * 2, ngf, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ngf),\n            nn.ReLU(True),\n            # state size. (ngf) x 32 x 32\n            nn.ConvTranspose2d(ngf, nc, 4, 2, 1, bias=False),\n            nn.Tanh()\n            # state size. (nc) x 64 x 64\n        )\n\n    def forward(self, input):\n        if isinstance(input.data, torch.cuda.FloatTensor) and self.ngpu > 1:\n            output = nn.parallel.data_parallel(self.main, input, range(self.ngpu))\n        else:\n            output = self.main(input)\n        return output\n\n\nclass NetD(nn.Module):\n    def __init__(self, ndf, nc, ngpu):\n        super(NetD, self).__init__()\n        self.ngpu = ngpu\n        self.main = nn.Sequential(\n            # input is (nc) x 64 x 64\n            nn.Conv2d(nc, ndf, 4, 2, 1, bias=False),\n            nn.LeakyReLU(0.2, inplace=True),\n            # state size. (ndf) x 32 x 32\n            nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ndf * 2),\n            nn.LeakyReLU(0.2, inplace=True),\n            # state size. (ndf*2) x 16 x 16\n            nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ndf * 4),\n            nn.LeakyReLU(0.2, inplace=True),\n            # state size. (ndf*4) x 8 x 8\n            nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ndf * 8),\n            nn.LeakyReLU(0.2, inplace=True),\n            # state size. (ndf*8) x 4 x 4\n            nn.Conv2d(ndf * 8, 1, 4, 1, 0, bias=False),\n            nn.Sigmoid()\n        )\n\n    def forward(self, input):\n        if isinstance(input.data, torch.cuda.FloatTensor) and self.ngpu > 1:\n            output = nn.parallel.data_parallel(self.main, input, range(self.ngpu))\n        else:\n            output = self.main(input)\n\n        return output.view(-1, 1).squeeze(1)\n'"
random_bonus/image-segmentation(updating)/argparser.py,0,"b'import os\nimport argparse\n\n\ndef parse_param_file(filepath):\n    with open(filepath, \'r\') as f:\n        kw_exprs = [x.strip() for x in f.readlines() if x.strip()]\n    return eval(\'dict({})\'.format(\',\'.join(kw_exprs)))\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser(\n        description=\'Simple Demo of Image Segmentation with U-Net\',\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n\n    # general options\n    parser.add_argument(\'mode\',\n                        help=\'train/test\')\n    parser.add_argument(\'dataroot\',\n                        help=\'Directory containing training images in ""images"" and ""segmentations"" or test images\')\n    parser.add_argument(\'config\',\n                        help=\'Path to config file\')\n    parser.add_argument(\'--cpu\',\n                        help=\'Set to CPU mode\', action=\'store_true\')\n    parser.add_argument(\'--output-dir\',\n                        help=\'Directory of output for both train/test\',\n                        type=str, default=\'\')\n\n    # test options\n    parser.add_argument(\'--model\',\n                        help=\'Path to pre-trained model\',\n                        type=str, default=\'\')\n\n    args = parser.parse_args()\n\n    params = {\n        # general params\n        \'network\': \'triangle\',\n        \'layers\': [32, 64, 128, 256, 512],\n        \'groups\': 1, \n        \'color_labels\': [], \n        \'image_width\': None,\n        \'image_height\': None\n    }\n\n    kwargs = parse_param_file(args.config)\n    \n    # other params specified in config file\n    if args.mode == \'train\':\n\n        # default: no augmentation, with batch-norm\n\n        train_params = {\n            # training params\n            \'optimizer\': \'SGD\',\n            \'lr_policy\': {0: 1e-4},\n            \'momentum\': 0.9,\n            \'nesterov\': True,\n            \'batch_norm\': True,\n            \'batch_size\': 4,\n            \'val_batch_size\': None,\n            \'epochs\': 24,\n            \'print_interval\': 50,\n            \'validation_interval\': 1000,\n            \'checkpoint_interval\': 10000,\n            \'random_horizontal_flip\': False,\n            \'random_square_crop\': False,\n            \'random_crop\': None,  # example: (0.81, 0.1), use 0.81 as area ratio, & 0.1 as the hw ratio variation\n            \'random_rotation\': 0,\n            \'img_dir\': \'images\',\n            \'seg_dir\': \'segmentations\',\n            \'regression\': False,\n        }\n\n        params.update(train_params)\n        if params[\'val_batch_size\'] is None:\n            params[\'val_batch_size\'] = params[\'batch_size\']\n\n    # update params from config\n    for k, v in kwargs.items():\n        if k in params:\n            params[k] = v\n\n    # set params to args\n    for k, v in params.items():\n        setattr(args, k, v)\n\n    args.dataroot = args.dataroot.rstrip(os.sep)\n\n    return args\n'"
random_bonus/image-segmentation(updating)/loss_visualizer.py,0,"b""import os\nimport sys\nimport numpy\nfrom matplotlib import pyplot\n\nLOG_FILENAME = 'log.txt'\nTRAIN_LOSS_KEYWORD = '| Training loss: '\nVAL_LOSS_KEYWORD = '| Validation loss: '\nITER_INDEX = 4\nLOSS_INDEX = -1\nMIOU_INDEX = -5\nMPA_INDEX = -9\n\n\ndef parse_log(filepath):\n    with open(filepath, 'r') as f:\n        train_curve = []\n        val_curve = []\n        line = f.readline()\n        while line:\n            if TRAIN_LOSS_KEYWORD in line or VAL_LOSS_KEYWORD in line:\n                tokens = line.split()\n                measure = [int(tokens[ITER_INDEX]), float(tokens[LOSS_INDEX])]\n                if TRAIN_LOSS_KEYWORD in line:\n                    train_curve.append(measure)\n                else:\n                    measure.extend([float(tokens[MPA_INDEX]), float(tokens[MIOU_INDEX])])\n                    val_curve.append(measure)\n\n            line = f.readline()\n    return train_curve, val_curve\n\nroot_dir = sys.argv[1].rstrip(os.sep)\nkeyword = sys.argv[2] if len(sys.argv) > 2 else None\n\ngroups = [x for x in os.listdir(root_dir) if os.path.isdir(x) and (True if keyword is None else (keyword in x))]\n\nfor group in groups:\n    log_path = os.sep.join([root_dir, group, LOG_FILENAME])\n    train_loss, val_loss = parse_log(log_path)\n    train_loss = numpy.array(train_loss)\n    val_loss = numpy.array(val_loss)\n    pyplot.figure('Train/Test Loss Curves')\n    pyplot.plot(train_loss[:, 0], train_loss[:, 1], label=group)\n    pyplot.plot(val_loss[:, 0], val_loss[:, 1], '--', label=group)\n    pyplot.figure('mPA/mIOU Curves')\n    pyplot.plot(val_loss[:, 0], val_loss[:, 2], label='{}-mPA'.format(group))\n    pyplot.plot(val_loss[:, 0], val_loss[:, 3], '--', label='{}-mIOU'.format(group))\n\npyplot.figure('Train/Test Loss Curves')\npyplot.legend(loc='upper right')\npyplot.figure('mPA/mIOU Curves')\npyplot.legend(loc='lower right')\npyplot.show()\n"""
random_bonus/image-segmentation(updating)/main.py,8,"b'import logging\nimport os\nimport numpy\nfrom PIL import Image\nimport torch\nimport torchvision\nimport torch.utils.data\nimport torch.nn as nn\nimport torch.functional as F\nfrom torch.autograd import Variable\nfrom argparser import parse_args\nimport utils\nimport networks\n\n\ndef train(args):\n    # set logger\n    logging_dir = args.output_dir if args.output_dir else \'train-{}\'.format(utils.get_datetime_string())\n    os.mkdir(\'{}\'.format(logging_dir))\n    logging.basicConfig(\n        level=logging.INFO,\n        filename=\'{}/log.txt\'.format(logging_dir),\n        format=\'%(asctime)s %(message)s\',\n        filemode=\'w\'\n    )\n\n    console = logging.StreamHandler()\n    console.setLevel(logging.INFO)\n    formatter = logging.Formatter(\'%(asctime)s %(message)s\')\n    console.setFormatter(formatter)\n    logging.getLogger(\'\').addHandler(console)\n\n    logging.info(\'=========== Taks {} started! ===========\'.format(args.output_dir))\n    for arg in vars(args):\n        logging.info(\'{}: {}\'.format(arg, getattr(args, arg)))\n    logging.info(\'========================================\')\n\n    # initialize loader\n    multi_scale = len(args.layers) if args.network != \'unet\' else 0\n    train_set = utils.SegmentationImageFolder(os.sep.join([args.dataroot, \'train\']),\n                                              image_folder=args.img_dir,\n                                              segmentation_folder=args.seg_dir,\n                                              labels=args.color_labels,\n                                              image_size=(args.image_width, args.image_height),\n                                              random_horizontal_flip=args.random_horizontal_flip,\n                                              random_rotation=args.random_rotation,\n                                              random_crop=args.random_crop,\n                                              random_square_crop=args.random_square_crop,\n                                              label_regr=args.regression,\n                                              multi_scale=multi_scale)\n    val_set = utils.SegmentationImageFolder(os.sep.join([args.dataroot, \'val\']),\n                                            image_folder=args.img_dir,\n                                            segmentation_folder=args.seg_dir,\n                                            labels=args.color_labels,\n                                            image_size=(args.image_width, args.image_height),\n                                            random_square_crop=args.random_square_crop,\n                                            label_regr=args.regression)\n    train_loader = torch.utils.data.DataLoader(train_set, batch_size=args.batch_size, shuffle=True)\n    val_loader = torch.utils.data.DataLoader(val_set, batch_size=args.val_batch_size)\n\n    # initialize model, input channels need to be calculated by hand\n    n_classes = len(args.color_labels)\n\n    if args.network == \'unet\':\n        network = networks.UNet\n        criterion = nn.MSELoss() if args.regression else utils.CrossEntropyLoss2D()\n    elif args.network == \'triangle\':\n        network = networks.TriangleNet\n        criterion = utils.MSCrossEntropyLoss2D([0.15]+[0.85/float(multi_scale)]*multi_scale)\n    else:\n        pass\n    val_criterion = utils.CrossEntropyLoss2D()\n\n    if args.regression:\n        model = network(args.layers, 3, 1, groups=args.groups)\n    else:\n        model = network(args.layers, 3, n_classes, groups=args.groups)\n    if not args.cpu:\n        model.cuda()\n\n    # train\n    iterations = 0\n    for epoch in range(args.epochs):\n        model.train()\n        # update lr according to lr policy\n        if epoch in args.lr_policy:\n            lr = args.lr_policy[epoch]\n            optimizer = utils.get_optimizer(args.optimizer, model.parameters(),\n                                            lr=lr, momentum=args.momentum, nesterov=args.nesterov)\n            if epoch > 0:\n                logging.info(\'| Learning Rate | Epoch: {: >3d} | Change learning rate to {}\'.format(epoch+1, lr))\n            else:\n                logging.info(\'| Learning Rate | Initial learning rate: {}\'.format(lr))\n\n        # iterate all samples\n        losses = utils.AverageMeter()\n        for i_batch, (img, seg) in enumerate(train_loader):\n\n            img = Variable(img)\n            seg = Variable(seg) if not multi_scale else [Variable(x) for x in seg]\n\n            if not args.cpu:\n                img = img.cuda()\n                seg = seg.cuda() if not multi_scale else [x.cuda() for x in seg]\n\n            # compute output\n            output = model(img)\n            loss = criterion(output, seg)\n            losses.update(loss.data[0])\n\n            # compute gradient and do SGD step\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n            # logging training curve\n            if iterations % args.print_interval == 0:\n                logging.info(\n                    \'| Iterations: {: >6d} \'\n                    \'| Epoch: {: >3d}/{: >3d} \'\n                    \'| Batch: {: >4d}/{: >4d} \'\n                    \'| Training loss: {:.6f}\'.format(\n                        iterations, \n                        epoch+1, args.epochs,\n                        i_batch, len(train_loader)-1,\n                        losses.avg\n                    )\n                )\n                losses = utils.AverageMeter()\n\n            # validation on all val samples\n            if iterations % args.validation_interval == 0:\n                model.eval()\n                val_losses = utils.AverageMeter()\n                gt_pixel_count = [0] * n_classes\n                pred_pixel_count = [0] * n_classes\n                intersection_pixel_count = [0] * n_classes\n                union_pixel_count = [0] * n_classes\n\n                for img, seg in val_loader:\n\n                    img = Variable(img)\n                    seg = Variable(seg)\n\n                    if not args.cpu:\n                        img = img.cuda()\n                        seg = seg.cuda()\n\n                    # compute output\n                    output = model(img)\n                    loss = val_criterion(output, seg)\n                    val_losses.update(loss.data[0], float(img.size(0))/float(args.batch_size))\n                    output_numpy = output.data.numpy() if args.cpu else output.data.cpu().numpy()\n                    pred_labels = numpy.argmax(output_numpy, axis=1)\n                    gt_labels = seg.data.numpy() if args.cpu else seg.data.cpu().numpy()\n\n                    pred_labels = pred_labels.flatten()\n                    gt_labels = gt_labels.flatten()\n\n                    for i in range(n_classes):\n                        pred_pixel_count[i] += (pred_labels == i).sum()\n                        gt_pixel_count[i] += (gt_labels == i).sum()\n                        gt_dumb = numpy.full(gt_labels.shape, -1, dtype=numpy.int)\n                        pred_dumb = numpy.full(pred_labels.shape, -2, dtype=numpy.int)\n                        gt_dumb[gt_labels == i] = 0\n                        pred_dumb[pred_labels == i] = 0\n                        intersection_pixel_count[i] += (gt_dumb == pred_dumb).sum()\n                        pred_dumb[gt_labels == i] = 0\n                        union_pixel_count[i] += (pred_dumb == 0).sum()\n\n                # calculate mPA & mIOU\n                mPA = 0\n                mIOU = 0\n                for i in range(n_classes):\n                    mPA += float(intersection_pixel_count[i]) / float(gt_pixel_count[i])\n                    mIOU += float(intersection_pixel_count[i]) / float(union_pixel_count[i])\n                mPA /= float(n_classes)\n                mIOU /= float(n_classes)\n\n                logging.info(\n                    \'| Iterations: {: >6d} \'\n                    \'| Epoch: {: >3d}/{: >3d} \'\n                    \'| Average mPA: {:.4f} \'\n                    \'| Average mIOU: {:.4f} \'\n                    \'| Validation loss: {:.6f} \'.format(\n                        iterations, \n                        epoch+1, args.epochs,\n                        mPA,\n                        mIOU,\n                        val_losses.avg\n                    )\n                )\n\n                model.train()\n\n            if iterations % args.checkpoint_interval == 0 and iterations > 0:\n                model_weights_path = \'{}/iterations-{:0>6d}-epoch-{:0>3d}.pth\'.format(logging_dir, iterations, epoch+1)\n                torch.save(model.state_dict(), model_weights_path)\n                logging.info(\'| Checkpoint | {} is saved!\'.format(model_weights_path))\n\n            iterations += 1\n\n\ndef test(args):\n    if not args.model:\n        print(\'Need a pretrained model!\')\n        return\n\n    if not args.color_labels:\n        print(\'Need to specify color labels\')\n        return\n\n    resize_img = False if args.image_width is None or args.image_height is None else True\n\n    # check if output dir exists\n    output_dir = args.output_dir if args.output_dir else \'test-{}\'.format(utils.get_datetime_string())\n    if not os.path.exists(output_dir):\n        os.mkdir(output_dir)\n\n    # load model\n    if args.network == \'unet\':\n        network = networks.UNet\n    elif args.network == \'triangle\':\n        network = networks.TriangleNet\n    else:\n        pass\n\n    model = network(args.unet_layers, 3, len(args.color_labels), groups=args.groups)\n    model.load_state_dict(torch.load(args.model))\n    model = model.eval()\n\n    if not args.cpu:\n        model.cuda()\n\n    # iterate all images with one by one\n    transform = torchvision.transforms.ToTensor()\n    for filename in [x for x in os.listdir(args.dataroot)]:\n        filepath = os.sep.join([args.dataroot, filename])\n        with open(filepath, \'r\') as f:\n            img = Image.open(f)\n            img = img.resize((args.image_width, args.image_height))\n            img = transform(img)\n            img = img.view(1, *img.shape)\n            img = Variable(img)\n        if not args.cpu:\n            img = img.cuda()\n        output = model(img)\n        _, c, h, w = output.data.shape\n        output_numpy = output.data.numpy()[0] if args.cpu else output.data.cpu().numpy()[0]\n        output_argmax = numpy.argmax(output_numpy, axis=0)\n        out_img = numpy.zeros((h, w, 3), dtype=numpy.uint8)\n        for i, color in enumerate(args.color_labels):\n            out_img[output_argmax == i] = numpy.array(args.color_labels[i], dtype=numpy.uint8)\n        out_img = Image.fromarray(out_img)\n        seg_filepath = os.sep.join([output_dir, filename[:filename.rfind(\'.\')]+\'.png\'])\n        out_img.save(seg_filepath)\n        print(\'{} is exported!\'.format(seg_filepath))\n\n\nif __name__ == \'__main__\':\n\n    args = parse_args()\n    if args.mode == \'train\':\n        if args.image_width is None or args.image_height is None:\n            print(\'Image size must be specified for training!\')\n            exit(0)\n        train(args)\n    elif args.mode == \'test\':\n        test(args)\n    else:\n        print(\'Wrong input! Please specify ""train"" or ""test""\')\n\n'"
random_bonus/image-segmentation(updating)/networks.py,5,"b""import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torchvision.models import resnet\nfrom torchvision.models.resnet import conv3x3\n\n\nclass UNetConvBlock(nn.Module):\n    def __init__(self, input_nch, output_nch, kernel_size=3, activation=F.leaky_relu, use_bn=True, same_conv=True):\n        super(UNetConvBlock, self).__init__()\n        padding = kernel_size // 2 if same_conv else 0  # only support odd kernel\n        self.conv0 = nn.Conv2d(input_nch, output_nch, kernel_size, padding=padding)\n        self.conv1 = nn.Conv2d(output_nch, output_nch, kernel_size, padding=padding)\n        self.act = activation\n        self.batch_norm = nn.BatchNorm2d(output_nch) if use_bn else None\n\n    def forward(self, x):\n        x = self.conv0(x)\n        if self.batch_norm:\n            x = self.batch_norm(x)\n        x = self.act(x)\n        x = self.conv1(x)\n        if self.batch_norm:\n            x = self.batch_norm(x)\n        return self.act(x)\n\n\nclass UNet(nn.Module):\n    def __init__(self, conv_channels, input_nch=3, output_nch=2, use_bn=True):\n        super(UNet, self).__init__()\n        self.n_stages = len(conv_channels)\n        # define convolution blocks\n        down_convs = []\n        up_convs = []\n\n        self.max_pooling = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n\n        in_nch = input_nch\n        for i, out_nch in enumerate(conv_channels):\n            down_convs.append(UNetConvBlock(in_nch, out_nch, use_bn=use_bn))\n            up_conv_in_ch = 2 * out_nch if i < self.n_stages - 1 else out_nch # first up conv with equal channels\n            up_conv_out_ch = out_nch if i == 0 else in_nch  # last up conv with channels equal to labels\n            up_convs.insert(0, UNetConvBlock(up_conv_in_ch, up_conv_out_ch, use_bn=use_bn))\n            in_nch = out_nch\n\n        self.down_convs = nn.ModuleList(down_convs)\n        self.up_convs = nn.ModuleList(up_convs)\n\n        # define output convolution\n        self.out_conv = nn.Conv2d(conv_channels[0], output_nch, 1)\n\n    def forward(self, x):\n        # conv & downsampling\n        down_sampled_fmaps = []\n        for i in range(self.n_stages-1):\n            x = self.down_convs[i](x)\n            x = self.max_pooling(x)\n            down_sampled_fmaps.insert(0, x)\n\n        # center convs\n        x = self.down_convs[self.n_stages-1](x)\n        x = self.up_convs[0](x)\n\n        # conv & upsampling\n        for i, down_sampled_fmap in enumerate(down_sampled_fmaps):\n            x = torch.cat([x, down_sampled_fmap], 1)\n            x = self.up_convs[i+1](x)\n            x = F.upsample(x, scale_factor=2, mode='bilinear')\n\n        return self.out_conv(x)\n        #x = self.out_conv(x)\n        #return x if self.out_conv.out_channels == 1 else F.relu(x)\n\n\nclass BasicResBlock(nn.Module):\n\n    def __init__(self, input_nch, output_nch, groups=1):\n        super(BasicResBlock, self).__init__()\n        self.transform_conv = nn.Conv2d(input_nch, output_nch, 1)\n        self.bn1 = nn.BatchNorm2d(output_nch)\n        self.conv1 = nn.Conv2d(output_nch, output_nch, 3, padding=1, groups=groups, bias=False)\n        self.bn2 = nn.BatchNorm2d(output_nch)\n        self.conv2 = nn.Conv2d(output_nch, output_nch, 3, padding=1, groups=groups, bias=False)\n        self.act = nn.LeakyReLU(inplace=True)\n\n    def forward(self, x):\n        x = self.transform_conv(x)\n        residual = x\n\n        out = self.bn1(x)\n        out = self.act(out)\n        out = self.conv1(out)\n\n        out = self.bn2(out)\n        out = self.act(out)\n        out = self.conv2(out)\n\n        out += residual\n\n        return out\n\n\nclass TriangleNet(nn.Module):\n    def __init__(self, conv_channels, input_nch, output_nch, groups=1):\n        super(TriangleNet, self).__init__()\n        self.input_nch = input_nch\n        self.output_nch = output_nch\n        self.pyramid_height = len(conv_channels)\n\n        blocks = [list() for _ in range(self.pyramid_height)]\n        for i in range(self.pyramid_height):\n            for j in range(i, self.pyramid_height):\n                if i == 0 and j == 0:\n                    blocks[i].append(BasicResBlock(input_nch, conv_channels[j], groups=groups))\n                else:\n                    blocks[i].append(BasicResBlock(conv_channels[j-1], conv_channels[j], groups=groups))\n\n        for i in range(self.pyramid_height):\n            blocks[i] = nn.ModuleList(blocks[i])\n        self.blocks = nn.ModuleList(blocks)\n\n        self.down_sample = nn.MaxPool2d(3, 2, 1)\n        self.up_samples = nn.ModuleList([nn.Upsample(scale_factor=2**i, mode='bilinear') for i in range(1, self.pyramid_height)])\n\n        self.channel_out_convs = nn.ModuleList([nn.Conv2d(conv_channels[-1], output_nch, 1) for _ in range(self.pyramid_height)])\n        self.out_conv = nn.Conv2d(self.pyramid_height * conv_channels[-1], output_nch, 1)\n\n    def forward(self, x):\n        # forward & expand\n        x = [self.blocks[0][0](x)]\n        for i in range(1, self.pyramid_height):\n            x.append(self.down_sample(x[-1]))\n            for j in range(i+1):\n                x[j] = self.blocks[j][i-j](x[j])\n\n        # upsampling & conv\n        if self.training:\n            ms_out = [self.channel_out_convs[i](x[i]) for i in range(self.pyramid_height)]\n        x = [x[0]] + [self.up_samples[i-1](x[i]) for i in range(1, self.pyramid_height)]\n\n        # final 1x1 conv\n        out = self.out_conv(torch.cat(x, 1))\n        return [out] + ms_out if self.training else out\n\n\nclass PSPTriangleNet(nn.Module):\n    def __init__(self, conv_channels, input_nch, output_nch, groups):\n        super(PSPTriangleNet, self).__init__()\n        self.input_nch = input_nch\n        self.output_nch = output_nch\n        self.pyramid_height = len(conv_channels)\n\n        blocks = []\n        for i in range(self.pyramid_height-1):\n            if i == 0:\n                blocks.append(BasicResBlock(input_nch, conv_channels[i], groups=groups))\n            else:\n                blocks.append(BasicResBlock(conv_channels[i-1], conv_channels[i], groups=groups))\n\n        ms_blocks = []\n        for i in range(self.pyramid_height):\n            ms_blocks.append(BasicResBlock(conv_channels[-2], conv_channels[-1]//self.pyramid_height))\n        self.blocks = nn.ModuleList(blocks)\n        self.ms_blocks = nn.ModuleList(ms_blocks)\n\n        self.down_samples = nn.ModuleList([nn.MaxPool2d(2**i+1, 2**i, 2**(i-1)) for i in range(1, self.pyramid_height)])\n        self.up_samples = nn.ModuleList([nn.Upsample(scale_factor=2**i, mode='bilinear') for i in range(1, self.pyramid_height)])\n\n        self.channel_out_convs = nn.ModuleList([nn.Conv2d(conv_channels[-1]//self.pyramid_height, output_nch, 1) for _ in range(self.pyramid_height)])\n        self.out_conv = nn.Conv2d(conv_channels[-1], output_nch, 1)\n\n    def forward(self, x):\n        # forward & expand\n        for i in range(self.pyramid_height-1):\n            x = self.blocks[i](x)\n        x = [self.ms_blocks[0](x)] + [self.down_samples[i](self.ms_blocks[i](x)) for i in range(self.pyramid_height-1)]\n\n        # upsampling & conv\n        if self.training:\n            ms_out = [self.channel_out_convs[i](x[i]) for i in range(self.pyramid_height)]\n        x = [x[0]] + [self.up_samples[i-1](x[i]) for i in range(1, self.pyramid_height)]\n\n        # final 1x1 conv\n        out = self.out_conv(torch.cat(x, 1))\n        return [out] + ms_out if self.training else out\n\n"""
random_bonus/image-segmentation(updating)/utils.py,5,"b'from datetime import datetime\nimport random\nimport numpy\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision\nfrom torchvision.datasets.folder import *\nfrom torch.optim import SGD, Adadelta, Adam, Adagrad, RMSprop, ASGD\nimport cv2\n\nOPTIMIZERS = {\n    \'sgd\': SGD,\n    \'adadelta\': Adadelta,\n    \'adam\': Adam,\n    \'adagrad\': Adagrad,\n    \'rmsprop\': RMSprop,\n    \'asgd\': ASGD\n}\n\n\nclass SegmentationImageFolder(ImageFolder):\n    """"""A simplified segmentation data loader where the images are arranged in this way: ::\n\n        root/images/001.png\n        root/images/002.png\n        root/images/003.png\n\n        root/segmentations/001.png\n        root/segmentations/002.png\n        root/segmentations/003.png\n\n        images in the two folder should be corresponding, sorting by name\n\n    Args:\n        please refer to\n        https://github.com/frombeijingwithlove/dlcv_for_beginners/blob/master/chap6/data_augmentation/image_augmentation.py\n    """"""\n\n    def __init__(self, root,\n                 image_folder=\'images\', segmentation_folder=\'segmentations\',\n                 labels=[(0, 0, 0), (255, 255, 255)],\n                 image_size=None,\n                 random_horizontal_flip=False,\n                 random_rotation=0,\n                 random_crop=None,\n                 random_square_crop=False,\n                 loader=default_loader,\n                 label_regr=False,\n                 multi_scale=0):\n        super(SegmentationImageFolder, self).__init__(root, loader=loader)\n        pair_len = len(self.imgs) // 2\n        assert image_folder in self.classes and segmentation_folder in self.classes\n        if image_folder < segmentation_folder:\n            self.imgs = [(self.imgs[i][0], self.imgs[i+pair_len][0]) for i in range(pair_len)]\n        else:\n            self.imgs = [(self.imgs[i+pair_len][0], self.imgs[i][0]) for i in range(pair_len)]\n        self.img_folder = image_folder\n        self.seg_folder = segmentation_folder\n        self.labels = [numpy.array(x, dtype=numpy.uint8) for x in labels]\n        self.image_size = image_size\n        self.flip_lr = random_horizontal_flip\n        self.random_rotation = random_rotation\n        self.random_crop = random_crop\n        self.random_square_crop = random_square_crop\n        self.label_regr=label_regr\n        self.multi_scale=multi_scale\n\n    def __getitem__(self, index):\n        """"""\n        Args:\n            index (int): Index\n\n        Returns:\n            tuple: (image, target) where target is class_index of the target class.\n        """"""\n        imgpath, segpath = self.imgs[index]\n        img = self.loader(imgpath)\n        seg = self.loader(segpath)\n\n        # manually transform to incorporate horizontal flip & one-hot coding for segmentation labels\n        if self.random_rotation:\n            w, h = img.size\n            angle = self.random_rotation % 360\n            img = img.rotate(angle)\n            seg = seg.rotate(angle)\n\n            angle_crop = angle % 180\n            if angle_crop > 90:\n                angle_crop = 180 - angle_crop\n            theta = angle_crop * numpy.pi / 180.0\n            hw_ratio = float(h) / float(w)\n            tan_theta = numpy.tan(theta)\n            numerator = numpy.cos(theta) + numpy.sin(theta) * tan_theta\n            r = hw_ratio if h > w else 1 / hw_ratio\n            denominator = r * tan_theta + 1\n            crop_mult = numerator / denominator\n            w_crop = int(round(crop_mult * w))\n            h_crop = int(round(crop_mult * h))\n            x0 = int((w - w_crop) / 2)\n            y0 = int((h - h_crop) / 2)\n\n            img = img.crop((x0, y0, x0+w_crop, y0+h_crop))\n            seg = seg.crop((x0, y0, x0+w_crop, y0+h_crop))\n\n        if self.random_crop:\n            area_ratio, hw_vari = self.random_crop\n            w, h = img.size\n            hw_delta = numpy.random.uniform(-hw_vari, hw_vari)\n            hw_mult = 1 + hw_delta\n            w_crop = int(round(w * numpy.sqrt(area_ratio * hw_mult)))\n            if w_crop > w - 2:\n                w_crop = w - 2\n            h_crop = int(round(h * numpy.sqrt(area_ratio / hw_mult)))\n            if h_crop > h - 2:\n                h_crop = h - 2\n            x0 = numpy.random.randint(0, w - w_crop - 1)\n            y0 = numpy.random.randint(0, h - h_crop - 1)\n            img = img.crop((x0, y0, x0+w_crop, y0+h_crop))\n            seg = seg.crop((x0, y0, x0+w_crop, y0+h_crop))\n\n        if self.random_square_crop:\n            w, h = img.size\n            if w > h:\n                x0 = random.randint(0, w-h-1)\n                img = img.crop((x0, 0, x0+h, h))\n                seg = seg.crop((x0, 0, x0+h, h))\n            elif w < h:\n                y0 = random.randint(0, h-w-1)\n                img = img.crop((0, y0, w, y0+w))\n                seg = seg.crop((0, y0, w, y0+w))\n\n        if self.image_size:\n            img = img.resize(self.image_size)\n            seg = seg.resize(self.image_size, Image.NEAREST)\n\n        # random horizontal flip\n        if random.random() > 0.5:\n            img = img.transpose(Image.FLIP_LEFT_RIGHT)\n            seg = seg.transpose(Image.FLIP_LEFT_RIGHT)\n\n        # one-hot coding for segmentation labels\n        seg_arr = numpy.array(seg)\n        seg = numpy.zeros(seg_arr.shape[:2], dtype=numpy.int64)\n        for i, label_color in enumerate(self.labels):\n            label_indices = numpy.where(seg_arr == label_color)[:2]\n            seg[label_indices[0], label_indices[1]] = i\n\n        if self.multi_scale:\n            h, w = seg.shape\n            seg = [seg] + [cv2.resize(seg, (w//(2**i), h//(2**i)), interpolation=cv2.INTER_NEAREST).astype(numpy.int64) for i in range(1, self.multi_scale)]\n\n        # to tensor\n        transform = torchvision.transforms.Compose([\n            torchvision.transforms.ToTensor(),\n            torchvision.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n        ])\n        img = transform(img)\n        if self.multi_scale:\n            seg = [torch.Tensor(x) if self.label_regr else torch.LongTensor(x) for x in seg]\n        else:\n            seg = torch.Tensor(seg) if self.label_regr else torch.LongTensor(seg)\n\n        return img, seg\n\n    def __len__(self):\n        return len(self.imgs)\n\n\nclass CrossEntropyLoss2D(nn.Module):\n    def __init__(self, size_average=True):\n        super(CrossEntropyLoss2D, self).__init__()\n        self.nll_loss_2d = nn.NLLLoss2d(size_average=size_average)\n\n    def forward(self, outputs, targets):\n        return self.nll_loss_2d(F.log_softmax(outputs), targets)\n\n\nclass MSCrossEntropyLoss2D(nn.Module):\n    def __init__(self, weights, size_average=True):\n        super(MSCrossEntropyLoss2D, self).__init__()\n        self.nll_loss_2d = nn.NLLLoss2d(size_average=size_average)\n        self.weights = weights\n\n    def forward(self, outputs, targets):\n        loss = self.weights[0] * self.nll_loss_2d(F.log_softmax(outputs[0]), targets[0])\n        for i in range(len(self.weights)-1):\n            loss += self.weights[i+1] * self.nll_loss_2d(F.log_softmax(outputs[i+1]), targets[i])\n        return loss\n\n\ndef get_datetime_string():\n    datetime_now = datetime.now()\n    return \'{}-{}-{}-{}-{}-{}\'.format(\n        datetime_now.year,\n        datetime_now.month,\n        datetime_now.day,\n        datetime_now.hour,\n        datetime_now.minute,\n        datetime_now.second\n    )\n\n\n# borrowed from \n# https://github.com/pytorch/examples/tree/master/imagenet\nclass AverageMeter(object):\n    """"""Computes and stores the average and current value""""""\n    def __init__(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count\n\n\ndef get_optimizer(name, model_params, **kwargs):\n    name = name.lower()\n    if name == \'sgd\':\n        optimizer = OPTIMIZERS[name](\n            model_params,\n            lr=kwargs[\'lr\'],\n            momentum=kwargs[\'momentum\'],\n            nesterov=kwargs[\'nesterov\']\n        )\n    elif name in [\'adadelta\', \'adam\', \'adagrad\', \'asgd\']:\n        optimizer = OPTIMIZERS[name](model_params, lr=kwargs[\'lr\'])\n    elif name == \'rmsprop\':\n        optimizer = OPTIMIZERS[name](\n            model_params,\n            lr=kwargs[\'lr\'],\n            momentum=kwargs[\'momentum\'],\n        )\n    else:\n        raise Exception(\'Not supported optimizer!\')\n\n    return optimizer\n'"
random_bonus/multiple_models_fusion_caffe/convert_mnist.py,0,"b""import os\r\nimport pickle, gzip\r\nfrom matplotlib import pyplot\r\n\r\n# Load the dataset\r\nprint('Loading data from mnist.pkl.gz ...')\r\nwith gzip.open('mnist.pkl.gz', 'rb') as f:\r\n    train_set, valid_set, test_set = pickle.load(f)\r\n\r\nimgs_dir = 'mnist'\r\nos.system('mkdir -p {}'.format(imgs_dir))\r\ndatasets = {'train': train_set, 'val': valid_set, 'test': test_set}\r\nfor dataname, dataset in datasets.items():\r\n    print('Converting {} dataset ...'.format(dataname))\r\n    data_dir = os.sep.join([imgs_dir, dataname])\r\n    os.system('mkdir -p {}'.format(data_dir))\r\n    for i, (img, label) in enumerate(zip(*dataset)):\r\n        filename = '{:0>6d}_{}.jpg'.format(i, label)\r\n        filepath = os.sep.join([data_dir, filename])\r\n        img = img.reshape((28, 28))\r\n        pyplot.imsave(filepath, img, cmap='gray')\r\n        if (i+1) % 10000 == 0:\r\n            print('{} images converted!'.format(i+1))\r\n\r\n"""
random_bonus/multiple_models_fusion_caffe/fuse_model.py,0,"b""import sys\nsys.path.append('/path/to/caffe/python')\nimport caffe\n\nfusion_net = caffe.Net('lenet_fusion_train_val.prototxt', caffe.TEST)\n\nmodel_list = [\n    ('even', 'lenet_even_train_val.prototxt', 'mnist_lenet_even_iter_30000.caffemodel'),\n    ('odd', 'lenet_odd_train_val.prototxt', 'mnist_lenet_odd_iter_30000.caffemodel')\n]\n\nfor prefix, model_def, model_weight in model_list:\n    net = caffe.Net(model_def, model_weight, caffe.TEST)\n\n    for layer_name, param in net.params.iteritems():\n        n_params = len(param)\n        try:\n            for i in range(n_params):\n                fusion_net.params['{}/{}'.format(prefix, layer_name)][i].data[...] = param[i].data[...]\n        except Exception as e:\n            print(e)\n\nfusion_net.save('init_fusion.caffemodel')\n"""
random_bonus/multiple_models_fusion_caffe/gen_img_list.py,0,"b""import os\nimport sys\n\nmnist_path = 'mnist'\ndata_sets = ['train', 'val']\n\nfor data_set in data_sets:\n    odd_list = '{}_odd.txt'.format(data_set)\n    even_list = '{}_even.txt'.format(data_set)\n    all_list = '{}_all.txt'.format(data_set)\n    root = os.sep.join([mnist_path, data_set])\n    filenames = os.listdir(root)\n    with open(odd_list, 'w') as f_odd, open(even_list, 'w') as f_even, open(all_list, 'w') as f_all:\n        for filename in filenames:\n            filepath = os.sep.join([root, filename])\n            label = int(filename[:filename.rfind('.')].split('_')[1])\n            line = '{} {}\\n'.format(filepath, label)\n            f_all.write(line)\n\n            line = '{} {}\\n'.format(filepath, int(label/2))\n            if label % 2:\n                f_odd.write(line)\n            else:\n                f_even.write(line)\n"""
random_bonus/multiple_models_fusion_caffe/rename_n_freeze_layers.py,0,"b'import sys\nimport re\n\nlayer_name_regex = re.compile(\'name:\\s*""(.*?)""\')\nlr_mult_regex = re.compile(\'lr_mult:\\s*\\d+\\.*\\d*\')\n\ninput_filepath = sys.argv[1]\noutput_filepath = sys.argv[2]\nprefix = sys.argv[3]\n\nwith open(input_filepath, \'r\') as fr, open(output_filepath, \'w\') as fw:\n    prototxt = fr.read()\n    layer_names = set(layer_name_regex.findall(prototxt))\n    for layer_name in layer_names:\n        prototxt = prototxt.replace(layer_name, \'{}/{}\'.format(prefix, layer_name))\n\n    lr_mult_statements = set(lr_mult_regex.findall(prototxt))\n    for lr_mult_statement in lr_mult_statements:\n        prototxt = prototxt.replace(lr_mult_statement, \'lr_mult: 0\')\n\n    fw.write(prototxt)\n'"
