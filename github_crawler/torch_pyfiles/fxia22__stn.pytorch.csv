file_path,api_count,code
script/build.py,2,"b""import os\nimport torch\nfrom torch.utils.ffi import create_extension\n\n#this_file = os.path.dirname(__file__)\n\nsources = ['src/my_lib.c']\nheaders = ['src/my_lib.h']\ndefines = []\nwith_cuda = False\n\nif torch.cuda.is_available():\n    print('Including CUDA code.')\n    sources += ['src/my_lib_cuda.c']\n    headers += ['src/my_lib_cuda.h']\n    defines += [('WITH_CUDA', None)]\n    with_cuda = True\n\nthis_file = os.path.dirname(os.path.realpath(__file__))\nprint(this_file)\nextra_objects = ['src/my_lib_cuda_kernel.cu.o']\nextra_objects = [os.path.join(this_file, fname) for fname in extra_objects]\n\nffi = create_extension(\n    '_ext.my_lib',\n    headers=headers,\n    sources=sources,\n    define_macros=defines,\n    relative_to=__file__,\n    with_cuda=with_cuda,\n    extra_objects=extra_objects\n)\n\nif __name__ == '__main__':\n    ffi.build()\n"""
script/test.py,7,"b""from __future__ import print_function\n\nimport torch\nimport numpy as np\nimport torch.nn as nn\nfrom torch.autograd import Variable\n\nfrom modules.stn import STN\nfrom modules.gridgen import AffineGridGen, CylinderGridGen, CylinderGridGenV2, DenseAffine3DGridGen, DenseAffine3DGridGen_rotate\n\nimport time\n\nnframes = 64\nheight = 64\nwidth = 128\nchannels = 64\n\ninputImages = torch.zeros(nframes, height, width, channels)\ngrids = torch.zeros(nframes, height, width, 2)\n\ninput1, input2 = Variable(inputImages, requires_grad=True), Variable(grids, requires_grad=True)\n\ninput1.data.uniform_()\ninput2.data.uniform_(-1,1)\n\ninput = Variable(torch.from_numpy(np.array([[[0.8, 0.3, 1], [0.5, 0, 0]]], dtype=np.float32)), requires_grad = True)\nprint(input)\n\ng = AffineGridGen(64, 128, aux_loss = True)\nout, aux = g(input)\nprint((out.size()))\nout.backward(out.data)\nprint(input.grad.size())\n\n#print input2.data\ns = STN()\nstart = time.time()\nout = s(input1, input2)\nprint(out.size(), 'time:', time.time() - start)\nstart = time.time()\nout.backward(input1.data)\nprint(input1.grad.size(), 'time:', time.time() - start)\n\nwith torch.cuda.device(3):\n    input1 = input1.cuda()\n    input2 = input2.cuda()\n    start = time.time()\n    out = s(input1, input2)\n    print(out.size(), 'time:', time.time() - start)\n    start = time.time()\n    out.backward(input1.data.cuda())\n    print('time:', time.time() - start)\n\ns2 = STN(layout = 'BCHW')\ninput1, input2 = Variable(inputImages.transpose(2,3).transpose(1,2), requires_grad=True), Variable(grids.transpose(2,3).transpose(1,2), requires_grad=True)\ninput1.data.uniform_()\ninput2.data.uniform_(-1,1)\nstart = time.time()\nout = s2(input1, input2)\nprint(out.size(), 'time:', time.time() - start)\nstart = time.time()\nout.backward(input1.data)\nprint(input1.grad.size(), 'time:', time.time() - start)\n\nwith torch.cuda.device(1):\n    input1 = input1.cuda()\n    input2 = input2.cuda()\n    start = time.time()\n    out = s2(input1, input2)\n    print(out.size(), 'time:', time.time() - start)\n    start = time.time()\n    out.backward(input1.data.cuda())\n    print('time:', time.time() - start)\n\n"""
script/functions/__init__.py,0,b''
script/functions/gridgen.py,14,"b""# functions/add.py\nimport torch\nfrom torch.autograd import Function\nimport numpy as np\n\n\nclass AffineGridGenFunction(Function):\n    def __init__(self, height, width,lr=1):\n        super(AffineGridGenFunction, self).__init__()\n        self.lr = lr\n        self.height, self.width = height, width\n        self.grid = np.zeros( [self.height, self.width, 3], dtype=np.float32)\n        self.grid[:,:,0] = np.expand_dims(np.repeat(np.expand_dims(np.arange(-1, 1, 2.0/self.height), 0), repeats = self.width, axis = 0).T, 0)\n        self.grid[:,:,1] = np.expand_dims(np.repeat(np.expand_dims(np.arange(-1, 1, 2.0/self.width), 0), repeats = self.height, axis = 0), 0)\n        self.grid[:,:,2] = np.ones([self.height, width])\n        self.grid = torch.from_numpy(self.grid.astype(np.float32))\n        #print(self.grid)\n\n    def forward(self, input1):\n        self.input1 = input1\n        output = torch.zeros(torch.Size([input1.size(0)]) + self.grid.size())\n        self.batchgrid = torch.zeros(torch.Size([input1.size(0)]) + self.grid.size())\n        for i in range(input1.size(0)):\n            self.batchgrid[i] = self.grid\n\n        if input1.is_cuda:\n            self.batchgrid = self.batchgrid.cuda()\n            output = output.cuda()\n\n        for i in range(input1.size(0)):\n                output = torch.bmm(self.batchgrid.view(-1, self.height*self.width, 3), torch.transpose(input1, 1, 2)).view(-1, self.height, self.width, 2)\n\n        return output\n\n    def backward(self, grad_output):\n\n        grad_input1 = torch.zeros(self.input1.size())\n\n        if grad_output.is_cuda:\n            self.batchgrid = self.batchgrid.cuda()\n            grad_input1 = grad_input1.cuda()\n            #print('gradout:',grad_output.size())\n        grad_input1 = torch.baddbmm(grad_input1, torch.transpose(grad_output.view(-1, self.height*self.width, 2), 1,2), self.batchgrid.view(-1, self.height*self.width, 3))\n\n        #print(grad_input1)\n        return grad_input1\n\nclass CylinderGridGenFunction(Function):\n    def __init__(self, height, width,lr=1):\n        super(CylinderGridGenFunction, self).__init__()\n        self.lr = lr\n        self.height, self.width = height, width\n        self.grid = np.zeros( [self.height, self.width, 3], dtype=np.float32)\n        self.grid[:,:,0] = np.expand_dims(np.repeat(np.expand_dims(np.arange(-1, 1, 2.0/self.height), 0), repeats = self.width, axis = 0).T, 0)\n        self.grid[:,:,1] = np.expand_dims(np.repeat(np.expand_dims(np.arange(-1, 1, 2.0/self.width), 0), repeats = self.height, axis = 0), 0)\n        self.grid[:,:,2] = np.ones([self.height, width])\n        self.grid = torch.from_numpy(self.grid.astype(np.float32))\n        #print self.grid\n\n    def forward(self, input1):\n        self.input1 = (1+torch.cos(input1))/2\n\n        output = torch.zeros(torch.Size([input1.size(0), self.height, self.width, 2]) )\n\n        if not self.input1.is_cuda:\n            for i in range(self.input1.size(0)):\n\n\n                x = self.input1[i][0]\n                low = int(np.ceil(self.width*self.input1[i][0]))\n                frac =  self.width*self.input1[i][0] - low\n                interp =  frac * 2 * (1-x) + (1-frac) * 2 * (-x)\n\n                output[i,:,:,1] = torch.zeros(self.grid[:,:,1].size())\n                if low <= self.width and low > 0:\n                    output[i,:,:low,1].fill_(2*(1-x))\n\n                if low < self.width and low >= 0:\n                    output[i,:,low:,1].fill_(2*(-x))\n\n                output[i,:,:,1] = output[i,:,:,1] + self.grid[:,:,1]\n                output[i,:,:,0] = self.grid[:,:,0]\n        else:\n            print('not implemented')\n        return output\n\n    def backward(self, grad_output):\n        grad_input1 = torch.zeros(self.input1.size())\n        if not grad_output.is_cuda:\n            for i in range(self.input1.size(0)):\n                #print(torch.sum(grad_output[i,:,:,1],1).size())\n                grad_input1[i] = -torch.sum(torch.sum(grad_output[i,:,:,1],1)) * torch.sin(self.input1[i]) / 2\n        else:\n            print('not implemented')\n        return grad_input1 * self.lr\n"""
script/functions/stn.py,11,"b'# functions/add.py\nimport torch\nfrom torch.autograd import Function\nfrom _ext import my_lib\nfrom cffi import FFI\nffi = FFI()\n\nclass STNFunction(Function):\n    def forward(self, input1, input2):\n        self.input1 = input1\n        self.input2 = input2\n        self.device_c = ffi.new(""int *"")\n        output = torch.zeros(input1.size()[0], input2.size()[1], input2.size()[2], input1.size()[3])\n        #print(\'decice %d\' % torch.cuda.current_device())\n        if input1.is_cuda:\n            self.device = torch.cuda.current_device()\n        else:\n            self.device = -1\n        self.device_c[0] = self.device\n        if not input1.is_cuda:\n            my_lib.BilinearSamplerBHWD_updateOutput(input1, input2, output)\n        else:\n            output = output.cuda(self.device)\n            my_lib.BilinearSamplerBHWD_updateOutput_cuda(input1, input2, output, self.device_c)\n        return output\n\n    def backward(self, grad_output):\n        grad_input1 = torch.zeros(self.input1.size())\n        grad_input2 = torch.zeros(self.input2.size())\n        #print(\'backward decice %d\' % self.device)\n        if not grad_output.is_cuda:\n            my_lib.BilinearSamplerBHWD_updateGradInput(self.input1, self.input2, grad_input1, grad_input2, grad_output)\n        else:\n            grad_input1 = grad_input1.cuda(self.device)\n            grad_input2 = grad_input2.cuda(self.device)\n            my_lib.BilinearSamplerBHWD_updateGradInput_cuda(self.input1, self.input2, grad_input1, grad_input2, grad_output, self.device_c)\n        return grad_input1, grad_input2\n\n\n\nclass STNFunctionBCHW(Function):\n    def forward(self, input1, input2):\n        self.input1 = input1\n        self.input2 = input2\n        self.device_c = ffi.new(""int *"")\n        output = torch.zeros(input1.size()[0], input1.size()[1], input2.size()[2], input2.size()[3])\n        #print(\'decice %d\' % torch.cuda.current_device())\n        if input1.is_cuda:\n            self.device = torch.cuda.current_device()\n        else:\n            self.device = -1\n        self.device_c[0] = self.device\n        if not input1.is_cuda:\n            my_lib.BilinearSamplerBCHW_updateOutput(input1, input2, output)\n        else:\n\n            output = output.transpose(1,2).transpose(2,3).contiguous()\n            input1 = input1.transpose(1,2).transpose(2,3).contiguous()\n            input2 = input2.transpose(1,2).transpose(2,3).contiguous()\n            #print(output.size(), input1.size(), input2.size())\n            output = output.cuda(self.device)\n            my_lib.BilinearSamplerBHWD_updateOutput_cuda(input1, input2, output, self.device_c)\n            output = output.transpose(2,3).transpose(1,2)\n\n        return output\n\n    def backward(self, grad_output):\n        grad_input1 = torch.zeros(self.input1.size())\n        grad_input2 = torch.zeros(self.input2.size())\n        #print(\'backward decice %d\' % self.device)\n        if not grad_output.is_cuda:\n            my_lib.BilinearSamplerBCHW_updateGradInput(self.input1, self.input2, grad_input1, grad_input2, grad_output)\n        else:\n            grad_input1 = grad_input1.transpose(1,2).transpose(2,3).contiguous()\n            grad_input2 = grad_input2.transpose(1,2).transpose(2,3).contiguous()\n            grad_output = grad_output.transpose(1,2).transpose(2,3).contiguous()\n\n            grad_input1 = grad_input1.cuda(self.device)\n            grad_input2 = grad_input2.cuda(self.device)\n            my_lib.BilinearSamplerBHWD_updateGradInput_cuda(self.input1, self.input2, grad_input1, grad_input2, grad_output, self.device_c)\n\n            grad_input1 = grad_input1.transpose(2,3).transpose(1,2)\n            grad_input2 = grad_input2.transpose(2,3).transpose(1,2)\n\n        return grad_input1, grad_input2\n'"
script/modules/__init__.py,0,b''
script/modules/gridgen.py,80,"b'from torch.nn.modules.module import Module\nimport torch\nfrom torch.autograd import Variable\nimport numpy as np\nfrom functions.gridgen import AffineGridGenFunction, CylinderGridGenFunction\n\nimport pyximport\npyximport.install(setup_args={""include_dirs"":np.get_include()},\n                  reload_support=True)\n\n\nclass AffineGridGen(Module):\n    def __init__(self, height, width, lr = 1, aux_loss = False):\n        super(AffineGridGen, self).__init__()\n        self.height, self.width = height, width\n        self.aux_loss = aux_loss\n        self.f = AffineGridGenFunction(self.height, self.width, lr=lr)\n        self.lr = lr\n    def forward(self, input):\n        if not self.aux_loss:\n            return self.f(input)\n        else:\n            identity = torch.from_numpy(np.array([[1,0,0], [0,1,0]], dtype=np.float32))\n            batch_identity = torch.zeros([input.size(0), 2,3])\n            for i in range(input.size(0)):\n                batch_identity[i] = identity\n            batch_identity = Variable(batch_identity)\n            loss = torch.mul(input - batch_identity, input - batch_identity)\n            loss = torch.sum(loss,1)\n            loss = torch.sum(loss,2)\n\n            return self.f(input), loss.view(-1,1)\n\nclass CylinderGridGen(Module):\n    def __init__(self, height, width, lr = 1, aux_loss = False):\n        super(CylinderGridGen, self).__init__()\n        self.height, self.width = height, width\n        self.aux_loss = aux_loss\n        self.f = CylinderGridGenFunction(self.height, self.width, lr=lr)\n        self.lr = lr\n    def forward(self, input):\n\n        if not self.aux_loss:\n            return self.f(input)\n        else:\n            return self.f(input), torch.mul(input, input).view(-1,1)\n\n\nclass AffineGridGenV2(Module):\n    def __init__(self, height, width, lr = 1, aux_loss = False):\n        super(AffineGridGenV2, self).__init__()\n        self.height, self.width = height, width\n        self.aux_loss = aux_loss\n        self.lr = lr\n\n        self.grid = np.zeros( [self.height, self.width, 3], dtype=np.float32)\n        self.grid[:,:,0] = np.expand_dims(np.repeat(np.expand_dims(np.arange(-1, 1, 2.0/self.height), 0), repeats = self.width, axis = 0).T, 0)\n        self.grid[:,:,1] = np.expand_dims(np.repeat(np.expand_dims(np.arange(-1, 1, 2.0/self.width), 0), repeats = self.height, axis = 0), 0)\n        self.grid[:,:,2] = np.ones([self.height, width])\n        self.grid = torch.from_numpy(self.grid.astype(np.float32))\n\n\n    def forward(self, input1):\n        self.batchgrid = torch.zeros(torch.Size([input1.size(0)]) + self.grid.size())\n\n        for i in range(input1.size(0)):\n            self.batchgrid[i] = self.grid\n        self.batchgrid = Variable(self.batchgrid)\n\n        if input1.is_cuda:\n            self.batchgrid = self.batchgrid.cuda()\n\n        output = torch.bmm(self.batchgrid.view(-1, self.height*self.width, 3), torch.transpose(input1, 1, 2)).view(-1, self.height, self.width, 2)\n\n        return output\n\n\nclass CylinderGridGenV2(Module):\n    def __init__(self, height, width, lr = 1):\n        super(CylinderGridGenV2, self).__init__()\n        self.height, self.width = height, width\n        self.lr = lr\n        self.grid = np.zeros( [self.height, self.width, 3], dtype=np.float32)\n        self.grid[:,:,0] = np.expand_dims(np.repeat(np.expand_dims(np.arange(-1, 1, 2.0/self.height), 0), repeats = self.width, axis = 0).T, 0)\n        self.grid[:,:,1] = np.expand_dims(np.repeat(np.expand_dims(np.arange(-1, 1, 2.0/self.width), 0), repeats = self.height, axis = 0), 0)\n        self.grid[:,:,2] = np.ones([self.height, width])\n        self.grid = torch.from_numpy(self.grid.astype(np.float32))\n    def forward(self, input):\n        self.batchgrid = torch.zeros(torch.Size([input.size(0)]) + self.grid.size() )\n        #print(self.batchgrid.size())\n        for i in range(input.size(0)):\n            self.batchgrid[i,:,:,:] = self.grid\n        self.batchgrid = Variable(self.batchgrid)\n\n        #print(self.batchgrid.size())\n\n        input_u = input.view(-1,1,1,1).repeat(1,self.height, self.width,1)\n        #print(input_u.requires_grad, self.batchgrid)\n\n        output0 = self.batchgrid[:,:,:,0:1]\n        output1 = torch.atan(torch.tan(np.pi/2.0*(self.batchgrid[:,:,:,1:2] + self.batchgrid[:,:,:,2:] * input_u[:,:,:,:])))  /(np.pi/2)\n        #print(output0.size(), output1.size())\n\n        output = torch.cat([output0, output1], 3)\n        return output\n\n\nclass DenseAffineGridGen(Module):\n    def __init__(self, height, width, lr = 1, aux_loss = False):\n        super(DenseAffineGridGen, self).__init__()\n        self.height, self.width = height, width\n        self.aux_loss = aux_loss\n        self.lr = lr\n\n        self.grid = np.zeros( [self.height, self.width, 3], dtype=np.float32)\n        self.grid[:,:,0] = np.expand_dims(np.repeat(np.expand_dims(np.arange(-1, 1, 2.0/self.height), 0), repeats = self.width, axis = 0).T, 0)\n        self.grid[:,:,1] = np.expand_dims(np.repeat(np.expand_dims(np.arange(-1, 1, 2.0/self.width), 0), repeats = self.height, axis = 0), 0)\n        self.grid[:,:,2] = np.ones([self.height, width])\n        self.grid = torch.from_numpy(self.grid.astype(np.float32))\n\n\n    def forward(self, input1):\n        self.batchgrid = torch.zeros(torch.Size([input1.size(0)]) + self.grid.size())\n\n        for i in range(input1.size(0)):\n            self.batchgrid[i] = self.grid\n\n        self.batchgrid = Variable(self.batchgrid)\n        #print self.batchgrid,  input1[:,:,:,0:3]\n        #print self.batchgrid,  input1[:,:,:,4:6]\n        x = torch.mul(self.batchgrid, input1[:,:,:,0:3])\n        y = torch.mul(self.batchgrid, input1[:,:,:,3:6])\n\n        output = torch.cat([torch.sum(x,3),torch.sum(y,3)], 3)\n        return output\n\n\n\n\nclass DenseAffine3DGridGen(Module):\n    def __init__(self, height, width, lr = 1, aux_loss = False):\n        super(DenseAffine3DGridGen, self).__init__()\n        self.height, self.width = height, width\n        self.aux_loss = aux_loss\n        self.lr = lr\n\n        self.grid = np.zeros( [self.height, self.width, 3], dtype=np.float32)\n        self.grid[:,:,0] = np.expand_dims(np.repeat(np.expand_dims(np.arange(-1, 1, 2.0/self.height), 0), repeats = self.width, axis = 0).T, 0)\n        self.grid[:,:,1] = np.expand_dims(np.repeat(np.expand_dims(np.arange(-1, 1, 2.0/self.width), 0), repeats = self.height, axis = 0), 0)\n        self.grid[:,:,2] = np.ones([self.height, width])\n        self.grid = torch.from_numpy(self.grid.astype(np.float32))\n\n        self.theta = self.grid[:,:,0] * np.pi/2 + np.pi/2\n        self.phi = self.grid[:,:,1] * np.pi\n\n        self.x = torch.sin(self.theta) * torch.cos(self.phi)\n        self.y = torch.sin(self.theta) * torch.sin(self.phi)\n        self.z = torch.cos(self.theta)\n\n        self.grid3d = torch.from_numpy(np.zeros( [self.height, self.width, 4], dtype=np.float32))\n\n        self.grid3d[:,:,0] = self.x\n        self.grid3d[:,:,1] = self.y\n        self.grid3d[:,:,2] = self.z\n        self.grid3d[:,:,3] = self.grid[:,:,2]\n\n\n    def forward(self, input1):\n        self.batchgrid3d = torch.zeros(torch.Size([input1.size(0)]) + self.grid3d.size())\n\n        for i in range(input1.size(0)):\n            self.batchgrid3d[i] = self.grid3d\n\n        self.batchgrid3d = Variable(self.batchgrid3d)\n        #print(self.batchgrid3d)\n\n        x = torch.sum(torch.mul(self.batchgrid3d, input1[:,:,:,0:4]), 3)\n        y = torch.sum(torch.mul(self.batchgrid3d, input1[:,:,:,4:8]), 3)\n        z = torch.sum(torch.mul(self.batchgrid3d, input1[:,:,:,8:]), 3)\n        #print(x)\n        r = torch.sqrt(x**2 + y**2 + z**2) + 1e-5\n\n        #print(r)\n        theta = torch.acos(z/r)/(np.pi/2)  - 1\n        #phi = torch.atan(y/x)\n        phi = torch.atan(y/(x + 1e-5))  + np.pi * x.lt(0).type(torch.FloatTensor) * (y.ge(0).type(torch.FloatTensor) - y.lt(0).type(torch.FloatTensor))\n        phi = phi/np.pi\n\n\n        output = torch.cat([theta,phi], 3)\n\n        return output\n\n\n\n\n\nclass DenseAffine3DGridGen_rotate(Module):\n    def __init__(self, height, width, lr = 1, aux_loss = False):\n        super(DenseAffine3DGridGen_rotate, self).__init__()\n        self.height, self.width = height, width\n        self.aux_loss = aux_loss\n        self.lr = lr\n\n        self.grid = np.zeros( [self.height, self.width, 3], dtype=np.float32)\n        self.grid[:,:,0] = np.expand_dims(np.repeat(np.expand_dims(np.arange(-1, 1, 2.0/self.height), 0), repeats = self.width, axis = 0).T, 0)\n        self.grid[:,:,1] = np.expand_dims(np.repeat(np.expand_dims(np.arange(-1, 1, 2.0/self.width), 0), repeats = self.height, axis = 0), 0)\n        self.grid[:,:,2] = np.ones([self.height, width])\n        self.grid = torch.from_numpy(self.grid.astype(np.float32))\n\n        self.theta = self.grid[:,:,0] * np.pi/2 + np.pi/2\n        self.phi = self.grid[:,:,1] * np.pi\n\n        self.x = torch.sin(self.theta) * torch.cos(self.phi)\n        self.y = torch.sin(self.theta) * torch.sin(self.phi)\n        self.z = torch.cos(self.theta)\n\n        self.grid3d = torch.from_numpy(np.zeros( [self.height, self.width, 4], dtype=np.float32))\n\n        self.grid3d[:,:,0] = self.x\n        self.grid3d[:,:,1] = self.y\n        self.grid3d[:,:,2] = self.z\n        self.grid3d[:,:,3] = self.grid[:,:,2]\n\n\n    def forward(self, input1, input2):\n        self.batchgrid3d = torch.zeros(torch.Size([input1.size(0)]) + self.grid3d.size())\n\n        for i in range(input1.size(0)):\n            self.batchgrid3d[i] = self.grid3d\n\n        self.batchgrid3d = Variable(self.batchgrid3d)\n\n        self.batchgrid = torch.zeros(torch.Size([input1.size(0)]) + self.grid.size())\n\n        for i in range(input1.size(0)):\n            self.batchgrid[i] = self.grid\n\n        self.batchgrid = Variable(self.batchgrid)\n\n        #print(self.batchgrid3d)\n\n        x = torch.sum(torch.mul(self.batchgrid3d, input1[:,:,:,0:4]), 3)\n        y = torch.sum(torch.mul(self.batchgrid3d, input1[:,:,:,4:8]), 3)\n        z = torch.sum(torch.mul(self.batchgrid3d, input1[:,:,:,8:]), 3)\n        #print(x)\n        r = torch.sqrt(x**2 + y**2 + z**2) + 1e-5\n\n        #print(r)\n        theta = torch.acos(z/r)/(np.pi/2)  - 1\n        #phi = torch.atan(y/x)\n        phi = torch.atan(y/(x + 1e-5))  + np.pi * x.lt(0).type(torch.FloatTensor) * (y.ge(0).type(torch.FloatTensor) - y.lt(0).type(torch.FloatTensor))\n        phi = phi/np.pi\n\n        input_u = input2.view(-1,1,1,1).repeat(1,self.height, self.width,1)\n\n        output = torch.cat([theta,phi], 3)\n\n        output1 = torch.atan(torch.tan(np.pi/2.0*(output[:,:,:,1:2] + self.batchgrid[:,:,:,2:] * input_u[:,:,:,:])))  /(np.pi/2)\n        output2 = torch.cat([output[:,:,:,0:1], output1], 3)\n\n        return output2\n\n\nclass Depth3DGridGen(Module):\n    def __init__(self, height, width, lr = 1, aux_loss = False):\n        super(Depth3DGridGen, self).__init__()\n        self.height, self.width = height, width\n        self.aux_loss = aux_loss\n        self.lr = lr\n\n        self.grid = np.zeros( [self.height, self.width, 3], dtype=np.float32)\n        self.grid[:,:,0] = np.expand_dims(np.repeat(np.expand_dims(np.arange(-1, 1, 2.0/self.height), 0), repeats = self.width, axis = 0).T, 0)\n        self.grid[:,:,1] = np.expand_dims(np.repeat(np.expand_dims(np.arange(-1, 1, 2.0/self.width), 0), repeats = self.height, axis = 0), 0)\n        self.grid[:,:,2] = np.ones([self.height, width])\n        self.grid = torch.from_numpy(self.grid.astype(np.float32))\n\n        self.theta = self.grid[:,:,0] * np.pi/2 + np.pi/2\n        self.phi = self.grid[:,:,1] * np.pi\n\n        self.x = torch.sin(self.theta) * torch.cos(self.phi)\n        self.y = torch.sin(self.theta) * torch.sin(self.phi)\n        self.z = torch.cos(self.theta)\n\n        self.grid3d = torch.from_numpy(np.zeros( [self.height, self.width, 4], dtype=np.float32))\n\n        self.grid3d[:,:,0] = self.x\n        self.grid3d[:,:,1] = self.y\n        self.grid3d[:,:,2] = self.z\n        self.grid3d[:,:,3] = self.grid[:,:,2]\n\n\n    def forward(self, depth, trans0, trans1, rotate):\n        self.batchgrid3d = torch.zeros(torch.Size([depth.size(0)]) + self.grid3d.size())\n\n        for i in range(depth.size(0)):\n            self.batchgrid3d[i] = self.grid3d\n\n        self.batchgrid3d = Variable(self.batchgrid3d)\n\n        self.batchgrid = torch.zeros(torch.Size([depth.size(0)]) + self.grid.size())\n\n        for i in range(depth.size(0)):\n            self.batchgrid[i] = self.grid\n\n        self.batchgrid = Variable(self.batchgrid)\n\n        x = self.batchgrid3d[:,:,:,0:1] * depth + trans0.view(-1,1,1,1).repeat(1, self.height, self.width, 1)\n\n        y = self.batchgrid3d[:,:,:,1:2] * depth + trans1.view(-1,1,1,1).repeat(1, self.height, self.width, 1)\n        z = self.batchgrid3d[:,:,:,2:3] * depth\n        #print(x.size(), y.size(), z.size())\n        r = torch.sqrt(x**2 + y**2 + z**2) + 1e-5\n\n        #print(r)\n        theta = torch.acos(z/r)/(np.pi/2)  - 1\n        #phi = torch.atan(y/x)\n        phi = torch.atan(y/(x + 1e-5))  + np.pi * x.lt(0).type(torch.FloatTensor) * (y.ge(0).type(torch.FloatTensor) - y.lt(0).type(torch.FloatTensor))\n        phi = phi/np.pi\n\n        #print(theta.size(), phi.size())\n\n\n        input_u = rotate.view(-1,1,1,1).repeat(1,self.height, self.width,1)\n\n        output = torch.cat([theta,phi], 3)\n        #print(output.size())\n\n        output1 = torch.atan(torch.tan(np.pi/2.0*(output[:,:,:,1:2] + self.batchgrid[:,:,:,2:] * input_u[:,:,:,:])))  /(np.pi/2)\n        output2 = torch.cat([output[:,:,:,0:1], output1], 3)\n\n        return output2\n\n\n\n\n\nclass Depth3DGridGen_with_mask(Module):\n    def __init__(self, height, width, lr = 1, aux_loss = False, ray_tracing = False):\n        super(Depth3DGridGen_with_mask, self).__init__()\n        self.height, self.width = height, width\n        self.aux_loss = aux_loss\n        self.lr = lr\n        self.ray_tracing = ray_tracing\n\n        self.grid = np.zeros( [self.height, self.width, 3], dtype=np.float32)\n        self.grid[:,:,0] = np.expand_dims(np.repeat(np.expand_dims(np.arange(-1, 1, 2.0/self.height), 0), repeats = self.width, axis = 0).T, 0)\n        self.grid[:,:,1] = np.expand_dims(np.repeat(np.expand_dims(np.arange(-1, 1, 2.0/self.width), 0), repeats = self.height, axis = 0), 0)\n        self.grid[:,:,2] = np.ones([self.height, width])\n        self.grid = torch.from_numpy(self.grid.astype(np.float32))\n\n        self.theta = self.grid[:,:,0] * np.pi/2 + np.pi/2\n        self.phi = self.grid[:,:,1] * np.pi\n\n        self.x = torch.sin(self.theta) * torch.cos(self.phi)\n        self.y = torch.sin(self.theta) * torch.sin(self.phi)\n        self.z = torch.cos(self.theta)\n\n        self.grid3d = torch.from_numpy(np.zeros( [self.height, self.width, 4], dtype=np.float32))\n\n        self.grid3d[:,:,0] = self.x\n        self.grid3d[:,:,1] = self.y\n        self.grid3d[:,:,2] = self.z\n        self.grid3d[:,:,3] = self.grid[:,:,2]\n\n\n    def forward(self, depth, trans0, trans1, rotate):\n        self.batchgrid3d = torch.zeros(torch.Size([depth.size(0)]) + self.grid3d.size())\n\n        for i in range(depth.size(0)):\n            self.batchgrid3d[i] = self.grid3d\n\n        self.batchgrid3d = Variable(self.batchgrid3d)\n\n        self.batchgrid = torch.zeros(torch.Size([depth.size(0)]) + self.grid.size())\n\n        for i in range(depth.size(0)):\n            self.batchgrid[i] = self.grid\n\n        self.batchgrid = Variable(self.batchgrid)\n\n        if depth.is_cuda:\n            self.batchgrid = self.batchgrid.cuda()\n            self.batchgrid3d = self.batchgrid3d.cuda()\n\n\n        x_ = self.batchgrid3d[:,:,:,0:1] * depth + trans0.view(-1,1,1,1).repeat(1, self.height, self.width, 1)\n\n        y_ = self.batchgrid3d[:,:,:,1:2] * depth + trans1.view(-1,1,1,1).repeat(1, self.height, self.width, 1)\n        z = self.batchgrid3d[:,:,:,2:3] * depth\n        #print(x.size(), y.size(), z.size())\n\n        rotate_z = rotate.view(-1,1,1,1).repeat(1,self.height, self.width,1) * np.pi\n\n        x = x_ * torch.cos(rotate_z) - y_ * torch.sin(rotate_z)\n        y = x_ * torch.sin(rotate_z) + y_ * torch.cos(rotate_z)\n\n\n        r = torch.sqrt(x**2 + y**2 + z**2) + 1e-5\n\n        #print(r)\n        theta = torch.acos(z/r)/(np.pi/2)  - 1\n        #phi = torch.atan(y/x)\n\n        if depth.is_cuda:\n            phi = torch.atan(y/(x + 1e-5))  + np.pi * x.lt(0).type(torch.cuda.FloatTensor) * (y.ge(0).type(torch.cuda.FloatTensor) - y.lt(0).type(torch.cuda.FloatTensor))\n        else:\n            phi = torch.atan(y/(x + 1e-5))  + np.pi * x.lt(0).type(torch.FloatTensor) * (y.ge(0).type(torch.FloatTensor) - y.lt(0).type(torch.FloatTensor))\n\n\n        phi = phi/np.pi\n\n        output = torch.cat([theta,phi], 3)\n        return output\n'"
script/modules/stn.py,1,"b""from torch.nn.modules.module import Module\nfrom functions.stn import STNFunction, STNFunctionBCHW\n\nclass STN(Module):\n    def __init__(self, layout = 'BHWD'):\n        super(STN, self).__init__()\n        if layout == 'BHWD':\n            self.f = STNFunction()\n        else:\n            self.f = STNFunctionBCHW()\n    def forward(self, input1, input2):\n        return self.f(input1, input2)\n"""
