file_path,api_count,code
ddpg.py,8,"b""\nimport numpy as np\n\nimport torch\nimport torch.nn as nn\nfrom torch.optim import Adam\n\nfrom model import (Actor, Critic)\nfrom memory import SequentialMemory\nfrom random_process import OrnsteinUhlenbeckProcess\nfrom util import *\n\n# from ipdb import set_trace as debug\n\ncriterion = nn.MSELoss()\n\nclass DDPG(object):\n    def __init__(self, nb_states, nb_actions, args):\n        \n        if args.seed > 0:\n            self.seed(args.seed)\n\n        self.nb_states = nb_states\n        self.nb_actions= nb_actions\n        \n        # Create Actor and Critic Network\n        net_cfg = {\n            'hidden1':args.hidden1, \n            'hidden2':args.hidden2, \n            'init_w':args.init_w\n        }\n        self.actor = Actor(self.nb_states, self.nb_actions, **net_cfg)\n        self.actor_target = Actor(self.nb_states, self.nb_actions, **net_cfg)\n        self.actor_optim  = Adam(self.actor.parameters(), lr=args.prate)\n\n        self.critic = Critic(self.nb_states, self.nb_actions, **net_cfg)\n        self.critic_target = Critic(self.nb_states, self.nb_actions, **net_cfg)\n        self.critic_optim  = Adam(self.critic.parameters(), lr=args.rate)\n\n        hard_update(self.actor_target, self.actor) # Make sure target is with the same weight\n        hard_update(self.critic_target, self.critic)\n        \n        #Create replay buffer\n        self.memory = SequentialMemory(limit=args.rmsize, window_length=args.window_length)\n        self.random_process = OrnsteinUhlenbeckProcess(size=nb_actions, theta=args.ou_theta, mu=args.ou_mu, sigma=args.ou_sigma)\n\n        # Hyper-parameters\n        self.batch_size = args.bsize\n        self.tau = args.tau\n        self.discount = args.discount\n        self.depsilon = 1.0 / args.epsilon\n\n        # \n        self.epsilon = 1.0\n        self.s_t = None # Most recent state\n        self.a_t = None # Most recent action\n        self.is_training = True\n\n        # \n        if USE_CUDA: self.cuda()\n\n    def update_policy(self):\n        # Sample batch\n        state_batch, action_batch, reward_batch, \\\n        next_state_batch, terminal_batch = self.memory.sample_and_split(self.batch_size)\n\n        # Prepare for the target q batch\n        next_q_values = self.critic_target([\n            to_tensor(next_state_batch, volatile=True),\n            self.actor_target(to_tensor(next_state_batch, volatile=True)),\n        ])\n        next_q_values.volatile=False\n\n        target_q_batch = to_tensor(reward_batch) + \\\n            self.discount*to_tensor(terminal_batch.astype(np.float))*next_q_values\n\n        # Critic update\n        self.critic.zero_grad()\n\n        q_batch = self.critic([ to_tensor(state_batch), to_tensor(action_batch) ])\n        \n        value_loss = criterion(q_batch, target_q_batch)\n        value_loss.backward()\n        self.critic_optim.step()\n\n        # Actor update\n        self.actor.zero_grad()\n\n        policy_loss = -self.critic([\n            to_tensor(state_batch),\n            self.actor(to_tensor(state_batch))\n        ])\n\n        policy_loss = policy_loss.mean()\n        policy_loss.backward()\n        self.actor_optim.step()\n\n        # Target update\n        soft_update(self.actor_target, self.actor, self.tau)\n        soft_update(self.critic_target, self.critic, self.tau)\n\n    def eval(self):\n        self.actor.eval()\n        self.actor_target.eval()\n        self.critic.eval()\n        self.critic_target.eval()\n\n    def cuda(self):\n        self.actor.cuda()\n        self.actor_target.cuda()\n        self.critic.cuda()\n        self.critic_target.cuda()\n\n    def observe(self, r_t, s_t1, done):\n        if self.is_training:\n            self.memory.append(self.s_t, self.a_t, r_t, done)\n            self.s_t = s_t1\n\n    def random_action(self):\n        action = np.random.uniform(-1.,1.,self.nb_actions)\n        self.a_t = action\n        return action\n\n    def select_action(self, s_t, decay_epsilon=True):\n        action = to_numpy(\n            self.actor(to_tensor(np.array([s_t])))\n        ).squeeze(0)\n        action += self.is_training*max(self.epsilon, 0)*self.random_process.sample()\n        action = np.clip(action, -1., 1.)\n\n        if decay_epsilon:\n            self.epsilon -= self.depsilon\n        \n        self.a_t = action\n        return action\n\n    def reset(self, obs):\n        self.s_t = obs\n        self.random_process.reset_states()\n\n    def load_weights(self, output):\n        if output is None: return\n\n        self.actor.load_state_dict(\n            torch.load('{}/actor.pkl'.format(output))\n        )\n\n        self.critic.load_state_dict(\n            torch.load('{}/critic.pkl'.format(output))\n        )\n\n\n    def save_model(self,output):\n        torch.save(\n            self.actor.state_dict(),\n            '{}/actor.pkl'.format(output)\n        )\n        torch.save(\n            self.critic.state_dict(),\n            '{}/critic.pkl'.format(output)\n        )\n\n    def seed(self,s):\n        torch.manual_seed(s)\n        if USE_CUDA:\n            torch.cuda.manual_seed(s)\n"""
evaluator.py,0,"b""\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.io import savemat\n\nfrom util import *\n\nclass Evaluator(object):\n\n    def __init__(self, num_episodes, interval, save_path='', max_episode_length=None):\n        self.num_episodes = num_episodes\n        self.max_episode_length = max_episode_length\n        self.interval = interval\n        self.save_path = save_path\n        self.results = np.array([]).reshape(num_episodes,0)\n\n    def __call__(self, env, policy, debug=False, visualize=False, save=True):\n\n        self.is_training = False\n        observation = None\n        result = []\n\n        for episode in range(self.num_episodes):\n\n            # reset at the start of episode\n            observation = env.reset()\n            episode_steps = 0\n            episode_reward = 0.\n                \n            assert observation is not None\n\n            # start episode\n            done = False\n            while not done:\n                # basic operation, action ,reward, blablabla ...\n                action = policy(observation)\n\n                observation, reward, done, info = env.step(action)\n                if self.max_episode_length and episode_steps >= self.max_episode_length -1:\n                    done = True\n                \n                if visualize:\n                    env.render(mode='human')\n\n                # update\n                episode_reward += reward\n                episode_steps += 1\n\n            if debug: prYellow('[Evaluate] #Episode{}: episode_reward:{}'.format(episode,episode_reward))\n            result.append(episode_reward)\n\n        result = np.array(result).reshape(-1,1)\n        self.results = np.hstack([self.results, result])\n\n        if save:\n            self.save_results('{}/validate_reward'.format(self.save_path))\n        return np.mean(result)\n\n    def save_results(self, fn):\n\n        y = np.mean(self.results, axis=0)\n        error=np.std(self.results, axis=0)\n                    \n        x = range(0,self.results.shape[1]*self.interval,self.interval)\n        fig, ax = plt.subplots(1, 1, figsize=(6, 5))\n        plt.xlabel('Timestep')\n        plt.ylabel('Average Reward')\n        ax.errorbar(x, y, yerr=error, fmt='-o')\n        plt.savefig(fn+'.png')\n        savemat(fn+'.mat', {'reward':self.results})"""
main.py,0,"b'#!/usr/bin/env python3 \n\nimport numpy as np\nimport argparse\nfrom copy import deepcopy\nimport torch\nimport gym\n\nfrom normalized_env import NormalizedEnv\nfrom evaluator import Evaluator\nfrom ddpg import DDPG\nfrom util import *\n\ngym.undo_logger_setup()\n\ndef train(num_iterations, agent, env,  evaluate, validate_steps, output, max_episode_length=None, debug=False):\n\n    agent.is_training = True\n    step = episode = episode_steps = 0\n    episode_reward = 0.\n    observation = None\n    while step < num_iterations:\n        # reset if it is the start of episode\n        if observation is None:\n            observation = deepcopy(env.reset())\n            agent.reset(observation)\n\n        # agent pick action ...\n        if step <= args.warmup:\n            action = agent.random_action()\n        else:\n            action = agent.select_action(observation)\n        \n        # env response with next_observation, reward, terminate_info\n        observation2, reward, done, info = env.step(action)\n        observation2 = deepcopy(observation2)\n        if max_episode_length and episode_steps >= max_episode_length -1:\n            done = True\n\n        # agent observe and update policy\n        agent.observe(reward, observation2, done)\n        if step > args.warmup :\n            agent.update_policy()\n        \n        # [optional] evaluate\n        if evaluate is not None and validate_steps > 0 and step % validate_steps == 0:\n            policy = lambda x: agent.select_action(x, decay_epsilon=False)\n            validate_reward = evaluate(env, policy, debug=False, visualize=False)\n            if debug: prYellow(\'[Evaluate] Step_{:07d}: mean_reward:{}\'.format(step, validate_reward))\n\n        # [optional] save intermideate model\n        if step % int(num_iterations/3) == 0:\n            agent.save_model(output)\n\n        # update \n        step += 1\n        episode_steps += 1\n        episode_reward += reward\n        observation = deepcopy(observation2)\n\n        if done: # end of episode\n            if debug: prGreen(\'#{}: episode_reward:{} steps:{}\'.format(episode,episode_reward,step))\n\n            agent.memory.append(\n                observation,\n                agent.select_action(observation),\n                0., False\n            )\n\n            # reset\n            observation = None\n            episode_steps = 0\n            episode_reward = 0.\n            episode += 1\n\ndef test(num_episodes, agent, env, evaluate, model_path, visualize=True, debug=False):\n\n    agent.load_weights(model_path)\n    agent.is_training = False\n    agent.eval()\n    policy = lambda x: agent.select_action(x, decay_epsilon=False)\n\n    for i in range(num_episodes):\n        validate_reward = evaluate(env, policy, debug=debug, visualize=visualize, save=False)\n        if debug: prYellow(\'[Evaluate] #{}: mean_reward:{}\'.format(i, validate_reward))\n\n\nif __name__ == ""__main__"":\n\n    parser = argparse.ArgumentParser(description=\'PyTorch on TORCS with Multi-modal\')\n\n    parser.add_argument(\'--mode\', default=\'train\', type=str, help=\'support option: train/test\')\n    parser.add_argument(\'--env\', default=\'Pendulum-v0\', type=str, help=\'open-ai gym environment\')\n    parser.add_argument(\'--hidden1\', default=400, type=int, help=\'hidden num of first fully connect layer\')\n    parser.add_argument(\'--hidden2\', default=300, type=int, help=\'hidden num of second fully connect layer\')\n    parser.add_argument(\'--rate\', default=0.001, type=float, help=\'learning rate\')\n    parser.add_argument(\'--prate\', default=0.0001, type=float, help=\'policy net learning rate (only for DDPG)\')\n    parser.add_argument(\'--warmup\', default=100, type=int, help=\'time without training but only filling the replay memory\')\n    parser.add_argument(\'--discount\', default=0.99, type=float, help=\'\')\n    parser.add_argument(\'--bsize\', default=64, type=int, help=\'minibatch size\')\n    parser.add_argument(\'--rmsize\', default=6000000, type=int, help=\'memory size\')\n    parser.add_argument(\'--window_length\', default=1, type=int, help=\'\')\n    parser.add_argument(\'--tau\', default=0.001, type=float, help=\'moving average for target network\')\n    parser.add_argument(\'--ou_theta\', default=0.15, type=float, help=\'noise theta\')\n    parser.add_argument(\'--ou_sigma\', default=0.2, type=float, help=\'noise sigma\') \n    parser.add_argument(\'--ou_mu\', default=0.0, type=float, help=\'noise mu\') \n    parser.add_argument(\'--validate_episodes\', default=20, type=int, help=\'how many episode to perform during validate experiment\')\n    parser.add_argument(\'--max_episode_length\', default=500, type=int, help=\'\')\n    parser.add_argument(\'--validate_steps\', default=2000, type=int, help=\'how many steps to perform a validate experiment\')\n    parser.add_argument(\'--output\', default=\'output\', type=str, help=\'\')\n    parser.add_argument(\'--debug\', dest=\'debug\', action=\'store_true\')\n    parser.add_argument(\'--init_w\', default=0.003, type=float, help=\'\') \n    parser.add_argument(\'--train_iter\', default=200000, type=int, help=\'train iters each timestep\')\n    parser.add_argument(\'--epsilon\', default=50000, type=int, help=\'linear decay of exploration policy\')\n    parser.add_argument(\'--seed\', default=-1, type=int, help=\'\')\n    parser.add_argument(\'--resume\', default=\'default\', type=str, help=\'Resuming model path for testing\')\n    # parser.add_argument(\'--l2norm\', default=0.01, type=float, help=\'l2 weight decay\') # TODO\n    # parser.add_argument(\'--cuda\', dest=\'cuda\', action=\'store_true\') # TODO\n\n    args = parser.parse_args()\n    args.output = get_output_folder(args.output, args.env)\n    if args.resume == \'default\':\n        args.resume = \'output/{}-run0\'.format(args.env)\n\n    env = NormalizedEnv(gym.make(args.env))\n\n    if args.seed > 0:\n        np.random.seed(args.seed)\n        env.seed(args.seed)\n\n    nb_states = env.observation_space.shape[0]\n    nb_actions = env.action_space.shape[0]\n\n\n    agent = DDPG(nb_states, nb_actions, args)\n    evaluate = Evaluator(args.validate_episodes, \n        args.validate_steps, args.output, max_episode_length=args.max_episode_length)\n\n    if args.mode == \'train\':\n        train(args.train_iter, agent, env, evaluate, \n            args.validate_steps, args.output, max_episode_length=args.max_episode_length, debug=args.debug)\n\n    elif args.mode == \'test\':\n        test(args.validate_episodes, agent, env, evaluate, args.resume,\n            visualize=True, debug=args.debug)\n\n    else:\n        raise RuntimeError(\'undefined mode {}\'.format(args.mode))\n'"
memory.py,0,"b'from __future__ import absolute_import\nfrom collections import deque, namedtuple\nimport warnings\nimport random\n\nimport numpy as np\n\n# [reference] https://github.com/matthiasplappert/keras-rl/blob/master/rl/memory.py\n\n# This is to be understood as a transition: Given `state0`, performing `action`\n# yields `reward` and results in `state1`, which might be `terminal`.\nExperience = namedtuple(\'Experience\', \'state0, action, reward, state1, terminal1\')\n\n\ndef sample_batch_indexes(low, high, size):\n    if high - low >= size:\n        # We have enough data. Draw without replacement, that is each index is unique in the\n        # batch. We cannot use `np.random.choice` here because it is horribly inefficient as\n        # the memory grows. See https://github.com/numpy/numpy/issues/2764 for a discussion.\n        # `random.sample` does the same thing (drawing without replacement) and is way faster.\n        try:\n            r = xrange(low, high)\n        except NameError:\n            r = range(low, high)\n        batch_idxs = random.sample(r, size)\n    else:\n        # Not enough data. Help ourselves with sampling from the range, but the same index\n        # can occur multiple times. This is not good and should be avoided by picking a\n        # large enough warm-up phase.\n        warnings.warn(\'Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\')\n        batch_idxs = np.random.random_integers(low, high - 1, size=size)\n    assert len(batch_idxs) == size\n    return batch_idxs\n\n\nclass RingBuffer(object):\n    def __init__(self, maxlen):\n        self.maxlen = maxlen\n        self.start = 0\n        self.length = 0\n        self.data = [None for _ in range(maxlen)]\n\n    def __len__(self):\n        return self.length\n\n    def __getitem__(self, idx):\n        if idx < 0 or idx >= self.length:\n            raise KeyError()\n        return self.data[(self.start + idx) % self.maxlen]\n\n    def append(self, v):\n        if self.length < self.maxlen:\n            # We have space, simply increase the length.\n            self.length += 1\n        elif self.length == self.maxlen:\n            # No space, ""remove"" the first item.\n            self.start = (self.start + 1) % self.maxlen\n        else:\n            # This should never happen.\n            raise RuntimeError()\n        self.data[(self.start + self.length - 1) % self.maxlen] = v\n\n\ndef zeroed_observation(observation):\n    if hasattr(observation, \'shape\'):\n        return np.zeros(observation.shape)\n    elif hasattr(observation, \'__iter__\'):\n        out = []\n        for x in observation:\n            out.append(zeroed_observation(x))\n        return out\n    else:\n        return 0.\n\n\nclass Memory(object):\n    def __init__(self, window_length, ignore_episode_boundaries=False):\n        self.window_length = window_length\n        self.ignore_episode_boundaries = ignore_episode_boundaries\n\n        self.recent_observations = deque(maxlen=window_length)\n        self.recent_terminals = deque(maxlen=window_length)\n\n    def sample(self, batch_size, batch_idxs=None):\n        raise NotImplementedError()\n\n    def append(self, observation, action, reward, terminal, training=True):\n        self.recent_observations.append(observation)\n        self.recent_terminals.append(terminal)\n\n    def get_recent_state(self, current_observation):\n        # This code is slightly complicated by the fact that subsequent observations might be\n        # from different episodes. We ensure that an experience never spans multiple episodes.\n        # This is probably not that important in practice but it seems cleaner.\n        state = [current_observation]\n        idx = len(self.recent_observations) - 1\n        for offset in range(0, self.window_length - 1):\n            current_idx = idx - offset\n            current_terminal = self.recent_terminals[current_idx - 1] if current_idx - 1 >= 0 else False\n            if current_idx < 0 or (not self.ignore_episode_boundaries and current_terminal):\n                # The previously handled observation was terminal, don\'t add the current one.\n                # Otherwise we would leak into a different episode.\n                break\n            state.insert(0, self.recent_observations[current_idx])\n        while len(state) < self.window_length:\n            state.insert(0, zeroed_observation(state[0]))\n        return state\n\n    def get_config(self):\n        config = {\n            \'window_length\': self.window_length,\n            \'ignore_episode_boundaries\': self.ignore_episode_boundaries,\n        }\n        return config\n\nclass SequentialMemory(Memory):\n    def __init__(self, limit, **kwargs):\n        super(SequentialMemory, self).__init__(**kwargs)\n        \n        self.limit = limit\n\n        # Do not use deque to implement the memory. This data structure may seem convenient but\n        # it is way too slow on random access. Instead, we use our own ring buffer implementation.\n        self.actions = RingBuffer(limit)\n        self.rewards = RingBuffer(limit)\n        self.terminals = RingBuffer(limit)\n        self.observations = RingBuffer(limit)\n\n    def sample(self, batch_size, batch_idxs=None):\n        if batch_idxs is None:\n            # Draw random indexes such that we have at least a single entry before each\n            # index.\n            batch_idxs = sample_batch_indexes(0, self.nb_entries - 1, size=batch_size)\n        batch_idxs = np.array(batch_idxs) + 1\n        assert np.min(batch_idxs) >= 1\n        assert np.max(batch_idxs) < self.nb_entries\n        assert len(batch_idxs) == batch_size\n\n        # Create experiences\n        experiences = []\n        for idx in batch_idxs:\n            terminal0 = self.terminals[idx - 2] if idx >= 2 else False\n            while terminal0:\n                # Skip this transition because the environment was reset here. Select a new, random\n                # transition and use this instead. This may cause the batch to contain the same\n                # transition twice.\n                idx = sample_batch_indexes(1, self.nb_entries, size=1)[0]\n                terminal0 = self.terminals[idx - 2] if idx >= 2 else False\n            assert 1 <= idx < self.nb_entries\n\n            # This code is slightly complicated by the fact that subsequent observations might be\n            # from different episodes. We ensure that an experience never spans multiple episodes.\n            # This is probably not that important in practice but it seems cleaner.\n            state0 = [self.observations[idx - 1]]\n            for offset in range(0, self.window_length - 1):\n                current_idx = idx - 2 - offset\n                current_terminal = self.terminals[current_idx - 1] if current_idx - 1 > 0 else False\n                if current_idx < 0 or (not self.ignore_episode_boundaries and current_terminal):\n                    # The previously handled observation was terminal, don\'t add the current one.\n                    # Otherwise we would leak into a different episode.\n                    break\n                state0.insert(0, self.observations[current_idx])\n            while len(state0) < self.window_length:\n                state0.insert(0, zeroed_observation(state0[0]))\n            action = self.actions[idx - 1]\n            reward = self.rewards[idx - 1]\n            terminal1 = self.terminals[idx - 1]\n\n            # Okay, now we need to create the follow-up state. This is state0 shifted on timestep\n            # to the right. Again, we need to be careful to not include an observation from the next\n            # episode if the last state is terminal.\n            state1 = [np.copy(x) for x in state0[1:]]\n            state1.append(self.observations[idx])\n\n            assert len(state0) == self.window_length\n            assert len(state1) == len(state0)\n            experiences.append(Experience(state0=state0, action=action, reward=reward,\n                                          state1=state1, terminal1=terminal1))\n        assert len(experiences) == batch_size\n        return experiences\n\n    def sample_and_split(self, batch_size, batch_idxs=None):\n        experiences = self.sample(batch_size, batch_idxs)\n\n        state0_batch = []\n        reward_batch = []\n        action_batch = []\n        terminal1_batch = []\n        state1_batch = []\n        for e in experiences:\n            state0_batch.append(e.state0)\n            state1_batch.append(e.state1)\n            reward_batch.append(e.reward)\n            action_batch.append(e.action)\n            terminal1_batch.append(0. if e.terminal1 else 1.)\n\n        # Prepare and validate parameters.\n        state0_batch = np.array(state0_batch).reshape(batch_size,-1)\n        state1_batch = np.array(state1_batch).reshape(batch_size,-1)\n        terminal1_batch = np.array(terminal1_batch).reshape(batch_size,-1)\n        reward_batch = np.array(reward_batch).reshape(batch_size,-1)\n        action_batch = np.array(action_batch).reshape(batch_size,-1)\n\n        return state0_batch, action_batch, reward_batch, state1_batch, terminal1_batch\n\n\n    def append(self, observation, action, reward, terminal, training=True):\n        super(SequentialMemory, self).append(observation, action, reward, terminal, training=training)\n        \n        # This needs to be understood as follows: in `observation`, take `action`, obtain `reward`\n        # and weather the next state is `terminal` or not.\n        if training:\n            self.observations.append(observation)\n            self.actions.append(action)\n            self.rewards.append(reward)\n            self.terminals.append(terminal)\n\n    @property\n    def nb_entries(self):\n        return len(self.observations)\n\n    def get_config(self):\n        config = super(SequentialMemory, self).get_config()\n        config[\'limit\'] = self.limit\n        return config\n\n\nclass EpisodeParameterMemory(Memory):\n    def __init__(self, limit, **kwargs):\n        super(EpisodeParameterMemory, self).__init__(**kwargs)\n        self.limit = limit\n\n        self.params = RingBuffer(limit)\n        self.intermediate_rewards = []\n        self.total_rewards = RingBuffer(limit)\n\n    def sample(self, batch_size, batch_idxs=None):\n        if batch_idxs is None:\n            batch_idxs = sample_batch_indexes(0, self.nb_entries, size=batch_size)\n        assert len(batch_idxs) == batch_size\n\n        batch_params = []\n        batch_total_rewards = []\n        for idx in batch_idxs:\n            batch_params.append(self.params[idx])\n            batch_total_rewards.append(self.total_rewards[idx])\n        return batch_params, batch_total_rewards\n\n    def append(self, observation, action, reward, terminal, training=True):\n        super(EpisodeParameterMemory, self).append(observation, action, reward, terminal, training=training)\n        if training:\n            self.intermediate_rewards.append(reward)\n\n    def finalize_episode(self, params):\n        total_reward = sum(self.intermediate_rewards)\n        self.total_rewards.append(total_reward)\n        self.params.append(params)\n        self.intermediate_rewards = []\n\n    @property\n    def nb_entries(self):\n        return len(self.total_rewards)\n\n    def get_config(self):\n        config = super(SequentialMemory, self).get_config()\n        config[\'limit\'] = self.limit\n        return config\n'"
model.py,4,"b'\nimport numpy as np\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom ipdb import set_trace as debug\n\ndef fanin_init(size, fanin=None):\n    fanin = fanin or size[0]\n    v = 1. / np.sqrt(fanin)\n    return torch.Tensor(size).uniform_(-v, v)\n\nclass Actor(nn.Module):\n    def __init__(self, nb_states, nb_actions, hidden1=400, hidden2=300, init_w=3e-3):\n        super(Actor, self).__init__()\n        self.fc1 = nn.Linear(nb_states, hidden1)\n        self.fc2 = nn.Linear(hidden1, hidden2)\n        self.fc3 = nn.Linear(hidden2, nb_actions)\n        self.relu = nn.ReLU()\n        self.tanh = nn.Tanh()\n        self.init_weights(init_w)\n    \n    def init_weights(self, init_w):\n        self.fc1.weight.data = fanin_init(self.fc1.weight.data.size())\n        self.fc2.weight.data = fanin_init(self.fc2.weight.data.size())\n        self.fc3.weight.data.uniform_(-init_w, init_w)\n    \n    def forward(self, x):\n        out = self.fc1(x)\n        out = self.relu(out)\n        out = self.fc2(out)\n        out = self.relu(out)\n        out = self.fc3(out)\n        out = self.tanh(out)\n        return out\n\nclass Critic(nn.Module):\n    def __init__(self, nb_states, nb_actions, hidden1=400, hidden2=300, init_w=3e-3):\n        super(Critic, self).__init__()\n        self.fc1 = nn.Linear(nb_states, hidden1)\n        self.fc2 = nn.Linear(hidden1+nb_actions, hidden2)\n        self.fc3 = nn.Linear(hidden2, 1)\n        self.relu = nn.ReLU()\n        self.init_weights(init_w)\n    \n    def init_weights(self, init_w):\n        self.fc1.weight.data = fanin_init(self.fc1.weight.data.size())\n        self.fc2.weight.data = fanin_init(self.fc2.weight.data.size())\n        self.fc3.weight.data.uniform_(-init_w, init_w)\n    \n    def forward(self, xs):\n        x, a = xs\n        out = self.fc1(x)\n        out = self.relu(out)\n        # debug()\n        out = self.fc2(torch.cat([out,a],1))\n        out = self.relu(out)\n        out = self.fc3(out)\n        return out'"
normalized_env.py,0,"b'\nimport gym\n\n# https://github.com/openai/gym/blob/master/gym/core.py\nclass NormalizedEnv(gym.ActionWrapper):\n    """""" Wrap action """"""\n\n    def _action(self, action):\n        act_k = (self.action_space.high - self.action_space.low)/ 2.\n        act_b = (self.action_space.high + self.action_space.low)/ 2.\n        return act_k * action + act_b\n\n    def _reverse_action(self, action):\n        act_k_inv = 2./(self.action_space.high - self.action_space.low)\n        act_b = (self.action_space.high + self.action_space.low)/ 2.\n        return act_k_inv * (action - act_b)\n'"
random_process.py,0,"b'\nimport numpy as np \n\n# [reference] https://github.com/matthiasplappert/keras-rl/blob/master/rl/random.py\n\nclass RandomProcess(object):\n    def reset_states(self):\n        pass\n\nclass AnnealedGaussianProcess(RandomProcess):\n    def __init__(self, mu, sigma, sigma_min, n_steps_annealing):\n        self.mu = mu\n        self.sigma = sigma\n        self.n_steps = 0\n\n        if sigma_min is not None:\n            self.m = -float(sigma - sigma_min) / float(n_steps_annealing)\n            self.c = sigma\n            self.sigma_min = sigma_min\n        else:\n            self.m = 0.\n            self.c = sigma\n            self.sigma_min = sigma\n\n    @property\n    def current_sigma(self):\n        sigma = max(self.sigma_min, self.m * float(self.n_steps) + self.c)\n        return sigma\n\n\n# Based on http://math.stackexchange.com/questions/1287634/implementing-ornstein-uhlenbeck-in-matlab\nclass OrnsteinUhlenbeckProcess(AnnealedGaussianProcess):\n    def __init__(self, theta, mu=0., sigma=1., dt=1e-2, x0=None, size=1, sigma_min=None, n_steps_annealing=1000):\n        super(OrnsteinUhlenbeckProcess, self).__init__(mu=mu, sigma=sigma, sigma_min=sigma_min, n_steps_annealing=n_steps_annealing)\n        self.theta = theta\n        self.mu = mu\n        self.dt = dt\n        self.x0 = x0\n        self.size = size\n        self.reset_states()\n\n    def sample(self):\n        x = self.x_prev + self.theta * (self.mu - self.x_prev) * self.dt + self.current_sigma * np.sqrt(self.dt) * np.random.normal(size=self.size)\n        self.x_prev = x\n        self.n_steps += 1\n        return x\n\n    def reset_states(self):\n        self.x_prev = self.x0 if self.x0 is not None else np.zeros(self.size)'"
util.py,4,"b'\nimport os\nimport torch\nfrom torch.autograd import Variable\n\nUSE_CUDA = torch.cuda.is_available()\nFLOAT = torch.cuda.FloatTensor if USE_CUDA else torch.FloatTensor\n\ndef prRed(prt): print(""\\033[91m {}\\033[00m"" .format(prt))\ndef prGreen(prt): print(""\\033[92m {}\\033[00m"" .format(prt))\ndef prYellow(prt): print(""\\033[93m {}\\033[00m"" .format(prt))\ndef prLightPurple(prt): print(""\\033[94m {}\\033[00m"" .format(prt))\ndef prPurple(prt): print(""\\033[95m {}\\033[00m"" .format(prt))\ndef prCyan(prt): print(""\\033[96m {}\\033[00m"" .format(prt))\ndef prLightGray(prt): print(""\\033[97m {}\\033[00m"" .format(prt))\ndef prBlack(prt): print(""\\033[98m {}\\033[00m"" .format(prt))\n\ndef to_numpy(var):\n    return var.cpu().data.numpy() if USE_CUDA else var.data.numpy()\n\ndef to_tensor(ndarray, volatile=False, requires_grad=False, dtype=FLOAT):\n    return Variable(\n        torch.from_numpy(ndarray), volatile=volatile, requires_grad=requires_grad\n    ).type(dtype)\n\ndef soft_update(target, source, tau):\n    for target_param, param in zip(target.parameters(), source.parameters()):\n        target_param.data.copy_(\n            target_param.data * (1.0 - tau) + param.data * tau\n        )\n\ndef hard_update(target, source):\n    for target_param, param in zip(target.parameters(), source.parameters()):\n            target_param.data.copy_(param.data)\n\ndef get_output_folder(parent_dir, env_name):\n    """"""Return save folder.\n\n    Assumes folders in the parent_dir have suffix -run{run\n    number}. Finds the highest run number and sets the output folder\n    to that number + 1. This is just convenient so that if you run the\n    same script multiple times tensorboard can plot all of the results\n    on the same plots with different names.\n\n    Parameters\n    ----------\n    parent_dir: str\n      Path of the directory containing all experiment runs.\n\n    Returns\n    -------\n    parent_dir/run_dir\n      Path to this run\'s save directory.\n    """"""\n    os.makedirs(parent_dir, exist_ok=True)\n    experiment_id = 0\n    for folder_name in os.listdir(parent_dir):\n        if not os.path.isdir(os.path.join(parent_dir, folder_name)):\n            continue\n        try:\n            folder_name = int(folder_name.split(\'-run\')[-1])\n            if folder_name > experiment_id:\n                experiment_id = folder_name\n        except:\n            pass\n    experiment_id += 1\n\n    parent_dir = os.path.join(parent_dir, env_name)\n    parent_dir = parent_dir + \'-run{}\'.format(experiment_id)\n    os.makedirs(parent_dir, exist_ok=True)\n    return parent_dir\n\n\n'"
