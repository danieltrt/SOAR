file_path,api_count,code
logger.py,0,"b'import os\nfrom tensorboard_logger import configure, log_value\n\nclass Logger(object):\n    def __init__(self, log_dir):\n        # clean previous logged data under the same directory name\n        self._remove(log_dir)\n\n        # configure the project\n        configure(log_dir)\n\n        self.global_step = 0\n\n    def log_value(self, name, value):\n        log_value(name, value, self.global_step)\n        return self\n\n    def step(self):\n        self.global_step += 1\n\n    @staticmethod\n    def _remove(path):\n        """""" param <path> could either be relative or absolute. """"""\n        if os.path.isfile(path):\n            os.remove(path)  # remove the file\n        elif os.path.isdir(path):\n            import shutil\n            shutil.rmtree(path)  # remove dir and all contains'"
main.py,0,"b'##+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n## Created by: Albert Berenguel\n## Computer Vision Center (CVC). Universitat Autonoma de Barcelona\n## Email: aberenguel@cvc.uab.es\n## Copyright (c) 2017\n##\n## This source code is licensed under the MIT-style license found in the\n## LICENSE file in the root directory of this source tree\n##+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\nimport importlib\nfrom option import Options\nfrom logger import Logger\nimport numpy as np\n\n# Import params from Config\n# Parse other options\nargs = Options().parse()\n\n# load config info for task, data, and model\nopt = {}\nopt = importlib.import_module(args.task).params(opt)\nopt = importlib.import_module(args.data).params(opt)\nopt = importlib.import_module(args.model).params(opt)\nif not args.test == \'-\':\n    opt = importlib.import_module(args.test).params(opt)\nLOG_DIR = args.log_dir + \'/task_{}_data_{}_model_{}\' \\\n    .format(args.task,args.data,args.model)\n# create logger\nlogger = Logger(LOG_DIR)\n\n# Print options\nprint(\'Training with options:\')\nfor key in sorted(opt.iterkeys()):\n    print ""%s: %s"" % (key, opt[key])\n\n# set up meta-train, meta-validation and meta-test datasets\ndata = importlib.import_module(opt[\'dataLoader\']).getData(opt)\n# Run the training, validation and test.\nresults = importlib.import_module(opt[\'metaLearner\']).run(opt,data)\nprint(\'Task: %s. Data: %s. Model: %s\' % (args.task,args.data,args.model) )\n\n\n'"
option.py,0,"b""##+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n## Created by: Albert Berenguel\n## Computer Vision Center (CVC). Universitat Autonoma de Barcelona\n## Email: aberenguel@cvc.uab.es\n## Copyright (c) 2017\n##\n## This source code is licensed under the MIT-style license found in the\n## LICENSE file in the root directory of this source tree\n##+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n\nimport argparse\n\nclass Options():\n    def __init__(self):\n        # Training settings\n        parser = argparse.ArgumentParser(description='Few-Shot Learning')\n        parser.add_argument('--task', type=str, default='config.5-shot-5-class',\n                            help='path to config file for task')\n        parser.add_argument('--data', type=str, default='config.imagenet',\n                            help='path to config file for data')\n        parser.add_argument('--model', type=str, default='config.lstm.train-imagenet-5shot',\n        #parser.add_argument('--model', type=str, default='config.baselines.train-matching-net',\n                            help='path to config file for model')\n        parser.add_argument('--test', type=str, default='-',\n                            help='path to config file for test details')\n        parser.add_argument('--log-dir', default='./logs',\n                            help='folder to output model checkpoints')\n        self.parser = parser\n\n    def parse(self):\n        return self.parser.parse_args()\n"""
config/1-shot-5-class.py,0,"b""##+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n## Created by: Albert Berenguel\n## Computer Vision Center (CVC). Universitat Autonoma de Barcelona\n## Email: aberenguel@cvc.uab.es\n## Copyright (c) 2017\n##\n## This source code is licensed under the MIT-style license found in the\n## LICENSE file in the root directory of this source tree\n##+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\ndef params(opt):\n    opt['nClasses'] = {'train':5, 'val':5, 'test':5}\n    opt['nTrainShot'] = 1\n    opt['nEval'] = 15\n\n    opt['nTest'] = [100, 250, 600]\n    opt['nTestShot'] = [1, 5]\n    return opt"""
config/5-shot-5-class.py,0,"b""##+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n## Created by: Albert Berenguel\n## Computer Vision Center (CVC). Universitat Autonoma de Barcelona\n## Email: aberenguel@cvc.uab.es\n## Copyright (c) 2017\n##\n## This source code is licensed under the MIT-style license found in the\n## LICENSE file in the root directory of this source tree\n##+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\ndef params(opt):\n    opt['nClasses'] = {'train':5, 'val':5, 'test':5}\n    opt['nTrainShot'] = 5\n    opt['nEval'] = 15\n\n    opt['nTest'] = [100, 250, 600]\n    opt['nTestShot'] = [1, 5]\n    return opt"""
config/__init__.py,0,b''
config/imagenet.py,0,"b""##+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n## Created by: Albert Berenguel\n## Computer Vision Center (CVC). Universitat Autonoma de Barcelona\n## Email: aberenguel@cvc.uab.es\n## Copyright (c) 2017\n##\n## This source code is licensed under the MIT-style license found in the\n## LICENSE file in the root directory of this source tree\n##+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\ndef params(opt):\n    opt['nExamples'] = 20\n    opt['nDepth'] = 3\n    opt['nIn'] = 84\n\n    opt['rawDataDir'] = '/home/aberenguel/Dataset/miniImagenet'\n    opt['dataName'] = 'datasets.miniImagenet'\n    opt['dataLoader'] = 'datasets.data-loader'\n    opt['episodeSamplerKind'] = 'permutation'\n\n    return opt\n\n"""
datasets/__init__.py,0,b''
datasets/data-loader.py,0,"b""##+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n## Created by: Albert Berenguel\n## Computer Vision Center (CVC). Universitat Autonoma de Barcelona\n## Email: aberenguel@cvc.uab.es\n## Copyright (c) 2017\n##\n## This source code is licensed under the MIT-style license found in the\n## LICENSE file in the root directory of this source tree\n##+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n\nimport importlib\nimport numpy as np\n\ndef getData(opt):\n    # set up meta-train, meta-validation & meta-test datasets\n    dataTrain = importlib.import_module(opt['dataName']).DatasetLoader(dataroot=opt['rawDataDir'],\n                                                                       type='train',\n                                                                       nEpisodes=opt['nEpisode'],\n                                                                       classes_per_set=opt['nClasses']['train'],\n                                                                       samples_per_class=opt['nTrainShot'])\n\n    dataVal = importlib.import_module(opt['dataName']).DatasetLoader(dataroot=opt['rawDataDir'],\n                                                                     type='val',\n                                                                     nEpisodes=opt['nValidationEpisode'],\n                                                                     classes_per_set=opt['nClasses']['val'],\n                                                                     samples_per_class=opt['nEval'])\n    dataTest = []\n    for nTest in opt['nTest']:\n        dataTest.append(importlib.import_module(opt['dataName']).DatasetLoader(dataroot=opt['rawDataDir'],\n                                                                               type='test',\n                                                                               nEpisodes=np.sum(opt['nTest']),\n                                                                               classes_per_set=opt['nClasses']['test'],\n                                                                               samples_per_class=np.max(\n                                                                                   opt['nTestShot'])))\n    data = {'train': dataTrain, 'validation': dataVal, 'test': dataTest}\n    return data"""
datasets/miniImagenet.py,4,"b""##+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n## Created by: Albert Berenguel\n## Computer Vision Center (CVC). Universitat Autonoma de Barcelona\n## Email: aberenguel@cvc.uab.es\n## Copyright (c) 2017\n##\n## This source code is licensed under the MIT-style license found in the\n## LICENSE file in the root directory of this source tree\n##+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\nimport torch\nimport torch.utils.data as data\nimport torchvision.transforms as transforms\nfrom PIL import Image\nimport os.path\nimport csv\nimport math\nimport collections\nfrom tqdm import tqdm\n\nimport numpy as np\nnp.random.seed(2191)  # for reproducibility\n\n# LAMBDA FUNCTIONS\nfilenameToPILImage = lambda x: Image.open(x)\nPiLImageResize = lambda x: x.resize((84,84))\n\nclass DatasetLoader(data.Dataset):\n    def __init__(self, dataroot = './data/miniImagenet', type = 'train',\n                 nEpisodes = 1000, classes_per_set=10, samples_per_class=1):\n\n        self.nEpisodes = nEpisodes\n        self.classes_per_set = classes_per_set\n        self.samples_per_class = samples_per_class\n        self.samples_per_class_eval = 15\n        self.n_samples = self.samples_per_class * self.classes_per_set\n        self.n_samples_eval = self.samples_per_class_eval * self.classes_per_set\n        # Transformations to the image\n        self.transform = transforms.Compose([filenameToPILImage,\n                                             PiLImageResize,\n                                             transforms.ToTensor()\n                                             ])\n\n        def loadSplit(splitFile):\n            dictLabels = {}\n            with open(splitFile) as csvfile:\n                csvreader = csv.reader(csvfile, delimiter=',')\n                next(csvreader, None)\n                for i,row in enumerate(csvreader):\n                    filename = row[0]\n                    label = row[1]\n                    if label in dictLabels.keys():\n                        dictLabels[label].append(filename)\n                    else:\n                        dictLabels[label] = [filename]\n            return dictLabels\n\n        #requiredFiles = ['train','val','test']\n        self.miniImagenetImagesDir = os.path.join(dataroot,'images')\n        self.data = loadSplit(splitFile = os.path.join(dataroot,type + '.csv'))\n        self.data = collections.OrderedDict(sorted(self.data.items()))\n        self.classes_dict = {self.data.keys()[i]:i  for i in range(len(self.data.keys()))}\n        self.create_episodes(self.nEpisodes)\n\n    def create_episodes(self,episodes):\n\n        nClasses = len(self.data.keys())\n\n        self.support_set_x_batch = []\n        self.target_x_batch = []\n        for b in np.arange(episodes):\n            # select n classes_per_set randomly\n            selected_classes = np.random.choice(nClasses, self.classes_per_set, False)\n            support_set_x = []\n            target_x = []\n            for c in selected_classes:\n                selected_samples = np.random.choice(len(self.data[self.data.keys()[c]]),\n                               self.samples_per_class + self.samples_per_class_eval, False)\n                indexDtrain = np.array(selected_samples[:self.samples_per_class])\n                indexDtest = np.array(selected_samples[self.samples_per_class:])\n                support_set_x.append(np.array(self.data[self.data.keys()[c]])[indexDtrain].tolist())\n                target_x.append(np.array(self.data[self.data.keys()[c]])[indexDtest].tolist())\n            self.support_set_x_batch.append(support_set_x)\n            self.target_x_batch.append(target_x)\n\n    def __getitem__(self, index):\n\n        support_set_x = torch.FloatTensor(self.n_samples, 3, 84, 84)\n        support_set_y = np.zeros((self.n_samples), dtype=np.int)\n        target_x = torch.FloatTensor(self.n_samples_eval, 3, 84, 84)\n        target_y = np.zeros((self.n_samples_eval), dtype=np.int)\n\n        flatten_support_set_x_batch = [os.path.join(self.miniImagenetImagesDir,item)\n                                       for sublist in self.support_set_x_batch[index] for item in sublist]\n        support_set_y = np.array([self.classes_dict[item[:9]]\n                                      for sublist in self.support_set_x_batch[index] for item in sublist])\n        flatten_target_x = [os.path.join(self.miniImagenetImagesDir,item)\n                            for sublist in self.target_x_batch[index] for item in sublist]\n        target_y = np.array([self.classes_dict[item[:9]]\n                            for sublist in self.target_x_batch[index] for item in sublist])\n\n        for i,path in enumerate(flatten_support_set_x_batch):\n            if self.transform is not None:\n                support_set_x[i] = self.transform(path)\n\n        for i,path in enumerate(flatten_target_x):\n            if self.transform is not None:\n                target_x[i] = self.transform(path)\n        return support_set_x, torch.IntTensor(support_set_y), target_x, torch.IntTensor(target_y)\n\n    def __len__(self):\n        return self.nEpisodes\n\n\n"""
model/__init__.py,0,b''
model/lstm-classifier.py,2,"b'##+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n## Created by: Albert Berenguel\n## Computer Vision Center (CVC). Universitat Autonoma de Barcelona\n## Email: aberenguel@cvc.uab.es\n## Copyright (c) 2017\n##\n## This source code is licensed under the MIT-style license found in the\n## LICENSE file in the root directory of this source tree\n##+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n\nimport torch.nn as nn\nimport torch.nn.init as init\nimport numpy as np\nimport math\n\ndef convLayer(opt, layer_pos, nInput, nOutput, k ):\n    ""3x3 convolution with padding""\n    #if \'BN_momentum\' in opt.keys():\n    #    batchNorm = nn.BatchNorm2d(nOutput,momentum=opt[\'BN_momentum\'])\n    #else:\n    #    batchNorm = nn.BatchNorm2d(nOutput)\n        \n    seq = nn.Sequential(\n        nn.Conv2d(nInput, nOutput, kernel_size=k,\n                  stride=1, padding=1, bias=True),\n        #batchNorm,\n        opt[\'bnorm2d\'][layer_pos],\n        nn.ReLU(True),\n        nn.MaxPool2d(kernel_size=2, stride=2)\n    )\n    if opt[\'useDropout\']: # Add dropout module\n        list_seq = list(seq.modules())[1:]\n        list_seq.append(nn.Dropout(0.1))\n        seq = nn.Sequential(*list_seq)\n    return seq\n\nclass Classifier(nn.Module):\n    def __init__(self, opt):\n        super(Classifier, self).__init__()\n\n        finalSize = int(math.floor(opt[\'nIn\'] / (2 * 2 * 2 * 2)))\n\n        self.layer1 = convLayer(opt, 0, opt[\'nDepth\'], opt[\'nFilters\'], 3)\n        self.layer2 = convLayer(opt, 1, opt[\'nFilters\'], opt[\'nFilters\'], 3)\n        self.layer3 = convLayer(opt, 2, opt[\'nFilters\'], opt[\'nFilters\'], 3)\n        self.layer4 = convLayer(opt, 3, opt[\'nFilters\'], opt[\'nFilters\'], 3)\n\n        self.outSize = opt[\'nFilters\']*finalSize*finalSize\n        self.classify = opt[\'classify\']\n        if self.classify:\n            self.layer5 = nn.Linear(opt[\'nFilters\']*finalSize*finalSize, opt[\'nClasses\'][\'train\'])\n        self.outSize = opt[\'nClasses\'][\'train\']\n\n        # Initialize layers\n        self.reset()\n\n    def weights_init(self,module):\n        for m in module.modules():\n            if isinstance(m, nn.Conv2d):\n                init.xavier_uniform(m.weight, gain=np.sqrt(2))\n                init.constant(m.bias, 0)\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n\n    def reset(self):\n        self.weights_init(self.layer1)\n        self.weights_init(self.layer2)\n        self.weights_init(self.layer3)\n        self.weights_init(self.layer4)\n\n    def forward(self, x):\n        """"""\n        Runs the CNN producing the embeddings and the gradients.\n        :param image_input: Image input to produce embeddings for. [batch_size, 28, 28, 1]\n        :return: Embeddings of size [batch_size, 64]\n        """"""\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n        x = x.view(x.size(0), -1)\n        if self.classify:\n            x = self.layer5(x)\n        return x\n\n\nclass MatchingNetClassifier():\n    def __init__(self, opt):\n\n        self.net = Classifier(opt)\n        if opt[\'classify\']:\n            self.criterion = nn.CrossEntropyLoss()\n        else:\n            self.criterion = []\n        self.nParams = sum([i.view(-1).size()[0] for i in self.net.parameters()])\n        self.outSize = self.net.outSize\n\ndef build(opt):\n\n    model = MatchingNetClassifier(opt)\n    print(\'created net:\')\n    print(model.net)\n    return model\n'"
model/matching-net-classifier.py,2,"b'##+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n## Created by: Albert Berenguel\n## Computer Vision Center (CVC). Universitat Autonoma de Barcelona\n## Email: aberenguel@cvc.uab.es\n## Copyright (c) 2017\n##\n## This source code is licensed under the MIT-style license found in the\n## LICENSE file in the root directory of this source tree\n##+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n\nimport torch.nn as nn\nimport torch.nn.init as init\nimport numpy as np\nimport math\n\ndef convLayer(opt, nInput, nOutput, k):\n    ""3x3 convolution with padding""\n    seq = nn.Sequential(\n        nn.Conv2d(nInput, nOutput, kernel_size=k,\n                  stride=1, padding=1, bias=True),\n        nn.BatchNorm2d(nOutput),\n        nn.ReLU(True),\n        nn.MaxPool2d(kernel_size=2, stride=2)\n    )\n    if opt[\'useDropout\']: # Add dropout module\n        list_seq = list(seq.modules())[1:]\n        list_seq.append(nn.Dropout(0.1))\n        seq = nn.Sequential(*list_seq)\n    return seq\n\nclass Classifier(nn.Module):\n    def __init__(self, opt):\n        super(Classifier, self).__init__()\n\n        nFilters = 64\n        finalSize = int(math.floor(opt[\'nIn\'] / (2 * 2 * 2 * 2)))\n\n        self.layer1 = convLayer(opt, opt[\'nDepth\'], nFilters, 3)\n        self.layer2 = convLayer(opt, nFilters, nFilters, 3)\n        self.layer3 = convLayer(opt, nFilters, nFilters, 3)\n        self.layer4 = convLayer(opt, nFilters, nFilters, 3)\n\n        self.outSize = nFilters*finalSize*finalSize\n        self.classify = opt[\'classify\']\n        if self.classify:\n            self.layer5 = nn.Linear(nFilters*finalSize*finalSize, opt[\'nClasses\'][\'train\'])\n            self.outSize = opt[\'nClasses\'][\'train\']\n\n        # Initialize layers\n        self.weights_init(self.layer1)\n        self.weights_init(self.layer2)\n        self.weights_init(self.layer3)\n        self.weights_init(self.layer4)\n\n    def weights_init(self,module):\n        for m in module.modules():\n            if isinstance(m, nn.Conv2d):\n                init.xavier_uniform(m.weight, gain=np.sqrt(2))\n                init.constant(m.bias, 0)\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n\n    def forward(self, x):\n        """"""\n        Runs the CNN producing the embeddings and the gradients.\n        :param image_input: Image input to produce embeddings for. [batch_size, 28, 28, 1]\n        :return: Embeddings of size [batch_size, 64]\n        """"""\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n        x = x.view(x.size(0), -1)\n        if self.classify:\n            x = self.layer5(x)\n        return x\n\n\nclass MatchingNetClassifier():\n    def __init__(self, opt):\n\n        self.net = Classifier(opt)\n        if opt[\'classify\']:\n            self.criterion = nn.CrossEntropyLoss()\n        else:\n            self.criterion = []\n        self.nParams = np.sum([1 for i in self.net.parameters()])\n        self.outSize = self.net.outSize\n\ndef build(opt):\n\n    model = MatchingNetClassifier(opt)\n    print(\'created net:\')\n    print(model.net)\n    return model\n'"
utils/__init__.py,0,b''
utils/create_miniImagenet.py,0,"b'##+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n## Created by: Albert Berenguel\n## Computer Vision Center (CVC). Universitat Autonoma de Barcelona\n## Email: aberenguel@cvc.uab.es\n## Copyright (c) 2017\n##\n## This source code is licensed under the MIT-style license found in the\n## LICENSE file in the root directory of this source tree\n##+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n\n\'\'\'\nThis code creates the MiniImagenet dataset. Following the partitions given\nby Sachin Ravi and Hugo Larochelle in \nhttps://github.com/twitter/meta-learning-lstm/tree/master/data/miniImagenet\n\'\'\'\n\nimport numpy as np\nimport csv\nimport glob, os\nfrom shutil import copyfile\nimport cv2\nfrom tqdm import tqdm\n\npathImageNet = \'/home/aberenguel/Dataset/Imagenet/ILSVRC2012_img_train\'\npathminiImageNet = \'/home/aberenguel/TensorFlow/maml/data/miniImagenet/\'\npathImages = os.path.join(pathminiImageNet,\'images/\')\nfilesCSVSachinRavi = [os.path.join(pathminiImageNet,\'train.csv\'),\n                      os.path.join(pathminiImageNet,\'val.csv\'),\n                      os.path.join(pathminiImageNet,\'test.csv\')]\n\n# Check if the folder of images exist. If not create it.\nif not os.path.exists(pathImages):\n    os.makedirs(pathImages)\n\nfor filename in filesCSVSachinRavi:\n    with open(filename) as csvfile:\n        csv_reader = csv.reader(csvfile, delimiter=\',\')\n        next(csv_reader, None)\n        images = {}\n        print(\'Reading IDs....\')\n        for row in tqdm(csv_reader):\n            if row[1] in images.keys():\n                images[row[1]].append(row[0])\n            else:\n                images[row[1]] = [row[0]]\n\n        print(\'Writing photos....\')\n        for c in tqdm(images.keys()): # Iterate over all the classes\n            lst_files = []\n            for file in glob.glob(pathImageNet + ""/*""+c+""*""):\n                lst_files.append(file)\n            # TODO: Sort by name of by index number of the image???\n            # I sort by the number of the image\n            lst_index = [int(i[i.rfind(\'_\')+1:i.rfind(\'.\')]) for i in lst_files]\n            index_sorted = sorted(range(len(lst_index)), key=lst_index.__getitem__)\n\n            # Now iterate\n            index_selected = [int(i[i.index(\'.\') - 4:i.index(\'.\')]) for i in images[c]]\n            selected_images = np.array(index_sorted)[np.array(index_selected) - 1]\n            for i in np.arange(len(selected_images)):\n                # read file and resize to 84x84x3\n                #im = cv2.imread(os.path.join(pathImageNet,lst_files[selected_images[i]]))\n                #im_resized = cv2.resize(im, (84, 84), interpolation=cv2.INTER_AREA)\n                #cv2.imwrite(os.path.join(pathImages, images[c][i]),im_resized)\n                copyfile(os.path.join(pathImageNet,lst_files[selected_images[i]]),os.path.join(pathImages, images[c][i]))\n\n\n\n\n'"
utils/util.py,1,"b'import torch\nimport numpy as np\n\ndef createDictLabels(labels):\n\n    """"""\n    Creates dictionaries that fits data with non-sequential labels into a \n    sequential order label from [0...nClasses].\n    :param labels: all the non-sequential labels\n    :return: dict that converts from non-sequential to sequential, \n             dict that converts from sequential to non-sequential\n    """"""\n\n    # Re-arange the Target vectors between [0..nClasses_train]\n    labels = labels.numpy()\n    unique_labels = np.unique(labels)\n    dictLabels = {val: i for i, val in enumerate(unique_labels)}\n    dictLabelsInverse = {i: val for i, val in enumerate(unique_labels)}\n    return dictLabels,dictLabelsInverse\n\n\ndef fitLabelsToRange(dictLabels,labels):\n\n    """"""\n    Converts Tensor values to the values contained in the dictionary \n    :param dictLabels: dictionary with the conversion values\n    :param labels: Tensor to convert\n    :return: Tensor with the converted labels.\n    """"""\n    labels = labels.numpy()\n    unique_labels = np.unique(labels)\n    labels_temp = np.array(labels)\n    for i in dictLabels.keys():\n        labels_temp[labels == i] = dictLabels[i]\n    labels = labels_temp\n    return torch.from_numpy(labels)\n\ndef unflattenParams(model,flatParams):\n    flatParams = flatParams.squeeze()\n    indx = 0\n    for param in model.net.parameters():\n        lengthParam = param.view(-1).size()[0]\n        param.data = flatParams[indx:indx+lengthParam].view_as(param).data\n        indx = indx + lengthParam\n    a = 0\n\n\n'"
visualize/__init__.py,0,b''
visualize/visualize.py,3,"b'# Code from https://github.com/szagoruyko/functional-zoo/blob/master/visualize.py\nfrom graphviz import Digraph\nimport torch\nfrom torch.autograd import Variable\n\n\ndef make_dot(var, params=None):\n    """""" Produces Graphviz representation of PyTorch autograd graph\n    Blue nodes are the Variables that require grad, orange are Tensors\n    saved for backward in torch.autograd.Function\n    Args:\n        var: output Variable\n        params: dict of (name, Variable) to add names to node that\n            require grad (TODO: make optional)\n    """"""\n    if params is not None:\n        assert isinstance(params.values()[0], Variable)\n        param_map = {id(v): k for k, v in params.items()}\n\n    node_attr = dict(style=\'filled\',\n                     shape=\'box\',\n                     align=\'left\',\n                     fontsize=\'12\',\n                     ranksep=\'0.1\',\n                     height=\'0.2\')\n    dot = Digraph(node_attr=node_attr, graph_attr=dict(size=""12,12""))\n    seen = set()\n\n    def size_to_str(size):\n        return \'(\'+(\', \').join([\'%d\' % v for v in size])+\')\'\n\n    def add_nodes(var):\n        if var not in seen:\n            if torch.is_tensor(var):\n                dot.node(str(id(var)), size_to_str(var.size()), fillcolor=\'orange\')\n            elif hasattr(var, \'variable\'):\n                u = var.variable\n                name = param_map[id(u)] if params is not None else \'\'\n                node_name = \'%s\\n %s\' % (name, size_to_str(u.size()))\n                dot.node(str(id(var)), node_name, fillcolor=\'lightblue\')\n            else:\n                dot.node(str(id(var)), str(type(var).__name__))\n            seen.add(var)\n            if hasattr(var, \'next_functions\'):\n                for u in var.next_functions:\n                    if u[0] is not None:\n                        dot.edge(str(id(u[0])), str(id(var)))\n                        add_nodes(u[0])\n            if hasattr(var, \'saved_tensors\'):\n                for t in var.saved_tensors:\n                    dot.edge(str(id(t)), str(id(var)))\n                    add_nodes(t)\n    add_nodes(var.grad_fn)\n    return dot'"
config/baselines/__init__.py,0,b''
config/baselines/test-conv-nearest-neighbor.py,0,"b""##+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n## Created by: Albert Berenguel\n## Computer Vision Center (CVC). Universitat Autonoma de Barcelona\n## Email: aberenguel@cvc.uab.es\n## Copyright (c) 2017\n##\n## This source code is licensed under the MIT-style license found in the\n## LICENSE file in the root directory of this source tree\n##+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\ndef params(opt):\n    opt['nEpisode'] = 0\n    opt['paramsFile'] = 'saved_params/matching-net-FCE/matching-net_params_snapshot.th'\n    opt['networkFile'] = 'saved_params/matching-net-FCE/matching-net-models.th'\n    return opt\n"""
config/baselines/test-matching-net.py,0,"b""##+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n## Created by: Albert Berenguel\n## Computer Vision Center (CVC). Universitat Autonoma de Barcelona\n## Email: aberenguel@cvc.uab.es\n## Copyright (c) 2017\n##\n## This source code is licensed under the MIT-style license found in the\n## LICENSE file in the root directory of this source tree\n##+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\ndef params(opt):\n    opt['nEpochs'] = 0\n    opt['paramsFile'] = 'saved_params/conv-nearest-neighbor/conv-nearest-neighbor-model.th'\n    return opt"""
config/baselines/train-conv-nearest-neighbor.py,0,"b""##+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n## Created by: Albert Berenguel\n## Computer Vision Center (CVC). Universitat Autonoma de Barcelona\n## Email: aberenguel@cvc.uab.es\n## Copyright (c) 2017\n##\n## This source code is licensed under the MIT-style license found in the\n## LICENSE file in the root directory of this source tree\n##+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\ndef params(opt):\n    opt['learner'] = 'model.matching-net-classifier'\n    opt['metaLearner'] = 'model.baselines.conv-nearest-neighbor'\n\n    opt['trainFull'] = True\n    opt['nClasses.train'] = 64\n    opt['learningRate'] = 0.001\n    opt['trainBatchSize'] = 64\n    opt['nEpochs'] = 30000\n    opt['nValidationEpisode'] = 100\n    opt['printPer'] = 1000\n    opt['useCUDA'] = True\n    return opt\n\n"""
config/baselines/train-matching-net.py,0,"b""##+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n## Created by: Albert Berenguel\n## Computer Vision Center (CVC). Universitat Autonoma de Barcelona\n## Email: aberenguel@cvc.uab.es\n## Copyright (c) 2017\n##\n## This source code is licensed under the MIT-style license found in the\n## LICENSE file in the root directory of this source tree\n##+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\ndef params(opt):\n    opt['learner'] = 'model.matching-net-classifier'\n    opt['metaLearner'] = 'model.baselines.matching-net'\n\n    # simple or FCE - embedding model?\n    # opt['embedModel'] = 'model.baselines.simple-embedding'\n    opt['embedModel'] = 'model.baselines.fce-embedding'\n\n    opt['steps'] = 3\n    opt['classify'] = False\n    opt['useDropout'] = True\n    opt['optimMethod'] = 'adam'\n    opt['lr'] = 1e-03\n    opt['lr_decay'] = 1e-6\n    opt['weight_decay'] = 1e-4\n    opt['batchSize'] = opt['nClasses']['train'] * opt['nEval']\n    opt['nEpisode'] = 75000\n    opt['nValidationEpisode'] = 100\n    opt['printPer'] = 1000\n    opt['useCUDA'] = True\n    opt['ngpu'] = 2\n    return opt\n"""
config/baselines/train-pixel-nearest-neighbor.py,0,"b""##+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n## Created by: Albert Berenguel\n## Computer Vision Center (CVC). Universitat Autonoma de Barcelona\n## Email: aberenguel@cvc.uab.es\n## Copyright (c) 2017\n##\n## This source code is licensed under the MIT-style license found in the\n## LICENSE file in the root directory of this source tree\n##+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\ndef params(opt):\n    opt['learner'] = 'model.matching-net-classifier'\n    opt['metaLearner'] = 'model.baselines.pixel-nearest-neighbor'\n\n    opt['trainFull'] = True\n    opt['nClasses.train'] =  64 - (-20) - (-16)\n    opt['nAllClasses'] = 64 - (-4112)\n    opt['useCUDA'] = False\n    return opt\n\n"""
config/baselines/train-pre-trained-SGD.py,0,"b""##+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n## Created by: Albert Berenguel\n## Computer Vision Center (CVC). Universitat Autonoma de Barcelona\n## Email: aberenguel@cvc.uab.es\n## Copyright (c) 2017\n##\n## This source code is licensed under the MIT-style license found in the\n## LICENSE file in the root directory of this source tree\n##+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\ndef params(opt):\n    opt['learner'] = 'model.matching-net-classifier'\n    opt['metaLearner'] = 'model.baselines.pre-trained-SGD'\n\n\n    opt['trainFull'] = True\n    opt['nClasses.train'] = 64\n\n    opt['learningRate'] = 0.001\n    opt['trainBatchSize'] = 64\n\n    opt['learningRates'] = [0.5, 0.1, 0.01, 0.001, 0.0001, 0.00001]\n    opt['learningRateDecays'] = [1, 1e-1, 1e-2, 1e-3, 1e-4, 1e-5, 0]\n    opt['nUpdates'] = [15]\n\n    opt['nEpochs'] = 30000\n    opt['nValidationEpisode'] = 100\n    opt['printPer'] = 1000\n    opt['useCUDA'] = True\n    return opt\n\n"""
config/lstm/__init__.py,0,b''
config/lstm/test-lstm.py,0,"b""##+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n## Created by: Albert Berenguel\n## Computer Vision Center (CVC). Universitat Autonoma de Barcelona\n## Email: aberenguel@cvc.uab.es\n## Copyright (c) 2017\n##\n## This source code is licensed under the MIT-style license found in the\n## LICENSE file in the root directory of this source tree\n##+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\ndef params(opt):\n    opt['nEpisode'] = 0\n    opt['paramsFile'] = 'saved_params/miniImagenet/meta-learner-5shot-momentum/metaLearner_params_snapshot.th'\n    return opt\n\n"""
config/lstm/train-imagenet-1shot.py,0,"b""##+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n## Created by: Albert Berenguel\n## Computer Vision Center (CVC). Universitat Autonoma de Barcelona\n## Email: aberenguel@cvc.uab.es\n## Copyright (c) 2017\n##\n## This source code is licensed under the MIT-style license found in the\n## LICENSE file in the root directory of this source tree\n##+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\ndef params(opt):\n    opt['learner'] = 'model.lstm-classifier'\n    opt['metaLearner'] = 'model.lstm.train-lstm'\n\n\n    opt['BN_momentum'] = 0.9\n    opt['optimMethod'] = 'adam'\n    opt['lr'] = 1e-03\n    opt['lr_decay'] = 1e-6\n    opt['weight_decay'] = 1e-4\n    opt['maxGradNorm'] = 0.25\n\n    opt['batchSize'] = {1: 5, 5: 5}\n    opt['nEpochs'] = {1: 12, 5: 5}\n\n    opt['nEpisode'] = 7500\n    opt['nValidationEpisode'] = 100\n    opt['printPer'] = 1000\n    opt['useCUDA'] = True\n    return opt\n\n"""
config/lstm/train-imagenet-5shot.py,0,"b""##+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n## Created by: Albert Berenguel\n## Computer Vision Center (CVC). Universitat Autonoma de Barcelona\n## Email: aberenguel@cvc.uab.es\n## Copyright (c) 2017\n##\n## This source code is licensed under the MIT-style license found in the\n## LICENSE file in the root directory of this source tree\n##+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\ndef params(opt):\n    opt['learner'] = 'model.lstm-classifier'\n    opt['metaLearner'] = 'model.lstm.train-lstm'\n\n    opt['BN_momentum'] = 0.95\n    opt['classify'] = True\n    opt['useDropout'] = False\n\n    opt['optimMethod'] = 'adam'\n    opt['lr'] = 1e-03\n    opt['lr_decay'] = 1e-6\n    opt['weight_decay'] = 1e-4\n    opt['maxGradNorm'] = 0.25\n\n    opt['batchSize'] = {1:5, 5:25}\n    opt['nEpochs'] = {1: 5, 5: 8}\n\n    opt['nEpisode'] = 500\n    opt['nValidationEpisode'] = 100\n    opt['printPer'] = 100\n    opt['useCUDA'] = True\n    return opt"""
model/baselines/__init__.py,0,b''
model/baselines/fce-embedding.py,7,"b""##+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n## Created by: Albert Berenguel\n## Computer Vision Center (CVC). Universitat Autonoma de Barcelona\n## Email: aberenguel@cvc.uab.es\n## Copyright (c) 2017\n##\n## This source code is licensed under the MIT-style license found in the\n## LICENSE file in the root directory of this source tree\n##+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n\nimport os\nimport torch\nimport torch.nn as nn\nimport importlib\nimport pickle\nimport numpy as np\n#from model.lstm.bnlstm import RecurrentLSTMNetwork\n\nclass FceEmbedding():\n    def __init__(self, opt):\n        self.opt = opt # Store the parameters\n        self.maxGradNorm = opt['maxGradNorm'] if ['maxGradNorm'] in opt.keys() else 0.25\n        self.numLayersAttLstm = opt['numLayersAttLstm'] if ['numLayersAttLstm'] in opt.keys() else 1\n        self.numLayersBiLstm = opt['numLayersBiLstm'] if ['numLayersBiLstm'] in opt.keys() else 1\n        self.buildModels(self.opt)\n        self.setCuda()\n\n    # Build F and G models\n    def buildModels(self,opt):\n        # F function\n        modelF = importlib.import_module(opt['learner']).build(opt)\n        self.embedNetF = modelF.net\n        # G function\n        modelG = importlib.import_module(opt['learner']).build(opt)\n        self.embedNetG = modelG.net\n\n        '''\n        # Build LSTM for attention model.\n        self.attLSTM = RecurrentLSTMNetwork({\n            'inputFeatures': self.embedNetF.outSize + self.embedNetG.outSize,\n            'hiddenFeatures': self.embedNetF.outSize,\n            'outputType': 'all'\n        })\n\n        self.biLSTMForward = RecurrentLSTMNetwork({\n            'inputFeatures': self.embedNetG.outSize,\n            'hiddenFeatures': self.embedNetG.outSize,\n            'outputType': 'all'\n        })\n\n        self.biLSTMBackward = RecurrentLSTMNetwork({\n            'inputFeatures': self.embedNetG.outSize,\n            'hiddenFeatures': self.embedNetG.outSize,\n            'outputType': 'all'\n        })\n        '''\n\n        self.attLSTM = nn.LSTM(input_size=self.embedNetF.outSize + self.embedNetG.outSize,\n                                hidden_size=self.embedNetF.outSize,\n                                num_layers = self.numLayersAttLstm)\n        # Build bidirectional LSTM\n        self.biLSTM =  nn.LSTM(input_size=self.embedNetG.outSize,\n                               hidden_size=self.embedNetG.outSize,\n                               num_layers=self.numLayersBiLstm,\n                               bidirectional=True)\n\n        self.softmax = nn.Softmax()\n\n    # Build list of parameters for optim\n    def parameters(self):\n        # TODO: why in the original code creates a dictionary with the same\n        # parameters. model.params = {f=paramsG, g=paramsG, attLST, biLSTM}\n        return list(self.embedNetG.parameters()) + \\\n                list(self.embedNetG.parameters()) + \\\n                    list(self.attLSTM.parameters()) + \\\n                        list(self.biLSTM.parameters())\n\n    # Set training or evaluation mode\n    def set(self,mode):\n        if mode == 'training':\n            self.embedNetF.train()\n            self.embedNetG.train()\n        elif mode == 'evaluate':\n            self.embedNetF.eval()\n            self.embedNetG.eval()\n        else:\n            print('model.set: undefined mode - %s' % (mode))\n\n    def isTraining(self):\n        return self.embedNetF.training\n\n    def attLSTM_forward(self,gS,fX, K):\n\n        r = gS.mean(0).expand_as(fX)\n        for i in np.arange(K):\n            x = torch.cat((fX, r), 1)\n            x = x.unsqueeze(0)\n            if i == 0:\n                #dim: [sequence = 1, batch_size, num_features * 2]\n                output, (h, c) = self.attLSTM(x)\n            else:\n                output, (h, c) = self.attLSTM(x,(h,c))\n            h = fX.squeeze(0) + output\n\n            embed = None\n            # Iterate over batch size\n            for j in np.arange(h.size(1)):\n                hInd = h[0,i, :].expand_as(gS)\n                weight = (gS*hInd).sum(1).unsqueeze(1)\n                embed_tmp = (self.softmax(weight).expand_as(gS) * gS).sum(0).unsqueeze(0)\n                if embed is None:\n                    embed = embed_tmp\n                else:\n                    embed = torch.cat([embed,embed_tmp],0)\n        # output dim: [batch, num_features]\n        return h.squeeze(0)\n\n    def biLSTM_forward(self, input):\n        gX = input\n        # Expected input dimension of the form [sequence_length, batch_size, num_features]\n        gX = gX.unsqueeze(1)\n        output, (hn, cn) = self.biLSTM(gX)\n        # output dim: [sequence, batch_size, num_features * 2]\n        output = output[:, :, :self.embedNetG.outSize] + output[:, :, self.embedNetG.outSize:]\n        output = output.squeeze(1)\n        # output dim: [sequence, num_features]\n        return output\n\n    def embedG(self, input):\n        g = self.embedNetG(input)\n        return self.biLSTM_forward(g)\n\n    def embedF(self, input, g, K):\n        f = self.embedNetF(input)\n        return self.attLSTM_forward(g,f,K)\n\n    def save(self, path = './data'):\n        # Save the opt parameters\n        optParametersFile = open(os.path.join(path,'SimpleEmbedding_opt.pkl'), 'wb')\n        pickle.dump(self.opt, optParametersFile)\n        optParametersFile.close()\n        # Clean not needed data of the models\n        self.embedNetF.clearState()\n        self.embedNetG.clearState()\n        torch.save(self.embedNetF.state_dict(), os.path.join(path,'embedNetF.pth.tar'))\n        torch.save(self.embedNetG.state_dict(), os.path.join(path, 'embedNetG.pth.tar'))\n\n    def load(self, pathParams, pathModelF, pathModelG):\n        # Load opt parameters 'SimpleEmbedding_opt.pkl'\n        optParametersFile = open(pathParams, 'rb')\n        self.opt = pickle.load(optParametersFile)\n        optParametersFile.close()\n        # build the models\n        self.buildModels(self.opt)\n        # Load the weights and biases of F and G\n        checkpoint = torch.load(pathModelF)\n        self.embedNetF.load_state_dict(checkpoint['state_dict'])\n        checkpoint = torch.load(pathModelG)\n        self.embedNetG.load_state_dict(checkpoint['state_dict'])\n        # Set cuda\n        self.setCuda()\n\n    def setCuda(self, value = 'default'):\n        # If value is a string then use self.opt\n        # If it is not a string then it should be True or False\n        if type(value) == str:\n            value = self.opt['useCUDA']\n        else:\n            assert(type(value)==bool)\n\n        if value == True:\n            print('Check CUDA')\n            self.embedNetF.cuda()\n            self.embedNetG.cuda()\n            self.attLSTM.cuda()\n            self.biLSTM.cuda()\n        else:\n            self.embedNetF.cpu()\n            self.embedNetG.cpu()\n            self.attLSTM.cpu()\n            self.biLSTM.cpu()\n\ndef build(opt):\n    model = FceEmbedding(opt)\n    return model\n\n\n"""
model/baselines/matching-net.py,11,"b'##+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n## Created by: Albert Berenguel\n## Computer Vision Center (CVC). Universitat Autonoma de Barcelona\n## Email: aberenguel@cvc.uab.es\n## Copyright (c) 2017\n##\n## This source code is licensed under the MIT-style license found in the\n## LICENSE file in the root directory of this source tree\n##+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n\nimport os\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\nimport importlib\nimport numpy as np\nimport time\nimport math\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import accuracy_score\nfrom utils import util\n\nclass MatchingNet(nn.Module):\n    def __init__(self, opt):\n        super(MatchingNet, self).__init__()\n\n        # function cosine-similarity layer\n        self.cosineSim = nn.CosineSimilarity()\n\n        # local embedding model (simple or FCE)\n        self.embedModel = importlib.import_module(opt[\'embedModel\']).build(opt)\n        # set Cuda\n        self.embedModel.setCuda(opt[\'useCUDA\'])\n\n        # load loss. Why does not load with the model\n        self.lossF = nn.CrossEntropyLoss()\n\n    # Set training or evaluation mode\n    def set(self, mode):\n        self.embedModel.set(mode)\n\n    def forward(self, opt, input ):\n\n        trainInput = input[\'trainInput\']\n        trainTarget = input[\'trainTarget\']\n        testInput = input[\'testInput\']\n        testTarget = input[\'testTarget\']\n\n        # Create one-hot vector\n        trainTarget = trainTarget.view(-1,1)\n        y_one_hot = trainTarget.clone()\n        y_one_hot = y_one_hot.expand(\n            trainTarget.size()[0], opt[\'nClasses\'][\'train\'])\n        y_one_hot.data.zero_()\n        y_one_hot = y_one_hot.float().scatter_(1, trainTarget, 1)\n\n        # embed support set & test items using g and f respectively\n        gS = self.embedModel.embedG(trainInput)\n        fX = self.embedModel.embedF(testInput, gS, opt[\'steps\'])\n\n        # repeat tensors so that can get cosine sims in one call\n        repeatgS = gS.repeat(fX.size(0),1)\n        repeatfX = fX.repeat(1, gS.size(0)).view(fX.size(0)*gS.size(0),fX.size(1))\n\n        # weights are num_test x num_train (weigths per test item)\n        weights = self.cosineSim(repeatgS, repeatfX).view(fX.size(0), gS.size(0),1)\n\n        # one-hot matrix of train labels is expanded to num_train x num_test x num_labels\n        expandOneHot = y_one_hot.view(1,y_one_hot.size(0),y_one_hot.size(1)).expand(\n            fX.size(0),y_one_hot.size(0),y_one_hot.size(1))\n\n        # weights are expanded to match one-hot matrix\n        expandWeights = weights.expand_as(expandOneHot)\n\n        # cmul one-hot matrix by weights and sum along rows to get weight per label\n        # final size: num_train x num_labels\n        out = expandOneHot.mul(expandWeights).sum(1)\n\n        # calculate NLL\n        if self.embedModel.isTraining():\n            loss = self.lossF(out,testTarget)\n            return out, loss\n        else:\n            return out\n\ndef create_optimizer(opt, model):\n    if opt[\'optimMethod\'] == \'sgd\':\n        optimizer = torch.optim.SGD(model.parameters(), lr=opt[\'lr\'],\n                              momentum=0.9, dampening=0.9,\n                              weight_decay=opt[\'weight_decay\'])\n    elif opt[\'optimMethod\']:\n        optimizer = torch.optim.Adam(model.parameters(), lr=opt[\'lr\'],\n                               weight_decay=opt[\'weight_decay\'])\n    else:\n        raise Exception(\'Not supported optimizer: {0}\'.format(opt[\'optimMethod\']))\n    return optimizer\n\ndef adjust_learning_rate(opt,optimizer):\n    """"""Updates the learning rate given the learning rate decay.\n    The routine has been implemented according to the original Lua SGD optimizer\n    """"""\n    for group in optimizer.param_groups:\n        if \'step\' not in group:\n            group[\'step\'] = 0\n        group[\'step\'] += 1\n\n        group[\'lr\'] = opt[\'lr\'] / (1 + group[\'step\'] * opt[\'lr_decay\'])\n\n    return optimizer\n\ndef run(opt,data):\n\n    # Set the model\n    network = MatchingNet(opt)\n\n    # Keep track of errors\n    trainConf_pred = []\n    trainConf_gt = []\n    valConf_pred = {}\n    valConf_gt = {}\n    testConf_pred = {}\n    testConf_gt = {}\n    for i in opt[\'nTestShot\']:\n        valConf_pred[i] = []\n        valConf_gt[i] = []\n        testConf_pred[i] = []\n        testConf_gt[i] = []\n\n    # load params from file\n    # paramsFile format: <path>/SimpleEmbedding_opt.pkl\n    # pathModelF format: <path>/embedNet_F.pth.tar\n    # pathModelF format: <path>/embedNet_G.pth.tar\n    if np.all([key in opt.keys() for key in [\'paramsFile\',\'pathModelF\',\'pathModelG\']]):\n\n        if (os.path.isfile(opt[\'paramsFile\']) and \\\n             os.path.isfile(opt[\'pathModelF\']) and \\\n             os.path.isfile(opt[\'pathModelG\'])):\n            print(\'loading from params: %s\' % (opt[\'paramsFile\']))\n            print(\'loading model F: %s\' % (opt[\'pathModelF\']))\n            print(\'loading model G: %s\' % (opt[\'pathModelG\']))\n            network.embedModel.load(opt[\'paramsFile\'],\n                                    opt[\'pathModelF\'],\n                                    opt[\'pathModelG\'])\n\n    cost = 0\n    timer = time.time()\n\n    #################################################################\n    ############ Meta-training\n    #################################################################\n\n    # Init optimizer\n    optimizer = create_optimizer(opt, network.embedModel)\n\n    # set net for training\n    network.set(\'training\')\n\n    # train episode loop\n    for episodeTrain,(x_support_set, y_support_set, x_target, target_y) in enumerate(data[\'train\']):\n\n        # Re-arange the Target vectors between [0..nClasses_train]\n        dictLabels, dictLabelsInverse = util.createDictLabels(y_support_set)\n        y_support_set = util.fitLabelsToRange(dictLabels, y_support_set)\n        target_y = util.fitLabelsToRange(dictLabels, target_y)\n\n        # Convert them in Variables\n        input = {}\n        input[\'trainInput\'] = Variable(x_support_set).float()\n        input[\'trainTarget\'] = Variable(y_support_set,requires_grad=False).long()\n        input[\'testInput\'] = Variable(x_target).float()\n        input[\'testTarget\'] = Variable(target_y,requires_grad=False).long()\n\n        # Convert to GPU if needed\n        if opt[\'useCUDA\']:\n            input[\'trainInput\'] = input[\'trainInput\'].cuda()\n            input[\'trainTarget\'] = input[\'trainTarget\'].cuda()\n            input[\'testInput\'] = input[\'testInput\'].cuda()\n            input[\'testTarget\'] = input[\'testTarget\'].cuda()\n\n        output, loss = network(opt,input)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        # Adjust learning rate\n        optimizer = adjust_learning_rate(opt, optimizer)\n\n        cost = cost + loss\n\n        # update stats\n        values_pred, indices_pred = torch.max(output, 1)\n        target_y = util.fitLabelsToRange(dictLabelsInverse, target_y)\n        indices_pred = util.fitLabelsToRange(dictLabelsInverse, indices_pred.cpu().data)\n        trainConf_pred.append(indices_pred.numpy())\n        trainConf_gt.append(target_y.numpy())\n\n        if episodeTrain % opt[\'printPer\'] == 0:\n            trainConf_pred = np.concatenate(trainConf_pred, axis=0)\n            trainConf_gt = np.concatenate(trainConf_gt, axis=0)\n            target_names = [str(i) for i in np.unique(trainConf_gt)]\n            print(\n                \'Training Episode: [{}/{} ({:.0f}%)]\\tLoss: {:.3f}. Elapsed: {:.4f} s\'.format(\n                    episodeTrain, len(data[\'train\']), 100. * episodeTrain / len(data[\'train\']),\n                    (cost.cpu().data.numpy() / opt[\'printPer\'])[0],time.time() - timer))\n            print(classification_report(trainConf_gt, trainConf_pred,\n                                        target_names=target_names))\n            # Set to 0\n            trainConf_pred = []\n            trainConf_gt = []\n\n            #################################################################\n            ############ Meta-evaluation\n            #################################################################\n\n            timerEval = time.time()\n\n            # evaluate validation set\n            network.set(\'evaluate\')\n            # validation episode loop\n            for episodeValidation, (x_support_set, y_support_set, x_target, target_y) in enumerate(data[\'validation\']):\n\n                # Re-arange the Target vectors between [0..nClasses_train]\n                dictLabels, dictLabelsInverse = util.createDictLabels(y_support_set)\n                y_support_set = util.fitLabelsToRange(dictLabels, y_support_set)\n                target_y = util.fitLabelsToRange(dictLabels, target_y)\n                unique_labels = dictLabels.keys()\n\n                # k-shot loop\n                for k in opt[\'nTestShot\']:\n\n                    # Select k samples from each class from x_support_set and\n                    indexes_selected = []\n                    for k_selected in unique_labels:\n                        selected = np.random.choice(np.squeeze(np.where(y_support_set.numpy() == dictLabels[k_selected]))\n                                         ,k, False)\n                        indexes_selected.append(selected)\n\n                    # Select the k-shot examples from the Tensors\n                    x_support_set_k = x_support_set[torch.from_numpy(np.squeeze(indexes_selected).flatten())]\n                    y_support_set_k = y_support_set[torch.from_numpy(np.squeeze(indexes_selected).flatten())]\n\n                    # Convert them in Variables\n                    input = {}\n                    input[\'trainInput\'] = Variable(x_support_set_k).float()\n                    input[\'trainTarget\'] = Variable(y_support_set_k, requires_grad=False).long()\n                    input[\'testInput\'] = Variable(x_target).float()\n                    input[\'testTarget\'] = Variable(target_y, requires_grad=False).long()\n\n                    # Convert to GPU if needed\n                    if opt[\'useCUDA\']:\n                        input[\'trainInput\'] = input[\'trainInput\'].cuda()\n                        input[\'trainTarget\'] = input[\'trainTarget\'].cuda()\n                        input[\'testInput\'] = input[\'testInput\'].cuda()\n                        input[\'testTarget\'] = input[\'testTarget\'].cuda()\n\n                    output = network(opt, input)\n\n                    # update stats validation\n                    values_pred, indices_pred = torch.max(output, 1)\n                    target_y = util.fitLabelsToRange(dictLabelsInverse, target_y)\n                    indices_pred = util.fitLabelsToRange(dictLabelsInverse, indices_pred.cpu().data)\n                    valConf_pred[k].append(indices_pred.numpy())\n                    valConf_gt[k].append(target_y.numpy())\n\n            for k in opt[\'nTestShot\']:\n                valConf_pred[k] = np.concatenate(valConf_pred[k], axis=0)\n                valConf_gt[k] = np.concatenate(valConf_gt[k], axis=0)\n                print(\'Validation: {}-shot Acc: {:.3f}. Elapsed: {:.4f} s.\'.format(\n                        k,accuracy_score(valConf_gt[k],valConf_pred[k]),time.time() - timerEval))\n                target_names = [str(i) for i in np.unique(valConf_gt[k])]\n                print(classification_report(valConf_gt[k], valConf_pred[k],\n                                            target_names=target_names))\n                valConf_pred[k] = []\n                valConf_gt[k] = []\n\n        cost = 0\n        timer = time.time()\n        network.set(\'training\')\n\n    #################################################################\n    ############ Meta-testing\n    #################################################################\n    # set net for testing\n    network.set(\'evaluate\')\n\n    results = []\n    for n in np.arange(len(opt[\'nTest\'])):\n        # validation episode loop\n        for episodeTest, (x_support_set, y_support_set, x_target, target_y) in enumerate(data[\'test\'][n]):\n\n            # Re-arange the Target vectors between [0..nClasses_train]\n            dictLabels, dictLabelsInverse = util.createDictLabels(y_support_set)\n            y_support_set = util.fitLabelsToRange(dictLabels, y_support_set)\n            target_y = util.fitLabelsToRange(dictLabels, target_y)\n            unique_labels = dictLabels.keys()\n\n            # k-shot loop\n            for k in opt[\'nTestShot\']:\n\n                # Select k samples from each class from x_support_set and\n                indexes_selected = []\n                for k_selected in unique_labels:\n                    selected = np.random.choice(np.squeeze(np.where((y_support_set.numpy() == dictLabels[k_selected])))\n                                                , k, False)\n                    indexes_selected.append(selected)\n\n                # Select the k-shot examples from the Tensors\n                x_support_set_k = x_support_set[torch.from_numpy(np.squeeze(indexes_selected).flatten())]\n                y_support_set_k = y_support_set[torch.from_numpy(np.squeeze(indexes_selected).flatten())]\n\n                # Convert them in Variables\n                input = {}\n                input[\'trainInput\'] = Variable(x_support_set_k).float()\n                input[\'trainTarget\'] = Variable(y_support_set_k, requires_grad=False).long()\n                input[\'testInput\'] = Variable(x_target).float()\n                input[\'testTarget\'] = Variable(target_y, requires_grad=False).long()\n\n                # Convert to GPU if needed\n                if opt[\'useCUDA\']:\n                    input[\'trainInput\'] = input[\'trainInput\'].cuda()\n                    input[\'trainTarget\'] = input[\'trainTarget\'].cuda()\n                    input[\'testInput\'] = input[\'testInput\'].cuda()\n                    input[\'testTarget\'] = input[\'testTarget\'].cuda()\n\n                output = network(opt, input)\n\n                # update stats test\n                values_pred, indices_pred = torch.max(output, 1)\n                target_y = util.fitLabelsToRange(dictLabelsInverse, target_y)\n                indices_pred = util.fitLabelsToRange(dictLabelsInverse, indices_pred.cpu().data)\n                testConf_pred[k].append(indices_pred.numpy())\n                testConf_gt[k].append(target_y.numpy())\n\n        for k in opt[\'nTestShot\']:\n            acc = []\n            for i in np.arange(len(testConf_gt[k])):\n                acc.append(accuracy_score(testConf_gt[k][i],testConf_pred[k][i]))\n            low = np.mean(acc) - 1.96*(np.std(acc)/math.sqrt(len(acc)))\n            high = np.mean(acc) + 1.96 * (np.std(acc) / math.sqrt(len(acc)))\n            print(\'Test: nTest: {}. {}-shot. mAcc: {:.3f}. low mAcc: {:.3f}. high mAcc: {:.3f}.\'.format(\n                opt[\'nTest\'][n],k,np.mean(acc),low, high))\n            testConf_pred[k] = []\n            testConf_gt[k] = []\n            results.append((opt[\'nTest\'][n],k,np.mean(acc),low, high))\n\n    return results'"
model/baselines/simple-embedding.py,5,"b""##+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n## Created by: Albert Berenguel\n## Computer Vision Center (CVC). Universitat Autonoma de Barcelona\n## Email: aberenguel@cvc.uab.es\n## Copyright (c) 2017\n##\n## This source code is licensed under the MIT-style license found in the\n## LICENSE file in the root directory of this source tree\n##+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n\nimport os\nimport torch\nimport torch.nn as nn\nimport importlib\nimport pickle\n\nclass SimpleEmbedding():\n    def __init__(self, opt):\n        self.opt = opt # Store the parameters\n        self.buildModels(self.opt)\n        self.setCuda()\n\n    # Build F and G models\n    def buildModels(self,opt):\n        modelF = importlib.import_module(opt['learner']).build(opt)\n        self.embedNetF = modelF.net # F function\n        modelG = importlib.import_module(opt['learner']).build(opt)\n        self.embedNetG = modelG.net # G function\n\n    # Build list of parameters for optim\n    def parameters(self):\n        # TODO: why in the original code creates a dictionary with the same\n        # parameters. model.params = {f=paramsG, g=paramsG}\n        return list(self.embedNetG.parameters()) + list(self.embedNetG.parameters())\n\n    # Set training or evaluation mode\n    def set(self,mode):\n        if mode == 'training':\n            self.embedNetF.train()\n            self.embedNetG.train()\n        elif mode == 'evaluate':\n            self.embedNetF.eval()\n            self.embedNetG.eval()\n        else:\n            print('model.set: undefined mode - %s' % (mode))\n\n    def isTraining(self):\n        return self.embedNetF.training\n\n    def default(self, dfDefault):\n        self.df = dfDefault\n\n    def embedF(self, input, g = [], K = []):\n        return self.embedNetF(input)\n\n    def embedG(self, input):\n        return self.embedNetG(input)\n\n    def save(self, path = './data'):\n        # Save the opt parameters\n        optParametersFile = open(os.path.join(path,'SimpleEmbedding_opt.pkl'), 'wb')\n        pickle.dump(self.opt, optParametersFile)\n        optParametersFile.close()\n        # Clean not needed data of the models\n        self.embedNetF.clearState()\n        self.embedNetG.clearState()\n        torch.save(self.embedNetF.state_dict(), os.path.join(path,'embedNetF.pth.tar'))\n        torch.save(self.embedNetG.state_dict(), os.path.join(path, 'embedNetG.pth.tar'))\n\n    def load(self, pathParams, pathModelF, pathModelG):\n        # Load opt parameters 'SimpleEmbedding_opt.pkl'\n        optParametersFile = open(pathParams, 'rb')\n        self.opt = pickle.load(optParametersFile)\n        optParametersFile.close()\n        # build the models\n        self.buildModels(self.opt)\n        # Load the weights and biases of F and G\n        checkpoint = torch.load(pathModelF)\n        self.embedNetF.load_state_dict(checkpoint['state_dict'])\n        checkpoint = torch.load(pathModelG)\n        self.embedNetG.load_state_dict(checkpoint['state_dict'])\n        # Set cuda\n        self.setCuda()\n\n    def setCuda(self, value = 'default'):\n        # If value is a string then use self.opt\n        # If it is not a string then it should be True or False\n        if type(value) == str:\n            value = self.opt['useCUDA']\n        else:\n            assert(type(value)==bool)\n\n        if value == True:\n            print('Check CUDA')\n            self.embedNetF.cuda()\n            self.embedNetG.cuda()\n        else:\n            self.embedNetF.cpu()\n            self.embedNetG.cpu()\n\ndef build(opt):\n    model = SimpleEmbedding(opt)\n    return model\n\n\n"""
model/lstm/__init__.py,0,b''
model/lstm/bnlstm.py,34,"b'#PyTorch implementation of Recurrent Batch Normalization\n# proposed by Cooijmans et al. (2017). https://arxiv.org/abs/1603.09025\n# Source code from: https://github.com/jihunchoi/recurrent-batch-normalization-pytorch\n\nimport torch\nfrom torch import nn\nfrom torch.autograd import Variable\nfrom torch.nn import functional, init\n\nclass SeparatedBatchNorm1d(nn.Module):\n\n    """"""\n    A batch normalization module which keeps its running mean\n    and variance separately per timestep.\n    """"""\n\n    def __init__(self, num_features, max_length, eps=1e-5, momentum=0.1,\n                 affine=True):\n        """"""\n        Most parts are copied from\n        torch.nn.modules.batchnorm._BatchNorm.\n        """"""\n\n        super(SeparatedBatchNorm1d, self).__init__()\n        self.num_features = num_features\n        self.max_length = max_length\n        self.affine = affine\n        self.eps = eps\n        self.momentum = momentum\n        if self.affine:\n            self.weight = nn.Parameter(torch.FloatTensor(num_features))\n            self.bias = nn.Parameter(torch.FloatTensor(num_features))\n        else:\n            self.register_parameter(\'weight\', None)\n            self.register_parameter(\'bias\', None)\n        for i in range(max_length):\n            self.register_buffer(\n                \'running_mean_{}\'.format(i), torch.zeros(num_features))\n            self.register_buffer(\n                \'running_var_{}\'.format(i), torch.ones(num_features))\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        for i in range(self.max_length):\n            running_mean_i = getattr(self, \'running_mean_{}\'.format(i))\n            running_var_i = getattr(self, \'running_var_{}\'.format(i))\n            running_mean_i.zero_()\n            running_var_i.fill_(1)\n        if self.affine:\n            self.weight.data.uniform_()\n            self.bias.data.zero_()\n\n    def _check_input_dim(self, input_):\n        if input_.size(1) != self.running_mean_0.nelement():\n            raise ValueError(\'got {}-feature tensor, expected {}\'\n                             .format(input_.size(1), self.num_features))\n\n    def forward(self, input_, time):\n        self._check_input_dim(input_)\n        if time >= self.max_length:\n            time = self.max_length - 1\n        running_mean = getattr(self, \'running_mean_{}\'.format(time))\n        running_var = getattr(self, \'running_var_{}\'.format(time))\n        return functional.batch_norm(\n            input=input_, running_mean=running_mean, running_var=running_var,\n            weight=self.weight, bias=self.bias, training=self.training,\n            momentum=self.momentum, eps=self.eps)\n\n    def __repr__(self):\n        return (\'{name}({num_features}, eps={eps}, momentum={momentum},\'\n                \' max_length={max_length}, affine={affine})\'\n                .format(name=self.__class__.__name__, **self.__dict__))\n\n\nclass LSTMCell(nn.Module):\n\n    """"""A basic LSTM cell.""""""\n\n    def __init__(self, input_size, hidden_size, use_bias=True):\n        """"""\n        Most parts are copied from torch.nn.LSTMCell.\n        """"""\n\n        super(LSTMCell, self).__init__()\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.use_bias = use_bias\n        self.weight_ih = nn.Parameter(\n            torch.FloatTensor(input_size, 4 * hidden_size))\n        self.weight_hh = nn.Parameter(\n            torch.FloatTensor(hidden_size, 4 * hidden_size))\n        if use_bias:\n            self.bias = nn.Parameter(torch.FloatTensor(4 * hidden_size))\n        else:\n            self.register_parameter(\'bias\', None)\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        """"""\n        Initialize parameters following the way proposed in the paper.\n        """"""\n\n        init.orthogonal(self.weight_ih.data)\n        weight_hh_data = torch.eye(self.hidden_size)\n        weight_hh_data = weight_hh_data.repeat(1, 4)\n        self.weight_hh.data.set_(weight_hh_data)\n        # The bias is just set to zero vectors.\n        if self.use_bias:\n            init.constant(self.bias.data, val=0)\n\n    def forward(self, input_, hx):\n        """"""\n        Args:\n            input_: A (batch, input_size) tensor containing input\n                features.\n            hx: A tuple (h_0, c_0), which contains the initial hidden\n                and cell state, where the size of both states is\n                (batch, hidden_size).\n        Returns:\n            h_1, c_1: Tensors containing the next hidden and cell state.\n        """"""\n\n        h_0, c_0 = hx\n        batch_size = h_0.size(0)\n        bias_batch = (self.bias.unsqueeze(0)\n                      .expand(batch_size, *self.bias.size()))\n        wh_b = torch.addmm(bias_batch, h_0, self.weight_hh)\n        wi = torch.mm(input_, self.weight_ih)\n        f, i, o, g = torch.split(wh_b + wi,\n                                 split_size=self.hidden_size, dim=1)\n        c_1 = torch.sigmoid(f)*c_0 + torch.sigmoid(i)*torch.tanh(g)\n        h_1 = torch.sigmoid(o) * torch.tanh(c_1)\n        return h_1, c_1\n\n    def __repr__(self):\n        s = \'{name}({input_size}, {hidden_size})\'\n        return s.format(name=self.__class__.__name__, **self.__dict__)\n\n\nclass BNLSTMCell(nn.Module):\n\n    """"""A BN-LSTM cell.""""""\n\n    def __init__(self, input_size, hidden_size, max_length, use_bias=True):\n\n        super(BNLSTMCell, self).__init__()\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.max_length = max_length\n        self.use_bias = use_bias\n        self.weight_ih = nn.Parameter(\n            torch.FloatTensor(input_size, 4 * hidden_size))\n        self.weight_hh = nn.Parameter(\n            torch.FloatTensor(hidden_size, 4 * hidden_size))\n        if use_bias:\n            self.bias = nn.Parameter(torch.FloatTensor(4 * hidden_size))\n        else:\n            self.register_parameter(\'bias\', None)\n        # BN parameters\n        self.bn_ih = SeparatedBatchNorm1d(\n            num_features=4 * hidden_size, max_length=max_length)\n        self.bn_hh = SeparatedBatchNorm1d(\n            num_features=4 * hidden_size, max_length=max_length)\n        self.bn_c = SeparatedBatchNorm1d(\n            num_features=hidden_size, max_length=max_length)\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        """"""\n        Initialize parameters following the way proposed in the paper.\n        """"""\n\n        # The input-to-hidden weight matrix is initialized orthogonally.\n        init.orthogonal(self.weight_ih.data)\n        # The hidden-to-hidden weight matrix is initialized as an identity\n        # matrix.\n        weight_hh_data = torch.eye(self.hidden_size)\n        weight_hh_data = weight_hh_data.repeat(1, 4)\n        self.weight_hh.data.set_(weight_hh_data)\n        # The bias is just set to zero vectors.\n        init.constant(self.bias.data, val=0)\n        # Initialization of BN parameters.\n        self.bn_ih.reset_parameters()\n        self.bn_hh.reset_parameters()\n        self.bn_c.reset_parameters()\n        self.bn_ih.bias.data.fill_(0)\n        self.bn_hh.bias.data.fill_(0)\n        self.bn_ih.weight.data.fill_(0.1)\n        self.bn_hh.weight.data.fill_(0.1)\n        self.bn_c.weight.data.fill_(0.1)\n\n    def forward(self, input_, hx, time):\n        """"""\n        Args:\n            input_: A (batch, input_size) tensor containing input\n                features.\n            hx: A tuple (h_0, c_0), which contains the initial hidden\n                and cell state, where the size of both states is\n                (batch, hidden_size).\n            time: The current timestep value, which is used to\n                get appropriate running statistics.\n        Returns:\n            h_1, c_1: Tensors containing the next hidden and cell state.\n        """"""\n\n        h_0, c_0 = hx\n        batch_size = h_0.size(0)\n        bias_batch = (self.bias.unsqueeze(0)\n                      .expand(batch_size, *self.bias.size()))\n        wh = torch.mm(h_0, self.weight_hh)\n        wi = torch.mm(input_, self.weight_ih)\n        bn_wh = self.bn_hh(wh, time=time)\n        bn_wi = self.bn_ih(wi, time=time)\n        f, i, o, g = torch.split(bn_wh + bn_wi + bias_batch,\n                                 split_size=self.hidden_size, dim=1)\n        c_1 = torch.sigmoid(f)*c_0 + torch.sigmoid(i)*torch.tanh(g)\n        h_1 = torch.sigmoid(o) * torch.tanh(self.bn_c(c_1, time=time))\n        return h_1, c_1\n\n\nclass LSTM(nn.Module):\n\n    """"""A module that runs multiple steps of LSTM.""""""\n\n    def __init__(self, cell_class, input_size, hidden_size, num_layers=1,\n                 use_bias=True, batch_first=False, dropout=0, **kwargs):\n        super(LSTM, self).__init__()\n        self.cell_class = cell_class\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        self.use_bias = use_bias\n        self.batch_first = batch_first\n        self.dropout = dropout\n\n        self.cells = []\n        for layer in range(num_layers):\n            layer_input_size = input_size if layer == 0 else hidden_size\n            cell = cell_class(input_size=layer_input_size,\n                              hidden_size=hidden_size,\n                              **kwargs)\n            self.cells.append(cell)\n            setattr(self, \'cell_{}\'.format(layer), cell)\n        self.dropout_layer = nn.Dropout(dropout)\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        for cell in self.cells:\n            cell.reset_parameters()\n\n    @staticmethod\n    def _forward_rnn(cell, input_, length, hx):\n        max_time = input_.size(0)\n        output = []\n        for time in range(max_time):\n            if isinstance(cell, BNLSTMCell):\n                h_next, c_next = cell(input_=input_[time], hx=hx, time=time)\n            else:\n                h_next, c_next = cell(input_=input_[time], hx=hx)\n            mask = (time < length).float().unsqueeze(1).expand_as(h_next)\n            h_next = h_next*mask + hx[0]*(1 - mask)\n            c_next = c_next*mask + hx[1]*(1 - mask)\n            hx_next = (h_next, c_next)\n            output.append(h_next)\n            hx = hx_next\n        output = torch.stack(output, 0)\n        return output, hx\n\n    def forward(self, input_, length=None, hx=None):\n        if self.batch_first:\n            input_ = input_.transpose(0, 1)\n        max_time, batch_size, _ = input_.size()\n        if length is None:\n            length = Variable(torch.LongTensor([max_time] * batch_size))\n            if input_.is_cuda:\n                length = length.cuda()\n        if hx is None:\n            hx = Variable(input_.data.new(batch_size, self.hidden_size).zero_())\n            hx = (hx, hx)\n        h_n = []\n        c_n = []\n        layer_output = None\n        for layer in range(self.num_layers):\n            layer_output, (layer_h_n, layer_c_n) = LSTM._forward_rnn(\n                cell=self.cells[layer], input_=input_, length=length, hx=hx)\n            input_ = self.dropout_layer(layer_output)\n            h_n.append(layer_h_n)\n            c_n.append(layer_c_n)\n        output = layer_output\n        h_n = torch.stack(h_n, 0)\n        c_n = torch.stack(c_n, 0)\n        return output, (h_n, c_n)\n\n\'\'\'\nclass RecurrentLSTMNetwork(nn.Module):\n    def __init__(self, opt):\n        super(RecurrentLSTMNetwork, self).__init__()\n\n        self.inputFeatures = opt[\'inputFeatures\'] if \'inputFeatures\' in opt.keys() else 10\n        self.hiddenFeatures = opt[\'hiddenFeatures\'] if \'hiddenFeatures\' in opt.keys() else 100\n        self.outputType = opt[\'outputType\'] if \'outputType\' in opt.keys() else \'last\' # \'last\' or \'all\'\n        self.batchNormalization = opt[\'batchNormalization\'] if \'batchNormalization\' in opt.keys() else False\n        self.maxBatchNormalizationLayers = opt[\'maxBatchNormalizationLayers\'] if \'batchNormalization\' in opt.keys() else 10\n\n        # containers\n        self.layers = {}\n\n        # parameters\n        self.p = {}\n        self.p[\'W\'] = torch.zeros(self.inputFeatures+self.hiddenFeatures,4 * self.hiddenFeatures)\n\n        #TODO: delete this line. only for debugging\n        self.batchNormalization = True\n\n        if self.batchNormalization:\n            # TODO: check if nn.BatchNorm1d or torch.legacy.nn.BatchNormalization\n            # translation and scaling parameters are shared across time.\n            lstm_bn = nn.BatchNorm1d(4*self.hiddenFeatures)\n            cell_bn = nn.BatchNorm1d(self.hiddenFeatures)\n            self.layers = {\'lstm_bn\':[lstm_bn],\'cell_bn\':[cell_bn]}\n\n            for i in range(2,self.maxBatchNormalizationLayers):\n                lstm_bn = nn.BatchNorm1d(4*self.hiddenFeatures)\n                cell_bn = nn.BatchNorm1d(self.hiddenFeatures)\n                self.layers[\'lstm_bn\'].append(lstm_bn)\n                self.layers[\'cell_bn\'].append(cell_bn)\n\n            # Initializing scaling to <1 is recommended for LSTM batch norm\n            self.layers[\'lstm_bn\'][0].weight.data.fill_(0.1)\n            self.layers[\'lstm_bn\'][0].bias.data.zero_()\n            self.layers[\'cell_bn\'][0].weight.data.fill_(0.1)\n            self.layers[\'cell_bn\'][0].bias.data.zero_()\n        else:\n            self.p[\'b\'] = torch.zeros(1, 4*self.hiddenFeatures)\n\n    def forward(self, x, prevState = None ):\n\n        # dimensions\n        if len(x.size()) == 2: x = x.unsqueeze(0)\n        batch = x.size(0)\n        steps = x.size(1)\n\n        if prevState == None: prevState = {}\n        hs = {}\n        cs = {}\n        for t in range(steps):\n            # xt\n            xt = x[:,t,:]\n            # prev h and pre c\n            hp = hs[t-1] or prevState.h or torch.zeros()\n        a = 0\n\'\'\''"
model/lstm/learner.py,3,"b""import torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\nimport importlib\nimport numpy as np\n\nclass Learner(nn.Module):\n    def __init__(self, opt):\n        super(Learner, self).__init__()\n\n        # Note: we are using two networks to simulate learner where one network\n        # is used for backward pass on test set and the other is used simply to get\n        # gradients which serve as input to meta-learner.\n        # This is a simple way to make computation graph work\n        # so that it doesn't include gradients of learner\n\n        # Create another network with only shared 'running_mean' and 'running_var'\n        # this weights can be found in BatchNormalization layers (or InstanceNormalization)\n        # In torch with the instruction: model.net:clone('running_mean', 'running_var')\n        # it is already done but with Pytorch we need to copy those parameters with\n        # state_dict and load_state_dict every time we want to use one of the shared\n        # networks.\n\n        # Add dimension filters for the cnn\n        opt['nFilters'] = 32\n        # Create 4 layers with batch norm. Share layers between self.model and self.modelF\n        self.bn_layers = []\n        for i in range(4):\n            if 'BN_momentum' in opt.keys():\n                self.bn_layers.append(nn.BatchNorm2d(opt['nFilters'],\n                                                momentum=opt['BN_momentum']))\n            else:\n                self.bn_layers.append(nn.BatchNorm2d(opt['nFilters']))\n        opt['bnorm2d'] = self.bn_layers\n\n        # local embedding model\n        self.model = importlib.import_module(opt['learner']).build(opt)\n        self.modelF = importlib.import_module(opt['learner']).build(opt)\n        self.nParams = self.modelF.nParams\n        self.params = {param[0]: param[1] for param in self.modelF.net.named_parameters()}\n\n    def unflattenParams_net(self,flatParams):\n        flatParams = flatParams.squeeze()\n        indx = 0\n        for param in self.model.net.parameters():\n            lengthParam = param.view(-1).size()[0]\n            param = flatParams[indx:lengthParam].view_as(param).clone()\n\n    def forward(self, inputs, targets ):\n\n        output = self.modelF.net(inputs)\n        loss = self.modelF.criterion(output, targets)\n        return output, loss\n\n    def feval(self, inputs, targets):\n        # reset gradients\n        self.model.net.zero_grad()\n        # evaluate function for complete mini batch\n        outputs = self.model.net(inputs)\n        loss = self.model.criterion(outputs, targets)\n        loss.backward()\n        grads = torch.cat([param.grad.view(-1) for param in self.model.net.parameters()], 0)\n        return grads,loss\n\n    def reset(self):\n        self.model.net.reset()\n        self.modelF.net.reset()\n\n    # Set training or evaluation mode\n    def set(self,mode):\n        if mode == 'training':\n            self.model.net.train()\n            self.modelF.net.train()\n        elif mode == 'evaluate':\n            self.model.net.eval()\n            self.modelF.net.eval()\n        else:\n            print('model.set: undefined mode - %s' % (mode))\n\n    def setCuda(self, value = True):\n        # If value is a string then use self.opt\n        # If it is not a string then it should be True or False\n        if value == True:\n            self.model.net.cuda()\n            self.modelF.net.cuda()\n        else:\n            self.model.net.cpu()\n            self.modelF.net.cpu()"""
model/lstm/lstmhelper.py,16,"b'import torch\nfrom torch import nn\nfrom torch.autograd import Variable\n\nP = Variable(torch.FloatTensor(1).fill_(10))\nexpP = Variable(torch.exp(P.data))\nnegExpP = Variable(torch.exp(-P.data))\n\ndef preProc1(x):\n    # Access the global variables\n    global P,expP,negExpP\n    P = P.type_as(x)\n    expP = expP.type_as(x)\n    negExpP = negExpP.type_as(x)\n\n    # Create a variable filled with -1. Second part of the condition\n    z = Variable(torch.zeros(x.size()).fill_(-1)).type_as(x)\n    absX = torch.abs(x)\n    cond1 = torch.gt(absX, negExpP)\n    if (torch.sum(cond1) > 0).data.all():\n        x1 = torch.log(torch.abs(x[cond1]))/P\n        z[cond1] = x1\n    return z\n\ndef preProc2(x):\n    # Access the global variables\n    global P, expP, negExpP\n    P = P.type_as(x)\n    expP = expP.type_as(x)\n    negExpP = negExpP.type_as(x)\n\n    # Create a variable filled with -1. Second part of the condition\n    z = Variable(torch.zeros(x.size())).type_as(x)\n    absX = torch.abs(x)\n    cond1 = torch.gt(absX, negExpP)\n    cond2 = torch.le(absX, negExpP)\n    if (torch.sum(cond1) > 0).data.all():\n        x1 = torch.sign(x[cond1])\n        z[cond1] = x1\n    if (torch.sum(cond2) > 0).data.all():\n        x2 = x[cond2]*expP\n        z[cond2] = x2\n    return z\n\ndef preprocess(grad,loss):\n\n    #preGrad = Variable(grad.data.new(grad.data.size()[0], 1, 2).zero_())\n    #preGrad = grad.expand(grad.data.size()[0], 1, 2)\n    preGrad = grad.clone().expand(grad.data.size()[0], 1, 2)\n    preGrad[:, :, 0] = preProc1(grad)\n    preGrad[:, :, 1] = preProc2(grad)\n\n    #lossT = Variable(loss.data.new(1,1,1).zero_())\n    #lossT[0] = loss\n    #preLoss = Variable(loss.data.new(1,1,2).zero_())\n    #preLoss = loss.expand(1, 1, 2)\n    preLoss = loss.clone().expand(1, 1, 2)\n    preLoss[:, :, 0] = preProc1(loss)\n    preLoss[:, :, 1] = preProc2(loss)\n    return preGrad,preLoss\n'"
model/lstm/metaLearner.py,10,"b'import torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\nimport importlib\nimport model.lstm.bnlstm as bnlstm\nimport model.lstm.metalstm as metalstm\nfrom model.lstm.recurrentLSTMNetwork import RecurrentLSTMNetwork\nfrom model.lstm.lstmhelper import preprocess\nfrom utils import util\nfrom visualize.visualize import make_dot\n\nclass MetaLearner(nn.Module):\n    def __init__(self, opt):\n        super(MetaLearner, self).__init__()\n\n        self.nHidden = opt[\'nHidden\'] if \'nHidden\' in opt.keys() else 20\n        self.maxGradNorm = opt[\'maxGradNorm\'] if \'maxGradNorm\' in opt.keys() else 0.25\n\n        #inputFeatures = 4 #loss(2) + preGrad(2) = 4\n        inputFeatures = 2  # loss(2) + preGrad(2) = 4\n        batchNormalization1 = opt[\'BN1\'] if \'BN1\' in opt.keys() else False\n        maxBatchNormalizationLayers = opt[\'steps\'] if \'steps\' in opt.keys() else 1\n        batchNormalization1 = False\n        if batchNormalization1:\n            self.lstm = bnlstm.LSTM(cell_class=bnlstm.BNLSTMCell, input_size=inputFeatures,\n                         hidden_size=self.nHidden, batch_first=True,\n                         max_length=maxBatchNormalizationLayers)\n        else:\n            self.lstm = nn.LSTM(input_size=inputFeatures,\n                                 hidden_size=self.nHidden,\n                                 batch_first=True,\n                                 num_layers=maxBatchNormalizationLayers)\n\n        # set initial hidden layer and cell state\n        # num_layers * num_directions, batch, hidden_size\n        batch_size = 1\n        self.lstm_h0_c0 = None\n\n        #self.lstm_c0 = Variable(torch.rand(self.lstm.num_layers, batch_size, self.lstm.hidden_size),\n        #                        requires_grad=False).cuda()\n        #self.lstm_h0 = Variable(torch.rand(self.lstm.num_layers, batch_size, self.lstm.hidden_size),\n        #                        requires_grad=False).cuda()\n\n        # Meta-learner LSTM\n        # TODO: BatchNormalization in MetaLSTM\n        batchNormalization2 = opt[\'BN2\'] if \'BN2\' in opt.keys() else False\n        self.lstm2 = metalstm.MetaLSTM(input_size = opt[\'nParams\'],\n                             hidden_size = self.nHidden,\n                             batch_first=True,\n                             num_layers=maxBatchNormalizationLayers)\n\n        # set initial c0 and h0 states for lstm2\n        batch_size = 1\n        self.lstm2_fS_iS_cS_deltaS = None\n\n        # Join parameters as input for optimizer\n        self.params = lambda: list(self.lstm.named_parameters()) + list(self.lstm2.named_parameters())\n        self.params = { param[0]:param[1] for param in self.params()}\n\n        # initialize weights learner\n        for names in self.lstm._all_weights:\n            for name in filter(lambda n: ""weight"" in n,  names):\n                weight = getattr(self.lstm, name)\n                weight.data.uniform_(-0.01, 0.01)\n\n        # initialize weights meta-learner for all layers.\n        for params in self.lstm2.named_parameters():\n            if \'WF\' in names[0] or names[0] in names[0] or \'cI\' in params[0]:\n                params[1].data.uniform_(-0.01, 0.01)\n\n        # want initial forget value to be high and input value\n        # to be low so that model starts with gradient descent\n        for params in self.lstm2.named_parameters():\n            if ""cell_0.bF"" in names[0]:\n                params[0].data.uniform_(4, 5)\n            if ""cell_0.bI"" in names[0]:\n                params[0].data.uniform_(-4, -5)\n\n        # Set initial cell state = learner\'s initial parameters\n        initialParams = torch.cat([value.view(-1) for key,value in opt[\'learnerParams\'].items()], 0)\n        #self.lstm2.cells[0].cI = initialParams.unsqueeze(1).clone()\n\n        #torch.nn.Parameter(initial_param[\'weight\'])\n        for params in self.lstm2.named_parameters():\n            if ""cell_0.cI"" in params[0]:\n                params[1].data = initialParams.data.clone()\n        # self.lstm2.cells[0].cI.data = initialParams.view_as(self.lstm2.cells[0].cI).clone()\n        a = 0\n\n        #for params in self.lstm2.parameters():\n        #    params.retain_grad()\n\n        #for params in self.lstm.parameters():\n        #    params.retain_grad()\n\n\n    def forward(self, learner, trainInput, trainTarget, testInput, testTarget\n                , steps, batchSize, evaluate = False ):\n\n        trainSize = trainInput.size(0)\n\n        # reset parameters for each dataset\n        # Modules with learnable parameters have a reset(). This function\n        # allows to re-initialize parameters. It\'s also used for weight\n        # initialization.\n        learner.reset()\n        learner.set(\'training\')\n\n        # Set learner\'s initial parameters = initial cell state\n        util.unflattenParams(learner.model, self.lstm2.cells[0].cI)\n\n        #for params in self.lstm2.named_parameters():\n        #    if ""cell_0.cI"" in params[0]:\n        #        util.unflattenParams(learner.model, params[1])\n        #util.unflattenParams(learner.model, self.lstm2.cells[0].cI)\n\n        idx = 0\n        for s in range(steps):\n            for i in range(0,trainSize,batchSize):\n                # get image input & label\n                x = trainInput[i:batchSize,:]\n                y = trainTarget[i:batchSize]\n\n                #if idx > 0:\n                #    # break computational graph\n                #    learnerParams = output.detach()\n                #    # Unflatten params and copy parameters to learner network\n                #    util.unflattenParams(learner.model,learnerParams)\n\n                # get gradient and loss w/r/t learnerParams for input+label\n                grad_model, loss_model = learner.feval(x,y)\n                grad_model = grad_model.view(grad_model.size()[0], 1, 1)\n\n                # Delete albert\n                inputs = torch.cat((grad_model, loss_model.expand_as(grad_model)), 2)\n                \'\'\'\n                # preprocess grad & loss by DeepMind ""Learning to learn""\n                preGrad, preLoss = preprocess(grad_model,loss_model)\n                # use meta-learner to get learner\'s next parameters\n                lossExpand = preLoss.expand_as(preGrad)\n                inputs = torch.cat((lossExpand,preGrad),2)\n                \'\'\'\n                output, self.lstm_h0_c0 = self.lstm(inputs, self.lstm_h0_c0)\n                self.lstm2_fS_iS_cS_deltaS = self.lstm2((output,grad_model),\n                                                                self.lstm2_fS_iS_cS_deltaS)\n\n                ## Delete\n                util.unflattenParams(learner.modelF, self.lstm2_fS_iS_cS_deltaS[2])\n                output, loss = learner(testInput, testTarget)\n                #g = make_dot(loss)\n                #g.render(\'/home/aberenguel/tmp/g.gv\', view=True)\n                #torch.autograd.grad(loss, list(self.lstm2.parameters()) + list(self.lstm.parameters()))\n                ######\n\n                # get the internal cell state\n                output = self.lstm2_fS_iS_cS_deltaS[2]\n\n                # Unflatten params and copy parameters to learner network\n                util.unflattenParams(learner.model, output)\n\n                idx = idx + 1\n\n        # Unflatten params and copy parameters to learner network\n        util.unflattenParams(learner.modelF, output)\n\n        ## get loss + predictions from learner.\n        ## use batch-stats when meta-training; otherwise, use running-stats\n        if evaluate:\n            learner.set(\'evaluate\')\n        # Do a dummy forward / backward pass to get the correct grads into learner\n        output, loss = learner(testInput, testTarget)\n\n        # replace the gradients with the lstm2\n        torch.autograd.grad(loss, self.lstm2.parameters())\n\n        return output, loss\n\n\n    def gradNorm(self, loss):\n\n        print(\'Grads lstm + lstm2:\')\n        for params in self.lstm.parameters():\n            print(params.grad)\n\n        for params in self.lstm2.parameters():\n            print(params.grad)\n        a = 0\n\n    def setCuda(self, value = True):\n        if value:\n            self.lstm.cuda()\n            self.lstm2.cuda()\n        else:\n            self.lstm.cpu()\n            self.lstm2.cpu()'"
model/lstm/metalstm.py,26,"b'import torch\nfrom torch import nn\nfrom torch.autograd import Variable\nfrom torch.nn import functional, init\n\nclass MetaLSTMCell(nn.Module):\n\n    """"""A basic LSTM cell.""""""\n\n    def __init__(self, input_size, hidden_size):\n        """"""\n        Most parts are copied from torch.nn.LSTMCell.\n        """"""\n        super(MetaLSTMCell, self).__init__()\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n\n        self.WF = nn.Parameter(torch.FloatTensor(hidden_size + 2, 1))\n        self.WI = nn.Parameter(torch.FloatTensor(hidden_size + 2, 1))\n        # initial cell state is a param\n        self.cI = nn.Parameter(torch.FloatTensor(input_size, 1))\n        self.bI = nn.Parameter(torch.FloatTensor(1, 1))\n        self.bF = nn.Parameter(torch.FloatTensor(1, 1))\n        self.m = nn.Parameter(torch.FloatTensor(1))\n\n        \'\'\'\n        self.WF = Variable(torch.FloatTensor(hidden_size + 2, 1), requires_grad=True)\n        self.WI = Variable(torch.FloatTensor(hidden_size + 2, 1), requires_grad=True)\n        # initial cell state is a param\n        self.cI = Variable(torch.FloatTensor(input_size, 1), requires_grad=True)\n        self.bI = Variable(torch.FloatTensor(1, 1), requires_grad=True)\n        self.bF = Variable(torch.FloatTensor(1, 1), requires_grad=True)\n        self.m = Variable(torch.FloatTensor(1), requires_grad=True)\n        \'\'\'\n\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        """"""\n        Initialize parameters \n        """"""\n        self.WF.data.uniform_(-0.01, 0.01)\n        self.WI.data.uniform_(-0.01, 0.01)\n        self.cI.data.uniform_(-0.01, 0.01)\n        self.bI.data.zero_()\n        self.bF.data.zero_()\n        self.m.data.zero_()\n\n    def forward(self, input_, grads_, hx):\n        """"""\n        Args:\n            input_: A (batch, input_size) tensor containing input\n                features.\n            hx: A tuple (h_0, c_0), which contains the initial hidden\n                and cell state, where the size of both states is\n                (batch, hidden_size).\n        Returns:\n            h_1, c_1: Tensors containing the next hidden and cell state.\n        """"""\n\n        # next forget, input gate\n        (fS, iS, cS, deltaS) = hx\n        fS = torch.cat((cS, fS), 1)\n        iS = torch.cat((cS, iS), 1)\n\n        fS = torch.mm(torch.cat((input_,fS), 1),self.WF)\n        fS += self.bF.expand_as(fS)\n\n        iS = torch.mm(torch.cat((input_,iS), 1),self.WI)\n        iS += self.bI.expand_as(iS)\n\n        # next delta\n        deltaS = self.m * deltaS - nn.Sigmoid()(iS).mul(grads_)\n\n        # next cell/params\n        cS = nn.Sigmoid()(fS).mul(cS) + deltaS\n\n        return fS, iS, cS, deltaS\n\n    def __repr__(self):\n        s = \'{name}({input_size}, {hidden_size})\'\n        return s.format(name=self.__class__.__name__, **self.__dict__)\n\n\nclass MetaLSTM(nn.Module):\n\n    """"""A module that runs multiple steps of LSTM.""""""\n\n    def __init__(self, input_size, hidden_size,\n                    batch_first = False, num_layers=1):\n        super(MetaLSTM, self).__init__()\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        self.batch_first = batch_first\n\n        self.cells = []\n        for layer in range(num_layers):\n            layer_input_size = input_size if layer == 0 else hidden_size\n            cell = MetaLSTMCell(input_size=layer_input_size,\n                              hidden_size=hidden_size)\n            self.cells.append(cell)\n            setattr(self, \'cell_{}\'.format(layer), cell)\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        for cell in self.cells:\n            cell.reset_parameters()\n\n    @staticmethod\n    def _forward_rnn(cell, input_, grads_, length, hx):\n        max_time = input_.size(0)\n        output = []\n        for time in range(max_time):\n            hx = cell(input_=input_[time],grads_=grads_[time], hx=hx)\n            #mask = (time < length).float().unsqueeze(1).expand_as(h_next[0])\n            #fS_next = h_next[0] * mask + hx[0] * (1 - mask)\n            #iS_next = h_next[1] * mask + hx[1] * (1 - mask)\n            #cS_next = h_next[2] * mask + hx[2] * (1 - mask)\n            #deltaS_next = h_next[3] * mask + hx[3] * (1 - mask)\n            #hx_next = (fS_next, iS_next, cS_next, deltaS_next)\n            #output.append(h_next)\n            #hx = hx_next\n        #output = torch.stack(output, 0)\n        #return output,hx\n        #return hx[2],hx\n        return hx\n\n    def forward(self, input_, length=None, hx=None):\n\n        x_input = input_[0] # output from lstm\n        grad_input = input_[1] # gradients from learner\n        if self.batch_first:\n            x_input = x_input.transpose(0, 1)\n            grad_input = grad_input.transpose(0, 1)\n        max_time, batch_size, _ = x_input.data.size()\n        if length is None:\n            length = Variable(torch.LongTensor([max_time] * batch_size))\n            if x_input.is_cuda:\n                length = length.cuda()\n        # hidden variables. Here we have fS, iS and cS.\n        if hx is None:\n            fS = Variable(grad_input.data.new(batch_size, 1).zero_())\n            iS = Variable(grad_input.data.new(batch_size, 1).zero_())\n            cS = (self.cells[0].cI).unsqueeze(1)\n            deltaS = Variable(grad_input.data.new(batch_size, 1).zero_())\n            hx = (fS, iS, cS, deltaS)\n\n        fS_n = []\n        iS_n = []\n        cS_n = []\n        deltaS_n = []\n        for layer in range(self.num_layers):\n            hx_new = MetaLSTM._forward_rnn(\n                cell=self.cells[layer], input_=x_input,\n                grads_= grad_input, length=length, hx=hx)\n            fS_n.append(hx_new[0])\n            iS_n.append(hx_new[1])\n            cS_n.append(hx_new[2])\n            deltaS_n.append(hx_new[3])\n        fS_n = torch.stack(fS_n, 0)\n        iS_n = torch.stack(iS_n, 0)\n        cS_n = torch.stack(cS_n, 0)\n        fS_n = torch.stack(fS_n, 0)\n        deltaS_n = torch.stack(deltaS_n, 0)\n        # return cS and the actual state\n        return (fS_n, iS_n, cS_n, deltaS_n)\n\n'"
model/lstm/recurrentLSTMNetwork.py,5,"b""import torch\nfrom torch import nn\nfrom torch.autograd import Variable\n\nclass RecurrentLSTMNetwork(nn.Module):\n    def __init__(self, opt):\n        super(RecurrentLSTMNetwork, self).__init__()\n\n        self.inputFeatures = opt['inputFeatures'] if 'inputFeatures' in opt.keys() else 10\n        self.hiddenFeatures = opt['hiddenFeatures'] if 'hiddenFeatures' in opt.keys() else 100\n        self.outputType = opt['outputType'] if 'outputType' in opt.keys() else 'last' # 'last' or 'all'\n        self.batchNormalization = opt['batchNormalization'] if 'batchNormalization' in opt.keys() else False\n        self.maxBatchNormalizationLayers = opt['maxBatchNormalizationLayers'] if 'batchNormalization' in opt.keys() else 10\n\n        # parameters\n        self.p = {}\n        self.p['W'] = Variable(torch.zeros(self.inputFeatures+self.hiddenFeatures,4 * self.hiddenFeatures),\n                               requires_grad = True)\n        self.params = [self.p['W']]\n\n        #TODO: delete this line. only for debugging\n        self.batchNormalization = True\n\n        if self.batchNormalization:\n            # TODO: check if nn.BatchNorm1d or torch.legacy.nn.BatchNormalization\n            # translation and scaling parameters are shared across time.\n            lstm_bn = nn.BatchNorm1d(4*self.hiddenFeatures)\n            cell_bn = nn.BatchNorm1d(self.hiddenFeatures)\n            self.layers = {'lstm_bn':[lstm_bn],'cell_bn':[cell_bn]}\n\n            for i in range(2,self.maxBatchNormalizationLayers):\n                lstm_bn = nn.BatchNorm1d(4*self.hiddenFeatures)\n                cell_bn = nn.BatchNorm1d(self.hiddenFeatures)\n                self.layers['lstm_bn'].append(lstm_bn)\n                self.layers['cell_bn'].append(cell_bn)\n\n            # Initializing scaling to <1 is recommended for LSTM batch norm\n            # TODO: why only the first are initialized??\n            self.layers['lstm_bn'][0].weight.data.fill_(0.1)\n            self.layers['lstm_bn'][0].bias.data.zero_()\n            self.layers['cell_bn'][0].weight.data.fill_(0.1)\n            self.layers['cell_bn'][0].bias.data.zero_()\n\n            self.params = self.params + \\\n                              list(self.layers['lstm_bn'][0].parameters()) + \\\n                              list(self.layers['lstm_bn'][0].parameters())\n        else:\n            self.p['b'] = Variable(torch.zeros(1, 4*self.hiddenFeatures),\n                                   require_grad = True)\n            self.params = self.params + [self.p['b']]\n            self.layers = {}\n\n    def setCuda(self, value = True):\n        # If value is a string then use self.opt\n        # If it is not a string then it should be True or False\n        if value == True:\n            for key in self.p.keys():\n                self.p[key].cuda()\n            for key in self.layers.keys():\n                for i in range(len(self.layers[key])):\n                    self.layers[key][i].cuda()\n        else:\n            for key in self.p.keys():\n                self.p[key].cpu()\n            for key in self.layers.keys():\n                for i in range(len(self.layers[key])):\n                    self.layers[key][i].cpu()\n\n    def forward(self, x, prevState = None ):\n\n        # dimensions\n        if len(x.size()) == 2: x = x.unsqueeze(0)\n        batch = x.size(0)\n        steps = x.size(1)\n\n        if prevState == None: prevState = {}\n        hs = {}\n        cs = {}\n        for t in range(steps):\n            # xt\n            xt = x[:,t,:]\n            # prev h and pre c\n            hp = hs[t-1] or prevState.h or torch.zeros()\n        a = 0\n"""
model/lstm/train-lstm.py,5,"b'##+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n## Created by: Albert Berenguel\n## Computer Vision Center (CVC). Universitat Autonoma de Barcelona\n## Email: aberenguel@cvc.uab.es\n## Copyright (c) 2017\n##\n## This source code is licensed under the MIT-style license found in the\n## LICENSE file in the root directory of this source tree\n##+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n\nimport os\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\nimport importlib\nimport numpy as np\nimport time\nimport math\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import accuracy_score\nfrom utils import util\nfrom learner import Learner\nfrom metaLearner import MetaLearner\n\ndef create_optimizer(opt, params):\n    if opt[\'optimMethod\'] == \'sgd\':\n        optimizer = torch.optim.SGD(params, lr=opt[\'lr\'],\n                              momentum=0.9, dampening=0.9,\n                              weight_decay=opt[\'weight_decay\'])\n    elif opt[\'optimMethod\']:\n        optimizer = torch.optim.Adam(params, lr=opt[\'lr\'],\n                               weight_decay=opt[\'weight_decay\'])\n    else:\n        raise Exception(\'Not supported optimizer: {0}\'.format(opt[\'optimMethod\']))\n    return optimizer\n\ndef adjust_learning_rate(opt,optimizer):\n    """"""Updates the learning rate given the learning rate decay.\n    The routine has been implemented according to the original Lua SGD optimizer\n    """"""\n    for group in optimizer.param_groups:\n        if \'step\' not in group:\n            group[\'step\'] = 0\n        group[\'step\'] += 1\n\n        group[\'lr\'] = opt[\'lr\'] / (1 + group[\'step\'] * opt[\'lr_decay\'])\n\n    return optimizer\n\ndef run(opt,data):\n\n    # learner\n    learner = Learner(opt)\n    print(\'Learner nParams: %d\' % (learner.nParams))\n\n    # meta-learner\n    params_dict = {\'learnerParams\': learner.params,\n                   \'nParams\': learner.nParams}\n    for param in [\'debug\',\'homePath\',\'nHidden\',\'BN1\',\'BN2\']:\n        if param in opt.keys():\n            params_dict[param] = opt[param]\n    metaLearner = MetaLearner(params_dict)\n    # set cuda\n    metaLearner.setCuda(opt[\'useCUDA\'])\n    learner.setCuda(opt[\'useCUDA\'])\n\n    # Keep track of errors\n    trainConf_pred = []\n    trainConf_gt = []\n    valConf_pred = {}\n    valConf_gt = {}\n    testConf_pred = {}\n    testConf_gt = {}\n    for i in opt[\'nTestShot\']:\n        valConf_pred[i] = []\n        valConf_gt[i] = []\n        testConf_pred[i] = []\n        testConf_gt[i] = []\n\n    cost = 0\n    timer = time.time()\n\n    #################################################################\n    ############ Meta-training\n    #################################################################\n\n    # Init optimizer\n    #optimizer = create_optimizer(opt, metaLearner.params.values())\n    #optimizer = create_optimizer(opt, learner.modelF.net.parameters())\n    optimizer = create_optimizer(opt, list(metaLearner.lstm.parameters()) + list(metaLearner.lstm2.parameters()))\n\n    # train episode loop\n    for episodeTrain,(x_support_set, y_support_set, x_target, target_y) in enumerate(data[\'train\']):\n\n        # Re-arange the Target vectors between [0..nClasses_train]\n        dictLabels, dictLabelsInverse = util.createDictLabels(y_support_set)\n        y_support_set = util.fitLabelsToRange(dictLabels, y_support_set)\n        target_y = util.fitLabelsToRange(dictLabels, target_y)\n\n        # Convert them in Variables\n        input = {}\n        trainInput = Variable(x_support_set).float()\n        trainTarget = Variable(y_support_set,requires_grad=False).long()\n        testInput = Variable(x_target).float()\n        testTarget = Variable(target_y,requires_grad=False).long()\n\n        # Convert to GPU if needed\n        if opt[\'useCUDA\']:\n            trainInput = trainInput.cuda()\n            trainTarget = trainTarget.cuda()\n            testInput = testInput.cuda()\n            testTarget = testTarget.cuda()\n\n        # learner-optimizer with learner.model.net\n\n\n        # forward metalearner\n        output, loss = metaLearner(learner, trainInput, trainTarget,\n                                            testInput, testTarget,\n                                            opt[\'nEpochs\'][opt[\'nTrainShot\']],\n                                            opt[\'batchSize\'][opt[\'nTrainShot\']])\n        optimizer.zero_grad()\n        loss.backward()\n        metaLearner.gradNorm(loss)\n        optimizer.step()\n\n        # Adjust learning rate\n        optimizer = adjust_learning_rate(opt, optimizer)\n\n        cost = cost + loss\n\n        # update stats\n        values_pred, indices_pred = torch.max(output, 1)\n        target_y = util.fitLabelsToRange(dictLabelsInverse, target_y)\n        indices_pred = util.fitLabelsToRange(dictLabelsInverse, indices_pred.cpu().data)\n        trainConf_pred.append(indices_pred.numpy())\n        trainConf_gt.append(target_y.numpy())\n\n        print(\n            \'Training Episode: [{}/{} ({:.0f}%)]\\tLoss: {:.3f}. Elapsed: {:.4f} s\'.format(\n                episodeTrain, len(data[\'train\']), 100. * episodeTrain / len(data[\'train\']),\n                loss.data[0], time.time() - timer))\n\n\n        if episodeTrain % opt[\'printPer\'] == 0:\n            trainConf_pred = np.concatenate(trainConf_pred, axis=0)\n            trainConf_gt = np.concatenate(trainConf_gt, axis=0)\n            target_names = [str(i) for i in np.unique(trainConf_gt)]\n\n            print(\n                \'Training Episode: [{}/{} ({:.0f}%)]\\tCost: {:.3f}. Elapsed: {:.4f} s\'.format(\n                    episodeTrain, len(data[\'train\']), 100. * episodeTrain / len(data[\'train\']),\n                    (cost.cpu().data.numpy() / opt[\'printPer\'])[0],time.time() - timer))\n            print(classification_report(trainConf_gt, trainConf_pred,\n                                        target_names=target_names))\n            # Set to 0\n            trainConf_pred = []\n            trainConf_gt = []\n\n        cost = 0\n        timer = time.time()\n\n'"
