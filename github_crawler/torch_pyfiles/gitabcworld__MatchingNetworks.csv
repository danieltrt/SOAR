file_path,api_count,code
logger.py,0,"b'import os\nfrom tensorboard_logger import configure, log_value\n\nclass Logger(object):\n    def __init__(self, log_dir):\n        # clean previous logged data under the same directory name\n        self._remove(log_dir)\n\n        # configure the project\n        configure(log_dir)\n\n        self.global_step = 0\n\n    def log_value(self, name, value):\n        log_value(name, value, self.global_step)\n        return self\n\n    def step(self):\n        self.global_step += 1\n\n    @staticmethod\n    def _remove(path):\n        """""" param <path> could either be relative or absolute. """"""\n        if os.path.isfile(path):\n            os.remove(path)  # remove the file\n        elif os.path.isdir(path):\n            import shutil\n            shutil.rmtree(path)  # remove dir and all contains'"
mainMiniImageNet.py,0,"b'##+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n## Created by: Albert Berenguel\n## Computer Vision Center (CVC). Universitat Autonoma de Barcelona\n## Email: aberenguel@cvc.uab.es\n## Copyright (c) 2017\n##\n## This source code is licensed under the MIT-style license found in the\n## LICENSE file in the root directory of this source tree\n##+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n\nfrom datasets import miniImagenetOneShot\nfrom option import Options\nfrom experiments.OneShotMiniImageNetBuilder import miniImageNetBuilder\nimport tqdm\nfrom logger import Logger\n\n\'\'\'\n:param batch_size: Experiment batch_size\n:param classes_per_set: Integer indicating the number of classes per set\n:param samples_per_class: Integer indicating samples per class\n        e.g. For a 20-way, 1-shot learning task, use classes_per_set=20 and samples_per_class=1\n             For a 5-way, 10-shot learning task, use classes_per_set=5 and samples_per_class=10\n\'\'\'\n\n# Experiment Setup\nbatch_size = 10\nfce = True\nclasses_per_set = 5\nsamples_per_class = 5\nchannels = 3\n# Training setup\ntotal_epochs = 500\ntotal_train_batches = 100\ntotal_val_batches = 100\ntotal_test_batches = 250\n# Parse other options\nargs = Options().parse()\n\nLOG_DIR = args.log_dir + \'/miniImageNetOneShot_run-batchSize_{}-fce_{}-classes_per_set{}-samples_per_class{}-channels{}\' \\\n    .format(batch_size,fce,classes_per_set,samples_per_class,channels)\n\n# create logger\nlogger = Logger(LOG_DIR)\n\n#args.dataroot = \'/home/aberenguel/Dataset/miniImagenet\'\ndataTrain = miniImagenetOneShot.miniImagenetOneShotDataset(dataroot=args.dataroot,\n                                                           type = \'train\',\n                                                           nEpisodes = total_train_batches*batch_size,\n                                                           classes_per_set=classes_per_set,\n                                                           samples_per_class=samples_per_class)\n\ndataVal = miniImagenetOneShot.miniImagenetOneShotDataset(dataroot=args.dataroot,\n                                                         type = \'val\',\n                                                         nEpisodes = total_val_batches*batch_size,\n                                                         classes_per_set=classes_per_set,\n                                                         samples_per_class=samples_per_class)\n\ndataTest = miniImagenetOneShot.miniImagenetOneShotDataset(dataroot=args.dataroot,\n                                                          type = \'test\',\n                                                          nEpisodes = total_test_batches*batch_size,\n                                                          classes_per_set=classes_per_set,\n                                                          samples_per_class=samples_per_class)\n\nobj_oneShotBuilder = miniImageNetBuilder(dataTrain,dataVal,dataTest)\nobj_oneShotBuilder.build_experiment(batch_size, classes_per_set, samples_per_class, channels, fce)\n\nbest_val = 0.\nwith tqdm.tqdm(total=total_epochs) as pbar_e:\n    for e in range(0, total_epochs):\n        total_c_loss, total_accuracy = obj_oneShotBuilder.run_training_epoch()\n        print(""Epoch {}: train_loss: {}, train_accuracy: {}"".format(e, total_c_loss, total_accuracy))\n\n        total_val_c_loss, total_val_accuracy = obj_oneShotBuilder.run_validation_epoch()\n        print(""Epoch {}: val_loss: {}, val_accuracy: {}"".format(e, total_val_c_loss, total_val_accuracy))\n\n        logger.log_value(\'train_loss\', total_c_loss)\n        logger.log_value(\'train_acc\', total_accuracy)\n        logger.log_value(\'val_loss\', total_val_c_loss)\n        logger.log_value(\'val_acc\', total_val_accuracy)\n\n        if total_val_accuracy >= best_val:  # if new best val accuracy -> produce test statistics\n            best_val = total_val_accuracy\n            total_test_c_loss, total_test_accuracy = obj_oneShotBuilder.run_testing_epoch()\n            print(""Epoch {}: test_loss: {}, test_accuracy: {}"".format(e, total_test_c_loss, total_test_accuracy))\n            logger.log_value(\'test_loss\', total_test_c_loss)\n            logger.log_value(\'test_acc\', total_test_accuracy)\n        else:\n            total_test_c_loss = -1\n            total_test_accuracy = -1\n\n        pbar_e.update(1)\n        logger.step()'"
mainOmniglot.py,0,"b'##+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n## Created by: Albert Berenguel\n## Computer Vision Center (CVC). Universitat Autonoma de Barcelona\n## Email: aberenguel@cvc.uab.es\n## Copyright (c) 2017\n##\n## This source code is licensed under the MIT-style license found in the\n## LICENSE file in the root directory of this source tree\n##+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n\nfrom datasets import omniglotNShot\nfrom option import Options\nfrom experiments.OneShotBuilder import OneShotBuilder\nimport tqdm\nfrom logger import Logger\n\n\'\'\'\n:param batch_size: Experiment batch_size\n:param classes_per_set: Integer indicating the number of classes per set\n:param samples_per_class: Integer indicating samples per class\n        e.g. For a 20-way, 1-shot learning task, use classes_per_set=20 and samples_per_class=1\n             For a 5-way, 10-shot learning task, use classes_per_set=5 and samples_per_class=10\n\'\'\'\n\n# Experiment Setup\nbatch_size = 32\nfce = False\nclasses_per_set = 5\nsamples_per_class = 5\nchannels = 1\n# Training setup\ntotal_epochs = 500\ntotal_train_batches = 1000\ntotal_val_batches = 100\ntotal_test_batches = 250\n# Parse other options\nargs = Options().parse()\n\nLOG_DIR = args.log_dir + \'/1_run-batchSize_{}-fce_{}-classes_per_set{}-samples_per_class{}-channels{}\' \\\n    .format(batch_size,fce,classes_per_set,samples_per_class,channels)\n\n# create logger\nlogger = Logger(LOG_DIR)\n\ndata = omniglotNShot.OmniglotNShotDataset(dataroot=args.dataroot, batch_size = batch_size,\n                                          classes_per_set=classes_per_set,\n                                          samples_per_class=samples_per_class)\n\nobj_oneShotBuilder = OneShotBuilder(data)\nobj_oneShotBuilder.build_experiment(batch_size, classes_per_set, samples_per_class, channels, fce)\n\nbest_val = 0.\nwith tqdm.tqdm(total=total_epochs) as pbar_e:\n    for e in range(0, total_epochs):\n        total_c_loss, total_accuracy = obj_oneShotBuilder.run_training_epoch(total_train_batches=total_train_batches)\n        print(""Epoch {}: train_loss: {}, train_accuracy: {}"".format(e, total_c_loss, total_accuracy))\n\n        total_val_c_loss, total_val_accuracy = obj_oneShotBuilder.run_validation_epoch(\n            total_val_batches=total_val_batches)\n        print(""Epoch {}: val_loss: {}, val_accuracy: {}"".format(e, total_val_c_loss, total_val_accuracy))\n\n        logger.log_value(\'train_loss\', total_c_loss)\n        logger.log_value(\'train_acc\', total_accuracy)\n        logger.log_value(\'val_loss\', total_val_c_loss)\n        logger.log_value(\'val_acc\', total_val_accuracy)\n\n        if total_val_accuracy >= best_val:  # if new best val accuracy -> produce test statistics\n            best_val = total_val_accuracy\n            total_test_c_loss, total_test_accuracy = obj_oneShotBuilder.run_testing_epoch(\n                total_test_batches=total_test_batches)\n            print(""Epoch {}: test_loss: {}, test_accuracy: {}"".format(e, total_test_c_loss, total_test_accuracy))\n            logger.log_value(\'test_loss\', total_test_c_loss)\n            logger.log_value(\'test_acc\', total_test_accuracy)\n        else:\n            total_test_c_loss = -1\n            total_test_accuracy = -1\n\n        pbar_e.update(1)\n        logger.step()'"
option.py,0,"b""##+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n## Created by: Albert Berenguel\n## Computer Vision Center (CVC). Universitat Autonoma de Barcelona\n## Email: aberenguel@cvc.uab.es\n## Copyright (c) 2017\n##\n## This source code is licensed under the MIT-style license found in the\n## LICENSE file in the root directory of this source tree\n##+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n\nimport argparse\nimport os\n\nclass Options():\n    def __init__(self):\n        # Training settings\n        parser = argparse.ArgumentParser(description='Matching Network')\n        parser.add_argument('--dataroot', type=str, default='/tmp/omniglot',\n                            help='path to dataset')\n        parser.add_argument('--log-dir', default='./logs',\n                            help='folder to output model checkpoints')\n\n        self.parser = parser\n\n    def parse(self):\n        return self.parser.parse_args()\n"""
datasets/__init__.py,0,b''
datasets/miniImagenetOneShot.py,4,"b""##+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n## Created by: Albert Berenguel\n## Computer Vision Center (CVC). Universitat Autonoma de Barcelona\n## Email: aberenguel@cvc.uab.es\n## Copyright (c) 2017\n##\n## This source code is licensed under the MIT-style license found in the\n## LICENSE file in the root directory of this source tree\n##+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\nimport torch\nimport torch.utils.data as data\nimport torchvision.transforms as transforms\nfrom PIL import Image\nimport os.path\nimport csv\nimport math\nimport collections\nfrom tqdm import tqdm\n\nimport numpy as np\nnp.random.seed(2191)  # for reproducibility\n\n# LAMBDA FUNCTIONS\nfilenameToPILImage = lambda x: Image.open(x)\nPiLImageResize = lambda x: x.resize((84,84))\n\nclass miniImagenetOneShotDataset(data.Dataset):\n    def __init__(self, dataroot = '/home/aberenguel/Dataset/miniImagenet', type = 'train',\n                 nEpisodes = 1000, classes_per_set=10, samples_per_class=1):\n\n        self.nEpisodes = nEpisodes\n        self.classes_per_set = classes_per_set\n        self.samples_per_class = samples_per_class\n        self.n_samples = self.samples_per_class * self.classes_per_set\n        self.n_samplesNShot = 5 # Samples per meta-test. In this case 1 as is OneShot.\n        # Transformations to the image\n        self.transform = transforms.Compose([filenameToPILImage,\n                                             PiLImageResize,\n                                             transforms.ToTensor()\n                                             ])\n\n        def loadSplit(splitFile):\n            dictLabels = {}\n            with open(splitFile) as csvfile:\n                csvreader = csv.reader(csvfile, delimiter=',')\n                next(csvreader, None)\n                for i,row in enumerate(csvreader):\n                    filename = row[0]\n                    label = row[1]\n                    if label in dictLabels.keys():\n                        dictLabels[label].append(filename)\n                    else:\n                        dictLabels[label] = [filename]\n            return dictLabels\n\n        #requiredFiles = ['train','val','test']\n        self.miniImagenetImagesDir = os.path.join(dataroot,'images')\n        self.data = loadSplit(splitFile = os.path.join(dataroot,type + '.csv'))\n        self.data = collections.OrderedDict(sorted(self.data.items()))\n        self.classes_dict = {self.data.keys()[i]:i  for i in range(len(self.data.keys()))}\n        self.create_episodes(self.nEpisodes)\n\n    def create_episodes(self,episodes):\n\n        self.support_set_x_batch = []\n        self.target_x_batch = []\n        for b in np.arange(episodes):\n            # select n classes_per_set randomly\n            selected_classes = np.random.choice(len(self.data.keys()), self.classes_per_set, False)\n            selected_class_meta_test = np.random.choice(selected_classes)\n            support_set_x = []\n            target_x = []\n            for c in selected_classes:\n                number_of_samples = self.samples_per_class\n                if c == selected_class_meta_test:\n                    number_of_samples += self.n_samplesNShot\n                selected_samples = np.random.choice(len(self.data[self.data.keys()[c]]),\n                                                    number_of_samples, False)\n                indexDtrain = np.array(selected_samples[:self.samples_per_class])\n                support_set_x.append(np.array(self.data[self.data.keys()[c]])[indexDtrain].tolist())\n                if c == selected_class_meta_test:\n                    indexDtest = np.array(selected_samples[self.samples_per_class:])\n                    target_x.append(np.array(self.data[self.data.keys()[c]])[indexDtest].tolist())\n            self.support_set_x_batch.append(support_set_x)\n            self.target_x_batch.append(target_x)\n\n    def __getitem__(self, index):\n\n        support_set_x = torch.FloatTensor(self.n_samples, 3, 84, 84)\n        support_set_y = np.zeros((self.n_samples), dtype=np.int)\n        target_x = torch.FloatTensor(self.n_samplesNShot, 3, 84, 84)\n        target_y = np.zeros((self.n_samplesNShot), dtype=np.int)\n\n        flatten_support_set_x_batch = [os.path.join(self.miniImagenetImagesDir,item)\n                                       for sublist in self.support_set_x_batch[index] for item in sublist]\n        support_set_y = np.array([self.classes_dict[item[:9]]\n                                      for sublist in self.support_set_x_batch[index] for item in sublist])\n        flatten_target_x = [os.path.join(self.miniImagenetImagesDir,item)\n                            for sublist in self.target_x_batch[index] for item in sublist]\n        target_y = np.array([self.classes_dict[item[:9]]\n                            for sublist in self.target_x_batch[index] for item in sublist])\n\n        for i,path in enumerate(flatten_support_set_x_batch):\n            if self.transform is not None:\n                support_set_x[i] = self.transform(path)\n\n        for i,path in enumerate(flatten_target_x):\n            if self.transform is not None:\n                target_x[i] = self.transform(path)\n\n        # convert the targets number between [0, self.classes_per_set)\n        classes_dict_temp = {np.unique(support_set_y)[i]: i for i in np.arange(len(np.unique(support_set_y)))}\n        support_set_y = np.array([classes_dict_temp[i] for i in support_set_y])\n        target_y = np.array([classes_dict_temp[i] for i in target_y])\n\n        return support_set_x, torch.IntTensor(support_set_y), target_x, torch.IntTensor(target_y)\n\n    def __len__(self):\n        return self.nEpisodes\n\n\n"""
datasets/omniglot.py,1,"b'##+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n## Created by: Albert Berenguel\n## Computer Vision Center (CVC). Universitat Autonoma de Barcelona\n## Email: aberenguel@cvc.uab.es\n## Copyright (c) 2017\n##\n## This source code is licensed under the MIT-style license found in the\n## LICENSE file in the root directory of this source tree\n##\n## Arcknowledgments:\n## https://github.com/ludc. Using some parts of his Omniglot code.\n## https://github.com/AntreasAntoniou. Using some parts of his Omniglot code.\n##+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n\nfrom __future__ import print_function\nimport torch.utils.data as data\nimport os\nimport os.path\nimport errno\n\nclass OMNIGLOT(data.Dataset):\n    urls = [\n        \'https://github.com/brendenlake/omniglot/raw/master/python/images_background.zip\',\n        \'https://github.com/brendenlake/omniglot/raw/master/python/images_evaluation.zip\'\n    ]\n    raw_folder = \'raw\'\n    processed_folder = \'processed\'\n    training_file = \'training.pt\'\n    test_file = \'test.pt\'\n\n    \'\'\'\n    The items are (filename,category). The index of all the categories can be found in self.idx_classes\n    Args:\n    - root: the directory where the dataset will be stored\n    - transform: how to transform the input\n    - target_transform: how to transform the target\n    - download: need to download the dataset\n    \'\'\'\n    def __init__(self, root, transform=None, target_transform=None, download=False):\n        self.root = root\n        self.transform = transform\n        self.target_transform = target_transform\n\n        if download:\n            self.download()\n\n        if not self._check_exists():\n            raise RuntimeError(\'Dataset not found.\'\n                               + \' You can use download=True to download it\')\n\n        self.all_items=find_classes(os.path.join(self.root, self.processed_folder))\n        self.idx_classes=index_classes(self.all_items)\n\n    def __getitem__(self, index):\n        filename=self.all_items[index][0]\n        img=str.join(\'/\',[self.all_items[index][2],filename])\n\n        target=self.idx_classes[self.all_items[index][1]]\n        if self.transform is not None:\n            img = self.transform(img)\n        if self.target_transform is not None:\n            target = self.target_transform(target)\n\n        return  img,target\n\n    def __len__(self):\n        return len(self.all_items)\n\n    def _check_exists(self):\n        return os.path.exists(os.path.join(self.root, self.processed_folder, ""images_evaluation"")) and \\\n               os.path.exists(os.path.join(self.root, self.processed_folder, ""images_background""))\n\n    def download(self):\n        from six.moves import urllib\n        import zipfile\n\n        if self._check_exists():\n            return\n\n        # download files\n        try:\n            os.makedirs(os.path.join(self.root, self.raw_folder))\n            os.makedirs(os.path.join(self.root, self.processed_folder))\n        except OSError as e:\n            if e.errno == errno.EEXIST:\n                pass\n            else:\n                raise\n\n        for url in self.urls:\n            print(\'== Downloading \' + url)\n            data = urllib.request.urlopen(url)\n            filename = url.rpartition(\'/\')[2]\n            file_path = os.path.join(self.root, self.raw_folder, filename)\n            with open(file_path, \'wb\') as f:\n                f.write(data.read())\n            file_processed = os.path.join(self.root, self.processed_folder)\n            print(""== Unzip from ""+file_path+"" to ""+file_processed)\n            zip_ref = zipfile.ZipFile(file_path, \'r\')\n            zip_ref.extractall(file_processed)\n            zip_ref.close()\n        print(""Download finished."")\n\ndef find_classes(root_dir):\n    retour=[]\n    for (root,dirs,files) in os.walk(root_dir):\n        for f in files:\n            if (f.endswith(""png"")):\n                r=root.split(\'/\')\n                lr=len(r)\n                retour.append((f,r[lr-2]+""/""+r[lr-1],root))\n    print(""== Found %d items ""%len(retour))\n    return retour\n\ndef index_classes(items):\n    idx={}\n    for i in items:\n        if (not i[1] in idx):\n            idx[i[1]]=len(idx)\n    print(""== Found %d classes""% len(idx))\n    return idx'"
datasets/omniglotNShot.py,0,"b'##+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n## Created by: Albert Berenguel\n## Computer Vision Center (CVC). Universitat Autonoma de Barcelona\n## Email: aberenguel@cvc.uab.es\n## Copyright (c) 2017\n##\n## This source code is licensed under the MIT-style license found in the\n## LICENSE file in the root directory of this source tree\n##+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n\nfrom datasets import omniglot\nimport torchvision.transforms as transforms\nfrom PIL import Image\nimport os.path\n\nimport numpy as np\nnp.random.seed(2191)  # for reproducibility\n\n# LAMBDA FUNCTIONS\nfilenameToPILImage = lambda x: Image.open(x).convert(\'L\')\nPiLImageResize = lambda x: x.resize((28,28))\nnp_reshape = lambda x: np.reshape(x, (28, 28, 1))\n\nclass OmniglotNShotDataset():\n    def __init__(self, dataroot, batch_size = 100, classes_per_set=10, samples_per_class=1):\n\n        if not os.path.isfile(os.path.join(dataroot,\'data.npy\')):\n            self.x = omniglot.OMNIGLOT(dataroot, download=True,\n                                     transform=transforms.Compose([filenameToPILImage,\n                                                                   PiLImageResize,\n                                                                   np_reshape]))\n\n            """"""\n            # Convert to the format of AntreasAntoniou. Format [nClasses,nCharacters,28,28,1]\n            """"""\n            temp = dict()\n            for (img, label) in self.x:\n                if label in temp:\n                    temp[label].append(img)\n                else:\n                    temp[label]=[img]\n            self.x = [] # Free memory\n\n            for classes in temp.keys():\n                self.x.append(np.array(temp[temp.keys()[classes]]))\n            self.x = np.array(self.x)\n            temp = [] # Free memory\n            np.save(os.path.join(dataroot,\'data.npy\'),self.x)\n        else:\n            self.x = np.load(os.path.join(dataroot,\'data.npy\'))\n\n        """"""\n        Constructs an N-Shot omniglot Dataset\n        :param batch_size: Experiment batch_size\n        :param classes_per_set: Integer indicating the number of classes per set\n        :param samples_per_class: Integer indicating samples per class\n        e.g. For a 20-way, 1-shot learning task, use classes_per_set=20 and samples_per_class=1\n             For a 5-way, 10-shot learning task, use classes_per_set=5 and samples_per_class=10\n        """"""\n\n        shuffle_classes = np.arange(self.x.shape[0])\n        np.random.shuffle(shuffle_classes)\n        self.x = self.x[shuffle_classes]\n        self.x_train, self.x_test, self.x_val  = self.x[:1200], self.x[1200:1500], self.x[1500:]\n        self.normalization()\n\n        self.batch_size = batch_size\n        self.n_classes = self.x.shape[0]\n        self.classes_per_set = classes_per_set\n        self.samples_per_class = samples_per_class\n\n        self.indexes = {""train"": 0, ""val"": 0, ""test"": 0}\n        self.datasets = {""train"": self.x_train, ""val"": self.x_val, ""test"": self.x_test} #original data cached\n        self.datasets_cache = {""train"": self.load_data_cache(self.datasets[""train""]),  #current epoch data cached\n                               ""val"": self.load_data_cache(self.datasets[""val""]),\n                               ""test"": self.load_data_cache(self.datasets[""test""])}\n\n    def normalization(self):\n        """"""\n        Normalizes our data, to have a mean of 0 and sdt of 1\n        """"""\n        self.mean = np.mean(self.x_train)\n        self.std = np.std(self.x_train)\n        self.max = np.max(self.x_train)\n        self.min = np.min(self.x_train)\n        print(""train_shape"", self.x_train.shape, ""test_shape"", self.x_test.shape, ""val_shape"", self.x_val.shape)\n        #print(""before_normalization"", ""mean"", self.mean, ""max"", self.max, ""min"", self.min, ""std"", self.std)\n        self.x_train = (self.x_train - self.mean) / self.std\n        self.x_val = (self.x_val - self.mean) / self.std\n        self.x_test = (self.x_test - self.mean) / self.std\n        #self.mean = np.mean(self.x_train)\n        #self.std = np.std(self.x_train)\n        #self.max = np.max(self.x_train)\n        #self.min = np.min(self.x_train)\n        #print(""after_normalization"", ""mean"", self.mean, ""max"", self.max, ""min"", self.min, ""std"", self.std)\n\n    def load_data_cache(self, data_pack):\n        """"""\n        Collects 1000 batches data for N-shot learning\n        :param data_pack: Data pack to use (any one of train, val, test)\n        :return: A list with [support_set_x, support_set_y, target_x, target_y] ready to be fed to our networks\n        """"""\n        n_samples = self.samples_per_class * self.classes_per_set\n        data_cache = []\n        for sample in range(1000):\n            support_set_x = np.zeros((self.batch_size, n_samples, 28, 28, 1))\n            support_set_y = np.zeros((self.batch_size, n_samples))\n            target_x = np.zeros((self.batch_size, self.samples_per_class, 28, 28, 1), dtype=np.int)\n            target_y = np.zeros((self.batch_size, self.samples_per_class), dtype=np.int)\n            for i in range(self.batch_size):\n                pinds = np.random.permutation(n_samples)\n                classes = np.random.choice(data_pack.shape[0], self.classes_per_set, False)\n                # select 1-shot or 5-shot classes for test with repetition\n                x_hat_class = np.random.choice(classes, self.samples_per_class, True)\n                pinds_test = np.random.permutation(self.samples_per_class)\n                ind = 0\n                ind_test = 0\n                for j, cur_class in enumerate(classes):  # each class\n                    if cur_class in x_hat_class:\n                        # Count number of times this class is inside the meta-test\n                        n_test_samples = np.sum(cur_class == x_hat_class)\n                        example_inds = np.random.choice(data_pack.shape[1], self.samples_per_class + n_test_samples, False)\n                    else:\n                        example_inds = np.random.choice(data_pack.shape[1], self.samples_per_class, False)\n\n                    # meta-training\n                    for eind in example_inds[:self.samples_per_class]:\n                        support_set_x[i, pinds[ind], :, :, :] = data_pack[cur_class][eind]\n                        support_set_y[i, pinds[ind]] = j\n                        ind = ind + 1\n                    # meta-test\n                    for eind in example_inds[self.samples_per_class:]:\n                        target_x[i, pinds_test[ind_test], :, :, :] = data_pack[cur_class][eind]\n                        target_y[i, pinds_test[ind_test]] = j\n                        ind_test = ind_test + 1\n\n            data_cache.append([support_set_x, support_set_y, target_x, target_y])\n        return data_cache\n\n    def __get_batch(self, dataset_name):\n        """"""\n        Gets next batch from the dataset with name.\n        :param dataset_name: The name of the dataset (one of ""train"", ""val"", ""test"")\n        :return:\n        """"""\n        if self.indexes[dataset_name] >= len(self.datasets_cache[dataset_name]):\n            self.indexes[dataset_name] = 0\n            self.datasets_cache[dataset_name] = self.load_data_cache(self.datasets[dataset_name])\n        next_batch = self.datasets_cache[dataset_name][self.indexes[dataset_name]]\n        self.indexes[dataset_name] += 1\n        x_support_set, y_support_set, x_target, y_target = next_batch\n        return x_support_set, y_support_set, x_target, y_target\n\n    def get_batch(self,str_type, rotate_flag = False):\n\n        """"""\n        Get next batch\n        :return: Next batch\n        """"""\n        x_support_set, y_support_set, x_target, y_target = self.__get_batch(str_type)\n        if rotate_flag:\n            k = int(np.random.uniform(low=0, high=4))\n            # Iterate over the sequence. Extract batches.\n            for i in np.arange(x_support_set.shape[0]):\n                x_support_set[i,:,:,:,:] = self.__rotate_batch(x_support_set[i,:,:,:,:],k)\n            # Rotate all the batch of the target images\n            for i in np.arange(x_target.shape[0]):\n                x_target[i,:,:,:,:] = self.__rotate_batch(x_target[i,:,:,:,:], k)\n        return x_support_set, y_support_set, x_target, y_target\n\n\n    def __rotate_data(self, image, k):\n        """"""\n        Rotates one image by self.k * 90 degrees counter-clockwise\n        :param image: Image to rotate\n        :return: Rotated Image\n        """"""\n        return np.rot90(image, k)\n\n\n    def __rotate_batch(self, batch_images, k):\n        """"""\n        Rotates a whole image batch\n        :param batch_images: A batch of images\n        :param k: integer degree of rotation counter-clockwise\n        :return: The rotated batch of images\n        """"""\n        batch_size = len(batch_images)\n        for i in np.arange(batch_size):\n            batch_images[i] = self.__rotate_data(batch_images[i], k)\n        return batch_images\n'"
experiments/OneShotBuilder.py,25,"b'##+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n## Created by: Albert Berenguel\n## Computer Vision Center (CVC). Universitat Autonoma de Barcelona\n## Email: aberenguel@cvc.uab.es\n## Copyright (c) 2017\n##\n## This source code is licensed under the MIT-style license found in the\n## LICENSE file in the root directory of this source tree\n##+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n\nimport torch\nimport torch.backends.cudnn as cudnn\nimport tqdm\nfrom models.MatchingNetwork import MatchingNetwork\nfrom torch.autograd import Variable\n\nclass OneShotBuilder:\n\n    def __init__(self, data):\n        """"""\n        Initializes an OneShotBuilder object. The OneShotBuilder object takes care of setting up our experiment\n        and provides helper functions such as run_training_epoch and run_validation_epoch to simplify out training\n        and evaluation procedures.\n        :param data: A data provider class\n        """"""\n        self.data = data\n\n    def build_experiment(self, batch_size, classes_per_set, samples_per_class, channels, fce):\n\n        """"""\n        :param batch_size: The experiment batch size\n        :param classes_per_set: An integer indicating the number of classes per support set\n        :param samples_per_class: An integer indicating the number of samples per class\n        :param channels: The image channels\n        :param fce: Whether to use full context embeddings or not\n        :return: a matching_network object, along with the losses, the training ops and the init op\n        """"""\n        self.classes_per_set = classes_per_set\n        self.samples_per_class = samples_per_class\n        self.keep_prob = torch.FloatTensor(1)\n        self.matchingNet = MatchingNetwork(batch_size=batch_size,\n                                         keep_prob=self.keep_prob, num_channels=channels,\n                                         fce=fce,\n                                         num_classes_per_set=classes_per_set,\n                                         num_samples_per_class=samples_per_class,\n                                         nClasses = 0, image_size = 28)\n        self.optimizer = \'adam\'\n        self.lr = 1e-03\n        self.current_lr = 1e-03\n        self.lr_decay = 1e-6\n        self.wd = 1e-4\n        self.total_train_iter = 0\n        self.isCudaAvailable = torch.cuda.is_available()\n        if self.isCudaAvailable:\n            cudnn.benchmark = True\n            torch.cuda.manual_seed_all(0)\n            self.matchingNet.cuda()\n\n    def run_training_epoch(self, total_train_batches):\n        """"""\n        Runs one training epoch\n        :param total_train_batches: Number of batches to train on\n        :return: mean_training_categorical_crossentropy_loss and mean_training_accuracy\n        """"""\n        total_c_loss = 0.\n        total_accuracy = 0.\n        # Create the optimizer\n        optimizer = self.__create_optimizer(self.matchingNet, self.lr)\n\n        with tqdm.tqdm(total=total_train_batches) as pbar:\n            for i in range(total_train_batches):  # train epoch\n                x_support_set, y_support_set, x_target, y_target = \\\n                    self.data.get_batch(str_type = \'train\',rotate_flag = True)\n\n                x_support_set = Variable(torch.from_numpy(x_support_set)).float()\n                y_support_set = Variable(torch.from_numpy(y_support_set),requires_grad=False).long()\n                x_target = Variable(torch.from_numpy(x_target)).float()\n                y_target = Variable(torch.from_numpy(y_target),requires_grad=False).long()\n\n                # y_support_set: Add extra dimension for the one_hot\n                y_support_set = torch.unsqueeze(y_support_set, 2)\n                sequence_length = y_support_set.size()[1]\n                batch_size = y_support_set.size()[0]\n                y_support_set_one_hot = torch.FloatTensor(batch_size, sequence_length,\n                                                               self.classes_per_set).zero_()\n                y_support_set_one_hot.scatter_(2, y_support_set.data, 1)\n                y_support_set_one_hot = Variable(y_support_set_one_hot)\n\n                # Reshape channels\n                size = x_support_set.size()\n                x_support_set = x_support_set.view(size[0],size[1],size[4],size[2],size[3])\n                size = x_target.size()\n                x_target = x_target.view(size[0],size[1],size[4],size[2],size[3])\n                if self.isCudaAvailable:\n                    acc, c_loss_value = self.matchingNet(x_support_set.cuda(), y_support_set_one_hot.cuda(),\n                                                         x_target.cuda(), y_target.cuda())\n                else:\n                    acc, c_loss_value = self.matchingNet(x_support_set, y_support_set_one_hot,\n                                                         x_target, y_target)\n\n                # Before the backward pass, use the optimizer object to zero all of the\n                # gradients for the variables it will update (which are the learnable weights\n                # of the model)\n                optimizer.zero_grad()\n\n                # Backward pass: compute gradient of the loss with respect to model parameters\n                c_loss_value.backward()\n\n                # Calling the step function on an Optimizer makes an update to its parameters\n                optimizer.step()\n\n                # update the optimizer learning rate\n                self.__adjust_learning_rate(optimizer)\n\n                iter_out = ""tr_loss: {}, tr_accuracy: {}"".format(c_loss_value.data[0], acc.data[0])\n                pbar.set_description(iter_out)\n\n                pbar.update(1)\n                total_c_loss += c_loss_value.data[0]\n                total_accuracy += acc.data[0]\n\n                self.total_train_iter += 1\n                if self.total_train_iter % 2000 == 0:\n                    self.lr /= 2\n                    print(""change learning rate"", self.lr)\n\n        total_c_loss = total_c_loss / total_train_batches\n        total_accuracy = total_accuracy / total_train_batches\n        return total_c_loss, total_accuracy\n\n    def run_validation_epoch(self, total_val_batches):\n        """"""\n        Runs one validation epoch\n        :param total_val_batches: Number of batches to train on\n        :return: mean_validation_categorical_crossentropy_loss and mean_validation_accuracy\n        """"""\n        total_val_c_loss = 0.\n        total_val_accuracy = 0.\n\n        with tqdm.tqdm(total=total_val_batches) as pbar:\n            for i in range(total_val_batches):  # validation epoch\n                x_support_set, y_support_set, x_target, y_target = \\\n                    self.data.get_batch(str_type=\'val\', rotate_flag=False)\n\n                x_support_set = Variable(torch.from_numpy(x_support_set), volatile=True).float()\n                y_support_set = Variable(torch.from_numpy(y_support_set), volatile=True).long()\n                x_target = Variable(torch.from_numpy(x_target), volatile=True).float()\n                y_target = Variable(torch.from_numpy(y_target), volatile=True).long()\n\n                # y_support_set: Add extra dimension for the one_hot\n                y_support_set = torch.unsqueeze(y_support_set, 2)\n                sequence_length = y_support_set.size()[1]\n                batch_size = y_support_set.size()[0]\n                y_support_set_one_hot = torch.FloatTensor(batch_size, sequence_length,\n                                                          self.classes_per_set).zero_()\n                y_support_set_one_hot.scatter_(2, y_support_set.data, 1)\n                y_support_set_one_hot = Variable(y_support_set_one_hot)\n\n                # Reshape channels\n                size = x_support_set.size()\n                x_support_set = x_support_set.view(size[0], size[1], size[4], size[2], size[3])\n                size = x_target.size()\n                x_target = x_target.view(size[0],size[1],size[4],size[2],size[3])\n                if self.isCudaAvailable:\n                    acc, c_loss_value = self.matchingNet(x_support_set.cuda(), y_support_set_one_hot.cuda(),\n                                                         x_target.cuda(), y_target.cuda())\n                else:\n                    acc, c_loss_value = self.matchingNet(x_support_set, y_support_set_one_hot,\n                                                         x_target, y_target)\n\n                iter_out = ""val_loss: {}, val_accuracy: {}"".format(c_loss_value.data[0], acc.data[0])\n                pbar.set_description(iter_out)\n                pbar.update(1)\n\n                total_val_c_loss += c_loss_value.data[0]\n                total_val_accuracy += acc.data[0]\n\n        total_val_c_loss = total_val_c_loss / total_val_batches\n        total_val_accuracy = total_val_accuracy / total_val_batches\n\n        return total_val_c_loss, total_val_accuracy\n\n    def run_testing_epoch(self, total_test_batches):\n        """"""\n        Runs one testing epoch\n        :param total_test_batches: Number of batches to train on\n        :param sess: Session object\n        :return: mean_testing_categorical_crossentropy_loss and mean_testing_accuracy\n        """"""\n        total_test_c_loss = 0.\n        total_test_accuracy = 0.\n        with tqdm.tqdm(total=total_test_batches) as pbar:\n            for i in range(total_test_batches):\n                x_support_set, y_support_set, x_target, y_target = \\\n                    self.data.get_batch(str_type=\'test\', rotate_flag=False)\n\n                x_support_set = Variable(torch.from_numpy(x_support_set), volatile=True).float()\n                y_support_set = Variable(torch.from_numpy(y_support_set), volatile=True).long()\n                x_target = Variable(torch.from_numpy(x_target), volatile=True).float()\n                y_target = Variable(torch.from_numpy(y_target), volatile=True).long()\n\n                # y_support_set: Add extra dimension for the one_hot\n                y_support_set = torch.unsqueeze(y_support_set, 2)\n                sequence_length = y_support_set.size()[1]\n                batch_size = y_support_set.size()[0]\n                y_support_set_one_hot = torch.FloatTensor(batch_size, sequence_length,\n                                                          self.classes_per_set).zero_()\n                y_support_set_one_hot.scatter_(2, y_support_set.data, 1)\n                y_support_set_one_hot = Variable(y_support_set_one_hot)\n\n                # Reshape channels\n                size = x_support_set.size()\n                x_support_set = x_support_set.view(size[0], size[1], size[4], size[2], size[3])\n                size = x_target.size()\n                x_target = x_target.view(size[0],size[1],size[4],size[2],size[3])\n                if self.isCudaAvailable:\n                    acc, c_loss_value = self.matchingNet(x_support_set.cuda(), y_support_set_one_hot.cuda(),\n                                                         x_target.cuda(), y_target.cuda())\n                else:\n                    acc, c_loss_value = self.matchingNet(x_support_set, y_support_set_one_hot,\n                                                         x_target, y_target)\n\n                iter_out = ""test_loss: {}, test_accuracy: {}"".format(c_loss_value.data[0], acc.data[0])\n                pbar.set_description(iter_out)\n                pbar.update(1)\n\n                total_test_c_loss += c_loss_value.data[0]\n                total_test_accuracy += acc.data[0]\n            total_test_c_loss = total_test_c_loss / total_test_batches\n            total_test_accuracy = total_test_accuracy / total_test_batches\n        return total_test_c_loss, total_test_accuracy\n\n    def __adjust_learning_rate(self,optimizer):\n        """"""Updates the learning rate given the learning rate decay.\n        The routine has been implemented according to the original Lua SGD optimizer\n        """"""\n        for group in optimizer.param_groups:\n            if \'step\' not in group:\n                group[\'step\'] = 0\n            group[\'step\'] += 1\n\n            group[\'lr\'] = self.lr / (1 + group[\'step\'] * self.lr_decay)\n\n    def __create_optimizer(self,model, new_lr):\n        # setup optimizer\n        if self.optimizer == \'sgd\':\n            optimizer = torch.optim.SGD(model.parameters(), lr=new_lr,\n                                  momentum=0.9, dampening=0.9,\n                                  weight_decay=self.wd)\n        elif self.optimizer == \'adam\':\n            optimizer = torch.optim.Adam(model.parameters(), lr=new_lr,\n                                   weight_decay=self.wd)\n        else:\n            raise Exception(\'Not supported optimizer: {0}\'.format(self.optimizer))\n        return optimizer'"
experiments/OneShotMiniImageNetBuilder.py,16,"b'##+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n## Created by: Albert Berenguel\n## Computer Vision Center (CVC). Universitat Autonoma de Barcelona\n## Email: aberenguel@cvc.uab.es\n## Copyright (c) 2017\n##\n## This source code is licensed under the MIT-style license found in the\n## LICENSE file in the root directory of this source tree\n##+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n\nimport torch\nimport torch.backends.cudnn as cudnn\nfrom tqdm import tqdm\nfrom models.MatchingNetwork import MatchingNetwork\nfrom torch.autograd import Variable\n\nclass miniImageNetBuilder:\n\n    def __init__(self, dataTrain, dataVal, dataTest):\n        """"""\n        Initializes an OneShotBuilder object. The miniImageNetBuilder object takes care of setting up our experiment\n        and provides helper functions such as run_training_epoch and run_validation_epoch to simplify out training\n        and evaluation procedures.\n        :param data: A data provider class\n        """"""\n        self.dataTrain = dataTrain\n        self.dataVal = dataVal\n        self.dataTest = dataTest\n\n    def build_experiment(self, batch_size, classes_per_set, samples_per_class, channels, fce):\n\n        """"""\n        :param batch_size: The experiment batch size\n        :param classes_per_set: An integer indicating the number of classes per support set\n        :param samples_per_class: An integer indicating the number of samples per class\n        :param channels: The image channels\n        :param fce: Whether to use full context embeddings or not\n        :return: a matching_network object, along with the losses, the training ops and the init op\n        """"""\n\n        # Data Loaders\n        self.train_loader = torch.utils.data.DataLoader(self.dataTrain, batch_size=batch_size,\n                                                   shuffle=True, num_workers=4)\n        self.val_loader = torch.utils.data.DataLoader(self.dataVal, batch_size=batch_size,\n                                                        shuffle=True, num_workers=4)\n        self.test_loader = torch.utils.data.DataLoader(self.dataTest, batch_size=batch_size,\n                                                      shuffle=True, num_workers=4)\n\n        # Initialize parameters\n        self.classes_per_set = classes_per_set\n        self.samples_per_class = samples_per_class\n        self.keep_prob = torch.FloatTensor(1)\n\n        # Initialize model\n        self.matchingNet = MatchingNetwork(batch_size=batch_size,\n                                         keep_prob=self.keep_prob, num_channels=channels,\n                                         fce=fce,\n                                         num_classes_per_set=classes_per_set,\n                                         num_samples_per_class=samples_per_class,\n                                         nClasses = 0, image_size = 84)\n        self.isCudaAvailable = torch.cuda.is_available()\n        if self.isCudaAvailable:\n            cudnn.benchmark = True\n            torch.cuda.manual_seed_all(0)\n            self.matchingNet.cuda()\n\n        # Learning parameters\n        self.optimizer = \'adam\'\n        self.lr = 1e-03\n        self.current_lr = 1e-03\n        self.lr_decay = 1e-6\n        self.wd = 1e-4\n        self.total_train_iter = 0\n\n    def run_training_epoch(self):\n        """"""\n        Runs one training epoch\n        :return: mean_training_categorical_crossentropy_loss and mean_training_accuracy\n        """"""\n        total_c_loss = 0.\n        total_accuracy = 0.\n        total_train_batches = len(self.train_loader)\n        # Create the optimizer\n        optimizer = self.__create_optimizer(self.matchingNet, self.lr)\n\n        pbar = tqdm(enumerate(self.train_loader))\n        for batch_idx, (x_support_set, y_support_set, x_target, target_y) in pbar:\n\n                x_support_set = Variable(x_support_set).float()\n                y_support_set = Variable(y_support_set,requires_grad=False).long()\n                x_target = Variable(x_target.squeeze()).float()\n                y_target = Variable(target_y.squeeze(),requires_grad=False).long()\n\n                # y_support_set: Add extra dimension for the one_hot\n                y_support_set = torch.unsqueeze(y_support_set, 2)\n                sequence_length = y_support_set.size()[1]\n                batch_size = y_support_set.size()[0]\n                y_support_set_one_hot = torch.FloatTensor(batch_size, sequence_length,\n                                                          self.classes_per_set).zero_()\n                y_support_set_one_hot.scatter_(2, y_support_set.data, 1)\n                y_support_set_one_hot = Variable(y_support_set_one_hot)\n\n                # Reshape channels\n                #size = x_support_set.size()\n                #x_support_set = x_support_set.view(size[0],size[1],size[4],size[2],size[3])\n                #size = x_target.size()\n                #x_target = x_target.view(size[0], size[3], size[1], size[2])\n                if self.isCudaAvailable:\n                    acc, c_loss_value = self.matchingNet(x_support_set.cuda(), y_support_set_one_hot.cuda(),\n                                                         x_target.cuda(), y_target.cuda())\n                else:\n                    acc, c_loss_value = self.matchingNet(x_support_set, y_support_set_one_hot,\n                                                         x_target, y_target)\n\n                # Before the backward pass, use the optimizer object to zero all of the\n                # gradients for the variables it will update (which are the learnable weights\n                # of the model)\n                optimizer.zero_grad()\n\n                # Backward pass: compute gradient of the loss with respect to model parameters\n                c_loss_value.backward()\n\n                # Calling the step function on an Optimizer makes an update to its parameters\n                optimizer.step()\n\n                # update the optimizer learning rate\n                self.__adjust_learning_rate(optimizer)\n\n                iter_out = ""tr_loss: {}, tr_accuracy: {}"".format(c_loss_value.data[0], acc.data[0])\n                pbar.set_description(iter_out)\n\n                pbar.update(1)\n                total_c_loss += c_loss_value.data[0]\n                total_accuracy += acc.data[0]\n\n                self.total_train_iter += 1\n                if self.total_train_iter % 2000 == 0:\n                    self.lr /= 2\n                    print(""change learning rate"", self.lr)\n\n        total_c_loss = total_c_loss / total_train_batches\n        total_accuracy = total_accuracy / total_train_batches\n        return total_c_loss, total_accuracy\n\n    def run_validation_epoch(self):\n        """"""\n        Runs one validation epoch\n        :param total_val_batches: Number of batches to train on\n        :return: mean_validation_categorical_crossentropy_loss and mean_validation_accuracy\n        """"""\n        total_val_c_loss = 0.\n        total_val_accuracy = 0.\n        total_val_batches = len(self.val_loader)\n        pbar = tqdm(enumerate(self.val_loader))\n        for batch_idx, (x_support_set, y_support_set, x_target, target_y) in pbar:\n\n                x_support_set = Variable(x_support_set).float()\n                y_support_set = Variable(y_support_set,requires_grad=False).long()\n                x_target = Variable(x_target.squeeze()).float()\n                y_target = Variable(target_y.squeeze(),requires_grad=False).long()\n\n                # y_support_set: Add extra dimension for the one_hot\n                y_support_set = torch.unsqueeze(y_support_set, 2)\n                sequence_length = y_support_set.size()[1]\n                batch_size = y_support_set.size()[0]\n                y_support_set_one_hot = torch.FloatTensor(batch_size, sequence_length,\n                                                          self.classes_per_set).zero_()\n                y_support_set_one_hot.scatter_(2, y_support_set.data, 1)\n                y_support_set_one_hot = Variable(y_support_set_one_hot)\n\n                if self.isCudaAvailable:\n                    acc, c_loss_value = self.matchingNet(x_support_set.cuda(), y_support_set_one_hot.cuda(),\n                                                         x_target.cuda(), y_target.cuda())\n                else:\n                    acc, c_loss_value = self.matchingNet(x_support_set, y_support_set_one_hot,\n                                                         x_target, y_target)\n\n                iter_out = ""val_loss: {}, val_accuracy: {}"".format(c_loss_value.data[0], acc.data[0])\n                pbar.set_description(iter_out)\n                pbar.update(1)\n\n                total_val_c_loss += c_loss_value.data[0]\n                total_val_accuracy += acc.data[0]\n\n        total_val_c_loss = total_val_c_loss / total_val_batches\n        total_val_accuracy = total_val_accuracy / total_val_batches\n\n        return total_val_c_loss, total_val_accuracy\n\n    def run_testing_epoch(self):\n        """"""\n        Runs one testing epoch\n        :param total_test_batches: Number of batches to train on\n        :param sess: Session object\n        :return: mean_testing_categorical_crossentropy_loss and mean_testing_accuracy\n        """"""\n        total_test_c_loss = 0.\n        total_test_accuracy = 0.\n        total_test_batches = len(self.test_loader)\n        pbar = tqdm(enumerate(self.test_loader))\n        for batch_idx, (x_support_set, y_support_set, x_target, target_y) in pbar:\n\n                x_support_set = Variable(x_support_set).float()\n                y_support_set = Variable(y_support_set,requires_grad=False).long()\n                x_target = Variable(x_target.squeeze()).float()\n                y_target = Variable(target_y.squeeze(),requires_grad=False).long()\n\n                # y_support_set: Add extra dimension for the one_hot\n                y_support_set = torch.unsqueeze(y_support_set, 2)\n                sequence_length = y_support_set.size()[1]\n                batch_size = y_support_set.size()[0]\n                y_support_set_one_hot = torch.FloatTensor(batch_size, sequence_length,\n                                                          self.classes_per_set).zero_()\n                y_support_set_one_hot.scatter_(2, y_support_set.data, 1)\n                y_support_set_one_hot = Variable(y_support_set_one_hot)\n\n                if self.isCudaAvailable:\n                    acc, c_loss_value = self.matchingNet(x_support_set.cuda(), y_support_set_one_hot.cuda(),\n                                                         x_target.cuda(), y_target.cuda())\n                else:\n                    acc, c_loss_value = self.matchingNet(x_support_set, y_support_set_one_hot,\n                                                         x_target, y_target)\n\n                iter_out = ""test_loss: {}, test_accuracy: {}"".format(c_loss_value.data[0], acc.data[0])\n                pbar.set_description(iter_out)\n                pbar.update(1)\n\n                total_test_c_loss += c_loss_value.data[0]\n                total_test_accuracy += acc.data[0]\n\n        total_test_c_loss = total_test_c_loss / total_test_batches\n        total_test_accuracy = total_test_accuracy / total_test_batches\n        return total_test_c_loss, total_test_accuracy\n\n    def __adjust_learning_rate(self,optimizer):\n        """"""Updates the learning rate given the learning rate decay.\n        The routine has been implemented according to the original Lua SGD optimizer\n        """"""\n        for group in optimizer.param_groups:\n            if \'step\' not in group:\n                group[\'step\'] = 0\n            group[\'step\'] += 1\n\n            group[\'lr\'] = self.lr / (1 + group[\'step\'] * self.lr_decay)\n\n    def __create_optimizer(self,model, new_lr):\n        # setup optimizer\n        if self.optimizer == \'sgd\':\n            optimizer = torch.optim.SGD(model.parameters(), lr=new_lr,\n                                  momentum=0.9, dampening=0.9,\n                                  weight_decay=self.wd)\n        elif self.optimizer == \'adam\':\n            optimizer = torch.optim.Adam(model.parameters(), lr=new_lr,\n                                   weight_decay=self.wd)\n        else:\n            raise Exception(\'Not supported optimizer: {0}\'.format(self.optimizer))\n        return optimizer'"
experiments/__init__.py,0,b''
models/AttentionalClassify.py,1,"b'##+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n## Created by: Albert Berenguel\n## Computer Vision Center (CVC). Universitat Autonoma de Barcelona\n## Email: aberenguel@cvc.uab.es\n## Copyright (c) 2017\n##\n## This source code is licensed under the MIT-style license found in the\n## LICENSE file in the root directory of this source tree\n##+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n\nimport torch.nn as nn\nimport unittest\n\nclass AttentionalClassify(nn.Module):\n    def __init__(self):\n        super(AttentionalClassify, self).__init__()\n\n    def forward(self, similarities, support_set_y):\n\n        """"""\n        Produces pdfs over the support set classes for the target set image.\n        :param similarities: A tensor with cosine similarities of size [sequence_length, batch_size]\n        :param support_set_y: A tensor with the one hot vectors of the targets for each support set image\n                                                                            [sequence_length,  batch_size, num_classes]\n        :return: Softmax pdf\n        """"""\n        softmax = nn.Softmax()\n        softmax_similarities = softmax(similarities)\n        preds = softmax_similarities.unsqueeze(1).bmm(support_set_y).squeeze()\n        return preds\n\nclass AttentionalClassifyTest(unittest.TestCase):\n    def setUp(self):\n        pass\n    def tearDown(self):\n        pass\n\n    def test_forward(self):\n        pass\n\nif __name__ == \'__main__\':\n\n    unittest.main()'"
models/BidirectionalLSTM.py,4,"b'##+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n## Created by: Albert Berenguel\n## Computer Vision Center (CVC). Universitat Autonoma de Barcelona\n## Email: aberenguel@cvc.uab.es\n## Copyright (c) 2017\n##\n## This source code is licensed under the MIT-style license found in the\n## LICENSE file in the root directory of this source tree\n##+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\nimport unittest\n\nclass BidirectionalLSTM(nn.Module):\n    def __init__(self, layer_sizes, batch_size, vector_dim):\n        super(BidirectionalLSTM, self).__init__()\n        """"""\n        Initializes a multi layer bidirectional LSTM\n        :param layer_sizes: A list containing the neuron numbers per layer \n                            e.g. [100, 100, 100] returns a 3 layer, 100\n        :param batch_size: The experiments batch size\n        """"""\n        self.batch_size = batch_size\n        self.hidden_size = layer_sizes[0]\n        self.vector_dim = vector_dim\n        self.num_layers = len(layer_sizes)\n\n        \'\'\'\n        input_size: The number of expected features in the input x\n        hidden_size: The number of features in the hidden state h\n        num_layers: Number of recurrent layers.\n        bias: If False, then the layer does not use bias weights b_ih and b_hh. Default: True\n        batch_first: If True, then the input and output tensors are provided as (batch, seq, feature)\n        dropout: If non-zero, introduces a dropout layer on the outputs of each RNN layer except the last layer\n        bidirectional: If True, becomes a bidirectional RNN. Default: False\n        \'\'\'\n        self.lstm = nn.LSTM(input_size=self.vector_dim,\n                            num_layers=self.num_layers,\n                            hidden_size=self.hidden_size,\n                            bidirectional=True)\n\n    def forward(self, inputs):\n        """"""\n        Runs the bidirectional LSTM, produces outputs and saves both forward and backward states as well as gradients.\n        :param x: The inputs should be a list of shape [sequence_length, batch_size, 64]\n        :return: Returns the LSTM outputs, as well as the forward and backward hidden states.\n        """"""\n        c0 = Variable(torch.rand(self.lstm.num_layers*2, self.batch_size, self.lstm.hidden_size),\n                      requires_grad=False).cuda()\n        h0 = Variable(torch.rand(self.lstm.num_layers*2, self.batch_size, self.lstm.hidden_size),\n                      requires_grad=False).cuda()\n        output, (hn, cn) = self.lstm(inputs, (h0, c0))\n        return output, hn, cn\n\n\nclass BidirectionalLSTMTest(unittest.TestCase):\n    def setUp(self):\n        pass\n    def tearDown(self):\n        pass\n\n    def test_forward(self):\n        pass\n\n\nif __name__ == \'__main__\':\n    unittest.main()\n\n'"
models/Classifier.py,3,"b'##+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n## Created by: Albert Berenguel\n## Computer Vision Center (CVC). Universitat Autonoma de Barcelona\n## Email: aberenguel@cvc.uab.es\n## Copyright (c) 2017\n##\n## This source code is licensed under the MIT-style license found in the\n## LICENSE file in the root directory of this source tree\n##+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.init as init\nfrom torch.autograd import Variable\nimport unittest\nimport numpy as np\nimport math\n\ndef convLayer(in_planes, out_planes, useDropout = False):\n    ""3x3 convolution with padding""\n    seq = nn.Sequential(\n        nn.Conv2d(in_planes, out_planes, kernel_size=3,\n                  stride=1, padding=1, bias=True),\n        nn.BatchNorm2d(out_planes),\n        nn.ReLU(True),\n        nn.MaxPool2d(kernel_size=2, stride=2)\n    )\n    if useDropout: # Add dropout module\n        list_seq = list(seq.modules())[1:]\n        list_seq.append(nn.Dropout(0.1))\n        seq = nn.Sequential(*list_seq)\n\n    return seq\n\nclass Classifier(nn.Module):\n    def __init__(self, layer_size, nClasses = 0, num_channels = 1, useDropout = False, image_size = 28):\n        super(Classifier, self).__init__()\n\n        """"""\n        Builds a CNN to produce embeddings\n        :param layer_sizes: A list of length 4 containing the layer sizes\n        :param nClasses: If nClasses>0, we want a FC layer at the end with nClasses size.\n        :param num_channels: Number of channels of images\n        :param useDroput: use Dropout with p=0.1 in each Conv block\n        """"""\n        self.layer1 = convLayer(num_channels, layer_size, useDropout)\n        self.layer2 = convLayer(layer_size, layer_size, useDropout)\n        self.layer3 = convLayer(layer_size, layer_size, useDropout)\n        self.layer4 = convLayer(layer_size, layer_size, useDropout)\n\n        finalSize = int(math.floor(image_size / (2 * 2 * 2 * 2)))\n        self.outSize = finalSize * finalSize * layer_size\n        if nClasses>0: # We want a linear\n            self.useClassification = True\n            self.layer5 = nn.Linear(self.outSize,nClasses)\n            self.outSize = nClasses\n        else:\n            self.useClassification = False\n\n        self.weights_init(self.layer1)\n        self.weights_init(self.layer2)\n        self.weights_init(self.layer3)\n        self.weights_init(self.layer4)\n\n    def weights_init(self,module):\n        for m in module.modules():\n            if isinstance(m, nn.Conv2d):\n                init.xavier_uniform(m.weight, gain=np.sqrt(2))\n                init.constant(m.bias, 0)\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n\n\n    def forward(self, image_input):\n        """"""\n        Runs the CNN producing the embeddings and the gradients.\n        :param image_input: Image input to produce embeddings for. [batch_size, 28, 28, 1]\n        :return: Embeddings of size [batch_size, 64]\n        """"""\n        x = self.layer1(image_input)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n        x = x.view(x.size(0), -1)\n        if self.useClassification:\n            x = self.layer5(x)\n        return x\n\n\nclass ClassifierTest(unittest.TestCase):\n    def setUp(self):\n        pass\n\n    def tearDown(self):\n        pass\n\n    def test_forward(self):\n        pass\n\nif __name__ == \'__main__\':\n    unittest.main()\n'"
models/DistanceNetwork.py,3,"b'##+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n## Created by: Albert Berenguel\n## Computer Vision Center (CVC). Universitat Autonoma de Barcelona\n## Email: aberenguel@cvc.uab.es\n## Copyright (c) 2017\n##\n## This source code is licensed under the MIT-style license found in the\n## LICENSE file in the root directory of this source tree\n##+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n\nimport torch\nimport torch.nn as nn\nimport unittest\n\nclass DistanceNetwork(nn.Module):\n    def __init__(self):\n        super(DistanceNetwork, self).__init__()\n\n    def forward(self, support_set, input_image):\n\n        """"""\n        Produces pdfs over the support set classes for the target set image.\n        :param support_set: The embeddings of the support set images, tensor of shape [sequence_length, batch_size, 64]\n        :param input_image: The embedding of the target image, tensor of shape [batch_size, 64]\n        :return: Softmax pdf. Tensor with cosine similarities of shape [batch_size, sequence_length]\n        """"""\n        eps = 1e-10\n        similarities = []\n        for support_image in support_set:\n            sum_support = torch.sum(torch.pow(support_image, 2), 1)\n            support_magnitude = sum_support.clamp(eps, float(""inf"")).rsqrt()\n            dot_product = input_image.unsqueeze(1).bmm(support_image.unsqueeze(2)).squeeze()\n            cosine_similarity = dot_product * support_magnitude\n            similarities.append(cosine_similarity)\n        similarities = torch.stack(similarities)\n        return similarities\n\nclass DistanceNetworkTest(unittest.TestCase):\n    def setUp(self):\n        pass\n\n    def tearDown(self):\n        pass\n\n    def test_forward(self):\n        pass\n\nif __name__ == \'__main__\':\n    unittest.main()'"
models/MatchingNetwork.py,5,"b'##+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n## Created by: Albert Berenguel\n## Computer Vision Center (CVC). Universitat Autonoma de Barcelona\n## Email: aberenguel@cvc.uab.es\n## Copyright (c) 2017\n##\n## This source code is licensed under the MIT-style license found in the\n## LICENSE file in the root directory of this source tree\n##+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n\nimport torch\nimport torch.nn as nn\nimport unittest\nimport numpy as np\nfrom BidirectionalLSTM import BidirectionalLSTM\nfrom Classifier import Classifier\nfrom DistanceNetwork import DistanceNetwork\nfrom AttentionalClassify import AttentionalClassify\nimport torch.nn.functional as F\n\nclass MatchingNetwork(nn.Module):\n    def __init__(self, keep_prob, \\\n                 batch_size=100, num_channels=1, learning_rate=0.001, fce=False, num_classes_per_set=5, \\\n                 num_samples_per_class=1, nClasses = 0, image_size = 28):\n        super(MatchingNetwork, self).__init__()\n\n        """"""\n        Builds a matching network, the training and evaluation ops as well as data augmentation routines.\n        :param keep_prob: A tf placeholder of type tf.float32 denotes the amount of dropout to be used\n        :param batch_size: The batch size for the experiment\n        :param num_channels: Number of channels of the images\n        :param is_training: Flag indicating whether we are training or evaluating\n        :param rotate_flag: Flag indicating whether to rotate the images\n        :param fce: Flag indicating whether to use full context embeddings (i.e. apply an LSTM on the CNN embeddings)\n        :param num_classes_per_set: Integer indicating the number of classes per set\n        :param num_samples_per_class: Integer indicating the number of samples per class\n        :param nClasses: total number of classes. It changes the output size of the classifier g with a final FC layer.\n        :param image_input: size of the input image. It is needed in case we want to create the last FC classification \n        """"""\n        self.batch_size = batch_size\n        self.fce = fce\n        self.g = Classifier(layer_size = 64, num_channels=num_channels,\n                            nClasses= nClasses, image_size = image_size )\n        if fce:\n            self.lstm = BidirectionalLSTM(layer_sizes=[32], batch_size=self.batch_size, vector_dim = self.g.outSize)\n        self.dn = DistanceNetwork()\n        self.classify = AttentionalClassify()\n        self.keep_prob = keep_prob\n        self.num_classes_per_set = num_classes_per_set\n        self.num_samples_per_class = num_samples_per_class\n        self.learning_rate = learning_rate\n\n    def forward(self, support_set_images, support_set_labels_one_hot, target_image, target_label):\n        """"""\n        Builds graph for Matching Networks, produces losses and summary statistics.\n        :param support_set_images: A tensor containing the support set images [batch_size, sequence_size, n_channels, 28, 28]\n        :param support_set_labels_one_hot: A tensor containing the support set labels [batch_size, sequence_size, n_classes]\n        :param target_image: A tensor containing the target image (image to produce label for) [batch_size, n_channels, 28, 28]\n        :param target_label: A tensor containing the target label [batch_size, 1]\n        :return: \n        """"""\n        # produce embeddings for support set images\n        encoded_images = []\n        for i in np.arange(support_set_images.size(1)):\n            gen_encode = self.g(support_set_images[:,i,:,:,:])\n            encoded_images.append(gen_encode)\n\n        # produce embeddings for target images\n        for i in np.arange(target_image.size(1)):\n            gen_encode = self.g(target_image[:,i,:,:,:])\n            encoded_images.append(gen_encode)\n            outputs = torch.stack(encoded_images)\n\n            if self.fce:\n                outputs, hn, cn = self.lstm(outputs)\n\n            # get similarity between support set embeddings and target\n            similarities = self.dn(support_set=outputs[:-1], input_image=outputs[-1])\n            similarities = similarities.t()\n\n            # produce predictions for target probabilities\n            preds = self.classify(similarities,support_set_y=support_set_labels_one_hot)\n\n            # calculate accuracy and crossentropy loss\n            values, indices = preds.max(1)\n            if i == 0:\n                accuracy = torch.mean((indices.squeeze() == target_label[:,i]).float())\n                crossentropy_loss = F.cross_entropy(preds, target_label[:,i].long())\n            else:\n                accuracy = accuracy + torch.mean((indices.squeeze() == target_label[:, i]).float())\n                crossentropy_loss = crossentropy_loss + F.cross_entropy(preds, target_label[:, i].long())\n\n            # delete the last target image encoding of encoded_images\n            encoded_images.pop()\n\n        return accuracy/target_image.size(1), crossentropy_loss/target_image.size(1)\n\n\nclass MatchingNetworkTest(unittest.TestCase):\n    def setUp(self):\n        pass\n    def tearDown(self):\n        pass\n    def test_accuracy(self):\n        pass\n\n\nif __name__ == \'__main__\':\n    unittest.main()\n\n\n\n'"
models/__init__.py,0,b''
utils/__init__.py,0,b''
utils/create_miniImagenet.py,0,"b'##+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n## Created by: Albert Berenguel\n## Computer Vision Center (CVC). Universitat Autonoma de Barcelona\n## Email: aberenguel@cvc.uab.es\n## Copyright (c) 2017\n##\n## This source code is licensed under the MIT-style license found in the\n## LICENSE file in the root directory of this source tree\n##+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n\n\'\'\'\nThis code creates the MiniImagenet dataset. Following the partitions given\nby Sachin Ravi and Hugo Larochelle in \nhttps://github.com/twitter/meta-learning-lstm/tree/master/data/miniImagenet\n\'\'\'\n\nimport numpy as np\nimport csv\nimport glob, os\n#from shutil import copyfile\nimport cv2\nfrom tqdm import tqdm\n\npathImageNet = \'/home/aberenguel/Dataset/Imagenet/ILSVRC2012_img_train\'\npathminiImageNet = \'/home/aberenguel/Dataset/miniImagenet/\'\npathImages = os.path.join(pathminiImageNet,\'images/\')\nfilesCSVSachinRavi = [os.path.join(pathminiImageNet,\'train.csv\'),\n                      os.path.join(pathminiImageNet,\'val.csv\'),\n                      os.path.join(pathminiImageNet,\'test.csv\')]\n\n# Check if the folder of images exist. If not create it.\nif not os.path.exists(pathImages):\n    os.makedirs(pathImages)\n\nfor filename in filesCSVSachinRavi:\n    with open(filename) as csvfile:\n        csv_reader = csv.reader(csvfile, delimiter=\',\')\n        next(csv_reader, None)\n        images = {}\n        print(\'Reading IDs....\')\n        for row in tqdm(csv_reader):\n            if row[1] in images.keys():\n                images[row[1]].append(row[0])\n            else:\n                images[row[1]] = [row[0]]\n\n        print(\'Writing photos....\')\n        for c in tqdm(images.keys()): # Iterate over all the classes\n            #os.chdir(pathImageNet) # TODO: change this line that is change the current folder.\n            lst_files = []\n            for file in glob.glob(pathImageNet + ""/*""+c+""*""):\n                lst_files.append(file)\n            # TODO: Sort by name of by index number of the image???\n            # I sort by the number of the image\n            lst_index = [int(i[i.rfind(\'_\')+1:i.rfind(\'.\')]) for i in lst_files]\n            index_sorted = sorted(range(len(lst_index)), key=lst_index.__getitem__)\n\n            # Now iterate\n            index_selected = [int(i[i.index(\'.\') - 4:i.index(\'.\')]) for i in images[c]]\n            selected_images = np.array(index_sorted)[np.array(index_selected) - 1]\n            for i in np.arange(len(selected_images)):\n                # read file and resize to 84x84x3\n                im = cv2.imread(os.path.join(pathImageNet,lst_files[selected_images[i]]))\n                im_resized = cv2.resize(im, (84, 84), interpolation=cv2.INTER_AREA)\n                cv2.imwrite(os.path.join(pathImages, images[c][i]),im_resized)\n\n                #copyfile(os.path.join(pathImageNet,lst_files[selected_images[i]]),os.path.join(pathImages, images[c][i]))\n\n\n\n\n'"
