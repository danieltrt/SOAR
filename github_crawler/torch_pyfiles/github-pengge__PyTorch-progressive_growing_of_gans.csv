file_path,api_count,code
began.py,17,"b""import numpy as np\nimport matplotlib as mpl\nmpl.use('Agg')\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec\nimport os, sys, time\nfrom torch.autograd import Variable\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\n\nsys.path.append('utils')\nfrom nets import *\nfrom data import *\n\n\ndef sample_z(m, n):\n\treturn np.random.uniform(-1., 1., size=[m, n])\n\n\nclass BEGAN():\n\tdef __init__(self, generator, discriminator, data, cuda=True):\n\t\tself.generator = generator\n\t\tself.discriminator = discriminator\n\t\tself.data = data\n\t\tself.cuda = cuda\n\n\t\tself.z_dim = self.data.z_dim\n\t\tself.size = self.data.size\n\t\tself.channel = self.data.channel\n\n\t\tif self.cuda:\n\t\t\tself.generator.cuda()\n\t\t\tself.discriminator.cuda()\n\n\tdef train(self, sample_dir, ckpt_dir, training_epochs=500000, batch_size=32):\n\t\tfig_count = 0\n\t\tg_lr = 2e-4\n\t\td_lr = 1e-4\n\t\tgamma = 0.75\n\t\tlambda_k = 0.001\n\t\tkt = 0.\n\t\tif self.cuda:\n\t\t\tinput = Variable(torch.FloatTensor(batch_size, self.channel, self.size, self.size).cuda())\n\t\t\tz = Variable(torch.FloatTensor(batch_size, self.z_dim).cuda())\n\t\telse:\n\t\t\tinput = Variable(torch.FloatTensor(batch_size, self.channel, self.size, self.size))\n\t\t\tz = Variable(torch.FloatTensor(batch_size, self.z_dim))\n\n\t\toptimizer_D = optim.Adam(self.discriminator.parameters(), lr=d_lr, betas=(0.5, 0.999))\n\t\toptimizer_G = optim.Adam(self.generator.parameters(), lr=g_lr, betas=(0.5, 0.999))\n\n\n\t\tfor epoch in range(training_epochs):\n\t\t\tbegin_time = time.time()\n\n\t\t\t# update D\n\t\t\tself.discriminator.zero_grad()\n\t\t\t# real samples\n\t\t\tX_b_real = self.data(batch_size)\n\t\t\tinput.data.copy_(torch.from_numpy(X_b_real))\n\t\t\tD_real = self.discriminator(input)\n\t\t\tD_loss_real = torch.mean(torch.abs(D_real - input))\n\n\t\t\t# fake samples\n\t\t\tz.data.copy_(torch.from_numpy(sample_z(batch_size, self.z_dim)))\n\t\t\tX_b_fake = self.generator(z)\n\t\t\tD_fake = self.discriminator(X_b_fake.detach())\n\t\t\tD_loss_fake = torch.mean(torch.abs(D_fake - X_b_fake))\n\t\t\tD_loss = D_loss_real - kt * D_loss_fake\n\t\t\tD_loss.backward()\n\n\t\t\toptimizer_D.step()\n\n\t\t\t# update G\n\t\t\tself.generator.zero_grad()\n\t\t\tz.data.copy_(torch.from_numpy(sample_z(batch_size, self.z_dim)))\n\t\t\tX_b_fake = self.generator(z)\n\t\t\tD_fake = self.discriminator(X_b_fake)\n\t\t\tG_loss = torch.mean(torch.abs(D_fake - X_b_fake))\n\t\t\tG_loss.backward()\n\t\t\toptimizer_G.step()\n\n\t\t\t# update kt\n\t\t\tkt = kt + lambda_k * (gamma * D_loss_real - G_loss)\n\t\t\tkt = float(kt.cpu().data.numpy())\n\t\t\tkt = min(1., max(0., kt))\n\n\t\t\t# compute M_global\n\t\t\tM_global = D_loss_real + torch.abs(gamma * D_loss_real - G_loss)\n\n\t\t\telapse_time = time.time() - begin_time\n\t\t\tprint('Iter[%s], d_loss: %.4f, g_loss: %.4f, kt: %.4f, M_global: %.4f, time elapsed: %.4fsec' % \\\n\t\t\t\t\t(epoch+1, D_loss.cpu().data.numpy(), G_loss.cpu().data.numpy(), kt, \\\n\t\t\t\t\t\tM_global.cpu().data.numpy(), elapse_time))\n\n\t\t\tif epoch % 500 == 0:\n\t\t\t\tz.data.copy_(torch.from_numpy(sample_z(batch_size, self.z_dim)))\n\t\t\t\tsamples = self.generator(z).cpu().data.numpy()\n\t\t\t\tfig = self.data.data2fig(samples)\n\t\t\t\tplt.savefig('{}/{}.png'.format(sample_dir, str(fig_count).zfill(3)), bbox_inches='tight')\n\t\t\t\tfig_count += 1\n\t\t\t\tplt.close(fig)\n\n\t\t\tif epoch % 5000 == 0:\n\t\t\t\ttorch.save(self.generator.state_dict(), os.path.join(ckpt_dir, 'G_epoch-%s.pth' % epoch))\n\t\t\t\ttorch.save(self.discriminator.state_dict(), os.path.join(ckpt_dir, 'D_epoch-%s.pth' % epoch))\n\n\nif __name__ == '__main__':\n\tos.environ['CUDA_VISIBLE_DEVICES'] = '1'\n\n\t# save generated images\n\tsample_dir = 'Samples/began'\n\tckpt_dir = 'Models/began'\n\tif not os.path.exists(sample_dir):\n\t\tos.makedirs(sample_dir)\n\tif not os.path.exists(ckpt_dir):\n\t\tos.makedirs(ckpt_dir)\n\n\tgenerator = G_conv()\n\tdiscriminator = D_autoencoder()\n\n\tprint('G:\\n', generator)\n\tprint('D:\\n', discriminator)\n\n\tdata = celebA()\n\n\tbegan = BEGAN(generator, discriminator, data)\n\tbegan.train(sample_dir, ckpt_dir, batch_size=64)\n\t"""
debug.py,4,"b""import torch\r\nfrom torch.autograd import Variable\r\nimport sys\r\nsys.path.append('./models')\r\nsys.path.append('./utils')\r\nfrom models.model import *\r\nfrom utils.data import CelebA\r\n\r\n\r\nG = Generator(num_channels=3, resolution=1024, fmap_max=512, fmap_base=8192, latent_size=512)\r\nD = Discriminator(num_channels=3, resolution=1024, fmap_max=512, fmap_base=8192)\r\n\r\nparam_G = G.named_parameters()\r\nprint('G:')\r\nfor name, p in param_G:\r\n\tprint(name, p.size())\r\n\r\nprint('\\n')\r\n\r\nparam_D = D.named_parameters()\r\nprint('D:')\r\nfor name, p in param_D:\r\n\tprint(name, p.size())\r\n\r\nprint(G)\r\nprint(D)\r\n\r\nG.cuda(1)\r\nD.cuda(1)\r\ndata = CelebA()\r\nz = Variable((torch.rand(3, 512)-0.5)*2).cuda(1)\r\nx = G(z, cur_level=1.2)\r\n# x = Variable(torch.from_numpy(data(3, size=8))).cuda(1)\r\nprint('x:', x.size())\r\nd = D(x, cur_level=1.2, gdrop_strength=0.2)\r\nd = torch.mean(d)\r\nprint(d)\r\nd.backward()\r\n\r\nprint('G:')\r\nfor name, p in G.named_parameters():\r\n\tif p.grad is not None:\r\n\t\tprint(name, p.size(), p.grad.mean().data[0])\r\n\r\nprint('D:')\r\nfor name, p in D.named_parameters():\r\n\tif p.grad is not None:\r\n\t\tprint(name, p.size(), p.grad.mean().data[0])\r\n"""
h5tool.py,0,"b'# Copyright (c) 2017, NVIDIA CORPORATION. All rights reserved.\n#\n# This work is licensed under the Creative Commons Attribution-NonCommercial\n# 4.0 International License. To view a copy of this license, visit\n# http://creativecommons.org/licenses/by-nc/4.0/ or send a letter to\n# Creative Commons, PO Box 1866, Mountain View, CA 94042, USA.\n\nimport os\nimport sys\nimport io\nimport glob\nimport pickle\nimport argparse\nimport threading\nimport Queue\nimport traceback\nimport numpy as np\nimport scipy.ndimage\nimport PIL.Image\nimport h5py # conda install h5py\n\n#----------------------------------------------------------------------------\n\nclass HDF5Exporter:\n    def __init__(self, h5_filename, resolution, channels=3):\n        rlog2 = int(np.floor(np.log2(resolution)))\n        assert resolution == 2 ** rlog2\n        self.resolution = resolution\n        self.channels = channels\n        self.h5_file = h5py.File(h5_filename, \'w\')\n        self.h5_lods = []\n        self.buffers = []\n        self.buffer_sizes = []\n        for lod in xrange(rlog2, -1, -1):\n            r = 2 ** lod; c = channels\n            bytes_per_item = c * (r ** 2)\n            chunk_size = int(np.ceil(128.0 / bytes_per_item))\n            buffer_size = int(np.ceil(512.0 * np.exp2(20) / bytes_per_item))\n            lod = self.h5_file.create_dataset(\'data%dx%d\' % (r,r), shape=(0,c,r,r), dtype=np.uint8,\n                maxshape=(None,c,r,r), chunks=(chunk_size,c,r,r), compression=\'gzip\', compression_opts=4)\n            self.h5_lods.append(lod)\n            self.buffers.append(np.zeros((buffer_size,c,r,r), dtype=np.uint8))\n            self.buffer_sizes.append(0)\n\n    def close(self):\n        for lod in xrange(len(self.h5_lods)):\n            self.flush_lod(lod)\n        self.h5_file.close()\n\n    def add_images(self, img):\n        assert img.ndim == 4 and img.shape[1] == self.channels and img.shape[2] == img.shape[3]\n        assert img.shape[2] >= self.resolution and img.shape[2] == 2 ** int(np.floor(np.log2(img.shape[2])))\n        for lod in xrange(len(self.h5_lods)):\n            while img.shape[2] > self.resolution / (2 ** lod):\n                img = img.astype(np.float32)\n                img = (img[:, :, 0::2, 0::2] + img[:, :, 0::2, 1::2] + img[:, :, 1::2, 0::2] + img[:, :, 1::2, 1::2]) * 0.25\n            quant = np.uint8(np.clip(np.round(img), 0, 255))\n            ofs = 0\n            while ofs < quant.shape[0]:\n                num = min(quant.shape[0] - ofs, self.buffers[lod].shape[0] - self.buffer_sizes[lod])\n                self.buffers[lod][self.buffer_sizes[lod] : self.buffer_sizes[lod] + num] = quant[ofs : ofs + num]\n                self.buffer_sizes[lod] += num\n                if self.buffer_sizes[lod] == self.buffers[lod].shape[0]:\n                    self.flush_lod(lod)\n                ofs += num\n\n    def num_images(self):\n        return self.h5_lods[0].shape[0] + self.buffer_sizes[0]\n        \n    def flush_lod(self, lod):\n        num = self.buffer_sizes[lod]\n        if num > 0:\n            self.h5_lods[lod].resize(self.h5_lods[lod].shape[0] + num, axis=0)\n            self.h5_lods[lod][-num:] = self.buffers[lod][:num]\n            self.buffer_sizes[lod] = 0\n\n#----------------------------------------------------------------------------\n\nclass ExceptionInfo(object):\n    def __init__(self):\n        self.type, self.value = sys.exc_info()[:2]\n        self.traceback = traceback.format_exc()\n\n#----------------------------------------------------------------------------\n\nclass WorkerThread(threading.Thread):\n    def __init__(self, task_queue):\n        threading.Thread.__init__(self)\n        self.task_queue = task_queue\n\n    def run(self):\n        while True:\n            func, args, result_queue = self.task_queue.get()\n            if func is None:\n                break\n            try:\n                result = func(*args)\n            except:\n                result = ExceptionInfo()\n            result_queue.put((result, args))\n\n#----------------------------------------------------------------------------\n\nclass ThreadPool(object):\n    def __init__(self, num_threads):\n        assert num_threads >= 1\n        self.task_queue = Queue.Queue()\n        self.result_queues = dict()\n        self.num_threads = num_threads\n        for idx in xrange(self.num_threads):\n            thread = WorkerThread(self.task_queue)\n            thread.daemon = True\n            thread.start()\n\n    def add_task(self, func, args=()):\n        assert hasattr(func, \'__call__\') # must be a function\n        if func not in self.result_queues:\n            self.result_queues[func] = Queue.Queue()\n        self.task_queue.put((func, args, self.result_queues[func]))\n\n    def get_result(self, func, verbose_exceptions=True): # returns (result, args)\n        result, args = self.result_queues[func].get()\n        if isinstance(result, ExceptionInfo):\n            if verbose_exceptions:\n                print(\'\\n\\nWorker thread caught an exception:\\n\' + result.traceback + \'\\n\')\n            raise Exception(\'%s, %s\' % (result.type, result.value))\n        return result, args\n\n    def finish(self):\n        for idx in xrange(self.num_threads):\n            self.task_queue.put((None, (), None))\n\n    def __enter__(self): # for \'with\' statement\n        return self\n\n    def __exit__(self, *excinfo):\n        self.finish()\n\n    def process_items_concurrently(self, item_iterator, process_func=lambda x: x, pre_func=lambda x: x, post_func=lambda x: x, max_items_in_flight=None):\n        if max_items_in_flight is None: max_items_in_flight = self.num_threads * 4\n        assert max_items_in_flight >= 1\n        results = []\n        retire_idx = [0]\n\n        def task_func(prepared, idx):\n            return process_func(prepared)\n           \n        def retire_result():\n            processed, (prepared, idx) = self.get_result(task_func)\n            results[idx] = processed\n            while retire_idx[0] < len(results) and results[retire_idx[0]] is not None:\n                yield post_func(results[retire_idx[0]])\n                results[retire_idx[0]] = None\n                retire_idx[0] += 1\n    \n        for idx, item in enumerate(item_iterator):\n            prepared = pre_func(item)\n            results.append(None)\n            self.add_task(func=task_func, args=(prepared, idx))\n            while retire_idx[0] < idx - max_items_in_flight + 2:\n                for res in retire_result(): yield res\n        while retire_idx[0] < len(results):\n            for res in retire_result(): yield res\n\n#----------------------------------------------------------------------------\n\ndef inspect(h5_filename):\n    print(\'%-20s%s\' % (\'HDF5 filename\', h5_filename))\n    file_size = os.stat(h5_filename).st_size\n    print(\'%-20s%.2f GB\' % (\'Total size\', float(file_size) / np.exp2(30)))\n    \n    h5 = h5py.File(h5_filename, \'r\')\n    lods = sorted([value for key, value in h5.iteritems() if key.startswith(\'data\')], key=lambda lod: -lod.shape[3])\n    shapes = [lod.shape for lod in lods]\n    shape = shapes[0]\n    h5.close()\n    print(\'%-20s%d\' % (\'Total images\', shape[0]))\n    print(\'%-20s%dx%d\' % (\'Resolution\', shape[3], shape[2]))\n    print(\'%-20s%d\' % (\'Color channels\', shape[1]))\n    print(\'%-20s%.2f KB\' % (\'Size per image\', float(file_size) / shape[0] / np.exp2(10)))\n    \n    if len(lods) != int(np.log2(shape[3])) + 1:\n        print(\'Warning: The HDF5 file contains incorrect number of LODs\')\n    if any(s[0] != shape[0] for s in shapes):\n        print(\'Warning: The HDF5 file contains inconsistent number of images in different LODs\')\n        print(\'Perhaps the dataset creation script was terminated abruptly?\')\n\n#----------------------------------------------------------------------------\n\ndef compare(first_h5, second_h5):\n    print(\'Comparing %s vs. %s\' % (first_h5, second_h5))\n    h5_a = h5py.File(first_h5, \'r\')\n    h5_b = h5py.File(second_h5, \'r\')\n    lods_a = sorted([value for key, value in h5_a.iteritems() if key.startswith(\'data\')], key=lambda lod: -lod.shape[3])\n    lods_b = sorted([value for key, value in h5_b.iteritems() if key.startswith(\'data\')], key=lambda lod: -lod.shape[3])\n    shape_a = lods_a[0].shape\n    shape_b = lods_b[0].shape\n    \n    if shape_a[1] != shape_b[1]:\n        print(\'The datasets have different number of color channels: %d vs. %d\' % (shape_a[1], shape_b[1]))\n    elif shape_a[3] != shape_b[3] or shape_a[2] != shape_b[2]:\n        print(\'The datasets have different resolution: %dx%d vs. %dx%d\' % (shape_a[3], shape_a[2], shape_b[3], shape_b[2]))\n    else:\n        min_images = min(shape_a[0], shape_b[0])\n        num_diffs = 0\n        for idx in range(min_images):\n            print(\'%d / %d\\r\' % (idx, min_images))\n            if np.any(lods_a[0][idx] != lods_b[0][idx]):\n                print(\'%-40s\\r\' % \'\')\n                print(\'Different image: %d\' % idx)\n                num_diffs += 1\n        if shape_a[0] != shape_b[0]:\n            print(\'The datasets contain different number of images: %d vs. %d\' % (shape_a[0], shape_b[0]))\n        if num_diffs == 0:\n            print(\'All %d images are identical.\' % min_images)\n        else:\n            print(\'%d images out of %d are different.\' % (num_diffs, min_images))\n            \n    h5_a.close()\n    h5_b.close()\n\n#----------------------------------------------------------------------------\n\ndef display(h5_filename, start=None, stop=None, step=None):\n    print(\'Displaying images from %s\' % h5_filename)\n    h5 = h5py.File(h5_filename, \'r\')\n    lods = sorted([value for key, value in h5.iteritems() if key.startswith(\'data\')], key=lambda lod: -lod.shape[3])\n    indices = range(lods[0].shape[0])\n    indices = indices[start : stop : step]\n    \n    import cv2 # pip install opencv-python\n    window_name = \'h5tool\'\n    cv2.namedWindow(window_name)\n    print(\'Press SPACE or ENTER to advance, ESC to exit.\')\n\n    for idx in indices:\n        print(\'%d / %d\\r\' % (idx, lods[0].shape[0]))\n        img = lods[0][idx]\n        img = img.transpose(1, 2, 0) # CHW => HWC\n        img = img[:, :, ::-1] # RGB => BGR\n        cv2.imshow(window_name, img)\n        c = cv2.waitKey()\n        if c == 27:\n            break\n            \n    h5.close()\n    print(\'%-40s\\r\' % \'\')\n    print(\'Done.\')\n\n#----------------------------------------------------------------------------\n\ndef extract(h5_filename, output_dir, start=None, stop=None, step=None):\n    print(\'Extracting images from %s to %s\' % (h5_filename, output_dir))\n    h5 = h5py.File(h5_filename, \'r\')\n    lods = sorted([value for key, value in h5.iteritems() if key.startswith(\'data\')], key=lambda lod: -lod.shape[3])\n    shape = lods[0].shape\n    indices = range(shape[0])[start : stop : step]\n    if not os.path.isdir(output_dir):\n        os.makedirs(output_dir)\n        \n    for idx in indices:\n        print(\'%d / %d\\r\' % (idx, shape[0]))\n        img = lods[0][idx]\n        if img.shape[0] == 1:\n            img = PIL.Image.fromarray(img[0], \'L\')\n        else:\n            img = PIL.Image.fromarray(img.transpose(1, 2, 0), \'RGB\')\n        img.save(os.path.join(output_dir, \'img%08d.png\' % idx))\n        \n    h5.close()\n    print(\'%-40s\\r\' % \'\')\n    print(\'Extracted %d images.\' % len(indices))\n\n#----------------------------------------------------------------------------\n\ndef create_custom(h5_filename, image_dir):\n    print(\'Creating custom dataset %s from %s\' % (h5_filename, image_dir))\n    glob_pattern = os.path.join(image_dir, \'*\')\n    image_filenames = sorted(glob.glob(glob_pattern))\n    if len(image_filenames) == 0:\n        print(\'Error: No input images found in %s\' % glob_pattern)\n        return\n        \n    img = np.asarray(PIL.Image.open(image_filenames[0]))\n    resolution = img.shape[0]\n    channels = img.shape[2] if img.ndim == 3 else 1\n    if img.shape[1] != resolution:\n        print(\'Error: Input images must have the same width and height\')\n        return\n    if resolution != 2 ** int(np.floor(np.log2(resolution))):\n        print(\'Error: Input image resolution must be a power-of-two\')\n        return\n    if channels not in [1, 3]:\n        print(\'Error: Input images must be stored as RGB or grayscale\')\n    \n    h5 = HDF5Exporter(h5_filename, resolution, channels)\n    for idx in xrange(len(image_filenames)):\n        print(\'%d / %d\\r\' % (idx, len(image_filenames)))\n        img = np.asarray(PIL.Image.open(image_filenames[idx]))\n        if channels == 1:\n            img = img[np.newaxis, :, :] # HW => CHW\n        else:\n            img = img.transpose(2, 0, 1) # HWC => CHW\n        h5.add_images(img[np.newaxis])\n\n    print(\'%-40s\\r\' % \'Flushing data...\')\n    h5.close()\n    print(\'%-40s\\r\' % \'\')\n    print(\'Added %d images.\' % len(image_filenames))\n\n#----------------------------------------------------------------------------\n\ndef create_mnist(h5_filename, mnist_dir, export_labels=False):\n    print(\'Loading MNIST data from %s\' % mnist_dir)\n    import gzip\n    with gzip.open(os.path.join(mnist_dir, \'train-images-idx3-ubyte.gz\'), \'rb\') as file:\n        images = np.frombuffer(file.read(), np.uint8, offset=16)\n    with gzip.open(os.path.join(mnist_dir, \'train-labels-idx1-ubyte.gz\'), \'rb\') as file:\n        labels = np.frombuffer(file.read(), np.uint8, offset=8)\n    images = images.reshape(-1, 1, 28, 28)\n    images = np.pad(images, [(0,0), (0,0), (2,2), (2,2)], \'constant\', constant_values=0)\n    assert images.shape == (60000, 1, 32, 32) and images.dtype == np.uint8\n    assert labels.shape == (60000,) and labels.dtype == np.uint8\n    assert np.min(images) == 0 and np.max(images) == 255\n    assert np.min(labels) == 0 and np.max(labels) == 9\n    \n    print(\'Creating %s\' % h5_filename)\n    h5 = HDF5Exporter(h5_filename, 32, 1)\n    h5.add_images(images)\n    h5.close()\n    \n    if export_labels:\n        npy_filename = os.path.splitext(h5_filename)[0] + \'-labels.npy\'        \n        print(\'Creating %s\' % npy_filename)\n        onehot = np.zeros((labels.size, np.max(labels) + 1), dtype=np.float32)\n        onehot[np.arange(labels.size), labels] = 1.0\n        np.save(npy_filename, onehot)\n    print(\'Added %d images.\' % images.shape[0])\n\n#----------------------------------------------------------------------------\n\ndef create_mnist_rgb(h5_filename, mnist_dir, num_images=1000000, random_seed=123):\n    print(\'Loading MNIST data from %s\' % mnist_dir)\n    import gzip\n    with gzip.open(os.path.join(mnist_dir, \'train-images-idx3-ubyte.gz\'), \'rb\') as file:\n        images = np.frombuffer(file.read(), np.uint8, offset=16)\n    images = images.reshape(-1, 28, 28)\n    images = np.pad(images, [(0,0), (2,2), (2,2)], \'constant\', constant_values=0)\n    assert images.shape == (60000, 32, 32) and images.dtype == np.uint8\n    assert np.min(images) == 0 and np.max(images) == 255\n    \n    print(\'Creating %s\' % h5_filename)\n    h5 = HDF5Exporter(h5_filename, 32, 3)\n    np.random.seed(random_seed)\n    for idx in xrange(num_images):\n        if idx % 100 == 0:\n            print(\'%d / %d\\r\' % (idx, num_images))\n        h5.add_images(images[np.newaxis, np.random.randint(images.shape[0], size=3)])\n\n    print(\'%-40s\\r\' % \'Flushing data...\')\n    h5.close()\n    print(\'%-40s\\r\' % \'\')\n    print(\'Added %d images.\' % num_images)\n\n#----------------------------------------------------------------------------\n\ndef create_cifar10(h5_filename, cifar10_dir, export_labels=False):\n    print(\'Loading CIFAR-10 data from %s\' % cifar10_dir)\n    images = []\n    labels = []\n    for batch in xrange(1, 6):\n        with open(os.path.join(cifar10_dir, \'data_batch_%d\' % batch), \'rb\') as file:\n            data = pickle.load(file)\n        images.append(data[\'data\'].reshape(-1, 3, 32, 32))\n        labels.append(np.uint8(data[\'labels\']))\n    images = np.concatenate(images)\n    labels = np.concatenate(labels)\n    \n    assert images.shape == (50000, 3, 32, 32) and images.dtype == np.uint8\n    assert labels.shape == (50000,) and labels.dtype == np.uint8\n    assert np.min(images) == 0 and np.max(images) == 255\n    assert np.min(labels) == 0 and np.max(labels) == 9\n\n    print(\'Creating %s\' % h5_filename)\n    h5 = HDF5Exporter(h5_filename, 32, 3)\n    h5.add_images(images)\n    h5.close()\n    \n    if export_labels:\n        npy_filename = os.path.splitext(h5_filename)[0] + \'-labels.npy\'        \n        print(\'Creating %s\' % npy_filename)\n        onehot = np.zeros((labels.size, np.max(labels) + 1), dtype=np.float32)\n        onehot[np.arange(labels.size), labels] = 1.0\n        np.save(npy_filename, onehot)\n    print(\'Added %d images.\' % images.shape[0])\n\n#----------------------------------------------------------------------------\n\ndef create_lsun(h5_filename, lmdb_dir, resolution=256, max_images=None):\n    print(\'Creating LSUN dataset %s from %s\' % (h5_filename, lmdb_dir))\n    import lmdb # pip install lmdb\n    import cv2 # pip install opencv-python\n    with lmdb.open(lmdb_dir, readonly=True).begin(write=False) as txn:\n        total_images = txn.stat()[\'entries\']\n        if max_images is None:\n            max_images = total_images\n            \n        h5 = HDF5Exporter(h5_filename, resolution, 3)\n        for idx, (key, value) in enumerate(txn.cursor()):\n            print(\'%d / %d\\r\' % (h5.num_images(), min(h5.num_images() + total_images - idx, max_images)))\n            try:\n                try:\n                    img = cv2.imdecode(np.fromstring(value, dtype=np.uint8), 1)\n                    if img is None:\n                        raise IOError(\'cv2.imdecode failed\')\n                    img = img[:, :, ::-1] # BGR => RGB\n                except IOError:\n                    img = np.asarray(PIL.Image.open(io.BytesIO(value)))\n                crop = np.min(img.shape[:2])\n                img = img[(img.shape[0] - crop) / 2 : (img.shape[0] + crop) / 2, (img.shape[1] - crop) / 2 : (img.shape[1] + crop) / 2]\n                img = PIL.Image.fromarray(img, \'RGB\')\n                img = img.resize((resolution, resolution), PIL.Image.ANTIALIAS)\n                img = np.asarray(img)\n                img = img.transpose(2, 0, 1) # HWC => CHW\n                h5.add_images(img[np.newaxis])\n            except:\n                print(\'%-40s\\r\' % \'\')\n                print(sys.exc_info()[1])\n                raise\n            if h5.num_images() == max_images:\n                break\n\n    print(\'%-40s\\r\' % \'Flushing data...\')\n    num_added = h5.num_images()\n    h5.close()\n    print(\'%-40s\\r\' % \'\')\n    print(\'Added %d images.\' % num_added)\n        \n#----------------------------------------------------------------------------\n\ndef create_celeba(h5_filename, celeba_dir, cx=89, cy=121):\n    print(\'Creating CelebA dataset %s from %s\' % (h5_filename, celeba_dir))\n    glob_pattern = os.path.join(celeba_dir, \'img_align_celeba_png\', \'*.png\')\n    image_filenames = sorted(glob.glob(glob_pattern))\n    num_images = 202599\n    if len(image_filenames) != num_images:\n        print(\'Error: Expected to find %d images in %s\' % (num_images, glob_pattern))\n        return\n    \n    h5 = HDF5Exporter(h5_filename, 128, 3)\n    for idx in xrange(num_images):\n        print(\'%d / %d\\r\' % (idx, num_images))\n        img = np.asarray(PIL.Image.open(image_filenames[idx]))\n        assert img.shape == (218, 178, 3)\n        img = img[cy - 64 : cy + 64, cx - 64 : cx + 64]\n        img = img.transpose(2, 0, 1) # HWC => CHW\n        h5.add_images(img[np.newaxis])\n\n    print(\'%-40s\\r\' % \'Flushing data...\')\n    h5.close()\n    print(\'%-40s\\r\' % \'\')\n    print(\'Added %d images.\' % num_images)\n\n#----------------------------------------------------------------------------\n\ndef create_celeba_hq(h5_filename, celeba_dir, delta_dir, num_threads=4, num_tasks=100):\n    print(\'Loading CelebA data from %s\' % celeba_dir)\n    glob_pattern = os.path.join(celeba_dir, \'img_celeba\', \'*.jpg\')\n    glob_expected = 202599\n    if len(glob.glob(glob_pattern)) != glob_expected:\n        print(\'Error: Expected to find %d images in %s\' % (glob_expected, glob_pattern))\n        return\n    with open(os.path.join(celeba_dir, \'Anno\', \'list_landmarks_celeba.txt\'), \'rt\') as file:\n        landmarks = [[float(value) for value in line.split()[1:]] for line in file.readlines()[2:]]\n        landmarks = np.float32(landmarks).reshape(-1, 5, 2)\n        \n    print(\'Loading CelebA-HQ deltas from %s\' % delta_dir)\n    import hashlib\n    import bz2\n    import zipfile\n    import base64\n    import cryptography.hazmat.primitives.hashes\n    import cryptography.hazmat.backends\n    import cryptography.hazmat.primitives.kdf.pbkdf2\n    import cryptography.fernet\n    glob_pattern = os.path.join(delta_dir, \'delta*.zip\')\n    glob_expected = 30\n    if len(glob.glob(glob_pattern)) != glob_expected:\n        print(\'Error: Expected to find %d zips in %s\' % (glob_expected, glob_pattern))\n        return\n    with open(os.path.join(delta_dir, \'image_list.txt\'), \'rt\') as file:\n        lines = [line.split() for line in file]\n        fields = dict()\n        for idx, field in enumerate(lines[0]):\n            type = int if field.endswith(\'idx\') else str\n            fields[field] = [type(line[idx]) for line in lines[1:]]\n\n    def rot90(v):\n        return np.array([-v[1], v[0]])\n\n    def process_func(idx):\n        # Load original image.\n        orig_idx = fields[\'orig_idx\'][idx]\n        orig_file = fields[\'orig_file\'][idx]\n        orig_path = os.path.join(celeba_dir, \'img_celeba\', orig_file)\n        img = PIL.Image.open(orig_path)\n\n        # Choose oriented crop rectangle.\n        lm = landmarks[orig_idx]\n        eye_avg = (lm[0] + lm[1]) * 0.5 + 0.5\n        mouth_avg = (lm[3] + lm[4]) * 0.5 + 0.5\n        eye_to_eye = lm[1] - lm[0]\n        eye_to_mouth = mouth_avg - eye_avg\n        x = eye_to_eye - rot90(eye_to_mouth)\n        x /= np.hypot(*x)\n        x *= max(np.hypot(*eye_to_eye) * 2.0, np.hypot(*eye_to_mouth) * 1.8)\n        y = rot90(x)\n        c = eye_avg + eye_to_mouth * 0.1\n        quad = np.stack([c - x - y, c - x + y, c + x + y, c + x - y])\n        zoom = 1024 / (np.hypot(*x) * 2)\n\n        # Shrink.\n        shrink = int(np.floor(0.5 / zoom))\n        if shrink > 1:\n            size = (int(np.round(float(img.size[0]) / shrink)), int(np.round(float(img.size[1]) / shrink)))\n            img = img.resize(size, PIL.Image.ANTIALIAS)\n            quad /= shrink\n            zoom *= shrink\n\n        # Crop.\n        border = max(int(np.round(1024 * 0.1 / zoom)), 3)\n        crop = (int(np.floor(min(quad[:,0]))), int(np.floor(min(quad[:,1]))), int(np.ceil(max(quad[:,0]))), int(np.ceil(max(quad[:,1]))))\n        crop = (max(crop[0] - border, 0), max(crop[1] - border, 0), min(crop[2] + border, img.size[0]), min(crop[3] + border, img.size[1]))\n        if crop[2] - crop[0] < img.size[0] or crop[3] - crop[1] < img.size[1]:\n            img = img.crop(crop)\n            quad -= crop[0:2]\n\n        # Simulate super-resolution.\n        superres = int(np.exp2(np.ceil(np.log2(zoom))))\n        if superres > 1:\n            img = img.resize((img.size[0] * superres, img.size[1] * superres), PIL.Image.ANTIALIAS)\n            quad *= superres\n            zoom /= superres\n\n        # Pad.\n        pad = (int(np.floor(min(quad[:,0]))), int(np.floor(min(quad[:,1]))), int(np.ceil(max(quad[:,0]))), int(np.ceil(max(quad[:,1]))))\n        pad = (max(-pad[0] + border, 0), max(-pad[1] + border, 0), max(pad[2] - img.size[0] + border, 0), max(pad[3] - img.size[1] + border, 0))\n        if max(pad) > border - 4:\n            pad = np.maximum(pad, int(np.round(1024 * 0.3 / zoom)))\n            img = np.pad(np.float32(img), ((pad[1], pad[3]), (pad[0], pad[2]), (0, 0)), \'reflect\')\n            h, w, _ = img.shape\n            y, x, _ = np.mgrid[:h, :w, :1]\n            mask = 1.0 - np.minimum(np.minimum(np.float32(x) / pad[0], np.float32(y) / pad[1]), np.minimum(np.float32(w-1-x) / pad[2], np.float32(h-1-y) / pad[3]))\n            blur = 1024 * 0.02 / zoom\n            img += (scipy.ndimage.gaussian_filter(img, [blur, blur, 0]) - img) * np.clip(mask * 3.0 + 1.0, 0.0, 1.0)\n            img += (np.median(img, axis=(0,1)) - img) * np.clip(mask, 0.0, 1.0)\n            img = PIL.Image.fromarray(np.uint8(np.clip(np.round(img), 0, 255)), \'RGB\')\n            quad += pad[0:2]\n            \n        # Transform.\n        img = img.transform((4096, 4096), PIL.Image.QUAD, (quad + 0.5).flatten(), PIL.Image.BILINEAR)\n        img = img.resize((1024, 1024), PIL.Image.ANTIALIAS)\n        img = np.asarray(img).transpose(2, 0, 1)\n        \n        # Verify MD5.\n        md5 = hashlib.md5()\n        md5.update(img.tobytes())\n        # assert md5.hexdigest() == fields[\'proc_md5\'][idx]  # disable md5 verify\n        \n        # Load delta image and original JPG.\n        with zipfile.ZipFile(os.path.join(delta_dir, \'deltas%05d.zip\' % (idx - idx % 1000)), \'r\') as zip:\n            delta_bytes = zip.read(\'delta%05d.dat\' % idx)\n        with open(orig_path, \'rb\') as file:\n            orig_bytes = file.read()\n        \n        # Decrypt delta image, using original JPG data as decryption key.\n        algorithm = cryptography.hazmat.primitives.hashes.SHA256()\n        backend = cryptography.hazmat.backends.default_backend()\n        kdf = cryptography.hazmat.primitives.kdf.pbkdf2.PBKDF2HMAC(algorithm=algorithm, length=32, salt=orig_file, iterations=100000, backend=backend)\n        key = base64.urlsafe_b64encode(kdf.derive(orig_bytes))\n        delta = np.frombuffer(bz2.decompress(cryptography.fernet.Fernet(key).decrypt(delta_bytes)), dtype=np.uint8).reshape(3, 1024, 1024)\n        \n        # Apply delta image.\n        img = img + delta\n        \n        # Verify MD5.\n        md5 = hashlib.md5()\n        md5.update(img.tobytes())\n        # assert md5.hexdigest() == fields[\'final_md5\'][idx]  # disable md5 verify\n        return idx, img\n\n    print(\'Creating %s\' % h5_filename)\n    h5 = HDF5Exporter(h5_filename, 1024, 3)\n    with ThreadPool(num_threads) as pool:\n        print(\'%d / %d\\r\' % (0, len(fields[\'idx\'])))\n        for idx, img in pool.process_items_concurrently(fields[\'idx\'], process_func=process_func, max_items_in_flight=num_tasks):\n            h5.add_images(img[np.newaxis])\n            print(\'%d / %d\\r\' % (idx + 1, len(fields[\'idx\'])))\n\n    print(\'%-40s\\r\' % \'Flushing data...\')\n    h5.close()\n    print(\'%-40s\\r\' % \'\')\n    print(\'Added %d images.\' % len(fields[\'idx\']))\n\n#----------------------------------------------------------------------------\n\ndef execute_cmdline(argv):\n    prog = argv[0]\n    parser = argparse.ArgumentParser(\n        prog        = prog,\n        description = \'Tool for creating, extracting, and visualizing HDF5 datasets.\',\n        epilog      = \'Type ""%s <command> -h"" for more information.\' % prog)\n        \n    subparsers = parser.add_subparsers(dest=\'command\')\n    def add_command(cmd, desc, example=None):\n        epilog = \'Example: %s %s\' % (prog, example) if example is not None else None\n        return subparsers.add_parser(cmd, description=desc, help=desc, epilog=epilog)\n\n    p = add_command(    \'inspect\',          \'Print information about HDF5 dataset.\',\n                                            \'inspect mnist-32x32.h5\')\n    p.add_argument(     \'h5_filename\',      help=\'HDF5 file to inspect\')\n\n    p = add_command(    \'compare\',          \'Compare two HDF5 datasets.\',\n                                            \'compare mydataset.h5 mnist-32x32.h5\')\n    p.add_argument(     \'first_h5\',         help=\'First HDF5 file to compare\')\n    p.add_argument(     \'second_h5\',        help=\'Second HDF5 file to compare\')\n\n    p = add_command(    \'display\',          \'Display images in HDF5 dataset.\',\n                                            \'display mnist-32x32.h5\')\n    p.add_argument(     \'h5_filename\',      help=\'HDF5 file to visualize\')\n    p.add_argument(     \'--start\',          help=\'Start index (inclusive)\', type=int, default=None)\n    p.add_argument(     \'--stop\',           help=\'Stop index (exclusive)\', type=int, default=None)\n    p.add_argument(     \'--step\',           help=\'Step between consecutive indices\', type=int, default=None)\n  \n    p = add_command(    \'extract\',          \'Extract images from HDF5 dataset.\',\n                                            \'extract mnist-32x32.h5 cifar10-images\')\n    p.add_argument(     \'h5_filename\',      help=\'HDF5 file to extract\')\n    p.add_argument(     \'output_dir\',       help=\'Directory to extract the images into\')\n    p.add_argument(     \'--start\',          help=\'Start index (inclusive)\', type=int, default=None)\n    p.add_argument(     \'--stop\',           help=\'Stop index (exclusive)\', type=int, default=None)\n    p.add_argument(     \'--step\',           help=\'Step between consecutive indices\', type=int, default=None)\n\n    p = add_command(    \'create_custom\',    \'Create HDF5 dataset for custom images.\',\n                                            \'create_custom mydataset.h5 myimagedir\')\n    p.add_argument(     \'h5_filename\',      help=\'HDF5 file to create\')\n    p.add_argument(     \'image_dir\',        help=\'Directory to read the images from\')\n\n    p = add_command(    \'create_mnist\',     \'Create HDF5 dataset for MNIST.\',\n                                            \'create_mnist mnist-32x32.h5 ~/mnist --export_labels\')\n    p.add_argument(     \'h5_filename\',      help=\'HDF5 file to create\')\n    p.add_argument(     \'mnist_dir\',        help=\'Directory to read MNIST data from\')\n    p.add_argument(     \'--export_labels\',  help=\'Create *-labels.npy alongside the HDF5\', action=\'store_true\')\n\n    p = add_command(    \'create_mnist_rgb\', \'Create HDF5 dataset for MNIST-RGB.\',\n                                            \'create_mnist_rgb mnist-rgb-32x32.h5 ~/mnist\')\n    p.add_argument(     \'h5_filename\',      help=\'HDF5 file to create\')\n    p.add_argument(     \'mnist_dir\',        help=\'Directory to read MNIST data from\')\n    p.add_argument(     \'--num_images\',     help=\'Number of composite images to create (default: 1000000)\', type=int, default=1000000)\n    p.add_argument(     \'--random_seed\',    help=\'Random seed (default: 123)\', type=int, default=123)\n\n    p = add_command(    \'create_cifar10\',   \'Create HDF5 dataset for CIFAR-10.\',\n                                            \'create_cifar10 cifar-10-32x32.h5 ~/cifar10 --export_labels\')\n    p.add_argument(     \'h5_filename\',      help=\'HDF5 file to create\')\n    p.add_argument(     \'cifar10_dir\',      help=\'Directory to read CIFAR-10 data from\')\n    p.add_argument(     \'--export_labels\',  help=\'Create *-labels.npy alongside the HDF5\', action=\'store_true\')\n\n    p = add_command(    \'create_lsun\',      \'Create HDF5 dataset for single LSUN category.\',\n                                            \'create_lsun lsun-airplane-256x256-100k.h5 ~/lsun/airplane_lmdb --resolution 256 --max_images 100000\')\n    p.add_argument(     \'h5_filename\',      help=\'HDF5 file to create\')\n    p.add_argument(     \'lmdb_dir\',         help=\'Directory to read LMDB database from\')\n    p.add_argument(     \'--resolution\',     help=\'Output resolution (default: 256)\', type=int, default=256)\n    p.add_argument(     \'--max_images\',     help=\'Maximum number of images (default: none)\', type=int, default=None)\n\n    p = add_command(    \'create_celeba\',    \'Create HDF5 dataset for CelebA.\',\n                                            \'create_celeba celeba-128x128.h5 ~/celeba\')\n    p.add_argument(     \'h5_filename\',      help=\'HDF5 file to create\')\n    p.add_argument(     \'celeba_dir\',       help=\'Directory to read CelebA data from\')\n    p.add_argument(     \'--cx\',             help=\'Center X coordinate (default: 89)\', type=int, default=89)\n    p.add_argument(     \'--cy\',             help=\'Center Y coordinate (default: 121)\', type=int, default=121)\n\n    p = add_command(    \'create_celeba_hq\', \'Create HDF5 dataset for CelebA-HQ.\',\n                                            \'create_celeba_hq celeba-hq-1024x1024.h5 ~/celeba ~/celeba-hq-deltas\')\n    p.add_argument(     \'h5_filename\',      help=\'HDF5 file to create\')\n    p.add_argument(     \'celeba_dir\',       help=\'Directory to read CelebA data from\')\n    p.add_argument(     \'delta_dir\',        help=\'Directory to read CelebA-HQ deltas from\')\n    p.add_argument(     \'--num_threads\',    help=\'Number of concurrent threads (default: 4)\', type=int, default=4)\n    p.add_argument(     \'--num_tasks\',      help=\'Number of concurrent processing tasks (default: 100)\', type=int, default=100)\n\n    args = parser.parse_args(argv[1:])\n    func = globals()[args.command]\n    del args.command\n    func(**vars(args))\n\n#----------------------------------------------------------------------------\n\nif __name__ == ""__main__"":\n    execute_cmdline(sys.argv)\n\n#----------------------------------------------------------------------------'"
train.py,13,"b'# -*- coding: utf-8 -*-\nimport torch\nimport torch.optim as optim\nfrom torch.autograd import Variable\n# from torch.optim.lr_scheduler import StepLR\nimport sys, os, time\nsys.path.append(\'utils\')\nsys.path.append(\'models\')\nfrom utils.data import CelebA, RandomNoiseGenerator\nfrom models.model import Generator, Discriminator\nimport argparse\nimport numpy as np\nfrom scipy.misc import imsave\nfrom utils.logger import Logger\n\nclass PGGAN():\n    def __init__(self, G, D, data, noise, opts):\n        self.G = G\n        self.D = D\n        self.data = data\n        self.noise = noise\n        self.opts = opts\n        self.current_time = time.strftime(\'%Y-%m-%d %H%M%S\')\n        self.logger = Logger(\'./logs/\' + self.current_time + ""/"")\n        gpu = self.opts[\'gpu\']\n        self.use_cuda = len(gpu) > 0\n        os.environ[\'CUDA_VISIBLE_DEVICES\'] = gpu\n\n        self.bs_map = {2**R: self.get_bs(2**R) for R in range(2, 11)} # batch size map keyed by resolution_level\n        self.rows_map = {32: 8, 16: 4, 8: 4, 4: 2, 2: 2}\n\n        self.restore_model()\n\n        # save opts\n        with open(os.path.join(self.opts[\'exp_dir\'], self.time, \'options_%s.txt\'%self.current_time), \'w\') as f:\n            for k, v in self.opts.items():\n                print(\'%s: %s\' % (k, v), file=f)\n            print(\'batch_size_map: %s\' % self.bs_map, file=f)\n\n    def restore_model(self):\n        exp_dir = self.opts[\'restore_dir\']\n        which_file = self.opts[\'which_file\']  # 128x128-fade_in-105000\n        self.current_time = time.strftime(\'%Y-%m-%d %H%M%S\')\n        if exp_dir == \'\' or which_file == \'\':\n            self.time = self.current_time\n            self._from_resol = self.opts[\'first_resol\']\n            self._phase = \'stabilize\'\n            self._epoch = 0\n            self.is_restored = False\n            self.opts[\'sample_dir\'] = os.path.join(self.opts[\'exp_dir\'], self.current_time, \'samples\')\n            self.opts[\'ckpt_dir\'] = os.path.join(self.opts[\'exp_dir\'], self.current_time, \'ckpts\')\n            os.makedirs(self.opts[\'sample_dir\'])\n            os.makedirs(self.opts[\'ckpt_dir\'])\n            return \n        else:\n            pattern = which_file.split(\'-\')\n            self._from_resol = int(pattern[0].split(\'x\')[0])\n            self._phase = pattern[1]\n            self._epoch = int(pattern[2])\n            tmp = exp_dir.split(\'/\')\n            self.opts[\'exp_dir\'] = \'/\'.join(tmp[:-1])\n            self.time = tmp[-1]\n            self.opts[\'sample_dir\'] = os.path.join(exp_dir, \'samples\')\n            self.opts[\'ckpt_dir\'] = os.path.join(exp_dir, \'ckpts\')\n            assert os.path.exists(self.opts[\'sample_dir\']) and os.path.exists(self.opts[\'ckpt_dir\'])\n\n            G_model = os.path.join(self.opts[\'ckpt_dir\'], which_file+\'-G.pth\')\n            D_model = os.path.join(self.opts[\'ckpt_dir\'], which_file+\'-D.pth\')\n            assert os.path.exists(G_model) and os.path.exists(D_model)\n            self.G.load_state_dict(torch.load(G_model))\n            self.D.load_state_dict(torch.load(D_model))\n            self.is_restored = True\n            print(\'Restored from dir: %s, pattern: %s\' % (exp_dir, which_file))\n\n    def get_bs(self, resolution):\n        R = int(np.log2(resolution))\n        if R < 7:\n            bs = 32 / 2**(max(0, R-4))\n        else:\n            bs = 8 / 2**(min(2, R-7))\n        return int(bs)\n\n    def register_on_gpu(self):\n        if self.use_cuda:\n            self.G.cuda()\n            self.D.cuda()\n\n    def create_optimizer(self):\n        self.optim_G = optim.Adam(self.G.parameters(), lr=self.opts[\'g_lr_max\'], betas=(self.opts[\'beta1\'], self.opts[\'beta2\']))\n        self.optim_D = optim.Adam(self.D.parameters(), lr=self.opts[\'d_lr_max\'], betas=(self.opts[\'beta1\'], self.opts[\'beta2\']))\n        \n    def create_criterion(self):\n        # w is for gan\n        if self.opts[\'gan\'] == \'lsgan\':\n            self.adv_criterion = lambda p,t,w: torch.mean((p-t)**2)  # sigmoid is applied here\n        elif self.opts[\'gan\'] == \'wgan_gp\':\n            self.adv_criterion = lambda p,t,w: (-2*t+1) * torch.mean(p)\n        elif self.opts[\'gan\'] == \'gan\':\n            self.adv_criterion = lambda p,t,w: -w*(torch.mean(t*torch.log(p+1e-8)) + torch.mean((1-t)*torch.log(1-p+1e-8)))\n        else:\n            raise ValueError(\'Invalid/Unsupported GAN: %s.\' % self.opts[\'gan\'])\n\n    def compute_adv_loss(self, prediction, target, w):\n        return self.adv_criterion(prediction, target, w)\n\n    def compute_additional_g_loss(self):\n        return 0.0\n\n    def compute_additional_d_loss(self):  # drifting loss and gradient penalty, weighting inside this function\n        return 0.0\n\n    def _get_data(self, d):\n        return d.data[0] if isinstance(d, Variable) else d\n\n    def compute_G_loss(self):\n        g_adv_loss = self.compute_adv_loss(self.d_fake, True, 1)\n        g_add_loss = self.compute_additional_g_loss()\n        self.g_adv_loss = self._get_data(g_adv_loss)\n        self.g_add_loss = self._get_data(g_add_loss)\n        return g_adv_loss + g_add_loss\n\n    def compute_D_loss(self):\n        self.d_adv_loss_real = self.compute_adv_loss(self.d_real, True, 0.5)\n        self.d_adv_loss_fake = self.compute_adv_loss(self.d_fake, False, 0.5) * self.opts[\'fake_weight\']\n        d_adv_loss = self.d_adv_loss_real + self.d_adv_loss_fake\n        d_add_loss = self.compute_additional_d_loss()\n        self.d_adv_loss = self._get_data(d_adv_loss)\n        self.d_add_loss = self._get_data(d_add_loss)\n\n        return d_adv_loss + d_add_loss\n\n    def _rampup(self, epoch, rampup_length):\n        if epoch < rampup_length:\n            p = max(0.0, float(epoch)) / float(rampup_length)\n            p = 1.0 - p\n            return np.exp(-p*p*5.0)\n        else:\n            return 1.0\n\n    def _rampdown_linear(self, epoch, num_epochs, rampdown_length):\n        if epoch >= num_epochs - rampdown_length:\n            return float(num_epochs - epoch) / rampdown_length\n        else:\n            return 1.0\n\n    \'\'\'Update Learning rate\n    \'\'\'\n    def update_lr(self, cur_nimg):\n        for param_group in self.optim_G.param_groups:\n            lrate_coef = self._rampup(cur_nimg / 1000.0, self.opts[\'rampup_kimg\'])\n            lrate_coef *= self._rampdown_linear(cur_nimg / 1000.0, self.opts[\'total_kimg\'], self.opts[\'rampdown_kimg\'])\n            param_group[\'lr\'] = lrate_coef * self.opts[\'g_lr_max\']\n        for param_group in self.optim_D.param_groups:\n            lrate_coef = self._rampup(cur_nimg / 1000.0, self.opts[\'rampup_kimg\'])\n            lrate_coef *= self._rampdown_linear(cur_nimg / 1000.0, self.opts[\'total_kimg\'], self.opts[\'rampdown_kimg\'])\n            param_group[\'lr\'] = lrate_coef * self.opts[\'d_lr_max\']\n\n    def postprocess(self):\n        # TODO: weight cliping or others\n        pass\n\n    def _numpy2var(self, x):\n        var = Variable(torch.from_numpy(x))\n        if self.use_cuda:\n            var = var.cuda()\n        return var\n\n    def _var2numpy(self, var):\n        if self.use_cuda:\n            return var.cpu().data.numpy()\n        return var.data.numpy()\n\n    # def add_noise(self, x):\n    #     # TODO: support more method of adding noise.\n    #     if self.opts.get(\'no_noise\', False):\n    #         return x\n\n    #     if hasattr(self, \'_d_\'):\n    #         self._d_ = self._d_ * 0.9 + torch.mean(self.d_real).data[0] * 0.1\n    #     else:\n    #         self._d_ = 0.0\n    #     strength = 0.2 * max(0, self._d_ - 0.5)**2\n    #     noise = self._numpy2var(np.random.randn(*x.size()).astype(np.float32) * strength)\n    #     return x + noise\n\n    def compute_noise_strength(self):\n        if self.opts.get(\'no_noise\', False):\n            return 0\n\n        if hasattr(self, \'_d_\'):\n            self._d_ = self._d_ * 0.9 + np.clip(torch.mean(self.d_real).data[0], 0.0, 1.0) * 0.1\n        else:\n            self._d_ = 0.0\n        strength = 0.2 * max(0, self._d_ - 0.5)**2\n        return strength\n\n    def preprocess(self, z, real):\n        self.z = self._numpy2var(z)\n        self.real = self._numpy2var(real)\n\n    def forward_G(self, cur_level):\n        self.d_fake = self.D(self.fake, cur_level=cur_level)\n    \n    def forward_D(self, cur_level, detach=True):\n        self.fake = self.G(self.z, cur_level=cur_level)\n        strength = self.compute_noise_strength()\n        self.d_real = self.D(self.real, cur_level=cur_level, gdrop_strength=strength)\n        self.d_fake = self.D(self.fake.detach() if detach else self.fake, cur_level=cur_level)\n        # print(\'d_real\', self.d_real.view(-1))\n        # print(\'d_fake\', self.d_fake.view(-1))\n        # print(self.fake[0].view(-1))\n\n    def backward_G(self):\n        g_loss = self.compute_G_loss()\n        g_loss.backward()\n        self.optim_G.step()\n        self.g_loss = self._get_data(g_loss)\n\n    def backward_D(self, retain_graph=False):\n        d_loss = self.compute_D_loss()\n        d_loss.backward(retain_graph=retain_graph)\n        self.optim_D.step()\n        self.d_loss = self._get_data(d_loss)\n\n    def report(self, it, num_it, phase, resol):\n        formation = \'Iter[%d|%d], %s, %s, G: %.3f, D: %.3f, G_adv: %.3f, G_add: %.3f, D_adv: %.3f, D_add: %.3f\'\n        values = (it, num_it, phase, resol, self.g_loss, self.d_loss, self.g_adv_loss, self.g_add_loss, self.d_adv_loss, self.d_add_loss)\n        print(formation % values)\n\n    def tensorboard(self, it, num_it, phase, resol, samples):\n        # (1) Log the scalar values\n        prefix = str(resol)+\'/\'+phase+\'/\'\n        info = {prefix + \'G_loss\': self.g_loss,\n                prefix + \'G_adv_loss\': self.g_adv_loss,\n                prefix + \'G_add_loss\': self.g_add_loss,\n                prefix + \'D_loss\': self.d_loss,\n                prefix + \'D_adv_loss\': self.d_adv_loss,\n                prefix + \'D_add_loss\': self.d_add_loss,\n                prefix + \'D_adv_loss_fake\': self._get_data(self.d_adv_loss_fake),\n                prefix + \'D_adv_loss_real\': self._get_data(self.d_adv_loss_real)}\n\n        for tag, value in info.items():\n            self.logger.scalar_summary(tag, value, it)\n\n        # (2) Log values and gradients of the parameters (histogram)\n        for tag, value in self.G.named_parameters():\n            tag = tag.replace(\'.\', \'/\')\n            self.logger.histo_summary(\'G/\' + prefix +tag, self._var2numpy(value), it)\n            if value.grad is not None:\n                self.logger.histo_summary(\'G/\' + prefix +tag + \'/grad\', self._var2numpy(value.grad), it)\n\n        for tag, value in self.D.named_parameters():\n            tag = tag.replace(\'.\', \'/\')\n            self.logger.histo_summary(\'D/\' + prefix + tag, self._var2numpy(value), it)\n            if value.grad is not None:\n                self.logger.histo_summary(\'D/\' + prefix + tag + \'/grad\',\n                                          self._var2numpy(value.grad), it)\n\n        # (3) Log the images\n        # info = {\'images\': samples[:10]}\n        # for tag, images in info.items():\n        #     logger.image_summary(tag, images, it)\n\n    def train_phase(self, R, phase, batch_size, cur_nimg, from_it, total_it):\n        assert total_it >= from_it\n        resol = 2 ** (R+1)\n \n        for it in range(from_it, total_it):\n            if phase == \'stabilize\':\n                cur_level = R\n            else:\n                cur_level = R + total_it/float(from_it)\n            cur_resol = 2 ** int(np.ceil(cur_level+1))\n\n            # get a batch noise and real images\n            z = self.noise(batch_size)\n            x = self.data(batch_size, cur_resol, cur_level)\n\n            # ===preprocess===\n            self.preprocess(z, x)\n            self.update_lr(cur_nimg)\n\n            # ===update D===\n            self.optim_D.zero_grad()\n            self.forward_D(cur_level, detach=True)\n            self.backward_D()\n\n            # ===update G===\n            self.optim_G.zero_grad()\n            self.forward_G(cur_level)\n            self.backward_G()\n\n            # ===report ===\n            self.report(it, total_it, phase, cur_resol)\n\n            cur_nimg += batch_size\n\n            # ===generate sample images===\n            samples = []\n            if (it % self.opts[\'sample_freq\'] == 0) or it == total_it-1:\n                samples = self.sample()\n                imsave(os.path.join(self.opts[\'sample_dir\'],\n                                    \'%dx%d-%s-%s.png\' % (cur_resol, cur_resol, phase, str(it).zfill(6))), samples)\n\n            # ===tensorboard visualization===\n            if (it % self.opts[\'sample_freq\'] == 0) or it == total_it - 1:\n                self.tensorboard(it, total_it, phase, cur_resol, samples)\n\n            # ===save model===\n            if (it % self.opts[\'save_freq\'] == 0 and it > 0) or it == total_it-1:\n                self.save(os.path.join(self.opts[\'ckpt_dir\'], \'%dx%d-%s-%s\' % (cur_resol, cur_resol, phase, str(it).zfill(6))))\n        \n    def train(self):\n        # prepare\n        self.create_optimizer()\n        self.create_criterion()\n        self.register_on_gpu()\n\n        to_level = int(np.log2(self.opts[\'target_resol\']))\n        from_level = int(np.log2(self._from_resol))\n        assert 2**to_level == self.opts[\'target_resol\'] and 2**from_level == self._from_resol and to_level >= from_level >= 2\n\n        train_kimg = int(self.opts[\'train_kimg\'] * 1000)\n        transition_kimg = int(self.opts[\'transition_kimg\'] * 1000)\n\n        for R in range(from_level-1, to_level):\n            batch_size = self.bs_map[2 ** (R+1)]\n\n            phases = {\'stabilize\':[0, train_kimg//batch_size], \'fade_in\':[train_kimg//batch_size+1, (transition_kimg+train_kimg)//batch_size]}\n            if self.is_restored and R == from_level-1:\n                phases[self._phase][0] = self._epoch + 1\n                if self._phase == \'fade_in\':\n                    del phases[\'stabilize\']\n\n            for phase in [\'stabilize\', \'fade_in\']:\n                if phase in phases:\n                    _range = phases[phase]\n                    self.train_phase(R, phase, batch_size, _range[0]*batch_size, _range[0], _range[1])\n\n    def sample(self):\n        batch_size = self.z.size(0)\n        n_row = self.rows_map[batch_size]\n        n_col = int(np.ceil(batch_size / float(n_row)))\n        samples = []\n        i = j = 0\n        for row in range(n_row):\n            one_row = []\n            # fake\n            for col in range(n_col):\n                one_row.append(self.fake[i].cpu().data.numpy())\n                i += 1\n            # real\n            for col in range(n_col):\n                one_row.append(self.real[j].cpu().data.numpy())\n                j += 1\n            samples += [np.concatenate(one_row, axis=2)]\n        samples = np.concatenate(samples, axis=1).transpose([1, 2, 0])\n\n        half = samples.shape[1] // 2\n        samples[:, :half, :] = samples[:, :half, :] - np.min(samples[:, :half, :])\n        samples[:, :half, :] = samples[:, :half, :] / np.max(samples[:, :half, :])\n        samples[:, half:, :] = samples[:, half:, :] - np.min(samples[:, half:, :])\n        samples[:, half:, :] = samples[:, half:, :] / np.max(samples[:, half:, :])\n        return samples\n\n    def save(self, file_name):\n        g_file = file_name + \'-G.pth\'\n        d_file = file_name + \'-D.pth\'\n        torch.save(self.G.state_dict(), g_file)\n        torch.save(self.D.state_dict(), d_file)\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'--gpu\', default=\'\', type=str, help=\'gpu(s) to use.\')\n    parser.add_argument(\'--train_kimg\', default=600, type=float, help=\'# * 1000 real samples for each stabilizing training phase.\')\n    parser.add_argument(\'--transition_kimg\', default=600, type=float, help=\'# * 1000 real samples for each fading in phase.\')\n    parser.add_argument(\'--total_kimg\', default=10000, type=float, help=\'total_kimg: a param to compute lr.\')\n    parser.add_argument(\'--rampup_kimg\', default=10000, type=float, help=\'rampup_kimg.\')\n    parser.add_argument(\'--rampdown_kimg\', default=10000, type=float, help=\'rampdown_kimg.\')\n    parser.add_argument(\'--g_lr_max\', default=1e-3, type=float, help=\'Generator learning rate\')\n    parser.add_argument(\'--d_lr_max\', default=1e-3, type=float, help=\'Discriminator learning rate\')\n    parser.add_argument(\'--fake_weight\', default=0.1, type=float, help=""weight of fake images\' loss of D"")\n    parser.add_argument(\'--beta1\', default=0, type=float, help=\'beta1 for adam\')\n    parser.add_argument(\'--beta2\', default=0.99, type=float, help=\'beta2 for adam\')\n    parser.add_argument(\'--gan\', default=\'lsgan\', type=str, help=\'model: lsgan/wgan_gp/gan, currently only support lsgan or gan with no_noise option.\')\n    parser.add_argument(\'--first_resol\', default=4, type=int, help=\'first resolution\')\n    parser.add_argument(\'--target_resol\', default=256, type=int, help=\'target resolution\')\n    parser.add_argument(\'--drift\', default=1e-3, type=float, help=\'drift, only available for wgan_gp.\')\n    parser.add_argument(\'--mbstat_avg\', default=\'all\', type=str, help=\'MinibatchStatConcatLayer averaging strategy (Which dimensions to average the statistic over?)\')\n    parser.add_argument(\'--sample_freq\', default=500, type=int, help=\'sampling frequency.\')\n    parser.add_argument(\'--save_freq\', default=5000, type=int, help=\'save model frequency.\')\n    parser.add_argument(\'--exp_dir\', default=\'./exp\', type=str, help=\'experiment dir.\')\n    parser.add_argument(\'--no_noise\', action=\'store_true\', help=\'do not add noise to real data.\')\n    parser.add_argument(\'--no_tanh\', action=\'store_true\', help=\'do not use tanh in the last layer of the generator.\')\n    parser.add_argument(\'--restore_dir\', default=\'\', type=str, help=\'restore from which exp dir.\')\n    parser.add_argument(\'--which_file\', default=\'\', type=str, help=\'restore from which file, e.g. 128x128-fade_in-105000.\')\n\n    # TODO: support conditional inputs\n\n    args = parser.parse_args()\n    opts = {k:v for k,v in args._get_kwargs()}\n\n    # Dimensionality of the latent vector.\n    latent_size = 512\n    # Use sigmoid activation for the last layer?\n    sigmoid_at_end = args.gan in [\'lsgan\', \'gan\']\n    if hasattr(args, \'no_tanh\'):\n        tanh_at_end = False\n    else:\n        tanh_at_end = True\n\n    G = Generator(num_channels=3, latent_size=latent_size, resolution=args.target_resol, fmap_max=latent_size, fmap_base=8192, tanh_at_end=tanh_at_end)\n    D = Discriminator(num_channels=3, mbstat_avg=args.mbstat_avg, resolution=args.target_resol, fmap_max=latent_size, fmap_base=8192, sigmoid_at_end=sigmoid_at_end)\n    print(G)\n    print(D)\n    data = CelebA()\n    noise = RandomNoiseGenerator(latent_size, \'gaussian\')\n    pggan = PGGAN(G, D, data, noise, opts)\n    pggan.train()\n'"
train_no_tanh.py,9,"b'# -*- coding: utf-8 -*-\nimport torch\nimport torch.optim as optim\nfrom torch.autograd import Variable\nimport sys, os, time\nsys.path.append(\'utils\')\nsys.path.append(\'models\')\nfrom utils.data import CelebA, RandomNoiseGenerator\nfrom models.model import Generator, Discriminator\nimport argparse\nimport numpy as np\nfrom scipy.misc import imsave\nfrom utils.logger import Logger\n\nclass PGGAN():\n    def __init__(self, G, D, data, noise, opts):\n        self.G = G\n        self.D = D\n        self.data = data\n        self.noise = noise\n        self.opts = opts\n        self.current_time = time.strftime(\'%Y-%m-%d %H%M%S\')\n        self.logger = Logger(\'./logs/\' + self.current_time + ""/"")\n        gpu = self.opts[\'gpu\']\n        self.use_cuda = len(gpu) > 0\n        os.environ[\'CUDA_VISIBLE_DEVICES\'] = gpu\n\n        current_time = time.strftime(\'%Y-%m-%d %H%M%S\')\n        self.opts[\'sample_dir\'] = os.path.join(os.path.join(self.opts[\'exp_dir\'], current_time), \'samples\')\n        self.opts[\'ckpt_dir\'] = os.path.join(os.path.join(self.opts[\'exp_dir\'], current_time), \'ckpts\')\n        os.makedirs(self.opts[\'sample_dir\'])\n        os.makedirs(self.opts[\'ckpt_dir\'])\n\n        self.bs_map = {2**R: self.get_bs(2**R) for R in range(2, 11)}\n        self.rows_map = {32: 8, 16: 4, 8: 4, 4: 2, 2: 2}\n\n        # save opts\n        with open(os.path.join(os.path.join(self.opts[\'exp_dir\'], current_time), \'options.txt\'), \'w\') as f:\n            for k, v in self.opts.items():\n                print(\'%s: %s\' % (k, v), file=f)\n            print(\'batch_size_map: %s\' % self.bs_map, file=f)\n\n    def get_bs(self, resolution):\n        R = int(np.log2(resolution))\n        if R < 7:\n            bs = 32 / 2**(max(0, R-4))\n        else:\n            bs = 8 / 2**(min(2, R-7))\n        return int(bs)\n\n    def register_on_gpu(self):\n        if self.use_cuda:\n            self.G.cuda()\n            self.D.cuda()\n\n    def create_optimizer(self):\n        self.optim_G = optim.Adam(self.G.parameters(), lr=self.opts[\'g_lr_max\'], betas=(self.opts[\'beta1\'], self.opts[\'beta2\']))\n        self.optim_D = optim.Adam(self.D.parameters(), lr=self.opts[\'d_lr_max\'], betas=(self.opts[\'beta1\'], self.opts[\'beta2\']))\n\n    def create_criterion(self):\n        # w is for gan\n        if self.opts[\'gan\'] == \'lsgan\':\n            self.adv_criterion = lambda p,t,w: torch.mean((p-t)**2)  # sigmoid is applied here\n        elif self.opts[\'gan\'] == \'wgan_gp\':\n            self.adv_criterion = lambda p,t,w: (-2*t+1) * torch.mean(p)\n        elif self.opts[\'gan\'] == \'gan\':\n            lambda p,t,w: -w*(torch.mean(t*torch.log(p+1e-8)) + torch.mean((1-t)*torch.log(1-p+1e-8)))\n        else:\n            raise ValueError(\'Invalid/Unsupported GAN: %s.\' % self.opts[\'gan\'])\n\n    def compute_adv_loss(self, prediction, target, w):\n        return self.adv_criterion(prediction, target, w)\n\n    def compute_additional_g_loss(self):\n        return 0.0\n\n    def compute_additional_d_loss(self):  # drifting loss and gradient penalty, weighting inside this function\n        return 0.0\n\n    def _get_data(self, d):\n        return d.data[0] if isinstance(d, Variable) else d\n\n    def compute_G_loss(self):\n        g_adv_loss = self.compute_adv_loss(self.d_fake, True, 1)\n        g_add_loss = self.compute_additional_g_loss()\n        self.g_adv_loss = self._get_data(g_adv_loss)\n        self.g_add_loss = self._get_data(g_add_loss)\n        return g_adv_loss + g_add_loss\n\n    def compute_D_loss(self):\n        self.d_adv_loss_real = self.compute_adv_loss(self.d_real, True, 0.5)\n        self.d_adv_loss_fake = self.compute_adv_loss(self.d_fake, False, 0.5) * self.opts[\'fake_weight\']\n        d_adv_loss = self.d_adv_loss_real + self.d_adv_loss_fake\n        d_add_loss = self.compute_additional_d_loss()\n        self.d_adv_loss = self._get_data(d_adv_loss)\n        self.d_add_loss = self._get_data(d_add_loss)\n\n        return d_adv_loss + d_add_loss\n\n    def postprocess(self):\n        # TODO: weight cliping or others\n        pass\n\n    def _numpy2var(self, x):\n        var = Variable(torch.from_numpy(x))\n        if self.use_cuda:\n            var = var.cuda()\n        return var\n\n    def _var2numpy(self, var):\n        if self.use_cuda:\n            return var.cpu().data.numpy()\n        return var.data.numpy()\n\n    def add_noise(self, x):\n        # TODO: support more method of adding noise.\n        if self.opts.get(\'no_noise\', False):\n            return x\n\n        if hasattr(self, \'_d_\'):\n            self._d_ = self._d_ * 0.9 + torch.mean(self.d_real).data[0] * 0.1\n        else:\n            self._d_ = 0.0\n        strength = 0.2 * max(0, self._d_ - 0.5)**2\n        noise = self._numpy2var(np.random.randn(*x.size()).astype(np.float32) * strength)\n        return x + noise\n\n    def preprocess(self, z, real):\n        self.z = self._numpy2var(z)\n        self.real = self._numpy2var(real)\n\n    def forward_G(self, cur_level):\n        self.d_fake = self.D(self.fake, cur_level=cur_level)\n    \n    def forward_D(self, cur_level, detach=True):\n        self.fake = self.G(self.z, cur_level=cur_level)\n        self.d_real = self.D(self.add_noise(self.real), cur_level=cur_level)\n        self.d_fake = self.D(self.fake.detach() if detach else self.fake, cur_level=cur_level)\n        # print(\'d_real\', self.d_real.view(-1))\n        # print(\'d_fake\', self.d_fake.view(-1))\n        # print(self.fake[0].view(-1))\n\n    def backward_G(self):\n        g_loss = self.compute_G_loss()\n        g_loss.backward()\n        self.optim_G.step()\n        self.g_loss = self._get_data(g_loss)\n\n    def backward_D(self, retain_graph=False):\n        d_loss = self.compute_D_loss()\n        d_loss.backward(retain_graph=retain_graph)\n        self.optim_D.step()\n        self.d_loss = self._get_data(d_loss)\n\n    def report(self, it, num_it, phase, resol):\n        formation = \'Iter[%d|%d], %s, %s, G: %.3f, D: %.3f, G_adv: %.3f, G_add: %.3f, D_adv: %.3f, D_add: %.3f\'\n        values = (it, num_it, phase, resol, self.g_loss, self.d_loss, self.g_adv_loss, self.g_add_loss, self.d_adv_loss, self.d_add_loss)\n        print(formation % values)\n\n    def tensorboard(self, it, num_it, phase, resol, samples):\n        # (1) Log the scalar values\n        prefix = str(resol)+\'/\'+phase+\'/\'\n        info = {prefix + \'G_loss\': self.g_loss,\n                prefix + \'G_adv_loss\': self.g_adv_loss,\n                prefix + \'G_add_loss\': self.g_add_loss,\n                prefix + \'D_loss\': self.d_loss,\n                prefix + \'D_adv_loss\': self.d_adv_loss,\n                prefix + \'D_add_loss\': self.d_add_loss,\n                prefix + \'D_adv_loss_fake\': self._get_data(self.d_adv_loss_fake),\n                prefix + \'D_adv_loss_real\': self._get_data(self.d_adv_loss_real)}\n\n        for tag, value in info.items():\n            self.logger.scalar_summary(tag, value, it)\n\n        # (2) Log values and gradients of the parameters (histogram)\n        for tag, value in self.G.named_parameters():\n            tag = tag.replace(\'.\', \'/\')\n            self.logger.histo_summary(\'G/\' + prefix +tag, self._var2numpy(value), it)\n            if value.grad is not None:\n                self.logger.histo_summary(\'G/\' + prefix +tag + \'/grad\', self._var2numpy(value.grad), it)\n\n        for tag, value in self.D.named_parameters():\n            tag = tag.replace(\'.\', \'/\')\n            self.logger.histo_summary(\'D/\' + prefix + tag, self._var2numpy(value), it)\n            if value.grad is not None:\n                self.logger.histo_summary(\'D/\' + prefix + tag + \'/grad\',\n                                          self._var2numpy(value.grad), it)\n\n        # (3) Log the images\n        # info = {\'images\': samples[:10]}\n        # for tag, images in info.items():\n        #     logger.image_summary(tag, images, it)\n\n    def train(self):\n        # prepare\n        self.create_optimizer()\n        self.create_criterion()\n        self.registe_on_gpu()\n\n        to_level = int(np.log2(self.opts[\'target_resol\']))\n        from_level = int(np.log2(self.opts[\'first_resol\']))\n        assert 2**to_level == self.opts[\'target_resol\'] and 2**from_level == self.opts[\'first_resol\'] and to_level >= from_level >= 2\n        cur_level = from_level\n\n        for R in range(from_level-1, to_level-1):\n            batch_size = self.bs_map[2 ** (R+1)]\n            train_kimg = int(self.opts[\'train_kimg\'] * 1000)\n            transition_kimg = int(self.opts[\'transition_kimg\'] * 1000)\n            if R == to_level-1:\n                transition_kimg = 0\n            cur_nimg = 0\n            _len = len(str(train_kimg + transition_kimg))\n            _num_it = (train_kimg + transition_kimg) // batch_size\n            for it in range(_num_it):\n                # determined current level: int for stabilizing and float for fading in\n                cur_level = R + float(max(cur_nimg-train_kimg, 0)) / transition_kimg \n                cur_resol = 2 ** int(np.ceil(cur_level+1))\n                phase = \'stabilize\' if int(cur_level) == cur_level else \'fade_in\'\n\n                # get a batch noise and real images\n                z = self.noise(batch_size)\n                x = self.data(batch_size, cur_resol, cur_level)\n\n                # preprocess\n                self.preprocess(z, x)\n\n                # update D\n                self.optim_D.zero_grad()\n                self.forward_D(cur_level, detach=True)  # TODO: feed gdrop_strength\n                self.backward_D()\n\n                # update G\n                self.optim_G.zero_grad()\n                self.forward_G(cur_level)\n                self.backward_G()\n\n                # report \n                self.report(it, _num_it, phase, cur_resol)\n                \n                cur_nimg += batch_size\n\n                # sampling\n                samples = []\n                if (it % self.opts[\'sample_freq\'] == 0) or it == _num_it-1:\n                    samples = self.sample()\n                    imsave(os.path.join(self.opts[\'sample_dir\'],\n                                        \'%dx%d-%s-%s.png\' % (cur_resol, cur_resol, phase, str(it).zfill(6))), samples)\n\n                # ===tensorboard visualization===\n                if (it % self.opts[\'sample_freq\'] == 0) or it == _num_it - 1:\n                    self.tensorboard(it, _num_it, phase, cur_resol, samples)\n\n                # save model\n                if (it % self.opts[\'save_freq\'] == 0 and it > 0) or it == _num_it-1:\n                    self.save(os.path.join(self.opts[\'ckpt_dir\'], \'%dx%d-%s-%s\' % (cur_resol, cur_resol, phase, str(it).zfill(6))))\n\n    def sample(self, file_name):\n        batch_size = self.z.size(0)\n        n_row = self.rows_map[batch_size]\n        n_col = int(np.ceil(batch_size / float(n_row)))\n        samples = []\n        i = j = 0\n        for row in range(n_row):\n            one_row = []\n            # fake\n            for col in range(n_col):\n                one_row.append(self.fake[i].cpu().data.numpy())\n                i += 1\n            # real\n            for col in range(n_col):\n                one_row.append(self.real[j].cpu().data.numpy())\n                j += 1\n            samples += [np.concatenate(one_row, axis=2)]\n        samples = np.concatenate(samples, axis=1).transpose([1, 2, 0])\n\n        half = samples.shape[1] // 2\n        samples[:,:half,:] = samples[:,:half,:] - np.min(samples[:,:half,:])\n        samples[:,:half,:] = samples[:,:half,:] / np.max(samples[:,:half,:])\n        samples[:,half:,:] = samples[:,half:,:] - np.min(samples[:,half:,:])\n        samples[:,half:,:] = samples[:,half:,:] / np.max(samples[:,half:,:])\n        return samples\n\n    def save(self, file_name):\n        g_file = file_name + \'-G.pth\'\n        d_file = file_name + \'-D.pth\'\n        torch.save(self.G.state_dict(), g_file)\n        torch.save(self.D.state_dict(), d_file)\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'--gpu\', default=\'\', type=str, help=\'gpu(s) to use.\')\n    parser.add_argument(\'--train_kimg\', default=600, type=float, help=\'# * 1000 real samples for each stabilizing training phase.\')\n    parser.add_argument(\'--transition_kimg\', default=600, type=float, help=\'# * 1000 real samples for each fading in phase.\')\n    parser.add_argument(\'--g_lr_max\', default=1e-3, type=float, help=\'Generator learning rate\')\n    parser.add_argument(\'--d_lr_max\', default=1e-3, type=float, help=\'Discriminator learning rate\')\n    parser.add_argument(\'--beta1\', default=0, type=float, help=\'beta1 for adam\')\n    parser.add_argument(\'--beta2\', default=0.99, type=float, help=\'beta2 for adam\')\n    parser.add_argument(\'--gan\', default=\'lsgan\', type=str, help=\'model: lsgan/wgan_gp/gan, currently only support lsgan or gan with no_noise option.\')\n    parser.add_argument(\'--first_resol\', default=4, type=int, help=\'first resolution\')\n    parser.add_argument(\'--target_resol\', default=256, type=int, help=\'target resolution\')\n    parser.add_argument(\'--drift\', default=1e-3, type=float, help=\'drift, only available for wgan_gp.\')\n    parser.add_argument(\'--sample_freq\', default=500, type=int, help=\'sampling frequency.\')\n    parser.add_argument(\'--save_freq\', default=5000, type=int, help=\'save model frequency.\')\n    parser.add_argument(\'--exp_dir\', default=\'./exp\', type=str, help=\'experiment dir.\')\n    parser.add_argument(\'--no_noise\', action=\'store_true\', help=\'do not add noise to real data.\')\n\n    # TODO: support conditional inputs\n\n    args = parser.parse_args()\n    opts = {k:v for k,v in args._get_kwargs()}\n\n    latent_size = 512\n    sigmoid_at_end = args.gan in [\'lsgan\', \'gan\']\n\n    G = Generator(num_channels=3, latent_size=latent_size, resolution=args.target_resol, fmap_max=latent_size, fmap_base=8192, tanh_at_end=False)\n    D = Discriminator(num_channels=3, resolution=args.target_resol, fmap_max=latent_size, fmap_base=8192, sigmoid_at_end=sigmoid_at_end)\n    print(G)\n    print(D)\n    data = CelebA()\n    noise = RandomNoiseGenerator(latent_size, \'gaussian\')\n    pggan = PGGAN(G, D, data, noise, opts)\n    pggan.train()\n'"
models/base_model.py,20,"b'# -*- coding: utf-8 -*-\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\nfrom torch.nn.parameter import Parameter\nfrom torch.nn import functional as F\nfrom torch.nn.init import kaiming_normal, calculate_gain\nimport numpy as np\nimport sys\nif sys.version_info.major == 3:\n    from functools import reduce\n\nDEBUG = False\n\n\nclass PixelNormLayer(nn.Module):\n    """"""\n    Pixelwise feature vector normalization.\n    """"""\n    def __init__(self, eps=1e-8):\n        super(PixelNormLayer, self).__init__()\n        self.eps = eps\n    \n    def forward(self, x):\n        return x / torch.sqrt(torch.mean(x ** 2, dim=1, keepdim=True) + 1e-8)\n\n    def __repr__(self):\n        return self.__class__.__name__ + \'(eps = %s)\' % (self.eps)\n\n\nclass WScaleLayer(nn.Module):\n    """"""\n    Applies equalized learning rate to the preceding layer.\n    """"""\n    def __init__(self, incoming):\n        super(WScaleLayer, self).__init__()\n        self.incoming = incoming\n        self.scale = (torch.mean(self.incoming.weight.data ** 2)) ** 0.5\n        self.incoming.weight.data.copy_(self.incoming.weight.data / self.scale)\n        self.bias = None\n        if self.incoming.bias is not None:\n            self.bias = self.incoming.bias\n            self.incoming.bias = None\n\n    def forward(self, x):\n        x = self.scale * x\n        if self.bias is not None:\n            x += self.bias.view(1, self.bias.size()[0], 1, 1)\n        return x\n\n    def __repr__(self):\n        param_str = \'(incoming = %s)\' % (self.incoming.__class__.__name__)\n        return self.__class__.__name__ + param_str\n\n\ndef mean(tensor, axis, **kwargs):\n    if isinstance(axis, int):\n        axis = [axis]\n    for ax in axis:\n        tensor = torch.mean(tensor, axis=ax, **kwargs)\n    return tensor\n\n\nclass MinibatchStatConcatLayer(nn.Module):\n    """"""Minibatch stat concatenation layer.\n    - averaging tells how much averaging to use (\'all\', \'spatial\', \'none\')\n    """"""\n    def __init__(self, averaging=\'all\'):\n        super(MinibatchStatConcatLayer, self).__init__()\n        self.averaging = averaging.lower()\n        if \'group\' in self.averaging:\n            self.n = int(self.averaging[5:])\n        else:\n            assert self.averaging in [\'all\', \'flat\', \'spatial\', \'none\', \'gpool\'], \'Invalid averaging mode\'%self.averaging\n        self.adjusted_std = lambda x, **kwargs: torch.sqrt(torch.mean((x - torch.mean(x, **kwargs)) ** 2, **kwargs) + 1e-8) #Tstdeps in the original implementation\n\n    def forward(self, x):\n        shape = list(x.size())\n        target_shape = shape.copy()\n        vals = self.adjusted_std(x, dim=0, keepdim=True)# per activation, over minibatch dim\n        if self.averaging == \'all\':  # average everything --> 1 value per minibatch\n            target_shape[1] = 1\n            vals = torch.mean(vals, dim=1, keepdim=True)#vals = torch.mean(vals, keepdim=True)\n\n        elif self.averaging == \'spatial\':  # average spatial locations\n            if len(shape) == 4:\n                vals = mean(vals, axis=[2,3], keepdim=True)  # torch.mean(torch.mean(vals, 2, keepdim=True), 3, keepdim=True)\n        elif self.averaging == \'none\':  # no averaging, pass on all information\n            target_shape = [target_shape[0]] + [s for s in target_shape[1:]]\n        elif self.averaging == \'gpool\':  # EXPERIMENTAL: compute variance (func) over minibatch AND spatial locations.\n            if len(shape) == 4:\n                vals = mean(x, [0,2,3], keepdim=True)  # torch.mean(torch.mean(torch.mean(x, 2, keepdim=True), 3, keepdim=True), 0, keepdim=True)\n        elif self.averaging == \'flat\':  # variance of ALL activations --> 1 value per minibatch\n            target_shape[1] = 1\n            vals = torch.FloatTensor([self.adjusted_std(x)])\n        else:  # self.averaging == \'group\'  # average everything over n groups of feature maps --> n values per minibatch\n            target_shape[1] = self.n\n            vals = vals.view(self.n, self.shape[1]/self.n, self.shape[2], self.shape[3])\n            vals = mean(vals, axis=0, keepdim=True).view(1, self.n, 1, 1)\n        vals = vals.expand(*target_shape)\n        return torch.cat([x, vals], 1) # feature-map concatanation\n\n    def __repr__(self):\n        return self.__class__.__name__ + \'(averaging = %s)\' % (self.averaging)\n\n\nclass MinibatchDiscriminationLayer(nn.Module):\n    def __init__(self, num_kernels):\n        super(MinibatchDiscriminationLayer, self).__init__()\n        self.num_kernels = num_kernels\n\n    def forward(self, x):\n        pass\n\n\nclass GDropLayer(nn.Module):\n    """"""\n    # Generalized dropout layer. Supports arbitrary subsets of axes and different\n    # modes. Mainly used to inject multiplicative Gaussian noise in the network.\n    """"""\n    def __init__(self, mode=\'mul\', strength=0.2, axes=(0,1), normalize=False):\n        super(GDropLayer, self).__init__()\n        self.mode = mode.lower()\n        assert self.mode in [\'mul\', \'drop\', \'prop\'], \'Invalid GDropLayer mode\'%mode\n        self.strength = strength\n        self.axes = [axes] if isinstance(axes, int) else list(axes)\n        self.normalize = normalize\n        self.gain = None\n\n    def forward(self, x, deterministic=False):\n        if deterministic or not self.strength:\n            return x\n\n        rnd_shape = [s if axis in self.axes else 1 for axis, s in enumerate(x.size())]  # [x.size(axis) for axis in self.axes]\n        if self.mode == \'drop\':\n            p = 1 - self.strength\n            rnd = np.random.binomial(1, p=p, size=rnd_shape) / p\n        elif self.mode == \'mul\':\n            rnd = (1 + self.strength) ** np.random.normal(size=rnd_shape)\n        else:\n            coef = self.strength * x.size(1) ** 0.5\n            rnd = np.random.normal(size=rnd_shape) * coef + 1\n\n        if self.normalize:\n            rnd = rnd / np.linalg.norm(rnd, keepdims=True)\n        rnd = Variable(torch.from_numpy(rnd).type(x.data.type()))\n        if x.is_cuda:\n            rnd = rnd.cuda()\n        return x * rnd\n\n    def __repr__(self):\n        param_str = \'(mode = %s, strength = %s, axes = %s, normalize = %s)\' % (self.mode, self.strength, self.axes, self.normalize)\n        return self.__class__.__name__ + param_str\n\n\nclass LayerNormLayer(nn.Module):\n    """"""\n    Layer normalization. Custom reimplementation based on the paper: https://arxiv.org/abs/1607.06450\n    """"""\n    def __init__(self, incoming, eps=1e-4):\n        super(LayerNormLayer, self).__init__()\n        self.incoming = incoming\n        self.eps = eps\n        self.gain = Parameter(torch.FloatTensor([1.0]), requires_grad=True)\n        self.bias = None\n\n        if self.incoming.bias is not None:\n            self.bias = self.incoming.bias\n            self.incoming.bias = None\n\n    def forward(self, x):\n        x = x - mean(x, axis=range(1, len(x.size())))\n        x = x * 1.0/(torch.sqrt(mean(x**2, axis=range(1, len(x.size())), keepdim=True) + self.eps))\n        x = x * self.gain\n        if self.bias is not None:\n            x += self.bias\n        return x\n\n    def __repr__(self):\n        param_str = \'(incoming = %s, eps = %s)\' % (self.incoming.__class__.__name__, self.eps)\n        return self.__class__.__name__ + param_str\n\n\ndef resize_activations(v, so):\n    """"""\n    Resize activation tensor \'v\' of shape \'si\' to match shape \'so\'.\n    :param v:\n    :param so:\n    :return:\n    """"""\n    si = list(v.size())\n    so = list(so)\n    assert len(si) == len(so) and si[0] == so[0]\n\n    # Decrease feature maps.\n    if si[1] > so[1]:\n        v = v[:, :so[1]]\n\n    # Shrink spatial axes.\n    if len(si) == 4 and (si[2] > so[2] or si[3] > so[3]):\n        assert si[2] % so[2] == 0 and si[3] % so[3] == 0\n        ks = (si[2] // so[2], si[3] // so[3])\n        v = F.avg_pool2d(v, kernel_size=ks, stride=ks, ceil_mode=False, padding=0, count_include_pad=False)\n\n    # Extend spatial axes. Below is a wrong implementation\n    # shape = [1, 1]\n    # for i in range(2, len(si)):\n    #     if si[i] < so[i]:\n    #         assert so[i] % si[i] == 0\n    #         shape += [so[i] // si[i]]\n    #     else:\n    #         shape += [1]\n    # v = v.repeat(*shape)\n    if si[2] < so[2]: \n        assert so[2] % si[2] == 0 and so[2] / si[2] == so[3] / si[3]  # currently only support this case\n        v = F.upsample(v, scale_factor=so[2]//si[2], mode=\'nearest\')\n\n    # Increase feature maps.\n    if si[1] < so[1]:\n        z = torch.zeros((v.shape[0], so[1] - si[1]) + so[2:])\n        v = torch.cat([v, z], 1)\n    return v\n\n\nclass GSelectLayer(nn.Module):\n    def __init__(self, pre, chain, post):\n        super(GSelectLayer, self).__init__()\n        assert len(chain) == len(post)\n        self.pre = pre\n        self.chain = chain\n        self.post = post\n        self.N = len(self.chain)\n\n    def forward(self, x, y=None, cur_level=None, insert_y_at=None):\n        if cur_level is None:\n            cur_level = self.N  # cur_level: physical index\n        if y is not None:\n            assert insert_y_at is not None\n\n        min_level, max_level = int(np.floor(cur_level-1)), int(np.ceil(cur_level-1))\n        min_level_weight, max_level_weight = int(cur_level+1)-cur_level, cur_level-int(cur_level)\n        \n        _from, _to, _step = 0, max_level+1, 1\n\n        if self.pre is not None:\n            x = self.pre(x)\n\n        out = {}\n        if DEBUG:\n            print(\'G: level=%s, size=%s\' % (\'in\', x.size()))\n        for level in range(_from, _to, _step):\n            if level == insert_y_at:\n                x = self.chain[level](x, y)\n            else:\n                x = self.chain[level](x)\n\n            if DEBUG:\n                print(\'G: level=%d, size=%s\' % (level, x.size()))\n\n            if level == min_level:\n                out[\'min_level\'] = self.post[level](x)\n            if level == max_level:\n                out[\'max_level\'] = self.post[level](x)\n                x = resize_activations(out[\'min_level\'], out[\'max_level\'].size()) * min_level_weight + \\\n                        out[\'max_level\'] * max_level_weight\n        if DEBUG:\n            print(\'G:\', x.size())\n        return x\n\n\nclass DSelectLayer(nn.Module):\n    def __init__(self, pre, chain, inputs):\n        super(DSelectLayer, self).__init__()\n        assert len(chain) == len(inputs)\n        self.pre = pre\n        self.chain = chain\n        self.inputs = inputs\n        self.N = len(self.chain)\n\n    def forward(self, x, y=None, cur_level=None, insert_y_at=None):\n        if cur_level is None:\n            cur_level = self.N  # cur_level: physical index\n        if y is not None:\n            assert insert_y_at is not None\n\n        max_level, min_level = int(np.floor(self.N-cur_level)), int(np.ceil(self.N-cur_level))\n        min_level_weight, max_level_weight = int(cur_level+1)-cur_level, cur_level-int(cur_level)\n        \n        _from, _to, _step = min_level+1, self.N, 1\n\n        if self.pre is not None:\n            x = self.pre(x)\n\n        if DEBUG:\n            print(\'D: level=%s, size=%s, max_level=%s, min_level=%s\' % (\'in\', x.size(), max_level, min_level))\n\n        if max_level == min_level:\n            x = self.inputs[max_level](x)\n            if max_level == insert_y_at:\n                x = self.chain[max_level](x, y)\n            else:\n                x = self.chain[max_level](x)\n        else:\n            out = {}\n            tmp = self.inputs[max_level](x)\n            if max_level == insert_y_at:\n                tmp = self.chain[max_level](tmp, y)\n            else:\n                tmp = self.chain[max_level](tmp)\n            out[\'max_level\'] = tmp\n            out[\'min_level\'] = self.inputs[min_level](x)\n            x = resize_activations(out[\'min_level\'], out[\'max_level\'].size()) * min_level_weight + \\\n                                out[\'max_level\'] * max_level_weight\n            if min_level == insert_y_at:\n                x = self.chain[min_level](x, y)\n            else:\n                x = self.chain[min_level](x)\n\n        for level in range(_from, _to, _step):\n            if level == insert_y_at:\n                x = self.chain[level](x, y)\n            else:\n                x = self.chain[level](x)\n\n            if DEBUG:\n                print(\'D: level=%d, size=%s\' % (level, x.size()))\n        return x\n\n\nclass AEDSelectLayer(nn.Module):\n    def __init__(self, pre, chain, nins):\n        super(AEDSelectLayer, self).__init__()\n        assert len(chain) == len(nins)\n        self.pre = pre\n        self.chain = chain\n        self.nins = nins\n        self.N = len(self.chain) // 2 \n\n    def forward(self, x, cur_level=None):\n        if cur_level is None:\n            cur_level = self.N  # cur_level: physical index\n\n        max_level, min_level = int(np.floor(self.N-cur_level)), int(np.ceil(self.N-cur_level))\n        min_level_weight, max_level_weight = int(cur_level+1)-cur_level, cur_level-int(cur_level)\n        \n        _from, _to, _step = min_level, self.N, 1\n\n        if self.pre is not None:\n            x = self.pre(x)\n\n        if DEBUG:\n            print(\'D: level=%s, size=%s, max_level=%s, min_level=%s\' % (\'in\', x.size(), max_level, min_level))\n\n        # encoder\n        if max_level == min_level:\n            in_max_level = 0\n        else:\n            in_max_level = self.chain[max_level](self.nins[max_level](x))\n            if DEBUG:\n                print(\'D: level=%s(max_level), size=%s, encoder\' % (max_level, in_max_level.size()))\n        \n        for level in range(_from, _to, _step):\n            if level == min_level:\n                in_min_level = self.nins[level](x)\n                target_shape = in_max_level.size() if max_level != min_level else in_min_level.size()\n                x = min_level_weight * resize_activations(in_min_level, target_shape) + max_level_weight * in_max_level\n            x = self.chain[level](x)\n\n            if DEBUG:\n                print(\'D: level=%s, size=%s, encoder\' % (level, x.size()))\n\n        # decoder\n        from_, to_, step_ = self.N, 2*self.N-min_level, 1\n        for level in range(from_, to_, step_):\n            x = self.chain[level](x)\n            if level == 2*self.N-min_level-1:  # min output level\n                out_min_level = self.nins[level](x)\n\n            if DEBUG:\n                print(\'D: level=%s, size=%s, decoder\' % (level, x.size()))\n\n        if max_level == min_level:\n            out_max_level = 0\n        else:\n            out_max_level = self.nins[2*self.N-max_level-1](self.chain[2*self.N-max_level-1](x))\n\n        target_shape = out_max_level.size() if max_level != min_level else out_min_level.size()\n        x = min_level_weight * resize_activations(out_min_level, target_shape) + max_level_weight * out_max_level\n\n        if DEBUG:\n            print(\'D: level=%s, size=%s\' % (\'out\', x.size()))\n        return x\n\n        # if max_level == min_level:\n        #     x = self.nins[max_level](x)\n        #     if not min_level+1 == self.N-1:\n        #         x = self.chain[max_level](x)\n        #     if DEBUG:\n        #         print(\'D: level=%d, size=%s\' % (min_level, x.size()))\n        # else:\n        #     out = {}\n        #     tmp = self.nins[max_level](x)\n        #     tmp = self.chain[max_level](tmp)\n        #     out[\'max_level\'] = tmp\n        #     if DEBUG:\n        #         print(\'D: level=%d, size=%s\' % (max_level, tmp.size()))\n        #     out[\'min_level\'] = self.nins[min_level](x)\n        #     x = resize_activations(out[\'min_level\'], out[\'max_level\'].size()) * min_level_weight + \\\n        #                         out[\'max_level\'] * max_level_weight\n        #     if not min_level == self.N-1:\n        #         x = self.chain[min_level](x)\n        #     if DEBUG:\n        #         print(\'D: level=%d, size=%s\' % (min_level, x.size()))\n\n        # for level in range(_from, _to, _step):\n        #     x = self.chain[level](x)\n\n        #     if DEBUG:\n        #         print(\'D: level=%d, size=%s, encoder\' % (level, x.size()))\n\n        # for level in range(_to, _to-_from+_to, _step):\n        #     x = self.chain[level](x)\n\n        #     if DEBUG:\n        #         print(\'D: level=%d, size=%s, decoder\' % (level, x.size()))\n        \n        # if min_level == max_level:\n        #     if not min_level+1 == self.N-1:\n        #         x = self.chain[_to-_from+_to](x)\n        #     x = self.nins[_to-_from+_to+1](x)\n        # else:\n        #     out = {}\n        #     if not min_level+1 == self.N-1:\n        #         tmp = self.chain[_to-_from+_to-1](x)\n        #     else:\n        #         tmp = x\n        #     out[\'min_level\'] = self.nins[_to-_from+_to](tmp)\n        #     if DEBUG:\n        #         print(\'D: level=%d, size=%s, min_level\' % (_to-_from+_to, out[\'min_level\'].size()))\n        #     x = self.chain[_to-_from+_to](x)\n        #     out[\'max_level\'] = self.nins[_to-_from+_to+1](x)\n        #     x = resize_activations(out[\'min_level\'], out[\'max_level\'].size()) * min_level_weight + \\\n        #                 out[\'max_level\'] * max_level_weight\n\n        # if DEBUG:\n        #     print(\'D: size=%s\' % (x.size(),))\n        # return x\n\n\nclass ConcatLayer(nn.Module):\n    def __init__(self):\n        super(ConcatLayer, self).__init__()\n\n    def forward(self, x, y):\n        return torch.cat([x, y], 1)\n\n\nclass ReshapeLayer(nn.Module):\n    def __init__(self, new_shape):\n        super(ReshapeLayer, self).__init__()\n        self.new_shape = new_shape  # not include minibatch dimension\n\n    def forward(self, x):\n        assert reduce(lambda u,v: u*v, self.new_shape) == reduce(lambda u,v: u*v, x.size()[1:])\n        return x.view(-1, *self.new_shape)\n\n\ndef he_init(layer, nonlinearity=\'conv2d\', param=None):\n    nonlinearity = nonlinearity.lower()\n    if nonlinearity not in [\'linear\', \'conv1d\', \'conv2d\', \'conv3d\', \'relu\', \'leaky_relu\', \'sigmoid\', \'tanh\']:\n        if not hasattr(layer, \'gain\') or layer.gain is None:\n            gain = 0  # default\n        else:\n            gain = layer.gain\n    elif nonlinearity == \'leaky_relu\':\n        assert param is not None, \'Negative_slope(param) should be given.\'\n        gain = calculate_gain(nonlinearity, param)\n    else:\n        gain = calculate_gain(nonlinearity)\n    kaiming_normal(layer.weight, a=gain)\n\n'"
models/model.py,0,"b""# -*- coding: utf-8 -*-\r\nfrom models.base_model import *\r\n\r\n\r\ndef G_conv(incoming, in_channels, out_channels, kernel_size, padding, nonlinearity, init, param=None, \r\n        to_sequential=True, use_wscale=True, use_batchnorm=False, use_pixelnorm=True):\r\n    layers = incoming\r\n    layers += [nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=1, padding=padding)]\r\n    he_init(layers[-1], init, param)  # init layers\r\n    if use_wscale:\r\n        layers += [WScaleLayer(layers[-1])]\r\n    layers += [nonlinearity]\r\n    if use_batchnorm:\r\n        layers += [nn.BatchNorm2d(out_channels)]\r\n    if use_pixelnorm:\r\n        layers += [PixelNormLayer()]\r\n    if to_sequential:\r\n        return nn.Sequential(*layers)\r\n    else:\r\n        return layers\r\n\r\n\r\ndef NINLayer(incoming, in_channels, out_channels, nonlinearity, init, param=None, \r\n            to_sequential=True, use_wscale=True):\r\n    layers = incoming\r\n    layers += [nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=1, stride=1, padding=0)]  # NINLayer in lasagne\r\n    he_init(layers[-1], init, param)  # init layers\r\n    if use_wscale:\r\n        layers += [WScaleLayer(layers[-1])]\r\n    if not (nonlinearity == 'linear'):\r\n        layers += [nonlinearity]\r\n    if to_sequential:\r\n        return nn.Sequential(*layers)\r\n    else:\r\n        return layers\r\n\r\n\r\nclass Generator(nn.Module):\r\n    def __init__(self, \r\n                num_channels        = 1,        # Overridden based on dataset.\r\n                resolution          = 32,       # Overridden based on dataset.\r\n                label_size          = 0,        # Overridden based on dataset.\r\n                fmap_base           = 4096,\r\n                fmap_decay          = 1.0,\r\n                fmap_max            = 256,\r\n                latent_size         = None,\r\n                normalize_latents   = True,\r\n                use_wscale          = True,\r\n                use_pixelnorm       = True,\r\n                use_leakyrelu       = True,\r\n                use_batchnorm       = False,\r\n                tanh_at_end         = None):\r\n        super(Generator, self).__init__()\r\n        self.num_channels = num_channels\r\n        self.resolution = resolution\r\n        self.label_size = label_size\r\n        self.fmap_base = fmap_base\r\n        self.fmap_decay = fmap_decay\r\n        self.fmap_max = fmap_max\r\n        self.latent_size = latent_size\r\n        self.normalize_latents = normalize_latents\r\n        self.use_wscale = use_wscale\r\n        self.use_pixelnorm = use_pixelnorm\r\n        self.use_leakyrelu = use_leakyrelu\r\n        self.use_batchnorm = use_batchnorm\r\n        self.tanh_at_end = tanh_at_end\r\n\r\n        R = int(np.log2(resolution))\r\n        assert resolution == 2**R and resolution >= 4\r\n        if latent_size is None: \r\n            latent_size = self.get_nf(0)\r\n\r\n        negative_slope = 0.2\r\n        act = nn.LeakyReLU(negative_slope=negative_slope) if self.use_leakyrelu else nn.ReLU()\r\n        iact = 'leaky_relu' if self.use_leakyrelu else 'relu'\r\n        output_act = nn.Tanh() if self.tanh_at_end else 'linear'\r\n        output_iact = 'tanh' if self.tanh_at_end else 'linear'\r\n\r\n        pre = None\r\n        lods = nn.ModuleList()\r\n        nins = nn.ModuleList()\r\n        layers = []\r\n\r\n        if self.normalize_latents:\r\n            pre = PixelNormLayer()\r\n\r\n        if self.label_size:\r\n            layers += [ConcatLayer()]\r\n\r\n        layers += [ReshapeLayer([latent_size, 1, 1])]\r\n        layers = G_conv(layers, latent_size, self.get_nf(1), 4, 3, act, iact, negative_slope, \r\n                    False, self.use_wscale, self.use_batchnorm, self.use_pixelnorm) \r\n        net = G_conv(layers, latent_size, self.get_nf(1), 3, 1, act, iact, negative_slope, \r\n                    True, self.use_wscale, self.use_batchnorm, self.use_pixelnorm)  # first block\r\n        \r\n        lods.append(net)\r\n        nins.append(NINLayer([], self.get_nf(1), self.num_channels, output_act, output_iact, None, True, self.use_wscale))  # to_rgb layer\r\n\r\n        for I in range(2, R):  # following blocks\r\n            ic, oc = self.get_nf(I-1), self.get_nf(I)\r\n            layers = [nn.Upsample(scale_factor=2, mode='nearest')]  # upsample\r\n            layers = G_conv(layers, ic, oc, 3, 1, act, iact, negative_slope, False, self.use_wscale, self.use_batchnorm, self.use_pixelnorm)\r\n            net = G_conv(layers, oc, oc, 3, 1, act, iact, negative_slope, True, self.use_wscale, self.use_batchnorm, self.use_pixelnorm)\r\n            lods.append(net)\r\n            nins.append(NINLayer([], oc, self.num_channels, output_act, output_iact, None, True, self.use_wscale))  # to_rgb layer\r\n\r\n        self.output_layer = GSelectLayer(pre, lods, nins)\r\n\r\n    def get_nf(self, stage):\r\n        return min(int(self.fmap_base / (2.0 ** (stage * self.fmap_decay))), self.fmap_max)\r\n\r\n    def forward(self, x, y=None, cur_level=None, insert_y_at=None):\r\n        return self.output_layer(x, y, cur_level, insert_y_at)\r\n\r\n\r\ndef D_conv(incoming, in_channels, out_channels, kernel_size, padding, nonlinearity, init, param=None, \r\n        to_sequential=True, use_wscale=True, use_gdrop=True, use_layernorm=False, gdrop_param=dict()):\r\n    layers = incoming\r\n    if use_gdrop:\r\n        layers += [GDropLayer(**gdrop_param)]\r\n    layers += [nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=1, padding=padding)]\r\n    he_init(layers[-1], init, param)  # init layers\r\n    if use_wscale:\r\n        layers += [WScaleLayer(layers[-1])]\r\n    layers += [nonlinearity]\r\n    if use_layernorm:\r\n        layers += [LayerNormLayer()]  # TODO: requires incoming layer\r\n    if to_sequential:\r\n        return nn.Sequential(*layers)\r\n    else:\r\n        return layers\r\n\r\n\r\nclass Discriminator(nn.Module):\r\n    def __init__(self, \r\n                num_channels    = 1,        # Overridden based on dataset.\r\n                resolution      = 32,       # Overridden based on dataset.\r\n                label_size      = 0,        # Overridden based on dataset.\r\n                fmap_base       = 4096,\r\n                fmap_decay      = 1.0,\r\n                fmap_max        = 256,\r\n                mbstat_avg      = 'all',\r\n                mbdisc_kernels  = None,\r\n                use_wscale      = True,\r\n                use_gdrop       = True,\r\n                use_layernorm   = False,\r\n                sigmoid_at_end  = False):\r\n        super(Discriminator, self).__init__()\r\n        self.num_channels = num_channels\r\n        self.resolution = resolution\r\n        self.label_size = label_size\r\n        self.fmap_base = fmap_base\r\n        self.fmap_decay = fmap_decay\r\n        self.fmap_max = fmap_max\r\n        self.mbstat_avg = mbstat_avg\r\n        self.mbdisc_kernels = mbdisc_kernels\r\n        self.use_wscale = use_wscale\r\n        self.use_gdrop = use_gdrop\r\n        self.use_layernorm = use_layernorm\r\n        self.sigmoid_at_end = sigmoid_at_end\r\n\r\n        R = int(np.log2(resolution))\r\n        assert resolution == 2**R and resolution >= 4\r\n        gdrop_strength = 0.0\r\n\r\n        negative_slope = 0.2\r\n        act = nn.LeakyReLU(negative_slope=negative_slope)\r\n        # input activation\r\n        iact = 'leaky_relu'\r\n        # output activation\r\n        output_act = nn.Sigmoid() if self.sigmoid_at_end else 'linear'\r\n        output_iact = 'sigmoid' if self.sigmoid_at_end else 'linear'\r\n        gdrop_param = {'mode': 'prop', 'strength': gdrop_strength}\r\n\r\n        nins = nn.ModuleList()\r\n        lods = nn.ModuleList()\r\n        pre = None\r\n\r\n        nins.append(NINLayer([], self.num_channels, self.get_nf(R-1), act, iact, negative_slope, True, self.use_wscale))\r\n\r\n        for I in range(R-1, 1, -1):\r\n            ic, oc = self.get_nf(I), self.get_nf(I-1)\r\n            net = D_conv([], ic, ic, 3, 1, act, iact, negative_slope, False, \r\n                        self.use_wscale, self.use_gdrop, self.use_layernorm, gdrop_param)\r\n            net = D_conv(net, ic, oc, 3, 1, act, iact, negative_slope, False, \r\n                        self.use_wscale, self.use_gdrop, self.use_layernorm, gdrop_param)\r\n            net += [nn.AvgPool2d(kernel_size=2, stride=2, ceil_mode=False, count_include_pad=False)]\r\n            lods.append(nn.Sequential(*net))\r\n            # nin = [nn.AvgPool2d(kernel_size=2, stride=2, ceil_mode=False, count_include_pad=False)]\r\n            nin = []\r\n            nin = NINLayer(nin, self.num_channels, oc, act, iact, negative_slope, True, self.use_wscale)\r\n            nins.append(nin)\r\n\r\n        net = []\r\n        ic = oc = self.get_nf(1)\r\n        if self.mbstat_avg is not None:\r\n            net += [MinibatchStatConcatLayer(averaging=self.mbstat_avg)]\r\n            ic += 1\r\n        net = D_conv(net, ic, oc, 3, 1, act, iact, negative_slope, False, \r\n                    self.use_wscale, self.use_gdrop, self.use_layernorm, gdrop_param)\r\n        net = D_conv(net, oc, self.get_nf(0), 4, 0, act, iact, negative_slope, False,\r\n                    self.use_wscale, self.use_gdrop, self.use_layernorm, gdrop_param)\r\n\r\n        # Increasing Variation Using MINIBATCH Standard Deviation\r\n        if self.mbdisc_kernels:\r\n            net += [MinibatchDiscriminationLayer(num_kernels=self.mbdisc_kernels)]\r\n\r\n        oc = 1 + self.label_size\r\n        # lods.append(NINLayer(net, self.get_nf(0), oc, 'linear', 'linear', None, True, self.use_wscale))\r\n        lods.append(NINLayer(net, self.get_nf(0), oc, output_act, output_iact, None, True, self.use_wscale))\r\n\r\n        self.output_layer = DSelectLayer(pre, lods, nins)\r\n\r\n    def get_nf(self, stage):\r\n        return min(int(self.fmap_base / (2.0 ** (stage * self.fmap_decay))), self.fmap_max)\r\n\r\n    def forward(self, x, y=None, cur_level=None, insert_y_at=None, gdrop_strength=0.0):\r\n        for module in self.modules():\r\n            if hasattr(module, 'strength'):\r\n                module.strength = gdrop_strength\r\n        return self.output_layer(x, y, cur_level, insert_y_at)\r\n\r\n\r\n# class AutoencodingDiscriminator(nn.Module):\r\n#     def __init__(self, \r\n#                 num_channels    = 1,        # Overridden based on dataset.\r\n#                 resolution      = 32,       # Overridden based on dataset.\r\n#                 fmap_base       = 4096,\r\n#                 fmap_decay      = 1.0,\r\n#                 fmap_max        = 256,\r\n#                 tanh_at_end     = False):\r\n#         super(AutoencodingDiscriminator, self).__init__()\r\n#         self.num_channels = num_channels\r\n#         self.resolution = resolution\r\n#         self.fmap_base = fmap_base\r\n#         self.fmap_decay = fmap_decay\r\n#         self.fmap_max = fmap_max\r\n#         self.tanh_at_end = tanh_at_end\r\n\r\n#         R = int(np.log2(resolution))\r\n#         assert resolution == 2**R and resolution >= 4\r\n        \r\n#         negative_slope = 0.2\r\n#         act = nn.LeakyReLU(negative_slope=negative_slope)\r\n#         iact = 'leaky_relu'\r\n#         output_act = nn.Tanh() if self.tanh_at_end else 'linear'\r\n#         output_iact = 'tanh' if self.tanh_at_end else 'linear'\r\n\r\n#         nins = nn.ModuleList()\r\n#         lods = nn.ModuleList()\r\n#         pre = None\r\n\r\n#         for I in range(R, 1, -1):\r\n#             ic, oc = self.get_nf(I), self.get_nf(I-1)\r\n#             nins.append(NINLayer([], self.num_channels, ic, act, iact, negative_slope, True, True))  # from_rgb layer\r\n\r\n#             net = [nn.Conv2d(ic, oc, 3, 1, 1), act]\r\n#             net += [nn.BatchNorm2d(oc), nn.AvgPool2d(kernel_size=2, stride=2, ceil_mode=False, count_include_pad=False)]\r\n#             he_init(net[0], iact, negative_slope)\r\n#             lods.append(nn.Sequential(*net))\r\n\r\n#         for I in range(2, R+1):\r\n#             ic, oc = self.get_nf(I-1), self.get_nf(I)\r\n#             net = [nn.Upsample(scale_factor=2, mode='nearest'), nn.Conv2d(ic, oc, 3, 1, 1), act, nn.BatchNorm2d(oc)]\r\n#             he_init(net[1], iact, negative_slope)\r\n#             lods.append(nn.Sequential(*net))\r\n#             nins.append(NINLayer([], oc, self.num_channels, output_act, output_iact, None, True, True))  # to_rgb layer\r\n\r\n#         self.output_layer = AEDSelectLayer(pre, lods, nins)\r\n\r\n#     def get_nf(self, stage):\r\n#         return min(int(self.fmap_base / (2.0 ** (stage * self.fmap_decay))), self.fmap_max)\r\n\r\n#     def forward(self, x, cur_level=None):\r\n#         return self.output_layer(x, cur_level)\r\n"""
models/test.py,2,"b""import sys\r\nsys.path.append('models')\r\nfrom models import AutoencodingDiscriminator\r\nimport torch\r\nfrom torch.autograd import Variable\r\nD = AutoencodingDiscriminator(3, 32)\r\nprint(D)\r\nx = Variable(torch.randn(1,3,4,4))\r\nxx = D(x, cur_level=1)  # , cur_level=4\r\nprint(xx.size())\r\n\r\nfor name, p in D.named_parameters():\r\n\tprint(name, p.size())"""
utils/data.py,0,"b""# -*- coding: utf-8 -*-\nimport os, scipy.misc\nfrom glob import glob\nimport numpy as np \nimport h5py\n\n\n#prefix = 'C:\\\\Users\\\\yuan\\\\Downloads'\n# prefix = '/Users/yuan/Downloads/'\nprefix = './datasets/'\n\ndef get_img(img_path, is_crop=True, crop_h=256, resize_h=64, normalize=False):\n    img = scipy.misc.imread(img_path, mode='RGB').astype(np.float)\n    resize_w = resize_h\n    if is_crop:\n        crop_w = crop_h\n        h, w = img.shape[:2]\n        j = int(round((h - crop_h)/2.))\n        i = int(round((w - crop_w)/2.))\n        cropped_image = scipy.misc.imresize(img[j:j+crop_h, i:i+crop_w],[resize_h, resize_w])\n    else:\n        cropped_image = scipy.misc.imresize(img,[resize_h, resize_w])\n    if normalize:\n        cropped_image = cropped_image/127.5 - 1.0\n    return np.transpose(cropped_image, [2, 0, 1])\n\n\n# class CelebA():\n#     def __init__(self):\n#         datapath = os.path.join(prefix, 'celeba/aligned')\n#         self.channel = 3\n#         self.data = glob(os.path.join(datapath, '*.jpg'))\n\n#     def __call__(self, batch_size, size):\n#         batch_number = len(self.data)/batch_size\n#         path_list = [self.data[i] for i in np.random.randint(len(self.data), size=batch_size)]\n#         file_list = [p.split('/')[-1] for p in path_list]\n#         batch = [get_img(img_path, True, 178, size, True) for img_path in path_list]\n#         batch_imgs = np.array(batch).astype(np.float32)\n#         return batch_imgs\n\n#     def save_imgs(self, samples, file_name):\n#         N_samples, channel, height, width = samples.shape\n#         N_row = N_col = int(np.ceil(N_samples**0.5))\n#         combined_imgs = np.ones((channel, N_row*height, N_col*width))\n#         for i in range(N_row):\n#             for j in range(N_col):\n#                 if i*N_col+j < samples.shape[0]:\n#                     combined_imgs[:,i*height:(i+1)*height, j*width:(j+1)*width] = samples[i*N_col+j]\n#         combined_imgs = np.transpose(combined_imgs, [1, 2, 0])\n#         scipy.misc.imsave(file_name+'.png', combined_imgs)\n\n\nclass CelebA():\n    def __init__(self):\n        datapath = 'celeba-hq-1024x1024.h5'\n        resolution = ['data2x2', 'data4x4', 'data8x8', 'data16x16', 'data32x32', 'data64x64', \\\n                        'data128x128', 'data256x256', 'data512x512', 'data1024x1024']\n        self._base_key = 'data'\n        self.dataset = h5py.File(os.path.join(prefix, datapath), 'r')\n        self._len = {k:len(self.dataset[k]) for k in resolution}\n        assert all([resol in self.dataset.keys() for resol in resolution])\n\n    def __call__(self, batch_size, size, level=None):\n        key = self._base_key + '{}x{}'.format(size, size)\n        idx = np.random.randint(self._len[key], size=batch_size)\n        batch_x = np.array([self.dataset[key][i]/127.5-1.0 for i in idx], dtype=np.float32)\n        if level is not None:\n            if level != int(level):\n                min_lw, max_lw = int(level+1)-level, level-int(level)\n                lr_key = self._base_key + '{}x{}'.format(size//2, size//2)\n                low_resol_batch_x = np.array([self.dataset[lr_key][i]/127.5-1.0 for i in idx], dtype=np.float32).repeat(2, axis=2).repeat(2, axis=3)\n                batch_x = batch_x * max_lw + low_resol_batch_x * min_lw\n        return batch_x\n\n    def save_imgs(self, samples, file_name):\n        N_samples, channel, height, width = samples.shape\n        N_row = N_col = int(np.ceil(N_samples**0.5))\n        combined_imgs = np.ones((channel, N_row*height, N_col*width))\n        for i in range(N_row):\n            for j in range(N_col):\n                if i*N_col+j < samples.shape[0]:\n                    combined_imgs[:,i*height:(i+1)*height, j*width:(j+1)*width] = samples[i*N_col+j]\n        combined_imgs = np.transpose(combined_imgs, [1, 2, 0])\n        scipy.misc.imsave(file_name+'.png', combined_imgs)\n\n\nclass RandomNoiseGenerator():\n    def __init__(self, size, noise_type='gaussian'):\n        self.size = size\n        self.noise_type = noise_type.lower()\n        assert self.noise_type in ['gaussian', 'uniform']\n        self.generator_map = {'gaussian': np.random.randn, 'uniform': np.random.uniform}\n        if self.noise_type == 'gaussian':\n            self.generator = lambda s: np.random.randn(*s)\n        elif self.noise_type == 'uniform':\n            self.generator = lambda s: np.random.uniform(-1, 1, size=s)\n\n    def __call__(self, batch_size):\n        return self.generator([batch_size, self.size]).astype(np.float32)\n"""
utils/logger.py,0,"b'# -*- coding: utf-8 -*-\n""""""\nReference : https://github.com/SherlockLiao/pytorch-beginner/tree/master/04-Convolutional%20Neural%20Network\n""""""\nimport tensorflow as tf\nimport numpy as np\nimport scipy.misc\ntry:\n    from StringIO import StringIO  # Python 2.7\nexcept ImportError:\n    from io import BytesIO         # Python 3.x\n\n\nclass Logger(object):\n\n    def __init__(self, log_dir):\n        """"""Create a summary writer logging to log_dir.""""""\n        self.writer = tf.summary.FileWriter(log_dir)\n\n    def scalar_summary(self, tag, value, step):\n        """"""Log a scalar variable.""""""\n        summary = tf.Summary(value=[tf.Summary.Value(tag=tag,\n                                                     simple_value=value)])\n        self.writer.add_summary(summary, step)\n\n    def image_summary(self, tag, images, step):\n        """"""Log a list of images.""""""\n\n        img_summaries = []\n        for i, img in enumerate(images):\n            # Write the image to a string\n            try:\n                s = StringIO()\n            except:\n                s = BytesIO()\n            scipy.misc.toimage(img).save(s, format=""png"")\n\n            # Create an Image object\n            img_sum = tf.Summary.Image(encoded_image_string=s.getvalue(),\n                                       height=img.shape[0],\n                                       width=img.shape[1])\n            # Create a Summary value\n            img_summaries.append(\n                tf.Summary.Value(tag=\'%s/%d\' % (tag, i), image=img_sum))\n\n        # Create and write Summary\n        summary = tf.Summary(value=img_summaries)\n        self.writer.add_summary(summary, step)\n\n    def histo_summary(self, tag, values, step, bins=1000):\n        """"""Log a histogram of the tensor of values.""""""\n\n        # Create a histogram using numpy\n        counts, bin_edges = np.histogram(values, bins=bins)\n\n        # Fill the fields of the histogram proto\n        hist = tf.HistogramProto()\n        hist.min = float(np.min(values))\n        hist.max = float(np.max(values))\n        hist.num = int(np.prod(values.shape))\n        hist.sum = float(np.sum(values))\n        hist.sum_squares = float(np.sum(values**2))\n\n        # Drop the start of the first bin\n        bin_edges = bin_edges[1:]\n\n        # Add bin edges and counts\n        for edge in bin_edges:\n            hist.bucket_limit.append(edge)\n        for c in counts:\n            hist.bucket.append(c)\n\n        # Create and write Summary\n        summary = tf.Summary(value=[tf.Summary.Value(tag=tag, histo=hist)])\n        self.writer.add_summary(summary, step)\n        self.writer.flush()\n'"
