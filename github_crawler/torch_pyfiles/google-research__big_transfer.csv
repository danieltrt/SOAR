file_path,api_count,code
__init__.py,0,"b'# Copyright 2020 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n'"
bit_common.py,0,"b'# Copyright 2020 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# Lint as: python3\n# coding: utf-8\n\nimport argparse\nimport logging\nimport logging.config\nimport os\n\nimport bit_hyperrule\n\n\ndef argparser(known_models):\n  parser = argparse.ArgumentParser(description=""Fine-tune BiT-M model."")\n  parser.add_argument(""--name"", required=True,\n                      help=""Name of this run. Used for monitoring and checkpointing."")\n  parser.add_argument(""--model"", choices=list(known_models),\n                      help=""Which variant to use; BiT-M gives best results."")\n  parser.add_argument(""--logdir"", required=True,\n                      help=""Where to log training info (small)."")\n  parser.add_argument(""--bit_pretrained_dir"", default=\'.\',\n                      help=""Where to search for pretrained BiT models."")\n\n  parser.add_argument(""--dataset"", choices=list(bit_hyperrule.known_dataset_sizes.keys()),\n                      help=""Choose the dataset. It should be easy to add your own! ""\n                      ""Don\'t forget to set --datadir if necessary."")\n  parser.add_argument(""--examples_per_class"", type=int, default=None,\n                      help=""For the few-shot variant, use this many examples ""\n                      ""per class only."")\n  parser.add_argument(""--examples_per_class_seed"", type=int, default=0,\n                      help=""Random seed for selecting examples."")\n\n  parser.add_argument(""--batch"", type=int, default=512,\n                      help=""Batch size."")\n  parser.add_argument(""--batch_split"", type=int, default=1,\n                      help=""Number of batches to compute gradient on before updating weights."")\n  parser.add_argument(""--base_lr"", type=float, default=0.003,\n                      help=""Base learning-rate for fine-tuning. Most likely default is best."")\n  parser.add_argument(""--eval_every"", type=int, default=None,\n                      help=""Run prediction on validation set every so many steps.""\n                      ""Will always run one evaluation at the end of training."")\n  return parser\n\n\ndef setup_logger(args):\n  """"""Creates and returns a fancy logger.""""""\n  # return logging.basicConfig(level=logging.INFO, format=""[%(asctime)s] %(message)s"")\n  # Why is setting up proper logging so !@?#! ugly?\n  os.makedirs(os.path.join(args.logdir, args.name), exist_ok=True)\n  logging.config.dictConfig({\n      ""version"": 1,\n      ""disable_existing_loggers"": False,\n      ""formatters"": {\n          ""standard"": {\n              ""format"": ""%(asctime)s [%(levelname)s] %(name)s: %(message)s""\n          },\n      },\n      ""handlers"": {\n          ""stderr"": {\n              ""level"": ""INFO"",\n              ""formatter"": ""standard"",\n              ""class"": ""logging.StreamHandler"",\n              ""stream"": ""ext://sys.stderr"",\n          },\n          ""logfile"": {\n              ""level"": ""DEBUG"",\n              ""formatter"": ""standard"",\n              ""class"": ""logging.FileHandler"",\n              ""filename"": os.path.join(args.logdir, args.name, ""train.log""),\n              ""mode"": ""a"",\n          }\n      },\n      ""loggers"": {\n          """": {\n              ""handlers"": [""stderr"", ""logfile""],\n              ""level"": ""DEBUG"",\n              ""propagate"": True\n          },\n      }\n  })\n  logger = logging.getLogger(__name__)\n  logger.flush = lambda: [h.flush() for h in logger.handlers]\n  logger.info(args)\n  return logger\n'"
bit_hyperrule.py,0,"b'# Copyright 2020 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\ndef get_resolution(original_resolution):\n  """"""Takes (H,W) and returns (precrop, crop).""""""\n  area = original_resolution[0] * original_resolution[1]\n  return (160, 128) if area < 96*96 else (512, 480)\n\n\nknown_dataset_sizes = {\n  \'cifar10\': (32, 32),\n  \'cifar100\': (32, 32),\n  \'oxford_iiit_pet\': (224, 224),\n  \'oxford_flowers102\': (224, 224),\n  \'imagenet2012\': (224, 224),\n}\n\n\ndef get_resolution_from_dataset(dataset):\n  if dataset not in known_dataset_sizes:\n    raise ValueError(f""Unsupported dataset {dataset}. Add your own here :)"")\n  return get_resolution(known_dataset_sizes[dataset])\n\n\ndef get_mixup(dataset_size):\n  return 0.0 if dataset_size < 20_000 else 0.1\n\n\ndef get_schedule(dataset_size):\n  if dataset_size < 20_000:\n    return [100, 200, 300, 400, 500]\n  elif dataset_size < 500_000:\n    return [500, 3000, 6000, 9000, 10_000]\n  else:\n    return [500, 6000, 12_000, 18_000, 20_000]\n\n\ndef get_lr(step, dataset_size, base_lr=0.003):\n  """"""Returns learning-rate for `step` or None at the end.""""""\n  supports = get_schedule(dataset_size)\n  # Linear warmup\n  if step < supports[0]:\n    return base_lr * step / supports[0]\n  # End of training\n  elif step >= supports[-1]:\n    return None\n  # Staircase decays by factor of 10\n  else:\n    for s in supports[1:]:\n      if s < step:\n        base_lr /= 10\n    return base_lr\n'"
input_pipeline_tf2_or_jax.py,0,"b'# Copyright 2020 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport tensorflow as tf\nimport tensorflow_probability as tfp\nimport tensorflow_datasets as tfds\n\nimport numpy as np\n\n# A workaround to avoid crash because tfds may open to many files.\nimport resource\nlow, high = resource.getrlimit(resource.RLIMIT_NOFILE)\nresource.setrlimit(resource.RLIMIT_NOFILE, (high, high))\n\n# Adjust depending on the available RAM.\nMAX_IN_MEMORY = 200_000\n\n\nDATASET_SPLITS = {\n  \'cifar10\': {\'train\': \'train[:98%]\', \'test\': \'test\'},\n  \'cifar100\': {\'train\': \'train[:98%]\', \'test\': \'test\'},\n  \'imagenet2012\': {\'train\': \'train[:99%]\', \'test\': \'validation\'},\n}\n\n\ndef get_dataset_info(dataset, split, examples_per_class):\n  data_builder = tfds.builder(dataset)\n  original_num_examples = data_builder.info.splits[split].num_examples\n  num_classes = data_builder.info.features[\'label\'].num_classes\n  if examples_per_class is not None:\n    num_examples = examples_per_class * num_classes\n  else:\n    num_examples = original_num_examples\n  return {\'original_num_examples\': original_num_examples,\n          \'num_examples\': num_examples,\n          \'num_classes\': num_classes}\n\n\ndef sample_subset(data, num_examples, num_classes,\n                  examples_per_class, examples_per_class_seed):\n  data = data.batch(min(num_examples, MAX_IN_MEMORY))\n\n  data = data.as_numpy_iterator().next()\n\n  np.random.seed(examples_per_class_seed)\n  indices = [idx\n             for c in range(num_classes)\n             for idx in np.random.choice(np.where(data[\'label\'] == c)[0],\n                                         examples_per_class,\n                                         replace=False)]\n\n  data = {\'image\': data[\'image\'][indices],\n          \'label\': data[\'label\'][indices]}\n\n  data = tf.data.Dataset.zip(\n    (tf.data.Dataset.from_tensor_slices(data[\'image\']),\n     tf.data.Dataset.from_tensor_slices(data[\'label\'])))\n  return data.map(lambda x, y: {\'image\': x, \'label\': y},\n                  tf.data.experimental.AUTOTUNE)\n\n\ndef get_data(dataset, mode,\n             repeats, batch_size,\n             resize_size, crop_size,\n             mixup_alpha,\n             examples_per_class, examples_per_class_seed,\n             num_devices,\n             tfds_manual_dir):\n\n  split = DATASET_SPLITS[dataset][mode]\n  dataset_info = get_dataset_info(dataset, split, examples_per_class)\n\n  data_builder = tfds.builder(dataset)\n  data_builder.download_and_prepare(\n   download_config=tfds.download.DownloadConfig(manual_dir=tfds_manual_dir))\n  data = data_builder.as_dataset(\n    split=split,\n    decoders={\'image\': tfds.decode.SkipDecoding()})\n  decoder = data_builder.info.features[\'image\'].decode_example\n\n  if (mode == \'train\') and (examples_per_class is not None):\n    data = sample_subset(data,\n                         dataset_info[\'original_num_examples\'],\n                         dataset_info[\'num_classes\'],\n                         examples_per_class, examples_per_class_seed)\n\n  def _pp(data):\n    im = decoder(data[\'image\'])\n    if mode == \'train\':\n      im = tf.image.resize(im, [resize_size, resize_size])\n      im = tf.image.random_crop(im, [crop_size, crop_size, 3])\n      im = tf.image.flip_left_right(im)\n    else:\n      # usage of crop_size here is intentional\n      im = tf.image.resize(im, [crop_size, crop_size])\n    im = (im - 127.5) / 127.5\n    label = tf.one_hot(data[\'label\'], dataset_info[\'num_classes\'])\n    return {\'image\': im, \'label\': label}\n\n  data = data.cache()\n  data = data.repeat(repeats)\n  if mode == \'train\':\n    data = data.shuffle(min(dataset_info[\'num_examples\'], MAX_IN_MEMORY))\n  data = data.map(_pp, tf.data.experimental.AUTOTUNE)\n  data = data.batch(batch_size, drop_remainder=True)\n\n  def _mixup(data):\n    beta_dist = tfp.distributions.Beta(mixup_alpha, mixup_alpha)\n    beta = tf.cast(beta_dist.sample([]), tf.float32)\n    data[\'image\'] = (beta * data[\'image\'] +\n                     (1 - beta) * tf.reverse(data[\'image\'], axis=[0]))\n    data[\'label\'] = (beta * data[\'label\'] +\n                     (1 - beta) * tf.reverse(data[\'label\'], axis=[0]))\n    return data\n\n  if mixup_alpha is not None and mixup_alpha > 0.0 and mode == \'train\':\n    data = data.map(_mixup, tf.data.experimental.AUTOTUNE)\n\n  # Shard data such that it can be distributed accross devices\n  def _shard(data):\n    data[\'image\'] = tf.reshape(data[\'image\'],\n                               [num_devices, -1, crop_size, crop_size, 3])\n    data[\'label\'] = tf.reshape(data[\'label\'],\n                               [num_devices, -1, dataset_info[\'num_classes\']])\n    return data\n  if num_devices is not None:\n    data = data.map(_shard, tf.data.experimental.AUTOTUNE)\n\n  return data.prefetch(1)\n'"
bit_jax/__init__.py,0,"b'# Copyright 2020 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n'"
bit_jax/models.py,0,"b'# Copyright 2020 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport jax\nimport jax.numpy as jnp\n\nimport flax.nn as nn\n\n\ndef fixed_padding(x, kernel_size):\n  pad_total = kernel_size - 1\n  pad_beg = pad_total // 2\n  pad_end = pad_total - pad_beg\n\n  x = jax.lax.pad(x, 0.0,\n                  ((0, 0, 0),\n                   (pad_beg, pad_end, 0), (pad_beg, pad_end, 0),\n                   (0, 0, 0)))\n  return x\n\n\ndef standardize(x, axis, eps):\n  x = x - jnp.mean(x, axis=axis, keepdims=True)\n  x = x / jnp.sqrt(jnp.mean(jnp.square(x), axis=axis, keepdims=True) + eps)\n  return x\n\n\nclass GroupNorm(nn.Module):\n  """"""Group normalization (arxiv.org/abs/1803.08494).""""""\n\n  def apply(self, x, num_groups=32):\n\n    input_shape = x.shape\n    group_shape = x.shape[:-1] + (num_groups, x.shape[-1] // num_groups)\n\n    x = x.reshape(group_shape)\n\n    # Standardize along spatial and group dimensions\n    x = standardize(x, axis=[1, 2, 4], eps=1e-5)\n    x = x.reshape(input_shape)\n\n    bias_scale_shape = tuple([1, 1, 1] + [input_shape[-1]])\n    x = x * self.param(\'scale\', bias_scale_shape, nn.initializers.ones)\n    x = x + self.param(\'bias\', bias_scale_shape, nn.initializers.zeros)\n    return x\n\n\nclass StdConv(nn.Conv):\n\n  def param(self, name, shape, initializer):\n    param = super().param(name, shape, initializer)\n    if name == \'kernel\':\n      param = standardize(param, axis=[0, 1, 2], eps=1e-10)\n    return param\n\n\nclass RootBlock(nn.Module):\n\n  def apply(self, x, width):\n    x = fixed_padding(x, 7)\n    x = StdConv(x, width, (7, 7), (2, 2),\n                padding=""VALID"",\n                bias=False,\n                name=""conv_root"")\n\n    x = fixed_padding(x, 3)\n    x = nn.max_pool(x, (3, 3), strides=(2, 2), padding=""VALID"")\n\n    return x\n\n\nclass ResidualUnit(nn.Module):\n  """"""Bottleneck ResNet block.""""""\n\n  def apply(self, x, nout, strides=(1, 1)):\n    x_shortcut = x\n    needs_projection = x.shape[-1] != nout * 4 or strides != (1, 1)\n\n    group_norm = GroupNorm\n    conv = StdConv.partial(bias=False)\n\n    x = group_norm(x, name=""gn1"")\n    x = nn.relu(x)\n    if needs_projection:\n      x_shortcut = conv(x, nout * 4, (1, 1), strides, name=""conv_proj"")\n    x = conv(x, nout, (1, 1), name=""conv1"")\n\n    x = group_norm(x, name=""gn2"")\n    x = nn.relu(x)\n    x = fixed_padding(x, 3)\n    x = conv(x, nout, (3, 3), strides, name=""conv2"", padding=\'VALID\')\n\n    x = group_norm(x, name=""gn3"")\n    x = nn.relu(x)\n    x = conv(x, nout * 4, (1, 1), name=""conv3"")\n\n    return x + x_shortcut\n\n\nclass ResidualBlock(nn.Module):\n\n  def apply(self, x, block_size, nout, first_stride):\n    x = ResidualUnit(\n        x, nout, strides=first_stride,\n        name=""unit01"")\n    for i in range(1, block_size):\n      x = ResidualUnit(\n          x, nout, strides=(1, 1),\n          name=f""unit{i+1:02d}"")\n    return x\n\n\nclass ResNet(nn.Module):\n  """"""ResNetV2.""""""\n\n  def apply(self, x, num_classes=1000,\n            width_factor=1, num_layers=50):\n    block_sizes = _block_sizes[num_layers]\n\n    width = 64 * width_factor\n\n    root_block = RootBlock.partial(width=width)\n    x = root_block(x, name=\'root_block\')\n\n    # Blocks\n    for i, block_size in enumerate(block_sizes):\n      x = ResidualBlock(x, block_size, width * 2 ** i,\n                        first_stride=(1, 1) if i == 0 else (2, 2),\n                        name=f""block{i + 1}"")\n\n    # Pre-head\n    x = GroupNorm(x, name=\'norm-pre-head\')\n    x = nn.relu(x)\n    x = jnp.mean(x, axis=(1, 2))\n\n    # Head\n    x = nn.Dense(x, num_classes, name=""conv_head"",\n                 kernel_init=nn.initializers.zeros)\n\n    return x.astype(jnp.float32)\n\n\n_block_sizes = {\n      50: [3, 4, 6, 3],\n      101: [3, 4, 23, 3],\n      152: [3, 8, 36, 3],\n  }\n\n\nKNOWN_MODELS = dict(\n  [(bit + f\'-R{l}x{w}\', ResNet.partial(num_layers=l, width_factor=w))\n   for bit in [\'BiT-S\', \'BiT-M\']\n   for l, w in [(50, 1), (50, 3), (101, 1), (152, 2), (101, 3), (152, 4)]]\n)\n'"
bit_jax/tf2jax.py,0,"b'# Copyright 2020 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport numpy as np\nimport re\n\n\ndef transform_params(params, params_tf, num_classes):\n  # BiT and JAX models have different naming conventions, so we need to\n  # properly map TF weights to JAX weights\n  params[\'root_block\'][\'conv_root\'][\'kernel\'] = (\n    params_tf[\'resnet/root_block/standardized_conv2d/kernel\'])\n\n  for block in [\'block1\', \'block2\', \'block3\', \'block4\']:\n    units = set([re.findall(r\'unit\\d+\', p)[0] for p in params_tf.keys()\n                 if p.find(block) >= 0])\n    for unit in units:\n      for i, group in enumerate([\'a\', \'b\', \'c\']):\n        params[block][unit][f\'conv{i+1}\'][\'kernel\'] = (\n          params_tf[f\'resnet/{block}/{unit}/{group}/\'\n                    \'standardized_conv2d/kernel\'])\n        params[block][unit][f\'gn{i+1}\'][\'bias\'] = (\n          params_tf[f\'resnet/{block}/{unit}/{group}/\'\n                    \'group_norm/beta\'][None, None, None])\n        params[block][unit][f\'gn{i+1}\'][\'scale\'] = (\n          params_tf[f\'resnet/{block}/{unit}/{group}/\'\n                    \'group_norm/gamma\'][None, None, None])\n\n      projs = [p for p in params_tf.keys()\n               if p.find(f\'{block}/{unit}/a/proj\') >= 0]\n      assert len(projs) <= 1\n      if projs:\n        params[block][unit][\'conv_proj\'][\'kernel\'] = params_tf[projs[0]]\n\n  params[\'norm-pre-head\'][\'bias\'] = (\n    params_tf[\'resnet/group_norm/beta\'][None, None, None])\n  params[\'norm-pre-head\'][\'scale\'] = (\n    params_tf[\'resnet/group_norm/gamma\'][None, None, None])\n\n  params[\'conv_head\'][\'kernel\'] = np.zeros(\n    (params[\'conv_head\'][\'kernel\'].shape[0], num_classes), dtype=np.float32)\n  params[\'conv_head\'][\'bias\'] = np.zeros(num_classes, dtype=np.float32)\n\n'"
bit_jax/train.py,0,"b'# Copyright 2020 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport os\n\nfrom functools import partial\n\nimport numpy as np\n\nimport jax\nimport jax.numpy as jnp\n\nimport flax.optim as optim\nimport flax.jax_utils as flax_utils\n\nimport input_pipeline_tf2_or_jax as input_pipeline\nimport bit_jax.models as models\nimport bit_jax.tf2jax as tf2jax\n\nimport bit_common\nimport bit_hyperrule\n\n\ndef main(args):\n  logger = bit_common.setup_logger(args)\n\n  logger.info(f\'Available devices: {jax.devices()}\')\n\n  model = models.KNOWN_MODELS[args.model]\n\n  # Load weigths of a BiT model\n  bit_model_file = os.path.join(args.bit_pretrained_dir, f\'{args.model}.npz\')\n  if not os.path.exists(bit_model_file):\n    raise FileNotFoundError(\n      f\'Model file is not found in ""{args.bit_pretrained_dir}"" directory.\')\n  with open(bit_model_file, \'rb\') as f:\n    params_tf = np.load(f)\n    params_tf = dict(zip(params_tf.keys(), params_tf.values()))\n\n  resize_size, crop_size = bit_hyperrule.get_resolution_from_dataset(\n    args.dataset)\n\n  # Setup input pipeline\n  dataset_info = input_pipeline.get_dataset_info(\n    args.dataset, \'train\', args.examples_per_class)\n\n  data_train = input_pipeline.get_data(\n    dataset=args.dataset,\n    mode=\'train\',\n    repeats=None, batch_size=args.batch,\n    resize_size=resize_size, crop_size=crop_size,\n    examples_per_class=args.examples_per_class,\n    examples_per_class_seed=args.examples_per_class_seed,\n    mixup_alpha=bit_hyperrule.get_mixup(dataset_info[\'num_examples\']),\n    num_devices=jax.local_device_count(),\n    tfds_manual_dir=args.tfds_manual_dir)\n  logger.info(data_train)\n  data_test = input_pipeline.get_data(\n    dataset=args.dataset,\n    mode=\'test\',\n    repeats=1, batch_size=args.batch_eval,\n    resize_size=resize_size, crop_size=crop_size,\n    examples_per_class=None, examples_per_class_seed=0,\n    mixup_alpha=None,\n    num_devices=jax.local_device_count(),\n    tfds_manual_dir=args.tfds_manual_dir)\n  logger.info(data_test)\n\n  # Build ResNet architecture\n  ResNet = model.partial(num_classes=dataset_info[\'num_classes\'])\n  _, params = ResNet.init_by_shape(\n    jax.random.PRNGKey(0),\n    [([1, crop_size, crop_size, 3], jnp.float32)])\n  resnet_fn = ResNet.call\n\n  # pmap replicates the models over all GPUs\n  resnet_fn_repl = jax.pmap(ResNet.call)\n\n  def cross_entropy_loss(*, logits, labels):\n    logp = jax.nn.log_softmax(logits)\n    return -jnp.mean(jnp.sum(logp * labels, axis=1))\n\n  def loss_fn(params, images, labels):\n    logits = resnet_fn(params, images)\n    return cross_entropy_loss(logits=logits, labels=labels)\n\n  # Update step, replicated over all GPUs\n  @partial(jax.pmap, axis_name=\'batch\')\n  def update_fn(opt, lr, batch):\n    l, g = jax.value_and_grad(loss_fn)(opt.target,\n                                       batch[\'image\'],\n                                       batch[\'label\'])\n    g = jax.tree_map(lambda x: jax.lax.pmean(x, axis_name=\'batch\'), g)\n    opt = opt.apply_gradient(g, learning_rate=lr)\n    return opt\n\n  # In-place update of randomly initialized weights by BiT weigths\n  tf2jax.transform_params(params, params_tf,\n                          num_classes=dataset_info[\'num_classes\'])\n\n  # Create optimizer and replicate it over all GPUs\n  opt = optim.Momentum(beta=0.9).create(params)\n  opt_repl = flax_utils.replicate(opt)\n\n  # Delete referenes to the objects that are not needed anymore\n  del opt\n  del params\n\n  total_steps = bit_hyperrule.get_schedule(dataset_info[\'num_examples\'])[-1]\n\n  # Run training loop\n  for step, batch in zip(range(1, total_steps + 1),\n                         data_train.as_numpy_iterator()):\n    lr = bit_hyperrule.get_lr(step - 1,\n                              dataset_info[\'num_examples\'],\n                              args.base_lr)\n    opt_repl = update_fn(opt_repl, flax_utils.replicate(lr), batch)\n\n    # Run eval step\n    if ((args.eval_every and step % args.eval_every == 0)\n         or (step == total_steps)):\n\n      accuracy_test = np.mean([\n        c\n        for batch in data_test.as_numpy_iterator()\n        for c in (\n          np.argmax(resnet_fn_repl(opt_repl.target, batch[\'image\']), axis=2) ==\n          np.argmax(batch[\'label\'], axis=2)).ravel()])\n\n      logger.info(\n              f\'Step: {step}, \'\n              f\'learning rate: {lr:.07f}, \'\n              f\'Test accuracy: {accuracy_test:0.3f}\')\n\n\nif __name__ == ""__main__"":\n  parser = bit_common.argparser(models.KNOWN_MODELS.keys())\n  parser.add_argument(""--tfds_manual_dir"", default=None,\n                      help=""Path to maually downloaded dataset."")\n  parser.add_argument(""--batch_eval"", default=32, type=int,\n                      help=""Eval batch size."")\n  main(parser.parse_args())\n'"
bit_pytorch/__init__.py,0,"b'# Copyright 2020 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n'"
bit_pytorch/fewshot.py,3,"b'# Copyright 2020 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# Lint as: python3\n""""""Utility to find k-shot dataset indices, outputs the indices on stdout.""""""\n#!/usr/bin/env python3\n# coding: utf-8\n\nfrom collections import *\nfrom functools import *\nimport random\nimport sys\n\nimport torch\nimport torchvision as tv\n\n\nclass AddIndexIter(torch.utils.data.dataloader._SingleProcessDataLoaderIter):\n  def _next_data(self):\n    index = self._next_index()  # may raise StopIteration\n    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration\n    if self._pin_memory:\n      data = torch.utils.data._utils.pin_memory.pin_memory(data)\n    return index, data\n\n\ndef find_indices_loader(loader, n_shots, n_classes):\n  per_label_indices = defaultdict(partial(deque, maxlen=n_shots))\n\n  for ibatch, (indices, (images, labels)) in enumerate(AddIndexIter(loader)):\n    for idx, lbl in zip(indices, labels):\n      per_label_indices[lbl.item()].append(idx)\n  \n      findings = sum(map(len, per_label_indices.values()))\n      if findings == n_shots * n_classes:\n        return per_label_indices\n  raise RuntimeError(""Unable to find enough examples!"")\n\n\ndef find_fewshot_indices(dataset, n_shots):\n  n_classes = len(dataset.classes)\n\n  orig_transform = dataset.transform\n  dataset.transform = tv.transforms.Compose([\n      tv.transforms.CenterCrop(1),\n      tv.transforms.ToTensor()\n  ])\n\n  # TODO(lbeyer): if dataset isinstance DatasetFolder, we can (maybe?) do much better!\n\n  loader = torch.utils.data.DataLoader(dataset, batch_size=1024, shuffle=True, num_workers=0)\n\n  per_label_indices = find_indices_loader(loader, n_shots, n_classes)\n  all_indices = [i for indices in per_label_indices.values() for i in indices]\n  random.shuffle(all_indices)\n\n  dataset.transform = orig_transform\n  return all_indices\n\n\nif __name__ == ""__main__"":\n  dataset = tv.datasets.ImageFolder(sys.argv[2], preprocess)\n  all_indices = find_fewshot_indices(dataset, int(sys.argv[1]))\n  for i in all_indices:\n      print(i)\n'"
bit_pytorch/lbtoolbox.py,0,"b'# Copyright 2020 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# Lint as: python3\n""""""Various utilities from my toolbox at github.com/lucasb-eyer/lbtoolbox.""""""\n\nimport collections\nimport json\nimport signal\nimport time\n\nimport numpy as np\n\n\nclass Uninterrupt:\n  """"""Context manager to gracefully handle interrupts.\n\n  Use as:\n  with Uninterrupt() as u:\n      while not u.interrupted:\n          # train\n  """"""\n\n  def __init__(self, sigs=(signal.SIGINT, signal.SIGTERM), verbose=False):\n    self.sigs = sigs\n    self.verbose = verbose\n    self.interrupted = False\n    self.orig_handlers = None\n\n  def __enter__(self):\n    if self.orig_handlers is not None:\n      raise ValueError(""Can only enter `Uninterrupt` once!"")\n\n    self.interrupted = False\n    self.orig_handlers = [signal.getsignal(sig) for sig in self.sigs]\n\n    def handler(signum, frame):\n      del signum  # unused\n      del frame  # unused\n      self.release()\n      self.interrupted = True\n      if self.verbose:\n        print(""Interruption scheduled..."", flush=True)\n\n    for sig in self.sigs:\n      signal.signal(sig, handler)\n\n    return self\n\n  def __exit__(self, type_, value, tb):\n    self.release()\n\n  def release(self):\n    if self.orig_handlers is not None:\n      for sig, orig in zip(self.sigs, self.orig_handlers):\n        signal.signal(sig, orig)\n      self.orig_handlers = None\n\n\nclass Timer:\n  """"""Context timing its scope.""""""\n\n  def __init__(self, donecb):\n    self.cb = donecb\n\n  def __enter__(self):\n    self.t0 = time.time()\n\n  def __exit__(self, exc_type, exc_value, traceback):\n    t = time.time() - self.t0\n    self.cb(t)\n\n\nclass Chrono:\n  """"""Chronometer for poor-man\'s (but convenient!) profiling.""""""\n\n  def __init__(self):\n    self.timings = collections.OrderedDict()\n\n  def measure(self, what):\n    return Timer(lambda t: self._done(what, t))\n\n  def _done(self, what, t):\n    self.timings.setdefault(what, []).append(t)\n\n  def times(self, what):\n    return self.timings[what]\n\n  def avgtime(self, what, dropfirst=False):\n    timings = self.timings[what]\n    if dropfirst and len(timings) > 1:\n      timings = timings[1:]\n    return sum(timings)/len(timings)\n\n  def __str__(self, fmt=""{:{w}.5f}"", dropfirst=False):\n    avgtimes = {k: self.avgtime(k, dropfirst) for k in self.timings}\n    l = max(map(len, avgtimes))\n    w = max(len(fmt.format(v, w=0)) for v in avgtimes.values())\n    avg_by_time = sorted(avgtimes.items(), key=lambda t: t[1], reverse=True)\n    return ""\\n"".join(f""{name:{l}s}: "" + fmt.format(t, w=w) + ""s""\n                     for name, t in avg_by_time)\n\n\ndef create_dat(basename, dtype, shape, fillvalue=None, **meta):\n  """"""Creates mem-mapped numpy array plus metadata.\n\n  Creates a data file at `basename` and returns a writeable mem-map backed\n  numpy array to it. Can also be passed any json-serializable keys and values\n  in `meta`.\n  """"""\n  xm = np.memmap(basename, mode=""w+"", dtype=dtype, shape=shape)\n  xa = np.ndarray.__new__(np.ndarray, dtype=dtype, shape=shape, buffer=xm)\n  # xa.flush = xm.flush  # Sadly, we can\'t just add attributes to a numpy array, need to subclass it.\n\n  if fillvalue is not None:\n    xa.fill(fillvalue)\n    # xa.flush()\n    xm.flush()\n\n  meta.setdefault(""dtype"", np.dtype(dtype).str)\n  meta.setdefault(""shape"", shape)\n  json.dump(meta, open(basename + "".json"", ""w+""))\n\n  return xa\n\n\ndef load_dat(basename, mode=""r""):\n  """"""Loads file created via `create_dat` as mem-mapped numpy array.\n\n  Returns a read-only mem-mapped numpy array to file at `basename`.\n  If `mode` is set to `\'r+\'`, the data can be written, too.\n  """"""\n  desc = json.load(open(basename + "".json"", ""r""))\n  dtype, shape = desc[""dtype""], desc[""shape""]\n  xm = np.memmap(basename, mode=mode, dtype=dtype, shape=shape)\n  xa = np.ndarray.__new__(np.ndarray, dtype=dtype, shape=shape, buffer=xm)\n  # xa.flush = xm.flush  # Sadly, we can\'t just add attributes to a numpy array, need to subclass it.\n  return xa\n'"
bit_pytorch/models.py,7,"b'# Copyright 2020 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# Lint as: python3\n""""""Bottleneck ResNet v2 with GroupNorm and Weight Standardization.""""""\n\nfrom collections import OrderedDict  # pylint: disable=g-importing-member\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass StdConv2d(nn.Conv2d):\n\n  def forward(self, x):\n    w = self.weight\n    v, m = torch.var_mean(w, dim=[1, 2, 3], keepdim=True, unbiased=False)\n    w = (w - m) / torch.sqrt(v + 1e-10)\n    return F.conv2d(x, w, self.bias, self.stride, self.padding,\n                    self.dilation, self.groups)\n\n\ndef conv3x3(cin, cout, stride=1, groups=1, bias=False):\n  return StdConv2d(cin, cout, kernel_size=3, stride=stride,\n                   padding=1, bias=bias, groups=groups)\n\n\ndef conv1x1(cin, cout, stride=1, bias=False):\n  return StdConv2d(cin, cout, kernel_size=1, stride=stride,\n                   padding=0, bias=bias)\n\n\ndef tf2th(conv_weights):\n  """"""Possibly convert HWIO to OIHW.""""""\n  if conv_weights.ndim == 4:\n    conv_weights = conv_weights.transpose([3, 2, 0, 1])\n  return torch.from_numpy(conv_weights)\n\n\nclass PreActBottleneck(nn.Module):\n  """"""Pre-activation (v2) bottleneck block.\n\n  Follows the implementation of ""Identity Mappings in Deep Residual Networks"":\n  https://github.com/KaimingHe/resnet-1k-layers/blob/master/resnet-pre-act.lua\n\n  Except it puts the stride on 3x3 conv when available.\n  """"""\n\n  def __init__(self, cin, cout=None, cmid=None, stride=1):\n    super().__init__()\n    cout = cout or cin\n    cmid = cmid or cout//4\n\n    self.gn1 = nn.GroupNorm(32, cin)\n    self.conv1 = conv1x1(cin, cmid)\n    self.gn2 = nn.GroupNorm(32, cmid)\n    self.conv2 = conv3x3(cmid, cmid, stride)  # Original code has it on conv1!!\n    self.gn3 = nn.GroupNorm(32, cmid)\n    self.conv3 = conv1x1(cmid, cout)\n    self.relu = nn.ReLU(inplace=True)\n\n    if (stride != 1 or cin != cout):\n      # Projection also with pre-activation according to paper.\n      self.downsample = conv1x1(cin, cout, stride)\n\n  def forward(self, x):\n    out = self.relu(self.gn1(x))\n\n    # Residual branch\n    residual = x\n    if hasattr(self, \'downsample\'):\n      residual = self.downsample(out)\n\n    # Unit\'s branch\n    out = self.conv1(out)\n    out = self.conv2(self.relu(self.gn2(out)))\n    out = self.conv3(self.relu(self.gn3(out)))\n\n    return out + residual\n\n  def load_from(self, weights, prefix=\'\'):\n    convname = \'standardized_conv2d\'\n    with torch.no_grad():\n      self.conv1.weight.copy_(tf2th(weights[f\'{prefix}a/{convname}/kernel\']))\n      self.conv2.weight.copy_(tf2th(weights[f\'{prefix}b/{convname}/kernel\']))\n      self.conv3.weight.copy_(tf2th(weights[f\'{prefix}c/{convname}/kernel\']))\n      self.gn1.weight.copy_(tf2th(weights[f\'{prefix}a/group_norm/gamma\']))\n      self.gn2.weight.copy_(tf2th(weights[f\'{prefix}b/group_norm/gamma\']))\n      self.gn3.weight.copy_(tf2th(weights[f\'{prefix}c/group_norm/gamma\']))\n      self.gn1.bias.copy_(tf2th(weights[f\'{prefix}a/group_norm/beta\']))\n      self.gn2.bias.copy_(tf2th(weights[f\'{prefix}b/group_norm/beta\']))\n      self.gn3.bias.copy_(tf2th(weights[f\'{prefix}c/group_norm/beta\']))\n      if hasattr(self, \'downsample\'):\n        w = weights[f\'{prefix}a/proj/{convname}/kernel\']\n        self.downsample.weight.copy_(tf2th(w))\n\n\nclass ResNetV2(nn.Module):\n  """"""Implementation of Pre-activation (v2) ResNet mode.""""""\n\n  def __init__(self, block_units, width_factor, head_size=21843, zero_head=False):\n    super().__init__()\n    wf = width_factor  # shortcut \'cause we\'ll use it a lot.\n\n    # The following will be unreadable if we split lines.\n    # pylint: disable=line-too-long\n    self.root = nn.Sequential(OrderedDict([\n        (\'conv\', StdConv2d(3, 64*wf, kernel_size=7, stride=2, padding=3, bias=False)),\n        (\'pad\', nn.ConstantPad2d(1, 0)),\n        (\'pool\', nn.MaxPool2d(kernel_size=3, stride=2, padding=0)),\n        # The following is subtly not the same!\n        # (\'pool\', nn.MaxPool2d(kernel_size=3, stride=2, padding=1)),\n    ]))\n\n    self.body = nn.Sequential(OrderedDict([\n        (\'block1\', nn.Sequential(OrderedDict(\n            [(\'unit01\', PreActBottleneck(cin=64*wf, cout=256*wf, cmid=64*wf))] +\n            [(f\'unit{i:02d}\', PreActBottleneck(cin=256*wf, cout=256*wf, cmid=64*wf)) for i in range(2, block_units[0] + 1)],\n        ))),\n        (\'block2\', nn.Sequential(OrderedDict(\n            [(\'unit01\', PreActBottleneck(cin=256*wf, cout=512*wf, cmid=128*wf, stride=2))] +\n            [(f\'unit{i:02d}\', PreActBottleneck(cin=512*wf, cout=512*wf, cmid=128*wf)) for i in range(2, block_units[1] + 1)],\n        ))),\n        (\'block3\', nn.Sequential(OrderedDict(\n            [(\'unit01\', PreActBottleneck(cin=512*wf, cout=1024*wf, cmid=256*wf, stride=2))] +\n            [(f\'unit{i:02d}\', PreActBottleneck(cin=1024*wf, cout=1024*wf, cmid=256*wf)) for i in range(2, block_units[2] + 1)],\n        ))),\n        (\'block4\', nn.Sequential(OrderedDict(\n            [(\'unit01\', PreActBottleneck(cin=1024*wf, cout=2048*wf, cmid=512*wf, stride=2))] +\n            [(f\'unit{i:02d}\', PreActBottleneck(cin=2048*wf, cout=2048*wf, cmid=512*wf)) for i in range(2, block_units[3] + 1)],\n        ))),\n    ]))\n    # pylint: enable=line-too-long\n\n    self.zero_head = zero_head\n    self.head = nn.Sequential(OrderedDict([\n        (\'gn\', nn.GroupNorm(32, 2048*wf)),\n        (\'relu\', nn.ReLU(inplace=True)),\n        (\'avg\', nn.AdaptiveAvgPool2d(output_size=1)),\n        (\'conv\', nn.Conv2d(2048*wf, head_size, kernel_size=1, bias=True)),\n    ]))\n\n  def forward(self, x):\n    x = self.head(self.body(self.root(x)))\n    assert x.shape[-2:] == (1, 1)  # We should have no spatial shape left.\n    return x[...,0,0]\n\n  def load_from(self, weights, prefix=\'resnet/\'):\n    with torch.no_grad():\n      self.root.conv.weight.copy_(tf2th(weights[f\'{prefix}root_block/standardized_conv2d/kernel\']))  # pylint: disable=line-too-long\n      self.head.gn.weight.copy_(tf2th(weights[f\'{prefix}group_norm/gamma\']))\n      self.head.gn.bias.copy_(tf2th(weights[f\'{prefix}group_norm/beta\']))\n      if self.zero_head:\n        nn.init.zeros_(self.head.conv.weight)\n        nn.init.zeros_(self.head.conv.bias)\n      else:\n        self.head.conv.weight.copy_(tf2th(weights[f\'{prefix}head/conv2d/kernel\']))  # pylint: disable=line-too-long\n        self.head.conv.bias.copy_(tf2th(weights[f\'{prefix}head/conv2d/bias\']))\n\n      for bname, block in self.body.named_children():\n        for uname, unit in block.named_children():\n          unit.load_from(weights, prefix=f\'{prefix}{bname}/{uname}/\')\n\n\nKNOWN_MODELS = OrderedDict([\n    (\'BiT-M-R50x1\', lambda *a, **kw: ResNetV2([3, 4, 6, 3], 1, *a, **kw)),\n    (\'BiT-M-R50x3\', lambda *a, **kw: ResNetV2([3, 4, 6, 3], 3, *a, **kw)),\n    (\'BiT-M-R101x1\', lambda *a, **kw: ResNetV2([3, 4, 23, 3], 1, *a, **kw)),\n    (\'BiT-M-R101x3\', lambda *a, **kw: ResNetV2([3, 4, 23, 3], 3, *a, **kw)),\n    (\'BiT-M-R152x2\', lambda *a, **kw: ResNetV2([3, 8, 36, 3], 2, *a, **kw)),\n    (\'BiT-M-R152x4\', lambda *a, **kw: ResNetV2([3, 8, 36, 3], 4, *a, **kw)),\n    (\'BiT-S-R50x1\', lambda *a, **kw: ResNetV2([3, 4, 6, 3], 1, *a, **kw)),\n    (\'BiT-S-R50x3\', lambda *a, **kw: ResNetV2([3, 4, 6, 3], 3, *a, **kw)),\n    (\'BiT-S-R101x1\', lambda *a, **kw: ResNetV2([3, 4, 23, 3], 1, *a, **kw)),\n    (\'BiT-S-R101x3\', lambda *a, **kw: ResNetV2([3, 4, 23, 3], 3, *a, **kw)),\n    (\'BiT-S-R152x2\', lambda *a, **kw: ResNetV2([3, 8, 36, 3], 2, *a, **kw)),\n    (\'BiT-S-R152x4\', lambda *a, **kw: ResNetV2([3, 8, 36, 3], 4, *a, **kw)),\n])\n'"
bit_pytorch/train.py,18,"b'# Copyright 2020 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# Lint as: python3\n""""""Fine-tune a BiT model on some downstream dataset.""""""\n#!/usr/bin/env python3\n# coding: utf-8\nfrom os.path import join as pjoin  # pylint: disable=g-importing-member\nimport time\n\nimport numpy as np\nimport torch\nimport torchvision as tv\n\nimport bit_pytorch.fewshot as fs\nimport bit_pytorch.lbtoolbox as lb\nimport bit_pytorch.models as models\n\nimport bit_common\nimport bit_hyperrule\n\n\ndef topk(output, target, ks=(1,)):\n  """"""Returns one boolean vector for each k, whether the target is within the output\'s top-k.""""""\n  _, pred = output.topk(max(ks), 1, True, True)\n  pred = pred.t()\n  correct = pred.eq(target.view(1, -1).expand_as(pred))\n  return [correct[:k].max(0)[0] for k in ks]\n\n\ndef recycle(iterable):\n  """"""Variant of itertools.cycle that does not save iterates.""""""\n  while True:\n    for i in iterable:\n      yield i\n\n\ndef mktrainval(args, logger):\n  """"""Returns train and validation datasets.""""""\n  precrop, crop = bit_hyperrule.get_resolution_from_dataset(args.dataset)\n  train_tx = tv.transforms.Compose([\n      tv.transforms.Resize((precrop, precrop)),\n      tv.transforms.RandomCrop((crop, crop)),\n      tv.transforms.RandomHorizontalFlip(),\n      tv.transforms.ToTensor(),\n      tv.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n  ])\n  val_tx = tv.transforms.Compose([\n      tv.transforms.Resize((crop, crop)),\n      tv.transforms.ToTensor(),\n      tv.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n  ])\n\n  if args.dataset == ""cifar10"":\n    train_set = tv.datasets.CIFAR10(args.datadir, transform=train_tx, train=True, download=True)\n    valid_set = tv.datasets.CIFAR10(args.datadir, transform=val_tx, train=False, download=True)\n  elif args.dataset == ""cifar100"":\n    train_set = tv.datasets.CIFAR100(args.datadir, transform=train_tx, train=True, download=True)\n    valid_set = tv.datasets.CIFAR100(args.datadir, transform=val_tx, train=False, download=True)\n  elif args.dataset == ""imagenet2012"":\n    train_set = tv.datasets.ImageFolder(pjoin(args.datadir, ""train""), train_tx)\n    valid_set = tv.datasets.ImageFolder(pjoin(args.datadir, ""val""), val_tx)\n  else:\n    raise ValueError(f""Sorry, we have not spent time implementing the ""\n                     f""{args.dataset} dataset in the PyTorch codebase. ""\n                     f""In principle, it should be easy to add :)"")\n\n  if args.examples_per_class is not None:\n    logger.info(f""Looking for {args.examples_per_class} images per class..."")\n    indices = fs.find_fewshot_indices(train_set, args.examples_per_class)\n    train_set = torch.utils.data.Subset(train_set, indices=indices)\n\n  logger.info(f""Using a training set with {len(train_set)} images."")\n  logger.info(f""Using a validation set with {len(valid_set)} images."")\n\n  micro_batch_size = args.batch // args.batch_split\n\n  valid_loader = torch.utils.data.DataLoader(\n      valid_set, batch_size=micro_batch_size, shuffle=False,\n      num_workers=args.workers, pin_memory=True, drop_last=False)\n\n  if micro_batch_size <= len(train_set):\n    train_loader = torch.utils.data.DataLoader(\n        train_set, batch_size=micro_batch_size, shuffle=True,\n        num_workers=args.workers, pin_memory=True, drop_last=False)\n  else:\n    # In the few-shot cases, the total dataset size might be smaller than the batch-size.\n    # In these cases, the default sampler doesn\'t repeat, so we need to make it do that\n    # if we want to match the behaviour from the paper.\n    train_loader = torch.utils.data.DataLoader(\n        train_set, batch_size=micro_batch_size, num_workers=args.workers, pin_memory=True,\n        sampler=torch.utils.data.RandomSampler(train_set, replacement=True, num_samples=micro_batch_size))\n\n  return train_set, valid_set, train_loader, valid_loader\n\n\ndef run_eval(model, data_loader, device, chrono, logger, step):\n  # switch to evaluate mode\n  model.eval()\n\n  logger.info(""Running validation..."")\n  logger.flush()\n\n  all_c, all_top1, all_top5 = [], [], []\n  end = time.time()\n  for b, (x, y) in enumerate(data_loader):\n    with torch.no_grad():\n      x = x.to(device, non_blocking=True)\n      y = y.to(device, non_blocking=True)\n\n      # measure data loading time\n      chrono._done(""eval load"", time.time() - end)\n\n      # compute output, measure accuracy and record loss.\n      with chrono.measure(""eval fprop""):\n        logits = model(x)\n        c = torch.nn.CrossEntropyLoss(reduction=\'none\')(logits, y)\n        top1, top5 = topk(logits, y, ks=(1, 5))\n        all_c.extend(c.cpu())  # Also ensures a sync point.\n        all_top1.extend(top1.cpu())\n        all_top5.extend(top5.cpu())\n\n    # measure elapsed time\n    end = time.time()\n\n  model.train()\n  logger.info(f""Validation@{step} loss {np.mean(all_c):.5f}, ""\n              f""top1 {np.mean(all_top1):.2%}, ""\n              f""top5 {np.mean(all_top5):.2%}"")\n  logger.flush()\n  return all_c, all_top1, all_top5\n\n\ndef mixup_data(x, y, l):\n  """"""Returns mixed inputs, pairs of targets, and lambda""""""\n  indices = torch.randperm(x.shape[0]).to(x.device)\n\n  mixed_x = l * x + (1 - l) * x[indices]\n  y_a, y_b = y, y[indices]\n  return mixed_x, y_a, y_b\n\n\ndef mixup_criterion(criterion, pred, y_a, y_b, l):\n  return l * criterion(pred, y_a) + (1 - l) * criterion(pred, y_b)\n\n\ndef main(args):\n  logger = bit_common.setup_logger(args)\n\n  # Lets cuDNN benchmark conv implementations and choose the fastest.\n  # Only good if sizes stay the same within the main loop!\n  torch.backends.cudnn.benchmark = True\n\n  device = torch.device(""cuda:0"" if torch.cuda.is_available() else ""cpu"")\n  logger.info(f""Going to train on {device}"")\n\n  train_set, valid_set, train_loader, valid_loader = mktrainval(args, logger)\n\n  logger.info(f""Loading model from {args.model}.npz"")\n  model = models.KNOWN_MODELS[args.model](head_size=len(valid_set.classes), zero_head=True)\n  model.load_from(np.load(f""{args.model}.npz""))\n\n  logger.info(""Moving model onto all GPUs"")\n  model = torch.nn.DataParallel(model)\n\n  # Optionally resume from a checkpoint.\n  # Load it to CPU first as we\'ll move the model to GPU later.\n  # This way, we save a little bit of GPU memory when loading.\n  step = 0\n\n  # Note: no weight-decay!\n  optim = torch.optim.SGD(model.parameters(), lr=0.003, momentum=0.9)\n\n  # Resume fine-tuning if we find a saved model.\n  savename = pjoin(args.logdir, args.name, ""bit.pth.tar"")\n  try:\n    logger.info(f""Model will be saved in \'{savename}\'"")\n    checkpoint = torch.load(savename, map_location=""cpu"")\n    logger.info(f""Found saved model to resume from at \'{savename}\'"")\n\n    step = checkpoint[""step""]\n    model.load_state_dict(checkpoint[""model""])\n    optim.load_state_dict(checkpoint[""optim""])\n    logger.info(f""Resumed at step {step}"")\n  except FileNotFoundError:\n    logger.info(""Fine-tuning from BiT"")\n\n  model = model.to(device)\n  optim.zero_grad()\n\n  model.train()\n  mixup = bit_hyperrule.get_mixup(len(train_set))\n  cri = torch.nn.CrossEntropyLoss().to(device)\n\n  logger.info(""Starting training!"")\n  chrono = lb.Chrono()\n  accum_steps = 0\n  mixup_l = np.random.beta(mixup, mixup) if mixup > 0 else 1\n  end = time.time()\n\n  with lb.Uninterrupt() as u:\n    for x, y in recycle(train_loader):\n      # measure data loading time, which is spent in the `for` statement.\n      chrono._done(""load"", time.time() - end)\n\n      if u.interrupted:\n        break\n\n      # Schedule sending to GPU(s)\n      x = x.to(device, non_blocking=True)\n      y = y.to(device, non_blocking=True)\n\n      # Update learning-rate, including stop training if over.\n      lr = bit_hyperrule.get_lr(step, len(train_set), args.base_lr)\n      if lr is None:\n        break\n      for param_group in optim.param_groups:\n        param_group[""lr""] = lr\n\n      if mixup > 0.0:\n        x, y_a, y_b = mixup_data(x, y, mixup_l)\n\n      # compute output\n      with chrono.measure(""fprop""):\n        logits = model(x)\n        if mixup > 0.0:\n          c = mixup_criterion(cri, logits, y_a, y_b, mixup_l)\n        else:\n          c = cri(logits, y)\n        c_num = float(c.data.cpu().numpy())  # Also ensures a sync point.\n\n      # Accumulate grads\n      with chrono.measure(""grads""):\n        (c / args.batch_split).backward()\n        accum_steps += 1\n\n      accstep = f"" ({accum_steps}/{args.batch_split})"" if args.batch_split > 1 else """"\n      logger.info(f""[step {step}{accstep}]: loss={c_num:.5f} (lr={lr:.1e})"")  # pylint: disable=logging-format-interpolation\n      logger.flush()\n\n      # Update params\n      if accum_steps == args.batch_split:\n        with chrono.measure(""update""):\n          optim.step()\n          optim.zero_grad()\n        step += 1\n        accum_steps = 0\n        # Sample new mixup ratio for next batch\n        mixup_l = np.random.beta(mixup, mixup) if mixup > 0 else 1\n\n        # Run evaluation and save the model.\n        if args.eval_every and step % args.eval_every == 0:\n          run_eval(model, valid_loader, device, chrono, logger, step)\n          if args.save:\n            torch.save({\n                ""step"": step,\n                ""model"": model.state_dict(),\n                ""optim"" : optim.state_dict(),\n            }, savename)\n\n      end = time.time()\n\n    # Final eval at end of training.\n    run_eval(model, valid_loader, device, chrono, logger, step=\'end\')\n\n  logger.info(f""Timings:\\n{chrono}"")\n\n\nif __name__ == ""__main__"":\n  parser = bit_common.argparser(models.KNOWN_MODELS.keys())\n  parser.add_argument(""--datadir"", required=True,\n                      help=""Path to the ImageNet data folder, preprocessed for torchvision."")\n  parser.add_argument(""--workers"", type=int, default=8,\n                      help=""Number of background threads used to load data."")\n  parser.add_argument(""--no-save"", dest=""save"", action=""store_false"")\n  main(parser.parse_args())\n'"
bit_tf2/__init__.py,0,"b'# Copyright 2020 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n'"
bit_tf2/models.py,0,"b'# Copyright 2020 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""ResNet architecture as used in BiT.""""""\n\nimport tensorflow.compat.v2 as tf\nfrom . import normalization\n\n\ndef add_name_prefix(name, prefix=None):\n  return prefix + ""/"" + name if prefix else name\n\n\nclass ReLU(tf.keras.layers.ReLU):\n\n  def compute_output_shape(self, input_shape):\n    return tf.TensorShape(input_shape)\n\n\nclass PaddingFromKernelSize(tf.keras.layers.Layer):\n  """"""Layer that adds padding to an image taking into a given kernel size.""""""\n\n  def __init__(self, kernel_size, **kwargs):\n    super(PaddingFromKernelSize, self).__init__(**kwargs)\n    pad_total = kernel_size - 1\n    self._pad_beg = pad_total // 2\n    self._pad_end = pad_total - self._pad_beg\n\n  def compute_output_shape(self, input_shape):\n    batch_size, height, width, channels = tf.TensorShape(input_shape).as_list()\n    if height is not None:\n      height = height + self._pad_beg + self._pad_end\n    if width is not None:\n      width = width + self._pad_beg + self._pad_end\n    return tf.TensorShape((batch_size, height, width, channels))\n\n  def call(self, x):\n    padding = [\n        [0, 0],\n        [self._pad_beg, self._pad_end],\n        [self._pad_beg, self._pad_end],\n        [0, 0]]\n    return tf.pad(x, padding)\n\n\nclass StandardizedConv2D(tf.keras.layers.Conv2D):\n  """"""Implements the abs/1903.10520 technique (see go/dune-gn).\n\n  You can simply replace any Conv2D with this one to use re-parametrized\n  convolution operation in which the kernels are standardized before conv.\n\n  Note that it does not come with extra learnable scale/bias parameters,\n  as those used in ""Weight normalization"" (abs/1602.07868). This does not\n  matter if combined with BN/GN/..., but it would matter if the convolution\n  was used standalone.\n\n  Author: Lucas Beyer\n  """"""\n\n  def build(self, input_shape):\n    super(StandardizedConv2D, self).build(input_shape)\n    # Wrap a standardization around the conv OP.\n    default_conv_op = self._convolution_op\n\n    def standardized_conv_op(inputs, kernel):\n      # Kernel has shape HWIO, normalize over HWI\n      mean, var = tf.nn.moments(kernel, axes=[0, 1, 2], keepdims=True)\n      # Author code uses std + 1e-5\n      return default_conv_op(inputs, (kernel - mean) / tf.sqrt(var + 1e-10))\n\n    self._convolution_op = standardized_conv_op\n    self.built = True\n\n\nclass BottleneckV2Unit(tf.keras.layers.Layer):\n  """"""Implements a standard ResNet\'s unit (version 2).\n  """"""\n\n  def __init__(self, num_filters, stride=1, **kwargs):\n    """"""Initializer.\n\n    Args:\n      num_filters: number of filters in the bottleneck.\n      stride: specifies block\'s stride.\n      **kwargs: other tf.keras.layers.Layer keyword arguments.\n    """"""\n    super(BottleneckV2Unit, self).__init__(**kwargs)\n    self._num_filters = num_filters\n    self._stride = stride\n\n    self._proj = None\n    self._unit_a = tf.keras.Sequential([\n        normalization.GroupNormalization(name=""group_norm""),\n        ReLU(),\n    ], name=""a"")\n    self._unit_a_conv = StandardizedConv2D(\n        filters=num_filters,\n        kernel_size=1,\n        use_bias=False,\n        padding=""VALID"",\n        trainable=self.trainable,\n        name=""a/standardized_conv2d"")\n\n    self._unit_b = tf.keras.Sequential([\n        normalization.GroupNormalization(name=""group_norm""),\n        ReLU(),\n        PaddingFromKernelSize(kernel_size=3),\n        StandardizedConv2D(\n            filters=num_filters,\n            kernel_size=3,\n            strides=stride,\n            use_bias=False,\n            padding=""VALID"",\n            trainable=self.trainable,\n            name=""standardized_conv2d"")\n    ], name=""b"")\n\n    self._unit_c = tf.keras.Sequential([\n        normalization.GroupNormalization(name=""group_norm""),\n        ReLU(),\n        StandardizedConv2D(\n            filters=4 * num_filters,\n            kernel_size=1,\n            use_bias=False,\n            padding=""VALID"",\n            trainable=self.trainable,\n            name=""standardized_conv2d"")\n    ], name=""c"")\n\n  def build(self, input_shape):\n    input_shape = tf.TensorShape(input_shape).as_list()\n\n    # Add projection layer if necessary.\n    if (self._stride > 1) or (4 * self._num_filters != input_shape[-1]):\n      self._proj = StandardizedConv2D(\n          filters=4 * self._num_filters,\n          kernel_size=1,\n          strides=self._stride,\n          use_bias=False,\n          padding=""VALID"",\n          trainable=self.trainable,\n          name=""a/proj/standardized_conv2d"")\n    self.built = True\n\n  def compute_output_shape(self, input_shape):\n    current_shape = self._unit_a.compute_output_shape(input_shape)\n    current_shape = self._unit_a_conv.compute_output_shape(current_shape)\n    current_shape = self._unit_b.compute_output_shape(current_shape)\n    current_shape = self._unit_c.compute_output_shape(current_shape)\n    return current_shape\n\n  def call(self, x):\n    x_shortcut = x\n    # Unit ""a"".\n    x = self._unit_a(x)\n    if self._proj is not None:\n      x_shortcut = self._proj(x)\n    x = self._unit_a_conv(x)\n    # Unit ""b"".\n    x = self._unit_b(x)\n    # Unit ""c"".\n    x = self._unit_c(x)\n\n    return x + x_shortcut\n\n\nclass ResnetV2(tf.keras.Model):\n  """"""Generic ResnetV2 architecture, as used in the BiT paper.""""""\n\n  def __init__(self,\n               num_units=(3, 4, 6, 3),\n               num_outputs=1000,\n               filters_factor=4,\n               strides=(1, 2, 2, 2),\n               **kwargs):\n    super(ResnetV2, self).__init__(**kwargs)\n\n    num_blocks = len(num_units)\n    num_filters = tuple(16 * filters_factor * 2**b for b in range(num_blocks))\n\n    self._root = self._create_root_block(num_filters=num_filters[0])\n    self._blocks = []\n    for b, (f, u, s) in enumerate(zip(num_filters, num_units, strides), 1):\n      n = ""block{}"".format(b)\n      self._blocks.append(\n          self._create_block(num_units=u, num_filters=f, stride=s, name=n))\n    self._pre_head = [\n        normalization.GroupNormalization(name=""group_norm""),\n        ReLU(),\n        tf.keras.layers.GlobalAveragePooling2D()\n    ]\n    self._head = None\n    if num_outputs:\n      self._head = tf.keras.layers.Dense(\n          units=num_outputs,\n          use_bias=True,\n          kernel_initializer=""zeros"",\n          trainable=self.trainable,\n          name=""head/dense"")\n\n  def _create_root_block(self,\n                         num_filters,\n                         conv_size=7,\n                         conv_stride=2,\n                         pool_size=3,\n                         pool_stride=2):\n    layers = [\n        PaddingFromKernelSize(conv_size),\n        StandardizedConv2D(\n            filters=num_filters,\n            kernel_size=conv_size,\n            strides=conv_stride,\n            trainable=self.trainable,\n            use_bias=False,\n            name=""standardized_conv2d""),\n        PaddingFromKernelSize(pool_size),\n        tf.keras.layers.MaxPool2D(\n            pool_size=pool_size, strides=pool_stride, padding=""valid"")\n    ]\n    return tf.keras.Sequential(layers, name=""root_block"")\n\n  def _create_block(self, num_units, num_filters, stride, name):\n    layers = []\n    for i in range(1, num_units + 1):\n      layers.append(\n          BottleneckV2Unit(\n              num_filters=num_filters,\n              stride=(stride if i == 1 else 1),\n              name=""unit%02d"" % i))\n    return tf.keras.Sequential(layers, name=name)\n\n  def compute_output_shape(self, input_shape):\n    current_shape = self._root.compute_output_shape(input_shape)\n    for block in self._blocks:\n      current_shape = block.compute_output_shape(current_shape)\n    for layer in self._pre_head:\n      current_shape = layer.compute_output_shape(current_shape)\n    if self._head is not None:\n      batch_size, features = current_shape.as_list()\n      current_shape = (batch_size, 1, 1, features)\n      current_shape = self._head.compute_output_shape(current_shape).as_list()\n      current_shape = (current_shape[0], current_shape[3])\n    return tf.TensorShape(current_shape)\n\n  def call(self, x):\n    x = self._root(x)\n    for block in self._blocks:\n      x = block(x)\n    for layer in self._pre_head:\n      x = layer(x)\n    if self._head is not None:\n      x = self._head(x)\n    return x\n\n\nKNOWN_MODELS = {\n    f\'{bit}-R{l}x{w}\': f\'gs://bit_models/{bit}-R{l}x{w}.h5\'\n    for bit in [\'BiT-S\', \'BiT-M\']\n    for l, w in [(50, 1), (50, 3), (101, 1), (101, 3), (152, 4)]\n}\n\nNUM_UNITS = {\n    k: (3, 4, 6, 3) if \'R50\' in k else\n       (3, 4, 23, 3) if \'R101\' in k else\n       (3, 8, 36, 3)\n    for k in KNOWN_MODELS\n}\n'"
bit_tf2/normalization.py,0,"b'# Copyright 2020 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Group normalization.""""""\n\nimport tensorflow.compat.v2 as tf\n\n\ndef group_normalize(x, gamma, beta, num_groups=None, group_size=None, eps=1e-5):\n  """"""Applies group-normalization to NHWC `x` (see abs/1803.08494, go/dune-gn).\n\n  This function just does the math, if you want a ""layer"" that creates the\n  necessary variables etc., see `group_norm` below.\n\n  You must either specify a fixed number of groups `num_groups`, which will\n  automatically select a corresponding group size depending on the input\'s\n  number of channels, or you must specify a `group_size`, which leads to an\n  automatic number of groups depending on the input\'s number of channels.\n\n  Args:\n    x: N..C-tensor, the input to group-normalize. For images, this would be a\n      NHWC-tensor, for time-series a NTC, for videos a NHWTC or NTHWC, all of\n      them work, as normalization includes everything between N and C. Even just\n      NC shape works, as C is grouped and normalized.\n    gamma: tensor with C entries, learnable scale after normalization.\n    beta: tensor with C entries, learnable bias after normalization.\n    num_groups: int, number of groups to normalize over (divides C).\n    group_size: int, size of the groups to normalize over (divides C).\n    eps: float, a small additive constant to avoid /sqrt(0).\n\n  Returns:\n    Group-normalized `x`, of the same shape and type as `x`.\n\n  Author: Lucas Beyer\n  """"""\n  assert x.shape.ndims >= 2, (\n      ""Less than 2-dim Tensor passed to GroupNorm. Something\'s fishy."")\n\n  num_channels = x.shape[-1]\n  assert num_channels is not None, ""Cannot apply GroupNorm on dynamic channels.""\n  assert (num_groups is None) != (group_size is None), (\n      ""You must specify exactly one of `num_groups`, `group_size`"")\n\n  if group_size is not None:\n    num_groups = num_channels // group_size\n\n  assert num_channels % num_groups == 0, (\n      ""GroupNorm: {} not divisible by {}"".format(num_channels, num_groups))\n\n  orig_shape = tf.shape(x)\n\n  # This shape is NHWGS where G is #groups and S is group-size.\n  extra_shape = [num_groups, num_channels // num_groups]\n  group_shape = tf.concat([orig_shape[:-1], extra_shape], axis=-1)\n  x = tf.reshape(x, group_shape)\n\n  # The dimensions to normalize over: HWS for images, but more generally all\n  # dimensions except N (batch, first) and G (cross-groups, next-to-last).\n  # So more visually, normdims are the dots in N......G. (note the last one is\n  # also a dot, not a full-stop, argh!)\n  normdims = list(range(1, x.shape.ndims - 2)) + [x.shape.ndims - 1]\n  mean, var = tf.nn.moments(x, normdims, keepdims=True)\n\n  # Interestingly, we don\'t have a beta/gamma per group, but still one per\n  # channel, at least according to the original paper. Reshape such that they\n  # broadcast correctly.\n  beta = tf.reshape(beta, extra_shape)\n  gamma = tf.reshape(gamma, extra_shape)\n  x = tf.nn.batch_normalization(x, mean, var, beta, gamma, eps)\n  return tf.reshape(x, orig_shape)\n\n\nclass GroupNormalization(tf.keras.layers.Layer):\n  """"""A group-norm ""layer"" (see abs/1803.08494 go/dune-gn).\n\n  This function creates beta/gamma variables in a name_scope, and uses them to\n  apply `group_normalize` on the input `x`.\n\n  You can either specify a fixed number of groups `num_groups`, which will\n  automatically select a corresponding group size depending on the input\'s\n  number of channels, or you must specify a `group_size`, which leads to an\n  automatic number of groups depending on the input\'s number of channels.\n  If you specify neither, the paper\'s recommended `num_groups=32` is used.\n\n  Authors: Lucas Beyer, Joan Puigcerver.\n  """"""\n\n  def __init__(self,\n               num_groups=None,\n               group_size=None,\n               eps=1e-5,\n               beta_init=tf.zeros_initializer(),\n               gamma_init=tf.ones_initializer(),\n               **kwargs):\n    """"""Initializer.\n\n    Args:\n      num_groups: int, the number of channel-groups to normalize over.\n      group_size: int, size of the groups to normalize over.\n      eps: float, a small additive constant to avoid /sqrt(0).\n      beta_init: initializer for bias, defaults to zeros.\n      gamma_init: initializer for scale, defaults to ones.\n      **kwargs: other tf.keras.layers.Layer arguments.\n    """"""\n    super(GroupNormalization, self).__init__(**kwargs)\n    if num_groups is None and group_size is None:\n      num_groups = 32\n\n    self._num_groups = num_groups\n    self._group_size = group_size\n    self._eps = eps\n    self._beta_init = beta_init\n    self._gamma_init = gamma_init\n\n  def build(self, input_size):\n    channels = input_size[-1]\n    assert channels is not None, ""Cannot apply GN on dynamic channels.""\n    self._gamma = self.add_weight(\n        name=""gamma"", shape=(channels,), initializer=self._gamma_init,\n        dtype=self.dtype)\n    self._beta = self.add_weight(\n        name=""beta"", shape=(channels,), initializer=self._beta_init,\n        dtype=self.dtype)\n    super(GroupNormalization, self).build(input_size)\n\n  def call(self, x):\n    return group_normalize(x, self._gamma, self._beta, self._num_groups,\n                           self._group_size, self._eps)\n'"
bit_tf2/train.py,0,"b'# Copyright 2020 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# Lint as: python3\n# coding: utf-8\n\nfrom functools import partial\nimport time\nimport os\n\nimport tensorflow.compat.v2 as tf\ntf.enable_v2_behavior()\n\nimport bit_common\nimport bit_hyperrule\nimport bit_tf2.models as models\nimport input_pipeline_tf2_or_jax as input_pipeline\n\n\ndef reshape_for_keras(features, batch_size, crop_size):\n  features[""image""] = tf.reshape(features[""image""], (batch_size, crop_size, crop_size, 3))\n  features[""label""] = tf.reshape(features[""label""], (batch_size, -1))\n  return (features[""image""], features[""label""])\n\n\nclass BiTLRSched(tf.keras.callbacks.Callback):\n  def __init__(self, base_lr, num_samples):\n    self.step = 0\n    self.base_lr = base_lr\n    self.num_samples = num_samples\n\n  def on_train_batch_begin(self, batch, logs=None):\n    lr = bit_hyperrule.get_lr(self.step, self.num_samples, self.base_lr)\n    tf.keras.backend.set_value(self.model.optimizer.lr, lr)\n    self.step += 1\n\n\n\ndef main(args):\n  tf.io.gfile.makedirs(args.logdir)\n  logger = bit_common.setup_logger(args)\n\n  logger.info(f\'Available devices: {tf.config.list_physical_devices()}\')\n\n  tf.io.gfile.makedirs(args.bit_pretrained_dir)\n  bit_model_file = os.path.join(args.bit_pretrained_dir, f\'{args.model}.h5\')\n  if not tf.io.gfile.exists(bit_model_file):\n    model_url = models.KNOWN_MODELS[args.model]\n    logger.info(f\'Downloading the model from {model_url}...\')\n    tf.io.gfile.copy(model_url, bit_model_file)\n\n  # Set up input pipeline\n  dataset_info = input_pipeline.get_dataset_info(\n    args.dataset, \'train\', args.examples_per_class)\n\n  # Distribute training\n  strategy = tf.distribute.MirroredStrategy()\n  num_devices = strategy.num_replicas_in_sync\n  print(\'Number of devices: {}\'.format(num_devices))\n\n  resize_size, crop_size = bit_hyperrule.get_resolution_from_dataset(args.dataset)\n  data_train = input_pipeline.get_data(\n    dataset=args.dataset, mode=\'train\',\n    repeats=None, batch_size=args.batch,\n    resize_size=resize_size, crop_size=crop_size,\n    examples_per_class=args.examples_per_class,\n    examples_per_class_seed=args.examples_per_class_seed,\n    mixup_alpha=bit_hyperrule.get_mixup(dataset_info[\'num_examples\']),\n    num_devices=num_devices,\n    tfds_manual_dir=args.tfds_manual_dir)\n  data_test = input_pipeline.get_data(\n    dataset=args.dataset, mode=\'test\',\n    repeats=1, batch_size=args.batch,\n    resize_size=resize_size, crop_size=crop_size,\n    examples_per_class=1, examples_per_class_seed=0,\n    mixup_alpha=None,\n    num_devices=num_devices,\n    tfds_manual_dir=args.tfds_manual_dir)\n\n  data_train = data_train.map(lambda x: reshape_for_keras(\n    x, batch_size=args.batch, crop_size=crop_size))\n  data_test = data_test.map(lambda x: reshape_for_keras(\n    x, batch_size=args.batch, crop_size=crop_size))\n\n  with strategy.scope():\n    filters_factor = int(args.model[-1])*4\n    model = models.ResnetV2(\n        num_units=models.NUM_UNITS[args.model],\n        num_outputs=21843,\n        filters_factor=filters_factor,\n        name=""resnet"",\n        trainable=True,\n        dtype=tf.float32)\n\n    model.build((None, None, None, 3))\n    logger.info(f\'Loading weights...\')\n    model.load_weights(bit_model_file)\n    logger.info(f\'Weights loaded into model!\')\n\n    model._head = tf.keras.layers.Dense(\n        units=dataset_info[\'num_classes\'],\n        use_bias=True,\n        kernel_initializer=""zeros"",\n        trainable=True,\n        name=""head/dense"")\n\n    lr_supports = bit_hyperrule.get_schedule(dataset_info[\'num_examples\'])\n\n    schedule_length = lr_supports[-1]\n    # NOTE: Let\'s not do that unless verified necessary and we do the same\n    # across all three codebases.\n    # schedule_length = schedule_length * 512 / args.batch\n\n    optimizer = tf.keras.optimizers.SGD(momentum=0.9)\n    loss_fn = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n\n    model.compile(optimizer=optimizer, loss=loss_fn, metrics=[\'accuracy\'])\n\n  logger.info(f\'Fine-tuning the model...\')\n  steps_per_epoch = args.eval_every or schedule_length\n  history = model.fit(\n      data_train,\n      steps_per_epoch=steps_per_epoch,\n      epochs=schedule_length // steps_per_epoch,\n      validation_data=data_test,  # here we are only using\n                                  # this data to evaluate our performance\n      callbacks=[BiTLRSched(args.base_lr, dataset_info[\'num_examples\'])],\n  )\n\n  for epoch, accu in enumerate(history.history[\'val_accuracy\']):\n    logger.info(\n            f\'Step: {epoch * args.eval_every}, \'\n            f\'Test accuracy: {accu:0.3f}\')\n\n\nif __name__ == ""__main__"":\n  parser = bit_common.argparser(models.KNOWN_MODELS.keys())\n  parser.add_argument(""--tfds_manual_dir"", default=None,\n                      help=""Path to maually downloaded dataset."")\n  parser.add_argument(""--batch_eval"", default=32, type=int,\n                      help=""Eval batch size."")\n  main(parser.parse_args())\n'"
