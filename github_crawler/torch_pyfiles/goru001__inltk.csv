file_path,api_count,code
setup.py,1,"b'import setuptools\n\nwith open(""README.md"", ""r"") as fh:\n    long_description = fh.read()\n\nsetuptools.setup(\n    name=""inltk"",\n    version=""0.8.1"",\n    author=""Gaurav"",\n    author_email=""contactgauravforwork@gmail.com"",\n    description=""Natural Language Toolkit for Indian Languages (iNLTK)"",\n    long_description=long_description,\n    long_description_content_type=""text/markdown"",\n    url=""https://github.com/goru001/inltk"",\n    packages=setuptools.find_packages(),\n    classifiers=[\n        ""Programming Language :: Python :: 3"",\n        ""License :: OSI Approved :: MIT License"",\n        ""Operating System :: POSIX :: Linux""\n    ],\n    dependency_links=[\n        \'http://download.pytorch.org/whl/cpu/torch-1.0.0-cp36-cp36m-linux_x86_64.whl\'\n    ],\n    install_requires=[\n        \'aiohttp>=3.5.4\',\n        \'async-timeout>=3.0.1\',\n        ""Pillow"",\n        ""beautifulsoup4"",\n        ""bottleneck"",\n        ""dataclasses;python_version<\'3.7\'"",\n        ""fastprogress>=0.1.19"",\n        ""matplotlib"",\n        ""numexpr"",\n        ""numpy>=1.15"",\n        ""nvidia-ml-py3"",\n        ""packaging"",\n        ""pandas"",\n        ""pynvx>=1.0.0;platform_system==\'Darwin\'"",\n        ""pyyaml"",\n        ""requests"",\n        ""scipy"",\n        ""spacy>=2.0.18"",\n        ""typing"",\n        \'fastai==1.0.57\',\n        ""sentencepiece""\n    ],\n)\n'"
docs/conf.py,0,"b'# Configuration file for the Sphinx documentation builder.\n#\n# This file only contains a selection of the most common options. For a full\n# list see the documentation:\n# https://www.sphinx-doc.org/en/master/usage/configuration.html\n\n# -- Path setup --------------------------------------------------------------\n\n# If extensions (or modules to document with autodoc) are in another directory,\n# add these directories to sys.path here. If the directory is relative to the\n# documentation root, use os.path.abspath to make it absolute, like shown here.\n#\nimport os\nimport sys\nsys.path.insert(0, os.path.abspath(\'../..\'))\nsys.setrecursionlimit(1500)\n\n\n# -- Project information -----------------------------------------------------\n\nproject = \'iNLTK\'\ncopyright = \'2019, Gaurav\'\nauthor = \'Gaurav\'\n\n# The short X.Y version\nversion = \'\'\n# The full version, including alpha/beta/rc tags\nrelease = \'latest\'\n\n# -- General configuration ---------------------------------------------------\n\n# The suffix(es) of source filenames.\n# You can specify multiple suffix as a list of string:\n#\n# source_suffix = [\'.rst\', \'.md\']\nsource_parsers = {\n    \'.md\': \'recommonmark.parser.CommonMarkParser\',\n}\nsource_suffix = [\'.rst\', \'.md\']\n\n\n# Add any Sphinx extension module names here, as strings. They can be\n# extensions coming with Sphinx (named \'sphinx.ext.*\') or your custom\n# ones.\nextensions = [\n    \'sphinx_markdown_tables\',\n    \'sphinx.ext.autodoc\',\n    \'sphinx.ext.intersphinx\',\n    \'sphinx.ext.ifconfig\',\n    \'sphinx.ext.viewcode\',\n    \'sphinx.ext.githubpages\',\n]\n\n# Add any paths that contain templates here, relative to this directory.\ntemplates_path = [\'_templates\']\n\n# The master toctree document.\nmaster_doc = \'index\'\n\n# List of patterns, relative to source directory, that match files and\n# directories to ignore when looking for source files.\n# This pattern also affects html_static_path and html_extra_path.\nexclude_patterns = [\'_build\', \'Thumbs.db\', \'.DS_Store\']\n\n# The name of the Pygments (syntax highlighting) style to use.\npygments_style = \'sphinx\'\n\n# -- Options for HTML output -------------------------------------------------\n\n# The theme to use for HTML and HTML Help pages.  See the documentation for\n# a list of builtin themes.\n#\nhtml_theme = \'alabaster\'\n\n# Add any paths that contain custom static files (such as style sheets) here,\n# relative to this directory. They are copied after the builtin static files,\n# so a file named ""default.css"" will overwrite the builtin ""default.css"".\nhtml_static_path = [\'_static\']\n\nhtml_theme_options = {\n    ""description"": ""Natural Language Toolkit for Indic Languages"",\n    \'github_user\': \'goru001\',\n    \'github_repo\': \'inltk\',\n    \'github_banner\': True,\n    \'github_button\': True,\n    \'font_size\': \'15px\'\n}'"
inltk/__init__.py,0,"b'name = ""inltk""'"
inltk/config.py,0,"b""class LanguageCodes:\n    bengali = 'bn'\n    gujarati = 'gu'\n    hindi = 'hi'\n    kannada = 'kn'\n    malyalam = 'ml'\n    marathi = 'mr'\n    nepali = 'ne'\n    odia = 'or'\n    panjabi = 'pa'\n    sanskrit = 'sa'\n    tamil = 'ta'\n    urdu = 'ur'\n    english = 'en'\n\n    def get_all_language_codes(self):\n        return [self.bengali, self.gujarati, self.hindi,\n                self.kannada, self.malyalam, self.marathi,\n                self.nepali, self.odia, self.panjabi,\n                self.sanskrit, self.tamil, self.urdu, self.english]\n\n\nclass LMConfigs:\n    all_language_codes = LanguageCodes()\n    lm_model_file_url = {\n        all_language_codes.bengali: 'https://www.dropbox.com/s/4berhstpw836kcw/export.pkl?raw=1',\n        all_language_codes.gujarati: 'https://www.dropbox.com/s/6ldfcs35tw2fan3/export.pkl?raw=1',\n        all_language_codes.hindi: 'https://www.dropbox.com/s/sakocwz413eyzt6/export.pkl?raw=1',\n        all_language_codes.kannada: 'https://www.dropbox.com/s/h04gp20t59gv4ra/export.pkl?raw=1',\n        all_language_codes.malyalam: 'https://www.dropbox.com/s/laj4dd0tivquw3d/export.pkl?raw=1',\n        all_language_codes.marathi: 'https://www.dropbox.com/s/o1582btk32pk7tk/export.pkl?raw=1',\n        all_language_codes.nepali: 'https://www.dropbox.com/s/koxiy7l3zbkgzn6/export.pkl?raw=1',\n        all_language_codes.odia: 'https://www.dropbox.com/s/dl3t6sp8p3ifp4q/export.pkl?raw=1',\n        all_language_codes.panjabi: 'https://www.dropbox.com/s/ejiv5pdsi2mhhxa/export.pkl?raw=1',\n        all_language_codes.sanskrit: 'https://www.dropbox.com/s/4ay1by5ryz6k39l/sanskrit_export.pkl?raw=1',\n        all_language_codes.tamil: 'https://www.dropbox.com/s/88klv70zl82u39b/export.pkl?raw=1',\n        all_language_codes.urdu: 'https://www.dropbox.com/s/0ovetjk27np0fcz/urdu_export.pkl?raw=1',\n        all_language_codes.english: 'https://www.dropbox.com/s/fnzfz23tukv3aku/export.pkl?raw=1'\n    }\n    tokenizer_model_file_url = {\n        all_language_codes.bengali: 'https://www.dropbox.com/s/29h7vqme1kb8pmw/bengali_lm.model?raw=1',\n        all_language_codes.gujarati: 'https://www.dropbox.com/s/8ivj97gaprhq5pv/gujarati_lm.model?raw=1',\n        all_language_codes.hindi: 'https://www.dropbox.com/s/xrsjt8zbhwo7zxq/hindi_lm.model?raw=1',\n        all_language_codes.kannada: 'https://www.dropbox.com/s/m8qlc3wgw1m8ggp/kannada_lm.model?raw=1',\n        all_language_codes.malyalam: 'https://www.dropbox.com/s/2lqbb93tzz8vb8a/malyalam_lm.model?raw=1',\n        all_language_codes.marathi: 'https://www.dropbox.com/s/nnq9erkr9z49th7/marathi_lm.model?raw=1',\n        all_language_codes.nepali: 'https://www.dropbox.com/s/kmpc8i3c3n0if23/nepali_lm.model?raw=1',\n        all_language_codes.odia: 'https://www.dropbox.com/s/1xnibv1sytgt9ci/oriya_lm.model?raw=1',\n        all_language_codes.panjabi: 'https://www.dropbox.com/s/jxwr9ytn0zfzulc/panjabi_lm.model?raw=1',\n        all_language_codes.sanskrit: 'https://www.dropbox.com/s/e13401nsekulq17/tokenizer.model?raw=1',\n        all_language_codes.tamil: 'https://www.dropbox.com/s/jpg4kaqyfb71g1v/tokenizer.model?raw=1',\n        all_language_codes.urdu: 'https://www.dropbox.com/s/m5l1yy41ij6vwxa/urdu_lm.model?raw=1',\n        all_language_codes.english: 'https://www.dropbox.com/s/2u3greusrnyh7qy/vocab.pkl?raw=1'\n    }\n\n    def __init__(self, language_code: str):\n        self.language_code = language_code\n\n    def get_config(self):\n        return {\n            'lm_model_url': self.lm_model_file_url[self.language_code],\n            'lm_model_file_name': 'export.pkl',\n            'tokenizer_model_url': self.tokenizer_model_file_url[self.language_code],\n            'tokenizer_model_file_name': 'vocab.pkl' if self.language_code == LMConfigs.all_language_codes.english else 'tokenizer.model'\n        }\n\n\nclass AllLanguageConfig(object):\n\n    @staticmethod\n    def get_config():\n        return {\n            'all_languages_identifying_model_name': 'export.pkl',\n            'all_languages_identifying_model_url': 'https://www.dropbox.com/s/a06fa0zlr7bfif0/export.pkl?raw=1',\n            'all_languages_identifying_tokenizer_name': 'tokenizer.model',\n            'all_languages_identifying_tokenizer_url':\n                'https://www.dropbox.com/s/t4mypdd8aproj88/all_language.model?raw=1'\n        }\n"""
inltk/const.py,0,"b""tokenizer_special_cases = [\n    'xxbos',\n    'xxeos',\n]\n"""
inltk/download_assets.py,0,"b'from pathlib import Path\n\nimport aiohttp as aiohttp\nimport os\n\nfrom .config import LanguageCodes, LMConfigs, AllLanguageConfig\n\nall_language_codes = LanguageCodes()\n\npath = Path(__file__).parent\n\n\nasync def download_file(url, dest, fname):\n    if (dest/f\'{fname}\').exists(): return False\n    os.makedirs(dest, exist_ok=True)\n    print(\'Downloading Model. This might take time, depending on your internet connection. Please be patient.\\n\'\n          \'We\\\'ll only do this for the first time.\')\n    async with aiohttp.ClientSession() as session:\n        async with session.get(url) as response:\n            data = await response.read()\n            with open(dest/f\'{fname}\', \'wb\') as f:\n                f.write(data)\n    return True\n\n\nasync def setup_language(language_code: str):\n    lmconfig = LMConfigs(language_code)\n    config = lmconfig.get_config()\n    await download_file(config[\'lm_model_url\'], path/\'models\'/f\'{language_code}\', config[""lm_model_file_name""])\n    await download_file(config[\'tokenizer_model_url\'], path/\'models\'/f\'{language_code}\',\n                        config[""tokenizer_model_file_name""])\n    print(\'Done!\')\n    return True\n\n\ndef verify_language(language_code: str):\n    lmconfig = LMConfigs(language_code)\n    config = lmconfig.get_config()\n    if (path/\'models\'/f\'{language_code}\'/f\'{config[""lm_model_file_name""]}\').exists() and \\\n            (path/\'models\'/f\'{language_code}\'/f\'{config[""tokenizer_model_file_name""]}\').exists():\n        return True\n    else:\n        return False\n\n\nasync def check_all_languages_identifying_model():\n    config = AllLanguageConfig.get_config()\n    if (path/\'models\'/\'all\'/f\'{config[""all_languages_identifying_model_name""]}\').exists() and \\\n            (path/\'models\'/\'all\'/f\'{config[""all_languages_identifying_tokenizer_name""]}\').exists():\n        return True\n    done = await download_file(config[""all_languages_identifying_model_url""], path/\'models\'/\'all\',\n                        config[""all_languages_identifying_model_name""])\n    done = await download_file(config[""all_languages_identifying_tokenizer_url""], path/\'models\'/\'all\',\n                        config[""all_languages_identifying_tokenizer_name""])\n    return done\n\n'"
inltk/inltk.py,6,"b""import asyncio\nimport random\nfrom math import ceil\nfrom sklearn.metrics.pairwise import cosine_similarity\n\nfrom fastai.text import *\nfrom inltk.config import LanguageCodes\nfrom inltk.download_assets import setup_language, verify_language, check_all_languages_identifying_model\nfrom inltk.tokenizer import LanguageTokenizer\nfrom inltk.const import tokenizer_special_cases\nfrom inltk.utils import cos_sim, reset_models, is_english\n\nlcodes = LanguageCodes()\nall_language_codes = lcodes.get_all_language_codes()\n\n\nasync def download(language_code: str):\n    if language_code not in all_language_codes:\n        raise Exception(f'Language code should be one of {all_language_codes} and not {language_code}')\n    learn = await setup_language(language_code)\n    return learn\n\n\ndef setup(language_code: str):\n    asyncio.set_event_loop(asyncio.new_event_loop())\n    loop = asyncio.get_event_loop()\n    tasks = [asyncio.ensure_future(download(language_code))]\n    learn = loop.run_until_complete(asyncio.gather(*tasks))[0]\n    loop.close()\n\n\ndef check_input_language(language_code: str):\n    if language_code not in all_language_codes:\n        raise Exception(f'Language code should be one of {all_language_codes} and not {language_code}')\n    if not verify_language(language_code):\n        raise Exception(f'You need to do setup for the **first time** for language of your choice so that '\n                        f'we can download models. So, '\n                        f'Please run setup({language_code}) first!')\n\n\ndef predict_next_words(input: str, n_words: int, language_code: str, randomness=0.8):\n    check_input_language(language_code)\n    defaults.device = torch.device('cpu')\n    path = Path(__file__).parent\n    learn = load_learner(path / 'models' / f'{language_code}')\n    output = learn.predict(input, n_words, randomness)\n    # UTF-8 encoding takes care of both LTR and RTL languages\n    if language_code != LanguageCodes.english:\n        output = input + (''.join(output.replace(input, '').split(' '))).replace('\xe2\x96\x81', ' ')\n    for special_str in tokenizer_special_cases:\n        output = output.replace(special_str, '\\n')\n    return output\n\n\ndef tokenize(input: str, language_code: str):\n    check_input_language(language_code)\n    tok = LanguageTokenizer(language_code)\n    output = tok.tokenizer(input)\n    return output\n\n\ndef identify_language(input: str):\n    if is_english(input):\n        return 'en'\n    asyncio.set_event_loop(asyncio.new_event_loop())\n    loop = asyncio.get_event_loop()\n    tasks = [asyncio.ensure_future(check_all_languages_identifying_model())]\n    done = loop.run_until_complete(asyncio.gather(*tasks))[0]\n    loop.close()\n    defaults.device = torch.device('cpu')\n    path = Path(__file__).parent\n    learn = load_learner(path / 'models' / 'all')\n    output = learn.predict(input)\n    return str(output[0])\n\n\ndef remove_foreign_languages(input: str, host_language_code: str):\n    check_input_language(host_language_code)\n    tok = LanguageTokenizer(host_language_code)\n    output = tok.remove_foreign_tokens(input)\n    return output\n\n\ndef reset_language_identifying_models():\n    reset_models('all')\n\n\ndef get_embedding_vectors(input: str, language_code: str):\n    check_input_language(language_code)\n    tok = LanguageTokenizer(language_code)\n    token_ids = tok.numericalize(input)\n    # get learner\n    defaults.device = torch.device('cpu')\n    path = Path(__file__).parent\n    learn = load_learner(path / 'models' / f'{language_code}')\n    encoder = get_model(learn.model)[0]\n    encoder.reset()\n    embeddings = encoder.state_dict()['encoder.weight']\n    embeddings = np.array(embeddings)\n    embedding_vectors = []\n    for token in token_ids:\n        embedding_vectors.append(embeddings[token])\n    return embedding_vectors\n\n\ndef get_sentence_encoding(input: str, language_code: str):\n    check_input_language(language_code)\n    tok = LanguageTokenizer(language_code)\n    token_ids = tok.numericalize(input)\n    # get learner\n    defaults.device = torch.device('cpu')\n    path = Path(__file__).parent\n    learn = load_learner(path / 'models' / f'{language_code}')\n    encoder = learn.model[0]\n    encoder.reset()\n    kk0 = encoder(Tensor([token_ids]).to(torch.int64))\n    return np.array(kk0[0][-1][0][-1])\n\n\ndef get_sentence_similarity(sen1: str, sen2: str, language_code: str, cmp: Callable = cos_sim):\n    check_input_language(language_code)\n    enc1 = get_sentence_encoding(sen1, language_code)\n    enc2 = get_sentence_encoding(sen2, language_code)\n    return cmp(enc1, enc2)\n\n\ndef get_similar_sentences(sen: str, no_of_variations: int, language_code: str, degree_of_aug: float = 0.1):\n    check_input_language(language_code)\n    # get embedding vectors for sen\n    tok = LanguageTokenizer(language_code)\n    token_ids = tok.numericalize(sen)\n    embedding_vectors = get_embedding_vectors(sen, language_code)\n    # get learner\n    defaults.device = torch.device('cpu')\n    path = Path(__file__).parent\n    learn = load_learner(path / 'models' / f'{language_code}')\n    encoder = get_model(learn.model)[0]\n    encoder.reset()\n    embeddings = encoder.state_dict()['encoder.weight']\n    embeddings = np.array(embeddings)\n    # cos similarity of vectors\n    scores = cosine_similarity(embedding_vectors,embeddings)\n    word_ids = [np.argpartition(-np.array(score), no_of_variations+1)[:no_of_variations+1] for score in scores]\n    word_ids = [ids.tolist() for ids in word_ids]\n    for i, ids in enumerate(word_ids):\n        word_ids[i] = [wid for wid in word_ids[i] if wid != token_ids[i]]\n    # generating more variations than required so that we can then filter out the best ones\n    buffer_multiplicity = 2\n    new_sen_tokens = []\n    for i in range(no_of_variations):\n        for k in range(buffer_multiplicity):\n            new_token_ids = []\n            ids = sorted(random.sample(range(len(token_ids)), max(1, int(degree_of_aug * len(token_ids)))))\n            for j in range(len(token_ids)):\n                if j in ids:\n                    new_token_ids.append(word_ids[j][(i + k) % len(word_ids[j])])\n                else:\n                    new_token_ids.append(token_ids[j])\n            new_token_ids = list(map(lambda x: int(x), new_token_ids))\n            new_sen_tokens.append(new_token_ids)\n    new_sens = [tok.textify(sen_tokens) for sen_tokens in new_sen_tokens]\n    while sen in new_sens:\n        new_sens.remove(sen)\n    sen_with_sim_score = [(new_sen, get_sentence_similarity(sen, new_sen, language_code)) for new_sen in new_sens]\n    sen_with_sim_score.sort(key=lambda x: x[1], reverse=True)\n    new_sens = [sen for sen, _ in sen_with_sim_score]\n    return new_sens[:no_of_variations]\n"""
inltk/tokenizer.py,0,"b""from fastai.text import *\nimport sentencepiece as spm\nfrom pathlib import Path\n\nfrom inltk.config import LanguageCodes\n\npath = Path(__file__).parent\n\n\nclass LanguageTokenizer(BaseTokenizer):\n    def __init__(self, lang: str):\n        self.lang = lang\n        self.base = EnglishTokenizer(lang) if lang == LanguageCodes.english else IndicTokenizer(lang)\n\n    def tokenizer(self, t: str) -> List[str]:\n        return self.base.tokenizer(t)\n\n    def numericalize(self, t: str) -> List[int]:\n        return self.base.numericalize(t)\n\n    def textify(self, ids: List[int]) -> str:\n        return self.base.textify(ids)\n\n    def remove_foreign_tokens(self, t: str):\n        return self.base.remove_foreign_tokens(t)\n\n\n# Because we're using spacy tokenizer for english and sentence-piece for other languages\nclass EnglishTokenizer(BaseTokenizer):\n    def __init__(self, lang: str):\n        super().__init__(lang)\n        self.lang = lang\n        with open(path / f'models/{lang}/vocab.pkl', 'rb') as f:\n            self.vocab = Vocab(pickle.load(f))\n        self.tok = SpacyTokenizer(lang)\n\n    def tokenizer(self, t: str) -> List[str]:\n        tok = Tokenizer()\n        tokens = tok.process_text(t, self.tok)\n        tokens = [token for token in tokens if token not in defaults.text_spec_tok]\n        return tokens\n\n    def numericalize(self, t: str):\n        token_ids = self.tokenizer(t)\n        return self.vocab.numericalize(token_ids)\n\n    def textify(self, ids: List[int]):\n        return self.vocab.textify(ids)\n\n    def remove_foreign_tokens(self, t: str):\n        local_pieces = []\n        for i in self.numericalize(t):\n            local_pieces.append(self.textify([i]))\n        return local_pieces\n\n\nclass IndicTokenizer(BaseTokenizer):\n    def __init__(self, lang: str):\n        self.lang = lang\n        self.sp = spm.SentencePieceProcessor()\n        model_path = path/f'models/{lang}/tokenizer.model'\n        self.sp.Load(str(model_path))\n\n    def tokenizer(self, t: str) -> List[str]:\n        return self.sp.EncodeAsPieces(t)\n\n    def numericalize(self, t: str) -> List[int]:\n        return self.sp.EncodeAsIds(t)\n\n    def textify(self, ids: List[int]) -> str:\n        return (''.join([self.sp.IdToPiece(id).replace('\xe2\x96\x81', ' ') for id in ids])).strip()\n\n    def remove_foreign_tokens(self, t: str):\n        local_pieces = []\n        for i in self.sp.EncodeAsIds(t):\n            local_pieces.append(self.sp.IdToPiece(i))\n        return local_pieces\n\nclass AllLanguageTokenizer(LanguageTokenizer):\n    def __init__(self, lang: str):\n        LanguageTokenizer.__init__(self, lang)\n\n\nclass SanskritTokenizer(LanguageTokenizer):\n    def __init__(self, lang: str):\n        LanguageTokenizer.__init__(self, lang)\n\n\nclass BengaliTokenizer(LanguageTokenizer):\n    def __init__(self, lang: str):\n        LanguageTokenizer.__init__(self, lang)\n\n\nclass GujaratiTokenizer(LanguageTokenizer):\n    def __init__(self, lang: str):\n        LanguageTokenizer.__init__(self, lang)\n\n\nclass HindiTokenizer(LanguageTokenizer):\n    def __init__(self, lang: str):\n        LanguageTokenizer.__init__(self, lang)\n\n\nclass KannadaTokenizer(LanguageTokenizer):\n    def __init__(self, lang: str):\n        LanguageTokenizer.__init__(self, lang)\n\n\nclass MalyalamTokenizer(LanguageTokenizer):\n    def __init__(self, lang: str):\n        LanguageTokenizer.__init__(self, lang)\n\n\nclass MarathiTokenizer(LanguageTokenizer):\n    def __init__(self, lang: str):\n        LanguageTokenizer.__init__(self, lang)\n\n\nclass NepaliTokenizer(LanguageTokenizer):\n    def __init__(self, lang: str):\n        LanguageTokenizer.__init__(self, lang)\n\n\nclass OriyaTokenizer(LanguageTokenizer):\n    def __init__(self, lang: str):\n        LanguageTokenizer.__init__(self, lang)\n\n\nclass PanjabiTokenizer(LanguageTokenizer):\n    def __init__(self, lang: str):\n        LanguageTokenizer.__init__(self, lang)\n\n\nclass TamilTokenizer(LanguageTokenizer):\n    def __init__(self, lang: str):\n        LanguageTokenizer.__init__(self, lang)\n\n\nclass UrduTokenizer(LanguageTokenizer):\n    def __init__(self, lang: str):\n        LanguageTokenizer.__init__(self, lang)\n"""
inltk/utils.py,0,"b""from fastai.text import *\n\n\n# cosine similarity\ndef cos_sim(v1, v2):\n    return F.cosine_similarity(Tensor(v1).unsqueeze(0), Tensor(v2).unsqueeze(0)).mean().item()\n\n\ndef reset_models(folder_name: str):\n    path = Path(__file__).parent\n    shutil.rmtree(path / 'models' / f'{folder_name}')\n    return\n\n\ndef is_english(s: str) -> bool:\n    try:\n        s.encode(encoding='utf-8').decode('ascii')\n    except UnicodeDecodeError:\n        return False\n    else:\n        return True\n"""
