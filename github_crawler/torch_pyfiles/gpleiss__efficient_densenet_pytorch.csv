file_path,api_count,code
demo.py,27,"b'import fire\nimport os\nimport time\nimport torch\nfrom torchvision import datasets, transforms\nfrom models import DenseNet\n\n\nclass AverageMeter(object):\n    """"""\n    Computes and stores the average and current value\n    Copied from: https://github.com/pytorch/examples/blob/master/imagenet/main.py\n    """"""\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count\n\n\ndef train_epoch(model, loader, optimizer, epoch, n_epochs, print_freq=1):\n    batch_time = AverageMeter()\n    losses = AverageMeter()\n    error = AverageMeter()\n\n    # Model on train mode\n    model.train()\n\n    end = time.time()\n    for batch_idx, (input, target) in enumerate(loader):\n        # Create vaiables\n        if torch.cuda.is_available():\n            input = input.cuda()\n            target = target.cuda()\n\n        # compute output\n        output = model(input)\n        loss = torch.nn.functional.cross_entropy(output, target)\n\n        # measure accuracy and record loss\n        batch_size = target.size(0)\n        _, pred = output.data.cpu().topk(1, dim=1)\n        error.update(torch.ne(pred.squeeze(), target.cpu()).float().sum().item() / batch_size, batch_size)\n        losses.update(loss.item(), batch_size)\n\n        # compute gradient and do SGD step\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        # measure elapsed time\n        batch_time.update(time.time() - end)\n        end = time.time()\n\n        # print stats\n        if batch_idx % print_freq == 0:\n            res = \'\\t\'.join([\n                \'Epoch: [%d/%d]\' % (epoch + 1, n_epochs),\n                \'Iter: [%d/%d]\' % (batch_idx + 1, len(loader)),\n                \'Time %.3f (%.3f)\' % (batch_time.val, batch_time.avg),\n                \'Loss %.4f (%.4f)\' % (losses.val, losses.avg),\n                \'Error %.4f (%.4f)\' % (error.val, error.avg),\n            ])\n            print(res)\n\n    # Return summary statistics\n    return batch_time.avg, losses.avg, error.avg\n\n\ndef test_epoch(model, loader, print_freq=1, is_test=True):\n    batch_time = AverageMeter()\n    losses = AverageMeter()\n    error = AverageMeter()\n\n    # Model on eval mode\n    model.eval()\n\n    end = time.time()\n    with torch.no_grad():\n        for batch_idx, (input, target) in enumerate(loader):\n            # Create vaiables\n            if torch.cuda.is_available():\n                input = input.cuda()\n                target = target.cuda()\n\n            # compute output\n            output = model(input)\n            loss = torch.nn.functional.cross_entropy(output, target)\n\n            # measure accuracy and record loss\n            batch_size = target.size(0)\n            _, pred = output.data.cpu().topk(1, dim=1)\n            error.update(torch.ne(pred.squeeze(), target.cpu()).float().sum().item() / batch_size, batch_size)\n            losses.update(loss.item(), batch_size)\n\n            # measure elapsed time\n            batch_time.update(time.time() - end)\n            end = time.time()\n\n            # print stats\n            if batch_idx % print_freq == 0:\n                res = \'\\t\'.join([\n                    \'Test\' if is_test else \'Valid\',\n                    \'Iter: [%d/%d]\' % (batch_idx + 1, len(loader)),\n                    \'Time %.3f (%.3f)\' % (batch_time.val, batch_time.avg),\n                    \'Loss %.4f (%.4f)\' % (losses.val, losses.avg),\n                    \'Error %.4f (%.4f)\' % (error.val, error.avg),\n                ])\n                print(res)\n\n    # Return summary statistics\n    return batch_time.avg, losses.avg, error.avg\n\n\ndef train(model, train_set, valid_set, test_set, save, n_epochs=300,\n          batch_size=64, lr=0.1, wd=0.0001, momentum=0.9, seed=None):\n    if seed is not None:\n        torch.manual_seed(seed)\n\n    # Data loaders\n    train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True,\n                                               pin_memory=(torch.cuda.is_available()), num_workers=0)\n    test_loader = torch.utils.data.DataLoader(test_set, batch_size=batch_size, shuffle=False,\n                                              pin_memory=(torch.cuda.is_available()), num_workers=0)\n    if valid_set is None:\n        valid_loader = None\n    else:\n        valid_loader = torch.utils.data.DataLoader(valid_set, batch_size=batch_size, shuffle=False,\n                                                   pin_memory=(torch.cuda.is_available()), num_workers=0)\n    # Model on cuda\n    if torch.cuda.is_available():\n        model = model.cuda()\n\n    # Wrap model for multi-GPUs, if necessary\n    model_wrapper = model\n    if torch.cuda.is_available() and torch.cuda.device_count() > 1:\n        model_wrapper = torch.nn.DataParallel(model).cuda()\n\n    # Optimizer\n    optimizer = torch.optim.SGD(model_wrapper.parameters(), lr=lr, momentum=momentum, nesterov=True, weight_decay=wd)\n    scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[0.5 * n_epochs, 0.75 * n_epochs],\n                                                     gamma=0.1)\n\n    # Start log\n    with open(os.path.join(save, \'results.csv\'), \'w\') as f:\n        f.write(\'epoch,train_loss,train_error,valid_loss,valid_error,test_error\\n\')\n\n    # Train model\n    best_error = 1\n    for epoch in range(n_epochs):\n        _, train_loss, train_error = train_epoch(\n            model=model_wrapper,\n            loader=train_loader,\n            optimizer=optimizer,\n            epoch=epoch,\n            n_epochs=n_epochs,\n        )\n        scheduler.step()\n        _, valid_loss, valid_error = test_epoch(\n            model=model_wrapper,\n            loader=valid_loader if valid_loader else test_loader,\n            is_test=(not valid_loader)\n        )\n\n        # Determine if model is the best\n        if valid_loader:\n            if valid_error < best_error:\n                best_error = valid_error\n                print(\'New best error: %.4f\' % best_error)\n                torch.save(model.state_dict(), os.path.join(save, \'model.dat\'))\n        else:\n            torch.save(model.state_dict(), os.path.join(save, \'model.dat\'))\n\n        # Log results\n        with open(os.path.join(save, \'results.csv\'), \'a\') as f:\n            f.write(\'%03d,%0.6f,%0.6f,%0.5f,%0.5f,\\n\' % (\n                (epoch + 1),\n                train_loss,\n                train_error,\n                valid_loss,\n                valid_error,\n            ))\n\n    # Final test of model on test set\n    model.load_state_dict(torch.load(os.path.join(save, \'model.dat\')))\n    if torch.cuda.is_available() and torch.cuda.device_count() > 1:\n        model = torch.nn.DataParallel(model).cuda()\n    test_results = test_epoch(\n        model=model,\n        loader=test_loader,\n        is_test=True\n    )\n    _, _, test_error = test_results\n    with open(os.path.join(save, \'results.csv\'), \'a\') as f:\n        f.write(\',,,,,%0.5f\\n\' % (test_error))\n    print(\'Final test error: %.4f\' % test_error)\n\n\ndef demo(data, save, depth=100, growth_rate=12, efficient=True, valid_size=5000,\n         n_epochs=300, batch_size=64, seed=None):\n    """"""\n    A demo to show off training of efficient DenseNets.\n    Trains and evaluates a DenseNet-BC on CIFAR-10.\n\n    Args:\n        data (str) - path to directory where data should be loaded from/downloaded\n            (default $DATA_DIR)\n        save (str) - path to save the model to (default /tmp)\n\n        depth (int) - depth of the network (number of convolution layers) (default 40)\n        growth_rate (int) - number of features added per DenseNet layer (default 12)\n        efficient (bool) - use the memory efficient implementation? (default True)\n\n        valid_size (int) - size of validation set\n        n_epochs (int) - number of epochs for training (default 300)\n        batch_size (int) - size of minibatch (default 256)\n        seed (int) - manually set the random seed (default None)\n    """"""\n\n    # Get densenet configuration\n    if (depth - 4) % 3:\n        raise Exception(\'Invalid depth\')\n    block_config = [(depth - 4) // 6 for _ in range(3)]\n\n    # Data transforms\n    mean = [0.5071, 0.4867, 0.4408]\n    stdv = [0.2675, 0.2565, 0.2761]\n    train_transforms = transforms.Compose([\n        transforms.RandomCrop(32, padding=4),\n        transforms.RandomHorizontalFlip(),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=mean, std=stdv),\n    ])\n    test_transforms = transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Normalize(mean=mean, std=stdv),\n    ])\n\n    # Datasets\n    train_set = datasets.CIFAR10(data, train=True, transform=train_transforms, download=True)\n    test_set = datasets.CIFAR10(data, train=False, transform=test_transforms, download=False)\n\n    if valid_size:\n        valid_set = datasets.CIFAR10(data, train=True, transform=test_transforms)\n        indices = torch.randperm(len(train_set))\n        train_indices = indices[:len(indices) - valid_size]\n        valid_indices = indices[len(indices) - valid_size:]\n        train_set = torch.utils.data.Subset(train_set, train_indices)\n        valid_set = torch.utils.data.Subset(valid_set, valid_indices)\n    else:\n        valid_set = None\n\n    # Models\n    model = DenseNet(\n        growth_rate=growth_rate,\n        block_config=block_config,\n        num_init_features=growth_rate*2,\n        num_classes=10,\n        small_inputs=True,\n        efficient=efficient,\n    )\n    print(model)\n    \n    # Print number of parameters\n    num_params = sum(p.numel() for p in model.parameters())\n    print(""Total parameters: "", num_params)\n\n    # Make save directory\n    if not os.path.exists(save):\n        os.makedirs(save)\n    if not os.path.isdir(save):\n        raise Exception(\'%s is not a dir\' % save)\n\n    # Train the model\n    train(model=model, train_set=train_set, valid_set=valid_set, test_set=test_set, save=save,\n          n_epochs=n_epochs, batch_size=batch_size, seed=seed)\n    print(\'Done!\')\n\n\n""""""\nA demo to show off training of efficient DenseNets.\nTrains and evaluates a DenseNet-BC on CIFAR-10.\n\nTry out the efficient DenseNet implementation:\npython demo.py --efficient True --data <path_to_data_dir> --save <path_to_save_dir>\n\nTry out the naive DenseNet implementation:\npython demo.py --efficient False --data <path_to_data_dir> --save <path_to_save_dir>\n\nOther args:\n    --depth (int) - depth of the network (number of convolution layers) (default 40)\n    --growth_rate (int) - number of features added per DenseNet layer (default 12)\n    --n_epochs (int) - number of epochs for training (default 300)\n    --batch_size (int) - size of minibatch (default 256)\n    --seed (int) - manually set the random seed (default None)\n""""""\nif __name__ == \'__main__\':\n    fire.Fire(demo)\n'"
models/__init__.py,0,"b'from .densenet import DenseNet\n\n\n__all__ = [\n    DenseNet,\n]\n'"
models/densenet.py,6,"b'# This implementation is based on the DenseNet-BC implementation in torchvision\n# https://github.com/pytorch/vision/blob/master/torchvision/models/densenet.py\n\nimport math\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.utils.checkpoint as cp\nfrom collections import OrderedDict\n\n\ndef _bn_function_factory(norm, relu, conv):\n    def bn_function(*inputs):\n        concated_features = torch.cat(inputs, 1)\n        bottleneck_output = conv(relu(norm(concated_features)))\n        return bottleneck_output\n\n    return bn_function\n\n\nclass _DenseLayer(nn.Module):\n    def __init__(self, num_input_features, growth_rate, bn_size, drop_rate, efficient=False):\n        super(_DenseLayer, self).__init__()\n        self.add_module(\'norm1\', nn.BatchNorm2d(num_input_features)),\n        self.add_module(\'relu1\', nn.ReLU(inplace=True)),\n        self.add_module(\'conv1\', nn.Conv2d(num_input_features, bn_size * growth_rate,\n                        kernel_size=1, stride=1, bias=False)),\n        self.add_module(\'norm2\', nn.BatchNorm2d(bn_size * growth_rate)),\n        self.add_module(\'relu2\', nn.ReLU(inplace=True)),\n        self.add_module(\'conv2\', nn.Conv2d(bn_size * growth_rate, growth_rate,\n                        kernel_size=3, stride=1, padding=1, bias=False)),\n        self.drop_rate = drop_rate\n        self.efficient = efficient\n\n    def forward(self, *prev_features):\n        bn_function = _bn_function_factory(self.norm1, self.relu1, self.conv1)\n        if self.efficient and any(prev_feature.requires_grad for prev_feature in prev_features):\n            bottleneck_output = cp.checkpoint(bn_function, *prev_features)\n        else:\n            bottleneck_output = bn_function(*prev_features)\n        new_features = self.conv2(self.relu2(self.norm2(bottleneck_output)))\n        if self.drop_rate > 0:\n            new_features = F.dropout(new_features, p=self.drop_rate, training=self.training)\n        return new_features\n\n\nclass _Transition(nn.Sequential):\n    def __init__(self, num_input_features, num_output_features):\n        super(_Transition, self).__init__()\n        self.add_module(\'norm\', nn.BatchNorm2d(num_input_features))\n        self.add_module(\'relu\', nn.ReLU(inplace=True))\n        self.add_module(\'conv\', nn.Conv2d(num_input_features, num_output_features,\n                                          kernel_size=1, stride=1, bias=False))\n        self.add_module(\'pool\', nn.AvgPool2d(kernel_size=2, stride=2))\n\n\nclass _DenseBlock(nn.Module):\n    def __init__(self, num_layers, num_input_features, bn_size, growth_rate, drop_rate, efficient=False):\n        super(_DenseBlock, self).__init__()\n        for i in range(num_layers):\n            layer = _DenseLayer(\n                num_input_features + i * growth_rate,\n                growth_rate=growth_rate,\n                bn_size=bn_size,\n                drop_rate=drop_rate,\n                efficient=efficient,\n            )\n            self.add_module(\'denselayer%d\' % (i + 1), layer)\n\n    def forward(self, init_features):\n        features = [init_features]\n        for name, layer in self.named_children():\n            new_features = layer(*features)\n            features.append(new_features)\n        return torch.cat(features, 1)\n\n\nclass DenseNet(nn.Module):\n    r""""""Densenet-BC model class, based on\n    `""Densely Connected Convolutional Networks"" <https://arxiv.org/pdf/1608.06993.pdf>`\n    Args:\n        growth_rate (int) - how many filters to add each layer (`k` in paper)\n        block_config (list of 3 or 4 ints) - how many layers in each pooling block\n        num_init_features (int) - the number of filters to learn in the first convolution layer\n        bn_size (int) - multiplicative factor for number of bottle neck layers\n            (i.e. bn_size * k features in the bottleneck layer)\n        drop_rate (float) - dropout rate after each dense layer\n        num_classes (int) - number of classification classes\n        small_inputs (bool) - set to True if images are 32x32. Otherwise assumes images are larger.\n        efficient (bool) - set to True to use checkpointing. Much more memory efficient, but slower.\n    """"""\n    def __init__(self, growth_rate=12, block_config=(16, 16, 16), compression=0.5,\n                 num_init_features=24, bn_size=4, drop_rate=0,\n                 num_classes=10, small_inputs=True, efficient=False):\n\n        super(DenseNet, self).__init__()\n        assert 0 < compression <= 1, \'compression of densenet should be between 0 and 1\'\n\n        # First convolution\n        if small_inputs:\n            self.features = nn.Sequential(OrderedDict([\n                (\'conv0\', nn.Conv2d(3, num_init_features, kernel_size=3, stride=1, padding=1, bias=False)),\n            ]))\n        else:\n            self.features = nn.Sequential(OrderedDict([\n                (\'conv0\', nn.Conv2d(3, num_init_features, kernel_size=7, stride=2, padding=3, bias=False)),\n            ]))\n            self.features.add_module(\'norm0\', nn.BatchNorm2d(num_init_features))\n            self.features.add_module(\'relu0\', nn.ReLU(inplace=True))\n            self.features.add_module(\'pool0\', nn.MaxPool2d(kernel_size=3, stride=2, padding=1,\n                                                           ceil_mode=False))\n\n        # Each denseblock\n        num_features = num_init_features\n        for i, num_layers in enumerate(block_config):\n            block = _DenseBlock(\n                num_layers=num_layers,\n                num_input_features=num_features,\n                bn_size=bn_size,\n                growth_rate=growth_rate,\n                drop_rate=drop_rate,\n                efficient=efficient,\n            )\n            self.features.add_module(\'denseblock%d\' % (i + 1), block)\n            num_features = num_features + num_layers * growth_rate\n            if i != len(block_config) - 1:\n                trans = _Transition(num_input_features=num_features,\n                                    num_output_features=int(num_features * compression))\n                self.features.add_module(\'transition%d\' % (i + 1), trans)\n                num_features = int(num_features * compression)\n\n        # Final batch norm\n        self.features.add_module(\'norm_final\', nn.BatchNorm2d(num_features))\n\n        # Linear layer\n        self.classifier = nn.Linear(num_features, num_classes)\n\n        # Initialization\n        for name, param in self.named_parameters():\n            if \'conv\' in name and \'weight\' in name:\n                n = param.size(0) * param.size(2) * param.size(3)\n                param.data.normal_().mul_(math.sqrt(2. / n))\n            elif \'norm\' in name and \'weight\' in name:\n                param.data.fill_(1)\n            elif \'norm\' in name and \'bias\' in name:\n                param.data.fill_(0)\n            elif \'classifier\' in name and \'bias\' in name:\n                param.data.fill_(0)\n\n    def forward(self, x):\n        features = self.features(x)\n        out = F.relu(features, inplace=True)\n        out = F.adaptive_avg_pool2d(out, (1, 1))\n        out = torch.flatten(out, 1)\n        out = self.classifier(out)\n        return out\n'"
