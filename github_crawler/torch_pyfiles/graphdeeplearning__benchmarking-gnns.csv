file_path,api_count,code
main_CitationGraphs_node_classification.py,11,"b'\n\n\n\n\n""""""\n    IMPORTING LIBS\n""""""\nimport dgl\n\nimport numpy as np\nimport os\nimport socket\nimport time\nimport random\nimport glob\nimport argparse, json\nimport pickle\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\n\nfrom tensorboardX import SummaryWriter\nfrom tqdm import tqdm\n\nclass DotDict(dict):\n    def __init__(self, **kwds):\n        self.update(kwds)\n        self.__dict__ = self\n\n\n\n\n\n\n""""""\n    IMPORTING CUSTOM MODULES/METHODS\n""""""\n\nfrom nets.CitationGraphs_node_classification.load_net import gnn_model # import GNNs\nfrom data.data import LoadData # import dataset\nfrom train.train_CitationGraphs_node_classification import train_epoch, evaluate_network # import train functions\n\n\n\n\n""""""\n    GPU Setup\n""""""\ndef gpu_setup(use_gpu, gpu_id):\n    os.environ[""CUDA_DEVICE_ORDER""] = ""PCI_BUS_ID""\n    os.environ[""CUDA_VISIBLE_DEVICES""] = str(gpu_id)  \n\n    if torch.cuda.is_available() and use_gpu:\n        print(\'cuda available with GPU:\',torch.cuda.get_device_name(0))\n        device = torch.device(""cuda"")\n    else:\n        print(\'cuda not available\')\n        device = torch.device(""cpu"")\n    return device\n\n\n\n\n\n\n\n\n\n\n""""""\n    VIEWING MODEL CONFIG AND PARAMS\n""""""\ndef view_model_param(MODEL_NAME, net_params):\n    model = gnn_model(MODEL_NAME, net_params)\n    total_param = 0\n    print(""MODEL DETAILS:\\n"")\n    #print(model)\n    for param in model.parameters():\n        #print(param.data.size())\n        total_param += np.prod(list(param.data.size()))\n    print(\'MODEL/Total parameters:\', MODEL_NAME, total_param)\n    return total_param\n\n\n""""""\n    TRAINING CODE\n""""""\n\ndef train_val_pipeline(MODEL_NAME, dataset, params, net_params, dirs):\n    \n    start0 = time.time()\n    per_epoch_time = []\n    \n    DATASET_NAME = dataset.name\n    \n    if MODEL_NAME in [\'GCN\', \'GAT\']:\n        if net_params[\'self_loop\']:\n            print(""[!] Adding graph self-loops for GCN/GAT models (central node trick)."")\n            dataset._add_self_loops()\n    \n\n    root_log_dir, root_ckpt_dir, write_file_name, write_config_file = dirs\n    device = net_params[\'device\']\n\n    train_mask = dataset.train_mask.to(device)\n    val_mask = dataset.val_mask.to(device)\n    test_mask = dataset.test_mask.to(device)\n    labels = dataset.labels.to(device)\n    \n    # Write network and optimization hyper-parameters in folder config/\n    with open(write_config_file + \'.txt\', \'w\') as f:\n        f.write(""""""Dataset: {},\\nModel: {}\\n\\nparams={}\\n\\nnet_params={}\\n\\n\\nTotal Parameters: {}\\n\\n""""""                .format(DATASET_NAME, MODEL_NAME, params, net_params, net_params[\'total_param\']))\n        \n    log_dir = os.path.join(root_log_dir, ""RUN_"" + str(0))\n    writer = SummaryWriter(log_dir=log_dir)\n\n    # setting seeds\n    random.seed(params[\'seed\'])\n    np.random.seed(params[\'seed\'])\n    torch.manual_seed(params[\'seed\'])\n    if device == \'cuda\':\n        torch.cuda.manual_seed(params[\'seed\'])\n    \n    print(""Training Nodes: "", train_mask.int().sum().item())\n    print(""Validation Nodes: "", val_mask.int().sum().item())\n    print(""Test Nodes: "", test_mask.int().sum().item())\n    print(""Number of Classes: "", net_params[\'n_classes\'])\n\n    model = gnn_model(MODEL_NAME, net_params)\n    model = model.to(device)\n\n    optimizer = optim.Adam(model.parameters(), lr=params[\'init_lr\'], weight_decay=params[\'weight_decay\'])\n    #scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\'min\',\n    #                                                 factor=params[\'lr_reduce_factor\'],\n    #                                                 patience=params[\'lr_schedule_patience\'],\n    #                                                 verbose=True)\n    \n    epoch_train_losses, epoch_val_losses = [], []\n    epoch_train_accs, epoch_val_accs = [], [] \n\n    graph = dataset.graph\n    nfeat = graph.ndata[\'feat\'].to(device)\n    efeat = graph.edata[\'feat\'].to(device)\n    norm_n = dataset.norm_n.to(device)\n    norm_e = dataset.norm_e.to(device)\n    \n    # At any point you can hit Ctrl + C to break out of training early.\n    try:\n        with tqdm(range(params[\'epochs\'])) as t:\n            for epoch in t:\n\n                t.set_description(\'Epoch %d\' % epoch)\n\n                start = time.time()\n\n                epoch_train_loss, epoch_train_acc, optimizer = train_epoch(model, optimizer, device, graph, nfeat, efeat, norm_n, norm_e, train_mask, labels, epoch)\n\n                epoch_val_loss, epoch_val_acc = evaluate_network(model, optimizer, device, graph, nfeat, efeat, norm_n, norm_e, val_mask, labels, epoch)\n                epoch_test_loss, epoch_test_acc = evaluate_network(model, optimizer, device, graph, nfeat, efeat, norm_n, norm_e, test_mask, labels, epoch)\n\n                epoch_train_losses.append(epoch_train_loss)\n                epoch_val_losses.append(epoch_val_loss)\n                epoch_train_accs.append(epoch_train_acc)\n                epoch_val_accs.append(epoch_val_acc)\n\n                writer.add_scalar(\'train/_loss\', epoch_train_loss, epoch)\n                writer.add_scalar(\'val/_loss\', epoch_val_loss, epoch)\n                writer.add_scalar(\'train/_acc\', epoch_train_acc, epoch)\n                writer.add_scalar(\'val/_acc\', epoch_val_acc, epoch)\n                writer.add_scalar(\'learning_rate\', optimizer.param_groups[0][\'lr\'], epoch)\n\n                _, epoch_test_acc = evaluate_network(model, optimizer, device, graph, nfeat, efeat, norm_n, norm_e, test_mask, labels, epoch)\n                t.set_postfix(time=time.time()-start, lr=optimizer.param_groups[0][\'lr\'],\n                              train_loss=epoch_train_loss, val_loss=epoch_val_loss,\n                              train_acc=epoch_train_acc, val_acc=epoch_val_acc,\n                              test_acc=epoch_test_acc)\n\n                per_epoch_time.append(time.time()-start)\n\n                # Saving checkpoint\n                ckpt_dir = os.path.join(root_ckpt_dir, ""RUN_"")\n                if not os.path.exists(ckpt_dir):\n                    os.makedirs(ckpt_dir)\n                torch.save(model.state_dict(), \'{}.pkl\'.format(ckpt_dir + ""/epoch_"" + str(epoch)))\n\n                files = glob.glob(ckpt_dir + \'/*.pkl\')\n                for file in files:\n                    epoch_nb = file.split(\'_\')[-1]\n                    epoch_nb = int(epoch_nb.split(\'.\')[0])\n                    if epoch_nb < epoch-1:\n                        os.remove(file)\n\n                #scheduler.step(epoch_val_loss)\n\n                if optimizer.param_groups[0][\'lr\'] < params[\'min_lr\']:\n                    optimizer.param_groups[0][\'lr\'] = params[\'min_lr\']\n                    #print(""\\n!! LR SMALLER OR EQUAL TO MIN LR THRESHOLD."")\n                    #break\n                    \n                # Stop training after params[\'max_time\'] hours\n                if time.time()-start0 > params[\'max_time\']*3600:\n                    print(\'-\' * 89)\n                    print(""Max_time for training elapsed {:.2f} hours, so stopping"".format(params[\'max_time\']))\n                    break\n    \n    except KeyboardInterrupt:\n        print(\'-\' * 89)\n        print(\'Exiting from training early because of KeyboardInterrupt\')\n    \n    \n    _, test_acc = evaluate_network(model, optimizer, device, graph, nfeat, efeat, norm_n, norm_e, test_mask, labels, epoch)\n    _, train_acc = evaluate_network(model, optimizer, device, graph, nfeat, efeat, norm_n, norm_e, train_mask, labels, epoch)\n    print(""Test Accuracy: {:.4f}"".format(test_acc))\n    print(""Train Accuracy: {:.4f}"".format(train_acc))\n    print(""TOTAL TIME TAKEN: {:.4f}s"".format(time.time()-start0))\n    print(""AVG TIME PER EPOCH: {:.4f}s"".format(np.mean(per_epoch_time)))\n\n    writer.close()\n\n    """"""\n        Write the results in out_dir/results folder\n    """"""\n    with open(write_file_name + \'.txt\', \'w\') as f:\n        f.write(""""""Dataset: {},\\nModel: {}\\n\\nparams={}\\n\\nnet_params={}\\n\\n{}\\n\\nTotal Parameters: {}\\n\\n\n    FINAL RESULTS\\nTEST ACCURACY: {:.4f}\\nTRAIN ACCURACY: {:.4f}\\n\\n\n    Total Time Taken: {:.4f} hrs\\nAverage Time Per Epoch: {:.4f} s\\n\\n\\n""""""\\\n          .format(DATASET_NAME, MODEL_NAME, params, net_params, model, net_params[\'total_param\'],\n                  test_acc, train_acc, (time.time()-start0)/3600, np.mean(per_epoch_time)))\n\n        \n    # send results to gmail\n    try:\n        from gmail import send\n        subject = \'Result for Dataset: {}, Model: {}\'.format(DATASET_NAME, MODEL_NAME)\n        body = """"""Dataset: {},\\nModel: {}\\n\\nparams={}\\n\\nnet_params={}\\n\\n{}\\n\\nTotal Parameters: {}\\n\\n\n    FINAL RESULTS\\nTEST ACCURACY: {:.4f}\\nTRAIN ACCURACY: {:.4f}\\n\\n\n    Total Time Taken: {:.4f} hrs\\nAverage Time Per Epoch: {:.4f} s\\n\\n\\n""""""\\\n          .format(DATASET_NAME, MODEL_NAME, params, net_params, model, net_params[\'total_param\'],\n                  test_acc, train_acc, (time.time()-start0)/3600, np.mean(per_epoch_time))\n        send(subject, body)\n    except:\n        pass\n        \n\n\n\n\ndef main():    \n    """"""\n        USER CONTROLS\n    """"""\n    \n    \n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'--config\', help=""Please give a config.json file with training/model/data/param details"")\n    parser.add_argument(\'--gpu_id\', help=""Please give a value for gpu id"")\n    parser.add_argument(\'--model\', help=""Please give a value for model name"")\n    parser.add_argument(\'--dataset\', help=""Please give a value for dataset name"")\n    parser.add_argument(\'--builtin\', help=""Please give a value for builtin"")\n    parser.add_argument(\'--out_dir\', help=""Please give a value for out_dir"")\n    parser.add_argument(\'--seed\', help=""Please give a value for seed"")\n    parser.add_argument(\'--epochs\', help=""Please give a value for epochs"")\n    parser.add_argument(\'--batch_size\', help=""Please give a value for batch_size"")\n    parser.add_argument(\'--init_lr\', help=""Please give a value for init_lr"")\n    parser.add_argument(\'--lr_reduce_factor\', help=""Please give a value for lr_reduce_factor"")\n    parser.add_argument(\'--lr_schedule_patience\', help=""Please give a value for lr_schedule_patience"")\n    parser.add_argument(\'--min_lr\', help=""Please give a value for min_lr"")\n    parser.add_argument(\'--weight_decay\', help=""Please give a value for weight_decay"")\n    parser.add_argument(\'--print_epoch_interval\', help=""Please give a value for print_epoch_interval"")    \n    parser.add_argument(\'--L\', help=""Please give a value for L"")\n    parser.add_argument(\'--hidden_dim\', help=""Please give a value for hidden_dim"")\n    parser.add_argument(\'--out_dim\', help=""Please give a value for out_dim"")\n    parser.add_argument(\'--residual\', help=""Please give a value for residual"")\n    parser.add_argument(\'--edge_feat\', help=""Please give a value for edge_feat"")\n    parser.add_argument(\'--readout\', help=""Please give a value for readout"")\n    parser.add_argument(\'--kernel\', help=""Please give a value for kernel"")\n    parser.add_argument(\'--n_heads\', help=""Please give a value for n_heads"")\n    parser.add_argument(\'--gated\', help=""Please give a value for gated"")\n    parser.add_argument(\'--in_feat_dropout\', help=""Please give a value for in_feat_dropout"")\n    parser.add_argument(\'--dropout\', help=""Please give a value for dropout"")\n    parser.add_argument(\'--graph_norm\', help=""Please give a value for graph_norm"")\n    parser.add_argument(\'--batch_norm\', help=""Please give a value for batch_norm"")\n    parser.add_argument(\'--sage_aggregator\', help=""Please give a value for sage_aggregator"")\n    parser.add_argument(\'--data_mode\', help=""Please give a value for data_mode"")\n    parser.add_argument(\'--num_pool\', help=""Please give a value for num_pool"")\n    parser.add_argument(\'--gnn_per_block\', help=""Please give a value for gnn_per_block"")\n    parser.add_argument(\'--embedding_dim\', help=""Please give a value for embedding_dim"")\n    parser.add_argument(\'--pool_ratio\', help=""Please give a value for pool_ratio"")\n    parser.add_argument(\'--linkpred\', help=""Please give a value for linkpred"")\n    parser.add_argument(\'--cat\', help=""Please give a value for cat"")\n    parser.add_argument(\'--self_loop\', help=""Please give a value for self_loop"")\n    parser.add_argument(\'--max_time\', help=""Please give a value for max_time"")\n    args = parser.parse_args()\n    with open(args.config) as f:\n        config = json.load(f)\n        \n    # device\n    if args.gpu_id is not None:\n        config[\'gpu\'][\'id\'] = int(args.gpu_id)\n        config[\'gpu\'][\'use\'] = True\n    device = gpu_setup(config[\'gpu\'][\'use\'], config[\'gpu\'][\'id\'])\n    # model, dataset, out_dir\n    if args.model is not None:\n        MODEL_NAME = args.model\n    else:\n        MODEL_NAME = config[\'model\']\n    if args.dataset is not None:\n        DATASET_NAME = args.dataset\n    else:\n        DATASET_NAME = config[\'dataset\']\n    dataset = LoadData(DATASET_NAME)\n    if args.out_dir is not None:\n        out_dir = args.out_dir\n    else:\n        out_dir = config[\'out_dir\']\n    # parameters\n    params = config[\'params\']\n    if args.seed is not None:\n        params[\'seed\'] = int(args.seed)\n    if args.epochs is not None:\n        params[\'epochs\'] = int(args.epochs)\n    if args.batch_size is not None:\n        params[\'batch_size\'] = int(args.batch_size)\n    if args.init_lr is not None:\n        params[\'init_lr\'] = float(args.init_lr)\n    if args.lr_reduce_factor is not None:\n        params[\'lr_reduce_factor\'] = float(args.lr_reduce_factor)\n    if args.lr_schedule_patience is not None:\n        params[\'lr_schedule_patience\'] = int(args.lr_schedule_patience)\n    if args.min_lr is not None:\n        params[\'min_lr\'] = float(args.min_lr)\n    if args.weight_decay is not None:\n        params[\'weight_decay\'] = float(args.weight_decay)\n    if args.print_epoch_interval is not None:\n        params[\'print_epoch_interval\'] = int(args.print_epoch_interval)\n    if args.max_time is not None:\n        params[\'max_time\'] = float(args.max_time)\n    # network parameters\n    net_params = config[\'net_params\']\n    net_params[\'device\'] = device\n    net_params[\'gpu_id\'] = config[\'gpu\'][\'id\']\n    net_params[\'batch_size\'] = params[\'batch_size\']\n    if args.L is not None:\n        net_params[\'L\'] = int(args.L)\n    if args.hidden_dim is not None:\n        net_params[\'hidden_dim\'] = int(args.hidden_dim)\n    if args.out_dim is not None:\n        net_params[\'out_dim\'] = int(args.out_dim)   \n    if args.residual is not None:\n        net_params[\'residual\'] = True if args.residual==\'True\' else False\n    if args.edge_feat is not None:\n        net_params[\'edge_feat\'] = True if args.edge_feat==\'True\' else False\n    if args.readout is not None:\n        net_params[\'readout\'] = args.readout\n    if args.kernel is not None:\n        net_params[\'kernel\'] = int(args.kernel)\n    if args.n_heads is not None:\n        net_params[\'n_heads\'] = int(args.n_heads)\n    if args.gated is not None:\n        net_params[\'gated\'] = True if args.gated==\'True\' else False\n    if args.in_feat_dropout is not None:\n        net_params[\'in_feat_dropout\'] = float(args.in_feat_dropout)\n    if args.dropout is not None:\n        net_params[\'dropout\'] = float(args.dropout)\n    if args.graph_norm is not None:\n        net_params[\'graph_norm\'] = True if args.graph_norm==\'True\' else False\n    if args.batch_norm is not None:\n        net_params[\'batch_norm\'] = True if args.batch_norm==\'True\' else False\n    if args.sage_aggregator is not None:\n        net_params[\'sage_aggregator\'] = args.sage_aggregator\n    if args.data_mode is not None:\n        net_params[\'data_mode\'] = args.data_mode\n    if args.num_pool is not None:\n        net_params[\'num_pool\'] = int(args.num_pool)\n    if args.gnn_per_block is not None:\n        net_params[\'gnn_per_block\'] = int(args.gnn_per_block)\n    if args.embedding_dim is not None:\n        net_params[\'embedding_dim\'] = int(args.embedding_dim)\n    if args.pool_ratio is not None:\n        net_params[\'pool_ratio\'] = float(args.pool_ratio)\n    if args.linkpred is not None:\n        net_params[\'linkpred\'] = True if args.linkpred==\'True\' else False\n    if args.cat is not None:\n        net_params[\'cat\'] = True if args.cat==\'True\' else False\n    if args.self_loop is not None:\n        net_params[\'self_loop\'] = True if args.self_loop==\'True\' else False\n    if args.builtin is not None:\n        net_params[\'builtin\'] = True if args.builtin == \'True\' else False\n        \n    # CitationGraph \n    net_params[\'in_dim\'] = dataset.num_dims # node_dim (feat is an integer)\n    net_params[\'n_classes\'] = dataset.num_classes\n\n\n    if MODEL_NAME in [\'MLP\', \'MLP_GATED\']:\n        builtin = \'\'\n    else:\n        builtin = \'DGL\' if net_params[\'builtin\'] else \'Custom\'\n    root_log_dir = out_dir + \'logs/\' + MODEL_NAME + ""_"" + DATASET_NAME + ""_GPU"" + str(config[\'gpu\'][\'id\']) + ""_"" + time.strftime(\'%Hh%Mm%Ss_on_%b_%d_%Y\') + builtin\n    root_ckpt_dir = out_dir + \'checkpoints/\' + MODEL_NAME + ""_"" + DATASET_NAME + ""_GPU"" + str(config[\'gpu\'][\'id\']) + ""_"" + time.strftime(\'%Hh%Mm%Ss_on_%b_%d_%Y\') + builtin\n    write_file_name = out_dir + \'results/result_\' + MODEL_NAME + ""_"" + DATASET_NAME + ""_GPU"" + str(config[\'gpu\'][\'id\']) + ""_"" + time.strftime(\'%Hh%Mm%Ss_on_%b_%d_%Y\') + builtin\n    write_config_file = out_dir + \'configs/config_\' + MODEL_NAME + ""_"" + DATASET_NAME + ""_GPU"" + str(config[\'gpu\'][\'id\']) + ""_"" + time.strftime(\'%Hh%Mm%Ss_on_%b_%d_%Y\') + builtin\n    dirs = root_log_dir, root_ckpt_dir, write_file_name, write_config_file\n\n    if not os.path.exists(out_dir + \'results\'):\n        os.makedirs(out_dir + \'results\')\n        \n    if not os.path.exists(out_dir + \'configs\'):\n        os.makedirs(out_dir + \'configs\')\n\n    net_params[\'total_param\'] = view_model_param(MODEL_NAME, net_params)\n    train_val_pipeline(MODEL_NAME, dataset, params, net_params, dirs)\n\n    \n    \n    \n    \n    \n    \nmain()    \n\n\n\n\n\n\n'"
main_SBMs_node_classification.py,13,"b'\n\n\n\n\n""""""\n    IMPORTING LIBS\n""""""\nimport dgl\n\nimport numpy as np\nimport os\nimport socket\nimport time\nimport random\nimport glob\nimport argparse, json\nimport pickle\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\n\nfrom tensorboardX import SummaryWriter\nfrom tqdm import tqdm\n\nclass DotDict(dict):\n    def __init__(self, **kwds):\n        self.update(kwds)\n        self.__dict__ = self\n\n\n\n\n\n\n""""""\n    IMPORTING CUSTOM MODULES/METHODS\n""""""\n\nfrom nets.SBMs_node_classification.load_net import gnn_model # import GNNs\nfrom data.data import LoadData # import dataset\nfrom train.train_SBMs_node_classification import train_epoch, evaluate_network # import train functions\n\n\n\n\n""""""\n    GPU Setup\n""""""\ndef gpu_setup(use_gpu, gpu_id):\n    os.environ[""CUDA_DEVICE_ORDER""] = ""PCI_BUS_ID""\n    os.environ[""CUDA_VISIBLE_DEVICES""] = str(gpu_id)  \n\n    if torch.cuda.is_available() and use_gpu:\n        print(\'cuda available with GPU:\',torch.cuda.get_device_name(0))\n        device = torch.device(""cuda"")\n    else:\n        print(\'cuda not available\')\n        device = torch.device(""cpu"")\n    return device\n\n\n\n\n\n\n\n\n\n\n""""""\n    VIEWING MODEL CONFIG AND PARAMS\n""""""\ndef view_model_param(MODEL_NAME, net_params):\n    model = gnn_model(MODEL_NAME, net_params)\n    total_param = 0\n    print(""MODEL DETAILS:\\n"")\n    #print(model)\n    for param in model.parameters():\n        # print(param.data.size())\n        total_param += np.prod(list(param.data.size()))\n    print(\'MODEL/Total parameters:\', MODEL_NAME, total_param)\n    return total_param\n\n\n""""""\n    TRAINING CODE\n""""""\n\ndef train_val_pipeline(MODEL_NAME, dataset, params, net_params, dirs):\n    \n    start0 = time.time()\n    per_epoch_time = []\n    \n    DATASET_NAME = dataset.name\n    \n    if MODEL_NAME in [\'GCN\', \'GAT\']:\n        if net_params[\'self_loop\']:\n            print(""[!] Adding graph self-loops for GCN/GAT models (central node trick)."")\n            dataset._add_self_loops()\n    \n    trainset, valset, testset = dataset.train, dataset.val, dataset.test\n        \n    root_log_dir, root_ckpt_dir, write_file_name, write_config_file = dirs\n    device = net_params[\'device\']\n    \n    # Write network and optimization hyper-parameters in folder config/\n    with open(write_config_file + \'.txt\', \'w\') as f:\n        f.write(""""""Dataset: {},\\nModel: {}\\n\\nparams={}\\n\\nnet_params={}\\n\\n\\nTotal Parameters: {}\\n\\n""""""                .format(DATASET_NAME, MODEL_NAME, params, net_params, net_params[\'total_param\']))\n        \n    log_dir = os.path.join(root_log_dir, ""RUN_"" + str(0))\n    writer = SummaryWriter(log_dir=log_dir)\n\n    # setting seeds\n    random.seed(params[\'seed\'])\n    np.random.seed(params[\'seed\'])\n    torch.manual_seed(params[\'seed\'])\n    if device == \'cuda\':\n        torch.cuda.manual_seed(params[\'seed\'])\n    \n    print(""Training Graphs: "", len(trainset))\n    print(""Validation Graphs: "", len(valset))\n    print(""Test Graphs: "", len(testset))\n    print(""Number of Classes: "", net_params[\'n_classes\'])\n\n    model = gnn_model(MODEL_NAME, net_params)\n    model = model.to(device)\n\n    optimizer = optim.Adam(model.parameters(), lr=params[\'init_lr\'], weight_decay=params[\'weight_decay\'])\n    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\'min\',\n                                                     factor=params[\'lr_reduce_factor\'],\n                                                     patience=params[\'lr_schedule_patience\'],\n                                                     verbose=True)\n    \n    epoch_train_losses, epoch_val_losses = [], []\n    epoch_train_accs, epoch_val_accs = [], [] \n    \n    train_loader = DataLoader(trainset, batch_size=params[\'batch_size\'], shuffle=True, collate_fn=dataset.collate)\n    val_loader = DataLoader(valset, batch_size=params[\'batch_size\'], shuffle=False, collate_fn=dataset.collate)\n    test_loader = DataLoader(testset, batch_size=params[\'batch_size\'], shuffle=False, collate_fn=dataset.collate)\n        \n    # At any point you can hit Ctrl + C to break out of training early.\n    try:\n        with tqdm(range(params[\'epochs\'])) as t:\n            for epoch in t:\n\n                t.set_description(\'Epoch %d\' % epoch)\n\n                start = time.time()\n\n                epoch_train_loss, epoch_train_acc, optimizer = train_epoch(model, optimizer, device, train_loader, epoch)\n                epoch_val_loss, epoch_val_acc = evaluate_network(model, device, val_loader, epoch)\n                epoch_test_loss, epoch_test_acc = evaluate_network(model, device, test_loader, epoch)\n\n                epoch_train_losses.append(epoch_train_loss)\n                epoch_val_losses.append(epoch_val_loss)\n                epoch_train_accs.append(epoch_train_acc)\n                epoch_val_accs.append(epoch_val_acc)\n\n                writer.add_scalar(\'train/_loss\', epoch_train_loss, epoch)\n                writer.add_scalar(\'val/_loss\', epoch_val_loss, epoch)\n                writer.add_scalar(\'train/_acc\', epoch_train_acc, epoch)\n                writer.add_scalar(\'val/_acc\', epoch_val_acc, epoch)\n                writer.add_scalar(\'learning_rate\', optimizer.param_groups[0][\'lr\'], epoch)\n\n                _, epoch_test_acc = evaluate_network(model, device, test_loader, epoch)        \n                t.set_postfix(time=time.time()-start, lr=optimizer.param_groups[0][\'lr\'],\n                              train_loss=epoch_train_loss, val_loss=epoch_val_loss,\n                              train_acc=epoch_train_acc, val_acc=epoch_val_acc,\n                              test_acc=epoch_test_acc)\n\n                per_epoch_time.append(time.time()-start)\n\n                # Saving checkpoint\n                ckpt_dir = os.path.join(root_ckpt_dir, ""RUN_"")\n                if not os.path.exists(ckpt_dir):\n                    os.makedirs(ckpt_dir)\n                torch.save(model.state_dict(), \'{}.pkl\'.format(ckpt_dir + ""/epoch_"" + str(epoch)))\n\n                files = glob.glob(ckpt_dir + \'/*.pkl\')\n                for file in files:\n                    epoch_nb = file.split(\'_\')[-1]\n                    epoch_nb = int(epoch_nb.split(\'.\')[0])\n                    if epoch_nb < epoch-1:\n                        os.remove(file)\n\n                scheduler.step(epoch_val_loss)\n\n                if optimizer.param_groups[0][\'lr\'] < params[\'min_lr\']:\n                    print(""\\n!! LR SMALLER OR EQUAL TO MIN LR THRESHOLD."")\n                    break\n                    \n                # Stop training after params[\'max_time\'] hours\n                if time.time()-start0 > params[\'max_time\']*3600:\n                    print(\'-\' * 89)\n                    print(""Max_time for training elapsed {:.2f} hours, so stopping"".format(params[\'max_time\']))\n                    break\n    \n    except KeyboardInterrupt:\n        print(\'-\' * 89)\n        print(\'Exiting from training early because of KeyboardInterrupt\')\n    \n    \n    _, test_acc = evaluate_network(model, device, test_loader, epoch)\n    _, train_acc = evaluate_network(model, device, train_loader, epoch)\n    print(""Test Accuracy: {:.4f}"".format(test_acc))\n    print(""Train Accuracy: {:.4f}"".format(train_acc))\n    print(""TOTAL TIME TAKEN: {:.4f}s"".format(time.time()-start0))\n    print(""AVG TIME PER EPOCH: {:.4f}s"".format(np.mean(per_epoch_time)))\n\n    writer.close()\n\n    """"""\n        Write the results in out_dir/results folder\n    """"""\n    with open(write_file_name + \'.txt\', \'w\') as f:\n        f.write(""""""Dataset: {},\\nModel: {}\\n\\nparams={}\\n\\nnet_params={}\\n\\n{}\\n\\nTotal Parameters: {}\\n\\n\n    FINAL RESULTS\\nTEST ACCURACY: {:.4f}\\nTRAIN ACCURACY: {:.4f}\\n\\n\n    Total Time Taken: {:.4f} hrs\\nAverage Time Per Epoch: {:.4f} s\\n\\n\\n""""""\\\n          .format(DATASET_NAME, MODEL_NAME, params, net_params, model, net_params[\'total_param\'],\n                  test_acc, train_acc, (time.time()-start0)/3600, np.mean(per_epoch_time)))\n\n        \n    # send results to gmail\n    try:\n        from gmail import send\n        subject = \'Result for Dataset: {}, Model: {}\'.format(DATASET_NAME, MODEL_NAME)\n        body = """"""Dataset: {},\\nModel: {}\\n\\nparams={}\\n\\nnet_params={}\\n\\n{}\\n\\nTotal Parameters: {}\\n\\n\n    FINAL RESULTS\\nTEST ACCURACY: {:.4f}\\nTRAIN ACCURACY: {:.4f}\\n\\n\n    Total Time Taken: {:.4f} hrs\\nAverage Time Per Epoch: {:.4f} s\\n\\n\\n""""""\\\n          .format(DATASET_NAME, MODEL_NAME, params, net_params, model, net_params[\'total_param\'],\n                  test_acc, train_acc, (time.time()-start0)/3600, np.mean(per_epoch_time))\n        send(subject, body)\n    except:\n        pass\n        \n\n\n\n\ndef main():    \n    """"""\n        USER CONTROLS\n    """"""\n    \n    \n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'--config\', help=""Please give a config.json file with training/model/data/param details"")\n    parser.add_argument(\'--gpu_id\', help=""Please give a value for gpu id"")\n    parser.add_argument(\'--model\', help=""Please give a value for model name"")\n    parser.add_argument(\'--dataset\', help=""Please give a value for dataset name"")\n    parser.add_argument(\'--out_dir\', help=""Please give a value for out_dir"")\n    parser.add_argument(\'--seed\', help=""Please give a value for seed"")\n    parser.add_argument(\'--epochs\', help=""Please give a value for epochs"")\n    parser.add_argument(\'--batch_size\', help=""Please give a value for batch_size"")\n    parser.add_argument(\'--init_lr\', help=""Please give a value for init_lr"")\n    parser.add_argument(\'--lr_reduce_factor\', help=""Please give a value for lr_reduce_factor"")\n    parser.add_argument(\'--lr_schedule_patience\', help=""Please give a value for lr_schedule_patience"")\n    parser.add_argument(\'--min_lr\', help=""Please give a value for min_lr"")\n    parser.add_argument(\'--weight_decay\', help=""Please give a value for weight_decay"")\n    parser.add_argument(\'--print_epoch_interval\', help=""Please give a value for print_epoch_interval"")    \n    parser.add_argument(\'--L\', help=""Please give a value for L"")\n    parser.add_argument(\'--hidden_dim\', help=""Please give a value for hidden_dim"")\n    parser.add_argument(\'--out_dim\', help=""Please give a value for out_dim"")\n    parser.add_argument(\'--residual\', help=""Please give a value for residual"")\n    parser.add_argument(\'--edge_feat\', help=""Please give a value for edge_feat"")\n    parser.add_argument(\'--readout\', help=""Please give a value for readout"")\n    parser.add_argument(\'--kernel\', help=""Please give a value for kernel"")\n    parser.add_argument(\'--n_heads\', help=""Please give a value for n_heads"")\n    parser.add_argument(\'--gated\', help=""Please give a value for gated"")\n    parser.add_argument(\'--in_feat_dropout\', help=""Please give a value for in_feat_dropout"")\n    parser.add_argument(\'--dropout\', help=""Please give a value for dropout"")\n    parser.add_argument(\'--graph_norm\', help=""Please give a value for graph_norm"")\n    parser.add_argument(\'--batch_norm\', help=""Please give a value for batch_norm"")\n    parser.add_argument(\'--sage_aggregator\', help=""Please give a value for sage_aggregator"")\n    parser.add_argument(\'--data_mode\', help=""Please give a value for data_mode"")\n    parser.add_argument(\'--num_pool\', help=""Please give a value for num_pool"")\n    parser.add_argument(\'--gnn_per_block\', help=""Please give a value for gnn_per_block"")\n    parser.add_argument(\'--embedding_dim\', help=""Please give a value for embedding_dim"")\n    parser.add_argument(\'--pool_ratio\', help=""Please give a value for pool_ratio"")\n    parser.add_argument(\'--linkpred\', help=""Please give a value for linkpred"")\n    parser.add_argument(\'--cat\', help=""Please give a value for cat"")\n    parser.add_argument(\'--self_loop\', help=""Please give a value for self_loop"")\n    parser.add_argument(\'--max_time\', help=""Please give a value for max_time"")\n    args = parser.parse_args()\n    with open(args.config) as f:\n        config = json.load(f)\n        \n    # device\n    if args.gpu_id is not None:\n        config[\'gpu\'][\'id\'] = int(args.gpu_id)\n        config[\'gpu\'][\'use\'] = True\n    device = gpu_setup(config[\'gpu\'][\'use\'], config[\'gpu\'][\'id\'])\n    # model, dataset, out_dir\n    if args.model is not None:\n        MODEL_NAME = args.model\n    else:\n        MODEL_NAME = config[\'model\']\n    if args.dataset is not None:\n        DATASET_NAME = args.dataset\n    else:\n        DATASET_NAME = config[\'dataset\']\n    dataset = LoadData(DATASET_NAME)\n    if args.out_dir is not None:\n        out_dir = args.out_dir\n    else:\n        out_dir = config[\'out_dir\']\n    # parameters\n    params = config[\'params\']\n    if args.seed is not None:\n        params[\'seed\'] = int(args.seed)\n    if args.epochs is not None:\n        params[\'epochs\'] = int(args.epochs)\n    if args.batch_size is not None:\n        params[\'batch_size\'] = int(args.batch_size)\n    if args.init_lr is not None:\n        params[\'init_lr\'] = float(args.init_lr)\n    if args.lr_reduce_factor is not None:\n        params[\'lr_reduce_factor\'] = float(args.lr_reduce_factor)\n    if args.lr_schedule_patience is not None:\n        params[\'lr_schedule_patience\'] = int(args.lr_schedule_patience)\n    if args.min_lr is not None:\n        params[\'min_lr\'] = float(args.min_lr)\n    if args.weight_decay is not None:\n        params[\'weight_decay\'] = float(args.weight_decay)\n    if args.print_epoch_interval is not None:\n        params[\'print_epoch_interval\'] = int(args.print_epoch_interval)\n    if args.max_time is not None:\n        params[\'max_time\'] = float(args.max_time)\n    # network parameters\n    net_params = config[\'net_params\']\n    net_params[\'device\'] = device\n    net_params[\'gpu_id\'] = config[\'gpu\'][\'id\']\n    net_params[\'batch_size\'] = params[\'batch_size\']\n    if args.L is not None:\n        net_params[\'L\'] = int(args.L)\n    if args.hidden_dim is not None:\n        net_params[\'hidden_dim\'] = int(args.hidden_dim)\n    if args.out_dim is not None:\n        net_params[\'out_dim\'] = int(args.out_dim)   \n    if args.residual is not None:\n        net_params[\'residual\'] = True if args.residual==\'True\' else False\n    if args.edge_feat is not None:\n        net_params[\'edge_feat\'] = True if args.edge_feat==\'True\' else False\n    if args.readout is not None:\n        net_params[\'readout\'] = args.readout\n    if args.kernel is not None:\n        net_params[\'kernel\'] = int(args.kernel)\n    if args.n_heads is not None:\n        net_params[\'n_heads\'] = int(args.n_heads)\n    if args.gated is not None:\n        net_params[\'gated\'] = True if args.gated==\'True\' else False\n    if args.in_feat_dropout is not None:\n        net_params[\'in_feat_dropout\'] = float(args.in_feat_dropout)\n    if args.dropout is not None:\n        net_params[\'dropout\'] = float(args.dropout)\n    if args.graph_norm is not None:\n        net_params[\'graph_norm\'] = True if args.graph_norm==\'True\' else False\n    if args.batch_norm is not None:\n        net_params[\'batch_norm\'] = True if args.batch_norm==\'True\' else False\n    if args.sage_aggregator is not None:\n        net_params[\'sage_aggregator\'] = args.sage_aggregator\n    if args.data_mode is not None:\n        net_params[\'data_mode\'] = args.data_mode\n    if args.num_pool is not None:\n        net_params[\'num_pool\'] = int(args.num_pool)\n    if args.gnn_per_block is not None:\n        net_params[\'gnn_per_block\'] = int(args.gnn_per_block)\n    if args.embedding_dim is not None:\n        net_params[\'embedding_dim\'] = int(args.embedding_dim)\n    if args.pool_ratio is not None:\n        net_params[\'pool_ratio\'] = float(args.pool_ratio)\n    if args.linkpred is not None:\n        net_params[\'linkpred\'] = True if args.linkpred==\'True\' else False\n    if args.cat is not None:\n        net_params[\'cat\'] = True if args.cat==\'True\' else False\n    if args.self_loop is not None:\n        net_params[\'self_loop\'] = True if args.self_loop==\'True\' else False\n        \n    # SBM\n    net_params[\'in_dim\'] = torch.unique(dataset.train[0][0].ndata[\'feat\'],dim=0).size(0) # node_dim (feat is an integer)\n    net_params[\'n_classes\'] = torch.unique(dataset.train[0][1],dim=0).size(0)\n\n    root_log_dir = out_dir + \'logs/\' + MODEL_NAME + ""_"" + DATASET_NAME + ""_GPU"" + str(config[\'gpu\'][\'id\']) + ""_"" + time.strftime(\'%Hh%Mm%Ss_on_%b_%d_%Y\')\n    root_ckpt_dir = out_dir + \'checkpoints/\' + MODEL_NAME + ""_"" + DATASET_NAME + ""_GPU"" + str(config[\'gpu\'][\'id\']) + ""_"" + time.strftime(\'%Hh%Mm%Ss_on_%b_%d_%Y\')\n    write_file_name = out_dir + \'results/result_\' + MODEL_NAME + ""_"" + DATASET_NAME + ""_GPU"" + str(config[\'gpu\'][\'id\']) + ""_"" + time.strftime(\'%Hh%Mm%Ss_on_%b_%d_%Y\')\n    write_config_file = out_dir + \'configs/config_\' + MODEL_NAME + ""_"" + DATASET_NAME + ""_GPU"" + str(config[\'gpu\'][\'id\']) + ""_"" + time.strftime(\'%Hh%Mm%Ss_on_%b_%d_%Y\')\n    dirs = root_log_dir, root_ckpt_dir, write_file_name, write_config_file\n\n    if not os.path.exists(out_dir + \'results\'):\n        os.makedirs(out_dir + \'results\')\n        \n    if not os.path.exists(out_dir + \'configs\'):\n        os.makedirs(out_dir + \'configs\')\n\n    net_params[\'total_param\'] = view_model_param(MODEL_NAME, net_params)\n    train_val_pipeline(MODEL_NAME, dataset, params, net_params, dirs)\n\n    \n    \n    \n    \n    \n    \nmain()    \n\n\n\n\n\n\n'"
main_TSP_edge_classification.py,11,"b'\n\n\n\n\n""""""\n    IMPORTING LIBS\n""""""\nimport dgl\n\nimport numpy as np\nimport os\nimport socket\nimport time\nimport random\nimport glob\nimport argparse, json\nimport pickle\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\n\nfrom tensorboardX import SummaryWriter\nfrom tqdm import tqdm\n\nclass DotDict(dict):\n    def __init__(self, **kwds):\n        self.update(kwds)\n        self.__dict__ = self\n        \n\n\n\n\n\n\n""""""\n    IMPORTING CUSTOM MODULES/METHODS\n""""""\nfrom nets.TSP_edge_classification.load_net import gnn_model # import all GNNS\nfrom data.data import LoadData # import dataset\nfrom train.train_TSP_edge_classification import train_epoch, evaluate_network # import train functions\n\n\n\n\n""""""\n    GPU Setup\n""""""\ndef gpu_setup(use_gpu, gpu_id):\n    os.environ[""CUDA_DEVICE_ORDER""] = ""PCI_BUS_ID""\n    os.environ[""CUDA_VISIBLE_DEVICES""] = str(gpu_id)  \n\n    if torch.cuda.is_available() and use_gpu:\n        print(\'cuda available with GPU:\',torch.cuda.get_device_name(0))\n        device = torch.device(""cuda"")\n    else:\n        print(\'cuda not available\')\n        device = torch.device(""cpu"")\n    return device\n\n\n\n\n\n\n\n\n\n\n""""""\n    VIEWING MODEL CONFIG AND PARAMS\n""""""\ndef view_model_param(MODEL_NAME, net_params):\n    model = gnn_model(MODEL_NAME, net_params)\n    total_param = 0\n    print(""MODEL DETAILS:\\n"")\n    #print(model)\n    for param in model.parameters():\n        # print(param.data.size())\n        total_param += np.prod(list(param.data.size()))\n    print(\'MODEL/Total parameters:\', MODEL_NAME, total_param)\n    return total_param\n\n\n""""""\n    TRAINING CODE\n""""""\n\ndef train_val_pipeline(MODEL_NAME, dataset, params, net_params, dirs):\n    t0 = time.time()\n    per_epoch_time = []\n        \n    DATASET_NAME = dataset.name\n    \n    #assert net_params[\'self_loop\'] == False, ""No self-loop support for %s dataset"" % DATASET_NAME\n    \n    trainset, valset, testset = dataset.train, dataset.val, dataset.test\n        \n    root_log_dir, root_ckpt_dir, write_file_name, write_config_file = dirs\n    device = net_params[\'device\']\n    \n    # Write the network and optimization hyper-parameters in folder config/\n    with open(write_config_file + \'.txt\', \'w\') as f:\n        f.write(""""""Dataset: {},\\nModel: {}\\n\\nparams={}\\n\\nnet_params={}\\n\\n\\nTotal Parameters: {}\\n\\n""""""                .format(DATASET_NAME, MODEL_NAME, params, net_params, net_params[\'total_param\']))\n        \n    log_dir = os.path.join(root_log_dir, ""RUN_"" + str(0))\n    writer = SummaryWriter(log_dir=log_dir)\n\n    # setting seeds\n    random.seed(params[\'seed\'])\n    np.random.seed(params[\'seed\'])\n    torch.manual_seed(params[\'seed\'])\n    if device == \'cuda\':\n        torch.cuda.manual_seed(params[\'seed\'])\n    \n    print(""Training Graphs: "", len(trainset))\n    print(""Validation Graphs: "", len(valset))\n    print(""Test Graphs: "", len(testset))\n    print(""Number of Classes: "", net_params[\'n_classes\'])\n\n    model = gnn_model(MODEL_NAME, net_params)\n    model = model.to(device)\n\n    optimizer = optim.Adam(model.parameters(), lr=params[\'init_lr\'], weight_decay=params[\'weight_decay\'])\n    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\'min\',\n                                                     factor=params[\'lr_reduce_factor\'],\n                                                     patience=params[\'lr_schedule_patience\'],\n                                                     verbose=True)\n    \n    epoch_train_losses, epoch_val_losses = [], []\n    epoch_train_f1s, epoch_val_f1s = [], [] \n    \n    train_loader = DataLoader(trainset, batch_size=params[\'batch_size\'], shuffle=True, collate_fn=dataset.collate)\n    val_loader = DataLoader(valset, batch_size=params[\'batch_size\'], shuffle=False, collate_fn=dataset.collate)\n    test_loader = DataLoader(testset, batch_size=params[\'batch_size\'], shuffle=False, collate_fn=dataset.collate)\n\n    # At any point you can hit Ctrl + C to break out of training early.\n    try:\n        with tqdm(range(params[\'epochs\'])) as t:\n            for epoch in t:\n\n                t.set_description(\'Epoch %d\' % epoch)    \n\n                start = time.time()\n\n                epoch_train_loss, epoch_train_f1, optimizer = train_epoch(model, optimizer, device, train_loader, epoch)\n                epoch_val_loss, epoch_val_f1 = evaluate_network(model, device, val_loader, epoch)\n\n                epoch_train_losses.append(epoch_train_loss)\n                epoch_val_losses.append(epoch_val_loss)\n                epoch_train_f1s.append(epoch_train_f1)\n                epoch_val_f1s.append(epoch_val_f1)\n\n                writer.add_scalar(\'train/_loss\', epoch_train_loss, epoch)\n                writer.add_scalar(\'val/_loss\', epoch_val_loss, epoch)\n                writer.add_scalar(\'train/_f1\', epoch_train_f1, epoch)\n                writer.add_scalar(\'val/_f1\', epoch_val_f1, epoch)\n                writer.add_scalar(\'learning_rate\', optimizer.param_groups[0][\'lr\'], epoch)   \n\n                _, epoch_test_f1 = evaluate_network(model, device, test_loader, epoch)                        \n                t.set_postfix(time=time.time()-start, lr=optimizer.param_groups[0][\'lr\'],\n                              train_loss=epoch_train_loss, val_loss=epoch_val_loss,\n                              train_f1=epoch_train_f1, val_f1=epoch_val_f1,\n                              test_f1=epoch_test_f1) \n\n                per_epoch_time.append(time.time()-start)\n\n                # Saving checkpoint\n                ckpt_dir = os.path.join(root_ckpt_dir, ""RUN_"")\n                if not os.path.exists(ckpt_dir):\n                    os.makedirs(ckpt_dir)\n                torch.save(model.state_dict(), \'{}.pkl\'.format(ckpt_dir + ""/epoch_"" + str(epoch)))\n\n                files = glob.glob(ckpt_dir + \'/*.pkl\')\n                for file in files:\n                    epoch_nb = file.split(\'_\')[-1]\n                    epoch_nb = int(epoch_nb.split(\'.\')[0])\n                    if epoch_nb < epoch-1:\n                        os.remove(file)\n\n                scheduler.step(epoch_val_loss)\n\n                if optimizer.param_groups[0][\'lr\'] < params[\'min_lr\']:\n                    print(""\\n!! LR EQUAL TO MIN LR SET."")\n                    break\n                    \n                # Stop training after params[\'max_time\'] hours\n                if time.time()-t0 > params[\'max_time\']*3600:\n                    print(\'-\' * 89)\n                    print(""Max_time for training elapsed {:.2f} hours, so stopping"".format(params[\'max_time\']))\n                    break\n    \n    except KeyboardInterrupt:\n        print(\'-\' * 89)\n        print(\'Exiting from training early because of KeyboardInterrupt\')\n    \n    _, test_f1 = evaluate_network(model, device, test_loader, epoch)\n    _, train_f1 = evaluate_network(model, device, train_loader, epoch)\n    print(""Test F1: {:.4f}"".format(test_f1))\n    print(""Train F1: {:.4f}"".format(train_f1))\n    print(""TOTAL TIME TAKEN: {:.4f}s"".format(time.time()-t0))\n    print(""AVG TIME PER EPOCH: {:.4f}s"".format(np.mean(per_epoch_time)))\n\n    writer.close()\n\n    """"""\n        Write the results in out_dir/results folder\n    """"""\n    with open(write_file_name + \'.txt\', \'w\') as f:\n        f.write(""""""Dataset: {},\\nModel: {}\\n\\nparams={}\\n\\nnet_params={}\\n\\n{}\\n\\nTotal Parameters: {}\\n\\n\n    FINAL RESULTS\\nTEST F1: {:.4f}\\nTRAIN F1: {:.4f}\\n\\n\n    Total Time Taken: {:.4f}hrs\\nAverage Time Per Epoch: {:.4f}s\\n\\n\\n""""""\\\n          .format(DATASET_NAME, MODEL_NAME, params, net_params, model, net_params[\'total_param\'],\n                  np.mean(np.array(test_f1)), np.mean(np.array(train_f1)), (time.time()-t0)/3600, np.mean(per_epoch_time)))\n        \n\n    # send results to gmail\n    try:\n        from gmail import send\n        subject = \'Result for Dataset: {}, Model: {}\'.format(DATASET_NAME, MODEL_NAME)\n        body = """"""Dataset: {},\\nModel: {}\\n\\nparams={}\\n\\nnet_params={}\\n\\n{}\\n\\nTotal Parameters: {}\\n\\n\n    FINAL RESULTS\\nTEST F1: {:.4f}\\nTRAIN F1: {:.4f}\\n\\n\n    Total Time Taken: {:.4f}hrs\\nAverage Time Per Epoch: {:.4f}s\\n\\n\\n""""""\\\n          .format(DATASET_NAME, MODEL_NAME, params, net_params, model, net_params[\'total_param\'],\n                  np.mean(np.array(test_f1)), np.mean(np.array(train_f1)), (time.time()-t0)/3600, np.mean(per_epoch_time))\n        send(subject, body)\n    except:\n        pass\n    \n\n\n\n\ndef main():    \n    """"""\n        USER CONTROLS\n    """"""\n    \n    \n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'--config\', help=""Please give a config.json file with training/model/data/param details"")\n    parser.add_argument(\'--gpu_id\', help=""Please give a value for gpu id"")\n    parser.add_argument(\'--model\', help=""Please give a value for model name"")\n    parser.add_argument(\'--dataset\', help=""Please give a value for dataset name"")\n    parser.add_argument(\'--out_dir\', help=""Please give a value for out_dir"")\n    parser.add_argument(\'--seed\', help=""Please give a value for seed"")\n    parser.add_argument(\'--epochs\', help=""Please give a value for epochs"")\n    parser.add_argument(\'--batch_size\', help=""Please give a value for batch_size"")\n    parser.add_argument(\'--init_lr\', help=""Please give a value for init_lr"")\n    parser.add_argument(\'--lr_reduce_factor\', help=""Please give a value for lr_reduce_factor"")\n    parser.add_argument(\'--lr_schedule_patience\', help=""Please give a value for lr_schedule_patience"")\n    parser.add_argument(\'--min_lr\', help=""Please give a value for min_lr"")\n    parser.add_argument(\'--weight_decay\', help=""Please give a value for weight_decay"")\n    parser.add_argument(\'--print_epoch_interval\', help=""Please give a value for print_epoch_interval"")    \n    parser.add_argument(\'--L\', help=""Please give a value for L"")\n    parser.add_argument(\'--hidden_dim\', help=""Please give a value for hidden_dim"")\n    parser.add_argument(\'--out_dim\', help=""Please give a value for out_dim"")\n    parser.add_argument(\'--residual\', help=""Please give a value for residual"")\n    parser.add_argument(\'--edge_feat\', help=""Please give a value for edge_feat"")\n    parser.add_argument(\'--readout\', help=""Please give a value for readout"")\n    parser.add_argument(\'--kernel\', help=""Please give a value for kernel"")\n    parser.add_argument(\'--n_heads\', help=""Please give a value for n_heads"")\n    parser.add_argument(\'--gated\', help=""Please give a value for gated"")\n    parser.add_argument(\'--in_feat_dropout\', help=""Please give a value for in_feat_dropout"")\n    parser.add_argument(\'--dropout\', help=""Please give a value for dropout"")\n    parser.add_argument(\'--graph_norm\', help=""Please give a value for graph_norm"")\n    parser.add_argument(\'--batch_norm\', help=""Please give a value for batch_norm"")\n    parser.add_argument(\'--sage_aggregator\', help=""Please give a value for sage_aggregator"")\n    parser.add_argument(\'--data_mode\', help=""Please give a value for data_mode"")\n    parser.add_argument(\'--num_pool\', help=""Please give a value for num_pool"")\n    parser.add_argument(\'--gnn_per_block\', help=""Please give a value for gnn_per_block"")\n    parser.add_argument(\'--embedding_dim\', help=""Please give a value for embedding_dim"")\n    parser.add_argument(\'--pool_ratio\', help=""Please give a value for pool_ratio"")\n    parser.add_argument(\'--linkpred\', help=""Please give a value for linkpred"")\n    parser.add_argument(\'--cat\', help=""Please give a value for cat"")\n    parser.add_argument(\'--self_loop\', help=""Please give a value for self_loop"")\n    parser.add_argument(\'--max_time\', help=""Please give a value for max_time"")\n    args = parser.parse_args()\n    with open(args.config) as f:\n        config = json.load(f)\n        \n    # device\n    if args.gpu_id is not None:\n        config[\'gpu\'][\'id\'] = int(args.gpu_id)\n        config[\'gpu\'][\'use\'] = True\n    device = gpu_setup(config[\'gpu\'][\'use\'], config[\'gpu\'][\'id\'])\n    # model, dataset, out_dir\n    if args.model is not None:\n        MODEL_NAME = args.model\n    else:\n        MODEL_NAME = config[\'model\']\n    if args.dataset is not None:\n        DATASET_NAME = args.dataset\n    else:\n        DATASET_NAME = config[\'dataset\']\n    dataset = LoadData(DATASET_NAME)\n    if args.out_dir is not None:\n        out_dir = args.out_dir\n    else:\n        out_dir = config[\'out_dir\']\n    # parameters\n    params = config[\'params\']\n    if args.seed is not None:\n        params[\'seed\'] = int(args.seed)\n    if args.epochs is not None:\n        params[\'epochs\'] = int(args.epochs)\n    if args.batch_size is not None:\n        params[\'batch_size\'] = int(args.batch_size)\n    if args.init_lr is not None:\n        params[\'init_lr\'] = float(args.init_lr)\n    if args.lr_reduce_factor is not None:\n        params[\'lr_reduce_factor\'] = float(args.lr_reduce_factor)\n    if args.lr_schedule_patience is not None:\n        params[\'lr_schedule_patience\'] = int(args.lr_schedule_patience)\n    if args.min_lr is not None:\n        params[\'min_lr\'] = float(args.min_lr)\n    if args.weight_decay is not None:\n        params[\'weight_decay\'] = float(args.weight_decay)\n    if args.print_epoch_interval is not None:\n        params[\'print_epoch_interval\'] = int(args.print_epoch_interval)\n    if args.max_time is not None:\n        params[\'max_time\'] = float(args.max_time)\n    # network parameters\n    net_params = config[\'net_params\']\n    net_params[\'device\'] = device\n    net_params[\'gpu_id\'] = config[\'gpu\'][\'id\']\n    net_params[\'batch_size\'] = params[\'batch_size\']\n    if args.L is not None:\n        net_params[\'L\'] = int(args.L)\n    if args.hidden_dim is not None:\n        net_params[\'hidden_dim\'] = int(args.hidden_dim)\n    if args.out_dim is not None:\n        net_params[\'out_dim\'] = int(args.out_dim)   \n    if args.residual is not None:\n        net_params[\'residual\'] = True if args.residual==\'True\' else False\n    if args.edge_feat is not None:\n        net_params[\'edge_feat\'] = True if args.edge_feat==\'True\' else False\n    if args.readout is not None:\n        net_params[\'readout\'] = args.readout\n    if args.kernel is not None:\n        net_params[\'kernel\'] = int(args.kernel)\n    if args.n_heads is not None:\n        net_params[\'n_heads\'] = int(args.n_heads)\n    if args.gated is not None:\n        net_params[\'gated\'] = True if args.gated==\'True\' else False\n    if args.in_feat_dropout is not None:\n        net_params[\'in_feat_dropout\'] = float(args.in_feat_dropout)\n    if args.dropout is not None:\n        net_params[\'dropout\'] = float(args.dropout)\n    if args.graph_norm is not None:\n        net_params[\'graph_norm\'] = True if args.graph_norm==\'True\' else False\n    if args.batch_norm is not None:\n        net_params[\'batch_norm\'] = True if args.batch_norm==\'True\' else False\n    if args.sage_aggregator is not None:\n        net_params[\'sage_aggregator\'] = args.sage_aggregator\n    if args.data_mode is not None:\n        net_params[\'data_mode\'] = args.data_mode\n    if args.num_pool is not None:\n        net_params[\'num_pool\'] = int(args.num_pool)\n    if args.gnn_per_block is not None:\n        net_params[\'gnn_per_block\'] = int(args.gnn_per_block)\n    if args.embedding_dim is not None:\n        net_params[\'embedding_dim\'] = int(args.embedding_dim)\n    if args.pool_ratio is not None:\n        net_params[\'pool_ratio\'] = float(args.pool_ratio)\n    if args.linkpred is not None:\n        net_params[\'linkpred\'] = True if args.linkpred==\'True\' else False\n    if args.cat is not None:\n        net_params[\'cat\'] = True if args.cat==\'True\' else False\n    if args.self_loop is not None:\n        net_params[\'self_loop\'] = True if args.self_loop==\'True\' else False\n \n\n      \n    \n    # TSP\n    net_params[\'in_dim\'] = dataset.train[0][0].ndata[\'feat\'][0].shape[0]\n    net_params[\'in_dim_edge\'] = dataset.train[0][0].edata[\'feat\'][0].size(0)\n    num_classes = len(np.unique(np.concatenate(dataset.train[:][1])))\n    net_params[\'n_classes\'] = num_classes\n    \n    root_log_dir = out_dir + \'logs/\' + MODEL_NAME + ""_"" + DATASET_NAME + ""_GPU"" + str(config[\'gpu\'][\'id\']) + ""_"" + time.strftime(\'%Hh%Mm%Ss_on_%b_%d_%Y\')\n    root_ckpt_dir = out_dir + \'checkpoints/\' + MODEL_NAME + ""_"" + DATASET_NAME + ""_GPU"" + str(config[\'gpu\'][\'id\']) + ""_"" + time.strftime(\'%Hh%Mm%Ss_on_%b_%d_%Y\')\n    write_file_name = out_dir + \'results/result_\' + MODEL_NAME + ""_"" + DATASET_NAME + ""_GPU"" + str(config[\'gpu\'][\'id\']) + ""_"" + time.strftime(\'%Hh%Mm%Ss_on_%b_%d_%Y\')\n    write_config_file = out_dir + \'configs/config_\' + MODEL_NAME + ""_"" + DATASET_NAME + ""_GPU"" + str(config[\'gpu\'][\'id\']) + ""_"" + time.strftime(\'%Hh%Mm%Ss_on_%b_%d_%Y\')\n    dirs = root_log_dir, root_ckpt_dir, write_file_name, write_config_file\n\n    if not os.path.exists(out_dir + \'results\'):\n        os.makedirs(out_dir + \'results\')\n        \n    if not os.path.exists(out_dir + \'configs\'):\n        os.makedirs(out_dir + \'configs\')\n\n    net_params[\'total_param\'] = view_model_param(MODEL_NAME, net_params)\n    train_val_pipeline(MODEL_NAME, dataset, params, net_params, dirs)\n\n    \n    \n    \n    \n    \n    \n    \nmain()    \n\n\n\n\n\n\n'"
main_TUs_graph_classification.py,11,"b'\n\n\n\n\n""""""\n    IMPORTING LIBS\n""""""\nimport dgl\n\nimport numpy as np\nimport os\nimport socket\nimport time\nimport random\nimport glob\nimport argparse, json\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\n\nfrom tensorboardX import SummaryWriter\nfrom tqdm import tqdm\n\nclass DotDict(dict):\n    def __init__(self, **kwds):\n        self.update(kwds)\n        self.__dict__ = self\n        \n\n\n\n\n\n\n""""""\n    IMPORTING CUSTOM MODULES/METHODS\n""""""\n\nfrom nets.TUs_graph_classification.load_net import gnn_model # import GNNs\nfrom data.data import LoadData # import dataset\nfrom train.train_TUs_graph_classification import train_epoch, evaluate_network # import train functions\n\n\n\n\n""""""\n    GPU Setup\n""""""\ndef gpu_setup(use_gpu, gpu_id):\n    os.environ[""CUDA_DEVICE_ORDER""] = ""PCI_BUS_ID""\n    os.environ[""CUDA_VISIBLE_DEVICES""] = str(gpu_id)  \n\n    if torch.cuda.is_available() and use_gpu:\n        print(\'cuda available with GPU:\',torch.cuda.get_device_name(0))\n        device = torch.device(""cuda"")\n    else:\n        print(\'cuda not available\')\n        device = torch.device(""cpu"")\n    return device\n\n\n\n\n\n\n\n\n\n\n""""""\n    VIEWING MODEL CONFIG AND PARAMS\n""""""\ndef view_model_param(MODEL_NAME, net_params):\n    model = gnn_model(MODEL_NAME, net_params)\n    total_param = 0\n    print(""MODEL DETAILS:\\n"")\n    #print(model)\n    for param in model.parameters():\n        # print(param.data.size())\n        total_param += np.prod(list(param.data.size()))\n    print(\'MODEL/Total parameters:\', MODEL_NAME, total_param)\n    return total_param\n\n""""""\n    TRAINING CODE\n""""""\n\ndef train_val_pipeline(MODEL_NAME, DATASET_NAME, params, net_params, dirs):\n    avg_test_acc = []\n    avg_train_acc = []\n\n    t0 = time.time()\n    per_epoch_time = []\n\n    dataset = LoadData(DATASET_NAME)\n    \n    if MODEL_NAME in [\'GCN\', \'GAT\']:\n        if net_params[\'self_loop\']:\n            print(""[!] Adding graph self-loops for GCN/GAT models (central node trick)."")\n            dataset._add_self_loops()\n    \n    trainset, valset, testset = dataset.train, dataset.val, dataset.test\n    \n    root_log_dir, root_ckpt_dir, write_file_name, write_config_file = dirs\n    device = net_params[\'device\']\n    \n    # Write the network and optimization hyper-parameters in folder config/\n    with open(write_config_file + \'.txt\', \'w\') as f:\n        f.write(""""""Dataset: {},\\nModel: {}\\n\\nparams={}\\n\\nnet_params={}\\n\\n\\nTotal Parameters: {}\\n\\n""""""                .format(DATASET_NAME, MODEL_NAME, params, net_params, net_params[\'total_param\']))\n    \n    # At any point you can hit Ctrl + C to break out of training early.\n    try:\n        for split_number in range(10):\n            t0_split = time.time()\n            log_dir = os.path.join(root_log_dir, ""RUN_"" + str(split_number))\n            writer = SummaryWriter(log_dir=log_dir)\n\n            # setting seeds\n            random.seed(params[\'seed\'])\n            np.random.seed(params[\'seed\'])\n            torch.manual_seed(params[\'seed\'])\n            if device == \'cuda\':\n                torch.cuda.manual_seed(params[\'seed\'])\n\n            print(""RUN NUMBER: "", split_number)\n            trainset, valset, testset = dataset.train[split_number], dataset.val[split_number], dataset.test[split_number]\n            print(""Training Graphs: "", len(trainset))\n            print(""Validation Graphs: "", len(valset))\n            print(""Test Graphs: "", len(testset))\n            print(""Number of Classes: "", net_params[\'n_classes\'])\n\n            model = gnn_model(MODEL_NAME, net_params)\n            model = model.to(device)\n            optimizer = optim.Adam(model.parameters(), lr=params[\'init_lr\'], weight_decay=params[\'weight_decay\'])\n            scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\'min\',\n                                                             factor=params[\'lr_reduce_factor\'],\n                                                             patience=params[\'lr_schedule_patience\'],\n                                                             verbose=True)\n\n            epoch_train_losses, epoch_val_losses = [], []\n            epoch_train_accs, epoch_val_accs = [], [] \n\n            # batching exception for Diffpool\n            drop_last = True if MODEL_NAME == \'DiffPool\' else False\n\n            train_loader = DataLoader(trainset, batch_size=params[\'batch_size\'], shuffle=True, drop_last=drop_last, collate_fn=dataset.collate)\n            val_loader = DataLoader(valset, batch_size=params[\'batch_size\'], shuffle=False, drop_last=drop_last, collate_fn=dataset.collate)\n            test_loader = DataLoader(testset, batch_size=params[\'batch_size\'], shuffle=False, drop_last=drop_last, collate_fn=dataset.collate)\n\n\n            with tqdm(range(params[\'epochs\'])) as t:\n                for epoch in t:\n\n                    t.set_description(\'Epoch %d\' % epoch)    \n\n                    start = time.time()\n                    epoch_train_loss, epoch_train_acc, optimizer = train_epoch(model, optimizer, device, train_loader, epoch)\n                    epoch_val_loss, epoch_val_acc = evaluate_network(model, device, val_loader, epoch)\n\n                    epoch_train_losses.append(epoch_train_loss)\n                    epoch_val_losses.append(epoch_val_loss)\n                    epoch_train_accs.append(epoch_train_acc)\n                    epoch_val_accs.append(epoch_val_acc)\n\n                    writer.add_scalar(\'train/_loss\', epoch_train_loss, epoch)\n                    writer.add_scalar(\'val/_loss\', epoch_val_loss, epoch)\n                    writer.add_scalar(\'train/_acc\', epoch_train_acc, epoch)\n                    writer.add_scalar(\'val/_acc\', epoch_val_acc, epoch)\n                    writer.add_scalar(\'learning_rate\', optimizer.param_groups[0][\'lr\'], epoch)\n\n                    _, epoch_test_acc = evaluate_network(model, device, test_loader, epoch)\n                    t.set_postfix(time=time.time()-start, lr=optimizer.param_groups[0][\'lr\'],\n                                  train_loss=epoch_train_loss, val_loss=epoch_val_loss,\n                                  train_acc=epoch_train_acc, val_acc=epoch_val_acc,\n                                  test_acc=epoch_test_acc)  \n\n                    per_epoch_time.append(time.time()-start)\n\n                    # Saving checkpoint\n                    ckpt_dir = os.path.join(root_ckpt_dir, ""RUN_"" + str(split_number))\n                    if not os.path.exists(ckpt_dir):\n                        os.makedirs(ckpt_dir)\n                    torch.save(model.state_dict(), \'{}.pkl\'.format(ckpt_dir + ""/epoch_"" + str(epoch)))\n\n                    files = glob.glob(ckpt_dir + \'/*.pkl\')\n                    for file in files:\n                        epoch_nb = file.split(\'_\')[-1]\n                        epoch_nb = int(epoch_nb.split(\'.\')[0])\n                        if epoch_nb < epoch-1:\n                            os.remove(file)\n\n                    scheduler.step(epoch_val_loss)\n\n                    if optimizer.param_groups[0][\'lr\'] < params[\'min_lr\']:\n                        print(""\\n!! LR EQUAL TO MIN LR SET."")\n                        break\n                        \n                    # Stop training after params[\'max_time\'] hours\n                    if time.time()-t0_split > params[\'max_time\']*3600/10:       # Dividing max_time by 10, since there are 10 runs in TUs\n                        print(\'-\' * 89)\n                        print(""Max_time for one train-val-test split experiment elapsed {:.3f} hours, so stopping"".format(params[\'max_time\']/10))\n                        break\n\n            _, test_acc = evaluate_network(model, device, test_loader, epoch)   \n            _, train_acc = evaluate_network(model, device, train_loader, epoch)    \n            avg_test_acc.append(test_acc)   \n            avg_train_acc.append(train_acc)\n\n            print(""Test Accuracy [LAST EPOCH]: {:.4f}"".format(test_acc))\n            print(""Train Accuracy [LAST EPOCH]: {:.4f}"".format(train_acc))\n    \n    except KeyboardInterrupt:\n        print(\'-\' * 89)\n        print(\'Exiting from training early because of KeyboardInterrupt\')\n        \n    \n    print(""TOTAL TIME TAKEN: {:.4f}hrs"".format((time.time()-t0)/3600))\n    print(""AVG TIME PER EPOCH: {:.4f}s"".format(np.mean(per_epoch_time)))\n\n    # Final test accuracy value averaged over 10-fold\n    print(""""""\\n\\n\\nFINAL RESULTS\\n\\nTEST ACCURACY averaged: {:.4f} with s.d. {:.4f}""""""          .format(np.mean(np.array(avg_test_acc))*100, np.std(avg_test_acc)*100))\n    print(""\\nAll splits Test Accuracies:\\n"", avg_test_acc)\n    print(""""""\\n\\n\\nFINAL RESULTS\\n\\nTRAIN ACCURACY averaged: {:.4f} with s.d. {:.4f}""""""          .format(np.mean(np.array(avg_train_acc))*100, np.std(avg_train_acc)*100))\n    print(""\\nAll splits Train Accuracies:\\n"", avg_train_acc)\n\n    writer.close()\n\n    """"""\n        Write the results in out/results folder\n    """"""\n    with open(write_file_name + \'.txt\', \'w\') as f:\n        f.write(""""""Dataset: {},\\nModel: {}\\n\\nparams={}\\n\\nnet_params={}\\n\\n{}\\n\\nTotal Parameters: {}\\n\\n\n    FINAL RESULTS\\nTEST ACCURACY averaged: {:.4f} with s.d. {:.4f}\\nTRAIN ACCURACY averaged: {:.4f} with s.d. {:.4f}\\n\\n\n    Total Time Taken: {:.4f} hrs\\nAverage Time Per Epoch: {:.4f} s\\n\\n\\nAll Splits Test Accuracies: {}""""""\\\n          .format(DATASET_NAME, MODEL_NAME, params, net_params, model, net_params[\'total_param\'],\n                  np.mean(np.array(avg_test_acc))*100, np.std(avg_test_acc)*100,\n                  np.mean(np.array(avg_train_acc))*100, np.std(avg_train_acc)*100,\n               (time.time()-t0)/3600, np.mean(per_epoch_time), avg_test_acc))\n        \n\n    # send results to gmail\n    try:\n        from gmail import send\n        subject = \'Result for Dataset: {}, Model: {}\'.format(DATASET_NAME, MODEL_NAME)\n        body = """"""Dataset: {},\\nModel: {}\\n\\nparams={}\\n\\nnet_params={}\\n\\n{}\\n\\nTotal Parameters: {}\\n\\n\n    FINAL RESULTS\\nTEST ACCURACY averaged: {:.4f} with s.d. {:.4f}\\nTRAIN ACCURACY averaged: {:.4f} with s.d. {:.4f}\\n\\n\n    Total Time Taken: {:.4f} hrs\\nAverage Time Per Epoch: {:.4f} s\\n\\n\\nAll Splits Test Accuracies: {}""""""\\\n          .format(DATASET_NAME, MODEL_NAME, params, net_params, model, net_params[\'total_param\'],\n                  np.mean(np.array(avg_test_acc))*100, np.std(avg_test_acc)*100,\n                  np.mean(np.array(avg_train_acc))*100, np.std(avg_train_acc)*100,\n               (time.time()-t0)/3600, np.mean(per_epoch_time), avg_test_acc)\n        send(subject, body)\n    except:\n        pass\n        \n\n\n\n\ndef main():    \n    """"""\n        USER CONTROLS\n    """"""\n    \n    \n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'--config\', help=""Please give a config.json file with training/model/data/param details"")\n    parser.add_argument(\'--gpu_id\', help=""Please give a value for gpu id"")\n    parser.add_argument(\'--model\', help=""Please give a value for model name"")\n    parser.add_argument(\'--dataset\', help=""Please give a value for dataset name"")\n    parser.add_argument(\'--out_dir\', help=""Please give a value for out_dir"")\n    parser.add_argument(\'--seed\', help=""Please give a value for seed"")\n    parser.add_argument(\'--epochs\', help=""Please give a value for epochs"")\n    parser.add_argument(\'--batch_size\', help=""Please give a value for batch_size"")\n    parser.add_argument(\'--init_lr\', help=""Please give a value for init_lr"")\n    parser.add_argument(\'--lr_reduce_factor\', help=""Please give a value for lr_reduce_factor"")\n    parser.add_argument(\'--lr_schedule_patience\', help=""Please give a value for lr_schedule_patience"")\n    parser.add_argument(\'--min_lr\', help=""Please give a value for min_lr"")\n    parser.add_argument(\'--weight_decay\', help=""Please give a value for weight_decay"")\n    parser.add_argument(\'--print_epoch_interval\', help=""Please give a value for print_epoch_interval"")    \n    parser.add_argument(\'--L\', help=""Please give a value for L"")\n    parser.add_argument(\'--hidden_dim\', help=""Please give a value for hidden_dim"")\n    parser.add_argument(\'--out_dim\', help=""Please give a value for out_dim"")\n    parser.add_argument(\'--residual\', help=""Please give a value for residual"")\n    parser.add_argument(\'--edge_feat\', help=""Please give a value for edge_feat"")\n    parser.add_argument(\'--readout\', help=""Please give a value for readout"")\n    parser.add_argument(\'--kernel\', help=""Please give a value for kernel"")\n    parser.add_argument(\'--n_heads\', help=""Please give a value for n_heads"")\n    parser.add_argument(\'--gated\', help=""Please give a value for gated"")\n    parser.add_argument(\'--in_feat_dropout\', help=""Please give a value for in_feat_dropout"")\n    parser.add_argument(\'--dropout\', help=""Please give a value for dropout"")\n    parser.add_argument(\'--graph_norm\', help=""Please give a value for graph_norm"")\n    parser.add_argument(\'--batch_norm\', help=""Please give a value for batch_norm"")\n    parser.add_argument(\'--sage_aggregator\', help=""Please give a value for sage_aggregator"")\n    parser.add_argument(\'--data_mode\', help=""Please give a value for data_mode"")\n    parser.add_argument(\'--num_pool\', help=""Please give a value for num_pool"")\n    parser.add_argument(\'--gnn_per_block\', help=""Please give a value for gnn_per_block"")\n    parser.add_argument(\'--embedding_dim\', help=""Please give a value for embedding_dim"")\n    parser.add_argument(\'--pool_ratio\', help=""Please give a value for pool_ratio"")\n    parser.add_argument(\'--linkpred\', help=""Please give a value for linkpred"")\n    parser.add_argument(\'--cat\', help=""Please give a value for cat"")\n    parser.add_argument(\'--self_loop\', help=""Please give a value for self_loop"")\n    parser.add_argument(\'--max_time\', help=""Please give a value for max_time"")\n    args = parser.parse_args()\n    with open(args.config) as f:\n        config = json.load(f)\n        \n    # device\n    if args.gpu_id is not None:\n        config[\'gpu\'][\'id\'] = int(args.gpu_id)\n        config[\'gpu\'][\'use\'] = True\n    device = gpu_setup(config[\'gpu\'][\'use\'], config[\'gpu\'][\'id\'])\n    # model, dataset, out_dir\n    if args.model is not None:\n        MODEL_NAME = args.model\n    else:\n        MODEL_NAME = config[\'model\']\n    if args.dataset is not None:\n        DATASET_NAME = args.dataset\n    else:\n        DATASET_NAME = config[\'dataset\']\n    dataset = LoadData(DATASET_NAME)\n    if args.out_dir is not None:\n        out_dir = args.out_dir\n    else:\n        out_dir = config[\'out_dir\']\n    # parameters\n    params = config[\'params\']\n    if args.seed is not None:\n        params[\'seed\'] = int(args.seed)\n    if args.epochs is not None:\n        params[\'epochs\'] = int(args.epochs)\n    if args.batch_size is not None:\n        params[\'batch_size\'] = int(args.batch_size)\n    if args.init_lr is not None:\n        params[\'init_lr\'] = float(args.init_lr)\n    if args.lr_reduce_factor is not None:\n        params[\'lr_reduce_factor\'] = float(args.lr_reduce_factor)\n    if args.lr_schedule_patience is not None:\n        params[\'lr_schedule_patience\'] = int(args.lr_schedule_patience)\n    if args.min_lr is not None:\n        params[\'min_lr\'] = float(args.min_lr)\n    if args.weight_decay is not None:\n        params[\'weight_decay\'] = float(args.weight_decay)\n    if args.print_epoch_interval is not None:\n        params[\'print_epoch_interval\'] = int(args.print_epoch_interval)\n    if args.max_time is not None:\n        params[\'max_time\'] = float(args.max_time)\n    # network parameters\n    net_params = config[\'net_params\']\n    net_params[\'device\'] = device\n    net_params[\'gpu_id\'] = config[\'gpu\'][\'id\']\n    net_params[\'batch_size\'] = params[\'batch_size\']\n    if args.L is not None:\n        net_params[\'L\'] = int(args.L)\n    if args.hidden_dim is not None:\n        net_params[\'hidden_dim\'] = int(args.hidden_dim)\n    if args.out_dim is not None:\n        net_params[\'out_dim\'] = int(args.out_dim)   \n    if args.residual is not None:\n        net_params[\'residual\'] = True if args.residual==\'True\' else False\n    if args.edge_feat is not None:\n        net_params[\'edge_feat\'] = True if args.edge_feat==\'True\' else False\n    if args.readout is not None:\n        net_params[\'readout\'] = args.readout\n    if args.kernel is not None:\n        net_params[\'kernel\'] = int(args.kernel)\n    if args.n_heads is not None:\n        net_params[\'n_heads\'] = int(args.n_heads)\n    if args.gated is not None:\n        net_params[\'gated\'] = True if args.gated==\'True\' else False\n    if args.in_feat_dropout is not None:\n        net_params[\'in_feat_dropout\'] = float(args.in_feat_dropout)\n    if args.dropout is not None:\n        net_params[\'dropout\'] = float(args.dropout)\n    if args.graph_norm is not None:\n        net_params[\'graph_norm\'] = True if args.graph_norm==\'True\' else False\n    if args.batch_norm is not None:\n        net_params[\'batch_norm\'] = True if args.batch_norm==\'True\' else False\n    if args.sage_aggregator is not None:\n        net_params[\'sage_aggregator\'] = args.sage_aggregator\n    if args.data_mode is not None:\n        net_params[\'data_mode\'] = args.data_mode\n    if args.num_pool is not None:\n        net_params[\'num_pool\'] = int(args.num_pool)\n    if args.gnn_per_block is not None:\n        net_params[\'gnn_per_block\'] = int(args.gnn_per_block)\n    if args.embedding_dim is not None:\n        net_params[\'embedding_dim\'] = int(args.embedding_dim)\n    if args.pool_ratio is not None:\n        net_params[\'pool_ratio\'] = float(args.pool_ratio)\n    if args.linkpred is not None:\n        net_params[\'linkpred\'] = True if args.linkpred==\'True\' else False\n    if args.cat is not None:\n        net_params[\'cat\'] = True if args.cat==\'True\' else False\n    if args.self_loop is not None:\n        net_params[\'self_loop\'] = True if args.self_loop==\'True\' else False\n        \n      \n    \n    # TUs\n    net_params[\'in_dim\'] = dataset.all.graph_lists[0].ndata[\'feat\'][0].shape[0]\n    num_classes = len(np.unique(dataset.all.graph_labels))\n    net_params[\'n_classes\'] = num_classes\n    \n    if MODEL_NAME == \'DiffPool\':\n        # calculate assignment dimension: pool_ratio * largest graph\'s maximum\n        # number of nodes  in the dataset\n        num_nodes = [dataset.all[i][0].number_of_nodes() for i in range(len(dataset.all))]\n        max_num_node = max(num_nodes)\n        net_params[\'assign_dim\'] = int(max_num_node * net_params[\'pool_ratio\']) * net_params[\'batch_size\']\n    \n    root_log_dir = out_dir + \'logs/\' + MODEL_NAME + ""_"" + DATASET_NAME + ""_GPU"" + str(config[\'gpu\'][\'id\']) + ""_"" + time.strftime(\'%Hh%Mm%Ss_on_%b_%d_%Y\')\n    root_ckpt_dir = out_dir + \'checkpoints/\' + MODEL_NAME + ""_"" + DATASET_NAME + ""_GPU"" + str(config[\'gpu\'][\'id\']) + ""_"" + time.strftime(\'%Hh%Mm%Ss_on_%b_%d_%Y\')\n    write_file_name = out_dir + \'results/result_\' + MODEL_NAME + ""_"" + DATASET_NAME + ""_GPU"" + str(config[\'gpu\'][\'id\']) + ""_"" + time.strftime(\'%Hh%Mm%Ss_on_%b_%d_%Y\')\n    write_config_file = out_dir + \'configs/config_\' + MODEL_NAME + ""_"" + DATASET_NAME + ""_GPU"" + str(config[\'gpu\'][\'id\']) + ""_"" + time.strftime(\'%Hh%Mm%Ss_on_%b_%d_%Y\')\n    dirs = root_log_dir, root_ckpt_dir, write_file_name, write_config_file\n\n    if not os.path.exists(out_dir + \'results\'):\n        os.makedirs(out_dir + \'results\')\n        \n    if not os.path.exists(out_dir + \'configs\'):\n        os.makedirs(out_dir + \'configs\')\n\n    net_params[\'total_param\'] = view_model_param(MODEL_NAME, net_params)\n    train_val_pipeline(MODEL_NAME, DATASET_NAME, params, net_params, dirs)\n\n    \n\n    \n    \n    \n    \n    \n    \nmain()    \n\n\n\n\n\n\n'"
main_molecules_graph_regression.py,11,"b'\n\n\n\n\n""""""\n    IMPORTING LIBS\n""""""\nimport dgl\n\nimport numpy as np\nimport os\nimport socket\nimport time\nimport random\nimport glob\nimport argparse, json\nimport pickle\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\n\nfrom tensorboardX import SummaryWriter\nfrom tqdm import tqdm\n\nclass DotDict(dict):\n    def __init__(self, **kwds):\n        self.update(kwds)\n        self.__dict__ = self\n\n\n\n\n\n\n""""""\n    IMPORTING CUSTOM MODULES/METHODS\n""""""\nfrom nets.molecules_graph_regression.load_net import gnn_model # import all GNNS\nfrom data.data import LoadData # import dataset\nfrom train.train_molecules_graph_regression import train_epoch, evaluate_network # import train functions\n\n\n\n\n""""""\n    GPU Setup\n""""""\ndef gpu_setup(use_gpu, gpu_id):\n    os.environ[""CUDA_DEVICE_ORDER""] = ""PCI_BUS_ID""\n    os.environ[""CUDA_VISIBLE_DEVICES""] = str(gpu_id)  \n\n    if torch.cuda.is_available() and use_gpu:\n        print(\'cuda available with GPU:\',torch.cuda.get_device_name(0))\n        device = torch.device(""cuda"")\n    else:\n        print(\'cuda not available\')\n        device = torch.device(""cpu"")\n    return device\n\n\n\n\n\n\n\n\n\n\n""""""\n    VIEWING MODEL CONFIG AND PARAMS\n""""""\ndef view_model_param(MODEL_NAME, net_params):\n    model = gnn_model(MODEL_NAME, net_params)\n    total_param = 0\n    print(""MODEL DETAILS:\\n"")\n    #print(model)\n    for param in model.parameters():\n        # print(param.data.size())\n        total_param += np.prod(list(param.data.size()))\n    print(\'MODEL/Total parameters:\', MODEL_NAME, total_param)\n    return total_param\n\n\n""""""\n    TRAINING CODE\n""""""\n\ndef train_val_pipeline(MODEL_NAME, dataset, params, net_params, dirs):\n    t0 = time.time()\n    per_epoch_time = []\n        \n    DATASET_NAME = dataset.name\n    \n    if MODEL_NAME in [\'GCN\', \'GAT\']:\n        if net_params[\'self_loop\']:\n            print(""[!] Adding graph self-loops for GCN/GAT models (central node trick)."")\n            dataset._add_self_loops()\n    \n    trainset, valset, testset = dataset.train, dataset.val, dataset.test\n        \n    root_log_dir, root_ckpt_dir, write_file_name, write_config_file = dirs\n    device = net_params[\'device\']\n    \n    # Write the network and optimization hyper-parameters in folder config/\n    with open(write_config_file + \'.txt\', \'w\') as f:\n        f.write(""""""Dataset: {},\\nModel: {}\\n\\nparams={}\\n\\nnet_params={}\\n\\n\\nTotal Parameters: {}\\n\\n""""""                .format(DATASET_NAME, MODEL_NAME, params, net_params, net_params[\'total_param\']))\n        \n    log_dir = os.path.join(root_log_dir, ""RUN_"" + str(0))\n    writer = SummaryWriter(log_dir=log_dir)\n\n    # setting seeds\n    random.seed(params[\'seed\'])\n    np.random.seed(params[\'seed\'])\n    torch.manual_seed(params[\'seed\'])\n    if device == \'cuda\':\n        torch.cuda.manual_seed(params[\'seed\'])\n    \n    print(""Training Graphs: "", len(trainset))\n    print(""Validation Graphs: "", len(valset))\n    print(""Test Graphs: "", len(testset))\n\n    model = gnn_model(MODEL_NAME, net_params)\n    model = model.to(device)\n\n    optimizer = optim.Adam(model.parameters(), lr=params[\'init_lr\'], weight_decay=params[\'weight_decay\'])\n    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\'min\',\n                                                     factor=params[\'lr_reduce_factor\'],\n                                                     patience=params[\'lr_schedule_patience\'],\n                                                     verbose=True)\n    \n    epoch_train_losses, epoch_val_losses = [], []\n    epoch_train_MAEs, epoch_val_MAEs = [], [] \n    \n    # batching exception for Diffpool\n    drop_last = True if MODEL_NAME == \'DiffPool\' else False\n    \n    train_loader = DataLoader(trainset, batch_size=params[\'batch_size\'], shuffle=True, drop_last=drop_last, collate_fn=dataset.collate)\n    val_loader = DataLoader(valset, batch_size=params[\'batch_size\'], shuffle=False, drop_last=drop_last, collate_fn=dataset.collate)\n    test_loader = DataLoader(testset, batch_size=params[\'batch_size\'], shuffle=False, drop_last=drop_last, collate_fn=dataset.collate)\n        \n    \n    # At any point you can hit Ctrl + C to break out of training early.\n    try:\n        with tqdm(range(params[\'epochs\'])) as t:\n            for epoch in t:\n\n                t.set_description(\'Epoch %d\' % epoch)\n\n                start = time.time()\n\n                epoch_train_loss, epoch_train_mae, optimizer = train_epoch(model, optimizer, device, train_loader, epoch)\n                epoch_val_loss, epoch_val_mae = evaluate_network(model, device, val_loader, epoch)\n\n                epoch_train_losses.append(epoch_train_loss)\n                epoch_val_losses.append(epoch_val_loss)\n                epoch_train_MAEs.append(epoch_train_mae)\n                epoch_val_MAEs.append(epoch_val_mae)\n\n                writer.add_scalar(\'train/_loss\', epoch_train_loss, epoch)\n                writer.add_scalar(\'val/_loss\', epoch_val_loss, epoch)\n                writer.add_scalar(\'train/_mae\', epoch_train_mae, epoch)\n                writer.add_scalar(\'val/_mae\', epoch_val_mae, epoch)\n                writer.add_scalar(\'learning_rate\', optimizer.param_groups[0][\'lr\'], epoch)\n\n                _, epoch_test_mae = evaluate_network(model, device, test_loader, epoch)        \n                t.set_postfix(time=time.time()-start, lr=optimizer.param_groups[0][\'lr\'],\n                              train_loss=epoch_train_loss, val_loss=epoch_val_loss,\n                              train_MAE=epoch_train_mae.item(), val_MAE=epoch_val_mae.item(),\n                              test_MAE=epoch_test_mae.item())\n\n\n                per_epoch_time.append(time.time()-start)\n\n                # Saving checkpoint\n                ckpt_dir = os.path.join(root_ckpt_dir, ""RUN_"")\n                if not os.path.exists(ckpt_dir):\n                    os.makedirs(ckpt_dir)\n                torch.save(model.state_dict(), \'{}.pkl\'.format(ckpt_dir + ""/epoch_"" + str(epoch)))\n\n                files = glob.glob(ckpt_dir + \'/*.pkl\')\n                for file in files:\n                    epoch_nb = file.split(\'_\')[-1]\n                    epoch_nb = int(epoch_nb.split(\'.\')[0])\n                    if epoch_nb < epoch-1:\n                        os.remove(file)\n\n                scheduler.step(epoch_val_loss)\n\n                if optimizer.param_groups[0][\'lr\'] < params[\'min_lr\']:\n                    print(""\\n!! LR EQUAL TO MIN LR SET."")\n                    break\n                \n                # Stop training after params[\'max_time\'] hours\n                if time.time()-t0 > params[\'max_time\']*3600:\n                    print(\'-\' * 89)\n                    print(""Max_time for training elapsed {:.2f} hours, so stopping"".format(params[\'max_time\']))\n                    break\n                \n    except KeyboardInterrupt:\n        print(\'-\' * 89)\n        print(\'Exiting from training early because of KeyboardInterrupt\')\n    \n    _, test_mae = evaluate_network(model, device, test_loader, epoch)\n    _, train_mae = evaluate_network(model, device, train_loader, epoch)\n    print(""Test MAE: {:.4f}"".format(test_mae))\n    print(""Train MAE: {:.4f}"".format(train_mae))\n    print(""TOTAL TIME TAKEN: {:.4f}s"".format(time.time()-t0))\n    print(""AVG TIME PER EPOCH: {:.4f}s"".format(np.mean(per_epoch_time)))\n\n    writer.close()\n\n    """"""\n        Write the results in out_dir/results folder\n    """"""\n    with open(write_file_name + \'.txt\', \'w\') as f:\n        f.write(""""""Dataset: {},\\nModel: {}\\n\\nparams={}\\n\\nnet_params={}\\n\\n{}\\n\\nTotal Parameters: {}\\n\\n\n    FINAL RESULTS\\nTEST MAE: {:.4f}\\nTRAIN MAE: {:.4f}\\n\\n\n    Total Time Taken: {:.4f} hrs\\nAverage Time Per Epoch: {:.4f} s\\n\\n\\n""""""\\\n          .format(DATASET_NAME, MODEL_NAME, params, net_params, model, net_params[\'total_param\'],\n                  np.mean(np.array(test_mae.cpu())), np.array(train_mae.cpu()), (time.time()-t0)/3600, np.mean(per_epoch_time)))\n        \n\n        \n    # send results to gmail\n    try:\n        from gmail import send\n        subject = \'Result for Dataset: {}, Model: {}\'.format(DATASET_NAME, MODEL_NAME)\n        body = """"""Dataset: {},\\nModel: {}\\n\\nparams={}\\n\\nnet_params={}\\n\\n{}\\n\\nTotal Parameters: {}\\n\\n\n    FINAL RESULTS\\nTEST MAE: {:.4f}\\nTRAIN MAE: {:.4f}\\n\\n\n    Total Time Taken: {:.4f} hrs\\nAverage Time Per Epoch: {:.4f} s\\n\\n\\n""""""\\\n          .format(DATASET_NAME, MODEL_NAME, params, net_params, model, net_params[\'total_param\'],\n                  np.mean(np.array(test_mae.cpu())), np.array(train_mae.cpu()), (time.time()-t0)/3600, np.mean(per_epoch_time))\n        send(subject, body)\n    except:\n        pass\n        \n\n\n\n\ndef main():    \n    """"""\n        USER CONTROLS\n    """"""\n    \n    \n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'--config\', help=""Please give a config.json file with training/model/data/param details"")\n    parser.add_argument(\'--gpu_id\', help=""Please give a value for gpu id"")\n    parser.add_argument(\'--model\', help=""Please give a value for model name"")\n    parser.add_argument(\'--dataset\', help=""Please give a value for dataset name"")\n    parser.add_argument(\'--out_dir\', help=""Please give a value for out_dir"")\n    parser.add_argument(\'--seed\', help=""Please give a value for seed"")\n    parser.add_argument(\'--epochs\', help=""Please give a value for epochs"")\n    parser.add_argument(\'--batch_size\', help=""Please give a value for batch_size"")\n    parser.add_argument(\'--init_lr\', help=""Please give a value for init_lr"")\n    parser.add_argument(\'--lr_reduce_factor\', help=""Please give a value for lr_reduce_factor"")\n    parser.add_argument(\'--lr_schedule_patience\', help=""Please give a value for lr_schedule_patience"")\n    parser.add_argument(\'--min_lr\', help=""Please give a value for min_lr"")\n    parser.add_argument(\'--weight_decay\', help=""Please give a value for weight_decay"")\n    parser.add_argument(\'--print_epoch_interval\', help=""Please give a value for print_epoch_interval"")    \n    parser.add_argument(\'--L\', help=""Please give a value for L"")\n    parser.add_argument(\'--hidden_dim\', help=""Please give a value for hidden_dim"")\n    parser.add_argument(\'--out_dim\', help=""Please give a value for out_dim"")\n    parser.add_argument(\'--residual\', help=""Please give a value for residual"")\n    parser.add_argument(\'--edge_feat\', help=""Please give a value for edge_feat"")\n    parser.add_argument(\'--readout\', help=""Please give a value for readout"")\n    parser.add_argument(\'--kernel\', help=""Please give a value for kernel"")\n    parser.add_argument(\'--n_heads\', help=""Please give a value for n_heads"")\n    parser.add_argument(\'--gated\', help=""Please give a value for gated"")\n    parser.add_argument(\'--in_feat_dropout\', help=""Please give a value for in_feat_dropout"")\n    parser.add_argument(\'--dropout\', help=""Please give a value for dropout"")\n    parser.add_argument(\'--graph_norm\', help=""Please give a value for graph_norm"")\n    parser.add_argument(\'--batch_norm\', help=""Please give a value for batch_norm"")\n    parser.add_argument(\'--sage_aggregator\', help=""Please give a value for sage_aggregator"")\n    parser.add_argument(\'--data_mode\', help=""Please give a value for data_mode"")\n    parser.add_argument(\'--num_pool\', help=""Please give a value for num_pool"")\n    parser.add_argument(\'--gnn_per_block\', help=""Please give a value for gnn_per_block"")\n    parser.add_argument(\'--embedding_dim\', help=""Please give a value for embedding_dim"")\n    parser.add_argument(\'--pool_ratio\', help=""Please give a value for pool_ratio"")\n    parser.add_argument(\'--linkpred\', help=""Please give a value for linkpred"")\n    parser.add_argument(\'--cat\', help=""Please give a value for cat"")\n    parser.add_argument(\'--self_loop\', help=""Please give a value for self_loop"")\n    parser.add_argument(\'--max_time\', help=""Please give a value for max_time"")\n    args = parser.parse_args()\n    with open(args.config) as f:\n        config = json.load(f)\n        \n    # device\n    if args.gpu_id is not None:\n        config[\'gpu\'][\'id\'] = int(args.gpu_id)\n        config[\'gpu\'][\'use\'] = True\n    device = gpu_setup(config[\'gpu\'][\'use\'], config[\'gpu\'][\'id\'])\n    # model, dataset, out_dir\n    if args.model is not None:\n        MODEL_NAME = args.model\n    else:\n        MODEL_NAME = config[\'model\']\n    if args.dataset is not None:\n        DATASET_NAME = args.dataset\n    else:\n        DATASET_NAME = config[\'dataset\']\n    dataset = LoadData(DATASET_NAME)\n    if args.out_dir is not None:\n        out_dir = args.out_dir\n    else:\n        out_dir = config[\'out_dir\']\n    # parameters\n    params = config[\'params\']\n    if args.seed is not None:\n        params[\'seed\'] = int(args.seed)\n    if args.epochs is not None:\n        params[\'epochs\'] = int(args.epochs)\n    if args.batch_size is not None:\n        params[\'batch_size\'] = int(args.batch_size)\n    if args.init_lr is not None:\n        params[\'init_lr\'] = float(args.init_lr)\n    if args.lr_reduce_factor is not None:\n        params[\'lr_reduce_factor\'] = float(args.lr_reduce_factor)\n    if args.lr_schedule_patience is not None:\n        params[\'lr_schedule_patience\'] = int(args.lr_schedule_patience)\n    if args.min_lr is not None:\n        params[\'min_lr\'] = float(args.min_lr)\n    if args.weight_decay is not None:\n        params[\'weight_decay\'] = float(args.weight_decay)\n    if args.print_epoch_interval is not None:\n        params[\'print_epoch_interval\'] = int(args.print_epoch_interval)\n    if args.max_time is not None:\n        params[\'max_time\'] = float(args.max_time)\n    # network parameters\n    net_params = config[\'net_params\']\n    net_params[\'device\'] = device\n    net_params[\'gpu_id\'] = config[\'gpu\'][\'id\']\n    net_params[\'batch_size\'] = params[\'batch_size\']\n    if args.L is not None:\n        net_params[\'L\'] = int(args.L)\n    if args.hidden_dim is not None:\n        net_params[\'hidden_dim\'] = int(args.hidden_dim)\n    if args.out_dim is not None:\n        net_params[\'out_dim\'] = int(args.out_dim)   \n    if args.residual is not None:\n        net_params[\'residual\'] = True if args.residual==\'True\' else False\n    if args.edge_feat is not None:\n        net_params[\'edge_feat\'] = True if args.edge_feat==\'True\' else False\n    if args.readout is not None:\n        net_params[\'readout\'] = args.readout\n    if args.kernel is not None:\n        net_params[\'kernel\'] = int(args.kernel)\n    if args.n_heads is not None:\n        net_params[\'n_heads\'] = int(args.n_heads)\n    if args.gated is not None:\n        net_params[\'gated\'] = True if args.gated==\'True\' else False\n    if args.in_feat_dropout is not None:\n        net_params[\'in_feat_dropout\'] = float(args.in_feat_dropout)\n    if args.dropout is not None:\n        net_params[\'dropout\'] = float(args.dropout)\n    if args.graph_norm is not None:\n        net_params[\'graph_norm\'] = True if args.graph_norm==\'True\' else False\n    if args.batch_norm is not None:\n        net_params[\'batch_norm\'] = True if args.batch_norm==\'True\' else False\n    if args.sage_aggregator is not None:\n        net_params[\'sage_aggregator\'] = args.sage_aggregator\n    if args.data_mode is not None:\n        net_params[\'data_mode\'] = args.data_mode\n    if args.num_pool is not None:\n        net_params[\'num_pool\'] = int(args.num_pool)\n    if args.gnn_per_block is not None:\n        net_params[\'gnn_per_block\'] = int(args.gnn_per_block)\n    if args.embedding_dim is not None:\n        net_params[\'embedding_dim\'] = int(args.embedding_dim)\n    if args.pool_ratio is not None:\n        net_params[\'pool_ratio\'] = float(args.pool_ratio)\n    if args.linkpred is not None:\n        net_params[\'linkpred\'] = True if args.linkpred==\'True\' else False\n    if args.cat is not None:\n        net_params[\'cat\'] = True if args.cat==\'True\' else False\n    if args.self_loop is not None:\n        net_params[\'self_loop\'] = True if args.self_loop==\'True\' else False\n        \n    \n    # ZINC\n    net_params[\'num_atom_type\'] = dataset.num_atom_type\n    net_params[\'num_bond_type\'] = dataset.num_bond_type\n\n    \n    if MODEL_NAME == \'DiffPool\':\n        # calculate assignment dimension: pool_ratio * largest graph\'s maximum\n        # number of nodes  in the dataset\n        num_nodes = [dataset.train[i][0].number_of_nodes() for i in range(len(dataset.train))]\n        max_num_node = max(num_nodes)\n        net_params[\'assign_dim\'] = int(max_num_node * net_params[\'pool_ratio\']) * net_params[\'batch_size\']\n    \n    root_log_dir = out_dir + \'logs/\' + MODEL_NAME + ""_"" + DATASET_NAME + ""_GPU"" + str(config[\'gpu\'][\'id\']) + ""_"" + time.strftime(\'%Hh%Mm%Ss_on_%b_%d_%Y\')\n    root_ckpt_dir = out_dir + \'checkpoints/\' + MODEL_NAME + ""_"" + DATASET_NAME + ""_GPU"" + str(config[\'gpu\'][\'id\']) + ""_"" + time.strftime(\'%Hh%Mm%Ss_on_%b_%d_%Y\')\n    write_file_name = out_dir + \'results/result_\' + MODEL_NAME + ""_"" + DATASET_NAME + ""_GPU"" + str(config[\'gpu\'][\'id\']) + ""_"" + time.strftime(\'%Hh%Mm%Ss_on_%b_%d_%Y\')\n    write_config_file = out_dir + \'configs/config_\' + MODEL_NAME + ""_"" + DATASET_NAME + ""_GPU"" + str(config[\'gpu\'][\'id\']) + ""_"" + time.strftime(\'%Hh%Mm%Ss_on_%b_%d_%Y\')\n    dirs = root_log_dir, root_ckpt_dir, write_file_name, write_config_file\n\n    if not os.path.exists(out_dir + \'results\'):\n        os.makedirs(out_dir + \'results\')\n        \n    if not os.path.exists(out_dir + \'configs\'):\n        os.makedirs(out_dir + \'configs\')\n\n    net_params[\'total_param\'] = view_model_param(MODEL_NAME, net_params)\n    train_val_pipeline(MODEL_NAME, dataset, params, net_params, dirs)\n\n    \n    \n    \n    \n    \n    \n    \nmain()    \n\n\n\n\n\n\n\n\n\n\n\n'"
main_superpixels_graph_classification.py,11,"b'\n\n\n\n\n""""""\n    IMPORTING LIBS\n""""""\nimport dgl\n\nimport numpy as np\nimport os\nimport socket\nimport time\nimport random\nimport glob\nimport argparse, json\nimport pickle\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\n\nfrom tensorboardX import SummaryWriter\nfrom tqdm import tqdm\n\nclass DotDict(dict):\n    def __init__(self, **kwds):\n        self.update(kwds)\n        self.__dict__ = self\n\n\n\n\n\n\n""""""\n    IMPORTING CUSTOM MODULES/METHODS\n""""""\nfrom nets.superpixels_graph_classification.load_net import gnn_model # import all GNNS\nfrom data.data import LoadData # import dataset\nfrom train.train_superpixels_graph_classification import train_epoch, evaluate_network # import train functions\n\n\n\n\n""""""\n    GPU Setup\n""""""\ndef gpu_setup(use_gpu, gpu_id):\n    os.environ[""CUDA_DEVICE_ORDER""] = ""PCI_BUS_ID""\n    os.environ[""CUDA_VISIBLE_DEVICES""] = str(gpu_id)  \n\n    if torch.cuda.is_available() and use_gpu:\n        print(\'cuda available with GPU:\',torch.cuda.get_device_name(0))\n        device = torch.device(""cuda"")\n    else:\n        print(\'cuda not available\')\n        device = torch.device(""cpu"")\n    return device\n\n\n\n\n\n\n\n\n\n\n""""""\n    VIEWING MODEL CONFIG AND PARAMS\n""""""\ndef view_model_param(MODEL_NAME, net_params):\n    model = gnn_model(MODEL_NAME, net_params)\n    total_param = 0\n    print(""MODEL DETAILS:\\n"")\n    #print(model)\n    for param in model.parameters():\n        # print(param.data.size())\n        total_param += np.prod(list(param.data.size()))\n    print(\'MODEL/Total parameters:\', MODEL_NAME, total_param)\n    return total_param\n\n\n""""""\n    TRAINING CODE\n""""""\n\ndef train_val_pipeline(MODEL_NAME, dataset, params, net_params, dirs):\n    t0 = time.time()\n    per_epoch_time = []\n        \n    DATASET_NAME = dataset.name\n    \n    if MODEL_NAME in [\'GCN\', \'GAT\']:\n        if net_params[\'self_loop\']:\n            print(""[!] Adding graph self-loops for GCN/GAT models (central node trick)."")\n            dataset._add_self_loops()\n    \n    trainset, valset, testset = dataset.train, dataset.val, dataset.test\n        \n    root_log_dir, root_ckpt_dir, write_file_name, write_config_file = dirs\n    device = net_params[\'device\']\n    \n    # Write the network and optimization hyper-parameters in folder config/\n    with open(write_config_file + \'.txt\', \'w\') as f:\n        f.write(""""""Dataset: {},\\nModel: {}\\n\\nparams={}\\n\\nnet_params={}\\n\\n\\nTotal Parameters: {}\\n\\n""""""                .format(DATASET_NAME, MODEL_NAME, params, net_params, net_params[\'total_param\']))\n        \n    log_dir = os.path.join(root_log_dir, ""RUN_"" + str(0))\n    writer = SummaryWriter(log_dir=log_dir)\n\n    # setting seeds\n    random.seed(params[\'seed\'])\n    np.random.seed(params[\'seed\'])\n    torch.manual_seed(params[\'seed\'])\n    if device == \'cuda\':\n        torch.cuda.manual_seed(params[\'seed\'])\n    \n    print(""Training Graphs: "", len(trainset))\n    print(""Validation Graphs: "", len(valset))\n    print(""Test Graphs: "", len(testset))\n    print(""Number of Classes: "", net_params[\'n_classes\'])\n\n    model = gnn_model(MODEL_NAME, net_params)\n    model = model.to(device)\n\n    optimizer = optim.Adam(model.parameters(), lr=params[\'init_lr\'], weight_decay=params[\'weight_decay\'])\n    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\'min\',\n                                                     factor=params[\'lr_reduce_factor\'],\n                                                     patience=params[\'lr_schedule_patience\'],\n                                                     verbose=True)\n    \n    epoch_train_losses, epoch_val_losses = [], []\n    epoch_train_accs, epoch_val_accs = [], [] \n    \n    # batching exception for Diffpool\n    drop_last = True if MODEL_NAME == \'DiffPool\' else False\n    \n    train_loader = DataLoader(trainset, batch_size=params[\'batch_size\'], shuffle=True, drop_last=drop_last, collate_fn=dataset.collate)\n    val_loader = DataLoader(valset, batch_size=params[\'batch_size\'], shuffle=False, drop_last=drop_last, collate_fn=dataset.collate)\n    test_loader = DataLoader(testset, batch_size=params[\'batch_size\'], shuffle=False, drop_last=drop_last, collate_fn=dataset.collate)\n\n    # At any point you can hit Ctrl + C to break out of training early.\n    try:\n        with tqdm(range(params[\'epochs\'])) as t:\n            for epoch in t:\n\n                t.set_description(\'Epoch %d\' % epoch)\n\n                start = time.time()\n\n                epoch_train_loss, epoch_train_acc, optimizer = train_epoch(model, optimizer, device, train_loader, epoch)\n                epoch_val_loss, epoch_val_acc = evaluate_network(model, device, val_loader, epoch)\n\n                epoch_train_losses.append(epoch_train_loss)\n                epoch_val_losses.append(epoch_val_loss)\n                epoch_train_accs.append(epoch_train_acc)\n                epoch_val_accs.append(epoch_val_acc)\n\n                writer.add_scalar(\'train/_loss\', epoch_train_loss, epoch)\n                writer.add_scalar(\'val/_loss\', epoch_val_loss, epoch)\n                writer.add_scalar(\'train/_acc\', epoch_train_acc, epoch)\n                writer.add_scalar(\'val/_acc\', epoch_val_acc, epoch)\n                writer.add_scalar(\'learning_rate\', optimizer.param_groups[0][\'lr\'], epoch)\n\n                _, epoch_test_acc = evaluate_network(model, device, test_loader, epoch)                \n                t.set_postfix(time=time.time()-start, lr=optimizer.param_groups[0][\'lr\'],\n                              train_loss=epoch_train_loss, val_loss=epoch_val_loss,\n                              train_acc=epoch_train_acc, val_acc=epoch_val_acc,\n                              test_acc=epoch_test_acc)    \n\n                per_epoch_time.append(time.time()-start)\n\n                # Saving checkpoint\n                ckpt_dir = os.path.join(root_ckpt_dir, ""RUN_"")\n                if not os.path.exists(ckpt_dir):\n                    os.makedirs(ckpt_dir)\n                torch.save(model.state_dict(), \'{}.pkl\'.format(ckpt_dir + ""/epoch_"" + str(epoch)))\n\n                files = glob.glob(ckpt_dir + \'/*.pkl\')\n                for file in files:\n                    epoch_nb = file.split(\'_\')[-1]\n                    epoch_nb = int(epoch_nb.split(\'.\')[0])\n                    if epoch_nb < epoch-1:\n                        os.remove(file)\n\n                scheduler.step(epoch_val_loss)\n\n                if optimizer.param_groups[0][\'lr\'] < params[\'min_lr\']:\n                    print(""\\n!! LR EQUAL TO MIN LR SET."")\n                    break\n                    \n                # Stop training after params[\'max_time\'] hours\n                if time.time()-t0 > params[\'max_time\']*3600:\n                    print(\'-\' * 89)\n                    print(""Max_time for training elapsed {:.2f} hours, so stopping"".format(params[\'max_time\']))\n                    break\n    \n    except KeyboardInterrupt:\n        print(\'-\' * 89)\n        print(\'Exiting from training early because of KeyboardInterrupt\')\n    \n    _, test_acc = evaluate_network(model, device, test_loader, epoch)\n    _, train_acc = evaluate_network(model, device, train_loader, epoch)\n    print(""Test Accuracy: {:.4f}"".format(test_acc))\n    print(""Train Accuracy: {:.4f}"".format(train_acc))\n    print(""TOTAL TIME TAKEN: {:.4f}s"".format(time.time()-t0))\n    print(""AVG TIME PER EPOCH: {:.4f}s"".format(np.mean(per_epoch_time)))\n\n    writer.close()\n\n    """"""\n        Write the results in out_dir/results folder\n    """"""\n    with open(write_file_name + \'.txt\', \'w\') as f:\n        f.write(""""""Dataset: {},\\nModel: {}\\n\\nparams={}\\n\\nnet_params={}\\n\\n{}\\n\\nTotal Parameters: {}\\n\\n\n    FINAL RESULTS\\nTEST ACCURACY: {:.4f}\\nTRAIN ACCURACY: {:.4f}\\n\\n\n    Total Time Taken: {:.4f} hrs\\nAverage Time Per Epoch: {:.4f} s\\n\\n\\n""""""\\\n          .format(DATASET_NAME, MODEL_NAME, params, net_params, model, net_params[\'total_param\'],\n                  np.mean(np.array(test_acc))*100, np.mean(np.array(train_acc))*100, (time.time()-t0)/3600, np.mean(per_epoch_time)))\n        \n    \n    # send results to gmail\n    try:\n        from gmail import send\n        subject = \'Result for Dataset: {}, Model: {}\'.format(DATASET_NAME, MODEL_NAME)\n        body = """"""Dataset: {},\\nModel: {}\\n\\nparams={}\\n\\nnet_params={}\\n\\n{}\\n\\nTotal Parameters: {}\\n\\n\n    FINAL RESULTS\\nTEST ACCURACY: {:.4f}\\nTRAIN ACCURACY: {:.4f}\\n\\n\n    Total Time Taken: {:.4f} hrs\\nAverage Time Per Epoch: {:.4f} s\\n\\n\\n""""""\\\n          .format(DATASET_NAME, MODEL_NAME, params, net_params, model, net_params[\'total_param\'],\n                  np.mean(np.array(test_acc))*100, np.mean(np.array(train_acc))*100, (time.time()-t0)/3600, np.mean(per_epoch_time))\n        send(subject, body)\n    except:\n        pass\n        \n\n\n\n\ndef main():    \n    """"""\n        USER CONTROLS\n    """"""\n    \n    \n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'--config\', help=""Please give a config.json file with training/model/data/param details"")\n    parser.add_argument(\'--gpu_id\', help=""Please give a value for gpu id"")\n    parser.add_argument(\'--model\', help=""Please give a value for model name"")\n    parser.add_argument(\'--dataset\', help=""Please give a value for dataset name"")\n    parser.add_argument(\'--out_dir\', help=""Please give a value for out_dir"")\n    parser.add_argument(\'--seed\', help=""Please give a value for seed"")\n    parser.add_argument(\'--epochs\', help=""Please give a value for epochs"")\n    parser.add_argument(\'--batch_size\', help=""Please give a value for batch_size"")\n    parser.add_argument(\'--init_lr\', help=""Please give a value for init_lr"")\n    parser.add_argument(\'--lr_reduce_factor\', help=""Please give a value for lr_reduce_factor"")\n    parser.add_argument(\'--lr_schedule_patience\', help=""Please give a value for lr_schedule_patience"")\n    parser.add_argument(\'--min_lr\', help=""Please give a value for min_lr"")\n    parser.add_argument(\'--weight_decay\', help=""Please give a value for weight_decay"")\n    parser.add_argument(\'--print_epoch_interval\', help=""Please give a value for print_epoch_interval"")    \n    parser.add_argument(\'--L\', help=""Please give a value for L"")\n    parser.add_argument(\'--hidden_dim\', help=""Please give a value for hidden_dim"")\n    parser.add_argument(\'--out_dim\', help=""Please give a value for out_dim"")\n    parser.add_argument(\'--residual\', help=""Please give a value for residual"")\n    parser.add_argument(\'--edge_feat\', help=""Please give a value for edge_feat"")\n    parser.add_argument(\'--readout\', help=""Please give a value for readout"")\n    parser.add_argument(\'--kernel\', help=""Please give a value for kernel"")\n    parser.add_argument(\'--n_heads\', help=""Please give a value for n_heads"")\n    parser.add_argument(\'--gated\', help=""Please give a value for gated"")\n    parser.add_argument(\'--in_feat_dropout\', help=""Please give a value for in_feat_dropout"")\n    parser.add_argument(\'--dropout\', help=""Please give a value for dropout"")\n    parser.add_argument(\'--graph_norm\', help=""Please give a value for graph_norm"")\n    parser.add_argument(\'--batch_norm\', help=""Please give a value for batch_norm"")\n    parser.add_argument(\'--sage_aggregator\', help=""Please give a value for sage_aggregator"")\n    parser.add_argument(\'--data_mode\', help=""Please give a value for data_mode"")\n    parser.add_argument(\'--num_pool\', help=""Please give a value for num_pool"")\n    parser.add_argument(\'--gnn_per_block\', help=""Please give a value for gnn_per_block"")\n    parser.add_argument(\'--embedding_dim\', help=""Please give a value for embedding_dim"")\n    parser.add_argument(\'--pool_ratio\', help=""Please give a value for pool_ratio"")\n    parser.add_argument(\'--linkpred\', help=""Please give a value for linkpred"")\n    parser.add_argument(\'--cat\', help=""Please give a value for cat"")\n    parser.add_argument(\'--self_loop\', help=""Please give a value for self_loop"")\n    parser.add_argument(\'--max_time\', help=""Please give a value for max_time"")\n    args = parser.parse_args()\n    with open(args.config) as f:\n        config = json.load(f)\n        \n    # device\n    if args.gpu_id is not None:\n        config[\'gpu\'][\'id\'] = int(args.gpu_id)\n        config[\'gpu\'][\'use\'] = True\n    device = gpu_setup(config[\'gpu\'][\'use\'], config[\'gpu\'][\'id\'])\n    # model, dataset, out_dir\n    if args.model is not None:\n        MODEL_NAME = args.model\n    else:\n        MODEL_NAME = config[\'model\']\n    if args.dataset is not None:\n        DATASET_NAME = args.dataset\n    else:\n        DATASET_NAME = config[\'dataset\']\n    dataset = LoadData(DATASET_NAME)\n    if args.out_dir is not None:\n        out_dir = args.out_dir\n    else:\n        out_dir = config[\'out_dir\']\n    # parameters\n    params = config[\'params\']\n    if args.seed is not None:\n        params[\'seed\'] = int(args.seed)\n    if args.epochs is not None:\n        params[\'epochs\'] = int(args.epochs)\n    if args.batch_size is not None:\n        params[\'batch_size\'] = int(args.batch_size)\n    if args.init_lr is not None:\n        params[\'init_lr\'] = float(args.init_lr)\n    if args.lr_reduce_factor is not None:\n        params[\'lr_reduce_factor\'] = float(args.lr_reduce_factor)\n    if args.lr_schedule_patience is not None:\n        params[\'lr_schedule_patience\'] = int(args.lr_schedule_patience)\n    if args.min_lr is not None:\n        params[\'min_lr\'] = float(args.min_lr)\n    if args.weight_decay is not None:\n        params[\'weight_decay\'] = float(args.weight_decay)\n    if args.print_epoch_interval is not None:\n        params[\'print_epoch_interval\'] = int(args.print_epoch_interval)\n    if args.max_time is not None:\n        params[\'max_time\'] = float(args.max_time)\n    # network parameters\n    net_params = config[\'net_params\']\n    net_params[\'device\'] = device\n    net_params[\'gpu_id\'] = config[\'gpu\'][\'id\']\n    net_params[\'batch_size\'] = params[\'batch_size\']\n    if args.L is not None:\n        net_params[\'L\'] = int(args.L)\n    if args.hidden_dim is not None:\n        net_params[\'hidden_dim\'] = int(args.hidden_dim)\n    if args.out_dim is not None:\n        net_params[\'out_dim\'] = int(args.out_dim)   \n    if args.residual is not None:\n        net_params[\'residual\'] = True if args.residual==\'True\' else False\n    if args.edge_feat is not None:\n        net_params[\'edge_feat\'] = True if args.edge_feat==\'True\' else False\n    if args.readout is not None:\n        net_params[\'readout\'] = args.readout\n    if args.kernel is not None:\n        net_params[\'kernel\'] = int(args.kernel)\n    if args.n_heads is not None:\n        net_params[\'n_heads\'] = int(args.n_heads)\n    if args.gated is not None:\n        net_params[\'gated\'] = True if args.gated==\'True\' else False\n    if args.in_feat_dropout is not None:\n        net_params[\'in_feat_dropout\'] = float(args.in_feat_dropout)\n    if args.dropout is not None:\n        net_params[\'dropout\'] = float(args.dropout)\n    if args.graph_norm is not None:\n        net_params[\'graph_norm\'] = True if args.graph_norm==\'True\' else False\n    if args.batch_norm is not None:\n        net_params[\'batch_norm\'] = True if args.batch_norm==\'True\' else False\n    if args.sage_aggregator is not None:\n        net_params[\'sage_aggregator\'] = args.sage_aggregator\n    if args.data_mode is not None:\n        net_params[\'data_mode\'] = args.data_mode\n    if args.num_pool is not None:\n        net_params[\'num_pool\'] = int(args.num_pool)\n    if args.gnn_per_block is not None:\n        net_params[\'gnn_per_block\'] = int(args.gnn_per_block)\n    if args.embedding_dim is not None:\n        net_params[\'embedding_dim\'] = int(args.embedding_dim)\n    if args.pool_ratio is not None:\n        net_params[\'pool_ratio\'] = float(args.pool_ratio)\n    if args.linkpred is not None:\n        net_params[\'linkpred\'] = True if args.linkpred==\'True\' else False\n    if args.cat is not None:\n        net_params[\'cat\'] = True if args.cat==\'True\' else False\n    if args.self_loop is not None:\n        net_params[\'self_loop\'] = True if args.self_loop==\'True\' else False\n        \n    # Superpixels\n    net_params[\'in_dim\'] = dataset.train[0][0].ndata[\'feat\'][0].size(0)\n    net_params[\'in_dim_edge\'] = dataset.train[0][0].edata[\'feat\'][0].size(0)\n    num_classes = len(np.unique(np.array(dataset.train[:][1])))\n    net_params[\'n_classes\'] = num_classes\n\n    if MODEL_NAME == \'DiffPool\':\n        # calculate assignment dimension: pool_ratio * largest graph\'s maximum\n        # number of nodes  in the dataset\n        max_num_nodes_train = max([dataset.train[i][0].number_of_nodes() for i in range(len(dataset.train))])\n        max_num_nodes_test = max([dataset.test[i][0].number_of_nodes() for i in range(len(dataset.test))])\n        max_num_node = max(max_num_nodes_train, max_num_nodes_test)\n        net_params[\'assign_dim\'] = int(max_num_node * net_params[\'pool_ratio\']) * net_params[\'batch_size\']\n    \n    root_log_dir = out_dir + \'logs/\' + MODEL_NAME + ""_"" + DATASET_NAME + ""_GPU"" + str(config[\'gpu\'][\'id\']) + ""_"" + time.strftime(\'%Hh%Mm%Ss_on_%b_%d_%Y\')\n    root_ckpt_dir = out_dir + \'checkpoints/\' + MODEL_NAME + ""_"" + DATASET_NAME + ""_GPU"" + str(config[\'gpu\'][\'id\']) + ""_"" + time.strftime(\'%Hh%Mm%Ss_on_%b_%d_%Y\')\n    write_file_name = out_dir + \'results/result_\' + MODEL_NAME + ""_"" + DATASET_NAME + ""_GPU"" + str(config[\'gpu\'][\'id\']) + ""_"" + time.strftime(\'%Hh%Mm%Ss_on_%b_%d_%Y\')\n    write_config_file = out_dir + \'configs/config_\' + MODEL_NAME + ""_"" + DATASET_NAME + ""_GPU"" + str(config[\'gpu\'][\'id\']) + ""_"" + time.strftime(\'%Hh%Mm%Ss_on_%b_%d_%Y\')\n    dirs = root_log_dir, root_ckpt_dir, write_file_name, write_config_file\n\n    if not os.path.exists(out_dir + \'results\'):\n        os.makedirs(out_dir + \'results\')\n        \n    if not os.path.exists(out_dir + \'configs\'):\n        os.makedirs(out_dir + \'configs\')\n\n    net_params[\'total_param\'] = view_model_param(MODEL_NAME, net_params)\n    train_val_pipeline(MODEL_NAME, dataset, params, net_params, dirs)\n\n    \n    \n    \nmain()    \n\n\n\n\n\n\n\n\n\n\n\n'"
data/CitationGraphs.py,13,"b'import torch\nimport pickle\nimport torch.utils.data\nimport time\nimport os\nimport numpy as np\n\nimport csv\n\nimport dgl\nfrom dgl.data import CoraDataset\nfrom dgl.data import CitationGraphDataset\nimport networkx as nx\n\nimport random\nrandom.seed(42)\n\n\ndef self_loop(g):\n    """"""\n        Utility function only, to be used only when necessary as per user self_loop flag\n        : Overwriting the function dgl.transform.add_self_loop() to not miss ndata[\'feat\'] and edata[\'feat\']\n        \n        \n        This function is called inside a function in CitationGraphsDataset class.\n    """"""\n    new_g = dgl.DGLGraph()\n    new_g.add_nodes(g.number_of_nodes())\n    new_g.ndata[\'feat\'] = g.ndata[\'feat\']\n    \n    src, dst = g.all_edges(order=""eid"")\n    src = dgl.backend.zerocopy_to_numpy(src)\n    dst = dgl.backend.zerocopy_to_numpy(dst)\n    non_self_edges_idx = src != dst\n    nodes = np.arange(g.number_of_nodes())\n    new_g.add_edges(src[non_self_edges_idx], dst[non_self_edges_idx])\n    new_g.add_edges(nodes, nodes)\n    \n    # This new edata is not used since this function gets called only for GCN, GAT\n    # However, we need this for the generic requirement of ndata and edata\n    new_g.edata[\'feat\'] = torch.zeros(new_g.number_of_edges())\n    return new_g\n\n\n\n    \nclass CitationGraphsDataset(torch.utils.data.Dataset):\n    def __init__(self, name):\n        t0 = time.time()\n        self.name = name.lower()\n        \n        if self.name == \'cora\':\n            dataset = CoraDataset()\n        else:\n            dataset = CitationGraphDataset(self.name)\n        dataset.graph.remove_edges_from(nx.selfloop_edges(dataset.graph))\n        graph = dgl.DGLGraph(dataset.graph)\n        E = graph.number_of_edges()\n        N = graph.number_of_nodes()\n        D = dataset.features.shape[1]\n        graph.ndata[\'feat\'] = torch.Tensor(dataset.features)\n        graph.edata[\'feat\'] = torch.zeros((E, D))\n        graph.batch_num_nodes = [N]\n\n\n        self.norm_n = torch.FloatTensor(N,1).fill_(1./float(N)).sqrt()\n        self.norm_e = torch.FloatTensor(E,1).fill_(1./float(E)).sqrt()\n        self.graph = graph\n        self.train_mask = torch.BoolTensor(dataset.train_mask)\n        self.val_mask = torch.BoolTensor(dataset.val_mask)\n        self.test_mask = torch.BoolTensor(dataset.test_mask)\n        self.labels = torch.LongTensor(dataset.labels)\n        self.num_classes = dataset.num_labels\n        self.num_dims = D\n\n\n\n        print(""[!] Dataset: "", self.name)\n\n        \n        print(""Time taken: {:.4f}s"".format(time.time()-t0))\n    \n    \n    def _add_self_loops(self):\n        # function for adding self loops\n        # this function will be called only if self_loop flag is True\n        self.graph = self_loop(self.graph)\n        norm = torch.pow(self.graph.in_degrees().float().clamp(min=1), -0.5)\n        shp = norm.shape + (1,) * (self.graph.ndata[\'feat\'].dim() - 1)\n        self.norm_n = torch.reshape(norm, shp)\n\n'"
data/SBMs.py,10,"b'\nimport time\nimport os\nimport pickle\nimport numpy as np\n\nimport dgl\nimport torch\n\n\n\n\nclass load_SBMsDataSetDGL(torch.utils.data.Dataset):\n\n    def __init__(self,\n                 data_dir,\n                 name,\n                 split):\n\n        self.split = split\n        self.is_test = split.lower() in [\'test\', \'val\'] \n        with open(os.path.join(data_dir, name + \'_%s.pkl\' % self.split), \'rb\') as f:\n            self.dataset = pickle.load(f)\n        self.node_labels = []\n        self.graph_lists = []\n        self.n_samples = len(self.dataset)\n        self._prepare()\n    \n\n    def _prepare(self):\n\n        print(""preparing %d graphs for the %s set..."" % (self.n_samples, self.split.upper()))\n\n        for data in self.dataset:\n\n            node_features = data.node_feat\n            edge_list = (data.W != 0).nonzero()  # converting adj matrix to edge_list\n\n            # Create the DGL Graph\n            g = dgl.DGLGraph()\n            g.add_nodes(node_features.size(0))\n            g.ndata[\'feat\'] = node_features.long()\n            for src, dst in edge_list:\n                g.add_edges(src.item(), dst.item())\n\n            # adding edge features for Residual Gated ConvNet\n            #edge_feat_dim = g.ndata[\'feat\'].size(1) # dim same as node feature dim\n            edge_feat_dim = 1 # dim same as node feature dim\n            g.edata[\'feat\'] = torch.ones(g.number_of_edges(), edge_feat_dim)\n\n            self.graph_lists.append(g)\n            self.node_labels.append(data.node_label)\n\n\n    def __len__(self):\n        """"""Return the number of graphs in the dataset.""""""\n        return self.n_samples\n\n    def __getitem__(self, idx):\n        """"""\n            Get the idx^th sample.\n            Parameters\n            ---------\n            idx : int\n                The sample index.\n            Returns\n            -------\n            (dgl.DGLGraph, int)\n                DGLGraph with node feature stored in `feat` field\n                And its label.\n        """"""\n        return self.graph_lists[idx], self.node_labels[idx]\n\n\nclass SBMsDatasetDGL(torch.utils.data.Dataset):\n\n    def __init__(self, name):\n        """"""\n            TODO\n        """"""\n        start = time.time()\n        print(""[I] Loading data ..."")\n        self.name = name\n        data_dir = \'data/SBMs\'\n        self.train = load_SBMsDataSetDGL(data_dir, name, split=\'train\')\n        self.test = load_SBMsDataSetDGL(data_dir, name, split=\'test\')\n        self.val = load_SBMsDataSetDGL(data_dir, name, split=\'val\')\n        print(""[I] Finished loading."")\n        print(""[I] Data load time: {:.4f}s"".format(time.time()-start))\n\n\n\n\ndef self_loop(g):\n    """"""\n        Utility function only, to be used only when necessary as per user self_loop flag\n        : Overwriting the function dgl.transform.add_self_loop() to not miss ndata[\'feat\'] and edata[\'feat\']\n        \n        \n        This function is called inside a function in SBMsDataset class.\n    """"""\n    new_g = dgl.DGLGraph()\n    new_g.add_nodes(g.number_of_nodes())\n    new_g.ndata[\'feat\'] = g.ndata[\'feat\']\n    \n    src, dst = g.all_edges(order=""eid"")\n    src = dgl.backend.zerocopy_to_numpy(src)\n    dst = dgl.backend.zerocopy_to_numpy(dst)\n    non_self_edges_idx = src != dst\n    nodes = np.arange(g.number_of_nodes())\n    new_g.add_edges(src[non_self_edges_idx], dst[non_self_edges_idx])\n    new_g.add_edges(nodes, nodes)\n    \n    # This new edata is not used since this function gets called only for GCN, GAT\n    # However, we need this for the generic requirement of ndata and edata\n    new_g.edata[\'feat\'] = torch.zeros(new_g.number_of_edges())\n    return new_g\n\n\n\nclass SBMsDataset(torch.utils.data.Dataset):\n\n    def __init__(self, name):\n        """"""\n            Loading SBM datasets\n        """"""\n        start = time.time()\n        print(""[I] Loading dataset %s..."" % (name))\n        self.name = name\n        data_dir = \'data/SBMs/\'\n        with open(data_dir+name+\'.pkl\',""rb"") as f:\n            f = pickle.load(f)\n            self.train = f[0]\n            self.val = f[1]\n            self.test = f[2]\n        print(\'train, test, val sizes :\',len(self.train),len(self.test),len(self.val))\n        print(""[I] Finished loading."")\n        print(""[I] Data load time: {:.4f}s"".format(time.time()-start))\n\n\n    # form a mini batch from a given list of samples = [(graph, label) pairs]\n    def collate(self, samples):\n        # The input samples is a list of pairs (graph, label).\n        graphs, labels = map(list, zip(*samples))\n        labels = torch.cat(labels).long()\n        tab_sizes_n = [ graphs[i].number_of_nodes() for i in range(len(graphs))]\n        tab_snorm_n = [ torch.FloatTensor(size,1).fill_(1./float(size)) for size in tab_sizes_n ]\n        snorm_n = torch.cat(tab_snorm_n).sqrt()  \n        tab_sizes_e = [ graphs[i].number_of_edges() for i in range(len(graphs))]\n        tab_snorm_e = [ torch.FloatTensor(size,1).fill_(1./float(size)) for size in tab_sizes_e ]\n        snorm_e = torch.cat(tab_snorm_e).sqrt()\n        batched_graph = dgl.batch(graphs)\n\n        return batched_graph, labels, snorm_n, snorm_e\n\n    def _add_self_loops(self):\n        \n        # function for adding self loops\n        # this function will be called only if self_loop flag is True\n            \n        self.train.graph_lists = [self_loop(g) for g in self.train.graph_lists]\n        self.val.graph_lists = [self_loop(g) for g in self.val.graph_lists]\n        self.test.graph_lists = [self_loop(g) for g in self.test.graph_lists]\n\n\n\n\n'"
data/TSP.py,9,"b'import time\nimport pickle\nimport numpy as np\nimport itertools\nfrom scipy.spatial.distance import pdist, squareform\n\nimport dgl\nimport torch\nfrom torch.utils.data import Dataset\n\n\nclass TSP(Dataset):\n    def __init__(self, data_dir, split=""train"", num_neighbors=25, max_samples=10000):    \n        self.data_dir = data_dir\n        self.split = split\n        self.filename = f\'{data_dir}/tsp50-500_{split}.txt\'\n        self.max_samples = max_samples\n        self.num_neighbors = num_neighbors\n        self.is_test = split.lower() in [\'test\', \'val\']\n        \n        self.graph_lists = []\n        self.edge_labels = []\n        self._prepare()\n        self.n_samples = len(self.edge_labels)\n    \n    def _prepare(self):\n        print(\'preparing all graphs for the %s set...\' % self.split.upper())\n        \n        file_data = open(self.filename, ""r"").readlines()[:self.max_samples]\n        \n        for graph_idx, line in enumerate(file_data):\n            line = line.split("" "")  # Split into list\n            num_nodes = int(line.index(\'output\')//2)\n            \n            # Convert node coordinates to required format\n            nodes_coord = []\n            for idx in range(0, 2 * num_nodes, 2):\n                nodes_coord.append([float(line[idx]), float(line[idx + 1])])\n\n            # Compute distance matrix\n            W_val = squareform(pdist(nodes_coord, metric=\'euclidean\'))\n            # Determine k-nearest neighbors for each node\n            knns = np.argpartition(W_val, kth=self.num_neighbors, axis=-1)[:, self.num_neighbors::-1]\n\n            # Convert tour nodes to required format\n            # Don\'t add final connection for tour/cycle\n            tour_nodes = [int(node) - 1 for node in line[line.index(\'output\') + 1:-1]][:-1]\n\n            # Compute an edge adjacency matrix representation of tour\n            edges_target = np.zeros((num_nodes, num_nodes))\n            for idx in range(len(tour_nodes) - 1):\n                i = tour_nodes[idx]\n                j = tour_nodes[idx + 1]\n                edges_target[i][j] = 1\n                edges_target[j][i] = 1\n            # Add final connection of tour in edge target\n            edges_target[j][tour_nodes[0]] = 1\n            edges_target[tour_nodes[0]][j] = 1\n            \n            # Construct the DGL graph\n            g = dgl.DGLGraph()\n            g.add_nodes(num_nodes)\n            g.ndata[\'feat\'] = torch.Tensor(nodes_coord)\n            \n            edge_feats = []  # edge features i.e. euclidean distances between nodes\n            edge_labels = []  # edges_targets as a list\n            # Important!: order of edge_labels must be the same as the order of edges in DGLGraph g\n            # We ensure this by adding them together\n            for idx in range(num_nodes):\n                for n_idx in knns[idx]:\n                    if n_idx != idx:  # No self-connection\n                        g.add_edge(idx, n_idx)\n                        edge_feats.append(W_val[idx][n_idx])\n                        edge_labels.append(int(edges_target[idx][n_idx]))\n            # dgl.transform.remove_self_loop(g)\n            \n            # Sanity check\n            assert len(edge_feats) == g.number_of_edges() == len(edge_labels)\n            \n            # Add edge features\n            g.edata[\'feat\'] = torch.Tensor(edge_feats).unsqueeze(-1)\n            \n            # # Uncomment to add dummy edge features instead (for Residual Gated ConvNet)\n            # edge_feat_dim = g.ndata[\'feat\'].shape[1] # dim same as node feature dim\n            # g.edata[\'feat\'] = torch.ones(g.number_of_edges(), edge_feat_dim)\n            \n            self.graph_lists.append(g)\n            self.edge_labels.append(edge_labels)\n\n    def __len__(self):\n        """"""Return the number of graphs in the dataset.""""""\n        return self.n_samples\n\n    def __getitem__(self, idx):\n        """"""\n            Get the idx^th sample.\n            Parameters\n            ---------\n            idx : int\n                The sample index.\n            Returns\n            -------\n            (dgl.DGLGraph, list)\n                DGLGraph with node feature stored in `feat` field\n                And a list of labels for each edge in the DGLGraph.\n        """"""\n        return self.graph_lists[idx], self.edge_labels[idx]\n\n\nclass TSPDatasetDGL(Dataset):\n    def __init__(self, name):\n        self.name = name\n        self.train = TSP(data_dir=\'./data/TSP\', split=\'train\', num_neighbors=25, max_samples=10000) \n        self.val = TSP(data_dir=\'./data/TSP\', split=\'val\', num_neighbors=25, max_samples=1000)\n        self.test = TSP(data_dir=\'./data/TSP\', split=\'test\', num_neighbors=25, max_samples=1000)\n        \n\nclass TSPDataset(Dataset):\n    def __init__(self, name):\n        start = time.time()\n        print(""[I] Loading dataset %s..."" % (name))\n        self.name = name\n        data_dir = \'data/TSP/\'\n        with open(data_dir+name+\'.pkl\',""rb"") as f:\n            f = pickle.load(f)\n            self.train = f[0]\n            self.test = f[1]\n            self.val = f[2]\n        print(\'train, test, val sizes :\',len(self.train),len(self.test),len(self.val))\n        print(""[I] Finished loading."")\n        print(""[I] Data load time: {:.4f}s"".format(time.time()-start))\n    \n    # form a mini batch from a given list of samples = [(graph, label) pairs]\n    def collate(self, samples):\n        # The input samples is a list of pairs (graph, label).\n        graphs, labels = map(list, zip(*samples))\n        # Edge classification labels need to be flattened to 1D lists\n        labels = torch.LongTensor(np.array(list(itertools.chain(*labels))))\n        tab_sizes_n = [ graphs[i].number_of_nodes() for i in range(len(graphs))]\n        tab_snorm_n = [ torch.FloatTensor(size,1).fill_(1./float(size)) for size in tab_sizes_n ]\n        snorm_n = torch.cat(tab_snorm_n).sqrt()  \n        tab_sizes_e = [ graphs[i].number_of_edges() for i in range(len(graphs))]\n        tab_snorm_e = [ torch.FloatTensor(size,1).fill_(1./float(size)) for size in tab_sizes_e ]\n        snorm_e = torch.cat(tab_snorm_e).sqrt()\n        batched_graph = dgl.batch(graphs)\n\n        return batched_graph, labels, snorm_n, snorm_e\n    \n    \n    def _add_self_loops(self):\n        """"""\n           No self-loop support since TSP edge classification dataset. \n        """"""\n        raise NotImplementedError\n        \n        '"
data/TUs.py,11,"b'import torch\nimport pickle\nimport torch.utils.data\nimport time\nimport os\nimport numpy as np\n\nimport csv\n\nimport dgl\nfrom dgl.data import TUDataset\nfrom dgl.data import LegacyTUDataset\n\n\nimport random\nrandom.seed(42)\n\nfrom sklearn.model_selection import StratifiedKFold, train_test_split\n\nimport csv\n\n\n\n\ndef get_all_split_idx(dataset):\n    """"""\n        - Split total number of graphs into 3 (train, val and test) in 80:10:10\n        - Stratified split proportionate to original distribution of data with respect to classes\n        - Using sklearn to perform the split and then save the indexes\n        - Preparing 10 such combinations of indexes split to be used in Graph NNs\n        - As with KFold, each of the 10 fold have unique test set.\n    """"""\n    root_idx_dir = \'./data/TUs/\'\n    if not os.path.exists(root_idx_dir):\n        os.makedirs(root_idx_dir)\n    all_idx = {}\n    \n    # If there are no idx files, do the split and store the files\n    if not (os.path.exists(root_idx_dir + dataset.name + \'_train.index\')):\n        print(""[!] Splitting the data into train/val/test ..."")\n        \n        # Using 10-fold cross val to compare with benchmark papers\n        k_splits = 10\n\n        cross_val_fold = StratifiedKFold(n_splits=k_splits, shuffle=True)\n        k_data_splits = []\n        \n        # this is a temporary index assignment, to be used below for val splitting\n        for i in range(len(dataset.graph_lists)):\n            dataset[i][0].a = lambda: None\n            setattr(dataset[i][0].a, \'index\', i)\n            \n        for indexes in cross_val_fold.split(dataset.graph_lists, dataset.graph_labels):\n            remain_index, test_index = indexes[0], indexes[1]    \n\n            remain_set = format_dataset([dataset[index] for index in remain_index])\n\n            # Gets final \'train\' and \'val\'\n            train, val, _, __ = train_test_split(remain_set,\n                                                    range(len(remain_set.graph_lists)),\n                                                    test_size=0.111,\n                                                    stratify=remain_set.graph_labels)\n\n            train, val = format_dataset(train), format_dataset(val)\n            test = format_dataset([dataset[index] for index in test_index])\n\n            # Extracting only idxs\n            idx_train = [item[0].a.index for item in train]\n            idx_val = [item[0].a.index for item in val]\n            idx_test = [item[0].a.index for item in test]\n\n            f_train_w = csv.writer(open(root_idx_dir + dataset.name + \'_train.index\', \'a+\'))\n            f_val_w = csv.writer(open(root_idx_dir + dataset.name + \'_val.index\', \'a+\'))\n            f_test_w = csv.writer(open(root_idx_dir + dataset.name + \'_test.index\', \'a+\'))\n            \n            f_train_w.writerow(idx_train)\n            f_val_w.writerow(idx_val)\n            f_test_w.writerow(idx_test)\n\n        print(""[!] Splitting done!"")\n        \n    # reading idx from the files\n    for section in [\'train\', \'val\', \'test\']:\n        with open(root_idx_dir + dataset.name + \'_\'+ section + \'.index\', \'r\') as f:\n            reader = csv.reader(f)\n            all_idx[section] = [list(map(int, idx)) for idx in reader]\n    return all_idx\n\nclass DGLFormDataset(torch.utils.data.Dataset):\n    """"""\n        DGLFormDataset wrapping graph list and label list as per pytorch Dataset.\n        *lists (list): lists of \'graphs\' and \'labels\' with same len().\n    """"""\n    def __init__(self, *lists):\n        assert all(len(lists[0]) == len(li) for li in lists)\n        self.lists = lists\n        self.graph_lists = lists[0]\n        self.graph_labels = lists[1]\n\n    def __getitem__(self, index):\n        return tuple(li[index] for li in self.lists)\n\n    def __len__(self):\n        return len(self.lists[0])\n\n\n\ndef self_loop(g):\n    """"""\n        Utility function only, to be used only when necessary as per user self_loop flag\n        : Overwriting the function dgl.transform.add_self_loop() to not miss ndata[\'feat\'] and edata[\'feat\']\n        \n        \n        This function is called inside a function in TUsDataset class.\n    """"""\n    new_g = dgl.DGLGraph()\n    new_g.add_nodes(g.number_of_nodes())\n    new_g.ndata[\'feat\'] = g.ndata[\'feat\']\n    \n    src, dst = g.all_edges(order=""eid"")\n    src = dgl.backend.zerocopy_to_numpy(src)\n    dst = dgl.backend.zerocopy_to_numpy(dst)\n    non_self_edges_idx = src != dst\n    nodes = np.arange(g.number_of_nodes())\n    new_g.add_edges(src[non_self_edges_idx], dst[non_self_edges_idx])\n    new_g.add_edges(nodes, nodes)\n    \n    # This new edata is not used since this function gets called only for GCN, GAT\n    # However, we need this for the generic requirement of ndata and edata\n    new_g.edata[\'feat\'] = torch.zeros(new_g.number_of_edges())\n    return new_g\n\n\n\n    \nclass TUsDataset(torch.utils.data.Dataset):\n    def __init__(self, name):\n        t0 = time.time()\n        self.name = name\n        \n        #dataset = TUDataset(self.name, hidden_size=1)\n        dataset = LegacyTUDataset(self.name, hidden_size=1) # dgl 4.0\n\n        # frankenstein has labels 0 and 2; so correcting them as 0 and 1\n        if self.name == ""FRANKENSTEIN"":\n            dataset.graph_labels = np.array([1 if x==2 else x for x in dataset.graph_labels])\n\n        print(""[!] Dataset: "", self.name)\n\n        # this function splits data into train/val/test and returns the indices\n        self.all_idx = get_all_split_idx(dataset)\n        \n        self.all = dataset\n        self.train = [self.format_dataset([dataset[idx] for idx in self.all_idx[\'train\'][split_num]]) for split_num in range(10)]\n        self.val = [self.format_dataset([dataset[idx] for idx in self.all_idx[\'val\'][split_num]]) for split_num in range(10)]\n        self.test = [self.format_dataset([dataset[idx] for idx in self.all_idx[\'test\'][split_num]]) for split_num in range(10)]\n        \n        print(""Time taken: {:.4f}s"".format(time.time()-t0))\n    \n    def format_dataset(self, dataset):  \n        """"""\n            Utility function to recover data,\n            INTO-> dgl/pytorch compatible format \n        """"""\n        graphs = [data[0] for data in dataset]\n        labels = [data[1] for data in dataset]\n\n        for graph in graphs:\n            #graph.ndata[\'feat\'] = torch.FloatTensor(graph.ndata[\'feat\'])\n            graph.ndata[\'feat\'] = graph.ndata[\'feat\'].float() # dgl 4.0\n            # adding edge features for Residual Gated ConvNet, if not there\n            if \'feat\' not in graph.edata.keys():\n                edge_feat_dim = graph.ndata[\'feat\'].shape[1] # dim same as node feature dim\n                graph.edata[\'feat\'] = torch.ones(graph.number_of_edges(), edge_feat_dim)\n\n        return DGLFormDataset(graphs, labels)\n    \n    \n    # form a mini batch from a given list of samples = [(graph, label) pairs]\n    def collate(self, samples):\n        # The input samples is a list of pairs (graph, label).\n        graphs, labels = map(list, zip(*samples))\n        labels = torch.tensor(np.array(labels))\n        tab_sizes_n = [ graphs[i].number_of_nodes() for i in range(len(graphs))]\n        tab_snorm_n = [ torch.FloatTensor(size,1).fill_(1./float(size)) for size in tab_sizes_n ]\n        snorm_n = torch.cat(tab_snorm_n).sqrt()  \n        tab_sizes_e = [ graphs[i].number_of_edges() for i in range(len(graphs))]\n        tab_snorm_e = [ torch.FloatTensor(size,1).fill_(1./float(size)) for size in tab_sizes_e ]\n        snorm_e = torch.cat(tab_snorm_e).sqrt()\n        batched_graph = dgl.batch(graphs)\n        return batched_graph, labels, snorm_n, snorm_e\n    \n    \n    def _add_self_loops(self):\n\n        # function for adding self loops\n        # this function will be called only if self_loop flag is True\n        for split_num in range(10):\n            self.train[split_num].graph_lists = [self_loop(g) for g in self.train[split_num].graph_lists]\n            self.val[split_num].graph_lists = [self_loop(g) for g in self.val[split_num].graph_lists]\n            self.test[split_num].graph_lists = [self_loop(g) for g in self.test[split_num].graph_lists]\n            \n        for split_num in range(10):\n            self.train[split_num] = DGLFormDataset(self.train[split_num].graph_lists, self.train[split_num].graph_labels)\n            self.val[split_num] = DGLFormDataset(self.val[split_num].graph_lists, self.val[split_num].graph_labels)\n            self.test[split_num] = DGLFormDataset(self.test[split_num].graph_lists, self.test[split_num].graph_labels)\n'"
data/data.py,0,"b'""""""\n    File to load dataset based on user control from main file\n""""""\nfrom data.superpixels import SuperPixDataset\nfrom data.molecules import MoleculeDataset\nfrom data.TUs import TUsDataset\nfrom data.SBMs import SBMsDataset\nfrom data.TSP import TSPDataset\nfrom data.CitationGraphs import CitationGraphsDataset\n\ndef LoadData(DATASET_NAME):\n    """"""\n        This function is called in the main.py file \n        returns:\n        ; dataset object\n    """"""\n    # handling for MNIST or CIFAR Superpixels\n    if DATASET_NAME == \'MNIST\' or DATASET_NAME == \'CIFAR10\':\n        return SuperPixDataset(DATASET_NAME)\n    \n    # handling for (ZINC) molecule dataset\n    if DATASET_NAME == \'ZINC\':\n        return MoleculeDataset(DATASET_NAME)\n\n    # handling for the TU Datasets\n    TU_DATASETS = [\'COLLAB\', \'ENZYMES\', \'DD\', \'PROTEINS_full\']\n    if DATASET_NAME in TU_DATASETS: \n        return TUsDataset(DATASET_NAME)\n\n    # handling for SBM datasets\n    SBM_DATASETS = [\'SBM_CLUSTER\', \'SBM_PATTERN\']\n    if DATASET_NAME in SBM_DATASETS: \n        return SBMsDataset(DATASET_NAME)\n    \n    # handling for TSP dataset\n    if DATASET_NAME == \'TSP\':\n        return TSPDataset(DATASET_NAME)\n\n    # handling for the CITATIONGRAPHS Datasets\n    CITATIONGRAPHS_DATASETS = [\'CORA\', \'CITESEER\', \'PUBMED\']\n    if DATASET_NAME in CITATIONGRAPHS_DATASETS: \n        return CitationGraphsDataset(DATASET_NAME)\n'"
data/molecules.py,10,"b'import torch\nimport pickle\nimport torch.utils.data\nimport time\nimport os\nimport numpy as np\n\nimport csv\n\nimport dgl\n\n# *NOTE\n# The dataset pickle and index files are in ./zinc_molecules/ dir\n# [<split>.pickle and <split>.index; for split \'train\', \'val\' and \'test\']\n\n\n\n\nclass MoleculeDGL(torch.utils.data.Dataset):\n    def __init__(self, data_dir, split, num_graphs):\n        self.data_dir = data_dir\n        self.split = split\n        self.num_graphs = num_graphs\n        \n        with open(data_dir + ""/%s.pickle"" % self.split,""rb"") as f:\n            self.data = pickle.load(f)\n\n        # loading the sampled indices from file ./zinc_molecules/<split>.index\n        with open(data_dir + ""/%s.index"" % self.split,""r"") as f:\n            data_idx = [list(map(int, idx)) for idx in csv.reader(f)]\n            self.data = [ self.data[i] for i in data_idx[0] ]\n            \n        assert len(self.data)==num_graphs, ""Sample num_graphs again; available idx: train/val/test => 10k/1k/1k""\n        \n        """"""\n        data is a list of Molecule dict objects with following attributes\n        \n          molecule = data[idx]\n        ; molecule[\'num_atom\'] : nb of atoms, an integer (N)\n        ; molecule[\'atom_type\'] : tensor of size N, each element is an atom type, an integer between 0 and num_atom_type\n        ; molecule[\'bond_type\'] : tensor of size N x N, each element is a bond type, an integer between 0 and num_bond_type\n        ; molecule[\'logP_SA_cycle_normalized\'] : the chemical property to regress, a float variable\n        """"""\n        \n        self.graph_lists = []\n        self.graph_labels = []\n        self.n_samples = len(self.data)\n        self._prepare()\n    \n    def _prepare(self):\n        print(""preparing %d graphs for the %s set..."" % (self.num_graphs, self.split.upper()))\n        \n        for molecule in self.data:\n            node_features = molecule[\'atom_type\'].long()\n            \n            adj = molecule[\'bond_type\']\n            edge_list = (adj != 0).nonzero()  # converting adj matrix to edge_list\n            \n            edge_idxs_in_adj = edge_list.split(1, dim=1)\n            edge_features = adj[edge_idxs_in_adj].reshape(-1).long()\n            \n            # Create the DGL Graph\n            g = dgl.DGLGraph()\n            g.add_nodes(molecule[\'num_atom\'])\n            g.ndata[\'feat\'] = node_features\n            \n            for src, dst in edge_list:\n                g.add_edges(src.item(), dst.item())\n            g.edata[\'feat\'] = edge_features\n            \n            self.graph_lists.append(g)\n            self.graph_labels.append(molecule[\'logP_SA_cycle_normalized\'])\n        \n    def __len__(self):\n        """"""Return the number of graphs in the dataset.""""""\n        return self.n_samples\n\n    def __getitem__(self, idx):\n        """"""\n            Get the idx^th sample.\n            Parameters\n            ---------\n            idx : int\n                The sample index.\n            Returns\n            -------\n            (dgl.DGLGraph, int)\n                DGLGraph with node feature stored in `feat` field\n                And its label.\n        """"""\n        return self.graph_lists[idx], self.graph_labels[idx]\n    \n    \nclass MoleculeDatasetDGL(torch.utils.data.Dataset):\n    def __init__(self, name=\'Zinc\'):\n        t0 = time.time()\n        self.name = name\n        \n        self.num_atom_type = 28 # known meta-info about the zinc dataset; can be calculated as well\n        self.num_bond_type = 4 # known meta-info about the zinc dataset; can be calculated as well\n        \n        data_dir=\'./data/molecules\'\n        \n        self.train = MoleculeDGL(data_dir, \'train\', num_graphs=10000)\n        self.val = MoleculeDGL(data_dir, \'val\', num_graphs=1000)\n        self.test = MoleculeDGL(data_dir, \'test\', num_graphs=1000)\n        print(""Time taken: {:.4f}s"".format(time.time()-t0))\n        \n\n\ndef self_loop(g):\n    """"""\n        Utility function only, to be used only when necessary as per user self_loop flag\n        : Overwriting the function dgl.transform.add_self_loop() to not miss ndata[\'feat\'] and edata[\'feat\']\n        \n        \n        This function is called inside a function in MoleculeDataset class.\n    """"""\n    new_g = dgl.DGLGraph()\n    new_g.add_nodes(g.number_of_nodes())\n    new_g.ndata[\'feat\'] = g.ndata[\'feat\']\n    \n    src, dst = g.all_edges(order=""eid"")\n    src = dgl.backend.zerocopy_to_numpy(src)\n    dst = dgl.backend.zerocopy_to_numpy(dst)\n    non_self_edges_idx = src != dst\n    nodes = np.arange(g.number_of_nodes())\n    new_g.add_edges(src[non_self_edges_idx], dst[non_self_edges_idx])\n    new_g.add_edges(nodes, nodes)\n    \n    # This new edata is not used since this function gets called only for GCN, GAT\n    # However, we need this for the generic requirement of ndata and edata\n    new_g.edata[\'feat\'] = torch.zeros(new_g.number_of_edges())\n    return new_g\n\n\n\nclass MoleculeDataset(torch.utils.data.Dataset):\n\n    def __init__(self, name):\n        """"""\n            Loading SBM datasets\n        """"""\n        start = time.time()\n        print(""[I] Loading dataset %s..."" % (name))\n        self.name = name\n        data_dir = \'data/molecules/\'\n        with open(data_dir+name+\'.pkl\',""rb"") as f:\n            f = pickle.load(f)\n            self.train = f[0]\n            self.val = f[1]\n            self.test = f[2]\n            self.num_atom_type = f[3]\n            self.num_bond_type = f[4]\n        print(\'train, test, val sizes :\',len(self.train),len(self.test),len(self.val))\n        print(""[I] Finished loading."")\n        print(""[I] Data load time: {:.4f}s"".format(time.time()-start))\n\n\n    # form a mini batch from a given list of samples = [(graph, label) pairs]\n    def collate(self, samples):\n        # The input samples is a list of pairs (graph, label).\n        graphs, labels = map(list, zip(*samples))\n        labels = torch.tensor(np.array(labels)).unsqueeze(1)\n        tab_sizes_n = [ graphs[i].number_of_nodes() for i in range(len(graphs))]\n        tab_snorm_n = [ torch.FloatTensor(size,1).fill_(1./float(size)) for size in tab_sizes_n ]\n        snorm_n = torch.cat(tab_snorm_n).sqrt()  \n        tab_sizes_e = [ graphs[i].number_of_edges() for i in range(len(graphs))]\n        tab_snorm_e = [ torch.FloatTensor(size,1).fill_(1./float(size)) for size in tab_sizes_e ]\n        snorm_e = torch.cat(tab_snorm_e).sqrt()\n        batched_graph = dgl.batch(graphs)\n        return batched_graph, labels, snorm_n, snorm_e\n    \n    \n    def _add_self_loops(self):\n        \n        # function for adding self loops\n        # this function will be called only if self_loop flag is True\n            \n        self.train.graph_lists = [self_loop(g) for g in self.train.graph_lists]\n        self.val.graph_lists = [self_loop(g) for g in self.val.graph_lists]\n        self.test.graph_lists = [self_loop(g) for g in self.test.graph_lists]\n\n\n\n\n'"
data/superpixels.py,16,"b'import os\nimport pickle\nfrom scipy.spatial.distance import cdist\nimport numpy as np\nimport itertools\n\nimport dgl\nimport torch\nimport torch.utils.data\n\nimport time\n\nimport csv\nfrom sklearn.model_selection import StratifiedShuffleSplit\n\n\n\n\ndef sigma(dists, kth=8):\n    # Compute sigma and reshape\n    try:\n        # Get k-nearest neighbors for each node\n        knns = np.partition(dists, kth, axis=-1)[:, kth::-1]\n        sigma = knns.sum(axis=1).reshape((knns.shape[0], 1))/kth\n    except ValueError:     # handling for graphs with num_nodes less than kth\n        num_nodes = dists.shape[0]\n        # this sigma value is irrelevant since not used for final compute_edge_list\n        sigma = np.array([1]*num_nodes).reshape(num_nodes,1)\n        \n    return sigma + 1e-8 # adding epsilon to avoid zero value of sigma\n\n\ndef compute_adjacency_matrix_images(coord, feat, use_feat=True, kth=8):\n    coord = coord.reshape(-1, 2)\n    # Compute coordinate distance\n    c_dist = cdist(coord, coord)\n    \n    if use_feat:\n        # Compute feature distance\n        f_dist = cdist(feat, feat)\n        # Compute adjacency\n        A = np.exp(- (c_dist/sigma(c_dist))**2 - (f_dist/sigma(f_dist))**2 )\n    else:\n        A = np.exp(- (c_dist/sigma(c_dist))**2)\n        \n    # Convert to symmetric matrix\n    A = 0.5 * (A + A.T)\n    A[np.diag_indices_from(A)] = 0\n    return A        \n\n\ndef compute_edges_list(A, kth=8+1):\n    # Get k-similar neighbor indices for each node\n\n    num_nodes = A.shape[0]\n    new_kth = num_nodes - kth\n    \n    if num_nodes > 9:\n        knns = np.argpartition(A, new_kth-1, axis=-1)[:, new_kth:-1]\n        knn_values = np.partition(A, new_kth-1, axis=-1)[:, new_kth:-1] # NEW\n    else:\n        # handling for graphs with less than kth nodes\n        # in such cases, the resulting graph will be fully connected\n        knns = np.tile(np.arange(num_nodes), num_nodes).reshape(num_nodes, num_nodes)\n        knn_values = A # NEW\n        \n        # removing self loop\n        if num_nodes != 1:\n            knn_values = A[knns != np.arange(num_nodes)[:,None]].reshape(num_nodes,-1) # NEW\n            knns = knns[knns != np.arange(num_nodes)[:,None]].reshape(num_nodes,-1)\n    return knns, knn_values # NEW\n\n\nclass SuperPixDGL(torch.utils.data.Dataset):\n    def __init__(self,\n                 data_dir,\n                 dataset,\n                 split,\n                 use_mean_px=True,\n                 use_coord=True):\n\n        self.split = split\n        \n        self.graph_lists = []\n        \n        if dataset == \'MNIST\':\n            self.img_size = 28\n            with open(os.path.join(data_dir, \'mnist_75sp_%s.pkl\' % split), \'rb\') as f:\n                self.labels, self.sp_data = pickle.load(f)\n                self.graph_labels = torch.LongTensor(self.labels)\n        elif dataset == \'CIFAR10\':\n            self.img_size = 32\n            with open(os.path.join(data_dir, \'cifar10_150sp_%s.pkl\' % split), \'rb\') as f:\n                self.labels, self.sp_data = pickle.load(f)\n                self.graph_labels = torch.LongTensor(self.labels)\n                \n        self.use_mean_px = use_mean_px\n        self.use_coord = use_coord\n        self.n_samples = len(self.labels)\n        \n        self._prepare()\n    \n    def _prepare(self):\n        print(""preparing %d graphs for the %s set..."" % (self.n_samples, self.split.upper()))\n        self.Adj_matrices, self.node_features, self.edges_lists, self.edge_features = [], [], [], []\n        for index, sample in enumerate(self.sp_data):\n            mean_px, coord = sample[:2]\n            \n            try:\n                coord = coord / self.img_size\n            except AttributeError:\n                VOC_has_variable_image_sizes = True\n                \n            if self.use_mean_px:\n                A = compute_adjacency_matrix_images(coord, mean_px) # using super-pixel locations + features\n            else:\n                A = compute_adjacency_matrix_images(coord, mean_px, False) # using only super-pixel locations\n            edges_list, edge_values_list = compute_edges_list(A) # NEW\n\n            N_nodes = A.shape[0]\n            \n            mean_px = mean_px.reshape(N_nodes, -1)\n            coord = coord.reshape(N_nodes, 2)\n            x = np.concatenate((mean_px, coord), axis=1)\n\n            edge_values_list = edge_values_list.reshape(-1) # NEW # TO DOUBLE-CHECK !\n            \n            self.node_features.append(x)\n            self.edge_features.append(edge_values_list) # NEW\n            self.Adj_matrices.append(A)\n            self.edges_lists.append(edges_list)\n        \n        for index in range(len(self.sp_data)):\n            g = dgl.DGLGraph()\n            g.add_nodes(self.node_features[index].shape[0])\n            g.ndata[\'feat\'] = torch.Tensor(self.node_features[index]).half() \n\n            for src, dsts in enumerate(self.edges_lists[index]):\n                # handling for 1 node where the self loop would be the only edge\n                # since, VOC Superpixels has few samples (5 samples) with only 1 node\n                if self.node_features[index].shape[0] == 1:\n                    g.add_edges(src, dsts)\n                else:\n                    g.add_edges(src, dsts[dsts!=src])\n            \n            # adding edge features for Residual Gated ConvNet\n            edge_feat_dim = g.ndata[\'feat\'].shape[1] # dim same as node feature dim\n            #g.edata[\'feat\'] = torch.ones(g.number_of_edges(), edge_feat_dim).half() \n            g.edata[\'feat\'] = torch.Tensor(self.edge_features[index]).unsqueeze(1).half()  # NEW \n\n            self.graph_lists.append(g)\n\n    def __len__(self):\n        """"""Return the number of graphs in the dataset.""""""\n        return self.n_samples\n\n    def __getitem__(self, idx):\n        """"""\n            Get the idx^th sample.\n            Parameters\n            ---------\n            idx : int\n                The sample index.\n            Returns\n            -------\n            (dgl.DGLGraph, int)\n                DGLGraph with node feature stored in `feat` field\n                And its label.\n        """"""\n        return self.graph_lists[idx], self.graph_labels[idx]\n\n\nclass DGLFormDataset(torch.utils.data.Dataset):\n    """"""\n        DGLFormDataset wrapping graph list and label list as per pytorch Dataset.\n        *lists (list): lists of \'graphs\' and \'labels\' with same len().\n    """"""\n    def __init__(self, *lists):\n        assert all(len(lists[0]) == len(li) for li in lists)\n        self.lists = lists\n        self.graph_lists = lists[0]\n        self.graph_labels = lists[1]\n\n    def __getitem__(self, index):\n        return tuple(li[index] for li in self.lists)\n\n    def __len__(self):\n        return len(self.lists[0])\n    \n    \nclass SuperPixDatasetDGL(torch.utils.data.Dataset):\n    def __init__(self, name, num_val=5000):\n        """"""\n            Takes input standard image dataset name (MNIST/CIFAR10) \n            and returns the superpixels graph.\n            \n            This class uses results from the above SuperPix class.\n            which contains the steps for the generation of the Superpixels\n            graph from a superpixel .pkl file that has been given by\n            https://github.com/bknyaz/graph_attention_pool\n            \n            Please refer the SuperPix class for details.\n        """"""\n        t_data = time.time()\n        self.name = name\n\n        use_mean_px = True # using super-pixel locations + features\n        use_mean_px = False # using only super-pixel locations\n        if use_mean_px:\n            print(\'Adj matrix defined from super-pixel locations + features\')\n        else:\n            print(\'Adj matrix defined from super-pixel locations (only)\')\n        use_coord = True\n        self.test = SuperPixDGL(""./data/superpixels"", dataset=self.name, split=\'test\', \n                            use_mean_px=use_mean_px, \n                            use_coord=use_coord)\n\n        self.train_ = SuperPixDGL(""./data/superpixels"", dataset=self.name, split=\'train\', \n                             use_mean_px=use_mean_px, \n                             use_coord=use_coord)\n\n        _val_graphs, _val_labels = self.train_[:num_val]\n        _train_graphs, _train_labels = self.train_[num_val:]\n\n        self.val = DGLFormDataset(_val_graphs, _val_labels)\n        self.train = DGLFormDataset(_train_graphs, _train_labels)\n\n        print(""[I] Data load time: {:.4f}s"".format(time.time()-t_data))\n        \n\n\ndef self_loop(g):\n    """"""\n        Utility function only, to be used only when necessary as per user self_loop flag\n        : Overwriting the function dgl.transform.add_self_loop() to not miss ndata[\'feat\'] and edata[\'feat\']\n        \n        \n        This function is called inside a function in SuperPixDataset class.\n    """"""\n    new_g = dgl.DGLGraph()\n    new_g.add_nodes(g.number_of_nodes())\n    new_g.ndata[\'feat\'] = g.ndata[\'feat\']\n    \n    src, dst = g.all_edges(order=""eid"")\n    src = dgl.backend.zerocopy_to_numpy(src)\n    dst = dgl.backend.zerocopy_to_numpy(dst)\n    non_self_edges_idx = src != dst\n    nodes = np.arange(g.number_of_nodes())\n    new_g.add_edges(src[non_self_edges_idx], dst[non_self_edges_idx])\n    new_g.add_edges(nodes, nodes)\n    \n    # This new edata is not used since this function gets called only for GCN, GAT\n    # However, we need this for the generic requirement of ndata and edata\n    new_g.edata[\'feat\'] = torch.zeros(new_g.number_of_edges())\n    return new_g\n\n    \n\nclass SuperPixDataset(torch.utils.data.Dataset):\n\n    def __init__(self, name):\n        """"""\n            Loading Superpixels datasets\n        """"""\n        start = time.time()\n        print(""[I] Loading dataset %s..."" % (name))\n        self.name = name\n        data_dir = \'data/superpixels/\'\n        with open(data_dir+name+\'.pkl\',""rb"") as f:\n            f = pickle.load(f)\n            self.train = f[0]\n            self.val = f[1]\n            self.test = f[2]\n        print(\'train, test, val sizes :\',len(self.train),len(self.test),len(self.val))\n        print(""[I] Finished loading."")\n        print(""[I] Data load time: {:.4f}s"".format(time.time()-start))\n\n\n    # form a mini batch from a given list of samples = [(graph, label) pairs]\n    def collate(self, samples):\n        # The input samples is a list of pairs (graph, label).\n        graphs, labels = map(list, zip(*samples))\n        labels = torch.tensor(np.array(labels))\n        tab_sizes_n = [ graphs[i].number_of_nodes() for i in range(len(graphs))]\n        tab_snorm_n = [ torch.FloatTensor(size,1).fill_(1./float(size)) for size in tab_sizes_n ]\n        snorm_n = torch.cat(tab_snorm_n).sqrt()  \n        tab_sizes_e = [ graphs[i].number_of_edges() for i in range(len(graphs))]\n        tab_snorm_e = [ torch.FloatTensor(size,1).fill_(1./float(size)) for size in tab_sizes_e ]\n        snorm_e = torch.cat(tab_snorm_e).sqrt()\n        for idx, graph in enumerate(graphs):\n            graphs[idx].ndata[\'feat\'] = graph.ndata[\'feat\'].float()\n            graphs[idx].edata[\'feat\'] = graph.edata[\'feat\'].float()\n        batched_graph = dgl.batch(graphs)\n        return batched_graph, labels, snorm_n, snorm_e\n    \n    def _add_self_loops(self):\n        \n        # function for adding self loops\n        # this function will be called only if self_loop flag is True\n            \n        self.train.graph_lists = [self_loop(g) for g in self.train.graph_lists]\n        self.val.graph_lists = [self_loop(g) for g in self.val.graph_lists]\n        self.test.graph_lists = [self_loop(g) for g in self.test.graph_lists]\n        \n        self.train = DGLFormDataset(self.train.graph_lists, self.train.graph_labels)\n        self.val = DGLFormDataset(self.val.graph_lists, self.val.graph_labels)\n        self.test = DGLFormDataset(self.test.graph_lists, self.test.graph_labels)\n\n                            \n\n'"
layers/diffpool_layer.py,14,"b'import torch\nimport torch.nn as nn\n\nimport numpy as np\nfrom scipy.linalg import block_diag\n\nfrom torch.autograd import Function\n\n""""""\n    DIFFPOOL:\n    Z. Ying, J. You, C. Morris, X. Ren, W. Hamilton, and J. Leskovec, \n    Hierarchical graph representation learning with differentiable pooling (NeurIPS 2018)\n    https://arxiv.org/pdf/1806.08804.pdf\n    \n    ! code started from dgl diffpool examples dir\n""""""\n\nfrom layers.graphsage_layer import GraphSageLayer\n\n\ndef masked_softmax(matrix, mask, dim=-1, memory_efficient=True,\n                   mask_fill_value=-1e32):\n    \'\'\'\n    masked_softmax for dgl batch graph\n    code snippet contributed by AllenNLP (https://github.com/allenai/allennlp)\n    \'\'\'\n    if mask is None:\n        result = torch.nn.functional.softmax(matrix, dim=dim)\n    else:\n        mask = mask.float()\n        while mask.dim() < matrix.dim():\n            mask = mask.unsqueeze(1)\n        if not memory_efficient:\n            result = torch.nn.functional.softmax(matrix * mask, dim=dim)\n            result = result * mask\n            result = result / (result.sum(dim=dim, keepdim=True) + 1e-13)\n        else:\n            masked_matrix = matrix.masked_fill((1 - mask).byte(),\n                                               mask_fill_value)\n            result = torch.nn.functional.softmax(masked_matrix, dim=dim)\n    return result\n\n\nclass EntropyLoss(nn.Module):\n    # Return Scalar\n    # loss used in diffpool\n    def forward(self, adj, anext, s_l):\n        entropy = (torch.distributions.Categorical(\n            probs=s_l).entropy()).sum(-1).mean(-1)\n        assert not torch.isnan(entropy)\n        return entropy\n\n\nclass DiffPoolLayer(nn.Module):\n\n    def __init__(self, input_dim, assign_dim, output_feat_dim,\n                 activation, dropout, aggregator_type, graph_norm, batch_norm, link_pred):\n        super().__init__()\n        self.embedding_dim = input_dim\n        self.assign_dim = assign_dim\n        self.hidden_dim = output_feat_dim\n        self.link_pred = link_pred\n        self.feat_gc = GraphSageLayer(\n            input_dim,\n            output_feat_dim,\n            activation,\n            dropout,\n            aggregator_type,\n            graph_norm,\n            batch_norm)\n        self.pool_gc = GraphSageLayer(\n            input_dim,\n            assign_dim,\n            activation,\n            dropout,\n            aggregator_type, \n            graph_norm,\n            batch_norm)\n        self.reg_loss = nn.ModuleList([])\n        self.loss_log = {}\n        self.reg_loss.append(EntropyLoss())\n\n    def forward(self, g, h, snorm_n):\n        feat = self.feat_gc(g, h, snorm_n)\n        assign_tensor = self.pool_gc(g, h, snorm_n)\n        device = feat.device\n        assign_tensor_masks = []\n        batch_size = len(g.batch_num_nodes)\n        for g_n_nodes in g.batch_num_nodes:\n            mask = torch.ones((g_n_nodes,\n                               int(assign_tensor.size()[1] / batch_size)))\n            assign_tensor_masks.append(mask)\n        """"""\n        The first pooling layer is computed on batched graph.\n        We first take the adjacency matrix of the batched graph, which is block-wise diagonal.\n        We then compute the assignment matrix for the whole batch graph, which will also be block diagonal\n        """"""\n        mask = torch.FloatTensor(\n            block_diag(\n                *\n                assign_tensor_masks)).to(\n            device=device)\n        assign_tensor = masked_softmax(assign_tensor, mask,\n                                       memory_efficient=False)\n        h = torch.matmul(torch.t(assign_tensor), feat)                     # equation (3) of DIFFPOOL paper\n        adj = g.adjacency_matrix(ctx=device)\n        \n        adj_new = torch.sparse.mm(adj, assign_tensor)                      \n        adj_new = torch.mm(torch.t(assign_tensor), adj_new)                # equation (4) of DIFFPOOL paper\n\n        if self.link_pred:\n            current_lp_loss = torch.norm(adj.to_dense() -\n                                         torch.mm(assign_tensor, torch.t(assign_tensor))) / np.power(g.number_of_nodes(), 2)\n            self.loss_log[\'LinkPredLoss\'] = current_lp_loss\n\n        for loss_layer in self.reg_loss:\n            loss_name = str(type(loss_layer).__name__)\n            self.loss_log[loss_name] = loss_layer(adj, adj_new, assign_tensor)\n\n        return adj_new, h\n\n\n\n\n'"
layers/gat_layer.py,6,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom dgl.nn.pytorch import GATConv\n\n""""""\n    GAT: Graph Attention Network\n    Graph Attention Networks (Veli\xc4\x8dkovi\xc4\x87 et al., ICLR 2018)\n    https://arxiv.org/abs/1710.10903\n""""""\n\nclass GATHeadLayer(nn.Module):\n    def __init__(self, in_dim, out_dim, dropout, graph_norm, batch_norm):\n        super().__init__()\n        self.dropout = dropout\n        self.graph_norm = graph_norm\n        self.batch_norm = batch_norm\n        \n        self.fc = nn.Linear(in_dim, out_dim, bias=False)\n        self.attn_fc = nn.Linear(2 * out_dim, 1, bias=False)\n        self.batchnorm_h = nn.BatchNorm1d(out_dim)\n\n    def edge_attention(self, edges):\n        z2 = torch.cat([edges.src[\'z\'], edges.dst[\'z\']], dim=1)\n        a = self.attn_fc(z2)\n        return {\'e\': F.leaky_relu(a)}\n\n    def message_func(self, edges):\n        return {\'z\': edges.src[\'z\'], \'e\': edges.data[\'e\']}\n\n    def reduce_func(self, nodes):\n        alpha = F.softmax(nodes.mailbox[\'e\'], dim=1)\n        alpha = F.dropout(alpha, self.dropout, training=self.training)\n        h = torch.sum(alpha * nodes.mailbox[\'z\'], dim=1)\n        return {\'h\': h}\n\n    def forward(self, g, h, snorm_n):\n        z = self.fc(h)\n        g.ndata[\'z\'] = z\n        g.apply_edges(self.edge_attention)\n        g.update_all(self.message_func, self.reduce_func)\n        h = g.ndata[\'h\']\n        if self.graph_norm:\n            h = h * snorm_n\n        if self.batch_norm:\n            h = self.batchnorm_h(h)\n        h = F.elu(h)\n        h = F.dropout(h, self.dropout, training=self.training)\n        return h\n\nclass GATLayer(nn.Module):\n    """"""\n        Param: [in_dim, out_dim, n_heads]\n    """"""\n    def __init__(self, in_dim, out_dim, num_heads, dropout, graph_norm, batch_norm, residual=False, activation=None, dgl_builtin=False):\n\n        super().__init__()\n        self.dgl_builtin = dgl_builtin\n\n        if dgl_builtin == False:\n            self.in_channels = in_dim\n            self.out_channels = out_dim\n            self.num_heads = num_heads\n            self.residual = residual\n            \n            if in_dim != (out_dim*num_heads):\n                self.residual = False\n            \n            self.heads = nn.ModuleList()\n            for i in range(num_heads):\n                self.heads.append(GATHeadLayer(in_dim, out_dim, dropout, graph_norm, batch_norm))\n            self.merge = \'cat\' \n\n        else:\n            self.in_channels = in_dim\n            self.out_channels = out_dim\n            self.num_heads = num_heads\n            self.residual = residual\n            self.activation = activation\n            self.graph_norm = graph_norm\n            self.batch_norm = batch_norm\n            \n            if in_dim != (out_dim*num_heads):\n                self.residual = False\n\n            # Both feat and weighting dropout tied together here\n            self.conv = GATConv(in_dim, out_dim, num_heads, dropout, dropout)\n            self.batchnorm_h = nn.BatchNorm1d(out_dim)\n\n\n\n    def forward(self, g, h, snorm_n):\n        if self.dgl_builtin == False:\n            h_in = h # for residual connection\n            head_outs = [attn_head(g, h, snorm_n) for attn_head in self.heads]\n            \n            if self.merge == \'cat\':\n                h = torch.cat(head_outs, dim=1)\n            else:\n                h = torch.mean(torch.stack(head_outs))\n            \n            if self.residual:\n                h = h_in + h # residual connection\n            return h\n        else:\n            h_in = h # for residual connection\n\n            h = self.conv(g, h).flatten(1)\n\n            if self.graph_norm:\n                h = h * snorm_n\n            if self.batch_norm:\n                h = self.batchnorm_h(h)\n            \n            if self.residual:\n                h = h_in + h # residual connection\n\n            if self.activation:\n                h = self.activation(h)\n            return h\n        \n    def __repr__(self):\n        return \'{}(in_channels={}, out_channels={}, heads={}, residual={})\'.format(self.__class__.__name__,\n                                             self.in_channels,\n                                             self.out_channels, self.num_heads, self.residual)\n\n\n'"
layers/gated_gcn_layer.py,5,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n""""""\n    ResGatedGCN: Residual Gated Graph ConvNets\n    An Experimental Study of Neural Networks for Variable Graphs (Xavier Bresson and Thomas Laurent, ICLR 2018)\n    https://arxiv.org/pdf/1711.07553v2.pdf\n""""""\n\nclass GatedGCNLayer(nn.Module):\n    """"""\n        Param: []\n    """"""\n    def __init__(self, input_dim, output_dim, dropout, graph_norm, batch_norm, residual=False):\n        super().__init__()\n        self.in_channels = input_dim\n        self.out_channels = output_dim\n        self.dropout = dropout\n        self.graph_norm = graph_norm\n        self.batch_norm = batch_norm\n        self.residual = residual\n        \n        if input_dim != output_dim:\n            self.residual = False\n        \n        self.A = nn.Linear(input_dim, output_dim, bias=True)\n        self.B = nn.Linear(input_dim, output_dim, bias=True)\n        self.C = nn.Linear(input_dim, output_dim, bias=True)\n        self.D = nn.Linear(input_dim, output_dim, bias=True)\n        self.E = nn.Linear(input_dim, output_dim, bias=True)\n        self.bn_node_h = nn.BatchNorm1d(output_dim)\n        self.bn_node_e = nn.BatchNorm1d(output_dim)\n\n    def message_func(self, edges):\n        Bh_j = edges.src[\'Bh\']    \n        e_ij = edges.data[\'Ce\'] +  edges.src[\'Dh\'] + edges.dst[\'Eh\'] # e_ij = Ce_ij + Dhi + Ehj\n        edges.data[\'e\'] = e_ij\n        return {\'Bh_j\' : Bh_j, \'e_ij\' : e_ij}\n\n    def reduce_func(self, nodes):\n        Ah_i = nodes.data[\'Ah\']\n        Bh_j = nodes.mailbox[\'Bh_j\']\n        e = nodes.mailbox[\'e_ij\'] \n        sigma_ij = torch.sigmoid(e) # sigma_ij = sigmoid(e_ij)\n        #h = Ah_i + torch.mean( sigma_ij * Bh_j, dim=1 ) # hi = Ahi + mean_j alpha_ij * Bhj \n        h = Ah_i + torch.sum( sigma_ij * Bh_j, dim=1 ) / ( torch.sum( sigma_ij, dim=1 ) + 1e-6 )  # hi = Ahi + sum_j eta_ij/sum_j\' eta_ij\' * Bhj <= dense attention       \n        return {\'h\' : h}\n    \n    def forward(self, g, h, e, snorm_n, snorm_e):\n        \n        h_in = h # for residual connection\n        e_in = e # for residual connection\n        \n        g.ndata[\'h\']  = h \n        g.ndata[\'Ah\'] = self.A(h) \n        g.ndata[\'Bh\'] = self.B(h) \n        g.ndata[\'Dh\'] = self.D(h)\n        g.ndata[\'Eh\'] = self.E(h) \n        g.edata[\'e\']  = e \n        g.edata[\'Ce\'] = self.C(e) \n        g.update_all(self.message_func,self.reduce_func) \n        h = g.ndata[\'h\'] # result of graph convolution\n        e = g.edata[\'e\'] # result of graph convolution\n        \n        if self.graph_norm:\n            h = h* snorm_n # normalize activation w.r.t. graph size\n            e = e* snorm_e # normalize activation w.r.t. graph size\n        \n        if self.batch_norm:\n            h = self.bn_node_h(h) # batch normalization  \n            e = self.bn_node_e(e) # batch normalization  \n        \n        h = F.relu(h) # non-linear activation\n        e = F.relu(e) # non-linear activation\n        \n        if self.residual:\n            h = h_in + h # residual connection\n            e = e_in + e # residual connection\n        \n        h = F.dropout(h, self.dropout, training=self.training)\n        e = F.dropout(e, self.dropout, training=self.training)\n        \n        return h, e\n    \n    def __repr__(self):\n        return \'{}(in_channels={}, out_channels={})\'.format(self.__class__.__name__,\n                                             self.in_channels,\n                                             self.out_channels)'"
layers/gcn_layer.py,3,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nimport dgl.function as fn\nfrom dgl.nn.pytorch import GraphConv\n\n""""""\n    GCN: Graph Convolutional Networks\n    Thomas N. Kipf, Max Welling, Semi-Supervised Classification with Graph Convolutional Networks (ICLR 2017)\n    http://arxiv.org/abs/1609.02907\n""""""\n    \n# Sends a message of node feature h\n# Equivalent to => return {\'m\': edges.src[\'h\']}\nmsg = fn.copy_src(src=\'h\', out=\'m\')\n\ndef reduce(nodes):\n    accum = torch.mean(nodes.mailbox[\'m\'], 1)\n    return {\'h\': accum}\n\nclass NodeApplyModule(nn.Module):\n    # Update node feature h_v with (Wh_v+b)\n    def __init__(self, in_dim, out_dim):\n        super().__init__()\n        self.linear = nn.Linear(in_dim, out_dim)\n        \n    def forward(self, node):\n        h = self.linear(node.data[\'h\'])\n        return {\'h\': h}\n\nclass GCNLayer(nn.Module):\n    """"""\n        Param: [in_dim, out_dim]\n    """"""\n    def __init__(self, in_dim, out_dim, activation, dropout, graph_norm, batch_norm, residual=False, dgl_builtin=False):\n        super().__init__()\n        self.in_channels = in_dim\n        self.out_channels = out_dim\n        self.graph_norm = graph_norm\n        self.batch_norm = batch_norm\n        self.residual = residual\n        self.dgl_builtin = dgl_builtin\n        \n        if in_dim != out_dim:\n            self.residual = False\n        \n        self.batchnorm_h = nn.BatchNorm1d(out_dim)\n        self.activation = activation\n        self.dropout = nn.Dropout(dropout)\n        if self.dgl_builtin == False:\n            self.apply_mod = NodeApplyModule(in_dim, out_dim)\n        else:\n            self.conv = GraphConv(in_dim, out_dim)\n\n        \n    def forward(self, g, feature, snorm_n):\n        h_in = feature   # to be used for residual connection\n\n        if self.dgl_builtin == False:\n            g.ndata[\'h\'] = feature\n            g.update_all(msg, reduce)\n            g.apply_nodes(func=self.apply_mod)\n            h = g.ndata[\'h\'] # result of graph convolution\n        else:\n            h = self.conv(g, feature)\n\n        if self.graph_norm:\n            h = h * snorm_n # normalize activation w.r.t. graph size\n\n        \n        if self.batch_norm:\n            h = self.batchnorm_h(h) # batch normalization  \n       \n        if self.activation:\n            h = self.activation(h)\n        \n        if self.residual:\n            h = h_in + h # residual connection\n            \n        h = self.dropout(h)\n        return h\n    \n    def __repr__(self):\n        return \'{}(in_channels={}, out_channels={}, residual={})\'.format(self.__class__.__name__,\n                                             self.in_channels,\n                                             self.out_channels, self.residual)\n'"
layers/gin_layer.py,6,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport dgl.function as fn\n\n""""""\n    GIN: Graph Isomorphism Networks\n    HOW POWERFUL ARE GRAPH NEURAL NETWORKS? (Keyulu Xu, Weihua Hu, Jure Leskovec and Stefanie Jegelka, ICLR 2019)\n    https://arxiv.org/pdf/1810.00826.pdf\n""""""\n\nclass GINLayer(nn.Module):\n    """"""\n    [!] code adapted from dgl implementation of GINConv\n\n    Parameters\n    ----------\n    apply_func : callable activation function/layer or None\n        If not None, apply this function to the updated node feature,\n        the :math:`f_\\Theta` in the formula.\n    aggr_type :\n        Aggregator type to use (``sum``, ``max`` or ``mean``).\n    out_dim :\n        Rquired for batch norm layer; should match out_dim of apply_func if not None.\n    dropout :\n        Required for dropout of output features.\n    graph_norm : \n        boolean flag for output features normalization w.r.t. graph sizes.\n    batch_norm :\n        boolean flag for batch_norm layer.\n    residual :\n        boolean flag for using residual connection.\n    init_eps : optional\n        Initial :math:`\\epsilon` value, default: ``0``.\n    learn_eps : bool, optional\n        If True, :math:`\\epsilon` will be a learnable parameter.\n    \n    """"""\n    def __init__(self, apply_func, aggr_type, dropout, graph_norm, batch_norm, residual=False, init_eps=0, learn_eps=False, activation=None):\n        super().__init__()\n        self.apply_func = apply_func\n        self.activation = activation\n        \n        if aggr_type == \'sum\':\n            self._reducer = fn.sum\n        elif aggr_type == \'max\':\n            self._reducer = fn.max\n        elif aggr_type == \'mean\':\n            self._reducer = fn.mean\n        else:\n            raise KeyError(\'Aggregator type {} not recognized.\'.format(aggr_type))\n            \n        self.graph_norm = graph_norm\n        self.batch_norm = batch_norm\n        self.residual = residual\n        self.dropout = dropout\n        \n        in_dim = apply_func.mlp.input_dim\n        out_dim = apply_func.mlp.output_dim\n        \n        if in_dim != out_dim:\n            self.residual = False\n            \n        # to specify whether eps is trainable or not.\n        if learn_eps:\n            self.eps = torch.nn.Parameter(torch.FloatTensor([init_eps]))\n        else:\n            self.register_buffer(\'eps\', torch.FloatTensor([init_eps]))\n            \n        self.bn_node_h = nn.BatchNorm1d(out_dim)\n\n    def forward(self, g, h, snorm_n):\n        h_in = h # for residual connection\n        \n        g = g.local_var()\n        g.ndata[\'h\'] = h\n        g.update_all(fn.copy_u(\'h\', \'m\'), self._reducer(\'m\', \'neigh\'))\n        h = (1 + self.eps) * h + g.ndata[\'neigh\']\n        if self.apply_func is not None:\n            h = self.apply_func(h)\n\n        if self.graph_norm:\n            h = h * snorm_n # normalize activation w.r.t. graph size\n        \n        if self.batch_norm:\n            h = self.bn_node_h(h) # batch normalization  \n       \n        if self.activation:\n            h = F.relu(h) # non-linear activation\n        \n        if self.residual:\n            h = h_in + h # residual connection\n        \n        h = F.dropout(h, self.dropout, training=self.training)\n        \n        return h\n    \n    \nclass ApplyNodeFunc(nn.Module):\n    """"""\n        This class is used in class GINNet\n        Update the node feature hv with MLP\n    """"""\n    def __init__(self, mlp):\n        super().__init__()\n        self.mlp = mlp\n        self.bn = nn.BatchNorm1d(self.mlp.output_dim)\n\n    def forward(self, h):\n        h = self.mlp(h)\n        return h\n\n\nclass MLP(nn.Module):\n    """"""MLP with linear output""""""\n    def __init__(self, num_layers, input_dim, hidden_dim, output_dim):\n\n        super().__init__()\n        self.linear_or_not = True  # default is linear model\n        self.num_layers = num_layers\n        self.output_dim = output_dim\n        self.input_dim = input_dim\n\n        if num_layers < 1:\n            raise ValueError(""number of layers should be positive!"")\n        elif num_layers == 1:\n            # Linear model\n            self.linear = nn.Linear(input_dim, output_dim)\n        else:\n            # Multi-layer model\n            self.linear_or_not = False\n            self.linears = torch.nn.ModuleList()\n            self.batch_norms = torch.nn.ModuleList()\n\n            self.linears.append(nn.Linear(input_dim, hidden_dim))\n            for layer in range(num_layers - 2):\n                self.linears.append(nn.Linear(hidden_dim, hidden_dim))\n            self.linears.append(nn.Linear(hidden_dim, output_dim))\n\n            for layer in range(num_layers - 1):\n                self.batch_norms.append(nn.BatchNorm1d((hidden_dim)))\n\n    def forward(self, x):\n        if self.linear_or_not:\n            # If linear model\n            return self.linear(x)\n        else:\n            # If MLP\n            h = x\n            for i in range(self.num_layers - 1):\n                h = F.relu(self.batch_norms[i](self.linears[i](h)))\n            return self.linears[-1](h)\n'"
layers/gmm_layer.py,7,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.nn import init\nimport dgl.function as fn\n\n""""""\n    GMM: Gaussian Mixture Model Convolution layer\n    Geometric Deep Learning on Graphs and Manifolds using Mixture Model CNNs (Federico Monti et al., CVPR 2017)\n    https://arxiv.org/pdf/1611.08402.pdf\n""""""\n\nclass GMMLayer(nn.Module):\n    """"""\n    [!] code adapted from dgl implementation of GMMConv\n\n    Parameters\n    ----------\n    in_dim : \n        Number of input features.\n    out_dim : \n        Number of output features.\n    dim : \n        Dimensionality of pseudo-coordinte.\n    kernel : \n        Number of kernels :math:`K`.\n    aggr_type : \n        Aggregator type (``sum``, ``mean``, ``max``).\n    dropout :\n        Required for dropout of output features.\n    graph_norm : \n        boolean flag for output features normalization w.r.t. graph sizes.\n    batch_norm :\n        boolean flag for batch_norm layer.\n    residual : \n        If True, use residual connection inside this layer. Default: ``False``.\n    bias : \n        If True, adds a learnable bias to the output. Default: ``True``.\n    \n    """"""\n    def __init__(self, in_dim, out_dim, dim, kernel, aggr_type, dropout,\n                 graph_norm, batch_norm, residual=False, bias=True):\n        super().__init__()\n        \n        self.in_dim = in_dim\n        self.out_dim = out_dim\n        self.dim = dim\n        self.kernel = kernel\n        self.graph_norm = graph_norm\n        self.batch_norm = batch_norm\n        self.residual = residual\n        self.dropout = dropout\n        \n        if aggr_type == \'sum\':\n            self._reducer = fn.sum\n        elif aggr_type == \'mean\':\n            self._reducer = fn.mean\n        elif aggr_type == \'max\':\n            self._reducer = fn.max\n        else:\n            raise KeyError(""Aggregator type {} not recognized."".format(aggr_type))\n\n        self.mu = nn.Parameter(torch.Tensor(kernel, dim))\n        self.inv_sigma = nn.Parameter(torch.Tensor(kernel, dim))\n        self.fc = nn.Linear(in_dim, kernel * out_dim, bias=False)\n        \n        self.bn_node_h = nn.BatchNorm1d(out_dim)\n        \n        if in_dim != out_dim:\n            self.residual = False\n        \n        if bias:\n            self.bias = nn.Parameter(torch.Tensor(out_dim))\n        else:\n            self.register_buffer(\'bias\', None)\n        self.reset_parameters()\n    \n    def reset_parameters(self):\n        """"""Reinitialize learnable parameters.""""""\n        gain = init.calculate_gain(\'relu\')\n        init.xavier_normal_(self.fc.weight, gain=gain)\n        init.normal_(self.mu.data, 0, 0.1)\n        init.constant_(self.inv_sigma.data, 1)\n        if self.bias is not None:\n            init.zeros_(self.bias.data)\n    \n    def forward(self, g, h, pseudo, snorm_n):\n        h_in = h # for residual connection\n        \n        g = g.local_var()\n        g.ndata[\'h\'] = self.fc(h).view(-1, self.kernel, self.out_dim)\n        E = g.number_of_edges()\n        \n        # compute gaussian weight\n        gaussian = -0.5 * ((pseudo.view(E, 1, self.dim) -\n                            self.mu.view(1, self.kernel, self.dim)) ** 2)\n        gaussian = gaussian * (self.inv_sigma.view(1, self.kernel, self.dim) ** 2)\n        gaussian = torch.exp(gaussian.sum(dim=-1, keepdim=True)) # (E, K, 1)\n        g.edata[\'w\'] = gaussian\n        g.update_all(fn.u_mul_e(\'h\', \'w\', \'m\'), self._reducer(\'m\', \'h\'))\n        h = g.ndata[\'h\'].sum(1)\n        \n        if self.graph_norm:\n            h = h* snorm_n # normalize activation w.r.t. graph size\n        \n        if self.batch_norm:\n            h = self.bn_node_h(h) # batch normalization  \n        \n        h = F.relu(h) # non-linear activation\n        \n        if self.residual:\n            h = h_in + h # residual connection\n        \n        if self.bias is not None:\n            h = h + self.bias\n        \n        h = F.dropout(h, self.dropout, training=self.training)\n        \n        return h\n'"
layers/graphsage_layer.py,2,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nimport dgl.function as fn\nfrom dgl.nn.pytorch import SAGEConv\n\n""""""\n    GraphSAGE: \n    William L. Hamilton, Rex Ying, Jure Leskovec, Inductive Representation Learning on Large Graphs (NeurIPS 2017)\n    https://cs.stanford.edu/people/jure/pubs/graphsage-nips17.pdf\n""""""\n\nfrom layers.sage_aggregator_layer import MaxPoolAggregator, MeanAggregator, LSTMAggregator\nfrom layers.node_apply_layer import NodeApply\n\nclass GraphSageLayer(nn.Module):\n\n    def __init__(self, in_feats, out_feats, activation, dropout,\n                 aggregator_type, graph_norm, batch_norm, residual=False, bias=True,\n                 dgl_builtin=False):\n        super().__init__()\n        self.in_channels = in_feats\n        self.out_channels = out_feats\n        self.aggregator_type = aggregator_type\n        self.graph_norm = graph_norm\n        self.batch_norm = batch_norm\n        self.residual = residual\n        self.dgl_builtin = dgl_builtin\n        \n        if in_feats != out_feats:\n            self.residual = False\n        \n        self.dropout = nn.Dropout(p=dropout)\n\n        if dgl_builtin == False:\n            self.nodeapply = NodeApply(in_feats, out_feats, activation, dropout,\n                                   bias=bias)\n            if aggregator_type == ""pool"":\n                self.aggregator = MaxPoolAggregator(in_feats, in_feats,\n                                                    activation, bias)\n            elif aggregator_type == ""lstm"":\n                self.aggregator = LSTMAggregator(in_feats, in_feats)\n            else:\n                self.aggregator = MeanAggregator()\n        else:\n            self.sageconv = SAGEConv(in_feats, out_feats, aggregator_type,\n                    dropout, activation=activation)\n        \n        if self.batch_norm:\n            self.batchnorm_h = nn.BatchNorm1d(out_feats)\n\n    def forward(self, g, h, snorm_n=None):\n        h_in = h              # for residual connection\n        \n        if self.dgl_builtin == False:\n            h = self.dropout(h)\n            g.ndata[\'h\'] = h\n            g.update_all(fn.copy_src(src=\'h\', out=\'m\'), self.aggregator,\n                         self.nodeapply)\n            h = g.ndata[\'h\']\n        else:\n            h = self.sageconv(g, h)\n\n        if self.graph_norm:\n            h = h * snorm_n\n\n        if self.batch_norm:\n            h = self.batchnorm_h(h)\n        \n        if self.residual:\n            h = h_in + h       # residual connection\n        \n        return h\n    \n    def __repr__(self):\n        return \'{}(in_channels={}, out_channels={}, aggregator={}, residual={})\'.format(self.__class__.__name__,\n                                              self.in_channels,\n                                              self.out_channels, self.aggregator_type, self.residual)\n'"
layers/mlp_readout_layer.py,2,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n""""""\n    MLP Layer used after graph vector representation\n""""""\n\nclass MLPReadout(nn.Module):\n\n    def __init__(self, input_dim, output_dim, L=2): #L=nb_hidden_layers\n        super().__init__()\n        list_FC_layers = [ nn.Linear( input_dim//2**l , input_dim//2**(l+1) , bias=True ) for l in range(L) ]\n        list_FC_layers.append(nn.Linear( input_dim//2**L , output_dim , bias=True ))\n        self.FC_layers = nn.ModuleList(list_FC_layers)\n        self.L = L\n        \n    def forward(self, x):\n        y = x\n        for l in range(self.L):\n            y = self.FC_layers[l](y)\n            y = F.relu(y)\n        y = self.FC_layers[self.L](y)\n        return y'"
layers/node_apply_layer.py,3,"b'""""""\n\n! Code started from dgl diffpool examples dir\n""""""\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass NodeApply(nn.Module):\n    """"""\n    Works -> the node_apply function in DGL paradigm\n    """"""\n\n    def __init__(self, in_feats, out_feats, activation, dropout, bias=True):\n        super().__init__()\n        self.dropout = nn.Dropout(p=dropout)\n        self.linear = nn.Linear(in_feats * 2, out_feats, bias)\n        self.activation = activation\n\n#         nn.init.xavier_uniform_(self.linear.weight,\n#                                 gain=nn.init.calculate_gain(\'relu\'))\n\n    def concat(self, h, aggre_result):\n        bundle = torch.cat((h, aggre_result), 1)\n        bundle = self.linear(bundle)\n        return bundle\n\n    def forward(self, node):\n        h = node.data[\'h\']\n        c = node.data[\'c\']\n        bundle = self.concat(h, c)\n        bundle = F.normalize(bundle, p=2, dim=1)\n        if self.activation:\n            bundle = self.activation(bundle)\n        return {""h"": bundle}'"
layers/sage_aggregator_layer.py,7,"b'""""""\nAggregator class(s) for the GraphSAGE example\n\n! Code started from dgl diffpool examples dir\n""""""\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Aggregator(nn.Module):\n    """"""\n    Base Aggregator class. \n    """"""\n\n    def __init__(self):\n        super().__init__()\n\n    def forward(self, node):\n        neighbour = node.mailbox[\'m\']\n        c = self.aggre(neighbour)\n        return {""c"": c}\n\n    def aggre(self, neighbour):\n        # N x F\n        raise NotImplementedError\n\n\nclass MeanAggregator(Aggregator):\n    """"""\n    Mean Aggregator for graphsage\n    """"""\n\n    def __init__(self):\n        super().__init__()\n\n    def aggre(self, neighbour):\n        mean_neighbour = torch.mean(neighbour, dim=1)\n        return mean_neighbour\n\n\nclass MaxPoolAggregator(Aggregator):\n    """"""\n    Maxpooling aggregator for graphsage\n    """"""\n\n    def __init__(self, in_feats, out_feats, activation, bias):\n        super().__init__()\n        self.linear = nn.Linear(in_feats, out_feats, bias=bias)\n        self.activation = activation\n        # Xavier initialization of weight\n#         nn.init.xavier_uniform_(self.linear.weight,\n#                                 gain=nn.init.calculate_gain(\'relu\'))\n\n    def aggre(self, neighbour):\n        neighbour = self.linear(neighbour)\n        if self.activation:\n            neighbour = self.activation(neighbour)\n        maxpool_neighbour = torch.max(neighbour, dim=1)[0]\n        return maxpool_neighbour\n\n\nclass LSTMAggregator(Aggregator):\n    """"""\n    LSTM aggregator for graphsage\n    """"""\n\n    def __init__(self, in_feats, hidden_feats):\n        super().__init__()\n        self.lstm = nn.LSTM(in_feats, hidden_feats, batch_first=True)\n        self.hidden_dim = hidden_feats\n        self.hidden = self.init_hidden()\n\n        nn.init.xavier_uniform_(self.lstm.weight,\n                                gain=nn.init.calculate_gain(\'relu\'))\n\n    def init_hidden(self):\n        """"""\n        Defaulted to initialite all zero\n        """"""\n        return (torch.zeros(1, 1, self.hidden_dim),\n                torch.zeros(1, 1, self.hidden_dim))\n\n    def aggre(self, neighbours):\n        """"""\n        aggregation function\n        """"""\n        # N X F\n        rand_order = torch.randperm(neighbours.size()[1])\n        neighbours = neighbours[:, rand_order, :]\n\n        (lstm_out, self.hidden) = self.lstm(neighbours.view(neighbours.size()[0],\n                                                            neighbours.size()[\n            1],\n            -1))\n        return lstm_out[:, -1, :]\n\n    def forward(self, node):\n        neighbour = node.mailbox[\'m\']\n        c = self.aggre(neighbour)\n        return {""c"": c}'"
train/metrics.py,3,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import f1_score\nimport numpy as np\n\n\ndef MAE(scores, targets):\n    MAE = F.l1_loss(scores, targets)\n    return MAE\n\n\ndef accuracy_TU(scores, targets):\n    scores = scores.detach().argmax(dim=1)\n    acc = (scores==targets).float().sum().item()\n    return acc\n\n\ndef accuracy_MNIST_CIFAR(scores, targets):\n    scores = scores.detach().argmax(dim=1)\n    acc = (scores==targets).float().sum().item()\n    return acc\n\ndef accuracy_CITATION_GRAPH(scores, targets):\n    scores = scores.detach().argmax(dim=1)\n    acc = (scores==targets).float().sum().item()\n    acc = acc / len(targets)\n    return acc\n\n\ndef accuracy_SBM(scores, targets):\n    S = targets.cpu().numpy()\n    C = np.argmax( torch.nn.Softmax(dim=1)(scores).cpu().detach().numpy() , axis=1 )\n    CM = confusion_matrix(S,C).astype(np.float32)\n    nb_classes = CM.shape[0]\n    targets = targets.cpu().detach().numpy()\n    nb_non_empty_classes = 0\n    pr_classes = np.zeros(nb_classes)\n    for r in range(nb_classes):\n        cluster = np.where(targets==r)[0]\n        if cluster.shape[0] != 0:\n            pr_classes[r] = CM[r,r]/ float(cluster.shape[0])\n            if CM[r,r]>0:\n                nb_non_empty_classes += 1\n        else:\n            pr_classes[r] = 0.0\n    acc = 100.* np.sum(pr_classes)/ float(nb_non_empty_classes)\n    return acc\n\n\ndef binary_f1_score(scores, targets):\n    """"""Computes the F1 score using scikit-learn for binary class labels. \n    \n    Returns the F1 score for the positive class, i.e. labelled \'1\'.\n    """"""\n    y_true = targets.cpu().numpy()\n    y_pred = scores.argmax(dim=1).cpu().numpy()\n    return f1_score(y_true, y_pred, average=\'binary\')\n\n  \ndef accuracy_VOC(scores, targets):\n    scores = scores.detach().argmax(dim=1).cpu()\n    targets = targets.cpu().detach().numpy()\n    acc = f1_score(scores, targets, average=\'weighted\')\n    return acc\n'"
train/train_CitationGraphs_node_classification.py,2,"b'""""""\n    Utility functions for training one epoch \n    and evaluating one epoch\n""""""\nimport torch\nimport torch.nn as nn\nimport math\nimport dgl\n\nfrom train.metrics import accuracy_CITATION_GRAPH as accuracy\n\n\ndef train_epoch(model, optimizer, device, graph, nfeat, efeat, norm_n, norm_e, train_mask, labels, epoch):\n\n    model.train()\n    epoch_loss = 0\n    epoch_train_acc = 0\n    nb_data = 0\n    gpu_mem = 0\n\n    #logits = model.forward(graph, nfeat, efeat, norm_n, norm_e)\n    logits = model(graph, nfeat, efeat, norm_n, norm_e)\n    loss = model.loss(logits[train_mask], labels[train_mask])\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n    epoch_loss = loss.detach().item()\n    epoch_train_acc = accuracy(logits[train_mask], labels[train_mask])\n    return epoch_loss, epoch_train_acc, optimizer\n\n\ndef evaluate_network(model, optimizer, device, graph, nfeat, efeat, norm_n, norm_e, mask, labels, epoch):\n    \n    model.eval()\n    epoch_test_loss = 0\n    epoch_test_acc = 0\n    nb_data = 0\n    with torch.no_grad():\n        logits = model.forward(graph, nfeat, efeat, norm_n, norm_e)\n        loss = model.loss(logits[mask], labels[mask])\n        epoch_test_loss = loss.detach().item()\n        epoch_test_acc = accuracy(logits[mask], labels[mask])\n\n    return epoch_test_loss, epoch_test_acc\n'"
train/train_SBMs_node_classification.py,2,"b'""""""\n    Utility functions for training one epoch \n    and evaluating one epoch\n""""""\nimport torch\nimport torch.nn as nn\nimport math\nimport dgl\n\nfrom train.metrics import accuracy_SBM as accuracy\n\n\ndef train_epoch(model, optimizer, device, data_loader, epoch):\n\n    model.train()\n    epoch_loss = 0\n    epoch_train_acc = 0\n    nb_data = 0\n    gpu_mem = 0\n    for iter, (batch_graphs, batch_labels, batch_snorm_n, batch_snorm_e) in enumerate(data_loader):\n        batch_x = batch_graphs.ndata[\'feat\'].to(device)  # num x feat\n        batch_e = batch_graphs.edata[\'feat\'].to(device)\n        batch_snorm_e = batch_snorm_e.to(device)\n        batch_labels = batch_labels.to(device)\n        batch_snorm_n = batch_snorm_n.to(device)         # num x 1\n        optimizer.zero_grad()\n        batch_scores = model.forward(batch_graphs, batch_x, batch_e, batch_snorm_n, batch_snorm_e)\n        loss = model.loss(batch_scores, batch_labels)\n        loss.backward()\n        optimizer.step()\n        epoch_loss += loss.detach().item()\n        epoch_train_acc += accuracy(batch_scores, batch_labels)\n    epoch_loss /= (iter + 1)\n    epoch_train_acc /= (iter + 1)\n    \n    return epoch_loss, epoch_train_acc, optimizer\n\n\ndef evaluate_network(model, device, data_loader, epoch):\n    \n    model.eval()\n    epoch_test_loss = 0\n    epoch_test_acc = 0\n    nb_data = 0\n    with torch.no_grad():\n        for iter, (batch_graphs, batch_labels, batch_snorm_n, batch_snorm_e) in enumerate(data_loader):\n            batch_x = batch_graphs.ndata[\'feat\'].to(device)\n            batch_e = batch_graphs.edata[\'feat\'].to(device)\n            batch_snorm_e = batch_snorm_e.to(device)\n            batch_labels = batch_labels.to(device)\n            batch_snorm_n = batch_snorm_n.to(device)\n            batch_scores = model.forward(batch_graphs, batch_x, batch_e, batch_snorm_n, batch_snorm_e)\n            loss = model.loss(batch_scores, batch_labels) \n            epoch_test_loss += loss.detach().item()\n            epoch_test_acc += accuracy(batch_scores, batch_labels)\n        epoch_test_loss /= (iter + 1)\n        epoch_test_acc /= (iter + 1)\n        \n    return epoch_test_loss, epoch_test_acc\n\n\n'"
train/train_TSP_edge_classification.py,2,"b'""""""\n    Utility functions for training one epoch \n    and evaluating one epoch\n""""""\nimport torch\nimport torch.nn as nn\nimport math\nimport dgl\n\nfrom train.metrics import binary_f1_score\n\n\ndef train_epoch(model, optimizer, device, data_loader, epoch):\n\n    model.train()\n    epoch_loss = 0\n    epoch_train_f1 = 0\n    nb_data = 0\n    gpu_mem = 0\n    for iter, (batch_graphs, batch_labels, batch_snorm_n, batch_snorm_e) in enumerate(data_loader):\n        batch_x = batch_graphs.ndata[\'feat\'].to(device)  # num x feat\n        batch_e = batch_graphs.edata[\'feat\'].to(device)\n        batch_labels = batch_labels.to(device)\n        batch_snorm_e = batch_snorm_e.to(device)\n        batch_snorm_n = batch_snorm_n.to(device)         # num x 1\n        optimizer.zero_grad()\n        \n        batch_scores = model.forward(batch_graphs, batch_x, batch_e, batch_snorm_n, batch_snorm_e)\n        loss = model.loss(batch_scores, batch_labels)\n        loss.backward()\n        optimizer.step()\n        epoch_loss += loss.detach().item()\n        epoch_train_f1 += binary_f1_score(batch_scores, batch_labels)\n    epoch_loss /= (iter + 1)\n    epoch_train_f1 /= (iter + 1)\n    \n    return epoch_loss, epoch_train_f1, optimizer\n\n\ndef evaluate_network(model, device, data_loader, epoch):\n    \n    model.eval()\n    epoch_test_loss = 0\n    epoch_test_f1 = 0\n    nb_data = 0\n    with torch.no_grad():\n        for iter, (batch_graphs, batch_labels, batch_snorm_n, batch_snorm_e) in enumerate(data_loader):\n            batch_x = batch_graphs.ndata[\'feat\'].to(device)\n            batch_e = batch_graphs.edata[\'feat\'].to(device)\n            batch_labels = batch_labels.to(device)\n            batch_snorm_e = batch_snorm_e.to(device)\n            batch_snorm_n = batch_snorm_n.to(device)\n\n            batch_scores = model.forward(batch_graphs, batch_x, batch_e, batch_snorm_n, batch_snorm_e)\n            loss = model.loss(batch_scores, batch_labels) \n            epoch_test_loss += loss.detach().item()\n            epoch_test_f1 += binary_f1_score(batch_scores, batch_labels)\n        epoch_test_loss /= (iter + 1)\n        epoch_test_f1 /= (iter + 1)\n        \n    return epoch_test_loss, epoch_test_f1\n\n\n'"
train/train_TUs_graph_classification.py,2,"b'""""""\n    Utility functions for training one epoch \n    and evaluating one epoch\n""""""\nimport torch\nimport torch.nn as nn\nimport math\n\nfrom train.metrics import accuracy_TU as accuracy\n\ndef train_epoch(model, optimizer, device, data_loader, epoch):\n    model.train()\n    epoch_loss = 0\n    epoch_train_acc = 0\n    nb_data = 0\n    gpu_mem = 0\n    for iter, (batch_graphs, batch_labels, batch_snorm_n, batch_snorm_e) in enumerate(data_loader):\n        batch_x = batch_graphs.ndata[\'feat\'].to(device)  # num x feat\n        batch_e = batch_graphs.edata[\'feat\'].to(device)\n        batch_snorm_e = batch_snorm_e.to(device)\n        batch_labels = batch_labels.to(device)\n        batch_snorm_n = batch_snorm_n.to(device)         # num x 1\n        optimizer.zero_grad()\n        \n        batch_scores = model.forward(batch_graphs, batch_x, batch_e, batch_snorm_n, batch_snorm_e)\n        loss = model.loss(batch_scores, batch_labels) \n        loss.backward()\n        optimizer.step()\n        epoch_loss += loss.detach().item()\n        epoch_train_acc += accuracy(batch_scores, batch_labels)\n        nb_data += batch_labels.size(0)\n    epoch_loss /= (iter + 1)\n    epoch_train_acc /= nb_data\n    \n    return epoch_loss, epoch_train_acc, optimizer\n\ndef evaluate_network(model, device, data_loader, epoch):\n    model.eval()\n    epoch_test_loss = 0\n    epoch_test_acc = 0\n    nb_data = 0\n    with torch.no_grad():\n        for iter, (batch_graphs, batch_labels, batch_snorm_n, batch_snorm_e) in enumerate(data_loader):\n            batch_x = batch_graphs.ndata[\'feat\'].to(device)\n            batch_e = batch_graphs.edata[\'feat\'].to(device)\n            batch_snorm_e = batch_snorm_e.to(device)\n            batch_labels = batch_labels.to(device)\n            batch_snorm_n = batch_snorm_n.to(device)\n            \n            batch_scores = model.forward(batch_graphs, batch_x, batch_e, batch_snorm_n, batch_snorm_e)\n            loss = model.loss(batch_scores, batch_labels) \n            epoch_test_loss += loss.detach().item()\n            epoch_test_acc += accuracy(batch_scores, batch_labels)\n            nb_data += batch_labels.size(0)\n        epoch_test_loss /= (iter + 1)\n        epoch_test_acc /= nb_data\n        \n    return epoch_test_loss, epoch_test_acc\n\ndef check_patience(all_losses, best_loss, best_epoch, curr_loss, curr_epoch, counter):\n    if curr_loss < best_loss:\n        counter = 0\n        best_loss = curr_loss\n        best_epoch = curr_epoch\n    else:\n        counter += 1\n    return best_loss, best_epoch, counter'"
train/train_molecules_graph_regression.py,2,"b'""""""\n    Utility functions for training one epoch \n    and evaluating one epoch\n""""""\nimport torch\nimport torch.nn as nn\nimport math\n\nfrom train.metrics import MAE\n\ndef train_epoch(model, optimizer, device, data_loader, epoch):\n    model.train()\n    epoch_loss = 0\n    epoch_train_mae = 0\n    nb_data = 0\n    gpu_mem = 0\n    for iter, (batch_graphs, batch_targets, batch_snorm_n, batch_snorm_e) in enumerate(data_loader):\n        batch_x = batch_graphs.ndata[\'feat\'].to(device)  # num x feat\n        batch_e = batch_graphs.edata[\'feat\'].to(device)\n        batch_snorm_e = batch_snorm_e.to(device)\n        batch_targets = batch_targets.to(device)\n        batch_snorm_n = batch_snorm_n.to(device)         # num x 1\n        optimizer.zero_grad()\n        \n        batch_scores = model.forward(batch_graphs, batch_x, batch_e, batch_snorm_n, batch_snorm_e)\n        loss = model.loss(batch_scores, batch_targets)\n        loss.backward()\n        optimizer.step()\n        epoch_loss += loss.detach().item()\n        epoch_train_mae += MAE(batch_scores, batch_targets)\n        nb_data += batch_targets.size(0)\n    epoch_loss /= (iter + 1)\n    epoch_train_mae /= (iter + 1)\n    \n    return epoch_loss, epoch_train_mae, optimizer\n\ndef evaluate_network(model, device, data_loader, epoch):\n    model.eval()\n    epoch_test_loss = 0\n    epoch_test_mae = 0\n    nb_data = 0\n    with torch.no_grad():\n        for iter, (batch_graphs, batch_targets, batch_snorm_n, batch_snorm_e) in enumerate(data_loader):\n            batch_x = batch_graphs.ndata[\'feat\'].to(device)\n            batch_e = batch_graphs.edata[\'feat\'].to(device)\n            batch_snorm_e = batch_snorm_e.to(device)\n            batch_targets = batch_targets.to(device)\n            batch_snorm_n = batch_snorm_n.to(device)\n            \n            batch_scores = model.forward(batch_graphs, batch_x, batch_e, batch_snorm_n, batch_snorm_e)\n            loss = model.loss(batch_scores, batch_targets)\n            epoch_test_loss += loss.detach().item()\n            epoch_test_mae += MAE(batch_scores, batch_targets)\n            nb_data += batch_targets.size(0)\n        epoch_test_loss /= (iter + 1)\n        epoch_test_mae /= (iter + 1)\n        \n    return epoch_test_loss, epoch_test_mae'"
train/train_superpixels_graph_classification.py,2,"b'""""""\n    Utility functions for training one epoch \n    and evaluating one epoch\n""""""\nimport torch\nimport torch.nn as nn\nimport math\n\nfrom train.metrics import accuracy_MNIST_CIFAR as accuracy\n\ndef train_epoch(model, optimizer, device, data_loader, epoch):\n    model.train()\n    epoch_loss = 0\n    epoch_train_acc = 0\n    nb_data = 0\n    gpu_mem = 0\n    for iter, (batch_graphs, batch_labels, batch_snorm_n, batch_snorm_e) in enumerate(data_loader):\n        batch_x = batch_graphs.ndata[\'feat\'].to(device)  # num x feat\n        batch_e = batch_graphs.edata[\'feat\'].to(device)\n        batch_snorm_e = batch_snorm_e.to(device)\n        batch_labels = batch_labels.to(device)\n        batch_snorm_n = batch_snorm_n.to(device)         # num x 1\n        optimizer.zero_grad()\n        \n        batch_scores = model.forward(batch_graphs, batch_x, batch_e, batch_snorm_n, batch_snorm_e)\n        loss = model.loss(batch_scores, batch_labels)\n        loss.backward()\n        optimizer.step()\n        epoch_loss += loss.detach().item()\n        epoch_train_acc += accuracy(batch_scores, batch_labels)\n        nb_data += batch_labels.size(0)\n    epoch_loss /= (iter + 1)\n    epoch_train_acc /= nb_data\n    \n    return epoch_loss, epoch_train_acc, optimizer\n\ndef evaluate_network(model, device, data_loader, epoch):\n    model.eval()\n    epoch_test_loss = 0\n    epoch_test_acc = 0\n    nb_data = 0\n    with torch.no_grad():\n        for iter, (batch_graphs, batch_labels, batch_snorm_n, batch_snorm_e) in enumerate(data_loader):\n            batch_x = batch_graphs.ndata[\'feat\'].to(device)\n            batch_e = batch_graphs.edata[\'feat\'].to(device)\n            batch_snorm_e = batch_snorm_e.to(device)\n            batch_labels = batch_labels.to(device)\n            batch_snorm_n = batch_snorm_n.to(device)\n            \n            batch_scores = model.forward(batch_graphs, batch_x, batch_e, batch_snorm_n, batch_snorm_e)\n            loss = model.loss(batch_scores, batch_labels) \n            epoch_test_loss += loss.detach().item()\n            epoch_test_acc += accuracy(batch_scores, batch_labels)\n            nb_data += batch_labels.size(0)\n        epoch_test_loss /= (iter + 1)\n        epoch_test_acc /= nb_data\n        \n    return epoch_test_loss, epoch_test_acc'"
utils/cleaner_main.py,0,"b'\n# Clean the main.py file after conversion from notebook.\n# Any notebook code is removed from the main.py file.\n\n\nimport subprocess\n\n\ndef cleaner_main(filename):\n\n\t# file names\n\tfile_notebook = filename + \'.ipynb\'\n\tfile_python = filename + \'.py\'\n\n\n\t# convert notebook to python file\n\tprint(\'Convert \' + file_notebook + \' to \' + file_python)\n\tsubprocess.check_output(\'jupyter nbconvert --to script \' + str(file_notebook) , shell=True)\n\n\tprint(\'Clean \' + file_python)\n\n\t# open file\n\twith open(file_python, ""r"") as f_in:\n\t    lines_in = f_in.readlines()\n\n\t# remove cell indices\n\tlines_in = [ line for i,line in enumerate(lines_in) if \'# In[\' not in line ]\n\n\t# remove comments\n\tlines_in = [ line for i,line in enumerate(lines_in) if line[0]!=\'#\' ]\n\n\t# remove ""in_ipynb()"" function\n\tidx_start_fnc = next((i for i, x in enumerate(lines_in) if \'def in_ipynb\' in x), None)\n\tif idx_start_fnc!=None:\n\t    idx_end_fnc = idx_start_fnc + next((i for i, x in enumerate(lines_in[idx_start_fnc+1:]) if x[:4] not in [\'\\n\',\'    \']), None)  \n\t    lines_in = [ line for i,line in enumerate(lines_in) if i not in range(idx_start_fnc,idx_end_fnc+1) ]\n\tlist_elements_to_remove = [\'in_ipynb()\', \'print(notebook_mode)\']\n\tfor elem in list_elements_to_remove:\n\t    lines_in = [ line for i,line in enumerate(lines_in) if elem not in line ]\n\t    \n\t# unindent ""if notebook_mode==False"" block\n\tidx_start_fnc = next((i for i, x in enumerate(lines_in) if \'if notebook_mode==False\' in x), None)\n\tif idx_start_fnc!=None:\n\t    idx_end_fnc = idx_start_fnc + next((i for i, x in enumerate(lines_in[idx_start_fnc+1:]) if x[:8] not in [\'\\n\',\'        \']), None)\n\t    for i in range(idx_start_fnc,idx_end_fnc+1):\n\t        lines_in[i] = lines_in[i][4:]\n\t    lines_in.pop(idx_start_fnc)\n\tlist_elements_to_remove = [\'# notebook mode\', \'# terminal mode\']\n\tfor elem in list_elements_to_remove:\n\t    lines_in = [ line for i,line in enumerate(lines_in) if elem not in line ]\n\n\t# remove remaining ""if notebook_mode==True"" blocks - single indent\n\trun = True\n\twhile run:\n\t    idx_start_fnc = next((i for i, x in enumerate(lines_in) if x[:16]==\'if notebook_mode\'), None)\n\t    if idx_start_fnc!=None:\n\t        idx_end_fnc = idx_start_fnc + next((i for i, x in enumerate(lines_in[idx_start_fnc+1:]) if x[:4] not in [\'\\n\',\'    \']), None)  \n\t        lines_in = [ line for i,line in enumerate(lines_in) if i not in range(idx_start_fnc,idx_end_fnc+1) ]\n\t    else:\n\t        run = False\n       \n\t# remove ""if notebook_mode==True"" block - double indents\n\tidx_start_fnc = next((i for i, x in enumerate(lines_in) if x[:20]==\'    if notebook_mode\'), None)\n\tif idx_start_fnc!=None:\n\t\tidx_end_fnc = idx_start_fnc + next((i for i, x in enumerate(lines_in[idx_start_fnc+1:]) if x[:8] not in [\'\\n\',\'        \']), None)  \n\t\tlines_in = [ line for i,line in enumerate(lines_in) if i not in range(idx_start_fnc,idx_end_fnc+1) ]\n\n\t# prepare main() for terminal mode\n\tidx = next((i for i, x in enumerate(lines_in) if \'def main\' in x), None)\n\tif idx!=None: lines_in[idx] = \'def main():\'\n\tidx = next((i for i, x in enumerate(lines_in) if x[:5]==\'else:\'), None)\n\tif idx!=None: lines_in.pop(idx)\n\tidx = next((i for i, x in enumerate(lines_in) if x[:10]==\'    main()\'), None)\n\tif idx!=None: lines_in[idx] = \'main()\'\n\n\t# remove notebook variables\n\tidx = next((i for i, x in enumerate(lines_in) if \'use_gpu = True\' in x), None)\n\tif idx!=None: lines_in.pop(idx)\n\tidx = next((i for i, x in enumerate(lines_in) if \'gpu_id = -1\' in x), None)\n\tif idx!=None: lines_in.pop(idx)\n\tidx = next((i for i, x in enumerate(lines_in) if \'device = None\' in x), None)\n\tif idx!=None: lines_in.pop(idx)\n\trun = True\n\twhile run:\n\t\tidx = next((i for i, x in enumerate(lines_in) if x[:10]==\'MODEL_NAME\'), None)\n\t\tif idx!=None: \n\t\t\tlines_in.pop(idx)\n\t\telse:\n\t\t\trun = False\n\n\t# save clean file\n\tlines_out = str()\n\tfor line in lines_in: lines_out += line\n\twith open(file_python, \'w\') as f_out:\n\t    f_out.write(lines_out)\n\t    \n\tprint(\'Done. \')\n\n\n\n\n\n'"
data/TSP/generate_TSP.py,0,"b'import time\nimport argparse\nimport pprint as pp\nimport os\n\nimport numpy as np\nfrom concorde.tsp import TSPSolver  # Install from https://github.com/jvkersch/pyconcorde\n\n\nif __name__ == ""__main__"":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(""--min_nodes"", type=int, default=50)\n    parser.add_argument(""--max_nodes"", type=int, default=500)\n    parser.add_argument(""--num_samples"", type=int, default=10000)\n    parser.add_argument(""--filename"", type=str, default=None)\n    parser.add_argument(""--node_dim"", type=int, default=2)\n    parser.add_argument(""--seed"", type=int, default=1234)\n    opts = parser.parse_args()\n    \n    if opts.filename is None:\n        opts.filename = f""tsp{opts.min_nodes}-{opts.max_nodes}.txt""\n    \n    # Pretty print the run args\n    pp.pprint(vars(opts))\n    \n    np.random.seed(opts.seed)\n    \n    with open(opts.filename, ""w"") as f:\n        start_time = time.time()\n        idx = 0\n        while idx < opts.num_samples:\n            num_nodes = np.random.randint(low=opts.min_nodes, high=opts.max_nodes+1)\n            \n            nodes_coord = np.random.random([num_nodes, opts.node_dim])\n            solver = TSPSolver.from_data(nodes_coord[:, 0], nodes_coord[:, 1], norm=""GEO"")  \n            solution = solver.solve()\n            \n            # Only write instances with valid solutions\n            if (np.sort(solution.tour) == np.arange(num_nodes)).all():\n                f.write( "" "".join( str(x)+str("" "")+str(y) for x,y in nodes_coord) )\n                f.write( str("" "") + str(\'output\') + str("" "") )\n                f.write( str("" "").join( str(node_idx+1) for node_idx in solution.tour) )\n                f.write( str("" "") + str(solution.tour[0]+1) + str("" "") )\n                f.write( ""\\n"" )\n                idx += 1\n        \n        end_time = time.time() - start_time\n    \n    print(f""Completed generation of {opts.num_samples} samples of TSP{opts.min_nodes}-{opts.max_nodes}."")\n    print(f""Total time: {end_time/60:.1f}m"")\n    print(f""Average time: {end_time/opts.num_samples:.1f}s"")\n'"
layers/tensorized/assignment_layer.py,2,"b'import torch\n\nfrom torch import nn as nn\nfrom torch.nn import functional as F\nfrom torch.autograd import Variable\n\n""""""\n    This layer is the generating the Assignment matrix as shown in\n    equation (6) of the DIFFPOOL paper.\n    ! code started from dgl diffpool examples dir\n""""""\n\nfrom .dense_graphsage_layer import DenseGraphSage\n\nclass DiffPoolAssignment(nn.Module):\n    def __init__(self, nfeat, nnext):\n        super().__init__()\n        self.assign_mat = DenseGraphSage(nfeat, nnext, use_bn=True)\n\n    def forward(self, x, adj, log=False):\n        s_l_init = self.assign_mat(x, adj)\n        s_l = F.softmax(s_l_init, dim=-1)\n        return s_l'"
layers/tensorized/dense_diffpool_layer.py,3,"b'import torch\nfrom torch import nn as nn\n\n""""""\n    <Dense/Tensorzied version of the Diffpool layer>\n    \n    DIFFPOOL:\n    Z. Ying, J. You, C. Morris, X. Ren, W. Hamilton, and J. Leskovec, \n    Hierarchical graph representation learning with differentiable pooling (NeurIPS 2018)\n    https://arxiv.org/pdf/1806.08804.pdf\n    \n    ! code started from dgl diffpool examples dir\n""""""\n\nfrom .assignment_layer import DiffPoolAssignment\nfrom .dense_graphsage_layer import DenseGraphSage\n\n\nclass EntropyLoss(nn.Module):\n    # Return Scalar\n    # loss used in diffpool\n    def forward(self, adj, anext, s_l):\n        entropy = (torch.distributions.Categorical(\n            probs=s_l).entropy()).sum(-1).mean(-1)\n        assert not torch.isnan(entropy)\n        return entropy\n\n\nclass LinkPredLoss(nn.Module):\n    # loss used in diffpool\n    def forward(self, adj, anext, s_l):\n        link_pred_loss = (\n            adj - s_l.matmul(s_l.transpose(-1, -2))).norm(dim=(1, 2))\n        link_pred_loss = link_pred_loss / (adj.size(1) * adj.size(2))\n        return link_pred_loss.mean()\n\n\nclass DenseDiffPool(nn.Module):\n    def __init__(self, nfeat, nnext, nhid, link_pred=False, entropy=True):\n        super().__init__()\n        self.link_pred = link_pred\n        self.log = {}\n        self.link_pred_layer = self.LinkPredLoss()\n        self.embed = DenseGraphSage(nfeat, nhid, use_bn=True)\n        self.assign = DiffPoolAssignment(nfeat, nnext)\n        self.reg_loss = nn.ModuleList([])\n        self.loss_log = {}\n        if link_pred:\n            self.reg_loss.append(LinkPredLoss())\n        if entropy:\n            self.reg_loss.append(EntropyLoss())\n\n    def forward(self, x, adj, log=False):\n        z_l = self.embed(x, adj)\n        s_l = self.assign(x, adj)\n        if log:\n            self.log[\'s\'] = s_l.cpu().numpy()\n        xnext = torch.matmul(s_l.transpose(-1, -2), z_l)\n        anext = (s_l.transpose(-1, -2)).matmul(adj).matmul(s_l)\n\n        for loss_layer in self.reg_loss:\n            loss_name = str(type(loss_layer).__name__)\n            self.loss_log[loss_name] = loss_layer(adj, anext, s_l)\n        if log:\n            self.log[\'a\'] = anext.cpu().numpy()\n        return xnext, anext\n\n'"
layers/tensorized/dense_graphsage_layer.py,3,"b'import torch\nfrom torch import nn as nn\nfrom torch.nn import functional as F\n\n""""""\n    <Dense/Tensorzied version of the GraphSage layer>\n    \n    GraphSAGE: \n    William L. Hamilton, Rex Ying, Jure Leskovec, Inductive Representation Learning on Large Graphs (NeurIPS 2017)\n    https://cs.stanford.edu/people/jure/pubs/graphsage-nips17.pdf\n    \n    ! code started from the dgl diffpool examples dir\n""""""\n\nclass DenseGraphSage(nn.Module):\n    def __init__(self, infeat, outfeat, residual=False, use_bn=True,\n                 mean=False, add_self=False):\n        super().__init__()\n        self.add_self = add_self\n        self.use_bn = use_bn\n        self.mean = mean\n        self.residual = residual\n        \n        if infeat != outfeat:\n            self.residual = False\n        \n        self.W = nn.Linear(infeat, outfeat, bias=True)\n\n        nn.init.xavier_uniform_(\n            self.W.weight,\n            gain=nn.init.calculate_gain(\'relu\'))\n\n    def forward(self, x, adj):\n        h_in = x               # for residual connection\n        \n        if self.use_bn and not hasattr(self, \'bn\'):\n            self.bn = nn.BatchNorm1d(adj.size(1)).to(adj.device)\n\n        if self.add_self:\n            adj = adj + torch.eye(adj.size(0)).to(adj.device)\n\n        if self.mean:\n            adj = adj / adj.sum(1, keepdim=True)\n\n        h_k_N = torch.matmul(adj, x)\n        h_k = self.W(h_k_N)\n        h_k = F.normalize(h_k, dim=2, p=2)\n        h_k = F.relu(h_k)\n        \n        if self.residual:\n            h_k = h_in + h_k    # residual connection\n        \n        if self.use_bn:\n            h_k = self.bn(h_k)\n        return h_k\n\n    def __repr__(self):\n        if self.use_bn:\n            return \'BN\' + super(DenseGraphSage, self).__repr__()\n        else:\n            return super(DenseGraphSage, self).__repr__()'"
nets/CitationGraphs_node_classification/gat_net.py,2,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nimport dgl\nfrom dgl.nn.pytorch import GATConv\n\n""""""\n    GAT: Graph Attention Network\n    Graph Attention Networks (Veli\xc4\x8dkovi\xc4\x87 et al., ICLR 2018)\n    https://arxiv.org/abs/1710.10903\n""""""\nfrom layers.gat_layer import GATLayer\n\nclass GATNet(nn.Module):\n\n    def __init__(self, net_params):\n        super().__init__()\n\n        in_dim = net_params[\'in_dim\']\n        hidden_dim = net_params[\'hidden_dim\']\n        out_dim = net_params[\'out_dim\']\n        n_classes = net_params[\'n_classes\']\n        num_heads = net_params[\'n_heads\']\n        in_feat_dropout = net_params[\'in_feat_dropout\']\n        dropout = net_params[\'dropout\']\n        n_layers = net_params[\'L\']\n\n        self.graph_norm = net_params[\'graph_norm\']\n        self.batch_norm = net_params[\'batch_norm\']\n        self.residual = net_params[\'residual\']\n        self.dropout = dropout\n        self.n_classes = n_classes\n        self.device = net_params[\'device\']\n        self.dgl_builtin = net_params[\'builtin\']\n\n        feat_drop = dropout\n        attn_drop = dropout \n        negative_slope = 0.2\n        residual = False\n        self.layers = nn.ModuleList()\n        self.activation = F.elu\n        # input projection (no residual)\n        self.layers.append(GATLayer(\n            in_dim, hidden_dim, num_heads,\n            dropout, self.graph_norm, self.batch_norm, self.residual,\n            activation=self.activation, dgl_builtin=self.dgl_builtin))\n        # hidden layers\n        for l in range(1, n_layers):\n            # due to multi-head, the in_dim = hidden_dim * num_heads\n            self.layers.append(GATLayer(\n                hidden_dim * num_heads, hidden_dim, num_heads,\n                dropout, self.graph_norm, self.batch_norm, self.residual,\n                activation=self.activation, dgl_builtin=self.dgl_builtin))\n        # output projection\n        self.layers.append(GATLayer(\n                hidden_dim * num_heads, n_classes, 1,\n                dropout, self.graph_norm, self.batch_norm, self.residual,\n                activation=None, dgl_builtin=self.dgl_builtin))\n\n    def forward(self, g, h, e, snorm_n, snorm_e):\n\n        for conv in self.layers[:-1]:\n            h = conv(g, h, snorm_n)\n\n        h = self.layers[-1](g, h, snorm_n)\n\n        return h\n    \n    \n    def loss(self, pred, label):\n        # Cross-entropy \n        criterion = nn.CrossEntropyLoss()\n        loss = criterion(pred, label)\n\n        return loss\n'"
nets/CitationGraphs_node_classification/gated_gcn_net.py,2,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nimport dgl\nimport numpy as np\n\n""""""\n    ResGatedGCN: Residual Gated Graph ConvNets\n    An Experimental Study of Neural Networks for Variable Graphs (Xavier Bresson and Thomas Laurent, ICLR 2018)\n    https://arxiv.org/pdf/1711.07553v2.pdf\n""""""\nfrom layers.gated_gcn_layer import GatedGCNLayer\nfrom layers.mlp_readout_layer import MLPReadout\n\nclass GatedGCNNet(nn.Module):\n    \n    def __init__(self, net_params):\n        super().__init__()\n\n        in_dim = net_params[\'in_dim\']\n        hidden_dim = net_params[\'hidden_dim\']\n        n_classes = net_params[\'n_classes\']\n        dropout = net_params[\'dropout\']\n        n_layers = net_params[\'L\']\n        self.graph_norm = net_params[\'graph_norm\']\n        self.batch_norm = net_params[\'batch_norm\']\n        self.residual = net_params[\'residual\']\n        self.n_classes = n_classes\n        self.device = net_params[\'device\']\n        \n        self.embedding_h = nn.Linear(in_dim, hidden_dim)\n        self.embedding_e = nn.Linear(in_dim, hidden_dim)\n        self.layers = nn.ModuleList([ GatedGCNLayer(hidden_dim, hidden_dim, dropout,\n                                                       self.graph_norm, self.batch_norm, self.residual) for _ in range(n_layers) ])\n        self.MLP_layer = MLPReadout(hidden_dim, n_classes, L=0)\n        \n\n    def forward(self, g, h, e, snorm_n, snorm_e):\n\n        # input embedding\n        h = self.embedding_h(h)\n        e = self.embedding_e(e)\n        \n        # res gated convnets\n        for conv in self.layers:\n            h, e = conv(g, h, e, snorm_n.unsqueeze(-1), snorm_e.unsqueeze(-1))\n\n        # output\n        h_out = self.MLP_layer(h)\n\n        return h_out\n        \n\n    def loss(self, pred, label):\n\n        criterion = nn.CrossEntropyLoss()\n        loss = criterion(pred, label)\n\n        return loss\n\n'"
nets/CitationGraphs_node_classification/gcn_net.py,2,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nimport dgl\nimport numpy as np\n\n""""""\n    GCN: Graph Convolutional Networks\n    Thomas N. Kipf, Max Welling, Semi-Supervised Classification with Graph Convolutional Networks (ICLR 2017)\n    http://arxiv.org/abs/1609.02907\n""""""\nfrom layers.gcn_layer import GCNLayer\nfrom dgl.nn.pytorch import GraphConv\n\nclass GCNNet(nn.Module):\n\n    def __init__(self, net_params):\n        super().__init__()\n\n        in_dim = net_params[\'in_dim\']\n        hidden_dim = net_params[\'hidden_dim\']\n        out_dim = net_params[\'out_dim\']\n        n_classes = net_params[\'n_classes\']\n        in_feat_dropout = net_params[\'in_feat_dropout\']\n        dropout = net_params[\'dropout\']\n        n_layers = net_params[\'L\']\n        self.graph_norm = net_params[\'graph_norm\']\n        self.batch_norm = net_params[\'batch_norm\']\n        self.residual = net_params[\'residual\']\n        self.n_classes = n_classes\n        self.device = net_params[\'device\']\n        self.dgl_builtin = net_params[\'builtin\']\n\n        self.layers = nn.ModuleList()\n        # input\n        self.layers.append(GCNLayer(in_dim, hidden_dim, F.relu, dropout,\n            self.graph_norm, self.batch_norm, self.residual,\n            dgl_builtin=self.dgl_builtin))\n\n        # hidden\n        self.layers.extend(nn.ModuleList([GCNLayer(hidden_dim, hidden_dim,\n            F.relu, dropout, self.graph_norm, self.batch_norm, self.residual,\n            dgl_builtin=self.dgl_builtin)\n            for _ in range(n_layers-1)]))\n\n        # output\n        self.layers.append(GCNLayer(hidden_dim, n_classes, None, 0,\n            self.graph_norm, self.batch_norm, self.residual,\n            dgl_builtin=self.dgl_builtin))\n\n        self.dropout = nn.Dropout(p=dropout)\n\n    def forward(self, g, h, e, snorm_n, snorm_e):\n      \n        # GCN\n        for i, conv in enumerate(self.layers):\n            h = conv(g, h, snorm_n)\n        return h\n\n    \n    def loss(self, pred, label):\n\n        criterion = nn.CrossEntropyLoss()\n        loss = criterion(pred, label)\n\n        return loss\n\n\n\n\n\n\n\n\n\n\n\n'"
nets/CitationGraphs_node_classification/gin_net.py,4,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nimport dgl\nfrom dgl.nn.pytorch.glob import SumPooling, AvgPooling, MaxPooling\n\n""""""\n    GIN: Graph Isomorphism Networks\n    HOW POWERFUL ARE GRAPH NEURAL NETWORKS? (Keyulu Xu, Weihua Hu, Jure Leskovec and Stefanie Jegelka, ICLR 2019)\n    https://arxiv.org/pdf/1810.00826.pdf\n""""""\n\nfrom layers.gin_layer import GINLayer, ApplyNodeFunc, MLP\n\nclass GINNet(nn.Module):\n    \n    def __init__(self, net_params):\n        super().__init__()\n        in_dim = net_params[\'in_dim\']\n        hidden_dim = net_params[\'hidden_dim\']\n        n_classes = net_params[\'n_classes\']\n        dropout = net_params[\'dropout\']\n        self.n_layers = net_params[\'L\']\n        n_mlp_layers = net_params[\'n_mlp_GIN\']               # GIN\n        learn_eps = net_params[\'learn_eps_GIN\']              # GIN\n        neighbor_aggr_type = net_params[\'neighbor_aggr_GIN\'] # GIN\n        graph_norm = net_params[\'graph_norm\']      \n        batch_norm = net_params[\'batch_norm\']\n        residual = net_params[\'residual\']     \n        \n        # List of MLPs\n        self.ginlayers = torch.nn.ModuleList()\n        \n        self.embedding_h = nn.Linear(in_dim, hidden_dim)\n       \n        # Input layer\n        mlp = MLP(1, in_dim, hidden_dim, hidden_dim)\n        self.ginlayers.append(GINLayer(ApplyNodeFunc(mlp), neighbor_aggr_type,\n                                       dropout, graph_norm, batch_norm,\n                                       residual, 0, learn_eps,\n                                       activation=F.relu))\n\n        # Hidden layers\n        for layer in range(self.n_layers-1):\n            mlp = MLP(n_mlp_layers, hidden_dim, hidden_dim, hidden_dim)\n            \n            self.ginlayers.append(GINLayer(ApplyNodeFunc(mlp),\n                neighbor_aggr_type, dropout, graph_norm, batch_norm, residual,\n                0, learn_eps, activation=F.relu))\n\n        # Output layer\n        mlp = MLP(1, hidden_dim, n_classes, n_classes)\n        self.ginlayers.append(GINLayer(ApplyNodeFunc(mlp), neighbor_aggr_type,\n                                       dropout, graph_norm, batch_norm,\n                                       residual, 0, learn_eps))\n        \n    def forward(self, g, h, e, snorm_n, snorm_e):\n        \n        for i in range(self.n_layers):\n            h = self.ginlayers[i](g, h, snorm_n)\n\n\n        return h\n        \n    def loss(self, pred, label):\n        criterion = nn.CrossEntropyLoss()\n        loss = criterion(pred, label)\n        return loss\n'"
nets/CitationGraphs_node_classification/graphsage_net.py,3,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nimport dgl\n\n""""""\n    GraphSAGE: \n    William L. Hamilton, Rex Ying, Jure Leskovec, Inductive Representation Learning on Large Graphs (NeurIPS 2017)\n    https://cs.stanford.edu/people/jure/pubs/graphsage-nips17.pdf\n""""""\n\nfrom layers.graphsage_layer import GraphSageLayer\nfrom layers.mlp_readout_layer import MLPReadout\nfrom dgl.nn.pytorch.conv import SAGEConv\n\nclass GraphSageNet(nn.Module):\n    """"""\n    Grahpsage network with multiple GraphSageLayer layers\n    """"""\n    def __init__(self, net_params):\n        super().__init__()\n        in_dim = net_params[\'in_dim\']\n        hidden_dim = net_params[\'hidden_dim\']\n        out_dim = net_params[\'out_dim\']\n        n_classes = net_params[\'n_classes\']\n        in_feat_dropout = net_params[\'in_feat_dropout\']\n        dropout = net_params[\'dropout\']\n        aggregator_type = net_params[\'sage_aggregator\']\n        n_layers = net_params[\'L\']\n        self.residual = net_params[\'residual\']\n        dgl_builtin = net_params[\'builtin\']\n        bnorm = net_params[\'batch_norm\']\n        \n        self.layers = nn.ModuleList()\n        # Input\n        self.layers.append(GraphSageLayer(in_dim, hidden_dim, F.relu,\n            dropout, aggregator_type, self.residual,\n            batch_norm=bnorm, dgl_builtin=dgl_builtin))\n        # Hidden layers\n        self.layers.extend(nn.ModuleList([GraphSageLayer(hidden_dim,\n            hidden_dim, F.relu, dropout, aggregator_type, self.residual,\n            batch_norm=bnorm, dgl_builtin=dgl_builtin) for _ in range(n_layers-1)]))\n        # Output layer\n        self.layers.append(GraphSageLayer(hidden_dim, n_classes, None,\n            dropout, aggregator_type, self.residual, batch_norm=bnorm,\n            dgl_builtin=dgl_builtin))\n        \n    def forward(self, g, h, e, snorm_n, snorm_e):\n        for conv in self.layers:\n            h = conv(g, h)\n        return h\n\n        \n    def loss(self, pred, label):\n        criterion = nn.CrossEntropyLoss()\n        loss = criterion(pred, label)\n        return loss\n    \n'"
nets/CitationGraphs_node_classification/load_net.py,0,"b'""""""\n    Utility file to select GraphNN model as\n    selected by the user\n""""""\n\nfrom nets.CitationGraphs_node_classification.gcn_net import GCNNet\nfrom nets.CitationGraphs_node_classification.gat_net import GATNet\nfrom nets.CitationGraphs_node_classification.graphsage_net import GraphSageNet\nfrom nets.CitationGraphs_node_classification.mlp_net import MLPNet\n\n\n\ndef GCN(net_params):\n    return GCNNet(net_params)\n\ndef GAT(net_params):\n    return GATNet(net_params)\n\ndef GraphSage(net_params):\n    return GraphSageNet(net_params)\n\ndef MLP(net_params):\n    return MLPNet(net_params)\n\ndef gnn_model(MODEL_NAME, net_params):\n    models = {\n        \'GCN\': GCN,\n        \'GAT\': GAT,\n        \'GraphSage\': GraphSage,\n        \'MLP\': MLP,\n    }\n        \n    return models[MODEL_NAME](net_params)\n'"
nets/CitationGraphs_node_classification/mlp_net.py,3,"b""import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nimport dgl\n\nfrom layers.mlp_readout_layer import MLPReadout\n\nclass MLPNet(nn.Module):\n    def __init__(self, net_params):\n        super().__init__()\n        in_dim = net_params['in_dim']\n        hidden_dim = net_params['hidden_dim']\n        n_classes = net_params['n_classes']\n        in_feat_dropout = net_params['in_feat_dropout']\n        dropout = net_params['dropout']\n        n_layers = net_params['L']\n        self.gated = net_params['gated']\n        \n        self.in_feat_dropout = nn.Dropout(in_feat_dropout)\n        \n        feat_mlp_modules = [\n            nn.Linear(in_dim, hidden_dim, bias=True),\n            nn.ReLU(),\n            nn.Dropout(dropout),\n        ]\n        for _ in range(n_layers-1):\n            feat_mlp_modules.append(nn.Linear(hidden_dim, hidden_dim, bias=True))\n            feat_mlp_modules.append(nn.ReLU())\n            feat_mlp_modules.append(nn.Dropout(dropout))\n        self.feat_mlp = nn.Sequential(*feat_mlp_modules)\n        \n        if self.gated:\n            self.gates = nn.Linear(hidden_dim, hidden_dim, bias=True)\n        \n        self.readout_mlp = MLPReadout(hidden_dim, n_classes)\n\n    def forward(self, g, h, e, snorm_n, snorm_e):\n        h = self.in_feat_dropout(h)\n        h = self.feat_mlp(h)\n        if self.gated:\n            h = torch.sigmoid(self.gates(h)) * h\n            g.ndata['h'] = h       \n        \n        else:\n            g.ndata['h'] = h\n        \n        h_out = self.readout_mlp(h)\n        return h_out\n\n        \n    def loss(self, pred, label):\n        criterion = nn.CrossEntropyLoss()\n        loss = criterion(pred, label)\n        return loss\n       \n"""
nets/SBMs_node_classification/gat_net.py,5,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nimport dgl\n\n""""""\n    GAT: Graph Attention Network\n    Graph Attention Networks (Veli\xc4\x8dkovi\xc4\x87 et al., ICLR 2018)\n    https://arxiv.org/abs/1710.10903\n""""""\nfrom layers.gat_layer import GATLayer\nfrom layers.mlp_readout_layer import MLPReadout\n\nclass GATNet(nn.Module):\n\n    def __init__(self, net_params):\n        super().__init__()\n\n        in_dim_node = net_params[\'in_dim\'] # node_dim (feat is an integer)\n        hidden_dim = net_params[\'hidden_dim\']\n        out_dim = net_params[\'out_dim\']\n        n_classes = net_params[\'n_classes\']\n        num_heads = net_params[\'n_heads\']\n        in_feat_dropout = net_params[\'in_feat_dropout\']\n        dropout = net_params[\'dropout\']\n        n_layers = net_params[\'L\']\n\n        self.readout = net_params[\'readout\']\n        self.graph_norm = net_params[\'graph_norm\']\n        self.batch_norm = net_params[\'batch_norm\']\n        self.residual = net_params[\'residual\']\n        self.dropout = dropout\n        self.n_classes = n_classes\n        self.device = net_params[\'device\']\n        \n        self.embedding_h = nn.Embedding(in_dim_node, hidden_dim * num_heads) # node feat is an integer\n        \n        self.in_feat_dropout = nn.Dropout(in_feat_dropout)\n        \n        self.layers = nn.ModuleList([GATLayer(hidden_dim * num_heads, hidden_dim, num_heads,\n                                              dropout, self.graph_norm, self.batch_norm, self.residual) for _ in range(n_layers-1)])\n        self.layers.append(GATLayer(hidden_dim * num_heads, out_dim, 1, dropout, self.graph_norm, self.batch_norm, self.residual))\n        self.MLP_layer = MLPReadout(out_dim, n_classes)\n\n\n    def forward(self, g, h, e, snorm_n, snorm_e):\n\n        # input embedding\n        h = self.embedding_h(h)\n        h = self.in_feat_dropout(h)\n\n        # GAT\n        for conv in self.layers:\n            h = conv(g, h, snorm_n)\n            \n        # output\n        h_out = self.MLP_layer(h)\n\n        return h_out\n    \n    \n    def loss(self, pred, label):\n\n        # calculating label weights for weighted loss computation\n        V = label.size(0)\n        label_count = torch.bincount(label)\n        label_count = label_count[label_count.nonzero()].squeeze()\n        cluster_sizes = torch.zeros(self.n_classes).long().to(self.device)\n        cluster_sizes[torch.unique(label)] = label_count\n        weight = (V - cluster_sizes).float() / V\n        weight *= (cluster_sizes>0).float()\n        \n        # weighted cross-entropy for unbalanced classes\n        criterion = nn.CrossEntropyLoss(weight=weight)\n        loss = criterion(pred, label)\n\n        return loss\n\n\n\n        \n'"
nets/SBMs_node_classification/gated_gcn_net.py,5,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nimport dgl\nimport numpy as np\n\n""""""\n    ResGatedGCN: Residual Gated Graph ConvNets\n    An Experimental Study of Neural Networks for Variable Graphs (Xavier Bresson and Thomas Laurent, ICLR 2018)\n    https://arxiv.org/pdf/1711.07553v2.pdf\n""""""\nfrom layers.gated_gcn_layer import GatedGCNLayer\nfrom layers.mlp_readout_layer import MLPReadout\n\nclass GatedGCNNet(nn.Module):\n    \n    def __init__(self, net_params):\n        super().__init__()\n\n        in_dim_node = net_params[\'in_dim\'] # node_dim (feat is an integer)\n        in_dim_edge = 1 # edge_dim (feat is a float)\n        hidden_dim = net_params[\'hidden_dim\']\n        n_classes = net_params[\'n_classes\']\n        dropout = net_params[\'dropout\']\n        n_layers = net_params[\'L\']\n        self.readout = net_params[\'readout\']\n        self.graph_norm = net_params[\'graph_norm\']\n        self.batch_norm = net_params[\'batch_norm\']\n        self.residual = net_params[\'residual\']\n        self.n_classes = n_classes\n        self.device = net_params[\'device\']\n        \n        self.embedding_h = nn.Embedding(in_dim_node, hidden_dim) # node feat is an integer\n        self.embedding_e = nn.Linear(in_dim_edge, hidden_dim) # edge feat is a float\n        self.layers = nn.ModuleList([ GatedGCNLayer(hidden_dim, hidden_dim, dropout,\n                                                       self.graph_norm, self.batch_norm, self.residual) for _ in range(n_layers) ])\n        self.MLP_layer = MLPReadout(hidden_dim, n_classes)\n        \n\n    def forward(self, g, h, e, snorm_n, snorm_e):\n\n        # input embedding\n        h = self.embedding_h(h)\n        e = self.embedding_e(e)\n        \n        # res gated convnets\n        for conv in self.layers:\n            h, e = conv(g, h, e, snorm_n, snorm_e)\n\n        # output\n        h_out = self.MLP_layer(h)\n\n        return h_out\n        \n\n    def loss(self, pred, label):\n\n        # calculating label weights for weighted loss computation\n        V = label.size(0)\n        label_count = torch.bincount(label)\n        label_count = label_count[label_count.nonzero()].squeeze()\n        cluster_sizes = torch.zeros(self.n_classes).long().to(self.device)\n        cluster_sizes[torch.unique(label)] = label_count\n        weight = (V - cluster_sizes).float() / V\n        weight *= (cluster_sizes>0).float()\n        \n        # weighted cross-entropy for unbalanced classes\n        criterion = nn.CrossEntropyLoss(weight=weight)\n        loss = criterion(pred, label)\n\n        return loss\n\n'"
nets/SBMs_node_classification/gcn_net.py,5,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nimport dgl\nimport numpy as np\n\n""""""\n    GCN: Graph Convolutional Networks\n    Thomas N. Kipf, Max Welling, Semi-Supervised Classification with Graph Convolutional Networks (ICLR 2017)\n    http://arxiv.org/abs/1609.02907\n""""""\nfrom layers.gcn_layer import GCNLayer\nfrom layers.mlp_readout_layer import MLPReadout\n\nclass GCNNet(nn.Module):\n\n    def __init__(self, net_params):\n        super().__init__()\n\n        in_dim_node = net_params[\'in_dim\'] # node_dim (feat is an integer)\n        hidden_dim = net_params[\'hidden_dim\']\n        out_dim = net_params[\'out_dim\']\n        n_classes = net_params[\'n_classes\']\n        in_feat_dropout = net_params[\'in_feat_dropout\']\n        dropout = net_params[\'dropout\']\n        n_layers = net_params[\'L\']\n        self.readout = net_params[\'readout\']\n        self.graph_norm = net_params[\'graph_norm\']\n        self.batch_norm = net_params[\'batch_norm\']\n        self.residual = net_params[\'residual\']\n        self.n_classes = n_classes\n        self.device = net_params[\'device\']\n        \n        self.embedding_h = nn.Embedding(in_dim_node, hidden_dim) # node feat is an integer\n        self.in_feat_dropout = nn.Dropout(in_feat_dropout)\n        self.layers = nn.ModuleList([GCNLayer(hidden_dim, hidden_dim, F.relu, dropout,\n                                              self.graph_norm, self.batch_norm, self.residual) for _ in range(n_layers-1)])\n        self.layers.append(GCNLayer(hidden_dim, out_dim, F.relu, dropout, self.graph_norm, self.batch_norm, self.residual))\n        self.MLP_layer = MLPReadout(out_dim, n_classes)        \n\n\n    def forward(self, g, h, e, snorm_n, snorm_e):\n\n        # input embedding\n        h = self.embedding_h(h)\n        h = self.in_feat_dropout(h)\n        \n        # GCN\n        for conv in self.layers:\n            h = conv(g, h, snorm_n)\n\n        # output\n        h_out = self.MLP_layer(h)\n\n        return h_out\n\n    \n    def loss(self, pred, label):\n\n        # calculating label weights for weighted loss computation\n        V = label.size(0)\n        label_count = torch.bincount(label)\n        label_count = label_count[label_count.nonzero()].squeeze()\n        cluster_sizes = torch.zeros(self.n_classes).long().to(self.device)\n        cluster_sizes[torch.unique(label)] = label_count\n        weight = (V - cluster_sizes).float() / V\n        weight *= (cluster_sizes>0).float()\n        \n        # weighted cross-entropy for unbalanced classes\n        criterion = nn.CrossEntropyLoss(weight=weight)\n        loss = criterion(pred, label)\n\n        return loss\n\n\n\n\n\n\n\n\n\n\n\n'"
nets/SBMs_node_classification/gin_net.py,8,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nimport dgl\nfrom dgl.nn.pytorch.glob import SumPooling, AvgPooling, MaxPooling\n\n""""""\n    GIN: Graph Isomorphism Networks\n    HOW POWERFUL ARE GRAPH NEURAL NETWORKS? (Keyulu Xu, Weihua Hu, Jure Leskovec and Stefanie Jegelka, ICLR 2019)\n    https://arxiv.org/pdf/1810.00826.pdf\n""""""\n\nfrom layers.gin_layer import GINLayer, ApplyNodeFunc, MLP\n\nclass GINNet(nn.Module):\n    \n    def __init__(self, net_params):\n        super().__init__()\n        in_dim = net_params[\'in_dim\']\n        hidden_dim = net_params[\'hidden_dim\']\n        n_classes = net_params[\'n_classes\']\n        dropout = net_params[\'dropout\']\n        self.n_layers = net_params[\'L\']\n        n_mlp_layers = net_params[\'n_mlp_GIN\']               # GIN\n        learn_eps = net_params[\'learn_eps_GIN\']              # GIN\n        neighbor_aggr_type = net_params[\'neighbor_aggr_GIN\'] # GIN\n        readout = net_params[\'readout\']                      # this is graph_pooling_type\n        graph_norm = net_params[\'graph_norm\']      \n        batch_norm = net_params[\'batch_norm\']\n        residual = net_params[\'residual\']\n        self.n_classes = n_classes\n        self.device = net_params[\'device\']\n        \n        # List of MLPs\n        self.ginlayers = torch.nn.ModuleList()\n        \n        self.embedding_h = nn.Embedding(in_dim, hidden_dim)\n        \n        for layer in range(self.n_layers):\n            mlp = MLP(n_mlp_layers, hidden_dim, hidden_dim, hidden_dim)\n            \n            self.ginlayers.append(GINLayer(ApplyNodeFunc(mlp), neighbor_aggr_type,\n                                           dropout, graph_norm, batch_norm, residual, 0, learn_eps))\n\n        # Linear function for output of each layer\n        # which maps the output of different layers into a prediction score\n        self.linears_prediction = torch.nn.ModuleList()\n\n        for layer in range(self.n_layers+1):\n            self.linears_prediction.append(nn.Linear(hidden_dim, n_classes))\n        \n        \n    def forward(self, g, h, e, snorm_n, snorm_e):\n        \n        h = self.embedding_h(h)\n        \n        # list of hidden representation at each layer (including input)\n        hidden_rep = [h]\n\n        for i in range(self.n_layers):\n            h = self.ginlayers[i](g, h, snorm_n)\n            hidden_rep.append(h)\n\n        score_over_layer = 0\n\n        for i, h in enumerate(hidden_rep):\n            score_over_layer += self.linears_prediction[i](h)\n\n        return score_over_layer\n        \n    def loss(self, pred, label):\n\n        # calculating label weights for weighted loss computation\n        V = label.size(0)\n        label_count = torch.bincount(label)\n        label_count = label_count[label_count.nonzero()].squeeze()\n        cluster_sizes = torch.zeros(self.n_classes).long().to(self.device)\n        cluster_sizes[torch.unique(label)] = label_count\n        weight = (V - cluster_sizes).float() / V\n        weight *= (cluster_sizes>0).float()\n        \n        # weighted cross-entropy for unbalanced classes\n        criterion = nn.CrossEntropyLoss(weight=weight)\n        loss = criterion(pred, label)\n\n        return loss'"
nets/SBMs_node_classification/graphsage_net.py,5,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nimport dgl\n\n""""""\n    GraphSAGE: \n    William L. Hamilton, Rex Ying, Jure Leskovec, Inductive Representation Learning on Large Graphs (NeurIPS 2017)\n    https://cs.stanford.edu/people/jure/pubs/graphsage-nips17.pdf\n""""""\n\nfrom layers.graphsage_layer import GraphSageLayer\nfrom layers.mlp_readout_layer import MLPReadout\n\nclass GraphSageNet(nn.Module):\n    """"""\n    Grahpsage network with multiple GraphSageLayer layers\n    """"""\n\n    def __init__(self, net_params):\n        super().__init__()\n\n        in_dim_node = net_params[\'in_dim\'] # node_dim (feat is an integer)\n        hidden_dim = net_params[\'hidden_dim\']\n        out_dim = net_params[\'out_dim\']\n        n_classes = net_params[\'n_classes\']\n        in_feat_dropout = net_params[\'in_feat_dropout\']\n        dropout = net_params[\'dropout\']\n        aggregator_type = net_params[\'sage_aggregator\']\n        n_layers = net_params[\'L\']\n        graph_norm = net_params[\'graph_norm\']      \n        batch_norm = net_params[\'batch_norm\']\n        residual = net_params[\'residual\']\n        self.readout = net_params[\'readout\']\n        self.n_classes = n_classes\n        self.device = net_params[\'device\']\n        \n        self.embedding_h = nn.Embedding(in_dim_node, hidden_dim) # node feat is an integer\n        self.in_feat_dropout = nn.Dropout(in_feat_dropout)\n        \n        self.layers = nn.ModuleList([GraphSageLayer(hidden_dim, hidden_dim, F.relu,\n                                              dropout, aggregator_type, graph_norm, batch_norm, residual) for _ in range(n_layers-1)])\n        self.layers.append(GraphSageLayer(hidden_dim, out_dim, F.relu, dropout, aggregator_type, graph_norm, batch_norm, residual))\n        self.MLP_layer = MLPReadout(out_dim, n_classes)\n        \n\n    def forward(self, g, h, e, snorm_n, snorm_e):\n\n        # input embedding\n        h = self.embedding_h(h)\n        h = self.in_feat_dropout(h)\n\n        # graphsage\n        for conv in self.layers:\n            h = conv(g, h, snorm_n)\n\n        # output\n        h_out = self.MLP_layer(h)\n\n        return h_out\n    \n\n    def loss(self, pred, label):\n\n        # calculating label weights for weighted loss computation\n        V = label.size(0)\n        label_count = torch.bincount(label)\n        label_count = label_count[label_count.nonzero()].squeeze()\n        cluster_sizes = torch.zeros(self.n_classes).long().to(self.device)\n        cluster_sizes[torch.unique(label)] = label_count\n        weight = (V - cluster_sizes).float() / V\n        weight *= (cluster_sizes>0).float()\n        \n        # weighted cross-entropy for unbalanced classes\n        criterion = nn.CrossEntropyLoss(weight=weight)\n        loss = criterion(pred, label)\n\n        return loss\n\n\n        \n'"
nets/SBMs_node_classification/load_net.py,0,"b'""""""\n    Utility file to select GraphNN model as\n    selected by the user\n""""""\n\nfrom nets.SBMs_node_classification.gated_gcn_net import GatedGCNNet\nfrom nets.SBMs_node_classification.gcn_net import GCNNet\nfrom nets.SBMs_node_classification.gat_net import GATNet\nfrom nets.SBMs_node_classification.graphsage_net import GraphSageNet\nfrom nets.SBMs_node_classification.mlp_net import MLPNet\nfrom nets.SBMs_node_classification.gin_net import GINNet\nfrom nets.SBMs_node_classification.mo_net import MoNet as MoNet_\n\n\ndef GatedGCN(net_params):\n    return GatedGCNNet(net_params)\n\ndef GCN(net_params):\n    return GCNNet(net_params)\n\ndef GAT(net_params):\n    return GATNet(net_params)\n\ndef GraphSage(net_params):\n    return GraphSageNet(net_params)\n\ndef MLP(net_params):\n    return MLPNet(net_params)\n\ndef GIN(net_params):\n    return GINNet(net_params)\n\ndef MoNet(net_params):\n    return MoNet_(net_params)\n\ndef gnn_model(MODEL_NAME, net_params):\n    models = {\n        \'GatedGCN\': GatedGCN,\n        \'GCN\': GCN,\n        \'GAT\': GAT,\n        \'GraphSage\': GraphSage,\n        \'MLP\': MLP,\n        \'GIN\': GIN,\n        \'MoNet\': MoNet\n    }\n        \n    return models[MODEL_NAME](net_params)'"
nets/SBMs_node_classification/mlp_net.py,6,"b""import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nimport dgl\n\nfrom layers.mlp_readout_layer import MLPReadout\n\n\nclass MLPNet(nn.Module):\n\n    def __init__(self, net_params):\n        super().__init__()\n\n        in_dim_node = net_params['in_dim'] # node_dim (feat is an integer)\n        hidden_dim = net_params['hidden_dim']\n        n_classes = net_params['n_classes']\n        in_feat_dropout = net_params['in_feat_dropout']\n        dropout = net_params['dropout']\n        n_layers = net_params['L']\n        self.gated = net_params['gated']\n        self.n_classes = n_classes\n        self.device = net_params['device']\n        \n        self.embedding_h = nn.Embedding(in_dim_node, hidden_dim) # node feat is an integer\n        self.in_feat_dropout = nn.Dropout(in_feat_dropout)\n        \n        feat_mlp_modules = [\n            nn.Linear(hidden_dim, hidden_dim, bias=True),\n            nn.ReLU(),\n            nn.Dropout(dropout),\n        ]\n        for _ in range(n_layers-1):\n            feat_mlp_modules.append(nn.Linear(hidden_dim, hidden_dim, bias=True))\n            feat_mlp_modules.append(nn.ReLU())\n            feat_mlp_modules.append(nn.Dropout(dropout))\n        self.feat_mlp = nn.Sequential(*feat_mlp_modules)\n        \n        if self.gated:\n            self.gates = nn.Linear(hidden_dim, hidden_dim, bias=True)\n        \n        self.readout_mlp = MLPReadout(hidden_dim, n_classes)\n\n\n    def forward(self, g, h, e, snorm_n, snorm_e):\n\n        # input embedding\n        h = self.embedding_h(h)\n        h = self.in_feat_dropout(h)\n\n        # MLP\n        h = self.feat_mlp(h)\n        if self.gated:\n            h = torch.sigmoid(self.gates(h)) * h\n        \n        # output\n        h_out = self.readout_mlp(h)\n\n        return h_out\n\n        \n    def loss(self, pred, label):\n\n        # calculating label weights for weighted loss computation\n        V = label.size(0)\n        label_count = torch.bincount(label)\n        label_count = label_count[label_count.nonzero()].squeeze()\n        cluster_sizes = torch.zeros(self.n_classes).long().to(self.device)\n        cluster_sizes[torch.unique(label)] = label_count\n        weight = (V - cluster_sizes).float() / V\n        weight *= (cluster_sizes>0).float()\n        \n        # weighted cross-entropy for unbalanced classes\n        criterion = nn.CrossEntropyLoss(weight=weight)\n        loss = criterion(pred, label)\n\n        return loss\n\n\n\n        \n"""
nets/SBMs_node_classification/mo_net.py,6,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nimport dgl\n\nimport numpy as np\n\n""""""\n    GMM: Gaussian Mixture Model Convolution layer\n    Geometric Deep Learning on Graphs and Manifolds using Mixture Model CNNs (Federico Monti et al., CVPR 2017)\n    https://arxiv.org/pdf/1611.08402.pdf\n""""""\n\nfrom layers.gmm_layer import GMMLayer\nfrom layers.mlp_readout_layer import MLPReadout\n\nclass MoNet(nn.Module):\n    def __init__(self, net_params):\n        super().__init__()\n        \n        in_dim = net_params[\'in_dim\']\n        hidden_dim = net_params[\'hidden_dim\']\n        out_dim = net_params[\'out_dim\']\n        kernel = net_params[\'kernel\']                       # for MoNet\n        dim = net_params[\'pseudo_dim_MoNet\']                # for MoNet\n        n_classes = net_params[\'n_classes\']\n        dropout = net_params[\'dropout\']\n        n_layers = net_params[\'L\']\n        self.readout = net_params[\'readout\']                      \n        graph_norm = net_params[\'graph_norm\']      \n        batch_norm = net_params[\'batch_norm\']\n        residual = net_params[\'residual\']  \n        self.device = net_params[\'device\']\n        self.n_classes = n_classes\n        \n        aggr_type = ""sum""                                    # default for MoNet\n        \n        self.embedding_h = nn.Embedding(in_dim, hidden_dim)\n        \n        self.layers = nn.ModuleList()\n        self.pseudo_proj = nn.ModuleList()\n\n        # Hidden layer\n        for _ in range(n_layers-1):\n            self.layers.append(GMMLayer(hidden_dim, hidden_dim, dim, kernel, aggr_type,\n                                        dropout, graph_norm, batch_norm, residual))\n            self.pseudo_proj.append(nn.Sequential(nn.Linear(2, dim), nn.Tanh()))\n            \n        # Output layer\n        self.layers.append(GMMLayer(hidden_dim, out_dim, dim, kernel, aggr_type,\n                                    dropout, graph_norm, batch_norm, residual))\n        self.pseudo_proj.append(nn.Sequential(nn.Linear(2, dim), nn.Tanh()))\n        \n        self.MLP_layer = MLPReadout(out_dim, n_classes)\n\n    def forward(self, g, h, e, snorm_n, snorm_e):\n        h = self.embedding_h(h)\n        \n        # computing the \'pseudo\' named tensor which depends on node degrees\n        us, vs = g.edges()\n        # to avoid zero division in case in_degree is 0, we add constant \'1\' in all node degrees denoting self-loop\n        pseudo = [ [1/np.sqrt(g.in_degree(us[i])+1), 1/np.sqrt(g.in_degree(vs[i])+1)] for i in range(g.number_of_edges()) ]\n        pseudo = torch.Tensor(pseudo).to(self.device)\n        \n        for i in range(len(self.layers)):\n            h = self.layers[i](g, h, self.pseudo_proj[i](pseudo), snorm_n)\n\n        return self.MLP_layer(h)\n        \n    def loss(self, pred, label):\n\n        # calculating label weights for weighted loss computation\n        V = label.size(0)\n        label_count = torch.bincount(label)\n        label_count = label_count[label_count.nonzero()].squeeze()\n        cluster_sizes = torch.zeros(self.n_classes).long().to(self.device)\n        cluster_sizes[torch.unique(label)] = label_count\n        weight = (V - cluster_sizes).float() / V\n        weight *= (cluster_sizes>0).float()\n        \n        # weighted cross-entropy for unbalanced classes\n        criterion = nn.CrossEntropyLoss(weight=weight)\n        loss = criterion(pred, label)\n\n        return loss'"
nets/TSP_edge_classification/gat_net.py,3,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nimport dgl\n\n""""""\n    GAT: Graph Attention Network\n    Graph Attention Networks (Veli\xc4\x8dkovi\xc4\x87 et al., ICLR 2018)\n    https://arxiv.org/abs/1710.10903\n""""""\nfrom layers.gat_layer import GATLayer\nfrom layers.mlp_readout_layer import MLPReadout\n\nclass GATNet(nn.Module):\n    def __init__(self, net_params):\n        super().__init__()\n        in_dim = net_params[\'in_dim\']\n        hidden_dim = net_params[\'hidden_dim\']\n        out_dim = net_params[\'out_dim\']\n        n_classes = net_params[\'n_classes\']\n        num_heads = net_params[\'n_heads\']\n        in_feat_dropout = net_params[\'in_feat_dropout\']\n        dropout = net_params[\'dropout\']\n        n_layers = net_params[\'L\']\n        self.readout = net_params[\'readout\']\n        self.graph_norm = net_params[\'graph_norm\']\n        self.batch_norm = net_params[\'batch_norm\']\n        self.residual = net_params[\'residual\']\n        self.dropout = dropout\n        self.n_classes = n_classes\n        self.device = net_params[\'device\']\n        \n        self.embedding_h = nn.Linear(in_dim, hidden_dim * num_heads)\n        \n        self.in_feat_dropout = nn.Dropout(in_feat_dropout)\n        \n        self.layers = nn.ModuleList([GATLayer(hidden_dim * num_heads, hidden_dim, num_heads,\n                                              dropout, self.graph_norm, self.batch_norm, self.residual) for _ in range(n_layers-1)])\n        self.layers.append(GATLayer(hidden_dim * num_heads, out_dim, 1, dropout, self.graph_norm, self.batch_norm, self.residual))\n        self.MLP_layer = MLPReadout(2*out_dim, n_classes)\n        \n    def forward(self, g, h, e, snorm_n, snorm_e):\n        h = self.embedding_h(h.float())\n        h = self.in_feat_dropout(h)\n        for conv in self.layers:\n            h = conv(g, h, snorm_n)\n        g.ndata[\'h\'] = h\n        \n        def _edge_feat(edges):\n            e = torch.cat([edges.src[\'h\'], edges.dst[\'h\']], dim=1)\n            e = self.MLP_layer(e)\n            return {\'e\': e}\n        g.apply_edges(_edge_feat)\n        \n        return g.edata[\'e\']\n    \n    def loss(self, pred, label):\n        criterion = nn.CrossEntropyLoss(weight=None)\n        loss = criterion(pred, label)\n\n        return loss'"
nets/TSP_edge_classification/gated_gcn_net.py,4,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nimport dgl\n\n""""""\n    ResGatedGCN: Residual Gated Graph ConvNets\n    An Experimental Study of Neural Networks for Variable Graphs (Xavier Bresson and Thomas Laurent, ICLR 2018)\n    https://arxiv.org/pdf/1711.07553v2.pdf\n""""""\nfrom layers.gated_gcn_layer import GatedGCNLayer\nfrom layers.mlp_readout_layer import MLPReadout\n\nclass GatedGCNNet(nn.Module):\n    \n    def __init__(self, net_params):\n        super().__init__()\n        in_dim = net_params[\'in_dim\']\n        in_dim_edge = net_params[\'in_dim_edge\']\n        hidden_dim = net_params[\'hidden_dim\']\n        out_dim = net_params[\'out_dim\']\n        n_classes = net_params[\'n_classes\']\n        dropout = net_params[\'dropout\']\n        n_layers = net_params[\'L\']\n        self.readout = net_params[\'readout\']\n        self.graph_norm = net_params[\'graph_norm\']\n        self.batch_norm = net_params[\'batch_norm\']\n        self.residual = net_params[\'residual\']\n        self.edge_feat = net_params[\'edge_feat\']\n        self.n_classes = n_classes\n        self.device = net_params[\'device\']\n        \n        self.embedding_h = nn.Linear(in_dim, hidden_dim)\n        self.embedding_e = nn.Linear(in_dim_edge, hidden_dim)\n        self.layers = nn.ModuleList([ GatedGCNLayer(hidden_dim, hidden_dim, dropout,\n                                                       self.graph_norm, self.batch_norm, self.residual) for _ in range(n_layers-1) ]) \n        self.layers.append(GatedGCNLayer(hidden_dim, out_dim, dropout, self.graph_norm, self.batch_norm, self.residual))\n        self.MLP_layer = MLPReadout(2*out_dim, n_classes)\n        \n    def forward(self, g, h, e, snorm_n, snorm_e):\n        \n        h = self.embedding_h(h.float())\n        if not self.edge_feat:\n            e = torch.ones_like(e).to(self.device)\n        e = self.embedding_e(e.float())\n        \n        # convnets\n        for conv in self.layers:\n            h, e = conv(g, h, e, snorm_n, snorm_e)\n        g.ndata[\'h\'] = h\n        \n        def _edge_feat(edges):\n            e = torch.cat([edges.src[\'h\'], edges.dst[\'h\']], dim=1)\n            e = self.MLP_layer(e)\n            return {\'e\': e}\n        g.apply_edges(_edge_feat)\n        \n        return g.edata[\'e\']\n    \n    def loss(self, pred, label):\n        criterion = nn.CrossEntropyLoss(weight=None)\n        loss = criterion(pred, label)\n\n        return loss\n    '"
nets/TSP_edge_classification/gcn_net.py,3,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nimport dgl\n\n""""""\n    GCN: Graph Convolutional Networks\n    Thomas N. Kipf, Max Welling, Semi-Supervised Classification with Graph Convolutional Networks (ICLR 2017)\n    http://arxiv.org/abs/1609.02907\n""""""\nfrom layers.gcn_layer import GCNLayer\nfrom layers.mlp_readout_layer import MLPReadout\n\nclass GCNNet(nn.Module):\n    def __init__(self, net_params):\n        super().__init__()\n        in_dim = net_params[\'in_dim\']\n        hidden_dim = net_params[\'hidden_dim\']\n        out_dim = net_params[\'out_dim\']\n        n_classes = net_params[\'n_classes\']\n        in_feat_dropout = net_params[\'in_feat_dropout\']\n        dropout = net_params[\'dropout\']\n        n_layers = net_params[\'L\']\n        self.readout = net_params[\'readout\']\n        self.graph_norm = net_params[\'graph_norm\']\n        self.batch_norm = net_params[\'batch_norm\']\n        self.residual = net_params[\'residual\']\n        self.n_classes = n_classes\n        self.device = net_params[\'device\']\n        \n        self.embedding_h = nn.Linear(in_dim, hidden_dim)\n        self.in_feat_dropout = nn.Dropout(in_feat_dropout)\n        \n        self.layers = nn.ModuleList([GCNLayer(hidden_dim, hidden_dim, F.relu, dropout,\n                                              self.graph_norm, self.batch_norm, self.residual) for _ in range(n_layers-1)])\n        self.layers.append(GCNLayer(hidden_dim, out_dim, F.relu, dropout, self.graph_norm, self.batch_norm, self.residual))\n        self.MLP_layer = MLPReadout(2*out_dim, n_classes)        \n\n    def forward(self, g, h, e, snorm_n, snorm_e):\n        h = self.embedding_h(h.float())\n        h = self.in_feat_dropout(h)\n        for conv in self.layers:\n            h = conv(g, h, snorm_n)\n        g.ndata[\'h\'] = h\n        \n        def _edge_feat(edges):\n            e = torch.cat([edges.src[\'h\'], edges.dst[\'h\']], dim=1)\n            e = self.MLP_layer(e)\n            return {\'e\': e}\n        g.apply_edges(_edge_feat)\n        \n        return g.edata[\'e\']\n    \n    def loss(self, pred, label):\n        criterion = nn.CrossEntropyLoss(weight=None)\n        loss = criterion(pred, label)\n\n        return loss'"
nets/TSP_edge_classification/gin_net.py,6,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nimport dgl\nfrom dgl.nn.pytorch.glob import SumPooling, AvgPooling, MaxPooling\n\n""""""\n    GIN: Graph Isomorphism Networks\n    HOW POWERFUL ARE GRAPH NEURAL NETWORKS? (Keyulu Xu, Weihua Hu, Jure Leskovec and Stefanie Jegelka, ICLR 2019)\n    https://arxiv.org/pdf/1810.00826.pdf\n""""""\n\nfrom layers.gin_layer import GINLayer, ApplyNodeFunc, MLP\n\nclass GINNet(nn.Module):\n    \n    def __init__(self, net_params):\n        super().__init__()\n        in_dim = net_params[\'in_dim\']\n        hidden_dim = net_params[\'hidden_dim\']\n        n_classes = net_params[\'n_classes\']\n        dropout = net_params[\'dropout\']\n        self.n_layers = net_params[\'L\']\n        n_mlp_layers = net_params[\'n_mlp_GIN\']               # GIN\n        learn_eps = net_params[\'learn_eps_GIN\']              # GIN\n        neighbor_aggr_type = net_params[\'neighbor_aggr_GIN\'] # GIN\n        readout = net_params[\'readout\']                      # this is graph_pooling_type\n        graph_norm = net_params[\'graph_norm\']      \n        batch_norm = net_params[\'batch_norm\']\n        residual = net_params[\'residual\']\n        self.n_classes = n_classes\n        self.device = net_params[\'device\']\n        \n        # List of MLPs\n        self.ginlayers = torch.nn.ModuleList()\n        \n        self.embedding_h = nn.Linear(in_dim, hidden_dim)\n        \n        for layer in range(self.n_layers):\n            mlp = MLP(n_mlp_layers, hidden_dim, hidden_dim, hidden_dim)\n            \n            self.ginlayers.append(GINLayer(ApplyNodeFunc(mlp), neighbor_aggr_type,\n                                           dropout, graph_norm, batch_norm, residual, 0, learn_eps))\n\n        # Non-linear function for output of each layer\n        # which maps the output of different layers into a prediction score\n        self.prediction = torch.nn.ModuleList()\n\n        for layer in range(self.n_layers + 1):\n            self.prediction.append(\n                nn.Sequential(\n                    nn.Linear(2*hidden_dim, hidden_dim),\n                    nn.ReLU(),\n                    nn.Linear(hidden_dim, n_classes)\n                )\n            )\n        \n    def forward(self, g, h, e, snorm_n, snorm_e):\n        \n        def _edge_feat(edges):\n            e = torch.cat([edges.src[\'h\'], edges.dst[\'h\']], dim=1)\n            return {\'e\': e}\n        \n        h = self.embedding_h(h.float())\n        g.ndata[\'h\'] = h\n        g.apply_edges(_edge_feat)\n        \n        # list of hidden representation at each layer (including input)\n        hidden_rep = [g.edata[\'e\']]\n        \n        for i in range(self.n_layers):\n            h = self.ginlayers[i](g, h, snorm_n)\n            g.ndata[\'h\'] = h\n            g.apply_edges(_edge_feat)\n            hidden_rep.append(g.edata[\'e\'])\n\n        score_over_layer = 0\n        for i, e in enumerate(hidden_rep):\n            score_over_layer += self.prediction[i](e)\n        \n        return score_over_layer\n        \n    def loss(self, pred, label):\n        criterion = nn.CrossEntropyLoss(weight=None)\n        loss = criterion(pred, label)\n\n        return loss'"
nets/TSP_edge_classification/graphsage_net.py,3,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nimport dgl\n\n""""""\n    GraphSAGE: \n    William L. Hamilton, Rex Ying, Jure Leskovec, Inductive Representation Learning on Large Graphs (NeurIPS 2017)\n    https://cs.stanford.edu/people/jure/pubs/graphsage-nips17.pdf\n""""""\n\nfrom layers.graphsage_layer import GraphSageLayer\nfrom layers.mlp_readout_layer import MLPReadout\n\nclass GraphSageNet(nn.Module):\n    """"""\n    Grahpsage network with multiple GraphSageLayer layers\n    """"""\n    def __init__(self, net_params):\n        super().__init__()\n        in_dim = net_params[\'in_dim\']\n        hidden_dim = net_params[\'hidden_dim\']\n        out_dim = net_params[\'out_dim\']\n        n_classes = net_params[\'n_classes\']\n        in_feat_dropout = net_params[\'in_feat_dropout\']\n        dropout = net_params[\'dropout\']\n        aggregator_type = net_params[\'sage_aggregator\']\n        n_layers = net_params[\'L\']\n        graph_norm = net_params[\'graph_norm\']      \n        batch_norm = net_params[\'batch_norm\']\n        residual = net_params[\'residual\']\n        self.readout = net_params[\'readout\']\n        self.n_classes = n_classes\n        self.device = net_params[\'device\']\n        \n        self.embedding_h = nn.Linear(in_dim, hidden_dim)\n        self.in_feat_dropout = nn.Dropout(in_feat_dropout)\n        \n        self.layers = nn.ModuleList([GraphSageLayer(hidden_dim, hidden_dim, F.relu,\n                                              dropout, aggregator_type, graph_norm, batch_norm, residual) for _ in range(n_layers-1)])\n        self.layers.append(GraphSageLayer(hidden_dim, out_dim, F.relu, dropout, aggregator_type, graph_norm, batch_norm, residual))\n        self.MLP_layer = MLPReadout(2*out_dim, n_classes)\n        \n    def forward(self, g, h, e, snorm_n, snorm_e):\n        h = self.embedding_h(h.float())\n        h = self.in_feat_dropout(h)\n        for conv in self.layers:\n            h = conv(g, h, snorm_n)\n        g.ndata[\'h\'] = h\n        \n        def _edge_feat(edges):\n            e = torch.cat([edges.src[\'h\'], edges.dst[\'h\']], dim=1)\n            e = self.MLP_layer(e)\n            return {\'e\': e}\n        g.apply_edges(_edge_feat)\n        \n        return g.edata[\'e\']\n    \n    def loss(self, pred, label):\n        criterion = nn.CrossEntropyLoss(weight=None)\n        loss = criterion(pred, label)\n\n        return loss'"
nets/TSP_edge_classification/load_net.py,0,"b'""""""\n    Utility file to select GraphNN model as\n    selected by the user\n""""""\n\nfrom nets.TSP_edge_classification.gated_gcn_net import GatedGCNNet\nfrom nets.TSP_edge_classification.gcn_net import GCNNet\nfrom nets.TSP_edge_classification.gat_net import GATNet\nfrom nets.TSP_edge_classification.graphsage_net import GraphSageNet\nfrom nets.TSP_edge_classification.gin_net import GINNet\nfrom nets.TSP_edge_classification.mo_net import MoNet as MoNet_\nfrom nets.TSP_edge_classification.mlp_net import MLPNet\n\n\ndef GatedGCN(net_params):\n    return GatedGCNNet(net_params)\n\ndef GCN(net_params):\n    return GCNNet(net_params)\n\ndef GAT(net_params):\n    return GATNet(net_params)\n\ndef GraphSage(net_params):\n    return GraphSageNet(net_params)\n\ndef GIN(net_params):\n    return GINNet(net_params)\n\ndef MoNet(net_params):\n    return MoNet_(net_params)\n\ndef MLP(net_params):\n    return MLPNet(net_params)\n\ndef gnn_model(MODEL_NAME, net_params):\n    models = {\n        \'GatedGCN\': GatedGCN,\n        \'GCN\': GCN,\n        \'GAT\': GAT,\n        \'GraphSage\': GraphSage,\n        \'GIN\': GIN,\n        \'MoNet\': MoNet,\n        \'MLP\': MLP\n    }\n        \n    return models[MODEL_NAME](net_params)'"
nets/TSP_edge_classification/mlp_net.py,4,"b""import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nimport dgl\n\nfrom layers.mlp_readout_layer import MLPReadout\n\nclass MLPNet(nn.Module):\n    def __init__(self, net_params):\n        super().__init__()\n        in_dim = net_params['in_dim']\n        hidden_dim = net_params['hidden_dim']\n        n_classes = net_params['n_classes']\n        in_feat_dropout = net_params['in_feat_dropout']\n        dropout = net_params['dropout']\n        n_layers = net_params['L']\n        self.gated = net_params['gated']\n        self.n_classes = n_classes\n        self.device = net_params['device']\n        \n        self.in_feat_dropout = nn.Dropout(in_feat_dropout)\n        \n        feat_mlp_modules = [\n            nn.Linear(in_dim, hidden_dim, bias=True),\n            nn.ReLU(),\n            nn.Dropout(dropout),\n        ]\n        for _ in range(n_layers-1):\n            feat_mlp_modules.append(nn.Linear(hidden_dim, hidden_dim, bias=True))\n            feat_mlp_modules.append(nn.ReLU())\n            feat_mlp_modules.append(nn.Dropout(dropout))\n        self.feat_mlp = nn.Sequential(*feat_mlp_modules)\n        \n        if self.gated:\n            self.gates = nn.Linear(hidden_dim, hidden_dim, bias=True)\n        \n        self.readout_mlp = MLPReadout(2*hidden_dim, n_classes)\n\n    def forward(self, g, h, e, snorm_n, snorm_e):\n        h = self.in_feat_dropout(h)\n        h = self.feat_mlp(h)\n        if self.gated:\n            h = torch.sigmoid(self.gates(h)) * h\n        g.ndata['h'] = h\n        \n        def _edge_feat(edges):\n            e = torch.cat([edges.src['h'], edges.dst['h']], dim=1)\n            e = self.readout_mlp(e)\n            return {'e': e}\n        g.apply_edges(_edge_feat)\n        \n        return g.edata['e']\n        \n    def loss(self, pred, label):\n        criterion = nn.CrossEntropyLoss(weight=None)\n        loss = criterion(pred, label)\n\n        return loss\n"""
nets/TSP_edge_classification/mo_net.py,4,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nimport dgl\n\nimport numpy as np\n\n""""""\n    GMM: Gaussian Mixture Model Convolution layer\n    Geometric Deep Learning on Graphs and Manifolds using Mixture Model CNNs (Federico Monti et al., CVPR 2017)\n    https://arxiv.org/pdf/1611.08402.pdf\n""""""\n\nfrom layers.gmm_layer import GMMLayer\nfrom layers.mlp_readout_layer import MLPReadout\n\nclass MoNet(nn.Module):\n    def __init__(self, net_params):\n        super().__init__()\n        \n        in_dim = net_params[\'in_dim\']\n        hidden_dim = net_params[\'hidden_dim\']\n        out_dim = net_params[\'out_dim\']\n        kernel = net_params[\'kernel\']                       # for MoNet\n        dim = net_params[\'pseudo_dim_MoNet\']                # for MoNet\n        n_classes = net_params[\'n_classes\']\n        dropout = net_params[\'dropout\']\n        n_layers = net_params[\'L\']\n        self.readout = net_params[\'readout\']                      \n        graph_norm = net_params[\'graph_norm\']      \n        batch_norm = net_params[\'batch_norm\']\n        residual = net_params[\'residual\']  \n        self.device = net_params[\'device\']\n        self.n_classes = n_classes\n        \n        aggr_type = ""sum""                                    # default for MoNet\n        \n        self.embedding_h = nn.Linear(in_dim, hidden_dim)\n        \n        self.layers = nn.ModuleList()\n        self.pseudo_proj = nn.ModuleList()\n\n        # Hidden layer\n        for _ in range(n_layers-1):\n            self.layers.append(GMMLayer(hidden_dim, hidden_dim, dim, kernel, aggr_type,\n                                        dropout, graph_norm, batch_norm, residual))\n            self.pseudo_proj.append(nn.Sequential(nn.Linear(2, dim), nn.Tanh()))\n            \n        # Output layer\n        self.layers.append(GMMLayer(hidden_dim, out_dim, dim, kernel, aggr_type,\n                                    dropout, graph_norm, batch_norm, residual))\n        self.pseudo_proj.append(nn.Sequential(nn.Linear(2, dim), nn.Tanh()))\n        \n        self.MLP_layer = MLPReadout(2*out_dim, n_classes)\n\n    def forward(self, g, h, e, snorm_n, snorm_e):\n        h = self.embedding_h(h.float())\n        \n        # computing the \'pseudo\' named tensor which depends on node degrees\n        us, vs = g.edges()\n        # to avoid zero division in case in_degree is 0, we add constant \'1\' in all node degrees denoting self-loop\n        pseudo = [ [1/np.sqrt(g.in_degree(us[i])+1), 1/np.sqrt(g.in_degree(vs[i])+1)] for i in range(g.number_of_edges()) ]\n        pseudo = torch.Tensor(pseudo).to(self.device)\n        \n        for i in range(len(self.layers)):\n            h = self.layers[i](g, h, self.pseudo_proj[i](pseudo), snorm_n)\n        g.ndata[\'h\'] = h\n        \n        def _edge_feat(edges):\n            e = torch.cat([edges.src[\'h\'], edges.dst[\'h\']], dim=1)\n            e = self.MLP_layer(e)\n            return {\'e\': e}\n        g.apply_edges(_edge_feat)\n        \n        return g.edata[\'e\']\n    \n    def loss(self, pred, label):\n        criterion = nn.CrossEntropyLoss(weight=None)\n        loss = criterion(pred, label)\n\n        return loss'"
nets/TUs_graph_classification/diffpool_net.py,15,"b'import torch\nimport torch.nn as nn\nfrom torch.nn import init\nimport torch.nn.functional as F\n\nimport time\nimport numpy as np\nfrom scipy.linalg import block_diag\n\nimport dgl\n\n""""""\n    <Diffpool Fuse with GNN layers and pooling layers>\n    \n    DIFFPOOL:\n    Z. Ying, J. You, C. Morris, X. Ren, W. Hamilton, and J. Leskovec, \n    Hierarchical graph representation learning with differentiable pooling (NeurIPS 2018)\n    https://arxiv.org/pdf/1806.08804.pdf\n    \n    ! code started from dgl diffpool examples dir\n""""""\n\nfrom layers.graphsage_layer import GraphSageLayer   # this is GraphSageLayer, DiffPoolBatchedGraphLayer\nfrom layers.diffpool_layer import DiffPoolLayer   # this is GraphSageLayer, DiffPoolBatchedGraphLayer\n# from .graphsage_net import GraphSageNet   # this is GraphSage\n# replace BatchedDiffPool with DenseDiffPool and BatchedGraphSAGE with DenseGraphSage\nfrom layers.tensorized.dense_graphsage_layer import DenseGraphSage\nfrom layers.tensorized.dense_diffpool_layer import DenseDiffPool\n\n\nclass DiffPoolNet(nn.Module):\n    """"""\n    DiffPool Fuse with GNN layers and pooling layers in sequence\n    """"""\n\n    def __init__(self, net_params):\n        \n        super().__init__()\n        input_dim = net_params[\'in_dim\']\n        hidden_dim = net_params[\'hidden_dim\']\n        embedding_dim = net_params[\'embedding_dim\']\n        label_dim = net_params[\'n_classes\']\n        activation = F.relu\n        n_layers = net_params[\'L\'] # this is the gnn_per_block param\n        dropout = net_params[\'dropout\']\n        self.graph_norm = net_params[\'graph_norm\']\n        self.batch_norm = net_params[\'batch_norm\']\n        self.residual = net_params[\'residual\']\n        aggregator_type = net_params[\'sage_aggregator\']\n        pool_ratio = net_params[\'pool_ratio\']\n\n        self.device = net_params[\'device\']\n        self.link_pred = net_params[\'linkpred\']\n        self.concat = net_params[\'cat\']\n        self.n_pooling = net_params[\'num_pool\']\n        self.batch_size = net_params[\'batch_size\']\n        self.link_pred_loss = []\n        self.entropy_loss = []\n\n        self.embedding_h = nn.Linear(input_dim, hidden_dim)\n        \n        # list of GNN modules before the first diffpool operation\n        self.gc_before_pool = nn.ModuleList()\n\n        self.assign_dim = net_params[\'assign_dim\']\n        # self.bn = True\n        self.num_aggs = 1\n\n        # constructing layers\n        # layers before diffpool\n        assert n_layers >= 3, ""n_layers too few""\n        self.gc_before_pool.append(GraphSageLayer(hidden_dim, hidden_dim, activation,\n                                                  dropout, aggregator_type, self.graph_norm, self.batch_norm, self.residual))\n        \n        for _ in range(n_layers - 2):\n            self.gc_before_pool.append(GraphSageLayer(hidden_dim, hidden_dim, activation,\n                                                      dropout, aggregator_type, self.graph_norm, self.batch_norm, self.residual))\n        \n        self.gc_before_pool.append(GraphSageLayer(hidden_dim, embedding_dim, None, dropout, aggregator_type, self.graph_norm, self.batch_norm, self.residual))\n\n        \n        assign_dims = []\n        assign_dims.append(self.assign_dim)\n        if self.concat:\n            # diffpool layer receive pool_emedding_dim node feature tensor\n            # and return pool_embedding_dim node embedding\n            pool_embedding_dim = hidden_dim * (n_layers - 1) + embedding_dim\n        else:\n\n            pool_embedding_dim = embedding_dim\n\n        self.first_diffpool_layer = DiffPoolLayer(pool_embedding_dim, self.assign_dim, hidden_dim,\n                                                  activation, dropout, aggregator_type, self.graph_norm, self.batch_norm, self.link_pred)\n        gc_after_per_pool = nn.ModuleList()\n\n        # list of list of GNN modules, each list after one diffpool operation\n        self.gc_after_pool = nn.ModuleList()\n        \n        for _ in range(n_layers - 1):\n            gc_after_per_pool.append(DenseGraphSage(hidden_dim, hidden_dim, self.residual))\n        gc_after_per_pool.append(DenseGraphSage(hidden_dim, embedding_dim, self.residual))\n        self.gc_after_pool.append(gc_after_per_pool)\n\n        self.assign_dim = int(self.assign_dim * pool_ratio)\n        \n        self.diffpool_layers = nn.ModuleList()\n        # each pooling module\n        for _ in range(self.n_pooling - 1):\n            self.diffpool_layers.append(DenseDiffPool(pool_embedding_dim, self.assign_dim, hidden_dim, self.link_pred))\n            \n            gc_after_per_pool = nn.ModuleList()\n            \n            for _ in range(n_layers - 1):\n                gc_after_per_pool.append(DenseGraphSage(hidden_dim, hidden_dim, self.residual))\n            gc_after_per_pool.append(DenseGraphSage(hidden_dim, embedding_dim, self.residual))\n            self.gc_after_pool.append(gc_after_per_pool)\n            \n            assign_dims.append(self.assign_dim)\n            self.assign_dim = int(self.assign_dim * pool_ratio)\n\n        # predicting layer\n        if self.concat:\n            self.pred_input_dim = pool_embedding_dim * \\\n                self.num_aggs * (n_pooling + 1)\n        else:\n            self.pred_input_dim = embedding_dim * self.num_aggs\n        self.pred_layer = nn.Linear(self.pred_input_dim, label_dim)\n\n        # weight initialization\n        for m in self.modules():\n            if isinstance(m, nn.Linear):\n                m.weight.data = init.xavier_uniform_(m.weight.data,\n                                                     gain=nn.init.calculate_gain(\'relu\'))\n                if m.bias is not None:\n                    m.bias.data = init.constant_(m.bias.data, 0.0)\n\n    def gcn_forward(self, g, h, snorm_n, gc_layers, cat=False):\n        """"""\n        Return gc_layer embedding cat.\n        """"""\n        block_readout = []\n        for gc_layer in gc_layers[:-1]:\n            h = gc_layer(g, h, snorm_n)\n            block_readout.append(h)\n        h = gc_layers[-1](g, h, snorm_n)\n        block_readout.append(h)\n        if cat:\n            block = torch.cat(block_readout, dim=1)  # N x F, F = F1 + F2 + ...\n        else:\n            block = h\n        return block\n\n    def gcn_forward_tensorized(self, h, adj, gc_layers, cat=False):\n        block_readout = []\n        for gc_layer in gc_layers:\n            h = gc_layer(h, adj)\n            block_readout.append(h)\n        if cat:\n            block = torch.cat(block_readout, dim=2)  # N x F, F = F1 + F2 + ...\n        else:\n            block = h\n        return block\n\n    def forward(self, g, h, e, snorm_n, snorm_e):\n        self.link_pred_loss = []\n        self.entropy_loss = []\n        \n        # node feature for assignment matrix computation is the same as the\n        # original node feature\n        h = self.embedding_h(h)\n        h_a = h\n\n        out_all = []\n\n        # we use GCN blocks to get an embedding first\n        g_embedding = self.gcn_forward(g, h, snorm_n, self.gc_before_pool, self.concat)\n\n        g.ndata[\'h\'] = g_embedding\n\n        readout = dgl.sum_nodes(g, \'h\')\n        out_all.append(readout)\n        if self.num_aggs == 2:\n            readout = dgl.max_nodes(g, \'h\')\n            out_all.append(readout)\n\n        adj, h = self.first_diffpool_layer(g, g_embedding, snorm_n)\n        node_per_pool_graph = int(adj.size()[0] / self.batch_size)\n\n        h, adj = self.batch2tensor(adj, h, node_per_pool_graph)\n        h = self.gcn_forward_tensorized(h, adj, self.gc_after_pool[0], self.concat)\n        \n        readout = torch.sum(h, dim=1)\n        out_all.append(readout)\n        if self.num_aggs == 2:\n            readout, _ = torch.max(h, dim=1)\n            out_all.append(readout)\n\n        for i, diffpool_layer in enumerate(self.diffpool_layers):\n            h, adj = diffpool_layer(h, adj)\n            h = self.gcn_forward_tensorized(h, adj, self.gc_after_pool[i + 1], self.concat)\n            \n            readout = torch.sum(h, dim=1)\n            out_all.append(readout)\n            \n            if self.num_aggs == 2:\n                readout, _ = torch.max(h, dim=1)\n                out_all.append(readout)\n        \n        if self.concat or self.num_aggs > 1:\n            final_readout = torch.cat(out_all, dim=1)\n        else:\n            final_readout = readout\n        ypred = self.pred_layer(final_readout)\n        return ypred\n\n    def batch2tensor(self, batch_adj, batch_feat, node_per_pool_graph):\n        """"""\n        transform a batched graph to batched adjacency tensor and node feature tensor\n        """"""\n        batch_size = int(batch_adj.size()[0] / node_per_pool_graph)\n        adj_list = []\n        feat_list = []\n\n        for i in range(batch_size):\n            start = i * node_per_pool_graph\n            end = (i + 1) * node_per_pool_graph\n\n            # 1/sqrt(V) normalization\n            snorm_n = torch.FloatTensor(node_per_pool_graph, 1).fill_(1./float(node_per_pool_graph)).sqrt().to(self.device)\n\n            adj_list.append(batch_adj[start:end, start:end])\n            feat_list.append((batch_feat[start:end, :])*snorm_n)\n        adj_list = list(map(lambda x: torch.unsqueeze(x, 0), adj_list))\n        feat_list = list(map(lambda x: torch.unsqueeze(x, 0), feat_list))\n        adj = torch.cat(adj_list, dim=0)\n        feat = torch.cat(feat_list, dim=0)\n\n        return feat, adj\n    \n    def loss(self, pred, label):\n        \'\'\'\n        loss function\n        \'\'\'\n        #softmax + CE\n        criterion = nn.CrossEntropyLoss()\n        loss = criterion(pred, label)\n        for diffpool_layer in self.diffpool_layers:\n            for key, value in diffpool_layer.loss_log.items():\n                loss += value\n        return loss\n    \n    '"
nets/TUs_graph_classification/gat_net.py,2,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nimport dgl\n\n""""""\n    GAT: Graph Attention Network\n    Graph Attention Networks (Veli\xc4\x8dkovi\xc4\x87 et al., ICLR 2018)\n    https://arxiv.org/abs/1710.10903\n""""""\nfrom layers.gat_layer import GATLayer\nfrom layers.mlp_readout_layer import MLPReadout\n\nclass GATNet(nn.Module):\n    def __init__(self, net_params):\n        super().__init__()\n        in_dim = net_params[\'in_dim\']\n        hidden_dim = net_params[\'hidden_dim\']\n        out_dim = net_params[\'out_dim\']\n        n_classes = net_params[\'n_classes\']\n        num_heads = net_params[\'n_heads\']\n        in_feat_dropout = net_params[\'in_feat_dropout\']\n        dropout = net_params[\'dropout\']\n        n_layers = net_params[\'L\']\n        self.readout = net_params[\'readout\']\n        self.graph_norm = net_params[\'graph_norm\']\n        self.batch_norm = net_params[\'batch_norm\']\n        self.residual = net_params[\'residual\']\n        \n        self.dropout = dropout\n        \n        self.embedding_h = nn.Linear(in_dim, hidden_dim * num_heads)\n        self.in_feat_dropout = nn.Dropout(in_feat_dropout)\n        \n        self.layers = nn.ModuleList([GATLayer(hidden_dim * num_heads, hidden_dim, num_heads,\n                                              dropout, self.graph_norm, self.batch_norm, self.residual) for _ in range(n_layers-1)])\n        self.layers.append(GATLayer(hidden_dim * num_heads, out_dim, 1, dropout, self.graph_norm, self.batch_norm, self.residual))\n        self.MLP_layer = MLPReadout(out_dim, n_classes)\n        \n    def forward(self, g, h, e, snorm_n, snorm_e):\n        h = self.embedding_h(h)\n        h = self.in_feat_dropout(h)\n        for conv in self.layers:\n            h = conv(g, h, snorm_n)\n        g.ndata[\'h\'] = h\n        \n        if self.readout == ""sum"":\n            hg = dgl.sum_nodes(g, \'h\')\n        elif self.readout == ""max"":\n            hg = dgl.max_nodes(g, \'h\')\n        elif self.readout == ""mean"":\n            hg = dgl.mean_nodes(g, \'h\')\n        else:\n            hg = dgl.mean_nodes(g, \'h\')  # default readout is mean nodes\n            \n        return self.MLP_layer(hg)\n    \n    def loss(self, pred, label):\n        criterion = nn.CrossEntropyLoss()\n        loss = criterion(pred, label)\n        return loss\n    \n\n    '"
nets/TUs_graph_classification/gated_gcn_net.py,2,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nimport dgl\n\n""""""\n    ResGatedGCN: Residual Gated Graph ConvNets\n    An Experimental Study of Neural Networks for Variable Graphs (Xavier Bresson and Thomas Laurent, ICLR 2018)\n    https://arxiv.org/pdf/1711.07553v2.pdf\n""""""\nfrom layers.gated_gcn_layer import GatedGCNLayer\nfrom layers.mlp_readout_layer import MLPReadout\n\nclass GatedGCNNet(nn.Module):\n    \n    def __init__(self, net_params):\n        super().__init__()\n        in_dim = net_params[\'in_dim\']\n        hidden_dim = net_params[\'hidden_dim\']\n        out_dim = net_params[\'out_dim\']\n        n_classes = net_params[\'n_classes\']\n        dropout = net_params[\'dropout\']\n        n_layers = net_params[\'L\']\n        self.readout = net_params[\'readout\']\n        self.graph_norm = net_params[\'graph_norm\']\n        self.batch_norm = net_params[\'batch_norm\']\n        self.residual = net_params[\'residual\']\n        \n        self.embedding_h = nn.Linear(in_dim, hidden_dim)\n        self.embedding_e = nn.Linear(in_dim, hidden_dim)\n        self.layers = nn.ModuleList([ GatedGCNLayer(hidden_dim, hidden_dim, dropout, self.graph_norm,\n                                                       self.batch_norm, self.residual) for _ in range(n_layers-1) ]) \n        self.layers.append(GatedGCNLayer(hidden_dim, out_dim, dropout, self.graph_norm, self.batch_norm, self.residual))\n        self.MLP_layer = MLPReadout(out_dim, n_classes)\n        \n    def forward(self, g, h, e, snorm_n, snorm_e):\n        h = self.embedding_h(h)\n        e = self.embedding_e(e)\n        \n        # convnets\n        for conv in self.layers:\n            h, e = conv(g, h, e, snorm_n, snorm_e)\n        g.ndata[\'h\'] = h\n        \n        if self.readout == ""sum"":\n            hg = dgl.sum_nodes(g, \'h\')\n        elif self.readout == ""max"":\n            hg = dgl.max_nodes(g, \'h\')\n        elif self.readout == ""mean"":\n            hg = dgl.mean_nodes(g, \'h\')\n        else:\n            hg = dgl.mean_nodes(g, \'h\')  # default readout is mean nodes\n            \n        return self.MLP_layer(hg)\n        \n    def loss(self, pred, label):\n        criterion = nn.CrossEntropyLoss()\n        loss = criterion(pred, label)\n        return loss\n    \n\n        \n'"
nets/TUs_graph_classification/gcn_net.py,2,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nimport dgl\n\n""""""\n    GCN: Graph Convolutional Networks\n    Thomas N. Kipf, Max Welling, Semi-Supervised Classification with Graph Convolutional Networks (ICLR 2017)\n    http://arxiv.org/abs/1609.02907\n""""""\nfrom layers.gcn_layer import GCNLayer\nfrom layers.mlp_readout_layer import MLPReadout\n\nclass GCNNet(nn.Module):\n    def __init__(self, net_params):\n        super().__init__()\n        in_dim = net_params[\'in_dim\']\n        hidden_dim = net_params[\'hidden_dim\']\n        out_dim = net_params[\'out_dim\']\n        n_classes = net_params[\'n_classes\']\n        in_feat_dropout = net_params[\'in_feat_dropout\']\n        dropout = net_params[\'dropout\']\n        n_layers = net_params[\'L\']\n        self.readout = net_params[\'readout\']\n        self.graph_norm = net_params[\'graph_norm\']\n        self.batch_norm = net_params[\'batch_norm\']\n        self.residual = net_params[\'residual\']\n        \n        self.embedding_h = nn.Linear(in_dim, hidden_dim)\n        self.in_feat_dropout = nn.Dropout(in_feat_dropout)\n        \n        self.layers = nn.ModuleList([GCNLayer(hidden_dim, hidden_dim, F.relu, dropout,\n                                              self.graph_norm, self.batch_norm, self.residual) for _ in range(n_layers-1)])\n        self.layers.append(GCNLayer(hidden_dim, out_dim, F.relu, dropout, self.graph_norm, self.batch_norm, self.residual))\n        self.MLP_layer = MLPReadout(out_dim, n_classes)        \n\n    def forward(self, g, h, e, snorm_n, snorm_e):\n        h = self.embedding_h(h)\n        h = self.in_feat_dropout(h)\n        for conv in self.layers:\n            h = conv(g, h, snorm_n)\n        g.ndata[\'h\'] = h\n        \n        if self.readout == ""sum"":\n            hg = dgl.sum_nodes(g, \'h\')\n        elif self.readout == ""max"":\n            hg = dgl.max_nodes(g, \'h\')\n        elif self.readout == ""mean"":\n            hg = dgl.mean_nodes(g, \'h\')\n        else:\n            hg = dgl.mean_nodes(g, \'h\')  # default readout is mean nodes\n            \n        return self.MLP_layer(hg)\n    \n    def loss(self, pred, label):\n        criterion = nn.CrossEntropyLoss()\n        loss = criterion(pred, label)\n        return loss\n        \n    '"
nets/TUs_graph_classification/gin_net.py,5,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nimport dgl\nfrom dgl.nn.pytorch.glob import SumPooling, AvgPooling, MaxPooling\n\n""""""\n    GIN: Graph Isomorphism Networks\n    HOW POWERFUL ARE GRAPH NEURAL NETWORKS? (Keyulu Xu, Weihua Hu, Jure Leskovec and Stefanie Jegelka, ICLR 2019)\n    https://arxiv.org/pdf/1810.00826.pdf\n""""""\n\nfrom layers.gin_layer import GINLayer, ApplyNodeFunc, MLP\n\nclass GINNet(nn.Module):\n    \n    def __init__(self, net_params):\n        super().__init__()\n        in_dim = net_params[\'in_dim\']\n        hidden_dim = net_params[\'hidden_dim\']\n        n_classes = net_params[\'n_classes\']\n        dropout = net_params[\'dropout\']\n        self.n_layers = net_params[\'L\']\n        n_mlp_layers = net_params[\'n_mlp_GIN\']               # GIN\n        learn_eps = net_params[\'learn_eps_GIN\']              # GIN\n        neighbor_aggr_type = net_params[\'neighbor_aggr_GIN\'] # GIN\n        readout = net_params[\'readout\']                      # this is graph_pooling_type\n        graph_norm = net_params[\'graph_norm\']      \n        batch_norm = net_params[\'batch_norm\']\n        residual = net_params[\'residual\']     \n        \n        # List of MLPs\n        self.ginlayers = torch.nn.ModuleList()\n        \n        self.embedding_h = nn.Linear(in_dim, hidden_dim)\n        \n        for layer in range(self.n_layers):\n            mlp = MLP(n_mlp_layers, hidden_dim, hidden_dim, hidden_dim)\n            \n            self.ginlayers.append(GINLayer(ApplyNodeFunc(mlp), neighbor_aggr_type,\n                                           dropout, graph_norm, batch_norm, residual, 0, learn_eps))\n\n        # Linear function for graph poolings (readout) of output of each layer\n        # which maps the output of different layers into a prediction score\n        self.linears_prediction = torch.nn.ModuleList()\n\n        for layer in range(self.n_layers+1):\n            self.linears_prediction.append(nn.Linear(hidden_dim, n_classes))\n        \n        if readout == \'sum\':\n            self.pool = SumPooling()\n        elif readout == \'mean\':\n            self.pool = AvgPooling()\n        elif readout == \'max\':\n            self.pool = MaxPooling()\n        else:\n            raise NotImplementedError\n        \n    def forward(self, g, h, e, snorm_n, snorm_e):\n        \n        h = self.embedding_h(h)\n        \n        # list of hidden representation at each layer (including input)\n        hidden_rep = [h]\n\n        for i in range(self.n_layers):\n            h = self.ginlayers[i](g, h, snorm_n)\n            hidden_rep.append(h)\n\n        score_over_layer = 0\n\n        # perform pooling over all nodes in each graph in every layer\n        for i, h in enumerate(hidden_rep):\n            pooled_h = self.pool(g, h)\n            score_over_layer += self.linears_prediction[i](pooled_h)\n\n        return score_over_layer\n        \n    def loss(self, pred, label):\n        criterion = nn.CrossEntropyLoss()\n        loss = criterion(pred, label)\n        return loss'"
nets/TUs_graph_classification/graphsage_net.py,2,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nimport dgl\n\n""""""\n    GraphSAGE: \n    William L. Hamilton, Rex Ying, Jure Leskovec, Inductive Representation Learning on Large Graphs (NeurIPS 2017)\n    https://cs.stanford.edu/people/jure/pubs/graphsage-nips17.pdf\n""""""\n\nfrom layers.graphsage_layer import GraphSageLayer\nfrom layers.mlp_readout_layer import MLPReadout\n\nclass GraphSageNet(nn.Module):\n    """"""\n    Grahpsage network with multiple GraphSageLayer layers\n    """"""\n    def __init__(self, net_params):\n        super().__init__()\n        in_dim = net_params[\'in_dim\']\n        hidden_dim = net_params[\'hidden_dim\']\n        out_dim = net_params[\'out_dim\']\n        n_classes = net_params[\'n_classes\']\n        in_feat_dropout = net_params[\'in_feat_dropout\']\n        dropout = net_params[\'dropout\']\n        aggregator_type = net_params[\'sage_aggregator\']\n        n_layers = net_params[\'L\']\n        graph_norm = net_params[\'graph_norm\']      \n        batch_norm = net_params[\'batch_norm\']\n        residual = net_params[\'residual\']\n        self.readout = net_params[\'readout\']\n        \n        self.embedding_h = nn.Linear(in_dim, hidden_dim)\n        self.in_feat_dropout = nn.Dropout(in_feat_dropout)\n        \n        self.layers = nn.ModuleList([GraphSageLayer(hidden_dim, hidden_dim, F.relu,\n                                              dropout, aggregator_type, graph_norm, batch_norm, residual) for _ in range(n_layers-1)])\n        self.layers.append(GraphSageLayer(hidden_dim, out_dim, F.relu, dropout, aggregator_type, graph_norm, batch_norm, residual))\n        self.MLP_layer = MLPReadout(out_dim, n_classes)\n        \n    def forward(self, g, h, e, snorm_n, snorm_e):\n        h = self.embedding_h(h)\n        h = self.in_feat_dropout(h)\n        for conv in self.layers:\n            h = conv(g, h, snorm_n)\n        g.ndata[\'h\'] = h\n        \n        if self.readout == ""sum"":\n            hg = dgl.sum_nodes(g, \'h\')\n        elif self.readout == ""max"":\n            hg = dgl.max_nodes(g, \'h\')\n        elif self.readout == ""mean"":\n            hg = dgl.mean_nodes(g, \'h\')\n        else:\n            hg = dgl.mean_nodes(g, \'h\')  # default readout is mean nodes\n            \n        return self.MLP_layer(hg)\n\n        \n    def loss(self, pred, label):\n        criterion = nn.CrossEntropyLoss()\n        loss = criterion(pred, label)\n        return loss\n    '"
nets/TUs_graph_classification/load_net.py,0,"b'""""""\n    Utility file to select GraphNN model as\n    selected by the user\n""""""\n\nfrom nets.TUs_graph_classification.gated_gcn_net import GatedGCNNet\nfrom nets.TUs_graph_classification.gcn_net import GCNNet\nfrom nets.TUs_graph_classification.gat_net import GATNet\nfrom nets.TUs_graph_classification.graphsage_net import GraphSageNet\nfrom nets.TUs_graph_classification.gin_net import GINNet\nfrom nets.TUs_graph_classification.mo_net import MoNet as MoNet_\nfrom nets.TUs_graph_classification.diffpool_net import DiffPoolNet\nfrom nets.TUs_graph_classification.mlp_net import MLPNet\n\n\ndef GatedGCN(net_params):\n    return GatedGCNNet(net_params)\n\ndef GCN(net_params):\n    return GCNNet(net_params)\n\ndef GAT(net_params):\n    return GATNet(net_params)\n\ndef GraphSage(net_params):\n    return GraphSageNet(net_params)\n\ndef GIN(net_params):\n    return GINNet(net_params)\n\ndef MoNet(net_params):\n    return MoNet_(net_params)\n\ndef DiffPool(net_params):\n    return DiffPoolNet(net_params)\n\ndef MLP(net_params):\n    return MLPNet(net_params)\n\ndef gnn_model(MODEL_NAME, net_params):\n    models = {\n        \'GatedGCN\': GatedGCN,\n        \'GCN\': GCN,\n        \'GAT\': GAT,\n        \'GraphSage\': GraphSage,\n        \'GIN\': GIN,\n        \'MoNet\': MoNet_,\n        \'DiffPool\': DiffPool,\n        \'MLP\': MLP\n    }\n        \n    return models[MODEL_NAME](net_params)'"
nets/TUs_graph_classification/mlp_net.py,4,"b""import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nimport dgl\n\nfrom layers.mlp_readout_layer import MLPReadout\n\nclass MLPNet(nn.Module):\n    def __init__(self, net_params):\n        super().__init__()\n        in_dim = net_params['in_dim']\n        hidden_dim = net_params['hidden_dim']\n        n_classes = net_params['n_classes']\n        in_feat_dropout = net_params['in_feat_dropout']\n        dropout = net_params['dropout']\n        n_layers = net_params['L']\n        self.gated = net_params['gated']\n        \n        self.in_feat_dropout = nn.Dropout(in_feat_dropout)\n        \n        feat_mlp_modules = [\n            nn.Linear(in_dim, hidden_dim, bias=True),\n            nn.ReLU(),\n            nn.Dropout(dropout),\n        ]\n        for _ in range(n_layers-1):\n            feat_mlp_modules.append(nn.Linear(hidden_dim, hidden_dim, bias=True))\n            feat_mlp_modules.append(nn.ReLU())\n            feat_mlp_modules.append(nn.Dropout(dropout))\n        self.feat_mlp = nn.Sequential(*feat_mlp_modules)\n        \n        if self.gated:\n            self.gates = nn.Linear(hidden_dim, hidden_dim, bias=True)\n        \n        self.readout_mlp = MLPReadout(hidden_dim, n_classes)\n\n    def forward(self, g, h, e, snorm_n, snorm_e):\n        h = self.in_feat_dropout(h)\n        h = self.feat_mlp(h)\n        if self.gated:\n            h = torch.sigmoid(self.gates(h)) * h\n            g.ndata['h'] = h       \n            hg = dgl.sum_nodes(g, 'h')\n            # hg = torch.cat(\n            #     (\n            #         dgl.sum_nodes(g, 'h'),\n            #         dgl.max_nodes(g, 'h')\n            #     ),\n            #     dim=1\n            # )\n        \n        else:\n            g.ndata['h'] = h\n            hg = dgl.mean_nodes(g, 'h')\n        \n        return self.readout_mlp(hg)\n\n        \n    def loss(self, pred, label):\n        criterion = nn.CrossEntropyLoss()\n        loss = criterion(pred, label)\n        return loss\n       """
nets/TUs_graph_classification/mo_net.py,3,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nimport dgl\n\nimport numpy as np\n\n""""""\n    GMM: Gaussian Mixture Model Convolution layer\n    Geometric Deep Learning on Graphs and Manifolds using Mixture Model CNNs (Federico Monti et al., CVPR 2017)\n    https://arxiv.org/pdf/1611.08402.pdf\n""""""\n\nfrom layers.gmm_layer import GMMLayer\nfrom layers.mlp_readout_layer import MLPReadout\n\nclass MoNet(nn.Module):\n    def __init__(self, net_params):\n        super().__init__()\n        \n        in_dim = net_params[\'in_dim\']\n        hidden_dim = net_params[\'hidden_dim\']\n        out_dim = net_params[\'out_dim\']\n        kernel = net_params[\'kernel\']                       # for MoNet\n        dim = net_params[\'pseudo_dim_MoNet\']                # for MoNet\n        n_classes = net_params[\'n_classes\']\n        dropout = net_params[\'dropout\']\n        n_layers = net_params[\'L\']\n        self.readout = net_params[\'readout\']                      \n        graph_norm = net_params[\'graph_norm\']      \n        batch_norm = net_params[\'batch_norm\']\n        residual = net_params[\'residual\']  \n        self.device = net_params[\'device\']\n        \n        aggr_type = ""sum""                                    # default for MoNet\n        \n        self.embedding_h = nn.Linear(in_dim, hidden_dim)\n        \n        self.layers = nn.ModuleList()\n        self.pseudo_proj = nn.ModuleList()\n\n        # Hidden layer\n        for _ in range(n_layers-1):\n            self.layers.append(GMMLayer(hidden_dim, hidden_dim, dim, kernel, aggr_type,\n                                        dropout, graph_norm, batch_norm, residual))\n            self.pseudo_proj.append(nn.Sequential(nn.Linear(2, dim), nn.Tanh()))\n            \n        # Output layer\n        self.layers.append(GMMLayer(hidden_dim, out_dim, dim, kernel, aggr_type,\n                                    dropout, graph_norm, batch_norm, residual))\n        self.pseudo_proj.append(nn.Sequential(nn.Linear(2, dim), nn.Tanh()))\n        \n        self.MLP_layer = MLPReadout(out_dim, n_classes)\n\n    def forward(self, g, h, e, snorm_n, snorm_e):\n        h = self.embedding_h(h)\n        \n        # computing the \'pseudo\' named tensor which depends on node degrees\n        us, vs = g.edges()\n        # to avoid zero division in case in_degree is 0, we add constant \'1\' in all node degrees denoting self-loop\n        pseudo = [ [1/np.sqrt(g.in_degree(us[i])+1), 1/np.sqrt(g.in_degree(vs[i])+1)] for i in range(g.number_of_edges()) ]\n        pseudo = torch.Tensor(pseudo).to(self.device)\n        \n        for i in range(len(self.layers)):\n            h = self.layers[i](g, h, self.pseudo_proj[i](pseudo), snorm_n)\n        g.ndata[\'h\'] = h\n            \n        if self.readout == ""sum"":\n            hg = dgl.sum_nodes(g, \'h\')\n        elif self.readout == ""max"":\n            hg = dgl.max_nodes(g, \'h\')\n        elif self.readout == ""mean"":\n            hg = dgl.mean_nodes(g, \'h\')\n        else:\n            hg = dgl.mean_nodes(g, \'h\')  # default readout is mean nodes\n\n        return self.MLP_layer(hg)\n        \n    def loss(self, pred, label):\n        criterion = nn.CrossEntropyLoss()\n        loss = criterion(pred, label)\n        return loss'"
nets/molecules_graph_regression/diffpool_net.py,15,"b'import torch\nimport torch.nn as nn\nfrom torch.nn import init\nimport torch.nn.functional as F\n\nimport time\nimport numpy as np\nfrom scipy.linalg import block_diag\n\nimport dgl\n\n""""""\n    <Diffpool Fuse with GNN layers and pooling layers>\n    \n    DIFFPOOL:\n    Z. Ying, J. You, C. Morris, X. Ren, W. Hamilton, and J. Leskovec, \n    Hierarchical graph representation learning with differentiable pooling (NeurIPS 2018)\n    https://arxiv.org/pdf/1806.08804.pdf\n    \n    ! code started from dgl diffpool examples dir\n""""""\n\nfrom layers.graphsage_layer import GraphSageLayer   # this is GraphSageLayer\nfrom layers.diffpool_layer import DiffPoolLayer   # this is DiffPoolBatchedGraphLayer\n# from .graphsage_net import GraphSageNet   # this is GraphSage\n# replace BatchedDiffPool with DenseDiffPool and BatchedGraphSAGE with DenseGraphSage\nfrom layers.tensorized.dense_graphsage_layer import DenseGraphSage\nfrom layers.tensorized.dense_diffpool_layer import DenseDiffPool\n\nclass DiffPoolNet(nn.Module):\n    """"""\n    DiffPool Fuse with GNN layers and pooling layers in sequence\n    """"""\n\n    def __init__(self, net_params):\n        \n        super().__init__()\n        num_atom_type = net_params[\'num_atom_type\']\n        num_bond_type = net_params[\'num_bond_type\']\n        hidden_dim = net_params[\'hidden_dim\']\n        embedding_dim = net_params[\'embedding_dim\']\n        activation = F.relu\n        n_layers = net_params[\'L\'] # this is the same \'gnn_per_block\' param\n        dropout = net_params[\'dropout\']\n        self.graph_norm = net_params[\'graph_norm\']\n        self.batch_norm = net_params[\'batch_norm\']\n        aggregator_type = net_params[\'sage_aggregator\']\n        pool_ratio = net_params[\'pool_ratio\']\n\n        self.device = net_params[\'device\']\n        self.link_pred = net_params[\'linkpred\']\n        self.concat = net_params[\'cat\']\n        self.residual = net_params[\'residual\']\n        self.n_pooling = net_params[\'num_pool\']\n        self.batch_size = net_params[\'batch_size\']\n        self.link_pred_loss = []\n        self.entropy_loss = []\n        \n        self.embedding_h = nn.Embedding(num_atom_type, hidden_dim)\n\n        # list of GNN modules before the first diffpool operation\n        self.gc_before_pool = nn.ModuleList()\n\n        self.assign_dim = net_params[\'assign_dim\']\n        # self.bn = True\n        self.num_aggs = 1\n\n        # constructing layers\n        # layers before diffpool\n        assert n_layers >= 3, ""n_layers too few""\n        self.gc_before_pool.append(GraphSageLayer(hidden_dim, hidden_dim, activation, dropout, aggregator_type, self.graph_norm, self.batch_norm, self.residual))\n        \n        for _ in range(n_layers - 2):\n            self.gc_before_pool.append(GraphSageLayer(hidden_dim, hidden_dim, activation, dropout, aggregator_type, self.graph_norm, self.batch_norm, self.residual))\n        \n        self.gc_before_pool.append(GraphSageLayer(hidden_dim, embedding_dim, None, dropout, aggregator_type, self.graph_norm, self.batch_norm, self.residual))\n\n        \n        assign_dims = []\n        assign_dims.append(self.assign_dim)\n        if self.concat:\n            # diffpool layer receive pool_emedding_dim node feature tensor\n            # and return pool_embedding_dim node embedding\n            pool_embedding_dim = hidden_dim * (n_layers - 1) + embedding_dim\n        else:\n\n            pool_embedding_dim = embedding_dim\n\n        self.first_diffpool_layer = DiffPoolLayer(pool_embedding_dim, self.assign_dim, hidden_dim,\n                                                  activation, dropout, aggregator_type, self.graph_norm, self.batch_norm, self.link_pred)\n        gc_after_per_pool = nn.ModuleList()\n\n        # list of list of GNN modules, each list after one diffpool operation\n        self.gc_after_pool = nn.ModuleList()\n        \n        for _ in range(n_layers - 1):\n            gc_after_per_pool.append(DenseGraphSage(hidden_dim, hidden_dim, self.residual))\n        gc_after_per_pool.append(DenseGraphSage(hidden_dim, embedding_dim, self.residual))\n        self.gc_after_pool.append(gc_after_per_pool)\n\n        self.assign_dim = int(self.assign_dim * pool_ratio)\n        \n        self.diffpool_layers = nn.ModuleList()\n        # each pooling module\n        for _ in range(self.n_pooling - 1):\n            self.diffpool_layers.append(DenseDiffPool(pool_embedding_dim, self.assign_dim, hidden_dim, self.link_pred))\n            \n            gc_after_per_pool = nn.ModuleList()\n            \n            for _ in range(n_layers - 1):\n                gc_after_per_pool.append(DenseGraphSage(hidden_dim, hidden_dim, self.residual))\n            gc_after_per_pool.append(DenseGraphSage(hidden_dim, embedding_dim, self.residual))\n            self.gc_after_pool.append(gc_after_per_pool)\n            \n            assign_dims.append(self.assign_dim)\n            self.assign_dim = int(self.assign_dim * pool_ratio)\n\n        # predicting layer\n        if self.concat:\n            self.pred_input_dim = pool_embedding_dim * \\\n                self.num_aggs * (n_pooling + 1)\n        else:\n            self.pred_input_dim = embedding_dim * self.num_aggs\n        self.pred_layer = nn.Linear(self.pred_input_dim, 1)   # 1 out dim since regression problem\n\n        # weight initialization\n        for m in self.modules():\n            if isinstance(m, nn.Linear):\n                m.weight.data = init.xavier_uniform_(m.weight.data,\n                                                     gain=nn.init.calculate_gain(\'relu\'))\n                if m.bias is not None:\n                    m.bias.data = init.constant_(m.bias.data, 0.0)\n\n    def gcn_forward(self, g, h, snorm_n, gc_layers, cat=False):\n        """"""\n        Return gc_layer embedding cat.\n        """"""\n        block_readout = []\n        for gc_layer in gc_layers[:-1]:\n            h = gc_layer(g, h, snorm_n)\n            block_readout.append(h)\n        h = gc_layers[-1](g, h, snorm_n)\n        block_readout.append(h)\n        if cat:\n            block = torch.cat(block_readout, dim=1)  # N x F, F = F1 + F2 + ...\n        else:\n            block = h\n        return block\n\n    def gcn_forward_tensorized(self, h, adj, gc_layers, cat=False):\n        block_readout = []\n        for gc_layer in gc_layers:\n            h = gc_layer(h, adj)\n            block_readout.append(h)\n        if cat:\n            block = torch.cat(block_readout, dim=2)  # N x F, F = F1 + F2 + ...\n        else:\n            block = h\n        return block\n\n    def forward(self, g, h, e, snorm_n, snorm_e):\n        self.link_pred_loss = []\n        self.entropy_loss = []\n        \n        # node feature for assignment matrix computation is the same as the\n        # original node feature\n        h = self.embedding_h(h)\n        h_a = h\n\n        out_all = []\n\n        # we use GCN blocks to get an embedding first\n        g_embedding = self.gcn_forward(g, h, snorm_n, self.gc_before_pool, self.concat)\n\n        g.ndata[\'h\'] = g_embedding\n\n        readout = dgl.sum_nodes(g, \'h\')\n        out_all.append(readout)\n        if self.num_aggs == 2:\n            readout = dgl.max_nodes(g, \'h\')\n            out_all.append(readout)\n\n        adj, h = self.first_diffpool_layer(g, g_embedding, snorm_n)\n        node_per_pool_graph = int(adj.size()[0] / self.batch_size)\n\n        h, adj = self.batch2tensor(adj, h, node_per_pool_graph)\n        h = self.gcn_forward_tensorized(h, adj, self.gc_after_pool[0], self.concat)\n        \n        readout = torch.sum(h, dim=1)\n        out_all.append(readout)\n        if self.num_aggs == 2:\n            readout, _ = torch.max(h, dim=1)\n            out_all.append(readout)\n\n        for i, diffpool_layer in enumerate(self.diffpool_layers):\n            h, adj = diffpool_layer(h, adj)\n            h = self.gcn_forward_tensorized(h, adj, self.gc_after_pool[i + 1], self.concat)\n            \n            readout = torch.sum(h, dim=1)\n            out_all.append(readout)\n            \n            if self.num_aggs == 2:\n                readout, _ = torch.max(h, dim=1)\n                out_all.append(readout)\n        \n        if self.concat or self.num_aggs > 1:\n            final_readout = torch.cat(out_all, dim=1)\n        else:\n            final_readout = readout\n        ypred = self.pred_layer(final_readout)\n        return ypred\n    \n    def batch2tensor(self, batch_adj, batch_feat, node_per_pool_graph):\n        """"""\n        transform a batched graph to batched adjacency tensor and node feature tensor\n        """"""\n        batch_size = int(batch_adj.size()[0] / node_per_pool_graph)\n        adj_list = []\n        feat_list = []\n\n        for i in range(batch_size):\n            start = i * node_per_pool_graph\n            end = (i + 1) * node_per_pool_graph\n\n            # 1/sqrt(V) normalization\n            snorm_n = torch.FloatTensor(node_per_pool_graph, 1).fill_(1./float(node_per_pool_graph)).sqrt().to(self.device)\n\n            adj_list.append(batch_adj[start:end, start:end])\n            feat_list.append((batch_feat[start:end, :])*snorm_n)\n        adj_list = list(map(lambda x: torch.unsqueeze(x, 0), adj_list))\n        feat_list = list(map(lambda x: torch.unsqueeze(x, 0), feat_list))\n        adj = torch.cat(adj_list, dim=0)\n        feat = torch.cat(feat_list, dim=0)\n\n        return feat, adj\n    \n    def loss(self, pred, label):\n        \'\'\'\n        loss function\n        \'\'\'\n        criterion = nn.L1Loss()\n        loss = criterion(pred, label)\n        for diffpool_layer in self.diffpool_layers:\n            for key, value in diffpool_layer.loss_log.items():\n                loss += value\n        return loss\n'"
nets/molecules_graph_regression/gat_net.py,2,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nimport dgl\n\n""""""\n    GAT: Graph Attention Network\n    Graph Attention Networks (Veli\xc4\x8dkovi\xc4\x87 et al., ICLR 2018)\n    https://arxiv.org/abs/1710.10903\n""""""\nfrom layers.gat_layer import GATLayer\nfrom layers.mlp_readout_layer import MLPReadout\n\nclass GATNet(nn.Module):\n    def __init__(self, net_params):\n        super().__init__()\n        num_atom_type = net_params[\'num_atom_type\']\n        num_bond_type = net_params[\'num_bond_type\']\n        hidden_dim = net_params[\'hidden_dim\']\n        num_heads = net_params[\'n_heads\']\n        out_dim = net_params[\'out_dim\']\n        in_feat_dropout = net_params[\'in_feat_dropout\']\n        dropout = net_params[\'dropout\']\n        n_layers = net_params[\'L\']\n        self.readout = net_params[\'readout\']\n        self.graph_norm = net_params[\'graph_norm\']\n        self.batch_norm = net_params[\'batch_norm\']\n        self.residual = net_params[\'residual\']\n        \n        self.dropout = dropout\n        \n        self.embedding_h = nn.Embedding(num_atom_type, hidden_dim*num_heads)\n        self.in_feat_dropout = nn.Dropout(in_feat_dropout)\n        \n        self.layers = nn.ModuleList([GATLayer(hidden_dim * num_heads, hidden_dim, num_heads,\n                                              dropout, self.graph_norm, self.batch_norm, self.residual) for _ in range(n_layers-1)])\n        self.layers.append(GATLayer(hidden_dim * num_heads, out_dim, 1,\n                                    dropout, self.graph_norm, self.batch_norm, self.residual))\n        self.MLP_layer = MLPReadout(out_dim, 1)   # 1 out dim since regression problem\n        \n    def forward(self, g, h, e, snorm_n, snorm_e):\n        h = self.embedding_h(h)\n        h = self.in_feat_dropout(h)\n        for conv in self.layers:\n            h = conv(g, h, snorm_n)\n        g.ndata[\'h\'] = h\n        \n        if self.readout == ""sum"":\n            hg = dgl.sum_nodes(g, \'h\')\n        elif self.readout == ""max"":\n            hg = dgl.max_nodes(g, \'h\')\n        elif self.readout == ""mean"":\n            hg = dgl.mean_nodes(g, \'h\')\n        else:\n            hg = dgl.mean_nodes(g, \'h\')  # default readout is mean nodes\n            \n        return self.MLP_layer(hg)\n    \n    def loss(self, scores, targets):\n        # loss = nn.MSELoss()(scores,targets)\n        loss = nn.L1Loss()(scores, targets)\n        return loss\n       '"
nets/molecules_graph_regression/gated_gcn_net.py,3,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nimport dgl\n\n""""""\n    ResGatedGCN: Residual Gated Graph ConvNets\n    An Experimental Study of Neural Networks for Variable Graphs (Xavier Bresson and Thomas Laurent, ICLR 2018)\n    https://arxiv.org/pdf/1711.07553v2.pdf\n""""""\nfrom layers.gated_gcn_layer import GatedGCNLayer\nfrom layers.mlp_readout_layer import MLPReadout\n\nclass GatedGCNNet(nn.Module):\n    def __init__(self, net_params):\n        super().__init__()\n        num_atom_type = net_params[\'num_atom_type\']\n        num_bond_type = net_params[\'num_bond_type\']\n        hidden_dim = net_params[\'hidden_dim\']\n        out_dim = net_params[\'out_dim\']\n        in_feat_dropout = net_params[\'in_feat_dropout\']\n        dropout = net_params[\'dropout\']\n        n_layers = net_params[\'L\']\n        self.readout = net_params[\'readout\']\n        self.graph_norm = net_params[\'graph_norm\']\n        self.batch_norm = net_params[\'batch_norm\']\n        self.residual = net_params[\'residual\']\n        self.edge_feat = net_params[\'edge_feat\']\n        self.device = net_params[\'device\']\n        \n        self.embedding_h = nn.Embedding(num_atom_type, hidden_dim)\n\n        if self.edge_feat:\n            self.embedding_e = nn.Embedding(num_bond_type, hidden_dim)\n        else:\n            self.embedding_e = nn.Linear(1, hidden_dim)\n        \n        self.in_feat_dropout = nn.Dropout(in_feat_dropout)\n        \n        self.layers = nn.ModuleList([ GatedGCNLayer(hidden_dim, hidden_dim, dropout,\n                                                       self.graph_norm, self.batch_norm, self.residual) for _ in range(n_layers-1) ]) \n        self.layers.append(GatedGCNLayer(hidden_dim, out_dim, dropout, self.graph_norm, self.batch_norm, self.residual))\n        self.MLP_layer = MLPReadout(out_dim, 1)   # 1 out dim since regression problem        \n        \n    def forward(self, g, h, e, snorm_n, snorm_e):\n\n        # input embedding\n        h = self.embedding_h(h)\n        h = self.in_feat_dropout(h)\n        if not self.edge_feat: # edge feature set to 1\n            e = torch.ones(e.size(0),1).to(self.device)\n        e = self.embedding_e(e)   \n        \n        # convnets\n        for conv in self.layers:\n            h, e = conv(g, h, e, snorm_n, snorm_e)\n        g.ndata[\'h\'] = h\n        \n        if self.readout == ""sum"":\n            hg = dgl.sum_nodes(g, \'h\')\n        elif self.readout == ""max"":\n            hg = dgl.max_nodes(g, \'h\')\n        elif self.readout == ""mean"":\n            hg = dgl.mean_nodes(g, \'h\')\n        else:\n            hg = dgl.mean_nodes(g, \'h\')  # default readout is mean nodes\n            \n        return self.MLP_layer(hg)\n        \n    def loss(self, scores, targets):\n        # loss = nn.MSELoss()(scores,targets)\n        loss = nn.L1Loss()(scores, targets)\n        return loss\n'"
nets/molecules_graph_regression/gcn_net.py,2,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nimport dgl\n\n""""""\n    GCN: Graph Convolutional Networks\n    Thomas N. Kipf, Max Welling, Semi-Supervised Classification with Graph Convolutional Networks (ICLR 2017)\n    http://arxiv.org/abs/1609.02907\n""""""\nfrom layers.gcn_layer import GCNLayer\nfrom layers.mlp_readout_layer import MLPReadout\n\nclass GCNNet(nn.Module):\n    def __init__(self, net_params):\n        super().__init__()\n        num_atom_type = net_params[\'num_atom_type\']\n        num_bond_type = net_params[\'num_bond_type\']\n        hidden_dim = net_params[\'hidden_dim\']\n        out_dim = net_params[\'out_dim\']\n        in_feat_dropout = net_params[\'in_feat_dropout\']\n        dropout = net_params[\'dropout\']\n        n_layers = net_params[\'L\']\n        self.readout = net_params[\'readout\']\n        self.graph_norm = net_params[\'graph_norm\']\n        self.batch_norm = net_params[\'batch_norm\']\n        self.residual = net_params[\'residual\']\n        \n        self.in_feat_dropout = nn.Dropout(in_feat_dropout)\n        \n        self.embedding_h = nn.Embedding(num_atom_type, hidden_dim)\n        \n        self.layers = nn.ModuleList([GCNLayer(hidden_dim, hidden_dim, F.relu,\n                                              dropout, self.graph_norm, self.batch_norm, self.residual) for _ in range(n_layers-1)])\n        self.layers.append(GCNLayer(hidden_dim, out_dim, F.relu,\n                                    dropout, self.graph_norm, self.batch_norm, self.residual))\n        self.MLP_layer = MLPReadout(out_dim, 1)   # 1 out dim since regression problem        \n\n    def forward(self, g, h, e, snorm_n, snorm_e):\n        h = self.embedding_h(h)\n        h = self.in_feat_dropout(h)\n        \n        for conv in self.layers:\n            h = conv(g, h, snorm_n)\n        g.ndata[\'h\'] = h\n        \n        if self.readout == ""sum"":\n            hg = dgl.sum_nodes(g, \'h\')\n        elif self.readout == ""max"":\n            hg = dgl.max_nodes(g, \'h\')\n        elif self.readout == ""mean"":\n            hg = dgl.mean_nodes(g, \'h\')\n        else:\n            hg = dgl.mean_nodes(g, \'h\')  # default readout is mean nodes\n            \n        return self.MLP_layer(hg)\n    \n    def loss(self, scores, targets):\n        # loss = nn.MSELoss()(scores,targets)\n        loss = nn.L1Loss()(scores, targets)\n        return loss\n    '"
nets/molecules_graph_regression/gin_net.py,5,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nimport dgl\nfrom dgl.nn.pytorch.glob import SumPooling, AvgPooling, MaxPooling\n\n""""""\n    GIN: Graph Isomorphism Networks\n    HOW POWERFUL ARE GRAPH NEURAL NETWORKS? (Keyulu Xu, Weihua Hu, Jure Leskovec and Stefanie Jegelka, ICLR 2019)\n    https://arxiv.org/pdf/1810.00826.pdf\n""""""\n\nfrom layers.gin_layer import GINLayer, ApplyNodeFunc, MLP\n\nclass GINNet(nn.Module):\n    \n    def __init__(self, net_params):\n        super().__init__()\n        num_atom_type = net_params[\'num_atom_type\']\n        hidden_dim = net_params[\'hidden_dim\']\n        dropout = net_params[\'dropout\']\n        self.n_layers = net_params[\'L\']\n        n_mlp_layers = net_params[\'n_mlp_GIN\']               # GIN\n        learn_eps = net_params[\'learn_eps_GIN\']              # GIN\n        neighbor_aggr_type = net_params[\'neighbor_aggr_GIN\'] # GIN\n        readout = net_params[\'readout\']                      # this is graph_pooling_type\n        graph_norm = net_params[\'graph_norm\']      \n        batch_norm = net_params[\'batch_norm\']\n        residual = net_params[\'residual\']\n        \n        # List of MLPs\n        self.ginlayers = torch.nn.ModuleList()\n        \n        self.embedding_h = nn.Embedding(num_atom_type, hidden_dim)\n        \n        for layer in range(self.n_layers):\n            mlp = MLP(n_mlp_layers, hidden_dim, hidden_dim, hidden_dim)\n            \n            self.ginlayers.append(GINLayer(ApplyNodeFunc(mlp), neighbor_aggr_type,\n                                           dropout, graph_norm, batch_norm, residual, 0, learn_eps))\n\n        # Linear function for graph poolings (readout) of output of each layer\n        # which maps the output of different layers into a prediction score\n        self.linears_prediction = torch.nn.ModuleList()\n\n        for layer in range(self.n_layers+1):\n            self.linears_prediction.append(nn.Linear(hidden_dim, 1))   # 1 out dim since regression problem \n        \n        if readout == \'sum\':\n            self.pool = SumPooling()\n        elif readout == \'mean\':\n            self.pool = AvgPooling()\n        elif readout == \'max\':\n            self.pool = MaxPooling()\n        else:\n            raise NotImplementedError\n        \n    def forward(self, g, h, e, snorm_n, snorm_e):\n        \n        h = self.embedding_h(h)\n        \n        # list of hidden representation at each layer (including input)\n        hidden_rep = [h]\n\n        for i in range(self.n_layers):\n            h = self.ginlayers[i](g, h, snorm_n)\n            hidden_rep.append(h)\n\n        score_over_layer = 0\n\n        # perform pooling over all nodes in each graph in every layer\n        for i, h in enumerate(hidden_rep):\n            pooled_h = self.pool(g, h)\n            score_over_layer += self.linears_prediction[i](pooled_h)\n\n        return score_over_layer\n        \n    def loss(self, scores, targets):\n        # loss = nn.MSELoss()(scores,targets)\n        loss = nn.L1Loss()(scores, targets)\n        return loss\n'"
nets/molecules_graph_regression/graphsage_net.py,2,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nimport dgl\n\n""""""\n    GraphSAGE: \n    William L. Hamilton, Rex Ying, Jure Leskovec, Inductive Representation Learning on Large Graphs (NeurIPS 2017)\n    https://cs.stanford.edu/people/jure/pubs/graphsage-nips17.pdf\n""""""\n\nfrom layers.graphsage_layer import GraphSageLayer\nfrom layers.mlp_readout_layer import MLPReadout\n\nclass GraphSageNet(nn.Module):\n    """"""\n    Grahpsage network with multiple GraphSageLayer layers\n    """"""\n    def __init__(self, net_params):\n        super().__init__()\n        num_atom_type = net_params[\'num_atom_type\']\n        num_bond_type = net_params[\'num_bond_type\']\n        hidden_dim = net_params[\'hidden_dim\']\n        out_dim = net_params[\'out_dim\']\n        in_feat_dropout = net_params[\'in_feat_dropout\']\n        dropout = net_params[\'dropout\']\n        aggregator_type = net_params[\'sage_aggregator\']\n        n_layers = net_params[\'L\']\n        graph_norm = net_params[\'graph_norm\']      \n        batch_norm = net_params[\'batch_norm\']\n        residual = net_params[\'residual\']\n        self.readout = net_params[\'readout\']\n        \n        self.embedding_h = nn.Embedding(num_atom_type, hidden_dim)\n        \n        self.in_feat_dropout = nn.Dropout(in_feat_dropout)\n        \n        self.layers = nn.ModuleList([GraphSageLayer(hidden_dim, hidden_dim, F.relu,\n                                              dropout, aggregator_type, graph_norm, batch_norm, residual) for _ in range(n_layers-1)])\n        self.layers.append(GraphSageLayer(hidden_dim, out_dim, F.relu, dropout, aggregator_type, graph_norm, batch_norm, residual))\n        self.MLP_layer = MLPReadout(out_dim, 1)    # 1 out dim since regression problem\n        \n    def forward(self, g, h, e, snorm_n, snorm_e):\n        h = self.embedding_h(h)\n        h = self.in_feat_dropout(h)\n        for conv in self.layers:\n            h = conv(g, h, snorm_n)\n        g.ndata[\'h\'] = h\n        \n        if self.readout == ""sum"":\n            hg = dgl.sum_nodes(g, \'h\')\n        elif self.readout == ""max"":\n            hg = dgl.max_nodes(g, \'h\')\n        elif self.readout == ""mean"":\n            hg = dgl.mean_nodes(g, \'h\')\n        else:\n            hg = dgl.mean_nodes(g, \'h\')  # default readout is mean nodes\n            \n        return self.MLP_layer(hg)\n    \n    def loss(self, scores, targets):\n        # loss = nn.MSELoss()(scores,targets)\n        loss = nn.L1Loss()(scores, targets)\n        return loss\n'"
nets/molecules_graph_regression/load_net.py,0,"b'""""""\n    Utility file to select GraphNN model as\n    selected by the user\n""""""\n\nfrom nets.molecules_graph_regression.gated_gcn_net import GatedGCNNet\nfrom nets.molecules_graph_regression.gcn_net import GCNNet\nfrom nets.molecules_graph_regression.gat_net import GATNet\nfrom nets.molecules_graph_regression.graphsage_net import GraphSageNet\nfrom nets.molecules_graph_regression.gin_net import GINNet\nfrom nets.molecules_graph_regression.mo_net import MoNet as MoNet_\nfrom nets.molecules_graph_regression.diffpool_net import DiffPoolNet\nfrom nets.molecules_graph_regression.mlp_net import MLPNet\n\n\ndef GatedGCN(net_params):\n    return GatedGCNNet(net_params)\n\ndef GCN(net_params):\n    return GCNNet(net_params)\n\ndef GAT(net_params):\n    return GATNet(net_params)\n\ndef GraphSage(net_params):\n    return GraphSageNet(net_params)\n\ndef GIN(net_params):\n    return GINNet(net_params)\n\ndef MoNet(net_params):\n    return MoNet_(net_params)\n\ndef DiffPool(net_params):\n    return DiffPoolNet(net_params)\n\ndef MLP(net_params):\n    return MLPNet(net_params)\n\ndef gnn_model(MODEL_NAME, net_params):\n    models = {\n        \'GatedGCN\': GatedGCN,\n        \'GCN\': GCN,\n        \'GAT\': GAT,\n        \'GraphSage\': GraphSage,\n        \'GIN\': GIN,\n        \'MoNet\': MoNet,\n        \'DiffPool\': DiffPool,\n        \'MLP\': MLP\n    }\n        \n    return models[MODEL_NAME](net_params)'"
nets/molecules_graph_regression/mlp_net.py,4,"b""import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nimport dgl\n\nfrom layers.mlp_readout_layer import MLPReadout\n\nclass MLPNet(nn.Module):\n    def __init__(self, net_params):\n        super().__init__()\n        num_atom_type = net_params['num_atom_type']\n        num_bond_type = net_params['num_bond_type']\n        hidden_dim = net_params['hidden_dim']\n        out_dim = net_params['out_dim']\n        in_feat_dropout = net_params['in_feat_dropout']\n        dropout = net_params['dropout']\n        n_layers = net_params['L']\n        self.gated = net_params['gated']\n        self.readout = net_params['readout']\n        \n        self.embedding_h = nn.Embedding(num_atom_type, hidden_dim)\n        \n        self.in_feat_dropout = nn.Dropout(in_feat_dropout)\n        \n        feat_mlp_modules = [\n            nn.Linear(hidden_dim, hidden_dim, bias=True),\n            nn.ReLU(),\n            nn.Dropout(dropout),\n        ]\n        for _ in range(n_layers-1):\n            feat_mlp_modules.append(nn.Linear(hidden_dim, hidden_dim, bias=True))\n            feat_mlp_modules.append(nn.ReLU())\n            feat_mlp_modules.append(nn.Dropout(dropout))\n        self.feat_mlp = nn.Sequential(*feat_mlp_modules)\n        \n        if self.gated:\n            self.gates = nn.Linear(hidden_dim, hidden_dim, bias=True)\n        \n        self.readout_mlp = MLPReadout(out_dim, 1)    # 1 out dim since regression problem\n\n    def forward(self, g, h, e, snorm_n, snorm_e):\n        h = self.embedding_h(h)\n        h = self.in_feat_dropout(h)\n        h = self.feat_mlp(h)\n        if self.gated:\n            h = torch.sigmoid(self.gates(h)) * h\n            g.ndata['h'] = h       \n            hg = dgl.sum_nodes(g, 'h')\n            # hg = torch.cat(\n            #     (\n            #         dgl.sum_nodes(g, 'h'),\n            #         dgl.max_nodes(g, 'h')\n            #     ),\n            #     dim=1\n            # )\n        \n        else:\n            g.ndata['h'] = h\n            hg = dgl.mean_nodes(g, 'h')\n        \n        return self.readout_mlp(hg)\n        \n        \n    def loss(self, scores, targets):\n        # loss = nn.MSELoss()(scores,targets)\n        loss = nn.L1Loss()(scores, targets)\n        return loss\n       """
nets/molecules_graph_regression/mo_net.py,3,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nimport dgl\n\nimport numpy as np\n\n""""""\n    GMM: Gaussian Mixture Model Convolution layer\n    Geometric Deep Learning on Graphs and Manifolds using Mixture Model CNNs (Federico Monti et al., CVPR 2017)\n    https://arxiv.org/pdf/1611.08402.pdf\n""""""\n\nfrom layers.gmm_layer import GMMLayer\nfrom layers.mlp_readout_layer import MLPReadout\n\nclass MoNet(nn.Module):\n    def __init__(self, net_params):\n        super().__init__()\n        num_atom_type = net_params[\'num_atom_type\']\n        hidden_dim = net_params[\'hidden_dim\']\n        out_dim = net_params[\'out_dim\']\n        kernel = net_params[\'kernel\']                       # for MoNet\n        dim = net_params[\'pseudo_dim_MoNet\']                # for MoNet\n        dropout = net_params[\'dropout\']\n        n_layers = net_params[\'L\']\n        self.readout = net_params[\'readout\']                      \n        graph_norm = net_params[\'graph_norm\']      \n        batch_norm = net_params[\'batch_norm\']\n        residual = net_params[\'residual\']  \n        self.device = net_params[\'device\']\n        \n        aggr_type = ""sum""                                    # default for MoNet\n        \n        self.embedding_h = nn.Embedding(num_atom_type, hidden_dim)\n        \n        self.layers = nn.ModuleList()\n        self.pseudo_proj = nn.ModuleList()\n\n        # Hidden layer\n        for _ in range(n_layers-1):\n            self.layers.append(GMMLayer(hidden_dim, hidden_dim, dim, kernel, aggr_type,\n                                        dropout, graph_norm, batch_norm, residual))\n            self.pseudo_proj.append(nn.Sequential(nn.Linear(2, dim), nn.Tanh()))\n            \n        # Output layer\n        self.layers.append(GMMLayer(hidden_dim, out_dim, dim, kernel, aggr_type,\n                                    dropout, graph_norm, batch_norm, residual))\n        self.pseudo_proj.append(nn.Sequential(nn.Linear(2, dim), nn.Tanh()))\n        \n        self.MLP_layer = MLPReadout(out_dim, 1) # out dim 1 since regression\n\n    def forward(self, g, h, e, snorm_n, snorm_e):\n        h = self.embedding_h(h)\n        \n        # computing the \'pseudo\' named tensor which depends on node degrees\n        us, vs = g.edges()\n        # to avoid zero division in case in_degree is 0, we add constant \'1\' in all node degrees denoting self-loop\n        pseudo = [ [1/np.sqrt(g.in_degree(us[i])+1), 1/np.sqrt(g.in_degree(vs[i])+1)] for i in range(g.number_of_edges()) ]\n        pseudo = torch.Tensor(pseudo).to(self.device)\n        \n        for i in range(len(self.layers)):\n            h = self.layers[i](g, h, self.pseudo_proj[i](pseudo), snorm_n)\n        g.ndata[\'h\'] = h\n            \n        if self.readout == ""sum"":\n            hg = dgl.sum_nodes(g, \'h\')\n        elif self.readout == ""max"":\n            hg = dgl.max_nodes(g, \'h\')\n        elif self.readout == ""mean"":\n            hg = dgl.mean_nodes(g, \'h\')\n        else:\n            hg = dgl.mean_nodes(g, \'h\')  # default readout is mean nodes\n\n        return self.MLP_layer(hg)\n\n    def loss(self, scores, targets):\n        # loss = nn.MSELoss()(scores,targets)\n        loss = nn.L1Loss()(scores, targets)\n        return loss\n'"
nets/superpixels_graph_classification/diffpool_net.py,15,"b'import torch\nimport torch.nn as nn\nfrom torch.nn import init\nimport torch.nn.functional as F\n\nimport time\nimport numpy as np\nfrom scipy.linalg import block_diag\n\nimport dgl\n\n""""""\n    <Diffpool Fuse with GNN layers and pooling layers>\n    \n    DIFFPOOL:\n    Z. Ying, J. You, C. Morris, X. Ren, W. Hamilton, and J. Leskovec, \n    Hierarchical graph representation learning with differentiable pooling (NeurIPS 2018)\n    https://arxiv.org/pdf/1806.08804.pdf\n    \n    ! code started from dgl diffpool examples dir\n""""""\n\nfrom layers.graphsage_layer import GraphSageLayer   # this is GraphSageLayer\nfrom layers.diffpool_layer import DiffPoolLayer   # this is DiffPoolBatchedGraphLayer\n# from .graphsage_net import GraphSageNet   # this is GraphSage\n# replace BatchedDiffPool with DenseDiffPool and BatchedGraphSAGE with DenseGraphSage\nfrom layers.tensorized.dense_graphsage_layer import DenseGraphSage\nfrom layers.tensorized.dense_diffpool_layer import DenseDiffPool\n\nclass DiffPoolNet(nn.Module):\n    """"""\n    DiffPool Fuse with GNN layers and pooling layers in sequence\n    """"""\n\n    def __init__(self, net_params):\n        \n        super().__init__()\n        input_dim = net_params[\'in_dim\']\n        hidden_dim = net_params[\'hidden_dim\']\n        embedding_dim = net_params[\'embedding_dim\']\n        label_dim = net_params[\'n_classes\']\n        activation = F.relu\n        n_layers = net_params[\'L\'] # this is the gnn_per_block param\n        dropout = net_params[\'dropout\']\n        self.graph_norm = net_params[\'graph_norm\']\n        self.batch_norm = net_params[\'batch_norm\']\n        self.residual = net_params[\'residual\']\n        aggregator_type = net_params[\'sage_aggregator\']\n        pool_ratio = net_params[\'pool_ratio\']\n\n        self.device = net_params[\'device\']\n        self.link_pred = net_params[\'linkpred\']\n        self.concat = net_params[\'cat\']\n        self.n_pooling = net_params[\'num_pool\']\n        self.batch_size = net_params[\'batch_size\']\n        self.link_pred_loss = []\n        self.entropy_loss = []\n        \n        self.embedding_h = nn.Linear(input_dim, hidden_dim)\n        \n        # list of GNN modules before the first diffpool operation\n        self.gc_before_pool = nn.ModuleList()\n\n        self.assign_dim = net_params[\'assign_dim\']\n        # self.bn = True\n        self.num_aggs = 1\n\n        # constructing layers\n        # layers before diffpool\n        assert n_layers >= 3, ""n_layers too few""\n        self.gc_before_pool.append(GraphSageLayer(hidden_dim, hidden_dim, activation,\n                                                  dropout, aggregator_type, self.graph_norm, self.batch_norm, self.residual))\n        \n        for _ in range(n_layers - 2):\n            self.gc_before_pool.append(GraphSageLayer(hidden_dim, hidden_dim, activation,\n                                                      dropout, aggregator_type, self.graph_norm, self.batch_norm, self.residual))\n        \n        self.gc_before_pool.append(GraphSageLayer(hidden_dim, embedding_dim, None, dropout, aggregator_type, self.graph_norm, self.batch_norm, self.residual))\n\n        \n        assign_dims = []\n        assign_dims.append(self.assign_dim)\n        if self.concat:\n            # diffpool layer receive pool_emedding_dim node feature tensor\n            # and return pool_embedding_dim node embedding\n            pool_embedding_dim = hidden_dim * (n_layers - 1) + embedding_dim\n        else:\n\n            pool_embedding_dim = embedding_dim\n\n        self.first_diffpool_layer = DiffPoolLayer(pool_embedding_dim, self.assign_dim, hidden_dim,\n                                                  activation, dropout, aggregator_type, self.graph_norm, self.batch_norm, self.link_pred)\n        gc_after_per_pool = nn.ModuleList()\n\n        # list of list of GNN modules, each list after one diffpool operation\n        self.gc_after_pool = nn.ModuleList()\n        \n        for _ in range(n_layers - 1):\n            gc_after_per_pool.append(DenseGraphSage(hidden_dim, hidden_dim, self.residual))\n        gc_after_per_pool.append(DenseGraphSage(hidden_dim, embedding_dim, self.residual))\n        self.gc_after_pool.append(gc_after_per_pool)\n\n        self.assign_dim = int(self.assign_dim * pool_ratio)\n        \n        self.diffpool_layers = nn.ModuleList()\n        # each pooling module\n        for _ in range(self.n_pooling - 1):\n            self.diffpool_layers.append(DenseDiffPool(pool_embedding_dim, self.assign_dim, hidden_dim, self.link_pred))\n            \n            gc_after_per_pool = nn.ModuleList()\n            \n            for _ in range(n_layers - 1):\n                gc_after_per_pool.append(DenseGraphSage(hidden_dim, hidden_dim, self.residual))\n            gc_after_per_pool.append(DenseGraphSage(hidden_dim, embedding_dim, self.residual))\n            self.gc_after_pool.append(gc_after_per_pool)\n            \n            assign_dims.append(self.assign_dim)\n            self.assign_dim = int(self.assign_dim * pool_ratio)\n\n        # predicting layer\n        if self.concat:\n            self.pred_input_dim = pool_embedding_dim * \\\n                self.num_aggs * (n_pooling + 1)\n        else:\n            self.pred_input_dim = embedding_dim * self.num_aggs\n        self.pred_layer = nn.Linear(self.pred_input_dim, label_dim)\n\n        # weight initialization\n        for m in self.modules():\n            if isinstance(m, nn.Linear):\n                m.weight.data = init.xavier_uniform_(m.weight.data,\n                                                     gain=nn.init.calculate_gain(\'relu\'))\n                if m.bias is not None:\n                    m.bias.data = init.constant_(m.bias.data, 0.0)\n\n    def gcn_forward(self, g, h, snorm_n, gc_layers, cat=False):\n        """"""\n        Return gc_layer embedding cat.\n        """"""\n        block_readout = []\n        for gc_layer in gc_layers[:-1]:\n            h = gc_layer(g, h, snorm_n)\n            block_readout.append(h)\n        h = gc_layers[-1](g, h, snorm_n)\n        block_readout.append(h)\n        if cat:\n            block = torch.cat(block_readout, dim=1)  # N x F, F = F1 + F2 + ...\n        else:\n            block = h\n        return block\n\n    def gcn_forward_tensorized(self, h, adj, gc_layers, cat=False):\n        block_readout = []\n        for gc_layer in gc_layers:\n            h = gc_layer(h, adj)\n            block_readout.append(h)\n        if cat:\n            block = torch.cat(block_readout, dim=2)  # N x F, F = F1 + F2 + ...\n        else:\n            block = h\n        return block\n\n    def forward(self, g, h, e, snorm_n, snorm_e):\n        self.link_pred_loss = []\n        self.entropy_loss = []\n        \n        # node feature for assignment matrix computation is the same as the\n        # original node feature\n        h = self.embedding_h(h)\n        h_a = h\n\n        out_all = []\n\n        # we use GCN blocks to get an embedding first\n        g_embedding = self.gcn_forward(g, h, snorm_n, self.gc_before_pool, self.concat)\n\n        g.ndata[\'h\'] = g_embedding\n\n        readout = dgl.sum_nodes(g, \'h\')\n        out_all.append(readout)\n        if self.num_aggs == 2:\n            readout = dgl.max_nodes(g, \'h\')\n            out_all.append(readout)\n\n        adj, h = self.first_diffpool_layer(g, g_embedding, snorm_n)\n        node_per_pool_graph = int(adj.size()[0] / self.batch_size)\n\n        h, adj = self.batch2tensor(adj, h, node_per_pool_graph)\n        h = self.gcn_forward_tensorized(h, adj, self.gc_after_pool[0], self.concat)\n        \n        readout = torch.sum(h, dim=1)\n        out_all.append(readout)\n        if self.num_aggs == 2:\n            readout, _ = torch.max(h, dim=1)\n            out_all.append(readout)\n\n        for i, diffpool_layer in enumerate(self.diffpool_layers):\n            h, adj = diffpool_layer(h, adj)\n            h = self.gcn_forward_tensorized(h, adj, self.gc_after_pool[i + 1], self.concat)\n            \n            readout = torch.sum(h, dim=1)\n            out_all.append(readout)\n            \n            if self.num_aggs == 2:\n                readout, _ = torch.max(h, dim=1)\n                out_all.append(readout)\n        \n        if self.concat or self.num_aggs > 1:\n            final_readout = torch.cat(out_all, dim=1)\n        else:\n            final_readout = readout\n        ypred = self.pred_layer(final_readout)\n        return ypred\n\n    def batch2tensor(self, batch_adj, batch_feat, node_per_pool_graph):\n        """"""\n        transform a batched graph to batched adjacency tensor and node feature tensor\n        """"""\n        batch_size = int(batch_adj.size()[0] / node_per_pool_graph)\n        adj_list = []\n        feat_list = []\n\n        for i in range(batch_size):\n            start = i * node_per_pool_graph\n            end = (i + 1) * node_per_pool_graph\n\n            # 1/sqrt(V) normalization\n            snorm_n = torch.FloatTensor(node_per_pool_graph, 1).fill_(1./float(node_per_pool_graph)).sqrt().to(self.device)\n\n            adj_list.append(batch_adj[start:end, start:end])\n            feat_list.append((batch_feat[start:end, :])*snorm_n)\n        adj_list = list(map(lambda x: torch.unsqueeze(x, 0), adj_list))\n        feat_list = list(map(lambda x: torch.unsqueeze(x, 0), feat_list))\n        adj = torch.cat(adj_list, dim=0)\n        feat = torch.cat(feat_list, dim=0)\n\n        return feat, adj\n    \n    def loss(self, pred, label):\n        \'\'\'\n        loss function\n        \'\'\'\n        #softmax + CE\n        criterion = nn.CrossEntropyLoss()\n        loss = criterion(pred, label)\n        for diffpool_layer in self.diffpool_layers:\n            for key, value in diffpool_layer.loss_log.items():\n                loss += value\n        return loss\n'"
nets/superpixels_graph_classification/gat_net.py,2,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nimport dgl\n\n""""""\n    GAT: Graph Attention Network\n    Graph Attention Networks (Veli\xc4\x8dkovi\xc4\x87 et al., ICLR 2018)\n    https://arxiv.org/abs/1710.10903\n""""""\nfrom layers.gat_layer import GATLayer\nfrom layers.mlp_readout_layer import MLPReadout\n\nclass GATNet(nn.Module):\n    def __init__(self, net_params):\n        super().__init__()\n        in_dim = net_params[\'in_dim\']\n        hidden_dim = net_params[\'hidden_dim\']\n        out_dim = net_params[\'out_dim\']\n        n_classes = net_params[\'n_classes\']\n        num_heads = net_params[\'n_heads\']\n        in_feat_dropout = net_params[\'in_feat_dropout\']\n        dropout = net_params[\'dropout\']\n        n_layers = net_params[\'L\']\n        self.readout = net_params[\'readout\']\n        self.graph_norm = net_params[\'graph_norm\']\n        self.batch_norm = net_params[\'batch_norm\']\n        self.residual = net_params[\'residual\']\n        self.dropout = dropout\n        \n        self.embedding_h = nn.Linear(in_dim, hidden_dim * num_heads)\n        \n        self.in_feat_dropout = nn.Dropout(in_feat_dropout)\n        \n        self.layers = nn.ModuleList([GATLayer(hidden_dim * num_heads, hidden_dim, num_heads,\n                                              dropout, self.graph_norm, self.batch_norm, self.residual) for _ in range(n_layers-1)])\n        self.layers.append(GATLayer(hidden_dim * num_heads, out_dim, 1, dropout, self.graph_norm, self.batch_norm, self.residual))\n        self.MLP_layer = MLPReadout(out_dim, n_classes)\n        \n    def forward(self, g, h, e, snorm_n, snorm_e):\n        h = self.embedding_h(h)\n        h = self.in_feat_dropout(h)\n        for conv in self.layers:\n            h = conv(g, h, snorm_n)\n        g.ndata[\'h\'] = h\n        \n        if self.readout == ""sum"":\n            hg = dgl.sum_nodes(g, \'h\')\n        elif self.readout == ""max"":\n            hg = dgl.max_nodes(g, \'h\')\n        elif self.readout == ""mean"":\n            hg = dgl.mean_nodes(g, \'h\')\n        else:\n            hg = dgl.mean_nodes(g, \'h\')  # default readout is mean nodes\n            \n        return self.MLP_layer(hg)\n    \n    def loss(self, pred, label):\n        criterion = nn.CrossEntropyLoss()\n        loss = criterion(pred, label)\n        return loss'"
nets/superpixels_graph_classification/gated_gcn_net.py,3,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nimport dgl\n\n""""""\n    ResGatedGCN: Residual Gated Graph ConvNets\n    An Experimental Study of Neural Networks for Variable Graphs (Xavier Bresson and Thomas Laurent, ICLR 2018)\n    https://arxiv.org/pdf/1711.07553v2.pdf\n""""""\nfrom layers.gated_gcn_layer import GatedGCNLayer\nfrom layers.mlp_readout_layer import MLPReadout\n\nclass GatedGCNNet(nn.Module):\n    \n    def __init__(self, net_params):\n        super().__init__()\n        in_dim = net_params[\'in_dim\']\n        in_dim_edge = net_params[\'in_dim_edge\']\n        hidden_dim = net_params[\'hidden_dim\']\n        out_dim = net_params[\'out_dim\']\n        n_classes = net_params[\'n_classes\']\n        dropout = net_params[\'dropout\']\n        n_layers = net_params[\'L\']\n        self.readout = net_params[\'readout\']\n        self.graph_norm = net_params[\'graph_norm\']\n        self.batch_norm = net_params[\'batch_norm\']\n        self.residual = net_params[\'residual\']\n        self.edge_feat = net_params[\'edge_feat\']\n        self.device = net_params[\'device\']\n        \n        self.embedding_h = nn.Linear(in_dim, hidden_dim)\n        self.embedding_e = nn.Linear(in_dim_edge, hidden_dim)\n        self.layers = nn.ModuleList([ GatedGCNLayer(hidden_dim, hidden_dim, dropout,\n                                                       self.graph_norm, self.batch_norm, self.residual) for _ in range(n_layers-1) ]) \n        self.layers.append(GatedGCNLayer(hidden_dim, out_dim, dropout, self.graph_norm, self.batch_norm, self.residual))\n        self.MLP_layer = MLPReadout(out_dim, n_classes)\n        \n    def forward(self, g, h, e, snorm_n, snorm_e):\n\n        # input embedding\n        h = self.embedding_h(h)\n        if not self.edge_feat: # edge feature set to 1\n            e = torch.ones_like(e).to(self.device)\n        e = self.embedding_e(e)\n        \n        # convnets\n        for conv in self.layers:\n            h, e = conv(g, h, e, snorm_n, snorm_e)\n        g.ndata[\'h\'] = h\n        \n        if self.readout == ""sum"":\n            hg = dgl.sum_nodes(g, \'h\')\n        elif self.readout == ""max"":\n            hg = dgl.max_nodes(g, \'h\')\n        elif self.readout == ""mean"":\n            hg = dgl.mean_nodes(g, \'h\')\n        else:\n            hg = dgl.mean_nodes(g, \'h\')  # default readout is mean nodes\n            \n        return self.MLP_layer(hg)\n        \n    def loss(self, pred, label):\n        criterion = nn.CrossEntropyLoss()\n        loss = criterion(pred, label)\n        return loss'"
nets/superpixels_graph_classification/gcn_net.py,2,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nimport dgl\n\n""""""\n    GCN: Graph Convolutional Networks\n    Thomas N. Kipf, Max Welling, Semi-Supervised Classification with Graph Convolutional Networks (ICLR 2017)\n    http://arxiv.org/abs/1609.02907\n""""""\nfrom layers.gcn_layer import GCNLayer\nfrom layers.mlp_readout_layer import MLPReadout\n\nclass GCNNet(nn.Module):\n    def __init__(self, net_params):\n        super().__init__()\n        in_dim = net_params[\'in_dim\']\n        hidden_dim = net_params[\'hidden_dim\']\n        out_dim = net_params[\'out_dim\']\n        n_classes = net_params[\'n_classes\']\n        in_feat_dropout = net_params[\'in_feat_dropout\']\n        dropout = net_params[\'dropout\']\n        n_layers = net_params[\'L\']\n        self.readout = net_params[\'readout\']\n        self.graph_norm = net_params[\'graph_norm\']\n        self.batch_norm = net_params[\'batch_norm\']\n        self.residual = net_params[\'residual\']\n        \n        self.embedding_h = nn.Linear(in_dim, hidden_dim)\n        self.in_feat_dropout = nn.Dropout(in_feat_dropout)\n        \n        self.layers = nn.ModuleList([GCNLayer(hidden_dim, hidden_dim, F.relu, dropout,\n                                              self.graph_norm, self.batch_norm, self.residual) for _ in range(n_layers-1)])\n        self.layers.append(GCNLayer(hidden_dim, out_dim, F.relu, dropout, self.graph_norm, self.batch_norm, self.residual))\n        self.MLP_layer = MLPReadout(out_dim, n_classes)        \n\n    def forward(self, g, h, e, snorm_n, snorm_e):\n        h = self.embedding_h(h)\n        h = self.in_feat_dropout(h)\n        for conv in self.layers:\n            h = conv(g, h, snorm_n)\n        g.ndata[\'h\'] = h\n        \n        if self.readout == ""sum"":\n            hg = dgl.sum_nodes(g, \'h\')\n        elif self.readout == ""max"":\n            hg = dgl.max_nodes(g, \'h\')\n        elif self.readout == ""mean"":\n            hg = dgl.mean_nodes(g, \'h\')\n        else:\n            hg = dgl.mean_nodes(g, \'h\')  # default readout is mean nodes\n            \n        return self.MLP_layer(hg)\n    \n    def loss(self, pred, label):\n        criterion = nn.CrossEntropyLoss()\n        loss = criterion(pred, label)\n        return loss'"
nets/superpixels_graph_classification/gin_net.py,5,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nimport dgl\nfrom dgl.nn.pytorch.glob import SumPooling, AvgPooling, MaxPooling\n\n""""""\n    GIN: Graph Isomorphism Networks\n    HOW POWERFUL ARE GRAPH NEURAL NETWORKS? (Keyulu Xu, Weihua Hu, Jure Leskovec and Stefanie Jegelka, ICLR 2019)\n    https://arxiv.org/pdf/1810.00826.pdf\n""""""\n\nfrom layers.gin_layer import GINLayer, ApplyNodeFunc, MLP\n\nclass GINNet(nn.Module):\n    \n    def __init__(self, net_params):\n        super().__init__()\n        in_dim = net_params[\'in_dim\']\n        hidden_dim = net_params[\'hidden_dim\']\n        n_classes = net_params[\'n_classes\']\n        dropout = net_params[\'dropout\']\n        self.n_layers = net_params[\'L\']\n        n_mlp_layers = net_params[\'n_mlp_GIN\']               # GIN\n        learn_eps = net_params[\'learn_eps_GIN\']              # GIN\n        neighbor_aggr_type = net_params[\'neighbor_aggr_GIN\'] # GIN\n        readout = net_params[\'readout\']                      # this is graph_pooling_type\n        graph_norm = net_params[\'graph_norm\']      \n        batch_norm = net_params[\'batch_norm\']\n        residual = net_params[\'residual\']     \n        \n        # List of MLPs\n        self.ginlayers = torch.nn.ModuleList()\n        \n        self.embedding_h = nn.Linear(in_dim, hidden_dim)\n        \n        for layer in range(self.n_layers):\n            mlp = MLP(n_mlp_layers, hidden_dim, hidden_dim, hidden_dim)\n            \n            self.ginlayers.append(GINLayer(ApplyNodeFunc(mlp), neighbor_aggr_type,\n                                           dropout, graph_norm, batch_norm, residual, 0, learn_eps))\n\n        # Linear function for graph poolings (readout) of output of each layer\n        # which maps the output of different layers into a prediction score\n        self.linears_prediction = torch.nn.ModuleList()\n\n        for layer in range(self.n_layers+1):\n            self.linears_prediction.append(nn.Linear(hidden_dim, n_classes))\n        \n        if readout == \'sum\':\n            self.pool = SumPooling()\n        elif readout == \'mean\':\n            self.pool = AvgPooling()\n        elif readout == \'max\':\n            self.pool = MaxPooling()\n        else:\n            raise NotImplementedError\n        \n    def forward(self, g, h, e, snorm_n, snorm_e):\n        \n        h = self.embedding_h(h)\n        \n        # list of hidden representation at each layer (including input)\n        hidden_rep = [h]\n\n        for i in range(self.n_layers):\n            h = self.ginlayers[i](g, h, snorm_n)\n            hidden_rep.append(h)\n\n        score_over_layer = 0\n\n        # perform pooling over all nodes in each graph in every layer\n        for i, h in enumerate(hidden_rep):\n            pooled_h = self.pool(g, h)\n            score_over_layer += self.linears_prediction[i](pooled_h)\n\n        return score_over_layer\n        \n    def loss(self, pred, label):\n        criterion = nn.CrossEntropyLoss()\n        loss = criterion(pred, label)\n        return loss'"
nets/superpixels_graph_classification/graphsage_net.py,2,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nimport dgl\n\n""""""\n    GraphSAGE: \n    William L. Hamilton, Rex Ying, Jure Leskovec, Inductive Representation Learning on Large Graphs (NeurIPS 2017)\n    https://cs.stanford.edu/people/jure/pubs/graphsage-nips17.pdf\n""""""\n\nfrom layers.graphsage_layer import GraphSageLayer\nfrom layers.mlp_readout_layer import MLPReadout\n\nclass GraphSageNet(nn.Module):\n    """"""\n    Grahpsage network with multiple GraphSageLayer layers\n    """"""\n    def __init__(self, net_params):\n        super().__init__()\n        in_dim = net_params[\'in_dim\']\n        hidden_dim = net_params[\'hidden_dim\']\n        out_dim = net_params[\'out_dim\']\n        n_classes = net_params[\'n_classes\']\n        in_feat_dropout = net_params[\'in_feat_dropout\']\n        dropout = net_params[\'dropout\']\n        aggregator_type = net_params[\'sage_aggregator\']\n        n_layers = net_params[\'L\']\n        graph_norm = net_params[\'graph_norm\']      \n        batch_norm = net_params[\'batch_norm\']\n        residual = net_params[\'residual\']\n        self.readout = net_params[\'readout\']\n        \n        self.embedding_h = nn.Linear(in_dim, hidden_dim)\n        self.in_feat_dropout = nn.Dropout(in_feat_dropout)\n        \n        self.layers = nn.ModuleList([GraphSageLayer(hidden_dim, hidden_dim, F.relu,\n                                              dropout, aggregator_type, graph_norm, batch_norm, residual) for _ in range(n_layers-1)])\n        self.layers.append(GraphSageLayer(hidden_dim, out_dim, F.relu, dropout, aggregator_type, graph_norm, batch_norm, residual))\n        self.MLP_layer = MLPReadout(out_dim, n_classes)\n        \n    def forward(self, g, h, e, snorm_n, snorm_e):\n        h = self.embedding_h(h)\n        h = self.in_feat_dropout(h)\n        for conv in self.layers:\n            h = conv(g, h, snorm_n)\n        g.ndata[\'h\'] = h\n        \n        if self.readout == ""sum"":\n            hg = dgl.sum_nodes(g, \'h\')\n        elif self.readout == ""max"":\n            hg = dgl.max_nodes(g, \'h\')\n        elif self.readout == ""mean"":\n            hg = dgl.mean_nodes(g, \'h\')\n        else:\n            hg = dgl.mean_nodes(g, \'h\')  # default readout is mean nodes\n            \n        return self.MLP_layer(hg)\n    \n    def loss(self, pred, label):\n        criterion = nn.CrossEntropyLoss()\n        loss = criterion(pred, label)\n        return loss'"
nets/superpixels_graph_classification/load_net.py,0,"b'""""""\n    Utility file to select GraphNN model as\n    selected by the user\n""""""\n\nfrom nets.superpixels_graph_classification.gated_gcn_net import GatedGCNNet\nfrom nets.superpixels_graph_classification.gcn_net import GCNNet\nfrom nets.superpixels_graph_classification.gat_net import GATNet\nfrom nets.superpixels_graph_classification.graphsage_net import GraphSageNet\nfrom nets.superpixels_graph_classification.gin_net import GINNet\nfrom nets.superpixels_graph_classification.mo_net import MoNet as MoNet_\nfrom nets.superpixels_graph_classification.diffpool_net import DiffPoolNet\nfrom nets.superpixels_graph_classification.mlp_net import MLPNet\n\n\ndef GatedGCN(net_params):\n    return GatedGCNNet(net_params)\n\ndef GCN(net_params):\n    return GCNNet(net_params)\n\ndef GAT(net_params):\n    return GATNet(net_params)\n\ndef GraphSage(net_params):\n    return GraphSageNet(net_params)\n\ndef GIN(net_params):\n    return GINNet(net_params)\n\ndef MoNet(net_params):\n    return MoNet_(net_params)\n\ndef DiffPool(net_params):\n    return DiffPoolNet(net_params)\n\ndef MLP(net_params):\n    return MLPNet(net_params)\n\ndef gnn_model(MODEL_NAME, net_params):\n    models = {\n        \'GatedGCN\': GatedGCN,\n        \'GCN\': GCN,\n        \'GAT\': GAT,\n        \'GraphSage\': GraphSage,\n        \'GIN\': GIN,\n        \'MoNet\': MoNet,\n        \'DiffPool\': DiffPool,\n        \'MLP\': MLP\n    }\n        \n    return models[MODEL_NAME](net_params)'"
nets/superpixels_graph_classification/mlp_net.py,4,"b""import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nimport dgl\n\nfrom layers.mlp_readout_layer import MLPReadout\n\nclass MLPNet(nn.Module):\n    def __init__(self, net_params):\n        super().__init__()\n        in_dim = net_params['in_dim']\n        hidden_dim = net_params['hidden_dim']\n        n_classes = net_params['n_classes']\n        in_feat_dropout = net_params['in_feat_dropout']\n        dropout = net_params['dropout']\n        n_layers = net_params['L']\n        self.gated = net_params['gated']\n        \n        self.in_feat_dropout = nn.Dropout(in_feat_dropout)\n        \n        feat_mlp_modules = [\n            nn.Linear(in_dim, hidden_dim, bias=True),\n            nn.ReLU(),\n            nn.Dropout(dropout),\n        ]\n        for _ in range(n_layers-1):\n            feat_mlp_modules.append(nn.Linear(hidden_dim, hidden_dim, bias=True))\n            feat_mlp_modules.append(nn.ReLU())\n            feat_mlp_modules.append(nn.Dropout(dropout))\n        self.feat_mlp = nn.Sequential(*feat_mlp_modules)\n        \n        if self.gated:\n            self.gates = nn.Linear(hidden_dim, hidden_dim, bias=True)\n        \n        self.readout_mlp = MLPReadout(hidden_dim, n_classes)\n\n    def forward(self, g, h, e, snorm_n, snorm_e):\n        h = self.in_feat_dropout(h)\n        h = self.feat_mlp(h)\n        if self.gated:\n            h = torch.sigmoid(self.gates(h)) * h\n            g.ndata['h'] = h       \n            hg = dgl.sum_nodes(g, 'h')\n            # hg = torch.cat(\n            #     (\n            #         dgl.sum_nodes(g, 'h'),\n            #         dgl.max_nodes(g, 'h')\n            #     ),\n            #     dim=1\n            # )\n        \n        else:\n            g.ndata['h'] = h\n            hg = dgl.mean_nodes(g, 'h')\n        \n        return self.readout_mlp(hg)\n        \n        \n    def loss(self, pred, label):\n        criterion = nn.CrossEntropyLoss()\n        loss = criterion(pred, label)\n        return loss"""
nets/superpixels_graph_classification/mo_net.py,3,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nimport dgl\n\nimport numpy as np\n\n""""""\n    GMM: Gaussian Mixture Model Convolution layer\n    Geometric Deep Learning on Graphs and Manifolds using Mixture Model CNNs (Federico Monti et al., CVPR 2017)\n    https://arxiv.org/pdf/1611.08402.pdf\n""""""\n\nfrom layers.gmm_layer import GMMLayer\nfrom layers.mlp_readout_layer import MLPReadout\n\nclass MoNet(nn.Module):\n    def __init__(self, net_params):\n        super().__init__()\n        \n        in_dim = net_params[\'in_dim\']\n        hidden_dim = net_params[\'hidden_dim\']\n        out_dim = net_params[\'out_dim\']\n        kernel = net_params[\'kernel\']                       # for MoNet\n        dim = net_params[\'pseudo_dim_MoNet\']                # for MoNet\n        n_classes = net_params[\'n_classes\']\n        dropout = net_params[\'dropout\']\n        n_layers = net_params[\'L\']\n        self.readout = net_params[\'readout\']                      \n        graph_norm = net_params[\'graph_norm\']      \n        batch_norm = net_params[\'batch_norm\']\n        residual = net_params[\'residual\']  \n        self.device = net_params[\'device\']\n        \n        aggr_type = ""sum""                                    # default for MoNet\n        \n        self.embedding_h = nn.Linear(in_dim, hidden_dim)\n        \n        self.layers = nn.ModuleList()\n        self.pseudo_proj = nn.ModuleList()\n\n        # Hidden layer\n        for _ in range(n_layers-1):\n            self.layers.append(GMMLayer(hidden_dim, hidden_dim, dim, kernel, aggr_type,\n                                        dropout, graph_norm, batch_norm, residual))\n            self.pseudo_proj.append(nn.Sequential(nn.Linear(2, dim), nn.Tanh()))\n            \n        # Output layer\n        self.layers.append(GMMLayer(hidden_dim, out_dim, dim, kernel, aggr_type,\n                                    dropout, graph_norm, batch_norm, residual))\n        self.pseudo_proj.append(nn.Sequential(nn.Linear(2, dim), nn.Tanh()))\n        \n        self.MLP_layer = MLPReadout(out_dim, n_classes)\n\n    def forward(self, g, h, e, snorm_n, snorm_e):\n        h = self.embedding_h(h)\n        \n        # computing the \'pseudo\' named tensor which depends on node degrees\n        us, vs = g.edges()\n        # to avoid zero division in case in_degree is 0, we add constant \'1\' in all node degrees denoting self-loop\n        pseudo = [ [1/np.sqrt(g.in_degree(us[i])+1), 1/np.sqrt(g.in_degree(vs[i])+1)] for i in range(g.number_of_edges()) ]\n        pseudo = torch.Tensor(pseudo).to(self.device)\n        \n        for i in range(len(self.layers)):\n            h = self.layers[i](g, h, self.pseudo_proj[i](pseudo), snorm_n)\n        g.ndata[\'h\'] = h\n            \n        if self.readout == ""sum"":\n            hg = dgl.sum_nodes(g, \'h\')\n        elif self.readout == ""max"":\n            hg = dgl.max_nodes(g, \'h\')\n        elif self.readout == ""mean"":\n            hg = dgl.mean_nodes(g, \'h\')\n        else:\n            hg = dgl.mean_nodes(g, \'h\')  # default readout is mean nodes\n\n        return self.MLP_layer(hg)\n        \n    def loss(self, pred, label):\n        criterion = nn.CrossEntropyLoss()\n        loss = criterion(pred, label)\n        return loss'"
