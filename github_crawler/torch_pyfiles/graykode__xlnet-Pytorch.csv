file_path,api_count,code
data_utils.py,39,"b'# -*- coding: utf-8 -*-\n\n""""""\nCopyright 2019 Tae Hwan Jung\n\nLicensed under the Apache License, Version 2.0 (the ""License"");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an ""AS IS"" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport random\n\nimport torch\nimport numpy as np\n\nspecial_symbols = {\n    ""[UNK]""  : 0,\n    ""[CLS]""  : 1,\n    ""[SEP]""  : 2,\n    ""[PAD]""  : 3,\n    ""[MASK]"" : 4,\n}\nUNK_ID = special_symbols[""[UNK]""]\nCLS_ID = special_symbols[""[CLS]""]\nSEP_ID = special_symbols[""[SEP]""]\nMASK_ID = special_symbols[""[MASK]""]\n\ndef _split_a_and_b(data, sent_ids, begin_idx, tot_len, extend_target=False):\n    """"""Split two segments from `data` starting from the index `begin_idx`.""""""\n\n    data_len = data.shape[0]\n    if begin_idx + tot_len >= data_len:\n        print(""[_split_a_and_b] returns None: ""\n                ""begin_idx %d + tot_len %d >= data_len %d"",\n                begin_idx, tot_len, data_len)\n        return None\n\n    end_idx = begin_idx + 1\n    cut_points = []\n    while end_idx < data_len:\n        if sent_ids[end_idx] != sent_ids[end_idx - 1]:\n            if end_idx - begin_idx >= tot_len: break\n            cut_points.append(end_idx)\n        end_idx += 1\n\n    a_begin = begin_idx\n    if len(cut_points) == 0 or random.random() < 0.5:\n        # NotNext\n        label = 0\n        if len(cut_points) == 0:\n            a_end = end_idx\n        else:\n            a_end = random.choice(cut_points)\n\n        b_len = max(1, tot_len - (a_end - a_begin))\n        # (zihang): `data_len - 1` to account for extend_target\n        b_begin = random.randint(0, data_len - 1 - b_len)\n        b_end = b_begin + b_len\n        while b_begin > 0 and sent_ids[b_begin - 1] == sent_ids[b_begin]:\n            b_begin -= 1\n        # (zihang): `data_len - 1` to account for extend_target\n        while b_end < data_len - 1 and sent_ids[b_end - 1] == sent_ids[b_end]:\n            b_end += 1\n\n        new_begin = a_end\n    else:\n        # isNext\n        label = 1\n        a_end = random.choice(cut_points)\n        b_begin = a_end\n        b_end = end_idx\n\n        new_begin = b_end\n\n    while a_end - a_begin + b_end - b_begin > tot_len:\n        if a_end - a_begin > b_end - b_begin:\n            # delete the right side only for the LM objective\n            a_end -= 1\n        else:\n            b_end -= 1\n\n    ret = [data[a_begin: a_end], data[b_begin: b_end], label, new_begin]\n\n    if extend_target:\n        if a_end >= data_len or b_end >= data_len:\n            print(""[_split_a_and_b] returns None: ""\n                          ""a_end %d or b_end %d >= data_len %d"",\n                          a_end, b_end, data_len)\n            return None\n        a_target = data[a_begin + 1: a_end + 1]\n        b_target = data[b_begin: b_end + 1]\n        ret.extend([a_target, b_target])\n\n    return ret\n\ndef _is_start_piece(piece):\n    special_pieces = set(list(\'!""#$%&\\""()*+,-./:;?@[\\\\]^_`{|}~\'))\n    piece = \'\'.join(piece)\n    if (piece.startswith(""\xe2\x96\x81"") or piece.startswith(""<"")\n        or piece in special_pieces):\n        return True\n    else:\n        return False\n\ndef _sample_mask(sp, seg, mask_alpha, mask_beta,\n                 reverse=False, max_gram=5, goal_num_predict=None):\n    """"""Sample `goal_num_predict` tokens for partial prediction.\n    About `mask_beta` tokens are chosen in a context of `mask_alpha` tokens.""""""\n\n    seg_len = len(seg)\n    mask = np.array([False] * seg_len, dtype=np.bool)\n\n    num_predict = 0\n\n    ngrams = np.arange(1, max_gram + 1, dtype=np.int64)\n    pvals = 1. / np.arange(1, max_gram + 1)\n    pvals /= pvals.sum(keepdims=True)\n\n    if reverse:\n        seg = np.flip(seg, 0)\n\n    cur_len = 0\n    while cur_len < seg_len:\n        if goal_num_predict is not None and num_predict >= goal_num_predict: break\n\n        n = np.random.choice(ngrams, p=pvals)\n        if goal_num_predict is not None:\n            n = min(n, goal_num_predict - num_predict)\n        ctx_size = (n * mask_alpha) // mask_beta\n        l_ctx = np.random.choice(ctx_size)\n        r_ctx = ctx_size - l_ctx\n\n        # Find the start position of a complete token\n        beg = cur_len + l_ctx\n        while beg < seg_len and not _is_start_piece(sp.convert_ids_to_tokens([seg[beg].item()])):\n            beg += 1\n        if beg >= seg_len:\n            break\n\n        # Find the end position of the n-gram (start pos of the n+1-th gram)\n        end = beg + 1\n        cnt_ngram = 1\n        while end < seg_len:\n            if _is_start_piece(sp.convert_ids_to_tokens([seg[beg].item()])):\n                cnt_ngram += 1\n                if cnt_ngram > n:\n                    break\n            end += 1\n        if end >= seg_len:\n            break\n\n        # Update\n        mask[beg:end] = True\n        num_predict += end - beg\n\n        cur_len = end + r_ctx\n\n    while goal_num_predict is not None and num_predict < goal_num_predict:\n        i = np.random.randint(seg_len)\n        if not mask[i]:\n            mask[i] = True\n            num_predict += 1\n\n    if reverse:\n        mask = np.flip(mask, 0)\n\n    return mask\n\ndef _create_data(sp, input_paths, seq_len, reuse_len,\n                bi_data, num_predict, mask_alpha, mask_beta):\n    features = []\n\n    f = open(input_paths, \'r\')\n    lines = f.readlines()\n    input_data, sent_ids, sent_id = [], [], True\n\n    for line in lines:\n        tokens = sp.tokenize(line)\n        cur_sent = sp.convert_tokens_to_ids(tokens)\n        input_data.extend(cur_sent)\n        sent_ids.extend([sent_id] * len(cur_sent))\n        sent_id = not sent_id\n\n    # shape of data : [1, 582]\n    data = np.array([input_data], dtype=np.int64)\n    sent_ids = np.array([sent_ids], dtype=np.bool)\n\n    assert reuse_len < seq_len - 3\n\n    data_len = data.shape[1]\n    sep_array = np.array([SEP_ID], dtype=np.int64)\n    cls_array = np.array([CLS_ID], dtype=np.int64)\n\n    i = 0\n    while i + seq_len <= data_len:\n        inp = data[0, i: i + reuse_len]\n        tgt = data[0, i + 1: i + reuse_len + 1]\n\n        results = _split_a_and_b(\n            data[0], # all line in one Text file.\n            sent_ids[0],\n            begin_idx=i + reuse_len,\n            tot_len=seq_len - reuse_len - 3,\n            extend_target=True)\n\n        # unpack the results\n        (a_data, b_data, label, _, a_target, b_target) = tuple(results)\n\n        # sample ngram spans to predict\n        reverse = bi_data\n        if num_predict is None:\n            num_predict_0 = num_predict_1 = None\n        else:\n            num_predict_1 = num_predict // 2\n            num_predict_0 = num_predict - num_predict_1\n\n        mask_0 = _sample_mask(sp, inp, mask_alpha, mask_beta, reverse=reverse,\n                              goal_num_predict=num_predict_0)\n        mask_1 = _sample_mask(sp, np.concatenate([a_data, sep_array, b_data,\n                                                  sep_array, cls_array]),\n                              mask_alpha, mask_beta,\n                              reverse=reverse, goal_num_predict=num_predict_1)\n\n        # concatenate data\n        cat_data = np.concatenate([inp, a_data, sep_array, b_data,\n                                   sep_array, cls_array])\n        seg_id = ([0] * (reuse_len + a_data.shape[0]) + [0] +\n                  [1] * b_data.shape[0] + [1] + [2])\n        assert cat_data.shape[0] == seq_len\n        assert mask_0.shape[0] == seq_len // 2\n        assert mask_1.shape[0] == seq_len // 2\n\n        # the last two CLS\'s are not used, just for padding purposes\n        tgt = np.concatenate([tgt, a_target, b_target, cls_array, cls_array])\n        assert tgt.shape[0] == seq_len\n\n        is_masked = np.concatenate([mask_0, mask_1], 0)\n        if num_predict is not None:\n            assert np.sum(is_masked) == num_predict\n\n        feature = {\n            ""input"": cat_data,\n            ""is_masked"": is_masked,\n            ""target"": tgt,\n            ""seg_id"": seg_id,\n            ""label"": [label],\n        }\n        features.append(feature)\n\n        i += reuse_len\n\n    f.close()\n    return features\n\ndef _local_perm(inputs, targets, is_masked, perm_size, seq_len):\n    """"""\n    Sample a permutation of the factorization order, and create an\n    attention mask accordingly.\n\n    Args:\n    inputs: int64 Tensor in shape [seq_len], input ids.\n    targets: int64 Tensor in shape [seq_len], target ids.\n    is_masked: bool Tensor in shape [seq_len]. True means being selected\n      for partial prediction.\n    perm_size: the length of longest permutation. Could be set to be reuse_len.\n      Should not be larger than reuse_len or there will be data leaks.\n    seq_len: int, sequence length.\n    """"""\n\n    # Generate permutation indices\n    index = torch.arange(seq_len, dtype=torch.int64)\n\n    index = torch.reshape(index, [-1, perm_size]).t()\n    index = index[torch.randperm(index.shape[0])]\n    index = torch.reshape(index.t(), [-1])\n\n    # `perm_mask` and `target_mask`\n    # non-functional tokens\n    non_func_tokens = ~(torch.eq(inputs, SEP_ID) | torch.eq(inputs, CLS_ID))\n    non_mask_tokens = (~is_masked) & non_func_tokens\n    masked_or_func_tokens = ~non_mask_tokens\n\n    # Set the permutation indices of non-masked (& non-funcional) tokens to the\n    # smallest index (-1):\n    # (1) they can be seen by all other positions\n    # (2) they cannot see masked positions, so there won""t be information leak\n    smallest_index = -torch.ones([seq_len], dtype=torch.int64)\n\n    # put -1 if `non_mask_tokens(real token not cls or sep)` not permutation index\n    rev_index = torch.where(non_mask_tokens, smallest_index, index)\n\n    # Create `target_mask`: non-funcional and maksed tokens\n    # 1: use mask as input and have loss\n    # 0: use token (or [SEP], [CLS]) as input and do not have loss\n    target_tokens = masked_or_func_tokens & non_func_tokens\n    target_mask = target_tokens.type(torch.float32)\n\n    # Create `perm_mask`\n    # `target_tokens` cannot see themselves\n    # put `rev_index` if real mask(not cls or sep) else `rev_index + 1`\n    self_rev_index = torch.where(target_tokens, rev_index, rev_index + 1)\n\n    # 1: cannot attend if i <= j and j is not non-masked (masked_or_func_tokens)\n    # 0: can attend if i > j or j is non-masked\n    perm_mask = (self_rev_index[:, None] <= rev_index[None, :]) &  masked_or_func_tokens\n    perm_mask = perm_mask.type(torch.float32)\n\n    # new target: [next token] for LM and [curr token] (self) for PLM\n    new_targets = torch.cat([inputs[0: 1], targets[: -1]], dim=0)\n\n    # construct inputs_k\n    inputs_k = inputs\n\n    # construct inputs_q\n    inputs_q = target_mask\n\n    return perm_mask, new_targets, target_mask, inputs_k, inputs_q\n\ndef make_permute(feature, reuse_len, seq_len, perm_size, num_predict):\n\n    inputs = torch.LongTensor(feature.pop(""input""))\n    target = torch.LongTensor(feature.pop(""target""))\n    is_masked = torch.ByteTensor(feature.pop(""is_masked""))\n\n    non_reuse_len = seq_len - reuse_len\n    assert perm_size <= reuse_len and perm_size <= non_reuse_len\n\n    perm_mask_0, target_0, target_mask_0, input_k_0, input_q_0 = _local_perm(\n        inputs[:reuse_len], # inp\n        target[:reuse_len],\n        is_masked[:reuse_len],\n        perm_size,\n        reuse_len)\n\n    perm_mask_1, target_1, target_mask_1, input_k_1, input_q_1 = _local_perm(\n        inputs[reuse_len:], # (senA, seq, senBm seq, cls)\n        target[reuse_len:],\n        is_masked[reuse_len:],\n        perm_size,\n        non_reuse_len)\n\n    perm_mask_0 = torch.cat([perm_mask_0, torch.ones([reuse_len, non_reuse_len])],\n                            dim=1)\n    perm_mask_1 = torch.cat([torch.zeros([non_reuse_len, reuse_len]), perm_mask_1],\n                            dim=1)\n\n    perm_mask = torch.cat([perm_mask_0, perm_mask_1], dim=0)\n    target = torch.cat([target_0, target_1], dim=0)\n    target_mask = torch.cat([target_mask_0, target_mask_1], dim=0)\n    input_k = torch.cat([input_k_0, input_k_1], dim=0)\n    input_q = torch.cat([input_q_0, input_q_1], dim=0)\n\n    if num_predict is not None:\n        indices = torch.arange(seq_len, dtype=torch.int64)\n        bool_target_mask = target_mask.byte()\n        indices = indices[bool_target_mask]\n\n        ##### extra padding due to CLS/SEP introduced after prepro\n        actual_num_predict = indices.shape[0]\n        pad_len = num_predict - actual_num_predict\n\n        assert seq_len >= actual_num_predict\n\n        ##### target_mapping\n        target_mapping = torch.eye(seq_len, dtype=torch.float32)[indices]\n        paddings = torch.zeros([pad_len, seq_len], dtype=target_mapping.dtype)\n        target_mapping = torch.cat([target_mapping, paddings], dim=0)\n        feature[""target_mapping""] = torch.reshape(target_mapping,\n                                                [num_predict, seq_len])\n        ##### target\n        target = target[bool_target_mask]\n        paddings = torch.zeros([pad_len], dtype=target.dtype)\n        target = torch.cat([target, paddings], dim=0)\n        feature[""target""] = torch.reshape(target, [num_predict])\n\n        ##### target mask\n        target_mask = torch.cat(\n            [torch.ones([actual_num_predict], dtype=torch.float32),\n             torch.zeros([pad_len], dtype=torch.float32)],\n            dim=0)\n        feature[""target_mask""] = torch.reshape(target_mask, [num_predict])\n    else:\n        feature[""target""] = torch.reshape(target, [seq_len])\n        feature[""target_mask""] = torch.reshape(target_mask, [seq_len])\n\n    # reshape back to fixed shape\n    feature[""seg_id""] = torch.IntTensor(feature[""seg_id""])\n    feature[""perm_mask""] = torch.reshape(perm_mask, [seq_len, seq_len])\n    feature[""input_k""] = torch.reshape(input_k, [seq_len])\n    feature[""input_q""] = torch.reshape(input_q, [seq_len])\n\n    return feature'"
main.py,3,"b'# -*- coding: utf-8 -*-\n\n""""""\nCopyright 2019 Tae Hwan Jung\n\nLicensed under the Apache License, Version 2.0 (the ""License"");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an ""AS IS"" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport data_utils\nimport argparse\n\nimport xlnet\nimport torch\nimport numpy as np\nimport torch.nn as nn\nimport torch.optim as optim\nfrom pytorch_pretrained_bert import BertTokenizer\n\nif __name__ == ""__main__"":\n    parser = argparse.ArgumentParser(description=\'PyTorch XLNet Language Model\')\n    parser.add_argument(\'--data\', type=str, default=\'data.txt\')\n    parser.add_argument(\'--tokenizer\', type=str, default=\'bert-base-uncased\',\n                        help=\'Path to the sentence piece model from pytorch-pretrained-BERT\')\n    parser.add_argument(\'--seq_len\', type=int, default=512, help=""Sequence length."")\n    parser.add_argument(\'--reuse_len\', type=int, default=256,\n                        help=""Number of token that can be reused as memory. ""\n                             ""Could be half of `seq_len`."")\n    parser.add_argument(\'--perm_size\', type=int,\n                        default=256,\n                        help=""the length of longest permutation. Could be set to be reuse_len."")\n    parser.add_argument(\'--bi_data\', type=bool, default=False,\n                        help=""whether to create bidirectional data"")\n    parser.add_argument(\'--mask_alpha\', type=int,\n                        default=6, help=""How many tokens to form a group."")\n    parser.add_argument(\'--mask_beta\', type=int,\n                        default=1, help=""How many tokens to mask within each group."")\n    parser.add_argument(\'--num_predict\', type=int,\n                        default=85, help=""Num of tokens to predict."")\n    parser.add_argument(\'--mem_len\', type=int,\n                        default=384, help=""Number of steps to cache"")\n    parser.add_argument(\'--num_epoch\', type=int,\n                        default=100, help=""Number of epochs"")\n\n    args = parser.parse_args()\n\n    sp = BertTokenizer.from_pretrained(args.tokenizer)\n    model = xlnet.XLNet(n_token=len(sp.vocab), n_layer=6, n_head=4, d_head=8,\n                        d_inner=32, d_model=32,\n                        dropout=0.1, dropatt=0.1,\n                        attn_type=""bi"", bi_data=args.bi_data,\n                        clamp_len=-1, same_length=False,\n                        reuse_len=args.reuse_len, mem_len=args.mem_len)\n\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=0.01)\n\n    for num_epoch in range(args.num_epoch):\n        mems = None\n\n        features = data_utils._create_data(sp=sp,\n                                           input_paths=args.data,\n                                           seq_len=args.seq_len,\n                                           reuse_len=args.reuse_len,\n                                           bi_data=args.bi_data,\n                                           num_predict=args.num_predict,\n                                           mask_alpha=args.mask_alpha,\n                                           mask_beta=args.mask_beta)\n\n        num_step = 0\n        for feature in features:\n            permutation = data_utils.make_permute(feature,\n                                                  reuse_len=args.reuse_len,\n                                                  seq_len=args.seq_len,\n                                                  perm_size=args.perm_size,\n                                                  num_predict=args.num_predict)\n\n            # batch size is 1\n            inp_k = permutation[\'input_k\'].unsqueeze(-1) # [seq_len, 1(=bsz)]\n            seg_id = permutation[\'seg_id\'].unsqueeze(-1) # [seq_len, 1(=bsz)]\n            target = permutation[\'target\'].unsqueeze(-1) # [num_predict, 1(=bsz)]\n            perm_mask = permutation[\'perm_mask\'].unsqueeze(-1) # [seq_len, seq_len, 1(=bsz)]\n            target_mapping = \\\n                permutation[\'target_mapping\'].unsqueeze(-1) # [num_predict, seq_len, 1(=bsz)]\n            inp_q = permutation[\'input_q\'].unsqueeze(-1) # [seq_len, 1(=bsz)]\n            tgt_mask = permutation[\'target_mask\'].unsqueeze(-1) # [num_predict, 1(=bsz)]\n\n            logits, new_mems = model(inp_k=inp_k, seg_id=seg_id, input_mask=None,\n                  mems=mems, perm_mask=perm_mask,\n                  target_mapping=target_mapping, inp_q=inp_q)\n\n            lm_loss = criterion(logits.transpose(1, 2), target).type(torch.float32)\n            tgt_mask_sum = tgt_mask.reshape(-1).sum()\n            lm_loss_sum = (lm_loss * tgt_mask).reshape(-1).sum()\n\n            optimizer.zero_grad()\n            total_loss = lm_loss_sum / tgt_mask_sum\n            print(\'Number of Epoch: %04d in %04d Step\' % ((num_epoch + 1), (num_step + 1)),\n                  \'cost =\', \'{:.6f}\'.format(total_loss))\n            num_step += 1\n\n            total_loss.backward()\n            optimizer.step()\n\n            mems = new_mems'"
xlnet.py,65,"b'# -*- coding: utf-8 -*-\n\n""""""\nCopyright 2019 Tae Hwan Jung\n\nLicensed under the Apache License, Version 2.0 (the ""License"");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an ""AS IS"" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass XLNet(nn.Module):\n    """"""\n        Defines a Transformer-XL computation graph with additional\n        support for XLNet.\n\n        Args:\n\n        inp_k: int32 Tensor in shape [len, bsz], the input token IDs.\n        seg_id: int32 Tensor in shape [len, bsz], the input segment IDs.\n        input_mask: float32 Tensor in shape [len, bsz], the input mask.\n          0 for real tokens and 1 for padding.\n        mems: a list of float32 Tensors in shape [mem_len, bsz, d_model], memory\n          from previous batches. The length of the list equals n_layer.\n          If None, no memory is used.\n        perm_mask: float32 Tensor in shape [len, len, bsz].\n          If perm_mask[i, j, k] = 0, i attend to j in batch k;\n          if perm_mask[i, j, k] = 1, i does not attend to j in batch k.\n          If None, each position attends to all the others.\n        target_mapping: float32 Tensor in shape [num_predict, len, bsz].\n          If target_mapping[i, j, k] = 1, the i-th predict in batch k is\n          on the j-th token.\n          Only used during pretraining for partial prediction.\n          Set to None during finetuning.\n        inp_q: float32 Tensor in shape [len, bsz].\n          1 for tokens with losses and 0 for tokens without losses.\n          Only used during pretraining for two-stream attention.\n          Set to None during finetuning.\n\n        n_layer: int, the number of layers.\n        d_model: int, the hidden size.\n        n_head: int, the number of attention heads.\n        d_head: int, the dimension size of each attention head.\n        d_inner: int, the hidden size in feed-forward layers.\n        ff_activation: str, ""relu"" or ""gelu"".\n        n_token: int, the vocab size.\n\n        dropout: float, dropout rate.\n        dropatt: float, dropout rate on attention probabilities.\n\n        mem_len: int, the number of tokens to cache.\n        reuse_len: int, the number of tokens in the currect batch to be cached\n          and reused in the future.\n        bi_data: bool, whether to use bidirectional input pipeline.\n          Usually set to True during pretraining and False during finetuning.\n        clamp_len: int, clamp all relative distances larger than clamp_len.\n          -1 means no clamping.\n\n      """"""\n    def __init__(self, n_token, n_layer, n_head, d_head, d_inner, d_model, dropout, dropatt,\n                 attn_type, bi_data, clamp_len, same_length, reuse_len, mem_len):\n        super(XLNet, self).__init__()\n\n        self.n_token = n_token\n        self.n_layer = n_layer\n        self.n_head = n_head\n        self.d_head = d_head\n        self.d_inner = d_inner\n        self.d_model = d_model\n        self.dropout = dropout\n        self.dropatt = dropatt\n        self.attn_type = attn_type\n        self.bi_data = bi_data\n        self.clamp_len = clamp_len\n        self.same_length = same_length\n        self.reuse_len = reuse_len\n        self.mem_len = mem_len\n\n        self.embedding = nn.Embedding(n_token, d_model)\n        self.Dropout = nn.Dropout(p=dropout)\n        self.DropAttn = nn.Dropout(p=dropatt)\n\n        self.r_w_bias = nn.Parameter(torch.randn(self.n_layer,\n                                                  self.n_head,self.d_head))\n        self.r_r_bias = nn.Parameter(torch.randn(self.n_layer,\n                                                  self.n_head, self.d_head))\n\n        ##### Segment embedding\n        self.r_s_bias = nn.Parameter(torch.randn(self.n_layer,\n                                                  self.n_head,self.d_head))\n\n        self.seg_embed = nn.Parameter(torch.randn(self.n_layer, 2,\n                                                   self.n_head, self.d_head))\n\n        self.mask_emb = nn.Parameter(torch.randn(1, 1, d_model))\n\n        # post-attention projection (back to `d_model`)\n        self.proj_o = nn.Parameter(torch.randn(self.d_model,\n                                                self.n_head, self.d_head))\n\n        #### Project hidden states to a specific head with a 4D-shape.\n        self.q_proj_weight = nn.Parameter(torch.randn(self.d_model,\n                                                       self.n_head, self.d_head))\n        self.k_proj_weight = nn.Parameter(torch.randn(self.d_model,\n                                                       self.n_head, self.d_head))\n        self.v_proj_weight = nn.Parameter(torch.randn(self.d_model,\n                                                       self.n_head, self.d_head))\n        self.r_proj_weight = nn.Parameter(torch.randn(self.d_model,\n                                                       self.n_head, self.d_head))\n\n        self.layer_norm = nn.LayerNorm(d_model)\n\n        self.conv1 = nn.Linear(d_model, d_inner)\n        self.conv2 = nn.Linear(d_inner, d_model)\n        self.relu = nn.ReLU(inplace=True)\n\n        self.softmax_b = nn.Parameter(torch.zeros(self.n_token))\n\n\n    def gelu(self, x):\n        """"""Gaussian Error Linear Unit.\n\n        This is a smoother version of the RELU.\n        Original paper: https://arxiv.org/abs/1606.08415\n        Args:\n          x: float Tensor to perform activation.\n\n        Returns:\n          `x` with the GELU activation applied.\n        """"""\n        cdf = 0.5 * (1.0 + torch.tanh(\n            (np.sqrt(2 / np.pi) * (x + 0.044715 * torch.pow(x, 3)))))\n        return x * cdf\n\n    def rel_shift(self, x, klen=-1):\n        """"""perform relative shift to form the relative attention score.""""""\n        x_size = x.shape\n\n        x = torch.reshape(x, [x_size[1], x_size[0], x_size[2], x_size[3]])\n        x = x[1:, 0:, 0:, 0:] # tf.slice(x, [1, 0, 0, 0], [-1, -1, -1, -1])\n        x = torch.reshape(x, [x_size[0], x_size[1] - 1, x_size[2], x_size[3]])\n        x = x[0:, 0:klen, 0:, 0:] # tf.slice(x, [0, 0, 0, 0], [-1, klen, -1, -1])\n\n        return x\n\n    def positionwise_ffn(self, inp, activation_type=\'relu\'):\n\n        """"""Position-wise Feed-forward Network.""""""\n        output = self.conv1(inp)\n        output = self.Dropout(output)\n        if activation_type == \'relu\':\n            output = self.relu(output)\n        elif activation_type == \'gelu\':\n            output = self.gelu(output)\n        else:\n            raise ValueError(\'Unsupported activation type {}\'.format(activation_type))\n\n        output = self.layer_norm(output + inp)\n        return output\n\n    def post_attention(self, h, attn_vec, residual=True):\n        """"""Post-attention processing.""""""\n\n        # post-attention projection (back to `d_model`)\n        attn_out = torch.einsum(\'ibnd,hnd->ibh\', attn_vec, self.proj_o)\n\n        attn_out = self.Dropout(attn_out)\n        if residual:\n            output = self.layer_norm(attn_out + h)\n        else:\n            output = self.layer_norm(attn_out)\n\n        return output\n\n    def head_projection(self, h, name):\n        """"""Project hidden states to a specific head with a 4D-shape.""""""\n        proj_weight = None\n        if name == \'q\':\n            proj_weight = self.q_proj_weight\n        elif name == \'k\':\n            proj_weight = self.k_proj_weight\n        elif name ==\'v\':\n            proj_weight = self.v_proj_weight\n        elif name == \'r\':\n            proj_weight = self.r_proj_weight\n        else:\n            raise ValueError(\'Unknown `name` {}.\'.format(name))\n\n        head = torch.einsum(\'ibh,hnd->ibnd\', h, proj_weight)\n\n        return head\n\n    def rel_attn_core(self, q_head, k_head_h, v_head_h, k_head_r, seg_embed, seg_mat,\n                      r_w_bias, r_r_bias, r_s_bias, attn_mask, scale):\n\n        """"""Core relative positional attention operations.""""""\n\n        # content based attention score\n        ac = torch.einsum(\'ibnd,jbnd->ijbn\', q_head + r_w_bias, k_head_h)\n\n        # position based attention score\n        bd = torch.einsum(\'ibnd,jbnd->ijbn\', q_head + r_r_bias, k_head_r)\n        bd = self.rel_shift(bd, klen=ac.shape[1])\n\n        # segment based attention score\n        if seg_mat is None:\n            ef = 0\n        else:\n            ef = torch.einsum(\'ibnd,snd->ibns\', q_head + r_s_bias, seg_embed)\n            ef = torch.einsum(\'ijbs,ibns->ijbn\', seg_mat, ef)\n\n        # merge attention scores and perform masking\n        attn_score = (ac + bd + ef) * scale\n        if attn_mask is not None:\n            # attn_score = attn_score * (1 - attn_mask) - 1e30 * attn_mask\n            attn_score = attn_score - 1e30 * attn_mask\n\n        # attention probability\n        attn_prob = F.softmax(attn_score, dim=1)\n        attn_prob = self.DropAttn(attn_prob)\n\n        # attention output\n        attn_vec = torch.einsum(\'ijbn,jbnd->ibnd\', attn_prob, v_head_h)\n\n        return attn_vec\n\n    def rel_multihead_attn(self, h, r, r_w_bias, r_r_bias, seg_mat, r_s_bias, seg_embed,\n                           attn_mask, mems, d_model, n_head, d_head, dropout, dropatt):\n        """"""Multi-head attention with relative positional encoding.""""""\n\n        scale = 1 / (d_head ** 0.5)\n        if mems is not None and len(mems.size()) > 1:\n            cat = torch.cat([mems, h], dim=0)\n        else:\n            cat = h\n\n        # content heads\n        q_head_h = self.head_projection(h, \'q\')\n        k_head_h = self.head_projection(cat, \'k\')\n        v_head_h = self.head_projection(cat, \'v\')\n\n        # positional heads\n        k_head_r = self.head_projection(r, \'r\')\n\n        # core attention ops\n        attn_vec = self.rel_attn_core(\n            q_head_h, k_head_h, v_head_h, k_head_r, seg_embed, seg_mat, r_w_bias,\n            r_r_bias, r_s_bias, attn_mask, scale)\n\n        # post processing\n        output = self.post_attention(h, attn_vec)\n\n        return output\n\n    def two_stream_rel_attn(self, h, g, r, mems, r_w_bias, r_r_bias, seg_mat, r_s_bias,\n                            seg_embed, attn_mask_h, attn_mask_g, target_mapping):\n        scale = 1 / (self.d_head ** 0.5)\n\n        # content based attention score\n        if mems is not None and len(mems.size()) > 1:\n            cat = torch.cat([mems, h], dim=0)\n        else:\n            cat = h\n\n        # content-based key head\n        k_head_h = self.head_projection(cat, \'k\')\n\n        # content-based value head\n        v_head_h = self.head_projection(cat, \'v\')\n\n        # position-based key head\n        k_head_r = self.head_projection(r, \'r\')\n\n        ##### h-stream\n        # content-stream query head\n        q_head_h = self.head_projection(h, \'q\')\n\n        # core attention ops\n        # h\xcb\x86(m)_zt = LayerNorm\x10(h^(m-1)_zt + RelAttn(h^(m-1)_zt + [h~^(m-1), hT(m-1)_z<=t]))\n        attn_vec_h = self.rel_attn_core(\n            q_head_h, k_head_h, v_head_h, k_head_r, seg_embed, seg_mat, r_w_bias,\n            r_r_bias, r_s_bias, attn_mask_h, scale)\n\n        # post processing\n        output_h = self.post_attention(h, attn_vec_h)\n\n        ##### g-stream\n        # query-stream query head\n        q_head_g = self.head_projection(g, \'q\')\n\n        # core attention ops\n        # g\xcb\x86(m)_zt = LayerNorm\x10(g^(m-1)_zt + RelAttn(g^(m-1)_zt + [h~^(m-1), hT(m-1)_z<=t]))\n        if target_mapping is not None:\n            q_head_g = torch.einsum(\'mbnd,mlb->lbnd\', q_head_g, target_mapping)\n            attn_vec_g = self.rel_attn_core(\n                q_head_g, k_head_h, v_head_h, k_head_r, seg_embed, seg_mat, r_w_bias,\n                r_r_bias, r_s_bias, attn_mask_g, scale)\n            attn_vec_g = torch.einsum(\'lbnd,mlb->mbnd\', attn_vec_g, target_mapping)\n        else:\n            attn_vec_g = self.rel_attn_core(\n                q_head_g, k_head_h, v_head_h, k_head_r, seg_embed, seg_mat, r_w_bias,\n                r_r_bias, r_s_bias, attn_mask_g, scale)\n\n        # post processing\n        output_g = self.post_attention(g, attn_vec_g)\n\n        return output_h, output_g\n\n\n    def _create_mask(self, qlen, mlen, dtype, same_length=False):\n        """"""create causal attention mask.""""""\n        # [[0,1,1],\n        #  [0,0,1],\n        #  [0,0,0]]\n        attn_mask = torch.ones([qlen, qlen], dtype=dtype)\n        mask_u = torch.triu(attn_mask) # Upper triangular part.\n        mask_dia = torch.tril(attn_mask) & torch.triu(attn_mask) # Diagonal. Figure 2(c)\n        attn_mask_pad = torch.zeros([qlen, mlen], dtype=dtype)\n        ret = torch.cat([attn_mask_pad, mask_u - mask_dia], dim=1) # [qlen, mlen]\n        if same_length:\n            # [[0,1,1],\n            #  [1,0,1],\n            #  [1,1,0]]\n            mask_l = torch.tril(attn_mask) # Lower triangular part.\n            ret = torch.cat([ret[:, :qlen] + mask_l - mask_dia, ret[:, qlen:]], dim=1)\n\n        return ret.type(dtype=torch.float32) # [qlen, qlen]\n\n    def positional_embedding(self, pos_seq, inv_freq):\n        sinusoid_inp = torch.einsum(\'i,d->id\', pos_seq, inv_freq)\n        pos_emb = torch.cat([torch.sin(sinusoid_inp), torch.cos(sinusoid_inp)], dim=-1)\n        pos_emb = pos_emb[:, None, :]\n\n        return pos_emb\n\n    def _cache_mem(self, curr_out, prev_mem, mem_len, reuse_len=None):\n        """"""cache hidden states into memory.""""""\n\n        with torch.no_grad():\n            if mem_len is None or mem_len == 0:\n                return None\n            else:\n                if reuse_len is not None and reuse_len > 0:\n                    curr_out = curr_out[:reuse_len]\n\n                if prev_mem is None:\n                    new_mem = curr_out[-mem_len:]\n                else:\n                    new_mem = torch.cat([prev_mem, curr_out], dim=0)[-mem_len:]\n\n            return new_mem\n\n\n    def relative_positional_encoding(self, qlen, klen, d_model, clamp_len, attn_type,\n                                     bi_data, bsz=None, dtype=None):\n        """"""create relative positional encoding.""""""\n\n        freq_seq = torch.arange(0, d_model, 2.0)\n        if dtype is not None and dtype != torch.float32:\n            freq_seq = freq_seq.type(dtype)\n        inv_freq = 1 / (10000 ** (freq_seq / d_model))\n\n        if attn_type == \'bi\':\n            # beg, end = klen - 1, -qlen\n            beg, end = klen, -qlen\n        elif attn_type == \'uni\':\n            # beg, end = klen - 1, -1\n            beg, end = klen, -1\n        else:\n            raise ValueError(\'Unknown `attn_type` {}.\'.format(attn_type))\n\n        if bi_data and bsz%2 is 0:\n            fwd_pos_seq = torch.arange(beg, end, -1.0)\n            bwd_pos_seq = torch.arange(-beg, -end, 1.0)\n\n            if dtype is not None and dtype != torch.float32:\n                fwd_pos_seq = fwd_pos_seq.type(dtype=dtype)\n                bwd_pos_seq = bwd_pos_seq.type(dtype=dtype)\n\n            if clamp_len > 0:\n                fwd_pos_seq = torch.clamp(fwd_pos_seq, -clamp_len, clamp_len)\n                bwd_pos_seq = torch.clamp(bwd_pos_seq, -clamp_len, clamp_len)\n\n            fwd_pos_emb = self.positional_embedding(fwd_pos_seq, inv_freq)\n            bwd_pos_emb = self.positional_embedding(bwd_pos_seq, inv_freq)\n\n            pos_emb = torch.cat([fwd_pos_emb, bwd_pos_emb], dim=1)\n        else:\n            fwd_pos_seq = torch.arange(beg, end, -1.0)\n            if dtype is not None and dtype != torch.float32:\n                fwd_pos_seq = fwd_pos_seq.type(dtype=dtype)\n            if clamp_len > 0:\n                fwd_pos_seq = torch.clamp(fwd_pos_seq, -clamp_len, clamp_len)\n            pos_emb = self.positional_embedding(fwd_pos_seq, inv_freq)\n\n        return pos_emb\n\n    def forward(self, inp_k, seg_id, input_mask, mems, perm_mask, target_mapping, inp_q):\n        new_mems = []\n\n        bsz = inp_k.shape[1]\n        qlen = inp_k.shape[0]\n        mlen = mems[0].size(0) if mems is not None else 0\n        klen = mlen + qlen\n\n        ##### Attention mask\n        # causal attention mask\n        if self.attn_type == \'uni\':\n            attn_mask = self._create_mask(qlen, mlen, torch.int64, self.same_length)\n            attn_mask = attn_mask[:, :, None, None]\n        elif self.attn_type == \'bi\':\n            attn_mask = None\n        else:\n            raise ValueError(\'Unsupported attention type: {}\'.format(self.attn_type))\n\n        # data mask: input mask & perm mask\n        if input_mask is not None and perm_mask is not None:\n            data_mask = input_mask[None] + perm_mask\n        elif input_mask is not None and perm_mask is None:\n            data_mask = input_mask[None]\n        elif input_mask is None and perm_mask is not None:\n            data_mask = perm_mask\n        else:\n            data_mask = None\n\n        if data_mask is not None:\n            # all mems can be attended to\n            mems_mask = torch.zeros([data_mask.shape[0], mlen, bsz],\n                                 dtype=torch.float32)\n            data_mask = torch.cat([mems_mask, data_mask], dim=1)\n            if attn_mask is None:\n                attn_mask = data_mask[:, :, :, None]\n            else:\n                attn_mask += data_mask[:, :, :, None]\n\n        if attn_mask is not None:\n            attn_mask = attn_mask.gt(0).type(torch.float32)\n\n        if attn_mask is not None:\n            non_tgt_mask = -torch.eye(qlen, dtype=torch.float32) # [qlen, qlen]\n            non_tgt_mask = torch.cat([torch.zeros([qlen, mlen], dtype=torch.float32), # [qlen, klen]\n                                        non_tgt_mask],\n                                        dim=-1)\n            non_tgt_mask = (attn_mask +\n                            non_tgt_mask[:, :, None, None]).gt(0).type(dtype=torch.float32)\n        else:\n            non_tgt_mask = None\n\n        ##### Word embedding\n        lookup_table = self.embedding\n        word_emb_k = lookup_table(inp_k)\n\n        if inp_q is not None:\n            if target_mapping is not None:\n                word_emb_q = self.mask_emb.repeat(target_mapping.shape[0], bsz, 1)\n            else:\n                inp_q_ext = inp_q[:, :, None]\n                word_emb_q = inp_q_ext * self.mask_emb + (1 - inp_q_ext) * word_emb_k\n\n        #### Figure 2(a), Content Stream(Original Attention), h^(0)_t = e(x_i) = e(inp_k)\n        output_h = self.Dropout(word_emb_k)\n        if inp_q is not None:\n            #### Query Stream, g^(0)_t = w\n            #### the first layer query stream is initialized with a trainable vector\n            output_g = self.Dropout(word_emb_q)\n\n        ##### Segment embedding\n        # paper\n        # Given a pair of positions i and j in the sequence, if\n        # i and j are from the same segment\n        if seg_id is not None:\n            # Convert `seg_id` to one-hot `seg_mat`\n            mem_pad = torch.zeros([mlen, bsz], dtype=torch.int32)\n            cat_ids = torch.cat([mem_pad, seg_id], dim=0)\n\n            # `1` indicates not in the same segment [qlen x klen x bsz]\n            seg_mat = (~torch.eq(seg_id[:, None], cat_ids[None, :])).type(torch.long)\n            seg_mat = torch.eye(2, dtype=torch.float32)[seg_mat]\n        else:\n            seg_mat = None\n\n        ##### Positional encoding\n        pos_emb = self.relative_positional_encoding(\n            qlen, klen, self.d_model, self.clamp_len, self.attn_type, self.bi_data,\n            bsz=bsz, dtype=torch.float32)\n        pos_emb = self.Dropout(pos_emb)\n\n        ##### Attention layers\n        if mems is None:\n            mems = [None] * self.n_layer\n\n        for i in range(self.n_layer):\n            # cache new mems\n            new_mems.append(self._cache_mem(output_h, mems[i], self.mem_len, self.reuse_len))\n\n            # segment bias\n            if seg_id is None:\n                r_s_bias_i = None\n                seg_embed_i = None\n            else:\n                r_s_bias_i = self.r_s_bias[i]\n                seg_embed_i = self.seg_embed[i]\n\n            if inp_q is not None:\n                output_h, output_g = self.two_stream_rel_attn(\n                    h=output_h,\n                    g=output_g,\n                    r=pos_emb,\n                    r_w_bias= self.r_w_bias[i],\n                    r_r_bias= self.r_r_bias[i],\n                    seg_mat=seg_mat,\n                    r_s_bias=r_s_bias_i,\n                    seg_embed=seg_embed_i,\n                    attn_mask_h=non_tgt_mask,\n                    attn_mask_g=attn_mask,\n                    mems=mems[i],\n                    target_mapping=target_mapping)\n            else:\n                output_h = self.rel_multihead_attn(\n                    h=output_h,\n                    r=pos_emb,\n                    r_w_bias=self.r_w_bias[i],\n                    r_r_bias=self.r_r_bias[i],\n                    seg_mat=seg_mat,\n                    r_s_bias=r_s_bias_i,\n                    seg_embed=seg_embed_i,\n                    attn_mask=non_tgt_mask,\n                    mems=mems[i])\n\n            if inp_q is not None:\n                output_g = self.positionwise_ffn(inp=output_g)\n\n            output_h = self.positionwise_ffn(inp=output_h)\n\n        if inp_q is not None:\n            output = self.Dropout(output_g)\n        else:\n            output = self.Dropout(output_h)\n\n        logits = torch.einsum(\'ibd,nd->ibn\', output, lookup_table.weight) + self.softmax_b\n\n        return logits, new_mems'"
