file_path,api_count,code
combine_rgb_flow.py,0,"b""#!/usr/bin/env python\n#\n# Script for combining the submission files for the RGB and Flow networks\n#\n# Contributor: Gunnar Atli Sigurdsson\n\nimport numpy as np\nimport sys\nimport pdb\nfrom itertools import groupby\n\nrgbfile = sys.argv[1]\nflowfile = sys.argv[2]\nw = [0.5,0.5]\nnclasses = 157\n\ndef loadfile(path):\n    with open(path) as f:\n        lines = [x.strip().split(' ') for x in f.readlines()]\n    localization = len(lines[0]) == nclasses+2\n    if localization:\n        data = [(x[0]+' '+x[1],np.array([float(y) for y in x[2:]])) for x in lines]\n    else:\n        data = [(x[0],np.array([float(y) for y in x[1:]])) for x in lines]\n    return data\n\nrgb = loadfile(rgbfile)\nflow = loadfile(flowfile)\n\nrgbdict = dict(rgb)\nflowdict = dict(flow)\n\nkeys = list(set(rgbdict.keys()+flowdict.keys()))\nw = [x/sum(w) for x in w]\n\ndef normme(x):\n    x = x-np.mean(x)\n    x = x/(0.00001+np.std(x))\n    return x\n\nN = 157\ndef lookup(d,key):\n    if key in d:\n        return d[key]\n    else:\n        sys.stderr.write('error ' + key + '\\n')\n        return np.zeros((nclasses,))\n\nfor id0 in keys:\n    r = lookup(rgbdict,id0)\n    f = lookup(flowdict,id0)\n    out = r*w[0]+f*w[1] #unnormalized combination\n    #out = normme(r)*w[0]+normme(f)*w[1] #normalize first\n    #out = np.exp(np.log(r)*w[0]+np.log(f)*w[1]) #weighted geometric mean\n    out = [str(x) for x in out]\n    print('{} {}'.format(id0,' '.join(out)))\n"""
pytorch/checkpoints.py,2,"b'"""""" Defines functions used for checkpointing models and storing model scores """"""\nimport os\nimport torch\nimport shutil\nfrom collections import OrderedDict\n\n\ndef ordered_load_state(model, chkpoint):\n    """""" \n        Wrapping the model with parallel/dataparallel seems to\n        change the variable names for the states\n        This attempts to load normally and otherwise aligns the labels\n        of the two statese and tries again.\n    """"""\n    try:\n        model.load_state_dict(chkpoint)\n    except Exception:  # assume order is the same, and use new labels\n        print(\'keys do not match model, trying to align\')\n        modelkeys = model.state_dict().keys()\n        fixed = OrderedDict([(z,y) \n                             for (x,y),z in zip(chkpoint.items(), modelkeys)])\n        model.load_state_dict(fixed)\n\n\ndef load(args, model, optimizer):\n    if args.resume:\n        if os.path.isfile(args.resume):\n            print(""=> loading checkpoint \'{}\'"".format(args.resume))\n            chkpoint = torch.load(args.resume)\n            if isinstance(chkpoint, dict) and \'state_dict\' in chkpoint:\n                args.start_epoch = chkpoint[\'epoch\']\n                mAP = chkpoint[\'mAP\']\n                ordered_load_state(model, chkpoint[\'state_dict\'])\n                optimizer.load_state_dict(chkpoint[\'optimizer\'])\n                print(""=> loaded checkpoint \'{}\' (epoch {})""\n                      .format(args.resume, chkpoint[\'epoch\']))\n                return mAP\n            else:\n                ordered_load_state(model, chkpoint)\n                print(""=> loaded checkpoint \'{}\' (just weights)""\n                      .format(args.resume))\n                return 0\n        else:\n            raise ValueError(""no checkpoint found at \'{}\'"".format(args.resume))\n    return 0\n\n\ndef score_file(scores, filename):\n    with open(filename, \'w\') as f:\n        for key, val in sorted(scores.items()):\n            f.write(\'{} {}\\n\'.format(key, val))\n\n\ndef save(epoch, args, model, optimizer, is_best, scores):\n    state = {\n        \'epoch\': epoch + 1,\n        \'arch\': args.arch,\n        \'state_dict\': model.state_dict(),\n        \'mAP\': scores[\'mAP\'],\n        \'optimizer\': optimizer.state_dict(),\n    }\n    filename = ""{}/model.pth.tar"".format(args.cache)\n    score_file(scores, ""{}/model_{:03d}.txt"".format(args.cache, epoch+1))\n    torch.save(state, filename)\n    if is_best:\n        bestname = ""{}/model_best.pth.tar"".format(args.cache)\n        score_file(scores, ""{}/model_best.txt"".format(args.cache, epoch+1))\n        shutil.copyfile(filename, bestname)\n'"
pytorch/main.py,2,"b'#!/usr/bin/env python\n\n""""""Charades activity recognition baseline code\n   Can be run directly or throught config scripts under exp/\n\n   Gunnar Sigurdsson, 2018\n"""""" \nimport torch\nimport numpy as np\nimport random\nimport train\nfrom models import create_model\nfrom datasets import get_dataset\nimport checkpoints\nfrom opts import parse\nfrom utils import tee\n\n\ndef seed(manualseed):\n    random.seed(manualseed)\n    np.random.seed(manualseed)\n    torch.manual_seed(manualseed)\n    torch.cuda.manual_seed(manualseed)\n\n\nbest_mAP = 0\ndef main():\n    global opt, best_mAP\n    opt = parse()\n    tee.Tee(opt.cache+\'/log.txt\')\n    print(vars(opt))\n    seed(opt.manual_seed)\n\n    model, criterion, optimizer = create_model(opt)\n    if opt.resume: best_mAP = checkpoints.load(opt, model, optimizer)\n    print(model)\n    trainer = train.Trainer()\n    train_loader, val_loader, valvideo_loader = get_dataset(opt)\n\n    if opt.evaluate:\n        trainer.validate(val_loader, model, criterion, -1, opt)\n        trainer.validate_video(valvideo_loader, model, -1, opt)\n        return\n\n    for epoch in range(opt.start_epoch, opt.epochs):\n        if opt.distributed:\n            trainer.train_sampler.set_epoch(epoch)\n        top1,top5 = trainer.train(train_loader, model, criterion, optimizer, epoch, opt)\n        top1val,top5val = trainer.validate(val_loader, model, criterion, epoch, opt)\n        mAP = trainer.validate_video(valvideo_loader, model, epoch, opt)\n        is_best = mAP > best_mAP\n        best_mAP = max(mAP, best_mAP)\n        scores = {\'top1train\':top1,\'top5train\':top5,\'top1val\':top1val,\'top5val\':top5val,\'mAP\':mAP}\n        checkpoints.save(epoch, opt, model, optimizer, is_best, scores)\n\n\nif __name__ == \'__main__\':\n    main()\n'"
pytorch/opts.py,0,"b'"""""" Define and parse commandline arguments """"""\nimport argparse\nimport os\n\n\ndef parse():\n    print(\'parsing arguments\')\n    parser = argparse.ArgumentParser(description=\'PyTorch Charades Training\')\n    parser.add_argument(\'--data\', metavar=\'DIR\', default=\'/scratch/gsigurds/Charades_v1_rgb/\',\n                        help=\'path to dataset\')\n    parser.add_argument(\'--dataset\', metavar=\'DIR\', default=\'fake\',\n                        help=\'name of dataset under datasets/\')\n    parser.add_argument(\'--train-file\', default=\'./Charades_v1_train.csv\', type=str)\n    parser.add_argument(\'--val-file\', default=\'./Charades_v1_test.csv\', type=str)\n    parser.add_argument(\'--arch\', \'-a\', metavar=\'ARCH\', default=\'alexnet\',\n                        help=\'model architecture: \')\n    parser.add_argument(\'-j\', \'--workers\', default=4, type=int, metavar=\'N\',\n                        help=\'number of data loading workers (default: 4)\')\n    parser.add_argument(\'--epochs\', default=20, type=int, metavar=\'N\',\n                        help=\'number of total epochs to run\')\n    parser.add_argument(\'--start-epoch\', default=0, type=int, metavar=\'N\',\n                        help=\'manual epoch number (useful on restarts)\')\n    parser.add_argument(\'-b\', \'--batch-size\', default=256, type=int,\n                        metavar=\'N\', help=\'mini-batch size (default: 256)\')\n    parser.add_argument(\'--lr\', \'--learning-rate\', default=1e-3, type=float,\n                        metavar=\'LR\', help=\'initial learning rate\')\n    parser.add_argument(\'--lr-decay-rate\',default=6, type=int)\n    parser.add_argument(\'--momentum\', default=0.9, type=float, metavar=\'M\',\n                        help=\'momentum\')\n    parser.add_argument(\'--weight-decay\', \'--wd\', default=1e-4, type=float,\n                        metavar=\'W\', help=\'weight decay (default: 1e-4)\')\n    parser.add_argument(\'--print-freq\', \'-p\', default=10, type=int,\n                        metavar=\'N\', help=\'print frequency (default: 10)\')\n    parser.add_argument(\'--resume\', default=\'\', type=str, metavar=\'PATH\',\n                        help=\'path to latest checkpoint (default: none)\')\n    parser.add_argument(\'-e\', \'--evaluate\', dest=\'evaluate\', action=\'store_true\',\n                        help=\'evaluate model on validation set\')\n    parser.add_argument(\'--pretrained\', dest=\'pretrained\', action=\'store_true\',\n                        help=\'use pre-trained model\')\n    parser.add_argument(\'--pretrained-weights\', default=\'\', type=str)\n    parser.add_argument(\'--inputsize\', default=224, type=int)\n    parser.add_argument(\'--world-size\', default=1, type=int,\n                        help=\'number of distributed processes\')\n    parser.add_argument(\'--manual-seed\', default=0, type=int)\n    parser.add_argument(\'--dist-url\', default=\'tcp://224.66.41.62:23456\', type=str,\n                        help=\'url used to set up distributed training\')\n    parser.add_argument(\'--dist-backend\', default=\'gloo\', type=str,\n                        help=\'distributed backend\')\n    parser.add_argument(\'--train-size\', default=1.0, type=float)\n    parser.add_argument(\'--val-size\', default=1.0, type=float)\n    parser.add_argument(\'--cache-dir\', default=\'./cache/\', type=str)\n    parser.add_argument(\'--name\', default=\'test\', type=str)\n    parser.add_argument(\'--nclass\', default=157, type=int)\n    parser.add_argument(\'--accum-grad\', default=4, type=int)\n    args = parser.parse_args()\n    args.distributed = args.world_size > 1\n    args.cache = args.cache_dir+args.name+\'/\'\n    if not os.path.exists(args.cache):\n        os.makedirs(args.cache)\n\n    return args\n'"
pytorch/train.py,6,"b'"""""" Defines the Trainer class which handles train/validation/validation_video\n""""""\nimport time\nimport torch\nimport itertools\nimport numpy as np\nfrom utils import map\n\n\nclass AverageMeter(object):\n    """"""Computes and stores the average and current value""""""\n\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count\n\n\ndef adjust_learning_rate(startlr, decay_rate, optimizer, epoch):\n    """"""Sets the learning rate to the initial LR decayed by 10 every 30 epochs""""""\n    lr = startlr * (0.1 ** (epoch // decay_rate))\n    for param_group in optimizer.param_groups:\n        param_group[\'lr\'] = lr\n\n\ndef accuracy(output, target, topk=(1,)):\n    """"""Computes the precision@k for the specified values of k""""""\n    maxk = max(topk)\n    batch_size = target.size(0)\n\n    _, pred = output.topk(maxk, 1, True, True)\n    pred = pred.t()\n    correct = pred.eq(target.view(1, -1).expand_as(pred))\n\n    res = []\n    for k in topk:\n        correct_k = correct[:k].view(-1).float().sum(0, keepdim=True)\n        res.append(correct_k.mul_(100.0 / batch_size))\n    return res\n\n\ndef submission_file(ids, outputs, filename):\n    """""" write list of ids and outputs to filename""""""\n    with open(filename, \'w\') as f:\n        for vid, output in zip(ids, outputs):\n            scores = [\'{:g}\'.format(x)\n                      for x in output]\n            f.write(\'{} {}\\n\'.format(vid, \' \'.join(scores)))\n\n\nclass Trainer():\n    def train(self, loader, model, criterion, optimizer, epoch, args):\n        adjust_learning_rate(args.lr, args.lr_decay_rate, optimizer, epoch)\n        batch_time = AverageMeter()\n        data_time = AverageMeter()\n        losses = AverageMeter()\n        top1 = AverageMeter()\n        top5 = AverageMeter()\n\n        # switch to train mode\n        model.train()\n        optimizer.zero_grad()\n\n        def part(x): return itertools.islice(x, int(len(x)*args.train_size))\n        end = time.time()\n        for i, (input, target, meta) in enumerate(part(loader)):\n            data_time.update(time.time() - end)\n\n            target = target.long().cuda(async=True)\n            input_var = torch.autograd.Variable(input.cuda())\n            target_var = torch.autograd.Variable(target)\n            output = model(input_var)\n            loss = None\n            # for nets that have multiple outputs such as inception\n            if isinstance(output, tuple):\n                loss = sum((criterion(o,target_var) for o in output))\n                output = output[0]\n            else:\n                loss = criterion(output, target_var)\n            prec1, prec5 = accuracy(output.data, target, topk=(1, 5))\n            losses.update(loss.data[0], input.size(0))\n            top1.update(prec1[0], input.size(0))\n            top5.update(prec5[0], input.size(0))\n\n            loss.backward()\n            if i % args.accum_grad == args.accum_grad-1:\n                print(\'updating parameters\')\n                optimizer.step()\n                optimizer.zero_grad()\n\n            batch_time.update(time.time() - end)\n            end = time.time()\n\n            if i % args.print_freq == 0:\n                print(\'Epoch: [{0}][{1}/{2}({3})]\\t\'\n                      \'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t\'\n                      \'Data {data_time.val:.3f} ({data_time.avg:.3f})\\t\'\n                      \'Loss {loss.val:.4f} ({loss.avg:.4f})\\t\'\n                      \'Prec@1 {top1.val:.3f} ({top1.avg:.3f})\\t\'\n                      \'Prec@5 {top5.val:.3f} ({top5.avg:.3f})\'.format(\n                          epoch, i, int(\n                              len(loader)*args.train_size), len(loader),\n                          batch_time=batch_time, data_time=data_time, loss=losses, top1=top1, top5=top5))\n        return top1.avg,top5.avg\n\n    def validate(self, loader, model, criterion, epoch, args):\n        batch_time = AverageMeter()\n        losses = AverageMeter()\n        top1 = AverageMeter()\n        top5 = AverageMeter()\n\n        # switch to evaluate mode\n        model.eval()\n\n        def part(x): return itertools.islice(x, int(len(x)*args.val_size))\n        end = time.time()\n        for i, (input, target, meta) in enumerate(part(loader)):\n            target = target.long().cuda(async=True)\n            input_var = torch.autograd.Variable(input.cuda(), volatile=True)\n            target_var = torch.autograd.Variable(target, volatile=True)\n            output = model(input_var)\n            loss = criterion(output, target_var)\n\n            prec1, prec5 = accuracy(output.data, target, topk=(1, 5))\n            losses.update(loss.data[0], input.size(0))\n            top1.update(prec1[0], input.size(0))\n            top5.update(prec5[0], input.size(0))\n            batch_time.update(time.time() - end)\n            end = time.time()\n\n            if i % args.print_freq == 0:\n                print(\'Test: [{0}/{1} ({2})]\\t\'\n                      \'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t\'\n                      \'Loss {loss.val:.4f} ({loss.avg:.4f})\\t\'\n                      \'Prec@1 {top1.val:.3f} ({top1.avg:.3f})\\t\'\n                      \'Prec@5 {top5.val:.3f} ({top5.avg:.3f})\'.format(\n                          i, int(len(loader)*args.val_size), len(loader),\n                          batch_time=batch_time, loss=losses,\n                          top1=top1, top5=top5))\n\n        print(\' * Prec@1 {top1.avg:.3f} Prec@5 {top5.avg:.3f}\'\n              .format(top1=top1, top5=top5))\n\n        return top1.avg,top5.avg\n\n    def validate_video(self, loader, model, epoch, args):\n        """""" Run video-level validation on the Charades test set""""""\n        batch_time = AverageMeter()\n        outputs = []\n        gts = []\n        ids = []\n\n        # switch to evaluate mode\n        model.eval()\n\n        end = time.time()\n        for i, (input, target, meta) in enumerate(loader):\n            target = target.long().cuda(async=True)\n            assert target[0,:].eq(target[1,:]).all(), ""val_video not synced""\n            input_var = torch.autograd.Variable(input.cuda(), volatile=True)\n            output = model(input_var)\n            output = torch.nn.Softmax(dim=1)(output)\n\n            # store predictions\n            output_video = output.mean(dim=0)\n            outputs.append(output_video.data.cpu().numpy())\n            gts.append(target[0,:])\n            ids.append(meta[\'id\'][0])\n            batch_time.update(time.time() - end)\n            end = time.time()\n\n            if i % args.print_freq == 0:\n                print(\'Test2: [{0}/{1}]\\t\'\n                      \'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\'.format(\n                          i, len(loader), batch_time=batch_time))\n        #mAP, _, ap = map.map(np.vstack(outputs), np.vstack(gts))\n        mAP, _, ap = map.charades_map(np.vstack(outputs), np.vstack(gts))\n        print(ap)\n        print(\' * mAP {:.3f}\'.format(mAP))\n        submission_file(\n            ids, outputs, \'{}/epoch_{:03d}.txt\'.format(args.cache, epoch+1))\n        return mAP\n'"
pytorch/datasets/__init__.py,6,"b'"""""" Initilize the datasets module\n    New datasets can be added with python scripts under datasets/\n""""""\nimport torch\nimport torch.utils.data\nimport torch.utils.data.distributed\nimport importlib\n\n\ndef get_dataset(args):\n    dataset = importlib.import_module(\'.\'+args.dataset, package=\'datasets\')\n    train_dataset, val_dataset, valvideo_dataset = dataset.get(args)\n\n    if args.distributed:\n        train_sampler = torch.utils.data.distributed.DistributedSampler(\n            train_dataset)\n    else:\n        train_sampler = None\n\n    train_loader = torch.utils.data.DataLoader(\n        train_dataset, batch_size=args.batch_size, shuffle=(\n            train_sampler is None),\n        num_workers=args.workers, pin_memory=True, sampler=train_sampler)\n\n    val_loader = torch.utils.data.DataLoader(\n        val_dataset, batch_size=args.batch_size, shuffle=True,\n        num_workers=args.workers, pin_memory=True)\n\n    valvideo_loader = torch.utils.data.DataLoader(\n        valvideo_dataset, batch_size=25, shuffle=False,\n        num_workers=args.workers, pin_memory=True)\n\n    return train_loader, val_loader, valvideo_loader\n'"
pytorch/datasets/charadesflow.py,3,"b'"""""" Dataset loader for the Charades dataset """"""\nimport torch\nimport torchvision.transforms as transforms\nimport transforms as arraytransforms\nfrom charadesrgb import Charades, cls2int\nfrom PIL import Image\nimport numpy as np\nfrom glob import glob\n\n\ndef pil_loader(path):\n    # open path as file to avoid ResourceWarning (https://github.com/python-pillow/Pillow/issues/835)\n    with open(path, \'rb\') as f:\n        img = Image.open(f)\n        return img.convert(\'L\')\n\n\ndef accimage_loader(path):\n    import accimage\n    try:\n        return accimage.Image(path)\n    except IOError:\n        # Potentially a decoding problem, fall back to PIL.Image\n        return pil_loader(path)\n\n\ndef default_loader(path):\n    from torchvision import get_image_backend\n    if get_image_backend() == \'accimage\':\n        return accimage_loader(path)\n    else:\n        return pil_loader(path)\n\n\nclass Charadesflow(Charades):\n    def __init__(self, *args, **kwargs):\n        super(Charadesflow,self).__init__(*args, **kwargs)\n        \n    def prepare(self, path, labels, split):\n        FPS, GAP, testGAP = 24, 4, 25\n        STACK=10\n        datadir = path\n        image_paths, targets, ids = [], [], []\n\n        for i, (vid, label) in enumerate(labels.iteritems()):\n            iddir = datadir + \'/\' + vid\n            lines = glob(iddir+\'/*.jpg\')\n            n = len(lines)/2\n            if i % 100 == 0:\n                print(""{} {}"".format(i, iddir))\n            if n == 0:\n                continue\n            if split == \'val_video\':\n                target = torch.IntTensor(157).zero_()\n                for x in label:\n                    target[cls2int(x[\'class\'])] = 1\n                spacing = np.linspace(0, n-1-STACK-1, testGAP)  # fit 10 optical flow pairs\n                for loc in spacing:\n                    impath = \'{}/{}-{:06d}x.jpg\'.format(\n                        iddir, vid, int(np.floor(loc))+1)\n                    image_paths.append(impath)\n                    targets.append(target)\n                    ids.append(vid)\n            else:\n                for x in label:\n                    for ii in range(0, n-1, GAP):\n                        if x[\'start\'] < ii/float(FPS) < x[\'end\']:\n                            if ii>n-1-STACK-1: continue  # fit 10 optical flow pairs\n                            impath = \'{}/{}-{:06d}x.jpg\'.format(\n                                iddir, vid, ii+1)\n                            image_paths.append(impath)\n                            targets.append(cls2int(x[\'class\']))\n                            ids.append(vid)\n        return {\'image_paths\': image_paths, \'targets\': targets, \'ids\': ids}\n\n    def __getitem__(self, index):\n        """"""\n        Args:\n            index (int): Index\n        Returns:\n            tuple: (image, target) where target is class_index of the target class.\n        """"""\n        path = self.data[\'image_paths\'][index]\n        base = path[:-5-6]\n        framenr = int(path[-5-6:-5])\n        assert \'{}{:06d}x.jpg\'.format(base,framenr) == path\n        STACK=10\n        img = []\n        for i in range(STACK):\n            x = \'{}{:06d}x.jpg\'.format(base,framenr+i)\n            y = \'{}{:06d}y.jpg\'.format(base,framenr+i)\n            imgx = default_loader(x)\n            imgy = default_loader(y)\n            img.append(imgx)\n            img.append(imgy)\n        target = self.data[\'targets\'][index]\n        meta = {}\n        meta[\'id\'] = self.data[\'ids\'][index]\n        if self.transform is not None:\n            img = self.transform(img)\n        if self.target_transform is not None:\n            target = self.target_transform(target)\n        return img, target, meta\n\n\ndef get(args):\n    """""" Entry point. Call this function to get all Charades dataloaders """"""\n    normalize = arraytransforms.Normalize(mean=[0.502], std=[1.0])\n    train_file = args.train_file\n    val_file = args.val_file\n    train_dataset = Charadesflow(\n        args.data, \'train\', train_file, args.cache,\n        transform=transforms.Compose([\n            arraytransforms.RandomResizedCrop(224),\n            arraytransforms.ToTensor(),\n            normalize,\n            transforms.Lambda(lambda x: torch.cat(x)),\n        ]))\n    val_transforms = transforms.Compose([\n            arraytransforms.Resize(256),\n            arraytransforms.CenterCrop(224),\n            arraytransforms.ToTensor(),\n            normalize,\n            transforms.Lambda(lambda x: torch.cat(x)),\n        ])\n    val_dataset = Charadesflow(\n        args.data, \'val\', val_file, args.cache, transform=val_transforms)\n    valvideo_dataset = Charadesflow(\n        args.data, \'val_video\', val_file, args.cache, transform=val_transforms)\n    return train_dataset, val_dataset, valvideo_dataset\n'"
pytorch/datasets/charadesrgb.py,2,"b'"""""" Dataset loader for the Charades dataset """"""\nimport torch\nimport torchvision.transforms as transforms\nimport torch.utils.data as data\nfrom PIL import Image\nimport numpy as np\nfrom glob import glob\nimport csv\nimport cPickle as pickle\nimport os\n\n\ndef parse_charades_csv(filename):\n    labels = {}\n    with open(filename) as f:\n        reader = csv.DictReader(f)\n        for row in reader:\n            vid = row[\'id\']\n            actions = row[\'actions\']\n            if actions == \'\':\n                actions = []\n            else:\n                actions = [a.split(\' \') for a in actions.split(\';\')]\n                actions = [{\'class\': x, \'start\': float(\n                    y), \'end\': float(z)} for x, y, z in actions]\n            labels[vid] = actions\n    return labels\n\n\ndef cls2int(x):\n    return int(x[1:])\n\n\ndef pil_loader(path):\n    # open path as file to avoid ResourceWarning (https://github.com/python-pillow/Pillow/issues/835)\n    with open(path, \'rb\') as f:\n        img = Image.open(f)\n        return img.convert(\'RGB\')\n\n\ndef accimage_loader(path):\n    import accimage\n    try:\n        return accimage.Image(path)\n    except IOError:\n        # Potentially a decoding problem, fall back to PIL.Image\n        return pil_loader(path)\n\n\ndef default_loader(path):\n    from torchvision import get_image_backend\n    if get_image_backend() == \'accimage\':\n        return accimage_loader(path)\n    else:\n        return pil_loader(path)\n\n\ndef cache(cachefile):\n    """""" Creates a decorator that caches the result to cachefile """"""\n    def cachedecorator(fn):\n        def newf(*args, **kwargs):\n            print(\'cachefile {}\'.format(cachefile))\n            if os.path.exists(cachefile):\n                with open(cachefile, \'rb\') as f:\n                    print(""Loading cached result from \'%s\'"" % cachefile)\n                    return pickle.load(f)\n            res = fn(*args, **kwargs)\n            with open(cachefile, \'wb\') as f:\n                print(""Saving result to cache \'%s\'"" % cachefile)\n                pickle.dump(res, f)\n            return res\n        return newf\n    return cachedecorator\n\n\nclass Charades(data.Dataset):\n    def __init__(self, root, split, labelpath, cachedir, transform=None, target_transform=None):\n        self.num_classes = 157\n        self.transform = transform\n        self.target_transform = target_transform\n        self.labels = parse_charades_csv(labelpath)\n        self.root = root\n        cachename = \'{}/{}_{}.pkl\'.format(cachedir,\n                                          self.__class__.__name__, split)\n        self.data = cache(cachename)(self.prepare)(root, self.labels, split)\n\n    def prepare(self, path, labels, split):\n        FPS, GAP, testGAP = 24, 4, 25\n        datadir = path\n        image_paths, targets, ids = [], [], []\n\n        for i, (vid, label) in enumerate(labels.iteritems()):\n            iddir = datadir + \'/\' + vid\n            lines = glob(iddir+\'/*.jpg\')\n            n = len(lines)\n            if i % 100 == 0:\n                print(""{} {}"".format(i, iddir))\n            if n == 0:\n                continue\n            if split == \'val_video\':\n                target = torch.IntTensor(157).zero_()\n                for x in label:\n                    target[cls2int(x[\'class\'])] = 1\n                spacing = np.linspace(0, n-1, testGAP)\n                for loc in spacing:\n                    impath = \'{}/{}-{:06d}.jpg\'.format(\n                        iddir, vid, int(np.floor(loc))+1)\n                    image_paths.append(impath)\n                    targets.append(target)\n                    ids.append(vid)\n            else:\n                for x in label:\n                    for ii in range(0, n-1, GAP):\n                        if x[\'start\'] < ii/float(FPS) < x[\'end\']:\n                            impath = \'{}/{}-{:06d}.jpg\'.format(\n                                iddir, vid, ii+1)\n                            image_paths.append(impath)\n                            targets.append(cls2int(x[\'class\']))\n                            ids.append(vid)\n        return {\'image_paths\': image_paths, \'targets\': targets, \'ids\': ids}\n\n    def __getitem__(self, index):\n        """"""\n        Args:\n            index (int): Index\n        Returns:\n            tuple: (image, target) where target is class_index of the target class.\n        """"""\n        path = self.data[\'image_paths\'][index]\n        target = self.data[\'targets\'][index]\n        meta = {}\n        meta[\'id\'] = self.data[\'ids\'][index]\n        img = default_loader(path)\n        if self.transform is not None:\n            img = self.transform(img)\n        if self.target_transform is not None:\n            target = self.target_transform(target)\n        return img, target, meta\n\n    def __len__(self):\n        return len(self.data[\'image_paths\'])\n\n    def __repr__(self):\n        fmt_str = \'Dataset \' + self.__class__.__name__ + \'\\n\'\n        fmt_str += \'    Number of datapoints: {}\\n\'.format(self.__len__())\n        fmt_str += \'    Root Location: {}\\n\'.format(self.root)\n        tmp = \'    Transforms (if any): \'\n        fmt_str += \'{0}{1}\\n\'.format(\n            tmp, self.transform.__repr__().replace(\'\\n\', \'\\n\' + \' \' * len(tmp)))\n        tmp = \'    Target Transforms (if any): \'\n        fmt_str += \'{0}{1}\'.format(\n            tmp, self.target_transform.__repr__().replace(\'\\n\', \'\\n\' + \' \' * len(tmp)))\n        return fmt_str\n\n\ndef get(args):\n    """""" Entry point. Call this function to get all Charades dataloaders """"""\n    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                                     std=[0.229, 0.224, 0.225])\n    train_file = args.train_file\n    val_file = args.val_file\n    train_dataset = Charades(\n        args.data, \'train\', train_file, args.cache,\n        transform=transforms.Compose([\n            transforms.RandomResizedCrop(args.inputsize),\n            transforms.ColorJitter(\n                brightness=0.4, contrast=0.4, saturation=0.4),\n            transforms.RandomHorizontalFlip(),\n            transforms.ToTensor(),  # missing PCA lighting jitter\n            normalize,\n        ]))\n    val_dataset = Charades(\n        args.data, \'val\', val_file, args.cache,\n        transform=transforms.Compose([\n            transforms.Resize(int(256./224*args.inputsize)),\n            transforms.CenterCrop(args.inputsize),\n            transforms.ToTensor(),\n            normalize,\n        ]))\n    valvideo_dataset = Charades(\n        args.data, \'val_video\', val_file, args.cache,\n        transform=transforms.Compose([\n            transforms.Resize(int(256./224*args.inputsize)),\n            transforms.CenterCrop(args.inputsize),\n            transforms.ToTensor(),\n            normalize,\n        ]))\n    return train_dataset, val_dataset, valvideo_dataset\n'"
pytorch/datasets/fake.py,0,"b'"""""" Define random data for quick debugging """"""\nimport torchvision\nimport torchvision.transforms as transforms\n\n\ndef get(args):\n    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                                     std=[0.229, 0.224, 0.225])\n\n    train_dataset = torchvision.datasets.FakeData(\n        transform=transforms.Compose([\n            transforms.RandomResizedCrop(224),\n            transforms.RandomHorizontalFlip(),\n            transforms.ToTensor(),\n            normalize,\n        ]))\n\n    val_dataset = torchvision.datasets.FakeData(\n        transform=transforms.Compose([\n            transforms.Resize(256),\n            transforms.CenterCrop(224),\n            transforms.ToTensor(),\n            normalize,\n        ]))\n\n    return train_dataset, val_dataset, val_dataset\n'"
pytorch/datasets/transforms.py,0,"b'"""""" Overloading Torchvision transforms to operate on a list """"""\n\nimport torchvision.transforms as parents\n\nclass CenterCrop(parents.CenterCrop):\n    def __init__(self, *args, **kwargs):\n        super(CenterCrop, self).__init__(*args, **kwargs)\n    def __call__(self, img):\n        return [super(CenterCrop, self).__call__(im) for im in img]\n        \n\nclass RandomCrop(parents.RandomCrop):\n    def __init__(self, *args, **kwargs):\n        super(RandomCrop, self).__init__(*args, **kwargs)\n    def __call__(self, img):\n        return [super(RandomCrop, self).__call__(im) for im in img]\n\n\nclass RandomResizedCrop(parents.RandomResizedCrop):\n    def __init__(self, *args):\n        super(RandomResizedCrop, self).__init__(*args)\n    def __call__(self, img):\n        return [super(RandomResizedCrop, self).__call__(im) for im in img]\n\n\nclass Resize(parents.Resize):\n    def __init__(self, *args, **kwargs):\n        super(Resize, self).__init__(*args, **kwargs)\n    def __call__(self, img):\n        return [super(Resize, self).__call__(im) for im in img]\n\n\nclass ToTensor(parents.ToTensor):\n    def __init__(self, *args, **kwargs):\n        super(ToTensor, self).__init__(*args, **kwargs)\n    def __call__(self, img):\n        return [super(ToTensor, self).__call__(im) for im in img]\n\n\nclass Normalize(parents.Normalize):\n    def __init__(self, *args, **kwargs):\n        super(Normalize, self).__init__(*args, **kwargs)\n    def __call__(self, img):\n        return [super(Normalize, self).__call__(im) for im in img]\n\n'"
pytorch/exp/flownet.py,0,"b""#!/usr/bin/env python\nimport sys\n#sys.path.insert(0, '..')\nsys.path.insert(0, '.')\nfrom main import main\n\nargs = [\n    '--name', __file__.split('/')[-1].split('.')[0],  # name is filename\n    '--print-freq', '1',\n    '--dataset', 'charadesflow',\n    '--data','/scratch/gsigurds/Charades_v1_flow/',\n    '--arch', 'vgg16flow',\n    '--pretrained-weights', './vgg16flow_ucf101.pth',\n    '--lr', '5e-3',\n    '--lr-decay-rate','15',\n    '--epochs','40',\n    '--batch-size', '64',\n    '--train-size', '0.2',\n    '--val-size', '0.1',\n    '--cache-dir', '/nfs.yoda/gsigurds/ai2/caches/',\n    '--pretrained',\n    #'--evaluate',\n]\nsys.argv.extend(args)\nmain()\n"""
pytorch/exp/flownet_test.py,0,"b""#!/usr/bin/env python\nimport sys\n#sys.path.insert(0, '..')\nsys.path.insert(0, '.')\nfrom main import main\n\nargs = [\n    '--name', __file__.split('/')[-1].split('.')[0],  # name is filename\n    '--print-freq', '1',\n    '--dataset', 'charadesflow',\n    '--data','/scratch/gsigurds/Charades_v1_flow/',\n    '--arch', 'vgg16flow',\n    '--lr', '5e-3',\n    '--lr-decay-rate','15',\n    '--epochs','40',\n    '--batch-size', '64',\n    '--train-size', '0.2',\n    '--val-size', '0.001',\n    '--cache-dir', '/nfs.yoda/gsigurds/ai2/caches/',\n    '--pretrained',\n    '--resume', './twostream_flow.pth',\n    '--evaluate',\n]\nsys.argv.extend(args)\nmain()\n"""
pytorch/exp/rgbnet.py,0,"b""#!/usr/bin/env python\nimport sys\n#sys.path.insert(0, '..')\nsys.path.insert(0, '.')\nfrom main import main\n\nargs = [\n    '--name', __file__.split('/')[-1].split('.')[0],  # name is filename\n    '--print-freq', '1',\n    '--dataset', 'charadesrgb',\n    '--arch', 'vgg16',\n    '--lr', '1e-3',\n    '--batch-size', '64',\n    '--train-size', '0.1',\n    '--val-size', '0.1',\n    '--cache-dir', '/nfs.yoda/gsigurds/ai2/caches/',\n    '--pretrained',\n    #'--evaluate',\n]\nsys.argv.extend(args)\nmain()\n"""
pytorch/exp/rgbnet_inception.py,0,"b""#!/usr/bin/env python\nimport sys\n#sys.path.insert(0, '..')\nsys.path.insert(0, '.')\nfrom main import main\n\nargs = [\n    '--name', __file__.split('/')[-1].split('.')[0],  # name is filename\n    '--print-freq', '1',\n    '--dataset', 'charadesrgb',\n    '--arch', 'inception_v3',\n    '--inputsize','299',\n    '--lr', '1e-3',\n    '--batch-size', '64',\n    '--train-size', '0.1',\n    '--val-size', '0.1',\n    '--cache-dir', '/nfs.yoda/gsigurds/ai2/caches/',\n    '--pretrained',\n    #'--evaluate',\n]\nsys.argv.extend(args)\nmain()\n"""
pytorch/exp/rgbnet_resnet.py,0,"b""#!/usr/bin/env python\nimport sys\n#sys.path.insert(0, '..')\nsys.path.insert(0, '.')\nfrom main import main\n\nargs = [\n    '--name', __file__.split('/')[-1].split('.')[0],  # name is filename\n    '--print-freq', '1',\n    '--dataset', 'charadesrgb',\n    '--arch', 'resnet152',\n    '--lr', '1e-3',\n    '--batch-size', '50',\n    '--train-size', '0.1',\n    '--val-size', '0.1',\n    '--cache-dir', '/nfs.yoda/gsigurds/ai2/caches/',\n    '--pretrained',\n    #'--evaluate',\n]\nsys.argv.extend(args)\nmain()\n"""
pytorch/exp/rgbnet_test.py,0,"b""#!/usr/bin/env python\nimport sys\n#sys.path.insert(0, '..')\nsys.path.insert(0, '.')\nfrom main import main\n\nargs = [\n    '--name', __file__.split('/')[-1].split('.')[0],  # name is filename\n    '--print-freq', '1',\n    '--dataset', 'charadesrgb',\n    '--arch', 'vgg16',\n    '--lr', '1e-3',\n    '--batch-size', '64',\n    '--train-size', '0.1',\n    '--val-size', '0.1',\n    '--cache-dir', '/nfs.yoda/gsigurds/ai2/caches/',\n    '--pretrained',\n    '--resume', './twostream_rgb.pth.tar',\n    '--evaluate',\n]\nsys.argv.extend(args)\nmain()\n"""
pytorch/models/__init__.py,9,"b'""""""\nInitialize the model module\nNew models can be defined by adding scripts under models/\n""""""\nimport torch\nimport torch.nn as nn\nimport torch.nn.parallel\nimport torch.backends.cudnn as cudnn\nimport torch.distributed as dist\nimport torchvision.models as tmodels\nimport importlib\n\n\ndef create_model(args):\n    if args.arch in tmodels.__dict__:  # torchvision models\n        if args.pretrained:\n            print(""=> using pre-trained model \'{}\'"".format(args.arch))\n            model = tmodels.__dict__[args.arch](pretrained=True)\n            model = model.cuda()\n        else:\n            print(""=> creating model \'{}\'"".format(args.arch))\n            model = tmodels.__dict__[args.arch]()\n    else:  # defined as script in this directory\n        model = importlib.import_module(\'.\'+args.arch, package=\'models\').model\n        if not args.pretrained_weights == \'\':\n            print(\'loading pretrained-weights from {}\'.format(args.pretrained_weights))\n            model.load_state_dict(torch.load(args.pretrained_weights))\n\n    # replace last layer\n    if hasattr(model, \'classifier\'):\n        newcls = list(model.classifier.children())\n        newcls = newcls[:-1] + [nn.Linear(newcls[-1].in_features, args.nclass).cuda()]\n        model.classifier = nn.Sequential(*newcls)\n    elif hasattr(model, \'fc\'):\n        model.fc = nn.Linear(model.fc.in_features, args.nclass)\n        if hasattr(model, \'AuxLogits\'):\n            model.AuxLogits.fc = nn.Linear(model.AuxLogits.fc.in_features, args.nclass)\n    else:\n        newcls = list(model.children())\n        if hasattr(model, \'in_features\'):\n            in_features = model.in_features\n        else:\n            in_features = newcls[-1].in_features\n        newcls = newcls[:-1] + [nn.Linear(in_features, args.nclass).cuda()]\n        model = nn.Sequential(*newcls)\n\n    if args.distributed:\n        dist.init_process_group(backend=args.dist_backend, init_method=args.dist_url,\n                                world_size=args.world_size)\n        model.cuda()\n        model = torch.nn.parallel.DistributedDataParallel(model)\n    else:\n        if hasattr(model, \'features\'):\n            model.features = torch.nn.DataParallel(model.features)\n            model.cuda()\n        else:\n            model = torch.nn.DataParallel(model).cuda()\n\n    # define loss function and optimizer\n    criterion = nn.CrossEntropyLoss().cuda()\n    optimizer = torch.optim.SGD(model.parameters(), args.lr,\n                                momentum=args.momentum,\n                                weight_decay=args.weight_decay)\n    cudnn.benchmark = True\n    return model, criterion, optimizer\n'"
pytorch/models/vgg16flow.py,2,"b'\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\n\nclass LambdaBase(nn.Sequential):\n    def __init__(self, fn, *args):\n        super(LambdaBase, self).__init__(*args)\n        self.lambda_func = fn\n\n    def forward_prepare(self, input):\n        output = []\n        for module in self._modules.values():\n            output.append(module(input))\n        return output if output else input\n\nclass Lambda(LambdaBase):\n    def forward(self, input):\n        return self.lambda_func(self.forward_prepare(input))\n\n\nmodel = nn.Sequential( # Sequential,\n\tnn.Conv2d(20,64,(3, 3),(1, 1),(1, 1)),\n\tnn.ReLU(),\n\tnn.Conv2d(64,64,(3, 3),(1, 1),(1, 1)),\n\tnn.ReLU(),\n\tnn.MaxPool2d((2, 2),(2, 2),(0, 0),ceil_mode=True),\n\tnn.Conv2d(64,128,(3, 3),(1, 1),(1, 1)),\n\tnn.ReLU(),\n\tnn.Conv2d(128,128,(3, 3),(1, 1),(1, 1)),\n\tnn.ReLU(),\n\tnn.MaxPool2d((2, 2),(2, 2),(0, 0),ceil_mode=True),\n\tnn.Conv2d(128,256,(3, 3),(1, 1),(1, 1)),\n\tnn.ReLU(),\n\tnn.Conv2d(256,256,(3, 3),(1, 1),(1, 1)),\n\tnn.ReLU(),\n\tnn.Conv2d(256,256,(3, 3),(1, 1),(1, 1)),\n\tnn.ReLU(),\n\tnn.MaxPool2d((2, 2),(2, 2),(0, 0),ceil_mode=True),\n\tnn.Conv2d(256,512,(3, 3),(1, 1),(1, 1)),\n\tnn.ReLU(),\n\tnn.Conv2d(512,512,(3, 3),(1, 1),(1, 1)),\n\tnn.ReLU(),\n\tnn.Conv2d(512,512,(3, 3),(1, 1),(1, 1)),\n\tnn.ReLU(),\n\tnn.MaxPool2d((2, 2),(2, 2),(0, 0),ceil_mode=True),\n\tnn.Conv2d(512,512,(3, 3),(1, 1),(1, 1)),\n\tnn.ReLU(),\n\tnn.Conv2d(512,512,(3, 3),(1, 1),(1, 1)),\n\tnn.ReLU(),\n\tnn.Conv2d(512,512,(3, 3),(1, 1),(1, 1)),\n\tnn.ReLU(),\n\tnn.MaxPool2d((2, 2),(2, 2),(0, 0),ceil_mode=True),\n\tLambda(lambda x: x.view(x.size(0),-1)), # View,\n\tnn.Sequential(Lambda(lambda x: x.view(1,-1) if 1==len(x.size()) else x ),nn.Linear(25088,4096)), # Linear,\n\tnn.ReLU(),\n\tnn.Dropout(0.9),\n\tnn.Sequential(Lambda(lambda x: x.view(1,-1) if 1==len(x.size()) else x ),nn.Linear(4096,4096)), # Linear,\n\tnn.ReLU(),\n\tnn.Dropout(0.8),\n\tnn.Sequential(Lambda(lambda x: x.view(1,-1) if 1==len(x.size()) else x ),nn.Linear(4096,101)), # Linear,\n)\nmodel.in_features = 4096\n'"
pytorch/utils/__init__.py,0,b''
pytorch/utils/map.py,0,"b'import numpy as np\n\n\ndef map(submission_array, gt_array):\n    """""" Returns mAP, weighted mAP, and AP array """"""\n    m_aps = []\n    n_classes = submission_array.shape[1]\n    for oc_i in range(n_classes):\n        sorted_idxs = np.argsort(-submission_array[:, oc_i])\n        tp = gt_array[:, oc_i][sorted_idxs] == 1\n        fp = np.invert(tp)\n        n_pos = tp.sum()\n        if n_pos < 0.1:\n            m_aps.append(float(\'nan\'))\n            continue\n        fp.sum()\n        f_pcs = np.cumsum(fp)\n        t_pcs = np.cumsum(tp)\n        prec = t_pcs / (f_pcs+t_pcs).astype(float)\n        avg_prec = 0\n        for i in range(submission_array.shape[0]):\n            if tp[i]:\n                avg_prec += prec[i]\n        m_aps.append(avg_prec / n_pos.astype(float))\n    m_aps = np.array(m_aps)\n    m_ap = np.mean(m_aps)\n    w_ap = (m_aps * gt_array.sum(axis=0) / gt_array.sum().sum().astype(float))\n    return m_ap, w_ap, m_aps\n\n\ndef charades_map(submission_array, gt_array):\n    """""" \n    Approximate version of the charades evaluation function\n    For precise numbers, use the submission file with the official matlab script\n    """"""\n    fix = submission_array.copy()\n    empty = np.sum(gt_array, axis=1)==0\n    fix[empty, :] = np.NINF\n    return map(fix, gt_array)\n'"
pytorch/utils/tee.py,0,"b'""""""\nImplements a crude stdout-to-file redirect for keep history of experiments\nThe following code initializes the redirect:\nimport tee\ntee.Tee(filename)\n""""""\nimport logging\nimport sys\n\n\nclass StreamToLogger(object):\n    def __init__(self, stream, logger, log_level=logging.INFO):\n        self.logger = logger\n        self.log_level = log_level\n        self.linebuf = \'\'\n        self.stream = stream\n\n    def write(self, buf):\n        self.stream.write(buf)\n        for line in buf.rstrip().splitlines():\n            self.logger.log(self.log_level, line.rstrip())\n\n    def flush(self):\n        self.stream.flush()\n\n\nclass Tee(object):\n    def __init__(self, filename):\n        self.filename = filename\n        logging.basicConfig(\n            level=logging.DEBUG,\n            format=\'%(asctime)s:%(message)s\',\n            filename=filename,\n            filemode=\'a\'\n        )\n        stdout_logger = logging.getLogger(\'STDOUT\')\n        sl = StreamToLogger(sys.stdout, stdout_logger, logging.INFO)\n        sys.stdout = sl\n\n        stderr_logger = logging.getLogger(\'STDERR\')\n        sl = StreamToLogger(sys.stderr, stderr_logger, logging.ERROR)\n        sys.stderr = sl\n        print ""Logging to file {}"".format(filename)\n'"
