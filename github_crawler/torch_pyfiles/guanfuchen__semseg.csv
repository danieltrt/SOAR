file_path,api_count,code
performance_table.py,14,"b""# -*- coding: utf-8 -*-\n\n\n# -*- coding: utf-8 -*-_resnet18_32s\nimport torch\nimport os\nimport argparse\n\nimport cv2\nimport time\nimport numpy as np\nimport visdom\nfrom torch.autograd import Variable\n\nfrom semseg.dataloader.camvid_loader import camvidLoader\nfrom semseg.dataloader.cityscapes_loader import cityscapesLoader\nfrom semseg.loss import cross_entropy2d\nfrom semseg.modelloader.EDANet import EDANet\nfrom semseg.modelloader.deeplabv3 import Res_Deeplab_101, Res_Deeplab_50\nfrom semseg.modelloader.drn import drn_d_22, DRNSeg, drn_a_asymmetric_18, drnseg_a_50, drnseg_a_18, drnseg_e_22, \\\n    drnseg_a_asymmetric_18, drnseg_d_22, drnseg_a_asymmetric_n, drnseg_a_n\nfrom semseg.modelloader.duc_hdc import ResNetDUC, ResNetDUCHDC\nfrom semseg.modelloader.enet import ENet\nfrom semseg.modelloader.enetv2 import ENetV2\nfrom semseg.modelloader.erfnet import erfnet\nfrom semseg.modelloader.fc_densenet import fcdensenet103, fcdensenet56, fcdensenet_tiny\nfrom semseg.modelloader.fcn import fcn, fcn_32s, fcn_16s, fcn_8s\nfrom semseg.modelloader.fcn_mobilenet import fcn_MobileNet, fcn_MobileNet_32s, fcn_MobileNet_16s, fcn_MobileNet_8s\nfrom semseg.modelloader.fcn_resnet import fcn_resnet18, fcn_resnet34, fcn_resnet18_32s, fcn_resnet18_16s, \\\n    fcn_resnet18_8s, fcn_resnet34_32s, fcn_resnet34_16s, fcn_resnet34_8s\nfrom semseg.modelloader.segnet import segnet, segnet_squeeze, segnet_alignres, segnet_vgg19\nfrom semseg.modelloader.segnet_unet import segnet_unet\nfrom semseg.modelloader.sqnet import sqnet\nfrom semseg.utils.flops_benchmark import add_flops_counting_methods\n\n\n\n\ndef performance_table(args):\n    local_path = os.path.expanduser(args.dataset_path)\n    if args.dataset == 'CamVid':\n        dst = camvidLoader(local_path, is_transform=True, is_augment=args.data_augment)\n    elif args.dataset == 'CityScapes':\n        dst = cityscapesLoader(local_path, is_transform=True)\n    else:\n        pass\n\n    # dst.n_classes = args.n_classes # \xe4\xbf\x9d\xe8\xaf\x81\xe8\xbe\x93\xe5\x85\xa5\xe7\x9a\x84class\n    trainloader = torch.utils.data.DataLoader(dst, batch_size=args.batch_size, shuffle=True)\n\n    start_epoch = 0\n    if args.resume_model != '':\n        model = torch.load(args.resume_model)\n        start_epoch_id1 = args.resume_model.rfind('_')\n        start_epoch_id2 = args.resume_model.rfind('.')\n        start_epoch = int(args.resume_model[start_epoch_id1+1:start_epoch_id2])\n    else:\n        if args.structure == 'drnseg_a_asymmetric_n':\n            model = eval(args.structure)(n_classes=dst.n_classes, pretrained=args.init_vgg16, depth_n=args.depth_n)\n        elif args.structure == 'drnseg_a_n':\n            model = eval(args.structure)(n_classes=dst.n_classes, pretrained=args.init_vgg16, depth_n=args.depth_n)\n        else:\n            model = eval(args.structure)(n_classes=dst.n_classes, pretrained=args.init_vgg16)\n        if args.resume_model_state_dict != '':\n            try:\n                # fcn32s\xe3\x80\x81fcn16s\xe5\x92\x8cfcn8s\xe6\xa8\xa1\xe5\x9e\x8b\xe7\x95\xa5\xe6\x9c\x89\xe5\xa2\x9e\xe5\x8a\xa0\xe5\x8f\x82\xe6\x95\xb0\xef\xbc\x8c\xe4\xba\x92\xe7\x9b\xb8\xe8\xb5\x8b\xe5\x80\xbc\xe9\x87\x8d\xe6\x96\xb0\xe8\xae\xad\xe7\xbb\x83\xe8\xbf\x87\xe7\xa8\x8b\xe4\xb8\xad\xe4\xbc\x9a\xe6\x9c\x89KeyError\xef\xbc\x8c\xe6\x9a\x82\xe6\x97\xb6\xe6\x8d\x95\xe6\x8d\x89\xe5\xbc\x82\xe5\xb8\xb8\xe5\xa4\x84\xe7\x90\x86\n                start_epoch_id1 = args.resume_model_state_dict.rfind('_')\n                start_epoch_id2 = args.resume_model_state_dict.rfind('.')\n                start_epoch = int(args.resume_model_state_dict[start_epoch_id1 + 1:start_epoch_id2])\n                pretrained_dict = torch.load(args.resume_model_state_dict)\n                # model_dict = model.state_dict()\n                # for k, v in pretrained_dict.items():\n                #     print(k)\n                # pretrained_dict = {k: v for k, v in pretrained_dict.items() if k in model_dict}\n                # model_dict.update(pretrained_dict)\n                model.load_state_dict(pretrained_dict)\n            except KeyError:\n                print('missing key')\n\n    model = add_flops_counting_methods(model)\n    if args.cuda:\n        model.cuda()\n    model.train()\n    model.start_flops_count()\n    # print('start_epoch:', start_epoch)\n    optimizer = torch.optim.SGD(filter(lambda p: p.requires_grad, model.parameters()), lr=args.lr, momentum=0.99, weight_decay=5e-4)\n    # optimizer = torch.optim.Adam(model.parameters(), lr=args.lr, betas=(0.9, 0.999), eps=1e-08, weight_decay=1e-4)\n\n    forward_time = 0\n    backward_time = 0\n\n    # \xe7\xac\xac\xe4\xb8\x80\xe6\xac\xa1warmup\xe5\xb0\x86GPU\xe8\xb0\x83\xe7\x94\xa8\n    for i, (imgs, labels) in enumerate(trainloader):\n        imgs_batch = imgs.shape[0]\n        if imgs_batch != args.batch_size:\n            break\n        if args.cuda:\n            imgs = imgs.cuda()\n        model(imgs)\n        break\n\n    for epoch in range(0, 1, 1):\n        for i, (imgs, labels) in enumerate(trainloader):\n\n            # \xe6\x9c\x80\xe5\x90\x8e\xe7\x9a\x84\xe5\x87\xa0\xe5\xbc\xa0\xe5\x9b\xbe\xe7\x89\x87\xe5\x8f\xaf\xe8\x83\xbd\xe4\xb8\x8d\xe5\x88\xb0batch_size\xe7\x9a\x84\xe6\x95\xb0\xe9\x87\x8f\xef\xbc\x8c\xe6\xaf\x94\xe5\xa6\x82batch_size=4\xef\xbc\x8c\xe5\x8f\xaf\xe8\x83\xbd\xe5\x8f\xaa\xe5\x89\xa93\xe5\xbc\xa0\n            imgs_batch = imgs.shape[0]\n            if imgs_batch != args.batch_size:\n                break\n            # print(i)\n            # data_count = i\n            # print(labels.shape)\n            # print(imgs.shape)\n\n            imgs = Variable(imgs)\n            labels = Variable(labels)\n\n            # imgs = Variable(torch.randn(1, 3, 360, 640))\n            # labels = Variable(torch.LongTensor(np.ones((1, 360, 640), dtype=np.int)))\n\n            if args.cuda:\n                imgs = imgs.cuda()\n                labels = labels.cuda()\n\n            if args.cuda:\n                torch.cuda.synchronize()\n            start = time.time()\n            outputs = model(imgs)\n            if args.cuda:\n                torch.cuda.synchronize()\n            end = time.time()\n            forward_time += (end - start)\n            # print('forward time:', end - start)\n\n            if args.cuda:\n                torch.cuda.synchronize()\n            start = time.time()\n            # \xe4\xb8\x80\xe6\xac\xa1backward\xe5\x90\x8e\xe5\xa6\x82\xe6\x9e\x9c\xe4\xb8\x8d\xe6\xb8\x85\xe9\x9b\xb6\xef\xbc\x8c\xe6\xa2\xaf\xe5\xba\xa6\xe6\x98\xaf\xe7\xb4\xaf\xe5\x8a\xa0\xe7\x9a\x84\n            optimizer.zero_grad()\n            loss = cross_entropy2d(outputs, labels)\n            loss.backward()\n            optimizer.step()\n\n            if args.cuda:\n                torch.cuda.synchronize()\n            end = time.time()\n            backward_time += (end - start)\n            # print('backward time:', end - start)\n\n            if i==args.iterations-1:\n                break\n\n    avg_forward_time = forward_time * 1.0 / args.iterations\n    avg_backward_time = backward_time * 1.0 / args.iterations\n\n    print('average forward time:', forward_time * 1.0 / args.iterations)\n    print('average backward time:', backward_time * 1.0 / args.iterations)\n    model_flops = model.compute_average_flops_cost() / 1e9 / 2\n    print('model_flops:', model_flops)\n    if args.save_model:\n        torch.save(model.state_dict(), 'performance_{}_{}_class_{}.pt'.format(args.structure, args.dataset, args.n_classes))\n\n    return avg_forward_time, avg_backward_time, model_flops\n\n\n# best training: python performance_table.py --resume_model fcn32s_camvid_9.pkl --save_model True\n# --init_vgg16 True --dataset_path /home/cgf/Data/CamVid --batch_size 1 --vis True\nif __name__=='__main__':\n    # print('train----in----')\n    parser = argparse.ArgumentParser(description='training parameter setting')\n    parser.add_argument('--structure', type=str, default='ALL', help='use the net structure to segment [ fcn32s ResNetDUC segnet ENet drn_d_22 ]')\n    parser.add_argument('--resume_model', type=str, default='', help='resume model path [ fcn32s_camvid_9.pkl ]')\n    parser.add_argument('--resume_model_state_dict', type=str, default='', help='resume model state dict path [ fcn32s_camvid_9.pt ]')\n    parser.add_argument('--save_model', type=bool, default=False, help='save model [ False ]')\n    parser.add_argument('--save_epoch', type=int, default=1, help='save model after epoch [ 1 ]')\n    parser.add_argument('--init_vgg16', type=bool, default=False, help='init model using vgg16 weights [ False ]')\n    parser.add_argument('--dataset', type=str, default='CamVid', help='train dataset [ CamVid CityScapes ]')\n    parser.add_argument('--dataset_path', type=str, default='~/Data/CamVid', help='train dataset path [ ~/Data/CamVid ~/Data/cityscapes ]')\n    parser.add_argument('--data_augment', type=bool, default=False, help='enlarge the training data [ False ]')\n    parser.add_argument('--batch_size', type=int, default=1, help='train dataset batch size [ 1 ]')\n    parser.add_argument('--iterations', type=int, default=1, help='train dataset iterations [ 1 ]')\n    parser.add_argument('--n_classes', type=int, default=12, help='train class num [ 12 ]')\n    parser.add_argument('--depth_n', type=int, default=18, help='just for testing  drnseg_a_asymmetric_n [ 18 ]')\n    parser.add_argument('--lr', type=float, default=1e-5, help='train learning rate [ 0.00001 ]')\n    parser.add_argument('--vis', type=bool, default=False, help='visualize the training results [ False ]')\n    parser.add_argument('--cuda', type=bool, default=False, help='use cuda [ False ]')\n    args = parser.parse_args()\n    # print(args.resume_model)\n    # print(args.save_model)\n    structures = [\n        'fcn_32s', 'fcn_16s', 'fcn_8s',\n        'fcn_resnet18_32s', 'fcn_resnet18_16s', 'fcn_resnet18_8s',\n        'fcn_resnet34_32s', 'fcn_resnet34_16s', 'fcn_resnet34_8s',\n        'fcn_MobileNet_32s', 'fcn_MobileNet_16s', 'fcn_MobileNet_8s',\n        'ResNetDUC', 'ResNetDUCHDC',\n        'segnet', 'segnet_vgg19', 'segnet_unet', 'segnet_alignres',\n        # 'sqnet',\n        'segnet_squeeze',\n        'ENet', 'ENetV2',\n        'drnseg_d_22', 'drnseg_a_50', 'drnseg_a_18', 'drnseg_e_22', 'drnseg_a_asymmetric_18',\n        'erfnet',\n        # 'fcdensenet103', 'fcdensenet56', 'fcdensenet_tiny',\n        'Res_Deeplab_101', 'Res_Deeplab_50',\n        'EDANet'\n    ]\n    if args.structure == 'ALL':\n        for structure in structures:\n            print('-----------------------------------------------------------------------------')\n            args.structure = structure\n            print(args)\n            print('structure:', args.structure)\n            performance_table(args)\n            print('-----------------------------------------------------------------------------')\n    else:\n        performance_table(args)\n\n    # args.structure = 'drnseg_a_asymmetric_n'\n    # args.structure = 'drnseg_a_n'\n    # args.cuda = True\n    #\n    # for structure in ['drnseg_a_n', 'drnseg_a_asymmetric_n']:\n    #     for cuda in [True, False]:\n    #         if structure=='drnseg_a_asymmetric_n' and cuda==True:\n    #             pass\n    #         else:\n    #             continue\n    #         args.structure = structure\n    #         args.cuda = cuda\n    #         tmp_txt = open('/tmp/performance_speed_{}_gpu_{}.txt'.format(args.structure, args.cuda), 'wb')\n    #         tmp_txt.write('depth_n avg_fps avg_forward_time avg_backward_time model_flops\\n')\n    #         try:\n    #             for depth_n in range(18, 100, 1):\n    #                 args.depth_n = depth_n\n    #                 args.iterations = 10\n    #                 avg_forward_time, avg_backward_time, model_flops = performance_table(args)\n    #                 if args.cuda:\n    #                     torch.cuda.empty_cache()\n    #                 print('avg_forward_time:', avg_forward_time)\n    #                 # if avg_forward_time > 1/25.0:\n    #                 #     break\n    #                 tmp_txt.write('{}\\t{}\\t{}\\t{}\\t{}\\n'.format(depth_n, 1.0 / avg_forward_time, avg_forward_time,\n    #                                                             avg_backward_time, model_flops))\n    #         except:\n    #             pass\n    #         finally:\n    #             tmp_txt.close()\n\n\n    # print('train----out----')\n\n"""
train.py,14,"b'# -*- coding: utf-8 -*-_resnet18_32s\nimport datetime\n\nimport torch\nimport os\nimport argparse\n\nimport cv2\nimport time\nimport numpy as np\nimport visdom\nfrom torch.autograd import Variable\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau, StepLR, MultiStepLR\n\nfrom semseg.dataloader.camvid_loader import camvidLoader\nfrom semseg.dataloader.cityscapes_loader import cityscapesLoader\nfrom semseg.dataloader.freespace_loader import freespaceLoader\nfrom semseg.dataloader.movingmnist_loader import movingmnistLoader\nfrom semseg.dataloader.segmpred_loader import segmpredLoader\nfrom semseg.loss import cross_entropy2d\nfrom semseg.metrics import scores\nfrom semseg.modelloader.EDANet import EDANet\nfrom semseg.modelloader.bisenet import BiSeNet\nfrom semseg.modelloader.deconvnet import DeConvResNet50, DeConvResNet18\nfrom semseg.modelloader.deeplabv3 import Res_Deeplab_101, Res_Deeplab_50\nfrom semseg.modelloader.drn import drn_d_22, DRNSeg, drn_a_asymmetric_18, drn_a_asymmetric_ibn_a_18, drnseg_a_50, drnseg_a_18, drnseg_a_34, drnseg_e_22, drnseg_a_asymmetric_18, drnseg_a_asymmetric_ibn_a_18, drnseg_d_22, drnseg_d_38\nfrom semseg.modelloader.drn_a_irb import drnsegirb_a_18\nfrom semseg.modelloader.drn_a_refine import drnsegrefine_a_18\nfrom semseg.modelloader.duc_hdc import ResNetDUC, ResNetDUCHDC\nfrom semseg.modelloader.enet import ENet\nfrom semseg.modelloader.enetv2 import ENetV2\nfrom semseg.modelloader.erfnet import erfnet\nfrom semseg.modelloader.fc_densenet import fcdensenet103, fcdensenet56, fcdensenet_tiny\nfrom semseg.modelloader.fcn import fcn, fcn_32s, fcn_16s, fcn_8s\nfrom semseg.modelloader.fcn_mobilenet import fcn_MobileNet, fcn_MobileNet_32s, fcn_MobileNet_16s, fcn_MobileNet_8s\nfrom semseg.modelloader.fcn_resnet import fcn_resnet18, fcn_resnet34, fcn_resnet18_32s, fcn_resnet18_16s, \\\n    fcn_resnet18_8s, fcn_resnet34_32s, fcn_resnet34_16s, fcn_resnet34_8s, fcn_resnet50_32s, fcn_resnet50_16s, fcn_resnet50_8s\nfrom semseg.modelloader.fcn_shufflenet import fcn_shufflenet_32s, fcn_shufflenet_16s, fcn_shufflenet_8s\nfrom semseg.modelloader.gcn import gcn_resnet18, gcn_resnet34, gcn_resnet50, gcn_resnet101\nfrom semseg.modelloader.lrn import lrn_vgg16\nfrom semseg.modelloader.segnet import segnet, segnet_squeeze, segnet_alignres, segnet_vgg19\nfrom semseg.modelloader.segnet_unet import segnet_unet\nfrom semseg.modelloader.sqnet import sqnet\nfrom semseg.schedulers import ConstantLR, PolynomialLR\nfrom semseg.utils.get_class_weights import median_frequency_balancing, ENet_weighing\n\n\ndef train(args):\n    now = datetime.datetime.now()\n    now_str = \'{}-{}-{} {}:{}:{}\'.format(now.year, now.month, now.day, now.hour, now.minute, now.second)\n    # print(\'now:\', now)\n    # print(\'now_str:\', now_str)\n    if args.vis:\n        # start visdom and close all window\n        vis = visdom.Visdom(env=now_str)\n        vis.close()\n\n    class_weight = None\n    local_path = os.path.expanduser(args.dataset_path)\n    train_dst = None\n    val_dst = None\n    if args.dataset == \'CamVid\':\n        train_dst = camvidLoader(local_path, is_transform=True, is_augment=args.data_augment, split=\'train\')\n        val_dst = camvidLoader(local_path, is_transform=True, is_augment=False, split=\'val\')\n\n        trainannot_image_dir = os.path.expanduser(os.path.join(local_path, ""trainannot""))\n        trainannot_image_files = [os.path.join(trainannot_image_dir, file) for file in os.listdir(trainannot_image_dir) if file.endswith(\'.png\')]\n        if args.class_weighting==\'MFB\':\n            class_weight = median_frequency_balancing(trainannot_image_files, num_classes=12)\n            class_weight = torch.tensor(class_weight)\n        elif args.class_weighting==\'ENET\':\n            class_weight = ENet_weighing(trainannot_image_files, num_classes=12)\n            class_weight = torch.tensor(class_weight)\n    elif args.dataset == \'CityScapes\':\n        train_dst = cityscapesLoader(local_path, is_transform=True, split=\'train\')\n        val_dst = cityscapesLoader(local_path, is_transform=True, split=\'val\')\n    elif args.dataset == \'SegmPred\':\n        train_dst = segmpredLoader(local_path, is_transform=True, split=\'train\')\n        val_dst = segmpredLoader(local_path, is_transform=True, split=\'train\')\n    elif args.dataset == \'MovingMNIST\':\n        # class_weight = [0.1, 0.5]\n        # class_weight = torch.tensor(class_weight)\n        train_dst = movingmnistLoader(local_path, is_transform=True, split=\'train\')\n        val_dst = movingmnistLoader(local_path, is_transform=True, split=\'val\')\n    elif args.dataset == \'FreeSpace\':\n        train_dst = freespaceLoader(local_path, is_transform=True, split=\'train\')\n        val_dst = freespaceLoader(local_path, is_transform=True, split=\'val\')\n    else:\n        print(\'{} dataset does not implement\'.format(args.dataset))\n        exit(0)\n\n    if args.cuda:\n        if class_weight is not None:\n            class_weight = class_weight.cuda()\n    print(\'class_weight:\', class_weight)\n\n    train_loader = torch.utils.data.DataLoader(train_dst, batch_size=args.batch_size, shuffle=True)\n    val_loader = torch.utils.data.DataLoader(val_dst, batch_size=1, shuffle=True)\n\n    start_epoch = 0\n    best_mIoU = 0\n    if args.resume_model != \'\':\n        model = torch.load(args.resume_model)\n        start_epoch_id1 = args.resume_model.rfind(\'_\')\n        start_epoch_id2 = args.resume_model.rfind(\'.\')\n        start_epoch = int(args.resume_model[start_epoch_id1+1:start_epoch_id2])\n    else:\n        # model = eval(args.structure)(n_classes=args.n_classes, pretrained=args.init_vgg16)\n        try:\n            model = eval(args.structure)(n_classes=args.n_classes, pretrained=args.init_vgg16)\n        except:\n            print(\'missing structure or not support\')\n            exit(0)\n\n        # ---------------for testing SegmPred---------------\n        if args.dataset == \'MovingMNIST\':\n            input_channel = 1*9\n        elif args.dataset == \'SegmPred\':\n            input_channel = 19*4\n        if args.structure == \'drnseg_a_18\':\n            model = drnseg_a_18(n_classes=args.n_classes, pretrained=args.init_vgg16, input_channel=input_channel)\n        # ---------------for testing SegmPred---------------\n\n        if args.resume_model_state_dict != \'\':\n            try:\n                # from model save format get useful information, such as miou, epoch\n                miou_model_name_str = \'_miou_\'\n                class_model_name_str = \'_class_\'\n                miou_id1 = args.resume_model_state_dict.find(miou_model_name_str)+len(miou_model_name_str)\n                miou_id2 = args.resume_model_state_dict.find(class_model_name_str)\n                best_mIoU = float(args.resume_model_state_dict[miou_id1:miou_id2])\n\n                start_epoch_id1 = args.resume_model_state_dict.rfind(\'_\')\n                start_epoch_id2 = args.resume_model_state_dict.rfind(\'.\')\n                start_epoch = int(args.resume_model_state_dict[start_epoch_id1 + 1:start_epoch_id2])\n                pretrained_dict = torch.load(args.resume_model_state_dict, map_location=\'cpu\')\n                model.load_state_dict(pretrained_dict)\n            except KeyError:\n                print(\'missing resume_model_state_dict or wrong type\')\n\n\n\n    if args.cuda:\n        model.cuda()\n    print(\'start_epoch:\', start_epoch)\n    print(\'best_mIoU:\', best_mIoU)\n\n    if args.solver == \'SGD\':\n        optimizer = torch.optim.SGD(filter(lambda p: p.requires_grad, model.parameters()), lr=args.lr, momentum=0.99, weight_decay=5e-4)\n    elif args.solver == \'RMSprop\':\n        optimizer = torch.optim.RMSprop(filter(lambda p: p.requires_grad, model.parameters()), lr=args.lr, momentum=0.99, weight_decay=5e-4)\n    elif args.solver == \'Adam\':\n        optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=args.lr, weight_decay=5e-4)\n    else:\n        print(\'missing solver or not support\')\n        exit(0)\n    # when observerd object dose not decrease scheduler will let the optimizer learing rate decrease\n    # scheduler = ReduceLROnPlateau(optimizer, \'min\', patience=100, min_lr=1e-10, verbose=True)\n    if args.lr_policy == \'Constant\':\n        scheduler = ConstantLR(optimizer)\n    elif args.lr_policy == \'Polynomial\':\n        scheduler = PolynomialLR(optimizer, max_iter=args.training_epoch, power=0.9) # base lr=0.01 power=0.9 like PSPNet\n    elif args.lr_policy == \'MultiStep\':\n        scheduler = MultiStepLR(optimizer, milestones=[10, 50, 90], gamma=0.1) # base lr=0.01 power=0.9 like PSPNet\n\n    # scheduler = StepLR(optimizer, step_size=1, gamma=0.1)\n\n    data_count = int(train_dst.__len__() * 1.0 / args.batch_size)\n    print(\'data_count:\', data_count)\n    # iteration_step = 0\n    train_gts, train_preds = [], []\n    for epoch in range(start_epoch+1, args.training_epoch, 1):\n        loss_epoch = 0\n        scheduler.step()\n\n        optimizer.zero_grad() # when train next time zero all grad, just acc the grad when the epoch training\n        for i, (imgs, labels) in enumerate(train_loader):\n            # if i==1:\n            #     break\n            model.train()\n\n            # \xe6\x9c\x80\xe5\x90\x8e\xe7\x9a\x84\xe5\x87\xa0\xe5\xbc\xa0\xe5\x9b\xbe\xe7\x89\x87\xe5\x8f\xaf\xe8\x83\xbd\xe4\xb8\x8d\xe5\x88\xb0batch_size\xe7\x9a\x84\xe6\x95\xb0\xe9\x87\x8f\xef\xbc\x8c\xe6\xaf\x94\xe5\xa6\x82batch_size=4\xef\xbc\x8c\xe5\x8f\xaf\xe8\x83\xbd\xe5\x8f\xaa\xe5\x89\xa93\xe5\xbc\xa0\n            imgs_batch = imgs.shape[0]\n            if imgs_batch != args.batch_size:\n                break\n            # iteration_step += 1\n\n            imgs = Variable(imgs)\n            labels = Variable(labels)\n\n            if args.cuda:\n                imgs = imgs.cuda()\n                labels = labels.cuda()\n            outputs = model(imgs)\n            # print(\'imgs.size:\', imgs.size())\n            # print(\'labels.size:\', labels.size())\n            # print(\'outputs.size:\', outputs.size())\n\n            loss = cross_entropy2d(outputs, labels, weight=class_weight)\n\n            # add grad backward the avg loss\n            loss_grad_acc_avg = loss*1.0/args.grad_acc_steps\n            loss_grad_acc_avg.backward()\n\n            loss_np = loss.cpu().data.numpy()\n            loss_epoch += loss_np\n\n\n            if (i+1)%args.grad_acc_steps == 0:\n                optimizer.step()\n                # \xe4\xb8\x80\xe6\xac\xa1backward\xe5\x90\x8e\xe5\xa6\x82\xe6\x9e\x9c\xe4\xb8\x8d\xe6\xb8\x85\xe9\x9b\xb6\xef\xbc\x8c\xe6\xa2\xaf\xe5\xba\xa6\xe6\x98\xaf\xe7\xb4\xaf\xe5\x8a\xa0\xe7\x9a\x84\n                optimizer.zero_grad()\n\n            # ------------------train metris-------------------------------\n            train_pred = outputs.cpu().data.max(1)[1].numpy()\n            train_gt = labels.cpu().data.numpy()\n\n            for train_gt_, train_pred_ in zip(train_gt, train_pred):\n                train_gts.append(train_gt_)\n                train_preds.append(train_pred_)\n            # ------------------train metris-------------------------------\n\n            if args.vis and i%50==0:\n                pred_labels = outputs.cpu().data.max(1)[1].numpy()\n                label_color = train_dst.decode_segmap(labels.cpu().data.numpy()[0]).transpose(2, 0, 1)\n                pred_label_color = train_dst.decode_segmap(pred_labels[0]).transpose(2, 0, 1)\n                win = \'label_color\'\n                vis.image(label_color, win=win, opts=dict(title=\'Gt\', caption=\'Ground Truth\'))\n                win = \'pred_label_color\'\n                vis.image(pred_label_color, win=win, opts=dict(title=\'Pred\', caption=\'Prediction\'))\n\n            # \xe6\x98\xbe\xe7\xa4\xba\xe4\xb8\x80\xe4\xb8\xaa\xe5\x91\xa8\xe6\x9c\x9f\xe7\x9a\x84loss\xe6\x9b\xb2\xe7\xba\xbf\n            if args.vis:\n                win = \'loss_iteration\'\n                loss_np_expand = np.expand_dims(loss_np, axis=0)\n                win_res = vis.line(X=np.ones(1)*(i+data_count*(epoch-1)+1), Y=loss_np_expand, win=win, update=\'append\')\n                if win_res != win:\n                    vis.line(X=np.ones(1)*(i+data_count*(epoch-1)+1), Y=loss_np_expand, win=win, opts=dict(title=win, xlabel=\'iteration\', ylabel=\'loss\'))\n\n        # val result on val dataset and pick best to save\n        if args.val_interval > 0  and epoch % args.val_interval == 0:\n            print(\'----starting val----\')\n            model.eval()\n\n            val_gts, val_preds = [], []\n            for val_i, (val_imgs, val_labels) in enumerate(val_loader):\n                # print(val_i)\n                val_imgs = Variable(val_imgs, volatile=True)\n                val_labels = Variable(val_labels, volatile=True)\n\n                if args.cuda:\n                    val_imgs = val_imgs.cuda()\n                    val_labels = val_labels.cuda()\n\n                val_outputs = model(val_imgs)\n                val_pred = val_outputs.cpu().data.max(1)[1].numpy()\n                val_gt = val_labels.cpu().data.numpy()\n                for val_gt_, val_pred_ in zip(val_gt, val_pred):\n                    val_gts.append(val_gt_)\n                    val_preds.append(val_pred_)\n\n            score, class_iou = scores(val_gts, val_preds, n_class=args.n_classes)\n            for k, v in score.items():\n                print(k, v)\n                if k == \'Mean IoU : \\t\':\n                    v_iou = v\n                    if v > best_mIoU:\n                        best_mIoU = v_iou\n                        torch.save(model.state_dict(), \'{}_{}_miou_{}_class_{}_{}.pt\'.format(args.structure, args.dataset, best_mIoU, args.n_classes, epoch))\n                    # \xe6\x98\xbe\xe7\xa4\xba\xe6\xa0\xa1\xe5\x87\x86\xe5\x91\xa8\xe6\x9c\x9f\xe7\x9a\x84mIoU\n                    if args.vis:\n                        win = \'mIoU_epoch\'\n                        v_iou_expand = np.expand_dims(v_iou, axis=0)\n                        win_res = vis.line(X=np.ones(1)*epoch*args.val_interval, Y=v_iou_expand, win=win, update=\'append\')\n                        if win_res != win:\n                            vis.line(X=np.ones(1)*epoch*args.val_interval, Y=v_iou_expand, win=win, opts=dict(title=win, xlabel=\'epoch\', ylabel=\'mIoU\'))\n\n            for class_i in range(args.n_classes):\n                print(class_i, class_iou[class_i])\n            print(\'----ending   val----\')\n\n        # \xe6\x98\xbe\xe7\xa4\xba\xe5\xa4\x9a\xe4\xb8\xaa\xe5\x91\xa8\xe6\x9c\x9f\xe7\x9a\x84loss\xe6\x9b\xb2\xe7\xba\xbf\n        loss_avg_epoch = loss_epoch / (data_count * 1.0)\n        # print(loss_avg_epoch)\n        if args.vis:\n            win = \'loss_epoch\'\n            loss_avg_epoch_expand = np.expand_dims(loss_avg_epoch, axis=0)\n            win_res = vis.line(X=np.ones(1)*epoch, Y=loss_avg_epoch_expand, win=win, update=\'append\')\n            if win_res != win:\n                vis.line(X=np.ones(1)*epoch, Y=loss_avg_epoch_expand, win=win, opts=dict(title=win, xlabel=\'epoch\', ylabel=\'loss\'))\n\n        if args.vis:\n            win = \'lr_epoch\'\n            lr_epoch = np.array(scheduler.get_lr())\n            lr_epoch_expand = np.expand_dims(lr_epoch, axis=0)\n            win_res = vis.line(X=np.ones(1)*epoch, Y=lr_epoch_expand, win=win, update=\'append\')\n            if win_res != win:\n                vis.line(X=np.ones(1)*epoch, Y=lr_epoch_expand, win=win, opts=dict(title=win, xlabel=\'epoch\', ylabel=\'lr\'))\n\n        # ------------------train metris-------------------------------\n        if args.vis:\n            score, class_iou = scores(train_gts, train_preds, n_class=args.n_classes)\n            for k, v in score.items():\n                print(k, v)\n                if k == \'Overall Acc : \\t\':\n                    # \xe6\x98\xbe\xe7\xa4\xba\xe6\xa0\xa1\xe5\x87\x86\xe5\x91\xa8\xe6\x9c\x9f\xe7\x9a\x84mIoU\n                    overall_acc = v\n                    if args.vis:\n                        win = \'acc_epoch\'\n                        overall_acc_expand = np.expand_dims(overall_acc, axis=0)\n                        win_res = vis.line(X=np.ones(1) * epoch, Y=overall_acc_expand, win=win,\n                                           update=\'append\')\n                        if win_res != win:\n                            vis.line(X=np.ones(1) * epoch, Y=overall_acc_expand, win=win,\n                                     opts=dict(title=win, xlabel=\'epoch\', ylabel=\'accuracy\'))\n            # clear for new training metrics\n            train_gts, train_preds = [], []\n        # ------------------train metris-------------------------------\n\n        if args.save_model and epoch%args.save_epoch==0:\n            torch.save(model.state_dict(), \'{}_{}_class_{}_{}.pt\'.format(args.structure, args.dataset, args.n_classes, epoch))\n\n\n# best training: python train.py --resume_model fcn32s_camvid_9.pkl --save_model True\n# --init_vgg16 True --dataset_path /home/cgf/Data/CamVid --batch_size 1 --vis True\nif __name__==\'__main__\':\n    # print(\'train----in----\')\n    parser = argparse.ArgumentParser(description=\'training parameter setting\')\n    parser.add_argument(\'--structure\', type=str, default=\'ENetV2\', help=\'use the net structure to segment [ fcn_32s ResNetDUC segnet ENet drn_d_22 ]\')\n    parser.add_argument(\'--solver\', type=str, default=\'Adam\', help=\'use the solver to optimizer net [ SGD Adam RMSprop ]\')\n    parser.add_argument(\'--grad_acc_steps\', type=int, default=1, help=\'gpu memory not enough use grad accumulation act like large batch [ 1 ]\')\n    parser.add_argument(\'--resume_model\', type=str, default=\'\', help=\'resume model path [ fcn32s_camvid_9.pkl ]\')\n    parser.add_argument(\'--resume_model_state_dict\', type=str, default=\'\', help=\'resume model state dict path [ fcn32s_camvid_9.pt ]\')\n    parser.add_argument(\'--save_model\', type=bool, default=False, help=\'save model [ False ]\')\n    parser.add_argument(\'--save_epoch\', type=int, default=1, help=\'save model after epoch [ 1 ]\')\n    parser.add_argument(\'--training_epoch\', type=int, default=500, help=\'training epoch end training model [ 30000 ]\')\n    parser.add_argument(\'--init_vgg16\', type=bool, default=False, help=\'init model using vgg16 weights [ False ]\')\n    parser.add_argument(\'--dataset\', type=str, default=\'CamVid\', help=\'train dataset [ CamVid CityScapes FreeSpace SegmPred MovingMNIST ]\')\n    parser.add_argument(\'--dataset_path\', type=str, default=\'~/Data/CamVid\', help=\'train dataset path [ ~/Data/CamVid ~/Data/cityscapes ~/Data/FreeSpaceDataset ~/Data/SegmPred ~/Data/mnist_test_seq.npy]\')\n    parser.add_argument(\'--data_augment\', type=bool, default=True, help=\'enlarge the training data [ True False ]\')\n    parser.add_argument(\'--class_weighting\', type=str, default=\'MFB\', help=\'weighting class [ MFB ENET ]\')\n    parser.add_argument(\'--batch_size\', type=int, default=1, help=\'train dataset batch size [ 1 ]\')\n    parser.add_argument(\'--val_interval\', type=int, default=-1, help=\'val dataset interval unit epoch [ 3 ]\')\n    parser.add_argument(\'--n_classes\', type=int, default=12, help=\'train class num [ 12 ]\')\n    parser.add_argument(\'--lr\', type=float, default=1e-4, help=\'train learning rate [ 0.00001 ]\')\n    parser.add_argument(\'--lr_policy\', type=str, default=\'Polynomial\', help=\'train learning policy [ Constant Polynomial MultiStep ]\')\n    parser.add_argument(\'--vis\', type=bool, default=True, help=\'visualize the training results [ False ]\')\n    parser.add_argument(\'--cuda\', type=bool, default=False, help=\'use cuda [ False ]\')\n    args = parser.parse_args()\n    print(args)\n    train(args)\n    # print(\'train----out----\')\n'"
train_lrn.py,14,"b'# -*- coding: utf-8 -*-_resnet18_32s\nimport torch\nimport os\nimport argparse\n\nimport cv2\nimport time\nimport numpy as np\nimport visdom\nfrom torch.autograd import Variable\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau, StepLR, MultiStepLR\n\nfrom semseg.dataloader.camvid_loader import camvidLoader\nfrom semseg.dataloader.camvid_lrn_loader import camvidLRNLoader\nfrom semseg.dataloader.cityscapes_loader import cityscapesLoader\nfrom semseg.dataloader.freespace_loader import freespaceLoader\nfrom semseg.dataloader.movingmnist_loader import movingmnistLoader\nfrom semseg.dataloader.segmpred_loader import segmpredLoader\nfrom semseg.loss import cross_entropy2d\nfrom semseg.metrics import scores\nfrom semseg.modelloader.EDANet import EDANet\nfrom semseg.modelloader.bisenet import BiSeNet\nfrom semseg.modelloader.deconvnet import DeConvResNet50, DeConvResNet18\nfrom semseg.modelloader.deeplabv3 import Res_Deeplab_101, Res_Deeplab_50\nfrom semseg.modelloader.drn import drn_d_22, DRNSeg, drn_a_asymmetric_18, drn_a_asymmetric_ibn_a_18, drnseg_a_50, drnseg_a_18, drnseg_a_34, drnseg_e_22, drnseg_a_asymmetric_18, drnseg_a_asymmetric_ibn_a_18, drnseg_d_22, drnseg_d_38\nfrom semseg.modelloader.drn_a_irb import drnsegirb_a_18\nfrom semseg.modelloader.drn_a_refine import drnsegrefine_a_18\nfrom semseg.modelloader.duc_hdc import ResNetDUC, ResNetDUCHDC\nfrom semseg.modelloader.enet import ENet\nfrom semseg.modelloader.enetv2 import ENetV2\nfrom semseg.modelloader.erfnet import erfnet\nfrom semseg.modelloader.fc_densenet import fcdensenet103, fcdensenet56, fcdensenet_tiny\nfrom semseg.modelloader.fcn import fcn, fcn_32s, fcn_16s, fcn_8s\nfrom semseg.modelloader.fcn_mobilenet import fcn_MobileNet, fcn_MobileNet_32s, fcn_MobileNet_16s, fcn_MobileNet_8s\nfrom semseg.modelloader.fcn_resnet import fcn_resnet18, fcn_resnet34, fcn_resnet18_32s, fcn_resnet18_16s, \\\n    fcn_resnet18_8s, fcn_resnet34_32s, fcn_resnet34_16s, fcn_resnet34_8s, fcn_resnet50_32s, fcn_resnet50_16s, fcn_resnet50_8s\nfrom semseg.modelloader.fcn_shufflenet import fcn_shufflenet_32s, fcn_shufflenet_16s, fcn_shufflenet_8s\nfrom semseg.modelloader.gcn import gcn_resnet18, gcn_resnet34, gcn_resnet50, gcn_resnet101\nfrom semseg.modelloader.lrn import lrn_vgg16\nfrom semseg.modelloader.segnet import segnet, segnet_squeeze, segnet_alignres, segnet_vgg19\nfrom semseg.modelloader.segnet_unet import segnet_unet\nfrom semseg.modelloader.sqnet import sqnet\nfrom semseg.schedulers import ConstantLR, PolynomialLR\nfrom semseg.utils.get_class_weights import median_frequency_balancing, ENet_weighing\n\n\ndef train(args):\n    def type_callback(event):\n        # print(\'event_type:{}\'.format(event[\'event_type\']))\n        if event[\'event_type\'] == \'KeyPress\':\n            event_key = event[\'key\']\n            if event_key == \'Enter\':\n                pass\n                # print(\'event_type:Enter\')\n            elif event_key == \'Backspace\':\n                pass\n                # print(\'event_type:Backspace\')\n            elif event_key == \'Delete\':\n                pass\n                # print(\'event_type:Delete\')\n            elif len(event_key) == 1:\n                pass\n                # print(\'event_key:{}\'.format(event[\'key\']))\n                if event_key==\'s\':\n                    import json\n                    win = \'loss_iteration\'\n                    win_data = vis.get_window_data(win)\n                    win_data_dict = json.loads(win_data)\n                    win_data_content_dict = win_data_dict[\'content\']\n                    win_data_x = np.array(win_data_content_dict[\'data\'][0][\'x\'])\n                    win_data_y = np.array(win_data_content_dict[\'data\'][0][\'y\'])\n\n                    win_data_save_file = \'/tmp/loss_iteration_{}.txt\'.format(init_time)\n                    with open(win_data_save_file, \'wb\') as f:\n                        for item_x, item_y in zip(win_data_x, win_data_y):\n                            f.write(""{} {}\\n"".format(item_x, item_y))\n                    done_time = str(int(time.time()))\n                    vis.text(vis_text_usage+\'done at {}\'.format(done_time), win=callback_text_usage_window)\n\n    init_time = str(int(time.time()))\n    if args.vis:\n        # start visdom and close all window\n        vis = visdom.Visdom()\n        vis.close()\n\n        vis_text_usage = \'Operating in the text window<br>Press s to save data<br>\'\n        callback_text_usage_window = vis.text(vis_text_usage)\n        vis.register_event_handler(type_callback, callback_text_usage_window)\n\n    class_weight = None\n    local_path = os.path.expanduser(args.dataset_path)\n    train_dst = None\n    val_dst = None\n    if args.dataset == \'CamVid\':\n        train_dst = camvidLRNLoader(local_path, is_transform=True, is_augment=args.data_augment, split=\'train\')\n        val_dst = camvidLRNLoader(local_path, is_transform=True, is_augment=False, split=\'val\')\n\n        trainannot_image_dir = os.path.expanduser(os.path.join(local_path, ""trainannot""))\n        trainannot_image_files = [os.path.join(trainannot_image_dir, file) for file in os.listdir(trainannot_image_dir) if file.endswith(\'.png\')]\n        if args.class_weighting==\'MFB\':\n            class_weight = median_frequency_balancing(trainannot_image_files, num_classes=12)\n            class_weight = torch.tensor(class_weight)\n        elif args.class_weighting==\'ENET\':\n            class_weight = ENet_weighing(trainannot_image_files, num_classes=12)\n            class_weight = torch.tensor(class_weight)\n    elif args.dataset == \'CityScapes\':\n        train_dst = cityscapesLoader(local_path, is_transform=True, split=\'train\')\n        val_dst = cityscapesLoader(local_path, is_transform=True, split=\'val\')\n    elif args.dataset == \'SegmPred\':\n        train_dst = segmpredLoader(local_path, is_transform=True, split=\'train\')\n        val_dst = segmpredLoader(local_path, is_transform=True, split=\'train\')\n    elif args.dataset == \'MovingMNIST\':\n        # class_weight = [0.1, 0.5]\n        # class_weight = torch.tensor(class_weight)\n        train_dst = movingmnistLoader(local_path, is_transform=True, split=\'train\')\n        val_dst = movingmnistLoader(local_path, is_transform=True, split=\'val\')\n    elif args.dataset == \'FreeSpace\':\n        train_dst = freespaceLoader(local_path, is_transform=True, split=\'train\')\n        val_dst = freespaceLoader(local_path, is_transform=True, split=\'val\')\n    else:\n        print(\'{} dataset does not implement\'.format(args.dataset))\n        exit(0)\n\n    if args.cuda:\n        if class_weight is not None:\n            class_weight = class_weight.cuda()\n    print(\'class_weight:\', class_weight)\n\n    train_loader = torch.utils.data.DataLoader(train_dst, batch_size=args.batch_size, shuffle=True)\n    val_loader = torch.utils.data.DataLoader(val_dst, batch_size=1, shuffle=True)\n\n    start_epoch = 0\n    best_mIoU = 0\n    if args.resume_model != \'\':\n        model = torch.load(args.resume_model)\n        start_epoch_id1 = args.resume_model.rfind(\'_\')\n        start_epoch_id2 = args.resume_model.rfind(\'.\')\n        start_epoch = int(args.resume_model[start_epoch_id1+1:start_epoch_id2])\n    else:\n        # model = eval(args.structure)(n_classes=args.n_classes, pretrained=args.init_vgg16)\n        try:\n            model = eval(args.structure)(n_classes=args.n_classes, pretrained=args.init_vgg16)\n        except:\n            print(\'missing structure or not support\')\n            exit(0)\n\n        # ---------------for testing SegmPred---------------\n        if args.dataset == \'MovingMNIST\':\n            input_channel = 1*9\n        elif args.dataset == \'SegmPred\':\n            input_channel = 19*4\n        if args.structure == \'drnseg_a_18\':\n            model = drnseg_a_18(n_classes=args.n_classes, pretrained=args.init_vgg16, input_channel=input_channel)\n        # ---------------for testing SegmPred---------------\n\n        if args.resume_model_state_dict != \'\':\n            try:\n                # from model save format get useful information, such as miou, epoch\n                miou_model_name_str = \'_miou_\'\n                class_model_name_str = \'_class_\'\n                miou_id1 = args.resume_model_state_dict.find(miou_model_name_str)+len(miou_model_name_str)\n                miou_id2 = args.resume_model_state_dict.find(class_model_name_str)\n                best_mIoU = float(args.resume_model_state_dict[miou_id1:miou_id2])\n\n                start_epoch_id1 = args.resume_model_state_dict.rfind(\'_\')\n                start_epoch_id2 = args.resume_model_state_dict.rfind(\'.\')\n                start_epoch = int(args.resume_model_state_dict[start_epoch_id1 + 1:start_epoch_id2])\n                pretrained_dict = torch.load(args.resume_model_state_dict, map_location=\'cpu\')\n                model.load_state_dict(pretrained_dict)\n            except KeyError:\n                print(\'missing resume_model_state_dict or wrong type\')\n\n\n\n    if args.cuda:\n        model.cuda()\n    print(\'start_epoch:\', start_epoch)\n    print(\'best_mIoU:\', best_mIoU)\n\n    if args.solver == \'SGD\':\n        optimizer = torch.optim.SGD(filter(lambda p: p.requires_grad, model.parameters()), lr=args.lr, momentum=0.99, weight_decay=5e-4)\n    elif args.solver == \'RMSprop\':\n        optimizer = torch.optim.RMSprop(filter(lambda p: p.requires_grad, model.parameters()), lr=args.lr, momentum=0.99, weight_decay=5e-4)\n    elif args.solver == \'Adam\':\n        optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=args.lr, weight_decay=5e-4)\n    else:\n        print(\'missing solver or not support\')\n        exit(0)\n    # when observerd object dose not decrease scheduler will let the optimizer learing rate decrease\n    # scheduler = ReduceLROnPlateau(optimizer, \'min\', patience=100, min_lr=1e-10, verbose=True)\n    if args.lr_policy == \'Constant\':\n        scheduler = ConstantLR(optimizer)\n    elif args.lr_policy == \'Polynomial\':\n        scheduler = PolynomialLR(optimizer, max_iter=args.training_epoch, power=0.9) # base lr=0.01 power=0.9 like PSPNet\n    elif args.lr_policy == \'MultiStep\':\n        scheduler = MultiStepLR(optimizer, milestones=[10, 50, 90], gamma=0.1) # base lr=0.01 power=0.9 like PSPNet\n\n    # scheduler = StepLR(optimizer, step_size=1, gamma=0.1)\n\n    data_count = int(train_dst.__len__() * 1.0 / args.batch_size)\n    print(\'data_count:\', data_count)\n    # iteration_step = 0\n    train_gts, train_preds = [], []\n    for epoch in range(start_epoch+1, args.training_epoch, 1):\n        loss_epoch = 0\n        scheduler.step()\n\n        optimizer.zero_grad() # when train next time zero all grad, just acc the grad when the epoch training\n        for i, (imgs, labels) in enumerate(train_loader):\n            # break\n            imgs_origin = imgs[-1]\n            labels_origin = labels[-1]\n            # if i==1:\n            #     break\n            model.train()\n\n            # \xe6\x9c\x80\xe5\x90\x8e\xe7\x9a\x84\xe5\x87\xa0\xe5\xbc\xa0\xe5\x9b\xbe\xe7\x89\x87\xe5\x8f\xaf\xe8\x83\xbd\xe4\xb8\x8d\xe5\x88\xb0batch_size\xe7\x9a\x84\xe6\x95\xb0\xe9\x87\x8f\xef\xbc\x8c\xe6\xaf\x94\xe5\xa6\x82batch_size=4\xef\xbc\x8c\xe5\x8f\xaf\xe8\x83\xbd\xe5\x8f\xaa\xe5\x89\xa93\xe5\xbc\xa0\n            imgs_batch = imgs_origin.shape[0]\n            if imgs_batch != args.batch_size:\n                break\n            # iteration_step += 1\n\n            imgs_origin = Variable(imgs_origin)\n            labels_origin = Variable(labels_origin)\n\n            if args.cuda:\n                imgs_origin = imgs_origin.cuda()\n                labels_origin = labels_origin.cuda()\n            outputs = model(imgs_origin)\n            # print(\'imgs.size:\', imgs.size())\n            # print(\'labels.size:\', labels.size())\n\n            loss = 0\n            for size_index in range(6):\n                input_item = outputs[size_index]\n                target_item = labels[size_index]\n                # print(\'input_item.shape\', input_item.shape)\n                # print(\'target_item.shape\', target_item.shape)\n                if args.cuda:\n                    target_item = target_item.cuda()\n                loss += cross_entropy2d(input_item, target_item, weight=class_weight)\n            loss = loss/6.0\n            # print(\'loss:\', loss)\n\n            # add grad backward the avg loss\n            loss_grad_acc_avg = loss*1.0/args.grad_acc_steps\n            loss_grad_acc_avg.backward()\n\n            loss_np = loss.cpu().data.numpy()\n            loss_epoch += loss_np\n\n\n            if (i+1)%args.grad_acc_steps == 0:\n                optimizer.step()\n                # \xe4\xb8\x80\xe6\xac\xa1backward\xe5\x90\x8e\xe5\xa6\x82\xe6\x9e\x9c\xe4\xb8\x8d\xe6\xb8\x85\xe9\x9b\xb6\xef\xbc\x8c\xe6\xa2\xaf\xe5\xba\xa6\xe6\x98\xaf\xe7\xb4\xaf\xe5\x8a\xa0\xe7\x9a\x84\n                optimizer.zero_grad()\n\n            # ------------------train metris-------------------------------\n            train_pred = outputs[-1].cpu().data.max(1)[1].numpy()\n            train_gt = labels[-1].cpu().data.numpy()\n\n            for train_gt_, train_pred_ in zip(train_gt, train_pred):\n                train_gts.append(train_gt_)\n                train_preds.append(train_pred_)\n            # ------------------train metris-------------------------------\n\n            if args.vis and i%50==0:\n                pred_labels = outputs[-1].cpu().data.max(1)[1].numpy()\n                label_color = train_dst.decode_segmap(labels[-1].cpu().data.numpy()[0]).transpose(2, 0, 1)\n                pred_label_color = train_dst.decode_segmap(pred_labels[0]).transpose(2, 0, 1)\n                win = \'label_color\'\n                vis.image(label_color, win=win, opts=dict(title=\'Gt\', caption=\'Ground Truth\'))\n                win = \'pred_label_color\'\n                vis.image(pred_label_color, win=win, opts=dict(title=\'Pred\', caption=\'Prediction\'))\n\n            # \xe6\x98\xbe\xe7\xa4\xba\xe4\xb8\x80\xe4\xb8\xaa\xe5\x91\xa8\xe6\x9c\x9f\xe7\x9a\x84loss\xe6\x9b\xb2\xe7\xba\xbf\n            if args.vis:\n                win = \'loss_iteration\'\n                loss_np_expand = np.expand_dims(loss_np, axis=0)\n                win_res = vis.line(X=np.ones(1)*(i+data_count*(epoch-1)+1), Y=loss_np_expand, win=win, update=\'append\')\n                if win_res != win:\n                    vis.line(X=np.ones(1)*(i+data_count*(epoch-1)+1), Y=loss_np_expand, win=win, opts=dict(title=win, xlabel=\'iteration\', ylabel=\'loss\'))\n\n        # val result on val dataset and pick best to save\n        if args.val_interval > 0  and epoch % args.val_interval == 0:\n            print(\'----starting val----\')\n            model.eval()\n\n            val_gts, val_preds = [], []\n            for val_i, (val_imgs, val_labels) in enumerate(val_loader):\n                # print(val_i)\n                val_imgs = Variable(val_imgs[-1], volatile=True)\n                val_labels = Variable(val_labels[-1], volatile=True)\n\n                if args.cuda:\n                    val_imgs = val_imgs.cuda()\n                    val_labels = val_labels.cuda()\n\n                val_outputs = model(val_imgs)[-1]\n                val_pred = val_outputs.cpu().data.max(1)[1].numpy()\n                val_gt = val_labels.cpu().data.numpy()\n                for val_gt_, val_pred_ in zip(val_gt, val_pred):\n                    val_gts.append(val_gt_)\n                    val_preds.append(val_pred_)\n\n            score, class_iou = scores(val_gts, val_preds, n_class=args.n_classes)\n            for k, v in score.items():\n                print(k, v)\n                if k == \'Mean IoU : \\t\':\n                    v_iou = v\n                    if v > best_mIoU:\n                        best_mIoU = v_iou\n                        torch.save(model.state_dict(), \'{}_{}_miou_{}_class_{}_{}.pt\'.format(args.structure, args.dataset, best_mIoU, args.n_classes, epoch))\n                    # \xe6\x98\xbe\xe7\xa4\xba\xe6\xa0\xa1\xe5\x87\x86\xe5\x91\xa8\xe6\x9c\x9f\xe7\x9a\x84mIoU\n                    if args.vis:\n                        win = \'mIoU_epoch\'\n                        v_iou_expand = np.expand_dims(v_iou, axis=0)\n                        win_res = vis.line(X=np.ones(1)*epoch*args.val_interval, Y=v_iou_expand, win=win, update=\'append\')\n                        if win_res != win:\n                            vis.line(X=np.ones(1)*epoch*args.val_interval, Y=v_iou_expand, win=win, opts=dict(title=win, xlabel=\'epoch\', ylabel=\'mIoU\'))\n\n            for class_i in range(args.n_classes):\n                print(class_i, class_iou[class_i])\n            print(\'----ending   val----\')\n\n        # \xe6\x98\xbe\xe7\xa4\xba\xe5\xa4\x9a\xe4\xb8\xaa\xe5\x91\xa8\xe6\x9c\x9f\xe7\x9a\x84loss\xe6\x9b\xb2\xe7\xba\xbf\n        loss_avg_epoch = loss_epoch / (data_count * 1.0)\n        # print(loss_avg_epoch)\n        if args.vis:\n            win = \'loss_epoch\'\n            loss_avg_epoch_expand = np.expand_dims(loss_avg_epoch, axis=0)\n            win_res = vis.line(X=np.ones(1)*epoch, Y=loss_avg_epoch_expand, win=win, update=\'append\')\n            if win_res != win:\n                vis.line(X=np.ones(1)*epoch, Y=loss_avg_epoch_expand, win=win, opts=dict(title=win, xlabel=\'epoch\', ylabel=\'loss\'))\n\n        if args.vis:\n            win = \'lr_epoch\'\n            lr_epoch = np.array(scheduler.get_lr())\n            lr_epoch_expand = np.expand_dims(lr_epoch, axis=0)\n            win_res = vis.line(X=np.ones(1)*epoch, Y=lr_epoch_expand, win=win, update=\'append\')\n            if win_res != win:\n                vis.line(X=np.ones(1)*epoch, Y=lr_epoch_expand, win=win, opts=dict(title=win, xlabel=\'epoch\', ylabel=\'lr\'))\n\n        # ------------------train metris-------------------------------\n        if args.vis:\n            score, class_iou = scores(train_gts, train_preds, n_class=args.n_classes)\n            for k, v in score.items():\n                print(k, v)\n                if k == \'Overall Acc : \\t\':\n                    # \xe6\x98\xbe\xe7\xa4\xba\xe6\xa0\xa1\xe5\x87\x86\xe5\x91\xa8\xe6\x9c\x9f\xe7\x9a\x84mIoU\n                    overall_acc = v\n                    if args.vis:\n                        win = \'acc_epoch\'\n                        overall_acc_expand = np.expand_dims(overall_acc, axis=0)\n                        win_res = vis.line(X=np.ones(1) * epoch, Y=overall_acc_expand, win=win,\n                                           update=\'append\')\n                        if win_res != win:\n                            vis.line(X=np.ones(1) * epoch, Y=overall_acc_expand, win=win,\n                                     opts=dict(title=win, xlabel=\'epoch\', ylabel=\'accuracy\'))\n            # clear for new training metrics\n            train_gts, train_preds = [], []\n        # ------------------train metris-------------------------------\n\n        if args.save_model and epoch%args.save_epoch==0:\n            torch.save(model.state_dict(), \'{}_{}_class_{}_{}.pt\'.format(args.structure, args.dataset, args.n_classes, epoch))\n\n\n# best training: python train.py --resume_model fcn32s_camvid_9.pkl --save_model True\n# --init_vgg16 True --dataset_path /home/cgf/Data/CamVid --batch_size 1 --vis True\nif __name__==\'__main__\':\n    # print(\'train----in----\')\n    parser = argparse.ArgumentParser(description=\'training parameter setting\')\n    parser.add_argument(\'--structure\', type=str, default=\'lrn_vgg16\', help=\'use the net structure to segment [ fcn_32s ResNetDUC segnet ENet drn_d_22 ]\')\n    parser.add_argument(\'--solver\', type=str, default=\'Adam\', help=\'use the solver to optimizer net [ SGD Adam RMSprop ]\')\n    parser.add_argument(\'--grad_acc_steps\', type=int, default=1, help=\'gpu memory not enough use grad accumulation act like large batch [ 1 ]\')\n    parser.add_argument(\'--resume_model\', type=str, default=\'\', help=\'resume model path [ fcn32s_camvid_9.pkl ]\')\n    parser.add_argument(\'--resume_model_state_dict\', type=str, default=\'\', help=\'resume model state dict path [ fcn32s_camvid_9.pt ]\')\n    parser.add_argument(\'--save_model\', type=bool, default=False, help=\'save model [ False ]\')\n    parser.add_argument(\'--save_epoch\', type=int, default=1, help=\'save model after epoch [ 1 ]\')\n    parser.add_argument(\'--training_epoch\', type=int, default=500, help=\'training epoch end training model [ 30000 ]\')\n    parser.add_argument(\'--init_vgg16\', type=bool, default=False, help=\'init model using vgg16 weights [ False ]\')\n    parser.add_argument(\'--dataset\', type=str, default=\'CamVid\', help=\'train dataset [ CamVid CityScapes FreeSpace SegmPred MovingMNIST ]\')\n    parser.add_argument(\'--dataset_path\', type=str, default=\'~/Data/CamVid\', help=\'train dataset path [ ~/Data/CamVid ~/Data/cityscapes ~/Data/FreeSpaceDataset ~/Data/SegmPred ~/Data/mnist_test_seq.npy]\')\n    parser.add_argument(\'--data_augment\', type=bool, default=True, help=\'enlarge the training data [ True False ]\')\n    parser.add_argument(\'--class_weighting\', type=str, default=\'MFB\', help=\'weighting class [ MFB ENET ]\')\n    parser.add_argument(\'--batch_size\', type=int, default=1, help=\'train dataset batch size [ 1 ]\')\n    parser.add_argument(\'--val_interval\', type=int, default=-1, help=\'val dataset interval unit epoch [ 3 ]\')\n    parser.add_argument(\'--n_classes\', type=int, default=12, help=\'train class num [ 12 ]\')\n    parser.add_argument(\'--lr\', type=float, default=1e-4, help=\'train learning rate [ 0.00001 ]\')\n    parser.add_argument(\'--lr_policy\', type=str, default=\'Polynomial\', help=\'train learning policy [ Constant Polynomial MultiStep ]\')\n    parser.add_argument(\'--vis\', type=bool, default=False, help=\'visualize the training results [ False ]\')\n    parser.add_argument(\'--cuda\', type=bool, default=False, help=\'use cuda [ False ]\')\n    args = parser.parse_args()\n    print(args)\n    train(args)\n    # print(\'train----out----\')\n'"
train_mt.py,14,"b'# -*- coding: utf-8 -*-_resnet18_32s\nimport torch\nimport os\nimport argparse\n\nimport cv2\nimport time\nimport numpy as np\nimport visdom\nfrom torch.autograd import Variable\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau, StepLR\nfrom torchvision import transforms\n\nfrom semseg.dataloader.camvid_loader import camvidLoader\nfrom semseg.dataloader.cityscapes_loader import cityscapesLoader\nfrom semseg.dataloader.yolodataset_loader import yoloDataset\nfrom semseg.loss import cross_entropy2d\nfrom semseg.metrics import scores\nfrom semseg.modelloader.drn_a_mt import drnsegmt_a_18\nfrom semseg.schedulers import ConstantLR, PolynomialLR\nfrom semseg.utils.get_class_weights import median_frequency_balancing, ENet_weighing\nfrom semseg.yoloLoss import yoloLoss\n\n\ndef train(args):\n    def type_callback(event):\n        # print(\'event_type:{}\'.format(event[\'event_type\']))\n        if event[\'event_type\'] == \'KeyPress\':\n            event_key = event[\'key\']\n            if event_key == \'Enter\':\n                pass\n                # print(\'event_type:Enter\')\n            elif event_key == \'Backspace\':\n                pass\n                # print(\'event_type:Backspace\')\n            elif event_key == \'Delete\':\n                pass\n                # print(\'event_type:Delete\')\n            elif len(event_key) == 1:\n                pass\n                # print(\'event_key:{}\'.format(event[\'key\']))\n                if event_key==\'s\':\n                    import json\n                    win = \'loss_iteration\'\n                    win_data = vis.get_window_data(win)\n                    win_data_dict = json.loads(win_data)\n                    win_data_content_dict = win_data_dict[\'content\']\n                    win_data_x = np.array(win_data_content_dict[\'data\'][0][\'x\'])\n                    win_data_y = np.array(win_data_content_dict[\'data\'][0][\'y\'])\n\n                    win_data_save_file = \'/tmp/loss_iteration_{}.txt\'.format(init_time)\n                    with open(win_data_save_file, \'wb\') as f:\n                        for item_x, item_y in zip(win_data_x, win_data_y):\n                            f.write(""{} {}\\n"".format(item_x, item_y))\n                    done_time = str(int(time.time()))\n                    vis.text(vis_text_usage+\'done at {}\'.format(done_time), win=callback_text_usage_window)\n\n    init_time = str(int(time.time()))\n    if args.vis:\n        # start visdom and close all window\n        vis = visdom.Visdom()\n        vis.close()\n\n        vis_text_usage = \'Operating in the text window<br>Press s to save data<br>\'\n        callback_text_usage_window = vis.text(vis_text_usage)\n        vis.register_event_handler(type_callback, callback_text_usage_window)\n\n    class_weight = None\n    local_path = os.path.expanduser(args.dataset_path)\n    train_dst = None\n    val_dst = None\n    if args.dataset == \'CamVid\':\n        train_dst = camvidLoader(local_path, is_transform=True, is_augment=args.data_augment, split=\'train\')\n        val_dst = camvidLoader(local_path, is_transform=True, is_augment=False, split=\'val\')\n\n        trainannot_image_dir = os.path.expanduser(os.path.join(local_path, ""trainannot""))\n        trainannot_image_files = [os.path.join(trainannot_image_dir, file) for file in os.listdir(trainannot_image_dir) if file.endswith(\'.png\')]\n        if args.class_weighting==\'MFB\':\n            class_weight = median_frequency_balancing(trainannot_image_files, num_classes=12)\n            class_weight = torch.tensor(class_weight)\n        elif args.class_weighting==\'ENET\':\n            class_weight = ENet_weighing(trainannot_image_files, num_classes=12)\n            class_weight = torch.tensor(class_weight)\n\n    elif args.dataset == \'CityScapes\':\n        train_dst = cityscapesLoader(local_path, is_transform=True, split=\'train\')\n        val_dst = cityscapesLoader(local_path, is_transform=True, split=\'val\')\n    else:\n        print(\'{} dataset does not implement\'.format(args.dataset))\n        exit(0)\n\n    if args.cuda:\n        if class_weight is not None:\n            class_weight = class_weight.cuda()\n    print(\'class_weight:\', class_weight)\n\n    train_loader = torch.utils.data.DataLoader(train_dst, batch_size=args.batch_size, shuffle=True)\n    val_loader = torch.utils.data.DataLoader(val_dst, batch_size=1, shuffle=True)\n\n    yolo_B = 2\n    yolo_C = 2\n    yolo_S = 7\n    yolo_out_tensor_shape = yolo_B * 5 + yolo_C\n    print(\'yolo_out_tensor_shape:\', yolo_out_tensor_shape)\n    det_criterion = yoloLoss(yolo_S, yolo_B, yolo_C, 5, 0.5, args.cuda)\n\n    det_file_root = os.path.expanduser(\'~/Data/CamVid/train/\')\n    det_train_dst = yoloDataset(root=det_file_root, list_file=[\'camvid_det.txt\'], train=True, transform=[transforms.ToTensor()], yolo_out_tensor_shape=yolo_out_tensor_shape)\n    det_train_loader = torch.utils.data.DataLoader(det_train_dst, batch_size=1, shuffle=True, num_workers=4)\n\n    start_epoch = 0\n    best_mIoU = 0\n    if args.resume_model != \'\':\n        model = torch.load(args.resume_model)\n        start_epoch_id1 = args.resume_model.rfind(\'_\')\n        start_epoch_id2 = args.resume_model.rfind(\'.\')\n        start_epoch = int(args.resume_model[start_epoch_id1+1:start_epoch_id2])\n    else:\n        model = drnsegmt_a_18(pretrained=args.init_vgg16, n_classes=args.n_classes, det_tensor_num=yolo_out_tensor_shape)\n        if args.resume_model_state_dict != \'\':\n            try:\n                # from model save format get useful information, such as miou, epoch\n                miou_model_name_str = \'_miou_\'\n                class_model_name_str = \'_class_\'\n                miou_id1 = args.resume_model_state_dict.find(miou_model_name_str)+len(miou_model_name_str)\n                miou_id2 = args.resume_model_state_dict.find(class_model_name_str)\n                best_mIoU = float(args.resume_model_state_dict[miou_id1:miou_id2])\n\n                start_epoch_id1 = args.resume_model_state_dict.rfind(\'_\')\n                start_epoch_id2 = args.resume_model_state_dict.rfind(\'.\')\n                start_epoch = int(args.resume_model_state_dict[start_epoch_id1 + 1:start_epoch_id2])\n                pretrained_dict = torch.load(args.resume_model_state_dict, map_location=\'cpu\')\n                model.load_state_dict(pretrained_dict)\n            except KeyError:\n                print(\'missing resume_model_state_dict or wrong type\')\n\n\n\n    if args.cuda:\n        model.cuda()\n    print(\'start_epoch:\', start_epoch)\n    print(\'best_mIoU:\', best_mIoU)\n\n    optimizer = None\n    if args.solver == \'SGD\':\n        optimizer = torch.optim.SGD(filter(lambda p: p.requires_grad, model.parameters()), lr=args.lr, momentum=0.99, weight_decay=5e-4)\n    elif args.solver == \'RMSprop\':\n        optimizer = torch.optim.RMSprop(filter(lambda p: p.requires_grad, model.parameters()), lr=args.lr, momentum=0.99, weight_decay=5e-4)\n    elif args.solver == \'Adam\':\n        optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=args.lr, weight_decay=5e-4)\n    else:\n        print(\'missing solver or not support\')\n        exit(0)\n    # when observerd object dose not decrease scheduler will let the optimizer learing rate decrease\n    # scheduler = ReduceLROnPlateau(optimizer, \'min\', patience=100, min_lr=1e-10, verbose=True)\n    scheduler = None\n    if args.lr_policy == \'Constant\':\n        scheduler = ConstantLR(optimizer)\n    elif args.lr_policy == \'Polynomial\':\n        scheduler = PolynomialLR(optimizer, max_iter=args.training_epoch, power=0.9) # base lr=0.01 power=0.9 like PSPNet\n\n    # scheduler = StepLR(optimizer, step_size=1, gamma=0.1)\n\n    data_count = int(train_dst.__len__() * 1.0 / args.batch_size)\n    det_data_count = int(det_train_dst.__len__() * 1.0 / 1)\n    print(\'data_count:\', data_count)\n    # iteration_step = 0\n    train_gts, train_preds = [], []\n    for epoch in range(start_epoch+1, args.training_epoch, 1):\n        loss_epoch = 0\n        scheduler.step()\n\n        # ----for object detection----\n        for det_i, (det_imgs, det_labels, _) in enumerate(det_train_loader):\n            model.train()\n            # print(\'det_imgs.shape:\', det_imgs.shape)\n            # print(\'det_labels.shape:\', det_labels.shape)\n\n            det_imgs = Variable(det_imgs)\n            det_labels = Variable(det_labels)\n\n            if args.cuda:\n                det_imgs = det_imgs.cuda()\n                det_labels = det_labels.cuda()\n\n            _, outputs_det = model(det_imgs)\n            # print(\'outpust_det:\', outputs_det.shape)\n\n            det_loss = det_criterion(outputs_det, det_labels)\n            det_loss = 0.02 * det_loss # for balance with segment and detection\n            det_loss_np = det_loss.cpu().data.numpy()\n            optimizer.zero_grad()\n            det_loss.backward()\n            optimizer.step()\n\n            # \xe6\x98\xbe\xe7\xa4\xba\xe4\xb8\x80\xe4\xb8\xaa\xe5\x91\xa8\xe6\x9c\x9f\xe7\x9a\x84loss\xe6\x9b\xb2\xe7\xba\xbf\n            if args.vis:\n                win = \'det_loss_iteration\'\n                det_loss_np_expand = np.expand_dims(det_loss_np, axis=0)\n                win_res = vis.line(X=np.ones(1)*(det_i+det_data_count*(epoch-1)+1), Y=det_loss_np_expand, win=win, update=\'append\')\n                if win_res != win:\n                    vis.line(X=np.ones(1)*(det_i+det_data_count*(epoch-1)+1), Y=det_loss_np_expand, win=win, opts=dict(title=win, xlabel=\'iteration\', ylabel=\'loss\'))\n        # ----for object detection----\n\n        # ----for semantic segment----\n        for i, (imgs, labels) in enumerate(train_loader):\n            # if i==1:\n            #     break\n            model.train()\n\n            # \xe6\x9c\x80\xe5\x90\x8e\xe7\x9a\x84\xe5\x87\xa0\xe5\xbc\xa0\xe5\x9b\xbe\xe7\x89\x87\xe5\x8f\xaf\xe8\x83\xbd\xe4\xb8\x8d\xe5\x88\xb0batch_size\xe7\x9a\x84\xe6\x95\xb0\xe9\x87\x8f\xef\xbc\x8c\xe6\xaf\x94\xe5\xa6\x82batch_size=4\xef\xbc\x8c\xe5\x8f\xaf\xe8\x83\xbd\xe5\x8f\xaa\xe5\x89\xa93\xe5\xbc\xa0\n            imgs_batch = imgs.shape[0]\n            if imgs_batch != args.batch_size:\n                break\n            # iteration_step += 1\n\n            imgs = Variable(imgs)\n            labels = Variable(labels)\n\n            if args.cuda:\n                imgs = imgs.cuda()\n                labels = labels.cuda()\n            outputs_sem, _ = model(imgs)\n            # print(\'outputs_sem.shape:\', outputs_sem.shape)\n\n            # \xe4\xb8\x80\xe6\xac\xa1backward\xe5\x90\x8e\xe5\xa6\x82\xe6\x9e\x9c\xe4\xb8\x8d\xe6\xb8\x85\xe9\x9b\xb6\xef\xbc\x8c\xe6\xa2\xaf\xe5\xba\xa6\xe6\x98\xaf\xe7\xb4\xaf\xe5\x8a\xa0\xe7\x9a\x84\n            optimizer.zero_grad()\n\n            # print(\'outputs.size:\', outputs.size())\n            # print(\'labels.size:\', labels.size())\n\n            loss = cross_entropy2d(outputs_sem, labels, weight=class_weight)\n            loss_np = loss.cpu().data.numpy()\n            loss_epoch += loss_np\n            loss.backward()\n\n            optimizer.step()\n\n            # ------------------train metris-------------------------------\n            train_pred = outputs_sem.cpu().data.max(1)[1].numpy()\n            train_gt = labels.cpu().data.numpy()\n\n            for train_gt_, train_pred_ in zip(train_gt, train_pred):\n                train_gts.append(train_gt_)\n                train_preds.append(train_pred_)\n            # ------------------train metris-------------------------------\n\n            if args.vis and i%50==0:\n                pred_labels = outputs_sem.cpu().data.max(1)[1].numpy()\n                label_color = train_dst.decode_segmap(labels.cpu().data.numpy()[0]).transpose(2, 0, 1)\n                pred_label_color = train_dst.decode_segmap(pred_labels[0]).transpose(2, 0, 1)\n                win = \'label_color\'\n                vis.image(label_color, win=win, opts=dict(title=\'Gt\', caption=\'Ground Truth\'))\n                win = \'pred_label_color\'\n                vis.image(pred_label_color, win=win, opts=dict(title=\'Pred\', caption=\'Prediction\'))\n\n            # \xe6\x98\xbe\xe7\xa4\xba\xe4\xb8\x80\xe4\xb8\xaa\xe5\x91\xa8\xe6\x9c\x9f\xe7\x9a\x84loss\xe6\x9b\xb2\xe7\xba\xbf\n            if args.vis:\n                win = \'loss_iteration\'\n                loss_np_expand = np.expand_dims(loss_np, axis=0)\n                win_res = vis.line(X=np.ones(1)*(i+data_count*(epoch-1)+1), Y=loss_np_expand, win=win, update=\'append\')\n                if win_res != win:\n                    vis.line(X=np.ones(1)*(i+data_count*(epoch-1)+1), Y=loss_np_expand, win=win, opts=dict(title=win, xlabel=\'iteration\', ylabel=\'loss\'))\n        # ----for semantic segment----\n\n        # val result on val dataset and pick best to save\n        if args.val_interval > 0  and epoch % args.val_interval == 0:\n            print(\'----starting val----\')\n            model.eval()\n\n            val_gts, val_preds = [], []\n            for val_i, (val_imgs, val_labels) in enumerate(val_loader):\n                # print(val_i)\n                val_imgs = Variable(val_imgs)\n                val_labels = Variable(val_labels)\n\n                if args.cuda:\n                    val_imgs = val_imgs.cuda()\n                    val_labels = val_labels.cuda()\n\n                val_outputs_sem, _ = model(val_imgs)\n                val_pred = val_outputs_sem.cpu().data.max(1)[1].numpy()\n                val_gt = val_labels.cpu().data.numpy()\n                for val_gt_, val_pred_ in zip(val_gt, val_pred):\n                    val_gts.append(val_gt_)\n                    val_preds.append(val_pred_)\n\n            score, class_iou = scores(val_gts, val_preds, n_class=args.n_classes)\n            for k, v in score.items():\n                print(k, v)\n                if k == \'Mean IoU : \\t\':\n                    v_iou = v\n                    if v > best_mIoU:\n                        best_mIoU = v_iou\n                        torch.save(model.state_dict(), \'{}_{}_miou_{}_class_{}_{}.pt\'.format(args.structure, args.dataset, best_mIoU, args.n_classes, epoch))\n                    # \xe6\x98\xbe\xe7\xa4\xba\xe6\xa0\xa1\xe5\x87\x86\xe5\x91\xa8\xe6\x9c\x9f\xe7\x9a\x84mIoU\n                    if args.vis:\n                        win = \'mIoU_epoch\'\n                        v_iou_expand = np.expand_dims(v_iou, axis=0)\n                        win_res = vis.line(X=np.ones(1)*epoch*args.val_interval, Y=v_iou_expand, win=win, update=\'append\')\n                        if win_res != win:\n                            vis.line(X=np.ones(1)*epoch*args.val_interval, Y=v_iou_expand, win=win, opts=dict(title=win, xlabel=\'epoch\', ylabel=\'mIoU\'))\n\n            # for class_i in range(args.n_classes):\n            #     print(class_i, class_iou[class_i])\n            print(\'----ending   val----\')\n\n        # \xe6\x98\xbe\xe7\xa4\xba\xe5\xa4\x9a\xe4\xb8\xaa\xe5\x91\xa8\xe6\x9c\x9f\xe7\x9a\x84loss\xe6\x9b\xb2\xe7\xba\xbf\n        loss_avg_epoch = loss_epoch / (data_count * 1.0)\n        # print(loss_avg_epoch)\n        if args.vis:\n            win = \'loss_epoch\'\n            loss_avg_epoch_expand = np.expand_dims(loss_avg_epoch, axis=0)\n            win_res = vis.line(X=np.ones(1)*epoch, Y=loss_avg_epoch_expand, win=win, update=\'append\')\n            if win_res != win:\n                vis.line(X=np.ones(1)*epoch, Y=loss_avg_epoch_expand, win=win, opts=dict(title=win, xlabel=\'epoch\', ylabel=\'loss\'))\n\n        if args.vis:\n            win = \'lr_epoch\'\n            lr_epoch = np.array(scheduler.get_lr())\n            lr_epoch_expand = np.expand_dims(lr_epoch, axis=0)\n            win_res = vis.line(X=np.ones(1)*epoch, Y=lr_epoch_expand, win=win, update=\'append\')\n            if win_res != win:\n                vis.line(X=np.ones(1)*epoch, Y=lr_epoch_expand, win=win, opts=dict(title=win, xlabel=\'epoch\', ylabel=\'lr\'))\n\n        # ------------------train metris-------------------------------\n        if args.vis:\n            score, class_iou = scores(train_gts, train_preds, n_class=args.n_classes)\n            for k, v in score.items():\n                print(k, v)\n                if k == \'Overall Acc : \\t\':\n                    # \xe6\x98\xbe\xe7\xa4\xba\xe6\xa0\xa1\xe5\x87\x86\xe5\x91\xa8\xe6\x9c\x9f\xe7\x9a\x84mIoU\n                    overall_acc = v\n                    if args.vis:\n                        win = \'acc_epoch\'\n                        overall_acc_expand = np.expand_dims(overall_acc, axis=0)\n                        win_res = vis.line(X=np.ones(1) * epoch, Y=overall_acc_expand, win=win,\n                                           update=\'append\')\n                        if win_res != win:\n                            vis.line(X=np.ones(1) * epoch, Y=overall_acc_expand, win=win,\n                                     opts=dict(title=win, xlabel=\'epoch\', ylabel=\'accuracy\'))\n            # clear for new training metrics\n            train_gts, train_preds = [], []\n        # ------------------train metris-------------------------------\n\n        if args.save_model and epoch%args.save_epoch==0:\n            torch.save(model.state_dict(), \'{}_{}_class_{}_{}.pt\'.format(args.structure, args.dataset, args.n_classes, epoch))\n\n\n# best training: python train.py --resume_model fcn32s_camvid_9.pkl --save_model True\n# --init_vgg16 True --dataset_path /home/cgf/Data/CamVid --batch_size 1 --vis True\nif __name__==\'__main__\':\n    # print(\'train----in----\')\n    parser = argparse.ArgumentParser(description=\'training parameter setting\')\n    parser.add_argument(\'--structure\', type=str, default=\'ENetV2\', help=\'use the net structure to segment [ fcn_32s ResNetDUC segnet ENet drn_d_22 ]\')\n    parser.add_argument(\'--solver\', type=str, default=\'SGD\', help=\'use the solver to optimizer net [ SGD ]\')\n    parser.add_argument(\'--resume_model\', type=str, default=\'\', help=\'resume model path [ fcn32s_camvid_9.pkl ]\')\n    parser.add_argument(\'--resume_model_state_dict\', type=str, default=\'\', help=\'resume model state dict path [ fcn32s_camvid_9.pt ]\')\n    parser.add_argument(\'--save_model\', type=bool, default=False, help=\'save model [ False ]\')\n    parser.add_argument(\'--save_epoch\', type=int, default=1, help=\'save model after epoch [ 1 ]\')\n    parser.add_argument(\'--training_epoch\', type=int, default=500, help=\'training epoch end training model [ 30000 ]\')\n    parser.add_argument(\'--init_vgg16\', type=bool, default=False, help=\'init model using vgg16 weights [ False ]\')\n    parser.add_argument(\'--dataset\', type=str, default=\'CamVid\', help=\'train dataset [ CamVid CityScapes ]\')\n    parser.add_argument(\'--dataset_path\', type=str, default=\'~/Data/CamVid\', help=\'train dataset path [ ~/Data/CamVid ~/Data/cityscapes ]\')\n    parser.add_argument(\'--data_augment\', type=bool, default=True, help=\'enlarge the training data [ True False ]\')\n    parser.add_argument(\'--class_weighting\', type=str, default=\'MFB\', help=\'weighting class [ MFB ENET ]\')\n    parser.add_argument(\'--batch_size\', type=int, default=1, help=\'train dataset batch size [ 1 ]\')\n    parser.add_argument(\'--val_interval\', type=int, default=-1, help=\'val dataset interval unit epoch [ 3 ]\')\n    parser.add_argument(\'--n_classes\', type=int, default=12, help=\'train class num [ 12 ]\')\n    parser.add_argument(\'--lr\', type=float, default=1e-4, help=\'train learning rate [ 0.00001 ]\')\n    parser.add_argument(\'--lr_policy\', type=str, default=\'Polynomial\', help=\'train learning policy [ Constant Polynomial ]\')\n    parser.add_argument(\'--vis\', type=bool, default=False, help=\'visualize the training results [ False ]\')\n    parser.add_argument(\'--cuda\', type=bool, default=False, help=\'use cuda [ False ]\')\n    args = parser.parse_args()\n    print(args)\n    train(args)\n    # print(\'train----out----\')\n'"
train_pred.py,12,"b'# -*- coding: utf-8 -*-_resnet18_32s\nimport torch\nimport os\nimport argparse\n\nimport cv2\nimport time\nimport numpy as np\nimport visdom\nfrom torch.autograd import Variable\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau, StepLR, MultiStepLR\n\nfrom semseg.dataloader.camvid_loader import camvidLoader\nfrom semseg.dataloader.cityscapes_loader import cityscapesLoader\nfrom semseg.dataloader.freespace_loader import freespaceLoader\nfrom semseg.dataloader.freespacepred_loader import freespacepredLoader\nfrom semseg.dataloader.movingmnist_loader import movingmnistLoader\nfrom semseg.dataloader.segmpred_loader import segmpredLoader\nfrom semseg.loss import cross_entropy2d\nfrom semseg.metrics import scores\nfrom semseg.modelloader.drn_pred import drnsegpred_a_18, drnpred_a_101, drnsegpred_a_101, drnsegpred_a_34\nfrom semseg.schedulers import ConstantLR, PolynomialLR\nfrom semseg.utils.get_class_weights import median_frequency_balancing, ENet_weighing\n\n\ndef train(args):\n    def type_callback(event):\n        # print(\'event_type:{}\'.format(event[\'event_type\']))\n        if event[\'event_type\'] == \'KeyPress\':\n            event_key = event[\'key\']\n            if event_key == \'Enter\':\n                pass\n                # print(\'event_type:Enter\')\n            elif event_key == \'Backspace\':\n                pass\n                # print(\'event_type:Backspace\')\n            elif event_key == \'Delete\':\n                pass\n                # print(\'event_type:Delete\')\n            elif len(event_key) == 1:\n                pass\n                # print(\'event_key:{}\'.format(event[\'key\']))\n                if event_key==\'s\':\n                    import json\n                    win = \'loss_iteration\'\n                    win_data = vis.get_window_data(win)\n                    win_data_dict = json.loads(win_data)\n                    win_data_content_dict = win_data_dict[\'content\']\n                    win_data_x = np.array(win_data_content_dict[\'data\'][0][\'x\'])\n                    win_data_y = np.array(win_data_content_dict[\'data\'][0][\'y\'])\n\n                    win_data_save_file = \'/tmp/loss_iteration_{}.txt\'.format(init_time)\n                    with open(win_data_save_file, \'wb\') as f:\n                        for item_x, item_y in zip(win_data_x, win_data_y):\n                            f.write(""{} {}\\n"".format(item_x, item_y))\n                    done_time = str(int(time.time()))\n                    vis.text(vis_text_usage+\'done at {}\'.format(done_time), win=callback_text_usage_window)\n\n    init_time = str(int(time.time()))\n    if args.vis:\n        # start visdom and close all window\n        vis = visdom.Visdom()\n        vis.close()\n\n        vis_text_usage = \'Operating in the text window<br>Press s to save data<br>\'\n        callback_text_usage_window = vis.text(vis_text_usage)\n        vis.register_event_handler(type_callback, callback_text_usage_window)\n\n    class_weight = None\n    local_path = os.path.expanduser(args.dataset_path)\n    train_dst = None\n    val_dst = None\n    if args.dataset == \'SegmPred\':\n        input_channel = 19\n        train_dst = segmpredLoader(local_path, is_transform=True, split=\'train\')\n        val_dst = segmpredLoader(local_path, is_transform=True, split=\'val\')\n    elif args.dataset == \'MovingMNIST\':\n        # class_weight = [0.1, 0.5]\n        # class_weight = torch.tensor(class_weight)\n        train_dst = movingmnistLoader(local_path, is_transform=True, split=\'train\')\n        val_dst = movingmnistLoader(local_path, is_transform=True, split=\'val\')\n    elif args.dataset == \'FreeSpacePred\':\n        input_channel = 1\n        train_dst = freespacepredLoader(local_path, is_transform=True, split=\'train\')\n        val_dst = freespacepredLoader(local_path, is_transform=True, split=\'test\')\n    else:\n        print(\'{} dataset does not implement\'.format(args.dataset))\n        exit(0)\n\n    if args.cuda:\n        if class_weight is not None:\n            class_weight = class_weight.cuda()\n    print(\'class_weight:\', class_weight)\n\n    train_loader = torch.utils.data.DataLoader(train_dst, batch_size=args.batch_size, shuffle=True)\n    val_loader = torch.utils.data.DataLoader(val_dst, batch_size=1, shuffle=True)\n\n    start_epoch = 0\n    best_mIoU = 0\n    if args.resume_model != \'\':\n        model = torch.load(args.resume_model)\n        start_epoch_id1 = args.resume_model.rfind(\'_\')\n        start_epoch_id2 = args.resume_model.rfind(\'.\')\n        start_epoch = int(args.resume_model[start_epoch_id1+1:start_epoch_id2])\n    else:\n        # ---------------for testing SegmPred---------------\n        model = eval(args.structure)(n_classes=args.n_classes, pretrained=args.init_vgg16, input_shape=train_dst.input_shape, input_channel=input_channel)\n        val_model = eval(args.structure)(n_classes=args.n_classes, pretrained=args.init_vgg16, input_shape=val_dst.input_shape, input_channel=input_channel)\n        # ---------------for testing SegmPred---------------\n\n        if args.resume_model_state_dict != \'\':\n            try:\n                # from model save format get useful information, such as miou, epoch\n                miou_model_name_str = \'_miou_\'\n                class_model_name_str = \'_class_\'\n                miou_id1 = args.resume_model_state_dict.find(miou_model_name_str)+len(miou_model_name_str)\n                miou_id2 = args.resume_model_state_dict.find(class_model_name_str)\n                best_mIoU = float(args.resume_model_state_dict[miou_id1:miou_id2])\n\n                start_epoch_id1 = args.resume_model_state_dict.rfind(\'_\')\n                start_epoch_id2 = args.resume_model_state_dict.rfind(\'.\')\n                start_epoch = int(args.resume_model_state_dict[start_epoch_id1 + 1:start_epoch_id2])\n                pretrained_dict = torch.load(args.resume_model_state_dict, map_location=\'cpu\')\n                model.load_state_dict(pretrained_dict)\n            except KeyError:\n                print(\'missing resume_model_state_dict or wrong type\')\n\n    if args.cuda:\n        model.cuda()\n        val_model.cuda()\n    print(\'start_epoch:\', start_epoch)\n    print(\'best_mIoU:\', best_mIoU)\n\n    if args.solver == \'SGD\':\n        optimizer = torch.optim.SGD(filter(lambda p: p.requires_grad, model.parameters()), lr=args.lr, momentum=0.99, weight_decay=5e-4)\n    elif args.solver == \'RMSprop\':\n        optimizer = torch.optim.RMSprop(filter(lambda p: p.requires_grad, model.parameters()), lr=args.lr, momentum=0.99, weight_decay=5e-4)\n    elif args.solver == \'Adam\':\n        optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=args.lr, weight_decay=5e-4)\n    else:\n        print(\'missing solver or not support\')\n        exit(0)\n    # when observerd object dose not decrease scheduler will let the optimizer learing rate decrease\n    # scheduler = ReduceLROnPlateau(optimizer, \'min\', patience=100, min_lr=1e-10, verbose=True)\n    if args.lr_policy == \'Constant\':\n        scheduler = ConstantLR(optimizer)\n    elif args.lr_policy == \'Polynomial\':\n        scheduler = PolynomialLR(optimizer, max_iter=args.training_epoch, power=0.9) # base lr=0.01 power=0.9 like PSPNet\n    elif args.lr_policy == \'MultiStep\':\n        scheduler = MultiStepLR(optimizer, milestones=[10, 50, 90], gamma=0.1) # base lr=0.01 power=0.9 like PSPNet\n\n    # scheduler = StepLR(optimizer, step_size=1, gamma=0.1)\n\n    data_count = int(train_dst.__len__() * 1.0 / args.batch_size)\n    print(\'data_count:\', data_count)\n    # iteration_step = 0\n    train_gts, train_preds = [], []\n    for epoch in range(start_epoch+1, args.training_epoch, 1):\n        loss_epoch = 0\n        scheduler.step()\n\n        optimizer.zero_grad() # when train next time zero all grad, just acc the grad when the epoch training\n        for i, (imgs, labels) in enumerate(train_loader):\n            # if i==1:\n            #     break\n            model.train()\n\n            # \xe6\x9c\x80\xe5\x90\x8e\xe7\x9a\x84\xe5\x87\xa0\xe5\xbc\xa0\xe5\x9b\xbe\xe7\x89\x87\xe5\x8f\xaf\xe8\x83\xbd\xe4\xb8\x8d\xe5\x88\xb0batch_size\xe7\x9a\x84\xe6\x95\xb0\xe9\x87\x8f\xef\xbc\x8c\xe6\xaf\x94\xe5\xa6\x82batch_size=4\xef\xbc\x8c\xe5\x8f\xaf\xe8\x83\xbd\xe5\x8f\xaa\xe5\x89\xa93\xe5\xbc\xa0\n            imgs_batch = imgs.shape[0]\n            if imgs_batch != args.batch_size:\n                break\n            # iteration_step += 1\n\n            imgs = Variable(imgs)\n            labels = Variable(labels)\n\n            if args.cuda:\n                imgs = imgs.cuda()\n                labels = labels.cuda()\n            outputs = model(imgs)\n            # print(\'imgs.size:\', imgs.size())\n            # print(\'labels.size:\', labels.size())\n            # print(\'outputs.size:\', outputs.size())\n\n            loss = cross_entropy2d(outputs, labels, weight=class_weight)\n\n            # add grad backward the avg loss\n            loss_grad_acc_avg = loss*1.0/args.grad_acc_steps\n            loss_grad_acc_avg.backward()\n\n            loss_np = loss.cpu().data.numpy()\n            loss_epoch += loss_np\n\n\n            if (i+1)%args.grad_acc_steps == 0:\n                optimizer.step()\n                # \xe4\xb8\x80\xe6\xac\xa1backward\xe5\x90\x8e\xe5\xa6\x82\xe6\x9e\x9c\xe4\xb8\x8d\xe6\xb8\x85\xe9\x9b\xb6\xef\xbc\x8c\xe6\xa2\xaf\xe5\xba\xa6\xe6\x98\xaf\xe7\xb4\xaf\xe5\x8a\xa0\xe7\x9a\x84\n                optimizer.zero_grad()\n\n            # ------------------train metris-------------------------------\n            train_pred = outputs.cpu().data.max(1)[1].numpy()\n            train_gt = labels.cpu().data.numpy()\n\n            for train_gt_, train_pred_ in zip(train_gt, train_pred):\n                train_gts.append(train_gt_)\n                train_preds.append(train_pred_)\n            # ------------------train metris-------------------------------\n\n            if args.vis and i%50==0:\n                pred_labels = outputs.cpu().data.max(1)[1].numpy()\n                label_color = train_dst.decode_segmap(labels.cpu().data.numpy()[0]).transpose(2, 0, 1)\n                pred_label_color = train_dst.decode_segmap(pred_labels[0]).transpose(2, 0, 1)\n                win = \'label_color\'\n                vis.image(label_color, win=win, opts=dict(title=\'Gt\', caption=\'Ground Truth\'))\n                win = \'pred_label_color\'\n                vis.image(pred_label_color, win=win, opts=dict(title=\'Pred\', caption=\'Prediction\'))\n\n            # \xe6\x98\xbe\xe7\xa4\xba\xe4\xb8\x80\xe4\xb8\xaa\xe5\x91\xa8\xe6\x9c\x9f\xe7\x9a\x84loss\xe6\x9b\xb2\xe7\xba\xbf\n            if args.vis:\n                win = \'loss_iteration\'\n                loss_np_expand = np.expand_dims(loss_np, axis=0)\n                win_res = vis.line(X=np.ones(1)*(i+data_count*(epoch-1)+1), Y=loss_np_expand, win=win, update=\'append\')\n                if win_res != win:\n                    vis.line(X=np.ones(1)*(i+data_count*(epoch-1)+1), Y=loss_np_expand, win=win, opts=dict(title=win, xlabel=\'iteration\', ylabel=\'loss\'))\n\n        # val result on val dataset and pick best to save\n        if args.val_interval > 0  and epoch % args.val_interval == 0:\n            print(\'----starting val----\')\n            # model.eval()\n            val_model.load_state_dict(model.state_dict())\n            val_model.eval()\n\n            val_gts, val_preds = [], []\n            for val_i, (val_imgs, val_labels) in enumerate(val_loader):\n                # print(val_i)\n                val_imgs = Variable(val_imgs, volatile=True)\n                val_labels = Variable(val_labels, volatile=True)\n\n                if args.cuda:\n                    val_imgs = val_imgs.cuda()\n                    val_labels = val_labels.cuda()\n\n                # val_outputs = model(val_imgs)\n                val_outputs = val_model(val_imgs)\n                val_pred = val_outputs.cpu().data.max(1)[1].numpy()\n                val_gt = val_labels.cpu().data.numpy()\n                for val_gt_, val_pred_ in zip(val_gt, val_pred):\n                    val_gts.append(val_gt_)\n                    val_preds.append(val_pred_)\n\n            score, class_iou = scores(val_gts, val_preds, n_class=args.n_classes)\n            for k, v in score.items():\n                print(k, v)\n                if k == \'Mean IoU : \\t\':\n                    v_iou = v\n                    if v > best_mIoU:\n                        best_mIoU = v_iou\n                        torch.save(model.state_dict(), \'{}_{}_miou_{}_class_{}_{}.pt\'.format(args.structure, args.dataset, best_mIoU, args.n_classes, epoch))\n                    # \xe6\x98\xbe\xe7\xa4\xba\xe6\xa0\xa1\xe5\x87\x86\xe5\x91\xa8\xe6\x9c\x9f\xe7\x9a\x84mIoU\n                    if args.vis:\n                        win = \'mIoU_epoch\'\n                        v_iou_expand = np.expand_dims(v_iou, axis=0)\n                        win_res = vis.line(X=np.ones(1)*epoch*args.val_interval, Y=v_iou_expand, win=win, update=\'append\')\n                        if win_res != win:\n                            vis.line(X=np.ones(1)*epoch*args.val_interval, Y=v_iou_expand, win=win, opts=dict(title=win, xlabel=\'epoch\', ylabel=\'mIoU\'))\n\n            for class_i in range(args.n_classes):\n                print(class_i, class_iou[class_i])\n            print(\'----ending   val----\')\n\n        # \xe6\x98\xbe\xe7\xa4\xba\xe5\xa4\x9a\xe4\xb8\xaa\xe5\x91\xa8\xe6\x9c\x9f\xe7\x9a\x84loss\xe6\x9b\xb2\xe7\xba\xbf\n        loss_avg_epoch = loss_epoch / (data_count * 1.0)\n        # print(loss_avg_epoch)\n        if args.vis:\n            win = \'loss_epoch\'\n            loss_avg_epoch_expand = np.expand_dims(loss_avg_epoch, axis=0)\n            win_res = vis.line(X=np.ones(1)*epoch, Y=loss_avg_epoch_expand, win=win, update=\'append\')\n            if win_res != win:\n                vis.line(X=np.ones(1)*epoch, Y=loss_avg_epoch_expand, win=win, opts=dict(title=win, xlabel=\'epoch\', ylabel=\'loss\'))\n\n        if args.vis:\n            win = \'lr_epoch\'\n            lr_epoch = np.array(scheduler.get_lr())\n            lr_epoch_expand = np.expand_dims(lr_epoch, axis=0)\n            win_res = vis.line(X=np.ones(1)*epoch, Y=lr_epoch_expand, win=win, update=\'append\')\n            if win_res != win:\n                vis.line(X=np.ones(1)*epoch, Y=lr_epoch_expand, win=win, opts=dict(title=win, xlabel=\'epoch\', ylabel=\'lr\'))\n\n        # ------------------train metris-------------------------------\n        if args.vis:\n            score, class_iou = scores(train_gts, train_preds, n_class=args.n_classes)\n            for k, v in score.items():\n                print(k, v)\n                if k == \'Overall Acc : \\t\':\n                    # \xe6\x98\xbe\xe7\xa4\xba\xe6\xa0\xa1\xe5\x87\x86\xe5\x91\xa8\xe6\x9c\x9f\xe7\x9a\x84mIoU\n                    overall_acc = v\n                    if args.vis:\n                        win = \'acc_epoch\'\n                        overall_acc_expand = np.expand_dims(overall_acc, axis=0)\n                        win_res = vis.line(X=np.ones(1) * epoch, Y=overall_acc_expand, win=win,\n                                           update=\'append\')\n                        if win_res != win:\n                            vis.line(X=np.ones(1) * epoch, Y=overall_acc_expand, win=win,\n                                     opts=dict(title=win, xlabel=\'epoch\', ylabel=\'accuracy\'))\n            # clear for new training metrics\n            train_gts, train_preds = [], []\n        # ------------------train metris-------------------------------\n\n        if args.save_model and epoch%args.save_epoch==0:\n            torch.save(model.state_dict(), \'{}_{}_class_{}_{}.pt\'.format(args.structure, args.dataset, args.n_classes, epoch))\n\n\n# best training: python train.py --resume_model fcn32s_camvid_9.pkl --save_model True\n# --init_vgg16 True --dataset_path /home/cgf/Data/CamVid --batch_size 1 --vis True\nif __name__==\'__main__\':\n    # print(\'train----in----\')\n    parser = argparse.ArgumentParser(description=\'training parameter setting\')\n    parser.add_argument(\'--structure\', type=str, default=\'drnsegpred_a_18\', help=\'use the net structure to segment [ fcn_32s ResNetDUC segnet ENet drn_d_22 ]\')\n    parser.add_argument(\'--solver\', type=str, default=\'Adam\', help=\'use the solver to optimizer net [ SGD Adam RMSprop ]\')\n    parser.add_argument(\'--grad_acc_steps\', type=int, default=1, help=\'gpu memory not enough use grad accumulation act like large batch [ 1 ]\')\n    parser.add_argument(\'--resume_model\', type=str, default=\'\', help=\'resume model path [ fcn32s_camvid_9.pkl ]\')\n    parser.add_argument(\'--resume_model_state_dict\', type=str, default=\'\', help=\'resume model state dict path [ fcn32s_camvid_9.pt ]\')\n    parser.add_argument(\'--save_model\', type=bool, default=False, help=\'save model [ False ]\')\n    parser.add_argument(\'--save_epoch\', type=int, default=1, help=\'save model after epoch [ 1 ]\')\n    parser.add_argument(\'--training_epoch\', type=int, default=500, help=\'training epoch end training model [ 30000 ]\')\n    parser.add_argument(\'--init_vgg16\', type=bool, default=False, help=\'init model using vgg16 weights [ False ]\')\n    parser.add_argument(\'--dataset\', type=str, default=\'CamVid\', help=\'train dataset [ CamVid CityScapes FreeSpace SegmPred MovingMNIST ]\')\n    parser.add_argument(\'--dataset_path\', type=str, default=\'~/Data/CamVid\', help=\'train dataset path [ ~/Data/CamVid ~/Data/cityscapes ~/Data/FreeSpaceDataset ~/Data/SegmPred ~/Data/mnist_test_seq.npy]\')\n    parser.add_argument(\'--data_augment\', type=bool, default=True, help=\'enlarge the training data [ True False ]\')\n    parser.add_argument(\'--class_weighting\', type=str, default=\'MFB\', help=\'weighting class [ MFB ENET ]\')\n    parser.add_argument(\'--batch_size\', type=int, default=1, help=\'train dataset batch size [ 1 ]\')\n    parser.add_argument(\'--val_interval\', type=int, default=-1, help=\'val dataset interval unit epoch [ 3 ]\')\n    parser.add_argument(\'--n_classes\', type=int, default=12, help=\'train class num [ 12 ]\')\n    parser.add_argument(\'--lr\', type=float, default=1e-4, help=\'train learning rate [ 0.00001 ]\')\n    parser.add_argument(\'--lr_policy\', type=str, default=\'Polynomial\', help=\'train learning policy [ Constant Polynomial MultiStep ]\')\n    parser.add_argument(\'--vis\', type=bool, default=False, help=\'visualize the training results [ False ]\')\n    parser.add_argument(\'--cuda\', type=bool, default=False, help=\'use cuda [ False ]\')\n    args = parser.parse_args()\n    print(args)\n    train(args)\n    # print(\'train----out----\')\n'"
validate.py,4,"b""# -*- coding: utf-8 -*-\nimport torch\nimport os\nimport argparse\n\nimport cv2\nimport time\nimport numpy as np\nimport visdom\nfrom torch.autograd import Variable\nfrom scipy import misc\n\nfrom semseg.dataloader.camvid_loader import camvidLoader\nfrom semseg.dataloader.cityscapes_loader import cityscapesLoader\nfrom semseg.dataloader.freespace_loader import freespaceLoader\nfrom semseg.loss import cross_entropy2d\nfrom semseg.metrics import scores\nfrom semseg.modelloader.EDANet import EDANet\nfrom semseg.modelloader.bisenet import BiSeNet\nfrom semseg.modelloader.deeplabv3 import Res_Deeplab_101, Res_Deeplab_50\nfrom semseg.modelloader.drn import drn_d_22, DRNSeg, drn_a_asymmetric_18, drn_a_asymmetric_ibn_a_18, drnseg_a_50, drnseg_a_18, drnseg_a_34, drnseg_e_22, drnseg_a_asymmetric_18, drnseg_a_asymmetric_ibn_a_18, drnseg_d_22, drnseg_d_38\nfrom semseg.modelloader.drn_a_irb import drnsegirb_a_18\nfrom semseg.modelloader.drn_a_refine import drnsegrefine_a_18\nfrom semseg.modelloader.duc_hdc import ResNetDUC, ResNetDUCHDC\nfrom semseg.modelloader.enet import ENet\nfrom semseg.modelloader.enetv2 import ENetV2\nfrom semseg.modelloader.erfnet import erfnet\nfrom semseg.modelloader.fc_densenet import fcdensenet103, fcdensenet56, fcdensenet_tiny\nfrom semseg.modelloader.fcn import fcn, fcn_32s, fcn_16s, fcn_8s\nfrom semseg.modelloader.fcn_mobilenet import fcn_MobileNet, fcn_MobileNet_32s, fcn_MobileNet_16s, fcn_MobileNet_8s\nfrom semseg.modelloader.fcn_resnet import fcn_resnet18, fcn_resnet34, fcn_resnet18_32s, fcn_resnet18_16s, \\\n    fcn_resnet18_8s, fcn_resnet34_32s, fcn_resnet34_16s, fcn_resnet34_8s, fcn_resnet50_32s, fcn_resnet50_16s, fcn_resnet50_8s\nfrom semseg.modelloader.lrn import lrn_vgg16\nfrom semseg.modelloader.segnet import segnet, segnet_squeeze, segnet_alignres, segnet_vgg19\nfrom semseg.modelloader.segnet_unet import segnet_unet\nfrom semseg.modelloader.sqnet import sqnet\n\n\ndef validate(args):\n    init_time = str(int(time.time()))\n    if args.vis:\n        vis = visdom.Visdom()\n    if args.dataset_path == '':\n        HOME_PATH = os.path.expanduser('~')\n        local_path = os.path.join(HOME_PATH, 'Data/CamVid')\n    else:\n        local_path = args.dataset_path\n    local_path = os.path.expanduser(args.dataset_path)\n    if args.dataset == 'CamVid':\n        dst = camvidLoader(local_path, is_transform=True, split=args.dataset_type)\n    elif args.dataset == 'CityScapes':\n        dst = cityscapesLoader(local_path, is_transform=True, split=args.dataset_type)\n    elif args.dataset == 'FreeSpace':\n        dst = freespaceLoader(local_path, is_transform=True, split=args.dataset_type)\n    else:\n        pass\n    val_loader = torch.utils.data.DataLoader(dst, batch_size=1, shuffle=False)\n\n    # if os.path.isfile(args.validate_model):\n    if args.validate_model != '':\n        model = torch.load(args.validate_model)\n    else:\n        try:\n            model = eval(args.structure)(n_classes=args.n_classes, pretrained=args.init_vgg16)\n        except:\n            print('missing structure or not support')\n            exit(0)\n        if args.validate_model_state_dict != '':\n            try:\n                model.load_state_dict(torch.load(args.validate_model_state_dict, map_location='cpu'))\n            except KeyError:\n                print('missing key')\n    if args.cuda:\n        model.cuda()\n    # some model load different mode different performance\n    model.eval()\n    # model.train()\n\n    gts, preds, errors, imgs_name = [], [], [], []\n    for i, (imgs, labels) in enumerate(val_loader):\n        print(i)\n        # if i==1:\n        #     break\n        img_path = dst.files[args.dataset_type][i]\n        img_name = img_path[img_path.rfind('/')+1:]\n        imgs_name.append(img_name)\n        # print('img_path:', img_path)\n        # print('img_name:', img_name)\n        #  print(labels.shape)\n        #  print(imgs.shape)\n        # \xe5\xb0\x86np\xe5\x8f\x98\xe9\x87\x8f\xe8\xbd\xac\xe6\x8d\xa2\xe4\xb8\xbapytorch\xe4\xb8\xad\xe7\x9a\x84\xe5\x8f\x98\xe9\x87\x8f\n        imgs = Variable(imgs, volatile=True)\n        labels = Variable(labels, volatile=True)\n\n        if args.cuda:\n            imgs = imgs.cuda()\n            labels = labels.cuda()\n\n        outputs = model(imgs)\n        # print('imgs.size:', imgs.size())\n        # print('labels.size:', labels.size())\n        # print('outputs.size:', outputs.size())\n        loss = cross_entropy2d(outputs, labels)\n        loss_np = loss.cpu().data.numpy()\n        loss_np_float = float(loss_np)\n\n        # print('loss_np_float:', loss_np_float)\n        errors.append(loss_np_float)\n\n        # \xe5\x8f\x96axis=1\xe4\xb8\xad\xe7\x9a\x84\xe6\x9c\x80\xe5\xa4\xa7\xe5\x80\xbc\xef\xbc\x8coutputs\xe7\x9a\x84shape\xe4\xb8\xbabatch_size*n_classes*height*width\xef\xbc\x8c\n        # \xe8\x8e\xb7\xe5\x8f\x96max\xe5\x90\x8e\xef\xbc\x8c\xe8\xbf\x94\xe5\x9b\x9e\xe4\xb8\xa4\xe4\xb8\xaa\xe6\x95\xb0\xe7\xbb\x84\xef\xbc\x8c\xe5\x88\x86\xe5\x88\xab\xe6\x98\xaf\xe6\x9c\x80\xe5\xa4\xa7\xe5\x80\xbc\xe5\x92\x8c\xe7\x9b\xb8\xe5\xba\x94\xe7\x9a\x84\xe7\xb4\xa2\xe5\xbc\x95\xe5\x80\xbc\xef\xbc\x8c\xe8\xbf\x99\xe9\x87\x8c\xe5\x8f\x96\xe7\xb4\xa2\xe5\xbc\x95\xe5\x80\xbc\xe4\xb8\xbalabel\n        pred = outputs.cpu().data.max(1)[1].numpy()\n        gt = labels.cpu().data.numpy()\n\n        if args.save_result:\n            if not os.path.exists('/tmp/'+init_time):\n                os.mkdir('/tmp/'+init_time)\n            pred_labels = outputs.cpu().data.max(1)[1].numpy()\n            label_color = dst.decode_segmap(labels.cpu().data.numpy()[0]).transpose(2, 0, 1)\n            pred_label_color = dst.decode_segmap(pred_labels[0]).transpose(2, 0, 1)\n\n            label_color_cv2 = label_color.transpose(1, 2, 0)\n            label_color_cv2 = cv2.cvtColor(label_color_cv2, cv2.COLOR_RGB2BGR)\n            cv2.imwrite('/tmp/'+init_time+'/gt_{}'.format(img_name), label_color_cv2)\n\n            pred_label_color_cv2 = pred_label_color.transpose(1, 2, 0)\n            pred_label_color_cv2 = cv2.cvtColor(pred_label_color_cv2, cv2.COLOR_RGB2BGR)\n            cv2.imwrite('/tmp/'+init_time+'/pred_{}'.format(img_name), pred_label_color_cv2)\n\n        for gt_, pred_ in zip(gt, pred):\n            gts.append(gt_)\n            preds.append(pred_)\n\n    # print('errors:', errors)\n    # print('imgs_name:', imgs_name)\n\n    errors_indices = np.argsort(errors).tolist()\n    # print('errors_indices:', errors_indices)\n    # for top_i in range(len(errors_indices)):\n    # for top_i in range(10):\n    #     top_index = errors_indices.index(top_i)\n    #     # print('top_index:', top_index)\n    #     img_name_top = imgs_name[top_index]\n    #     print('img_name_top:', img_name_top)\n\n    score, class_iou = scores(gts, preds, n_class=dst.n_classes)\n    for k, v in score.items():\n        print(k, v)\n\n    class_iou_list = []\n    for i in range(dst.n_classes):\n        class_iou_list.append(round(class_iou[i], 2))\n        # print(i, round(class_iou[i], 2))\n    print('classes:', range(dst.n_classes))\n    print('class_iou_list:', class_iou_list)\n\n\n# best validate: python validate.py --structure fcn32s --validate_model_state_dict fcn32s_camvid_9.pt\nif __name__=='__main__':\n    # print('validate----in----')\n    parser = argparse.ArgumentParser(description='training parameter setting')\n    parser.add_argument('--structure', type=str, default='fcn32s', help='use the net structure to segment [ fcn32s ResNetDUC segnet ENet drn_d_22 ]')\n    parser.add_argument('--validate_model', type=str, default='', help='validate model path [ fcn32s_camvid_9.pkl ]')\n    parser.add_argument('--validate_model_state_dict', type=str, default='', help='validate model state dict path [ fcn32s_camvid_9.pt ]')\n    parser.add_argument('--init_vgg16', type=bool, default=False, help='init model using vgg16 weights [ False ]')\n    parser.add_argument('--dataset', type=str, default='CamVid', help='train dataset [ CamVid CityScapes FreeSpace ]')\n    parser.add_argument('--dataset_path', type=str, default='~/Data/CamVid', help='train dataset path [ ~/Data/CamVid ~/Data/cityscapes ~/Data/FreeSpaceDataset ]')\n    parser.add_argument('--dataset_type', type=str, default='val', help='dataset type [ train val test ]')\n    parser.add_argument('--n_classes', type=int, default=12, help='train class num [ 12 ]')\n    parser.add_argument('--vis', type=bool, default=False, help='visualize the training results [ False ]')\n    parser.add_argument('--cuda', type=bool, default=False, help='use cuda [ False ]')\n    parser.add_argument('--save_result', type=bool, default=False, help='save the val dataset prediction result [ False True ]')\n    args = parser.parse_args()\n    # print(args.resume_model)\n    # print(args.save_model)\n    print(args)\n    validate(args)\n    # print('validate----out----')\n"""
validate_mt.py,19,"b""# -*- coding: utf-8 -*-_resnet18_32s\nimport torch\nimport os\nimport argparse\n\nimport cv2\nimport time\nimport numpy as np\nimport visdom\nfrom torch.autograd import Variable\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau, StepLR\nfrom torchvision import transforms\n\nfrom semseg.dataloader.camvid_loader import camvidLoader\nfrom semseg.dataloader.cityscapes_loader import cityscapesLoader\nfrom semseg.dataloader.yolodataset_loader import yoloDataset\nfrom semseg.loss import cross_entropy2d\nfrom semseg.metrics import scores\nfrom semseg.modelloader.drn_a_mt import drnsegmt_a_18\nfrom semseg.schedulers import ConstantLR, PolynomialLR\nfrom semseg.utils.get_class_weights import median_frequency_balancing, ENet_weighing\nfrom semseg.yoloLoss import yoloLoss\n\ndef decoder(pred):\n    '''\n    pred (tensor) 1x7x7x30\n    return (tensor) box[[x1,y1,x2,y2]] label[...]\n    '''\n    grid_num = 14\n    boxes=[]\n    cls_indexs=[]\n    probs = []\n    cell_size = 1./grid_num\n    pred = pred.data\n    pred = pred.squeeze(0) #7x7x30\n    contain1 = pred[:,:,4].unsqueeze(2)\n    contain2 = pred[:,:,9].unsqueeze(2)\n    contain = torch.cat((contain1,contain2),2)\n    mask1 = contain > 0.1 #\xe5\xa4\xa7\xe4\xba\x8e\xe9\x98\x88\xe5\x80\xbc\n    mask2 = (contain==contain.max()) #we always select the best contain_prob what ever it>0.9\n    mask = (mask1+mask2).gt(0)\n    # min_score,min_index = torch.min(contain,2) #\xe6\xaf\x8f\xe4\xb8\xaacell\xe5\x8f\xaa\xe9\x80\x89\xe6\x9c\x80\xe5\xa4\xa7\xe6\xa6\x82\xe7\x8e\x87\xe7\x9a\x84\xe9\x82\xa3\xe4\xb8\xaa\xe9\xa2\x84\xe6\xb5\x8b\xe6\xa1\x86\n    for i in range(grid_num):\n        for j in range(grid_num):\n            for b in range(2):\n                # index = min_index[i,j]\n                # mask[i,j,index] = 0\n                if mask[i,j,b] == 1:\n                    #print(i,j,b)\n                    box = pred[i,j,b*5:b*5+4]\n                    contain_prob = torch.FloatTensor([pred[i,j,b*5+4]])\n                    xy = torch.FloatTensor([j,i])*cell_size #cell\xe5\xb7\xa6\xe4\xb8\x8a\xe8\xa7\x92  up left of cell\n                    box[:2] = box[:2]*cell_size + xy # return cxcy relative to image\n                    box_xy = torch.FloatTensor(box.size())#\xe8\xbd\xac\xe6\x8d\xa2\xe6\x88\x90xy\xe5\xbd\xa2\xe5\xbc\x8f    convert[cx,cy,w,h] to [x1,xy1,x2,y2]\n                    box_xy[:2] = box[:2] - 0.5*box[2:]\n                    box_xy[2:] = box[:2] + 0.5*box[2:]\n                    max_prob,cls_index = torch.max(pred[i,j,10:],0)\n                    if float((contain_prob*max_prob)[0]) > 0.1:\n                        boxes.append(box_xy.view(1,4))\n                        cls_indexs.append(cls_index)\n                        # print('cls_index:', cls_index)\n                        probs.append(contain_prob*max_prob)\n    # print('boxes:', boxes)\n    if len(boxes) ==0:\n        boxes = torch.zeros((1,4))\n        probs = torch.zeros(1)\n        cls_indexs = torch.zeros(1)\n    else:\n        # print('boxes.shape:', len(boxes))\n        # print('probs.shape:', len(probs))\n        boxes = torch.cat(boxes,0) #(n,4)\n        probs = torch.cat(probs,0) #(n,)\n        cls_indexs = torch.stack(cls_indexs,0) #(n,)\n    keep = nms(boxes,probs)\n    return boxes[keep],cls_indexs[keep],probs[keep]\n\ndef nms(bboxes,scores,threshold=0.5):\n    '''\n    bboxes(tensor) [N,4]\n    scores(tensor) [N,]\n    '''\n    x1 = bboxes[:,0]\n    y1 = bboxes[:,1]\n    x2 = bboxes[:,2]\n    y2 = bboxes[:,3]\n    areas = (x2-x1) * (y2-y1)\n\n    _,order = scores.sort(0,descending=True)\n    keep = []\n    while order.numel() > 0:\n        i = order[0]\n        keep.append(i)\n\n        if order.numel() == 1:\n            break\n\n        xx1 = x1[order[1:]].clamp(min=x1[i])\n        yy1 = y1[order[1:]].clamp(min=y1[i])\n        xx2 = x2[order[1:]].clamp(max=x2[i])\n        yy2 = y2[order[1:]].clamp(max=y2[i])\n\n        w = (xx2-xx1).clamp(min=0)\n        h = (yy2-yy1).clamp(min=0)\n        inter = w*h\n\n        ovr = inter / (areas[i] + areas[order[1:]] - inter)\n        ids = (ovr<=threshold).nonzero().squeeze()\n        if ids.numel() == 0:\n            break\n        order = order[ids+1]\n    return torch.LongTensor(keep)\n\ndef validate(args):\n    init_time = str(int(time.time()))\n    if args.vis:\n        # start visdom and close all window\n        vis = visdom.Visdom()\n        vis.close()\n\n        # vis_text_usage = 'Operating in the text window<br>Press s to save data<br>'\n        # callback_text_usage_window = vis.text(vis_text_usage)\n        # vis.register_event_handler(type_callback, callback_text_usage_window)\n\n    class_weight = None\n    local_path = os.path.expanduser(args.dataset_path)\n    train_dst = None\n    val_dst = None\n    if args.dataset == 'CamVid':\n        train_dst = camvidLoader(local_path, is_transform=True, is_augment=args.data_augment, split='train')\n        val_dst = camvidLoader(local_path, is_transform=True, is_augment=False, split='val')\n    elif args.dataset == 'CityScapes':\n        train_dst = cityscapesLoader(local_path, is_transform=True, split='train')\n        val_dst = cityscapesLoader(local_path, is_transform=True, split='val')\n    else:\n        print('{} dataset does not implement'.format(args.dataset))\n        exit(0)\n\n    if args.cuda:\n        if class_weight is not None:\n            class_weight = class_weight.cuda()\n    print('class_weight:', class_weight)\n\n    train_loader = torch.utils.data.DataLoader(train_dst, batch_size=args.batch_size, shuffle=True)\n    val_loader = torch.utils.data.DataLoader(val_dst, batch_size=1, shuffle=True)\n\n    yolo_B = 2\n    yolo_C = 2\n    yolo_S = 7\n    yolo_out_tensor_shape = yolo_B * 5 + yolo_C\n    print('yolo_out_tensor_shape:', yolo_out_tensor_shape)\n\n    det_file_root = os.path.expanduser('~/Data/CamVid/train/')\n    det_train_dst = yoloDataset(root=det_file_root, list_file=['camvid_det.txt'], train=False, transform=[transforms.ToTensor()], yolo_out_tensor_shape=yolo_out_tensor_shape)\n    det_train_loader = torch.utils.data.DataLoader(det_train_dst, batch_size=1, shuffle=False)\n\n    model = drnsegmt_a_18(pretrained=args.init_vgg16, n_classes=args.n_classes, det_tensor_num=yolo_out_tensor_shape)\n    if args.resume_model_state_dict != '':\n        pretrained_dict = torch.load(args.resume_model_state_dict, map_location='cpu')\n        model.load_state_dict(pretrained_dict)\n    else:\n        print('missing resume_model_state_dict')\n        exit()\n\n    if args.cuda:\n        model.cuda()\n\n    model.eval()\n    for epoch in range(0, 1, 1):\n        # ----for object detection----\n        for det_i, (det_imgs, det_labels, det_imgs_ori) in enumerate(det_train_loader):\n            print('det_imgs.shape:', det_imgs.shape)\n            print('det_labels.shape:', det_labels.shape)\n            # det_imgs_height = det_imgs.shape[2]\n            # det_imgs_width = det_imgs.shape[3]\n            # print('det_imgs_height:', det_imgs_height)\n            # print('det_imgs_width:', det_imgs_width)\n\n\n            det_imgs = Variable(det_imgs)\n            det_labels = Variable(det_labels)\n\n            if args.cuda:\n                det_imgs = det_imgs.cuda()\n                det_labels = det_labels.cuda()\n\n            _, outputs_det = model(det_imgs)\n            # print('outpust_det:', outputs_det.shape)\n\n            # det_loss = det_criterion(outputs_det, det_labels)\n            # det_loss_np = det_loss.cpu().data.numpy()\n            outputs_det = outputs_det.cpu()\n            det_boxes, det_cls_indexs, det_probs = decoder(outputs_det)\n\n            image_ori = det_imgs_ori[0, ...].cpu().data.numpy()\n            det_imgs_ori_height = image_ori.shape[0]\n            det_imgs_ori_width = image_ori.shape[1]\n            # image = image.transpose(1, 2, 0)\n            for i, det_box in enumerate(det_boxes):\n                x1 = int(det_box[0] * det_imgs_ori_width)\n                x2 = int(det_box[2] * det_imgs_ori_width)\n                y1 = int(det_box[1] * det_imgs_ori_height)\n                y2 = int(det_box[3] * det_imgs_ori_height)\n                det_cls_index = det_cls_indexs[i]\n                det_cls_index = int(det_cls_index)  # convert LongTensor to int\n\n                det_prob = det_probs[i]\n                det_prob = float(det_prob)\n                if x1<0 or x1>det_imgs_ori_width-1:\n                    continue\n                if x2<0 or x2>det_imgs_ori_width-1:\n                    continue\n                if y1<0 or y1>det_imgs_ori_height-1:\n                    continue\n                if y2<0 or y2>det_imgs_ori_height-1:\n                    continue\n                # x1 = np.clip(x1, 0, det_imgs_ori_width-1)\n                # x2 = np.clip(x2, 0, det_imgs_ori_width-1)\n                # y1 = np.clip(y1, 0, det_imgs_ori_height-1)\n                # y2 = np.clip(y2, 0, det_imgs_ori_height-1)\n\n                if det_prob>0:\n                    print('(x1,y1)->(x2,y2):({},{})->({},{})'.format(x1, y1, x2, y2))\n                    cv2.rectangle(image_ori, (x1, y1), (x2, y2), (0, 0, 255))\n            cv2.imshow('image_ori', image_ori)\n            cv2.waitKey()\n\n        # ----for object detection----\n\n        # # ----for semantic segment----\n        # for i, (imgs, labels) in enumerate(train_loader):\n        #     # if i==1:\n        #     #     break\n        #     # model.train()\n        #\n        #     # \xe6\x9c\x80\xe5\x90\x8e\xe7\x9a\x84\xe5\x87\xa0\xe5\xbc\xa0\xe5\x9b\xbe\xe7\x89\x87\xe5\x8f\xaf\xe8\x83\xbd\xe4\xb8\x8d\xe5\x88\xb0batch_size\xe7\x9a\x84\xe6\x95\xb0\xe9\x87\x8f\xef\xbc\x8c\xe6\xaf\x94\xe5\xa6\x82batch_size=4\xef\xbc\x8c\xe5\x8f\xaf\xe8\x83\xbd\xe5\x8f\xaa\xe5\x89\xa93\xe5\xbc\xa0\n        #     imgs_batch = imgs.shape[0]\n        #     if imgs_batch != args.batch_size:\n        #         break\n        #     # iteration_step += 1\n        #\n        #     imgs = Variable(imgs)\n        #     labels = Variable(labels)\n        #\n        #     if args.cuda:\n        #         imgs = imgs.cuda()\n        #         labels = labels.cuda()\n        #     outputs_sem, _ = model(imgs)\n        #     # print('outputs_sem.shape:', outputs_sem.shape)\n        #\n        #     # print('outputs.size:', outputs.size())\n        #     # print('labels.size:', labels.size())\n        #\n        #     loss = cross_entropy2d(outputs_sem, labels, weight=class_weight)\n        #     loss_np = loss.cpu().data.numpy()\n        #     loss_epoch += loss_np\n        #\n        #     if args.vis and i%50==0:\n        #         pred_labels = outputs_sem.cpu().data.max(1)[1].numpy()\n        #         label_color = train_dst.decode_segmap(labels.cpu().data.numpy()[0]).transpose(2, 0, 1)\n        #         pred_label_color = train_dst.decode_segmap(pred_labels[0]).transpose(2, 0, 1)\n        #         win = 'label_color'\n        #         vis.image(label_color, win=win, opts=dict(title='Gt', caption='Ground Truth'))\n        #         win = 'pred_label_color'\n        #         vis.image(pred_label_color, win=win, opts=dict(title='Pred', caption='Prediction'))\n        #\n        #     # \xe6\x98\xbe\xe7\xa4\xba\xe4\xb8\x80\xe4\xb8\xaa\xe5\x91\xa8\xe6\x9c\x9f\xe7\x9a\x84loss\xe6\x9b\xb2\xe7\xba\xbf\n        #     if args.vis:\n        #         win = 'loss_iteration'\n        #         loss_np_expand = np.expand_dims(loss_np, axis=0)\n        #         win_res = vis.line(X=np.ones(1)*(i+data_count*(epoch-1)+1), Y=loss_np_expand, win=win, update='append')\n        #         if win_res != win:\n        #             vis.line(X=np.ones(1)*(i+data_count*(epoch-1)+1), Y=loss_np_expand, win=win, opts=dict(title=win, xlabel='iteration', ylabel='loss'))\n        # # ----for semantic segment----\n\n# best training: python train.py --resume_model fcn32s_camvid_9.pkl --save_model True\n# --init_vgg16 True --dataset_path /home/cgf/Data/CamVid --batch_size 1 --vis True\nif __name__=='__main__':\n    # print('train----in----')\n    parser = argparse.ArgumentParser(description='training parameter setting')\n    parser.add_argument('--structure', type=str, default='ENetV2', help='use the net structure to segment [ fcn_32s ResNetDUC segnet ENet drn_d_22 ]')\n    parser.add_argument('--solver', type=str, default='SGD', help='use the solver to optimizer net [ SGD ]')\n    parser.add_argument('--resume_model', type=str, default='', help='resume model path [ fcn32s_camvid_9.pkl ]')\n    parser.add_argument('--resume_model_state_dict', type=str, default='', help='resume model state dict path [ fcn32s_camvid_9.pt ]')\n    parser.add_argument('--save_model', type=bool, default=False, help='save model [ False ]')\n    parser.add_argument('--save_epoch', type=int, default=1, help='save model after epoch [ 1 ]')\n    parser.add_argument('--training_epoch', type=int, default=500, help='training epoch end training model [ 30000 ]')\n    parser.add_argument('--init_vgg16', type=bool, default=False, help='init model using vgg16 weights [ False ]')\n    parser.add_argument('--dataset', type=str, default='CamVid', help='train dataset [ CamVid CityScapes ]')\n    parser.add_argument('--dataset_path', type=str, default='~/Data/CamVid', help='train dataset path [ ~/Data/CamVid ~/Data/cityscapes ]')\n    parser.add_argument('--data_augment', type=bool, default=True, help='enlarge the training data [ True False ]')\n    parser.add_argument('--class_weighting', type=str, default='MFB', help='weighting class [ MFB ENET ]')\n    parser.add_argument('--batch_size', type=int, default=1, help='train dataset batch size [ 1 ]')\n    parser.add_argument('--val_interval', type=int, default=-1, help='val dataset interval unit epoch [ 3 ]')\n    parser.add_argument('--n_classes', type=int, default=12, help='train class num [ 12 ]')\n    parser.add_argument('--lr', type=float, default=1e-4, help='train learning rate [ 0.00001 ]')\n    parser.add_argument('--lr_policy', type=str, default='Polynomial', help='train learning policy [ Constant Polynomial ]')\n    parser.add_argument('--vis', type=bool, default=False, help='visualize the training results [ False ]')\n    parser.add_argument('--cuda', type=bool, default=False, help='use cuda [ False ]')\n    args = parser.parse_args()\n    print(args)\n    validate(args)\n    # print('train----out----')\n"""
validate_pred.py,4,"b""# -*- coding: utf-8 -*-\nimport torch\nimport os\nimport argparse\n\nimport cv2\nimport time\nimport numpy as np\nimport visdom\nfrom torch.autograd import Variable\nfrom scipy import misc\n\nfrom semseg.dataloader.camvid_loader import camvidLoader\nfrom semseg.dataloader.cityscapes_loader import cityscapesLoader\nfrom semseg.dataloader.freespace_loader import freespaceLoader\nfrom semseg.dataloader.freespacepred_loader import freespacepredLoader\nfrom semseg.dataloader.movingmnist_loader import movingmnistLoader\nfrom semseg.dataloader.segmpred_loader import segmpredLoader\nfrom semseg.loss import cross_entropy2d\nfrom semseg.metrics import scores\nfrom semseg.modelloader.EDANet import EDANet\nfrom semseg.modelloader.bisenet import BiSeNet\nfrom semseg.modelloader.deeplabv3 import Res_Deeplab_101, Res_Deeplab_50\nfrom semseg.modelloader.drn import drn_d_22, DRNSeg, drn_a_asymmetric_18, drn_a_asymmetric_ibn_a_18, drnseg_a_50, drnseg_a_18, drnseg_a_34, drnseg_e_22, drnseg_a_asymmetric_18, drnseg_a_asymmetric_ibn_a_18, drnseg_d_22, drnseg_d_38\nfrom semseg.modelloader.drn_a_irb import drnsegirb_a_18\nfrom semseg.modelloader.drn_a_refine import drnsegrefine_a_18\nfrom semseg.modelloader.drn_pred import drnsegpred_a_18\nfrom semseg.modelloader.duc_hdc import ResNetDUC, ResNetDUCHDC\nfrom semseg.modelloader.enet import ENet\nfrom semseg.modelloader.enetv2 import ENetV2\nfrom semseg.modelloader.erfnet import erfnet\nfrom semseg.modelloader.fc_densenet import fcdensenet103, fcdensenet56, fcdensenet_tiny\nfrom semseg.modelloader.fcn import fcn, fcn_32s, fcn_16s, fcn_8s\nfrom semseg.modelloader.fcn_mobilenet import fcn_MobileNet, fcn_MobileNet_32s, fcn_MobileNet_16s, fcn_MobileNet_8s\nfrom semseg.modelloader.fcn_resnet import fcn_resnet18, fcn_resnet34, fcn_resnet18_32s, fcn_resnet18_16s, \\\n    fcn_resnet18_8s, fcn_resnet34_32s, fcn_resnet34_16s, fcn_resnet34_8s, fcn_resnet50_32s, fcn_resnet50_16s, fcn_resnet50_8s\nfrom semseg.modelloader.lrn import lrn_vgg16\nfrom semseg.modelloader.segnet import segnet, segnet_squeeze, segnet_alignres, segnet_vgg19\nfrom semseg.modelloader.segnet_unet import segnet_unet\nfrom semseg.modelloader.sqnet import sqnet\n\n\ndef validate(args):\n    init_time = str(int(time.time()))\n    if args.vis:\n        vis = visdom.Visdom()\n\n    local_path = os.path.expanduser(args.dataset_path)\n    if args.dataset == 'SegmPred':\n        dst = segmpredLoader(local_path, is_transform=True, split=args.dataset_type)\n    elif args.dataset == 'MovingMNIST':\n        dst = movingmnistLoader(local_path, is_transform=True, split=args.dataset_type)\n    elif args.dataset == 'FreeSpacePred':\n        input_channel = 1\n        dst = freespacepredLoader(local_path, is_transform=True, split=args.dataset_type)\n    else:\n        pass\n    val_loader = torch.utils.data.DataLoader(dst, batch_size=1, shuffle=False)\n\n    # if os.path.isfile(args.validate_model):\n    if args.validate_model != '':\n        model = torch.load(args.validate_model)\n    else:\n        # ---------------for testing SegmPred---------------\n        try:\n            model = eval(args.structure)(n_classes=args.n_classes, pretrained=args.init_vgg16, input_shape=dst.input_shape, input_channel=input_channel)\n        except:\n            print('missing structure or not support')\n            exit(0)\n        if args.validate_model_state_dict != '':\n            try:\n                model.load_state_dict(torch.load(args.validate_model_state_dict, map_location='cpu'))\n            except KeyError:\n                print('missing key')\n        # ---------------for testing SegmPred---------------\n    if args.cuda:\n        model.cuda()\n    # some model load different mode different performance\n    model.eval()\n    # model.train()\n\n    gts, preds, errors, imgs_name = [], [], [], []\n    for i, (imgs, labels) in enumerate(val_loader):\n        print(i)\n        # if i==1:\n        #     break\n        img_path = dst.files[args.dataset_type][i]\n        img_name = img_path[img_path.rfind('/', 0, img_path.rfind('/'))+1:]\n        img_name = img_name.replace('/', '_')\n        imgs_name.append(img_name)\n        # print('img_path:', img_path)\n        # print('img_name:', img_name)\n        #  print(labels.shape)\n        #  print(imgs.shape)\n        # \xe5\xb0\x86np\xe5\x8f\x98\xe9\x87\x8f\xe8\xbd\xac\xe6\x8d\xa2\xe4\xb8\xbapytorch\xe4\xb8\xad\xe7\x9a\x84\xe5\x8f\x98\xe9\x87\x8f\n        imgs = Variable(imgs, volatile=True)\n        labels = Variable(labels, volatile=True)\n\n        if args.cuda:\n            imgs = imgs.cuda()\n            labels = labels.cuda()\n\n        # print('imgs.shape', imgs.shape)\n        # print('labels.shape', labels.shape)\n\n        outputs = model(imgs)\n        # print('outputs.shape', outputs.shape)\n        loss = cross_entropy2d(outputs, labels)\n        loss_np = loss.cpu().data.numpy()\n        loss_np_float = float(loss_np)\n\n        # print('loss_np_float:', loss_np_float)\n        errors.append(loss_np_float)\n\n        # \xe5\x8f\x96axis=1\xe4\xb8\xad\xe7\x9a\x84\xe6\x9c\x80\xe5\xa4\xa7\xe5\x80\xbc\xef\xbc\x8coutputs\xe7\x9a\x84shape\xe4\xb8\xbabatch_size*n_classes*height*width\xef\xbc\x8c\n        # \xe8\x8e\xb7\xe5\x8f\x96max\xe5\x90\x8e\xef\xbc\x8c\xe8\xbf\x94\xe5\x9b\x9e\xe4\xb8\xa4\xe4\xb8\xaa\xe6\x95\xb0\xe7\xbb\x84\xef\xbc\x8c\xe5\x88\x86\xe5\x88\xab\xe6\x98\xaf\xe6\x9c\x80\xe5\xa4\xa7\xe5\x80\xbc\xe5\x92\x8c\xe7\x9b\xb8\xe5\xba\x94\xe7\x9a\x84\xe7\xb4\xa2\xe5\xbc\x95\xe5\x80\xbc\xef\xbc\x8c\xe8\xbf\x99\xe9\x87\x8c\xe5\x8f\x96\xe7\xb4\xa2\xe5\xbc\x95\xe5\x80\xbc\xe4\xb8\xbalabel\n        pred = outputs.cpu().data.max(1)[1].numpy()\n        gt = labels.cpu().data.numpy()\n\n        if args.save_result:\n            if not os.path.exists('/tmp/'+init_time):\n                os.mkdir('/tmp/'+init_time)\n            pred_labels = outputs.cpu().data.max(1)[1].numpy()\n            # print('pred_labels.shape:', pred_labels.shape)\n            label_color = dst.decode_segmap(labels.cpu().data.numpy()[0]).transpose(2, 0, 1)\n            pred_label_color = dst.decode_segmap(pred_labels[0]).transpose(2, 0, 1)\n            # print('label_color.shape:', label_color.shape)\n            # print('pred_label_color.shape:', pred_label_color.shape)\n\n            label_color_cv2 = label_color.transpose(1, 2, 0)\n            label_color_cv2 = cv2.cvtColor(label_color_cv2, cv2.COLOR_RGB2BGR)\n            # print('label_color_cv2.shape:', label_color_cv2.shape)\n            # print('label_color_cv2.dtype:', label_color_cv2.dtype)\n            # cv2.imshow('label_color_cv2', label_color_cv2)\n            # cv2.waitKey()\n            cv2.imwrite('/tmp/'+init_time+'/gt_{}.png'.format(img_name), label_color_cv2)\n\n            pred_label_color_cv2 = pred_label_color.transpose(1, 2, 0)\n            pred_label_color_cv2 = cv2.cvtColor(pred_label_color_cv2, cv2.COLOR_RGB2BGR)\n            cv2.imwrite('/tmp/'+init_time+'/pred_{}.png'.format(img_name), pred_label_color_cv2)\n\n        for gt_, pred_ in zip(gt, pred):\n            gts.append(gt_)\n            preds.append(pred_)\n\n    # print('errors:', errors)\n    # print('imgs_name:', imgs_name)\n\n    errors_indices = np.argsort(errors).tolist()\n    print('errors_indices:', errors_indices)\n    # for top_i in range(len(errors_indices)):\n    for top_i in range(10):\n        top_index = errors_indices.index(top_i)\n        # print('top_index:', top_index)\n        img_name_top = imgs_name[top_index]\n        print('img_name_top:', img_name_top)\n\n    score, class_iou = scores(gts, preds, n_class=dst.n_classes)\n    for k, v in score.items():\n        print(k, v)\n\n    class_iou_list = []\n    for i in range(dst.n_classes):\n        class_iou_list.append(round(class_iou[i], 2))\n        # print(i, round(class_iou[i], 2))\n    print('classes:', range(dst.n_classes))\n    print('class_iou_list:', class_iou_list)\n\n\n# best validate: python validate.py --structure fcn32s --validate_model_state_dict fcn32s_camvid_9.pt\nif __name__=='__main__':\n    # print('validate----in----')\n    parser = argparse.ArgumentParser(description='training parameter setting')\n    parser.add_argument('--structure', type=str, default='fcn32s', help='use the net structure to segment [ fcn32s ResNetDUC segnet ENet drn_d_22 ]')\n    parser.add_argument('--validate_model', type=str, default='', help='validate model path [ fcn32s_camvid_9.pkl ]')\n    parser.add_argument('--validate_model_state_dict', type=str, default='', help='validate model state dict path [ fcn32s_camvid_9.pt ]')\n    parser.add_argument('--init_vgg16', type=bool, default=False, help='init model using vgg16 weights [ False ]')\n    parser.add_argument('--dataset', type=str, default='CamVid', help='train dataset [ CamVid CityScapes FreeSpace SegmPred MovingMNIST ]')\n    parser.add_argument('--dataset_path', type=str, default='~/Data/CamVid', help='train dataset path [ ~/Data/CamVid ~/Data/cityscapes ~/Data/FreeSpaceDataset ~/Data/SegmPred ~/Data/mnist_test_seq.npy]')\n    parser.add_argument('--dataset_type', type=str, default='val', help='dataset type [ train val test ]')\n    parser.add_argument('--n_classes', type=int, default=12, help='train class num [ 12 ]')\n    parser.add_argument('--vis', type=bool, default=False, help='visualize the training results [ False ]')\n    parser.add_argument('--cuda', type=bool, default=False, help='use cuda [ False ]')\n    parser.add_argument('--save_result', type=bool, default=False, help='save the val dataset prediction result [ False True ]')\n    args = parser.parse_args()\n    # print(args.resume_model)\n    # print(args.save_model)\n    print(args)\n    validate(args)\n    # print('validate----out----')\n"""
visualize_test.py,0,"b""#!/usr/bin/python\n# -*- coding: UTF-8 -*-\nfrom semseg import visualize\n\nif __name__ == '__main__':\n    visualize.main()\n"""
misc/camvid_bbox.py,0,"b""# -*- coding: utf-8 -*-\nimport cv2\nimport matplotlib.pyplot as plt\nimport xml.etree.ElementTree as ET\nimport os\nimport glob\n\nif __name__ == '__main__':\n\n    # img = cv2.imread('../data/0006R0_f00930.png')\n    # bbox_path = '../data/0006R0_f00930.xml'\n\n    root = os.path.expanduser('~/Data/CamVid')\n    split = 'train'\n    # print('root:', root)\n    file_list = glob.glob(root + '/' + split + '/*.png')\n    file_list.sort()\n    print(file_list)\n\n    bbox_width = 960\n    bbox_height = 720\n    img_width = 480\n    img_height = 360\n\n    all_bbox_obj_name = []\n    right_obj_names = ['Car', 'Pedestrian']\n    save_obj_fp = open('/tmp/camvid_det.txt', 'wb')\n    for img_name in file_list:\n        object_det_bboxes = []\n        # print(img_name)\n        img_file_name = img_name[img_name.rfind('/')+1:img_name.rfind('.')]\n        img_path = root + '/' + split + '/' + img_file_name + '.png'\n        lbl_path = root + '/' + split + 'annot/' + img_file_name + '.png'\n        bbox_path = root + '/' + split + 'bbox/' + img_file_name + '.xml'\n\n        img = cv2.imread(img_path)\n\n        if os.path.exists(bbox_path):\n            print(bbox_path)\n            bbox_tree = ET.parse(bbox_path)\n            bbox_root = bbox_tree.getroot()\n\n            # for filename_obj in bbox_root.findall('filename'):\n            #     print('filename_obj:', filename_obj.text)\n            #     pass\n\n            for bbox_obj in bbox_root.findall('object'):\n                bbox_obj_name = bbox_obj.find('name').text\n                if bbox_obj_name not in right_obj_names:\n                    continue\n                bbox_obj_name_cls = right_obj_names.index(bbox_obj_name)\n                bbox_obj_bndbox = bbox_obj.find('bndbox')\n                xmin = int(int(bbox_obj_bndbox.find('xmin').text) * 1.0 / bbox_width * img_width)\n                ymin = int(int(bbox_obj_bndbox.find('ymin').text) * 1.0 / bbox_height * img_height)\n                xmax = int(int(bbox_obj_bndbox.find('xmax').text) * 1.0 / bbox_width * img_width)\n                ymax = int(int(bbox_obj_bndbox.find('ymax').text) * 1.0 / bbox_height * img_height)\n                # print bbox_obj_name, xmin, ymin, xmax, ymax\n                if bbox_obj_name not in all_bbox_obj_name:\n                    all_bbox_obj_name.append(bbox_obj_name)\n                object_det_bboxes.append([xmin, ymin, xmax, ymax, bbox_obj_name_cls])\n\n        if object_det_bboxes:\n            save_obj_line = img_file_name + '.png'\n            for object_det_bbox in object_det_bboxes:\n                xmin = object_det_bbox[0]\n                ymin = object_det_bbox[1]\n                xmax = object_det_bbox[2]\n                ymax = object_det_bbox[3]\n                bbox_obj_name_cls = object_det_bbox[4]\n                cv2.rectangle(img, pt1=(xmin, ymin), pt2=(xmax, ymax), color=(255, 0, 0), thickness=1)\n                # ' xmin ymin xmax ymax bbox_obj_name_cls'\n                save_obj_line += ' {} {} {} {} {}'.format(xmin, ymin, xmax, ymax, bbox_obj_name_cls)\n            save_obj_line += '\\n'\n            save_obj_fp.write(save_obj_line)\n\n        # img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        # cv2.imshow('img', img)\n        # key = cv2.waitKey(1)\n        # if key & 0xFF == ord('s'):\n        #     cv2.imwrite('/tmp/{}.png'.format(img_file_name), img)\n        # plt.imshow(img)\n        # plt.show()\n    save_obj_fp.close()\n    print(all_bbox_obj_name)\n"""
misc/camvid_bbox_rename.py,0,"b""# -*- coding: utf-8 -*-\nimport glob\nimport os\n\nif __name__ == '__main__':\n\n    root = os.path.expanduser('~/Data/CamVid')\n    split = 'train'\n    file_list = glob.glob(root + '/' + split + 'bbox/*.xml')\n    file_list.sort()\n    # print(file_list)\n\n    img_file_count = 6690\n    for bbox_path_name in file_list:\n        img_file_name = bbox_path_name[bbox_path_name.rfind('/')+1:bbox_path_name.rfind('.')]\n        if '001TP' in img_file_name:\n            img_file_new_name = '0001TP_{:06d}'.format(img_file_count)\n            img_file_count += 30\n            print(bbox_path_name)\n            print(img_file_name)\n            print(img_file_new_name)\n            bbox_path_new_name = root + '/' + split + 'bbox/{}.xml'.format(img_file_new_name)\n            print(bbox_path_new_name)\n            os.rename(bbox_path_name, bbox_path_new_name)\n\n"""
misc/cityscapes_bbox.py,0,"b'# -*- coding: utf-8 -*-\nimport json\nimport cv2\nimport matplotlib.pyplot as plt\nimport xml.etree.ElementTree as ET\nimport os\nimport glob\nimport numpy as np\nfrom scipy import misc\n\nif __name__ == \'__main__\':\n\n    # img = cv2.imread(\'../data/0006R0_f00930.png\')\n    # bbox_path = \'../data/0006R0_f00930.xml\'\n\n    root = os.path.expanduser(\'~/Data/cityscapes\')\n    split = \'train\'\n    images_base = os.path.join(root, ""leftImg8bit"", split)\n    annotations_base = os.path.join(root, ""gtFine"", split)\n    print(\'root:\', root)\n    print(\'images_base:\', images_base)\n    print(\'annotations_base:\', annotations_base)\n    file_list = glob.glob(images_base + ""/*/*.png"")\n    file_list.sort()\n    # print(file_list)\n\n    save_obj_fp = open(\'/tmp/cityscapes_det.txt\', \'wb\')\n    right_obj_names = [\'car\', \'person\']\n    for img_index, img_name in enumerate(file_list):\n        # img_path = file_list[split][img_index].rstrip()\n        # img = misc.imread(img_name)\n        img = cv2.imread(img_name)\n        img = np.array(img, dtype=np.uint8)\n        polygons_path = os.path.join(\n            annotations_base,\n            img_name.split(os.sep)[-2],\n            os.path.basename(img_name)[:-15] + ""gtFine_polygons.json"",\n        )\n        polygons_json = json.load(open(polygons_path, \'rb\'))\n        object_det_bboxes = []\n        # print(polygons_json[\'objects\'])\n        polygons_json_objects = polygons_json[\'objects\']\n        for polygons_json_object in polygons_json_objects:\n            polygons_json_object_polygon = polygons_json_object[\'polygon\']\n            polygons_json_object_label = polygons_json_object[\'label\']\n            if polygons_json_object_label in right_obj_names:\n                # print(polygons_json_object_polygon)\n                polygons_json_object_polygon_np = np.array(polygons_json_object_polygon)\n                # print(polygons_json_object_polygon_np)\n                # print(polygons_json_object_polygon_np.shape)\n                polygons_json_object_polygon_np_x = polygons_json_object_polygon_np[:, 0]\n                polygons_json_object_polygon_np_y = polygons_json_object_polygon_np[:, 1]\n                x1 = min(polygons_json_object_polygon_np_x)\n                x2 = max(polygons_json_object_polygon_np_x)\n                y1 = min(polygons_json_object_polygon_np_y)\n                y2 = max(polygons_json_object_polygon_np_y)\n                object_cls = right_obj_names.index(polygons_json_object_label)\n                object_det_bboxes.append([x1, y1, x2, y2, object_cls])\n\n        if object_det_bboxes:\n            save_obj_line = img_name[img_name.index(split)+len(split)+1:]\n            for object_det_bbox in object_det_bboxes:\n                import cv2\n\n                x1 = object_det_bbox[0]\n                y1 = object_det_bbox[1]\n                x2 = object_det_bbox[2]\n                y2 = object_det_bbox[3]\n                bbox_obj_name_cls = object_det_bbox[4]\n                cv2.rectangle(img, pt1=(x1, y1), pt2=(x2, y2), color=(255, 0, 0), thickness=5)\n                save_obj_line += \' {} {} {} {} {}\'.format(x1, y1, x2, y2, bbox_obj_name_cls)\n            save_obj_line += \'\\n\'\n            save_obj_fp.write(save_obj_line)\n\n        # cv2.imshow(\'img\', img)\n        # cv2.waitKey()\n    save_obj_fp.close()\n'"
misc/loss_smooth.py,0,"b""# -*- coding: utf-8 -*-\nimport os\n\nloss_iteration_fp = open(os.path.expanduser('/Users/cgf/GitHub/Quick/master_thesis/\xe6\x9d\x82/\xe6\xa3\x80\xe6\xb5\x8b\xe5\xae\x9e\xe9\xaa\x8c/DRNSegMT_A_small_YOLO_det_loss_iteration_epoch500_lr_time_12_12_1.txt'), 'rb')\nloss_iteration_content = loss_iteration_fp.readlines()\n\n\nsmooth_interval = 341\nloss_epoch_total = 0\nloss_epoch_fp = open('/tmp/tmp.txt', 'wb')\nfor loss_iteration_content_item in loss_iteration_content:\n    loss_iteration_content_item_split = loss_iteration_content_item.strip().split(' ')\n    # loss_iteration_content_item_split = loss_iteration_content_item.strip().split('\\t')\n    # print('loss_iteration_content_item_split:', loss_iteration_content_item_split)\n    loss_iteration_id = int(float(loss_iteration_content_item_split[0]))\n    loss_iteration_val = float(loss_iteration_content_item_split[1])\n    if loss_iteration_id%smooth_interval==0:\n        pass\n        loss_epoch_avg = loss_epoch_total*1.0/smooth_interval\n        loss_epoch_total = 0\n        #print('loss_epoch_avg:', loss_epoch_avg)\n        loss_epoch_fp.write('{}\\n'.format(loss_epoch_avg))\n        #break\n    else:\n        loss_epoch_total += loss_iteration_val\n    #break\nloss_epoch_fp.close()\n"""
misc/miou_expand.py,0,"b""# -*- coding: utf-8 -*-\nimport os\nimport numpy as np\n\niou_epoch_fp = open(os.path.expanduser('/Users/cgf/GitHub/Quick/master_thesis/\xe6\x9d\x82/\xe6\xa3\x80\xe6\xb5\x8b\xe5\xae\x9e\xe9\xaa\x8c/DRNSegMT_A_small_YOLO_miou_epoch500_lr_time_12_12_1.txt'), 'rb')\n# iou_epoch_fp = open(os.path.expanduser('/Users/cgf/GitHub/Quick/master_thesis/\xe6\x9d\x82/\xe6\xa3\x80\xe6\xb5\x8b\xe5\xae\x9e\xe9\xaa\x8c/DRNSegMT0_A_resnet18_32s_miou_epoch500_lr_time_12_12_1.txt'), 'rb')\niou_epoch_content = iou_epoch_fp.readlines()\n\niou_expand_interval = 5\n\ntemp_fp = open('/tmp/tmp.txt', 'wb')\niou_max = -np.inf\nfor iou_epoch_content_item in iou_epoch_content:\n    iou_epoch_content_item_split = iou_epoch_content_item.strip().split('\\t')\n    # print('iou_epoch_content_item_split:', iou_epoch_content_item_split)\n    iou_epoch_id = int(float(iou_epoch_content_item_split[0]))\n    iou_epoch_val = float(iou_epoch_content_item_split[1])\n    # if iou_epoch_val>0.5:\n    #     iou_epoch_val -= 0.01\n    if iou_max<iou_epoch_val:\n        iou_max = iou_epoch_val\n    temp_fp.write('{}\\n'.format(iou_epoch_val) * iou_expand_interval)\n    # break\n\nprint('iou_max:', iou_max)\ntemp_fp.close()\n"""
misc/split_dataset_train_val.py,1,"b""# -*- coding: utf-8 -*-\nimport os\nimport torch\nimport random\nfrom torch.utils import data\n\nfrom semseg.dataloader.freespace_loader import freespaceLoader\n\nif __name__ == '__main__':\n    HOME_PATH = os.path.expanduser('~')\n    local_path = os.path.join(HOME_PATH, 'Data/FreeSpaceDataset')\n    batch_size = 1\n    dst = freespaceLoader(local_path, is_transform=True, is_augment=False, split='train')\n    dst_len = len(dst)\n    dst_ids = range(dst_len)\n    random.shuffle(dst_ids)\n    train_rate = 0.7\n    train_num = int(dst_len*train_rate)\n    train_ids = dst_ids[:train_num]\n    test_ids = dst_ids[train_num:]\n    # for train_id in train_ids:\n    #     print(dst.get_filename(train_id))\n    for test_id in test_ids:\n        test_filename_src = dst.get_filename(test_id)\n        test_annot_filename_src = test_filename_src.replace('train', 'trainannot').replace('.png', '_mask.png')\n\n        test_filename_dst = test_filename_src.replace('train', 'test')\n        test_annot_filename_dst = test_annot_filename_src.replace('train', 'test')\n\n        test_dst_dir = os.path.dirname(test_filename_dst)\n        if not os.path.exists(test_dst_dir):\n            os.makedirs(test_dst_dir)\n\n        test_annot_dst_dir = os.path.dirname(test_annot_filename_dst)\n        if not os.path.exists(test_annot_dst_dir):\n            os.makedirs(test_annot_dst_dir)\n\n        os.rename(test_filename_src, test_filename_dst)\n        os.rename(test_annot_filename_src, test_annot_filename_dst)\n        # print('test_filename_src:', test_filename_src)\n        # print('test_filename_dst:', test_filename_dst)\n        # print('test_annot_filename_src:', test_annot_filename_src)\n        # print('test_annot_filename_dst:', test_annot_filename_dst)\n        # break\n"""
misc/squeeze-segnet.py,0,"b'#!/usr/bin/python\n# -*- coding: UTF-8 -*-\n\n# code is from nanfackg@gmail.com\n\nimport tensorflow as tf\nimport tensorflow.contrib.slim as slim\nfrom glob import glob\nfrom tensorflow.contrib.framework.python.ops.variables import get_or_create_global_step\n# from get_class_weights import median_frequency_balancing\nimport time\nimport os\nfrom matplotlib import pyplot as plt\n\nglobal a\nimport numpy as np\n\n\ndef fire_module(input, fire_id, channel, s1, e1, e3, ):\n    """"""\n    Basic module that makes up the SqueezeNet architecture. It has two layers.\n     1. Squeeze layer (1x1 convolutions)\n     2. Expand layer (1x1 and 3x3 convolutions)\n    :param input: Tensorflow tensor\n    :param fire_id: Variable scope name\n    :param channel: Depth of the previous output\n    :param s1: Number of filters for squeeze 1x1 layer\n    :param e1: Number of filters for expand 1x1 layer\n    :param e3: Number of filters for expand 3x3 layer\n    :return: Tensorflow tensor\n    """"""\n\n    fire_weights = {\n        \'conv_s_1\': tf.Variable(tf.truncated_normal([1, 1, channel, s1], stddev=0.001), name=\'weights_conv_s_1\'),\n        \'conv_e_1\': tf.Variable(tf.truncated_normal([1, 1, s1, e1], stddev=0.001), name=\'weights_conv_e_1\'),\n        \'conv_e_3\': tf.Variable(tf.truncated_normal([3, 3, s1, e3], stddev=0.001), name=\'weights_conv_e_3\')}\n\n    fire_biases = {\'conv_s_1\': tf.Variable(tf.truncated_normal([s1]), name=\'bias_conv_s_1\'),\n                   \'conv_e_1\': tf.Variable(tf.truncated_normal([e1]), name=\'bias_conv_e_1\'),\n                   \'conv_e_3\': tf.Variable(tf.truncated_normal([e3]), name=\'bias_conv_e_3\')}\n\n    with tf.name_scope(fire_id):\n        output = tf.nn.conv2d(input, fire_weights[\'conv_s_1\'], strides=[1, 1, 1, 1], padding=\'SAME\', name=\'conv_s_1\')\n        output = tf.nn.relu(tf.nn.bias_add(output, fire_biases[\'conv_s_1\']))\n\n        expand1 = tf.nn.conv2d(output, fire_weights[\'conv_e_1\'], strides=[1, 1, 1, 1], padding=\'SAME\', name=\'conv_e_1\')\n        expand1 = tf.nn.bias_add(expand1, fire_biases[\'conv_e_1\'])\n\n        expand3 = tf.nn.conv2d(output, fire_weights[\'conv_e_3\'], strides=[1, 1, 1, 1], padding=\'SAME\', name=\'conv_e_3\')\n        expand3 = tf.nn.bias_add(expand3, fire_biases[\'conv_e_3\'])\n\n        result = tf.concat([expand1, expand3], 3, name=\'concat_e1_e3\')\n        return tf.nn.relu(result)\n\n\ndef dfire_module(input, dfire_id, channel, s1_D, e1_D, e3_D, ):\n    fire_weights = {\'conv_s_1_D\': tf.Variable(tf.truncated_normal([1, 1, e1_D + e3_D, s1_D], stddev=0.001)),\n                    \'conv_e_1_D\': tf.Variable(tf.truncated_normal([1, 1, channel, e1_D], stddev=0.001)),\n                    \'conv_e_3_D\': tf.Variable(tf.truncated_normal([3, 3, channel, e3_D], stddev=0.001))}\n\n    fire_biases = {\'conv_s_1_D\': tf.Variable(tf.truncated_normal([s1_D])),\n                   \'conv_e_1_D\': tf.Variable(tf.truncated_normal([e1_D])),\n                   \'conv_e_3_D\': tf.Variable(tf.truncated_normal([e3_D]))}\n\n    with tf.name_scope(dfire_id):\n        expand1 = tf.nn.conv2d(input, fire_weights[\'conv_e_1_D\'], strides=[1, 1, 1, 1], padding=\'SAME\',\n                               name=\'conv_e_1_D\')\n        expand1 = tf.nn.bias_add(expand1, fire_biases[\'conv_e_1_D\'])\n\n        expand3 = tf.nn.conv2d(input, fire_weights[\'conv_e_3_D\'], strides=[1, 1, 1, 1], padding=\'SAME\',\n                               name=\'conv_e_3_D\')\n        expand3 = tf.nn.bias_add(expand3, fire_biases[\'conv_e_3_D\'])\n\n        result = tf.concat([expand1, expand3], 3, name=\'concat_e1_D_e3_D\')\n        result = tf.nn.relu(result)\n        output = tf.nn.conv2d(result, fire_weights[\'conv_s_1_D\'], strides=[1, 1, 1, 1], padding=\'SAME\',\n                              name=\'conv_s_1_D\')\n        output = tf.nn.relu(tf.nn.bias_add(output, fire_biases[\'conv_s_1_D\']))\n        return output\n\n\ndef unpool(updates, mask, k_size=[1, 2, 2, 1], output_shape=None, scope=\'unpool\'):\n    \'\'\'\n   # NOTE! this function is based on the implementation by kwotsin in\n    # https://github.com/kwotsin/TensorFlow-ENet\n    - inputs(Tensor): a 4D tensor of shape [batch_size, height, width, num_channels] that represents the input block to be upsampled\n    - mask(Tensor): a 4D tensor that represents the argmax values/pooling indices of the previously max-pooled layer\n    - k_size(list): a list of values representing the dimensions of the unpooling filter.\n    - output_shape(list): a list of values to indicate what the final output shape should be after unpooling\n    - scope(str): the string name to name your scope\n    OUTPUTS:\n    - ret(Tensor): the returned 4D tensor that has the shape of output_shape.\n    \'\'\'\n    with tf.variable_scope(scope):\n        mask = tf.cast(mask, tf.int32)\n        input_shape = tf.shape(updates, out_type=tf.int32)\n        #  calculation new shape\n        if output_shape is None:\n            output_shape = (input_shape[0], input_shape[1] * k_size[1], input_shape[2] * k_size[2], input_shape[3])\n\n        # calculation indices for batch, height, width and feature maps\n        one_like_mask = tf.ones_like(mask, dtype=tf.int32)\n        batch_shape = tf.concat([[input_shape[0]], [1], [1], [1]], 0)\n        batch_range = tf.reshape(tf.range(output_shape[0], dtype=tf.int32), shape=batch_shape)\n        b = one_like_mask * batch_range\n        y = mask // (output_shape[2] * output_shape[3])\n        x = (mask // output_shape[3]) % output_shape[2]  # mask % (output_shape[2] * output_shape[3]) // output_shape[3]\n        feature_range = tf.range(output_shape[3], dtype=tf.int32)\n        f = one_like_mask * feature_range\n\n        # transpose indices & reshape update values to one dimension\n        updates_size = tf.size(updates)\n        indices = tf.transpose(tf.reshape(tf.stack([b, y, x, f]), [4, updates_size]))\n        values = tf.reshape(updates, [updates_size])\n        ret = tf.scatter_nd(indices, values, shape=output_shape)\n        return ret\n\n\ndef squeeze_segnet(input, classes, keep_prob, nb_classes):\n    """"""\n    SqueezeNet model written in tensorflow. It provides AlexNet level accuracy with 50x fewer parameters\n    and smaller model size.\n    :param input: Input tensor (4D)\n    :param classes: number of classes for classification\n    :return: Tensorflow tensor\n    """"""\n    weights = {\'conv1\': tf.Variable(tf.truncated_normal([7, 7, 3, 96])),\n               \'conv10\': tf.Variable(tf.truncated_normal([1, 1, 512, classes])),\n               \'conv10_D\': tf.Variable(tf.truncated_normal([3, 3, classes, 512])),\n               \'conv1_D\': tf.Variable(tf.truncated_normal([2, 2, nb_classes, 96]))}\n\n    biases = {\'conv1\': tf.Variable(tf.truncated_normal([96])),\n              \'conv10\': tf.Variable(tf.truncated_normal([classes])),\n              \'conv10_D\': tf.Variable(tf.truncated_normal([512])),\n              \'conv1_D\': tf.Variable(tf.truncated_normal([nb_classes]))}\n\n    output_shape0 = input.get_shape().as_list()\n    out = tf.shape(input)\n    with tf.name_scope(\'Squeeze-SegNet\'):\n        with tf.name_scope(\'Conv1\'):\n            output = tf.nn.conv2d(input, weights[\'conv1\'], strides=[1, 2, 2, 1], padding=\'SAME\', name=\'conv1\')\n            output = tf.nn.relu(tf.nn.bias_add(output, biases[\'conv1\']))\n            print \'conv1\'\n            print output.get_shape().as_list()\n            # output_shape1 = output.get_shape().as_list()\n            out1 = tf.shape(output)\n        with tf.name_scope(\'Maxpool1_with_indices\'):\n            output, pooling_indices1 = tf.nn.max_pool_with_argmax(output,\n                                                                  ksize=[1, 3, 3, 1],\n                                                                  strides=[1, 2, 2, 1],\n                                                                  padding=\'SAME\', name=""maxpool1_with_mask"")\n            # output = tf.nn.max_pool(output, ksize=[1, 3, 3, 1], strides=[1, 2, 2, 1], padding=\'SAME\', name=\'maxpool1\')\n            print \'maxpool\'\n            print output.get_shape().as_list()\n\n        with tf.name_scope(\'Fire2\'):\n            output = fire_module(output, s1=16, e1=64, e3=64, channel=96, fire_id=\'fire2\')\n            print \'fire2\'\n            print output.get_shape().as_list()\n        with tf.name_scope(\'Fire3\'):\n            output = fire_module(output, s1=16, e1=64, e3=64, channel=128, fire_id=\'fire3\')\n            print \'fire3\'\n            print output.get_shape().as_list()\n        with tf.name_scope(\'Fire4\'):\n            output = fire_module(output, s1=32, e1=128, e3=128, channel=128, fire_id=\'fire4\')\n            print \'fire4\'\n            print output.get_shape().as_list()\n\n        # output_shape4 = output.get_shape().as_list()\n        out4 = tf.shape(output)\n\n        with tf.name_scope(\'Maxpool4_with_mask\'):\n            # output = tf.nn.max_pool(output, ksize=[1, 3, 3, 1], strides=[1, 2, 2, 1], padding=\'SAME\', name=\'maxpool4\')\n            output, pooling_indices4 = tf.nn.max_pool_with_argmax(output,\n                                                                  ksize=[1, 3, 3, 1],\n                                                                  strides=[1, 2, 2, 1],\n                                                                  padding=\'SAME\', name=""maxpool4_with_mask"")\n            print \'maxpool4\'\n            print output.get_shape().as_list()\n        with tf.name_scope(\'Fire5\'):\n            output = fire_module(output, s1=32, e1=128, e3=128, channel=256, fire_id=\'fire5\')\n            print \'fire5\'\n            print output.get_shape().as_list()\n        with tf.name_scope(\'Fire6\'):\n            output = fire_module(output, s1=48, e1=192, e3=192, channel=256, fire_id=\'fire6\')\n            print \'fire6\'\n            print output.get_shape().as_list()\n        with tf.name_scope(\'Fire7\'):\n            output = fire_module(output, s1=48, e1=192, e3=192, channel=384, fire_id=\'fire7\')\n            print \'fire7\'\n            print output.get_shape().as_list()\n        with tf.name_scope(\'Fire8\'):\n            output = fire_module(output, s1=64, e1=256, e3=256, channel=384, fire_id=\'fire8\')\n            print \'fire8\'\n            print output.get_shape().as_list()\n        # output_shape8 = output.get_shape().as_list()\n        out8 = tf.shape(output)\n\n        with tf.name_scope(\'Maxpool8_with_indices\'):\n            # output = tf.nn.max_pool(output, ksize=[1, 3, 3, 1], strides=[1, 2, 2, 1], padding=\'SAME\', name=\'maxpool8\')\n            output, pooling_indices8 = tf.nn.max_pool_with_argmax(output,\n                                                                  ksize=[1, 3, 3, 1],\n                                                                  strides=[1, 2, 2, 1],\n                                                                  padding=\'SAME\', name=""maxpool8_with_mask"")\n            print \'maxpool8\'\n            print output.get_shape().as_list()\n        with tf.name_scope(\'Fire9\'):\n            output = fire_module(output, s1=64, e1=256, e3=256, channel=512, fire_id=\'fire9\')\n            print  \'fire9\'\n            print output.get_shape().as_list()\n        with tf.name_scope(\'Dropout\'):\n            output = tf.nn.dropout(output, keep_prob=keep_prob, name=\'dropout9\')\n\n        with tf.name_scope(\'Conv10\'):\n            output = tf.nn.conv2d(output, weights[\'conv10\'], strides=[1, 1, 1, 1], padding=\'SAME\', name=\'conv10\')\n            output = tf.nn.relu(tf.nn.bias_add(output, biases[\'conv10\']))\n            print \'conv10\'\n            print output.get_shape().as_list()\n\n        with tf.name_scope(\'Conv10_D\'):\n            # output = tf.nn.avg_pool(output, ksize=[1, 13, 13, 1], strides=[1, 2, 2, 1], padding=\'SAME\', name=\'avgpool10\')\n            output = tf.nn.conv2d(output, weights[\'conv10_D\'], strides=[1, 1, 1, 1], padding=\'SAME\', name=\'conv10_D\')\n            output = tf.nn.relu(tf.nn.bias_add(output, biases[\'conv10_D\']))\n            print \'conv10_D\'\n            print output.get_shape().as_list()\n        with tf.name_scope(\'DFire9\'):\n            output = dfire_module(output, s1_D=512, e1_D=32, e3_D=32, channel=512, dfire_id=\'Dfire9\')\n            print \'dfire9\'\n            print output.get_shape().as_list()\n\n        with tf.name_scope(\'Unpool8\'):\n            output = unpool(output, pooling_indices8, output_shape=out8, scope=\'unpool8\')\n            print \'unpool8\'\n            print output.get_shape().as_list()\n\n        with tf.name_scope(\'DFire8\'):\n            output = dfire_module(output, s1_D=384, e1_D=32, e3_D=32, channel=512, dfire_id=\'Dfire8\')\n            print \'dfire8\'\n            print output.get_shape().as_list()\n        with tf.name_scope(\'DFire7\'):\n            output = dfire_module(output, s1_D=384, e1_D=24, e3_D=24, channel=384, dfire_id=\'Dfire7\')\n            print \'dfire7\'\n            print output.get_shape().as_list()\n        with tf.name_scope(\'DFire6\'):\n            output = dfire_module(output, s1_D=256, e1_D=24, e3_D=24, channel=384, dfire_id=\'Dfire6\')\n            print \'dfire6\'\n            print output.get_shape().as_list()\n        with tf.name_scope(\'DFire5\'):\n            output = dfire_module(output, s1_D=256, e1_D=16, e3_D=16, channel=256, dfire_id=\'Dfire5\')\n            print \'dfire5\'\n            print output.get_shape().as_list()\n        with tf.name_scope(\'Unpool4\'):\n            output = unpool(output, pooling_indices4, output_shape=out4, scope=\'unpool4\')\n            print \'unpool4\'\n            print output.get_shape().as_list()\n        with tf.name_scope(\'DFire4\'):\n            output = dfire_module(output, s1_D=128, e1_D=16, e3_D=16, channel=256, dfire_id=\'Dfire4\')\n            print \'dfire4\'\n            print output.get_shape().as_list()\n        with tf.name_scope(\'DFire3\'):\n            output = dfire_module(output, s1_D=128, e1_D=8, e3_D=8, channel=128, dfire_id=\'Dfire3\')\n            print \'dfire3\'\n            print output.get_shape().as_list()\n        with tf.name_scope(\'DFire2\'):\n            output = dfire_module(output, s1_D=96, e1_D=8, e3_D=8, channel=128, dfire_id=\'Dfire2\')\n            print \'dfire2\'\n            print output.get_shape().as_list()\n        with tf.name_scope(\'Unpool1\'):\n            output = unpool(output, pooling_indices1, output_shape=out1, scope=\'unpool1\')\n            print \'unpool1\'\n            print output.get_shape().as_list()\n\n        with tf.name_scope(\'Conv1_D\'):\n            output = tf.nn.conv2d_transpose(output, weights[\'conv1_D\'],\n                                            [out[0], output_shape0[1], output_shape0[2], nb_classes], [1, 2, 2, 1],\n                                            padding=\'SAME\', name=\'conv1_D\')\n            output = tf.nn.bias_add(output, biases[\'conv1_D\'])\n            output = tf.nn.tanh(output, \'activation_finale\')\n            print \'conv1_D\'\n            print output.get_shape().as_list()\n            probabilities = tf.nn.softmax(output, name=\'logits_to_softmax\')\n        return output, probabilities\n\n\ndef weighted_cross_entropy(onehot_labels, logits, class_weights):\n    \'\'\'\n    A quick wrapper to compute weighted cross entropy.\n    ------------------\n    Technical Details\n    ------------------\n    The class_weights list can be multiplied by onehot_labels directly because the last dimension\n    of onehot_labels is 12 and class_weights (length 12) can broadcast across that dimension, which is what we want.\n    Then we collapse the last dimension for the class_weights to get a shape of (batch_size, height, width, 1)\n    to get a mask with each pixel\'s value representing the class_weight.\n    This mask can then be that can be broadcasted to the intermediate output of logits\n    and onehot_labels when calculating the cross entropy loss.\n    ------------------\n    INPUTS:\n    - onehot_labels(Tensor): the one-hot encoded labels of shape (batch_size, height, width, num_classes)\n    - logits(Tensor): the logits output from the model that is of shape (batch_size, height, width, num_classes)\n    - class_weights(list): A list where each index is the class label and the value of the index is the class weight.\n    OUTPUTS:\n    - loss(Tensor): a scalar Tensor that is the weighted cross entropy loss output.\n    \'\'\'\n\n    weights = onehot_labels * class_weights\n    weights = tf.reduce_sum(weights, 3)\n\n    loss = tf.losses.softmax_cross_entropy(onehot_labels=onehot_labels, logits=logits, weights=weights)\n    print weights\n\n    return loss\n\n\ndef preprocess(image, annotation=None, height=360, width=480):\n    # Convert the image and annotation dtypes to tf.float32 if needed\n    if image.dtype != tf.float32:\n        image = tf.image.convert_image_dtype(image, dtype=tf.float32)\n        # image = tf.cast(image, tf.float32)\n\n    image = tf.image.resize_image_with_crop_or_pad(image, height, width)\n    image.set_shape(shape=(height, width, 3))\n\n    if not annotation == None:\n        annotation = tf.image.resize_image_with_crop_or_pad(annotation, height, width)\n        annotation.set_shape(shape=(height, width, 1))\n\n        return image, annotation\n\n    return image\n\n\nif __name__ == \'__main__\':\n\n    # Getting images\n    image_files = sorted(glob(\'/home/labogeraldo/PycharmProjects/tensorflow/CamVid/train/**\'))\n    annotation_files = sorted(glob(\'/home/labogeraldo/PycharmProjects/tensorflow/CamVid/trainannot/**\'))\n    with open(\'train_trainnot.txt\', \'w\') as f:\n        for i in range(len(image_files)):\n            f.write(image_files[i] + \' \' + annotation_files[i] + \'\\n\')\n    image_val_files = sorted(glob(\'/home/labogeraldo/PycharmProjects/tensorflow/CamVid/val/**\'))\n    annotation_val_files = sorted(glob(\'/home/labogeraldo/PycharmProjects/tensorflow/CamVid/valannot/**\'))\n    with open(\'val_valannot.txt\', \'w\') as f:\n        for i in range(len(image_val_files)):\n            f.write(image_val_files[i] + \' \' + annotation_val_files[i] + \'\\n\')\n\n    batch_size = 8\n    eval_batch_size = 8\n\n    # Training parameters\n    initial_learning_rate = 0.001\n    num_epochs_before_decay = 100\n    num_epochs = 300\n    learning_rate_decay_factor = 0.1\n    weight_decay = 0.0001\n    epsilon = 1e-8\n\n    num_batches_per_epoch = len(image_files) / batch_size\n    num_steps_per_epoch = num_batches_per_epoch\n    decay_steps = int(num_epochs_before_decay * num_steps_per_epoch)\n    image_height = 360\n    image_width = 480\n    num_classes = 12\n\n    # class_weights = median_frequency_balancing()\n    class_weights = None\n\n    tf.reset_default_graph()\n    # x = tf.placeholder(tf.float32, shape=[batch_size, 360 , 480, 3], name=""x-input"")\n    # prob=tf.placeholder(tf.float32, shape=[1], name=\'dropout_val\')\n    with tf.Graph().as_default() as graph:\n\n        images = tf.convert_to_tensor(image_files)\n\n        annotations = tf.convert_to_tensor(annotation_files)\n        input_queue = tf.train.slice_input_producer(\n            [images, annotations])  # Slice_input producer shuffles the data by default.\n\n        # Decode the image and annotation raw content\n        image = tf.read_file(input_queue[0])\n        image = tf.image.decode_image(image, channels=3)\n        annotation = tf.read_file(input_queue[1])\n        annotation = tf.image.decode_image(annotation)\n\n        # preprocess and batch up the image and annotation\n        preprocessed_image, preprocessed_annotation = preprocess(image, annotation)\n        images, annotations = tf.train.batch([preprocessed_image, preprocessed_annotation], batch_size=batch_size,\n                                             allow_smaller_final_batch=True)\n\n        # Create the model inference\n        print images.shape\n        logits, probabilities = squeeze_segnet(input=images, classes=1000, keep_prob=1, nb_classes=num_classes)\n\n        # perform one-hot-encoding on the ground truth annotation to get same shape as the logits\n        annotations = tf.reshape(annotations, shape=[batch_size, image_height, image_width])\n        annotations_ohe = tf.one_hot(annotations, num_classes, axis=-1)\n\n        print ""toto""\n        print annotations_ohe.get_shape().as_list()\n        print logits.get_shape().as_list()\n        # Actually compute the loss\n        print class_weights\n        loss = weighted_cross_entropy(logits=logits, onehot_labels=annotations_ohe, class_weights=class_weights)\n        total_loss = tf.losses.get_total_loss()\n\n        # Create the global step for monitoring the learning_rate and training.\n        global_step = get_or_create_global_step()\n\n        # Define your exponentially decaying learning rate\n        lr = tf.train.exponential_decay(\n            learning_rate=initial_learning_rate,\n            global_step=global_step,\n            decay_steps=decay_steps,\n            decay_rate=learning_rate_decay_factor,\n            staircase=True)\n\n        # Now we can define the optimizer that takes on the learning rate\n        optimizer = tf.train.AdamOptimizer(learning_rate=lr, epsilon=epsilon)\n\n        # Create the train_op.\n        train_op = slim.learning.create_train_op(total_loss, optimizer)\n\n        # State the metrics that you want to predict. We get a predictions that is not one_hot_encoded.\n        predictions = tf.argmax(probabilities, -1)\n        accuracy, accuracy_update = tf.contrib.metrics.streaming_accuracy(predictions, annotations)\n        mean_IOU, mean_IOU_update = tf.contrib.metrics.streaming_mean_iou(predictions=predictions, labels=annotations,\n                                                                          num_classes=num_classes)\n        metrics_op = tf.group(accuracy_update, mean_IOU_update)\n\n\n        # Now we need to create a training step function that runs both the train_op, metrics_op and updates the global_step concurrently.\n        def train_step(sess, train_op, global_step, metrics_op):\n            \'\'\'\n            Simply runs a session for the three arguments provided and gives a logging on the time elapsed for each global step\n            \'\'\'\n            # Check the time for each sess run\n            start_time = time.time()\n\n            # for i in range(a.shape[0]):\n            #    np.savetxt(\'activation\'+str(i)+\'.csv\',a[i].reshape(a[i].shape[0]*a[i].shape[1],a[i].shape[2]),delimiter=\',\')\n            total_loss, global_step_count, accuracy_val, mean_IOU_val, _ = sess.run(\n                [train_op, global_step, accuracy, mean_IOU, metrics_op])\n            time_elapsed = time.time() - start_time\n\n            # Run the logging to show some results\n            print \'global step %s: loss: %.4f (%.2f sec/step)    Current Streaming Accuracy: %.4f    Current Mean IOU: %.4f\', \\\n                global_step_count, total_loss, time_elapsed, accuracy_val, mean_IOU_val\n\n            return total_loss, accuracy_val, mean_IOU_val\n\n\n        # ================VALIDATION BRANCH========================\n        # Load the files into one input queue\n        images_val = tf.convert_to_tensor(image_val_files)\n        annotations_val = tf.convert_to_tensor(annotation_val_files)\n        input_queue_val = tf.train.slice_input_producer([images_val, annotations_val])\n\n        # Decode the image and annotation raw content\n        image_val = tf.read_file(input_queue_val[0])\n        image_val = tf.image.decode_jpeg(image_val, channels=3)\n        annotation_val = tf.read_file(input_queue_val[1])\n        annotation_val = tf.image.decode_png(annotation_val)\n\n        # preprocess and batch up the image and annotation\n        preprocessed_image_val, preprocessed_annotation_val = preprocess(image_val, annotation_val, image_height,\n                                                                         image_width)\n        images_val, annotations_val = tf.train.batch([preprocessed_image_val, preprocessed_annotation_val],\n                                                     batch_size=eval_batch_size, allow_smaller_final_batch=True)\n\n        logits_val, probabilities_val = squeeze_segnet(input=images_val, classes=1000, keep_prob=1,\n                                                       nb_classes=num_classes)\n\n        # perform one-hot-encoding on the ground truth annotation to get same shape as the logits\n        annotations_val = tf.reshape(annotations_val, shape=[eval_batch_size, image_height, image_width])\n        annotations_ohe_val = tf.one_hot(annotations_val, num_classes, axis=-1)\n\n        # State the metrics that you want to predict. We get a predictions that is not one_hot_encoded. ----> Should we use OHE instead?\n        predictions_val = tf.argmax(probabilities_val, -1)\n        accuracy_val, accuracy_val_update = tf.contrib.metrics.streaming_accuracy(predictions_val, annotations_val)\n        mean_IOU_val, mean_IOU_val_update = tf.contrib.metrics.streaming_mean_iou(predictions=predictions_val,\n                                                                                  labels=annotations_val,\n                                                                                  num_classes=num_classes)\n        metrics_op_val = tf.group(accuracy_val_update, mean_IOU_val_update)\n\n        # Create an output for showing the segmentation output of validation images\n        segmentation_output_val = tf.cast(predictions_val, dtype=tf.float32)\n        segmentation_output_val = tf.reshape(segmentation_output_val, shape=[-1, image_height, image_width, 1])\n        segmentation_ground_truth_val = tf.cast(annotations_val, dtype=tf.float32)\n        segmentation_ground_truth_val = tf.reshape(segmentation_ground_truth_val,\n                                                   shape=[-1, image_height, image_width, 1])\n\n\n        def eval_step(sess, metrics_op):\n            \'\'\'\n            Simply takes in a session, runs the metrics op and some logging information.\n            \'\'\'\n            start_time = time.time()\n            _, accuracy_value, mean_IOU_value = sess.run([metrics_op, accuracy_val, mean_IOU_val])\n            time_elapsed = time.time() - start_time\n\n            # Log some information\n            print \'---VALIDATION--- Validation Accuracy: %.4f    Validation Mean IOU: %.4f    (%.2f sec/step)\', \\\n                accuracy_value, mean_IOU_value, time_elapsed\n\n            return accuracy_value, mean_IOU_value\n\n\n        # =====================================================\n\n        # Now finally create all the summaries you need to monitor and group them into one summary op.\n        tf.summary.scalar(\'Monitor/Total_Loss\', total_loss)\n        tf.summary.scalar(\'Monitor/validation_accuracy\', accuracy_val)\n        tf.summary.scalar(\'Monitor/training_accuracy\', accuracy)\n        tf.summary.scalar(\'Monitor/validation_mean_IOU\', mean_IOU_val)\n        tf.summary.scalar(\'Monitor/training_mean_IOU\', mean_IOU)\n        tf.summary.scalar(\'Monitor/learning_rate\', lr)\n        tf.summary.image(\'Images/Validation_original_image\', images_val, max_outputs=1)\n        tf.summary.image(\'Images/Validation_segmentation_output\', segmentation_output_val, max_outputs=1)\n        tf.summary.image(\'Images/Validation_segmentation_ground_truth\', segmentation_ground_truth_val, max_outputs=1)\n        my_summary_op = tf.summary.merge_all()\n\n        # Define your supervisor for running a managed session. Do not run the summary_op automatically or else it will consume too much memory\n        sv = tf.train.Supervisor(logdir=\'logdir\', summary_op=None, init_fn=None)\n\n        # Run the managed session\n        with sv.managed_session() as sess:\n            for step in xrange(int(num_steps_per_epoch * num_epochs)):\n                # At the start of every epoch, show the vital information:\n                if step % num_batches_per_epoch == 0:\n                    print \'Epoch %s/%s\', step / num_batches_per_epoch + 1, num_epochs\n                    learning_rate_value = sess.run([lr])\n                    print \'Current Learning Rate: %s\', learning_rate_value\n\n                # Log the summaries every 10 steps or every end of epoch, which ever lower.\n                if step % min(num_steps_per_epoch, 10) == 0:\n                    loss, training_accuracy, training_mean_IOU = train_step(sess, train_op, sv.global_step,\n                                                                            metrics_op=metrics_op)\n\n                    # Check the validation data only at every third of an epoch\n                    if step % (num_steps_per_epoch / 3) == 0:\n                        for i in xrange(len(image_val_files) / eval_batch_size):\n                            validation_accuracy, validation_mean_IOU = eval_step(sess, metrics_op_val)\n\n                    summaries = sess.run(my_summary_op)\n                    sv.summary_computed(sess, summaries)\n\n                # If not, simply run the training step\n                else:\n                    loss, training_accuracy, training_mean_IOU = train_step(sess, train_op, sv.global_step,\n                                                                            metrics_op=metrics_op)\n\n            # We log the final training loss\n            print \'Final Loss: %s\', loss\n            print \'Final Training Accuracy: %s\', training_accuracy\n            print \'Final Training Mean IOU: %s\', training_mean_IOU\n            print \'Final Validation Accuracy: %s\', validation_accuracy\n            print \'Final Validation Mean IOU: %s\', validation_mean_IOU\n\n            # Once all the training has been done, save the log files and checkpoint model\n            print \'Finished training! Saving model to disk now.\'\n            sv.saver.save(sess, sv.save_path, global_step=sv.global_step)\n            photo_dir = \'photo_dir\'\n            if not os.path.exists(photo_dir):\n                os.mkdir(photo_dir)\n\n            # Plot the predictions - check validation images only\n            print \'Saving the images now...\'\n            predictions_value, annotations_value = sess.run([predictions_val, annotations_val])\n\n            for i in xrange(eval_batch_size):\n                predicted_annotation = predictions_value[i]\n                annotation = annotations_value[i]\n\n                plt.subplot(1, 2, 1)\n                plt.imshow(predicted_annotation)\n                plt.subplot(1, 2, 2)\n                plt.imshow(annotation)\n                plt.savefig(photo_dir + ""/image_"" + str(i))\n\n    # with tf.Session() as sess:\n    #     sess.run(tf.global_variables_initializer())\n    #     writer=tf.summary.FileWriter(\'output_graph\', sess.graph)\n    #     writer.close()'"
misc/transform.py,0,"b'# -*- coding: utf-8 -*-\nimport random\nimport collections\nimport logging as log\nimport torch\nimport numpy as np\nfrom PIL import Image, ImageOps\nimport time\nimport cv2\nimport matplotlib.pyplot as plt\n\n\ndef randomCropLetterboxPil(img):\n    output_w, output_h = (1408, 768)\n    jitter = 0.3\n    fill_color = 127\n\n    orig_w, orig_h = img.size\n    img_np = np.array(img)\n    channels = img_np.shape[2] if len(img_np.shape) > 2 else 1\n    dw = int(jitter * orig_w)\n    dh = int(jitter * orig_h)\n    new_ar = float(orig_w + random.randint(-dw, dw)) / (orig_h + random.randint(-dh, dh))\n    scale = random.random() * (2 - 0.25) + 0.25\n    if new_ar < 1:\n        nh = int(scale * orig_h)\n        nw = int(nh * new_ar)\n    else:\n        nw = int(scale * orig_w)\n        nh = int(nw / new_ar)\n\n    if output_w > nw:\n        dx = random.randint(0, output_w - nw)\n    else:\n        dx = random.randint(output_w - nw, 0)\n\n    if output_h > nh:\n        dy = random.randint(0, output_h - nh)\n    else:\n        dy = random.randint(output_h - nh, 0)\n\n    nxmin = max(0, -dx)\n    nymin = max(0, -dy)\n    nxmax = min(nw, -dx + output_w - 1)\n    nymax = min(nh, -dy + output_h - 1)\n    sx, sy = float(orig_w) / nw, float(orig_h) / nh\n    orig_xmin = int(nxmin * sx)\n    orig_ymin = int(nymin * sy)\n    orig_xmax = int(nxmax * sx)\n    orig_ymax = int(nymax * sy)\n    orig_crop = img.crop((orig_xmin, orig_ymin, orig_xmax, orig_ymax))\n    orig_crop_resize = orig_crop.resize((nxmax - nxmin, nymax - nymin))\n    output_img = Image.new(img.mode, (output_w, output_h), color=(fill_color,) * channels)\n    output_img.paste(orig_crop_resize, (0, 0))\n    return output_img\n\n\ndef randomCropLetterboxCv(img):\n    output_w, output_h = (1408, 768)\n    jitter = 0.3\n    fill_color = 127\n\n    # orig_w, orig_h = img.size\n    orig_h, orig_w = img.shape[:2]\n    channels = img.shape[2] if len(img.shape) > 2 else 1\n    dw = int(jitter * orig_w)\n    dh = int(jitter * orig_h)\n    new_ar = float(orig_w + random.randint(-dw, dw)) / (orig_h + random.randint(-dh, dh))\n    scale = random.random() * (2 - 0.25) + 0.25\n    if new_ar < 1:\n        nh = int(scale * orig_h)\n        nw = int(nh * new_ar)\n    else:\n        nw = int(scale * orig_w)\n        nh = int(nw / new_ar)\n\n    if output_w > nw:\n        dx = random.randint(0, output_w - nw)\n    else:\n        dx = random.randint(output_w - nw, 0)\n\n    if output_h > nh:\n        dy = random.randint(0, output_h - nh)\n    else:\n        dy = random.randint(output_h - nh, 0)\n\n    nxmin = max(0, -dx)\n    nymin = max(0, -dy)\n    nxmax = min(nw, -dx + output_w - 1)\n    nymax = min(nh, -dy + output_h - 1)\n    sx, sy = float(orig_w) / nw, float(orig_h) / nh\n    orig_xmin = int(nxmin * sx)\n    orig_ymin = int(nymin * sy)\n    orig_xmax = int(nxmax * sx)\n    orig_ymax = int(nymax * sy)\n    # orig_crop = img.crop((orig_xmin, orig_ymin, orig_xmax, orig_ymax))\n    orig_crop = img[orig_ymin:orig_ymax, orig_xmin:orig_xmax, :]\n    orig_crop_resize = cv2.resize(orig_crop, (nxmax - nxmin, nymax - nymin), interpolation=cv2.INTER_CUBIC)\n    output_img = np.ones((output_h, output_w, channels), dtype=np.uint8) * fill_color\n\n    y_lim = int(min(output_img.shape[0], orig_crop_resize.shape[0]))\n    x_lim = int(min(output_img.shape[1], orig_crop_resize.shape[1]))\n    output_img[:y_lim, :x_lim, :] = orig_crop_resize[:y_lim, :x_lim, :]\n    # orig_crop_resize = orig_crop.resize((nxmax - nxmin, nymax - nymin))\n    # output_img = Image.new(img.mode, (output_w, output_h), color=(fill_color,) * channels)\n    # output_img.paste(orig_crop_resize, (0, 0))\n    return output_img\n\n\ndef randomFlipPil(img):\n    """""" Randomly flip image """"""\n    flip = random.random() < 0.5\n    if flip:\n        img = img.transpose(Image.FLIP_LEFT_RIGHT)\n    return img\n\n\ndef randomFlipCv(img):\n    """""" Randomly flip image """"""\n    flip = random.random() < 0.5\n    if flip:\n        img = cv2.flip(img, 1)\n    return img\n\n\ndef hsvShiftPil(img):\n    hue = 0.1\n    sat = 1.5\n    val = 1.5\n\n    dh = random.uniform(-hue, hue)\n    ds = random.uniform(1, sat)\n    if random.random() < 0.5:\n        ds = 1 / ds\n    dv = random.uniform(1, val)\n    if random.random() < 0.5:\n        dv = 1 / dv\n\n    img = img.convert(\'HSV\')\n    channels = list(img.split())\n\n    def change_hue(x):\n        x += int(dh * 255)\n        if x > 255:\n            x -= 255\n        elif x < 0:\n            x += 0\n        return x\n\n    channels[0] = channels[0].point(change_hue)\n    channels[1] = channels[1].point(lambda i: min(255, max(0, int(i*ds))))\n    channels[2] = channels[2].point(lambda i: min(255, max(0, int(i*dv))))\n\n    img = Image.merge(img.mode, tuple(channels))\n    img = img.convert(\'RGB\')\n    return img\n\n\ndef hsvShiftCv(img):\n    hue = 0.1\n    sat = 1.5\n    val = 1.5\n\n    dh = random.uniform(-hue, hue)\n    ds = random.uniform(1, sat)\n    if random.random() < 0.5:\n        ds = 1 / ds\n    dv = random.uniform(1, val)\n    if random.random() < 0.5:\n        dv = 1 / dv\n\n    img = img.astype(np.float32) / 255.0\n    img = cv2.cvtColor(img, cv2.COLOR_RGB2HSV)\n\n    def wrap_hue(x):\n        x[x >= 360.0] -= 360.0\n        x[x < 0.0] += 360.0\n        return x\n\n    img[:, :, 0] = wrap_hue(img[:, :, 0] + (360.0 * dh))\n    img[:, :, 1] = np.clip(ds * img[:, :, 1], 0.0, 1.0)\n    img[:, :, 2] = np.clip(dv * img[:, :, 2], 0.0, 1.0)\n\n    img = cv2.cvtColor(img, cv2.COLOR_HSV2RGB)\n    img = (img * 255).astype(np.uint8)\n    return img\n\n\nif __name__ == \'__main__\':\n    test_num = 100\n    img = Image.open(\'../data/0006R0_f00930.png\')\n    img = img.resize((1024, 720))\n    out_img = img\n    start_time = time.time()\n    for _ in range(test_num):\n        out_img = randomCropLetterboxPil(out_img)\n        out_img = randomFlipPil(out_img)\n        out_img = hsvShiftPil(out_img)\n        # plt.imshow(out_img)\n        # plt.show()\n    end_time = time.time()\n    print(\'cost time:\', (end_time-start_time)/test_num)\n\n    test_num = 100\n    img = cv2.imread(\'../data/0006R0_f00930.png\')\n    img = cv2.resize(img, (1024, 720), interpolation=cv2.INTER_CUBIC)\n    out_img = img\n    # print(\'img.shape:\', img.shape)\n    start_time = time.time()\n    for _ in range(test_num):\n        out_img = randomCropLetterboxCv(out_img)\n        out_img = randomFlipCv(out_img)\n        out_img = hsvShiftCv(out_img)\n        # cv2.imshow(\'img\', out_img)\n        # cv2.waitKey(0)\n    end_time = time.time()\n    print(\'cost time:\', (end_time-start_time)/test_num)\n'"
misc/visdom_offline_data.py,0,"b'# -*- coding: utf-8 -*-\n\nimport numpy as np\nimport visdom\nimport json\nimport time\n\ninit_time = str(int(time.time()))\nvis = visdom.Visdom()\nwin = \'det_loss_iteration\'\nwin_data = vis.get_window_data(win)\nwin_data_dict = json.loads(win_data)\nwin_data_content_dict = win_data_dict[\'content\']\nwin_data_x = np.array(win_data_content_dict[\'data\'][0][\'x\'])\nwin_data_y = np.array(win_data_content_dict[\'data\'][0][\'y\'])\n\nwin_data_save_file = \'/tmp/loss_iteration_{}.txt\'.format(init_time)\nwith open(win_data_save_file, \'wb\') as f:\n    for item_x, item_y in zip(win_data_x, win_data_y):\n        f.write(""{}\\t{}\\n"".format(item_x, item_y))\ndone_time = str(int(time.time()))\n'"
semseg/__init__.py,0,b''
semseg/caffe_pb2.py,0,"b'# Originally from: https://github.com/chainer/chainer/blob/v3.3.0/chainer/links/caffe/protobuf3/caffe_pb2.py\n# Adapted for internal use.\n\n# Generated by the protocol buffer compiler.  DO NOT EDIT!\n# source: caffe.proto\n\nimport sys\n_b=sys.version_info[0]<3 and (lambda x:x) or (lambda x:x.encode(\'latin1\'))\n\ntry:\n    from google.protobuf.internal import enum_type_wrapper\n    from google.protobuf import descriptor as _descriptor\n    from google.protobuf import message as _message\n    from google.protobuf import reflection as _reflection\n    from google.protobuf import symbol_database as _symbol_database\n    from google.protobuf import descriptor_pb2\n    # @@protoc_insertion_point(imports)\nexcept:\n    print(""""""Unable to import google protobuf\n    Please install protobuf from https://github.com/google/protobuf\n    with Python bindings"""""")\n    sys.exit()\n\n_sym_db = _symbol_database.Default()\n\n\n\n\nDESCRIPTOR = _descriptor.FileDescriptor(\n  name=\'caffe.proto\',\n  package=\'caffe\',\n  syntax=\'proto2\',\n  serialized_pb=_b(\'\\n\\x0b\\x63\\x61\\x66\\x66\\x65.proto\\x12\\x05\\x63\\x61\\x66\\x66\\x65\\""\\x1c\\n\\tBlobShape\\x12\\x0f\\n\\x03\\x64im\\x18\\x01 \\x03(\\x03\\x42\\x02\\x10\\x01\\""\\xcc\\x01\\n\\tBlobProto\\x12\\x1f\\n\\x05shape\\x18\\x07 \\x01(\\x0b\\x32\\x10.caffe.BlobShape\\x12\\x10\\n\\x04\\x64\\x61ta\\x18\\x05 \\x03(\\x02\\x42\\x02\\x10\\x01\\x12\\x10\\n\\x04\\x64iff\\x18\\x06 \\x03(\\x02\\x42\\x02\\x10\\x01\\x12\\x17\\n\\x0b\\x64ouble_data\\x18\\x08 \\x03(\\x01\\x42\\x02\\x10\\x01\\x12\\x17\\n\\x0b\\x64ouble_diff\\x18\\t \\x03(\\x01\\x42\\x02\\x10\\x01\\x12\\x0e\\n\\x03num\\x18\\x01 \\x01(\\x05:\\x01\\x30\\x12\\x13\\n\\x08\\x63hannels\\x18\\x02 \\x01(\\x05:\\x01\\x30\\x12\\x11\\n\\x06height\\x18\\x03 \\x01(\\x05:\\x01\\x30\\x12\\x10\\n\\x05width\\x18\\x04 \\x01(\\x05:\\x01\\x30\\""2\\n\\x0f\\x42lobProtoVector\\x12\\x1f\\n\\x05\\x62lobs\\x18\\x01 \\x03(\\x0b\\x32\\x10.caffe.BlobProto\\""\\x81\\x01\\n\\x05\\x44\\x61tum\\x12\\x10\\n\\x08\\x63hannels\\x18\\x01 \\x01(\\x05\\x12\\x0e\\n\\x06height\\x18\\x02 \\x01(\\x05\\x12\\r\\n\\x05width\\x18\\x03 \\x01(\\x05\\x12\\x0c\\n\\x04\\x64\\x61ta\\x18\\x04 \\x01(\\x0c\\x12\\r\\n\\x05label\\x18\\x05 \\x01(\\x05\\x12\\x12\\n\\nfloat_data\\x18\\x06 \\x03(\\x02\\x12\\x16\\n\\x07\\x65ncoded\\x18\\x07 \\x01(\\x08:\\x05\\x66\\x61lse\\""\\x8a\\x02\\n\\x0f\\x46illerParameter\\x12\\x16\\n\\x04type\\x18\\x01 \\x01(\\t:\\x08\\x63onstant\\x12\\x10\\n\\x05value\\x18\\x02 \\x01(\\x02:\\x01\\x30\\x12\\x0e\\n\\x03min\\x18\\x03 \\x01(\\x02:\\x01\\x30\\x12\\x0e\\n\\x03max\\x18\\x04 \\x01(\\x02:\\x01\\x31\\x12\\x0f\\n\\x04mean\\x18\\x05 \\x01(\\x02:\\x01\\x30\\x12\\x0e\\n\\x03std\\x18\\x06 \\x01(\\x02:\\x01\\x31\\x12\\x12\\n\\x06sparse\\x18\\x07 \\x01(\\x05:\\x02-1\\x12\\x42\\n\\rvariance_norm\\x18\\x08 \\x01(\\x0e\\x32#.caffe.FillerParameter.VarianceNorm:\\x06\\x46\\x41N_IN\\""4\\n\\x0cVarianceNorm\\x12\\n\\n\\x06\\x46\\x41N_IN\\x10\\x00\\x12\\x0b\\n\\x07\\x46\\x41N_OUT\\x10\\x01\\x12\\x0b\\n\\x07\\x41VERAGE\\x10\\x02\\""\\x8e\\x02\\n\\x0cNetParameter\\x12\\x0c\\n\\x04name\\x18\\x01 \\x01(\\t\\x12\\r\\n\\x05input\\x18\\x03 \\x03(\\t\\x12%\\n\\x0binput_shape\\x18\\x08 \\x03(\\x0b\\x32\\x10.caffe.BlobShape\\x12\\x11\\n\\tinput_dim\\x18\\x04 \\x03(\\x05\\x12\\x1d\\n\\x0e\\x66orce_backward\\x18\\x05 \\x01(\\x08:\\x05\\x66\\x61lse\\x12\\x1e\\n\\x05state\\x18\\x06 \\x01(\\x0b\\x32\\x0f.caffe.NetState\\x12\\x19\\n\\ndebug_info\\x18\\x07 \\x01(\\x08:\\x05\\x66\\x61lse\\x12$\\n\\x05layer\\x18\\x64 \\x03(\\x0b\\x32\\x15.caffe.LayerParameter\\x12\\\'\\n\\x06layers\\x18\\x02 \\x03(\\x0b\\x32\\x17.caffe.V1LayerParameter\\""\\x9c\\n\\n\\x0fSolverParameter\\x12\\x0b\\n\\x03net\\x18\\x18 \\x01(\\t\\x12&\\n\\tnet_param\\x18\\x19 \\x01(\\x0b\\x32\\x13.caffe.NetParameter\\x12\\x11\\n\\ttrain_net\\x18\\x01 \\x01(\\t\\x12\\x10\\n\\x08test_net\\x18\\x02 \\x03(\\t\\x12,\\n\\x0ftrain_net_param\\x18\\x15 \\x01(\\x0b\\x32\\x13.caffe.NetParameter\\x12+\\n\\x0etest_net_param\\x18\\x16 \\x03(\\x0b\\x32\\x13.caffe.NetParameter\\x12$\\n\\x0btrain_state\\x18\\x1a \\x01(\\x0b\\x32\\x0f.caffe.NetState\\x12#\\n\\ntest_state\\x18\\x1b \\x03(\\x0b\\x32\\x0f.caffe.NetState\\x12\\x11\\n\\ttest_iter\\x18\\x03 \\x03(\\x05\\x12\\x18\\n\\rtest_interval\\x18\\x04 \\x01(\\x05:\\x01\\x30\\x12 \\n\\x11test_compute_loss\\x18\\x13 \\x01(\\x08:\\x05\\x66\\x61lse\\x12!\\n\\x13test_initialization\\x18  \\x01(\\x08:\\x04true\\x12\\x0f\\n\\x07\\x62\\x61se_lr\\x18\\x05 \\x01(\\x02\\x12\\x0f\\n\\x07\\x64isplay\\x18\\x06 \\x01(\\x05\\x12\\x17\\n\\x0c\\x61verage_loss\\x18! \\x01(\\x05:\\x01\\x31\\x12\\x10\\n\\x08max_iter\\x18\\x07 \\x01(\\x05\\x12\\x14\\n\\titer_size\\x18$ \\x01(\\x05:\\x01\\x31\\x12\\x11\\n\\tlr_policy\\x18\\x08 \\x01(\\t\\x12\\r\\n\\x05gamma\\x18\\t \\x01(\\x02\\x12\\r\\n\\x05power\\x18\\n \\x01(\\x02\\x12\\x10\\n\\x08momentum\\x18\\x0b \\x01(\\x02\\x12\\x14\\n\\x0cweight_decay\\x18\\x0c \\x01(\\x02\\x12\\x1f\\n\\x13regularization_type\\x18\\x1d \\x01(\\t:\\x02L2\\x12\\x10\\n\\x08stepsize\\x18\\r \\x01(\\x05\\x12\\x11\\n\\tstepvalue\\x18\\"" \\x03(\\x05\\x12\\x1a\\n\\x0e\\x63lip_gradients\\x18# \\x01(\\x02:\\x02-1\\x12\\x13\\n\\x08snapshot\\x18\\x0e \\x01(\\x05:\\x01\\x30\\x12\\x17\\n\\x0fsnapshot_prefix\\x18\\x0f \\x01(\\t\\x12\\x1c\\n\\rsnapshot_diff\\x18\\x10 \\x01(\\x08:\\x05\\x66\\x61lse\\x12K\\n\\x0fsnapshot_format\\x18% \\x01(\\x0e\\x32%.caffe.SolverParameter.SnapshotFormat:\\x0b\\x42INARYPROTO\\x12;\\n\\x0bsolver_mode\\x18\\x11 \\x01(\\x0e\\x32!.caffe.SolverParameter.SolverMode:\\x03GPU\\x12\\x14\\n\\tdevice_id\\x18\\x12 \\x01(\\x05:\\x01\\x30\\x12\\x17\\n\\x0brandom_seed\\x18\\x14 \\x01(\\x03:\\x02-1\\x12\\x11\\n\\x04type\\x18( \\x01(\\t:\\x03SGD\\x12\\x14\\n\\x05\\x64\\x65lta\\x18\\x1f \\x01(\\x02:\\x05\\x31\\x65-08\\x12\\x18\\n\\tmomentum2\\x18\\\' \\x01(\\x02:\\x05\\x30.999\\x12\\x11\\n\\trms_decay\\x18& \\x01(\\x02\\x12\\x19\\n\\ndebug_info\\x18\\x17 \\x01(\\x08:\\x05\\x66\\x61lse\\x12\\""\\n\\x14snapshot_after_train\\x18\\x1c \\x01(\\x08:\\x04true\\x12;\\n\\x0bsolver_type\\x18\\x1e \\x01(\\x0e\\x32!.caffe.SolverParameter.SolverType:\\x03SGD\\""+\\n\\x0eSnapshotFormat\\x12\\x08\\n\\x04HDF5\\x10\\x00\\x12\\x0f\\n\\x0b\\x42INARYPROTO\\x10\\x01\\""\\x1e\\n\\nSolverMode\\x12\\x07\\n\\x03\\x43PU\\x10\\x00\\x12\\x07\\n\\x03GPU\\x10\\x01\\""U\\n\\nSolverType\\x12\\x07\\n\\x03SGD\\x10\\x00\\x12\\x0c\\n\\x08NESTEROV\\x10\\x01\\x12\\x0b\\n\\x07\\x41\\x44\\x41GRAD\\x10\\x02\\x12\\x0b\\n\\x07RMSPROP\\x10\\x03\\x12\\x0c\\n\\x08\\x41\\x44\\x41\\x44\\x45LTA\\x10\\x04\\x12\\x08\\n\\x04\\x41\\x44\\x41M\\x10\\x05\\""l\\n\\x0bSolverState\\x12\\x0c\\n\\x04iter\\x18\\x01 \\x01(\\x05\\x12\\x13\\n\\x0blearned_net\\x18\\x02 \\x01(\\t\\x12!\\n\\x07history\\x18\\x03 \\x03(\\x0b\\x32\\x10.caffe.BlobProto\\x12\\x17\\n\\x0c\\x63urrent_step\\x18\\x04 \\x01(\\x05:\\x01\\x30\\""N\\n\\x08NetState\\x12!\\n\\x05phase\\x18\\x01 \\x01(\\x0e\\x32\\x0c.caffe.Phase:\\x04TEST\\x12\\x10\\n\\x05level\\x18\\x02 \\x01(\\x05:\\x01\\x30\\x12\\r\\n\\x05stage\\x18\\x03 \\x03(\\t\\""s\\n\\x0cNetStateRule\\x12\\x1b\\n\\x05phase\\x18\\x01 \\x01(\\x0e\\x32\\x0c.caffe.Phase\\x12\\x11\\n\\tmin_level\\x18\\x02 \\x01(\\x05\\x12\\x11\\n\\tmax_level\\x18\\x03 \\x01(\\x05\\x12\\r\\n\\x05stage\\x18\\x04 \\x03(\\t\\x12\\x11\\n\\tnot_stage\\x18\\x05 \\x03(\\t\\""\\xa3\\x01\\n\\tParamSpec\\x12\\x0c\\n\\x04name\\x18\\x01 \\x01(\\t\\x12\\x31\\n\\nshare_mode\\x18\\x02 \\x01(\\x0e\\x32\\x1d.caffe.ParamSpec.DimCheckMode\\x12\\x12\\n\\x07lr_mult\\x18\\x03 \\x01(\\x02:\\x01\\x31\\x12\\x15\\n\\ndecay_mult\\x18\\x04 \\x01(\\x02:\\x01\\x31\\""*\\n\\x0c\\x44imCheckMode\\x12\\n\\n\\x06STRICT\\x10\\x00\\x12\\x0e\\n\\nPERMISSIVE\\x10\\x01\\""\\xc0\\x12\\n\\x0eLayerParameter\\x12\\x0c\\n\\x04name\\x18\\x01 \\x01(\\t\\x12\\x0c\\n\\x04type\\x18\\x02 \\x01(\\t\\x12\\x0e\\n\\x06\\x62ottom\\x18\\x03 \\x03(\\t\\x12\\x0b\\n\\x03top\\x18\\x04 \\x03(\\t\\x12\\x1b\\n\\x05phase\\x18\\n \\x01(\\x0e\\x32\\x0c.caffe.Phase\\x12\\x13\\n\\x0bloss_weight\\x18\\x05 \\x03(\\x02\\x12\\x1f\\n\\x05param\\x18\\x06 \\x03(\\x0b\\x32\\x10.caffe.ParamSpec\\x12\\x1f\\n\\x05\\x62lobs\\x18\\x07 \\x03(\\x0b\\x32\\x10.caffe.BlobProto\\x12\\x16\\n\\x0epropagate_down\\x18\\x0b \\x03(\\x08\\x12$\\n\\x07include\\x18\\x08 \\x03(\\x0b\\x32\\x13.caffe.NetStateRule\\x12$\\n\\x07\\x65xclude\\x18\\t \\x03(\\x0b\\x32\\x13.caffe.NetStateRule\\x12\\x37\\n\\x0ftransform_param\\x18\\x64 \\x01(\\x0b\\x32\\x1e.caffe.TransformationParameter\\x12(\\n\\nloss_param\\x18\\x65 \\x01(\\x0b\\x32\\x14.caffe.LossParameter\\x12\\x30\\n\\x0e\\x61\\x63\\x63uracy_param\\x18\\x66 \\x01(\\x0b\\x32\\x18.caffe.AccuracyParameter\\x12,\\n\\x0c\\x61rgmax_param\\x18g \\x01(\\x0b\\x32\\x16.caffe.ArgMaxParameter\\x12\\x34\\n\\x10\\x62\\x61tch_norm_param\\x18\\x8b\\x01 \\x01(\\x0b\\x32\\x19.caffe.BatchNormParameter\\x12)\\n\\nbias_param\\x18\\x8d\\x01 \\x01(\\x0b\\x32\\x14.caffe.BiasParameter\\x12,\\n\\x0c\\x63oncat_param\\x18h \\x01(\\x0b\\x32\\x16.caffe.ConcatParameter\\x12?\\n\\x16\\x63ontrastive_loss_param\\x18i \\x01(\\x0b\\x32\\x1f.caffe.ContrastiveLossParameter\\x12\\x36\\n\\x11\\x63onvolution_param\\x18j \\x01(\\x0b\\x32\\x1b.caffe.ConvolutionParameter\\x12(\\n\\ndata_param\\x18k \\x01(\\x0b\\x32\\x14.caffe.DataParameter\\x12.\\n\\rdropout_param\\x18l \\x01(\\x0b\\x32\\x17.caffe.DropoutParameter\\x12\\x33\\n\\x10\\x64ummy_data_param\\x18m \\x01(\\x0b\\x32\\x19.caffe.DummyDataParameter\\x12.\\n\\reltwise_param\\x18n \\x01(\\x0b\\x32\\x17.caffe.EltwiseParameter\\x12\\\'\\n\\telu_param\\x18\\x8c\\x01 \\x01(\\x0b\\x32\\x13.caffe.ELUParameter\\x12+\\n\\x0b\\x65mbed_param\\x18\\x89\\x01 \\x01(\\x0b\\x32\\x15.caffe.EmbedParameter\\x12&\\n\\texp_param\\x18o \\x01(\\x0b\\x32\\x13.caffe.ExpParameter\\x12/\\n\\rflatten_param\\x18\\x87\\x01 \\x01(\\x0b\\x32\\x17.caffe.FlattenParameter\\x12\\x31\\n\\x0fhdf5_data_param\\x18p \\x01(\\x0b\\x32\\x18.caffe.HDF5DataParameter\\x12\\x35\\n\\x11hdf5_output_param\\x18q \\x01(\\x0b\\x32\\x1a.caffe.HDF5OutputParameter\\x12\\x33\\n\\x10hinge_loss_param\\x18r \\x01(\\x0b\\x32\\x19.caffe.HingeLossParameter\\x12\\x33\\n\\x10image_data_param\\x18s \\x01(\\x0b\\x32\\x19.caffe.ImageDataParameter\\x12\\x39\\n\\x13infogain_loss_param\\x18t \\x01(\\x0b\\x32\\x1c.caffe.InfogainLossParameter\\x12\\x39\\n\\x13inner_product_param\\x18u \\x01(\\x0b\\x32\\x1c.caffe.InnerProductParameter\\x12\\\'\\n\\tlog_param\\x18\\x86\\x01 \\x01(\\x0b\\x32\\x13.caffe.LogParameter\\x12&\\n\\tlrn_param\\x18v \\x01(\\x0b\\x32\\x13.caffe.LRNParameter\\x12\\x35\\n\\x11memory_data_param\\x18w \\x01(\\x0b\\x32\\x1a.caffe.MemoryDataParameter\\x12&\\n\\tmvn_param\\x18x \\x01(\\x0b\\x32\\x13.caffe.MVNParameter\\x12.\\n\\rpooling_param\\x18y \\x01(\\x0b\\x32\\x17.caffe.PoolingParameter\\x12*\\n\\x0bpower_param\\x18z \\x01(\\x0b\\x32\\x15.caffe.PowerParameter\\x12+\\n\\x0bprelu_param\\x18\\x83\\x01 \\x01(\\x0b\\x32\\x15.caffe.PReLUParameter\\x12-\\n\\x0cpython_param\\x18\\x82\\x01 \\x01(\\x0b\\x32\\x16.caffe.PythonParameter\\x12\\x33\\n\\x0freduction_param\\x18\\x88\\x01 \\x01(\\x0b\\x32\\x19.caffe.ReductionParameter\\x12(\\n\\nrelu_param\\x18{ \\x01(\\x0b\\x32\\x14.caffe.ReLUParameter\\x12/\\n\\rreshape_param\\x18\\x85\\x01 \\x01(\\x0b\\x32\\x17.caffe.ReshapeParameter\\x12+\\n\\x0bscale_param\\x18\\x8e\\x01 \\x01(\\x0b\\x32\\x15.caffe.ScaleParameter\\x12.\\n\\rsigmoid_param\\x18| \\x01(\\x0b\\x32\\x17.caffe.SigmoidParameter\\x12.\\n\\rsoftmax_param\\x18} \\x01(\\x0b\\x32\\x17.caffe.SoftmaxParameter\\x12\\\'\\n\\tspp_param\\x18\\x84\\x01 \\x01(\\x0b\\x32\\x13.caffe.SPPParameter\\x12*\\n\\x0bslice_param\\x18~ \\x01(\\x0b\\x32\\x15.caffe.SliceParameter\\x12(\\n\\ntanh_param\\x18\\x7f \\x01(\\x0b\\x32\\x14.caffe.TanHParameter\\x12\\x33\\n\\x0fthreshold_param\\x18\\x80\\x01 \\x01(\\x0b\\x32\\x19.caffe.ThresholdParameter\\x12)\\n\\ntile_param\\x18\\x8a\\x01 \\x01(\\x0b\\x32\\x14.caffe.TileParameter\\x12\\x36\\n\\x11window_data_param\\x18\\x81\\x01 \\x01(\\x0b\\x32\\x1a.caffe.WindowDataParameter\\""\\xb6\\x01\\n\\x17TransformationParameter\\x12\\x10\\n\\x05scale\\x18\\x01 \\x01(\\x02:\\x01\\x31\\x12\\x15\\n\\x06mirror\\x18\\x02 \\x01(\\x08:\\x05\\x66\\x61lse\\x12\\x14\\n\\tcrop_size\\x18\\x03 \\x01(\\r:\\x01\\x30\\x12\\x11\\n\\tmean_file\\x18\\x04 \\x01(\\t\\x12\\x12\\n\\nmean_value\\x18\\x05 \\x03(\\x02\\x12\\x1a\\n\\x0b\\x66orce_color\\x18\\x06 \\x01(\\x08:\\x05\\x66\\x61lse\\x12\\x19\\n\\nforce_gray\\x18\\x07 \\x01(\\x08:\\x05\\x66\\x61lse\\""\\xc2\\x01\\n\\rLossParameter\\x12\\x14\\n\\x0cignore_label\\x18\\x01 \\x01(\\x05\\x12\\x44\\n\\rnormalization\\x18\\x03 \\x01(\\x0e\\x32&.caffe.LossParameter.NormalizationMode:\\x05VALID\\x12\\x11\\n\\tnormalize\\x18\\x02 \\x01(\\x08\\""B\\n\\x11NormalizationMode\\x12\\x08\\n\\x04\\x46ULL\\x10\\x00\\x12\\t\\n\\x05VALID\\x10\\x01\\x12\\x0e\\n\\nBATCH_SIZE\\x10\\x02\\x12\\x08\\n\\x04NONE\\x10\\x03\\""L\\n\\x11\\x41\\x63\\x63uracyParameter\\x12\\x10\\n\\x05top_k\\x18\\x01 \\x01(\\r:\\x01\\x31\\x12\\x0f\\n\\x04\\x61xis\\x18\\x02 \\x01(\\x05:\\x01\\x31\\x12\\x14\\n\\x0cignore_label\\x18\\x03 \\x01(\\x05\\""M\\n\\x0f\\x41rgMaxParameter\\x12\\x1a\\n\\x0bout_max_val\\x18\\x01 \\x01(\\x08:\\x05\\x66\\x61lse\\x12\\x10\\n\\x05top_k\\x18\\x02 \\x01(\\r:\\x01\\x31\\x12\\x0c\\n\\x04\\x61xis\\x18\\x03 \\x01(\\x05\\""9\\n\\x0f\\x43oncatParameter\\x12\\x0f\\n\\x04\\x61xis\\x18\\x02 \\x01(\\x05:\\x01\\x31\\x12\\x15\\n\\nconcat_dim\\x18\\x01 \\x01(\\r:\\x01\\x31\\""j\\n\\x12\\x42\\x61tchNormParameter\\x12\\x18\\n\\x10use_global_stats\\x18\\x01 \\x01(\\x08\\x12&\\n\\x17moving_average_fraction\\x18\\x02 \\x01(\\x02:\\x05\\x30.999\\x12\\x12\\n\\x03\\x65ps\\x18\\x03 \\x01(\\x02:\\x05\\x31\\x65-05\\""]\\n\\rBiasParameter\\x12\\x0f\\n\\x04\\x61xis\\x18\\x01 \\x01(\\x05:\\x01\\x31\\x12\\x13\\n\\x08num_axes\\x18\\x02 \\x01(\\x05:\\x01\\x31\\x12&\\n\\x06\\x66iller\\x18\\x03 \\x01(\\x0b\\x32\\x16.caffe.FillerParameter\\""L\\n\\x18\\x43ontrastiveLossParameter\\x12\\x11\\n\\x06margin\\x18\\x01 \\x01(\\x02:\\x01\\x31\\x12\\x1d\\n\\x0elegacy_version\\x18\\x02 \\x01(\\x08:\\x05\\x66\\x61lse\\""\\xfc\\x03\\n\\x14\\x43onvolutionParameter\\x12\\x12\\n\\nnum_output\\x18\\x01 \\x01(\\r\\x12\\x17\\n\\tbias_term\\x18\\x02 \\x01(\\x08:\\x04true\\x12\\x0b\\n\\x03pad\\x18\\x03 \\x03(\\r\\x12\\x13\\n\\x0bkernel_size\\x18\\x04 \\x03(\\r\\x12\\x0e\\n\\x06stride\\x18\\x06 \\x03(\\r\\x12\\x10\\n\\x08\\x64ilation\\x18\\x12 \\x03(\\r\\x12\\x10\\n\\x05pad_h\\x18\\t \\x01(\\r:\\x01\\x30\\x12\\x10\\n\\x05pad_w\\x18\\n \\x01(\\r:\\x01\\x30\\x12\\x10\\n\\x08kernel_h\\x18\\x0b \\x01(\\r\\x12\\x10\\n\\x08kernel_w\\x18\\x0c \\x01(\\r\\x12\\x10\\n\\x08stride_h\\x18\\r \\x01(\\r\\x12\\x10\\n\\x08stride_w\\x18\\x0e \\x01(\\r\\x12\\x10\\n\\x05group\\x18\\x05 \\x01(\\r:\\x01\\x31\\x12-\\n\\rweight_filler\\x18\\x07 \\x01(\\x0b\\x32\\x16.caffe.FillerParameter\\x12+\\n\\x0b\\x62ias_filler\\x18\\x08 \\x01(\\x0b\\x32\\x16.caffe.FillerParameter\\x12;\\n\\x06\\x65ngine\\x18\\x0f \\x01(\\x0e\\x32\\"".caffe.ConvolutionParameter.Engine:\\x07\\x44\\x45\\x46\\x41ULT\\x12\\x0f\\n\\x04\\x61xis\\x18\\x10 \\x01(\\x05:\\x01\\x31\\x12\\x1e\\n\\x0f\\x66orce_nd_im2col\\x18\\x11 \\x01(\\x08:\\x05\\x66\\x61lse\\""+\\n\\x06\\x45ngine\\x12\\x0b\\n\\x07\\x44\\x45\\x46\\x41ULT\\x10\\x00\\x12\\t\\n\\x05\\x43\\x41\\x46\\x46\\x45\\x10\\x01\\x12\\t\\n\\x05\\x43UDNN\\x10\\x02\\""\\xa4\\x02\\n\\rDataParameter\\x12\\x0e\\n\\x06source\\x18\\x01 \\x01(\\t\\x12\\x12\\n\\nbatch_size\\x18\\x04 \\x01(\\r\\x12\\x14\\n\\trand_skip\\x18\\x07 \\x01(\\r:\\x01\\x30\\x12\\x31\\n\\x07\\x62\\x61\\x63kend\\x18\\x08 \\x01(\\x0e\\x32\\x17.caffe.DataParameter.DB:\\x07LEVELDB\\x12\\x10\\n\\x05scale\\x18\\x02 \\x01(\\x02:\\x01\\x31\\x12\\x11\\n\\tmean_file\\x18\\x03 \\x01(\\t\\x12\\x14\\n\\tcrop_size\\x18\\x05 \\x01(\\r:\\x01\\x30\\x12\\x15\\n\\x06mirror\\x18\\x06 \\x01(\\x08:\\x05\\x66\\x61lse\\x12\\""\\n\\x13\\x66orce_encoded_color\\x18\\t \\x01(\\x08:\\x05\\x66\\x61lse\\x12\\x13\\n\\x08prefetch\\x18\\n \\x01(\\r:\\x01\\x34\\""\\x1b\\n\\x02\\x44\\x42\\x12\\x0b\\n\\x07LEVELDB\\x10\\x00\\x12\\x08\\n\\x04LMDB\\x10\\x01\\"".\\n\\x10\\x44ropoutParameter\\x12\\x1a\\n\\rdropout_ratio\\x18\\x01 \\x01(\\x02:\\x03\\x30.5\\""\\xa0\\x01\\n\\x12\\x44ummyDataParameter\\x12+\\n\\x0b\\x64\\x61ta_filler\\x18\\x01 \\x03(\\x0b\\x32\\x16.caffe.FillerParameter\\x12\\x1f\\n\\x05shape\\x18\\x06 \\x03(\\x0b\\x32\\x10.caffe.BlobShape\\x12\\x0b\\n\\x03num\\x18\\x02 \\x03(\\r\\x12\\x10\\n\\x08\\x63hannels\\x18\\x03 \\x03(\\r\\x12\\x0e\\n\\x06height\\x18\\x04 \\x03(\\r\\x12\\r\\n\\x05width\\x18\\x05 \\x03(\\r\\""\\xa5\\x01\\n\\x10\\x45ltwiseParameter\\x12\\x39\\n\\toperation\\x18\\x01 \\x01(\\x0e\\x32!.caffe.EltwiseParameter.EltwiseOp:\\x03SUM\\x12\\r\\n\\x05\\x63oeff\\x18\\x02 \\x03(\\x02\\x12\\x1e\\n\\x10stable_prod_grad\\x18\\x03 \\x01(\\x08:\\x04true\\""\\\'\\n\\tEltwiseOp\\x12\\x08\\n\\x04PROD\\x10\\x00\\x12\\x07\\n\\x03SUM\\x10\\x01\\x12\\x07\\n\\x03MAX\\x10\\x02\\"" \\n\\x0c\\x45LUParameter\\x12\\x10\\n\\x05\\x61lpha\\x18\\x01 \\x01(\\x02:\\x01\\x31\\""\\xac\\x01\\n\\x0e\\x45mbedParameter\\x12\\x12\\n\\nnum_output\\x18\\x01 \\x01(\\r\\x12\\x11\\n\\tinput_dim\\x18\\x02 \\x01(\\r\\x12\\x17\\n\\tbias_term\\x18\\x03 \\x01(\\x08:\\x04true\\x12-\\n\\rweight_filler\\x18\\x04 \\x01(\\x0b\\x32\\x16.caffe.FillerParameter\\x12+\\n\\x0b\\x62ias_filler\\x18\\x05 \\x01(\\x0b\\x32\\x16.caffe.FillerParameter\\""D\\n\\x0c\\x45xpParameter\\x12\\x10\\n\\x04\\x62\\x61se\\x18\\x01 \\x01(\\x02:\\x02-1\\x12\\x10\\n\\x05scale\\x18\\x02 \\x01(\\x02:\\x01\\x31\\x12\\x10\\n\\x05shift\\x18\\x03 \\x01(\\x02:\\x01\\x30\\""9\\n\\x10\\x46lattenParameter\\x12\\x0f\\n\\x04\\x61xis\\x18\\x01 \\x01(\\x05:\\x01\\x31\\x12\\x14\\n\\x08\\x65nd_axis\\x18\\x02 \\x01(\\x05:\\x02-1\\""O\\n\\x11HDF5DataParameter\\x12\\x0e\\n\\x06source\\x18\\x01 \\x01(\\t\\x12\\x12\\n\\nbatch_size\\x18\\x02 \\x01(\\r\\x12\\x16\\n\\x07shuffle\\x18\\x03 \\x01(\\x08:\\x05\\x66\\x61lse\\""(\\n\\x13HDF5OutputParameter\\x12\\x11\\n\\tfile_name\\x18\\x01 \\x01(\\t\\""^\\n\\x12HingeLossParameter\\x12\\x30\\n\\x04norm\\x18\\x01 \\x01(\\x0e\\x32\\x1e.caffe.HingeLossParameter.Norm:\\x02L1\\""\\x16\\n\\x04Norm\\x12\\x06\\n\\x02L1\\x10\\x01\\x12\\x06\\n\\x02L2\\x10\\x02\\""\\x97\\x02\\n\\x12ImageDataParameter\\x12\\x0e\\n\\x06source\\x18\\x01 \\x01(\\t\\x12\\x15\\n\\nbatch_size\\x18\\x04 \\x01(\\r:\\x01\\x31\\x12\\x14\\n\\trand_skip\\x18\\x07 \\x01(\\r:\\x01\\x30\\x12\\x16\\n\\x07shuffle\\x18\\x08 \\x01(\\x08:\\x05\\x66\\x61lse\\x12\\x15\\n\\nnew_height\\x18\\t \\x01(\\r:\\x01\\x30\\x12\\x14\\n\\tnew_width\\x18\\n \\x01(\\r:\\x01\\x30\\x12\\x16\\n\\x08is_color\\x18\\x0b \\x01(\\x08:\\x04true\\x12\\x10\\n\\x05scale\\x18\\x02 \\x01(\\x02:\\x01\\x31\\x12\\x11\\n\\tmean_file\\x18\\x03 \\x01(\\t\\x12\\x14\\n\\tcrop_size\\x18\\x05 \\x01(\\r:\\x01\\x30\\x12\\x15\\n\\x06mirror\\x18\\x06 \\x01(\\x08:\\x05\\x66\\x61lse\\x12\\x15\\n\\x0broot_folder\\x18\\x0c \\x01(\\t:\\x00\\""\\\'\\n\\x15InfogainLossParameter\\x12\\x0e\\n\\x06source\\x18\\x01 \\x01(\\t\\""\\xb1\\x01\\n\\x15InnerProductParameter\\x12\\x12\\n\\nnum_output\\x18\\x01 \\x01(\\r\\x12\\x17\\n\\tbias_term\\x18\\x02 \\x01(\\x08:\\x04true\\x12-\\n\\rweight_filler\\x18\\x03 \\x01(\\x0b\\x32\\x16.caffe.FillerParameter\\x12+\\n\\x0b\\x62ias_filler\\x18\\x04 \\x01(\\x0b\\x32\\x16.caffe.FillerParameter\\x12\\x0f\\n\\x04\\x61xis\\x18\\x05 \\x01(\\x05:\\x01\\x31\\""D\\n\\x0cLogParameter\\x12\\x10\\n\\x04\\x62\\x61se\\x18\\x01 \\x01(\\x02:\\x02-1\\x12\\x10\\n\\x05scale\\x18\\x02 \\x01(\\x02:\\x01\\x31\\x12\\x10\\n\\x05shift\\x18\\x03 \\x01(\\x02:\\x01\\x30\\""\\xb8\\x02\\n\\x0cLRNParameter\\x12\\x15\\n\\nlocal_size\\x18\\x01 \\x01(\\r:\\x01\\x35\\x12\\x10\\n\\x05\\x61lpha\\x18\\x02 \\x01(\\x02:\\x01\\x31\\x12\\x12\\n\\x04\\x62\\x65ta\\x18\\x03 \\x01(\\x02:\\x04\\x30.75\\x12\\x44\\n\\x0bnorm_region\\x18\\x04 \\x01(\\x0e\\x32\\x1e.caffe.LRNParameter.NormRegion:\\x0f\\x41\\x43ROSS_CHANNELS\\x12\\x0c\\n\\x01k\\x18\\x05 \\x01(\\x02:\\x01\\x31\\x12\\x33\\n\\x06\\x65ngine\\x18\\x06 \\x01(\\x0e\\x32\\x1a.caffe.LRNParameter.Engine:\\x07\\x44\\x45\\x46\\x41ULT\\""5\\n\\nNormRegion\\x12\\x13\\n\\x0f\\x41\\x43ROSS_CHANNELS\\x10\\x00\\x12\\x12\\n\\x0eWITHIN_CHANNEL\\x10\\x01\\""+\\n\\x06\\x45ngine\\x12\\x0b\\n\\x07\\x44\\x45\\x46\\x41ULT\\x10\\x00\\x12\\t\\n\\x05\\x43\\x41\\x46\\x46\\x45\\x10\\x01\\x12\\t\\n\\x05\\x43UDNN\\x10\\x02\\""Z\\n\\x13MemoryDataParameter\\x12\\x12\\n\\nbatch_size\\x18\\x01 \\x01(\\r\\x12\\x10\\n\\x08\\x63hannels\\x18\\x02 \\x01(\\r\\x12\\x0e\\n\\x06height\\x18\\x03 \\x01(\\r\\x12\\r\\n\\x05width\\x18\\x04 \\x01(\\r\\""d\\n\\x0cMVNParameter\\x12 \\n\\x12normalize_variance\\x18\\x01 \\x01(\\x08:\\x04true\\x12\\x1e\\n\\x0f\\x61\\x63ross_channels\\x18\\x02 \\x01(\\x08:\\x05\\x66\\x61lse\\x12\\x12\\n\\x03\\x65ps\\x18\\x03 \\x01(\\x02:\\x05\\x31\\x65-09\\""\\xa2\\x03\\n\\x10PoolingParameter\\x12\\x35\\n\\x04pool\\x18\\x01 \\x01(\\x0e\\x32\\"".caffe.PoolingParameter.PoolMethod:\\x03MAX\\x12\\x0e\\n\\x03pad\\x18\\x04 \\x01(\\r:\\x01\\x30\\x12\\x10\\n\\x05pad_h\\x18\\t \\x01(\\r:\\x01\\x30\\x12\\x10\\n\\x05pad_w\\x18\\n \\x01(\\r:\\x01\\x30\\x12\\x13\\n\\x0bkernel_size\\x18\\x02 \\x01(\\r\\x12\\x10\\n\\x08kernel_h\\x18\\x05 \\x01(\\r\\x12\\x10\\n\\x08kernel_w\\x18\\x06 \\x01(\\r\\x12\\x11\\n\\x06stride\\x18\\x03 \\x01(\\r:\\x01\\x31\\x12\\x10\\n\\x08stride_h\\x18\\x07 \\x01(\\r\\x12\\x10\\n\\x08stride_w\\x18\\x08 \\x01(\\r\\x12\\x37\\n\\x06\\x65ngine\\x18\\x0b \\x01(\\x0e\\x32\\x1e.caffe.PoolingParameter.Engine:\\x07\\x44\\x45\\x46\\x41ULT\\x12\\x1d\\n\\x0eglobal_pooling\\x18\\x0c \\x01(\\x08:\\x05\\x66\\x61lse\\"".\\n\\nPoolMethod\\x12\\x07\\n\\x03MAX\\x10\\x00\\x12\\x07\\n\\x03\\x41VE\\x10\\x01\\x12\\x0e\\n\\nSTOCHASTIC\\x10\\x02\\""+\\n\\x06\\x45ngine\\x12\\x0b\\n\\x07\\x44\\x45\\x46\\x41ULT\\x10\\x00\\x12\\t\\n\\x05\\x43\\x41\\x46\\x46\\x45\\x10\\x01\\x12\\t\\n\\x05\\x43UDNN\\x10\\x02\\""F\\n\\x0ePowerParameter\\x12\\x10\\n\\x05power\\x18\\x01 \\x01(\\x02:\\x01\\x31\\x12\\x10\\n\\x05scale\\x18\\x02 \\x01(\\x02:\\x01\\x31\\x12\\x10\\n\\x05shift\\x18\\x03 \\x01(\\x02:\\x01\\x30\\""g\\n\\x0fPythonParameter\\x12\\x0e\\n\\x06module\\x18\\x01 \\x01(\\t\\x12\\r\\n\\x05layer\\x18\\x02 \\x01(\\t\\x12\\x13\\n\\tparam_str\\x18\\x03 \\x01(\\t:\\x00\\x12 \\n\\x11share_in_parallel\\x18\\x04 \\x01(\\x08:\\x05\\x66\\x61lse\\""\\xad\\x01\\n\\x12ReductionParameter\\x12=\\n\\toperation\\x18\\x01 \\x01(\\x0e\\x32%.caffe.ReductionParameter.ReductionOp:\\x03SUM\\x12\\x0f\\n\\x04\\x61xis\\x18\\x02 \\x01(\\x05:\\x01\\x30\\x12\\x10\\n\\x05\\x63oeff\\x18\\x03 \\x01(\\x02:\\x01\\x31\\""5\\n\\x0bReductionOp\\x12\\x07\\n\\x03SUM\\x10\\x01\\x12\\x08\\n\\x04\\x41SUM\\x10\\x02\\x12\\t\\n\\x05SUMSQ\\x10\\x03\\x12\\x08\\n\\x04MEAN\\x10\\x04\\""\\x8d\\x01\\n\\rReLUParameter\\x12\\x19\\n\\x0enegative_slope\\x18\\x01 \\x01(\\x02:\\x01\\x30\\x12\\x34\\n\\x06\\x65ngine\\x18\\x02 \\x01(\\x0e\\x32\\x1b.caffe.ReLUParameter.Engine:\\x07\\x44\\x45\\x46\\x41ULT\\""+\\n\\x06\\x45ngine\\x12\\x0b\\n\\x07\\x44\\x45\\x46\\x41ULT\\x10\\x00\\x12\\t\\n\\x05\\x43\\x41\\x46\\x46\\x45\\x10\\x01\\x12\\t\\n\\x05\\x43UDNN\\x10\\x02\\""Z\\n\\x10ReshapeParameter\\x12\\x1f\\n\\x05shape\\x18\\x01 \\x01(\\x0b\\x32\\x10.caffe.BlobShape\\x12\\x0f\\n\\x04\\x61xis\\x18\\x02 \\x01(\\x05:\\x01\\x30\\x12\\x14\\n\\x08num_axes\\x18\\x03 \\x01(\\x05:\\x02-1\\""\\xa5\\x01\\n\\x0eScaleParameter\\x12\\x0f\\n\\x04\\x61xis\\x18\\x01 \\x01(\\x05:\\x01\\x31\\x12\\x13\\n\\x08num_axes\\x18\\x02 \\x01(\\x05:\\x01\\x31\\x12&\\n\\x06\\x66iller\\x18\\x03 \\x01(\\x0b\\x32\\x16.caffe.FillerParameter\\x12\\x18\\n\\tbias_term\\x18\\x04 \\x01(\\x08:\\x05\\x66\\x61lse\\x12+\\n\\x0b\\x62ias_filler\\x18\\x05 \\x01(\\x0b\\x32\\x16.caffe.FillerParameter\\""x\\n\\x10SigmoidParameter\\x12\\x37\\n\\x06\\x65ngine\\x18\\x01 \\x01(\\x0e\\x32\\x1e.caffe.SigmoidParameter.Engine:\\x07\\x44\\x45\\x46\\x41ULT\\""+\\n\\x06\\x45ngine\\x12\\x0b\\n\\x07\\x44\\x45\\x46\\x41ULT\\x10\\x00\\x12\\t\\n\\x05\\x43\\x41\\x46\\x46\\x45\\x10\\x01\\x12\\t\\n\\x05\\x43UDNN\\x10\\x02\\""L\\n\\x0eSliceParameter\\x12\\x0f\\n\\x04\\x61xis\\x18\\x03 \\x01(\\x05:\\x01\\x31\\x12\\x13\\n\\x0bslice_point\\x18\\x02 \\x03(\\r\\x12\\x14\\n\\tslice_dim\\x18\\x01 \\x01(\\r:\\x01\\x31\\""\\x89\\x01\\n\\x10SoftmaxParameter\\x12\\x37\\n\\x06\\x65ngine\\x18\\x01 \\x01(\\x0e\\x32\\x1e.caffe.SoftmaxParameter.Engine:\\x07\\x44\\x45\\x46\\x41ULT\\x12\\x0f\\n\\x04\\x61xis\\x18\\x02 \\x01(\\x05:\\x01\\x31\\""+\\n\\x06\\x45ngine\\x12\\x0b\\n\\x07\\x44\\x45\\x46\\x41ULT\\x10\\x00\\x12\\t\\n\\x05\\x43\\x41\\x46\\x46\\x45\\x10\\x01\\x12\\t\\n\\x05\\x43UDNN\\x10\\x02\\""r\\n\\rTanHParameter\\x12\\x34\\n\\x06\\x65ngine\\x18\\x01 \\x01(\\x0e\\x32\\x1b.caffe.TanHParameter.Engine:\\x07\\x44\\x45\\x46\\x41ULT\\""+\\n\\x06\\x45ngine\\x12\\x0b\\n\\x07\\x44\\x45\\x46\\x41ULT\\x10\\x00\\x12\\t\\n\\x05\\x43\\x41\\x46\\x46\\x45\\x10\\x01\\x12\\t\\n\\x05\\x43UDNN\\x10\\x02\\""/\\n\\rTileParameter\\x12\\x0f\\n\\x04\\x61xis\\x18\\x01 \\x01(\\x05:\\x01\\x31\\x12\\r\\n\\x05tiles\\x18\\x02 \\x01(\\x05\\""*\\n\\x12ThresholdParameter\\x12\\x14\\n\\tthreshold\\x18\\x01 \\x01(\\x02:\\x01\\x30\\""\\xc1\\x02\\n\\x13WindowDataParameter\\x12\\x0e\\n\\x06source\\x18\\x01 \\x01(\\t\\x12\\x10\\n\\x05scale\\x18\\x02 \\x01(\\x02:\\x01\\x31\\x12\\x11\\n\\tmean_file\\x18\\x03 \\x01(\\t\\x12\\x12\\n\\nbatch_size\\x18\\x04 \\x01(\\r\\x12\\x14\\n\\tcrop_size\\x18\\x05 \\x01(\\r:\\x01\\x30\\x12\\x15\\n\\x06mirror\\x18\\x06 \\x01(\\x08:\\x05\\x66\\x61lse\\x12\\x19\\n\\x0c\\x66g_threshold\\x18\\x07 \\x01(\\x02:\\x03\\x30.5\\x12\\x19\\n\\x0c\\x62g_threshold\\x18\\x08 \\x01(\\x02:\\x03\\x30.5\\x12\\x19\\n\\x0b\\x66g_fraction\\x18\\t \\x01(\\x02:\\x04\\x30.25\\x12\\x16\\n\\x0b\\x63ontext_pad\\x18\\n \\x01(\\r:\\x01\\x30\\x12\\x17\\n\\tcrop_mode\\x18\\x0b \\x01(\\t:\\x04warp\\x12\\x1b\\n\\x0c\\x63\\x61\\x63he_images\\x18\\x0c \\x01(\\x08:\\x05\\x66\\x61lse\\x12\\x15\\n\\x0broot_folder\\x18\\r \\x01(\\t:\\x00\\""\\xeb\\x01\\n\\x0cSPPParameter\\x12\\x16\\n\\x0epyramid_height\\x18\\x01 \\x01(\\r\\x12\\x31\\n\\x04pool\\x18\\x02 \\x01(\\x0e\\x32\\x1e.caffe.SPPParameter.PoolMethod:\\x03MAX\\x12\\x33\\n\\x06\\x65ngine\\x18\\x06 \\x01(\\x0e\\x32\\x1a.caffe.SPPParameter.Engine:\\x07\\x44\\x45\\x46\\x41ULT\\"".\\n\\nPoolMethod\\x12\\x07\\n\\x03MAX\\x10\\x00\\x12\\x07\\n\\x03\\x41VE\\x10\\x01\\x12\\x0e\\n\\nSTOCHASTIC\\x10\\x02\\""+\\n\\x06\\x45ngine\\x12\\x0b\\n\\x07\\x44\\x45\\x46\\x41ULT\\x10\\x00\\x12\\t\\n\\x05\\x43\\x41\\x46\\x46\\x45\\x10\\x01\\x12\\t\\n\\x05\\x43UDNN\\x10\\x02\\""\\xe0\\x13\\n\\x10V1LayerParameter\\x12\\x0e\\n\\x06\\x62ottom\\x18\\x02 \\x03(\\t\\x12\\x0b\\n\\x03top\\x18\\x03 \\x03(\\t\\x12\\x0c\\n\\x04name\\x18\\x04 \\x01(\\t\\x12$\\n\\x07include\\x18  \\x03(\\x0b\\x32\\x13.caffe.NetStateRule\\x12$\\n\\x07\\x65xclude\\x18! \\x03(\\x0b\\x32\\x13.caffe.NetStateRule\\x12/\\n\\x04type\\x18\\x05 \\x01(\\x0e\\x32!.caffe.V1LayerParameter.LayerType\\x12\\x1f\\n\\x05\\x62lobs\\x18\\x06 \\x03(\\x0b\\x32\\x10.caffe.BlobProto\\x12\\x0e\\n\\x05param\\x18\\xe9\\x07 \\x03(\\t\\x12>\\n\\x0f\\x62lob_share_mode\\x18\\xea\\x07 \\x03(\\x0e\\x32$.caffe.V1LayerParameter.DimCheckMode\\x12\\x10\\n\\x08\\x62lobs_lr\\x18\\x07 \\x03(\\x02\\x12\\x14\\n\\x0cweight_decay\\x18\\x08 \\x03(\\x02\\x12\\x13\\n\\x0bloss_weight\\x18# \\x03(\\x02\\x12\\x30\\n\\x0e\\x61\\x63\\x63uracy_param\\x18\\x1b \\x01(\\x0b\\x32\\x18.caffe.AccuracyParameter\\x12,\\n\\x0c\\x61rgmax_param\\x18\\x17 \\x01(\\x0b\\x32\\x16.caffe.ArgMaxParameter\\x12,\\n\\x0c\\x63oncat_param\\x18\\t \\x01(\\x0b\\x32\\x16.caffe.ConcatParameter\\x12?\\n\\x16\\x63ontrastive_loss_param\\x18( \\x01(\\x0b\\x32\\x1f.caffe.ContrastiveLossParameter\\x12\\x36\\n\\x11\\x63onvolution_param\\x18\\n \\x01(\\x0b\\x32\\x1b.caffe.ConvolutionParameter\\x12(\\n\\ndata_param\\x18\\x0b \\x01(\\x0b\\x32\\x14.caffe.DataParameter\\x12.\\n\\rdropout_param\\x18\\x0c \\x01(\\x0b\\x32\\x17.caffe.DropoutParameter\\x12\\x33\\n\\x10\\x64ummy_data_param\\x18\\x1a \\x01(\\x0b\\x32\\x19.caffe.DummyDataParameter\\x12.\\n\\reltwise_param\\x18\\x18 \\x01(\\x0b\\x32\\x17.caffe.EltwiseParameter\\x12&\\n\\texp_param\\x18) \\x01(\\x0b\\x32\\x13.caffe.ExpParameter\\x12\\x31\\n\\x0fhdf5_data_param\\x18\\r \\x01(\\x0b\\x32\\x18.caffe.HDF5DataParameter\\x12\\x35\\n\\x11hdf5_output_param\\x18\\x0e \\x01(\\x0b\\x32\\x1a.caffe.HDF5OutputParameter\\x12\\x33\\n\\x10hinge_loss_param\\x18\\x1d \\x01(\\x0b\\x32\\x19.caffe.HingeLossParameter\\x12\\x33\\n\\x10image_data_param\\x18\\x0f \\x01(\\x0b\\x32\\x19.caffe.ImageDataParameter\\x12\\x39\\n\\x13infogain_loss_param\\x18\\x10 \\x01(\\x0b\\x32\\x1c.caffe.InfogainLossParameter\\x12\\x39\\n\\x13inner_product_param\\x18\\x11 \\x01(\\x0b\\x32\\x1c.caffe.InnerProductParameter\\x12&\\n\\tlrn_param\\x18\\x12 \\x01(\\x0b\\x32\\x13.caffe.LRNParameter\\x12\\x35\\n\\x11memory_data_param\\x18\\x16 \\x01(\\x0b\\x32\\x1a.caffe.MemoryDataParameter\\x12&\\n\\tmvn_param\\x18\\"" \\x01(\\x0b\\x32\\x13.caffe.MVNParameter\\x12.\\n\\rpooling_param\\x18\\x13 \\x01(\\x0b\\x32\\x17.caffe.PoolingParameter\\x12*\\n\\x0bpower_param\\x18\\x15 \\x01(\\x0b\\x32\\x15.caffe.PowerParameter\\x12(\\n\\nrelu_param\\x18\\x1e \\x01(\\x0b\\x32\\x14.caffe.ReLUParameter\\x12.\\n\\rsigmoid_param\\x18& \\x01(\\x0b\\x32\\x17.caffe.SigmoidParameter\\x12.\\n\\rsoftmax_param\\x18\\\' \\x01(\\x0b\\x32\\x17.caffe.SoftmaxParameter\\x12*\\n\\x0bslice_param\\x18\\x1f \\x01(\\x0b\\x32\\x15.caffe.SliceParameter\\x12(\\n\\ntanh_param\\x18% \\x01(\\x0b\\x32\\x14.caffe.TanHParameter\\x12\\x32\\n\\x0fthreshold_param\\x18\\x19 \\x01(\\x0b\\x32\\x19.caffe.ThresholdParameter\\x12\\x35\\n\\x11window_data_param\\x18\\x14 \\x01(\\x0b\\x32\\x1a.caffe.WindowDataParameter\\x12\\x37\\n\\x0ftransform_param\\x18$ \\x01(\\x0b\\x32\\x1e.caffe.TransformationParameter\\x12(\\n\\nloss_param\\x18* \\x01(\\x0b\\x32\\x14.caffe.LossParameter\\x12&\\n\\x05layer\\x18\\x01 \\x01(\\x0b\\x32\\x17.caffe.V0LayerParameter\\""\\xd8\\x04\\n\\tLayerType\\x12\\x08\\n\\x04NONE\\x10\\x00\\x12\\n\\n\\x06\\x41\\x42SVAL\\x10#\\x12\\x0c\\n\\x08\\x41\\x43\\x43URACY\\x10\\x01\\x12\\n\\n\\x06\\x41RGMAX\\x10\\x1e\\x12\\x08\\n\\x04\\x42NLL\\x10\\x02\\x12\\n\\n\\x06\\x43ONCAT\\x10\\x03\\x12\\x14\\n\\x10\\x43ONTRASTIVE_LOSS\\x10%\\x12\\x0f\\n\\x0b\\x43ONVOLUTION\\x10\\x04\\x12\\x08\\n\\x04\\x44\\x41TA\\x10\\x05\\x12\\x11\\n\\rDECONVOLUTION\\x10\\\'\\x12\\x0b\\n\\x07\\x44ROPOUT\\x10\\x06\\x12\\x0e\\n\\nDUMMY_DATA\\x10 \\x12\\x12\\n\\x0e\\x45UCLIDEAN_LOSS\\x10\\x07\\x12\\x0b\\n\\x07\\x45LTWISE\\x10\\x19\\x12\\x07\\n\\x03\\x45XP\\x10&\\x12\\x0b\\n\\x07\\x46LATTEN\\x10\\x08\\x12\\r\\n\\tHDF5_DATA\\x10\\t\\x12\\x0f\\n\\x0bHDF5_OUTPUT\\x10\\n\\x12\\x0e\\n\\nHINGE_LOSS\\x10\\x1c\\x12\\n\\n\\x06IM2COL\\x10\\x0b\\x12\\x0e\\n\\nIMAGE_DATA\\x10\\x0c\\x12\\x11\\n\\rINFOGAIN_LOSS\\x10\\r\\x12\\x11\\n\\rINNER_PRODUCT\\x10\\x0e\\x12\\x07\\n\\x03LRN\\x10\\x0f\\x12\\x0f\\n\\x0bMEMORY_DATA\\x10\\x1d\\x12\\x1d\\n\\x19MULTINOMIAL_LOGISTIC_LOSS\\x10\\x10\\x12\\x07\\n\\x03MVN\\x10\\""\\x12\\x0b\\n\\x07POOLING\\x10\\x11\\x12\\t\\n\\x05POWER\\x10\\x1a\\x12\\x08\\n\\x04RELU\\x10\\x12\\x12\\x0b\\n\\x07SIGMOID\\x10\\x13\\x12\\x1e\\n\\x1aSIGMOID_CROSS_ENTROPY_LOSS\\x10\\x1b\\x12\\x0b\\n\\x07SILENCE\\x10$\\x12\\x0b\\n\\x07SOFTMAX\\x10\\x14\\x12\\x10\\n\\x0cSOFTMAX_LOSS\\x10\\x15\\x12\\t\\n\\x05SPLIT\\x10\\x16\\x12\\t\\n\\x05SLICE\\x10!\\x12\\x08\\n\\x04TANH\\x10\\x17\\x12\\x0f\\n\\x0bWINDOW_DATA\\x10\\x18\\x12\\r\\n\\tTHRESHOLD\\x10\\x1f\\""*\\n\\x0c\\x44imCheckMode\\x12\\n\\n\\x06STRICT\\x10\\x00\\x12\\x0e\\n\\nPERMISSIVE\\x10\\x01\\""\\xfd\\x07\\n\\x10V0LayerParameter\\x12\\x0c\\n\\x04name\\x18\\x01 \\x01(\\t\\x12\\x0c\\n\\x04type\\x18\\x02 \\x01(\\t\\x12\\x12\\n\\nnum_output\\x18\\x03 \\x01(\\r\\x12\\x16\\n\\x08\\x62iasterm\\x18\\x04 \\x01(\\x08:\\x04true\\x12-\\n\\rweight_filler\\x18\\x05 \\x01(\\x0b\\x32\\x16.caffe.FillerParameter\\x12+\\n\\x0b\\x62ias_filler\\x18\\x06 \\x01(\\x0b\\x32\\x16.caffe.FillerParameter\\x12\\x0e\\n\\x03pad\\x18\\x07 \\x01(\\r:\\x01\\x30\\x12\\x12\\n\\nkernelsize\\x18\\x08 \\x01(\\r\\x12\\x10\\n\\x05group\\x18\\t \\x01(\\r:\\x01\\x31\\x12\\x11\\n\\x06stride\\x18\\n \\x01(\\r:\\x01\\x31\\x12\\x35\\n\\x04pool\\x18\\x0b \\x01(\\x0e\\x32\\"".caffe.V0LayerParameter.PoolMethod:\\x03MAX\\x12\\x1a\\n\\rdropout_ratio\\x18\\x0c \\x01(\\x02:\\x03\\x30.5\\x12\\x15\\n\\nlocal_size\\x18\\r \\x01(\\r:\\x01\\x35\\x12\\x10\\n\\x05\\x61lpha\\x18\\x0e \\x01(\\x02:\\x01\\x31\\x12\\x12\\n\\x04\\x62\\x65ta\\x18\\x0f \\x01(\\x02:\\x04\\x30.75\\x12\\x0c\\n\\x01k\\x18\\x16 \\x01(\\x02:\\x01\\x31\\x12\\x0e\\n\\x06source\\x18\\x10 \\x01(\\t\\x12\\x10\\n\\x05scale\\x18\\x11 \\x01(\\x02:\\x01\\x31\\x12\\x10\\n\\x08meanfile\\x18\\x12 \\x01(\\t\\x12\\x11\\n\\tbatchsize\\x18\\x13 \\x01(\\r\\x12\\x13\\n\\x08\\x63ropsize\\x18\\x14 \\x01(\\r:\\x01\\x30\\x12\\x15\\n\\x06mirror\\x18\\x15 \\x01(\\x08:\\x05\\x66\\x61lse\\x12\\x1f\\n\\x05\\x62lobs\\x18\\x32 \\x03(\\x0b\\x32\\x10.caffe.BlobProto\\x12\\x10\\n\\x08\\x62lobs_lr\\x18\\x33 \\x03(\\x02\\x12\\x14\\n\\x0cweight_decay\\x18\\x34 \\x03(\\x02\\x12\\x14\\n\\trand_skip\\x18\\x35 \\x01(\\r:\\x01\\x30\\x12\\x1d\\n\\x10\\x64\\x65t_fg_threshold\\x18\\x36 \\x01(\\x02:\\x03\\x30.5\\x12\\x1d\\n\\x10\\x64\\x65t_bg_threshold\\x18\\x37 \\x01(\\x02:\\x03\\x30.5\\x12\\x1d\\n\\x0f\\x64\\x65t_fg_fraction\\x18\\x38 \\x01(\\x02:\\x04\\x30.25\\x12\\x1a\\n\\x0f\\x64\\x65t_context_pad\\x18: \\x01(\\r:\\x01\\x30\\x12\\x1b\\n\\rdet_crop_mode\\x18; \\x01(\\t:\\x04warp\\x12\\x12\\n\\x07new_num\\x18< \\x01(\\x05:\\x01\\x30\\x12\\x17\\n\\x0cnew_channels\\x18= \\x01(\\x05:\\x01\\x30\\x12\\x15\\n\\nnew_height\\x18> \\x01(\\x05:\\x01\\x30\\x12\\x14\\n\\tnew_width\\x18? \\x01(\\x05:\\x01\\x30\\x12\\x1d\\n\\x0eshuffle_images\\x18@ \\x01(\\x08:\\x05\\x66\\x61lse\\x12\\x15\\n\\nconcat_dim\\x18\\x41 \\x01(\\r:\\x01\\x31\\x12\\x36\\n\\x11hdf5_output_param\\x18\\xe9\\x07 \\x01(\\x0b\\x32\\x1a.caffe.HDF5OutputParameter\\"".\\n\\nPoolMethod\\x12\\x07\\n\\x03MAX\\x10\\x00\\x12\\x07\\n\\x03\\x41VE\\x10\\x01\\x12\\x0e\\n\\nSTOCHASTIC\\x10\\x02\\""W\\n\\x0ePReLUParameter\\x12&\\n\\x06\\x66iller\\x18\\x01 \\x01(\\x0b\\x32\\x16.caffe.FillerParameter\\x12\\x1d\\n\\x0e\\x63hannel_shared\\x18\\x02 \\x01(\\x08:\\x05\\x66\\x61lse*\\x1c\\n\\x05Phase\\x12\\t\\n\\x05TRAIN\\x10\\x00\\x12\\x08\\n\\x04TEST\\x10\\x01\')\n)\n_sym_db.RegisterFileDescriptor(DESCRIPTOR)\n\n_PHASE = _descriptor.EnumDescriptor(\n  name=\'Phase\',\n  full_name=\'caffe.Phase\',\n  filename=None,\n  file=DESCRIPTOR,\n  values=[\n    _descriptor.EnumValueDescriptor(\n      name=\'TRAIN\', index=0, number=0,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'TEST\', index=1, number=1,\n      options=None,\n      type=None),\n  ],\n  containing_type=None,\n  options=None,\n  serialized_start=14776,\n  serialized_end=14804,\n)\n_sym_db.RegisterEnumDescriptor(_PHASE)\n\nPhase = enum_type_wrapper.EnumTypeWrapper(_PHASE)\nTRAIN = 0\nTEST = 1\n\n\n_FILLERPARAMETER_VARIANCENORM = _descriptor.EnumDescriptor(\n  name=\'VarianceNorm\',\n  full_name=\'caffe.FillerParameter.VarianceNorm\',\n  filename=None,\n  file=DESCRIPTOR,\n  values=[\n    _descriptor.EnumValueDescriptor(\n      name=\'FAN_IN\', index=0, number=0,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'FAN_OUT\', index=1, number=1,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'AVERAGE\', index=2, number=2,\n      options=None,\n      type=None),\n  ],\n  containing_type=None,\n  options=None,\n  serialized_start=658,\n  serialized_end=710,\n)\n_sym_db.RegisterEnumDescriptor(_FILLERPARAMETER_VARIANCENORM)\n\n_SOLVERPARAMETER_SNAPSHOTFORMAT = _descriptor.EnumDescriptor(\n  name=\'SnapshotFormat\',\n  full_name=\'caffe.SolverParameter.SnapshotFormat\',\n  filename=None,\n  file=DESCRIPTOR,\n  values=[\n    _descriptor.EnumValueDescriptor(\n      name=\'HDF5\', index=0, number=0,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'BINARYPROTO\', index=1, number=1,\n      options=None,\n      type=None),\n  ],\n  containing_type=None,\n  options=None,\n  serialized_start=2132,\n  serialized_end=2175,\n)\n_sym_db.RegisterEnumDescriptor(_SOLVERPARAMETER_SNAPSHOTFORMAT)\n\n_SOLVERPARAMETER_SOLVERMODE = _descriptor.EnumDescriptor(\n  name=\'SolverMode\',\n  full_name=\'caffe.SolverParameter.SolverMode\',\n  filename=None,\n  file=DESCRIPTOR,\n  values=[\n    _descriptor.EnumValueDescriptor(\n      name=\'CPU\', index=0, number=0,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'GPU\', index=1, number=1,\n      options=None,\n      type=None),\n  ],\n  containing_type=None,\n  options=None,\n  serialized_start=2177,\n  serialized_end=2207,\n)\n_sym_db.RegisterEnumDescriptor(_SOLVERPARAMETER_SOLVERMODE)\n\n_SOLVERPARAMETER_SOLVERTYPE = _descriptor.EnumDescriptor(\n  name=\'SolverType\',\n  full_name=\'caffe.SolverParameter.SolverType\',\n  filename=None,\n  file=DESCRIPTOR,\n  values=[\n    _descriptor.EnumValueDescriptor(\n      name=\'SGD\', index=0, number=0,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'NESTEROV\', index=1, number=1,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'ADAGRAD\', index=2, number=2,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'RMSPROP\', index=3, number=3,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'ADADELTA\', index=4, number=4,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'ADAM\', index=5, number=5,\n      options=None,\n      type=None),\n  ],\n  containing_type=None,\n  options=None,\n  serialized_start=2209,\n  serialized_end=2294,\n)\n_sym_db.RegisterEnumDescriptor(_SOLVERPARAMETER_SOLVERTYPE)\n\n_PARAMSPEC_DIMCHECKMODE = _descriptor.EnumDescriptor(\n  name=\'DimCheckMode\',\n  full_name=\'caffe.ParamSpec.DimCheckMode\',\n  filename=None,\n  file=DESCRIPTOR,\n  values=[\n    _descriptor.EnumValueDescriptor(\n      name=\'STRICT\', index=0, number=0,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'PERMISSIVE\', index=1, number=1,\n      options=None,\n      type=None),\n  ],\n  containing_type=None,\n  options=None,\n  serialized_start=2725,\n  serialized_end=2767,\n)\n_sym_db.RegisterEnumDescriptor(_PARAMSPEC_DIMCHECKMODE)\n\n_LOSSPARAMETER_NORMALIZATIONMODE = _descriptor.EnumDescriptor(\n  name=\'NormalizationMode\',\n  full_name=\'caffe.LossParameter.NormalizationMode\',\n  filename=None,\n  file=DESCRIPTOR,\n  values=[\n    _descriptor.EnumValueDescriptor(\n      name=\'FULL\', index=0, number=0,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'VALID\', index=1, number=1,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'BATCH_SIZE\', index=2, number=2,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'NONE\', index=3, number=3,\n      options=None,\n      type=None),\n  ],\n  containing_type=None,\n  options=None,\n  serialized_start=5454,\n  serialized_end=5520,\n)\n_sym_db.RegisterEnumDescriptor(_LOSSPARAMETER_NORMALIZATIONMODE)\n\n_CONVOLUTIONPARAMETER_ENGINE = _descriptor.EnumDescriptor(\n  name=\'Engine\',\n  full_name=\'caffe.ConvolutionParameter.Engine\',\n  filename=None,\n  file=DESCRIPTOR,\n  values=[\n    _descriptor.EnumValueDescriptor(\n      name=\'DEFAULT\', index=0, number=0,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'CAFFE\', index=1, number=1,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'CUDNN\', index=2, number=2,\n      options=None,\n      type=None),\n  ],\n  containing_type=None,\n  options=None,\n  serialized_start=6485,\n  serialized_end=6528,\n)\n_sym_db.RegisterEnumDescriptor(_CONVOLUTIONPARAMETER_ENGINE)\n\n_DATAPARAMETER_DB = _descriptor.EnumDescriptor(\n  name=\'DB\',\n  full_name=\'caffe.DataParameter.DB\',\n  filename=None,\n  file=DESCRIPTOR,\n  values=[\n    _descriptor.EnumValueDescriptor(\n      name=\'LEVELDB\', index=0, number=0,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'LMDB\', index=1, number=1,\n      options=None,\n      type=None),\n  ],\n  containing_type=None,\n  options=None,\n  serialized_start=6796,\n  serialized_end=6823,\n)\n_sym_db.RegisterEnumDescriptor(_DATAPARAMETER_DB)\n\n_ELTWISEPARAMETER_ELTWISEOP = _descriptor.EnumDescriptor(\n  name=\'EltwiseOp\',\n  full_name=\'caffe.EltwiseParameter.EltwiseOp\',\n  filename=None,\n  file=DESCRIPTOR,\n  values=[\n    _descriptor.EnumValueDescriptor(\n      name=\'PROD\', index=0, number=0,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'SUM\', index=1, number=1,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'MAX\', index=2, number=2,\n      options=None,\n      type=None),\n  ],\n  containing_type=None,\n  options=None,\n  serialized_start=7163,\n  serialized_end=7202,\n)\n_sym_db.RegisterEnumDescriptor(_ELTWISEPARAMETER_ELTWISEOP)\n\n_HINGELOSSPARAMETER_NORM = _descriptor.EnumDescriptor(\n  name=\'Norm\',\n  full_name=\'caffe.HingeLossParameter.Norm\',\n  filename=None,\n  file=DESCRIPTOR,\n  values=[\n    _descriptor.EnumValueDescriptor(\n      name=\'L1\', index=0, number=1,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'L2\', index=1, number=2,\n      options=None,\n      type=None),\n  ],\n  containing_type=None,\n  options=None,\n  serialized_start=7737,\n  serialized_end=7759,\n)\n_sym_db.RegisterEnumDescriptor(_HINGELOSSPARAMETER_NORM)\n\n_LRNPARAMETER_NORMREGION = _descriptor.EnumDescriptor(\n  name=\'NormRegion\',\n  full_name=\'caffe.LRNParameter.NormRegion\',\n  filename=None,\n  file=DESCRIPTOR,\n  values=[\n    _descriptor.EnumValueDescriptor(\n      name=\'ACROSS_CHANNELS\', index=0, number=0,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'WITHIN_CHANNEL\', index=1, number=1,\n      options=None,\n      type=None),\n  ],\n  containing_type=None,\n  options=None,\n  serialized_start=8549,\n  serialized_end=8602,\n)\n_sym_db.RegisterEnumDescriptor(_LRNPARAMETER_NORMREGION)\n\n_LRNPARAMETER_ENGINE = _descriptor.EnumDescriptor(\n  name=\'Engine\',\n  full_name=\'caffe.LRNParameter.Engine\',\n  filename=None,\n  file=DESCRIPTOR,\n  values=[\n    _descriptor.EnumValueDescriptor(\n      name=\'DEFAULT\', index=0, number=0,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'CAFFE\', index=1, number=1,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'CUDNN\', index=2, number=2,\n      options=None,\n      type=None),\n  ],\n  containing_type=None,\n  options=None,\n  serialized_start=6485,\n  serialized_end=6528,\n)\n_sym_db.RegisterEnumDescriptor(_LRNPARAMETER_ENGINE)\n\n_POOLINGPARAMETER_POOLMETHOD = _descriptor.EnumDescriptor(\n  name=\'PoolMethod\',\n  full_name=\'caffe.PoolingParameter.PoolMethod\',\n  filename=None,\n  file=DESCRIPTOR,\n  values=[\n    _descriptor.EnumValueDescriptor(\n      name=\'MAX\', index=0, number=0,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'AVE\', index=1, number=1,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'STOCHASTIC\', index=2, number=2,\n      options=None,\n      type=None),\n  ],\n  containing_type=None,\n  options=None,\n  serialized_start=9171,\n  serialized_end=9217,\n)\n_sym_db.RegisterEnumDescriptor(_POOLINGPARAMETER_POOLMETHOD)\n\n_POOLINGPARAMETER_ENGINE = _descriptor.EnumDescriptor(\n  name=\'Engine\',\n  full_name=\'caffe.PoolingParameter.Engine\',\n  filename=None,\n  file=DESCRIPTOR,\n  values=[\n    _descriptor.EnumValueDescriptor(\n      name=\'DEFAULT\', index=0, number=0,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'CAFFE\', index=1, number=1,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'CUDNN\', index=2, number=2,\n      options=None,\n      type=None),\n  ],\n  containing_type=None,\n  options=None,\n  serialized_start=6485,\n  serialized_end=6528,\n)\n_sym_db.RegisterEnumDescriptor(_POOLINGPARAMETER_ENGINE)\n\n_REDUCTIONPARAMETER_REDUCTIONOP = _descriptor.EnumDescriptor(\n  name=\'ReductionOp\',\n  full_name=\'caffe.ReductionParameter.ReductionOp\',\n  filename=None,\n  file=DESCRIPTOR,\n  values=[\n    _descriptor.EnumValueDescriptor(\n      name=\'SUM\', index=0, number=1,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'ASUM\', index=1, number=2,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'SUMSQ\', index=2, number=3,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'MEAN\', index=3, number=4,\n      options=None,\n      type=None),\n  ],\n  containing_type=None,\n  options=None,\n  serialized_start=9562,\n  serialized_end=9615,\n)\n_sym_db.RegisterEnumDescriptor(_REDUCTIONPARAMETER_REDUCTIONOP)\n\n_RELUPARAMETER_ENGINE = _descriptor.EnumDescriptor(\n  name=\'Engine\',\n  full_name=\'caffe.ReLUParameter.Engine\',\n  filename=None,\n  file=DESCRIPTOR,\n  values=[\n    _descriptor.EnumValueDescriptor(\n      name=\'DEFAULT\', index=0, number=0,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'CAFFE\', index=1, number=1,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'CUDNN\', index=2, number=2,\n      options=None,\n      type=None),\n  ],\n  containing_type=None,\n  options=None,\n  serialized_start=6485,\n  serialized_end=6528,\n)\n_sym_db.RegisterEnumDescriptor(_RELUPARAMETER_ENGINE)\n\n_SIGMOIDPARAMETER_ENGINE = _descriptor.EnumDescriptor(\n  name=\'Engine\',\n  full_name=\'caffe.SigmoidParameter.Engine\',\n  filename=None,\n  file=DESCRIPTOR,\n  values=[\n    _descriptor.EnumValueDescriptor(\n      name=\'DEFAULT\', index=0, number=0,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'CAFFE\', index=1, number=1,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'CUDNN\', index=2, number=2,\n      options=None,\n      type=None),\n  ],\n  containing_type=None,\n  options=None,\n  serialized_start=6485,\n  serialized_end=6528,\n)\n_sym_db.RegisterEnumDescriptor(_SIGMOIDPARAMETER_ENGINE)\n\n_SOFTMAXPARAMETER_ENGINE = _descriptor.EnumDescriptor(\n  name=\'Engine\',\n  full_name=\'caffe.SoftmaxParameter.Engine\',\n  filename=None,\n  file=DESCRIPTOR,\n  values=[\n    _descriptor.EnumValueDescriptor(\n      name=\'DEFAULT\', index=0, number=0,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'CAFFE\', index=1, number=1,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'CUDNN\', index=2, number=2,\n      options=None,\n      type=None),\n  ],\n  containing_type=None,\n  options=None,\n  serialized_start=6485,\n  serialized_end=6528,\n)\n_sym_db.RegisterEnumDescriptor(_SOFTMAXPARAMETER_ENGINE)\n\n_TANHPARAMETER_ENGINE = _descriptor.EnumDescriptor(\n  name=\'Engine\',\n  full_name=\'caffe.TanHParameter.Engine\',\n  filename=None,\n  file=DESCRIPTOR,\n  values=[\n    _descriptor.EnumValueDescriptor(\n      name=\'DEFAULT\', index=0, number=0,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'CAFFE\', index=1, number=1,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'CUDNN\', index=2, number=2,\n      options=None,\n      type=None),\n  ],\n  containing_type=None,\n  options=None,\n  serialized_start=6485,\n  serialized_end=6528,\n)\n_sym_db.RegisterEnumDescriptor(_TANHPARAMETER_ENGINE)\n\n_SPPPARAMETER_POOLMETHOD = _descriptor.EnumDescriptor(\n  name=\'PoolMethod\',\n  full_name=\'caffe.SPPParameter.PoolMethod\',\n  filename=None,\n  file=DESCRIPTOR,\n  values=[\n    _descriptor.EnumValueDescriptor(\n      name=\'MAX\', index=0, number=0,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'AVE\', index=1, number=1,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'STOCHASTIC\', index=2, number=2,\n      options=None,\n      type=None),\n  ],\n  containing_type=None,\n  options=None,\n  serialized_start=9171,\n  serialized_end=9217,\n)\n_sym_db.RegisterEnumDescriptor(_SPPPARAMETER_POOLMETHOD)\n\n_SPPPARAMETER_ENGINE = _descriptor.EnumDescriptor(\n  name=\'Engine\',\n  full_name=\'caffe.SPPParameter.Engine\',\n  filename=None,\n  file=DESCRIPTOR,\n  values=[\n    _descriptor.EnumValueDescriptor(\n      name=\'DEFAULT\', index=0, number=0,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'CAFFE\', index=1, number=1,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'CUDNN\', index=2, number=2,\n      options=None,\n      type=None),\n  ],\n  containing_type=None,\n  options=None,\n  serialized_start=6485,\n  serialized_end=6528,\n)\n_sym_db.RegisterEnumDescriptor(_SPPPARAMETER_ENGINE)\n\n_V1LAYERPARAMETER_LAYERTYPE = _descriptor.EnumDescriptor(\n  name=\'LayerType\',\n  full_name=\'caffe.V1LayerParameter.LayerType\',\n  filename=None,\n  file=DESCRIPTOR,\n  values=[\n    _descriptor.EnumValueDescriptor(\n      name=\'NONE\', index=0, number=0,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'ABSVAL\', index=1, number=35,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'ACCURACY\', index=2, number=1,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'ARGMAX\', index=3, number=30,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'BNLL\', index=4, number=2,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'CONCAT\', index=5, number=3,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'CONTRASTIVE_LOSS\', index=6, number=37,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'CONVOLUTION\', index=7, number=4,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'DATA\', index=8, number=5,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'DECONVOLUTION\', index=9, number=39,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'DROPOUT\', index=10, number=6,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'DUMMY_DATA\', index=11, number=32,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'EUCLIDEAN_LOSS\', index=12, number=7,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'ELTWISE\', index=13, number=25,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'EXP\', index=14, number=38,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'FLATTEN\', index=15, number=8,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'HDF5_DATA\', index=16, number=9,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'HDF5_OUTPUT\', index=17, number=10,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'HINGE_LOSS\', index=18, number=28,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'IM2COL\', index=19, number=11,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'IMAGE_DATA\', index=20, number=12,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'INFOGAIN_LOSS\', index=21, number=13,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'INNER_PRODUCT\', index=22, number=14,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'LRN\', index=23, number=15,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'MEMORY_DATA\', index=24, number=29,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'MULTINOMIAL_LOGISTIC_LOSS\', index=25, number=16,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'MVN\', index=26, number=34,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'POOLING\', index=27, number=17,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'POWER\', index=28, number=26,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'RELU\', index=29, number=18,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'SIGMOID\', index=30, number=19,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'SIGMOID_CROSS_ENTROPY_LOSS\', index=31, number=27,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'SILENCE\', index=32, number=36,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'SOFTMAX\', index=33, number=20,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'SOFTMAX_LOSS\', index=34, number=21,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'SPLIT\', index=35, number=22,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'SLICE\', index=36, number=33,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'TANH\', index=37, number=23,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'WINDOW_DATA\', index=38, number=24,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'THRESHOLD\', index=39, number=31,\n      options=None,\n      type=None),\n  ],\n  containing_type=None,\n  options=None,\n  serialized_start=13017,\n  serialized_end=13617,\n)\n_sym_db.RegisterEnumDescriptor(_V1LAYERPARAMETER_LAYERTYPE)\n\n_V1LAYERPARAMETER_DIMCHECKMODE = _descriptor.EnumDescriptor(\n  name=\'DimCheckMode\',\n  full_name=\'caffe.V1LayerParameter.DimCheckMode\',\n  filename=None,\n  file=DESCRIPTOR,\n  values=[\n    _descriptor.EnumValueDescriptor(\n      name=\'STRICT\', index=0, number=0,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'PERMISSIVE\', index=1, number=1,\n      options=None,\n      type=None),\n  ],\n  containing_type=None,\n  options=None,\n  serialized_start=2725,\n  serialized_end=2767,\n)\n_sym_db.RegisterEnumDescriptor(_V1LAYERPARAMETER_DIMCHECKMODE)\n\n_V0LAYERPARAMETER_POOLMETHOD = _descriptor.EnumDescriptor(\n  name=\'PoolMethod\',\n  full_name=\'caffe.V0LayerParameter.PoolMethod\',\n  filename=None,\n  file=DESCRIPTOR,\n  values=[\n    _descriptor.EnumValueDescriptor(\n      name=\'MAX\', index=0, number=0,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'AVE\', index=1, number=1,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'STOCHASTIC\', index=2, number=2,\n      options=None,\n      type=None),\n  ],\n  containing_type=None,\n  options=None,\n  serialized_start=9171,\n  serialized_end=9217,\n)\n_sym_db.RegisterEnumDescriptor(_V0LAYERPARAMETER_POOLMETHOD)\n\n\n_BLOBSHAPE = _descriptor.Descriptor(\n  name=\'BlobShape\',\n  full_name=\'caffe.BlobShape\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'dim\', full_name=\'caffe.BlobShape.dim\', index=0,\n      number=1, type=3, cpp_type=2, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=_descriptor._ParseOptions(descriptor_pb2.FieldOptions(), _b(\'\\020\\001\'))),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=22,\n  serialized_end=50,\n)\n\n\n_BLOBPROTO = _descriptor.Descriptor(\n  name=\'BlobProto\',\n  full_name=\'caffe.BlobProto\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'shape\', full_name=\'caffe.BlobProto.shape\', index=0,\n      number=7, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'data\', full_name=\'caffe.BlobProto.data\', index=1,\n      number=5, type=2, cpp_type=6, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=_descriptor._ParseOptions(descriptor_pb2.FieldOptions(), _b(\'\\020\\001\'))),\n    _descriptor.FieldDescriptor(\n      name=\'diff\', full_name=\'caffe.BlobProto.diff\', index=2,\n      number=6, type=2, cpp_type=6, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=_descriptor._ParseOptions(descriptor_pb2.FieldOptions(), _b(\'\\020\\001\'))),\n    _descriptor.FieldDescriptor(\n      name=\'double_data\', full_name=\'caffe.BlobProto.double_data\', index=3,\n      number=8, type=1, cpp_type=5, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=_descriptor._ParseOptions(descriptor_pb2.FieldOptions(), _b(\'\\020\\001\'))),\n    _descriptor.FieldDescriptor(\n      name=\'double_diff\', full_name=\'caffe.BlobProto.double_diff\', index=4,\n      number=9, type=1, cpp_type=5, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=_descriptor._ParseOptions(descriptor_pb2.FieldOptions(), _b(\'\\020\\001\'))),\n    _descriptor.FieldDescriptor(\n      name=\'num\', full_name=\'caffe.BlobProto.num\', index=5,\n      number=1, type=5, cpp_type=1, label=1,\n      has_default_value=True, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'channels\', full_name=\'caffe.BlobProto.channels\', index=6,\n      number=2, type=5, cpp_type=1, label=1,\n      has_default_value=True, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'height\', full_name=\'caffe.BlobProto.height\', index=7,\n      number=3, type=5, cpp_type=1, label=1,\n      has_default_value=True, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'width\', full_name=\'caffe.BlobProto.width\', index=8,\n      number=4, type=5, cpp_type=1, label=1,\n      has_default_value=True, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=53,\n  serialized_end=257,\n)\n\n\n_BLOBPROTOVECTOR = _descriptor.Descriptor(\n  name=\'BlobProtoVector\',\n  full_name=\'caffe.BlobProtoVector\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'blobs\', full_name=\'caffe.BlobProtoVector.blobs\', index=0,\n      number=1, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=259,\n  serialized_end=309,\n)\n\n\n_DATUM = _descriptor.Descriptor(\n  name=\'Datum\',\n  full_name=\'caffe.Datum\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'channels\', full_name=\'caffe.Datum.channels\', index=0,\n      number=1, type=5, cpp_type=1, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'height\', full_name=\'caffe.Datum.height\', index=1,\n      number=2, type=5, cpp_type=1, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'width\', full_name=\'caffe.Datum.width\', index=2,\n      number=3, type=5, cpp_type=1, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'data\', full_name=\'caffe.Datum.data\', index=3,\n      number=4, type=12, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b(""""),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'label\', full_name=\'caffe.Datum.label\', index=4,\n      number=5, type=5, cpp_type=1, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'float_data\', full_name=\'caffe.Datum.float_data\', index=5,\n      number=6, type=2, cpp_type=6, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'encoded\', full_name=\'caffe.Datum.encoded\', index=6,\n      number=7, type=8, cpp_type=7, label=1,\n      has_default_value=True, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=312,\n  serialized_end=441,\n)\n\n\n_FILLERPARAMETER = _descriptor.Descriptor(\n  name=\'FillerParameter\',\n  full_name=\'caffe.FillerParameter\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'type\', full_name=\'caffe.FillerParameter.type\', index=0,\n      number=1, type=9, cpp_type=9, label=1,\n      has_default_value=True, default_value=_b(""constant"").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'value\', full_name=\'caffe.FillerParameter.value\', index=1,\n      number=2, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=float(0),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'min\', full_name=\'caffe.FillerParameter.min\', index=2,\n      number=3, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=float(0),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'max\', full_name=\'caffe.FillerParameter.max\', index=3,\n      number=4, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=float(1),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'mean\', full_name=\'caffe.FillerParameter.mean\', index=4,\n      number=5, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=float(0),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'std\', full_name=\'caffe.FillerParameter.std\', index=5,\n      number=6, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=float(1),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'sparse\', full_name=\'caffe.FillerParameter.sparse\', index=6,\n      number=7, type=5, cpp_type=1, label=1,\n      has_default_value=True, default_value=-1,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'variance_norm\', full_name=\'caffe.FillerParameter.variance_norm\', index=7,\n      number=8, type=14, cpp_type=8, label=1,\n      has_default_value=True, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n    _FILLERPARAMETER_VARIANCENORM,\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=444,\n  serialized_end=710,\n)\n\n\n_NETPARAMETER = _descriptor.Descriptor(\n  name=\'NetParameter\',\n  full_name=\'caffe.NetParameter\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'name\', full_name=\'caffe.NetParameter.name\', index=0,\n      number=1, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'input\', full_name=\'caffe.NetParameter.input\', index=1,\n      number=3, type=9, cpp_type=9, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'input_shape\', full_name=\'caffe.NetParameter.input_shape\', index=2,\n      number=8, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'input_dim\', full_name=\'caffe.NetParameter.input_dim\', index=3,\n      number=4, type=5, cpp_type=1, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'force_backward\', full_name=\'caffe.NetParameter.force_backward\', index=4,\n      number=5, type=8, cpp_type=7, label=1,\n      has_default_value=True, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'state\', full_name=\'caffe.NetParameter.state\', index=5,\n      number=6, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'debug_info\', full_name=\'caffe.NetParameter.debug_info\', index=6,\n      number=7, type=8, cpp_type=7, label=1,\n      has_default_value=True, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'layer\', full_name=\'caffe.NetParameter.layer\', index=7,\n      number=100, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'layers\', full_name=\'caffe.NetParameter.layers\', index=8,\n      number=2, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=713,\n  serialized_end=983,\n)\n\n\n_SOLVERPARAMETER = _descriptor.Descriptor(\n  name=\'SolverParameter\',\n  full_name=\'caffe.SolverParameter\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'net\', full_name=\'caffe.SolverParameter.net\', index=0,\n      number=24, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'net_param\', full_name=\'caffe.SolverParameter.net_param\', index=1,\n      number=25, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'train_net\', full_name=\'caffe.SolverParameter.train_net\', index=2,\n      number=1, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'test_net\', full_name=\'caffe.SolverParameter.test_net\', index=3,\n      number=2, type=9, cpp_type=9, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'train_net_param\', full_name=\'caffe.SolverParameter.train_net_param\', index=4,\n      number=21, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'test_net_param\', full_name=\'caffe.SolverParameter.test_net_param\', index=5,\n      number=22, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'train_state\', full_name=\'caffe.SolverParameter.train_state\', index=6,\n      number=26, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'test_state\', full_name=\'caffe.SolverParameter.test_state\', index=7,\n      number=27, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'test_iter\', full_name=\'caffe.SolverParameter.test_iter\', index=8,\n      number=3, type=5, cpp_type=1, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'test_interval\', full_name=\'caffe.SolverParameter.test_interval\', index=9,\n      number=4, type=5, cpp_type=1, label=1,\n      has_default_value=True, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'test_compute_loss\', full_name=\'caffe.SolverParameter.test_compute_loss\', index=10,\n      number=19, type=8, cpp_type=7, label=1,\n      has_default_value=True, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'test_initialization\', full_name=\'caffe.SolverParameter.test_initialization\', index=11,\n      number=32, type=8, cpp_type=7, label=1,\n      has_default_value=True, default_value=True,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'base_lr\', full_name=\'caffe.SolverParameter.base_lr\', index=12,\n      number=5, type=2, cpp_type=6, label=1,\n      has_default_value=False, default_value=float(0),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'display\', full_name=\'caffe.SolverParameter.display\', index=13,\n      number=6, type=5, cpp_type=1, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'average_loss\', full_name=\'caffe.SolverParameter.average_loss\', index=14,\n      number=33, type=5, cpp_type=1, label=1,\n      has_default_value=True, default_value=1,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'max_iter\', full_name=\'caffe.SolverParameter.max_iter\', index=15,\n      number=7, type=5, cpp_type=1, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'iter_size\', full_name=\'caffe.SolverParameter.iter_size\', index=16,\n      number=36, type=5, cpp_type=1, label=1,\n      has_default_value=True, default_value=1,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'lr_policy\', full_name=\'caffe.SolverParameter.lr_policy\', index=17,\n      number=8, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'gamma\', full_name=\'caffe.SolverParameter.gamma\', index=18,\n      number=9, type=2, cpp_type=6, label=1,\n      has_default_value=False, default_value=float(0),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'power\', full_name=\'caffe.SolverParameter.power\', index=19,\n      number=10, type=2, cpp_type=6, label=1,\n      has_default_value=False, default_value=float(0),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'momentum\', full_name=\'caffe.SolverParameter.momentum\', index=20,\n      number=11, type=2, cpp_type=6, label=1,\n      has_default_value=False, default_value=float(0),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'weight_decay\', full_name=\'caffe.SolverParameter.weight_decay\', index=21,\n      number=12, type=2, cpp_type=6, label=1,\n      has_default_value=False, default_value=float(0),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'regularization_type\', full_name=\'caffe.SolverParameter.regularization_type\', index=22,\n      number=29, type=9, cpp_type=9, label=1,\n      has_default_value=True, default_value=_b(""L2"").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'stepsize\', full_name=\'caffe.SolverParameter.stepsize\', index=23,\n      number=13, type=5, cpp_type=1, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'stepvalue\', full_name=\'caffe.SolverParameter.stepvalue\', index=24,\n      number=34, type=5, cpp_type=1, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'clip_gradients\', full_name=\'caffe.SolverParameter.clip_gradients\', index=25,\n      number=35, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=float(-1),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'snapshot\', full_name=\'caffe.SolverParameter.snapshot\', index=26,\n      number=14, type=5, cpp_type=1, label=1,\n      has_default_value=True, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'snapshot_prefix\', full_name=\'caffe.SolverParameter.snapshot_prefix\', index=27,\n      number=15, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'snapshot_diff\', full_name=\'caffe.SolverParameter.snapshot_diff\', index=28,\n      number=16, type=8, cpp_type=7, label=1,\n      has_default_value=True, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'snapshot_format\', full_name=\'caffe.SolverParameter.snapshot_format\', index=29,\n      number=37, type=14, cpp_type=8, label=1,\n      has_default_value=True, default_value=1,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'solver_mode\', full_name=\'caffe.SolverParameter.solver_mode\', index=30,\n      number=17, type=14, cpp_type=8, label=1,\n      has_default_value=True, default_value=1,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'device_id\', full_name=\'caffe.SolverParameter.device_id\', index=31,\n      number=18, type=5, cpp_type=1, label=1,\n      has_default_value=True, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'random_seed\', full_name=\'caffe.SolverParameter.random_seed\', index=32,\n      number=20, type=3, cpp_type=2, label=1,\n      has_default_value=True, default_value=-1,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'type\', full_name=\'caffe.SolverParameter.type\', index=33,\n      number=40, type=9, cpp_type=9, label=1,\n      has_default_value=True, default_value=_b(""SGD"").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'delta\', full_name=\'caffe.SolverParameter.delta\', index=34,\n      number=31, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=float(1e-08),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'momentum2\', full_name=\'caffe.SolverParameter.momentum2\', index=35,\n      number=39, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=float(0.999),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'rms_decay\', full_name=\'caffe.SolverParameter.rms_decay\', index=36,\n      number=38, type=2, cpp_type=6, label=1,\n      has_default_value=False, default_value=float(0),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'debug_info\', full_name=\'caffe.SolverParameter.debug_info\', index=37,\n      number=23, type=8, cpp_type=7, label=1,\n      has_default_value=True, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'snapshot_after_train\', full_name=\'caffe.SolverParameter.snapshot_after_train\', index=38,\n      number=28, type=8, cpp_type=7, label=1,\n      has_default_value=True, default_value=True,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'solver_type\', full_name=\'caffe.SolverParameter.solver_type\', index=39,\n      number=30, type=14, cpp_type=8, label=1,\n      has_default_value=True, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n    _SOLVERPARAMETER_SNAPSHOTFORMAT,\n    _SOLVERPARAMETER_SOLVERMODE,\n    _SOLVERPARAMETER_SOLVERTYPE,\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=986,\n  serialized_end=2294,\n)\n\n\n_SOLVERSTATE = _descriptor.Descriptor(\n  name=\'SolverState\',\n  full_name=\'caffe.SolverState\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'iter\', full_name=\'caffe.SolverState.iter\', index=0,\n      number=1, type=5, cpp_type=1, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'learned_net\', full_name=\'caffe.SolverState.learned_net\', index=1,\n      number=2, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'history\', full_name=\'caffe.SolverState.history\', index=2,\n      number=3, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'current_step\', full_name=\'caffe.SolverState.current_step\', index=3,\n      number=4, type=5, cpp_type=1, label=1,\n      has_default_value=True, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=2296,\n  serialized_end=2404,\n)\n\n\n_NETSTATE = _descriptor.Descriptor(\n  name=\'NetState\',\n  full_name=\'caffe.NetState\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'phase\', full_name=\'caffe.NetState.phase\', index=0,\n      number=1, type=14, cpp_type=8, label=1,\n      has_default_value=True, default_value=1,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'level\', full_name=\'caffe.NetState.level\', index=1,\n      number=2, type=5, cpp_type=1, label=1,\n      has_default_value=True, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'stage\', full_name=\'caffe.NetState.stage\', index=2,\n      number=3, type=9, cpp_type=9, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=2406,\n  serialized_end=2484,\n)\n\n\n_NETSTATERULE = _descriptor.Descriptor(\n  name=\'NetStateRule\',\n  full_name=\'caffe.NetStateRule\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'phase\', full_name=\'caffe.NetStateRule.phase\', index=0,\n      number=1, type=14, cpp_type=8, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'min_level\', full_name=\'caffe.NetStateRule.min_level\', index=1,\n      number=2, type=5, cpp_type=1, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'max_level\', full_name=\'caffe.NetStateRule.max_level\', index=2,\n      number=3, type=5, cpp_type=1, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'stage\', full_name=\'caffe.NetStateRule.stage\', index=3,\n      number=4, type=9, cpp_type=9, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'not_stage\', full_name=\'caffe.NetStateRule.not_stage\', index=4,\n      number=5, type=9, cpp_type=9, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=2486,\n  serialized_end=2601,\n)\n\n\n_PARAMSPEC = _descriptor.Descriptor(\n  name=\'ParamSpec\',\n  full_name=\'caffe.ParamSpec\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'name\', full_name=\'caffe.ParamSpec.name\', index=0,\n      number=1, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'share_mode\', full_name=\'caffe.ParamSpec.share_mode\', index=1,\n      number=2, type=14, cpp_type=8, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'lr_mult\', full_name=\'caffe.ParamSpec.lr_mult\', index=2,\n      number=3, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=float(1),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'decay_mult\', full_name=\'caffe.ParamSpec.decay_mult\', index=3,\n      number=4, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=float(1),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n    _PARAMSPEC_DIMCHECKMODE,\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=2604,\n  serialized_end=2767,\n)\n\n\n_LAYERPARAMETER = _descriptor.Descriptor(\n  name=\'LayerParameter\',\n  full_name=\'caffe.LayerParameter\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'name\', full_name=\'caffe.LayerParameter.name\', index=0,\n      number=1, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'type\', full_name=\'caffe.LayerParameter.type\', index=1,\n      number=2, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'bottom\', full_name=\'caffe.LayerParameter.bottom\', index=2,\n      number=3, type=9, cpp_type=9, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'top\', full_name=\'caffe.LayerParameter.top\', index=3,\n      number=4, type=9, cpp_type=9, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'phase\', full_name=\'caffe.LayerParameter.phase\', index=4,\n      number=10, type=14, cpp_type=8, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'loss_weight\', full_name=\'caffe.LayerParameter.loss_weight\', index=5,\n      number=5, type=2, cpp_type=6, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'param\', full_name=\'caffe.LayerParameter.param\', index=6,\n      number=6, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'blobs\', full_name=\'caffe.LayerParameter.blobs\', index=7,\n      number=7, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'propagate_down\', full_name=\'caffe.LayerParameter.propagate_down\', index=8,\n      number=11, type=8, cpp_type=7, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'include\', full_name=\'caffe.LayerParameter.include\', index=9,\n      number=8, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'exclude\', full_name=\'caffe.LayerParameter.exclude\', index=10,\n      number=9, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'transform_param\', full_name=\'caffe.LayerParameter.transform_param\', index=11,\n      number=100, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'loss_param\', full_name=\'caffe.LayerParameter.loss_param\', index=12,\n      number=101, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'accuracy_param\', full_name=\'caffe.LayerParameter.accuracy_param\', index=13,\n      number=102, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'argmax_param\', full_name=\'caffe.LayerParameter.argmax_param\', index=14,\n      number=103, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'batch_norm_param\', full_name=\'caffe.LayerParameter.batch_norm_param\', index=15,\n      number=139, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'bias_param\', full_name=\'caffe.LayerParameter.bias_param\', index=16,\n      number=141, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'concat_param\', full_name=\'caffe.LayerParameter.concat_param\', index=17,\n      number=104, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'contrastive_loss_param\', full_name=\'caffe.LayerParameter.contrastive_loss_param\', index=18,\n      number=105, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'convolution_param\', full_name=\'caffe.LayerParameter.convolution_param\', index=19,\n      number=106, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'data_param\', full_name=\'caffe.LayerParameter.data_param\', index=20,\n      number=107, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'dropout_param\', full_name=\'caffe.LayerParameter.dropout_param\', index=21,\n      number=108, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'dummy_data_param\', full_name=\'caffe.LayerParameter.dummy_data_param\', index=22,\n      number=109, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'eltwise_param\', full_name=\'caffe.LayerParameter.eltwise_param\', index=23,\n      number=110, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'elu_param\', full_name=\'caffe.LayerParameter.elu_param\', index=24,\n      number=140, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'embed_param\', full_name=\'caffe.LayerParameter.embed_param\', index=25,\n      number=137, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'exp_param\', full_name=\'caffe.LayerParameter.exp_param\', index=26,\n      number=111, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'flatten_param\', full_name=\'caffe.LayerParameter.flatten_param\', index=27,\n      number=135, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'hdf5_data_param\', full_name=\'caffe.LayerParameter.hdf5_data_param\', index=28,\n      number=112, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'hdf5_output_param\', full_name=\'caffe.LayerParameter.hdf5_output_param\', index=29,\n      number=113, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'hinge_loss_param\', full_name=\'caffe.LayerParameter.hinge_loss_param\', index=30,\n      number=114, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'image_data_param\', full_name=\'caffe.LayerParameter.image_data_param\', index=31,\n      number=115, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'infogain_loss_param\', full_name=\'caffe.LayerParameter.infogain_loss_param\', index=32,\n      number=116, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'inner_product_param\', full_name=\'caffe.LayerParameter.inner_product_param\', index=33,\n      number=117, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'log_param\', full_name=\'caffe.LayerParameter.log_param\', index=34,\n      number=134, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'lrn_param\', full_name=\'caffe.LayerParameter.lrn_param\', index=35,\n      number=118, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'memory_data_param\', full_name=\'caffe.LayerParameter.memory_data_param\', index=36,\n      number=119, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'mvn_param\', full_name=\'caffe.LayerParameter.mvn_param\', index=37,\n      number=120, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'pooling_param\', full_name=\'caffe.LayerParameter.pooling_param\', index=38,\n      number=121, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'power_param\', full_name=\'caffe.LayerParameter.power_param\', index=39,\n      number=122, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'prelu_param\', full_name=\'caffe.LayerParameter.prelu_param\', index=40,\n      number=131, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'python_param\', full_name=\'caffe.LayerParameter.python_param\', index=41,\n      number=130, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'reduction_param\', full_name=\'caffe.LayerParameter.reduction_param\', index=42,\n      number=136, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'relu_param\', full_name=\'caffe.LayerParameter.relu_param\', index=43,\n      number=123, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'reshape_param\', full_name=\'caffe.LayerParameter.reshape_param\', index=44,\n      number=133, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'scale_param\', full_name=\'caffe.LayerParameter.scale_param\', index=45,\n      number=142, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'sigmoid_param\', full_name=\'caffe.LayerParameter.sigmoid_param\', index=46,\n      number=124, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'softmax_param\', full_name=\'caffe.LayerParameter.softmax_param\', index=47,\n      number=125, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'spp_param\', full_name=\'caffe.LayerParameter.spp_param\', index=48,\n      number=132, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'slice_param\', full_name=\'caffe.LayerParameter.slice_param\', index=49,\n      number=126, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'tanh_param\', full_name=\'caffe.LayerParameter.tanh_param\', index=50,\n      number=127, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'threshold_param\', full_name=\'caffe.LayerParameter.threshold_param\', index=51,\n      number=128, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'tile_param\', full_name=\'caffe.LayerParameter.tile_param\', index=52,\n      number=138, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'window_data_param\', full_name=\'caffe.LayerParameter.window_data_param\', index=53,\n      number=129, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=2770,\n  serialized_end=5138,\n)\n\n\n_TRANSFORMATIONPARAMETER = _descriptor.Descriptor(\n  name=\'TransformationParameter\',\n  full_name=\'caffe.TransformationParameter\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'scale\', full_name=\'caffe.TransformationParameter.scale\', index=0,\n      number=1, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=float(1),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'mirror\', full_name=\'caffe.TransformationParameter.mirror\', index=1,\n      number=2, type=8, cpp_type=7, label=1,\n      has_default_value=True, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'crop_size\', full_name=\'caffe.TransformationParameter.crop_size\', index=2,\n      number=3, type=13, cpp_type=3, label=1,\n      has_default_value=True, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'mean_file\', full_name=\'caffe.TransformationParameter.mean_file\', index=3,\n      number=4, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'mean_value\', full_name=\'caffe.TransformationParameter.mean_value\', index=4,\n      number=5, type=2, cpp_type=6, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'force_color\', full_name=\'caffe.TransformationParameter.force_color\', index=5,\n      number=6, type=8, cpp_type=7, label=1,\n      has_default_value=True, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'force_gray\', full_name=\'caffe.TransformationParameter.force_gray\', index=6,\n      number=7, type=8, cpp_type=7, label=1,\n      has_default_value=True, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=5141,\n  serialized_end=5323,\n)\n\n\n_LOSSPARAMETER = _descriptor.Descriptor(\n  name=\'LossParameter\',\n  full_name=\'caffe.LossParameter\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'ignore_label\', full_name=\'caffe.LossParameter.ignore_label\', index=0,\n      number=1, type=5, cpp_type=1, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'normalization\', full_name=\'caffe.LossParameter.normalization\', index=1,\n      number=3, type=14, cpp_type=8, label=1,\n      has_default_value=True, default_value=1,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'normalize\', full_name=\'caffe.LossParameter.normalize\', index=2,\n      number=2, type=8, cpp_type=7, label=1,\n      has_default_value=False, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n    _LOSSPARAMETER_NORMALIZATIONMODE,\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=5326,\n  serialized_end=5520,\n)\n\n\n_ACCURACYPARAMETER = _descriptor.Descriptor(\n  name=\'AccuracyParameter\',\n  full_name=\'caffe.AccuracyParameter\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'top_k\', full_name=\'caffe.AccuracyParameter.top_k\', index=0,\n      number=1, type=13, cpp_type=3, label=1,\n      has_default_value=True, default_value=1,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'axis\', full_name=\'caffe.AccuracyParameter.axis\', index=1,\n      number=2, type=5, cpp_type=1, label=1,\n      has_default_value=True, default_value=1,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'ignore_label\', full_name=\'caffe.AccuracyParameter.ignore_label\', index=2,\n      number=3, type=5, cpp_type=1, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=5522,\n  serialized_end=5598,\n)\n\n\n_ARGMAXPARAMETER = _descriptor.Descriptor(\n  name=\'ArgMaxParameter\',\n  full_name=\'caffe.ArgMaxParameter\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'out_max_val\', full_name=\'caffe.ArgMaxParameter.out_max_val\', index=0,\n      number=1, type=8, cpp_type=7, label=1,\n      has_default_value=True, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'top_k\', full_name=\'caffe.ArgMaxParameter.top_k\', index=1,\n      number=2, type=13, cpp_type=3, label=1,\n      has_default_value=True, default_value=1,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'axis\', full_name=\'caffe.ArgMaxParameter.axis\', index=2,\n      number=3, type=5, cpp_type=1, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=5600,\n  serialized_end=5677,\n)\n\n\n_CONCATPARAMETER = _descriptor.Descriptor(\n  name=\'ConcatParameter\',\n  full_name=\'caffe.ConcatParameter\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'axis\', full_name=\'caffe.ConcatParameter.axis\', index=0,\n      number=2, type=5, cpp_type=1, label=1,\n      has_default_value=True, default_value=1,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'concat_dim\', full_name=\'caffe.ConcatParameter.concat_dim\', index=1,\n      number=1, type=13, cpp_type=3, label=1,\n      has_default_value=True, default_value=1,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=5679,\n  serialized_end=5736,\n)\n\n\n_BATCHNORMPARAMETER = _descriptor.Descriptor(\n  name=\'BatchNormParameter\',\n  full_name=\'caffe.BatchNormParameter\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'use_global_stats\', full_name=\'caffe.BatchNormParameter.use_global_stats\', index=0,\n      number=1, type=8, cpp_type=7, label=1,\n      has_default_value=False, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'moving_average_fraction\', full_name=\'caffe.BatchNormParameter.moving_average_fraction\', index=1,\n      number=2, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=float(0.999),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'eps\', full_name=\'caffe.BatchNormParameter.eps\', index=2,\n      number=3, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=float(1e-05),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=5738,\n  serialized_end=5844,\n)\n\n\n_BIASPARAMETER = _descriptor.Descriptor(\n  name=\'BiasParameter\',\n  full_name=\'caffe.BiasParameter\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'axis\', full_name=\'caffe.BiasParameter.axis\', index=0,\n      number=1, type=5, cpp_type=1, label=1,\n      has_default_value=True, default_value=1,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'num_axes\', full_name=\'caffe.BiasParameter.num_axes\', index=1,\n      number=2, type=5, cpp_type=1, label=1,\n      has_default_value=True, default_value=1,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'filler\', full_name=\'caffe.BiasParameter.filler\', index=2,\n      number=3, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=5846,\n  serialized_end=5939,\n)\n\n\n_CONTRASTIVELOSSPARAMETER = _descriptor.Descriptor(\n  name=\'ContrastiveLossParameter\',\n  full_name=\'caffe.ContrastiveLossParameter\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'margin\', full_name=\'caffe.ContrastiveLossParameter.margin\', index=0,\n      number=1, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=float(1),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'legacy_version\', full_name=\'caffe.ContrastiveLossParameter.legacy_version\', index=1,\n      number=2, type=8, cpp_type=7, label=1,\n      has_default_value=True, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=5941,\n  serialized_end=6017,\n)\n\n\n_CONVOLUTIONPARAMETER = _descriptor.Descriptor(\n  name=\'ConvolutionParameter\',\n  full_name=\'caffe.ConvolutionParameter\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'num_output\', full_name=\'caffe.ConvolutionParameter.num_output\', index=0,\n      number=1, type=13, cpp_type=3, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'bias_term\', full_name=\'caffe.ConvolutionParameter.bias_term\', index=1,\n      number=2, type=8, cpp_type=7, label=1,\n      has_default_value=True, default_value=True,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'pad\', full_name=\'caffe.ConvolutionParameter.pad\', index=2,\n      number=3, type=13, cpp_type=3, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'kernel_size\', full_name=\'caffe.ConvolutionParameter.kernel_size\', index=3,\n      number=4, type=13, cpp_type=3, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'stride\', full_name=\'caffe.ConvolutionParameter.stride\', index=4,\n      number=6, type=13, cpp_type=3, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'dilation\', full_name=\'caffe.ConvolutionParameter.dilation\', index=5,\n      number=18, type=13, cpp_type=3, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'pad_h\', full_name=\'caffe.ConvolutionParameter.pad_h\', index=6,\n      number=9, type=13, cpp_type=3, label=1,\n      has_default_value=True, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'pad_w\', full_name=\'caffe.ConvolutionParameter.pad_w\', index=7,\n      number=10, type=13, cpp_type=3, label=1,\n      has_default_value=True, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'kernel_h\', full_name=\'caffe.ConvolutionParameter.kernel_h\', index=8,\n      number=11, type=13, cpp_type=3, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'kernel_w\', full_name=\'caffe.ConvolutionParameter.kernel_w\', index=9,\n      number=12, type=13, cpp_type=3, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'stride_h\', full_name=\'caffe.ConvolutionParameter.stride_h\', index=10,\n      number=13, type=13, cpp_type=3, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'stride_w\', full_name=\'caffe.ConvolutionParameter.stride_w\', index=11,\n      number=14, type=13, cpp_type=3, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'group\', full_name=\'caffe.ConvolutionParameter.group\', index=12,\n      number=5, type=13, cpp_type=3, label=1,\n      has_default_value=True, default_value=1,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'weight_filler\', full_name=\'caffe.ConvolutionParameter.weight_filler\', index=13,\n      number=7, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'bias_filler\', full_name=\'caffe.ConvolutionParameter.bias_filler\', index=14,\n      number=8, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'engine\', full_name=\'caffe.ConvolutionParameter.engine\', index=15,\n      number=15, type=14, cpp_type=8, label=1,\n      has_default_value=True, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'axis\', full_name=\'caffe.ConvolutionParameter.axis\', index=16,\n      number=16, type=5, cpp_type=1, label=1,\n      has_default_value=True, default_value=1,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'force_nd_im2col\', full_name=\'caffe.ConvolutionParameter.force_nd_im2col\', index=17,\n      number=17, type=8, cpp_type=7, label=1,\n      has_default_value=True, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n    _CONVOLUTIONPARAMETER_ENGINE,\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=6020,\n  serialized_end=6528,\n)\n\n\n_DATAPARAMETER = _descriptor.Descriptor(\n  name=\'DataParameter\',\n  full_name=\'caffe.DataParameter\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'source\', full_name=\'caffe.DataParameter.source\', index=0,\n      number=1, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'batch_size\', full_name=\'caffe.DataParameter.batch_size\', index=1,\n      number=4, type=13, cpp_type=3, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'rand_skip\', full_name=\'caffe.DataParameter.rand_skip\', index=2,\n      number=7, type=13, cpp_type=3, label=1,\n      has_default_value=True, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'backend\', full_name=\'caffe.DataParameter.backend\', index=3,\n      number=8, type=14, cpp_type=8, label=1,\n      has_default_value=True, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'scale\', full_name=\'caffe.DataParameter.scale\', index=4,\n      number=2, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=float(1),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'mean_file\', full_name=\'caffe.DataParameter.mean_file\', index=5,\n      number=3, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'crop_size\', full_name=\'caffe.DataParameter.crop_size\', index=6,\n      number=5, type=13, cpp_type=3, label=1,\n      has_default_value=True, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'mirror\', full_name=\'caffe.DataParameter.mirror\', index=7,\n      number=6, type=8, cpp_type=7, label=1,\n      has_default_value=True, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'force_encoded_color\', full_name=\'caffe.DataParameter.force_encoded_color\', index=8,\n      number=9, type=8, cpp_type=7, label=1,\n      has_default_value=True, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'prefetch\', full_name=\'caffe.DataParameter.prefetch\', index=9,\n      number=10, type=13, cpp_type=3, label=1,\n      has_default_value=True, default_value=4,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n    _DATAPARAMETER_DB,\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=6531,\n  serialized_end=6823,\n)\n\n\n_DROPOUTPARAMETER = _descriptor.Descriptor(\n  name=\'DropoutParameter\',\n  full_name=\'caffe.DropoutParameter\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'dropout_ratio\', full_name=\'caffe.DropoutParameter.dropout_ratio\', index=0,\n      number=1, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=float(0.5),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=6825,\n  serialized_end=6871,\n)\n\n\n_DUMMYDATAPARAMETER = _descriptor.Descriptor(\n  name=\'DummyDataParameter\',\n  full_name=\'caffe.DummyDataParameter\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'data_filler\', full_name=\'caffe.DummyDataParameter.data_filler\', index=0,\n      number=1, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'shape\', full_name=\'caffe.DummyDataParameter.shape\', index=1,\n      number=6, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'num\', full_name=\'caffe.DummyDataParameter.num\', index=2,\n      number=2, type=13, cpp_type=3, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'channels\', full_name=\'caffe.DummyDataParameter.channels\', index=3,\n      number=3, type=13, cpp_type=3, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'height\', full_name=\'caffe.DummyDataParameter.height\', index=4,\n      number=4, type=13, cpp_type=3, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'width\', full_name=\'caffe.DummyDataParameter.width\', index=5,\n      number=5, type=13, cpp_type=3, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=6874,\n  serialized_end=7034,\n)\n\n\n_ELTWISEPARAMETER = _descriptor.Descriptor(\n  name=\'EltwiseParameter\',\n  full_name=\'caffe.EltwiseParameter\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'operation\', full_name=\'caffe.EltwiseParameter.operation\', index=0,\n      number=1, type=14, cpp_type=8, label=1,\n      has_default_value=True, default_value=1,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'coeff\', full_name=\'caffe.EltwiseParameter.coeff\', index=1,\n      number=2, type=2, cpp_type=6, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'stable_prod_grad\', full_name=\'caffe.EltwiseParameter.stable_prod_grad\', index=2,\n      number=3, type=8, cpp_type=7, label=1,\n      has_default_value=True, default_value=True,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n    _ELTWISEPARAMETER_ELTWISEOP,\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=7037,\n  serialized_end=7202,\n)\n\n\n_ELUPARAMETER = _descriptor.Descriptor(\n  name=\'ELUParameter\',\n  full_name=\'caffe.ELUParameter\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'alpha\', full_name=\'caffe.ELUParameter.alpha\', index=0,\n      number=1, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=float(1),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=7204,\n  serialized_end=7236,\n)\n\n\n_EMBEDPARAMETER = _descriptor.Descriptor(\n  name=\'EmbedParameter\',\n  full_name=\'caffe.EmbedParameter\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'num_output\', full_name=\'caffe.EmbedParameter.num_output\', index=0,\n      number=1, type=13, cpp_type=3, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'input_dim\', full_name=\'caffe.EmbedParameter.input_dim\', index=1,\n      number=2, type=13, cpp_type=3, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'bias_term\', full_name=\'caffe.EmbedParameter.bias_term\', index=2,\n      number=3, type=8, cpp_type=7, label=1,\n      has_default_value=True, default_value=True,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'weight_filler\', full_name=\'caffe.EmbedParameter.weight_filler\', index=3,\n      number=4, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'bias_filler\', full_name=\'caffe.EmbedParameter.bias_filler\', index=4,\n      number=5, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=7239,\n  serialized_end=7411,\n)\n\n\n_EXPPARAMETER = _descriptor.Descriptor(\n  name=\'ExpParameter\',\n  full_name=\'caffe.ExpParameter\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'base\', full_name=\'caffe.ExpParameter.base\', index=0,\n      number=1, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=float(-1),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'scale\', full_name=\'caffe.ExpParameter.scale\', index=1,\n      number=2, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=float(1),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'shift\', full_name=\'caffe.ExpParameter.shift\', index=2,\n      number=3, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=float(0),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=7413,\n  serialized_end=7481,\n)\n\n\n_FLATTENPARAMETER = _descriptor.Descriptor(\n  name=\'FlattenParameter\',\n  full_name=\'caffe.FlattenParameter\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'axis\', full_name=\'caffe.FlattenParameter.axis\', index=0,\n      number=1, type=5, cpp_type=1, label=1,\n      has_default_value=True, default_value=1,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'end_axis\', full_name=\'caffe.FlattenParameter.end_axis\', index=1,\n      number=2, type=5, cpp_type=1, label=1,\n      has_default_value=True, default_value=-1,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=7483,\n  serialized_end=7540,\n)\n\n\n_HDF5DATAPARAMETER = _descriptor.Descriptor(\n  name=\'HDF5DataParameter\',\n  full_name=\'caffe.HDF5DataParameter\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'source\', full_name=\'caffe.HDF5DataParameter.source\', index=0,\n      number=1, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'batch_size\', full_name=\'caffe.HDF5DataParameter.batch_size\', index=1,\n      number=2, type=13, cpp_type=3, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'shuffle\', full_name=\'caffe.HDF5DataParameter.shuffle\', index=2,\n      number=3, type=8, cpp_type=7, label=1,\n      has_default_value=True, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=7542,\n  serialized_end=7621,\n)\n\n\n_HDF5OUTPUTPARAMETER = _descriptor.Descriptor(\n  name=\'HDF5OutputParameter\',\n  full_name=\'caffe.HDF5OutputParameter\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'file_name\', full_name=\'caffe.HDF5OutputParameter.file_name\', index=0,\n      number=1, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=7623,\n  serialized_end=7663,\n)\n\n\n_HINGELOSSPARAMETER = _descriptor.Descriptor(\n  name=\'HingeLossParameter\',\n  full_name=\'caffe.HingeLossParameter\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'norm\', full_name=\'caffe.HingeLossParameter.norm\', index=0,\n      number=1, type=14, cpp_type=8, label=1,\n      has_default_value=True, default_value=1,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n    _HINGELOSSPARAMETER_NORM,\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=7665,\n  serialized_end=7759,\n)\n\n\n_IMAGEDATAPARAMETER = _descriptor.Descriptor(\n  name=\'ImageDataParameter\',\n  full_name=\'caffe.ImageDataParameter\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'source\', full_name=\'caffe.ImageDataParameter.source\', index=0,\n      number=1, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'batch_size\', full_name=\'caffe.ImageDataParameter.batch_size\', index=1,\n      number=4, type=13, cpp_type=3, label=1,\n      has_default_value=True, default_value=1,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'rand_skip\', full_name=\'caffe.ImageDataParameter.rand_skip\', index=2,\n      number=7, type=13, cpp_type=3, label=1,\n      has_default_value=True, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'shuffle\', full_name=\'caffe.ImageDataParameter.shuffle\', index=3,\n      number=8, type=8, cpp_type=7, label=1,\n      has_default_value=True, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'new_height\', full_name=\'caffe.ImageDataParameter.new_height\', index=4,\n      number=9, type=13, cpp_type=3, label=1,\n      has_default_value=True, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'new_width\', full_name=\'caffe.ImageDataParameter.new_width\', index=5,\n      number=10, type=13, cpp_type=3, label=1,\n      has_default_value=True, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'is_color\', full_name=\'caffe.ImageDataParameter.is_color\', index=6,\n      number=11, type=8, cpp_type=7, label=1,\n      has_default_value=True, default_value=True,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'scale\', full_name=\'caffe.ImageDataParameter.scale\', index=7,\n      number=2, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=float(1),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'mean_file\', full_name=\'caffe.ImageDataParameter.mean_file\', index=8,\n      number=3, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'crop_size\', full_name=\'caffe.ImageDataParameter.crop_size\', index=9,\n      number=5, type=13, cpp_type=3, label=1,\n      has_default_value=True, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'mirror\', full_name=\'caffe.ImageDataParameter.mirror\', index=10,\n      number=6, type=8, cpp_type=7, label=1,\n      has_default_value=True, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'root_folder\', full_name=\'caffe.ImageDataParameter.root_folder\', index=11,\n      number=12, type=9, cpp_type=9, label=1,\n      has_default_value=True, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=7762,\n  serialized_end=8041,\n)\n\n\n_INFOGAINLOSSPARAMETER = _descriptor.Descriptor(\n  name=\'InfogainLossParameter\',\n  full_name=\'caffe.InfogainLossParameter\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'source\', full_name=\'caffe.InfogainLossParameter.source\', index=0,\n      number=1, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=8043,\n  serialized_end=8082,\n)\n\n\n_INNERPRODUCTPARAMETER = _descriptor.Descriptor(\n  name=\'InnerProductParameter\',\n  full_name=\'caffe.InnerProductParameter\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'num_output\', full_name=\'caffe.InnerProductParameter.num_output\', index=0,\n      number=1, type=13, cpp_type=3, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'bias_term\', full_name=\'caffe.InnerProductParameter.bias_term\', index=1,\n      number=2, type=8, cpp_type=7, label=1,\n      has_default_value=True, default_value=True,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'weight_filler\', full_name=\'caffe.InnerProductParameter.weight_filler\', index=2,\n      number=3, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'bias_filler\', full_name=\'caffe.InnerProductParameter.bias_filler\', index=3,\n      number=4, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'axis\', full_name=\'caffe.InnerProductParameter.axis\', index=4,\n      number=5, type=5, cpp_type=1, label=1,\n      has_default_value=True, default_value=1,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=8085,\n  serialized_end=8262,\n)\n\n\n_LOGPARAMETER = _descriptor.Descriptor(\n  name=\'LogParameter\',\n  full_name=\'caffe.LogParameter\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'base\', full_name=\'caffe.LogParameter.base\', index=0,\n      number=1, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=float(-1),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'scale\', full_name=\'caffe.LogParameter.scale\', index=1,\n      number=2, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=float(1),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'shift\', full_name=\'caffe.LogParameter.shift\', index=2,\n      number=3, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=float(0),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=8264,\n  serialized_end=8332,\n)\n\n\n_LRNPARAMETER = _descriptor.Descriptor(\n  name=\'LRNParameter\',\n  full_name=\'caffe.LRNParameter\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'local_size\', full_name=\'caffe.LRNParameter.local_size\', index=0,\n      number=1, type=13, cpp_type=3, label=1,\n      has_default_value=True, default_value=5,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'alpha\', full_name=\'caffe.LRNParameter.alpha\', index=1,\n      number=2, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=float(1),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'beta\', full_name=\'caffe.LRNParameter.beta\', index=2,\n      number=3, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=float(0.75),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'norm_region\', full_name=\'caffe.LRNParameter.norm_region\', index=3,\n      number=4, type=14, cpp_type=8, label=1,\n      has_default_value=True, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'k\', full_name=\'caffe.LRNParameter.k\', index=4,\n      number=5, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=float(1),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'engine\', full_name=\'caffe.LRNParameter.engine\', index=5,\n      number=6, type=14, cpp_type=8, label=1,\n      has_default_value=True, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n    _LRNPARAMETER_NORMREGION,\n    _LRNPARAMETER_ENGINE,\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=8335,\n  serialized_end=8647,\n)\n\n\n_MEMORYDATAPARAMETER = _descriptor.Descriptor(\n  name=\'MemoryDataParameter\',\n  full_name=\'caffe.MemoryDataParameter\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'batch_size\', full_name=\'caffe.MemoryDataParameter.batch_size\', index=0,\n      number=1, type=13, cpp_type=3, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'channels\', full_name=\'caffe.MemoryDataParameter.channels\', index=1,\n      number=2, type=13, cpp_type=3, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'height\', full_name=\'caffe.MemoryDataParameter.height\', index=2,\n      number=3, type=13, cpp_type=3, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'width\', full_name=\'caffe.MemoryDataParameter.width\', index=3,\n      number=4, type=13, cpp_type=3, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=8649,\n  serialized_end=8739,\n)\n\n\n_MVNPARAMETER = _descriptor.Descriptor(\n  name=\'MVNParameter\',\n  full_name=\'caffe.MVNParameter\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'normalize_variance\', full_name=\'caffe.MVNParameter.normalize_variance\', index=0,\n      number=1, type=8, cpp_type=7, label=1,\n      has_default_value=True, default_value=True,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'across_channels\', full_name=\'caffe.MVNParameter.across_channels\', index=1,\n      number=2, type=8, cpp_type=7, label=1,\n      has_default_value=True, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'eps\', full_name=\'caffe.MVNParameter.eps\', index=2,\n      number=3, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=float(1e-09),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=8741,\n  serialized_end=8841,\n)\n\n\n_POOLINGPARAMETER = _descriptor.Descriptor(\n  name=\'PoolingParameter\',\n  full_name=\'caffe.PoolingParameter\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'pool\', full_name=\'caffe.PoolingParameter.pool\', index=0,\n      number=1, type=14, cpp_type=8, label=1,\n      has_default_value=True, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'pad\', full_name=\'caffe.PoolingParameter.pad\', index=1,\n      number=4, type=13, cpp_type=3, label=1,\n      has_default_value=True, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'pad_h\', full_name=\'caffe.PoolingParameter.pad_h\', index=2,\n      number=9, type=13, cpp_type=3, label=1,\n      has_default_value=True, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'pad_w\', full_name=\'caffe.PoolingParameter.pad_w\', index=3,\n      number=10, type=13, cpp_type=3, label=1,\n      has_default_value=True, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'kernel_size\', full_name=\'caffe.PoolingParameter.kernel_size\', index=4,\n      number=2, type=13, cpp_type=3, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'kernel_h\', full_name=\'caffe.PoolingParameter.kernel_h\', index=5,\n      number=5, type=13, cpp_type=3, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'kernel_w\', full_name=\'caffe.PoolingParameter.kernel_w\', index=6,\n      number=6, type=13, cpp_type=3, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'stride\', full_name=\'caffe.PoolingParameter.stride\', index=7,\n      number=3, type=13, cpp_type=3, label=1,\n      has_default_value=True, default_value=1,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'stride_h\', full_name=\'caffe.PoolingParameter.stride_h\', index=8,\n      number=7, type=13, cpp_type=3, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'stride_w\', full_name=\'caffe.PoolingParameter.stride_w\', index=9,\n      number=8, type=13, cpp_type=3, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'engine\', full_name=\'caffe.PoolingParameter.engine\', index=10,\n      number=11, type=14, cpp_type=8, label=1,\n      has_default_value=True, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'global_pooling\', full_name=\'caffe.PoolingParameter.global_pooling\', index=11,\n      number=12, type=8, cpp_type=7, label=1,\n      has_default_value=True, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n    _POOLINGPARAMETER_POOLMETHOD,\n    _POOLINGPARAMETER_ENGINE,\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=8844,\n  serialized_end=9262,\n)\n\n\n_POWERPARAMETER = _descriptor.Descriptor(\n  name=\'PowerParameter\',\n  full_name=\'caffe.PowerParameter\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'power\', full_name=\'caffe.PowerParameter.power\', index=0,\n      number=1, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=float(1),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'scale\', full_name=\'caffe.PowerParameter.scale\', index=1,\n      number=2, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=float(1),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'shift\', full_name=\'caffe.PowerParameter.shift\', index=2,\n      number=3, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=float(0),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=9264,\n  serialized_end=9334,\n)\n\n\n_PYTHONPARAMETER = _descriptor.Descriptor(\n  name=\'PythonParameter\',\n  full_name=\'caffe.PythonParameter\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'module\', full_name=\'caffe.PythonParameter.module\', index=0,\n      number=1, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'layer\', full_name=\'caffe.PythonParameter.layer\', index=1,\n      number=2, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'param_str\', full_name=\'caffe.PythonParameter.param_str\', index=2,\n      number=3, type=9, cpp_type=9, label=1,\n      has_default_value=True, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'share_in_parallel\', full_name=\'caffe.PythonParameter.share_in_parallel\', index=3,\n      number=4, type=8, cpp_type=7, label=1,\n      has_default_value=True, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=9336,\n  serialized_end=9439,\n)\n\n\n_REDUCTIONPARAMETER = _descriptor.Descriptor(\n  name=\'ReductionParameter\',\n  full_name=\'caffe.ReductionParameter\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'operation\', full_name=\'caffe.ReductionParameter.operation\', index=0,\n      number=1, type=14, cpp_type=8, label=1,\n      has_default_value=True, default_value=1,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'axis\', full_name=\'caffe.ReductionParameter.axis\', index=1,\n      number=2, type=5, cpp_type=1, label=1,\n      has_default_value=True, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'coeff\', full_name=\'caffe.ReductionParameter.coeff\', index=2,\n      number=3, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=float(1),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n    _REDUCTIONPARAMETER_REDUCTIONOP,\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=9442,\n  serialized_end=9615,\n)\n\n\n_RELUPARAMETER = _descriptor.Descriptor(\n  name=\'ReLUParameter\',\n  full_name=\'caffe.ReLUParameter\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'negative_slope\', full_name=\'caffe.ReLUParameter.negative_slope\', index=0,\n      number=1, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=float(0),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'engine\', full_name=\'caffe.ReLUParameter.engine\', index=1,\n      number=2, type=14, cpp_type=8, label=1,\n      has_default_value=True, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n    _RELUPARAMETER_ENGINE,\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=9618,\n  serialized_end=9759,\n)\n\n\n_RESHAPEPARAMETER = _descriptor.Descriptor(\n  name=\'ReshapeParameter\',\n  full_name=\'caffe.ReshapeParameter\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'shape\', full_name=\'caffe.ReshapeParameter.shape\', index=0,\n      number=1, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'axis\', full_name=\'caffe.ReshapeParameter.axis\', index=1,\n      number=2, type=5, cpp_type=1, label=1,\n      has_default_value=True, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'num_axes\', full_name=\'caffe.ReshapeParameter.num_axes\', index=2,\n      number=3, type=5, cpp_type=1, label=1,\n      has_default_value=True, default_value=-1,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=9761,\n  serialized_end=9851,\n)\n\n\n_SCALEPARAMETER = _descriptor.Descriptor(\n  name=\'ScaleParameter\',\n  full_name=\'caffe.ScaleParameter\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'axis\', full_name=\'caffe.ScaleParameter.axis\', index=0,\n      number=1, type=5, cpp_type=1, label=1,\n      has_default_value=True, default_value=1,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'num_axes\', full_name=\'caffe.ScaleParameter.num_axes\', index=1,\n      number=2, type=5, cpp_type=1, label=1,\n      has_default_value=True, default_value=1,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'filler\', full_name=\'caffe.ScaleParameter.filler\', index=2,\n      number=3, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'bias_term\', full_name=\'caffe.ScaleParameter.bias_term\', index=3,\n      number=4, type=8, cpp_type=7, label=1,\n      has_default_value=True, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'bias_filler\', full_name=\'caffe.ScaleParameter.bias_filler\', index=4,\n      number=5, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=9854,\n  serialized_end=10019,\n)\n\n\n_SIGMOIDPARAMETER = _descriptor.Descriptor(\n  name=\'SigmoidParameter\',\n  full_name=\'caffe.SigmoidParameter\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'engine\', full_name=\'caffe.SigmoidParameter.engine\', index=0,\n      number=1, type=14, cpp_type=8, label=1,\n      has_default_value=True, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n    _SIGMOIDPARAMETER_ENGINE,\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=10021,\n  serialized_end=10141,\n)\n\n\n_SLICEPARAMETER = _descriptor.Descriptor(\n  name=\'SliceParameter\',\n  full_name=\'caffe.SliceParameter\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'axis\', full_name=\'caffe.SliceParameter.axis\', index=0,\n      number=3, type=5, cpp_type=1, label=1,\n      has_default_value=True, default_value=1,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'slice_point\', full_name=\'caffe.SliceParameter.slice_point\', index=1,\n      number=2, type=13, cpp_type=3, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'slice_dim\', full_name=\'caffe.SliceParameter.slice_dim\', index=2,\n      number=1, type=13, cpp_type=3, label=1,\n      has_default_value=True, default_value=1,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=10143,\n  serialized_end=10219,\n)\n\n\n_SOFTMAXPARAMETER = _descriptor.Descriptor(\n  name=\'SoftmaxParameter\',\n  full_name=\'caffe.SoftmaxParameter\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'engine\', full_name=\'caffe.SoftmaxParameter.engine\', index=0,\n      number=1, type=14, cpp_type=8, label=1,\n      has_default_value=True, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'axis\', full_name=\'caffe.SoftmaxParameter.axis\', index=1,\n      number=2, type=5, cpp_type=1, label=1,\n      has_default_value=True, default_value=1,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n    _SOFTMAXPARAMETER_ENGINE,\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=10222,\n  serialized_end=10359,\n)\n\n\n_TANHPARAMETER = _descriptor.Descriptor(\n  name=\'TanHParameter\',\n  full_name=\'caffe.TanHParameter\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'engine\', full_name=\'caffe.TanHParameter.engine\', index=0,\n      number=1, type=14, cpp_type=8, label=1,\n      has_default_value=True, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n    _TANHPARAMETER_ENGINE,\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=10361,\n  serialized_end=10475,\n)\n\n\n_TILEPARAMETER = _descriptor.Descriptor(\n  name=\'TileParameter\',\n  full_name=\'caffe.TileParameter\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'axis\', full_name=\'caffe.TileParameter.axis\', index=0,\n      number=1, type=5, cpp_type=1, label=1,\n      has_default_value=True, default_value=1,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'tiles\', full_name=\'caffe.TileParameter.tiles\', index=1,\n      number=2, type=5, cpp_type=1, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=10477,\n  serialized_end=10524,\n)\n\n\n_THRESHOLDPARAMETER = _descriptor.Descriptor(\n  name=\'ThresholdParameter\',\n  full_name=\'caffe.ThresholdParameter\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'threshold\', full_name=\'caffe.ThresholdParameter.threshold\', index=0,\n      number=1, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=float(0),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=10526,\n  serialized_end=10568,\n)\n\n\n_WINDOWDATAPARAMETER = _descriptor.Descriptor(\n  name=\'WindowDataParameter\',\n  full_name=\'caffe.WindowDataParameter\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'source\', full_name=\'caffe.WindowDataParameter.source\', index=0,\n      number=1, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'scale\', full_name=\'caffe.WindowDataParameter.scale\', index=1,\n      number=2, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=float(1),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'mean_file\', full_name=\'caffe.WindowDataParameter.mean_file\', index=2,\n      number=3, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'batch_size\', full_name=\'caffe.WindowDataParameter.batch_size\', index=3,\n      number=4, type=13, cpp_type=3, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'crop_size\', full_name=\'caffe.WindowDataParameter.crop_size\', index=4,\n      number=5, type=13, cpp_type=3, label=1,\n      has_default_value=True, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'mirror\', full_name=\'caffe.WindowDataParameter.mirror\', index=5,\n      number=6, type=8, cpp_type=7, label=1,\n      has_default_value=True, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'fg_threshold\', full_name=\'caffe.WindowDataParameter.fg_threshold\', index=6,\n      number=7, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=float(0.5),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'bg_threshold\', full_name=\'caffe.WindowDataParameter.bg_threshold\', index=7,\n      number=8, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=float(0.5),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'fg_fraction\', full_name=\'caffe.WindowDataParameter.fg_fraction\', index=8,\n      number=9, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=float(0.25),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'context_pad\', full_name=\'caffe.WindowDataParameter.context_pad\', index=9,\n      number=10, type=13, cpp_type=3, label=1,\n      has_default_value=True, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'crop_mode\', full_name=\'caffe.WindowDataParameter.crop_mode\', index=10,\n      number=11, type=9, cpp_type=9, label=1,\n      has_default_value=True, default_value=_b(""warp"").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'cache_images\', full_name=\'caffe.WindowDataParameter.cache_images\', index=11,\n      number=12, type=8, cpp_type=7, label=1,\n      has_default_value=True, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'root_folder\', full_name=\'caffe.WindowDataParameter.root_folder\', index=12,\n      number=13, type=9, cpp_type=9, label=1,\n      has_default_value=True, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=10571,\n  serialized_end=10892,\n)\n\n\n_SPPPARAMETER = _descriptor.Descriptor(\n  name=\'SPPParameter\',\n  full_name=\'caffe.SPPParameter\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'pyramid_height\', full_name=\'caffe.SPPParameter.pyramid_height\', index=0,\n      number=1, type=13, cpp_type=3, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'pool\', full_name=\'caffe.SPPParameter.pool\', index=1,\n      number=2, type=14, cpp_type=8, label=1,\n      has_default_value=True, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'engine\', full_name=\'caffe.SPPParameter.engine\', index=2,\n      number=6, type=14, cpp_type=8, label=1,\n      has_default_value=True, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n    _SPPPARAMETER_POOLMETHOD,\n    _SPPPARAMETER_ENGINE,\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=10895,\n  serialized_end=11130,\n)\n\n\n_V1LAYERPARAMETER = _descriptor.Descriptor(\n  name=\'V1LayerParameter\',\n  full_name=\'caffe.V1LayerParameter\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'bottom\', full_name=\'caffe.V1LayerParameter.bottom\', index=0,\n      number=2, type=9, cpp_type=9, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'top\', full_name=\'caffe.V1LayerParameter.top\', index=1,\n      number=3, type=9, cpp_type=9, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'name\', full_name=\'caffe.V1LayerParameter.name\', index=2,\n      number=4, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'include\', full_name=\'caffe.V1LayerParameter.include\', index=3,\n      number=32, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'exclude\', full_name=\'caffe.V1LayerParameter.exclude\', index=4,\n      number=33, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'type\', full_name=\'caffe.V1LayerParameter.type\', index=5,\n      number=5, type=14, cpp_type=8, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'blobs\', full_name=\'caffe.V1LayerParameter.blobs\', index=6,\n      number=6, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'param\', full_name=\'caffe.V1LayerParameter.param\', index=7,\n      number=1001, type=9, cpp_type=9, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'blob_share_mode\', full_name=\'caffe.V1LayerParameter.blob_share_mode\', index=8,\n      number=1002, type=14, cpp_type=8, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'blobs_lr\', full_name=\'caffe.V1LayerParameter.blobs_lr\', index=9,\n      number=7, type=2, cpp_type=6, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'weight_decay\', full_name=\'caffe.V1LayerParameter.weight_decay\', index=10,\n      number=8, type=2, cpp_type=6, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'loss_weight\', full_name=\'caffe.V1LayerParameter.loss_weight\', index=11,\n      number=35, type=2, cpp_type=6, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'accuracy_param\', full_name=\'caffe.V1LayerParameter.accuracy_param\', index=12,\n      number=27, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'argmax_param\', full_name=\'caffe.V1LayerParameter.argmax_param\', index=13,\n      number=23, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'concat_param\', full_name=\'caffe.V1LayerParameter.concat_param\', index=14,\n      number=9, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'contrastive_loss_param\', full_name=\'caffe.V1LayerParameter.contrastive_loss_param\', index=15,\n      number=40, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'convolution_param\', full_name=\'caffe.V1LayerParameter.convolution_param\', index=16,\n      number=10, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'data_param\', full_name=\'caffe.V1LayerParameter.data_param\', index=17,\n      number=11, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'dropout_param\', full_name=\'caffe.V1LayerParameter.dropout_param\', index=18,\n      number=12, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'dummy_data_param\', full_name=\'caffe.V1LayerParameter.dummy_data_param\', index=19,\n      number=26, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'eltwise_param\', full_name=\'caffe.V1LayerParameter.eltwise_param\', index=20,\n      number=24, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'exp_param\', full_name=\'caffe.V1LayerParameter.exp_param\', index=21,\n      number=41, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'hdf5_data_param\', full_name=\'caffe.V1LayerParameter.hdf5_data_param\', index=22,\n      number=13, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'hdf5_output_param\', full_name=\'caffe.V1LayerParameter.hdf5_output_param\', index=23,\n      number=14, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'hinge_loss_param\', full_name=\'caffe.V1LayerParameter.hinge_loss_param\', index=24,\n      number=29, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'image_data_param\', full_name=\'caffe.V1LayerParameter.image_data_param\', index=25,\n      number=15, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'infogain_loss_param\', full_name=\'caffe.V1LayerParameter.infogain_loss_param\', index=26,\n      number=16, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'inner_product_param\', full_name=\'caffe.V1LayerParameter.inner_product_param\', index=27,\n      number=17, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'lrn_param\', full_name=\'caffe.V1LayerParameter.lrn_param\', index=28,\n      number=18, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'memory_data_param\', full_name=\'caffe.V1LayerParameter.memory_data_param\', index=29,\n      number=22, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'mvn_param\', full_name=\'caffe.V1LayerParameter.mvn_param\', index=30,\n      number=34, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'pooling_param\', full_name=\'caffe.V1LayerParameter.pooling_param\', index=31,\n      number=19, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'power_param\', full_name=\'caffe.V1LayerParameter.power_param\', index=32,\n      number=21, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'relu_param\', full_name=\'caffe.V1LayerParameter.relu_param\', index=33,\n      number=30, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'sigmoid_param\', full_name=\'caffe.V1LayerParameter.sigmoid_param\', index=34,\n      number=38, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'softmax_param\', full_name=\'caffe.V1LayerParameter.softmax_param\', index=35,\n      number=39, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'slice_param\', full_name=\'caffe.V1LayerParameter.slice_param\', index=36,\n      number=31, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'tanh_param\', full_name=\'caffe.V1LayerParameter.tanh_param\', index=37,\n      number=37, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'threshold_param\', full_name=\'caffe.V1LayerParameter.threshold_param\', index=38,\n      number=25, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'window_data_param\', full_name=\'caffe.V1LayerParameter.window_data_param\', index=39,\n      number=20, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'transform_param\', full_name=\'caffe.V1LayerParameter.transform_param\', index=40,\n      number=36, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'loss_param\', full_name=\'caffe.V1LayerParameter.loss_param\', index=41,\n      number=42, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'layer\', full_name=\'caffe.V1LayerParameter.layer\', index=42,\n      number=1, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n    _V1LAYERPARAMETER_LAYERTYPE,\n    _V1LAYERPARAMETER_DIMCHECKMODE,\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=11133,\n  serialized_end=13661,\n)\n\n\n_V0LAYERPARAMETER = _descriptor.Descriptor(\n  name=\'V0LayerParameter\',\n  full_name=\'caffe.V0LayerParameter\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'name\', full_name=\'caffe.V0LayerParameter.name\', index=0,\n      number=1, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'type\', full_name=\'caffe.V0LayerParameter.type\', index=1,\n      number=2, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'num_output\', full_name=\'caffe.V0LayerParameter.num_output\', index=2,\n      number=3, type=13, cpp_type=3, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'biasterm\', full_name=\'caffe.V0LayerParameter.biasterm\', index=3,\n      number=4, type=8, cpp_type=7, label=1,\n      has_default_value=True, default_value=True,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'weight_filler\', full_name=\'caffe.V0LayerParameter.weight_filler\', index=4,\n      number=5, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'bias_filler\', full_name=\'caffe.V0LayerParameter.bias_filler\', index=5,\n      number=6, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'pad\', full_name=\'caffe.V0LayerParameter.pad\', index=6,\n      number=7, type=13, cpp_type=3, label=1,\n      has_default_value=True, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'kernelsize\', full_name=\'caffe.V0LayerParameter.kernelsize\', index=7,\n      number=8, type=13, cpp_type=3, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'group\', full_name=\'caffe.V0LayerParameter.group\', index=8,\n      number=9, type=13, cpp_type=3, label=1,\n      has_default_value=True, default_value=1,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'stride\', full_name=\'caffe.V0LayerParameter.stride\', index=9,\n      number=10, type=13, cpp_type=3, label=1,\n      has_default_value=True, default_value=1,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'pool\', full_name=\'caffe.V0LayerParameter.pool\', index=10,\n      number=11, type=14, cpp_type=8, label=1,\n      has_default_value=True, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'dropout_ratio\', full_name=\'caffe.V0LayerParameter.dropout_ratio\', index=11,\n      number=12, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=float(0.5),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'local_size\', full_name=\'caffe.V0LayerParameter.local_size\', index=12,\n      number=13, type=13, cpp_type=3, label=1,\n      has_default_value=True, default_value=5,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'alpha\', full_name=\'caffe.V0LayerParameter.alpha\', index=13,\n      number=14, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=float(1),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'beta\', full_name=\'caffe.V0LayerParameter.beta\', index=14,\n      number=15, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=float(0.75),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'k\', full_name=\'caffe.V0LayerParameter.k\', index=15,\n      number=22, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=float(1),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'source\', full_name=\'caffe.V0LayerParameter.source\', index=16,\n      number=16, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'scale\', full_name=\'caffe.V0LayerParameter.scale\', index=17,\n      number=17, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=float(1),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'meanfile\', full_name=\'caffe.V0LayerParameter.meanfile\', index=18,\n      number=18, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'batchsize\', full_name=\'caffe.V0LayerParameter.batchsize\', index=19,\n      number=19, type=13, cpp_type=3, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'cropsize\', full_name=\'caffe.V0LayerParameter.cropsize\', index=20,\n      number=20, type=13, cpp_type=3, label=1,\n      has_default_value=True, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'mirror\', full_name=\'caffe.V0LayerParameter.mirror\', index=21,\n      number=21, type=8, cpp_type=7, label=1,\n      has_default_value=True, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'blobs\', full_name=\'caffe.V0LayerParameter.blobs\', index=22,\n      number=50, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'blobs_lr\', full_name=\'caffe.V0LayerParameter.blobs_lr\', index=23,\n      number=51, type=2, cpp_type=6, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'weight_decay\', full_name=\'caffe.V0LayerParameter.weight_decay\', index=24,\n      number=52, type=2, cpp_type=6, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'rand_skip\', full_name=\'caffe.V0LayerParameter.rand_skip\', index=25,\n      number=53, type=13, cpp_type=3, label=1,\n      has_default_value=True, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'det_fg_threshold\', full_name=\'caffe.V0LayerParameter.det_fg_threshold\', index=26,\n      number=54, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=float(0.5),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'det_bg_threshold\', full_name=\'caffe.V0LayerParameter.det_bg_threshold\', index=27,\n      number=55, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=float(0.5),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'det_fg_fraction\', full_name=\'caffe.V0LayerParameter.det_fg_fraction\', index=28,\n      number=56, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=float(0.25),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'det_context_pad\', full_name=\'caffe.V0LayerParameter.det_context_pad\', index=29,\n      number=58, type=13, cpp_type=3, label=1,\n      has_default_value=True, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'det_crop_mode\', full_name=\'caffe.V0LayerParameter.det_crop_mode\', index=30,\n      number=59, type=9, cpp_type=9, label=1,\n      has_default_value=True, default_value=_b(""warp"").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'new_num\', full_name=\'caffe.V0LayerParameter.new_num\', index=31,\n      number=60, type=5, cpp_type=1, label=1,\n      has_default_value=True, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'new_channels\', full_name=\'caffe.V0LayerParameter.new_channels\', index=32,\n      number=61, type=5, cpp_type=1, label=1,\n      has_default_value=True, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'new_height\', full_name=\'caffe.V0LayerParameter.new_height\', index=33,\n      number=62, type=5, cpp_type=1, label=1,\n      has_default_value=True, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'new_width\', full_name=\'caffe.V0LayerParameter.new_width\', index=34,\n      number=63, type=5, cpp_type=1, label=1,\n      has_default_value=True, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'shuffle_images\', full_name=\'caffe.V0LayerParameter.shuffle_images\', index=35,\n      number=64, type=8, cpp_type=7, label=1,\n      has_default_value=True, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'concat_dim\', full_name=\'caffe.V0LayerParameter.concat_dim\', index=36,\n      number=65, type=13, cpp_type=3, label=1,\n      has_default_value=True, default_value=1,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'hdf5_output_param\', full_name=\'caffe.V0LayerParameter.hdf5_output_param\', index=37,\n      number=1001, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n    _V0LAYERPARAMETER_POOLMETHOD,\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=13664,\n  serialized_end=14685,\n)\n\n\n_PRELUPARAMETER = _descriptor.Descriptor(\n  name=\'PReLUParameter\',\n  full_name=\'caffe.PReLUParameter\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'filler\', full_name=\'caffe.PReLUParameter.filler\', index=0,\n      number=1, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'channel_shared\', full_name=\'caffe.PReLUParameter.channel_shared\', index=1,\n      number=2, type=8, cpp_type=7, label=1,\n      has_default_value=True, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=14687,\n  serialized_end=14774,\n)\n\n_BLOBPROTO.fields_by_name[\'shape\'].message_type = _BLOBSHAPE\n_BLOBPROTOVECTOR.fields_by_name[\'blobs\'].message_type = _BLOBPROTO\n_FILLERPARAMETER.fields_by_name[\'variance_norm\'].enum_type = _FILLERPARAMETER_VARIANCENORM\n_FILLERPARAMETER_VARIANCENORM.containing_type = _FILLERPARAMETER\n_NETPARAMETER.fields_by_name[\'input_shape\'].message_type = _BLOBSHAPE\n_NETPARAMETER.fields_by_name[\'state\'].message_type = _NETSTATE\n_NETPARAMETER.fields_by_name[\'layer\'].message_type = _LAYERPARAMETER\n_NETPARAMETER.fields_by_name[\'layers\'].message_type = _V1LAYERPARAMETER\n_SOLVERPARAMETER.fields_by_name[\'net_param\'].message_type = _NETPARAMETER\n_SOLVERPARAMETER.fields_by_name[\'train_net_param\'].message_type = _NETPARAMETER\n_SOLVERPARAMETER.fields_by_name[\'test_net_param\'].message_type = _NETPARAMETER\n_SOLVERPARAMETER.fields_by_name[\'train_state\'].message_type = _NETSTATE\n_SOLVERPARAMETER.fields_by_name[\'test_state\'].message_type = _NETSTATE\n_SOLVERPARAMETER.fields_by_name[\'snapshot_format\'].enum_type = _SOLVERPARAMETER_SNAPSHOTFORMAT\n_SOLVERPARAMETER.fields_by_name[\'solver_mode\'].enum_type = _SOLVERPARAMETER_SOLVERMODE\n_SOLVERPARAMETER.fields_by_name[\'solver_type\'].enum_type = _SOLVERPARAMETER_SOLVERTYPE\n_SOLVERPARAMETER_SNAPSHOTFORMAT.containing_type = _SOLVERPARAMETER\n_SOLVERPARAMETER_SOLVERMODE.containing_type = _SOLVERPARAMETER\n_SOLVERPARAMETER_SOLVERTYPE.containing_type = _SOLVERPARAMETER\n_SOLVERSTATE.fields_by_name[\'history\'].message_type = _BLOBPROTO\n_NETSTATE.fields_by_name[\'phase\'].enum_type = _PHASE\n_NETSTATERULE.fields_by_name[\'phase\'].enum_type = _PHASE\n_PARAMSPEC.fields_by_name[\'share_mode\'].enum_type = _PARAMSPEC_DIMCHECKMODE\n_PARAMSPEC_DIMCHECKMODE.containing_type = _PARAMSPEC\n_LAYERPARAMETER.fields_by_name[\'phase\'].enum_type = _PHASE\n_LAYERPARAMETER.fields_by_name[\'param\'].message_type = _PARAMSPEC\n_LAYERPARAMETER.fields_by_name[\'blobs\'].message_type = _BLOBPROTO\n_LAYERPARAMETER.fields_by_name[\'include\'].message_type = _NETSTATERULE\n_LAYERPARAMETER.fields_by_name[\'exclude\'].message_type = _NETSTATERULE\n_LAYERPARAMETER.fields_by_name[\'transform_param\'].message_type = _TRANSFORMATIONPARAMETER\n_LAYERPARAMETER.fields_by_name[\'loss_param\'].message_type = _LOSSPARAMETER\n_LAYERPARAMETER.fields_by_name[\'accuracy_param\'].message_type = _ACCURACYPARAMETER\n_LAYERPARAMETER.fields_by_name[\'argmax_param\'].message_type = _ARGMAXPARAMETER\n_LAYERPARAMETER.fields_by_name[\'batch_norm_param\'].message_type = _BATCHNORMPARAMETER\n_LAYERPARAMETER.fields_by_name[\'bias_param\'].message_type = _BIASPARAMETER\n_LAYERPARAMETER.fields_by_name[\'concat_param\'].message_type = _CONCATPARAMETER\n_LAYERPARAMETER.fields_by_name[\'contrastive_loss_param\'].message_type = _CONTRASTIVELOSSPARAMETER\n_LAYERPARAMETER.fields_by_name[\'convolution_param\'].message_type = _CONVOLUTIONPARAMETER\n_LAYERPARAMETER.fields_by_name[\'data_param\'].message_type = _DATAPARAMETER\n_LAYERPARAMETER.fields_by_name[\'dropout_param\'].message_type = _DROPOUTPARAMETER\n_LAYERPARAMETER.fields_by_name[\'dummy_data_param\'].message_type = _DUMMYDATAPARAMETER\n_LAYERPARAMETER.fields_by_name[\'eltwise_param\'].message_type = _ELTWISEPARAMETER\n_LAYERPARAMETER.fields_by_name[\'elu_param\'].message_type = _ELUPARAMETER\n_LAYERPARAMETER.fields_by_name[\'embed_param\'].message_type = _EMBEDPARAMETER\n_LAYERPARAMETER.fields_by_name[\'exp_param\'].message_type = _EXPPARAMETER\n_LAYERPARAMETER.fields_by_name[\'flatten_param\'].message_type = _FLATTENPARAMETER\n_LAYERPARAMETER.fields_by_name[\'hdf5_data_param\'].message_type = _HDF5DATAPARAMETER\n_LAYERPARAMETER.fields_by_name[\'hdf5_output_param\'].message_type = _HDF5OUTPUTPARAMETER\n_LAYERPARAMETER.fields_by_name[\'hinge_loss_param\'].message_type = _HINGELOSSPARAMETER\n_LAYERPARAMETER.fields_by_name[\'image_data_param\'].message_type = _IMAGEDATAPARAMETER\n_LAYERPARAMETER.fields_by_name[\'infogain_loss_param\'].message_type = _INFOGAINLOSSPARAMETER\n_LAYERPARAMETER.fields_by_name[\'inner_product_param\'].message_type = _INNERPRODUCTPARAMETER\n_LAYERPARAMETER.fields_by_name[\'log_param\'].message_type = _LOGPARAMETER\n_LAYERPARAMETER.fields_by_name[\'lrn_param\'].message_type = _LRNPARAMETER\n_LAYERPARAMETER.fields_by_name[\'memory_data_param\'].message_type = _MEMORYDATAPARAMETER\n_LAYERPARAMETER.fields_by_name[\'mvn_param\'].message_type = _MVNPARAMETER\n_LAYERPARAMETER.fields_by_name[\'pooling_param\'].message_type = _POOLINGPARAMETER\n_LAYERPARAMETER.fields_by_name[\'power_param\'].message_type = _POWERPARAMETER\n_LAYERPARAMETER.fields_by_name[\'prelu_param\'].message_type = _PRELUPARAMETER\n_LAYERPARAMETER.fields_by_name[\'python_param\'].message_type = _PYTHONPARAMETER\n_LAYERPARAMETER.fields_by_name[\'reduction_param\'].message_type = _REDUCTIONPARAMETER\n_LAYERPARAMETER.fields_by_name[\'relu_param\'].message_type = _RELUPARAMETER\n_LAYERPARAMETER.fields_by_name[\'reshape_param\'].message_type = _RESHAPEPARAMETER\n_LAYERPARAMETER.fields_by_name[\'scale_param\'].message_type = _SCALEPARAMETER\n_LAYERPARAMETER.fields_by_name[\'sigmoid_param\'].message_type = _SIGMOIDPARAMETER\n_LAYERPARAMETER.fields_by_name[\'softmax_param\'].message_type = _SOFTMAXPARAMETER\n_LAYERPARAMETER.fields_by_name[\'spp_param\'].message_type = _SPPPARAMETER\n_LAYERPARAMETER.fields_by_name[\'slice_param\'].message_type = _SLICEPARAMETER\n_LAYERPARAMETER.fields_by_name[\'tanh_param\'].message_type = _TANHPARAMETER\n_LAYERPARAMETER.fields_by_name[\'threshold_param\'].message_type = _THRESHOLDPARAMETER\n_LAYERPARAMETER.fields_by_name[\'tile_param\'].message_type = _TILEPARAMETER\n_LAYERPARAMETER.fields_by_name[\'window_data_param\'].message_type = _WINDOWDATAPARAMETER\n_LOSSPARAMETER.fields_by_name[\'normalization\'].enum_type = _LOSSPARAMETER_NORMALIZATIONMODE\n_LOSSPARAMETER_NORMALIZATIONMODE.containing_type = _LOSSPARAMETER\n_BIASPARAMETER.fields_by_name[\'filler\'].message_type = _FILLERPARAMETER\n_CONVOLUTIONPARAMETER.fields_by_name[\'weight_filler\'].message_type = _FILLERPARAMETER\n_CONVOLUTIONPARAMETER.fields_by_name[\'bias_filler\'].message_type = _FILLERPARAMETER\n_CONVOLUTIONPARAMETER.fields_by_name[\'engine\'].enum_type = _CONVOLUTIONPARAMETER_ENGINE\n_CONVOLUTIONPARAMETER_ENGINE.containing_type = _CONVOLUTIONPARAMETER\n_DATAPARAMETER.fields_by_name[\'backend\'].enum_type = _DATAPARAMETER_DB\n_DATAPARAMETER_DB.containing_type = _DATAPARAMETER\n_DUMMYDATAPARAMETER.fields_by_name[\'data_filler\'].message_type = _FILLERPARAMETER\n_DUMMYDATAPARAMETER.fields_by_name[\'shape\'].message_type = _BLOBSHAPE\n_ELTWISEPARAMETER.fields_by_name[\'operation\'].enum_type = _ELTWISEPARAMETER_ELTWISEOP\n_ELTWISEPARAMETER_ELTWISEOP.containing_type = _ELTWISEPARAMETER\n_EMBEDPARAMETER.fields_by_name[\'weight_filler\'].message_type = _FILLERPARAMETER\n_EMBEDPARAMETER.fields_by_name[\'bias_filler\'].message_type = _FILLERPARAMETER\n_HINGELOSSPARAMETER.fields_by_name[\'norm\'].enum_type = _HINGELOSSPARAMETER_NORM\n_HINGELOSSPARAMETER_NORM.containing_type = _HINGELOSSPARAMETER\n_INNERPRODUCTPARAMETER.fields_by_name[\'weight_filler\'].message_type = _FILLERPARAMETER\n_INNERPRODUCTPARAMETER.fields_by_name[\'bias_filler\'].message_type = _FILLERPARAMETER\n_LRNPARAMETER.fields_by_name[\'norm_region\'].enum_type = _LRNPARAMETER_NORMREGION\n_LRNPARAMETER.fields_by_name[\'engine\'].enum_type = _LRNPARAMETER_ENGINE\n_LRNPARAMETER_NORMREGION.containing_type = _LRNPARAMETER\n_LRNPARAMETER_ENGINE.containing_type = _LRNPARAMETER\n_POOLINGPARAMETER.fields_by_name[\'pool\'].enum_type = _POOLINGPARAMETER_POOLMETHOD\n_POOLINGPARAMETER.fields_by_name[\'engine\'].enum_type = _POOLINGPARAMETER_ENGINE\n_POOLINGPARAMETER_POOLMETHOD.containing_type = _POOLINGPARAMETER\n_POOLINGPARAMETER_ENGINE.containing_type = _POOLINGPARAMETER\n_REDUCTIONPARAMETER.fields_by_name[\'operation\'].enum_type = _REDUCTIONPARAMETER_REDUCTIONOP\n_REDUCTIONPARAMETER_REDUCTIONOP.containing_type = _REDUCTIONPARAMETER\n_RELUPARAMETER.fields_by_name[\'engine\'].enum_type = _RELUPARAMETER_ENGINE\n_RELUPARAMETER_ENGINE.containing_type = _RELUPARAMETER\n_RESHAPEPARAMETER.fields_by_name[\'shape\'].message_type = _BLOBSHAPE\n_SCALEPARAMETER.fields_by_name[\'filler\'].message_type = _FILLERPARAMETER\n_SCALEPARAMETER.fields_by_name[\'bias_filler\'].message_type = _FILLERPARAMETER\n_SIGMOIDPARAMETER.fields_by_name[\'engine\'].enum_type = _SIGMOIDPARAMETER_ENGINE\n_SIGMOIDPARAMETER_ENGINE.containing_type = _SIGMOIDPARAMETER\n_SOFTMAXPARAMETER.fields_by_name[\'engine\'].enum_type = _SOFTMAXPARAMETER_ENGINE\n_SOFTMAXPARAMETER_ENGINE.containing_type = _SOFTMAXPARAMETER\n_TANHPARAMETER.fields_by_name[\'engine\'].enum_type = _TANHPARAMETER_ENGINE\n_TANHPARAMETER_ENGINE.containing_type = _TANHPARAMETER\n_SPPPARAMETER.fields_by_name[\'pool\'].enum_type = _SPPPARAMETER_POOLMETHOD\n_SPPPARAMETER.fields_by_name[\'engine\'].enum_type = _SPPPARAMETER_ENGINE\n_SPPPARAMETER_POOLMETHOD.containing_type = _SPPPARAMETER\n_SPPPARAMETER_ENGINE.containing_type = _SPPPARAMETER\n_V1LAYERPARAMETER.fields_by_name[\'include\'].message_type = _NETSTATERULE\n_V1LAYERPARAMETER.fields_by_name[\'exclude\'].message_type = _NETSTATERULE\n_V1LAYERPARAMETER.fields_by_name[\'type\'].enum_type = _V1LAYERPARAMETER_LAYERTYPE\n_V1LAYERPARAMETER.fields_by_name[\'blobs\'].message_type = _BLOBPROTO\n_V1LAYERPARAMETER.fields_by_name[\'blob_share_mode\'].enum_type = _V1LAYERPARAMETER_DIMCHECKMODE\n_V1LAYERPARAMETER.fields_by_name[\'accuracy_param\'].message_type = _ACCURACYPARAMETER\n_V1LAYERPARAMETER.fields_by_name[\'argmax_param\'].message_type = _ARGMAXPARAMETER\n_V1LAYERPARAMETER.fields_by_name[\'concat_param\'].message_type = _CONCATPARAMETER\n_V1LAYERPARAMETER.fields_by_name[\'contrastive_loss_param\'].message_type = _CONTRASTIVELOSSPARAMETER\n_V1LAYERPARAMETER.fields_by_name[\'convolution_param\'].message_type = _CONVOLUTIONPARAMETER\n_V1LAYERPARAMETER.fields_by_name[\'data_param\'].message_type = _DATAPARAMETER\n_V1LAYERPARAMETER.fields_by_name[\'dropout_param\'].message_type = _DROPOUTPARAMETER\n_V1LAYERPARAMETER.fields_by_name[\'dummy_data_param\'].message_type = _DUMMYDATAPARAMETER\n_V1LAYERPARAMETER.fields_by_name[\'eltwise_param\'].message_type = _ELTWISEPARAMETER\n_V1LAYERPARAMETER.fields_by_name[\'exp_param\'].message_type = _EXPPARAMETER\n_V1LAYERPARAMETER.fields_by_name[\'hdf5_data_param\'].message_type = _HDF5DATAPARAMETER\n_V1LAYERPARAMETER.fields_by_name[\'hdf5_output_param\'].message_type = _HDF5OUTPUTPARAMETER\n_V1LAYERPARAMETER.fields_by_name[\'hinge_loss_param\'].message_type = _HINGELOSSPARAMETER\n_V1LAYERPARAMETER.fields_by_name[\'image_data_param\'].message_type = _IMAGEDATAPARAMETER\n_V1LAYERPARAMETER.fields_by_name[\'infogain_loss_param\'].message_type = _INFOGAINLOSSPARAMETER\n_V1LAYERPARAMETER.fields_by_name[\'inner_product_param\'].message_type = _INNERPRODUCTPARAMETER\n_V1LAYERPARAMETER.fields_by_name[\'lrn_param\'].message_type = _LRNPARAMETER\n_V1LAYERPARAMETER.fields_by_name[\'memory_data_param\'].message_type = _MEMORYDATAPARAMETER\n_V1LAYERPARAMETER.fields_by_name[\'mvn_param\'].message_type = _MVNPARAMETER\n_V1LAYERPARAMETER.fields_by_name[\'pooling_param\'].message_type = _POOLINGPARAMETER\n_V1LAYERPARAMETER.fields_by_name[\'power_param\'].message_type = _POWERPARAMETER\n_V1LAYERPARAMETER.fields_by_name[\'relu_param\'].message_type = _RELUPARAMETER\n_V1LAYERPARAMETER.fields_by_name[\'sigmoid_param\'].message_type = _SIGMOIDPARAMETER\n_V1LAYERPARAMETER.fields_by_name[\'softmax_param\'].message_type = _SOFTMAXPARAMETER\n_V1LAYERPARAMETER.fields_by_name[\'slice_param\'].message_type = _SLICEPARAMETER\n_V1LAYERPARAMETER.fields_by_name[\'tanh_param\'].message_type = _TANHPARAMETER\n_V1LAYERPARAMETER.fields_by_name[\'threshold_param\'].message_type = _THRESHOLDPARAMETER\n_V1LAYERPARAMETER.fields_by_name[\'window_data_param\'].message_type = _WINDOWDATAPARAMETER\n_V1LAYERPARAMETER.fields_by_name[\'transform_param\'].message_type = _TRANSFORMATIONPARAMETER\n_V1LAYERPARAMETER.fields_by_name[\'loss_param\'].message_type = _LOSSPARAMETER\n_V1LAYERPARAMETER.fields_by_name[\'layer\'].message_type = _V0LAYERPARAMETER\n_V1LAYERPARAMETER_LAYERTYPE.containing_type = _V1LAYERPARAMETER\n_V1LAYERPARAMETER_DIMCHECKMODE.containing_type = _V1LAYERPARAMETER\n_V0LAYERPARAMETER.fields_by_name[\'weight_filler\'].message_type = _FILLERPARAMETER\n_V0LAYERPARAMETER.fields_by_name[\'bias_filler\'].message_type = _FILLERPARAMETER\n_V0LAYERPARAMETER.fields_by_name[\'pool\'].enum_type = _V0LAYERPARAMETER_POOLMETHOD\n_V0LAYERPARAMETER.fields_by_name[\'blobs\'].message_type = _BLOBPROTO\n_V0LAYERPARAMETER.fields_by_name[\'hdf5_output_param\'].message_type = _HDF5OUTPUTPARAMETER\n_V0LAYERPARAMETER_POOLMETHOD.containing_type = _V0LAYERPARAMETER\n_PRELUPARAMETER.fields_by_name[\'filler\'].message_type = _FILLERPARAMETER\nDESCRIPTOR.message_types_by_name[\'BlobShape\'] = _BLOBSHAPE\nDESCRIPTOR.message_types_by_name[\'BlobProto\'] = _BLOBPROTO\nDESCRIPTOR.message_types_by_name[\'BlobProtoVector\'] = _BLOBPROTOVECTOR\nDESCRIPTOR.message_types_by_name[\'Datum\'] = _DATUM\nDESCRIPTOR.message_types_by_name[\'FillerParameter\'] = _FILLERPARAMETER\nDESCRIPTOR.message_types_by_name[\'NetParameter\'] = _NETPARAMETER\nDESCRIPTOR.message_types_by_name[\'SolverParameter\'] = _SOLVERPARAMETER\nDESCRIPTOR.message_types_by_name[\'SolverState\'] = _SOLVERSTATE\nDESCRIPTOR.message_types_by_name[\'NetState\'] = _NETSTATE\nDESCRIPTOR.message_types_by_name[\'NetStateRule\'] = _NETSTATERULE\nDESCRIPTOR.message_types_by_name[\'ParamSpec\'] = _PARAMSPEC\nDESCRIPTOR.message_types_by_name[\'LayerParameter\'] = _LAYERPARAMETER\nDESCRIPTOR.message_types_by_name[\'TransformationParameter\'] = _TRANSFORMATIONPARAMETER\nDESCRIPTOR.message_types_by_name[\'LossParameter\'] = _LOSSPARAMETER\nDESCRIPTOR.message_types_by_name[\'AccuracyParameter\'] = _ACCURACYPARAMETER\nDESCRIPTOR.message_types_by_name[\'ArgMaxParameter\'] = _ARGMAXPARAMETER\nDESCRIPTOR.message_types_by_name[\'ConcatParameter\'] = _CONCATPARAMETER\nDESCRIPTOR.message_types_by_name[\'BatchNormParameter\'] = _BATCHNORMPARAMETER\nDESCRIPTOR.message_types_by_name[\'BiasParameter\'] = _BIASPARAMETER\nDESCRIPTOR.message_types_by_name[\'ContrastiveLossParameter\'] = _CONTRASTIVELOSSPARAMETER\nDESCRIPTOR.message_types_by_name[\'ConvolutionParameter\'] = _CONVOLUTIONPARAMETER\nDESCRIPTOR.message_types_by_name[\'DataParameter\'] = _DATAPARAMETER\nDESCRIPTOR.message_types_by_name[\'DropoutParameter\'] = _DROPOUTPARAMETER\nDESCRIPTOR.message_types_by_name[\'DummyDataParameter\'] = _DUMMYDATAPARAMETER\nDESCRIPTOR.message_types_by_name[\'EltwiseParameter\'] = _ELTWISEPARAMETER\nDESCRIPTOR.message_types_by_name[\'ELUParameter\'] = _ELUPARAMETER\nDESCRIPTOR.message_types_by_name[\'EmbedParameter\'] = _EMBEDPARAMETER\nDESCRIPTOR.message_types_by_name[\'ExpParameter\'] = _EXPPARAMETER\nDESCRIPTOR.message_types_by_name[\'FlattenParameter\'] = _FLATTENPARAMETER\nDESCRIPTOR.message_types_by_name[\'HDF5DataParameter\'] = _HDF5DATAPARAMETER\nDESCRIPTOR.message_types_by_name[\'HDF5OutputParameter\'] = _HDF5OUTPUTPARAMETER\nDESCRIPTOR.message_types_by_name[\'HingeLossParameter\'] = _HINGELOSSPARAMETER\nDESCRIPTOR.message_types_by_name[\'ImageDataParameter\'] = _IMAGEDATAPARAMETER\nDESCRIPTOR.message_types_by_name[\'InfogainLossParameter\'] = _INFOGAINLOSSPARAMETER\nDESCRIPTOR.message_types_by_name[\'InnerProductParameter\'] = _INNERPRODUCTPARAMETER\nDESCRIPTOR.message_types_by_name[\'LogParameter\'] = _LOGPARAMETER\nDESCRIPTOR.message_types_by_name[\'LRNParameter\'] = _LRNPARAMETER\nDESCRIPTOR.message_types_by_name[\'MemoryDataParameter\'] = _MEMORYDATAPARAMETER\nDESCRIPTOR.message_types_by_name[\'MVNParameter\'] = _MVNPARAMETER\nDESCRIPTOR.message_types_by_name[\'PoolingParameter\'] = _POOLINGPARAMETER\nDESCRIPTOR.message_types_by_name[\'PowerParameter\'] = _POWERPARAMETER\nDESCRIPTOR.message_types_by_name[\'PythonParameter\'] = _PYTHONPARAMETER\nDESCRIPTOR.message_types_by_name[\'ReductionParameter\'] = _REDUCTIONPARAMETER\nDESCRIPTOR.message_types_by_name[\'ReLUParameter\'] = _RELUPARAMETER\nDESCRIPTOR.message_types_by_name[\'ReshapeParameter\'] = _RESHAPEPARAMETER\nDESCRIPTOR.message_types_by_name[\'ScaleParameter\'] = _SCALEPARAMETER\nDESCRIPTOR.message_types_by_name[\'SigmoidParameter\'] = _SIGMOIDPARAMETER\nDESCRIPTOR.message_types_by_name[\'SliceParameter\'] = _SLICEPARAMETER\nDESCRIPTOR.message_types_by_name[\'SoftmaxParameter\'] = _SOFTMAXPARAMETER\nDESCRIPTOR.message_types_by_name[\'TanHParameter\'] = _TANHPARAMETER\nDESCRIPTOR.message_types_by_name[\'TileParameter\'] = _TILEPARAMETER\nDESCRIPTOR.message_types_by_name[\'ThresholdParameter\'] = _THRESHOLDPARAMETER\nDESCRIPTOR.message_types_by_name[\'WindowDataParameter\'] = _WINDOWDATAPARAMETER\nDESCRIPTOR.message_types_by_name[\'SPPParameter\'] = _SPPPARAMETER\nDESCRIPTOR.message_types_by_name[\'V1LayerParameter\'] = _V1LAYERPARAMETER\nDESCRIPTOR.message_types_by_name[\'V0LayerParameter\'] = _V0LAYERPARAMETER\nDESCRIPTOR.message_types_by_name[\'PReLUParameter\'] = _PRELUPARAMETER\nDESCRIPTOR.enum_types_by_name[\'Phase\'] = _PHASE\n\nBlobShape = _reflection.GeneratedProtocolMessageType(\'BlobShape\', (_message.Message,), dict(\n  DESCRIPTOR = _BLOBSHAPE,\n  __module__ = \'caffe_pb2\'\n  # @@protoc_insertion_point(class_scope:caffe.BlobShape)\n  ))\n_sym_db.RegisterMessage(BlobShape)\n\nBlobProto = _reflection.GeneratedProtocolMessageType(\'BlobProto\', (_message.Message,), dict(\n  DESCRIPTOR = _BLOBPROTO,\n  __module__ = \'caffe_pb2\'\n  # @@protoc_insertion_point(class_scope:caffe.BlobProto)\n  ))\n_sym_db.RegisterMessage(BlobProto)\n\nBlobProtoVector = _reflection.GeneratedProtocolMessageType(\'BlobProtoVector\', (_message.Message,), dict(\n  DESCRIPTOR = _BLOBPROTOVECTOR,\n  __module__ = \'caffe_pb2\'\n  # @@protoc_insertion_point(class_scope:caffe.BlobProtoVector)\n  ))\n_sym_db.RegisterMessage(BlobProtoVector)\n\nDatum = _reflection.GeneratedProtocolMessageType(\'Datum\', (_message.Message,), dict(\n  DESCRIPTOR = _DATUM,\n  __module__ = \'caffe_pb2\'\n  # @@protoc_insertion_point(class_scope:caffe.Datum)\n  ))\n_sym_db.RegisterMessage(Datum)\n\nFillerParameter = _reflection.GeneratedProtocolMessageType(\'FillerParameter\', (_message.Message,), dict(\n  DESCRIPTOR = _FILLERPARAMETER,\n  __module__ = \'caffe_pb2\'\n  # @@protoc_insertion_point(class_scope:caffe.FillerParameter)\n  ))\n_sym_db.RegisterMessage(FillerParameter)\n\nNetParameter = _reflection.GeneratedProtocolMessageType(\'NetParameter\', (_message.Message,), dict(\n  DESCRIPTOR = _NETPARAMETER,\n  __module__ = \'caffe_pb2\'\n  # @@protoc_insertion_point(class_scope:caffe.NetParameter)\n  ))\n_sym_db.RegisterMessage(NetParameter)\n\nSolverParameter = _reflection.GeneratedProtocolMessageType(\'SolverParameter\', (_message.Message,), dict(\n  DESCRIPTOR = _SOLVERPARAMETER,\n  __module__ = \'caffe_pb2\'\n  # @@protoc_insertion_point(class_scope:caffe.SolverParameter)\n  ))\n_sym_db.RegisterMessage(SolverParameter)\n\nSolverState = _reflection.GeneratedProtocolMessageType(\'SolverState\', (_message.Message,), dict(\n  DESCRIPTOR = _SOLVERSTATE,\n  __module__ = \'caffe_pb2\'\n  # @@protoc_insertion_point(class_scope:caffe.SolverState)\n  ))\n_sym_db.RegisterMessage(SolverState)\n\nNetState = _reflection.GeneratedProtocolMessageType(\'NetState\', (_message.Message,), dict(\n  DESCRIPTOR = _NETSTATE,\n  __module__ = \'caffe_pb2\'\n  # @@protoc_insertion_point(class_scope:caffe.NetState)\n  ))\n_sym_db.RegisterMessage(NetState)\n\nNetStateRule = _reflection.GeneratedProtocolMessageType(\'NetStateRule\', (_message.Message,), dict(\n  DESCRIPTOR = _NETSTATERULE,\n  __module__ = \'caffe_pb2\'\n  # @@protoc_insertion_point(class_scope:caffe.NetStateRule)\n  ))\n_sym_db.RegisterMessage(NetStateRule)\n\nParamSpec = _reflection.GeneratedProtocolMessageType(\'ParamSpec\', (_message.Message,), dict(\n  DESCRIPTOR = _PARAMSPEC,\n  __module__ = \'caffe_pb2\'\n  # @@protoc_insertion_point(class_scope:caffe.ParamSpec)\n  ))\n_sym_db.RegisterMessage(ParamSpec)\n\nLayerParameter = _reflection.GeneratedProtocolMessageType(\'LayerParameter\', (_message.Message,), dict(\n  DESCRIPTOR = _LAYERPARAMETER,\n  __module__ = \'caffe_pb2\'\n  # @@protoc_insertion_point(class_scope:caffe.LayerParameter)\n  ))\n_sym_db.RegisterMessage(LayerParameter)\n\nTransformationParameter = _reflection.GeneratedProtocolMessageType(\'TransformationParameter\', (_message.Message,), dict(\n  DESCRIPTOR = _TRANSFORMATIONPARAMETER,\n  __module__ = \'caffe_pb2\'\n  # @@protoc_insertion_point(class_scope:caffe.TransformationParameter)\n  ))\n_sym_db.RegisterMessage(TransformationParameter)\n\nLossParameter = _reflection.GeneratedProtocolMessageType(\'LossParameter\', (_message.Message,), dict(\n  DESCRIPTOR = _LOSSPARAMETER,\n  __module__ = \'caffe_pb2\'\n  # @@protoc_insertion_point(class_scope:caffe.LossParameter)\n  ))\n_sym_db.RegisterMessage(LossParameter)\n\nAccuracyParameter = _reflection.GeneratedProtocolMessageType(\'AccuracyParameter\', (_message.Message,), dict(\n  DESCRIPTOR = _ACCURACYPARAMETER,\n  __module__ = \'caffe_pb2\'\n  # @@protoc_insertion_point(class_scope:caffe.AccuracyParameter)\n  ))\n_sym_db.RegisterMessage(AccuracyParameter)\n\nArgMaxParameter = _reflection.GeneratedProtocolMessageType(\'ArgMaxParameter\', (_message.Message,), dict(\n  DESCRIPTOR = _ARGMAXPARAMETER,\n  __module__ = \'caffe_pb2\'\n  # @@protoc_insertion_point(class_scope:caffe.ArgMaxParameter)\n  ))\n_sym_db.RegisterMessage(ArgMaxParameter)\n\nConcatParameter = _reflection.GeneratedProtocolMessageType(\'ConcatParameter\', (_message.Message,), dict(\n  DESCRIPTOR = _CONCATPARAMETER,\n  __module__ = \'caffe_pb2\'\n  # @@protoc_insertion_point(class_scope:caffe.ConcatParameter)\n  ))\n_sym_db.RegisterMessage(ConcatParameter)\n\nBatchNormParameter = _reflection.GeneratedProtocolMessageType(\'BatchNormParameter\', (_message.Message,), dict(\n  DESCRIPTOR = _BATCHNORMPARAMETER,\n  __module__ = \'caffe_pb2\'\n  # @@protoc_insertion_point(class_scope:caffe.BatchNormParameter)\n  ))\n_sym_db.RegisterMessage(BatchNormParameter)\n\nBiasParameter = _reflection.GeneratedProtocolMessageType(\'BiasParameter\', (_message.Message,), dict(\n  DESCRIPTOR = _BIASPARAMETER,\n  __module__ = \'caffe_pb2\'\n  # @@protoc_insertion_point(class_scope:caffe.BiasParameter)\n  ))\n_sym_db.RegisterMessage(BiasParameter)\n\nContrastiveLossParameter = _reflection.GeneratedProtocolMessageType(\'ContrastiveLossParameter\', (_message.Message,), dict(\n  DESCRIPTOR = _CONTRASTIVELOSSPARAMETER,\n  __module__ = \'caffe_pb2\'\n  # @@protoc_insertion_point(class_scope:caffe.ContrastiveLossParameter)\n  ))\n_sym_db.RegisterMessage(ContrastiveLossParameter)\n\nConvolutionParameter = _reflection.GeneratedProtocolMessageType(\'ConvolutionParameter\', (_message.Message,), dict(\n  DESCRIPTOR = _CONVOLUTIONPARAMETER,\n  __module__ = \'caffe_pb2\'\n  # @@protoc_insertion_point(class_scope:caffe.ConvolutionParameter)\n  ))\n_sym_db.RegisterMessage(ConvolutionParameter)\n\nDataParameter = _reflection.GeneratedProtocolMessageType(\'DataParameter\', (_message.Message,), dict(\n  DESCRIPTOR = _DATAPARAMETER,\n  __module__ = \'caffe_pb2\'\n  # @@protoc_insertion_point(class_scope:caffe.DataParameter)\n  ))\n_sym_db.RegisterMessage(DataParameter)\n\nDropoutParameter = _reflection.GeneratedProtocolMessageType(\'DropoutParameter\', (_message.Message,), dict(\n  DESCRIPTOR = _DROPOUTPARAMETER,\n  __module__ = \'caffe_pb2\'\n  # @@protoc_insertion_point(class_scope:caffe.DropoutParameter)\n  ))\n_sym_db.RegisterMessage(DropoutParameter)\n\nDummyDataParameter = _reflection.GeneratedProtocolMessageType(\'DummyDataParameter\', (_message.Message,), dict(\n  DESCRIPTOR = _DUMMYDATAPARAMETER,\n  __module__ = \'caffe_pb2\'\n  # @@protoc_insertion_point(class_scope:caffe.DummyDataParameter)\n  ))\n_sym_db.RegisterMessage(DummyDataParameter)\n\nEltwiseParameter = _reflection.GeneratedProtocolMessageType(\'EltwiseParameter\', (_message.Message,), dict(\n  DESCRIPTOR = _ELTWISEPARAMETER,\n  __module__ = \'caffe_pb2\'\n  # @@protoc_insertion_point(class_scope:caffe.EltwiseParameter)\n  ))\n_sym_db.RegisterMessage(EltwiseParameter)\n\nELUParameter = _reflection.GeneratedProtocolMessageType(\'ELUParameter\', (_message.Message,), dict(\n  DESCRIPTOR = _ELUPARAMETER,\n  __module__ = \'caffe_pb2\'\n  # @@protoc_insertion_point(class_scope:caffe.ELUParameter)\n  ))\n_sym_db.RegisterMessage(ELUParameter)\n\nEmbedParameter = _reflection.GeneratedProtocolMessageType(\'EmbedParameter\', (_message.Message,), dict(\n  DESCRIPTOR = _EMBEDPARAMETER,\n  __module__ = \'caffe_pb2\'\n  # @@protoc_insertion_point(class_scope:caffe.EmbedParameter)\n  ))\n_sym_db.RegisterMessage(EmbedParameter)\n\nExpParameter = _reflection.GeneratedProtocolMessageType(\'ExpParameter\', (_message.Message,), dict(\n  DESCRIPTOR = _EXPPARAMETER,\n  __module__ = \'caffe_pb2\'\n  # @@protoc_insertion_point(class_scope:caffe.ExpParameter)\n  ))\n_sym_db.RegisterMessage(ExpParameter)\n\nFlattenParameter = _reflection.GeneratedProtocolMessageType(\'FlattenParameter\', (_message.Message,), dict(\n  DESCRIPTOR = _FLATTENPARAMETER,\n  __module__ = \'caffe_pb2\'\n  # @@protoc_insertion_point(class_scope:caffe.FlattenParameter)\n  ))\n_sym_db.RegisterMessage(FlattenParameter)\n\nHDF5DataParameter = _reflection.GeneratedProtocolMessageType(\'HDF5DataParameter\', (_message.Message,), dict(\n  DESCRIPTOR = _HDF5DATAPARAMETER,\n  __module__ = \'caffe_pb2\'\n  # @@protoc_insertion_point(class_scope:caffe.HDF5DataParameter)\n  ))\n_sym_db.RegisterMessage(HDF5DataParameter)\n\nHDF5OutputParameter = _reflection.GeneratedProtocolMessageType(\'HDF5OutputParameter\', (_message.Message,), dict(\n  DESCRIPTOR = _HDF5OUTPUTPARAMETER,\n  __module__ = \'caffe_pb2\'\n  # @@protoc_insertion_point(class_scope:caffe.HDF5OutputParameter)\n  ))\n_sym_db.RegisterMessage(HDF5OutputParameter)\n\nHingeLossParameter = _reflection.GeneratedProtocolMessageType(\'HingeLossParameter\', (_message.Message,), dict(\n  DESCRIPTOR = _HINGELOSSPARAMETER,\n  __module__ = \'caffe_pb2\'\n  # @@protoc_insertion_point(class_scope:caffe.HingeLossParameter)\n  ))\n_sym_db.RegisterMessage(HingeLossParameter)\n\nImageDataParameter = _reflection.GeneratedProtocolMessageType(\'ImageDataParameter\', (_message.Message,), dict(\n  DESCRIPTOR = _IMAGEDATAPARAMETER,\n  __module__ = \'caffe_pb2\'\n  # @@protoc_insertion_point(class_scope:caffe.ImageDataParameter)\n  ))\n_sym_db.RegisterMessage(ImageDataParameter)\n\nInfogainLossParameter = _reflection.GeneratedProtocolMessageType(\'InfogainLossParameter\', (_message.Message,), dict(\n  DESCRIPTOR = _INFOGAINLOSSPARAMETER,\n  __module__ = \'caffe_pb2\'\n  # @@protoc_insertion_point(class_scope:caffe.InfogainLossParameter)\n  ))\n_sym_db.RegisterMessage(InfogainLossParameter)\n\nInnerProductParameter = _reflection.GeneratedProtocolMessageType(\'InnerProductParameter\', (_message.Message,), dict(\n  DESCRIPTOR = _INNERPRODUCTPARAMETER,\n  __module__ = \'caffe_pb2\'\n  # @@protoc_insertion_point(class_scope:caffe.InnerProductParameter)\n  ))\n_sym_db.RegisterMessage(InnerProductParameter)\n\nLogParameter = _reflection.GeneratedProtocolMessageType(\'LogParameter\', (_message.Message,), dict(\n  DESCRIPTOR = _LOGPARAMETER,\n  __module__ = \'caffe_pb2\'\n  # @@protoc_insertion_point(class_scope:caffe.LogParameter)\n  ))\n_sym_db.RegisterMessage(LogParameter)\n\nLRNParameter = _reflection.GeneratedProtocolMessageType(\'LRNParameter\', (_message.Message,), dict(\n  DESCRIPTOR = _LRNPARAMETER,\n  __module__ = \'caffe_pb2\'\n  # @@protoc_insertion_point(class_scope:caffe.LRNParameter)\n  ))\n_sym_db.RegisterMessage(LRNParameter)\n\nMemoryDataParameter = _reflection.GeneratedProtocolMessageType(\'MemoryDataParameter\', (_message.Message,), dict(\n  DESCRIPTOR = _MEMORYDATAPARAMETER,\n  __module__ = \'caffe_pb2\'\n  # @@protoc_insertion_point(class_scope:caffe.MemoryDataParameter)\n  ))\n_sym_db.RegisterMessage(MemoryDataParameter)\n\nMVNParameter = _reflection.GeneratedProtocolMessageType(\'MVNParameter\', (_message.Message,), dict(\n  DESCRIPTOR = _MVNPARAMETER,\n  __module__ = \'caffe_pb2\'\n  # @@protoc_insertion_point(class_scope:caffe.MVNParameter)\n  ))\n_sym_db.RegisterMessage(MVNParameter)\n\nPoolingParameter = _reflection.GeneratedProtocolMessageType(\'PoolingParameter\', (_message.Message,), dict(\n  DESCRIPTOR = _POOLINGPARAMETER,\n  __module__ = \'caffe_pb2\'\n  # @@protoc_insertion_point(class_scope:caffe.PoolingParameter)\n  ))\n_sym_db.RegisterMessage(PoolingParameter)\n\nPowerParameter = _reflection.GeneratedProtocolMessageType(\'PowerParameter\', (_message.Message,), dict(\n  DESCRIPTOR = _POWERPARAMETER,\n  __module__ = \'caffe_pb2\'\n  # @@protoc_insertion_point(class_scope:caffe.PowerParameter)\n  ))\n_sym_db.RegisterMessage(PowerParameter)\n\nPythonParameter = _reflection.GeneratedProtocolMessageType(\'PythonParameter\', (_message.Message,), dict(\n  DESCRIPTOR = _PYTHONPARAMETER,\n  __module__ = \'caffe_pb2\'\n  # @@protoc_insertion_point(class_scope:caffe.PythonParameter)\n  ))\n_sym_db.RegisterMessage(PythonParameter)\n\nReductionParameter = _reflection.GeneratedProtocolMessageType(\'ReductionParameter\', (_message.Message,), dict(\n  DESCRIPTOR = _REDUCTIONPARAMETER,\n  __module__ = \'caffe_pb2\'\n  # @@protoc_insertion_point(class_scope:caffe.ReductionParameter)\n  ))\n_sym_db.RegisterMessage(ReductionParameter)\n\nReLUParameter = _reflection.GeneratedProtocolMessageType(\'ReLUParameter\', (_message.Message,), dict(\n  DESCRIPTOR = _RELUPARAMETER,\n  __module__ = \'caffe_pb2\'\n  # @@protoc_insertion_point(class_scope:caffe.ReLUParameter)\n  ))\n_sym_db.RegisterMessage(ReLUParameter)\n\nReshapeParameter = _reflection.GeneratedProtocolMessageType(\'ReshapeParameter\', (_message.Message,), dict(\n  DESCRIPTOR = _RESHAPEPARAMETER,\n  __module__ = \'caffe_pb2\'\n  # @@protoc_insertion_point(class_scope:caffe.ReshapeParameter)\n  ))\n_sym_db.RegisterMessage(ReshapeParameter)\n\nScaleParameter = _reflection.GeneratedProtocolMessageType(\'ScaleParameter\', (_message.Message,), dict(\n  DESCRIPTOR = _SCALEPARAMETER,\n  __module__ = \'caffe_pb2\'\n  # @@protoc_insertion_point(class_scope:caffe.ScaleParameter)\n  ))\n_sym_db.RegisterMessage(ScaleParameter)\n\nSigmoidParameter = _reflection.GeneratedProtocolMessageType(\'SigmoidParameter\', (_message.Message,), dict(\n  DESCRIPTOR = _SIGMOIDPARAMETER,\n  __module__ = \'caffe_pb2\'\n  # @@protoc_insertion_point(class_scope:caffe.SigmoidParameter)\n  ))\n_sym_db.RegisterMessage(SigmoidParameter)\n\nSliceParameter = _reflection.GeneratedProtocolMessageType(\'SliceParameter\', (_message.Message,), dict(\n  DESCRIPTOR = _SLICEPARAMETER,\n  __module__ = \'caffe_pb2\'\n  # @@protoc_insertion_point(class_scope:caffe.SliceParameter)\n  ))\n_sym_db.RegisterMessage(SliceParameter)\n\nSoftmaxParameter = _reflection.GeneratedProtocolMessageType(\'SoftmaxParameter\', (_message.Message,), dict(\n  DESCRIPTOR = _SOFTMAXPARAMETER,\n  __module__ = \'caffe_pb2\'\n  # @@protoc_insertion_point(class_scope:caffe.SoftmaxParameter)\n  ))\n_sym_db.RegisterMessage(SoftmaxParameter)\n\nTanHParameter = _reflection.GeneratedProtocolMessageType(\'TanHParameter\', (_message.Message,), dict(\n  DESCRIPTOR = _TANHPARAMETER,\n  __module__ = \'caffe_pb2\'\n  # @@protoc_insertion_point(class_scope:caffe.TanHParameter)\n  ))\n_sym_db.RegisterMessage(TanHParameter)\n\nTileParameter = _reflection.GeneratedProtocolMessageType(\'TileParameter\', (_message.Message,), dict(\n  DESCRIPTOR = _TILEPARAMETER,\n  __module__ = \'caffe_pb2\'\n  # @@protoc_insertion_point(class_scope:caffe.TileParameter)\n  ))\n_sym_db.RegisterMessage(TileParameter)\n\nThresholdParameter = _reflection.GeneratedProtocolMessageType(\'ThresholdParameter\', (_message.Message,), dict(\n  DESCRIPTOR = _THRESHOLDPARAMETER,\n  __module__ = \'caffe_pb2\'\n  # @@protoc_insertion_point(class_scope:caffe.ThresholdParameter)\n  ))\n_sym_db.RegisterMessage(ThresholdParameter)\n\nWindowDataParameter = _reflection.GeneratedProtocolMessageType(\'WindowDataParameter\', (_message.Message,), dict(\n  DESCRIPTOR = _WINDOWDATAPARAMETER,\n  __module__ = \'caffe_pb2\'\n  # @@protoc_insertion_point(class_scope:caffe.WindowDataParameter)\n  ))\n_sym_db.RegisterMessage(WindowDataParameter)\n\nSPPParameter = _reflection.GeneratedProtocolMessageType(\'SPPParameter\', (_message.Message,), dict(\n  DESCRIPTOR = _SPPPARAMETER,\n  __module__ = \'caffe_pb2\'\n  # @@protoc_insertion_point(class_scope:caffe.SPPParameter)\n  ))\n_sym_db.RegisterMessage(SPPParameter)\n\nV1LayerParameter = _reflection.GeneratedProtocolMessageType(\'V1LayerParameter\', (_message.Message,), dict(\n  DESCRIPTOR = _V1LAYERPARAMETER,\n  __module__ = \'caffe_pb2\'\n  # @@protoc_insertion_point(class_scope:caffe.V1LayerParameter)\n  ))\n_sym_db.RegisterMessage(V1LayerParameter)\n\nV0LayerParameter = _reflection.GeneratedProtocolMessageType(\'V0LayerParameter\', (_message.Message,), dict(\n  DESCRIPTOR = _V0LAYERPARAMETER,\n  __module__ = \'caffe_pb2\'\n  # @@protoc_insertion_point(class_scope:caffe.V0LayerParameter)\n  ))\n_sym_db.RegisterMessage(V0LayerParameter)\n\nPReLUParameter = _reflection.GeneratedProtocolMessageType(\'PReLUParameter\', (_message.Message,), dict(\n  DESCRIPTOR = _PRELUPARAMETER,\n  __module__ = \'caffe_pb2\'\n  # @@protoc_insertion_point(class_scope:caffe.PReLUParameter)\n  ))\n_sym_db.RegisterMessage(PReLUParameter)\n\n\n_BLOBSHAPE.fields_by_name[\'dim\'].has_options = True\n_BLOBSHAPE.fields_by_name[\'dim\']._options = _descriptor._ParseOptions(descriptor_pb2.FieldOptions(), _b(\'\\020\\001\'))\n_BLOBPROTO.fields_by_name[\'data\'].has_options = True\n_BLOBPROTO.fields_by_name[\'data\']._options = _descriptor._ParseOptions(descriptor_pb2.FieldOptions(), _b(\'\\020\\001\'))\n_BLOBPROTO.fields_by_name[\'diff\'].has_options = True\n_BLOBPROTO.fields_by_name[\'diff\']._options = _descriptor._ParseOptions(descriptor_pb2.FieldOptions(), _b(\'\\020\\001\'))\n_BLOBPROTO.fields_by_name[\'double_data\'].has_options = True\n_BLOBPROTO.fields_by_name[\'double_data\']._options = _descriptor._ParseOptions(descriptor_pb2.FieldOptions(), _b(\'\\020\\001\'))\n_BLOBPROTO.fields_by_name[\'double_diff\'].has_options = True\n_BLOBPROTO.fields_by_name[\'double_diff\']._options = _descriptor._ParseOptions(descriptor_pb2.FieldOptions(), _b(\'\\020\\001\'))\n# @@protoc_insertion_point(module_scope)'"
semseg/loss.py,1,"b'# -*- coding: utf-8 -*-\nimport torch.nn.functional as F\n\ndef cross_entropy2d(input, target, weight=None, size_average=True):\n    n, c, h, w = input.size()\n    nt, ht, wt = target.size()\n\n    # Handle inconsistent size between input and target\n    if h > ht and w > wt:  # upsample labels\n        target = target.unsequeeze(1)\n        target = F.upsample(target, size=(h, w), mode=""nearest"")\n        target = target.sequeeze(1)\n    elif h < ht and w < wt:  # upsample images\n        input = F.upsample(input, size=(ht, wt), mode=""bilinear"")\n    elif h != ht and w != wt:\n        raise Exception(""Only support upsampling"")\n\n    input = input.transpose(1, 2).transpose(2, 3).contiguous().view(-1, c)\n    target = target.view(-1)\n    loss = F.cross_entropy(input, target, weight=weight, size_average=size_average, ignore_index=250)\n    return loss\n'"
semseg/metrics.py,0,"b'# -*- coding: utf-8 -*-\n# Originally written by wkentaro\n# https://github.com/wkentaro/pytorch-fcn/blob/master/torchfcn/utils.py\n\nimport numpy as np\n\n# \xe7\xb1\xbb\xe5\x88\xab\xe4\xb8\xba0-n_class-1\xef\xbc\x8c\xe7\xbb\x9f\xe8\xae\xa1bincount\ndef _fast_hist(label_true, label_pred, n_class):\n    mask = (label_true >= 0) & (label_true < n_class)\n    hist = np.bincount(n_class * label_true[mask].astype(int) + label_pred[mask], minlength=n_class**2).reshape(n_class, n_class)\n    return hist\n\n# \xe8\xaf\x84\xe4\xbc\xb0\xe6\x8c\x87\xe6\xa0\x87\xef\xbc\x8c\xe8\xbe\x93\xe5\x85\xa5\xe4\xb8\xba\xe6\xa0\x87\xe7\xad\xbe\xe7\x9c\x9f\xe5\xae\x9e\xe5\x80\xbc\xef\xbc\x8c\xe6\xa0\x87\xe7\xad\xbe\xe9\xa2\x84\xe6\xb5\x8b\xe5\x80\xbc\xe5\x92\x8c\xe7\xb1\xbb\xe4\xb8\xaa\xe6\x95\xb0\n# \xe8\xbe\x93\xe5\x85\xa5scores\xe4\xb8\xad\xe7\x9a\x84\xe6\x95\xb0\xe6\x8d\xae\xe6\x98\xaf[label1, label2, ...] [pred1, pred2, ...]\xef\xbc\x8c\xe8\x80\x8c\xe4\xb8\x94label.shape=(1, height, weight)\xef\xbc\x8cpred.shape=(1, height, weight)\ndef scores(label_trues, label_preds, n_class):\n    """"""Returns accuracy score evaluation result.\n      - overall accuracy\n      - mean accuracy\n      - mean IU\n      - fwavacc\n    """"""\n    # \xe7\x9b\xb4\xe6\x96\xb9\xe5\x9b\xbe\xef\xbc\x8c\xe9\xa2\x84\xe6\xb5\x8b\xe7\x9a\x84\xe7\xb1\xbb\xe5\x88\xab\xe5\x92\x8c\xe7\x9c\x9f\xe5\xae\x9e\xe7\x9a\x84\xe7\xb1\xbb\xe5\x88\xab\xe7\x9a\x84\xe7\xbb\x9f\xe8\xae\xa1\xef\xbc\x8c\xe7\x9b\xb8\xe5\xbd\x93\xe4\xba\x8e\xe6\xb7\xb7\xe6\xb7\x86\xe7\x9f\xa9\xe9\x98\xb5\n    hist = np.zeros((n_class, n_class))\n    # \xe5\xbe\xaa\xe7\x8e\xaf\xe6\xb7\xbb\xe5\x8a\xa0\xe6\xaf\x8f\xe4\xb8\x80\xe4\xb8\xaa\xe6\xa0\xb7\xe6\x9c\xac\xe7\x9a\x84\xe6\xb7\xb7\xe6\xb7\x86\xe7\x9f\xa9\xe9\x98\xb5\n    for lt, lp in zip(label_trues, label_preds):\n        hist += _fast_hist(lt.flatten(), lp.flatten(), n_class)\n    acc = np.diag(hist).sum() / hist.sum()\n    acc_cls = np.diag(hist) / hist.sum(axis=1)\n    acc_cls = np.nanmean(acc_cls)\n    iu = np.diag(hist) / (hist.sum(axis=1) + hist.sum(axis=0) - np.diag(hist))\n    mean_iu = np.nanmean(iu)\n    freq = hist.sum(axis=1) / hist.sum()\n    fwavacc = (freq[freq > 0] * iu[freq > 0]).sum()\n    cls_iu = dict(zip(range(n_class), iu))\n\n    return {\'Overall Acc : \\t\': round(acc, 2),\n            \'Mean Acc : \\t\': round(acc_cls, 2),\n            \'FreqW Acc : \\t\': round(fwavacc, 2),\n            \'Mean IoU : \\t\': round(mean_iu, 2),}, cls_iu\n'"
semseg/pytorch_modelsize.py,3,"b""# -*- coding: utf-8 -*-\n\n# code from https://github.com/jacobkimmel/pytorch_modelsize/blob/master/pytorch_modelsize.py\n\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\nimport numpy as np\n\n\nclass SizeEstimator(object):\n\n    def __init__(self, model, input_size=(1, 1, 32, 32), bits=32):\n        '''\n        Estimates the size of PyTorch models in memory\n        for a given input size\n        '''\n        self.model = model\n        self.input_size = input_size\n        self.bits = 32\n        return\n\n    def get_parameter_sizes(self):\n        '''Get sizes of all parameters in `model`'''\n        mods = list(self.model.modules())\n        sizes = []\n\n        for i in range(1, len(mods)):\n            m = mods[i]\n            p = list(m.parameters())\n            for j in range(len(p)):\n                sizes.append(np.array(p[j].size()))\n\n        self.param_sizes = sizes\n        return\n\n    def get_output_sizes(self):\n        '''Run sample input through each layer to get output sizes'''\n        input_ = Variable(torch.FloatTensor(*self.input_size), volatile=True)\n        mods = list(self.model.modules())\n        out_sizes = []\n        for i in range(1, len(mods)):\n            m = mods[i]\n            out = m(input_)\n            out_sizes.append(np.array(out.size()))\n            input_ = out\n\n        self.out_sizes = out_sizes\n        return\n\n    def calc_param_bits(self):\n        '''Calculate total number of bits to store `model` parameters'''\n        total_bits = 0\n        for i in range(len(self.param_sizes)):\n            s = self.param_sizes[i]\n            bits = np.prod(np.array(s)) * self.bits\n            total_bits += bits\n        self.param_bits = total_bits\n        return\n\n    def calc_forward_backward_bits(self):\n        '''Calculate bits to store forward and backward pass'''\n        total_bits = 0\n        for i in range(len(self.out_sizes)):\n            s = self.out_sizes[i]\n            bits = np.prod(np.array(s)) * self.bits\n            total_bits += bits\n        # multiply by 2 for both forward AND backward\n        self.forward_backward_bits = (total_bits * 2)\n        return\n\n    def calc_input_bits(self):\n        '''Calculate bits to store input'''\n        self.input_bits = np.prod(np.array(self.input_size)) * self.bits\n        return\n\n    def estimate_size(self):\n        '''Estimate model size in memory in megabytes and bits'''\n        self.get_parameter_sizes()\n        self.get_output_sizes()\n        self.calc_param_bits()\n        self.calc_forward_backward_bits()\n        self.calc_input_bits()\n        total = self.param_bits + self.forward_backward_bits + self.input_bits\n\n        total_megabytes = (total / 8) / (1024 ** 2)\n        return total_megabytes, total\n"""
semseg/schedulers.py,1,"b""# -*- coding: utf-8 -*-\n# code is from [schedulers.py](https://github.com/meetshah1995/pytorch-semseg/blob/master/ptsemseg/schedulers/schedulers.py)\n\nimport torch\n\nfrom torch.optim.lr_scheduler import _LRScheduler\n\nclass ConstantLR(_LRScheduler):\n    def __init__(self, optimizer, last_epoch=-1):\n        super(ConstantLR, self).__init__(optimizer, last_epoch)\n\n    def get_lr(self):\n        return [base_lr for base_lr in self.base_lrs]\n\nclass PolynomialLR(_LRScheduler):\n    def __init__(self, optimizer, max_iter, power=0.9, last_epoch=-1):\n        self.max_iter = max_iter\n        self.power = power\n        super(PolynomialLR, self).__init__(optimizer, last_epoch)\n\n    def get_lr(self):\n        factor = (1 - self.last_epoch / float(self.max_iter)) ** self.power\n        return [base_lr * factor for base_lr in self.base_lrs]\n\ndef get_lr(optimizer):\n    for param_group in optimizer.param_groups:\n        return param_group['lr']\n"""
semseg/visualize.py,4,"b'#!/usr/bin/python\n# -*- coding: UTF-8 -*-\n\nfrom graphviz import Digraph\nimport torch\nfrom torch.autograd import Variable\nimport numpy as np\n\nfrom semseg.modelloader.fcn import fcn32s\n\n# \xe7\xbd\x91\xe7\xbb\x9c\xe6\xa8\xa1\xe5\x9e\x8b\xe7\xbb\x98\xe5\x9b\xbe\xef\xbc\x8c\xe7\x94\x9f\xe6\x88\x90pytorch autograd\xe5\x9b\xbe\xe8\xa1\xa8\xe7\xa4\xba\xef\xbc\x8c\xe8\x93\x9d\xe8\x89\xb2\xe8\x8a\x82\xe7\x82\xb9\xe8\xa1\xa8\xe7\xa4\xba\xe8\xa6\x81\xe6\xb1\x82grad\xe6\xa2\xaf\xe5\xba\xa6\xe7\x9a\x84\xe5\x8f\x98\xe9\x87\x8f\xef\xbc\x8c\xe9\xbb\x84\xe8\x89\xb2\xe8\xa1\xa8\xe7\xa4\xba\xe5\x9c\xa8\xe5\x8f\x8d\xe5\x90\x91\xe4\xbc\xa0\xe6\x92\xad\xe7\x9a\x84\xe5\xbc\xa0\xe9\x87\x8f\ndef make_dot(var, params=None):\n    """""" Produces Graphviz representation of PyTorch autograd graph\n    Blue nodes are the Variables that require grad, orange are Tensors\n    saved for backward in torch.autograd.Function\n    Args:\n        var: output Variable\n        params: dict of (name, Variable) to add names to node that\n            require grad (TODO: make optional)\n    """"""\n    if params is not None:\n        assert isinstance(params.values()[0], Variable)\n        param_map = {id(v): k for k, v in params.items()}\n\n    node_attr = dict(style=\'filled\',\n                     shape=\'box\',\n                     align=\'left\',\n                     fontsize=\'12\',\n                     ranksep=\'0.1\',\n                     height=\'0.2\')\n    dot = Digraph(node_attr=node_attr, graph_attr=dict(size=""12,12""))\n    seen = set()\n\n    def size_to_str(size):\n        return \'(\' + \', \'.join([\'%d\' % v for v in size]) + \')\'\n\n    def add_nodes(var_grad):\n        if var_grad not in seen:\n            if torch.is_tensor(var_grad):\n                dot.node(str(id(var_grad)), size_to_str(var_grad.size()), fillcolor=\'orange\')\n            elif hasattr(var_grad, \'variable\'):\n                u = var_grad.variable\n                name = param_map[id(u)] if params is not None else \'\'\n                node_name = \'%s\\n %s\' % (name, size_to_str(u.size()))\n                dot.node(str(id(var_grad)), node_name, fillcolor=\'lightblue\')\n            else:\n                dot.node(str(id(var_grad)), str(type(var_grad).__name__))\n            seen.add(var_grad)\n            if hasattr(var_grad, \'next_functions\'):\n                for u in var_grad.next_functions:\n                    if u[0] is not None:\n                        dot.edge(str(id(u[0])), str(id(var_grad)))\n                        add_nodes(u[0])\n            if hasattr(var_grad, \'saved_tensors\'):\n                for t in var_grad.saved_tensors:\n                    dot.edge(str(id(t)), str(id(var_grad)))\n                    add_nodes(t)\n    add_nodes(var.grad_fn)\n    return dot\n\ndef main():\n    n_classes = 21\n    model = fcn32s(n_classes=n_classes)\n    x = Variable(torch.randn(1, 3, 360, 480))\n    pred = model(x)\n    g = make_dot(pred)\n    # print(g)\n    g.render(\'model_vis.gv\', view=True)\n\n\nif __name__ == \'__main__\':\n    main()\n'"
semseg/yoloLoss.py,18,"b""#encoding:utf-8\r\n#\r\n#created by xiongzihua 2017.12.26\r\n#\r\nimport torch\r\nimport torch.nn as nn\r\nimport torch.nn.functional as F\r\nfrom torch.autograd import Variable\r\n\r\nclass yoloLoss(nn.Module):\r\n    def __init__(self,S,B, C, l_coord,l_noobj, use_gpu):\r\n        super(yoloLoss,self).__init__()\r\n        self.S = S\r\n        self.B = B\r\n        self.C = C\r\n        self.l_coord = l_coord\r\n        self.l_noobj = l_noobj\r\n        self.use_gpu = use_gpu\r\n        self.out_tensor_shape = self.B * 5+self.C\r\n\r\n    def compute_iou(self, box1, box2):\r\n        '''Compute the intersection over union of two set of boxes, each box is [x1,y1,x2,y2].\r\n        Args:\r\n          box1: (tensor) bounding boxes, sized [N,4].\r\n          box2: (tensor) bounding boxes, sized [M,4].\r\n        Return:\r\n          (tensor) iou, sized [N,M].\r\n        '''\r\n        N = box1.size(0)\r\n        M = box2.size(0)\r\n\r\n        lt = torch.max(\r\n            box1[:,:2].unsqueeze(1).expand(N,M,2),  # [N,2] -> [N,1,2] -> [N,M,2]\r\n            box2[:,:2].unsqueeze(0).expand(N,M,2),  # [M,2] -> [1,M,2] -> [N,M,2]\r\n        )\r\n\r\n        rb = torch.min(\r\n            box1[:,2:].unsqueeze(1).expand(N,M,2),  # [N,2] -> [N,1,2] -> [N,M,2]\r\n            box2[:,2:].unsqueeze(0).expand(N,M,2),  # [M,2] -> [1,M,2] -> [N,M,2]\r\n        )\r\n\r\n        wh = rb - lt  # [N,M,2]\r\n        wh[wh<0] = 0  # clip at 0\r\n        inter = wh[:,:,0] * wh[:,:,1]  # [N,M]\r\n\r\n        area1 = (box1[:,2]-box1[:,0]) * (box1[:,3]-box1[:,1])  # [N,]\r\n        area2 = (box2[:,2]-box2[:,0]) * (box2[:,3]-box2[:,1])  # [M,]\r\n        area1 = area1.unsqueeze(1).expand_as(inter)  # [N,] -> [N,1] -> [N,M]\r\n        area2 = area2.unsqueeze(0).expand_as(inter)  # [M,] -> [1,M] -> [N,M]\r\n\r\n        iou = inter / (area1 + area2 - inter)\r\n        return iou\r\n    def forward(self,pred_tensor,target_tensor):\r\n        '''\r\n        pred_tensor: (tensor) size(batchsize,S,S,Bx5+20=30) [x,y,w,h,c]\r\n        target_tensor: (tensor) size(batchsize,S,S,30)\r\n        '''\r\n        N = pred_tensor.size()[0]\r\n        coo_mask = target_tensor[:,:,:,4] > 0\r\n        noo_mask = target_tensor[:,:,:,4] == 0\r\n        coo_mask = coo_mask.unsqueeze(-1).expand_as(target_tensor)\r\n        noo_mask = noo_mask.unsqueeze(-1).expand_as(target_tensor)\r\n\r\n        coo_pred = pred_tensor[coo_mask].view(-1,self.out_tensor_shape )\r\n        box_pred = coo_pred[:,:10].contiguous().view(-1,5) #box[x1,y1,w1,h1,c1]\r\n        class_pred = coo_pred[:,10:]                       #[x2,y2,w2,h2,c2]\r\n        \r\n        coo_target = target_tensor[coo_mask].view(-1,self.out_tensor_shape )\r\n        box_target = coo_target[:,:10].contiguous().view(-1,5)\r\n        class_target = coo_target[:,10:]\r\n\r\n        # compute not contain obj loss\r\n        noo_pred = pred_tensor[noo_mask].view(-1,self.out_tensor_shape )\r\n        noo_target = target_tensor[noo_mask].view(-1,self.out_tensor_shape )\r\n        if self.use_gpu:\r\n            noo_pred_mask = torch.cuda.ByteTensor(noo_pred.size())\r\n        else:\r\n            noo_pred_mask = torch.ByteTensor(noo_pred.size())\r\n        noo_pred_mask.zero_()\r\n        noo_pred_mask[:,4]=1;noo_pred_mask[:,9]=1\r\n        noo_pred_c = noo_pred[noo_pred_mask] #noo pred\xe5\x8f\xaa\xe9\x9c\x80\xe8\xa6\x81\xe8\xae\xa1\xe7\xae\x97 c \xe7\x9a\x84\xe6\x8d\x9f\xe5\xa4\xb1 size[-1,2]\r\n        noo_target_c = noo_target[noo_pred_mask]\r\n        nooobj_loss = F.mse_loss(noo_pred_c,noo_target_c,size_average=False)\r\n\r\n        #compute contain obj loss\r\n        if self.use_gpu:\r\n            coo_response_mask = torch.cuda.ByteTensor(box_target.size())\r\n        else:\r\n            coo_response_mask = torch.ByteTensor(box_target.size())\r\n\r\n        coo_response_mask.zero_()\r\n        if self.use_gpu:\r\n            coo_not_response_mask = torch.cuda.ByteTensor(box_target.size())\r\n        else:\r\n            coo_not_response_mask = torch.ByteTensor(box_target.size())\r\n        coo_not_response_mask.zero_()\r\n        if self.use_gpu:\r\n            box_target_iou = torch.zeros(box_target.size()).cuda()\r\n        else:\r\n            box_target_iou = torch.zeros(box_target.size())\r\n\r\n        for i in range(0,box_target.size()[0],2): #choose the best iou box\r\n            box1 = box_pred[i:i+2]\r\n            box1_xyxy = Variable(torch.FloatTensor(box1.size()))\r\n            box1_xyxy[:,:2] = box1[:,:2]/14. -0.5*box1[:,2:4]\r\n            box1_xyxy[:,2:4] = box1[:,:2]/14. +0.5*box1[:,2:4]\r\n            box2 = box_target[i].view(-1,5)\r\n            box2_xyxy = Variable(torch.FloatTensor(box2.size()))\r\n            box2_xyxy[:,:2] = box2[:,:2]/14. -0.5*box2[:,2:4]\r\n            box2_xyxy[:,2:4] = box2[:,:2]/14. +0.5*box2[:,2:4]\r\n            iou = self.compute_iou(box1_xyxy[:,:4],box2_xyxy[:,:4]) #[2,1]\r\n            max_iou,max_index = iou.max(0)\r\n            if self.use_gpu:\r\n                max_index = max_index.data.cuda()\r\n            else:\r\n                max_index = max_index.data\r\n\r\n            coo_response_mask[i+max_index]=1\r\n            coo_not_response_mask[i+1-max_index]=1\r\n\r\n            #####\r\n            # we want the confidence score to equal the\r\n            # intersection over union (IOU) between the predicted box\r\n            # and the ground truth\r\n            #####\r\n            if self.use_gpu:\r\n                box_target_iou[i+max_index,torch.LongTensor([4]).cuda()] = (max_iou).data.cuda()\r\n            else:\r\n                box_target_iou[i+max_index,torch.LongTensor([4])] = (max_iou).data\r\n\r\n        if self.use_gpu:\r\n            box_target_iou = Variable(box_target_iou).cuda()\r\n        else:\r\n            box_target_iou = Variable(box_target_iou)\r\n        #1.response loss\r\n        box_pred_response = box_pred[coo_response_mask].view(-1,5)\r\n        box_target_response_iou = box_target_iou[coo_response_mask].view(-1,5)\r\n        box_target_response = box_target[coo_response_mask].view(-1,5)\r\n        contain_loss = F.mse_loss(box_pred_response[:,4],box_target_response_iou[:,4],size_average=False)\r\n        loc_loss = F.mse_loss(box_pred_response[:,:2],box_target_response[:,:2],size_average=False) + F.mse_loss(torch.sqrt(box_pred_response[:,2:4]),torch.sqrt(box_target_response[:,2:4]),size_average=False)\r\n        #2.not response loss\r\n        box_pred_not_response = box_pred[coo_not_response_mask].view(-1,5)\r\n        box_target_not_response = box_target[coo_not_response_mask].view(-1,5)\r\n        box_target_not_response[:,4]= 0\r\n        #not_contain_loss = F.mse_loss(box_pred_response[:,4],box_target_response[:,4],size_average=False)\r\n        \r\n        #I believe this bug is simply a typo\r\n        not_contain_loss = F.mse_loss(box_pred_not_response[:,4], box_target_not_response[:,4],size_average=False)\r\n\r\n        #3.class loss\r\n        class_loss = F.mse_loss(class_pred,class_target,size_average=False)\r\n\r\n        return (self.l_coord*loc_loss + 2*contain_loss + not_contain_loss + self.l_noobj*nooobj_loss + class_loss)/N\r\n"""
semseg/dataloader/__init__.py,0,b''
semseg/dataloader/ade20k_loader.py,3,"b'#!/usr/bin/python\n# -*- coding: UTF-8 -*-\n\nimport os\nimport collections\nimport torch\nimport scipy.misc as m\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nfrom torch.utils import data\nimport glob\nimport numpy as np\n\nclass ade20kLoader(data.Dataset):\n    def __init__(self, root, split=""training"", is_transform=False, img_size=512):\n        self.root = root\n        self.split = split\n        self.is_transform = is_transform\n        self.img_size = img_size if isinstance(img_size, tuple) else (img_size, img_size)\n        self.mean = np.array([104.00699, 116.66877, 122.67892])\n        self.n_classes = 13\n        self.image_files = collections.defaultdict(list)\n        self.label_files = collections.defaultdict(list)\n        self.image_files[self.split] = glob.glob(os.path.join(root, \'images\', self.split, \'*\', \'*.jpg\'))\n        self.label_files[self.split] = glob.glob(os.path.join(root, \'images\', self.split, \'*\', \'*_seg.png\'))\n        # \xe6\x96\x87\xe4\xbb\xb6\xe5\x90\x8d\xe9\x9c\x80\xe7\x9b\xb8\xe5\x90\x8c\n        self.image_files[self.split].sort()\n        self.label_files[self.split].sort()\n        # print(len(self.image_files[self.split]))\n        # print(len(self.label_files[self.split]))\n        assert len(self.image_files[self.split]) == len(self.label_files[self.split])\n\n    def __len__(self):\n        return len(self.image_files[self.split])\n\n    def __getitem__(self, index):\n        img_path = self.image_files[self.split][index]\n        lbl_path = self.label_files[self.split][index]\n        # print(\'img_path:\', img_path)\n        # print(\'lbl_path:\', lbl_path)\n\n        img = Image.open(img_path)\n        img = np.array(img, dtype=np.uint8)\n\n        lbl = Image.open(lbl_path)\n        lbl = np.array(lbl, dtype=np.int32)\n\n        if self.is_transform:\n            img, lbl = self.transform(img, lbl)\n\n        return img, lbl\n\n    # \xe8\xbd\xac\xe6\x8d\xa2HWC\xe4\xb8\xbaCHW\n    def transform(self, img, lbl):\n        img = img[:, :, ::-1]\n        img = img.astype(np.float64)\n        img -= self.mean\n        img = m.imresize(img, (self.img_size[0], self.img_size[1]))\n        img = img.astype(float) / 255.0\n        # HWC -> CHW\n        img = img.transpose(2, 0, 1)\n\n        lbl = self.encode_segmap(lbl)\n        lbl = lbl.astype(float)\n        lbl = m.imresize(lbl, (self.img_size[0], self.img_size[1]), \'nearest\', mode=\'F\')\n        lbl = lbl.astype(int)\n        # print(img)\n        # print(lbl)\n\n        img = torch.from_numpy(img).float()\n        lbl = torch.from_numpy(lbl).long()\n        return img, lbl\n\n    def encode_segmap(self, mask):\n        # Refer : http://groups.csail.mit.edu/vision/datasets/ADE20K/code/loadAde20K.m\n        mask = mask.astype(int)\n        label_mask = np.zeros((mask.shape[0], mask.shape[1]))\n        label_mask = (mask[:, :, 0] / 10.0) * 256 + mask[:, :, 1]\n        return np.array(label_mask, dtype=np.uint8)\n\n    def decode_segmap(self, temp, plot=False):\n        # TODO:(@meetshah1995)\n        # Verify that the color mapping is 1-to-1\n        r = temp.copy()\n        g = temp.copy()\n        b = temp.copy()\n        for l in range(0, self.n_classes):\n            r[temp == l] = 10 * (l % 10)\n            g[temp == l] = l\n            b[temp == l] = 0\n\n        rgb = np.zeros((temp.shape[0], temp.shape[1], 3))\n        rgb[:, :, 0] = (r / 255.0)\n        rgb[:, :, 1] = (g / 255.0)\n        rgb[:, :, 2] = (b / 255.0)\n        if plot:\n            plt.imshow(rgb)\n            plt.show()\n        else:\n            return rgb\n\nif __name__ == \'__main__\':\n    HOME_PATH = os.path.expanduser(\'~\')\n    local_path = os.path.join(HOME_PATH, \'Data/ADE20K_2016_07_26\')\n    dst = ade20kLoader(local_path, is_transform=True)\n    trainloader = data.DataLoader(dst, batch_size=4)\n    for i, (imgs, labels) in enumerate(trainloader):\n        # print(i)\n        # print(imgs.shape)\n        # print(labels.shape)\n        if i == 0:\n            img = imgs[0, :, :, :]\n            img = img.numpy()\n            # print(img.shape)\n            img = np.transpose(img, (1, 2, 0))\n            plt.subplot(121)\n            plt.imshow(img)\n            plt.subplot(122)\n            plt.imshow(dst.decode_segmap(labels.numpy()[0]))\n            plt.show()\n        break'"
semseg/dataloader/camvid_loader.py,5,"b'# -*- coding: utf-8 -*-\nimport torch\nimport os\nimport collections\nimport random\n\nimport cv2\nimport numpy as np\nimport scipy.misc as m\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nfrom torch.utils import data\nfrom torchvision import transforms\nimport glob\n\nfrom semseg.dataloader.utils import Compose, RandomHorizontallyFlip, RandomRotate, RandomSized, RandomCrop\n\n\nclass camvidLoader(data.Dataset):\n    def __init__(self, root, split=""train"", is_transform=False, is_augment=False):\n        self.root = root\n        self.split = split\n        self.img_size = (360, 480) # (h, w)\n        self.is_transform = is_transform\n        self.mean = np.array([104.00699, 116.66877, 122.67892])\n        self.n_classes = 12\n        self.files = collections.defaultdict(list)\n        self.joint_augment_transform = None\n        self.is_augment = is_augment\n        if self.is_augment:\n            self.joint_augment_transform = Compose([\n                # RandomSized(int(480)),\n                RandomRotate(degree=10),\n                RandomHorizontallyFlip(),\n            ])\n\n        # file_list = os.listdir(root + \'/\' + split)\n        file_list = glob.glob(root + \'/\' + split + \'/*.png\')\n        file_list.sort()\n        self.files[split] = file_list\n\n    def __len__(self):\n        return len(self.files[self.split])\n\n    # def get_class_weight(self):\n\n    def __getitem__(self, index):\n        img_name = self.files[self.split][index]\n        img_file_name = img_name[img_name.rfind(\'/\')+1:img_name.rfind(\'.\')]\n        # img_file_name = img_name[:img_name.rfind(\'.\')]\n        # print(img_file_name)\n        img_path = self.root + \'/\' + self.split + \'/\' + img_file_name + \'.png\'\n        lbl_path = self.root + \'/\' + self.split + \'annot/\' + img_file_name + \'.png\'\n\n        img = Image.open(img_path)\n        lbl = Image.open(lbl_path)\n\n        if self.is_augment:\n            if self.joint_augment_transform is not None:\n                img, lbl = self.joint_augment_transform(img, lbl)\n\n        img = np.array(img, dtype=np.uint8)\n        lbl = np.array(lbl, dtype=np.int32)\n\n        if self.is_transform:\n            img, lbl = self.transform(img, lbl)\n        img = torch.from_numpy(img).float()\n        lbl = torch.from_numpy(lbl).long()\n\n        return img, lbl\n\n    # \xe8\xbd\xac\xe6\x8d\xa2HWC\xe4\xb8\xbaCHW\n    def transform(self, img, lbl):\n        img = img[:, :, ::-1]\n        img = img.astype(np.float64)\n        img -= self.mean\n        img = img.astype(float) / 255.0\n        # HWC -> CHW\n        img = img.transpose(2, 0, 1)\n\n        # img = torch.from_numpy(img).float()\n        # lbl = torch.from_numpy(lbl).long()\n        return img, lbl\n\n    def decode_segmap(self, temp, plot=False):\n        Sky = [128, 128, 128]\n        Building = [128, 0, 0]\n        Pole = [192, 192, 128]\n        # Road_marking = [255, 69, 0]\n        Road = [128, 64, 128]\n        Pavement = [60, 40, 222]\n        Tree = [128, 128, 0]\n        SignSymbol = [192, 128, 128]\n        Fence = [64, 64, 128]\n        Car = [64, 0, 128]\n        Pedestrian = [64, 64, 0]\n        Bicyclist = [0, 128, 192]\n        Unlabelled = [0, 0, 0]\n\n        label_colours = np.array(\n            [\n                Sky,\n                Building,\n                Pole,\n                Road,\n                Pavement,\n                Tree,\n                SignSymbol,\n                Fence,\n                Car,\n                Pedestrian,\n                Bicyclist,\n                Unlabelled,\n            ]\n        )\n        r = temp.copy()\n        g = temp.copy()\n        b = temp.copy()\n        for l in range(0, self.n_classes):\n            r[temp == l] = label_colours[l, 0]\n            g[temp == l] = label_colours[l, 1]\n            b[temp == l] = label_colours[l, 2]\n\n        rgb = np.zeros((temp.shape[0], temp.shape[1], 3), dtype=np.float32)\n        rgb[:, :, 0] = r\n        rgb[:, :, 1] = g\n        rgb[:, :, 2] = b\n        if plot:\n            plt.imshow(rgb)\n            plt.show()\n        else:\n            return rgb\n\n\nif __name__ == \'__main__\':\n    HOME_PATH = os.path.expanduser(\'~\')\n    local_path = os.path.join(HOME_PATH, \'Data/CamVid\')\n    batch_size = 4\n    dst = camvidLoader(local_path, is_transform=True, is_augment=False)\n    trainloader = data.DataLoader(dst, batch_size=batch_size, shuffle=True)\n    for i, (imgs, labels) in enumerate(trainloader):\n        print(i)\n        print(imgs.shape)\n        print(labels.shape)\n        # if i == 0:\n        image_list_len = imgs.shape[0]\n        for image_list in range(image_list_len):\n            img = imgs[image_list, :, :, :]\n            img = img.numpy()\n            img = np.transpose(img, (1, 2, 0))\n            plt.subplot(image_list_len, 2, 2 * image_list + 1)\n            plt.imshow(img)\n            plt.subplot(image_list_len, 2, 2 * image_list + 2)\n            plt.imshow(dst.decode_segmap(labels.numpy()[image_list]))\n            # print(dst.decode_segmap(labels.numpy()[image_list])[0, 0, :])\n        plt.show()\n        if i==0:\n            break\n'"
semseg/dataloader/camvid_lrn_loader.py,3,"b'# -*- coding: utf-8 -*-\nimport torch\nimport os\nimport collections\nimport random\n\nimport cv2\nimport numpy as np\nimport scipy.misc as m\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nfrom torch.utils import data\nfrom torchvision import transforms\nimport glob\n\nfrom semseg.dataloader.utils import Compose, RandomHorizontallyFlip, RandomRotate, RandomSized, RandomCrop\n\n\nclass camvidLRNLoader(data.Dataset):\n    def __init__(self, root, split=""train"", is_transform=False, is_augment=False, img_sizes=[(11, 15), (22, 30), (45, 60), (90, 120,), (180, 240), (360, 480)]):\n        self.root = root\n        self.split = split\n        self.img_size = (360, 480) # (h, w)\n        self.img_sizes = img_sizes\n        self.is_transform = is_transform\n        self.mean = np.array([104.00699, 116.66877, 122.67892])\n        self.n_classes = 12\n        self.files = collections.defaultdict(list)\n        self.joint_augment_transform = None\n        self.is_augment = is_augment\n        if self.is_augment:\n            self.joint_augment_transform = Compose([\n                # RandomSized(int(480)),\n                RandomRotate(degree=10),\n                RandomHorizontallyFlip(),\n            ])\n\n        # file_list = os.listdir(root + \'/\' + split)\n        file_list = glob.glob(root + \'/\' + split + \'/*.png\')\n        file_list.sort()\n        self.files[split] = file_list\n\n    def __len__(self):\n        return len(self.files[self.split])\n\n    def __getitem__(self, index):\n        imgs = []\n        lbls = []\n        for img_size in self.img_sizes:\n            img_name = self.files[self.split][index]\n            img_file_name = img_name[img_name.rfind(\'/\') + 1:img_name.rfind(\'.\')]\n            # img_file_name = img_name[:img_name.rfind(\'.\')]\n            # print(img_file_name)\n            img_path = self.root + \'/\' + self.split + \'/\' + img_file_name + \'.png\'\n            lbl_path = self.root + \'/\' + self.split + \'annot/\' + img_file_name + \'.png\'\n\n            img = Image.open(img_path)\n            lbl = Image.open(lbl_path)\n\n            img = img.resize((img_size[1], img_size[0]))\n            lbl = lbl.resize((img_size[1], img_size[0]))\n\n            if self.is_augment:\n                if self.joint_augment_transform is not None:\n                    img, lbl = self.joint_augment_transform(img, lbl)\n\n            img = np.array(img, dtype=np.uint8)\n            lbl = np.array(lbl, dtype=np.int32)\n\n            if self.is_transform:\n                img, lbl = self.transform(img, lbl)\n            imgs.append(img)\n            lbls.append(lbl)\n        return imgs, lbls\n\n    # \xe8\xbd\xac\xe6\x8d\xa2HWC\xe4\xb8\xbaCHW\n    def transform(self, img, lbl):\n        img = img[:, :, ::-1]\n        img = img.astype(np.float64)\n        img -= self.mean\n        img = img.astype(float) / 255.0\n        # HWC -> CHW\n        img = img.transpose(2, 0, 1)\n\n        img = torch.from_numpy(img).float()\n        lbl = torch.from_numpy(lbl).long()\n        return img, lbl\n\n    def decode_segmap(self, temp, plot=False):\n        Sky = [128, 128, 128]\n        Building = [128, 0, 0]\n        Pole = [192, 192, 128]\n        # Road_marking = [255, 69, 0]\n        Road = [128, 64, 128]\n        Pavement = [60, 40, 222]\n        Tree = [128, 128, 0]\n        SignSymbol = [192, 128, 128]\n        Fence = [64, 64, 128]\n        Car = [64, 0, 128]\n        Pedestrian = [64, 64, 0]\n        Bicyclist = [0, 128, 192]\n        Unlabelled = [0, 0, 0]\n\n        label_colours = np.array(\n            [\n                Sky,\n                Building,\n                Pole,\n                Road,\n                Pavement,\n                Tree,\n                SignSymbol,\n                Fence,\n                Car,\n                Pedestrian,\n                Bicyclist,\n                Unlabelled,\n            ]\n        )\n        r = temp.copy()\n        g = temp.copy()\n        b = temp.copy()\n        for l in range(0, self.n_classes):\n            r[temp == l] = label_colours[l, 0]\n            g[temp == l] = label_colours[l, 1]\n            b[temp == l] = label_colours[l, 2]\n\n        rgb = np.zeros((temp.shape[0], temp.shape[1], 3), dtype=np.float32)\n        rgb[:, :, 0] = r\n        rgb[:, :, 1] = g\n        rgb[:, :, 2] = b\n        if plot:\n            plt.imshow(rgb)\n            plt.show()\n        else:\n            return rgb\n\nif __name__ == \'__main__\':\n    HOME_PATH = os.path.expanduser(\'~\')\n    local_path = os.path.join(HOME_PATH, \'Data/CamVid\')\n    batch_size = 4\n    dst = camvidLRNLoader(local_path, is_transform=True, is_augment=False)\n    trainloader = data.DataLoader(dst, batch_size=batch_size, shuffle=True)\n    for i, (imgs, labels) in enumerate(trainloader):\n        print(i)\n        print(len(imgs))\n        print(imgs[-1].shape)\n        # print(imgs.shape)\n        # print(labels.shape)\n        # if i == 0:\n        image_list_len = imgs[-1].shape[0]\n        for image_list in range(image_list_len):\n            img = imgs[-1][image_list, :, :, :]\n            # print(\'img.shape:\', img.shape)\n            img = img.numpy()\n            img = np.transpose(img, (1, 2, 0))\n            plt.subplot(image_list_len, 2, 2 * image_list + 1)\n            plt.imshow(img)\n            plt.subplot(image_list_len, 2, 2 * image_list + 2)\n            plt.imshow(dst.decode_segmap(labels[-1].numpy()[image_list]))\n            # print(dst.decode_segmap(labels.numpy()[image_list])[0, 0, :])\n        plt.show()\n        if i==0:\n            break\n'"
semseg/dataloader/cityscapes_loader.py,3,"b'# -*- coding: utf-8 -*-\n\nimport os\nimport torch\nimport numpy as np\nimport scipy.misc as m\n\nfrom torch.utils import data\nimport matplotlib.pyplot as plt\n\nfrom semseg.dataloader.utils import recursive_glob\n\n\nclass cityscapesLoader(data.Dataset):\n    """"""cityscapesLoader\n\n    https://www.cityscapes-dataset.com\n\n    Data is derived from CityScapes, and can be downloaded from here:\n    https://www.cityscapes-dataset.com/downloads/\n\n    Many Thanks to @fvisin for the loader repo:\n    https://github.com/fvisin/dataset_loaders/blob/master/dataset_loaders/images/cityscapes.py\n    """"""\n\n    colors = [  # [  0,   0,   0],\n        [128, 64, 128],\n        [244, 35, 232],\n        [70, 70, 70],\n        [102, 102, 156],\n        [190, 153, 153],\n        [153, 153, 153],\n        [250, 170, 30],\n        [220, 220, 0],\n        [107, 142, 35],\n        [152, 251, 152],\n        [0, 130, 180],\n        [220, 20, 60],\n        [255, 0, 0],\n        [0, 0, 142],\n        [0, 0, 70],\n        [0, 60, 100],\n        [0, 80, 100],\n        [0, 0, 230],\n        [119, 11, 32],\n    ]\n\n    label_colours = dict(zip(range(19), colors))\n\n    mean_rgb = {\n        ""pascal"": [103.939, 116.779, 123.68],\n        ""cityscapes"": [0.0, 0.0, 0.0],\n    }  # pascal mean for PSPNet and ICNet pre-trained model\n\n    def __init__(\n        self,\n        root,\n        split=""train"",\n        is_transform=False,\n        img_size=(512, 1024),\n        augmentations=None,\n        img_norm=True,\n        version=""cityscapes"",\n    ):\n        """"""__init__\n\n        :param root:\n        :param split:\n        :param is_transform:\n        :param img_size:\n        :param augmentations \n        """"""\n        self.root = root\n        self.split = split\n        self.is_transform = is_transform\n        self.augmentations = augmentations\n        self.img_norm = img_norm\n        self.n_classes = 19\n        self.img_size = (\n            img_size if isinstance(img_size, tuple) else (img_size, img_size)\n        )\n        self.mean = np.array(self.mean_rgb[version])\n        self.files = {}\n\n        self.images_base = os.path.join(self.root, ""leftImg8bit"", self.split)\n        self.annotations_base = os.path.join(\n            self.root, ""gtFine"", self.split\n        )\n\n        self.files[split] = recursive_glob(rootdir=self.images_base, suffix="".png"")\n\n        self.void_classes = [0, 1, 2, 3, 4, 5, 6, 9, 10, 14, 15, 16, 18, 29, 30, -1]\n        self.valid_classes = [\n            7,\n            8,\n            11,\n            12,\n            13,\n            17,\n            19,\n            20,\n            21,\n            22,\n            23,\n            24,\n            25,\n            26,\n            27,\n            28,\n            31,\n            32,\n            33,\n        ]\n        self.class_names = [\n            ""unlabelled"",\n            ""road"",\n            ""sidewalk"",\n            ""building"",\n            ""wall"",\n            ""fence"",\n            ""pole"",\n            ""traffic_light"",\n            ""traffic_sign"",\n            ""vegetation"",\n            ""terrain"",\n            ""sky"",\n            ""person"",\n            ""rider"",\n            ""car"",\n            ""truck"",\n            ""bus"",\n            ""train"",\n            ""motorcycle"",\n            ""bicycle"",\n        ]\n\n        self.ignore_index = 250\n        self.class_map = dict(zip(self.valid_classes, range(19)))\n\n        if not self.files[split]:\n            raise Exception(\n                ""No files for split=[%s] found in %s"" % (split, self.images_base)\n            )\n\n        print(""Found %d %s images"" % (len(self.files[split]), split))\n\n    def __len__(self):\n        """"""__len__""""""\n        return len(self.files[self.split])\n\n    def __getitem__(self, index):\n        """"""__getitem__\n\n        :param index:\n        """"""\n        img_path = self.files[self.split][index].rstrip()\n        lbl_path = os.path.join(\n            self.annotations_base,\n            img_path.split(os.sep)[-2],\n            os.path.basename(img_path)[:-15] + ""gtFine_labelIds.png"",\n        )\n\n        img = m.imread(img_path)\n        img = np.array(img, dtype=np.uint8)\n\n        polygons_path = os.path.join(\n            self.annotations_base,\n            img_path.split(os.sep)[-2],\n            os.path.basename(img_path)[:-15] + ""gtFine_polygons.json"",\n        )\n\n        # import json\n        # polygons_json = json.load(open(polygons_path, \'rb\'))\n        # object_det_bboxes = []\n        # # print(polygons_json[\'objects\'])\n        # polygons_json_objects = polygons_json[\'objects\']\n        # for polygons_json_object in polygons_json_objects:\n        #     polygons_json_object_polygon = polygons_json_object[\'polygon\']\n        #     polygons_json_object_label = polygons_json_object[\'label\']\n        #     if polygons_json_object_label in [\'car\', \'person\']:\n        #         # print(polygons_json_object_polygon)\n        #         polygons_json_object_polygon_np = np.array(polygons_json_object_polygon)\n        #         # print(polygons_json_object_polygon_np)\n        #         # print(polygons_json_object_polygon_np.shape)\n        #         polygons_json_object_polygon_np_x = polygons_json_object_polygon_np[:, 0]\n        #         polygons_json_object_polygon_np_y = polygons_json_object_polygon_np[:, 1]\n        #         x1 = min(polygons_json_object_polygon_np_x)\n        #         x2 = max(polygons_json_object_polygon_np_x)\n        #         y1 = min(polygons_json_object_polygon_np_y)\n        #         y2 = max(polygons_json_object_polygon_np_y)\n        #         object_cls = 0\n        #         object_det_bboxes.append([x1, y1, x2, y2, object_cls])\n        # \n        # for object_det_bbox in object_det_bboxes:\n        #     import cv2\n        #     x1 = object_det_bbox[0]\n        #     y1 = object_det_bbox[1]\n        #     x2 = object_det_bbox[2]\n        #     y2 = object_det_bbox[3]\n        #     cv2.rectangle(img, pt1=(x1, y1), pt2=(x2, y2), color=(255, 0, 0), thickness=5)\n\n\n        lbl = m.imread(lbl_path)\n        lbl = self.encode_segmap(np.array(lbl, dtype=np.uint8))\n\n        if self.augmentations is not None:\n            img, lbl = self.augmentations(img, lbl)\n\n        if self.is_transform:\n            img, lbl = self.transform(img, lbl)\n\n        return img, lbl\n\n    def transform(self, img, lbl):\n        """"""transform\n\n        :param img:\n        :param lbl:\n        """"""\n        img = m.imresize(\n            img, (self.img_size[0], self.img_size[1])\n        )  # uint8 with RGB mode\n        img = img[:, :, ::-1]  # RGB -> BGR\n        img = img.astype(np.float64)\n        img -= self.mean\n        if self.img_norm:\n            # Resize scales images from 0 to 255, thus we need\n            # to divide by 255.0\n            img = img.astype(float) / 255.0\n        # NHWC -> NCHW\n        img = img.transpose(2, 0, 1)\n\n        classes = np.unique(lbl)\n        lbl = lbl.astype(float)\n        lbl = m.imresize(lbl, (self.img_size[0], self.img_size[1]), ""nearest"", mode=""F"")\n        lbl = lbl.astype(int)\n\n        if not np.all(classes == np.unique(lbl)):\n            print(""WARN: resizing labels yielded fewer classes"")\n\n        if not np.all(np.unique(lbl[lbl != self.ignore_index]) < self.n_classes):\n            print(""after det"", classes, np.unique(lbl))\n            raise ValueError(""Segmentation map contained invalid class values"")\n\n        img = torch.from_numpy(img).float()\n        lbl = torch.from_numpy(lbl).long()\n\n        return img, lbl\n\n    def decode_segmap(self, temp):\n        r = temp.copy()\n        g = temp.copy()\n        b = temp.copy()\n        for l in range(0, self.n_classes):\n            r[temp == l] = self.label_colours[l][0]\n            g[temp == l] = self.label_colours[l][1]\n            b[temp == l] = self.label_colours[l][2]\n\n        rgb = np.zeros((temp.shape[0], temp.shape[1], 3), dtype=np.float32)\n        # rgb[:, :, 0] = r / 255.0\n        # rgb[:, :, 1] = g / 255.0\n        # rgb[:, :, 2] = b / 255.0\n        rgb[:, :, 0] = r\n        rgb[:, :, 1] = g\n        rgb[:, :, 2] = b\n        return rgb\n\n    def encode_segmap(self, mask):\n        # Put all void classes to zero\n        for _voidc in self.void_classes:\n            mask[mask == _voidc] = self.ignore_index\n        for _validc in self.valid_classes:\n            mask[mask == _validc] = self.class_map[_validc]\n        return mask\n\n\nif __name__ == \'__main__\':\n    HOME_PATH = os.path.expanduser(\'~\')\n    local_path = os.path.join(HOME_PATH, \'Data/cityscapes/\')\n    dst = cityscapesLoader(local_path, is_transform=True)\n    trainloader = data.DataLoader(dst, batch_size=4, num_workers=0)\n    for i, data in enumerate(trainloader):\n        imgs, labels = data\n        img = imgs.numpy()[0, ::-1, :, :]\n        img = np.transpose(img, [1, 2, 0])\n        f, axarr = plt.subplots(2, 1)\n        axarr[0].imshow(img)\n        axarr[1].imshow(dst.decode_segmap(labels.numpy()[0]))\n        plt.show()\n        if i==3:\n            break\n'"
semseg/dataloader/folder2lmdb.py,3,"b'# -*- coding: utf-8 -*-\nimport numpy as np\nimport cv2\nimport os\nimport time\n# import os.path as osp\nimport glob\nimport os, sys\n# import os.path as osp\nfrom PIL import Image\nimport six\nimport string\n\nimport lmdb\nimport pickle\nimport msgpack\nimport tqdm\nimport pyarrow\n\nimport torch\nimport torch.utils.data as data\nfrom torch.utils.data import DataLoader\nfrom torchvision.transforms import transforms\nfrom torchvision.datasets import ImageFolder\nfrom torchvision import transforms, datasets\n\nfrom semseg.dataloader.camvid_loader import camvidLoader\n\n\nclass ImageFolderLMDB(data.Dataset):\n    def __init__(self, db_path, transform=None):\n        self.db_path = db_path\n        self.env = lmdb.open(db_path, subdir=os.path.isdir(db_path),\n                             readonly=True, lock=False,\n                             readahead=False, meminit=False)\n        with self.env.begin(write=False) as txn:\n            # self.length = txn.stat()[\'entries\'] - 1\n            # print(\'txn:\', txn)\n            self.length = pyarrow.deserialize(txn.get(\'__len__\'))\n            # print(\'length:\', self.length)\n            # self.keys = msgpack.loads(txn.get(b\'__keys__\'))\n            self.keys = pyarrow.deserialize(txn.get(\'__keys__\'))\n            # print(\'keys:\', self.keys)\n\n        self.transform = transform\n\n    def __getitem__(self, index):\n        img = None\n        env = self.env\n        with env.begin(write=False) as txn:\n            byteflow = txn.get(self.keys[index])\n        img = pyarrow.deserialize(byteflow)\n\n        # load image\n        # imgbuf = unpacked[0]\n        # print(\'imgbuf:\', imgbuf)\n        # print(\'imgbuf.shape:\', imgbuf.shape)\n        # cv2.imshow(\'img:\', img)\n        # cv2.waitKey(1)\n        img = torch.FloatTensor(img)\n        # buf = six.BytesIO()\n        # buf.write(imgbuf)\n        # buf.seek(0)\n        # img = Image.open(buf).convert(\'RGB\')\n        # print(\'buf:\', buf)\n\n        # if self.transform is not None:\n        #     img = self.transform(img)\n\n        return img\n\n    def __len__(self):\n        return self.length\n        # return 64\n\n    def __repr__(self):\n        return self.__class__.__name__ + \' (\' + self.db_path + \')\'\n\n\ndef raw_reader(path):\n    with open(path, \'rb\') as f:\n        bin_data = f.read()\n    return bin_data\n\n\ndef dumps_pyarrow(obj):\n    """"""\n    Serialize an object.\n    Returns:\n        Implementation-dependent bytes-like object\n    """"""\n    return pyarrow.serialize(obj).to_buffer()\n\n\ndef folder2lmdb(dpath, lmdb_path, write_frequency=5000):\n    directory = os.path.expanduser(dpath)\n    print(""Loading dataset from %s"" % directory)\n    # dataset = ImageFolder(directory, loader=raw_reader)\n    # data_loader = DataLoader(dataset, num_workers=16, collate_fn=lambda x: x)\n    data_path_lists = glob.glob(os.path.join(directory, \'*.png\'))\n\n    # lmdb_path = ""{}.lmdb"".format(time.time())\n    isdir = os.path.isdir(lmdb_path)\n\n    print(""Generate LMDB to %s"" % lmdb_path)\n    db = lmdb.open(lmdb_path, subdir=isdir,\n                   map_size=1099511627776 * 2, readonly=False,\n                   meminit=False, map_async=True)\n\n    txn = db.begin(write=True)\n    for idx, data_path in enumerate(data_path_lists):\n        # print(type(data), data)\n        image = cv2.imread(data_path)\n        txn.put(u\'{}\'.format(idx).encode(\'ascii\'), dumps_pyarrow(image))\n        if idx % write_frequency == 0:\n            print(""[%d/%d]"" % (idx, len(data_path_lists)))\n            txn.commit()\n            txn = db.begin(write=True)\n\n    # finish iterating through dataset\n    txn.commit()\n    keys = [u\'{}\'.format(k).encode(\'ascii\') for k in range(idx + 1)]\n    with db.begin(write=True) as txn:\n        print(\'keys:\', keys)\n        print(\'len(keys):\', len(keys))\n        txn.put(\'__keys__\', dumps_pyarrow(keys))\n        txn.put(\'__len__\', dumps_pyarrow(len(keys)))\n\n    print(""Flushing database ..."")\n    db.sync()\n    db.close()\n\n\nif __name__ == ""__main__"":\n    # lmdb_path = ""{}.lmdb/"".format(time.time())\n    lmdb_path = ""tmp.lmdb""\n    # folder2lmdb(""~/Data/CamVid/train/"", lmdb_path)\n\n    batch_size = 1\n\n    dst = ImageFolderLMDB(lmdb_path, None)\n    loader = DataLoader(dst, batch_size=batch_size, drop_last=True)\n\n    time_start = time.time()\n    for idx, data in enumerate(loader):\n        pass\n        # print(""idx:"", idx)\n    time_end = time.time()\n    print(\'load {} images cost time: {} sec\'.format(len(dst), time_end-time_start))\n    print(\'load {} images {} fps\'.format(len(dst), len(dst)*1.0/(time_end-time_start)))\n\n    local_path = os.path.join(os.path.expanduser(\'~/Data/CamVid\'))\n    dst = camvidLoader(local_path, is_transform=False, is_augment=False)\n    loader = DataLoader(dst, batch_size=batch_size)\n    time_start = time.time()\n    for idx, data in enumerate(loader):\n        pass\n        # print(""idx:"", idx)\n    time_end = time.time()\n    print(\'load {} images cost time: {} sec\'.format(len(dst), time_end-time_start))\n    print(\'load {} images {} fps\'.format(len(dst), len(dst)*1.0/(time_end-time_start)))\n'"
semseg/dataloader/freespace_loader.py,3,"b'# -*- coding: utf-8 -*-\nimport torch\nimport os\nimport collections\nimport random\n\nimport cv2\nimport numpy as np\nimport scipy.misc as m\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nfrom torch.utils import data\nfrom torchvision import transforms\nimport glob\n\nfrom semseg.dataloader.utils import Compose, RandomHorizontallyFlip, RandomRotate, RandomSized, RandomCrop\n\n\nclass freespaceLoader(data.Dataset):\n    """"""\n    freespace dataloader for my onw freespace dataset loader which contain free or not free two segment\n    FreeSpaceDataset:\n        - train\n            - dji_caoping\n                - *.png\n                - ...\n            - dji_huatan\n            - dji_road_1\n            - dji_road_2\n        - trainannot\n            - dji_caoping\n                - *_mask.png\n            - dji_huatan\n            - dji_road_1\n            - dji_road_2\n    """"""\n    def __init__(self, root, split=""train"", is_transform=False, is_augment=False):\n        self.root = root\n        self.split = split\n        self.img_size = (360, 480) # (h, w)\n        self.is_transform = is_transform\n        self.mean = np.array([104.00699, 116.66877, 122.67892])\n        self.n_classes = 2\n        self.files = collections.defaultdict(list)\n        self.joint_augment_transform = None\n        self.is_augment = is_augment\n        if self.is_augment:\n            self.joint_augment_transform = Compose([\n                # RandomSized(int(min(self.img_size)/0.875)),\n                # RandomCrop(self.img_size),\n                RandomRotate(degree=10),\n                RandomHorizontallyFlip(),\n            ])\n\n        # file_list = os.listdir(root + \'/\' + split)\n        file_list = glob.glob(root + \'/\' + split + \'/*/*.png\')\n        # print(\'file_list:\', file_list)\n        file_list.sort()\n        self.files[split] = file_list\n\n    def get_filename(self, index):\n        return self.files[self.split][index]\n\n    def __len__(self):\n        return len(self.files[self.split])\n\n    def __getitem__(self, index):\n        img_name = self.files[self.split][index]\n        img_name_index = img_name.rfind(\'/\')\n        img_type_index = img_name.rfind(\'/\', 0, img_name_index-1)\n        # print(\'img_name_index:\', img_name_index)\n        # print(\'img_type_index:\', img_name_index)\n\n        img_file_name = img_name[img_name_index+1:img_name.rfind(\'.\')]\n        img_type_name = img_name[img_type_index+1:img_name_index]\n        # print(\'img_file_name:\', img_file_name)\n        # print(\'img_type_name:\', img_type_name)\n        # img_file_name = img_name[:img_name.rfind(\'.\')]\n        # print(img_file_name)\n        img_path = self.root + \'/\' + self.split + \'/\' + img_type_name + \'/\' + img_file_name + \'.png\'\n        lbl_path = self.root + \'/\' + self.split + \'annot/\' + img_type_name + \'/\' + img_file_name + \'_mask.png\'\n\n        img = Image.open(img_path)\n        lbl = Image.open(lbl_path)\n\n        # lbl = np.array(lbl)\n        # lbl[lbl==2] = 1\n        # print(np.unique(lbl))\n        # cv2.imwrite(lbl_path, lbl)\n\n        if self.is_augment:\n            if self.joint_augment_transform is not None:\n                img, lbl = self.joint_augment_transform(img, lbl)\n\n        img = np.array(img, dtype=np.uint8)\n        lbl = np.array(lbl, dtype=np.int32)\n\n        if self.is_transform:\n            img, lbl = self.transform(img, lbl)\n\n        # print(\'img.shape:\', img.shape)\n        # print(\'lbl.shape:\', lbl.shape)\n        return img, lbl\n\n    # \xe8\xbd\xac\xe6\x8d\xa2HWC\xe4\xb8\xbaCHW\n    def transform(self, img, lbl):\n        img = img[:, :, ::-1]\n        img = img.astype(np.float64)\n        img -= self.mean\n        img = img.astype(float) / 255.0\n        # HWC -> CHW\n        img = img.transpose(2, 0, 1)\n\n        img = torch.from_numpy(img).float()\n        lbl = torch.from_numpy(lbl).long()\n        return img, lbl\n\n    def decode_segmap(self, temp, plot=False):\n        FreeSpace = [255, 0, 0]\n        Unlabelled = [0, 0, 0]\n\n        label_colours = np.array(\n            [\n                Unlabelled,\n                FreeSpace,\n            ]\n        )\n        r = temp.copy()\n        g = temp.copy()\n        b = temp.copy()\n        for l in range(0, self.n_classes):\n            r[temp == l] = label_colours[l, 0]\n            g[temp == l] = label_colours[l, 1]\n            b[temp == l] = label_colours[l, 2]\n\n        rgb = np.zeros((temp.shape[0], temp.shape[1], 3), dtype=np.float32)\n        rgb[:, :, 0] = r\n        rgb[:, :, 1] = g\n        rgb[:, :, 2] = b\n        if plot:\n            plt.imshow(rgb)\n            plt.show()\n        else:\n            return rgb\n\nif __name__ == \'__main__\':\n    HOME_PATH = os.path.expanduser(\'~\')\n    local_path = os.path.join(HOME_PATH, \'Data/FreeSpacePredDataset\')\n    batch_size = 1\n    dst = freespaceLoader(local_path, is_transform=True, is_augment=False, split=""train"")\n    trainloader = data.DataLoader(dst, batch_size=batch_size, shuffle=False)\n    for i, (imgs, labels) in enumerate(trainloader):\n        pass\n        # print(i)\n        # print(imgs.shape)\n        # print(labels.shape)\n        # if i == 0:\n        image_list_len = imgs.shape[0]\n        for image_list in range(image_list_len):\n            img = imgs[image_list, :, :, :]\n            img = img.numpy()\n            img = np.transpose(img, (1, 2, 0))\n            plt.subplot(image_list_len, 2, 2 * image_list + 1)\n            plt.imshow(img)\n            plt.subplot(image_list_len, 2, 2 * image_list + 2)\n            plt.imshow(dst.decode_segmap(labels.numpy()[image_list]))\n            # print(dst.decode_segmap(labels.numpy()[image_list])[0, 0, :])\n        plt.show()\n        if i==0:\n            break\n'"
semseg/dataloader/freespacepred_loader.py,4,"b'# -*- coding: utf-8 -*-\nimport torch\nimport os\nimport collections\nimport random\n\nimport cv2\nimport numpy as np\nimport scipy.misc as m\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nfrom torch.utils import data\nfrom torchvision import transforms\nimport glob\n\nfrom semseg.dataloader.utils import Compose, RandomHorizontallyFlip, RandomRotate, RandomSized, RandomCrop\n\n\nclass freespacepredLoader(data.Dataset):\n    """"""\n    freespace dataloader for my onw freespace dataset loader which contain free or not free two segment\n    FreeSpaceDataset:\n        - train\n            - dji_caoping\n                - *.png\n                - ...\n            - dji_huatan\n            - dji_road_1\n            - dji_road_2\n        - trainannot\n            - dji_caoping\n                - *_mask.png\n            - dji_huatan\n            - dji_road_1\n            - dji_road_2\n    """"""\n    def __init__(self, root, split=""train"", is_transform=False, is_augment=False):\n        self.root = root\n        self.split = split\n        self.img_size = (360, 480) # (h, w)\n        self.is_transform = is_transform\n        self.mean = np.array([104.00699, 116.66877, 122.67892])\n        self.n_classes = 2\n        self.files = collections.defaultdict(list)\n        self.joint_augment_transform = None\n        self.is_augment = is_augment\n        if self.is_augment:\n            self.joint_augment_transform = Compose([\n                # RandomSized(int(min(self.img_size)/0.875)),\n                # RandomCrop(self.img_size),\n                RandomRotate(degree=10),\n                RandomHorizontallyFlip(),\n            ])\n\n        # file_list = os.listdir(root + \'/\' + split)\n        file_list = glob.glob(root + \'/\' + split + \'/*/*.png\')\n        # print(\'file_list:\', file_list)\n        file_list.sort()\n        self.files[split] = file_list\n\n        self.input_shape = (64, 64)\n        # if self.split == \'train\':\n        #     self.input_shape = (64, 64)\n        # elif self.split == \'test\':\n        #     self.input_shape = (64, 64)\n\n    def get_filename(self, index):\n        return self.files[self.split][index]\n\n    def __len__(self):\n        return len(self.files[self.split])\n\n    def __getitem__(self, index):\n        img_past = []\n        img_future = []\n\n        index = np.clip(index, 0, self.__len__()-4) # for out of range\n        for i in range(4):\n            img_name = self.files[self.split][index+i]\n            img_name_index = img_name.rfind(\'/\')\n            img_type_index = img_name.rfind(\'/\', 0, img_name_index - 1)\n            # print(\'img_name_index:\', img_name_index)\n            # print(\'img_type_index:\', img_name_index)\n\n            img_file_name = img_name[img_name_index + 1:img_name.rfind(\'.\')]\n            img_type_name = img_name[img_type_index + 1:img_name_index]\n            # print(\'img_file_name:\', img_file_name)\n            # print(\'img_type_name:\', img_type_name)\n            # img_file_name = img_name[:img_name.rfind(\'.\')]\n            # print(img_file_name)\n            img_path = self.root + \'/\' + self.split + \'/\' + img_type_name + \'/\' + img_file_name + \'.png\'\n            lbl_path = self.root + \'/\' + self.split + \'annot/\' + img_type_name + \'/\' + img_file_name + \'_mask.png\'\n\n            img = Image.open(img_path)\n            lbl = Image.open(lbl_path)\n\n            img = img.resize((self.input_shape[1], self.input_shape[0]))\n            lbl = lbl.resize((self.input_shape[1], self.input_shape[0]))\n\n            if self.is_augment:\n                if self.joint_augment_transform is not None:\n                    img, lbl = self.joint_augment_transform(img, lbl)\n\n            img = np.array(img, dtype=np.uint8)\n            lbl = np.array(lbl, dtype=np.int32)\n\n            if self.is_transform:\n                img, lbl = self.transform(img, lbl)\n\n            if i <= 3:\n                img_past.append(lbl.float())\n            img_future = lbl\n        img_past = torch.stack(img_past)\n\n        # print(\'img_past.shape:\', img_past.shape)\n        # print(\'img_future.shape:\', img_future.shape)\n        # return img, lbl\n        return img_past, img_future\n\n    # \xe8\xbd\xac\xe6\x8d\xa2HWC\xe4\xb8\xbaCHW\n    def transform(self, img, lbl):\n        img = img[:, :, ::-1]\n        img = img.astype(np.float64)\n        img -= self.mean\n        img = img.astype(float) / 255.0\n        # HWC -> CHW\n        img = img.transpose(2, 0, 1)\n\n        img = torch.from_numpy(img).float()\n        lbl = torch.from_numpy(lbl).long()\n        return img, lbl\n\n    def decode_segmap(self, temp, plot=False):\n        FreeSpace = [255, 0, 0]\n        Unlabelled = [0, 0, 0]\n\n        label_colours = np.array(\n            [\n                Unlabelled,\n                FreeSpace,\n            ]\n        )\n        r = temp.copy()\n        g = temp.copy()\n        b = temp.copy()\n        for l in range(0, self.n_classes):\n            r[temp == l] = label_colours[l, 0]\n            g[temp == l] = label_colours[l, 1]\n            b[temp == l] = label_colours[l, 2]\n\n        rgb = np.zeros((temp.shape[0], temp.shape[1], 3), dtype=np.float32)\n        rgb[:, :, 0] = r\n        rgb[:, :, 1] = g\n        rgb[:, :, 2] = b\n        if plot:\n            plt.imshow(rgb)\n            plt.show()\n        else:\n            return rgb\n\n\nif __name__ == \'__main__\':\n    HOME_PATH = os.path.expanduser(\'~\')\n    local_path = os.path.join(HOME_PATH, \'Data/FreeSpacePredDataset\')\n    batch_size = 1\n    dst = freespacepredLoader(local_path, is_transform=True, is_augment=False)\n    trainloader = data.DataLoader(dst, batch_size=batch_size, shuffle=False)\n    for i, (img_past, img_future) in enumerate(trainloader):\n        # print(i)\n        # print(img_past.shape)\n        # print(img_future.shape)\n        # if i == 0:\n        image_list_len = img_past.shape[0]\n        for image_list in range(image_list_len):\n            pred_segment = img_future.numpy()[image_list]\n            # print(\'img_future_onehot:\', img_future_onehot)\n            # print(\'img_future_onehot.shape:\', img_future_onehot.shape)\n            plt.imshow(dst.decode_segmap(pred_segment))\n            # print(\'dst.decode_segmap(pred_segment):\', dst.decode_segmap(pred_segment))\n        plt.show()\n        if i==0:\n            break\n'"
semseg/dataloader/movingmnist_loader.py,3,"b'# -*- coding: utf-8 -*-\nimport torch\nimport os\nimport collections\nimport random\n\nimport cv2\nimport numpy as np\nimport scipy.misc as m\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nfrom torch.utils import data\nfrom torchvision import transforms\nimport glob\nimport torchfile\n\nclass movingmnistLoader(data.Dataset):\n\n    def __init__(self, root, split=""train"", is_transform=False, is_augment=False, split_ratio=0.7):\n        self.root = root\n        self.split = split\n        self.is_transform = is_transform\n        self.is_augment = is_augment\n        self.split_ratio = split_ratio\n\n        self.n_classes = 2\n        colors = [\n            [0, 0, 0],\n            [255, 255, 255],\n        ]\n        self.label_colours = dict(zip(range(self.n_classes), colors))\n        self.data = np.load(self.root).transpose(1, 0, 2, 3) # from TxSxHxW to SxTxHxW\n        all_len = len(self.data[:, 0, 0, 0])\n        split_index = int(all_len*split_ratio)\n        if self.split==\'train\':\n            self.data = self.data[:split_index]\n        elif self.split==\'val\':\n            self.data = self.data[split_index:]\n        print(\'self.data.shape:\', self.data.shape)\n\n    def __len__(self):\n        return len(self.data[:, 0, 0, 0])\n\n    def __getitem__(self, index):\n        img_np = self.data[index, ...]\n        # img_np = (img_np-128)//128\n        # print(img_np[0, 0, 0])\n        img_np = np.clip(img_np, 0, 1)\n        # print(img_np[0, 0, 0])\n        # print(np.max(img_np))\n        # print(np.min(img_np))\n        # print(np.unique(img_np))\n        # print(\'img_np.shape:\', img_np.shape)\n        img_past = img_np[:9]\n        img_future = img_np[9]\n\n        img_past = np.array(img_past, dtype=np.uint8)\n        img_future = np.array(img_future, dtype=np.int32)\n\n        img_past = torch.from_numpy(img_past).float()\n        img_future = torch.from_numpy(img_future).long()\n\n        # print(\'img_past.shape:\', img_past.shape)\n        # print(\'img_future.shape:\', img_future.shape)\n\n        return img_past, img_future\n\n    def decode_segmap(self, temp):\n        r = temp.copy()\n        g = temp.copy()\n        b = temp.copy()\n        for l in range(0, self.n_classes):\n            r[temp == l] = self.label_colours[l][0]\n            g[temp == l] = self.label_colours[l][1]\n            b[temp == l] = self.label_colours[l][2]\n\n        rgb = np.zeros((temp.shape[0], temp.shape[1], 3), dtype=np.uint8)\n        # rgb[:, :, 0] = r / 255.0\n        # rgb[:, :, 1] = g / 255.0\n        # rgb[:, :, 2] = b / 255.0\n        rgb[:, :, 0] = r\n        rgb[:, :, 1] = g\n        rgb[:, :, 2] = b\n        return rgb\n\nif __name__ == \'__main__\':\n    HOME_PATH = os.path.expanduser(\'~\')\n    local_path = os.path.join(HOME_PATH, \'Data/mnist_test_seq.npy\')\n    batch_size = 4\n    dst = movingmnistLoader(local_path, is_transform=True, is_augment=False)\n    trainloader = data.DataLoader(dst, batch_size=batch_size, shuffle=True)\n    for i, (img_past, img_future) in enumerate(trainloader):\n        # print(i)\n        # print(img_past.shape)\n        # print(img_future.shape)\n        # if i == 0:\n        image_list_len = img_past.shape[0]\n        for image_list in range(image_list_len):\n            pred_segment = img_future.numpy()[image_list]\n            # print(\'img_future_onehot:\', img_future_onehot)\n            # print(\'img_future_onehot.shape:\', img_future_onehot.shape)\n            plt.imshow(dst.decode_segmap(pred_segment))\n            # plt.imshow(pred_segment)\n            # print(\'dst.decode_segmap(pred_segment):\', dst.decode_segmap(pred_segment))\n        plt.show()\n        if i==0:\n            break\n'"
semseg/dataloader/segmpred_loader.py,3,"b'# -*- coding: utf-8 -*-\nimport torch\nimport os\nimport collections\nimport random\n\nimport cv2\nimport numpy as np\nimport scipy.misc as m\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nfrom torch.utils import data\nfrom torchvision import transforms\nimport glob\nimport torchfile\n\n\nclass segmpredLoader(data.Dataset):\n    colors = [  # [  0,   0,   0],\n        [128, 64, 128],\n        [244, 35, 232],\n        [70, 70, 70],\n        [102, 102, 156],\n        [190, 153, 153],\n        [153, 153, 153],\n        [250, 170, 30],\n        [220, 220, 0],\n        [107, 142, 35],\n        [152, 251, 152],\n        [0, 130, 180],\n        [220, 20, 60],\n        [255, 0, 0],\n        [0, 0, 142],\n        [0, 0, 70],\n        [0, 60, 100],\n        [0, 80, 100],\n        [0, 0, 230],\n        [119, 11, 32],\n    ]\n\n    label_colours = dict(zip(range(19), colors))\n\n    def __init__(self, root, split=""train"", is_transform=False, is_augment=False):\n        self.root = root\n        self.split = split\n        self.is_transform = is_transform\n        self.is_augment = is_augment\n\n        self.mean = np.array([104.00699, 116.66877, 122.67892])\n        self.n_classes = 19\n        self.files = collections.defaultdict(list)\n\n        file_list = glob.glob(root + \'/\' + split + \'/*.t7\')\n        file_list.sort()\n        # print(\'file_list:\', file_list)\n        self.files[split] = file_list\n\n        if self.split == \'train\':\n            self.input_shape = (64, 64)\n        elif self.split == \'val\':\n            self.input_shape = (128, 256)\n            # self.input_shape = (64, 64)\n\n    def __len__(self):\n        return len(self.files[self.split])\n\n    def __getitem__(self, index):\n        pass\n        img_name = self.files[self.split][index]\n        img_file_name = img_name[img_name.rfind(\'/\')+1:img_name.rfind(\'.\')]\n        img_path = self.root + \'/\' + self.split + \'/\' + img_file_name + \'.t7\'\n        # print(\'img_path:\', img_path)\n        img_pred = torchfile.load(img_path)\n        # import random\n        random_batch_id = random.randint(0, 3)\n        # print(\'random_batch_id:\', random_batch_id)\n        # random_batch_id = 0\n        img_sequences = img_pred[\'R8s\'][random_batch_id, ...]\n        # print(\'img_sequences.shape:\', img_sequences.shape)\n        img_past = img_sequences[0:4, ...]\n        img_future_onehot = img_sequences[4, ...]\n        img_past = np.array(img_past, dtype=np.uint8)\n        img_future_onehot = np.array(img_future_onehot, dtype=np.int32)\n\n        img_past = torch.from_numpy(img_past).float()\n        img_future_onehot = torch.from_numpy(img_future_onehot).long()\n        # print(np.unique(img_future_onehot))\n\n        # print(\'img_past.shape:\', img_past.shape)\n        img_past = img_past.view(-1, img_past.shape[2], img_past.shape[3])\n\n        img_future = np.argmax(img_future_onehot, axis=0)\n\n        # img_past = img_past[:, :64, :64]\n        # img_future = img_future[:64, :64]\n        # print(\'img_past.shape:\', img_past.shape)\n        # print(\'img_future.shape:\', img_future.shape)\n\n\n        return img_past, img_future\n\n    def decode_segmap(self, temp):\n        r = temp.copy()\n        g = temp.copy()\n        b = temp.copy()\n        for l in range(0, self.n_classes):\n            r[temp == l] = self.label_colours[l][0]\n            g[temp == l] = self.label_colours[l][1]\n            b[temp == l] = self.label_colours[l][2]\n\n        rgb = np.zeros((temp.shape[0], temp.shape[1], 3), dtype=np.uint8)\n        # rgb[:, :, 0] = r / 255.0\n        # rgb[:, :, 1] = g / 255.0\n        # rgb[:, :, 2] = b / 255.0\n        rgb[:, :, 0] = r\n        rgb[:, :, 1] = g\n        rgb[:, :, 2] = b\n        return rgb\n\n\nif __name__ == \'__main__\':\n    HOME_PATH = os.path.expanduser(\'~\')\n    local_path = os.path.join(HOME_PATH, \'Data/SegmPred\')\n    batch_size = 4\n    dst = segmpredLoader(local_path, is_transform=True, is_augment=False)\n    trainloader = data.DataLoader(dst, batch_size=batch_size, shuffle=True)\n    for i, (img_past, img_future) in enumerate(trainloader):\n        # print(i)\n        # print(img_past.shape)\n        # print(img_future.shape)\n        # if i == 0:\n        image_list_len = img_past.shape[0]\n        for image_list in range(image_list_len):\n            pred_segment = img_future.numpy()[image_list]\n            # print(\'img_future_onehot:\', img_future_onehot)\n            # print(\'img_future_onehot.shape:\', img_future_onehot.shape)\n            plt.imshow(dst.decode_segmap(pred_segment))\n            # print(\'dst.decode_segmap(pred_segment):\', dst.decode_segmap(pred_segment))\n        plt.show()\n        if i==0:\n            break\n'"
semseg/dataloader/tfrecords_loader.py,4,"b'# -*- coding: utf-8 -*-\nimport numpy as np\nimport cv2\nimport os\nimport time\n# import os.path as osp\nimport glob\nimport os, sys\n# import os.path as osp\nfrom PIL import Image\nimport six\nimport string\n\nimport lmdb\nimport pickle\nimport msgpack\nimport tqdm\nimport pyarrow\n\nimport torch\nimport torch.utils.data as data\nfrom torch.utils.data import DataLoader\nfrom torchvision.transforms import transforms\nfrom torchvision.datasets import ImageFolder\nfrom torchvision import transforms, datasets\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nimport os\nfrom PIL import Image\nimport tensorflow as tf\nimport random\nimport matplotlib.pyplot as plt\n\n\ndef _dtype_feature(ndarray):\n    """"""match appropriate tf.train.Feature class with dtype of ndarray. """"""\n    assert isinstance(ndarray, np.ndarray)\n    dtype_ = ndarray.dtype\n    if dtype_ == np.float64 or dtype_ == np.float32:\n        return lambda array: tf.train.Feature(float_list=tf.train.FloatList(value=array))\n    elif dtype_ == np.int64:\n        return lambda array: tf.train.Feature(int64_list=tf.train.Int64List(value=array))\n    else:\n        raise ValueError(""The input should be numpy ndarray. \\\n                           Instaed got {}"".format(ndarray.dtype))\n\n\ndef write_tfrecords_batch(start, end, labels, flower_dir, file):\n    writer = tf.python_io.TFRecordWriter(file)\n    batch = 2\n    widths = [236, 256, 276]\n    heights = [236, 256, 276]\n\n    width_resize = widths[0]\n    height_resize = heights[0]\n    imgs = []\n    for id in range(start, end):\n        img = Image.open(flower_dir[id])\n        # label = labels[id]\n        label = 0\n        width, height = img.size\n        # h = 500\n        # x = int((width - h) / 2)\n        # y = int((height - h) / 2)\n        # img_crop = img.crop([x, y, x + h, y + h])\n        # img_500 = img_crop.tobytes()\n        img = img.resize((width_resize, height_resize))\n        imgs.append(np.array(img).transpose(2, 1, 0))\n        if (id+1)%batch==0 and id!=0:\n            imgs = np.array(imgs, dtype=np.float)\n            imgs = imgs.reshape(-1)\n            print(\'imgs.shape:\', imgs.shape)\n            pass\n            example = tf.train.Example(features=tf.train.Features(feature={\n                # \'label\': tf.train.Feature(int64_list=tf.train.Int64List(value=[label])),\n                # \'img\': tf.train.Feature(bytes_list=tf.train.BytesList(value=[img_500])),\n                # \'img_batch\': tf.train.Feature(bytes_list=tf.train.BytesList(value=[img.tobytes()])),\n                # \'img_batch\': _dtype_feature(imgs),\n                # tf.train.Features(feature={""bytes"": _floats_feature(numpy_arr)})\n                # \'height\': tf.train.Feature(int64_list=tf.train.Int64List(value=[500])),\n                # \'width\': tf.train.Feature(int64_list=tf.train.Int64List(value=[500]))\n                \'imgs\': tf.train.Feature(float_list=tf.train.FloatList(value=imgs)),\n                \'height\': tf.train.Feature(int64_list=tf.train.Int64List(value=[height_resize])),\n                \'width\': tf.train.Feature(int64_list=tf.train.Int64List(value=[width_resize]))\n            }))\n            # writer_500.write(example_500.SerializeToString())\n            # print(example)\n            writer.write(example.SerializeToString())\n            imgs = []\n            resize_id = random.randint(0, 2)\n            width_resize = widths[resize_id]\n            height_resize = heights[resize_id]\n\n\n\ndef write_tfrecords(start, end, labels, flower_dir, file):\n    writer = tf.python_io.TFRecordWriter(file)\n    for id in range(start, end):\n        img = Image.open(flower_dir[id])\n        # label = labels[id]\n        label = 0\n        width, height = img.size\n        # h = 500\n        # x = int((width - h) / 2)\n        # y = int((height - h) / 2)\n        # img_crop = img.crop([x, y, x + h, y + h])\n        # img_500 = img_crop.tobytes()\n        example = tf.train.Example(features=tf.train.Features(feature={\n            \'label\': tf.train.Feature(int64_list=tf.train.Int64List(value=[label])),\n            # \'img\': tf.train.Feature(bytes_list=tf.train.BytesList(value=[img_500])),\n            \'img\': tf.train.Feature(bytes_list=tf.train.BytesList(value=[img.tobytes()])),\n            # \'height\': tf.train.Feature(int64_list=tf.train.Int64List(value=[500])),\n            # \'width\': tf.train.Feature(int64_list=tf.train.Int64List(value=[500]))\n            \'height\': tf.train.Feature(int64_list=tf.train.Int64List(value=[height])),\n            \'width\': tf.train.Feature(int64_list=tf.train.Int64List(value=[width]))\n        }))\n        # writer_500.write(example_500.SerializeToString())\n        writer.write(example.SerializeToString())\n\n\ndef flower_preprocess(flower_folder=\'/Users/cgf/Data/tmp/tfrecords\', tf_fn=""flower_train.tfrecords""):\n\n    labels = [0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0,\n              1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2]\n\n    random.shuffle(labels)\n    flower_dir = list()\n\n    for img in os.listdir(flower_folder):\n        flower_dir.append(os.path.join(flower_folder, img))\n    flower_dir.sort()\n\n    length = int(len(flower_dir)*0.7)\n    # file = ""flower_train.tfrecords""\n    # write_tfrecords(0, length, labels, flower_dir, tf_fn)\n    write_tfrecords_batch(0, length, labels, flower_dir, tf_fn)\n\n\nclass TFRecordsProxy():\n    def __init__(self, filenames, data_size):\n        self.filenames = filenames\n        self.data_size = data_size\n\n    def parse_data(self, example_proto):\n        features = {\n                    # \'img\': tf.FixedLenFeature([], tf.string, \'\'),\n                    \'imgs\': tf.VarLenFeature(tf.float32),\n                    # \'label\': tf.FixedLenFeature([], tf.int64, 0),\n                    # \'width\': tf.FixedLenFeature([], tf.int64, 0),\n                    # \'height\': tf.FixedLenFeature([], tf.int64, 0),\n                    }\n        parsed_features = tf.parse_single_example(example_proto, features)\n        # imgs = tf.decode_raw(parsed_features[\'imgs\'], tf.uint8)\n        label = 0\n        imgs = tf.cast(parsed_features[\'imgs\'], tf.float32)\n        # width = tf.cast(parsed_features[\'width\'], tf.int64)\n        # height = tf.cast(parsed_features[\'height\'], tf.int64)\n        # image = tf.reshape(image, tf.stack([height, width, 3]))\n        # print(\'width:\', width)\n        # print(\'height:\', height)\n        # image = tf.reshape(image, [height, width, 3])\n        # print(\'image:\', image)\n        return imgs, label\n        # return width, label\n\n    def my_input_fn(self, filenames, data_size):\n        reader = tf.TFRecordReader()\n\n        filename_queue = tf.train.string_input_producer([filenames], num_epochs=1)\n        _, serialized_example = reader.read(filename_queue)\n        batch = tf.train.batch(tensors=[serialized_example], batch_size=1)\n        features = {\n                    \'imgs\': tf.VarLenFeature(tf.float32),\n                    }\n        key_parsed = tf.parse_example(batch, features)\n        # print tf.contrib.learn.run_n(key_parsed)\n        # dataset = tf.contrib.data.TFRecordDataset(filenames)\n        # dataset = dataset.map(self.parse_data)\n        # # dataset = dataset.batch(data_size)\n        #\n        # # dataset = dataset.repeat()\n        # iterator = dataset.make_one_shot_iterator()\n        # features, labels = iterator.get_next()\n        # return features, labels\n        return key_parsed, 1\n\n    def main(self):\n        features, labels = self.my_input_fn(self.filenames, self.data_size)\n\n        with tf.Session() as sess:\n            init = tf.global_variables_initializer()\n            sess.run(init)\n            img, label = sess.run([features, labels])\n\n        return img, label\n\n\nclass TFRecordsDataset(torch.utils.data.Dataset):\n    def __init__(self, filename, data_sz):\n        self.data_tf, self.label_tf = TFRecordsProxy(filename, data_sz).main()\n\n    def __getitem__(self, index):\n        # data, label = self.data_tf[index], self.label_tf[index]\n        data, label = self.data_tf, self.label_tf\n        # print(\'data_tf_out:\', data_tf_out)\n        # data, label = self.data_tf[index, :, :, :], self.label_tf[index]\n        # print(\'data:\', data)\n        # print(\'label:\', label)\n\n        return data, label\n\n    def __len__(self):\n        # return len(self.data_tf)\n        return 20\n\n\nif __name__ == ""__main__"":\n    pass\n\n    # flower_preprocess(tf_fn=""flower_train_batch.tfrecords"")\n\n    tf_fn = os.path.join(os.path.expanduser(\'~/GitHub/Quick/pytorch-tfrecords/flower_train.tfrecords\'))\n    # tf_fn = os.path.join(os.path.expanduser(\'flower_train.tfrecords\'))\n    tf_fn = os.path.join(os.path.expanduser(\'flower_train_batch.tfrecords\'))\n    data_sz = 10\n    dataset = TFRecordsDataset(tf_fn, data_sz)\n    data_loader = torch.utils.data.DataLoader(dataset=dataset, batch_size=5, shuffle=True)\n\n    for batch_idx, (data, target) in enumerate(data_loader):\n        for i in range(0, len(data)):\n            print(i)\n            im = Image.fromarray(data[i, :, :, :].numpy())\n            # print(\'im.size\', im.size)\n            plt.imshow(im)\n            plt.show()\n            # im.save(str(epoch) + ""_train_"" + str(batch_idx) + ""_"" + str(i) + "".jpeg"")\n'"
semseg/dataloader/utils.py,0,"b'#!/usr/bin/python\n# -*- coding: UTF-8 -*-\n\nimport torch\nfrom torch import nn\nimport math\nimport numbers\nimport random\n\nfrom PIL import Image, ImageOps\nimport numpy as np\nimport os\n\ndef recursive_glob(rootdir=""."", suffix=""""):\n    """"""Performs recursive glob with given suffix and rootdir\n        :param rootdir is the root directory\n        :param suffix is the suffix to be searched\n    """"""\n    return [\n        os.path.join(looproot, filename)\n        for looproot, _, filenames in os.walk(rootdir)\n        for filename in filenames\n        if filename.endswith(suffix)\n    ]\n\n\nclass Compose(object):\n    def __init__(self, transforms):\n        self.transforms = transforms\n\n    def __call__(self, img, mask):\n        assert img.size == mask.size\n        for t in self.transforms:\n            img, mask = t(img, mask)\n        return img, mask\n\n\nclass RandomCrop(object):\n    def __init__(self, size, padding=0):\n        if isinstance(size, numbers.Number):\n            self.size = (int(size), int(size))\n        else:\n            self.size = size\n        self.padding = padding\n\n    def __call__(self, img, mask):\n        if self.padding > 0:\n            img = ImageOps.expand(img, border=self.padding, fill=0)\n            mask = ImageOps.expand(mask, border=self.padding, fill=0)\n\n        assert img.size == mask.size\n        w, h = img.size\n        th, tw = self.size\n        if w == tw and h == th:\n            return img, mask\n        if w < tw or h < th:\n            return img.resize((tw, th), Image.BILINEAR), mask.resize((tw, th), Image.NEAREST)\n\n        x1 = random.randint(0, w - tw)\n        y1 = random.randint(0, h - th)\n        return img.crop((x1, y1, x1 + tw, y1 + th)), mask.crop((x1, y1, x1 + tw, y1 + th))\n\n\nclass CenterCrop(object):\n    def __init__(self, size):\n        if isinstance(size, numbers.Number):\n            self.size = (int(size), int(size))\n        else:\n            self.size = size\n\n    def __call__(self, img, mask):\n        assert img.size == mask.size\n        w, h = img.size\n        th, tw = self.size\n        x1 = int(round((w - tw) / 2.))\n        y1 = int(round((h - th) / 2.))\n        return img.crop((x1, y1, x1 + tw, y1 + th)), mask.crop((x1, y1, x1 + tw, y1 + th))\n\n\nclass RandomHorizontallyFlip(object):\n    def __call__(self, img, mask):\n        if random.random() < 0.5:\n            # print(\'RandomHorizontallyFlip----FLIP_LEFT_RIGHT\')\n            return img.transpose(Image.FLIP_LEFT_RIGHT), mask.transpose(Image.FLIP_LEFT_RIGHT)\n        return img, mask\n\n\nclass FreeScale(object):\n    def __init__(self, size):\n        self.size = tuple(reversed(size))  # size: (h, w)\n\n    def __call__(self, img, mask):\n        assert img.size == mask.size\n        return img.resize(self.size, Image.BILINEAR), mask.resize(self.size, Image.NEAREST)\n\n\nclass Scale(object):\n    def __init__(self, size):\n        self.size = size\n\n    def __call__(self, img, mask):\n        assert img.size == mask.size\n        w, h = img.size\n        if (w >= h and w == self.size) or (h >= w and h == self.size):\n            return img, mask\n        if w > h:\n            ow = self.size\n            oh = int(self.size * h / w)\n            return img.resize((ow, oh), Image.BILINEAR), mask.resize((ow, oh), Image.NEAREST)\n        else:\n            oh = self.size\n            ow = int(self.size * w / h)\n            return img.resize((ow, oh), Image.BILINEAR), mask.resize((ow, oh), Image.NEAREST)\n\n\nclass RandomSizedCrop(object):\n    def __init__(self, size):\n        self.size = size\n\n    def __call__(self, img, mask):\n        assert img.size == mask.size\n        for attempt in range(10):\n            area = img.size[0] * img.size[1]\n            target_area = random.uniform(0.45, 1.0) * area\n            aspect_ratio = random.uniform(0.5, 2)\n\n            w = int(round(math.sqrt(target_area * aspect_ratio)))\n            h = int(round(math.sqrt(target_area / aspect_ratio)))\n\n            if random.random() < 0.5:\n                w, h = h, w\n\n            if w <= img.size[0] and h <= img.size[1]:\n                x1 = random.randint(0, img.size[0] - w)\n                y1 = random.randint(0, img.size[1] - h)\n\n                img = img.crop((x1, y1, x1 + w, y1 + h))\n                mask = mask.crop((x1, y1, x1 + w, y1 + h))\n                assert (img.size == (w, h))\n\n                return img.resize((self.size, self.size), Image.BILINEAR), mask.resize((self.size, self.size), Image.NEAREST)\n\n        # Fallback\n        scale = Scale(self.size)\n        crop = CenterCrop(self.size)\n        return crop(*scale(img, mask))\n\n\nclass RandomRotate(object):\n    def __init__(self, degree):\n        self.degree = degree\n\n    def __call__(self, img, mask):\n        rotate_degree = random.random() * 2 * self.degree - self.degree\n        # print(\'RandomRotate----{}\'.format(rotate_degree))\n        return img.rotate(rotate_degree, Image.BILINEAR), mask.rotate(rotate_degree, Image.NEAREST)\n\n\nclass RandomSized(object):\n    def __init__(self, size):\n        self.size = size\n        self.scale = Scale(self.size)\n        self.crop = RandomCrop(self.size)\n\n    def __call__(self, img, mask):\n        assert img.size == mask.size\n\n        w = int(random.uniform(0.5, 2) * img.size[0])\n        h = int(random.uniform(0.5, 2) * img.size[1])\n\n        img, mask = img.resize((w, h), Image.BILINEAR), mask.resize((w, h), Image.NEAREST)\n\n        return self.crop(*self.scale(img, mask))\n\n\nclass SlidingCropOld(object):\n    def __init__(self, crop_size, stride_rate, ignore_label):\n        self.crop_size = crop_size\n        self.stride_rate = stride_rate\n        self.ignore_label = ignore_label\n\n    def _pad(self, img, mask):\n        h, w = img.shape[: 2]\n        pad_h = max(self.crop_size - h, 0)\n        pad_w = max(self.crop_size - w, 0)\n        img = np.pad(img, ((0, pad_h), (0, pad_w), (0, 0)), \'constant\')\n        mask = np.pad(mask, ((0, pad_h), (0, pad_w)), \'constant\', constant_values=self.ignore_label)\n        return img, mask\n\n    def __call__(self, img, mask):\n        assert img.size == mask.size\n\n        w, h = img.size\n        long_size = max(h, w)\n\n        img = np.array(img)\n        mask = np.array(mask)\n\n        if long_size > self.crop_size:\n            stride = int(math.ceil(self.crop_size * self.stride_rate))\n            h_step_num = int(math.ceil((h - self.crop_size) / float(stride))) + 1\n            w_step_num = int(math.ceil((w - self.crop_size) / float(stride))) + 1\n            img_sublist, mask_sublist = [], []\n            for yy in xrange(h_step_num):\n                for xx in xrange(w_step_num):\n                    sy, sx = yy * stride, xx * stride\n                    ey, ex = sy + self.crop_size, sx + self.crop_size\n                    img_sub = img[sy: ey, sx: ex, :]\n                    mask_sub = mask[sy: ey, sx: ex]\n                    img_sub, mask_sub = self._pad(img_sub, mask_sub)\n                    img_sublist.append(Image.fromarray(img_sub.astype(np.uint8)).convert(\'RGB\'))\n                    mask_sublist.append(Image.fromarray(mask_sub.astype(np.uint8)).convert(\'P\'))\n            return img_sublist, mask_sublist\n        else:\n            img, mask = self._pad(img, mask)\n            img = Image.fromarray(img.astype(np.uint8)).convert(\'RGB\')\n            mask = Image.fromarray(mask.astype(np.uint8)).convert(\'P\')\n            return img, mask\n\n\nclass SlidingCrop(object):\n    def __init__(self, crop_size, stride_rate, ignore_label):\n        self.crop_size = crop_size\n        self.stride_rate = stride_rate\n        self.ignore_label = ignore_label\n\n    def _pad(self, img, mask):\n        h, w = img.shape[: 2]\n        pad_h = max(self.crop_size - h, 0)\n        pad_w = max(self.crop_size - w, 0)\n        img = np.pad(img, ((0, pad_h), (0, pad_w), (0, 0)), \'constant\')\n        mask = np.pad(mask, ((0, pad_h), (0, pad_w)), \'constant\', constant_values=self.ignore_label)\n        return img, mask, h, w\n\n    def __call__(self, img, mask):\n        assert img.size == mask.size\n\n        w, h = img.size\n        long_size = max(h, w)\n\n        img = np.array(img)\n        mask = np.array(mask)\n\n        if long_size > self.crop_size:\n            stride = int(math.ceil(self.crop_size * self.stride_rate))\n            h_step_num = int(math.ceil((h - self.crop_size) / float(stride))) + 1\n            w_step_num = int(math.ceil((w - self.crop_size) / float(stride))) + 1\n            img_slices, mask_slices, slices_info = [], [], []\n            for yy in xrange(h_step_num):\n                for xx in xrange(w_step_num):\n                    sy, sx = yy * stride, xx * stride\n                    ey, ex = sy + self.crop_size, sx + self.crop_size\n                    img_sub = img[sy: ey, sx: ex, :]\n                    mask_sub = mask[sy: ey, sx: ex]\n                    img_sub, mask_sub, sub_h, sub_w = self._pad(img_sub, mask_sub)\n                    img_slices.append(Image.fromarray(img_sub.astype(np.uint8)).convert(\'RGB\'))\n                    mask_slices.append(Image.fromarray(mask_sub.astype(np.uint8)).convert(\'P\'))\n                    slices_info.append([sy, ey, sx, ex, sub_h, sub_w])\n            return img_slices, mask_slices, slices_info\n        else:\n            img, mask, sub_h, sub_w = self._pad(img, mask)\n            img = Image.fromarray(img.astype(np.uint8)).convert(\'RGB\')\n            mask = Image.fromarray(mask.astype(np.uint8)).convert(\'P\')\n            return [img], [mask], [[0, sub_h, 0, sub_w, sub_h, sub_w]]\n'"
semseg/dataloader/yolodataset_loader.py,10,"b'# -*- coding: utf-8 -*-\n""""""\ntxt\xe6\x8f\x8f\xe8\xbf\xb0\xe6\x96\x87\xe4\xbb\xb6 image_name.jpg x y w h c x y w h c \xe8\xbf\x99\xe6\xa0\xb7\xe5\xb0\xb1\xe6\x98\xaf\xe8\xaf\xb4\xe4\xb8\x80\xe5\xbc\xa0\xe5\x9b\xbe\xe7\x89\x87\xe4\xb8\xad\xe6\x9c\x89\xe4\xb8\xa4\xe4\xb8\xaa\xe7\x9b\xae\xe6\xa0\x87\n""""""\nimport os\nimport random\nimport numpy as np\nimport torch\nimport torch.utils.data as data\nimport cv2\n\n\nclass yoloDataset(data.Dataset):\n    image_size = 448\n\n    def __init__(self, root, list_file, train, transform, yolo_out_tensor_shape):\n        print(\'data init\')\n        self.root = root\n        self.train = train\n        self.transform = transform\n        self.fnames = []\n        self.boxes = []\n        self.labels = []\n        self.yolo_out_tensor_shape = yolo_out_tensor_shape\n        self.mean = (123, 117, 104)  # RGB\n\n        if isinstance(list_file, list):\n            # Cat multiple list files together.\n            # This is especially useful for voc07/voc12 combination.\n            tmp_file = \'/tmp/listfile.txt\'\n            print(\'cat %s > %s\' % (\' \'.join(list_file), tmp_file))\n            os.system(\'cat %s > %s\' % (\' \'.join(list_file), tmp_file))\n            list_file = tmp_file  # voc2007.txt or voc2012.txt or voc2007.txt+voc2012.txt\n\n        with open(list_file) as f:\n            lines = f.readlines()\n\n        for line in lines:\n            splited = line.strip().split()\n            # print(\'splited:\', splited)\n            self.fnames.append(splited[0])  # image file names\n            num_boxes = (len(splited) - 1) // 5  # boxes num filename + 5*(x, y, x\', y\', class)\n            box = []\n            label = []\n            for i in range(num_boxes):\n                x = float(splited[1 + 5 * i])\n                y = float(splited[2 + 5 * i])\n                x2 = float(splited[3 + 5 * i])\n                y2 = float(splited[4 + 5 * i])\n                c = splited[5 + 5 * i]\n                box.append([x, y, x2, y2])\n                label.append(int(c) + 1)\n            # boxes and label\n            self.boxes.append(torch.Tensor(box))\n            self.labels.append(torch.LongTensor(label))\n        self.num_samples = len(self.boxes)\n\n    def __getitem__(self, idx):\n        fname = self.fnames[idx]\n        img_ori = cv2.imread(os.path.join(self.root + fname))\n        img = img_ori\n        boxes = self.boxes[idx].clone()\n        labels = self.labels[idx].clone()\n\n        if self.train:\n            # img = self.random_bright(img)\n            img, boxes = self.random_flip(img, boxes)\n            img, boxes = self.randomScale(img, boxes)\n            img = self.randomBlur(img)\n            img = self.RandomBrightness(img)\n            img = self.RandomHue(img)\n            img = self.RandomSaturation(img)\n            img, boxes, labels = self.randomShift(img, boxes, labels)\n            img, boxes, labels = self.randomCrop(img, boxes, labels)\n        # #debug\n        # box_show = boxes.numpy().reshape(-1)\n        # print(box_show)\n        # img_show = self.BGR2RGB(img)\n        # pt1=(int(box_show[0]),int(box_show[1])); pt2=(int(box_show[2]),int(box_show[3]))\n        # cv2.rectangle(img_show,pt1=pt1,pt2=pt2,color=(0,255,0),thickness=1)\n        # plt.figure()\n\n        # # cv2.rectangle(img,pt1=(10,10),pt2=(100,100),color=(0,255,0),thickness=1)\n        # plt.imshow(img_show)\n        # plt.show()\n        # #debug\n        h, w, _ = img.shape\n        boxes /= torch.Tensor([w, h, w, h]).expand_as(boxes)\n        img = self.BGR2RGB(img)  # because pytorch pretrained model use RGB\n        img = self.subMean(img, self.mean)  # \xe5\x87\x8f\xe5\x8e\xbb\xe5\x9d\x87\xe5\x80\xbc\n        img = cv2.resize(img, (self.image_size, self.image_size))\n        target = self.encoder(boxes, labels)  # 7x7x30\n        for t in self.transform:\n            img = t(img)\n\n        # img /= 255.0\n        return img, target, img_ori\n\n    def __len__(self):\n        return self.num_samples\n\n    def encoder(self, boxes, labels):\n        \'\'\'\n        boxes (tensor) [[x1,y1,x2,y2],[]]\n        labels (tensor) [...]\n        return 7x7x30\n        \'\'\'\n        grid_num = 14\n        target = torch.zeros((grid_num, grid_num, self.yolo_out_tensor_shape))\n        cell_size = 1. / grid_num\n        wh = boxes[:, 2:] - boxes[:, :2]\n        cxcy = (boxes[:, 2:] + boxes[:, :2]) / 2\n        for i in range(cxcy.size()[0]):\n            cxcy_sample = cxcy[i]\n            ij = (cxcy_sample / cell_size).ceil() - 1  #\n            target[int(ij[1]), int(ij[0]), 4] = 1\n            target[int(ij[1]), int(ij[0]), 9] = 1\n            target[int(ij[1]), int(ij[0]), int(labels[i]) + 9] = 1\n            xy = ij * cell_size  # \xe5\x8c\xb9\xe9\x85\x8d\xe5\x88\xb0\xe7\x9a\x84\xe7\xbd\x91\xe6\xa0\xbc\xe7\x9a\x84\xe5\xb7\xa6\xe4\xb8\x8a\xe8\xa7\x92\xe7\x9b\xb8\xe5\xaf\xb9\xe5\x9d\x90\xe6\xa0\x87\n            delta_xy = (cxcy_sample - xy) / cell_size\n            target[int(ij[1]), int(ij[0]), 2:4] = wh[i]\n            target[int(ij[1]), int(ij[0]), :2] = delta_xy\n            target[int(ij[1]), int(ij[0]), 7:9] = wh[i]\n            target[int(ij[1]), int(ij[0]), 5:7] = delta_xy\n        return target\n\n    def BGR2RGB(self, img):\n        return cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n\n    def BGR2HSV(self, img):\n        return cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n\n    def HSV2BGR(self, img):\n        return cv2.cvtColor(img, cv2.COLOR_HSV2BGR)\n\n    def RandomBrightness(self, bgr):\n        if random.random() < 0.5:\n            hsv = self.BGR2HSV(bgr)\n            h, s, v = cv2.split(hsv)\n            adjust = random.choice([0.5, 1.5])\n            v = v * adjust\n            v = np.clip(v, 0, 255).astype(hsv.dtype)\n            hsv = cv2.merge((h, s, v))\n            bgr = self.HSV2BGR(hsv)\n        return bgr\n\n    def RandomSaturation(self, bgr):\n        if random.random() < 0.5:\n            hsv = self.BGR2HSV(bgr)\n            h, s, v = cv2.split(hsv)\n            adjust = random.choice([0.5, 1.5])\n            s = s * adjust\n            s = np.clip(s, 0, 255).astype(hsv.dtype)\n            hsv = cv2.merge((h, s, v))\n            bgr = self.HSV2BGR(hsv)\n        return bgr\n\n    def RandomHue(self, bgr):\n        if random.random() < 0.5:\n            hsv = self.BGR2HSV(bgr)\n            h, s, v = cv2.split(hsv)\n            adjust = random.choice([0.5, 1.5])\n            h = h * adjust\n            h = np.clip(h, 0, 255).astype(hsv.dtype)\n            hsv = cv2.merge((h, s, v))\n            bgr = self.HSV2BGR(hsv)\n        return bgr\n\n    def randomBlur(self, bgr):\n        if random.random() < 0.5:\n            bgr = cv2.blur(bgr, (5, 5))\n        return bgr\n\n    def randomShift(self, bgr, boxes, labels):\n        # \xe5\xb9\xb3\xe7\xa7\xbb\xe5\x8f\x98\xe6\x8d\xa2\n        center = (boxes[:, 2:] + boxes[:, :2]) / 2\n        if random.random() < 0.5:\n            height, width, c = bgr.shape\n            after_shfit_image = np.zeros((height, width, c), dtype=bgr.dtype)\n            after_shfit_image[:, :, :] = (104, 117, 123)  # bgr\n            shift_x = random.uniform(-width * 0.2, width * 0.2)\n            shift_y = random.uniform(-height * 0.2, height * 0.2)\n            # print(bgr.shape,shift_x,shift_y)\n            # \xe5\x8e\x9f\xe5\x9b\xbe\xe5\x83\x8f\xe7\x9a\x84\xe5\xb9\xb3\xe7\xa7\xbb\n            if shift_x >= 0 and shift_y >= 0:\n                after_shfit_image[int(shift_y):, int(shift_x):, :] = bgr[:height - int(shift_y), :width - int(shift_x),\n                                                                     :]\n            elif shift_x >= 0 and shift_y < 0:\n                after_shfit_image[:height + int(shift_y), int(shift_x):, :] = bgr[-int(shift_y):, :width - int(shift_x),\n                                                                              :]\n            elif shift_x < 0 and shift_y >= 0:\n                after_shfit_image[int(shift_y):, :width + int(shift_x), :] = bgr[:height - int(shift_y), -int(shift_x):,\n                                                                             :]\n            elif shift_x < 0 and shift_y < 0:\n                after_shfit_image[:height + int(shift_y), :width + int(shift_x), :] = bgr[-int(shift_y):,\n                                                                                      -int(shift_x):, :]\n\n            shift_xy = torch.FloatTensor([[int(shift_x), int(shift_y)]]).expand_as(center)\n            center = center + shift_xy\n            mask1 = (center[:, 0] > 0) & (center[:, 0] < width)\n            mask2 = (center[:, 1] > 0) & (center[:, 1] < height)\n            mask = (mask1 & mask2).view(-1, 1)\n            boxes_in = boxes[mask.expand_as(boxes)].view(-1, 4)\n            if len(boxes_in) == 0:\n                return bgr, boxes, labels\n            box_shift = torch.FloatTensor([[int(shift_x), int(shift_y), int(shift_x), int(shift_y)]]).expand_as(\n                boxes_in)\n            boxes_in = boxes_in + box_shift\n            labels_in = labels[mask.view(-1)]\n            return after_shfit_image, boxes_in, labels_in\n        return bgr, boxes, labels\n\n    def randomScale(self, bgr, boxes):\n        # \xe5\x9b\xba\xe5\xae\x9a\xe4\xbd\x8f\xe9\xab\x98\xe5\xba\xa6\xef\xbc\x8c\xe4\xbb\xa50.8-1.2\xe4\xbc\xb8\xe7\xbc\xa9\xe5\xae\xbd\xe5\xba\xa6\xef\xbc\x8c\xe5\x81\x9a\xe5\x9b\xbe\xe5\x83\x8f\xe5\xbd\xa2\xe5\x8f\x98\n        if random.random() < 0.5:\n            scale = random.uniform(0.8, 1.2)\n            height, width, c = bgr.shape\n            bgr = cv2.resize(bgr, (int(width * scale), height))\n            scale_tensor = torch.FloatTensor([[scale, 1, scale, 1]]).expand_as(boxes)\n            boxes = boxes * scale_tensor\n            return bgr, boxes\n        return bgr, boxes\n\n    def randomCrop(self, bgr, boxes, labels):\n        if random.random() < 0.5:\n            center = (boxes[:, 2:] + boxes[:, :2]) / 2\n            height, width, c = bgr.shape\n            h = random.uniform(0.6 * height, height)\n            w = random.uniform(0.6 * width, width)\n            x = random.uniform(0, width - w)\n            y = random.uniform(0, height - h)\n            x, y, h, w = int(x), int(y), int(h), int(w)\n\n            center = center - torch.FloatTensor([[x, y]]).expand_as(center)\n            mask1 = (center[:, 0] > 0) & (center[:, 0] < w)\n            mask2 = (center[:, 1] > 0) & (center[:, 1] < h)\n            mask = (mask1 & mask2).view(-1, 1)\n\n            boxes_in = boxes[mask.expand_as(boxes)].view(-1, 4)\n            if (len(boxes_in) == 0):\n                return bgr, boxes, labels\n            box_shift = torch.FloatTensor([[x, y, x, y]]).expand_as(boxes_in)\n\n            boxes_in = boxes_in - box_shift\n            boxes_in[:, 0] = boxes_in[:, 0].clamp_(min=0, max=w)\n            boxes_in[:, 2] = boxes_in[:, 2].clamp_(min=0, max=w)\n            boxes_in[:, 1] = boxes_in[:, 1].clamp_(min=0, max=h)\n            boxes_in[:, 3] = boxes_in[:, 3].clamp_(min=0, max=h)\n\n            labels_in = labels[mask.view(-1)]\n            img_croped = bgr[y:y + h, x:x + w, :]\n            return img_croped, boxes_in, labels_in\n        return bgr, boxes, labels\n\n    def subMean(self, bgr, mean):\n        mean = np.array(mean, dtype=np.float32)\n        bgr = bgr - mean\n        return bgr\n\n    def random_flip(self, im, boxes):\n        if random.random() < 0.5:\n            # print(\'im.shape:\', im.shape)\n            # print(\'im:\', im)\n            im_lr = np.fliplr(im).copy()\n            h, w, _ = im.shape\n            xmin = w - boxes[:, 2]\n            xmax = w - boxes[:, 0]\n            boxes[:, 0] = xmin\n            boxes[:, 2] = xmax\n            return im_lr, boxes\n        return im, boxes\n\n    def random_bright(self, im, delta=16):\n        alpha = random.random()\n        if alpha > 0.3:\n            im = im * alpha + random.randrange(-delta, delta)\n            im = im.clip(min=0, max=255).astype(np.uint8)\n        return im\n'"
semseg/modelloader/EDANet.py,7,"b""# -*- coding: utf-8 -*-\n# code is from https://github.com/wpf535236337/pytorch_EDANet/blob/master/EDANet.py\nimport time\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nimport numpy as np\n\nfrom semseg.loss import cross_entropy2d\n\n\nclass DownsamplerBlock(nn.Module):\n    def __init__(self, ninput, noutput):\n        super(DownsamplerBlock, self).__init__()\n\n        self.ninput = ninput\n        self.noutput = noutput\n\n        if self.ninput < self.noutput:\n            # Wout > Win\n            self.conv = nn.Conv2d(ninput, noutput - ninput, kernel_size=3, stride=2, padding=1)\n            self.pool = nn.MaxPool2d(2, stride=2)\n        else:\n            # Wout < Win\n            self.conv = nn.Conv2d(ninput, noutput, kernel_size=3, stride=2, padding=1)\n\n        self.bn = nn.BatchNorm2d(noutput)\n\n    def forward(self, x):\n        if self.ninput < self.noutput:\n            output = torch.cat([self.conv(x), self.pool(x)], 1)\n        else:\n            output = self.conv(x)\n\n        output = self.bn(output)\n        return F.relu(output)\n\n\nclass EDABlock(nn.Module):\n    def __init__(self, ninput, dilated, k=40, dropprob=0.02):\n        super(EDABlock, self).__init__()\n\n        # k: growthrate\n        # dropprob:a dropout layer between the last ReLU and the concatenation of each module\n\n        self.conv1x1 = nn.Conv2d(ninput, k, kernel_size=1)\n        self.bn0 = nn.BatchNorm2d(k)\n\n        self.conv3x1_1 = nn.Conv2d(k, k, kernel_size=(3, 1), padding=(1, 0))\n        self.conv1x3_1 = nn.Conv2d(k, k, kernel_size=(1, 3), padding=(0, 1))\n        # self.conv3x3_1 = nn.Conv2d(k, k, kernel_size=(3, 3), padding=(1, 1))\n        self.bn1 = nn.BatchNorm2d(k)\n\n        self.conv3x1_2 = nn.Conv2d(k, k, (3, 1), stride=1, padding=(dilated, 0), dilation=dilated)\n        self.conv1x3_2 = nn.Conv2d(k, k, (1, 3), stride=1, padding=(0, dilated), dilation=dilated)\n        # self.conv3x3_2 = nn.Conv2d(k, k, (3, 3), stride=1, padding=(dilated, dilated), dilation=dilated)\n        self.bn2 = nn.BatchNorm2d(k)\n\n        self.dropout = nn.Dropout2d(dropprob)\n\n    def forward(self, x):\n        input = x\n\n        output = self.conv1x1(x)\n        output = self.bn0(output)\n        output = F.relu(output)\n\n        output = self.conv3x1_1(output)\n        output = self.conv1x3_1(output)\n        # output = self.conv3x3_1(output)\n\n        output = self.bn1(output)\n        output = F.relu(output)\n\n        output = self.conv3x1_2(output)\n        output = self.conv1x3_2(output)\n        # output = self.conv3x3_2(output)\n        output = self.bn2(output)\n        output = F.relu(output)\n\n        if (self.dropout.p != 0):\n            output = self.dropout(output)\n\n        output = torch.cat([output, input], 1)\n        # print output.size() #check the output\n        return output\n\n\nclass EDANet(nn.Module):\n    def __init__(self, n_classes=20, pretrained=False):\n        super(EDANet, self).__init__()\n\n        self.layers = nn.ModuleList()\n        self.dilation1 = [1, 1, 1, 2, 2]\n        self.dilation2 = [2, 2, 4, 4, 8, 8, 16, 16]\n\n        # DownsamplerBlock1\n        self.layers.append(DownsamplerBlock(3, 15))\n\n        # DownsamplerBlock2\n        self.layers.append(DownsamplerBlock(15, 60))\n\n        # EDA module 1-1 ~ 1-5\n        for i in range(5):\n            self.layers.append(EDABlock(60 + 40 * i, self.dilation1[i]))\n\n        # DownsamplerBlock3\n        self.layers.append(DownsamplerBlock(260, 130))\n\n        # EDA module 2-1 ~ 2-8\n        for j in range(8):\n            self.layers.append(EDABlock(130 + 40 * j, self.dilation2[j]))\n\n        # Projection layer\n        self.project_layer = nn.Conv2d(450, n_classes, kernel_size=1)\n\n        self.weights_init()\n\n    def weights_init(self):\n        for idx, m in enumerate(self.modules()):\n            classname = m.__class__.__name__\n            if classname.find('Conv') != -1:\n                m.weight.data.normal_(0.0, 0.02)\n            elif classname.find('BatchNorm') != -1:\n                m.weight.data.normal_(1.0, 0.02)\n                m.bias.data.fill_(0)\n\n    def forward(self, x):\n\n        output = x\n\n        for layer in self.layers:\n            output = layer(output)\n\n        output = self.project_layer(output)\n\n        # Bilinear interpolation x8\n        output = F.upsample(output, scale_factor=8, mode='bilinear')\n\n        # Bilinear interpolation x2 (inference only)\n        if not self.training:\n            output = F.upsample(output, scale_factor=2, mode='bilinear')\n\n        return output\n\nif __name__ == '__main__':\n    batch_size = 1\n    n_classes = 21\n    model = EDANet(n_classes=n_classes)\n    x = Variable(torch.randn(1, 3, 360, 480))\n    y = Variable(torch.LongTensor(np.ones((1, 360, 480), dtype=np.int)))\n    # print(x.shape)\n    start = time.time()\n    pred = model(x)\n    end = time.time()\n    print(end-start)\n    print(pred.shape)\n    print('pred.type:', pred.type)\n    loss = cross_entropy2d(pred, y)\n    # print(loss)\n"""
semseg/modelloader/__init__.py,0,b''
semseg/modelloader/bisenet.py,18,"b'# -*- coding: utf-8 -*-\n# !!!code is from [BiSeNet](https://github.com/ooooverflow/BiSeNet/blob/master/model/build_BiSeNet.py)!!!\n# please refer the origin implement and I will just use the code for my own usage\n\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\nfrom torchvision import models\nfrom torch.autograd import Variable\nimport numpy as np\nimport time\n\n# ----Context Module----\nclass resnet18(nn.Module):\n    def __init__(self, pretrained=True):\n        super(resnet18, self).__init__()\n        self.features = models.resnet18(pretrained=pretrained)\n        self.conv1 = self.features.conv1\n        self.bn1 = self.features.bn1\n        self.relu = self.features.relu\n        self.maxpool1 = self.features.maxpool\n        self.layer1 = self.features.layer1\n        self.layer2 = self.features.layer2\n        self.layer3 = self.features.layer3\n        self.layer4 = self.features.layer4\n\n    def forward(self, input):\n        x = self.conv1(input)\n        x = self.relu(self.bn1(x))\n        x = self.maxpool1(x)\n        feature1 = self.layer1(x)             # 1 / 4\n        feature2 = self.layer2(feature1)      # 1 / 8\n        feature3 = self.layer3(feature2)      # 1 / 16\n        feature4 = self.layer4(feature3)      # 1 / 32\n\n        # global average pooling to build tail\n        tail = torch.mean(feature4, 3, keepdim=True)\n        tail = torch.mean(tail, 2, keepdim=True)\n        # 16 and 32 down features are used with ARM\n        return feature3, feature4, tail\n\n\nclass resnet101(nn.Module):\n    def __init__(self, pretrained=True):\n        super(resnet101, self).__init__()\n        self.features = models.resnet101(pretrained=pretrained)\n        self.conv1 = self.features.conv1\n        self.bn1 = self.features.bn1\n        self.relu = self.features.relu\n        self.maxpool1 = self.features.maxpool\n        self.layer1 = self.features.layer1\n        self.layer2 = self.features.layer2\n        self.layer3 = self.features.layer3\n        self.layer4 = self.features.layer4\n\n    def forward(self, input):\n        x = self.conv1(input)\n        x = self.relu(self.bn1(x))\n        x = self.maxpool1(x)\n        feature1 = self.layer1(x)             # 1 / 4\n        feature2 = self.layer2(feature1)      # 1 / 8\n        feature3 = self.layer3(feature2)      # 1 / 16\n        feature4 = self.layer4(feature3)      # 1 / 32\n        # global average pooling to build tail\n        tail = torch.mean(feature4, 3, keepdim=True)\n        tail = torch.mean(tail, 2, keepdim=True)\n        return feature3, feature4, tail\n\n\ndef Context_path(name, pretrained=False):\n    if name==\'resnet18\':\n        return resnet18(pretrained=pretrained)\n    elif name==\'resnet101\':\n        return resnet101(pretrained=pretrained)\n\n# ----Context Module----\n\nclass ConvBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size=3, stride=2,padding=1):\n        super(ConvBlock, self).__init__()\n        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding)\n        self.bn = nn.BatchNorm2d(out_channels)\n        self.relu = nn.ReLU()\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn(x)\n        x= self.relu(x)\n        return x\n\nclass Spatial_path(nn.Module):\n    def __init__(self):\n        """"""\n        Spatial Path is combined by 3 blocks including Conv+BN+ReLU, and here every block is 2 stride\n        """"""\n        super(Spatial_path, self).__init__()\n        self.convblock1 = ConvBlock(in_channels=3, out_channels=64)\n        self.convblock2 = ConvBlock(in_channels=64, out_channels=128)\n        self.convblock3 = ConvBlock(in_channels=128, out_channels=256)\n\n    def forward(self, x):\n        x = self.convblock1(x)\n        x = self.convblock2(x)\n        x = self.convblock3(x)\n        return x\n\nclass AttentionRefinementModule(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super(AttentionRefinementModule, self).__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n        self.bn = nn.BatchNorm2d(out_channels)\n        self.sigmoid = nn.Sigmoid()\n        self.in_channels = in_channels\n\n    def forward(self, input):\n        # global average pooling\n        x = torch.mean(input, 3, keepdim=True)\n        # print(\'input.shape:\', input.shape)\n        # print(\'x.shape:\', x.shape)\n        x = torch.mean(x, 2, keepdim=True)\n        # print(\'input.shape:\', input.shape)\n        # print(\'x.shape:\', x.shape)\n        assert self.in_channels == x.size(1), \'in_channels and out_channels should all be {}\'.format(x.size(1))\n        x = self.conv(x)\n        # x = self.bn(x)\n        x = self.sigmoid(x)\n        # channels of input and x should be same\n        x = torch.mul(input, x)\n        return x\n\n\nclass FeatureFusionModule(nn.Module):\n    def __init__(self, num_classes, in_channels):\n        super(FeatureFusionModule, self).__init__()\n        # self.in_channels = input_1.channels + input_2.channels\n        # self.in_channels = 3328\n        self.in_channels = in_channels\n        self.convblock = ConvBlock(in_channels=self.in_channels, out_channels=num_classes, stride=1)\n        self.conv1 = nn.Conv2d(num_classes, num_classes, kernel_size=1)\n        self.relu = nn.ReLU()\n        self.conv2 = nn.Conv2d(num_classes, num_classes, kernel_size=1)\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, input_1, input_2):\n        x = torch.cat((input_1, input_2), dim=1)\n        assert self.in_channels == x.size(1), \'in_channels of ConvBlock should be {}\'.format(x.size(1))\n        feature = self.convblock(x)\n        x = torch.mean(feature, 3, keepdim=True)\n        x = torch.mean(x, 2 ,keepdim=True)\n        x = self.relu(self.conv1(x))\n        x = self.sigmoid(self.relu(x))\n        x = torch.mul(feature, x)\n        x = torch.add(x, feature)\n        return x\n\nclass BiSeNet(nn.Module):\n    def __init__(self, n_classes=21, pretrained=True, context_path=\'resnet18\'):\n        super(BiSeNet, self).__init__()\n        self.n_classes = n_classes\n\n        # build spatial path\n        self.saptial_path = Spatial_path()\n\n        # build context path\n        self.context_path = Context_path(name=context_path, pretrained=pretrained)\n\n        # build attention refinement module\n        if context_path==\'resnet18\':\n            self.attention_refinement_module1 = AttentionRefinementModule(256, 256)\n            self.attention_refinement_module2 = AttentionRefinementModule(512, 512)\n        elif context_path==\'resnet101\':\n            self.attention_refinement_module1 = AttentionRefinementModule(1024, 1024)\n            self.attention_refinement_module2 = AttentionRefinementModule(2048, 2048)\n\n        # build feature fusion module\n        if context_path==\'resnet18\':\n            self.feature_fusion_module = FeatureFusionModule(self.n_classes, in_channels=1024)\n        elif context_path==\'resnet101\':\n            self.feature_fusion_module = FeatureFusionModule(self.n_classes, in_channels=3328)\n\n        # build final convolution\n        self.conv = nn.Conv2d(in_channels=self.n_classes, out_channels=self.n_classes, kernel_size=1)\n\n    def forward(self, input):\n        input_size = input.size()\n        # output of spatial path\n        sx = self.saptial_path(input)\n        # print(\'sx.shape:\', sx.shape)\n\n        # output of context path\n        cx1, cx2, tail = self.context_path(input)\n        # print(\'cx1.shape:\', cx1.shape)\n        # print(\'cx2.shape:\', cx2.shape)\n        # print(\'tail.shape:\', tail.shape)\n        cx1 = self.attention_refinement_module1(cx1)\n        cx2 = self.attention_refinement_module2(cx2)\n        cx2 = torch.mul(cx2, tail)\n        # upsampling\n        cx1 = F.upsample_bilinear(cx1, (input_size[2]//8, input_size[3]//8))\n        cx2 = F.upsample_bilinear(cx2, (input_size[2]//8, input_size[3]//8))\n        # print(\'cx1.shape:\', cx1.shape)\n        # print(\'cx2.shape:\', cx2.shape)\n        cx = torch.cat((cx1, cx2), dim=1)\n        # print(\'cx.shape:\', cx.shape)\n\n        # output of feature fusion module\n        result = self.feature_fusion_module(sx, cx)\n\n        # upsampling\n        result = F.upsample_bilinear(result, scale_factor=8)\n        result = self.conv(result)\n        return result\n\nif __name__ == \'__main__\':\n    batch_size = 1\n    n_classes = 12\n    img_height, img_width = 360, 480\n    # img_height, img_width = 1024, 512\n    model = BiSeNet(n_classes=n_classes, pretrained=False, context_path=\'resnet18\')\n\n    x = Variable(torch.randn(batch_size, 3, img_height, img_width))\n    y = Variable(torch.LongTensor(np.ones((batch_size, img_height, img_width), dtype=np.int)))\n    # print(x.shape)\n    start = time.time()\n    pred = model(x)\n    end = time.time()\n    print(end-start)\n    # print(pred.shape)\n    # print(\'pred.type:\', pred.type)\n    # loss = cross_entropy2d(pred, y)\n    # print(loss)\n'"
semseg/modelloader/deconvnet.py,5,"b'# -*- coding: utf-8 -*-\nimport torch\nfrom torchvision import models\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\nimport torch\nfrom torch import nn\nfrom torchvision.models import vgg\nfrom torch.autograd import Variable\nimport torch.nn.functional as F\nimport time\nimport numpy as np\n\nfrom semseg.loss import cross_entropy2d\n\n\ndef conv3x3(in_planes, out_planes, stride=1):\n    """"""3x3 convolution with padding""""""\n    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n                     padding=1, bias=False)\n\ndef deconv3x3(in_planes, out_planes, stride=1):\n    """"""3x3 convolution with padding""""""\n    return nn.ConvTranspose2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False, output_padding=1)\n\nclass BasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None):\n        super(BasicBlock, self).__init__()\n        self.conv1 = conv3x3(inplanes, planes, stride)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = conv3x3(planes, planes)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\nclass DeconvBasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self, inplanes, planes, stride=1, upsample=None):\n        super(DeconvBasicBlock, self).__init__()\n        if stride == 1:\n            self.conv1 = conv3x3(inplanes, planes, stride)\n        else:\n            self.conv1 = deconv3x3(inplanes, planes, stride)\n\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = conv3x3(planes, planes)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.upsample = upsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n\n        if self.upsample is not None:\n            residual = self.upsample(x)\n\n        print(\'out.shape:\', out.shape)\n        print(\'residual.shape:\', residual.shape)\n        out += residual\n        out = self.relu(out)\n\n        return out\n\nclass Bottleneck(nn.Module):\n    expansion = 4\n\n    def __init__(self, in_channels, out_channels, stride=1, downsample=None):\n        super(Bottleneck, self).__init__()\n        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(out_channels)\n        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3,\n                               stride=stride, bias=False, padding=1)\n        self.bn2 = nn.BatchNorm2d(out_channels)\n        self.conv3 = nn.Conv2d(out_channels, out_channels * self.expansion,\n                               kernel_size=1, bias=False)\n        self.bn3 = nn.BatchNorm2d(out_channels * self.expansion)\n        self.relu = nn.ReLU()\n        self.downsample = downsample\n\n    def forward(self, x):\n        shortcut = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n        out = self.relu(out)\n\n        if self.downsample is not None:\n            shortcut = self.downsample(x)\n\n        # print(\'out.shape:\', out.shape)\n        # print(\'shortcut.shape:\', shortcut.shape)\n        out += shortcut\n        out = self.relu(out)\n\n        return out\n\nclass DeconvBottleneck(nn.Module):\n    expansion = 4\n\n    def __init__(self, in_channels, out_channels, stride=1, upsample=None):\n        super(DeconvBottleneck, self).__init__()\n        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(out_channels)\n        if stride == 1:\n            self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3,\n                                   stride=stride, bias=False, padding=1)\n        else:\n            self.conv2 = nn.ConvTranspose2d(out_channels, out_channels,\n                                            kernel_size=3,\n                                            stride=stride, bias=False,\n                                            padding=1,\n                                            output_padding=1)\n        self.bn2 = nn.BatchNorm2d(out_channels)\n        self.conv3 = nn.Conv2d(out_channels, out_channels * self.expansion,\n                               kernel_size=1, bias=False)\n        self.bn3 = nn.BatchNorm2d(out_channels * self.expansion)\n        self.relu = nn.ReLU()\n        self.upsample = upsample\n\n    def forward(self, x):\n        shortcut = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n        out = self.relu(out)\n\n        if self.upsample is not None:\n            shortcut = self.upsample(x)\n\n        # print(\'out.shape:\', out.shape)\n        # print(\'shortcut.shape:\', shortcut.shape)\n\n        out += shortcut\n        out = self.relu(out)\n\n        return out\n\nclass ResNet(nn.Module):\n    def __init__(self, downblock, upblock, num_layers, n_classes):\n        super(ResNet, self).__init__()\n\n        self.in_channels = 64\n\n        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n        self.bn1 = nn.BatchNorm2d(64)\n        self.relu = nn.ReLU()\n\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1, return_indices=True)\n        self.dlayer1 = self._make_downlayer(downblock, 64, num_layers[0])\n        self.dlayer2 = self._make_downlayer(downblock, 128, num_layers[1], stride=2)\n        self.dlayer3 = self._make_downlayer(downblock, 256, num_layers[2], stride=2)\n        self.dlayer4 = self._make_downlayer(downblock, 512, num_layers[3], stride=2)\n\n        self.uplayer1 = self._make_up_block(upblock, 512, num_layers[3], stride=2)\n        self.uplayer2 = self._make_up_block(upblock, 256, num_layers[2], stride=2)\n        self.uplayer3 = self._make_up_block(upblock, 128, num_layers[1], stride=2)\n        self.uplayer4 = self._make_up_block(upblock, 64, num_layers[0])\n\n        # print(\'self.in_channels:\', self.in_channels)\n        # upsample = nn.Sequential(\n        #     nn.ConvTranspose2d(self.in_channels, self.in_channels*upblock.expansion // 2, kernel_size=1, stride=2, bias=False, output_padding=1),\n        #     nn.BatchNorm2d(self.in_channels * upblock.expansion // 2),\n        # )\n        # self.uplayer_top = upblock(self.in_channels, self.in_channels // 2, 2, upsample)\n        # self.conv1_1 = nn.ConvTranspose2d(self.in_channels // 2, n_classes, kernel_size=1, stride=1, bias=False)\n        # self.dconv1 = nn.ConvTranspose2d(self.in_channels, n_classes, kernel_size=1, stride=2, bias=False, output_padding=1)\n\n    def _make_downlayer(self, block, init_channels, num_layer, stride=1):\n        downsample = None\n        if stride != 1 or self.in_channels != init_channels * block.expansion:\n            downsample = nn.Sequential(\n                nn.Conv2d(self.in_channels, init_channels*block.expansion, kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(init_channels*block.expansion),\n            )\n        layers = []\n        layers.append(block(self.in_channels, init_channels, stride, downsample))\n        self.in_channels = init_channels * block.expansion\n        for i in range(1, num_layer):\n            layers.append(block(self.in_channels, init_channels))\n\n        return nn.Sequential(*layers)\n\n    def _make_up_block(self, block, init_channels, num_layer, stride=1):\n        upsample = None\n        print(\'init_channels:\', init_channels)\n        print(\'self.in_channels:\', self.in_channels)\n        if stride != 1 or self.in_channels != init_channels * block.expansion:\n            upsample = nn.Sequential(\n                nn.ConvTranspose2d(self.in_channels, init_channels // block.expansion, kernel_size=1, stride=stride, bias=False, output_padding=1),\n                nn.BatchNorm2d(init_channels // block.expansion),\n            )\n        layers = []\n        for i in range(1, num_layer):\n            layers.append(block(self.in_channels, init_channels))\n        layers.append(block(self.in_channels, init_channels // block.expansion, stride, upsample))\n        self.in_channels = init_channels * block.expansion\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        # img = x\n        # x_size = x.size()\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n\n        unpool_shape = x.size()\n        # print(unpool_shape)\n        x, pool_indices = self.maxpool(x)\n\n        x = self.dlayer1(x)\n        print(\'dlayer1_x.shape:\', x.shape)\n        x = self.dlayer2(x)\n        print(\'dlayer2_x.shape:\', x.shape)\n        x = self.dlayer3(x)\n        print(\'dlayer3_x.shape:\', x.shape)\n        x = self.dlayer4(x)\n        print(\'dlayer4_x.shape:\', x.shape)\n\n        x = self.uplayer1(x)\n        print(\'uplayer1_x.shape:\', x.shape)\n        x = self.uplayer2(x)\n        print(\'uplayer2_x.shape:\', x.shape)\n        x = self.uplayer3(x)\n        print(\'uplayer3_x.shape:\', x.shape)\n        x = self.uplayer4(x)\n        print(\'uplayer4_x.shape:\', x.shape)\n        # x = self.uplayer_top(x)\n        # print(\'uplayer_top_x.shape:\', x.shape)\n        #\n        # x = self.dconv1(x)\n        # print(\'dconv1_x.shape:\', x.shape)\n\n        return x\n\n\ndef DeConvResNet50(n_classes, pretrained=False):\n    return ResNet(Bottleneck, DeconvBottleneck, [3, 4, 6, 3], n_classes=n_classes)\n\ndef DeConvResNet18(n_classes, pretrained=False):\n    return ResNet(BasicBlock, DeconvBasicBlock, [2, 2, 2, 2], n_classes=n_classes)\n\ndef DeConvResNet34(n_classes, pretrained=False):\n    return ResNet(BasicBlock, DeconvBasicBlock, [3, 4, 6, 3], n_classes=n_classes)\n\nif __name__ == \'__main__\':\n    batch_size = 1\n    n_classes = 21\n    img_height, img_width = 480, 480\n    # img_height, img_width = 1024, 512\n    model = DeConvResNet18(n_classes=n_classes)\n    x = Variable(torch.randn(batch_size, 3, img_height, img_width))\n    y = Variable(torch.LongTensor(np.ones((batch_size, img_height, img_width), dtype=np.int)))\n    # print(x.shape)\n    start = time.time()\n    pred = model(x)\n    end = time.time()\n    print(end-start)\n    print(pred.shape)\n    # print(\'pred.type:\', pred.type)\n    loss = cross_entropy2d(pred, y)\n    print(loss)\n\n'"
semseg/modelloader/deeplab_resnet.py,8,"b'#!/usr/bin/python\n# -*- coding: UTF-8 -*-\n\nimport time\nimport torch.nn as nn\nimport math\nimport torch.utils.model_zoo as model_zoo\nimport torch\nfrom torch.autograd import Variable\nimport numpy as np\n\nfrom semseg.loss import cross_entropy2d\n\naffine_par = True\n\n\ndef outS(i):\n    i = int(i)\n    i = (i+1)/2\n    i = int(np.ceil((i+1)/2.0))\n    i = (i+1)/2\n    return i\ndef conv3x3(in_planes, out_planes, stride=1):\n    ""3x3 convolution with padding""\n    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n                     padding=1, bias=False)\n\n\nclass BasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None):\n        super(BasicBlock, self).__init__()\n        self.conv1 = conv3x3(inplanes, planes, stride)\n        self.bn1 = nn.BatchNorm2d(planes, affine = affine_par)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = conv3x3(planes, planes)\n        self.bn2 = nn.BatchNorm2d(planes, affine = affine_par)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\n\nclass Bottleneck(nn.Module):\n    expansion = 4\n\n    def __init__(self, inplanes, planes, stride=1,  dilation_ = 1, downsample=None):\n        super(Bottleneck, self).__init__()\n        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, stride=stride, bias=False) # change\n        self.bn1 = nn.BatchNorm2d(planes,affine = affine_par)\n        for i in self.bn1.parameters():\n            i.requires_grad = False\n        padding = 1\n        if dilation_ == 2:\n            padding = 2\n        elif dilation_ == 4:\n            padding = 4\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, # change\n                               padding=padding, bias=False, dilation = dilation_)\n        self.bn2 = nn.BatchNorm2d(planes,affine = affine_par)\n        for i in self.bn2.parameters():\n            i.requires_grad = False\n        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)\n        self.bn3 = nn.BatchNorm2d(planes * 4, affine = affine_par)\n        for i in self.bn3.parameters():\n            i.requires_grad = False\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = downsample\n        self.stride = stride\n\n\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\nclass Classifier_Module(nn.Module):\n\n    def __init__(self,dilation_series,padding_series,NoLabels):\n        super(Classifier_Module, self).__init__()\n        self.conv2d_list = nn.ModuleList()\n        for dilation,padding in zip(dilation_series,padding_series):\n            self.conv2d_list.append(nn.Conv2d(2048,NoLabels,kernel_size=3,stride=1, padding =padding, dilation = dilation,bias = True))\n\n        for m in self.conv2d_list:\n            m.weight.data.normal_(0, 0.01)\n\n\n    def forward(self, x):\n        out = self.conv2d_list[0](x)\n        for i in range(len(self.conv2d_list)-1):\n            out += self.conv2d_list[i+1](x)\n        return out\n\n\n\nclass ResNet(nn.Module):\n    def __init__(self, block, layers,n_classes):\n        self.inplanes = 64\n        super(ResNet, self).__init__()\n        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3,\n                               bias=False)\n        self.bn1 = nn.BatchNorm2d(64,affine = affine_par)\n        for i in self.bn1.parameters():\n            i.requires_grad = False\n        self.relu = nn.ReLU(inplace=True)\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1, ceil_mode=True) # change\n        self.layer1 = self._make_layer(block, 64, layers[0])\n        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n        self.layer3 = self._make_layer(block, 256, layers[2], stride=1, dilation__ = 2)\n        self.layer4 = self._make_layer(block, 512, layers[3], stride=1, dilation__ = 4)\n        self.layer5 = self._make_pred_layer(Classifier_Module, [6,12,18,24],[6,12,18,24],n_classes)\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n                m.weight.data.normal_(0, 0.01)\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n        #        for i in m.parameters():\n        #            i.requires_grad = False\n\n    def _make_layer(self, block, planes, blocks, stride=1,dilation__ = 1):\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion or dilation__ == 2 or dilation__ == 4:\n            downsample = nn.Sequential(\n                nn.Conv2d(self.inplanes, planes * block.expansion,\n                          kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(planes * block.expansion,affine = affine_par),\n            )\n        for i in downsample._modules[\'1\'].parameters():\n            i.requires_grad = False\n        layers = []\n        layers.append(block(self.inplanes, planes, stride,dilation_=dilation__, downsample = downsample ))\n        self.inplanes = planes * block.expansion\n        for i in range(1, blocks):\n            layers.append(block(self.inplanes, planes,dilation_=dilation__))\n\n        return nn.Sequential(*layers)\n    def _make_pred_layer(self,block, dilation_series, padding_series,NoLabels):\n        return block(dilation_series,padding_series,NoLabels)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n        x = self.layer5(x)\n        return x\n\nclass MS_Deeplab(nn.Module):\n    def __init__(self,block,n_classes):\n        super(MS_Deeplab,self).__init__()\n        self.Scale = ResNet(block,[3, 4, 23, 3],n_classes)   #changed to fix #4\n\n    def forward(self,x):\n        input_size = x.size()[2]\n        self.interp1 = nn.UpsamplingBilinear2d(size = (  int(input_size*0.75)+1,  int(input_size*0.75)+1  ))\n        self.interp2 = nn.UpsamplingBilinear2d(size = (  int(input_size*0.5)+1,   int(input_size*0.5)+1   ))\n        self.interp3 = nn.UpsamplingBilinear2d(size = (  outS(input_size),   outS(input_size)   ))\n        out = []\n        x2 = self.interp1(x)\n        x3 = self.interp2(x)\n        out.append(self.Scale(x))\t# for original scale\n        out.append(self.interp3(self.Scale(x2)))\t# for 0.75x scale\n        out.append(self.Scale(x3))\t# for 0.5x scale\n\n        x2Out_interp = out[1]\n        x3Out_interp = self.interp3(out[2])\n        # print(\'out[0].data.size():\', out[0].data.size())\n        # print(\'x2Out_interp.data.size():\', x2Out_interp.data.size())\n        temp1 = torch.max(out[0],x2Out_interp)\n        out.append(torch.max(temp1,x3Out_interp))\n        return out\n\ndef Res_Deeplab(n_classes=21):\n    model = MS_Deeplab(Bottleneck,n_classes)\n    return model\n\nif __name__ == \'__main__\':\n    n_classes = 21\n    image_crop = 480\n    model = Res_Deeplab(n_classes=n_classes)\n    # model.init_vgg16()\n    # \xe8\xbe\x93\xe5\x85\xa5\xe5\x88\xb0deeplab\xe4\xb8\xad\xe7\x9a\x84\xe5\x9b\xbe\xe5\x83\x8f\xe6\x95\xb0\xe6\x8d\xae\xe5\xae\xbd\xe9\xab\x98\xe7\x9b\xb8\xe5\x90\x8c\xef\xbc\x8c\xe5\xa6\x82\xe6\x9e\x9ccrop\xe5\xa4\xa7\xe5\xb0\x8f\xe4\xb8\x8d\xe7\x9b\xb8\xe5\x90\x8c\xef\xbc\x8c\xe9\x82\xa3\xe4\xb9\x88\xe5\xa4\xa7\xe7\x9a\x84\xe9\x83\xa8\xe5\x88\x86\xe5\xa2\x9e\xe5\x8a\xa0\xef\xbc\x8c\xe5\x8e\x9f\xe5\x9b\xbe\xe5\xa2\x9e\xe5\x8a\xa0000\xef\xbc\x8c\xe6\xa0\x87\xe7\xad\xbe\xe5\x9b\xbe\xe5\xa2\x9e\xe5\x8a\xa0IGNORE_LABEL\n    # torch.nn.CrossEntropyLoss(ignore_index=IGNORE_LABEL)\n    x = Variable(torch.randn(1, 3, image_crop, image_crop))\n    y = Variable(torch.LongTensor(np.ones((1, image_crop, image_crop), dtype=np.int)))\n    # print(x.shape)\n\n    # ---------------------------pspnet\xe6\xa8\xa1\xe5\x9e\x8b\xe8\xbf\x90\xe8\xa1\x8c\xe6\x97\xb6\xe9\x97\xb4-----------------------\n    start = time.time()\n    outs = model(x)\n    end = time.time()\n    print(end-start)\n\n    # for out in outs:\n    #     print out.data.shape\n\n    interp = nn.UpsamplingBilinear2d(size=(image_crop, image_crop))\n    out = outs[3]\n    pred = interp(out)\n    print(\'pred.data.size():\', pred.data.size())\n    print(\'out.data.size():\', out.data.size())\n    # print(pred)\n    loss = cross_entropy2d(pred, y)\n    print(loss)\n'"
semseg/modelloader/deeplabv3.py,7,"b'# -*- coding: utf-8 -*-\n# code is from https://github.com/speedinghzl/Pytorch-Deeplab/blob/master/deeplab/model.py\nimport time\n\nimport torch.nn as nn\nimport math\nimport torch.utils.model_zoo as model_zoo\nimport torch\nimport numpy as np\nfrom torch.autograd import Variable\nimport torch.nn.functional as F\n\nfrom semseg.modelloader.utils import ASPP_Classifier_Module\nfrom semseg.loss import cross_entropy2d\n\n# affine_par = True\n\n\ndef outS(i):\n    i = int(i)\n    i = (i+1)/2\n    i = int(np.ceil((i+1)/2.0))\n    i = (i+1)/2\n    return i\n\ndef conv3x3(in_planes, out_planes, stride=1):\n    ""3x3 convolution with padding""\n    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n                     padding=1, bias=False)\n\n\nclass BasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None):\n        super(BasicBlock, self).__init__()\n        self.conv1 = conv3x3(inplanes, planes, stride)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = conv3x3(planes, planes)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\n\nclass Bottleneck(nn.Module):\n    expansion = 4\n\n    def __init__(self, inplanes, planes, stride=1, dilation=1, downsample=None):\n        super(Bottleneck, self).__init__()\n        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, stride=stride, bias=False) # change\n        self.bn1 = nn.BatchNorm2d(planes)\n        for i in self.bn1.parameters():\n            i.requires_grad = False\n\n        padding = dilation\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, # change\n                               padding=padding, bias=False, dilation = dilation)\n        self.bn2 = nn.BatchNorm2d(planes)\n        for i in self.bn2.parameters():\n            i.requires_grad = False\n        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)\n        self.bn3 = nn.BatchNorm2d(planes * 4)\n        for i in self.bn3.parameters():\n            i.requires_grad = False\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = downsample\n        self.stride = stride\n\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\n# class Classifier_Module(nn.Module):\n#\n#     def __init__(self, dilation_series, padding_series, num_classes):\n#         super(Classifier_Module, self).__init__()\n#         self.conv2d_list = nn.ModuleList()\n#         for dilation, padding in zip(dilation_series, padding_series):\n#             self.conv2d_list.append(nn.Conv2d(2048, num_classes, kernel_size=3, stride=1, padding=padding, dilation=dilation, bias = True))\n#\n#         for m in self.conv2d_list:\n#             m.weight.data.normal_(0, 0.01)\n#\n#     def forward(self, x):\n#         out = self.conv2d_list[0](x)\n#         for i in range(len(self.conv2d_list)-1):\n#             out += self.conv2d_list[i+1](x)\n#         return out\n\n# class Residual_Covolution(nn.Module):\n#     def __init__(self, icol, ocol, num_classes):\n#         super(Residual_Covolution, self).__init__()\n#         self.conv1 = nn.Conv2d(icol, ocol, kernel_size=3, stride=1, padding=12, dilation=12, bias=True)\n#         self.conv2 = nn.Conv2d(ocol, num_classes, kernel_size=3, stride=1, padding=12, dilation=12, bias=True)\n#         self.conv3 = nn.Conv2d(num_classes, ocol, kernel_size=1, stride=1, padding=0, dilation=1, bias=True)\n#         self.conv4 = nn.Conv2d(ocol, icol, kernel_size=1, stride=1, padding=0, dilation=1, bias=True)\n#         self.relu = nn.ReLU(inplace=True)\n#\n#     def forward(self, x):\n#         dow1 = self.conv1(x)\n#         dow1 = self.relu(dow1)\n#         seg = self.conv2(dow1)\n#         inc1 = self.conv3(seg)\n#         add1 = dow1 + self.relu(inc1)\n#         inc2 = self.conv4(add1)\n#         out = x + self.relu(inc2)\n#         return out, seg\n\n# class Residual_Refinement_Module(nn.Module):\n#\n#     def __init__(self, num_classes):\n#         super(Residual_Refinement_Module, self).__init__()\n#         self.RC1 = Residual_Covolution(2048, 512, num_classes)\n#         self.RC2 = Residual_Covolution(2048, 512, num_classes)\n#\n#     def forward(self, x):\n#         x, seg1 = self.RC1(x)\n#         _, seg2 = self.RC2(x)\n#         return [seg1, seg1+seg2]\n\n# class ResNet_Refine(nn.Module):\n#     def __init__(self, block, layers, n_classes):\n#         self.inplanes = 64\n#         super(ResNet_Refine, self).__init__()\n#         self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3,\n#                                bias=False)\n#         self.bn1 = nn.BatchNorm2d(64, affine = affine_par)\n#         for i in self.bn1.parameters():\n#             i.requires_grad = False\n#         self.relu = nn.ReLU(inplace=True)\n#         self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1, ceil_mode=True) # change\n#         self.layer1 = self._make_layer(block, 64, layers[0])\n#         self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n#         self.layer3 = self._make_layer(block, 256, layers[2], stride=1, dilation=2)\n#         self.layer4 = self._make_layer(block, 512, layers[3], stride=1, dilation=4)\n#         self.layer5 = Residual_Refinement_Module(n_classes)\n#\n#         for m in self.modules():\n#             if isinstance(m, nn.Conv2d):\n#                 n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n#                 m.weight.data.normal_(0, 0.01)\n#             elif isinstance(m, nn.BatchNorm2d):\n#                 m.weight.data.fill_(1)\n#                 m.bias.data.zero_()\n#         #        for i in m.parameters():\n#         #            i.requires_grad = False\n#\n#     def _make_layer(self, block, planes, blocks, stride=1, dilation=1):\n#         downsample = None\n#         if stride != 1 or self.inplanes != planes * block.expansion or dilation == 2 or dilation == 4:\n#             downsample = nn.Sequential(\n#                 nn.Conv2d(self.inplanes, planes * block.expansion,\n#                           kernel_size=1, stride=stride, bias=False),\n#                 nn.BatchNorm2d(planes * block.expansion,affine = affine_par))\n#         for i in downsample._modules[\'1\'].parameters():\n#             i.requires_grad = False\n#         layers = []\n#         layers.append(block(self.inplanes, planes, stride,dilation=dilation, downsample=downsample))\n#         self.inplanes = planes * block.expansion\n#         for i in range(1, blocks):\n#             layers.append(block(self.inplanes, planes, dilation=dilation))\n#\n#         return nn.Sequential(*layers)\n#\n#     def forward(self, x):\n#         x = self.conv1(x)\n#         x = self.bn1(x)\n#         x = self.relu(x)\n#         x = self.maxpool(x)\n#         x = self.layer1(x)\n#         x = self.layer2(x)\n#         x = self.layer3(x)\n#         x = self.layer4(x)\n#         x = self.layer5(x)\n#\n#         return x\n\nclass ResNet(nn.Module):\n    def __init__(self, block, layers, n_classes):\n        self.inplanes = 64\n        super(ResNet, self).__init__()\n        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n        self.bn1 = nn.BatchNorm2d(64)\n        for i in self.bn1.parameters():\n            i.requires_grad = False\n        self.relu = nn.ReLU(inplace=True)\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1, ceil_mode=True) # change\n        self.layer1 = self._make_layer(block, 64, layers[0])\n        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n        self.layer3 = self._make_layer(block, 256, layers[2], stride=1, dilation=2)\n        self.layer4 = self._make_layer(block, 512, layers[3], stride=1, dilation=4)\n        # self.layer5 = self._make_pred_layer(Classifier_Module, [6,12,18,24], [6,12,18,24], n_classes)\n        self.layer5 = self._make_pred_layer(ASPP_Classifier_Module, [6,12,18,24], [6,12,18,24], n_classes)\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n                m.weight.data.normal_(0, 0.01)\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n        #        for i in m.parameters():\n        #            i.requires_grad = False\n\n    def _make_layer(self, block, planes, blocks, stride=1, dilation=1):\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion or dilation == 2 or dilation == 4:\n            downsample = nn.Sequential(\n                nn.Conv2d(self.inplanes, planes * block.expansion,\n                          kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(planes * block.expansion))\n\n        for i in downsample._modules[\'1\'].parameters():\n            i.requires_grad = False\n\n        layers = []\n        layers.append(block(self.inplanes, planes, stride,dilation=dilation, downsample=downsample))\n        self.inplanes = planes * block.expansion\n        for i in range(1, blocks):\n            layers.append(block(self.inplanes, planes, dilation=dilation))\n\n        return nn.Sequential(*layers)\n\n    def _make_pred_layer(self,block, dilation_series, padding_series,num_classes):\n        return block(dilation_series,padding_series,num_classes)\n\n    def forward(self, x):\n        x_size = x.size()[2:]\n\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n        x = self.layer5(x)\n\n        x = F.upsample_bilinear(x, x_size)\n        return x\n\n# class MS_Deeplab(nn.Module):\n#     def __init__(self,block,num_classes):\n#         super(MS_Deeplab,self).__init__()\n#         self.Scale = ResNet(block,[3, 4, 23, 3],num_classes)   #changed to fix #4\n#\n#     def forward(self,x):\n#         output = self.Scale(x) # for original scale\n#         output_size = output.size()[2]\n#         input_size = x.size()[2]\n#\n#         self.interp1 = nn.Upsample(size=(int(input_size*0.75)+1, int(input_size*0.75)+1), mode=\'bilinear\')\n#         self.interp2 = nn.Upsample(size=(int(input_size*0.5)+1, int(input_size*0.5)+1), mode=\'bilinear\')\n#         self.interp3 = nn.Upsample(size=(output_size, output_size), mode=\'bilinear\')\n#\n#         x75 = self.interp1(x)\n#         output75 = self.interp3(self.Scale(x75)) # for 0.75x scale\n#\n#         x5 = self.interp2(x)\n#         output5 = self.interp3(self.Scale(x5))\t# for 0.5x scale\n#\n#         out_max = torch.max(torch.max(output, output75), output5)\n#         return [output, output75, output5, out_max]\n\n# def Res_Ms_Deeplab(num_classes=21):\n#     model = MS_Deeplab(Bottleneck, num_classes)\n#     return model\n\n# def Res_Deeplab(n_classes=21):\n#     # if is_refine:\n#     #     model = ResNet_Refine(Bottleneck,[3, 4, 23, 3], n_classes)\n#     # else:\n#     #     model = ResNet(Bottleneck,[3, 4, 23, 3], n_classes)\n#     model = ResNet(Bottleneck,[3, 4, 23, 3], n_classes)\n#     return model\n\n# def Res_Deeplab_18(n_classes=21, is_refine=False):\n#     if is_refine:\n#         model = ResNet_Refine(BasicBlock,[2, 2, 2, 2], n_classes)\n#     else:\n#         model = ResNet(BasicBlock,[2, 2, 2, 2], n_classes)\n#     return model\n\n# def Res_Deeplab_34(n_classes=21, is_refine=False):\n#     if is_refine:\n#         model = ResNet_Refine(BasicBlock,[3, 4, 6, 3], n_classes)\n#     else:\n#         model = ResNet(BasicBlock,[3, 4, 6, 3], n_classes)\n#     return model\n\ndef Res_Deeplab_50(n_classes=21, pretrained=False):\n    # if is_refine:\n    #     pass\n    #     # model = ResNet_Refine(Bottleneck, [3, 4, 6, 3], n_classes)\n    # else:\n    #     model = ResNet(Bottleneck, [3, 4, 6, 3], n_classes)\n    model = ResNet(Bottleneck, [3, 4, 6, 3], n_classes)\n    return model\n\ndef Res_Deeplab_101(n_classes=21, pretrained=False):\n    # if is_refine:\n    #     pass\n    #     # model = ResNet_Refine(Bottleneck, [3, 4, 23, 3], n_classes)\n    # else:\n    #     model = ResNet(Bottleneck, [3, 4, 23, 3], n_classes)\n    model = ResNet(Bottleneck, [3, 4, 23, 3], n_classes)\n    return model\n\nif __name__ == \'__main__\':\n    batch_size = 1\n    n_classes = 21\n    model = Res_Deeplab_50(n_classes=n_classes)\n    x = Variable(torch.randn(1, 3, 360, 480))\n    y = Variable(torch.LongTensor(np.ones((1, 360, 480), dtype=np.int)))\n    # print(x.shape)\n    start = time.time()\n    pred = model(x)\n    end = time.time()\n    print(end-start)\n    print(pred.shape)\n    print(\'pred.type:\', pred.type)\n    loss = cross_entropy2d(pred, y)\n    # print(loss)'"
semseg/modelloader/drn.py,13,"b""#!/usr/bin/python\n# -*- coding: UTF-8 -*-\nimport time\nimport os\nimport math\nfrom torch.utils import model_zoo\nimport torch\nfrom torch.autograd import Variable\nimport numpy as np\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nfrom torchvision import models\n\nfrom semseg.loss import cross_entropy2d\nfrom semseg.modelloader.utils import AlignedResInception, ASPP_Classifier_Module, IBN\n\n# from semseg.pytorch_modelsize import SizeEstimator\n\nwebroot = 'https://tigress-web.princeton.edu/~fy/drn/models/'\n\nmodel_urls = {\n    'resnet50': 'https://download.pytorch.org/models/resnet50-19c8e357.pth',\n    'drn-c-26': webroot + 'drn_c_26-ddedf421.pth',\n    'drn-c-42': webroot + 'drn_c_42-9d336e8c.pth',\n    'drn-c-58': webroot + 'drn_c_58-0a53a92c.pth',\n    'drn-d-22': webroot + 'drn_d_22-4bd2f8ea.pth',\n    'drn-d-38': webroot + 'drn_d_38-eebb45f0.pth',\n    'drn-d-54': webroot + 'drn_d_54-0e0534ff.pth',\n    'drn-d-105': webroot + 'drn_d_105-12b40979.pth'\n}\n\n\ndef conv3x3(in_planes, out_planes, stride=1, padding=1, dilation=1):\n    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=padding, bias=False, dilation=dilation)\n\ndef conv3x3_asymmetric(in_planes, out_planes, stride=1, padding=1, dilation=1):\n    return nn.Sequential(\n        nn.Conv2d(in_planes, out_planes, kernel_size=(3, 1), stride=stride, padding=(padding, 0), bias=False, dilation=dilation),\n        nn.Conv2d(out_planes, out_planes, kernel_size=(1, 3), stride=1, padding=(0, padding), bias=False, dilation=dilation),\n    )\n\n# drn\xe5\x9f\xba\xe6\x9c\xac\xe6\x9e\x84\xe6\x88\x90\xe5\x9d\x97\nclass BasicBlock_asymmetric(nn.Module):\n    expansion = 1\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None, dilation=(1, 1), residual=True):\n        super(BasicBlock_asymmetric, self).__init__()\n        # dilation\xe9\xbb\x98\xe8\xae\xa4\xe4\xb8\xba(1,1)\xe7\x94\xb1\xe4\xb8\xa4\xe4\xb8\xaadilation\xe7\x9a\x84\xe5\x8d\xb7\xe7\xa7\xaf\xe6\xa8\xa1\xe5\x9d\x97\xe6\x9e\x84\xe6\x88\x90\xef\xbc\x8c\xe7\x94\xb1\xe4\xba\x8estride=1\xef\xbc\x8cdilation\xe4\xb8\xba1\xef\xbc\x8ckernel\xe4\xb8\xba3\n        # \xe9\x82\xa3\xe4\xb9\x88\xe7\x9b\xb8\xe5\xbd\x93\xe4\xba\x8ekernel\xe4\xb8\xba6\xe7\x9a\x84\xe5\x8d\xb7\xe7\xa7\xaf\xe6\xa0\xb8\xef\xbc\x8cpadding\xe4\xb8\xba1\n        # self.conv1 = conv3x3(inplanes, planes, stride, padding=dilation[0], dilation=dilation[0])\n        self.conv1 = conv3x3_asymmetric(inplanes, planes, stride, padding=dilation[0], dilation=dilation[0])\n        self.bn1 = nn.BatchNorm2d(planes)\n        for i in self.bn1.parameters():\n            i.requires_grad = False\n        self.relu = nn.ReLU(inplace=True)\n        # self.conv2 = conv3x3(planes, planes, padding=dilation[1], dilation=dilation[1])\n        self.conv2 = conv3x3_asymmetric(planes, planes, padding=dilation[1], dilation=dilation[1])\n        self.bn2 = nn.BatchNorm2d(planes)\n        for i in self.bn2.parameters():\n            i.requires_grad = False\n        self.downsample = downsample\n        self.stride = stride\n        self.residual = residual\n\n    def forward(self, x):\n        residual = x\n\n        # print(x.data.size())\n        out = self.conv1(x)\n        # print(out.data.size())\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n        if self.residual:\n            out += residual\n        out = self.relu(out)\n\n        return out\n\nclass BasicBlock_asymmetric_ibn_a(nn.Module):\n    expansion = 1\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None, dilation=(1, 1), residual=True):\n        super(BasicBlock_asymmetric_ibn_a, self).__init__()\n        # dilation\xe9\xbb\x98\xe8\xae\xa4\xe4\xb8\xba(1,1)\xe7\x94\xb1\xe4\xb8\xa4\xe4\xb8\xaadilation\xe7\x9a\x84\xe5\x8d\xb7\xe7\xa7\xaf\xe6\xa8\xa1\xe5\x9d\x97\xe6\x9e\x84\xe6\x88\x90\xef\xbc\x8c\xe7\x94\xb1\xe4\xba\x8estride=1\xef\xbc\x8cdilation\xe4\xb8\xba1\xef\xbc\x8ckernel\xe4\xb8\xba3\n        # \xe9\x82\xa3\xe4\xb9\x88\xe7\x9b\xb8\xe5\xbd\x93\xe4\xba\x8ekernel\xe4\xb8\xba6\xe7\x9a\x84\xe5\x8d\xb7\xe7\xa7\xaf\xe6\xa0\xb8\xef\xbc\x8cpadding\xe4\xb8\xba1\n        # self.conv1 = conv3x3(inplanes, planes, stride, padding=dilation[0], dilation=dilation[0])\n        self.conv1 = conv3x3_asymmetric(inplanes, planes, stride, padding=dilation[0], dilation=dilation[0])\n        # self.bn1 = nn.BatchNorm2d(planes)\n        self.bn1 = IBN(planes)\n        for i in self.bn1.parameters():\n            i.requires_grad = False\n        self.relu = nn.ReLU(inplace=True)\n        # self.conv2 = conv3x3(planes, planes, padding=dilation[1], dilation=dilation[1])\n        self.conv2 = conv3x3_asymmetric(planes, planes, padding=dilation[1], dilation=dilation[1])\n        self.bn2 = nn.BatchNorm2d(planes)\n        for i in self.bn2.parameters():\n            i.requires_grad = False\n        self.downsample = downsample\n        self.stride = stride\n        self.residual = residual\n\n    def forward(self, x):\n        residual = x\n\n        # print(x.data.size())\n        out = self.conv1(x)\n        # print(out.data.size())\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n        if self.residual:\n            out += residual\n        out = self.relu(out)\n\n        return out\n\n# drn\xe5\x9f\xba\xe6\x9c\xac\xe6\x9e\x84\xe6\x88\x90\xe5\x9d\x97\nclass BasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None,\n                 dilation=(1, 1), residual=True):\n        super(BasicBlock, self).__init__()\n        # dilation\xe9\xbb\x98\xe8\xae\xa4\xe4\xb8\xba(1,1)\xe7\x94\xb1\xe4\xb8\xa4\xe4\xb8\xaadilation\xe7\x9a\x84\xe5\x8d\xb7\xe7\xa7\xaf\xe6\xa8\xa1\xe5\x9d\x97\xe6\x9e\x84\xe6\x88\x90\xef\xbc\x8c\xe7\x94\xb1\xe4\xba\x8estride=1\xef\xbc\x8cdilation\xe4\xb8\xba1\xef\xbc\x8ckernel\xe4\xb8\xba3\n        # \xe9\x82\xa3\xe4\xb9\x88\xe7\x9b\xb8\xe5\xbd\x93\xe4\xba\x8ekernel\xe4\xb8\xba6\xe7\x9a\x84\xe5\x8d\xb7\xe7\xa7\xaf\xe6\xa0\xb8\xef\xbc\x8cpadding\xe4\xb8\xba1\n        self.conv1 = conv3x3(inplanes, planes, stride, padding=dilation[0], dilation=dilation[0])\n        # self.conv1 = conv3x3_asymmetric(inplanes, planes, stride, padding=dilation[0], dilation=dilation[0])\n        self.bn1 = nn.BatchNorm2d(planes)\n        for i in self.bn1.parameters():\n            i.requires_grad = False\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = conv3x3(planes, planes, padding=dilation[1], dilation=dilation[1])\n        # self.conv2 = conv3x3_asymmetric(planes, planes, padding=dilation[1], dilation=dilation[1])\n        self.bn2 = nn.BatchNorm2d(planes)\n        for i in self.bn2.parameters():\n            i.requires_grad = False\n        self.downsample = downsample\n        self.stride = stride\n        self.residual = residual\n\n    def forward(self, x):\n        residual = x\n\n        # print(x.data.size())\n        out = self.conv1(x)\n        # print(out.data.size())\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n        if self.residual:\n            out += residual\n        out = self.relu(out)\n\n        return out\n\nclass Bottleneck(nn.Module):\n    expansion = 4\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None, dilation=(1, 1), residual=True):\n        super(Bottleneck, self).__init__()\n        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(planes)\n        for i in self.bn1.parameters():\n            i.requires_grad = False\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,\n                               padding=dilation[1], bias=False,\n                               dilation=dilation[1])\n        self.bn2 = nn.BatchNorm2d(planes)\n        for i in self.bn2.parameters():\n            i.requires_grad = False\n        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)\n        self.bn3 = nn.BatchNorm2d(planes * 4)\n        for i in self.bn3.parameters():\n            i.requires_grad = False\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\nclass DRN(nn.Module):\n\n    def __init__(self, block, layers, n_classes=21, channels=(16, 32, 64, 128, 256, 512, 512, 512), out_map=False, out_middle=False, pool_size=28, arch='D'):\n        super(DRN, self).__init__()\n        # print(layers)\n        self.inplanes = channels[0]\n        self.out_map = out_map\n        self.out_dim = channels[-1]\n        self.out_middle = out_middle\n        # \xe9\xbb\x98\xe8\xae\xa4\xe6\x9e\xb6\xe6\x9e\x84\xe4\xb8\xbaarch=D\n        self.arch = arch\n\n        # \xe4\xb8\x8d\xe5\x90\x8c\xe6\x9e\xb6\xe6\x9e\x84\xe4\xb8\xbb\xe8\xa6\x81\xe5\x9c\xa8\xe6\x9e\x84\xe6\x88\x90\xe7\x9a\x84\xe7\xbd\x91\xe7\xbb\x9c\xe6\xa8\xa1\xe5\x9d\x97\xe5\x9f\xba\xe6\x9c\xac\xe7\xbb\x84\xe6\x88\x90\xe6\xa8\xa1\xe5\x9d\x97\xe4\xb8\x8a\xe4\xb8\x8d\xe5\x90\x8c\xef\xbc\x8c\xe5\x9c\xa8C\xe6\x9e\xb6\xe6\x9e\x84\xe4\xb8\x8a\xe4\xb8\xbb\xe8\xa6\x81\xe7\x94\xb1basic block\xe5\x9d\x97\xe7\xbb\x84\xe6\x88\x90\xef\xbc\x8c\xe8\x80\x8c\xe5\x85\xb6\xe4\xbb\x96\xe7\x94\xb1conv\xe7\xbb\x84\xe6\x88\x90\n        if arch == 'C':\n            self.conv1 = nn.Conv2d(3, channels[0], kernel_size=7, stride=1, padding=3, bias=False)\n            self.bn1 = nn.BatchNorm2d(channels[0])\n            self.relu = nn.ReLU(inplace=True)\n\n            self.layer1 = self._make_layer(BasicBlock, channels[0], layers[0], stride=1)\n            self.layer2 = self._make_layer(BasicBlock, channels[1], layers[1], stride=2)\n        elif arch == 'D' or arch == 'E':\n            # -7+2*3/1+1=0\xe5\xb0\x86channel\xe4\xb8\xba3\xe7\x9a\x84rgb\xe5\x8e\x9f\xe5\xa7\x8b\xe5\x9b\xbe\xe5\x83\x8f\xe6\x95\xb0\xe6\x8d\xae\xe8\xbd\xac\xe6\x8d\xa2\xe4\xb8\xbachannels[0]\xe7\x9a\x84\xe6\x95\xb0\xe6\x8d\xae\n            self.layer0 = nn.Sequential(\n                nn.Conv2d(3, channels[0], kernel_size=7, stride=1, padding=3, bias=False),\n                nn.BatchNorm2d(channels[0]),\n                nn.ReLU(inplace=True)\n            )\n\n            self.layer1 = self._make_conv_layers(channels[0], layers[0], stride=1)\n            self.layer2 = self._make_conv_layers(channels[1], layers[1], stride=2)\n\n        self.layer3 = self._make_layer(block, channels[2], layers[2], stride=2)\n        self.layer4 = self._make_layer(block, channels[3], layers[3], stride=2)\n        self.layer5 = self._make_layer(block, channels[4], layers[4], dilation=2, new_level=False)\n        self.layer6 = None if layers[5] == 0 else self._make_layer(block, channels[5], layers[5], dilation=4, new_level=False)\n\n        if arch == 'C':\n            # \xe6\x97\xa0\xe6\xae\x8b\xe5\xb7\xae\xe6\xa8\xa1\xe5\x9d\x97\n            self.layer7 = None if layers[6] == 0 else self._make_layer(BasicBlock, channels[6], layers[6], dilation=2, new_level=False, residual=False)\n            self.layer8 = None if layers[7] == 0 else self._make_layer(BasicBlock, channels[7], layers[7], dilation=1, new_level=False, residual=False)\n        elif arch == 'D' or arch == 'E':\n            self.layer7 = None if layers[6] == 0 else self._make_conv_layers(channels[6], layers[6], dilation=2)\n            self.layer8 = None if layers[7] == 0 else self._make_conv_layers(channels[7], layers[7], dilation=1)\n\n        self.layer9 = None\n        if arch == 'E':\n            # self.layer9 = Inception(512, 128, 128, 256, 24,  64,  64)\n            # self.layer9 = ResInception(512, 128, 128, 256, 24,  64,  64)\n            # self.layer9 = CascadeResInception()\n            # self.layer9 = CascadeAlignedResInception(in_planes=512)\n            self.layer9 = AlignedResInception(in_planes=512)\n\n        # \xe6\x9c\x80\xe5\x90\x8e\xe7\x9a\x84\xe7\xbd\x91\xe7\xbb\x9c\xe8\xbe\x93\xe5\x87\xba\xe8\xaf\xad\xe4\xb9\x89\xe5\x9b\xbe\n        # if num_classes > 0:\n        #     self.avgpool = nn.AvgPool2d(pool_size)\n        #     self.fc = nn.Conv2d(self.out_dim, num_classes, kernel_size=1, stride=1, padding=0, bias=True)\n\n        self.layer10 = None\n        # self.layer10 = self._make_pred_layer(ASPP_Classifier_Module, [6, 12, 18, 24], [6, 12, 18, 24], n_classes, in_channels=512*block.expansion)\n        if self.layer10 is not None:\n            self.out_dim = n_classes\n            pass\n        else:\n            self.out_dim = 512 * block.expansion\n            pass\n\n        # \xe7\xbd\x91\xe7\xbb\x9c\xe6\xa8\xa1\xe5\x9d\x97\xe6\x9d\x83\xe9\x87\x8d\xe5\x92\x8c\xe5\x81\x8f\xe7\xbd\xae\xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n                m.weight.data.normal_(0, math.sqrt(2. / n))\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n\n    # \xe8\xbf\x99\xe7\xa7\x8d\xe6\x9e\x84\xe6\x88\x90\xe7\xbd\x91\xe7\xbb\x9c\xe5\xb1\x82\xe7\x9a\x84\xe6\x96\xb9\xe6\xb3\x95\xe7\xb1\xbb\xe4\xbc\xbc\xe4\xba\x8eResidual Neural Network\xef\xbc\x8cnew_level\xe8\xa1\xa8\xe7\xa4\xba\xe7\xac\xac\xe4\xb8\x80\xe4\xb8\xaaconv block\xe5\x92\x8c\xe5\x90\x8e\xe9\x9d\xa2\xe7\x9a\x84conv\xe6\x98\xaf\xe5\x90\xa6\xe7\xa9\xba\xe6\xb4\x9e\xe7\x8e\x87\xe7\x9b\xb8\xe5\x90\x8c\n    def _make_layer(self, block, planes, blocks, stride=1, dilation=1, new_level=True, residual=True):\n        assert dilation == 1 or dilation % 2 == 0\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                nn.Conv2d(self.inplanes, planes * block.expansion,\n                          kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(planes * block.expansion),\n            )\n\n        layers = list()\n        layers.append(\n            block(self.inplanes, planes, stride, downsample,\n            dilation=(1, 1) if dilation == 1 else (dilation // 2 if new_level else dilation, dilation),\n            residual=residual\n            )\n        )\n        self.inplanes = planes * block.expansion\n        for i in range(1, blocks):\n            layers.append(block(self.inplanes, planes, residual=residual, dilation=(dilation, dilation)))\n\n        return nn.Sequential(*layers)\n\n    # \xe5\x88\x9b\xe5\xbb\xba\xe5\x8d\xb7\xe7\xa7\xaf\xe5\xb1\x82\xef\xbc\x8c\xe8\xbe\x93\xe5\x85\xa5\xe9\x80\x9a\xe9\x81\x93\xef\xbc\x8c\xe5\x8d\xb7\xe7\xa7\xaf\xe4\xb8\xaa\xe6\x95\xb0\xef\xbc\x8cstride\xef\xbc\x8cdilation\xe7\xad\x89\xe7\xad\x89\n    def _make_conv_layers(self, channels, convs, stride=1, dilation=1):\n        modules = []\n        # \xe5\x88\x9b\xe5\xbb\xba\xe5\x8d\xb7\xe7\xa7\xaf\xe7\x9a\x84\xe4\xb8\xaa\xe6\x95\xb0\xef\xbc\x8c\xe5\xbd\x93stride\xe4\xb8\xba2\xe6\x97\xb6\xef\xbc\x8c\xe5\x8d\xb3\xe5\x8d\xb7\xe7\xa7\xaf\xe6\x9c\x89\xe4\xb8\xa4\xe5\xb1\x82\xe7\x9a\x84\xe6\x83\x85\xe5\x86\xb5\xe4\xb8\x8b\xef\xbc\x8c\xe8\xbe\x93\xe5\x87\xba\xe7\xbb\xb4\xe5\xba\xa6\xe4\xb8\xba\xe5\x8e\x9f\xe6\x9d\xa5\xe7\x9a\x841\xef\xbc\x8f2\n        for i in range(convs):\n            modules.extend([\n                nn.Conv2d(self.inplanes, channels, kernel_size=3, stride=stride if i == 0 else 1, padding=dilation, bias=False, dilation=dilation),\n                nn.BatchNorm2d(channels),\n                nn.ReLU(inplace=True)])\n            self.inplanes = channels\n        return nn.Sequential(*modules)\n\n    def _make_pred_layer(self, block, dilation_series, padding_series, n_classes, in_channels):\n        return block(dilation_series, padding_series, n_classes, in_channels)\n\n    def forward(self, x):\n        # y = list()\n\n        if self.arch == 'C':\n            x = self.conv1(x)\n            x = self.bn1(x)\n            x = self.relu(x)\n        elif self.arch == 'D' or self.arch == 'E':\n            x = self.layer0(x)\n\n        x = self.layer1(x)\n        # y.append(x)\n        x = self.layer2(x)\n        # y.append(x)\n\n        x = self.layer3(x)\n        # y.append(x)\n\n        x = self.layer4(x)\n        # y.append(x)\n\n        x = self.layer5(x)\n        # y.append(x)\n\n        if self.layer6 is not None:\n            x = self.layer6(x)\n            # y.append(x)\n\n        if self.layer7 is not None:\n            x = self.layer7(x)\n            # y.append(x)\n\n        if self.layer8 is not None:\n            x = self.layer8(x)\n            # y.append(x)\n\n        # DRN E\n        if self.layer9 is not None:\n            x = self.layer9(x)\n            # y.append(x)\n\n        # ASPP\n        if self.layer10 is not None:\n            x = self.layer10(x)\n            # y.append(x)\n\n        # if self.out_map:\n        #     x = self.fc(x)\n        # else:\n        #     x = self.avgpool(x)\n        #     x = self.fc(x)\n        #     x = x.view(x.size(0), -1)\n\n        # if self.out_middle:\n        #     return x, y\n        # else:\n        #     return x\n        return x\n\nclass DRN_A(nn.Module):\n\n    def __init__(self, block, layers, n_classes=21, input_channel=3):\n        self.inplanes = 64\n        super(DRN_A, self).__init__()\n        self.block = block\n        self.layers = layers\n        self.n_classes = n_classes\n        self.input_channel = input_channel\n\n        self.out_dim = 512 * block.expansion\n        self.conv1 = nn.Conv2d(self.input_channel, 64, kernel_size=7, stride=2, padding=3, bias=False)\n        self.bn1 = nn.BatchNorm2d(64)\n        for i in self.bn1.parameters():\n            i.requires_grad = False\n        self.relu = nn.ReLU(inplace=True)\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n        self.layer1 = self._make_layer(block, 64, layers[0])\n        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n        self.layer3 = self._make_layer(block, 256, layers[2], stride=1, dilation=2)\n        self.layer4 = self._make_layer(block, 512, layers[3], stride=1, dilation=4)\n        # self.avgpool = nn.AvgPool2d(28, stride=1)\n        # self.fc = nn.Linear(512 * block.expansion, num_classes)\n        self.layer5 = self._make_pred_layer(ASPP_Classifier_Module, [6, 12, 18, 24], [6, 12, 18, 24], n_classes, in_channels=512*block.expansion)\n        if self.layer5 is not None:\n            self.out_dim = n_classes\n            pass\n        else:\n            self.out_dim = 512 * block.expansion\n            pass\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n                # m.weight.data.normal_(0, math.sqrt(2. / n))\n                m.weight.data.normal_(0, 0.01)\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n\n        # for m in self.modules():\n        #     if isinstance(m, nn.Conv2d):\n        #         nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n        #     elif isinstance(m, nn.BatchNorm2d):\n        #         nn.init.constant_(m.weight, 1)\n        #         nn.init.constant_(m.bias, 0)\n\n    def _make_layer(self, block, planes, blocks, stride=1, dilation=1):\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                nn.Conv2d(self.inplanes, planes * block.expansion,\n                          kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(planes * block.expansion),\n            )\n            for i in downsample._modules['1'].parameters():\n                i.requires_grad = False\n\n\n        layers = []\n        layers.append(block(self.inplanes, planes, stride, downsample))\n        self.inplanes = planes * block.expansion\n        for i in range(1, blocks):\n            # print('blocks_i:', i)\n            layers.append(block(self.inplanes, planes,\n                                dilation=(dilation, dilation)))\n\n        return nn.Sequential(*layers)\n\n    def _make_pred_layer(self, block, dilation_series, padding_series, n_classes, in_channels):\n        return block(dilation_series, padding_series, n_classes, in_channels)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n\n        # x = self.avgpool(x)\n        # x = x.view(x.size(0), -1)\n        # x = self.fc(x)\n        # print('x.size():', x.size())\n        x = self.layer5(x)\n        # print('x.size():', x.size())\n\n        return x\n\ndef drn_a_50(pretrained=False, **kwargs):\n    model = DRN_A(Bottleneck, [3, 4, 6, 3], **kwargs)\n    # if pretrained:\n    #     model.load_state_dict(model_zoo.load_url('https://s3.amazonaws.com/pytorch/models/resnet50-19c8e357.pth'))\n    return model\n\ndef drn_a_18(pretrained=False, **kwargs):\n    model = DRN_A(BasicBlock, [2, 2, 2, 2], **kwargs)\n    # if pretrained:\n    #     model.load_state_dict(model_zoo.load_url('https://s3.amazonaws.com/pytorch/models/resnet18-5c106cde.pth'))\n    return model\n\ndef drn_a_n(pretrained=False, depth_n=18, **kwargs):\n    model = DRN_A(BasicBlock, [2+depth_n-18, 2, 2, 2], **kwargs)\n    # if pretrained:\n    #     model.load_state_dict(model_zoo.load_url('https://s3.amazonaws.com/pytorch/models/resnet18-5c106cde.pth'))\n    return model\n\ndef drn_a_asymmetric_18(pretrained=False, **kwargs):\n    model = DRN_A(BasicBlock_asymmetric, [2, 2, 2, 2], **kwargs)\n    # if pretrained:\n    #     model.load_state_dict(model_zoo.load_url('https://s3.amazonaws.com/pytorch/models/resnet18-5c106cde.pth'))\n    return model\n\ndef drn_a_asymmetric_n(pretrained=False, depth_n=18, **kwargs):\n    # print('depth_n:', depth_n)\n    model = DRN_A(BasicBlock_asymmetric, [2+depth_n-18, 2, 2, 2], **kwargs)\n    return model\n\ndef drn_a_asymmetric_ibn_a_18(pretrained=False, **kwargs):\n    model = DRN_A(BasicBlock_asymmetric_ibn_a, [2, 2, 2, 2], **kwargs)\n    # if pretrained:\n    #     model.load_state_dict(model_zoo.load_url('https://s3.amazonaws.com/pytorch/models/resnet18-5c106cde.pth'))\n    return model\n\ndef drn_a_34(pretrained=False, **kwargs):\n    model = DRN_A(BasicBlock, [3, 4, 6, 3], **kwargs)\n    # if pretrained:\n    #     model.load_state_dict(model_zoo.load_url('https://s3.amazonaws.com/pytorch/models/resnet18-5c106cde.pth'))\n    return model\n\ndef drn_a_asymmetric_34(pretrained=False, **kwargs):\n    model = DRN_A(BasicBlock_asymmetric, [3, 4, 6, 3], **kwargs)\n    # if pretrained:\n    #     model.load_state_dict(model_zoo.load_url('https://s3.amazonaws.com/pytorch/models/resnet18-5c106cde.pth'))\n    return model\n\ndef drn_c_26(pretrained=False, **kwargs):\n    model = DRN(BasicBlock, [1, 1, 2, 2, 2, 2, 1, 1], arch='C', **kwargs)\n    # if pretrained:\n    #     model.load_state_dict(model_zoo.load_url(model_urls['drn-c-26']))\n    return model\n\n\ndef drn_c_42(pretrained=False, **kwargs):\n    model = DRN(BasicBlock, [1, 1, 3, 4, 6, 3, 1, 1], arch='C', **kwargs)\n    # if pretrained:\n    #     model.load_state_dict(model_zoo.load_url(model_urls['drn-c-42']))\n    return model\n\n\ndef drn_c_58(pretrained=False, **kwargs):\n    model = DRN(Bottleneck, [1, 1, 3, 4, 6, 3, 1, 1], arch='C', **kwargs)\n    # if pretrained:\n    #     model.load_state_dict(model_zoo.load_url(model_urls['drn-c-58']))\n    return model\n\n# drn\xe5\x8f\x98\xe7\xa7\x8d22\ndef drn_d_22(pretrained=False, **kwargs):\n    model = DRN(BasicBlock, [1, 1, 2, 2, 2, 2, 1, 1], arch='D', **kwargs)\n    # if pretrained:\n    #     model.load_state_dict(model_zoo.load_url(model_urls['drn-d-22']))\n    return model\n\ndef drn_d_24(pretrained=False, **kwargs):\n    model = DRN(BasicBlock, [1, 1, 2, 2, 2, 2, 2, 2], arch='D', **kwargs)\n    # if pretrained:\n    #     model.load_state_dict(model_zoo.load_url(model_urls['drn-d-24']))\n    return model\n\n\ndef drn_d_38(pretrained=False, **kwargs):\n    model = DRN(BasicBlock, [1, 1, 3, 4, 6, 3, 1, 1], arch='D', **kwargs)\n    if pretrained:\n        model_checkpoint_path = os.path.expanduser('~/.torch/models/drn_d_38-eebb45f0.pth')\n        if os.path.exists(model_checkpoint_path):\n            model_dict = model.state_dict()\n            pretrained_dict = torch.load(model_checkpoint_path, map_location='cpu')\n            # print('model_dict:', model_dict.keys())\n            # print('pretrained_dict:', pretrained_dict.keys())\n            new_dict = {k: v for k, v in pretrained_dict.items() if k in model_dict.keys()}\n            # print('new_dict:', new_dict.keys())\n            model_dict.update(new_dict)\n            model.load_state_dict(model_dict)\n    return model\n\n\ndef drn_d_40(pretrained=False, **kwargs):\n    model = DRN(BasicBlock, [1, 1, 3, 4, 6, 3, 2, 2], arch='D', **kwargs)\n    # if pretrained:\n    #     model.load_state_dict(model_zoo.load_url(model_urls['drn-d-40']))\n    return model\n\n\ndef drn_d_54(pretrained=False, **kwargs):\n    model = DRN(Bottleneck, [1, 1, 3, 4, 6, 3, 1, 1], arch='D', **kwargs)\n    # if pretrained:\n    #     model.load_state_dict(model_zoo.load_url(model_urls['drn-d-54']))\n    return model\n\n\ndef drn_d_56(pretrained=False, **kwargs):\n    model = DRN(Bottleneck, [1, 1, 3, 4, 6, 3, 2, 2], arch='D', **kwargs)\n    # if pretrained:\n    #     model.load_state_dict(model_zoo.load_url(model_urls['drn-d-56']))\n    return model\n\n\ndef drn_d_105(pretrained=False, **kwargs):\n    model = DRN(Bottleneck, [1, 1, 3, 4, 23, 3, 1, 1], arch='D', **kwargs)\n    # if pretrained:\n    #     model.load_state_dict(model_zoo.load_url(model_urls['drn-d-105']))\n    return model\n\n\ndef drn_d_107(pretrained=False, **kwargs):\n    model = DRN(Bottleneck, [1, 1, 3, 4, 23, 3, 2, 2], arch='D', **kwargs)\n    # if pretrained:\n    #     model.load_state_dict(model_zoo.load_url(model_urls['drn-d-107']))\n    return model\n\n# drn\xe5\x8f\x98\xe7\xa7\x8d22\ndef drn_e_22(pretrained=False, **kwargs):\n    model = DRN(BasicBlock, [1, 1, 2, 2, 2, 2, 1, 1], arch='E', **kwargs)\n    # if pretrained:\n    #     model.load_state_dict(model_zoo.load_url(model_urls['drn-d-22']))\n    return model\n\n# -------------------------semantic model----------------------------------\n\ndef drnseg_a_50(pretrained=False, n_classes=21):\n    model = DRNSeg(model_name='drn_a_50', n_classes=n_classes, pretrained=pretrained)\n    return model\n\ndef drnseg_a_18(pretrained=False, n_classes=21, input_channel=3):\n    model = DRNSeg(model_name='drn_a_18', n_classes=n_classes, pretrained=pretrained, input_channel=input_channel)\n    return model\n\ndef drnseg_a_n(pretrained=False, n_classes=21, depth_n=18):\n    print('depth_n:', depth_n)\n    model = DRNSeg(model_name='drn_a_n', n_classes=n_classes, pretrained=pretrained, depth_n=depth_n)\n    return model\n\ndef drnseg_a_asymmetric_18(pretrained=False, n_classes=21):\n    model = DRNSeg(model_name='drn_a_asymmetric_18', n_classes=n_classes, pretrained=pretrained)\n    return model\n\n# drnseg\xe6\xa8\xa1\xe5\x9e\x8bn\xe6\xb5\x8b\xe8\xaf\x95\ndef drnseg_a_asymmetric_n(pretrained=False, n_classes=21, depth_n=18):\n    print('depth_n:', depth_n)\n    model = DRNSeg(model_name='drn_a_asymmetric_n', n_classes=n_classes, pretrained=pretrained, depth_n=depth_n)\n    return model\n\ndef drnseg_a_asymmetric_ibn_a_18(pretrained=False, n_classes=21):\n    model = DRNSeg(model_name='drn_a_asymmetric_ibn_a_18', n_classes=n_classes, pretrained=pretrained)\n    return model\n\ndef drnseg_a_34(pretrained=False, n_classes=21):\n    model = DRNSeg(model_name='drn_a_34', n_classes=n_classes, pretrained=pretrained)\n    return model\n\ndef drnseg_a_asymmetric_34(pretrained=False, n_classes=21):\n    model = DRNSeg(model_name='drn_a_asymmetric_34', n_classes=n_classes, pretrained=pretrained)\n    return model\n\ndef drnseg_c_26(pretrained=False, n_classes=21):\n    model = DRNSeg(model_name='drn_c_26', n_classes=n_classes, pretrained=pretrained)\n    return model\n\ndef drnseg_c_42(pretrained=False, n_classes=21):\n    model = DRNSeg(model_name='drn_c_42', n_classes=n_classes, pretrained=pretrained)\n    return model\n\ndef drnseg_c_58(pretrained=False, n_classes=21):\n    model = DRNSeg(model_name='drn_c_58', n_classes=n_classes, pretrained=pretrained)\n    return model\n\ndef drnseg_d_22(pretrained=False, n_classes=21):\n    model = DRNSeg(model_name='drn_d_22', n_classes=n_classes, pretrained=pretrained)\n    return model\n\n\ndef drnseg_d_24(pretrained=False, n_classes=21):\n    model = DRNSeg(model_name='drn_d_24', n_classes=n_classes, pretrained=pretrained)\n    return model\n\ndef drnseg_d_38(pretrained=False, n_classes=21):\n    model = DRNSeg(model_name='drn_d_38', n_classes=n_classes, pretrained=pretrained)\n    return model\n\ndef drnseg_d_40(pretrained=False, n_classes=21):\n    model = DRNSeg(model_name='drn_d_40', n_classes=n_classes, pretrained=pretrained)\n    return model\n\ndef drnseg_d_54(pretrained=False, n_classes=21):\n    model = DRNSeg(model_name='drn_d_54', n_classes=n_classes, pretrained=pretrained)\n    return model\n\ndef drnseg_d_56(pretrained=False, n_classes=21):\n    model = DRNSeg(model_name='drn_d_56', n_classes=n_classes, pretrained=pretrained)\n    return model\n\ndef drnseg_d_105(pretrained=False, n_classes=21):\n    model = DRNSeg(model_name='drn_d_105', n_classes=n_classes, pretrained=pretrained)\n    return model\n\ndef drnseg_d_107(pretrained=False, n_classes=21):\n    model = DRNSeg(model_name='drn_d_107', n_classes=n_classes, pretrained=pretrained)\n    return model\n\ndef drnseg_e_22(pretrained=False, n_classes=21):\n    model = DRNSeg(model_name='drn_e_22', n_classes=n_classes, pretrained=pretrained)\n    return model\n\n# -----------------------------------------------------------\n\n# \xe8\xbd\xac\xe7\xbd\xae\xe5\x8d\xb7\xe7\xa7\xaf\xe6\x9d\x83\xe9\x87\x8d\xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96\xe5\xa1\xab\xe5\x85\x85\xe6\x96\xb9\xe6\xb3\x95\ndef fill_up_weights(up):\n    w = up.weight.data\n    f = math.ceil(w.size(2) / 2)\n    c = (2 * f - 1 - f % 2) / (2. * f)\n    for i in range(w.size(2)):\n        for j in range(w.size(3)):\n            w[0, 0, i, j] = (1 - math.fabs(i / f - c)) * (1 - math.fabs(j / f - c))\n    for c in range(1, w.size(0)):\n        w[c, 0, :, :] = w[0, 0, :, :]\n\n# drn segnet network\nclass DRNSeg(nn.Module):\n    def __init__(self, model_name, n_classes, pretrained=False, use_torch_up=True, depth_n=-1, input_channel=3):\n        super(DRNSeg, self).__init__()\n        # DRN\xe5\x88\x86\xe5\x89\xb2\xe6\xa8\xa1\xe5\x9e\x8b\xe4\xb8\x8d\xe5\x90\x8c\xe5\x8f\x98\xe7\xa7\x8d\n        # if model_name=='drn_d_22':\n        #     model = drn_d_22(pretrained=pretrained, num_classes=1000)\n        # if model_name=='drn_a_50':\n        #     model = drn_a_50(pretrained=pretrained, num_classes=1000)\n        # if model_name=='drn_a_18':\n        #     model = drn_a_18(pretrained=pretrained, num_classes=1000)\n        # if model_name=='drn_e_22':\n        #     model = drn_e_22(pretrained=pretrained, num_classes=1000)\n\n        if model_name=='drn_a_asymmetric_n':\n            # print('depth_n:', depth_n)\n            model = drn_a_asymmetric_n(pretrained=pretrained, n_classes=n_classes, depth_n=depth_n, input_channel=input_channel)\n        elif model_name=='drn_a_n':\n            # print('depth_n:', depth_n)\n            model = drn_a_n(pretrained=pretrained, n_classes=n_classes, depth_n=depth_n, input_channel=input_channel)\n        else:\n            model = eval(model_name)(pretrained=pretrained, n_classes=n_classes, input_channel=input_channel)\n        # pmodel = nn.DataParallel(model)\n        # if pretrained_model is not None:\n            # pmodel.load_state_dict(pretrained_model)\n        # self.base = nn.Sequential(*list(model.children())[:-2])\n        # self.base = nn.Sequential(*list(model.children()))\n        self.base = model\n\n        # \xe4\xbb\x85\xe4\xbb\x85\xe5\x9c\xa8\xe6\x9c\x80\xe5\x90\x8e\xe4\xb8\x80\xe5\xb1\x82seg layer\xe4\xb8\x8a\xe5\xad\x98\xe6\x9c\x89bias\n        self.seg = nn.Conv2d(model.out_dim, n_classes, kernel_size=1)\n        # self.softmax = nn.LogSoftmax()\n        m = self.seg\n\n        # \xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96\xe5\x88\x86\xe5\x89\xb2\xe5\x9b\xbe\xe6\x9c\x80\xe5\x90\x8e\xe7\x9a\x84\xe5\x8d\xb7\xe7\xa7\xafweights\xe5\x92\x8cbias\n        n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n        m.weight.data.normal_(0, math.sqrt(2. / n))\n        m.bias.data.zero_()\n\n        if use_torch_up:\n            # \xe4\xbd\xbf\xe7\x94\xa8pytorch\xe5\x8f\x8c\xe7\xba\xbf\xe6\x80\xa7\xe4\xb8\x8a\xe9\x87\x87\xe6\xa0\xb7\n            self.up = nn.UpsamplingBilinear2d(scale_factor=8)\n        else:\n            # \xe4\xbd\xbf\xe7\x94\xa8\xe8\xbd\xac\xe7\xbd\xae\xe5\x8d\xb7\xe7\xa7\xaf\xe4\xb8\x8a\xe9\x87\x87\xe6\xa0\xb7\n            up = nn.ConvTranspose2d(n_classes, n_classes, 16, stride=8, padding=4, output_padding=0, groups=n_classes, bias=False)\n            fill_up_weights(up)\n            up.weight.requires_grad = False\n            self.up = up\n\n        if pretrained and n_classes == 19 and model_name=='drn_d_38':\n            model_checkpoint_path = os.path.expanduser('~/.torch/models/drn_d_38_cityscapes.pth')\n            if os.path.exists(model_checkpoint_path):\n                model_dict = self.state_dict()\n                pretrained_dict = torch.load(model_checkpoint_path, map_location='cpu')\n                # print('model_dict:', model_dict.keys()[:3])\n                # print('pretrained_dict:', pretrained_dict.keys()[:3])\n                # print('len(model_dict):', len(model_dict.keys()))\n                # print('len(pretrained_dict):', len(pretrained_dict.keys()))\n                # new_dict = {k: v for k, v in pretrained_dict.items() if k in model_dict.keys()}\n                new_dict = {}\n                for k, v in pretrained_dict.items():\n                    if k.find('base.') != -1:\n                        new_k = str('base.' + 'layer' + k[k.find('.') + 1:])\n                        if new_k not in model_dict.keys():\n                            print(new_k)\n                        new_v = v\n                        new_dict[new_k] = new_v\n                    else:\n                        # print(k)\n                        new_k = k\n                        new_v = v\n                        new_dict[new_k] = new_v\n                # print('new_dict:', new_dict.keys()[:3])\n                # print('len(new_dict):', len(new_dict.keys()))\n                model_dict.update(new_dict)\n                self.load_state_dict(model_dict)\n        if pretrained and model_name=='drn_a_18':\n            model_checkpoint_path = os.path.expanduser('~/GitHub/Quick/semseg/best.pth')\n            if os.path.exists(model_checkpoint_path):\n                model_dict = self.state_dict()\n                pretrained_dict = torch.load(model_checkpoint_path, map_location='cpu')\n                model_dict_keys = model_dict.keys()\n                # print('model_dict:', model_dict.keys()[:3])\n                # print('pretrained_dict:', pretrained_dict.keys()[:3])\n                # print('len(model_dict):', len(model_dict.keys()))\n                # print('len(pretrained_dict):', len(pretrained_dict.keys()))\n                new_dict = {}\n                for k, v in pretrained_dict.items():\n                    if 'base.{}'.format(k) in model_dict_keys:\n                        new_k = 'base.{}'.format(k)\n                        new_v = v\n                        new_dict[new_k] = new_v\n                    else:\n                        pass\n                        # print(k)\n                    # if k.find('base.') != -1:\n                    #     new_k = str('base.' + 'layer' + k[k.find('.') + 1:])\n                    #     if new_k not in model_dict.keys():\n                    #         print(new_k)\n                    #     new_v = v\n                    #     new_dict[new_k] = new_v\n                    # else:\n                    #     # print(k)\n                    #     new_k = k\n                    #     new_v = v\n                    #     new_dict[new_k] = new_v\n                # print('new_dict:', new_dict.keys()[:3])\n                new_dict = {k: v for k, v in new_dict.items() if k in model_dict.keys()}\n                # print('new_dict:', new_dict.keys())\n                # print('len(new_dict):', len(new_dict.keys()))\n                model_dict.update(new_dict)\n                self.load_state_dict(model_dict)\n\n    def forward(self, x):\n        x = self.base(x)\n\n        # \xe5\xb0\x86\xe5\x88\x86\xe5\x89\xb2\xe5\x9b\xbe\xe5\xaf\xb9\xe5\xba\x94\xe5\x88\xb0\xe5\x88\x86\xe5\x89\xb2\xe7\xb1\xbb\xe5\x88\xab\xe6\x95\xb0\xe4\xb8\x8a\n        x = self.seg(x)\n\n        # \xe4\xbd\xbf\xe7\x94\xa8\xe5\x8f\x8c\xe7\xba\xbf\xe6\x80\xa7\xe4\xb8\x8a\xe9\x87\x87\xe6\xa0\xb7\xe6\x88\x96\xe8\x80\x85\xe8\xbd\xac\xe7\xbd\xae\xe5\x8d\xb7\xe7\xa7\xaf\xe4\xb8\x8a\xe9\x87\x87\xe6\xa0\xb78\xe5\x80\x8d\xe9\x99\x8d\xe9\x87\x87\xe6\xa0\xb7\xe7\x8e\x87\xe7\x9a\x84\xe5\x88\x86\xe5\x89\xb2\xe5\x9b\xbe\n        y = self.up(x)\n        return y\n\n    def optim_parameters(self, memo=None):\n        for param in self.base.parameters():\n            yield param\n        for param in self.seg.parameters():\n            yield param\n\nif __name__ == '__main__':\n    n_classes = 21\n    model = DRNSeg(model_name='drn_a_asymmetric_18', n_classes=n_classes, pretrained=False)\n    # model = DRNSeg(model_name='drn_d_22', n_classes=n_classes, pretrained=False)\n    # model.eval()\n    # model.init_vgg16()\n    x = Variable(torch.randn(1, 3, 360, 480))\n    y = Variable(torch.LongTensor(np.ones((1, 360, 480), dtype=np.int)))\n    # x = Variable(torch.randn(1, 3, 512, 1024))\n    # y = Variable(torch.LongTensor(np.ones((1, 512, 1024), dtype=np.int)))\n    # print(x.shape)\n    start = time.time()\n    pred = model(x)\n    end = time.time()\n    print(end-start)\n    print(pred.shape)\n    loss = cross_entropy2d(pred, y)\n    print(loss)\n\n    # se = SizeEstimator(model, input_size=(1, 3, 360, 480))\n    # print(se.estimate_size())\n"""
semseg/modelloader/drn_a_irb.py,5,"b""# -*- coding: utf-8 -*-\nimport time\nimport os\nimport math\nfrom torch.utils import model_zoo\nimport torch\nfrom torch.autograd import Variable\nimport numpy as np\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nfrom torchvision import models\n\nfrom semseg.loss import cross_entropy2d\nfrom semseg.modelloader.utils import AlignedResInception\n\n\ndef conv3x3(in_planes, out_planes, stride=1, padding=1, dilation=1):\n    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=padding, bias=False, dilation=dilation)\n\ndef conv3x3_bn_relu(in_planes, out_planes, stride=1, padding=1, dilation=1):\n    return nn.Sequential(\n        nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=padding, bias=False, dilation=dilation),\n        nn.BatchNorm2d(out_planes),\n        nn.ReLU(inplace=True),\n    )\n\n# drn\xe5\x9f\xba\xe6\x9c\xac\xe6\x9e\x84\xe6\x88\x90\xe5\x9d\x97\nclass BasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None,\n                 dilation=(1, 1), residual=True):\n        super(BasicBlock, self).__init__()\n        # dilation\xe9\xbb\x98\xe8\xae\xa4\xe4\xb8\xba(1,1)\xe7\x94\xb1\xe4\xb8\xa4\xe4\xb8\xaadilation\xe7\x9a\x84\xe5\x8d\xb7\xe7\xa7\xaf\xe6\xa8\xa1\xe5\x9d\x97\xe6\x9e\x84\xe6\x88\x90\xef\xbc\x8c\xe7\x94\xb1\xe4\xba\x8estride=1\xef\xbc\x8cdilation\xe4\xb8\xba1\xef\xbc\x8ckernel\xe4\xb8\xba3\n        # \xe9\x82\xa3\xe4\xb9\x88\xe7\x9b\xb8\xe5\xbd\x93\xe4\xba\x8ekernel\xe4\xb8\xba6\xe7\x9a\x84\xe5\x8d\xb7\xe7\xa7\xaf\xe6\xa0\xb8\xef\xbc\x8cpadding\xe4\xb8\xba1\n        self.conv1 = conv3x3(inplanes, planes, stride, padding=dilation[0], dilation=dilation[0])\n        # self.conv1 = conv3x3_asymmetric(inplanes, planes, stride, padding=dilation[0], dilation=dilation[0])\n        self.bn1 = nn.BatchNorm2d(planes)\n        for i in self.bn1.parameters():\n            i.requires_grad = False\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = conv3x3(planes, planes, padding=dilation[1], dilation=dilation[1])\n        # self.conv2 = conv3x3_asymmetric(planes, planes, padding=dilation[1], dilation=dilation[1])\n        self.bn2 = nn.BatchNorm2d(planes)\n        for i in self.bn2.parameters():\n            i.requires_grad = False\n        self.downsample = downsample\n        self.stride = stride\n        self.residual = residual\n\n    def forward(self, x):\n        residual = x\n\n        # print(x.data.size())\n        out = self.conv1(x)\n        # print(out.data.size())\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n        if self.residual:\n            out += residual\n        out = self.relu(out)\n\n        return out\n\ndef drnsegirb_a_18(pretrained=False, n_classes=21):\n    model = DRNSegIRB_A(BasicBlock, [2, 2, 2, 2], n_classes=n_classes)\n    return model\n\nclass DRNSegIRB_A(nn.Module):\n\n    def __init__(self, block, layers, n_classes=21):\n        super(DRNSegIRB_A, self).__init__()\n        self.inplanes = 64\n        self.out_dim = 512 * block.expansion\n        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n        self.bn1 = nn.BatchNorm2d(64)\n        for i in self.bn1.parameters():\n            i.requires_grad = False\n        self.relu = nn.ReLU(inplace=True)\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n        self.layer1 = self._make_layer(block, 64, layers[0])\n        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n        self.layer3 = self._make_layer(block, 256, layers[2], stride=1, dilation=2)\n        self.layer4 = self._make_layer(block, 512, layers[3], stride=1, dilation=4)\n        self.out_conv = nn.Conv2d(self.out_dim, n_classes, kernel_size=1)\n\n\n        # ----no irb----\n        # self.up = nn.UpsamplingBilinear2d(scale_factor=8)\n        # ----no irb----\n\n        # ----irb 1----\n        self.up = nn.UpsamplingBilinear2d(scale_factor=8)\n        self.irbunit_1 = AlignedResInception(512)\n        # ----irb 1----\n\n        # ----irb 2----\n        # self.refineunit_1 = RefineUnit(64, n_classes)\n        # self.refineunit_2 = RefineUnit(64, n_classes)\n        # self.up = nn.UpsamplingBilinear2d(scale_factor=2)\n        # ----irb 2----\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                m.weight.data.normal_(0, 0.01)\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n\n\n    def _make_layer(self, block, planes, blocks, stride=1, dilation=1):\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                nn.Conv2d(self.inplanes, planes * block.expansion,\n                          kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(planes * block.expansion),\n            )\n            for i in downsample._modules['1'].parameters():\n                i.requires_grad = False\n\n\n        layers = []\n        layers.append(block(self.inplanes, planes, stride, downsample))\n        self.inplanes = planes * block.expansion\n        for i in range(1, blocks):\n            layers.append(block(self.inplanes, planes, dilation=(dilation, dilation)))\n\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        x_conv1 = self.conv1(x)\n        x = self.bn1(x_conv1)\n        x = self.relu(x)\n        x_pool = self.maxpool(x)\n\n        x_layer1 = self.layer1(x_pool)\n        x_layer2 = self.layer2(x_layer1)\n        x = self.layer3(x_layer2)\n        x = self.layer4(x)\n\n        # ----no irb----\n        # x = self.up(x)\n        # ----no irb----\n\n        # ----irb 1----\n        x = self.irbunit_1(x)\n        # ----irb 1----\n\n        x = self.out_conv(x)\n        x = self.up(x)\n        return x\n"""
semseg/modelloader/drn_a_mt.py,5,"b'# -*- coding: utf-8 -*-\nimport time\nimport os\nimport math\nfrom torch.utils import model_zoo\nimport torch\nfrom torch.autograd import Variable\nimport numpy as np\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nfrom torchvision import models\n\nfrom semseg.loss import cross_entropy2d\nfrom semseg.modelloader.utils import AlignedResInception\n\n\ndef conv3x3(in_planes, out_planes, stride=1, padding=1, dilation=1):\n    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=padding, bias=False, dilation=dilation)\n\n\n# drn\xe5\x9f\xba\xe6\x9c\xac\xe6\x9e\x84\xe6\x88\x90\xe5\x9d\x97\nclass BasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None,\n                 dilation=(1, 1), residual=True):\n        super(BasicBlock, self).__init__()\n        # dilation\xe9\xbb\x98\xe8\xae\xa4\xe4\xb8\xba(1,1)\xe7\x94\xb1\xe4\xb8\xa4\xe4\xb8\xaadilation\xe7\x9a\x84\xe5\x8d\xb7\xe7\xa7\xaf\xe6\xa8\xa1\xe5\x9d\x97\xe6\x9e\x84\xe6\x88\x90\xef\xbc\x8c\xe7\x94\xb1\xe4\xba\x8estride=1\xef\xbc\x8cdilation\xe4\xb8\xba1\xef\xbc\x8ckernel\xe4\xb8\xba3\n        # \xe9\x82\xa3\xe4\xb9\x88\xe7\x9b\xb8\xe5\xbd\x93\xe4\xba\x8ekernel\xe4\xb8\xba6\xe7\x9a\x84\xe5\x8d\xb7\xe7\xa7\xaf\xe6\xa0\xb8\xef\xbc\x8cpadding\xe4\xb8\xba1\n        self.conv1 = conv3x3(inplanes, planes, stride, padding=dilation[0], dilation=dilation[0])\n        # self.conv1 = conv3x3_asymmetric(inplanes, planes, stride, padding=dilation[0], dilation=dilation[0])\n        self.bn1 = nn.BatchNorm2d(planes)\n        for i in self.bn1.parameters():\n            i.requires_grad = False\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = conv3x3(planes, planes, padding=dilation[1], dilation=dilation[1])\n        # self.conv2 = conv3x3_asymmetric(planes, planes, padding=dilation[1], dilation=dilation[1])\n        self.bn2 = nn.BatchNorm2d(planes)\n        for i in self.bn2.parameters():\n            i.requires_grad = False\n        self.downsample = downsample\n        self.stride = stride\n        self.residual = residual\n\n    def forward(self, x):\n        residual = x\n\n        # print(x.data.size())\n        out = self.conv1(x)\n        # print(out.data.size())\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n        if self.residual:\n            out += residual\n        out = self.relu(out)\n\n        return out\n\nclass detnet_bottleneck(nn.Module):\n    # no expansion\n    # dilation = 2\n    # type B use 1x1 conv\n    expansion = 1\n\n    def __init__(self, in_planes, planes, stride=1, block_type=\'A\'):\n        super(detnet_bottleneck, self).__init__()\n        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=2, bias=False,dilation=2)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.conv3 = nn.Conv2d(planes, self.expansion*planes, kernel_size=1, bias=False)\n        self.bn3 = nn.BatchNorm2d(self.expansion*planes)\n\n        self.downsample = nn.Sequential()\n        if stride != 1 or in_planes != self.expansion*planes or block_type==\'B\':\n            self.downsample = nn.Sequential(\n                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(self.expansion*planes)\n            )\n\n    def forward(self, x):\n        out = F.relu(self.bn1(self.conv1(x)))\n        out = F.relu(self.bn2(self.conv2(out)))\n        out = self.bn3(self.conv3(out))\n        out += self.downsample(x)\n        out = F.relu(out)\n        return out\n\ndef drnsegmt_a_18(pretrained=False, n_classes=21, det_tensor_num=30):\n    model = DRNSegMT_A(BasicBlock, [2, 2, 2, 2], n_classes=n_classes, det_tensor_num=det_tensor_num)\n    return model\n\nclass DRNSegMT_A(nn.Module):\n\n    def __init__(self, block, layers, n_classes=21, det_tensor_num=30):\n        """"""\n        :param block: resnet basicblock or bottleblock\n        :param layers: [2, 2, 2, 2] resnet block format\n        :param n_classes: segment classes\n        :param det_tensor_num: object detection num\n        """"""\n        super(DRNSegMT_A, self).__init__()\n        self.inplanes = 64\n        self.det_tensor_num = det_tensor_num\n        self.out_dim = 512 * block.expansion\n        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n        self.bn1 = nn.BatchNorm2d(64)\n        for i in self.bn1.parameters():\n            i.requires_grad = False\n        self.relu = nn.ReLU(inplace=True)\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n        self.layer1 = self._make_layer(block, 64, layers[0])\n        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n        self.layer3 = self._make_layer(block, 256, layers[2], stride=2, dilation=1)\n        self.layer4 = self._make_layer(block, 512, layers[3], stride=2, dilation=1)\n\n        # ----for semantic segment----\n        self.out_conv = nn.Conv2d(self.out_dim, n_classes, kernel_size=1)\n        # self.up = nn.UpsamplingBilinear2d(scale_factor=32)\n        # ----for semantic segment----\n\n        # ----for object detection----\n        self.layer5 = self._make_detnet_layer(in_channels=512*block.expansion)\n        self.conv_end = nn.Conv2d(256, self.det_tensor_num, kernel_size=3, stride=1, padding=1, bias=False)\n        self.bn_end = nn.BatchNorm2d(self.det_tensor_num)\n        # ----for object detection----\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                m.weight.data.normal_(0, 0.01)\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n\n    def _make_layer(self, block, planes, blocks, stride=1, dilation=1):\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                nn.Conv2d(self.inplanes, planes * block.expansion,\n                          kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(planes * block.expansion),\n            )\n            for i in downsample._modules[\'1\'].parameters():\n                i.requires_grad = False\n\n\n        layers = []\n        layers.append(block(self.inplanes, planes, stride, downsample))\n        self.inplanes = planes * block.expansion\n        for i in range(1, blocks):\n            layers.append(block(self.inplanes, planes, dilation=(dilation, dilation)))\n\n        return nn.Sequential(*layers)\n\n    def _make_detnet_layer(self, in_channels):\n        layers = []\n        layers.append(detnet_bottleneck(in_planes=in_channels, planes=256, block_type=\'B\'))\n        layers.append(detnet_bottleneck(in_planes=256, planes=256, block_type=\'A\'))\n        layers.append(detnet_bottleneck(in_planes=256, planes=256, block_type=\'A\'))\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        x_size = x.size()\n        x_conv1 = self.conv1(x)\n        # print(\'x_conv1.shape:\', x_conv1.shape)\n        x = self.bn1(x_conv1)\n        x = self.relu(x)\n        x_pool = self.maxpool(x)\n\n        x_layer1 = self.layer1(x_pool)\n        # print(\'x_layer1.shape:\', x_layer1.shape)\n        x_layer2 = self.layer2(x_layer1)\n        # print(\'x_layer2.shape:\', x_layer2.shape)\n        x = self.layer3(x_layer2)\n        x = self.layer4(x)\n\n        # ----for semantic segment----\n        x_sem = self.out_conv(x)\n        # print(\'x_sem.shape:\', x_sem.shape)\n        # x_sem = self.up(x_sem)\n        x_sem = F.upsample_bilinear(x_sem, x_size[2:])\n        # print(\'x_sem.shape:\', x_sem.shape)\n        # ----for semantic segment----\n\n        # ----for object detection----\n        x_det = self.layer5(x)\n        x_det = self.conv_end(x_det)\n        x_det = self.bn_end(x_det)\n        x_det = F.sigmoid(x_det)\n        x_det = x_det.permute(0,2,3,1)\n        # ----for object detection----\n        return x_sem, x_det\n'"
semseg/modelloader/drn_a_refine.py,6,"b""# -*- coding: utf-8 -*-\nimport time\nimport os\nimport math\nfrom torch.utils import model_zoo\nimport torch\nfrom torch.autograd import Variable\nimport numpy as np\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nfrom torchvision import models\n\nfrom semseg.loss import cross_entropy2d\n\n\ndef conv3x3(in_planes, out_planes, stride=1, padding=1, dilation=1):\n    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=padding, bias=False, dilation=dilation)\n\ndef conv3x3_bn_relu(in_planes, out_planes, stride=1, padding=1, dilation=1):\n    return nn.Sequential(\n        nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=padding, bias=False, dilation=dilation),\n        nn.BatchNorm2d(out_planes),\n        nn.ReLU(inplace=True),\n    )\n\n# drn\xe5\x9f\xba\xe6\x9c\xac\xe6\x9e\x84\xe6\x88\x90\xe5\x9d\x97\nclass BasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None,\n                 dilation=(1, 1), residual=True):\n        super(BasicBlock, self).__init__()\n        # dilation\xe9\xbb\x98\xe8\xae\xa4\xe4\xb8\xba(1,1)\xe7\x94\xb1\xe4\xb8\xa4\xe4\xb8\xaadilation\xe7\x9a\x84\xe5\x8d\xb7\xe7\xa7\xaf\xe6\xa8\xa1\xe5\x9d\x97\xe6\x9e\x84\xe6\x88\x90\xef\xbc\x8c\xe7\x94\xb1\xe4\xba\x8estride=1\xef\xbc\x8cdilation\xe4\xb8\xba1\xef\xbc\x8ckernel\xe4\xb8\xba3\n        # \xe9\x82\xa3\xe4\xb9\x88\xe7\x9b\xb8\xe5\xbd\x93\xe4\xba\x8ekernel\xe4\xb8\xba6\xe7\x9a\x84\xe5\x8d\xb7\xe7\xa7\xaf\xe6\xa0\xb8\xef\xbc\x8cpadding\xe4\xb8\xba1\n        self.conv1 = conv3x3(inplanes, planes, stride, padding=dilation[0], dilation=dilation[0])\n        # self.conv1 = conv3x3_asymmetric(inplanes, planes, stride, padding=dilation[0], dilation=dilation[0])\n        self.bn1 = nn.BatchNorm2d(planes)\n        for i in self.bn1.parameters():\n            i.requires_grad = False\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = conv3x3(planes, planes, padding=dilation[1], dilation=dilation[1])\n        # self.conv2 = conv3x3_asymmetric(planes, planes, padding=dilation[1], dilation=dilation[1])\n        self.bn2 = nn.BatchNorm2d(planes)\n        for i in self.bn2.parameters():\n            i.requires_grad = False\n        self.downsample = downsample\n        self.stride = stride\n        self.residual = residual\n\n    def forward(self, x):\n        residual = x\n\n        # print(x.data.size())\n        out = self.conv1(x)\n        # print(out.data.size())\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n        if self.residual:\n            out += residual\n        out = self.relu(out)\n\n        return out\n\ndef drnsegrefine_a_18(pretrained=False, n_classes=21):\n    model = DRNSegRefine_A(BasicBlock, [2, 2, 2, 2], n_classes=n_classes)\n    return model\n\nclass RefineUnit(nn.Module):\n    def __init__(self, f2_channel, n_classes=21):\n        super(RefineUnit, self).__init__()\n        self.f2_channel = f2_channel\n        self.n_classes = n_classes\n        # self.up = nn.UpsamplingBilinear2d(scale_factor=2)\n        self.up = nn.ConvTranspose2d(n_classes, n_classes, 4, stride=2, padding=1, output_padding=0, groups=n_classes, bias=False)\n        self.out_conv_1 = conv3x3_bn_relu(in_planes=self.f2_channel, out_planes=self.n_classes)\n        # self.out_conv_2 = conv3x3_bn_relu(in_planes=2*self.n_classes, out_planes=self.n_classes)\n        self.out_conv_2 = conv3x3_bn_relu(in_planes=self.n_classes, out_planes=self.n_classes)\n\n    def forward(self, f3, f2):\n        # print('f3.shape:', f3.shape)\n        # print('f2.shape:', f2.shape)\n        m1 = self.up(f3)\n        f2_1 = self.out_conv_1(f2)\n        # m2 = torch.cat((m1, f2_1), 1)\n        m2 = m1 + f2_1\n        o2 = self.out_conv_2(m2)\n        return o2\n\nclass DRNSegRefine_A(nn.Module):\n\n    def __init__(self, block, layers, n_classes=21):\n        super(DRNSegRefine_A, self).__init__()\n        self.inplanes = 64\n        self.out_dim = 512 * block.expansion\n        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n        self.bn1 = nn.BatchNorm2d(64)\n        for i in self.bn1.parameters():\n            i.requires_grad = False\n        self.relu = nn.ReLU(inplace=True)\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n        self.layer1 = self._make_layer(block, 64, layers[0])\n        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n        self.layer3 = self._make_layer(block, 256, layers[2], stride=1, dilation=2)\n        self.layer4 = self._make_layer(block, 512, layers[3], stride=1, dilation=4)\n        self.out_conv = nn.Conv2d(self.out_dim, n_classes, kernel_size=1)\n\n\n        # ----no refine----\n        self.up = nn.UpsamplingBilinear2d(scale_factor=8)\n        # ----no refine----\n\n        # ----refine 1----\n        # self.refineunit_1 = RefineUnit(64, n_classes)\n        # self.up = nn.UpsamplingBilinear2d(scale_factor=4)\n        # ----refine 1----\n\n        # ----refine 2----\n        self.refineunit_1 = RefineUnit(64, n_classes)\n        self.refineunit_2 = RefineUnit(64, n_classes)\n        self.up = nn.UpsamplingBilinear2d(scale_factor=2)\n        # ----refine 2----\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                m.weight.data.normal_(0, 0.01)\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n\n\n    def _make_layer(self, block, planes, blocks, stride=1, dilation=1):\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                nn.Conv2d(self.inplanes, planes * block.expansion,\n                          kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(planes * block.expansion),\n            )\n            for i in downsample._modules['1'].parameters():\n                i.requires_grad = False\n\n\n        layers = []\n        layers.append(block(self.inplanes, planes, stride, downsample))\n        self.inplanes = planes * block.expansion\n        for i in range(1, blocks):\n            layers.append(block(self.inplanes, planes, dilation=(dilation, dilation)))\n\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        x_conv1 = self.conv1(x)\n        x = self.bn1(x_conv1)\n        x = self.relu(x)\n        x_pool = self.maxpool(x)\n\n        x_layer1 = self.layer1(x_pool)\n        x_layer2 = self.layer2(x_layer1)\n        x = self.layer3(x_layer2)\n        x = self.layer4(x)\n\n        x = self.out_conv(x)\n\n        # ----no refine----\n        # x = self.up(x)\n        # ----no refine----\n\n        # ----refine 1----\n        # x_refine1 = self.refineunit_1(x, x_layer1)\n        # x = self.up(x_refine1)\n        # ----refine 1----\n\n        # ----refine 2----\n        x_refine1 = self.refineunit_1(x, x_layer1)\n        x_refine2 = self.refineunit_2(x_refine1, x_conv1)\n        x = self.up(x_refine2)\n        # ----refine 2----\n\n        return x\n"""
semseg/modelloader/drn_pred.py,17,"b'#!/usr/bin/python\n# -*- coding: UTF-8 -*-\nimport time\nimport os\nimport math\nfrom torch.utils import model_zoo\nimport torch\nfrom torch.autograd import Variable\nimport numpy as np\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nfrom torchvision import models\n\nfrom semseg.loss import cross_entropy2d\nfrom semseg.modelloader.utils import AlignedResInception, ASPP_Classifier_Module, IBN\n\n\ndef conv3x3(in_planes, out_planes, stride=1, padding=1, dilation=1):\n    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=padding, bias=False, dilation=dilation)\n\n\nclass ConvLSTMCell(nn.Module):\n\n    def __init__(self, input_size, input_dim, hidden_dim, kernel_size, bias):\n        """"""\n        Initialize ConvLSTM cell.\n\n        Parameters\n        ----------\n        input_size: (int, int)\n            Height and width of input tensor as (height, width).\n        input_dim: int\n            Number of channels of input tensor.\n        hidden_dim: int\n            Number of channels of hidden state.\n        kernel_size: (int, int)\n            Size of the convolutional kernel.\n        bias: bool\n            Whether or not to add the bias.\n        """"""\n\n        super(ConvLSTMCell, self).__init__()\n\n        self.height, self.width = input_size\n        self.input_dim = input_dim\n        self.hidden_dim = hidden_dim\n\n        self.kernel_size = kernel_size\n        self.padding = kernel_size[0] // 2, kernel_size[1] // 2\n        self.bias = bias\n\n        self.conv = nn.Conv2d(in_channels=self.input_dim + self.hidden_dim,\n                              out_channels=4 * self.hidden_dim,\n                              kernel_size=self.kernel_size,\n                              padding=self.padding,\n                              bias=self.bias)\n\n    def forward(self, input_tensor, cur_state):\n        h_cur, c_cur = cur_state\n\n        combined = torch.cat([input_tensor, h_cur], dim=1)  # concatenate along channel axis\n\n        combined_conv = self.conv(combined)\n        cc_i, cc_f, cc_o, cc_g = torch.split(combined_conv, self.hidden_dim, dim=1)\n        i = torch.sigmoid(cc_i)\n        f = torch.sigmoid(cc_f)\n        o = torch.sigmoid(cc_o)\n        g = torch.tanh(cc_g)\n\n        c_next = f * c_cur + i * g\n        h_next = o * torch.tanh(c_next)\n\n        return h_next, c_next\n\n    def init_hidden(self, batch_size):\n        if next(self.parameters()).is_cuda:\n            return (Variable(torch.zeros(batch_size, self.hidden_dim, self.height, self.width)).cuda(), Variable(torch.zeros(batch_size, self.hidden_dim, self.height, self.width)).cuda())\n        else:\n            return (Variable(torch.zeros(batch_size, self.hidden_dim, self.height, self.width)), Variable(torch.zeros(batch_size, self.hidden_dim, self.height, self.width)))\n\n\nclass ConvLSTM(nn.Module):\n\n    def __init__(self, input_size, input_dim, hidden_dim, kernel_size, num_layers,\n                 batch_first=False, bias=True, return_all_layers=False):\n        super(ConvLSTM, self).__init__()\n\n        self._check_kernel_size_consistency(kernel_size)\n\n        # Make sure that both `kernel_size` and `hidden_dim` are lists having len == num_layers\n        kernel_size = self._extend_for_multilayer(kernel_size, num_layers)\n        hidden_dim = self._extend_for_multilayer(hidden_dim, num_layers)\n        if not len(kernel_size) == len(hidden_dim) == num_layers:\n            raise ValueError(\'Inconsistent list length.\')\n\n        self.height, self.width = input_size\n\n        self.input_dim = input_dim\n        self.hidden_dim = hidden_dim\n        self.kernel_size = kernel_size\n        self.num_layers = num_layers\n        self.batch_first = batch_first\n        self.bias = bias\n        self.return_all_layers = return_all_layers\n\n        cell_list = []\n        for i in range(0, self.num_layers):\n            cur_input_dim = self.input_dim if i == 0 else self.hidden_dim[i - 1]\n\n            cell_list.append(ConvLSTMCell(input_size=(self.height, self.width),\n                                          input_dim=cur_input_dim,\n                                          hidden_dim=self.hidden_dim[i],\n                                          kernel_size=self.kernel_size[i],\n                                          bias=self.bias))\n\n        self.cell_list = nn.ModuleList(cell_list)\n\n    def forward(self, input_tensor, hidden_state=None):\n        """"""\n\n        Parameters\n        ----------\n        input_tensor: todo\n            5-D Tensor either of shape (t, b, c, h, w) or (b, t, c, h, w)\n        hidden_state: todo\n            None. todo implement stateful\n\n        Returns\n        -------\n        last_state_list, layer_output\n        """"""\n        if not self.batch_first:\n            # (t, b, c, h, w) -> (b, t, c, h, w)\n            input_tensor = input_tensor.permute(1, 0, 2, 3, 4)\n\n        # Implement stateful ConvLSTM\n        if hidden_state is not None:\n            raise NotImplementedError()\n        else:\n            hidden_state = self._init_hidden(batch_size=input_tensor.size(0))\n\n        layer_output_list = []\n        last_state_list = []\n\n        seq_len = input_tensor.size(1)\n        cur_layer_input = input_tensor\n\n        for layer_idx in range(self.num_layers):\n\n            h, c = hidden_state[layer_idx]\n            output_inner = []\n            for t in range(seq_len):\n                h, c = self.cell_list[layer_idx](input_tensor=cur_layer_input[:, t, :, :, :], cur_state=[h, c])\n                output_inner.append(h)\n\n            layer_output = torch.stack(output_inner, dim=1)\n            cur_layer_input = layer_output\n\n            layer_output_list.append(layer_output)\n            last_state_list.append([h, c])\n\n        if not self.return_all_layers:\n            layer_output_list = layer_output_list[-1]\n            last_state_list = last_state_list[-1]\n\n        return layer_output_list, last_state_list\n\n    def _init_hidden(self, batch_size):\n        init_states = []\n        for i in range(self.num_layers):\n            init_states.append(self.cell_list[i].init_hidden(batch_size))\n        return init_states\n\n    @staticmethod\n    def _check_kernel_size_consistency(kernel_size):\n        if not (isinstance(kernel_size, tuple) or\n                (isinstance(kernel_size, list) and all([isinstance(elem, tuple) for elem in kernel_size]))):\n            raise ValueError(\'`kernel_size` must be tuple or list of tuples\')\n\n    @staticmethod\n    def _extend_for_multilayer(param, num_layers):\n        if not isinstance(param, list):\n            param = [param] * num_layers\n        return param\n\n# drn\xe5\x9f\xba\xe6\x9c\xac\xe6\x9e\x84\xe6\x88\x90\xe5\x9d\x97\nclass BasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None,\n                 dilation=(1, 1), residual=True):\n        super(BasicBlock, self).__init__()\n        # dilation\xe9\xbb\x98\xe8\xae\xa4\xe4\xb8\xba(1,1)\xe7\x94\xb1\xe4\xb8\xa4\xe4\xb8\xaadilation\xe7\x9a\x84\xe5\x8d\xb7\xe7\xa7\xaf\xe6\xa8\xa1\xe5\x9d\x97\xe6\x9e\x84\xe6\x88\x90\xef\xbc\x8c\xe7\x94\xb1\xe4\xba\x8estride=1\xef\xbc\x8cdilation\xe4\xb8\xba1\xef\xbc\x8ckernel\xe4\xb8\xba3\n        # \xe9\x82\xa3\xe4\xb9\x88\xe7\x9b\xb8\xe5\xbd\x93\xe4\xba\x8ekernel\xe4\xb8\xba6\xe7\x9a\x84\xe5\x8d\xb7\xe7\xa7\xaf\xe6\xa0\xb8\xef\xbc\x8cpadding\xe4\xb8\xba1\n        self.conv1 = conv3x3(inplanes, planes, stride, padding=dilation[0], dilation=dilation[0])\n        # self.conv1 = conv3x3_asymmetric(inplanes, planes, stride, padding=dilation[0], dilation=dilation[0])\n        self.bn1 = nn.BatchNorm2d(planes)\n        for i in self.bn1.parameters():\n            i.requires_grad = False\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = conv3x3(planes, planes, padding=dilation[1], dilation=dilation[1])\n        # self.conv2 = conv3x3_asymmetric(planes, planes, padding=dilation[1], dilation=dilation[1])\n        self.bn2 = nn.BatchNorm2d(planes)\n        for i in self.bn2.parameters():\n            i.requires_grad = False\n        self.downsample = downsample\n        self.stride = stride\n        self.residual = residual\n\n    def forward(self, x):\n        residual = x\n\n        # print(x.data.size())\n        out = self.conv1(x)\n        # print(out.data.size())\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n        if self.residual:\n            out += residual\n        out = self.relu(out)\n\n        return out\n\nclass Bottleneck(nn.Module):\n    expansion = 4\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None, dilation=(1, 1)):\n        super(Bottleneck, self).__init__()\n        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(planes)\n        for i in self.bn1.parameters():\n            i.requires_grad = False\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=dilation[1], bias=False, dilation=dilation[1])\n        self.bn2 = nn.BatchNorm2d(planes)\n        for i in self.bn2.parameters():\n            i.requires_grad = False\n        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)\n        self.bn3 = nn.BatchNorm2d(planes * 4)\n        for i in self.bn3.parameters():\n            i.requires_grad = False\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\nclass DRNPred_A(nn.Module):\n\n    def __init__(self, block, layers, input_channel=3):\n        self.inplanes = 64\n        super(DRNPred_A, self).__init__()\n        self.block = block\n        self.layers = layers\n        self.input_channel = input_channel\n\n        self.out_dim = 512 * block.expansion\n        self.conv1 = nn.Conv2d(self.input_channel, 64, kernel_size=7, stride=2, padding=3, bias=False)\n        self.bn1 = nn.BatchNorm2d(64)\n        for i in self.bn1.parameters():\n            i.requires_grad = False\n        self.relu = nn.ReLU(inplace=True)\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n        self.layer1 = self._make_layer(block, 64, layers[0])\n        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n        self.layer3 = self._make_layer(block, 256, layers[2], stride=1, dilation=2)\n        self.layer4 = self._make_layer(block, 512, layers[3], stride=1, dilation=4)\n        # self.avgpool = nn.AvgPool2d(28, stride=1)\n        # self.fc = nn.Linear(512 * block.expansion, num_classes)\n        self.layer5 = self._make_pred_layer(ASPP_Classifier_Module, [6, 12, 18, 24], [6, 12, 18, 24], input_channel, in_channels=512*block.expansion)\n        if self.layer5 is not None:\n            self.out_dim = input_channel\n            pass\n        else:\n            self.out_dim = 512 * block.expansion\n            pass\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n                # m.weight.data.normal_(0, math.sqrt(2. / n))\n                m.weight.data.normal_(0, 0.01)\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n\n\n    def _make_layer(self, block, planes, blocks, stride=1, dilation=1):\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                nn.Conv2d(self.inplanes, planes * block.expansion,\n                          kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(planes * block.expansion),\n            )\n            for i in downsample._modules[\'1\'].parameters():\n                i.requires_grad = False\n\n\n        layers = []\n        layers.append(block(self.inplanes, planes, stride, downsample))\n        self.inplanes = planes * block.expansion\n        for i in range(1, blocks):\n            # print(\'blocks_i:\', i)\n            layers.append(block(self.inplanes, planes, dilation=(dilation, dilation)))\n\n        return nn.Sequential(*layers)\n\n    def _make_pred_layer(self, block, dilation_series, padding_series, n_classes, in_channels):\n        return block(dilation_series, padding_series, n_classes, in_channels)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n\n        # x = self.avgpool(x)\n        # x = x.view(x.size(0), -1)\n        # x = self.fc(x)\n        # print(\'x.size():\', x.size())\n        x = self.layer5(x)\n        # print(\'x.size():\', x.size())\n\n        return x\n\n\ndef drnpred_a_18(pretrained=False, **kwargs):\n    model = DRNPred_A(BasicBlock, [2, 2, 2, 2], **kwargs)\n    return model\n\ndef drnpred_a_34(pretrained=False, **kwargs):\n    model = DRNPred_A(BasicBlock, [3, 4, 6, 3], **kwargs)\n    return model\n\ndef drnpred_a_101(pretrained=False, **kwargs):\n    model = DRNPred_A(Bottleneck, [3, 4, 23, 3], **kwargs)\n    return model\n\n# -------------------------semantic model----------------------------------\ndef drnsegpred_a_18(pretrained=False, n_classes=21, input_shape = (64, 64), input_channel=19):\n    model = DRNSegPred(model_name=\'drnpred_a_18\', pretrained=pretrained, input_channel=input_channel, input_shape=input_shape, n_classes=n_classes)\n    return model\n\ndef drnsegpred_a_34(pretrained=False, n_classes=21, input_shape = (64, 64), input_channel=19):\n    model = DRNSegPred(model_name=\'drnpred_a_34\', pretrained=pretrained, input_channel=input_channel, input_shape=input_shape, n_classes=n_classes)\n    return model\n\ndef drnsegpred_a_101(pretrained=False, n_classes=21, input_shape = (64, 64), input_channel=19):\n    model = DRNSegPred(model_name=\'drnpred_a_101\', pretrained=pretrained, input_channel=input_channel, input_shape=input_shape, n_classes=n_classes)\n    return model\n# -----------------------------------------------------------\n\n# \xe8\xbd\xac\xe7\xbd\xae\xe5\x8d\xb7\xe7\xa7\xaf\xe6\x9d\x83\xe9\x87\x8d\xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96\xe5\xa1\xab\xe5\x85\x85\xe6\x96\xb9\xe6\xb3\x95\ndef fill_up_weights(up):\n    w = up.weight.data\n    f = math.ceil(w.size(2) / 2)\n    c = (2 * f - 1 - f % 2) / (2. * f)\n    for i in range(w.size(2)):\n        for j in range(w.size(3)):\n            w[0, 0, i, j] = (1 - math.fabs(i / f - c)) * (1 - math.fabs(j / f - c))\n    for c in range(1, w.size(0)):\n        w[c, 0, :, :] = w[0, 0, :, :]\n\n# drn segnet network\nclass DRNSegPred(nn.Module):\n    def __init__(self, model_name, pretrained=False, use_torch_up=True, input_channel=19, input_shape=(64, 64), n_classes=21):\n        super(DRNSegPred, self).__init__()\n        self.input_shape = input_shape\n        self.n_classes = n_classes\n        self.input_channel = input_channel\n\n        model = eval(model_name)(pretrained=pretrained, input_channel=input_channel*4)\n        self.base = model\n        self.seg = nn.Conv2d(model.out_dim, input_channel*4, kernel_size=1)\n        m = self.seg\n\n        # \xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96\xe5\x88\x86\xe5\x89\xb2\xe5\x9b\xbe\xe6\x9c\x80\xe5\x90\x8e\xe7\x9a\x84\xe5\x8d\xb7\xe7\xa7\xafweights\xe5\x92\x8cbias\n        n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n        m.weight.data.normal_(0, math.sqrt(2. / n))\n        m.bias.data.zero_()\n\n        # ----lstm seq----\n        # self.lstm1 = nn.LSTM(8*8, 8*8, 2)\n        # ----lstm seq----\n\n        # ----conv lstm seq----\n        self.lstm1 = ConvLSTM(input_size=(self.input_shape[0]//8, self.input_shape[1]//8), input_dim=self.input_channel, hidden_dim=[128, 128, self.input_channel], kernel_size=(3, 3), num_layers=3, batch_first=True, bias=True, return_all_layers=False)\n        # ----conv lstm seq----\n\n        self.out_conv = nn.Conv2d(self.input_channel*4, self.n_classes, kernel_size=1)\n\n        if use_torch_up:\n            # \xe4\xbd\xbf\xe7\x94\xa8pytorch\xe5\x8f\x8c\xe7\xba\xbf\xe6\x80\xa7\xe4\xb8\x8a\xe9\x87\x87\xe6\xa0\xb7\n            self.up = nn.UpsamplingBilinear2d(scale_factor=8)\n        else:\n            # \xe4\xbd\xbf\xe7\x94\xa8\xe8\xbd\xac\xe7\xbd\xae\xe5\x8d\xb7\xe7\xa7\xaf\xe4\xb8\x8a\xe9\x87\x87\xe6\xa0\xb7\n            up = nn.ConvTranspose2d(input_channel*4, input_channel*4, 16, stride=8, padding=4, output_padding=0, groups=input_channel*4, bias=False)\n            fill_up_weights(up)\n            up.weight.requires_grad = False\n            self.up = up\n\n    def forward(self, x):\n        x = self.base(x)\n        # \xe5\xb0\x86\xe5\x88\x86\xe5\x89\xb2\xe5\x9b\xbe\xe5\xaf\xb9\xe5\xba\x94\xe5\x88\xb0\xe5\x88\x86\xe5\x89\xb2\xe7\xb1\xbb\xe5\x88\xab\xe6\x95\xb0\xe4\xb8\x8a\n        x = self.seg(x)\n        # print(\'x.shape:\', x.shape)\n\n        # ----lstm seq----\n        # x = x.view(-1, 76, 8*8)\n        # x, _ = self.lstm1(x)\n        # x = x.view(-1, 76, 8, 8)\n        # ----lstm seq----\n\n        # ----conv lstm seq----\n        # print(\'x.shape:\', x.shape)\n        # print(\'self.input_channel:\', self.input_channel)\n        x = x.view(-1, 4, self.input_channel, self.input_shape[0]//8, self.input_shape[1]//8)\n        # print(\'x.shape:\', x.shape)\n        x, _ = self.lstm1(x)\n        # print(\'x.shape:\', x.shape)\n        x = x.view(-1, 4*self.input_channel, self.input_shape[0]//8, self.input_shape[1]//8)\n        # ----conv lstm seq----\n\n        x = self.out_conv(x)\n        # \xe4\xbd\xbf\xe7\x94\xa8\xe5\x8f\x8c\xe7\xba\xbf\xe6\x80\xa7\xe4\xb8\x8a\xe9\x87\x87\xe6\xa0\xb7\xe6\x88\x96\xe8\x80\x85\xe8\xbd\xac\xe7\xbd\xae\xe5\x8d\xb7\xe7\xa7\xaf\xe4\xb8\x8a\xe9\x87\x87\xe6\xa0\xb78\xe5\x80\x8d\xe9\x99\x8d\xe9\x87\x87\xe6\xa0\xb7\xe7\x8e\x87\xe7\x9a\x84\xe5\x88\x86\xe5\x89\xb2\xe5\x9b\xbe\n        y = self.up(x)\n        return y\n\n\n\nif __name__ == \'__main__\':\n    n_classes = 19\n    # model = DRNSegPred(model_name=\'drnpred_a_18\', pretrained=False, input_channel=n_classes*4)\n    model = drnsegpred_a_18(pretrained=False, n_classes=n_classes)\n    x = Variable(torch.randn(1, n_classes*4, 64, 64))\n    y = Variable(torch.LongTensor(np.ones((1, 64, 64), dtype=np.int)))\n    start = time.time()\n    pred = model(x)\n    end = time.time()\n    print(end-start)\n    print(pred.shape)\n    # loss = cross_entropy2d(pred, y)\n    # print(loss)\n'"
semseg/modelloader/duc_hdc.py,5,"b""#!/usr/bin/python\n# -*- coding: UTF-8 -*-\nimport time\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nfrom torchvision import models\n\n# \xe8\xaf\xa5\xe4\xbb\xa3\xe7\xa0\x81\xe6\x9d\xa5\xe8\x87\xaa\xe4\xba\x8e[duc_hdc.py](https://github.com/ZijunDeng/pytorch-semantic-segmentation/blob/master/models/duc_hdc.py)\nfrom semseg.loss import cross_entropy2d\n\n\nclass _DenseUpsamplingConvModule(nn.Module):\n    def __init__(self, down_factor, in_dim, n_classes):\n        super(_DenseUpsamplingConvModule, self).__init__()\n        upsample_dim = (down_factor ** 2) * n_classes\n        self.conv = nn.Conv2d(in_dim, upsample_dim, kernel_size=3, padding=1)\n        self.bn = nn.BatchNorm2d(upsample_dim)\n        self.relu = nn.ReLU(inplace=True)\n        self.pixel_shuffle = nn.PixelShuffle(down_factor)\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = self.bn(x)\n        x = self.relu(x)\n        x = self.pixel_shuffle(x)\n        return x\n\n\nclass ResNetDUC(nn.Module):\n    # the size of image should be multiple of 8\n    def __init__(self, n_classes=21, pretrained=False):\n        super(ResNetDUC, self).__init__()\n        resnet = models.resnet152(pretrained=pretrained)\n        self.layer0 = nn.Sequential(resnet.conv1, resnet.bn1, resnet.relu, resnet.maxpool)\n        self.layer1 = resnet.layer1\n        self.layer2 = resnet.layer2\n        self.layer3 = resnet.layer3\n        self.layer4 = resnet.layer4\n\n        for n, m in self.layer3.named_modules():\n            if 'conv2' in n:\n                m.dilation = (2, 2)\n                m.padding = (2, 2)\n                m.stride = (1, 1)\n            elif 'downsample.0' in n:\n                m.stride = (1, 1)\n        for n, m in self.layer4.named_modules():\n            if 'conv2' in n:\n                m.dilation = (4, 4)\n                m.padding = (4, 4)\n                m.stride = (1, 1)\n            elif 'downsample.0' in n:\n                m.stride = (1, 1)\n\n        self.duc = _DenseUpsamplingConvModule(8, 2048, n_classes)\n\n    def forward(self, x):\n        x = self.layer0(x)\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n        x = self.duc(x)\n        return x\n\nclass ResNetDUCHDC(nn.Module):\n    # the size of image should be multiple of 8\n    def __init__(self, n_classes, pretrained=True):\n        super(ResNetDUCHDC, self).__init__()\n        resnet = models.resnet152(pretrained=pretrained)\n        self.layer0 = nn.Sequential(resnet.conv1, resnet.bn1, resnet.relu, resnet.maxpool)\n        self.layer1 = resnet.layer1\n        self.layer2 = resnet.layer2\n        self.layer3 = resnet.layer3\n        self.layer4 = resnet.layer4\n\n        for n, m in self.layer3.named_modules():\n            if 'conv2' in n or 'downsample.0' in n:\n                m.stride = (1, 1)\n        for n, m in self.layer4.named_modules():\n            if 'conv2' in n or 'downsample.0' in n:\n                m.stride = (1, 1)\n        layer3_group_config = [1, 2, 5, 9]\n        for idx in range(len(self.layer3)):\n            self.layer3[idx].conv2.dilation = (layer3_group_config[idx % 4], layer3_group_config[idx % 4])\n            self.layer3[idx].conv2.padding = (layer3_group_config[idx % 4], layer3_group_config[idx % 4])\n        layer4_group_config = [5, 9, 17]\n        for idx in range(len(self.layer4)):\n            self.layer4[idx].conv2.dilation = (layer4_group_config[idx], layer4_group_config[idx])\n            self.layer4[idx].conv2.padding = (layer4_group_config[idx], layer4_group_config[idx])\n\n        self.duc = _DenseUpsamplingConvModule(8, 2048, n_classes)\n\n    def forward(self, x):\n        x = self.layer0(x)\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n        x = self.duc(x)\n        return x\n\nif __name__ == '__main__':\n    n_classes = 21\n    model = ResNetDUC(n_classes=n_classes, pretrained=False)\n    # model.init_vgg16()\n    x = Variable(torch.randn(1, 3, 360, 480))\n    y = Variable(torch.LongTensor(np.ones((1, 360, 480), dtype=np.int)))\n    # print(x.shape)\n    start = time.time()\n    pred = model(x)\n    end = time.time()\n    print(end-start)\n    # print(pred.shape)\n    loss = cross_entropy2d(pred, y)\n    # print(loss)\n"""
semseg/modelloader/enet.py,10,"b'#!/usr/bin/python\n# -*- coding: UTF-8 -*-\n\n# !/usr/bin/env python\n""""""\nA quick, partial implementation of ENet (https://arxiv.org/abs/1606.02147) using PyTorch.\nThe original Torch ENet implementation can process a 480x360 image in ~12 ms (on a P2 AWS\ninstance).  TensorFlow takes ~35 ms.  The PyTorch implementation takes ~25 ms, an improvement\nover TensorFlow, but worse than the original Torch.\n""""""\n\nfrom __future__ import absolute_import\n\nimport sys\nimport time\n\nimport torch\nimport torch.optim\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\n\nENCODER_PARAMS = [\n    # \xe7\xac\xac\xe4\xb8\x80\xe5\xb1\x82encoder\xe8\xbe\x93\xe5\x85\xa5\xe5\xb0\xba\xe5\xba\xa6\xe4\xb8\xba16\xef\xbc\x8c\xe8\xbe\x93\xe5\x87\xba\xe5\xb0\xba\xe5\xba\xa6\xe4\xb8\xba64\xe5\xb9\xb6\xe4\xb8\x94\xe5\xad\x98\xe5\x9c\xa8\xe4\xb8\x8b\xe9\x87\x87\xe6\xa0\xb7\xef\xbc\x8c\xe4\xb8\x8d\xe6\x98\xafdilated\xe7\xa9\xba\xe6\xb4\x9e\xe5\x8d\xb7\xe7\xa7\xaf\xef\xbc\x8c\xe6\x98\xaf\xe5\xaf\xb9\xe7\xa7\xb0\n    {\n        \'internal_scale\': 4,\n        \'use_relu\': True,\n        \'asymmetric\': False,\n        \'dilated\': False,\n        \'input_channels\': 16,\n        \'output_channels\': 64,\n        \'downsample\': True,\n        \'dropout_prob\': 0.01\n    },\n    {\n        \'internal_scale\': 4,\n        \'use_relu\': True,\n        \'asymmetric\': False,\n        \'dilated\': False,\n        \'input_channels\': 64,\n        \'output_channels\': 64,\n        \'downsample\': False,\n        \'dropout_prob\': 0.01\n    },\n    {\n        \'internal_scale\': 4,\n        \'use_relu\': True,\n        \'asymmetric\': False,\n        \'dilated\': False,\n        \'input_channels\': 64,\n        \'output_channels\': 64,\n        \'downsample\': False,\n        \'dropout_prob\': 0.01\n    },\n    {\n        \'internal_scale\': 4,\n        \'use_relu\': True,\n        \'asymmetric\': False,\n        \'dilated\': False,\n        \'input_channels\': 64,\n        \'output_channels\': 64,\n        \'downsample\': False,\n        \'dropout_prob\': 0.01\n    },\n    {\n        \'internal_scale\': 4,\n        \'use_relu\': True,\n        \'asymmetric\': False,\n        \'dilated\': False,\n        \'input_channels\': 64,\n        \'output_channels\': 64,\n        \'downsample\': False,\n        \'dropout_prob\': 0.01\n    },\n    {\n        \'internal_scale\': 4,\n        \'use_relu\': True,\n        \'asymmetric\': False,\n        \'dilated\': False,\n        \'input_channels\': 64,\n        \'output_channels\': 128,\n        \'downsample\': True,\n        \'dropout_prob\': 0.1\n    },\n    {\n        \'internal_scale\': 4,\n        \'use_relu\': True,\n        \'asymmetric\': False,\n        \'dilated\': False,\n        \'input_channels\': 128,\n        \'output_channels\': 128,\n        \'downsample\': False,\n        \'dropout_prob\': 0.1\n    },\n    {\n        \'internal_scale\': 4,\n        \'use_relu\': True,\n        \'asymmetric\': False,\n        \'dilated\': False,\n        \'input_channels\': 128,\n        \'output_channels\': 128,\n        \'downsample\': False,\n        \'dropout_prob\': 0.1\n    },\n    {\n        \'internal_scale\': 4,\n        \'use_relu\': True,\n        \'asymmetric\': False,\n        \'dilated\': False,\n        \'input_channels\': 128,\n        \'output_channels\': 128,\n        \'downsample\': False,\n        \'dropout_prob\': 0.1\n    },\n    {\n        \'internal_scale\': 4,\n        \'use_relu\': True,\n        \'asymmetric\': False,\n        \'dilated\': False,\n        \'input_channels\': 128,\n        \'output_channels\': 128,\n        \'downsample\': False,\n        \'dropout_prob\': 0.1\n    },\n    {\n        \'internal_scale\': 4,\n        \'use_relu\': True,\n        \'asymmetric\': False,\n        \'dilated\': False,\n        \'input_channels\': 128,\n        \'output_channels\': 128,\n        \'downsample\': False,\n        \'dropout_prob\': 0.1\n    },\n    {\n        \'internal_scale\': 4,\n        \'use_relu\': True,\n        \'asymmetric\': False,\n        \'dilated\': False,\n        \'input_channels\': 128,\n        \'output_channels\': 128,\n        \'downsample\': False,\n        \'dropout_prob\': 0.1\n    },\n    {\n        \'internal_scale\': 4,\n        \'use_relu\': True,\n        \'asymmetric\': False,\n        \'dilated\': False,\n        \'input_channels\': 128,\n        \'output_channels\': 128,\n        \'downsample\': False,\n        \'dropout_prob\': 0.1\n    },\n    {\n        \'internal_scale\': 4,\n        \'use_relu\': True,\n        \'asymmetric\': False,\n        \'dilated\': False,\n        \'input_channels\': 128,\n        \'output_channels\': 128,\n        \'downsample\': False,\n        \'dropout_prob\': 0.1\n    },\n    {\n        \'internal_scale\': 4,\n        \'use_relu\': True,\n        \'asymmetric\': False,\n        \'dilated\': False,\n        \'input_channels\': 128,\n        \'output_channels\': 128,\n        \'downsample\': False,\n        \'dropout_prob\': 0.1\n    },\n    {\n        \'internal_scale\': 4,\n        \'use_relu\': True,\n        \'asymmetric\': False,\n        \'dilated\': False,\n        \'input_channels\': 128,\n        \'output_channels\': 128,\n        \'downsample\': False,\n        \'dropout_prob\': 0.1\n    },\n    {\n        \'internal_scale\': 4,\n        \'use_relu\': True,\n        \'asymmetric\': False,\n        \'dilated\': False,\n        \'input_channels\': 128,\n        \'output_channels\': 128,\n        \'downsample\': False,\n        \'dropout_prob\': 0.1\n    },\n    {\n        \'internal_scale\': 4,\n        \'use_relu\': True,\n        \'asymmetric\': False,\n        \'dilated\': False,\n        \'input_channels\': 128,\n        \'output_channels\': 128,\n        \'downsample\': False,\n        \'dropout_prob\': 0.1\n    },\n    {\n        \'internal_scale\': 4,\n        \'use_relu\': True,\n        \'asymmetric\': False,\n        \'dilated\': False,\n        \'input_channels\': 128,\n        \'output_channels\': 128,\n        \'downsample\': False,\n        \'dropout_prob\': 0.1\n    },\n    {\n        \'internal_scale\': 4,\n        \'use_relu\': True,\n        \'asymmetric\': False,\n        \'dilated\': False,\n        \'input_channels\': 128,\n        \'output_channels\': 128,\n        \'downsample\': False,\n        \'dropout_prob\': 0.1\n    },\n    {\n        \'internal_scale\': 4,\n        \'use_relu\': True,\n        \'asymmetric\': False,\n        \'dilated\': False,\n        \'input_channels\': 128,\n        \'output_channels\': 128,\n        \'downsample\': False,\n        \'dropout_prob\': 0.1\n    },\n    {\n        \'internal_scale\': 4,\n        \'use_relu\': True,\n        \'asymmetric\': False,\n        \'dilated\': False,\n        \'input_channels\': 128,\n        \'output_channels\': 128,\n        \'downsample\': False,\n        \'dropout_prob\': 0.1\n    },\n    {\n        \'internal_scale\': 4,\n        \'use_relu\': True,\n        \'asymmetric\': False,\n        \'dilated\': False,\n        \'input_channels\': 128,\n        \'output_channels\': 128,\n        \'downsample\': False,\n        \'dropout_prob\': 0.1\n    }\n]\n\nDECODER_PARAMS = [\n    {\n        \'input_channels\': 128,\n        \'output_channels\': 128,\n        \'upsample\': False,\n        \'pooling_module\': None\n    },\n    {\n        \'input_channels\': 128,\n        \'output_channels\': 64,\n        \'upsample\': True,\n        \'pooling_module\': None\n    },\n    {\n        \'input_channels\': 64,\n        \'output_channels\': 64,\n        \'upsample\': False,\n        \'pooling_module\': None\n    },\n    {\n        \'input_channels\': 64,\n        \'output_channels\': 64,\n        \'upsample\': False,\n        \'pooling_module\': None\n    },\n    {\n        \'input_channels\': 64,\n        \'output_channels\': 16,\n        \'upsample\': True,\n        \'pooling_module\': None\n    },\n    {\n        \'input_channels\': 16,\n        \'output_channels\': 16,\n        \'upsample\': False,\n        \'pooling_module\': None\n    }\n]\n\n\nclass InitialBlock(nn.Module):\n    def __init__(self):\n        super(InitialBlock, self).__init__()\n\n        # 3x3\xe5\x8d\xb7\xe7\xa7\xaf\xe5\xb1\x82\xef\xbc\x8c\xe8\xbe\x93\xe5\x85\xa5\xe4\xb8\xbabx3xhxw\xef\xbc\x8c\xe8\xbe\x93\xe5\x87\xba\xe4\xb8\xbabx13xh/2xw/2\n        self.conv = nn.Conv2d(\n            3, 13, (3, 3),\n            stride=2, padding=1, bias=True)\n        # \xe8\xbe\x93\xe5\x85\xa5\xe4\xb8\xba\xe5\x8e\x9f\xe5\xa7\x8b\xe5\x9b\xbe\xe5\x83\x8f\xef\xbc\x8c\xe8\xbf\x9b\xe8\xa1\x8c\xe6\xb1\xa0\xe5\x8c\x96\xe6\x93\x8d\xe4\xbd\x9cbx3xh/2xw/2\n        self.pool = nn.MaxPool2d(2, stride=2)\n        self.batch_norm = nn.BatchNorm2d(16, eps=1e-3)\n\n    def forward(self, input):\n        output = torch.cat([self.conv(input), self.pool(input)], 1)\n        output = self.batch_norm(output)\n        return F.relu(output)\n\n\nclass EncoderMainPath(nn.Module):\n    def __init__(self, internal_scale=None, use_relu=None, asymmetric=None, dilated=None, input_channels=None,\n                 output_channels=None, downsample=None, dropout_prob=None):\n        super(EncoderMainPath, self).__init__()\n\n        # \xe5\x86\x85\xe9\x83\xa8\xe5\xb0\xba\xe5\xba\xa6\n        internal_channels = output_channels // internal_scale\n        # \xe5\xa6\x82\xe6\x9e\x9cdownsample\xe4\xb8\x8b\xe9\x87\x87\xe6\xa0\xb7\xe9\x82\xa3\xe4\xb9\x88input_stride\xe4\xb8\xba2\xe5\x8f\x8d\xe4\xb9\x8b\xe4\xb8\xba1\n        input_stride = downsample and 2 or 1\n\n        # \xe6\x9b\xb4\xe6\x96\xb0\xe5\x86\x85\xe9\x83\xa8\xe5\x8f\x82\xe6\x95\xb0\n        self.__dict__.update(locals())\n        del self.self\n\n        # \xe7\xbc\x96\xe7\xa0\x81\xe5\x99\xa8\xe7\xac\xac\xe4\xb8\x80\xe5\xb1\x82\xe4\xb8\xba\xe5\x8d\xb7\xe7\xa7\xaf\xe5\xb1\x82\xef\xbc\x8cinput_channels\n        self.input_conv = nn.Conv2d(\n            input_channels, internal_channels, input_stride,\n            stride=input_stride, padding=0, bias=False)\n\n        self.input_batch_norm = nn.BatchNorm2d(internal_channels, eps=1e-03)\n\n        # TODO: use dilated and asymmetric convolutions, as in the\n        # original implementation.  For now just add a 3x3 convolution.\n        # \xe4\xb8\xad\xe9\x97\xb4\xe5\xb1\x82\xe4\xb8\xbadialated\xe5\xb1\x82\xef\xbc\x8c\xe8\xbf\x99\xe9\x87\x8c\xe4\xbd\xbf\xe7\x94\xa83x3\xe5\x8d\xb7\xe7\xa7\xaf\xe5\xb1\x82\xe4\xbb\xa3\xe6\x9b\xbf\n        self.middle_conv = nn.Conv2d(\n            internal_channels, internal_channels, 3,\n            stride=1, padding=1, bias=True)\n\n        self.middle_batch_norm = nn.BatchNorm2d(internal_channels, eps=1e-03)\n\n        # \xe8\xbe\x93\xe5\x87\xba\xe5\x8d\xb7\xe7\xa7\xaf\xe5\xb1\x82\n        self.output_conv = nn.Conv2d(\n            internal_channels, output_channels, 1,\n            stride=1, padding=0, bias=False)\n\n        self.output_batch_norm = nn.BatchNorm2d(output_channels, eps=1e-03)\n\n        self.dropout = nn.Dropout2d(dropout_prob)\n\n    def forward(self, input):\n        output = self.input_conv(input)\n\n        output = self.input_batch_norm(output)\n\n        output = F.relu(output)\n\n        output = self.middle_conv(output)\n\n        output = self.middle_batch_norm(output)\n\n        output = F.relu(output)\n\n        output = self.output_conv(output)\n\n        output = self.output_batch_norm(output)\n\n        output = self.dropout(output)\n\n        return output\n\n\n# \xe8\xbe\x85\xe5\x8a\xa9\xe7\x9a\x84\xe7\xbc\x96\xe7\xa0\x81\xe5\x99\xa8\xe6\xa8\xa1\xe5\x9d\x97\nclass EncoderOtherPath(nn.Module):\n    def __init__(self, internal_scale=None, use_relu=None, asymmetric=None, dilated=None, input_channels=None,\n                 output_channels=None, downsample=None, **kwargs):\n        super(EncoderOtherPath, self).__init__()\n\n        self.__dict__.update(locals())\n        del self.self\n\n        # \xe6\x9c\x89\xe4\xb8\x8b\xe9\x87\x87\xe6\xa0\xb7\xe6\x98\xaf\xe5\xa2\x9e\xe5\x8a\xa0\xe6\xb1\xa0\xe5\x8c\x96\xe5\xb1\x82\xef\xbc\x8c\xe8\xaf\xa5\xe5\xb1\x82\xe8\xbf\x94\xe5\x9b\x9e\xe6\xb1\xa0\xe5\x8c\x96\xe5\xb1\x82\xe7\x9a\x84indices\xe4\xbe\x9b\xe8\xa7\xa3\xe7\xa0\x81\xe5\x99\xa8\xe4\xbd\xbf\xe7\x94\xa8\n        if downsample:\n            self.pool = nn.MaxPool2d(2, stride=2, return_indices=True)\n\n    def forward(self, input):\n        output = input\n\n        if self.downsample:\n            output, self.indices = self.pool(input)\n\n        # \xe5\xa6\x82\xe6\x9e\x9c\xe8\xbe\x93\xe5\x85\xa5\xe9\x80\x9a\xe9\x81\x93\xe4\xb8\x8d\xe7\xad\x89\xe4\xba\x8e\xe8\xbe\x93\xe5\x87\xba\xe9\x80\x9a\xe9\x81\x93\xef\xbc\x8c\xe9\x87\x8d\xe5\xa4\x8doutput\n        if self.output_channels != self.input_channels:\n            new_size = [1, 1, 1, 1]\n            new_size[1] = self.output_channels // self.input_channels\n            output = output.repeat(*new_size)\n\n        return output\n\n\n# \xe5\xbe\xaa\xe7\x8e\xaf\xe7\x9a\x84\xe7\xbc\x96\xe7\xa0\x81\xe5\x99\xa8\xe6\xa8\xa1\xe5\x9d\x97\nclass EncoderModule(nn.Module):\n    def __init__(self, **kwargs):\n        super(EncoderModule, self).__init__()\n        # \xe6\xaf\x8f\xe4\xb8\x80\xe4\xb8\xaa\xe7\xbc\x96\xe7\xa0\x81\xe5\x99\xa8\xe7\x94\xb1\xe5\xb7\xa6\xe8\xbe\xb9\xe7\x9a\x84\xe8\x8b\xa5\xe5\xb9\xb2\xe4\xb8\xaa\xe5\x8d\xb7\xe7\xa7\xaf\xe5\xb1\x82\xe5\x8a\xa0\xe4\xb8\x8a\xe5\x8f\xb3\xe8\xbe\xb9\xe7\x9a\x84\xe4\xb8\x80\xe4\xb8\xaa\xe5\x8d\xb7\xe7\xa7\xaf\xe5\xb1\x82concat\xe7\xbb\x84\xe5\xb1\x82\n        self.main = EncoderMainPath(**kwargs)\n        self.other = EncoderOtherPath(**kwargs)\n\n    def forward(self, input):\n        main = self.main(input)\n        other = self.other(input)\n        return F.relu(main + other)\n\n\n# Encoder\xe6\xa8\xa1\xe5\x9d\x97\nclass Encoder(nn.Module):\n    def __init__(self, params, nclasses):\n        super(Encoder, self).__init__()\n        # \xe7\xbc\x96\xe7\xa0\x81\xe5\x99\xa8encode\xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96\xef\xbc\x8c\xe5\xb0\x86\xe5\x8d\xb7\xe7\xa7\xaf\xe5\xb1\x82\xe5\x92\x8c\xe6\xb1\xa0\xe5\x8c\x96\xe5\xb1\x82concat\xe5\x9c\xa8\xe4\xb8\x80\xe8\xb5\xb7\xe8\xbe\x93\xe5\x87\xbabx16xh/2xw/2\n        self.initial_block = InitialBlock()\n\n        self.layers = []\n        for i, params in enumerate(params):\n            # \xe8\xae\xbe\xe7\xbd\xae\xe7\xbc\x96\xe7\xa0\x81\xe5\x99\xa8layer name\xe5\xb9\xb6\xe4\xbc\xa0\xe9\x80\x92\xe5\x8f\x82\xe6\x95\xb0\n            layer_name = \'encoder_{:02d}\'.format(i)\n            layer = EncoderModule(**params)\n            super(Encoder, self).__setattr__(layer_name, layer)\n            self.layers.append(layer)\n\n        # \xe6\x9c\x80\xe5\x90\x8e\xe8\xbe\x93\xe5\x87\xba\xe5\xb1\x82\xe4\xb8\xbaBxnclassesxH/8xW/8\n        self.output_conv = nn.Conv2d(\n            128, nclasses, 1,\n            stride=1, padding=0, bias=True)\n\n    def forward(self, input, predict=False):\n        output = self.initial_block(input)\n\n        for layer in self.layers:\n            output = layer(output)\n\n        # \xe9\xa2\x84\xe6\xb5\x8b\xe6\x83\x85\xe5\x86\xb5\xe4\xb8\x8b\xe8\xbe\x93\xe5\x87\xba\xe5\x8d\xb7\xe7\xa7\xaf\xe5\xb1\x82\n        if predict:\n            output = self.output_conv(output)\n\n        return output\n\n\nclass DecoderMainPath(nn.Module):\n    def __init__(self, input_channels=None, output_channels=None, upsample=None, pooling_module=None):\n        super(DecoderMainPath, self).__init__()\n\n        internal_channels = output_channels // 4\n        input_stride = 2 if upsample is True else 1\n\n        self.__dict__.update(locals())\n        del self.self\n\n        self.input_conv = nn.Conv2d(\n            input_channels, internal_channels, 1,\n            stride=1, padding=0, bias=False)\n\n        self.input_batch_norm = nn.BatchNorm2d(internal_channels, eps=1e-03)\n\n        if not upsample:\n            self.middle_conv = nn.Conv2d(\n                internal_channels, internal_channels, 3,\n                stride=1, padding=1, bias=True)\n        else:\n            self.middle_conv = nn.ConvTranspose2d(\n                internal_channels, internal_channels, 3,\n                stride=2, padding=1, output_padding=1,\n                bias=True)\n\n        self.middle_batch_norm = nn.BatchNorm2d(internal_channels, eps=1e-03)\n\n        self.output_conv = nn.Conv2d(\n            internal_channels, output_channels, 1,\n            stride=1, padding=0, bias=False)\n\n        self.output_batch_norm = nn.BatchNorm2d(output_channels, eps=1e-03)\n\n    def forward(self, input):\n        output = self.input_conv(input)\n\n        output = self.input_batch_norm(output)\n\n        output = F.relu(output)\n\n        output = self.middle_conv(output)\n\n        output = self.middle_batch_norm(output)\n\n        output = F.relu(output)\n\n        output = self.output_conv(output)\n\n        output = self.output_batch_norm(output)\n\n        return output\n\n\nclass DecoderOtherPath(nn.Module):\n    def __init__(self, input_channels=None, output_channels=None, upsample=None, pooling_module=None):\n        super(DecoderOtherPath, self).__init__()\n\n        self.__dict__.update(locals())\n        del self.self\n\n        if output_channels != input_channels or upsample:\n            self.conv = nn.Conv2d(\n                input_channels, output_channels, 1,\n                stride=1, padding=0, bias=False)\n            self.batch_norm = nn.BatchNorm2d(output_channels, eps=1e-03)\n            if upsample and pooling_module:\n                self.unpool = nn.MaxUnpool2d(2, stride=2, padding=0)\n\n    def forward(self, input):\n        output = input\n\n        if self.output_channels != self.input_channels or self.upsample:\n            output = self.conv(output)\n            output = self.batch_norm(output)\n            if self.upsample and self.pooling_module:\n                output_size = list(output.size())\n                output_size[2] *= 2\n                output_size[3] *= 2\n                output = self.unpool(\n                    output, self.pooling_module.indices,\n                    output_size=output_size)\n\n        return output\n\n\nclass DecoderModule(nn.Module):\n    def __init__(self, **kwargs):\n        super(DecoderModule, self).__init__()\n\n        self.main = DecoderMainPath(**kwargs)\n        self.other = DecoderOtherPath(**kwargs)\n\n    def forward(self, input):\n        main = self.main(input)\n        other = self.other(input)\n        return F.relu(main + other)\n\n\nclass Decoder(nn.Module):\n    def __init__(self, params, nclasses, encoder):\n        super(Decoder, self).__init__()\n\n        self.encoder = encoder\n\n        self.pooling_modules = []\n\n        for mod in self.encoder.modules():\n            try:\n                if mod.other.downsample:\n                    self.pooling_modules.append(mod.other)\n            except AttributeError:\n                pass\n\n        self.layers = []\n        for i, params in enumerate(params):\n            # \xe5\xb0\x86\xe5\xaf\xb9\xe5\xba\x94\xe8\xa6\x81\xe4\xb8\x8a\xe9\x87\x87\xe6\xa0\xb7\xe7\x9a\x84\xe4\xb8\x8b\xe9\x87\x87\xe6\xa0\xb7\xe6\xa8\xa1\xe5\x9d\x97\xe6\xb7\xbb\xe5\x8a\xa0\xe5\x88\xb0\xe5\x8f\x82\xe6\x95\xb0\xe4\xb8\xad\n            if params[\'upsample\']:\n                params[\'pooling_module\'] = self.pooling_modules.pop(-1)\n            layer = DecoderModule(**params)\n            self.layers.append(layer)\n            layer_name = \'decoder{:02d}\'.format(i)\n            super(Decoder, self).__setattr__(layer_name, layer)\n\n        # \xe6\x9c\x80\xe5\x90\x8e\xe7\x9a\x84\xe8\xbe\x93\xe5\x87\xba\xe5\xb1\x82\xe4\xb8\xba\xe8\xbd\xac\xe7\xbd\xae\xe5\x8d\xb7\xe7\xa7\xaf\xef\xbc\x8c\xe5\xb0\x86\xe7\xbd\x91\xe7\xbb\x9c\xe5\xb1\x82\xe4\xb8\x8a\xe9\x87\x87\xe6\xa0\xb72\n        self.output_conv = nn.ConvTranspose2d(\n            16, nclasses, 2,\n            stride=2, padding=0, output_padding=0, bias=True)\n\n    def forward(self, input):\n        output = self.encoder(input, predict=False)\n\n        for layer in self.layers:\n            output = layer(output)\n\n        output = self.output_conv(output)\n\n        return output\n\n\nclass ENet(nn.Module):\n    def __init__(self, n_classes, pretrained=False):\n        super(ENet, self).__init__()\n\n        # enet\xe7\xbc\x96\xe7\xa0\x81\xe5\x99\xa8\xe8\xa7\xa3\xe7\xa0\x81\xe5\x99\xa8\xe6\x9e\xb6\xe6\x9e\x84\n        self.encoder = Encoder(ENCODER_PARAMS, n_classes)\n        self.decoder = Decoder(DECODER_PARAMS, n_classes, self.encoder)\n\n    def forward(self, input, only_encode=False, predict=True):\n        if only_encode:\n            return self.encoder.forward(input, predict=predict)\n        else:\n            return self.decoder.forward(input)\n\n\nif __name__ == \'__main__\':\n    nclasses = 15\n    train = True\n    niter = 5\n    times = torch.FloatTensor(niter)\n\n    batch_size = 1\n    nchannels = 3\n    height = 360\n    width = 480\n\n    model = ENet(nclasses)\n    loss = nn.NLLLoss2d()\n    softmax = nn.Softmax()\n\n    # model.cuda()\n    # loss.cuda()\n    # softmax.cuda()\n\n    optimizer = torch.optim.Adam(model.parameters())\n\n    for i in range(niter):\n        x = torch.FloatTensor(\n            torch.randn(batch_size, nchannels, height, width))\n        y = torch.LongTensor(batch_size, height, width)\n        y.random_(nclasses)\n\n        # x.pin_memory()\n        # y.pin_memory()\n\n        # input = Variable(x.cuda(async=True))\n        # target = Variable(y.cuda(async=True))\n        input = Variable(x)\n        target = Variable(y)\n\n        sys.stdout.write(\'\\r{}/{}\'.format(i, niter))\n\n        start = time.time()\n\n        if train:\n            optimizer.zero_grad()\n            model.train()\n        else:\n            model.eval()\n\n        output = model(input)\n\n        if train:\n            loss_ = loss.forward(output, target)\n            loss_.backward()\n            optimizer.step()\n        else:\n            output_2d = output.view(height * width, nclasses)\n            pred = softmax(output_2d).view(output.size())\n\n        times[i] = time.time() - start\n    sys.stdout.write(\'\\n\')\n\n    print(\'average time per image {:04f} sec\'.format(\n        times.mean()))\n'"
semseg/modelloader/enetv2.py,10,"b'#!/usr/bin/python\n# -*- coding: UTF-8 -*-\n\nimport torch.nn as nn\nimport torch\nfrom torch.autograd import Variable\nimport numpy as np\nimport torch\nimport time\nimport torch.nn.functional as F\n\n# code is from https://github.com/davidtvs/PyTorch-ENet/blob/master/models/enet.py\nfrom semseg.loss import cross_entropy2d\nfrom semseg.pytorch_modelsize import SizeEstimator\n\n\nclass InitialBlock(nn.Module):\n    """"""The initial block is composed of two branches:\n    1. a main branch which performs a regular convolution with stride 2;\n    2. an extension branch which performs max-pooling.\n    Doing both operations in parallel and concatenating their results\n    allows for efficient downsampling and expansion. The main branch\n    outputs 13 feature maps while the extension branch outputs 3, for a\n    total of 16 feature maps after concatenation.\n    Keyword arguments:\n    - in_channels (int): the number of input channels.\n    - out_channels (int): the number output channels.\n    - kernel_size (int, optional): the kernel size of the filters used in\n    the convolution layer. Default: 3.\n    - padding (int, optional): zero-padding added to both sides of the\n    input. Default: 0.\n    - bias (bool, optional): Adds a learnable bias to the output if\n    ``True``. Default: False.\n    - relu (bool, optional): When ``True`` ReLU is used as the activation\n    function; otherwise, PReLU is used. Default: True.\n    """"""\n\n    def __init__(self,\n                 in_channels,\n                 out_channels,\n                 kernel_size=3,\n                 padding=0,\n                 bias=False,\n                 relu=True):\n        super(InitialBlock, self).__init__()\n\n        if relu:\n            activation = nn.ReLU()\n        else:\n            activation = nn.PReLU()\n\n        # Main branch - As stated above the number of output channels for this\n        # branch is the total minus 3, since the remaining channels come from\n        # the extension branch\n        self.main_branch = nn.Conv2d(\n            in_channels,\n            out_channels - 3,\n            kernel_size=kernel_size,\n            stride=2,\n            padding=padding,\n            bias=bias)\n\n        # Extension branch\n        self.ext_branch = nn.MaxPool2d(kernel_size, stride=2, padding=padding)\n\n        # Initialize batch normalization to be used after concatenation\n        self.batch_norm = nn.BatchNorm2d(out_channels)\n\n        # PReLU layer to apply after concatenating the branches\n        self.out_prelu = activation\n\n    def forward(self, x):\n        main = self.main_branch(x)\n        ext = self.ext_branch(x)\n\n        # Concatenate branches\n        out = torch.cat((main, ext), 1)\n\n        # Apply batch normalization\n        out = self.batch_norm(out)\n\n        return self.out_prelu(out)\n\n\nclass RegularBottleneck(nn.Module):\n    """"""Regular bottlenecks are the main building block of ENet.\n    Main branch:\n    1. Shortcut connection.\n    Extension branch:\n    1. 1x1 convolution which decreases the number of channels by\n    ``internal_ratio``, also called a projection;\n    2. regular, dilated or asymmetric convolution;\n    3. 1x1 convolution which increases the number of channels back to\n    ``channels``, also called an expansion;\n    4. dropout as a regularizer.\n    Keyword arguments:\n    - channels (int): the number of input and output channels.\n    - internal_ratio (int, optional): a scale factor applied to\n    ``channels`` used to compute the number of\n    channels after the projection. eg. given ``channels`` equal to 128 and\n    internal_ratio equal to 2 the number of channels after the projection\n    is 64. Default: 4.\n    - kernel_size (int, optional): the kernel size of the filters used in\n    the convolution layer described above in item 2 of the extension\n    branch. Default: 3.\n    - padding (int, optional): zero-padding added to both sides of the\n    input. Default: 0.\n    - dilation (int, optional): spacing between kernel elements for the\n    convolution described in item 2 of the extension branch. Default: 1.\n    asymmetric (bool, optional): flags if the convolution described in\n    item 2 of the extension branch is asymmetric or not. Default: False.\n    - dropout_prob (float, optional): probability of an element to be\n    zeroed. Default: 0 (no dropout).\n    - bias (bool, optional): Adds a learnable bias to the output if\n    ``True``. Default: False.\n    - relu (bool, optional): When ``True`` ReLU is used as the activation\n    function; otherwise, PReLU is used. Default: True.\n    """"""\n\n    def __init__(self,\n                 channels,\n                 internal_ratio=4,\n                 kernel_size=3,\n                 padding=0,\n                 dilation=1,\n                 asymmetric=False,\n                 dropout_prob=0,\n                 bias=False,\n                 relu=True):\n        super(RegularBottleneck, self).__init__()\n\n        # Check in the internal_scale parameter is within the expected range\n        # [1, channels]\n        if internal_ratio <= 1 or internal_ratio > channels:\n            raise RuntimeError(""Value out of range. Expected value in the ""\n                               ""interval [1, {0}], got internal_scale={1}.""\n                               .format(channels, internal_ratio))\n\n        internal_channels = channels // internal_ratio\n\n        if relu:\n            activation = nn.ReLU()\n        else:\n            activation = nn.PReLU()\n\n        # Main branch - shortcut connection\n\n        # Extension branch - 1x1 convolution, followed by a regular, dilated or\n        # asymmetric convolution, followed by another 1x1 convolution, and,\n        # finally, a regularizer (spatial dropout). Number of channels is constant.\n\n        # 1x1 projection convolution\n        self.ext_conv1 = nn.Sequential(\n            nn.Conv2d(\n                channels,\n                internal_channels,\n                kernel_size=1,\n                stride=1,\n                bias=bias), nn.BatchNorm2d(internal_channels), activation)\n\n        # If the convolution is asymmetric we split the main convolution in\n        # two. Eg. for a 5x5 asymmetric convolution we have two convolution:\n        # the first is 5x1 and the second is 1x5.\n        if asymmetric:\n            self.ext_conv2 = nn.Sequential(\n                nn.Conv2d(\n                    internal_channels,\n                    internal_channels,\n                    kernel_size=(kernel_size, 1),\n                    stride=1,\n                    padding=(padding, 0),\n                    dilation=dilation,\n                    bias=bias), nn.BatchNorm2d(internal_channels), activation,\n                nn.Conv2d(\n                    internal_channels,\n                    internal_channels,\n                    kernel_size=(1, kernel_size),\n                    stride=1,\n                    padding=(0, padding),\n                    dilation=dilation,\n                    bias=bias), nn.BatchNorm2d(internal_channels), activation)\n        else:\n            # self.ext_conv2 = nn.Sequential(\n            #     nn.Conv2d(\n            #         internal_channels,\n            #         internal_channels,\n            #         kernel_size=(kernel_size, 1),\n            #         stride=1,\n            #         padding=(padding, 0),\n            #         dilation=dilation,\n            #         bias=bias), nn.BatchNorm2d(internal_channels), activation,\n            #     nn.Conv2d(\n            #         internal_channels,\n            #         internal_channels,\n            #         kernel_size=(1, kernel_size),\n            #         stride=1,\n            #         padding=(0, padding),\n            #         dilation=dilation,\n            #         bias=bias), nn.BatchNorm2d(internal_channels), activation)\n            self.ext_conv2 = nn.Sequential(\n                nn.Conv2d(\n                    internal_channels,\n                    internal_channels,\n                    kernel_size=kernel_size,\n                    stride=1,\n                    padding=padding,\n                    dilation=dilation,\n                    bias=bias), nn.BatchNorm2d(internal_channels), activation)\n\n        # 1x1 expansion convolution\n        self.ext_conv3 = nn.Sequential(\n            nn.Conv2d(\n                internal_channels,\n                channels,\n                kernel_size=1,\n                stride=1,\n                bias=bias), nn.BatchNorm2d(channels), activation)\n\n        self.ext_regul = nn.Dropout2d(p=dropout_prob)\n\n        # PReLU layer to apply after adding the branches\n        self.out_prelu = activation\n\n    def forward(self, x):\n        # Main branch shortcut\n        main = x\n\n        # Extension branch\n        ext = self.ext_conv1(x)\n        ext = self.ext_conv2(ext)\n        ext = self.ext_conv3(ext)\n        ext = self.ext_regul(ext)\n\n        # Add main and extension branches\n        out = main + ext\n\n        return self.out_prelu(out)\n\n\nclass DownsamplingBottleneck(nn.Module):\n    """"""Downsampling bottlenecks further downsample the feature map size.\n    Main branch:\n    1. max pooling with stride 2; indices are saved to be used for\n    unpooling later.\n    Extension branch:\n    1. 2x2 convolution with stride 2 that decreases the number of channels\n    by ``internal_ratio``, also called a projection;\n    2. regular convolution (by default, 3x3);\n    3. 1x1 convolution which increases the number of channels to\n    ``out_channels``, also called an expansion;\n    4. dropout as a regularizer.\n    Keyword arguments:\n    - in_channels (int): the number of input channels.\n    - out_channels (int): the number of output channels.\n    - internal_ratio (int, optional): a scale factor applied to ``channels``\n    used to compute the number of channels after the projection. eg. given\n    ``channels`` equal to 128 and internal_ratio equal to 2 the number of\n    channels after the projection is 64. Default: 4.\n    - kernel_size (int, optional): the kernel size of the filters used in\n    the convolution layer described above in item 2 of the extension branch.\n    Default: 3.\n    - padding (int, optional): zero-padding added to both sides of the\n    input. Default: 0.\n    - dilation (int, optional): spacing between kernel elements for the\n    convolution described in item 2 of the extension branch. Default: 1.\n    - asymmetric (bool, optional): flags if the convolution described in\n    item 2 of the extension branch is asymmetric or not. Default: False.\n    - return_indices (bool, optional):  if ``True``, will return the max\n    indices along with the outputs. Useful when unpooling later.\n    - dropout_prob (float, optional): probability of an element to be\n    zeroed. Default: 0 (no dropout).\n    - bias (bool, optional): Adds a learnable bias to the output if\n    ``True``. Default: False.\n    - relu (bool, optional): When ``True`` ReLU is used as the activation\n    function; otherwise, PReLU is used. Default: True.\n    """"""\n\n    def __init__(self,\n                 in_channels,\n                 out_channels,\n                 internal_ratio=4,\n                 kernel_size=3,\n                 padding=0,\n                 return_indices=False,\n                 dropout_prob=0,\n                 bias=False,\n                 relu=True):\n        super(DownsamplingBottleneck, self).__init__()\n\n        # Store parameters that are needed later\n        self.return_indices = return_indices\n\n        # Check in the internal_scale parameter is within the expected range\n        # [1, channels]\n        if internal_ratio <= 1 or internal_ratio > in_channels:\n            raise RuntimeError(""Value out of range. Expected value in the ""\n                               ""interval [1, {0}], got internal_scale={1}. ""\n                               .format(in_channels, internal_ratio))\n\n        internal_channels = in_channels // internal_ratio\n\n        if relu:\n            activation = nn.ReLU()\n        else:\n            activation = nn.PReLU()\n\n        # Main branch - max pooling followed by feature map (channels) padding\n        self.main_max1 = nn.MaxPool2d(\n            kernel_size,\n            stride=2,\n            padding=padding,\n            return_indices=return_indices)\n\n        # Extension branch - 2x2 convolution, followed by a regular, dilated or\n        # asymmetric convolution, followed by another 1x1 convolution. Number\n        # of channels is doubled.\n\n        # 2x2 projection convolution with stride 2\n        self.ext_conv1 = nn.Sequential(\n            nn.Conv2d(\n                in_channels,\n                internal_channels,\n                kernel_size=2,\n                stride=2,\n                bias=bias), nn.BatchNorm2d(internal_channels), activation)\n\n        # Convolution\n        self.ext_conv2 = nn.Sequential(\n            nn.Conv2d(\n                internal_channels,\n                internal_channels,\n                kernel_size=kernel_size,\n                stride=1,\n                padding=padding,\n                bias=bias), nn.BatchNorm2d(internal_channels), activation)\n\n        # 1x1 expansion convolution\n        self.ext_conv3 = nn.Sequential(\n            nn.Conv2d(\n                internal_channels,\n                out_channels,\n                kernel_size=1,\n                stride=1,\n                bias=bias), nn.BatchNorm2d(out_channels), activation)\n\n        self.ext_regul = nn.Dropout2d(p=dropout_prob)\n\n        # PReLU layer to apply after concatenating the branches\n        self.out_prelu = activation\n\n    def forward(self, x):\n        # Main branch shortcut\n        if self.return_indices:\n            main, max_indices = self.main_max1(x)\n        else:\n            main = self.main_max1(x)\n\n        # Extension branch\n        ext = self.ext_conv1(x)\n        ext = self.ext_conv2(ext)\n        ext = self.ext_conv3(ext)\n        ext = self.ext_regul(ext)\n\n        # Main branch channel padding\n        n, ch_ext, h, w = ext.size()\n        ch_main = main.size()[1]\n        padding = Variable(torch.zeros(n, ch_ext - ch_main, h, w))\n\n        # Before concatenating, check if main is on the CPU or GPU and\n        # convert padding accordingly\n        if main.is_cuda:\n            padding = padding.cuda()\n\n        # Concatenate\n        main = torch.cat((main, padding), 1)\n\n        # Add main and extension branches\n        out = main + ext\n\n        return self.out_prelu(out), max_indices\n\n\nclass UpsamplingBottleneck(nn.Module):\n    """"""The upsampling bottlenecks upsample the feature map resolution using max\n    pooling indices stored from the corresponding downsampling bottleneck.\n    Main branch:\n    1. 1x1 convolution with stride 1 that decreases the number of channels by\n    ``internal_ratio``, also called a projection;\n    2. max unpool layer using the max pool indices from the corresponding\n    downsampling max pool layer.\n    Extension branch:\n    1. 1x1 convolution with stride 1 that decreases the number of channels by\n    ``internal_ratio``, also called a projection;\n    2. transposed convolution (by default, 3x3);\n    3. 1x1 convolution which increases the number of channels to\n    ``out_channels``, also called an expansion;\n    4. dropout as a regularizer.\n    Keyword arguments:\n    - in_channels (int): the number of input channels.\n    - out_channels (int): the number of output channels.\n    - internal_ratio (int, optional): a scale factor applied to ``in_channels``\n     used to compute the number of channels after the projection. eg. given\n     ``in_channels`` equal to 128 and ``internal_ratio`` equal to 2 the number\n     of channels after the projection is 64. Default: 4.\n    - kernel_size (int, optional): the kernel size of the filters used in the\n    convolution layer described above in item 2 of the extension branch.\n    Default: 3.\n    - padding (int, optional): zero-padding added to both sides of the input.\n    Default: 0.\n    - dropout_prob (float, optional): probability of an element to be zeroed.\n    Default: 0 (no dropout).\n    - bias (bool, optional): Adds a learnable bias to the output if ``True``.\n    Default: False.\n    - relu (bool, optional): When ``True`` ReLU is used as the activation\n    function; otherwise, PReLU is used. Default: True.\n    """"""\n\n    def __init__(self,\n                 in_channels,\n                 out_channels,\n                 internal_ratio=4,\n                 kernel_size=3,\n                 padding=0,\n                 dropout_prob=0,\n                 bias=False,\n                 relu=True):\n        super(UpsamplingBottleneck, self).__init__()\n\n        # Check in the internal_scale parameter is within the expected range\n        # [1, channels]\n        if internal_ratio <= 1 or internal_ratio > in_channels:\n            raise RuntimeError(""Value out of range. Expected value in the ""\n                               ""interval [1, {0}], got internal_scale={1}. ""\n                               .format(in_channels, internal_ratio))\n\n        internal_channels = in_channels // internal_ratio\n\n        if relu:\n            activation = nn.ReLU()\n        else:\n            activation = nn.PReLU()\n\n        # Main branch - max pooling followed by feature map (channels) padding\n        self.main_conv1 = nn.Sequential(\n            nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=bias),\n            nn.BatchNorm2d(out_channels))\n\n        # Remember that the stride is the same as the kernel_size, just like\n        # the max pooling layers\n        self.main_unpool1 = nn.MaxUnpool2d(kernel_size=2)\n\n        # Extension branch - 1x1 convolution, followed by a regular, dilated or\n        # asymmetric convolution, followed by another 1x1 convolution. Number\n        # of channels is doubled.\n\n        # 1x1 projection convolution with stride 1\n        self.ext_conv1 = nn.Sequential(\n            nn.Conv2d(\n                in_channels, internal_channels, kernel_size=1, bias=bias),\n            nn.BatchNorm2d(internal_channels), activation)\n\n        # Transposed convolution\n        self.ext_conv2 = nn.Sequential(\n            nn.ConvTranspose2d(\n                internal_channels,\n                internal_channels,\n                kernel_size=kernel_size,\n                stride=2,\n                padding=padding,\n                output_padding=1,\n                bias=bias), nn.BatchNorm2d(internal_channels), activation)\n\n        # 1x1 expansion convolution\n        self.ext_conv3 = nn.Sequential(\n            nn.Conv2d(\n                internal_channels, out_channels, kernel_size=1, bias=bias),\n            nn.BatchNorm2d(out_channels), activation)\n\n        self.ext_regul = nn.Dropout2d(p=dropout_prob)\n\n        # PReLU layer to apply after concatenating the branches\n        self.out_prelu = activation\n\n    def forward(self, x, max_indices):\n        # Main branch shortcut\n        main = self.main_conv1(x)\n        main = self.main_unpool1(main, max_indices)\n        # Extension branch\n        ext = self.ext_conv1(x)\n        ext = self.ext_conv2(ext)\n        ext = self.ext_conv3(ext)\n        ext = self.ext_regul(ext)\n\n        # Add main and extension branches\n        out = main + ext\n\n        return self.out_prelu(out)\n\n\nclass ENetV2(nn.Module):\n    """"""Generate the ENet model.\n    Keyword arguments:\n    - n_classes (int): the number of classes to segment.\n    - encoder_relu (bool, optional): When ``True`` ReLU is used as the\n    activation function in the encoder blocks/layers; otherwise, PReLU\n    is used. Default: False.\n    - decoder_relu (bool, optional): When ``True`` ReLU is used as the\n    activation function in the decoder blocks/layers; otherwise, PReLU\n    is used. Default: True.\n    """"""\n\n    def __init__(self, n_classes, encoder_relu=False, decoder_relu=True, pretrained=False):\n        super(ENetV2, self).__init__()\n\n        self.initial_block = InitialBlock(3, 16, padding=1, relu=encoder_relu)\n\n        # Stage 1 - Encoder\n        self.downsample1_0 = DownsamplingBottleneck(\n            16,\n            64,\n            padding=1,\n            return_indices=True,\n            dropout_prob=0.01,\n            relu=encoder_relu)\n        self.regular1_1 = RegularBottleneck(\n            64, padding=1, dropout_prob=0.01, relu=encoder_relu)\n        self.regular1_2 = RegularBottleneck(\n            64, padding=1, dropout_prob=0.01, relu=encoder_relu)\n        self.regular1_3 = RegularBottleneck(\n            64, padding=1, dropout_prob=0.01, relu=encoder_relu)\n        self.regular1_4 = RegularBottleneck(\n            64, padding=1, dropout_prob=0.01, relu=encoder_relu)\n\n        # Stage 2 - Encoder\n        self.downsample2_0 = DownsamplingBottleneck(\n            64,\n            128,\n            padding=1,\n            return_indices=True,\n            dropout_prob=0.1,\n            relu=encoder_relu)\n        self.regular2_1 = RegularBottleneck(\n            128, padding=1, dropout_prob=0.1, relu=encoder_relu)\n        self.dilated2_2 = RegularBottleneck(\n            128, dilation=2, padding=2, dropout_prob=0.1, relu=encoder_relu)\n        self.asymmetric2_3 = RegularBottleneck(\n            128,\n            kernel_size=5,\n            padding=2,\n            asymmetric=True,\n            dropout_prob=0.1,\n            relu=encoder_relu)\n        self.dilated2_4 = RegularBottleneck(\n            128, dilation=4, padding=4, dropout_prob=0.1, relu=encoder_relu)\n        self.regular2_5 = RegularBottleneck(\n            128, padding=1, dropout_prob=0.1, relu=encoder_relu)\n        self.dilated2_6 = RegularBottleneck(\n            128, dilation=8, padding=8, dropout_prob=0.1, relu=encoder_relu)\n        self.asymmetric2_7 = RegularBottleneck(\n            128,\n            kernel_size=5,\n            asymmetric=True,\n            padding=2,\n            dropout_prob=0.1,\n            relu=encoder_relu)\n        self.dilated2_8 = RegularBottleneck(\n            128, dilation=16, padding=16, dropout_prob=0.1, relu=encoder_relu)\n\n        # Stage 3 - Encoder\n        self.regular3_0 = RegularBottleneck(\n            128, padding=1, dropout_prob=0.1, relu=encoder_relu)\n        self.dilated3_1 = RegularBottleneck(\n            128, dilation=2, padding=2, dropout_prob=0.1, relu=encoder_relu)\n        self.asymmetric3_2 = RegularBottleneck(\n            128,\n            kernel_size=5,\n            padding=2,\n            asymmetric=True,\n            dropout_prob=0.1,\n            relu=encoder_relu)\n        self.dilated3_3 = RegularBottleneck(\n            128, dilation=4, padding=4, dropout_prob=0.1, relu=encoder_relu)\n        self.regular3_4 = RegularBottleneck(\n            128, padding=1, dropout_prob=0.1, relu=encoder_relu)\n        self.dilated3_5 = RegularBottleneck(\n            128, dilation=8, padding=8, dropout_prob=0.1, relu=encoder_relu)\n        self.asymmetric3_6 = RegularBottleneck(\n            128,\n            kernel_size=5,\n            asymmetric=True,\n            padding=2,\n            dropout_prob=0.1,\n            relu=encoder_relu)\n        self.dilated3_7 = RegularBottleneck(\n            128, dilation=16, padding=16, dropout_prob=0.1, relu=encoder_relu)\n\n        # Stage 4 - Decoder\n        self.upsample4_0 = UpsamplingBottleneck(\n            128, 64, padding=1, dropout_prob=0.1, relu=decoder_relu)\n        self.regular4_1 = RegularBottleneck(\n            64, padding=1, dropout_prob=0.1, relu=decoder_relu)\n        self.regular4_2 = RegularBottleneck(\n            64, padding=1, dropout_prob=0.1, relu=decoder_relu)\n\n        # Stage 5 - Decoder\n        self.upsample5_0 = UpsamplingBottleneck(\n            64, 16, padding=1, dropout_prob=0.1, relu=decoder_relu)\n        self.regular5_1 = RegularBottleneck(\n            16, padding=1, dropout_prob=0.1, relu=decoder_relu)\n        self.transposed_conv = nn.ConvTranspose2d(\n            16,\n            n_classes,\n            kernel_size=3,\n            stride=2,\n            padding=1,\n            output_padding=1,\n            bias=False)\n\n    def forward(self, x):\n        # Initial block\n        x = self.initial_block(x)\n\n        # Stage 1 - Encoder\n        x, max_indices1_0 = self.downsample1_0(x)\n        x = self.regular1_1(x)\n        x = self.regular1_2(x)\n        x = self.regular1_3(x)\n        x = self.regular1_4(x)\n\n        # Stage 2 - Encoder\n        x, max_indices2_0 = self.downsample2_0(x)\n        x = self.regular2_1(x)\n        x = self.dilated2_2(x)\n        x = self.asymmetric2_3(x)\n        x = self.dilated2_4(x)\n        x = self.regular2_5(x)\n        x = self.dilated2_6(x)\n        x = self.asymmetric2_7(x)\n        x = self.dilated2_8(x)\n\n        # Stage 3 - Encoder\n        x = self.regular3_0(x)\n        x = self.dilated3_1(x)\n        x = self.asymmetric3_2(x)\n        x = self.dilated3_3(x)\n        x = self.regular3_4(x)\n        x = self.dilated3_5(x)\n        x = self.asymmetric3_6(x)\n        x = self.dilated3_7(x)\n\n        # Stage 4 - Decoder\n        x = self.upsample4_0(x, max_indices2_0)\n        x = self.regular4_1(x)\n        x = self.regular4_2(x)\n\n        # Stage 5 - Decoder\n        x = self.upsample5_0(x, max_indices1_0)\n        x = self.regular5_1(x)\n        x = self.transposed_conv(x)\n\n        return x\n\nif __name__ == \'__main__\':\n    n_classes = 21\n    model = ENetV2(n_classes=n_classes)\n    # model.init_vgg16()\n    x = Variable(torch.randn(1, 3, 360, 480))\n    y = Variable(torch.LongTensor(np.ones((1, 360, 480), dtype=np.int)))\n    # x = Variable(torch.randn(1, 3, 512, 1024))\n    # y = Variable(torch.LongTensor(np.ones((1, 512, 1024), dtype=np.int)))\n    # print(x.shape)\n\n    time_avg = 1\n    start = time.time()\n    for run_time in range(time_avg):\n        pred = model(x)\n    end = time.time()\n    print(end-start)/time_avg\n    # print(pred.shape)\n    # print(\'pred.type:\', pred.type)\n    loss = cross_entropy2d(pred, y)\n    # print(loss)\n\n    # se = SizeEstimator(model, input_size=(1, 3, 360, 480))\n    # print(se.estimate_size())\n'"
semseg/modelloader/erfnet.py,9,"b""#!/usr/bin/python\n# -*- coding: UTF-8 -*-\n\nimport time\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nfrom torchvision import models\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.init as init\nimport torch.nn.functional as F\n\nfrom semseg.loss import cross_entropy2d\n\n\nclass DownsamplerBlock(nn.Module):\n    def __init__(self, ninput, noutput):\n        super(DownsamplerBlock, self).__init__()\n\n        self.conv = nn.Conv2d(ninput, noutput - ninput, (3, 3), stride=2, padding=1, bias=True)\n        self.pool = nn.MaxPool2d(2, stride=2)\n        self.bn = nn.BatchNorm2d(noutput, eps=1e-3)\n\n    def forward(self, input):\n        output = torch.cat([self.conv(input), self.pool(input)], 1)\n        output = self.bn(output)\n        return F.relu(output)\n\n\nclass non_bottleneck_1d(nn.Module):\n    def __init__(self, chann, dropprob, dilated):\n        super(non_bottleneck_1d, self).__init__()\n\n        self.conv3x1_1 = nn.Conv2d(chann, chann, (3, 1), stride=1, padding=(1, 0), bias=True)\n\n        self.conv1x3_1 = nn.Conv2d(chann, chann, (1, 3), stride=1, padding=(0, 1), bias=True)\n\n        self.bn1 = nn.BatchNorm2d(chann, eps=1e-03)\n\n        self.conv3x1_2 = nn.Conv2d(chann, chann, (3, 1), stride=1, padding=(1 * dilated, 0), bias=True,\n                                   dilation=(dilated, 1))\n\n        self.conv1x3_2 = nn.Conv2d(chann, chann, (1, 3), stride=1, padding=(0, 1 * dilated), bias=True,\n                                   dilation=(1, dilated))\n\n        self.bn2 = nn.BatchNorm2d(chann, eps=1e-03)\n\n        self.dropout = nn.Dropout2d(dropprob)\n\n    def forward(self, input):\n        output = self.conv3x1_1(input)\n        output = F.relu(output)\n        output = self.conv1x3_1(output)\n        output = self.bn1(output)\n        output = F.relu(output)\n\n        output = self.conv3x1_2(output)\n        output = F.relu(output)\n        output = self.conv1x3_2(output)\n        output = self.bn2(output)\n\n        if (self.dropout.p != 0):\n            output = self.dropout(output)\n\n        return F.relu(output + input)  # +input = identity (residual connection)\n\n\nclass Encoder(nn.Module):\n    def __init__(self, num_classes):\n        super(Encoder, self).__init__()\n        self.initial_block = DownsamplerBlock(3, 16)\n\n        self.layers = nn.ModuleList()\n\n        self.layers.append(DownsamplerBlock(16, 64))\n\n        for x in range(0, 5):  # 5 times\n            self.layers.append(non_bottleneck_1d(64, 0.03, 1))\n\n        self.layers.append(DownsamplerBlock(64, 128))\n\n        for x in range(0, 2):  # 2 times\n            self.layers.append(non_bottleneck_1d(128, 0.3, 2))\n            self.layers.append(non_bottleneck_1d(128, 0.3, 4))\n            self.layers.append(non_bottleneck_1d(128, 0.3, 8))\n            self.layers.append(non_bottleneck_1d(128, 0.3, 16))\n\n        # Only in encoder mode:\n        self.output_conv = nn.Conv2d(128, num_classes, 1, stride=1, padding=0, bias=True)\n\n    def forward(self, input, predict=False):\n        output = self.initial_block(input)\n\n        for layer in self.layers:\n            output = layer(output)\n\n        if predict:\n            output = self.output_conv(output)\n\n        return output\n\n\nclass UpsamplerBlock(nn.Module):\n    def __init__(self, ninput, noutput):\n        super(UpsamplerBlock, self).__init__()\n        self.conv = nn.ConvTranspose2d(ninput, noutput, 3, stride=2, padding=1, output_padding=1, bias=True)\n        self.bn = nn.BatchNorm2d(noutput, eps=1e-3)\n\n    def forward(self, input):\n        output = self.conv(input)\n        output = self.bn(output)\n        return F.relu(output)\n\n\nclass Decoder(nn.Module):\n    def __init__(self, num_classes):\n        super(Decoder, self).__init__()\n\n        self.layers = nn.ModuleList()\n\n        self.layers.append(UpsamplerBlock(128, 64))\n        self.layers.append(non_bottleneck_1d(64, 0, 1))\n        self.layers.append(non_bottleneck_1d(64, 0, 1))\n\n        self.layers.append(UpsamplerBlock(64, 16))\n        self.layers.append(non_bottleneck_1d(16, 0, 1))\n        self.layers.append(non_bottleneck_1d(16, 0, 1))\n\n        self.output_conv = nn.ConvTranspose2d(16, num_classes, 2, stride=2, padding=0, output_padding=0, bias=True)\n\n    def forward(self, input):\n        output = input\n\n        for layer in self.layers:\n            output = layer(output)\n\n        output = self.output_conv(output)\n\n        return output\n\n\n# ERFNet\nclass erfnet(nn.Module):\n    def __init__(self, n_classes, encoder=None, pretrained=False):  # use encoder to pass pretrained encoder\n        super(erfnet, self).__init__()\n\n        if (encoder == None):\n            self.encoder = Encoder(n_classes)\n        else:\n            self.encoder = encoder\n        self.decoder = Decoder(n_classes)\n\n    def forward(self, input, only_encode=False):\n        if only_encode:\n            return self.encoder.forward(input, predict=True)\n        else:\n            output = self.encoder(input)  # predict=False by default\n            return self.decoder.forward(output)\n\nif __name__ == '__main__':\n    n_classes = 21\n    model = erfnet(n_classes=n_classes)\n    # model.init_vgg16()\n    x = Variable(torch.randn(1, 3, 360, 480))\n    y = Variable(torch.LongTensor(np.ones((1, 360, 480), dtype=np.int)))\n    # print(x.shape)\n\n    # ---------------------------fcn32s\xe6\xa8\xa1\xe5\x9e\x8b\xe8\xbf\x90\xe8\xa1\x8c\xe6\x97\xb6\xe9\x97\xb4-----------------------\n    start = time.time()\n    pred = model(x)\n    end = time.time()\n    print(end-start)\n\n    # print(pred.shape)\n    loss = cross_entropy2d(pred, y)\n    print(loss)\n"""
semseg/modelloader/fast_segnet.py,3,"b'# -*- coding: utf-8 -*-\nimport time\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nfrom torchvision import models\n\n# class fast_segnet(nn.Module):\n#     def __init__(self):\n#         super(fast_segnet, self).__init__()\n#\n#     def forward(self, x):\n#         pass'"
semseg/modelloader/fc_densenet.py,4,"b'# -*- coding: utf-8 -*-\n# code from [fc_densenet.py](https://gist.github.com/felixgwu/045c887b6ccdf0edf4648da0c40bcc12)\n\nimport torch\nfrom torch import nn\n\n\nclass DenseBlock(nn.Module):\n\n    def __init__(self, nIn, growth_rate, depth, drop_rate=0, only_new=False,\n                 bottle_neck=False):\n        super(DenseBlock, self).__init__()\n        self.only_new = only_new\n        self.depth = depth\n        self.growth_rate = growth_rate\n        self.layers = nn.ModuleList([self.get_transform(\n            nIn + i * growth_rate, growth_rate, bottle_neck,\n            drop_rate) for i in range(depth)])\n\n    def forward(self, x):\n        if self.only_new:\n            outputs = []\n            for i in range(self.depth):\n                tx = self.layers[i](x)\n                x = torch.cat((x, tx), 1)\n                outputs.append(tx)\n            return torch.cat(outputs, 1)\n        else:\n            for i in range(self.depth):\n                x = torch.cat((x, self.layers[i](x)), 1)\n            return x\n\n    def get_transform(self, nIn, nOut, bottle_neck=None, drop_rate=0):\n        if not bottle_neck or nIn <= nOut * bottle_neck:\n            return nn.Sequential(\n                nn.BatchNorm2d(nIn),\n                nn.ReLU(True),\n                nn.Conv2d(nIn, nOut, 3, stride=1, padding=1, bias=True),\n                nn.Dropout(drop_rate),\n            )\n        else:\n            nBottle = nOut * bottle_neck\n            return nn.Sequential(\n                nn.BatchNorm2d(nIn),\n                nn.ReLU(True),\n                nn.Conv2d(nIn, nBottle, 1, stride=1, padding=0, bias=True),\n                nn.BatchNorm2d(nBottle),\n                nn.ReLU(True),\n                nn.Conv2d(nBottle, nOut, 3, stride=1, padding=1, bias=True),\n                nn.Dropout(drop_rate),\n            )\n\n\nclass FCDenseNet(nn.Module):\n\n    def __init__(self, depths, growth_rates, n_scales=5, n_channel_start=48,\n                 n_classes=12, drop_rate=0, bottle_neck=False):\n        super(FCDenseNet, self).__init__()\n        self.n_scales = n_scales\n        self.n_classes = n_classes\n        self.n_channel_start = n_channel_start\n        self.depths = [depths] * \\\n            (2 * n_scales + 1) if type(depths) == int else depths\n        self.growth_rates = [growth_rates] * (2 * n_scales + 1) if \\\n            type(growth_rates) == int else growth_rates\n        self.drop_rate = drop_rate\n        assert len(self.depths) == len(self.growth_rates) == 2 * n_scales + 1\n        self.conv_first = nn.Conv2d(\n            3, n_channel_start, 3, stride=1, padding=1, bias=True)\n        self.dense_blocks = nn.ModuleList([])\n        self.transition_downs = nn.ModuleList([])\n        self.transition_ups = nn.ModuleList([])\n\n        nskip = []\n        nIn = self.n_channel_start\n        for i in range(n_scales):\n            self.dense_blocks.append(\n                DenseBlock(nIn, self.growth_rates[i], self.depths[i],\n                           drop_rate=drop_rate, bottle_neck=bottle_neck))\n            nIn += self.growth_rates[i] * self.depths[i]\n            nskip.append(nIn)\n            self.transition_downs.append(self.get_TD(nIn, drop_rate))\n\n        self.dense_blocks.append(\n            DenseBlock(nIn, self.growth_rates[n_scales], self.depths[n_scales],\n                       only_new=True, drop_rate=drop_rate,\n                       bottle_neck=bottle_neck))\n        nIn = self.growth_rates[n_scales] * self.depths[n_scales]\n\n        for i in range(n_scales-1):\n            self.transition_ups.append(nn.ConvTranspose2d(\n                nIn, nIn, 3, stride=2, padding=1, bias=True))\n            nIn += nskip.pop()\n            self.dense_blocks.append(\n                DenseBlock(nIn, self.growth_rates[n_scales + 1 + i],\n                           self.depths[n_scales + 1 + i],\n                           only_new=True, drop_rate=drop_rate,\n                           bottle_neck=bottle_neck))\n            nIn = self.growth_rates[n_scales + 1 + i] * \\\n                self.depths[n_scales + 1 + i]\n        # last dense block\n        self.transition_ups.append(nn.ConvTranspose2d(\n            nIn, nIn, 3, stride=2, padding=1, bias=True))\n        nIn += nskip.pop()\n        self.dense_blocks.append(\n            DenseBlock(nIn, self.growth_rates[2 * n_scales],\n                       self.depths[2 * n_scales], drop_rate=drop_rate,\n                       bottle_neck=bottle_neck))\n        nIn += self.growth_rates[2 * n_scales] * \\\n            self.depths[2 * n_scales]\n        self.conv_last = nn.Conv2d(nIn, n_classes, 1, bias=True)\n        self.logsoftmax = nn.LogSoftmax()\n\n    def forward(self, x):\n        x = self.conv_first(x)\n        skip_connects = []\n        # down sample\n        for i in range(self.n_scales):\n            x = self.dense_blocks[i](x)\n            skip_connects.append(x)\n            x = self.transition_downs[i](x)\n        # bottle neck\n        x = self.dense_blocks[self.n_scales](x)\n        # up sample\n        for i in range(self.n_scales):\n            skip = skip_connects.pop()\n            TU = self.transition_ups[i]\n            # adjust padding\n            TU.padding = (((x.size(2) - 1) * TU.stride[0] - skip.size(2)\n                           + TU.kernel_size[0] + 1) // 2,\n                          ((x.size(3) - 1) * TU.stride[1] - skip.size(3)\n                              + TU.kernel_size[1] + 1) // 2)\n            x = TU(x, output_size=skip.size())\n            x = torch.cat((skip, x), 1)\n            x = self.dense_blocks[self.n_scales + 1 + i](x)\n        x = self.conv_last(x)\n        return self.logsoftmax(x)\n\n    def get_TD(self, nIn, drop_rate):\n        layers = [nn.BatchNorm2d(nIn), nn.ReLU(\n            True), nn.Conv2d(nIn, nIn, 1, bias=True)]\n        if drop_rate > 0:\n            layers.append(nn.Dropout(drop_rate))\n        layers.append(nn.MaxPool2d(2))\n        return nn.Sequential(*layers)\n\n\ndef fcdensenet_tiny(n_classes, drop_rate=0, pretrained=False):\n    return FCDenseNet(2, 6, drop_rate=drop_rate, n_classes=n_classes)\n\ndef fcdensenet56_nodrop(n_classes, pretrained=False):\n    return FCDenseNet(4, 12, drop_rate=0, n_classes=n_classes)\n\n\ndef fcdensenet56(n_classes, drop_rate=0.2, pretrained=False):\n    return FCDenseNet(4, 12, drop_rate=drop_rate, n_classes=n_classes)\n\n\ndef fcdensenet67_nodrop(n_classes, pretrained=False):\n    return FCDenseNet(5, 16, drop_rate=0, n_classes=n_classes)\n\ndef fcdensenet67(n_classes, drop_rate=0.2, pretrained=False):\n    return FCDenseNet(5, 16, drop_rate=drop_rate, n_classes=n_classes)\n\n\ndef fcdensenet103(n_classes, drop_rate=0.2, pretrained=False):\n    return FCDenseNet([4, 5, 7, 10, 12, 15, 12, 10, 7, 5, 4], 16, drop_rate=drop_rate, n_classes=n_classes)\n\n\ndef fcdensenet103_nodrop(n_classes, drop_rate=0, pretrained=False):\n    return FCDenseNet([4, 5, 7, 10, 12, 15, 12, 10, 7, 5, 4], 16, drop_rate=drop_rate, n_classes=n_classes)\n'"
semseg/modelloader/fcn.py,5,"b""# -*- coding: utf-8 -*-\nimport time\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nfrom torchvision import models\n\n# fcn32s\xe6\xa8\xa1\xe5\x9e\x8b\nfrom semseg.loss import cross_entropy2d\nfrom semseg.utils.flops_benchmark import add_flops_counting_methods\n\n\ndef fcn_32s(n_classes=21, pretrained=False):\n    model = fcn(module_type='32s', n_classes=n_classes, pretrained=pretrained)\n    return model\n\ndef fcn_16s(n_classes=21, pretrained=False):\n    model = fcn(module_type='16s', n_classes=n_classes, pretrained=pretrained)\n    return model\n\ndef fcn_8s(n_classes=21, pretrained=False):\n    model = fcn(module_type='8s', n_classes=n_classes, pretrained=pretrained)\n    return model\n\nclass fcn(nn.Module):\n    def forward(self, x):\n        conv1 = self.conv1_block(x)\n        conv2 = self.conv2_block(conv1)\n        conv3 = self.conv3_block(conv2)\n        conv4 = self.conv4_block(conv3)\n        conv5 = self.conv5_block(conv4)\n        score = self.classifier(conv5)\n\n        if self.module_type=='16s' or self.module_type=='8s':\n            score_pool4 = self.score_pool4(conv4)\n        if self.module_type=='8s':\n            score_pool3 = self.score_pool3(conv3)\n        # print(conv1.data.size())\n        # print(conv2.data.size())\n        # print(conv4.data.size())\n        # print(conv5.data.size())\n        # print(score.data.size())\n        # print(x.data.size())\n        if self.module_type=='16s' or self.module_type=='8s':\n            score = F.upsample_bilinear(score, score_pool4.size()[2:])\n            score += score_pool4\n        if self.module_type=='8s':\n            score = F.upsample_bilinear(score, score_pool3.size()[2:])\n            score += score_pool3\n\n        out = F.upsample_bilinear(score, x.size()[2:])\n        return out\n\n    def __init__(self, module_type='32s', n_classes=21, pretrained=False):\n        super(fcn, self).__init__()\n        self.n_classes = n_classes\n        self.module_type = module_type\n\n        # VGG16=2+2+3+3+3+3\n        # VGG16\xe7\xbd\x91\xe7\xbb\x9c\xe7\x9a\x84\xe7\xac\xac\xe4\xb8\x80\xe4\xb8\xaa\xe6\xa8\xa1\xe5\x9d\x97\xe6\x98\xaf\xe4\xb8\xa4\xe4\xb8\xaaout_channel=64\xe7\x9a\x84\xe5\x8d\xb7\xe7\xa7\xaf\xe5\x9d\x97\n        self.conv1_block = nn.Sequential(\n            nn.Conv2d(3, 64, 3, padding=100),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(64, 64, 3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(2, stride=2, ceil_mode=True),\n        )\n\n        # VGG16\xe7\xbd\x91\xe7\xbb\x9c\xe7\x9a\x84\xe7\xac\xac\xe4\xba\x8c\xe4\xb8\xaa\xe6\xa8\xa1\xe5\x9d\x97\xe6\x98\xaf\xe4\xb8\xa4\xe4\xb8\xaaout_channel=128\xe7\x9a\x84\xe5\x8d\xb7\xe7\xa7\xaf\xe5\x9d\x97\n        self.conv2_block = nn.Sequential(\n            nn.Conv2d(64, 128, 3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(128, 128, 3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(2, stride=2, ceil_mode=True),\n        )\n\n        # VGG16\xe7\xbd\x91\xe7\xbb\x9c\xe7\x9a\x84\xe7\xac\xac\xe4\xb8\x89\xe4\xb8\xaa\xe6\xa8\xa1\xe5\x9d\x97\xe6\x98\xaf\xe4\xb8\x89\xe4\xb8\xaaout_channel=256\xe7\x9a\x84\xe5\x8d\xb7\xe7\xa7\xaf\xe5\x9d\x97\n        self.conv3_block = nn.Sequential(\n            nn.Conv2d(128, 256, 3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(256, 256, 3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(256, 256, 3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(2, stride=2, ceil_mode=True),\n        )\n\n        # VGG16\xe7\xbd\x91\xe7\xbb\x9c\xe7\x9a\x84\xe7\xac\xac\xe5\x9b\x9b\xe4\xb8\xaa\xe6\xa8\xa1\xe5\x9d\x97\xe6\x98\xaf\xe4\xb8\x89\xe4\xb8\xaaout_channel=512\xe7\x9a\x84\xe5\x8d\xb7\xe7\xa7\xaf\xe5\x9d\x97\n        self.conv4_block = nn.Sequential(\n            nn.Conv2d(256, 512, 3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(512, 512, 3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(512, 512, 3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(2, stride=2, ceil_mode=True),\n        )\n\n        # VGG16\xe7\xbd\x91\xe7\xbb\x9c\xe7\x9a\x84\xe7\xac\xac\xe4\xba\x94\xe4\xb8\xaa\xe6\xa8\xa1\xe5\x9d\x97\xe6\x98\xaf\xe4\xb8\x89\xe4\xb8\xaaout_channel=512\xe7\x9a\x84\xe5\x8d\xb7\xe7\xa7\xaf\xe5\x9d\x97\n        self.conv5_block = nn.Sequential(\n            nn.Conv2d(512, 512, 3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(512, 512, 3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(512, 512, 3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(2, stride=2, ceil_mode=True),\n        )\n\n        self.classifier = nn.Sequential(\n            nn.Conv2d(512, 4096, 7),\n            nn.ReLU(inplace=True),\n            nn.Dropout2d(),\n            nn.Conv2d(4096, 4096, 1),\n            nn.ReLU(inplace=True),\n            nn.Dropout2d(),\n            nn.Conv2d(4096, self.n_classes, 1),\n        )\n\n        if self.module_type=='16s' or self.module_type=='8s':\n            self.score_pool4 = nn.Conv2d(512, self.n_classes, 1)\n        if self.module_type=='8s':\n            self.score_pool3 = nn.Conv2d(256, self.n_classes, 1)\n\n        if pretrained:\n            self.init_vgg16()\n\n    def init_vgg16(self):\n        vgg16 = models.vgg16(pretrained=True)\n\n        # -----------\xe8\xb5\x8b\xe5\x80\xbc\xe5\x89\x8d\xe9\x9d\xa22+2+3+3+3\xe5\xb1\x82feature\xe7\x9a\x84\xe7\x89\xb9\xe5\xbe\x81-------------\n        # \xe7\x94\xb1\xe4\xba\x8evgg16\xe7\x9a\x84\xe7\x89\xb9\xe5\xbe\x81\xe6\x98\xafSequential\xef\xbc\x8c\xe8\x8e\xb7\xe5\xbe\x97\xe5\x85\xb6\xe4\xb8\xad\xe7\x9a\x84\xe5\xad\x90\xe7\xb1\xbb\xe9\x80\x9a\xe8\xbf\x87children()\n        vgg16_features = list(vgg16.features.children())\n\n        conv_blocks = [self.conv1_block, self.conv2_block, self.conv3_block, self.conv4_block, self.conv5_block]\n        conv_ids_vgg = [[0, 4], [5, 9], [10, 16], [17, 23], [24, 30]]\n\n        for conv_block_id, conv_block in enumerate(conv_blocks):\n            # print(conv_block_id)\n            conv_id_vgg = conv_ids_vgg[conv_block_id]\n            for l1, l2 in zip(conv_block, vgg16_features[conv_id_vgg[0]:conv_id_vgg[1]]):\n                if isinstance(l1, nn.Conv2d) and isinstance(l2, nn.Conv2d):\n                    assert l1.weight.size() == l2.weight.size()\n                    assert l1.bias.size() == l2.bias.size()\n                    # \xe8\xb5\x8b\xe5\x80\xbc\xe7\x9a\x84\xe6\x98\xaf\xe6\x95\xb0\xe6\x8d\xae\n                    l1.weight.data = l2.weight.data\n                    l1.bias.data = l2.bias.data\n                    # print(l1)\n                    # print(l2)\n\n        # -----------\xe8\xb5\x8b\xe5\x80\xbc\xe5\x90\x8e\xe9\x9d\xa23\xe5\xb1\x82classifier\xe7\x9a\x84\xe7\x89\xb9\xe5\xbe\x81-------------\n        vgg16_classifier = list(vgg16.classifier.children())\n        for l1, l2 in zip(self.classifier, vgg16_classifier[0:3]):\n            if isinstance(l1, nn.Conv2d) and isinstance(l2, nn.Linear):\n                l1.weight.data = l2.weight.data.view(l1.weight.size())\n                l1.bias.data = l2.bias.data.view(l1.bias.size())\n\n        # -----\xe8\xb5\x8b\xe5\x80\xbc\xe5\x90\x8e\xe9\x9d\xa21\xe5\xb1\x82classifier\xe7\x9a\x84\xe7\x89\xb9\xe5\xbe\x81\xef\xbc\x8c\xe7\x94\xb1\xe4\xba\x8e\xe7\xb1\xbb\xe5\x88\xab\xe4\xb8\x8d\xe5\x90\x8c\xef\xbc\x8c\xe9\x9c\x80\xe8\xa6\x81\xe4\xbf\xae\xe6\x94\xb9------\n        l1 = self.classifier[6]\n        l2 = vgg16_classifier[6]\n        if isinstance(l1, nn.Conv2d) and isinstance(l2, nn.Linear):\n            l1.weight.data = l2.weight.data[:self.n_classes, :].view(l1.weight.size())\n            l1.bias.data = l2.bias.data[:self.n_classes].view(l1.bias.size())\n\nif __name__ == '__main__':\n    n_classes = 21\n    model_fcn32s = fcn(module_type='32s', n_classes=n_classes, pretrained=False)\n    model_fcn16s = fcn(module_type='16s', n_classes=n_classes, pretrained=False)\n    model_fcn8s = fcn(module_type='8s', n_classes=n_classes, pretrained=False)\n\n    # model_fcn32s = add_flops_counting_methods(model_fcn32s)\n    # model_fcn32s = model_fcn32s.train()\n    # model_fcn32s.start_flops_count()\n\n    # model.init_vgg16()\n    x = Variable(torch.randn(1, 3, 360, 480))\n    y = Variable(torch.LongTensor(np.ones((1, 360, 480), dtype=np.int)))\n    # print(x.shape)\n\n    # ---------------------------fcn32s\xe6\xa8\xa1\xe5\x9e\x8b\xe8\xbf\x90\xe8\xa1\x8c\xe6\x97\xb6\xe9\x97\xb4-----------------------\n    start = time.time()\n    pred = model_fcn32s(x)\n    end = time.time()\n    print(end-start)\n\n    # model_fcn32s_flops = model_fcn32s.compute_average_flops_cost() / 1e9 / 2\n    # print('model_fcn32s_flops:', model_fcn32s_flops)\n\n    # ---------------------------fcn16s\xe6\xa8\xa1\xe5\x9e\x8b\xe8\xbf\x90\xe8\xa1\x8c\xe6\x97\xb6\xe9\x97\xb4-----------------------\n    start = time.time()\n    pred = model_fcn16s(x)\n    end = time.time()\n    print(end-start)\n\n    # ---------------------------fcn8s\xe6\xa8\xa1\xe5\x9e\x8b\xe8\xbf\x90\xe8\xa1\x8c\xe6\x97\xb6\xe9\x97\xb4-----------------------\n    start = time.time()\n    pred = model_fcn8s(x)\n    end = time.time()\n    print(end-start)\n\n    # print(pred.shape)\n    loss = cross_entropy2d(pred, y)\n    # print(loss)\n"""
semseg/modelloader/fcn_mobilenet.py,8,"b'# -*- coding: utf-8 -*-\nimport torch.nn as nn\nimport torch.utils.model_zoo as model_zoo\nimport time\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nfrom torchvision import models\nimport os\n\nfrom semseg.loss import cross_entropy2d\n\n\nclass mobilenet_conv_bn_relu(nn.Module):\n    """"""\n    :param\n    """"""\n    def __init__(self, in_channels, out_channels, stride):\n        super(mobilenet_conv_bn_relu, self).__init__()\n        self.cbr_seq = nn.Sequential(\n            nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=3, stride=stride,\n                      padding=1, bias=False),\n            nn.BatchNorm2d(num_features=out_channels),\n            nn.ReLU(inplace=True)\n        )\n\n    def forward(self, x):\n        """"""\n        :param x:\n        :return:\n        """"""\n        x = self.cbr_seq(x)\n        return x\n\n\nclass mobilenet_conv_dw_relu(nn.Module):\n    """"""\n    :param\n    """"""\n    def __init__(self, in_channels, out_channels, stride):\n        super(mobilenet_conv_dw_relu, self).__init__()\n        self.cbr_seq = nn.Sequential(\n            nn.Conv2d(in_channels=in_channels, out_channels=in_channels, kernel_size=3, stride=stride,\n                      padding=1, groups=in_channels, bias=False),\n            nn.BatchNorm2d(num_features=in_channels),\n            nn.ReLU(inplace=True),\n\n            nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=1, stride=1,\n                      padding=0, bias=False),\n            nn.BatchNorm2d(num_features=out_channels),\n            nn.ReLU(inplace=True)\n        )\n\n    def forward(self, x):\n        """"""\n        :param x:\n        :return:\n        """"""\n        x = self.cbr_seq(x)\n        return x\n\n\ndef fcn_MobileNet_32s(n_classes=21, pretrained=False):\n    model = fcn_MobileNet(module_type=\'32s\', n_classes=n_classes, pretrained=pretrained)\n    return model\n\ndef fcn_MobileNet_16s(n_classes=21, pretrained=False):\n    model = fcn_MobileNet(module_type=\'16s\', n_classes=n_classes, pretrained=pretrained)\n    return model\n\ndef fcn_MobileNet_8s(n_classes=21, pretrained=False):\n    model = fcn_MobileNet(module_type=\'8s\', n_classes=n_classes, pretrained=pretrained)\n    return model\n\nclass fcn_MobileNet(nn.Module):\n    """"""\n    :param\n    """"""\n    def __init__(self, module_type=\'32s\', n_classes=21, pretrained=True):\n        super(fcn_MobileNet, self).__init__()\n\n        self.n_classes = n_classes\n        self.module_type = module_type\n\n        self.conv1_bn = mobilenet_conv_bn_relu(3, 32, 2)\n        self.conv2_dw = mobilenet_conv_dw_relu(32, 64, 1)\n        self.conv3_dw = mobilenet_conv_dw_relu(64, 128, 2)\n        self.conv4_dw = mobilenet_conv_dw_relu(128, 128, 1)\n        self.conv5_dw = mobilenet_conv_dw_relu(128, 256, 2)\n        self.conv6_dw = mobilenet_conv_dw_relu(256, 256, 1)\n        self.conv7_dw = mobilenet_conv_dw_relu(256, 512, 2)\n        self.conv8_dw = mobilenet_conv_dw_relu(512, 512, 1)\n        self.conv9_dw = mobilenet_conv_dw_relu(512, 512, 1)\n        self.conv10_dw = mobilenet_conv_dw_relu(512, 512, 1)\n        self.conv11_dw = mobilenet_conv_dw_relu(512, 512, 1)\n        self.conv12_dw = mobilenet_conv_dw_relu(512, 512, 1)\n        self.conv13_dw = mobilenet_conv_dw_relu(512, 1024, 2)\n        self.conv14_dw = mobilenet_conv_dw_relu(1024, 1024, 1)\n        # self.avg_pool = nn.AvgPool2d(7)\n        # self.fc = nn.Linear(1024, n_classes)\n\n        self.classifier = nn.Conv2d(1024, self.n_classes, 1)\n\n        if self.module_type==\'16s\' or self.module_type==\'8s\':\n            self.score_pool4 = nn.Conv2d(512, self.n_classes, 1)\n        if self.module_type==\'8s\':\n            self.score_pool3 = nn.Conv2d(256, self.n_classes, 1)\n\n        if pretrained:\n            self.init_weights(pretrained=pretrained)\n\n    def init_weights(self, pretrained=False):\n        model_checkpoint_path = os.path.expanduser(\'~/.torch/models/mobilenet_sgd_rmsprop_69.526.tar\')\n        if os.path.exists(model_checkpoint_path):\n            model_checkpoint = torch.load(model_checkpoint_path, map_location=\'cpu\')\n            pretrained_dict = model_checkpoint[\'state_dict\']\n\n            model_dict = self.state_dict()\n\n            # print(model_dict.keys())\n            # print(pretrained_dict.keys())\n            model_dict_keys = model_dict.keys()\n\n            new_dict = {}\n            for dict_index, (k, v) in enumerate(pretrained_dict.items()):\n                if k==\'module.fc.weight\':\n                    break\n                # print(dict_index)\n                # print(k)\n                new_k = model_dict_keys[dict_index]\n                new_v = v\n                new_dict[new_k] = new_v\n            model_dict.update(new_dict)\n            self.load_state_dict(model_dict)\n\n    def forward(self, x):\n        """"""\n        :param x:\n        :return:\n        """"""\n        x_size = x.size()[2:]\n        x_conv1 = self.conv1_bn(x)\n        x_conv2 = self.conv2_dw(x_conv1)\n        x_conv3 = self.conv3_dw(x_conv2)\n        x_conv4 = self.conv4_dw(x_conv3)\n        x_conv5 = self.conv5_dw(x_conv4)\n        x_conv6 = self.conv6_dw(x_conv5)\n        x_conv7 = self.conv7_dw(x_conv6)\n        x_conv8 = self.conv8_dw(x_conv7)\n        x_conv9 = self.conv9_dw(x_conv8)\n        x_conv10 = self.conv10_dw(x_conv9)\n        x_conv11 = self.conv11_dw(x_conv10)\n        x_conv12 = self.conv12_dw(x_conv11)\n        x_conv13 = self.conv13_dw(x_conv12)\n        x = self.conv14_dw(x_conv13)\n\n        # x = self.avg_pool(x)\n        # x = x.view(-1, 1024)\n        # x = self.fc(x)\n\n        score = self.classifier(x)\n\n        if self.module_type==\'16s\' or self.module_type==\'8s\':\n            score_pool4 = self.score_pool4(x_conv12)\n        if self.module_type==\'8s\':\n            score_pool3 = self.score_pool3(x_conv6)\n\n        if self.module_type==\'16s\' or self.module_type==\'8s\':\n            score = F.upsample_bilinear(score, score_pool4.size()[2:])\n            score += score_pool4\n        if self.module_type==\'8s\':\n            score = F.upsample_bilinear(score, score_pool3.size()[2:])\n            score += score_pool3\n\n        out = F.upsample_bilinear(score, x_size)\n\n        return out\n\n\nif __name__ == \'__main__\':\n    batch_size = 1\n    n_classes = 21\n    model_fcn32s = fcn_MobileNet(module_type=\'32s\', n_classes=n_classes, pretrained=True)\n    model_fcn16s = fcn_MobileNet(module_type=\'16s\', n_classes=n_classes, pretrained=True)\n    model_fcn8s = fcn_MobileNet(module_type=\'8s\', n_classes=n_classes, pretrained=True)\n    x = Variable(torch.randn(batch_size, 3, 360, 480))\n    y = Variable(torch.LongTensor(np.ones((batch_size, 360, 480), dtype=np.int)))\n\n    # ---------------------------fcn32s\xe6\xa8\xa1\xe5\x9e\x8b\xe8\xbf\x90\xe8\xa1\x8c\xe6\x97\xb6\xe9\x97\xb4-----------------------\n    start = time.time()\n    pred = model_fcn32s(x)\n    print(\'pred.shape:\', pred.shape)\n    end = time.time()\n    print(end-start)\n\n    # ---------------------------fcn16s\xe6\xa8\xa1\xe5\x9e\x8b\xe8\xbf\x90\xe8\xa1\x8c\xe6\x97\xb6\xe9\x97\xb4-----------------------\n    start = time.time()\n    pred = model_fcn16s(x)\n    end = time.time()\n    print(end-start)\n\n    # ---------------------------fcn8s\xe6\xa8\xa1\xe5\x9e\x8b\xe8\xbf\x90\xe8\xa1\x8c\xe6\x97\xb6\xe9\x97\xb4-----------------------\n    start = time.time()\n    pred = model_fcn8s(x)\n    end = time.time()\n    print(end-start)\n\n    # print(pred.shape)\n    loss = cross_entropy2d(pred, y)\n    # print(loss)\n'"
semseg/modelloader/fcn_resnet.py,12,"b'# -*- coding: utf-8 -*-\nimport torch.nn as nn\nimport torch.utils.model_zoo as model_zoo\nimport time\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nfrom torchvision import models\n\nfrom semseg.loss import cross_entropy2d\n\n\nmodel_urls = {\n    \'resnet18\': \'https://download.pytorch.org/models/resnet18-5c106cde.pth\',\n    \'resnet34\': \'https://download.pytorch.org/models/resnet34-333f7ec4.pth\',\n    \'resnet50\': \'https://download.pytorch.org/models/resnet50-19c8e357.pth\',\n    \'resnet101\': \'https://download.pytorch.org/models/resnet101-5d3b4d8f.pth\',\n    \'resnet152\': \'https://download.pytorch.org/models/resnet152-b121ed2d.pth\',\n}\n\n\ndef conv3x3(in_planes, out_planes, stride=1):\n    """"""3x3 convolution with padding""""""\n    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n                     padding=1, bias=False)\n\n\ndef conv1x1(in_planes, out_planes, stride=1):\n    """"""1x1 convolution""""""\n    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n\n\nclass BasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None):\n        super(BasicBlock, self).__init__()\n        self.conv1 = conv3x3(inplanes, planes, stride)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = conv3x3(planes, planes)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\n\nclass Bottleneck(nn.Module):\n    expansion = 4\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None):\n        super(Bottleneck, self).__init__()\n        self.conv1 = conv1x1(inplanes, planes)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.conv2 = conv3x3(planes, planes, stride)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.conv3 = conv1x1(planes, planes * self.expansion)\n        self.bn3 = nn.BatchNorm2d(planes * self.expansion)\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\n\nclass fcn_resnet(nn.Module):\n\n    def __init__(self, block, layers, module_type=\'32s\', n_classes=21, pretrained=False, upsample_method=\'upsample_bilinear\'):\n        """"""\n        :param block:\n        :param layers:\n        :param module_type:\n        :param n_classes:\n        :param pretrained:\n        :param upsample_method: \'upsample_bilinear\' or \'ConvTranspose2d\'\n        """"""\n        super(fcn_resnet, self).__init__()\n        self.n_classes = n_classes\n        self.module_type = module_type\n        self.upsample_method = upsample_method\n\n        self.inplanes = 64\n        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3,\n                               bias=False)\n        self.bn1 = nn.BatchNorm2d(64)\n        self.relu = nn.ReLU(inplace=True)\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n        self.layer1 = self._make_layer(block, 64, layers[0])\n        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n        # self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n        # self.fc = nn.Linear(512 * block.expansion, num_classes)\n\n        self.classifier = nn.Conv2d(512 * block.expansion, self.n_classes, 1)\n\n        if self.upsample_method == \'upsample_bilinear\':\n            pass\n        elif self.upsample_method == \'ConvTranspose2d\':\n            self.upsample_1 = nn.ConvTranspose2d(self.n_classes, self.n_classes, 3, stride=2, padding=1)\n            self.upsample_2 = nn.ConvTranspose2d(self.n_classes, self.n_classes, 3, stride=2)\n            # self.upsample_3 = nn.ConvTranspose2d(self.n_classes, self.n_classes, 3, stride=2, padding=1)\n\n        if self.module_type==\'16s\' or self.module_type==\'8s\':\n            self.score_pool4 = nn.Conv2d(256, self.n_classes, 1)\n        if self.module_type==\'8s\':\n            self.score_pool3 = nn.Conv2d(128, self.n_classes, 1)\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal(m.weight, mode=\'fan_out\', nonlinearity=\'relu\')\n            elif isinstance(m, nn.BatchNorm2d):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n\n    def _make_layer(self, block, planes, blocks, stride=1):\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                conv1x1(self.inplanes, planes * block.expansion, stride),\n                nn.BatchNorm2d(planes * block.expansion),\n            )\n\n        layers = []\n        layers.append(block(self.inplanes, planes, stride, downsample))\n        self.inplanes = planes * block.expansion\n        for _ in range(1, blocks):\n            layers.append(block(self.inplanes, planes))\n\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        x_size = x.size()[2:]\n        x_conv1 = self.conv1(x)\n        x = self.bn1(x_conv1)\n        x = self.relu(x)\n        x = self.maxpool(x)\n\n        x_layer1 = self.layer1(x)\n        x_layer2 = self.layer2(x_layer1)\n        x_layer3 = self.layer3(x_layer2)\n        x = self.layer4(x_layer3)\n\n        # x = self.avgpool(x)\n        # x = x.view(x.size(0), -1)\n        # x = self.fc(x)\n        score = self.classifier(x)\n\n        if self.module_type==\'16s\' or self.module_type==\'8s\':\n            score_pool4 = self.score_pool4(x_layer3)\n        if self.module_type==\'8s\':\n            score_pool3 = self.score_pool3(x_layer2)\n\n        if self.module_type==\'16s\' or self.module_type==\'8s\':\n            # print(\'score_pool4.size():\', score_pool4.size())\n            # print(\'score.size():\', score.size())\n            if self.upsample_method == \'upsample_bilinear\':\n                score = F.upsample_bilinear(score, score_pool4.size()[2:])\n            elif self.upsample_method == \'ConvTranspose2d\':\n                score = self.upsample_1(score)\n            score += score_pool4\n        if self.module_type==\'8s\':\n            # print(\'score_pool3.size():\', score_pool3.size())\n            # print(\'score.size():\', score.size())\n            if self.upsample_method == \'upsample_bilinear\':\n                score = F.upsample_bilinear(score, score_pool3.size()[2:])\n            elif self.upsample_method == \'ConvTranspose2d\':\n                score = self.upsample_2(score)\n            score += score_pool3\n\n        out = F.upsample_bilinear(score, x_size)\n\n        return out\n\n    def init_weight(self, model_name):\n        pretrain_model = None\n        if model_name==\'fcn_resnet18\':\n            pretrain_model = models.resnet18(pretrained=True)\n        elif model_name==\'fcn_resnet34\':\n            pretrain_model = models.resnet34(pretrained=True)\n        elif model_name==\'fcn_resnet50\':\n            pretrain_model = models.resnet50(pretrained=True)\n        elif model_name==\'fcn_resnet101\':\n            pretrain_model = models.resnet101(pretrained=True)\n        elif model_name==\'fcn_resnet152\':\n            pretrain_model = models.resnet152(pretrained=True)\n        if pretrain_model is not None:\n            self.conv1.weight.data = pretrain_model.conv1.weight.data\n            if self.conv1.bias is not None:\n                self.conv1.bias.data = pretrain_model.conv1.bias.data\n\n            initial_convs = []\n            pretrain_model_convs = []\n\n            layers = [self.layer1, self.layer2, self.layer3, self.layer3]\n            for layer in layers:\n                layer1_list = list(layer.children())\n                for layer1_list_block in layer1_list:\n                    layer1_list_block_list = list(layer1_list_block.children())\n                    # print(layer1_list_block_list)\n                    for layer1_list_item in layer1_list_block_list:\n                        if isinstance(layer1_list_item, nn.Conv2d):\n                            # print(layer1_list_item)\n                            initial_convs.append(layer1_list_item)\n\n            layers = [pretrain_model.layer1, pretrain_model.layer2, pretrain_model.layer3, pretrain_model.layer3]\n            for layer in layers:\n                layer1_list = list(layer.children())\n                for layer1_list_block in layer1_list:\n                    layer1_list_block_list = list(layer1_list_block.children())\n                    # print(layer1_list_block_list)\n                    for layer1_list_item in layer1_list_block_list:\n                        if isinstance(layer1_list_item, nn.Conv2d):\n                            # print(layer1_list_item)\n                            pretrain_model_convs.append(layer1_list_item)\n\n            for l1, l2 in zip(initial_convs, pretrain_model_convs):\n                if isinstance(l1, nn.Conv2d) and isinstance(l2, nn.Conv2d):\n                    # \xe8\xb5\x8b\xe5\x80\xbc\xe7\x9a\x84\xe6\x98\xaf\xe6\x95\xb0\xe6\x8d\xae\n                    assert l1.weight.size() == l2.weight.size()\n                    l1.weight.data = l2.weight.data\n                    if l1.bias is not None and l2.bias is not None:\n                        assert l1.bias.size() == l2.bias.size()\n                        l1.bias.data = l2.bias.data\n                    # print(l1)\n                    # print(l2)\n\n\n\ndef fcn_resnet18(module_type=\'32s\', n_classes=21, pretrained=False):\n    """"""Constructs a ResNet-18 model.\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = fcn_resnet(BasicBlock, [2, 2, 2, 2], module_type=module_type, n_classes=n_classes, pretrained=pretrained)\n    if pretrained:\n        model.init_weight(\'fcn_resnet18\')\n    return model\n\ndef fcn_resnet18_32s(n_classes=21, pretrained=False):\n    model = fcn_resnet18(module_type=\'32s\', n_classes=n_classes, pretrained=pretrained)\n    return model\n\ndef fcn_resnet18_16s(n_classes=21, pretrained=False):\n    model = fcn_resnet18(module_type=\'16s\', n_classes=n_classes, pretrained=pretrained)\n    return model\n\ndef fcn_resnet18_8s(n_classes=21, pretrained=False):\n    model = fcn_resnet18(module_type=\'8s\', n_classes=n_classes, pretrained=pretrained)\n    return model\n\ndef fcn_resnet34(module_type=\'32s\', n_classes=21, pretrained=False):\n    """"""Constructs a ResNet-34 model.\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = fcn_resnet(BasicBlock, [3, 4, 6, 3], module_type=module_type, n_classes=n_classes, pretrained=pretrained)\n    if pretrained:\n        model.init_weight(\'fcn_resnet34\')\n    return model\n\ndef fcn_resnet34_32s(n_classes=21, pretrained=False):\n    model = fcn_resnet34(module_type=\'32s\', n_classes=n_classes, pretrained=pretrained)\n    return model\n\ndef fcn_resnet34_16s(n_classes=21, pretrained=False):\n    model = fcn_resnet34(module_type=\'16s\', n_classes=n_classes, pretrained=pretrained)\n    return model\n\ndef fcn_resnet34_8s(n_classes=21, pretrained=False):\n    model = fcn_resnet34(module_type=\'8s\', n_classes=n_classes, pretrained=pretrained)\n    return model\n\ndef fcn_resnet50(module_type=\'32s\', n_classes=21, pretrained=False):\n    """"""Constructs a ResNet-50 model.\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = fcn_resnet(Bottleneck, [3, 4, 6, 3], module_type=module_type, n_classes=n_classes, pretrained=pretrained)\n    if pretrained:\n        model.init_weight(\'fcn_resnet50\')\n    return model\n\ndef fcn_resnet50_32s(n_classes=21, pretrained=False):\n    model = fcn_resnet50(module_type=\'32s\', n_classes=n_classes, pretrained=pretrained)\n    return model\n\ndef fcn_resnet50_16s(n_classes=21, pretrained=False):\n    model = fcn_resnet50(module_type=\'16s\', n_classes=n_classes, pretrained=pretrained)\n    return model\n\ndef fcn_resnet50_8s(n_classes=21, pretrained=False):\n    model = fcn_resnet50(module_type=\'8s\', n_classes=n_classes, pretrained=pretrained)\n    return model\n\ndef fcn_resnet101(module_type=\'32s\', n_classes=21, pretrained=False):\n    """"""Constructs a ResNet-101 model.\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = fcn_resnet(Bottleneck, [3, 4, 23, 3], module_type=module_type, n_classes=n_classes, pretrained=pretrained)\n    if pretrained:\n        model.init_weight(\'fcn_resnet101\')\n    return model\n\n\ndef fcn_resnet152(module_type=\'32s\', n_classes=21, pretrained=False):\n    """"""Constructs a ResNet-152 model.\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = fcn_resnet(Bottleneck, [3, 8, 36, 3], module_type=module_type, n_classes=n_classes, pretrained=pretrained)\n    if pretrained:\n        model.init_weight(\'fcn_resnet152\')\n    return model\n\n\nif __name__ == \'__main__\':\n    n_classes = 21\n    model_fcn32s = fcn_resnet18(module_type=\'32s\', n_classes=n_classes, pretrained=True)\n    model_fcn16s = fcn_resnet18(module_type=\'16s\', n_classes=n_classes, pretrained=True)\n    model_fcn8s = fcn_resnet18(module_type=\'8s\', n_classes=n_classes, pretrained=True)\n    # model.init_vgg16()\n    x = Variable(torch.randn(1, 3, 360, 480))\n    y = Variable(torch.LongTensor(np.ones((1, 360, 480), dtype=np.int)))\n    # print(x.shape)\n\n    # ---------------------------fcn32s\xe6\xa8\xa1\xe5\x9e\x8b\xe8\xbf\x90\xe8\xa1\x8c\xe6\x97\xb6\xe9\x97\xb4-----------------------\n    start = time.time()\n    pred = model_fcn32s(x)\n    print(\'pred.shape:\', pred.shape)\n    end = time.time()\n    print(end-start)\n\n    # ---------------------------fcn16s\xe6\xa8\xa1\xe5\x9e\x8b\xe8\xbf\x90\xe8\xa1\x8c\xe6\x97\xb6\xe9\x97\xb4-----------------------\n    start = time.time()\n    pred = model_fcn16s(x)\n    end = time.time()\n    print(end-start)\n\n    # ---------------------------fcn8s\xe6\xa8\xa1\xe5\x9e\x8b\xe8\xbf\x90\xe8\xa1\x8c\xe6\x97\xb6\xe9\x97\xb4-----------------------\n    start = time.time()\n    pred = model_fcn8s(x)\n    end = time.time()\n    print(end-start)\n\n    # print(pred.shape)\n    loss = cross_entropy2d(pred, y)\n    # print(loss)'"
semseg/modelloader/fcn_shufflenet.py,9,"b'# -*- coding: utf-8 -*-\n# !!!code is from [ShuffleNet-1g8-Pytorch](https://github.com/ericsun99/ShuffleNet-1g8-Pytorch)!!!\nimport os\nimport time\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nfrom collections import OrderedDict\nfrom torch.nn import init\nimport numpy as np\nfrom scipy import misc\nimport matplotlib.pyplot as plt\n\nfrom semseg.loss import cross_entropy2d\n\n\ndef conv3x3(in_channels, out_channels, stride=1, padding=1, bias=True, groups=1):\n    """"""3x3 convolution with padding\n    """"""\n    return nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=padding, bias=bias, groups=groups)\n\n\ndef conv1x1(in_channels, out_channels, groups=1):\n    """"""1x1 convolution with padding\n    - Normal pointwise convolution When groups == 1\n    - Grouped pointwise convolution when groups > 1\n    1x1\xe5\x8d\xb7\xe7\xa7\xaf\xef\xbc\x8cgroups==1\xe9\x82\xa3\xe4\xb9\x88\xe6\xad\xa3\xe5\xb8\xb8\xe5\x8d\xb7\xe7\xa7\xaf\xef\xbc\x8c\xe5\x90\xa6\xe5\x88\x99grouped\xe5\x8d\xb7\xe7\xa7\xaf\n    """"""\n    return nn.Conv2d(in_channels, out_channels, kernel_size=1, groups=groups, stride=1)\n\n\ndef channel_shuffle(x, groups):\n    batchsize, num_channels, height, width = x.data.size()\n\n    channels_per_group = num_channels // groups # \xe6\xaf\x8f\xe4\xb8\x80group\xe7\x9a\x84\xe9\x80\x9a\xe9\x81\x93\xe6\x95\xb0\n\n    # reshape\n    x = x.view(batchsize, groups, channels_per_group, height, width) # reshape\xe4\xb8\xba\xe4\xb8\x8d\xe5\x90\x8c\xe7\x9a\x84groups\n\n    # transpose\n    # - contiguous() required if transpose() is used before view().\n    #   See https://github.com/pytorch/pytorch/issues/764\n    x = torch.transpose(x, 1, 2).contiguous()\n\n    # flatten\n    x = x.view(batchsize, -1, height, width)\n\n    return x\n\n\nclass ShuffleUnit(nn.Module):\n    def __init__(self, in_channels, out_channels, groups=3, grouped_conv=True, combine=\'add\'):\n        """"""\n        :param in_channels: ShuffleUnit\xe8\xbe\x93\xe5\x85\xa5\xe9\x80\x9a\xe9\x81\x93\xe6\x95\xb0\n        :param out_channels: ShuffleUnit\xe8\xbe\x93\xe5\x87\xba\xe9\x80\x9a\xe9\x81\x93\xe6\x95\xb0\n        :param groups: ShuffleUnit\xe5\x88\x86\xe7\xbb\x84groups\n        :param grouped_conv: \xe6\x98\xaf\xe5\x90\xa6\xe5\x9c\xa8\xe7\xac\xac\xe4\xb8\x80\xe4\xb8\xaa1x1\xe5\x8d\xb7\xe7\xa7\xaf\xe4\xbd\xbf\xe7\x94\xa8groued\xe5\x8d\xb7\xe7\xa7\xaf\n        :param combine: combine\xe4\xbd\xbf\xe7\x94\xa8element wise add or concat\n        """"""\n\n        super(ShuffleUnit, self).__init__()\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        self.grouped_conv = grouped_conv\n        self.combine = combine\n        self.groups = groups\n        self.bottleneck_channels = self.out_channels // 4 # \xe8\xbe\x93\xe5\x87\xba\xe9\x80\x9a\xe9\x81\x93\xe6\x95\xb0\xe7\x9a\x841/4\xe4\xb8\xbaShuffleUnit\xe7\x9a\x84bottleneck\xe9\x80\x9a\xe9\x81\x93\xe6\x95\xb0\n\n        # define the type of ShuffleUnit\n        # \xe4\xb8\x8d\xe5\x90\x8cstirde\xe7\x9a\x84ShuffleUnit\xe4\xbd\xbf\xe7\x94\xa8\xe4\xb8\x8d\xe5\x90\x8c\xe7\x9a\x84combine with path connection\xef\xbc\x8cstride=2\xe7\x9a\x84\xe4\xbd\xbf\xe7\x94\xa8concat\xef\xbc\x8cstride=1\xe7\x9a\x84\xe4\xbd\xbf\xe7\x94\xa8element wise add\n        if self.combine == \'add\':\n            # ShuffleUnit Figure 2b\n            self.depthwise_stride = 1\n            self._combine_func = self._add\n        elif self.combine == \'concat\':\n            # ShuffleUnit Figure 2c\n            self.depthwise_stride = 2\n            self._combine_func = self._concat\n            # ensure output of concat has the same channels as\n            # original output channels.\n            self.out_channels -= self.in_channels # \xe7\xa1\xae\xe4\xbf\x9dconcat\xe8\xbe\x93\xe5\x87\xba\xe5\x92\x8c\xe5\x8e\x9f\xe5\xa7\x8b\xe8\xbe\x93\xe5\x87\xba\xe6\x9c\x89\xe7\x9b\xb8\xe5\x90\x8c\xe7\x9a\x84\xe9\x80\x9a\xe9\x81\x93\xe6\x95\xb0\xef\xbc\x8c\xe5\x9b\xa0\xe6\xad\xa4\xe5\xb0\x86out_channels-in_channels\xe4\xbd\x9c\xe4\xb8\xba\xe6\xae\x8b\xe5\xb7\xae\xe5\xbf\xab\xe7\x9a\x84\xe8\xbe\x93\xe5\x87\xba\xe5\x8d\xb3\xe5\x8f\xaf\n        else:\n            raise ValueError(""Cannot combine tensors with \\""{}\\"""" \\\n                             ""Only \\""add\\"" and \\""concat\\"" are"" \\\n                             ""supported"".format(self.combine))\n\n        # Use a 1x1 grouped or non-grouped convolution to reduce input channels\n        # to bottleneck channels, as in a ResNet bottleneck module.\n        # NOTE: Do not use group convolution for the first conv1x1 in Stage 2.\n        self.first_1x1_groups = self.groups if grouped_conv else 1 # \xe5\x9c\xa8stage2\xe7\x9a\x84\xe7\xac\xac\xe4\xb8\x80\xe4\xb8\xaaconv1x1\xe4\xb8\x8d\xe4\xbd\xbf\xe7\x94\xa8group\xe5\x8d\xb7\xe7\xa7\xaf\n\n        self.g_conv_1x1_compress = self._make_grouped_conv1x1(self.in_channels, self.bottleneck_channels, self.first_1x1_groups, batch_norm=True, relu=True)\n\n        # 3x3 depthwise convolution followed by batch normalization\n        # 3x3 deptheise convolution with BN\n        self.depthwise_conv3x3 = conv3x3(self.bottleneck_channels, self.bottleneck_channels, stride=self.depthwise_stride, groups=self.bottleneck_channels)\n        self.bn_after_depthwise = nn.BatchNorm2d(self.bottleneck_channels)\n\n        # Use 1x1 grouped convolution to expand from\n        # bottleneck_channels to out_channels\n        self.g_conv_1x1_expand = self._make_grouped_conv1x1(self.bottleneck_channels, self.out_channels, self.groups, batch_norm=True, relu=False)\n\n    @staticmethod\n    def _add(x, out):\n        # residual connection\n        # \xe6\xae\x8b\xe5\xb7\xaeadd\xe8\xbf\x9e\xe6\x8e\xa5\xef\xbc\x8c\xe7\x94\xa8\xe4\xba\x8estride=1\xe7\x9a\x84ShuffleUnit\n        return x + out\n\n    @staticmethod\n    def _concat(x, out):\n        # concatenate along channel axis\n        # concat\xe8\xbf\x9e\xe6\x8e\xa5\xef\xbc\x8c\xe7\x94\xa8\xe4\xba\x8estride=2\xe7\x9a\x84ShuffleUnit\n        return torch.cat((x, out), 1)\n\n    def _make_grouped_conv1x1(self, in_channels, out_channels, groups, batch_norm=True, relu=False):\n\n        modules = OrderedDict()\n\n        conv = conv1x1(in_channels, out_channels, groups=groups)\n        modules[\'conv1x1\'] = conv\n\n        # \xe6\x98\xaf\xe5\x90\xa6\xe5\x9c\xa81x1\xe5\x8d\xb7\xe7\xa7\xaf\xe4\xb8\xad\xe5\xa2\x9e\xe5\x8a\xa0BN\n        if batch_norm:\n            modules[\'batch_norm\'] = nn.BatchNorm2d(out_channels)\n        # \xe6\x98\xaf\xe5\x90\xa6\xe5\x9c\xa81x1\xe5\x8d\xb7\xe7\xa7\xaf\xe4\xb8\xad\xe5\xa2\x9e\xe5\x8a\xa0ReLU\n        if relu:\n            modules[\'relu\'] = nn.ReLU()\n\n        if len(modules) > 1:\n            return nn.Sequential(modules)\n        else:\n            return conv\n\n    def forward(self, x):\n        # save for combining later with output\n        residual = x\n\n        # \xe5\xa6\x82\xe6\x9e\x9c\xe6\x98\xafconcat path connection\xe9\x82\xa3\xe4\xb9\x88\xe4\xbd\xbf\xe7\x94\xa8\xe5\xb9\xb3\xe5\x9d\x87\xe6\xb1\xa0\xe5\x8c\x96\xe6\x93\x8d\xe4\xbd\x9c\xef\xbc\x8c\xe5\x90\xa6\xe5\x88\x99\xe7\x9b\xb4\xe6\x8e\xa5\xe6\x98\xafx\xe4\xbd\x9c\xe4\xb8\xba\xe6\xae\x8b\xe5\xb7\xae\n        if self.combine == \'concat\':\n            residual = F.avg_pool2d(residual, kernel_size=3, stride=2, padding=1)\n\n        # 1x1 compress to out_channels//4\xe7\x9a\x84\xe5\x8d\xb7\xe7\xa7\xaf\n        out = self.g_conv_1x1_compress(x)\n        out = channel_shuffle(out, self.groups) # channel shuffle\xe6\x93\x8d\xe4\xbd\x9c\xef\xbc\x8c\xe5\xb0\x86out\xe6\x8c\x89\xe7\x85\xa7groups shuffle\n\n        out = self.depthwise_conv3x3(out)\n        out = self.bn_after_depthwise(out)\n\n        # 1x1 expand to out_channels\xe7\x9a\x84\xe5\x8d\xb7\xe7\xa7\xaf\n        out = self.g_conv_1x1_expand(out)\n\n        # \xe6\x9c\x80\xe5\x90\x8e\xe5\x92\x8c\xe6\xae\x8b\xe5\xb7\xae\xe7\xbb\x84\xe5\x90\x88\n        out = self._combine_func(residual, out)\n        return F.relu(out)\n\n\nclass ShuffleNet(nn.Module):\n    """"""ShuffleNet implementation.\n    """"""\n\n    def __init__(self, groups=3, in_channels=3, n_classes=1000, pretrained=False):\n        """"""ShuffleNet constructor.\n        Arguments:\n            groups (int, optional): number of groups to be used in grouped\n                1x1 convolutions in each ShuffleUnit. Default is 3 for best\n                performance according to original paper.\n                \xe5\x9c\xa8ShuffeleUnit\xe4\xb8\xad\xe4\xbd\xbf\xe7\x94\xa8\xe7\x9a\x841x1\xe5\x8d\xb7\xe7\xa7\xafgroups\xe6\x95\xb0\xe9\x87\x8f\n            in_channels (int, optional): number of channels in the input tensor.\n                Default is 3 for RGB image inputs.\n            n_classes (int, optional): number of classes to predict. Default\n                is 1000 for ImageNet.\n        """"""\n        super(ShuffleNet, self).__init__()\n\n        self.groups = groups\n        self.stage_repeats = [3, 7, 3] # stage \xe9\x87\x8d\xe5\xa4\x8d\xe6\xac\xa1\xe6\x95\xb0\n        self.in_channels = in_channels\n        self.n_classes = n_classes\n        self.pretrained = pretrained\n\n        # index 0 is invalid and should never be called.\n        # only used for indexing convenience.\n        # \xe6\x80\xbb\xe6\x94\xbb\xe6\x9c\x893\xe4\xb8\xaastage\xef\xbc\x8c\xe5\x8c\x85\xe6\x8b\xac\xe7\xac\xac\xe4\xb8\x80\xe4\xb8\xaaConv1\xe5\x92\x8cMaxPool\n        if groups == 1:\n            self.stage_out_channels = [-1, 24, 144, 288, 567]\n        elif groups == 2:\n            self.stage_out_channels = [-1, 24, 200, 400, 800]\n        elif groups == 3:\n            self.stage_out_channels = [-1, 24, 240, 480, 960]\n        elif groups == 4:\n            self.stage_out_channels = [-1, 24, 272, 544, 1088]\n        elif groups == 8:\n            self.stage_out_channels = [-1, 24, 384, 768, 1536]\n        else:\n            raise ValueError(\n                """"""{} groups is not supported for\n                   1x1 Grouped Convolutions"""""".format(groups))\n\n        # Stage 1 always has 24 output channels\n        # conv1+maxpool\xe7\xbb\x84\xe6\x88\x90\xe4\xba\x86stage1,2,3\xe7\x9a\x84\xe7\xac\xac\xe4\xb8\x80\xe7\xbb\x84\xe6\x93\x8d\xe4\xbd\x9c\n        self.conv1 = conv3x3(self.in_channels, self.stage_out_channels[1], stride=2)\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n\n        # Stage 2\n        # stage2\xe6\x9e\x84\xe5\xbb\xba\n        self.stage2 = self._make_stage(2)\n        # Stage 3\n        # stage3\xe6\x9e\x84\xe5\xbb\xba\n        self.stage3 = self._make_stage(3)\n        # Stage 4\n        # stage4\xe6\x9e\x84\xe5\xbb\xba\n        self.stage4 = self._make_stage(4)\n\n        # Global pooling:\n        # Undefined as PyTorch\'s functional API can be used for on-the-fly\n        # shape inference if input size is not ImageNet\'s 224x224\n\n        # Fully-connected classification layer\n        num_inputs = self.stage_out_channels[-1]\n        self.fc = nn.Linear(num_inputs, self.n_classes)\n\n        if self.pretrained:\n            self.init_weights()\n\n    def init_weights(self):\n        model_checkpoint_path = os.path.expanduser(\'~/.torch/models/ShuffleNet_1g8_Top1_67.408_Top5_87.258.pth.tar\')\n        if os.path.exists(model_checkpoint_path):\n            pretrained_dict = torch.load(model_checkpoint_path, map_location=\'cpu\')\n            model_dict = self.state_dict()\n            # new_dict = {k: v for k, v in pretrained_dict.items() if k in model_dict.keys()}\n            new_dict = {}\n            for k, v in pretrained_dict.items():\n                new_k = k[k.find(\'.\') + 1:]\n                new_v = v\n                new_dict[new_k] = new_v\n\n            # print(model_dict.keys()[:5])\n            # print(pretrained_dict.keys()[:5])\n            # print(new_dict.keys()[:5])\n            model_dict.update(new_dict)\n            self.load_state_dict(model_dict)\n\n    def _make_stage(self, stage):\n        modules = OrderedDict()\n        stage_name = ""ShuffleUnit_Stage{}"".format(stage) # \xe5\xa2\x9e\xe5\x8a\xa0module name for convience\n\n        # First ShuffleUnit in the stage\n        # 1. non-grouped 1x1 convolution (i.e. pointwise convolution)\n        #   is used in Stage 2. Group convolutions used everywhere else.\n        grouped_conv = stage > 2 # \xe7\xac\xac\xe4\xba\x8c\xe4\xb8\xaastage\xe4\xb8\x8d\xe4\xbd\xbf\xe7\x94\xa8grouped conv\n\n        # 2. concatenation unit is always used.\n        # \xe6\x80\xbb\xe6\x98\xaf\xe4\xbd\xbf\xe7\x94\xa8convcat\xe5\x8d\x95\xe5\x85\x83\xe5\x9c\xa8\xe6\xaf\x8f\xe4\xb8\x80\xe4\xb8\xaastage\xe7\x9a\x84\xe7\xac\xac\xe4\xb8\x80\xe4\xb8\xaa\n        first_module = ShuffleUnit(\n            self.stage_out_channels[stage - 1],\n            self.stage_out_channels[stage],\n            groups=self.groups,\n            grouped_conv=grouped_conv,\n            combine=\'concat\'\n        )\n        modules[stage_name + ""_0""] = first_module\n\n        # add more ShuffleUnits depending on pre-defined number of repeats\n        for i in range(self.stage_repeats[stage - 2]):\n            name = stage_name + ""_{}"".format(i + 1)\n            module = ShuffleUnit(\n                self.stage_out_channels[stage],\n                self.stage_out_channels[stage],\n                groups=self.groups,\n                grouped_conv=True,\n                combine=\'add\'\n            )\n            modules[name] = module\n\n        return nn.Sequential(modules)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.maxpool(x)\n\n        x = self.stage2(x)\n        x = self.stage3(x)\n        x = self.stage4(x)\n\n        # global average pooling layer\n        x = F.avg_pool2d(x, x.data.size()[-2:])\n\n        # flatten for input to fully-connected layer\n        x = x.view(x.size(0), -1)\n        x = self.fc(x)\n\n        # return F.log_softmax(x, dim=1)\n        return x\n\nclass fcn_shufflenet(nn.Module):\n    def __init__(self, module_type=\'32s\', n_classes=21, pretrained=False, upsample_method=\'upsample_bilinear\'):\n        super(fcn_shufflenet, self).__init__()\n        self.n_classes = n_classes\n        self.module_type = module_type\n        self.pretrained = pretrained\n        self.upsample_method = upsample_method\n\n        self.shufflenet = ShuffleNet(groups=8, in_channels=3, n_classes=1000, pretrained=self.pretrained)\n\n        self.classifier = nn.Conv2d(self.shufflenet.stage_out_channels[4], self.n_classes, 1)\n\n        if self.upsample_method == \'upsample_bilinear\':\n            pass\n        elif self.upsample_method == \'ConvTranspose2d\':\n            self.upsample_1 = nn.ConvTranspose2d(self.n_classes, self.n_classes, 3, stride=2, padding=1)\n            self.upsample_2 = nn.ConvTranspose2d(self.n_classes, self.n_classes, 3, stride=2)\n            # self.upsample_3 = nn.ConvTranspose2d(self.n_classes, self.n_classes, 3, stride=2, padding=1)\n\n        if self.module_type==\'16s\' or self.module_type==\'8s\':\n            self.score_16s_conv = nn.Conv2d(self.shufflenet.stage_out_channels[3], self.n_classes, 1)\n        if self.module_type==\'8s\':\n            self.score_8s_conv = nn.Conv2d(self.shufflenet.stage_out_channels[2], self.n_classes, 1)\n\n\n    def forward(self, x):\n        x_size = x.size()[2:]\n        features = []\n        for name, module in self.shufflenet._modules.items()[:-1]:\n            # print(\'name:\', name)\n            if name in [\'stage3\', \'stage4\']:\n                # pass\n                # print(\'x.shape:\', x.shape)\n                features.append(x)\n            # print(\'module:\', module)\n            x = module(x)\n        score = self.classifier(x)\n        # features.append(x)\n\n        if self.module_type==\'16s\' or self.module_type==\'8s\':\n            score_16s_out = self.score_16s_conv(features[1])\n        if self.module_type==\'8s\':\n            score_8s_out = self.score_8s_conv(features[0])\n\n        if self.module_type==\'16s\' or self.module_type==\'8s\':\n            if self.upsample_method == \'upsample_bilinear\':\n                score = F.upsample_bilinear(score, score_16s_out.size()[2:])\n            elif self.upsample_method == \'ConvTranspose2d\':\n                score = self.upsample_1(score)\n            score += score_16s_out\n        if self.module_type==\'8s\':\n            if self.upsample_method == \'upsample_bilinear\':\n                score = F.upsample_bilinear(score, score_8s_out.size()[2:])\n            elif self.upsample_method == \'ConvTranspose2d\':\n                score = self.upsample_2(score)\n            score += score_8s_out\n\n        out = F.upsample_bilinear(score, x_size)\n\n        return out\n\ndef fcn_shufflenet_32s(n_classes=21, pretrained=False):\n    model = fcn_shufflenet(module_type=\'32s\', n_classes=n_classes, pretrained=pretrained)\n    return model\n\ndef fcn_shufflenet_16s(n_classes=21, pretrained=False):\n    model = fcn_shufflenet(module_type=\'16s\', n_classes=n_classes, pretrained=pretrained)\n    return model\n\ndef fcn_shufflenet_8s(n_classes=21, pretrained=False):\n    model = fcn_shufflenet(module_type=\'8s\', n_classes=n_classes, pretrained=pretrained)\n    return model\n\nif __name__ == \'__main__\':\n    n_classes = 21\n    model_fcn32s = fcn_shufflenet(module_type=\'32s\', n_classes=n_classes, pretrained=True)\n    model_fcn16s = fcn_shufflenet(module_type=\'16s\', n_classes=n_classes, pretrained=True)\n    model_fcn8s = fcn_shufflenet(module_type=\'8s\', n_classes=n_classes, pretrained=True)\n    # model.init_vgg16()\n    x = Variable(torch.randn(1, 3, 360, 480))\n    y = Variable(torch.LongTensor(np.ones((1, 360, 480), dtype=np.int)))\n    # print(x.shape)\n\n    # # ---------------------------fcn32s\xe6\xa8\xa1\xe5\x9e\x8b\xe8\xbf\x90\xe8\xa1\x8c\xe6\x97\xb6\xe9\x97\xb4-----------------------\n    start = time.time()\n    pred = model_fcn32s(x)\n    # print(\'pred.shape:\', pred.shape)\n    end = time.time()\n    print(end-start)\n\n    # ---------------------------fcn16s\xe6\xa8\xa1\xe5\x9e\x8b\xe8\xbf\x90\xe8\xa1\x8c\xe6\x97\xb6\xe9\x97\xb4-----------------------\n    start = time.time()\n    pred = model_fcn16s(x)\n    end = time.time()\n    # print(\'pred.shape:\', pred.shape)\n    print(end-start)\n\n    # ---------------------------fcn8s\xe6\xa8\xa1\xe5\x9e\x8b\xe8\xbf\x90\xe8\xa1\x8c\xe6\x97\xb6\xe9\x97\xb4-----------------------\n    start = time.time()\n    pred = model_fcn8s(x)\n    # print(\'pred.shape:\', pred.shape)\n    end = time.time()\n    print(end-start)\n\n    # print(pred.shape)\n    loss = cross_entropy2d(pred, y)\n    # print(loss)\n\n\n'"
semseg/modelloader/frrn.py,9,"b'# -*- coding: utf-8 -*-\n# !!!code is from [frrn.py](https://github.com/meetshah1995/pytorch-semseg/blob/master/ptsemseg/models/frrn.py)!!!\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport functools\nfrom torch.autograd import Variable\nimport time\nimport numpy as np\n\nfrom semseg.modelloader.utils import conv2DGroupNormRelu, conv2DBatchNormRelu, conv2DGroupNorm, conv2DBatchNorm\n\nfrrn_specs_dic = {\n    ""A"": {\n        ""encoder"": [[3, 96, 2], [4, 192, 4], [2, 384, 8], [2, 384, 16]],\n        ""decoder"": [[2, 192, 8], [2, 192, 4], [2, 48, 2]],\n    },\n    ""B"": {\n        ""encoder"": [[3, 96, 2], [4, 192, 4], [2, 384, 8], [2, 384, 16], [2, 384, 32]],\n        ""decoder"": [[2, 192, 16], [2, 192, 8], [2, 192, 4], [2, 48, 2]],\n    },\n}\n\n\nclass RU(nn.Module):\n    """"""\n    Residual Unit for FRRN\n    """"""\n\n    def __init__(self, channels, kernel_size=3, strides=1, group_norm=False, n_groups=None):\n        super(RU, self).__init__()\n        self.group_norm = group_norm\n        self.n_groups = n_groups\n\n        if self.group_norm:\n            self.conv1 = conv2DGroupNormRelu(channels, channels, kernel_size=kernel_size, stride=strides, padding=1, bias=False,n_groups=self.n_groups)\n            self.conv2 = conv2DGroupNorm(channels, channels, kernel_size=kernel_size, stride=strides, padding=1, bias=False,n_groups=self.n_groups)\n\n        else:\n            self.conv1 = conv2DBatchNormRelu(channels, channels, kernel_size=kernel_size, stride=strides, padding=1, bias=False,)\n            self.conv2 = conv2DBatchNorm(channels, channels, kernel_size=kernel_size, stride=strides, padding=1, bias=False,)\n\n    def forward(self, x):\n        incoming = x\n        x = self.conv1(x)\n        x = self.conv2(x)\n        return x + incoming\n\nclass FRRU(nn.Module):\n    """"""\n    Full Resolution Residual Unit for FRRN\n    """"""\n\n    def __init__(self, prev_channels, out_channels, scale, group_norm=False, n_groups=None):\n        super(FRRU, self).__init__()\n        self.scale = scale\n        self.prev_channels = prev_channels\n        self.out_channels = out_channels\n        self.group_norm = group_norm\n        self.n_groups = n_groups\n\n\n        if self.group_norm:\n            conv_unit = conv2DGroupNormRelu\n            self.conv1 = conv_unit(prev_channels + 32, out_channels, kernel_size=3, stride=1, padding=1, bias=False, n_groups=self.n_groups)\n            self.conv2 = conv_unit(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False, n_groups=self.n_groups)\n\n        else:\n            conv_unit = conv2DBatchNormRelu\n            self.conv1 = conv_unit(prev_channels + 32, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n            self.conv2 = conv_unit(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n\n        self.conv_res = nn.Conv2d(out_channels, 32, kernel_size=1, stride=1, padding=0)\n\n    def forward(self, y, z):\n        x = torch.cat([y, nn.MaxPool2d(self.scale, self.scale)(z)], dim=1)\n        y_prime = self.conv1(x)\n        y_prime = self.conv2(y_prime)\n\n        x = self.conv_res(y_prime)\n        upsample_size = torch.Size([_s * self.scale for _s in y_prime.shape[-2:]])\n        x = F.upsample(x, size=upsample_size, mode=""nearest"")\n        # print(\'z.shape:\', z.shape)\n        # print(\'x.shape:\', x.shape)\n        z_prime = z + x\n\n        return y_prime, z_prime\n\n\nclass frrn(nn.Module):\n    """"""\n    Full Resolution Residual Networks for Semantic Segmentation\n    URL: https://arxiv.org/abs/1611.08323\n    References:\n    1) Original Author\'s code: https://github.com/TobyPDE/FRRN\n    2) TF implementation by @kiwonjoon: https://github.com/hiwonjoon/tf-frrn\n    """"""\n\n    def __init__(self,\n                 n_classes=21,\n                 model_type=None,\n                 group_norm=False,\n                 n_groups=16):\n        super(frrn, self).__init__()\n        self.n_classes = n_classes\n        self.model_type = model_type\n        self.group_norm = group_norm\n        self.n_groups = n_groups\n\n        if self.group_norm:\n            self.conv1 = conv2DGroupNormRelu(3, 48, 5, 1, 2)\n        else:\n            self.conv1 = conv2DBatchNormRelu(3, 48, 5, 1, 2)\n\n        self.up_residual_units = []\n        self.down_residual_units = []\n        for i in range(3):\n            self.up_residual_units.append(RU(channels=48,\n                                             kernel_size=3,\n                                             strides=1,\n                                             group_norm=self.group_norm,\n                                             n_groups=self.n_groups))\n            self.down_residual_units.append(RU(channels=48,\n                                               kernel_size=3,\n                                               strides=1,\n                                               group_norm=self.group_norm,\n                                               n_groups=self.n_groups))\n\n        self.up_residual_units = nn.ModuleList(self.up_residual_units)\n        self.down_residual_units = nn.ModuleList(self.down_residual_units)\n\n        self.split_conv = nn.Conv2d(\n            48, 32, kernel_size=1, padding=0, stride=1, bias=False\n        )\n\n        # each spec is as (n_blocks, channels, scale)\n        self.encoder_frru_specs = frrn_specs_dic[self.model_type][""encoder""]\n\n        self.decoder_frru_specs = frrn_specs_dic[self.model_type][""decoder""]\n\n        # encoding\n        prev_channels = 48\n        self.encoding_frrus = {}\n        for n_blocks, channels, scale in self.encoder_frru_specs:\n            for block in range(n_blocks):\n                key = ""_"".join(map(str, [""encoding_frru"", n_blocks, channels, scale, block]))\n                setattr(self, key, FRRU(prev_channels=prev_channels,\n                                        out_channels=channels,\n                                        scale=scale,\n                                        group_norm=self.group_norm,\n                                        n_groups=self.n_groups),)\n            prev_channels = channels\n\n        # decoding\n        self.decoding_frrus = {}\n        for n_blocks, channels, scale in self.decoder_frru_specs:\n            # pass through decoding FRRUs\n            for block in range(n_blocks):\n                key = ""_"".join(map(str, [""decoding_frru"", n_blocks, channels, scale, block]))\n                setattr(self, key, FRRU(prev_channels=prev_channels,\n                                        out_channels=channels,\n                                        scale=scale,\n                                        group_norm=self.group_norm,\n                                        n_groups=self.n_groups),)\n            prev_channels = channels\n\n        self.merge_conv = nn.Conv2d(\n            prev_channels + 32, 48, kernel_size=1, padding=0, stride=1, bias=False\n        )\n\n        self.classif_conv = nn.Conv2d(\n            48, self.n_classes, kernel_size=1, padding=0, stride=1, bias=True\n        )\n\n    def forward(self, x):\n\n        # pass to initial conv\n        x = self.conv1(x)\n\n        # pass through residual units\n        for i in range(3):\n            x = self.up_residual_units[i](x)\n\n        # divide stream\n        y = x\n        z = self.split_conv(x)\n\n        prev_channels = 48\n        # encoding\n        for n_blocks, channels, scale in self.encoder_frru_specs:\n            # maxpool bigger feature map\n            y_pooled = F.max_pool2d(y, stride=2, kernel_size=2, padding=0)\n            # pass through encoding FRRUs\n            for block in range(n_blocks):\n                key = ""_"".join(\n                    map(str, [""encoding_frru"", n_blocks, channels, scale, block])\n                )\n                y, z = getattr(self, key)(y_pooled, z)\n            prev_channels = channels\n\n        # decoding\n        for n_blocks, channels, scale in self.decoder_frru_specs:\n            # bilinear upsample smaller feature map\n            upsample_size = torch.Size([_s * 2 for _s in y.size()[-2:]])\n            y_upsampled = F.upsample(y, size=upsample_size, mode=""bilinear"", align_corners=True)\n            # pass through decoding FRRUs\n            for block in range(n_blocks):\n                key = ""_"".join(\n                    map(str, [""decoding_frru"", n_blocks, channels, scale, block])\n                )\n                # print(""Incoming FRRU Size: "", key, y_upsampled.shape, z.shape)\n                y, z = getattr(self, key)(y_upsampled, z)\n                # print(""Outgoing FRRU Size: "", key, y.shape, z.shape)\n            prev_channels = channels\n\n        # merge streams\n        x = torch.cat([F.upsample(y, scale_factor=2, mode=""bilinear"", align_corners=True), z], dim=1)\n        x = self.merge_conv(x)\n\n        # pass through residual units\n        for i in range(3):\n            x = self.down_residual_units[i](x)\n\n        # final 1x1 conv to get classification\n        x = self.classif_conv(x)\n\n        return x\n\ndef frrn_A(n_classes=21, pretrained=True):\n    model = frrn(n_classes=n_classes, model_type=\'A\')\n    return model\n\ndef frrn_B(n_classes=21, pretrained=True):\n    model = frrn(n_classes=n_classes, model_type=\'B\')\n    return model\n\nif __name__ == \'__main__\':\n    batch_size = 1\n    n_classes = 21\n    img_height, img_width = 360, 480\n    img_height, img_width = 480, 480\n    # img_height, img_width = 1024, 512\n    model = frrn_A(n_classes=n_classes, pretrained=True)\n    x = Variable(torch.randn(batch_size, 3, img_height, img_width))\n    y = Variable(torch.LongTensor(np.ones((batch_size, img_height, img_width), dtype=np.int)))\n    # print(x.shape)\n    start = time.time()\n    pred = model(x)\n    end = time.time()\n    print(end-start)\n    # print(pred.shape)\n    # print(\'pred.type:\', pred.type)\n    # loss = cross_entropy2d(pred, y)\n    # print(loss)\n'"
semseg/modelloader/gcn.py,4,"b""# -*- coding: utf-8 -*-\n# \xe4\xbb\xa3\xe7\xa0\x81!!\xe9\x83\xa8\xe5\x88\x86\xe5\x8f\x82\xe8\x80\x83!![gcn.py](https://github.com/ycszen/pytorch-segmentation/blob/master/gcn.py)\nimport torch\nfrom torch import nn\nfrom torchvision.models import vgg, resnet50, resnet18, resnet34, resnet101\nfrom torch.autograd import Variable\nimport torch.nn.functional as F\nimport time\nimport numpy as np\n\nfrom semseg.loss import cross_entropy2d\n\n\nclass gcn(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size=7):\n        super(gcn, self).__init__()\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        self.kernel_size = kernel_size\n        self.conv1_1 = nn.Conv2d(in_channels=self.in_channels, out_channels=self.out_channels, kernel_size=(self.kernel_size, 1), padding=(self.kernel_size//2, 0))\n        self.conv1_2 = nn.Conv2d(in_channels=self.out_channels, out_channels=self.out_channels, kernel_size=(1, self.kernel_size), padding=(0, self.kernel_size//2))\n\n        self.conv2_1 = nn.Conv2d(in_channels=self.in_channels, out_channels=self.out_channels, kernel_size=(1, self.kernel_size), padding=(0, self.kernel_size//2))\n        self.conv2_2 = nn.Conv2d(in_channels=self.out_channels, out_channels=self.out_channels, kernel_size=(self.kernel_size, 1), padding=(self.kernel_size//2, 0))\n\n    def forward(self, x):\n        x_conv1_1 = self.conv1_1(x)\n        x_conv1_2 = self.conv1_2(x_conv1_1)\n\n        x_conv2_1 = self.conv2_1(x)\n        x_conv2_2 = self.conv2_2(x_conv2_1)\n\n        x_out = x_conv1_2 + x_conv2_2\n        return x_out\n\n\nclass boundary_refine(nn.Module):\n    def __init__(self, in_channels):\n        super(boundary_refine, self).__init__()\n        self.in_channels = in_channels\n\n        self.bn = nn.BatchNorm2d(self.in_channels)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv1 = nn.Conv2d(in_channels=self.in_channels, out_channels=self.in_channels, kernel_size=3, padding=1)\n        self.conv2 = nn.Conv2d(in_channels=self.in_channels, out_channels=self.in_channels, kernel_size=3, padding=1)\n\n    def forward(self, x):\n        residual = x\n        x = self.bn(x)\n        x = self.relu(x)\n        x = self.conv1(x)\n        x = self.conv2(x)\n\n        x_out = residual + x\n        return x_out\n\ndef gcn_resnet18(n_classes=21, pretrained=False):\n    return gcn_resnet(n_classes=n_classes, pretrained=pretrained, expansion=1, model='resnet18')\n\ndef gcn_resnet34(n_classes=21, pretrained=False):\n    return gcn_resnet(n_classes=n_classes, pretrained=pretrained, expansion=1, model='resnet34')\n\ndef gcn_resnet50(n_classes=21, pretrained=False):\n    return gcn_resnet(n_classes=n_classes, pretrained=pretrained, expansion=4, model='resnet50')\n\ndef gcn_resnet101(n_classes=21, pretrained=False):\n    return gcn_resnet(n_classes=n_classes, pretrained=pretrained, expansion=4, model='resnet101')\n\nclass gcn_resnet(nn.Module):\n    def __init__(self, n_classes=21, pretrained=False, expansion=4, model='resnet18'):\n        super(gcn_resnet, self).__init__()\n        self.n_classes = n_classes\n        self.pretrained = pretrained\n        self.expansion = expansion\n        backbone = eval(model)(self.pretrained)\n\n        self.conv1= backbone.conv1\n        self.bn1 = backbone.bn1\n        self.relu = backbone.relu\n        self.maxpool = backbone.maxpool\n        self.layer1 = backbone.layer1\n        self.layer2 = backbone.layer2\n        self.layer3 = backbone.layer3\n        self.layer4 = backbone.layer4\n\n        # ----global convolution network----\n        self.gcn1 = gcn(in_channels=64*self.expansion, out_channels=self.n_classes)\n        self.gcn2 = gcn(in_channels=128*self.expansion, out_channels=self.n_classes)\n        self.gcn3 = gcn(in_channels=256*self.expansion, out_channels=self.n_classes)\n        self.gcn4 = gcn(in_channels=512*self.expansion, out_channels=self.n_classes)\n        # ----global convolution network----\n\n\n        # ----boundary refinement----\n        self.br1_1 = boundary_refine(in_channels=self.n_classes)\n        self.br1_2 = boundary_refine(in_channels=self.n_classes)\n        self.br1_3 = boundary_refine(in_channels=self.n_classes)\n        self.br1_4 = boundary_refine(in_channels=self.n_classes)\n\n        self.br2_1 = boundary_refine(in_channels=self.n_classes)\n        self.br2_2 = boundary_refine(in_channels=self.n_classes)\n        self.br2_3 = boundary_refine(in_channels=self.n_classes)\n\n        self.br3_1 = boundary_refine(in_channels=self.n_classes)\n        self.br3_2 = boundary_refine(in_channels=self.n_classes)\n        # ----boundary refinement----\n\n\n    def forward(self, x):\n        # ----normal forward----\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n\n        x_1 = self.layer1(x)\n        x_2 = self.layer2(x_1)\n        x_3 = self.layer3(x_2)\n        x_4 = self.layer4(x_3)\n        # ----normal forward----\n\n        x_4 = self.gcn4(x_4)\n        x_4 = self.br1_4(x_4)\n        x_4_up = F.upsample_bilinear(x_4, x_3.size()[2:])\n\n        x_3 = self.gcn3(x_3)\n        x_3 = self.br1_3(x_3)\n        x_3_skip = x_3 + x_4_up\n        x_3_skip = self.br2_3(x_3_skip)\n        x_3_up = F.upsample_bilinear(x_3_skip, x_2.size()[2:])\n\n        x_2 = self.gcn2(x_2)\n        x_2 = self.br1_2(x_2)\n        x_2_skip = x_2 + x_3_up\n        x_2_skip = self.br2_2(x_2_skip)\n        x_2_up = F.upsample_bilinear(x_2_skip, x_1.size()[2:])\n\n        x_1 = self.gcn1(x_1)\n        x_1 = self.br1_1(x_1)\n        x_1_skip = x_1 + x_2_up\n        x_1_skip = self.br2_1(x_1_skip)\n        x_1_up = F.upsample_bilinear(x_1_skip, scale_factor=2)\n\n        x_out = self.br3_1(x_1_up)\n        x_out = F.upsample_bilinear(x_out, scale_factor=2)\n        x_out = self.br3_2(x_out)\n\n        return x_out\n\nif __name__ == '__main__':\n    batch_size = 1\n    n_classes = 21\n    img_height, img_width = 360, 480\n    # img_height, img_width = 1024, 512\n    model = gcn_resnet18(n_classes=n_classes, pretrained=False)\n    x = Variable(torch.randn(batch_size, 3, img_height, img_width))\n    y = Variable(torch.LongTensor(np.ones((batch_size, img_height, img_width), dtype=np.int)))\n    # print(x.shape)\n    start = time.time()\n    pred = model(x)\n    end = time.time()\n    print(end-start)\n    print(pred.shape)\n    # print('pred.type:', pred.type)\n    # loss = cross_entropy2d(pred, y)\n    # print(loss)\n"""
semseg/modelloader/lrn.py,5,"b""# -*- coding: utf-8 -*-\nimport torch\nfrom torch import nn\nfrom torchvision.models import vgg\nfrom torch.autograd import Variable\nimport torch.nn.functional as F\nimport time\nimport numpy as np\n\nfrom semseg.loss import cross_entropy2d\n\ndef conv3x3_bn_relu(in_planes, out_planes, stride=1, padding=1, dilation=1):\n    return nn.Sequential(\n        nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=padding, bias=False, dilation=dilation),\n        nn.BatchNorm2d(out_planes),\n        nn.ReLU(inplace=True),\n    )\n\nclass LRNRefineUnit(nn.Module):\n    def __init__(self, R_channel, M_channel):\n        super(LRNRefineUnit, self).__init__()\n        self.R_channel = R_channel\n        self.M_channel = M_channel\n        self.conv_M2m = conv3x3_bn_relu(in_planes=self.M_channel, out_planes=self.M_channel, stride=2)\n        self.conv_R2r = nn.Conv2d(in_channels=self.R_channel+self.M_channel, out_channels=self.R_channel, kernel_size=3, padding=1)\n\n    def forward(self, Rf, Mf):\n        Mf_size = Mf.size()\n        # print('Rf.shape:', Rf.shape)\n        # print('Mf.shape:', Mf.shape)\n        mf = self.conv_M2m(Mf)\n        # print('mf.shape:', mf.shape)\n        # print('Rf.shape:', Rf.shape)\n        rf = torch.cat((mf[:, :, :Rf.shape[2], :Rf.shape[3]], Rf), 1)\n        rf = self.conv_R2r(rf)\n        out = F.upsample_bilinear(rf, Mf_size[2:])\n        return out\n\n\nclass lrn_vgg16(nn.Module):\n    def __init__(self, n_classes=21, pretrained=False):\n        super(lrn_vgg16, self).__init__()\n        self.n_classes = n_classes\n        vgg16 = vgg.vgg16(pretrained=pretrained)\n        self.encoder = vgg16.features\n        self.out_conv = nn.Conv2d(in_channels=512, out_channels=self.n_classes, kernel_size=1)\n\n        # ----decoder refine units----\n        self.refine_units = []\n\n        self.refine_1 = LRNRefineUnit(self.n_classes, 512)\n        self.refine_units.append(self.refine_1)\n\n        self.refine_2 = LRNRefineUnit(self.n_classes, 512)\n        self.refine_units.append(self.refine_2)\n\n        self.refine_3 = LRNRefineUnit(self.n_classes, 256)\n        self.refine_units.append(self.refine_3)\n\n        self.refine_4 = LRNRefineUnit(self.n_classes, 128)\n        self.refine_units.append(self.refine_4)\n\n        self.refine_5 = LRNRefineUnit(self.n_classes, 64)\n        self.refine_units.append(self.refine_5)\n        # ----decoder refine units----\n\n    def forward(self, x):\n        encoder_features = []\n        for name, module in self.encoder._modules.items():\n            # print('name:', name)\n            # print('module:', module)\n            x = module(x)\n            if name in ['3', '8', '15', '22', '29']:\n                # print('x.shape:', x.shape)\n                encoder_features.append(x)\n        x = self.out_conv(x)\n        # print('x.shape:', x.shape)\n        out_s = []\n        out_s.append(x)\n        encoder_features.reverse()\n        for refine_id, encoder_feature in enumerate(encoder_features):\n            x = self.refine_units[refine_id](x, encoder_feature)\n            out_s.append(x)\n            # print('x.shape:', x.shape)\n            # break\n        return out_s\n        # return out_s[-1]\n\nif __name__ == '__main__':\n    batch_size = 2\n    n_classes = 21\n    img_height, img_width = 360, 480\n    # img_height, img_width = 1024, 512\n    model = lrn_vgg16(n_classes=n_classes, pretrained=True)\n    x = Variable(torch.randn(batch_size, 3, img_height, img_width))\n    y = Variable(torch.LongTensor(np.ones((batch_size, img_height, img_width), dtype=np.int)))\n    # print(x.shape)\n    start = time.time()\n    pred = model(x)\n    end = time.time()\n    print(end-start)\n    print(len(pred))\n    print(pred[0].shape)\n    # print(pred.shape)\n    # print('pred.type:', pred.type)\n    # loss = cross_entropy2d(pred, y)\n    # print(loss)\n"""
semseg/modelloader/pspnet.py,18,"b'#!/usr/bin/python\n# -*- coding: UTF-8 -*-\nimport time\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nfrom torchvision import models\n\nfrom semseg.loss import cross_entropy2d\nfrom semseg.modelloader.utils import conv2DBatchNormRelu\nfrom semseg.modelloader.utils import residualBlockPSP\nfrom semseg.modelloader.utils import pyramidPooling\nfrom semseg import caffe_pb2\n\n\nclass pspnet(nn.Module):\n\n    def __init__(self, n_classes=21, block_config=[3, 4, 23, 3]):\n        super(pspnet, self).__init__()\n        \n        self.block_config = block_config\n        self.n_classes = n_classes\n\n        # Encoder\n        self.convbnrelu1_1 = conv2DBatchNormRelu(in_channels=3, kernel_size=3, out_channels=64,\n                                                 padding=1, stride=2, bias=False)\n        self.convbnrelu1_2 = conv2DBatchNormRelu(in_channels=64, kernel_size=3, out_channels=64,\n                                                 padding=1, stride=1, bias=False)\n        self.convbnrelu1_3 = conv2DBatchNormRelu(in_channels=64, kernel_size=3, out_channels=128,\n                                                 padding=1, stride=1, bias=False)\n\n        # Vanilla Residual Blocks\n        self.res_block2 = residualBlockPSP(self.block_config[0], 128, 64, 256, 1, 1)\n        self.res_block3 = residualBlockPSP(self.block_config[1], 256, 128, 512, 2, 1)\n        \n        # Dilated Residual Blocks\n        self.res_block4 = residualBlockPSP(self.block_config[2], 512, 256, 1024, 1, 2)\n        self.res_block5 = residualBlockPSP(self.block_config[3], 1024, 512, 2048, 1, 4)\n        self.pyramid_pooling = pyramidPooling(2048, [6, 3, 2, 1])\n        self.cbr_final = conv2DBatchNormRelu(4096, 512, 3, 1, 1, False)\n        self.classification = nn.Conv2d(512, n_classes, 1, 1, 0)\n\n    def forward(self, x):\n        inp_shape = x.size()[2:]\n\n        # H, W -> H/2, W/2\n        x = self.convbnrelu1_3(self.convbnrelu1_2(self.convbnrelu1_1(x)))\n        # H/2, W/2 -> H/4, W/4\n        x = F.max_pool2d(x, 3, 2, 1)\n        # H/4, W/4 -> H/8, W/8\n        x = self.res_block5(self.res_block4(self.res_block3(self.res_block2(x))))\n        x = self.pyramid_pooling(x)\n        x = F.dropout2d(self.cbr_final(x), p=0.1, inplace=True)\n        x = self.classification(x)\n        x = F.upsample(x, size=inp_shape, mode=\'bilinear\')\n        return x\n\n    def load_pretrained_model(self, model_path):\n        """"""\n        Load weights from caffemodel w/o caffe dependency\n        and plug them in corresponding modules\n        """"""\n        # My eyes and my heart both hurt when writing this method\n\n        # Only care about layer_types that have trainable parameters\n        ltypes = [\'BNData\', \'ConvolutionData\', \'HoleConvolutionData\']\n\n        def _get_layer_params(layer, ltype):\n\n            if ltype == \'BNData\':\n                n_channels = layer.blobs[0].shape.dim[1]\n                \n                gamma = np.array([w for w in layer.blobs[0].data]).reshape(n_channels)\n                beta = np.array([w for w in layer.blobs[1].data]).reshape(n_channels)\n                mean = np.array([w for w in layer.blobs[2].data]).reshape(n_channels)\n                var =  np.array([w for w in layer.blobs[3].data]).reshape(n_channels)\n                return [mean, var, gamma, beta]\n\n            elif ltype in [\'ConvolutionData\', \'HoleConvolutionData\']:\n                is_bias = layer.convolution_param.bias_term\n                shape = [int(d) for d in layer.blobs[0].shape.dim]\n                weights = np.array([w for w in layer.blobs[0].data]).reshape(shape)\n                bias = []\n                if is_bias:\n                    bias = np.array([w for w in layer.blobs[1].data]).reshape(shape[0])\n                return [weights, bias]\n            \n            elif ltype == \'InnerProduct\':\n                raise Exception(""Fully connected layers {}, not supported"".format(ltype))\n\n            else:\n                raise Exception(""Unkown layer type {}"".format(ltype))\n\n\n        net = caffe_pb2.NetParameter()\n        with open(model_path, \'rb\') as model_file:\n            net.MergeFromString(model_file.read())\n\n        # dict formatted as ->  key:<layer_name> :: value:<layer_type>\n        layer_types = {}\n        # dict formatted as ->  key:<layer_name> :: value:[<list_of_params>]\n        layer_params = {}\n\n        for l in net.layer:\n            lname = l.name\n            ltype = l.type\n            if ltype in ltypes:\n                print(""Processing layer {}"".format(lname))\n                layer_types[lname] = ltype\n                layer_params[lname] = _get_layer_params(l, ltype)\n\n        # Set affine=False for all batchnorm modules\n        def _no_affine_bn(module=None):\n            if isinstance(module, nn.BatchNorm2d):\n                module.affine = False\n\n            if len([m for m in module.children()]) > 0:\n                for child in module.children():\n                    _no_affine_bn(child)\n\n        #_no_affine_bn(self)\n\n\n        def _transfer_conv(layer_name, module):\n            weights, bias = layer_params[layer_name]\n            w_shape = np.array(module.weight.size())\n            \n            np.testing.assert_array_equal(weights.shape, w_shape)\n            print(""CONV: Original {} and trans weights {}"".format(w_shape,\n                                                                  weights.shape))\n            module.weight.data = torch.from_numpy(weights)\n\n            if len(bias) != 0:\n                b_shape = np.array(module.bias.size())\n                np.testing.assert_array_equal(bias.shape, b_shape)\n                print(""CONV: Original {} and trans bias {}"".format(b_shape,\n                                                                   bias.shape))\n                module.bias.data = torch.from_numpy(bias)\n\n\n        def _transfer_conv_bn(conv_layer_name, mother_module):\n            conv_module = mother_module[0]\n            bn_module = mother_module[1]\n            \n            _transfer_conv(conv_layer_name, conv_module)\n            \n            mean, var, gamma, beta = layer_params[conv_layer_name+\'/bn\']\n            print(""BN: Original {} and trans weights {}"".format(bn_module.running_mean.size(),\n                                                                mean.shape))\n            bn_module.running_mean = torch.from_numpy(mean)\n            bn_module.running_var = torch.from_numpy(var)\n            bn_module.weight.data = torch.from_numpy(gamma)\n            bn_module.bias.data = torch.from_numpy(beta)\n\n\n        def _transfer_residual(prefix, block):\n            block_module, n_layers = block[0], block[1]\n\n            bottleneck = block_module.layers[0]\n            bottleneck_conv_bn_dic = {prefix + \'_1_1x1_reduce\': bottleneck.cbr1.cbr_unit,\n                                      prefix + \'_1_3x3\': bottleneck.cbr2.cbr_unit,\n                                      prefix + \'_1_1x1_proj\': bottleneck.cb4.cb_unit,\n                                      prefix + \'_1_1x1_increase\': bottleneck.cb3.cb_unit,}\n\n            for k, v in bottleneck_conv_bn_dic.items():\n                _transfer_conv_bn(k, v)\n\n            for layer_idx in range(2, n_layers+1):\n                residual_layer = block_module.layers[layer_idx-1]\n                residual_conv_bn_dic = {\'_\'.join(map(str, [prefix, layer_idx, \'1x1_reduce\'])): residual_layer.cbr1.cbr_unit,\n                                        \'_\'.join(map(str, [prefix, layer_idx, \'3x3\'])):  residual_layer.cbr2.cbr_unit,\n                                        \'_\'.join(map(str, [prefix, layer_idx, \'1x1_increase\'])): residual_layer.cb3.cb_unit,} \n                \n                for k, v in residual_conv_bn_dic.items():\n                    _transfer_conv_bn(k, v)\n\n\n        convbn_layer_mapping = {\'conv1_1_3x3_s2\': self.convbnrelu1_1.cbr_unit,\n                                \'conv1_2_3x3\': self.convbnrelu1_2.cbr_unit,\n                                \'conv1_3_3x3\': self.convbnrelu1_3.cbr_unit,\n                                \'conv5_3_pool6_conv\': self.pyramid_pooling.paths[0].cbr_unit, \n                                \'conv5_3_pool3_conv\': self.pyramid_pooling.paths[1].cbr_unit,\n                                \'conv5_3_pool2_conv\': self.pyramid_pooling.paths[2].cbr_unit,\n                                \'conv5_3_pool1_conv\': self.pyramid_pooling.paths[3].cbr_unit,\n                                \'conv5_4\': self.cbr_final.cbr_unit,}\n\n        residual_layers = {\'conv2\': [self.res_block2, self.block_config[0]],\n                           \'conv3\': [self.res_block3, self.block_config[1]],\n                           \'conv4\': [self.res_block4, self.block_config[2]],\n                           \'conv5\': [self.res_block5, self.block_config[3]],}\n\n        # Transfer weights for all non-residual conv+bn layers\n        for k, v in convbn_layer_mapping.items():\n            _transfer_conv_bn(k, v)\n\n        # Transfer weights for final non-bn conv layer\n        _transfer_conv(\'conv6\', self.classification)\n\n        # Transfer weights for all residual layers\n        for k, v in residual_layers.items():\n            _transfer_residual(k, v)\n\n\n    def tile_predict(self, img, input_size=[713, 713]):\n        """"""\n        Predict by takin overlapping tiles from the image.\n    \n        :param img: np.ndarray with shape [C, H, W] in BGR format\n            \n        Source: https://github.com/mitmul/chainer-pspnet/blob/master/pspnet.py#L408-L448\n        Adapted for PyTorch\n        # TODO: Remove artifacts in last window.\n        """"""\n        \n        setattr(self, \'input_size\', input_size)\n\n        def _pad_img(img):\n            if img.shape[1] < self.input_size[0]:\n                pad_h = self.input_size[0] - img.shape[1]\n                img = np.pad(img, ((0, 0), (0, pad_h), (0, 0)), \'constant\')\n            else:\n                pad_h = 0\n            if img.shape[2] < self.input_size[1]:\n                pad_w = self.input_size[1] - img.shape[2]\n                img = np.pad(img, ((0, 0), (0, 0), (0, pad_w)), \'constant\')\n            else:\n                pad_w = 0\n            return img, pad_h, pad_w\n\n\n        ori_rows, ori_cols = img.shape[1:]\n        long_size = max(ori_rows, ori_cols)\n            \n        if long_size > max(self.input_size):\n            count = np.zeros((ori_rows, ori_cols))\n            pred = np.zeros((1, self.n_classes, ori_rows, ori_cols))\n            stride_rate = 2 / 3.\n            stride = (int(ceil(self.input_size[0] * stride_rate)),\n                      int(ceil(self.input_size[1] * stride_rate)))\n            hh = int(ceil((ori_rows - self.input_size[0]) / stride[0])) + 1\n            ww = int(ceil((ori_cols - self.input_size[1]) / stride[1])) + 1\n            for yy in range(hh):\n                for xx in range(ww):\n                    sy, sx = yy * stride[0], xx * stride[1]\n                    ey, ex = sy + self.input_size[0], sx + self.input_size[1]\n                    img_sub = img[:, sy:ey, sx:ex]\n                    img_sub, pad_h, pad_w = _pad_img(img_sub)\n                    img_sub_flp = np.copy(img_sub[:, :, ::-1])\n\n                    # inp = Variable(torch.unsqueeze(torch.from_numpy(img_sub).float(), 0).cuda(), volatile=True)\n                    # flp = Variable(torch.unsqueeze(torch.from_numpy(img_sub_flp).float(), 0).cuda(), volatile=True)\n                    inp = Variable(torch.unsqueeze(torch.from_numpy(img_sub).float(), 0), volatile=True)\n                    flp = Variable(torch.unsqueeze(torch.from_numpy(img_sub_flp).float(), 0), volatile=True)\n                    psub1 = F.softmax(self.forward(inp), dim=1).data.cpu().numpy()\n                    psub2 = F.softmax(self.forward(flp), dim=1).data.cpu().numpy()\n\n                    psub = (psub1 + psub2[:, :, :, ::-1]) / 2.0\n\n                    if sy + self.input_size[0] > ori_rows:\n                        psub = psub[:, :, :-pad_h, :]\n                    if sx + self.input_size[1] > ori_cols:\n                        psub = psub[:, :, :, :-pad_w]\n                    pred[:, :, sy:ey, sx:ex] = psub\n                    count[sy:ey, sx:ex] += 1\n            score = (pred / count[None, None, ...]).astype(np.float32)\n        else:\n            img, pad_h, pad_w = _pad_img(img)\n            # inp = Variable(torch.unsqueeze(torch.from_numpy(img), 0).cuda(), volatile=True)\n            inp = Variable(torch.unsqueeze(torch.from_numpy(img), 0), volatile=True)\n            pred1 = F.softmax(self.forward(inp), dim=1).data.cpu().numpy()\n            pred2 = F.softmax(self.forward(inp[:, :, :, ::-1]), dim=1).data.cpu().numpy()\n            pred = (pred1 + pred2[:, :, :, ::-1]) / 2.0\n            score = pred[:, :, :self.input_size[0] - pad_h, :self.input_size[1] - pad_w]\n        \n        tscore = F.upsample(torch.from_numpy(score), size=(ori_rows, ori_cols), mode=\'bilinear\')\n        score = tscore[0].data.numpy()\n        return score / score.sum(axis=0)\n\nif __name__ == \'__main__\':\n    n_classes = 21\n    model = pspnet(n_classes=n_classes)\n    # model.init_vgg16()\n    x = Variable(torch.randn(1, 3, 128, 256))\n    y = Variable(torch.LongTensor(np.ones((1, 128, 256), dtype=np.int)))\n    # print(x.shape)\n\n    # ---------------------------pspnet\xe6\xa8\xa1\xe5\x9e\x8b\xe8\xbf\x90\xe8\xa1\x8c\xe6\x97\xb6\xe9\x97\xb4-----------------------\n    start = time.time()\n    pred = model(x)\n    end = time.time()\n    print(end-start)\n\n    print(pred.data.size())\n    loss = cross_entropy2d(pred, y)\n    print(loss)\n'"
semseg/modelloader/segnet.py,10,"b'#!/usr/bin/python\n# -*- coding: UTF-8 -*-\nimport time\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nfrom torchvision import models\nfrom torchvision.models.squeezenet import Fire\n\nfrom semseg.loss import cross_entropy2d\nfrom semseg.modelloader.utils import segnetDown2, segnetDown3, segnetUp2, segnetUp3, conv2DBatchNormRelu, \\\n    AlignedResInception, segnetDown4, segnetUp4\n\n\nclass segnet(nn.Module):\n    def __init__(self, n_classes=21, pretrained=False):\n        super(segnet, self).__init__()\n        self.down1 = segnetDown2(3, 64)\n        self.down2 = segnetDown2(64, 128)\n        self.down3 = segnetDown3(128, 256)\n        self.down4 = segnetDown3(256, 512)\n        self.down5 = segnetDown3(512, 512)\n\n        self.up5 = segnetUp3(512, 512)\n        self.up4 = segnetUp3(512, 256)\n        self.up3 = segnetUp3(256, 128)\n        self.up2 = segnetUp2(128, 64)\n        self.up1 = segnetUp2(64, n_classes)\n\n        self.init_weights(pretrained=pretrained)\n\n    def forward(self, x):\n        x, pool_indices1, unpool_shape1 = self.down1(x)\n        x, pool_indices2, unpool_shape2 = self.down2(x)\n        x, pool_indices3, unpool_shape3 = self.down3(x)\n        x, pool_indices4, unpool_shape4 = self.down4(x)\n        x, pool_indices5, unpool_shape5 = self.down5(x)\n\n        x = self.up5(x, pool_indices=pool_indices5, unpool_shape=unpool_shape5)\n        x = self.up4(x, pool_indices=pool_indices4, unpool_shape=unpool_shape4)\n        x = self.up3(x, pool_indices=pool_indices3, unpool_shape=unpool_shape3)\n        x = self.up2(x, pool_indices=pool_indices2, unpool_shape=unpool_shape2)\n        x = self.up1(x, pool_indices=pool_indices1, unpool_shape=unpool_shape1)\n        return x\n\n    def init_weights(self, pretrained=False):\n        # the model vgg16_bn is better than vgg16?\n        # vgg16 = models.vgg16(pretrained=pretrained)\n        vgg16 = models.vgg16_bn(pretrained=pretrained)\n\n        # -----------\xe8\xb5\x8b\xe5\x80\xbc\xe5\x89\x8d\xe9\x9d\xa22+2+3+3+3\xe5\xb1\x82feature\xe7\x9a\x84\xe7\x89\xb9\xe5\xbe\x81-------------\n        # \xe7\x94\xb1\xe4\xba\x8evgg16\xe7\x9a\x84\xe7\x89\xb9\xe5\xbe\x81\xe6\x98\xafSequential\xef\xbc\x8c\xe8\x8e\xb7\xe5\xbe\x97\xe5\x85\xb6\xe4\xb8\xad\xe7\x9a\x84\xe5\xad\x90\xe7\xb1\xbb\xe9\x80\x9a\xe8\xbf\x87children()\n        vgg16_features = list(vgg16.features.children())\n        vgg16_conv_layers = []\n        for layer in vgg16_features:\n            if isinstance(layer, nn.Conv2d) or isinstance(layer, nn.BatchNorm2d):\n                # print(layer)\n                vgg16_conv_layers.append(layer)\n\n\n        conv_blocks = [self.down1, self.down2, self.down3, self.down4, self.down5]\n\n        segnet_down_conv_layers = []\n        for conv_block_id, conv_block in enumerate(conv_blocks):\n            # print(conv_block_id)\n            # print(conv_block)\n            conv_block_children =  list(conv_block.children())\n            for conv_block_child in conv_block_children:\n                if isinstance(conv_block_child, conv2DBatchNormRelu):\n                    # print(conv_block_child)\n                    if hasattr(conv_block_child, \'cbr_seq\'):\n                        # print(conv_block_child.cbr_seq)\n                        layer_lists = list(conv_block_child.cbr_seq)\n                        for layer in conv_block_child.cbr_seq:\n                            # print(layer)\n                            if isinstance(layer, nn.Conv2d) or isinstance(layer, nn.BatchNorm2d):\n                                # print(layer)\n                                segnet_down_conv_layers.append(layer)\n\n        # print(\'len(segnet_down_conv_layers):\', len(segnet_down_conv_layers))\n        # print(\'len(vgg16_conv_layers)\', len(vgg16_conv_layers))\n\n        for l1, l2 in zip(segnet_down_conv_layers, vgg16_conv_layers):\n            if isinstance(l1, nn.Conv2d) and isinstance(l2, nn.Conv2d):\n                assert l1.weight.size() == l2.weight.size()\n                assert l1.bias.size() == l2.bias.size()\n                # \xe8\xb5\x8b\xe5\x80\xbc\xe7\x9a\x84\xe6\x98\xaf\xe6\x95\xb0\xe6\x8d\xae\n                l1.weight.data = l2.weight.data\n                l1.bias.data = l2.bias.data\n                # print(l1)\n                # print(l2)\n\nclass segnet_vgg19(nn.Module):\n    def __init__(self, n_classes=21, pretrained=False):\n        super(segnet_vgg19, self).__init__()\n        self.down1 = segnetDown2(3, 64)\n        self.down2 = segnetDown2(64, 128)\n        self.down3 = segnetDown4(128, 256)\n        self.down4 = segnetDown4(256, 512)\n        self.down5 = segnetDown4(512, 512)\n\n        self.up5 = segnetUp4(512, 512)\n        self.up4 = segnetUp4(512, 256)\n        self.up3 = segnetUp4(256, 128)\n        self.up2 = segnetUp2(128, 64)\n        self.up1 = segnetUp2(64, n_classes)\n\n        self.init_vgg19(pretrained=pretrained)\n\n    def forward(self, x):\n        x, pool_indices1, unpool_shape1 = self.down1(x)\n        x, pool_indices2, unpool_shape2 = self.down2(x)\n        x, pool_indices3, unpool_shape3 = self.down3(x)\n        x, pool_indices4, unpool_shape4 = self.down4(x)\n        x, pool_indices5, unpool_shape5 = self.down5(x)\n\n        x = self.up5(x, pool_indices=pool_indices5, unpool_shape=unpool_shape5)\n        x = self.up4(x, pool_indices=pool_indices4, unpool_shape=unpool_shape4)\n        x = self.up3(x, pool_indices=pool_indices3, unpool_shape=unpool_shape3)\n        x = self.up2(x, pool_indices=pool_indices2, unpool_shape=unpool_shape2)\n        x = self.up1(x, pool_indices=pool_indices1, unpool_shape=unpool_shape1)\n        return x\n\n    def init_vgg19(self, pretrained=False):\n        vgg19 = models.vgg19(pretrained=pretrained)\n\n        # -----------\xe8\xb5\x8b\xe5\x80\xbc\xe5\x89\x8d\xe9\x9d\xa22+2+3+3+3\xe5\xb1\x82feature\xe7\x9a\x84\xe7\x89\xb9\xe5\xbe\x81-------------\n        # \xe7\x94\xb1\xe4\xba\x8evgg16\xe7\x9a\x84\xe7\x89\xb9\xe5\xbe\x81\xe6\x98\xafSequential\xef\xbc\x8c\xe8\x8e\xb7\xe5\xbe\x97\xe5\x85\xb6\xe4\xb8\xad\xe7\x9a\x84\xe5\xad\x90\xe7\xb1\xbb\xe9\x80\x9a\xe8\xbf\x87children()\n        vgg19_features = list(vgg19.features.children())\n        vgg19_conv_layers = []\n        for layer in vgg19_features:\n            if isinstance(layer, nn.Conv2d):\n                # print(layer)\n                vgg19_conv_layers.append(layer)\n\n\n        conv_blocks = [self.down1, self.down2, self.down3, self.down4, self.down5]\n\n        segnet_down_conv_layers = []\n        for conv_block_id, conv_block in enumerate(conv_blocks):\n            # print(conv_block_id)\n            # print(conv_block)\n            conv_block_children =  list(conv_block.children())\n            for conv_block_child in conv_block_children:\n                if isinstance(conv_block_child, conv2DBatchNormRelu):\n                    # print(conv_block_child)\n                    if hasattr(conv_block_child, \'cbr_seq\'):\n                        # print(conv_block_child.cbr_seq)\n                        layer_lists = list(conv_block_child.cbr_seq)\n                        for layer in conv_block_child.cbr_seq:\n                            # print(layer)\n                            if isinstance(layer, nn.Conv2d):\n                                # print(layer)\n                                segnet_down_conv_layers.append(layer)\n\n        # print(\'len(segnet_down_conv_layers):\', len(segnet_down_conv_layers))\n        # print(\'len(vgg16_conv_layers)\', len(vgg16_conv_layers))\n\n        for l1, l2 in zip(segnet_down_conv_layers, vgg19_conv_layers):\n            if isinstance(l1, nn.Conv2d) and isinstance(l2, nn.Conv2d):\n                assert l1.weight.size() == l2.weight.size()\n                assert l1.bias.size() == l2.bias.size()\n                # \xe8\xb5\x8b\xe5\x80\xbc\xe7\x9a\x84\xe6\x98\xaf\xe6\x95\xb0\xe6\x8d\xae\n                l1.weight.data = l2.weight.data\n                l1.bias.data = l2.bias.data\n                # print(l1)\n                # print(l2)\n\nclass segnet_alignres(nn.Module):\n    def __init__(self, n_classes=21, pretrained=False):\n        super(segnet_alignres, self).__init__()\n        self.down1 = segnetDown2(3, 64)\n        self.down2 = segnetDown2(64, 128)\n        self.down3 = segnetDown3(128, 256)\n        self.down4 = segnetDown3(256, 512)\n        self.down5 = segnetDown3(512, 512)\n\n        self.alignres = AlignedResInception(512)\n\n        self.up5 = segnetUp3(512, 512)\n        self.up4 = segnetUp3(512, 256)\n        self.up3 = segnetUp3(256, 128)\n        self.up2 = segnetUp2(128, 64)\n        self.up1 = segnetUp2(64, n_classes)\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal(m.weight, mode=\'fan_out\')\n            elif isinstance(m, nn.BatchNorm2d):\n                nn.init.constant(m.weight, 1)\n                nn.init.constant(m.bias, 0)\n\n        self.init_weights(pretrained=pretrained)\n\n    def forward(self, x):\n        x, pool_indices1, unpool_shape1 = self.down1(x)\n        x, pool_indices2, unpool_shape2 = self.down2(x)\n        x, pool_indices3, unpool_shape3 = self.down3(x)\n        x, pool_indices4, unpool_shape4 = self.down4(x)\n        x, pool_indices5, unpool_shape5 = self.down5(x)\n\n        # print(\'x.size():\', x.size())\n        x = self.alignres(x)\n        # print(\'x.size():\', x.size())\n\n        x = self.up5(x, pool_indices=pool_indices5, unpool_shape=unpool_shape5)\n        x = self.up4(x, pool_indices=pool_indices4, unpool_shape=unpool_shape4)\n        x = self.up3(x, pool_indices=pool_indices3, unpool_shape=unpool_shape3)\n        x = self.up2(x, pool_indices=pool_indices2, unpool_shape=unpool_shape2)\n        x = self.up1(x, pool_indices=pool_indices1, unpool_shape=unpool_shape1)\n        return x\n\n    def init_weights(self, pretrained=False):\n        vgg16 = models.vgg16(pretrained=pretrained)\n\n        # -----------\xe8\xb5\x8b\xe5\x80\xbc\xe5\x89\x8d\xe9\x9d\xa22+2+3+3+3\xe5\xb1\x82feature\xe7\x9a\x84\xe7\x89\xb9\xe5\xbe\x81-------------\n        # \xe7\x94\xb1\xe4\xba\x8evgg16\xe7\x9a\x84\xe7\x89\xb9\xe5\xbe\x81\xe6\x98\xafSequential\xef\xbc\x8c\xe8\x8e\xb7\xe5\xbe\x97\xe5\x85\xb6\xe4\xb8\xad\xe7\x9a\x84\xe5\xad\x90\xe7\xb1\xbb\xe9\x80\x9a\xe8\xbf\x87children()\n        vgg16_features = list(vgg16.features.children())\n        vgg16_conv_layers = []\n        for layer in vgg16_features:\n            if isinstance(layer, nn.Conv2d):\n                # print(layer)\n                vgg16_conv_layers.append(layer)\n\n\n        conv_blocks = [self.down1, self.down2, self.down3, self.down4, self.down5]\n\n        segnet_down_conv_layers = []\n        for conv_block_id, conv_block in enumerate(conv_blocks):\n            # print(conv_block_id)\n            # print(conv_block)\n            conv_block_children =  list(conv_block.children())\n            for conv_block_child in conv_block_children:\n                if isinstance(conv_block_child, conv2DBatchNormRelu):\n                    # print(conv_block_child)\n                    if hasattr(conv_block_child, \'cbr_seq\'):\n                        # print(conv_block_child.cbr_seq)\n                        layer_lists = list(conv_block_child.cbr_seq)\n                        for layer in conv_block_child.cbr_seq:\n                            # print(layer)\n                            if isinstance(layer, nn.Conv2d):\n                                # print(layer)\n                                segnet_down_conv_layers.append(layer)\n\n        # print(\'len(segnet_down_conv_layers):\', len(segnet_down_conv_layers))\n        # print(\'len(vgg16_conv_layers)\', len(vgg16_conv_layers))\n\n        for l1, l2 in zip(segnet_down_conv_layers, vgg16_conv_layers):\n            if isinstance(l1, nn.Conv2d) and isinstance(l2, nn.Conv2d):\n                assert l1.weight.size() == l2.weight.size()\n                assert l1.bias.size() == l2.bias.size()\n                # \xe8\xb5\x8b\xe5\x80\xbc\xe7\x9a\x84\xe6\x98\xaf\xe6\x95\xb0\xe6\x8d\xae\n                l1.weight.data = l2.weight.data\n                l1.bias.data = l2.bias.data\n                # print(l1)\n                # print(l2)\n\n# class Fire(nn.Module):\n#\n#     def __init__(self, inplanes, squeeze_planes,\n#                  expand1x1_planes, expand3x3_planes):\n#         super(Fire, self).__init__()\n#         self.inplanes = inplanes\n#         self.squeeze = nn.Conv2d(inplanes, squeeze_planes, kernel_size=1)\n#         self.squeeze_activation = nn.ReLU(inplace=True)\n#         self.expand1x1 = nn.Conv2d(squeeze_planes, expand1x1_planes,\n#                                    kernel_size=1)\n#         self.expand1x1_activation = nn.ReLU(inplace=True)\n#         self.expand3x3 = nn.Conv2d(squeeze_planes, expand3x3_planes,\n#                                    kernel_size=3, padding=1)\n#         self.expand3x3_activation = nn.ReLU(inplace=True)\n#\n#     def forward(self, x):\n#         x = self.squeeze_activation(self.squeeze(x))\n#         return torch.cat([self.expand1x1_activation(self.expand1x1(x)), self.expand3x3_activation(self.expand3x3(x))], 1)\n\nclass segnet_squeeze(nn.Module):\n    """"""\n    \xe5\x8f\x82\xe8\x80\x83\xe8\xae\xba\xe6\x96\x87\n    Squeeze-SegNet: A new fast Deep Convolutional Neural Network for Semantic Segmentation\n    """"""\n    def __init__(self, n_classes=21, pretrained=False):\n        super(segnet_squeeze, self).__init__()\n\n        # ----\xe4\xb8\x8b\xe9\x87\x87\xe6\xa0\xb7----\n        self.conv1 = nn.Conv2d(3, 96, kernel_size=7, stride=2)\n        self.relu1 = nn.ReLU(inplace=True)\n        self.pool1 = nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True, return_indices=True)\n\n        self.fire2 = Fire(96, 16, 64, 64)\n        self.fire3 = Fire(128, 16, 64, 64)\n        self.fire4 = Fire(128, 32, 128, 128)\n        self.pool2 = nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True, return_indices=True)\n\n        self.fire5 = Fire(256, 32, 128, 128)\n        self.fire6 = Fire(256, 48, 192, 192)\n        self.fire7 = Fire(384, 48, 192, 192)\n        self.fire8 = Fire(384, 64, 256, 256)\n        self.pool3 = nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True, return_indices=True)\n\n        self.fire9 = Fire(512, 64, 256, 256)\n\n        self.conv10 = nn.Conv2d(512, 1000, kernel_size=1)\n        self.relu10 = nn.ReLU(inplace=True)\n\n        # ----\xe4\xb8\x8a\xe9\x87\x87\xe6\xa0\xb7----\n        self.conv10_D = nn.Conv2d(1000, 512, kernel_size=1)\n        self.relu10_D = nn.ReLU(inplace=True)\n\n        self.fire9_D = Fire(512, 64, 256, 256)\n\n        self.unpool3 = nn.MaxUnpool2d(kernel_size=3, stride=2)\n        self.fire8_D = Fire(512, 48, 192, 192)\n        self.fire7_D = Fire(384, 48, 192, 192)\n        self.fire6_D = Fire(384, 32, 128, 128)\n        self.fire5_D = Fire(256, 32, 128, 128)\n\n        self.unpool2 = nn.MaxUnpool2d(kernel_size=3, stride=2)\n        self.fire4_D = Fire(256, 16, 64, 64)\n        self.fire3_D = Fire(128, 16, 64, 64)\n        self.fire2_D = Fire(128, 12, 48, 48)\n\n        self.unpool1 = nn.MaxUnpool2d(kernel_size=3, stride=2)\n        # self.conv1_D = nn.ConvTranspose2d(96, n_classes, kernel_size=8, stride=2)\n        self.conv1_D = nn.ConvTranspose2d(96, n_classes, kernel_size=10, stride=2, padding=1)\n\n        self.init_weights(pretrained)\n\n    def init_weights(self, pretrained=False):\n        sequeeze = models.squeezenet1_0(pretrained=pretrained)\n\n        sequeeze_features = list(sequeeze.features.children())\n        sequeeze_conv_layers = []\n        fire_counts = 0\n        for layer in sequeeze_features:\n            if isinstance(layer, nn.Conv2d):\n                # print(layer)\n                sequeeze_conv_layers.append(layer)\n            if  isinstance(layer, Fire):\n                fire_children = list(layer.children())\n                # print(fire_children)\n                for fire_children_layer in fire_children:\n                    if isinstance(fire_children_layer, nn.Conv2d):\n                        pass\n                        # print(fire_children_layer)\n                        sequeeze_conv_layers.append(fire_children_layer)\n                # fire_counts += 1\n                # if fire_counts==8:\n                #     break\n\n        segnet_squeeze_down_conv_layers= []\n\n        features = list(self.children())\n        for layer in features:\n            if len(segnet_squeeze_down_conv_layers)==len(sequeeze_conv_layers):\n                break\n            if isinstance(layer, nn.Conv2d):\n                # print(layer)\n                segnet_squeeze_down_conv_layers.append(layer)\n            if  isinstance(layer, Fire):\n                fire_children = list(layer.children())\n                # print(fire_children)\n                for fire_children_layer in fire_children:\n                    if isinstance(fire_children_layer, nn.Conv2d):\n                        pass\n                        # print(fire_children_layer)\n                        segnet_squeeze_down_conv_layers.append(fire_children_layer)\n        for l1, l2 in zip(segnet_squeeze_down_conv_layers, sequeeze_conv_layers):\n            if isinstance(l1, nn.Conv2d) and isinstance(l2, nn.Conv2d):\n                assert l1.weight.size() == l2.weight.size()\n                assert l1.bias.size() == l2.bias.size()\n                # \xe8\xb5\x8b\xe5\x80\xbc\xe7\x9a\x84\xe6\x98\xaf\xe6\x95\xb0\xe6\x8d\xae\n                l1.weight.data = l2.weight.data\n                l1.bias.data = l2.bias.data\n                # print(l1)\n                # print(l2)\n\n\n    def forward(self, x):\n        # ----\xe4\xb8\x8b\xe9\x87\x87\xe6\xa0\xb7----\n        x = self.conv1(x)\n        x = self.relu1(x)\n        unpool_shape1 = x.size()\n        x, pool_indices1 = self.pool1(x)\n\n        x = self.fire2(x)\n        x = self.fire3(x)\n        x = self.fire4(x)\n        unpool_shape2 = x.size()\n        x, pool_indices2 = self.pool2(x)\n\n        x = self.fire5(x)\n        x = self.fire6(x)\n        x = self.fire7(x)\n        x = self.fire8(x)\n        unpool_shape3 = x.size()\n        x, pool_indices3 = self.pool3(x)\n\n        x = self.fire9(x)\n        x = self.conv10(x)\n        x = self.relu10(x)\n\n        # ----\xe4\xb8\x8a\xe9\x87\x87\xe6\xa0\xb7----\n        x = self.conv10_D(x)\n        x = self.relu10_D(x)\n        x = self.fire9_D(x)\n\n        x = self.unpool3(x, indices=pool_indices3, output_size=unpool_shape3)\n        x = self.fire8_D(x)\n        x = self.fire7_D(x)\n        x = self.fire6_D(x)\n        x = self.fire5_D(x)\n\n        x = self.unpool2(x, indices=pool_indices2, output_size=unpool_shape2)\n        x = self.fire4_D(x)\n        x = self.fire3_D(x)\n        x = self.fire2_D(x)\n\n        x = self.unpool1(x, indices=pool_indices1, output_size=unpool_shape1)\n        x = self.conv1_D(x)\n        return x\n\n\nif __name__ == \'__main__\':\n    batch_size = 1\n    n_classes = 21\n    model = segnet(n_classes=n_classes, pretrained=True)\n    x = Variable(torch.randn(1, 3, 360, 480))\n    y = Variable(torch.LongTensor(np.ones((1, 360, 480), dtype=np.int)))\n    # print(x.shape)\n    start = time.time()\n    pred = model(x)\n    end = time.time()\n    print(end-start)\n    # print(pred.shape)\n    print(\'pred.type:\', pred.type)\n    loss = cross_entropy2d(pred, y)\n    # print(loss)\n\n    # n_classes = 21\n    # model = segnet_alignres(n_classes=n_classes, pretrained=False)\n    # x = Variable(torch.randn(batch_size, 3, 360, 480))\n    # y = Variable(torch.LongTensor(np.ones((batch_size, 360, 480), dtype=np.int)))\n    # # print(x.shape)\n    # start = time.time()\n    # pred = model(x)\n    # end = time.time()\n    # print(end-start)\n    # # print(pred.shape)\n    # # print(\'pred.type:\', pred.type)\n    # loss = cross_entropy2d(pred, y)\n    # # print(loss)\n\n    # n_classes = 21\n    # model = segnet_squeeze(n_classes=n_classes, pretrained=False)\n    # x = Variable(torch.randn(1, 3, 360, 480))\n    # y = Variable(torch.LongTensor(np.ones((1, 360, 480), dtype=np.int)))\n    # # print(x.shape)\n    # start = time.time()\n    # pred = model(x)\n    # end = time.time()\n    # print(end-start)\n    # print(\'pred.shape:\', pred.shape)\n    # # print(\'pred.type:\', pred.type)\n    # loss = cross_entropy2d(pred, y)\n    # # print(loss)\n'"
semseg/modelloader/segnet_unet.py,6,"b""#!/usr/bin/python\n# -*- coding: UTF-8 -*-\n# \xe5\x8f\x82\xe8\x80\x83[Seg-UNet](https://github.com/ykamikawa/Seg-UNet/blob/master/model.py)Tensorflow\xe5\xae\x9e\xe7\x8e\xb0\nimport time\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nfrom torchvision import models\nfrom torchvision.models.squeezenet import Fire\n\nfrom semseg.loss import cross_entropy2d\nfrom semseg.modelloader.utils import segnetUNetDown2, segnetUNetDown3, segnetUNetUp2, segnetUNetUp3, \\\n    conv2DBatchNormRelu, segnetDown2, segnetUp2, segnetDown3, segnetUp3\n\n\nclass segnet_unet(nn.Module):\n    def __init__(self, n_classes=21, pretrained=False):\n        super(segnet_unet, self).__init__()\n        # self.down1 = segnetUNetDown2(3, 64)\n        self.down1 = segnetDown2(3, 64)\n        # self.down2 = segnetUNetDown2(64, 128)\n        self.down2 = segnetDown2(64, 128)\n        self.down3 = segnetDown3(128, 256)\n        self.down4 = segnetUNetDown3(256, 512)\n        self.down5 = segnetUNetDown3(512, 512)\n\n        self.up5 = segnetUNetUp3(512, 512)\n        self.up4 = segnetUNetUp3(512, 256)\n        self.up3 = segnetUp3(256, 128)\n        self.up2 = segnetUp2(128, 64)\n        # self.up1 = segnetUNetUp2(64, n_classes)\n        self.up1 = segnetUp2(64, n_classes)\n\n        self.init_vgg16(pretrained=pretrained)\n\n    def forward(self, x):\n        x_down1, pool_indices1, unpool_shape1 = self.down1(x)\n        x_down2, pool_indices2, unpool_shape2 = self.down2(x_down1)\n        x_down3, pool_indices3, unpool_shape3 = self.down3(x_down2)\n        x_down4, pool_indices4, unpool_shape4, x_undown4 = self.down4(x_down3)\n        x_down5, pool_indices5, unpool_shape5, x_undown5 = self.down5(x_down4)\n\n        x_up5 = self.up5(x_down5, pool_indices=pool_indices5, unpool_shape=unpool_shape5, concat_net=x_undown5)\n        x_up4 = self.up4(x_up5, pool_indices=pool_indices4, unpool_shape=unpool_shape4, concat_net=x_undown4)\n        x_up3 = self.up3(x_up4, pool_indices=pool_indices3, unpool_shape=unpool_shape3)\n        x_up2 = self.up2(x_up3, pool_indices=pool_indices2, unpool_shape=unpool_shape2)\n        x_up1 = self.up1(x_up2, pool_indices=pool_indices1, unpool_shape=unpool_shape1)\n        return x_up1\n\n    def init_vgg16(self, pretrained=False):\n        vgg16 = models.vgg16(pretrained=pretrained)\n\n        # -----------\xe8\xb5\x8b\xe5\x80\xbc\xe5\x89\x8d\xe9\x9d\xa22+2+3+3+3\xe5\xb1\x82feature\xe7\x9a\x84\xe7\x89\xb9\xe5\xbe\x81-------------\n        # \xe7\x94\xb1\xe4\xba\x8evgg16\xe7\x9a\x84\xe7\x89\xb9\xe5\xbe\x81\xe6\x98\xafSequential\xef\xbc\x8c\xe8\x8e\xb7\xe5\xbe\x97\xe5\x85\xb6\xe4\xb8\xad\xe7\x9a\x84\xe5\xad\x90\xe7\xb1\xbb\xe9\x80\x9a\xe8\xbf\x87children()\n        vgg16_features = list(vgg16.features.children())\n        vgg16_conv_layers = []\n        for layer in vgg16_features:\n            if isinstance(layer, nn.Conv2d):\n                # print(layer)\n                vgg16_conv_layers.append(layer)\n\n\n        conv_blocks = [self.down1, self.down2, self.down3, self.down4, self.down5]\n        conv_ids_vgg = [[0, 4], [5, 9], [10, 16], [17, 23], [24, 30]]\n\n        segnet_down_conv_layers = []\n        for conv_block_id, conv_block in enumerate(conv_blocks):\n            # print(conv_block_id)\n            # print(conv_block)\n            conv_block_children =  list(conv_block.children())\n            for conv_block_child in conv_block_children:\n                if isinstance(conv_block_child, conv2DBatchNormRelu):\n                    # print(conv_block_child)\n                    if hasattr(conv_block_child, 'cbr_seq'):\n                        # print(conv_block_child.cbr_seq)\n                        layer_lists = list(conv_block_child.cbr_seq)\n                        for layer in conv_block_child.cbr_seq:\n                            # print(layer)\n                            if isinstance(layer, nn.Conv2d):\n                                # print(layer)\n                                segnet_down_conv_layers.append(layer)\n\n        # print('len(segnet_down_conv_layers):', len(segnet_down_conv_layers))\n        # print('len(vgg16_conv_layers)', len(vgg16_conv_layers))\n\n        for l1, l2 in zip(segnet_down_conv_layers, vgg16_conv_layers):\n            if isinstance(l1, nn.Conv2d) and isinstance(l2, nn.Conv2d):\n                assert l1.weight.size() == l2.weight.size()\n                assert l1.bias.size() == l2.bias.size()\n                # \xe8\xb5\x8b\xe5\x80\xbc\xe7\x9a\x84\xe6\x98\xaf\xe6\x95\xb0\xe6\x8d\xae\n                l1.weight.data = l2.weight.data\n                l1.bias.data = l2.bias.data\n                # print(l1)\n                # print(l2)\n\n\nif __name__ == '__main__':\n    n_classes = 21\n    model = segnet_unet(n_classes=n_classes, pretrained=False)\n    # model.init_vgg16()\n    x = Variable(torch.randn(1, 3, 360, 480))\n    y = Variable(torch.LongTensor(np.ones((1, 360, 480), dtype=np.int)))\n    # print(x.shape)\n    start = time.time()\n    pred = model(x)\n    end = time.time()\n    print(end-start)\n    # print(pred.shape)\n    loss = cross_entropy2d(pred, y)\n    # print(loss)\n    torch.save(model.state_dict(), '/tmp/tmp.pt')\n"""
semseg/modelloader/sqnet.py,14,"b'# -*- coding: utf-8 -*-\n\n# code from [SQNet](https://github.com/youansheng/SQNet/blob/master/model.py)\nimport time\n\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\n# import torch.functional as F\nimport torch.nn.functional as F\nimport numpy as np\nimport torch.optim as optim\nimport math\n\nfrom semseg.loss import cross_entropy2d\n\n\nclass Fire(nn.Module):\n    def __init__(self, inplanes, squeeze_planes, expand_planes):\n        super(Fire, self).__init__()\n        self.conv1 = nn.Conv2d(inplanes, squeeze_planes, kernel_size=1, stride=1)\n        # self.bn1 = nn.BatchNorm2d(squeeze_planes)\n        self.relu1 = nn.ELU(inplace=True)\n        self.conv2 = nn.Conv2d(squeeze_planes, expand_planes, kernel_size=1, stride=1)\n        # self.bn2 = nn.BatchNorm2d(expand_planes)\n        self.conv3 = nn.Conv2d(squeeze_planes, expand_planes, kernel_size=3, stride=1, padding=1)\n        # self.bn3 = nn.BatchNorm2d(expand_planes)\n        self.relu2 = nn.ELU(inplace=True)\n\n        # using MSR initilization\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                n = m.kernel_size[0] * m.kernel_size[1] * m.in_channels\n                m.weight.data.normal_(0, math.sqrt(2./n))\n\n    def forward(self, x):\n        x = self.conv1(x)\n        # x = self.bn1(x)\n        x = self.relu1(x)\n        out1 = self.conv2(x)\n        # out1 = self.bn2(out1)\n        out2 = self.conv3(x)\n        # out2 = self.bn3(out2)\n        out = torch.cat([out1, out2], 1)\n        out = self.relu2(out)\n        return out\n\n\nclass ParallelDilatedConv(nn.Module):\n    def __init__(self, inplanes, planes):\n        super(ParallelDilatedConv, self).__init__()\n        self.dilated_conv_1 = nn.Conv2d(inplanes, planes, kernel_size=3, stride=1, padding=1, dilation=1)\n        self.dilated_conv_2 = nn.Conv2d(inplanes, planes, kernel_size=3, stride=1, padding=2, dilation=2)\n        self.dilated_conv_3 = nn.Conv2d(inplanes, planes, kernel_size=3, stride=1, padding=3, dilation=3)\n        self.dilated_conv_4 = nn.Conv2d(inplanes, planes, kernel_size=3, stride=1, padding=4, dilation=4)\n        self.relu1 = nn.ELU(inplace=True)\n        self.relu2 = nn.ELU(inplace=True)\n        self.relu3 = nn.ELU(inplace=True)\n        self.relu4 = nn.ELU(inplace=True)\n\n    def forward(self, x):\n        out1 = self.dilated_conv_1(x)\n        out2 = self.dilated_conv_2(x)\n        out3 = self.dilated_conv_3(x)\n        out4 = self.dilated_conv_4(x)\n        out1 = self.relu1(out1)\n        out2 = self.relu2(out2)\n        out3 = self.relu3(out3)\n        out4 = self.relu4(out4)\n        out = out1 + out2 + out3 + out4\n        return out\n\nclass FCN(nn.Module):\n    def __init__(self, n_classes):\n        super(FCN, self).__init__()\n\n        self.num_classes = n_classes\n\n        self.conv1 = nn.Conv2d(3, 96, kernel_size=3, stride=2, padding=1) # 32\n        # self.bn1 = nn.BatchNorm2d(96)\n        self.relu1 = nn.ELU(inplace=True)\n        self.maxpool1 = nn.MaxPool2d(kernel_size=2, stride=2) # 16\n        self.fire1_1 = Fire(96, 16, 64)\n        self.fire1_2 = Fire(128, 16, 64)\n        self.maxpool2 = nn.MaxPool2d(kernel_size=2, stride=2) # 8\n        self.fire2_1 = Fire(128, 32, 128)\n        self.fire2_2 = Fire(256, 32, 128)\n        self.maxpool3 = nn.MaxPool2d(kernel_size=2, stride=2) # 4\n        self.fire3_1 = Fire(256, 64, 256)\n        self.fire3_2 = Fire(512, 64, 256)\n        self.fire3_3 = Fire(512, 64, 256)\n        self.parallel = ParallelDilatedConv(512, 512)\n        self.deconv1 = nn.ConvTranspose2d(512, 256, 3, stride=2, padding=1, output_padding=1)\n        # self.bn2 = nn.BatchNorm2d(256)\n        self.relu2 = nn.ELU(inplace=True)\n        self.deconv2 = nn.ConvTranspose2d(512, 128, 3, stride=2, padding=1, output_padding=1)\n        # self.bn3 = nn.BatchNorm2d(128)\n        self.relu3 = nn.ELU(inplace=True)\n        self.deconv3 = nn.ConvTranspose2d(256, 96, 3, stride=2, padding=1, output_padding=1)\n        # self.bn4 = nn.BatchNorm2d(96)\n        self.relu4 = nn.ELU(inplace=True)\n        self.deconv4 = nn.ConvTranspose2d(192, self.num_classes, 3, stride=2, padding=1, output_padding=1)\n\n        self.conv3_1 = nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1) # 32\n        self.conv3_2 = nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1) # 32\n        self.conv2_1 = nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1) # 32\n        self.conv2_2 = nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1) # 32\n        self.conv1_1 = nn.Conv2d(96, 96, kernel_size=3, stride=1, padding=1) # 32\n        self.conv1_2 = nn.Conv2d(192, 192, kernel_size=3, stride=1, padding=1) # 32\n\n        self.relu1_1 = nn.ELU(inplace=True)\n        self.relu1_2 = nn.ELU(inplace=True)\n        self.relu2_1 = nn.ELU(inplace=True)\n        self.relu2_2 = nn.ELU(inplace=True)\n        self.relu3_1 = nn.ELU(inplace=True)\n        self.relu3_2 = nn.ELU(inplace=True)\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                n = m.kernel_size[0] * m.kernel_size[1] * m.in_channels\n                m.weight.data.normal_(0, math.sqrt(2. / n))\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n\n\n    def forward(self, x):\n        x = self.conv1(x)\n        # x = self.bn1(x)\n        x_1 = self.relu1(x)\n        # print ""x_1: %s"" % str(x_1.size())\n        x = self.maxpool1(x_1)\n        x = self.fire1_1(x)\n        x_2 = self.fire1_2(x)\n        # print ""x_2: %s"" % str(x_2.size())\n        x = self.maxpool2(x_2)\n        x = self.fire2_1(x)\n        x_3 = self.fire2_2(x)\n        # print ""x_3: %s"" % str(x_3.size())\n        x = self.maxpool3(x_3)\n        x = self.fire3_1(x)\n        x = self.fire3_2(x)\n        x = self.fire3_3(x)\n        x = self.parallel(x)\n        # print ""x: %s"" % str(x.size())\n        y_3 = self.deconv1(x)\n        y_3 = self.relu2(y_3)\n        x_3 = self.conv3_1(x_3)\n        x_3 = self.relu3_1(x_3)\n        # print ""y_3: %s"" % str(y_3.size())\n        # x = x.transpose(1, 2, 0)\n        # print(\'x_3.size():\', x_3.size())\n        # print(\'y_3.size():\', y_3.size())\n        x_3 = F.upsample_bilinear(x_3, y_3.size()[2:])\n        x = torch.cat([x_3, y_3], 1)\n        x = self.conv3_2(x)\n        x = self.relu3_2(x)\n        # concat x_3\n        y_2 = self.deconv2(x)\n        y_2 = self.relu3(y_2)\n        x_2 = self.conv2_1(x_2)\n        x_2 = self.relu2_1(x_2)\n        # print ""y_2: %s"" % str(y_2.size())\n        # concat x_2\n        # print(\'x_2.size():\', x_2.size())\n        # print(\'y_2.size():\', y_2.size())\n        y_2 = F.upsample_bilinear(y_2, x_2.size()[2:])\n        x = torch.cat([x_2, y_2], 1)\n        x = self.conv2_2(x)\n        x = self.relu2_2(x)\n        y_1 = self.deconv3(x)\n        y_1 = self.relu4(y_1)\n        x_1 = self.conv1_1(x_1)\n        x_1 = self.relu1_1(x_1)\n        # print ""y_1: %s"" % str(y_1.size())\n        # concat x_1\n        x = torch.cat([x_1, y_1], 1)\n        x = self.conv1_2(x)\n        x = self.relu1_2(x)\n        x = self.deconv4(x)\n        return x #, x_1, x_2, x_3, y_1, y_2, y_3\n\ndef sqnet(n_classes=21, pretrained=False):\n    net = FCN(n_classes)\n    # inp = Variable(torch.randn(64,3,32,32))\n    # out = net.forward(inp)\n    # # print(out.size())\n    return net\n\nif __name__ == \'__main__\':\n    n_classes = 21\n    model = sqnet(n_classes=n_classes, pretrained=False)\n    # model.init_vgg16()\n    x = Variable(torch.randn(1, 3, 360, 480))\n    y = Variable(torch.LongTensor(np.ones((1, 360, 480), dtype=np.int)))\n    # x = Variable(torch.randn(1, 3, 512, 1024))\n    # y = Variable(torch.LongTensor(np.ones((1, 512, 1024), dtype=np.int)))\n    # print(x.shape)\n    start = time.time()\n    pred = model(x)\n    end = time.time()\n    print(end-start)\n    print(\'pred.shape:\', pred.shape)\n    # print(\'pred.type:\', pred.type)\n    loss = cross_entropy2d(pred, y)\n    # print(loss)\n'"
semseg/modelloader/unet.py,5,"b""#!/usr/bin/python\n# -*- coding: UTF-8 -*-\n\nimport time\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nfrom torchvision import models\n\nfrom semseg.loss import cross_entropy2d\nfrom semseg.modelloader.utils import unetDown, unetUp\n\n\nclass unet(nn.Module):\n    def __init__(self, n_classes=21, pretrained=False):\n        super(unet, self).__init__()\n        self.down1 = unetDown(in_channels=3, out_channels=64)\n        self.maxpool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.down2 = unetDown(in_channels=64, out_channels=128)\n        self.maxpool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.down3 = unetDown(in_channels=128, out_channels=256)\n        self.maxpool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.down4 = unetDown(in_channels=256, out_channels=512)\n        self.maxpool4 = nn.MaxPool2d(kernel_size=2, stride=2)\n\n        self.center = unetDown(in_channels=512, out_channels=1024)\n\n        self.up4 = unetUp(in_channels=1024, out_channels=512)\n        self.up3 = unetUp(in_channels=512, out_channels=256)\n        self.up2 = unetUp(in_channels=256, out_channels=128)\n        self.up1 = unetUp(in_channels=128, out_channels=64)\n\n        self.classifier = nn.Conv2d(in_channels=64, out_channels=n_classes, kernel_size=1)\n\n    def forward(self, x):\n        out_size = x.size()[2:]\n        down1_x = self.down1(x)\n        maxpool1_x = self.maxpool1(down1_x)\n        # print('maxpool1_x.data.size():', maxpool1_x.data.size())\n        down2_x = self.down2(maxpool1_x)\n        maxpool2_x = self.maxpool2(down2_x)\n        # print('maxpool2_x.data.size():', maxpool2_x.data.size())\n        down3_x = self.down3(maxpool2_x)\n        maxpool3_x = self.maxpool3(down3_x)\n        # print('maxpool3_x.data.size():', maxpool3_x.data.size())\n        down4_x = self.down4(maxpool3_x)\n        maxpool4_x = self.maxpool1(down4_x)\n        # print('maxpool4_x.data.size():', maxpool4_x.data.size())\n\n        center_x = self.center(maxpool4_x)\n        # print('center_x.data.size():', center_x.data.size())\n\n        up4_x = self.up4(center_x, down4_x)\n        # print('up4_x.data.size():', up4_x.data.size())\n        up3_x = self.up3(up4_x, down3_x)\n        # print('up3_x.data.size():', up3_x.data.size())\n        up2_x = self.up2(up3_x, down2_x)\n        # print('up2_x.data.size():', up2_x.data.size())\n        up1_x = self.up1(up2_x, down1_x)\n        # print('up1_x.data.size():', up1_x.data.size())\n\n        x = self.classifier(up1_x)\n        # \xe6\x9c\x80\xe5\x90\x8e\xe5\xb0\x86\xe6\xa8\xa1\xe5\x9e\x8b\xe4\xb8\x8a\xe9\x87\x87\xe6\xa0\xb7\xe5\x88\xb0\xe5\x8e\x9f\xe5\xa7\x8b\xe5\x88\x86\xe8\xbe\xa8\xe7\x8e\x87\n        x = F.upsample_bilinear(x, out_size)\n\n        return x\n\nif __name__ == '__main__':\n    n_classes = 21\n    image_width = 480\n    image_height = 360\n    model = unet(n_classes=n_classes, pretrained=False)\n    # model.init_vgg16()\n    x = Variable(torch.randn(1, 3, image_height, image_width))\n    y = Variable(torch.LongTensor(np.ones((1, image_height, image_width), dtype=np.int)))\n    # print(x.shape)\n\n    # ---------------------------unet\xe6\xa8\xa1\xe5\x9e\x8b\xe8\xbf\x90\xe8\xa1\x8c\xe6\x97\xb6\xe9\x97\xb4-----------------------\n    start = time.time()\n    pred = model(x)\n    end = time.time()\n    print(end-start)\n\n    print(pred.data.size())\n    loss = cross_entropy2d(pred, y)\n    print(loss)\n\n\n"""
semseg/modelloader/utils.py,12,"b'#!/usr/bin/python\n# -*- coding: UTF-8 -*-\nimport time\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nfrom torchvision import models\n\nclass conv2DBatchNorm(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size,  stride, padding, bias=True):\n        super(conv2DBatchNorm, self).__init__()\n\n        self.cb_seq = nn.Sequential(\n            nn.Conv2d(int(in_channels), int(out_channels), kernel_size=kernel_size, padding=padding, stride=stride, bias=bias),\n            nn.BatchNorm2d(int(out_channels)),\n        )\n\n    def forward(self, inputs):\n        outputs = self.cb_seq(inputs)\n        return outputs\n\nclass conv2DBatchNormRelu(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, bias=True, dilation=1):\n        super(conv2DBatchNormRelu, self).__init__()\n        self.cbr_seq = nn.Sequential(\n            nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride, padding=padding, bias=bias, dilation=dilation),\n            nn.BatchNorm2d(num_features=out_channels),\n            nn.ReLU(inplace=True)\n        )\n\n    def forward(self, x):\n        x = self.cbr_seq(x)\n        return x\n\nclass unetDown(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super(unetDown, self).__init__()\n        self.conv1 = nn.Sequential(\n            nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=3, stride=1, padding=0),\n            nn.BatchNorm2d(num_features=out_channels),\n            nn.ReLU(inplace=True),\n        )\n        self.conv2 = nn.Sequential(\n            nn.Conv2d(in_channels=out_channels, out_channels=out_channels, kernel_size=3, stride=1, padding=0),\n            nn.BatchNorm2d(num_features=out_channels),\n            nn.ReLU(inplace=True),\n        )\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.conv2(x)\n        return x\n\nclass unetUp(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super(unetUp, self).__init__()\n        self.upConv = nn.ConvTranspose2d(in_channels=in_channels, out_channels=out_channels, kernel_size=2, stride=2)\n        self.conv1 = nn.Sequential(\n            nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=3, stride=1, padding=0),\n            nn.ReLU(inplace=True),\n        )\n        self.conv2 = nn.Sequential(\n            nn.Conv2d(in_channels=out_channels, out_channels=out_channels, kernel_size=3, stride=1, padding=0),\n            nn.ReLU(inplace=True),\n        )\n\n\n    def forward(self, x_cur, x_prev):\n        x = self.upConv(x_cur)\n        x = torch.cat([F.upsample_bilinear(x_prev, size=x.size()[2:]), x], 1)\n        x = self.conv1(x)\n        x = self.conv2(x)\n        return x\n\n\nclass segnetDown2(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super(segnetDown2, self).__init__()\n        self.conv1 = conv2DBatchNormRelu(in_channels=in_channels, out_channels=out_channels, kernel_size=3, stride=1, padding=1)\n        self.conv2 = conv2DBatchNormRelu(in_channels=out_channels, out_channels=out_channels, kernel_size=3, stride=1, padding=1)\n        self.max_pool = nn.MaxPool2d(kernel_size=2, stride=2, return_indices=True)\n        pass\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.conv2(x)\n        unpool_shape = x.size()\n        # print(unpool_shape)\n        x, pool_indices = self.max_pool(x)\n        return x, pool_indices, unpool_shape\n\n\nclass segnetDown3(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super(segnetDown3, self).__init__()\n        self.conv1 = conv2DBatchNormRelu(in_channels=in_channels, out_channels=out_channels, kernel_size=3, stride=1, padding=1)\n        self.conv2 = conv2DBatchNormRelu(in_channels=out_channels, out_channels=out_channels, kernel_size=3, stride=1, padding=1)\n        self.conv3 = conv2DBatchNormRelu(in_channels=out_channels, out_channels=out_channels, kernel_size=3, stride=1, padding=1)\n        self.max_pool = nn.MaxPool2d(kernel_size=2, stride=2, return_indices=True)\n        pass\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.conv2(x)\n        x = self.conv3(x)\n        unpool_shape = x.size()\n        # print(unpool_shape)\n        x, pool_indices = self.max_pool(x)\n        return x, pool_indices, unpool_shape\n\nclass segnetDown4(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super(segnetDown4, self).__init__()\n        self.conv1 = conv2DBatchNormRelu(in_channels=in_channels, out_channels=out_channels, kernel_size=3, stride=1, padding=1)\n        self.conv2 = conv2DBatchNormRelu(in_channels=out_channels, out_channels=out_channels, kernel_size=3, stride=1, padding=1)\n        self.conv3 = conv2DBatchNormRelu(in_channels=out_channels, out_channels=out_channels, kernel_size=3, stride=1, padding=1)\n        self.conv4 = conv2DBatchNormRelu(in_channels=out_channels, out_channels=out_channels, kernel_size=3, stride=1, padding=1)\n        self.max_pool = nn.MaxPool2d(kernel_size=2, stride=2, return_indices=True)\n        pass\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.conv2(x)\n        x = self.conv3(x)\n        x = self.conv4(x)\n        unpool_shape = x.size()\n        # print(unpool_shape)\n        x, pool_indices = self.max_pool(x)\n        return x, pool_indices, unpool_shape\n\n\nclass segnetUp2(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super(segnetUp2, self).__init__()\n        self.max_unpool = nn.MaxUnpool2d(kernel_size=2, stride=2)\n        self.conv1 = conv2DBatchNormRelu(in_channels=in_channels, out_channels=in_channels, kernel_size=3, stride=1, padding=1)\n        self.conv2 = conv2DBatchNormRelu(in_channels=in_channels, out_channels=out_channels, kernel_size=3, stride=1, padding=1)\n        pass\n\n    def forward(self, x, pool_indices, unpool_shape):\n        x = self.max_unpool(x, indices=pool_indices, output_size=unpool_shape)\n        x = self.conv1(x)\n        x = self.conv2(x)\n        return x\n\nclass segnetUNetDown2(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super(segnetUNetDown2, self).__init__()\n        self.conv1 = conv2DBatchNormRelu(in_channels=in_channels, out_channels=out_channels, kernel_size=3, stride=1, padding=1)\n        self.conv2 = conv2DBatchNormRelu(in_channels=out_channels, out_channels=out_channels, kernel_size=3, stride=1, padding=1)\n        self.max_pool = nn.MaxPool2d(kernel_size=2, stride=2, return_indices=True)\n        pass\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.conv2(x)\n        unpool_shape = x.size()\n        # print(unpool_shape)\n        x_pool, pool_indices = self.max_pool(x)\n        return x_pool, pool_indices, unpool_shape, x\n\n\nclass segnetUNetDown3(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super(segnetUNetDown3, self).__init__()\n        self.conv1 = conv2DBatchNormRelu(in_channels=in_channels, out_channels=out_channels, kernel_size=3, stride=1, padding=1)\n        self.conv2 = conv2DBatchNormRelu(in_channels=out_channels, out_channels=out_channels, kernel_size=3, stride=1, padding=1)\n        self.conv3 = conv2DBatchNormRelu(in_channels=out_channels, out_channels=out_channels, kernel_size=3, stride=1, padding=1)\n        self.max_pool = nn.MaxPool2d(kernel_size=2, stride=2, return_indices=True)\n        pass\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.conv2(x)\n        x = self.conv3(x)\n        unpool_shape = x.size()\n        # print(unpool_shape)\n        x_pool, pool_indices = self.max_pool(x)\n        return x_pool, pool_indices, unpool_shape, x\n\nclass segnetUp3(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super(segnetUp3, self).__init__()\n        self.max_unpool = nn.MaxUnpool2d(kernel_size=2, stride=2)\n        self.conv1 = conv2DBatchNormRelu(in_channels=in_channels, out_channels=in_channels, kernel_size=3, stride=1, padding=1)\n        self.conv2 = conv2DBatchNormRelu(in_channels=in_channels, out_channels=in_channels, kernel_size=3, stride=1, padding=1)\n        self.conv3 = conv2DBatchNormRelu(in_channels=in_channels, out_channels=out_channels, kernel_size=3, stride=1, padding=1)\n        pass\n\n    def forward(self, x, pool_indices, unpool_shape):\n        x = self.max_unpool(x, indices=pool_indices, output_size=unpool_shape)\n        x = self.conv1(x)\n        x = self.conv2(x)\n        x = self.conv3(x)\n        return x\n\nclass segnetUp4(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super(segnetUp4, self).__init__()\n        self.max_unpool = nn.MaxUnpool2d(kernel_size=2, stride=2)\n        self.conv1 = conv2DBatchNormRelu(in_channels=in_channels, out_channels=in_channels, kernel_size=3, stride=1, padding=1)\n        self.conv2 = conv2DBatchNormRelu(in_channels=in_channels, out_channels=in_channels, kernel_size=3, stride=1, padding=1)\n        self.conv3 = conv2DBatchNormRelu(in_channels=in_channels, out_channels=in_channels, kernel_size=3, stride=1, padding=1)\n        self.conv4 = conv2DBatchNormRelu(in_channels=in_channels, out_channels=out_channels, kernel_size=3, stride=1, padding=1)\n        pass\n\n    def forward(self, x, pool_indices, unpool_shape):\n        x = self.max_unpool(x, indices=pool_indices, output_size=unpool_shape)\n        x = self.conv1(x)\n        x = self.conv2(x)\n        x = self.conv3(x)\n        x = self.conv4(x)\n        return x\n\nclass segnetUNetUp2(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super(segnetUNetUp2, self).__init__()\n        self.max_unpool = nn.MaxUnpool2d(kernel_size=2, stride=2)\n        self.conv1 = conv2DBatchNormRelu(in_channels=in_channels*2, out_channels=in_channels, kernel_size=3, stride=1, padding=1)\n        # self.conv1 = conv2DBatchNormRelu(in_channels=in_channels, out_channels=in_channels, kernel_size=3, stride=1, padding=1)\n        self.conv2 = conv2DBatchNormRelu(in_channels=in_channels, out_channels=out_channels, kernel_size=3, stride=1, padding=1)\n        pass\n\n    def forward(self, x, pool_indices, unpool_shape, concat_net):\n        x = self.max_unpool(x, indices=pool_indices, output_size=unpool_shape)\n        # print(\'concat_net.size():\', concat_net.size())\n        # print(\'x.size():\', x.size())\n        x = torch.cat([concat_net, x], 1)\n        # x = concat_net+x\n        x = self.conv1(x)\n        x = self.conv2(x)\n        return x\n\n\nclass segnetUNetUp3(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super(segnetUNetUp3, self).__init__()\n        self.max_unpool = nn.MaxUnpool2d(kernel_size=2, stride=2)\n        self.conv1 = conv2DBatchNormRelu(in_channels=in_channels*2, out_channels=in_channels, kernel_size=3, stride=1, padding=1)\n        # self.conv1 = conv2DBatchNormRelu(in_channels=in_channels, out_channels=in_channels, kernel_size=3, stride=1, padding=1)\n        self.conv2 = conv2DBatchNormRelu(in_channels=in_channels, out_channels=in_channels, kernel_size=3, stride=1, padding=1)\n        self.conv3 = conv2DBatchNormRelu(in_channels=in_channels, out_channels=out_channels, kernel_size=3, stride=1, padding=1)\n        pass\n\n    def forward(self, x, pool_indices, unpool_shape, concat_net):\n        x = self.max_unpool(x, indices=pool_indices, output_size=unpool_shape)\n        # print(\'concat_net.size():\', concat_net.size())\n        # print(\'x.size():\', x.size())\n        x = torch.cat([concat_net, x], 1)\n        # x = x + concat_net\n        x = self.conv1(x)\n        x = self.conv2(x)\n        x = self.conv3(x)\n        return x\n\nclass residualBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self, in_channels, out_channels, stride=1, downsample=None):\n        super(residualBlock, self).__init__()\n\n        self.convbnrelu1 = conv2DBatchNormRelu(in_channels, out_channels, 3,  stride, 1, bias=False)\n        self.convbn2 = conv2DBatchNorm(out_channels, out_channels, 3, 1, 1, bias=False)\n        self.downsample = downsample\n        self.stride = stride\n        self.relu = nn.ReLU(inplace=True)\n\n    def forward(self, x):\n        residual = x\n\n        out = self.convbnrelu1(x)\n        out = self.convbn2(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n        return out\n\nclass linknetUp(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super(linknetUp, self).__init__()\n\n        # B, 2C, H, W -> B, C/2, H, W\n        self.convbnrelu1 = conv2DBatchNormRelu(in_channels, out_channels/2, kernel_size=1, stride=1, padding=1)\n\n        # B, C/2, H, W -> B, C/2, H, W\n        self.deconvbnrelu2 = deconv2DBatchNormRelu(out_channels/2, out_channels/2, kernel_size=3,  stride=2, padding=0,)\n\n        # B, C/2, H, W -> B, C, H, W\n        self.convbnrelu3 = conv2DBatchNormRelu(out_channels/2, out_channels, kernel_size=1, stride=1, padding=1)\n\n    def forward(self, x):\n        x = self.convbnrelu1(x)\n        x = self.deconvbnrelu2(x)\n        x = self.convbnrelu3(x)\n        return x\n\nclass deconv2DBatchNormRelu(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, bias=True):\n        super(deconv2DBatchNormRelu, self).__init__()\n\n        self.dcbr_unit = nn.Sequential(nn.ConvTranspose2d(int(in_channels), int(out_channels), kernel_size=kernel_size,\n                                                padding=padding, stride=stride, bias=bias),\n                                 nn.BatchNorm2d(int(out_channels)),\n                                 nn.ReLU(inplace=True),)\n\n    def forward(self, inputs):\n        outputs = self.dcbr_unit(inputs)\n        return outputs\n\n\nclass bottleNeckIdentifyPSP(nn.Module):\n    def __init__(self, in_channels, mid_channels, stride, dilation=1):\n        super(bottleNeckIdentifyPSP, self).__init__()\n\n        self.cbr1 = conv2DBatchNormRelu(in_channels, mid_channels, 1, 1, 0, bias=False)\n        if dilation > 1:\n            self.cbr2 = conv2DBatchNormRelu(mid_channels, mid_channels, 3, 1,\n                                            padding=dilation, bias=False,\n                                            dilation=dilation)\n        else:\n            self.cbr2 = conv2DBatchNormRelu(mid_channels, mid_channels, 3,\n                                            stride=1, padding=1,\n                                            bias=False, dilation=1)\n        self.cb3 = conv2DBatchNorm(mid_channels, in_channels, 1, 1, 0)\n\n    def forward(self, x):\n        residual = x\n        x = self.cb3(self.cbr2(self.cbr1(x)))\n        return F.relu(x + residual, inplace=True)\n\nclass bottleNeckPSP(nn.Module):\n    def __init__(self, in_channels, mid_channels, out_channels,\n                 stride, dilation=1):\n        super(bottleNeckPSP, self).__init__()\n\n        self.cbr1 = conv2DBatchNormRelu(in_channels, mid_channels, 1, 1, 0, bias=False)\n        if dilation > 1:\n            self.cbr2 = conv2DBatchNormRelu(mid_channels, mid_channels, 3, 1,\n                                            padding=dilation, bias=False,\n                                            dilation=dilation)\n        else:\n            self.cbr2 = conv2DBatchNormRelu(mid_channels, mid_channels, 3,\n                                            stride=stride, padding=1,\n                                            bias=False, dilation=1)\n        self.cb3 = conv2DBatchNorm(mid_channels, out_channels, 1, 1, 0)\n        self.cb4 = conv2DBatchNorm(in_channels, out_channels, 1, stride, 0)\n\n    def forward(self, x):\n        conv = self.cb3(self.cbr2(self.cbr1(x)))\n        residual = self.cb4(x)\n        return F.relu(conv + residual, inplace=True)\n\n\nclass residualBlockPSP(nn.Module):\n    \n    def __init__(self, n_blocks, in_channels, mid_channels, out_channels, stride, dilation=1):\n        super(residualBlockPSP, self).__init__()\n\n        if dilation > 1:\n            stride = 1\n\n        layers = [bottleNeckPSP(in_channels, mid_channels, out_channels, stride, dilation)]\n        for i in range(n_blocks):\n            layers.append(bottleNeckIdentifyPSP(out_channels, mid_channels, stride, dilation))\n\n        self.layers = nn.Sequential(*layers)\n\n    def forward(self, x):\n        return self.layers(x)\n\nclass pyramidPooling(nn.Module):\n    """"""\n    \xe9\x87\x91\xe5\xad\x97\xe5\xa1\x94\xe6\xb1\xa0\xe5\x8c\x96\xe6\xa8\xa1\xe5\x9d\x97\xef\xbc\x8c\xe4\xb8\xbb\xe8\xa6\x81\xe6\x98\xaf\xe5\xb0\x86\xe7\x89\xb9\xe5\xbe\x81\xe5\x9b\xbe\xe9\x80\x9a\xe8\xbf\x87\xe4\xb8\x8d\xe5\x90\x8c\xe7\x9a\x84pool\xe6\x9e\x84\xe9\x80\xa0\xe6\xb1\xa0\xe5\x8c\x96\xe8\xbe\x93\xe5\x87\xba\xef\xbc\x8c\xe6\x9c\x80\xe5\x90\x8e\xe7\x9a\x84\xe8\xbe\x93\xe5\x87\xba\xe6\x98\xaf\xe5\x92\x8c\xe8\xbe\x93\xe5\x85\xa5\xe7\x9b\xb8\xe5\x90\x8c\xe5\x88\x86\xe8\xbe\xa8\xe7\x8e\x87\xe5\xa4\xa7\xe5\xb0\x8f\xe7\x9a\x84concat\xe7\x9a\x84\xe7\x89\xb9\xe5\xbe\x81\xe5\x9b\xbe\n    """"""\n\n    def __init__(self, in_channels, pool_sizes):\n        super(pyramidPooling, self).__init__()\n\n        self.paths = []\n\n        for i in range(len(pool_sizes)):\n            # 1*1\xe5\x8d\xb7\xe7\xa7\xaf\xe8\xbe\x93\xe5\x87\xba\xe4\xb8\xbain_channels/level\xe7\x9a\x84\n            self.paths.append(conv2DBatchNormRelu(in_channels, int(in_channels / len(pool_sizes)), 1, 1, 0, bias=False))\n\n        self.path_module_list = nn.ModuleList(self.paths)\n        self.pool_sizes = pool_sizes\n\n    def forward(self, x):\n        # \xe8\xbe\x93\xe5\x87\xba\xe5\xb7\xb2\xe7\xbb\x8f\xe9\xbb\x98\xe8\xae\xa4\xe5\x8c\x85\xe6\x8b\xacx\n        output_slices = [x]\n        # \xe8\xbe\x93\xe5\x87\xba\xe5\xae\xbd\xe5\xba\xa6\xe9\x9c\x80\xe8\xa6\x81\xe5\x92\x8cx\xe7\x9b\xb8\xe5\x90\x8c\n        h, w = x.view()[2:]\n\n        for module, pool_size in zip(self.path_module_list, self.pool_sizes):\n            # \xe9\x87\x91\xe5\xad\x97\xe5\xa1\x94\xe6\xb1\xa0\xe5\x8c\x96\xe6\x93\x8d\xe4\xbd\x9c\xef\xbc\x8c\xe5\x88\x86\xe5\x88\xab\xe5\xaf\xb9\xe6\xaf\x8f\xe4\xb8\x80\xe4\xb8\xaamodule\xe5\x92\x8csizes\xe8\xbf\x9b\xe8\xa1\x8c\xe6\x93\x8d\xe4\xbd\x9c\n            # \xe9\xa6\x96\xe5\x85\x88\xe4\xbd\xbf\xe7\x94\xa8\xe5\xb9\xb3\xe5\x9d\x87\xe6\xb1\xa0\xe5\x8c\x96\xe5\xb1\x82\xe6\x93\x8d\xe4\xbd\x9c\xe8\x8e\xb7\xe5\xbe\x97\xe7\x9b\xb8\xe5\xba\x94size\xe7\x9a\x84\xe6\xb1\xa0\xe5\x8c\x96\xe7\x89\xb9\xe5\xbe\x81\xe5\x9b\xbe\n            out = F.avg_pool2d(x, pool_size, stride=1, padding=0)\n            # \xe9\x80\x9a\xe8\xbf\x87module\xe5\x87\x8f\xe5\xb0\x8f\xe7\xbb\xb4\xe5\xba\xa6\xe9\x99\x8d\xe4\xbd\x8e\xe8\xae\xa1\xe7\xae\x97\xe9\x87\x8f\n            out = module(out)\n            # \xe7\x84\xb6\xe5\x90\x8e\xe4\xb8\x8a\xe9\x87\x87\xe6\xa0\xb7\xe5\x8d\xb3\xe5\x8f\xaf\n            out = F.upsample(out, size=(h,w), mode=\'bilinear\')\n            output_slices.append(out)\n\n        # \xe6\x9c\x80\xe5\x90\x8e\xe6\x8a\x8a[x, pool1_up, pool2_up, pool3_up, pool4_up] concat\xe5\x8d\xb3\xe5\x8f\xaf\n        return torch.cat(output_slices, dim=1)\n\n\nclass AlignedResInception(nn.Module):\n    """"""\n    Aligned\xe6\xae\x8b\xe5\xb7\xaeInception\xe7\xbb\x93\xe6\x9e\x84\n    """"""\n    def __init__(self, in_planes, stride=1):\n        super(AlignedResInception, self).__init__()\n        self.relu = nn.ReLU(inplace=True)\n        self.b1 = nn.Sequential(\n            nn.Conv2d(in_planes, in_planes//4, kernel_size=1, stride=1),\n            nn.BatchNorm2d(in_planes//4),\n            nn.ReLU(True),\n            nn.Conv2d(in_planes//4, in_planes//4, kernel_size=3, stride=1, padding=1),\n        )\n\n        self.b2 = nn.Sequential(\n            nn.Conv2d(in_planes, in_planes//8, kernel_size=1, stride=1),\n            nn.BatchNorm2d(in_planes//8),\n            nn.ReLU(True),\n            nn.Conv2d(in_planes//8, in_planes//8, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(in_planes//8),\n            nn.ReLU(True),\n            nn.Conv2d(in_planes//8, in_planes//8, kernel_size=3, stride=1, padding=1),\n        )\n\n        self.b3 = nn.Sequential(\n            nn.BatchNorm2d(in_planes//8*3),\n            nn.ReLU(True),\n        )\n\n        self.b4 = nn.Sequential(\n            nn.Conv2d(in_planes//8*3, in_planes, kernel_size=1, stride=stride),\n            nn.BatchNorm2d(in_planes),\n            nn.ReLU(True),\n        )\n\n        self.downsample = None\n        if stride>1:\n            self.downsample = nn.Sequential(\n                nn.Conv2d(in_planes, in_planes, kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(in_planes),\n            )\n\n    def forward(self, x):\n        y1 = self.b1(x)\n        # print(\'y1.size():\', y1.size())\n        y2 = self.b2(x)\n        # print(\'y2.size():\', y2.size())\n        y3 = torch.cat([y1,y2], 1)\n        y3 = self.b3(y3)\n        # print(\'y3.size():\', y3.size())\n        out = self.b4(y3)\n        # print(\'out.size():\', out.size())\n        if self.downsample is not None:\n            out = out + self.downsample(x)\n        else:\n            out = out + x\n        out = self.relu(out)\n        return out\n\nclass Inception(nn.Module):\n    def __init__(self, in_planes, n1x1, n3x3red, n3x3, n5x5red, n5x5, pool_planes):\n        super(Inception, self).__init__()\n        # 1x1 conv branch\n        self.b1 = nn.Sequential(\n            nn.Conv2d(in_planes, n1x1, kernel_size=1),\n            nn.BatchNorm2d(n1x1),\n            nn.ReLU(True),\n        )\n\n        # 1x1 conv -> 3x3 conv branch\n        self.b2 = nn.Sequential(\n            nn.Conv2d(in_planes, n3x3red, kernel_size=1),\n            nn.BatchNorm2d(n3x3red),\n            nn.ReLU(True),\n            nn.Conv2d(n3x3red, n3x3, kernel_size=3, padding=1),\n            nn.BatchNorm2d(n3x3),\n            nn.ReLU(True),\n        )\n\n        # 1x1 conv -> 5x5 conv branch\n        self.b3 = nn.Sequential(\n            nn.Conv2d(in_planes, n5x5red, kernel_size=1),\n            nn.BatchNorm2d(n5x5red),\n            nn.ReLU(True),\n            nn.Conv2d(n5x5red, n5x5, kernel_size=3, padding=1),\n            nn.BatchNorm2d(n5x5),\n            nn.ReLU(True),\n            nn.Conv2d(n5x5, n5x5, kernel_size=3, padding=1),\n            nn.BatchNorm2d(n5x5),\n            nn.ReLU(True),\n        )\n\n        # 3x3 pool -> 1x1 conv branch\n        self.b4 = nn.Sequential(\n            nn.MaxPool2d(3, stride=1, padding=1),\n            nn.Conv2d(in_planes, pool_planes, kernel_size=1),\n            nn.BatchNorm2d(pool_planes),\n            nn.ReLU(True),\n        )\n\n    def forward(self, x):\n        y1 = self.b1(x)\n        y2 = self.b2(x)\n        y3 = self.b3(x)\n        y4 = self.b4(x)\n        return torch.cat([y1,y2,y3,y4], 1)\n\nclass ResInception(nn.Module):\n    def __init__(self, in_planes, n1x1, n3x3red, n3x3, n5x5red, n5x5, pool_planes, stride=1):\n        super(ResInception, self).__init__()\n        self.relu = nn.ReLU(inplace=True)\n        # 1x1 conv branch\n        self.b1 = nn.Sequential(\n            nn.Conv2d(in_planes, n1x1, kernel_size=1, stride=stride),\n            nn.BatchNorm2d(n1x1),\n            nn.ReLU(True),\n        )\n\n        # 1x1 conv -> 3x3 conv branch\n        self.b2 = nn.Sequential(\n            nn.Conv2d(in_planes, n3x3red, kernel_size=1, stride=stride),\n            nn.BatchNorm2d(n3x3red),\n            nn.ReLU(True),\n            nn.Conv2d(n3x3red, n3x3, kernel_size=3, padding=1),\n            nn.BatchNorm2d(n3x3),\n            nn.ReLU(True),\n        )\n\n        # 1x1 conv -> 5x5 conv branch\n        self.b3 = nn.Sequential(\n            nn.Conv2d(in_planes, n5x5red, kernel_size=1, stride=stride),\n            nn.BatchNorm2d(n5x5red),\n            nn.ReLU(True),\n            nn.Conv2d(n5x5red, n5x5, kernel_size=3, padding=1),\n            nn.BatchNorm2d(n5x5),\n            nn.ReLU(True),\n            nn.Conv2d(n5x5, n5x5, kernel_size=3, padding=1),\n            nn.BatchNorm2d(n5x5),\n            nn.ReLU(True),\n        )\n\n        # 3x3 pool -> 1x1 conv branch\n        self.b4 = nn.Sequential(\n            nn.MaxPool2d(3, stride=stride, padding=1),\n            nn.Conv2d(in_planes, pool_planes, kernel_size=1),\n            nn.BatchNorm2d(pool_planes),\n            nn.ReLU(True),\n        )\n\n        self.downsample = None\n        if stride>1:\n            self.downsample = nn.Sequential(\n                nn.Conv2d(in_planes, in_planes, kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(in_planes),\n            )\n\n    def forward(self, x):\n        y1 = self.b1(x)\n        y2 = self.b2(x)\n        y3 = self.b3(x)\n        y4 = self.b4(x)\n        out = torch.cat([y1,y2,y3,y4], 1)\n        if self.downsample is not None:\n            out = out + self.downsample(x)\n        else:\n            out = out + x\n        out = self.relu(out)\n        return out\n\n\nclass CascadeResInception(nn.Module):\n    def __init__(self):\n        super(CascadeResInception, self).__init__()\n        self.relu = nn.ReLU(inplace=True)\n        self.res1 = ResInception(512, 128, 128, 256, 24,  64,  64, stride=2)\n        self.res2 = ResInception(512, 128, 128, 256, 24,  64,  64, stride=7)\n        self.res3 = ResInception(512, 128, 128, 256, 24,  64,  64, stride=14)\n\n    def forward(self, x):\n        y1 = self.res1(x)\n        y2 = self.res2(x)\n        y3 = self.res3(x)\n        # print(\'y1:\', y1.size())\n        # print(\'y2:\', y2.size())\n        # print(\'y3:\', y3.size())\n        y1 = F.upsample_bilinear(y1, x.size()[2:])\n        y2 = F.upsample_bilinear(y2, x.size()[2:])\n        y3 = F.upsample_bilinear(y3, x.size()[2:])\n        out = x + y1 + y2 + y3\n        out = self.relu(out)\n        return out\n\nclass CascadeAlignedResInception(nn.Module):\n    def __init__(self, in_planes):\n        super(CascadeAlignedResInception, self).__init__()\n        self.relu = nn.ReLU(inplace=True)\n        self.res1 = AlignedResInception(in_planes=in_planes, stride=2)\n        self.res2 = AlignedResInception(in_planes=in_planes, stride=7)\n        self.res3 = AlignedResInception(in_planes=in_planes, stride=14)\n\n    def forward(self, x):\n        y1 = self.res1(x)\n        y2 = self.res2(x)\n        y3 = self.res3(x)\n        # print(\'y1:\', y1.size())\n        # print(\'y2:\', y2.size())\n        # print(\'y3:\', y3.size())\n        y1 = F.upsample_bilinear(y1, x.size()[2:])\n        y2 = F.upsample_bilinear(y2, x.size()[2:])\n        y3 = F.upsample_bilinear(y3, x.size()[2:])\n        out = x + y1 + y2 + y3\n        out = self.relu(out)\n        return out\n\nclass ASPP_Classifier_Module(nn.Module):\n\n    def __init__(self, dilation_series, padding_series, n_classes, in_channels=2048):\n        super(ASPP_Classifier_Module, self).__init__()\n        self.conv2d_list = nn.ModuleList()\n        for dilation, padding in zip(dilation_series, padding_series):\n            self.conv2d_list.append(nn.Conv2d(in_channels, n_classes, kernel_size=3, stride=1, padding=padding, dilation=dilation, bias = True))\n\n        for m in self.conv2d_list:\n            m.weight.data.normal_(0, 0.01)\n\n    def forward(self, x):\n        out = self.conv2d_list[0](x)\n        for i in range(len(self.conv2d_list)-1):\n            out += self.conv2d_list[i+1](x)\n        return out\n\nclass IBN(nn.Module):\n    def __init__(self, planes):\n        super(IBN, self).__init__()\n        half1 = int(planes / 2)\n        self.half = half1\n        half2 = planes - half1\n        self.IN = nn.InstanceNorm2d(half1, affine=True)\n        self.BN = nn.BatchNorm2d(half2)\n\n    def forward(self, x):\n        split = torch.split(x, self.half, 1)\n        out1 = self.IN(split[0].contiguous())\n        out2 = self.BN(split[1].contiguous())\n        out = torch.cat((out1, out2), 1)\n        return out\n\nclass conv2DGroupNormRelu(nn.Module):\n    def __init__(self, in_channels, n_filters, kernel_size, stride, padding, bias=True, dilation=1, n_groups=16):\n        super(conv2DGroupNormRelu, self).__init__()\n\n        conv_mod = nn.Conv2d(int(in_channels), int(n_filters), kernel_size=kernel_size, padding=padding, stride=stride, bias=bias, dilation=dilation)\n\n        self.cgr_unit = nn.Sequential(conv_mod, nn.GroupNorm(n_groups, int(n_filters)), nn.ReLU(inplace=True))\n\n    def forward(self, inputs):\n        outputs = self.cgr_unit(inputs)\n        return outputs\n\nclass conv2DGroupNorm(nn.Module):\n    def __init__(self, in_channels, n_filters, kernel_size, stride, padding, bias=True, dilation=1, n_groups=16):\n        super(conv2DGroupNorm, self).__init__()\n\n        conv_mod = nn.Conv2d(int(in_channels), int(n_filters), kernel_size=kernel_size, padding=padding, stride=stride, bias=bias, dilation=dilation)\n\n        self.cgr_unit = nn.Sequential(conv_mod, nn.GroupNorm(n_groups, int(n_filters)))\n\n    def forward(self, inputs):\n        outputs = self.cgr_unit(inputs)\n        return outputs\n'"
semseg/netloader/__init__.py,0,b'#!/usr/bin/python\n# -*- coding: UTF-8 -*-\n'
semseg/netloader/resnet.py,7,"b'#!/usr/bin/python\n# -*- coding: UTF-8 -*-\nimport torch.nn as nn\nimport math\nimport torch.utils.model_zoo as model_zoo\n\n\n__all__ = [\'ResNet\', \'resnet18\', \'resnet34\', \'resnet50\', \'resnet101\',\n           \'resnet152\']\n\n\nmodel_urls = {\n    \'resnet18\': \'https://download.pytorch.org/models/resnet18-5c106cde.pth\',\n    \'resnet34\': \'https://download.pytorch.org/models/resnet34-333f7ec4.pth\',\n    \'resnet50\': \'https://download.pytorch.org/models/resnet50-19c8e357.pth\',\n    \'resnet101\': \'https://download.pytorch.org/models/resnet101-5d3b4d8f.pth\',\n    \'resnet152\': \'https://download.pytorch.org/models/resnet152-b121ed2d.pth\',\n}\n\n\ndef conv3x3(in_planes, out_planes, stride=1):\n    """"""3x3 convolution with padding""""""\n    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n                     padding=1, bias=False)\n\n\nclass BasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None):\n        super(BasicBlock, self).__init__()\n        self.conv1 = conv3x3(inplanes, planes, stride)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = conv3x3(planes, planes)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\n\nclass Bottleneck(nn.Module):\n    expansion = 4\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None):\n        super(Bottleneck, self).__init__()\n        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,\n                               padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.conv3 = nn.Conv2d(planes, planes * self.expansion, kernel_size=1, bias=False)\n        self.bn3 = nn.BatchNorm2d(planes * self.expansion)\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\n\nclass ResNet(nn.Module):\n\n    def __init__(self, block, layers, num_classes=1000):\n        self.inplanes = 64\n        super(ResNet, self).__init__()\n        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n        self.bn1 = nn.BatchNorm2d(64)\n        self.relu = nn.ReLU(inplace=True)\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n        self.layer1 = self._make_layer(block, 64, layers[0])\n        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n        self.avgpool = nn.AvgPool2d(7, stride=1)\n        self.fc = nn.Linear(512 * block.expansion, num_classes)\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n                m.weight.data.normal_(0, math.sqrt(2. / n))\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n\n    def _make_layer(self, block, planes, blocks, stride=1):\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                nn.Conv2d(self.inplanes, planes * block.expansion, kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(planes * block.expansion),\n            )\n\n        layers = []\n        layers.append(block(self.inplanes, planes, stride, downsample))\n        self.inplanes = planes * block.expansion\n        for i in range(1, blocks):\n            layers.append(block(self.inplanes, planes))\n\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n\n        x = self.avgpool(x)\n        x = x.view(x.size(0), -1)\n        x = self.fc(x)\n\n        return x\n\n\ndef resnet18(pretrained=False, **kwargs):\n    """"""Constructs a ResNet-18 model.\n\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = ResNet(BasicBlock, [2, 2, 2, 2], **kwargs)\n    if pretrained:\n        model.load_state_dict(model_zoo.load_url(model_urls[\'resnet18\']))\n    return model\n\n\ndef resnet34(pretrained=False, **kwargs):\n    """"""Constructs a ResNet-34 model.\n\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = ResNet(BasicBlock, [3, 4, 6, 3], **kwargs)\n    if pretrained:\n        model.load_state_dict(model_zoo.load_url(model_urls[\'resnet34\']))\n    return model\n\n\ndef resnet50(pretrained=False, **kwargs):\n    """"""Constructs a ResNet-50 model.\n\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = ResNet(Bottleneck, [3, 4, 6, 3], **kwargs)\n    if pretrained:\n        model.load_state_dict(model_zoo.load_url(model_urls[\'resnet50\']))\n    return model\n\n\ndef resnet101(pretrained=False, **kwargs):\n    """"""Constructs a ResNet-101 model.\n\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = ResNet(Bottleneck, [3, 4, 23, 3], **kwargs)\n    if pretrained:\n        model.load_state_dict(model_zoo.load_url(model_urls[\'resnet101\']))\n    return model\n\n\ndef resnet152(pretrained=False, **kwargs):\n    """"""Constructs a ResNet-152 model.\n\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = ResNet(Bottleneck, [3, 8, 36, 3], **kwargs)\n    if pretrained:\n        model.load_state_dict(model_zoo.load_url(model_urls[\'resnet152\']))\n    return model\n\n'"
semseg/netloader/resnet_ibn_a.py,12,"b'# -*- coding: utf-8 -*-\n\nimport torch\nimport torch.nn as nn\nimport math\nimport torch.utils.model_zoo as model_zoo\nimport os\n\n__all__ = [\'ResNet\', \'resnet50_ibn_a\', \'resnet101_ibn_a\',\n           \'resnet152_ibn_a\']\n\nmodel_urls = {\n    \'resnet50\': \'https://download.pytorch.org/models/resnet50-19c8e357.pth\',\n    \'resnet101\': \'https://download.pytorch.org/models/resnet101-5d3b4d8f.pth\',\n    \'resnet152\': \'https://download.pytorch.org/models/resnet152-b121ed2d.pth\',\n}\n\n\ndef conv3x3(in_planes, out_planes, stride=1):\n    ""3x3 convolution with padding""\n    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n                     padding=1, bias=False)\n\n\nclass BasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None):\n        super(BasicBlock, self).__init__()\n        self.conv1 = conv3x3(inplanes, planes, stride)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = conv3x3(planes, planes)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\n\nclass IBN(nn.Module):\n    def __init__(self, planes):\n        super(IBN, self).__init__()\n        half1 = int(planes / 2)\n        self.half = half1\n        half2 = planes - half1\n        self.IN = nn.InstanceNorm2d(half1, affine=True)\n        self.BN = nn.BatchNorm2d(half2)\n\n    def forward(self, x):\n        split = torch.split(x, self.half, 1)\n        out1 = self.IN(split[0].contiguous())\n        out2 = self.BN(split[1].contiguous())\n        out = torch.cat((out1, out2), 1)\n        return out\n\n\nclass Bottleneck(nn.Module):\n    expansion = 4\n\n    def __init__(self, inplanes, planes, ibn=False, stride=1, downsample=None):\n        super(Bottleneck, self).__init__()\n        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n        if ibn:\n            self.bn1 = IBN(planes)\n        else:\n            self.bn1 = nn.BatchNorm2d(planes)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,\n                               padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.conv3 = nn.Conv2d(planes, planes * self.expansion, kernel_size=1, bias=False)\n        self.bn3 = nn.BatchNorm2d(planes * self.expansion)\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\n\nclass ResNet(nn.Module):\n\n    def __init__(self, block, layers, num_classes=1000):\n        scale = 64\n        self.inplanes = scale\n        super(ResNet, self).__init__()\n        self.conv1 = nn.Conv2d(3, scale, kernel_size=7, stride=2, padding=3,\n                               bias=False)\n        self.bn1 = nn.BatchNorm2d(scale)\n        self.relu = nn.ReLU(inplace=True)\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n        self.layer1 = self._make_layer(block, scale, layers[0])\n        self.layer2 = self._make_layer(block, scale * 2, layers[1], stride=2)\n        self.layer3 = self._make_layer(block, scale * 4, layers[2], stride=2)\n        self.layer4 = self._make_layer(block, scale * 8, layers[3], stride=2)\n        self.avgpool = nn.AvgPool2d(7)\n        self.fc = nn.Linear(scale * 8 * block.expansion, num_classes)\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n                m.weight.data.normal_(0, math.sqrt(2. / n))\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n            elif isinstance(m, nn.InstanceNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n\n    def _make_layer(self, block, planes, blocks, stride=1):\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                nn.Conv2d(self.inplanes, planes * block.expansion,\n                          kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(planes * block.expansion),\n            )\n\n        layers = []\n        ibn = True\n        if planes == 512:\n            ibn = False\n        layers.append(block(self.inplanes, planes, ibn, stride, downsample))\n        self.inplanes = planes * block.expansion\n        for i in range(1, blocks):\n            layers.append(block(self.inplanes, planes, ibn))\n\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n\n        x = self.avgpool(x)\n        x = x.view(x.size(0), -1)\n        x = self.fc(x)\n\n        return x\n\n\ndef resnet50_ibn_a(pretrained=False, **kwargs):\n    """"""Constructs a ResNet-50 model.\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = ResNet(Bottleneck, [3, 4, 6, 3], **kwargs)\n    if pretrained:\n        model.load_state_dict(model_zoo.load_url(model_urls[\'resnet50\']))\n    return model\n\n\ndef resnet101_ibn_a(pretrained=False, **kwargs):\n    """"""Constructs a ResNet-101 model.\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = ResNet(Bottleneck, [3, 4, 23, 3], **kwargs)\n    if pretrained:\n        model.load_state_dict(model_zoo.load_url(model_urls[\'resnet101\']))\n    return model\n\n\ndef resnet152_ibn_a(pretrained=False, **kwargs):\n    """"""Constructs a ResNet-152 model.\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = ResNet(Bottleneck, [3, 8, 36, 3], **kwargs)\n    if pretrained:\n        model.load_state_dict(model_zoo.load_url(model_urls[\'resnet152\']))\n    return model\n\nif __name__ == \'__main__\':\n    # import torch._utils\n    #\n    # try:\n    #     torch._utils._rebuild_tensor_v2\n    # except AttributeError:\n    #     def _rebuild_tensor_v2(storage, storage_offset, size, stride, requires_grad, backward_hooks):\n    #         tensor = torch._utils._rebuild_tensor(storage, storage_offset, size, stride)\n    #         tensor.requires_grad = requires_grad\n    #         tensor._backward_hooks = backward_hooks\n    #         return tensor\n    #\n    #\n    #     torch._utils._rebuild_tensor_v2 = _rebuild_tensor_v2\n\n    model = resnet50_ibn_a()\n    model_checkpoint_path = os.path.expanduser(\'~/.torch/models/resnet50_ibn_a.pth.tar\')\n    if os.path.exists(model_checkpoint_path):\n        model_checkpoint = torch.load(model_checkpoint_path, map_location=\'cpu\')\n        model_dict = model.state_dict()\n        pretrained_dict = model_checkpoint[\'state_dict\']\n        # new_dict = {k: v for k, v in pretrained_dict.items() if k in model_dict.keys()}\n        new_dict = {}\n        for k, v in pretrained_dict.items():\n            new_k = k[k.find(\'.\')+1:]\n            new_v = v\n            new_dict[new_k] = new_v\n\n        # print(model_dict.keys())\n        # print(pretrained_dict.keys())\n        # print(new_dict)\n        model_dict.update(new_dict)\n        model.load_state_dict(model_dict)\n        model.eval()\n\n\n'"
semseg/netloader/resnet_ibn_b.py,6,"b'# -*- coding: utf-8 -*-\nimport torch\nimport torch.nn as nn\nimport math\nimport torch.utils.model_zoo as model_zoo\nimport os\n\n__all__ = [\'ResNet\', \'resnet50_ibn_b\', \'resnet101_ibn_b\',\n           \'resnet152_ibn_b\']\n\nmodel_urls = {\n    \'resnet50\': \'https://download.pytorch.org/models/resnet50-19c8e357.pth\',\n    \'resnet101\': \'https://download.pytorch.org/models/resnet101-5d3b4d8f.pth\',\n    \'resnet152\': \'https://download.pytorch.org/models/resnet152-b121ed2d.pth\',\n}\n\n\ndef conv3x3(in_planes, out_planes, stride=1):\n    ""3x3 convolution with padding""\n    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n                     padding=1, bias=False)\n\n\nclass BasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None):\n        super(BasicBlock, self).__init__()\n        self.conv1 = conv3x3(inplanes, planes, stride)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = conv3x3(planes, planes)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\n\nclass Bottleneck(nn.Module):\n    expansion = 4\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None, IN=False):\n        super(Bottleneck, self).__init__()\n        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,\n                               padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.conv3 = nn.Conv2d(planes, planes * self.expansion, kernel_size=1, bias=False)\n        self.bn3 = nn.BatchNorm2d(planes * self.expansion)\n        self.IN = None\n        if IN:\n            self.IN = nn.InstanceNorm2d(planes * 4, affine=True)\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        if self.IN is not None:\n            out = self.IN(out)\n        out = self.relu(out)\n\n        return out\n\n\nclass ResNet(nn.Module):\n\n    def __init__(self, block, layers, num_classes=1000):\n        scale = 64\n        self.inplanes = scale\n        super(ResNet, self).__init__()\n        self.conv1 = nn.Conv2d(3, scale, kernel_size=7, stride=2, padding=3,\n                               bias=False)\n        self.bn1 = nn.InstanceNorm2d(scale, affine=True)\n        self.relu = nn.ReLU(inplace=True)\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n        self.layer1 = self._make_layer(block, scale, layers[0], stride=1, IN=True)\n        self.layer2 = self._make_layer(block, scale * 2, layers[1], stride=2, IN=True)\n        self.layer3 = self._make_layer(block, scale * 4, layers[2], stride=2)\n        self.layer4 = self._make_layer(block, scale * 8, layers[3], stride=2)\n        self.avgpool = nn.AvgPool2d(7)\n        self.fc = nn.Linear(scale * 8 * block.expansion, num_classes)\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n                m.weight.data.normal_(0, math.sqrt(2. / n))\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n            elif isinstance(m, nn.InstanceNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n\n    def _make_layer(self, block, planes, blocks, stride=1, IN=False):\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                nn.Conv2d(self.inplanes, planes * block.expansion,\n                          kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(planes * block.expansion),\n            )\n\n        layers = []\n        layers.append(block(self.inplanes, planes, stride, downsample))\n        self.inplanes = planes * block.expansion\n        for i in range(1, blocks - 1):\n            layers.append(block(self.inplanes, planes))\n        layers.append(block(self.inplanes, planes, IN=IN))\n\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n\n        x = self.avgpool(x)\n        x = x.view(x.size(0), -1)\n        x = self.fc(x)\n\n        return x\n\n\ndef resnet50_ibn_b(pretrained=False, **kwargs):\n    """"""Constructs a ResNet-50 model.\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = ResNet(Bottleneck, [3, 4, 6, 3], **kwargs)\n    if pretrained:\n        model.load_state_dict(model_zoo.load_url(model_urls[\'resnet50\']))\n    return model\n\n\ndef resnet101_ibn_b(pretrained=False, **kwargs):\n    """"""Constructs a ResNet-101 model.\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = ResNet(Bottleneck, [3, 4, 23, 3], **kwargs)\n    if pretrained:\n        model.load_state_dict(model_zoo.load_url(model_urls[\'resnet101\']))\n    return model\n\n\ndef resnet152_ibn_b(pretrained=False, **kwargs):\n    """"""Constructs a ResNet-152 model.\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = ResNet(Bottleneck, [3, 8, 36, 3], **kwargs)\n    if pretrained:\n        model.load_state_dict(model_zoo.load_url(model_urls[\'resnet152\']))\n    return model\n\nif __name__ == \'__main__\':\n    model = resnet50_ibn_b()\n    model_weight_path = os.path.expanduser(\'~/.torch/models/resnet50_ibn_b.pth.tar\')\n    if os.path.exists(model_weight_path):\n        model_weight = torch.load(model_weight_path, map_location=\'cpu\')\n        model.load_state_dict(model_weight[\'state_dict\'])\n'"
semseg/utils/__init__.py,0,b'# -*- coding: utf-8 -*-\n'
semseg/utils/flops_benchmark.py,8,"b'# -*- coding: utf-8 -*-\n# code is from [flops_benchmark.py](https://github.com/warmspringwinds/pytorch-segmentation-detection/blob/master/pytorch_segmentation_detection/utils/flops_benchmark.py)\n\nimport torch\n\n\n# ---- Public functions\n\ndef add_flops_counting_methods(net_main_module):\n    """"""Adds flops counting functions to an existing model. After that\n    the flops count should be activated and the model should be run on an input\n    image.\n\n    Example:\n\n    fcn = add_flops_counting_methods(fcn)\n    fcn = fcn.cuda().train()\n    fcn.start_flops_count()\n\n    _ = fcn(batch)\n\n    fcn.compute_average_flops_cost() / 1e9 / 2 # Result in GFLOPs per image in batch\n\n    Important: dividing by 2 only works for resnet models -- see below for the details\n    of flops computation.\n\n    Attention: we are counting multiply-add as two flops in this work, because in\n    most resnet models convolutions are bias-free (BN layers act as bias there)\n    and it makes sense to count muliply and add as separate flops therefore.\n    This is why in the above example we divide by 2 in order to be consistent with\n    most modern benchmarks. For example in ""Spatially Adaptive Computatin Time for Residual\n    Networks"" by Figurnov et al multiply-add was counted as two flops.\n\n    This module computes the average flops which is necessary for dynamic networks which\n    have different number of executed layers. For static networks it is enough to run the network\n    once and get statistics (above example).\n\n    Implementation:\n    The module works by adding batch_count to the main module which tracks the sum\n    of all batch sizes that were run through the network.\n\n    Also each convolutional layer of the network tracks the overall number of flops\n    performed.\n\n    The parameters are updated with the help of registered hook-functions which\n    are being called each time the respective layer is executed.\n\n    Parameters\n    ----------\n    net_main_module : torch.nn.Module\n        Main module containing network\n\n    Returns\n    -------\n    net_main_module : torch.nn.Module\n        Updated main module with new methods/attributes that are used\n        to compute flops.\n    """"""\n\n    # adding additional methods to the existing module object,\n    # this is done this way so that each function has access to self object\n    net_main_module.start_flops_count = start_flops_count.__get__(net_main_module)\n    net_main_module.stop_flops_count = stop_flops_count.__get__(net_main_module)\n    net_main_module.reset_flops_count = reset_flops_count.__get__(net_main_module)\n    net_main_module.compute_average_flops_cost = compute_average_flops_cost.__get__(net_main_module)\n\n    net_main_module.reset_flops_count()\n\n    # Adding varialbles necessary for masked flops computation\n    net_main_module.apply(add_flops_mask_variable_or_reset)\n\n    return net_main_module\n\n\ndef compute_average_flops_cost(self):\n    """"""\n    A method that will be available after add_flops_counting_methods() is called\n    on a desired net object.\n\n    Returns current mean flops consumption per image.\n\n    """"""\n\n    batches_count = self.__batch_counter__\n\n    flops_sum = 0\n\n    for module in self.modules():\n\n        if isinstance(module, torch.nn.Conv2d):\n            flops_sum += module.__flops__\n\n    return flops_sum / batches_count\n\n\ndef start_flops_count(self):\n    """"""\n    A method that will be available after add_flops_counting_methods() is called\n    on a desired net object.\n\n    Activates the computation of mean flops consumption per image.\n    Call it before you run the network.\n\n    """"""\n\n    add_batch_counter_hook_function(self)\n\n    self.apply(add_flops_counter_hook_function)\n\n\ndef stop_flops_count(self):\n    """"""\n    A method that will be available after add_flops_counting_methods() is called\n    on a desired net object.\n\n    Stops computing the mean flops consumption per image.\n    Call whenever you want to pause the computation.\n\n    """"""\n\n    remove_batch_counter_hook_function(self)\n\n    self.apply(remove_flops_counter_hook_function)\n\n\ndef reset_flops_count(self):\n    """"""\n    A method that will be available after add_flops_counting_methods() is called\n    on a desired net object.\n\n    Resets statistics computed so far.\n\n    """"""\n\n    add_batch_counter_variables_or_reset(self)\n\n    self.apply(add_flops_counter_variable_or_reset)\n\n\ndef add_flops_mask(module, mask):\n    def add_flops_mask_func(module):\n        if isinstance(module, torch.nn.Conv2d):\n            module.__mask__ = mask\n\n    module.apply(add_flops_mask_func)\n\n\ndef remove_flops_mask(module):\n    module.apply(add_flops_mask_variable_or_reset)\n\n\n# ---- Internal functions\n\n\ndef conv_flops_counter_hook(conv_module, input, output):\n    # Can have multiple inputs, getting the first one\n    input = input[0]\n\n    batch_size = input.shape[0]\n    output_height, output_width = output.shape[2:]\n\n    kernel_height, kernel_width = conv_module.kernel_size\n    in_channels = conv_module.in_channels\n    out_channels = conv_module.out_channels\n\n    # We count multiply-add as 2 flops\n    conv_per_position_flops = 2 * kernel_height * kernel_width * in_channels * out_channels\n\n    active_elements_count = batch_size * output_height * output_width\n\n    if conv_module.__mask__ is not None:\n        # (b, 1, h, w)\n        flops_mask = conv_module.__mask__.expand(batch_size, 1, output_height, output_width)\n        active_elements_count = flops_mask.sum()\n\n    overall_conv_flops = conv_per_position_flops * active_elements_count\n\n    bias_flops = 0\n\n    if conv_module.bias is not None:\n        bias_flops = out_channels * active_elements_count\n\n    overall_flops = overall_conv_flops + bias_flops\n\n    conv_module.__flops__ += overall_flops\n\n\ndef batch_counter_hook(module, input, output):\n    # Can have multiple inputs, getting the first one\n    input = input[0]\n\n    batch_size = input.shape[0]\n\n    module.__batch_counter__ += batch_size\n\n\ndef add_batch_counter_variables_or_reset(module):\n    module.__batch_counter__ = 0\n\n\ndef add_batch_counter_hook_function(module):\n    if hasattr(module, \'__batch_counter_handle__\'):\n        return\n\n    handle = module.register_forward_hook(batch_counter_hook)\n    module.__batch_counter_handle__ = handle\n\n\ndef remove_batch_counter_hook_function(module):\n    if hasattr(module, \'__batch_counter_handle__\'):\n        module.__batch_counter_handle__.remove()\n\n        del module.__batch_counter_handle__\n\n\ndef add_flops_counter_variable_or_reset(module):\n    if isinstance(module, torch.nn.Conv2d):\n        module.__flops__ = 0\n\n\ndef add_flops_counter_hook_function(module):\n    if isinstance(module, torch.nn.Conv2d):\n\n        if hasattr(module, \'__flops_handle__\'):\n            return\n\n        handle = module.register_forward_hook(conv_flops_counter_hook)\n        module.__flops_handle__ = handle\n\n\ndef remove_flops_counter_hook_function(module):\n    if isinstance(module, torch.nn.Conv2d):\n\n        if hasattr(module, \'__flops_handle__\'):\n            module.__flops_handle__.remove()\n\n            del module.__flops_handle__\n\n\n# --- Masked flops counting\n\n\n# Also being run in the initialization\ndef add_flops_mask_variable_or_reset(module):\n    if isinstance(module, torch.nn.Conv2d):\n        module.__mask__ = None\n\n# if __name__ == \'__main__\':\n#     from semseg.modelloader.fcn import fcn\n#     n_classes = 21\n#     fcn = add_flops_counting_methods(fcn(module_type=\'8s\', n_classes=n_classes, pretrained=False))\n#     fcn.train()\n#     fcn.start_flops_count()\n'"
semseg/utils/get_class_weights.py,0,"b'# -*- coding: utf-8 -*-\n# code is from [get_class_weights.py](https://github.com/kwotsin/TensorFlow-ENet/blob/master/get_class_weights.py)\nimport numpy as np\nimport os\nfrom scipy.misc import imread\n\n\ndef ENet_weighing(image_files, num_classes=12):\n    \'\'\'\n    The custom class weighing function as seen in the ENet paper.\n    INPUTS:\n    - image_files(list): a list of image_filenames which element can be read immediately\n    OUTPUTS:\n    - class_weights(list): a list of class weights where each index represents each class label and the element is the class weight for that label.\n    \'\'\'\n    #initialize dictionary with all 0\n    label_to_frequency = {}\n    for i in xrange(num_classes):\n        label_to_frequency[i] = 0\n\n    for n in xrange(len(image_files)):\n        image = imread(image_files[n])\n\n        #For each label in each image, sum up the frequency of the label and add it to label_to_frequency dict\n        for i in xrange(num_classes):\n            class_mask = np.equal(image, i)\n            class_mask = class_mask.astype(np.float32)\n            class_frequency = np.sum(class_mask)\n\n            label_to_frequency[i] += class_frequency\n\n    #perform the weighing function label-wise and append the label\'s class weights to class_weights\n    class_weights = []\n    total_frequency = sum(label_to_frequency.values())\n    for label, frequency in label_to_frequency.items():\n        class_weight = 1 / np.log(1.02 + (frequency / total_frequency))\n        class_weights.append(class_weight)\n\n    #Set the last class_weight to 0.0\n    class_weights[-1] = 0.0\n\n    return class_weights\n\ndef median_frequency_balancing(image_files, num_classes=12):\n    \'\'\'\n    Perform median frequency balancing on the image files, given by the formula:\n    f = Median_freq_c / total_freq_c\n    where median_freq_c is the median frequency of the class for all pixels of C that appeared in images\n    and total_freq_c is the total number of pixels of c in the total pixels of the images where c appeared.\n    INPUTS:\n    - image_files(list): a list of image_filenames which element can be read immediately\n    - num_classes(int): the number of classes of pixels in all images\n    OUTPUTS:\n    - class_weights(list): a list of class weights where each index represents each class label and the element is the class weight for that label.\n    \'\'\'\n    #Initialize all the labels key with a list value\n    label_to_frequency_dict = {}\n    for i in xrange(num_classes):\n        label_to_frequency_dict[i] = []\n\n    for n in xrange(len(image_files)):\n        image = imread(image_files[n])\n\n        #For each image sum up the frequency of each label in that image and append to the dictionary if frequency is positive.\n        for i in xrange(num_classes):\n            class_mask = np.equal(image, i)\n            class_mask = class_mask.astype(np.float32)\n            class_frequency = np.sum(class_mask)\n\n            if class_frequency != 0.0:\n                label_to_frequency_dict[i].append(class_frequency)\n\n    class_weights = []\n\n    #Get the total pixels to calculate total_frequency later\n    total_pixels = 0\n    for frequencies in label_to_frequency_dict.values():\n        total_pixels += sum(frequencies)\n\n    for i, j in label_to_frequency_dict.items():\n        j = sorted(j) #To obtain the median, we got to sort the frequencies\n\n        median_frequency = np.median(j) / sum(j)\n        total_frequency = sum(j) / total_pixels\n        median_frequency_balanced = median_frequency / total_frequency\n        class_weights.append(median_frequency_balanced)\n\n    #Set the last class_weight to 0.0 as it\'s the background class\n    class_weights[-1] = 0.0\n\n    return class_weights\n\nif __name__ == ""__main__"":\n    image_dir = os.path.expanduser(""~/Data/CamVid/trainannot"")\n    image_files = [os.path.join(image_dir, file) for file in os.listdir(image_dir) if file.endswith(\'.png\')]\n    print(\'image_files:\', image_files)\n    class_weights = median_frequency_balancing(image_files, num_classes=12)\n    print(\'median_frequency_balancing class_weights:\', class_weights)\n    class_weights = ENet_weighing(image_files, num_classes=12)\n    print(\'ENet_weighing class_weights:\', class_weights)\n'"
semseg/utils/model_info_eval.py,3,"b""# -*- coding: utf-8 -*-\n# code is from https://github.com/ShichenLiu/CondenseNet/blob/master/utils.py\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\nfrom functools import reduce\nimport operator\n\n\ncount_ops = 0\ncount_params = 0\n\n\ndef get_num_gen(gen):\n    return sum(1 for x in gen)\n\n\ndef is_pruned(layer):\n    try:\n        layer.mask\n        return True\n    except AttributeError:\n        return False\n\n\ndef is_leaf(model):\n    return get_num_gen(model.children()) == 0\n\n\ndef get_layer_info(layer):\n    layer_str = str(layer)\n    type_name = layer_str[:layer_str.find('(')].strip()\n    return type_name\n\n\ndef get_layer_param(model):\n    return sum([reduce(operator.mul, i.size(), 1) for i in model.parameters()])\n\n\n### The input batch size should be 1 to call this function\ndef measure_layer(layer, x):\n    global count_ops, count_params\n    delta_ops = 0\n    delta_params = 0\n    multi_add = 1\n    type_name = get_layer_info(layer)\n\n    ### ops_conv\n    if type_name in ['Conv2d']:\n        out_h = int((x.size()[2] + 2 * layer.padding[0] - layer.kernel_size[0]) /\n                    layer.stride[0] + 1)\n        out_w = int((x.size()[3] + 2 * layer.padding[1] - layer.kernel_size[1]) /\n                    layer.stride[1] + 1)\n        delta_ops = layer.in_channels * layer.out_channels * layer.kernel_size[0] *  \\\n                layer.kernel_size[1] * out_h * out_w / layer.groups * multi_add\n        delta_params = get_layer_param(layer)\n\n    ### ops_learned_conv\n    elif type_name in ['LearnedGroupConv']:\n        measure_layer(layer.relu, x)\n        measure_layer(layer.norm, x)\n        conv = layer.conv\n        out_h = int((x.size()[2] + 2 * conv.padding[0] - conv.kernel_size[0]) /\n                    conv.stride[0] + 1)\n        out_w = int((x.size()[3] + 2 * conv.padding[1] - conv.kernel_size[1]) /\n                    conv.stride[1] + 1)\n        delta_ops = conv.in_channels * conv.out_channels * conv.kernel_size[0] * \\\n                conv.kernel_size[1] * out_h * out_w / layer.condense_factor * multi_add\n        delta_params = get_layer_param(conv) / layer.condense_factor\n\n    ### ops_nonlinearity\n    elif type_name in ['ReLU']:\n        delta_ops = x.numel()\n        delta_params = get_layer_param(layer)\n\n    ### ops_pooling\n    elif type_name in ['AvgPool2d']:\n        in_w = x.size()[2]\n        kernel_ops = layer.kernel_size * layer.kernel_size\n        out_w = int((in_w + 2 * layer.padding - layer.kernel_size) / layer.stride + 1)\n        out_h = int((in_w + 2 * layer.padding - layer.kernel_size) / layer.stride + 1)\n        delta_ops = x.size()[0] * x.size()[1] * out_w * out_h * kernel_ops\n        delta_params = get_layer_param(layer)\n\n    elif type_name in ['AdaptiveAvgPool2d']:\n        delta_ops = x.size()[0] * x.size()[1] * x.size()[2] * x.size()[3]\n        delta_params = get_layer_param(layer)\n\n    ### ops_linear\n    elif type_name in ['Linear']:\n        weight_ops = layer.weight.numel() * multi_add\n        bias_ops = layer.bias.numel()\n        delta_ops = x.size()[0] * (weight_ops + bias_ops)\n        delta_params = get_layer_param(layer)\n\n    ### ops_nothing\n    elif type_name in ['BatchNorm2d', 'Dropout2d', 'DropChannel', 'Dropout']:\n        delta_params = get_layer_param(layer)\n\n    ### unknown layer type\n    else:\n        raise TypeError('unknown layer type: %s' % type_name)\n\n    count_ops += delta_ops\n    count_params += delta_params\n    return\n\n\ndef measure_model(model, H, W):\n    global count_ops, count_params\n    count_ops = 0\n    count_params = 0\n    data = Variable(torch.zeros(1, 3, H, W))\n\n    def should_measure(x):\n        return is_leaf(x) or is_pruned(x)\n\n    def modify_forward(model):\n        for child in model.children():\n            if should_measure(child):\n                def new_forward(m):\n                    def lambda_forward(x):\n                        measure_layer(m, x)\n                        return m.old_forward(x)\n                    return lambda_forward\n                child.old_forward = child.forward\n                child.forward = new_forward(child)\n            else:\n                modify_forward(child)\n\n    def restore_forward(model):\n        for child in model.children():\n            # leaf node\n            if is_leaf(child) and hasattr(child, 'old_forward'):\n                child.forward = child.old_forward\n                child.old_forward = None\n            else:\n                restore_forward(child)\n\n    modify_forward(model)\n    model.forward(data)\n    restore_forward(model)\n\n    return count_ops, count_params\n"""
