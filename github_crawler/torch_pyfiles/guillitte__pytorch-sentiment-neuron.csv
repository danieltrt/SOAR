file_path,api_count,code
convert_to_cpu.py,5,"b'import os\nimport torch\nfrom torch.autograd import Variable\nfrom torch import optim\nimport torch.nn.functional as F\nimport torch.nn as nn\nimport argparse\nimport models\n\n\nparser = argparse.ArgumentParser(description=\'convert_to_cpu.py\')\n\nparser.add_argument(\'-load_model\', default=\'\',\n                    help=""""""Model filename to load and convert."""""")                   \n                    \nopt = parser.parse_args()    \n\ncheckpoint = torch.load(opt.load_model)\nembed = checkpoint[\'embed\']\nrnn = checkpoint[\'rnn\']\n\ncheckpoint = {\n\t\'rnn\': rnn.cpu(),\n\t\'embed\': embed.cpu(),\n\t\'opt\': checkpoint[\'opt\'],\n\t\'epoch\': checkpoint[\'epoch\']\n\t}\nsave_file = opt.load_model + \'.cpu\'\nprint(\'Saving to \'+ save_file)\ntorch.save(checkpoint, save_file)\n'"
lm.py,8,"b'import os\nimport torch\nfrom torch.autograd import Variable\nfrom torch import optim\nimport torch.nn.functional as F\nimport torch.nn as nn\nimport numpy as np\nimport models\nimport argparse\nimport time\nimport math\n\nparser = argparse.ArgumentParser(description=\'lm.py\')\n\nparser.add_argument(\'-save_model\', default=\'lm\',\n                    help=""""""Model filename to save"""""")\nparser.add_argument(\'-load_model\', default=\'\',\n                    help=""""""Model filename to load"""""")\nparser.add_argument(\'-train\', default=\'data/input.txt\',\n                    help=""""""Text filename for training"""""")\nparser.add_argument(\'-valid\', default=\'data/valid.txt\',\n                    help=""""""Text filename for validation"""""")                    \nparser.add_argument(\'-rnn_type\', default=\'mlstm\',\n                    help=\'mlstm, lstm or gru\')\nparser.add_argument(\'-layers\', type=int, default=1,\n                    help=\'Number of layers in the rnn\')\nparser.add_argument(\'-rnn_size\', type=int, default=4096,\n                    help=\'Size of hidden states\')\nparser.add_argument(\'-embed_size\', type=int, default=128,\n                    help=\'Size of embeddings\')\nparser.add_argument(\'-seq_length\', type=int, default=20,\n                    help=""Maximum sequence length"")\nparser.add_argument(\'-batch_size\', type=int, default=64,\n                    help=\'Maximum batch size\')\nparser.add_argument(\'-learning_rate\', type=float, default=0.001,\n                    help=""""""Starting learning rate."""""")\nparser.add_argument(\'-dropout\', type=float, default=0.1,\n                    help=\'Dropout probability.\')\nparser.add_argument(\'-param_init\', type=float, default=0.05,\n                    help=""""""Parameters are initialized over uniform distribution\n                    with support (-param_init, param_init)"""""")\nparser.add_argument(\'-clip\', type=float, default=5,\n                    help=""""""Clip gradients at this value."""""")\nparser.add_argument(\'--seed\', type=int, default=1234,\n                    help=\'random seed\')   \n# GPU\nparser.add_argument(\'-cuda\', action=\'store_true\',\n                    help=""Use CUDA"")\n\n         \n\nopt = parser.parse_args()    \nlearning_rate = opt.learning_rate\n\npath = opt.train\ntorch.manual_seed(opt.seed)\nif opt.cuda:\n\ttorch.cuda.manual_seed(opt.seed)\n\ndef tokenize(path):\n        """"""Tokenizes a text file.""""""\n        assert os.path.exists(path)\n        # Count bytes\n        with open(path, \'r\') as f:\n            tokens = 0\n            for line in f:              \n                tokens += len(line.encode())\n                \n        print(tokens)\n        # Tokenize file content\n        with open(path, \'r\') as f:\n            ids = torch.ByteTensor(tokens)\n            token = 0\n            for line in f:\n                \n                for char in line.encode():\n                    ids[token] = char\n                    token += 1\n\n        return ids\n\ndef batchify(data, bsz):\n    nbatch = data.size(0) // bsz\n    data = data.narrow(0, 0, nbatch * bsz)\n    data = data.view(bsz, -1).t().contiguous()\n    return data        \n\n\n\nbatch_size = opt.batch_size\nhidden_size =opt.rnn_size\ninput_size = opt.embed_size\ndata_size = 256\nTIMESTEPS = opt.seq_length\n\nif len(opt.load_model)>0:\n    checkpoint = torch.load(opt.load_model)\n    embed = checkpoint[\'embed\']\n    rnn = checkpoint[\'rnn\']\nelse:\n    embed = nn.Embedding(256, input_size)\n    if opt.rnn_type == \'gru\':\n    \trnn = models.StackedRNN(nn.GRUCell, opt.layers, input_size, hidden_size, data_size, opt.dropout)\n    elif opt.rnn_type == \'mlstm\':\n    \trnn = models.StackedLSTM(models.mLSTM, opt.layers, input_size, hidden_size, data_size, opt.dropout)\n    else:#default to lstm\n    \trnn = models.StackedLSTM(nn.LSTMCell, opt.layers, input_size, hidden_size, data_size, opt.dropout)\n\n\n\nloss_fn = nn.CrossEntropyLoss() \n\nnParams = sum([p.nelement() for p in rnn.parameters()])\nprint(\'* number of parameters: %d\' % nParams)\ntext = tokenize(path)\ntext = batchify(text, batch_size)\nvalid = tokenize(opt.valid)\nvalid = batchify(valid, batch_size)\n\nlearning_rate =opt.learning_rate\n\nn_batch = text.size(0)//TIMESTEPS\nnv_batch = valid.size(0)//TIMESTEPS\n\nprint(text.size(0))\nprint(n_batch)\nembed_optimizer = optim.SGD(embed.parameters(), lr=learning_rate)\nrnn_optimizer = optim.SGD(rnn.parameters(), lr=learning_rate)\n   \ndef update_lr(optimizer, lr):\n    for group in optimizer.param_groups:\n        group[\'lr\'] = lr\n    return\n\t\ndef clip_gradient_coeff(model, clip):\n    """"""Computes a gradient clipping coefficient based on gradient norm.""""""\n    totalnorm = 0\n    for p in model.parameters():\n        modulenorm = p.grad.data.norm()\n        totalnorm += modulenorm ** 2\n    totalnorm = math.sqrt(totalnorm)\n    return min(1, clip / (totalnorm + 1e-6))\n\ndef calc_grad_norm(model):\n    """"""Computes a gradient clipping coefficient based on gradient norm.""""""\n    totalnorm = 0\n    for p in model.parameters():\n        modulenorm = p.grad.data.norm()\n        totalnorm += modulenorm ** 2\n    return math.sqrt(totalnorm)\n    \ndef calc_grad_norms(model):\n    """"""Computes a gradient clipping coefficient based on gradient norm.""""""\n    norms = []\n    for p in model.parameters():\n        modulenorm = p.grad.data.norm()\n        norms += [modulenorm]\n    return norms\n    \ndef clip_gradient(model, clip):\n    """"""Clip the gradient.""""""\n    totalnorm = 0\n    for p in model.parameters():\n        p.grad.data = p.grad.data.clamp(-clip,clip)\n\n        \ndef make_cuda(state):\n    if isinstance(state, tuple):\n    \treturn (state[0].cuda(), state[1].cuda())\n    else:\n    \treturn state.cuda()\n    \t\ndef copy_state(state):\n    if isinstance(state, tuple):\n    \treturn (Variable(state[0].data), Variable(state[1].data))\n    else:\n    \treturn Variable(state.data)    \t\n\n\ndef evaluate():\n    hidden_init = rnn.state0(opt.batch_size)  \t\t\n    if opt.cuda:\n\t    embed.cuda()\n\t    rnn.cuda()\n\t    hidden_init = make_cuda(hidden_init)\n\n    loss_avg = 0\n    for s in range(nv_batch-1):\n        batch = Variable(valid.narrow(0,s*TIMESTEPS,TIMESTEPS+1).long())\n        start = time.time()\n        hidden = hidden_init\n        if opt.cuda:\n            batch = batch.cuda()\n\n        loss = 0\n        for t in range(TIMESTEPS):                  \n            emb = embed(batch[t])\n            hidden, output = rnn(emb, hidden)\n            loss += loss_fn(output, batch[t+1])\n\n        hidden_init = copy_state(hidden)\n        loss_avg = loss_avg + loss.data[0]/TIMESTEPS\n        if s % 10 == 0:\n            print(\'v %s / %s loss %.4f loss avg %.4f time %.4f\' % ( s, nv_batch, loss.data[0]/TIMESTEPS, loss_avg/(s+1), time.time()-start))\n    return loss_avg/nv_batch\n\ndef train_epoch(epoch):\n\thidden_init = rnn.state0(opt.batch_size)    \t\t\n\tif opt.cuda:\n\t    embed.cuda()\n\t    rnn.cuda()\n\t    hidden_init = make_cuda(hidden_init)\n\n\tloss_avg = 0\n\n\tfor s in range(n_batch-1):\n\n\t\tembed_optimizer.zero_grad()\n\t\trnn_optimizer.zero_grad()\n\t\tbatch = Variable(text.narrow(0,s*TIMESTEPS,TIMESTEPS+1).long())\n\t\tstart = time.time()\n\t\thidden = hidden_init\n\t\tif opt.cuda:\n\t\t\tbatch = batch.cuda()\n\t\tloss = 0\n\t\tfor t in range(TIMESTEPS):                  \n\t\t\temb = embed(batch[t])\n\t\t\thidden, output = rnn(emb, hidden)\n\t\t\tloss += loss_fn(output, batch[t+1])\n        \n        \n\t\tloss.backward()\n    \n\t\thidden_init = copy_state(hidden)\n\t\tgn =calc_grad_norm(rnn)\n\t\tclip_gradient(rnn, opt.clip)\n\t\tclip_gradient(embed, opt.clip)\n\t\tembed_optimizer.step()\n\t\trnn_optimizer.step()\n\t\tloss_avg = .99*loss_avg + .01*loss.data[0]/TIMESTEPS\n\t\tif s % 10 == 0:\n\t\t\tprint(\'e%s %s / %s loss %.4f loss avg %.4f time %.4f grad_norm %.4f\' % (epoch, s, n_batch, loss.data[0]/TIMESTEPS, loss_avg, time.time()-start, gn))\n\n\nfor e in range(10):\n\ttry:\n\t\ttrain_epoch(e)\n\texcept KeyboardInterrupt:\n\t\tprint(\'Exiting from training early\')\n\tloss_avg = evaluate()\n\tcheckpoint = {\n            \'rnn\': rnn,\n            \'embed\': embed,\n            \'opt\': opt,\n            \'epoch\': e\n        }\n\tsave_file = (\'%s_e%s_%.2f.pt\' % (opt.save_model, e, loss_avg))\n\tprint(\'Saving to \'+ save_file)\n\ttorch.save(checkpoint, save_file)\n\tlearning_rate *= 0.7\n\tupdate_lr(rnn_optimizer, learning_rate)\n\tupdate_lr(embed_optimizer, learning_rate)\n'"
load_from_numpy.py,12,"b'import os\nimport torch\nfrom torch.autograd import Variable\nfrom torch import optim\nimport torch.nn.functional as F\nimport torch.nn as nn\nimport numpy as np\nimport models\nimport argparse\nimport time\nimport math\n\n\nparser = argparse.ArgumentParser(description=\'load_from_numpy.py\')\n\nparser.add_argument(\'-save_model\', default=\'mlstm-ns.pt\',\n                    help=""""""Model filename to save"""""")\nparser.add_argument(\'-load_model\', default=\'\',\n                    help=""""""Model filename to load"""""")\nparser.add_argument(\'-train\', default=\'data/input.txt\',\n                    help=""""""Text filename for training"""""")\nparser.add_argument(\'-valid\', default=\'data/valid.txt\',\n                    help=""""""Text filename for validation"""""")                    \nparser.add_argument(\'-rnn_type\', default=\'mlstm\',\n                    help=\'mlstm, lstm or gru\')\nparser.add_argument(\'-layers\', type=int, default=1,\n                    help=\'Number of layers in the encoder/decoder\')\nparser.add_argument(\'-rnn_size\', type=int, default=4096,\n                    help=\'Size of hidden states\')\nparser.add_argument(\'-embed_size\', type=int, default=128,\n                    help=\'Size of embeddings\')\nparser.add_argument(\'-seq_length\', type=int, default=20,\n                    help=""Maximum sequence length"")\nparser.add_argument(\'-batch_size\', type=int, default=64,\n                    help=\'Maximum batch size\')\nparser.add_argument(\'-learning_rate\', type=float, default=0.001,\n                    help=""""""Starting learning rate."""""")\nparser.add_argument(\'-dropout\', type=float, default=0.0,\n                    help=\'Dropout probability.\')\nparser.add_argument(\'-param_init\', type=float, default=0.05,\n                    help=""""""Parameters are initialized over uniform distribution\n                    with support (-param_init, param_init)"""""")\nparser.add_argument(\'-clip\', type=float, default=5,\n                    help=""""""Clip gradients at this value."""""")\nparser.add_argument(\'--seed\', type=int, default=1234,\n                    help=\'random seed\')   \n# GPU\nparser.add_argument(\'-cuda\', action=\'store_true\',\n                    help=""Use CUDA"")\n               \n\nopt = parser.parse_args()   \n\nembed = nn.Embedding(256, opt.embed_size)\nrnn = models.StackedLSTM(models.mLSTM, opt.layers, opt.embed_size, opt.rnn_size, 256, opt.dropout)\n\nembed.weight.data = torch.from_numpy(np.load(""weights/embd.npy""))\nrnn.h2o.weight.data = torch.from_numpy(np.load(""weights/w.npy"")).t()\nrnn.h2o.bias.data = torch.from_numpy(np.load(""weights/b.npy""))\nrnn.layers[0].wx.weight.data = torch.from_numpy(np.load(""weights/wx.npy"")).t()\nrnn.layers[0].wh.weight.data = torch.from_numpy(np.load(""weights/wh.npy"")).t()\nrnn.layers[0].wh.bias.data = torch.from_numpy(np.load(""weights/b0.npy""))\nrnn.layers[0].wmx.weight.data = torch.from_numpy(np.load(""weights/wmx.npy"")).t()\nrnn.layers[0].wmh.weight.data = torch.from_numpy(np.load(""weights/wmh.npy"")).t()\ncheckpoint = {\n    \'rnn\': rnn,\n    \'embed\': embed,\n    \'opt\': opt,\n    \'epoch\': 0\n    }\nsave_file = opt.save_model\nprint(\'Saving to \'+ save_file)\ntorch.save(checkpoint, save_file)\n'"
models.py,7,"b""import numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\nfrom torch import optim\nimport torch.nn.functional as F\n\nclass mLSTM(nn.Module):\n\n\tdef __init__(self, data_size, hidden_size, n_layers = 1):\n\t\tsuper(mLSTM, self).__init__()\n        \n\t\tself.hidden_size = hidden_size\n\t\tself.data_size = data_size\n\t\tself.n_layers = n_layers\n\t\tinput_size = data_size + hidden_size\n        \n       \n\t\tself.wx = nn.Linear(data_size, 4*hidden_size, bias = False)\n\t\tself.wh = nn.Linear(hidden_size, 4*hidden_size, bias = True)\n\t\tself.wmx = nn.Linear(data_size, hidden_size, bias = False) \n\t\tself.wmh = nn.Linear(hidden_size, hidden_size, bias = False) \n  \n\tdef forward(self, data, last_hidden):\n\n\t\thx, cx = last_hidden\n\t\tm = self.wmx(data) * self.wmh(hx)\n\t\tgates = self.wx(data) + self.wh(m)\n\t\ti, f, o, u = gates.chunk(4, 1)\n\n\t\ti = F.sigmoid(i)\n\t\tf = F.sigmoid(f)\n\t\tu = F.tanh(u)\n\t\to = F.sigmoid(o)\n\n\t\tcy = f * cx + i * u\n\t\thy = o * F.tanh(cy)\n\n\t\treturn hy, cy\n\nclass StackedLSTM(nn.Module):\n    def __init__(self, cell, num_layers, input_size, rnn_size, output_size, dropout):\n        super(StackedLSTM, self).__init__()\n        self.dropout = nn.Dropout(dropout)\n        self.num_layers = num_layers\n        self.rnn_size = rnn_size\n        self.h2o = nn.Linear(rnn_size, output_size)\n        \n        self.layers = []\n        for i in range(num_layers):\n            layer = cell(input_size, rnn_size)\n            self.add_module('layer_%d' % i, layer)\n            self.layers += [layer]\n            input_size = rnn_size\n\n    def forward(self, input, hidden):\n        h_0, c_0 = hidden\n        h_1, c_1 = [], []\n        for i, layer in enumerate(self.layers):\n            h_1_i, c_1_i = layer(input, (h_0[i], c_0[i]))\n            if i == 0:\n            \tinput = h_1_i\n            else:\n            \tinput = input + h_1_i\n            if i != len(self.layers):\n                input = self.dropout(input)\n            h_1 += [h_1_i]\n            c_1 += [c_1_i]\n\n        h_1 = torch.stack(h_1)\n        c_1 = torch.stack(c_1)\n        output = self.h2o(input)\n\n        return (h_1, c_1),output\n        \n    def state0(self, batch_size):\n            h_0 = Variable(torch.zeros(self.num_layers, batch_size, self.rnn_size), requires_grad=False)\n            c_0 = Variable(torch.zeros(self.num_layers, batch_size, self.rnn_size), requires_grad=False)\n            return (h_0, c_0) \n"""
visualize.py,9,"b'import os\nimport torch\nfrom torch.autograd import Variable\nfrom torch import optim\nimport torch.nn.functional as F\nimport torch.nn as nn\nimport argparse\nimport models\nimport math\n\nparser = argparse.ArgumentParser(description=\'sample.py\')\n\nparser.add_argument(\'-init\', default=\'The meaning of life is \',\n                    help=""""""Initial text """""")\nparser.add_argument(\'-load_model\', default=\'\',\n                    help=""""""Model filename to load"""""")                   \nparser.add_argument(\'-seq_length\', type=int, default=50,\n                    help=""""""Maximum sequence length"""""")\nparser.add_argument(\'-temperature\', type=float, default=0.4,\n                    help=""""""Temperature for sampling."""""")\nparser.add_argument(\'-neuron\', type=int, default=0,\n                    help=""""""Neuron to read."""""")\nparser.add_argument(\'-overwrite\', type=float, default=0,\n                    help=""""""Value used to overwrite the neuron. 0 means don\'t overwrite."""""")\nparser.add_argument(\'-layer\', type=int, default=-1,\n                    help=""""""Layer to read. -1 = last layer"""""")\n# GPU\nparser.add_argument(\'-cuda\', action=\'store_true\',\n                    help=""""""Use CUDA"""""")\n                    \nopt = parser.parse_args()    \n\n\ndef batchify(data, bsz):\n    tokens = len(data.encode())\n    ids = torch.LongTensor(tokens)\n    token = 0\n    for char in data.encode():\n        ids[token] = char\n        token += 1\n    nbatch = ids.size(0) // bsz\n    ids = ids.narrow(0, 0, nbatch * bsz)\n    ids = ids.view(bsz, -1).t().contiguous()\n    return ids        \n\n\ndef color(p):\n\tp = math.tanh(3*p)*.5+.5\n\tq = 1.-p*1.3\n\tr = 1.-abs(0.5-p)*1.3+.3*q\n\tp=1.3*p-.3\n\ti = int(p*255)\t\n\tj = int(q*255)\n\tk = int(r*255)\n\tif j<0:\n\t\tj=0\t\n\tif k<0:\n\t\tk=0\n\tif k >255:\n\t\tk=255\n\tif i<0:\n\t\ti = 0\n\treturn (\'\\033[38;2;%d;%d;%dm\' % (j, k, i)).encode()\n\n\n\nbatch_size = 1\n\ncheckpoint = torch.load(opt.load_model)\nembed = checkpoint[\'embed\']\nrnn = checkpoint[\'rnn\']\n\nloss_fn = nn.CrossEntropyLoss() \n\n\ntext = batchify(opt.init, batch_size)\n\ndef make_cuda(state):\n    if isinstance(state, tuple):\n    \treturn (state[0].cuda(), state[1].cuda())\n    else:\n    \treturn state.cuda()\n\nbatch = Variable(text) \nstates = rnn.state0(batch_size)\nif isinstance(states, tuple):\n\thidden, cell = states\nelse:\n\thidden = states\nlast = hidden.size(0)-1\nif opt.layer <= last and opt.layer >= 0:\n\tlast = opt.layer\n\nif opt.cuda:\n    batch =batch.cuda()\n    states = make_cuda(states)\n    embed.cuda()\n    rnn.cuda()\n    \nloss_avg = 0\nloss = 0\ngen = bytearray()\nfor t in range(text.size(0)):                           \n    emb = embed(batch[t])\n    ni = (batch[t]).data[0]\n    states, output = rnn(emb, states)\n    if isinstance(states, tuple):\n        hidden, cell = states\n    else:\n        hidden = states\n    feat = hidden.data[last,0,opt.neuron]\n    if ni< 128:\n        col = color(feat)\n        gen+=(col)\n    gen.append(ni)        \t\n\nprint(opt.init)        \t\n\n\nif opt.temperature == 0:\n    topv, topi = output.data.topk(1)\n    ni = topi[0][0]\n    gen.append(ni)\n    inp = Variable(topi[0], volatile=True)\n    if opt.cuda:\n        inp = inp.cuda()  \n\n    for t in range(opt.seq_length):\n        \n        emb = embed(inp)\n        states, output = rnn(emb, states)        \n        topv, topi = output.data.topk(1)\n        ni = topi[0][0]\n        gen.append(ni)\n        inp = Variable(topi[0])\n        if opt.cuda:\n            inp = inp.cuda() \n\nelse:\n    probs = F.softmax(output[0].squeeze().div(opt.temperature)).data.cpu()\n    ni = torch.multinomial(probs,1)[0]\n    feat = hidden.data[last,0,opt.neuron]\n    \n    if ni < 128:\n    \tcol = color(feat)\n    \tgen+=(col)\n    \t\n    gen.append(ni)\t\n    \n    \n    inp = Variable(torch.LongTensor([ni]), volatile=True) \n    if opt.cuda:\n        inp = inp.cuda()      \n    for t in range(opt.seq_length):\n        \n        emb = embed(inp)\n        states, output = rnn(emb, states)\n        if isinstance(states, tuple):\n        \thidden, cell = states\n        else:\n        \thidden = states\n        feat = hidden.data[last,0,opt.neuron] \n        if isinstance(output, list): \n        \toutput =output[0]      \n        probs = F.softmax(output.squeeze().div(opt.temperature)).data.cpu()\n        ni = torch.multinomial(probs,1)[0]\n        \n        if ni< 128:\n        \tcol = color(feat)\n        \tgen+=(col)\n        gen.append(ni)\n        inp = Variable(torch.LongTensor([ni]))\n        if opt.cuda:\n            inp = inp.cuda() \n        if opt.overwrite != 0: \n        \thidden.data[last,0,opt.neuron] = opt.overwrite\n\ngen+=(\'\\033[0m\').encode()\n\nprint(gen.decode(""utf-8"",errors = \'ignore\' ))\n'"
