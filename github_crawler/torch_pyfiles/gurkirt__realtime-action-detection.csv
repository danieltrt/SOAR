file_path,api_count,code
ssd.py,7,"b'\n"""""" SSD network Classes\n\nOriginal author: Ellis Brown, Max deGroot for VOC dataset\nhttps://github.com/amdegroot/ssd.pytorch\n\nUpdated by Gurkirt Singh for ucf101-24 dataset\n""""""\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nfrom layers import *\nfrom data import v2\nimport os\n\n\nclass SSD(nn.Module):\n    """"""Single Shot Multibox Architecture\n    The network is composed of a base VGG network followed by the\n    added multibox conv layers.  Each multibox layer branches into\n        1) conv2d for class conf scores\n        2) conv2d for localization predictions\n        3) associated priorbox layer to produce default bounding\n           boxes specific to the layer\'s feature map size.\n    See: https://arxiv.org/pdf/1512.02325.pdf for more details.\n\n    Args:\n        base: VGG16 layers for input, size of either 300 or 500\n        extras: extra layers that feed to multibox loc and conf layers\n        head: ""multibox head"" consists of loc and conf conv layers\n    """"""\n\n    def __init__(self, base, extras, head, num_classes):\n        super(SSD, self).__init__()\n\n        self.num_classes = num_classes\n        # TODO: implement __call__ in PriorBox\n        self.priorbox = PriorBox(v2)\n        with torch.no_grad():\n            self.priors = self.priorbox.forward().cuda()\n            self.num_priors = self.priors.size(0)\n            self.size = 300\n\n        # SSD network\n        self.vgg = nn.ModuleList(base)\n        # Layer learns to scale the l2 normalized features from conv4_3\n        self.L2Norm = L2Norm(512, 20)\n        self.extras = nn.ModuleList(extras)\n\n        self.loc = nn.ModuleList(head[0])\n        self.conf = nn.ModuleList(head[1])\n\n        self.softmax = nn.Softmax(dim=1).cuda()\n        # self.detect = Detect(num_classes, 0, 200, 0.001, 0.45)\n\n    def forward(self, x):\n\n        """"""Applies network layers and ops on input image(s) x.\n\n        Args:\n            x: input image or batch of images. Shape: [batch,3*batch,300,300].\n\n        Return:\n            Depending on phase:\n            test:\n                Variable(tensor) of output class label predictions,\n                confidence score, and corresponding location predictions for\n                each object detected. Shape: [batch,topk,7]\n\n            train:\n                list of concat outputs from:\n                    1: confidence layers, Shape: [batch*num_priors,num_classes]\n                    2: localization layers, Shape: [batch,num_priors*4]\n                    3: priorbox layers, Shape: [2,num_priors*4]\n        """"""\n\n        sources = list()\n        loc = list()\n        conf = list()\n\n        # apply vgg up to conv4_3 relu\n        for k in range(23):\n            x = self.vgg[k](x)\n\n        s = self.L2Norm(x)\n        sources.append(s)\n\n        # apply vgg up to fc7\n        for k in range(23, len(self.vgg)):\n            x = self.vgg[k](x)\n        sources.append(x)\n\n        # apply extra layers and cache source layer outputs\n        for k, v in enumerate(self.extras):\n            x = F.relu(v(x), inplace=True)\n            if k % 2 == 1:\n                sources.append(x)\n\n        # apply multibox head to source layers\n        for (x, l, c) in zip(sources, self.loc, self.conf):\n            loc.append(l(x).permute(0, 2, 3, 1).contiguous())\n            conf.append(c(x).permute(0, 2, 3, 1).contiguous())\n\n        loc = torch.cat([o.view(o.size(0), -1) for o in loc], 1)\n        conf = torch.cat([o.view(o.size(0), -1) for o in conf], 1)\n        output = (loc.view(loc.size(0), -1, 4),\n                  conf.view(conf.size(0), -1, self.num_classes),\n                  self.priors\n                  )\n        return output\n\n    def load_weights(self, base_file):\n        other, ext = os.path.splitext(base_file)\n        if ext == \'.pkl\' or \'.pth\':\n            print(\'Loading weights into state dict...\')\n            self.load_state_dict(torch.load(base_file, map_location=lambda storage, loc: storage))\n            print(\'Finished!\')\n        else:\n            print(\'Sorry only .pth and .pkl files supported.\')\n\n\n# This function is derived from torchvision VGG make_layers()\n# https://github.com/pytorch/vision/blob/master/torchvision/models/vgg.py\ndef vgg(cfg, i, batch_norm=False):\n    layers = []\n    in_channels = i\n    for v in cfg:\n        if v == \'M\':\n            layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n        elif v == \'C\':\n            layers += [nn.MaxPool2d(kernel_size=2, stride=2, ceil_mode=True)]\n        else:\n            conv2d = nn.Conv2d(in_channels, v, kernel_size=3, padding=1)\n            if batch_norm:\n                layers += [conv2d, nn.BatchNorm2d(v), nn.ReLU(inplace=True)]\n            else:\n                layers += [conv2d, nn.ReLU(inplace=True)]\n            in_channels = v\n    pool5 = nn.MaxPool2d(kernel_size=3, stride=1, padding=1)\n    conv6 = nn.Conv2d(512, 1024, kernel_size=3, padding=6, dilation=6)\n    conv7 = nn.Conv2d(1024, 1024, kernel_size=1)\n    layers += [pool5, conv6,\n               nn.ReLU(inplace=True), conv7, nn.ReLU(inplace=True)]\n    return layers\n\n\ndef add_extras(cfg, i, batch_norm=False):\n    # Extra layers added to VGG for feature scaling\n    layers = []\n    in_channels = i\n    flag = False\n    for k, v in enumerate(cfg):\n        if in_channels != \'S\':\n            if v == \'S\':\n                layers += [nn.Conv2d(in_channels, cfg[k + 1],\n                           kernel_size=(1, 3)[flag], stride=2, padding=1)]\n            else:\n                layers += [nn.Conv2d(in_channels, v, kernel_size=(1, 3)[flag])]\n            flag = not flag\n        in_channels = v\n    return layers\n\n\ndef multibox(vgg, extra_layers, cfg, num_classes):\n    loc_layers = []\n    conf_layers = []\n    vgg_source = [24, -2]\n    for k, v in enumerate(vgg_source):\n        loc_layers += [nn.Conv2d(vgg[v].out_channels,\n                                 cfg[k] * 4, kernel_size=3, padding=1)]\n        conf_layers += [nn.Conv2d(vgg[v].out_channels,\n                        cfg[k] * num_classes, kernel_size=3, padding=1)]\n    for k, v in enumerate(extra_layers[1::2], 2):\n        loc_layers += [nn.Conv2d(v.out_channels, cfg[k]\n                                 * 4, kernel_size=3, padding=1)]\n        conf_layers += [nn.Conv2d(v.out_channels, cfg[k]\n                                  * num_classes, kernel_size=3, padding=1)]\n    return vgg, extra_layers, (loc_layers, conf_layers)\n\n\nbase = {\n    \'300\': [64, 64, \'M\', 128, 128, \'M\', 256, 256, 256, \'C\', 512, 512, 512, \'M\',\n            512, 512, 512],\n    \'512\': [],\n}\nextras = {\n    \'300\': [256, \'S\', 512, 128, \'S\', 256, 128, 256, 128, 256],\n    \'512\': [],\n}\nmbox = {\n    \'300\': [4, 6, 6, 6, 4, 4],  # number of boxes per feature map location\n    \'512\': [],\n}\n\n\ndef build_ssd(size=300, num_classes=21):\n\n    if size != 300:\n        print(""Error: Sorry only SSD300 is supported currently!"")\n        return\n\n    return SSD(*multibox(vgg(base[str(size)], 3),\n                                add_extras(extras[str(size)], 1024),\n                                mbox[str(size)], num_classes), num_classes)\n'"
test-ucf24.py,16,"b'""""""\n    Copyright (c) 2017, Gurkirt Singh\n\n    This code and is available\n    under the terms of MIT License provided in LICENSE.\n    Please retain this notice and LICENSE if you use\n    this file (or any portion of it) in your project.\n    ---------------------------------------------------------\n""""""\n\nimport torch\nimport torch.backends.cudnn as cudnn\nfrom torch.autograd import Variable\nfrom data import AnnotationTransform, UCF24Detection, BaseTransform, CLASSES, detection_collate, v2\nfrom ssd import build_ssd\nimport torch.utils.data as data\nfrom layers.box_utils import decode, nms\nfrom utils.evaluation import evaluate_detections\nimport os, time\nimport argparse\nimport numpy as np\nimport pickle\nimport scipy.io as sio # to save detection as mat files\ncfg = v2\n\ndef str2bool(v):\n    return v.lower() in (""yes"", ""true"", ""t"", ""1"")\n\nparser = argparse.ArgumentParser(description=\'Single Shot MultiBox Detector Training\')\nparser.add_argument(\'--version\', default=\'v2\', help=\'conv11_2(v2) or pool6(v1) as last layer\')\nparser.add_argument(\'--basenet\', default=\'vgg16_reducedfc.pth\', help=\'pretrained base model\')\nparser.add_argument(\'--dataset\', default=\'ucf24\', help=\'pretrained base model\')\nparser.add_argument(\'--ssd_dim\', default=300, type=int, help=\'Input Size for SSD\') # only support 300 now\nparser.add_argument(\'--input_type\', default=\'rgb\', type=str, help=\'INput tyep default rgb can take flow as well\')\nparser.add_argument(\'--jaccard_threshold\', default=0.5, type=float, help=\'Min Jaccard index for matching\')\nparser.add_argument(\'--batch_size\', default=32, type=int, help=\'Batch size for training\')\nparser.add_argument(\'--resume\', default=None, type=str, help=\'Resume from checkpoint\')\nparser.add_argument(\'--num_workers\', default=0, type=int, help=\'Number of workers used in dataloading\')\nparser.add_argument(\'--eval_iter\', default=\'120000,\', type=str, help=\'Number of training iterations\')\nparser.add_argument(\'--cuda\', default=True, type=str2bool, help=\'Use cuda to train model\')\nparser.add_argument(\'--ngpu\', default=1, type=str2bool, help=\'Use cuda to train model\')\nparser.add_argument(\'--lr\', \'--learning-rate\', default=1e-3, type=float, help=\'initial learning rate\')\nparser.add_argument(\'--visdom\', default=False, type=str2bool, help=\'Use visdom to for loss visualization\')\nparser.add_argument(\'--data_root\', default=\'/mnt/mars-fast/datasets/\', help=\'Location of VOC root directory\')\nparser.add_argument(\'--save_root\', default=\'/mnt/mars-gamma/datasets/\', help=\'Location to save checkpoint models\')\nparser.add_argument(\'--iou_thresh\', default=0.5, type=float, help=\'Evaluation threshold\')\nparser.add_argument(\'--conf_thresh\', default=0.01, type=float, help=\'Confidence threshold for evaluation\')\nparser.add_argument(\'--nms_thresh\', default=0.45, type=float, help=\'NMS threshold\')\nparser.add_argument(\'--topk\', default=20, type=int, help=\'topk for evaluation\')\n\nargs = parser.parse_args()\n\nif args.input_type != \'rgb\':\n    args.conf_thresh = 0.05\n\nif args.cuda and torch.cuda.is_available():\n    torch.set_default_tensor_type(\'torch.cuda.FloatTensor\')\nelse:\n    torch.set_default_tensor_type(\'torch.FloatTensor\')\n\n\ndef test_net(net, save_root, exp_name, input_type, dataset, iteration, num_classes, thresh=0.5 ):\n    """""" Test a SSD network on an Action image database. """"""\n\n    val_data_loader = data.DataLoader(dataset, args.batch_size, num_workers=args.num_workers,\n                            shuffle=False, collate_fn=detection_collate, pin_memory=True)\n    image_ids = dataset.ids\n    save_ids = []\n    val_step = 250\n    num_images = len(dataset)\n    video_list = dataset.video_list\n    det_boxes = [[] for _ in range(len(CLASSES))]\n    gt_boxes = []\n    print_time = True\n    batch_iterator = None\n    count = 0\n    torch.cuda.synchronize()\n    ts = time.perf_counter()\n    num_batches = len(val_data_loader)\n    det_file = save_root + \'cache/\' + exp_name + \'/detection-\'+str(iteration).zfill(6)+\'.pkl\'\n    print(\'Number of images \', len(dataset),\' number of batchs\', num_batches)\n    frame_save_dir = save_root+\'detections/CONV-\'+input_type+\'-\'+args.listid+\'-\'+str(iteration).zfill(6)+\'/\'\n    print(\'\\n\\n\\nDetections will be store in \',frame_save_dir,\'\\n\\n\')\n    with torch.no_grad():\n        for val_itr in range(len(val_data_loader)):\n            if not batch_iterator:\n                batch_iterator = iter(val_data_loader)\n\n            torch.cuda.synchronize()\n            t1 = time.perf_counter()\n\n            images, targets, img_indexs = next(batch_iterator)\n            batch_size = images.size(0)\n            height, width = images.size(2), images.size(3)\n\n            if args.cuda:\n                images = images.cuda()\n            output = net(images)\n\n            loc_data = output[0]\n            conf_preds = output[1]\n            prior_data = output[2]\n\n            if print_time and val_itr%val_step == 0:\n                torch.cuda.synchronize()\n                tf = time.perf_counter()\n                print(\'Forward Time {:0.3f}\'.format(tf - t1))\n            for b in range(batch_size):\n                gt = targets[b].numpy()\n                gt[:, 0] *= width\n                gt[:, 2] *= width\n                gt[:, 1] *= height\n                gt[:, 3] *= height\n                gt_boxes.append(gt)\n                decoded_boxes = decode(loc_data[b].data, prior_data.data, cfg[\'variance\']).clone()\n                conf_scores = net.softmax(conf_preds[b]).data.clone()\n                index = img_indexs[b]\n                annot_info = image_ids[index]\n\n                frame_num = annot_info[1]; video_id = annot_info[0]; videoname = video_list[video_id]\n                output_dir = frame_save_dir+videoname\n                if not os.path.isdir(output_dir):\n                    os.makedirs(output_dir)\n\n                output_file_name = output_dir+\'/{:05d}.mat\'.format(int(frame_num))\n                save_ids.append(output_file_name)\n                sio.savemat(output_file_name, mdict={\'scores\':conf_scores.cpu().numpy(),\'loc\':decoded_boxes.cpu().numpy()})\n\n                for cl_ind in range(1, num_classes):\n                    scores = conf_scores[:, cl_ind].squeeze()\n                    c_mask = scores.gt(args.conf_thresh)  # greater than minmum threshold\n                    scores = scores[c_mask].squeeze()\n                    # print(\'scores size\',scores.size())\n                    if scores.dim() == 0:\n                        # print(len(\'\'), \' dim ==0 \')\n                        det_boxes[cl_ind - 1].append(np.asarray([]))\n                        continue\n                    boxes = decoded_boxes.clone()\n                    l_mask = c_mask.unsqueeze(1).expand_as(boxes)\n                    boxes = boxes[l_mask].view(-1, 4)\n                    # idx of highest scoring and non-overlapping boxes per class\n                    ids, counts = nms(boxes, scores, args.nms_thresh, args.topk)  # idsn - ids after nms\n                    scores = scores[ids[:counts]].cpu().numpy()\n                    boxes = boxes[ids[:counts]].cpu().numpy()\n                    # print(\'boxes sahpe\',boxes.shape)\n                    boxes[:, 0] *= width\n                    boxes[:, 2] *= width\n                    boxes[:, 1] *= height\n                    boxes[:, 3] *= height\n\n                    for ik in range(boxes.shape[0]):\n                        boxes[ik, 0] = max(0, boxes[ik, 0])\n                        boxes[ik, 2] = min(width, boxes[ik, 2])\n                        boxes[ik, 1] = max(0, boxes[ik, 1])\n                        boxes[ik, 3] = min(height, boxes[ik, 3])\n\n                    cls_dets = np.hstack((boxes, scores[:, np.newaxis])).astype(np.float32, copy=True)\n                    det_boxes[cl_ind - 1].append(cls_dets)\n\n                count += 1\n            if val_itr%val_step == 0:\n                torch.cuda.synchronize()\n                te = time.perf_counter()\n                print(\'im_detect: {:d}/{:d} time taken {:0.3f}\'.format(count, num_images, te - ts))\n                torch.cuda.synchronize()\n                ts = time.perf_counter()\n            if print_time and val_itr%val_step == 0:\n                torch.cuda.synchronize()\n                te = time.perf_counter()\n                print(\'NMS stuff Time {:0.3f}\'.format(te - tf))\n    print(\'Evaluating detections for itration number \', iteration)\n\n    #Save detection after NMS along with GT\n    with open(det_file, \'wb\') as f:\n        pickle.dump([gt_boxes, det_boxes, save_ids], f, pickle.HIGHEST_PROTOCOL)\n\n    return evaluate_detections(gt_boxes, det_boxes, CLASSES, iou_thresh=thresh)\n\n\ndef main():\n\n    means = (104, 117, 123)  # only support voc now\n\n    exp_name = \'CONV-SSD-{}-{}-bs-{}-{}-lr-{:05d}\'.format(args.dataset, args.input_type,\n                            args.batch_size, args.basenet[:-14], int(args.lr * 100000))\n\n    args.save_root += args.dataset+\'/\'\n    args.data_root += args.dataset+\'/\'\n    args.listid = \'01\' ## would be usefull in JHMDB-21\n    print(\'Exp name\', exp_name, args.listid)\n    for iteration in [int(itr) for itr in args.eval_iter.split(\',\')]:\n        log_file = open(args.save_root + \'cache/\' + exp_name + ""/testing-{:d}.log"".format(iteration), ""w"", 1)\n        log_file.write(exp_name + \'\\n\')\n        trained_model_path = args.save_root + \'cache/\' + exp_name + \'/ssd300_ucf24_\' + repr(iteration) + \'.pth\'\n        log_file.write(trained_model_path+\'\\n\')\n        num_classes = len(CLASSES) + 1  #7 +1 background\n        net = build_ssd(300, num_classes)  # initialize SSD\n        net.load_state_dict(torch.load(trained_model_path))\n        net.eval()\n        if args.cuda:\n            net = net.cuda()\n            cudnn.benchmark = True\n        print(\'Finished loading model %d !\' % iteration)\n        # Load dataset\n        dataset = UCF24Detection(args.data_root, \'test\', BaseTransform(args.ssd_dim, means), AnnotationTransform(),\n                                 input_type=args.input_type, full_test=True)\n        # evaluation\n        torch.cuda.synchronize()\n        tt0 = time.perf_counter()\n        log_file.write(\'Testing net \\n\')\n        mAP, ap_all, ap_strs = test_net(net, args.save_root, exp_name, args.input_type, dataset, iteration, num_classes)\n        for ap_str in ap_strs:\n            print(ap_str)\n            log_file.write(ap_str + \'\\n\')\n        ptr_str = \'\\nMEANAP:::=>\' + str(mAP) + \'\\n\'\n        print(ptr_str)\n        log_file.write(ptr_str)\n\n        torch.cuda.synchronize()\n        print(\'Complete set time {:0.2f}\'.format(time.perf_counter() - tt0))\n        log_file.close()\n\nif __name__ == \'__main__\':\n    main()\n'"
train-ucf24.py,31,"b'\n"""""" Adapted from:\n    @longcw faster_rcnn_pytorch: https://github.com/longcw/faster_rcnn_pytorch\n    @rbgirshick py-faster-rcnn https://github.com/rbgirshick/py-faster-rcnn\n    Which was adopated by: Ellis Brown, Max deGroot\n    https://github.com/amdegroot/ssd.pytorch\n\n    Further:\n    Updated by Gurkirt Singh for ucf101-24 dataset\n    Licensed under The MIT License [see LICENSE for details]\n""""""\n\nimport os\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.init as init\nimport argparse\nimport torch.utils.data as data\nfrom data import v2, UCF24Detection, AnnotationTransform, detection_collate, CLASSES, BaseTransform\nfrom utils.augmentations import SSDAugmentation\nfrom layers.modules import MultiBoxLoss\nfrom ssd import build_ssd\nimport numpy as np\nimport time\nfrom utils.evaluation import evaluate_detections\nfrom layers.box_utils import decode, nms\nfrom utils import  AverageMeter\nfrom torch.optim.lr_scheduler import MultiStepLR\n\ndef str2bool(v):\n    return v.lower() in (""yes"", ""true"", ""t"", ""1"")\n\n\nparser = argparse.ArgumentParser(description=\'Single Shot MultiBox Detector Training\')\nparser.add_argument(\'--version\', default=\'v2\', help=\'conv11_2(v2) or pool6(v1) as last layer\')\nparser.add_argument(\'--basenet\', default=\'vgg16_reducedfc.pth\', help=\'pretrained base model\')\nparser.add_argument(\'--dataset\', default=\'ucf24\', help=\'pretrained base model\')\nparser.add_argument(\'--ssd_dim\', default=300, type=int, help=\'Input Size for SSD\') # only support 300 now\nparser.add_argument(\'--input_type\', default=\'rgb\', type=str, help=\'INput tyep default rgb options are [rgb,brox,fastOF]\')\nparser.add_argument(\'--jaccard_threshold\', default=0.5, type=float, help=\'Min Jaccard index for matching\')\nparser.add_argument(\'--batch_size\', default=32, type=int, help=\'Batch size for training\')\nparser.add_argument(\'--resume\', default=None, type=str, help=\'Resume from checkpoint\')\nparser.add_argument(\'--num_workers\', default=4, type=int, help=\'Number of workers used in dataloading\')\nparser.add_argument(\'--max_iter\', default=120000, type=int, help=\'Number of training iterations\')\nparser.add_argument(\'--man_seed\', default=123, type=int, help=\'manualseed for reproduction\')\nparser.add_argument(\'--cuda\', default=True, type=str2bool, help=\'Use cuda to train model\')\nparser.add_argument(\'--ngpu\', default=1, type=str2bool, help=\'Use cuda to train model\')\nparser.add_argument(\'--lr\', \'--learning-rate\', default=1e-3, type=float, help=\'initial learning rate\')\nparser.add_argument(\'--momentum\', default=0.9, type=float, help=\'momentum\')\nparser.add_argument(\'--stepvalues\', default=\'30000,60000,100000\', type=str, help=\'iter numbers where learing rate to be dropped\')\nparser.add_argument(\'--weight_decay\', default=5e-4, type=float, help=\'Weight decay for SGD\')\nparser.add_argument(\'--gamma\', default=0.1, type=float, help=\'Gamma update for SGD\')\nparser.add_argument(\'--visdom\', default=False, type=str2bool, help=\'Use visdom to for loss visualization\')\nparser.add_argument(\'--vis_port\', default=8097, type=int, help=\'Port for Visdom Server\')\nparser.add_argument(\'--data_root\', default=\'/mnt/mercury-beta/\', help=\'Location of VOC root directory\')\nparser.add_argument(\'--save_root\', default=\'/mnt/mercury-beta/\', help=\'Location to save checkpoint models\')\nparser.add_argument(\'--iou_thresh\', default=0.5, type=float, help=\'Evaluation threshold\')\nparser.add_argument(\'--conf_thresh\', default=0.05, type=float, help=\'Confidence threshold for evaluation\')\nparser.add_argument(\'--nms_thresh\', default=0.45, type=float, help=\'NMS threshold\')\nparser.add_argument(\'--topk\', default=50, type=int, help=\'topk for evaluation\')\n\n## Parse arguments\nargs = parser.parse_args()\n## set random seeds\nnp.random.seed(args.man_seed)\ntorch.manual_seed(args.man_seed)\nif args.cuda:\n    torch.cuda.manual_seed_all(args.man_seed)\n\n\ntorch.set_default_tensor_type(\'torch.FloatTensor\')\n\n\ndef main():\n    args.cfg = v2\n    args.train_sets = \'train\'\n    args.means = (104, 117, 123)\n    num_classes = len(CLASSES) + 1\n    args.num_classes = num_classes\n    args.stepvalues = [int(val) for val in args.stepvalues.split(\',\')]\n    args.loss_reset_step = 30\n    args.eval_step = 10000\n    args.print_step = 10\n\n    ## Define the experiment Name will used to same directory and ENV for visdom\n    args.exp_name = \'CONV-SSD-{}-{}-bs-{}-{}-lr-{:05d}\'.format(args.dataset,\n                args.input_type, args.batch_size, args.basenet[:-14], int(args.lr*100000))\n\n    args.save_root += args.dataset+\'/\'\n    args.save_root = args.save_root+\'cache/\'+args.exp_name+\'/\'\n\n    if not os.path.isdir(args.save_root):\n        os.makedirs(args.save_root)\n\n    net = build_ssd(300, args.num_classes)\n\n    if args.cuda:\n        net = net.cuda()\n\n    def xavier(param):\n        init.xavier_uniform(param)\n\n    def weights_init(m):\n        if isinstance(m, nn.Conv2d):\n            xavier(m.weight.data)\n            m.bias.data.zero_()\n\n\n    print(\'Initializing weights for extra layers and HEADs...\')\n    # initialize newly added layers\' weights with xavier method\n    net.extras.apply(weights_init)\n    net.loc.apply(weights_init)\n    net.conf.apply(weights_init)\n\n    if args.input_type == \'fastOF\':\n        print(\'Download pretrained brox flow trained model weights and place them at:::=> \',args.data_root + \'ucf24/train_data/brox_wieghts.pth\')\n        pretrained_weights = args.data_root + \'ucf24/train_data/brox_wieghts.pth\'\n        print(\'Loading base network...\')\n        net.load_state_dict(torch.load(pretrained_weights))\n    else:\n        vgg_weights = torch.load(args.data_root +\'ucf24/train_data/\' + args.basenet)\n        print(\'Loading base network...\')\n        net.vgg.load_state_dict(vgg_weights)\n\n    args.data_root += args.dataset + \'/\'\n\n    parameter_dict = dict(net.named_parameters()) # Get parmeter of network in dictionary format wtih name being key\n    params = []\n\n    #Set different learning rate to bias layers and set their weight_decay to 0\n    for name, param in parameter_dict.items():\n        if name.find(\'bias\') > -1:\n            print(name, \'layer parameters will be trained @ {}\'.format(args.lr*2))\n            params += [{\'params\': [param], \'lr\': args.lr*2, \'weight_decay\': 0}]\n        else:\n            print(name, \'layer parameters will be trained @ {}\'.format(args.lr))\n            params += [{\'params\':[param], \'lr\': args.lr, \'weight_decay\':args.weight_decay}]\n\n    optimizer = optim.SGD(params, lr=args.lr, momentum=args.momentum, weight_decay=args.weight_decay)\n    criterion = MultiBoxLoss(args.num_classes, 0.5, True, 0, True, 3, 0.5, False, args.cuda)\n    scheduler = MultiStepLR(optimizer, milestones=args.stepvalues, gamma=args.gamma)\n    train(args, net, optimizer, criterion, scheduler)\n\n\ndef train(args, net, optimizer, criterion, scheduler):\n    log_file = open(args.save_root+""training.log"", ""w"", 1)\n    log_file.write(args.exp_name+\'\\n\')\n    for arg in vars(args):\n        print(arg, getattr(args, arg))\n        log_file.write(str(arg)+\': \'+str(getattr(args, arg))+\'\\n\')\n    log_file.write(str(net))\n    net.train()\n\n    # loss counters\n    batch_time = AverageMeter()\n    losses = AverageMeter()\n    loc_losses = AverageMeter()\n    cls_losses = AverageMeter()\n\n    print(\'Loading Dataset...\')\n    train_dataset = UCF24Detection(args.data_root, args.train_sets, SSDAugmentation(args.ssd_dim, args.means),\n                                   AnnotationTransform(), input_type=args.input_type)\n    val_dataset = UCF24Detection(args.data_root, \'test\', BaseTransform(args.ssd_dim, args.means),\n                                 AnnotationTransform(), input_type=args.input_type,\n                                 full_test=False)\n    epoch_size = len(train_dataset) // args.batch_size\n    print(\'Training SSD on\', train_dataset.name)\n\n    if args.visdom:\n\n        import visdom\n        viz = visdom.Visdom()\n        viz.port = args.vis_port\n        viz.env = args.exp_name\n        # initialize visdom loss plot\n        lot = viz.line(\n            X=torch.zeros((1,)).cpu(),\n            Y=torch.zeros((1, 6)).cpu(),\n            opts=dict(\n                xlabel=\'Iteration\',\n                ylabel=\'Loss\',\n                title=\'Current SSD Training Loss\',\n                legend=[\'REG\', \'CLS\', \'AVG\', \'S-REG\', \' S-CLS\', \' S-AVG\']\n            )\n        )\n        # initialize visdom meanAP and class APs plot\n        legends = [\'meanAP\']\n        for cls in CLASSES:\n            legends.append(cls)\n        val_lot = viz.line(\n            X=torch.zeros((1,)).cpu(),\n            Y=torch.zeros((1,args.num_classes)).cpu(),\n            opts=dict(\n                xlabel=\'Iteration\',\n                ylabel=\'Mean AP\',\n                title=\'Current SSD Validation mean AP\',\n                legend=legends\n            )\n        )\n\n\n    batch_iterator = None\n    train_data_loader = data.DataLoader(train_dataset, args.batch_size, num_workers=args.num_workers,\n                                  shuffle=True, collate_fn=detection_collate, pin_memory=True)\n    val_data_loader = data.DataLoader(val_dataset, args.batch_size, num_workers=args.num_workers,\n                                 shuffle=False, collate_fn=detection_collate, pin_memory=True)\n    itr_count = 0\n    torch.cuda.synchronize()\n    t0 = time.perf_counter()\n    iteration = 0\n    while iteration <= args.max_iter:\n        for i, (images, targets, img_indexs) in enumerate(train_data_loader):\n\n            if iteration > args.max_iter:\n                break\n            iteration += 1\n            if args.cuda:\n                images = images.cuda(0, non_blocking=True)\n                targets = [anno.cuda(0, non_blocking=True) for anno in targets]\n                \n            # forward\n            out = net(images)\n            # backprop\n            optimizer.zero_grad()\n\n            loss_l, loss_c = criterion(out, targets)\n            loss = loss_l + loss_c\n            loss.backward()\n            optimizer.step()\n            scheduler.step()\n            loc_loss = loss_l.item()\n            conf_loss = loss_c.item()\n            # print(\'Loss data type \',type(loc_loss))\n            loc_losses.update(loc_loss)\n            cls_losses.update(conf_loss)\n            losses.update((loc_loss + conf_loss)/2.0)\n\n\n            if iteration % args.print_step == 0 and iteration>0:\n                if args.visdom:\n                    losses_list = [loc_losses.val, cls_losses.val, losses.val, loc_losses.avg, cls_losses.avg, losses.avg]\n                    viz.line(X=torch.ones((1, 6)).cpu() * iteration,\n                        Y=torch.from_numpy(np.asarray(losses_list)).unsqueeze(0).cpu(),\n                        win=lot,\n                        update=\'append\')\n\n\n                torch.cuda.synchronize()\n                t1 = time.perf_counter()\n                batch_time.update(t1 - t0)\n\n                print_line = \'Itration {:06d}/{:06d} loc-loss {:.3f}({:.3f}) cls-loss {:.3f}({:.3f}) \' \\\n                             \'average-loss {:.3f}({:.3f}) Timer {:0.3f}({:0.3f})\'.format(\n                              iteration, args.max_iter, loc_losses.val, loc_losses.avg, cls_losses.val,\n                              cls_losses.avg, losses.val, losses.avg, batch_time.val, batch_time.avg)\n\n                torch.cuda.synchronize()\n                t0 = time.perf_counter()\n                log_file.write(print_line+\'\\n\')\n                print(print_line)\n\n                # if args.visdom and args.send_images_to_visdom:\n                #     random_batch_index = np.random.randint(images.size(0))\n                #     viz.image(images.data[random_batch_index].cpu().numpy())\n                itr_count += 1\n\n                if itr_count % args.loss_reset_step == 0 and itr_count > 0:\n                    loc_losses.reset()\n                    cls_losses.reset()\n                    losses.reset()\n                    batch_time.reset()\n                    print(\'Reset accumulators of \', args.exp_name,\' at\', itr_count*args.print_step)\n                    itr_count = 0\n\n            if (iteration % args.eval_step == 0 or iteration == 5000) and iteration>0:\n                torch.cuda.synchronize()\n                tvs = time.perf_counter()\n                print(\'Saving state, iter:\', iteration)\n                torch.save(net.state_dict(), args.save_root+\'ssd300_ucf24_\' +\n                           repr(iteration) + \'.pth\')\n\n                net.eval() # switch net to evaluation mode\n                mAP, ap_all, ap_strs = validate(args, net, val_data_loader, val_dataset, iteration, iou_thresh=args.iou_thresh)\n\n                for ap_str in ap_strs:\n                    print(ap_str)\n                    log_file.write(ap_str+\'\\n\')\n                ptr_str = \'\\nMEANAP:::=>\'+str(mAP)+\'\\n\'\n                print(ptr_str)\n                log_file.write(ptr_str)\n\n                if args.visdom:\n                    aps = [mAP]\n                    for ap in ap_all:\n                        aps.append(ap)\n                    viz.line(\n                        X=torch.ones((1, args.num_classes)).cpu() * iteration,\n                        Y=torch.from_numpy(np.asarray(aps)).unsqueeze(0).cpu(),\n                        win=val_lot,\n                        update=\'append\'\n                            )\n                net.train() # Switch net back to training mode\n                torch.cuda.synchronize()\n                t0 = time.perf_counter()\n                prt_str = \'\\nValidation TIME::: {:0.3f}\\n\\n\'.format(t0-tvs)\n                print(prt_str)\n                log_file.write(ptr_str)\n\n    log_file.close()\n\n\ndef validate(args, net, val_data_loader, val_dataset, iteration_num, iou_thresh=0.5):\n    """"""Test a SSD network on an image database.""""""\n    print(\'Validating at \', iteration_num)\n    num_images = len(val_dataset)\n    num_classes = args.num_classes\n\n    det_boxes = [[] for _ in range(len(CLASSES))]\n    gt_boxes = []\n    print_time = True\n    batch_iterator = None\n    val_step = 100\n    count = 0\n    torch.cuda.synchronize()\n    ts = time.perf_counter()\n    with torch.no_grad():\n        for val_itr in range(len(val_data_loader)):\n            if not batch_iterator:\n                batch_iterator = iter(val_data_loader)\n\n            torch.cuda.synchronize()\n            t1 = time.perf_counter()\n\n            images, targets, img_indexs = next(batch_iterator)\n            batch_size = images.size(0)\n            height, width = images.size(2), images.size(3)\n\n            if args.cuda:\n                images = images.cuda(0, non_blocking=True)\n            \n            output = net(images)\n\n            loc_data = output[0]\n            conf_preds = output[1]\n            prior_data = output[2]\n\n            if print_time and val_itr%val_step == 0:\n                torch.cuda.synchronize()\n                tf = time.perf_counter()\n                print(\'Forward Time {:0.3f}\'.format(tf-t1))\n            \n            for b in range(batch_size):\n                gt = targets[b].numpy()\n                gt[:,0] *= width\n                gt[:,2] *= width\n                gt[:,1] *= height\n                gt[:,3] *= height\n                gt_boxes.append(gt)\n                decoded_boxes = decode(loc_data[b].data, prior_data.data, args.cfg[\'variance\']).clone()\n                conf_scores = net.softmax(conf_preds[b]).data.clone()\n                # print(conf_scores.sum(1), conf_scores.shape)\n                for cl_ind in range(1, num_classes):\n                    scores = conf_scores[:, cl_ind].squeeze()\n                    c_mask = scores.gt(args.conf_thresh)  # greater than minmum threshold\n                    scores = scores[c_mask].squeeze()\n                    # print(\'scores size\',scores.size())\n                    if scores.dim() == 0 or scores.shape[0] == 0:\n                        # print(len(\'\'), \' dim ==0 \')\n                        det_boxes[cl_ind - 1].append(np.asarray([]))\n                        continue\n                    boxes = decoded_boxes.clone()\n                    l_mask = c_mask.unsqueeze(1).expand_as(boxes)\n                    boxes = boxes[l_mask].view(-1, 4)\n                    # idx of highest scoring and non-overlapping boxes per class\n                    ids, counts = nms(boxes, scores, args.nms_thresh, args.topk)  # idsn - ids after nms\n                    scores = scores[ids[:counts]].cpu().numpy()\n                    boxes = boxes[ids[:counts]].cpu().numpy()\n                    # print(\'boxes sahpe\',boxes.shape)\n                    boxes[:,0] *= width\n                    boxes[:,2] *= width\n                    boxes[:,1] *= height\n                    boxes[:,3] *= height\n\n                    for ik in range(boxes.shape[0]):\n                        boxes[ik, 0] = max(0, boxes[ik, 0])\n                        boxes[ik, 2] = min(width, boxes[ik, 2])\n                        boxes[ik, 1] = max(0, boxes[ik, 1])\n                        boxes[ik, 3] = min(height, boxes[ik, 3])\n\n                    cls_dets = np.hstack((boxes, scores[:, np.newaxis])).astype(np.float32, copy=True)\n\n                    det_boxes[cl_ind-1].append(cls_dets)\n                count += 1\n            if val_itr%val_step == 0:\n                torch.cuda.synchronize()\n                te = time.perf_counter()\n                print(\'im_detect: {:d}/{:d} time taken {:0.3f}\'.format(count, num_images, te-ts))\n                torch.cuda.synchronize()\n                ts = time.perf_counter()\n            if print_time and val_itr%val_step == 0:\n                torch.cuda.synchronize()\n                te = time.perf_counter()\n                print(\'NMS stuff Time {:0.3f}\'.format(te - tf))\n    print(\'Evaluating detections for itration number \', iteration_num)\n    return evaluate_detections(gt_boxes, det_boxes, CLASSES, iou_thresh=iou_thresh)\n\n\nif __name__ == \'__main__\':\n    main()\n'"
data/__init__.py,0,"b'#from .voc0712 import VOCDetection, AnnotationTransform, detection_collate, VOC_CLASSES\nfrom .ucf24 import UCF24Detection, AnnotationTransform, detection_collate, CLASSES\nfrom .config import *\nimport cv2\nimport numpy as np\n\n\ndef base_transform(image, size, mean):\n    x = cv2.resize(image, (size, size)).astype(np.float32)\n    # x = cv2.resize(np.array(image), (size, size)).astype(np.float32)\n    x -= mean\n    x = x.astype(np.float32)\n    return x\n\n\nclass BaseTransform:\n    def __init__(self, size, mean):\n        self.size = size\n        self.mean = np.array(mean, dtype=np.float32)\n\n    def __call__(self, image, boxes=None, labels=None):\n        return base_transform(image, self.size, self.mean), boxes, labels\n'"
data/config.py,0,"b'# config.py\n""""""  SSD network configs\n\nOriginal author: Ellis Brown, Max deGroot for VOC dataset\nhttps://github.com/amdegroot/ssd.pytorch\n\n""""""\n\n#SSD300 CONFIGS\n# newer version: use additional conv11_2 layer as last layer before multibox layers\nv2 = {\n    \'feature_maps\' : [38, 19, 10, 5, 3, 1],\n\n    \'min_dim\' : 300,\n\n    \'steps\' : [8, 16, 32, 64, 100, 300],\n\n    \'min_sizes\' : [30, 60, 111, 162, 213, 264],\n\n    \'max_sizes\' : [60, 111, 162, 213, 264, 315],\n\n    # \'aspect_ratios\' : [[2, 1/2], [2, 1/2, 3, 1/3], [2, 1/2, 3, 1/3],\n    #                    [2, 1/2, 3, 1/3], [2, 1/2], [2, 1/2]],\n    \'aspect_ratios\' : [[2], [2, 3], [2, 3], [2, 3], [2], [2]],\n\n    \'variance\' : [0.1, 0.2],\n\n    \'clip\' : True,\n\n    \'name\' : \'v2\',\n}\n\n# use average pooling layer as last layer before multibox layers\nv1 = {\n    \'feature_maps\' : [38, 19, 10, 5, 3, 1],\n\n    \'min_dim\' : 300,\n\n    \'steps\' : [8, 16, 32, 64, 100, 300],\n\n    \'min_sizes\' : [30, 60, 114, 168, 222, 276],\n\n    \'max_sizes\' : [-1, 114, 168, 222, 276, 330],\n\n    # \'aspect_ratios\' : [[2], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3]],\n    \'aspect_ratios\' : [[1,1,2,1/2],[1,1,2,1/2,3,1/3],[1,1,2,1/2,3,1/3],\n                        [1,1,2,1/2,3,1/3],[1,1,2,1/2,3,1/3],[1,1,2,1/2,3,1/3]],\n\n    \'variance\' : [0.1, 0.2],\n\n    \'clip\' : True,\n\n    \'name\' : \'v1\',\n}\n'"
data/ucf24.py,5,"b'""""""UCF24 Dataset Classes\n\nAuthor: Gurkirt Singh for ucf101-24 dataset\n\n""""""\n\nimport os\nimport os.path\nimport torch\nimport torch.utils.data as data\nimport cv2, pickle\nimport numpy as np\n\nCLASSES = (  # always index 0\n        \'Basketball\', \'BasketballDunk\', \'Biking\', \'CliffDiving\', \'CricketBowling\', \'Diving\', \'Fencing\',\n        \'FloorGymnastics\', \'GolfSwing\', \'HorseRiding\', \'IceDancing\', \'LongJump\', \'PoleVault\', \'RopeClimbing\',\n        \'SalsaSpin\',\'SkateBoarding\', \'Skiing\', \'Skijet\', \'SoccerJuggling\',\n        \'Surfing\', \'TennisSwing\', \'TrampolineJumping\', \'VolleyballSpiking\', \'WalkingWithDog\')\n\n\nclass AnnotationTransform(object):\n    """"""\n    Same as original\n    Transforms a VOC annotation into a Tensor of bbox coords and label index\n    Initilized with a dictionary lookup of classnames to indexes\n    Arguments:\n        class_to_ind (dict, optional): dictionary lookup of classnames -> indexes\n            (default: alphabetic indexing of UCF24\'s 24 classes)\n        keep_difficult (bool, optional): keep difficult instances or not\n            (default: False)\n        height (int): height\n        width (int): width\n    """"""\n\n    def __init__(self, class_to_ind=None, keep_difficult=False):\n        self.class_to_ind = class_to_ind or dict(\n            zip(CLASSES, range(len(CLASSES))))\n        self.ind_to_class = dict(zip(range(len(CLASSES)),CLASSES))\n\n    def __call__(self, bboxs, labels, width, height):\n        res = []\n        for t in range(len(labels)):\n            bbox = bboxs[t,:]\n            label = labels[t]\n            \'\'\'pts = [\'xmin\', \'ymin\', \'xmax\', \'ymax\']\'\'\'\n            bndbox = []\n            for i in range(4):\n                cur_pt = max(0,int(bbox[i]) - 1)\n                scale =  width if i % 2 == 0 else height\n                cur_pt = min(scale, int(bbox[i]))\n                cur_pt = float(cur_pt) / scale\n                bndbox.append(cur_pt)\n            bndbox.append(label)\n            res += [bndbox]  # [xmin, ymin, xmax, ymax, label_ind]\n            # img_id = target.find(\'filename\').text[:-4]\n        return res  # [[xmin, ymin, xmax, ymax, label_ind], ... ]\n\n\ndef readsplitfile(splitfile):\n    with open(splitfile, \'r\') as f:\n        temptrainvideos = f.readlines()\n    trainvideos = []\n    for vid in temptrainvideos:\n        vid = vid.rstrip(\'\\n\')\n        trainvideos.append(vid)\n    return trainvideos\n\n\ndef make_lists(rootpath, imgtype, split=1, fulltest=False):\n    imagesDir = rootpath + imgtype + \'/\'\n    splitfile = rootpath + \'splitfiles/trainlist{:02d}.txt\'.format(split)\n    trainvideos = readsplitfile(splitfile)\n    trainlist = []\n    testlist = []\n\n    with open(rootpath + \'splitfiles/pyannot.pkl\',\'rb\') as fff:\n        database = pickle.load(fff)\n\n    train_action_counts = np.zeros(len(CLASSES), dtype=np.int32)\n    test_action_counts = np.zeros(len(CLASSES), dtype=np.int32)\n\n    #4500ratios = np.asarray([1.1, 0.8, 4.7, 1.4, 0.9, 2.6, 2.2, 3.0, 3.0, 5.0, 6.2, 2.7,\n    #                     3.5, 3.1, 4.3, 2.5, 4.5, 3.4, 6.7, 3.6, 1.6, 3.4, 0.6, 4.3])\n    ratios = np.asarray([1.03, 0.75, 4.22, 1.32, 0.8, 2.36, 1.99, 2.66, 2.68, 4.51, 5.56, 2.46, 3.17, 2.76, 3.89, 2.28, 4.01, 3.08, 6.06, 3.28, 1.51, 3.05, 0.6, 3.84])\n    #ratios = np.ones_like(ratios) #TODO:uncomment this line and line 155, 156 to compute new ratios might be useful for JHMDB21\n    video_list = []\n    for vid, videoname in enumerate(sorted(database.keys())):\n        video_list.append(videoname)\n        actidx = database[videoname][\'label\']\n        istrain = True\n        step = ratios[actidx]\n        numf = database[videoname][\'numf\']\n        lastf = numf-1\n        if videoname not in trainvideos:\n            istrain = False\n            step = max(1, ratios[actidx])*3\n        if fulltest:\n            step = 1\n            lastf = numf\n\n        annotations = database[videoname][\'annotations\']\n        num_tubes = len(annotations)\n\n        tube_labels = np.zeros((numf,num_tubes),dtype=np.int16) # check for each tube if present in\n        tube_boxes = [[[] for _ in range(num_tubes)] for _ in range(numf)]\n        for tubeid, tube in enumerate(annotations):\n            # print(\'numf00\', numf, tube[\'sf\'], tube[\'ef\'])\n            for frame_id, frame_num in enumerate(np.arange(tube[\'sf\'], tube[\'ef\'], 1)): # start of the tube to end frame of the tube\n                label = tube[\'label\']\n                assert actidx == label, \'Tube label and video label should be same\'\n                box = tube[\'boxes\'][frame_id, :]  # get the box as an array\n                box = box.astype(np.float32)\n                box[2] += box[0]  #convert width to xmax\n                box[3] += box[1]  #converst height to ymax\n                tube_labels[frame_num, tubeid] = 1 #label+1  # change label in tube_labels matrix to 1 form 0\n                tube_boxes[frame_num][tubeid] = box  # put the box in matrix of lists\n\n        possible_frame_nums = np.arange(0, lastf, step)\n        # print(\'numf\',numf,possible_frame_nums[-1])\n        for frame_num in possible_frame_nums: # loop from start to last possible frame which can make a legit sequence\n            frame_num = int(frame_num)\n            check_tubes = tube_labels[frame_num,:]\n\n            if np.sum(check_tubes)>0:  # check if there aren\'t any semi overlapping tubes\n                all_boxes = []\n                labels = []\n                image_name = imagesDir + videoname+\'/{:05d}.jpg\'.format(frame_num+1)\n                #label_name = rootpath + \'labels/\' + videoname + \'/{:05d}.txt\'.format(frame_num + 1)\n                # assert os.path.isfile(image_name), \'Image does not exist\'+image_name\n                for tubeid, tube in enumerate(annotations):\n                    label = tube[\'label\']\n                    if tube_labels[frame_num, tubeid]>0:\n                        box = np.asarray(tube_boxes[frame_num][tubeid])\n                        all_boxes.append(box)\n                        labels.append(label)\n\n                if istrain: # if it is training video\n                    trainlist.append([vid, frame_num+1, np.asarray(labels), np.asarray(all_boxes)])\n                    train_action_counts[actidx] += 1 #len(labels)\n                else: # if test video and has micro-tubes with GT\n                    testlist.append([vid, frame_num+1, np.asarray(labels), np.asarray(all_boxes)])\n                    test_action_counts[actidx] += 1 #len(labels)\n            elif fulltest and not istrain: # if test video with no ground truth and fulltest is trues\n                testlist.append([vid, frame_num+1, np.asarray([9999]), np.zeros((1,4))])\n\n    for actidx, act_count in enumerate(train_action_counts): # just to see the distribution of train and test sets\n        print(\'train {:05d} test {:05d} action {:02d} {:s}\'.format(act_count, test_action_counts[actidx] , int(actidx), CLASSES[actidx]))\n\n    newratios = train_action_counts/5000\n    #print(\'new   ratios\', newratios)\n    line = \'[\'\n    for r in newratios:\n        line +=\'{:0.2f}, \'.format(r)\n    print(line+\']\')\n    print(\'Trainlistlen\', len(trainlist), \' testlist \', len(testlist))\n\n    return trainlist, testlist, video_list\n\n\nclass UCF24Detection(data.Dataset):\n    """"""UCF24 Action Detection Dataset\n    to access input images and target which is annotation\n    """"""\n\n    def __init__(self, root, image_set, transform=None, target_transform=None,\n                 dataset_name=\'ucf24\', input_type=\'rgb\', full_test=False):\n\n        self.input_type = input_type\n        input_type = input_type+\'-images\'\n        self.root = root\n        self.CLASSES = CLASSES\n        self.image_set = image_set\n        self.transform = transform\n        self.target_transform = target_transform\n        self.name = dataset_name\n        self._annopath = os.path.join(root, \'labels/\', \'%s.txt\')\n        self._imgpath = os.path.join(root, input_type)\n        self.ids = list()\n\n        trainlist, testlist, video_list = make_lists(root, input_type, split=1, fulltest=full_test)\n        self.video_list = video_list\n        if self.image_set == \'train\':\n            self.ids = trainlist\n        elif self.image_set == \'test\':\n            self.ids = testlist\n        else:\n            print(\'spacify correct subset \')\n\n    def __getitem__(self, index):\n        im, gt, img_index = self.pull_item(index)\n\n        return im, gt, img_index\n\n    def __len__(self):\n        return len(self.ids)\n\n    def pull_item(self, index):\n        annot_info = self.ids[index]\n        frame_num = annot_info[1]\n        video_id = annot_info[0]\n        videoname = self.video_list[video_id]\n        img_name = self._imgpath + \'/{:s}/{:05d}.jpg\'.format(videoname, frame_num)\n        # print(img_name)\n        img = cv2.imread(img_name)\n        height, width, channels = img.shape\n\n        target = self.target_transform(annot_info[3], annot_info[2], width, height)\n\n        if self.transform is not None:\n            target = np.array(target)\n            img, boxes, labels = self.transform(img, target[:, :4], target[:, 4])\n            img = img[:, :, (2, 1, 0)]\n            # img = img.transpose(2, 0, 1)\n            target = np.hstack((boxes, np.expand_dims(labels, axis=1)))\n        # print(height, width,target)\n        return torch.from_numpy(img).permute(2, 0, 1), target, index\n        # return torch.from_numpy(img), target, height, width\n\n\ndef detection_collate(batch):\n    """"""Custom collate fn for dealing with batches of images that have a different\n    number of associated object annotations (bounding boxes).\n    Arguments:\n        batch: (tuple) A tuple of tensor images and lists of annotations\n    Return:\n        A tuple containing:\n            1) (tensor) batch of images stacked on their 0 dim\n            2) (list of tensors) annotations for a given image are stacked on 0 dim\n    """"""\n\n    targets = []\n    imgs = []\n    image_ids = []\n    for sample in batch:\n        imgs.append(sample[0])\n        targets.append(torch.FloatTensor(sample[1]))\n        image_ids.append(sample[2])\n    return torch.stack(imgs, 0), targets, image_ids\n'"
layers/__init__.py,0,b'from .functions import *\nfrom .modules import *\n'
layers/box_utils.py,23,"b'"""""" Bounding box utilities\n\nOriginal author: Ellis Brown, Max deGroot for VOC dataset\nhttps://github.com/amdegroot/ssd.pytorch\n\n""""""\n\nimport torch\n\ndef point_form(boxes):\n    """""" Convert prior_boxes to (xmin, ymin, xmax, ymax)\n    representation for comparison to point form ground truth data.\n    Args:\n        boxes: (tensor) center-size default boxes from priorbox layers.\n    Return:\n        boxes: (tensor) Converted xmin, ymin, xmax, ymax form of boxes.\n    """"""\n    return torch.cat((boxes[:, :2] - boxes[:, 2:]/2,     # xmin, ymin\n                     boxes[:, :2] + boxes[:, 2:]/2), 1)  # xmax, ymax\n\n\ndef center_size(boxes):\n    """""" Convert prior_boxes to (cx, cy, w, h)\n    representation for comparison to center-size form ground truth data.\n    Args:\n        boxes: (tensor) point_form boxes\n    Return:\n        boxes: (tensor) Converted xmin, ymin, xmax, ymax form of boxes.\n    """"""\n    return torch.cat((boxes[:, 2:] + boxes[:, :2])/2,  # cx, cy\n                     boxes[:, 2:] - boxes[:, :2], 1)  # w, h\n\n\ndef intersect(box_a, box_b):\n    """""" We resize both tensors to [A,B,2] without new malloc:\n    [A,2] -> [A,1,2] -> [A,B,2]\n    [B,2] -> [1,B,2] -> [A,B,2]\n    Then we compute the area of intersect between box_a and box_b.\n    Args:\n      box_a: (tensor) bounding boxes, Shape: [A,4].\n      box_b: (tensor) bounding boxes, Shape: [B,4].\n    Return:\n      (tensor) intersection area, Shape: [A,B].\n    """"""\n    A = box_a.size(0)\n    B = box_b.size(0)\n    max_xy = torch.min(box_a[:, 2:].unsqueeze(1).expand(A, B, 2),\n                       box_b[:, 2:].unsqueeze(0).expand(A, B, 2))\n    min_xy = torch.max(box_a[:, :2].unsqueeze(1).expand(A, B, 2),\n                       box_b[:, :2].unsqueeze(0).expand(A, B, 2))\n    inter = torch.clamp((max_xy - min_xy), min=0)\n    return inter[:, :, 0] * inter[:, :, 1]\n\n\ndef jaccard(box_a, box_b):\n    """"""Compute the jaccard overlap of two sets of boxes.  The jaccard overlap\n    is simply the intersection over union of two boxes.  Here we operate on\n    ground truth boxes and default boxes.\n    E.g.:\n        A \xe2\x88\xa9 B / A \xe2\x88\xaa B = A \xe2\x88\xa9 B / (area(A) + area(B) - A \xe2\x88\xa9 B)\n    Args:\n        box_a: (tensor) Ground truth bounding boxes, Shape: [num_objects,4]\n        box_b: (tensor) Prior boxes from priorbox layers, Shape: [num_priors,4]\n    Return:\n        jaccard overlap: (tensor) Shape: [box_a.size(0), box_b.size(0)]\n    """"""\n    inter = intersect(box_a, box_b)\n    area_a = ((box_a[:, 2]-box_a[:, 0]) *\n              (box_a[:, 3]-box_a[:, 1])).unsqueeze(1).expand_as(inter)  # [A,B]\n    area_b = ((box_b[:, 2]-box_b[:, 0]) *\n              (box_b[:, 3]-box_b[:, 1])).unsqueeze(0).expand_as(inter)  # [A,B]\n    union = area_a + area_b - inter\n    return inter / union  # [A,B]\n\n\ndef match(threshold, truths, priors, variances, labels, loc_t, conf_t, idx):\n    """"""Match each prior box with the ground truth box of the highest jaccard\n    overlap, encode the bounding boxes, then return the matched indices\n    corresponding to both confidence and location preds.\n    Args:\n        threshold: (float) The overlap threshold used when mathing boxes.\n        truths: (tensor) Ground truth boxes, Shape: [num_obj, num_priors].\n        priors: (tensor) Prior boxes from priorbox layers, Shape: [n_priors,4].\n        variances: (tensor) Variances corresponding to each prior coord,\n            Shape: [num_priors, 4].\n        labels: (tensor) All the class labels for the image, Shape: [num_obj].\n        loc_t: (tensor) Tensor to be filled w/ endcoded location targets.\n        conf_t: (tensor) Tensor to be filled w/ matched indices for conf preds.\n        idx: (int) current batch index\n    Return:\n        The matched indices corresponding to 1)location and 2)confidence preds.\n    """"""\n    # jaccard index\n    overlaps = jaccard(\n        truths,\n        point_form(priors)\n    )\n    # (Bipartite Matching)\n    # [1,num_objects] best prior for each ground truth\n    best_prior_overlap, best_prior_idx = overlaps.max(1, keepdim=True)\n    # [1,num_priors] best ground truth for each prior\n    best_truth_overlap, best_truth_idx = overlaps.max(0, keepdim=True)\n    best_truth_idx.squeeze_(0)\n    best_truth_overlap.squeeze_(0)\n    best_prior_idx.squeeze_(1)\n    best_prior_overlap.squeeze_(1)\n    best_truth_overlap.index_fill_(0, best_prior_idx, 2)  # ensure best prior\n    # TODO refactor: index  best_prior_idx with long tensor\n    # ensure every gt matches with its prior of max overlap\n    for j in range(best_prior_idx.size(0)):\n        best_truth_idx[best_prior_idx[j]] = j\n    matches = truths[best_truth_idx]          # Shape: [num_priors,4]\n    conf = labels[best_truth_idx] + 1         # Shape: [num_priors]\n    conf[best_truth_overlap < threshold] = 0  # label as background\n    loc = encode(matches, priors, variances)\n    loc_t[idx] = loc    # [num_priors,4] encoded offsets to learn\n    conf_t[idx] = conf  # [num_priors] top class label for each prior\n\n\ndef encode(matched, priors, variances):\n    """"""Encode the variances from the priorbox layers into the ground truth boxes\n    we have matched (based on jaccard overlap) with the prior boxes.\n    Args:\n        matched: (tensor) Coords of ground truth for each prior in point-form\n            Shape: [num_priors, 4].\n        priors: (tensor) Prior boxes in center-offset form\n            Shape: [num_priors,4].\n        variances: (list[float]) Variances of priorboxes\n    Return:\n        encoded boxes (tensor), Shape: [num_priors, 4]\n    """"""\n\n    # dist b/t match center and prior\'s center\n    g_cxcy = (matched[:, :2] + matched[:, 2:])/2 - priors[:, :2]\n    # encode variance\n    g_cxcy /= (variances[0] * priors[:, 2:])\n    # match wh / prior wh\n    g_wh = (matched[:, 2:] - matched[:, :2]) / priors[:, 2:]\n    g_wh = torch.log(g_wh) / variances[1]\n    # return target for smooth_l1_loss\n    return torch.cat([g_cxcy, g_wh], 1)  # [num_priors,4]\n\n\n# Adapted from https://github.com/Hakuyume/chainer-ssd\ndef decode(loc, priors, variances):\n    """"""Decode locations from predictions using priors to undo\n    the encoding we did for offset regression at train time.\n    Args:\n        loc (tensor): location predictions for loc layers,\n            Shape: [num_priors,4]\n        priors (tensor): Prior boxes in center-offset form.\n            Shape: [num_priors,4].\n        variances: (list[float]) Variances of priorboxes\n    Return:\n        decoded bounding box predictions\n    """"""\n\n    boxes = torch.cat((\n        priors[:, :2] + loc[:, :2] * variances[0] * priors[:, 2:],\n        priors[:, 2:] * torch.exp(loc[:, 2:] * variances[1])), 1)\n    boxes[:, :2] -= boxes[:, 2:] / 2\n    boxes[:, 2:] += boxes[:, :2]\n    return boxes\n\n\ndef log_sum_exp(x):\n    """"""Utility function for computing log_sum_exp while determining\n    This will be used to determine unaveraged confidence loss across\n    all examples in a batch.\n    Args:\n        x (Variable(tensor)): conf_preds from conf layers\n    """"""\n    x_max = x.data.max()\n    return torch.log(torch.sum(torch.exp(x-x_max), 1, keepdim=True)) + x_max\n\n\n# Original author: Francisco Massa:\n# https://github.com/fmassa/object-detection.torch\n# Ported to PyTorch by Max deGroot (02/01/2017)\ndef nms(boxes, scores, overlap=0.5, top_k=200):\n    """"""Apply non-maximum suppression at test time to avoid detecting too many\n    overlapping bounding boxes for a given object.\n    Args:\n        boxes: (tensor) The location preds for the img, Shape: [num_priors,4].\n        scores: (tensor) The class predscores for the img, Shape:[num_priors].\n        overlap: (float) The overlap thresh for suppressing unnecessary boxes.\n        top_k: (int) The Maximum number of box preds to consider.\n    Return:\n        The indices of the kept boxes with respect to num_priors.\n    """"""\n\n    keep = scores.new(scores.size(0)).zero_().long()\n    if boxes.numel() == 0:\n        return keep\n    x1 = boxes[:, 0]\n    y1 = boxes[:, 1]\n    x2 = boxes[:, 2]\n    y2 = boxes[:, 3]\n    area = torch.mul(x2 - x1, y2 - y1)\n    v, idx = scores.sort(0)  # sort in ascending order\n    # I = I[v >= 0.01]\n    idx = idx[-top_k:]  # indices of the top-k largest vals\n    xx1 = boxes.new()\n    yy1 = boxes.new()\n    xx2 = boxes.new()\n    yy2 = boxes.new()\n    w = boxes.new()\n    h = boxes.new()\n\n    # keep = torch.Tensor()\n    count = 0\n    while idx.numel() > 0:\n        i = idx[-1]  # index of current largest val\n        # keep.append(i)\n        keep[count] = i\n        count += 1\n        if idx.size(0) == 1:\n            break\n        idx = idx[:-1]  # remove kept element from view\n        # load bboxes of next highest vals\n        torch.index_select(x1, 0, idx, out=xx1)\n        torch.index_select(y1, 0, idx, out=yy1)\n        torch.index_select(x2, 0, idx, out=xx2)\n        torch.index_select(y2, 0, idx, out=yy2)\n        # store element-wise max with next highest score\n        xx1 = torch.clamp(xx1, min=x1[i])\n        yy1 = torch.clamp(yy1, min=y1[i])\n        xx2 = torch.clamp(xx2, max=x2[i])\n        yy2 = torch.clamp(yy2, max=y2[i])\n        w.resize_as_(xx2)\n        h.resize_as_(yy2)\n        w = xx2 - xx1\n        h = yy2 - yy1\n        # check sizes of xx1 and xx2.. after each iteration\n        w = torch.clamp(w, min=0.0)\n        h = torch.clamp(h, min=0.0)\n        inter = w*h\n        # IoU = i / (area(a) + area(b) - i)\n        rem_areas = torch.index_select(area, 0, idx)  # load remaining areas)\n        union = (rem_areas - inter) + area[i]\n        IoU = inter/union  # store result in iou\n        # keep only elements with an IoU <= overlap\n        idx = idx[IoU.le(overlap)]\n    return keep, count\n'"
utils/__init__.py,0,"b'class AverageMeter(object):\n    """"""Computes and stores the average and current value""""""\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count'"
utils/augmentations.py,2,"b'\n"""""" Agumentation code for SSD network\n\nOriginal author: Ellis Brown, Max deGroot for VOC dataset\nhttps://github.com/amdegroot/ssd.pytorch\n\n""""""\n\nimport torch\nfrom torchvision import transforms\nimport cv2\nimport numpy as np\nimport types\nfrom numpy import random\n\n\ndef intersect(box_a, box_b):\n    max_xy = np.minimum(box_a[:, 2:], box_b[2:])\n    min_xy = np.maximum(box_a[:, :2], box_b[:2])\n    inter = np.clip((max_xy - min_xy), a_min=0, a_max=np.inf)\n    return inter[:, 0] * inter[:, 1]\n\n\ndef jaccard_numpy(box_a, box_b):\n    """"""Compute the jaccard overlap of two sets of boxes.  The jaccard overlap\n    is simply the intersection over union of two boxes.\n    E.g.:\n        A \xe2\x88\xa9 B / A \xe2\x88\xaa B = A \xe2\x88\xa9 B / (area(A) + area(B) - A \xe2\x88\xa9 B)\n    Args:\n        box_a: Multiple bounding boxes, Shape: [num_boxes,4]\n        box_b: Single bounding box, Shape: [4]\n    Return:\n        jaccard overlap: Shape: [box_a.shape[0], box_a.shape[1]]\n    """"""\n    inter = intersect(box_a, box_b)\n    area_a = ((box_a[:, 2]-box_a[:, 0]) *\n              (box_a[:, 3]-box_a[:, 1]))  # [A,B]\n    area_b = ((box_b[2]-box_b[0]) *\n              (box_b[3]-box_b[1]))  # [A,B]\n    union = area_a + area_b - inter\n    return inter / union  # [A,B]\n\n\nclass Compose(object):\n    """"""Composes several augmentations together.\n    Args:\n        transforms (List[Transform]): list of transforms to compose.\n    Example:\n        >>> augmentations.Compose([\n        >>>     transforms.CenterCrop(10),\n        >>>     transforms.ToTensor(),\n        >>> ])\n    """"""\n\n    def __init__(self, transforms):\n        self.transforms = transforms\n\n    def __call__(self, img, boxes=None, labels=None):\n        for t in self.transforms:\n            img, boxes, labels = t(img, boxes, labels)\n        return img, boxes, labels\n\n\nclass Lambda(object):\n    """"""Applies a lambda as a transform.""""""\n\n    def __init__(self, lambd):\n        assert isinstance(lambd, types.LambdaType)\n        self.lambd = lambd\n\n    def __call__(self, img, boxes=None, labels=None):\n        return self.lambd(img, boxes, labels)\n\n\nclass ConvertFromInts(object):\n    def __call__(self, image, boxes=None, labels=None):\n        return image.astype(np.float32), boxes, labels\n\n\nclass SubtractMeans(object):\n    def __init__(self, mean):\n        self.mean = np.array(mean, dtype=np.float32)\n\n    def __call__(self, image, boxes=None, labels=None):\n        image = image.astype(np.float32)\n        image -= self.mean\n        return image.astype(np.float32), boxes, labels\n\n\nclass ToAbsoluteCoords(object):\n    def __call__(self, image, boxes=None, labels=None):\n        height, width, channels = image.shape\n        boxes[:, 0] *= width\n        boxes[:, 2] *= width\n        boxes[:, 1] *= height\n        boxes[:, 3] *= height\n\n        return image, boxes, labels\n\n\nclass ToPercentCoords(object):\n    def __call__(self, image, boxes=None, labels=None):\n        height, width, channels = image.shape\n        boxes[:, 0] /= width\n        boxes[:, 2] /= width\n        boxes[:, 1] /= height\n        boxes[:, 3] /= height\n\n        return image, boxes, labels\n\n\nclass Resize(object):\n    def __init__(self, size=300):\n        self.size = size\n\n    def __call__(self, image, boxes=None, labels=None):\n        image = cv2.resize(image, (self.size,\n                                 self.size))\n        return image, boxes, labels\n\n\nclass RandomSaturation(object):\n    def __init__(self, lower=0.5, upper=1.5):\n        self.lower = lower\n        self.upper = upper\n        assert self.upper >= self.lower, ""contrast upper must be >= lower.""\n        assert self.lower >= 0, ""contrast lower must be non-negative.""\n\n    def __call__(self, image, boxes=None, labels=None):\n        if random.randint(2):\n            image[:, :, 1] *= random.uniform(self.lower, self.upper)\n\n        return image, boxes, labels\n\n\nclass RandomHue(object):\n    def __init__(self, delta=18.0):\n        assert delta >= 0.0 and delta <= 360.0\n        self.delta = delta\n\n    def __call__(self, image, boxes=None, labels=None):\n        if random.randint(2):\n            image[:, :, 0] += random.uniform(-self.delta, self.delta)\n            image[:, :, 0][image[:, :, 0] > 360.0] -= 360.0\n            image[:, :, 0][image[:, :, 0] < 0.0] += 360.0\n        return image, boxes, labels\n\n\nclass RandomLightingNoise(object):\n    def __init__(self):\n        self.perms = ((0, 1, 2), (0, 2, 1),\n                      (1, 0, 2), (1, 2, 0),\n                      (2, 0, 1), (2, 1, 0))\n\n    def __call__(self, image, boxes=None, labels=None):\n        if random.randint(2):\n            swap = self.perms[random.randint(len(self.perms))]\n            shuffle = SwapChannels(swap)  # shuffle channels\n            image = shuffle(image)\n        return image, boxes, labels\n\n\nclass ConvertColor(object):\n    def __init__(self, current=\'BGR\', transform=\'HSV\'):\n        self.transform = transform\n        self.current = current\n\n    def __call__(self, image, boxes=None, labels=None):\n        if self.current == \'BGR\' and self.transform == \'HSV\':\n            image = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n        elif self.current == \'HSV\' and self.transform == \'BGR\':\n            image = cv2.cvtColor(image, cv2.COLOR_HSV2BGR)\n        else:\n            raise NotImplementedError\n        return image, boxes, labels\n\n\nclass RandomContrast(object):\n    def __init__(self, lower=0.5, upper=1.5):\n        self.lower = lower\n        self.upper = upper\n        assert self.upper >= self.lower, ""contrast upper must be >= lower.""\n        assert self.lower >= 0, ""contrast lower must be non-negative.""\n\n    # expects float image\n    def __call__(self, image, boxes=None, labels=None):\n        if random.randint(2):\n            alpha = random.uniform(self.lower, self.upper)\n            image *= alpha\n        return image, boxes, labels\n\n\nclass RandomBrightness(object):\n    def __init__(self, delta=32):\n        assert delta >= 0.0\n        assert delta <= 255.0\n        self.delta = delta\n\n    def __call__(self, image, boxes=None, labels=None):\n        if random.randint(2):\n            delta = random.uniform(-self.delta, self.delta)\n            image += delta\n        return image, boxes, labels\n\n\nclass ToCV2Image(object):\n    def __call__(self, tensor, boxes=None, labels=None):\n        return tensor.cpu().numpy().astype(np.float32).transpose((1, 2, 0)), boxes, labels\n\n\nclass ToTensor(object):\n    def __call__(self, cvimage, boxes=None, labels=None):\n        return torch.from_numpy(cvimage.astype(np.float32)).permute(2, 0, 1), boxes, labels\n\n\nclass RandomSampleCrop(object):\n    """"""Crop\n    Arguments:\n        img (Image): the image being input during training\n        boxes (Tensor): the original bounding boxes in pt form\n        labels (Tensor): the class labels for each bbox\n        mode (float tuple): the min and max jaccard overlaps\n    Return:\n        (img, boxes, classes)\n            img (Image): the cropped image\n            boxes (Tensor): the adjusted bounding boxes in pt form\n            labels (Tensor): the class labels for each bbox\n    """"""\n    def __init__(self):\n        self.sample_options = (\n            # using entire original input image\n            None,\n            # sample a patch s.t. MIN jaccard w/ obj in .1,.3,.4,.7,.9\n            (0.1, None),\n            (0.3, None),\n            (0.5, None),\n            (0.7, None),\n            (0.9, None),\n            # randomly sample a patch\n            (None, None),\n        )\n\n    def __call__(self, image, boxes=None, labels=None):\n        height, width, _ = image.shape\n        while True:\n            # randomly choose a mode\n            mode = random.choice(self.sample_options)\n            if mode is None:\n                return image, boxes, labels\n\n            min_iou, max_iou = mode\n            if min_iou is None:\n                min_iou = float(\'-inf\')\n            if max_iou is None:\n                max_iou = float(\'inf\')\n\n            # max trails (50)\n            for _ in range(50):\n                current_image = image\n\n                w = random.uniform(0.3 * width, width)\n                h = random.uniform(0.3 * height, height)\n\n                # aspect ratio constraint b/t .5 & 2\n                if h / w < 0.5 or h / w > 2:\n                    continue\n\n                left = random.uniform(width - w)\n                top = random.uniform(height - h)\n\n                # convert to integer rect x1,y1,x2,y2\n                rect = np.array([int(left), int(top), int(left+w), int(top+h)])\n\n                # calculate IoU (jaccard overlap) b/t the cropped and gt boxes\n                overlap = jaccard_numpy(boxes, rect)\n\n                # is min and max overlap coniou.max() <= max_ioustraint satisfied? if not try again\n                if overlap.min() < min_iou or overlap.max() > max_iou:\n                    continue\n\n                # cut the crop from the image\n                current_image = current_image[rect[1]:rect[3], rect[0]:rect[2], :]\n\n                # keep overlap with gt box IF center in sampled patch\n                centers = (boxes[:, :2] + boxes[:, 2:]) / 2.0\n\n                # mask in all gt boxes that above and to the left of centers\n                m1 = (rect[0] < centers[:, 0]) * (rect[1] < centers[:, 1])\n\n                # mask in all gt boxes that under and to the right of centers\n                m2 = (rect[2] > centers[:, 0]) * (rect[3] > centers[:, 1])\n\n                # mask in that both m1 and m2 are true\n                mask = m1 * m2\n\n                # have any valid boxes? try again if not\n                if not mask.any():\n                    continue\n\n                # take only matching gt boxes\n                current_boxes = boxes[mask, :].copy()\n\n                # take only matching gt labels\n                current_labels = labels[mask]\n\n                # should we use the box left and top corner or the crop\'s\n                current_boxes[:, :2] = np.maximum(current_boxes[:, :2],\n                                                  rect[:2])\n                # adjust to crop (by substracting crop\'s left,top)\n                current_boxes[:, :2] -= rect[:2]\n\n                current_boxes[:, 2:] = np.minimum(current_boxes[:, 2:],\n                                                  rect[2:])\n                # adjust to crop (by substracting crop\'s left,top)\n                current_boxes[:, 2:] -= rect[:2]\n\n                return current_image, current_boxes, current_labels\n\n\nclass Expand(object):\n    def __init__(self, mean):\n        self.mean = mean\n\n    def __call__(self, image, boxes, labels):\n        if random.randint(2):\n            return image, boxes, labels\n\n        height, width, depth = image.shape\n        ratio = random.uniform(1, 4)\n        left = random.uniform(0, width*ratio - width)\n        top = random.uniform(0, height*ratio - height)\n\n        expand_image = np.zeros(\n            (int(height*ratio), int(width*ratio), depth),\n            dtype=image.dtype)\n        expand_image[:, :, :] = self.mean\n        expand_image[int(top):int(top + height),\n                     int(left):int(left + width)] = image\n        image = expand_image\n\n        boxes = boxes.copy()\n        boxes[:, :2] += (int(left), int(top))\n        boxes[:, 2:] += (int(left), int(top))\n\n        return image, boxes, labels\n\n\nclass RandomMirror(object):\n    def __call__(self, image, boxes, classes):\n        _, width, _ = image.shape\n        if random.randint(2):\n            image = image[:, ::-1]\n            boxes = boxes.copy()\n            boxes[:, 0::2] = width - boxes[:, 2::-2]\n        return image, boxes, classes\n\n\nclass SwapChannels(object):\n    """"""Transforms a tensorized image by swapping the channels in the order\n     specified in the swap tuple.\n    Args:\n        swaps (int triple): final order of channels\n            eg: (2, 1, 0)\n    """"""\n\n    def __init__(self, swaps):\n        self.swaps = swaps\n\n    def __call__(self, image):\n        """"""\n        Args:\n            image (Tensor): image tensor to be transformed\n        Return:\n            a tensor with channels swapped according to swap\n        """"""\n        # if torch.is_tensor(image):\n        #     image = image.data.cpu().numpy()\n        # else:\n        #     image = np.array(image)\n        image = image[:, :, self.swaps]\n        return image\n\n\nclass PhotometricDistort(object):\n    def __init__(self):\n        self.pd = [\n            RandomContrast(),\n            ConvertColor(transform=\'HSV\'),\n            RandomSaturation(),\n            RandomHue(),\n            ConvertColor(current=\'HSV\', transform=\'BGR\'),\n            RandomContrast()\n        ]\n        self.rand_brightness = RandomBrightness()\n        self.rand_light_noise = RandomLightingNoise()\n\n    def __call__(self, image, boxes, labels):\n        im = image.copy()\n        im, boxes, labels = self.rand_brightness(im, boxes, labels)\n        if random.randint(2):\n            distort = Compose(self.pd[:-1])\n        else:\n            distort = Compose(self.pd[1:])\n        im, boxes, labels = distort(im, boxes, labels)\n        return self.rand_light_noise(im, boxes, labels)\n\n\nclass SSDAugmentation(object):\n    def __init__(self, size=300, mean=(104, 117, 123)):\n        self.mean = mean\n        self.size = size\n        self.augment = Compose([\n            ConvertFromInts(),\n            ToAbsoluteCoords(),\n            PhotometricDistort(),\n            Expand(self.mean),\n            RandomSampleCrop(),\n            RandomMirror(),\n            ToPercentCoords(),\n            Resize(self.size),\n            SubtractMeans(self.mean)\n        ])\n\n    def __call__(self, img, boxes, labels):\n        return self.augment(img, boxes, labels)\n'"
utils/evaluation.py,0,"b'\n"""""" Evaluation code based on VOC protocol\n\nOriginal author: Ellis Brown, Max deGroot for VOC dataset\nhttps://github.com/amdegroot/ssd.pytorch\n\nUpdated by Gurkirt Singh for ucf101-24 dataset\n\n""""""\n\nimport os\nimport numpy as np\n\ndef voc_ap(rec, prec, use_07_metric=False):\n    """""" ap = voc_ap(rec, prec, [use_07_metric])\n    Compute VOC AP given precision and recall.\n    If use_07_metric is true, uses the\n    VOC 07 11 point method (default:False).\n    """"""\n    # print(\'voc_ap() - use_07_metric:=\' + str(use_07_metric))\n    if use_07_metric:\n        # 11 point metric\n        ap = 0.\n        for t in np.arange(0., 1.1, 0.1):\n            if np.sum(rec >= t) == 0:\n                p = 0\n            else:\n                p = np.max(prec[rec >= t])\n            ap = ap + p / 11.\n    else:\n        # correct AP calculation\n        # first append sentinel values at the end\n        mrec = np.concatenate(([0.], rec, [1.]))\n        mpre = np.concatenate(([0.], prec, [0.]))\n\n        # compute the precision envelope\n        for i in range(mpre.size - 1, 0, -1):\n            mpre[i - 1] = np.maximum(mpre[i - 1], mpre[i])\n\n        # to calculate area under PR curve, look for points\n        # where X axis (recall) changes value\n        i = np.where(mrec[1:] != mrec[:-1])[0]\n\n        # and sum (\\Delta recall) * prec\n        ap = np.sum((mrec[i + 1] - mrec[i]) * mpre[i + 1])\n    return ap\n\n\ndef get_gt_of_cls(gt_boxes, cls):\n    cls_gt_boxes = []\n    for i in range(len(gt_boxes)):\n        if gt_boxes[i,-1] == cls:\n            cls_gt_boxes.append(gt_boxes[i, :-1])\n    return np.asarray(cls_gt_boxes)\n\n\ndef compute_iou(cls_gt_boxes, box):\n    ious = np.zeros(cls_gt_boxes.shape[0])\n\n    for m in range(ious.shape[0]):\n        gtbox = cls_gt_boxes[m]\n\n        xmin = max(gtbox[0],box[0])\n        ymin = max(gtbox[1], box[1])\n        xmax = min(gtbox[2], box[2])\n        ymax = min(gtbox[3], box[3])\n        iw = np.maximum(xmax - xmin, 0.)\n        ih = np.maximum(ymax - ymin, 0.)\n        if iw>0 and ih>0:\n            intsc = iw*ih\n        else:\n            intsc = 0.0\n        # print (intsc)\n        union = (gtbox[2] - gtbox[0]) * (gtbox[3] - gtbox[1]) + (box[2] - box[0]) * (box[3] - box[1]) - intsc\n        ious[m] = intsc/union\n\n    return ious\n\ndef evaluate_detections(gt_boxes, det_boxes, CLASSES=[], iou_thresh=0.5):\n\n    ap_strs = []\n    num_frames = len(gt_boxes)\n    print(\'Evaluating for \', num_frames, \'frames\')\n    ap_all = np.zeros(len(CLASSES), dtype=np.float32)\n    for cls_ind, cls in enumerate(CLASSES): # loop over each class \'cls\'\n        scores = np.zeros(num_frames * 220)\n        istp = np.zeros(num_frames * 220)\n        det_count = 0\n        num_postives = 0.0\n        for nf in range(num_frames): # loop over each frame \'nf\'\n                # if len(gt_boxes[nf])>0 and len(det_boxes[cls_ind][nf]):\n                frame_det_boxes = np.copy(det_boxes[cls_ind][nf]) # get frame detections for class cls in nf\n                cls_gt_boxes = get_gt_of_cls(np.copy(gt_boxes[nf]), cls_ind) # get gt boxes for class cls in nf frame\n                num_postives += cls_gt_boxes.shape[0]\n                if frame_det_boxes.shape[0]>0: # check if there are dection for class cls in nf frame\n                    argsort_scores = np.argsort(-frame_det_boxes[:,-1]) # sort in descending order\n                    for i, k in enumerate(argsort_scores): # start from best scoring detection of cls to end\n                        box = frame_det_boxes[k, :-1] # detection bounfing box\n                        score = frame_det_boxes[k,-1] # detection score\n                        ispositive = False # set ispostive to false every time\n                        if cls_gt_boxes.shape[0]>0: # we can only find a postive detection\n                            # if there is atleast one gt bounding for class cls is there in frame nf\n                            iou = compute_iou(cls_gt_boxes, box) # compute IOU between remaining gt boxes\n                            # and detection boxes\n                            maxid = np.argmax(iou)  # get the max IOU window gt index\n                            if iou[maxid] >= iou_thresh: # check is max IOU is greater than detection threshold\n                                ispositive = True # if yes then this is ture positive detection\n                                cls_gt_boxes = np.delete(cls_gt_boxes, maxid, 0) # remove assigned gt box\n                        scores[det_count] = score # fill score array with score of current detection\n                        if ispositive:\n                            istp[det_count] = 1 # set current detection index (det_count)\n                            #  to 1 if it is true postive example\n                        det_count += 1\n        if num_postives<1:\n            num_postives =1\n        scores = scores[:det_count]\n        istp = istp[:det_count]\n        argsort_scores = np.argsort(-scores) # sort in descending order\n        istp = istp[argsort_scores] # reorder istp\'s on score sorting\n        fp = np.cumsum(istp == 0) # get false positives\n        tp = np.cumsum(istp == 1) # get  true positives\n        fp = fp.astype(np.float64)\n        tp = tp.astype(np.float64)\n        recall = tp / float(num_postives) # compute recall\n        precision = tp / np.maximum(tp + fp, np.finfo(np.float64).eps) # compute precision\n        cls_ap = voc_ap(recall, precision) # compute average precision using voc2007 metric\n        ap_all[cls_ind] = cls_ap\n        # print(cls_ind,CLASSES[cls_ind], cls_ap)\n        ap_str = str(CLASSES[cls_ind]) + \' : \' + str(num_postives) + \' : \' + str(det_count) + \' : \' + str(cls_ap)\n        ap_strs.append(ap_str)\n\n    # print (\'mean ap \', np.mean(ap_all))\n    return np.mean(ap_all), ap_all, ap_strs\n\n\ndef save_detection_framewise(det_boxes, image_ids, iteration):\n    det_save_dir = \'/mnt/mars-beta/gur-workspace/use-ssd-data/UCF101/detections/RGB-01-{:06d}/\'.format(iteration)\n    print(\'Saving detections to\', det_save_dir)\n    num_images = len(image_ids)\n    for idx in range(num_images):\n        img_id = image_ids[idx]\n        save_path = det_save_dir+img_id[:-5]\n        if not os.path.isdir(save_path):\n            os.system(\'mkdir -p \'+save_path)\n        fid = open(det_save_dir+img_id+\'.txt\',\'w\')\n        for cls_ind in range(len(det_boxes)):\n            frame_det_boxes = det_boxes[cls_ind][idx]\n            for d in range(len(frame_det_boxes)):\n                line = str(cls_ind+1)\n                for k in range(5):\n                    line += \' {:f}\'.format(frame_det_boxes[d,k])\n                line += \'\\n\'\n                fid.write(line)\n        fid.close()\n\n'"
layers/functions/__init__.py,0,"b""\nfrom .prior_box import PriorBox\n\n\n__all__ = ['PriorBox']\n"""
layers/functions/prior_box.py,1,"b'"""""" Generates prior boxes for SSD netowrk\n\nOriginal author: Ellis Brown, Max deGroot for VOC dataset\nhttps://github.com/amdegroot/ssd.pytorch\n\n""""""\n\nimport torch\nfrom math import sqrt as sqrt\nfrom itertools import product as product\n\nclass PriorBox(object):\n    """"""Compute priorbox coordinates in center-offset form for each source\n    feature map.\n    Note:\n    This \'layer\' has changed between versions of the original SSD\n    paper, so we include both versions, but note v2 is the most tested and most\n    recent version of the paper.\n\n    """"""\n    def __init__(self, cfg):\n        super(PriorBox, self).__init__()\n        # self.type = cfg.name\n        self.image_size = cfg[\'min_dim\']\n        # number of priors for feature map location (either 4 or 6)\n        self.num_priors = len(cfg[\'aspect_ratios\'])\n        self.variance = cfg[\'variance\'] or [0.1]\n        self.feature_maps = cfg[\'feature_maps\']\n        self.min_sizes = cfg[\'min_sizes\']\n        self.max_sizes = cfg[\'max_sizes\']\n        self.steps = cfg[\'steps\']\n        self.aspect_ratios = cfg[\'aspect_ratios\']\n        self.clip = cfg[\'clip\']\n        self.version = cfg[\'name\']\n        for v in self.variance:\n            if v <= 0:\n                raise ValueError(\'Variances must be greater than 0\')\n\n    def forward(self):\n        mean = []\n        # TODO merge these\n        if self.version == \'v2\':\n            for k, f in enumerate(self.feature_maps):\n                for i, j in product(range(f), repeat=2):\n                    f_k = self.image_size / self.steps[k]\n                    # unit center x,y\n                    cx = (j + 0.5) / f_k\n                    cy = (i + 0.5) / f_k\n\n                    # aspect_ratio: 1\n                    # rel size: min_size\n                    s_k = self.min_sizes[k]/self.image_size\n                    mean += [cx, cy, s_k, s_k]\n\n                    # aspect_ratio: 1\n                    # rel size: sqrt(s_k * s_(k+1))\n                    s_k_prime = sqrt(s_k * (self.max_sizes[k]/self.image_size))\n                    mean += [cx, cy, s_k_prime, s_k_prime]\n\n                    # rest of aspect ratios\n                    for ar in self.aspect_ratios[k]:\n                        mean += [cx, cy, s_k*sqrt(ar), s_k/sqrt(ar)]\n                        mean += [cx, cy, s_k/sqrt(ar), s_k*sqrt(ar)]\n\n        else:\n            # original version generation of prior (default) boxes\n            for i, k in enumerate(self.feature_maps):\n                step_x = step_y = self.image_size/k\n                for h, w in product(range(k), repeat=2):\n                    c_x = ((w+0.5) * step_x)\n                    c_y = ((h+0.5) * step_y)\n                    c_w = c_h = self.min_sizes[i] / 2\n                    s_k = self.image_size  # 300\n                    # aspect_ratio: 1,\n                    # size: min_size\n                    mean += [(c_x-c_w)/s_k, (c_y-c_h)/s_k,\n                             (c_x+c_w)/s_k, (c_y+c_h)/s_k]\n                    if self.max_sizes[i] > 0:\n                        # aspect_ratio: 1\n                        # size: sqrt(min_size * max_size)/2\n                        c_w = c_h = sqrt(self.min_sizes[i] *\n                                         self.max_sizes[i])/2\n                        mean += [(c_x-c_w)/s_k, (c_y-c_h)/s_k,\n                                 (c_x+c_w)/s_k, (c_y+c_h)/s_k]\n                    # rest of prior boxes\n                    for ar in self.aspect_ratios[i]:\n                        if not (abs(ar-1) < 1e-6):\n                            c_w = self.min_sizes[i] * sqrt(ar)/2\n                            c_h = self.min_sizes[i] / sqrt(ar)/2\n                            mean += [(c_x-c_w)/s_k, (c_y-c_h)/s_k,\n                                     (c_x+c_w)/s_k, (c_y+c_h)/s_k]\n        # back to torch land\n        output = torch.Tensor(mean).view(-1, 4)\n        if self.clip:\n            output.clamp_(max=1, min=0)\n        return output\n'"
layers/modules/__init__.py,0,"b""from .l2norm import L2Norm\nfrom .multibox_loss import MultiBoxLoss\n\n__all__ = ['L2Norm', 'MultiBoxLoss']\n"""
layers/modules/l2norm.py,4,"b'\nimport torch.nn as nn\nimport torch\nimport torch.nn.functional as F\n\n# class L2Norm(nn.Module):\n#     def __init__(self,n_channels, scale):\n#         super(L2Norm,self).__init__()\n#         self.n_channels = n_channels\n#         self.gamma = scale or None\n#         self.eps = 1e-10\n#         self.weight = nn.Parameter(torch.Tensor(self.n_channels))\n#         self.reset_parameters()\n\n#     def reset_parameters(self):\n#         init.constant(self.weight,self.gamma)\n\n#     def forward(self, x):\n#         norm = x.pow(2).sum(dim=1, keepdim=True).sqrt()+self.eps\n#         x /= norm\n#         out = self.weight.unsqueeze(0).unsqueeze(2).unsqueeze(3).expand_as(x) * x\n#         return out\n\nclass L2Norm(nn.Module):\n    def __init__(self, in_channels, initial_scale):\n        super(L2Norm, self).__init__()\n        self.in_channels = in_channels\n        self.weight = nn.Parameter(torch.Tensor(in_channels))\n        self.initial_scale = initial_scale\n        self.reset_parameters()\n\n    def forward(self, x):\n        return (F.normalize(x, p=2, dim=1)\n                * self.weight.unsqueeze(0).unsqueeze(2).unsqueeze(3))\n\n    def reset_parameters(self):\n        self.weight.data.fill_(self.initial_scale)'"
layers/modules/multibox_loss.py,12,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom data import v2 as cfg\nfrom ..box_utils import match, log_sum_exp\n\nclass MultiBoxLoss(nn.Module):\n    """"""SSD Weighted Loss Function\n    Compute Targets:\n        1) Produce Confidence Target Indices by matching  ground truth boxes\n           with (default) \'priorboxes\' that have jaccard index > threshold parameter\n           (default threshold: 0.5).\n        2) Produce localization target by \'encoding\' variance into offsets of ground\n           truth boxes and their matched  \'priorboxes\'.\n        3) Hard negative mining to filter the excessive number of negative examples\n           that comes with using a large number of default bounding boxes.\n           (default negative:positive ratio 3:1)\n    Objective Loss:\n        L(x,c,l,g) = (Lconf(x, c) + \xce\xb1Lloc(x,l,g)) / N\n        Where, Lconf is the CrossEntropy Loss and Lloc is the SmoothL1 Loss\n        weighted by \xce\xb1 which is set to 1 by cross val.\n        Args:\n            c: class confidences,\n            l: predicted boxes,\n            g: ground truth boxes\n            N: number of matched default boxes\n        See: https://arxiv.org/pdf/1512.02325.pdf for more details.\n    """"""\n\n    def __init__(self, num_classes, overlap_thresh, prior_for_matching,\n                 bkg_label, neg_mining, neg_pos, neg_overlap, encode_target,\n                 use_gpu=True):\n        super(MultiBoxLoss, self).__init__()\n        self.use_gpu = use_gpu\n        self.num_classes = num_classes\n        self.threshold = overlap_thresh\n        self.background_label = bkg_label\n        self.encode_target = encode_target\n        self.use_prior_for_matching = prior_for_matching\n        self.do_neg_mining = neg_mining\n        self.negpos_ratio = neg_pos\n        self.neg_overlap = neg_overlap\n        self.variance = cfg[\'variance\']\n\n    def forward(self, predictions, targets):\n        """"""Multibox Loss\n        Args:\n            predictions (tuple): A tuple containing loc preds, conf preds,\n            and prior boxes from SSD net.\n                conf shape: torch.size(batch_size,num_priors,num_classes)\n                loc shape: torch.size(batch_size,num_priors,4)\n                priors shape: torch.size(num_priors,4)\n\n            ground_truth (tensor): Ground truth boxes and labels for a batch,\n                shape: [batch_size,num_objs,5] (last idx is the label).\n        """"""\n        loc_data, conf_data, priors = predictions\n        num = loc_data.size(0)\n        priors = priors[:loc_data.size(1), :]\n        num_priors = (priors.size(0))\n        num_classes = self.num_classes\n\n        # match priors (default boxes) and ground truth boxes\n        with torch.no_grad():\n            if self.use_gpu:\n                loc_t = torch.cuda.FloatTensor(num, num_priors, 4)\n                conf_t = torch.cuda.LongTensor(num, num_priors)\n            else:\n                loc_t = torch.Tensor(num, num_priors, 4)\n                conf_t = torch.LongTensor(num, num_priors)\n            for idx in range(num):\n                truths = targets[idx][:, :-1].data\n                labels = targets[idx][:, -1].data\n                defaults = priors.data\n                match(self.threshold, truths, defaults, self.variance, labels,\n                    loc_t, conf_t, idx)\n            if self.use_gpu:\n                loc_t = loc_t.cuda()\n                conf_t = conf_t.cuda()\n            # wrap targets\n            # loc_t = Variable(loc_t, requires_grad=False)\n            # conf_t = Variable(conf_t, requires_grad=False)\n\n            pos = conf_t > 0\n        #num_pos = pos.sum(keepdim=True)\n\n        # Localization Loss (Smooth L1)\n        # Shape: [batch,num_priors,4]\n        pos_idx = pos.unsqueeze(pos.dim()).expand_as(loc_data)\n        loc_p = loc_data[pos_idx].view(-1, 4)\n        loc_t = loc_t[pos_idx].view(-1, 4)\n        loss_l = F.smooth_l1_loss(loc_p, loc_t, reduction=\'sum\')\n        with torch.no_grad():\n            # Compute max conf across batch for hard negative mining\n            batch_conf = conf_data.view(-1, self.num_classes)\n\n            loss_c = log_sum_exp(batch_conf) - batch_conf.gather(1, conf_t.view(-1, 1))\n\n            # Hard Negative Mining\n            loss_c[pos.view(-1,1)] = 0  # filter out pos boxes for now\n            loss_c = loss_c.view(num, -1)\n            _, loss_idx = loss_c.sort(1, descending=True)\n            _, idx_rank = loss_idx.sort(1)\n            num_pos = pos.long().sum(1, keepdim=True)\n            num_neg = torch.clamp(self.negpos_ratio*num_pos, max=pos.size(1)-1)\n            neg = idx_rank < num_neg.expand_as(idx_rank)\n\n            # Confidence Loss Including Positive and Negative Examples\n            pos_idx = pos.unsqueeze(2).expand_as(conf_data)\n            neg_idx = neg.unsqueeze(2).expand_as(conf_data)\n\n        conf_p = conf_data[(pos_idx+neg_idx).gt(0)].view(-1, self.num_classes)\n        targets_weighted = conf_t[(pos+neg).gt(0)]\n        loss_c = F.cross_entropy(conf_p, targets_weighted, reduction=\'sum\')\n\n        # Sum of losses: L(x,c,l,g) = (Lconf(x, c) + \xce\xb1Lloc(x,l,g)) / N\n\n        N = float(num_pos.data.sum())\n        loss_l /= N\n        loss_c /= N\n        return loss_l, loss_c\n'"
