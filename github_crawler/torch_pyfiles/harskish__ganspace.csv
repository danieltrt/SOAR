file_path,api_count,code
TkTorchWindow.py,6,"b'# Copyright 2020 Erik H\xc3\xa4rk\xc3\xb6nen. All rights reserved.\n# This file is licensed to you under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License. You may obtain a copy\n# of the License at http://www.apache.org/licenses/LICENSE-2.0\n\n# Unless required by applicable law or agreed to in writing, software distributed under\n# the License is distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR REPRESENTATIONS\n# OF ANY KIND, either express or implied. See the License for the specific language\n# governing permissions and limitations under the License.\n\nimport tkinter as tk\nimport numpy as np\nimport time\nfrom contextlib import contextmanager\nimport pycuda.driver\nfrom pycuda.gl import graphics_map_flags\nfrom glumpy import gloo, gl\nfrom pyopengltk import OpenGLFrame\nimport torch\nfrom torch.autograd import Variable\n\n# TkInter widget that can draw torch tensors directly from GPU memory\n\n@contextmanager\ndef cuda_activate(img):\n    """"""Context manager simplifying use of pycuda.gl.RegisteredImage""""""\n    mapping = img.map()\n    yield mapping.array(0,0)\n    mapping.unmap()\n\ndef create_shared_texture(w, h, c=4,\n        map_flags=graphics_map_flags.WRITE_DISCARD,\n        dtype=np.uint8):\n    """"""Create and return a Texture2D with gloo and pycuda views.""""""\n    tex = np.zeros((h,w,c), dtype).view(gloo.Texture2D)\n    tex.activate() # force gloo to create on GPU\n    tex.deactivate()\n    cuda_buffer = pycuda.gl.RegisteredImage(\n        int(tex.handle), tex.target, map_flags)\n    return tex, cuda_buffer\n\n# Shape batch as square if possible\ndef get_grid_dims(B):\n    S = int(B**0.5 + 0.5)\n    while B % S != 0:\n        S -= 1\n    return (B // S, S)\n\ndef create_gl_texture(tensor_shape):\n    if len(tensor_shape) != 4:\n        raise RuntimeError(\'Please provide a tensor of shape NCHW\')\n    \n    N, C, H, W = tensor_shape\n\n    cols, rows = get_grid_dims(N)\n    tex, cuda_buffer = create_shared_texture(W*cols, H*rows, 4)\n\n    return tex, cuda_buffer\n\n# Create window with OpenGL context\nclass TorchImageView(OpenGLFrame):\n    def __init__(self, root = None, show_fps=True, **kwargs):\n        self.root = root or tk.Tk()\n        self.width = kwargs.get(\'width\', 512)\n        self.height = kwargs.get(\'height\', 512)\n        self.show_fps = show_fps\n        self.pycuda_initialized = False\n        self.animate = 0 # disable internal main loop\n        OpenGLFrame.__init__(self, root, **kwargs)\n\n    # Called by pyopengltk.BaseOpenGLFrame\n    # when the frame goes onto the screen\n    def initgl(self):\n        if not self.pycuda_initialized:\n            self.setup_gl(self.width, self.height)\n            self.pycuda_initialized = True\n        \n        """"""Initalize gl states when the frame is created""""""\n        gl.glViewport(0, 0, self.width, self.height)\n        gl.glClearColor(0.0, 0.0, 0.0, 0.0)\n        self.dt_history = [1000/60]\n        self.t0 = time.time()\n        self.t_last = self.t0\n        self.nframes = 0\n\n    def setup_gl(self, width, height):\n        # setup pycuda and torch\n        import pycuda.gl.autoinit\n        import pycuda.gl\n\n        assert torch.cuda.is_available(), ""PyTorch: CUDA is not available""\n        print(\'Using GPU {}\'.format(torch.cuda.current_device()))\n        \n        # Create tensor to be shared between GL and CUDA\n        # Always overwritten so no sharing is necessary\n        dummy = torch.cuda.FloatTensor((1))\n        dummy.uniform_()\n        dummy = Variable(dummy)\n        \n        # Create a buffer with pycuda and gloo views, using tensor created above\n        self.tex, self.cuda_buffer = create_gl_texture((1, 3, width, height))\n        \n        # create a shader to program to draw to the screen\n        vertex = """"""\n        uniform float scale;\n        attribute vec2 position;\n        attribute vec2 texcoord;\n        varying vec2 v_texcoord;\n        void main()\n        {\n            v_texcoord = texcoord;\n            gl_Position = vec4(scale*position, 0.0, 1.0);\n        } """"""\n        fragment = """"""\n        uniform sampler2D tex;\n        varying vec2 v_texcoord;\n        void main()\n        {\n            gl_FragColor = texture2D(tex, v_texcoord);\n        } """"""\n        # Build the program and corresponding buffers (with 4 vertices)\n        self.screen = gloo.Program(vertex, fragment, count=4)\n        \n        # NDC coordinates:         Texcoords:          Vertex order,\n        # (-1, +1)       (+1, +1)   (0,0)     (1,0)    triangle strip:\n        #        +-------+               +----+          1----3\n        #        |  NDC  |               |    |          |  / | \n        #        | SPACE |               |    |          | /  |\n        #        +-------+               +----+          2----4\n        # (-1, -1)       (+1, -1)   (0,1)     (1,1)\n        \n        # Upload data to GPU\n        self.screen[\'position\'] = [(-1,+1), (-1,-1), (+1,+1), (+1,-1)]\n        self.screen[\'texcoord\'] = [(0,0), (0,1), (1,0), (1,1)]\n        self.screen[\'scale\'] = 1.0\n        self.screen[\'tex\'] = self.tex\n\n    # Don\'t call directly, use update() instead\n    def redraw(self):\n        t_now = time.time()\n        dt = t_now - self.t_last\n        self.t_last = t_now\n\n        self.dt_history = ([dt] + self.dt_history)[:50]\n        dt_mean = sum(self.dt_history) / len(self.dt_history)\n\n        if self.show_fps and self.nframes % 60 == 0:\n            self.master.title(\'FPS: {:.0f}\'.format(1 / dt_mean))\n\n    def draw(self, img):\n        assert len(img.shape) == 4, ""Please provide an NCHW image tensor""\n        assert img.device.type == ""cuda"", ""Please provide a CUDA tensor""\n\n        if img.dtype.is_floating_point:\n            img = (255*img).byte()\n        \n        # Tile images\n        N, C, H, W = img.shape\n\n        if N > 1:\n            cols, rows = get_grid_dims(N)\n            img = img.reshape(cols, rows, C, H, W)\n            img = img.permute(2, 1, 3, 0, 4) # [C, rows, H, cols, W]\n            img = img.reshape(1, C, rows*H, cols*W)\n\n        tensor = img.squeeze().permute(1, 2, 0).data # CHW => HWC\n        if C == 3:\n            tensor = torch.cat((tensor, tensor[:,:,0:1]),2) # add the alpha channel\n            tensor[:,:,3] = 1 # set alpha\n        \n        tensor = tensor.contiguous()\n\n        tex_h, tex_w, _ = self.tex.shape\n        tensor_h, tensor_w, _ = tensor.shape\n\n        if (tex_h, tex_w) != (tensor_h, tensor_w):\n            print(f\'Resizing texture to {tensor_w}*{tensor_h}\')\n            self.tex, self.cuda_buffer = create_gl_texture((N, C, H, W)) # original shape\n            self.screen[\'tex\'] = self.tex\n\n        # copy from torch into buffer\n        assert self.tex.nbytes == tensor.numel()*tensor.element_size(), ""Tensor and texture shape mismatch!""\n        with cuda_activate(self.cuda_buffer) as ary:\n            cpy = pycuda.driver.Memcpy2D()\n            cpy.set_src_device(tensor.data_ptr())\n            cpy.set_dst_array(ary)\n            cpy.width_in_bytes = cpy.src_pitch = cpy.dst_pitch = self.tex.nbytes//tensor_h\n            cpy.height = tensor_h\n            cpy(aligned=False)\n            torch.cuda.synchronize()\n        \n        # draw to screen\n        self.screen.draw(gl.GL_TRIANGLE_STRIP)\n\n    def update(self):\n        self.update_idletasks()\n        self.tkMakeCurrent()\n        self.redraw()\n        self.tkSwapBuffers()\n\n# USAGE:\n# root = tk.Tk()\n# iv = TorchImageView(root, width=512, height=512)\n# iv.pack(fill=\'both\', expand=True)\n# while True:\n#     iv.draw(nchw_tensor)\n#     root.update()\n#     iv.update()'"
config.py,0,"b'# Copyright 2020 Erik H\xc3\xa4rk\xc3\xb6nen. All rights reserved.\n# This file is licensed to you under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License. You may obtain a copy\n# of the License at http://www.apache.org/licenses/LICENSE-2.0\n\n# Unless required by applicable law or agreed to in writing, software distributed under\n# the License is distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR REPRESENTATIONS\n# OF ANY KIND, either express or implied. See the License for the specific language\n# governing permissions and limitations under the License.\n\nimport sys\nimport argparse\nimport json\nfrom copy import deepcopy\n\nclass Config:\n    def __init__(self, **kwargs):\n        self.from_args([]) # set all defaults\n        self.default_args = deepcopy(self.__dict__)\n        self.from_dict(kwargs) # override\n\n    def __str__(self):\n        custom = {}\n        default = {}\n\n        # Find non-default arguments\n        for k, v in self.__dict__.items():\n            if k == \'default_args\':\n                continue\n            \n            in_default = k in self.default_args\n            same_value = self.default_args.get(k) == v\n            \n            if in_default and same_value:\n                default[k] = v\n            else:\n                custom[k] = v\n\n        config = {\n            \'custom\': custom,\n            \'default\': default\n        }\n\n        return json.dumps(config, indent=4)\n    \n    def __repr__(self):\n        return self.__str__()\n    \n    def from_dict(self, dictionary):\n        for k, v in dictionary.items():\n            setattr(self, k, v)\n        return self\n    \n    def from_args(self, args=sys.argv[1:]):\n        parser = argparse.ArgumentParser(description=\'GAN component analysis config\')\n        parser.add_argument(\'--model\', dest=\'model\', type=str, default=\'StyleGAN\', help=\'The network to analyze\') # StyleGAN, DCGAN, ProGAN, BigGAN-XYZ\n        parser.add_argument(\'--layer\', dest=\'layer\', type=str, default=\'g_mapping\', help=\'The layer to analyze\')\n        parser.add_argument(\'--class\', dest=\'output_class\', type=str, default=None, help=\'Output class to generate (BigGAN: Imagenet, ProGAN: LSUN)\')\n        parser.add_argument(\'--est\', dest=\'estimator\', type=str, default=\'ipca\', help=\'The algorithm to use [pca, fbpca, cupca, spca, ica]\')\n        parser.add_argument(\'--sparsity\', type=float, default=1.0, help=\'Sparsity parameter of SPCA\')\n        parser.add_argument(\'--video\', dest=\'make_video\', action=\'store_true\', help=\'Generate output videos (MP4s)\')\n        parser.add_argument(\'--batch\', dest=\'batch_mode\', action=\'store_true\', help=""Don\'t open windows, instead save results to file"")\n        parser.add_argument(\'-b\', dest=\'batch_size\', type=int, default=None, help=\'Minibatch size, leave empty for automatic detection\')\n        parser.add_argument(\'-c\', dest=\'components\', type=int, default=80, help=\'Number of components to keep\')\n        parser.add_argument(\'-n\', type=int, default=300_000, help=\'Number of examples to use in decomposition\')\n        parser.add_argument(\'--use_w\', action=\'store_true\', help=\'Use W latent space (StyleGAN(2))\')\n        parser.add_argument(\'--sigma\', type=float, default=2.0, help=\'Number of stdevs to walk in visualize.py\')\n        parser.add_argument(\'--inputs\', type=str, default=None, help=\'Path to directory with named components\')\n        parser.add_argument(\'--seed\', type=int, default=None, help=\'Seed used in decomposition\')\n        args = parser.parse_args(args)\n\n        return self.from_dict(args.__dict__)'"
decomposition.py,20,"b'# Copyright 2020 Erik H\xc3\xa4rk\xc3\xb6nen. All rights reserved.\n# This file is licensed to you under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License. You may obtain a copy\n# of the License at http://www.apache.org/licenses/LICENSE-2.0\n\n# Unless required by applicable law or agreed to in writing, software distributed under\n# the License is distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR REPRESENTATIONS\n# OF ANY KIND, either express or implied. See the License for the specific language\n# governing permissions and limitations under the License.\n\n# Patch for broken CTRL+C handler\n# https://github.com/ContinuumIO/anaconda-issues/issues/905\nimport os\nos.environ[\'FOR_DISABLE_CONSOLE_CTRL_HANDLER\'] = \'1\'\n\nimport numpy as np\nimport os\nfrom pathlib import Path\nimport re\nimport sys\nimport datetime\nimport argparse\nimport torch\nimport json\nfrom types import SimpleNamespace\nfrom scipy.cluster.vq import kmeans\nfrom tqdm import trange\nfrom netdissect.nethook import InstrumentedModel\nfrom config import Config\nfrom estimators import get_estimator\nfrom models import get_instrumented_model\n\nSEED_SAMPLING = 1\nSEED_RANDOM_DIRS = 2\nSEED_LINREG = 3\nSEED_VISUALIZATION = 5\n\nB = 20\nn_clusters = 500\n\ndef get_random_dirs(components, dimensions):\n    gen = np.random.RandomState(seed=SEED_RANDOM_DIRS)\n    dirs = gen.normal(size=(components, dimensions))\n    dirs /= np.sqrt(np.sum(dirs**2, axis=1, keepdims=True))\n    return dirs.astype(np.float32)\n\n# Compute maximum batch size for given VRAM and network\ndef get_max_batch_size(inst, device, layer_name=None):\n    inst.remove_edits()\n\n    # Reset statistics\n    torch.cuda.reset_max_memory_cached(device)\n    torch.cuda.reset_max_memory_allocated(device)\n    total_mem = torch.cuda.get_device_properties(device).total_memory\n\n    B_max = 20\n\n    # Measure actual usage\n    for i in range(2, B_max, 2):\n        z = inst.model.sample_latent(n_samples=i)\n        if layer_name:\n            inst.model.partial_forward(z, layer_name)\n        else:\n            inst.model.forward(z)\n\n        maxmem = torch.cuda.max_memory_allocated(device)\n        del z\n\n        if maxmem > 0.5*total_mem:\n            print(\'Batch size {:d}: memory usage {:.0f}MB\'.format(i, maxmem / 1e6))\n            return i\n\n    return B_max\n\n# Solve for directions in latent space that match PCs in activaiton space\ndef linreg_lstsq(comp_np, mean_np, stdev_np, inst, config):\n    print(\'Performing least squares regression\', flush=True)\n\n    torch.manual_seed(SEED_LINREG)\n    np.random.seed(SEED_LINREG)\n\n    comp = torch.from_numpy(comp_np).float().to(inst.model.device)\n    mean = torch.from_numpy(mean_np).float().to(inst.model.device)\n    stdev = torch.from_numpy(stdev_np).float().to(inst.model.device)\n\n    n_samp = max(10_000, config.n) // B * B # make divisible\n    n_comp = comp.shape[0]\n    latent_dims = inst.model.get_latent_dims()\n    \n    # We\'re looking for M s.t. M*P*G\'(Z) = Z => M*A = Z\n    #   Z = batch of latent vectors (n_samples x latent_dims)\n    #   G\'(Z) = batch of activations at intermediate layer\n    #   A = P*G\'(Z) = projected activations (n_samples x pca_coords)\n    #   M = linear mapping (pca_coords x latent_dims)\n\n    # Minimization min_M ||MA - Z||_l2 rewritten as min_M.T ||A.T*M.T - Z.T||_l2\n    # to match format expected by pytorch.lstsq\n\n    # TODO: regression on pixel-space outputs? (using nonlinear optimizer)\n    # min_M lpips(G_full(MA), G_full(Z))\n\n    # Tensors to fill with data\n    # Dimensions other way around, so these are actually the transposes\n    A = np.zeros((n_samp, n_comp), dtype=np.float32)\n    Z = np.zeros((n_samp, latent_dims), dtype=np.float32)\n    \n    # Project tensor X onto PCs, return coordinates\n    def project(X, comp):\n        N = X.shape[0]\n        K = comp.shape[0]\n        coords = torch.bmm(comp.expand([N]+[-1]*comp.ndim), X.view(N, -1, 1))\n        return coords.reshape(N, K)\n\n    for i in trange(n_samp // B, desc=\'Collecting samples\', ascii=True):\n        z = inst.model.sample_latent(B)\n        inst.model.partial_forward(z, config.layer)\n        act = inst.retained_features()[config.layer].reshape(B, -1)\n\n        # Project onto basis\n        act = act - mean\n        coords = project(act, comp)\n        coords_scaled = coords / stdev\n\n        A[i*B:(i+1)*B] = coords_scaled.detach().cpu().numpy()\n        Z[i*B:(i+1)*B] = z.detach().cpu().numpy().reshape(B, -1)\n\n    # Solve least squares fit\n    print(\'Performing least squares fit\')\n    M_t = np.linalg.lstsq(A, Z, rcond=None)[0] # torch.lstsq(Z, A)[0][:n_comp, :]\n    \n    # Solution given by rows of M_t\n    Z_comp = M_t[:n_comp, :]\n    Z_mean = np.mean(Z, axis=0, keepdims=True)\n\n    return Z_comp, Z_mean\n\ndef regression(comp, mean, stdev, inst, config):\n    # Sanity check: verify orthonormality\n    M = np.dot(comp, comp.T)\n    if not np.allclose(M, np.identity(M.shape[0])):\n        det = np.linalg.det(M)\n        print(f\'WARNING: Computed basis is not orthonormal (determinant={det})\')\n\n    return linreg_lstsq(comp, mean, stdev, inst, config)\n\ndef compute(config, dump_name, instrumented_model):\n    global B\n\n    timestamp = lambda : datetime.datetime.now().strftime(""%d.%m %H:%M"")\n    print(f\'[{timestamp()}] Computing\', dump_name.name)\n\n    # Ensure reproducibility\n    torch.manual_seed(0) # also sets cuda seeds\n    np.random.seed(0)\n\n    # Speed up backend\n    torch.backends.cudnn.benchmark = True\n\n    has_gpu = torch.cuda.is_available()\n    device = torch.device(\'cuda\' if has_gpu else \'cpu\')\n    layer_key = config.layer\n\n    if instrumented_model is None:\n        inst = get_instrumented_model(config.model, config.output_class, layer_key, device)\n        model = inst.model\n    else:\n        print(\'Reusing InstrumentedModel instance\')\n        inst = instrumented_model\n        model = inst.model\n        inst.remove_edits()\n        model.set_output_class(config.output_class)\n\n    # Regress back to w space\n    if config.use_w:\n        print(\'Using W latent space\')\n        model.use_w()\n\n    inst.retain_layer(layer_key)\n    model.partial_forward(model.sample_latent(1), layer_key)\n    sample_shape = inst.retained_features()[layer_key].shape\n    sample_dims = np.prod(sample_shape)\n    print(\'Feature shape:\', sample_shape)\n\n    input_shape = inst.model.get_latent_shape()\n    input_dims = inst.model.get_latent_dims()\n\n    config.components = min(config.components, sample_dims)\n    transformer = get_estimator(config.estimator, config.components, config.sparsity)\n\n    X = None\n    X_global_mean = None\n\n    # Figure out batch size if not provided\n    B = config.batch_size or get_max_batch_size(inst, device, layer_key)\n\n    # Divisible by B (ignored in output name)\n    N = config.n // B * B\n\n    # Compute maximum batch size based on RAM + pagefile budget\n    target_bytes = 20 * 1_000_000_000 # GB\n    feat_size_bytes = sample_dims * np.dtype(\'float64\').itemsize\n    N_limit_RAM = np.floor_divide(target_bytes, feat_size_bytes)\n    if not transformer.batch_support and N > N_limit_RAM:\n        print(\'WARNING: estimator does not support batching, \' \\\n            \'given config will use {:.1f} GB memory.\'.format(feat_size_bytes / 1_000_000_000 * N))\n\n    # 32-bit LAPACK gets very unhappy about huge matrices (in linalg.svd)\n    if config.estimator == \'ica\':\n        lapack_max_N = np.floor_divide(np.iinfo(np.int32).max // 4, sample_dims) # 4x extra buffer\n        if N > lapack_max_N:\n            raise RuntimeError(f\'Matrices too large for ICA, please use N <= {lapack_max_N}\')\n\n    print(\'B={}, N={}, dims={}, N/dims={:.1f}\'.format(B, N, sample_dims, N/sample_dims), flush=True)\n\n    # Must not depend on chosen batch size (reproducibility)\n    NB = max(B, max(2_000, 3*config.components)) # ipca: as large as possible!\n    \n    samples = None\n    if not transformer.batch_support:\n        samples = np.zeros((N + NB, sample_dims), dtype=np.float32)\n\n    torch.manual_seed(config.seed or SEED_SAMPLING)\n    np.random.seed(config.seed or SEED_SAMPLING)\n\n    # Use exactly the same latents regardless of batch size\n    # Store in main memory, since N might be huge (1M+)\n    # Run in batches, since sample_latent() might perform Z -> W mapping\n    n_lat = ((N + NB - 1) // B + 1) * B\n    latents = np.zeros((n_lat, *input_shape[1:]), dtype=np.float32)\n    with torch.no_grad():\n        for i in trange(n_lat // B, desc=\'Sampling latents\'):\n            latents[i*B:(i+1)*B] = model.sample_latent(n_samples=B).cpu().numpy()\n\n    # Decomposition on non-Gaussian latent space\n    samples_are_latents = layer_key in [\'g_mapping\', \'style\'] and inst.model.latent_space_name() == \'W\'\n\n    canceled = False\n    try:\n        X = np.ones((NB, sample_dims), dtype=np.float32)\n        action = \'Fitting\' if transformer.batch_support else \'Collecting\'\n        for gi in trange(0, N, NB, desc=f\'{action} batches (NB={NB})\', ascii=True):\n            for mb in range(0, NB, B):\n                z = torch.from_numpy(latents[gi+mb:gi+mb+B]).to(device)\n                \n                if samples_are_latents:\n                    # Decomposition on latents directly (e.g. StyleGAN W)\n                    batch = z.reshape((B, -1))\n                else:\n                    # Decomposition on intermediate layer\n                    with torch.no_grad():\n                        model.partial_forward(z, layer_key)\n                    \n                    # Permuted to place PCA dimensions last\n                    batch = inst.retained_features()[layer_key].reshape((B, -1))\n\n                space_left = min(B, NB - mb)\n                X[mb:mb+space_left] = batch.cpu().numpy()[:space_left]\n\n            if transformer.batch_support:\n                if not transformer.fit_partial(X.reshape(-1, sample_dims)):\n                    break\n            else:\n                samples[gi:gi+NB, :] = X.copy()\n    except KeyboardInterrupt:\n        if not transformer.batch_support:\n            sys.exit(1) # no progress yet\n        \n        dump_name = dump_name.parent / dump_name.name.replace(f\'n{N}\', f\'n{gi}\')\n        print(f\'Saving current state to ""{dump_name.name}"" before exiting\')\n        canceled = True\n        \n    if not transformer.batch_support:\n        X = samples # Use all samples\n        X_global_mean = X.mean(axis=0, keepdims=True, dtype=np.float32) # TODO: activations surely multi-modal...!\n        X -= X_global_mean\n        \n        print(f\'[{timestamp()}] Fitting whole batch\')\n        t_start_fit = datetime.datetime.now()\n\n        transformer.fit(X)\n        \n        print(f\'[{timestamp()}] Done in {datetime.datetime.now() - t_start_fit}\')\n        assert np.all(transformer.transformer.mean_ < 1e-3), \'Mean of normalized data should be zero\'\n    else:\n        X_global_mean = transformer.transformer.mean_.reshape((1, sample_dims))\n        X = X.reshape(-1, sample_dims)\n        X -= X_global_mean\n\n    X_comp, X_stdev, X_var_ratio = transformer.get_components()\n    \n    assert X_comp.shape[1] == sample_dims \\\n        and X_comp.shape[0] == config.components \\\n        and X_global_mean.shape[1] == sample_dims \\\n        and X_stdev.shape[0] == config.components, \'Invalid shape\'\n\n    # \'Activations\' are really latents in a secondary latent space\n    if samples_are_latents:\n        Z_comp = X_comp\n        Z_global_mean = X_global_mean\n    else:\n        Z_comp, Z_global_mean = regression(X_comp, X_global_mean, X_stdev, inst, config)\n\n    # Normalize\n    Z_comp /= np.linalg.norm(Z_comp, axis=-1, keepdims=True)\n\n    # Random projections\n    # We expect these to explain much less of the variance\n    random_dirs = get_random_dirs(config.components, np.prod(sample_shape))\n    n_rand_samples = min(5000, X.shape[0])\n    X_view = X[:n_rand_samples, :].T\n    assert np.shares_memory(X_view, X), ""Error: slice produced copy""\n    X_stdev_random = np.dot(random_dirs, X_view).std(axis=1)\n\n    # Inflate back to proper shapes (for easier broadcasting)\n    X_comp = X_comp.reshape(-1, *sample_shape)\n    X_global_mean = X_global_mean.reshape(sample_shape)\n    Z_comp = Z_comp.reshape(-1, *input_shape)\n    Z_global_mean = Z_global_mean.reshape(input_shape)\n\n    # Compute stdev in latent space if non-Gaussian\n    lat_stdev = np.ones_like(X_stdev)\n    if config.use_w:\n        samples = model.sample_latent(5000).reshape(5000, input_dims).detach().cpu().numpy()\n        coords = np.dot(Z_comp.reshape(-1, input_dims), samples.T)\n        lat_stdev = coords.std(axis=1)\n\n    os.makedirs(dump_name.parent, exist_ok=True)\n    np.savez_compressed(dump_name, **{\n        \'act_comp\': X_comp.astype(np.float32),\n        \'act_mean\': X_global_mean.astype(np.float32),\n        \'act_stdev\': X_stdev.astype(np.float32),\n        \'lat_comp\': Z_comp.astype(np.float32),\n        \'lat_mean\': Z_global_mean.astype(np.float32),\n        \'lat_stdev\': lat_stdev.astype(np.float32),\n        \'var_ratio\': X_var_ratio.astype(np.float32),\n        \'random_stdevs\': X_stdev_random.astype(np.float32),\n    })\n\n    if canceled:\n        sys.exit(1)\n\n    # Don\'t shutdown if passed as param\n    if instrumented_model is None:\n        inst.close()\n        del inst\n        del model\n\n    del X\n    del X_comp\n    del random_dirs\n    del batch\n    del samples\n    del latents\n    torch.cuda.empty_cache()\n\n# Return cached results or commpute if needed\n# Pass existing InstrumentedModel instance to reuse it\ndef get_or_compute(config, model=None, submit_config=None, force_recompute=False):\n    if submit_config is None:\n        wrkdir = str(Path(__file__).parent.resolve())\n        submit_config = SimpleNamespace(run_dir_root = wrkdir, run_dir = wrkdir)\n    \n    # Called directly by run.py\n    return _compute(submit_config, config, model, force_recompute)\n\ndef _compute(submit_config, config, model=None, force_recompute=False):\n    basedir = Path(submit_config.run_dir)\n    outdir = basedir / \'out\'\n    \n    if config.n is None:\n        raise RuntimeError(\'Must specify number of samples with -n=XXX\')\n\n    if model and not isinstance(model, InstrumentedModel):\n        raise RuntimeError(\'Passed model has to be wrapped in ""InstrumentedModel""\')\n    \n    if config.use_w and not \'StyleGAN\' in config.model:\n        raise RuntimeError(f\'Cannot change latent space of non-StyleGAN model {config.model}\')\n\n    transformer = get_estimator(config.estimator, config.components, config.sparsity)\n    dump_name = ""{}-{}_{}_{}_n{}{}{}.npz"".format(\n        config.model.lower(),\n        config.output_class.replace(\' \', \'_\'),\n        config.layer.lower(),\n        transformer.get_param_str(),\n        config.n,\n        \'_w\' if config.use_w else \'\',\n        f\'_seed{config.seed}\' if config.seed else \'\'\n    )\n\n    dump_path = basedir / \'cache\' / \'components\' / dump_name\n\n    if not dump_path.is_file() or force_recompute:\n        print(\'Not cached\')\n        t_start = datetime.datetime.now()\n        compute(config, dump_path, model)\n        print(\'Total time:\', datetime.datetime.now() - t_start)\n    \n    return dump_path'"
estimators.py,0,"b'# Copyright 2020 Erik H\xc3\xa4rk\xc3\xb6nen. All rights reserved.\n# This file is licensed to you under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License. You may obtain a copy\n# of the License at http://www.apache.org/licenses/LICENSE-2.0\n\n# Unless required by applicable law or agreed to in writing, software distributed under\n# the License is distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR REPRESENTATIONS\n# OF ANY KIND, either express or implied. See the License for the specific language\n# governing permissions and limitations under the License.\n\nfrom sklearn.decomposition import FastICA, PCA, IncrementalPCA, MiniBatchSparsePCA, SparsePCA, KernelPCA\nimport fbpca\nimport numpy as np\nimport itertools\nfrom types import SimpleNamespace\n\n# ICA\nclass ICAEstimator():\n    def __init__(self, n_components):\n        self.n_components = n_components\n        self.maxiter = 10000\n        self.whiten = True # ICA: whitening is essential, should not be skipped\n        self.transformer = FastICA(n_components, random_state=0, whiten=self.whiten, max_iter=self.maxiter)\n        self.batch_support = False\n        self.stdev = np.zeros((n_components,))\n        self.total_var = 0.0\n\n    def get_param_str(self):\n        return ""ica_c{}{}"".format(self.n_components, \'_w\' if self.whiten else \'\')\n    \n    def fit(self, X):\n        self.transformer.fit(X)\n        if self.transformer.n_iter_ >= self.maxiter:\n            raise RuntimeError(f\'FastICA did not converge (N={X.shape[0]}, it={self.maxiter})\')\n\n        # Normalize components\n        self.transformer.components_ /= np.sqrt(np.sum(self.transformer.components_**2, axis=-1, keepdims=True))\n\n        # Save variance for later\n        self.total_var = X.var(axis=0).sum()\n\n        # Compute projected standard deviations\n        self.stdev = np.dot(self.transformer.components_, X.T).std(axis=1)\n\n        # Sort components based on explained variance\n        idx = np.argsort(self.stdev)[::-1]\n        self.stdev = self.stdev[idx]\n        self.transformer.components_[:] = self.transformer.components_[idx]\n\n    def get_components(self):\n        var_ratio = self.stdev**2 / self.total_var\n        return self.transformer.components_, self.stdev, var_ratio # ICA outputs are not normalized\n\n# Incremental PCA\nclass IPCAEstimator():\n    def __init__(self, n_components):\n        self.n_components = n_components\n        self.whiten = False\n        self.transformer = IncrementalPCA(n_components, whiten=self.whiten, batch_size=max(100, 2*n_components))\n        self.batch_support = True\n\n    def get_param_str(self):\n        return ""ipca_c{}{}"".format(self.n_components, \'_w\' if self.whiten else \'\')\n\n    def fit(self, X):\n        self.transformer.fit(X)\n\n    def fit_partial(self, X):\n        try:\n            self.transformer.partial_fit(X)\n            self.transformer.n_samples_seen_ = \\\n                self.transformer.n_samples_seen_.astype(np.int64) # avoid overflow\n            return True\n        except ValueError as e:\n            print(f\'\\nIPCA error:\', e)\n            return False\n\n    def get_components(self):\n        stdev = np.sqrt(self.transformer.explained_variance_) # already sorted\n        var_ratio = self.transformer.explained_variance_ratio_\n        return self.transformer.components_, stdev, var_ratio # PCA outputs are normalized\n\n# Standard PCA\nclass PCAEstimator():\n    def __init__(self, n_components):\n        self.n_components = n_components\n        self.solver = \'full\'\n        self.transformer = PCA(n_components, svd_solver=self.solver)\n        self.batch_support = False\n\n    def get_param_str(self):\n        return f""pca-{self.solver}_c{self.n_components}""\n\n    def fit(self, X):\n        self.transformer.fit(X)\n\n        # Save variance for later\n        self.total_var = X.var(axis=0).sum()\n\n        # Compute projected standard deviations\n        self.stdev = np.dot(self.transformer.components_, X.T).std(axis=1)\n\n        # Sort components based on explained variance\n        idx = np.argsort(self.stdev)[::-1]\n        self.stdev = self.stdev[idx]\n        self.transformer.components_[:] = self.transformer.components_[idx]\n\n        # Check orthogonality\n        dotps = [np.dot(*self.transformer.components_[[i, j]])\n            for (i, j) in itertools.combinations(range(self.n_components), 2)]\n        if not np.allclose(dotps, 0, atol=1e-4):\n            print(\'IPCA components not orghogonal, max dot\', np.abs(dotps).max())\n\n        self.transformer.mean_ = X.mean(axis=0, keepdims=True)\n\n    def get_components(self):\n        var_ratio = self.stdev**2 / self.total_var\n        return self.transformer.components_, self.stdev, var_ratio\n\n# Facebook\'s PCA\n# Good default choice: very fast and accurate.\n# Very high sample counts won\'t fit into RAM,\n# in which case IncrementalPCA must be used.\nclass FacebookPCAEstimator():\n    def __init__(self, n_components):\n        self.n_components = n_components\n        self.transformer = SimpleNamespace()\n        self.batch_support = False\n        self.n_iter = 2\n        self.l = 2*self.n_components\n\n    def get_param_str(self):\n        return ""fbpca_c{}_it{}_l{}"".format(self.n_components, self.n_iter, self.l)\n\n    def fit(self, X):\n        U, s, Va = fbpca.pca(X, k=self.n_components, n_iter=self.n_iter, raw=True, l=self.l)\n        self.transformer.components_ = Va\n        \n        # Save variance for later\n        self.total_var = X.var(axis=0).sum()\n\n        # Compute projected standard deviations\n        self.stdev = np.dot(self.transformer.components_, X.T).std(axis=1)\n\n        # Sort components based on explained variance\n        idx = np.argsort(self.stdev)[::-1]\n        self.stdev = self.stdev[idx]\n        self.transformer.components_[:] = self.transformer.components_[idx]\n\n        # Check orthogonality\n        dotps = [np.dot(*self.transformer.components_[[i, j]])\n            for (i, j) in itertools.combinations(range(self.n_components), 2)]\n        if not np.allclose(dotps, 0, atol=1e-4):\n            print(\'FBPCA components not orghogonal, max dot\', np.abs(dotps).max())\n\n        self.transformer.mean_ = X.mean(axis=0, keepdims=True)\n        \n    def get_components(self):\n        var_ratio = self.stdev**2 / self.total_var\n        return self.transformer.components_, self.stdev, var_ratio\n\n# Sparse PCA\n# The algorithm is online along the features direction, not the samples direction\n#   => no partial_fit\nclass SPCAEstimator():\n    def __init__(self, n_components, alpha=10.0):\n        self.n_components = n_components\n        self.whiten = False\n        self.alpha = alpha  # higher alpha => sparser components\n        #self.transformer = MiniBatchSparsePCA(n_components, alpha=alpha, n_iter=100,\n        #    batch_size=max(20, n_components//5), random_state=0, normalize_components=True)\n        self.transformer = SparsePCA(n_components, alpha=alpha, ridge_alpha=0.01,\n            max_iter=100, random_state=0, n_jobs=-1, normalize_components=True) # TODO: warm start using PCA result?\n        self.batch_support = False # maybe through memmap and HDD-stored tensor\n        self.stdev = np.zeros((n_components,))\n        self.total_var = 0.0\n\n    def get_param_str(self):\n        return ""spca_c{}_a{}{}"".format(self.n_components, self.alpha, \'_w\' if self.whiten else \'\')\n        \n    def fit(self, X):\n        self.transformer.fit(X)\n\n        # Save variance for later\n        self.total_var = X.var(axis=0).sum()\n\n        # Compute projected standard deviations\n        # NB: cannot simply project with dot product!\n        self.stdev = self.transformer.transform(X).std(axis=0) # X = (n_samples, n_features)\n\n        # Sort components based on explained variance\n        idx = np.argsort(self.stdev)[::-1]\n        self.stdev = self.stdev[idx]\n        self.transformer.components_[:] = self.transformer.components_[idx]\n\n        # Check orthogonality\n        dotps = [np.dot(*self.transformer.components_[[i, j]])\n            for (i, j) in itertools.combinations(range(self.n_components), 2)]\n        if not np.allclose(dotps, 0, atol=1e-4):\n            print(\'SPCA components not orghogonal, max dot\', np.abs(dotps).max())\n\n    def get_components(self):\n        var_ratio = self.stdev**2 / self.total_var\n        return self.transformer.components_, self.stdev, var_ratio # SPCA outputs are normalized\n\ndef get_estimator(name, n_components, alpha):\n    if name == \'pca\':\n        return PCAEstimator(n_components)\n    if name == \'ipca\':\n        return IPCAEstimator(n_components)\n    elif name == \'fbpca\':\n        return FacebookPCAEstimator(n_components)\n    elif name == \'ica\':\n        return ICAEstimator(n_components)\n    elif name == \'spca\':\n        return SPCAEstimator(n_components, alpha)\n    else:\n        raise RuntimeError(\'Unknown estimator\')'"
interactive.py,20,"b'# Copyright 2020 Erik H\xc3\xa4rk\xc3\xb6nen. All rights reserved.\n# This file is licensed to you under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License. You may obtain a copy\n# of the License at http://www.apache.org/licenses/LICENSE-2.0\n\n# Unless required by applicable law or agreed to in writing, software distributed under\n# the License is distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR REPRESENTATIONS\n# OF ANY KIND, either express or implied. See the License for the specific language\n# governing permissions and limitations under the License.\n\n# An interactive glumpy (OpenGL) + tkinter viewer for interacting with principal components.\n# Requires OpenGL and CUDA support for rendering.\n\nimport torch\nimport numpy as np\nimport tkinter as tk\nfrom tkinter import ttk\nfrom types import SimpleNamespace\nimport matplotlib.pyplot as plt\nfrom pathlib import Path\nfrom os import makedirs\nfrom models import get_instrumented_model\nfrom config import Config\nfrom decomposition import get_or_compute\nfrom torch.nn.functional import interpolate\nfrom TkTorchWindow import TorchImageView\nfrom functools import partial\nfrom platform import system\nfrom PIL import Image\nfrom utils import pad_frames, prettify_name\nimport pickle\n\n# For platform specific UI tweaks\nis_windows = \'Windows\' in system()\nis_linux = \'Linux\' in system()\nis_mac = \'Darwin\' in system()\n\n# Read input parameters\nargs = Config().from_args()\n\n# Don\'t bother without GPU\nassert torch.cuda.is_available(), \'Interactive mode requires CUDA\'\n\n# Use syntax from paper\ndef get_edit_name(idx, s, e, name=None):\n    return \'E({comp}, {edit_range}){edit_name}\'.format(\n        comp = idx,\n        edit_range = f\'{s}-{e}\' if e > s else s,\n        edit_name = f\': {name}\' if name else \'\'\n    )\n\n# Load or compute PCA basis vectors\ndef load_components(class_name, inst):\n    global components, state, use_named_latents\n\n    config = args.from_dict({ \'output_class\': class_name })\n    dump_name = get_or_compute(config, inst)\n    data = np.load(dump_name, allow_pickle=False)\n    X_comp = data[\'act_comp\']\n    X_mean = data[\'act_mean\']\n    X_stdev = data[\'act_stdev\']\n    Z_comp = data[\'lat_comp\']\n    Z_mean = data[\'lat_mean\']\n    Z_stdev = data[\'lat_stdev\']\n    random_stdev_act = np.mean(data[\'random_stdevs\'])\n    n_comp = X_comp.shape[0]\n    data.close()\n\n    # Transfer to GPU\n    components = SimpleNamespace(\n        X_comp = torch.from_numpy(X_comp).cuda().float(),\n        X_mean = torch.from_numpy(X_mean).cuda().float(),\n        X_stdev = torch.from_numpy(X_stdev).cuda().float(),\n        Z_comp = torch.from_numpy(Z_comp).cuda().float(),\n        Z_stdev = torch.from_numpy(Z_stdev).cuda().float(),\n        Z_mean = torch.from_numpy(Z_mean).cuda().float(),\n        names = [f\'Component {i}\' for i in range(n_comp)],\n        latent_types = [model.latent_space_name()]*n_comp,\n        ranges = [(0, model.get_max_latents())]*n_comp,\n    )\n    \n    state.component_class = class_name # invalidates cache\n    use_named_latents = False\n    print(\'Loaded components for\', class_name, \'from\', dump_name)\n\n# Load previously exported named components from\n# directory specified with \'--inputs=path/to/comp\'\ndef load_named_components(path, class_name):\n    global components, state, use_named_latents\n\n    import glob\n    matches = glob.glob(f\'{path}/*.pkl\')\n\n    selected = []\n    for dump_path in matches:\n        with open(dump_path, \'rb\') as f:\n            data = pickle.load(f)\n            if data[\'model_name\'] != model_name or data[\'output_class\'] != class_name:\n                continue\n\n            if data[\'latent_space\'] != model.latent_space_name():\n                print(\'Skipping\', dump_path, \'(wrong latent space)\')\n                continue\n            \n            selected.append(data)\n            print(\'Using\', dump_path)\n\n    if len(selected) == 0:\n        raise RuntimeError(\'No valid components in given path.\')\n\n    comp_dict = { k : [] for k in [\'X_comp\', \'Z_comp\', \'X_stdev\', \'Z_stdev\', \'names\', \'types\', \'layer_names\', \'ranges\', \'latent_types\'] }\n    components = SimpleNamespace(**comp_dict)\n\n    for d in selected:\n        s = d[\'edit_start\']\n        e = d[\'edit_end\']\n        title = get_edit_name(d[\'component_index\'], s, e - 1, d[\'name\']) # show inclusive\n        components.X_comp.append(torch.from_numpy(d[\'act_comp\']).cuda())\n        components.Z_comp.append(torch.from_numpy(d[\'lat_comp\']).cuda())\n        components.X_stdev.append(d[\'act_stdev\'])\n        components.Z_stdev.append(d[\'lat_stdev\'])\n        components.names.append(title)\n        components.types.append(d[\'edit_type\'])\n        components.layer_names.append(d[\'decomposition\'][\'layer\']) # only for act\n        components.ranges.append((s, e))\n        components.latent_types.append(d[\'latent_space\']) # W or Z\n    \n    use_named_latents = True\n    print(\'Loaded named components\')\n\ndef setup_model():\n    global model, inst, layer_name, model_name, feat_shape, args, class_name\n\n    model_name = args.model\n    layer_name = args.layer\n    class_name = args.output_class\n\n    # Speed up pytorch\n    torch.autograd.set_grad_enabled(False)\n    torch.backends.cudnn.benchmark = True\n\n    # Load model\n    inst = get_instrumented_model(model_name, class_name, layer_name, torch.device(\'cuda\'), use_w=args.use_w)\n    model = inst.model\n\n    feat_shape = inst.feature_shape[layer_name]\n    sample_dims = np.prod(feat_shape)\n\n    # Initialize \n    if args.inputs:\n        load_named_components(args.inputs, class_name)\n    else:\n        load_components(class_name, inst)\n\n# Project tensor \'X\' onto orthonormal basis \'comp\', return coordinates\ndef project_ortho(X, comp):\n    N = comp.shape[0]\n    coords = (comp.reshape(N, -1) * X.reshape(-1)).sum(dim=1)\n    return coords.reshape([N]+[1]*X.ndim)\n\ndef zero_sliders():\n    for v in ui_state.sliders:\n        v.set(0.0)\n\ndef reset_sliders(zero_on_failure=True):\n    global ui_state\n\n    mode = ui_state.mode.get()\n\n    # Not orthogonal: need to solve least-norm problem\n    # Not batch size 1: one set of sliders not enough\n    # Not principal components: unsupported format\n    is_ortho = not (mode == \'latent\' and model.latent_space_name() == \'Z\')\n    is_single = state.z.shape[0] == 1\n    is_pcs = not use_named_latents\n\n    state.lat_slider_offset = 0\n    state.act_slider_offset = 0\n\n    enabled = False\n    if not (enabled and is_ortho and is_single and is_pcs):\n        if zero_on_failure:\n            zero_sliders()\n        return\n\n    if  mode == \'activation\':\n        val = state.base_act\n        mean = components.X_mean\n        comp = components.X_comp\n        stdev = components.X_stdev\n    else:\n        val = state.z\n        mean = components.Z_mean\n        comp = components.Z_comp\n        stdev = components.Z_stdev\n\n    n_sliders = len(ui_state.sliders)\n    coords = project_ortho(val - mean, comp)\n    offset = torch.sum(coords[:n_sliders] * comp[:n_sliders], dim=0)\n    scaled_coords = (coords.view(-1) / stdev).detach().cpu().numpy()\n    \n    # Part representable by sliders\n    if mode == \'activation\':\n        state.act_slider_offset = offset\n    else:\n        state.lat_slider_offset = offset\n\n    for i in range(n_sliders):\n        ui_state.sliders[i].set(round(scaled_coords[i], ndigits=1))\n\ndef setup_ui():\n    global root, toolbar, ui_state, app, canvas\n\n    root = tk.Tk()\n    scale = 1.0\n    app = TorchImageView(root, width=int(scale*1024), height=int(scale*1024), show_fps=False)\n    app.pack(fill=tk.BOTH, expand=tk.YES)\n    root.protocol(""WM_DELETE_WINDOW"", shutdown)\n    root.title(\'GANspace\')\n\n    toolbar = tk.Toplevel(root)\n    toolbar.protocol(""WM_DELETE_WINDOW"", shutdown)\n    toolbar.geometry(""215x800+0+0"")\n    toolbar.title(\'\')\n\n    N_COMPONENTS = min(70, len(components.names))\n    ui_state = SimpleNamespace(\n        sliders = [tk.DoubleVar(value=0.0) for _ in range(N_COMPONENTS)],\n        scales = [],\n        truncation = tk.DoubleVar(value=0.9),\n        outclass = tk.StringVar(value=class_name),\n        random_seed = tk.StringVar(value=\'0\'),\n        mode = tk.StringVar(value=\'latent\'),\n        batch_size = tk.IntVar(value=1), # how many images to show in window\n        edit_layer_start = tk.IntVar(value=0),\n        edit_layer_end = tk.IntVar(value=model.get_max_latents() - 1),\n        slider_max_val = 10.0\n    )\n\n    # Z vs activation mode button\n    #tk.Radiobutton(toolbar, text=f""Latent ({model.latent_space_name()})"", variable=ui_state.mode, command=reset_sliders, value=\'latent\').pack(fill=""x"")\n    #tk.Radiobutton(toolbar, text=""Activation"", variable=ui_state.mode, command=reset_sliders, value=\'activation\').pack(fill=""x"")\n\n    # Choose range where latents are modified\n    def set_min(val):\n        ui_state.edit_layer_start.set(min(int(val), ui_state.edit_layer_end.get()))\n    def set_max(val):\n        ui_state.edit_layer_end.set(max(int(val), ui_state.edit_layer_start.get()))\n    max_latent_idx = model.get_max_latents() - 1\n    \n    if not use_named_latents:\n        slider_min = tk.Scale(toolbar, command=set_min, variable=ui_state.edit_layer_start,\n            label=\'Layer start\', from_=0, to=max_latent_idx, orient=tk.HORIZONTAL).pack(fill=""x"")\n        slider_max = tk.Scale(toolbar, command=set_max, variable=ui_state.edit_layer_end,\n            label=\'Layer end\', from_=0, to=max_latent_idx, orient=tk.HORIZONTAL).pack(fill=""x"")\n\n    # Scrollable list of components\n    outer_frame = tk.Frame(toolbar, borderwidth=2, relief=tk.SUNKEN)\n    canvas = tk.Canvas(outer_frame, highlightthickness=0, borderwidth=0)\n    frame = tk.Frame(canvas)\n    vsb = tk.Scrollbar(outer_frame, orient=""vertical"", command=canvas.yview)\n    canvas.configure(yscrollcommand=vsb.set)\n\n    vsb.pack(side=""right"", fill=""y"")\n    canvas.pack(side=""left"", fill=""both"", expand=True)\n    canvas.create_window((4,4), window=frame, anchor=""nw"")\n\n    def onCanvasConfigure(event):\n        canvas.itemconfigure(""all"", width=event.width)\n        canvas.configure(scrollregion=canvas.bbox(""all""))\n    canvas.bind(""<Configure>"", onCanvasConfigure)\n\n    def on_scroll(event):\n        delta = 1 if (event.num == 5 or event.delta < 0) else -1\n        canvas.yview_scroll(delta, ""units"")\n\n    canvas.bind_all(""<Button-4>"", on_scroll)\n    canvas.bind_all(""<Button-5>"", on_scroll)\n    canvas.bind_all(""<MouseWheel>"", on_scroll)\n    canvas.bind_all(""<Key>"", lambda event : handle_keypress(event.keysym_num))\n\n    # Sliders and buttons\n    for i in range(N_COMPONENTS):\n        inner = tk.Frame(frame, borderwidth=1, background=""#aaaaaa"")\n        scale = tk.Scale(inner, variable=ui_state.sliders[i], from_=-ui_state.slider_max_val,\n            to=ui_state.slider_max_val, resolution=0.1, orient=tk.HORIZONTAL, label=components.names[i])\n        scale.pack(fill=tk.X, side=tk.LEFT, expand=True)\n        ui_state.scales.append(scale) # for changing label later\n        if not use_named_latents:\n            tk.Button(inner, text=f""Save"", command=partial(export_direction, i, inner)).pack(fill=tk.Y, side=tk.RIGHT)\n        inner.pack(fill=tk.X)\n\n    outer_frame.pack(fill=""both"", expand=True, pady=0)\n\n    tk.Button(toolbar, text=""Reset"", command=reset_sliders).pack(anchor=tk.CENTER, fill=tk.X, padx=4, pady=4)\n\n    tk.Scale(toolbar, variable=ui_state.truncation, from_=0.01, to=1.0,\n        resolution=0.01, orient=tk.HORIZONTAL, label=\'Truncation\').pack(fill=""x"")\n\n    tk.Scale(toolbar, variable=ui_state.batch_size, from_=1, to=9,\n        resolution=1, orient=tk.HORIZONTAL, label=\'Batch size\').pack(fill=""x"")\n    \n    # Output class\n    frame = tk.Frame(toolbar)\n    tk.Label(frame, text=""Class name"").pack(fill=""x"", side=""left"")\n    tk.Entry(frame, textvariable=ui_state.outclass).pack(fill=""x"", side=""right"", expand=True, padx=5)\n    frame.pack(fill=tk.X, pady=3)\n\n    # Random seed\n    def update_seed():\n        seed_str = ui_state.random_seed.get()\n        if seed_str.isdigit():\n            resample_latent(int(seed_str))\n    frame = tk.Frame(toolbar)\n    tk.Label(frame, text=""Seed"").pack(fill=""x"", side=""left"")\n    tk.Entry(frame, textvariable=ui_state.random_seed, width=12).pack(fill=""x"", side=""left"", expand=True, padx=2)\n    tk.Button(frame, text=""Update"", command=update_seed).pack(fill=""y"", side=""right"", padx=3)\n    frame.pack(fill=tk.X, pady=3)\n    \n    # Get new latent or new components\n    tk.Button(toolbar, text=""Resample latent"", command=partial(resample_latent, None, False)).pack(anchor=tk.CENTER, fill=tk.X, padx=4, pady=4)\n    #tk.Button(toolbar, text=""Recompute"", command=recompute_components).pack(anchor=tk.CENTER, fill=tk.X)\n\n# App state\nstate = SimpleNamespace(\n    z=None,                  # current latent(s)\n    lat_slider_offset = 0,   # part of lat that is explained by sliders\n    act_slider_offset = 0,   # part of act that is explained by sliders\n    component_class=None,    # name of current PCs\' image class\n    seed=0,                  # Latent z_i generated by seed+i\n    base_act = None,         # activation of considered layer given z\n)\n\ndef resample_latent(seed=None, only_style=False):\n    class_name = ui_state.outclass.get()\n    if class_name.isnumeric():\n        class_name = int(class_name)\n    \n    if hasattr(model, \'is_valid_class\'):\n        if not model.is_valid_class(class_name):\n            return\n\n    model.set_output_class(class_name)\n    \n    B = ui_state.batch_size.get()\n    state.seed = np.random.randint(np.iinfo(np.int32).max - B) if seed is None else seed\n    ui_state.random_seed.set(str(state.seed))\n    \n    # Use consecutive seeds along batch dimension (for easier reproducibility)\n    trunc = ui_state.truncation.get()\n    latents = [model.sample_latent(1, seed=state.seed + i, truncation=trunc) for i in range(B)]\n\n    state.z = torch.cat(latents).clone().detach() # make leaf node\n    assert state.z.is_leaf, \'Latent is not leaf node!\'\n    \n    if hasattr(model, \'truncation\'):\n        model.truncation = ui_state.truncation.get()\n    print(f\'Seeds: {state.seed} -> {state.seed + B - 1}\' if B > 1 else f\'Seed: {state.seed}\')\n\n    torch.manual_seed(state.seed)\n    model.partial_forward(state.z, layer_name)\n    state.base_act = inst.retained_features()[layer_name]\n    \n    reset_sliders(zero_on_failure=False)\n\n    # Remove focus from text entry\n    canvas.focus_set()\n\n# Used to recompute after changing class of conditional model\ndef recompute_components():\n    class_name = ui_state.outclass.get()\n    if class_name.isnumeric():\n        class_name = int(class_name)\n    \n    if hasattr(model, \'is_valid_class\'):\n        if not model.is_valid_class(class_name):\n            return\n\n    if hasattr(model, \'set_output_class\'):\n        model.set_output_class(class_name)\n    \n    load_components(class_name, inst)\n\n# Used to detect parameter changes for lazy recomputation\nclass ParamCache():\n    def update(self, **kwargs):\n        dirty = False\n        for argname, val in kwargs.items():\n            # Check pointer, then value\n            current = getattr(self, argname, 0)\n            if current is not val and pickle.dumps(current) != pickle.dumps(val):\n                setattr(self, argname, val)\n                dirty = True\n        return dirty\n\ncache = ParamCache()\n\ndef l2norm(t):\n    return torch.norm(t.view(t.shape[0], -1), p=2, dim=1, keepdim=True)\n\ndef apply_edit(z0, delta):\n    return z0 + delta\n\ndef reposition_toolbar():\n    size, X, Y = root.winfo_geometry().split(\'+\')\n    W, H = size.split(\'x\')\n    toolbar_W = toolbar.winfo_geometry().split(\'x\')[0]\n    offset_y = -30 if is_linux else 0 # window title bar\n    toolbar.geometry(f\'{toolbar_W}x{H}+{int(X)-int(toolbar_W)}+{int(Y)+offset_y}\')\n    toolbar.update()\n\ndef on_draw():\n    global img\n\n    n_comp = len(ui_state.sliders)\n    slider_vals = np.array([s.get() for s in ui_state.sliders], dtype=np.float32)\n    \n    # Run model sparingly\n    mode = ui_state.mode.get()\n    latent_start = ui_state.edit_layer_start.get()\n    latent_end = ui_state.edit_layer_end.get() + 1 # save as exclusive, show as inclusive\n\n    if cache.update(coords=slider_vals, comp=state.component_class, mode=mode, z=state.z, s=latent_start, e=latent_end):\n        with torch.no_grad():\n            z_base = state.z - state.lat_slider_offset\n            z_deltas = [0.0]*model.get_max_latents()\n            z_delta_global = 0.0\n            \n            n_comp = slider_vals.size\n            act_deltas = {}\n            \n            if torch.is_tensor(state.act_slider_offset):\n                act_deltas[layer_name] = -state.act_slider_offset\n\n            for space in components.latent_types:\n                assert space == model.latent_space_name(), \\\n                    \'Cannot mix latent spaces (for now)\'\n\n            for c in range(n_comp):\n                coord = slider_vals[c]\n                if coord == 0:\n                    continue\n\n                edit_mode = components.types[c] if use_named_latents else mode\n                \n                # Activation offset\n                if edit_mode in [\'activation\', \'both\']:\n                    delta = components.X_comp[c] * components.X_stdev[c] * coord\n                    name = components.layer_names[c] if use_named_latents else layer_name\n                    act_deltas[name] = act_deltas.get(name, 0.0) + delta\n\n                # Latent offset\n                if edit_mode in [\'latent\', \'both\']:\n                    delta = components.Z_comp[c] * components.Z_stdev[c] * coord\n                    edit_range = components.ranges[c] if use_named_latents else (latent_start, latent_end)\n                    full_range = (edit_range == (0, model.get_max_latents()))\n                    \n                    # Single or multiple offsets?\n                    if full_range:\n                        z_delta_global = z_delta_global + delta\n                    else:\n                        for l in range(*edit_range):\n                            z_deltas[l] = z_deltas[l] + delta\n\n            # Apply activation deltas\n            inst.remove_edits()\n            for layer, delta in act_deltas.items():\n                inst.edit_layer(layer, offset=delta)\n            \n            # Evaluate\n            has_offsets = any(torch.is_tensor(t) for t in z_deltas)\n            z_final = apply_edit(z_base, z_delta_global)\n            if has_offsets:\n                z_final = [apply_edit(z_final, d) for d in z_deltas]\n            img = model.forward(z_final).clamp(0.0, 1.0)\n\n    app.draw(img)\n\n# Save necessary data to disk for later loading\ndef export_direction(idx, button_frame):\n    name = tk.StringVar(value=\'\')\n    num_strips = tk.IntVar(value=0)\n    strip_width = tk.IntVar(value=5)\n\n    slider_values = np.array([s.get() for s in ui_state.sliders])\n    slider_value = slider_values[idx]\n    if (slider_values != 0).sum() > 1:\n        print(\'Please modify only one slider\')\n        return\n    elif slider_value == 0:\n        print(\'Modify selected slider to set usable range (currently 0)\')\n        return\n    \n    popup = tk.Toplevel(root)\n    popup.geometry(""200x200+0+0"")\n    tk.Label(popup, text=""Edit name"").pack()\n    tk.Entry(popup, textvariable=name).pack(pady=5)\n    # tk.Scale(popup, from_=0, to=30, variable=num_strips,\n    #    resolution=1, orient=tk.HORIZONTAL, length=200, label=\'Image strips to export\').pack()\n    # tk.Scale(popup, from_=3, to=15, variable=strip_width,\n    #    resolution=1, orient=tk.HORIZONTAL, length=200, label=\'Image strip width\').pack()\n    tk.Button(popup, text=\'OK\', command=popup.quit).pack()\n    \n    canceled = False\n    def on_close():\n        nonlocal canceled\n        canceled = True\n        popup.quit()\n\n    popup.protocol(""WM_DELETE_WINDOW"", on_close)\n    x = button_frame.winfo_rootx()\n    y = button_frame.winfo_rooty()\n    w = int(button_frame.winfo_geometry().split(\'x\')[0])\n    popup.geometry(\'%dx%d+%d+%d\' % (180, 90, x + w, y))\n    popup.mainloop()\n    popup.destroy()\n\n    # Update slider name\n    label = get_edit_name(idx, ui_state.edit_layer_start.get(),\n        ui_state.edit_layer_end.get(), name.get())\n    ui_state.scales[idx].config(label=label)\n\n    if canceled:\n        return\n\n    params = {\n        \'name\': name.get(),\n        \'sigma_range\': slider_value,\n        \'component_index\': idx,\n        \'act_comp\': components.X_comp[idx].detach().cpu().numpy(),\n        \'lat_comp\': components.Z_comp[idx].detach().cpu().numpy(), # either Z or W\n        \'latent_space\': model.latent_space_name(),\n        \'act_stdev\': components.X_stdev[idx].item(),\n        \'lat_stdev\': components.Z_stdev[idx].item(),\n        \'model_name\': model_name,\n        \'output_class\': ui_state.outclass.get(), # applied onto\n        \'decomposition\': {\n            \'name\': args.estimator,\n            \'components\': args.components,\n            \'samples\': args.n,\n            \'layer\': args.layer,\n            \'class_name\': state.component_class # computed from\n        },\n        \'edit_type\': ui_state.mode.get(),\n        \'truncation\': ui_state.truncation.get(),\n        \'edit_start\': ui_state.edit_layer_start.get(),\n        \'edit_end\': ui_state.edit_layer_end.get() + 1, # show as inclusive, save as exclusive\n        \'example_seed\': state.seed,\n    }\n\n    edit_mode_str = params[\'edit_type\']\n    if edit_mode_str == \'latent\':\n        edit_mode_str = model.latent_space_name().lower()\n\n    comp_class = state.component_class\n    appl_class = params[\'output_class\']\n    if comp_class != appl_class:\n        comp_class = f\'{comp_class}_onto_{appl_class}\'\n\n    file_ident = ""{model}-{name}-{cls}-{est}-{mode}-{layer}-comp{idx}-range{start}-{end}"".format(\n        model=model_name,\n        name=prettify_name(params[\'name\']),\n        cls=comp_class,\n        est=args.estimator,\n        mode=edit_mode_str,\n        layer=args.layer,\n        idx=idx,\n        start=params[\'edit_start\'],\n        end=params[\'edit_end\'],\n    )\n\n    out_dir = Path(__file__).parent / \'out\' / \'directions\'\n    makedirs(out_dir / file_ident, exist_ok=True)\n\n    with open(out_dir / f""{file_ident}.pkl"", \'wb\') as outfile:\n        pickle.dump(params, outfile)\n\n    print(f\'Direction ""{name.get()}"" saved as ""{file_ident}.pkl""\')\n\n    batch_size = ui_state.batch_size.get()\n    len_padded = ((num_strips.get() - 1) // batch_size + 1) * batch_size\n    orig_seed = state.seed\n\n    reset_sliders()\n\n    # Limit max resolution\n    max_H = 512\n    ratio = min(1.0, max_H / inst.output_shape[2])\n\n    strips = [[] for _ in range(len_padded)]\n    for b in range(0, len_padded, batch_size):\n        # Resample\n        resample_latent((orig_seed + b) % np.iinfo(np.int32).max)\n\n        sigmas = np.linspace(slider_value, -slider_value, strip_width.get(), dtype=np.float32)\n        for sid, sigma in enumerate(sigmas):\n            ui_state.sliders[idx].set(sigma)\n            \n            # Advance and show results on screen\n            on_draw()\n            root.update()\n            app.update()\n            \n            batch_res = (255*img).byte().permute(0, 2, 3, 1).detach().cpu().numpy()\n\n            for i, data in enumerate(batch_res):\n                # Save individual\n                name_nodots = file_ident.replace(\'.\', \'_\')\n                outname = out_dir / file_ident / f""{name_nodots}_ex{b+i}_{sid}.png""\n                im = Image.fromarray(data)\n                im = im.resize((int(ratio*im.size[0]), int(ratio*im.size[1])), Image.ANTIALIAS)\n                im.save(outname)\n                strips[b+i].append(data)\n\n    for i, strip in enumerate(strips[:num_strips.get()]):\n        print(f\'Saving strip {i + 1}/{num_strips.get()}\', end=\'\\r\', flush=True)\n        data = np.hstack(pad_frames(strip))\n        im = Image.fromarray(data)\n        im = im.resize((int(ratio*im.size[0]), int(ratio*im.size[1])), Image.ANTIALIAS)\n        im.save(out_dir / file_ident / f""{file_ident}_ex{i}.png"")\n\n    # Reset to original state\n    resample_latent(orig_seed)\n    ui_state.sliders[idx].set(slider_value)\n\n\n# Shared by glumpy and tkinter\ndef handle_keypress(code):\n    if code == 65307: # ESC\n        shutdown()\n    elif code == 65360: # HOME\n        reset_sliders()\n    elif code == 114: # R\n        pass #reset_sliders()\n    \ndef shutdown():\n    global pending_close\n    pending_close = True\n\ndef on_key_release(symbol, modifiers):\n    handle_keypress(symbol)\n\nif __name__==\'__main__\':\n    setup_model()\n    setup_ui()\n    resample_latent()\n\n    pending_close = False\n    while not pending_close:\n        root.update()\n        app.update()\n        on_draw()\n        reposition_toolbar()\n\n    root.destroy()'"
utils.py,0,"b'# Copyright 2020 Erik H\xc3\xa4rk\xc3\xb6nen. All rights reserved.\n# This file is licensed to you under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License. You may obtain a copy\n# of the License at http://www.apache.org/licenses/LICENSE-2.0\n\n# Unless required by applicable law or agreed to in writing, software distributed under\n# the License is distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR REPRESENTATIONS\n# OF ANY KIND, either express or implied. See the License for the specific language\n# governing permissions and limitations under the License.\n\nimport string\nimport numpy as np\nfrom pathlib import Path\nimport requests\nimport pickle\nimport sys\nimport re\n\ndef prettify_name(name):\n    valid = ""-_%s%s"" % (string.ascii_letters, string.digits)\n    return \'\'.join(map(lambda c : c if c in valid else \'_\', name))\n\n# Add padding to sequence of images\n# Used in conjunction with np.hstack/np.vstack\n# By default: adds one 64th of the width of horizontal padding\ndef pad_frames(strip, pad_fract_horiz=64, pad_fract_vert=0, pad_value=None):\n    dtype = strip[0].dtype\n    if pad_value is None:\n        if dtype in [np.float32, np.float64]:\n            pad_value = 1.0\n        else:\n            pad_value = np.iinfo(dtype).max\n    \n    frames = [strip[0]]\n    for frame in strip[1:]:\n        if pad_fract_horiz > 0:\n            frames.append(pad_value*np.ones((frame.shape[0], frame.shape[1]//pad_fract_horiz, 3), dtype=dtype))\n        elif pad_fract_vert > 0:\n            frames.append(pad_value*np.ones((frame.shape[0]//pad_fract_vert, frame.shape[1], 3), dtype=dtype))\n        frames.append(frame)\n    return frames\n\n\ndef download_google_drive(url, output_name):\n    print(\'Downloading\', url)\n    session = requests.Session()\n    r = session.get(url, allow_redirects=True)\n    r.raise_for_status()\n\n    # Google Drive virus check message\n    if r.encoding is not None:\n        tokens = re.search(\'(confirm=.+)&amp;id\', str(r.content))\n        assert tokens is not None, \'Could not extract token from response\'\n\n        url = url.replace(\'id=\', f\'{tokens[1]}&id=\')\n        r = session.get(url, allow_redirects=True)\n        r.raise_for_status()\n\n    assert r.encoding is None, f\'Failed to download weight file from {url}\'\n\n    with open(output_name, \'wb\') as f:\n        f.write(r.content)\n\ndef download_generic(url, output_name):\n    print(\'Downloading\', url)\n    session = requests.Session()\n    r = session.get(url, allow_redirects=True)\n    r.raise_for_status()\n\n    # No encoding means raw data\n    if r.encoding is None:\n        with open(output_name, \'wb\') as f:\n            f.write(r.content)\n    else:\n        download_manual(url, output_name)\n\ndef download_manual(url, output_name):\n    outpath = Path(output_name).resolve()\n    while not outpath.is_file():\n        print(\'Could not find checkpoint\')\n        print(f\'Please download the checkpoint from\\n{url}\\nand save it as\\n{outpath}\')\n        input(\'Press any key to continue...\')\n\ndef download_ckpt(url, output_name):\n    if \'drive.google\' in url:\n        download_google_drive(url, output_name)\n    elif \'mega.nz\' in url:\n        download_manual(url, output_name)\n    else:\n        download_generic(url, output_name)'"
visualize.py,14,"b'# Copyright 2020 Erik H\xc3\xa4rk\xc3\xb6nen. All rights reserved.\n# This file is licensed to you under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License. You may obtain a copy\n# of the License at http://www.apache.org/licenses/LICENSE-2.0\n\n# Unless required by applicable law or agreed to in writing, software distributed under\n# the License is distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR REPRESENTATIONS\n# OF ANY KIND, either express or implied. See the License for the specific language\n# governing permissions and limitations under the License.\n\n# Patch for broken CTRL+C handler\n# https://github.com/ContinuumIO/anaconda-issues/issues/905\nimport os\nos.environ[\'FOR_DISABLE_CONSOLE_CTRL_HANDLER\'] = \'1\'\n\nimport torch, json, numpy as np\nfrom types import SimpleNamespace\nimport matplotlib.pyplot as plt\nfrom pathlib import Path\nfrom os import makedirs\nfrom PIL import Image\nfrom netdissect import proggan, nethook, easydict, zdataset\nfrom netdissect.modelconfig import create_instrumented_model\nfrom estimators import get_estimator\nfrom models import get_instrumented_model\nfrom scipy.cluster.vq import kmeans\nimport re\nimport sys\nimport datetime\nimport argparse\nfrom tqdm import trange\nfrom config import Config\nfrom decomposition import get_random_dirs, get_or_compute, get_max_batch_size, SEED_VISUALIZATION\nfrom utils import pad_frames \n\ndef x_closest(p):\n    distances = np.sqrt(np.sum((X - p)**2, axis=-1))\n    idx = np.argmin(distances)\n    return distances[idx], X[idx]\n\ndef make_gif(imgs, duration_secs, outname):\n    head, *tail = [Image.fromarray((x * 255).astype(np.uint8)) for x in imgs]\n    ms_per_frame = 1000 * duration_secs / instances\n    head.save(outname, format=\'GIF\', append_images=tail, save_all=True, duration=ms_per_frame, loop=0)\n\ndef make_mp4(imgs, duration_secs, outname):\n    import shutil\n    import subprocess as sp\n\n    FFMPEG_BIN = shutil.which(""ffmpeg"")\n    assert FFMPEG_BIN is not None, \'ffmpeg not found, install with ""conda install -c conda-forge ffmpeg""\'\n    assert len(imgs[0].shape) == 3, \'Invalid shape of frame data\'\n    \n    resolution = imgs[0].shape[0:2]\n    fps = int(len(imgs) / duration_secs)\n\n    command = [ FFMPEG_BIN,\n        \'-y\', # overwrite output file\n        \'-f\', \'rawvideo\',\n        \'-vcodec\',\'rawvideo\',\n        \'-s\', f\'{resolution[0]}x{resolution[1]}\', # size of one frame\n        \'-pix_fmt\', \'rgb24\',\n        \'-r\', f\'{fps}\',\n        \'-i\', \'-\', # imput from pipe\n        \'-an\', # no audio\n        \'-c:v\', \'libx264\',\n        \'-preset\', \'slow\',\n        \'-crf\', \'17\',\n        str(Path(outname).with_suffix(\'.mp4\')) ]\n    \n    frame_data = np.concatenate([(x * 255).astype(np.uint8).reshape(-1) for x in imgs])\n    with sp.Popen(command, stdin=sp.PIPE, stdout=sp.PIPE, stderr=sp.PIPE) as p:\n        ret = p.communicate(frame_data.tobytes())\n        if p.returncode != 0:\n            print(ret[1].decode(""utf-8""))\n            raise sp.CalledProcessError(p.returncode, command)\n\n\ndef make_grid(latent, lat_mean, lat_comp, lat_stdev, act_mean, act_comp, act_stdev, scale=1, n_rows=10, n_cols=5, make_plots=True, edit_type=\'latent\'):\n    from notebooks.notebook_utils import create_strip_centered\n\n    inst.remove_edits()\n    x_range = np.linspace(-scale, scale, n_cols, dtype=np.float32) # scale in sigmas\n\n    rows = []\n    for r in range(n_rows):\n        curr_row = []\n        out_batch = create_strip_centered(inst, edit_type, layer_key, [latent],\n            act_comp[r], lat_comp[r], act_stdev[r], lat_stdev[r], act_mean, lat_mean, scale, 0, -1, n_cols)[0]\n        for i, img in enumerate(out_batch):\n            curr_row.append((\'c{}_{:.2f}\'.format(r, x_range[i]), img))\n\n        rows.append(curr_row[:n_cols])\n\n    inst.remove_edits()\n    \n    if make_plots:\n        # If more rows than columns, make several blocks side by side\n        n_blocks = 2 if n_rows > n_cols else 1\n        \n        for r, data in enumerate(rows):\n            # Add white borders\n            imgs = pad_frames([img for _, img in data]) \n            \n            coord = ((r * n_blocks) % n_rows) + ((r * n_blocks) // n_rows)\n            plt.subplot(n_rows//n_blocks, n_blocks, 1 + coord)\n            plt.imshow(np.hstack(imgs))\n            \n            # Custom x-axis labels\n            W = imgs[0].shape[1] # image width\n            P = imgs[1].shape[1] # padding width\n            locs = [(0.5*W + i*(W+P)) for i in range(n_cols)]\n            plt.xticks(locs, [""{:.2f}"".format(v) for v in x_range])\n            plt.yticks([])\n            plt.ylabel(f\'C{r}\')\n\n        plt.tight_layout()\n        plt.subplots_adjust(top=0.96) # make room for suptitle\n\n    return [img for row in rows for img in row]\n\n\n######################\n### Visualize results\n######################\n\nif __name__ == \'__main__\':\n    global max_batch, sample_shape, feature_shape, inst, args, layer_key, model\n\n    args = Config().from_args()\n    t_start = datetime.datetime.now()\n    timestamp = lambda : datetime.datetime.now().strftime(""%d.%m %H:%M"")\n    print(f\'[{timestamp()}] {args.model}, {args.layer}, {args.estimator}\')\n\n    # Ensure reproducibility\n    torch.manual_seed(0) # also sets cuda seeds\n    np.random.seed(0)\n\n    # Speed up backend\n    torch.backends.cudnn.benchmark = True\n    torch.autograd.set_grad_enabled(False)\n\n    has_gpu = torch.cuda.is_available()\n    device = torch.device(\'cuda\' if has_gpu else \'cpu\')\n    layer_key = args.layer\n    layer_name = layer_key #layer_key.lower().split(\'.\')[-1]\n\n    basedir = Path(__file__).parent.resolve()\n    outdir = basedir / \'out\'\n\n    # Load model\n    inst = get_instrumented_model(args.model, args.output_class, layer_key, device, use_w=args.use_w)\n    model = inst.model\n    feature_shape = inst.feature_shape[layer_key]\n    latent_shape = model.get_latent_shape()\n    print(\'Feature shape:\', feature_shape)\n\n    # Layout of activations\n    if len(feature_shape) != 4: # non-spatial\n        axis_mask = np.ones(len(feature_shape), dtype=np.int32)\n    else:\n        axis_mask = np.array([0, 1, 1, 1]) # only batch fixed => whole activation volume used\n\n    # Shape of sample passed to PCA\n    sample_shape = feature_shape*axis_mask\n    sample_shape[sample_shape == 0] = 1\n\n    # Load or compute components\n    dump_name = get_or_compute(args, inst)\n    data = np.load(dump_name, allow_pickle=False) # does not contain object arrays\n    X_comp = data[\'act_comp\']\n    X_global_mean = data[\'act_mean\']\n    X_stdev = data[\'act_stdev\']\n    X_var_ratio = data[\'var_ratio\']\n    X_stdev_random = data[\'random_stdevs\']\n    Z_global_mean = data[\'lat_mean\']\n    Z_comp = data[\'lat_comp\']\n    Z_stdev = data[\'lat_stdev\']\n    n_comp = X_comp.shape[0]\n    data.close()\n\n    # Transfer components to device\n    tensors = SimpleNamespace(\n        X_comp = torch.from_numpy(X_comp).to(device).float(), #-1, 1, C, H, W\n        X_global_mean = torch.from_numpy(X_global_mean).to(device).float(), # 1, C, H, W\n        X_stdev = torch.from_numpy(X_stdev).to(device).float(),\n        Z_comp = torch.from_numpy(Z_comp).to(device).float(),\n        Z_stdev = torch.from_numpy(Z_stdev).to(device).float(),\n        Z_global_mean = torch.from_numpy(Z_global_mean).to(device).float(),\n    )\n\n    transformer = get_estimator(args.estimator, n_comp, args.sparsity)\n    tr_param_str = transformer.get_param_str()\n\n    # Compute max batch size given VRAM usage\n    max_batch = args.batch_size or (get_max_batch_size(inst, device) if has_gpu else 1)\n    print(\'Batch size:\', max_batch)\n\n    def show():\n        if args.batch_mode:\n            plt.close(\'all\')\n        else:\n            plt.show()\n\n    print(f\'[{timestamp()}] Creating visualizations\')\n\n    # Ensure visualization gets new samples\n    torch.manual_seed(SEED_VISUALIZATION)\n    np.random.seed(SEED_VISUALIZATION)\n\n    # Make output directories\n    est_id = f\'spca_{args.sparsity}\' if args.estimator == \'spca\' else args.estimator\n    outdir_comp = outdir/model.name/layer_key.lower()/est_id/\'comp\'\n    outdir_inst = outdir/model.name/layer_key.lower()/est_id/\'inst\'\n    outdir_summ = outdir/model.name/layer_key.lower()/est_id/\'summ\'\n    makedirs(outdir_comp, exist_ok=True)\n    makedirs(outdir_inst, exist_ok=True)\n    makedirs(outdir_summ, exist_ok=True)\n\n    # Measure component sparsity (!= activation sparsity)\n    sparsity = np.mean(X_comp == 0) # percentage of zero values in components\n    print(f\'Sparsity: {sparsity:.2f}\')\n\n    def get_edit_name(mode):\n        if mode == \'activation\':\n            is_stylegan = \'StyleGAN\' in args.model\n            is_w = layer_key in [\'style\', \'g_mapping\']\n            return \'W\' if (is_stylegan and is_w) else \'ACT\'\n        elif mode == \'latent\':\n            return model.latent_space_name()\n        elif mode == \'both\':\n            return \'BOTH\'\n        else:\n            raise RuntimeError(f\'Unknown edit mode {mode}\')\n\n    # Only visualize applicable edit modes\n    if args.use_w and layer_key in [\'style\', \'g_mapping\']:\n        edit_modes = [\'latent\'] # activation edit is the same\n    else:\n        edit_modes = [\'activation\', \'latent\']\n\n    # Summary grid, real components\n    for edit_mode in edit_modes:\n        plt.figure(figsize = (14,12))\n        plt.suptitle(f""{args.estimator.upper()}: {model.name} - {layer_name}, {get_edit_name(edit_mode)} edit"", size=16)\n        make_grid(tensors.Z_global_mean, tensors.Z_global_mean, tensors.Z_comp, tensors.Z_stdev, tensors.X_global_mean,\n            tensors.X_comp, tensors.X_stdev, scale=args.sigma, edit_type=edit_mode, n_rows=14)\n        plt.savefig(outdir_summ / f\'components_{get_edit_name(edit_mode)}.jpg\', dpi=300)\n        show()\n\n    if args.make_video:\n        components = 15\n        instances = 150\n        \n        # One reasonable, one over the top\n        for sigma in [args.sigma, 3*args.sigma]:\n            for c in range(components):\n                for edit_mode in edit_modes:\n                    frames = make_grid(tensors.Z_global_mean, tensors.Z_global_mean, tensors.Z_comp[c:c+1, :, :], tensors.Z_stdev[c:c+1], tensors.X_global_mean,\n                        tensors.X_comp[c:c+1, :, :], tensors.X_stdev[c:c+1], n_rows=1, n_cols=instances, scale=sigma, make_plots=False, edit_type=edit_mode)\n                    plt.close(\'all\')\n\n                    frames = [x for _, x in frames]\n                    frames = frames + frames[::-1]\n                    make_mp4(frames, 5, outdir_comp / f\'{get_edit_name(edit_mode)}_sigma{sigma}_comp{c}.mp4\')\n\n    \n    # Summary grid, random directions\n    # Using the stdevs of the principal components for same norm\n    random_dirs_act = torch.from_numpy(get_random_dirs(n_comp, np.prod(sample_shape)).reshape(-1, *sample_shape)).to(device)\n    random_dirs_z = torch.from_numpy(get_random_dirs(n_comp, np.prod(inst.input_shape)).reshape(-1, *latent_shape)).to(device)\n    \n    for edit_mode in edit_modes:\n        plt.figure(figsize = (14,12))\n        plt.suptitle(f""{model.name} - {layer_name}, random directions w/ PC stdevs, {get_edit_name(edit_mode)} edit"", size=16)\n        make_grid(tensors.Z_global_mean, tensors.Z_global_mean, random_dirs_z, tensors.Z_stdev,\n            tensors.X_global_mean, random_dirs_act, tensors.X_stdev, scale=args.sigma, edit_type=edit_mode, n_rows=14)\n        plt.savefig(outdir_summ / f\'random_dirs_{get_edit_name(edit_mode)}.jpg\', dpi=300)\n        show()\n\n    # Random instances w/ components added\n    n_random_imgs = 10\n    latents = model.sample_latent(n_samples=n_random_imgs)\n\n    for img_idx in trange(n_random_imgs, desc=\'Random images\', ascii=True):\n        #print(f\'Creating visualizations for random image {img_idx+1}/{n_random_imgs}\')\n        z = latents[img_idx][None, ...]\n\n        # Summary grid, real components\n        for edit_mode in edit_modes:\n            plt.figure(figsize = (14,12))\n            plt.suptitle(f""{args.estimator.upper()}: {model.name} - {layer_name}, {get_edit_name(edit_mode)} edit"", size=16)\n            make_grid(z, tensors.Z_global_mean, tensors.Z_comp, tensors.Z_stdev,\n                tensors.X_global_mean, tensors.X_comp, tensors.X_stdev, scale=args.sigma, edit_type=edit_mode, n_rows=14)\n            plt.savefig(outdir_summ / f\'samp{img_idx}_real_{get_edit_name(edit_mode)}.jpg\', dpi=300)\n            show()\n\n        if args.make_video:\n            components = 5\n            instances = 150\n            \n            # One reasonable, one over the top\n            for sigma in [args.sigma, 3*args.sigma]: #[2, 5]:\n                for edit_mode in edit_modes:\n                    imgs = make_grid(z, tensors.Z_global_mean, tensors.Z_comp, tensors.Z_stdev, tensors.X_global_mean, tensors.X_comp, tensors.X_stdev,\n                        n_rows=components, n_cols=instances, scale=sigma, make_plots=False, edit_type=edit_mode)\n                    plt.close(\'all\')\n\n                    for c in range(components):\n                        frames = [x for _, x in imgs[c*instances:(c+1)*instances]]\n                        frames = frames + frames[::-1]\n                        make_mp4(frames, 5, outdir_inst / f\'{get_edit_name(edit_mode)}_sigma{sigma}_img{img_idx}_comp{c}.mp4\')\n\n    print(\'Done in\', datetime.datetime.now() - t_start)'"
models/__init__.py,0,"b'# Copyright 2020 Erik H\xc3\xa4rk\xc3\xb6nen. All rights reserved.\n# This file is licensed to you under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License. You may obtain a copy\n# of the License at http://www.apache.org/licenses/LICENSE-2.0\n\n# Unless required by applicable law or agreed to in writing, software distributed under\n# the License is distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR REPRESENTATIONS\n# OF ANY KIND, either express or implied. See the License for the specific language\n# governing permissions and limitations under the License.\n\nfrom .wrappers import *'"
models/wrappers.py,22,"b'# Copyright 2020 Erik H\xc3\xa4rk\xc3\xb6nen. All rights reserved.\n# This file is licensed to you under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License. You may obtain a copy\n# of the License at http://www.apache.org/licenses/LICENSE-2.0\n\n# Unless required by applicable law or agreed to in writing, software distributed under\n# the License is distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR REPRESENTATIONS\n# OF ANY KIND, either express or implied. See the License for the specific language\n# governing permissions and limitations under the License.\n\nimport torch\nimport numpy as np\nimport re\nimport os\nimport random\nfrom pathlib import Path\nfrom types import SimpleNamespace\nfrom utils import download_ckpt\nfrom netdissect import proggan, zdataset\nfrom . import biggan\nfrom . import stylegan\nfrom . import stylegan2\nfrom abc import abstractmethod, ABC as AbstractBaseClass\n\nclass BaseModel(AbstractBaseClass, torch.nn.Module):\n\n    # Set parameters for identifying model from instance\n    def __init__(self, model_name, class_name):\n        super(BaseModel, self).__init__()\n        self.model_name = model_name\n        self.outclass = class_name\n\n    # Stop model evaluation as soon as possible after\n    # given layer has been executed, used to speed up\n    # netdissect.InstrumentedModel::retain_layer().\n    # Validate with tests/partial_forward_test.py\n    # Can use forward() as fallback at the cost of performance.\n    @abstractmethod\n    def partial_forward(self, x, layer_name):\n        pass\n\n    # Generate batch of latent vectors\n    @abstractmethod\n    def sample_latent(self, n_samples=1, seed=None, truncation=None):\n        pass\n\n    # Maximum number of latents that can be provided\n    # Typically one for each layer\n    def get_max_latents(self):\n        return 1\n\n    # Name of primary latent space\n    # E.g. StyleGAN can alternatively use W\n    def latent_space_name(self):\n        return \'Z\'\n\n    def get_latent_shape(self):\n        return tuple(self.sample_latent(1).shape)\n\n    def get_latent_dims(self):\n        return np.prod(self.get_latent_shape())\n\n    def set_output_class(self, new_class):\n        self.outclass = new_class\n\n    # Map from typical range [-1, 1] to [0, 1]\n    def forward(self, x):\n        out = self.model.forward(x)\n        return 0.5*(out+1)\n\n    # Generate images and convert to numpy\n    def sample_np(self, z=None, n_samples=1, seed=None):\n        if z is None:\n            z = self.sample_latent(n_samples, seed=seed)\n        elif isinstance(z, list):\n            z = [torch.tensor(l).to(self.device) if not torch.is_tensor(l) else l for l in z]\n        elif not torch.is_tensor(z):\n            z = torch.tensor(z).to(self.device)\n        img = self.forward(z)\n        img_np = img.permute(0, 2, 3, 1).cpu().detach().numpy()\n        return np.clip(img_np, 0.0, 1.0).squeeze()\n\n    # For models that use part of latent as conditioning\n    def get_conditional_state(self, z):\n        return None\n\n    # For models that use part of latent as conditioning\n    def set_conditional_state(self, z, c):\n        return z\n\n    def named_modules(self, *args, **kwargs):\n        return self.model.named_modules(*args, **kwargs)\n\n# PyTorch port of StyleGAN 2\nclass StyleGAN2(BaseModel):\n    def __init__(self, device, class_name, truncation=1.0, use_w=False):\n        super(StyleGAN2, self).__init__(\'StyleGAN2\', class_name or \'ffhq\')\n        self.device = device\n        self.truncation = truncation\n        self.latent_avg = None\n        self.w_primary = use_w # use W as primary latent space?\n\n        # Image widths\n        configs = {\n            # Converted NVIDIA official\n            \'ffhq\': 1024,\n            \'car\': 512,\n            \'cat\': 256,\n            \'church\': 256,\n            \'horse\': 256,\n            # Tuomas\n            \'bedrooms\': 256,\n            \'kitchen\': 256,\n            \'places\': 256,\n        }\n\n        assert self.outclass in configs, \\\n            f\'Invalid StyleGAN2 class {self.outclass}, should be one of [{"", "".join(configs.keys())}]\'\n\n        self.resolution = configs[self.outclass]\n        self.name = f\'StyleGAN2-{self.outclass}\'\n        self.has_latent_residual = True\n        self.load_model()\n        self.set_noise_seed(0)\n\n    def latent_space_name(self):\n        return \'W\' if self.w_primary else \'Z\'\n\n    def use_w(self):\n        self.w_primary = True\n\n    def use_z(self):\n        self.w_primary = False\n\n    # URLs created with https://sites.google.com/site/gdocs2direct/\n    def download_checkpoint(self, outfile):\n        checkpoints = {\n            \'horse\': \'https://drive.google.com/uc?export=download&id=18SkqWAkgt0fIwDEf2pqeaenNi4OoCo-0\',\n            \'ffhq\': \'https://drive.google.com/uc?export=download&id=1FJRwzAkV-XWbxgTwxEmEACvuqF5DsBiV\',\n            \'church\': \'https://drive.google.com/uc?export=download&id=1HFM694112b_im01JT7wop0faftw9ty5g\',\n            \'car\': \'https://drive.google.com/uc?export=download&id=1iRoWclWVbDBAy5iXYZrQnKYSbZUqXI6y\',\n            \'cat\': \'https://drive.google.com/uc?export=download&id=15vJP8GDr0FlRYpE8gD7CdeEz2mXrQMgN\',\n            \'places\': \'https://drive.google.com/uc?export=download&id=1X8-wIH3aYKjgDZt4KMOtQzN1m4AlCVhm\',\n            \'bedrooms\': \'https://drive.google.com/uc?export=download&id=1nZTW7mjazs-qPhkmbsOLLA_6qws-eNQu\',\n            \'kitchen\': \'https://drive.google.com/uc?export=download&id=15dCpnZ1YLAnETAPB0FGmXwdBclbwMEkZ\'\n        }\n\n        url = checkpoints[self.outclass]\n        download_ckpt(url, outfile)\n\n    def load_model(self):\n        checkpoint_root = os.environ.get(\'GANCONTROL_CHECKPOINT_DIR\', Path(__file__).parent / \'checkpoints\')\n        checkpoint = Path(checkpoint_root) / f\'stylegan2/stylegan2_{self.outclass}_{self.resolution}.pt\'\n        \n        self.model = stylegan2.Generator(self.resolution, 512, 8).to(self.device)\n\n        if not checkpoint.is_file():\n            os.makedirs(checkpoint.parent, exist_ok=True)\n            self.download_checkpoint(checkpoint)\n        \n        ckpt = torch.load(checkpoint)\n        self.model.load_state_dict(ckpt[\'g_ema\'], strict=False)\n        self.latent_avg = ckpt[\'latent_avg\'].to(self.device)\n\n    def sample_latent(self, n_samples=1, seed=None, truncation=None):\n        if seed is None:\n            seed = np.random.randint(np.iinfo(np.int32).max) # use (reproducible) global rand state\n\n        rng = np.random.RandomState(seed)\n        z = torch.from_numpy(\n                rng.standard_normal(512 * n_samples)\n                .reshape(n_samples, 512)).float().to(self.device) #[N, 512]\n        \n        if self.w_primary:\n            z = self.model.style(z)\n\n        return z\n\n    def get_max_latents(self):\n        return self.model.n_latent\n\n    def set_output_class(self, new_class):\n        if self.outclass != new_class:\n            raise RuntimeError(\'StyleGAN2: cannot change output class without reloading\')\n    \n    def forward(self, x):\n        x = x if isinstance(x, list) else [x]\n        out, _ = self.model(x, noise=self.noise,\n            truncation=self.truncation, truncation_latent=self.latent_avg, input_is_w=self.w_primary)\n        return 0.5*(out+1)\n\n    def partial_forward(self, x, layer_name):\n        styles = x if isinstance(x, list) else [x]\n        inject_index = None\n        noise = self.noise\n\n        if not self.w_primary:\n            styles = [self.model.style(s) for s in styles]\n\n        if len(styles) == 1:\n            # One global latent\n            inject_index = self.model.n_latent\n            latent = self.model.strided_style(styles[0].unsqueeze(1).repeat(1, inject_index, 1)) # [N, 18, 512]\n        elif len(styles) == 2:\n            # Latent mixing with two latents\n            if inject_index is None:\n                inject_index = random.randint(1, self.model.n_latent - 1)\n\n            latent = styles[0].unsqueeze(1).repeat(1, inject_index, 1)\n            latent2 = styles[1].unsqueeze(1).repeat(1, self.model.n_latent - inject_index, 1)\n\n            latent = self.model.strided_style(torch.cat([latent, latent2], 1))\n        else:\n            # One latent per layer\n            assert len(styles) == self.model.n_latent, f\'Expected {self.model.n_latents} latents, got {len(styles)}\'\n            styles = torch.stack(styles, dim=1) # [N, 18, 512]\n            latent = self.model.strided_style(styles)\n\n        if \'style\' in layer_name:\n            return\n\n        out = self.model.input(latent)\n        if \'input\' == layer_name:\n            return\n\n        out = self.model.conv1(out, latent[:, 0], noise=noise[0])\n        if \'conv1\' in layer_name:\n            return\n\n        skip = self.model.to_rgb1(out, latent[:, 1])\n        if \'to_rgb1\' in layer_name:\n            return\n\n        i = 1\n        noise_i = 1\n\n        for conv1, conv2, to_rgb in zip(\n            self.model.convs[::2], self.model.convs[1::2], self.model.to_rgbs\n        ):\n            out = conv1(out, latent[:, i], noise=noise[noise_i])\n            if f\'convs.{i-1}\' in layer_name:\n                return\n\n            out = conv2(out, latent[:, i + 1], noise=noise[noise_i + 1])\n            if f\'convs.{i}\' in layer_name:\n                return\n            \n            skip = to_rgb(out, latent[:, i + 2], skip)\n            if f\'to_rgbs.{i//2}\' in layer_name:\n                return\n\n            i += 2\n            noise_i += 2\n\n        image = skip\n\n        raise RuntimeError(f\'Layer {layer_name} not encountered in partial_forward\')\n\n    def set_noise_seed(self, seed):\n        torch.manual_seed(seed)\n        self.noise = [torch.randn(1, 1, 2 ** 2, 2 ** 2, device=self.device)]\n\n        for i in range(3, self.model.log_size + 1):\n            for _ in range(2):\n                self.noise.append(torch.randn(1, 1, 2 ** i, 2 ** i, device=self.device))\n\n# PyTorch port of StyleGAN 1\nclass StyleGAN(BaseModel):\n    def __init__(self, device, class_name, truncation=1.0, use_w=False):\n        super(StyleGAN, self).__init__(\'StyleGAN\', class_name or \'ffhq\')\n        self.device = device\n        self.w_primary = use_w # is W primary latent space?\n\n        configs = {\n            # Official\n            \'ffhq\': 1024,\n            \'celebahq\': 1024,\n            \'bedrooms\': 256,\n            \'cars\': 512,\n            \'cats\': 256,\n            \n            # From https://github.com/justinpinkney/awesome-pretrained-stylegan\n            \'vases\': 1024,\n            \'wikiart\': 512,\n            \'fireworks\': 512,\n            \'abstract\': 512,\n            \'anime\': 512,\n            \'ukiyo-e\': 512,\n        }\n\n        assert self.outclass in configs, \\\n            f\'Invalid StyleGAN class {self.outclass}, should be one of [{"", "".join(configs.keys())}]\'\n\n        self.resolution = configs[self.outclass]\n        self.name = f\'StyleGAN-{self.outclass}\'\n        self.has_latent_residual = True\n        self.load_model()\n        self.set_noise_seed(0)\n\n    def latent_space_name(self):\n        return \'W\' if self.w_primary else \'Z\'\n\n    def use_w(self):\n        self.w_primary = True\n\n    def use_z(self):\n        self.w_primary = False\n\n    def load_model(self):\n        checkpoint_root = os.environ.get(\'GANCONTROL_CHECKPOINT_DIR\', Path(__file__).parent / \'checkpoints\')\n        checkpoint = Path(checkpoint_root) / f\'stylegan/stylegan_{self.outclass}_{self.resolution}.pt\'\n        \n        self.model = stylegan.StyleGAN_G(self.resolution).to(self.device)\n\n        urls_tf = {\n            \'vases\': \'https://thisvesseldoesnotexist.s3-us-west-2.amazonaws.com/public/network-snapshot-008980.pkl\',\n            \'fireworks\': \'https://mega.nz/#!7uBHnACY!quIW-pjdDa7NqnZOYh1z5UemWwPOW6HkYSoJ4usCg9U\',\n            \'abstract\': \'https://mega.nz/#!vCQyHQZT!zdeOg3VvT4922Z2UfxO51xgAfJD-NAK2nW7H_jMlilU\',\n            \'anime\': \'https://mega.nz/#!vawjXISI!F7s13yRicxDA3QYqYDL2kjnc2K7Zk3DwCIYETREmBP4\',\n            \'ukiyo-e\': \'https://drive.google.com/uc?id=1CHbJlci9NhVFifNQb3vCGu6zw4eqzvTd\',\n        }\n\n        urls_torch = {\n            \'celebahq\': \'https://drive.google.com/uc?export=download&id=1lGcRwNoXy_uwXkD6sy43aAa-rMHRR7Ad\',\n            \'bedrooms\': \'https://drive.google.com/uc?export=download&id=1r0_s83-XK2dKlyY3WjNYsfZ5-fnH8QgI\',\n            \'ffhq\': \'https://drive.google.com/uc?export=download&id=1GcxTcLDPYxQqcQjeHpLUutGzwOlXXcks\',\n            \'cars\': \'https://drive.google.com/uc?export=download&id=1aaUXHRHjQ9ww91x4mtPZD0w50fsIkXWt\',\n            \'cats\': \'https://drive.google.com/uc?export=download&id=1JzA5iiS3qPrztVofQAjbb0N4xKdjOOyV\',\n            \'wikiart\': \'https://drive.google.com/uc?export=download&id=1fN3noa7Rsl9slrDXsgZVDsYFxV0O08Vx\',\n        }\n\n        if not checkpoint.is_file():\n            os.makedirs(checkpoint.parent, exist_ok=True)\n            if self.outclass in urls_torch:\n                download_ckpt(urls_torch[self.outclass], checkpoint)\n            else:\n                checkpoint_tf = checkpoint.with_suffix(\'.pkl\')\n                if not checkpoint_tf.is_file():\n                    download_ckpt(urls_tf[self.outclass], checkpoint_tf)\n                print(\'Converting TensorFlow checkpoint to PyTorch\')\n                self.model.export_from_tf(checkpoint_tf)\n        \n        self.model.load_weights(checkpoint)\n\n    def sample_latent(self, n_samples=1, seed=None, truncation=None):\n        if seed is None:\n            seed = np.random.randint(np.iinfo(np.int32).max) # use (reproducible) global rand state\n\n        rng = np.random.RandomState(seed)\n        noise = torch.from_numpy(\n                rng.standard_normal(512 * n_samples)\n                .reshape(n_samples, 512)).float().to(self.device) #[N, 512]\n        \n        if self.w_primary:\n            noise = self.model._modules[\'g_mapping\'].forward(noise)\n        \n        return noise\n\n    def get_max_latents(self):\n        return 18\n\n    def set_output_class(self, new_class):\n        if self.outclass != new_class:\n            raise RuntimeError(\'StyleGAN: cannot change output class without reloading\')\n\n    def forward(self, x):\n        out = self.model.forward(x, latent_is_w=self.w_primary)\n        return 0.5*(out+1)\n\n    # Run model only until given layer\n    def partial_forward(self, x, layer_name):\n        mapping = self.model._modules[\'g_mapping\']\n        G = self.model._modules[\'g_synthesis\']\n        trunc = self.model._modules.get(\'truncation\', lambda x : x)\n\n        if not self.w_primary:\n            x = mapping.forward(x) # handles list inputs\n\n        if isinstance(x, list):\n            x = torch.stack(x, dim=1)\n        else:\n            x = x.unsqueeze(1).expand(-1, 18, -1)\n\n        # Whole mapping\n        if \'g_mapping\' in layer_name:\n            return\n\n        x = trunc(x)\n        if layer_name == \'truncation\':\n            return\n\n        # Get names of children\n        def iterate(m, name, seen):\n            children = getattr(m, \'_modules\', [])\n            if len(children) > 0:\n                for child_name, module in children.items():\n                    seen += iterate(module, f\'{name}.{child_name}\', seen)\n                return seen\n            else:\n                return [name]\n\n        # Generator\n        batch_size = x.size(0)\n        for i, (n, m) in enumerate(G.blocks.items()): # InputBlock or GSynthesisBlock\n            if i == 0:\n                r = m(x[:, 2*i:2*i+2])\n            else:\n                r = m(r, x[:, 2*i:2*i+2])\n\n            children = iterate(m, f\'g_synthesis.blocks.{n}\', [])\n            for c in children:\n                if layer_name in c: # substring\n                    return\n\n        raise RuntimeError(f\'Layer {layer_name} not encountered in partial_forward\')\n\n\n    def set_noise_seed(self, seed):\n        G = self.model._modules[\'g_synthesis\']\n\n        def for_each_child(this, name, func):\n            children = getattr(this, \'_modules\', [])\n            for child_name, module in children.items():\n                for_each_child(module, f\'{name}.{child_name}\', func)            \n            func(this, name)\n\n        def modify(m, name):\n            if isinstance(m, stylegan.NoiseLayer):\n                H, W = [int(s) for s in name.split(\'.\')[2].split(\'x\')]\n                torch.random.manual_seed(seed)\n                m.noise = torch.randn(1, 1, H, W, device=self.device, dtype=torch.float32)\n                #m.noise = 1.0 # should be [N, 1, H, W], but this also works\n\n        for_each_child(G, \'g_synthesis\', modify)\n\nclass GANZooModel(BaseModel):\n    def __init__(self, device, model_name):\n        super(GANZooModel, self).__init__(model_name, \'default\')\n        self.device = device\n        self.base_model = torch.hub.load(\'facebookresearch/pytorch_GAN_zoo:hub\',\n            model_name, pretrained=True, useGPU=(device.type == \'cuda\'))\n        self.model = self.base_model.netG.to(self.device)\n        self.name = model_name\n        self.has_latent_residual = False\n\n    def sample_latent(self, n_samples=1, seed=0, truncation=None):\n        # Uses torch.randn\n        noise, _ = self.base_model.buildNoiseData(n_samples)\n        return noise\n\n    # Don\'t bother for now\n    def partial_forward(self, x, layer_name):\n        return self.forward(x)\n\n    def get_conditional_state(self, z):\n        return z[:, -20:] # last 20 = conditioning\n\n    def set_conditional_state(self, z, c):\n        z[:, -20:] = c\n        return z\n    \n    def forward(self, x):\n        out = self.base_model.test(x)\n        return 0.5*(out+1)\n\n\nclass ProGAN(BaseModel):\n    def __init__(self, device, lsun_class=None):\n        super(ProGAN, self).__init__(\'ProGAN\', lsun_class)\n        self.device = device\n\n        # These are downloaded by GANDissect\n        valid_classes = [ \'bedroom\', \'churchoutdoor\', \'conferenceroom\', \'diningroom\', \'kitchen\', \'livingroom\', \'restaurant\' ]\n        assert self.outclass in valid_classes, \\\n            f\'Invalid LSUN class {self.outclass}, should be one of {valid_classes}\'\n\n        self.load_model()\n        self.name = f\'ProGAN-{self.outclass}\'\n        self.has_latent_residual = False\n\n    def load_model(self):\n        checkpoint_root = os.environ.get(\'GANCONTROL_CHECKPOINT_DIR\', Path(__file__).parent / \'checkpoints\')\n        checkpoint = Path(checkpoint_root) / f\'progan/{self.outclass}_lsun.pth\'\n        \n        if not checkpoint.is_file():\n            os.makedirs(checkpoint.parent, exist_ok=True)\n            url = f\'http://netdissect.csail.mit.edu/data/ganmodel/karras/{self.outclass}_lsun.pth\'\n            download_ckpt(url, checkpoint)\n\n        self.model = proggan.from_pth_file(str(checkpoint.resolve())).to(self.device)\n\n    def sample_latent(self, n_samples=1, seed=None, truncation=None):\n        if seed is None:\n            seed = np.random.randint(np.iinfo(np.int32).max) # use (reproducible) global rand state\n        noise = zdataset.z_sample_for_model(self.model, n_samples, seed=seed)[...]\n        return noise.to(self.device)\n\n    # Run model only until given layer\n    def partial_forward(self, x, layer_name):\n        assert isinstance(self.model, torch.nn.Sequential), \'Expected sequential model\'\n\n        x = x.view(x.shape[0], x.shape[1], 1, 1)\n        for name, module in self.model._modules.items(): # ordered dict\n            x = module(x)\n            if name == layer_name:\n                return\n\n        raise RuntimeError(f\'Layer {layer_name} not encountered in partial_forward\')\n\n\nclass BigGAN(BaseModel):\n    def __init__(self, device, resolution, class_name, truncation=1.0):\n        super(BigGAN, self).__init__(f\'BigGAN-{resolution}\', class_name)\n        self.device = device\n        self.truncation = truncation\n        self.load_model(f\'biggan-deep-{resolution}\')\n        self.set_output_class(class_name or \'husky\')\n        self.name = f\'BigGAN-{resolution}-{self.outclass}-t{self.truncation}\'\n        self.has_latent_residual = True\n\n    # Default implementaiton fails without an internet\n    # connection, even if the model has been cached\n    def load_model(self, name):        \n        if name not in biggan.model.PRETRAINED_MODEL_ARCHIVE_MAP:\n            raise RuntimeError(\'Unknown BigGAN model name\', name)\n        \n        checkpoint_root = os.environ.get(\'GANCONTROL_CHECKPOINT_DIR\', Path(__file__).parent / \'checkpoints\')\n        model_path = Path(checkpoint_root) / name\n\n        os.makedirs(model_path, exist_ok=True)\n        \n        model_file = model_path / biggan.model.WEIGHTS_NAME\n        config_file = model_path / biggan.model.CONFIG_NAME\n        model_url = biggan.model.PRETRAINED_MODEL_ARCHIVE_MAP[name]\n        config_url = biggan.model.PRETRAINED_CONFIG_ARCHIVE_MAP[name]\n\n        for filename, url in ((model_file, model_url), (config_file, config_url)):\n            if not filename.is_file():\n                print(\'Downloading\', url)\n                with open(filename, \'wb\') as f:\n                    if url.startswith(""s3://""):\n                        biggan.s3_get(url, f)\n                    else:\n                        biggan.http_get(url, f)\n\n        self.model = biggan.BigGAN.from_pretrained(model_path).to(self.device)\n\n    def sample_latent(self, n_samples=1, truncation=None, seed=None):\n        if seed is None:\n            seed = np.random.randint(np.iinfo(np.int32).max) # use (reproducible) global rand state\n        \n        noise_vector = biggan.truncated_noise_sample(truncation=truncation or self.truncation, batch_size=n_samples, seed=seed)\n        noise = torch.from_numpy(noise_vector) #[N, 128] \n        \n        return noise.to(self.device)\n\n    # One extra for gen_z\n    def get_max_latents(self):\n        return len(self.model.config.layers) + 1\n\n    def get_conditional_state(self, z):\n        return self.v_class\n\n    def set_conditional_state(self, z, c):\n        self.v_class = c\n    \n    def is_valid_class(self, class_id):\n        if isinstance(class_id, int):\n            return class_id < 1000\n        elif isinstance(class_id, str):\n            return biggan.one_hot_from_names([class_id.replace(\' \', \'_\')]) is not None\n        else:\n            raise RuntimeError(f\'Unknown class identifier {class_id}\')\n\n    def set_output_class(self, class_id):\n        if isinstance(class_id, int):\n            self.v_class = torch.from_numpy(biggan.one_hot_from_int([class_id])).to(self.device)\n            self.outclass = f\'class{class_id}\'\n        elif isinstance(class_id, str):\n            self.outclass = class_id.replace(\' \', \'_\')\n            self.v_class = torch.from_numpy(biggan.one_hot_from_names([class_id])).to(self.device)\n        else:\n            raise RuntimeError(f\'Unknown class identifier {class_id}\')\n    \n    def forward(self, x):        \n        # Duplicate along batch dimension\n        if isinstance(x, list):\n            c = self.v_class.repeat(x[0].shape[0], 1)\n            class_vector = len(x)*[c]\n        else:\n            class_vector = self.v_class.repeat(x.shape[0], 1)\n        out = self.model.forward(x, class_vector, self.truncation)  # [N, 3, 128, 128], in [-1, 1]\n        return 0.5*(out+1)\n\n    # Run model only until given layer\n    # Used to speed up PCA sample collection\n    def partial_forward(self, x, layer_name):\n        if layer_name in [\'embeddings\', \'generator.gen_z\']:\n            n_layers = 0\n        elif \'generator.layers\' in layer_name:\n            layer_base = re.match(\'^generator\\.layers\\.[0-9]+\', layer_name)[0]\n            n_layers = int(layer_base.split(\'.\')[-1]) + 1\n        else:\n            n_layers = len(self.model.config.layers)\n\n        if not isinstance(x, list):\n            x = self.model.n_latents*[x]\n\n        if isinstance(self.v_class, list):\n            labels = [c.repeat(x[0].shape[0], 1) for c in class_label]\n            embed = [self.model.embeddings(l) for l in labels]\n        else:\n            class_label = self.v_class.repeat(x[0].shape[0], 1)\n            embed = len(x)*[self.model.embeddings(class_label)]\n        \n        assert len(x) == self.model.n_latents, f\'Expected {self.model.n_latents} latents, got {len(x)}\'\n        assert len(embed) == self.model.n_latents, f\'Expected {self.model.n_latents} class vectors, got {len(class_label)}\'\n\n        cond_vectors = [torch.cat((z, e), dim=1) for (z, e) in zip(x, embed)]\n\n        # Generator forward\n        z = self.model.generator.gen_z(cond_vectors[0])\n        z = z.view(-1, 4, 4, 16 * self.model.generator.config.channel_width)\n        z = z.permute(0, 3, 1, 2).contiguous()\n\n        cond_idx = 1\n        for i, layer in enumerate(self.model.generator.layers[:n_layers]):\n            if isinstance(layer, biggan.GenBlock):\n                z = layer(z, cond_vectors[cond_idx], self.truncation)\n                cond_idx += 1\n            else:\n                z = layer(z)\n\n        return None\n\ndef get_model(name, output_class, device, **kwargs):\n    # Check if optionally provided existing model can be reused\n    inst = kwargs.get(\'inst\', None)\n    model = kwargs.get(\'model\', None)\n    \n    if inst or model:\n        cached = model or inst.model\n        \n        network_same = (cached.model_name == name)\n        outclass_same = (cached.outclass == output_class)\n        can_change_class = (\'BigGAN\' in name)\n        \n        if network_same and (outclass_same or can_change_class):\n            cached.set_output_class(output_class)\n            return cached\n    \n    if name == \'DCGAN\':\n        import warnings\n        warnings.filterwarnings(""ignore"", message=""nn.functional.tanh is deprecated"")\n        model = GANZooModel(device, \'DCGAN\')\n    elif name == \'ProGAN\':\n        model = ProGAN(device, output_class)\n    elif \'BigGAN\' in name:\n        assert \'-\' in name, \'Please specify BigGAN resolution, e.g. BigGAN-512\'\n        model = BigGAN(device, name.split(\'-\')[-1], class_name=output_class)\n    elif name == \'StyleGAN\':\n        model = StyleGAN(device, class_name=output_class)\n    elif name == \'StyleGAN2\':\n        model = StyleGAN2(device, class_name=output_class)\n    else:\n        raise RuntimeError(f\'Unknown model {name}\')\n\n    return model\n\ndef get_instrumented_model(name, output_class, layers, device, **kwargs):\n    model = get_model(name, output_class, device, **kwargs)\n    model.eval()\n\n    inst = kwargs.get(\'inst\', None)\n    if inst:\n        inst.close()\n\n    if not isinstance(layers, list):\n        layers = [layers]\n\n    # Verify given layer names\n    module_names = [name for (name, _) in model.named_modules()]\n    for layer_name in layers:\n        if not layer_name in module_names:\n            print(f""Layer \'{layer_name}\' not found in model!"")\n            print(""Available layers:"", \'\\n\'.join(module_names))\n            raise RuntimeError(f""Unknown layer \'{layer_name}\'\'"")\n    \n    # Reset StyleGANs to z mode for shape annotation\n    if hasattr(model, \'use_z\'):\n        model.use_z()\n\n    from netdissect.modelconfig import create_instrumented_model\n    inst = create_instrumented_model(SimpleNamespace(\n        model = model,\n        layers = layers,\n        cuda = device.type == \'cuda\',\n        gen = True,\n        latent_shape = model.get_latent_shape()\n    ))\n\n    if kwargs.get(\'use_w\', False):\n        model.use_w()\n\n    return inst'"
netdissect/__init__.py,0,"b""'''\nNetdissect package.\n\nTo run dissection:\n\n1. Load up the convolutional model you wish to dissect, and wrap it\n   in an InstrumentedModel.  Call imodel.retain_layers([layernames,..])\n   to analyze a specified set of layers.\n2. Load the segmentation dataset using the BrodenDataset class;\n   use the transform_image argument to normalize images to be\n   suitable for the model, or the size argument to truncate the dataset.\n3. Write a function to recover the original image (with RGB scaled to\n   [0...1]) given a normalized dataset image; ReverseNormalize in this\n   package inverts transforms.Normalize for this purpose.\n4. Choose a directory in which to write the output, and call\n   dissect(outdir, model, dataset).\n\nExample:\n\n    from netdissect import InstrumentedModel, dissect\n    from netdissect import BrodenDataset, ReverseNormalize\n\n    model = InstrumentedModel(load_my_model())\n    model.eval()\n    model.cuda()\n    model.retain_layers(['conv1', 'conv2', 'conv3', 'conv4', 'conv5'])\n    bds = BrodenDataset('dataset/broden1_227',\n            transform_image=transforms.Compose([\n                transforms.ToTensor(),\n                transforms.Normalize(IMAGE_MEAN, IMAGE_STDEV)]),\n            size=1000)\n    dissect('result/dissect', model, bds,\n            recover_image=ReverseNormalize(IMAGE_MEAN, IMAGE_STDEV),\n            examples_per_unit=10)\n'''\n\nfrom .dissection import dissect, ReverseNormalize\nfrom .dissection import ClassifierSegRunner, GeneratorSegRunner\nfrom .dissection import ImageOnlySegRunner\nfrom .broden import BrodenDataset, ScaleSegmentation, scatter_batch\nfrom .segdata import MultiSegmentDataset\nfrom .nethook import InstrumentedModel\nfrom .zdataset import z_dataset_for_model, z_sample_for_model, standard_z_sample\nfrom . import actviz\nfrom . import progress\nfrom . import runningstats\nfrom . import sampler\n\n__all__ = [\n    'dissect', 'ReverseNormalize',\n    'ClassifierSegRunner', 'GeneratorSegRunner', 'ImageOnlySegRunner',\n    'BrodenDataset', 'ScaleSegmentation', 'scatter_batch',\n    'MultiSegmentDataset',\n    'InstrumentedModel',\n    'z_dataset_for_model', 'z_sample_for_model', 'standard_z_sample'\n    'actviz',\n    'progress',\n    'runningstats',\n    'sampler'\n]\n"""
netdissect/__main__.py,4,"b'import torch, sys, os, argparse, textwrap, numbers, numpy, json, PIL\nfrom torchvision import transforms\nfrom torch.utils.data import TensorDataset\nfrom netdissect.progress import verbose_progress, print_progress\nfrom netdissect import InstrumentedModel, BrodenDataset, dissect\nfrom netdissect import MultiSegmentDataset, GeneratorSegRunner\nfrom netdissect import ImageOnlySegRunner\nfrom netdissect.parallelfolder import ParallelImageFolders\nfrom netdissect.zdataset import z_dataset_for_model\nfrom netdissect.autoeval import autoimport_eval\nfrom netdissect.modelconfig import create_instrumented_model\nfrom netdissect.pidfile import exit_if_job_done, mark_job_done\n\nhelp_epilog = \'\'\'\\\nExample: to dissect three layers of the pretrained alexnet in torchvision:\n\npython -m netdissect \\\\\n        --model ""torchvision.models.alexnet(pretrained=True)"" \\\\\n        --layers features.6:conv3 features.8:conv4 features.10:conv5 \\\\\n        --imgsize 227 \\\\\n        --outdir dissect/alexnet-imagenet\n\nTo dissect a progressive GAN model:\n\npython -m netdissect \\\\\n        --model ""proggan.from_pth_file(\'model/churchoutdoor.pth\')"" \\\\\n        --gan\n\'\'\'\n\ndef main():\n    # Training settings\n    def strpair(arg):\n        p = tuple(arg.split(\':\'))\n        if len(p) == 1:\n            p = p + p\n        return p\n    def intpair(arg):\n        p = arg.split(\',\')\n        if len(p) == 1:\n            p = p + p\n        return tuple(int(v) for v in p)\n\n    parser = argparse.ArgumentParser(description=\'Net dissect utility\',\n            prog=\'python -m netdissect\',\n            epilog=textwrap.dedent(help_epilog),\n            formatter_class=argparse.RawDescriptionHelpFormatter)\n    parser.add_argument(\'--model\', type=str, default=None,\n                        help=\'constructor for the model to test\')\n    parser.add_argument(\'--pthfile\', type=str, default=None,\n                        help=\'filename of .pth file for the model\')\n    parser.add_argument(\'--unstrict\', action=\'store_true\', default=False,\n                        help=\'ignore unexpected pth parameters\')\n    parser.add_argument(\'--submodule\', type=str, default=None,\n                        help=\'submodule to load from pthfile\')\n    parser.add_argument(\'--outdir\', type=str, default=\'dissect\',\n                        help=\'directory for dissection output\')\n    parser.add_argument(\'--layers\', type=strpair, nargs=\'+\',\n                        help=\'space-separated list of layer names to dissect\' +\n                        \', in the form layername[:reportedname]\')\n    parser.add_argument(\'--segments\', type=str, default=\'dataset/broden\',\n                        help=\'directory containing segmentation dataset\')\n    parser.add_argument(\'--segmenter\', type=str, default=None,\n                        help=\'constructor for asegmenter class\')\n    parser.add_argument(\'--download\', action=\'store_true\', default=False,\n                        help=\'downloads Broden dataset if needed\')\n    parser.add_argument(\'--imagedir\', type=str, default=None,\n                        help=\'directory containing image-only dataset\')\n    parser.add_argument(\'--imgsize\', type=intpair, default=(227, 227),\n                        help=\'input image size to use\')\n    parser.add_argument(\'--netname\', type=str, default=None,\n                        help=\'name for network in generated reports\')\n    parser.add_argument(\'--meta\', type=str, nargs=\'+\',\n                        help=\'json files of metadata to add to report\')\n    parser.add_argument(\'--merge\', type=str,\n                        help=\'json file of unit data to merge in report\')\n    parser.add_argument(\'--examples\', type=int, default=20,\n                        help=\'number of image examples per unit\')\n    parser.add_argument(\'--size\', type=int, default=10000,\n                        help=\'dataset subset size to use\')\n    parser.add_argument(\'--batch_size\', type=int, default=100,\n                        help=\'batch size for forward pass\')\n    parser.add_argument(\'--num_workers\', type=int, default=24,\n                        help=\'number of DataLoader workers\')\n    parser.add_argument(\'--quantile_threshold\', type=strfloat, default=None,\n                        choices=[FloatRange(0.0, 1.0), \'iqr\'],\n                        help=\'quantile to use for masks\')\n    parser.add_argument(\'--no-labels\', action=\'store_true\', default=False,\n                        help=\'disables labeling of units\')\n    parser.add_argument(\'--maxiou\', action=\'store_true\', default=False,\n                        help=\'enables maxiou calculation\')\n    parser.add_argument(\'--covariance\', action=\'store_true\', default=False,\n                        help=\'enables covariance calculation\')\n    parser.add_argument(\'--rank_all_labels\', action=\'store_true\', default=False,\n                        help=\'include low-information labels in rankings\')\n    parser.add_argument(\'--no-images\', action=\'store_true\', default=False,\n                        help=\'disables generation of unit images\')\n    parser.add_argument(\'--no-report\', action=\'store_true\', default=False,\n                        help=\'disables generation report summary\')\n    parser.add_argument(\'--no-cuda\', action=\'store_true\', default=False,\n                        help=\'disables CUDA usage\')\n    parser.add_argument(\'--gen\', action=\'store_true\', default=False,\n                        help=\'test a generator model (e.g., a GAN)\')\n    parser.add_argument(\'--gan\', action=\'store_true\', default=False,\n                        help=\'synonym for --gen\')\n    parser.add_argument(\'--perturbation\', default=None,\n                        help=\'filename of perturbation attack to apply\')\n    parser.add_argument(\'--add_scale_offset\', action=\'store_true\', default=None,\n                        help=\'offsets masks according to stride and padding\')\n    parser.add_argument(\'--quiet\', action=\'store_true\', default=False,\n                        help=\'silences console output\')\n    if len(sys.argv) == 1:\n        parser.print_usage(sys.stderr)\n        sys.exit(1)\n    args = parser.parse_args()\n    args.images = not args.no_images\n    args.report = not args.no_report\n    args.labels = not args.no_labels\n    if args.gan:\n        args.gen = args.gan\n\n    # Set up console output\n    verbose_progress(not args.quiet)\n\n    # Exit right away if job is already done or being done.\n    if args.outdir is not None:\n        exit_if_job_done(args.outdir)\n\n    # Speed up pytorch\n    torch.backends.cudnn.benchmark = True\n\n    # Special case: download flag without model to test.\n    if args.model is None and args.download:\n        from netdissect.broden import ensure_broden_downloaded\n        for resolution in [224, 227, 384]:\n            ensure_broden_downloaded(args.segments, resolution, 1)\n        from netdissect.segmenter import ensure_upp_segmenter_downloaded\n        ensure_upp_segmenter_downloaded(\'dataset/segmodel\')\n        sys.exit(0)\n\n    # Help if broden is not present\n    if not args.gen and not args.imagedir and not os.path.isdir(args.segments):\n        print_progress(\'Segmentation dataset not found at %s.\' % args.segments)\n        print_progress(\'Specify dataset directory using --segments [DIR]\')\n        print_progress(\'To download Broden, run: netdissect --download\')\n        sys.exit(1)\n\n    # Default segmenter class\n    if args.gen and args.segmenter is None:\n        args.segmenter = (""netdissect.segmenter.UnifiedParsingSegmenter("" +\n                ""segsizes=[256], segdiv=\'quad\')"")\n\n    # Default threshold\n    if args.quantile_threshold is None:\n        if args.gen:\n            args.quantile_threshold = \'iqr\'\n        else:\n            args.quantile_threshold = 0.005\n\n    # Set up CUDA\n    args.cuda = not args.no_cuda and torch.cuda.is_available()\n    if args.cuda:\n        torch.backends.cudnn.benchmark = True\n\n    # Construct the network with specified layers instrumented\n    if args.model is None:\n        print_progress(\'No model specified\')\n        sys.exit(1)\n    model = create_instrumented_model(args)\n\n    # Update any metadata from files, if any\n    meta = getattr(model, \'meta\', {})\n    if args.meta:\n        for mfilename in args.meta:\n            with open(mfilename) as f:\n                meta.update(json.load(f))\n\n    # Load any merge data from files\n    mergedata = None\n    if args.merge:\n        with open(args.merge) as f:\n            mergedata = json.load(f)\n\n    # Set up the output directory, verify write access\n    if args.outdir is None:\n        args.outdir = os.path.join(\'dissect\', type(model).__name__)\n        exit_if_job_done(args.outdir)\n        print_progress(\'Writing output into %s.\' % args.outdir)\n    os.makedirs(args.outdir, exist_ok=True)\n    train_dataset = None\n\n    if not args.gen:\n        # Load dataset for classifier case.\n        # Load perturbation\n        perturbation = numpy.load(args.perturbation\n                ) if args.perturbation else None\n        segrunner = None\n\n        # Load broden dataset\n        if args.imagedir is not None:\n            dataset = try_to_load_images(args.imagedir, args.imgsize,\n                    perturbation, args.size)\n            segrunner = ImageOnlySegRunner(dataset)\n        else:\n            dataset = try_to_load_broden(args.segments, args.imgsize, 1,\n                perturbation, args.download, args.size)\n        if dataset is None:\n            dataset = try_to_load_multiseg(args.segments, args.imgsize,\n                    perturbation, args.size)\n        if dataset is None:\n            print_progress(\'No segmentation dataset found in %s\',\n                    args.segments)\n            print_progress(\'use --download to download Broden.\')\n            sys.exit(1)\n    else:\n        # For segmenter case the dataset is just a random z\n        dataset = z_dataset_for_model(model, args.size)\n        train_dataset = z_dataset_for_model(model, args.size, seed=2)\n        segrunner = GeneratorSegRunner(autoimport_eval(args.segmenter))\n\n    # Run dissect\n    dissect(args.outdir, model, dataset,\n            train_dataset=train_dataset,\n            segrunner=segrunner,\n            examples_per_unit=args.examples,\n            netname=args.netname,\n            quantile_threshold=args.quantile_threshold,\n            meta=meta,\n            merge=mergedata,\n            make_images=args.images,\n            make_labels=args.labels,\n            make_maxiou=args.maxiou,\n            make_covariance=args.covariance,\n            make_report=args.report,\n            make_row_images=args.images,\n            make_single_images=True,\n            rank_all_labels=args.rank_all_labels,\n            batch_size=args.batch_size,\n            num_workers=args.num_workers,\n            settings=vars(args))\n\n    # Mark the directory so that it\'s not done again.\n    mark_job_done(args.outdir)\n\nclass AddPerturbation(object):\n    def __init__(self, perturbation):\n        self.perturbation = perturbation\n\n    def __call__(self, pic):\n        if self.perturbation is None:\n            return pic\n        # Convert to a numpy float32 array\n        npyimg = numpy.array(pic, numpy.uint8, copy=False\n                ).astype(numpy.float32)\n        # Center the perturbation\n        oy, ox = ((self.perturbation.shape[d] - npyimg.shape[d]) // 2\n                for d in [0, 1])\n        npyimg += self.perturbation[\n                oy:oy+npyimg.shape[0], ox:ox+npyimg.shape[1]]\n        # Pytorch conventions: as a float it should be [0..1]\n        npyimg.clip(0, 255, npyimg)\n        return npyimg / 255.0\n\ndef test_dissection():\n    verbose_progress(True)\n    from torchvision.models import alexnet\n    from torchvision import transforms\n    model = InstrumentedModel(alexnet(pretrained=True))\n    model.eval()\n    # Load an alexnet\n    model.retain_layers([\n        (\'features.0\', \'conv1\'),\n        (\'features.3\', \'conv2\'),\n        (\'features.6\', \'conv3\'),\n        (\'features.8\', \'conv4\'),\n        (\'features.10\', \'conv5\') ])\n    # load broden dataset\n    bds = BrodenDataset(\'dataset/broden\',\n            transform=transforms.Compose([\n                transforms.ToTensor(),\n                transforms.Normalize(IMAGE_MEAN, IMAGE_STDEV)]),\n            size=100)\n    # run dissect\n    dissect(\'dissect/test\', model, bds,\n            examples_per_unit=10)\n\ndef try_to_load_images(directory, imgsize, perturbation, size):\n    # Load plain image dataset\n    # TODO: allow other normalizations.\n    return ParallelImageFolders(\n            [directory],\n            transform=transforms.Compose([\n                transforms.Resize(imgsize),\n                AddPerturbation(perturbation),\n                transforms.ToTensor(),\n                transforms.Normalize(IMAGE_MEAN, IMAGE_STDEV)]),\n            size=size)\n\ndef try_to_load_broden(directory, imgsize, broden_version, perturbation,\n        download, size):\n    # Load broden dataset\n    ds_resolution = (224 if max(imgsize) <= 224 else\n                     227 if max(imgsize) <= 227 else 384)\n    if not os.path.isfile(os.path.join(directory,\n           \'broden%d_%d\' % (broden_version, ds_resolution), \'index.csv\')):\n        return None\n    return BrodenDataset(directory,\n            resolution=ds_resolution,\n            download=download,\n            broden_version=broden_version,\n            transform=transforms.Compose([\n                transforms.Resize(imgsize),\n                AddPerturbation(perturbation),\n                transforms.ToTensor(),\n                transforms.Normalize(IMAGE_MEAN, IMAGE_STDEV)]),\n            size=size)\n\ndef try_to_load_multiseg(directory, imgsize, perturbation, size):\n    if not os.path.isfile(os.path.join(directory, \'labelnames.json\')):\n        return None\n    minsize = min(imgsize) if hasattr(imgsize, \'__iter__\') else imgsize\n    return MultiSegmentDataset(directory,\n            transform=(transforms.Compose([\n                transforms.Resize(minsize),\n                transforms.CenterCrop(imgsize),\n                AddPerturbation(perturbation),\n                transforms.ToTensor(),\n                transforms.Normalize(IMAGE_MEAN, IMAGE_STDEV)]),\n            transforms.Compose([\n                transforms.Resize(minsize, interpolation=PIL.Image.NEAREST),\n                transforms.CenterCrop(imgsize)])),\n            size=size)\n\ndef add_scale_offset_info(model, layer_names):\n    \'\'\'\n    Creates a \'scale_offset\' property on the model which guesses\n    how to offset the featuremap, in cases where the convolutional\n    padding does not exacly correspond to keeping featuremap pixels\n    centered on the downsampled regions of the input.  This mainly\n    shows up in AlexNet: ResNet and VGG pad convolutions to keep\n    them centered and do not need this.\n    \'\'\'\n    model.scale_offset = {}\n    seen = set()\n    sequence = []\n    aka_map = {}\n    for name in layer_names:\n        aka = name\n        if not isinstance(aka, str):\n            name, aka = name\n        aka_map[name] = aka\n    for name, layer in model.named_modules():\n        sequence.append(layer)\n        if name in aka_map:\n            seen.add(name)\n            aka = aka_map[name]\n            model.scale_offset[aka] = sequence_scale_offset(sequence)\n    for name in aka_map:\n        assert name in seen, (\'Layer %s not found\' % name)\n\ndef dilation_scale_offset(dilations):\n    \'\'\'Composes a list of (k, s, p) into a single total scale and offset.\'\'\'\n    if len(dilations) == 0:\n        return (1, 0)\n    scale, offset = dilation_scale_offset(dilations[1:])\n    kernel, stride, padding = dilations[0]\n    scale *= stride\n    offset *= stride\n    offset += (kernel - 1) / 2.0 - padding\n    return scale, offset\n\ndef dilations(modulelist):\n    \'\'\'Converts a list of modules to (kernel_size, stride, padding)\'\'\'\n    result = []\n    for module in modulelist:\n        settings = tuple(getattr(module, n, d)\n            for n, d in ((\'kernel_size\', 1), (\'stride\', 1), (\'padding\', 0)))\n        settings = (((s, s) if not isinstance(s, tuple) else s)\n            for s in settings)\n        if settings != ((1, 1), (1, 1), (0, 0)):\n            result.append(zip(*settings))\n    return zip(*result)\n\ndef sequence_scale_offset(modulelist):\n    \'\'\'Returns (yscale, yoffset), (xscale, xoffset) given a list of modules\'\'\'\n    return tuple(dilation_scale_offset(d) for d in dilations(modulelist))\n\n\ndef strfloat(s):\n    try:\n        return float(s)\n    except:\n        return s\n\nclass FloatRange(object):\n    def __init__(self, start, end):\n        self.start = start\n        self.end = end\n    def __eq__(self, other):\n        return isinstance(other, float) and self.start <= other <= self.end\n    def __repr__(self):\n        return \'[%g-%g]\' % (self.start, self.end)\n\n# Many models use this normalization.\nIMAGE_MEAN = [0.485, 0.456, 0.406]\nIMAGE_STDEV = [0.229, 0.224, 0.225]\n\nif __name__ == \'__main__\':\n    main()\n'"
netdissect/aceoptimize.py,74,"b'# Instantiate the segmenter gadget.\n# Instantiate the GAN to optimize over\n# Instrument the GAN for editing and optimization.\n# Read quantile stats to learn 99.9th percentile for each unit,\n# and also the 0.01th percentile.\n# Read the median activation conditioned on door presence.\n\nimport os, sys, numpy, torch, argparse, skimage, json, shutil\nfrom PIL import Image\nfrom torch.utils.data import TensorDataset\nfrom matplotlib.figure import Figure\nfrom matplotlib.backends.backend_agg import FigureCanvasAgg as FigureCanvas\nimport matplotlib.gridspec as gridspec\nfrom scipy.ndimage.morphology import binary_dilation\n\nimport netdissect.zdataset\nimport netdissect.nethook\nfrom netdissect.dissection import safe_dir_name\nfrom netdissect.progress import verbose_progress, default_progress\nfrom netdissect.progress import print_progress, desc_progress, post_progress\nfrom netdissect.easydict import EasyDict\nfrom netdissect.workerpool import WorkerPool, WorkerBase\nfrom netdissect.runningstats import RunningQuantile\nfrom netdissect.pidfile import pidfile_taken\nfrom netdissect.modelconfig import create_instrumented_model\nfrom netdissect.autoeval import autoimport_eval\n\ndef main():\n    parser = argparse.ArgumentParser(description=\'ACE optimization utility\',\n            prog=\'python -m netdissect.aceoptimize\')\n    parser.add_argument(\'--model\', type=str, default=None,\n                        help=\'constructor for the model to test\')\n    parser.add_argument(\'--pthfile\', type=str, default=None,\n                        help=\'filename of .pth file for the model\')\n    parser.add_argument(\'--segmenter\', type=str, default=None,\n                        help=\'constructor for asegmenter class\')\n    parser.add_argument(\'--classname\', type=str, default=None,\n                        help=\'intervention classname\')\n    parser.add_argument(\'--layer\', type=str, default=\'layer4\',\n                        help=\'layer name\')\n    parser.add_argument(\'--search_size\', type=int, default=10000,\n                        help=\'size of search for finding training locations\')\n    parser.add_argument(\'--train_size\', type=int, default=1000,\n                        help=\'size of training set\')\n    parser.add_argument(\'--eval_size\', type=int, default=200,\n                        help=\'size of eval set\')\n    parser.add_argument(\'--inference_batch_size\', type=int, default=10,\n                        help=\'forward pass batch size\')\n    parser.add_argument(\'--train_batch_size\', type=int, default=2,\n                        help=\'backprop pass batch size\')\n    parser.add_argument(\'--train_update_freq\', type=int, default=10,\n                        help=\'number of batches for each training update\')\n    parser.add_argument(\'--train_epochs\', type=int, default=10,\n                        help=\'number of epochs of training\')\n    parser.add_argument(\'--l2_lambda\', type=float, default=0.005,\n                        help=\'l2 regularizer hyperparameter\')\n    parser.add_argument(\'--eval_only\', action=\'store_true\', default=False,\n                        help=\'reruns eval only on trained snapshots\')\n    parser.add_argument(\'--no-cuda\', action=\'store_true\', default=False,\n                        help=\'disables CUDA usage\')\n    parser.add_argument(\'--no-cache\', action=\'store_true\', default=False,\n                        help=\'disables reading of cache\')\n    parser.add_argument(\'--outdir\', type=str, default=None,\n                        help=\'dissection directory\')\n    parser.add_argument(\'--variant\', type=str, default=None,\n                        help=\'experiment variant\')\n    args = parser.parse_args()\n    args.cuda = not args.no_cuda and torch.cuda.is_available()\n    torch.backends.cudnn.benchmark = True\n\n    run_command(args)\n\ndef run_command(args):\n    verbose_progress(True)\n    progress = default_progress()\n    classname = args.classname # \'door\'\n    layer = args.layer # \'layer4\'\n    num_eval_units = 20\n\n    assert os.path.isfile(os.path.join(args.outdir, \'dissect.json\')), (\n            ""Should be a dissection directory"")\n\n    if args.variant is None:\n        args.variant = \'ace\'\n\n    if args.l2_lambda != 0.005:\n        args.variant = \'%s_reg%g\' % (args.variant, args.l2_lambda)\n\n    cachedir = os.path.join(args.outdir, safe_dir_name(layer), args.variant,\n            classname)\n\n    if pidfile_taken(os.path.join(cachedir, \'lock.pid\'), True):\n        sys.exit(0)\n\n    # Take defaults for model constructor etc from dissect.json settings.\n    with open(os.path.join(args.outdir, \'dissect.json\')) as f:\n        dissection = EasyDict(json.load(f))\n    if args.model is None:\n        args.model = dissection.settings.model\n    if args.pthfile is None:\n        args.pthfile = dissection.settings.pthfile\n    if args.segmenter is None:\n        args.segmenter = dissection.settings.segmenter\n    # Default segmenter class\n    if args.segmenter is None:\n        args.segmenter = (""netdissect.segmenter.UnifiedParsingSegmenter("" +\n                ""segsizes=[256], segdiv=\'quad\')"")\n\n    if (not args.no_cache and\n        os.path.isfile(os.path.join(cachedir, \'snapshots\', \'epoch-%d.npy\' % (\n            args.train_epochs - 1))) and\n        os.path.isfile(os.path.join(cachedir, \'report.json\'))):\n        print(\'%s already done\' % cachedir)\n        sys.exit(0)\n\n    os.makedirs(cachedir, exist_ok=True)\n\n    # Instantiate generator\n    model = create_instrumented_model(args, gen=True, edit=True,\n            layers=[args.layer])\n    if model is None:\n        print(\'No model specified\')\n        sys.exit(1)\n    # Instantiate segmenter\n    segmenter = autoimport_eval(args.segmenter)\n    labelnames, catname = segmenter.get_label_and_category_names()\n    classnum = [i for i, (n, c) in enumerate(labelnames) if n == classname][0]\n    num_classes = len(labelnames)\n    with open(os.path.join(cachedir, \'labelnames.json\'), \'w\') as f:\n        json.dump(labelnames, f, indent=1)\n\n    # Sample sets for training.\n    full_sample = netdissect.zdataset.z_sample_for_model(model,\n            args.search_size, seed=10)\n    second_sample = netdissect.zdataset.z_sample_for_model(model,\n            args.search_size, seed=11)\n    # Load any cached data.\n    cache_filename = os.path.join(cachedir, \'corpus.npz\')\n    corpus = EasyDict()\n    try:\n        if not args.no_cache:\n            corpus = EasyDict({k: torch.from_numpy(v)\n                for k, v in numpy.load(cache_filename).items()})\n    except:\n        pass\n\n    # The steps for the computation.\n    compute_present_locations(args, corpus, cache_filename,\n            model, segmenter, classnum, full_sample)\n    compute_mean_present_features(args, corpus, cache_filename, model)\n    compute_feature_quantiles(args, corpus, cache_filename, model, full_sample)\n    compute_candidate_locations(args, corpus, cache_filename, model, segmenter,\n            classnum, second_sample)\n    # visualize_training_locations(args, corpus, cachedir, model)\n    init_ablation = initial_ablation(args, args.outdir)\n    scores = train_ablation(args, corpus, cache_filename,\n            model, segmenter, classnum, init_ablation)\n    summarize_scores(args, corpus, cachedir, layer, classname,\n            args.variant, scores)\n    if args.variant == \'ace\':\n        add_ace_ranking_to_dissection(args.outdir, layer, classname, scores)\n    # TODO: do some evaluation.\n\nclass SaveImageWorker(WorkerBase):\n    def work(self, data, filename):\n        Image.fromarray(data).save(filename, optimize=True, quality=80)\n\ndef plot_heatmap(output_filename, data, size=256):\n    fig = Figure(figsize=(1, 1), dpi=size)\n    canvas = FigureCanvas(fig)\n    gs = gridspec.GridSpec(1, 1, left=0.0, right=1.0, bottom=0.0, top=1.0)\n    ax = fig.add_subplot(gs[0])\n    ax.set_axis_off()\n    ax.imshow(data, cmap=\'hot\', aspect=\'equal\', interpolation=\'nearest\',\n              vmin=-1, vmax=1)\n    canvas.print_figure(output_filename, format=\'png\')\n\n\ndef draw_heatmap(output_filename, data, size=256):\n    fig = Figure(figsize=(1, 1), dpi=size)\n    canvas = FigureCanvas(fig)\n    gs = gridspec.GridSpec(1, 1, left=0.0, right=1.0, bottom=0.0, top=1.0)\n    ax = fig.add_subplot(gs[0])\n    ax.set_axis_off()\n    ax.imshow(data, cmap=\'hot\', aspect=\'equal\', interpolation=\'nearest\',\n              vmin=-1, vmax=1)\n    canvas.draw()       # draw the canvas, cache the renderer\n    image = numpy.fromstring(canvas.tostring_rgb(), dtype=\'uint8\').reshape(\n            (size, size, 3))\n    return image\n\ndef compute_present_locations(args, corpus, cache_filename,\n        model, segmenter, classnum, full_sample):\n    # Phase 1.  Identify a set of locations where there are doorways.\n    # Segment the image and find featuremap pixels that maximize the number\n    # of doorway pixels under the featuremap pixel.\n    if all(k in corpus for k in [\'present_indices\',\n            \'object_present_sample\', \'object_present_location\',\n            \'object_location_popularity\', \'weighted_mean_present_feature\']):\n        return\n    progress = default_progress()\n    feature_shape = model.feature_shape[args.layer][2:]\n    num_locations = numpy.prod(feature_shape).item()\n    num_units = model.feature_shape[args.layer][1]\n    with torch.no_grad():\n        weighted_feature_sum = torch.zeros(num_units).cuda()\n        object_presence_scores = []\n        for [zbatch] in progress(\n                torch.utils.data.DataLoader(TensorDataset(full_sample),\n                batch_size=args.inference_batch_size, num_workers=10,\n                pin_memory=True),\n                desc=""Object pool""):\n            zbatch = zbatch.cuda()\n            tensor_image = model(zbatch)\n            segmented_image = segmenter.segment_batch(tensor_image,\n                    downsample=2)\n            mask = (segmented_image == classnum).max(1)[0]\n            score = torch.nn.functional.adaptive_avg_pool2d(\n                    mask.float(), feature_shape)\n            object_presence_scores.append(score.cpu())\n            feat = model.retained_layer(args.layer)\n            weighted_feature_sum += (feat * score[:,None,:,:]).view(\n                    feat.shape[0],feat.shape[1], -1).sum(2).sum(0)\n        object_presence_at_feature = torch.cat(object_presence_scores)\n        object_presence_at_image, object_location_in_image = (\n                object_presence_at_feature.view(args.search_size, -1).max(1))\n        best_presence_scores, best_presence_images = torch.sort(\n                -object_presence_at_image)\n        all_present_indices = torch.sort(\n                best_presence_images[:(args.train_size+args.eval_size)])[0]\n        corpus.present_indices = all_present_indices[:args.train_size]\n        corpus.object_present_sample = full_sample[corpus.present_indices]\n        corpus.object_present_location = object_location_in_image[\n                corpus.present_indices]\n        corpus.object_location_popularity = torch.bincount(\n            corpus.object_present_location,\n            minlength=num_locations)\n        corpus.weighted_mean_present_feature = (weighted_feature_sum.cpu() / (\n            1e-20 + object_presence_at_feature.view(-1).sum()))\n        corpus.eval_present_indices = all_present_indices[-args.eval_size:]\n        corpus.eval_present_sample = full_sample[corpus.eval_present_indices]\n        corpus.eval_present_location = object_location_in_image[\n                corpus.eval_present_indices]\n\n    if cache_filename:\n        numpy.savez(cache_filename, **corpus)\n\ndef compute_mean_present_features(args, corpus, cache_filename, model):\n    # Phase 1.5.  Figure mean activations for every channel where there\n    # is a doorway.\n    if all(k in corpus for k in [\'mean_present_feature\']):\n        return\n    progress = default_progress()\n    with torch.no_grad():\n        total_present_feature = 0\n        for [zbatch, featloc] in progress(\n                torch.utils.data.DataLoader(TensorDataset(\n                    corpus.object_present_sample,\n                    corpus.object_present_location),\n                batch_size=args.inference_batch_size, num_workers=10,\n                pin_memory=True),\n                desc=""Mean activations""):\n            zbatch = zbatch.cuda()\n            featloc = featloc.cuda()\n            tensor_image = model(zbatch)\n            feat = model.retained_layer(args.layer)\n            flatfeat = feat.view(feat.shape[0], feat.shape[1], -1)\n            sum_feature_at_obj = flatfeat[\n                    torch.arange(feat.shape[0]).to(feat.device), :, featloc\n                    ].sum(0)\n            total_present_feature = total_present_feature + sum_feature_at_obj\n        corpus.mean_present_feature = (total_present_feature / len(\n                corpus.object_present_sample)).cpu()\n    if cache_filename:\n        numpy.savez(cache_filename, **corpus)\n\ndef compute_feature_quantiles(args, corpus, cache_filename, model, full_sample):\n    # Phase 1.6.  Figure the 99% and 99.9%ile of every feature.\n    if all(k in corpus for k in [\'feature_99\', \'feature_999\']):\n        return\n    progress = default_progress()\n    with torch.no_grad():\n        rq = RunningQuantile(resolution=10000) # 10x what\'s needed.\n        for [zbatch] in progress(\n                torch.utils.data.DataLoader(TensorDataset(full_sample),\n                batch_size=args.inference_batch_size, num_workers=10,\n                pin_memory=True),\n                desc=""Calculating 0.999 quantile""):\n            zbatch = zbatch.cuda()\n            tensor_image = model(zbatch)\n            feat = model.retained_layer(args.layer)\n            rq.add(feat.permute(0, 2, 3, 1\n                ).contiguous().view(-1, feat.shape[1]))\n        result = rq.quantiles([0.001, 0.01, 0.1, 0.5, 0.9, 0.99, 0.999])\n        corpus.feature_001 = result[:, 0].cpu()\n        corpus.feature_01 = result[:, 1].cpu()\n        corpus.feature_10 = result[:, 2].cpu()\n        corpus.feature_50 = result[:, 3].cpu()\n        corpus.feature_90 = result[:, 4].cpu()\n        corpus.feature_99 = result[:, 5].cpu()\n        corpus.feature_999 = result[:, 6].cpu()\n    numpy.savez(cache_filename, **corpus)\n\ndef compute_candidate_locations(args, corpus, cache_filename, model,\n        segmenter, classnum, second_sample):\n    # Phase 2.  Identify a set of candidate locations for doorways.\n    # Place the median doorway activation in every location of an image\n    # and identify where it can go that doorway pixels increase.\n    if all(k in corpus for k in [\'candidate_indices\',\n            \'candidate_sample\', \'candidate_score\',\n            \'candidate_location\', \'object_score_at_candidate\',\n            \'candidate_location_popularity\']):\n        return\n    progress = default_progress()\n    feature_shape = model.feature_shape[args.layer][2:]\n    num_locations = numpy.prod(feature_shape).item()\n    with torch.no_grad():\n        # Simplify - just treat all locations as possible\n        possible_locations = numpy.arange(num_locations)\n\n        # Speed up search for locations, by weighting probed locations\n        # according to observed distribution.\n        location_weights = (corpus.object_location_popularity).double()\n        location_weights += (location_weights.mean()) / 10.0\n        location_weights = location_weights / location_weights.sum()\n\n        candidate_scores = []\n        object_scores = []\n        prng = numpy.random.RandomState(1)\n        for [zbatch] in progress(\n                torch.utils.data.DataLoader(TensorDataset(second_sample),\n                batch_size=args.inference_batch_size, num_workers=10,\n                pin_memory=True),\n                desc=""Candidate pool""):\n            batch_scores = torch.zeros((len(zbatch),) + feature_shape).cuda()\n            flat_batch_scores = batch_scores.view(len(zbatch), -1)\n            zbatch = zbatch.cuda()\n            tensor_image = model(zbatch)\n            segmented_image = segmenter.segment_batch(tensor_image,\n                    downsample=2)\n            mask = (segmented_image == classnum).max(1)[0]\n            object_score = torch.nn.functional.adaptive_avg_pool2d(\n                    mask.float(), feature_shape)\n            baseline_presence = mask.float().view(mask.shape[0], -1).sum(1)\n\n            edit_mask = torch.zeros((1, 1) + feature_shape).cuda()\n            if \'_tcm\' in args.variant:\n                # variant: top-conditional-mean\n                replace_vec = (corpus.mean_present_feature\n                        [None,:,None,None].cuda())\n            else: # default: weighted mean\n                replace_vec = (corpus.weighted_mean_present_feature\n                        [None,:,None,None].cuda())\n            # Sample 10 random locations to examine.\n            for loc in prng.choice(possible_locations, replace=False,\n                    p=location_weights, size=5):\n                edit_mask.zero_()\n                edit_mask.view(-1)[loc] = 1\n                model.edit_layer(args.layer,\n                        ablation=edit_mask, replacement=replace_vec)\n                tensor_image = model(zbatch)\n                segmented_image = segmenter.segment_batch(tensor_image,\n                    downsample=2)\n                mask = (segmented_image == classnum).max(1)[0]\n                modified_presence = mask.float().view(\n                        mask.shape[0], -1).sum(1)\n                flat_batch_scores[:,loc] = (\n                        modified_presence - baseline_presence)\n            candidate_scores.append(batch_scores.cpu())\n            object_scores.append(object_score.cpu())\n\n        object_scores = torch.cat(object_scores)\n        candidate_scores = torch.cat(candidate_scores)\n        # Eliminate candidates where the object is present.\n        candidate_scores = candidate_scores * (object_scores == 0).float()\n        candidate_score_at_image, candidate_location_in_image = (\n                candidate_scores.view(args.search_size, -1).max(1))\n        best_candidate_scores, best_candidate_images = torch.sort(\n                -candidate_score_at_image)\n        all_candidate_indices = torch.sort(\n                best_candidate_images[:(args.train_size+args.eval_size)])[0]\n        corpus.candidate_indices = all_candidate_indices[:args.train_size]\n        corpus.candidate_sample = second_sample[corpus.candidate_indices]\n        corpus.candidate_location = candidate_location_in_image[\n                corpus.candidate_indices]\n        corpus.candidate_score = candidate_score_at_image[\n                corpus.candidate_indices]\n        corpus.object_score_at_candidate = object_scores.view(\n                len(object_scores), -1)[\n                corpus.candidate_indices, corpus.candidate_location]\n        corpus.candidate_location_popularity = torch.bincount(\n            corpus.candidate_location,\n            minlength=num_locations)\n        corpus.eval_candidate_indices = all_candidate_indices[\n                -args.eval_size:]\n        corpus.eval_candidate_sample = second_sample[\n                corpus.eval_candidate_indices]\n        corpus.eval_candidate_location = candidate_location_in_image[\n                corpus.eval_candidate_indices]\n    numpy.savez(cache_filename, **corpus)\n\ndef visualize_training_locations(args, corpus, cachedir, model):\n    # Phase 2.5 Create visualizations of the corpus images.\n    progress = default_progress()\n    feature_shape = model.feature_shape[args.layer][2:]\n    num_locations = numpy.prod(feature_shape).item()\n    with torch.no_grad():\n        imagedir = os.path.join(cachedir, \'image\')\n        os.makedirs(imagedir, exist_ok=True)\n        image_saver = WorkerPool(SaveImageWorker)\n        for group, group_sample, group_location, group_indices in [\n                (\'present\',\n                    corpus.object_present_sample,\n                    corpus.object_present_location,\n                    corpus.present_indices),\n                (\'candidate\',\n                    corpus.candidate_sample,\n                    corpus.candidate_location,\n                    corpus.candidate_indices)]:\n            for [zbatch, featloc, indices] in progress(\n                    torch.utils.data.DataLoader(TensorDataset(\n                        group_sample, group_location, group_indices),\n                        batch_size=args.inference_batch_size, num_workers=10,\n                        pin_memory=True),\n                    desc=""Visualize %s"" % group):\n                zbatch = zbatch.cuda()\n                tensor_image = model(zbatch)\n                feature_mask = torch.zeros((len(zbatch), 1) + feature_shape)\n                feature_mask.view(len(zbatch), -1).scatter_(\n                        1, featloc[:,None], 1)\n                feature_mask = torch.nn.functional.adaptive_max_pool2d(\n                        feature_mask.float(), tensor_image.shape[-2:]).cuda()\n                yellow = torch.Tensor([1.0, 1.0, -1.0]\n                        )[None, :, None, None].cuda()\n                tensor_image = tensor_image * (1 - 0.5 * feature_mask) + (\n                        0.5 * feature_mask * yellow)\n                byte_image = (((tensor_image+1)/2)*255).clamp(0, 255).byte()\n                numpy_image = byte_image.permute(0, 2, 3, 1).cpu().numpy()\n                for i, index in enumerate(indices):\n                    image_saver.add(numpy_image[i], os.path.join(imagedir,\n                        \'%s_%d.jpg\' % (group, index)))\n    image_saver.join()\n\ndef scale_summary(scale, lownums, highnums):\n    value, order = (-(scale.detach())).cpu().sort(0)\n    lowsum = \' \'.join(\'%d: %.3g\' % (o.item(), -v.item())\n            for v, o in zip(value[:lownums], order[:lownums]))\n    highsum = \' \'.join(\'%d: %.3g\' % (o.item(), -v.item())\n            for v, o in zip(value[-highnums:], order[-highnums:]))\n    return lowsum + \' ... \' + highsum\n\n# Phase 3.  Given those two sets, now optimize a such that:\n#   Door pred lost if we take 0 * a at a candidate (1)\n#   Door pred gained If we take 99.9th activation * a at a candiate (1)\n#\n\n# ADE_au = E | on - E | off)\n#       = cand-frac E_cand | on + nocand-frac E_cand | on\n#        -  door-frac E_door | off + nodoor-frac E_nodoor | off\n#       approx = cand-frac E_cand | on - door-frac E_door | off + K\n# Each batch has both types, and minimizes\n#     door-frac sum(s_c) when pixel off - cand-frac sum(s_c) when pixel on\n\ndef initial_ablation(args, dissectdir):\n    # Load initialization from dissection, based on iou scores.\n    with open(os.path.join(dissectdir, \'dissect.json\')) as f:\n        dissection = EasyDict(json.load(f))\n    lrec = [l for l in dissection.layers if l.layer == args.layer][0]\n    rrec = [r for r in lrec.rankings if r.name == \'%s-iou\' % args.classname\n            ][0]\n    init_scores = -torch.tensor(rrec.score)\n    return init_scores / init_scores.max()\n\ndef ace_loss(segmenter, classnum, model, layer, high_replacement, ablation,\n        pbatch, ploc, cbatch, cloc, run_backward=False,\n        discrete_pixels=False,\n        discrete_units=False,\n        mixed_units=False,\n        ablation_only=False,\n        fullimage_measurement=False,\n        fullimage_ablation=False,\n        ):\n    feature_shape = model.feature_shape[layer][2:]\n    if discrete_units: # discretize ablation to the top N units\n        assert discrete_units > 0\n        d = torch.zeros_like(ablation)\n        top_units = torch.topk(ablation.view(-1), discrete_units)[1]\n        if mixed_units:\n            d.view(-1)[top_units] = ablation.view(-1)[top_units]\n        else:\n            d.view(-1)[top_units] = 1\n        ablation = d\n    # First, ablate a sample of locations with positive presence\n    # and see how much the presence is reduced.\n    p_mask = torch.zeros((len(pbatch), 1) + feature_shape)\n    if fullimage_ablation:\n        p_mask[...] = 1\n    else:\n        p_mask.view(len(pbatch), -1).scatter_(1, ploc[:,None], 1)\n    p_mask = p_mask.cuda()\n    a_p_mask = (ablation * p_mask)\n    model.edit_layer(layer, ablation=a_p_mask, replacement=None)\n    tensor_images = model(pbatch.cuda())\n    assert model._ablation[layer] is a_p_mask\n    erase_effect, erased_mask = segmenter.predict_single_class(\n            tensor_images, classnum, downsample=2)\n    if discrete_pixels: # pixel loss: use mask instead of pred\n        erase_effect = erased_mask.float()\n    erase_downsampled = torch.nn.functional.adaptive_avg_pool2d(\n            erase_effect[:,None,:,:], feature_shape)[:,0,:,:]\n    if fullimage_measurement:\n        erase_loss = erase_downsampled.sum()\n    else:\n        erase_at_loc = erase_downsampled.view(len(erase_downsampled), -1\n                )[torch.arange(len(erase_downsampled)), ploc]\n        erase_loss = erase_at_loc.sum()\n    if run_backward:\n        erase_loss.backward()\n    if ablation_only:\n        return erase_loss\n    # Second, activate a sample of locations that are candidates for\n    # insertion and see how much the presence is increased.\n    c_mask = torch.zeros((len(cbatch), 1) + feature_shape)\n    c_mask.view(len(cbatch), -1).scatter_(1, cloc[:,None], 1)\n    c_mask = c_mask.cuda()\n    a_c_mask = (ablation * c_mask)\n    model.edit_layer(layer, ablation=a_c_mask, replacement=high_replacement)\n    tensor_images = model(cbatch.cuda())\n    assert model._ablation[layer] is a_c_mask\n    add_effect, added_mask = segmenter.predict_single_class(\n            tensor_images, classnum, downsample=2)\n    if discrete_pixels: # pixel loss: use mask instead of pred\n        add_effect = added_mask.float()\n    add_effect = -add_effect\n    add_downsampled = torch.nn.functional.adaptive_avg_pool2d(\n            add_effect[:,None,:,:], feature_shape)[:,0,:,:]\n    if fullimage_measurement:\n        add_loss = add_downsampled.mean()\n    else:\n        add_at_loc = add_downsampled.view(len(add_downsampled), -1\n                )[torch.arange(len(add_downsampled)), ploc]\n        add_loss = add_at_loc.sum()\n    if run_backward:\n        add_loss.backward()\n    return erase_loss + add_loss\n\ndef train_ablation(args, corpus, cachefile, model, segmenter, classnum,\n        initial_ablation=None):\n    progress = default_progress()\n    cachedir = os.path.dirname(cachefile)\n    snapdir = os.path.join(cachedir, \'snapshots\')\n    os.makedirs(snapdir, exist_ok=True)\n\n    # high_replacement = corpus.feature_99[None,:,None,None].cuda()\n    if \'_h99\' in args.variant:\n        high_replacement = corpus.feature_99[None,:,None,None].cuda()\n    elif \'_tcm\' in args.variant:\n        # variant: top-conditional-mean\n        high_replacement = (\n                corpus.mean_present_feature[None,:,None,None].cuda())\n    else: # default: weighted mean\n        high_replacement = (\n                corpus.weighted_mean_present_feature[None,:,None,None].cuda())\n    fullimage_measurement = False\n    ablation_only = False\n    fullimage_ablation = False\n    if \'_fim\' in args.variant:\n        fullimage_measurement = True\n    elif \'_fia\' in args.variant:\n        fullimage_measurement = True\n        ablation_only = True\n        fullimage_ablation = True\n    high_replacement.requires_grad = False\n    for p in model.parameters():\n        p.requires_grad = False\n\n    ablation = torch.zeros(high_replacement.shape).cuda()\n    if initial_ablation is not None:\n        ablation.view(-1)[...] = initial_ablation\n    ablation.requires_grad = True\n    optimizer = torch.optim.Adam([ablation], lr=0.01)\n    start_epoch = 0\n    epoch = 0\n\n    def eval_loss_and_reg():\n        discrete_experiments = dict(\n           # dpixel=dict(discrete_pixels=True),\n           # dunits20=dict(discrete_units=20),\n           # dumix20=dict(discrete_units=20, mixed_units=True),\n           # dunits10=dict(discrete_units=10),\n           # abonly=dict(ablation_only=True),\n           # fimabl=dict(ablation_only=True,\n           #             fullimage_ablation=True,\n           #             fullimage_measurement=True),\n           dboth20=dict(discrete_units=20, discrete_pixels=True),\n           # dbothm20=dict(discrete_units=20, mixed_units=True,\n           #              discrete_pixels=True),\n           # abdisc20=dict(discrete_units=20, discrete_pixels=True,\n           #             ablation_only=True),\n           # abdiscm20=dict(discrete_units=20, mixed_units=True,\n           #             discrete_pixels=True,\n           #             ablation_only=True),\n           # fimadp=dict(discrete_pixels=True,\n           #             ablation_only=True,\n           #             fullimage_ablation=True,\n           #             fullimage_measurement=True),\n           # fimadu10=dict(discrete_units=10,\n           #             ablation_only=True,\n           #             fullimage_ablation=True,\n           #             fullimage_measurement=True),\n           # fimadb10=dict(discrete_units=10, discrete_pixels=True,\n           #             ablation_only=True,\n           #             fullimage_ablation=True,\n           #             fullimage_measurement=True),\n           fimadbm10=dict(discrete_units=10, mixed_units=True,\n                       discrete_pixels=True,\n                       ablation_only=True,\n                       fullimage_ablation=True,\n                       fullimage_measurement=True),\n           # fimadu20=dict(discrete_units=20,\n           #             ablation_only=True,\n           #             fullimage_ablation=True,\n           #             fullimage_measurement=True),\n           # fimadb20=dict(discrete_units=20, discrete_pixels=True,\n           #             ablation_only=True,\n           #             fullimage_ablation=True,\n           #             fullimage_measurement=True),\n           fimadbm20=dict(discrete_units=20, mixed_units=True,\n                       discrete_pixels=True,\n                       ablation_only=True,\n                       fullimage_ablation=True,\n                       fullimage_measurement=True)\n           )\n        with torch.no_grad():\n            total_loss = 0\n            discrete_losses = {k: 0 for k in discrete_experiments}\n            for [pbatch, ploc, cbatch, cloc] in progress(\n                    torch.utils.data.DataLoader(TensorDataset(\n                        corpus.eval_present_sample,\n                        corpus.eval_present_location,\n                        corpus.eval_candidate_sample,\n                        corpus.eval_candidate_location),\n                    batch_size=args.inference_batch_size, num_workers=10,\n                    shuffle=False, pin_memory=True),\n                    desc=""Eval""):\n                # First, put in zeros for the selected units.\n                # Loss is amount of remaining object.\n                total_loss = total_loss + ace_loss(segmenter, classnum,\n                        model, args.layer, high_replacement, ablation,\n                        pbatch, ploc, cbatch, cloc, run_backward=False,\n                        ablation_only=ablation_only,\n                        fullimage_measurement=fullimage_measurement)\n                for k, config in discrete_experiments.items():\n                    discrete_losses[k] = discrete_losses[k] + ace_loss(\n                        segmenter, classnum,\n                        model, args.layer, high_replacement, ablation,\n                        pbatch, ploc, cbatch, cloc, run_backward=False,\n                        **config)\n            avg_loss = (total_loss / args.eval_size).item()\n            avg_d_losses = {k: (d / args.eval_size).item()\n                    for k, d in discrete_losses.items()}\n            regularizer = (args.l2_lambda * ablation.pow(2).sum())\n            print_progress(\'Epoch %d Loss %g Regularizer %g\' %\n                    (epoch, avg_loss, regularizer))\n            print_progress(\' \'.join(\'%s: %g\' % (k, d)\n                    for k, d in avg_d_losses.items()))\n            print_progress(scale_summary(ablation.view(-1), 10, 3))\n            return avg_loss, regularizer, avg_d_losses\n\n    if args.eval_only:\n        # For eval_only, just load each snapshot and re-run validation eval\n        # pass on each one.\n        for epoch in range(-1, args.train_epochs):\n            snapfile = os.path.join(snapdir, \'epoch-%d.pth\' % epoch)\n            if not os.path.exists(snapfile):\n                data = {}\n                if epoch >= 0:\n                    print(\'No epoch %d\' % epoch)\n                    continue\n            else:\n                data = torch.load(snapfile)\n                with torch.no_grad():\n                    ablation[...] = data[\'ablation\'].to(ablation.device)\n                    optimizer.load_state_dict(data[\'optimizer\'])\n            avg_loss, regularizer, new_extra = eval_loss_and_reg()\n            # Keep old values, and update any new ones.\n            extra = {k: v for k, v in data.items()\n                    if k not in [\'ablation\', \'optimizer\', \'avg_loss\']}\n            extra.update(new_extra)\n            torch.save(dict(ablation=ablation, optimizer=optimizer.state_dict(),\n                avg_loss=avg_loss, **extra),\n                os.path.join(snapdir, \'epoch-%d.pth\' % epoch))\n        # Return loaded ablation.\n        return ablation.view(-1).detach().cpu().numpy()\n\n    if not args.no_cache:\n        for start_epoch in reversed(range(args.train_epochs)):\n            snapfile = os.path.join(snapdir, \'epoch-%d.pth\' % start_epoch)\n            if os.path.exists(snapfile):\n                data = torch.load(snapfile)\n                with torch.no_grad():\n                    ablation[...] = data[\'ablation\'].to(ablation.device)\n                    optimizer.load_state_dict(data[\'optimizer\'])\n                start_epoch += 1\n                break\n\n    if start_epoch < args.train_epochs:\n        epoch = start_epoch - 1\n        avg_loss, regularizer, extra = eval_loss_and_reg()\n        if epoch == -1:\n            torch.save(dict(ablation=ablation, optimizer=optimizer.state_dict(),\n                avg_loss=avg_loss, **extra),\n                os.path.join(snapdir, \'epoch-%d.pth\' % epoch))\n\n    update_size = args.train_update_freq * args.train_batch_size\n    for epoch in range(start_epoch, args.train_epochs):\n        candidate_shuffle = torch.randperm(len(corpus.candidate_sample))\n        train_loss = 0\n        for batch_num, [pbatch, ploc, cbatch, cloc] in enumerate(progress(\n                torch.utils.data.DataLoader(TensorDataset(\n                    corpus.object_present_sample,\n                    corpus.object_present_location,\n                    corpus.candidate_sample[candidate_shuffle],\n                    corpus.candidate_location[candidate_shuffle]),\n                batch_size=args.train_batch_size, num_workers=10,\n                shuffle=True, pin_memory=True),\n                desc=""ACE opt epoch %d"" % epoch)):\n            if batch_num % args.train_update_freq == 0:\n                optimizer.zero_grad()\n            # First, put in zeros for the selected units.  Loss is amount\n            # of remaining object.\n            loss = ace_loss(segmenter, classnum,\n                    model, args.layer, high_replacement, ablation,\n                    pbatch, ploc, cbatch, cloc, run_backward=True,\n                    ablation_only=ablation_only,\n                    fullimage_measurement=fullimage_measurement)\n            with torch.no_grad():\n                train_loss = train_loss + loss\n            if (batch_num + 1) % args.train_update_freq == 0:\n                # Third, add some L2 loss to encourage sparsity.\n                regularizer = (args.l2_lambda * update_size\n                        * ablation.pow(2).sum())\n                regularizer.backward()\n                optimizer.step()\n                with torch.no_grad():\n                    ablation.clamp_(0, 1)\n                    post_progress(l=(train_loss/update_size).item(),\n                            r=(regularizer/update_size).item())\n                    train_loss = 0\n\n        avg_loss, regularizer, extra = eval_loss_and_reg()\n        torch.save(dict(ablation=ablation, optimizer=optimizer.state_dict(),\n            avg_loss=avg_loss, **extra),\n            os.path.join(snapdir, \'epoch-%d.pth\' % epoch))\n        numpy.save(os.path.join(snapdir, \'epoch-%d.npy\' % epoch),\n                ablation.detach().cpu().numpy())\n\n    # The output of this phase is this set of scores.\n    return ablation.view(-1).detach().cpu().numpy()\n\n\ndef tensor_to_numpy_image_batch(tensor_image):\n    byte_image = (((tensor_image+1)/2)*255).clamp(0, 255).byte()\n    numpy_image = byte_image.permute(0, 2, 3, 1).cpu().numpy()\n    return numpy_image\n\n# Phase 4: evaluation of intervention\n\ndef evaluate_ablation(args, model, segmenter, eval_sample, classnum, layer,\n        ordering):\n    total_bincount = 0\n    data_size = 0\n    progress = default_progress()\n    for l in model.ablation:\n        model.ablation[l] = None\n    feature_units = model.feature_shape[args.layer][1]\n    feature_shape = model.feature_shape[args.layer][2:]\n    repeats = len(ordering)\n    total_scores = torch.zeros(repeats + 1)\n    for i, batch in enumerate(progress(torch.utils.data.DataLoader(\n                TensorDataset(eval_sample),\n                batch_size=args.inference_batch_size, num_workers=10,\n                pin_memory=True),\n                desc=""Evaluate interventions"")):\n        tensor_image = model(zbatch)\n        segmented_image = segmenter.segment_batch(tensor_image,\n                    downsample=2)\n        mask = (segmented_image == classnum).max(1)[0]\n        downsampled_seg = torch.nn.functional.adaptive_avg_pool2d(\n                mask.float()[:,None,:,:], feature_shape)[:,0,:,:]\n        total_scores[0] += downsampled_seg.sum().cpu()\n        # Now we need to do an intervention for every location\n        # that had a nonzero downsampled_seg, if any.\n        interventions_needed = downsampled_seg.nonzero()\n        location_count = len(interventions_needed)\n        if location_count == 0:\n            continue\n        interventions_needed = interventions_needed.repeat(repeats, 1)\n        inter_z = batch[0][interventions_needed[:,0]].to(device)\n        inter_chan = torch.zeros(repeats, location_count, feature_units,\n                device=device)\n        for j, u in enumerate(ordering):\n            inter_chan[j:, :, u] = 1\n        inter_chan = inter_chan.view(len(inter_z), feature_units)\n        inter_loc = interventions_needed[:,1:]\n        scores = torch.zeros(len(inter_z))\n        batch_size = len(batch[0])\n        for j in range(0, len(inter_z), batch_size):\n            ibz = inter_z[j:j+batch_size]\n            ibl = inter_loc[j:j+batch_size].t()\n            imask = torch.zeros((len(ibz),) + feature_shape, device=ibz.device)\n            imask[(torch.arange(len(ibz)),) + tuple(ibl)] = 1\n            ibc = inter_chan[j:j+batch_size]\n            model.edit_layer(args.layer, ablation=(\n                    imask.float()[:,None,:,:] * ibc[:,:,None,None]))\n            _, seg, _, _, _ = (\n                recovery.recover_im_seg_bc_and_features(\n                    [ibz], model))\n            mask = (seg == classnum).max(1)[0]\n            downsampled_iseg = torch.nn.functional.adaptive_avg_pool2d(\n                    mask.float()[:,None,:,:], feature_shape)[:,0,:,:]\n            scores[j:j+batch_size] = downsampled_iseg[\n                    (torch.arange(len(ibz)),) + tuple(ibl)]\n        scores = scores.view(repeats, location_count).sum(1)\n        total_scores[1:] += scores\n    return total_scores\n\ndef evaluate_interventions(args, model, segmenter, eval_sample,\n        classnum, layer, units):\n    total_bincount = 0\n    data_size = 0\n    progress = default_progress()\n    for l in model.ablation:\n        model.ablation[l] = None\n    feature_units = model.feature_shape[args.layer][1]\n    feature_shape = model.feature_shape[args.layer][2:]\n    repeats = len(ordering)\n    total_scores = torch.zeros(repeats + 1)\n    for i, batch in enumerate(progress(torch.utils.data.DataLoader(\n                TensorDataset(eval_sample),\n                batch_size=args.inference_batch_size, num_workers=10,\n                pin_memory=True),\n                desc=""Evaluate interventions"")):\n        tensor_image = model(zbatch)\n        segmented_image = segmenter.segment_batch(tensor_image,\n                    downsample=2)\n        mask = (segmented_image == classnum).max(1)[0]\n        downsampled_seg = torch.nn.functional.adaptive_avg_pool2d(\n                mask.float()[:,None,:,:], feature_shape)[:,0,:,:]\n        total_scores[0] += downsampled_seg.sum().cpu()\n        # Now we need to do an intervention for every location\n        # that had a nonzero downsampled_seg, if any.\n        interventions_needed = downsampled_seg.nonzero()\n        location_count = len(interventions_needed)\n        if location_count == 0:\n            continue\n        interventions_needed = interventions_needed.repeat(repeats, 1)\n        inter_z = batch[0][interventions_needed[:,0]].to(device)\n        inter_chan = torch.zeros(repeats, location_count, feature_units,\n                device=device)\n        for j, u in enumerate(ordering):\n            inter_chan[j:, :, u] = 1\n        inter_chan = inter_chan.view(len(inter_z), feature_units)\n        inter_loc = interventions_needed[:,1:]\n        scores = torch.zeros(len(inter_z))\n        batch_size = len(batch[0])\n        for j in range(0, len(inter_z), batch_size):\n            ibz = inter_z[j:j+batch_size]\n            ibl = inter_loc[j:j+batch_size].t()\n            imask = torch.zeros((len(ibz),) + feature_shape, device=ibz.device)\n            imask[(torch.arange(len(ibz)),) + tuple(ibl)] = 1\n            ibc = inter_chan[j:j+batch_size]\n            model.ablation[args.layer] = (\n                    imask.float()[:,None,:,:] * ibc[:,:,None,None])\n            _, seg, _, _, _ = (\n                recovery.recover_im_seg_bc_and_features(\n                    [ibz], model))\n            mask = (seg == classnum).max(1)[0]\n            downsampled_iseg = torch.nn.functional.adaptive_avg_pool2d(\n                    mask.float()[:,None,:,:], feature_shape)[:,0,:,:]\n            scores[j:j+batch_size] = downsampled_iseg[\n                    (torch.arange(len(ibz)),) + tuple(ibl)]\n        scores = scores.view(repeats, location_count).sum(1)\n        total_scores[1:] += scores\n    return total_scores\n\n\ndef add_ace_ranking_to_dissection(outdir, layer, classname, total_scores):\n    source_filename = os.path.join(outdir, \'dissect.json\')\n    source_filename_bak = os.path.join(outdir, \'dissect.json.bak\')\n\n    # Back up the dissection (if not already backed up) before modifying\n    if not os.path.exists(source_filename_bak):\n        shutil.copy(source_filename, source_filename_bak)\n\n    with open(source_filename) as f:\n        dissection = EasyDict(json.load(f))\n\n    ranking_name = \'%s-ace\' % classname\n\n    # Remove any old ace ranking with the same name\n    lrec = [l for l in dissection.layers if l.layer == layer][0]\n    lrec.rankings = [r for r in lrec.rankings if r.name != ranking_name]\n\n    # Now convert ace scores to rankings\n    new_rankings = [dict(\n        name=ranking_name,\n        score=(-total_scores).flatten().tolist(),\n        metric=\'ace\')]\n\n    # Prepend to list.\n    lrec.rankings[2:2] = new_rankings\n\n    # Replace the old dissect.json in-place\n    with open(source_filename, \'w\') as f:\n        json.dump(dissection, f, indent=1)\n\ndef summarize_scores(args, corpus, cachedir, layer, classname, variant, scores):\n    target_filename = os.path.join(cachedir, \'summary.json\')\n\n    ranking_name = \'%s-%s\' % (classname, variant)\n    # Now convert ace scores to rankings\n    new_rankings = [dict(\n        name=ranking_name,\n        score=(-scores).flatten().tolist(),\n        metric=variant)]\n    result = dict(layers=[dict(layer=layer, rankings=new_rankings)])\n\n    # Replace the old dissect.json in-place\n    with open(target_filename, \'w\') as f:\n        json.dump(result, f, indent=1)\n\nif __name__ == \'__main__\':\n    main()\n'"
netdissect/aceplotablate.py,0,"b""import os, sys, argparse, json, shutil\nfrom matplotlib.backends.backend_agg import FigureCanvasAgg as FigureCanvas\nfrom matplotlib.figure import Figure\nfrom matplotlib.ticker import MaxNLocator\nimport matplotlib\n\ndef main():\n    parser = argparse.ArgumentParser(description='ACE optimization utility',\n            prog='python -m netdissect.aceoptimize')\n    parser.add_argument('--classname', type=str, default=None,\n                        help='intervention classname')\n    parser.add_argument('--layer', type=str, default='layer4',\n                        help='layer name')\n    parser.add_argument('--outdir', type=str, default=None,\n                        help='dissection directory')\n    parser.add_argument('--metric', type=str, default=None,\n                        help='experiment variant')\n    args = parser.parse_args()\n\n    if args.metric is None:\n        args.metric = 'ace'\n\n    run_command(args)\n\ndef run_command(args):\n    fig = Figure(figsize=(4.5,3.5))\n    FigureCanvas(fig)\n    ax = fig.add_subplot(111)\n    for metric in [args.metric, 'iou']:\n        jsonname = os.path.join(args.outdir, args.layer, 'fullablation',\n            '%s-%s.json' % (args.classname, metric))\n        with open(jsonname) as f:\n            summary = json.load(f)\n        baseline = summary['baseline']\n        effects = summary['ablation_effects'][:26]\n        norm_effects = [0] + [1.0 - e / baseline for e in effects]\n        ax.plot(norm_effects, label=\n                'Units by ACE' if 'ace' in metric else 'Top units by IoU')\n    ax.set_title('Effect of ablating units for %s' % (args.classname))\n    ax.grid(True)\n    ax.legend()\n    ax.set_ylabel('Portion of %s pixels removed' % args.classname)\n    ax.set_xlabel('Number of units ablated')\n    ax.set_ylim(0, 1.0)\n    ax.set_xlim(0, 25)\n    fig.tight_layout()\n    dirname = os.path.join(args.outdir, args.layer, 'fullablation')\n    fig.savefig(os.path.join(dirname, 'effect-%s-%s.png' %\n        (args.classname, args.metric)))\n    fig.savefig(os.path.join(dirname, 'effect-%s-%s.pdf' %\n        (args.classname, args.metric)))\n\nif __name__ == '__main__':\n    main()\n"""
netdissect/acesummarize.py,1,"b""import os, sys, numpy, torch, argparse, skimage, json, shutil\nfrom matplotlib.backends.backend_agg import FigureCanvasAgg as FigureCanvas\nfrom matplotlib.figure import Figure\nfrom matplotlib.ticker import MaxNLocator\nimport matplotlib\n\ndef main():\n    parser = argparse.ArgumentParser(description='ACE optimization utility',\n            prog='python -m netdissect.aceoptimize')\n    parser.add_argument('--classname', type=str, default=None,\n                        help='intervention classname')\n    parser.add_argument('--layer', type=str, default='layer4',\n                        help='layer name')\n    parser.add_argument('--l2_lambda', type=float, nargs='+',\n                        help='l2 regularizer hyperparameter')\n    parser.add_argument('--outdir', type=str, default=None,\n                        help='dissection directory')\n    parser.add_argument('--variant', type=str, default=None,\n                        help='experiment variant')\n    args = parser.parse_args()\n\n    if args.variant is None:\n        args.variant = 'ace'\n\n    run_command(args)\n\ndef run_command(args):\n    fig = Figure(figsize=(4.5,3.5))\n    FigureCanvas(fig)\n    ax = fig.add_subplot(111)\n    for l2_lambda in args.l2_lambda:\n        variant = args.variant\n        if l2_lambda != 0.01:\n            variant += '_reg%g' % l2_lambda\n\n        dirname = os.path.join(args.outdir, args.layer, variant, args.classname)\n        snapshots = os.path.join(dirname, 'snapshots')\n        try:\n            dat = [torch.load(os.path.join(snapshots, 'epoch-%d.pth' % i))\n                 for i in range(10)]\n        except:\n            print('Missing %s snapshots' % dirname)\n            return\n        print('reg %g' % l2_lambda)\n        for i in range(10):\n            print(i, dat[i]['avg_loss'],\n                  len((dat[i]['ablation'] == 1).nonzero()))\n\n        ax.plot([dat[i]['avg_loss'] for i in range(10)],\n            label='reg %g' % l2_lambda)\n    ax.set_title('%s %s' % (args.classname, args.variant))\n    ax.grid(True)\n    ax.legend()\n    ax.set_ylabel('Loss')\n    ax.set_xlabel('Epochs')\n    fig.tight_layout()\n    dirname = os.path.join(args.outdir, args.layer,\n            args.variant, args.classname)\n    fig.savefig(os.path.join(dirname, 'loss-plot.png'))\n\nif __name__ == '__main__':\n    main()\n"""
netdissect/actviz.py,0,"b'import os\nimport numpy\nfrom scipy.interpolate import RectBivariateSpline\n\ndef activation_visualization(image, data, level, alpha=0.5, source_shape=None,\n        crop=False, zoom=None, border=2, negate=False, return_mask=False,\n        **kwargs):\n    """"""\n    Makes a visualiztion image of activation data overlaid on the image.\n    Params:\n        image The original image.\n        data The single channel feature map.\n        alpha The darkening to apply in inactive regions of the image.\n        level The threshold of activation levels to highlight.\n    """"""\n    if len(image.shape) == 2:\n        # Puff up grayscale image to RGB.\n        image = image[:,:,None] * numpy.array([[[1, 1, 1]]])\n    surface = activation_surface(data, target_shape=image.shape[:2],\n            source_shape=source_shape, **kwargs)\n    if negate:\n        surface = -surface\n        level = -level\n    if crop:\n        # crop to source_shape\n        if source_shape is not None:\n            ch, cw = ((t - s) // 2 for s, t in zip(\n                source_shape, image.shape[:2]))\n            image = image[ch:ch+source_shape[0], cw:cw+source_shape[1]]\n            surface = surface[ch:ch+source_shape[0], cw:cw+source_shape[1]]\n        if crop is True:\n            crop = surface.shape\n        elif not hasattr(crop, \'__len__\'):\n            crop = (crop, crop)\n        if zoom is not None:\n            source_rect = best_sub_rect(surface >= level, crop, zoom,\n                    pad=border)\n        else:\n            source_rect = (0, surface.shape[0], 0, surface.shape[1])\n        image = zoom_image(image, source_rect, crop)\n        surface = zoom_image(surface, source_rect, crop)\n    mask = (surface >= level)\n    # Add a yellow border at the edge of the mask for contrast\n    result = (mask[:, :, None] * (1 - alpha) + alpha) * image\n    if border:\n        edge = mask_border(mask)[:,:,None]\n        result = numpy.maximum(edge * numpy.array([[[200, 200, 0]]]), result)\n    if not return_mask:\n        return result\n    mask_image = (1 - mask[:, :, None]) * numpy.array(\n            [[[0, 0, 0, 255 * (1 - alpha)]]], dtype=numpy.uint8)\n    if border:\n        mask_image = numpy.maximum(edge * numpy.array([[[200, 200, 0, 255]]]),\n                mask_image)\n    return result, mask_image\n\ndef activation_surface(data, target_shape=None, source_shape=None,\n        scale_offset=None, deg=1, pad=True):\n    """"""\n    Generates an upsampled activation sample.\n    Params:\n        target_shape Shape of the output array.\n        source_shape The centered shape of the output to match with data\n            when upscaling.  Defaults to the whole target_shape.\n        scale_offset The amount by which to scale, then offset data\n            dimensions to end up with target dimensions.  A pair of pairs.\n        deg Degree of interpolation to apply (1 = linear, etc).\n        pad True to zero-pad the edge instead of doing a funny edge interp.\n    """"""\n    # Default is that nothing is resized.\n    if target_shape is None:\n        target_shape = data.shape\n    # Make a default scale_offset to fill the image if there isn\'t one\n    if scale_offset is None:\n        scale = tuple(float(ts) / ds\n                for ts, ds in zip(target_shape, data.shape))\n        offset = tuple(0.5 * s - 0.5 for s in scale)\n    else:\n        scale, offset = (v for v in zip(*scale_offset))\n    # Now we adjust offsets to take into account cropping and so on\n    if source_shape is not None:\n        offset = tuple(o + (ts - ss) / 2.0\n                for o, ss, ts in zip(offset, source_shape, target_shape))\n    # Pad the edge with zeros for sensible edge behavior\n    if pad:\n        zeropad = numpy.zeros(\n                (data.shape[0] + 2, data.shape[1] + 2), dtype=data.dtype)\n        zeropad[1:-1, 1:-1] = data\n        data = zeropad\n        offset = tuple((o - s) for o, s in zip(offset, scale))\n    # Upsample linearly\n    ty, tx = (numpy.arange(ts) for ts in target_shape)\n    sy, sx = (numpy.arange(ss) * s + o\n            for ss, s, o in zip(data.shape, scale, offset))\n    levels = RectBivariateSpline(\n            sy, sx, data, kx=deg, ky=deg)(ty, tx, grid=True)\n    # Return the mask.\n    return levels\n\ndef mask_border(mask, border=2):\n    """"""Given a mask computes a border mask""""""\n    from scipy import ndimage\n    struct = ndimage.generate_binary_structure(2, 2)\n    erosion = numpy.ones((mask.shape[0] + 10, mask.shape[1] + 10), dtype=\'int\')\n    erosion[5:5+mask.shape[0], 5:5+mask.shape[1]] = ~mask\n    for _ in range(border):\n        erosion = ndimage.binary_erosion(erosion, struct)\n    return ~mask ^ erosion[5:5+mask.shape[0], 5:5+mask.shape[1]]\n\ndef bounding_rect(mask, pad=0):\n    """"""Returns (r, b, l, r) boundaries so that all nonzero pixels in mask\n    have locations (i, j) with  t <= i < b, and l <= j < r.""""""\n    nz = mask.nonzero()\n    if len(nz[0]) == 0:\n        # print(\'no pixels\')\n        return (0, mask.shape[0], 0, mask.shape[1])\n    (t, b), (l, r) = [(max(0, p.min() - pad), min(s, p.max() + 1 + pad))\n            for p, s in zip(nz, mask.shape)]\n    return (t, b, l, r)\n\ndef best_sub_rect(mask, shape, max_zoom=None, pad=2):\n    """"""Finds the smallest subrectangle containing all the nonzeros of mask,\n    matching the aspect ratio of shape, and where the zoom-up ratio is no\n    more than max_zoom""""""\n    t, b, l, r = bounding_rect(mask, pad=pad)\n    height = max(b - t, int(round(float(shape[0]) * (r - l) / shape[1])))\n    if max_zoom is not None:\n        height = int(max(round(float(shape[0]) / max_zoom), height))\n    width = int(round(float(shape[1]) * height / shape[0]))\n    nt = min(mask.shape[0] - height, max(0, (b + t - height) // 2))\n    nb = nt + height\n    nl = min(mask.shape[1] - width, max(0, (r + l - width) // 2))\n    nr = nl + width\n    return (nt, nb, nl, nr)\n\ndef zoom_image(img, source_rect, target_shape=None):\n    """"""Zooms pixels from the source_rect of img to target_shape.""""""\n    import warnings\n    from scipy.ndimage import zoom\n    if target_shape is None:\n        target_shape = img.shape\n    st, sb, sl, sr = source_rect\n    source = img[st:sb, sl:sr]\n    if source.shape == target_shape:\n        return source\n    zoom_tuple = tuple(float(t) / s\n            for t, s in zip(target_shape, source.shape[:2])\n            ) + (1,) * (img.ndim - 2)\n    with warnings.catch_warnings():\n        warnings.simplefilter(\'ignore\', UserWarning) # ""output shape of zoom""\n        target = zoom(source, zoom_tuple)\n    assert target.shape[:2] == target_shape, (target.shape, target_shape)\n    return target\n\ndef scale_offset(dilations):\n    if len(dilations) == 0:\n        return (1, 0)\n    scale, offset = scale_offset(dilations[1:])\n    kernel, stride, padding = dilations[0]\n    scale *= stride\n    offset *= stride\n    offset += (kernel - 1) / 2.0 - padding\n    return scale, offset\n\ndef choose_level(feature_map, percentile=0.8):\n    \'\'\'\n    Chooses the top 80% level (or whatever the level chosen).\n    \'\'\'\n    data_range = numpy.sort(feature_map.flatten())\n    return numpy.interp(\n            percentile, numpy.linspace(0, 1, len(data_range)), data_range)\n\ndef dilations(modulelist):\n    result = []\n    for module in modulelist:\n        settings = tuple(getattr(module, n, d)\n            for n, d in ((\'kernel_size\', 1), (\'stride\', 1), (\'padding\', 0)))\n        settings = (((s, s) if not isinstance(s, tuple) else s)\n            for s in settings)\n        if settings != ((1, 1), (1, 1), (0, 0)):\n            result.append(zip(*settings))\n    return zip(*result)\n\ndef grid_scale_offset(modulelist):\n    \'\'\'Returns (yscale, yoffset), (xscale, xoffset) given a list of modules\'\'\'\n    return tuple(scale_offset(d) for d in dilations(modulelist))\n\n'"
netdissect/autoeval.py,0,"b""from collections import defaultdict\nfrom importlib import import_module\n\ndef autoimport_eval(term):\n    '''\n    Used to evaluate an arbitrary command-line constructor specifying\n    a class, with automatic import of global module names.\n    '''\n\n    class DictNamespace(object):\n        def __init__(self, d):\n            self.__d__ = d\n        def __getattr__(self, key):\n            return self.__d__[key]\n\n    class AutoImportDict(defaultdict):\n        def __init__(self, wrapped=None, parent=None):\n            super().__init__()\n            self.wrapped = wrapped\n            self.parent = parent\n        def __missing__(self, key):\n            if self.wrapped is not None:\n                if key in self.wrapped:\n                    return self.wrapped[key]\n            if self.parent is not None:\n                key = self.parent + '.' + key\n            if key in __builtins__:\n                return __builtins__[key]\n            mdl = import_module(key)\n            # Return an AutoImportDict for any namespace packages\n            if hasattr(mdl, '__path__'): # and not hasattr(mdl, '__file__'):\n                return DictNamespace(\n                        AutoImportDict(wrapped=mdl.__dict__, parent=key))\n            return mdl\n\n    return eval(term, {}, AutoImportDict())\n\n"""
netdissect/broden.py,4,"b""import os, errno, numpy, torch, csv, re, shutil, os, zipfile\nfrom collections import OrderedDict\nfrom torchvision.datasets.folder import default_loader\nfrom torchvision import transforms\nfrom scipy import ndimage\nfrom urllib.request import urlopen\n\nclass BrodenDataset(torch.utils.data.Dataset):\n    '''\n    A multicategory segmentation data set.\n\n    Returns three streams:\n    (1) The image (3, h, w).\n    (2) The multicategory segmentation (labelcount, h, w).\n    (3) A bincount of pixels in the segmentation (labelcount).\n\n    Net dissect also assumes that the dataset object has three properties\n    with human-readable labels:\n\n    ds.labels = ['red', 'black', 'car', 'tree', 'grid', ...]\n    ds.categories = ['color', 'part', 'object', 'texture']\n    ds.label_category = [0, 0, 2, 2, 3, ...] # The category for each label\n    '''\n    def __init__(self, directory='dataset/broden', resolution=384,\n            split='train', categories=None,\n            transform=None, transform_segment=None,\n            download=False, size=None, include_bincount=True,\n            broden_version=1, max_segment_depth=6):\n        assert resolution in [224, 227, 384]\n        if download:\n            ensure_broden_downloaded(directory, resolution, broden_version)\n        self.directory = directory\n        self.resolution = resolution\n        self.resdir = os.path.join(directory, 'broden%d_%d' %\n                (broden_version, resolution))\n        self.loader = default_loader\n        self.transform = transform\n        self.transform_segment = transform_segment\n        self.include_bincount = include_bincount\n        # The maximum number of multilabel layers that coexist at an image.\n        self.max_segment_depth = max_segment_depth\n        with open(os.path.join(self.resdir, 'category.csv'),\n                encoding='utf-8') as f:\n            self.category_info = OrderedDict()\n            for row in csv.DictReader(f):\n                self.category_info[row['name']] = row\n        if categories is not None:\n            # Filter out unused categories\n            categories = set([c for c in categories if c in self.category_info])\n            for cat in list(self.category_info.keys()):\n                if cat not in categories:\n                    del self.category_info[cat]\n        categories = list(self.category_info.keys())\n        self.categories = categories\n\n        # Filter out unneeded images.\n        with open(os.path.join(self.resdir, 'index.csv'),\n                encoding='utf-8') as f:\n            all_images = [decode_index_dict(r) for r in csv.DictReader(f)]\n        self.image = [row for row in all_images\n            if index_has_any_data(row, categories) and row['split'] == split]\n        if size is not None:\n            self.image = self.image[:size]\n        with open(os.path.join(self.resdir, 'label.csv'),\n                encoding='utf-8') as f:\n            self.label_info = build_dense_label_array([\n                decode_label_dict(r) for r in csv.DictReader(f)])\n            self.labels = [l['name'] for l in self.label_info]\n        # Build dense remapping arrays for labels, so that you can\n        # get dense ranges of labels for each category.\n        self.category_map = {}\n        self.category_unmap = {}\n        self.category_label = {}\n        for cat in self.categories:\n            with open(os.path.join(self.resdir, 'c_%s.csv' % cat),\n                    encoding='utf-8') as f:\n                c_data = [decode_label_dict(r) for r in csv.DictReader(f)]\n            self.category_unmap[cat], self.category_map[cat] = (\n                    build_numpy_category_map(c_data))\n            self.category_label[cat] = build_dense_label_array(\n                    c_data, key='code')\n        self.num_labels = len(self.labels)\n        # Primary categories for each label is the category in which it\n        # appears with the maximum coverage.\n        self.label_category = numpy.zeros(self.num_labels, dtype=int)\n        for i in range(self.num_labels):\n            maxcoverage, self.label_category[i] = max(\n               (self.category_label[cat][self.category_map[cat][i]]['coverage']\n                    if i < len(self.category_map[cat])\n                       and self.category_map[cat][i] else 0, ic)\n                for ic, cat in enumerate(categories))\n\n    def __len__(self):\n        return len(self.image)\n\n    def __getitem__(self, idx):\n        record = self.image[idx]\n        # example record: {\n        #    'image': 'opensurfaces/25605.jpg', 'split': 'train',\n        #    'ih': 384, 'iw': 384, 'sh': 192, 'sw': 192,\n        #    'color': ['opensurfaces/25605_color.png'],\n        #    'object': [], 'part': [],\n        #    'material': ['opensurfaces/25605_material.png'],\n        #    'scene': [], 'texture': []}\n        image = self.loader(os.path.join(self.resdir, 'images',\n            record['image']))\n        segment = numpy.zeros(shape=(self.max_segment_depth,\n            record['sh'], record['sw']), dtype=int)\n        if self.include_bincount:\n            bincount = numpy.zeros(shape=(self.num_labels,), dtype=int)\n        depth = 0\n        for cat in self.categories:\n            for layer in record[cat]:\n                if isinstance(layer, int):\n                    segment[depth,:,:] = layer\n                    if self.include_bincount:\n                        bincount[layer] += segment.shape[1] * segment.shape[2]\n                else:\n                    png = numpy.asarray(self.loader(os.path.join(\n                        self.resdir, 'images', layer)))\n                    segment[depth,:,:] = png[:,:,0] + png[:,:,1] * 256\n                    if self.include_bincount:\n                        bincount += numpy.bincount(segment[depth,:,:].flatten(),\n                            minlength=self.num_labels)\n                depth += 1\n        if self.transform:\n            image = self.transform(image)\n        if self.transform_segment:\n            segment = self.transform_segment(segment)\n        if self.include_bincount:    \n            bincount[0] = 0\n            return (image, segment, bincount)\n        else:\n            return (image, segment)\n\ndef build_dense_label_array(label_data, key='number', allow_none=False):\n    '''\n    Input: set of rows with 'number' fields (or another field name key).\n    Output: array such that a[number] = the row with the given number.\n    '''\n    result = [None] * (max([d[key] for d in label_data]) + 1)\n    for d in label_data:\n        result[d[key]] = d\n    # Fill in none\n    if not allow_none:\n        example = label_data[0]\n        def make_empty(k):\n            return dict((c, k if c is key else type(v)())\n                    for c, v in example.items())\n        for i, d in enumerate(result):\n            if d is None:\n                result[i] = dict(make_empty(i))\n    return result\n\ndef build_numpy_category_map(map_data, key1='code', key2='number'):\n    '''\n    Input: set of rows with 'number' fields (or another field name key).\n    Output: array such that a[number] = the row with the given number.\n    '''\n    results = list(numpy.zeros((max([d[key] for d in map_data]) + 1),\n            dtype=numpy.int16) for key in (key1, key2))\n    for d in map_data:\n        results[0][d[key1]] = d[key2]\n        results[1][d[key2]] = d[key1]\n    return results\n\ndef index_has_any_data(row, categories):\n    for c in categories:\n        for data in row[c]:\n            if data: return True\n    return False\n\ndef decode_label_dict(row):\n    result = {}\n    for key, val in row.items():\n        if key == 'category':\n            result[key] = dict((c, int(n))\n                for c, n in [re.match('^([^(]*)\\(([^)]*)\\)$', f).groups()\n                    for f in val.split(';')])\n        elif key == 'name':\n            result[key] = val\n        elif key == 'syns':\n            result[key] = val.split(';')\n        elif re.match('^\\d+$', val):\n            result[key] = int(val)\n        elif re.match('^\\d+\\.\\d*$', val):\n            result[key] = float(val)\n        else:\n            result[key] = val\n    return result\n\ndef decode_index_dict(row):\n    result = {}\n    for key, val in row.items():\n        if key in ['image', 'split']:\n            result[key] = val\n        elif key in ['sw', 'sh', 'iw', 'ih']:\n            result[key] = int(val)\n        else:\n            item = [s for s in val.split(';') if s]\n            for i, v in enumerate(item):\n                if re.match('^\\d+$', v):\n                    item[i] = int(v)\n            result[key] = item\n    return result\n\nclass ScaleSegmentation:\n    '''\n    Utility for scaling segmentations, using nearest-neighbor zooming.\n    '''\n    def __init__(self, target_height, target_width):\n        self.target_height = target_height\n        self.target_width = target_width\n    def __call__(self, seg):\n        ratio = (1, self.target_height / float(seg.shape[1]),\n                self.target_width / float(seg.shape[2]))\n        return ndimage.zoom(seg, ratio, order=0)\n\ndef scatter_batch(seg, num_labels, omit_zero=True, dtype=torch.uint8):\n    '''\n    Utility for scattering semgentations into a one-hot representation.\n    '''\n    result = torch.zeros(*((seg.shape[0], num_labels,) + seg.shape[2:]),\n            dtype=dtype, device=seg.device)\n    result.scatter_(1, seg, 1)\n    if omit_zero:\n        result[:,0] = 0\n    return result\n\ndef ensure_broden_downloaded(directory, resolution, broden_version=1):\n    assert resolution in [224, 227, 384]\n    baseurl = 'http://netdissect.csail.mit.edu/data/'\n    dirname = 'broden%d_%d' % (broden_version, resolution)\n    if os.path.isfile(os.path.join(directory, dirname, 'index.csv')):\n        return # Already downloaded\n    zipfilename = 'broden1_%d.zip' % resolution\n    download_dir = os.path.join(directory, 'download')\n    os.makedirs(download_dir, exist_ok=True)\n    full_zipfilename = os.path.join(download_dir, zipfilename)\n    if not os.path.exists(full_zipfilename):\n        url = '%s/%s' % (baseurl, zipfilename)\n        print('Downloading %s' % url)\n        data = urlopen(url)\n        with open(full_zipfilename, 'wb') as f:\n            f.write(data.read())\n    print('Unzipping %s' % zipfilename)\n    with zipfile.ZipFile(full_zipfilename, 'r') as zip_ref:\n        zip_ref.extractall(directory)\n    assert os.path.isfile(os.path.join(directory, dirname, 'index.csv'))\n\ndef test_broden_dataset():\n    '''\n    Testing code.\n    '''\n    bds = BrodenDataset('dataset/broden', resolution=384,\n            transform=transforms.Compose([\n                        transforms.Resize(224),\n                        transforms.ToTensor()]),\n            transform_segment=transforms.Compose([\n                        ScaleSegmentation(224, 224)\n                        ]),\n            include_bincount=True)\n    loader = torch.utils.data.DataLoader(bds, batch_size=100, num_workers=24)\n    for i in range(1,20):\n        print(bds.label[i]['name'],\n                list(bds.category.keys())[bds.primary_category[i]])\n    for i, (im, seg, bc) in enumerate(loader):\n        print(i, im.shape, seg.shape, seg.max(), bc.shape)\n\nif __name__ == '__main__':\n    test_broden_dataset()\n"""
netdissect/dissection.py,74,"b'\'\'\'\nTo run dissection:\n\n1. Load up the convolutional model you wish to dissect, and wrap it in\n   an InstrumentedModel; then call imodel.retain_layers([layernames,..])\n   to instrument the layers of interest.\n2. Load the segmentation dataset using the BrodenDataset class;\n   use the transform_image argument to normalize images to be\n   suitable for the model, or the size argument to truncate the dataset.\n3. Choose a directory in which to write the output, and call\n   dissect(outdir, model, dataset).\n\nExample:\n\n    from dissect import InstrumentedModel, dissect\n    from broden import BrodenDataset\n\n    model = InstrumentedModel(load_my_model())\n    model.eval()\n    model.cuda()\n    model.retain_layers([\'conv1\', \'conv2\', \'conv3\', \'conv4\', \'conv5\'])\n    bds = BrodenDataset(\'dataset/broden1_227\',\n            transform_image=transforms.Compose([\n                transforms.ToTensor(),\n                transforms.Normalize(IMAGE_MEAN, IMAGE_STDEV)]),\n            size=1000)\n    dissect(\'result/dissect\', model, bds,\n            examples_per_unit=10)\n\'\'\'\n\nimport torch, numpy, os, re, json, shutil, types, tempfile, torchvision\n# import warnings\n# warnings.simplefilter(\'error\', UserWarning)\nfrom PIL import Image\nfrom xml.etree import ElementTree as et\nfrom collections import OrderedDict, defaultdict\nfrom .progress import verbose_progress, default_progress, print_progress\nfrom .progress import desc_progress\nfrom .runningstats import RunningQuantile, RunningTopK\nfrom .runningstats import RunningCrossCovariance, RunningConditionalQuantile\nfrom .sampler import FixedSubsetSampler\nfrom .actviz import activation_visualization\nfrom .segviz import segment_visualization, high_contrast\nfrom .workerpool import WorkerBase, WorkerPool\nfrom .segmenter import UnifiedParsingSegmenter\n\ndef dissect(outdir, model, dataset,\n        segrunner=None,\n        train_dataset=None,\n        model_segmenter=None,\n        quantile_threshold=0.005,\n        iou_threshold=0.05,\n        iqr_threshold=0.01,\n        examples_per_unit=100,\n        batch_size=100,\n        num_workers=24,\n        seg_batch_size=5,\n        make_images=True,\n        make_labels=True,\n        make_maxiou=False,\n        make_covariance=False,\n        make_report=True,\n        make_row_images=True,\n        make_single_images=False,\n        rank_all_labels=False,\n        netname=None,\n        meta=None,\n        merge=None,\n        settings=None,\n        ):\n    \'\'\'\n    Runs net dissection in-memory, using pytorch, and saves visualizations\n    and metadata into outdir.\n    \'\'\'\n    assert not model.training, \'Run model.eval() before dissection\'\n    if netname is None:\n        netname = type(model).__name__\n    if segrunner is None:\n        segrunner = ClassifierSegRunner(dataset)\n    if train_dataset is None:\n        train_dataset = dataset\n    make_iqr = (quantile_threshold == \'iqr\')\n    with torch.no_grad():\n        device = next(model.parameters()).device\n        levels = None\n        labelnames, catnames = None, None\n        maxioudata, iqrdata = None, None\n        labeldata = None\n        iqrdata, cov = None, None\n\n        labelnames, catnames = segrunner.get_label_and_category_names()\n        label_category = [catnames.index(c) if c in catnames else 0\n                for l, c in labelnames]\n\n        # First, always collect qunatiles and topk information.\n        segloader = torch.utils.data.DataLoader(dataset,\n                batch_size=batch_size, num_workers=num_workers,\n                pin_memory=(device.type == \'cuda\'))\n        quantiles, topk = collect_quantiles_and_topk(outdir, model,\n            segloader, segrunner, k=examples_per_unit)\n\n        # Thresholds can be automatically chosen by maximizing iqr\n        if make_iqr:\n            # Get thresholds based on an IQR optimization\n            segloader = torch.utils.data.DataLoader(train_dataset,\n                    batch_size=1, num_workers=num_workers,\n                    pin_memory=(device.type == \'cuda\'))\n            iqrdata = collect_iqr(outdir, model, segloader, segrunner)\n            max_iqr, full_iqr_levels = iqrdata[:2]\n            max_iqr_agreement = iqrdata[4]\n            # qualified_iqr[max_iqr_quantile[layer] > 0.5] = 0\n            levels = {layer: full_iqr_levels[layer][\n                    max_iqr[layer].max(0)[1],\n                    torch.arange(max_iqr[layer].shape[1])].to(device)\n                    for layer in full_iqr_levels}\n        else:\n            levels = {k: qc.quantiles([1.0 - quantile_threshold])[:,0]\n                      for k, qc in quantiles.items()}\n\n        quantiledata = (topk, quantiles, levels, quantile_threshold)\n\n        if make_images:\n            segloader = torch.utils.data.DataLoader(dataset,\n                    batch_size=batch_size, num_workers=num_workers,\n                    pin_memory=(device.type == \'cuda\'))\n            generate_images(outdir, model, dataset, topk, levels, segrunner,\n                    row_length=examples_per_unit, batch_size=seg_batch_size,\n                    row_images=make_row_images,\n                    single_images=make_single_images,\n                    num_workers=num_workers)\n\n        if make_maxiou:\n            assert train_dataset, ""Need training dataset for maxiou.""\n            segloader = torch.utils.data.DataLoader(train_dataset,\n                    batch_size=1, num_workers=num_workers,\n                    pin_memory=(device.type == \'cuda\'))\n            maxioudata = collect_maxiou(outdir, model, segloader,\n                    segrunner)\n\n        if make_labels:\n            segloader = torch.utils.data.DataLoader(dataset,\n                    batch_size=1, num_workers=num_workers,\n                    pin_memory=(device.type == \'cuda\'))\n            iou_scores, iqr_scores, tcs, lcs, ccs, ics = (\n                    collect_bincounts(outdir, model, segloader,\n                    levels, segrunner))\n            labeldata = (iou_scores, iqr_scores, lcs, ccs, ics, iou_threshold,\n                    iqr_threshold)\n\n        if make_covariance:\n            segloader = torch.utils.data.DataLoader(dataset,\n                    batch_size=seg_batch_size,\n                    num_workers=num_workers,\n                    pin_memory=(device.type == \'cuda\'))\n            cov = collect_covariance(outdir, model, segloader, segrunner)\n\n        if make_report:\n            generate_report(outdir,\n                    quantiledata=quantiledata,\n                    labelnames=labelnames,\n                    catnames=catnames,\n                    labeldata=labeldata,\n                    maxioudata=maxioudata,\n                    iqrdata=iqrdata,\n                    covariancedata=cov,\n                    rank_all_labels=rank_all_labels,\n                    netname=netname,\n                    meta=meta,\n                    mergedata=merge,\n                    settings=settings)\n\n        return quantiledata, labeldata\n\ndef generate_report(outdir, quantiledata, labelnames=None, catnames=None,\n        labeldata=None, maxioudata=None, iqrdata=None, covariancedata=None,\n        rank_all_labels=False, netname=\'Model\', meta=None, settings=None,\n        mergedata=None):\n    \'\'\'\n    Creates dissection.json reports and summary bargraph.svg files in the\n    specified output directory, and copies a dissection.html interface\n    to go along with it.\n    \'\'\'\n    all_layers = []\n    # Current source code directory, for html to copy.\n    srcdir = os.path.realpath(\n       os.path.join(os.getcwd(), os.path.dirname(__file__)))\n    # Unpack arguments\n    topk, quantiles, levels, quantile_threshold = quantiledata\n    top_record = dict(\n            netname=netname,\n            meta=meta,\n            default_ranking=\'unit\',\n            quantile_threshold=quantile_threshold)\n    if settings is not None:\n        top_record[\'settings\'] = settings\n    if labeldata is not None:\n        iou_scores, iqr_scores, lcs, ccs, ics, iou_threshold, iqr_threshold = (\n                labeldata)\n        catorder = {\'object\': -7, \'scene\': -6, \'part\': -5,\n                    \'piece\': -4,\n                    \'material\': -3, \'texture\': -2, \'color\': -1}\n        for i, cat in enumerate(c for c in catnames if c not in catorder):\n            catorder[cat] = i\n        catnumber = {n: i for i, n in enumerate(catnames)}\n        catnumber[\'-\'] = 0\n        top_record[\'default_ranking\'] = \'label\'\n        top_record[\'iou_threshold\'] = iou_threshold\n        top_record[\'iqr_threshold\'] = iqr_threshold\n        labelnumber = dict((name[0], num)\n                for num, name in enumerate(labelnames))\n    # Make a segmentation color dictionary\n    segcolors = {}\n    for i, name in enumerate(labelnames):\n        key = \',\'.join(str(s) for s in high_contrast[i % len(high_contrast)])\n        if key in segcolors:\n            segcolors[key] += \'/\' + name[0]\n        else:\n            segcolors[key] = name[0]\n    top_record[\'segcolors\'] = segcolors\n    for layer in topk.keys():\n        units, rankings = [], []\n        record = dict(layer=layer, units=units, rankings=rankings)\n        # For every unit, we always have basic visualization information.\n        topa, topi = topk[layer].result()\n        lev = levels[layer]\n        for u in range(len(topa)):\n            units.append(dict(\n                unit=u,\n                interp=True,\n                level=lev[u].item(),\n                top=[dict(imgnum=i.item(), maxact=a.item())\n                    for i, a in zip(topi[u], topa[u])],\n                ))\n        rankings.append(dict(name=""unit"", score=list([\n            u for u in range(len(topa))])))\n        # TODO: consider including stats and ranking based on quantiles,\n        # variance, connectedness here.\n\n        # if we have labeldata, then every unit also gets a bunch of other info\n        if labeldata is not None:\n            lscore, qscore, cc, ic = [dat[layer]\n                    for dat in [iou_scores, iqr_scores, ccs, ics]]\n            if iqrdata is not None:\n                # If we have IQR thresholds, assign labels based on that\n                max_iqr, max_iqr_level = iqrdata[:2]\n                best_label = max_iqr[layer].max(0)[1]\n                best_score = lscore[best_label, torch.arange(lscore.shape[1])]\n                best_qscore = qscore[best_label, torch.arange(lscore.shape[1])]\n            else:\n                # Otherwise, assign labels based on max iou\n                best_score, best_label = lscore.max(0)\n                best_qscore = qscore[best_label, torch.arange(qscore.shape[1])]\n            record[\'iou_threshold\'] = iou_threshold,\n            for u, urec in enumerate(units):\n                score, qscore, label = (\n                        best_score[u], best_qscore[u], best_label[u])\n                urec.update(dict(\n                    iou=score.item(),\n                    iou_iqr=qscore.item(),\n                    lc=lcs[label].item(),\n                    cc=cc[catnumber[labelnames[label][1]], u].item(),\n                    ic=ic[label, u].item(),\n                    interp=(qscore.item() > iqr_threshold and\n                        score.item() > iou_threshold),\n                    iou_labelnum=label.item(),\n                    iou_label=labelnames[label.item()][0],\n                    iou_cat=labelnames[label.item()][1],\n                    ))\n        if maxioudata is not None:\n            max_iou, max_iou_level, max_iou_quantile = maxioudata\n            qualified_iou = max_iou[layer].clone()\n            # qualified_iou[max_iou_quantile[layer] > 0.75] = 0\n            best_score, best_label = qualified_iou.max(0)\n            for u, urec in enumerate(units):\n                urec.update(dict(\n                    maxiou=best_score[u].item(),\n                    maxiou_label=labelnames[best_label[u].item()][0],\n                    maxiou_cat=labelnames[best_label[u].item()][1],\n                    maxiou_level=max_iou_level[layer][best_label[u], u].item(),\n                    maxiou_quantile=max_iou_quantile[layer][\n                        best_label[u], u].item()))\n        if iqrdata is not None:\n            [max_iqr, max_iqr_level, max_iqr_quantile,\n                    max_iqr_iou, max_iqr_agreement] = iqrdata\n            qualified_iqr = max_iqr[layer].clone()\n            qualified_iqr[max_iqr_quantile[layer] > 0.5] = 0\n            best_score, best_label = qualified_iqr.max(0)\n            for u, urec in enumerate(units):\n                urec.update(dict(\n                    iqr=best_score[u].item(),\n                    iqr_label=labelnames[best_label[u].item()][0],\n                    iqr_cat=labelnames[best_label[u].item()][1],\n                    iqr_level=max_iqr_level[layer][best_label[u], u].item(),\n                    iqr_quantile=max_iqr_quantile[layer][\n                        best_label[u], u].item(),\n                    iqr_iou=max_iqr_iou[layer][best_label[u], u].item()\n                    ))\n        if covariancedata is not None:\n            score = covariancedata[layer].correlation()\n            best_score, best_label = score.max(1)\n            for u, urec in enumerate(units):\n                urec.update(dict(\n                    cor=best_score[u].item(),\n                    cor_label=labelnames[best_label[u].item()][0],\n                    cor_cat=labelnames[best_label[u].item()][1]\n                    ))\n        if mergedata is not None:\n            # Final step: if the user passed any data to merge into the\n            # units, merge them now.  This can be used, for example, to\n            # indiate that a unit is not interpretable based on some\n            # outside analysis of unit statistics.\n            for lrec in mergedata.get(\'layers\', []):\n                if lrec[\'layer\'] == layer:\n                    break\n            else:\n                lrec = None\n            for u, urec in enumerate(lrec.get(\'units\', []) if lrec else []):\n                units[u].update(urec)\n        # After populating per-unit info, populate per-layer ranking info\n        if labeldata is not None:\n            # Collect all labeled units\n            labelunits = defaultdict(list)\n            all_labelunits = defaultdict(list)\n            for u, urec in enumerate(units):\n                if urec[\'interp\']:\n                    labelunits[urec[\'iou_labelnum\']].append(u)\n                all_labelunits[urec[\'iou_labelnum\']].append(u)\n            # Sort all units in order with most popular label first.\n            label_ordering = sorted(units,\n                # Sort by:\n                key=lambda r: (-1 if r[\'interp\'] else 0,  # interpretable\n                    -len(labelunits[r[\'iou_labelnum\']]),  # label freq, score\n                    -max([units[u][\'iou\']\n                        for u in labelunits[r[\'iou_labelnum\']]], default=0),\n                    r[\'iou_labelnum\'],                    # label\n                    -r[\'iou\']))                           # unit score\n            # Add label and iou ranking.\n            rankings.append(dict(name=""label"", score=(numpy.argsort(list(\n                ur[\'unit\'] for ur in label_ordering))).tolist()))\n            rankings.append(dict(name=""max iou"", metric=""iou"", score=list(\n                -ur[\'iou\'] for ur in units)))\n            # Add ranking for top labels\n            # for labelnum in [n for n in sorted(\n            #     all_labelunits.keys(), key=lambda x:\n            #         -len(all_labelunits[x])) if len(all_labelunits[n])]:\n            #     label = labelnames[labelnum][0]\n            #     rankings.append(dict(name=""%s-iou"" % label,\n            #         concept=label, metric=\'iou\',\n            #         score=(-lscore[labelnum, :]).tolist()))\n            # Collate labels by category then frequency.\n            record[\'labels\'] = [dict(\n                        label=labelnames[label][0],\n                        labelnum=label,\n                        units=labelunits[label],\n                        cat=labelnames[label][1])\n                    for label in (sorted(labelunits.keys(),\n                        # Sort by:\n                        key=lambda l: (catorder.get(          # category\n                            labelnames[l][1], 0),\n                            -len(labelunits[l]),              # label freq\n                            -max([units[u][\'iou\'] for u in labelunits[l]],\n                                default=0) # score\n                            ))) if len(labelunits[label])]\n            # Total number of interpretable units.\n            record[\'interpretable\'] = sum(len(group[\'units\'])\n                    for group in record[\'labels\'])\n            # Make a bargraph of labels\n            os.makedirs(os.path.join(outdir, safe_dir_name(layer)),\n                    exist_ok=True)\n            catgroups = OrderedDict()\n            for _, cat in sorted([(v, k) for k, v in catorder.items()]):\n                catgroups[cat] = []\n            for rec in record[\'labels\']:\n                if rec[\'cat\'] not in catgroups:\n                    catgroups[rec[\'cat\']] = []\n                catgroups[rec[\'cat\']].append(rec[\'label\'])\n            make_svg_bargraph(\n                    [rec[\'label\'] for rec in record[\'labels\']],\n                    [len(rec[\'units\']) for rec in record[\'labels\']],\n                    [(cat, len(group)) for cat, group in catgroups.items()],\n                    filename=os.path.join(outdir, safe_dir_name(layer),\n                        \'bargraph.svg\'))\n            # Only show the bargraph if it is non-empty.\n            if len(record[\'labels\']):\n                record[\'bargraph\'] = \'bargraph.svg\'\n        if maxioudata is not None:\n            rankings.append(dict(name=""max maxiou"", metric=""maxiou"", score=list(\n                    -ur[\'maxiou\'] for ur in units)))\n        if iqrdata is not None:\n            rankings.append(dict(name=""max iqr"", metric=""iqr"", score=list(\n                    -ur[\'iqr\'] for ur in units)))\n        if covariancedata is not None:\n            rankings.append(dict(name=""max cor"", metric=""cor"", score=list(\n                    -ur[\'cor\'] for ur in units)))\n\n        all_layers.append(record)\n    # Now add the same rankings to every layer...\n    all_labels = None\n    if rank_all_labels:\n        all_labels = [name for name, cat in labelnames]\n    if labeldata is not None:\n        # Count layers+quadrants with a given label, and sort by freq\n        counted_labels = defaultdict(int)\n        for label in [\n                re.sub(r\'-(?:t|b|l|r|tl|tr|bl|br)$\', \'\', unitrec[\'iou_label\'])\n                for record in all_layers for unitrec in record[\'units\']]:\n            counted_labels[label] += 1\n        if all_labels is None:\n            all_labels = [label for count, label in sorted((-v, k)\n                for k, v in counted_labels.items())]\n        for record in all_layers:\n            layer = record[\'layer\']\n            for label in all_labels:\n                labelnum = labelnumber[label]\n                record[\'rankings\'].append(dict(name=""%s-iou"" % label,\n                    concept=label, metric=\'iou\',\n                    score=(-iou_scores[layer][labelnum, :]).tolist()))\n\n    if maxioudata is not None:\n        if all_labels is None:\n            counted_labels = defaultdict(int)\n            for label in [\n                    re.sub(r\'-(?:t|b|l|r|tl|tr|bl|br)$\', \'\',\n                        unitrec[\'maxiou_label\'])\n                    for record in all_layers for unitrec in record[\'units\']]:\n                counted_labels[label] += 1\n            all_labels = [label for count, label in sorted((-v, k)\n                for k, v in counted_labels.items())]\n        qualified_iou = max_iou[layer].clone()\n        qualified_iou[max_iou_quantile[layer] > 0.5] = 0\n        for record in all_layers:\n            layer = record[\'layer\']\n            for label in all_labels:\n                labelnum = labelnumber[label]\n                record[\'rankings\'].append(dict(name=""%s-maxiou"" % label,\n                    concept=label, metric=\'maxiou\',\n                    score=(-qualified_iou[labelnum, :]).tolist()))\n\n    if iqrdata is not None:\n        if all_labels is None:\n            counted_labels = defaultdict(int)\n            for label in [\n                    re.sub(r\'-(?:t|b|l|r|tl|tr|bl|br)$\', \'\',\n                        unitrec[\'iqr_label\'])\n                    for record in all_layers for unitrec in record[\'units\']]:\n                counted_labels[label] += 1\n            all_labels = [label for count, label in sorted((-v, k)\n                for k, v in counted_labels.items())]\n        # qualified_iqr[max_iqr_quantile[layer] > 0.5] = 0\n        for record in all_layers:\n            layer = record[\'layer\']\n            qualified_iqr = max_iqr[layer].clone()\n            for label in all_labels:\n                labelnum = labelnumber[label]\n                record[\'rankings\'].append(dict(name=""%s-iqr"" % label,\n                    concept=label, metric=\'iqr\',\n                    score=(-qualified_iqr[labelnum, :]).tolist()))\n\n    if covariancedata is not None:\n        if all_labels is None:\n            counted_labels = defaultdict(int)\n            for label in [\n                    re.sub(r\'-(?:t|b|l|r|tl|tr|bl|br)$\', \'\',\n                        unitrec[\'cor_label\'])\n                    for record in all_layers for unitrec in record[\'units\']]:\n                counted_labels[label] += 1\n            all_labels = [label for count, label in sorted((-v, k)\n                for k, v in counted_labels.items())]\n        for record in all_layers:\n            layer = record[\'layer\']\n            score = covariancedata[layer].correlation()\n            for label in all_labels:\n                labelnum = labelnumber[label]\n                record[\'rankings\'].append(dict(name=""%s-cor"" % label,\n                    concept=label, metric=\'cor\',\n                    score=(-score[:, labelnum]).tolist()))\n\n    for record in all_layers:\n        layer = record[\'layer\']\n        # Dump per-layer json inside per-layer directory\n        record[\'dirname\'] = \'.\'\n        with open(os.path.join(outdir, safe_dir_name(layer), \'dissect.json\'),\n                \'w\') as jsonfile:\n            top_record[\'layers\'] = [record]\n            json.dump(top_record, jsonfile, indent=1)\n        # Copy the per-layer html\n        shutil.copy(os.path.join(srcdir, \'dissect.html\'),\n                os.path.join(outdir, safe_dir_name(layer), \'dissect.html\'))\n        record[\'dirname\'] = safe_dir_name(layer)\n\n    # Dump all-layer json in parent directory\n    with open(os.path.join(outdir, \'dissect.json\'), \'w\') as jsonfile:\n        top_record[\'layers\'] = all_layers\n        json.dump(top_record, jsonfile, indent=1)\n    # Copy the all-layer html\n    shutil.copy(os.path.join(srcdir, \'dissect.html\'),\n            os.path.join(outdir, \'dissect.html\'))\n    shutil.copy(os.path.join(srcdir, \'edit.html\'),\n            os.path.join(outdir, \'edit.html\'))\n\n\ndef generate_images(outdir, model, dataset, topk, levels,\n        segrunner, row_length=None, gap_pixels=5,\n        row_images=True, single_images=False, prefix=\'\',\n        batch_size=100, num_workers=24):\n    \'\'\'\n    Creates an image strip file for every unit of every retained layer\n    of the model, in the format [outdir]/[layername]/[unitnum]-top.jpg.\n    Assumes that the indexes of topk refer to the indexes of dataset.\n    Limits each strip to the top row_length images.\n    \'\'\'\n    progress = default_progress()\n    needed_images = {}\n    if row_images is False:\n        row_length = 1\n    # Pass 1: needed_images lists all images that are topk for some unit.\n    for layer in topk:\n        topresult = topk[layer].result()[1].cpu()\n        for unit, row in enumerate(topresult):\n            for rank, imgnum in enumerate(row[:row_length]):\n                imgnum = imgnum.item()\n                if imgnum not in needed_images:\n                    needed_images[imgnum] = []\n                needed_images[imgnum].append((layer, unit, rank))\n    levels = {k: v.cpu().numpy() for k, v in levels.items()}\n    row_length = len(row[:row_length])\n    needed_sample = FixedSubsetSampler(sorted(needed_images.keys()))\n    device = next(model.parameters()).device\n    segloader = torch.utils.data.DataLoader(dataset,\n            batch_size=batch_size, num_workers=num_workers,\n            pin_memory=(device.type == \'cuda\'),\n            sampler=needed_sample)\n    vizgrid, maskgrid, origrid, seggrid = [{} for _ in range(4)]\n    # Pass 2: populate vizgrid with visualizations of top units.\n    pool = None\n    for i, batch in enumerate(\n            progress(segloader, desc=\'Making images\')):\n        # Reverse transformation to get the image in byte form.\n        seg, _, byte_im, _ = segrunner.run_and_segment_batch(batch, model,\n                want_rgb=True)\n        torch_features = model.retained_features()\n        scale_offset = getattr(model, \'scale_offset\', None)\n        if pool is None:\n            # Distribute the work across processes: create shared mmaps.\n            for layer, tf in torch_features.items():\n                [vizgrid[layer], maskgrid[layer], origrid[layer],\n                        seggrid[layer]] = [\n                    create_temp_mmap_grid((tf.shape[1],\n                        byte_im.shape[1], row_length,\n                        byte_im.shape[2] + gap_pixels, depth),\n                        dtype=\'uint8\',\n                        fill=255)\n                    for depth in [3, 4, 3, 3]]\n            # Pass those mmaps to worker processes.\n            pool = WorkerPool(worker=VisualizeImageWorker,\n                    memmap_grid_info=[\n                        {layer: (g.filename, g.shape, g.dtype)\n                            for layer, g in grid.items()}\n                        for grid in [vizgrid, maskgrid, origrid, seggrid]])\n        byte_im = byte_im.cpu().numpy()\n        numpy_seg = seg.cpu().numpy()\n        features = {}\n        for index in range(len(byte_im)):\n            imgnum = needed_sample.samples[index + i*segloader.batch_size]\n            for layer, unit, rank in needed_images[imgnum]:\n                if layer not in features:\n                    features[layer] = torch_features[layer].cpu().numpy()\n                pool.add(layer, unit, rank,\n                        byte_im[index],\n                        features[layer][index, unit],\n                        levels[layer][unit],\n                        scale_offset[layer] if scale_offset else None,\n                        numpy_seg[index])\n    pool.join()\n    # Pass 3: save image strips as [outdir]/[layer]/[unitnum]-[top/orig].jpg\n    pool = WorkerPool(worker=SaveImageWorker)\n    for layer, vg in progress(vizgrid.items(), desc=\'Saving images\'):\n        os.makedirs(os.path.join(outdir, safe_dir_name(layer),\n            prefix + \'image\'), exist_ok=True)\n        if single_images:\n           os.makedirs(os.path.join(outdir, safe_dir_name(layer),\n               prefix + \'s-image\'), exist_ok=True)\n        og, sg, mg = origrid[layer], seggrid[layer], maskgrid[layer]\n        for unit in progress(range(len(vg)), desc=\'Units\'):\n            for suffix, grid in [(\'top.jpg\', vg), (\'orig.jpg\', og),\n                    (\'seg.png\', sg), (\'mask.png\', mg)]:\n                strip = grid[unit].reshape(\n                        (grid.shape[1], grid.shape[2] * grid.shape[3],\n                            grid.shape[4]))\n                if row_images:\n                    filename = os.path.join(outdir, safe_dir_name(layer),\n                            prefix + \'image\', \'%d-%s\' % (unit, suffix))\n                    pool.add(strip[:,:-gap_pixels,:].copy(), filename)\n                    # Image.fromarray(strip[:,:-gap_pixels,:]).save(filename,\n                    #        optimize=True, quality=80)\n                if single_images:\n                    single_filename = os.path.join(outdir, safe_dir_name(layer),\n                        prefix + \'s-image\', \'%d-%s\' % (unit, suffix))\n                    pool.add(strip[:,:strip.shape[1] // row_length\n                        - gap_pixels,:].copy(), single_filename)\n                    # Image.fromarray(strip[:,:strip.shape[1] // row_length\n                    #     - gap_pixels,:]).save(single_filename,\n                    #             optimize=True, quality=80)\n    pool.join()\n    # Delete the shared memory map files\n    clear_global_shared_files([g.filename\n        for grid in [vizgrid, maskgrid, origrid, seggrid]\n        for g in grid.values()])\n\nglobal_shared_files = {}\ndef create_temp_mmap_grid(shape, dtype, fill):\n    dtype = numpy.dtype(dtype)\n    filename = os.path.join(tempfile.mkdtemp(), \'temp-%s-%s.mmap\' %\n            (\'x\'.join(\'%d\' % s for s in shape), dtype.name))\n    fid = open(filename, mode=\'w+b\')\n    original = numpy.memmap(fid, dtype=dtype, mode=\'w+\', shape=shape)\n    original.fid = fid\n    original[...] = fill\n    global_shared_files[filename] = original\n    return original\n\ndef shared_temp_mmap_grid(filename, shape, dtype):\n    if filename not in global_shared_files:\n        global_shared_files[filename] = numpy.memmap(\n                filename, dtype=dtype, mode=\'r+\', shape=shape)\n    return global_shared_files[filename]\n\ndef clear_global_shared_files(filenames):\n    for fn in filenames:\n        if fn in global_shared_files:\n            del global_shared_files[fn]\n        try:\n            os.unlink(fn)\n        except OSError:\n            pass\n\nclass VisualizeImageWorker(WorkerBase):\n    def setup(self, memmap_grid_info):\n        self.vizgrid, self.maskgrid, self.origrid, self.seggrid = [\n                {layer: shared_temp_mmap_grid(*info)\n                    for layer, info in grid.items()}\n                for grid in memmap_grid_info]\n    def work(self, layer, unit, rank,\n            byte_im, acts, level, scale_offset, seg):\n        self.origrid[layer][unit,:,rank,:byte_im.shape[0],:] = byte_im\n        [self.vizgrid[layer][unit,:,rank,:byte_im.shape[0],:],\n         self.maskgrid[layer][unit,:,rank,:byte_im.shape[0],:]] = (\n                    activation_visualization(\n                        byte_im,\n                        acts,\n                        level,\n                        scale_offset=scale_offset,\n                        return_mask=True))\n        self.seggrid[layer][unit,:,rank,:byte_im.shape[0],:] = (\n                    segment_visualization(seg, byte_im.shape[0:2]))\n\nclass SaveImageWorker(WorkerBase):\n    def work(self, data, filename):\n        Image.fromarray(data).save(filename, optimize=True, quality=80)\n\ndef score_tally_stats(label_category, tc, truth, cc, ic):\n    pred = cc[label_category]\n    total = tc[label_category][:, None]\n    truth = truth[:, None]\n    epsilon = 1e-20 # avoid division-by-zero\n    union = pred + truth - ic\n    iou = ic.double() / (union.double() + epsilon)\n    arr = torch.empty(size=(2, 2) + ic.shape, dtype=ic.dtype, device=ic.device)\n    arr[0, 0] = ic\n    arr[0, 1] = pred - ic\n    arr[1, 0] = truth - ic\n    arr[1, 1] = total - union\n    arr = arr.double() / total.double()\n    mi = mutual_information(arr)\n    je = joint_entropy(arr)\n    iqr = mi / je\n    iqr[torch.isnan(iqr)] = 0 # Zero out any 0/0\n    return iou, iqr\n\ndef collect_quantiles_and_topk(outdir, model, segloader,\n        segrunner, k=100, resolution=1024):\n    \'\'\'\n    Collects (estimated) quantile information and (exact) sorted top-K lists\n    for every channel in the retained layers of the model.  Returns\n    a map of quantiles (one RunningQuantile for each layer) along with\n    a map of topk (one RunningTopK for each layer).\n    \'\'\'\n    device = next(model.parameters()).device\n    features = model.retained_features()\n    cached_quantiles = {\n            layer: load_quantile_if_present(os.path.join(outdir,\n                safe_dir_name(layer)), \'quantiles.npz\',\n                device=torch.device(\'cpu\'))\n            for layer in features }\n    cached_topks = {\n            layer: load_topk_if_present(os.path.join(outdir,\n                safe_dir_name(layer)), \'topk.npz\',\n                device=torch.device(\'cpu\'))\n            for layer in features }\n    if (all(value is not None for value in cached_quantiles.values()) and\n        all(value is not None for value in cached_topks.values())):\n        return cached_quantiles, cached_topks\n\n    layer_batch_size = 8\n    all_layers = list(features.keys())\n    layer_batches = [all_layers[i:i+layer_batch_size]\n            for i in range(0, len(all_layers), layer_batch_size)]\n\n    quantiles, topks = {}, {}\n    progress = default_progress()\n    for layer_batch in layer_batches:\n        for i, batch in enumerate(progress(segloader, desc=\'Quantiles\')):\n            # We don\'t actually care about the model output.\n            model(batch[0].to(device))\n            features = model.retained_features()\n            # We care about the retained values\n            for key in layer_batch:\n                value = features[key]\n                if topks.get(key, None) is None:\n                    topks[key] = RunningTopK(k)\n                if quantiles.get(key, None) is None:\n                    quantiles[key] = RunningQuantile(resolution=resolution)\n                topvalue = value\n                if len(value.shape) > 2:\n                    topvalue, _ = value.view(*(value.shape[:2] + (-1,))).max(2)\n                    # Put the channel index last.\n                    value = value.permute(\n                            (0,) + tuple(range(2, len(value.shape))) + (1,)\n                            ).contiguous().view(-1, value.shape[1])\n                quantiles[key].add(value)\n                topks[key].add(topvalue)\n        # Save GPU memory\n        for key in layer_batch:\n            quantiles[key].to_(torch.device(\'cpu\'))\n            topks[key].to_(torch.device(\'cpu\'))\n    for layer in quantiles:\n        save_state_dict(quantiles[layer],\n                os.path.join(outdir, safe_dir_name(layer), \'quantiles.npz\'))\n        save_state_dict(topks[layer],\n                os.path.join(outdir, safe_dir_name(layer), \'topk.npz\'))\n    return quantiles, topks\n\ndef collect_bincounts(outdir, model, segloader, levels, segrunner):\n    \'\'\'\n    Returns label_counts, category_activation_counts, and intersection_counts,\n    across the data set, counting the pixels of intersection between upsampled,\n    thresholded model featuremaps, with segmentation classes in the segloader.\n\n    label_counts (independent of model): pixels across the data set that\n        are labeled with the given label.\n    category_activation_counts (one per layer): for each feature channel,\n        pixels across the dataset where the channel exceeds the level\n        threshold.  There is one count per category: activations only\n        contribute to the categories for which any category labels are\n        present on the images.\n    intersection_counts (one per layer): for each feature channel and\n        label, pixels across the dataset where the channel exceeds\n        the level, and the labeled segmentation class is also present.\n\n    This is a performance-sensitive function.  Best performance is\n    achieved with a counting scheme which assumes a segloader with\n    batch_size 1.\n    \'\'\'\n    # Load cached data if present\n    (iou_scores, iqr_scores,\n            total_counts, label_counts, category_activation_counts,\n            intersection_counts) = {}, {}, None, None, {}, {}\n    found_all = True\n    for layer in model.retained_features():\n        filename = os.path.join(outdir, safe_dir_name(layer), \'bincounts.npz\')\n        if os.path.isfile(filename):\n            data = numpy.load(filename)\n            iou_scores[layer] = torch.from_numpy(data[\'iou_scores\'])\n            iqr_scores[layer] = torch.from_numpy(data[\'iqr_scores\'])\n            total_counts = torch.from_numpy(data[\'total_counts\'])\n            label_counts = torch.from_numpy(data[\'label_counts\'])\n            category_activation_counts[layer] = torch.from_numpy(\n                    data[\'category_activation_counts\'])\n            intersection_counts[layer] = torch.from_numpy(\n                    data[\'intersection_counts\'])\n        else:\n            found_all = False\n    if found_all:\n        return (iou_scores, iqr_scores,\n            total_counts, label_counts, category_activation_counts,\n            intersection_counts)\n\n    device = next(model.parameters()).device\n    labelcat, categories = segrunner.get_label_and_category_names()\n    label_category = [categories.index(c) if c in categories else 0\n                for l, c in labelcat]\n    num_labels, num_categories = (len(n) for n in [labelcat, categories])\n\n    # One-hot vector of category for each label\n    labelcat = torch.zeros(num_labels, num_categories,\n            dtype=torch.long, device=device)\n    labelcat.scatter_(1, torch.from_numpy(numpy.array(label_category,\n        dtype=\'int64\')).to(device)[:,None], 1)\n    # Running bincounts\n    # activation_counts = {}\n    assert segloader.batch_size == 1 # category_activation_counts needs this.\n    category_activation_counts = {}\n    intersection_counts = {}\n    label_counts = torch.zeros(num_labels, dtype=torch.long, device=device)\n    total_counts = torch.zeros(num_categories, dtype=torch.long, device=device)\n    progress = default_progress()\n    scale_offset_map = getattr(model, \'scale_offset\', None)\n    upsample_grids = {}\n    # total_batch_categories = torch.zeros(\n    #         labelcat.shape[1], dtype=torch.long, device=device)\n    for i, batch in enumerate(progress(segloader, desc=\'Bincounts\')):\n        seg, batch_label_counts, _, imshape = segrunner.run_and_segment_batch(\n                batch, model, want_bincount=True, want_rgb=True)\n        bc = batch_label_counts.cpu()\n        batch_label_counts = batch_label_counts.to(device)\n        seg = seg.to(device)\n        features = model.retained_features()\n        # Accumulate bincounts and identify nonzeros\n        label_counts += batch_label_counts[0]\n        batch_labels = bc[0].nonzero()[:,0]\n        batch_categories = labelcat[batch_labels].max(0)[0]\n        total_counts += batch_categories * (\n                seg.shape[0] * seg.shape[2] * seg.shape[3])\n        for key, value in features.items():\n            if key not in upsample_grids:\n                upsample_grids[key] = upsample_grid(value.shape[2:],\n                        seg.shape[2:], imshape,\n                        scale_offset=scale_offset_map.get(key, None)\n                            if scale_offset_map is not None else None,\n                        dtype=value.dtype, device=value.device)\n            upsampled = torch.nn.functional.grid_sample(value,\n                    upsample_grids[key], padding_mode=\'border\')\n            amask = (upsampled > levels[key][None,:,None,None].to(\n                upsampled.device))\n            ac = amask.int().view(amask.shape[1], -1).sum(1)\n            # if key not in activation_counts:\n            #     activation_counts[key] = ac\n            # else:\n            #     activation_counts[key] += ac\n            # The fastest approach: sum over each label separately!\n            for label in batch_labels.tolist():\n                if label == 0:\n                    continue  # ignore the background label\n                imask = amask * ((seg == label).max(dim=1, keepdim=True)[0])\n                ic = imask.int().view(imask.shape[1], -1).sum(1)\n                if key not in intersection_counts:\n                    intersection_counts[key] = torch.zeros(num_labels,\n                            amask.shape[1], dtype=torch.long, device=device)\n                intersection_counts[key][label] += ic\n            # Count activations within images that have category labels.\n            # Note: This only makes sense with batch-size one\n            # total_batch_categories += batch_categories\n            cc = batch_categories[:,None] * ac[None,:]\n            if key not in category_activation_counts:\n                category_activation_counts[key] = cc\n            else:\n                category_activation_counts[key] += cc\n    iou_scores = {}\n    iqr_scores = {}\n    for k in intersection_counts:\n        iou_scores[k], iqr_scores[k] = score_tally_stats(\n            label_category, total_counts, label_counts,\n            category_activation_counts[k], intersection_counts[k])\n    for k in intersection_counts:\n        numpy.savez(os.path.join(outdir, safe_dir_name(k), \'bincounts.npz\'),\n                iou_scores=iou_scores[k].cpu().numpy(),\n                iqr_scores=iqr_scores[k].cpu().numpy(),\n                total_counts=total_counts.cpu().numpy(),\n                label_counts=label_counts.cpu().numpy(),\n                category_activation_counts=category_activation_counts[k]\n                    .cpu().numpy(),\n                intersection_counts=intersection_counts[k].cpu().numpy(),\n                levels=levels[k].cpu().numpy())\n    return (iou_scores, iqr_scores,\n            total_counts, label_counts, category_activation_counts,\n            intersection_counts)\n\ndef collect_cond_quantiles(outdir, model, segloader, segrunner):\n    \'\'\'\n    Returns maxiou and maxiou_level across the data set, one per layer.\n\n    This is a performance-sensitive function.  Best performance is\n    achieved with a counting scheme which assumes a segloader with\n    batch_size 1.\n    \'\'\'\n    device = next(model.parameters()).device\n    cached_cond_quantiles = {\n            layer: load_conditional_quantile_if_present(os.path.join(outdir,\n                safe_dir_name(layer)), \'cond_quantiles.npz\') # on cpu\n            for layer in model.retained_features() }\n    label_fracs = load_npy_if_present(outdir, \'label_fracs.npy\', \'cpu\')\n    if label_fracs is not None and all(\n            value is not None for value in cached_cond_quantiles.values()):\n        return cached_cond_quantiles, label_fracs\n\n    labelcat, categories = segrunner.get_label_and_category_names()\n    label_category = [categories.index(c) if c in categories else 0\n                for l, c in labelcat]\n    num_labels, num_categories = (len(n) for n in [labelcat, categories])\n\n    # One-hot vector of category for each label\n    labelcat = torch.zeros(num_labels, num_categories,\n            dtype=torch.long, device=device)\n    labelcat.scatter_(1, torch.from_numpy(numpy.array(label_category,\n        dtype=\'int64\')).to(device)[:,None], 1)\n    # Running maxiou\n    assert segloader.batch_size == 1 # category_activation_counts needs this.\n    conditional_quantiles = {}\n    label_counts = torch.zeros(num_labels, dtype=torch.long, device=device)\n    pixel_count = 0\n    progress = default_progress()\n    scale_offset_map = getattr(model, \'scale_offset\', None)\n    upsample_grids = {}\n    common_conditions = set()\n    if label_fracs is None or label_fracs is 0:\n        for i, batch in enumerate(progress(segloader, desc=\'label fracs\')):\n            seg, batch_label_counts, im, _ = segrunner.run_and_segment_batch(\n                    batch, model, want_bincount=True, want_rgb=True)\n            batch_label_counts = batch_label_counts.to(device)\n            features = model.retained_features()\n            # Accumulate bincounts and identify nonzeros\n            label_counts += batch_label_counts[0]\n            pixel_count += seg.shape[2] * seg.shape[3]\n        label_fracs = (label_counts.cpu().float() / pixel_count)[:, None, None]\n        numpy.save(os.path.join(outdir, \'label_fracs.npy\'), label_fracs)\n\n    skip_threshold = 1e-4\n    skip_labels = set(i.item()\n        for i in (label_fracs.view(-1) < skip_threshold).nonzero().view(-1))\n\n    for layer in progress(model.retained_features().keys(), desc=\'CQ layers\'):\n        if cached_cond_quantiles.get(layer, None) is not None:\n            conditional_quantiles[layer] = cached_cond_quantiles[layer]\n            continue\n\n        for i, batch in enumerate(progress(segloader, desc=\'Condquant\')):\n            seg, batch_label_counts, _, imshape = (\n                    segrunner.run_and_segment_batch(\n                         batch, model, want_bincount=True, want_rgb=True))\n            bc = batch_label_counts.cpu()\n            batch_label_counts = batch_label_counts.to(device)\n            features = model.retained_features()\n            # Accumulate bincounts and identify nonzeros\n            label_counts += batch_label_counts[0]\n            pixel_count += seg.shape[2] * seg.shape[3]\n            batch_labels = bc[0].nonzero()[:,0]\n            batch_categories = labelcat[batch_labels].max(0)[0]\n            cpu_seg = None\n            value = features[layer]\n            if layer not in upsample_grids:\n                upsample_grids[layer] = upsample_grid(value.shape[2:],\n                        seg.shape[2:], imshape,\n                        scale_offset=scale_offset_map.get(layer, None)\n                            if scale_offset_map is not None else None,\n                        dtype=value.dtype, device=value.device)\n            if layer not in conditional_quantiles:\n                conditional_quantiles[layer] = RunningConditionalQuantile(\n                        resolution=2048)\n            upsampled = torch.nn.functional.grid_sample(value,\n                    upsample_grids[layer], padding_mode=\'border\').view(\n                            value.shape[1], -1)\n            conditional_quantiles[layer].add((\'all\',), upsampled.t())\n            cpu_upsampled = None\n            for label in batch_labels.tolist():\n                if label in skip_labels:\n                    continue\n                label_key = (\'label\', label)\n                if label_key in common_conditions:\n                    imask = (seg == label).max(dim=1)[0].view(-1)\n                    intersected = upsampled[:, imask]\n                    conditional_quantiles[layer].add((\'label\', label),\n                            intersected.t())\n                else:\n                    if cpu_seg is None:\n                        cpu_seg = seg.cpu()\n                    if cpu_upsampled is None:\n                        cpu_upsampled = upsampled.cpu()\n                    imask = (cpu_seg == label).max(dim=1)[0].view(-1)\n                    intersected = cpu_upsampled[:, imask]\n                    conditional_quantiles[layer].add((\'label\', label),\n                            intersected.t())\n            if num_categories > 1:\n                for cat in batch_categories.nonzero()[:,0]:\n                    conditional_quantiles[layer].add((\'cat\', cat.item()),\n                            upsampled.t())\n            # Move the most common conditions to the GPU.\n            if i and not i & (i - 1):  # if i is a power of 2:\n                cq = conditional_quantiles[layer]\n                common_conditions = set(cq.most_common_conditions(64))\n                cq.to_(\'cpu\', [k for k in cq.running_quantiles.keys()\n                        if k not in common_conditions])\n        # When a layer is done, get it off the GPU\n        conditional_quantiles[layer].to_(\'cpu\')\n\n    label_fracs = (label_counts.cpu().float() / pixel_count)[:, None, None]\n\n    for cq in conditional_quantiles.values():\n        cq.to_(\'cpu\')\n\n    for layer in conditional_quantiles:\n        save_state_dict(conditional_quantiles[layer],\n            os.path.join(outdir, safe_dir_name(layer), \'cond_quantiles.npz\'))\n    numpy.save(os.path.join(outdir, \'label_fracs.npy\'), label_fracs)\n\n    return conditional_quantiles, label_fracs\n\n\ndef collect_maxiou(outdir, model, segloader, segrunner):\n    \'\'\'\n    Returns maxiou and maxiou_level across the data set, one per layer.\n\n    This is a performance-sensitive function.  Best performance is\n    achieved with a counting scheme which assumes a segloader with\n    batch_size 1.\n    \'\'\'\n    device = next(model.parameters()).device\n    conditional_quantiles, label_fracs = collect_cond_quantiles(\n            outdir, model, segloader, segrunner)\n\n    labelcat, categories = segrunner.get_label_and_category_names()\n    label_category = [categories.index(c) if c in categories else 0\n                for l, c in labelcat]\n    num_labels, num_categories = (len(n) for n in [labelcat, categories])\n\n    label_list = [(\'label\', i) for i in range(num_labels)]\n    category_list = [(\'all\',)] if num_categories <= 1 else (\n            [(\'cat\', i) for i in range(num_categories)])\n    max_iou, max_iou_level, max_iou_quantile = {}, {}, {}\n    fracs = torch.logspace(-3, 0, 100)\n    progress = default_progress()\n    for layer, cq in progress(conditional_quantiles.items(), desc=\'Maxiou\'):\n        levels = cq.conditional((\'all\',)).quantiles(1 - fracs)\n        denoms = 1 - cq.collected_normalize(category_list, levels)\n        isects = (1 - cq.collected_normalize(label_list, levels)) * label_fracs\n        unions = label_fracs + denoms[label_category, :, :] - isects\n        iou = isects / unions\n        # TODO: erase any for which threshold is bad\n        max_iou[layer], level_bucket = iou.max(2)\n        max_iou_level[layer] = levels[\n                torch.arange(levels.shape[0])[None,:], level_bucket]\n        max_iou_quantile[layer] = fracs[level_bucket]\n    for layer in model.retained_features():\n        numpy.savez(os.path.join(outdir, safe_dir_name(layer), \'max_iou.npz\'),\n            max_iou=max_iou[layer].cpu().numpy(),\n            max_iou_level=max_iou_level[layer].cpu().numpy(),\n            max_iou_quantile=max_iou_quantile[layer].cpu().numpy())\n    return (max_iou, max_iou_level, max_iou_quantile)\n\ndef collect_iqr(outdir, model, segloader, segrunner):\n    \'\'\'\n    Returns iqr and iqr_level.\n\n    This is a performance-sensitive function.  Best performance is\n    achieved with a counting scheme which assumes a segloader with\n    batch_size 1.\n    \'\'\'\n    max_iqr, max_iqr_level, max_iqr_quantile, max_iqr_iou  = {}, {}, {}, {}\n    max_iqr_agreement = {}\n    found_all = True\n    for layer in model.retained_features():\n        filename = os.path.join(outdir, safe_dir_name(layer), \'iqr.npz\')\n        if os.path.isfile(filename):\n            data = numpy.load(filename)\n            max_iqr[layer] = torch.from_numpy(data[\'max_iqr\'])\n            max_iqr_level[layer] = torch.from_numpy(data[\'max_iqr_level\'])\n            max_iqr_quantile[layer] = torch.from_numpy(data[\'max_iqr_quantile\'])\n            max_iqr_iou[layer] = torch.from_numpy(data[\'max_iqr_iou\'])\n            max_iqr_agreement[layer] = torch.from_numpy(\n                    data[\'max_iqr_agreement\'])\n        else:\n            found_all = False\n    if found_all:\n        return (max_iqr, max_iqr_level, max_iqr_quantile, max_iqr_iou,\n            max_iqr_agreement)\n\n\n    device = next(model.parameters()).device\n    conditional_quantiles, label_fracs = collect_cond_quantiles(\n            outdir, model, segloader, segrunner)\n\n    labelcat, categories = segrunner.get_label_and_category_names()\n    label_category = [categories.index(c) if c in categories else 0\n                for l, c in labelcat]\n    num_labels, num_categories = (len(n) for n in [labelcat, categories])\n\n    label_list = [(\'label\', i) for i in range(num_labels)]\n    category_list = [(\'all\',)] if num_categories <= 1 else (\n            [(\'cat\', i) for i in range(num_categories)])\n    full_mi, full_je, full_iqr = {}, {}, {}\n    fracs = torch.logspace(-3, 0, 100)\n    progress = default_progress()\n    for layer, cq in progress(conditional_quantiles.items(), desc=\'IQR\'):\n        levels = cq.conditional((\'all\',)).quantiles(1 - fracs)\n        truth = label_fracs.to(device)\n        preds = (1 - cq.collected_normalize(category_list, levels)\n                )[label_category, :, :].to(device)\n        cond_isects = 1 - cq.collected_normalize(label_list, levels).to(device)\n        isects = cond_isects * truth\n        unions = truth + preds - isects\n        arr = torch.empty(size=(2, 2) + isects.shape, dtype=isects.dtype,\n                device=device)\n        arr[0, 0] = isects\n        arr[0, 1] = preds - isects\n        arr[1, 0] = truth - isects\n        arr[1, 1] = 1 - unions\n        arr.clamp_(0, 1)\n        mi = mutual_information(arr)\n        mi[:,:,-1] = 0  # at the 1.0 quantile should be no MI.\n        # Don\'t trust mi when less than label_frac is less than 1e-3,\n        # because our samples are too small.\n        mi[label_fracs.view(-1) < 1e-3, :, :] = 0\n        je = joint_entropy(arr)\n        iqr = mi / je\n        iqr[torch.isnan(iqr)] = 0 # Zero out any 0/0\n        full_mi[layer] = mi.cpu()\n        full_je[layer] = je.cpu()\n        full_iqr[layer] = iqr.cpu()\n        del mi, je\n        agreement = isects + arr[1, 1]\n        # When optimizing, maximize only over those pairs where the\n        # unit is positively correlated with the label, and where the\n        # threshold level is positive\n        positive_iqr = iqr\n        positive_iqr[agreement <= 0.8] = 0\n        positive_iqr[(levels <= 0.0)[None, :, :].expand(positive_iqr.shape)] = 0\n        # TODO: erase any for which threshold is bad\n        maxiqr, level_bucket = positive_iqr.max(2)\n        max_iqr[layer] = maxiqr.cpu()\n        max_iqr_level[layer] = levels.to(device)[\n                torch.arange(levels.shape[0])[None,:], level_bucket].cpu()\n        max_iqr_quantile[layer] = fracs.to(device)[level_bucket].cpu()\n        max_iqr_agreement[layer] = agreement[\n                torch.arange(agreement.shape[0])[:, None],\n                torch.arange(agreement.shape[1])[None, :],\n                level_bucket].cpu()\n\n        # Compute the iou that goes with each maximized iqr\n        matching_iou = (isects[\n                torch.arange(isects.shape[0])[:, None],\n                torch.arange(isects.shape[1])[None, :],\n                level_bucket] /\n            unions[\n                torch.arange(unions.shape[0])[:, None],\n                torch.arange(unions.shape[1])[None, :],\n                level_bucket])\n        matching_iou[torch.isnan(matching_iou)] = 0\n        max_iqr_iou[layer] = matching_iou.cpu()\n    for layer in model.retained_features():\n        numpy.savez(os.path.join(outdir, safe_dir_name(layer), \'iqr.npz\'),\n            max_iqr=max_iqr[layer].cpu().numpy(),\n            max_iqr_level=max_iqr_level[layer].cpu().numpy(),\n            max_iqr_quantile=max_iqr_quantile[layer].cpu().numpy(),\n            max_iqr_iou=max_iqr_iou[layer].cpu().numpy(),\n            max_iqr_agreement=max_iqr_agreement[layer].cpu().numpy(),\n            full_mi=full_mi[layer].cpu().numpy(),\n            full_je=full_je[layer].cpu().numpy(),\n            full_iqr=full_iqr[layer].cpu().numpy())\n    return (max_iqr, max_iqr_level, max_iqr_quantile, max_iqr_iou,\n            max_iqr_agreement)\n\ndef mutual_information(arr):\n    total = 0\n    for j in range(arr.shape[0]):\n        for k in range(arr.shape[1]):\n            joint = arr[j,k]\n            ind = arr[j,:].sum(dim=0) * arr[:,k].sum(dim=0)\n            term = joint * (joint / ind).log()\n            term[torch.isnan(term)] = 0\n            total += term\n    return total.clamp_(0)\n\ndef joint_entropy(arr):\n    total = 0\n    for j in range(arr.shape[0]):\n        for k in range(arr.shape[1]):\n            joint = arr[j,k]\n            term = joint * joint.log()\n            term[torch.isnan(term)] = 0\n            total += term\n    return (-total).clamp_(0)\n\ndef information_quality_ratio(arr):\n    iqr = mutual_information(arr) / joint_entropy(arr)\n    iqr[torch.isnan(iqr)] = 0\n    return iqr\n\ndef collect_covariance(outdir, model, segloader, segrunner):\n    \'\'\'\n    Returns label_mean, label_variance, unit_mean, unit_variance,\n    and cross_covariance across the data set.\n\n    label_mean, label_variance (independent of model):\n        treating the label as a one-hot, each label\'s mean and variance.\n    unit_mean, unit_variance (one per layer): for each feature channel,\n        the mean and variance of the activations in that channel.\n    cross_covariance (one per layer): the cross covariance between the\n        labels and the units in the layer.\n    \'\'\'\n    device = next(model.parameters()).device\n    cached_covariance = {\n            layer: load_covariance_if_present(os.path.join(outdir,\n                safe_dir_name(layer)), \'covariance.npz\', device=device)\n            for layer in model.retained_features() }\n    if all(value is not None for value in cached_covariance.values()):\n        return cached_covariance\n    labelcat, categories = segrunner.get_label_and_category_names()\n    label_category = [categories.index(c) if c in categories else 0\n                for l, c in labelcat]\n    num_labels, num_categories = (len(n) for n in [labelcat, categories])\n\n    # Running covariance\n    cov = {}\n    progress = default_progress()\n    scale_offset_map = getattr(model, \'scale_offset\', None)\n    upsample_grids = {}\n    for i, batch in enumerate(progress(segloader, desc=\'Covariance\')):\n        seg, _, _, imshape = segrunner.run_and_segment_batch(batch, model,\n                want_rgb=True)\n        features = model.retained_features()\n        ohfeats = multilabel_onehot(seg, num_labels, ignore_index=0)\n        # Accumulate bincounts and identify nonzeros\n        for key, value in features.items():\n            if key not in upsample_grids:\n                upsample_grids[key] = upsample_grid(value.shape[2:],\n                        seg.shape[2:], imshape,\n                        scale_offset=scale_offset_map.get(key, None)\n                            if scale_offset_map is not None else None,\n                        dtype=value.dtype, device=value.device)\n            upsampled = torch.nn.functional.grid_sample(value,\n                    upsample_grids[key].expand(\n                        (value.shape[0],) + upsample_grids[key].shape[1:]),\n                    padding_mode=\'border\')\n            if key not in cov:\n                cov[key] = RunningCrossCovariance()\n            cov[key].add(upsampled, ohfeats)\n    for layer in cov:\n        save_state_dict(cov[layer],\n                os.path.join(outdir, safe_dir_name(layer), \'covariance.npz\'))\n    return cov\n\ndef multilabel_onehot(labels, num_labels, dtype=None, ignore_index=None):\n    \'\'\'\n    Converts a multilabel tensor into a onehot tensor.\n\n    The input labels is a tensor of shape (samples, multilabels, y, x).\n    The output is a tensor of shape (samples, num_labels, y, x).\n    If ignore_index is specified, labels with that index are ignored.\n    Each x in labels should be 0 <= x < num_labels, or x == ignore_index.\n    \'\'\'\n    assert ignore_index is None or ignore_index <= 0\n    if dtype is None:\n        dtype = torch.float\n    device = labels.device\n    chans = num_labels + (-ignore_index if ignore_index else 0)\n    outshape = (labels.shape[0], chans) + labels.shape[2:]\n    result = torch.zeros(outshape, device=device, dtype=dtype)\n    if ignore_index and ignore_index < 0:\n        labels = labels + (-ignore_index)\n    result.scatter_(1, labels, 1)\n    if ignore_index and ignore_index < 0:\n        result = result[:, -ignore_index:]\n    elif ignore_index is not None:\n        result[:, ignore_index] = 0\n    return result\n\ndef load_npy_if_present(outdir, filename, device):\n    filepath = os.path.join(outdir, filename)\n    if os.path.isfile(filepath):\n        data = numpy.load(filepath)\n        return torch.from_numpy(data).to(device)\n    return 0\n\ndef load_npz_if_present(outdir, filename, varnames, device):\n    filepath = os.path.join(outdir, filename)\n    if os.path.isfile(filepath):\n        data = numpy.load(filepath)\n        numpy_result = [data[n] for n in varnames]\n        return tuple(torch.from_numpy(data).to(device) for data in numpy_result)\n    return None\n\ndef load_quantile_if_present(outdir, filename, device):\n    filepath = os.path.join(outdir, filename)\n    if os.path.isfile(filepath):\n        data = numpy.load(filepath)\n        result = RunningQuantile(state=data)\n        result.to_(device)\n        return result\n    return None\n\ndef load_conditional_quantile_if_present(outdir, filename):\n    filepath = os.path.join(outdir, filename)\n    if os.path.isfile(filepath):\n        data = numpy.load(filepath)\n        result = RunningConditionalQuantile(state=data)\n        return result\n    return None\n\ndef load_topk_if_present(outdir, filename, device):\n    filepath = os.path.join(outdir, filename)\n    if os.path.isfile(filepath):\n        data = numpy.load(filepath)\n        result = RunningTopK(state=data)\n        result.to_(device)\n        return result\n    return None\n\ndef load_covariance_if_present(outdir, filename, device):\n    filepath = os.path.join(outdir, filename)\n    if os.path.isfile(filepath):\n        data = numpy.load(filepath)\n        result = RunningCrossCovariance(state=data)\n        result.to_(device)\n        return result\n    return None\n\ndef save_state_dict(obj, filepath):\n    dirname = os.path.dirname(filepath)\n    os.makedirs(dirname, exist_ok=True)\n    dic = obj.state_dict()\n    numpy.savez(filepath, **dic)\n\ndef upsample_grid(data_shape, target_shape, input_shape=None,\n        scale_offset=None, dtype=torch.float, device=None):\n    \'\'\'Prepares a grid to use with grid_sample to upsample a batch of\n    features in data_shape to the target_shape. Can use scale_offset\n    and input_shape to center the grid in a nondefault way: scale_offset\n    maps feature pixels to input_shape pixels, and it is assumed that\n    the target_shape is a uniform downsampling of input_shape.\'\'\'\n    # Default is that nothing is resized.\n    if target_shape is None:\n        target_shape = data_shape\n    # Make a default scale_offset to fill the image if there isn\'t one\n    if scale_offset is None:\n        scale = tuple(float(ts) / ds\n                for ts, ds in zip(target_shape, data_shape))\n        offset = tuple(0.5 * s - 0.5 for s in scale)\n    else:\n        scale, offset = (v for v in zip(*scale_offset))\n        # Handle downsampling for different input vs target shape.\n        if input_shape is not None:\n            scale = tuple(s * (ts - 1) / (ns - 1)\n                    for s, ns, ts in zip(scale, input_shape, target_shape))\n            offset = tuple(o * (ts - 1) / (ns - 1)\n                    for o, ns, ts in zip(offset, input_shape, target_shape))\n    # Pytorch needs target coordinates in terms of source coordinates [-1..1]\n    ty, tx = (((torch.arange(ts, dtype=dtype, device=device) - o)\n                  * (2 / (s * (ss - 1))) - 1)\n        for ts, ss, s, o, in zip(target_shape, data_shape, scale, offset))\n    # Whoa, note that grid_sample reverses the order y, x -> x, y.\n    grid = torch.stack(\n        (tx[None,:].expand(target_shape), ty[:,None].expand(target_shape)),2\n       )[None,:,:,:].expand((1, target_shape[0], target_shape[1], 2))\n    return grid\n\ndef safe_dir_name(filename):\n    keepcharacters = (\' \',\'.\',\'_\',\'-\')\n    return \'\'.join(c\n            for c in filename if c.isalnum() or c in keepcharacters).rstrip()\n\nbargraph_palette = [\n    (\'#4B4CBF\', \'#B6B6F2\'),\n    (\'#55B05B\', \'#B6F2BA\'),\n    (\'#50BDAC\', \'#A5E5DB\'),\n    (\'#81C679\', \'#C0FF9B\'),\n    (\'#F0883B\', \'#F2CFB6\'),\n    (\'#D4CF24\', \'#F2F1B6\'),\n    (\'#D92E2B\', \'#F2B6B6\'),\n    (\'#AB6BC6\', \'#CFAAFF\'),\n]\n\ndef make_svg_bargraph(labels, heights, categories,\n        barheight=100, barwidth=12, show_labels=True, filename=None):\n    # if len(labels) == 0:\n    #     return # Nothing to do\n    unitheight = float(barheight) / max(max(heights, default=1), 1)\n    textheight = barheight if show_labels else 0\n    labelsize = float(barwidth)\n    gap = float(barwidth) / 4\n    textsize = barwidth + gap\n    rollup = max(heights, default=1)\n    textmargin = float(labelsize) * 2 / 3\n    leftmargin = 32\n    rightmargin = 8\n    svgwidth = len(heights) * (barwidth + gap) + 2 * leftmargin + rightmargin\n    svgheight = barheight + textheight\n\n    # create an SVG XML element\n    svg = et.Element(\'svg\', width=str(svgwidth), height=str(svgheight),\n            version=\'1.1\', xmlns=\'http://www.w3.org/2000/svg\')\n\n    # Draw the bar graph\n    basey = svgheight - textheight\n    x = leftmargin\n    # Add units scale on left\n    if len(heights):\n        for h in [1, (max(heights) + 1) // 2, max(heights)]:\n            et.SubElement(svg, \'text\', x=\'0\', y=\'0\',\n                style=(\'font-family:sans-serif;font-size:%dpx;\' +\n                \'text-anchor:end;alignment-baseline:hanging;\' +\n                \'transform:translate(%dpx, %dpx);\') %\n                (textsize, x - gap, basey - h * unitheight)).text = str(h)\n        et.SubElement(svg, \'text\', x=\'0\', y=\'0\',\n                style=(\'font-family:sans-serif;font-size:%dpx;\' +\n                \'text-anchor:middle;\' +\n                \'transform:translate(%dpx, %dpx) rotate(-90deg)\') %\n                (textsize, x - gap - textsize, basey - h * unitheight / 2)\n                ).text = \'units\'\n    # Draw big category background rectangles\n    for catindex, (cat, catcount) in enumerate(categories):\n        if not catcount:\n            continue\n        et.SubElement(svg, \'rect\', x=str(x), y=str(basey - rollup * unitheight),\n                width=(str((barwidth + gap) * catcount - gap)),\n                height = str(rollup*unitheight),\n                fill=bargraph_palette[catindex % len(bargraph_palette)][1])\n        x += (barwidth + gap) * catcount\n    # Draw small bars as well as 45degree text labels\n    x = leftmargin\n    catindex = -1\n    catcount = 0\n    for label, height in zip(labels, heights):\n        while not catcount and catindex <= len(categories):\n            catindex += 1\n            catcount = categories[catindex][1]\n            color = bargraph_palette[catindex % len(bargraph_palette)][0]\n        et.SubElement(svg, \'rect\', x=str(x), y=str(basey-(height * unitheight)),\n                width=str(barwidth), height=str(height * unitheight),\n                fill=color)\n        x += barwidth\n        if show_labels:\n            et.SubElement(svg, \'text\', x=\'0\', y=\'0\',\n                style=(\'font-family:sans-serif;font-size:%dpx;text-anchor:end;\'+\n                \'transform:translate(%dpx, %dpx) rotate(-45deg);\') %\n                (labelsize, x, basey + textmargin)).text = readable(label)\n        x += gap\n        catcount -= 1\n    # Text labels for each category\n    x = leftmargin\n    for cat, catcount in categories:\n        if not catcount:\n            continue\n        et.SubElement(svg, \'text\', x=\'0\', y=\'0\',\n            style=(\'font-family:sans-serif;font-size:%dpx;text-anchor:end;\'+\n            \'transform:translate(%dpx, %dpx) rotate(-90deg);\') %\n            (textsize, x + (barwidth + gap) * catcount - gap,\n                basey - rollup * unitheight + gap)).text = \'%d %s\' % (\n                    catcount, readable(cat + (\'s\' if catcount != 1 else \'\')))\n        x += (barwidth + gap) * catcount\n    # Output - this is the bare svg.\n    result = et.tostring(svg)\n    if filename:\n        f = open(filename, \'wb\')\n        # When writing to a file a special header is needed.\n        f.write(\'\'.join([\n            \'<?xml version=\\""1.0\\"" standalone=\\""no\\""?>\\n\',\n            \'<!DOCTYPE svg PUBLIC \\""-//W3C//DTD SVG 1.1//EN\\""\\n\',\n            \'\\""http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\\"">\\n\']\n            ).encode(\'utf-8\'))\n        f.write(result)\n        f.close()\n    return result\n\nreadable_replacements = [(re.compile(r[0]), r[1]) for r in [\n    (r\'-[sc]$\', \'\'),\n    (r\'_\', \' \'),\n    ]]\n\ndef readable(label):\n    for pattern, subst in readable_replacements:\n        label= re.sub(pattern, subst, label)\n    return label\n\ndef reverse_normalize_from_transform(transform):\n    \'\'\'\n    Crawl around the transforms attached to a dataset looking for a\n    Normalize transform, and return it a corresponding ReverseNormalize,\n    or None if no normalization is found.\n    \'\'\'\n    if isinstance(transform, torchvision.transforms.Normalize):\n        return ReverseNormalize(transform.mean, transform.std)\n    t = getattr(transform, \'transform\', None)\n    if t is not None:\n        return reverse_normalize_from_transform(t)\n    transforms = getattr(transform, \'transforms\', None)\n    if transforms is not None:\n        for t in reversed(transforms):\n            result = reverse_normalize_from_transform(t)\n            if result is not None:\n                return result\n    return None\n\nclass ReverseNormalize:\n    \'\'\'\n    Applies the reverse of torchvision.transforms.Normalize.\n    \'\'\'\n    def __init__(self, mean, stdev):\n        mean = numpy.array(mean)\n        stdev = numpy.array(stdev)\n        self.mean = torch.from_numpy(mean)[None,:,None,None].float()\n        self.stdev = torch.from_numpy(stdev)[None,:,None,None].float()\n    def __call__(self, data):\n        device = data.device\n        return data.mul(self.stdev.to(device)).add_(self.mean.to(device))\n\nclass ImageOnlySegRunner:\n    def __init__(self, dataset, recover_image=None):\n        if recover_image is None:\n            recover_image = reverse_normalize_from_transform(dataset)\n        self.recover_image = recover_image\n        self.dataset = dataset\n    def get_label_and_category_names(self):\n        return [(\'-\', \'-\')], [\'-\']\n    def run_and_segment_batch(self, batch, model,\n            want_bincount=False, want_rgb=False):\n        [im] = batch\n        device = next(model.parameters()).device\n        if want_rgb:\n            rgb = self.recover_image(im.clone()\n                ).permute(0, 2, 3, 1).mul_(255).clamp(0, 255).byte()\n        else:\n            rgb = None\n        # Stubs for seg and bc\n        seg = torch.zeros(im.shape[0], 1, 1, 1, dtype=torch.long)\n        bc = torch.ones(im.shape[0], 1, dtype=torch.long)\n        # Run the model.\n        model(im.to(device))\n        return seg, bc, rgb, im.shape[2:]\n\nclass ClassifierSegRunner:\n    def __init__(self, dataset, recover_image=None):\n        # The dataset contains explicit segmentations\n        if recover_image is None:\n            recover_image = reverse_normalize_from_transform(dataset)\n        self.recover_image = recover_image\n        self.dataset = dataset\n    def get_label_and_category_names(self):\n        catnames = self.dataset.categories\n        label_and_cat_names = [(readable(label),\n            catnames[self.dataset.label_category[i]])\n                for i, label in enumerate(self.dataset.labels)]\n        return label_and_cat_names, catnames\n    def run_and_segment_batch(self, batch, model,\n            want_bincount=False, want_rgb=False):\n        \'\'\'\n        Runs the dissected model on one batch of the dataset, and\n        returns a multilabel semantic segmentation for the data.\n        Given a batch of size (n, c, y, x) the segmentation should\n        be a (long integer) tensor of size (n, d, y//r, x//r) where\n        d is the maximum number of simultaneous labels given to a pixel,\n        and where r is some (optional) resolution reduction factor.\n        In the segmentation returned, the label `0` is reserved for\n        the background ""no-label"".\n\n        In addition to the segmentation, bc, rgb, and shape are returned\n        where bc is a per-image bincount counting returned label pixels,\n        rgb is a viewable (n, y, x, rgb) byte image tensor for the data\n        for visualizations (reversing normalizations, for example), and\n        shape is the (y, x) size of the data.  If want_bincount or\n        want_rgb are False, those return values may be None.\n        \'\'\'\n        im, seg, bc = batch\n        device = next(model.parameters()).device\n        if want_rgb:\n            rgb = self.recover_image(im.clone()\n                ).permute(0, 2, 3, 1).mul_(255).clamp(0, 255).byte()\n        else:\n            rgb = None\n        # Run the model.\n        model(im.to(device))\n        return seg, bc, rgb, im.shape[2:]\n\nclass GeneratorSegRunner:\n    def __init__(self, segmenter):\n        # The segmentations are given by an algorithm\n        if segmenter is None:\n            segmenter = UnifiedParsingSegmenter(segsizes=[256], segdiv=\'quad\')\n        self.segmenter = segmenter\n        self.num_classes = len(segmenter.get_label_and_category_names()[0])\n    def get_label_and_category_names(self):\n        return self.segmenter.get_label_and_category_names()\n    def run_and_segment_batch(self, batch, model,\n            want_bincount=False, want_rgb=False):\n        \'\'\'\n        Runs the dissected model on one batch of the dataset, and\n        returns a multilabel semantic segmentation for the data.\n        Given a batch of size (n, c, y, x) the segmentation should\n        be a (long integer) tensor of size (n, d, y//r, x//r) where\n        d is the maximum number of simultaneous labels given to a pixel,\n        and where r is some (optional) resolution reduction factor.\n        In the segmentation returned, the label `0` is reserved for\n        the background ""no-label"".\n\n        In addition to the segmentation, bc, rgb, and shape are returned\n        where bc is a per-image bincount counting returned label pixels,\n        rgb is a viewable (n, y, x, rgb) byte image tensor for the data\n        for visualizations (reversing normalizations, for example), and\n        shape is the (y, x) size of the data.  If want_bincount or\n        want_rgb are False, those return values may be None.\n        \'\'\'\n        device = next(model.parameters()).device\n        z_batch = batch[0]\n        tensor_images = model(z_batch.to(device))\n        seg = self.segmenter.segment_batch(tensor_images, downsample=2)\n        if want_bincount:\n            index = torch.arange(z_batch.shape[0],\n                    dtype=torch.long, device=device)\n            bc = (seg + index[:, None, None, None] * self.num_classes).view(-1\n                ).bincount(minlength=z_batch.shape[0] * self.num_classes)\n            bc = bc.view(z_batch.shape[0], self.num_classes)\n        else:\n            bc = None\n        if want_rgb:\n            images = ((tensor_images + 1) / 2 * 255)\n            rgb = images.permute(0, 2, 3, 1).clamp(0, 255).byte()\n        else:\n            rgb = None\n        return seg, bc, rgb, tensor_images.shape[2:]\n'"
netdissect/easydict.py,0,"b'\'\'\'\nFrom https://github.com/makinacorpus/easydict.\n\'\'\'\n\nclass EasyDict(dict):\n    """"""\n    Get attributes\n\n    >>> d = EasyDict({\'foo\':3})\n    >>> d[\'foo\']\n    3\n    >>> d.foo\n    3\n    >>> d.bar\n    Traceback (most recent call last):\n    ...\n    AttributeError: \'EasyDict\' object has no attribute \'bar\'\n\n    Works recursively\n\n    >>> d = EasyDict({\'foo\':3, \'bar\':{\'x\':1, \'y\':2}})\n    >>> isinstance(d.bar, dict)\n    True\n    >>> d.bar.x\n    1\n\n    Bullet-proof\n\n    >>> EasyDict({})\n    {}\n    >>> EasyDict(d={})\n    {}\n    >>> EasyDict(None)\n    {}\n    >>> d = {\'a\': 1}\n    >>> EasyDict(**d)\n    {\'a\': 1}\n\n    Set attributes\n\n    >>> d = EasyDict()\n    >>> d.foo = 3\n    >>> d.foo\n    3\n    >>> d.bar = {\'prop\': \'value\'}\n    >>> d.bar.prop\n    \'value\'\n    >>> d\n    {\'foo\': 3, \'bar\': {\'prop\': \'value\'}}\n    >>> d.bar.prop = \'newer\'\n    >>> d.bar.prop\n    \'newer\'\n\n\n    Values extraction\n\n    >>> d = EasyDict({\'foo\':0, \'bar\':[{\'x\':1, \'y\':2}, {\'x\':3, \'y\':4}]})\n    >>> isinstance(d.bar, list)\n    True\n    >>> from operator import attrgetter\n    >>> map(attrgetter(\'x\'), d.bar)\n    [1, 3]\n    >>> map(attrgetter(\'y\'), d.bar)\n    [2, 4]\n    >>> d = EasyDict()\n    >>> d.keys()\n    []\n    >>> d = EasyDict(foo=3, bar=dict(x=1, y=2))\n    >>> d.foo\n    3\n    >>> d.bar.x\n    1\n\n    Still like a dict though\n\n    >>> o = EasyDict({\'clean\':True})\n    >>> o.items()\n    [(\'clean\', True)]\n\n    And like a class\n\n    >>> class Flower(EasyDict):\n    ...     power = 1\n    ...\n    >>> f = Flower()\n    >>> f.power\n    1\n    >>> f = Flower({\'height\': 12})\n    >>> f.height\n    12\n    >>> f[\'power\']\n    1\n    >>> sorted(f.keys())\n    [\'height\', \'power\']\n    """"""\n    def __init__(self, d=None, **kwargs):\n        if d is None:\n            d = {}\n        if kwargs:\n            d.update(**kwargs)\n        for k, v in d.items():\n            setattr(self, k, v)\n        # Class attributes\n        for k in self.__class__.__dict__.keys():\n            if not (k.startswith(\'__\') and k.endswith(\'__\')):\n                setattr(self, k, getattr(self, k))\n\n    def __setattr__(self, name, value):\n        if isinstance(value, (list, tuple)):\n            value = [self.__class__(x)\n                     if isinstance(x, dict) else x for x in value]\n        elif isinstance(value, dict) and not isinstance(value, self.__class__):\n            value = self.__class__(value)\n        super(EasyDict, self).__setattr__(name, value)\n        super(EasyDict, self).__setitem__(name, value)\n\n    __setitem__ = __setattr__\n\ndef load_json(filename):\n    import json\n    with open(filename) as f:\n        return EasyDict(json.load(f))\n\nif __name__ == ""__main__"":\n    import doctest\n    doctest.testmod()\n'"
netdissect/evalablate.py,13,"b'import torch, sys, os, argparse, textwrap, numbers, numpy, json, PIL\nfrom torchvision import transforms\nfrom torch.utils.data import TensorDataset\nfrom netdissect.progress import default_progress, post_progress, desc_progress\nfrom netdissect.progress import verbose_progress, print_progress\nfrom netdissect.nethook import edit_layers\nfrom netdissect.zdataset import standard_z_sample\nfrom netdissect.autoeval import autoimport_eval\nfrom netdissect.easydict import EasyDict\nfrom netdissect.modelconfig import create_instrumented_model\n\nhelp_epilog = \'\'\'\\\nExample:\n\npython -m netdissect.evalablate \\\n      --segmenter ""netdissect.segmenter.UnifiedParsingSegmenter(segsizes=[256], segdiv=\'quad\')"" \\\n      --model ""proggan.from_pth_file(\'models/lsun_models/${SCENE}_lsun.pth\')"" \\\n      --outdir dissect/dissectdir \\\n      --classes mirror coffeetable tree \\\n      --layers layer4 \\\n      --size 1000\n\nOutput layout:\ndissectdir/layer5/ablation/mirror-iqr.json\n{ class: ""mirror"",\n  classnum: 43,\n  pixel_total: 41342300,\n  class_pixels: 1234531,\n  layer: ""layer5"",\n  ranking: ""mirror-iqr"",\n  ablation_units: [341, 23, 12, 142, 83, ...]\n  ablation_pixels: [143242, 132344, 429931, ...]\n}\n\n\'\'\'\n\ndef main():\n    # Training settings\n    def strpair(arg):\n        p = tuple(arg.split(\':\'))\n        if len(p) == 1:\n            p = p + p\n        return p\n\n    parser = argparse.ArgumentParser(description=\'Ablation eval\',\n            epilog=textwrap.dedent(help_epilog),\n            formatter_class=argparse.RawDescriptionHelpFormatter)\n    parser.add_argument(\'--model\', type=str, default=None,\n                        help=\'constructor for the model to test\')\n    parser.add_argument(\'--pthfile\', type=str, default=None,\n                        help=\'filename of .pth file for the model\')\n    parser.add_argument(\'--outdir\', type=str, default=\'dissect\', required=True,\n                        help=\'directory for dissection output\')\n    parser.add_argument(\'--layers\', type=strpair, nargs=\'+\',\n                        help=\'space-separated list of layer names to edit\' + \n                        \', in the form layername[:reportedname]\')\n    parser.add_argument(\'--classes\', type=str, nargs=\'+\',\n                        help=\'space-separated list of class names to ablate\')\n    parser.add_argument(\'--metric\', type=str, default=\'iou\',\n                        help=\'ordering metric for selecting units\')\n    parser.add_argument(\'--unitcount\', type=int, default=30,\n                        help=\'number of units to ablate\')\n    parser.add_argument(\'--segmenter\', type=str,\n                        help=\'directory containing segmentation dataset\')\n    parser.add_argument(\'--netname\', type=str, default=None,\n                        help=\'name for network in generated reports\')\n    parser.add_argument(\'--batch_size\', type=int, default=5,\n                        help=\'batch size for forward pass\')\n    parser.add_argument(\'--size\', type=int, default=200,\n                        help=\'number of images to test\')\n    parser.add_argument(\'--no-cuda\', action=\'store_true\', default=False,\n                        help=\'disables CUDA usage\')\n    parser.add_argument(\'--quiet\', action=\'store_true\', default=False,\n                        help=\'silences console output\')\n    if len(sys.argv) == 1:\n        parser.print_usage(sys.stderr)\n        sys.exit(1)\n    args = parser.parse_args()\n\n    # Set up console output\n    verbose_progress(not args.quiet)\n\n    # Speed up pytorch\n    torch.backends.cudnn.benchmark = True\n\n    # Set up CUDA\n    args.cuda = not args.no_cuda and torch.cuda.is_available()\n    if args.cuda:\n        torch.backends.cudnn.benchmark = True\n\n    # Take defaults for model constructor etc from dissect.json settings.\n    with open(os.path.join(args.outdir, \'dissect.json\')) as f:\n        dissection = EasyDict(json.load(f))\n    if args.model is None:\n        args.model = dissection.settings.model\n    if args.pthfile is None:\n        args.pthfile = dissection.settings.pthfile\n    if args.segmenter is None:\n        args.segmenter = dissection.settings.segmenter\n\n    # Instantiate generator\n    model = create_instrumented_model(args, gen=True, edit=True)\n    if model is None:\n        print(\'No model specified\')\n        sys.exit(1)\n\n    # Instantiate model\n    device = next(model.parameters()).device\n    input_shape = model.input_shape\n\n    # 4d input if convolutional, 2d input if first layer is linear.\n    raw_sample = standard_z_sample(args.size, input_shape[1], seed=2).view(\n            (args.size,) + input_shape[1:])\n    dataset = TensorDataset(raw_sample)\n\n    # Create the segmenter\n    segmenter = autoimport_eval(args.segmenter)\n\n    # Now do the actual work.\n    labelnames, catnames = (\n                segmenter.get_label_and_category_names(dataset))\n    label_category = [catnames.index(c) if c in catnames else 0\n            for l, c in labelnames]\n    labelnum_from_name = {n[0]: i for i, n in enumerate(labelnames)}\n\n    segloader = torch.utils.data.DataLoader(dataset,\n                batch_size=args.batch_size, num_workers=10,\n                pin_memory=(device.type == \'cuda\'))\n\n    # Index the dissection layers by layer name.\n    dissect_layer = {lrec.layer: lrec for lrec in dissection.layers}\n\n    # First, collect a baseline\n    for l in model.ablation:\n        model.ablation[l] = None\n\n    # For each sort-order, do an ablation\n    progress = default_progress()\n    for classname in progress(args.classes):\n        post_progress(c=classname)\n        for layername in progress(model.ablation):\n            post_progress(l=layername)\n            rankname = \'%s-%s\' % (classname, args.metric)\n            classnum = labelnum_from_name[classname]\n            try:\n                ranking = next(r for r in dissect_layer[layername].rankings\n                        if r.name == rankname)\n            except:\n                print(\'%s not found\' % rankname)\n                sys.exit(1)\n            ordering = numpy.argsort(ranking.score)\n            # Check if already done\n            ablationdir = os.path.join(args.outdir, layername, \'pixablation\')\n            if os.path.isfile(os.path.join(ablationdir, \'%s.json\'%rankname)):\n                with open(os.path.join(ablationdir, \'%s.json\'%rankname)) as f:\n                    data = EasyDict(json.load(f))\n                # If the unit ordering is not the same, something is wrong\n                if not all(a == o\n                        for a, o in zip(data.ablation_units, ordering)):\n                    continue\n                if len(data.ablation_effects) >= args.unitcount:\n                    continue # file already done.\n                measurements = data.ablation_effects\n            measurements = measure_ablation(segmenter, segloader,\n                    model, classnum, layername, ordering[:args.unitcount])\n            measurements = measurements.cpu().numpy().tolist()\n            os.makedirs(ablationdir, exist_ok=True)\n            with open(os.path.join(ablationdir, \'%s.json\'%rankname), \'w\') as f:\n                json.dump(dict(\n                    classname=classname,\n                    classnum=classnum,\n                    baseline=measurements[0],\n                    layer=layername,\n                    metric=args.metric,\n                    ablation_units=ordering.tolist(),\n                    ablation_effects=measurements[1:]), f)\n\ndef measure_ablation(segmenter, loader, model, classnum, layer, ordering):\n    total_bincount = 0\n    data_size = 0\n    device = next(model.parameters()).device\n    progress = default_progress()\n    for l in model.ablation:\n        model.ablation[l] = None\n    feature_units = model.feature_shape[layer][1]\n    feature_shape = model.feature_shape[layer][2:]\n    repeats = len(ordering)\n    total_scores = torch.zeros(repeats + 1)\n    for i, batch in enumerate(progress(loader)):\n        z_batch = batch[0]\n        model.ablation[layer] = None\n        tensor_images = model(z_batch.to(device))\n        seg = segmenter.segment_batch(tensor_images, downsample=2)\n        mask = (seg == classnum).max(1)[0]\n        downsampled_seg = torch.nn.functional.adaptive_avg_pool2d(\n                mask.float()[:,None,:,:], feature_shape)[:,0,:,:]\n        total_scores[0] += downsampled_seg.sum().cpu()\n        # Now we need to do an intervention for every location\n        # that had a nonzero downsampled_seg, if any.\n        interventions_needed = downsampled_seg.nonzero()\n        location_count = len(interventions_needed)\n        if location_count == 0:\n            continue\n        interventions_needed = interventions_needed.repeat(repeats, 1)\n        inter_z = batch[0][interventions_needed[:,0]].to(device)\n        inter_chan = torch.zeros(repeats, location_count, feature_units,\n                device=device)\n        for j, u in enumerate(ordering):\n            inter_chan[j:, :, u] = 1\n        inter_chan = inter_chan.view(len(inter_z), feature_units)\n        inter_loc = interventions_needed[:,1:]\n        scores = torch.zeros(len(inter_z))\n        batch_size = len(batch[0])\n        for j in range(0, len(inter_z), batch_size):\n            ibz = inter_z[j:j+batch_size]\n            ibl = inter_loc[j:j+batch_size].t()\n            imask = torch.zeros((len(ibz),) + feature_shape, device=ibz.device)\n            imask[(torch.arange(len(ibz)),) + tuple(ibl)] = 1\n            ibc = inter_chan[j:j+batch_size]\n            model.ablation[layer] = (\n                    imask.float()[:,None,:,:] * ibc[:,:,None,None])\n            tensor_images = model(ibz)\n            seg = segmenter.segment_batch(tensor_images, downsample=2)\n            mask = (seg == classnum).max(1)[0]\n            downsampled_iseg = torch.nn.functional.adaptive_avg_pool2d(\n                    mask.float()[:,None,:,:], feature_shape)[:,0,:,:]\n            scores[j:j+batch_size] = downsampled_iseg[\n                    (torch.arange(len(ibz)),) + tuple(ibl)]\n        scores = scores.view(repeats, location_count).sum(1)\n        total_scores[1:] += scores\n    return total_scores\n\ndef count_segments(segmenter, loader, model):\n    total_bincount = 0\n    data_size = 0\n    progress = default_progress()\n    for i, batch in enumerate(progress(loader)):\n        tensor_images = model(z_batch.to(device))\n        seg = segmenter.segment_batch(tensor_images, downsample=2)\n        bc = (seg + index[:, None, None, None] * self.num_classes).view(-1\n                ).bincount(minlength=z_batch.shape[0] * self.num_classes)\n        data_size += seg.shape[0] * seg.shape[2] * seg.shape[3]\n        total_bincount += batch_label_counts.float().sum(0)\n    normalized_bincount = total_bincount / data_size\n    return normalized_bincount\n\nif __name__ == \'__main__\':\n    main()\n'"
netdissect/fullablate.py,9,"b'import torch, sys, os, argparse, textwrap, numbers, numpy, json, PIL\nfrom torchvision import transforms\nfrom torch.utils.data import TensorDataset\nfrom netdissect.progress import default_progress, post_progress, desc_progress\nfrom netdissect.progress import verbose_progress, print_progress\nfrom netdissect.nethook import edit_layers\nfrom netdissect.zdataset import standard_z_sample\nfrom netdissect.autoeval import autoimport_eval\nfrom netdissect.easydict import EasyDict\nfrom netdissect.modelconfig import create_instrumented_model\n\nhelp_epilog = \'\'\'\\\nExample:\n\npython -m netdissect.evalablate \\\n      --segmenter ""netdissect.GanImageSegmenter(segvocab=\'lowres\', segsizes=[160,288], segdiv=\'quad\')"" \\\n      --model ""proggan.from_pth_file(\'models/lsun_models/${SCENE}_lsun.pth\')"" \\\n      --outdir dissect/dissectdir \\\n      --classname tree \\\n      --layer layer4 \\\n      --size 1000\n\nOutput layout:\ndissectdir/layer5/ablation/mirror-iqr.json\n{ class: ""mirror"",\n  classnum: 43,\n  pixel_total: 41342300,\n  class_pixels: 1234531,\n  layer: ""layer5"",\n  ranking: ""mirror-iqr"",\n  ablation_units: [341, 23, 12, 142, 83, ...]\n  ablation_pixels: [143242, 132344, 429931, ...]\n}\n\n\'\'\'\n\ndef main():\n    # Training settings\n    def strpair(arg):\n        p = tuple(arg.split(\':\'))\n        if len(p) == 1:\n            p = p + p\n        return p\n\n    parser = argparse.ArgumentParser(description=\'Ablation eval\',\n            epilog=textwrap.dedent(help_epilog),\n            formatter_class=argparse.RawDescriptionHelpFormatter)\n    parser.add_argument(\'--model\', type=str, default=None,\n                        help=\'constructor for the model to test\')\n    parser.add_argument(\'--pthfile\', type=str, default=None,\n                        help=\'filename of .pth file for the model\')\n    parser.add_argument(\'--outdir\', type=str, default=\'dissect\', required=True,\n                        help=\'directory for dissection output\')\n    parser.add_argument(\'--layer\', type=strpair,\n                        help=\'space-separated list of layer names to edit\' + \n                        \', in the form layername[:reportedname]\')\n    parser.add_argument(\'--classname\', type=str,\n                        help=\'class name to ablate\')\n    parser.add_argument(\'--metric\', type=str, default=\'iou\',\n                        help=\'ordering metric for selecting units\')\n    parser.add_argument(\'--unitcount\', type=int, default=30,\n                        help=\'number of units to ablate\')\n    parser.add_argument(\'--segmenter\', type=str,\n                        help=\'directory containing segmentation dataset\')\n    parser.add_argument(\'--netname\', type=str, default=None,\n                        help=\'name for network in generated reports\')\n    parser.add_argument(\'--batch_size\', type=int, default=25,\n                        help=\'batch size for forward pass\')\n    parser.add_argument(\'--mixed_units\', action=\'store_true\', default=False,\n                        help=\'true to keep alpha for non-zeroed units\')\n    parser.add_argument(\'--size\', type=int, default=200,\n                        help=\'number of images to test\')\n    parser.add_argument(\'--no-cuda\', action=\'store_true\', default=False,\n                        help=\'disables CUDA usage\')\n    parser.add_argument(\'--quiet\', action=\'store_true\', default=False,\n                        help=\'silences console output\')\n    if len(sys.argv) == 1:\n        parser.print_usage(sys.stderr)\n        sys.exit(1)\n    args = parser.parse_args()\n\n    # Set up console output\n    verbose_progress(not args.quiet)\n\n    # Speed up pytorch\n    torch.backends.cudnn.benchmark = True\n\n    # Set up CUDA\n    args.cuda = not args.no_cuda and torch.cuda.is_available()\n    if args.cuda:\n        torch.backends.cudnn.benchmark = True\n\n    # Take defaults for model constructor etc from dissect.json settings.\n    with open(os.path.join(args.outdir, \'dissect.json\')) as f:\n        dissection = EasyDict(json.load(f))\n    if args.model is None:\n        args.model = dissection.settings.model\n    if args.pthfile is None:\n        args.pthfile = dissection.settings.pthfile\n    if args.segmenter is None:\n        args.segmenter = dissection.settings.segmenter\n    if args.layer is None:\n        args.layer = dissection.settings.layers[0]\n    args.layers = [args.layer]\n\n    # Also load specific analysis\n    layername = args.layer[1]\n    if args.metric == \'iou\':\n        summary = dissection\n    else:\n        with open(os.path.join(args.outdir, layername, args.metric,\n                args.classname, \'summary.json\')) as f:\n            summary = EasyDict(json.load(f))\n\n    # Instantiate generator\n    model = create_instrumented_model(args, gen=True, edit=True)\n    if model is None:\n        print(\'No model specified\')\n        sys.exit(1)\n\n    # Instantiate model\n    device = next(model.parameters()).device\n    input_shape = model.input_shape\n\n    # 4d input if convolutional, 2d input if first layer is linear.\n    raw_sample = standard_z_sample(args.size, input_shape[1], seed=3).view(\n            (args.size,) + input_shape[1:])\n    dataset = TensorDataset(raw_sample)\n\n    # Create the segmenter\n    segmenter = autoimport_eval(args.segmenter)\n\n    # Now do the actual work.\n    labelnames, catnames = (\n                segmenter.get_label_and_category_names(dataset))\n    label_category = [catnames.index(c) if c in catnames else 0\n            for l, c in labelnames]\n    labelnum_from_name = {n[0]: i for i, n in enumerate(labelnames)}\n\n    segloader = torch.utils.data.DataLoader(dataset,\n                batch_size=args.batch_size, num_workers=10,\n                pin_memory=(device.type == \'cuda\'))\n\n    # Index the dissection layers by layer name.\n\n    # First, collect a baseline\n    for l in model.ablation:\n        model.ablation[l] = None\n\n    # For each sort-order, do an ablation\n    progress = default_progress()\n    classname = args.classname\n    classnum = labelnum_from_name[classname]\n\n    # Get iou ranking from dissect.json\n    iou_rankname = \'%s-%s\' % (classname, \'iou\')\n    dissect_layer = {lrec.layer: lrec for lrec in dissection.layers}\n    iou_ranking = next(r for r in dissect_layer[layername].rankings\n                if r.name == iou_rankname)\n\n    # Get trained ranking from summary.json\n    rankname = \'%s-%s\' % (classname, args.metric)\n    summary_layer = {lrec.layer: lrec for lrec in summary.layers}\n    ranking = next(r for r in summary_layer[layername].rankings\n                if r.name == rankname)\n\n    # Get ordering, first by ranking, then break ties by iou.\n    ordering = [t[2] for t in sorted([(s1, s2, i)\n        for i, (s1, s2) in enumerate(zip(ranking.score, iou_ranking.score))])]\n    values = (-numpy.array(ranking.score))[ordering]\n    if not args.mixed_units:\n        values[...] = 1\n\n    ablationdir = os.path.join(args.outdir, layername, \'fullablation\')\n    measurements = measure_full_ablation(segmenter, segloader,\n            model, classnum, layername,\n            ordering[:args.unitcount], values[:args.unitcount])\n    measurements = measurements.cpu().numpy().tolist()\n    os.makedirs(ablationdir, exist_ok=True)\n    with open(os.path.join(ablationdir, \'%s.json\'%rankname), \'w\') as f:\n        json.dump(dict(\n            classname=classname,\n            classnum=classnum,\n            baseline=measurements[0],\n            layer=layername,\n            metric=args.metric,\n            ablation_units=ordering,\n            ablation_values=values.tolist(),\n            ablation_effects=measurements[1:]), f)\n\ndef measure_full_ablation(segmenter, loader, model, classnum, layer,\n        ordering, values):\n    \'\'\'\n    Quick and easy counting of segmented pixels reduced by ablating units.\n    \'\'\'\n    progress = default_progress()\n    device = next(model.parameters()).device\n    feature_units = model.feature_shape[layer][1]\n    feature_shape = model.feature_shape[layer][2:]\n    repeats = len(ordering)\n    total_scores = torch.zeros(repeats + 1)\n    print(ordering)\n    print(values.tolist())\n    with torch.no_grad():\n        for l in model.ablation:\n            model.ablation[l] = None\n        for i, [ibz] in enumerate(progress(loader)):\n            ibz = ibz.cuda()\n            for num_units in progress(range(len(ordering) + 1)):\n                ablation = torch.zeros(feature_units, device=device)\n                ablation[ordering[:num_units]] = torch.tensor(\n                        values[:num_units]).to(ablation.device, ablation.dtype)\n                model.ablation[layer] = ablation\n                tensor_images = model(ibz)\n                seg = segmenter.segment_batch(tensor_images, downsample=2)\n                mask = (seg == classnum).max(1)[0]\n                total_scores[num_units] += mask.sum().float().cpu()\n    return total_scores\n\ndef count_segments(segmenter, loader, model):\n    total_bincount = 0\n    data_size = 0\n    progress = default_progress()\n    for i, batch in enumerate(progress(loader)):\n        tensor_images = model(z_batch.to(device))\n        seg = segmenter.segment_batch(tensor_images, downsample=2)\n        bc = (seg + index[:, None, None, None] * self.num_classes).view(-1\n                ).bincount(minlength=z_batch.shape[0] * self.num_classes)\n        data_size += seg.shape[0] * seg.shape[2] * seg.shape[3]\n        total_bincount += batch_label_counts.float().sum(0)\n    normalized_bincount = total_bincount / data_size\n    return normalized_bincount\n\nif __name__ == \'__main__\':\n    main()\n'"
netdissect/modelconfig.py,10,"b""'''\nOriginal from https://github.com/CSAILVision/GANDissect\nModified by Erik H\xc3\xa4rk\xc3\xb6nen, 29.11.2019\n'''\n\nimport numbers\nimport torch\nfrom netdissect.autoeval import autoimport_eval\nfrom netdissect.progress import print_progress\nfrom netdissect.nethook import InstrumentedModel\nfrom netdissect.easydict import EasyDict\n\ndef create_instrumented_model(args, **kwargs):\n    '''\n    Creates an instrumented model out of a namespace of arguments that\n    correspond to ArgumentParser command-line args:\n      model: a string to evaluate as a constructor for the model.\n      pthfile: (optional) filename of .pth file for the model.\n      layers: a list of layers to instrument, defaulted if not provided.\n      edit: True to instrument the layers for editing.\n      gen: True for a generator model.  One-pixel input assumed.\n      imgsize: For non-generator models, (y, x) dimensions for RGB input.\n      cuda: True to use CUDA.\n  \n    The constructed model will be decorated with the following attributes:\n      input_shape: (usually 4d) tensor shape for single-image input.\n      output_shape: 4d tensor shape for output.\n      feature_shape: map of layer names to 4d tensor shape for featuremaps.\n      retained: map of layernames to tensors, filled after every evaluation.\n      ablation: if editing, map of layernames to [0..1] alpha values to fill.\n      replacement: if editing, map of layernames to values to fill.\n\n    When editing, the feature value x will be replaced by:\n        `x = (replacement * ablation) + (x * (1 - ablation))`\n    '''\n\n    args = EasyDict(vars(args), **kwargs)\n\n    # Construct the network\n    if args.model is None:\n        print_progress('No model specified')\n        return None\n    if isinstance(args.model, torch.nn.Module):\n        model = args.model\n    else:\n        model = autoimport_eval(args.model)\n    # Unwrap any DataParallel-wrapped model\n    if isinstance(model, torch.nn.DataParallel):\n        model = next(model.children())\n\n    # Load its state dict\n    meta = {}\n    if getattr(args, 'pthfile', None) is not None:\n        data = torch.load(args.pthfile)\n        if 'state_dict' in data:\n            meta = {}\n            for key in data:\n                if isinstance(data[key], numbers.Number):\n                    meta[key] = data[key]\n            data = data['state_dict']\n        submodule = getattr(args, 'submodule', None)\n        if submodule is not None and len(submodule):\n            remove_prefix = submodule + '.'\n            data = { k[len(remove_prefix):]: v for k, v in data.items()\n                    if k.startswith(remove_prefix)}\n            if not len(data):\n                print_progress('No submodule %s found in %s' %\n                        (submodule, args.pthfile))\n                return None\n        model.load_state_dict(data, strict=not getattr(args, 'unstrict', False))\n\n    # Decide which layers to instrument.\n    if getattr(args, 'layer', None) is not None:\n        args.layers = [args.layer]\n    if getattr(args, 'layers', None) is None:\n        # Skip wrappers with only one named model\n        container = model\n        prefix = ''\n        while len(list(container.named_children())) == 1:\n            name, container = next(container.named_children())\n            prefix += name + '.'\n        # Default to all nontrivial top-level layers except last.\n        args.layers = [prefix + name\n                for name, module in container.named_children()\n                if type(module).__module__ not in [\n                    # Skip ReLU and other activations.\n                    'torch.nn.modules.activation',\n                    # Skip pooling layers.\n                    'torch.nn.modules.pooling']\n                ][:-1]\n        print_progress('Defaulting to layers: %s' % ' '.join(args.layers))\n\n    # Now wrap the model for instrumentation.\n    model = InstrumentedModel(model)\n    model.meta = meta\n\n    # Instrument the layers.\n    model.retain_layers(args.layers)\n    model.eval()\n    if args.cuda:\n        model.cuda()\n\n    # Annotate input, output, and feature shapes\n    annotate_model_shapes(model,\n            gen=getattr(args, 'gen', False),\n            imgsize=getattr(args, 'imgsize', None),\n            latent_shape=getattr(args, 'latent_shape', None))\n    return model\n\ndef annotate_model_shapes(model, gen=False, imgsize=None, latent_shape=None):\n    assert (imgsize is not None) or gen\n\n    # Figure the input shape.\n    if gen:\n        if latent_shape is None:\n            # We can guess a generator's input shape by looking at the model.\n            # Examine first conv in model to determine input feature size.\n            first_layer = [c for c in model.modules()\n                    if isinstance(c, (torch.nn.Conv2d, torch.nn.ConvTranspose2d,\n                        torch.nn.Linear))][0]\n            # 4d input if convolutional, 2d input if first layer is linear.\n            if isinstance(first_layer, (torch.nn.Conv2d, torch.nn.ConvTranspose2d)):\n                input_shape = (1, first_layer.in_channels, 1, 1)\n            else:\n                input_shape = (1, first_layer.in_features)\n        else:\n            # Specify input shape manually\n            input_shape = latent_shape\n    else:\n        # For a classifier, the input image shape is given as an argument.\n        input_shape = (1, 3) + tuple(imgsize)\n\n    # Run the model once to observe feature shapes.\n    device = next(model.parameters()).device\n    dry_run = torch.zeros(input_shape).to(device)\n    with torch.no_grad():\n        output = model(dry_run)\n\n    # Annotate shapes.\n    model.input_shape = input_shape\n    model.feature_shape = { layer: feature.shape\n            for layer, feature in model.retained_features().items() }\n    model.output_shape = output.shape\n    return model\n"""
netdissect/nethook.py,3,"b""'''\nUtilities for instrumenting a torch model.\n\nInstrumentedModel will wrap a pytorch model and allow hooking\narbitrary layers to monitor or modify their output directly.\n\nModified by Erik H\xc3\xa4rk\xc3\xb6nen:\n- 29.11.2019: Unhooking bugfix\n- 25.01.2020: Offset edits, removed old API\n'''\n\nimport torch, numpy, types\nfrom collections import OrderedDict\n\nclass InstrumentedModel(torch.nn.Module):\n    '''\n    A wrapper for hooking, probing and intervening in pytorch Modules.\n    Example usage:\n\n    ```\n    model = load_my_model()\n    with inst as InstrumentedModel(model):\n        inst.retain_layer(layername)\n        inst.edit_layer(layername, 0.5, target_features)\n        inst.edit_layer(layername, offset=offset_tensor)\n        inst(inputs)\n        original_features = inst.retained_layer(layername)\n    ```\n    '''\n\n    def __init__(self, model):\n        super(InstrumentedModel, self).__init__()\n        self.model = model\n        self._retained = OrderedDict()\n        self._ablation = {}\n        self._replacement = {}\n        self._offset = {}\n        self._hooked_layer = {}\n        self._old_forward = {}\n\n    def __enter__(self):\n        return self\n\n    def __exit__(self, type, value, traceback):\n        self.close()\n\n    def forward(self, *inputs, **kwargs):\n        return self.model(*inputs, **kwargs)\n\n    def retain_layer(self, layername):\n        '''\n        Pass a fully-qualified layer name (E.g., module.submodule.conv3)\n        to hook that layer and retain its output each time the model is run.\n        A pair (layername, aka) can be provided, and the aka will be used\n        as the key for the retained value instead of the layername.\n        '''\n        self.retain_layers([layername])\n\n    def retain_layers(self, layernames):\n        '''\n        Retains a list of a layers at once.\n        '''\n        self.add_hooks(layernames)\n        for layername in layernames:\n            aka = layername\n            if not isinstance(aka, str):\n                layername, aka = layername\n            if aka not in self._retained:\n                self._retained[aka] = None\n\n    def retained_features(self):\n        '''\n        Returns a dict of all currently retained features.\n        '''\n        return OrderedDict(self._retained)\n\n    def retained_layer(self, aka=None, clear=False):\n        '''\n        Retrieve retained data that was previously hooked by retain_layer.\n        Call this after the model is run.  If clear is set, then the\n        retained value will return and also cleared.\n        '''\n        if aka is None:\n            # Default to the first retained layer.\n            aka = next(self._retained.keys().__iter__())\n        result = self._retained[aka]\n        if clear:\n            self._retained[aka] = None\n        return result\n\n    def edit_layer(self, layername, ablation=None, replacement=None, offset=None):\n        '''\n        Pass a fully-qualified layer name (E.g., module.submodule.conv3)\n        to hook that layer and modify its output each time the model is run.\n        The output of the layer will be modified to be a convex combination\n        of the replacement and x interpolated according to the ablation, i.e.:\n        `output = x * (1 - a) + (r * a)`.\n        Additionally or independently, an offset can be added to the output.\n        '''\n        if not isinstance(layername, str):\n            layername, aka = layername\n        else:\n            aka = layername\n\n        # The default ablation if a replacement is specified is 1.0.\n        if ablation is None and replacement is not None:\n            ablation = 1.0\n        self.add_hooks([(layername, aka)])\n        if ablation is not None:\n            self._ablation[aka] = ablation\n        if replacement is not None:\n            self._replacement[aka] = replacement\n        if offset is not None:\n            self._offset[aka] = offset\n        # If needed, could add an arbitrary postprocessing lambda here.\n\n    def remove_edits(self, layername=None, remove_offset=True, remove_replacement=True):\n        '''\n        Removes edits at the specified layer, or removes edits at all layers\n        if no layer name is specified.\n        '''\n        if layername is None:\n            if remove_replacement:\n                self._ablation.clear()\n                self._replacement.clear()\n            if remove_offset:\n                self._offset.clear()\n            return\n\n        if not isinstance(layername, str):\n            layername, aka = layername\n        else:\n            aka = layername\n        if remove_replacement and aka in self._ablation:\n            del self._ablation[aka]\n        if remove_replacement and aka in self._replacement:\n            del self._replacement[aka]\n        if remove_offset and aka in self._offset:\n            del self._offset[aka]\n\n    def add_hooks(self, layernames):\n        '''\n        Sets up a set of layers to be hooked.\n\n        Usually not called directly: use edit_layer or retain_layer instead.\n        '''\n        needed = set()\n        aka_map = {}\n        for name in layernames:\n            aka = name\n            if not isinstance(aka, str):\n                name, aka = name\n            if self._hooked_layer.get(aka, None) != name:\n                aka_map[name] = aka\n                needed.add(name)\n        if not needed:\n            return\n        for name, layer in self.model.named_modules():\n            if name in aka_map:\n                needed.remove(name)\n                aka = aka_map[name]\n                self._hook_layer(layer, name, aka)\n        for name in needed:\n            raise ValueError('Layer %s not found in model' % name)\n\n    def _hook_layer(self, layer, layername, aka):\n        '''\n        Internal method to replace a forward method with a closure that\n        intercepts the call, and tracks the hook so that it can be reverted.\n        '''\n        if aka in self._hooked_layer:\n            raise ValueError('Layer %s already hooked' % aka)\n        if layername in self._old_forward:\n            raise ValueError('Layer %s already hooked' % layername)\n        self._hooked_layer[aka] = layername\n        self._old_forward[layername] = (layer, aka,\n                layer.__dict__.get('forward', None))\n        editor = self\n        original_forward = layer.forward\n        def new_forward(self, *inputs, **kwargs):\n            original_x = original_forward(*inputs, **kwargs)\n            x = editor._postprocess_forward(original_x, aka)\n            return x\n        layer.forward = types.MethodType(new_forward, layer)\n\n    def _unhook_layer(self, aka):\n        '''\n        Internal method to remove a hook, restoring the original forward method.\n        '''\n        if aka not in self._hooked_layer:\n            return\n        layername = self._hooked_layer[aka]\n        layer, check, old_forward = self._old_forward[layername]\n        assert check == aka\n        if old_forward is None:\n            if 'forward' in layer.__dict__:\n                del layer.__dict__['forward']\n        else:\n            layer.forward = old_forward\n        del self._old_forward[layername]\n        del self._hooked_layer[aka]\n        if aka in self._ablation:\n            del self._ablation[aka]\n        if aka in self._replacement:\n            del self._replacement[aka]\n        if aka in self._offset:\n            del self._offset[aka]\n        if aka in self._retained:\n            del self._retained[aka]\n\n    def _postprocess_forward(self, x, aka):\n        '''\n        The internal method called by the hooked layers after they are run.\n        '''\n        # Retain output before edits, if desired.\n        if aka in self._retained:\n            self._retained[aka] = x.detach()\n        \n        # Apply replacement edit\n        a = make_matching_tensor(self._ablation, aka, x)\n        if a is not None:\n            x = x * (1 - a)\n            v = make_matching_tensor(self._replacement, aka, x)\n            if v is not None:\n                x += (v * a)\n        \n        # Apply offset edit\n        b = make_matching_tensor(self._offset, aka, x)\n        if b is not None:\n            x = x + b\n        \n        return x\n\n    def close(self):\n        '''\n        Unhooks all hooked layers in the model.\n        '''\n        for aka in list(self._old_forward.keys()):\n            self._unhook_layer(aka)\n        assert len(self._old_forward) == 0\n\n\ndef make_matching_tensor(valuedict, name, data):\n    '''\n    Converts `valuedict[name]` to be a tensor with the same dtype, device,\n    and dimension count as `data`, and caches the converted tensor.\n    '''\n    v = valuedict.get(name, None)\n    if v is None:\n        return None\n    if not isinstance(v, torch.Tensor):\n        # Accept non-torch data.\n        v = torch.from_numpy(numpy.array(v))\n        valuedict[name] = v\n    if not v.device == data.device or not v.dtype == data.dtype:\n        # Ensure device and type matches.\n        assert not v.requires_grad, '%s wrong device or type' % (name)\n        v = v.to(device=data.device, dtype=data.dtype)\n        valuedict[name] = v\n    if len(v.shape) < len(data.shape):\n        # Ensure dimensions are unsqueezed as needed.\n        assert not v.requires_grad, '%s wrong dimensions' % (name)\n        v = v.view((1,) + tuple(v.shape) +\n                (1,) * (len(data.shape) - len(v.shape) - 1))\n        valuedict[name] = v\n    return v\n"""
netdissect/parallelfolder.py,1,"b'\'\'\'\nVariants of pytorch\'s ImageFolder for loading image datasets with more\ninformation, such as parallel feature channels in separate files,\ncached files with lists of filenames, etc.\n\'\'\'\n\nimport os, torch, re\nimport torch.utils.data as data\nfrom torchvision.datasets.folder import default_loader\nfrom PIL import Image\nfrom collections import OrderedDict\nfrom .progress import default_progress\n\ndef grayscale_loader(path):\n    with open(path, \'rb\') as f:\n        return Image.open(f).convert(\'L\')\n\nclass ParallelImageFolders(data.Dataset):\n    """"""\n    A data loader that looks for parallel image filenames, for example\n\n    photo1/park/004234.jpg\n    photo1/park/004236.jpg\n    photo1/park/004237.jpg\n\n    photo2/park/004234.png\n    photo2/park/004236.png\n    photo2/park/004237.png\n    """"""\n    def __init__(self, image_roots,\n            transform=None,\n            loader=default_loader,\n            stacker=None,\n            intersection=False,\n            verbose=None,\n            size=None):\n        self.image_roots = image_roots\n        self.images = make_parallel_dataset(image_roots,\n                intersection=intersection, verbose=verbose)\n        if len(self.images) == 0:\n            raise RuntimeError(""Found 0 images within: %s"" % image_roots)\n        if size is not None:\n            self.image = self.images[:size]\n        if transform is not None and not hasattr(transform, \'__iter__\'):\n            transform = [transform for _ in image_roots]\n        self.transforms = transform\n        self.stacker = stacker\n        self.loader = loader\n\n    def __getitem__(self, index):\n        paths = self.images[index]\n        sources = [self.loader(path) for path in paths]\n        # Add a common shared state dict to allow random crops/flips to be\n        # coordinated.\n        shared_state = {}\n        for s in sources:\n            s.shared_state = shared_state\n        if self.transforms is not None:\n            sources = [transform(source)\n                    for source, transform in zip(sources, self.transforms)]\n        if self.stacker is not None:\n            sources = self.stacker(sources)\n        else:\n            sources = tuple(sources)\n        return sources\n\n    def __len__(self):\n        return len(self.images)\n\ndef is_npy_file(path):\n    return path.endswith(\'.npy\') or path.endswith(\'.NPY\')\n\ndef is_image_file(path):\n    return None != re.search(r\'\\.(jpe?g|png)$\', path, re.IGNORECASE)\n\ndef walk_image_files(rootdir, verbose=None):\n    progress = default_progress(verbose)\n    indexfile = \'%s.txt\' % rootdir\n    if os.path.isfile(indexfile):\n        basedir = os.path.dirname(rootdir)\n        with open(indexfile) as f:\n            result = sorted([os.path.join(basedir, line.strip())\n                for line in progress(f.readlines(),\n                    desc=\'Reading %s\' % os.path.basename(indexfile))])\n            return result\n    result = []\n    for dirname, _, fnames in sorted(progress(os.walk(rootdir),\n            desc=\'Walking %s\' % os.path.basename(rootdir))):\n        for fname in sorted(fnames):\n            if is_image_file(fname) or is_npy_file(fname):\n                result.append(os.path.join(dirname, fname))\n    return result\n\ndef make_parallel_dataset(image_roots, intersection=False, verbose=None):\n    """"""\n    Returns [(img1, img2), (img1, img2)..]\n    """"""\n    image_roots = [os.path.expanduser(d) for d in image_roots]\n    image_sets = OrderedDict()\n    for j, root in enumerate(image_roots):\n        for path in walk_image_files(root, verbose=verbose):\n            key = os.path.splitext(os.path.relpath(path, root))[0]\n            if key not in image_sets:\n                image_sets[key] = []\n            if not intersection and len(image_sets[key]) != j:\n                raise RuntimeError(\n                    \'Images not parallel: %s missing from one dir\' % (key))\n            image_sets[key].append(path)\n    tuples = []\n    for key, value in image_sets.items():\n        if len(value) != len(image_roots):\n            if intersection:\n                continue\n            else:\n                raise RuntimeError(\n                    \'Images not parallel: %s missing from one dir\' % (key))\n        tuples.append(tuple(value))\n    return tuples\n'"
netdissect/pidfile.py,0,"b""'''\nUtility for simple distribution of work on multiple processes, by\nmaking sure only one process is working on a job at once.\n'''\n\nimport os, errno, socket, atexit, time, sys\n\ndef exit_if_job_done(directory):\n    if pidfile_taken(os.path.join(directory, 'lockfile.pid'), verbose=True):\n        sys.exit(0)\n    if os.path.isfile(os.path.join(directory, 'done.txt')):\n        with open(os.path.join(directory, 'done.txt')) as f:\n            msg = f.read()\n        print(msg)\n        sys.exit(0)\n\ndef mark_job_done(directory):\n    with open(os.path.join(directory, 'done.txt'), 'w') as f:\n        f.write('Done by %d@%s %s at %s' %\n                (os.getpid(), socket.gethostname(),\n                 os.getenv('STY', ''),\n                 time.strftime('%c')))\n\ndef pidfile_taken(path, verbose=False):\n    '''\n    Usage.  To grab an exclusive lock for the remaining duration of the\n    current process (and exit if another process already has the lock),\n    do this:\n\n    if pidfile_taken('job_423/lockfile.pid', verbose=True):\n        sys.exit(0)\n\n    To do a batch of jobs, just run a script that does them all on\n    each available machine, sharing a network filesystem.  When each\n    job grabs a lock, then this will automatically distribute the\n    jobs so that each one is done just once on one machine.\n    '''\n\n    # Try to create the file exclusively and write my pid into it.\n    try:\n        os.makedirs(os.path.dirname(path), exist_ok=True)\n        fd = os.open(path, os.O_CREAT | os.O_EXCL | os.O_RDWR)\n    except OSError as e:\n        if e.errno == errno.EEXIST:\n            # If we cannot because there was a race, yield the conflicter.\n            conflicter = 'race'\n            try:\n                with open(path, 'r') as lockfile:\n                    conflicter = lockfile.read().strip() or 'empty'\n            except:\n                pass\n            if verbose:\n                print('%s held by %s' % (path, conflicter))\n            return conflicter\n        else:\n            # Other problems get an exception.\n            raise\n    # Register to delete this file on exit.\n    lockfile = os.fdopen(fd, 'r+')\n    atexit.register(delete_pidfile, lockfile, path)\n    # Write my pid into the open file.\n    lockfile.write('%d@%s %s\\n' % (os.getpid(), socket.gethostname(),\n        os.getenv('STY', '')))\n    lockfile.flush()\n    os.fsync(lockfile)\n    # Return 'None' to say there was not a conflict.\n    return None\n\ndef delete_pidfile(lockfile, path):\n    '''\n    Runs at exit after pidfile_taken succeeds.\n    '''\n    if lockfile is not None:\n        try:\n            lockfile.close()\n        except:\n            pass\n    try:\n        os.unlink(path)\n    except:\n        pass\n"""
netdissect/plotutil.py,0,"b""import matplotlib.pyplot as plt\nimport numpy\n\ndef plot_tensor_images(data, **kwargs):\n    data = ((data + 1) / 2 * 255).permute(0, 2, 3, 1).byte().cpu().numpy()\n    width = int(numpy.ceil(numpy.sqrt(data.shape[0])))\n    height = int(numpy.ceil(data.shape[0] / float(width)))\n    kwargs = dict(kwargs)\n    margin = 0.01\n    if 'figsize' not in kwargs:\n        # Size figure to one display pixel per data pixel\n        dpi = plt.rcParams['figure.dpi']\n        kwargs['figsize'] = (\n                (1 + margin) * (width * data.shape[2] / dpi),\n                (1 + margin) * (height * data.shape[1] / dpi))\n    f, axarr = plt.subplots(height, width, **kwargs)\n    if len(numpy.shape(axarr)) == 0:\n        axarr = numpy.array([[axarr]])\n    if len(numpy.shape(axarr)) == 1:\n        axarr = axarr[None,:]\n    for i, im in enumerate(data):\n        ax = axarr[i // width, i % width]\n        ax.imshow(data[i])\n        ax.axis('off')\n    for i in range(i, width * height):\n        ax = axarr[i // width, i % width]\n        ax.axis('off')\n    plt.subplots_adjust(wspace=margin, hspace=margin,\n            left=0, right=1, bottom=0, top=1)\n    plt.show()\n\ndef plot_max_heatmap(data, shape=None, **kwargs):\n    if shape is None:\n        shape = data.shape[2:]\n    data = data.max(1)[0].cpu().numpy()\n    vmin = data.min()\n    vmax = data.max()\n    width = int(numpy.ceil(numpy.sqrt(data.shape[0])))\n    height = int(numpy.ceil(data.shape[0] / float(width)))\n    kwargs = dict(kwargs)\n    margin = 0.01\n    if 'figsize' not in kwargs:\n        # Size figure to one display pixel per data pixel\n        dpi = plt.rcParams['figure.dpi']\n        kwargs['figsize'] = (\n                width * shape[1] / dpi, height * shape[0] / dpi)\n    f, axarr = plt.subplots(height, width, **kwargs)\n    if len(numpy.shape(axarr)) == 0:\n        axarr = numpy.array([[axarr]])\n    if len(numpy.shape(axarr)) == 1:\n        axarr = axarr[None,:]\n    for i, im in enumerate(data):\n        ax = axarr[i // width, i % width]\n        img = ax.imshow(data[i], vmin=vmin, vmax=vmax, cmap='hot')\n        ax.axis('off')\n    for i in range(i, width * height):\n        ax = axarr[i // width, i % width]\n        ax.axis('off')\n    plt.subplots_adjust(wspace=margin, hspace=margin,\n            left=0, right=1, bottom=0, top=1)\n    plt.show()\n"""
netdissect/proggan.py,5,"b""import torch, numpy, itertools\nimport torch.nn as nn\nfrom collections import OrderedDict\n\n\ndef print_network(net, verbose=False):\n    num_params = 0\n    for param in net.parameters():\n        num_params += param.numel()\n    if verbose:\n        print(net)\n    print('Total number of parameters: {:3.3f} M'.format(num_params / 1e6))\n\n\ndef from_pth_file(filename):\n    '''\n    Instantiate from a pth file.\n    '''\n    state_dict = torch.load(filename)\n    if 'state_dict' in state_dict:\n        state_dict = state_dict['state_dict']\n    # Convert old version of parameter names\n    if 'features.0.conv.weight' in state_dict:\n        state_dict = state_dict_from_old_pt_dict(state_dict)\n    sizes = sizes_from_state_dict(state_dict)\n    result = ProgressiveGenerator(sizes=sizes)\n    result.load_state_dict(state_dict)\n    return result\n\n###############################################################################\n# Modules\n###############################################################################\n\nclass ProgressiveGenerator(nn.Sequential):\n    def __init__(self, resolution=None, sizes=None, modify_sequence=None,\n            output_tanh=False):\n        '''\n        A pytorch progessive GAN generator that can be converted directly\n        from either a tensorflow model or a theano model.  It consists of\n        a sequence of convolutional layers, organized in pairs, with an\n        upsampling and reduction of channels at every other layer; and\n        then finally followed by an output layer that reduces it to an\n        RGB [-1..1] image.\n\n        The network can be given more layers to increase the output\n        resolution.  The sizes argument indicates the fieature depth at\n        each upsampling, starting with the input z: [input-dim, 4x4-depth,\n        8x8-depth, 16x16-depth...].  The output dimension is 2 * 2**len(sizes)\n\n        Some default architectures can be selected by supplying the\n        resolution argument instead.\n\n        The optional modify_sequence function can be used to transform the\n        sequence of layers before the network is constructed.\n\n        If output_tanh is set to True, the network applies a tanh to clamp\n        the output to [-1,1] before output; otherwise the output is unclamped.\n        '''\n        assert (resolution is None) != (sizes is None)\n        if sizes is None:\n            sizes = {\n                    8: [512, 512, 512],\n                    16: [512, 512, 512, 512],\n                    32: [512, 512, 512, 512, 256],\n                    64: [512, 512, 512, 512, 256, 128],\n                    128: [512, 512, 512, 512, 256, 128, 64],\n                    256: [512, 512, 512, 512, 256, 128, 64, 32],\n                    1024: [512, 512, 512, 512, 512, 256, 128, 64, 32, 16]\n                }[resolution]\n        # Follow the schedule of upsampling given by sizes.\n        # layers are called: layer1, layer2, etc; then output_128x128\n        sequence = []\n        def add_d(layer, name=None):\n            if name is None:\n                name = 'layer%d' % (len(sequence) + 1)\n            sequence.append((name, layer))\n        add_d(NormConvBlock(sizes[0], sizes[1], kernel_size=4, padding=3))\n        add_d(NormConvBlock(sizes[1], sizes[1], kernel_size=3, padding=1))\n        for i, (si, so) in enumerate(zip(sizes[1:-1], sizes[2:])):\n            add_d(NormUpscaleConvBlock(si, so, kernel_size=3, padding=1))\n            add_d(NormConvBlock(so, so, kernel_size=3, padding=1))\n        # Create an output layer.  During training, the progressive GAN\n        # learns several such output layers for various resolutions; we\n        # just include the last (highest resolution) one.\n        dim = 4 * (2 ** (len(sequence) // 2 - 1))\n        add_d(OutputConvBlock(sizes[-1], tanh=output_tanh),\n                name='output_%dx%d' % (dim, dim))\n        # Allow the sequence to be modified\n        if modify_sequence is not None:\n            sequence = modify_sequence(sequence)\n        super().__init__(OrderedDict(sequence))\n\n    def forward(self, x):\n        # Convert vector input to 1x1 featuremap.\n        x = x.view(x.shape[0], x.shape[1], 1, 1)\n        return super().forward(x)\n\nclass PixelNormLayer(nn.Module):\n    def __init__(self):\n        super(PixelNormLayer, self).__init__()\n\n    def forward(self, x):\n        return x / torch.sqrt(torch.mean(x**2, dim=1, keepdim=True) + 1e-8)\n\nclass DoubleResolutionLayer(nn.Module):\n    def forward(self, x):\n        x = nn.functional.interpolate(x, scale_factor=2, mode='nearest')\n        return x\n\nclass WScaleLayer(nn.Module):\n    def __init__(self, size, fan_in, gain=numpy.sqrt(2)):\n        super(WScaleLayer, self).__init__()\n        self.scale = gain / numpy.sqrt(fan_in) # No longer a parameter\n        self.b = nn.Parameter(torch.randn(size))\n        self.size = size\n\n    def forward(self, x):\n        x_size = x.size()\n        x = x * self.scale + self.b.view(1, -1, 1, 1).expand(\n            x_size[0], self.size, x_size[2], x_size[3])\n        return x\n\nclass NormConvBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size, padding):\n        super(NormConvBlock, self).__init__()\n        self.norm = PixelNormLayer()\n        self.conv = nn.Conv2d(\n            in_channels, out_channels, kernel_size, 1, padding, bias=False)\n        self.wscale = WScaleLayer(out_channels, in_channels,\n                gain=numpy.sqrt(2) / kernel_size)\n        self.relu = nn.LeakyReLU(inplace=True, negative_slope=0.2)\n\n    def forward(self, x):\n        x = self.norm(x)\n        x = self.conv(x)\n        x = self.relu(self.wscale(x))\n        return x\n\nclass NormUpscaleConvBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size, padding):\n        super(NormUpscaleConvBlock, self).__init__()\n        self.norm = PixelNormLayer()\n        self.up = DoubleResolutionLayer()\n        self.conv = nn.Conv2d(\n            in_channels, out_channels, kernel_size, 1, padding, bias=False)\n        self.wscale = WScaleLayer(out_channels, in_channels,\n                gain=numpy.sqrt(2) / kernel_size)\n        self.relu = nn.LeakyReLU(inplace=True, negative_slope=0.2)\n\n    def forward(self, x):\n        x = self.norm(x)\n        x = self.up(x)\n        x = self.conv(x)\n        x = self.relu(self.wscale(x))\n        return x\n\nclass OutputConvBlock(nn.Module):\n    def __init__(self, in_channels, tanh=False):\n        super().__init__()\n        self.norm = PixelNormLayer()\n        self.conv = nn.Conv2d(\n                in_channels, 3, kernel_size=1, padding=0, bias=False)\n        self.wscale = WScaleLayer(3, in_channels, gain=1)\n        self.clamp = nn.Hardtanh() if tanh else (lambda x: x)\n\n    def forward(self, x):\n        x = self.norm(x)\n        x = self.conv(x)\n        x = self.wscale(x)\n        x = self.clamp(x)\n        return x\n\n###############################################################################\n# Conversion\n###############################################################################\n\ndef from_tf_parameters(parameters):\n    '''\n    Instantiate from tensorflow variables.\n    '''\n    state_dict = state_dict_from_tf_parameters(parameters)\n    sizes = sizes_from_state_dict(state_dict)\n    result = ProgressiveGenerator(sizes=sizes)\n    result.load_state_dict(state_dict)\n    return result\n\ndef from_old_pt_dict(parameters):\n    '''\n    Instantiate from old pytorch state dict.\n    '''\n    state_dict = state_dict_from_old_pt_dict(parameters)\n    sizes = sizes_from_state_dict(state_dict)\n    result = ProgressiveGenerator(sizes=sizes)\n    result.load_state_dict(state_dict)\n    return result\n\ndef sizes_from_state_dict(params):\n    '''\n    In a progressive GAN, the number of channels can change after each\n    upsampling.  This function reads the state dict to figure the\n    number of upsamplings and the channel depth of each filter.\n    '''\n    sizes = []\n    for i in itertools.count():\n        pt_layername = 'layer%d' % (i + 1)\n        try:\n            weight = params['%s.conv.weight' % pt_layername]\n        except KeyError:\n            break\n        if i == 0:\n            sizes.append(weight.shape[1])\n        if i % 2 == 0:\n            sizes.append(weight.shape[0])\n    return sizes\n\ndef state_dict_from_tf_parameters(parameters):\n    '''\n    Conversion from tensorflow parameters\n    '''\n    def torch_from_tf(data):\n        return torch.from_numpy(data.eval())\n\n    params = dict(parameters)\n    result = {}\n    sizes = []\n    for i in itertools.count():\n        resolution = 4 * (2 ** (i // 2))\n        # Translate parameter names.  For example:\n        # 4x4/Dense/weight -> layer1.conv.weight\n        # 32x32/Conv0_up/weight -> layer7.conv.weight\n        # 32x32/Conv1/weight -> layer8.conv.weight\n        tf_layername = '%dx%d/%s' % (resolution, resolution,\n                'Dense' if i == 0 else 'Conv' if i == 1 else\n                'Conv0_up' if i % 2 == 0 else 'Conv1')\n        pt_layername = 'layer%d' % (i + 1)\n        # Stop looping when we run out of parameters.\n        try:\n            weight = torch_from_tf(params['%s/weight' % tf_layername])\n        except KeyError:\n            break\n        # Transpose convolution weights into pytorch format.\n        if i == 0:\n            # Convert dense layer to 4x4 convolution\n            weight = weight.view(weight.shape[0], weight.shape[1] // 16,\n                   4, 4).permute(1, 0, 2, 3).flip(2, 3)\n            sizes.append(weight.shape[0])\n        elif i % 2 == 0:\n            # Convert inverse convolution to convolution\n            weight = weight.permute(2, 3, 0, 1).flip(2, 3)\n        else:\n            # Ordinary Conv2d conversion.\n            weight = weight.permute(3, 2, 0, 1)\n            sizes.append(weight.shape[1])\n        result['%s.conv.weight' % (pt_layername)] = weight\n        # Copy bias vector.\n        bias = torch_from_tf(params['%s/bias' % tf_layername])\n        result['%s.wscale.b' % (pt_layername)] = bias\n    # Copy just finest-grained ToRGB output layers.  For example:\n    # ToRGB_lod0/weight -> output.conv.weight\n    i -= 1\n    resolution = 4 * (2 ** (i // 2))\n    tf_layername = 'ToRGB_lod0'\n    pt_layername = 'output_%dx%d' % (resolution, resolution)\n    result['%s.conv.weight' % pt_layername] = torch_from_tf(\n            params['%s/weight' % tf_layername]).permute(3, 2, 0, 1)\n    result['%s.wscale.b' % pt_layername] = torch_from_tf(\n            params['%s/bias' % tf_layername])\n    # Return parameters\n    return result\n\ndef state_dict_from_old_pt_dict(params):\n    '''\n    Conversion from the old pytorch model layer names.\n    '''\n    result = {}\n    sizes = []\n    for i in itertools.count():\n        old_layername = 'features.%d' % i\n        pt_layername = 'layer%d' % (i + 1)\n        try:\n            weight = params['%s.conv.weight' % (old_layername)]\n        except KeyError:\n            break\n        if i == 0:\n            sizes.append(weight.shape[0])\n        if i % 2 == 0:\n            sizes.append(weight.shape[1])\n        result['%s.conv.weight' % (pt_layername)] = weight\n        result['%s.wscale.b' % (pt_layername)] = params[\n                '%s.wscale.b' % (old_layername)]\n    # Copy the output layers.\n    i -= 1\n    resolution = 4 * (2 ** (i // 2))\n    pt_layername = 'output_%dx%d' % (resolution, resolution)\n    result['%s.conv.weight' % pt_layername] = params['output.conv.weight']\n    result['%s.wscale.b' % pt_layername] = params['output.wscale.b']\n    # Return parameters and also network architecture sizes.\n    return result\n\n"""
netdissect/progress.py,0,"b""'''\nUtilities for showing progress bars, controlling default verbosity, etc.\n'''\n\n# If the tqdm package is not available, then do not show progress bars;\n# just connect print_progress to print.\ntry:\n    from tqdm import tqdm, tqdm_notebook\nexcept:\n    tqdm = None\n\ndefault_verbosity = False\n\ndef verbose_progress(verbose):\n    '''\n    Sets default verbosity level.  Set to True to see progress bars.\n    '''\n    global default_verbosity\n    default_verbosity = verbose\n\ndef tqdm_terminal(it, *args, **kwargs):\n    '''\n    Some settings for tqdm that make it run better in resizable terminals.\n    '''\n    return tqdm(it, *args, dynamic_ncols=True, ascii=True,\n            leave=(not nested_tqdm()), **kwargs)\n\ndef in_notebook():\n    '''\n    True if running inside a Jupyter notebook.\n    '''\n    # From https://stackoverflow.com/a/39662359/265298\n    try:\n        shell = get_ipython().__class__.__name__\n        if shell == 'ZMQInteractiveShell':\n            return True   # Jupyter notebook or qtconsole\n        elif shell == 'TerminalInteractiveShell':\n            return False  # Terminal running IPython\n        else:\n            return False  # Other type (?)\n    except NameError:\n        return False      # Probably standard Python interpreter\n\ndef nested_tqdm():\n    '''\n    True if there is an active tqdm progress loop on the stack.\n    '''\n    return hasattr(tqdm, '_instances') and len(tqdm._instances) > 0\n\ndef post_progress(**kwargs):\n    '''\n    When within a progress loop, post_progress(k=str) will display\n    the given k=str status on the right-hand-side of the progress\n    status bar.  If not within a visible progress bar, does nothing.\n    '''\n    if nested_tqdm():\n        innermost = max(tqdm._instances, key=lambda x: x.pos)\n        innermost.set_postfix(**kwargs)\n\ndef desc_progress(desc):\n    '''\n    When within a progress loop, desc_progress(str) changes the\n    left-hand-side description of the loop toe the given description.\n    '''\n    if nested_tqdm():\n        innermost = max(tqdm._instances, key=lambda x: x.pos)\n        innermost.set_description(desc)\n\ndef print_progress(*args):\n    '''\n    When within a progress loop, post_progress(k=str) will display\n    the given k=str status on the right-hand-side of the progress\n    status bar.  If not within a visible progress bar, does nothing.\n    '''\n    if default_verbosity:\n        printfn = print if tqdm is None else tqdm.write\n        printfn(' '.join(str(s) for s in args))\n\ndef default_progress(verbose=None, iftop=False):\n    '''\n    Returns a progress function that can wrap iterators to print\n    progress messages, if verbose is True.\n   \n    If verbose is False or if iftop is True and there is already\n    a top-level tqdm loop being reported, then a quiet non-printing\n    identity function is returned.\n\n    verbose can also be set to a spefific progress function rather\n    than True, and that function will be used.\n    '''\n    global default_verbosity\n    if verbose is None:\n        verbose = default_verbosity\n    if not verbose or (iftop and nested_tqdm()) or tqdm is None:\n        return lambda x, *args, **kw: x\n    if verbose == True:\n        return tqdm_notebook if in_notebook() else tqdm_terminal\n    return verbose\n"""
netdissect/runningstats.py,69,"b'\'\'\'\nRunning statistics on the GPU using pytorch.\n\nRunningTopK maintains top-k statistics for a set of channels in parallel.\nRunningQuantile maintains (sampled) quantile statistics for a set of channels.\n\'\'\'\n\nimport torch, math, numpy\nfrom collections import defaultdict\n\nclass RunningTopK:\n    \'\'\'\n    A class to keep a running tally of the the top k values (and indexes)\n    of any number of torch feature components.  Will work on the GPU if\n    the data is on the GPU.\n\n    This version flattens all arrays to avoid crashes.\n    \'\'\'\n    def __init__(self, k=100, state=None):\n        if state is not None:\n            self.set_state_dict(state)\n            return\n        self.k = k\n        self.count = 0\n        # This version flattens all data internally to 2-d tensors,\n        # to avoid crashes with the current pytorch topk implementation.\n        # The data is puffed back out to arbitrary tensor shapes on ouput.\n        self.data_shape = None\n        self.top_data = None\n        self.top_index = None\n        self.next = 0\n        self.linear_index = 0\n        self.perm = None\n\n    def add(self, data):\n        \'\'\'\n        Adds a batch of data to be considered for the running top k.\n        The zeroth dimension enumerates the observations.  All other\n        dimensions enumerate different features.\n        \'\'\'\n        if self.top_data is None:\n            # Allocation: allocate a buffer of size 5*k, at least 10, for each.\n            self.data_shape = data.shape[1:]\n            feature_size = int(numpy.prod(self.data_shape))\n            self.top_data = torch.zeros(\n                    feature_size, max(10, self.k * 5), out=data.new())\n            self.top_index = self.top_data.clone().long()\n            self.linear_index = 0 if len(data.shape) == 1 else torch.arange(\n                feature_size, out=self.top_index.new()).mul_(\n                        self.top_data.shape[-1])[:,None]\n        size = data.shape[0]\n        sk = min(size, self.k)\n        if self.top_data.shape[-1] < self.next + sk:\n            # Compression: if full, keep topk only.\n            self.top_data[:,:self.k], self.top_index[:,:self.k] = (\n                    self.result(sorted=False, flat=True))\n            self.next = self.k\n            free = self.top_data.shape[-1] - self.next\n        # Pick: copy the top sk of the next batch into the buffer.\n        # Currently strided topk is slow.  So we clone after transpose.\n        # TODO: remove the clone() if it becomes faster.\n        cdata = data.contiguous().view(size, -1).t().clone()\n        td, ti = cdata.topk(sk, sorted=False)\n        self.top_data[:,self.next:self.next+sk] = td\n        self.top_index[:,self.next:self.next+sk] = (ti + self.count)\n        self.next += sk\n        self.count += size\n\n    def result(self, sorted=True, flat=False):\n        \'\'\'\n        Returns top k data items and indexes in each dimension,\n        with channels in the first dimension and k in the last dimension.\n        \'\'\'\n        k = min(self.k, self.next)\n        # bti are top indexes relative to buffer array.\n        td, bti = self.top_data[:,:self.next].topk(k, sorted=sorted)\n        # we want to report top indexes globally, which is ti.\n        ti = self.top_index.view(-1)[\n                (bti + self.linear_index).view(-1)\n                ].view(*bti.shape)\n        if flat:\n            return td, ti\n        else:\n            return (td.view(*(self.data_shape + (-1,))),\n                    ti.view(*(self.data_shape + (-1,))))\n\n    def to_(self, device):\n        self.top_data = self.top_data.to(device)\n        self.top_index = self.top_index.to(device)\n        if isinstance(self.linear_index, torch.Tensor):\n            self.linear_index = self.linear_index.to(device)\n\n    def state_dict(self):\n        return dict(\n                constructor=self.__module__ + \'.\' +\n                    self.__class__.__name__ + \'()\',\n                k=self.k,\n                count=self.count,\n                data_shape=tuple(self.data_shape),\n                top_data=self.top_data.cpu().numpy(),\n                top_index=self.top_index.cpu().numpy(),\n                next=self.next,\n                linear_index=(self.linear_index.cpu().numpy()\n                    if isinstance(self.linear_index, torch.Tensor)\n                    else self.linear_index),\n                perm=self.perm)\n\n    def set_state_dict(self, dic):\n        self.k = dic[\'k\'].item()\n        self.count = dic[\'count\'].item()\n        self.data_shape = tuple(dic[\'data_shape\'])\n        self.top_data = torch.from_numpy(dic[\'top_data\'])\n        self.top_index = torch.from_numpy(dic[\'top_index\'])\n        self.next = dic[\'next\'].item()\n        self.linear_index = (torch.from_numpy(dic[\'linear_index\'])\n                if len(dic[\'linear_index\'].shape) > 0\n                else dic[\'linear_index\'].item())\n\nclass RunningQuantile:\n    """"""\n    Streaming randomized quantile computation for torch.\n\n    Add any amount of data repeatedly via add(data).  At any time,\n    quantile estimates (or old-style percentiles) can be read out using\n    quantiles(q) or percentiles(p).\n\n    Accuracy scales according to resolution: the default is to\n    set resolution to be accurate to better than 0.1%,\n    while limiting storage to about 50,000 samples.\n\n    Good for computing quantiles of huge data without using much memory.\n    Works well on arbitrary data with probability near 1.\n\n    Based on the optimal KLL quantile algorithm by Karnin, Lang, and Liberty\n    from FOCS 2016.  http://ieee-focs.org/FOCS-2016-Papers/3933a071.pdf\n    """"""\n\n    def __init__(self, resolution=6 * 1024, buffersize=None, seed=None,\n            state=None):\n        if state is not None:\n            self.set_state_dict(state)\n            return\n        self.depth = None\n        self.dtype = None\n        self.device = None\n        self.resolution = resolution\n        # Default buffersize: 128 samples (and smaller than resolution).\n        if buffersize is None:\n            buffersize = min(128, (resolution + 7) // 8)\n        self.buffersize = buffersize\n        self.samplerate = 1.0\n        self.data = None\n        self.firstfree = [0]\n        self.randbits = torch.ByteTensor(resolution)\n        self.currentbit = len(self.randbits) - 1\n        self.extremes = None\n        self.size = 0\n\n    def _lazy_init(self, incoming):\n        self.depth = incoming.shape[1]\n        self.dtype = incoming.dtype\n        self.device = incoming.device\n        self.data = [torch.zeros(self.depth, self.resolution,\n            dtype=self.dtype, device=self.device)]\n        self.extremes = torch.zeros(self.depth, 2,\n                dtype=self.dtype, device=self.device)\n        self.extremes[:,0] = float(\'inf\')\n        self.extremes[:,-1] = -float(\'inf\')\n\n    def to_(self, device):\n        """"""Switches internal storage to specified device.""""""\n        if device != self.device:\n            old_data = self.data\n            old_extremes = self.extremes\n            self.data = [d.to(device) for d in self.data]\n            self.extremes = self.extremes.to(device)\n            self.device = self.extremes.device\n            del old_data\n            del old_extremes\n\n    def add(self, incoming):\n        if self.depth is None:\n            self._lazy_init(incoming)\n        assert len(incoming.shape) == 2\n        assert incoming.shape[1] == self.depth, (incoming.shape[1], self.depth)\n        self.size += incoming.shape[0]\n        # Convert to a flat torch array.\n        if self.samplerate >= 1.0:\n            self._add_every(incoming)\n            return\n        # If we are sampling, then subsample a large chunk at a time.\n        self._scan_extremes(incoming)\n        chunksize = int(math.ceil(self.buffersize / self.samplerate))\n        for index in range(0, len(incoming), chunksize):\n            batch = incoming[index:index+chunksize]\n            sample = sample_portion(batch, self.samplerate)\n            if len(sample):\n                self._add_every(sample)\n\n    def _add_every(self, incoming):\n        supplied = len(incoming)\n        index = 0\n        while index < supplied:\n            ff = self.firstfree[0]\n            available = self.data[0].shape[1] - ff\n            if available == 0:\n                if not self._shift():\n                    # If we shifted by subsampling, then subsample.\n                    incoming = incoming[index:]\n                    if self.samplerate >= 0.5:\n                        # First time sampling - the data source is very large.\n                        self._scan_extremes(incoming)\n                    incoming = sample_portion(incoming, self.samplerate)\n                    index = 0\n                    supplied = len(incoming)\n                ff = self.firstfree[0]\n                available = self.data[0].shape[1] - ff\n            copycount = min(available, supplied - index)\n            self.data[0][:,ff:ff + copycount] = torch.t(\n                    incoming[index:index + copycount,:])\n            self.firstfree[0] += copycount\n            index += copycount\n\n    def _shift(self):\n        index = 0\n        # If remaining space at the current layer is less than half prev\n        # buffer size (rounding up), then we need to shift it up to ensure\n        # enough space for future shifting.\n        while self.data[index].shape[1] - self.firstfree[index] < (\n                -(-self.data[index-1].shape[1] // 2) if index else 1):\n            if index + 1 >= len(self.data):\n                return self._expand()\n            data = self.data[index][:,0:self.firstfree[index]]\n            data = data.sort()[0]\n            if index == 0 and self.samplerate >= 1.0:\n                self._update_extremes(data[:,0], data[:,-1])\n            offset = self._randbit()\n            position = self.firstfree[index + 1]\n            subset = data[:,offset::2]\n            self.data[index + 1][:,position:position + subset.shape[1]] = subset\n            self.firstfree[index] = 0\n            self.firstfree[index + 1] += subset.shape[1]\n            index += 1\n        return True\n\n    def _scan_extremes(self, incoming):\n        # When sampling, we need to scan every item still to get extremes\n        self._update_extremes(\n                torch.min(incoming, dim=0)[0],\n                torch.max(incoming, dim=0)[0])\n\n    def _update_extremes(self, minr, maxr):\n        self.extremes[:,0] = torch.min(\n                torch.stack([self.extremes[:,0], minr]), dim=0)[0]\n        self.extremes[:,-1] = torch.max(\n                torch.stack([self.extremes[:,-1], maxr]), dim=0)[0]\n\n    def _randbit(self):\n        self.currentbit += 1\n        if self.currentbit >= len(self.randbits):\n            self.randbits.random_(to=2)\n            self.currentbit = 0\n        return self.randbits[self.currentbit]\n\n    def state_dict(self):\n        return dict(\n                constructor=self.__module__ + \'.\' +\n                    self.__class__.__name__ + \'()\',\n                resolution=self.resolution,\n                depth=self.depth,\n                buffersize=self.buffersize,\n                samplerate=self.samplerate,\n                data=[d.cpu().numpy()[:,:f].T\n                    for d, f in zip(self.data, self.firstfree)],\n                sizes=[d.shape[1] for d in self.data],\n                extremes=self.extremes.cpu().numpy(),\n                size=self.size)\n\n    def set_state_dict(self, dic):\n        self.resolution = int(dic[\'resolution\'])\n        self.randbits = torch.ByteTensor(self.resolution)\n        self.currentbit = len(self.randbits) - 1\n        self.depth = int(dic[\'depth\'])\n        self.buffersize = int(dic[\'buffersize\'])\n        self.samplerate = float(dic[\'samplerate\'])\n        firstfree = []\n        buffers = []\n        for d, s in zip(dic[\'data\'], dic[\'sizes\']):\n            firstfree.append(d.shape[0])\n            buf = numpy.zeros((d.shape[1], s), dtype=d.dtype)\n            buf[:,:d.shape[0]] = d.T\n            buffers.append(torch.from_numpy(buf))\n        self.firstfree = firstfree\n        self.data = buffers\n        self.extremes = torch.from_numpy((dic[\'extremes\']))\n        self.size = int(dic[\'size\'])\n        self.dtype = self.extremes.dtype\n        self.device = self.extremes.device\n\n    def minmax(self):\n        if self.firstfree[0]:\n            self._scan_extremes(self.data[0][:,:self.firstfree[0]].t())\n        return self.extremes.clone()\n\n    def median(self):\n        return self.quantiles([0.5])[:,0]\n\n    def mean(self):\n        return self.integrate(lambda x: x) / self.size\n\n    def variance(self):\n        mean = self.mean()[:,None]\n        return self.integrate(lambda x: (x - mean).pow(2)) / (self.size - 1)\n\n    def stdev(self):\n        return self.variance().sqrt()\n\n    def _expand(self):\n        cap = self._next_capacity()\n        if cap > 0:\n            # First, make a new layer of the proper capacity.\n            self.data.insert(0, torch.zeros(self.depth, cap,\n                dtype=self.dtype, device=self.device))\n            self.firstfree.insert(0, 0)\n        else:\n            # Unless we\'re so big we are just subsampling.\n            assert self.firstfree[0] == 0\n            self.samplerate *= 0.5\n        for index in range(1, len(self.data)):\n            # Scan for existing data that needs to be moved down a level.\n            amount = self.firstfree[index]\n            if amount == 0:\n                continue\n            position = self.firstfree[index-1]\n            # Move data down if it would leave enough empty space there\n            # This is the key invariant: enough empty space to fit half\n            # of the previous level\'s buffer size (rounding up)\n            if self.data[index-1].shape[1] - (amount + position) >= (\n                    -(-self.data[index-2].shape[1] // 2) if (index-1) else 1):\n                self.data[index-1][:,position:position + amount] = (\n                        self.data[index][:,:amount])\n                self.firstfree[index-1] += amount\n                self.firstfree[index] = 0\n            else:\n                # Scrunch the data if it would not.\n                data = self.data[index][:,:amount]\n                data = data.sort()[0]\n                if index == 1:\n                    self._update_extremes(data[:,0], data[:,-1])\n                offset = self._randbit()\n                scrunched = data[:,offset::2]\n                self.data[index][:,:scrunched.shape[1]] = scrunched\n                self.firstfree[index] = scrunched.shape[1]\n        return cap > 0\n\n    def _next_capacity(self):\n        cap = int(math.ceil(self.resolution * (0.67 ** len(self.data))))\n        if cap < 2:\n            return 0\n        # Round up to the nearest multiple of 8 for better GPU alignment.\n        cap = -8 * (-cap // 8)\n        return max(self.buffersize, cap)\n\n    def _weighted_summary(self, sort=True):\n        if self.firstfree[0]:\n            self._scan_extremes(self.data[0][:,:self.firstfree[0]].t())\n        size = sum(self.firstfree) + 2\n        weights = torch.FloatTensor(size) # Floating point\n        summary = torch.zeros(self.depth, size,\n                dtype=self.dtype, device=self.device)\n        weights[0:2] = 0\n        summary[:,0:2] = self.extremes\n        index = 2\n        for level, ff in enumerate(self.firstfree):\n            if ff == 0:\n                continue\n            summary[:,index:index + ff] = self.data[level][:,:ff]\n            weights[index:index + ff] = 2.0 ** level\n            index += ff\n        assert index == summary.shape[1]\n        if sort:\n            summary, order = torch.sort(summary, dim=-1)\n            weights = weights[order.view(-1).cpu()].view(order.shape)\n        return (summary, weights)\n\n    def quantiles(self, quantiles, old_style=False):\n        if self.size == 0:\n            return torch.full((self.depth, len(quantiles)), torch.nan)\n        summary, weights = self._weighted_summary()\n        cumweights = torch.cumsum(weights, dim=-1) - weights / 2\n        if old_style:\n            # To be convenient with torch.percentile\n            cumweights -= cumweights[:,0:1].clone()\n            cumweights /= cumweights[:,-1:].clone()\n        else:\n            cumweights /= torch.sum(weights, dim=-1, keepdim=True)\n        result = torch.zeros(self.depth, len(quantiles),\n                dtype=self.dtype, device=self.device)\n        # numpy is needed for interpolation\n        if not hasattr(quantiles, \'cpu\'):\n            quantiles = torch.Tensor(quantiles)\n        nq = quantiles.cpu().numpy()\n        ncw = cumweights.cpu().numpy()\n        nsm = summary.cpu().numpy()\n        for d in range(self.depth):\n            result[d] = torch.tensor(numpy.interp(nq, ncw[d], nsm[d]),\n                    dtype=self.dtype, device=self.device)\n        return result\n\n    def integrate(self, fun):\n        result = None\n        for level, ff in enumerate(self.firstfree):\n            if ff == 0:\n                continue\n            term = torch.sum(\n                    fun(self.data[level][:,:ff]) * (2.0 ** level),\n                    dim=-1)\n            if result is None:\n                result = term\n            else:\n                result += term\n        if result is not None:\n            result /= self.samplerate\n        return result\n\n    def percentiles(self, percentiles):\n        return self.quantiles(percentiles, old_style=True)\n\n    def readout(self, count=1001, old_style=True):\n        return self.quantiles(\n                torch.linspace(0.0, 1.0, count), old_style=old_style)\n\n    def normalize(self, data):\n        \'\'\'\n        Given input data as taken from the training distirbution,\n        normalizes every channel to reflect quantile values,\n        uniformly distributed, within [0, 1].\n        \'\'\'\n        assert self.size > 0\n        assert data.shape[0] == self.depth\n        summary, weights = self._weighted_summary()\n        cumweights = torch.cumsum(weights, dim=-1) - weights / 2\n        cumweights /= torch.sum(weights, dim=-1, keepdim=True)\n        result = torch.zeros_like(data).float()\n        # numpy is needed for interpolation\n        ndata = data.cpu().numpy().reshape((data.shape[0], -1))\n        ncw = cumweights.cpu().numpy()\n        nsm = summary.cpu().numpy()\n        for d in range(self.depth):\n            normed = torch.tensor(numpy.interp(ndata[d], nsm[d], ncw[d]),\n                dtype=torch.float, device=data.device).clamp_(0.0, 1.0)\n            if len(data.shape) > 1:\n                normed = normed.view(*(data.shape[1:]))\n            result[d] = normed\n        return result\n\n\nclass RunningConditionalQuantile:\n    \'\'\'\n    Equivalent to a map from conditions (any python hashable type)\n    to RunningQuantiles.  The reason for the type is to allow limited\n    GPU memory to be exploited while counting quantile stats on many\n    different conditions, a few of which are common and which benefit\n    from GPU, but most of which are rare and would not all fit into\n    GPU RAM.\n\n    To move a set of conditions to a device, use rcq.to_(device, conds).\n    Then in the future, move the tallied data to the device before\n    calling rcq.add, that is, rcq.add(cond, data.to(device)).\n\n    To allow the caller to decide which conditions to allow to use GPU,\n    rcq.most_common_conditions(n) returns a list of the n most commonly\n    added conditions so far.\n    \'\'\'\n    def __init__(self, resolution=6 * 1024, buffersize=None, seed=None,\n            state=None):\n        self.first_rq = None\n        self.call_stats = defaultdict(int)\n        self.running_quantiles = {}\n        if state is not None:\n            self.set_state_dict(state)\n            return\n        self.rq_args = dict(resolution=resolution, buffersize=buffersize,\n                seed=seed)\n\n    def add(self, condition, incoming):\n        if condition not in self.running_quantiles:\n            self.running_quantiles[condition] = RunningQuantile(**self.rq_args)\n            if self.first_rq is None:\n                self.first_rq = self.running_quantiles[condition]\n        self.call_stats[condition] += 1\n        rq = self.running_quantiles[condition]\n        # For performance reasons, the caller can move some conditions to\n        # the CPU if they are not among the most common conditions.\n        if rq.device is not None and (rq.device != incoming.device):\n            rq.to_(incoming.device)\n        self.running_quantiles[condition].add(incoming)\n\n    def most_common_conditions(self, n):\n        return sorted(self.call_stats.keys(),\n                key=lambda c: -self.call_stats[c])[:n]\n\n    def collected_add(self, conditions, incoming):\n        for c in conditions:\n            self.add(c, incoming)\n\n    def conditional(self, c):\n        return self.running_quantiles[c]\n\n    def collected_quantiles(self, conditions, quantiles, old_style=False):\n        result = torch.zeros(\n                size=(len(conditions), self.first_rq.depth, len(quantiles)),\n                dtype=self.first_rq.dtype,\n                device=self.first_rq.device)\n        for i, c in enumerate(conditions):\n            if c in self.running_quantiles:\n                result[i] = self.running_quantiles[c].quantiles(\n                        quantiles, old_style)\n        return result\n\n    def collected_normalize(self, conditions, values):\n        result = torch.zeros(\n                size=(len(conditions), values.shape[0], values.shape[1]),\n                dtype=torch.float,\n                device=self.first_rq.device)\n        for i, c in enumerate(conditions):\n            if c in self.running_quantiles:\n                result[i] = self.running_quantiles[c].normalize(values)\n        return result\n\n    def to_(self, device, conditions=None):\n        if conditions is None:\n            conditions = self.running_quantiles.keys()\n        for cond in conditions:\n            if cond in self.running_quantiles:\n                self.running_quantiles[cond].to_(device)\n\n    def state_dict(self):\n        conditions = sorted(self.running_quantiles.keys())\n        result = dict(\n                constructor=self.__module__ + \'.\' +\n                    self.__class__.__name__ + \'()\',\n                rq_args=self.rq_args,\n                conditions=conditions)\n        for i, c in enumerate(conditions):\n            result.update({\n                \'%d.%s\' % (i, k): v\n                for k, v in self.running_quantiles[c].state_dict().items()})\n        return result\n\n    def set_state_dict(self, dic):\n        self.rq_args = dic[\'rq_args\'].item()\n        conditions = list(dic[\'conditions\'])\n        subdicts = defaultdict(dict)\n        for k, v in dic.items():\n            if \'.\' in k:\n                p, s = k.split(\'.\', 1)\n                subdicts[p][s] = v\n        self.running_quantiles = {\n                c: RunningQuantile(state=subdicts[str(i)])\n                for i, c in enumerate(conditions)}\n        if conditions:\n            self.first_rq = self.running_quantiles[conditions[0]]\n\n    # example usage:\n    # levels = rqc.conditional(()).quantiles(1 - fracs)\n    # denoms = 1 - rqc.collected_normalize(cats, levels)\n    # isects = 1 - rqc.collected_normalize(labels, levels)\n    # unions = fracs + denoms[cats] - isects\n    # iou = isects / unions\n\n\n\n\nclass RunningCrossCovariance:\n    \'\'\'\n    Running computation. Use this when an off-diagonal block of the\n    covariance matrix is needed (e.g., when the whole covariance matrix\n    does not fit in the GPU).\n\n    Chan-style numerically stable update of mean and full covariance matrix.\n    Chan, Golub. LeVeque. 1983. http://www.jstor.org/stable/2683386\n    \'\'\'\n    def __init__(self, state=None):\n        if state is not None:\n            self.set_state_dict(state)\n            return\n        self.count = 0\n        self._mean = None\n        self.cmom2 = None\n        self.v_cmom2 = None\n\n    def add(self, a, b):\n        if len(a.shape) == 1:\n            a = a[None, :]\n            b = b[None, :]\n        assert(a.shape[0] == b.shape[0])\n        if len(a.shape) > 2:\n            a, b = [d.view(d.shape[0], d.shape[1], -1).permute(0, 2, 1\n                ).contiguous().view(-1, d.shape[1]) for d in [a, b]]\n        batch_count = a.shape[0]\n        batch_mean = [d.sum(0) / batch_count for d in [a, b]]\n        centered = [d - bm for d, bm in zip([a, b], batch_mean)]\n        # If more than 10 billion operations, divide into batches.\n        sub_batch = -(-(10 << 30) // (a.shape[1] * b.shape[1]))\n        # Initial batch.\n        if self._mean is None:\n            self.count = batch_count\n            self._mean = batch_mean\n            self.v_cmom2 = [c.pow(2).sum(0) for c in centered]\n            self.cmom2 = a.new(a.shape[1], b.shape[1]).zero_()\n            progress_addbmm(self.cmom2, centered[0][:,:,None],\n                    centered[1][:,None,:], sub_batch)\n            return\n        # Update a batch using Chan-style update for numerical stability.\n        oldcount = self.count\n        self.count += batch_count\n        new_frac = float(batch_count) / self.count\n        # Update the mean according to the batch deviation from the old mean.\n        delta = [bm.sub_(m).mul_(new_frac)\n                for bm, m in zip(batch_mean, self._mean)]\n        for m, d in zip(self._mean, delta):\n            m.add_(d)\n        # Update the cross-covariance using the batch deviation\n        progress_addbmm(self.cmom2, centered[0][:,:,None],\n                centered[1][:,None,:], sub_batch)\n        self.cmom2.addmm_(alpha=new_frac * oldcount,\n                mat1=delta[0][:,None], mat2=delta[1][None,:])\n        # Update the variance using the batch deviation\n        for c, vc2, d in zip(centered, self.v_cmom2, delta):\n            vc2.add_(c.pow(2).sum(0))\n            vc2.add_(d.pow_(2).mul_(new_frac * oldcount))\n\n    def mean(self):\n        return self._mean\n\n    def variance(self):\n        return [vc2 / (self.count - 1) for vc2 in self.v_cmom2]\n\n    def stdev(self):\n        return [v.sqrt() for v in self.variance()]\n\n    def covariance(self):\n        return self.cmom2 / (self.count - 1)\n\n    def correlation(self):\n        covariance = self.covariance()\n        rstdev = [s.reciprocal() for s in self.stdev()]\n        cor = rstdev[0][:,None] * covariance * rstdev[1][None,:]\n        # Remove NaNs\n        cor[torch.isnan(cor)] = 0\n        return cor\n\n    def to_(self, device):\n        self._mean = [m.to(device) for m in self._mean]\n        self.v_cmom2 = [vcs.to(device) for vcs in self.v_cmom2]\n        self.cmom2 = self.cmom2.to(device)\n\n    def state_dict(self):\n        return dict(\n                constructor=self.__module__ + \'.\' +\n                    self.__class__.__name__ + \'()\',\n                count=self.count,\n                mean_a=self._mean[0].cpu().numpy(),\n                mean_b=self._mean[1].cpu().numpy(),\n                cmom2_a=self.v_cmom2[0].cpu().numpy(),\n                cmom2_b=self.v_cmom2[1].cpu().numpy(),\n                cmom2=self.cmom2.cpu().numpy())\n\n    def set_state_dict(self, dic):\n        self.count = dic[\'count\'].item()\n        self._mean = [torch.from_numpy(dic[k]) for k in [\'mean_a\', \'mean_b\']]\n        self.v_cmom2 = [torch.from_numpy(dic[k])\n                for k in [\'cmom2_a\', \'cmom2_b\']]\n        self.cmom2 = torch.from_numpy(dic[\'cmom2\'])\n\ndef progress_addbmm(accum, x, y, batch_size):\n    \'\'\'\n    Break up very large adbmm operations into batches so progress can be seen.\n    \'\'\'\n    from .progress import default_progress\n    if x.shape[0] <= batch_size:\n        return accum.addbmm_(x, y)\n    progress = default_progress(None)\n    for i in progress(range(0, x.shape[0], batch_size), desc=\'bmm\'):\n        accum.addbmm_(x[i:i+batch_size], y[i:i+batch_size])\n    return accum\n\n\ndef sample_portion(vec, p=0.5):\n    bits = torch.bernoulli(torch.zeros(vec.shape[0], dtype=torch.uint8,\n        device=vec.device), p)\n    return vec[bits]\n\nif __name__ == \'__main__\':\n    import warnings\n    warnings.filterwarnings(""error"")\n    import time\n    import argparse\n    parser = argparse.ArgumentParser(\n        description=\'Test things out\')\n    parser.add_argument(\'--mode\', default=\'cpu\', help=\'cpu or cuda\')\n    parser.add_argument(\'--test_size\', type=int, default=1000000)\n    args = parser.parse_args()\n\n    # An adverarial case: we keep finding more numbers in the middle\n    # as the stream goes on.\n    amount = args.test_size\n    quantiles = 1000\n    data = numpy.arange(float(amount))\n    data[1::2] = data[-1::-2] + (len(data) - 1)\n    data /= 2\n    depth = 50\n    test_cuda = torch.cuda.is_available()\n    alldata = data[:,None] + (numpy.arange(depth) * amount)[None, :]\n    actual_sum = torch.FloatTensor(numpy.sum(alldata * alldata, axis=0))\n    amt = amount // depth\n    for r in range(depth):\n        numpy.random.shuffle(alldata[r*amt:r*amt+amt,r])\n    if args.mode == \'cuda\':\n        alldata = torch.cuda.FloatTensor(alldata)\n        dtype = torch.float\n        device = torch.device(\'cuda\')\n    else:\n        alldata = torch.FloatTensor(alldata)\n        dtype = torch.float\n        device = None\n    starttime = time.time()\n    qc = RunningQuantile(resolution=6 * 1024)\n    qc.add(alldata)\n    # Test state dict\n    saved = qc.state_dict()\n    # numpy.savez(\'foo.npz\', **saved)\n    # saved = numpy.load(\'foo.npz\')\n    qc = RunningQuantile(state=saved)\n    assert not qc.device.type == \'cuda\'\n    qc.add(alldata)\n    actual_sum *= 2\n    ro = qc.readout(1001).cpu()\n    endtime = time.time()\n    gt = torch.linspace(0, amount, quantiles+1)[None,:] + (\n            torch.arange(qc.depth, dtype=torch.float) * amount)[:,None]\n    maxreldev = torch.max(torch.abs(ro - gt) / amount) * quantiles\n    print(""Maximum relative deviation among %d perentiles: %f"" % (\n        quantiles, maxreldev))\n    minerr = torch.max(torch.abs(qc.minmax().cpu()[:,0] -\n            torch.arange(qc.depth, dtype=torch.float) * amount))\n    maxerr = torch.max(torch.abs((qc.minmax().cpu()[:, -1] + 1) -\n            (torch.arange(qc.depth, dtype=torch.float) + 1) * amount))\n    print(""Minmax error %f, %f"" % (minerr, maxerr))\n    interr = torch.max(torch.abs(qc.integrate(lambda x: x * x).cpu()\n            - actual_sum) / actual_sum)\n    print(""Integral error: %f"" % interr)\n    medianerr = torch.max(torch.abs(qc.median() -\n        alldata.median(0)[0]) / alldata.median(0)[0]).cpu()\n    print(""Median error: %f"" % interr)\n    meanerr = torch.max(\n            torch.abs(qc.mean() - alldata.mean(0)) / alldata.mean(0)).cpu()\n    print(""Mean error: %f"" % meanerr)\n    varerr = torch.max(\n            torch.abs(qc.variance() - alldata.var(0)) / alldata.var(0)).cpu()\n    print(""Variance error: %f"" % varerr)\n    counterr = ((qc.integrate(lambda x: torch.ones(x.shape[-1]).cpu())\n                - qc.size) / (0.0 + qc.size)).item()\n    print(""Count error: %f"" % counterr)\n    print(""Time %f"" % (endtime - starttime))\n    # Algorithm is randomized, so some of these will fail with low probability.\n    assert maxreldev < 1.0\n    assert minerr == 0.0\n    assert maxerr == 0.0\n    assert interr < 0.01\n    assert abs(counterr) < 0.001\n    print(""OK"")\n'"
netdissect/sampler.py,1,"b'\'\'\'\nA sampler is just a list of integer listing the indexes of the\ninputs in a data set to sample.  For reproducibility, the\nFixedRandomSubsetSampler uses a seeded prng to produce the same\nsequence always.  FixedSubsetSampler is just a wrapper for an\nexplicit list of integers.\n\ncoordinate_sample solves another sampling problem: when testing\nconvolutional outputs, we can reduce data explosing by sampling\nrandom points of the feature map rather than the entire feature map.\ncoordinate_sample does this in a deterministic way that is also\nresolution-independent.\n\'\'\'\n\nimport numpy\nimport random\nfrom torch.utils.data.sampler import Sampler\n\nclass FixedSubsetSampler(Sampler):\n    """"""Represents a fixed sequence of data set indices.\n    Subsets can be created by specifying a subset of output indexes.\n    """"""\n    def __init__(self, samples):\n        self.samples = samples\n\n    def __iter__(self):\n        return iter(self.samples)\n\n    def __len__(self):\n        return len(self.samples)\n\n    def __getitem__(self, key):\n        return self.samples[key]\n\n    def subset(self, new_subset):\n        return FixedSubsetSampler(self.dereference(new_subset))\n\n    def dereference(self, indices):\n        \'\'\'\n        Translate output sample indices (small numbers indexing the sample)\n        to input sample indices (larger number indexing the original full set)\n        \'\'\'\n        return [self.samples[i] for i in indices]\n\n\nclass FixedRandomSubsetSampler(FixedSubsetSampler):\n    """"""Samples a fixed number of samples from the dataset, deterministically.\n    Arguments:\n        data_source,\n        sample_size,\n        seed (optional)\n    """"""\n    def __init__(self, data_source, start=None, end=None, seed=1):\n        rng = random.Random(seed)\n        shuffled = list(range(len(data_source)))\n        rng.shuffle(shuffled)\n        self.data_source = data_source\n        super(FixedRandomSubsetSampler, self).__init__(shuffled[start:end])\n\n    def class_subset(self, class_filter):\n        \'\'\'\n        Returns only the subset matching the given rule.\n        \'\'\'\n        if isinstance(class_filter, int):\n            rule = lambda d: d[1] == class_filter\n        else:\n            rule = class_filter\n        return self.subset([i for i, j in enumerate(self.samples)\n                if rule(self.data_source[j])])\n\ndef coordinate_sample(shape, sample_size, seeds, grid=13, seed=1, flat=False):\n    \'\'\'\n    Returns a (end-start) sets of sample_size grid points within\n    the shape given.  If the shape dimensions are a multiple of \'grid\',\n    then sampled points within the same row will never be duplicated.\n    \'\'\'\n    if flat:\n        sampind = numpy.zeros((len(seeds), sample_size), dtype=int)\n    else:\n        sampind = numpy.zeros((len(seeds), 2, sample_size), dtype=int)\n    assert sample_size <= grid\n    for j, seed in enumerate(seeds):\n        rng = numpy.random.RandomState(seed)\n        # Shuffle the 169 random grid squares, and pick :sample_size.\n        square_count = grid ** len(shape)\n        square = numpy.stack(numpy.unravel_index(\n            rng.choice(square_count, square_count)[:sample_size],\n            (grid,) * len(shape)))\n        # Then add a random offset to each x, y and put in the range [0...1)\n        # Notice this selects the same locations regardless of resolution.\n        uniform = (square + rng.uniform(size=square.shape)) / grid\n        # TODO: support affine scaling so that we can align receptive field\n        # centers exactly when sampling neurons in different layers.\n        coords = (uniform * numpy.array(shape)[:,None]).astype(int)\n        # Now take sample_size without replacement.  We do this in a way\n        # such that if sample_size is decreased or increased up to \'grid\',\n        # the selected points become a subset, not totally different points.\n        if flat:\n            sampind[j] = numpy.ravel_multi_index(coords, dims=shape)\n        else:\n            sampind[j] = coords\n    return sampind\n\nif __name__ == \'__main__\':\n    from numpy.testing import assert_almost_equal\n    # Test that coordinate_sample is deterministic, in-range, and scalable.\n    assert_almost_equal(coordinate_sample((26, 26), 10, range(101, 102)),\n            [[[14,  0, 12, 11,  8, 13, 11, 20,  7, 20],\n              [ 9, 22,  7, 11, 23, 18, 21, 15,  2,  5]]])\n    assert_almost_equal(coordinate_sample((13, 13), 10, range(101, 102)),\n            [[[ 7,  0,  6,  5,  4,  6,  5, 10,  3, 20 // 2],\n              [ 4, 11,  3,  5, 11,  9, 10,  7,  1,  5 // 2]]])\n    assert_almost_equal(coordinate_sample((13, 13), 10, range(100, 102),\n        flat=True),\n            [[  8,  24,  67, 103,  87,  79, 138,  94,  98,  53],\n             [ 95,  11,  81,  70,  63,  87,  75, 137,  40, 2+10*13]])\n    assert_almost_equal(coordinate_sample((13, 13), 10, range(101, 103),\n        flat=True),\n            [[ 95,  11,  81,  70,  63,  87,  75, 137,  40, 132],\n             [  0,  78, 114, 111,  66,  45,  72,  73,  79, 135]])\n    assert_almost_equal(coordinate_sample((26, 26), 10, range(101, 102),\n        flat=True),\n            [[373,  22, 319, 297, 231, 356, 307, 535, 184, 5+20*26]])\n    # Test FixedRandomSubsetSampler\n    fss = FixedRandomSubsetSampler(range(10))\n    assert len(fss) == 10\n    assert_almost_equal(list(fss), [8, 0, 3, 4, 5, 2, 9, 6, 7, 1])\n    fss = FixedRandomSubsetSampler(range(10), 3, 8)\n    assert len(fss) == 5\n    assert_almost_equal(list(fss), [4, 5, 2, 9, 6])\n    fss = FixedRandomSubsetSampler([(i, i % 3) for i in range(10)],\n            class_filter=1)\n    assert len(fss) == 3\n    assert_almost_equal(list(fss), [4, 7, 1])\n'"
netdissect/segdata.py,3,"b'import os, numpy, torch, json\nfrom .parallelfolder import ParallelImageFolders\nfrom torchvision import transforms\nfrom torchvision.transforms.functional import to_tensor, normalize\n\nclass FieldDef(object):\n    def __init__(self, field, index, bitshift, bitmask, labels):\n        self.field = field\n        self.index = index\n        self.bitshift = bitshift\n        self.bitmask = bitmask\n        self.labels = labels\n\nclass MultiSegmentDataset(object):\n    \'\'\'\n    Just like ClevrMulticlassDataset, but the second stream is a one-hot\n    segmentation tensor rather than a flat one-hot presence vector.\n\n    MultiSegmentDataset(\'dataset/clevrseg\',\n        imgdir=\'images/train/positive\',\n        segdir=\'images/train/segmentation\')\n    \'\'\'\n    def __init__(self, directory, transform=None,\n            imgdir=\'img\', segdir=\'seg\', val=False, size=None):\n        self.segdataset = ParallelImageFolders(\n                [os.path.join(directory, imgdir),\n                 os.path.join(directory, segdir)],\n                transform=transform)\n        self.fields = []\n        with open(os.path.join(directory, \'labelnames.json\'), \'r\') as f:\n            for defn in json.load(f):\n                self.fields.append(FieldDef(\n                    defn[\'field\'], defn[\'index\'], defn[\'bitshift\'],\n                    defn[\'bitmask\'], defn[\'label\']))\n        self.labels = [\'-\'] # Reserve label 0 to mean ""no label""\n        self.categories = []\n        self.label_category = [0]\n        for fieldnum, f in enumerate(self.fields):\n            self.categories.append(f.field)\n            f.firstchannel = len(self.labels)\n            f.channels = len(f.labels) - 1\n            for lab in f.labels[1:]:\n                self.labels.append(lab)\n                self.label_category.append(fieldnum)\n        # Reserve 25% of the dataset for validation.\n        first_val = int(len(self.segdataset) * 0.75)\n        self.val = val\n        self.first = first_val if val else 0\n        self.length = len(self.segdataset) - first_val if val else first_val\n        # Truncate the dataset if requested.\n        if size:\n            self.length = min(size, self.length)\n\n    def __len__(self):\n        return self.length\n\n    def __getitem__(self, index):\n        img, segimg = self.segdataset[index + self.first]\n        segin = numpy.array(segimg, numpy.uint8, copy=False)\n        segout = torch.zeros(len(self.categories),\n                segin.shape[0], segin.shape[1], dtype=torch.int64)\n        for i, field in enumerate(self.fields):\n            fielddata = ((torch.from_numpy(segin[:, :, field.index])\n                    >> field.bitshift) & field.bitmask)\n            segout[i] = field.firstchannel + fielddata - 1\n        bincount = numpy.bincount(segout.flatten(),\n                minlength=len(self.labels))\n        return img, segout, bincount\n\nif __name__ == \'__main__\':\n    ds = MultiSegmentDataset(\'dataset/clevrseg\')\n    print(ds[0])\n    import pdb; pdb.set_trace()\n\n'"
netdissect/segmenter.py,30,"b'# Usage as a simple differentiable segmenter base class\n\nimport os, torch, numpy, json, glob\nimport skimage.morphology\nfrom collections import OrderedDict\nfrom netdissect import upsegmodel\nfrom netdissect import segmodel as segmodel_module\nfrom netdissect.easydict import EasyDict\nfrom urllib.request import urlretrieve\n\nclass BaseSegmenter:\n    def get_label_and_category_names(self):\n        \'\'\'\n        Returns two lists: first, a list of tuples [(label, category), ...]\n        where the label and category are human-readable strings indicating\n        the meaning of a segmentation class.  The 0th segmentation class\n        should be reserved for a label (\'-\') that means ""no prediction.""\n        The second list should just be a list of [category,...] listing\n        all categories in a canonical order.\n        \'\'\'\n        raise NotImplemented()\n\n    def segment_batch(self, tensor_images, downsample=1):\n        \'\'\'\n        Returns a multilabel segmentation for the given batch of (RGB [-1...1])\n        images.  Each pixel of the result is a torch.long indicating a\n        predicted class number.  Multiple classes can be predicted for\n        the same pixel: output shape is (n, multipred, y, x), where\n        multipred is 3, 5, or 6, for how many different predicted labels can\n        be given for each pixel (depending on whether subdivision is being\n        used).  If downsample is specified, then the output y and x dimensions\n        are downsampled from the original image.\n        \'\'\'\n        raise NotImplemented()\n\n    def predict_single_class(self, tensor_images, classnum, downsample=1):\n        \'\'\'\n        Given a batch of images (RGB, normalized to [-1...1]) and\n        a specific segmentation class number, returns a tuple with\n           (1) a differentiable ([0..1]) prediction score for the class\n               at every pixel of the input image.\n           (2) a binary mask showing where in the input image the\n               specified class is the best-predicted label for the pixel.\n        Does not work on subdivided labels.\n        \'\'\'\n        raise NotImplemented()\n\nclass UnifiedParsingSegmenter(BaseSegmenter):\n    \'\'\'\n    This is a wrapper for a more complicated multi-class segmenter,\n    as described in https://arxiv.org/pdf/1807.10221.pdf, and as\n    released in https://github.com/CSAILVision/unifiedparsing.\n    For our purposes and to simplify processing, we do not use\n    whole-scene predictions, and we only consume part segmentations\n    for the three largest object classes (sky, building, person).\n    \'\'\'\n\n    def __init__(self, segsizes=None, segdiv=None):\n        # Create a segmentation model\n        if segsizes is None:\n            segsizes = [256]\n        if segdiv == None:\n            segdiv = \'undivided\'\n        segvocab = \'upp\'\n        segarch = (\'resnet50\', \'upernet\')\n        epoch = 40\n        segmodel = load_unified_parsing_segmentation_model(\n                segarch, segvocab, epoch)\n        segmodel.cuda()\n        self.segmodel = segmodel\n        self.segsizes = segsizes\n        self.segdiv = segdiv\n        mult = 1\n        if self.segdiv == \'quad\':\n            mult = 5\n        self.divmult = mult\n        # Assign class numbers for parts.\n        first_partnumber = (\n                (len(segmodel.labeldata[\'object\']) - 1) * mult + 1 +\n                (len(segmodel.labeldata[\'material\']) - 1))\n        # We only use parts for these three types of objects, for efficiency.\n        partobjects = [\'sky\', \'building\', \'person\']\n        partnumbers = {}\n        partnames = []\n        objectnumbers = {k: v\n                for v, k in enumerate(segmodel.labeldata[\'object\'])}\n        part_index_translation = []\n        # We merge some classes.  For example ""door"" is both an object\n        # and a part of a building.  To avoid confusion, we just count\n        # such classes as objects, and add part scores to the same index.\n        for owner in partobjects:\n            part_list = segmodel.labeldata[\'object_part\'][owner]\n            numeric_part_list = []\n            for part in part_list:\n                if part in objectnumbers:\n                    numeric_part_list.append(objectnumbers[part])\n                elif part in partnumbers:\n                    numeric_part_list.append(partnumbers[part])\n                else:\n                    partnumbers[part] = len(partnames) + first_partnumber\n                    partnames.append(part)\n                    numeric_part_list.append(partnumbers[part])\n            part_index_translation.append(torch.tensor(numeric_part_list))\n        self.objects_with_parts = [objectnumbers[obj] for obj in partobjects]\n        self.part_index = part_index_translation\n        self.part_names = partnames\n        # For now we\'ll just do object and material labels.\n        self.num_classes = 1 + (\n                len(segmodel.labeldata[\'object\']) - 1) * mult + (\n                len(segmodel.labeldata[\'material\']) - 1) + len(partnames)\n        self.num_object_classes = len(self.segmodel.labeldata[\'object\']) - 1\n\n    def get_label_and_category_names(self, dataset=None):\n        \'\'\'\n        Lists label and category names.\n        \'\'\'\n        # Labels are ordered as follows:\n        # 0, [object labels] [divided object labels] [materials] [parts]\n        # The zero label is reserved to mean \'no prediction\'.\n        if self.segdiv == \'quad\':\n            suffixes = [\'t\', \'l\', \'b\', \'r\']\n        else:\n            suffixes = []\n        divided_labels = []\n        for suffix in suffixes:\n            divided_labels.extend([(\'%s-%s\' % (label, suffix), \'part\')\n                for label in self.segmodel.labeldata[\'object\'][1:]])\n        # Create the whole list of labels\n        labelcats = (\n                [(label, \'object\')\n                    for label in self.segmodel.labeldata[\'object\']] +\n                divided_labels +\n                [(label, \'material\')\n                    for label in self.segmodel.labeldata[\'material\'][1:]] +\n                [(label, \'part\') for label in self.part_names])\n        return labelcats, [\'object\', \'part\', \'material\']\n\n    def raw_seg_prediction(self, tensor_images, downsample=1):\n        \'\'\'\n        Generates a segmentation by applying multiresolution voting on\n        the segmentation model, using (rounded to 32 pixels) a set of\n        resolutions in the example benchmark code.\n        \'\'\'\n        y, x = tensor_images.shape[2:]\n        b = len(tensor_images)\n        tensor_images = (tensor_images + 1) / 2 * 255\n        tensor_images = torch.flip(tensor_images, (1,)) # BGR!!!?\n        tensor_images -= torch.tensor([102.9801, 115.9465, 122.7717]).to(\n                   dtype=tensor_images.dtype, device=tensor_images.device\n                   )[None,:,None,None]\n        seg_shape = (y // downsample, x // downsample)\n        # We want these to be multiples of 32 for the model.\n        sizes = [(s, s) for s in self.segsizes]\n        pred = {category: torch.zeros(\n            len(tensor_images), len(self.segmodel.labeldata[category]),\n            seg_shape[0], seg_shape[1]).cuda()\n            for category in [\'object\', \'material\']}\n        part_pred = {partobj_index: torch.zeros(\n            len(tensor_images), len(partindex),\n            seg_shape[0], seg_shape[1]).cuda()\n            for partobj_index, partindex in enumerate(self.part_index)}\n        for size in sizes:\n            if size == tensor_images.shape[2:]:\n                resized = tensor_images\n            else:\n                resized = torch.nn.AdaptiveAvgPool2d(size)(tensor_images)\n            r_pred = self.segmodel(\n                dict(img=resized), seg_size=seg_shape)\n            for k in pred:\n                pred[k] += r_pred[k]\n            for k in part_pred:\n                part_pred[k] += r_pred[\'part\'][k]\n        return pred, part_pred\n\n    def segment_batch(self, tensor_images, downsample=1):\n        \'\'\'\n        Returns a multilabel segmentation for the given batch of (RGB [-1...1])\n        images.  Each pixel of the result is a torch.long indicating a\n        predicted class number.  Multiple classes can be predicted for\n        the same pixel: output shape is (n, multipred, y, x), where\n        multipred is 3, 5, or 6, for how many different predicted labels can\n        be given for each pixel (depending on whether subdivision is being\n        used).  If downsample is specified, then the output y and x dimensions\n        are downsampled from the original image.\n        \'\'\'\n        pred, part_pred = self.raw_seg_prediction(tensor_images,\n                downsample=downsample)\n        piece_channels = 2 if self.segdiv == \'quad\' else 0\n        y, x = tensor_images.shape[2:]\n        seg_shape = (y // downsample, x // downsample)\n        segs = torch.zeros(len(tensor_images), 3 + piece_channels,\n                seg_shape[0], seg_shape[1],\n                dtype=torch.long, device=tensor_images.device)\n        _, segs[:,0] = torch.max(pred[\'object\'], dim=1)\n        # Get materials and translate to shared numbering scheme\n        _, segs[:,1] = torch.max(pred[\'material\'], dim=1)\n        maskout = (segs[:,1] == 0)\n        segs[:,1] += (len(self.segmodel.labeldata[\'object\']) - 1) * self.divmult\n        segs[:,1][maskout] = 0\n        # Now deal with subparts of sky, buildings, people\n        for i, object_index in enumerate(self.objects_with_parts):\n            trans = self.part_index[i].to(segs.device)\n            # Get the argmax, and then translate to shared numbering scheme\n            seg = trans[torch.max(part_pred[i], dim=1)[1]]\n            # Only trust the parts where the prediction also predicts the\n            # owning object.\n            mask = (segs[:,0] == object_index)\n            segs[:,2][mask] = seg[mask]\n\n        if self.segdiv == \'quad\':\n            segs = self.expand_segment_quad(segs, self.segdiv)\n        return segs\n\n    def predict_single_class(self, tensor_images, classnum, downsample=1):\n        \'\'\'\n        Given a batch of images (RGB, normalized to [-1...1]) and\n        a specific segmentation class number, returns a tuple with\n           (1) a differentiable ([0..1]) prediction score for the class\n               at every pixel of the input image.\n           (2) a binary mask showing where in the input image the\n               specified class is the best-predicted label for the pixel.\n        Does not work on subdivided labels.\n        \'\'\'\n        result = 0\n        pred, part_pred = self.raw_seg_prediction(tensor_images,\n                downsample=downsample)\n        material_offset = (len(self.segmodel.labeldata[\'object\']) - 1\n            ) * self.divmult\n        if material_offset < classnum < material_offset + len(\n                self.segmodel.labeldata[\'material\']):\n            return (\n                pred[\'material\'][:, classnum - material_offset],\n                pred[\'material\'].max(dim=1)[1] == classnum - material_offset)\n        mask = None\n        if classnum < len(self.segmodel.labeldata[\'object\']):\n            result = pred[\'object\'][:, classnum]\n            mask = (pred[\'object\'].max(dim=1)[1] == classnum)\n        # Some objects, like \'door\', are also a part of other objects,\n        # so add the part prediction also.\n        for i, object_index in enumerate(self.objects_with_parts):\n            local_index = (self.part_index[i] == classnum).nonzero()\n            if len(local_index) == 0:\n                continue\n            local_index = local_index.item()\n            # Ignore part predictions outside the mask. (We could pay\n            # atttention to and penalize such predictions.)\n            mask2 = (pred[\'object\'].max(dim=1)[1] == object_index) * (\n                    part_pred[i].max(dim=1)[1] == local_index)\n            if mask is None:\n                mask = mask2\n            else:\n                mask = torch.max(mask, mask2)\n            result = result + (part_pred[i][:, local_index])\n        assert result is not 0, \'unrecognized class %d\' % classnum\n        return result, mask\n\n    def expand_segment_quad(self, segs, segdiv=\'quad\'):\n        shape = segs.shape\n        segs[:,3:] = segs[:,0:1] # start by copying the object channel\n        num_seg_labels = self.num_object_classes\n        # For every connected component present (using generator)\n        for i, mask in component_masks(segs[:,0:1]):\n            # Figure the bounding box of the label\n            top, bottom = mask.any(dim=1).nonzero()[[0, -1], 0]\n            left, right = mask.any(dim=0).nonzero()[[0, -1], 0]\n            # Chop the bounding box into four parts\n            vmid = (top + bottom + 1) // 2\n            hmid = (left + right + 1) // 2\n            # Construct top, bottom, right, left masks\n            quad_mask = mask[None,:,:].repeat(4, 1, 1)\n            quad_mask[0, vmid:, :] = 0   # top\n            quad_mask[1, :, hmid:] = 0   # right\n            quad_mask[2, :vmid, :] = 0   # bottom\n            quad_mask[3, :, :hmid] = 0   # left\n            quad_mask = quad_mask.long()\n            # Modify extra segmentation labels by offsetting\n            segs[i,3,:,:] += quad_mask[0] * num_seg_labels\n            segs[i,4,:,:] += quad_mask[1] * (2 * num_seg_labels)\n            segs[i,3,:,:] += quad_mask[2] * (3 * num_seg_labels)\n            segs[i,4,:,:] += quad_mask[3] * (4 * num_seg_labels)\n        # remove any components that were too small to subdivide\n        mask = segs[:,3:] <= self.num_object_classes\n        segs[:,3:][mask] = 0\n        return segs\n\nclass SemanticSegmenter(BaseSegmenter):\n    def __init__(self, modeldir=None, segarch=None, segvocab=None,\n            segsizes=None, segdiv=None, epoch=None):\n        # Create a segmentation model\n        if modeldir == None:\n            modeldir = \'dataset/segmodel\'\n        if segvocab == None:\n            segvocab = \'baseline\'\n        if segarch == None:\n            segarch = (\'resnet50_dilated8\', \'ppm_bilinear_deepsup\')\n        if segdiv == None:\n            segdiv = \'undivided\'\n        elif isinstance(segarch, str):\n            segarch = segarch.split(\',\')\n        segmodel = load_segmentation_model(modeldir, segarch, segvocab, epoch)\n        if segsizes is None:\n            segsizes = getattr(segmodel.meta, \'segsizes\', [256])\n        self.segsizes = segsizes\n        # Verify segmentation model to has every out_channel labeled.\n        assert len(segmodel.meta.labels) == list(c for c in segmodel.modules()\n            if isinstance(c, torch.nn.Conv2d))[-1].out_channels\n        segmodel.cuda()\n        self.segmodel = segmodel\n        self.segdiv = segdiv\n        # Image normalization\n        self.bgr = (segmodel.meta.imageformat.byteorder == \'BGR\')\n        self.imagemean = torch.tensor(segmodel.meta.imageformat.mean)\n        self.imagestd = torch.tensor(segmodel.meta.imageformat.stdev)\n        # Map from labels to external indexes, and labels to channel sets.\n        self.labelmap = {\'-\': 0}\n        self.channelmap = {\'-\': []}\n        self.labels = [(\'-\', \'-\')]\n        num_labels = 1\n        self.num_underlying_classes = len(segmodel.meta.labels)\n        # labelmap maps names to external indexes.\n        for i, label in enumerate(segmodel.meta.labels):\n            if label.name not in self.channelmap:\n                self.channelmap[label.name] = []\n            self.channelmap[label.name].append(i)\n            if getattr(label, \'internal\', None) or label.name in self.labelmap:\n                continue\n            self.labelmap[label.name] = num_labels\n            num_labels += 1\n            self.labels.append((label.name, label.category))\n        # Each category gets its own independent softmax.\n        self.category_indexes = { category.name:\n                [i for i, label in enumerate(segmodel.meta.labels)\n                   if label.category == category.name] \n                for category in segmodel.meta.categories }\n        # catindexmap maps names to category internal indexes\n        self.catindexmap = {}\n        for catname, indexlist in self.category_indexes.items():\n            for index, i in enumerate(indexlist):\n                self.catindexmap[segmodel.meta.labels[i].name] = (\n                        (catname, index))\n        # After the softmax, each category is mapped to external indexes.\n        self.category_map = { catname:\n                torch.tensor([\n                    self.labelmap.get(segmodel.meta.labels[ind].name, 0)\n                    for ind in catindex])\n                for catname, catindex in self.category_indexes.items()}\n        self.category_rules = segmodel.meta.categories\n        # Finally, naive subdivision can be applied.\n        mult = 1\n        if self.segdiv == \'quad\':\n            mult = 5\n            suffixes = [\'t\', \'l\', \'b\', \'r\']\n            divided_labels = []\n            for suffix in suffixes:\n                divided_labels.extend([(\'%s-%s\' % (label, suffix), cat)\n                    for label, cat in self.labels[1:]])\n                self.channelmap.update({\n                    \'%s-%s\' % (label, suffix): self.channelmap[label]\n                    for label, cat in self.labels[1:] })\n            self.labels.extend(divided_labels)\n        # For examining a single class\n        self.channellist = [self.channelmap[name] for name, _ in self.labels]\n\n    def get_label_and_category_names(self, dataset=None):\n        return self.labels, self.segmodel.categories\n\n    def segment_batch(self, tensor_images, downsample=1):\n        return self.raw_segment_batch(tensor_images, downsample)[0]\n\n    def raw_segment_batch(self, tensor_images, downsample=1):\n        pred = self.raw_seg_prediction(tensor_images, downsample)\n        catsegs = {}\n        for catkey, catindex in self.category_indexes.items():\n            _, segs = torch.max(pred[:, catindex], dim=1)\n            catsegs[catkey] = segs\n        masks = {}\n        segs = torch.zeros(len(tensor_images), len(self.category_rules),\n                pred.shape[2], pred.shape[2], device=pred.device,\n                dtype=torch.long)\n        for i, cat in enumerate(self.category_rules):\n            catmap = self.category_map[cat.name].to(pred.device)\n            translated = catmap[catsegs[cat.name]]\n            if getattr(cat, \'mask\', None) is not None:\n                if cat.mask not in masks:\n                    maskcat, maskind = self.catindexmap[cat.mask]\n                    masks[cat.mask] = (catsegs[maskcat] == maskind)\n                translated *= masks[cat.mask].long()\n            segs[:,i] = translated\n        if self.segdiv == \'quad\':\n            segs = self.expand_segment_quad(segs,\n                    self.num_underlying_classes, self.segdiv)\n        return segs, pred\n\n    def raw_seg_prediction(self, tensor_images, downsample=1):\n        \'\'\'\n        Generates a segmentation by applying multiresolution voting on\n        the segmentation model, using (rounded to 32 pixels) a set of\n        resolutions in the example benchmark code.\n        \'\'\'\n        y, x = tensor_images.shape[2:]\n        b = len(tensor_images)\n        # Flip the RGB order if specified.\n        if self.bgr:\n           tensor_images = torch.flip(tensor_images, (1,))\n        # Transform from our [-1..1] range to torch standard [0..1] range\n        # and then apply normalization.\n        tensor_images = ((tensor_images + 1) / 2\n                ).sub_(self.imagemean[None,:,None,None].to(tensor_images.device)\n                ).div_(self.imagestd[None,:,None,None].to(tensor_images.device))\n        # Output shape can be downsampled.\n        seg_shape = (y // downsample, x // downsample)\n        # We want these to be multiples of 32 for the model.\n        sizes = [(s, s) for s in self.segsizes]\n        pred = torch.zeros(\n            len(tensor_images), (self.num_underlying_classes),\n            seg_shape[0], seg_shape[1]).cuda()\n        for size in sizes:\n            if size == tensor_images.shape[2:]:\n                resized = tensor_images\n            else:\n                resized = torch.nn.AdaptiveAvgPool2d(size)(tensor_images)\n            raw_pred = self.segmodel(\n                dict(img_data=resized), segSize=seg_shape)\n            softmax_pred = torch.empty_like(raw_pred)\n            for catindex in self.category_indexes.values():\n                softmax_pred[:, catindex] = torch.nn.functional.softmax(\n                        raw_pred[:, catindex], dim=1)\n            pred += softmax_pred\n        return pred\n\n    def expand_segment_quad(self, segs, num_seg_labels, segdiv=\'quad\'):\n        shape = segs.shape\n        output = segs.repeat(1, 3, 1, 1)\n        # For every connected component present (using generator)\n        for i, mask in component_masks(segs):\n            # Figure the bounding box of the label\n            top, bottom = mask.any(dim=1).nonzero()[[0, -1], 0]\n            left, right = mask.any(dim=0).nonzero()[[0, -1], 0]\n            # Chop the bounding box into four parts\n            vmid = (top + bottom + 1) // 2\n            hmid = (left + right + 1) // 2\n            # Construct top, bottom, right, left masks\n            quad_mask = mask[None,:,:].repeat(4, 1, 1)\n            quad_mask[0, vmid:, :] = 0   # top\n            quad_mask[1, :, hmid:] = 0   # right\n            quad_mask[2, :vmid, :] = 0   # bottom\n            quad_mask[3, :, :hmid] = 0   # left\n            quad_mask = quad_mask.long()\n            # Modify extra segmentation labels by offsetting\n            output[i,1,:,:] += quad_mask[0] * num_seg_labels\n            output[i,2,:,:] += quad_mask[1] * (2 * num_seg_labels)\n            output[i,1,:,:] += quad_mask[2] * (3 * num_seg_labels)\n            output[i,2,:,:] += quad_mask[3] * (4 * num_seg_labels)\n        return output\n\n    def predict_single_class(self, tensor_images, classnum, downsample=1):\n        \'\'\'\n        Given a batch of images (RGB, normalized to [-1...1]) and\n        a specific segmentation class number, returns a tuple with\n           (1) a differentiable ([0..1]) prediction score for the class\n               at every pixel of the input image.\n           (2) a binary mask showing where in the input image the\n               specified class is the best-predicted label for the pixel.\n        Does not work on subdivided labels.\n        \'\'\'\n        seg, pred = self.raw_segment_batch(tensor_images,\n                downsample=downsample)\n        result = pred[:,self.channellist[classnum]].sum(dim=1)\n        mask = (seg == classnum).max(1)[0]\n        return result, mask\n\ndef component_masks(segmentation_batch):\n    \'\'\'\n    Splits connected components into regions (slower, requires cpu).\n    \'\'\'\n    npbatch = segmentation_batch.cpu().numpy()\n    for i in range(segmentation_batch.shape[0]):\n        labeled, num = skimage.morphology.label(npbatch[i][0], return_num=True)\n        labeled = torch.from_numpy(labeled).to(segmentation_batch.device)\n        for label in range(1, num):\n            yield i, (labeled == label)\n\ndef load_unified_parsing_segmentation_model(segmodel_arch, segvocab, epoch):\n    segmodel_dir = \'dataset/segmodel/%s-%s-%s\' % ((segvocab,) + segmodel_arch)\n    # Load json of class names and part/object structure\n    with open(os.path.join(segmodel_dir, \'labels.json\')) as f:\n        labeldata = json.load(f)\n    nr_classes={k: len(labeldata[k])\n                for k in [\'object\', \'scene\', \'material\']}\n    nr_classes[\'part\'] = sum(len(p) for p in labeldata[\'object_part\'].values())\n    # Create a segmentation model\n    segbuilder = upsegmodel.ModelBuilder()\n    # example segmodel_arch = (\'resnet101\', \'upernet\')\n    seg_encoder = segbuilder.build_encoder(\n            arch=segmodel_arch[0],\n            fc_dim=2048,\n            weights=os.path.join(segmodel_dir, \'encoder_epoch_%d.pth\' % epoch))\n    seg_decoder = segbuilder.build_decoder(\n            arch=segmodel_arch[1],\n            fc_dim=2048, use_softmax=True,\n            nr_classes=nr_classes,\n            weights=os.path.join(segmodel_dir, \'decoder_epoch_%d.pth\' % epoch))\n    segmodel = upsegmodel.SegmentationModule(\n            seg_encoder, seg_decoder, labeldata)\n    segmodel.categories = [\'object\', \'part\', \'material\']\n    segmodel.eval()\n    return segmodel\n\ndef load_segmentation_model(modeldir, segmodel_arch, segvocab, epoch=None):\n    # Load csv of class names\n    segmodel_dir = \'dataset/segmodel/%s-%s-%s\' % ((segvocab,) + segmodel_arch)\n    with open(os.path.join(segmodel_dir, \'labels.json\')) as f:\n        labeldata = EasyDict(json.load(f))\n    # Automatically pick the last epoch available.\n    if epoch is None:\n        choices = [os.path.basename(n)[14:-4] for n in\n                glob.glob(os.path.join(segmodel_dir, \'encoder_epoch_*.pth\'))]\n        epoch = max([int(c) for c in choices if c.isdigit()])\n    # Create a segmentation model\n    segbuilder = segmodel_module.ModelBuilder()\n    # example segmodel_arch = (\'resnet101\', \'upernet\')\n    seg_encoder = segbuilder.build_encoder(\n            arch=segmodel_arch[0],\n            fc_dim=2048,\n            weights=os.path.join(segmodel_dir, \'encoder_epoch_%d.pth\' % epoch))\n    seg_decoder = segbuilder.build_decoder(\n            arch=segmodel_arch[1],\n            fc_dim=2048, inference=True, num_class=len(labeldata.labels),\n            weights=os.path.join(segmodel_dir, \'decoder_epoch_%d.pth\' % epoch))\n    segmodel = segmodel_module.SegmentationModule(seg_encoder, seg_decoder,\n                                  torch.nn.NLLLoss(ignore_index=-1))\n    segmodel.categories = [cat.name for cat in labeldata.categories]\n    segmodel.labels = [label.name for label in labeldata.labels]\n    categories = OrderedDict()\n    label_category = numpy.zeros(len(segmodel.labels), dtype=int)\n    for i, label in enumerate(labeldata.labels):\n        label_category[i] = segmodel.categories.index(label.category)\n    segmodel.meta = labeldata\n    segmodel.eval()\n    return segmodel\n\ndef ensure_upp_segmenter_downloaded(directory):\n    baseurl = \'http://netdissect.csail.mit.edu/data/segmodel\'\n    dirname = \'upp-resnet50-upernet\'\n    files = [\'decoder_epoch_40.pth\', \'encoder_epoch_40.pth\', \'labels.json\']\n    download_dir = os.path.join(directory, dirname)\n    os.makedirs(download_dir, exist_ok=True)\n    for fn in files:\n        if os.path.isfile(os.path.join(download_dir, fn)):\n            continue # Skip files already downloaded\n        url = \'%s/%s/%s\' % (baseurl, dirname, fn)\n        print(\'Downloading %s\' % url)\n        urlretrieve(url, os.path.join(download_dir, fn))\n    assert os.path.isfile(os.path.join(directory, dirname, \'labels.json\'))\n\ndef test_main():\n    \'\'\'\n    Test the unified segmenter.\n    \'\'\'\n    from PIL import Image\n    testim = Image.open(\'script/testdata/test_church_242.jpg\')\n    tensor_im = (torch.from_numpy(numpy.asarray(testim)).permute(2, 0, 1)\n            .float() / 255 * 2 - 1)[None, :, :, :].cuda()\n    segmenter = UnifiedParsingSegmenter()\n    seg = segmenter.segment_batch(tensor_im)\n    bc = torch.bincount(seg.view(-1))\n    labels, cats = segmenter.get_label_and_category_names()\n    for label in bc.nonzero()[:,0]:\n        if label.item():\n            # What is the prediction for this class?\n            pred, mask = segmenter.predict_single_class(tensor_im, label.item())\n            assert mask.sum().item() == bc[label].item()\n            assert len(((seg == label).max(1)[0] - mask).nonzero()) == 0\n            inside_pred = pred[mask].mean().item()\n            outside_pred = pred[~mask].mean().item()\n            print(\'%s (%s, #%d): %d pixels, pred %.2g inside %.2g outside\' %\n                (labels[label.item()] + (label.item(), bc[label].item(),\n                    inside_pred, outside_pred)))\n\nif __name__ == \'__main__\':\n    test_main()\n'"
netdissect/segviz.py,0,"b""import numpy, scipy\n\ndef segment_visualization(seg, size):\n    result = numpy.zeros((seg.shape[1] * seg.shape[2], 3), dtype=numpy.uint8)\n    flatseg = seg.reshape(seg.shape[0], seg.shape[1] * seg.shape[2])\n    bc = numpy.bincount(flatseg.flatten())\n    top = numpy.argsort(-bc)\n    # In a multilabel segmentation, we can't draw everything.\n    # Draw the fewest-pixel labels last.  (We could pick the opposite order.)\n    for label in top:\n        if label == 0:\n            continue\n        if bc[label] == 0:\n            break\n        bitmap = ((flatseg == label).sum(axis=0) > 0)\n        result[bitmap] = high_contrast_arr[label % len(high_contrast_arr)]\n    result = result.reshape((seg.shape[1], seg.shape[2], 3))\n    if seg.shape[1:] != size:\n        result = scipy.misc.imresize(result, size, interp='nearest')\n    return result\n\n# A palette that maximizes perceptual contrast between entries.\n# https://stackoverflow.com/questions/33295120\nhigh_contrast = [\n    [0, 0, 0], [255, 255, 0], [28, 230, 255], [255, 52, 255],\n    [255, 74, 70], [0, 137, 65], [0, 111, 166], [163, 0, 89],\n    [255, 219, 229], [122, 73, 0], [0, 0, 166], [99, 255, 172],\n    [183, 151, 98], [0, 77, 67], [143, 176, 255], [153, 125, 135],\n    [90, 0, 7], [128, 150, 147], [254, 255, 230], [27, 68, 0],\n    [79, 198, 1], [59, 93, 255], [74, 59, 83], [255, 47, 128],\n    [97, 97, 90], [186, 9, 0], [107, 121, 0], [0, 194, 160],\n    [255, 170, 146], [255, 144, 201], [185, 3, 170], [209, 97, 0],\n    [221, 239, 255], [0, 0, 53], [123, 79, 75], [161, 194, 153],\n    [48, 0, 24], [10, 166, 216], [1, 51, 73], [0, 132, 111],\n    [55, 33, 1], [255, 181, 0], [194, 255, 237], [160, 121, 191],\n    [204, 7, 68], [192, 185, 178], [194, 255, 153], [0, 30, 9],\n    [0, 72, 156], [111, 0, 98], [12, 189, 102], [238, 195, 255],\n    [69, 109, 117], [183, 123, 104], [122, 135, 161], [120, 141, 102],\n    [136, 85, 120], [250, 208, 159], [255, 138, 154], [209, 87, 160],\n    [190, 196, 89], [69, 102, 72], [0, 134, 237], [136, 111, 76],\n    [52, 54, 45], [180, 168, 189], [0, 166, 170], [69, 44, 44],\n    [99, 99, 117], [163, 200, 201], [255, 145, 63], [147, 138, 129],\n    [87, 83, 41], [0, 254, 207], [176, 91, 111], [140, 208, 255],\n    [59, 151, 0], [4, 247, 87], [200, 161, 161], [30, 110, 0],\n    [121, 0, 215], [167, 117, 0], [99, 103, 169], [160, 88, 55],\n    [107, 0, 44], [119, 38, 0], [215, 144, 255], [155, 151, 0],\n    [84, 158, 121], [255, 246, 159], [32, 22, 37], [114, 65, 143],\n    [188, 35, 255], [153, 173, 192], [58, 36, 101], [146, 35, 41],\n    [91, 69, 52], [253, 232, 220], [64, 78, 85], [0, 137, 163],\n    [203, 126, 152], [164, 232, 4], [50, 78, 114], [106, 58, 76],\n    [131, 171, 88], [0, 28, 30], [209, 247, 206], [0, 75, 40],\n    [200, 208, 246], [163, 164, 137], [128, 108, 102], [34, 40, 0],\n    [191, 86, 80], [232, 48, 0], [102, 121, 109], [218, 0, 124],\n    [255, 26, 89], [138, 219, 180], [30, 2, 0], [91, 78, 81],\n    [200, 149, 197], [50, 0, 51], [255, 104, 50], [102, 225, 211],\n    [207, 205, 172], [208, 172, 148], [126, 211, 121], [1, 44, 88],\n    [122, 123, 255], [214, 142, 1], [53, 51, 57], [120, 175, 161],\n    [254, 178, 198], [117, 121, 124], [131, 115, 147], [148, 58, 77],\n    [181, 244, 255], [210, 220, 213], [149, 86, 189], [106, 113, 74],\n    [0, 19, 37], [2, 82, 95], [10, 163, 247], [233, 129, 118],\n    [219, 213, 221], [94, 188, 209], [61, 79, 68], [126, 100, 5],\n    [2, 104, 78], [150, 43, 117], [141, 133, 70], [150, 149, 197],\n    [231, 115, 206], [216, 106, 120], [62, 137, 190], [202, 131, 78],\n    [81, 138, 135], [91, 17, 60], [85, 129, 59], [231, 4, 196],\n    [0, 0, 95], [169, 115, 153], [75, 129, 96], [89, 115, 138],\n    [255, 93, 167], [247, 201, 191], [100, 49, 39], [81, 58, 1],\n    [107, 148, 170], [81, 160, 88], [164, 91, 2], [29, 23, 2],\n    [226, 0, 39], [231, 171, 99], [76, 96, 1], [156, 105, 102],\n    [100, 84, 123], [151, 151, 158], [0, 106, 102], [57, 20, 6],\n    [244, 215, 73], [0, 69, 210], [0, 108, 49], [221, 182, 208],\n    [124, 101, 113], [159, 178, 164], [0, 216, 145], [21, 160, 138],\n    [188, 101, 233], [255, 255, 254], [198, 220, 153], [32, 59, 60],\n    [103, 17, 144], [107, 58, 100], [245, 225, 255], [255, 160, 242],\n    [204, 170, 53], [55, 69, 39], [139, 180, 0], [121, 120, 104],\n    [198, 0, 90], [59, 0, 10], [200, 98, 64], [41, 96, 124],\n    [64, 35, 52], [125, 90, 68], [204, 184, 124], [184, 129, 131],\n    [170, 81, 153], [181, 214, 195], [163, 132, 105], [159, 148, 240],\n    [167, 69, 113], [184, 148, 166], [113, 187, 140], [0, 180, 51],\n    [120, 158, 201], [109, 128, 186], [149, 63, 0], [94, 255, 3],\n    [228, 255, 252], [27, 225, 119], [188, 177, 229], [118, 145, 47],\n    [0, 49, 9], [0, 96, 205], [210, 0, 150], [137, 85, 99],\n    [41, 32, 29], [91, 50, 19], [167, 111, 66], [137, 65, 46],\n    [26, 58, 42], [73, 75, 90], [168, 140, 133], [244, 171, 170],\n    [163, 243, 171], [0, 198, 200], [234, 139, 102], [149, 138, 159],\n    [189, 201, 210], [159, 160, 100], [190, 71, 0], [101, 129, 136],\n    [131, 164, 133], [69, 60, 35], [71, 103, 93], [58, 63, 0],\n    [6, 18, 3], [223, 251, 113], [134, 142, 126], [152, 208, 88],\n    [108, 143, 125], [215, 191, 194], [60, 62, 110], [216, 61, 102],\n    [47, 93, 155], [108, 94, 70], [210, 91, 136], [91, 101, 108],\n    [0, 181, 127], [84, 92, 70], [134, 96, 151], [54, 93, 37],\n    [37, 47, 153], [0, 204, 255], [103, 78, 96], [252, 0, 156],\n    [146, 137, 107], [30, 35, 36], [222, 201, 178], [157, 73, 72],\n    [133, 171, 180], [52, 33, 66], [208, 150, 133], [164, 172, 172],\n    [0, 255, 255], [174, 156, 134], [116, 42, 51], [14, 114, 197],\n    [175, 216, 236], [192, 100, 185], [145, 2, 140], [254, 237, 191],\n    [255, 183, 137], [156, 184, 228], [175, 255, 209], [42, 54, 76],\n    [79, 74, 67], [100, 112, 149], [52, 187, 255], [128, 119, 129],\n    [146, 0, 3], [179, 165, 167], [1, 134, 21], [241, 255, 200],\n    [151, 111, 92], [255, 59, 193], [255, 95, 107], [7, 125, 132],\n    [245, 109, 147], [87, 113, 218], [78, 30, 42], [131, 0, 85],\n    [2, 211, 70], [190, 69, 45], [0, 144, 94], [190, 0, 40],\n    [110, 150, 227], [0, 118, 153], [254, 201, 109], [156, 106, 125],\n    [63, 161, 184], [137, 61, 227], [121, 180, 214], [127, 212, 217],\n    [103, 81, 187], [178, 141, 45], [226, 122, 5], [221, 156, 184],\n    [170, 188, 122], [152, 0, 52], [86, 26, 2], [143, 127, 0],\n    [99, 80, 0], [205, 125, 174], [138, 94, 45], [255, 179, 225],\n    [107, 100, 102], [198, 211, 0], [1, 0, 226], [136, 236, 105],\n    [143, 204, 190], [33, 0, 28], [81, 31, 77], [227, 246, 227],\n    [255, 142, 177], [107, 79, 41], [163, 127, 70], [106, 89, 80],\n    [31, 42, 26], [4, 120, 77], [16, 24, 53], [230, 224, 208],\n    [255, 116, 254], [0, 164, 95], [143, 93, 248], [75, 0, 89],\n    [65, 47, 35], [216, 147, 158], [219, 157, 114], [96, 65, 67],\n    [181, 186, 206], [152, 158, 183], [210, 196, 219], [165, 135, 175],\n    [119, 215, 150], [127, 140, 148], [255, 155, 3], [85, 81, 150],\n    [49, 221, 174], [116, 182, 113], [128, 38, 71], [42, 55, 63],\n    [1, 74, 104], [105, 102, 40], [76, 123, 109], [0, 44, 39],\n    [122, 69, 34], [59, 88, 89], [229, 211, 129], [255, 243, 255],\n    [103, 159, 160], [38, 19, 0], [44, 87, 66], [145, 49, 175],\n    [175, 93, 136], [199, 112, 106], [97, 171, 31], [140, 242, 212],\n    [197, 217, 184], [159, 255, 251], [191, 69, 204], [73, 57, 65],\n    [134, 59, 96], [185, 0, 118], [0, 49, 119], [197, 130, 210],\n    [193, 179, 148], [96, 43, 112], [136, 120, 104], [186, 191, 176],\n    [3, 0, 18], [209, 172, 254], [127, 222, 254], [75, 92, 113],\n    [163, 160, 151], [230, 109, 83], [99, 123, 93], [146, 190, 165],\n    [0, 248, 179], [190, 221, 255], [61, 181, 167], [221, 50, 72],\n    [182, 228, 222], [66, 119, 69], [89, 140, 90], [185, 76, 89],\n    [129, 129, 213], [148, 136, 139], [254, 214, 189], [83, 109, 49],\n    [110, 255, 146], [228, 232, 255], [32, 226, 0], [255, 208, 242],\n    [76, 131, 161], [189, 115, 34], [145, 92, 78], [140, 71, 135],\n    [2, 81, 23], [162, 170, 69], [45, 27, 33], [169, 221, 176],\n    [255, 79, 120], [82, 133, 0], [0, 154, 46], [23, 252, 228],\n    [113, 85, 90], [82, 93, 130], [0, 25, 90], [150, 120, 116],\n    [85, 85, 88], [11, 33, 44], [30, 32, 43], [239, 191, 196],\n    [111, 151, 85], [111, 117, 134], [80, 29, 29], [55, 45, 0],\n    [116, 29, 22], [94, 179, 147], [181, 180, 0], [221, 74, 56],\n    [54, 61, 255], [173, 101, 82], [102, 53, 175], [131, 107, 186],\n    [152, 170, 127], [70, 72, 54], [50, 44, 62], [124, 185, 186],\n    [91, 105, 101], [112, 125, 61], [122, 0, 29], [110, 70, 54],\n    [68, 58, 56], [174, 129, 255], [72, 144, 121], [137, 115, 52],\n    [0, 144, 135], [218, 113, 60], [54, 22, 24], [255, 111, 1],\n    [0, 102, 121], [55, 14, 119], [75, 58, 131], [201, 226, 230],\n    [196, 65, 112], [255, 69, 38], [115, 190, 84], [196, 223, 114],\n    [173, 255, 96], [0, 68, 125], [220, 206, 201], [189, 148, 121],\n    [101, 110, 91], [236, 82, 0], [255, 110, 194], [122, 97, 126],\n    [221, 174, 162], [119, 131, 127], [165, 51, 39], [96, 142, 255],\n    [181, 153, 215], [165, 1, 73], [78, 0, 37], [201, 177, 169],\n    [3, 145, 154], [27, 42, 37], [229, 0, 241], [152, 46, 11],\n    [182, 113, 128], [224, 88, 89], [0, 96, 57], [87, 143, 155],\n    [48, 82, 48], [206, 147, 76], [179, 194, 190], [192, 186, 192],\n    [181, 6, 211], [23, 12, 16], [76, 83, 79], [34, 68, 81],\n    [62, 65, 65], [120, 114, 109], [182, 96, 43], [32, 4, 65],\n    [221, 181, 136], [73, 114, 0], [197, 170, 182], [3, 60, 97],\n    [113, 178, 245], [169, 224, 136], [73, 121, 176], [162, 195, 223],\n    [120, 65, 73], [45, 43, 23], [62, 14, 47], [87, 52, 76],\n    [0, 145, 190], [228, 81, 209], [75, 75, 106], [92, 1, 26],\n    [124, 128, 96], [255, 148, 145], [76, 50, 93], [0, 92, 139],\n    [229, 253, 164], [104, 209, 182], [3, 38, 65], [20, 0, 35],\n    [134, 131, 169], [207, 255, 0], [167, 44, 62], [52, 71, 90],\n    [177, 187, 154], [180, 160, 79], [141, 145, 142], [161, 104, 166],\n    [129, 61, 58], [66, 82, 24], [218, 131, 134], [119, 97, 51],\n    [86, 57, 48], [132, 152, 174], [144, 193, 211], [181, 102, 107],\n    [155, 88, 94], [133, 100, 101], [173, 124, 144], [226, 188, 0],\n    [227, 170, 224], [178, 194, 254], [253, 0, 57], [0, 155, 117],\n    [255, 244, 109], [232, 126, 172], [223, 227, 230], [132, 133, 144],\n    [170, 146, 151], [131, 161, 147], [87, 121, 119], [62, 113, 88],\n    [198, 66, 137], [234, 0, 114], [196, 168, 203], [85, 200, 153],\n    [231, 143, 207], [0, 69, 71], [246, 226, 227], [150, 103, 22],\n    [55, 143, 219], [67, 94, 106], [218, 0, 4], [27, 0, 15],\n    [91, 156, 143], [110, 43, 82], [1, 17, 21], [227, 232, 196],\n    [174, 59, 133], [234, 28, 169], [255, 158, 107], [69, 125, 139],\n    [146, 103, 139], [0, 205, 187], [156, 204, 4], [0, 46, 56],\n    [150, 197, 127], [207, 246, 180], [73, 40, 24], [118, 110, 82],\n    [32, 55, 14], [227, 209, 159], [46, 60, 48], [178, 234, 206],\n    [243, 189, 164], [162, 78, 61], [151, 111, 217], [140, 159, 168],\n    [124, 43, 115], [78, 95, 55], [93, 84, 98], [144, 149, 111],\n    [106, 167, 118], [219, 203, 246], [218, 113, 255], [152, 124, 149],\n    [82, 50, 60], [187, 60, 66], [88, 77, 57], [79, 193, 95],\n    [162, 185, 193], [121, 219, 33], [29, 89, 88], [189, 116, 78],\n    [22, 11, 0], [32, 34, 26], [107, 130, 149], [0, 224, 228],\n    [16, 36, 1], [27, 120, 42], [218, 169, 181], [176, 65, 93],\n    [133, 146, 83], [151, 160, 148], [6, 227, 196], [71, 104, 140],\n    [124, 103, 85], [7, 92, 0], [117, 96, 213], [125, 159, 0],\n    [195, 109, 150], [77, 145, 62], [95, 66, 118], [252, 228, 200],\n    [48, 48, 82], [79, 56, 27], [229, 165, 50], [112, 102, 144],\n    [170, 154, 146], [35, 115, 99], [115, 1, 62], [255, 144, 121],\n    [167, 154, 116], [2, 155, 219], [255, 1, 105], [199, 210, 231],\n    [202, 136, 105], [128, 255, 205], [187, 31, 105], [144, 176, 171],\n    [125, 116, 169], [252, 199, 219], [153, 55, 91], [0, 171, 77],\n    [171, 174, 209], [190, 157, 145], [230, 229, 167], [51, 44, 34],\n    [221, 88, 123], [245, 255, 247], [93, 48, 51], [109, 56, 0],\n    [255, 0, 32], [181, 123, 179], [215, 255, 230], [197, 53, 169],\n    [38, 0, 9], [106, 135, 129], [168, 171, 180], [212, 82, 98],\n    [121, 75, 97], [70, 33, 178], [141, 164, 219], [199, 200, 144],\n    [111, 233, 173], [162, 67, 167], [178, 176, 129], [24, 27, 0],\n    [40, 97, 84], [76, 164, 59], [106, 149, 115], [168, 68, 29],\n    [92, 114, 123], [115, 134, 113], [208, 207, 203], [137, 123, 119],\n    [31, 63, 34], [65, 69, 167], [218, 152, 148], [161, 117, 122],\n    [99, 36, 60], [173, 170, 255], [0, 205, 226], [221, 188, 98],\n    [105, 142, 177], [32, 132, 98], [0, 183, 224], [97, 74, 68],\n    [155, 187, 87], [122, 92, 84], [133, 122, 80], [118, 107, 126],\n    [1, 72, 51], [255, 131, 71], [122, 142, 186], [39, 71, 64],\n    [148, 100, 68], [235, 216, 230], [100, 98, 65], [55, 57, 23],\n    [106, 212, 80], [129, 129, 123], [212, 153, 227], [151, 148, 64],\n    [1, 26, 18], [82, 101, 84], [181, 136, 92], [164, 153, 165],\n    [3, 173, 137], [179, 0, 139], [227, 196, 181], [150, 83, 31],\n    [134, 113, 117], [116, 86, 158], [97, 125, 159], [231, 4, 82],\n    [6, 126, 175], [166, 151, 182], [183, 135, 168], [156, 255, 147],\n    [49, 29, 25], [58, 148, 89], [110, 116, 110], [176, 197, 174],\n    [132, 237, 247], [237, 52, 136], [117, 76, 120], [56, 70, 68],\n    [199, 132, 123], [0, 182, 197], [127, 166, 112], [193, 175, 158],\n    [42, 127, 255], [114, 165, 140], [255, 192, 127], [157, 235, 221],\n    [217, 124, 142], [126, 124, 147], [98, 230, 116], [181, 99, 158],\n    [255, 168, 97], [194, 165, 128], [141, 156, 131], [183, 5, 70],\n    [55, 43, 46], [0, 152, 255], [152, 89, 117], [32, 32, 76],\n    [255, 108, 96], [68, 80, 131], [133, 2, 170], [114, 54, 31],\n    [150, 118, 163], [72, 68, 73], [206, 214, 194], [59, 22, 74],\n    [204, 167, 99], [44, 127, 119], [2, 34, 123], [163, 126, 111],\n    [205, 230, 220], [205, 255, 251], [190, 129, 26], [247, 113, 131],\n    [237, 230, 226], [205, 198, 180], [255, 224, 158], [58, 114, 113],\n    [255, 123, 89], [78, 78, 1], [74, 198, 132], [139, 200, 145],\n    [188, 138, 150], [207, 99, 83], [220, 222, 92], [94, 170, 221],\n    [246, 160, 173], [226, 105, 170], [163, 218, 228], [67, 110, 131],\n    [0, 46, 23], [236, 251, 255], [161, 194, 182], [80, 0, 63],\n    [113, 105, 91], [103, 196, 187], [83, 110, 255], [93, 90, 72],\n    [137, 0, 57], [150, 147, 129], [55, 21, 33], [94, 70, 101],\n    [170, 98, 195], [141, 111, 129], [44, 97, 53], [65, 6, 1],\n    [86, 70, 32], [230, 144, 52], [109, 166, 189], [229, 142, 86],\n    [227, 166, 139], [72, 177, 118], [210, 125, 103], [181, 178, 104],\n    [127, 132, 39], [255, 132, 230], [67, 87, 64], [234, 228, 8],\n    [244, 245, 255], [50, 88, 0], [75, 107, 165], [173, 206, 255],\n    [155, 138, 204], [136, 81, 56], [88, 117, 193], [126, 115, 17],\n    [254, 165, 202], [159, 139, 91], [165, 91, 84], [137, 0, 106],\n    [175, 117, 111], [42, 32, 0], [116, 153, 161], [255, 181, 80],\n    [0, 1, 30], [209, 81, 28], [104, 129, 81], [188, 144, 138],\n    [120, 200, 235], [133, 2, 255], [72, 61, 48], [196, 34, 33],\n    [94, 167, 255], [120, 87, 21], [12, 234, 145], [255, 250, 237],\n    [179, 175, 157], [62, 61, 82], [90, 155, 194], [156, 47, 144],\n    [141, 87, 0], [173, 215, 156], [0, 118, 139], [51, 125, 0],\n    [197, 151, 0], [49, 86, 220], [148, 69, 117], [236, 255, 220],\n    [210, 76, 178], [151, 112, 60], [76, 37, 127], [158, 3, 102],\n    [136, 255, 236], [181, 100, 129], [57, 109, 43], [86, 115, 95],\n    [152, 131, 118], [155, 177, 149], [169, 121, 92], [228, 197, 211],\n    [159, 79, 103], [30, 43, 57], [102, 67, 39], [175, 206, 120],\n    [50, 46, 223], [134, 180, 135], [194, 48, 0], [171, 232, 107],\n    [150, 101, 109], [37, 14, 53], [166, 0, 25], [0, 128, 207],\n    [202, 239, 255], [50, 63, 97], [164, 73, 220], [106, 157, 59],\n    [255, 90, 228], [99, 106, 1], [209, 108, 218], [115, 96, 96],\n    [255, 186, 173], [211, 105, 180], [255, 222, 214], [108, 109, 116],\n    [146, 125, 94], [132, 93, 112], [91, 98, 193], [47, 74, 54],\n    [228, 95, 53], [255, 59, 83], [172, 132, 221], [118, 41, 136],\n    [112, 236, 152], [64, 133, 67], [44, 53, 51], [46, 24, 45],\n    [50, 57, 37], [25, 24, 27], [47, 46, 44], [2, 60, 50],\n    [155, 158, 226], [88, 175, 173], [92, 66, 77], [122, 197, 166],\n    [104, 93, 117], [185, 188, 189], [131, 67, 87], [26, 123, 66],\n    [46, 87, 170], [229, 81, 153], [49, 110, 71], [205, 0, 197],\n    [106, 0, 77], [127, 187, 236], [243, 86, 145], [215, 197, 74],\n    [98, 172, 183], [203, 161, 188], [162, 138, 154], [108, 63, 59],\n    [255, 228, 125], [220, 186, 227], [95, 129, 109], [58, 64, 74],\n    [125, 191, 50], [230, 236, 220], [133, 44, 25], [40, 83, 102],\n    [184, 203, 156], [14, 13, 0], [75, 93, 86], [107, 84, 63],\n    [226, 113, 114], [5, 104, 236], [46, 181, 0], [210, 22, 86],\n    [239, 175, 255], [104, 32, 33], [45, 32, 17], [218, 76, 255],\n    [112, 150, 142], [255, 123, 125], [74, 25, 48], [232, 194, 130],\n    [231, 219, 188], [166, 132, 134], [31, 38, 60], [54, 87, 78],\n    [82, 206, 121], [173, 170, 169], [138, 159, 69], [101, 66, 210],\n    [0, 251, 140], [93, 105, 123], [204, 210, 127], [148, 165, 161],\n    [121, 2, 41], [227, 131, 230], [126, 164, 193], [78, 68, 82],\n    [75, 44, 0], [98, 11, 112], [49, 76, 30], [135, 74, 166],\n    [227, 0, 145], [102, 70, 10], [235, 154, 139], [234, 195, 163],\n    [152, 234, 179], [171, 145, 128], [184, 85, 47], [26, 43, 47],\n    [148, 221, 197], [157, 140, 118], [156, 131, 51], [148, 169, 201],\n    [57, 41, 53], [140, 103, 94], [204, 233, 58], [145, 113, 0],\n    [1, 64, 11], [68, 152, 150], [28, 163, 112], [224, 141, 167],\n    [139, 74, 78], [102, 119, 118], [70, 146, 173], [103, 189, 168],\n    [105, 37, 92], [211, 191, 255], [74, 81, 50], [126, 146, 133],\n    [119, 115, 60], [231, 160, 204], [81, 162, 136], [44, 101, 106],\n    [77, 92, 94], [201, 64, 58], [221, 215, 243], [0, 88, 68],\n    [180, 162, 0], [72, 143, 105], [133, 129, 130], [212, 233, 185],\n    [61, 115, 151], [202, 232, 206], [214, 0, 52], [170, 103, 70],\n    [158, 85, 133], [186, 98, 0]\n]\n\nhigh_contrast_arr = numpy.array(high_contrast, dtype=numpy.uint8)\n"""
netdissect/server.py,0,"b'#!/usr/bin/env python\n\nimport argparse, connexion, os, sys, yaml, json, socket\nfrom netdissect.easydict import EasyDict\nfrom flask import send_from_directory, redirect\nfrom flask_cors import CORS\n\n\nfrom netdissect.serverstate import DissectionProject\n\n__author__ = \'Hendrik Strobelt, David Bau\'\n\nCONFIG_FILE_NAME = \'dissect.json\'\nprojects = {}\n\napp = connexion.App(__name__, debug=False)\n\n\ndef get_all_projects():\n    res = []\n    for key, project in projects.items():\n        # print key\n        res.append({\n            \'project\': key,\n            \'info\': {\n              \'layers\': [layer[\'layer\'] for layer in project.get_layers()]\n            }\n        })\n    return sorted(res, key=lambda x: x[\'project\'])\n\ndef get_layers(project):\n    return {\n        \'request\': {\'project\': project},\n        \'res\': projects[project].get_layers()\n    }\n\ndef get_units(project, layer):\n    return {\n        \'request\': {\'project\': project, \'layer\': layer},\n        \'res\': projects[project].get_units(layer)\n    }\n\ndef get_rankings(project, layer):\n    return {\n        \'request\': {\'project\': project, \'layer\': layer},\n        \'res\': projects[project].get_rankings(layer)\n    }\n\ndef get_levels(project, layer, quantiles):\n    return {\n        \'request\': {\'project\': project, \'layer\': layer, \'quantiles\': quantiles},\n        \'res\': projects[project].get_levels(layer, quantiles)\n    }\n\ndef get_channels(project, layer):\n    answer = dict(channels=projects[project].get_channels(layer))\n    return {\n        \'request\': {\'project\': project, \'layer\': layer},\n        \'res\': answer\n    }\n\ndef post_generate(gen_req):\n    project = gen_req[\'project\']\n    zs = gen_req.get(\'zs\', None)\n    ids = gen_req.get(\'ids\', None)\n    return_urls = gen_req.get(\'return_urls\', False)\n    assert (zs is None) != (ids is None) # one or the other, not both\n    ablations = gen_req.get(\'ablations\', [])\n    interventions = gen_req.get(\'interventions\', None)\n    # no z avilable if ablations\n    generated = projects[project].generate_images(zs, ids, interventions,\n            return_urls=return_urls)\n    return {\n        \'request\': gen_req,\n        \'res\': generated\n    }\n\ndef post_features(feat_req):\n    project = feat_req[\'project\']\n    ids = feat_req[\'ids\']\n    masks = feat_req.get(\'masks\', None)\n    layers = feat_req.get(\'layers\', None)\n    interventions = feat_req.get(\'interventions\', None)\n    features = projects[project].get_features(\n            ids, masks, layers, interventions)\n    return {\n        \'request\': feat_req,\n        \'res\': features\n    }\n\ndef post_featuremaps(feat_req):\n    project = feat_req[\'project\']\n    ids = feat_req[\'ids\']\n    layers = feat_req.get(\'layers\', None)\n    interventions = feat_req.get(\'interventions\', None)\n    featuremaps = projects[project].get_featuremaps(\n            ids, layers, interventions)\n    return {\n        \'request\': feat_req,\n        \'res\': featuremaps\n    }\n\n@app.route(\'/client/<path:path>\')\ndef send_static(path):\n    """""" serves all files from ./client/ to ``/client/<path:path>``\n\n    :param path: path from api call\n    """"""\n    return send_from_directory(args.client, path)\n\n@app.route(\'/data/<path:path>\')\ndef send_data(path):\n    """""" serves all files from the data dir to ``/dissect/<path:path>``\n\n    :param path: path from api call\n    """"""\n    print(\'Got the data route for\', path)\n    return send_from_directory(args.data, path)\n\n\n@app.route(\'/\')\ndef redirect_home():\n    return redirect(\'/client/index.html\', code=302)\n\n\ndef load_projects(directory):\n    """"""\n    searches for CONFIG_FILE_NAME in all subdirectories of directory\n    and creates data handlers for all of them\n\n    :param directory: scan directory\n    :return: null\n    """"""\n    project_dirs = []\n    # Don\'t search more than 2 dirs deep.\n    search_depth = 2 + directory.count(os.path.sep)\n    for root, dirs, files in os.walk(directory):\n        if CONFIG_FILE_NAME in files:\n            project_dirs.append(root)\n            # Don\'t get subprojects under a project dir.\n            del dirs[:]\n        elif root.count(os.path.sep) >= search_depth:\n            del dirs[:]\n    for p_dir in project_dirs:\n        print(\'Loading %s\' % os.path.join(p_dir, CONFIG_FILE_NAME))\n        with open(os.path.join(p_dir, CONFIG_FILE_NAME), \'r\') as jf:\n            config = EasyDict(json.load(jf))\n            dh_id = os.path.split(p_dir)[1]\n            projects[dh_id] = DissectionProject(\n                    config=config,\n                    project_dir=p_dir,\n                    path_url=\'data/\' + os.path.relpath(p_dir, directory),\n                    public_host=args.public_host)\n\napp.add_api(\'server.yaml\')\n\n# add CORS support\nCORS(app.app, headers=\'Content-Type\')\n\nparser = argparse.ArgumentParser()\nparser.add_argument(""--nodebug"", default=False)\nparser.add_argument(""--address"", default=""127.0.0.1"") # 0.0.0.0 for nonlocal use\nparser.add_argument(""--port"", default=""5001"")\nparser.add_argument(""--public_host"", default=None)\nparser.add_argument(""--nocache"", default=False)\nparser.add_argument(""--data"", type=str, default=\'dissect\')\nparser.add_argument(""--client"", type=str, default=\'client_dist\')\n\nif __name__ == \'__main__\':\n    args = parser.parse_args()\n    for d in [args.data, args.client]:\n        if not os.path.isdir(d):\n            print(\'No directory %s\' % d)\n            sys.exit(1)\n    args.data = os.path.abspath(args.data)\n    args.client = os.path.abspath(args.client)\n    if args.public_host is None:\n        args.public_host = \'%s:%d\' % (socket.getfqdn(), int(args.port))\n    app.run(port=int(args.port), debug=not args.nodebug, host=args.address,\n            use_reloader=False)\nelse:\n    args, _ = parser.parse_known_args()\n    if args.public_host is None:\n        args.public_host = \'%s:%d\' % (socket.getfqdn(), int(args.port))\n    load_projects(args.data)\n'"
netdissect/serverstate.py,26,"b'import os, torch, numpy, base64, json, re, threading, random\nfrom torch.utils.data import TensorDataset, DataLoader\nfrom collections import defaultdict\nfrom netdissect.easydict import EasyDict\nfrom netdissect.modelconfig import create_instrumented_model\nfrom netdissect.runningstats import RunningQuantile\nfrom netdissect.dissection import safe_dir_name\nfrom netdissect.zdataset import z_sample_for_model\nfrom PIL import Image\nfrom io import BytesIO\n\nclass DissectionProject:\n    \'\'\'\n    DissectionProject understand how to drive a GanTester within a\n    dissection project directory structure: it caches data in files,\n    creates image files, and translates data between plain python data\n    types and the pytorch-specific tensors required by GanTester.\n    \'\'\'\n    def __init__(self, config, project_dir, path_url, public_host):\n        print(\'config done\', project_dir)\n        self.use_cuda = torch.cuda.is_available()\n        self.dissect = config\n        self.project_dir = project_dir\n        self.path_url = path_url\n        self.public_host = public_host\n        self.cachedir = os.path.join(self.project_dir, \'cache\')\n        self.tester = GanTester(\n                config.settings, dissectdir=project_dir,\n                device=torch.device(\'cuda\') if self.use_cuda\n                     else torch.device(\'cpu\'))\n        self.stdz = []\n\n    def get_zs(self, size):\n        if size <= len(self.stdz):\n            return self.stdz[:size].tolist()\n        z_tensor = self.tester.standard_z_sample(size)\n        numpy_z = z_tensor.cpu().numpy()\n        self.stdz = numpy_z\n        return self.stdz.tolist()\n\n    def get_z(self, id):\n        if id < len(self.stdz):\n            return self.stdz[id]\n        return self.get_zs((id + 1) * 2)[id]\n\n    def get_zs_for_ids(self, ids):\n        max_id = max(ids)\n        if max_id >= len(self.stdz):\n            self.get_z(max_id)\n        return self.stdz[ids]\n\n    def get_layers(self):\n        result = []\n        layer_shapes = self.tester.layer_shapes()\n        for layer in self.tester.layers:\n            shape = layer_shapes[layer]\n            result.append(dict(\n                layer=layer,\n                channels=shape[1],\n                shape=[shape[2], shape[3]]))\n        return result\n\n    def get_units(self, layer):\n        try:\n            dlayer = [dl for dl in self.dissect[\'layers\']\n                    if dl[\'layer\'] == layer][0]\n        except:\n            return None\n\n        dunits = dlayer[\'units\']\n        result = [dict(unit=unit_num,\n                       img=\'/%s/%s/s-image/%d-top.jpg\' %\n                        (self.path_url, layer, unit_num),\n                       label=unit[\'iou_label\'])\n                  for unit_num, unit in enumerate(dunits)]\n        return result\n\n    def get_rankings(self, layer):\n        try:\n            dlayer = [dl for dl in self.dissect[\'layers\']\n                    if dl[\'layer\'] == layer][0]\n        except:\n            return None\n        result = [dict(name=ranking[\'name\'],\n                       metric=ranking.get(\'metric\', None),\n                       scores=ranking[\'score\'])\n                  for ranking in dlayer[\'rankings\']]\n        return result\n\n    def get_levels(self, layer, quantiles):\n        levels = self.tester.levels(\n                layer, torch.from_numpy(numpy.array(quantiles)))\n        return levels.cpu().numpy().tolist()\n\n    def generate_images(self, zs, ids, interventions, return_urls=False):\n        if ids is not None:\n            assert zs is None\n            zs = self.get_zs_for_ids(ids)\n            if not interventions:\n                # Do file caching when ids are given (and no ablations).\n                imgdir = os.path.join(self.cachedir, \'img\', \'id\')\n                os.makedirs(imgdir, exist_ok=True)\n                exist = set(os.listdir(imgdir))\n                unfinished = [(\'%d.jpg\' % id) not in exist for id in ids]\n                needed_z_tensor = torch.tensor(zs[unfinished]).float().to(\n                        self.tester.device)\n                needed_ids = numpy.array(ids)[unfinished]\n                # Generate image files for just the needed images.\n                if len(needed_z_tensor):\n                    imgs = self.tester.generate_images(needed_z_tensor\n                            ).cpu().numpy()\n                    for i, img in zip(needed_ids, imgs):\n                         Image.fromarray(img.transpose(1, 2, 0)).save(\n                                 os.path.join(imgdir, \'%d.jpg\' % i), \'jpeg\',\n                                 quality=99, optimize=True, progressive=True)\n                # Assemble a response.\n                imgurls = [\'/%s/cache/img/id/%d.jpg\'\n                      % (self.path_url, i) for i in ids]\n                return [dict(id=i, d=d) for i, d in zip(ids, imgurls)]\n        # No file caching when ids are not given (or ablations are applied)\n        z_tensor = torch.tensor(zs).float().to(self.tester.device)\n        imgs = self.tester.generate_images(z_tensor,\n                intervention=decode_intervention_array(interventions,\n                    self.tester.layer_shapes()),\n                ).cpu().numpy()\n        numpy_z = z_tensor.cpu().numpy()\n        if return_urls:\n            randdir = \'%03d\' % random.randrange(1000)\n            imgdir = os.path.join(self.cachedir, \'img\', \'uniq\', randdir)\n            os.makedirs(imgdir, exist_ok=True)\n            startind = random.randrange(100000)\n            imgurls = []\n            for i, img in enumerate(imgs):\n                filename = \'%d.jpg\' % (i + startind)\n                Image.fromarray(img.transpose(1, 2, 0)).save(\n                         os.path.join(imgdir, filename), \'jpeg\',\n                         quality=99, optimize=True, progressive=True)\n                image_url_path = (\'/%s/cache/img/uniq/%s/%s\'\n                      % (self.path_url, randdir, filename))\n                imgurls.append(image_url_path)\n                tweet_filename = \'tweet-%d.html\' % (i + startind)\n                tweet_url_path = (\'/%s/cache/img/uniq/%s/%s\'\n                      % (self.path_url, randdir, tweet_filename))\n                with open(os.path.join(imgdir, tweet_filename), \'w\') as f:\n                    f.write(twitter_card(image_url_path, tweet_url_path,\n                        self.public_host))\n            return [dict(d=d) for d in imgurls]\n        imgurls = [img2base64(img.transpose(1, 2, 0)) for img in imgs]\n        return [dict(d=d) for d in imgurls]\n\n    def get_features(self, ids, masks, layers, interventions):\n        zs = self.get_zs_for_ids(ids)\n        z_tensor = torch.tensor(zs).float().to(self.tester.device)\n        t_masks = torch.stack(\n                [torch.from_numpy(mask_to_numpy(mask)) for mask in masks]\n                )[:,None,:,:].to(self.tester.device)\n        t_features = self.tester.feature_stats(z_tensor, t_masks,\n                decode_intervention_array(interventions,\n                    self.tester.layer_shapes()), layers)\n        # Convert torch arrays to plain python lists before returning.\n        return { layer: { key: value.cpu().numpy().tolist()\n                          for key, value in feature.items() }\n                 for layer, feature in t_features.items() }\n\n    def get_featuremaps(self, ids, layers, interventions):\n        zs = self.get_zs_for_ids(ids)\n        z_tensor = torch.tensor(zs).float().to(self.tester.device)\n        # Quantilized features are returned.\n        q_features = self.tester.feature_maps(z_tensor,\n                decode_intervention_array(interventions,\n                    self.tester.layer_shapes()), layers)\n        # Scale them 0-255 and return them.\n        # TODO: turn them into pngs for returning.\n        return { layer: [\n            value.clamp(0, 1).mul(255).byte().cpu().numpy().tolist()\n            for value in valuelist ]\n            for layer, valuelist in q_features.items()\n            if (not layers) or (layer in layers) }\n\n    def get_recipes(self):\n        recipedir = os.path.join(self.project_dir, \'recipe\')\n        if not os.path.isdir(recipedir):\n            return []\n        result = []\n        for filename in os.listdir(recipedir):\n            with open(os.path.join(recipedir, filename)) as f:\n                result.append(json.load(f))\n        return result\n\n\n\n\nclass GanTester:\n    \'\'\'\n    GanTester holds on to a specific model to test.\n\n    (1) loads and instantiates the GAN;\n    (2) instruments it at every layer so that units can be ablated\n    (3) precomputes z dimensionality, and output image dimensions.\n    \'\'\'\n    def __init__(self, args, dissectdir=None, device=None):\n        self.cachedir = os.path.join(dissectdir, \'cache\')\n        self.device = device if device is not None else torch.device(\'cpu\')\n        self.dissectdir = dissectdir\n        self.modellock = threading.Lock()\n\n        # Load the generator from the pth file.\n        args_copy = EasyDict(args)\n        args_copy.edit = True\n        model = create_instrumented_model(args_copy)\n        model.eval()\n        self.model = model\n\n        # Get the set of layers of interest.\n        # Default: all shallow children except last.\n        self.layers = sorted(model.retained_features().keys())\n\n        # Move it to CUDA if wanted.\n        model.to(device)\n\n        self.quantiles = {\n            layer: load_quantile_if_present(os.path.join(self.dissectdir,\n                safe_dir_name(layer)), \'quantiles.npz\',\n                device=torch.device(\'cpu\'))\n            for layer in self.layers }\n\n    def layer_shapes(self):\n        return self.model.feature_shape\n\n    def standard_z_sample(self, size=100, seed=1, device=None):\n        \'\'\'\n        Generate a standard set of random Z as a (size, z_dimension) tensor.\n        With the same random seed, it always returns the same z (e.g.,\n        the first one is always the same regardless of the size.)\n        \'\'\'\n        result = z_sample_for_model(self.model, size)\n        if device is not None:\n            result = result.to(device)\n        return result\n\n    def reset_intervention(self):\n        self.model.remove_edits()\n\n    def apply_intervention(self, intervention):\n        \'\'\'\n        Applies an ablation recipe of the form [(layer, unit, alpha)...].\n        \'\'\'\n        self.reset_intervention()\n        if not intervention:\n            return\n        for layer, (a, v) in intervention.items():\n            self.model.edit_layer(layer, ablation=a, replacement=v)\n\n    def generate_images(self, z_batch, intervention=None):\n        \'\'\'\n        Makes some images.\n        \'\'\'\n        with torch.no_grad(), self.modellock:\n            batch_size = 10\n            self.apply_intervention(intervention)\n            test_loader = DataLoader(TensorDataset(z_batch[:,:,None,None]),\n                batch_size=batch_size,\n                pin_memory=(\'cuda\' == self.device.type\n                            and z_batch.device.type == \'cpu\'))\n            result_img = torch.zeros(\n                    *((len(z_batch), 3) + self.model.output_shape[2:]),\n                    dtype=torch.uint8, device=self.device)\n            for batch_num, [batch_z,] in enumerate(test_loader):\n                batch_z = batch_z.to(self.device)\n                out = self.model(batch_z)\n                result_img[batch_num*batch_size:\n                        batch_num*batch_size+len(batch_z)] = (\n                                (((out + 1) / 2) * 255).clamp(0, 255).byte())\n            return result_img\n\n    def get_layers(self):\n        return self.layers\n\n    def feature_stats(self, z_batch,\n            masks=None, intervention=None, layers=None):\n        feature_stat = defaultdict(dict)\n        with torch.no_grad(), self.modellock:\n            batch_size = 10\n            self.apply_intervention(intervention)\n            if masks is None:\n                masks = torch.ones(z_batch.size(0), 1, 1, 1,\n                        device=z_batch.device, dtype=z_batch.dtype)\n            else:\n                assert masks.shape[0] == z_batch.shape[0]\n                assert masks.shape[1] == 1\n            test_loader = DataLoader(\n                TensorDataset(z_batch[:,:,None,None], masks),\n                batch_size=batch_size,\n                pin_memory=(\'cuda\' == self.device.type\n                    and z_batch.device.type == \'cpu\'))\n            processed = 0\n            for batch_num, [batch_z, batch_m] in enumerate(test_loader):\n                batch_z, batch_m = [\n                        d.to(self.device) for d in [batch_z, batch_m]]\n                # Run model but disregard output\n                self.model(batch_z)\n                processing = batch_z.shape[0]\n                for layer, feature in self.model.retained_features().items():\n                    if layers is not None:\n                        if layer not in layers:\n                            continue\n                    # Compute max features touching mask\n                    resized_max = torch.nn.functional.adaptive_max_pool2d(\n                            batch_m,\n                            (feature.shape[2], feature.shape[3]))\n                    max_feature = (feature * resized_max).view(\n                            feature.shape[0], feature.shape[1], -1\n                            ).max(2)[0].max(0)[0]\n                    if \'max\' not in feature_stat[layer]:\n                        feature_stat[layer][\'max\'] = max_feature\n                    else:\n                        torch.max(feature_stat[layer][\'max\'], max_feature,\n                                    out=feature_stat[layer][\'max\'])\n                    # Compute mean features weighted by overlap with mask\n                    resized_mean = torch.nn.functional.adaptive_avg_pool2d(\n                            batch_m,\n                            (feature.shape[2], feature.shape[3]))\n                    mean_feature = (feature * resized_mean).view(\n                            feature.shape[0], feature.shape[1], -1\n                            ).sum(2).sum(0) / (resized_mean.sum() + 1e-15)\n                    if \'mean\' not in feature_stat[layer]:\n                        feature_stat[layer][\'mean\'] = mean_feature\n                    else:\n                        feature_stat[layer][\'mean\'] = (\n                                processed * feature_mean[layer][\'mean\']\n                                + processing * mean_feature) / (\n                                        processed + processing)\n                processed += processing\n            # After summaries are done, also compute quantile stats\n            for layer, stats in feature_stat.items():\n                if self.quantiles.get(layer, None) is not None:\n                    for statname in [\'max\', \'mean\']:\n                        stats[\'%s_quantile\' % statname] = (\n                            self.quantiles[layer].normalize(stats[statname]))\n        return feature_stat\n\n    def levels(self, layer, quantiles):\n        return self.quantiles[layer].quantiles(quantiles)\n\n    def feature_maps(self, z_batch, intervention=None, layers=None,\n            quantiles=True):\n        feature_map = defaultdict(list)\n        with torch.no_grad(), self.modellock:\n            batch_size = 10\n            self.apply_intervention(intervention)\n            test_loader = DataLoader(\n                TensorDataset(z_batch[:,:,None,None]),\n                batch_size=batch_size,\n                pin_memory=(\'cuda\' == self.device.type\n                    and z_batch.device.type == \'cpu\'))\n            processed = 0\n            for batch_num, [batch_z] in enumerate(test_loader):\n                batch_z = batch_z.to(self.device)\n                # Run model but disregard output\n                self.model(batch_z)\n                processing = batch_z.shape[0]\n                for layer, feature in self.model.retained_features().items():\n                    for single_featuremap in feature:\n                        if quantiles:\n                            feature_map[layer].append(self.quantiles[layer]\n                                    .normalize(single_featuremap))\n                        else:\n                            feature_map[layer].append(single_featuremap)\n        return feature_map\n\ndef load_quantile_if_present(outdir, filename, device):\n    filepath = os.path.join(outdir, filename)\n    if os.path.isfile(filepath):\n        data = numpy.load(filepath)\n        result = RunningQuantile(state=data)\n        result.to_(device)\n        return result\n    return None\n\nif __name__ == \'__main__\':\n    test_main()\n\ndef mask_to_numpy(mask_record):\n    # Detect a png image mask.\n    bitstring = mask_record[\'bitstring\']\n    bitnumpy = None\n    default_shape = (256, 256)\n    if \'image/png;base64,\' in bitstring:\n        bitnumpy = base642img(bitstring)\n        default_shape = bitnumpy.shape[:2]\n    # Set up results\n    shape = mask_record.get(\'shape\', None)\n    if not shape: # None or empty []\n        shape = default_shape\n    result = numpy.zeros(shape=shape, dtype=numpy.float32)\n    bitbounds = mask_record.get(\'bitbounds\', None)\n    if not bitbounds: # None or empty []\n        bitbounds = ([0] * len(result.shape)) + list(result.shape)\n    start = bitbounds[:len(result.shape)]\n    end = bitbounds[len(result.shape):]\n    if bitnumpy is not None:\n        if bitnumpy.shape[2] == 4:\n            # Mask is any nontransparent bits in the alpha channel if present\n            result[start[0]:end[0], start[1]:end[1]] = (bitnumpy[:,:,3] > 0)\n        else:\n            # Or any nonwhite pixels in the red channel if no alpha.\n            result[start[0]:end[0], start[1]:end[1]] = (bitnumpy[:,:,0] < 255)\n        return result\n    else:\n        # Or bitstring can be just ones and zeros.\n        indexes = start.copy()\n        bitindex = 0\n        while True:\n            result[tuple(indexes)] = (bitstring[bitindex] != \'0\')\n            for ii in range(len(indexes) - 1, -1, -1):\n                if indexes[ii] < end[ii] - 1:\n                    break\n                indexes[ii] = start[ii]\n            else:\n                assert (bitindex + 1) == len(bitstring)\n                return result\n            indexes[ii] += 1\n            bitindex += 1\n\ndef decode_intervention_array(interventions, layer_shapes):\n    result = {}\n    for channels in [decode_intervention(intervention, layer_shapes)\n            for intervention in (interventions or [])]:\n        for layer, channel in channels.items():\n            if layer not in result:\n                result[layer] = channel\n                continue\n            accum = result[layer]\n            newalpha = 1 - (1 - channel[:1]) * (1 - accum[:1])\n            newvalue = (accum[1:] * accum[:1] * (1 - channel[:1]) +\n                    channel[1:] * channel[:1]) / (newalpha + 1e-40)\n            accum[:1] = newalpha\n            accum[1:] = newvalue\n    return result\n\ndef decode_intervention(intervention, layer_shapes):\n    # Every plane of an intervention is a solid choice of activation\n    # over a set of channels, with a mask applied to alpha-blended channels\n    # (when the mask resolution is different from the feature map, it can\n    # be either a max-pooled or average-pooled to the proper resolution).\n    # This can be reduced to a single alpha-blended featuremap.\n    if intervention is None:\n        return None\n    mask = intervention.get(\'mask\', None)\n    if mask:\n        mask = torch.from_numpy(mask_to_numpy(mask))\n    maskpooling = intervention.get(\'maskpooling\', \'max\')\n    channels = {}  # layer -> ([alpha, val], c)\n    for arec in intervention.get(\'ablations\', []):\n        unit = arec[\'unit\']\n        layer = arec[\'layer\']\n        alpha = arec.get(\'alpha\', 1.0)\n        if alpha is None:\n            alpha = 1.0\n        value = arec.get(\'value\', 0.0)\n        if value is None:\n            value = 0.0\n        if alpha != 0.0 or value != 0.0:\n            if layer not in channels:\n                channels[layer] = torch.zeros(2, *layer_shapes[layer][1:])\n            channels[layer][0, unit] = alpha\n            channels[layer][1, unit] = value\n    if mask is not None:\n        for layer in channels:\n            layer_shape = layer_shapes[layer][2:]\n            if maskpooling == \'mean\':\n                layer_mask = torch.nn.functional.adaptive_avg_pool2d(\n                    mask[None,None,...], layer_shape)[0]\n            else:\n                layer_mask = torch.nn.functional.adaptive_max_pool2d(\n                    mask[None,None,...], layer_shape)[0]\n            channels[layer][0] *= layer_mask\n    return channels\n\ndef img2base64(imgarray, for_html=True, image_format=\'jpeg\'):\n    \'\'\'\n    Converts a numpy array to a jpeg base64 url\n    \'\'\'\n    input_image_buff = BytesIO()\n    Image.fromarray(imgarray).save(input_image_buff, image_format,\n            quality=99, optimize=True, progressive=True)\n    res = base64.b64encode(input_image_buff.getvalue()).decode(\'ascii\')\n    if for_html:\n        return \'data:image/\' + image_format + \';base64,\' + res\n    else:\n        return res\n\ndef base642img(stringdata):\n    stringdata = re.sub(\'^(?:data:)?image/\\w+;base64,\', \'\', stringdata)\n    im = Image.open(BytesIO(base64.b64decode(stringdata)))\n    return numpy.array(im)\n\ndef twitter_card(image_path, tweet_path, public_host):\n    return \'\'\'\\\n<!doctype html>\n<html>\n<head>\n<meta name=""twitter:card"" content=""summary_large_image"" />\n<meta name=""twitter:title"" content=""Painting with GANs from MIT-IBM Watson AI Lab"" />\n<meta name=""twitter:description"" content=""This demo lets you modify a selection of meaningful GAN units for a generated image by simply painting."" />\n<meta name=""twitter:image"" content=""http://{public_host}{image_path}"" />\n<meta name=""twitter:url"" content=""http://{public_host}{tweet_path}"" />\n<meta http-equiv=""refresh"" content=""10; url=http://bit.ly/ganpaint"">\n</head>\n<style>\nbody {{ font: 12px Arial, sans-serif; }}\n</style>\n<body>\n<center>\n<h1>Painting with GANs from MIT-IBM Watson AI Lab</h1>\n<p>This demo lets you modify a selection of meatningful GAN units for a generated image by simply painting.</p>\n<img src=""{image_path}"">\n<p>Redirecting to\n<a href=""http://bit.ly/ganpaint"">GANPaint</a>\n</p>\n</center>\n</body>\n\'\'\'.format(\n        image_path=image_path,\n        tweet_path=tweet_path,\n        public_host=public_host)\n'"
netdissect/statedict.py,4,"b""'''\nUtilities for dealing with simple state dicts as npz files instead of pth files.\n'''\n\nimport torch\nfrom collections.abc import MutableMapping, Mapping\n\ndef load_from_numpy_dict(model, numpy_dict, prefix='', examples=None):\n    '''\n    Loads a model from numpy_dict using load_state_dict.\n    Converts numpy types to torch types using the current state_dict\n    of the model to determine types and devices for the tensors.\n    Supports loading a subdict by prepending the given prefix to all keys.\n    '''\n    if prefix:\n        if not prefix.endswith('.'):\n            prefix = prefix + '.'\n        numpy_dict = PrefixSubDict(numpy_dict, prefix)\n    if examples is None:\n        exampels = model.state_dict()\n    torch_state_dict = TorchTypeMatchingDict(numpy_dict, examples)\n    model.load_state_dict(torch_state_dict)\n\ndef save_to_numpy_dict(model, numpy_dict, prefix=''):\n    '''\n    Saves a model by copying tensors to numpy_dict.\n    Converts torch types to numpy types using `t.detach().cpu().numpy()`.\n    Supports saving a subdict by prepending the given prefix to all keys.\n    '''\n    if prefix:\n        if not prefix.endswith('.'):\n            prefix = prefix + '.'\n    for k, v in model.numpy_dict().items():\n        if isinstance(v, torch.Tensor):\n            v = v.detach().cpu().numpy()\n        numpy_dict[prefix + k] = v\n\nclass TorchTypeMatchingDict(Mapping):\n    '''\n    Provides a view of a dict of numpy values as torch tensors, where the\n    types are converted to match the types and devices in the given\n    dict of examples.\n    '''\n    def __init__(self, data, examples):\n        self.data = data\n        self.examples = examples\n        self.cached_data = {}\n    def __getitem__(self, key):\n        if key in self.cached_data:\n            return self.cached_data[key]\n        val = self.data[key]\n        if key not in self.examples:\n            return val\n        example = self.examples.get(key, None)\n        example_type = type(example)\n        if example is not None and type(val) != example_type:\n            if isinstance(example, torch.Tensor):\n                val = torch.from_numpy(val)\n            else:\n                val = example_type(val)\n        if isinstance(example, torch.Tensor):\n            val = val.to(dtype=example.dtype, device=example.device)\n        self.cached_data[key] = val\n        return val\n    def __iter__(self):\n        return self.data.keys()\n    def __len__(self):\n        return len(self.data)\n\nclass PrefixSubDict(MutableMapping):\n    '''\n    Provides a view of the subset of a dict where string keys begin with\n    the given prefix.  The prefix is stripped from all keys of the view.\n    '''\n    def __init__(self, data, prefix=''):\n        self.data = data\n        self.prefix = prefix\n        self._cached_keys = None\n    def __getitem__(self, key):\n        return self.data[self.prefix + key]\n    def __setitem__(self, key, value):\n        pkey = self.prefix + key\n        if self._cached_keys is not None and pkey not in self.data:\n            self._cached_keys = None\n        self.data[pkey] = value\n    def __delitem__(self, key):\n        pkey = self.prefix + key\n        if self._cached_keys is not None and pkey in self.data:\n            self._cached_keys = None\n        del self.data[pkey]\n    def __cached_keys(self):\n        if self._cached_keys is None:\n            plen = len(self.prefix)\n            self._cached_keys = list(k[plen:] for k in self.data\n                    if k.startswith(self.prefix))\n        return self._cached_keys\n    def __iter__(self):\n        return iter(self.__cached_keys())\n    def __len__(self):\n        return len(self.__cached_keys())\n"""
netdissect/workerpool.py,0,"b""'''\nWorkerPool and WorkerBase for handling the common problems in managing\na multiprocess pool of workers that aren't done by multiprocessing.Pool,\nincluding setup with per-process state, debugging by putting the worker\non the main thread, and correct handling of unexpected errors, and ctrl-C.\n\nTo use it,\n1. Put the per-process setup and the per-task work in the\n   setup() and work() methods of your own WorkerBase subclass.\n2. To prepare the process pool, instantiate a WorkerPool, passing your\n   subclass type as the first (worker) argument, as well as any setup keyword\n   arguments.  The WorkerPool will instantiate one of your workers in each\n   worker process (passing in the setup arguments in those processes).\n   If debugging, the pool can have process_count=0 to force all the work\n   to be done immediately on the main thread; otherwise all the work\n   will be passed to other processes.\n3. Whenever there is a new piece of work to distribute, call pool.add(*args).\n   The arguments will be queued and passed as worker.work(*args) to the\n   next available worker.\n4. When all the work has been distributed, call pool.join() to wait for all\n   the work to complete and to finish and terminate all the worker processes.\n   When pool.join() returns, all the work will have been done.\n\nNo arrangement is made to collect the results of the work: for example,\nthe return value of work() is ignored.  If you need to collect the\nresults, use your own mechanism (filesystem, shared memory object, queue)\nwhich can be distributed using setup arguments.\n'''\n\nfrom multiprocessing import Process, Queue, cpu_count\nimport signal\nimport atexit\nimport sys\n\nclass WorkerBase(Process):\n    '''\n    Subclass this class and override its work() method (and optionally,\n    setup() as well) to define the units of work to be done in a process\n    worker in a woker pool.\n    '''\n    def __init__(self, i, process_count, queue, initargs):\n        if process_count > 0:\n            # Make sure we ignore ctrl-C if we are not on main process.\n            signal.signal(signal.SIGINT, signal.SIG_IGN)\n        self.process_id = i\n        self.process_count = process_count\n        self.queue = queue\n        super(WorkerBase, self).__init__()\n        self.setup(**initargs)\n    def run(self):\n        # Do the work until None is dequeued\n        while True:\n            try:\n                work_batch = self.queue.get()\n            except (KeyboardInterrupt, SystemExit):\n                print('Exiting...')\n                break\n            if work_batch is None:\n                self.queue.put(None) # for another worker\n                return\n            self.work(*work_batch)\n    def setup(self, **initargs):\n        '''\n        Override this method for any per-process initialization.\n        Keywoard args are passed from WorkerPool constructor.\n        '''\n        pass\n    def work(self, *args):\n        '''\n        Override this method for one-time initialization.\n        Args are passed from WorkerPool.add() arguments.\n        '''\n        raise NotImplementedError('worker subclass needed')\n\nclass WorkerPool(object):\n    '''\n    Instantiate this object (passing a WorkerBase subclass type\n    as its first argument) to create a worker pool.  Then call\n    pool.add(*args) to queue args to distribute to worker.work(*args),\n    and call pool.join() to wait for all the workers to complete.\n    '''\n    def __init__(self, worker=WorkerBase, process_count=None, **initargs):\n        global active_pools\n        if process_count is None:\n            process_count = cpu_count()\n        if process_count == 0:\n            # zero process_count uses only main process, for debugging.\n            self.queue = None\n            self.processes = None\n            self.worker = worker(None, 0, None, initargs)\n            return\n        # Ctrl-C strategy: worker processes should ignore ctrl-C.  Set\n        # this up to be inherited by child processes before forking.\n        original_sigint_handler = signal.signal(signal.SIGINT, signal.SIG_IGN)\n        active_pools[id(self)] = self\n        self.queue = Queue(maxsize=(process_count * 3))\n        self.processes = None   # Initialize before trying to construct workers\n        self.processes = [worker(i, process_count, self.queue, initargs)\n                for i in range(process_count)]\n        for p in self.processes:\n            p.start()\n        # The main process should handle ctrl-C.  Restore this now.\n        signal.signal(signal.SIGINT, original_sigint_handler)\n    def add(self, *work_batch):\n        if self.queue is None:\n            if hasattr(self, 'worker'):\n                self.worker.work(*work_batch)\n            else:\n                print('WorkerPool shutting down.', file=sys.stderr)\n        else:\n            try:\n                # The queue can block if the work is so slow it gets full.\n                self.queue.put(work_batch)\n            except (KeyboardInterrupt, SystemExit):\n                # Handle ctrl-C if done while waiting for the queue.\n                self.early_terminate()\n    def join(self):\n        # End the queue, and wait for all worker processes to complete nicely.\n        if self.queue is not None:\n            self.queue.put(None)\n            for p in self.processes:\n                p.join()\n            self.queue = None\n        # Remove myself from the set of pools that need cleanup on shutdown.\n        try:\n            del active_pools[id(self)]\n        except:\n            pass\n    def early_terminate(self):\n        # When shutting down unexpectedly, first end the queue.\n        if self.queue is not None:\n            try:\n                self.queue.put_nowait(None)  # Nonblocking put throws if full.\n                self.queue = None\n            except:\n                pass\n        # But then don't wait: just forcibly terminate workers.\n        if self.processes is not None:\n            for p in self.processes:\n                p.terminate()\n            self.processes = None\n        try:\n            del active_pools[id(self)]\n        except:\n            pass\n    def __del__(self):\n        if self.queue is not None:\n            print('ERROR: workerpool.join() not called!', file=sys.stderr)\n            self.join()\n\n# Error and ctrl-C handling: kill worker processes if the main process ends.\nactive_pools = {}\ndef early_terminate_pools():\n    for _, pool in list(active_pools.items()):\n        pool.early_terminate()\n\natexit.register(early_terminate_pools)\n\n"""
netdissect/zdataset.py,5,"b""import os, torch, numpy\nfrom torch.utils.data import TensorDataset\n\ndef z_dataset_for_model(model, size=100, seed=1):\n    return TensorDataset(z_sample_for_model(model, size, seed))\n\ndef z_sample_for_model(model, size=100, seed=1):\n    # If the model is marked with an input shape, use it.\n    if hasattr(model, 'input_shape'):\n        sample = standard_z_sample(size, model.input_shape[1], seed=seed).view(\n                (size,) + model.input_shape[1:])\n        return sample\n    # Examine first conv in model to determine input feature size.\n    first_layer = [c for c in model.modules()\n            if isinstance(c, (torch.nn.Conv2d, torch.nn.ConvTranspose2d,\n                torch.nn.Linear))][0]\n    # 4d input if convolutional, 2d input if first layer is linear.\n    if isinstance(first_layer, (torch.nn.Conv2d, torch.nn.ConvTranspose2d)):\n        sample = standard_z_sample(\n                size, first_layer.in_channels, seed=seed)[:,:,None,None]\n    else:\n        sample = standard_z_sample(\n                size, first_layer.in_features, seed=seed)\n    return sample\n\ndef standard_z_sample(size, depth, seed=1, device=None):\n\t'''\n\tGenerate a standard set of random Z as a (size, z_dimension) tensor.\n\tWith the same random seed, it always returns the same z (e.g.,\n\tthe first one is always the same regardless of the size.)\n\t'''\n\t# Use numpy RandomState since it can be done deterministically\n\t# without affecting global state\n\trng = numpy.random.RandomState(seed)\n\tresult = torch.from_numpy(\n\t\t\trng.standard_normal(size * depth)\n\t\t\t.reshape(size, depth)).float()\n\tif device is not None:\n\t\tresult = result.to(device)\n\treturn result\n\n"""
notebooks/notebook_init.py,4,"b'# Copyright 2020 Erik H\xc3\xa4rk\xc3\xb6nen. All rights reserved.\n# This file is licensed to you under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License. You may obtain a copy\n# of the License at http://www.apache.org/licenses/LICENSE-2.0\n\n# Unless required by applicable law or agreed to in writing, software distributed under\n# the License is distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR REPRESENTATIONS\n# OF ANY KIND, either express or implied. See the License for the specific language\n# governing permissions and limitations under the License.\n\n# Shared init code for noteoboks\n# Usage: from notebook_init import *\n\nimport torch\nimport numpy as np\nfrom os import makedirs\nfrom types import SimpleNamespace\nimport matplotlib.pyplot as plt\nfrom pathlib import Path\nfrom PIL import Image\nimport pickle\n\nimport sys\nsys.path.insert(0, \'..\')\nfrom models import get_instrumented_model, get_model\nfrom notebook_utils import create_strip, create_strip_centered, prettify_name, save_frames, pad_frames\nfrom config import Config\nfrom decomposition import get_or_compute\n\ntorch.autograd.set_grad_enabled(False)\ntorch.backends.cudnn.benchmark = True\n\nhas_gpu = torch.cuda.is_available()\ndevice = torch.device(\'cuda\' if has_gpu else \'cpu\')\ninst = None'"
notebooks/notebook_utils.py,10,"b'# Copyright 2020 Erik H\xc3\xa4rk\xc3\xb6nen. All rights reserved.\n# This file is licensed to you under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License. You may obtain a copy\n# of the License at http://www.apache.org/licenses/LICENSE-2.0\n\n# Unless required by applicable law or agreed to in writing, software distributed under\n# the License is distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR REPRESENTATIONS\n# OF ANY KIND, either express or implied. See the License for the specific language\n# governing permissions and limitations under the License.\n\nimport torch\nimport numpy as np\nfrom os import makedirs\nfrom PIL import Image\n\nimport sys\nfrom pathlib import Path\nsys.path.insert(0, str(Path(__file__).parent.parent))\nfrom utils import prettify_name, pad_frames\n\n# Apply edit to given latents, return strip of images\ndef create_strip(inst, mode, layer, latents, x_comp, z_comp, act_stdev, lat_stdev, sigma, layer_start, layer_end, num_frames=5):\n    return _create_strip_impl(inst, mode, layer, latents, x_comp, z_comp, act_stdev,\n                lat_stdev, None, None, sigma, layer_start, layer_end, num_frames, center=False)\n\n# Strip where the sample is centered along the component before manipulation\ndef create_strip_centered(inst, mode, layer, latents, x_comp, z_comp, act_stdev, lat_stdev, act_mean, lat_mean, sigma, layer_start, layer_end, num_frames=5):\n    return _create_strip_impl(inst, mode, layer, latents, x_comp, z_comp, act_stdev,\n                lat_stdev, act_mean, lat_mean, sigma, layer_start, layer_end, num_frames, center=True)\n\ndef _create_strip_impl(inst, mode, layer, latents, x_comp, z_comp, act_stdev, lat_stdev, act_mean, lat_mean, sigma, layer_start, layer_end, num_frames, center):\n    if not isinstance(latents, list):\n        latents = list(latents)\n\n    max_lat = inst.model.get_max_latents()\n    if layer_end < 0 or layer_end > max_lat:\n        layer_end = max_lat\n    layer_start = np.clip(layer_start, 0, layer_end)\n\n    if len(latents) > num_frames:\n        # Batch over latents\n        return _create_strip_batch_lat(inst, mode, layer, latents, x_comp, z_comp,\n            act_stdev, lat_stdev, act_mean, lat_mean, sigma, layer_start, layer_end, num_frames, center)\n    else:\n        # Batch over strip frames\n        return _create_strip_batch_sigma(inst, mode, layer, latents, x_comp, z_comp,\n            act_stdev, lat_stdev, act_mean, lat_mean, sigma, layer_start, layer_end, num_frames, center)\n\n# Batch over frames if there are more frames in strip than latents\ndef _create_strip_batch_sigma(inst, mode, layer, latents, x_comp, z_comp, act_stdev, lat_stdev, act_mean, lat_mean, sigma, layer_start, layer_end, num_frames, center):    \n    inst.close()\n    batch_frames = [[] for _ in range(len(latents))]\n    \n    B = min(num_frames, 5)\n    lep_padded = ((num_frames - 1) // B + 1) * B\n    sigma_range = np.linspace(-sigma, sigma, num_frames)\n    sigma_range = np.concatenate([sigma_range, np.zeros((lep_padded - num_frames))])\n    sigma_range = torch.from_numpy(sigma_range).float().to(inst.model.device)\n    normalize = lambda v : v / torch.sqrt(torch.sum(v**2, dim=-1, keepdim=True) + 1e-8)\n\n    for i_batch in range(lep_padded // B):\n        sigmas = sigma_range[i_batch*B:(i_batch+1)*B]\n        \n        for i_lat in range(len(latents)):\n            z_single = latents[i_lat]\n            z_batch = z_single.repeat_interleave(B, axis=0)\n            \n            zeroing_offset_act = 0\n            zeroing_offset_lat = 0\n            if center:\n                if mode == \'activation\':\n                    # Center along activation before applying offset\n                    inst.retain_layer(layer)\n                    _ = inst.model.sample_np(z_single)\n                    value = inst.retained_features()[layer].clone()\n                    dotp = torch.sum((value - act_mean)*normalize(x_comp), dim=-1, keepdim=True)\n                    zeroing_offset_act = normalize(x_comp)*dotp # offset that sets coordinate to zero\n                else:\n                    # Shift latent to lie on mean along given component\n                    dotp = torch.sum((z_single - lat_mean)*normalize(z_comp), dim=-1, keepdim=True)\n                    zeroing_offset_lat = dotp*normalize(z_comp)\n\n            with torch.no_grad():\n                z = z_batch\n\n                if mode in [\'latent\', \'both\']:\n                    z = [z]*inst.model.get_max_latents()\n                    delta = z_comp * sigmas.reshape([-1] + [1]*(z_comp.ndim - 1)) * lat_stdev\n                    for i in range(layer_start, layer_end):\n                        z[i] = z[i] - zeroing_offset_lat + delta\n\n                if mode in [\'activation\', \'both\']:\n                    comp_batch = x_comp.repeat_interleave(B, axis=0)\n                    delta = comp_batch * sigmas.reshape([-1] + [1]*(comp_batch.ndim - 1))\n                    inst.edit_layer(layer, offset=delta * act_stdev - zeroing_offset_act)\n\n                img_batch = inst.model.sample_np(z)\n                if img_batch.ndim == 3:\n                    img_batch = np.expand_dims(img_batch, axis=0)\n                    \n                for j, img in enumerate(img_batch):\n                    idx = i_batch*B + j\n                    if idx < num_frames:\n                        batch_frames[i_lat].append(img)\n\n    return batch_frames\n\n# Batch over latents if there are more latents than frames in strip\ndef _create_strip_batch_lat(inst, mode, layer, latents, x_comp, z_comp, act_stdev, lat_stdev, act_mean, lat_mean, sigma, layer_start, layer_end, num_frames, center):    \n    n_lat = len(latents)\n    B = min(n_lat, 5)\n\n    max_lat = inst.model.get_max_latents()\n    if layer_end < 0 or layer_end > max_lat:\n        layer_end = max_lat\n    layer_start = np.clip(layer_start, 0, layer_end)\n    \n    len_padded = ((n_lat - 1) // B + 1) * B\n    batch_frames = [[] for _ in range(n_lat)]\n\n    for i_batch in range(len_padded // B):\n        zs = latents[i_batch*B:(i_batch+1)*B]\n        if len(zs) == 0:\n            continue\n        \n        z_batch_single = torch.cat(zs, 0)\n\n        inst.close() # don\'t retain, remove edits\n        sigma_range = np.linspace(-sigma, sigma, num_frames, dtype=np.float32)\n\n        normalize = lambda v : v / torch.sqrt(torch.sum(v**2, dim=-1, keepdim=True) + 1e-8)\n        \n        zeroing_offset_act = 0\n        zeroing_offset_lat = 0\n        if center:\n            if mode == \'activation\':\n                # Center along activation before applying offset\n                inst.retain_layer(layer)\n                _ = inst.model.sample_np(z_batch_single)\n                value = inst.retained_features()[layer].clone()\n                dotp = torch.sum((value - act_mean)*normalize(x_comp), dim=-1, keepdim=True)\n                zeroing_offset_act = normalize(x_comp)*dotp # offset that sets coordinate to zero\n            else:\n                # Shift latent to lie on mean along given component\n                dotp = torch.sum((z_batch_single - lat_mean)*normalize(z_comp), dim=-1, keepdim=True)\n                zeroing_offset_lat = dotp*normalize(z_comp)\n\n        for i in range(len(sigma_range)):\n            s = sigma_range[i]\n\n            with torch.no_grad():\n                z = [z_batch_single]*inst.model.get_max_latents() # one per layer\n\n                if mode in [\'latent\', \'both\']:\n                    delta = z_comp*s*lat_stdev\n                    for i in range(layer_start, layer_end):\n                        z[i] = z[i] - zeroing_offset_lat + delta\n\n                if mode in [\'activation\', \'both\']:\n                    act_delta = x_comp*s*act_stdev\n                    inst.edit_layer(layer, offset=act_delta - zeroing_offset_act)\n\n                img_batch = inst.model.sample_np(z)\n                if img_batch.ndim == 3:\n                    img_batch = np.expand_dims(img_batch, axis=0)\n                    \n                for j, img in enumerate(img_batch):\n                    img_idx = i_batch*B + j\n                    if img_idx < n_lat:\n                        batch_frames[img_idx].append(img)\n\n    return batch_frames\n\n\ndef save_frames(title, model_name, rootdir, frames, strip_width=10):\n    test_name = prettify_name(title)\n    outdir = f\'{rootdir}/{model_name}/{test_name}\'\n    makedirs(outdir, exist_ok=True)\n    \n    # Limit maximum resolution\n    max_H = 512\n    real_H = frames[0][0].shape[0]\n    ratio = min(1.0, max_H / real_H)\n    \n    # Combined with first 10\n    strips = [np.hstack(frames) for frames in frames[:strip_width]]\n    if len(strips) >= strip_width:\n        left_col = np.vstack(strips[0:strip_width//2])\n        right_col = np.vstack(strips[5:10])\n        grid = np.hstack([left_col, np.ones_like(left_col[:, :30]), right_col])\n        im = Image.fromarray((255*grid).astype(np.uint8))\n        im = im.resize((int(ratio*im.size[0]), int(ratio*im.size[1])), Image.ANTIALIAS)\n        im.save(f\'{outdir}/{test_name}_all.png\')\n    else:\n        print(\'Too few strips to create grid, creating just strips!\')\n    \n    for ex_num, strip in enumerate(frames[:strip_width]):\n        im = Image.fromarray(np.uint8(255*np.hstack(pad_frames(strip))))\n        im = im.resize((int(ratio*im.size[0]), int(ratio*im.size[1])), Image.ANTIALIAS)\n        im.save(f\'{outdir}/{test_name}_{ex_num}.png\')'"
tests/layerwise_z_test.py,8,"b'# Copyright 2020 Erik H\xc3\xa4rk\xc3\xb6nen. All rights reserved.\n# This file is licensed to you under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License. You may obtain a copy\n# of the License at http://www.apache.org/licenses/LICENSE-2.0\n\n# Unless required by applicable law or agreed to in writing, software distributed under\n# the License is distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR REPRESENTATIONS\n# OF ANY KIND, either express or implied. See the License for the specific language\n# governing permissions and limitations under the License.\n\nimport torch, numpy as np\nfrom types import SimpleNamespace\nimport itertools\n\nimport sys\nfrom pathlib import Path\nsys.path.insert(0, str(Path(__file__).parent.parent))\nfrom models import get_model\nfrom config import Config\n\ntorch.backends.cudnn.benchmark = True\nhas_gpu = torch.cuda.is_available()\ndevice = torch.device(\'cuda\' if has_gpu else \'cpu\')\nB = 2 # test batch support\n\nmodels = [\n    (\'BigGAN-128\', \'husky\'),\n    (\'BigGAN-256\', \'husky\'),\n    (\'BigGAN-512\', \'husky\'),\n    (\'StyleGAN\', \'ffhq\'),\n    (\'StyleGAN2\', \'ffhq\'),\n]\n\nfor model_name, classname in models:\n    with torch.no_grad():\n        model = get_model(model_name, classname, device).to(device)\n        print(f\'Testing {model_name}-{classname}\', end=\'\')\n\n        n_latents = model.get_max_latents()\n        assert n_latents > 1, \'Model reports max_latents=1\'\n    \n        #if hasattr(model, \'use_w\'):\n        #    model.use_w()\n\n        seed = 1234\n        torch.manual_seed(seed)\n        np.random.seed(seed)\n        latents = [model.sample_latent(B, seed=seed) for _ in range(10)]\n\n        # Test that partial-forward supports layerwise latent inputs\n        try:\n            layer_name, _ = list(model.named_modules())[-1]\n            _ = model.partial_forward(n_latents*[latents[0]], layer_name)\n        except Exception as e:\n            print(\'Error:\', e)\n            raise RuntimeError(f""{model_name} partial forward doesn\'t support layerwise latent!"")\n\n        # Test that layerwise and single give same result\n        for z in latents:\n            torch.manual_seed(0)\n            np.random.seed(0)\n            out1 = model.forward(z)\n            \n            torch.manual_seed(0)\n            np.random.seed(0)\n            out2 = model.forward(n_latents*[z])\n        \n            dist_rel = (torch.abs(out1 - out2).sum() / out1.sum()).item()\n            assert dist_rel < 1e-3, f\'Layerwise latent mode working incorrectly for model {model_name}-{classname}: difference = {dist_rel*100}%\'\n            \n            print(\'.\', end=\'\')\n    \n    print(\'OK!\')\n\n\n'"
tests/partial_forward_test.py,13,"b'# Copyright 2020 Erik H\xc3\xa4rk\xc3\xb6nen. All rights reserved.\n# This file is licensed to you under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License. You may obtain a copy\n# of the License at http://www.apache.org/licenses/LICENSE-2.0\n\n# Unless required by applicable law or agreed to in writing, software distributed under\n# the License is distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR REPRESENTATIONS\n# OF ANY KIND, either express or implied. See the License for the specific language\n# governing permissions and limitations under the License.\n\nimport torch, numpy as np\nfrom types import SimpleNamespace\nimport itertools\n\nimport sys\nfrom pathlib import Path\nsys.path.insert(0, str(Path(__file__).parent.parent))\nfrom models import get_instrumented_model\n\n\nSEED = 1369\nSAMPLES = 100\nB = 10\n\ntorch.backends.cudnn.benchmark = True\nhas_gpu = torch.cuda.is_available()\ndevice = torch.device(\'cuda\' if has_gpu else \'cpu\')\n\n\ndef compare(model, layer, z1, z2):\n    # Run partial forward\n    torch.manual_seed(0)\n    np.random.seed(0)\n    inst._retained[layer] = None\n    with torch.no_grad():\n        model.partial_forward(z1, layer)\n    \n    assert inst._retained[layer] is not None, \'Layer not retained (partial)\'\n    feat_partial = inst._retained[layer].cpu().numpy().copy().reshape(-1)\n\n    # Run standard forward\n    torch.manual_seed(0)\n    np.random.seed(0)\n    inst._retained[layer] = None\n    with torch.no_grad():\n        model.forward(z2)\n    \n    assert inst._retained[layer] is not None, \'Layer not retained (full)\'\n    feat_full = inst.retained_features()[layer].cpu().numpy().copy().reshape(-1)\n\n    diff = np.sum(np.abs(feat_partial - feat_full))\n    return diff\n\n\nconfigs = []\n\n# StyleGAN2\nmodels = [\'StyleGAN2\']\nlayers = [\'convs.0\',]\nclasses = [\'cat\', \'ffhq\']\nconfigs.append(itertools.product(models, layers, classes))\n\n# StyleGAN\nmodels = [\'StyleGAN\']\nlayers = [\n    \'g_synthesis.blocks.128x128.conv0_up\',\n    \'g_synthesis.blocks.128x128.conv0_up.upscale\',\n    \'g_synthesis.blocks.256x256.conv0_up\',\n    \'g_synthesis.blocks.1024x1024.epi2.style_mod.lin\'\n]\nclasses = [\'ffhq\']\nconfigs.append(itertools.product(models, layers, classes))\n\n# ProGAN\nmodels = [\'ProGAN\']\nlayers = [\'layer2\', \'layer7\']\nclasses = [\'churchoutdoor\', \'bedroom\']\nconfigs.append(itertools.product(models, layers, classes))\n\n# BigGAN\nmodels = [\'BigGAN-512\', \'BigGAN-256\', \'BigGAN-128\']\nlayers = [\'generator.layers.2.conv_1\', \'generator.layers.5.relu\', \'generator.layers.10.bn_2\']\nclasses = [\'husky\']\nconfigs.append(itertools.product(models, layers, classes))\n\n# Run all configurations\nfor config in configs:\n    for model_name, layer, outclass in config:\n        print(\'Testing\', model_name, layer, outclass)\n        inst = get_instrumented_model(model_name, outclass, layer, device)\n        model = inst.model\n\n        # Test negative\n        z_dummy = model.sample_latent(B)\n        z1 = torch.zeros_like(z_dummy).to(device)\n        z2 = torch.ones_like(z_dummy).to(device)\n        diff = compare(model, layer, z1, z2)\n        assert diff > 1e-8, \'Partial and full should differ, but they do not!\'\n\n        # Test model randomness (should be seeded away)\n        z1 = model.sample_latent(1)\n        inst._retained[layer] = None\n        with torch.no_grad():\n            model.forward(z1)\n            feat1 = inst._retained[layer].reshape(-1)\n            model.forward(z1)\n            feat2 = inst._retained[layer].reshape(-1)\n            diff = torch.sum(torch.abs(feat1 - feat2))\n            assert diff < 1e-8, f\'Layer {layer} output contains randomness, diff={diff}\'\n    \n\n        # Test positive\n        torch.manual_seed(SEED)\n        np.random.seed(SEED)\n        latents = model.sample_latent(SAMPLES, seed=SEED)\n\n        for i in range(0, SAMPLES, B):\n            print(f\'Layer {layer}: {i}/{SAMPLES}\', end=\'\\r\')\n            z = latents[i:i+B]\n            diff = compare(model, layer, z, z)\n            assert diff < 1e-8, f\'Partial and full forward differ by {diff}\'\n\n        del model\n        torch.cuda.empty_cache()'"
models/biggan/__init__.py,0,"b""from pathlib import Path\nimport sys\n\nmodule_path = Path(__file__).parent / 'pytorch_biggan'\nsys.path.append(str(module_path.resolve()))\nfrom pytorch_pretrained_biggan import *\nfrom pytorch_pretrained_biggan.model import GenBlock\nfrom pytorch_pretrained_biggan.file_utils import http_get, s3_get"""
models/stylegan/__init__.py,0,"b'# Copyright 2020 Erik H\xc3\xa4rk\xc3\xb6nen. All rights reserved.\n# This file is licensed to you under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License. You may obtain a copy\n# of the License at http://www.apache.org/licenses/LICENSE-2.0\n\n# Unless required by applicable law or agreed to in writing, software distributed under\n# the License is distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR REPRESENTATIONS\n# OF ANY KIND, either express or implied. See the License for the specific language\n# governing permissions and limitations under the License.\n\nfrom pathlib import Path\nimport sys\n\n#module_path = Path(__file__).parent / \'pytorch_biggan\'\n#sys.path.append(str(module_path.resolve()))\n\nfrom .model import StyleGAN_G, NoiseLayer'"
models/stylegan/model.py,24,"b'# Copyright 2020 Erik H\xc3\xa4rk\xc3\xb6nen. All rights reserved.\n# This file is licensed to you under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License. You may obtain a copy\n# of the License at http://www.apache.org/licenses/LICENSE-2.0\n\n# Unless required by applicable law or agreed to in writing, software distributed under\n# the License is distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR REPRESENTATIONS\n# OF ANY KIND, either express or implied. See the License for the specific language\n# governing permissions and limitations under the License.\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom collections import OrderedDict\nfrom pathlib import Path\nimport requests\nimport pickle\nimport sys\n\nimport numpy as np\n\n# Reimplementation of StyleGAN in PyTorch\n# Source: https://github.com/lernapparat/lernapparat/blob/master/style_gan/pytorch_style_gan.ipynb\n\nclass MyLinear(nn.Module):\n    """"""Linear layer with equalized learning rate and custom learning rate multiplier.""""""\n    def __init__(self, input_size, output_size, gain=2**(0.5), use_wscale=False, lrmul=1, bias=True):\n        super().__init__()\n        he_std = gain * input_size**(-0.5) # He init\n        # Equalized learning rate and custom learning rate multiplier.\n        if use_wscale:\n            init_std = 1.0 / lrmul\n            self.w_mul = he_std * lrmul\n        else:\n            init_std = he_std / lrmul\n            self.w_mul = lrmul\n        self.weight = torch.nn.Parameter(torch.randn(output_size, input_size) * init_std)\n        if bias:\n            self.bias = torch.nn.Parameter(torch.zeros(output_size))\n            self.b_mul = lrmul\n        else:\n            self.bias = None\n\n    def forward(self, x):\n        bias = self.bias\n        if bias is not None:\n            bias = bias * self.b_mul\n        return F.linear(x, self.weight * self.w_mul, bias)\n\nclass MyConv2d(nn.Module):\n    """"""Conv layer with equalized learning rate and custom learning rate multiplier.""""""\n    def __init__(self, input_channels, output_channels, kernel_size, gain=2**(0.5), use_wscale=False, lrmul=1, bias=True,\n                intermediate=None, upscale=False):\n        super().__init__()\n        if upscale:\n            self.upscale = Upscale2d()\n        else:\n            self.upscale = None\n        he_std = gain * (input_channels * kernel_size ** 2) ** (-0.5) # He init\n        self.kernel_size = kernel_size\n        if use_wscale:\n            init_std = 1.0 / lrmul\n            self.w_mul = he_std * lrmul\n        else:\n            init_std = he_std / lrmul\n            self.w_mul = lrmul\n        self.weight = torch.nn.Parameter(torch.randn(output_channels, input_channels, kernel_size, kernel_size) * init_std)\n        if bias:\n            self.bias = torch.nn.Parameter(torch.zeros(output_channels))\n            self.b_mul = lrmul\n        else:\n            self.bias = None\n        self.intermediate = intermediate\n\n    def forward(self, x):\n        bias = self.bias\n        if bias is not None:\n            bias = bias * self.b_mul\n        \n        have_convolution = False\n        if self.upscale is not None and min(x.shape[2:]) * 2 >= 128:\n            # this is the fused upscale + conv from StyleGAN, sadly this seems incompatible with the non-fused way\n            # this really needs to be cleaned up and go into the conv...\n            w = self.weight * self.w_mul\n            w = w.permute(1, 0, 2, 3)\n            # probably applying a conv on w would be more efficient. also this quadruples the weight (average)?!\n            w = F.pad(w, (1,1,1,1))\n            w = w[:, :, 1:, 1:]+ w[:, :, :-1, 1:] + w[:, :, 1:, :-1] + w[:, :, :-1, :-1]\n            x = F.conv_transpose2d(x, w, stride=2, padding=(w.size(-1)-1)//2)\n            have_convolution = True\n        elif self.upscale is not None:\n            x = self.upscale(x)\n    \n        if not have_convolution and self.intermediate is None:\n            return F.conv2d(x, self.weight * self.w_mul, bias, padding=self.kernel_size//2)\n        elif not have_convolution:\n            x = F.conv2d(x, self.weight * self.w_mul, None, padding=self.kernel_size//2)\n        \n        if self.intermediate is not None:\n            x = self.intermediate(x)\n        if bias is not None:\n            x = x + bias.view(1, -1, 1, 1)\n        return x\n\nclass NoiseLayer(nn.Module):\n    """"""adds noise. noise is per pixel (constant over channels) with per-channel weight""""""\n    def __init__(self, channels):\n        super().__init__()\n        self.weight = nn.Parameter(torch.zeros(channels))\n        self.noise = None\n    \n    def forward(self, x, noise=None):\n        if noise is None and self.noise is None:\n            noise = torch.randn(x.size(0), 1, x.size(2), x.size(3), device=x.device, dtype=x.dtype)\n        elif noise is None:\n            # here is a little trick: if you get all the noiselayers and set each\n            # modules .noise attribute, you can have pre-defined noise.\n            # Very useful for analysis\n            noise = self.noise\n        x = x + self.weight.view(1, -1, 1, 1) * noise\n        return x\n\nclass StyleMod(nn.Module):\n    def __init__(self, latent_size, channels, use_wscale):\n        super(StyleMod, self).__init__()\n        self.lin = MyLinear(latent_size,\n                            channels * 2,\n                            gain=1.0, use_wscale=use_wscale)\n        \n    def forward(self, x, latent):\n        style = self.lin(latent) # style => [batch_size, n_channels*2]\n        shape = [-1, 2, x.size(1)] + (x.dim() - 2) * [1]\n        style = style.view(shape)  # [batch_size, 2, n_channels, ...]\n        x = x * (style[:, 0] + 1.) + style[:, 1]\n        return x\n\nclass PixelNormLayer(nn.Module):\n    def __init__(self, epsilon=1e-8):\n        super().__init__()\n        self.epsilon = epsilon\n    def forward(self, x):\n        return x * torch.rsqrt(torch.mean(x**2, dim=1, keepdim=True) + self.epsilon)\n\nclass BlurLayer(nn.Module):\n    def __init__(self, kernel=[1, 2, 1], normalize=True, flip=False, stride=1):\n        super(BlurLayer, self).__init__()\n        kernel=[1, 2, 1]\n        kernel = torch.tensor(kernel, dtype=torch.float32)\n        kernel = kernel[:, None] * kernel[None, :]\n        kernel = kernel[None, None]\n        if normalize:\n            kernel = kernel / kernel.sum()\n        if flip:\n            kernel = kernel[:, :, ::-1, ::-1]\n        self.register_buffer(\'kernel\', kernel)\n        self.stride = stride\n    \n    def forward(self, x):\n        # expand kernel channels\n        kernel = self.kernel.expand(x.size(1), -1, -1, -1)\n        x = F.conv2d(\n            x,\n            kernel,\n            stride=self.stride,\n            padding=int((self.kernel.size(2)-1)/2),\n            groups=x.size(1)\n        )\n        return x\n\ndef upscale2d(x, factor=2, gain=1):\n    assert x.dim() == 4\n    if gain != 1:\n        x = x * gain\n    if factor != 1:\n        shape = x.shape\n        x = x.view(shape[0], shape[1], shape[2], 1, shape[3], 1).expand(-1, -1, -1, factor, -1, factor)\n        x = x.contiguous().view(shape[0], shape[1], factor * shape[2], factor * shape[3])\n    return x\n\nclass Upscale2d(nn.Module):\n    def __init__(self, factor=2, gain=1):\n        super().__init__()\n        assert isinstance(factor, int) and factor >= 1\n        self.gain = gain\n        self.factor = factor\n    def forward(self, x):\n        return upscale2d(x, factor=self.factor, gain=self.gain)\n\nclass G_mapping(nn.Sequential):\n    def __init__(self, nonlinearity=\'lrelu\', use_wscale=True):\n        act, gain = {\'relu\': (torch.relu, np.sqrt(2)),\n                     \'lrelu\': (nn.LeakyReLU(negative_slope=0.2), np.sqrt(2))}[nonlinearity]\n        layers = [\n            (\'pixel_norm\', PixelNormLayer()),\n            (\'dense0\', MyLinear(512, 512, gain=gain, lrmul=0.01, use_wscale=use_wscale)),\n            (\'dense0_act\', act),\n            (\'dense1\', MyLinear(512, 512, gain=gain, lrmul=0.01, use_wscale=use_wscale)),\n            (\'dense1_act\', act),\n            (\'dense2\', MyLinear(512, 512, gain=gain, lrmul=0.01, use_wscale=use_wscale)),\n            (\'dense2_act\', act),\n            (\'dense3\', MyLinear(512, 512, gain=gain, lrmul=0.01, use_wscale=use_wscale)),\n            (\'dense3_act\', act),\n            (\'dense4\', MyLinear(512, 512, gain=gain, lrmul=0.01, use_wscale=use_wscale)),\n            (\'dense4_act\', act),\n            (\'dense5\', MyLinear(512, 512, gain=gain, lrmul=0.01, use_wscale=use_wscale)),\n            (\'dense5_act\', act),\n            (\'dense6\', MyLinear(512, 512, gain=gain, lrmul=0.01, use_wscale=use_wscale)),\n            (\'dense6_act\', act),\n            (\'dense7\', MyLinear(512, 512, gain=gain, lrmul=0.01, use_wscale=use_wscale)),\n            (\'dense7_act\', act)\n        ]\n        super().__init__(OrderedDict(layers))\n        \n    def forward(self, x):\n        return super().forward(x)\n\nclass Truncation(nn.Module):\n    def __init__(self, avg_latent, max_layer=8, threshold=0.7):\n        super().__init__()\n        self.max_layer = max_layer\n        self.threshold = threshold\n        self.register_buffer(\'avg_latent\', avg_latent)\n    def forward(self, x):\n        assert x.dim() == 3\n        interp = torch.lerp(self.avg_latent, x, self.threshold)\n        do_trunc = (torch.arange(x.size(1)) < self.max_layer).view(1, -1, 1)\n        return torch.where(do_trunc, interp, x)\n\nclass LayerEpilogue(nn.Module):\n    """"""Things to do at the end of each layer.""""""\n    def __init__(self, channels, dlatent_size, use_wscale, use_noise, use_pixel_norm, use_instance_norm, use_styles, activation_layer):\n        super().__init__()\n        layers = []\n        if use_noise:\n            layers.append((\'noise\', NoiseLayer(channels)))\n        layers.append((\'activation\', activation_layer))\n        if use_pixel_norm:\n            layers.append((\'pixel_norm\', PixelNorm()))\n        if use_instance_norm:\n            layers.append((\'instance_norm\', nn.InstanceNorm2d(channels)))\n        self.top_epi = nn.Sequential(OrderedDict(layers))\n        if use_styles:\n            self.style_mod = StyleMod(dlatent_size, channels, use_wscale=use_wscale)\n        else:\n            self.style_mod = None\n    def forward(self, x, dlatents_in_slice=None):\n        x = self.top_epi(x)\n        if self.style_mod is not None:\n            x = self.style_mod(x, dlatents_in_slice)\n        else:\n            assert dlatents_in_slice is None\n        return x\n\n\nclass InputBlock(nn.Module):\n    def __init__(self, nf, dlatent_size, const_input_layer, gain, use_wscale, use_noise, use_pixel_norm, use_instance_norm, use_styles, activation_layer):\n        super().__init__()\n        self.const_input_layer = const_input_layer\n        self.nf = nf\n        if self.const_input_layer:\n            # called \'const\' in tf\n            self.const = nn.Parameter(torch.ones(1, nf, 4, 4))\n            self.bias = nn.Parameter(torch.ones(nf))\n        else:\n            self.dense = MyLinear(dlatent_size, nf*16, gain=gain/4, use_wscale=use_wscale) # tweak gain to match the official implementation of Progressing GAN\n        self.epi1 = LayerEpilogue(nf, dlatent_size, use_wscale, use_noise, use_pixel_norm, use_instance_norm, use_styles, activation_layer)\n        self.conv = MyConv2d(nf, nf, 3, gain=gain, use_wscale=use_wscale)\n        self.epi2 = LayerEpilogue(nf, dlatent_size, use_wscale, use_noise, use_pixel_norm, use_instance_norm, use_styles, activation_layer)\n        \n    def forward(self, dlatents_in_range):\n        batch_size = dlatents_in_range.size(0)\n        if self.const_input_layer:\n            x = self.const.expand(batch_size, -1, -1, -1)\n            x = x + self.bias.view(1, -1, 1, 1)\n        else:\n            x = self.dense(dlatents_in_range[:, 0]).view(batch_size, self.nf, 4, 4)\n        x = self.epi1(x, dlatents_in_range[:, 0])\n        x = self.conv(x)\n        x = self.epi2(x, dlatents_in_range[:, 1])\n        return x\n\n\nclass GSynthesisBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, blur_filter, dlatent_size, gain, use_wscale, use_noise, use_pixel_norm, use_instance_norm, use_styles, activation_layer):\n        # 2**res x 2**res # res = 3..resolution_log2\n        super().__init__()\n        if blur_filter:\n            blur = BlurLayer(blur_filter)\n        else:\n            blur = None\n        self.conv0_up = MyConv2d(in_channels, out_channels, kernel_size=3, gain=gain, use_wscale=use_wscale,\n                                 intermediate=blur, upscale=True)\n        self.epi1 = LayerEpilogue(out_channels, dlatent_size, use_wscale, use_noise, use_pixel_norm, use_instance_norm, use_styles, activation_layer)\n        self.conv1 = MyConv2d(out_channels, out_channels, kernel_size=3, gain=gain, use_wscale=use_wscale)\n        self.epi2 = LayerEpilogue(out_channels, dlatent_size, use_wscale, use_noise, use_pixel_norm, use_instance_norm, use_styles, activation_layer)\n            \n    def forward(self, x, dlatents_in_range):\n        x = self.conv0_up(x)\n        x = self.epi1(x, dlatents_in_range[:, 0])\n        x = self.conv1(x)\n        x = self.epi2(x, dlatents_in_range[:, 1])\n        return x\n\nclass G_synthesis(nn.Module):\n    def __init__(self,\n            dlatent_size        = 512,          # Disentangled latent (W) dimensionality.\n            num_channels        = 3,            # Number of output color channels.\n            resolution          = 1024,         # Output resolution.\n            fmap_base           = 8192,         # Overall multiplier for the number of feature maps.\n            fmap_decay          = 1.0,          # log2 feature map reduction when doubling the resolution.\n            fmap_max            = 512,          # Maximum number of feature maps in any layer.\n            use_styles          = True,         # Enable style inputs?\n            const_input_layer   = True,         # First layer is a learned constant?\n            use_noise           = True,         # Enable noise inputs?\n            randomize_noise     = True,         # True = randomize noise inputs every time (non-deterministic), False = read noise inputs from variables.\n            nonlinearity        = \'lrelu\',      # Activation function: \'relu\', \'lrelu\'\n            use_wscale          = True,         # Enable equalized learning rate?\n            use_pixel_norm      = False,        # Enable pixelwise feature vector normalization?\n            use_instance_norm   = True,         # Enable instance normalization?\n            dtype               = torch.float32,  # Data type to use for activations and outputs.\n            blur_filter         = [1,2,1],      # Low-pass filter to apply when resampling activations. None = no filtering.\n        ):\n        \n        super().__init__()\n        def nf(stage):\n            return min(int(fmap_base / (2.0 ** (stage * fmap_decay))), fmap_max)\n        self.dlatent_size = dlatent_size\n        resolution_log2 = int(np.log2(resolution))\n        assert resolution == 2**resolution_log2 and resolution >= 4\n\n        act, gain = {\'relu\': (torch.relu, np.sqrt(2)),\n                     \'lrelu\': (nn.LeakyReLU(negative_slope=0.2), np.sqrt(2))}[nonlinearity]\n        num_layers = resolution_log2 * 2 - 2\n        num_styles = num_layers if use_styles else 1\n        torgbs = []\n        blocks = []\n        for res in range(2, resolution_log2 + 1):\n            channels = nf(res-1)\n            name = \'{s}x{s}\'.format(s=2**res)\n            if res == 2:\n                blocks.append((name,\n                               InputBlock(channels, dlatent_size, const_input_layer, gain, use_wscale,\n                                      use_noise, use_pixel_norm, use_instance_norm, use_styles, act)))\n                \n            else:\n                blocks.append((name,\n                               GSynthesisBlock(last_channels, channels, blur_filter, dlatent_size, gain, use_wscale, use_noise, use_pixel_norm, use_instance_norm, use_styles, act)))\n            last_channels = channels\n        self.torgb = MyConv2d(channels, num_channels, 1, gain=1, use_wscale=use_wscale)\n        self.blocks = nn.ModuleDict(OrderedDict(blocks))\n        \n    def forward(self, dlatents_in):\n        # Input: Disentangled latents (W) [minibatch, num_layers, dlatent_size].\n        # lod_in = tf.cast(tf.get_variable(\'lod\', initializer=np.float32(0), trainable=False), dtype)\n        batch_size = dlatents_in.size(0)       \n        for i, m in enumerate(self.blocks.values()):\n            if i == 0:\n                x = m(dlatents_in[:, 2*i:2*i+2])\n            else:\n                x = m(x, dlatents_in[:, 2*i:2*i+2])\n        rgb = self.torgb(x)\n        return rgb\n\n\nclass StyleGAN_G(nn.Sequential):\n    def __init__(self, resolution, truncation=1.0):\n        self.resolution = resolution\n        self.layers = OrderedDict([\n            (\'g_mapping\', G_mapping()),\n            #(\'truncation\', Truncation(avg_latent)),\n            (\'g_synthesis\', G_synthesis(resolution=resolution)),\n        ])\n        super().__init__(self.layers)\n\n    def forward(self, x, latent_is_w=False):\n        if isinstance(x, list):\n            assert len(x) == 18, \'Must provide 1 or 18 latents\'\n            if not latent_is_w:\n                x = [self.layers[\'g_mapping\'].forward(l) for l in x]\n            x = torch.stack(x, dim=1)\n        else:\n            if not latent_is_w:\n                x = self.layers[\'g_mapping\'].forward(x)\n            x = x.unsqueeze(1).expand(-1, 18, -1)\n\n        x = self.layers[\'g_synthesis\'].forward(x)\n\n        return x\n\n    # From: https://github.com/lernapparat/lernapparat/releases/download/v2019-02-01/\n    def load_weights(self, checkpoint):    \n        self.load_state_dict(torch.load(checkpoint))\n\n    def export_from_tf(self, pickle_path):\n        module_path = Path(__file__).parent / \'stylegan_tf\'\n        sys.path.append(str(module_path.resolve()))\n\n        import dnnlib, dnnlib.tflib, pickle, torch, collections\n        dnnlib.tflib.init_tf()\n\n        weights = pickle.load(open(pickle_path,\'rb\'))\n        weights_pt = [collections.OrderedDict([(k, torch.from_numpy(v.value().eval())) for k,v in w.trainables.items()]) for w in weights]\n        #torch.save(weights_pt, pytorch_name)\n\n        # then on the PyTorch side run\n        state_G, state_D, state_Gs = weights_pt #torch.load(\'./karras2019stylegan-ffhq-1024x1024.pt\')\n        def key_translate(k):\n            k = k.lower().split(\'/\')\n            if k[0] == \'g_synthesis\':\n                if not k[1].startswith(\'torgb\'):\n                    k.insert(1, \'blocks\')\n                k = \'.\'.join(k)\n                k = (k.replace(\'const.const\',\'const\').replace(\'const.bias\',\'bias\').replace(\'const.stylemod\',\'epi1.style_mod.lin\')\n                    .replace(\'const.noise.weight\',\'epi1.top_epi.noise.weight\')\n                    .replace(\'conv.noise.weight\',\'epi2.top_epi.noise.weight\')\n                    .replace(\'conv.stylemod\',\'epi2.style_mod.lin\')\n                    .replace(\'conv0_up.noise.weight\', \'epi1.top_epi.noise.weight\')\n                    .replace(\'conv0_up.stylemod\',\'epi1.style_mod.lin\')\n                    .replace(\'conv1.noise.weight\', \'epi2.top_epi.noise.weight\')\n                    .replace(\'conv1.stylemod\',\'epi2.style_mod.lin\')\n                    .replace(\'torgb_lod0\',\'torgb\'))\n            else:\n                k = \'.\'.join(k)\n            return k\n\n        def weight_translate(k, w):\n            k = key_translate(k)\n            if k.endswith(\'.weight\'):\n                if w.dim() == 2:\n                    w = w.t()\n                elif w.dim() == 1:\n                    pass\n                else:\n                    assert w.dim() == 4\n                    w = w.permute(3, 2, 0, 1)\n            return w\n\n        # we delete the useless torgb filters\n        param_dict = {key_translate(k) : weight_translate(k, v) for k,v in state_Gs.items() if \'torgb_lod\' not in key_translate(k)}\n        if 1:\n            sd_shapes = {k : v.shape for k,v in self.state_dict().items()}\n            param_shapes = {k : v.shape for k,v in param_dict.items() }\n\n            for k in list(sd_shapes)+list(param_shapes):\n                pds = param_shapes.get(k)\n                sds = sd_shapes.get(k)\n                if pds is None:\n                    print (""sd only"", k, sds)\n                elif sds is None:\n                    print (""pd only"", k, pds)\n                elif sds != pds:\n                    print (""mismatch!"", k, pds, sds)\n\n        self.load_state_dict(param_dict, strict=False) # needed for the blur kernels\n        torch.save(self.state_dict(), Path(pickle_path).with_suffix(\'.pt\'))'"
models/stylegan2/__init__.py,0,"b""import sys\nimport os\nimport shutil\nimport glob\nimport platform\nfrom pathlib import Path\n\ncurrent_path = os.getcwd()\n\nmodule_path = Path(__file__).parent / 'stylegan2-pytorch'\nsys.path.append(str(module_path.resolve()))\nos.chdir(module_path)\n\nfrom model import Generator\n\nos.chdir(current_path)"""
netdissect/segmodel/__init__.py,0,"b'from .models import ModelBuilder, SegmentationModule\n'"
netdissect/segmodel/models.py,11,"b'import torch\nimport torch.nn as nn\nimport torchvision\nfrom . import resnet, resnext\ntry:\n    from lib.nn import SynchronizedBatchNorm2d\nexcept ImportError:\n    from torch.nn import BatchNorm2d as SynchronizedBatchNorm2d\n\n\nclass SegmentationModuleBase(nn.Module):\n    def __init__(self):\n        super(SegmentationModuleBase, self).__init__()\n\n    def pixel_acc(self, pred, label):\n        _, preds = torch.max(pred, dim=1)\n        valid = (label >= 0).long()\n        acc_sum = torch.sum(valid * (preds == label).long())\n        pixel_sum = torch.sum(valid)\n        acc = acc_sum.float() / (pixel_sum.float() + 1e-10)\n        return acc\n\n\nclass SegmentationModule(SegmentationModuleBase):\n    def __init__(self, net_enc, net_dec, crit, deep_sup_scale=None):\n        super(SegmentationModule, self).__init__()\n        self.encoder = net_enc\n        self.decoder = net_dec\n        self.crit = crit\n        self.deep_sup_scale = deep_sup_scale\n\n    def forward(self, feed_dict, *, segSize=None):\n        if segSize is None: # training\n            if self.deep_sup_scale is not None: # use deep supervision technique\n                (pred, pred_deepsup) = self.decoder(self.encoder(feed_dict[\'img_data\'], return_feature_maps=True))\n            else:\n                pred = self.decoder(self.encoder(feed_dict[\'img_data\'], return_feature_maps=True))\n\n            loss = self.crit(pred, feed_dict[\'seg_label\'])\n            if self.deep_sup_scale is not None:\n                loss_deepsup = self.crit(pred_deepsup, feed_dict[\'seg_label\'])\n                loss = loss + loss_deepsup * self.deep_sup_scale\n\n            acc = self.pixel_acc(pred, feed_dict[\'seg_label\'])\n            return loss, acc\n        else: # inference\n            pred = self.decoder(self.encoder(feed_dict[\'img_data\'], return_feature_maps=True), segSize=segSize)\n            return pred\n\n\ndef conv3x3(in_planes, out_planes, stride=1, has_bias=False):\n    ""3x3 convolution with padding""\n    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n                     padding=1, bias=has_bias)\n\n\ndef conv3x3_bn_relu(in_planes, out_planes, stride=1):\n    return nn.Sequential(\n            conv3x3(in_planes, out_planes, stride),\n            SynchronizedBatchNorm2d(out_planes),\n            nn.ReLU(inplace=True),\n            )\n\n\nclass ModelBuilder():\n    # custom weights initialization\n    def weights_init(self, m):\n        classname = m.__class__.__name__\n        if classname.find(\'Conv\') != -1:\n            nn.init.kaiming_normal_(m.weight.data)\n        elif classname.find(\'BatchNorm\') != -1:\n            m.weight.data.fill_(1.)\n            m.bias.data.fill_(1e-4)\n        #elif classname.find(\'Linear\') != -1:\n        #    m.weight.data.normal_(0.0, 0.0001)\n\n    def build_encoder(self, arch=\'resnet50_dilated8\', fc_dim=512, weights=\'\'):\n        pretrained = True if len(weights) == 0 else False\n        if arch == \'resnet34\':\n            raise NotImplementedError\n            orig_resnet = resnet.__dict__[\'resnet34\'](pretrained=pretrained)\n            net_encoder = Resnet(orig_resnet)\n        elif arch == \'resnet34_dilated8\':\n            raise NotImplementedError\n            orig_resnet = resnet.__dict__[\'resnet34\'](pretrained=pretrained)\n            net_encoder = ResnetDilated(orig_resnet,\n                                        dilate_scale=8)\n        elif arch == \'resnet34_dilated16\':\n            raise NotImplementedError\n            orig_resnet = resnet.__dict__[\'resnet34\'](pretrained=pretrained)\n            net_encoder = ResnetDilated(orig_resnet,\n                                        dilate_scale=16)\n        elif arch == \'resnet50\':\n            orig_resnet = resnet.__dict__[\'resnet50\'](pretrained=pretrained)\n            net_encoder = Resnet(orig_resnet)\n        elif arch == \'resnet50_dilated8\':\n            orig_resnet = resnet.__dict__[\'resnet50\'](pretrained=pretrained)\n            net_encoder = ResnetDilated(orig_resnet,\n                                        dilate_scale=8)\n        elif arch == \'resnet50_dilated16\':\n            orig_resnet = resnet.__dict__[\'resnet50\'](pretrained=pretrained)\n            net_encoder = ResnetDilated(orig_resnet,\n                                        dilate_scale=16)\n        elif arch == \'resnet101\':\n            orig_resnet = resnet.__dict__[\'resnet101\'](pretrained=pretrained)\n            net_encoder = Resnet(orig_resnet)\n        elif arch == \'resnet101_dilated8\':\n            orig_resnet = resnet.__dict__[\'resnet101\'](pretrained=pretrained)\n            net_encoder = ResnetDilated(orig_resnet,\n                                        dilate_scale=8)\n        elif arch == \'resnet101_dilated16\':\n            orig_resnet = resnet.__dict__[\'resnet101\'](pretrained=pretrained)\n            net_encoder = ResnetDilated(orig_resnet,\n                                        dilate_scale=16)\n        elif arch == \'resnext101\':\n            orig_resnext = resnext.__dict__[\'resnext101\'](pretrained=pretrained)\n            net_encoder = Resnet(orig_resnext) # we can still use class Resnet\n        else:\n            raise Exception(\'Architecture undefined!\')\n\n        # net_encoder.apply(self.weights_init)\n        if len(weights) > 0:\n            # print(\'Loading weights for net_encoder\')\n            net_encoder.load_state_dict(\n                torch.load(weights, map_location=lambda storage, loc: storage), strict=False)\n        return net_encoder\n\n    def build_decoder(self, arch=\'ppm_bilinear_deepsup\',\n                      fc_dim=512, num_class=150,\n                      weights=\'\', inference=False, use_softmax=False):\n        if arch == \'c1_bilinear_deepsup\':\n            net_decoder = C1BilinearDeepSup(\n                num_class=num_class,\n                fc_dim=fc_dim,\n                inference=inference,\n                use_softmax=use_softmax)\n        elif arch == \'c1_bilinear\':\n            net_decoder = C1Bilinear(\n                num_class=num_class,\n                fc_dim=fc_dim,\n                inference=inference,\n                use_softmax=use_softmax)\n        elif arch == \'ppm_bilinear\':\n            net_decoder = PPMBilinear(\n                num_class=num_class,\n                fc_dim=fc_dim,\n                inference=inference,\n                use_softmax=use_softmax)\n        elif arch == \'ppm_bilinear_deepsup\':\n            net_decoder = PPMBilinearDeepsup(\n                num_class=num_class,\n                fc_dim=fc_dim,\n                inference=inference,\n                use_softmax=use_softmax)\n        elif arch == \'upernet_lite\':\n            net_decoder = UPerNet(\n                num_class=num_class,\n                fc_dim=fc_dim,\n                inference=inference,\n                use_softmax=use_softmax,\n                fpn_dim=256)\n        elif arch == \'upernet\':\n            net_decoder = UPerNet(\n                num_class=num_class,\n                fc_dim=fc_dim,\n                inference=inference,\n                use_softmax=use_softmax,\n                fpn_dim=512)\n        elif arch == \'upernet_tmp\':\n            net_decoder = UPerNetTmp(\n                num_class=num_class,\n                fc_dim=fc_dim,\n                inference=inference,\n                use_softmax=use_softmax,\n                fpn_dim=512)\n        else:\n            raise Exception(\'Architecture undefined!\')\n\n        net_decoder.apply(self.weights_init)\n        if len(weights) > 0:\n            # print(\'Loading weights for net_decoder\')\n            net_decoder.load_state_dict(\n                torch.load(weights, map_location=lambda storage, loc: storage), strict=False)\n        return net_decoder\n\n\nclass Resnet(nn.Module):\n    def __init__(self, orig_resnet):\n        super(Resnet, self).__init__()\n\n        # take pretrained resnet, except AvgPool and FC\n        self.conv1 = orig_resnet.conv1\n        self.bn1 = orig_resnet.bn1\n        self.relu1 = orig_resnet.relu1\n        self.conv2 = orig_resnet.conv2\n        self.bn2 = orig_resnet.bn2\n        self.relu2 = orig_resnet.relu2\n        self.conv3 = orig_resnet.conv3\n        self.bn3 = orig_resnet.bn3\n        self.relu3 = orig_resnet.relu3\n        self.maxpool = orig_resnet.maxpool\n        self.layer1 = orig_resnet.layer1\n        self.layer2 = orig_resnet.layer2\n        self.layer3 = orig_resnet.layer3\n        self.layer4 = orig_resnet.layer4\n\n    def forward(self, x, return_feature_maps=False):\n        conv_out = []\n\n        x = self.relu1(self.bn1(self.conv1(x)))\n        x = self.relu2(self.bn2(self.conv2(x)))\n        x = self.relu3(self.bn3(self.conv3(x)))\n        x = self.maxpool(x)\n\n        x = self.layer1(x); conv_out.append(x);\n        x = self.layer2(x); conv_out.append(x);\n        x = self.layer3(x); conv_out.append(x);\n        x = self.layer4(x); conv_out.append(x);\n\n        if return_feature_maps:\n            return conv_out\n        return [x]\n\n\nclass ResnetDilated(nn.Module):\n    def __init__(self, orig_resnet, dilate_scale=8):\n        super(ResnetDilated, self).__init__()\n        from functools import partial\n\n        if dilate_scale == 8:\n            orig_resnet.layer3.apply(\n                partial(self._nostride_dilate, dilate=2))\n            orig_resnet.layer4.apply(\n                partial(self._nostride_dilate, dilate=4))\n        elif dilate_scale == 16:\n            orig_resnet.layer4.apply(\n                partial(self._nostride_dilate, dilate=2))\n\n        # take pretrained resnet, except AvgPool and FC\n        self.conv1 = orig_resnet.conv1\n        self.bn1 = orig_resnet.bn1\n        self.relu1 = orig_resnet.relu1\n        self.conv2 = orig_resnet.conv2\n        self.bn2 = orig_resnet.bn2\n        self.relu2 = orig_resnet.relu2\n        self.conv3 = orig_resnet.conv3\n        self.bn3 = orig_resnet.bn3\n        self.relu3 = orig_resnet.relu3\n        self.maxpool = orig_resnet.maxpool\n        self.layer1 = orig_resnet.layer1\n        self.layer2 = orig_resnet.layer2\n        self.layer3 = orig_resnet.layer3\n        self.layer4 = orig_resnet.layer4\n\n    def _nostride_dilate(self, m, dilate):\n        classname = m.__class__.__name__\n        if classname.find(\'Conv\') != -1:\n            # the convolution with stride\n            if m.stride == (2, 2):\n                m.stride = (1, 1)\n                if m.kernel_size == (3, 3):\n                    m.dilation = (dilate//2, dilate//2)\n                    m.padding = (dilate//2, dilate//2)\n            # other convoluions\n            else:\n                if m.kernel_size == (3, 3):\n                    m.dilation = (dilate, dilate)\n                    m.padding = (dilate, dilate)\n\n    def forward(self, x, return_feature_maps=False):\n        conv_out = []\n\n        x = self.relu1(self.bn1(self.conv1(x)))\n        x = self.relu2(self.bn2(self.conv2(x)))\n        x = self.relu3(self.bn3(self.conv3(x)))\n        x = self.maxpool(x)\n\n        x = self.layer1(x); conv_out.append(x);\n        x = self.layer2(x); conv_out.append(x);\n        x = self.layer3(x); conv_out.append(x);\n        x = self.layer4(x); conv_out.append(x);\n\n        if return_feature_maps:\n            return conv_out\n        return [x]\n\n\n# last conv, bilinear upsample\nclass C1BilinearDeepSup(nn.Module):\n    def __init__(self, num_class=150, fc_dim=2048, inference=False, use_softmax=False):\n        super(C1BilinearDeepSup, self).__init__()\n        self.use_softmax = use_softmax\n        self.inference = inference\n\n        self.cbr = conv3x3_bn_relu(fc_dim, fc_dim // 4, 1)\n        self.cbr_deepsup = conv3x3_bn_relu(fc_dim // 2, fc_dim // 4, 1)\n\n        # last conv\n        self.conv_last = nn.Conv2d(fc_dim // 4, num_class, 1, 1, 0)\n        self.conv_last_deepsup = nn.Conv2d(fc_dim // 4, num_class, 1, 1, 0)\n\n    def forward(self, conv_out, segSize=None):\n        conv5 = conv_out[-1]\n\n        x = self.cbr(conv5)\n        x = self.conv_last(x)\n\n        if self.inference or self.use_softmax:  # is True during inference\n            x = nn.functional.interpolate(\n                x, size=segSize, mode=\'bilinear\', align_corners=False)\n            if self.use_softmax:\n                x = nn.functional.softmax(x, dim=1)\n            return x\n\n        # deep sup\n        conv4 = conv_out[-2]\n        _ = self.cbr_deepsup(conv4)\n        _ = self.conv_last_deepsup(_)\n\n        x = nn.functional.log_softmax(x, dim=1)\n        _ = nn.functional.log_softmax(_, dim=1)\n\n        return (x, _)\n\n\n# last conv, bilinear upsample\nclass C1Bilinear(nn.Module):\n    def __init__(self, num_class=150, fc_dim=2048, inference=False, use_softmax=False):\n        super(C1Bilinear, self).__init__()\n        self.use_softmax = use_softmax\n        self.inference = inference\n\n        self.cbr = conv3x3_bn_relu(fc_dim, fc_dim // 4, 1)\n\n        # last conv\n        self.conv_last = nn.Conv2d(fc_dim // 4, num_class, 1, 1, 0)\n\n    def forward(self, conv_out, segSize=None):\n        conv5 = conv_out[-1]\n        x = self.cbr(conv5)\n        x = self.conv_last(x)\n\n        if self.inference or self.use_softmax:  # is True during inference\n            x = nn.functional.interpolate(\n                x, size=segSize, mode=\'bilinear\', align_corners=False)\n            if self.use_softmax:\n                x = nn.functional.softmax(x, dim=1)\n        else:\n            x = nn.functional.log_softmax(x, dim=1)\n\n        return x\n\n\n# pyramid pooling, bilinear upsample\nclass PPMBilinear(nn.Module):\n    def __init__(self, num_class=150, fc_dim=4096,\n                 inference=False, use_softmax=False, pool_scales=(1, 2, 3, 6)):\n        super(PPMBilinear, self).__init__()\n        self.use_softmax = use_softmax\n        self.inference = inference\n\n        self.ppm = []\n        for scale in pool_scales:\n            self.ppm.append(nn.Sequential(\n                nn.AdaptiveAvgPool2d(scale),\n                nn.Conv2d(fc_dim, 512, kernel_size=1, bias=False),\n                SynchronizedBatchNorm2d(512),\n                nn.ReLU(inplace=True)\n            ))\n        self.ppm = nn.ModuleList(self.ppm)\n\n        self.conv_last = nn.Sequential(\n            nn.Conv2d(fc_dim+len(pool_scales)*512, 512,\n                      kernel_size=3, padding=1, bias=False),\n            SynchronizedBatchNorm2d(512),\n            nn.ReLU(inplace=True),\n            nn.Dropout2d(0.1),\n            nn.Conv2d(512, num_class, kernel_size=1)\n        )\n\n    def forward(self, conv_out, segSize=None):\n        conv5 = conv_out[-1]\n\n        input_size = conv5.size()\n        ppm_out = [conv5]\n        for pool_scale in self.ppm:\n            ppm_out.append(nn.functional.interpolate(\n                pool_scale(conv5),\n                (input_size[2], input_size[3]),\n                mode=\'bilinear\', align_corners=False))\n        ppm_out = torch.cat(ppm_out, 1)\n\n        x = self.conv_last(ppm_out)\n\n        if self.inference or self.use_softmax:  # is True during inference\n            x = nn.functional.interpolate(\n                x, size=segSize, mode=\'bilinear\', align_corners=False)\n            if self.use_softmax:\n                x = nn.functional.softmax(x, dim=1)\n        else:\n            x = nn.functional.log_softmax(x, dim=1)\n        return x\n\n\n# pyramid pooling, bilinear upsample\nclass PPMBilinearDeepsup(nn.Module):\n    def __init__(self, num_class=150, fc_dim=4096,\n                 inference=False, use_softmax=False, pool_scales=(1, 2, 3, 6)):\n        super(PPMBilinearDeepsup, self).__init__()\n        self.use_softmax = use_softmax\n        self.inference = inference\n\n        self.ppm = []\n        for scale in pool_scales:\n            self.ppm.append(nn.Sequential(\n                nn.AdaptiveAvgPool2d(scale),\n                nn.Conv2d(fc_dim, 512, kernel_size=1, bias=False),\n                SynchronizedBatchNorm2d(512),\n                nn.ReLU(inplace=True)\n            ))\n        self.ppm = nn.ModuleList(self.ppm)\n        self.cbr_deepsup = conv3x3_bn_relu(fc_dim // 2, fc_dim // 4, 1)\n\n        self.conv_last = nn.Sequential(\n            nn.Conv2d(fc_dim+len(pool_scales)*512, 512,\n                      kernel_size=3, padding=1, bias=False),\n            SynchronizedBatchNorm2d(512),\n            nn.ReLU(inplace=True),\n            nn.Dropout2d(0.1),\n            nn.Conv2d(512, num_class, kernel_size=1)\n        )\n        self.conv_last_deepsup = nn.Conv2d(fc_dim // 4, num_class, 1, 1, 0)\n        self.dropout_deepsup = nn.Dropout2d(0.1)\n\n    def forward(self, conv_out, segSize=None):\n        conv5 = conv_out[-1]\n\n        input_size = conv5.size()\n        ppm_out = [conv5]\n        for pool_scale in self.ppm:\n            ppm_out.append(nn.functional.interpolate(\n                pool_scale(conv5),\n                (input_size[2], input_size[3]),\n                mode=\'bilinear\', align_corners=False))\n        ppm_out = torch.cat(ppm_out, 1)\n\n        x = self.conv_last(ppm_out)\n\n        if self.inference or self.use_softmax:  # is True during inference\n            x = nn.functional.interpolate(\n                x, size=segSize, mode=\'bilinear\', align_corners=False)\n            if self.use_softmax:\n                x = nn.functional.softmax(x, dim=1)\n            return x\n\n        # deep sup\n        conv4 = conv_out[-2]\n        _ = self.cbr_deepsup(conv4)\n        _ = self.dropout_deepsup(_)\n        _ = self.conv_last_deepsup(_)\n\n        x = nn.functional.log_softmax(x, dim=1)\n        _ = nn.functional.log_softmax(_, dim=1)\n\n        return (x, _)\n\n\n# upernet\nclass UPerNet(nn.Module):\n    def __init__(self, num_class=150, fc_dim=4096,\n                 inference=False, use_softmax=False, pool_scales=(1, 2, 3, 6),\n                 fpn_inplanes=(256,512,1024,2048), fpn_dim=256):\n        super(UPerNet, self).__init__()\n        self.use_softmax = use_softmax\n        self.inference = inference\n\n        # PPM Module\n        self.ppm_pooling = []\n        self.ppm_conv = []\n\n        for scale in pool_scales:\n            self.ppm_pooling.append(nn.AdaptiveAvgPool2d(scale))\n            self.ppm_conv.append(nn.Sequential(\n                nn.Conv2d(fc_dim, 512, kernel_size=1, bias=False),\n                SynchronizedBatchNorm2d(512),\n                nn.ReLU(inplace=True)\n            ))\n        self.ppm_pooling = nn.ModuleList(self.ppm_pooling)\n        self.ppm_conv = nn.ModuleList(self.ppm_conv)\n        self.ppm_last_conv = conv3x3_bn_relu(fc_dim + len(pool_scales)*512, fpn_dim, 1)\n\n        # FPN Module\n        self.fpn_in = []\n        for fpn_inplane in fpn_inplanes[:-1]: # skip the top layer\n            self.fpn_in.append(nn.Sequential(\n                nn.Conv2d(fpn_inplane, fpn_dim, kernel_size=1, bias=False),\n                SynchronizedBatchNorm2d(fpn_dim),\n                nn.ReLU(inplace=True)\n            ))\n        self.fpn_in = nn.ModuleList(self.fpn_in)\n\n        self.fpn_out = []\n        for i in range(len(fpn_inplanes) - 1): # skip the top layer\n            self.fpn_out.append(nn.Sequential(\n                conv3x3_bn_relu(fpn_dim, fpn_dim, 1),\n            ))\n        self.fpn_out = nn.ModuleList(self.fpn_out)\n\n        self.conv_last = nn.Sequential(\n            conv3x3_bn_relu(len(fpn_inplanes) * fpn_dim, fpn_dim, 1),\n            nn.Conv2d(fpn_dim, num_class, kernel_size=1)\n        )\n\n    def forward(self, conv_out, segSize=None):\n        conv5 = conv_out[-1]\n\n        input_size = conv5.size()\n        ppm_out = [conv5]\n        for pool_scale, pool_conv in zip(self.ppm_pooling, self.ppm_conv):\n            ppm_out.append(pool_conv(nn.functional.interploate(\n                pool_scale(conv5),\n                (input_size[2], input_size[3]),\n                mode=\'bilinear\', align_corners=False)))\n        ppm_out = torch.cat(ppm_out, 1)\n        f = self.ppm_last_conv(ppm_out)\n\n        fpn_feature_list = [f]\n        for i in reversed(range(len(conv_out) - 1)):\n            conv_x = conv_out[i]\n            conv_x = self.fpn_in[i](conv_x) # lateral branch\n\n            f = nn.functional.interpolate(\n                f, size=conv_x.size()[2:], mode=\'bilinear\', align_corners=False) # top-down branch\n            f = conv_x + f\n\n            fpn_feature_list.append(self.fpn_out[i](f))\n\n        fpn_feature_list.reverse() # [P2 - P5]\n        output_size = fpn_feature_list[0].size()[2:]\n        fusion_list = [fpn_feature_list[0]]\n        for i in range(1, len(fpn_feature_list)):\n            fusion_list.append(nn.functional.interpolate(\n                fpn_feature_list[i],\n                output_size,\n                mode=\'bilinear\', align_corners=False))\n        fusion_out = torch.cat(fusion_list, 1)\n        x = self.conv_last(fusion_out)\n\n        if self.inference or self.use_softmax:  # is True during inference\n            x = nn.functional.interpolate(\n                x, size=segSize, mode=\'bilinear\', align_corners=False)\n            if self.use_softmax:\n                x = nn.functional.softmax(x, dim=1)\n            return x\n\n        x = nn.functional.log_softmax(x, dim=1)\n\n        return x\n'"
netdissect/segmodel/resnet.py,3,"b'import os\nimport sys\nimport torch\nimport torch.nn as nn\nimport math\ntry:\n    from lib.nn import SynchronizedBatchNorm2d\nexcept ImportError:\n    from torch.nn import BatchNorm2d as SynchronizedBatchNorm2d\n\ntry:\n    from urllib import urlretrieve\nexcept ImportError:\n    from urllib.request import urlretrieve\n\n\n__all__ = [\'ResNet\', \'resnet50\', \'resnet101\'] # resnet101 is coming soon!\n\n\nmodel_urls = {\n    \'resnet50\': \'http://sceneparsing.csail.mit.edu/model/pretrained_resnet/resnet50-imagenet.pth\',\n    \'resnet101\': \'http://sceneparsing.csail.mit.edu/model/pretrained_resnet/resnet101-imagenet.pth\'\n}\n\n\ndef conv3x3(in_planes, out_planes, stride=1):\n    ""3x3 convolution with padding""\n    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n                     padding=1, bias=False)\n\n\nclass BasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None):\n        super(BasicBlock, self).__init__()\n        self.conv1 = conv3x3(inplanes, planes, stride)\n        self.bn1 = SynchronizedBatchNorm2d(planes)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = conv3x3(planes, planes)\n        self.bn2 = SynchronizedBatchNorm2d(planes)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\n\nclass Bottleneck(nn.Module):\n    expansion = 4\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None):\n        super(Bottleneck, self).__init__()\n        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n        self.bn1 = SynchronizedBatchNorm2d(planes)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,\n                               padding=1, bias=False)\n        self.bn2 = SynchronizedBatchNorm2d(planes)\n        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)\n        self.bn3 = SynchronizedBatchNorm2d(planes * 4)\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\n\nclass ResNet(nn.Module):\n\n    def __init__(self, block, layers, num_classes=1000):\n        self.inplanes = 128\n        super(ResNet, self).__init__()\n        self.conv1 = conv3x3(3, 64, stride=2)\n        self.bn1 = SynchronizedBatchNorm2d(64)\n        self.relu1 = nn.ReLU(inplace=True)\n        self.conv2 = conv3x3(64, 64)\n        self.bn2 = SynchronizedBatchNorm2d(64)\n        self.relu2 = nn.ReLU(inplace=True)\n        self.conv3 = conv3x3(64, 128)\n        self.bn3 = SynchronizedBatchNorm2d(128)\n        self.relu3 = nn.ReLU(inplace=True)\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n\n        self.layer1 = self._make_layer(block, 64, layers[0])\n        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n        self.avgpool = nn.AvgPool2d(7, stride=1)\n        self.fc = nn.Linear(512 * block.expansion, num_classes)\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n                m.weight.data.normal_(0, math.sqrt(2. / n))\n            elif isinstance(m, SynchronizedBatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n\n    def _make_layer(self, block, planes, blocks, stride=1):\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                nn.Conv2d(self.inplanes, planes * block.expansion,\n                          kernel_size=1, stride=stride, bias=False),\n                SynchronizedBatchNorm2d(planes * block.expansion),\n            )\n\n        layers = []\n        layers.append(block(self.inplanes, planes, stride, downsample))\n        self.inplanes = planes * block.expansion\n        for i in range(1, blocks):\n            layers.append(block(self.inplanes, planes))\n\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        x = self.relu1(self.bn1(self.conv1(x)))\n        x = self.relu2(self.bn2(self.conv2(x)))\n        x = self.relu3(self.bn3(self.conv3(x)))\n        x = self.maxpool(x)\n\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n\n        x = self.avgpool(x)\n        x = x.view(x.size(0), -1)\n        x = self.fc(x)\n\n        return x\n\n\'\'\'\ndef resnet18(pretrained=False, **kwargs):\n    """"""Constructs a ResNet-18 model.\n\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on Places\n    """"""\n    model = ResNet(BasicBlock, [2, 2, 2, 2], **kwargs)\n    if pretrained:\n        model.load_state_dict(load_url(model_urls[\'resnet18\']))\n    return model\n\n\ndef resnet34(pretrained=False, **kwargs):\n    """"""Constructs a ResNet-34 model.\n\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on Places\n    """"""\n    model = ResNet(BasicBlock, [3, 4, 6, 3], **kwargs)\n    if pretrained:\n        model.load_state_dict(load_url(model_urls[\'resnet34\']))\n    return model\n\'\'\'\n\ndef resnet50(pretrained=False, **kwargs):\n    """"""Constructs a ResNet-50 model.\n\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on Places\n    """"""\n    model = ResNet(Bottleneck, [3, 4, 6, 3], **kwargs)\n    if pretrained:\n        model.load_state_dict(load_url(model_urls[\'resnet50\']), strict=False)\n    return model\n\n\ndef resnet101(pretrained=False, **kwargs):\n    """"""Constructs a ResNet-101 model.\n\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on Places\n    """"""\n    model = ResNet(Bottleneck, [3, 4, 23, 3], **kwargs)\n    if pretrained:\n        model.load_state_dict(load_url(model_urls[\'resnet101\']), strict=False)\n    return model\n\n# def resnet152(pretrained=False, **kwargs):\n#     """"""Constructs a ResNet-152 model.\n#\n#     Args:\n#         pretrained (bool): If True, returns a model pre-trained on Places\n#     """"""\n#     model = ResNet(Bottleneck, [3, 8, 36, 3], **kwargs)\n#     if pretrained:\n#         model.load_state_dict(load_url(model_urls[\'resnet152\']))\n#     return model\n\ndef load_url(url, model_dir=\'./pretrained\', map_location=None):\n    if not os.path.exists(model_dir):\n        os.makedirs(model_dir)\n    filename = url.split(\'/\')[-1]\n    cached_file = os.path.join(model_dir, filename)\n    if not os.path.exists(cached_file):\n        sys.stderr.write(\'Downloading: ""{}"" to {}\\n\'.format(url, cached_file))\n        urlretrieve(url, cached_file)\n    return torch.load(cached_file, map_location=map_location)\n'"
netdissect/segmodel/resnext.py,3,"b'import os\nimport sys\nimport torch\nimport torch.nn as nn\nimport math\ntry:\n    from lib.nn import SynchronizedBatchNorm2d\nexcept ImportError:\n    from torch.nn import BatchNorm2d as SynchronizedBatchNorm2d\ntry:\n    from urllib import urlretrieve\nexcept ImportError:\n    from urllib.request import urlretrieve\n\n\n__all__ = [\'ResNeXt\', \'resnext101\'] # support resnext 101\n\n\nmodel_urls = {\n    #\'resnext50\': \'http://sceneparsing.csail.mit.edu/model/pretrained_resnet/resnext50-imagenet.pth\',\n    \'resnext101\': \'http://sceneparsing.csail.mit.edu/model/pretrained_resnet/resnext101-imagenet.pth\'\n}\n\n\ndef conv3x3(in_planes, out_planes, stride=1):\n    ""3x3 convolution with padding""\n    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n                     padding=1, bias=False)\n\n\nclass GroupBottleneck(nn.Module):\n    expansion = 2\n\n    def __init__(self, inplanes, planes, stride=1, groups=1, downsample=None):\n        super(GroupBottleneck, self).__init__()\n        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n        self.bn1 = SynchronizedBatchNorm2d(planes)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,\n                               padding=1, groups=groups, bias=False)\n        self.bn2 = SynchronizedBatchNorm2d(planes)\n        self.conv3 = nn.Conv2d(planes, planes * 2, kernel_size=1, bias=False)\n        self.bn3 = SynchronizedBatchNorm2d(planes * 2)\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\n\nclass ResNeXt(nn.Module):\n\n    def __init__(self, block, layers, groups=32, num_classes=1000):\n        self.inplanes = 128\n        super(ResNeXt, self).__init__()\n        self.conv1 = conv3x3(3, 64, stride=2)\n        self.bn1 = SynchronizedBatchNorm2d(64)\n        self.relu1 = nn.ReLU(inplace=True)\n        self.conv2 = conv3x3(64, 64)\n        self.bn2 = SynchronizedBatchNorm2d(64)\n        self.relu2 = nn.ReLU(inplace=True)\n        self.conv3 = conv3x3(64, 128)\n        self.bn3 = SynchronizedBatchNorm2d(128)\n        self.relu3 = nn.ReLU(inplace=True)\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n\n        self.layer1 = self._make_layer(block, 128, layers[0], groups=groups)\n        self.layer2 = self._make_layer(block, 256, layers[1], stride=2, groups=groups)\n        self.layer3 = self._make_layer(block, 512, layers[2], stride=2, groups=groups)\n        self.layer4 = self._make_layer(block, 1024, layers[3], stride=2, groups=groups)\n        self.avgpool = nn.AvgPool2d(7, stride=1)\n        self.fc = nn.Linear(1024 * block.expansion, num_classes)\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels // m.groups\n                m.weight.data.normal_(0, math.sqrt(2. / n))\n            elif isinstance(m, SynchronizedBatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n\n    def _make_layer(self, block, planes, blocks, stride=1, groups=1):\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                nn.Conv2d(self.inplanes, planes * block.expansion,\n                          kernel_size=1, stride=stride, bias=False),\n                SynchronizedBatchNorm2d(planes * block.expansion),\n            )\n\n        layers = []\n        layers.append(block(self.inplanes, planes, stride, groups, downsample))\n        self.inplanes = planes * block.expansion\n        for i in range(1, blocks):\n            layers.append(block(self.inplanes, planes, groups=groups))\n\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        x = self.relu1(self.bn1(self.conv1(x)))\n        x = self.relu2(self.bn2(self.conv2(x)))\n        x = self.relu3(self.bn3(self.conv3(x)))\n        x = self.maxpool(x)\n\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n\n        x = self.avgpool(x)\n        x = x.view(x.size(0), -1)\n        x = self.fc(x)\n\n        return x\n\n\n\'\'\'\ndef resnext50(pretrained=False, **kwargs):\n    """"""Constructs a ResNet-50 model.\n\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on Places\n    """"""\n    model = ResNeXt(GroupBottleneck, [3, 4, 6, 3], **kwargs)\n    if pretrained:\n        model.load_state_dict(load_url(model_urls[\'resnext50\']), strict=False)\n    return model\n\'\'\'\n\n\ndef resnext101(pretrained=False, **kwargs):\n    """"""Constructs a ResNet-101 model.\n\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on Places\n    """"""\n    model = ResNeXt(GroupBottleneck, [3, 4, 23, 3], **kwargs)\n    if pretrained:\n        model.load_state_dict(load_url(model_urls[\'resnext101\']), strict=False)\n    return model\n\n\n# def resnext152(pretrained=False, **kwargs):\n#     """"""Constructs a ResNeXt-152 model.\n#\n#     Args:\n#         pretrained (bool): If True, returns a model pre-trained on Places\n#     """"""\n#     model = ResNeXt(GroupBottleneck, [3, 8, 36, 3], **kwargs)\n#     if pretrained:\n#         model.load_state_dict(load_url(model_urls[\'resnext152\']))\n#     return model\n\n\ndef load_url(url, model_dir=\'./pretrained\', map_location=None):\n    if not os.path.exists(model_dir):\n        os.makedirs(model_dir)\n    filename = url.split(\'/\')[-1]\n    cached_file = os.path.join(model_dir, filename)\n    if not os.path.exists(cached_file):\n        sys.stderr.write(\'Downloading: ""{}"" to {}\\n\'.format(url, cached_file))\n        urlretrieve(url, cached_file)\n    return torch.load(cached_file, map_location=map_location)\n'"
netdissect/tool/allunitsample.py,11,"b'\'\'\'\nA simple tool to generate sample of output of a GAN,\nsubject to filtering, sorting, or intervention.\n\'\'\'\n\nimport torch, numpy, os, argparse, sys, shutil, errno, numbers\nfrom PIL import Image\nfrom torch.utils.data import TensorDataset\nfrom netdissect.zdataset import standard_z_sample\nfrom netdissect.progress import default_progress, verbose_progress\nfrom netdissect.autoeval import autoimport_eval\nfrom netdissect.workerpool import WorkerBase, WorkerPool\nfrom netdissect.nethook import retain_layers\nfrom netdissect.runningstats import RunningTopK\n\ndef main():\n    parser = argparse.ArgumentParser(description=\'GAN sample making utility\')\n    parser.add_argument(\'--model\', type=str, default=None,\n            help=\'constructor for the model to test\')\n    parser.add_argument(\'--pthfile\', type=str, default=None,\n            help=\'filename of .pth file for the model\')\n    parser.add_argument(\'--outdir\', type=str, default=\'images\',\n            help=\'directory for image output\')\n    parser.add_argument(\'--size\', type=int, default=100,\n            help=\'number of images to output\')\n    parser.add_argument(\'--test_size\', type=int, default=None,\n            help=\'number of images to test\')\n    parser.add_argument(\'--layer\', type=str, default=None,\n            help=\'layer to inspect\')\n    parser.add_argument(\'--seed\', type=int, default=1,\n            help=\'seed\')\n    parser.add_argument(\'--quiet\', action=\'store_true\', default=False,\n            help=\'silences console output\')\n    if len(sys.argv) == 1:\n        parser.print_usage(sys.stderr)\n        sys.exit(1)\n    args = parser.parse_args()\n    verbose_progress(not args.quiet)\n\n    # Instantiate the model\n    model = autoimport_eval(args.model)\n    if args.pthfile is not None:\n        data = torch.load(args.pthfile)\n        if \'state_dict\' in data:\n            meta = {}\n            for key in data:\n                if isinstance(data[key], numbers.Number):\n                    meta[key] = data[key]\n            data = data[\'state_dict\']\n        model.load_state_dict(data)\n    # Unwrap any DataParallel-wrapped model\n    if isinstance(model, torch.nn.DataParallel):\n        model = next(model.children())\n    # Examine first conv in model to determine input feature size.\n    first_layer = [c for c in model.modules()\n            if isinstance(c, (torch.nn.Conv2d, torch.nn.ConvTranspose2d,\n                torch.nn.Linear))][0]\n    # 4d input if convolutional, 2d input if first layer is linear.\n    if isinstance(first_layer, (torch.nn.Conv2d, torch.nn.ConvTranspose2d)):\n        z_channels = first_layer.in_channels\n        spatialdims = (1, 1)\n    else:\n        z_channels = first_layer.in_features\n        spatialdims = ()\n    # Instrument the model\n    retain_layers(model, [args.layer])\n    model.cuda()\n\n    if args.test_size is None:\n        args.test_size = args.size * 20\n    z_universe = standard_z_sample(args.test_size, z_channels,\n            seed=args.seed)\n    z_universe = z_universe.view(tuple(z_universe.shape) + spatialdims)\n    indexes = get_all_highest_znums(\n            model, z_universe, args.size, seed=args.seed)\n    save_chosen_unit_images(args.outdir, model, z_universe, indexes,\n            lightbox=True)\n\n\ndef get_all_highest_znums(model, z_universe, size,\n        batch_size=10, seed=1):\n    # The model should have been instrumented already\n    retained_items = list(model.retained.items())\n    assert len(retained_items) == 1\n    layer = retained_items[0][0]\n    # By default, a 10% sample\n    progress = default_progress()\n    num_units = None\n    with torch.no_grad():\n        # Pass 1: collect max activation stats\n        z_loader = torch.utils.data.DataLoader(TensorDataset(z_universe),\n                    batch_size=batch_size, num_workers=2,\n                    pin_memory=True)\n        rtk = RunningTopK(k=size)\n        for [z] in progress(z_loader, desc=\'Finding max activations\'):\n            z = z.cuda()\n            model(z)\n            feature = model.retained[layer]\n            num_units = feature.shape[1]\n            max_feature = feature.view(\n                    feature.shape[0], num_units, -1).max(2)[0]\n            rtk.add(max_feature)\n        td, ti = rtk.result()\n        highest = ti.sort(1)[0]\n    return highest\n\ndef save_chosen_unit_images(dirname, model, z_universe, indices,\n        shared_dir=""shared_images"",\n        unitdir_template=""unit_{}"",\n        name_template=""image_{}.jpg"",\n        lightbox=False, batch_size=50, seed=1):\n    all_indices = torch.unique(indices.view(-1), sorted=True)\n    z_sample = z_universe[all_indices]\n    progress = default_progress()\n    sdir = os.path.join(dirname, shared_dir)\n    created_hashdirs = set()\n    for index in range(len(z_universe)):\n        hd = hashdir(index)\n        if hd not in created_hashdirs:\n            created_hashdirs.add(hd)\n            os.makedirs(os.path.join(sdir, hd), exist_ok=True)\n    with torch.no_grad():\n        # Pass 2: now generate images\n        z_loader = torch.utils.data.DataLoader(TensorDataset(z_sample),\n                    batch_size=batch_size, num_workers=2,\n                    pin_memory=True)\n        saver = WorkerPool(SaveImageWorker)\n        for batch_num, [z] in enumerate(progress(z_loader,\n                desc=\'Saving images\')):\n            z = z.cuda()\n            start_index = batch_num * batch_size\n            im = ((model(z) + 1) / 2 * 255).clamp(0, 255).byte().permute(\n                    0, 2, 3, 1).cpu()\n            for i in range(len(im)):\n                index = all_indices[i + start_index].item()\n                filename = os.path.join(sdir, hashdir(index),\n                        name_template.format(index))\n                saver.add(im[i].numpy(), filename)\n        saver.join()\n    linker = WorkerPool(MakeLinkWorker)\n    for u in progress(range(len(indices)), desc=\'Making links\'):\n        udir = os.path.join(dirname, unitdir_template.format(u))\n        os.makedirs(udir, exist_ok=True)\n        for r in range(indices.shape[1]):\n            index = indices[u,r].item()\n            fn = name_template.format(index)\n            # sourcename = os.path.join(\'..\', shared_dir, fn)\n            sourcename = os.path.join(sdir, hashdir(index), fn)\n            targname = os.path.join(udir, fn)\n            linker.add(sourcename, targname)\n        if lightbox:\n            copy_lightbox_to(udir)\n    linker.join()\n\ndef copy_lightbox_to(dirname):\n   srcdir = os.path.realpath(\n       os.path.join(os.getcwd(), os.path.dirname(__file__)))\n   shutil.copy(os.path.join(srcdir, \'lightbox.html\'),\n           os.path.join(dirname, \'+lightbox.html\'))\n\ndef hashdir(index):\n    # To keep the number of files the shared directory lower, split it\n    # into 100 subdirectories named as follows.\n    return \'%02d\' % (index % 100)\n\nclass SaveImageWorker(WorkerBase):\n    # Saving images can be sped up by sending jpeg encoding and\n    # file-writing work to a pool.\n    def work(self, data, filename):\n        Image.fromarray(data).save(filename, optimize=True, quality=100)\n\nclass MakeLinkWorker(WorkerBase):\n    # Creating symbolic links is a bit slow and can be done faster\n    # in parallel rather than waiting for each to be created.\n    def work(self, sourcename, targname):\n        try:\n            os.link(sourcename, targname)\n        except OSError as e:\n            if e.errno == errno.EEXIST:\n                os.remove(targname)\n                os.link(sourcename, targname)\n            else:\n                raise\n\nclass MakeSyminkWorker(WorkerBase):\n    # Creating symbolic links is a bit slow and can be done faster\n    # in parallel rather than waiting for each to be created.\n    def work(self, sourcename, targname):\n        try:\n            os.symlink(sourcename, targname)\n        except OSError as e:\n            if e.errno == errno.EEXIST:\n                os.remove(targname)\n                os.symlink(sourcename, targname)\n            else:\n                raise\n\nif __name__ == \'__main__\':\n    main()\n'"
netdissect/tool/ganseg.py,3,"b'\'\'\'\nA simple tool to generate sample of output of a GAN,\nand apply semantic segmentation on the output.\n\'\'\'\n\nimport torch, numpy, os, argparse, sys, shutil\nfrom PIL import Image\nfrom torch.utils.data import TensorDataset\nfrom netdissect.zdataset import standard_z_sample, z_dataset_for_model\nfrom netdissect.progress import default_progress, verbose_progress\nfrom netdissect.autoeval import autoimport_eval\nfrom netdissect.workerpool import WorkerBase, WorkerPool\nfrom netdissect.nethook import edit_layers, retain_layers\nfrom netdissect.segviz import segment_visualization\nfrom netdissect.segmenter import UnifiedParsingSegmenter\nfrom scipy.io import savemat\n\ndef main():\n    parser = argparse.ArgumentParser(description=\'GAN output segmentation util\')\n    parser.add_argument(\'--model\', type=str, default=\n            \'netdissect.proggan.from_pth_file(""\' + \n            \'models/karras/churchoutdoor_lsun.pth"")\',\n            help=\'constructor for the model to test\')\n    parser.add_argument(\'--outdir\', type=str, default=\'images\',\n            help=\'directory for image output\')\n    parser.add_argument(\'--size\', type=int, default=100,\n            help=\'number of images to output\')\n    parser.add_argument(\'--seed\', type=int, default=1,\n            help=\'seed\')\n    parser.add_argument(\'--quiet\', action=\'store_true\', default=False,\n            help=\'silences console output\')\n    #if len(sys.argv) == 1:\n    #    parser.print_usage(sys.stderr)\n    #    sys.exit(1)\n    args = parser.parse_args()\n    verbose_progress(not args.quiet)\n\n    # Instantiate the model\n    model = autoimport_eval(args.model)\n\n    # Make the standard z\n    z_dataset = z_dataset_for_model(model, size=args.size)\n\n    # Make the segmenter\n    segmenter = UnifiedParsingSegmenter()\n\n    # Write out text labels\n    labels, cats = segmenter.get_label_and_category_names()\n    with open(os.path.join(args.outdir, \'labels.txt\'), \'w\') as f:\n        for i, (label, cat) in enumerate(labels):\n            f.write(\'%s %s\\n\' % (label, cat))\n\n    # Move models to cuda\n    model.cuda()\n\n    batch_size = 10\n    progress = default_progress()\n    dirname = args.outdir\n\n    with torch.no_grad():\n        # Pass 2: now generate images\n        z_loader = torch.utils.data.DataLoader(z_dataset,\n                    batch_size=batch_size, num_workers=2,\n                    pin_memory=True)\n        for batch_num, [z] in enumerate(progress(z_loader,\n                desc=\'Saving images\')):\n            z = z.cuda()\n            start_index = batch_num * batch_size\n            tensor_im = model(z)\n            byte_im = ((tensor_im + 1) / 2 * 255).clamp(0, 255).byte().permute(\n                    0, 2, 3, 1).cpu()\n            seg = segmenter.segment_batch(tensor_im)\n            for i in range(len(tensor_im)):\n                index = i + start_index\n                filename = os.path.join(dirname, \'%d_img.jpg\' % index)\n                Image.fromarray(byte_im[i].numpy()).save(\n                        filename, optimize=True, quality=100)\n                filename = os.path.join(dirname, \'%d_seg.mat\' % index)\n                savemat(filename, dict(seg=seg[i].cpu().numpy()))\n                filename = os.path.join(dirname, \'%d_seg.png\' % index)\n                Image.fromarray(segment_visualization(seg[i].cpu().numpy(),\n                    tensor_im.shape[2:])).save(filename)\n    srcdir = os.path.realpath(\n       os.path.join(os.getcwd(), os.path.dirname(__file__)))\n    shutil.copy(os.path.join(srcdir, \'lightbox.html\'),\n           os.path.join(dirname, \'+lightbox.html\'))\n\nif __name__ == \'__main__\':\n    main()\n'"
netdissect/tool/makesample.py,14,"b'\'\'\'\nA simple tool to generate sample of output of a GAN,\nsubject to filtering, sorting, or intervention.\n\'\'\'\n\nimport torch, numpy, os, argparse, numbers, sys, shutil\nfrom PIL import Image\nfrom torch.utils.data import TensorDataset\nfrom netdissect.zdataset import standard_z_sample\nfrom netdissect.progress import default_progress, verbose_progress\nfrom netdissect.autoeval import autoimport_eval\nfrom netdissect.workerpool import WorkerBase, WorkerPool\nfrom netdissect.nethook import edit_layers, retain_layers\n\ndef main():\n    parser = argparse.ArgumentParser(description=\'GAN sample making utility\')\n    parser.add_argument(\'--model\', type=str, default=None,\n            help=\'constructor for the model to test\')\n    parser.add_argument(\'--pthfile\', type=str, default=None,\n            help=\'filename of .pth file for the model\')\n    parser.add_argument(\'--outdir\', type=str, default=\'images\',\n            help=\'directory for image output\')\n    parser.add_argument(\'--size\', type=int, default=100,\n            help=\'number of images to output\')\n    parser.add_argument(\'--test_size\', type=int, default=None,\n            help=\'number of images to test\')\n    parser.add_argument(\'--layer\', type=str, default=None,\n            help=\'layer to inspect\')\n    parser.add_argument(\'--seed\', type=int, default=1,\n            help=\'seed\')\n    parser.add_argument(\'--maximize_units\', type=int, nargs=\'+\', default=None,\n            help=\'units to maximize\')\n    parser.add_argument(\'--ablate_units\', type=int, nargs=\'+\', default=None,\n            help=\'units to ablate\')\n    parser.add_argument(\'--quiet\', action=\'store_true\', default=False,\n            help=\'silences console output\')\n    if len(sys.argv) == 1:\n        parser.print_usage(sys.stderr)\n        sys.exit(1)\n    args = parser.parse_args()\n    verbose_progress(not args.quiet)\n\n    # Instantiate the model\n    model = autoimport_eval(args.model)\n    if args.pthfile is not None:\n        data = torch.load(args.pthfile)\n        if \'state_dict\' in data:\n            meta = {}\n            for key in data:\n                if isinstance(data[key], numbers.Number):\n                    meta[key] = data[key]\n            data = data[\'state_dict\']\n        model.load_state_dict(data)\n    # Unwrap any DataParallel-wrapped model\n    if isinstance(model, torch.nn.DataParallel):\n        model = next(model.children())\n    # Examine first conv in model to determine input feature size.\n    first_layer = [c for c in model.modules()\n            if isinstance(c, (torch.nn.Conv2d, torch.nn.ConvTranspose2d,\n                torch.nn.Linear))][0]\n    # 4d input if convolutional, 2d input if first layer is linear.\n    if isinstance(first_layer, (torch.nn.Conv2d, torch.nn.ConvTranspose2d)):\n        z_channels = first_layer.in_channels\n        spatialdims = (1, 1)\n    else:\n        z_channels = first_layer.in_features\n        spatialdims = ()\n    # Instrument the model if needed\n    if args.maximize_units is not None:\n        retain_layers(model, [args.layer])\n    model.cuda()\n\n    # Get the sample of z vectors\n    if args.maximize_units is None:\n        indexes = torch.arange(args.size)\n        z_sample = standard_z_sample(args.size, z_channels, seed=args.seed)\n        z_sample = z_sample.view(tuple(z_sample.shape) + spatialdims)\n    else:\n        # By default, if maximizing units, get a \'top 5%\' sample.\n        if args.test_size is None:\n            args.test_size = args.size * 20\n        z_universe = standard_z_sample(args.test_size, z_channels,\n                seed=args.seed)\n        z_universe = z_universe.view(tuple(z_universe.shape) + spatialdims)\n        indexes = get_highest_znums(model, z_universe, args.maximize_units,\n                args.size, seed=args.seed)\n        z_sample = z_universe[indexes]\n\n    if args.ablate_units:\n        edit_layers(model, [args.layer])\n        dims = max(2, max(args.ablate_units) + 1) # >=2 to avoid broadcast\n        model.ablation[args.layer] = torch.zeros(dims)\n        model.ablation[args.layer][args.ablate_units] = 1\n\n    save_znum_images(args.outdir, model, z_sample, indexes,\n            args.layer, args.ablate_units)\n    copy_lightbox_to(args.outdir)\n\n\ndef get_highest_znums(model, z_universe, max_units, size,\n        batch_size=100, seed=1):\n    # The model should have been instrumented already\n    retained_items = list(model.retained.items())\n    assert len(retained_items) == 1\n    layer = retained_items[0][0]\n    # By default, a 10% sample\n    progress = default_progress()\n    num_units = None\n    with torch.no_grad():\n        # Pass 1: collect max activation stats\n        z_loader = torch.utils.data.DataLoader(TensorDataset(z_universe),\n                    batch_size=batch_size, num_workers=2,\n                    pin_memory=True)\n        scores = []\n        for [z] in progress(z_loader, desc=\'Finding max activations\'):\n            z = z.cuda()\n            model(z)\n            feature = model.retained[layer]\n            num_units = feature.shape[1]\n            max_feature = feature[:, max_units, ...].view(\n                    feature.shape[0], len(max_units), -1).max(2)[0]\n            total_feature = max_feature.sum(1)\n            scores.append(total_feature.cpu())\n        scores = torch.cat(scores, 0)\n        highest = (-scores).sort(0)[1][:size].sort(0)[0]\n    return highest\n\n\ndef save_znum_images(dirname, model, z_sample, indexes, layer, ablated_units,\n        name_template=""image_{}.png"", lightbox=False, batch_size=100, seed=1):\n    progress = default_progress()\n    os.makedirs(dirname, exist_ok=True)\n    with torch.no_grad():\n        # Pass 2: now generate images\n        z_loader = torch.utils.data.DataLoader(TensorDataset(z_sample),\n                    batch_size=batch_size, num_workers=2,\n                    pin_memory=True)\n        saver = WorkerPool(SaveImageWorker)\n        if ablated_units is not None:\n            dims = max(2, max(ablated_units) + 1) # >=2 to avoid broadcast\n            mask = torch.zeros(dims)\n            mask[ablated_units] = 1\n            model.ablation[layer] = mask[None,:,None,None].cuda()\n        for batch_num, [z] in enumerate(progress(z_loader,\n                desc=\'Saving images\')):\n            z = z.cuda()\n            start_index = batch_num * batch_size\n            im = ((model(z) + 1) / 2 * 255).clamp(0, 255).byte().permute(\n                    0, 2, 3, 1).cpu()\n            for i in range(len(im)):\n                index = i + start_index\n                if indexes is not None:\n                    index = indexes[index].item()\n                filename = os.path.join(dirname, name_template.format(index))\n                saver.add(im[i].numpy(), filename)\n    saver.join()\n\ndef copy_lightbox_to(dirname):\n   srcdir = os.path.realpath(\n       os.path.join(os.getcwd(), os.path.dirname(__file__)))\n   shutil.copy(os.path.join(srcdir, \'lightbox.html\'),\n           os.path.join(dirname, \'+lightbox.html\'))\n\nclass SaveImageWorker(WorkerBase):\n    def work(self, data, filename):\n        Image.fromarray(data).save(filename, optimize=True, quality=100)\n\nif __name__ == \'__main__\':\n    main()\n'"
netdissect/upsegmodel/__init__.py,0,"b'from .models import ModelBuilder, SegmentationModule\n'"
netdissect/upsegmodel/models.py,20,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision\nfrom . import resnet, resnext\ntry:\n    from lib.nn import SynchronizedBatchNorm2d\nexcept ImportError:\n    from torch.nn import BatchNorm2d as SynchronizedBatchNorm2d\n\n\nclass SegmentationModuleBase(nn.Module):\n    def __init__(self):\n        super(SegmentationModuleBase, self).__init__()\n\n    @staticmethod\n    def pixel_acc(pred, label, ignore_index=-1):\n        _, preds = torch.max(pred, dim=1)\n        valid = (label != ignore_index).long()\n        acc_sum = torch.sum(valid * (preds == label).long())\n        pixel_sum = torch.sum(valid)\n        acc = acc_sum.float() / (pixel_sum.float() + 1e-10)\n        return acc\n\n    @staticmethod\n    def part_pixel_acc(pred_part, gt_seg_part, gt_seg_object, object_label, valid):\n        mask_object = (gt_seg_object == object_label)\n        _, pred = torch.max(pred_part, dim=1)\n        acc_sum = mask_object * (pred == gt_seg_part)\n        acc_sum = torch.sum(acc_sum.view(acc_sum.size(0), -1), dim=1)\n        acc_sum = torch.sum(acc_sum * valid)\n        pixel_sum = torch.sum(mask_object.view(mask_object.size(0), -1), dim=1)\n        pixel_sum = torch.sum(pixel_sum * valid)\n        return acc_sum, pixel_sum \n\n    @staticmethod\n    def part_loss(pred_part, gt_seg_part, gt_seg_object, object_label, valid):\n        mask_object = (gt_seg_object == object_label)\n        loss = F.nll_loss(pred_part, gt_seg_part * mask_object.long(), reduction=\'none\')\n        loss = loss * mask_object.float()\n        loss = torch.sum(loss.view(loss.size(0), -1), dim=1)\n        nr_pixel = torch.sum(mask_object.view(mask_object.shape[0], -1), dim=1)\n        sum_pixel = (nr_pixel * valid).sum()\n        loss = (loss * valid.float()).sum() / torch.clamp(sum_pixel, 1).float()\n        return loss\n\n\nclass SegmentationModule(SegmentationModuleBase):\n    def __init__(self, net_enc, net_dec, labeldata, loss_scale=None):\n        super(SegmentationModule, self).__init__()\n        self.encoder = net_enc\n        self.decoder = net_dec\n        self.crit_dict = nn.ModuleDict()\n        if loss_scale is None:\n            self.loss_scale = {""object"": 1, ""part"": 0.5, ""scene"": 0.25, ""material"": 1}\n        else:\n            self.loss_scale = loss_scale\n\n        # criterion\n        self.crit_dict[""object""] = nn.NLLLoss(ignore_index=0)  # ignore background 0\n        self.crit_dict[""material""] = nn.NLLLoss(ignore_index=0)  # ignore background 0\n        self.crit_dict[""scene""] = nn.NLLLoss(ignore_index=-1)  # ignore unlabelled -1\n\n        # Label data - read from json\n        self.labeldata = labeldata\n        object_to_num = {k: v for v, k in enumerate(labeldata[\'object\'])}\n        part_to_num = {k: v for v, k in enumerate(labeldata[\'part\'])}\n        self.object_part = {object_to_num[k]:\n                [part_to_num[p] for p in v]\n                for k, v in labeldata[\'object_part\'].items()}\n        self.object_with_part = sorted(self.object_part.keys())\n        self.decoder.object_part = self.object_part\n        self.decoder.object_with_part = self.object_with_part\n\n    def forward(self, feed_dict, *, seg_size=None):\n        if seg_size is None: # training\n\n            if feed_dict[\'source_idx\'] == 0:\n                output_switch = {""object"": True, ""part"": True, ""scene"": True, ""material"": False}\n            elif feed_dict[\'source_idx\'] == 1:\n                output_switch = {""object"": False, ""part"": False, ""scene"": False, ""material"": True}\n            else:\n                raise ValueError\n\n            pred = self.decoder(\n                self.encoder(feed_dict[\'img\'], return_feature_maps=True),\n                output_switch=output_switch\n            )\n\n            # loss\n            loss_dict = {}\n            if pred[\'object\'] is not None:  # object\n                loss_dict[\'object\'] = self.crit_dict[\'object\'](pred[\'object\'], feed_dict[\'seg_object\'])\n            if pred[\'part\'] is not None:  # part\n                part_loss = 0\n                for idx_part, object_label in enumerate(self.object_with_part):\n                    part_loss += self.part_loss(\n                        pred[\'part\'][idx_part], feed_dict[\'seg_part\'],\n                        feed_dict[\'seg_object\'], object_label, feed_dict[\'valid_part\'][:, idx_part])\n                loss_dict[\'part\'] = part_loss\n            if pred[\'scene\'] is not None:  # scene\n                loss_dict[\'scene\'] = self.crit_dict[\'scene\'](pred[\'scene\'], feed_dict[\'scene_label\'])\n            if pred[\'material\'] is not None:  # material\n                loss_dict[\'material\'] = self.crit_dict[\'material\'](pred[\'material\'], feed_dict[\'seg_material\'])\n            loss_dict[\'total\'] = sum([loss_dict[k] * self.loss_scale[k] for k in loss_dict.keys()])\n\n            # metric \n            metric_dict= {}\n            if pred[\'object\'] is not None:\n                metric_dict[\'object\'] = self.pixel_acc(\n                    pred[\'object\'], feed_dict[\'seg_object\'], ignore_index=0)\n            if pred[\'material\'] is not None:\n                metric_dict[\'material\'] = self.pixel_acc(\n                    pred[\'material\'], feed_dict[\'seg_material\'], ignore_index=0)\n            if pred[\'part\'] is not None:\n                acc_sum, pixel_sum = 0, 0\n                for idx_part, object_label in enumerate(self.object_with_part):\n                    acc, pixel = self.part_pixel_acc(\n                        pred[\'part\'][idx_part], feed_dict[\'seg_part\'], feed_dict[\'seg_object\'],\n                        object_label, feed_dict[\'valid_part\'][:, idx_part])\n                    acc_sum += acc\n                    pixel_sum += pixel\n                metric_dict[\'part\'] = acc_sum.float() / (pixel_sum.float() + 1e-10)\n            if pred[\'scene\'] is not None:\n                metric_dict[\'scene\'] = self.pixel_acc(\n                    pred[\'scene\'], feed_dict[\'scene_label\'], ignore_index=-1)\n\n            return {\'metric\': metric_dict, \'loss\': loss_dict}\n        else: # inference\n            output_switch = {""object"": True, ""part"": True, ""scene"": True, ""material"": True}\n            pred = self.decoder(self.encoder(feed_dict[\'img\'], return_feature_maps=True),\n                                output_switch=output_switch, seg_size=seg_size)\n            return pred\n\n\ndef conv3x3(in_planes, out_planes, stride=1, has_bias=False):\n    ""3x3 convolution with padding""\n    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n                     padding=1, bias=has_bias)\n\n\ndef conv3x3_bn_relu(in_planes, out_planes, stride=1):\n    return nn.Sequential(\n            conv3x3(in_planes, out_planes, stride),\n            SynchronizedBatchNorm2d(out_planes),\n            nn.ReLU(inplace=True),\n            )\n\n\nclass ModelBuilder:\n    def __init__(self):\n        pass\n\n    # custom weights initialization\n    @staticmethod\n    def weights_init(m):\n        classname = m.__class__.__name__\n        if classname.find(\'Conv\') != -1:\n            nn.init.kaiming_normal_(m.weight.data, nonlinearity=\'relu\')\n        elif classname.find(\'BatchNorm\') != -1:\n            m.weight.data.fill_(1.)\n            m.bias.data.fill_(1e-4)\n        #elif classname.find(\'Linear\') != -1:\n        #    m.weight.data.normal_(0.0, 0.0001)\n\n    def build_encoder(self, arch=\'resnet50_dilated8\', fc_dim=512, weights=\'\'):\n        pretrained = True if len(weights) == 0 else False\n        if arch == \'resnet34\':\n            raise NotImplementedError\n            orig_resnet = resnet.__dict__[\'resnet34\'](pretrained=pretrained)\n            net_encoder = Resnet(orig_resnet)\n        elif arch == \'resnet34_dilated8\':\n            raise NotImplementedError\n            orig_resnet = resnet.__dict__[\'resnet34\'](pretrained=pretrained)\n            net_encoder = ResnetDilated(orig_resnet,\n                                        dilate_scale=8)\n        elif arch == \'resnet34_dilated16\':\n            raise NotImplementedError\n            orig_resnet = resnet.__dict__[\'resnet34\'](pretrained=pretrained)\n            net_encoder = ResnetDilated(orig_resnet,\n                                        dilate_scale=16)\n        elif arch == \'resnet50\':\n            orig_resnet = resnet.__dict__[\'resnet50\'](pretrained=pretrained)\n            net_encoder = Resnet(orig_resnet)\n        elif arch == \'resnet101\':\n            orig_resnet = resnet.__dict__[\'resnet101\'](pretrained=pretrained)\n            net_encoder = Resnet(orig_resnet)\n        elif arch == \'resnext101\':\n            orig_resnext = resnext.__dict__[\'resnext101\'](pretrained=pretrained)\n            net_encoder = Resnet(orig_resnext) # we can still use class Resnet\n        else:\n            raise Exception(\'Architecture undefined!\')\n\n        # net_encoder.apply(self.weights_init)\n        if len(weights) > 0:\n            # print(\'Loading weights for net_encoder\')\n            net_encoder.load_state_dict(\n                torch.load(weights, map_location=lambda storage, loc: storage), strict=False)\n        return net_encoder\n\n    def build_decoder(self, nr_classes,\n                      arch=\'ppm_bilinear_deepsup\', fc_dim=512,\n                      weights=\'\', use_softmax=False):\n        if arch == \'upernet_lite\':\n            net_decoder = UPerNet(\n                nr_classes=nr_classes,\n                fc_dim=fc_dim,\n                use_softmax=use_softmax,\n                fpn_dim=256)\n        elif arch == \'upernet\':\n            net_decoder = UPerNet(\n                nr_classes=nr_classes,\n                fc_dim=fc_dim,\n                use_softmax=use_softmax,\n                fpn_dim=512)\n        else:\n            raise Exception(\'Architecture undefined!\')\n\n        net_decoder.apply(self.weights_init)\n        if len(weights) > 0:\n            # print(\'Loading weights for net_decoder\')\n            net_decoder.load_state_dict(\n                torch.load(weights, map_location=lambda storage, loc: storage), strict=False)\n        return net_decoder\n\n\nclass Resnet(nn.Module):\n    def __init__(self, orig_resnet):\n        super(Resnet, self).__init__()\n\n        # take pretrained resnet, except AvgPool and FC\n        self.conv1 = orig_resnet.conv1\n        self.bn1 = orig_resnet.bn1\n        self.relu1 = orig_resnet.relu1\n        self.conv2 = orig_resnet.conv2\n        self.bn2 = orig_resnet.bn2\n        self.relu2 = orig_resnet.relu2\n        self.conv3 = orig_resnet.conv3\n        self.bn3 = orig_resnet.bn3\n        self.relu3 = orig_resnet.relu3\n        self.maxpool = orig_resnet.maxpool\n        self.layer1 = orig_resnet.layer1\n        self.layer2 = orig_resnet.layer2\n        self.layer3 = orig_resnet.layer3\n        self.layer4 = orig_resnet.layer4\n\n    def forward(self, x, return_feature_maps=False):\n        conv_out = []\n\n        x = self.relu1(self.bn1(self.conv1(x)))\n        x = self.relu2(self.bn2(self.conv2(x)))\n        x = self.relu3(self.bn3(self.conv3(x)))\n        x = self.maxpool(x)\n\n        x = self.layer1(x); conv_out.append(x);\n        x = self.layer2(x); conv_out.append(x);\n        x = self.layer3(x); conv_out.append(x);\n        x = self.layer4(x); conv_out.append(x);\n\n        if return_feature_maps:\n            return conv_out\n        return [x]\n\n\n# upernet\nclass UPerNet(nn.Module):\n    def __init__(self, nr_classes, fc_dim=4096,\n                 use_softmax=False, pool_scales=(1, 2, 3, 6),\n                 fpn_inplanes=(256,512,1024,2048), fpn_dim=256):\n        # Lazy import so that compilation isn\'t needed if not being used.\n        from .prroi_pool import PrRoIPool2D\n        super(UPerNet, self).__init__()\n        self.use_softmax = use_softmax\n\n        # PPM Module\n        self.ppm_pooling = []\n        self.ppm_conv = []\n\n        for scale in pool_scales:\n            # we use the feature map size instead of input image size, so down_scale = 1.0\n            self.ppm_pooling.append(PrRoIPool2D(scale, scale, 1.))\n            self.ppm_conv.append(nn.Sequential(\n                nn.Conv2d(fc_dim, 512, kernel_size=1, bias=False),\n                SynchronizedBatchNorm2d(512),\n                nn.ReLU(inplace=True)\n            ))\n        self.ppm_pooling = nn.ModuleList(self.ppm_pooling)\n        self.ppm_conv = nn.ModuleList(self.ppm_conv)\n        self.ppm_last_conv = conv3x3_bn_relu(fc_dim + len(pool_scales)*512, fpn_dim, 1)\n\n        # FPN Module\n        self.fpn_in = []\n        for fpn_inplane in fpn_inplanes[:-1]: # skip the top layer\n            self.fpn_in.append(nn.Sequential(\n                nn.Conv2d(fpn_inplane, fpn_dim, kernel_size=1, bias=False),\n                SynchronizedBatchNorm2d(fpn_dim),\n                nn.ReLU(inplace=True)\n            ))\n        self.fpn_in = nn.ModuleList(self.fpn_in)\n\n        self.fpn_out = []\n        for i in range(len(fpn_inplanes) - 1): # skip the top layer\n            self.fpn_out.append(nn.Sequential(\n                conv3x3_bn_relu(fpn_dim, fpn_dim, 1),\n            ))\n        self.fpn_out = nn.ModuleList(self.fpn_out)\n\n        self.conv_fusion = conv3x3_bn_relu(len(fpn_inplanes) * fpn_dim, fpn_dim, 1)\n\n        # background included. if ignore in loss, output channel 0 will not be trained.\n        self.nr_scene_class, self.nr_object_class, self.nr_part_class, self.nr_material_class = \\\n            nr_classes[\'scene\'], nr_classes[\'object\'], nr_classes[\'part\'], nr_classes[\'material\']\n\n        # input: PPM out, input_dim: fpn_dim\n        self.scene_head = nn.Sequential(\n            conv3x3_bn_relu(fpn_dim, fpn_dim, 1),\n            nn.AdaptiveAvgPool2d(1),\n            nn.Conv2d(fpn_dim, self.nr_scene_class, kernel_size=1, bias=True)\n        )\n\n        # input: Fusion out, input_dim: fpn_dim\n        self.object_head = nn.Sequential(\n            conv3x3_bn_relu(fpn_dim, fpn_dim, 1),\n            nn.Conv2d(fpn_dim, self.nr_object_class, kernel_size=1, bias=True)\n        )\n\n        # input: Fusion out, input_dim: fpn_dim\n        self.part_head = nn.Sequential(\n            conv3x3_bn_relu(fpn_dim, fpn_dim, 1),\n            nn.Conv2d(fpn_dim, self.nr_part_class, kernel_size=1, bias=True)\n        )\n\n        # input: FPN_2 (P2), input_dim: fpn_dim\n        self.material_head = nn.Sequential(\n            conv3x3_bn_relu(fpn_dim, fpn_dim, 1),\n            nn.Conv2d(fpn_dim, self.nr_material_class, kernel_size=1, bias=True)\n        )\n\n    def forward(self, conv_out, output_switch=None, seg_size=None):\n\n        output_dict = {k: None for k in output_switch.keys()}\n\n        conv5 = conv_out[-1]\n        input_size = conv5.size()\n        ppm_out = [conv5]\n        roi = [] # fake rois, just used for pooling\n        for i in range(input_size[0]): # batch size\n            roi.append(torch.Tensor([i, 0, 0, input_size[3], input_size[2]]).view(1, -1)) # b, x0, y0, x1, y1\n        roi = torch.cat(roi, dim=0).type_as(conv5)\n        ppm_out = [conv5]\n        for pool_scale, pool_conv in zip(self.ppm_pooling, self.ppm_conv):\n            ppm_out.append(pool_conv(F.interpolate(\n                pool_scale(conv5, roi.detach()),\n                (input_size[2], input_size[3]),\n                mode=\'bilinear\', align_corners=False)))\n        ppm_out = torch.cat(ppm_out, 1)\n        f = self.ppm_last_conv(ppm_out)\n\n        if output_switch[\'scene\']: # scene\n            output_dict[\'scene\'] = self.scene_head(f)\n\n        if output_switch[\'object\'] or output_switch[\'part\'] or output_switch[\'material\']:\n            fpn_feature_list = [f]\n            for i in reversed(range(len(conv_out) - 1)):\n                conv_x = conv_out[i]\n                conv_x = self.fpn_in[i](conv_x) # lateral branch\n\n                f = F.interpolate(\n                    f, size=conv_x.size()[2:], mode=\'bilinear\', align_corners=False) # top-down branch\n                f = conv_x + f\n\n                fpn_feature_list.append(self.fpn_out[i](f))\n            fpn_feature_list.reverse() # [P2 - P5]\n\n            # material\n            if output_switch[\'material\']:\n                output_dict[\'material\'] = self.material_head(fpn_feature_list[0])\n\n            if output_switch[\'object\'] or output_switch[\'part\']:\n                output_size = fpn_feature_list[0].size()[2:]\n                fusion_list = [fpn_feature_list[0]]\n                for i in range(1, len(fpn_feature_list)):\n                    fusion_list.append(F.interpolate(\n                        fpn_feature_list[i],\n                        output_size,\n                        mode=\'bilinear\', align_corners=False))\n                fusion_out = torch.cat(fusion_list, 1)\n                x = self.conv_fusion(fusion_out)\n\n                if output_switch[\'object\']: # object\n                    output_dict[\'object\'] = self.object_head(x)\n                if output_switch[\'part\']:\n                    output_dict[\'part\'] = self.part_head(x)\n\n        if self.use_softmax:  # is True during inference\n            # inference scene\n            x = output_dict[\'scene\']\n            x = x.squeeze(3).squeeze(2)\n            x = F.softmax(x, dim=1)\n            output_dict[\'scene\'] = x\n\n            # inference object, material\n            for k in [\'object\', \'material\']:\n                x = output_dict[k]\n                x = F.interpolate(x, size=seg_size, mode=\'bilinear\', align_corners=False)\n                x = F.softmax(x, dim=1)\n                output_dict[k] = x\n\n            # inference part\n            x = output_dict[\'part\']\n            x = F.interpolate(x, size=seg_size, mode=\'bilinear\', align_corners=False)\n            part_pred_list, head = [], 0\n            for idx_part, object_label in enumerate(self.object_with_part):\n                n_part = len(self.object_part[object_label])\n                _x = F.interpolate(x[:, head: head + n_part], size=seg_size, mode=\'bilinear\', align_corners=False)\n                _x = F.softmax(_x, dim=1)\n                part_pred_list.append(_x)\n                head += n_part\n            output_dict[\'part\'] = part_pred_list\n\n        else:   # Training\n            # object, scene, material\n            for k in [\'object\', \'scene\', \'material\']:\n                if output_dict[k] is None:\n                    continue\n                x = output_dict[k]\n                x = F.log_softmax(x, dim=1)\n                if k == ""scene"":  # for scene\n                    x = x.squeeze(3).squeeze(2)\n                output_dict[k] = x\n            if output_dict[\'part\'] is not None:\n                part_pred_list, head = [], 0\n                for idx_part, object_label in enumerate(self.object_with_part):\n                    n_part = len(self.object_part[object_label])\n                    x = output_dict[\'part\'][:, head: head + n_part]\n                    x = F.log_softmax(x, dim=1)\n                    part_pred_list.append(x)\n                    head += n_part\n                output_dict[\'part\'] = part_pred_list\n\n        return output_dict\n'"
netdissect/upsegmodel/resnet.py,3,"b'import os\nimport sys\nimport torch\nimport torch.nn as nn\nimport math\ntry:\n    from lib.nn import SynchronizedBatchNorm2d\nexcept ImportError:\n    from torch.nn import BatchNorm2d as SynchronizedBatchNorm2d\n\ntry:\n    from urllib import urlretrieve\nexcept ImportError:\n    from urllib.request import urlretrieve\n\n\n__all__ = [\'ResNet\', \'resnet50\', \'resnet101\'] # resnet101 is coming soon!\n\n\nmodel_urls = {\n    \'resnet50\': \'http://sceneparsing.csail.mit.edu/model/pretrained_resnet/resnet50-imagenet.pth\',\n    \'resnet101\': \'http://sceneparsing.csail.mit.edu/model/pretrained_resnet/resnet101-imagenet.pth\'\n}\n\n\ndef conv3x3(in_planes, out_planes, stride=1):\n    ""3x3 convolution with padding""\n    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n                     padding=1, bias=False)\n\n\nclass BasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None):\n        super(BasicBlock, self).__init__()\n        self.conv1 = conv3x3(inplanes, planes, stride)\n        self.bn1 = SynchronizedBatchNorm2d(planes)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = conv3x3(planes, planes)\n        self.bn2 = SynchronizedBatchNorm2d(planes)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\n\nclass Bottleneck(nn.Module):\n    expansion = 4\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None):\n        super(Bottleneck, self).__init__()\n        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n        self.bn1 = SynchronizedBatchNorm2d(planes)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,\n                               padding=1, bias=False)\n        self.bn2 = SynchronizedBatchNorm2d(planes)\n        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)\n        self.bn3 = SynchronizedBatchNorm2d(planes * 4)\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\n\nclass ResNet(nn.Module):\n\n    def __init__(self, block, layers, num_classes=1000):\n        self.inplanes = 128\n        super(ResNet, self).__init__()\n        self.conv1 = conv3x3(3, 64, stride=2)\n        self.bn1 = SynchronizedBatchNorm2d(64)\n        self.relu1 = nn.ReLU(inplace=True)\n        self.conv2 = conv3x3(64, 64)\n        self.bn2 = SynchronizedBatchNorm2d(64)\n        self.relu2 = nn.ReLU(inplace=True)\n        self.conv3 = conv3x3(64, 128)\n        self.bn3 = SynchronizedBatchNorm2d(128)\n        self.relu3 = nn.ReLU(inplace=True)\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n\n        self.layer1 = self._make_layer(block, 64, layers[0])\n        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n        self.avgpool = nn.AvgPool2d(7, stride=1)\n        self.fc = nn.Linear(512 * block.expansion, num_classes)\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n                m.weight.data.normal_(0, math.sqrt(2. / n))\n            elif isinstance(m, SynchronizedBatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n\n    def _make_layer(self, block, planes, blocks, stride=1):\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                nn.Conv2d(self.inplanes, planes * block.expansion,\n                          kernel_size=1, stride=stride, bias=False),\n                SynchronizedBatchNorm2d(planes * block.expansion),\n            )\n\n        layers = []\n        layers.append(block(self.inplanes, planes, stride, downsample))\n        self.inplanes = planes * block.expansion\n        for i in range(1, blocks):\n            layers.append(block(self.inplanes, planes))\n\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        x = self.relu1(self.bn1(self.conv1(x)))\n        x = self.relu2(self.bn2(self.conv2(x)))\n        x = self.relu3(self.bn3(self.conv3(x)))\n        x = self.maxpool(x)\n\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n\n        x = self.avgpool(x)\n        x = x.view(x.size(0), -1)\n        x = self.fc(x)\n\n        return x\n\n\'\'\'\ndef resnet18(pretrained=False, **kwargs):\n    """"""Constructs a ResNet-18 model.\n\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on Places\n    """"""\n    model = ResNet(BasicBlock, [2, 2, 2, 2], **kwargs)\n    if pretrained:\n        model.load_state_dict(load_url(model_urls[\'resnet18\']))\n    return model\n\n\ndef resnet34(pretrained=False, **kwargs):\n    """"""Constructs a ResNet-34 model.\n\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on Places\n    """"""\n    model = ResNet(BasicBlock, [3, 4, 6, 3], **kwargs)\n    if pretrained:\n        model.load_state_dict(load_url(model_urls[\'resnet34\']))\n    return model\n\'\'\'\n\ndef resnet50(pretrained=False, **kwargs):\n    """"""Constructs a ResNet-50 model.\n\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on Places\n    """"""\n    model = ResNet(Bottleneck, [3, 4, 6, 3], **kwargs)\n    if pretrained:\n        model.load_state_dict(load_url(model_urls[\'resnet50\']), strict=False)\n    return model\n\n\ndef resnet101(pretrained=False, **kwargs):\n    """"""Constructs a ResNet-101 model.\n\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on Places\n    """"""\n    model = ResNet(Bottleneck, [3, 4, 23, 3], **kwargs)\n    if pretrained:\n        model.load_state_dict(load_url(model_urls[\'resnet101\']), strict=False)\n    return model\n\n# def resnet152(pretrained=False, **kwargs):\n#     """"""Constructs a ResNet-152 model.\n#\n#     Args:\n#         pretrained (bool): If True, returns a model pre-trained on Places\n#     """"""\n#     model = ResNet(Bottleneck, [3, 8, 36, 3], **kwargs)\n#     if pretrained:\n#         model.load_state_dict(load_url(model_urls[\'resnet152\']))\n#     return model\n\ndef load_url(url, model_dir=\'./pretrained\', map_location=None):\n    if not os.path.exists(model_dir):\n        os.makedirs(model_dir)\n    filename = url.split(\'/\')[-1]\n    cached_file = os.path.join(model_dir, filename)\n    if not os.path.exists(cached_file):\n        sys.stderr.write(\'Downloading: ""{}"" to {}\\n\'.format(url, cached_file))\n        urlretrieve(url, cached_file)\n    return torch.load(cached_file, map_location=map_location)\n'"
netdissect/upsegmodel/resnext.py,3,"b'import os\nimport sys\nimport torch\nimport torch.nn as nn\nimport math\ntry:\n    from lib.nn import SynchronizedBatchNorm2d\nexcept ImportError:\n    from torch.nn import BatchNorm2d as SynchronizedBatchNorm2d\n\ntry:\n    from urllib import urlretrieve\nexcept ImportError:\n    from urllib.request import urlretrieve\n\n\n__all__ = [\'ResNeXt\', \'resnext101\'] # support resnext 101\n\n\nmodel_urls = {\n    #\'resnext50\': \'http://sceneparsing.csail.mit.edu/model/pretrained_resnet/resnext50-imagenet.pth\',\n    \'resnext101\': \'http://sceneparsing.csail.mit.edu/model/pretrained_resnet/resnext101-imagenet.pth\'\n}\n\n\ndef conv3x3(in_planes, out_planes, stride=1):\n    ""3x3 convolution with padding""\n    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n                     padding=1, bias=False)\n\n\nclass GroupBottleneck(nn.Module):\n    expansion = 2\n\n    def __init__(self, inplanes, planes, stride=1, groups=1, downsample=None):\n        super(GroupBottleneck, self).__init__()\n        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n        self.bn1 = SynchronizedBatchNorm2d(planes)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,\n                               padding=1, groups=groups, bias=False)\n        self.bn2 = SynchronizedBatchNorm2d(planes)\n        self.conv3 = nn.Conv2d(planes, planes * 2, kernel_size=1, bias=False)\n        self.bn3 = SynchronizedBatchNorm2d(planes * 2)\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\n\nclass ResNeXt(nn.Module):\n\n    def __init__(self, block, layers, groups=32, num_classes=1000):\n        self.inplanes = 128\n        super(ResNeXt, self).__init__()\n        self.conv1 = conv3x3(3, 64, stride=2)\n        self.bn1 = SynchronizedBatchNorm2d(64)\n        self.relu1 = nn.ReLU(inplace=True)\n        self.conv2 = conv3x3(64, 64)\n        self.bn2 = SynchronizedBatchNorm2d(64)\n        self.relu2 = nn.ReLU(inplace=True)\n        self.conv3 = conv3x3(64, 128)\n        self.bn3 = SynchronizedBatchNorm2d(128)\n        self.relu3 = nn.ReLU(inplace=True)\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n\n        self.layer1 = self._make_layer(block, 128, layers[0], groups=groups)\n        self.layer2 = self._make_layer(block, 256, layers[1], stride=2, groups=groups)\n        self.layer3 = self._make_layer(block, 512, layers[2], stride=2, groups=groups)\n        self.layer4 = self._make_layer(block, 1024, layers[3], stride=2, groups=groups)\n        self.avgpool = nn.AvgPool2d(7, stride=1)\n        self.fc = nn.Linear(1024 * block.expansion, num_classes)\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels // m.groups\n                m.weight.data.normal_(0, math.sqrt(2. / n))\n            elif isinstance(m, SynchronizedBatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n\n    def _make_layer(self, block, planes, blocks, stride=1, groups=1):\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                nn.Conv2d(self.inplanes, planes * block.expansion,\n                          kernel_size=1, stride=stride, bias=False),\n                SynchronizedBatchNorm2d(planes * block.expansion),\n            )\n\n        layers = []\n        layers.append(block(self.inplanes, planes, stride, groups, downsample))\n        self.inplanes = planes * block.expansion\n        for i in range(1, blocks):\n            layers.append(block(self.inplanes, planes, groups=groups))\n\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        x = self.relu1(self.bn1(self.conv1(x)))\n        x = self.relu2(self.bn2(self.conv2(x)))\n        x = self.relu3(self.bn3(self.conv3(x)))\n        x = self.maxpool(x)\n\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n\n        x = self.avgpool(x)\n        x = x.view(x.size(0), -1)\n        x = self.fc(x)\n\n        return x\n\n\n\'\'\'\ndef resnext50(pretrained=False, **kwargs):\n    """"""Constructs a ResNet-50 model.\n\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on Places\n    """"""\n    model = ResNeXt(GroupBottleneck, [3, 4, 6, 3], **kwargs)\n    if pretrained:\n        model.load_state_dict(load_url(model_urls[\'resnext50\']), strict=False)\n    return model\n\'\'\'\n\n\ndef resnext101(pretrained=False, **kwargs):\n    """"""Constructs a ResNet-101 model.\n\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on Places\n    """"""\n    model = ResNeXt(GroupBottleneck, [3, 4, 23, 3], **kwargs)\n    if pretrained:\n        model.load_state_dict(load_url(model_urls[\'resnext101\']), strict=False)\n    return model\n\n\n# def resnext152(pretrained=False, **kwargs):\n#     """"""Constructs a ResNeXt-152 model.\n#\n#     Args:\n#         pretrained (bool): If True, returns a model pre-trained on Places\n#     """"""\n#     model = ResNeXt(GroupBottleneck, [3, 8, 36, 3], **kwargs)\n#     if pretrained:\n#         model.load_state_dict(load_url(model_urls[\'resnext152\']))\n#     return model\n\n\ndef load_url(url, model_dir=\'./pretrained\', map_location=None):\n    if not os.path.exists(model_dir):\n        os.makedirs(model_dir)\n    filename = url.split(\'/\')[-1]\n    cached_file = os.path.join(model_dir, filename)\n    if not os.path.exists(cached_file):\n        sys.stderr.write(\'Downloading: ""{}"" to {}\\n\'.format(url, cached_file))\n        urlretrieve(url, cached_file)\n    return torch.load(cached_file, map_location=map_location)\n'"
models/biggan/pytorch_biggan/setup.py,0,"b'""""""\nSimple check list from AllenNLP repo: https://github.com/allenai/allennlp/blob/master/setup.py\n\nTo create the package for pypi.\n\n1. Change the version in __init__.py and setup.py.\n\n2. Commit these changes with the message: ""Release: VERSION""\n\n3. Add a tag in git to mark the release: ""git tag VERSION -m\'Adds tag VERSION for pypi\' ""\n   Push the tag to git: git push --tags origin master\n\n4. Build both the sources and the wheel. Do not change anything in setup.py between\n   creating the wheel and the source distribution (obviously).\n\n   For the wheel, run: ""python setup.py bdist_wheel"" in the top level allennlp directory.\n   (this will build a wheel for the python version you use to build it - make sure you use python 3.x).\n\n   For the sources, run: ""python setup.py sdist""\n   You should now have a /dist directory with both .whl and .tar.gz source versions of allennlp.\n\n5. Check that everything looks correct by uploading the package to the pypi test server:\n\n   twine upload dist/* -r pypitest\n   (pypi suggest using twine as other methods upload files via plaintext.)\n\n   Check that you can install it in a virtualenv by running:\n   pip install -i https://testpypi.python.org/pypi allennlp\n\n6. Upload the final version to actual pypi:\n   twine upload dist/* -r pypi\n\n7. Copy the release notes from RELEASE.md to the tag in github once everything is looking hunky-dory.\n\n""""""\nfrom io import open\nfrom setuptools import find_packages, setup\n\nsetup(\n    name=""pytorch_pretrained_biggan"",\n    version=""0.1.0"",\n    author=""Thomas Wolf"",\n    author_email=""thomas@huggingface.co"",\n    description=""PyTorch version of DeepMind\'s BigGAN model with pre-trained models"",\n    long_description=open(""README.md"", ""r"", encoding=\'utf-8\').read(),\n    long_description_content_type=""text/markdown"",\n    keywords=\'BIGGAN GAN deep learning google deepmind\',\n    license=\'Apache\',\n    url=""https://github.com/huggingface/pytorch-pretrained-BigGAN"",\n    packages=find_packages(exclude=[""*.tests"", ""*.tests.*"",\n                                    ""tests.*"", ""tests""]),\n    install_requires=[\'torch>=0.4.1\',\n                      \'numpy\',\n                      \'boto3\',\n                      \'requests\',\n                      \'tqdm\'],\n    tests_require=[\'pytest\'],\n    entry_points={\n      \'console_scripts\': [\n        ""pytorch_pretrained_biggan=pytorch_pretrained_biggan.convert_tf_to_pytorch:main"",\n      ]\n    },\n    classifiers=[\n          \'Intended Audience :: Science/Research\',\n          \'License :: OSI Approved :: Apache Software License\',\n          \'Programming Language :: Python :: 3\',\n          \'Topic :: Scientific/Engineering :: Artificial Intelligence\',\n    ],\n)\n'"
netdissect/upsegmodel/prroi_pool/__init__.py,0,"b'#! /usr/bin/env python3\n# -*- coding: utf-8 -*-\n# File   : __init__.py\n# Author : Jiayuan Mao, Tete Xiao\n# Email  : maojiayuan@gmail.com, jasonhsiao97@gmail.com\n# Date   : 07/13/2018\n# \n# This file is part of PreciseRoIPooling.\n# Distributed under terms of the MIT license.\n# Copyright (c) 2017 Megvii Technology Limited.\n\nfrom .prroi_pool import *\n\n'"
netdissect/upsegmodel/prroi_pool/build.py,2,"b""#! /usr/bin/env python3\n# -*- coding: utf-8 -*-\n# File   : build.py\n# Author : Jiayuan Mao, Tete Xiao\n# Email  : maojiayuan@gmail.com, jasonhsiao97@gmail.com\n# Date   : 07/13/2018\n# \n# This file is part of PreciseRoIPooling.\n# Distributed under terms of the MIT license.\n# Copyright (c) 2017 Megvii Technology Limited.\n\nimport os\nimport torch\n\nfrom torch.utils.ffi import create_extension\n\nheaders = []\nsources = []\ndefines = []\nextra_objects = []\nwith_cuda = False\n\nif torch.cuda.is_available():\n    with_cuda = True\n\n    headers+= ['src/prroi_pooling_gpu.h']\n    sources += ['src/prroi_pooling_gpu.c']\n    defines += [('WITH_CUDA', None)]\n\n    this_file = os.path.dirname(os.path.realpath(__file__))\n    extra_objects_cuda = ['src/prroi_pooling_gpu_impl.cu.o']\n    extra_objects_cuda = [os.path.join(this_file, fname) for fname in extra_objects_cuda]\n    extra_objects.extend(extra_objects_cuda)\nelse:\n    # TODO(Jiayuan Mao @ 07/13): remove this restriction after we support the cpu implementation.\n    raise NotImplementedError('Precise RoI Pooling only supports GPU (cuda) implememtations.')\n\nffi = create_extension(\n    '_prroi_pooling',\n    headers=headers,\n    sources=sources,\n    define_macros=defines,\n    relative_to=__file__,\n    with_cuda=with_cuda,\n    extra_objects=extra_objects\n)\n\nif __name__ == '__main__':\n    ffi.build()\n\n"""
netdissect/upsegmodel/prroi_pool/functional.py,2,"b""#! /usr/bin/env python3\n# -*- coding: utf-8 -*-\n# File   : functional.py\n# Author : Jiayuan Mao, Tete Xiao\n# Email  : maojiayuan@gmail.com, jasonhsiao97@gmail.com\n# Date   : 07/13/2018\n#\n# This file is part of PreciseRoIPooling.\n# Distributed under terms of the MIT license.\n# Copyright (c) 2017 Megvii Technology Limited.\n\nimport torch\nimport torch.autograd as ag\n\ntry:\n    from os.path import join as pjoin, dirname\n    from torch.utils.cpp_extension import load as load_extension\n    root_dir = pjoin(dirname(__file__), 'src')\n    _prroi_pooling = load_extension(\n        '_prroi_pooling',\n        [pjoin(root_dir, 'prroi_pooling_gpu.c'), pjoin(root_dir, 'prroi_pooling_gpu_impl.cu')],\n        verbose=False\n    )\nexcept ImportError:\n    raise ImportError('Can not compile Precise RoI Pooling library.')\n\n__all__ = ['prroi_pool2d']\n\n\nclass PrRoIPool2DFunction(ag.Function):\n    @staticmethod\n    def forward(ctx, features, rois, pooled_height, pooled_width, spatial_scale):\n        assert 'FloatTensor' in features.type() and 'FloatTensor' in rois.type(), \\\n                'Precise RoI Pooling only takes float input, got {} for features and {} for rois.'.format(features.type(), rois.type())\n\n        pooled_height = int(pooled_height)\n        pooled_width = int(pooled_width)\n        spatial_scale = float(spatial_scale)\n\n        features = features.contiguous()\n        rois = rois.contiguous()\n        params = (pooled_height, pooled_width, spatial_scale)\n\n        if features.is_cuda:\n            output = _prroi_pooling.prroi_pooling_forward_cuda(features, rois, *params)\n            ctx.params = params\n            # everything here is contiguous.\n            ctx.save_for_backward(features, rois, output)\n        else:\n            raise NotImplementedError('Precise RoI Pooling only supports GPU (cuda) implememtations.')\n\n        return output\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        features, rois, output = ctx.saved_tensors\n        grad_input = grad_coor = None\n\n        if features.requires_grad:\n            grad_output = grad_output.contiguous()\n            grad_input = _prroi_pooling.prroi_pooling_backward_cuda(features, rois, output, grad_output, *ctx.params)\n        if rois.requires_grad:\n            grad_output = grad_output.contiguous()\n            grad_coor = _prroi_pooling.prroi_pooling_coor_backward_cuda(features, rois, output, grad_output, *ctx.params)\n\n        return grad_input, grad_coor, None, None, None\n\n\nprroi_pool2d = PrRoIPool2DFunction.apply\n\n"""
netdissect/upsegmodel/prroi_pool/prroi_pool.py,1,"b""#! /usr/bin/env python3\n# -*- coding: utf-8 -*-\n# File   : prroi_pool.py\n# Author : Jiayuan Mao, Tete Xiao\n# Email  : maojiayuan@gmail.com, jasonhsiao97@gmail.com\n# Date   : 07/13/2018\n# \n# This file is part of PreciseRoIPooling.\n# Distributed under terms of the MIT license.\n# Copyright (c) 2017 Megvii Technology Limited.\n\nimport torch.nn as nn\n\nfrom .functional import prroi_pool2d\n\n__all__ = ['PrRoIPool2D']\n\n\nclass PrRoIPool2D(nn.Module):\n    def __init__(self, pooled_height, pooled_width, spatial_scale):\n        super().__init__()\n\n        self.pooled_height = int(pooled_height)\n        self.pooled_width = int(pooled_width)\n        self.spatial_scale = float(spatial_scale)\n\n    def forward(self, features, rois):\n        return prroi_pool2d(features, rois, self.pooled_height, self.pooled_width, self.spatial_scale)\n"""
netdissect/upsegmodel/prroi_pool/test_prroi_pooling2d.py,8,"b""# -*- coding: utf-8 -*-\n# File   : test_prroi_pooling2d.py\n# Author : Jiayuan Mao\n# Email  : maojiayuan@gmail.com\n# Date   : 18/02/2018\n#\n# This file is part of Jacinle.\n\nimport unittest\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom jactorch.utils.unittest import TorchTestCase\n\nfrom prroi_pool import PrRoIPool2D\n\n\nclass TestPrRoIPool2D(TorchTestCase):\n    def test_forward(self):\n        pool = PrRoIPool2D(7, 7, spatial_scale=0.5)\n        features = torch.rand((4, 16, 24, 32)).cuda()\n        rois = torch.tensor([\n            [0, 0, 0, 14, 14],\n            [1, 14, 14, 28, 28],\n        ]).float().cuda()\n\n        out = pool(features, rois)\n        out_gold = F.avg_pool2d(features, kernel_size=2, stride=1)\n\n        self.assertTensorClose(out, torch.stack((\n            out_gold[0, :, :7, :7],\n            out_gold[1, :, 7:14, 7:14],\n        ), dim=0))\n\n    def test_backward_shapeonly(self):\n        pool = PrRoIPool2D(2, 2, spatial_scale=0.5)\n\n        features = torch.rand((4, 2, 24, 32)).cuda()\n        rois = torch.tensor([\n            [0, 0, 0, 4, 4],\n            [1, 14, 14, 18, 18],\n        ]).float().cuda()\n        features.requires_grad = rois.requires_grad = True\n        out = pool(features, rois)\n\n        loss = out.sum()\n        loss.backward()\n\n        self.assertTupleEqual(features.size(), features.grad.size())\n        self.assertTupleEqual(rois.size(), rois.grad.size())\n\n\nif __name__ == '__main__':\n    unittest.main()\n"""
models/biggan/pytorch_biggan/pytorch_pretrained_biggan/__init__.py,0,"b'from .config import BigGANConfig\nfrom .model import BigGAN\nfrom .file_utils import PYTORCH_PRETRAINED_BIGGAN_CACHE, cached_path\nfrom .utils import (truncated_noise_sample, save_as_images,\n                    convert_to_images, display_in_terminal,\n                    one_hot_from_int, one_hot_from_names)\n'"
models/biggan/pytorch_biggan/pytorch_pretrained_biggan/config.py,0,"b'# coding: utf-8\n""""""\nBigGAN config.\n""""""\nfrom __future__ import (absolute_import, division, print_function, unicode_literals)\n\nimport copy\nimport json\n\nclass BigGANConfig(object):\n    """""" Configuration class to store the configuration of a `BigGAN`. \n        Defaults are for the 128x128 model.\n        layers tuple are (up-sample in the layer ?, input channels, output channels)\n    """"""\n    def __init__(self,\n                 output_dim=128,\n                 z_dim=128,\n                 class_embed_dim=128,\n                 channel_width=128,\n                 num_classes=1000,\n                 layers=[(False, 16, 16),\n                         (True, 16, 16),\n                         (False, 16, 16),\n                         (True, 16, 8),\n                         (False, 8, 8),\n                         (True, 8, 4),\n                         (False, 4, 4),\n                         (True, 4, 2),\n                         (False, 2, 2),\n                         (True, 2, 1)],\n                 attention_layer_position=8,\n                 eps=1e-4,\n                 n_stats=51):\n        """"""Constructs BigGANConfig. """"""\n        self.output_dim = output_dim\n        self.z_dim = z_dim\n        self.class_embed_dim = class_embed_dim\n        self.channel_width = channel_width\n        self.num_classes = num_classes\n        self.layers = layers\n        self.attention_layer_position = attention_layer_position\n        self.eps = eps\n        self.n_stats = n_stats\n\n    @classmethod\n    def from_dict(cls, json_object):\n        """"""Constructs a `BigGANConfig` from a Python dictionary of parameters.""""""\n        config = BigGANConfig()\n        for key, value in json_object.items():\n            config.__dict__[key] = value\n        return config\n\n    @classmethod\n    def from_json_file(cls, json_file):\n        """"""Constructs a `BigGANConfig` from a json file of parameters.""""""\n        with open(json_file, ""r"", encoding=\'utf-8\') as reader:\n            text = reader.read()\n        return cls.from_dict(json.loads(text))\n\n    def __repr__(self):\n        return str(self.to_json_string())\n\n    def to_dict(self):\n        """"""Serializes this instance to a Python dictionary.""""""\n        output = copy.deepcopy(self.__dict__)\n        return output\n\n    def to_json_string(self):\n        """"""Serializes this instance to a JSON string.""""""\n        return json.dumps(self.to_dict(), indent=2, sort_keys=True) + ""\\n""\n'"
models/biggan/pytorch_biggan/pytorch_pretrained_biggan/convert_tf_to_pytorch.py,14,"b'# coding: utf-8\n""""""\nConvert a TF Hub model for BigGAN in a PT one.\n""""""\nfrom __future__ import (absolute_import, division, print_function, unicode_literals)\n\nfrom itertools import chain\n\nimport os\nimport argparse\nimport logging\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.nn.functional import normalize\n\nfrom .model import BigGAN, WEIGHTS_NAME, CONFIG_NAME\nfrom .config import BigGANConfig\n\nlogger = logging.getLogger(__name__)\n\n\ndef extract_batch_norm_stats(tf_model_path, batch_norm_stats_path=None):\n    try:\n        import numpy as np\n        import tensorflow as tf\n        import tensorflow_hub as hub\n    except ImportError:\n        raise ImportError(""Loading a TensorFlow models in PyTorch, requires TensorFlow and TF Hub to be installed. ""\n                          ""Please see https://www.tensorflow.org/install/ for installation instructions for TensorFlow. ""\n                          ""And see https://github.com/tensorflow/hub for installing Hub. ""\n                          ""Probably pip install tensorflow tensorflow-hub"")\n    tf.reset_default_graph()\n    logger.info(\'Loading BigGAN module from: {}\'.format(tf_model_path))\n    module = hub.Module(tf_model_path)\n    inputs = {k: tf.placeholder(v.dtype, v.get_shape().as_list(), k)\n              for k, v in module.get_input_info_dict().items()}\n    output = module(inputs)\n\n    initializer = tf.global_variables_initializer()\n    sess = tf.Session()\n    stacks = sum(((i*10 + 1, i*10 + 3, i*10 + 6, i*10 + 8) for i in range(50)), ())\n    numpy_stacks = []\n    for i in stacks:\n        logger.info(""Retrieving module_apply_default/stack_{}"".format(i))\n        try:\n            stack_var = tf.get_default_graph().get_tensor_by_name(""module_apply_default/stack_%d:0"" % i)\n        except KeyError:\n            break  # We have all the stats\n        numpy_stacks.append(sess.run(stack_var))\n\n    if batch_norm_stats_path is not None:\n        torch.save(numpy_stacks, batch_norm_stats_path)\n    else:\n        return numpy_stacks\n\n\ndef build_tf_to_pytorch_map(model, config):\n    """""" Build a map from TF variables to PyTorch modules. """"""\n    tf_to_pt_map = {}\n\n    # Embeddings and GenZ\n    tf_to_pt_map.update({\'linear/w/ema_0.9999\': model.embeddings.weight,\n                         \'Generator/GenZ/G_linear/b/ema_0.9999\': model.generator.gen_z.bias,\n                         \'Generator/GenZ/G_linear/w/ema_0.9999\': model.generator.gen_z.weight_orig,\n                         \'Generator/GenZ/G_linear/u0\': model.generator.gen_z.weight_u})\n\n    # GBlock blocks\n    model_layer_idx = 0\n    for i, (up, in_channels, out_channels) in enumerate(config.layers):\n        if i == config.attention_layer_position:\n            model_layer_idx += 1\n        layer_str = ""Generator/GBlock_%d/"" % i if i > 0 else ""Generator/GBlock/""\n        layer_pnt = model.generator.layers[model_layer_idx]\n        for i in range(4):  #  Batchnorms\n            batch_str = layer_str + (""BatchNorm_%d/"" % i if i > 0 else ""BatchNorm/"")\n            batch_pnt = getattr(layer_pnt, \'bn_%d\' % i)\n            for name in (\'offset\', \'scale\'):\n                sub_module_str = batch_str + name + ""/""\n                sub_module_pnt = getattr(batch_pnt, name)\n                tf_to_pt_map.update({sub_module_str + ""w/ema_0.9999"": sub_module_pnt.weight_orig,\n                                     sub_module_str + ""u0"": sub_module_pnt.weight_u})\n        for i in range(4):  # Convolutions\n            conv_str = layer_str + ""conv%d/"" % i\n            conv_pnt = getattr(layer_pnt, \'conv_%d\' % i)\n            tf_to_pt_map.update({conv_str + ""b/ema_0.9999"": conv_pnt.bias,\n                                 conv_str + ""w/ema_0.9999"": conv_pnt.weight_orig,\n                                 conv_str + ""u0"": conv_pnt.weight_u})\n        model_layer_idx += 1\n\n    # Attention block\n    layer_str = ""Generator/attention/""\n    layer_pnt = model.generator.layers[config.attention_layer_position]\n    tf_to_pt_map.update({layer_str + ""gamma/ema_0.9999"": layer_pnt.gamma})\n    for pt_name, tf_name in zip([\'snconv1x1_g\', \'snconv1x1_o_conv\', \'snconv1x1_phi\', \'snconv1x1_theta\'],\n                                [\'g/\', \'o_conv/\', \'phi/\', \'theta/\']):\n        sub_module_str = layer_str + tf_name\n        sub_module_pnt = getattr(layer_pnt, pt_name)\n        tf_to_pt_map.update({sub_module_str + ""w/ema_0.9999"": sub_module_pnt.weight_orig,\n                             sub_module_str + ""u0"": sub_module_pnt.weight_u})\n\n    # final batch norm and conv to rgb\n    layer_str = ""Generator/BatchNorm/""\n    layer_pnt = model.generator.bn\n    tf_to_pt_map.update({layer_str + ""offset/ema_0.9999"": layer_pnt.bias,\n                         layer_str + ""scale/ema_0.9999"": layer_pnt.weight})\n    layer_str = ""Generator/conv_to_rgb/""\n    layer_pnt = model.generator.conv_to_rgb\n    tf_to_pt_map.update({layer_str + ""b/ema_0.9999"": layer_pnt.bias,\n                         layer_str + ""w/ema_0.9999"": layer_pnt.weight_orig,\n                         layer_str + ""u0"": layer_pnt.weight_u})\n    return tf_to_pt_map\n\n\ndef load_tf_weights_in_biggan(model, config, tf_model_path, batch_norm_stats_path=None):\n    """""" Load tf checkpoints and standing statistics in a pytorch model\n    """"""\n    try:\n        import numpy as np\n        import tensorflow as tf\n    except ImportError:\n        raise ImportError(""Loading a TensorFlow models in PyTorch, requires TensorFlow to be installed. Please see ""\n            ""https://www.tensorflow.org/install/ for installation instructions."")\n    # Load weights from TF model\n    checkpoint_path = tf_model_path + ""/variables/variables""\n    init_vars = tf.train.list_variables(checkpoint_path)\n    from pprint import pprint\n    pprint(init_vars)\n\n    # Extract batch norm statistics from model if needed\n    if batch_norm_stats_path:\n        stats = torch.load(batch_norm_stats_path)\n    else:\n        logger.info(""Extracting batch norm stats"")\n        stats = extract_batch_norm_stats(tf_model_path)\n\n    # Build TF to PyTorch weights loading map\n    tf_to_pt_map = build_tf_to_pytorch_map(model, config)\n\n    tf_weights = {}\n    for name in tf_to_pt_map.keys():\n        array = tf.train.load_variable(checkpoint_path, name)\n        tf_weights[name] = array\n        # logger.info(""Loading TF weight {} with shape {}"".format(name, array.shape))\n\n    # Load parameters\n    with torch.no_grad():\n        pt_params_pnt = set()\n        for name, pointer in tf_to_pt_map.items():\n            array = tf_weights[name]\n            if pointer.dim() == 1:\n                if pointer.dim() < array.ndim:\n                    array = np.squeeze(array)\n            elif pointer.dim() == 2:  # Weights\n                array = np.transpose(array)\n            elif pointer.dim() == 4:  # Convolutions\n                array = np.transpose(array, (3, 2, 0, 1))\n            else:\n                raise ""Wrong dimensions to adjust: "" + str((pointer.shape, array.shape))\n            if pointer.shape != array.shape:\n                raise ValueError(""Wrong dimensions: "" + str((pointer.shape, array.shape)))\n            logger.info(""Initialize PyTorch weight {} with shape {}"".format(name, pointer.shape))\n            pointer.data = torch.from_numpy(array) if isinstance(array, np.ndarray) else torch.tensor(array)\n            tf_weights.pop(name, None)\n            pt_params_pnt.add(pointer.data_ptr())\n\n        # Prepare SpectralNorm buffers by running one step of Spectral Norm (no need to train the model):\n        for module in model.modules():\n            for n, buffer in module.named_buffers():\n                if n == \'weight_v\':\n                    weight_mat = module.weight_orig\n                    weight_mat = weight_mat.reshape(weight_mat.size(0), -1)\n                    u = module.weight_u\n\n                    v = normalize(torch.mv(weight_mat.t(), u), dim=0, eps=config.eps)\n                    buffer.data = v\n                    pt_params_pnt.add(buffer.data_ptr())\n\n                    u = normalize(torch.mv(weight_mat, v), dim=0, eps=config.eps)\n                    module.weight_u.data = u\n                    pt_params_pnt.add(module.weight_u.data_ptr())\n\n        # Load batch norm statistics\n        index = 0\n        for layer in model.generator.layers:\n            if not hasattr(layer, \'bn_0\'):\n                continue\n            for i in range(4):  #  Batchnorms\n                bn_pointer = getattr(layer, \'bn_%d\' % i)\n                pointer = bn_pointer.running_means\n                if pointer.shape != stats[index].shape:\n                    raise ""Wrong dimensions: "" + str((pointer.shape, stats[index].shape))\n                pointer.data = torch.from_numpy(stats[index])\n                pt_params_pnt.add(pointer.data_ptr())\n\n                pointer = bn_pointer.running_vars\n                if pointer.shape != stats[index+1].shape:\n                    raise ""Wrong dimensions: "" + str((pointer.shape, stats[index].shape))\n                pointer.data = torch.from_numpy(stats[index+1])\n                pt_params_pnt.add(pointer.data_ptr())\n\n                index += 2\n\n        bn_pointer = model.generator.bn\n        pointer = bn_pointer.running_means\n        if pointer.shape != stats[index].shape:\n            raise ""Wrong dimensions: "" + str((pointer.shape, stats[index].shape))\n        pointer.data = torch.from_numpy(stats[index])\n        pt_params_pnt.add(pointer.data_ptr())\n\n        pointer = bn_pointer.running_vars\n        if pointer.shape != stats[index+1].shape:\n            raise ""Wrong dimensions: "" + str((pointer.shape, stats[index].shape))\n        pointer.data = torch.from_numpy(stats[index+1])\n        pt_params_pnt.add(pointer.data_ptr())\n\n    remaining_params = list(n for n, t in chain(model.named_parameters(), model.named_buffers()) \\\n                            if t.data_ptr() not in pt_params_pnt)\n\n    logger.info(""TF Weights not copied to PyTorch model: {} -"".format(\', \'.join(tf_weights.keys())))\n    logger.info(""Remanining parameters/buffers from PyTorch model: {} -"".format(\', \'.join(remaining_params)))\n\n    return model\n\n\nBigGAN128 = BigGANConfig(output_dim=128, z_dim=128, class_embed_dim=128, channel_width=128, num_classes=1000,\n                         layers=[(False, 16, 16),\n                                 (True, 16, 16),\n                                 (False, 16, 16),\n                                 (True, 16, 8),\n                                 (False, 8, 8),\n                                 (True, 8, 4),\n                                 (False, 4, 4),\n                                 (True, 4, 2),\n                                 (False, 2, 2),\n                                 (True, 2, 1)],\n                         attention_layer_position=8, eps=1e-4, n_stats=51)\n\nBigGAN256 = BigGANConfig(output_dim=256, z_dim=128, class_embed_dim=128, channel_width=128, num_classes=1000,\n                         layers=[(False, 16, 16),\n                                 (True, 16, 16),\n                                 (False, 16, 16),\n                                 (True, 16, 8),\n                                 (False, 8, 8),\n                                 (True, 8, 8),\n                                 (False, 8, 8),\n                                 (True, 8, 4),\n                                 (False, 4, 4),\n                                 (True, 4, 2),\n                                 (False, 2, 2),\n                                 (True, 2, 1)],\n                         attention_layer_position=8, eps=1e-4, n_stats=51)\n\nBigGAN512 = BigGANConfig(output_dim=512, z_dim=128, class_embed_dim=128, channel_width=128, num_classes=1000,\n                         layers=[(False, 16, 16),\n                                 (True, 16, 16),\n                                 (False, 16, 16),\n                                 (True, 16, 8),\n                                 (False, 8, 8),\n                                 (True, 8, 8),\n                                 (False, 8, 8),\n                                 (True, 8, 4),\n                                 (False, 4, 4),\n                                 (True, 4, 2),\n                                 (False, 2, 2),\n                                 (True, 2, 1),\n                                 (False, 1, 1),\n                                 (True, 1, 1)],\n                         attention_layer_position=8, eps=1e-4, n_stats=51)\n\n\ndef main():\n    parser = argparse.ArgumentParser(description=""Convert a BigGAN TF Hub model in a PyTorch model"")\n    parser.add_argument(""--model_type"", type=str, default="""", required=True,\n                        help=""BigGAN model type (128, 256, 512)"")\n    parser.add_argument(""--tf_model_path"", type=str, default="""", required=True,\n                        help=""Path of the downloaded TF Hub model"")\n    parser.add_argument(""--pt_save_path"", type=str, default="""",\n                        help=""Folder to save the PyTorch model (default: Folder of the TF Hub model)"")\n    parser.add_argument(""--batch_norm_stats_path"", type=str, default="""",\n                        help=""Path of previously extracted batch norm statistics"")\n    args = parser.parse_args()\n\n    logging.basicConfig(level=logging.INFO)\n\n    if not args.pt_save_path:\n        args.pt_save_path = args.tf_model_path\n\n    if args.model_type == ""128"":\n        config = BigGAN128\n    elif args.model_type == ""256"":\n        config = BigGAN256\n    elif args.model_type == ""512"":\n        config = BigGAN512\n    else:\n        raise ValueError(""model_type should be one of 128, 256 or 512"")\n\n    model = BigGAN(config)\n    model = load_tf_weights_in_biggan(model, config, args.tf_model_path, args.batch_norm_stats_path)\n\n    model_save_path = os.path.join(args.pt_save_path, WEIGHTS_NAME)\n    config_save_path = os.path.join(args.pt_save_path, CONFIG_NAME)\n\n    logger.info(""Save model dump to {}"".format(model_save_path))\n    torch.save(model.state_dict(), model_save_path)\n    logger.info(""Save configuration file to {}"".format(config_save_path))\n    with open(config_save_path, ""w"", encoding=""utf-8"") as f:\n        f.write(config.to_json_string())\n\nif __name__ == ""__main__"":\n    main()\n'"
models/biggan/pytorch_biggan/pytorch_pretrained_biggan/file_utils.py,0,"b'""""""\nUtilities for working with the local dataset cache.\nThis file is adapted from the AllenNLP library at https://github.com/allenai/allennlp\nCopyright by the AllenNLP authors.\n""""""\nfrom __future__ import (absolute_import, division, print_function, unicode_literals)\n\nimport json\nimport logging\nimport os\nimport shutil\nimport tempfile\nfrom functools import wraps\nfrom hashlib import sha256\nimport sys\nfrom io import open\n\nimport boto3\nimport requests\nfrom botocore.exceptions import ClientError\nfrom tqdm import tqdm\n\ntry:\n    from urllib.parse import urlparse\nexcept ImportError:\n    from urlparse import urlparse\n\ntry:\n    from pathlib import Path\n    PYTORCH_PRETRAINED_BIGGAN_CACHE = Path(os.getenv(\'PYTORCH_PRETRAINED_BIGGAN_CACHE\',\n                                                   Path.home() / \'.pytorch_pretrained_biggan\'))\nexcept (AttributeError, ImportError):\n    PYTORCH_PRETRAINED_BIGGAN_CACHE = os.getenv(\'PYTORCH_PRETRAINED_BIGGAN_CACHE\',\n                                              os.path.join(os.path.expanduser(""~""), \'.pytorch_pretrained_biggan\'))\n\nlogger = logging.getLogger(__name__)  # pylint: disable=invalid-name\n\n\ndef url_to_filename(url, etag=None):\n    """"""\n    Convert `url` into a hashed filename in a repeatable way.\n    If `etag` is specified, append its hash to the url\'s, delimited\n    by a period.\n    """"""\n    url_bytes = url.encode(\'utf-8\')\n    url_hash = sha256(url_bytes)\n    filename = url_hash.hexdigest()\n\n    if etag:\n        etag_bytes = etag.encode(\'utf-8\')\n        etag_hash = sha256(etag_bytes)\n        filename += \'.\' + etag_hash.hexdigest()\n\n    return filename\n\n\ndef filename_to_url(filename, cache_dir=None):\n    """"""\n    Return the url and etag (which may be ``None``) stored for `filename`.\n    Raise ``EnvironmentError`` if `filename` or its stored metadata do not exist.\n    """"""\n    if cache_dir is None:\n        cache_dir = PYTORCH_PRETRAINED_BIGGAN_CACHE\n    if sys.version_info[0] == 3 and isinstance(cache_dir, Path):\n        cache_dir = str(cache_dir)\n\n    cache_path = os.path.join(cache_dir, filename)\n    if not os.path.exists(cache_path):\n        raise EnvironmentError(""file {} not found"".format(cache_path))\n\n    meta_path = cache_path + \'.json\'\n    if not os.path.exists(meta_path):\n        raise EnvironmentError(""file {} not found"".format(meta_path))\n\n    with open(meta_path, encoding=""utf-8"") as meta_file:\n        metadata = json.load(meta_file)\n    url = metadata[\'url\']\n    etag = metadata[\'etag\']\n\n    return url, etag\n\n\ndef cached_path(url_or_filename, cache_dir=None):\n    """"""\n    Given something that might be a URL (or might be a local path),\n    determine which. If it\'s a URL, download the file and cache it, and\n    return the path to the cached file. If it\'s already a local path,\n    make sure the file exists and then return the path.\n    """"""\n    if cache_dir is None:\n        cache_dir = PYTORCH_PRETRAINED_BIGGAN_CACHE\n    if sys.version_info[0] == 3 and isinstance(url_or_filename, Path):\n        url_or_filename = str(url_or_filename)\n    if sys.version_info[0] == 3 and isinstance(cache_dir, Path):\n        cache_dir = str(cache_dir)\n\n    parsed = urlparse(url_or_filename)\n\n    if parsed.scheme in (\'http\', \'https\', \'s3\'):\n        # URL, so get it from the cache (downloading if necessary)\n        return get_from_cache(url_or_filename, cache_dir)\n    elif os.path.exists(url_or_filename):\n        # File, and it exists.\n        return url_or_filename\n    elif parsed.scheme == \'\':\n        # File, but it doesn\'t exist.\n        raise EnvironmentError(""file {} not found"".format(url_or_filename))\n    else:\n        # Something unknown\n        raise ValueError(""unable to parse {} as a URL or as a local path"".format(url_or_filename))\n\n\ndef split_s3_path(url):\n    """"""Split a full s3 path into the bucket name and path.""""""\n    parsed = urlparse(url)\n    if not parsed.netloc or not parsed.path:\n        raise ValueError(""bad s3 path {}"".format(url))\n    bucket_name = parsed.netloc\n    s3_path = parsed.path\n    # Remove \'/\' at beginning of path.\n    if s3_path.startswith(""/""):\n        s3_path = s3_path[1:]\n    return bucket_name, s3_path\n\n\ndef s3_request(func):\n    """"""\n    Wrapper function for s3 requests in order to create more helpful error\n    messages.\n    """"""\n\n    @wraps(func)\n    def wrapper(url, *args, **kwargs):\n        try:\n            return func(url, *args, **kwargs)\n        except ClientError as exc:\n            if int(exc.response[""Error""][""Code""]) == 404:\n                raise EnvironmentError(""file {} not found"".format(url))\n            else:\n                raise\n\n    return wrapper\n\n\n@s3_request\ndef s3_etag(url):\n    """"""Check ETag on S3 object.""""""\n    s3_resource = boto3.resource(""s3"")\n    bucket_name, s3_path = split_s3_path(url)\n    s3_object = s3_resource.Object(bucket_name, s3_path)\n    return s3_object.e_tag\n\n\n@s3_request\ndef s3_get(url, temp_file):\n    """"""Pull a file directly from S3.""""""\n    s3_resource = boto3.resource(""s3"")\n    bucket_name, s3_path = split_s3_path(url)\n    s3_resource.Bucket(bucket_name).download_fileobj(s3_path, temp_file)\n\n\ndef http_get(url, temp_file):\n    req = requests.get(url, stream=True)\n    content_length = req.headers.get(\'Content-Length\')\n    total = int(content_length) if content_length is not None else None\n    progress = tqdm(unit=""B"", total=total)\n    for chunk in req.iter_content(chunk_size=1024):\n        if chunk: # filter out keep-alive new chunks\n            progress.update(len(chunk))\n            temp_file.write(chunk)\n    progress.close()\n\n\ndef get_from_cache(url, cache_dir=None):\n    """"""\n    Given a URL, look for the corresponding dataset in the local cache.\n    If it\'s not there, download it. Then return the path to the cached file.\n    """"""\n    if cache_dir is None:\n        cache_dir = PYTORCH_PRETRAINED_BIGGAN_CACHE\n    if sys.version_info[0] == 3 and isinstance(cache_dir, Path):\n        cache_dir = str(cache_dir)\n\n    if not os.path.exists(cache_dir):\n        os.makedirs(cache_dir)\n\n    # Get eTag to add to filename, if it exists.\n    if url.startswith(""s3://""):\n        etag = s3_etag(url)\n    else:\n        response = requests.head(url, allow_redirects=True)\n        if response.status_code != 200:\n            raise IOError(""HEAD request failed for url {} with status code {}""\n                          .format(url, response.status_code))\n        etag = response.headers.get(""ETag"")\n\n    filename = url_to_filename(url, etag)\n\n    # get cache path to put the file\n    cache_path = os.path.join(cache_dir, filename)\n\n    if not os.path.exists(cache_path):\n        # Download to temporary file, then copy to cache dir once finished.\n        # Otherwise you get corrupt cache entries if the download gets interrupted.\n        with tempfile.NamedTemporaryFile() as temp_file:\n            logger.info(""%s not found in cache, downloading to %s"", url, temp_file.name)\n\n            # GET file object\n            if url.startswith(""s3://""):\n                s3_get(url, temp_file)\n            else:\n                http_get(url, temp_file)\n\n            # we are copying the file before closing it, so flush to avoid truncation\n            temp_file.flush()\n            # shutil.copyfileobj() starts at the current position, so go to the start\n            temp_file.seek(0)\n\n            logger.info(""copying %s to cache at %s"", temp_file.name, cache_path)\n            with open(cache_path, \'wb\') as cache_file:\n                shutil.copyfileobj(temp_file, cache_file)\n\n            logger.info(""creating metadata file for %s"", cache_path)\n            meta = {\'url\': url, \'etag\': etag}\n            meta_path = cache_path + \'.json\'\n            with open(meta_path, \'w\', encoding=""utf-8"") as meta_file:\n                json.dump(meta, meta_file)\n\n            logger.info(""removing temp file %s"", temp_file.name)\n\n    return cache_path\n\n\ndef read_set_from_file(filename):\n    \'\'\'\n    Extract a de-duped collection (set) of text from a file.\n    Expected file format is one item per line.\n    \'\'\'\n    collection = set()\n    with open(filename, \'r\', encoding=\'utf-8\') as file_:\n        for line in file_:\n            collection.add(line.rstrip())\n    return collection\n\n\ndef get_file_extension(path, dot=True, lower=True):\n    ext = os.path.splitext(path)[1]\n    ext = ext if dot else ext[1:]\n    return ext.lower() if lower else ext\n'"
models/biggan/pytorch_biggan/pytorch_pretrained_biggan/model.py,18,"b'# coding: utf-8\n"""""" BigGAN PyTorch model.\n    From ""Large Scale GAN Training for High Fidelity Natural Image Synthesis""\n    By Andrew Brock\x03y, Jeff Donahuey and Karen Simonyan.\n    https://openreview.net/forum?id=B1xsqj09Fm\n\n    PyTorch version implemented from the computational graph of the TF Hub module for BigGAN.\n    Some part of the code are adapted from https://github.com/brain-research/self-attention-gan\n\n    This version only comprises the generator (since the discriminator\'s weights are not released).\n    This version only comprises the ""deep"" version of BigGAN (see publication).\n\n    Modified by Erik H\xc3\xa4rk\xc3\xb6nen:\n    * Added support for per-layer latent vectors\n""""""\nfrom __future__ import (absolute_import, division, print_function, unicode_literals)\n\nimport os\nimport logging\nimport math\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom .config import BigGANConfig\nfrom .file_utils import cached_path\n\nlogger = logging.getLogger(__name__)\n\nPRETRAINED_MODEL_ARCHIVE_MAP = {\n    \'biggan-deep-128\': ""https://s3.amazonaws.com/models.huggingface.co/biggan/biggan-deep-128-pytorch_model.bin"",\n    \'biggan-deep-256\': ""https://s3.amazonaws.com/models.huggingface.co/biggan/biggan-deep-256-pytorch_model.bin"",\n    \'biggan-deep-512\': ""https://s3.amazonaws.com/models.huggingface.co/biggan/biggan-deep-512-pytorch_model.bin"",\n}\n\nPRETRAINED_CONFIG_ARCHIVE_MAP = {\n    \'biggan-deep-128\': ""https://s3.amazonaws.com/models.huggingface.co/biggan/biggan-deep-128-config.json"",\n    \'biggan-deep-256\': ""https://s3.amazonaws.com/models.huggingface.co/biggan/biggan-deep-256-config.json"",\n    \'biggan-deep-512\': ""https://s3.amazonaws.com/models.huggingface.co/biggan/biggan-deep-512-config.json"",\n}\n\nWEIGHTS_NAME = \'pytorch_model.bin\'\nCONFIG_NAME = \'config.json\'\n\n\ndef snconv2d(eps=1e-12, **kwargs):\n    return nn.utils.spectral_norm(nn.Conv2d(**kwargs), eps=eps)\n\ndef snlinear(eps=1e-12, **kwargs):\n    return nn.utils.spectral_norm(nn.Linear(**kwargs), eps=eps)\n\ndef sn_embedding(eps=1e-12, **kwargs):\n    return nn.utils.spectral_norm(nn.Embedding(**kwargs), eps=eps)\n\nclass SelfAttn(nn.Module):\n    """""" Self attention Layer""""""\n    def __init__(self, in_channels, eps=1e-12):\n        super(SelfAttn, self).__init__()\n        self.in_channels = in_channels\n        self.snconv1x1_theta = snconv2d(in_channels=in_channels, out_channels=in_channels//8,\n                                        kernel_size=1, bias=False, eps=eps)\n        self.snconv1x1_phi = snconv2d(in_channels=in_channels, out_channels=in_channels//8,\n                                      kernel_size=1, bias=False, eps=eps)\n        self.snconv1x1_g = snconv2d(in_channels=in_channels, out_channels=in_channels//2,\n                                    kernel_size=1, bias=False, eps=eps)\n        self.snconv1x1_o_conv = snconv2d(in_channels=in_channels//2, out_channels=in_channels,\n                                         kernel_size=1, bias=False, eps=eps)\n        self.maxpool = nn.MaxPool2d(2, stride=2, padding=0)\n        self.softmax  = nn.Softmax(dim=-1)\n        self.gamma = nn.Parameter(torch.zeros(1))\n\n    def forward(self, x):\n        _, ch, h, w = x.size()\n        # Theta path\n        theta = self.snconv1x1_theta(x)\n        theta = theta.view(-1, ch//8, h*w)\n        # Phi path\n        phi = self.snconv1x1_phi(x)\n        phi = self.maxpool(phi)\n        phi = phi.view(-1, ch//8, h*w//4)\n        # Attn map\n        attn = torch.bmm(theta.permute(0, 2, 1), phi)\n        attn = self.softmax(attn)\n        # g path\n        g = self.snconv1x1_g(x)\n        g = self.maxpool(g)\n        g = g.view(-1, ch//2, h*w//4)\n        # Attn_g - o_conv\n        attn_g = torch.bmm(g, attn.permute(0, 2, 1))\n        attn_g = attn_g.view(-1, ch//2, h, w)\n        attn_g = self.snconv1x1_o_conv(attn_g)\n        # Out\n        out = x + self.gamma*attn_g\n        return out\n\n\nclass BigGANBatchNorm(nn.Module):\n    """""" This is a batch norm module that can handle conditional input and can be provided with pre-computed\n        activation means and variances for various truncation parameters.\n\n        We cannot just rely on torch.batch_norm since it cannot handle\n        batched weights (pytorch 1.0.1). We computate batch_norm our-self without updating running means and variances.\n        If you want to train this model you should add running means and variance computation logic.\n    """"""\n    def __init__(self, num_features, condition_vector_dim=None, n_stats=51, eps=1e-4, conditional=True):\n        super(BigGANBatchNorm, self).__init__()\n        self.num_features = num_features\n        self.eps = eps\n        self.conditional = conditional\n\n        # We use pre-computed statistics for n_stats values of truncation between 0 and 1\n        self.register_buffer(\'running_means\', torch.zeros(n_stats, num_features))\n        self.register_buffer(\'running_vars\', torch.ones(n_stats, num_features))\n        self.step_size = 1.0 / (n_stats - 1)\n\n        if conditional:\n            assert condition_vector_dim is not None\n            self.scale = snlinear(in_features=condition_vector_dim, out_features=num_features, bias=False, eps=eps)\n            self.offset = snlinear(in_features=condition_vector_dim, out_features=num_features, bias=False, eps=eps)\n        else:\n            self.weight = torch.nn.Parameter(torch.Tensor(num_features))\n            self.bias = torch.nn.Parameter(torch.Tensor(num_features))\n\n    def forward(self, x, truncation, condition_vector=None):\n        # Retreive pre-computed statistics associated to this truncation\n        coef, start_idx = math.modf(truncation / self.step_size)\n        start_idx = int(start_idx)\n        if coef != 0.0:  # Interpolate\n            running_mean = self.running_means[start_idx] * coef + self.running_means[start_idx + 1] * (1 - coef)\n            running_var = self.running_vars[start_idx] * coef + self.running_vars[start_idx + 1] * (1 - coef)\n        else:\n            running_mean = self.running_means[start_idx]\n            running_var = self.running_vars[start_idx]\n\n        if self.conditional:\n            running_mean = running_mean.unsqueeze(0).unsqueeze(-1).unsqueeze(-1)\n            running_var = running_var.unsqueeze(0).unsqueeze(-1).unsqueeze(-1)\n\n            weight = 1 + self.scale(condition_vector).unsqueeze(-1).unsqueeze(-1)\n            bias = self.offset(condition_vector).unsqueeze(-1).unsqueeze(-1)\n\n            out = (x - running_mean) / torch.sqrt(running_var + self.eps) * weight + bias\n        else:\n            out = F.batch_norm(x, running_mean, running_var, self.weight, self.bias,\n                               training=False, momentum=0.0, eps=self.eps)\n\n        return out\n\n\nclass GenBlock(nn.Module):\n    def __init__(self, in_size, out_size, condition_vector_dim, reduction_factor=4, up_sample=False,\n                 n_stats=51, eps=1e-12):\n        super(GenBlock, self).__init__()\n        self.up_sample = up_sample\n        self.drop_channels = (in_size != out_size)\n        middle_size = in_size // reduction_factor\n\n        self.bn_0 = BigGANBatchNorm(in_size, condition_vector_dim, n_stats=n_stats, eps=eps, conditional=True)\n        self.conv_0 = snconv2d(in_channels=in_size, out_channels=middle_size, kernel_size=1, eps=eps)\n\n        self.bn_1 = BigGANBatchNorm(middle_size, condition_vector_dim, n_stats=n_stats, eps=eps, conditional=True)\n        self.conv_1 = snconv2d(in_channels=middle_size, out_channels=middle_size, kernel_size=3, padding=1, eps=eps)\n\n        self.bn_2 = BigGANBatchNorm(middle_size, condition_vector_dim, n_stats=n_stats, eps=eps, conditional=True)\n        self.conv_2 = snconv2d(in_channels=middle_size, out_channels=middle_size, kernel_size=3, padding=1, eps=eps)\n\n        self.bn_3 = BigGANBatchNorm(middle_size, condition_vector_dim, n_stats=n_stats, eps=eps, conditional=True)\n        self.conv_3 = snconv2d(in_channels=middle_size, out_channels=out_size, kernel_size=1, eps=eps)\n\n        self.relu = nn.ReLU()\n\n    def forward(self, x, cond_vector, truncation):\n        x0 = x\n\n        x = self.bn_0(x, truncation, cond_vector)\n        x = self.relu(x)\n        x = self.conv_0(x)\n\n        x = self.bn_1(x, truncation, cond_vector)\n        x = self.relu(x)\n        if self.up_sample:\n            x = F.interpolate(x, scale_factor=2, mode=\'nearest\')\n        x = self.conv_1(x)\n\n        x = self.bn_2(x, truncation, cond_vector)\n        x = self.relu(x)\n        x = self.conv_2(x)\n\n        x = self.bn_3(x, truncation, cond_vector)\n        x = self.relu(x)\n        x = self.conv_3(x)\n\n        if self.drop_channels:\n            new_channels = x0.shape[1] // 2\n            x0 = x0[:, :new_channels, ...]\n        if self.up_sample:\n            x0 = F.interpolate(x0, scale_factor=2, mode=\'nearest\')\n\n        out = x + x0\n        return out\n\nclass Generator(nn.Module):\n    def __init__(self, config):\n        super(Generator, self).__init__()\n        self.config = config\n        ch = config.channel_width\n        condition_vector_dim = config.z_dim * 2\n\n        self.gen_z = snlinear(in_features=condition_vector_dim,\n                              out_features=4 * 4 * 16 * ch, eps=config.eps)\n\n        layers = []\n        for i, layer in enumerate(config.layers):\n            if i == config.attention_layer_position:\n                layers.append(SelfAttn(ch*layer[1], eps=config.eps))\n            layers.append(GenBlock(ch*layer[1],\n                                   ch*layer[2],\n                                   condition_vector_dim,\n                                   up_sample=layer[0],\n                                   n_stats=config.n_stats,\n                                   eps=config.eps))\n        self.layers = nn.ModuleList(layers)\n\n        self.bn = BigGANBatchNorm(ch, n_stats=config.n_stats, eps=config.eps, conditional=False)\n        self.relu = nn.ReLU()\n        self.conv_to_rgb = snconv2d(in_channels=ch, out_channels=ch, kernel_size=3, padding=1, eps=config.eps)\n        self.tanh = nn.Tanh()\n\n    def forward(self, cond_vector, truncation):\n        z = self.gen_z(cond_vector[0])\n\n        # We use this conversion step to be able to use TF weights:\n        # TF convention on shape is [batch, height, width, channels]\n        # PT convention on shape is [batch, channels, height, width]\n        z = z.view(-1, 4, 4, 16 * self.config.channel_width)\n        z = z.permute(0, 3, 1, 2).contiguous()\n\n        cond_idx = 1\n        for i, layer in enumerate(self.layers):\n            if isinstance(layer, GenBlock):\n                z = layer(z, cond_vector[cond_idx], truncation)\n                cond_idx += 1\n            else:\n                z = layer(z)\n\n        z = self.bn(z, truncation)\n        z = self.relu(z)\n        z = self.conv_to_rgb(z)\n        z = z[:, :3, ...]\n        z = self.tanh(z)\n        return z\n\nclass BigGAN(nn.Module):\n    """"""BigGAN Generator.""""""\n\n    @classmethod\n    def from_pretrained(cls, pretrained_model_name_or_path, cache_dir=None, *inputs, **kwargs):\n        if pretrained_model_name_or_path in PRETRAINED_MODEL_ARCHIVE_MAP:\n            model_file = PRETRAINED_MODEL_ARCHIVE_MAP[pretrained_model_name_or_path]\n            config_file = PRETRAINED_CONFIG_ARCHIVE_MAP[pretrained_model_name_or_path]\n        else:\n            model_file = os.path.join(pretrained_model_name_or_path, WEIGHTS_NAME)\n            config_file = os.path.join(pretrained_model_name_or_path, CONFIG_NAME)\n\n        try:\n            resolved_model_file = cached_path(model_file, cache_dir=cache_dir)\n            resolved_config_file = cached_path(config_file, cache_dir=cache_dir)\n        except EnvironmentError:\n            logger.error(""Wrong model name, should be a valid path to a folder containing ""\n                         ""a {} file and a {} file or a model name in {}"".format(\n                         WEIGHTS_NAME, CONFIG_NAME, PRETRAINED_MODEL_ARCHIVE_MAP.keys()))\n            raise\n\n        logger.info(""loading model {} from cache at {}"".format(pretrained_model_name_or_path, resolved_model_file))\n\n        # Load config\n        config = BigGANConfig.from_json_file(resolved_config_file)\n        logger.info(""Model config {}"".format(config))\n\n        # Instantiate model.\n        model = cls(config, *inputs, **kwargs)\n        state_dict = torch.load(resolved_model_file, map_location=\'cpu\' if not torch.cuda.is_available() else None)\n        model.load_state_dict(state_dict, strict=False)\n        return model\n\n    def __init__(self, config):\n        super(BigGAN, self).__init__()\n        self.config = config\n        self.embeddings = nn.Linear(config.num_classes, config.z_dim, bias=False)\n        self.generator = Generator(config)\n        self.n_latents = len(config.layers) + 1 # one for gen_z + one per layer\n\n    def forward(self, z, class_label, truncation):\n        assert 0 < truncation <= 1\n\n        if not isinstance(z, list):\n            z = self.n_latents*[z]\n        \n        if isinstance(class_label, list):\n            embed = [self.embeddings(l) for l in class_label]\n        else:\n            embed = self.n_latents*[self.embeddings(class_label)]\n        \n        assert len(z) == self.n_latents, f\'Expected {self.n_latents} latents, got {len(z)}\'\n        assert len(embed) == self.n_latents, f\'Expected {self.n_latents} class vectors, got {len(class_label)}\'\n\n        cond_vectors = [torch.cat((z, e), dim=1) for (z, e) in zip(z, embed)]\n        z = self.generator(cond_vectors, truncation)\n        return z\n\n\nif __name__ == ""__main__"":\n    import PIL\n    from .utils import truncated_noise_sample, save_as_images, one_hot_from_names\n    from .convert_tf_to_pytorch import load_tf_weights_in_biggan\n\n    load_cache = False\n    cache_path = \'./saved_model.pt\'\n    config = BigGANConfig()\n    model = BigGAN(config)\n    if not load_cache:\n        model = load_tf_weights_in_biggan(model, config, \'./models/model_128/\', \'./models/model_128/batchnorms_stats.bin\')\n        torch.save(model.state_dict(), cache_path)\n    else:\n        model.load_state_dict(torch.load(cache_path))\n\n    model.eval()\n\n    truncation = 0.4\n    noise = truncated_noise_sample(batch_size=2, truncation=truncation)\n    label = one_hot_from_names(\'diver\', batch_size=2)\n\n    # Tests\n    # noise = np.zeros((1, 128))\n    # label = [983]\n\n    noise = torch.tensor(noise, dtype=torch.float)\n    label = torch.tensor(label, dtype=torch.float)\n    with torch.no_grad():\n        outputs = model(noise, label, truncation)\n    print(outputs.shape)\n\n    save_as_images(outputs)\n'"
models/biggan/pytorch_biggan/pytorch_pretrained_biggan/utils.py,0,"b'# coding: utf-8\n"""""" BigGAN utilities to prepare truncated noise samples and convert/save/display output images.\n    Also comprise ImageNet utilities to prepare one hot input vectors for ImageNet classes.\n    We use Wordnet so you can just input a name in a string and automatically get a corresponding\n    imagenet class if it exists (or a hypo/hypernym exists in imagenet).\n""""""\nfrom __future__ import absolute_import, division, print_function, unicode_literals\n\nimport json\nimport logging\nfrom io import BytesIO\n\nimport numpy as np\nfrom scipy.stats import truncnorm\n\nlogger = logging.getLogger(__name__)\n\nNUM_CLASSES = 1000\n\n\ndef truncated_noise_sample(batch_size=1, dim_z=128, truncation=1., seed=None):\n    """""" Create a truncated noise vector.\n        Params:\n            batch_size: batch size.\n            dim_z: dimension of z\n            truncation: truncation value to use\n            seed: seed for the random generator\n        Output:\n            array of shape (batch_size, dim_z)\n    """"""\n    state = None if seed is None else np.random.RandomState(seed)\n    values = truncnorm.rvs(-2, 2, size=(batch_size, dim_z), random_state=state).astype(np.float32)\n    return truncation * values\n\n\ndef convert_to_images(obj):\n    """""" Convert an output tensor from BigGAN in a list of images.\n        Params:\n            obj: tensor or numpy array of shape (batch_size, channels, height, width)\n        Output:\n            list of Pillow Images of size (height, width)\n    """"""\n    try:\n        import PIL\n    except ImportError:\n        raise ImportError(""Please install Pillow to use images: pip install Pillow"")\n\n    if not isinstance(obj, np.ndarray):\n        obj = obj.detach().numpy()\n\n    obj = obj.transpose((0, 2, 3, 1))\n    obj = np.clip(((obj + 1) / 2.0) * 256, 0, 255)\n\n    img = []\n    for i, out in enumerate(obj):\n        out_array = np.asarray(np.uint8(out), dtype=np.uint8)\n        img.append(PIL.Image.fromarray(out_array))\n    return img\n\n\ndef save_as_images(obj, file_name=\'output\'):\n    """""" Convert and save an output tensor from BigGAN in a list of saved images.\n        Params:\n            obj: tensor or numpy array of shape (batch_size, channels, height, width)\n            file_name: path and beggingin of filename to save.\n                Images will be saved as `file_name_{image_number}.png`\n    """"""\n    img = convert_to_images(obj)\n\n    for i, out in enumerate(img):\n        current_file_name = file_name + \'_%d.png\' % i\n        logger.info(""Saving image to {}"".format(current_file_name))\n        out.save(current_file_name, \'png\')\n\n\ndef display_in_terminal(obj):\n    """""" Convert and display an output tensor from BigGAN in the terminal.\n        This function use `libsixel` and will only work in a libsixel-compatible terminal.\n        Please refer to https://github.com/saitoha/libsixel for more details.\n\n        Params:\n            obj: tensor or numpy array of shape (batch_size, channels, height, width)\n            file_name: path and beggingin of filename to save.\n                Images will be saved as `file_name_{image_number}.png`\n    """"""\n    try:\n        import PIL\n        from libsixel import (sixel_output_new, sixel_dither_new, sixel_dither_initialize,\n                              sixel_dither_set_palette, sixel_dither_set_pixelformat,\n                              sixel_dither_get, sixel_encode, sixel_dither_unref,\n                              sixel_output_unref, SIXEL_PIXELFORMAT_RGBA8888,\n                              SIXEL_PIXELFORMAT_RGB888, SIXEL_PIXELFORMAT_PAL8,\n                              SIXEL_PIXELFORMAT_G8, SIXEL_PIXELFORMAT_G1)\n    except ImportError:\n        raise ImportError(""Display in Terminal requires Pillow, libsixel ""\n                          ""and a libsixel compatible terminal. ""\n                          ""Please read info at https://github.com/saitoha/libsixel ""\n                          ""and install with pip install Pillow libsixel-python"")\n\n    s = BytesIO()\n\n    images = convert_to_images(obj)\n    widths, heights = zip(*(i.size for i in images))\n\n    output_width = sum(widths)\n    output_height = max(heights)\n\n    output_image = PIL.Image.new(\'RGB\', (output_width, output_height))\n\n    x_offset = 0\n    for im in images:\n        output_image.paste(im, (x_offset,0))\n        x_offset += im.size[0]\n\n    try:\n        data = output_image.tobytes()\n    except NotImplementedError:\n        data = output_image.tostring()\n    output = sixel_output_new(lambda data, s: s.write(data), s)\n\n    try:\n        if output_image.mode == \'RGBA\':\n            dither = sixel_dither_new(256)\n            sixel_dither_initialize(dither, data, output_width, output_height, SIXEL_PIXELFORMAT_RGBA8888)\n        elif output_image.mode == \'RGB\':\n            dither = sixel_dither_new(256)\n            sixel_dither_initialize(dither, data, output_width, output_height, SIXEL_PIXELFORMAT_RGB888)\n        elif output_image.mode == \'P\':\n            palette = output_image.getpalette()\n            dither = sixel_dither_new(256)\n            sixel_dither_set_palette(dither, palette)\n            sixel_dither_set_pixelformat(dither, SIXEL_PIXELFORMAT_PAL8)\n        elif output_image.mode == \'L\':\n            dither = sixel_dither_get(SIXEL_BUILTIN_G8)\n            sixel_dither_set_pixelformat(dither, SIXEL_PIXELFORMAT_G8)\n        elif output_image.mode == \'1\':\n            dither = sixel_dither_get(SIXEL_BUILTIN_G1)\n            sixel_dither_set_pixelformat(dither, SIXEL_PIXELFORMAT_G1)\n        else:\n            raise RuntimeError(\'unexpected output_image mode\')\n        try:\n            sixel_encode(data, output_width, output_height, 1, dither, output)\n            print(s.getvalue().decode(\'ascii\'))\n        finally:\n            sixel_dither_unref(dither)\n    finally:\n        sixel_output_unref(output)\n\n\ndef one_hot_from_int(int_or_list, batch_size=1):\n    """""" Create a one-hot vector from a class index or a list of class indices.\n        Params:\n            int_or_list: int, or list of int, of the imagenet classes (between 0 and 999)\n            batch_size: batch size.\n                If int_or_list is an int create a batch of identical classes.\n                If int_or_list is a list, we should have `len(int_or_list) == batch_size`\n        Output:\n            array of shape (batch_size, 1000)\n    """"""\n    if isinstance(int_or_list, int):\n        int_or_list = [int_or_list]\n\n    if len(int_or_list) == 1 and batch_size > 1:\n        int_or_list = [int_or_list[0]] * batch_size\n\n    assert batch_size == len(int_or_list)\n\n    array = np.zeros((batch_size, NUM_CLASSES), dtype=np.float32)\n    for i, j in enumerate(int_or_list):\n        array[i, j] = 1.0\n    return array\n\n\ndef one_hot_from_names(class_name_or_list, batch_size=1):\n    """""" Create a one-hot vector from the name of an imagenet class (\'tennis ball\', \'daisy\', ...).\n        We use NLTK\'s wordnet search to try to find the relevant synset of ImageNet and take the first one.\n        If we can\'t find it direcly, we look at the hyponyms and hypernyms of the class name.\n\n        Params:\n            class_name_or_list: string containing the name of an imagenet object or a list of such strings (for a batch).\n        Output:\n            array of shape (batch_size, 1000)\n    """"""\n    try:\n        from nltk.corpus import wordnet as wn\n    except ImportError:\n        raise ImportError(""You need to install nltk to use this function"")\n\n    if not isinstance(class_name_or_list, (list, tuple)):\n        class_name_or_list = [class_name_or_list]\n    else:\n        batch_size = max(batch_size, len(class_name_or_list))\n\n    classes = []\n    for class_name in class_name_or_list:\n        class_name = class_name.replace("" "", ""_"")\n\n        original_synsets = wn.synsets(class_name)\n        original_synsets = list(filter(lambda s: s.pos() == \'n\', original_synsets))  # keep only names\n        if not original_synsets:\n            return None\n\n        possible_synsets = list(filter(lambda s: s.offset() in IMAGENET, original_synsets))\n        if possible_synsets:\n            classes.append(IMAGENET[possible_synsets[0].offset()])\n        else:\n            # try hypernyms and hyponyms\n            possible_synsets = sum([s.hypernyms() + s.hyponyms() for s in original_synsets], [])\n            possible_synsets = list(filter(lambda s: s.offset() in IMAGENET, possible_synsets))\n            if possible_synsets:\n                classes.append(IMAGENET[possible_synsets[0].offset()])\n\n    return one_hot_from_int(classes, batch_size=batch_size)\n\n\nIMAGENET = {1440764: 0, 1443537: 1, 1484850: 2, 1491361: 3, 1494475: 4, 1496331: 5, 1498041: 6, 1514668: 7, 1514859: 8, 1518878: 9, 1530575: 10, 1531178: 11, 1532829: 12, 1534433: 13, 1537544: 14, 1558993: 15, 1560419: 16, 1580077: 17, 1582220: 18, 1592084: 19, 1601694: 20, 1608432: 21, 1614925: 22, 1616318: 23, 1622779: 24, 1629819: 25, 1630670: 26, 1631663: 27, 1632458: 28, 1632777: 29, 1641577: 30, 1644373: 31, 1644900: 32, 1664065: 33, 1665541: 34, 1667114: 35, 1667778: 36, 1669191: 37, 1675722: 38, 1677366: 39, 1682714: 40, 1685808: 41, 1687978: 42, 1688243: 43, 1689811: 44, 1692333: 45, 1693334: 46, 1694178: 47, 1695060: 48, 1697457: 49, 1698640: 50, 1704323: 51, 1728572: 52, 1728920: 53, 1729322: 54, 1729977: 55, 1734418: 56, 1735189: 57, 1737021: 58, 1739381: 59, 1740131: 60, 1742172: 61, 1744401: 62, 1748264: 63, 1749939: 64, 1751748: 65, 1753488: 66, 1755581: 67, 1756291: 68, 1768244: 69, 1770081: 70, 1770393: 71, 1773157: 72, 1773549: 73, 1773797: 74, 1774384: 75, 1774750: 76, 1775062: 77, 1776313: 78, 1784675: 79, 1795545: 80, 1796340: 81, 1797886: 82, 1798484: 83, 1806143: 84, 1806567: 85, 1807496: 86, 1817953: 87, 1818515: 88, 1819313: 89, 1820546: 90, 1824575: 91, 1828970: 92, 1829413: 93, 1833805: 94, 1843065: 95, 1843383: 96, 1847000: 97, 1855032: 98, 1855672: 99, 1860187: 100, 1871265: 101, 1872401: 102, 1873310: 103, 1877812: 104, 1882714: 105, 1883070: 106, 1910747: 107, 1914609: 108, 1917289: 109, 1924916: 110, 1930112: 111, 1943899: 112, 1944390: 113, 1945685: 114, 1950731: 115, 1955084: 116, 1968897: 117, 1978287: 118, 1978455: 119, 1980166: 120, 1981276: 121, 1983481: 122, 1984695: 123, 1985128: 124, 1986214: 125, 1990800: 126, 2002556: 127, 2002724: 128, 2006656: 129, 2007558: 130, 2009229: 131, 2009912: 132, 2011460: 133, 2012849: 134, 2013706: 135, 2017213: 136, 2018207: 137, 2018795: 138, 2025239: 139, 2027492: 140, 2028035: 141, 2033041: 142, 2037110: 143, 2051845: 144, 2056570: 145, 2058221: 146, 2066245: 147, 2071294: 148, 2074367: 149, 2077923: 150, 2085620: 151, 2085782: 152, 2085936: 153, 2086079: 154, 2086240: 155, 2086646: 156, 2086910: 157, 2087046: 158, 2087394: 159, 2088094: 160, 2088238: 161, 2088364: 162, 2088466: 163, 2088632: 164, 2089078: 165, 2089867: 166, 2089973: 167, 2090379: 168, 2090622: 169, 2090721: 170, 2091032: 171, 2091134: 172, 2091244: 173, 2091467: 174, 2091635: 175, 2091831: 176, 2092002: 177, 2092339: 178, 2093256: 179, 2093428: 180, 2093647: 181, 2093754: 182, 2093859: 183, 2093991: 184, 2094114: 185, 2094258: 186, 2094433: 187, 2095314: 188, 2095570: 189, 2095889: 190, 2096051: 191, 2096177: 192, 2096294: 193, 2096437: 194, 2096585: 195, 2097047: 196, 2097130: 197, 2097209: 198, 2097298: 199, 2097474: 200, 2097658: 201, 2098105: 202, 2098286: 203, 2098413: 204, 2099267: 205, 2099429: 206, 2099601: 207, 2099712: 208, 2099849: 209, 2100236: 210, 2100583: 211, 2100735: 212, 2100877: 213, 2101006: 214, 2101388: 215, 2101556: 216, 2102040: 217, 2102177: 218, 2102318: 219, 2102480: 220, 2102973: 221, 2104029: 222, 2104365: 223, 2105056: 224, 2105162: 225, 2105251: 226, 2105412: 227, 2105505: 228, 2105641: 229, 2105855: 230, 2106030: 231, 2106166: 232, 2106382: 233, 2106550: 234, 2106662: 235, 2107142: 236, 2107312: 237, 2107574: 238, 2107683: 239, 2107908: 240, 2108000: 241, 2108089: 242, 2108422: 243, 2108551: 244, 2108915: 245, 2109047: 246, 2109525: 247, 2109961: 248, 2110063: 249, 2110185: 250, 2110341: 251, 2110627: 252, 2110806: 253, 2110958: 254, 2111129: 255, 2111277: 256, 2111500: 257, 2111889: 258, 2112018: 259, 2112137: 260, 2112350: 261, 2112706: 262, 2113023: 263, 2113186: 264, 2113624: 265, 2113712: 266, 2113799: 267, 2113978: 268, 2114367: 269, 2114548: 270, 2114712: 271, 2114855: 272, 2115641: 273, 2115913: 274, 2116738: 275, 2117135: 276, 2119022: 277, 2119789: 278, 2120079: 279, 2120505: 280, 2123045: 281, 2123159: 282, 2123394: 283, 2123597: 284, 2124075: 285, 2125311: 286, 2127052: 287, 2128385: 288, 2128757: 289, 2128925: 290, 2129165: 291, 2129604: 292, 2130308: 293, 2132136: 294, 2133161: 295, 2134084: 296, 2134418: 297, 2137549: 298, 2138441: 299, 2165105: 300, 2165456: 301, 2167151: 302, 2168699: 303, 2169497: 304, 2172182: 305, 2174001: 306, 2177972: 307, 2190166: 308, 2206856: 309, 2219486: 310, 2226429: 311, 2229544: 312, 2231487: 313, 2233338: 314, 2236044: 315, 2256656: 316, 2259212: 317, 2264363: 318, 2268443: 319, 2268853: 320, 2276258: 321, 2277742: 322, 2279972: 323, 2280649: 324, 2281406: 325, 2281787: 326, 2317335: 327, 2319095: 328, 2321529: 329, 2325366: 330, 2326432: 331, 2328150: 332, 2342885: 333, 2346627: 334, 2356798: 335, 2361337: 336, 2363005: 337, 2364673: 338, 2389026: 339, 2391049: 340, 2395406: 341, 2396427: 342, 2397096: 343, 2398521: 344, 2403003: 345, 2408429: 346, 2410509: 347, 2412080: 348, 2415577: 349, 2417914: 350, 2422106: 351, 2422699: 352, 2423022: 353, 2437312: 354, 2437616: 355, 2441942: 356, 2442845: 357, 2443114: 358, 2443484: 359, 2444819: 360, 2445715: 361, 2447366: 362, 2454379: 363, 2457408: 364, 2480495: 365, 2480855: 366, 2481823: 367, 2483362: 368, 2483708: 369, 2484975: 370, 2486261: 371, 2486410: 372, 2487347: 373, 2488291: 374, 2488702: 375, 2489166: 376, 2490219: 377, 2492035: 378, 2492660: 379, 2493509: 380, 2493793: 381, 2494079: 382, 2497673: 383, 2500267: 384, 2504013: 385, 2504458: 386, 2509815: 387, 2510455: 388, 2514041: 389, 2526121: 390, 2536864: 391, 2606052: 392, 2607072: 393, 2640242: 394, 2641379: 395, 2643566: 396, 2655020: 397, 2666196: 398, 2667093: 399, 2669723: 400, 2672831: 401, 2676566: 402, 2687172: 403, 2690373: 404, 2692877: 405, 2699494: 406, 2701002: 407, 2704792: 408, 2708093: 409, 2727426: 410, 2730930: 411, 2747177: 412, 2749479: 413, 2769748: 414, 2776631: 415, 2777292: 416, 2782093: 417, 2783161: 418, 2786058: 419, 2787622: 420, 2788148: 421, 2790996: 422, 2791124: 423, 2791270: 424, 2793495: 425, 2794156: 426, 2795169: 427, 2797295: 428, 2799071: 429, 2802426: 430, 2804414: 431, 2804610: 432, 2807133: 433, 2808304: 434, 2808440: 435, 2814533: 436, 2814860: 437, 2815834: 438, 2817516: 439, 2823428: 440, 2823750: 441, 2825657: 442, 2834397: 443, 2835271: 444, 2837789: 445, 2840245: 446, 2841315: 447, 2843684: 448, 2859443: 449, 2860847: 450, 2865351: 451, 2869837: 452, 2870880: 453, 2871525: 454, 2877765: 455, 2879718: 456, 2883205: 457, 2892201: 458, 2892767: 459, 2894605: 460, 2895154: 461, 2906734: 462, 2909870: 463, 2910353: 464, 2916936: 465, 2917067: 466, 2927161: 467, 2930766: 468, 2939185: 469, 2948072: 470, 2950826: 471, 2951358: 472, 2951585: 473, 2963159: 474, 2965783: 475, 2966193: 476, 2966687: 477, 2971356: 478, 2974003: 479, 2977058: 480, 2978881: 481, 2979186: 482, 2980441: 483, 2981792: 484, 2988304: 485, 2992211: 486, 2992529: 487, 2999410: 488, 3000134: 489, 3000247: 490, 3000684: 491, 3014705: 492, 3016953: 493, 3017168: 494, 3018349: 495, 3026506: 496, 3028079: 497, 3032252: 498, 3041632: 499, 3042490: 500, 3045698: 501, 3047690: 502, 3062245: 503, 3063599: 504, 3063689: 505, 3065424: 506, 3075370: 507, 3085013: 508, 3089624: 509, 3095699: 510, 3100240: 511, 3109150: 512, 3110669: 513, 3124043: 514, 3124170: 515, 3125729: 516, 3126707: 517, 3127747: 518, 3127925: 519, 3131574: 520, 3133878: 521, 3134739: 522, 3141823: 523, 3146219: 524, 3160309: 525, 3179701: 526, 3180011: 527, 3187595: 528, 3188531: 529, 3196217: 530, 3197337: 531, 3201208: 532, 3207743: 533, 3207941: 534, 3208938: 535, 3216828: 536, 3218198: 537, 3220513: 538, 3223299: 539, 3240683: 540, 3249569: 541, 3250847: 542, 3255030: 543, 3259280: 544, 3271574: 545, 3272010: 546, 3272562: 547, 3290653: 548, 3291819: 549, 3297495: 550, 3314780: 551, 3325584: 552, 3337140: 553, 3344393: 554, 3345487: 555, 3347037: 556, 3355925: 557, 3372029: 558, 3376595: 559, 3379051: 560, 3384352: 561, 3388043: 562, 3388183: 563, 3388549: 564, 3393912: 565, 3394916: 566, 3400231: 567, 3404251: 568, 3417042: 569, 3424325: 570, 3425413: 571, 3443371: 572, 3444034: 573, 3445777: 574, 3445924: 575, 3447447: 576, 3447721: 577, 3450230: 578, 3452741: 579, 3457902: 580, 3459775: 581, 3461385: 582, 3467068: 583, 3476684: 584, 3476991: 585, 3478589: 586, 3481172: 587, 3482405: 588, 3483316: 589, 3485407: 590, 3485794: 591, 3492542: 592, 3494278: 593, 3495258: 594, 3496892: 595, 3498962: 596, 3527444: 597, 3529860: 598, 3530642: 599, 3532672: 600, 3534580: 601, 3535780: 602, 3538406: 603, 3544143: 604, 3584254: 605, 3584829: 606, 3590841: 607, 3594734: 608, 3594945: 609, 3595614: 610, 3598930: 611, 3599486: 612, 3602883: 613, 3617480: 614, 3623198: 615, 3627232: 616, 3630383: 617, 3633091: 618, 3637318: 619, 3642806: 620, 3649909: 621, 3657121: 622, 3658185: 623, 3661043: 624, 3662601: 625, 3666591: 626, 3670208: 627, 3673027: 628, 3676483: 629, 3680355: 630, 3690938: 631, 3691459: 632, 3692522: 633, 3697007: 634, 3706229: 635, 3709823: 636, 3710193: 637, 3710637: 638, 3710721: 639, 3717622: 640, 3720891: 641, 3721384: 642, 3724870: 643, 3729826: 644, 3733131: 645, 3733281: 646, 3733805: 647, 3742115: 648, 3743016: 649, 3759954: 650, 3761084: 651, 3763968: 652, 3764736: 653, 3769881: 654, 3770439: 655, 3770679: 656, 3773504: 657, 3775071: 658, 3775546: 659, 3776460: 660, 3777568: 661, 3777754: 662, 3781244: 663, 3782006: 664, 3785016: 665, 3786901: 666, 3787032: 667, 3788195: 668, 3788365: 669, 3791053: 670, 3792782: 671, 3792972: 672, 3793489: 673, 3794056: 674, 3796401: 675, 3803284: 676, 3804744: 677, 3814639: 678, 3814906: 679, 3825788: 680, 3832673: 681, 3837869: 682, 3838899: 683, 3840681: 684, 3841143: 685, 3843555: 686, 3854065: 687, 3857828: 688, 3866082: 689, 3868242: 690, 3868863: 691, 3871628: 692, 3873416: 693, 3874293: 694, 3874599: 695, 3876231: 696, 3877472: 697, 3877845: 698, 3884397: 699, 3887697: 700, 3888257: 701, 3888605: 702, 3891251: 703, 3891332: 704, 3895866: 705, 3899768: 706, 3902125: 707, 3903868: 708, 3908618: 709, 3908714: 710, 3916031: 711, 3920288: 712, 3924679: 713, 3929660: 714, 3929855: 715, 3930313: 716, 3930630: 717, 3933933: 718, 3935335: 719, 3937543: 720, 3938244: 721, 3942813: 722, 3944341: 723, 3947888: 724, 3950228: 725, 3954731: 726, 3956157: 727, 3958227: 728, 3961711: 729, 3967562: 730, 3970156: 731, 3976467: 732, 3976657: 733, 3977966: 734, 3980874: 735, 3982430: 736, 3983396: 737, 3991062: 738, 3992509: 739, 3995372: 740, 3998194: 741, 4004767: 742, 4005630: 743, 4008634: 744, 4009552: 745, 4019541: 746, 4023962: 747, 4026417: 748, 4033901: 749, 4033995: 750, 4037443: 751, 4039381: 752, 4040759: 753, 4041544: 754, 4044716: 755, 4049303: 756, 4065272: 757, 4067472: 758, 4069434: 759, 4070727: 760, 4074963: 761, 4081281: 762, 4086273: 763, 4090263: 764, 4099969: 765, 4111531: 766, 4116512: 767, 4118538: 768, 4118776: 769, 4120489: 770, 4125021: 771, 4127249: 772, 4131690: 773, 4133789: 774, 4136333: 775, 4141076: 776, 4141327: 777, 4141975: 778, 4146614: 779, 4147183: 780, 4149813: 781, 4152593: 782, 4153751: 783, 4154565: 784, 4162706: 785, 4179913: 786, 4192698: 787, 4200800: 788, 4201297: 789, 4204238: 790, 4204347: 791, 4208210: 792, 4209133: 793, 4209239: 794, 4228054: 795, 4229816: 796, 4235860: 797, 4238763: 798, 4239074: 799, 4243546: 800, 4251144: 801, 4252077: 802, 4252225: 803, 4254120: 804, 4254680: 805, 4254777: 806, 4258138: 807, 4259630: 808, 4263257: 809, 4264628: 810, 4265275: 811, 4266014: 812, 4270147: 813, 4273569: 814, 4275548: 815, 4277352: 816, 4285008: 817, 4286575: 818, 4296562: 819, 4310018: 820, 4311004: 821, 4311174: 822, 4317175: 823, 4325704: 824, 4326547: 825, 4328186: 826, 4330267: 827, 4332243: 828, 4335435: 829, 4336792: 830, 4344873: 831, 4346328: 832, 4347754: 833, 4350905: 834, 4355338: 835, 4355933: 836, 4356056: 837, 4357314: 838, 4366367: 839, 4367480: 840, 4370456: 841, 4371430: 842, 4371774: 843, 4372370: 844, 4376876: 845, 4380533: 846, 4389033: 847, 4392985: 848, 4398044: 849, 4399382: 850, 4404412: 851, 4409515: 852, 4417672: 853, 4418357: 854, 4423845: 855, 4428191: 856, 4429376: 857, 4435653: 858, 4442312: 859, 4443257: 860, 4447861: 861, 4456115: 862, 4458633: 863, 4461696: 864, 4462240: 865, 4465501: 866, 4467665: 867, 4476259: 868, 4479046: 869, 4482393: 870, 4483307: 871, 4485082: 872, 4486054: 873, 4487081: 874, 4487394: 875, 4493381: 876, 4501370: 877, 4505470: 878, 4507155: 879, 4509417: 880, 4515003: 881, 4517823: 882, 4522168: 883, 4523525: 884, 4525038: 885, 4525305: 886, 4532106: 887, 4532670: 888, 4536866: 889, 4540053: 890, 4542943: 891, 4548280: 892, 4548362: 893, 4550184: 894, 4552348: 895, 4553703: 896, 4554684: 897, 4557648: 898, 4560804: 899, 4562935: 900, 4579145: 901, 4579432: 902, 4584207: 903, 4589890: 904, 4590129: 905, 4591157: 906, 4591713: 907, 4592741: 908, 4596742: 909, 4597913: 910, 4599235: 911, 4604644: 912, 4606251: 913, 4612504: 914, 4613696: 915, 6359193: 916, 6596364: 917, 6785654: 918, 6794110: 919, 6874185: 920, 7248320: 921, 7565083: 922, 7579787: 923, 7583066: 924, 7584110: 925, 7590611: 926, 7613480: 927, 7614500: 928, 7615774: 929, 7684084: 930, 7693725: 931, 7695742: 932, 7697313: 933, 7697537: 934, 7711569: 935, 7714571: 936, 7714990: 937, 7715103: 938, 7716358: 939, 7716906: 940, 7717410: 941, 7717556: 942, 7718472: 943, 7718747: 944, 7720875: 945, 7730033: 946, 7734744: 947, 7742313: 948, 7745940: 949, 7747607: 950, 7749582: 951, 7753113: 952, 7753275: 953, 7753592: 954, 7754684: 955, 7760859: 956, 7768694: 957, 7802026: 958, 7831146: 959, 7836838: 960, 7860988: 961, 7871810: 962, 7873807: 963, 7875152: 964, 7880968: 965, 7892512: 966, 7920052: 967, 7930864: 968, 7932039: 969, 9193705: 970, 9229709: 971, 9246464: 972, 9256479: 973, 9288635: 974, 9332890: 975, 9399592: 976, 9421951: 977, 9428293: 978, 9468604: 979, 9472597: 980, 9835506: 981, 10148035: 982, 10565667: 983, 11879895: 984, 11939491: 985, 12057211: 986, 12144580: 987, 12267677: 988, 12620546: 989, 12768682: 990, 12985857: 991, 12998815: 992, 13037406: 993, 13040303: 994, 13044778: 995, 13052670: 996, 13054560: 997, 13133613: 998, 15075141: 999}\n'"
