file_path,api_count,code
i3d_pt_demo.py,6,"b'import argparse\n\nimport numpy as np\nimport torch\n\nfrom src.i3dpt import I3D\n\nrgb_pt_checkpoint = \'model/model_rgb.pth\'\n\n\ndef run_demo(args):\n    kinetics_classes = [x.strip() for x in open(args.classes_path)]\n\n    def get_scores(sample, model):\n        sample_var = torch.autograd.Variable(torch.from_numpy(sample).cuda())\n        out_var, out_logit = model(sample_var)\n        out_tensor = out_var.data.cpu()\n\n        top_val, top_idx = torch.sort(out_tensor, 1, descending=True)\n\n        print(\n            \'Top {} classes and associated probabilities: \'.format(args.top_k))\n        for i in range(args.top_k):\n            print(\'[{}]: {:.6E}\'.format(kinetics_classes[top_idx[0, i]],\n                                        top_val[0, i]))\n        return out_logit\n\n    # Rung RGB model\n    if args.rgb:\n        i3d_rgb = I3D(num_classes=400, modality=\'rgb\')\n        i3d_rgb.eval()\n        i3d_rgb.load_state_dict(torch.load(args.rgb_weights_path))\n        i3d_rgb.cuda()\n\n        rgb_sample = np.load(args.rgb_sample_path).transpose(0, 4, 1, 2, 3)\n        out_rgb_logit = get_scores(rgb_sample, i3d_rgb)\n\n    # Run flow model\n    if args.flow:\n        i3d_flow = I3D(num_classes=400, modality=\'flow\')\n        i3d_flow.eval()\n        i3d_flow.load_state_dict(torch.load(args.flow_weights_path))\n        i3d_flow.cuda()\n\n        flow_sample = np.load(args.flow_sample_path).transpose(0, 4, 1, 2, 3)\n        out_flow_logit = get_scores(flow_sample, i3d_flow)\n\n    # Joint model\n    if args.flow and args.rgb:\n        out_logit = out_rgb_logit + out_flow_logit\n        out_softmax = torch.nn.functional.softmax(out_logit, 1).data.cpu()\n        top_val, top_idx = torch.sort(out_softmax, 1, descending=True)\n\n        print(\'===== Final predictions ====\')\n        print(\'logits proba class \'.format(args.top_k))\n        for i in range(args.top_k):\n            logit_score = out_logit[0, top_idx[0, i]].data.item()\n            print(\'{:.6e} {:.6e} {}\'.format(logit_score, top_val[0, i],\n                                            kinetics_classes[top_idx[0, i]]))\n\n\nif __name__ == ""__main__"":\n    parser = argparse.ArgumentParser(\'Runs inflated inception v1 network on\\\n    cricket sample from tensorflow demo (generate the network weights with\\\n    i3d_tf_to_pt.py first)\')\n\n    # RGB arguments\n    parser.add_argument(\n        \'--rgb\', action=\'store_true\', help=\'Evaluate RGB pretrained network\')\n    parser.add_argument(\n        \'--rgb_weights_path\',\n        type=str,\n        default=\'model/model_rgb.pth\',\n        help=\'Path to rgb model state_dict\')\n    parser.add_argument(\n        \'--rgb_sample_path\',\n        type=str,\n        default=\'data/kinetic-samples/v_CricketShot_g04_c01_rgb.npy\',\n        help=\'Path to kinetics rgb numpy sample\')\n\n    # Flow arguments\n    parser.add_argument(\n        \'--flow\', action=\'store_true\', help=\'Evaluate flow pretrained network\')\n    parser.add_argument(\n        \'--flow_weights_path\',\n        type=str,\n        default=\'model/model_flow.pth\',\n        help=\'Path to flow model state_dict\')\n    parser.add_argument(\n        \'--flow_sample_path\',\n        type=str,\n        default=\'data/kinetic-samples/v_CricketShot_g04_c01_flow.npy\',\n        help=\'Path to kinetics flow numpy sample\')\n\n    parser.add_argument(\n        \'--classes_path\',\n        type=str,\n        default=\'data/kinetic-samples/label_map.txt\',\n        help=\'Number of video_frames to use (should be a multiple of 8)\')\n    parser.add_argument(\n        \'--top_k\',\n        type=int,\n        default=\'5\',\n        help=\'When display_samples, number of top classes to display\')\n    args = parser.parse_args()\n    run_demo(args)\n'"
i3d_pt_profiling.py,6,"b""import argparse\n\nimport torch\nimport torchvision.transforms as transforms\nimport torchvision.datasets as datasets\n\nfrom src.i3dpt import I3D\n\n# Use this code to profile with kernprof\n# Install using `pip install line_profiler`\n# Launch `kernprof -lv i3d_pt_profiling.py`\n\n\n@profile\ndef run(model, dataloader, criterion, optimizer, frame_nb):\n    # Load data\n    for i, (input_2d, target) in enumerate(dataloader):\n        optimizer.zero_grad\n        # Prepare data for pytorch forward pass\n        input_3d = input_2d.clone().unsqueeze(2).repeat(1, 1, frame_nb, 1, 1)\n        input_3d_var = torch.autograd.Variable(input_3d.cuda())\n\n        # Pytorch forward pass\n        out_pt, _ = model(input_3d_var)\n        loss = criterion(out_pt, torch.ones_like(out_pt))\n        loss.backward()\n        optimizer.step()\n\n\ndef run_profile(args):\n    normalize = transforms.Normalize(\n        mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n\n    # Use pytorch image dataset, each image is duplicated in the\n    # temporal dimension in order to produce a proxy for a\n    # spatio-temporal video input\n    dataset_path = 'data/dummy-dataset'\n    dataset = datasets.ImageFolder(dataset_path,\n                                   transforms.Compose([\n                                       transforms.CenterCrop(args.im_size),\n                                       transforms.ToTensor(),\n                                       normalize,\n                                   ]))\n\n    # Initialize input params\n    batch_size = 2\n\n    # Initialize dataset\n    loader = torch.utils.data.DataLoader(\n        dataset, batch_size=batch_size, shuffle=False)\n\n    # Initialize pytorch I3D\n    i3nception_pt = I3D(num_classes=400)\n    i3nception_pt.eval()\n    i3nception_pt.load_state_dict(torch.load(args.rgb_weights_path))\n    i3nception_pt.train()\n    i3nception_pt.cuda()\n\n    l1_loss = torch.nn.L1Loss()\n    sgd = torch.optim.SGD(i3nception_pt.parameters(), lr=0.001, momentum=0.9)\n\n    run(i3nception_pt, loader, l1_loss, sgd, frame_nb=args.frame_nb)\n\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser('Runs inflated inception v1 network on\\\n    cricket sample from tensorflow demo (generate the network weights with\\\n    i3d_tf_to_pt.py first)')\n    parser.add_argument(\n        '--rgb_weights_path',\n        type=str,\n        default='model/model_rgb.pth',\n        help='Path to model state_dict')\n    parser.add_argument(\n        '--frame_nb',\n        type=int,\n        default='16',\n        help='Number of video_frames to use (should be a multiple of 8)')\n    parser.add_argument(\n        '--im_size', type=int, default='224', help='Size of center crop')\n    args = parser.parse_args()\n    run_profile(args)\n"""
i3d_tf_to_pt.py,7,"b'import argparse\n\nfrom matplotlib import pyplot as plt\nimport tensorflow as tf\nimport torch\nimport torchvision.transforms as transforms\nimport torchvision.datasets as datasets\n\nfrom src.i3dtf import InceptionI3d\nfrom src.i3dpt import I3D\nfrom src.monitorutils import compare_outputs\n\n\ndef transfer_weights(tf_checkpoint, pt_checkpoint, batch_size, modality=\'rgb\'):\n    intermediate_feature = False\n    im_size = 224\n    dataset = datasets.ImageFolder(\n        \'data/dummy-dataset\',\n        transforms.Compose([\n            transforms.CenterCrop(im_size),\n            transforms.ToTensor(),\n            #                                   normalize,\n        ]))\n\n    # Initialize input params\n    if modality == \'rgb\':\n        in_channels = 3\n    elif modality == \'flow\':\n        in_channels = 2\n    else:\n        raise ValueError(\n            \'{} not among known modalities [rgb|flow]\'.format(modality))\n\n    frame_nb = 16  # Number of items in depth (temporal) dimension\n    class_nb = 400\n\n    # Initialize dataset\n    loader = torch.utils.data.DataLoader(\n        dataset, batch_size=batch_size, shuffle=False)\n\n    # Initialize pytorch I3D\n    i3nception_pt = I3D(num_classes=400, modality=modality)\n\n    # Initialzie tensorflow I3D\n    if modality == \'rgb\':\n        scope = \'RGB\'\n    elif modality == \'flow\':\n        scope = \'Flow\'\n\n    with tf.variable_scope(scope):\n        rgb_model = InceptionI3d(class_nb, final_endpoint=\'Predictions\')\n        # Tensorflow forward pass\n        rgb_input = tf.placeholder(\n            tf.float32,\n            shape=(batch_size, frame_nb, im_size, im_size, in_channels))\n        rgb_logits, _ = rgb_model(\n            rgb_input, is_training=False, dropout_keep_prob=1.0)\n\n    # Get params for tensorflow weight retreival\n    rgb_variable_map = {}\n    for variable in tf.global_variables():\n        if variable.name.split(\'/\')[0] == scope:\n            rgb_variable_map[variable.name.replace(\':0\', \'\')] = variable\n\n    criterion = torch.nn.L1Loss()\n    rgb_saver = tf.train.Saver(var_list=rgb_variable_map, reshape=True)\n    with tf.Session() as sess:\n        # Load saved tensorflow weights\n        rgb_saver.restore(sess, tf_checkpoint)\n\n        # Transfer weights from tensorflow to pytorch\n        i3nception_pt.eval()\n        i3nception_pt.load_tf_weights(sess)\n\n        # Save pytorch weights for future loading\n        i3nception_state_dict = i3nception_pt.cpu().state_dict()\n        torch.save(i3nception_state_dict, pt_checkpoint)\n\n        # Load data\n        for i, (input_2d, target) in enumerate(loader):\n            input_2d = torch.from_numpy(input_2d.numpy())\n            if modality == \'flow\':\n                input_2d = input_2d[:, 0:2]  # Remove one dimension\n\n            # Prepare data for pytorch forward pass\n            target_var = torch.autograd.Variable(target)\n            input_3d = input_2d.clone().unsqueeze(2).repeat(\n                1, 1, frame_nb, 1, 1)\n            input_3d_var = torch.autograd.Variable(input_3d)\n\n            # Prepare data for tensorflow pass\n            feed_dict = {}\n            input_3d_tf = input_3d.numpy().transpose(0, 2, 3, 4, 1)\n            feed_dict[rgb_input] = input_3d_tf\n\n            # Tensorflow forward pass\n            tf_out3dsample = sess.run(rgb_logits, feed_dict=feed_dict)\n            out_tf_np = tf_out3dsample\n\n            if intermediate_feature:\n                # Reshape intermediary input to insure they are comparable\n                out_tf_np = tf_out3dsample.transpose((0, 4, 1, 2, 3))\n\n            # Pytorch forward pass\n            out_pt, _ = i3nception_pt(input_3d_var)\n            out_pt_np = out_pt.data.numpy()\n\n            # Make sure the tensorflow and pytorch outputs have the same shape\n            assert out_tf_np.shape == out_pt_np.shape, \'tf output: {} != pt output : {}\'.format(\n                out_tf_np.shape, out_pt_np.shape)\n            compare_outputs(out_tf_np, out_pt_np)\n\n            # Display slices of filter map for intermediate features\n            # for visual comparison\n            if intermediate_feature:\n                filter_idx = 219\n                img_tf = out_tf_np[0][filter_idx][0]\n                img_pt = out_pt_np[0][filter_idx][0]\n\n                max_v = max(img_tf.max(), img_pt.max())\n                min_v = min(img_tf.min(), img_pt.min())\n                plt.subplot(2, 2, 1)\n                plt.imshow(img_pt, vmax=max_v, vmin=min_v)\n                plt.subplot(2, 2, 2)\n                plt.imshow(img_tf, vmax=max_v, vmin=min_v)\n                plt.subplot(2, 2, 3)\n                plt.imshow(img_tf - img_pt)\n                plt.show()\n                print(\'min val : {}, max_val : {}, mean val : {}\'.format(\n                    min_v, max_v, out_pt_np.mean()))\n            loss = criterion(out_pt, torch.ones_like(out_pt))\n            loss.backward()\n\n\nif __name__ == ""__main__"":\n    parser = argparse.ArgumentParser(\n        \'Transfers the kinetics rgb pretrained i3d\\\n    inception v1 weights from tensorflow to pytorch and saves the weights as\\\n    as state_dict\')\n    parser.add_argument(\n        \'--rgb\', action=\'store_true\', help=\'Convert RGB pretrained network\')\n    parser.add_argument(\n        \'--rgb_tf_checkpoint\',\n        type=str,\n        default=\'model/tf_rgb_imagenet/model.ckpt\',\n        help=\'Path to tensorflow weight checkpoint trained on rgb\')\n    parser.add_argument(\n        \'--rgb_pt_checkpoint\',\n        type=str,\n        default=\'model/model_rgb.pth\',\n        help=\'Path for pytorch state_dict saving\')\n    parser.add_argument(\n        \'--flow\', action=\'store_true\', help=\'Convert Flow pretrained network\')\n    parser.add_argument(\n        \'--flow_tf_checkpoint\',\n        type=str,\n        default=\'model/tf_flow_imagenet/model.ckpt\',\n        help=\'Path to tensorflow weight checkpoint trained on flow\')\n    parser.add_argument(\n        \'--flow_pt_checkpoint\',\n        type=str,\n        default=\'model/model_flow.pth\',\n        help=\'Path for pytorch state_dict saving\')\n    parser.add_argument(\n        \'--batch_size\',\n        type=int,\n        default=\'2\',\n        help=\'Batch size for comparison between tensorflow and pytorch outputs\'\n    )\n    args = parser.parse_args()\n\n    if args.rgb:\n        transfer_weights(\n            args.rgb_tf_checkpoint,\n            args.rgb_pt_checkpoint,\n            batch_size=args.batch_size,\n            modality=\'rgb\')\n    if args.flow:\n        transfer_weights(\n            args.flow_tf_checkpoint,\n            args.flow_pt_checkpoint,\n            batch_size=args.batch_size,\n            modality=\'flow\')\n'"
src/__init__.py,0,b''
src/i3dpt.py,27,"b'import math\nimport os\n\nimport numpy as np\nimport torch\nfrom torch.nn import ReplicationPad3d\n\n\ndef get_padding_shape(filter_shape, stride):\n    def _pad_top_bottom(filter_dim, stride_val):\n        pad_along = max(filter_dim - stride_val, 0)\n        pad_top = pad_along // 2\n        pad_bottom = pad_along - pad_top\n        return pad_top, pad_bottom\n\n    padding_shape = []\n    for filter_dim, stride_val in zip(filter_shape, stride):\n        pad_top, pad_bottom = _pad_top_bottom(filter_dim, stride_val)\n        padding_shape.append(pad_top)\n        padding_shape.append(pad_bottom)\n    depth_top = padding_shape.pop(0)\n    depth_bottom = padding_shape.pop(0)\n    padding_shape.append(depth_top)\n    padding_shape.append(depth_bottom)\n\n    return tuple(padding_shape)\n\n\ndef simplify_padding(padding_shapes):\n    all_same = True\n    padding_init = padding_shapes[0]\n    for pad in padding_shapes[1:]:\n        if pad != padding_init:\n            all_same = False\n    return all_same, padding_init\n\n\nclass Unit3Dpy(torch.nn.Module):\n    def __init__(self,\n                 in_channels,\n                 out_channels,\n                 kernel_size=(1, 1, 1),\n                 stride=(1, 1, 1),\n                 activation=\'relu\',\n                 padding=\'SAME\',\n                 use_bias=False,\n                 use_bn=True):\n        super(Unit3Dpy, self).__init__()\n\n        self.padding = padding\n        self.activation = activation\n        self.use_bn = use_bn\n        if padding == \'SAME\':\n            padding_shape = get_padding_shape(kernel_size, stride)\n            simplify_pad, pad_size = simplify_padding(padding_shape)\n            self.simplify_pad = simplify_pad\n        elif padding == \'VALID\':\n            padding_shape = 0\n        else:\n            raise ValueError(\n                \'padding should be in [VALID|SAME] but got {}\'.format(padding))\n\n        if padding == \'SAME\':\n            if not simplify_pad:\n                self.pad = torch.nn.ConstantPad3d(padding_shape, 0)\n                self.conv3d = torch.nn.Conv3d(\n                    in_channels,\n                    out_channels,\n                    kernel_size,\n                    stride=stride,\n                    bias=use_bias)\n            else:\n                self.conv3d = torch.nn.Conv3d(\n                    in_channels,\n                    out_channels,\n                    kernel_size,\n                    stride=stride,\n                    padding=pad_size,\n                    bias=use_bias)\n        elif padding == \'VALID\':\n            self.conv3d = torch.nn.Conv3d(\n                in_channels,\n                out_channels,\n                kernel_size,\n                padding=padding_shape,\n                stride=stride,\n                bias=use_bias)\n        else:\n            raise ValueError(\n                \'padding should be in [VALID|SAME] but got {}\'.format(padding))\n\n        if self.use_bn:\n            self.batch3d = torch.nn.BatchNorm3d(out_channels)\n\n        if activation == \'relu\':\n            self.activation = torch.nn.functional.relu\n\n    def forward(self, inp):\n        if self.padding == \'SAME\' and self.simplify_pad is False:\n            inp = self.pad(inp)\n        out = self.conv3d(inp)\n        if self.use_bn:\n            out = self.batch3d(out)\n        if self.activation is not None:\n            out = torch.nn.functional.relu(out)\n        return out\n\n\nclass MaxPool3dTFPadding(torch.nn.Module):\n    def __init__(self, kernel_size, stride=None, padding=\'SAME\'):\n        super(MaxPool3dTFPadding, self).__init__()\n        if padding == \'SAME\':\n            padding_shape = get_padding_shape(kernel_size, stride)\n            self.padding_shape = padding_shape\n            self.pad = torch.nn.ConstantPad3d(padding_shape, 0)\n        self.pool = torch.nn.MaxPool3d(kernel_size, stride, ceil_mode=True)\n\n    def forward(self, inp):\n        inp = self.pad(inp)\n        out = self.pool(inp)\n        return out\n\n\nclass Mixed(torch.nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super(Mixed, self).__init__()\n        # Branch 0\n        self.branch_0 = Unit3Dpy(\n            in_channels, out_channels[0], kernel_size=(1, 1, 1))\n\n        # Branch 1\n        branch_1_conv1 = Unit3Dpy(\n            in_channels, out_channels[1], kernel_size=(1, 1, 1))\n        branch_1_conv2 = Unit3Dpy(\n            out_channels[1], out_channels[2], kernel_size=(3, 3, 3))\n        self.branch_1 = torch.nn.Sequential(branch_1_conv1, branch_1_conv2)\n\n        # Branch 2\n        branch_2_conv1 = Unit3Dpy(\n            in_channels, out_channels[3], kernel_size=(1, 1, 1))\n        branch_2_conv2 = Unit3Dpy(\n            out_channels[3], out_channels[4], kernel_size=(3, 3, 3))\n        self.branch_2 = torch.nn.Sequential(branch_2_conv1, branch_2_conv2)\n\n        # Branch3\n        branch_3_pool = MaxPool3dTFPadding(\n            kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=\'SAME\')\n        branch_3_conv2 = Unit3Dpy(\n            in_channels, out_channels[5], kernel_size=(1, 1, 1))\n        self.branch_3 = torch.nn.Sequential(branch_3_pool, branch_3_conv2)\n\n    def forward(self, inp):\n        out_0 = self.branch_0(inp)\n        out_1 = self.branch_1(inp)\n        out_2 = self.branch_2(inp)\n        out_3 = self.branch_3(inp)\n        out = torch.cat((out_0, out_1, out_2, out_3), 1)\n        return out\n\n\nclass I3D(torch.nn.Module):\n    def __init__(self,\n                 num_classes,\n                 modality=\'rgb\',\n                 dropout_prob=0,\n                 name=\'inception\'):\n        super(I3D, self).__init__()\n\n        self.name = name\n        self.num_classes = num_classes\n        if modality == \'rgb\':\n            in_channels = 3\n        elif modality == \'flow\':\n            in_channels = 2\n        else:\n            raise ValueError(\n                \'{} not among known modalities [rgb|flow]\'.format(modality))\n        self.modality = modality\n\n        conv3d_1a_7x7 = Unit3Dpy(\n            out_channels=64,\n            in_channels=in_channels,\n            kernel_size=(7, 7, 7),\n            stride=(2, 2, 2),\n            padding=\'SAME\')\n        # 1st conv-pool\n        self.conv3d_1a_7x7 = conv3d_1a_7x7\n        self.maxPool3d_2a_3x3 = MaxPool3dTFPadding(\n            kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=\'SAME\')\n        # conv conv\n        conv3d_2b_1x1 = Unit3Dpy(\n            out_channels=64,\n            in_channels=64,\n            kernel_size=(1, 1, 1),\n            padding=\'SAME\')\n        self.conv3d_2b_1x1 = conv3d_2b_1x1\n        conv3d_2c_3x3 = Unit3Dpy(\n            out_channels=192,\n            in_channels=64,\n            kernel_size=(3, 3, 3),\n            padding=\'SAME\')\n        self.conv3d_2c_3x3 = conv3d_2c_3x3\n        self.maxPool3d_3a_3x3 = MaxPool3dTFPadding(\n            kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=\'SAME\')\n\n        # Mixed_3b\n        self.mixed_3b = Mixed(192, [64, 96, 128, 16, 32, 32])\n        self.mixed_3c = Mixed(256, [128, 128, 192, 32, 96, 64])\n\n        self.maxPool3d_4a_3x3 = MaxPool3dTFPadding(\n            kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=\'SAME\')\n\n        # Mixed 4\n        self.mixed_4b = Mixed(480, [192, 96, 208, 16, 48, 64])\n        self.mixed_4c = Mixed(512, [160, 112, 224, 24, 64, 64])\n        self.mixed_4d = Mixed(512, [128, 128, 256, 24, 64, 64])\n        self.mixed_4e = Mixed(512, [112, 144, 288, 32, 64, 64])\n        self.mixed_4f = Mixed(528, [256, 160, 320, 32, 128, 128])\n\n        self.maxPool3d_5a_2x2 = MaxPool3dTFPadding(\n            kernel_size=(2, 2, 2), stride=(2, 2, 2), padding=\'SAME\')\n\n        # Mixed 5\n        self.mixed_5b = Mixed(832, [256, 160, 320, 32, 128, 128])\n        self.mixed_5c = Mixed(832, [384, 192, 384, 48, 128, 128])\n\n        self.avg_pool = torch.nn.AvgPool3d((2, 7, 7), (1, 1, 1))\n        self.dropout = torch.nn.Dropout(dropout_prob)\n        self.conv3d_0c_1x1 = Unit3Dpy(\n            in_channels=1024,\n            out_channels=self.num_classes,\n            kernel_size=(1, 1, 1),\n            activation=None,\n            use_bias=True,\n            use_bn=False)\n        self.softmax = torch.nn.Softmax(1)\n\n    def forward(self, inp):\n        # Preprocessing\n        out = self.conv3d_1a_7x7(inp)\n        out = self.maxPool3d_2a_3x3(out)\n        out = self.conv3d_2b_1x1(out)\n        out = self.conv3d_2c_3x3(out)\n        out = self.maxPool3d_3a_3x3(out)\n        out = self.mixed_3b(out)\n        out = self.mixed_3c(out)\n        out = self.maxPool3d_4a_3x3(out)\n        out = self.mixed_4b(out)\n        out = self.mixed_4c(out)\n        out = self.mixed_4d(out)\n        out = self.mixed_4e(out)\n        out = self.mixed_4f(out)\n        out = self.maxPool3d_5a_2x2(out)\n        out = self.mixed_5b(out)\n        out = self.mixed_5c(out)\n        out = self.avg_pool(out)\n        out = self.dropout(out)\n        out = self.conv3d_0c_1x1(out)\n        out = out.squeeze(3)\n        out = out.squeeze(3)\n        out = out.mean(2)\n        out_logits = out\n        out = self.softmax(out_logits)\n        return out, out_logits\n\n    def load_tf_weights(self, sess):\n        state_dict = {}\n        if self.modality == \'rgb\':\n            prefix = \'RGB/inception_i3d\'\n        elif self.modality == \'flow\':\n            prefix = \'Flow/inception_i3d\'\n        load_conv3d(state_dict, \'conv3d_1a_7x7\', sess,\n                    os.path.join(prefix, \'Conv3d_1a_7x7\'))\n        load_conv3d(state_dict, \'conv3d_2b_1x1\', sess,\n                    os.path.join(prefix, \'Conv3d_2b_1x1\'))\n        load_conv3d(state_dict, \'conv3d_2c_3x3\', sess,\n                    os.path.join(prefix, \'Conv3d_2c_3x3\'))\n\n        load_mixed(state_dict, \'mixed_3b\', sess,\n                   os.path.join(prefix, \'Mixed_3b\'))\n        load_mixed(state_dict, \'mixed_3c\', sess,\n                   os.path.join(prefix, \'Mixed_3c\'))\n        load_mixed(state_dict, \'mixed_4b\', sess,\n                   os.path.join(prefix, \'Mixed_4b\'))\n        load_mixed(state_dict, \'mixed_4c\', sess,\n                   os.path.join(prefix, \'Mixed_4c\'))\n        load_mixed(state_dict, \'mixed_4d\', sess,\n                   os.path.join(prefix, \'Mixed_4d\'))\n        load_mixed(state_dict, \'mixed_4e\', sess,\n                   os.path.join(prefix, \'Mixed_4e\'))\n        # Here goest to 0.1 max error with tf\n        load_mixed(state_dict, \'mixed_4f\', sess,\n                   os.path.join(prefix, \'Mixed_4f\'))\n\n        load_mixed(\n            state_dict,\n            \'mixed_5b\',\n            sess,\n            os.path.join(prefix, \'Mixed_5b\'),\n            fix_typo=True)\n        load_mixed(state_dict, \'mixed_5c\', sess,\n                   os.path.join(prefix, \'Mixed_5c\'))\n        load_conv3d(\n            state_dict,\n            \'conv3d_0c_1x1\',\n            sess,\n            os.path.join(prefix, \'Logits\', \'Conv3d_0c_1x1\'),\n            bias=True,\n            bn=False)\n        self.load_state_dict(state_dict)\n\n\ndef get_conv_params(sess, name, bias=False):\n    # Get conv weights\n    conv_weights_tensor = sess.graph.get_tensor_by_name(\n        os.path.join(name, \'w:0\'))\n    if bias:\n        conv_bias_tensor = sess.graph.get_tensor_by_name(\n            os.path.join(name, \'b:0\'))\n        conv_bias = sess.run(conv_bias_tensor)\n    conv_weights = sess.run(conv_weights_tensor)\n    conv_shape = conv_weights.shape\n\n    kernel_shape = conv_shape[0:3]\n    in_channels = conv_shape[3]\n    out_channels = conv_shape[4]\n\n    conv_op = sess.graph.get_operation_by_name(\n        os.path.join(name, \'convolution\'))\n    padding_name = conv_op.get_attr(\'padding\')\n    padding = _get_padding(padding_name, kernel_shape)\n    all_strides = conv_op.get_attr(\'strides\')\n    strides = all_strides[1:4]\n    conv_params = [\n        conv_weights, kernel_shape, in_channels, out_channels, strides, padding\n    ]\n    if bias:\n        conv_params.append(conv_bias)\n    return conv_params\n\n\ndef get_bn_params(sess, name):\n    moving_mean_tensor = sess.graph.get_tensor_by_name(\n        os.path.join(name, \'moving_mean:0\'))\n    moving_var_tensor = sess.graph.get_tensor_by_name(\n        os.path.join(name, \'moving_variance:0\'))\n    beta_tensor = sess.graph.get_tensor_by_name(os.path.join(name, \'beta:0\'))\n    moving_mean = sess.run(moving_mean_tensor)\n    moving_var = sess.run(moving_var_tensor)\n    beta = sess.run(beta_tensor)\n    return moving_mean, moving_var, beta\n\n\ndef _get_padding(padding_name, conv_shape):\n    padding_name = padding_name.decode(""utf-8"")\n    if padding_name == ""VALID"":\n        return [0, 0]\n    elif padding_name == ""SAME"":\n        # return [math.ceil(int(conv_shape[0])/2), math.ceil(int(conv_shape[1])/2)]\n        return [\n            math.floor(int(conv_shape[0]) / 2),\n            math.floor(int(conv_shape[1]) / 2),\n            math.floor(int(conv_shape[2]) / 2)\n        ]\n    else:\n        raise ValueError(\'Invalid padding name \' + padding_name)\n\n\ndef load_conv3d(state_dict, name_pt, sess, name_tf, bias=False, bn=True):\n    # Transfer convolution params\n    conv_name_tf = os.path.join(name_tf, \'conv_3d\')\n    conv_params = get_conv_params(sess, conv_name_tf, bias=bias)\n    if bias:\n        conv_weights, kernel_shape, in_channels, out_channels, strides, padding, conv_bias = conv_params\n    else:\n        conv_weights, kernel_shape, in_channels, out_channels, strides, padding = conv_params\n\n    conv_weights_rs = np.transpose(\n        conv_weights, (4, 3, 0, 1,\n                       2))  # to pt format (out_c, in_c, depth, height, width)\n    state_dict[name_pt + \'.conv3d.weight\'] = torch.from_numpy(conv_weights_rs)\n    if bias:\n        state_dict[name_pt + \'.conv3d.bias\'] = torch.from_numpy(conv_bias)\n\n    # Transfer batch norm params\n    if bn:\n        conv_tf_name = os.path.join(name_tf, \'batch_norm\')\n        moving_mean, moving_var, beta = get_bn_params(sess, conv_tf_name)\n\n        out_planes = conv_weights_rs.shape[0]\n        state_dict[name_pt + \'.batch3d.weight\'] = torch.ones(out_planes)\n        state_dict[name_pt +\n                   \'.batch3d.bias\'] = torch.from_numpy(beta.squeeze())\n        state_dict[name_pt\n                   + \'.batch3d.running_mean\'] = torch.from_numpy(moving_mean.squeeze())\n        state_dict[name_pt\n                   + \'.batch3d.running_var\'] = torch.from_numpy(moving_var.squeeze())\n\n\ndef load_mixed(state_dict, name_pt, sess, name_tf, fix_typo=False):\n    # Branch 0\n    load_conv3d(state_dict, name_pt + \'.branch_0\', sess,\n                os.path.join(name_tf, \'Branch_0/Conv3d_0a_1x1\'))\n\n    # Branch .1\n    load_conv3d(state_dict, name_pt + \'.branch_1.0\', sess,\n                os.path.join(name_tf, \'Branch_1/Conv3d_0a_1x1\'))\n    load_conv3d(state_dict, name_pt + \'.branch_1.1\', sess,\n                os.path.join(name_tf, \'Branch_1/Conv3d_0b_3x3\'))\n\n    # Branch 2\n    load_conv3d(state_dict, name_pt + \'.branch_2.0\', sess,\n                os.path.join(name_tf, \'Branch_2/Conv3d_0a_1x1\'))\n    if fix_typo:\n        load_conv3d(state_dict, name_pt + \'.branch_2.1\', sess,\n                    os.path.join(name_tf, \'Branch_2/Conv3d_0a_3x3\'))\n    else:\n        load_conv3d(state_dict, name_pt + \'.branch_2.1\', sess,\n                    os.path.join(name_tf, \'Branch_2/Conv3d_0b_3x3\'))\n\n    # Branch 3\n    load_conv3d(state_dict, name_pt + \'.branch_3.1\', sess,\n                os.path.join(name_tf, \'Branch_3/Conv3d_0b_1x1\'))\n'"
src/i3dtf.py,0,"b'# Copyright 2017 Google Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ============================================================================\n""""""Inception-v1 Inflated 3D ConvNet used for Kinetics CVPR paper.\n\nThe model is introduced in:\n\nQuo Vadis, Action Recognition? A New Model and the Kinetics Dataset\nJoao Carreira, Andrew Zisserman\nhttps://arxiv.org/pdf/1705.07750v1.pdf.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport sonnet as snt\nimport tensorflow as tf\n\n\nclass Unit3Dtf(snt.AbstractModule):\n    """"""Basic unit containing Conv3D + BatchNorm + non-linearity.""""""\n\n    def __init__(self,\n                 output_channels,\n                 kernel_shape=(1, 1, 1),\n                 stride=(1, 1, 1),\n                 activation_fn=tf.nn.relu,\n                 use_batch_norm=True,\n                 use_bias=False,\n                 name=\'unit_3d\'):\n        """"""Initializes Unit3Dtf module.""""""\n        super(Unit3Dtf, self).__init__(name=name)\n        self._output_channels = output_channels\n        self._kernel_shape = kernel_shape\n        self._stride = stride\n        self._use_batch_norm = use_batch_norm\n        self._activation_fn = activation_fn\n        self._use_bias = use_bias\n\n    def _build(self, inputs, is_training):\n        """"""Connects the module to inputs.\n\n    Args:\n    inputs: Inputs to the Unit3Dtf component.\n    is_training: whether to use training mode for snt.BatchNorm (boolean).\n\n    Returns:\n    Outputs from the module.\n        """"""\n        net = snt.Conv3D(\n            output_channels=self._output_channels,\n            kernel_shape=self._kernel_shape,\n            stride=self._stride,\n            padding=snt.SAME,\n            use_bias=self._use_bias)(inputs)\n        if self._use_batch_norm:\n            bn = snt.BatchNorm()\n            net = bn(net, is_training=is_training, test_local_stats=False)\n        if self._activation_fn is not None:\n            net = self._activation_fn(net)\n\n        return net\n\n\nclass InceptionI3d(snt.AbstractModule):\n    """"""Inception-v1 I3D architecture.\n\n    The model is introduced in:\n\n    Quo Vadis, Action Recognition? A New Model and the Kinetics Dataset\n    Joao Carreira, Andrew Zisserman\n    https://arxiv.org/pdf/1705.07750v1.pdf.\n\n    See also the Inception architecture, introduced in:\n\n    Going deeper with convolutions\n    Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed,\n    Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, Andrew Rabinovich.\n    http://arxiv.org/pdf/1409.4842v1.pdf.\n    """"""\n\n    # Endpoints of the model in order. During construction, all the endpoints up\n    # to a designated `final_endpoint` are returned in a dictionary as the\n    # second return value.\n    VALID_ENDPOINTS = (\n        \'Conv3d_1a_7x7\',\n        \'MaxPool3d_2a_3x3\',\n        \'Conv3d_2b_1x1\',\n        \'Conv3d_2c_3x3\',\n        \'MaxPool3d_3a_3x3\',\n        \'Mixed_3b\',\n        \'Mixed_3c\',\n        \'MaxPool3d_4a_3x3\',\n        \'Mixed_4b\',\n        \'Mixed_4c\',\n        \'Mixed_4d\',\n        \'Mixed_4e\',\n        \'Mixed_4f\',\n        \'MaxPool3d_5a_2x2\',\n        \'Mixed_5b\',\n        \'Mixed_5c\',\n        \'Logits\',\n        \'Predictions\', )\n\n    def __init__(self,\n                 num_classes=400,\n                 spatial_squeeze=True,\n                 final_endpoint=\'Logits\',\n                 name=\'inception_i3d\'):\n        """"""Initializes I3D model instance.\n\n    Args:\n    num_classes: The number of outputs in the logit layer (default 400, which\n    matches the Kinetics dataset).\n    spatial_squeeze: Whether to squeeze the spatial dimensions for the logits\n    before returning (default True).\n    final_endpoint: The model contains many possible endpoints.\n    `final_endpoint` specifies the last endpoint for the model to be built\n    up to. In addition to the output at `final_endpoint`, all the outputs\n    at endpoints up to `final_endpoint` will also be returned, in a\n    dictionary. `final_endpoint` must be one of\n    InceptionI3d.VALID_ENDPOINTS (default \'Logits\').\n    name: A string (optional). The name of this module.\n\n    Raises:\n    ValueError: if `final_endpoint` is not recognized.\n        """"""\n\n        if final_endpoint not in self.VALID_ENDPOINTS:\n            raise ValueError(\'Unknown final endpoint %s\' % final_endpoint)\n\n        super(InceptionI3d, self).__init__(name=name)\n        self._num_classes = num_classes\n        self._spatial_squeeze = spatial_squeeze\n        self._final_endpoint = final_endpoint\n\n    def _build(self, inputs, is_training, dropout_keep_prob=1.0):\n        """"""Connects the model to inputs.\n\n    Args:\n    inputs: Inputs to the model, which should have dimensions\n    `batch_size` x `num_frames` x 224 x 224 x `num_channels`.\n    is_training: whether to use training mode for snt.BatchNorm (boolean).\n    dropout_keep_prob: Probability for the tf.nn.dropout layer (float in\n    [0, 1)).\n\n    Returns:\n    A tuple consisting of:\n    1. Network output at location `self._final_endpoint`.\n    2. Dictionary containing all endpoints up to `self._final_endpoint`,\n    indexed by endpoint name.\n\n    Raises:\n    ValueError: if `self._final_endpoint` is not recognized.\n        """"""\n        if self._final_endpoint not in self.VALID_ENDPOINTS:\n            raise ValueError(\n                \'Unknown final endpoint %s\' % self._final_endpoint)\n\n        net = inputs\n        end_points = {}\n        end_point = \'Conv3d_1a_7x7\'\n        net = Unit3Dtf(\n            output_channels=64,\n            kernel_shape=[7, 7, 7],\n            stride=[2, 2, 2],\n            name=end_point)(\n                net, is_training=is_training)\n        end_points[end_point] = net\n        if self._final_endpoint == end_point:\n            return net, end_points\n        end_point = \'MaxPool3d_2a_3x3\'\n        net = tf.nn.max_pool3d(\n            net,\n            ksize=[1, 1, 3, 3, 1],\n            strides=[1, 1, 2, 2, 1],\n            padding=snt.SAME,\n            name=end_point)\n        end_points[end_point] = net\n        if self._final_endpoint == end_point:\n            return net, end_points\n        end_point = \'Conv3d_2b_1x1\'\n        net = Unit3Dtf(\n            output_channels=64, kernel_shape=[1, 1, 1], name=end_point)(\n                net, is_training=is_training)\n        end_points[end_point] = net\n        if self._final_endpoint == end_point:\n            return net, end_points\n        end_point = \'Conv3d_2c_3x3\'\n        net = Unit3Dtf(\n            output_channels=192, kernel_shape=[3, 3, 3], name=end_point)(\n                net, is_training=is_training)\n        end_points[end_point] = net\n        if self._final_endpoint == end_point: return net, end_points\n        end_point = \'MaxPool3d_3a_3x3\'\n        net = tf.nn.max_pool3d(\n            net,\n            ksize=[1, 1, 3, 3, 1],\n            strides=[1, 1, 2, 2, 1],\n            padding=snt.SAME,\n            name=end_point)\n        end_points[end_point] = net\n        if self._final_endpoint == end_point:\n            return net, end_points\n        end_point = \'Mixed_3b\'\n        with tf.variable_scope(end_point):\n            with tf.variable_scope(\'Branch_0\'):\n                branch_0 = Unit3Dtf(\n                    output_channels=64,\n                    kernel_shape=[1, 1, 1],\n                    name=\'Conv3d_0a_1x1\')(\n                        net, is_training=is_training)\n            with tf.variable_scope(\'Branch_1\'):\n                branch_1 = Unit3Dtf(\n                    output_channels=96,\n                    kernel_shape=[1, 1, 1],\n                    name=\'Conv3d_0a_1x1\')(\n                        net, is_training=is_training)\n                branch_1 = Unit3Dtf(\n                    output_channels=128,\n                    kernel_shape=[3, 3, 3],\n                    name=\'Conv3d_0b_3x3\')(\n                        branch_1, is_training=is_training)\n            with tf.variable_scope(\'Branch_2\'):\n                branch_2 = Unit3Dtf(\n                    output_channels=16,\n                    kernel_shape=[1, 1, 1],\n                    name=\'Conv3d_0a_1x1\')(\n                        net, is_training=is_training)\n                branch_2 = Unit3Dtf(\n                    output_channels=32,\n                    kernel_shape=[3, 3, 3],\n                    name=\'Conv3d_0b_3x3\')(\n                        branch_2, is_training=is_training)\n            with tf.variable_scope(\'Branch_3\'):\n                branch_3 = tf.nn.max_pool3d(\n                    net,\n                    ksize=[1, 3, 3, 3, 1],\n                    strides=[1, 1, 1, 1, 1],\n                    padding=snt.SAME,\n                    name=\'MaxPool3d_0a_3x3\')\n                branch_3 = Unit3Dtf(\n                    output_channels=32,\n                    kernel_shape=[1, 1, 1],\n                    name=\'Conv3d_0b_1x1\')(\n                        branch_3, is_training=is_training)\n\n            net = tf.concat([branch_0, branch_1, branch_2, branch_3], 4)\n        end_points[end_point] = net\n        if self._final_endpoint == end_point:\n            return net, end_points\n\n        end_point = \'Mixed_3c\'\n        with tf.variable_scope(end_point):\n            with tf.variable_scope(\'Branch_0\'):\n                branch_0 = Unit3Dtf(\n                    output_channels=128,\n                    kernel_shape=[1, 1, 1],\n                    name=\'Conv3d_0a_1x1\')(\n                        net, is_training=is_training)\n            with tf.variable_scope(\'Branch_1\'):\n                branch_1 = Unit3Dtf(\n                    output_channels=128,\n                    kernel_shape=[1, 1, 1],\n                    name=\'Conv3d_0a_1x1\')(\n                        net, is_training=is_training)\n                branch_1 = Unit3Dtf(\n                    output_channels=192,\n                    kernel_shape=[3, 3, 3],\n                    name=\'Conv3d_0b_3x3\')(\n                        branch_1, is_training=is_training)\n            with tf.variable_scope(\'Branch_2\'):\n                branch_2 = Unit3Dtf(\n                    output_channels=32,\n                    kernel_shape=[1, 1, 1],\n                    name=\'Conv3d_0a_1x1\')(\n                        net, is_training=is_training)\n                branch_2 = Unit3Dtf(\n                    output_channels=96,\n                    kernel_shape=[3, 3, 3],\n                    name=\'Conv3d_0b_3x3\')(\n                        branch_2, is_training=is_training)\n            with tf.variable_scope(\'Branch_3\'):\n                branch_3 = tf.nn.max_pool3d(\n                    net,\n                    ksize=[1, 3, 3, 3, 1],\n                    strides=[1, 1, 1, 1, 1],\n                    padding=snt.SAME,\n                    name=\'MaxPool3d_0a_3x3\')\n                branch_3 = Unit3Dtf(\n                    output_channels=64,\n                    kernel_shape=[1, 1, 1],\n                    name=\'Conv3d_0b_1x1\')(\n                        branch_3, is_training=is_training)\n            net = tf.concat([branch_0, branch_1, branch_2, branch_3], 4)\n        end_points[end_point] = net\n        if self._final_endpoint == end_point: return net, end_points\n\n        end_point = \'MaxPool3d_4a_3x3\'\n        net = tf.nn.max_pool3d(\n            net,\n            ksize=[1, 3, 3, 3, 1],\n            strides=[1, 2, 2, 2, 1],\n            padding=snt.SAME,\n            name=end_point)\n        end_points[end_point] = net\n        if self._final_endpoint == end_point: return net, end_points\n\n        end_point = \'Mixed_4b\'\n        with tf.variable_scope(end_point):\n            with tf.variable_scope(\'Branch_0\'):\n                branch_0 = Unit3Dtf(\n                    output_channels=192,\n                    kernel_shape=[1, 1, 1],\n                    name=\'Conv3d_0a_1x1\')(\n                        net, is_training=is_training)\n            with tf.variable_scope(\'Branch_1\'):\n                branch_1 = Unit3Dtf(\n                    output_channels=96,\n                    kernel_shape=[1, 1, 1],\n                    name=\'Conv3d_0a_1x1\')(\n                        net, is_training=is_training)\n                branch_1 = Unit3Dtf(\n                    output_channels=208,\n                    kernel_shape=[3, 3, 3],\n                    name=\'Conv3d_0b_3x3\')(\n                        branch_1, is_training=is_training)\n            with tf.variable_scope(\'Branch_2\'):\n                branch_2 = Unit3Dtf(\n                    output_channels=16,\n                    kernel_shape=[1, 1, 1],\n                    name=\'Conv3d_0a_1x1\')(\n                        net, is_training=is_training)\n                branch_2 = Unit3Dtf(\n                    output_channels=48,\n                    kernel_shape=[3, 3, 3],\n                    name=\'Conv3d_0b_3x3\')(\n                        branch_2, is_training=is_training)\n            with tf.variable_scope(\'Branch_3\'):\n                branch_3 = tf.nn.max_pool3d(\n                    net,\n                    ksize=[1, 3, 3, 3, 1],\n                    strides=[1, 1, 1, 1, 1],\n                    padding=snt.SAME,\n                    name=\'MaxPool3d_0a_3x3\')\n                branch_3 = Unit3Dtf(\n                    output_channels=64,\n                    kernel_shape=[1, 1, 1],\n                    name=\'Conv3d_0b_1x1\')(\n                        branch_3, is_training=is_training)\n            net = tf.concat([branch_0, branch_1, branch_2, branch_3], 4)\n        end_points[end_point] = net\n        if self._final_endpoint == end_point: return net, end_points\n\n        end_point = \'Mixed_4c\'\n        with tf.variable_scope(end_point):\n            with tf.variable_scope(\'Branch_0\'):\n                branch_0 = Unit3Dtf(\n                    output_channels=160,\n                    kernel_shape=[1, 1, 1],\n                    name=\'Conv3d_0a_1x1\')(\n                        net, is_training=is_training)\n            with tf.variable_scope(\'Branch_1\'):\n                branch_1 = Unit3Dtf(\n                    output_channels=112,\n                    kernel_shape=[1, 1, 1],\n                    name=\'Conv3d_0a_1x1\')(\n                        net, is_training=is_training)\n                branch_1 = Unit3Dtf(\n                    output_channels=224,\n                    kernel_shape=[3, 3, 3],\n                    name=\'Conv3d_0b_3x3\')(\n                        branch_1, is_training=is_training)\n            with tf.variable_scope(\'Branch_2\'):\n                branch_2 = Unit3Dtf(\n                    output_channels=24,\n                    kernel_shape=[1, 1, 1],\n                    name=\'Conv3d_0a_1x1\')(\n                        net, is_training=is_training)\n                branch_2 = Unit3Dtf(\n                    output_channels=64,\n                    kernel_shape=[3, 3, 3],\n                    name=\'Conv3d_0b_3x3\')(\n                        branch_2, is_training=is_training)\n            with tf.variable_scope(\'Branch_3\'):\n                branch_3 = tf.nn.max_pool3d(\n                    net,\n                    ksize=[1, 3, 3, 3, 1],\n                    strides=[1, 1, 1, 1, 1],\n                    padding=snt.SAME,\n                    name=\'MaxPool3d_0a_3x3\')\n                branch_3 = Unit3Dtf(\n                    output_channels=64,\n                    kernel_shape=[1, 1, 1],\n                    name=\'Conv3d_0b_1x1\')(\n                        branch_3, is_training=is_training)\n            net = tf.concat([branch_0, branch_1, branch_2, branch_3], 4)\n        end_points[end_point] = net\n        if self._final_endpoint == end_point: return net, end_points\n\n        end_point = \'Mixed_4d\'\n        with tf.variable_scope(end_point):\n            with tf.variable_scope(\'Branch_0\'):\n                branch_0 = Unit3Dtf(\n                    output_channels=128,\n                    kernel_shape=[1, 1, 1],\n                    name=\'Conv3d_0a_1x1\')(\n                        net, is_training=is_training)\n            with tf.variable_scope(\'Branch_1\'):\n                branch_1 = Unit3Dtf(\n                    output_channels=128,\n                    kernel_shape=[1, 1, 1],\n                    name=\'Conv3d_0a_1x1\')(\n                        net, is_training=is_training)\n                branch_1 = Unit3Dtf(\n                    output_channels=256,\n                    kernel_shape=[3, 3, 3],\n                    name=\'Conv3d_0b_3x3\')(\n                        branch_1, is_training=is_training)\n            with tf.variable_scope(\'Branch_2\'):\n                branch_2 = Unit3Dtf(\n                    output_channels=24,\n                    kernel_shape=[1, 1, 1],\n                    name=\'Conv3d_0a_1x1\')(\n                        net, is_training=is_training)\n                branch_2 = Unit3Dtf(\n                    output_channels=64,\n                    kernel_shape=[3, 3, 3],\n                    name=\'Conv3d_0b_3x3\')(\n                        branch_2, is_training=is_training)\n            with tf.variable_scope(\'Branch_3\'):\n                branch_3 = tf.nn.max_pool3d(\n                    net,\n                    ksize=[1, 3, 3, 3, 1],\n                    strides=[1, 1, 1, 1, 1],\n                    padding=snt.SAME,\n                    name=\'MaxPool3d_0a_3x3\')\n                branch_3 = Unit3Dtf(\n                    output_channels=64,\n                    kernel_shape=[1, 1, 1],\n                    name=\'Conv3d_0b_1x1\')(\n                        branch_3, is_training=is_training)\n            net = tf.concat([branch_0, branch_1, branch_2, branch_3], 4)\n        end_points[end_point] = net\n        if self._final_endpoint == end_point: return net, end_points\n\n        end_point = \'Mixed_4e\'\n        with tf.variable_scope(end_point):\n            with tf.variable_scope(\'Branch_0\'):\n                branch_0 = Unit3Dtf(\n                    output_channels=112,\n                    kernel_shape=[1, 1, 1],\n                    name=\'Conv3d_0a_1x1\')(\n                        net, is_training=is_training)\n            with tf.variable_scope(\'Branch_1\'):\n                branch_1 = Unit3Dtf(\n                    output_channels=144,\n                    kernel_shape=[1, 1, 1],\n                    name=\'Conv3d_0a_1x1\')(\n                        net, is_training=is_training)\n                branch_1 = Unit3Dtf(\n                    output_channels=288,\n                    kernel_shape=[3, 3, 3],\n                    name=\'Conv3d_0b_3x3\')(\n                        branch_1, is_training=is_training)\n            with tf.variable_scope(\'Branch_2\'):\n                branch_2 = Unit3Dtf(\n                    output_channels=32,\n                    kernel_shape=[1, 1, 1],\n                    name=\'Conv3d_0a_1x1\')(\n                        net, is_training=is_training)\n                branch_2 = Unit3Dtf(\n                    output_channels=64,\n                    kernel_shape=[3, 3, 3],\n                    name=\'Conv3d_0b_3x3\')(\n                        branch_2, is_training=is_training)\n            with tf.variable_scope(\'Branch_3\'):\n                branch_3 = tf.nn.max_pool3d(\n                    net,\n                    ksize=[1, 3, 3, 3, 1],\n                    strides=[1, 1, 1, 1, 1],\n                    padding=snt.SAME,\n                    name=\'MaxPool3d_0a_3x3\')\n                branch_3 = Unit3Dtf(\n                    output_channels=64,\n                    kernel_shape=[1, 1, 1],\n                    name=\'Conv3d_0b_1x1\')(\n                        branch_3, is_training=is_training)\n            net = tf.concat([branch_0, branch_1, branch_2, branch_3], 4)\n        end_points[end_point] = net\n        if self._final_endpoint == end_point: return net, end_points\n\n        end_point = \'Mixed_4f\'\n        with tf.variable_scope(end_point):\n            with tf.variable_scope(\'Branch_0\'):\n                branch_0 = Unit3Dtf(\n                    output_channels=256,\n                    kernel_shape=[1, 1, 1],\n                    name=\'Conv3d_0a_1x1\')(\n                        net, is_training=is_training)\n            with tf.variable_scope(\'Branch_1\'):\n                branch_1 = Unit3Dtf(\n                    output_channels=160,\n                    kernel_shape=[1, 1, 1],\n                    name=\'Conv3d_0a_1x1\')(\n                        net, is_training=is_training)\n                branch_1 = Unit3Dtf(\n                    output_channels=320,\n                    kernel_shape=[3, 3, 3],\n                    name=\'Conv3d_0b_3x3\')(\n                        branch_1, is_training=is_training)\n            with tf.variable_scope(\'Branch_2\'):\n                branch_2 = Unit3Dtf(\n                    output_channels=32,\n                    kernel_shape=[1, 1, 1],\n                    name=\'Conv3d_0a_1x1\')(\n                        net, is_training=is_training)\n                branch_2 = Unit3Dtf(\n                    output_channels=128,\n                    kernel_shape=[3, 3, 3],\n                    name=\'Conv3d_0b_3x3\')(\n                        branch_2, is_training=is_training)\n            with tf.variable_scope(\'Branch_3\'):\n                branch_3 = tf.nn.max_pool3d(\n                    net,\n                    ksize=[1, 3, 3, 3, 1],\n                    strides=[1, 1, 1, 1, 1],\n                    padding=snt.SAME,\n                    name=\'MaxPool3d_0a_3x3\')\n                branch_3 = Unit3Dtf(\n                    output_channels=128,\n                    kernel_shape=[1, 1, 1],\n                    name=\'Conv3d_0b_1x1\')(\n                        branch_3, is_training=is_training)\n            net = tf.concat([branch_0, branch_1, branch_2, branch_3], 4)\n        end_points[end_point] = net\n        if self._final_endpoint == end_point: return net, end_points\n\n        end_point = \'MaxPool3d_5a_2x2\'\n        net = tf.nn.max_pool3d(\n            net,\n            ksize=[1, 2, 2, 2, 1],\n            strides=[1, 2, 2, 2, 1],\n            padding=snt.SAME,\n            name=end_point)\n        end_points[end_point] = net\n        if self._final_endpoint == end_point: return net, end_points\n\n        end_point = \'Mixed_5b\'\n        with tf.variable_scope(end_point):\n            with tf.variable_scope(\'Branch_0\'):\n                branch_0 = Unit3Dtf(\n                    output_channels=256,\n                    kernel_shape=[1, 1, 1],\n                    name=\'Conv3d_0a_1x1\')(\n                        net, is_training=is_training)\n            with tf.variable_scope(\'Branch_1\'):\n                branch_1 = Unit3Dtf(\n                    output_channels=160,\n                    kernel_shape=[1, 1, 1],\n                    name=\'Conv3d_0a_1x1\')(\n                        net, is_training=is_training)\n                branch_1 = Unit3Dtf(\n                    output_channels=320,\n                    kernel_shape=[3, 3, 3],\n                    name=\'Conv3d_0b_3x3\')(\n                        branch_1, is_training=is_training)\n            with tf.variable_scope(\'Branch_2\'):\n                branch_2 = Unit3Dtf(\n                    output_channels=32,\n                    kernel_shape=[1, 1, 1],\n                    name=\'Conv3d_0a_1x1\')(\n                        net, is_training=is_training)\n                branch_2 = Unit3Dtf(\n                    output_channels=128,\n                    kernel_shape=[3, 3, 3],\n                    name=\'Conv3d_0a_3x3\')(\n                        branch_2, is_training=is_training)\n            with tf.variable_scope(\'Branch_3\'):\n                branch_3 = tf.nn.max_pool3d(\n                    net,\n                    ksize=[1, 3, 3, 3, 1],\n                    strides=[1, 1, 1, 1, 1],\n                    padding=snt.SAME,\n                    name=\'MaxPool3d_0a_3x3\')\n                branch_3 = Unit3Dtf(\n                    output_channels=128,\n                    kernel_shape=[1, 1, 1],\n                    name=\'Conv3d_0b_1x1\')(\n                        branch_3, is_training=is_training)\n            net = tf.concat([branch_0, branch_1, branch_2, branch_3], 4)\n        end_points[end_point] = net\n        if self._final_endpoint == end_point: return net, end_points\n\n        end_point = \'Mixed_5c\'\n        with tf.variable_scope(end_point):\n            with tf.variable_scope(\'Branch_0\'):\n                branch_0 = Unit3Dtf(\n                    output_channels=384,\n                    kernel_shape=[1, 1, 1],\n                    name=\'Conv3d_0a_1x1\')(\n                        net, is_training=is_training)\n            with tf.variable_scope(\'Branch_1\'):\n                branch_1 = Unit3Dtf(\n                    output_channels=192,\n                    kernel_shape=[1, 1, 1],\n                    name=\'Conv3d_0a_1x1\')(\n                        net, is_training=is_training)\n                branch_1 = Unit3Dtf(\n                    output_channels=384,\n                    kernel_shape=[3, 3, 3],\n                    name=\'Conv3d_0b_3x3\')(\n                        branch_1, is_training=is_training)\n            with tf.variable_scope(\'Branch_2\'):\n                branch_2 = Unit3Dtf(\n                    output_channels=48,\n                    kernel_shape=[1, 1, 1],\n                    name=\'Conv3d_0a_1x1\')(\n                        net, is_training=is_training)\n                branch_2 = Unit3Dtf(\n                    output_channels=128,\n                    kernel_shape=[3, 3, 3],\n                    name=\'Conv3d_0b_3x3\')(\n                        branch_2, is_training=is_training)\n            with tf.variable_scope(\'Branch_3\'):\n                branch_3 = tf.nn.max_pool3d(\n                    net,\n                    ksize=[1, 3, 3, 3, 1],\n                    strides=[1, 1, 1, 1, 1],\n                    padding=snt.SAME,\n                    name=\'MaxPool3d_0a_3x3\')\n                branch_3 = Unit3Dtf(\n                    output_channels=128,\n                    kernel_shape=[1, 1, 1],\n                    name=\'Conv3d_0b_1x1\')(\n                        branch_3, is_training=is_training)\n            net = tf.concat([branch_0, branch_1, branch_2, branch_3], 4)\n        end_points[end_point] = net\n        if self._final_endpoint == end_point: return net, end_points\n\n        end_point = \'Logits\'\n        with tf.variable_scope(end_point):\n            net = tf.nn.avg_pool3d(\n                net,\n                ksize=[1, 2, 7, 7, 1],\n                strides=[1, 1, 1, 1, 1],\n                padding=snt.VALID)\n            net = tf.nn.dropout(net, dropout_keep_prob)\n            logits = Unit3Dtf(\n                output_channels=self._num_classes,\n                kernel_shape=[1, 1, 1],\n                activation_fn=None,\n                use_batch_norm=False,\n                use_bias=True,\n                name=\'Conv3d_0c_1x1\')(\n                    net, is_training=is_training)\n            if self._spatial_squeeze:\n                logits = tf.squeeze(logits, [2, 3], name=\'SpatialSqueeze\')\n        averaged_logits = tf.reduce_mean(logits, axis=1)\n        end_points[end_point] = averaged_logits\n        if self._final_endpoint == end_point:\n            return averaged_logits, end_points\n\n        end_point = \'Predictions\'\n        predictions = tf.nn.softmax(averaged_logits)\n        end_points[end_point] = predictions\n        return predictions, end_points\n'"
src/monitorutils.py,0,"b'import numpy as np\n\n\ndef compare_outputs(tf_out, py_out):\n    """"""\n    Display some stats about the difference between two numpy arrays\n    """"""\n\n    out_diff = np.abs(py_out - tf_out)\n    mean_diff = out_diff.mean()\n    max_diff = out_diff.max()\n    print(\'===============\')\n    print(\'max diff : {}, mean diff : {}\'.format(max_diff, mean_diff))\n    print(\'mean val: tf {tf_mean} pt {pt_mean}\'.format(\n        tf_mean=tf_out.mean(), pt_mean=py_out.mean()))\n    print(\'max vals: tf {tf_max} pt {pt_max}\'.format(\n        tf_max=tf_out.max(), pt_max=py_out.max()))\n    print(\'max relative diff: tf {tf_rel} pt {pt_rel}\'.format(\n        tf_rel=(out_diff / np.abs(tf_out)).max(),\n        pt_rel=(out_diff / np.abs(py_out)).max()))\n    print(\'===============\')\n'"
src/test_batch_norm.py,4,"b'import torch\nfrom torch.autograd import Variable\n\nfrom src.inflate import inflate_batch_norm\n\ndef test_inflate_batch_norm():\n    torch.manual_seed(0)\n\n    input_space_dim = 10  # Dimensions of input image\n    in_channels = 3  # input feature map dim\n    out_channels = 2  # output feature map dim\n\n    filter_space_dim = 5  # conv filter spatial dim\n    filter_time_dim = 3  # conv filter temporal dim\n\n    frame_nb = 5\n    batch_dim = 10\n\n    # Initialize inputs\n    input_img = torch.rand(batch_dim, in_channels, input_space_dim, input_space_dim)\n    input_2d_var = Variable(input_img)\n    input_3d = input_img.unsqueeze(2).repeat(1, 1, frame_nb, 1, 1)\n    input_3d_var = Variable(input_3d)\n\n    # Initialize batch_norm\n    batch2d = torch.nn.BatchNorm2d(in_channels)\n    batch3d = inflate_batch_norm(batch2d)\n\n    # Compute outputs\n    out_2d = batch2d(input_2d_var)\n    out_3d = batch3d(input_3d_var)\n    expected_out_3d = out_2d.data.unsqueeze(2).repeat(1, 1, frame_nb, 1, 1)\n\n    output_diff = out_3d.data - expected_out_3d\n    assert(output_diff.max() == 0)\n    batch2d.eval()\n    batch3d.eval()\n    out_2d = batch2d(input_2d_var)\n    out_3d = batch3d(input_3d_var)\n    expected_out_3d = out_2d.data.unsqueeze(2).repeat(1, 1, frame_nb, 1, 1)\n\n    output_diff = out_3d.data - expected_out_3d\n    assert(output_diff.max() == 0)\n'"
src/test_first_block.py,6,"b""import torch\nimport torchvision\nimport torchvision.transforms as transforms\nimport torchvision.datasets as datasets\n\nfrom src import inflate\n\ndef test_input_block():\n    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                                     std=[0.229, 0.224, 0.225])\n    dataset = datasets.ImageFolder('/sequoia/data1/yhasson/datasets/test-dataset',\n            transforms.Compose([\n            transforms.RandomSizedCrop(224),\n            transforms.RandomHorizontalFlip(),\n            transforms.ToTensor(),\n            normalize,\n        ]))\n\n    densenet = torchvision.models.densenet121(pretrained=True)\n    features = densenet.features\n    seq2d = torch.nn.Sequential(\n        features.conv0, features.norm0, features.relu0, features.pool0)\n    seq3d = torch.nn.Sequential(\n        inflate.inflate_conv(features.conv0, 3),\n        inflate.inflate_batch_norm(features.norm0),\n        features.relu0,\n        inflate.inflate_pool(features.pool0, 1))\n\n    loader = torch.utils.data.DataLoader(dataset, batch_size=2, shuffle=False)\n    frame_nb = 4\n    for i, (input_2d, target) in enumerate(loader):\n        target = target.cuda()\n        target_var = torch.autograd.Variable(target)\n        input_2d_var = torch.autograd.Variable(input_2d)\n        out2d = seq2d(input_2d_var)\n        time_pad = torch.nn.ReplicationPad3d((0, 0, 0, 0, 1, 1))\n        input_3d = input_2d.unsqueeze(2).repeat(1, 1, frame_nb, 1, 1)\n        input_3d_var = time_pad(input_3d) \n        out3d = seq3d(input_3d_var)\n        expected_out_3d = out2d.data.unsqueeze(2).repeat(1, 1, frame_nb, 1, 1)\n        out_diff = expected_out_3d - out3d.data\n        print(out_diff.max())\n        assert(out_diff.max() < 0.0001)\n"""
src/test_inflate_conv.py,8,"b'import torch\nfrom torch.autograd import Variable\n\nfrom src.inflate import inflate_conv\n\ndef test_inflate_conv_no_padding():\n    torch.manual_seed(0)\n\n    input_space_dim = 10  # Dimensions of input image\n    in_channels = 3  # input feature map dim\n    out_channels = 2  # output feature map dim\n\n    filter_space_dim = 5  # conv filter spatial dim\n    filter_time_dim = 3  # conv filter temporal dim\n\n    frame_nb = 5\n\n    # Initialize inputs with batch dimension\n    input_img = torch.rand(in_channels, input_space_dim, input_space_dim)\n    input_2d_var = Variable(input_img.unsqueeze(0))\n    input_3d = input_img.unsqueeze(1).repeat(1, frame_nb, 1, 1)\n    input_3d_var = Variable(input_3d.unsqueeze(0))\n\n    # Initialize convolutions\n    conv2d = torch.nn.Conv2d(in_channels, out_channels, filter_space_dim, padding=1)\n    conv3d = inflate_conv(conv2d, filter_time_dim)\n\n    # Compute outputs\n    out_2d = conv2d(input_2d_var)\n    out_3d = conv3d(input_3d_var)\n    expected_out_3d = out_2d.data.unsqueeze(2).repeat(1, 1, frame_nb - 2*int(filter_time_dim/2), 1, 1)\n\n    output_diff = out_3d.data - expected_out_3d\n    assert(output_diff.max() < 0.00001)\n\n\ndef test_inflate_conv_padding():\n    torch.manual_seed(0)\n\n    input_space_dim = 10  # Dimensions of input image\n    in_channels = 3  # input feature map dim\n    out_channels = 2  # output feature map dim\n\n    filter_space_dim = 5  # conv filter spatial dim\n    filter_time_dim = 3  # conv filter temporal dim\n\n    frame_nb = 5\n    batch_size = 4\n    \n    # Padding params\n    pad_size = int(filter_time_dim/2)\n    time_pad = torch.nn.ReplicationPad3d((0, 0, 0, 0, pad_size, pad_size))\n\n    # Initialize inputs with batch dimension\n    input_img = torch.rand(batch_size, in_channels, input_space_dim, input_space_dim)\n    input_2d_var = Variable(input_img)\n    input_3d = input_img.unsqueeze(2).repeat(1, 1, frame_nb, 1, 1)\n    input_3d_var = time_pad(input_3d)\n\n    # Initialize convolutions\n    conv2d = torch.nn.Conv2d(in_channels, out_channels, filter_space_dim, padding=1)\n    conv3d = inflate_conv(conv2d, filter_time_dim)\n\n    # Compute outputs\n    out_2d = conv2d(input_2d_var)\n    out_3d = conv3d(input_3d_var)\n    expected_out_3d = out_2d.data.unsqueeze(2).repeat(1, 1, frame_nb, 1, 1)\n\n    output_diff = out_3d.data - expected_out_3d\n    assert(output_diff.max() < 0.00001)\n'"
