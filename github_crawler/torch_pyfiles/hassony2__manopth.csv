file_path,api_count,code
setup.py,0,"b'from setuptools import find_packages, setup\nimport warnings\n\nDEPENDENCY_PACKAGE_NAMES = [""matplotlib"", ""torch"", ""tqdm"", ""numpy"", ""cv2"",\n                            ""chumpy""]\n\n\ndef check_dependencies():\n    missing_dependencies = []\n    for package_name in DEPENDENCY_PACKAGE_NAMES:\n        try:\n            __import__(package_name)\n        except ImportError:\n            missing_dependencies.append(package_name)\n\n    if missing_dependencies:\n        warnings.warn(\n            \'Missing dependencies: {}. We recommend you follow \'\n            \'the installation instructions at \'\n            \'https://github.com/hassony2/manopth#installation\'.format(\n                missing_dependencies))\n\n\nwith open(""README.md"", ""r"") as fh:\n    long_description = fh.read()\n\ncheck_dependencies()\n\nsetup(\n    name=""manopth"",\n    version=""0.0.1"",\n    author=""Yana Hasson"",\n    author_email=""yana.hasson.inria@gmail.com"",\n    packages=find_packages(exclude=(\'tests\',)),\n    python_requires="">=3.5.0"",\n    description=""PyTorch mano layer"",\n    long_description=long_description,\n    long_description_content_type=""text/markdown"",\n    url=""https://github.com/hassony2/manopth"",\n    classifiers=[\n        ""Programming Language :: Python :: 3"",\n        ""License :: OSI Approved :: GNU GENERAL PUBLIC LICENSE"",\n        ""Operating System :: OS Independent"",\n    ],\n)\n'"
examples/manopth_demo.py,4,"b'import argparse\n\nfrom matplotlib import pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\nimport torch\nfrom tqdm import tqdm\n\nfrom manopth import argutils\nfrom manopth.manolayer import ManoLayer\nfrom manopth.demo import display_hand\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'--batch_size\', default=1, type=int)\n    parser.add_argument(\'--cuda\', action=\'store_true\')\n    parser.add_argument(\n        \'--no_display\',\n        action=\'store_true\',\n        help=""Disable display output of ManoLayer given random inputs"")\n    parser.add_argument(\'--side\', default=\'left\', choices=[\'left\', \'right\'])\n    parser.add_argument(\'--random_shape\', action=\'store_true\', help=""Random hand shape"")\n    parser.add_argument(\'--rand_mag\', type=float, default=1, help=""Controls pose variability"")\n    parser.add_argument(\n        \'--flat_hand_mean\',\n        action=\'store_true\',\n        help=""Use flat hand as mean instead of average hand pose"")\n    parser.add_argument(\n        \'--iters\',\n        type=int,\n        default=1,\n        help=\n        ""Use for quick profiling of forward and backward pass accross ManoLayer""\n    )\n    parser.add_argument(\'--mano_root\', default=\'mano/models\')\n    parser.add_argument(\'--root_rot_mode\', default=\'axisang\', choices=[\'rot6d\', \'axisang\'])\n    parser.add_argument(\'--no_pca\', action=\'store_true\', help=""Give axis-angle or rotation matrix as inputs instead of PCA coefficients"")\n    parser.add_argument(\'--joint_rot_mode\', default=\'axisang\', choices=[\'rotmat\', \'axisang\'], help=""Joint rotation inputs"")\n    parser.add_argument(\n        \'--mano_ncomps\', default=6, type=int, help=""Number of PCA components"")\n    args = parser.parse_args()\n\n    argutils.print_args(args)\n\n    layer = ManoLayer(\n        flat_hand_mean=args.flat_hand_mean,\n        side=args.side,\n        mano_root=args.mano_root,\n        ncomps=args.mano_ncomps,\n        use_pca=not args.no_pca,\n        root_rot_mode=args.root_rot_mode,\n        joint_rot_mode=args.joint_rot_mode)\n    if args.root_rot_mode == \'axisang\':\n        rot = 3\n    else:\n        rot = 6\n    print(rot)\n    if args.no_pca:\n        args.mano_ncomps = 45\n\n    # Generate random pose coefficients\n    pose_params = args.rand_mag * torch.rand(args.batch_size, args.mano_ncomps + rot)\n    pose_params.requires_grad = True\n    if args.random_shape:\n        shape = torch.rand(args.batch_size, 10)\n    else:\n        shape = torch.zeros(1)  # Hack to act like None for PyTorch JIT\n    if args.cuda:\n        pose_params = pose_params.cuda()\n        shape = shape.cuda()\n        layer.cuda()\n\n    # Loop for forward/backward quick profiling\n    for idx in tqdm(range(args.iters)):\n        # Forward pass\n        verts, Jtr = layer(pose_params, th_betas=shape)\n\n        # Backward pass\n        loss = torch.norm(verts)\n        loss.backward()\n\n    if not args.no_display:\n        verts, Jtr = layer(pose_params, th_betas=shape)\n        joints = Jtr.cpu().detach()\n        verts = verts.cpu().detach()\n\n        # Draw obtained vertices and joints\n        display_hand({\n            \'verts\': verts,\n            \'joints\': joints\n        },\n                     mano_faces=layer.th_faces)\n'"
examples/manopth_mindemo.py,2,"b""import torch\nfrom manopth.manolayer import ManoLayer\nfrom manopth import demo\n\nbatch_size = 10\n# Select number of principal components for pose space\nncomps = 6\n\n# Initialize MANO layer\nmano_layer = ManoLayer(\n    mano_root='mano/models', use_pca=True, ncomps=ncomps, flat_hand_mean=False)\n\n# Generate random shape parameters\nrandom_shape = torch.rand(batch_size, 10)\n# Generate random pose parameters, including 3 values for global axis-angle rotation\nrandom_pose = torch.rand(batch_size, ncomps + 3)\n\n# Forward pass through MANO layer\nhand_verts, hand_joints = mano_layer(random_pose, random_shape)\ndemo.display_hand({\n    'verts': hand_verts,\n    'joints': hand_joints\n},\n                  mano_faces=mano_layer.th_faces)\n"""
mano/__init__.py,0,b''
manopth/__init__.py,0,"b""name = 'manopth'\n"""
manopth/argutils.py,0,"b'import datetime\nimport os\nimport pickle\nimport subprocess\nimport sys\n\n\ndef print_args(args):\n    opts = vars(args)\n    print(\'======= Options ========\')\n    for k, v in sorted(opts.items()):\n        print(\'{}: {}\'.format(k, v))\n    print(\'========================\')\n\n\ndef save_args(args, save_folder, opt_prefix=\'opt\', verbose=True):\n    opts = vars(args)\n    # Create checkpoint folder\n    if not os.path.exists(save_folder):\n        os.makedirs(save_folder, exist_ok=True)\n\n    # Save options\n    opt_filename = \'{}.txt\'.format(opt_prefix)\n    opt_path = os.path.join(save_folder, opt_filename)\n    with open(opt_path, \'a\') as opt_file:\n        opt_file.write(\'====== Options ======\\n\')\n        for k, v in sorted(opts.items()):\n            opt_file.write(\n                \'{option}: {value}\\n\'.format(option=str(k), value=str(v)))\n        opt_file.write(\'=====================\\n\')\n        opt_file.write(\'launched {} at {}\\n\'.format(\n            str(sys.argv[0]), str(datetime.datetime.now())))\n\n        # Add git info\n        label = subprocess.check_output([""git"", ""describe"",\n                                         ""--always""]).strip()\n        if subprocess.call(\n            [""git"", ""branch""],\n                stderr=subprocess.STDOUT,\n                stdout=open(os.devnull, \'w\')) == 0:\n            opt_file.write(\'=== Git info ====\\n\')\n            opt_file.write(\'{}\\n\'.format(label))\n            commit = subprocess.check_output([\'git\', \'rev-parse\', \'HEAD\'])\n            opt_file.write(\'commit : {}\\n\'.format(commit.strip()))\n\n    opt_picklename = \'{}.pkl\'.format(opt_prefix)\n    opt_picklepath = os.path.join(save_folder, opt_picklename)\n    with open(opt_picklepath, \'wb\') as opt_file:\n        pickle.dump(opts, opt_file)\n    if verbose:\n        print(\'Saved options to {}\'.format(opt_path))\n'"
manopth/demo.py,1,"b'from matplotlib import pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom mpl_toolkits.mplot3d.art3d import Poly3DCollection\nimport numpy as np\nimport torch\n\nfrom manopth.manolayer import ManoLayer\n\n\ndef generate_random_hand(batch_size=1, ncomps=6, mano_root=\'mano/models\'):\n    nfull_comps = ncomps + 3  # Add global orientation dims to PCA\n    random_pcapose = torch.rand(batch_size, nfull_comps)\n    mano_layer = ManoLayer(mano_root=mano_root)\n    verts, joints = mano_layer(random_pcapose)\n    return {\'verts\': verts, \'joints\': joints, \'faces\': mano_layer.th_faces}\n\n\ndef display_hand(hand_info, mano_faces=None, ax=None, alpha=0.2, batch_idx=0, show=True):\n    """"""\n    Displays hand batch_idx in batch of hand_info, hand_info as returned by\n    generate_random_hand\n    """"""\n    if ax is None:\n        fig = plt.figure()\n        ax = fig.add_subplot(111, projection=\'3d\')\n    verts, joints = hand_info[\'verts\'][batch_idx], hand_info[\'joints\'][\n        batch_idx]\n    if mano_faces is None:\n        ax.scatter(verts[:, 0], verts[:, 1], verts[:, 2], alpha=0.1)\n    else:\n        mesh = Poly3DCollection(verts[mano_faces], alpha=alpha)\n        face_color = (141 / 255, 184 / 255, 226 / 255)\n        edge_color = (50 / 255, 50 / 255, 50 / 255)\n        mesh.set_edgecolor(edge_color)\n        mesh.set_facecolor(face_color)\n        ax.add_collection3d(mesh)\n    ax.scatter(joints[:, 0], joints[:, 1], joints[:, 2], color=\'r\')\n    cam_equal_aspect_3d(ax, verts.numpy())\n    if show:\n        plt.show()\n\n\ndef cam_equal_aspect_3d(ax, verts, flip_x=False):\n    """"""\n    Centers view on cuboid containing hand and flips y and z axis\n    and fixes azimuth\n    """"""\n    extents = np.stack([verts.min(0), verts.max(0)], axis=1)\n    sz = extents[:, 1] - extents[:, 0]\n    centers = np.mean(extents, axis=1)\n    maxsize = max(abs(sz))\n    r = maxsize / 2\n    if flip_x:\n        ax.set_xlim(centers[0] + r, centers[0] - r)\n    else:\n        ax.set_xlim(centers[0] - r, centers[0] + r)\n    # Invert y and z axis\n    ax.set_ylim(centers[1] + r, centers[1] - r)\n    ax.set_zlim(centers[2] + r, centers[2] - r)\n'"
manopth/manolayer.py,37,"b'import os\n\nimport numpy as np\nimport torch\nfrom torch.nn import Module\n\nfrom mano.webuser.smpl_handpca_wrapper_HAND_only import ready_arguments\nfrom manopth import rodrigues_layer, rotproj, rot6d\nfrom manopth.tensutils import (th_posemap_axisang, th_with_zeros, th_pack,\n                               subtract_flat_id, make_list)\n\n\nclass ManoLayer(Module):\n    __constants__ = [\n        \'use_pca\', \'rot\', \'ncomps\', \'ncomps\', \'kintree_parents\', \'check\',\n        \'side\', \'center_idx\', \'joint_rot_mode\'\n    ]\n\n    def __init__(self,\n                 center_idx=None,\n                 flat_hand_mean=True,\n                 ncomps=6,\n                 side=\'right\',\n                 mano_root=\'mano/models\',\n                 use_pca=True,\n                 root_rot_mode=\'axisang\',\n                 joint_rot_mode=\'axisang\',\n                 robust_rot=False):\n        """"""\n        Args:\n            center_idx: index of center joint in our computations,\n                if -1 centers on estimate of palm as middle of base\n                of middle finger and wrist\n            flat_hand_mean: if True, (0, 0, 0, ...) pose coefficients match\n                flat hand, else match average hand pose\n            mano_root: path to MANO pkl files for left and right hand\n            ncomps: number of PCA components form pose space (<45)\n            side: \'right\' or \'left\'\n            use_pca: Use PCA decomposition for pose space.\n            joint_rot_mode: \'axisang\' or \'rotmat\', ignored if use_pca\n        """"""\n        super().__init__()\n\n        self.center_idx = center_idx\n        self.robust_rot = robust_rot\n        if root_rot_mode == \'axisang\':\n            self.rot = 3\n        else:\n            self.rot = 6\n        self.flat_hand_mean = flat_hand_mean\n        self.side = side\n        self.use_pca = use_pca\n        self.joint_rot_mode = joint_rot_mode\n        self.root_rot_mode = root_rot_mode\n        if use_pca:\n            self.ncomps = ncomps\n        else:\n            self.ncomps = 45\n\n        if side == \'right\':\n            self.mano_path = os.path.join(mano_root, \'MANO_RIGHT.pkl\')\n        elif side == \'left\':\n            self.mano_path = os.path.join(mano_root, \'MANO_LEFT.pkl\')\n\n        smpl_data = ready_arguments(self.mano_path)\n\n        hands_components = smpl_data[\'hands_components\']\n\n        self.smpl_data = smpl_data\n\n        self.register_buffer(\'th_betas\',\n                             torch.Tensor(smpl_data[\'betas\'].r).unsqueeze(0))\n        self.register_buffer(\'th_shapedirs\',\n                             torch.Tensor(smpl_data[\'shapedirs\'].r))\n        self.register_buffer(\'th_posedirs\',\n                             torch.Tensor(smpl_data[\'posedirs\'].r))\n        self.register_buffer(\n            \'th_v_template\',\n            torch.Tensor(smpl_data[\'v_template\'].r).unsqueeze(0))\n        self.register_buffer(\n            \'th_J_regressor\',\n            torch.Tensor(np.array(smpl_data[\'J_regressor\'].toarray())))\n        self.register_buffer(\'th_weights\',\n                             torch.Tensor(smpl_data[\'weights\'].r))\n        self.register_buffer(\'th_faces\',\n                             torch.Tensor(smpl_data[\'f\'].astype(np.int32)).long())\n\n        # Get hand mean\n        hands_mean = np.zeros(hands_components.shape[1]\n                              ) if flat_hand_mean else smpl_data[\'hands_mean\']\n        hands_mean = hands_mean.copy()\n        th_hands_mean = torch.Tensor(hands_mean).unsqueeze(0)\n        if self.use_pca or self.joint_rot_mode == \'axisang\':\n            # Save as axis-angle\n            self.register_buffer(\'th_hands_mean\', th_hands_mean)\n            selected_components = hands_components[:ncomps]\n            self.register_buffer(\'th_selected_comps\',\n                                 torch.Tensor(selected_components))\n        else:\n            th_hands_mean_rotmat = rodrigues_layer.batch_rodrigues(\n                th_hands_mean.view(15, 3)).reshape(15, 3, 3)\n            self.register_buffer(\'th_hands_mean_rotmat\', th_hands_mean_rotmat)\n\n        # Kinematic chain params\n        self.kintree_table = smpl_data[\'kintree_table\']\n        parents = list(self.kintree_table[0].tolist())\n        self.kintree_parents = parents\n\n    def forward(self,\n                th_pose_coeffs,\n                th_betas=torch.zeros(1),\n                th_trans=torch.zeros(1),\n                root_palm=torch.Tensor([0]),\n                share_betas=torch.Tensor([0]),\n                ):\n        """"""\n        Args:\n        th_trans (Tensor (batch_size x ncomps)): if provided, applies trans to joints and vertices\n        th_betas (Tensor (batch_size x 10)): if provided, uses given shape parameters for hand shape\n        else centers on root joint (9th joint)\n        root_palm: return palm as hand root instead of wrist\n        """"""\n        # if len(th_pose_coeffs) == 0:\n        #     return th_pose_coeffs.new_empty(0), th_pose_coeffs.new_empty(0)\n\n        batch_size = th_pose_coeffs.shape[0]\n        # Get axis angle from PCA components and coefficients\n        if self.use_pca or self.joint_rot_mode == \'axisang\':\n            # Remove global rot coeffs\n            th_hand_pose_coeffs = th_pose_coeffs[:, self.rot:self.rot +\n                                                 self.ncomps]\n            if self.use_pca:\n                # PCA components --> axis angles\n                th_full_hand_pose = th_hand_pose_coeffs.mm(self.th_selected_comps)\n            else:\n                th_full_hand_pose = th_hand_pose_coeffs\n\n            # Concatenate back global rot\n            th_full_pose = torch.cat([\n                th_pose_coeffs[:, :self.rot],\n                self.th_hands_mean + th_full_hand_pose\n            ], 1)\n            if self.root_rot_mode == \'axisang\':\n                # compute rotation matrixes from axis-angle while skipping global rotation\n                th_pose_map, th_rot_map = th_posemap_axisang(th_full_pose)\n                root_rot = th_rot_map[:, :9].view(batch_size, 3, 3)\n                th_rot_map = th_rot_map[:, 9:]\n                th_pose_map = th_pose_map[:, 9:]\n            else:\n                # th_posemap offsets by 3, so add offset or 3 to get to self.rot=6\n                th_pose_map, th_rot_map = th_posemap_axisang(th_full_pose[:, 6:])\n                if self.robust_rot:\n                    root_rot = rot6d.robust_compute_rotation_matrix_from_ortho6d(th_full_pose[:, :6])\n                else:\n                    root_rot = rot6d.compute_rotation_matrix_from_ortho6d(th_full_pose[:, :6])\n        else:\n            assert th_pose_coeffs.dim() == 4, (\n                \'When not self.use_pca, \'\n                \'th_pose_coeffs should have 4 dims, got {}\'.format(\n                    th_pose_coeffs.dim()))\n            assert th_pose_coeffs.shape[2:4] == (3, 3), (\n                \'When not self.use_pca, th_pose_coeffs have 3x3 matrix for two\'\n                \'last dims, got {}\'.format(th_pose_coeffs.shape[2:4]))\n            th_pose_rots = rotproj.batch_rotprojs(th_pose_coeffs)\n            th_rot_map = th_pose_rots[:, 1:].view(batch_size, -1)\n            th_pose_map = subtract_flat_id(th_rot_map)\n            root_rot = th_pose_rots[:, 0]\n\n        # Full axis angle representation with root joint\n        if th_betas is None or th_betas.numel() == 1:\n            th_v_shaped = torch.matmul(self.th_shapedirs,\n                                       self.th_betas.transpose(1, 0)).permute(\n                                           2, 0, 1) + self.th_v_template\n            th_j = torch.matmul(self.th_J_regressor, th_v_shaped).repeat(\n                batch_size, 1, 1)\n\n        else:\n            if share_betas:\n                th_betas = th_betas.mean(0, keepdim=True).expand(th_betas.shape[0], 10)\n            th_v_shaped = torch.matmul(self.th_shapedirs,\n                                       th_betas.transpose(1, 0)).permute(\n                                           2, 0, 1) + self.th_v_template\n            th_j = torch.matmul(self.th_J_regressor, th_v_shaped)\n            # th_pose_map should have shape 20x135\n\n        th_v_posed = th_v_shaped + torch.matmul(\n            self.th_posedirs, th_pose_map.transpose(0, 1)).permute(2, 0, 1)\n        # Final T pose with transformation done !\n\n        # Global rigid transformation\n\n        root_j = th_j[:, 0, :].contiguous().view(batch_size, 3, 1)\n        root_trans = th_with_zeros(torch.cat([root_rot, root_j], 2))\n\n        all_rots = th_rot_map.view(th_rot_map.shape[0], 15, 3, 3)\n        lev1_idxs = [1, 4, 7, 10, 13]\n        lev2_idxs = [2, 5, 8, 11, 14]\n        lev3_idxs = [3, 6, 9, 12, 15]\n        lev1_rots = all_rots[:, [idx - 1 for idx in lev1_idxs]]\n        lev2_rots = all_rots[:, [idx - 1 for idx in lev2_idxs]]\n        lev3_rots = all_rots[:, [idx - 1 for idx in lev3_idxs]]\n        lev1_j = th_j[:, lev1_idxs]\n        lev2_j = th_j[:, lev2_idxs]\n        lev3_j = th_j[:, lev3_idxs]\n\n        # From base to tips\n        # Get lev1 results\n        all_transforms = [root_trans.unsqueeze(1)]\n        lev1_j_rel = lev1_j - root_j.transpose(1, 2)\n        lev1_rel_transform_flt = th_with_zeros(torch.cat([lev1_rots, lev1_j_rel.unsqueeze(3)], 3).view(-1, 3, 4))\n        root_trans_flt = root_trans.unsqueeze(1).repeat(1, 5, 1, 1).view(root_trans.shape[0] * 5, 4, 4)\n        lev1_flt = torch.matmul(root_trans_flt, lev1_rel_transform_flt)\n        all_transforms.append(lev1_flt.view(all_rots.shape[0], 5, 4, 4))\n\n        # Get lev2 results\n        lev2_j_rel = lev2_j - lev1_j\n        lev2_rel_transform_flt = th_with_zeros(torch.cat([lev2_rots, lev2_j_rel.unsqueeze(3)], 3).view(-1, 3, 4))\n        lev2_flt = torch.matmul(lev1_flt, lev2_rel_transform_flt)\n        all_transforms.append(lev2_flt.view(all_rots.shape[0], 5, 4, 4))\n\n        # Get lev3 results\n        lev3_j_rel = lev3_j - lev2_j\n        lev3_rel_transform_flt = th_with_zeros(torch.cat([lev3_rots, lev3_j_rel.unsqueeze(3)], 3).view(-1, 3, 4))\n        lev3_flt = torch.matmul(lev2_flt, lev3_rel_transform_flt)\n        all_transforms.append(lev3_flt.view(all_rots.shape[0], 5, 4, 4))\n\n        reorder_idxs = [0, 1, 6, 11, 2, 7, 12, 3, 8, 13, 4, 9, 14, 5, 10, 15]\n        th_results = torch.cat(all_transforms, 1)[:, reorder_idxs]\n        th_results_global = th_results\n\n        joint_js = torch.cat([th_j, th_j.new_zeros(th_j.shape[0], 16, 1)], 2)\n        tmp2 = torch.matmul(th_results, joint_js.unsqueeze(3))\n        th_results2 = (th_results - torch.cat([tmp2.new_zeros(*tmp2.shape[:2], 4, 3), tmp2], 3)).permute(0, 2, 3, 1)\n\n        th_T = torch.matmul(th_results2, self.th_weights.transpose(0, 1))\n\n        th_rest_shape_h = torch.cat([\n            th_v_posed.transpose(2, 1),\n            torch.ones((batch_size, 1, th_v_posed.shape[1]),\n                       dtype=th_T.dtype,\n                       device=th_T.device),\n        ], 1)\n\n        th_verts = (th_T * th_rest_shape_h.unsqueeze(1)).sum(2).transpose(2, 1)\n        th_verts = th_verts[:, :, :3]\n        th_jtr = th_results_global[:, :, :3, 3]\n        # In addition to MANO reference joints we sample vertices on each finger\n        # to serve as finger tips\n        if self.side == \'right\':\n            tips = th_verts[:, [745, 317, 444, 556, 673]]\n        else:\n            tips = th_verts[:, [745, 317, 445, 556, 673]]\n        if bool(root_palm):\n            palm = (th_verts[:, 95] + th_verts[:, 22]).unsqueeze(1) / 2\n            th_jtr = torch.cat([palm, th_jtr[:, 1:]], 1)\n        th_jtr = torch.cat([th_jtr, tips], 1)\n\n        # Reorder joints to match visualization utilities\n        th_jtr = th_jtr[:, [0, 13, 14, 15, 16, 1, 2, 3, 17, 4, 5, 6, 18, 10, 11, 12, 19, 7, 8, 9, 20]]\n\n        if th_trans is None or bool(torch.norm(th_trans) == 0):\n            if self.center_idx is not None:\n                center_joint = th_jtr[:, self.center_idx].unsqueeze(1)\n                th_jtr = th_jtr - center_joint\n                th_verts = th_verts - center_joint\n        else:\n            th_jtr = th_jtr + th_trans.unsqueeze(1)\n            th_verts = th_verts + th_trans.unsqueeze(1)\n\n        # Scale to milimeters\n        th_verts = th_verts * 1000\n        th_jtr = th_jtr * 1000\n        return th_verts, th_jtr\n'"
manopth/rodrigues_layer.py,13,"b'""""""\nThis part reuses code from https://github.com/MandyMo/pytorch_HMR/blob/master/src/util.py\nwhich is part of a PyTorch port of SMPL.\nThanks to Zhang Xiong (MandyMo) for making this great code available on github !\n""""""\n\nimport argparse\nfrom torch.autograd import gradcheck\nimport torch\nfrom torch.autograd import Variable\n\nfrom manopth import argutils\n\n\ndef quat2mat(quat):\n    """"""Convert quaternion coefficients to rotation matrix.\n    Args:\n        quat: size = [batch_size, 4] 4 <===>(w, x, y, z)\n    Returns:\n        Rotation matrix corresponding to the quaternion -- size = [batch_size, 3, 3]\n    """"""\n    norm_quat = quat\n    norm_quat = norm_quat / norm_quat.norm(p=2, dim=1, keepdim=True)\n    w, x, y, z = norm_quat[:, 0], norm_quat[:, 1], norm_quat[:,\n                                                             2], norm_quat[:,\n                                                                           3]\n\n    batch_size = quat.size(0)\n\n    w2, x2, y2, z2 = w.pow(2), x.pow(2), y.pow(2), z.pow(2)\n    wx, wy, wz = w * x, w * y, w * z\n    xy, xz, yz = x * y, x * z, y * z\n\n    rotMat = torch.stack([\n        w2 + x2 - y2 - z2, 2 * xy - 2 * wz, 2 * wy + 2 * xz, 2 * wz + 2 * xy,\n        w2 - x2 + y2 - z2, 2 * yz - 2 * wx, 2 * xz - 2 * wy, 2 * wx + 2 * yz,\n        w2 - x2 - y2 + z2\n    ],\n                         dim=1).view(batch_size, 3, 3)\n    return rotMat\n\n\ndef batch_rodrigues(axisang):\n    #axisang N x 3\n    axisang_norm = torch.norm(axisang + 1e-8, p=2, dim=1)\n    angle = torch.unsqueeze(axisang_norm, -1)\n    axisang_normalized = torch.div(axisang, angle)\n    angle = angle * 0.5\n    v_cos = torch.cos(angle)\n    v_sin = torch.sin(angle)\n    quat = torch.cat([v_cos, v_sin * axisang_normalized], dim=1)\n    rot_mat = quat2mat(quat)\n    rot_mat = rot_mat.view(rot_mat.shape[0], 9)\n    return rot_mat\n\n\ndef th_get_axis_angle(vector):\n    angle = torch.norm(vector, 2, 1)\n    axes = vector / angle.unsqueeze(1)\n    return axes, angle\n\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'--batch_size\', default=1, type=int)\n    parser.add_argument(\'--cuda\', action=\'store_true\')\n    args = parser.parse_args()\n\n    argutils.print_args(args)\n\n    n_components = 6\n    rot = 3\n    inputs = torch.rand(args.batch_size, rot)\n    inputs_var = Variable(inputs.double(), requires_grad=True)\n    if args.cuda:\n        inputs = inputs.cuda()\n    # outputs = batch_rodrigues(inputs)\n    test_function = gradcheck(batch_rodrigues, (inputs_var, ))\n    print(\'batch test passed !\')\n\n    inputs = torch.rand(rot)\n    inputs_var = Variable(inputs.double(), requires_grad=True)\n    test_function = gradcheck(th_cv2_rod_sub_id.apply, (inputs_var, ))\n    print(\'th_cv2_rod test passed\')\n\n    inputs = torch.rand(rot)\n    inputs_var = Variable(inputs.double(), requires_grad=True)\n    test_th = gradcheck(th_cv2_rod.apply, (inputs_var, ))\n    print(\'th_cv2_rod_id test passed !\')\n'"
manopth/rot6d.py,7,"b'import torch\n\n\ndef compute_rotation_matrix_from_ortho6d(poses):\n    """"""\n    Code from\n    https://github.com/papagina/RotationContinuity\n    On the Continuity of Rotation Representations in Neural Networks\n    Zhou et al. CVPR19\n    https://zhouyisjtu.github.io/project_rotation/rotation.html\n    """"""\n    x_raw = poses[:, 0:3]  # batch*3\n    y_raw = poses[:, 3:6]  # batch*3\n        \n    x = normalize_vector(x_raw)  # batch*3\n    z = cross_product(x, y_raw)  # batch*3\n    z = normalize_vector(z)  # batch*3\n    y = cross_product(z, x)  # batch*3\n        \n    x = x.view(-1, 3, 1)\n    y = y.view(-1, 3, 1)\n    z = z.view(-1, 3, 1)\n    matrix = torch.cat((x, y, z), 2)  # batch*3*3\n    return matrix\n\ndef robust_compute_rotation_matrix_from_ortho6d(poses):\n    """"""\n    Instead of making 2nd vector orthogonal to first\n    create a base that takes into account the two predicted\n    directions equally\n    """"""\n    x_raw = poses[:, 0:3]  # batch*3\n    y_raw = poses[:, 3:6]  # batch*3\n\n    x = normalize_vector(x_raw)  # batch*3\n    y = normalize_vector(y_raw)  # batch*3\n    middle = normalize_vector(x + y)\n    orthmid = normalize_vector(x - y)\n    x = normalize_vector(middle + orthmid)\n    y = normalize_vector(middle - orthmid)\n    # Their scalar product should be small !\n    # assert torch.einsum(""ij,ij->i"", [x, y]).abs().max() < 0.00001\n    z = normalize_vector(cross_product(x, y))\n\n    x = x.view(-1, 3, 1)\n    y = y.view(-1, 3, 1)\n    z = z.view(-1, 3, 1)\n    matrix = torch.cat((x, y, z), 2)  # batch*3*3\n    # Check for reflection in matrix ! If found, flip last vector TODO\n    assert (torch.stack([torch.det(mat) for mat in matrix ])< 0).sum() == 0\n    return matrix\n\n\ndef normalize_vector(v):\n    batch = v.shape[0]\n    v_mag = torch.sqrt(v.pow(2).sum(1))  # batch\n    v_mag = torch.max(v_mag, v.new([1e-8]))\n    v_mag = v_mag.view(batch, 1).expand(batch, v.shape[1])\n    v = v/v_mag\n    return v\n\n\ndef cross_product(u, v):\n    batch = u.shape[0]\n    i = u[:, 1] * v[:, 2] - u[:, 2] * v[:, 1]\n    j = u[:, 2] * v[:, 0] - u[:, 0] * v[:, 2]\n    k = u[:, 0] * v[:, 1] - u[:, 1] * v[:, 0]\n        \n    out = torch.cat((i.view(batch, 1), j.view(batch, 1), k.view(batch, 1)), 1)\n        \n    return out\n'"
manopth/rotproj.py,3,"b'import torch\n\n\ndef batch_rotprojs(batches_rotmats):\n    proj_rotmats = []\n    for batch_idx, batch_rotmats in enumerate(batches_rotmats):\n        proj_batch_rotmats = []\n        for rot_idx, rotmat in enumerate(batch_rotmats):\n            # GPU implementation of svd is VERY slow\n            # ~ 2 10^-3 per hit vs 5 10^-5 on cpu\n            U, S, V = rotmat.cpu().svd()\n            rotmat = torch.matmul(U, V.transpose(0, 1))\n            orth_det = rotmat.det()\n            # Remove reflection\n            if orth_det < 0:\n                rotmat[:, 2] = -1 * rotmat[:, 2]\n\n            rotmat = rotmat.cuda()\n            proj_batch_rotmats.append(rotmat)\n        proj_rotmats.append(torch.stack(proj_batch_rotmats))\n    return torch.stack(proj_rotmats)\n'"
manopth/tensutils.py,3,"b'import torch\n\nfrom manopth import rodrigues_layer\n\n\ndef th_posemap_axisang(pose_vectors):\n    rot_nb = int(pose_vectors.shape[1] / 3)\n    pose_vec_reshaped = pose_vectors.contiguous().view(-1, 3)\n    rot_mats = rodrigues_layer.batch_rodrigues(pose_vec_reshaped)\n    rot_mats = rot_mats.view(pose_vectors.shape[0], rot_nb * 9)\n    pose_maps = subtract_flat_id(rot_mats)\n    return pose_maps, rot_mats\n\n\ndef th_with_zeros(tensor):\n    batch_size = tensor.shape[0]\n    padding = tensor.new([0.0, 0.0, 0.0, 1.0])\n    padding.requires_grad = False\n\n    concat_list = [tensor, padding.view(1, 1, 4).repeat(batch_size, 1, 1)]\n    cat_res = torch.cat(concat_list, 1)\n    return cat_res\n\n\ndef th_pack(tensor):\n    batch_size = tensor.shape[0]\n    padding = tensor.new_zeros((batch_size, 4, 3))\n    padding.requires_grad = False\n    pack_list = [padding, tensor]\n    pack_res = torch.cat(pack_list, 2)\n    return pack_res\n\n\ndef subtract_flat_id(rot_mats):\n    # Subtracts identity as a flattened tensor\n    rot_nb = int(rot_mats.shape[1] / 9)\n    id_flat = torch.eye(\n        3, dtype=rot_mats.dtype, device=rot_mats.device).view(1, 9).repeat(\n            rot_mats.shape[0], rot_nb)\n    # id_flat.requires_grad = False\n    results = rot_mats - id_flat\n    return results\n\n\ndef make_list(tensor):\n    # type: (List[int]) -> List[int]\n    return tensor\n'"
test/test_demo.py,0,"b""import torch\n\nfrom manopth.demo import generate_random_hand\n\n\ndef test_generate_random_hand():\n    batch_size = 3\n    hand_info = generate_random_hand(batch_size=batch_size, ncomps=6)\n    verts = hand_info['verts']\n    joints = hand_info['joints']\n    assert verts.shape == (batch_size, 778, 3)\n    assert joints.shape == (batch_size, 21, 3)\n"""
mano/webuser/__init__.py,0,b''
mano/webuser/lbs.py,0,"b""'''\nCopyright 2017 Javier Romero, Dimitrios Tzionas, Michael J Black and the Max Planck Gesellschaft.  All rights reserved.\nThis software is provided for research purposes only.\nBy using this software you agree to the terms of the MANO/SMPL+H Model license here http://mano.is.tue.mpg.de/license\n\nMore information about MANO/SMPL+H is available at http://mano.is.tue.mpg.de.\nFor comments or questions, please email us at: mano@tue.mpg.de\n\n\nAbout this file:\n================\nThis file defines a wrapper for the loading functions of the MANO model.\n\nModules included:\n- load_model:\n  loads the MANO model from a given file location (i.e. a .pkl file location),\n  or a dictionary object.\n\n'''\n\n\nfrom mano.webuser.posemapper import posemap\nimport chumpy\nimport numpy as np\n\n\ndef global_rigid_transformation(pose, J, kintree_table, xp):\n    results = {}\n    pose = pose.reshape((-1, 3))\n    id_to_col = {kintree_table[1, i]: i for i in range(kintree_table.shape[1])}\n    parent = {\n        i: id_to_col[kintree_table[0, i]]\n        for i in range(1, kintree_table.shape[1])\n    }\n\n    if xp == chumpy:\n        from mano.webuser.posemapper import Rodrigues\n        rodrigues = lambda x: Rodrigues(x)\n    else:\n        import cv2\n        rodrigues = lambda x: cv2.Rodrigues(x)[0]\n\n    with_zeros = lambda x: xp.vstack((x, xp.array([[0.0, 0.0, 0.0, 1.0]])))\n    results[0] = with_zeros(\n        xp.hstack((rodrigues(pose[0, :]), J[0, :].reshape((3, 1)))))\n\n    for i in range(1, kintree_table.shape[1]):\n        results[i] = results[parent[i]].dot(\n            with_zeros(\n                xp.hstack((rodrigues(pose[i, :]), ((J[i, :] - J[parent[i], :]\n                                                    ).reshape((3, 1)))))))\n\n    pack = lambda x: xp.hstack([np.zeros((4, 3)), x.reshape((4, 1))])\n\n    results = [results[i] for i in sorted(results.keys())]\n    results_global = results\n\n    if True:\n        results2 = [\n            results[i] - (pack(results[i].dot(xp.concatenate(((J[i, :]), 0)))))\n            for i in range(len(results))\n        ]\n        results = results2\n    result = xp.dstack(results)\n    return result, results_global\n\n\ndef verts_core(pose, v, J, weights, kintree_table, want_Jtr=False, xp=chumpy):\n    A, A_global = global_rigid_transformation(pose, J, kintree_table, xp)\n    T = A.dot(weights.T)\n\n    rest_shape_h = xp.vstack((v.T, np.ones((1, v.shape[0]))))\n\n    v = (T[:, 0, :] * rest_shape_h[0, :].reshape(\n        (1, -1)) + T[:, 1, :] * rest_shape_h[1, :].reshape(\n            (1, -1)) + T[:, 2, :] * rest_shape_h[2, :].reshape(\n                (1, -1)) + T[:, 3, :] * rest_shape_h[3, :].reshape((1, -1))).T\n\n    v = v[:, :3]\n\n    if not want_Jtr:\n        return v\n    Jtr = xp.vstack([g[:3, 3] for g in A_global])\n    return (v, Jtr)\n"""
mano/webuser/posemapper.py,0,"b""'''\nCopyright 2017 Javier Romero, Dimitrios Tzionas, Michael J Black and the Max Planck Gesellschaft.  All rights reserved.\nThis software is provided for research purposes only.\nBy using this software you agree to the terms of the MANO/SMPL+H Model license here http://mano.is.tue.mpg.de/license\n\nMore information about MANO/SMPL+H is available at http://mano.is.tue.mpg.de.\nFor comments or questions, please email us at: mano@tue.mpg.de\n\n\nAbout this file:\n================\nThis file defines a wrapper for the loading functions of the MANO model.\n\nModules included:\n- load_model:\n  loads the MANO model from a given file location (i.e. a .pkl file location),\n  or a dictionary object.\n\n'''\n\n\nimport chumpy as ch\nimport numpy as np\nimport cv2\n\n\nclass Rodrigues(ch.Ch):\n    dterms = 'rt'\n\n    def compute_r(self):\n        return cv2.Rodrigues(self.rt.r)[0]\n\n    def compute_dr_wrt(self, wrt):\n        if wrt is self.rt:\n            return cv2.Rodrigues(self.rt.r)[1].T\n\n\ndef lrotmin(p):\n    if isinstance(p, np.ndarray):\n        p = p.ravel()[3:]\n        return np.concatenate(\n            [(cv2.Rodrigues(np.array(pp))[0] - np.eye(3)).ravel()\n             for pp in p.reshape((-1, 3))]).ravel()\n    if p.ndim != 2 or p.shape[1] != 3:\n        p = p.reshape((-1, 3))\n    p = p[1:]\n    return ch.concatenate([(Rodrigues(pp) - ch.eye(3)).ravel()\n                           for pp in p]).ravel()\n\n\ndef posemap(s):\n    if s == 'lrotmin':\n        return lrotmin\n    else:\n        raise Exception('Unknown posemapping: %s' % (str(s), ))\n"""
mano/webuser/serialization.py,0,"b""'''\nCopyright 2017 Javier Romero, Dimitrios Tzionas, Michael J Black and the Max Planck Gesellschaft.  All rights reserved.\nThis software is provided for research purposes only.\nBy using this software you agree to the terms of the MANO/SMPL+H Model license here http://mano.is.tue.mpg.de/license\n\nMore information about MANO/SMPL+H is available at http://mano.is.tue.mpg.de.\nFor comments or questions, please email us at: mano@tue.mpg.de\n\n\nAbout this file:\n================\nThis file defines a wrapper for the loading functions of the MANO model.\n\nModules included:\n- load_model:\n  loads the MANO model from a given file location (i.e. a .pkl file location),\n  or a dictionary object.\n\n'''\n\n\n__all__ = ['load_model', 'save_model']\n\nimport numpy as np\nimport pickle\nimport chumpy as ch\nfrom chumpy.ch import MatVecMult\nfrom mano.webuser.posemapper import posemap\nfrom mano.webuser.verts import verts_core\n\ndef ready_arguments(fname_or_dict):\n\n    if not isinstance(fname_or_dict, dict):\n        dd = pickle.load(open(fname_or_dict, 'rb'), encoding='latin1')\n    else:\n        dd = fname_or_dict\n\n    backwards_compatibility_replacements(dd)\n\n    want_shapemodel = 'shapedirs' in dd\n    nposeparms = dd['kintree_table'].shape[1] * 3\n\n    if 'trans' not in dd:\n        dd['trans'] = np.zeros(3)\n    if 'pose' not in dd:\n        dd['pose'] = np.zeros(nposeparms)\n    if 'shapedirs' in dd and 'betas' not in dd:\n        dd['betas'] = np.zeros(dd['shapedirs'].shape[-1])\n\n    for s in [\n            'v_template', 'weights', 'posedirs', 'pose', 'trans', 'shapedirs',\n            'betas', 'J'\n    ]:\n        if (s in dd) and not hasattr(dd[s], 'dterms'):\n            dd[s] = ch.array(dd[s])\n\n    if want_shapemodel:\n        dd['v_shaped'] = dd['shapedirs'].dot(dd['betas']) + dd['v_template']\n        v_shaped = dd['v_shaped']\n        J_tmpx = MatVecMult(dd['J_regressor'], v_shaped[:, 0])\n        J_tmpy = MatVecMult(dd['J_regressor'], v_shaped[:, 1])\n        J_tmpz = MatVecMult(dd['J_regressor'], v_shaped[:, 2])\n        dd['J'] = ch.vstack((J_tmpx, J_tmpy, J_tmpz)).T\n        dd['v_posed'] = v_shaped + dd['posedirs'].dot(\n            posemap(dd['bs_type'])(dd['pose']))\n    else:\n        dd['v_posed'] = dd['v_template'] + dd['posedirs'].dot(\n            posemap(dd['bs_type'])(dd['pose']))\n\n    return dd\n\n\ndef load_model(fname_or_dict):\n    dd = ready_arguments(fname_or_dict)\n\n    args = {\n        'pose': dd['pose'],\n        'v': dd['v_posed'],\n        'J': dd['J'],\n        'weights': dd['weights'],\n        'kintree_table': dd['kintree_table'],\n        'xp': ch,\n        'want_Jtr': True,\n        'bs_style': dd['bs_style']\n    }\n\n    result, Jtr = verts_core(**args)\n    result = result + dd['trans'].reshape((1, 3))\n    result.J_transformed = Jtr + dd['trans'].reshape((1, 3))\n\n    for k, v in dd.items():\n        setattr(result, k, v)\n\n    return result\n"""
mano/webuser/smpl_handpca_wrapper_HAND_only.py,0,"b""'''\nCopyright 2017 Javier Romero, Dimitrios Tzionas, Michael J Black and the Max Planck Gesellschaft.  All rights reserved.\nThis software is provided for research purposes only.\nBy using this software you agree to the terms of the MANO/SMPL+H Model license here http://mano.is.tue.mpg.de/license\n\nMore information about MANO/SMPL+H is available at http://mano.is.tue.mpg.de.\nFor comments or questions, please email us at: mano@tue.mpg.de\n\n\nAbout this file:\n================\nThis file defines a wrapper for the loading functions of the MANO model.\n\nModules included:\n- load_model:\n  loads the MANO model from a given file location (i.e. a .pkl file location),\n  or a dictionary object.\n\n'''\n\n\ndef ready_arguments(fname_or_dict, posekey4vposed='pose'):\n    import numpy as np\n    import pickle\n    import chumpy as ch\n    from chumpy.ch import MatVecMult\n    from mano.webuser.posemapper import posemap\n\n    if not isinstance(fname_or_dict, dict):\n        dd = pickle.load(open(fname_or_dict, 'rb'), encoding='latin1')\n        # dd = pickle.load(open(fname_or_dict, 'rb'))\n    else:\n        dd = fname_or_dict\n\n    want_shapemodel = 'shapedirs' in dd\n    nposeparms = dd['kintree_table'].shape[1] * 3\n\n    if 'trans' not in dd:\n        dd['trans'] = np.zeros(3)\n    if 'pose' not in dd:\n        dd['pose'] = np.zeros(nposeparms)\n    if 'shapedirs' in dd and 'betas' not in dd:\n        dd['betas'] = np.zeros(dd['shapedirs'].shape[-1])\n\n    for s in [\n            'v_template', 'weights', 'posedirs', 'pose', 'trans', 'shapedirs',\n            'betas', 'J'\n    ]:\n        if (s in dd) and not hasattr(dd[s], 'dterms'):\n            dd[s] = ch.array(dd[s])\n\n    assert (posekey4vposed in dd)\n    if want_shapemodel:\n        dd['v_shaped'] = dd['shapedirs'].dot(dd['betas']) + dd['v_template']\n        v_shaped = dd['v_shaped']\n        J_tmpx = MatVecMult(dd['J_regressor'], v_shaped[:, 0])\n        J_tmpy = MatVecMult(dd['J_regressor'], v_shaped[:, 1])\n        J_tmpz = MatVecMult(dd['J_regressor'], v_shaped[:, 2])\n        dd['J'] = ch.vstack((J_tmpx, J_tmpy, J_tmpz)).T\n        pose_map_res = posemap(dd['bs_type'])(dd[posekey4vposed])\n        dd['v_posed'] = v_shaped + dd['posedirs'].dot(pose_map_res)\n    else:\n        pose_map_res = posemap(dd['bs_type'])(dd[posekey4vposed])\n        dd_add = dd['posedirs'].dot(pose_map_res)\n        dd['v_posed'] = dd['v_template'] + dd_add\n\n    return dd\n\n\ndef load_model(fname_or_dict, ncomps=6, flat_hand_mean=False, v_template=None):\n    ''' This model loads the fully articulable HAND SMPL model,\n    and replaces the pose DOFS by ncomps from PCA'''\n\n    from mano.webuser.verts import verts_core\n    import numpy as np\n    import chumpy as ch\n    import pickle\n    import scipy.sparse as sp\n    np.random.seed(1)\n\n    if not isinstance(fname_or_dict, dict):\n        smpl_data = pickle.load(open(fname_or_dict, 'rb'), encoding='latin1')\n        # smpl_data = pickle.load(open(fname_or_dict, 'rb'))\n    else:\n        smpl_data = fname_or_dict\n\n    rot = 3  # for global orientation!!!\n\n    hands_components = smpl_data['hands_components']\n    hands_mean = np.zeros(hands_components.shape[\n        1]) if flat_hand_mean else smpl_data['hands_mean']\n    hands_coeffs = smpl_data['hands_coeffs'][:, :ncomps]\n\n    selected_components = np.vstack((hands_components[:ncomps]))\n    hands_mean = hands_mean.copy()\n\n    pose_coeffs = ch.zeros(rot + selected_components.shape[0])\n    full_hand_pose = pose_coeffs[rot:(rot + ncomps)].dot(selected_components)\n\n    smpl_data['fullpose'] = ch.concatenate((pose_coeffs[:rot],\n                                            hands_mean + full_hand_pose))\n    smpl_data['pose'] = pose_coeffs\n\n    Jreg = smpl_data['J_regressor']\n    if not sp.issparse(Jreg):\n        smpl_data['J_regressor'] = (sp.csc_matrix(\n            (Jreg.data, (Jreg.row, Jreg.col)), shape=Jreg.shape))\n\n    # slightly modify ready_arguments to make sure that it uses the fullpose\n    # (which will NOT be pose) for the computation of posedirs\n    dd = ready_arguments(smpl_data, posekey4vposed='fullpose')\n\n    # create the smpl formula with the fullpose,\n    # but expose the PCA coefficients as smpl.pose for compatibility\n    args = {\n        'pose': dd['fullpose'],\n        'v': dd['v_posed'],\n        'J': dd['J'],\n        'weights': dd['weights'],\n        'kintree_table': dd['kintree_table'],\n        'xp': ch,\n        'want_Jtr': True,\n        'bs_style': dd['bs_style'],\n    }\n\n    result_previous, meta = verts_core(**args)\n\n    result = result_previous + dd['trans'].reshape((1, 3))\n    result.no_translation = result_previous\n\n    if meta is not None:\n        for field in ['Jtr', 'A', 'A_global', 'A_weighted']:\n            if (hasattr(meta, field)):\n                setattr(result, field, getattr(meta, field))\n\n    setattr(result, 'Jtr', meta)\n    if hasattr(result, 'Jtr'):\n        result.J_transformed = result.Jtr + dd['trans'].reshape((1, 3))\n\n    for k, v in dd.items():\n        setattr(result, k, v)\n\n    if v_template is not None:\n        result.v_template[:] = v_template\n\n    return result\n\n\nif __name__ == '__main__':\n    load_model()\n"""
mano/webuser/verts.py,0,"b""'''\nCopyright 2017 Javier Romero, Dimitrios Tzionas, Michael J Black and the Max Planck Gesellschaft.  All rights reserved.\nThis software is provided for research purposes only.\nBy using this software you agree to the terms of the MANO/SMPL+H Model license here http://mano.is.tue.mpg.de/license\n\nMore information about MANO/SMPL+H is available at http://mano.is.tue.mpg.de.\nFor comments or questions, please email us at: mano@tue.mpg.de\n\n\nAbout this file:\n================\nThis file defines a wrapper for the loading functions of the MANO model.\n\nModules included:\n- load_model:\n  loads the MANO model from a given file location (i.e. a .pkl file location),\n  or a dictionary object.\n\n'''\n\n\nimport chumpy\nimport mano.webuser.lbs as lbs\nfrom mano.webuser.posemapper import posemap\nimport scipy.sparse as sp\nfrom chumpy.ch import MatVecMult\n\n\ndef ischumpy(x):\n    return hasattr(x, 'dterms')\n\n\ndef verts_decorated(trans,\n                    pose,\n                    v_template,\n                    J_regressor,\n                    weights,\n                    kintree_table,\n                    bs_style,\n                    f,\n                    bs_type=None,\n                    posedirs=None,\n                    betas=None,\n                    shapedirs=None,\n                    want_Jtr=False):\n\n    for which in [\n            trans, pose, v_template, weights, posedirs, betas, shapedirs\n    ]:\n        if which is not None:\n            assert ischumpy(which)\n\n    v = v_template\n\n    if shapedirs is not None:\n        if betas is None:\n            betas = chumpy.zeros(shapedirs.shape[-1])\n        v_shaped = v + shapedirs.dot(betas)\n    else:\n        v_shaped = v\n\n    if posedirs is not None:\n        v_posed = v_shaped + posedirs.dot(posemap(bs_type)(pose))\n    else:\n        v_posed = v_shaped\n\n    v = v_posed\n\n    if sp.issparse(J_regressor):\n        J_tmpx = MatVecMult(J_regressor, v_shaped[:, 0])\n        J_tmpy = MatVecMult(J_regressor, v_shaped[:, 1])\n        J_tmpz = MatVecMult(J_regressor, v_shaped[:, 2])\n        J = chumpy.vstack((J_tmpx, J_tmpy, J_tmpz)).T\n    else:\n        assert (ischumpy(J))\n\n    assert (bs_style == 'lbs')\n    result, Jtr = lbs.verts_core(\n        pose, v, J, weights, kintree_table, want_Jtr=True, xp=chumpy)\n\n    tr = trans.reshape((1, 3))\n    result = result + tr\n    Jtr = Jtr + tr\n\n    result.trans = trans\n    result.f = f\n    result.pose = pose\n    result.v_template = v_template\n    result.J = J\n    result.J_regressor = J_regressor\n    result.weights = weights\n    result.kintree_table = kintree_table\n    result.bs_style = bs_style\n    result.bs_type = bs_type\n    if posedirs is not None:\n        result.posedirs = posedirs\n        result.v_posed = v_posed\n    if shapedirs is not None:\n        result.shapedirs = shapedirs\n        result.betas = betas\n        result.v_shaped = v_shaped\n    if want_Jtr:\n        result.J_transformed = Jtr\n    return result\n\n\ndef verts_core(pose,\n               v,\n               J,\n               weights,\n               kintree_table,\n               bs_style,\n               want_Jtr=False,\n               xp=chumpy):\n\n    if xp == chumpy:\n        assert (hasattr(pose, 'dterms'))\n        assert (hasattr(v, 'dterms'))\n        assert (hasattr(J, 'dterms'))\n        assert (hasattr(weights, 'dterms'))\n\n    assert (bs_style == 'lbs')\n    result = lbs.verts_core(pose, v, J, weights, kintree_table, want_Jtr, xp)\n    return result\n"""
