file_path,api_count,code
CIFAR/baseline.py,13,"b'# -*- coding: utf-8 -*-\nimport numpy as np\nimport os\nimport argparse\nimport time\nimport torch\nimport torch.nn as nn\nimport torch.backends.cudnn as cudnn\nimport torchvision.transforms as trn\nimport torchvision.datasets as dset\nimport torch.nn.functional as F\nfrom tqdm import tqdm\nfrom models.allconv import AllConvNet\nfrom models.wrn import WideResNet\n\n# go through rigamaroo to do ...utils.display_results import show_performance\nif __package__ is None:\n    import sys\n    from os import path\n\n    sys.path.append(path.dirname(path.dirname(path.abspath(__file__))))\n    from utils.validation_dataset import validation_split\n\nparser = argparse.ArgumentParser(description=\'Trains a CIFAR Classifier\',\n                                 formatter_class=argparse.ArgumentDefaultsHelpFormatter)\nparser.add_argument(\'dataset\', type=str, choices=[\'cifar10\', \'cifar100\'],\n                    help=\'Choose between CIFAR-10, CIFAR-100.\')\nparser.add_argument(\'--model\', \'-m\', type=str, default=\'allconv\',\n                    choices=[\'allconv\', \'wrn\'], help=\'Choose architecture.\')\nparser.add_argument(\'--calibration\', \'-c\', action=\'store_true\',\n                    help=\'Train a model to be used for calibration. This holds out some data for validation.\')\n# Optimization options\nparser.add_argument(\'--epochs\', \'-e\', type=int, default=100, help=\'Number of epochs to train.\')\nparser.add_argument(\'--learning_rate\', \'-lr\', type=float, default=0.1, help=\'The initial learning rate.\')\nparser.add_argument(\'--batch_size\', \'-b\', type=int, default=128, help=\'Batch size.\')\nparser.add_argument(\'--test_bs\', type=int, default=200)\nparser.add_argument(\'--momentum\', type=float, default=0.9, help=\'Momentum.\')\nparser.add_argument(\'--decay\', \'-d\', type=float, default=0.0005, help=\'Weight decay (L2 penalty).\')\n# WRN Architecture\nparser.add_argument(\'--layers\', default=40, type=int, help=\'total number of layers\')\nparser.add_argument(\'--widen-factor\', default=2, type=int, help=\'widen factor\')\nparser.add_argument(\'--droprate\', default=0.3, type=float, help=\'dropout probability\')\n# Checkpoints\nparser.add_argument(\'--save\', \'-s\', type=str, default=\'./snapshots/baseline\', help=\'Folder to save checkpoints.\')\nparser.add_argument(\'--load\', \'-l\', type=str, default=\'\', help=\'Checkpoint path to resume / test.\')\nparser.add_argument(\'--test\', \'-t\', action=\'store_true\', help=\'Test only flag.\')\n# Acceleration\nparser.add_argument(\'--ngpu\', type=int, default=1, help=\'0 = CPU.\')\nparser.add_argument(\'--prefetch\', type=int, default=4, help=\'Pre-fetching threads.\')\nargs = parser.parse_args()\n\nstate = {k: v for k, v in args._get_kwargs()}\nprint(state)\n\ntorch.manual_seed(1)\nnp.random.seed(1)\n\n# mean and standard deviation of channels of CIFAR-10 images\nmean = [x / 255 for x in [125.3, 123.0, 113.9]]\nstd = [x / 255 for x in [63.0, 62.1, 66.7]]\n\ntrain_transform = trn.Compose([trn.RandomHorizontalFlip(), trn.RandomCrop(32, padding=4),\n                               trn.ToTensor(), trn.Normalize(mean, std)])\ntest_transform = trn.Compose([trn.ToTensor(), trn.Normalize(mean, std)])\n\nif args.dataset == \'cifar10\':\n    train_data = dset.CIFAR10(\'/share/data/vision-greg/cifarpy\', train=True, transform=train_transform)\n    test_data = dset.CIFAR10(\'/share/data/vision-greg/cifarpy\', train=False, transform=test_transform)\n    num_classes = 10\nelse:\n    train_data = dset.CIFAR100(\'/share/data/vision-greg/cifarpy\', train=True, transform=train_transform)\n    test_data = dset.CIFAR100(\'/share/data/vision-greg/cifarpy\', train=False, transform=test_transform)\n    num_classes = 100\n\n\ncalib_indicator = \'\'\nif args.calibration:\n    train_data, val_data = validation_split(train_data, val_share=0.1)\n    calib_indicator = \'_calib\'\n\ntrain_loader = torch.utils.data.DataLoader(\n    train_data, batch_size=args.batch_size, shuffle=True,\n    num_workers=args.prefetch, pin_memory=True)\ntest_loader = torch.utils.data.DataLoader(\n    test_data, batch_size=args.test_bs, shuffle=False,\n    num_workers=args.prefetch, pin_memory=True)\n\n# Create model\nif args.model == \'allconv\':\n    net = AllConvNet(num_classes)\nelse:\n    net = WideResNet(args.layers, num_classes, args.widen_factor, dropRate=args.droprate)\n\nstart_epoch = 0\n\n# Restore model if desired\nif args.load != \'\':\n    for i in range(1000 - 1, -1, -1):\n        model_name = os.path.join(args.load, args.dataset + calib_indicator + \'_\' + args.model +\n                                  \'_baseline_epoch_\' + str(i) + \'.pt\')\n        if os.path.isfile(model_name):\n            net.load_state_dict(torch.load(model_name))\n            print(\'Model restored! Epoch:\', i)\n            start_epoch = i + 1\n            break\n    if start_epoch == 0:\n        assert False, ""could not resume""\n\nif args.ngpu > 1:\n    net = torch.nn.DataParallel(net, device_ids=list(range(args.ngpu)))\n\nif args.ngpu > 0:\n    net.cuda()\n    torch.cuda.manual_seed(1)\n\ncudnn.benchmark = True  # fire on all cylinders\n\noptimizer = torch.optim.SGD(\n    net.parameters(), state[\'learning_rate\'], momentum=state[\'momentum\'],\n    weight_decay=state[\'decay\'], nesterov=True)\n\n\ndef cosine_annealing(step, total_steps, lr_max, lr_min):\n    return lr_min + (lr_max - lr_min) * 0.5 * (\n            1 + np.cos(step / total_steps * np.pi))\n\n\nscheduler = torch.optim.lr_scheduler.LambdaLR(\n    optimizer,\n    lr_lambda=lambda step: cosine_annealing(\n        step,\n        args.epochs * len(train_loader),\n        1,  # since lr_lambda computes multiplicative factor\n        1e-6 / args.learning_rate))\n\n\n# /////////////// Training ///////////////\n\ndef train():\n    net.train()  # enter train mode\n    loss_avg = 0.0\n    for data, target in train_loader:\n        data, target = data.cuda(), target.cuda()\n\n        # forward\n        x = net(data)\n\n        # backward\n        scheduler.step()\n        optimizer.zero_grad()\n        loss = F.cross_entropy(x, target)\n        loss.backward()\n        optimizer.step()\n\n        # exponential moving average\n        loss_avg = loss_avg * 0.8 + float(loss) * 0.2\n\n    state[\'train_loss\'] = loss_avg\n\n\n# test function\ndef test():\n    net.eval()\n    loss_avg = 0.0\n    correct = 0\n    with torch.no_grad():\n        for data, target in test_loader:\n            data, target = data.cuda(), target.cuda()\n\n            # forward\n            output = net(data)\n            loss = F.cross_entropy(output, target)\n\n            # accuracy\n            pred = output.data.max(1)[1]\n            correct += pred.eq(target.data).sum().item()\n\n            # test loss average\n            loss_avg += float(loss.data)\n\n    state[\'test_loss\'] = loss_avg / len(test_loader)\n    state[\'test_accuracy\'] = correct / len(test_loader.dataset)\n\n\nif args.test:\n    test()\n    print(state)\n    exit()\n\n# Make save directory\nif not os.path.exists(args.save):\n    os.makedirs(args.save)\nif not os.path.isdir(args.save):\n    raise Exception(\'%s is not a dir\' % args.save)\n\nwith open(os.path.join(args.save, args.dataset + calib_indicator + \'_\' + args.model +\n                                  \'_baseline_training_results.csv\'), \'w\') as f:\n    f.write(\'epoch,time(s),train_loss,test_loss,test_error(%)\\n\')\n\nprint(\'Beginning Training\\n\')\n\n# Main loop\nfor epoch in range(start_epoch, args.epochs):\n    state[\'epoch\'] = epoch\n\n    begin_epoch = time.time()\n\n    train()\n    test()\n\n    # Save model\n    torch.save(net.state_dict(),\n               os.path.join(args.save, args.dataset + calib_indicator + \'_\' + args.model +\n                            \'_baseline_epoch_\' + str(epoch) + \'.pt\'))\n    # Let us not waste space and delete the previous model\n    prev_path = os.path.join(args.save, args.dataset + calib_indicator + \'_\' + args.model +\n                             \'_baseline_epoch_\' + str(epoch - 1) + \'.pt\')\n    if os.path.exists(prev_path): os.remove(prev_path)\n\n    # Show results\n\n    with open(os.path.join(args.save, args.dataset + calib_indicator + \'_\' + args.model +\n                                      \'_baseline_training_results.csv\'), \'a\') as f:\n        f.write(\'%03d,%05d,%0.6f,%0.5f,%0.2f\\n\' % (\n            (epoch + 1),\n            time.time() - begin_epoch,\n            state[\'train_loss\'],\n            state[\'test_loss\'],\n            100 - 100. * state[\'test_accuracy\'],\n        ))\n\n    # # print state with rounded decimals\n    # print({k: round(v, 4) if isinstance(v, float) else v for k, v in state.items()})\n\n    print(\'Epoch {0:3d} | Time {1:5d} | Train Loss {2:.4f} | Test Loss {3:.3f} | Test Error {4:.2f}\'.format(\n        (epoch + 1),\n        int(time.time() - begin_epoch),\n        state[\'train_loss\'],\n        state[\'test_loss\'],\n        100 - 100. * state[\'test_accuracy\'])\n    )\n'"
CIFAR/oe_scratch.py,16,"b'# -*- coding: utf-8 -*-\nimport numpy as np\nimport os\nimport pickle\nimport argparse\nimport time\nimport torch\nimport torch.nn as nn\nimport torch.backends.cudnn as cudnn\nimport torchvision.transforms as trn\nimport torchvision.datasets as dset\nimport torch.nn.functional as F\nfrom tqdm import tqdm\nfrom models.allconv import AllConvNet\nfrom models.wrn import WideResNet\n\nif __package__ is None:\n    import sys\n    from os import path\n\n    sys.path.append(path.dirname(path.dirname(path.abspath(__file__))))\n    from utils.tinyimages_80mn_loader import TinyImages\n    from utils.validation_dataset import validation_split\n\nparser = argparse.ArgumentParser(description=\'Trains a CIFAR Classifier with OE\',\n                                 formatter_class=argparse.ArgumentDefaultsHelpFormatter)\nparser.add_argument(\'dataset\', type=str, choices=[\'cifar10\', \'cifar100\'],\n                    help=\'Choose between CIFAR-10, CIFAR-100.\')\nparser.add_argument(\'--model\', \'-m\', type=str, default=\'allconv\',\n                    choices=[\'allconv\', \'wrn\'], help=\'Choose architecture.\')\nparser.add_argument(\'--calibration\', \'-c\', action=\'store_true\',\n                    help=\'Train a model to be used for calibration. This holds out some data for validation.\')\n# Optimization options\nparser.add_argument(\'--epochs\', \'-e\', type=int, default=100, help=\'Number of epochs to train.\')\nparser.add_argument(\'--learning_rate\', \'-lr\', type=float, default=0.1, help=\'The initial learning rate.\')\nparser.add_argument(\'--batch_size\', \'-b\', type=int, default=128, help=\'Batch size.\')\nparser.add_argument(\'--oe_batch_size\', type=int, default=256, help=\'Batch size.\')\nparser.add_argument(\'--test_bs\', type=int, default=200)\nparser.add_argument(\'--momentum\', type=float, default=0.9, help=\'Momentum.\')\nparser.add_argument(\'--decay\', \'-d\', type=float, default=0.0005, help=\'Weight decay (L2 penalty).\')\n# WRN Architecture\nparser.add_argument(\'--layers\', default=40, type=int, help=\'total number of layers\')\nparser.add_argument(\'--widen-factor\', default=2, type=int, help=\'widen factor\')\nparser.add_argument(\'--droprate\', default=0.3, type=float, help=\'dropout probability\')\n# Checkpoints\nparser.add_argument(\'--save\', \'-s\', type=str, default=\'./snapshots/oe_scratch\', help=\'Folder to save checkpoints.\')\nparser.add_argument(\'--load\', \'-l\', type=str, default=\'\', help=\'Checkpoint path to resume / test.\')\nparser.add_argument(\'--test\', \'-t\', action=\'store_true\', help=\'Test only flag.\')\n# Acceleration\nparser.add_argument(\'--ngpu\', type=int, default=1, help=\'0 = CPU.\')\nparser.add_argument(\'--prefetch\', type=int, default=4, help=\'Pre-fetching threads.\')\nargs = parser.parse_args()\n\nstate = {k: v for k, v in args._get_kwargs()}\nprint(state)\n\ntorch.manual_seed(1)\nnp.random.seed(1)\n\n# mean and standard deviation of channels of CIFAR-10 images\nmean = [x / 255 for x in [125.3, 123.0, 113.9]]\nstd = [x / 255 for x in [63.0, 62.1, 66.7]]\n\ntrain_transform = trn.Compose([trn.RandomHorizontalFlip(), trn.RandomCrop(32, padding=4),\n                               trn.ToTensor(), trn.Normalize(mean, std)])\ntest_transform = trn.Compose([trn.ToTensor(), trn.Normalize(mean, std)])\n\nif args.dataset == \'cifar10\':\n    train_data_in = dset.CIFAR10(\'/share/data/vision-greg/cifarpy\', train=True, transform=train_transform)\n    test_data = dset.CIFAR10(\'/share/data/vision-greg/cifarpy\', train=False, transform=test_transform)\n    num_classes = 10\nelse:\n    train_data_in = dset.CIFAR100(\'/share/data/vision-greg/cifarpy\', train=True, transform=train_transform)\n    test_data = dset.CIFAR100(\'/share/data/vision-greg/cifarpy\', train=False, transform=test_transform)\n    num_classes = 100\n\n\ncalib_indicator = \'\'\nif args.calibration:\n    train_data_in, val_data = validation_split(train_data_in, val_share=0.1)\n    calib_indicator = \'_calib\'\n\n\nood_data = TinyImages(transform=trn.Compose(\n    [trn.ToTensor(), trn.ToPILImage(), trn.RandomCrop(32, padding=4),\n     trn.RandomHorizontalFlip(), trn.ToTensor(), trn.Normalize(mean, std)]))\n\ntrain_loader_in = torch.utils.data.DataLoader(\n    train_data_in,\n    batch_size=args.batch_size, shuffle=True,\n    num_workers=args.prefetch, pin_memory=True)\n\ntrain_loader_out = torch.utils.data.DataLoader(\n    ood_data,\n    batch_size=args.oe_batch_size, shuffle=False,\n    num_workers=args.prefetch, pin_memory=True)\n\ntest_loader = torch.utils.data.DataLoader(\n    test_data,\n    batch_size=args.batch_size, shuffle=False,\n    num_workers=args.prefetch, pin_memory=True)\n\n# Create model\nif args.model == \'allconv\':\n    net = AllConvNet(num_classes)\nelse:\n    net = WideResNet(args.layers, num_classes, args.widen_factor, dropRate=args.droprate)\n\n\nstart_epoch = 0\n# Restore model if desired\nif args.load != \'\':\n    for i in range(1000 - 1, -1, -1):\n        model_name = os.path.join(args.load, args.dataset + calib_indicator + \'_\' + args.model +\n                                  \'_oe_scratch_epoch_\' + str(i) + \'.pt\')\n        if os.path.isfile(model_name):\n            net.load_state_dict(torch.load(model_name))\n            print(\'Model restored! Epoch:\', i)\n            start_epoch = i + 1\n            break\n    if start_epoch == 0:\n        assert False, ""could not resume""\n\nif args.ngpu > 1:\n    net = torch.nn.DataParallel(net, device_ids=list(range(args.ngpu)))\n\nif args.ngpu > 0:\n    net.cuda()\n    torch.cuda.manual_seed(1)\n\ncudnn.benchmark = True  # fire on all cylinders\n\noptimizer = torch.optim.SGD(\n    net.parameters(), state[\'learning_rate\'], momentum=state[\'momentum\'],\n    weight_decay=state[\'decay\'], nesterov=True)\n\n\ndef cosine_annealing(step, total_steps, lr_max, lr_min):\n    return lr_min + (lr_max - lr_min) * 0.5 * (\n            1 + np.cos(step / total_steps * np.pi))\n\n\nscheduler = torch.optim.lr_scheduler.LambdaLR(\n    optimizer,\n    lr_lambda=lambda step: cosine_annealing(\n        step,\n        args.epochs * len(train_loader_in),\n        1,  # since lr_lambda computes multiplicative factor\n        1e-6 / args.learning_rate))\n\n\n# /////////////// Training ///////////////\n\ndef train():\n    net.train()  # enter train mode\n    loss_avg = 0.0\n\n    # start at a random point of the outlier dataset; this induces more randomness without obliterating locality\n    train_loader_out.dataset.offset = np.random.randint(len(train_loader_out.dataset))\n    for in_set, out_set in zip(train_loader_in, train_loader_out):\n        data = torch.cat((in_set[0], out_set[0]), 0)\n        target = in_set[1]\n\n        data, target = data.cuda(), target.cuda()\n\n        # forward\n        x = net(data)\n\n        # backward\n        scheduler.step()\n        optimizer.zero_grad()\n\n        loss = F.cross_entropy(x[:len(in_set[0])], target)\n        # cross-entropy from softmax distribution to uniform distribution\n        loss += 0.5 * -(x[len(in_set[0]):].mean(1) - torch.logsumexp(x[len(in_set[0]):], dim=1)).mean()\n\n        loss.backward()\n        optimizer.step()\n\n        # exponential moving average\n        loss_avg = loss_avg * 0.8 + float(loss) * 0.2\n\n    state[\'train_loss\'] = loss_avg\n\n\n# test function\ndef test():\n    net.eval()\n    loss_avg = 0.0\n    correct = 0\n    with torch.no_grad():\n        for data, target in test_loader:\n            data, target = data.cuda(), target.cuda()\n\n            # forward\n            output = net(data)\n            loss = F.cross_entropy(output, target)\n\n            # accuracy\n            pred = output.data.max(1)[1]\n            correct += pred.eq(target.data).sum().item()\n\n            # test loss average\n            loss_avg += float(loss.data)\n\n    state[\'test_loss\'] = loss_avg / len(test_loader)\n    state[\'test_accuracy\'] = correct / len(test_loader.dataset)\n\n\nif args.test:\n    test()\n    print(state)\n    exit()\n\n# Make save directory\nif not os.path.exists(args.save):\n    os.makedirs(args.save)\nif not os.path.isdir(args.save):\n    raise Exception(\'%s is not a dir\' % args.save)\n\nwith open(os.path.join(args.save, args.dataset + calib_indicator + \'_\' + args.model +\n                                  \'_oe_scratch_training_results.csv\'), \'w\') as f:\n    f.write(\'epoch,time(s),train_loss,test_loss,test_error(%)\\n\')\n\nprint(\'Beginning Training\\n\')\n\n# Main loop\nfor epoch in range(start_epoch, args.epochs):\n    state[\'epoch\'] = epoch\n\n    begin_epoch = time.time()\n\n    train()\n    test()\n\n    # Save model\n    torch.save(net.state_dict(),\n               os.path.join(args.save, args.dataset + calib_indicator + \'_\' + args.model +\n                            \'_oe_scratch_epoch_\' + str(epoch) + \'.pt\'))\n    # Let us not waste space and delete the previous model\n    prev_path = os.path.join(args.save, args.dataset + calib_indicator + \'_\' + args.model +\n                             \'_oe_scratch_epoch_\' + str(epoch - 1) + \'.pt\')\n    if os.path.exists(prev_path): os.remove(prev_path)\n\n    # Show results\n\n    with open(os.path.join(args.save, args.dataset + calib_indicator + \'_\' + args.model +\n                                      \'_oe_scratch_training_results.csv\'), \'a\') as f:\n        f.write(\'%03d,%05d,%0.6f,%0.5f,%0.2f\\n\' % (\n            (epoch + 1),\n            time.time() - begin_epoch,\n            state[\'train_loss\'],\n            state[\'test_loss\'],\n            100 - 100. * state[\'test_accuracy\'],\n        ))\n\n    # # print state with rounded decimals\n    # print({k: round(v, 4) if isinstance(v, float) else v for k, v in state.items()})\n\n    print(\'Epoch {0:3d} | Time {1:5d} | Train Loss {2:.4f} | Test Loss {3:.3f} | Test Error {4:.2f}\'.format(\n        (epoch + 1),\n        int(time.time() - begin_epoch),\n        state[\'train_loss\'],\n        state[\'test_loss\'],\n        100 - 100. * state[\'test_accuracy\'])\n    )\n'"
CIFAR/oe_tune.py,16,"b'# -*- coding: utf-8 -*-\nimport numpy as np\nimport os\nimport pickle\nimport argparse\nimport time\nimport torch\nimport torch.nn as nn\nimport torch.backends.cudnn as cudnn\nimport torchvision.transforms as trn\nimport torchvision.datasets as dset\nimport torch.nn.functional as F\nfrom tqdm import tqdm\nfrom models.allconv import AllConvNet\nfrom models.wrn import WideResNet\n\nif __package__ is None:\n    import sys\n    from os import path\n\n    sys.path.append(path.dirname(path.dirname(path.abspath(__file__))))\n    from utils.tinyimages_80mn_loader import TinyImages\n    from utils.validation_dataset import validation_split\n\nparser = argparse.ArgumentParser(description=\'Tunes a CIFAR Classifier with OE\',\n                                 formatter_class=argparse.ArgumentDefaultsHelpFormatter)\nparser.add_argument(\'dataset\', type=str, choices=[\'cifar10\', \'cifar100\'],\n                    help=\'Choose between CIFAR-10, CIFAR-100.\')\nparser.add_argument(\'--model\', \'-m\', type=str, default=\'allconv\',\n                    choices=[\'allconv\', \'wrn\'], help=\'Choose architecture.\')\nparser.add_argument(\'--calibration\', \'-c\', action=\'store_true\',\n                    help=\'Train a model to be used for calibration. This holds out some data for validation.\')\n# Optimization options\nparser.add_argument(\'--epochs\', \'-e\', type=int, default=10, help=\'Number of epochs to train.\')\nparser.add_argument(\'--learning_rate\', \'-lr\', type=float, default=0.001, help=\'The initial learning rate.\')\nparser.add_argument(\'--batch_size\', \'-b\', type=int, default=128, help=\'Batch size.\')\nparser.add_argument(\'--oe_batch_size\', type=int, default=256, help=\'Batch size.\')\nparser.add_argument(\'--test_bs\', type=int, default=200)\nparser.add_argument(\'--momentum\', type=float, default=0.9, help=\'Momentum.\')\nparser.add_argument(\'--decay\', \'-d\', type=float, default=0.0005, help=\'Weight decay (L2 penalty).\')\n# WRN Architecture\nparser.add_argument(\'--layers\', default=40, type=int, help=\'total number of layers\')\nparser.add_argument(\'--widen-factor\', default=2, type=int, help=\'widen factor\')\nparser.add_argument(\'--droprate\', default=0.3, type=float, help=\'dropout probability\')\n# Checkpoints\nparser.add_argument(\'--save\', \'-s\', type=str, default=\'./snapshots/oe_tune\', help=\'Folder to save checkpoints.\')\nparser.add_argument(\'--load\', \'-l\', type=str, default=\'./snapshots/baseline\', help=\'Checkpoint path to resume / test.\')\nparser.add_argument(\'--test\', \'-t\', action=\'store_true\', help=\'Test only flag.\')\n# Acceleration\nparser.add_argument(\'--ngpu\', type=int, default=1, help=\'0 = CPU.\')\nparser.add_argument(\'--prefetch\', type=int, default=4, help=\'Pre-fetching threads.\')\nargs = parser.parse_args()\n\nstate = {k: v for k, v in args._get_kwargs()}\nprint(state)\n\ntorch.manual_seed(1)\nnp.random.seed(1)\n\n# mean and standard deviation of channels of CIFAR-10 images\nmean = [x / 255 for x in [125.3, 123.0, 113.9]]\nstd = [x / 255 for x in [63.0, 62.1, 66.7]]\n\ntrain_transform = trn.Compose([trn.RandomHorizontalFlip(), trn.RandomCrop(32, padding=4),\n                               trn.ToTensor(), trn.Normalize(mean, std)])\ntest_transform = trn.Compose([trn.ToTensor(), trn.Normalize(mean, std)])\n\nif args.dataset == \'cifar10\':\n    train_data_in = dset.CIFAR10(\'/share/data/vision-greg/cifarpy\', train=True, transform=train_transform)\n    test_data = dset.CIFAR10(\'/share/data/vision-greg/cifarpy\', train=False, transform=test_transform)\n    num_classes = 10\nelse:\n    train_data_in = dset.CIFAR100(\'/share/data/vision-greg/cifarpy\', train=True, transform=train_transform)\n    test_data = dset.CIFAR100(\'/share/data/vision-greg/cifarpy\', train=False, transform=test_transform)\n    num_classes = 100\n\n\ncalib_indicator = \'\'\nif args.calibration:\n    train_data_in, val_data = validation_split(train_data_in, val_share=0.1)\n    calib_indicator = \'_calib\'\n\n\nood_data = TinyImages(transform=trn.Compose(\n    [trn.ToTensor(), trn.ToPILImage(), trn.RandomCrop(32, padding=4),\n     trn.RandomHorizontalFlip(), trn.ToTensor(), trn.Normalize(mean, std)]))\n\ntrain_loader_in = torch.utils.data.DataLoader(\n    train_data_in,\n    batch_size=args.batch_size, shuffle=True,\n    num_workers=args.prefetch, pin_memory=True)\n\ntrain_loader_out = torch.utils.data.DataLoader(\n    ood_data,\n    batch_size=args.oe_batch_size, shuffle=False,\n    num_workers=args.prefetch, pin_memory=True)\n\ntest_loader = torch.utils.data.DataLoader(\n    test_data,\n    batch_size=args.batch_size, shuffle=False,\n    num_workers=args.prefetch, pin_memory=True)\n\n# Create model\nif args.model == \'allconv\':\n    net = AllConvNet(num_classes)\nelse:\n    net = WideResNet(args.layers, num_classes, args.widen_factor, dropRate=args.droprate)\n\n# Restore model\nmodel_found = False\nif args.load != \'\':\n    for i in range(1000 - 1, -1, -1):\n        model_name = os.path.join(args.load, args.dataset + calib_indicator + \'_\' + args.model +\n                                  \'_baseline_epoch_\' + str(i) + \'.pt\')\n        if os.path.isfile(model_name):\n            net.load_state_dict(torch.load(model_name))\n            print(\'Model restored! Epoch:\', i)\n            model_found = True\n            break\n    if not model_found:\n        assert False, ""could not find model to restore""\n\nif args.ngpu > 1:\n    net = torch.nn.DataParallel(net, device_ids=list(range(args.ngpu)))\n\nif args.ngpu > 0:\n    net.cuda()\n    torch.cuda.manual_seed(1)\n\ncudnn.benchmark = True  # fire on all cylinders\n\noptimizer = torch.optim.SGD(\n    net.parameters(), state[\'learning_rate\'], momentum=state[\'momentum\'],\n    weight_decay=state[\'decay\'], nesterov=True)\n\n\ndef cosine_annealing(step, total_steps, lr_max, lr_min):\n    return lr_min + (lr_max - lr_min) * 0.5 * (\n            1 + np.cos(step / total_steps * np.pi))\n\n\nscheduler = torch.optim.lr_scheduler.LambdaLR(\n    optimizer,\n    lr_lambda=lambda step: cosine_annealing(\n        step,\n        args.epochs * len(train_loader_in),\n        1,  # since lr_lambda computes multiplicative factor\n        1e-6 / args.learning_rate))\n\n\n# /////////////// Training ///////////////\n\ndef train():\n    net.train()  # enter train mode\n    loss_avg = 0.0\n\n    # start at a random point of the outlier dataset; this induces more randomness without obliterating locality\n    train_loader_out.dataset.offset = np.random.randint(len(train_loader_out.dataset))\n    for in_set, out_set in zip(train_loader_in, train_loader_out):\n        data = torch.cat((in_set[0], out_set[0]), 0)\n        target = in_set[1]\n\n        data, target = data.cuda(), target.cuda()\n\n        # forward\n        x = net(data)\n\n        # backward\n        scheduler.step()\n        optimizer.zero_grad()\n\n        loss = F.cross_entropy(x[:len(in_set[0])], target)\n        # cross-entropy from softmax distribution to uniform distribution\n        loss += 0.5 * -(x[len(in_set[0]):].mean(1) - torch.logsumexp(x[len(in_set[0]):], dim=1)).mean()\n\n        loss.backward()\n        optimizer.step()\n\n        # exponential moving average\n        loss_avg = loss_avg * 0.8 + float(loss) * 0.2\n\n    state[\'train_loss\'] = loss_avg\n\n\n# test function\ndef test():\n    net.eval()\n    loss_avg = 0.0\n    correct = 0\n    with torch.no_grad():\n        for data, target in test_loader:\n            data, target = data.cuda(), target.cuda()\n\n            # forward\n            output = net(data)\n            loss = F.cross_entropy(output, target)\n\n            # accuracy\n            pred = output.data.max(1)[1]\n            correct += pred.eq(target.data).sum().item()\n\n            # test loss average\n            loss_avg += float(loss.data)\n\n    state[\'test_loss\'] = loss_avg / len(test_loader)\n    state[\'test_accuracy\'] = correct / len(test_loader.dataset)\n\n\nif args.test:\n    test()\n    print(state)\n    exit()\n\n# Make save directory\nif not os.path.exists(args.save):\n    os.makedirs(args.save)\nif not os.path.isdir(args.save):\n    raise Exception(\'%s is not a dir\' % args.save)\n\nwith open(os.path.join(args.save, args.dataset + calib_indicator + \'_\' + args.model +\n                                  \'_oe_tune_training_results.csv\'), \'w\') as f:\n    f.write(\'epoch,time(s),train_loss,test_loss,test_error(%)\\n\')\n\nprint(\'Beginning Training\\n\')\n\n# Main loop\nfor epoch in range(0, args.epochs):\n    state[\'epoch\'] = epoch\n\n    begin_epoch = time.time()\n\n    train()\n    test()\n\n    # Save model\n    torch.save(net.state_dict(),\n               os.path.join(args.save, args.dataset + calib_indicator + \'_\' + args.model +\n                            \'_oe_tune_epoch_\' + str(epoch) + \'.pt\'))\n    # Let us not waste space and delete the previous model\n    prev_path = os.path.join(args.save, args.dataset + calib_indicator + \'_\' + args.model +\n                             \'_oe_tune_epoch_\' + str(epoch - 1) + \'.pt\')\n    if os.path.exists(prev_path): os.remove(prev_path)\n\n    # Show results\n\n    with open(os.path.join(args.save, args.dataset + calib_indicator + \'_\' + args.model +\n                                      \'_oe_tune_training_results.csv\'), \'a\') as f:\n        f.write(\'%03d,%05d,%0.6f,%0.5f,%0.2f\\n\' % (\n            (epoch + 1),\n            time.time() - begin_epoch,\n            state[\'train_loss\'],\n            state[\'test_loss\'],\n            100 - 100. * state[\'test_accuracy\'],\n        ))\n\n    # # print state with rounded decimals\n    # print({k: round(v, 4) if isinstance(v, float) else v for k, v in state.items()})\n\n    print(\'Epoch {0:3d} | Time {1:5d} | Train Loss {2:.4f} | Test Loss {3:.3f} | Test Error {4:.2f}\'.format(\n        (epoch + 1),\n        int(time.time() - begin_epoch),\n        state[\'train_loss\'],\n        state[\'test_loss\'],\n        100 - 100. * state[\'test_accuracy\'])\n    )\n'"
CIFAR/test.py,46,"b'import numpy as np\nimport sys\nimport os\nimport pickle\nimport argparse\nimport torch\nimport torch.nn as nn\nimport torch.backends.cudnn as cudnn\nimport torchvision.transforms as trn\nimport torchvision.datasets as dset\nimport torch.nn.functional as F\nfrom models.allconv import AllConvNet\nfrom models.wrn import WideResNet\nfrom skimage.filters import gaussian as gblur\nfrom PIL import Image as PILImage\n\n# go through rigamaroo to do ...utils.display_results import show_performance\nif __package__ is None:\n    import sys\n    from os import path\n\n    sys.path.append(path.dirname(path.dirname(path.abspath(__file__))))\n    from utils.display_results import show_performance, get_measures, print_measures, print_measures_with_std\n    import utils.svhn_loader as svhn\n    import utils.lsun_loader as lsun_loader\n\nparser = argparse.ArgumentParser(description=\'Evaluates a CIFAR OOD Detector\',\n                                 formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n# Setup\nparser.add_argument(\'--test_bs\', type=int, default=200)\nparser.add_argument(\'--num_to_avg\', type=int, default=1, help=\'Average measures across num_to_avg runs.\')\nparser.add_argument(\'--validate\', \'-v\', action=\'store_true\', help=\'Evaluate performance on validation distributions.\')\nparser.add_argument(\'--use_xent\', \'-x\', action=\'store_true\', help=\'Use cross entropy scoring instead of the MSP.\')\nparser.add_argument(\'--method_name\', \'-m\', type=str, default=\'cifar10_allconv_baseline\', help=\'Method name.\')\n# Loading details\nparser.add_argument(\'--layers\', default=40, type=int, help=\'total number of layers\')\nparser.add_argument(\'--widen-factor\', default=2, type=int, help=\'widen factor\')\nparser.add_argument(\'--droprate\', default=0.3, type=float, help=\'dropout probability\')\nparser.add_argument(\'--load\', \'-l\', type=str, default=\'./snapshots\', help=\'Checkpoint path to resume / test.\')\nparser.add_argument(\'--ngpu\', type=int, default=1, help=\'0 = CPU.\')\nparser.add_argument(\'--prefetch\', type=int, default=4, help=\'Pre-fetching threads.\')\nargs = parser.parse_args()\n\n# torch.manual_seed(1)\n# np.random.seed(1)\n\n# mean and standard deviation of channels of CIFAR-10 images\nmean = [x / 255 for x in [125.3, 123.0, 113.9]]\nstd = [x / 255 for x in [63.0, 62.1, 66.7]]\n\ntest_transform = trn.Compose([trn.ToTensor(), trn.Normalize(mean, std)])\n\nif \'cifar10_\' in args.method_name:\n    test_data = dset.CIFAR10(\'/share/data/vision-greg/cifarpy\', train=False, transform=test_transform)\n    num_classes = 10\nelse:\n    test_data = dset.CIFAR100(\'/share/data/vision-greg/cifarpy\', train=False, transform=test_transform)\n    num_classes = 100\n\n\ntest_loader = torch.utils.data.DataLoader(test_data, batch_size=args.test_bs, shuffle=False,\n                                          num_workers=args.prefetch, pin_memory=True)\n\n# Create model\nif \'allconv\' in args.method_name:\n    net = AllConvNet(num_classes)\nelse:\n    net = WideResNet(args.layers, num_classes, args.widen_factor, dropRate=args.droprate)\n\nstart_epoch = 0\n\n# Restore model\nif args.load != \'\':\n    for i in range(1000 - 1, -1, -1):\n        if \'baseline\' in args.method_name:\n            subdir = \'baseline\'\n        elif \'oe_tune\' in args.method_name:\n            subdir = \'oe_tune\'\n        else:\n            subdir = \'oe_scratch\'\n\n        model_name = os.path.join(os.path.join(args.load, subdir), args.method_name + \'_epoch_\' + str(i) + \'.pt\')\n        if os.path.isfile(model_name):\n            net.load_state_dict(torch.load(model_name))\n            print(\'Model restored! Epoch:\', i)\n            start_epoch = i + 1\n            break\n    if start_epoch == 0:\n        assert False, ""could not resume""\n\nnet.eval()\n\nif args.ngpu > 1:\n    net = torch.nn.DataParallel(net, device_ids=list(range(args.ngpu)))\n\nif args.ngpu > 0:\n    net.cuda()\n    # torch.cuda.manual_seed(1)\n\ncudnn.benchmark = True  # fire on all cylinders\n\n# /////////////// Detection Prelims ///////////////\n\nood_num_examples = len(test_data) // 5\nexpected_ap = ood_num_examples / (ood_num_examples + len(test_data))\n\nconcat = lambda x: np.concatenate(x, axis=0)\nto_np = lambda x: x.data.cpu().numpy()\n\n\ndef get_ood_scores(loader, in_dist=False):\n    _score = []\n    _right_score = []\n    _wrong_score = []\n\n    with torch.no_grad():\n        for batch_idx, (data, target) in enumerate(loader):\n            if batch_idx >= ood_num_examples // args.test_bs and in_dist is False:\n                break\n\n            data = data.cuda()\n\n            output = net(data)\n            smax = to_np(F.softmax(output, dim=1))\n\n            if args.use_xent:\n                _score.append(to_np((output.mean(1) - torch.logsumexp(output, dim=1))))\n            else:\n                _score.append(-np.max(smax, axis=1))\n\n            if in_dist:\n                preds = np.argmax(smax, axis=1)\n                targets = target.numpy().squeeze()\n                right_indices = preds == targets\n                wrong_indices = np.invert(right_indices)\n\n                if args.use_xent:\n                    _right_score.append(to_np((output.mean(1) - torch.logsumexp(output, dim=1)))[right_indices])\n                    _wrong_score.append(to_np((output.mean(1) - torch.logsumexp(output, dim=1)))[wrong_indices])\n                else:\n                    _right_score.append(-np.max(smax[right_indices], axis=1))\n                    _wrong_score.append(-np.max(smax[wrong_indices], axis=1))\n\n    if in_dist:\n        return concat(_score).copy(), concat(_right_score).copy(), concat(_wrong_score).copy()\n    else:\n        return concat(_score)[:ood_num_examples].copy()\n\n\nin_score, right_score, wrong_score = get_ood_scores(test_loader, in_dist=True)\n\nnum_right = len(right_score)\nnum_wrong = len(wrong_score)\nprint(\'Error Rate {:.2f}\'.format(100 * num_wrong / (num_wrong + num_right)))\n\n# /////////////// End Detection Prelims ///////////////\n\nprint(\'\\nUsing CIFAR-10 as typical data\') if num_classes == 10 else print(\'\\nUsing CIFAR-100 as typical data\')\n\n# /////////////// Error Detection ///////////////\n\nprint(\'\\n\\nError Detection\')\nshow_performance(wrong_score, right_score, method_name=args.method_name)\n\n# /////////////// OOD Detection ///////////////\nauroc_list, aupr_list, fpr_list = [], [], []\n\n\ndef get_and_print_results(ood_loader, num_to_avg=args.num_to_avg):\n\n    aurocs, auprs, fprs = [], [], []\n    for _ in range(num_to_avg):\n        out_score = get_ood_scores(ood_loader)\n        measures = get_measures(out_score, in_score)\n        aurocs.append(measures[0]); auprs.append(measures[1]); fprs.append(measures[2])\n\n    auroc = np.mean(aurocs); aupr = np.mean(auprs); fpr = np.mean(fprs)\n    auroc_list.append(auroc); aupr_list.append(aupr); fpr_list.append(fpr)\n\n    if num_to_avg >= 5:\n        print_measures_with_std(aurocs, auprs, fprs, args.method_name)\n    else:\n        print_measures(auroc, aupr, fpr, args.method_name)\n\n\n# /////////////// Gaussian Noise ///////////////\n\ndummy_targets = torch.ones(ood_num_examples * args.num_to_avg)\nood_data = torch.from_numpy(np.float32(np.clip(\n    np.random.normal(size=(ood_num_examples * args.num_to_avg, 3, 32, 32), scale=0.5), -1, 1)))\nood_data = torch.utils.data.TensorDataset(ood_data, dummy_targets)\nood_loader = torch.utils.data.DataLoader(ood_data, batch_size=args.test_bs, shuffle=True,\n                                         num_workers=args.prefetch, pin_memory=True)\n\nprint(\'\\n\\nGaussian Noise (sigma = 0.5) Detection\')\nget_and_print_results(ood_loader)\n\n# /////////////// Rademacher Noise ///////////////\n\ndummy_targets = torch.ones(ood_num_examples * args.num_to_avg)\nood_data = torch.from_numpy(np.random.binomial(\n    n=1, p=0.5, size=(ood_num_examples * args.num_to_avg, 3, 32, 32)).astype(np.float32)) * 2 - 1\nood_data = torch.utils.data.TensorDataset(ood_data, dummy_targets)\nood_loader = torch.utils.data.DataLoader(ood_data, batch_size=args.test_bs, shuffle=True)\n\nprint(\'\\n\\nRademacher Noise Detection\')\nget_and_print_results(ood_loader)\n\n# /////////////// Blob ///////////////\n\nood_data = np.float32(np.random.binomial(n=1, p=0.7, size=(ood_num_examples * args.num_to_avg, 32, 32, 3)))\nfor i in range(ood_num_examples * args.num_to_avg):\n    ood_data[i] = gblur(ood_data[i], sigma=1.5, multichannel=False)\n    ood_data[i][ood_data[i] < 0.75] = 0.0\n\ndummy_targets = torch.ones(ood_num_examples * args.num_to_avg)\nood_data = torch.from_numpy(ood_data.transpose((0, 3, 1, 2))) * 2 - 1\nood_data = torch.utils.data.TensorDataset(ood_data, dummy_targets)\nood_loader = torch.utils.data.DataLoader(ood_data, batch_size=args.test_bs, shuffle=True,\n                                         num_workers=args.prefetch, pin_memory=True)\n\nprint(\'\\n\\nBlob Detection\')\nget_and_print_results(ood_loader)\n\n# /////////////// Textures ///////////////\n\nood_data = dset.ImageFolder(root=""/share/data/vision-greg2/users/dan/datasets/dtd/images"",\n                            transform=trn.Compose([trn.Resize(32), trn.CenterCrop(32),\n                                                   trn.ToTensor(), trn.Normalize(mean, std)]))\nood_loader = torch.utils.data.DataLoader(ood_data, batch_size=args.test_bs, shuffle=True,\n                                         num_workers=args.prefetch, pin_memory=True)\n\nprint(\'\\n\\nTexture Detection\')\nget_and_print_results(ood_loader)\n\n# /////////////// SVHN ///////////////\n\nood_data = svhn.SVHN(root=\'/share/data/vision-greg/svhn/\', split=""test"",\n                     transform=trn.Compose([trn.Resize(32), trn.ToTensor(), trn.Normalize(mean, std)]), download=False)\nood_loader = torch.utils.data.DataLoader(ood_data, batch_size=args.test_bs, shuffle=True,\n                                         num_workers=args.prefetch, pin_memory=True)\n\nprint(\'\\n\\nSVHN Detection\')\nget_and_print_results(ood_loader)\n\n# /////////////// Places365 ///////////////\n\nood_data = dset.ImageFolder(root=""/share/data/vision-greg2/places365/test_subset"",\n                            transform=trn.Compose([trn.Resize(32), trn.CenterCrop(32),\n                                                   trn.ToTensor(), trn.Normalize(mean, std)]))\nood_loader = torch.utils.data.DataLoader(ood_data, batch_size=args.test_bs, shuffle=True,\n                                         num_workers=args.prefetch, pin_memory=True)\n\nprint(\'\\n\\nPlaces365 Detection\')\nget_and_print_results(ood_loader)\n\n# /////////////// LSUN ///////////////\n\nood_data = lsun_loader.LSUN(""/share/data/vision-greg2/users/dan/datasets/LSUN/lsun-master/data"", classes=\'test\',\n                            transform=trn.Compose([trn.Resize(32), trn.CenterCrop(32),\n                                                   trn.ToTensor(), trn.Normalize(mean, std)]))\nood_loader = torch.utils.data.DataLoader(ood_data, batch_size=args.test_bs, shuffle=True,\n                                         num_workers=args.prefetch, pin_memory=True)\n\nprint(\'\\n\\nLSUN Detection\')\nget_and_print_results(ood_loader)\n\n# /////////////// CIFAR Data ///////////////\n\nif \'cifar10_\' in args.method_name:\n    ood_data = dset.CIFAR100(\'/share/data/vision-greg/cifarpy\', train=False, transform=test_transform)\nelse:\n    ood_data = dset.CIFAR10(\'/share/data/vision-greg/cifarpy\', train=False, transform=test_transform)\n\nood_loader = torch.utils.data.DataLoader(ood_data, batch_size=args.test_bs, shuffle=True,\n                                         num_workers=args.prefetch, pin_memory=True)\n\n\nprint(\'\\n\\nCIFAR-100 Detection\') if \'cifar100\' in args.method_name else print(\'\\n\\nCIFAR-10 Detection\')\nget_and_print_results(ood_loader)\n\n# /////////////// Mean Results ///////////////\n\nprint(\'\\n\\nMean Test Results\')\nprint_measures(np.mean(auroc_list), np.mean(aupr_list), np.mean(fpr_list), method_name=args.method_name)\n\n# /////////////// OOD Detection of Validation Distributions ///////////////\n\nif args.validate is False:\n    exit()\n\nauroc_list, aupr_list, fpr_list = [], [], []\n\n# /////////////// Uniform Noise ///////////////\n\ndummy_targets = torch.ones(ood_num_examples * args.num_to_avg)\nood_data = torch.from_numpy(\n    np.random.uniform(size=(ood_num_examples * args.num_to_avg, 3, 32, 32),\n                      low=-1.0, high=1.0).astype(np.float32))\nood_data = torch.utils.data.TensorDataset(ood_data, dummy_targets)\nood_loader = torch.utils.data.DataLoader(ood_data, batch_size=args.test_bs, shuffle=True)\n\nprint(\'\\n\\nUniform[-1,1] Noise Detection\')\nget_and_print_results(ood_loader)\n\n\n# /////////////// Arithmetic Mean of Images ///////////////\n\nif \'cifar10_\' in args.method_name:\n    ood_data = dset.CIFAR100(\'/share/data/vision-greg/cifarpy\', train=False, transform=test_transform)\nelse:\n    ood_data = dset.CIFAR10(\'/share/data/vision-greg/cifarpy\', train=False, transform=test_transform)\n\n\nclass AvgOfPair(torch.utils.data.Dataset):\n    def __init__(self, dataset):\n        self.dataset = dataset\n        self.shuffle_indices = np.arange(len(dataset))\n        np.random.shuffle(self.shuffle_indices)\n\n    def __getitem__(self, i):\n        random_idx = np.random.choice(len(self.dataset))\n        while random_idx == i:\n            random_idx = np.random.choice(len(self.dataset))\n\n        return self.dataset[i][0] / 2. + self.dataset[random_idx][0] / 2., 0\n\n    def __len__(self):\n        return len(self.dataset)\n\n\nood_loader = torch.utils.data.DataLoader(AvgOfPair(ood_data),\n                                         batch_size=args.test_bs, shuffle=True,\n                                         num_workers=args.prefetch, pin_memory=True)\n\nprint(\'\\n\\nArithmetic Mean of Random Image Pair Detection\')\nget_and_print_results(ood_loader)\n\n\n# /////////////// Geometric Mean of Images ///////////////\n\nif \'cifar10_\' in args.method_name:\n    ood_data = dset.CIFAR100(\'/share/data/vision-greg/cifarpy\', train=False, transform=trn.ToTensor())\nelse:\n    ood_data = dset.CIFAR10(\'/share/data/vision-greg/cifarpy\', train=False, transform=trn.ToTensor())\n\n\nclass GeomMeanOfPair(torch.utils.data.Dataset):\n    def __init__(self, dataset):\n        self.dataset = dataset\n        self.shuffle_indices = np.arange(len(dataset))\n        np.random.shuffle(self.shuffle_indices)\n\n    def __getitem__(self, i):\n        random_idx = np.random.choice(len(self.dataset))\n        while random_idx == i:\n            random_idx = np.random.choice(len(self.dataset))\n\n        return trn.Normalize(mean, std)(torch.sqrt(self.dataset[i][0] * self.dataset[random_idx][0])), 0\n\n    def __len__(self):\n        return len(self.dataset)\n\n\nood_loader = torch.utils.data.DataLoader(\n    GeomMeanOfPair(ood_data), batch_size=args.test_bs, shuffle=True,\n    num_workers=args.prefetch, pin_memory=True)\n\nprint(\'\\n\\nGeometric Mean of Random Image Pair Detection\')\nget_and_print_results(ood_loader)\n\n# /////////////// Jigsaw Images ///////////////\n\nood_loader = torch.utils.data.DataLoader(ood_data, batch_size=args.test_bs, shuffle=True,\n                                         num_workers=args.prefetch, pin_memory=True)\n\njigsaw = lambda x: torch.cat((\n    torch.cat((torch.cat((x[:, 8:16, :16], x[:, :8, :16]), 1),\n               x[:, 16:, :16]), 2),\n    torch.cat((x[:, 16:, 16:],\n               torch.cat((x[:, :16, 24:], x[:, :16, 16:24]), 2)), 2),\n), 1)\n\nood_loader.dataset.transform = trn.Compose([trn.ToTensor(), jigsaw, trn.Normalize(mean, std)])\n\nprint(\'\\n\\nJigsawed Images Detection\')\nget_and_print_results(ood_loader)\n\n# /////////////// Speckled Images ///////////////\n\nspeckle = lambda x: torch.clamp(x + x * torch.randn_like(x), 0, 1)\nood_loader.dataset.transform = trn.Compose([trn.ToTensor(), speckle, trn.Normalize(mean, std)])\n\nprint(\'\\n\\nSpeckle Noised Images Detection\')\nget_and_print_results(ood_loader)\n\n# /////////////// Pixelated Images ///////////////\n\npixelate = lambda x: x.resize((int(32 * 0.2), int(32 * 0.2)), PILImage.BOX).resize((32, 32), PILImage.BOX)\nood_loader.dataset.transform = trn.Compose([pixelate, trn.ToTensor(), trn.Normalize(mean, std)])\n\nprint(\'\\n\\nPixelate Detection\')\nget_and_print_results(ood_loader)\n\n# /////////////// RGB Ghosted/Shifted Images ///////////////\n\nrgb_shift = lambda x: torch.cat((x[1:2].index_select(2, torch.LongTensor([i for i in range(32 - 1, -1, -1)])),\n                                 x[2:, :, :], x[0:1, :, :]), 0)\nood_loader.dataset.transform = trn.Compose([trn.ToTensor(), rgb_shift, trn.Normalize(mean, std)])\n\nprint(\'\\n\\nRGB Ghosted/Shifted Image Detection\')\nget_and_print_results(ood_loader)\n\n# /////////////// Inverted Images ///////////////\n\n# not done on all channels to make image ood with higher probability\ninvert = lambda x: torch.cat((x[0:1, :, :], 1 - x[1:2, :, ], 1 - x[2:, :, :],), 0)\nood_loader.dataset.transform = trn.Compose([trn.ToTensor(), invert, trn.Normalize(mean, std)])\n\nprint(\'\\n\\nInverted Image Detection\')\nget_and_print_results(ood_loader)\n\n# /////////////// Mean Results ///////////////\n\nprint(\'\\n\\nMean Validation Results\')\nprint_measures(np.mean(auroc_list), np.mean(aupr_list), np.mean(fpr_list), method_name=args.method_name)\n'"
CIFAR/test_calibration.py,27,"b'import numpy as np\nimport sys\nimport os\nimport pickle\nimport argparse\nimport torch\nimport torch.nn as nn\nimport torch.backends.cudnn as cudnn\nimport torchvision.transforms as trn\nimport torchvision.datasets as dset\nimport torch.nn.functional as F\nfrom models.allconv import AllConvNet\nfrom models.wrn import WideResNet\nfrom skimage.filters import gaussian as gblur\nfrom PIL import Image as PILImage\n\n# go through rigamaroo to do ...utils.display_results import show_performance\nif __package__ is None:\n    import sys\n    from os import path\n\n    sys.path.append(path.dirname(path.dirname(path.abspath(__file__))))\n    from utils.display_results import show_performance, get_measures, print_measures, print_measures_with_std\n    import utils.svhn_loader as svhn\n    import utils.lsun_loader as lsun_loader\n    from utils.validation_dataset import validation_split\n    from utils.calibration_tools import *\n\nparser = argparse.ArgumentParser(description=\'Evaluates a CIFAR OOD Detector\',\n                                 formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n# Setup\nparser.add_argument(\'--test_bs\', type=int, default=200)\nparser.add_argument(\'--num_to_avg\', type=int, default=1, help=\'Average measures across num_to_avg runs.\')\nparser.add_argument(\'--validate\', \'-v\', action=\'store_true\', help=\'Evaluate performance on validation distributions.\')\nparser.add_argument(\'--method_name\', \'-m\', type=str, default=\'cifar10_allconv_calib_baseline\', help=\'Method name.\')\nparser.add_argument(\'--use_01\', \'-z\', action=\'store_true\', help=\'Use 0-1 Posterior Rescaling.\')\n# Loading details\nparser.add_argument(\'--layers\', default=40, type=int, help=\'total number of layers\')\nparser.add_argument(\'--widen-factor\', default=2, type=int, help=\'widen factor\')\nparser.add_argument(\'--droprate\', default=0.3, type=float, help=\'dropout probability\')\nparser.add_argument(\'--load\', \'-l\', type=str, default=\'./snapshots\', help=\'Checkpoint path to resume / test.\')\nparser.add_argument(\'--ngpu\', type=int, default=1, help=\'0 = CPU.\')\nparser.add_argument(\'--prefetch\', type=int, default=4, help=\'Pre-fetching threads.\')\nargs = parser.parse_args()\n\n# torch.manual_seed(1)\n# np.random.seed(1)\n\n# mean and standard deviation of channels of CIFAR-10 images\nmean = [x / 255 for x in [125.3, 123.0, 113.9]]\nstd = [x / 255 for x in [63.0, 62.1, 66.7]]\n\ntest_transform = trn.Compose([trn.ToTensor(), trn.Normalize(mean, std)])\n\nif \'cifar10_\' in args.method_name:\n    train_data = dset.CIFAR10(\'/share/data/vision-greg/cifarpy\', train=True, transform=test_transform)\n    test_data = dset.CIFAR10(\'/share/data/vision-greg/cifarpy\', train=False, transform=test_transform)\n    num_classes = 10\nelse:\n    train_data = dset.CIFAR100(\'/share/data/vision-greg/cifarpy\', train=True, transform=test_transform)\n    test_data = dset.CIFAR100(\'/share/data/vision-greg/cifarpy\', train=False, transform=test_transform)\n    num_classes = 100\n\ntrain_data, val_data = validation_split(train_data, val_share=0.1)\nval_loader = torch.utils.data.DataLoader(val_data, batch_size=args.test_bs, shuffle=False,\n                                         num_workers=args.prefetch, pin_memory=True)\ntest_loader = torch.utils.data.DataLoader(test_data, batch_size=args.test_bs, shuffle=False,\n                                          num_workers=args.prefetch, pin_memory=True)\n\n# Create model\nif \'allconv\' in args.method_name:\n    net = AllConvNet(num_classes)\nelse:\n    net = WideResNet(args.layers, num_classes, args.widen_factor, dropRate=args.droprate)\n\nstart_epoch = 0\n\n# Restore model\nif args.load != \'\':\n    for i in range(1000 - 1, -1, -1):\n        if \'baseline\' in args.method_name:\n            subdir = \'baseline\'\n        elif \'oe_tune\' in args.method_name:\n            subdir = \'oe_tune\'\n        else:\n            subdir = \'oe_scratch\'\n\n        model_name = os.path.join(os.path.join(args.load, subdir), args.method_name + \'_epoch_\' + str(i) + \'.pt\')\n        if os.path.isfile(model_name):\n            net.load_state_dict(torch.load(model_name))\n            print(\'Model restored! Epoch:\', i)\n            start_epoch = i + 1\n            break\n    if start_epoch == 0:\n        assert False, ""could not resume""\n\nnet.eval()\n\nif args.ngpu > 1:\n    net = torch.nn.DataParallel(net, device_ids=list(range(args.ngpu)))\n\nif args.ngpu > 0:\n    net.cuda()\n    # torch.cuda.manual_seed(1)\n\ncudnn.benchmark = True  # fire on all cylinders\n\n# /////////////// Calibration Prelims ///////////////\n\nood_num_examples = len(test_data) // 5\nexpected_ap = ood_num_examples / (ood_num_examples + len(test_data))\n\nconcat = lambda x: np.concatenate(x, axis=0)\nto_np = lambda x: x.data.to(\'cpu\').numpy()\n\n\ndef get_net_results(data_loader, in_dist=False, t=1):\n    logits = []\n    confidence = []\n    correct = []\n    labels = []\n\n    with torch.no_grad():\n        for batch_idx, (data, target) in enumerate(data_loader):\n            if batch_idx >= ood_num_examples // args.test_bs and in_dist is False:\n                break\n            data, target = data.cuda(), target.cuda()\n\n            output = net(data)\n\n            logits.extend(to_np(output).squeeze())\n\n            if args.use_01:\n                confidence.extend(to_np(\n                    (F.softmax(output/t, dim=1).max(1)[0] - 1./num_classes)/(1 - 1./num_classes)\n                ).squeeze().tolist())\n            else:\n                confidence.extend(to_np(F.softmax(output/t, dim=1).max(1)[0]).squeeze().tolist())\n\n            if in_dist:\n                pred = output.data.max(1)[1]\n                correct.extend(pred.eq(target).to(\'cpu\').numpy().squeeze().tolist())\n                labels.extend(target.to(\'cpu\').numpy().squeeze().tolist())\n\n    if in_dist:\n        return logits.copy(), confidence.copy(), correct.copy(), labels.copy()\n    else:\n        return logits[:ood_num_examples].copy(), confidence[:ood_num_examples].copy()\n\n\nval_logits, val_confidence, val_correct, val_labels = get_net_results(val_loader, in_dist=True)\n\nprint(\'\\nTuning Softmax Temperature\')\nt_star = tune_temp(val_logits, val_labels)\nprint(\'Softmax Temperature Tuned. Temperature is {:.3f}\'.format(t_star))\n\ntest_logits, test_confidence, test_correct, _ = get_net_results(test_loader, in_dist=True, t=t_star)\n\nprint(\'Error Rate {:.2f}\'.format(100*(len(test_correct) - sum(test_correct))/len(test_correct)))\n\n# /////////////// End Calibration Prelims ///////////////\n\nprint(\'\\nUsing CIFAR-10 as typical data\') if num_classes == 10 else print(\'\\nUsing CIFAR-100 as typical data\')\n\n# /////////////// In-Distribution Calibration ///////////////\n\nprint(\'\\n\\nIn-Distribution Data\')\nshow_calibration_results(np.array(test_confidence), np.array(test_correct), method_name=args.method_name)\n\n# /////////////// OOD Calibration ///////////////\nrms_list, mad_list, sf1_list = [], [], []\n\n\ndef get_and_print_results(ood_loader, num_to_avg=args.num_to_avg):\n\n    rmss, mads, sf1s = [], [], []\n    for _ in range(num_to_avg):\n        out_logits, out_confidence = get_net_results(ood_loader, t=t_star)\n\n        measures = get_measures(\n            concat([out_confidence, test_confidence]),\n            concat([np.zeros(len(out_confidence)), test_correct]))\n\n        rmss.append(measures[0]); mads.append(measures[1]); sf1s.append(measures[2])\n\n    rms = np.mean(rmss); mad = np.mean(mads); sf1 = np.mean(sf1s)\n    rms_list.append(rms); mad_list.append(mad); sf1_list.append(sf1)\n\n    if num_to_avg >= 5:\n        print_measures_with_std(rmss, mads, sf1s, args.method_name)\n    else:\n        print_measures(rms, mad, sf1, args.method_name)\n\n\n# /////////////// Gaussian Noise ///////////////\n\ndummy_targets = torch.ones(ood_num_examples * args.num_to_avg)\nood_data = torch.from_numpy(np.float32(np.clip(\n    np.random.normal(size=(ood_num_examples * args.num_to_avg, 3, 32, 32), scale=0.5), -1, 1)))\nood_data = torch.utils.data.TensorDataset(ood_data, dummy_targets)\nood_loader = torch.utils.data.DataLoader(ood_data, batch_size=args.test_bs, shuffle=True,\n                                         num_workers=args.prefetch, pin_memory=True)\n\nprint(\'\\n\\nGaussian Noise (sigma = 0.5) Calibration\')\nget_and_print_results(ood_loader)\n\n# /////////////// Rademacher Noise ///////////////\n\ndummy_targets = torch.ones(ood_num_examples * args.num_to_avg)\nood_data = torch.from_numpy(np.random.binomial(\n    n=1, p=0.5, size=(ood_num_examples * args.num_to_avg, 3, 32, 32)).astype(np.float32)) * 2 - 1\nood_data = torch.utils.data.TensorDataset(ood_data, dummy_targets)\nood_loader = torch.utils.data.DataLoader(ood_data, batch_size=args.test_bs, shuffle=True)\n\nprint(\'\\n\\nRademacher Noise Calibration\')\nget_and_print_results(ood_loader)\n\n# /////////////// Blob ///////////////\n\nood_data = np.float32(np.random.binomial(n=1, p=0.7, size=(ood_num_examples * args.num_to_avg, 32, 32, 3)))\nfor i in range(ood_num_examples * args.num_to_avg):\n    ood_data[i] = gblur(ood_data[i], sigma=1.5, multichannel=False)\n    ood_data[i][ood_data[i] < 0.75] = 0.0\n\ndummy_targets = torch.ones(ood_num_examples * args.num_to_avg)\nood_data = torch.from_numpy(ood_data.transpose((0, 3, 1, 2))) * 2 - 1\nood_data = torch.utils.data.TensorDataset(ood_data, dummy_targets)\nood_loader = torch.utils.data.DataLoader(ood_data, batch_size=args.test_bs, shuffle=True,\n                                         num_workers=args.prefetch, pin_memory=True)\n\nprint(\'\\n\\nBlob Calibration\')\nget_and_print_results(ood_loader)\n\n# /////////////// Textures ///////////////\n\nood_data = dset.ImageFolder(root=""/share/data/vision-greg2/users/dan/datasets/dtd/images"",\n                            transform=trn.Compose([trn.Resize(32), trn.CenterCrop(32),\n                                                   trn.ToTensor(), trn.Normalize(mean, std)]))\nood_loader = torch.utils.data.DataLoader(ood_data, batch_size=args.test_bs, shuffle=True,\n                                         num_workers=args.prefetch, pin_memory=True)\n\nprint(\'\\n\\nTexture Calibration\')\nget_and_print_results(ood_loader)\n\n# /////////////// SVHN ///////////////\n\nood_data = svhn.SVHN(root=\'/share/data/vision-greg/svhn/\', split=""test"",\n                     transform=trn.Compose([trn.Resize(32), trn.ToTensor(), trn.Normalize(mean, std)]), download=False)\nood_loader = torch.utils.data.DataLoader(ood_data, batch_size=args.test_bs, shuffle=True,\n                                         num_workers=args.prefetch, pin_memory=True)\n\nprint(\'\\n\\nSVHN Calibration\')\nget_and_print_results(ood_loader)\n\n# /////////////// Places365 ///////////////\n\nood_data = dset.ImageFolder(root=""/share/data/vision-greg2/places365/test_subset"",\n                            transform=trn.Compose([trn.Resize(32), trn.CenterCrop(32),\n                                                   trn.ToTensor(), trn.Normalize(mean, std)]))\nood_loader = torch.utils.data.DataLoader(ood_data, batch_size=args.test_bs, shuffle=True,\n                                         num_workers=args.prefetch, pin_memory=True)\n\nprint(\'\\n\\nPlaces365 Calibration\')\nget_and_print_results(ood_loader)\n\n# /////////////// LSUN ///////////////\n\nood_data = lsun_loader.LSUN(""/share/data/vision-greg2/users/dan/datasets/LSUN/lsun-master/data"", classes=\'test\',\n                            transform=trn.Compose([trn.Resize(32), trn.CenterCrop(32),\n                                                   trn.ToTensor(), trn.Normalize(mean, std)]))\nood_loader = torch.utils.data.DataLoader(ood_data, batch_size=args.test_bs, shuffle=True,\n                                         num_workers=args.prefetch, pin_memory=True)\n\nprint(\'\\n\\nLSUN Calibration\')\nget_and_print_results(ood_loader)\n\n# /////////////// CIFAR Data ///////////////\n\nif \'cifar10_\' in args.method_name:\n    ood_data = dset.CIFAR100(\'/share/data/vision-greg/cifarpy\', train=False, transform=test_transform)\nelse:\n    ood_data = dset.CIFAR10(\'/share/data/vision-greg/cifarpy\', train=False, transform=test_transform)\n\nood_loader = torch.utils.data.DataLoader(ood_data, batch_size=args.test_bs, shuffle=True,\n                                         num_workers=args.prefetch, pin_memory=True)\n\n\nprint(\'\\n\\nCIFAR-100 Detection\') if \'cifar100\' in args.method_name else print(\'\\n\\nCIFAR-10 Detection\')\nget_and_print_results(ood_loader)\n\n# /////////////// Mean Results ///////////////\n\nprint(\'\\n\\nMean Test Results\')\nprint_measures(np.mean(rms_list), np.mean(mad_list), np.mean(sf1_list), method_name=args.method_name)\n'"
MNIST/baseline.py,13,"b'# -*- coding: utf-8 -*-\nimport numpy as np\nimport os\nimport argparse\nimport time\nimport torch\nimport torch.nn as nn\nimport torch.backends.cudnn as cudnn\nimport torchvision.transforms as trn\nimport torchvision.datasets as dset\nimport torch.nn.functional as F\nfrom tqdm import tqdm\nfrom models.convnet import ConvNet\n\nif __package__ is None:\n    import sys\n    from os import path\n\n    sys.path.append(path.dirname(path.dirname(path.abspath(__file__))))\n    from utils.validation_dataset import validation_split\n\nparser = argparse.ArgumentParser(description=\'Trains an MNIST Classifier\',\n                                 formatter_class=argparse.ArgumentDefaultsHelpFormatter)\nparser.add_argument(\'--calibration\', \'-c\', action=\'store_true\',\n                    help=\'Train a model to be used for calibration. This holds out some data for validation.\')\n# Optimization options\nparser.add_argument(\'--epochs\', \'-e\', type=int, default=10, help=\'Number of epochs to train.\')\nparser.add_argument(\'--learning_rate\', \'-lr\', type=float, default=0.01, help=\'The initial learning rate.\')\nparser.add_argument(\'--batch_size\', \'-b\', type=int, default=50, help=\'Batch size.\')\nparser.add_argument(\'--test_bs\', type=int, default=200)\nparser.add_argument(\'--momentum\', type=float, default=0.9, help=\'Momentum.\')\n# Checkpoints\nparser.add_argument(\'--save\', \'-s\', type=str, default=\'./snapshots/baseline\', help=\'Folder to save checkpoints.\')\nparser.add_argument(\'--load\', \'-l\', type=str, default=\'\', help=\'Checkpoint path to resume / test.\')\nparser.add_argument(\'--test\', \'-t\', action=\'store_true\', help=\'Test only flag.\')\n# Acceleration\nparser.add_argument(\'--ngpu\', type=int, default=1, help=\'0 = CPU.\')\nparser.add_argument(\'--prefetch\', type=int, default=2, help=\'Pre-fetching threads.\')\nargs = parser.parse_args()\n\nstate = {k: v for k, v in args._get_kwargs()}\nprint(state)\n\ntorch.manual_seed(1)\nnp.random.seed(1)\n\ntrain_data = dset.MNIST(\'/home-nfs/dan/cifar_data/mnist\', train=True, transform=trn.ToTensor())\ntest_data = dset.MNIST(\'/home-nfs/dan/cifar_data/mnist\', train=False, transform=trn.ToTensor())\nnum_classes = 10\n\ncalib_indicator = \'\'\nif args.calibration:\n    train_data, val_data = validation_split(train_data, val_share=0.1)\n    calib_indicator = \'calib_\'\n\ntrain_loader = torch.utils.data.DataLoader(\n    train_data, batch_size=args.batch_size, shuffle=True,\n    num_workers=args.prefetch, pin_memory=True)\ntest_loader = torch.utils.data.DataLoader(\n    test_data, batch_size=args.test_bs, shuffle=False,\n    num_workers=args.prefetch, pin_memory=True)\n\n# Create model\nnet = ConvNet()\n\n\nstart_epoch = 0\n\n# Restore model if desired\nif args.load != \'\':\n    for i in range(1000 - 1, -1, -1):\n        model_name = os.path.join(args.load, calib_indicator + \'baseline_epoch_\' + str(i) + \'.pt\')\n        if os.path.isfile(model_name):\n            net.load_state_dict(torch.load(model_name))\n            print(\'Model restored! Epoch:\', i)\n            start_epoch = i + 1\n            break\n    if start_epoch == 0:\n        assert False, ""could not resume""\n\nif args.ngpu > 1:\n    net = torch.nn.DataParallel(net, device_ids=list(range(args.ngpu)))\n\nif args.ngpu > 0:\n    net.cuda()\n    torch.cuda.manual_seed(1)\n\ncudnn.benchmark = True  # fire on all cylinders\n\noptimizer = torch.optim.SGD(\n    net.parameters(), state[\'learning_rate\'],\n    momentum=state[\'momentum\'], nesterov=True)\n\n\ndef cosine_annealing(step, total_steps, lr_max, lr_min):\n    return lr_min + (lr_max - lr_min) * 0.5 * (\n            1 + np.cos(step / total_steps * np.pi))\n\n\nscheduler = torch.optim.lr_scheduler.LambdaLR(\n    optimizer,\n    lr_lambda=lambda step: cosine_annealing(\n        step,\n        args.epochs * len(train_loader),\n        1,  # since lr_lambda computes multiplicative factor\n        1e-6 / args.learning_rate))\n\n\n# /////////////// Training ///////////////\n\ndef train():\n    net.train()  # enter train mode\n    loss_avg = 0.0\n    for data, target in train_loader:\n        data, target = data.cuda(), target.cuda()\n\n        # forward\n        x = net(data)\n\n        # backward\n        scheduler.step()\n        optimizer.zero_grad()\n        loss = F.cross_entropy(x, target)\n        loss.backward()\n        optimizer.step()\n\n        # exponential moving average\n        loss_avg = loss_avg * 0.8 + float(loss) * 0.2\n\n    state[\'train_loss\'] = loss_avg\n\n\n# test function\ndef test():\n    net.eval()\n    loss_avg = 0.0\n    correct = 0\n    with torch.no_grad():\n        for data, target in test_loader:\n            data, target = data.cuda(), target.cuda()\n\n            # forward\n            output = net(data)\n            loss = F.cross_entropy(output, target)\n\n            # accuracy\n            pred = output.data.max(1)[1]\n            correct += pred.eq(target.data).sum().item()\n\n            # test loss average\n            loss_avg += float(loss.data)\n\n    state[\'test_loss\'] = loss_avg / len(test_loader)\n    state[\'test_accuracy\'] = correct / len(test_loader.dataset)\n\n\nif args.test:\n    test()\n    print(state)\n    exit()\n\n# Make save directory\nif not os.path.exists(args.save):\n    os.makedirs(args.save)\nif not os.path.isdir(args.save):\n    raise Exception(\'%s is not a dir\' % args.save)\n\nwith open(os.path.join(args.save, calib_indicator + \'baseline_training_results.csv\'), \'w\') as f:\n    f.write(\'epoch,time(s),train_loss,test_loss,test_error(%)\\n\')\n\nprint(\'Beginning Training\\n\')\n\n# Main loop\nfor epoch in range(start_epoch, args.epochs):\n    state[\'epoch\'] = epoch\n\n    begin_epoch = time.time()\n\n    train()\n    test()\n\n    # Save model\n    torch.save(net.state_dict(),\n               os.path.join(args.save, calib_indicator + \'baseline_epoch_\' + str(epoch) + \'.pt\'))\n    # Let us not waste space and delete the previous model\n    prev_path = os.path.join(args.save, calib_indicator + \'baseline_epoch_\' + str(epoch - 1) + \'.pt\')\n    if os.path.exists(prev_path): os.remove(prev_path)\n\n    # Show results\n\n    with open(os.path.join(args.save, calib_indicator + \'baseline_training_results.csv\'), \'a\') as f:\n        f.write(\'%03d,%05d,%0.6f,%0.5f,%0.2f\\n\' % (\n            (epoch + 1),\n            time.time() - begin_epoch,\n            state[\'train_loss\'],\n            state[\'test_loss\'],\n            100 - 100. * state[\'test_accuracy\'],\n        ))\n\n    # # print state with rounded decimals\n    # print({k: round(v, 4) if isinstance(v, float) else v for k, v in state.items()})\n\n    print(\'Epoch {0:3d} | Time {1:5d} | Train Loss {2:.4f} | Test Loss {3:.3f} | Test Error {4:.2f}\'.format(\n        (epoch + 1),\n        int(time.time() - begin_epoch),\n        state[\'train_loss\'],\n        state[\'test_loss\'],\n        100 - 100. * state[\'test_accuracy\'])\n    )\n'"
MNIST/oe_scratch.py,17,"b'# -*- coding: utf-8 -*-\nimport numpy as np\nimport os\nimport argparse\nimport time\nimport torch\nimport torch.nn as nn\nimport torch.backends.cudnn as cudnn\nimport torchvision.transforms as trn\nimport torchvision.datasets as dset\nimport torch.nn.functional as F\nfrom tqdm import tqdm\nfrom models.convnet import ConvNet\n\nif __package__ is None:\n    import sys\n    from os import path\n\n    sys.path.append(path.dirname(path.dirname(path.abspath(__file__))))\n    import utils.svhn_loader as svhn\n    from utils.tinyimages_80mn_loader import TinyImages\n    from utils.validation_dataset import validation_split\n\nparser = argparse.ArgumentParser(description=\'Trains an MNIST Classifier with OE\',\n                                 formatter_class=argparse.ArgumentDefaultsHelpFormatter)\nparser.add_argument(\'--calibration\', \'-c\', action=\'store_true\',\n                    help=\'Train a model to be used for calibration. This holds out some data for validation.\')\n# Optimization options\nparser.add_argument(\'--epochs\', \'-e\', type=int, default=10, help=\'Number of epochs to train.\')\nparser.add_argument(\'--learning_rate\', \'-lr\', type=float, default=0.01, help=\'The initial learning rate.\')\nparser.add_argument(\'--batch_size\', \'-b\', type=int, default=50, help=\'Batch size.\')\nparser.add_argument(\'--oe_batch_size\', type=int, default=100, help=\'Batch size.\')\nparser.add_argument(\'--test_bs\', type=int, default=200)\nparser.add_argument(\'--momentum\', type=float, default=0.9, help=\'Momentum.\')\n# Checkpoints\nparser.add_argument(\'--save\', \'-s\', type=str, default=\'./snapshots/oe_scratch\', help=\'Folder to save checkpoints.\')\nparser.add_argument(\'--load\', \'-l\', type=str, default=\'\', help=\'Checkpoint path to resume / test.\')\nparser.add_argument(\'--test\', \'-t\', action=\'store_true\', help=\'Test only flag.\')\n# Acceleration\nparser.add_argument(\'--ngpu\', type=int, default=1, help=\'0 = CPU.\')\nparser.add_argument(\'--prefetch\', type=int, default=4, help=\'Pre-fetching threads.\')\nargs = parser.parse_args()\n\nstate = {k: v for k, v in args._get_kwargs()}\nprint(state)\n\ntorch.manual_seed(1)\nnp.random.seed(1)\n\ntrain_data_in = dset.MNIST(\'/home-nfs/dan/cifar_data/mnist\', train=True, transform=trn.ToTensor())\ntest_data = dset.MNIST(\'/home-nfs/dan/cifar_data/mnist\', train=False, transform=trn.ToTensor())\nnum_classes = 10\n\ncalib_indicator = \'\'\nif args.calibration:\n    train_data_in, val_data = validation_split(train_data_in, val_share=0.1)\n    calib_indicator = \'calib_\'\n\ntiny_images = TinyImages(transform=trn.Compose(\n    [trn.ToTensor(), trn.ToPILImage(), trn.Resize(28),\n     trn.Lambda(lambda x: x.convert(\'L\', (0.2989, 0.5870, 0.1140, 0))),\n     trn.RandomHorizontalFlip(), trn.ToTensor()]))\n\n\ntrain_loader_in = torch.utils.data.DataLoader(\n    train_data_in,\n    batch_size=args.batch_size, shuffle=True,\n    num_workers=1, pin_memory=True)\n\ntrain_loader_out = torch.utils.data.DataLoader(\n    tiny_images,\n    batch_size=args.oe_batch_size, shuffle=False,\n    num_workers=3, pin_memory=True)\n\ntest_loader = torch.utils.data.DataLoader(\n    test_data,\n    batch_size=args.batch_size, shuffle=False,\n    num_workers=args.prefetch, pin_memory=True)\n\n# Create model\nnet = ConvNet()\n\n\nstart_epoch = 0\n\n# Restore model if desired\nif args.load != \'\':\n    for i in range(1000 - 1, -1, -1):\n        model_name = os.path.join(args.load, calib_indicator + \'oe_scratch_epoch_\' + str(i) + \'.pt\')\n        if os.path.isfile(model_name):\n            net.load_state_dict(torch.load(model_name))\n            print(\'Model restored! Epoch:\', i)\n            start_epoch = i + 1\n            break\n    if start_epoch == 0:\n        assert False, ""could not resume""\n\nif args.ngpu > 1:\n    net = torch.nn.DataParallel(net, device_ids=list(range(args.ngpu)))\n\nif args.ngpu > 0:\n    net.cuda()\n    torch.cuda.manual_seed(1)\n\ncudnn.benchmark = True  # fire on all cylinders\n\noptimizer = torch.optim.SGD(\n    net.parameters(), state[\'learning_rate\'], momentum=state[\'momentum\'], nesterov=True)\n\n\ndef cosine_annealing(step, total_steps, lr_max, lr_min):\n    return lr_min + (lr_max - lr_min) * 0.5 * (\n            1 + np.cos(step / total_steps * np.pi))\n\n\nscheduler = torch.optim.lr_scheduler.LambdaLR(\n    optimizer,\n    lr_lambda=lambda step: cosine_annealing(\n        step,\n        args.epochs * len(train_loader_in),\n        1,  # since lr_lambda computes multiplicative factor\n        1e-6 / args.learning_rate))\n\n\n# /////////////// Training ///////////////\n\ndef train():\n    net.train()  # enter train mode\n    loss_avg = 0.0\n\n    # start at a random point of the outlier dataset; this induces more randomness without destroying locality\n    train_loader_out.dataset.offset = np.random.randint(len(train_loader_out.dataset))\n    for in_set, out_set in zip(train_loader_in, train_loader_out):\n        data = torch.cat((in_set[0], out_set[0]), 0)\n        target = in_set[1]\n\n        data, target = data.cuda(), target.cuda()\n\n        # forward\n        x = net(data)\n\n        # backward\n        scheduler.step()\n        optimizer.zero_grad()\n\n        loss = F.cross_entropy(x[:len(in_set[0])], target)\n        # cross-entropy from softmax distribution to uniform distribution\n        loss += 0.5 * -(x[len(in_set[0]):].mean(1) - torch.logsumexp(x[len(in_set[0]):], dim=1)).mean()\n\n        # # online hard example mining\n        # scores = -(x[len(in_set[0]):].mean(1) - torch.logsumexp(x[len(in_set[0]):], dim=1))\n        # _, hard_indices = scores.topk(32)\n        # loss += 0.5 * scores[hard_indices].mean()\n\n        loss.backward()\n        optimizer.step()\n\n        # exponential moving average\n        loss_avg = loss_avg * 0.8 + float(loss) * 0.2\n\n    state[\'train_loss\'] = loss_avg\n\n\n# test function\ndef test():\n    net.eval()\n    loss_avg = 0.0\n    correct = 0\n    with torch.no_grad():\n        for data, target in test_loader:\n            data, target = data.cuda(), target.cuda()\n\n            # forward\n            output = net(data)\n            loss = F.cross_entropy(output, target)\n\n            # accuracy\n            pred = output.data.max(1)[1]\n            correct += pred.eq(target.data).sum().item()\n\n            # test loss average\n            loss_avg += float(loss.data)\n\n    state[\'test_loss\'] = loss_avg / len(test_loader)\n    state[\'test_accuracy\'] = correct / len(test_loader.dataset)\n\n\nif args.test:\n    test()\n    print(state)\n    exit()\n\n# Make save directory\nif not os.path.exists(args.save):\n    os.makedirs(args.save)\nif not os.path.isdir(args.save):\n    raise Exception(\'%s is not a dir\' % args.save)\n\nwith open(os.path.join(args.save, calib_indicator + \'oe_scratch_training_results.csv\'), \'w\') as f:\n    f.write(\'epoch,time(s),train_loss,test_loss,test_error(%)\\n\')\n\nprint(\'Beginning Training\\n\')\n\n# Main loop\nfor epoch in range(start_epoch, args.epochs):\n    state[\'epoch\'] = epoch\n\n    begin_epoch = time.time()\n\n    train()\n    test()\n\n    # Save model\n    torch.save(net.state_dict(),\n               os.path.join(args.save, calib_indicator + \'oe_scratch_epoch_\' + str(epoch) + \'.pt\'))\n    # Let us not waste space and delete the previous model\n    prev_path = os.path.join(args.save, calib_indicator + \'oe_scratch_epoch_\' + str(epoch - 1) + \'.pt\')\n    if os.path.exists(prev_path): os.remove(prev_path)\n\n    # Show results\n\n    with open(os.path.join(args.save, calib_indicator + \'oe_scratch_training_results.csv\'), \'a\') as f:\n        f.write(\'%03d,%05d,%0.6f,%0.5f,%0.2f\\n\' % (\n            (epoch + 1),\n            time.time() - begin_epoch,\n            state[\'train_loss\'],\n            state[\'test_loss\'],\n            100 - 100. * state[\'test_accuracy\'],\n        ))\n\n    # # print state with rounded decimals\n    # print({k: round(v, 4) if isinstance(v, float) else v for k, v in state.items()})\n\n    print(\'Epoch {0:3d} | Time {1:5d} | Train Loss {2:.4f} | Test Loss {3:.3f} | Test Error {4:.2f}\'.format(\n        (epoch + 1),\n        int(time.time() - begin_epoch),\n        state[\'train_loss\'],\n        state[\'test_loss\'],\n        100 - 100. * state[\'test_accuracy\'])\n    )\n'"
MNIST/oe_tune.py,16,"b'# -*- coding: utf-8 -*-\nimport numpy as np\nimport os\nimport argparse\nimport time\nimport torch\nimport torch.nn as nn\nimport torch.backends.cudnn as cudnn\nimport torchvision.transforms as trn\nimport torchvision.datasets as dset\nimport torch.nn.functional as F\nfrom tqdm import tqdm\nfrom models.convnet import ConvNet\n\nif __package__ is None:\n    import sys\n    from os import path\n\n    sys.path.append(path.dirname(path.dirname(path.abspath(__file__))))\n    import utils.svhn_loader as svhn\n    from utils.tinyimages_80mn_loader import TinyImages\n    from utils.validation_dataset import validation_split\n\nparser = argparse.ArgumentParser(description=\'Tunes an MNIST Classifier with OE\',\n                                 formatter_class=argparse.ArgumentDefaultsHelpFormatter)\nparser.add_argument(\'--calibration\', \'-c\', action=\'store_true\',\n                    help=\'Train a model to be used for calibration. This holds out some data for validation.\')\n# Optimization options\nparser.add_argument(\'--epochs\', \'-e\', type=int, default=5, help=\'Number of epochs to train.\')\nparser.add_argument(\'--learning_rate\', \'-lr\', type=float, default=0.01, help=\'The initial learning rate.\')\nparser.add_argument(\'--batch_size\', \'-b\', type=int, default=50, help=\'Batch size.\')\nparser.add_argument(\'--oe_batch_size\', type=int, default=100, help=\'Batch size.\')\nparser.add_argument(\'--test_bs\', type=int, default=200)\nparser.add_argument(\'--momentum\', type=float, default=0.9, help=\'Momentum.\')\n# Checkpoints\nparser.add_argument(\'--save\', \'-s\', type=str, default=\'./snapshots/oe_tune\', help=\'Folder to save checkpoints.\')\nparser.add_argument(\'--load\', \'-l\', type=str, default=\'./snapshots/baseline\', help=\'Checkpoint path to resume / test.\')\nparser.add_argument(\'--test\', \'-t\', action=\'store_true\', help=\'Test only flag.\')\n# Acceleration\nparser.add_argument(\'--ngpu\', type=int, default=1, help=\'0 = CPU.\')\nparser.add_argument(\'--prefetch\', type=int, default=4, help=\'Pre-fetching threads.\')\nargs = parser.parse_args()\n\nstate = {k: v for k, v in args._get_kwargs()}\nprint(state)\n\ntorch.manual_seed(1)\nnp.random.seed(1)\n\ntrain_data_in = dset.MNIST(\'/home-nfs/dan/cifar_data/mnist\', train=True, transform=trn.ToTensor())\ntest_data = dset.MNIST(\'/home-nfs/dan/cifar_data/mnist\', train=False, transform=trn.ToTensor())\nnum_classes = 10\n\ncalib_indicator = \'\'\nif args.calibration:\n    train_data_in, val_data = validation_split(train_data_in, val_share=0.1)\n    calib_indicator = \'calib_\'\n\ntiny_images = TinyImages(transform=trn.Compose(\n    [trn.ToTensor(), trn.ToPILImage(), trn.Resize(28),\n     trn.Lambda(lambda x: x.convert(\'L\', (0.2989, 0.5870, 0.1140, 0))),\n     trn.RandomHorizontalFlip(), trn.ToTensor()]))\n\n\ntrain_loader_in = torch.utils.data.DataLoader(\n    train_data_in,\n    batch_size=args.batch_size, shuffle=True,\n    num_workers=1, pin_memory=True)\n\ntrain_loader_out = torch.utils.data.DataLoader(\n    tiny_images,\n    batch_size=args.oe_batch_size, shuffle=False,\n    num_workers=3, pin_memory=True)\n\ntest_loader = torch.utils.data.DataLoader(\n    test_data,\n    batch_size=args.batch_size, shuffle=False,\n    num_workers=args.prefetch, pin_memory=True)\n\n# Create model\nnet = ConvNet()\n\n\n# Restore model if desired\nmodel_found = False\nif args.load != \'\':\n    for i in range(1000 - 1, -1, -1):\n        model_name = os.path.join(args.load, calib_indicator + \'baseline_epoch_\' + str(i) + \'.pt\')\n        if os.path.isfile(model_name):\n            net.load_state_dict(torch.load(model_name))\n            print(\'Model restored! Epoch:\', i)\n            model_found = True\n            break\n    if not model_found:\n        assert False, ""could not resume""\n\nif args.ngpu > 1:\n    net = torch.nn.DataParallel(net, device_ids=list(range(args.ngpu)))\n\nif args.ngpu > 0:\n    net.cuda()\n    torch.cuda.manual_seed(1)\n\ncudnn.benchmark = True  # fire on all cylinders\n\noptimizer = torch.optim.SGD(\n    net.parameters(), state[\'learning_rate\'], momentum=state[\'momentum\'], nesterov=True)\n\n\ndef cosine_annealing(step, total_steps, lr_max, lr_min):\n    return lr_min + (lr_max - lr_min) * 0.5 * (\n            1 + np.cos(step / total_steps * np.pi))\n\n\nscheduler = torch.optim.lr_scheduler.LambdaLR(\n    optimizer,\n    lr_lambda=lambda step: cosine_annealing(\n        step,\n        args.epochs * len(train_loader_in),\n        1,  # since lr_lambda computes multiplicative factor\n        1e-6 / args.learning_rate))\n\n\n# /////////////// Training ///////////////\n\ndef train():\n    net.train()  # enter train mode\n    loss_avg = 0.0\n\n    # start at a random point of the outlier dataset; this induces more randomness without destroying locality\n    train_loader_out.dataset.offset = np.random.randint(len(train_loader_out.dataset))\n    for in_set, out_set in zip(train_loader_in, train_loader_out):\n        data = torch.cat((in_set[0], out_set[0]), 0)\n        target = in_set[1]\n\n        data, target = data.cuda(), target.cuda()\n\n        # forward\n        x = net(data)\n\n        # backward\n        scheduler.step()\n        optimizer.zero_grad()\n\n        loss = F.cross_entropy(x[:len(in_set[0])], target)\n        # cross-entropy from softmax distribution to uniform distribution\n        loss += 0.5 * -(x[len(in_set[0]):].mean(1) - torch.logsumexp(x[len(in_set[0]):], dim=1)).mean()\n\n        loss.backward()\n        optimizer.step()\n\n        # exponential moving average\n        loss_avg = loss_avg * 0.8 + float(loss) * 0.2\n\n    state[\'train_loss\'] = loss_avg\n\n\n# test function\ndef test():\n    net.eval()\n    loss_avg = 0.0\n    correct = 0\n    with torch.no_grad():\n        for data, target in test_loader:\n            data, target = data.cuda(), target.cuda()\n\n            # forward\n            output = net(data)\n            loss = F.cross_entropy(output, target)\n\n            # accuracy\n            pred = output.data.max(1)[1]\n            correct += pred.eq(target.data).sum().item()\n\n            # test loss average\n            loss_avg += float(loss.data)\n\n    state[\'test_loss\'] = loss_avg / len(test_loader)\n    state[\'test_accuracy\'] = correct / len(test_loader.dataset)\n\n\nif args.test:\n    test()\n    print(state)\n    exit()\n\n# Make save directory\nif not os.path.exists(args.save):\n    os.makedirs(args.save)\nif not os.path.isdir(args.save):\n    raise Exception(\'%s is not a dir\' % args.save)\n\nwith open(os.path.join(args.save, calib_indicator + \'oe_tune_training_results.csv\'), \'w\') as f:\n    f.write(\'epoch,time(s),train_loss,test_loss,test_error(%)\\n\')\n\nprint(\'Beginning Training\\n\')\n\n# Main loop\nfor epoch in range(0, args.epochs):\n    state[\'epoch\'] = epoch\n\n    begin_epoch = time.time()\n\n    train()\n    test()\n\n    # Save model\n    torch.save(net.state_dict(),\n               os.path.join(args.save, calib_indicator + \'oe_tune_epoch_\' + str(epoch) + \'.pt\'))\n    # Let us not waste space and delete the previous model\n    prev_path = os.path.join(args.save, calib_indicator + \'oe_tune_epoch_\' + str(epoch - 1) + \'.pt\')\n    if os.path.exists(prev_path): os.remove(prev_path)\n\n    # Show results\n\n    with open(os.path.join(args.save, calib_indicator + \'oe_tune_training_results.csv\'), \'a\') as f:\n        f.write(\'%03d,%05d,%0.6f,%0.5f,%0.2f\\n\' % (\n            (epoch + 1),\n            time.time() - begin_epoch,\n            state[\'train_loss\'],\n            state[\'test_loss\'],\n            100 - 100. * state[\'test_accuracy\'],\n        ))\n\n    # # print state with rounded decimals\n    # print({k: round(v, 4) if isinstance(v, float) else v for k, v in state.items()})\n\n    print(\'Epoch {0:3d} | Time {1:5d} | Train Loss {2:.4f} | Test Loss {3:.3f} | Test Error {4:.2f}\'.format(\n        (epoch + 1),\n        int(time.time() - begin_epoch),\n        state[\'train_loss\'],\n        state[\'test_loss\'],\n        100 - 100. * state[\'test_accuracy\'])\n    )\n'"
MNIST/test.py,49,"b'import numpy as np\nimport sys\nimport os\nimport pickle\nimport argparse\nimport torch\nimport torch.nn as nn\nimport torch.backends.cudnn as cudnn\nimport torchvision.transforms as trn\nimport torchvision.datasets as dset\nimport torch.nn.functional as F\nfrom models.convnet import ConvNet\nfrom skimage.filters import gaussian as gblur\nfrom PIL import Image as PILImage\n\n# go through rigamaroo to do ...utils.display_results import show_performance\nif __package__ is None:\n    import sys\n    from os import path\n\n    sys.path.append(path.dirname(path.dirname(path.abspath(__file__))))\n    from utils.display_results import show_performance, get_measures, print_measures, print_measures_with_std\n    import utils.svhn_loader as svhn\n    import utils.lsun_loader as lsun_loader\n\nparser = argparse.ArgumentParser(description=\'Evaluates an MNIST OOD Detector\',\n                                 formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n# Setup\nparser.add_argument(\'--test_bs\', type=int, default=200)\nparser.add_argument(\'--num_to_avg\', type=int, default=1, help=\'Average measures across num_to_avg runs.\')\nparser.add_argument(\'--validate\', \'-v\', action=\'store_true\', help=\'Evaluate performance on validation distributions.\')\nparser.add_argument(\'--use_xent\', \'-x\', action=\'store_true\', help=\'Use cross entropy scoring instead of the MSP.\')\nparser.add_argument(\'--method_name\', \'-m\', type=str, default=\'baseline\', help=\'Method name.\')\n# Loading details\nparser.add_argument(\'--load\', \'-l\', type=str, default=\'./snapshots\', help=\'Checkpoint path to resume / test.\')\nparser.add_argument(\'--ngpu\', type=int, default=1, help=\'0 = CPU.\')\nparser.add_argument(\'--prefetch\', type=int, default=2, help=\'Pre-fetching threads.\')\nargs = parser.parse_args()\n\n# torch.manual_seed(1)\n# np.random.seed(1)\n\ntest_data = dset.MNIST(\'/home-nfs/dan/cifar_data/mnist\', train=False, transform=trn.ToTensor())\nnum_classes = 10\n\ntest_loader = torch.utils.data.DataLoader(\n    test_data, batch_size=args.test_bs, shuffle=False,\n    num_workers=args.prefetch, pin_memory=True)\n\n# Create model\nnet = ConvNet()\n\n\nstart_epoch = 0\n\n# Restore model\nif args.load != \'\':\n    for i in range(300 - 1, -1, -1):\n        if \'baseline\' in args.method_name:\n            subdir = \'baseline\'\n        elif \'oe_tune\' in args.method_name:\n            subdir = \'oe_tune\'\n        else:\n            subdir = \'oe_scratch\'\n\n        model_name = os.path.join(os.path.join(args.load, subdir), args.method_name + \'_epoch_\' + str(i) + \'.pt\')\n        if os.path.isfile(model_name):\n            net.load_state_dict(torch.load(model_name))\n            print(\'Model restored! Epoch:\', i)\n            start_epoch = i + 1\n            break\n    if start_epoch == 0:\n        assert False, ""could not resume""\n\nnet.eval()\n\nif args.ngpu > 1:\n    net = torch.nn.DataParallel(net, device_ids=list(range(args.ngpu)))\n\nif args.ngpu > 0:\n    net.cuda()\n    # torch.cuda.manual_seed(1)\n\ncudnn.benchmark = True  # fire on all cylinders\n\n# /////////////// Detection Prelims ///////////////\n\nood_num_examples = test_data.data.size(0) // 5\nexpected_ap = ood_num_examples / (ood_num_examples + test_data.data.size(0))\n\nconcat = lambda x: np.concatenate(x, axis=0)\nto_np = lambda x: x.data.cpu().numpy()\n\n\ndef get_ood_scores(loader, in_dist=False):\n    _score = []\n    _right_score = []\n    _wrong_score = []\n\n    with torch.no_grad():\n        for batch_idx, (data, target) in enumerate(loader):\n            if batch_idx >= ood_num_examples // args.test_bs and in_dist is False:\n                break\n\n            data = data.view(-1, 1, 28, 28).cuda()\n\n            output = net(data)\n            smax = to_np(F.softmax(output, dim=1))\n\n            if args.use_xent:\n                _score.append(to_np((output.mean(1) - torch.logsumexp(output, dim=1))))\n            else:\n                _score.append(-np.max(smax, axis=1))\n\n            if in_dist:\n                preds = np.argmax(smax, axis=1)\n                targets = target.numpy().squeeze()\n                right_indices = preds == targets\n                wrong_indices = np.invert(right_indices)\n\n                if args.use_xent:\n                    _right_score.append(to_np((output.mean(1) - torch.logsumexp(output, dim=1)))[right_indices])\n                    _wrong_score.append(to_np((output.mean(1) - torch.logsumexp(output, dim=1)))[wrong_indices])\n                else:\n                    _right_score.append(-np.max(smax[right_indices], axis=1))\n                    _wrong_score.append(-np.max(smax[wrong_indices], axis=1))\n    \n    if in_dist:\n        return concat(_score).copy(), concat(_right_score).copy(), concat(_wrong_score).copy()\n    else:\n        return concat(_score)[:ood_num_examples].copy()\n\n\nin_score, right_score, wrong_score = get_ood_scores(test_loader, in_dist=True)\n\nnum_right = len(right_score)\nnum_wrong = len(wrong_score)\nprint(\'Error Rate {:.2f}\'.format(100*num_wrong/(num_wrong + num_right)))\n\n# /////////////// End Detection Prelims ///////////////\n\nprint(\'\\nUsing MNIST as typical data\')\n\n# /////////////// Error Detection ///////////////\n\nprint(\'\\n\\nError Detection\')\nshow_performance(wrong_score, right_score, method_name=args.method_name)\n\n# /////////////// OOD Detection ///////////////\n\nauroc_list, aupr_list, fpr_list = [], [], []\n\n\ndef get_and_print_results(ood_loader, num_to_avg=args.num_to_avg):\n\n    aurocs, auprs, fprs = [], [], []\n    for _ in range(num_to_avg):\n        out_score = get_ood_scores(ood_loader)\n        measures = get_measures(out_score, in_score)\n        aurocs.append(measures[0]); auprs.append(measures[1]); fprs.append(measures[2])\n\n    auroc = np.mean(aurocs); aupr = np.mean(auprs); fpr = np.mean(fprs)\n    auroc_list.append(auroc); aupr_list.append(aupr); fpr_list.append(fpr)\n\n    if num_to_avg >= 5:\n        print_measures_with_std(aurocs, auprs, fprs, args.method_name)\n    else:\n        print_measures(auroc, aupr, fpr, args.method_name)\n\n\n# /////////////// Gaussian Noise ///////////////\n\ndummy_targets = torch.ones(ood_num_examples*args.num_to_avg)\nood_data = torch.from_numpy(\n    np.clip(np.random.normal(size=(ood_num_examples*args.num_to_avg, 1, 28, 28),\n                             loc=0.5, scale=0.5).astype(np.float32), 0, 1))\nood_data = torch.utils.data.TensorDataset(ood_data, dummy_targets)\nood_loader = torch.utils.data.DataLoader(ood_data, batch_size=args.test_bs, shuffle=True)\n\nprint(\'\\n\\nGaussian Noise (mu = sigma = 0.5) Detection\')\nget_and_print_results(ood_loader)\n\n# /////////////// Bernoulli Noise ///////////////\n\ndummy_targets = torch.ones(ood_num_examples*args.num_to_avg)\nood_data = torch.from_numpy(np.random.binomial(\n    n=1, p=0.5, size=(ood_num_examples*args.num_to_avg, 1, 28, 28)).astype(np.float32))\nood_data = torch.utils.data.TensorDataset(ood_data, dummy_targets)\nood_loader = torch.utils.data.DataLoader(ood_data, batch_size=args.test_bs, shuffle=True)\n\nprint(\'\\n\\nBernoulli Noise Detection\')\nget_and_print_results(ood_loader)\n\n# /////////////// CIFAR data ///////////////\n\nood_data = dset.CIFAR10(\n    \'/share/data/vision-greg/cifarpy\', train=False,\n    transform=trn.Compose([trn.Resize(28),\n                           trn.Lambda(lambda x: x.convert(\'L\', (0.2989, 0.5870, 0.1140, 0))),\n                           trn.ToTensor()]))\nood_loader = torch.utils.data.DataLoader(ood_data, batch_size=args.test_bs, shuffle=True,\n                                         num_workers=args.prefetch, pin_memory=True)\n\nprint(\'\\n\\nCIFAR-10 Detection\')\nget_and_print_results(ood_loader)\n\n# /////////////// Icons-50 ///////////////\n\nood_data = dset.ImageFolder(\'/share/data/vision-greg/DistortedImageNet/Icons-50\',\n                            transform=trn.Compose([trn.Resize((28, 28)),\n                                                   trn.Lambda(lambda x: x.convert(\'L\', (0.2989, 0.5870, 0.1140, 0))),\n                                                   trn.ToTensor()]))\n\nfiltered_imgs = []\nfor img in ood_data.imgs:\n    if \'numbers\' not in img[0]:     # img[0] is image name\n        filtered_imgs.append(img)\nood_data.imgs = filtered_imgs\n\nood_loader = torch.utils.data.DataLoader(ood_data, batch_size=args.test_bs, shuffle=True)\n\nprint(\'\\n\\nIcons-50 Detection\')\nget_and_print_results(ood_loader)\n\n# /////////////// Fashion-MNIST ///////////////\n\nood_data = dset.FashionMNIST(\'/share/data/vision-greg/fashion_mnist\', train=False,\n                             transform=trn.ToTensor(), download=False)\nood_loader = torch.utils.data.DataLoader(ood_data, batch_size=args.test_bs, shuffle=True)\n\nprint(\'\\n\\nFashion-MNIST Detection\')\nget_and_print_results(ood_loader)\n\n# /////////////// Negative MNIST ///////////////\n\nood_data = dset.MNIST(\'/home-nfs/dan/cifar_data/mnist\', train=False,\n                      transform=trn.Compose([trn.ToTensor(), trn.Lambda(lambda img: 1 - img)]))\n\nood_loader = torch.utils.data.DataLoader(ood_data, batch_size=args.test_bs, shuffle=True)\n\nprint(\'\\n\\nNegative MNIST Detection\')\nget_and_print_results(ood_loader)\n\n# /////////////// notMNIST ///////////////\n\npickle_file = \'/share/data/vision-greg2/users/dan/datasets/notMNIST.pickle\'\nwith open(pickle_file, \'rb\') as f:\n    notMNIST_data = pickle.load(f, encoding=\'latin1\')\n    notMNIST_data = notMNIST_data[\'test_dataset\'].reshape((-1, 28 * 28)) + 0.5\n\ndummy_targets = torch.ones(min(ood_num_examples*args.num_to_avg, notMNIST_data.shape[0]))\nood_data = torch.utils.data.TensorDataset(torch.from_numpy(\n    notMNIST_data[:ood_num_examples*args.num_to_avg].astype(np.float32)), dummy_targets)\nood_loader = torch.utils.data.DataLoader(ood_data, batch_size=args.test_bs, shuffle=True)\n\nprint(\'\\n\\nnotMNIST Detection\')\nget_and_print_results(ood_loader)\n\n# /////////////// Omniglot ///////////////\n\nimport scipy.io as sio\nimport scipy.misc as scimisc\n\n# other alphabets have characters which look like digits\nsafe_list = [0, 2, 5, 6, 8, 12, 13, 14, 15, 16, 17, 18, 19, 21, 26]\nm = sio.loadmat(""/share/data/vision-greg2/users/dan/datasets/omniglot.mat"")\n\nsquished_set = []\nfor safe_number in safe_list:\n    for alphabet in m[\'images\'][safe_number]:\n        for letters in alphabet:\n            for letter in letters:\n                for example in letter:\n                    squished_set.append(scimisc.imresize(1 - example[0], (28, 28)).reshape(1, 28 * 28))\n\nomni_images = np.concatenate(squished_set, axis=0)\n\ndummy_targets = torch.ones(min(ood_num_examples*args.num_to_avg, len(omni_images)))\nood_data = torch.utils.data.TensorDataset(torch.from_numpy(\n    omni_images[:ood_num_examples*args.num_to_avg].astype(np.float32)), dummy_targets)\nood_loader = torch.utils.data.DataLoader(ood_data, batch_size=args.test_bs, shuffle=True)\n\nprint(\'\\n\\nOmniglot Detection\')\nget_and_print_results(ood_loader)\n\n# /////////////// Mean Results ///////////////\n\nprint(\'\\n\\nMean Test Results\')\nprint_measures(np.mean(auroc_list), np.mean(aupr_list), np.mean(fpr_list), method_name=args.method_name)\n\n\n# /////////////// OOD Detection of Validation Distributions ///////////////\n\nif args.validate is False:\n    exit()\n\nauroc_list, aupr_list, fpr_list = [], [], []\n\n# /////////////// Uniform Noise ///////////////\n\ndummy_targets = torch.ones(ood_num_examples*args.num_to_avg)\nood_data = torch.from_numpy(\n    np.random.uniform(size=(ood_num_examples*args.num_to_avg, 1, 28, 28),\n                      low=0.0, high=1.0).astype(np.float32))\nood_data = torch.utils.data.TensorDataset(ood_data, dummy_targets)\nood_loader = torch.utils.data.DataLoader(ood_data, batch_size=args.test_bs, shuffle=True)\n\nprint(\'\\n\\nUniform[0,1] Noise Detection\')\nget_and_print_results(ood_loader)\n\n# /////////////// Arithmetic Mean of Images ///////////////\n\n\nclass AvgOfPair(torch.utils.data.Dataset):\n    def __init__(self, dataset):\n        self.dataset = dataset\n        self.shuffle_indices = np.arange(len(dataset))\n        np.random.shuffle(self.shuffle_indices)\n\n    def __getitem__(self, i):\n        random_idx = np.random.choice(len(self.dataset))\n        while random_idx == i:\n            random_idx = np.random.choice(len(self.dataset))\n\n        return self.dataset[i][0]/2. + self.dataset[random_idx][0]/2., 0\n\n    def __len__(self):\n        return len(self.dataset)\n\n\nood_loader = torch.utils.data.DataLoader(\n    AvgOfPair(test_data), batch_size=args.test_bs, shuffle=True,\n    num_workers=args.prefetch, pin_memory=True)\n\nprint(\'\\n\\nArithmetic Mean of Random Image Pair Detection\')\nget_and_print_results(ood_loader)\n\n# /////////////// Geometric Mean of Images ///////////////\n\n\nclass GeomMeanOfPair(torch.utils.data.Dataset):\n    def __init__(self, dataset):\n        self.dataset = dataset\n        self.shuffle_indices = np.arange(len(dataset))\n        np.random.shuffle(self.shuffle_indices)\n\n    def __getitem__(self, i):\n        random_idx = np.random.choice(len(self.dataset))\n        while random_idx == i:\n            random_idx = np.random.choice(len(self.dataset))\n\n        return torch.sqrt(self.dataset[i][0] * self.dataset[random_idx][0]), 0\n\n    def __len__(self):\n        return len(self.dataset)\n\n\nood_loader = torch.utils.data.DataLoader(\n    GeomMeanOfPair(test_data), batch_size=args.test_bs, shuffle=True,\n    num_workers=args.prefetch, pin_memory=True)\n\nprint(\'\\n\\nGeometric Mean of Random Image Pair Detection\')\nget_and_print_results(ood_loader)\n\n# /////////////// Jigsaw Images ///////////////\n\nood_loader = torch.utils.data.DataLoader(test_data, batch_size=args.test_bs, shuffle=True,\n                                         num_workers=args.prefetch, pin_memory=True)\n\njigsaw = lambda x: torch.cat((\n    torch.cat((torch.cat((x[:, 6:14, :14], x[:, :6, :14]), 1),\n               x[:, 14:, :14]), 2),\n    torch.cat((x[:, 14:, 14:],\n               torch.cat((x[:, :14, 22:], x[:, :14, 14:22]), 2)), 2),\n), 1)\n\n\nood_loader.dataset.transform = trn.Compose([trn.ToTensor(), jigsaw])\n\nprint(\'\\n\\nJigsawed Images Detection\')\nget_and_print_results(ood_loader)\n\n# /////////////// Speckled Images ///////////////\n\nspeckle = lambda x: torch.clamp(x + x * torch.randn_like(x), 0, 1)\nood_loader.dataset.transform = trn.Compose([trn.ToTensor(), speckle])\n\nprint(\'\\n\\nSpeckle Noised Images Detection\')\nget_and_print_results(ood_loader)\n\n# /////////////// Pixelated Images ///////////////\n\npixelate = lambda x: x.resize((int(28 * 0.2), int(28 * 0.2)), PILImage.BOX).resize((28, 28), PILImage.BOX)\nood_loader.dataset.transform = trn.Compose([pixelate, trn.ToTensor()])\n\nprint(\'\\n\\nPixelate Detection\')\nget_and_print_results(ood_loader)\n\n# /////////////// Mirrored MNIST digits ///////////////\n\nidxs = test_data.targets\nvert_idxs = np.squeeze(np.logical_and(idxs != 3, np.logical_and(idxs != 0, np.logical_and(idxs != 1, idxs != 8))))\nvert_digits = test_data.data.numpy()[vert_idxs][:, ::-1, :]\n\nhoriz_idxs = np.squeeze(np.logical_and(idxs != 0, np.logical_and(idxs != 1, idxs != 8)))\nhoriz_digits = test_data.data.numpy()[horiz_idxs][:, :, ::-1]\n\nflipped_digits = concat((vert_digits, horiz_digits))\n\ndummy_targets = torch.ones(flipped_digits.shape[0])\nood_data = torch.from_numpy(flipped_digits.astype(np.float32) / 255)\nood_data = torch.utils.data.TensorDataset(ood_data, dummy_targets)\nood_loader = torch.utils.data.DataLoader(ood_data, batch_size=args.test_bs, shuffle=True,\n                                         num_workers=args.prefetch)\n\nprint(\'\\n\\nMirrored MNIST Digit Detection\')\nget_and_print_results(ood_loader)\n\n# /////////////// Mean Results ///////////////\n\nprint(\'\\n\\nMean Validation Results\')\nprint_measures(np.mean(auroc_list), np.mean(aupr_list), np.mean(fpr_list), method_name=args.method_name)\n'"
MNIST/test_calibration.py,47,"b'import numpy as np\nimport sys\nimport os\nimport pickle\nimport argparse\nimport torch\nimport torch.nn as nn\nimport torch.backends.cudnn as cudnn\nimport torchvision.transforms as trn\nimport torchvision.datasets as dset\nimport torch.nn.functional as F\nfrom models.convnet import ConvNet\nfrom skimage.filters import gaussian as gblur\nfrom PIL import Image as PILImage\n\n# go through rigamaroo to do ...utils.display_results import show_performance\nif __package__ is None:\n    import sys\n    from os import path\n\n    sys.path.append(path.dirname(path.dirname(path.abspath(__file__))))\n    from utils.display_results import show_performance, get_measures, print_measures, print_measures_with_std\n    from utils.validation_dataset import validation_split\n    from utils.calibration_tools import *\n\nparser = argparse.ArgumentParser(description=\'Evaluates an MNIST OOD Detector\',\n                                 formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n# Setup\nparser.add_argument(\'--test_bs\', type=int, default=200)\nparser.add_argument(\'--num_to_avg\', type=int, default=1, help=\'Average measures across num_to_avg runs.\')\nparser.add_argument(\'--validate\', \'-v\', action=\'store_true\', help=\'Evaluate performance on validation distributions.\')\nparser.add_argument(\'--method_name\', \'-m\', type=str, default=\'calib_baseline\', help=\'Method name.\')\nparser.add_argument(\'--use_01\', \'-z\', action=\'store_true\', help=\'Use 0-1 Posterior Rescaling.\')\n# Loading details\nparser.add_argument(\'--load\', \'-l\', type=str, default=\'./snapshots\', help=\'Checkpoint path to resume / test.\')\nparser.add_argument(\'--ngpu\', type=int, default=1, help=\'0 = CPU.\')\nparser.add_argument(\'--prefetch\', type=int, default=2, help=\'Pre-fetching threads.\')\nargs = parser.parse_args()\n\n# torch.manual_seed(1)\n# np.random.seed(1)\n\ntrain_data = dset.MNIST(\'/home-nfs/dan/cifar_data/mnist\', train=True, transform=trn.ToTensor())\ntest_data = dset.MNIST(\'/home-nfs/dan/cifar_data/mnist\', train=False, transform=trn.ToTensor())\nnum_classes = 10\n\ntrain_data, val_data = validation_split(train_data, val_share=0.1)\n\nval_loader = torch.utils.data.DataLoader(\n    val_data, batch_size=args.test_bs, shuffle=False,\n    num_workers=args.prefetch, pin_memory=True)\ntest_loader = torch.utils.data.DataLoader(\n    test_data, batch_size=args.test_bs, shuffle=False,\n    num_workers=args.prefetch, pin_memory=True)\n\n# Create model\nnet = ConvNet()\n\n\nstart_epoch = 0\n\n# Restore model\nif args.load != \'\':\n    for i in range(300 - 1, -1, -1):\n        if \'baseline\' in args.method_name:\n            subdir = \'baseline\'\n        elif \'oe_tune\' in args.method_name:\n            subdir = \'oe_tune\'\n        else:\n            subdir = \'oe_scratch\'\n\n        model_name = os.path.join(os.path.join(args.load, subdir), args.method_name + \'_epoch_\' + str(i) + \'.pt\')\n        if os.path.isfile(model_name):\n            net.load_state_dict(torch.load(model_name))\n            print(\'Model restored! Epoch:\', i)\n            start_epoch = i + 1\n            break\n    if start_epoch == 0:\n        assert False, ""could not resume""\n\nnet.eval()\n\nif args.ngpu > 1:\n    net = torch.nn.DataParallel(net, device_ids=list(range(args.ngpu)))\n\nif args.ngpu > 0:\n    net.cuda()\n    # torch.cuda.manual_seed(1)\n\ncudnn.benchmark = True  # fire on all cylinders\n\n# /////////////// Calibration Prelims ///////////////\n\nood_num_examples = test_data.data.size(0) // 5\nexpected_ap = ood_num_examples / (ood_num_examples + test_data.data.size(0))\n\nconcat = lambda x: np.concatenate(x, axis=0)\nto_np = lambda x: x.data.cpu().numpy()\n\n\ndef get_net_results(data_loader, in_dist=False, t=1):\n    logits = []\n    confidence = []\n    correct = []\n\n    with torch.no_grad():\n        for batch_idx, (data, target) in enumerate(data_loader):\n            if batch_idx >= ood_num_examples // args.test_bs and in_dist is False:\n                break\n            data, target = data.view(-1, 1, 28, 28).cuda(), target.cuda()\n\n            output = net(data)\n\n            logits.extend(to_np(output).squeeze())\n\n            if args.use_01:\n                confidence.extend(to_np(\n                    (F.softmax(output/t, dim=1).max(1)[0] - 1./num_classes)/(1 - 1./num_classes)\n                ).squeeze().tolist())\n            else:\n                confidence.extend(to_np(F.softmax(output/t, dim=1).max(1)[0]).squeeze().tolist())\n\n            if in_dist:\n                pred = output.data.max(1)[1]\n                correct.extend(pred.eq(target).cpu().numpy().squeeze().tolist())\n\n    if in_dist:\n        return logits.copy(), confidence.copy(), correct.copy()\n    else:\n        return logits[:ood_num_examples].copy(), confidence[:ood_num_examples].copy()\n\n\nval_logits, val_confidence, val_correct = get_net_results(val_loader, in_dist=True)\n\nprint(\'\\nTuning Softmax Temperature\')\nval_labels = val_data.parent_ds.targets[val_data.offset:]\nt_star = tune_temp(val_logits, val_labels)\nprint(\'Softmax Temperature Tuned. Temperature is {:.3f}\'.format(t_star))\n\ntest_logits, test_confidence, test_correct = get_net_results(test_loader, in_dist=True, t=t_star)\n\nprint(\'Error Rate {:.2f}\'.format(100*(len(test_correct) - sum(test_correct))/len(test_correct)))\n\n# /////////////// End Calibration Prelims ///////////////\n\nprint(\'\\nUsing MNIST as typical data\')\n\n# /////////////// In-Distribution Calibration ///////////////\n\nprint(\'\\n\\nIn-Distribution Data\')\nshow_calibration_results(np.array(test_confidence), np.array(test_correct), method_name=args.method_name)\n\n# /////////////// OOD Calibration ///////////////\n\nrms_list, mad_list, sf1_list = [], [], []\n\n\ndef get_and_print_results(ood_loader, num_to_avg=args.num_to_avg):\n\n    rmss, mads, sf1s = [], [], []\n    for _ in range(num_to_avg):\n        out_logits, out_confidence = get_net_results(ood_loader, t=t_star)\n\n        measures = get_measures(\n            concat([out_confidence, test_confidence]),\n            concat([np.zeros(len(out_confidence)), test_correct]))\n\n        rmss.append(measures[0]); mads.append(measures[1]); sf1s.append(measures[2])\n\n    rms = np.mean(rmss); mad = np.mean(mads); sf1 = np.mean(sf1s)\n    rms_list.append(rms); mad_list.append(mad); sf1_list.append(sf1)\n\n    if num_to_avg >= 5:\n        print_measures_with_std(rmss, mads, sf1s, args.method_name)\n    else:\n        print_measures(rms, mad, sf1, args.method_name)\n\n\n# /////////////// Gaussian Noise ///////////////\n\ndummy_targets = torch.ones(ood_num_examples*args.num_to_avg)\nood_data = torch.from_numpy(\n    np.clip(np.random.normal(size=(ood_num_examples*args.num_to_avg, 1, 28, 28),\n                             loc=0.5, scale=0.5).astype(np.float32), 0, 1))\nood_data = torch.utils.data.TensorDataset(ood_data, dummy_targets)\nood_loader = torch.utils.data.DataLoader(ood_data, batch_size=args.test_bs, shuffle=True)\n\nprint(\'\\n\\nGaussian Noise (mu = sigma = 0.5) Calibration\')\nget_and_print_results(ood_loader)\n\n# /////////////// Bernoulli Noise ///////////////\n\ndummy_targets = torch.ones(ood_num_examples*args.num_to_avg)\nood_data = torch.from_numpy(np.random.binomial(\n    n=1, p=0.5, size=(ood_num_examples*args.num_to_avg, 1, 28, 28)).astype(np.float32))\nood_data = torch.utils.data.TensorDataset(ood_data, dummy_targets)\nood_loader = torch.utils.data.DataLoader(ood_data, batch_size=args.test_bs, shuffle=True)\n\nprint(\'\\n\\nBernoulli Noise Calibration\')\nget_and_print_results(ood_loader)\n\n# /////////////// CIFAR data ///////////////\n\nood_data = dset.CIFAR10(\n    \'/share/data/vision-greg/cifarpy\', train=False,\n    transform=trn.Compose([trn.Resize(28),\n                           trn.Lambda(lambda x: x.convert(\'L\', (0.2989, 0.5870, 0.1140, 0))),\n                           trn.ToTensor()]))\nood_loader = torch.utils.data.DataLoader(ood_data, batch_size=args.test_bs, shuffle=True,\n                                         num_workers=args.prefetch, pin_memory=True)\n\nprint(\'\\n\\nCIFAR-10 Calibration\')\nget_and_print_results(ood_loader)\n\n# /////////////// Icons-50 ///////////////\n\nood_data = dset.ImageFolder(\'/share/data/vision-greg/DistortedImageNet/Icons-50\',\n                            transform=trn.Compose([trn.Resize((28, 28)),\n                                                   trn.Lambda(lambda x: x.convert(\'L\', (0.2989, 0.5870, 0.1140, 0))),\n                                                   trn.ToTensor()]))\n\nfiltered_imgs = []\nfor img in ood_data.imgs:\n    if \'numbers\' not in img[0]:     # img[0] is image name\n        filtered_imgs.append(img)\nood_data.imgs = filtered_imgs\n\nood_loader = torch.utils.data.DataLoader(ood_data, batch_size=args.test_bs, shuffle=True)\n\nprint(\'\\n\\nIcons-50 Calibration\')\nget_and_print_results(ood_loader)\n\n# /////////////// Fashion-MNIST ///////////////\n\nood_data = dset.FashionMNIST(\'/share/data/vision-greg/fashion_mnist\', train=False,\n                             transform=trn.ToTensor(), download=False)\nood_loader = torch.utils.data.DataLoader(ood_data, batch_size=args.test_bs, shuffle=True)\n\nprint(\'\\n\\nFashion-MNIST Calibration\')\nget_and_print_results(ood_loader)\n\n# /////////////// Negative MNIST ///////////////\n\nood_data = dset.MNIST(\'/home-nfs/dan/cifar_data/mnist\', train=False,\n                      transform=trn.Compose([trn.ToTensor(), trn.Lambda(lambda img: 1 - img)]))\nood_loader = torch.utils.data.DataLoader(ood_data, batch_size=args.test_bs, shuffle=True)\n\nprint(\'\\n\\nNegative MNIST Calibration\')\nget_and_print_results(ood_loader)\n\n# /////////////// notMNIST ///////////////\n\npickle_file = \'/share/data/vision-greg2/users/dan/datasets/notMNIST.pickle\'\nwith open(pickle_file, \'rb\') as f:\n    notMNIST_data = pickle.load(f, encoding=\'latin1\')\n    notMNIST_data = notMNIST_data[\'test_dataset\'].reshape((-1, 28 * 28)) + 0.5\n\ndummy_targets = torch.ones(min(ood_num_examples*args.num_to_avg, notMNIST_data.shape[0]))\nood_data = torch.utils.data.TensorDataset(torch.from_numpy(\n    notMNIST_data[:ood_num_examples*args.num_to_avg].astype(np.float32)), dummy_targets)\nood_loader = torch.utils.data.DataLoader(ood_data, batch_size=args.test_bs, shuffle=True)\n\nprint(\'\\n\\nnotMNIST Calibration\')\nget_and_print_results(ood_loader)\n\n# /////////////// Omniglot ///////////////\n\nimport scipy.io as sio\nimport scipy.misc as scimisc\n\n# other alphabets have characters which look like digits\nsafe_list = [0, 2, 5, 6, 8, 12, 13, 14, 15, 16, 17, 18, 19, 21, 26]\nm = sio.loadmat(""/share/data/vision-greg2/users/dan/datasets/omniglot.mat"")\n\nsquished_set = []\nfor safe_number in safe_list:\n    for alphabet in m[\'images\'][safe_number]:\n        for letters in alphabet:\n            for letter in letters:\n                for example in letter:\n                    squished_set.append(scimisc.imresize(1 - example[0], (28, 28)).reshape(1, 28 * 28))\n\nomni_images = np.concatenate(squished_set, axis=0)\n\ndummy_targets = torch.ones(min(ood_num_examples*args.num_to_avg, len(omni_images)))\nood_data = torch.utils.data.TensorDataset(torch.from_numpy(\n    omni_images[:ood_num_examples*args.num_to_avg].astype(np.float32)), dummy_targets)\nood_loader = torch.utils.data.DataLoader(ood_data, batch_size=args.test_bs, shuffle=True)\n\nprint(\'\\n\\nOmniglot Calibration\')\nget_and_print_results(ood_loader)\n\n# /////////////// Mean Results ///////////////\n\nprint(\'\\n\\nMean Test Results\')\nprint_measures(np.mean(rms_list), np.mean(mad_list), np.mean(sf1_list), method_name=args.method_name)\n\n\n# /////////////// OOD Detection of Validation Distributions ///////////////\n\nif args.validate is False:\n    exit()\n\nrms_list, mad_list, sf1_list = [], [], []\n\n# /////////////// Uniform Noise ///////////////\n\ndummy_targets = torch.ones(ood_num_examples*args.num_to_avg)\nood_data = torch.from_numpy(\n    np.random.uniform(size=(ood_num_examples*args.num_to_avg, 1, 28, 28),\n                      low=0.0, high=1.0).astype(np.float32))\nood_data = torch.utils.data.TensorDataset(ood_data, dummy_targets)\nood_loader = torch.utils.data.DataLoader(ood_data, batch_size=args.test_bs, shuffle=True)\n\nprint(\'\\n\\nUniform[0,1] Noise Calibration\')\nget_and_print_results(ood_loader)\n\n# /////////////// Arithmetic Mean of Images ///////////////\n\n\nclass AvgOfPair(torch.utils.data.Dataset):\n    def __init__(self, dataset):\n        self.dataset = dataset\n        self.shuffle_indices = np.arange(len(dataset))\n        np.random.shuffle(self.shuffle_indices)\n\n    def __getitem__(self, i):\n        random_idx = np.random.choice(len(self.dataset))\n        while random_idx == i:\n            random_idx = np.random.choice(len(self.dataset))\n\n        return self.dataset[i][0]/2. + self.dataset[random_idx][0]/2., 0\n\n    def __len__(self):\n        return len(self.dataset)\n\n\nood_loader = torch.utils.data.DataLoader(\n    AvgOfPair(test_data), batch_size=args.test_bs, shuffle=True,\n    num_workers=args.prefetch, pin_memory=True)\n\nprint(\'\\n\\nArithmetic Mean of Random Image Pair Calibration\')\nget_and_print_results(ood_loader)\n\n# /////////////// Geometric Mean of Images ///////////////\n\n\nclass GeomMeanOfPair(torch.utils.data.Dataset):\n    def __init__(self, dataset):\n        self.dataset = dataset\n        self.shuffle_indices = np.arange(len(dataset))\n        np.random.shuffle(self.shuffle_indices)\n\n    def __getitem__(self, i):\n        random_idx = np.random.choice(len(self.dataset))\n        while random_idx == i:\n            random_idx = np.random.choice(len(self.dataset))\n\n        return torch.sqrt(self.dataset[i][0] * self.dataset[random_idx][0]), 0\n\n    def __len__(self):\n        return len(self.dataset)\n\n\nood_loader = torch.utils.data.DataLoader(\n    GeomMeanOfPair(test_data), batch_size=args.test_bs, shuffle=True,\n    num_workers=args.prefetch, pin_memory=True)\n\nprint(\'\\n\\nGeometric Mean of Random Image Pair Calibration\')\nget_and_print_results(ood_loader)\n\n# /////////////// Jigsaw Images ///////////////\n\nood_loader = torch.utils.data.DataLoader(test_data, batch_size=args.test_bs, shuffle=True,\n                                         num_workers=args.prefetch, pin_memory=True)\n\njigsaw = lambda x: torch.cat((\n    torch.cat((torch.cat((x[:, 6:14, :14], x[:, :6, :14]), 1),\n               x[:, 14:, :14]), 2),\n    torch.cat((x[:, 14:, 14:],\n               torch.cat((x[:, :14, 22:], x[:, :14, 14:22]), 2)), 2),\n), 1)\n\n\nood_loader.dataset.transform = trn.Compose([trn.ToTensor(), jigsaw])\n\nprint(\'\\n\\nJigsawed Images Calibration\')\nget_and_print_results(ood_loader)\n\n# /////////////// Speckled Images ///////////////\n\nspeckle = lambda x: torch.clamp(x + x * torch.randn_like(x), 0, 1)\nood_loader.dataset.transform = trn.Compose([trn.ToTensor(), speckle])\n\nprint(\'\\n\\nSpeckle Noised Images Calibration\')\nget_and_print_results(ood_loader)\n\n# /////////////// Pixelated Images ///////////////\n\npixelate = lambda x: x.resize((int(28 * 0.2), int(28 * 0.2)), PILImage.BOX).resize((28, 28), PILImage.BOX)\nood_loader.dataset.transform = trn.Compose([pixelate, trn.ToTensor()])\n\nprint(\'\\n\\nPixelate Calibration\')\nget_and_print_results(ood_loader)\n\n# /////////////// Mirrored MNIST digits ///////////////\n\nidxs = test_data.targets\nvert_idxs = np.squeeze(np.logical_and(idxs != 3, np.logical_and(idxs != 0, np.logical_and(idxs != 1, idxs != 8))))\nvert_digits = test_data.data.numpy()[vert_idxs][:, ::-1, :]\n\nhoriz_idxs = np.squeeze(np.logical_and(idxs != 0, np.logical_and(idxs != 1, idxs != 8)))\nhoriz_digits = test_data.data.numpy()[horiz_idxs][:, :, ::-1]\n\nflipped_digits = concat((vert_digits, horiz_digits))\n\ndummy_targets = torch.ones(flipped_digits.shape[0])\nood_data = torch.from_numpy(flipped_digits.astype(np.float32) / 255)\nood_data = torch.utils.data.TensorDataset(ood_data, dummy_targets)\nood_loader = torch.utils.data.DataLoader(ood_data, batch_size=args.test_bs, shuffle=True,\n                                         num_workers=args.prefetch)\n\nprint(\'\\n\\nMirrored MNIST Digit Calibration\')\nget_and_print_results(ood_loader)\n\n# /////////////// Mean Results ///////////////\n\nprint(\'\\n\\nMean Validation Results\')\nprint_measures(np.mean(rms_list), np.mean(mad_list), np.mean(sf1_list), method_name=args.method_name)\n'"
NLP_classification/download_wikitext.py,4,"b'# -*- coding: utf-8 -*-\n""""""\nTrains a MNIST classifier.\n""""""\n\nimport numpy as np\nimport sys\nimport os\nimport pickle\nimport argparse\nimport math\nimport time\nfrom bisect import bisect_left\nimport torch\nimport torch.nn as nn\nimport torch.backends.cudnn as cudnn\nimport torchvision.transforms as trn\nimport torchvision.datasets as dset\nimport torch.nn.functional as F\nfrom torch.autograd import Variable as V\nimport torchtext\n\nfrom torchtext import data\nfrom torchtext import datasets\n\nimport tqdm\n\n\n\nnp.random.seed(1)\n\nparser = argparse.ArgumentParser(description=\'SST OE\',\n                                 formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n# Optimization options\nparser.add_argument(\'--epochs\', \'-e\', type=int, default=5, help=\'Number of epochs to train.\')\nparser.add_argument(\'--batch_size\', \'-b\', type=int, default=64, help=\'Batch size.\')\nparser.add_argument(\'--learning_rate\', \'-lr\', type=float, default=0.01, help=\'The initial learning rate.\')\nparser.add_argument(\'--momentum\', \'-m\', type=float, default=0.5, help=\'Momentum.\')\nparser.add_argument(\'--test_bs\', type=int, default=256)\n# Checkpoints\nparser.add_argument(\'--save\', \'-s\', type=str, default=\'./snapshots\', help=\'Folder to save checkpoints.\')\nparser.add_argument(\'--load\', \'-l\', type=str, default=\'./snapshots\', help=\'Checkpoint path to resume / test.\')\nparser.add_argument(\'--test\', \'-t\', action=\'store_true\', help=\'Test only flag.\')\nparser.add_argument(\'--mix\', dest=\'mix\', action=\'store_true\', help=\'Mix outliers images with in-dist images.\')\n# Acceleration\nparser.add_argument(\'--prefetch\', type=int, default=2, help=\'Pre-fetching threads.\')\nargs = parser.parse_args()\n\n\n# ============================ SST ============================ #\n# set up fields\nTEXT_sst = data.Field(pad_first=True)\nLABEL_sst = data.Field(sequential=False)\n\n# make splits for data\ntrain_sst, val_sst, test_sst = datasets.SST.splits(\n    TEXT_sst, LABEL_sst, fine_grained=False, train_subtrees=False,\n    filter_pred=lambda ex: ex.label != \'neutral\')\n\n# build vocab\nTEXT_sst.build_vocab(train_sst, max_size=10000)\nLABEL_sst.build_vocab(train_sst, max_size=10000)\nprint(\'vocab length (including special tokens):\', len(TEXT_sst.vocab))\n\n# create our own iterator, avoiding the calls to build_vocab in SST.iters\ntrain_iter_sst, val_iter_sst, test_iter_sst = data.BucketIterator.splits(\n    (train_sst, val_sst, test_sst), batch_size=args.batch_size, repeat=False)\n# ============================ SST ============================ #\n\n# ============================ WikiText-2 ============================ #\n\n# set up fields\nTEXT_wtxt = data.Field(pad_first=True, lower=True)\n\n# make splits for data\ntrain_OE, val_OE, test_OE = datasets.WikiText2.splits(TEXT_wtxt)\n\n# build vocab\nTEXT_wtxt.build_vocab(train_sst.text, max_size=10000)\nprint(\'vocab length (including special tokens):\', len(TEXT_wtxt.vocab))\n\n# create our own iterator, avoiding the calls to build_vocab in SST.iters\ntrain_iter_oe, val_iter_oe, test_iter_oe = data.BPTTIterator.splits(\n    (train_OE, val_OE, test_OE), batch_size=args.batch_size, bptt_len=15, repeat=False)\n\n# ============================ WikiText-2 ============================ #\n\n# ============================ WikiText-103 ============================ #\n\n# set up fields\nTEXT_wtxt = data.Field(pad_first=True, lower=True)\n\n# make splits for data\ntrain_OE, val_OE, test_OE = datasets.WikiText103.splits(TEXT_wtxt)\n\n# build vocab\nTEXT_wtxt.build_vocab(train_sst.text, max_size=10000)\nprint(\'vocab length (including special tokens):\', len(TEXT_wtxt.vocab))\n\n# create our own iterator, avoiding the calls to build_vocab in SST.iters\ntrain_iter_oe, val_iter_oe, test_iter_oe = data.BPTTIterator.splits(\n    (train_OE, val_OE, test_OE), batch_size=args.batch_size, bptt_len=15, repeat=False)\n\n# ============================ WikiText-103 ============================ #\n\nexit()'"
NLP_classification/eval_OOD_20ng.py,9,"b'# -*- coding: utf-8 -*-\n""""""\nTrains a MNIST classifier.\n""""""\n\nimport numpy as np\nimport sys\nimport os\nimport pickle\nimport argparse\nimport math\nimport time\nfrom bisect import bisect_left\nimport torch\nimport torch.nn as nn\nimport torch.backends.cudnn as cudnn\nimport torchvision.transforms as trn\nimport torchvision.datasets as dset\nimport torch.nn.functional as F\nfrom torch.autograd import Variable as V\nimport torchtext\n\nfrom torchtext import data\nfrom torchtext import datasets\n\nimport tqdm\n\n\n\nnp.random.seed(1)\n\nparser = argparse.ArgumentParser(description=\'20NG OE\',\n                                 formatter_class=argparse.ArgumentDefaultsHelpFormatter)\nparser.add_argument(\'--batch_size\', \'-b\', type=int, default=64, help=\'Batch size.\')\nargs = parser.parse_args()\n\n\ntorch.set_grad_enabled(False)\ncudnn.benchmark = True  # fire on all cylinders\n\n\n# go through rigamaroo to do ..utils.display_results import show_performance\nif __package__ is None:\n    import sys\n    from os import path\n\n    sys.path.append(path.dirname(path.dirname(path.abspath(__file__))))\n    from utils.display_results import get_performance\n\n\n# ============================ 20 Newsgroups ============================ #\nTEXT_20ng = data.Field(pad_first=True, lower=True, fix_length=100)\nLABEL_20ng = data.Field(sequential=False)\n\ntrain_20ng = data.TabularDataset(path=\'./.data/20newsgroups/20ng-train.txt\',\n                                 format=\'csv\',\n                                 fields=[(\'label\', LABEL_20ng), (\'text\', TEXT_20ng)])\n\ntest_20ng = data.TabularDataset(path=\'./.data/20newsgroups/20ng-test.txt\',\n                                 format=\'csv\',\n                                 fields=[(\'label\', LABEL_20ng), (\'text\', TEXT_20ng)])\n\nTEXT_20ng.build_vocab(train_20ng, max_size=10000)\nLABEL_20ng.build_vocab(train_20ng, max_size=10000)\nprint(\'vocab length (including special tokens):\', len(TEXT_20ng.vocab))\n\ntrain_iter_20ng = data.BucketIterator(train_20ng, batch_size=args.batch_size, repeat=False)\ntest_iter_20ng = data.BucketIterator(test_20ng, batch_size=args.batch_size, repeat=False)\n# ============================ 20 Newsgroups ============================ #\n\nood_num_examples = len(test_iter_20ng.dataset) // 5\nexpected_ap = ood_num_examples / (ood_num_examples + len(test_iter_20ng.dataset))\nrecall_level = 0.9\n\n# ============================ IMDB ============================ #\n\n# set up fields\nTEXT_imdb = data.Field(pad_first=True, lower=True)\nLABEL_imdb = data.Field(sequential=False)\n\n# make splits for data\ntrain_imdb, test_imdb = datasets.IMDB.splits(TEXT_imdb, LABEL_imdb)\n\n# build vocab\nTEXT_imdb.build_vocab(train_20ng.text, max_size=10000)\nLABEL_imdb.build_vocab(train_imdb, max_size=10000)\nprint(\'vocab length (including special tokens):\', len(TEXT_imdb.vocab))\n\n# make iterators\ntrain_iter_imdb, test_iter_imdb = data.BucketIterator.splits(\n    (train_imdb, test_imdb), batch_size=args.batch_size, repeat=False)\n\n# ============================ IMDB ============================ #\n\n# ============================ SNLI ============================ #\n\n# set up fields\nTEXT_snli = data.Field(pad_first=True, lower=True)\nLABEL_snli = data.Field(sequential=False)\n\n# make splits for data\ntrain_snli, val_snli, test_snli = datasets.SNLI.splits(TEXT_snli, LABEL_snli)\n\n# build vocab\nTEXT_snli.build_vocab(train_20ng.text, max_size=10000)\nLABEL_snli.build_vocab(train_snli, max_size=10000)\nprint(\'vocab length (including special tokens):\', len(TEXT_snli.vocab))\n\n# make iterators\ntrain_iter_snli, val_iter_snli, test_iter_snli = data.BucketIterator.splits(\n    (train_snli, val_snli, test_snli), batch_size=args.batch_size, repeat=False)\n\n# ============================ SNLI ============================ #\n\n# ============================ Multi30K ============================ #\nTEXT_m30k = data.Field(pad_first=True, lower=True)\n\nm30k_data = data.TabularDataset(path=\'./.data/multi30k/train.txt\',\n                                  format=\'csv\',\n                                  fields=[(\'text\', TEXT_m30k)])\n\nTEXT_m30k.build_vocab(train_20ng.text, max_size=10000)\nprint(\'vocab length (including special tokens):\', len(TEXT_m30k.vocab))\n\ntrain_iter_m30k = data.BucketIterator(m30k_data, batch_size=args.batch_size, repeat=False)\n# ============================ Multi30K ============================ #\n\n# ============================ WMT16 ============================ #\nTEXT_wmt16 = data.Field(pad_first=True, lower=True)\n\nwmt16_data = data.TabularDataset(path=\'./.data/wmt16/wmt16_sentences\',\n                                  format=\'csv\',\n                                  fields=[(\'text\', TEXT_wmt16)])\n\nTEXT_wmt16.build_vocab(train_20ng.text, max_size=10000)\nprint(\'vocab length (including special tokens):\', len(TEXT_wmt16.vocab))\n\ntrain_iter_wmt16 = data.BucketIterator(wmt16_data, batch_size=args.batch_size, repeat=False)\n# ============================ WMT16 ============================ #\n\n# ============================ English Web Treebank (Answers) ============================ #\n\nTEXT_answers = data.Field(pad_first=True, lower=True)\n\ntreebank_path = \'./.data/eng_web_tbk/answers/conll/answers_penntrees.dev.conll\'\n\ntrain_answers = datasets.SequenceTaggingDataset(path=treebank_path, fields=((None, None), (\'text\', TEXT_answers)))\n\nTEXT_answers.build_vocab(train_20ng.text, max_size=10000)\nprint(\'vocab length (including special tokens):\', len(TEXT_answers.vocab))\n\n# make iterators\ntrain_iter_answers = data.BucketIterator.splits(\n    (train_answers,), batch_size=args.batch_size, repeat=False)[0]\n\n# ============================ English Web Treebank (Answers) ============================ #\n\n# ============================ English Web Treebank (Email) ============================ #\n\nTEXT_email = data.Field(pad_first=True, lower=True)\n\ntreebank_path = \'./.data/eng_web_tbk/email/conll/email_penntrees.dev.conll\'\n\ntrain_email = datasets.SequenceTaggingDataset(path=treebank_path, fields=((None, None), (\'text\', TEXT_email)))\n\nTEXT_email.build_vocab(train_20ng.text, max_size=10000)\nprint(\'vocab length (including special tokens):\', len(TEXT_email.vocab))\n\n# make iterators\ntrain_iter_email = data.BucketIterator.splits(\n    (train_email,), batch_size=args.batch_size, repeat=False)[0]\n\n# ============================ English Web Treebank (Email) ============================ #\n\n# ============================ English Web Treebank (Newsgroup) ============================ #\n\nTEXT_newsgroup = data.Field(pad_first=True, lower=True)\n\ntreebank_path = \'./.data/eng_web_tbk/newsgroup/conll/newsgroup_penntrees.dev.conll\'\n\ntrain_newsgroup = datasets.SequenceTaggingDataset(path=treebank_path, fields=((None, None), (\'text\', TEXT_newsgroup)))\n\nTEXT_newsgroup.build_vocab(train_20ng.text, max_size=10000)\nprint(\'vocab length (including special tokens):\', len(TEXT_newsgroup.vocab))\n\n# make iterators\ntrain_iter_newsgroup = data.BucketIterator.splits(\n    (train_newsgroup,), batch_size=args.batch_size, repeat=False)[0]\n\n# ============================ English Web Treebank (Newsgroup) ============================ #\n\n# ============================ English Web Treebank (Reviews) ============================ #\n\nTEXT_reviews = data.Field(pad_first=True, lower=True)\n\ntreebank_path = \'./.data/eng_web_tbk/reviews/conll/reviews_penntrees.dev.conll\'\n\ntrain_reviews = datasets.SequenceTaggingDataset(path=treebank_path, fields=((None, None), (\'text\', TEXT_reviews)))\n\nTEXT_reviews.build_vocab(train_20ng.text, max_size=10000)\nprint(\'vocab length (including special tokens):\', len(TEXT_reviews.vocab))\n\n# make iterators\ntrain_iter_reviews = data.BucketIterator.splits(\n    (train_reviews,), batch_size=args.batch_size, repeat=False)[0]\n\n# ============================ English Web Treebank (Reviews) ============================ #\n\n# ============================ English Web Treebank (Weblog) ============================ #\n\nTEXT_weblog = data.Field(pad_first=True, lower=True)\n\ntreebank_path = \'./.data/eng_web_tbk/weblog/conll/weblog_penntrees.dev.conll\'\n\ntrain_weblog = datasets.SequenceTaggingDataset(path=treebank_path, fields=((None, None), (\'text\', TEXT_weblog)))\n\nTEXT_weblog.build_vocab(train_20ng.text, max_size=10000)\nprint(\'vocab length (including special tokens):\', len(TEXT_weblog.vocab))\n\n# make iterators\ntrain_iter_weblog = data.BucketIterator.splits(\n    (train_weblog,), batch_size=args.batch_size, repeat=False)[0]\n\n# ============================ English Web Treebank (Weblog) ============================ #\n\n# ============================ Yelp Reviews ============================ #\nTEXT_yelp = data.Field(pad_first=True, lower=True)\n\nyelp_data = data.TabularDataset(path=\'./.data/yelp_review_full_csv/test.csv\',\n                                  format=\'csv\',\n                                  fields=[(None, None), (\'text\', TEXT_yelp)])\n\nTEXT_yelp.build_vocab(train_20ng.text, max_size=10000)\nprint(\'vocab length (including special tokens):\', len(TEXT_yelp.vocab))\n\ntrain_iter_yelp = data.BucketIterator(yelp_data, batch_size=args.batch_size, repeat=False)\n# ============================ Yelp Reviews ============================ #\n\n\n\n\nclass ClfGRU(nn.Module):\n    def __init__(self, num_classes):\n        super().__init__()\n        self.embedding = nn.Embedding(len(TEXT_20ng.vocab), 50, padding_idx=1)\n        self.gru = nn.GRU(input_size=50, hidden_size=128, num_layers=2, bias=True, batch_first=True, bidirectional=False)\n        self.linear = nn.Linear(128, num_classes)\n        self.num_classes = num_classes\n\n    def forward(self, x):\n        embeds = self.embedding(x)\n        hidden = self.gru(embeds)[1][1]  # select h_n, and select the 2nd layer\n        logits = self.linear(hidden)\n        return logits\n\n\n\nmodel = ClfGRU(20).cuda()\nmodel.load_state_dict(torch.load(\'./snapshots/20ng/baseline/model.dict\'))\nprint(\'\\nLoaded model.\\n\')\n\n\n\ndef get_scores(dataset_iterator, ood=False, snli=False):\n    model.eval()\n\n    outlier_scores = []\n\n    for batch_idx, batch in enumerate(iter(dataset_iterator)):\n        if ood and (batch_idx * args.batch_size > ood_num_examples):\n            break\n\n        if snli:\n            inputs = batch.hypothesis.t()\n        else:\n            inputs = batch.text.t()\n\n        logits = model(inputs)\n        smax = F.softmax(logits - torch.max(logits, dim=1, keepdim=True)[0], dim=1)\n        msp = -1 * torch.max(smax, dim=1)[0]\n\n        # ce_to_unif = F.log_softmax(logits - torch.max(logits, dim=1, keepdim=True)[0], dim=1).mean(1)\n\n        outlier_scores.extend(list(msp.data.cpu().numpy()))\n\n    return outlier_scores\n\n\n\n# ============================ OE ============================ #\n\ntest_scores = get_scores(test_iter_20ng)\n\ntitles = [\'SNLI\', \'IMDB\', \'Multi30K\', \'WMT16\', \'English Web Treebank (Answers)\',\n          \'English Web Treebank (Email)\', \'English Web Treebank (Newsgroup)\',\n          \'English Web Treebank (Reviews)\', \'English Web Treebank (Weblog)\',\n          \'Yelp Reviews\']\n\niterators = [test_iter_snli, test_iter_imdb, train_iter_m30k, train_iter_wmt16, train_iter_answers,\n             train_iter_email, train_iter_newsgroup, train_iter_reviews, train_iter_weblog,\n             train_iter_yelp]\n\n\nmean_fprs = []\nmean_aurocs = []\nmean_auprs = []\n\nfor i in range(len(titles)):\n    title = titles[i]\n    iterator = iterators[i]\n\n    print(\'\\n{}\'.format(title))\n    fprs, aurocs, auprs = [], [], []\n    for i in range(10):\n        ood_scores = get_scores(iterator, ood=True, snli=True) if \'SNLI\' in title else get_scores(iterator, ood=True)\n        fpr, auroc, aupr = get_performance(ood_scores, test_scores, expected_ap, recall_level=recall_level)\n        fprs.append(fpr)\n        aurocs.append(auroc)\n        auprs.append(aupr)\n\n    print(\'FPR{:d}:\\t\\t\\t{:.4f} ({:.4f})\'.format(int(100 * recall_level), np.mean(fprs), np.std(fprs)))\n    print(\'AUROC:\\t\\t\\t{:.4f} ({:.4f})\'.format(np.mean(aurocs), np.std(aurocs)))\n    print(\'AUPR:\\t\\t\\t{:.4f} ({:.4f})\'.format(np.mean(auprs), np.std(auprs)))\n\n    mean_fprs.append(np.mean(fprs))\n    mean_aurocs.append(np.mean(aurocs))\n    mean_auprs.append(np.mean(auprs))\n\nprint()\nprint(\'OOD dataset mean FPR: {:.4f}\'.format(np.mean(mean_fprs)))\nprint(\'OOD dataset mean AUROC: {:.4f}\'.format(np.mean(mean_aurocs)))\nprint(\'OOD dataset mean AUPR: {:.4f}\'.format(np.mean(mean_auprs)))'"
NLP_classification/eval_OOD_sst.py,11,"b'# -*- coding: utf-8 -*-\n""""""\nTrains a MNIST classifier.\n""""""\n\nimport numpy as np\nimport sys\nimport os\nimport pickle\nimport argparse\nimport math\nimport time\nfrom bisect import bisect_left\nimport torch\nimport torch.nn as nn\nimport torch.backends.cudnn as cudnn\nimport torchvision.transforms as trn\nimport torchvision.datasets as dset\nimport torch.nn.functional as F\nfrom torch.autograd import Variable as V\nimport torchtext\n\nfrom torchtext import data\nfrom torchtext import datasets\nimport spacy\nimport re\n\nimport tqdm\n\n\n\nnp.random.seed(1)\n\nparser = argparse.ArgumentParser(description=\'SST OE\',\n                                 formatter_class=argparse.ArgumentDefaultsHelpFormatter)\nparser.add_argument(\'--batch_size\', \'-b\', type=int, default=64, help=\'Batch size.\')\nargs = parser.parse_args()\n\n\ntorch.set_grad_enabled(False)\ncudnn.benchmark = True  # fire on all cylinders\n\n\n# go through rigamaroo to do ..utils.display_results import show_performance\nif __package__ is None:\n    import sys\n    from os import path\n\n    sys.path.append(path.dirname(path.dirname(path.abspath(__file__))))\n    from utils.display_results import get_performance\n\n\n\n# ============================ SST ============================ #\n# set up fields\nTEXT_sst = data.Field(pad_first=True)\nLABEL_sst = data.Field(sequential=False)\n\n# make splits for data\ntrain_sst, val_sst, test_sst = datasets.SST.splits(\n    TEXT_sst, LABEL_sst, fine_grained=False, train_subtrees=False,\n    filter_pred=lambda ex: ex.label != \'neutral\')\n\n# build vocab\nTEXT_sst.build_vocab(train_sst, max_size=10000)\nLABEL_sst.build_vocab(train_sst, max_size=10000)\nprint(\'vocab length (including special tokens):\', len(TEXT_sst.vocab))\n\n# create our own iterator, avoiding the calls to build_vocab in SST.iters\ntrain_iter_sst, val_iter_sst, test_iter_sst = data.BucketIterator.splits(\n    (train_sst, val_sst, test_sst), batch_size=args.batch_size, repeat=False)\n# ============================ SST ============================ #\n\nood_num_examples = len(test_iter_sst.dataset) // 5\nexpected_ap = ood_num_examples / (ood_num_examples + len(test_iter_sst.dataset))\nrecall_level = 0.9\n\n# ============================ IMDB ============================ #\n\n# set up fields\nTEXT_imdb = data.Field(pad_first=True, lower=True)\nLABEL_imdb = data.Field(sequential=False)\n\n# make splits for data\ntrain_imdb, test_imdb = datasets.IMDB.splits(TEXT_imdb, LABEL_imdb)\n\n# build vocab\nTEXT_imdb.build_vocab(train_sst.text, max_size=10000)\nLABEL_imdb.build_vocab(train_imdb, max_size=10000)\nprint(\'vocab length (including special tokens):\', len(TEXT_imdb.vocab))\n\n# make iterators\ntrain_iter_imdb, test_iter_imdb = data.BucketIterator.splits(\n    (train_imdb, test_imdb), batch_size=args.batch_size, repeat=False)\n\n# ============================ IMDB ============================ #\n\n# ============================ SNLI ============================ #\n\n# set up fields\nTEXT_snli = data.Field(pad_first=True, lower=True)\nLABEL_snli = data.Field(sequential=False)\n\n# make splits for data\ntrain_snli, val_snli, test_snli = datasets.SNLI.splits(TEXT_snli, LABEL_snli)\n\n# build vocab\nTEXT_snli.build_vocab(train_sst.text, max_size=10000)\nLABEL_snli.build_vocab(train_snli, max_size=10000)\nprint(\'vocab length (including special tokens):\', len(TEXT_snli.vocab))\n\n# make iterators\ntrain_iter_snli, val_iter_snli, test_iter_snli = data.BucketIterator.splits(\n    (train_snli, val_snli, test_snli), batch_size=args.batch_size, repeat=False)\n\n# ============================ SNLI ============================ #\n\n# ============================ Multi30K ============================ #\nTEXT_m30k = data.Field(pad_first=True, lower=True)\n\nm30k_data = data.TabularDataset(path=\'./.data/multi30k/train.txt\',\n                                  format=\'csv\',\n                                  fields=[(\'text\', TEXT_m30k)])\n\nTEXT_m30k.build_vocab(train_sst.text, max_size=10000)\nprint(\'vocab length (including special tokens):\', len(TEXT_m30k.vocab))\n\ntrain_iter_m30k = data.BucketIterator(m30k_data, batch_size=args.batch_size, repeat=False)\n# ============================ Multi30K ============================ #\n\n# ============================ WMT16 ============================ #\nTEXT_wmt16 = data.Field(pad_first=True, lower=True)\n\nwmt16_data = data.TabularDataset(path=\'./.data/wmt16/wmt16_sentences\',\n                                  format=\'csv\',\n                                  fields=[(\'text\', TEXT_wmt16)])\n\nTEXT_wmt16.build_vocab(train_sst.text, max_size=10000)\nprint(\'vocab length (including special tokens):\', len(TEXT_wmt16.vocab))\n\ntrain_iter_wmt16 = data.BucketIterator(wmt16_data, batch_size=args.batch_size, repeat=False)\n# ============================ WMT16 ============================ #\n\n# ============================ English Web Treebank (Answers) ============================ #\n\nTEXT_answers = data.Field(pad_first=True, lower=True)\n\ntreebank_path = \'./.data/eng_web_tbk/answers/conll/answers_penntrees.dev.conll\'\n\ntrain_answers = datasets.SequenceTaggingDataset(path=treebank_path, fields=((None, None), (\'text\', TEXT_answers)))\n\nTEXT_answers.build_vocab(train_sst.text, max_size=10000)\nprint(\'vocab length (including special tokens):\', len(TEXT_answers.vocab))\n\n# make iterators\ntrain_iter_answers = data.BucketIterator.splits(\n    (train_answers,), batch_size=args.batch_size, repeat=False)[0]\n\n# ============================ English Web Treebank (Answers) ============================ #\n\n# ============================ English Web Treebank (Email) ============================ #\n\nTEXT_email = data.Field(pad_first=True, lower=True)\n\ntreebank_path = \'./.data/eng_web_tbk/email/conll/email_penntrees.dev.conll\'\n\ntrain_email = datasets.SequenceTaggingDataset(path=treebank_path, fields=((None, None), (\'text\', TEXT_email)))\n\nTEXT_email.build_vocab(train_sst.text, max_size=10000)\nprint(\'vocab length (including special tokens):\', len(TEXT_email.vocab))\n\n# make iterators\ntrain_iter_email = data.BucketIterator.splits(\n    (train_email,), batch_size=args.batch_size, repeat=False)[0]\n\n# ============================ English Web Treebank (Email) ============================ #\n\n# ============================ English Web Treebank (Newsgroup) ============================ #\n\nTEXT_newsgroup = data.Field(pad_first=True, lower=True)\n\ntreebank_path = \'./.data/eng_web_tbk/newsgroup/conll/newsgroup_penntrees.dev.conll\'\n\ntrain_newsgroup = datasets.SequenceTaggingDataset(path=treebank_path, fields=((None, None), (\'text\', TEXT_newsgroup)))\n\nTEXT_newsgroup.build_vocab(train_sst.text, max_size=10000)\nprint(\'vocab length (including special tokens):\', len(TEXT_newsgroup.vocab))\n\n# make iterators\ntrain_iter_newsgroup = data.BucketIterator.splits(\n    (train_newsgroup,), batch_size=args.batch_size, repeat=False)[0]\n\n# ============================ English Web Treebank (Newsgroup) ============================ #\n\n# ============================ English Web Treebank (Reviews) ============================ #\n\nTEXT_reviews = data.Field(pad_first=True, lower=True)\n\ntreebank_path = \'./.data/eng_web_tbk/reviews/conll/reviews_penntrees.dev.conll\'\n\ntrain_reviews = datasets.SequenceTaggingDataset(path=treebank_path, fields=((None, None), (\'text\', TEXT_reviews)))\n\nTEXT_reviews.build_vocab(train_sst.text, max_size=10000)\nprint(\'vocab length (including special tokens):\', len(TEXT_reviews.vocab))\n\n# make iterators\ntrain_iter_reviews = data.BucketIterator.splits(\n    (train_reviews,), batch_size=args.batch_size, repeat=False)[0]\n\n# ============================ English Web Treebank (Reviews) ============================ #\n\n# ============================ English Web Treebank (Weblog) ============================ #\n\nTEXT_weblog = data.Field(pad_first=True, lower=True)\n\ntreebank_path = \'./.data/eng_web_tbk/weblog/conll/weblog_penntrees.dev.conll\'\n\ntrain_weblog = datasets.SequenceTaggingDataset(path=treebank_path, fields=((None, None), (\'text\', TEXT_weblog)))\n\nTEXT_weblog.build_vocab(train_sst.text, max_size=10000)\nprint(\'vocab length (including special tokens):\', len(TEXT_weblog.vocab))\n\n# make iterators\ntrain_iter_weblog = data.BucketIterator.splits(\n    (train_weblog,), batch_size=args.batch_size, repeat=False)[0]\n\n# ============================ English Web Treebank (Weblog) ============================ #\n\n# ============================ Yelp Reviews ============================ #\nTEXT_yelp = data.Field(pad_first=True, lower=True)\n\nyelp_data = data.TabularDataset(path=\'./.data/yelp_review_full_csv/test.csv\',\n                                  format=\'csv\',\n                                  fields=[(None, None), (\'text\', TEXT_yelp)])\n\nTEXT_yelp.build_vocab(train_sst.text, max_size=10000)\nprint(\'vocab length (including special tokens):\', len(TEXT_yelp.vocab))\n\ntrain_iter_yelp = data.BucketIterator(yelp_data, batch_size=args.batch_size, repeat=False)\n# ============================ Yelp Reviews ============================ #\n\n\n\n\nclass ClfGRU(nn.Module):\n    def __init__(self, num_classes):\n        super().__init__()\n        self.embedding = nn.Embedding(len(TEXT_sst.vocab), 50, padding_idx=1)\n        self.gru = nn.GRU(input_size=50, hidden_size=128, num_layers=2, bias=True, batch_first=True, bidirectional=False)\n        self.linear = nn.Linear(128, num_classes)\n        self.num_classes = num_classes\n\n    def forward(self, x):\n        embeds = self.embedding(x)\n        hidden = self.gru(embeds)[1][1]  # select h_n, and select the 2nd layer\n        logits = self.linear(hidden)\n        return logits\n\n\n\nmodel = ClfGRU(2).cuda()\nmodel.load_state_dict(torch.load(\'./snapshots/sst/baseline/model.dict\'))\nprint(\'\\nLoaded model.\\n\')\n\n\n\ndef get_scores(dataset_iterator, ood=False, snli=False):\n    model.eval()\n\n    outlier_scores = []\n\n    for batch_idx, batch in enumerate(iter(dataset_iterator)):\n        if ood and (batch_idx * args.batch_size > ood_num_examples):\n            break\n\n        if snli:\n            inputs = batch.hypothesis.t()\n        else:\n            inputs = batch.text.t()\n\n        logits = model(inputs)\n        smax = F.softmax(logits - torch.max(logits, dim=1, keepdim=True)[0], dim=1)\n        msp = -1 * torch.max(smax, dim=1)[0]\n\n        # ce_to_unif = F.log_softmax(logits - torch.max(logits, dim=1, keepdim=True)[0], dim=1).mean(1)  # negative cross entropy\n        # test = (F.softmax(logits - torch.max(logits, dim=1, keepdim=True)[0], dim=1) * (1 / torch.FloatTensor([logits.size(1)]).cuda().mean()).log()).sum(1)\n        # test = -1 * (F.log_softmax(logits - torch.max(logits, dim=1, keepdim=True)[0], dim=1) * smax).sum(1)\n\n        outlier_scores.extend(list(msp.data.cpu().numpy()))\n\n    return outlier_scores\n\n\n\n# ============================ OE ============================ #\n\ntest_scores = get_scores(test_iter_sst)\n\ntitles = [\'SNLI\', \'IMDB\', \'Multi30K\', \'WMT16\', \'English Web Treebank (Answers)\',\n          \'English Web Treebank (Email)\', \'English Web Treebank (Newsgroup)\',\n          \'English Web Treebank (Reviews)\', \'English Web Treebank (Weblog)\',\n          \'Yelp Reviews\']\n\niterators = [test_iter_snli, test_iter_imdb, train_iter_m30k, train_iter_wmt16, train_iter_answers,\n             train_iter_email, train_iter_newsgroup, train_iter_reviews, train_iter_weblog,\n             train_iter_yelp]\n\n\nmean_fprs = []\nmean_aurocs = []\nmean_auprs = []\n\nfor i in range(len(titles)):\n    title = titles[i]\n    iterator = iterators[i]\n\n    print(\'\\n{}\'.format(title))\n    fprs, aurocs, auprs = [], [], []\n    for i in range(10):\n        ood_scores = get_scores(iterator, ood=True, snli=True) if \'SNLI\' in title else get_scores(iterator, ood=True)\n        fpr, auroc, aupr = get_performance(ood_scores, test_scores, expected_ap, recall_level=recall_level)\n        fprs.append(fpr)\n        aurocs.append(auroc)\n        auprs.append(aupr)\n\n    print(\'FPR{:d}:\\t\\t\\t{:.4f} ({:.4f})\'.format(int(100 * recall_level), np.mean(fprs), np.std(fprs)))\n    print(\'AUROC:\\t\\t\\t{:.4f} ({:.4f})\'.format(np.mean(aurocs), np.std(aurocs)))\n    print(\'AUPR:\\t\\t\\t{:.4f} ({:.4f})\'.format(np.mean(auprs), np.std(auprs)))\n\n    mean_fprs.append(np.mean(fprs))\n    mean_aurocs.append(np.mean(aurocs))\n    mean_auprs.append(np.mean(auprs))\n\nprint()\nprint(\'OOD dataset mean FPR: {:.4f}\'.format(np.mean(mean_fprs)))\nprint(\'OOD dataset mean AUROC: {:.4f}\'.format(np.mean(mean_aurocs)))\nprint(\'OOD dataset mean AUPR: {:.4f}\'.format(np.mean(mean_auprs)))'"
NLP_classification/eval_OOD_trec.py,9,"b'# -*- coding: utf-8 -*-\n""""""\nTrains a MNIST classifier.\n""""""\n\nimport numpy as np\nimport sys\nimport os\nimport pickle\nimport argparse\nimport math\nimport time\nfrom bisect import bisect_left\nimport torch\nimport torch.nn as nn\nimport torch.backends.cudnn as cudnn\nimport torchvision.transforms as trn\nimport torchvision.datasets as dset\nimport torch.nn.functional as F\nfrom torch.autograd import Variable as V\nimport torchtext\n\nfrom torchtext import data\nfrom torchtext import datasets\n\nimport tqdm\n\n\n\nnp.random.seed(1)\n\nparser = argparse.ArgumentParser(description=\'TREC OE\',\n                                 formatter_class=argparse.ArgumentDefaultsHelpFormatter)\nparser.add_argument(\'--batch_size\', \'-b\', type=int, default=64, help=\'Batch size.\')\nargs = parser.parse_args()\n\n\ntorch.set_grad_enabled(False)\ncudnn.benchmark = True  # fire on all cylinders\n\n\n# go through rigamaroo to do ..utils.display_results import show_performance\nif __package__ is None:\n    import sys\n    from os import path\n\n    sys.path.append(path.dirname(path.dirname(path.abspath(__file__))))\n    from utils.display_results import get_performance\n\n\n# ============================ TREC ============================ #\n\n# set up fields\nTEXT_trec = data.Field(pad_first=True, lower=True)\nLABEL_trec = data.Field(sequential=False)\n\n# make splits for data\ntrain_trec, test_trec = datasets.TREC.splits(TEXT_trec, LABEL_trec, fine_grained=True)\n\n\n# build vocab\nTEXT_trec.build_vocab(train_trec, max_size=10000)\nLABEL_trec.build_vocab(train_trec, max_size=10000)\nprint(\'vocab length (including special tokens):\', len(TEXT_trec.vocab))\nprint(\'num labels:\', len(LABEL_trec.vocab))\n\n# make iterators\ntrain_iter_trec, test_iter_trec = data.BucketIterator.splits(\n    (train_trec, test_trec), batch_size=args.batch_size, repeat=False)\n\n# ============================ TREC ============================ #\n\nood_num_examples = len(test_iter_trec.dataset) // 5\nexpected_ap = ood_num_examples / (ood_num_examples + len(test_iter_trec.dataset))\nrecall_level = 0.9\n\n# ============================ IMDB ============================ #\n\n# set up fields\nTEXT_imdb = data.Field(pad_first=True, lower=True)\nLABEL_imdb = data.Field(sequential=False)\n\n# make splits for data\ntrain_imdb, test_imdb = datasets.IMDB.splits(TEXT_imdb, LABEL_imdb)\n\n# build vocab\nTEXT_imdb.build_vocab(train_trec.text, max_size=10000)\nLABEL_imdb.build_vocab(train_imdb, max_size=10000)\nprint(\'vocab length (including special tokens):\', len(TEXT_imdb.vocab))\n\n# make iterators\ntrain_iter_imdb, test_iter_imdb = data.BucketIterator.splits(\n    (train_imdb, test_imdb), batch_size=args.batch_size, repeat=False)\n\n# ============================ IMDB ============================ #\n\n# ============================ SNLI ============================ #\n\n# set up fields\nTEXT_snli = data.Field(pad_first=True, lower=True)\nLABEL_snli = data.Field(sequential=False)\n\n# make splits for data\ntrain_snli, val_snli, test_snli = datasets.SNLI.splits(TEXT_snli, LABEL_snli)\n\n# build vocab\nTEXT_snli.build_vocab(train_trec.text, max_size=10000)\nLABEL_snli.build_vocab(train_snli, max_size=10000)\nprint(\'vocab length (including special tokens):\', len(TEXT_snli.vocab))\n\n# make iterators\ntrain_iter_snli, val_iter_snli, test_iter_snli = data.BucketIterator.splits(\n    (train_snli, val_snli, test_snli), batch_size=args.batch_size, repeat=False)\n\n# ============================ SNLI ============================ #\n\n# ============================ Multi30K ============================ #\nTEXT_m30k = data.Field(pad_first=True, lower=True)\n\nm30k_data = data.TabularDataset(path=\'./.data/multi30k/train.txt\',\n                                  format=\'csv\',\n                                  fields=[(\'text\', TEXT_m30k)])\n\nTEXT_m30k.build_vocab(train_trec.text, max_size=10000)\nprint(\'vocab length (including special tokens):\', len(TEXT_m30k.vocab))\n\ntrain_iter_m30k = data.BucketIterator(m30k_data, batch_size=args.batch_size, repeat=False)\n# ============================ Multi30K ============================ #\n\n# ============================ WMT16 ============================ #\nTEXT_wmt16 = data.Field(pad_first=True, lower=True)\n\nwmt16_data = data.TabularDataset(path=\'./.data/wmt16/wmt16_sentences\',\n                                  format=\'csv\',\n                                  fields=[(\'text\', TEXT_wmt16)])\n\nTEXT_wmt16.build_vocab(train_trec.text, max_size=10000)\nprint(\'vocab length (including special tokens):\', len(TEXT_wmt16.vocab))\n\ntrain_iter_wmt16 = data.BucketIterator(wmt16_data, batch_size=args.batch_size, repeat=False)\n# ============================ WMT16 ============================ #\n\n# ============================ English Web Treebank (Answers) ============================ #\n\nTEXT_answers = data.Field(pad_first=True, lower=True)\n\ntreebank_path = \'./.data/eng_web_tbk/answers/conll/answers_penntrees.dev.conll\'\n\ntrain_answers = datasets.SequenceTaggingDataset(path=treebank_path, fields=((None, None), (\'text\', TEXT_answers)))\n\nTEXT_answers.build_vocab(train_trec.text, max_size=10000)\nprint(\'vocab length (including special tokens):\', len(TEXT_answers.vocab))\n\n# make iterators\ntrain_iter_answers = data.BucketIterator.splits(\n    (train_answers,), batch_size=args.batch_size, repeat=False)[0]\n\n# ============================ English Web Treebank (Answers) ============================ #\n\n# ============================ English Web Treebank (Email) ============================ #\n\nTEXT_email = data.Field(pad_first=True, lower=True)\n\ntreebank_path = \'./.data/eng_web_tbk/email/conll/email_penntrees.dev.conll\'\n\ntrain_email = datasets.SequenceTaggingDataset(path=treebank_path, fields=((None, None), (\'text\', TEXT_email)))\n\nTEXT_email.build_vocab(train_trec.text, max_size=10000)\nprint(\'vocab length (including special tokens):\', len(TEXT_email.vocab))\n\n# make iterators\ntrain_iter_email = data.BucketIterator.splits(\n    (train_email,), batch_size=args.batch_size, repeat=False)[0]\n\n# ============================ English Web Treebank (Email) ============================ #\n\n# ============================ English Web Treebank (Newsgroup) ============================ #\n\nTEXT_newsgroup = data.Field(pad_first=True, lower=True)\n\ntreebank_path = \'./.data/eng_web_tbk/newsgroup/conll/newsgroup_penntrees.dev.conll\'\n\ntrain_newsgroup = datasets.SequenceTaggingDataset(path=treebank_path, fields=((None, None), (\'text\', TEXT_newsgroup)))\n\nTEXT_newsgroup.build_vocab(train_trec.text, max_size=10000)\nprint(\'vocab length (including special tokens):\', len(TEXT_newsgroup.vocab))\n\n# make iterators\ntrain_iter_newsgroup = data.BucketIterator.splits(\n    (train_newsgroup,), batch_size=args.batch_size, repeat=False)[0]\n\n# ============================ English Web Treebank (Newsgroup) ============================ #\n\n# ============================ English Web Treebank (Reviews) ============================ #\n\nTEXT_reviews = data.Field(pad_first=True, lower=True)\n\ntreebank_path = \'./.data/eng_web_tbk/reviews/conll/reviews_penntrees.dev.conll\'\n\ntrain_reviews = datasets.SequenceTaggingDataset(path=treebank_path, fields=((None, None), (\'text\', TEXT_reviews)))\n\nTEXT_reviews.build_vocab(train_trec.text, max_size=10000)\nprint(\'vocab length (including special tokens):\', len(TEXT_reviews.vocab))\n\n# make iterators\ntrain_iter_reviews = data.BucketIterator.splits(\n    (train_reviews,), batch_size=args.batch_size, repeat=False)[0]\n\n# ============================ English Web Treebank (Reviews) ============================ #\n\n# ============================ English Web Treebank (Weblog) ============================ #\n\nTEXT_weblog = data.Field(pad_first=True, lower=True)\n\ntreebank_path = \'./.data/eng_web_tbk/weblog/conll/weblog_penntrees.dev.conll\'\n\ntrain_weblog = datasets.SequenceTaggingDataset(path=treebank_path, fields=((None, None), (\'text\', TEXT_weblog)))\n\nTEXT_weblog.build_vocab(train_trec.text, max_size=10000)\nprint(\'vocab length (including special tokens):\', len(TEXT_weblog.vocab))\n\n# make iterators\ntrain_iter_weblog = data.BucketIterator.splits(\n    (train_weblog,), batch_size=args.batch_size, repeat=False)[0]\n\n# ============================ English Web Treebank (Weblog) ============================ #\n\n# ============================ Yelp Reviews ============================ #\nTEXT_yelp = data.Field(pad_first=True, lower=True)\n\nyelp_data = data.TabularDataset(path=\'./.data/yelp_review_full_csv/test.csv\',\n                                  format=\'csv\',\n                                  fields=[(None, None), (\'text\', TEXT_yelp)])\n\nTEXT_yelp.build_vocab(train_trec.text, max_size=10000)\nprint(\'vocab length (including special tokens):\', len(TEXT_yelp.vocab))\n\ntrain_iter_yelp = data.BucketIterator(yelp_data, batch_size=args.batch_size, repeat=False)\n# ============================ Yelp Reviews ============================ #\n\n\n\n\nclass ClfGRU(nn.Module):\n    def __init__(self, num_classes):\n        super().__init__()\n        self.embedding = nn.Embedding(len(TEXT_trec.vocab), 50, padding_idx=1)\n        self.gru = nn.GRU(input_size=50, hidden_size=128, num_layers=2, bias=True, batch_first=True, bidirectional=False)\n        self.linear = nn.Linear(128, num_classes)\n        self.num_classes = num_classes\n\n    def forward(self, x):\n        embeds = self.embedding(x)\n        hidden = self.gru(embeds)[1][1]  # select h_n, and select the 2nd layer\n        logits = self.linear(hidden)\n        return logits\n\n\n\nmodel = ClfGRU(50).cuda()\nmodel.load_state_dict(torch.load(\'./snapshots/trec/OE/wikitext2/model_finetune.dict\'))\nprint(\'\\nLoaded model.\\n\')\n\n\n\ndef get_scores(dataset_iterator, ood=False, snli=False):\n    model.eval()\n\n    outlier_scores = []\n\n    for batch_idx, batch in enumerate(iter(dataset_iterator)):\n        if ood and (batch_idx * args.batch_size > ood_num_examples):\n            break\n\n        if snli:\n            inputs = batch.hypothesis.t()\n        else:\n            inputs = batch.text.t()\n\n        logits = model(inputs)\n        smax = F.softmax(logits - torch.max(logits, dim=1, keepdim=True)[0], dim=1)\n        msp = -1 * torch.max(smax, dim=1)[0]\n\n        # ce_to_unif = F.log_softmax(logits - torch.max(logits, dim=1, keepdim=True)[0], dim=1).mean(1)\n\n        outlier_scores.extend(list(msp.data.cpu().numpy()))\n\n    return outlier_scores\n\n\n\n# ============================ OE ============================ #\n\ntest_scores = get_scores(test_iter_trec)\n\ntitles = [\'SNLI\', \'IMDB\', \'Multi30K\', \'WMT16\', \'English Web Treebank (Answers)\',\n          \'English Web Treebank (Email)\', \'English Web Treebank (Newsgroup)\',\n          \'English Web Treebank (Reviews)\', \'English Web Treebank (Weblog)\',\n          \'Yelp Reviews\']\n\niterators = [test_iter_snli, test_iter_imdb, train_iter_m30k, train_iter_wmt16, train_iter_answers,\n             train_iter_email, train_iter_newsgroup, train_iter_reviews, train_iter_weblog,\n             train_iter_yelp]\n\n\nmean_fprs = []\nmean_aurocs = []\nmean_auprs = []\n\nfor i in range(len(titles)):\n    title = titles[i]\n    iterator = iterators[i]\n\n    print(\'\\n{}\'.format(title))\n    fprs, aurocs, auprs = [], [], []\n    for i in range(10):\n        ood_scores = get_scores(iterator, ood=True, snli=True) if \'SNLI\' in title else get_scores(iterator, ood=True)\n        fpr, auroc, aupr = get_performance(ood_scores, test_scores, expected_ap, recall_level=recall_level)\n        fprs.append(fpr)\n        aurocs.append(auroc)\n        auprs.append(aupr)\n\n    print(\'FPR{:d}:\\t\\t\\t{:.4f} ({:.4f})\'.format(int(100 * recall_level), np.mean(fprs), np.std(fprs)))\n    print(\'AUROC:\\t\\t\\t{:.4f} ({:.4f})\'.format(np.mean(aurocs), np.std(aurocs)))\n    print(\'AUPR:\\t\\t\\t{:.4f} ({:.4f})\'.format(np.mean(auprs), np.std(auprs)))\n\n    mean_fprs.append(np.mean(fprs))\n    mean_aurocs.append(np.mean(aurocs))\n    mean_auprs.append(np.mean(auprs))\n\nprint()\nprint(\'OOD dataset mean FPR: {:.4f}\'.format(np.mean(mean_fprs)))\nprint(\'OOD dataset mean AUROC: {:.4f}\'.format(np.mean(mean_aurocs)))\nprint(\'OOD dataset mean AUPR: {:.4f}\'.format(np.mean(mean_auprs)))'"
NLP_classification/train.py,7,"b'# -*- coding: utf-8 -*-\n""""""\nTrains a MNIST classifier.\n""""""\n\nimport numpy as np\nimport sys\nimport os\nimport pickle\nimport argparse\nimport math\nimport time\nfrom bisect import bisect_left\nimport torch\nimport torch.nn as nn\nimport torch.backends.cudnn as cudnn\nimport torchvision.transforms as trn\nimport torchvision.datasets as dset\nimport torch.nn.functional as F\nfrom torch.autograd import Variable as V\nimport torchtext\n\nfrom torchtext import data\nfrom torchtext import datasets\n\nimport tqdm\n\n\n\nnp.random.seed(1)\n\nparser = argparse.ArgumentParser(description=\'Train without OE\',\n                                 formatter_class=argparse.ArgumentDefaultsHelpFormatter)\nparser.add_argument(\'--in_dist_dataset\', type=str, choices=[\'sst\', \'20ng\', \'trec\'], default=\'sst\')\n# Optimization options\nparser.add_argument(\'--epochs\', \'-e\', type=int, default=5, help=\'Number of epochs to train.\')\nparser.add_argument(\'--batch_size\', \'-b\', type=int, default=64, help=\'Batch size.\')\nparser.add_argument(\'--learning_rate\', \'-lr\', type=float, default=0.01, help=\'The initial learning rate.\')\nparser.add_argument(\'--momentum\', \'-m\', type=float, default=0.5, help=\'Momentum.\')\nparser.add_argument(\'--test_bs\', type=int, default=256)\n# Checkpoints\nparser.add_argument(\'--save\', \'-s\', type=str, default=\'./snapshots\', help=\'Folder to save checkpoints.\')\nparser.add_argument(\'--load\', \'-l\', type=str, default=\'./snapshots\', help=\'Checkpoint path to resume / test.\')\nparser.add_argument(\'--test\', \'-t\', action=\'store_true\', help=\'Test only flag.\')\nparser.add_argument(\'--mix\', dest=\'mix\', action=\'store_true\', help=\'Mix outliers images with in-dist images.\')\n# Acceleration\nparser.add_argument(\'--prefetch\', type=int, default=2, help=\'Pre-fetching threads.\')\nargs = parser.parse_args()\n\n\nif args.in_dist_dataset == \'sst\':\n    # set up fields\n    TEXT = data.Field(pad_first=True)\n    LABEL = data.Field(sequential=False)\n\n    # make splits for data\n    train, val, test = datasets.SST.splits(\n        TEXT, LABEL, fine_grained=False, train_subtrees=False,\n        filter_pred=lambda ex: ex.label != \'neutral\')\n\n    # build vocab\n    TEXT.build_vocab(train, max_size=10000)\n    LABEL.build_vocab(train, max_size=10000)\n    print(\'vocab length (including special tokens):\', len(TEXT.vocab))\n\n    # create our own iterator, avoiding the calls to build_vocab in SST.iters\n    train_iter, val_iter, test_iter = data.BucketIterator.splits(\n        (train, val, test), batch_size=args.batch_size, repeat=False)\nelif args.in_dist_dataset == \'20ng\':\n    TEXT = data.Field(pad_first=True, lower=True, fix_length=100)\n    LABEL = data.Field(sequential=False)\n\n    train = data.TabularDataset(path=\'./.data/20newsgroups/20ng-train.txt\',\n                                     format=\'csv\',\n                                     fields=[(\'label\', LABEL), (\'text\', TEXT)])\n\n    test = data.TabularDataset(path=\'./.data/20newsgroups/20ng-test.txt\',\n                                     format=\'csv\',\n                                     fields=[(\'label\', LABEL), (\'text\', TEXT)])\n\n    TEXT.build_vocab(train, max_size=10000)\n    LABEL.build_vocab(train, max_size=10000)\n    print(\'vocab length (including special tokens):\', len(TEXT.vocab))\n\n    train_iter = data.BucketIterator(train, batch_size=args.batch_size, repeat=False)\n    test_iter = data.BucketIterator(test, batch_size=args.batch_size, repeat=False)\nelif args.in_dist_dataset == \'trec\':\n    # set up fields\n    TEXT = data.Field(pad_first=True, lower=True)\n    LABEL = data.Field(sequential=False)\n\n    # make splits for data\n    train, test = datasets.TREC.splits(TEXT, LABEL, fine_grained=True)\n\n\n    # build vocab\n    TEXT.build_vocab(train, max_size=10000)\n    LABEL.build_vocab(train, max_size=10000)\n    print(\'vocab length (including special tokens):\', len(TEXT.vocab))\n    print(\'num labels:\', len(LABEL.vocab))\n\n    # make iterators\n    train_iter, test_iter = data.BucketIterator.splits(\n        (train, test), batch_size=args.batch_size, repeat=False)\n\n\n\n\n\ncudnn.benchmark = True  # fire on all cylinders\n\n\nclass ClfGRU(nn.Module):\n    def __init__(self, num_classes):\n        super().__init__()\n        self.embedding = nn.Embedding(len(TEXT.vocab), 50, padding_idx=1)\n        self.gru = nn.GRU(input_size=50, hidden_size=128, num_layers=2,\n            bias=True, batch_first=True, bidirectional=False)\n        self.linear = nn.Linear(128, num_classes)\n\n    def forward(self, x):\n        embeds = self.embedding(x)\n        hidden = self.gru(embeds)[1][1]  # select h_n, and select the 2nd layer\n        logits = self.linear(hidden)\n        return logits\n\n\nmodel = ClfGRU(2).cuda()\n\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)\nscheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=args.epochs)\n\n\ndef train():\n    model.train()\n    loss_ema = 0\n\n    for batch_idx, batch in enumerate(iter(train_iter)):\n        inputs = batch.text.t()\n        labels = batch.label - 1\n\n        logits = model(inputs)\n\n        loss = F.cross_entropy(logits, labels)\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        loss_ema = loss_ema * 0.9 + loss.data.cpu().numpy() * 0.1\n\n        if batch_idx % 200 == 0:\n            print(\'iter: {} | loss_ema: {}\'.format(batch_idx, loss_ema))\n\n    scheduler.step()\n\n\ndef evaluate():\n    model.eval()\n    running_loss = 0\n    num_examples = 0\n    correct = 0\n\n    for batch_idx, batch in enumerate(iter(test_iter)):\n        inputs = batch.text.t()\n        labels = batch.label - 1\n\n        logits = model(inputs)\n\n        loss = F.cross_entropy(logits, labels, size_average=False)\n        running_loss += loss.data.cpu().numpy()\n\n        pred = logits.max(1)[1]\n        correct += pred.eq(labels).sum().data.cpu().numpy()\n\n        num_examples += inputs.shape[0]\n\n    acc = correct / num_examples\n    loss = running_loss / num_examples\n\n    return acc, loss\n\n\nacc, loss = evaluate()\nprint(\'test acc: {} \\t| test loss: {}\\n\'.format(acc, loss))\nfor epoch in range(args.epochs):\n    print(\'Epoch\', epoch)\n    train()\n    acc, loss = evaluate()\n    print(\'test acc: {} \\t| test loss: {}\\n\'.format(acc, loss))\n\n\ntorch.save(model.state_dict(), \'./snapshots/{}/baseline/model.dict\'.format(args.in_dist_dataset))\nprint(\'Saved model.\')'"
NLP_classification/train_OE.py,9,"b'# -*- coding: utf-8 -*-\n""""""\nTrains a MNIST classifier.\n""""""\n\nimport numpy as np\nimport sys\nimport os\nimport pickle\nimport argparse\nimport math\nimport time\nfrom bisect import bisect_left\nimport torch\nimport torch.nn as nn\nimport torch.backends.cudnn as cudnn\nimport torchvision.transforms as trn\nimport torchvision.datasets as dset\nimport torch.nn.functional as F\nfrom torch.autograd import Variable as V\nimport torchtext\n\nfrom torchtext import data\nfrom torchtext import datasets\n\nimport tqdm\n\n\n\nnp.random.seed(1)\n\nparser = argparse.ArgumentParser(description=\'Train with OE\',\n                                 formatter_class=argparse.ArgumentDefaultsHelpFormatter)\nparser.add_argument(\'--in_dist_dataset\', type=str, choices=[\'sst\', \'20ng\', \'trec\'], default=\'sst\')\nparser.add_argument(\'--oe_dataset\', type=str, choices=[\'wikitext2\', \'wikitext103\', \'gutenberg\'], default=\'wikitext2\')\n# Optimization options\nparser.add_argument(\'--epochs\', \'-e\', type=int, default=5, help=\'Number of epochs to train.\')\nparser.add_argument(\'--batch_size\', \'-b\', type=int, default=64, help=\'Batch size.\')\nparser.add_argument(\'--learning_rate\', \'-lr\', type=float, default=0.01, help=\'The initial learning rate.\')\nparser.add_argument(\'--momentum\', \'-m\', type=float, default=0.5, help=\'Momentum.\')\nparser.add_argument(\'--test_bs\', type=int, default=256)\n# Checkpoints\nparser.add_argument(\'--save\', \'-s\', type=str, default=\'./snapshots\', help=\'Folder to save checkpoints.\')\nparser.add_argument(\'--load\', \'-l\', type=str, default=\'./snapshots\', help=\'Checkpoint path to resume / test.\')\nparser.add_argument(\'--test\', \'-t\', action=\'store_true\', help=\'Test only flag.\')\nparser.add_argument(\'--mix\', dest=\'mix\', action=\'store_true\', help=\'Mix outliers images with in-dist images.\')\n# Acceleration\nparser.add_argument(\'--prefetch\', type=int, default=2, help=\'Pre-fetching threads.\')\nargs = parser.parse_args()\n\n\nif args.in_dist_dataset == \'sst\':\n    # set up fields\n    TEXT = data.Field(pad_first=True)\n    LABEL = data.Field(sequential=False)\n\n    # make splits for data\n    train, val, test = datasets.SST.splits(\n        TEXT, LABEL, fine_grained=False, train_subtrees=False,\n        filter_pred=lambda ex: ex.label != \'neutral\')\n\n    # build vocab\n    TEXT.build_vocab(train, max_size=10000)\n    LABEL.build_vocab(train, max_size=10000)\n    print(\'vocab length (including special tokens):\', len(TEXT.vocab))\n\n    # create our own iterator, avoiding the calls to build_vocab in SST.iters\n    train_iter, val_iter, test_iter = data.BucketIterator.splits(\n        (train, val, test), batch_size=args.batch_size, repeat=False)\nelif args.in_dist_dataset == \'20ng\':\n    TEXT = data.Field(pad_first=True, lower=True, fix_length=100)\n    LABEL = data.Field(sequential=False)\n\n    train = data.TabularDataset(path=\'./.data/20newsgroups/20ng-train.txt\',\n                                     format=\'csv\',\n                                     fields=[(\'label\', LABEL), (\'text\', TEXT)])\n\n    test = data.TabularDataset(path=\'./.data/20newsgroups/20ng-test.txt\',\n                                     format=\'csv\',\n                                     fields=[(\'label\', LABEL), (\'text\', TEXT)])\n\n    TEXT.build_vocab(train, max_size=10000)\n    LABEL.build_vocab(train, max_size=10000)\n    print(\'vocab length (including special tokens):\', len(TEXT.vocab))\n\n    train_iter = data.BucketIterator(train, batch_size=args.batch_size, repeat=False)\n    test_iter = data.BucketIterator(test, batch_size=args.batch_size, repeat=False)\nelif args.in_dist_dataset == \'trec\':\n    # set up fields\n    TEXT = data.Field(pad_first=True, lower=True)\n    LABEL = data.Field(sequential=False)\n\n    # make splits for data\n    train, test = datasets.TREC.splits(TEXT, LABEL, fine_grained=True)\n\n\n    # build vocab\n    TEXT.build_vocab(train, max_size=10000)\n    LABEL.build_vocab(train, max_size=10000)\n    print(\'vocab length (including special tokens):\', len(TEXT.vocab))\n    print(\'num labels:\', len(LABEL.vocab))\n\n    # make iterators\n    train_iter, test_iter = data.BucketIterator.splits(\n        (train, test), batch_size=args.batch_size, repeat=False)\n\n\nif args.oe_dataset == \'wikitext2\':\n    TEXT_custom = data.Field(pad_first=True, lower=True)\n    \n    custom_data = data.TabularDataset(path=\'./.data/wikitext_reformatted/wikitext2_sentences\',\n                                      format=\'csv\',\n                                      fields=[(\'text\', TEXT_custom)])\n\n    TEXT_custom.build_vocab(train.text, max_size=10000)\n    print(\'vocab length (including special tokens):\', len(TEXT_custom.vocab))\n\n    train_iter_oe = data.BucketIterator(custom_data, batch_size=args.batch_size, repeat=False)\nelif args.oe_dataset == \'wikitext103\':\n    TEXT_custom = data.Field(pad_first=True, lower=True)\n\n    custom_data = data.TabularDataset(path=\'./.data/wikitext_reformatted/wikitext103_sentences\',\n                                      format=\'csv\',\n                                      fields=[(\'text\', TEXT_custom)])\n\n    TEXT_custom.build_vocab(train.text, max_size=10000)\n    print(\'vocab length (including special tokens):\', len(TEXT_custom.vocab))\n\n    train_iter_oe = data.BucketIterator(custom_data, batch_size=args.batch_size, repeat=False)\nelif args.oe_dataset == \'gutenberg\':\n    TEXT_custom = data.Field(pad_first=True, lower=True)\n\n    custom_data = data.TabularDataset(path=\'./.data/gutenberg/gutenberg_sentences\',\n                                      format=\'csv\',\n                                      fields=[(\'text\', TEXT_custom)])\n\n    TEXT_custom.build_vocab(train.text, max_size=10000)\n    print(\'vocab length (including special tokens):\', len(TEXT_custom.vocab))\n\n    train_iter_oe = data.BucketIterator(custom_data, batch_size=args.batch_size, repeat=False)\n\n\ncudnn.benchmark = True  # fire on all cylinders\n\n\nclass ClfGRU(nn.Module):\n    def __init__(self, num_classes):\n        super().__init__()\n        self.embedding = nn.Embedding(len(TEXT.vocab), 50, padding_idx=1)\n        self.gru = nn.GRU(input_size=50, hidden_size=128, num_layers=2,\n            bias=True, batch_first=True,bidirectional=False)\n        self.linear = nn.Linear(128, num_classes)\n\n    def forward(self, x):\n        embeds = self.embedding(x)\n        hidden = self.gru(embeds)[1][1]  # select h_n, and select the 2nd layer\n        logits = self.linear(hidden)\n        return logits\n\n\nmodel = ClfGRU(2).cuda()  # change to match dataset\n\nmodel.load_state_dict(torch.load(\'./snapshots/sst/baseline/model.dict\'))\n\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)\nscheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=args.epochs)\n\n\ndef train():\n    model.train()\n    data_loss_ema = 0\n    oe_loss_ema = 0\n\n    for batch_idx, (batch, batch_oe) in enumerate(zip(iter(train_iter), iter(train_iter_oe))):\n        inputs = batch.text.t()\n        labels = batch.label - 1\n        logits = model(inputs)\n        data_loss = F.cross_entropy(logits, labels)\n\n        inputs_oe = batch_oe.text.t()\n        logits_oe = model(inputs_oe)\n        smax_oe = F.log_softmax(logits_oe - torch.max(logits_oe, dim=1, keepdim=True)[0], dim=1)\n        oe_loss = -1 * smax_oe.mean()  # minimizing cross entropy\n\n        loss = data_loss + oe_loss\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        data_loss_ema = data_loss_ema * 0.9 + data_loss.data.cpu().numpy() * 0.1\n        oe_loss_ema = oe_loss_ema * 0.9 + oe_loss.data.cpu().numpy() * 0.1\n\n        if (batch_idx % 200 == 0 or batch_idx < 10):\n            print(\'iter: {} \\t| data_loss_ema: {} \\t| oe_loss_ema: {}\'.format(\n                batch_idx, data_loss_ema, oe_loss_ema))\n\n    scheduler.step()\n\n\ndef evaluate():\n    model.eval()\n    running_loss = 0\n    num_examples = 0\n    correct = 0\n\n    for batch_idx, batch in enumerate(iter(test_iter)):\n        inputs = batch.text.t()\n        labels = batch.label - 1\n\n        logits = model(inputs)\n\n        loss = F.cross_entropy(logits, labels, size_average=False)\n        running_loss += loss.data.cpu().numpy()\n\n        pred = logits.max(1)[1]\n        correct += pred.eq(labels).sum().data.cpu().numpy()\n\n        num_examples += inputs.shape[0]\n\n    acc = correct / num_examples\n    loss = running_loss / num_examples\n\n    return acc, loss\n\n\nacc, loss = evaluate()\nprint(\'test acc: {} \\t| test loss: {}\\n\'.format(acc, loss))\nfor epoch in range(args.epochs):\n    print(\'Epoch\', epoch)\n    train()\n    acc, loss = evaluate()\n    print(\'test acc: {} \\t| test loss: {}\\n\'.format(acc, loss))\n\n\ntorch.save(model.state_dict(), \'./snapshots/{}/OE/{}/model_finetune.dict\'.format(\n    args.in_dist_dataset, args.oe_dataset))\nprint(\'Saved model.\')\n'"
NLP_language_modeling/data.py,3,"b'import os\nimport torch\n\nfrom collections import Counter\n\n\nclass Dictionary(object):\n    def __init__(self):\n        self.word2idx = {}\n        self.idx2word = []\n        self.counter = Counter()\n        self.total = 0\n\n    def add_word(self, word):\n        if word not in self.word2idx:\n            self.idx2word.append(word)\n            self.word2idx[word] = len(self.idx2word) - 1\n        token_id = self.word2idx[word]\n        self.counter[token_id] += 1\n        self.total += 1\n        return self.word2idx[word]\n\n    def __len__(self):\n        return len(self.idx2word)\n\n\nclass Corpus(object):\n    def __init__(self, path, dictionary=None):\n        """"""\n        :param path: path to train, val, or test data\n        :param dictionary: If None, create new dictionary. Else, use the given dictionary.\n        """"""\n        self.dictionary = Dictionary() if dictionary is None else dictionary\n        self.new_dict = True if dictionary is None else False\n        self.train = self.tokenize(os.path.join(path, \'train.txt\'))\n        self.valid = self.tokenize(os.path.join(path, \'valid.txt\'))\n        self.test = self.tokenize(os.path.join(path, \'test.txt\'))\n\n    def tokenize(self, path):\n        """"""Tokenizes a text file.""""""\n        assert os.path.exists(path)\n        # Add words to the dictionary\n        with open(path, \'r\') as f:\n            tokens = 0\n            for line in f:\n                words = line.split() + [\'<eos>\']\n                tokens += len(words)\n                if self.new_dict:  # if building a new dictionary, add all the new words you come across\n                    for word in words:\n                        self.dictionary.add_word(word)\n\n        # Tokenize file content\n        with open(path, \'r\') as f:\n            ids = torch.LongTensor(tokens)\n            token = 0\n            for line in f:\n                words = line.split() + [\'<eos>\']\n                for word in words:\n                    ids[token] = self.dictionary.word2idx.get(word, self.dictionary.word2idx[\'<unk>\'])\n                    token += 1\n\n        return ids\n\n\nclass CorpusWikiTextChar(object):\n    def __init__(self, path, dictionary):\n        """"""\n        :param path: path to train, val, or test data\n        :param dictionary: If None, create new dictionary. Else, use the given dictionary.\n        """"""\n        self.dictionary = dictionary\n        self.train = self.tokenize(os.path.join(path, \'train.txt\'))\n        self.valid = self.tokenize(os.path.join(path, \'valid.txt\'))\n        self.test = self.tokenize(os.path.join(path, \'test.txt\'))\n\n    def tokenize(self, path):\n        """"""Tokenizes a text file.""""""\n        assert os.path.exists(path)\n        # Add words to the dictionary\n        corpus = []\n        ids = []\n        with open(path, \'r\') as f:\n            for line in f:\n                if len(line) == 1:  # end of example\n                    continue\n\n                words = line.split()\n                for i in range(len(words)):\n                    word = words[i]\n                    word = word.lower()\n                    for char in [char for char in word]:\n                        if char in [0,1,2,3,4,5,6,7,8,9]:\n                            char = \'N\'\n                        if char not in self.dictionary.word2idx.keys():\n                            continue # don\'t append it to the corpus\n                        corpus.append(char)\n                        ids.append(self.dictionary.word2idx[char])\n\n                    if i < len(words) - 1:\n                        corpus.append(\'_\')\n                        ids.append(self.dictionary.word2idx[\'_\'])\n                corpus.append(\'<eos>\')\n                ids.append(self.dictionary.word2idx[\'<eos>\'])\n\n        return torch.LongTensor(ids)\n\n\nclass OODCorpus(object):\n    def __init__(self, path, dictionary, char=False):\n        """"""\n        :param path: path to train, val, or test data\n        :param dictionary: existing dictionary of words constructed with Corpus class on in-dist\n        :param char: if True, return character-level data\n        """"""\n        self.dictionary = dictionary\n        self.data_words, self.data = self.tokenize(path, char)\n\n    def tokenize(self, path, char=False):\n        """"""Tokenizes a text file.""""""\n        assert os.path.exists(path)\n        # Add words to the dictionary\n        corpus = []\n        ids = []\n        with open(path, \'r\') as f:\n            for line in f:\n                if len(line) == 1:  # end of example\n                    if char:\n                        corpus.append(\'<eos>\')\n                        ids.append(self.dictionary.word2idx[\'<eos>\'])\n                    else:\n                        corpus.append(\'<eos>\')\n                        ids.append(self.dictionary.word2idx[\'<eos>\'])\n                    continue\n                word = line.split(\'\\t\')[1]\n                if char:\n                    if word not in self.dictionary.word2idx.keys():\n                        word = \'<unk>\'\n                    corpus.extend(list(word))\n                    corpus.append(\'_\')\n                    ids.extend([self.dictionary.word2idx[char] for char in word])\n                    ids.append(self.dictionary.word2idx[\'_\'])\n                else:\n                    corpus.append(word)\n                    ids.append(self.dictionary.word2idx.get(word, self.dictionary.word2idx[\'<unk>\']))\n\n        return corpus, torch.LongTensor(ids)\n'"
NLP_language_modeling/embed_regularize.py,3,"b""import numpy as np\n\nimport torch\nfrom torch.autograd import Variable\n\ndef embedded_dropout(embed, words, dropout=0.1, scale=None):\n  if dropout:\n    mask = embed.weight.data.new().resize_((embed.weight.size(0), 1)).bernoulli_(1 - dropout).expand_as(embed.weight) / (1 - dropout)\n    mask = Variable(mask)\n    masked_embed_weight = mask * embed.weight\n  else:\n    masked_embed_weight = embed.weight\n  if scale:\n    masked_embed_weight = scale.expand_as(masked_embed_weight) * masked_embed_weight\n\n  padding_idx = embed.padding_idx\n  if padding_idx is None:\n      padding_idx = -1\n  X = embed._backend.Embedding.apply(words, masked_embed_weight,\n    padding_idx, embed.max_norm, embed.norm_type,\n    embed.scale_grad_by_freq, embed.sparse\n  )\n  return X\n\nif __name__ == '__main__':\n  V = 50\n  h = 4\n  bptt = 10\n  batch_size = 2\n\n  embed = torch.nn.Embedding(V, h)\n\n  words = np.random.random_integers(low=0, high=V-1, size=(batch_size, bptt))\n  words = torch.LongTensor(words)\n  words = Variable(words)\n\n  origX = embed(words)\n  X = embedded_dropout(embed, words)\n\n  print(origX)\n  print(X)\n"""
NLP_language_modeling/eval_ood.py,15,"b'import argparse\nimport time\nimport math\nimport numpy as np\nimport sklearn.metrics as sk\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\nimport torch.nn.functional as F\n\nimport data\nimport model\n\nfrom utils_lm import batchify, get_batch, repackage_hidden\n\n# go through rigamaroo to do ..utils.display_results import show_performance\nif __package__ is None:\n    import sys\n    from os import path\n\n    sys.path.append(path.dirname(path.dirname(path.abspath(__file__))))\n    from utils.display_results import show_performance\n    from utils.log_sum_exp import log_sum_exp\n\nparser = argparse.ArgumentParser(description=\'PyTorch PennTreeBank RNN/LSTM Language Model\')\nparser.add_argument(\'--data\', type=str, default=\'data/penn/\',\n                    help=\'location of the data corpus\')\nparser.add_argument(\'--model\', type=str, default=\'LSTM\',\n                    help=\'type of recurrent net (LSTM, QRNN, GRU)\')\nparser.add_argument(\'--emsize\', type=int, default=400,\n                    help=\'size of word embeddings\')\nparser.add_argument(\'--nhid\', type=int, default=1150,\n                    help=\'number of hidden units per layer\')\nparser.add_argument(\'--nlayers\', type=int, default=3,\n                    help=\'number of layers\')\nparser.add_argument(\'--lr\', type=float, default=30,\n                    help=\'initial learning rate\')\nparser.add_argument(\'--clip\', type=float, default=0.25,\n                    help=\'gradient clipping\')\nparser.add_argument(\'--epochs\', type=int, default=8000,\n                    help=\'upper epoch limit\')\nparser.add_argument(\'--batch_size\', type=int, default=80, metavar=\'N\',\n                    help=\'batch size\')\nparser.add_argument(\'--bptt\', type=int, default=70,\n                    help=\'sequence length\')\nparser.add_argument(\'--dropout\', type=float, default=0.4,\n                    help=\'dropout applied to layers (0 = no dropout)\')\nparser.add_argument(\'--dropouth\', type=float, default=0.3,\n                    help=\'dropout for rnn layers (0 = no dropout)\')\nparser.add_argument(\'--dropouti\', type=float, default=0.65,\n                    help=\'dropout for input embedding layers (0 = no dropout)\')\nparser.add_argument(\'--dropoute\', type=float, default=0.1,\n                    help=\'dropout to remove words from embedding layer (0 = no dropout)\')\nparser.add_argument(\'--wdrop\', type=float, default=0.5,\n                    help=\'amount of weight dropout to apply to the RNN hidden to hidden matrix\')\nparser.add_argument(\'--seed\', type=int, default=1111,\n                    help=\'random seed\')\nparser.add_argument(\'--nonmono\', type=int, default=5,\n                    help=\'random seed\')\nparser.add_argument(\'--cuda\', action=\'store_false\',\n                    help=\'use CUDA\')\nparser.add_argument(\'--log-interval\', type=int, default=200, metavar=\'N\',\n                    help=\'report interval\')\nrandomhash = \'\'.join(str(time.time()).split(\'.\'))\nparser.add_argument(\'--save\', type=str,  default=randomhash+\'.pt\',\n                    help=\'path to save the final model\')\nparser.add_argument(\'--alpha\', type=float, default=2,\n                    help=\'alpha L2 regularization on RNN activation (alpha = 0 means no regularization)\')\nparser.add_argument(\'--beta\', type=float, default=1,\n                    help=\'beta slowness regularization applied on RNN activiation (beta = 0 means no regularization)\')\nparser.add_argument(\'--wdecay\', type=float, default=1.2e-6,\n                    help=\'weight decay applied to all weights\')\nparser.add_argument(\'--resume\', type=str,  default=\'\',\n                    help=\'path of model to resume\')\nparser.add_argument(\'--optimizer\', type=str,  default=\'sgd\',\n                    help=\'optimizer to use (sgd, adam)\')\nparser.add_argument(\'--when\', nargs=""+"", type=int, default=[-1],\n                    help=\'When (which epochs) to divide the learning rate by 10 - accepts multiple\')\nparser.add_argument(\'--character_level\', action=\'store_true\', help=""Use this flag to evaluate character-level models."")\nargs = parser.parse_args()\nargs.tied = True\n\n# Set the random seed manually for reproducibility.\nnp.random.seed(args.seed)\ntorch.manual_seed(args.seed)\nif torch.cuda.is_available():\n    if not args.cuda:\n        print(""WARNING: You have a CUDA device, so you should probably run with --cuda"")\n    else:\n        torch.cuda.manual_seed(args.seed)\n\n###############################################################################\n# Load data\n###############################################################################\n\ndef model_save(fn):\n    with open(fn, \'wb\') as f:\n        torch.save([model, criterion, optimizer], f)\n\ndef model_load(fn):\n    global model, criterion, optimizer\n    with open(fn, \'rb\') as f:\n        model, criterion, optimizer = torch.load(f)\n\nimport os\nimport hashlib\nfn = \'corpus.{}.data\'.format(hashlib.md5(args.data.encode()).hexdigest())\nif os.path.exists(fn):\n    print(\'Loading cached dataset...\')\n    corpus = torch.load(fn)\nelse:\n    print(\'Producing dataset...\')\n    corpus = data.Corpus(args.data)\n    torch.save(corpus, fn)\n\neval_batch_size = 10\ntest_batch_size = 1  # DON\'T CHANGE THIS\ntrain_data = batchify(corpus.train, args.batch_size, args)\nval_data = batchify(corpus.valid, eval_batch_size, args)\ntest_data = batchify(corpus.test, test_batch_size, args)\n\n\nprint(\'Producing ood datasets...\')\n\nanswers_corpus = data.OODCorpus(\'eng_web_tbk/answers/conll/answers_penntrees.dev.conll\', corpus.dictionary, char=args.character_level)\nanswers_data = batchify(answers_corpus.data, test_batch_size, args)\n\nemail_corpus = data.OODCorpus(\'eng_web_tbk/email/conll/email_penntrees.dev.conll\', corpus.dictionary, char=args.character_level)\nemail_data = batchify(email_corpus.data, test_batch_size, args)\n\nnewsgroup_corpus = data.OODCorpus(\'eng_web_tbk/newsgroup/conll/newsgroup_penntrees.dev.conll\', corpus.dictionary, char=args.character_level)\nnewsgroup_data = batchify(newsgroup_corpus.data, test_batch_size, args)\n\nreviews_corpus = data.OODCorpus(\'eng_web_tbk/reviews/conll/reviews_penntrees.dev.conll\', corpus.dictionary, char=args.character_level)\nreviews_data = batchify(reviews_corpus.data, test_batch_size, args)\n\nweblog_corpus = data.OODCorpus(\'eng_web_tbk/weblog/conll/weblog_penntrees.dev.conll\', corpus.dictionary, char=args.character_level)\nweblog_data = batchify(weblog_corpus.data, test_batch_size, args)\n\n\n###############################################################################\n# Build the model\n###############################################################################\n\nfrom splitcross import SplitCrossEntropyLoss\ncriterion = None\n\nntokens = len(corpus.dictionary)\nmodel = model.RNNModel(args.model, ntokens, args.emsize, args.nhid, args.nlayers, args.dropout, args.dropouth, args.dropouti, args.dropoute, args.wdrop, args.tied)\n###\nassert args.resume, \'must provide a --resume argument\'\n\nprint(\'Resuming model ...\')\nmodel_load(args.resume)\noptimizer.param_groups[0][\'lr\'] = args.lr\nmodel.dropouti, model.dropouth, model.dropout, args.dropoute = args.dropouti, args.dropouth, args.dropout, args.dropoute\nif args.wdrop:\n    from weight_drop import WeightDrop\n    for rnn in model.rnns:\n        if type(rnn) == WeightDrop: rnn.dropout = args.wdrop\n        elif rnn.zoneout > 0: rnn.zoneout = args.wdrop\n###\nif not criterion:\n    splits = []\n    if ntokens > 500000:\n        # One Billion\n        # This produces fairly even matrix mults for the buckets:\n        # 0: 11723136, 1: 10854630, 2: 11270961, 3: 11219422\n        splits = [4200, 35000, 180000]\n    elif ntokens > 75000:\n        # WikiText-103\n        splits = [2800, 20000, 76000]\n    print(\'Using\', splits)\n    criterion = SplitCrossEntropyLoss(args.emsize, splits=splits, verbose=False)\n###\nif args.cuda:\n    model = model.cuda()\n    criterion = criterion.cuda()\n###\nparams = list(model.parameters()) + list(criterion.parameters())\ntotal_params = sum(x.size()[0] * x.size()[1] if len(x.size()) > 1 else x.size()[0] for x in params if x.size())\nprint(\'Args:\', args)\nprint(\'Model total parameters:\', total_params)\n\n###############################################################################\n# Eval code\n###############################################################################\n\nood_num_examples = test_data.size(0) // 5\nexpected_ap = ood_num_examples / (ood_num_examples + test_data.size(0))\nrecall_level = 0.9\n\n\ndef get_base_rates():\n    batch, i = 0, 0\n    seq_len = args.bptt\n    ntokens = len(corpus.dictionary)\n    token_counts = np.zeros(ntokens)\n    total_count = 0\n\n    for i in range(0, train_data.size(0), args.bptt):  # Assume OE dataset is larger. It is, because we\'re using wikitext-2.\n        data, targets = get_batch(train_data, i, args, seq_len=seq_len)\n        for j in range(targets.numel()):\n            token_counts[targets[j].data.cpu().numpy()[0]] += 1\n            total_count += 1\n        batch += 1\n\n    return token_counts / total_count\n\n\nprint(\'Getting base rates...\')\n# base_rates = get_base_rates()\n# np.save(\'./base_rates.npy\', base_rates)\nbase_rates = Variable(torch.from_numpy(np.load(\'./base_rates.npy\').astype(np.float32))).cuda().float().squeeze()  # shit happens\nuniform_base_rates = Variable(torch.from_numpy(np.ones(len(corpus.dictionary)).astype(np.float32))).cuda().float().squeeze()\nuniform_base_rates /= uniform_base_rates.numel()\nprint(\'Done.\')\n\n\ndef evaluate(data_source, corpus, batch_size=10, ood=False):\n    # Turn on evaluation mode which disables dropout.\n    model.eval()\n    if args.model == \'QRNN\': model.reset()\n    loss_accum = 0\n    losses = []\n    ntokens = len(corpus.dictionary)\n    for i in range(0, data_source.size(0) - 1, args.bptt):\n        if (i >= ood_num_examples // test_batch_size) and (ood is True):\n            break\n\n        hidden = model.init_hidden(batch_size)\n        hidden = repackage_hidden(hidden)\n\n        data, targets = get_batch(data_source, i, args, evaluation=True)\n        output, hidden = model(data, hidden)\n        \n        logits = model.decoder(output)\n        smaxes = F.softmax(logits - torch.max(logits, dim=1, keepdim=True)[0], dim=1)\n        tmp = smaxes[range(targets.size(0)), targets]\n        log_prob = torch.log(tmp).mean(0)  # divided by seq len, so this is the negative nats per char\n        loss = -log_prob.data.cpu().numpy()[0]\n        \n        loss_accum += loss\n        # losses.append(loss)\n        # Experimental!\n        # anomaly_score = -torch.max(smaxes, dim=1)[0].mean()  # negative MSP\n        anomaly_score = ((smaxes).add(1e-18).log() * uniform_base_rates.unsqueeze(0)).sum(1).mean(0)  # negative KL to uniform\n        losses.append(anomaly_score.data.cpu().numpy()[0])\n        #\n\n    return loss_accum / (len(data_source) // args.bptt), losses\n\n\n\n# Run on test data.\nprint(\'\\nPTB\')\ntest_loss, test_losses = evaluate(test_data, corpus, test_batch_size)\nprint(\'=\' * 89)\nprint(\'| End of training | test loss {:5.2f} | test ppl {:8.2f} | test bpc {:8.3f}\'.format(\n    test_loss, math.exp(test_loss), test_loss / math.log(2)))\nprint(\'=\' * 89)\n\n\nprint(\'\\nAnswers (OOD)\')\nood_loss, ood_losses = evaluate(answers_data, answers_corpus, test_batch_size, ood=True)\nprint(\'=\' * 89)\nprint(\'| End of training | test loss {:5.2f} | test ppl {:8.2f} | test bpc {:8.3f}\'.format(\n    ood_loss, math.exp(ood_loss), ood_loss / math.log(2)))\nprint(\'=\' * 89)\nshow_performance(ood_losses, test_losses, expected_ap, recall_level=recall_level)\n\n\nprint(\'\\nEmail (OOD)\')\nood_loss, ood_losses = evaluate(email_data, email_corpus, test_batch_size, ood=True)\nprint(\'=\' * 89)\nprint(\'| End of training | test loss {:5.2f} | test ppl {:8.2f} | test bpc {:8.3f}\'.format(\n    ood_loss, math.exp(ood_loss), ood_loss / math.log(2)))\nprint(\'=\' * 89)\nshow_performance(ood_losses, test_losses, expected_ap, recall_level=recall_level)\n\n\nprint(\'\\nNewsgroup (OOD)\')\nood_loss, ood_losses = evaluate(newsgroup_data, newsgroup_corpus, test_batch_size, ood=True)\nprint(\'=\' * 89)\nprint(\'| End of training | test loss {:5.2f} | test ppl {:8.2f} | test bpc {:8.3f}\'.format(\n    ood_loss, math.exp(ood_loss), ood_loss / math.log(2)))\nprint(\'=\' * 89)\nshow_performance(ood_losses, test_losses, expected_ap, recall_level=recall_level)\n\n\nprint(\'\\nReviews (OOD)\')\nood_loss, ood_losses = evaluate(reviews_data, reviews_corpus, test_batch_size, ood=True)\nprint(\'=\' * 89)\nprint(\'| End of training | test loss {:5.2f} | test ppl {:8.2f} | test bpc {:8.3f}\'.format(\n    ood_loss, math.exp(ood_loss), ood_loss / math.log(2)))\nprint(\'=\' * 89)\nshow_performance(ood_losses, test_losses, expected_ap, recall_level=recall_level)\n\n\nprint(\'\\nWeblog (OOD)\')\nood_loss, ood_losses = evaluate(weblog_data, weblog_corpus, test_batch_size, ood=True)\nprint(\'=\' * 89)\nprint(\'| End of training | test loss {:5.2f} | test ppl {:8.2f} | test bpc {:8.3f}\'.format(\n    ood_loss, math.exp(ood_loss), ood_loss / math.log(2)))\nprint(\'=\' * 89)\nshow_performance(ood_losses, test_losses, expected_ap, recall_level=recall_level)\n'"
NLP_language_modeling/locked_dropout.py,2,"b'import torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\n\nclass LockedDropout(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n    def forward(self, x, dropout=0.5):\n        if not self.training or not dropout:\n            return x\n        m = x.data.new(1, x.size(1), x.size(2)).bernoulli_(1 - dropout)\n        mask = Variable(m, requires_grad=False) / (1 - dropout)\n        mask = mask.expand_as(x)\n        return mask * x\n'"
NLP_language_modeling/model.py,5,"b'import torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\n\nfrom embed_regularize import embedded_dropout\nfrom locked_dropout import LockedDropout\nfrom weight_drop import WeightDrop\n\nclass RNNModel(nn.Module):\n    """"""Container module with an encoder, a recurrent module, and a decoder.""""""\n\n    def __init__(self, rnn_type, ntoken, ninp, nhid, nlayers, dropout=0.5, dropouth=0.5, dropouti=0.5, dropoute=0.1, wdrop=0, tie_weights=False):\n        super(RNNModel, self).__init__()\n        self.lockdrop = LockedDropout()\n        self.idrop = nn.Dropout(dropouti)\n        self.hdrop = nn.Dropout(dropouth)\n        self.drop = nn.Dropout(dropout)\n        self.encoder = nn.Embedding(ntoken, ninp)\n        assert rnn_type in [\'LSTM\', \'QRNN\', \'GRU\'], \'RNN type is not supported\'\n        if rnn_type == \'LSTM\':\n            self.rnns = [torch.nn.LSTM(ninp if l == 0 else nhid, nhid if l != nlayers - 1 else (ninp if tie_weights else nhid), 1, dropout=0) for l in range(nlayers)]\n            if wdrop:\n                self.rnns = [WeightDrop(rnn, [\'weight_hh_l0\'], dropout=wdrop) for rnn in self.rnns]\n        if rnn_type == \'GRU\':\n            self.rnns = [torch.nn.GRU(ninp if l == 0 else nhid, nhid if l != nlayers - 1 else ninp, 1, dropout=0) for l in range(nlayers)]\n            if wdrop:\n                self.rnns = [WeightDrop(rnn, [\'weight_hh_l0\'], dropout=wdrop) for rnn in self.rnns]\n        elif rnn_type == \'QRNN\':\n            from torchqrnn import QRNNLayer\n            self.rnns = [QRNNLayer(input_size=ninp if l == 0 else nhid, hidden_size=nhid if l != nlayers - 1 else (ninp if tie_weights else nhid), save_prev_x=True, zoneout=0, window=2 if l == 0 else 1, output_gate=True) for l in range(nlayers)]\n            for rnn in self.rnns:\n                rnn.linear = WeightDrop(rnn.linear, [\'weight\'], dropout=wdrop)\n        print(self.rnns)\n        self.rnns = torch.nn.ModuleList(self.rnns)\n        self.decoder = nn.Linear(nhid, ntoken)\n\n        # Optionally tie weights as in:\n        # ""Using the Output Embedding to Improve Language Models"" (Press & Wolf 2016)\n        # https://arxiv.org/abs/1608.05859\n        # and\n        # ""Tying Word Vectors and Word Classifiers: A Loss Framework for Language Modeling"" (Inan et al. 2016)\n        # https://arxiv.org/abs/1611.01462\n        if tie_weights:\n            #if nhid != ninp:\n            #    raise ValueError(\'When using the tied flag, nhid must be equal to emsize\')\n            self.decoder.weight = self.encoder.weight\n\n        self.init_weights()\n\n        self.rnn_type = rnn_type\n        self.ninp = ninp\n        self.nhid = nhid\n        self.nlayers = nlayers\n        self.dropout = dropout\n        self.dropouti = dropouti\n        self.dropouth = dropouth\n        self.dropoute = dropoute\n        self.tie_weights = tie_weights\n\n    def reset(self):\n        if self.rnn_type == \'QRNN\': [r.reset() for r in self.rnns]\n\n    def init_weights(self):\n        initrange = 0.1\n        self.encoder.weight.data.uniform_(-initrange, initrange)\n        self.decoder.bias.data.fill_(0)\n        self.decoder.weight.data.uniform_(-initrange, initrange)\n\n    def forward(self, input, hidden, return_h=False):\n        emb = embedded_dropout(self.encoder, input, dropout=self.dropoute if self.training else 0)\n        #emb = self.idrop(emb)\n\n        emb = self.lockdrop(emb, self.dropouti)\n\n        raw_output = emb\n        new_hidden = []\n        #raw_output, hidden = self.rnn(emb, hidden)\n        raw_outputs = []\n        outputs = []\n        for l, rnn in enumerate(self.rnns):\n            current_input = raw_output\n            raw_output, new_h = rnn(raw_output, hidden[l])\n            new_hidden.append(new_h)\n            raw_outputs.append(raw_output)\n            if l != self.nlayers - 1:\n                #self.hdrop(raw_output)\n                raw_output = self.lockdrop(raw_output, self.dropouth)\n                outputs.append(raw_output)\n        hidden = new_hidden\n\n        output = self.lockdrop(raw_output, self.dropout)\n        outputs.append(output)\n\n        result = output.view(output.size(0)*output.size(1), output.size(2))\n        if return_h:\n            return result, hidden, raw_outputs, outputs\n        return result, hidden\n\n    def init_hidden(self, bsz):\n        weight = next(self.parameters()).data\n        if self.rnn_type == \'LSTM\':\n            return [(Variable(weight.new(1, bsz, self.nhid if l != self.nlayers - 1 else (self.ninp if self.tie_weights else self.nhid)).zero_()),\n                    Variable(weight.new(1, bsz, self.nhid if l != self.nlayers - 1 else (self.ninp if self.tie_weights else self.nhid)).zero_()))\n                    for l in range(self.nlayers)]\n        elif self.rnn_type == \'QRNN\' or self.rnn_type == \'GRU\':\n            return [Variable(weight.new(1, bsz, self.nhid if l != self.nlayers - 1 else (self.ninp if self.tie_weights else self.nhid)).zero_())\n                    for l in range(self.nlayers)]\n'"
NLP_language_modeling/pointer.py,14,"b""import argparse\nimport time\nimport math\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\n\nimport data\nimport model\n\nfrom utils import batchify, get_batch, repackage_hidden\n\nparser = argparse.ArgumentParser(description='PyTorch PennTreeBank RNN/LSTM Language Model')\nparser.add_argument('--data', type=str, default='data/penn',\n                    help='location of the data corpus')\nparser.add_argument('--model', type=str, default='LSTM',\n                    help='type of recurrent net (LSTM, QRNN)')\nparser.add_argument('--save', type=str,default='best.pt',\n                    help='model to use the pointer over')\nparser.add_argument('--cuda', action='store_false',\n                    help='use CUDA')\nparser.add_argument('--bptt', type=int, default=5000,\n                    help='sequence length')\nparser.add_argument('--window', type=int, default=3785,\n                    help='pointer window length')\nparser.add_argument('--theta', type=float, default=0.6625523432485668,\n                    help='mix between uniform distribution and pointer softmax distribution over previous words')\nparser.add_argument('--lambdasm', type=float, default=0.12785920428335693,\n                    help='linear mix between only pointer (1) and only vocab (0) distribution')\nargs = parser.parse_args()\n\n###############################################################################\n# Load data\n###############################################################################\n\ncorpus = data.Corpus(args.data)\n\neval_batch_size = 1\ntest_batch_size = 1\n#train_data = batchify(corpus.train, args.batch_size)\nval_data = batchify(corpus.valid, test_batch_size, args)\ntest_data = batchify(corpus.test, test_batch_size, args)\n\n###############################################################################\n# Build the model\n###############################################################################\n\nntokens = len(corpus.dictionary)\ncriterion = nn.CrossEntropyLoss()\n\ndef one_hot(idx, size, cuda=True):\n    a = np.zeros((1, size), np.float32)\n    a[0][idx] = 1\n    v = Variable(torch.from_numpy(a))\n    if cuda: v = v.cuda()\n    return v\n\ndef evaluate(data_source, batch_size=10, window=args.window):\n    # Turn on evaluation mode which disables dropout.\n    if args.model == 'QRNN': model.reset()\n    model.eval()\n    total_loss = 0\n    ntokens = len(corpus.dictionary)\n    hidden = model.init_hidden(batch_size)\n    next_word_history = None\n    pointer_history = None\n    for i in range(0, data_source.size(0) - 1, args.bptt):\n        if i > 0: print(i, len(data_source), math.exp(total_loss / i))\n        data, targets = get_batch(data_source, i, evaluation=True, args=args)\n        output, hidden, rnn_outs, _ = model(data, hidden, return_h=True)\n        rnn_out = rnn_outs[-1].squeeze()\n        output_flat = output.view(-1, ntokens)\n        ###\n        # Fill pointer history\n        start_idx = len(next_word_history) if next_word_history is not None else 0\n        next_word_history = torch.cat([one_hot(t.data[0], ntokens) for t in targets]) if next_word_history is None else torch.cat([next_word_history, torch.cat([one_hot(t.data[0], ntokens) for t in targets])])\n        #print(next_word_history)\n        pointer_history = Variable(rnn_out.data) if pointer_history is None else torch.cat([pointer_history, Variable(rnn_out.data)], dim=0)\n        #print(pointer_history)\n        ###\n        # Built-in cross entropy\n        # total_loss += len(data) * criterion(output_flat, targets).data[0]\n        ###\n        # Manual cross entropy\n        # softmax_output_flat = torch.nn.functional.softmax(output_flat)\n        # soft = torch.gather(softmax_output_flat, dim=1, index=targets.view(-1, 1))\n        # entropy = -torch.log(soft)\n        # total_loss += len(data) * entropy.mean().data[0]\n        ###\n        # Pointer manual cross entropy\n        loss = 0\n        softmax_output_flat = torch.nn.functional.softmax(output_flat)\n        for idx, vocab_loss in enumerate(softmax_output_flat):\n            p = vocab_loss\n            if start_idx + idx > window:\n                valid_next_word = next_word_history[start_idx + idx - window:start_idx + idx]\n                valid_pointer_history = pointer_history[start_idx + idx - window:start_idx + idx]\n                logits = torch.mv(valid_pointer_history, rnn_out[idx])\n                theta = args.theta\n                ptr_attn = torch.nn.functional.softmax(theta * logits).view(-1, 1)\n                ptr_dist = (ptr_attn.expand_as(valid_next_word) * valid_next_word).sum(0).squeeze()\n                lambdah = args.lambdasm\n                p = lambdah * ptr_dist + (1 - lambdah) * vocab_loss\n            ###\n            target_loss = p[targets[idx].data]\n            loss += (-torch.log(target_loss)).data[0]\n        total_loss += loss / batch_size\n        ###\n        hidden = repackage_hidden(hidden)\n        next_word_history = next_word_history[-window:]\n        pointer_history = pointer_history[-window:]\n    return total_loss / len(data_source)\n\n# Load the best saved model.\nwith open(args.save, 'rb') as f:\n    if not args.cuda:\n        model = torch.load(f, map_location=lambda storage, loc: storage)\n    else:\n        model = torch.load(f)\nprint(model)\n\n# Run on val data.\nval_loss = evaluate(val_data, test_batch_size)\nprint('=' * 89)\nprint('| End of pointer | val loss {:5.2f} | val ppl {:8.2f}'.format(\n    val_loss, math.exp(val_loss)))\nprint('=' * 89)\n\n# Run on test data.\ntest_loss = evaluate(test_data, test_batch_size)\nprint('=' * 89)\nprint('| End of pointer | test loss {:5.2f} | test ppl {:8.2f}'.format(\n    test_loss, math.exp(test_loss)))\nprint('=' * 89)\n"""
NLP_language_modeling/splitcross.py,31,"b""from collections import defaultdict\n\nimport torch\nimport torch.nn as nn\n\nimport numpy as np\n\n\nclass SplitCrossEntropyLoss(nn.Module):\n    r'''SplitCrossEntropyLoss calculates an approximate softmax'''\n    def __init__(self, hidden_size, splits, verbose=False):\n        # We assume splits is [0, split1, split2, N] where N >= |V|\n        # For example, a vocab of 1000 words may have splits [0] + [100, 500] + [inf]\n        super(SplitCrossEntropyLoss, self).__init__()\n        self.hidden_size = hidden_size\n        self.splits = [0] + splits + [100 * 1000000]\n        self.nsplits = len(self.splits) - 1\n        self.stats = defaultdict(list)\n        self.verbose = verbose\n        # Each of the splits that aren't in the head require a pretend token, we'll call them tombstones\n        # The probability given to this tombstone is the probability of selecting an item from the represented split\n        if self.nsplits > 1:\n            self.tail_vectors = nn.Parameter(torch.zeros(self.nsplits - 1, hidden_size))\n            self.tail_bias = nn.Parameter(torch.zeros(self.nsplits - 1))\n\n    def logprob(self, weight, bias, hiddens, splits=None, softmaxed_head_res=None, verbose=False):\n        # First we perform the first softmax on the head vocabulary and the tombstones\n        if softmaxed_head_res is None:\n            start, end = self.splits[0], self.splits[1]\n            head_weight = None if end - start == 0 else weight[start:end]\n            head_bias = None if end - start == 0 else bias[start:end]\n            # We only add the tombstones if we have more than one split\n            if self.nsplits > 1:\n                head_weight = self.tail_vectors if head_weight is None else torch.cat([head_weight, self.tail_vectors])\n                head_bias = self.tail_bias if head_bias is None else torch.cat([head_bias, self.tail_bias])\n\n            # Perform the softmax calculation for the word vectors in the head for all splits\n            # We need to guard against empty splits as torch.cat does not like random lists\n            head_res = torch.nn.functional.linear(hiddens, head_weight, bias=head_bias)\n            softmaxed_head_res = torch.nn.functional.log_softmax(head_res)\n\n        if splits is None:\n            splits = list(range(self.nsplits))\n\n        results = []\n        running_offset = 0\n        for idx in splits:\n\n            # For those targets in the head (idx == 0) we only need to return their loss\n            if idx == 0:\n                results.append(softmaxed_head_res[:, :-(self.nsplits - 1)])\n\n            # If the target is in one of the splits, the probability is the p(tombstone) * p(word within tombstone)\n            else:\n                start, end = self.splits[idx], self.splits[idx + 1]\n                tail_weight = weight[start:end]\n                tail_bias = bias[start:end]\n\n                # Calculate the softmax for the words in the tombstone\n                tail_res = torch.nn.functional.linear(hiddens, tail_weight, bias=tail_bias)\n\n                # Then we calculate p(tombstone) * p(word in tombstone)\n                # Adding is equivalent to multiplication in log space\n                head_entropy = (softmaxed_head_res[:, -idx]).contiguous()\n                tail_entropy = torch.nn.functional.log_softmax(tail_res)\n                results.append(head_entropy.view(-1, 1) + tail_entropy)\n\n        if len(results) > 1:\n            return torch.cat(results, dim=1)\n        return results[0]\n\n    def split_on_targets(self, hiddens, targets):\n        # Split the targets into those in the head and in the tail\n        split_targets = []\n        split_hiddens = []\n\n        # Determine to which split each element belongs (for each start split value, add 1 if equal or greater)\n        # This method appears slower at least for WT-103 values for approx softmax\n        #masks = [(targets >= self.splits[idx]).view(1, -1) for idx in range(1, self.nsplits)]\n        #mask = torch.sum(torch.cat(masks, dim=0), dim=0)\n        ###\n        # This is equally fast for smaller splits as method below but scales linearly\n        mask = None\n        for idx in range(1, self.nsplits):\n            partial_mask = targets >= self.splits[idx]\n            mask = mask + partial_mask if mask is not None else partial_mask\n        ###\n        #masks = torch.stack([targets] * (self.nsplits - 1))\n        #mask = torch.sum(masks >= self.split_starts, dim=0)\n        for idx in range(self.nsplits):\n            # If there are no splits, avoid costly masked select\n            if self.nsplits == 1:\n                split_targets, split_hiddens = [targets], [hiddens]\n                continue\n            # If all the words are covered by earlier targets, we have empties so later stages don't freak out\n            if sum(len(t) for t in split_targets) == len(targets):\n                split_targets.append([])\n                split_hiddens.append([])\n                continue\n            # Are you in our split?\n            tmp_mask = mask == idx\n            split_targets.append(torch.masked_select(targets, tmp_mask))\n            split_hiddens.append(hiddens.masked_select(tmp_mask.unsqueeze(1).expand_as(hiddens)).view(-1, hiddens.size(1)))\n        return split_targets, split_hiddens\n\n    def forward(self, weight, bias, hiddens, targets, verbose=False):\n        if self.verbose or verbose:\n            for idx in sorted(self.stats):\n                print('{}: {}'.format(idx, int(np.mean(self.stats[idx]))), end=', ')\n            print()\n\n        total_loss = None\n        if len(hiddens.size()) > 2: hiddens = hiddens.view(-1, hiddens.size(2))\n\n        split_targets, split_hiddens = self.split_on_targets(hiddens, targets)\n\n        # First we perform the first softmax on the head vocabulary and the tombstones\n        start, end = self.splits[0], self.splits[1]\n        head_weight = None if end - start == 0 else weight[start:end]\n        head_bias = None if end - start == 0 else bias[start:end]\n\n        # We only add the tombstones if we have more than one split\n        if self.nsplits > 1:\n            head_weight = self.tail_vectors if head_weight is None else torch.cat([head_weight, self.tail_vectors])\n            head_bias = self.tail_bias if head_bias is None else torch.cat([head_bias, self.tail_bias])\n\n        # Perform the softmax calculation for the word vectors in the head for all splits\n        # We need to guard against empty splits as torch.cat does not like random lists\n        combo = torch.cat([split_hiddens[i] for i in range(self.nsplits) if len(split_hiddens[i])])\n        ###\n        all_head_res = torch.nn.functional.linear(combo, head_weight, bias=head_bias)\n        softmaxed_all_head_res = torch.nn.functional.log_softmax(all_head_res)\n        if self.verbose or verbose:\n            self.stats[0].append(combo.size()[0] * head_weight.size()[0])\n\n        running_offset = 0\n        for idx in range(self.nsplits):\n            # If there are no targets for this split, continue\n            if len(split_targets[idx]) == 0: continue\n\n            # For those targets in the head (idx == 0) we only need to return their loss\n            if idx == 0:\n                softmaxed_head_res = softmaxed_all_head_res[running_offset:running_offset + len(split_hiddens[idx])]\n                entropy = -torch.gather(softmaxed_head_res, dim=1, index=split_targets[idx].view(-1, 1))\n            # If the target is in one of the splits, the probability is the p(tombstone) * p(word within tombstone)\n            else:\n                softmaxed_head_res = softmaxed_all_head_res[running_offset:running_offset + len(split_hiddens[idx])]\n\n                if self.verbose or verbose:\n                    start, end = self.splits[idx], self.splits[idx + 1]\n                    tail_weight = weight[start:end]\n                    self.stats[idx].append(split_hiddens[idx].size()[0] * tail_weight.size()[0])\n\n                # Calculate the softmax for the words in the tombstone\n                tail_res = self.logprob(weight, bias, split_hiddens[idx], splits=[idx], softmaxed_head_res=softmaxed_head_res)\n\n                # Then we calculate p(tombstone) * p(word in tombstone)\n                # Adding is equivalent to multiplication in log space\n                head_entropy = softmaxed_head_res[:, -idx]\n                # All indices are shifted - if the first split handles [0,...,499] then the 500th in the second split will be 0 indexed\n                indices = (split_targets[idx] - self.splits[idx]).view(-1, 1)\n                # Warning: if you don't squeeze, you get an N x 1 return, which acts oddly with broadcasting\n                tail_entropy = torch.gather(torch.nn.functional.log_softmax(tail_res), dim=1, index=indices).squeeze()\n                entropy = -(head_entropy + tail_entropy)\n            ###\n            running_offset += len(split_hiddens[idx])\n            total_loss = entropy.float().sum() if total_loss is None else total_loss + entropy.float().sum()\n\n        return (total_loss / len(targets)).type_as(weight)\n\n\nif __name__ == '__main__':\n    np.random.seed(42)\n    torch.manual_seed(42)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed(42)\n\n    V = 8\n    H = 10\n    N = 100\n    E = 10\n\n    embed = torch.nn.Embedding(V, H)\n    crit = SplitCrossEntropyLoss(hidden_size=H, splits=[V // 2])\n    bias = torch.nn.Parameter(torch.ones(V))\n    optimizer = torch.optim.SGD(list(embed.parameters()) + list(crit.parameters()), lr=1)\n\n    for _ in range(E):\n        prev = torch.autograd.Variable((torch.rand(N, 1) * 0.999 * V).int().long())\n        x = torch.autograd.Variable((torch.rand(N, 1) * 0.999 * V).int().long())\n        y = embed(prev).squeeze()\n        c = crit(embed.weight, bias, y, x.view(N))\n        print('Crit', c.exp().data[0])\n\n        logprobs = crit.logprob(embed.weight, bias, y[:2]).exp()\n        print(logprobs)\n        print(logprobs.sum(dim=1))\n\n        optimizer.zero_grad()\n        c.backward()\n        optimizer.step()\n"""
NLP_language_modeling/train.py,20,"b'import argparse\nimport time\nimport math\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\nimport torch.nn.functional as F\n\nimport data\nimport model\n\nfrom utils_lm import batchify, get_batch, repackage_hidden\n\nparser = argparse.ArgumentParser(description=\'Train with OE using cross-entropy to uniform.\')\nparser.add_argument(\'--data\', type=str, default=\'data/penn/\',\n                    help=\'location of the data corpus\')\nparser.add_argument(\'--model\', type=str, default=\'LSTM\',\n                    help=\'type of recurrent net (LSTM, QRNN, GRU)\')\nparser.add_argument(\'--emsize\', type=int, default=400,\n                    help=\'size of word embeddings\')\nparser.add_argument(\'--nhid\', type=int, default=1150,\n                    help=\'number of hidden units per layer\')\nparser.add_argument(\'--nlayers\', type=int, default=3,\n                    help=\'number of layers\')\nparser.add_argument(\'--lr\', type=float, default=30,\n                    help=\'initial learning rate\')\nparser.add_argument(\'--clip\', type=float, default=0.25,\n                    help=\'gradient clipping\')\nparser.add_argument(\'--epochs\', type=int, default=8000,\n                    help=\'upper epoch limit\')\nparser.add_argument(\'--batch_size\', type=int, default=80, metavar=\'N\',\n                    help=\'batch size\')\nparser.add_argument(\'--bptt\', type=int, default=70,\n                    help=\'sequence length\')\nparser.add_argument(\'--dropout\', type=float, default=0.4,\n                    help=\'dropout applied to layers (0 = no dropout)\')\nparser.add_argument(\'--dropouth\', type=float, default=0.3,\n                    help=\'dropout for rnn layers (0 = no dropout)\')\nparser.add_argument(\'--dropouti\', type=float, default=0.65,\n                    help=\'dropout for input embedding layers (0 = no dropout)\')\nparser.add_argument(\'--dropoute\', type=float, default=0.1,\n                    help=\'dropout to remove words from embedding layer (0 = no dropout)\')\nparser.add_argument(\'--wdrop\', type=float, default=0.5,\n                    help=\'amount of weight dropout to apply to the RNN hidden to hidden matrix\')\nparser.add_argument(\'--seed\', type=int, default=1111,\n                    help=\'random seed\')\nparser.add_argument(\'--nonmono\', type=int, default=5,\n                    help=\'random seed\')\nparser.add_argument(\'--cuda\', action=\'store_false\',\n                    help=\'use CUDA\')\nparser.add_argument(\'--log-interval\', type=int, default=200, metavar=\'N\',\n                    help=\'report interval\')\nrandomhash = \'\'.join(str(time.time()).split(\'.\'))\nparser.add_argument(\'--save\', type=str,  default=randomhash+\'.pt\',\n                    help=\'path to save the final model\')\nparser.add_argument(\'--alpha\', type=float, default=2,\n                    help=\'alpha L2 regularization on RNN activation (alpha = 0 means no regularization)\')\nparser.add_argument(\'--beta\', type=float, default=1,\n                    help=\'beta slowness regularization applied on RNN activiation (beta = 0 means no regularization)\')\nparser.add_argument(\'--wdecay\', type=float, default=1.2e-6,\n                    help=\'weight decay applied to all weights\')\nparser.add_argument(\'--resume\', type=str,  default=\'\',\n                    help=\'path of model to resume\')\nparser.add_argument(\'--optimizer\', type=str,  default=\'sgd\',\n                    help=\'optimizer to use (sgd, adam)\')\nparser.add_argument(\'--when\', nargs=""+"", type=int, default=[-1],\n                    help=\'When (which epochs) to divide the learning rate by 10 - accepts multiple\')\nparser.add_argument(\'--use_OE\', type=str, choices=[\'yes\', \'no\'], help=\'Set to ""no"" if training without OE, ""yes"" if training with OE\')\nparser.add_argument(\'--wikitext_char\', action=\'store_true\', help=\'Load character-level WikiText. Use when in-dist is character-level.\')\nargs = parser.parse_args()\nprint(args)\nargs.tied = True\n\n# Set the random seed manually for reproducibility.\nnp.random.seed(args.seed)\ntorch.manual_seed(args.seed)\nif torch.cuda.is_available():\n    if not args.cuda:\n        print(""WARNING: You have a CUDA device, so you should probably run with --cuda"")\n    else:\n        torch.cuda.manual_seed(args.seed)\n\n###############################################################################\n# Load data\n###############################################################################\n\ndef model_save(fn):\n    with open(fn, \'wb\') as f:\n        torch.save([model, criterion, optimizer], f)\n\ndef model_load(fn):\n    global model, criterion, optimizer\n    with open(fn, \'rb\') as f:\n        model, criterion, optimizer = torch.load(f)\n\nimport os\nimport hashlib\nfn = \'corpus.{}.data\'.format(hashlib.md5(args.data.encode()).hexdigest())\nif os.path.exists(fn):\n    print(\'Loading cached dataset...\')\n    corpus = torch.load(fn)\nelse:\n    print(\'Producing dataset...\')\n    corpus = data.Corpus(args.data)\n    torch.save(corpus, fn)\n\neval_batch_size = 10\ntest_batch_size = 1\ntrain_data = batchify(corpus.train, args.batch_size, args)\nval_data = batchify(corpus.valid, eval_batch_size, args)\ntest_data = batchify(corpus.test, test_batch_size, args)\n\n###############################################################################\n# Load OE data\n###############################################################################\n\nprint(\'Producing dataset...\')\nif args.wikitext_char:\n    oe_corpus = data.CorpusWikiTextChar(\'data/wikitext-2\', corpus.dictionary)\n\n    oe_dataset = batchify(oe_corpus.train, args.batch_size, args)\n    oe_val_dataset = batchify(oe_corpus.valid, eval_batch_size, args)\nelse:\n    oe_corpus = data.Corpus(\'data/wikitext-2\', corpus.dictionary)\n\n    oe_dataset = batchify(oe_corpus.train, args.batch_size, args)\n    oe_val_dataset = batchify(oe_corpus.valid, eval_batch_size, args)\n\n###############################################################################\n# Build the model\n###############################################################################\n\nfrom splitcross import SplitCrossEntropyLoss\ncriterion = None\n\nntokens = len(corpus.dictionary)\nmodel = model.RNNModel(args.model, ntokens, args.emsize, args.nhid, args.nlayers, args.dropout, args.dropouth, args.dropouti, args.dropoute, args.wdrop, args.tied)\n###\nif args.resume:\n    print(\'Resuming model ...\')\n    model_load(args.resume)\n    optimizer.param_groups[0][\'lr\'] = args.lr\n    model.dropouti, model.dropouth, model.dropout, args.dropoute = args.dropouti, args.dropouth, args.dropout, args.dropoute\n    if args.wdrop:\n        from weight_drop import WeightDrop\n        for rnn in model.rnns:\n            if type(rnn) == WeightDrop: rnn.dropout = args.wdrop\n            elif rnn.zoneout > 0: rnn.zoneout = args.wdrop\n###\nif not criterion:\n    splits = []\n    if ntokens > 500000:\n        # One Billion\n        # This produces fairly even matrix mults for the buckets:\n        # 0: 11723136, 1: 10854630, 2: 11270961, 3: 11219422\n        splits = [4200, 35000, 180000]\n    elif ntokens > 75000:\n        # WikiText-103\n        splits = [2800, 20000, 76000]\n    print(\'Using\', splits)\n    criterion = SplitCrossEntropyLoss(args.emsize, splits=splits, verbose=False)\n###\nif args.cuda:\n    model = model.cuda()\n    criterion = criterion.cuda()\n###\nparams = list(model.parameters()) + list(criterion.parameters())\ntotal_params = sum(x.size()[0] * x.size()[1] if len(x.size()) > 1 else x.size()[0] for x in params if x.size())\nprint(\'Args:\', args)\nprint(\'Model total parameters:\', total_params)\n\n###############################################################################\n# Training code\n###############################################################################\n\ndef evaluate(data_source, batch_size=10, test=False):\n    # Turn on evaluation mode which disables dropout.\n    model.eval()\n    if args.model == \'QRNN\': model.reset()\n    total_loss = 0\n    total_oe_loss = 0\n    num_batches = 0\n    ntokens = len(corpus.dictionary)\n    for i in range(0, data_source.size(0) - 1, args.bptt):\n        data, targets = get_batch(data_source, i, args, evaluation=True)\n        data_oe, _ = get_batch(oe_val_dataset, i, args, evaluation=True)\n\n        if len(data.size()) == 1:  # happens for test set?\n            data.unsqueeze(-1)\n            data_oe.unsqueeze(-1)\n\n        if data.size(0) != data_oe.size(0):\n            continue\n\n        bs = test_batch_size if test else eval_batch_size\n        hidden = model.init_hidden(2 * bs) \n        hidden = repackage_hidden(hidden)\n\n        output, hidden, rnn_hs, dropped_rnn_hs = model(torch.cat([data, data_oe], dim=1), hidden, return_h=True)\n        output, output_oe = torch.chunk(dropped_rnn_hs[-1], dim=1, chunks=2)\n        output, output_oe = output.contiguous(), output_oe.contiguous()\n        output = output.view(output.size(0)*output.size(1), output.size(2))\n\n        loss = criterion(model.decoder.weight, model.decoder.bias, output, targets).data\n\n        # OE loss\n        logits_oe = model.decoder(output_oe)\n        smaxes_oe = F.softmax(logits_oe - torch.max(logits_oe, dim=-1, keepdim=True)[0], dim=-1)\n        loss_oe = -smaxes_oe.log().mean(-1)\n        loss_oe = loss_oe.mean().data\n        #\n\n        total_loss += loss\n        total_oe_loss += loss_oe\n        num_batches += 1\n    return total_loss[0] / num_batches, total_oe_loss[0] / num_batches\n\n\ndef train():\n    # Turn on training mode which enables dropout.\n    if args.model == \'QRNN\': model.reset()\n    total_loss = 0\n    total_oe_loss = 0\n    start_time = time.time()\n    ntokens = len(corpus.dictionary)\n    batch, i = 0, 0\n\n    # indices for randomizing order of segments\n    train_indices = np.arange(train_data.size(0) // args.bptt)\n    np.random.shuffle(train_indices)\n\n    oe_indices = np.arange(oe_dataset.size(0) // args.bptt)\n    np.random.shuffle(oe_indices)\n    #\n\n    seq_len = args.bptt\n\n    for i in range(0, train_data.size(0), args.bptt):  # Assume OE dataset is larger. It is, because we\'re using wikitext-2.\n\n        lr2 = optimizer.param_groups[0][\'lr\']\n        optimizer.param_groups[0][\'lr\'] = lr2 * seq_len / args.bptt\n        model.train()\n        data, targets = get_batch(train_data, i, args, seq_len=seq_len)\n        data_oe, _ = get_batch(oe_dataset, i, args, seq_len=seq_len)\n\n        if data.size(0) != data_oe.size(0):  # Don\'t train on this batch if the sequence lengths are different (happens at end of epoch).\n            continue\n\n        # We need a new hidden state for each segment, because this makes evaluation easier and more meaningful.\n        hidden = model.init_hidden(2 * args.batch_size)\n        hidden = repackage_hidden(hidden)\n\n        output, hidden, rnn_hs, dropped_rnn_hs = model(torch.cat([data, data_oe], dim=1), hidden, return_h=True)\n        output, output_oe = torch.chunk(dropped_rnn_hs[-1], dim=1, chunks=2)\n        output, output_oe = output.contiguous(), output_oe.contiguous()\n        output = output.view(output.size(0)*output.size(1), output.size(2))\n\n        raw_loss = criterion(model.decoder.weight, model.decoder.bias, output, targets)\n\n\n        loss = raw_loss\n        # Activiation Regularization\n        if args.alpha: loss = loss + sum(args.alpha * dropped_rnn_h.pow(2).mean() for dropped_rnn_h in dropped_rnn_hs[-1:])\n        # Temporal Activation Regularization (slowness)\n        if args.beta: loss = loss + sum(args.beta * (rnn_h[1:] - rnn_h[:-1]).pow(2).mean() for rnn_h in rnn_hs[-1:])\n\n        # OE loss\n        logits_oe = model.decoder(output_oe)\n        smaxes_oe = F.softmax(logits_oe - torch.max(logits_oe, dim=-1, keepdim=True)[0], dim=-1)\n        loss_oe = -smaxes_oe.log().mean(-1)  # for cross entropy\n        loss_oe = loss_oe.mean()  # for ERM\n        #\n\n        if args.use_OE == \'yes\':\n            loss_bp = loss + 0.5 * loss_oe\n        else:\n            loss_bp = loss\n\n        optimizer.zero_grad()\n        loss_bp.backward()\n\n        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n        if args.clip: torch.nn.utils.clip_grad_norm(params, args.clip)\n        optimizer.step()\n\n        total_loss += raw_loss.data\n        total_oe_loss += loss_oe.data\n        optimizer.param_groups[0][\'lr\'] = lr2\n        if batch % args.log_interval == 0 and batch > 0:\n            cur_loss = total_loss[0] / args.log_interval\n            cur_oe_loss = total_oe_loss[0] /args.log_interval\n            elapsed = time.time() - start_time\n            print(\'| epoch {:3d} | {:5d}/{:5d} batches | lr {:05.5f} | ms/batch {:5.2f} | \'\n                    \'loss {:5.2f} | oe_loss {:5.2f} | ppl {:8.2f} | bpc {:8.3f}\'.format(\n                epoch, batch, len(train_data) // args.bptt, optimizer.param_groups[0][\'lr\'],\n                elapsed * 1000 / args.log_interval, cur_loss, cur_oe_loss, math.exp(cur_loss), cur_loss / math.log(2)))\n            total_loss = 0\n            total_oe_loss = 0\n            start_time = time.time()\n        ###\n        batch += 1\n\n# Loop over epochs.\nlr = args.lr\nbest_val_loss = []\nstored_loss = 100000000\n\n# At any point you can hit Ctrl + C to break out of training early.\ntry:\n    optimizer = None\n    # Ensure the optimizer is optimizing params, which includes both the model\'s weights as well as the criterion\'s weight (i.e. Adaptive Softmax)\n    if args.optimizer == \'sgd\':\n        optimizer = torch.optim.SGD(params, lr=args.lr, weight_decay=args.wdecay)\n    if args.optimizer == \'adam\':\n        optimizer = torch.optim.Adam(params, lr=args.lr, weight_decay=args.wdecay)\n    for epoch in range(1, args.epochs+1):\n        epoch_start_time = time.time()\n        train()\n        if \'t0\' in optimizer.param_groups[0]:\n            tmp = {}\n            for prm in model.parameters():\n                tmp[prm] = prm.data.clone()\n                prm.data = optimizer.state[prm][\'ax\'].clone()\n\n            val_loss2, val_oe_loss = evaluate(val_data)\n            print(\'-\' * 89)\n            print(\'| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | val oe_loss {:5.2f} | \'\n                \'valid ppl {:8.2f} | valid bpc {:8.3f}\'.format(\n              epoch, (time.time() - epoch_start_time), val_loss, val_oe_loss, math.exp(val_loss), val_loss / math.log(2)))\n            print(\'-\' * 89)\n\n            if val_loss2 < stored_loss:\n                model_save(args.save)\n                print(\'Saving Averaged!\')\n                stored_loss = val_loss2\n\n            for prm in model.parameters():\n                prm.data = tmp[prm].clone()\n\n        else:\n            val_loss, val_oe_loss = evaluate(val_data, eval_batch_size)\n            print(\'-\' * 89)\n            print(\'| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | val oe_loss {:5.2f} | \'\n                \'valid ppl {:8.2f} | valid bpc {:8.3f}\'.format(\n              epoch, (time.time() - epoch_start_time), val_loss, val_oe_loss, math.exp(val_loss), val_loss / math.log(2)))\n            print(\'-\' * 89)\n\n            if val_loss < stored_loss:\n                model_save(args.save)\n                print(\'Saving model (new best validation)\')\n                stored_loss = val_loss\n\n            if args.optimizer == \'sgd\' and \'t0\' not in optimizer.param_groups[0] and (len(best_val_loss)>args.nonmono and val_loss > min(best_val_loss[:-args.nonmono])):\n                print(\'Switching to ASGD\')\n                optimizer = torch.optim.ASGD(model.parameters(), lr=args.lr, t0=0, lambd=0., weight_decay=args.wdecay)\n\n            if epoch in args.when:\n                print(\'Saving model before learning rate decreased\')\n                model_save(\'{}.e{}\'.format(args.save, epoch))\n                print(\'Dividing learning rate by 10\')\n                optimizer.param_groups[0][\'lr\'] /= 10.\n\n            best_val_loss.append(val_loss)\n\nexcept KeyboardInterrupt:\n    print(\'-\' * 89)\n    print(\'Exiting from training early\')\n\n# Load the best saved model.\nmodel_load(args.save)\n\n# Run on test data.\ntest_loss, val_oe_loss = evaluate(test_data, test_batch_size)\nprint(\'=\' * 89)\nprint(\'| End of training | test loss {:5.2f} | val oe_loss {:5.2f} | test ppl {:8.2f} | test bpc {:8.3f}\'.format(\n    test_loss, val_oe_loss, math.exp(test_loss), test_loss / math.log(2)))\nprint(\'=\' * 89)\n'"
NLP_language_modeling/train_base_rates.py,21,"b'import argparse\nimport time\nimport math\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\nimport torch.nn.functional as F\n\nimport data\nimport model\n\nfrom utils_lm import batchify, get_batch, repackage_hidden\n\nparser = argparse.ArgumentParser(description=\'Train with OE using cross-entropy to base rates.\')\nparser.add_argument(\'--data\', type=str, default=\'data/penn/\',\n                    help=\'location of the data corpus\')\nparser.add_argument(\'--model\', type=str, default=\'LSTM\',\n                    help=\'type of recurrent net (LSTM, QRNN, GRU)\')\nparser.add_argument(\'--emsize\', type=int, default=400,\n                    help=\'size of word embeddings\')\nparser.add_argument(\'--nhid\', type=int, default=1150,\n                    help=\'number of hidden units per layer\')\nparser.add_argument(\'--nlayers\', type=int, default=3,\n                    help=\'number of layers\')\nparser.add_argument(\'--lr\', type=float, default=30,\n                    help=\'initial learning rate\')\nparser.add_argument(\'--clip\', type=float, default=0.25,\n                    help=\'gradient clipping\')\nparser.add_argument(\'--epochs\', type=int, default=8000,\n                    help=\'upper epoch limit\')\nparser.add_argument(\'--batch_size\', type=int, default=80, metavar=\'N\',\n                    help=\'batch size\')\nparser.add_argument(\'--bptt\', type=int, default=70,\n                    help=\'sequence length\')\nparser.add_argument(\'--dropout\', type=float, default=0.4,\n                    help=\'dropout applied to layers (0 = no dropout)\')\nparser.add_argument(\'--dropouth\', type=float, default=0.3,\n                    help=\'dropout for rnn layers (0 = no dropout)\')\nparser.add_argument(\'--dropouti\', type=float, default=0.65,\n                    help=\'dropout for input embedding layers (0 = no dropout)\')\nparser.add_argument(\'--dropoute\', type=float, default=0.1,\n                    help=\'dropout to remove words from embedding layer (0 = no dropout)\')\nparser.add_argument(\'--wdrop\', type=float, default=0.5,\n                    help=\'amount of weight dropout to apply to the RNN hidden to hidden matrix\')\nparser.add_argument(\'--seed\', type=int, default=1111,\n                    help=\'random seed\')\nparser.add_argument(\'--nonmono\', type=int, default=5,\n                    help=\'random seed\')\nparser.add_argument(\'--cuda\', action=\'store_false\',\n                    help=\'use CUDA\')\nparser.add_argument(\'--log-interval\', type=int, default=200, metavar=\'N\',\n                    help=\'report interval\')\nrandomhash = \'\'.join(str(time.time()).split(\'.\'))\nparser.add_argument(\'--save\', type=str,  default=randomhash+\'.pt\',\n                    help=\'path to save the final model\')\nparser.add_argument(\'--alpha\', type=float, default=2,\n                    help=\'alpha L2 regularization on RNN activation (alpha = 0 means no regularization)\')\nparser.add_argument(\'--beta\', type=float, default=1,\n                    help=\'beta slowness regularization applied on RNN activiation (beta = 0 means no regularization)\')\nparser.add_argument(\'--wdecay\', type=float, default=1.2e-6,\n                    help=\'weight decay applied to all weights\')\nparser.add_argument(\'--resume\', type=str,  default=\'\',\n                    help=\'path of model to resume\')\nparser.add_argument(\'--optimizer\', type=str,  default=\'sgd\',\n                    help=\'optimizer to use (sgd, adam)\')\nparser.add_argument(\'--when\', nargs=""+"", type=int, default=[-1],\n                    help=\'When (which epochs) to divide the learning rate by 10 - accepts multiple\')\nparser.add_argument(\'--use_OE\', type=str)\nparser.add_argument(\'--wikitext_char\', action=\'store_true\', help=\'Load character-level WikiText. Use when in-dist is character-level.\')\nargs = parser.parse_args()\nprint(args)\nargs.tied = True\n\n# Set the random seed manually for reproducibility.\nnp.random.seed(args.seed)\ntorch.manual_seed(args.seed)\nif torch.cuda.is_available():\n    if not args.cuda:\n        print(""WARNING: You have a CUDA device, so you should probably run with --cuda"")\n    else:\n        torch.cuda.manual_seed(args.seed)\n\n###############################################################################\n# Load data\n###############################################################################\n\ndef model_save(fn):\n    with open(fn, \'wb\') as f:\n        torch.save([model, criterion, optimizer], f)\n\ndef model_load(fn):\n    global model, criterion, optimizer\n    with open(fn, \'rb\') as f:\n        model, criterion, optimizer = torch.load(f)\n\nimport os\nimport hashlib\nfn = \'corpus.{}.data\'.format(hashlib.md5(args.data.encode()).hexdigest())\nif os.path.exists(fn):\n    print(\'Loading cached dataset...\')\n    corpus = torch.load(fn)\nelse:\n    print(\'Producing dataset...\')\n    corpus = data.Corpus(args.data)\n    torch.save(corpus, fn)\n\neval_batch_size = 10\ntest_batch_size = 1\ntrain_data = batchify(corpus.train, args.batch_size, args)\nval_data = batchify(corpus.valid, eval_batch_size, args)\ntest_data = batchify(corpus.test, test_batch_size, args)\n\n###############################################################################\n# Load OE data\n###############################################################################\n\nprint(\'Producing dataset...\')\nif args.wikitext_char:\n    oe_corpus = data.CorpusWikiTextChar(\'data/wikitext-2\', corpus.dictionary)\n\n    oe_dataset = batchify(oe_corpus.train, args.batch_size, args)\n    oe_val_dataset = batchify(oe_corpus.valid, eval_batch_size, args)\nelse:\n    oe_corpus = data.Corpus(\'data/wikitext-2\', corpus.dictionary)\n\n    oe_dataset = batchify(oe_corpus.train, args.batch_size, args)\n    oe_val_dataset = batchify(oe_corpus.valid, eval_batch_size, args)\n\n###############################################################################\n# Build the model\n###############################################################################\n\nfrom splitcross import SplitCrossEntropyLoss\ncriterion = None\n\nntokens = len(corpus.dictionary)\nmodel = model.RNNModel(args.model, ntokens, args.emsize, args.nhid, args.nlayers, args.dropout, args.dropouth, args.dropouti, args.dropoute, args.wdrop, args.tied)\n###\nif args.resume:\n    print(\'Resuming model ...\')\n    model_load(args.resume)\n    optimizer.param_groups[0][\'lr\'] = args.lr\n    model.dropouti, model.dropouth, model.dropout, args.dropoute = args.dropouti, args.dropouth, args.dropout, args.dropoute\n    if args.wdrop:\n        from weight_drop import WeightDrop\n        for rnn in model.rnns:\n            if type(rnn) == WeightDrop: rnn.dropout = args.wdrop\n            elif rnn.zoneout > 0: rnn.zoneout = args.wdrop\n###\nif not criterion:\n    splits = []\n    if ntokens > 500000:\n        # One Billion\n        # This produces fairly even matrix mults for the buckets:\n        # 0: 11723136, 1: 10854630, 2: 11270961, 3: 11219422\n        splits = [4200, 35000, 180000]\n    elif ntokens > 75000:\n        # WikiText-103\n        splits = [2800, 20000, 76000]\n    print(\'Using\', splits)\n    criterion = SplitCrossEntropyLoss(args.emsize, splits=splits, verbose=False)\n###\nif args.cuda:\n    model = model.cuda()\n    criterion = criterion.cuda()\n###\nparams = list(model.parameters()) + list(criterion.parameters())\ntotal_params = sum(x.size()[0] * x.size()[1] if len(x.size()) > 1 else x.size()[0] for x in params if x.size())\nprint(\'Args:\', args)\nprint(\'Model total parameters:\', total_params)\n\n###############################################################################\n# Training code\n###############################################################################\n\ndef evaluate(data_source, batch_size=10, test=False):\n    # Turn on evaluation mode which disables dropout.\n    model.eval()\n    if args.model == \'QRNN\': model.reset()\n    total_loss = 0\n    total_oe_loss = 0\n    num_batches = 0\n    ntokens = len(corpus.dictionary)\n    for i in range(0, data_source.size(0) - 1, args.bptt):\n        data, targets = get_batch(data_source, i, args, evaluation=True)\n        data_oe, _ = get_batch(oe_val_dataset, i, args, evaluation=True)\n\n        if len(data.size()) == 1:  # happens for test set?\n            data.unsqueeze(-1)\n            data_oe.unsqueeze(-1)\n\n        if data.size(0) != data_oe.size(0):\n            continue\n\n        bs = test_batch_size if test else eval_batch_size\n        hidden = model.init_hidden(2 * bs) \n        hidden = repackage_hidden(hidden)\n\n        output, hidden, rnn_hs, dropped_rnn_hs = model(torch.cat([data, data_oe], dim=1), hidden, return_h=True)\n        output, output_oe = torch.chunk(dropped_rnn_hs[-1], dim=1, chunks=2)\n        output, output_oe = output.contiguous(), output_oe.contiguous()\n        output = output.view(output.size(0)*output.size(1), output.size(2))\n\n        loss = criterion(model.decoder.weight, model.decoder.bias, output, targets).data\n\n        # OE loss\n        logits_oe = model.decoder(output_oe)\n        smaxes_oe = F.softmax(logits_oe - torch.max(logits_oe, dim=-1, keepdim=True)[0], dim=-1)\n        loss_oe = -smaxes_oe.log().mean(-1)\n        loss_oe = loss_oe.mean().data\n        #\n\n        total_loss += loss\n        total_oe_loss += loss_oe\n        num_batches += 1\n    return total_loss[0] / num_batches, total_oe_loss[0] / num_batches\n\n\ndef get_base_rates():\n    batch, i = 0, 0\n    seq_len = args.bptt\n    ntokens = len(corpus.dictionary)\n    token_counts = np.zeros(ntokens)\n    total_count = 0\n\n    for i in range(0, train_data.size(0), args.bptt):  # Assume OE dataset is larger. It is, because we\'re using wikitext-2.\n        data, targets = get_batch(train_data, i, args, seq_len=seq_len)\n        for j in range(targets.numel()):\n            token_counts[targets[j].data.cpu().numpy()[0]] += 1\n            total_count += 1\n        batch += 1\n\n    return token_counts / total_count\n\n\ndef train(base_rates):\n    # Turn on training mode which enables dropout.\n    if args.model == \'QRNN\': model.reset()\n    total_loss = 0\n    total_oe_loss = 0\n    start_time = time.time()\n    ntokens = len(corpus.dictionary)\n    batch, i = 0, 0\n\n    # indices for randomizing order of segments\n    train_indices = np.arange(train_data.size(0) // args.bptt)\n    np.random.shuffle(train_indices)\n\n    oe_indices = np.arange(oe_dataset.size(0) // args.bptt)\n    np.random.shuffle(oe_indices)\n    #\n\n    seq_len = args.bptt\n\n    br = None\n\n    for i in range(0, train_data.size(0), args.bptt):  # Assume OE dataset is larger. It is, because we\'re using wikitext-2.\n\n        lr2 = optimizer.param_groups[0][\'lr\']\n        optimizer.param_groups[0][\'lr\'] = lr2 * seq_len / args.bptt\n        model.train()\n        data, targets = get_batch(train_data, i, args, seq_len=seq_len)\n        data_oe, _ = get_batch(oe_dataset, i, args, seq_len=seq_len)\n\n        if data.size(0) != data_oe.size(0):  # Don\'t train on this batch if the sequence lengths are different (happens at end of epoch).\n            continue\n\n        # We need a new hidden state for each segment, because this makes evaluation easier and more meaningful.\n        hidden = model.init_hidden(2 * args.batch_size)\n        hidden = repackage_hidden(hidden)\n\n        output, hidden, rnn_hs, dropped_rnn_hs = model(torch.cat([data, data_oe], dim=1), hidden, return_h=True)\n        output, output_oe = torch.chunk(dropped_rnn_hs[-1], dim=1, chunks=2)\n        output, output_oe = output.contiguous(), output_oe.contiguous()\n        output = output.view(output.size(0)*output.size(1), output.size(2))\n\n        raw_loss = criterion(model.decoder.weight, model.decoder.bias, output, targets)\n\n\n        loss = raw_loss\n        # Activiation Regularization\n        if args.alpha: loss = loss + sum(args.alpha * dropped_rnn_h.pow(2).mean() for dropped_rnn_h in dropped_rnn_hs[-1:])\n        # Temporal Activation Regularization (slowness)\n        if args.beta: loss = loss + sum(args.beta * (rnn_h[1:] - rnn_h[:-1]).pow(2).mean() for rnn_h in rnn_hs[-1:])\n\n        # OE loss\n        logits_oe = model.decoder(output_oe)\n        smaxes_oe = F.softmax(logits_oe - torch.max(logits_oe, dim=-1, keepdim=True)[0], dim=-1)\n        br = Variable(torch.FloatTensor(base_rates).unsqueeze(0).unsqueeze(0).expand_as(smaxes_oe)).cuda() if br is None else br\n        loss_oe = -(smaxes_oe.log() * br).sum(-1)  # for cross entropy\n        loss_oe = loss_oe.mean()  # for ERM\n        #\n\n        if args.use_OE == \'yes\':\n            loss_bp = loss + 0.5 * loss_oe\n        else:\n            loss_bp = loss\n\n        optimizer.zero_grad()\n        loss_bp.backward()\n\n        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n        if args.clip: torch.nn.utils.clip_grad_norm(params, args.clip)\n        optimizer.step()\n\n        total_loss += raw_loss.data\n        total_oe_loss += loss_oe.data\n        optimizer.param_groups[0][\'lr\'] = lr2\n        if batch % args.log_interval == 0 and batch > 0:\n            cur_loss = total_loss[0] / args.log_interval\n            cur_oe_loss = total_oe_loss[0] /args.log_interval\n            elapsed = time.time() - start_time\n            print(\'| epoch {:3d} | {:5d}/{:5d} batches | lr {:05.5f} | ms/batch {:5.2f} | \'\n                    \'loss {:5.2f} | oe_loss {:5.2f} | ppl {:8.2f} | bpc {:8.3f}\'.format(\n                epoch, batch, len(train_data) // args.bptt, optimizer.param_groups[0][\'lr\'],\n                elapsed * 1000 / args.log_interval, cur_loss, cur_oe_loss, math.exp(cur_loss), cur_loss / math.log(2)))\n            total_loss = 0\n            total_oe_loss = 0\n            start_time = time.time()\n        ###\n        batch += 1\n\n# Loop over epochs.\nlr = args.lr\nbest_val_loss = []\nstored_loss = 100000000\n\n# At any point you can hit Ctrl + C to break out of training early.\ntry:\n    optimizer = None\n    # Ensure the optimizer is optimizing params, which includes both the model\'s weights as well as the criterion\'s weight (i.e. Adaptive Softmax)\n    if args.optimizer == \'sgd\':\n        optimizer = torch.optim.SGD(params, lr=args.lr, weight_decay=args.wdecay)\n    if args.optimizer == \'adam\':\n        optimizer = torch.optim.Adam(params, lr=args.lr, weight_decay=args.wdecay)\n\n    base_rates = get_base_rates()\n    \n    for epoch in range(1, args.epochs+1):\n        epoch_start_time = time.time()\n        train(base_rates)\n        if \'t0\' in optimizer.param_groups[0]:\n            tmp = {}\n            for prm in model.parameters():\n                tmp[prm] = prm.data.clone()\n                prm.data = optimizer.state[prm][\'ax\'].clone()\n\n            val_loss2, val_oe_loss = evaluate(val_data)\n            print(\'-\' * 89)\n            print(\'| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | val oe_loss {:5.2f} | \'\n                \'valid ppl {:8.2f} | valid bpc {:8.3f}\'.format(\n              epoch, (time.time() - epoch_start_time), val_loss, val_oe_loss, math.exp(val_loss), val_loss / math.log(2)))\n            print(\'-\' * 89)\n\n            if val_loss2 < stored_loss:\n                model_save(args.save)\n                print(\'Saving Averaged!\')\n                stored_loss = val_loss2\n\n            for prm in model.parameters():\n                prm.data = tmp[prm].clone()\n\n        else:\n            val_loss, val_oe_loss = evaluate(val_data, eval_batch_size)\n            print(\'-\' * 89)\n            print(\'| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | val oe_loss {:5.2f} | \'\n                \'valid ppl {:8.2f} | valid bpc {:8.3f}\'.format(\n              epoch, (time.time() - epoch_start_time), val_loss, val_oe_loss, math.exp(val_loss), val_loss / math.log(2)))\n            print(\'-\' * 89)\n\n            if val_loss < stored_loss:\n                model_save(args.save)\n                print(\'Saving model (new best validation)\')\n                stored_loss = val_loss\n\n            if args.optimizer == \'sgd\' and \'t0\' not in optimizer.param_groups[0] and (len(best_val_loss)>args.nonmono and val_loss > min(best_val_loss[:-args.nonmono])):\n                print(\'Switching to ASGD\')\n                optimizer = torch.optim.ASGD(model.parameters(), lr=args.lr, t0=0, lambd=0., weight_decay=args.wdecay)\n\n            if epoch in args.when:\n                print(\'Saving model before learning rate decreased\')\n                model_save(\'{}.e{}\'.format(args.save, epoch))\n                print(\'Dividing learning rate by 10\')\n                optimizer.param_groups[0][\'lr\'] /= 10.\n\n            best_val_loss.append(val_loss)\n\nexcept KeyboardInterrupt:\n    print(\'-\' * 89)\n    print(\'Exiting from training early\')\n\n# Load the best saved model.\nmodel_load(args.save)\n\n# Run on test data.\ntest_loss, val_oe_loss = evaluate(test_data, test_batch_size)\nprint(\'=\' * 89)\nprint(\'| End of training | test loss {:5.2f} | val oe_loss {:5.2f} | test ppl {:8.2f} | test bpc {:8.3f}\'.format(\n    test_loss, val_oe_loss, math.exp(test_loss), test_loss / math.log(2)))\nprint(\'=\' * 89)\n'"
NLP_language_modeling/utils_lm.py,1,"b'from torch.autograd import Variable\n\ndef repackage_hidden(h):\n    """"""Wraps hidden states in new Variables, to detach them from their history.""""""\n    if type(h) == Variable:\n        return Variable(h.data)\n    else:\n        return tuple(repackage_hidden(v) for v in h)\n\ndef batchify(data, bsz, args):\n    # Work out how cleanly we can divide the dataset into bsz parts.\n    nbatch = data.size(0) // bsz\n    # Trim off any extra elements that wouldn\'t cleanly fit (remainders).\n    data = data.narrow(0, 0, nbatch * bsz)\n    # Evenly divide the data across the bsz batches.\n    data = data.view(bsz, -1).t().contiguous()\n    if args.cuda:\n        data = data.cuda()\n    return data\n\ndef get_batch(source, i, args, seq_len=None, evaluation=False):\n    seq_len = min(seq_len if seq_len else args.bptt, len(source) - 1 - i)\n    data = Variable(source[i:i+seq_len], volatile=evaluation)\n    target = Variable(source[i+1:i+1+seq_len].view(-1))\n    return data, target\n'"
NLP_language_modeling/weight_drop.py,9,"b""import torch\nfrom torch.nn import Parameter\nfrom functools import wraps\n\nclass WeightDrop(torch.nn.Module):\n    def __init__(self, module, weights, dropout=0, variational=False):\n        super(WeightDrop, self).__init__()\n        self.module = module\n        self.weights = weights\n        self.dropout = dropout\n        self.variational = variational\n        self._setup()\n\n    def widget_demagnetizer_y2k_edition(*args, **kwargs):\n        # We need to replace flatten_parameters with a nothing function\n        # It must be a function rather than a lambda as otherwise pickling explodes\n        # We can't write boring code though, so ... WIDGET DEMAGNETIZER Y2K EDITION!\n        # (\xe2\x95\xaf\xc2\xb0\xe2\x96\xa1\xc2\xb0\xef\xbc\x89\xe2\x95\xaf\xef\xb8\xb5 \xe2\x94\xbb\xe2\x94\x81\xe2\x94\xbb\n        return\n\n    def _setup(self):\n        # Terrible temporary solution to an issue regarding compacting weights re: CUDNN RNN\n        if issubclass(type(self.module), torch.nn.RNNBase):\n            self.module.flatten_parameters = self.widget_demagnetizer_y2k_edition\n\n        for name_w in self.weights:\n            print('Applying weight drop of {} to {}'.format(self.dropout, name_w))\n            w = getattr(self.module, name_w)\n            del self.module._parameters[name_w]\n            self.module.register_parameter(name_w + '_raw', Parameter(w.data))\n\n    def _setweights(self):\n        for name_w in self.weights:\n            raw_w = getattr(self.module, name_w + '_raw')\n            w = None\n            if self.variational:\n                mask = torch.autograd.Variable(torch.ones(raw_w.size(0), 1))\n                if raw_w.is_cuda: mask = mask.cuda()\n                mask = torch.nn.functional.dropout(mask, p=self.dropout, training=True)\n                w = mask.expand_as(raw_w) * raw_w\n            else:\n                w = torch.nn.functional.dropout(raw_w, p=self.dropout, training=self.training)\n            setattr(self.module, name_w, w)\n\n    def forward(self, *args):\n        self._setweights()\n        return self.module.forward(*args)\n\nif __name__ == '__main__':\n    import torch\n    from weight_drop import WeightDrop\n\n    # Input is (seq, batch, input)\n    x = torch.autograd.Variable(torch.randn(2, 1, 10)).cuda()\n    h0 = None\n\n    ###\n\n    print('Testing WeightDrop')\n    print('=-=-=-=-=-=-=-=-=-=')\n\n    ###\n\n    print('Testing WeightDrop with Linear')\n\n    lin = WeightDrop(torch.nn.Linear(10, 10), ['weight'], dropout=0.9)\n    lin.cuda()\n    run1 = [x.sum() for x in lin(x).data]\n    run2 = [x.sum() for x in lin(x).data]\n\n    print('All items should be different')\n    print('Run 1:', run1)\n    print('Run 2:', run2)\n\n    assert run1[0] != run2[0]\n    assert run1[1] != run2[1]\n\n    print('---')\n\n    ###\n\n    print('Testing WeightDrop with LSTM')\n\n    wdrnn = WeightDrop(torch.nn.LSTM(10, 10), ['weight_hh_l0'], dropout=0.9)\n    wdrnn.cuda()\n\n    run1 = [x.sum() for x in wdrnn(x, h0)[0].data]\n    run2 = [x.sum() for x in wdrnn(x, h0)[0].data]\n\n    print('First timesteps should be equal, all others should differ')\n    print('Run 1:', run1)\n    print('Run 2:', run2)\n\n    # First time step, not influenced by hidden to hidden weights, should be equal\n    assert run1[0] == run2[0]\n    # Second step should not\n    assert run1[1] != run2[1]\n\n    print('---')\n"""
SVHN/baseline.py,13,"b'# -*- coding: utf-8 -*-\nimport numpy as np\nimport os\nimport argparse\nimport time\nimport torch\nimport torch.nn as nn\nimport torch.backends.cudnn as cudnn\nimport torchvision.transforms as trn\nimport torchvision.datasets as dset\nimport torch.nn.functional as F\nfrom tqdm import tqdm\nfrom models.allconv import AllConvNet\nfrom models.wrn import WideResNet\n\nif __package__ is None:\n    import sys\n    from os import path\n\n    sys.path.append(path.dirname(path.dirname(path.abspath(__file__))))\n    import utils.svhn_loader as svhn\n    from utils.validation_dataset import validation_split\n\nparser = argparse.ArgumentParser(description=\'Trains an SVHN Classifier\',\n                                 formatter_class=argparse.ArgumentDefaultsHelpFormatter)\nparser.add_argument(\'--model\', \'-m\', type=str, default=\'allconv\',\n                    choices=[\'allconv\', \'wrn\'], help=\'Choose architecture.\')\nparser.add_argument(\'--calibration\', \'-c\', action=\'store_true\',\n                    help=\'Train a model to be used for calibration. This holds out some data for validation.\')\n# Optimization options\nparser.add_argument(\'--epochs\', \'-e\', type=int, default=20, help=\'Number of epochs to train.\')\nparser.add_argument(\'--learning_rate\', \'-lr\', type=float, default=0.01, help=\'The initial learning rate.\')\nparser.add_argument(\'--batch_size\', \'-b\', type=int, default=128, help=\'Batch size.\')\nparser.add_argument(\'--test_bs\', type=int, default=200)\nparser.add_argument(\'--momentum\', type=float, default=0.9, help=\'Momentum.\')\nparser.add_argument(\'--decay\', \'-d\', type=float, default=0.0005, help=\'Weight decay (L2 penalty).\')\n# WRN Architecture\nparser.add_argument(\'--layers\', default=16, type=int, help=\'total number of layers\')\nparser.add_argument(\'--widen-factor\', default=4, type=int, help=\'widen factor\')\nparser.add_argument(\'--droprate\', default=0.4, type=float, help=\'dropout probability\')\n# Checkpoints\nparser.add_argument(\'--save\', \'-s\', type=str, default=\'./snapshots/baseline\', help=\'Folder to save checkpoints.\')\nparser.add_argument(\'--load\', \'-l\', type=str, default=\'\', help=\'Checkpoint path to resume / test.\')\nparser.add_argument(\'--test\', \'-t\', action=\'store_true\', help=\'Test only flag.\')\n# Acceleration\nparser.add_argument(\'--ngpu\', type=int, default=1, help=\'0 = CPU.\')\nparser.add_argument(\'--prefetch\', type=int, default=2, help=\'Pre-fetching threads.\')\nargs = parser.parse_args()\n\nstate = {k: v for k, v in args._get_kwargs()}\nprint(state)\n\ntorch.manual_seed(1)\nnp.random.seed(1)\n\ntrain_data = svhn.SVHN(\'/share/data/vision-greg/svhn/\', split=\'train_and_extra\',\n                       transform=trn.ToTensor(), download=False)\ntest_data = svhn.SVHN(\'/share/data/vision-greg/svhn/\', split=\'test\',\n                      transform=trn.ToTensor(), download=False)\nnum_classes = 10\n\ncalib_indicator = \'\'\nif args.calibration:\n    train_data, val_data = validation_split(train_data, val_share=5000/604388.)\n    calib_indicator = \'calib_\'\n\ntrain_loader = torch.utils.data.DataLoader(\n    train_data, batch_size=args.batch_size, shuffle=True,\n    num_workers=args.prefetch, pin_memory=True)\ntest_loader = torch.utils.data.DataLoader(\n    test_data, batch_size=args.test_bs, shuffle=False,\n    num_workers=args.prefetch, pin_memory=True)\n\n# Create model\nif args.model == \'allconv\':\n    net = AllConvNet(num_classes)\nelse:\n    net = WideResNet(args.layers, num_classes, args.widen_factor, dropRate=args.droprate)\n\nstart_epoch = 0\n\n# Restore model if desired\nif args.load != \'\':\n    for i in range(1000 - 1, -1, -1):\n        model_name = os.path.join(args.load, calib_indicator + args.model + \'_baseline_epoch_\' + str(i) + \'.pt\')\n        if os.path.isfile(model_name):\n            net.load_state_dict(torch.load(model_name))\n            print(\'Model restored! Epoch:\', i)\n            start_epoch = i + 1\n            break\n    if start_epoch == 0:\n        assert False, ""could not resume""\n\nif args.ngpu > 1:\n    net = torch.nn.DataParallel(net, device_ids=list(range(args.ngpu)))\n\nif args.ngpu > 0:\n    net.cuda()\n    torch.cuda.manual_seed(1)\n\ncudnn.benchmark = True  # fire on all cylinders\n\noptimizer = torch.optim.SGD(\n    net.parameters(), state[\'learning_rate\'], momentum=state[\'momentum\'],\n    weight_decay=state[\'decay\'], nesterov=True)\n\n\ndef cosine_annealing(step, total_steps, lr_max, lr_min):\n    return lr_min + (lr_max - lr_min) * 0.5 * (\n            1 + np.cos(step / total_steps * np.pi))\n\n\nscheduler = torch.optim.lr_scheduler.LambdaLR(\n    optimizer,\n    lr_lambda=lambda step: cosine_annealing(\n        step,\n        args.epochs * len(train_loader),\n        1,  # since lr_lambda computes multiplicative factor\n        1e-6 / args.learning_rate))\n\n\n# /////////////// Training ///////////////\n\ndef train():\n    net.train()  # enter train mode\n    loss_avg = 0.0\n    for data, target in train_loader:\n        data, target = data.cuda(), target.long().cuda()\n\n        # forward\n        x = net(data)\n\n        # backward\n        scheduler.step()\n        optimizer.zero_grad()\n        loss = F.cross_entropy(x, target)\n        loss.backward()\n        optimizer.step()\n\n        # exponential moving average\n        loss_avg = loss_avg * 0.8 + float(loss) * 0.2\n\n    state[\'train_loss\'] = loss_avg\n\n\n# test function\ndef test():\n    net.eval()\n    loss_avg = 0.0\n    correct = 0\n    with torch.no_grad():\n        for data, target in test_loader:\n            data, target = data.cuda(), target.long().cuda()\n\n            # forward\n            output = net(data)\n            loss = F.cross_entropy(output, target)\n\n            # accuracy\n            pred = output.data.max(1)[1]\n            correct += pred.eq(target.data).sum().item()\n\n            # test loss average\n            loss_avg += float(loss.data)\n\n    state[\'test_loss\'] = loss_avg / len(test_loader)\n    state[\'test_accuracy\'] = correct / len(test_loader.dataset)\n\n\nif args.test:\n    test()\n    print(state)\n    exit()\n\n# Make save directory\nif not os.path.exists(args.save):\n    os.makedirs(args.save)\nif not os.path.isdir(args.save):\n    raise Exception(\'%s is not a dir\' % args.save)\n\nwith open(os.path.join(args.save, calib_indicator + args.model + \'_baseline_training_results.csv\'), \'w\') as f:\n    f.write(\'epoch,time(s),train_loss,test_loss,test_error(%)\\n\')\n\nprint(\'Beginning Training\\n\')\n\n# Main loop\nfor epoch in range(start_epoch, args.epochs):\n    state[\'epoch\'] = epoch\n\n    begin_epoch = time.time()\n\n    train()\n    test()\n\n    # Save model\n    torch.save(net.state_dict(),\n               os.path.join(args.save, calib_indicator + args.model + \'_baseline_epoch_\' + str(epoch) + \'.pt\'))\n    # Let us not waste space and delete the previous model\n    prev_path = os.path.join(args.save, calib_indicator + args.model + \'_baseline_epoch_\' + str(epoch - 1) + \'.pt\')\n    if os.path.exists(prev_path): os.remove(prev_path)\n\n    # Show results\n\n    with open(os.path.join(args.save, calib_indicator + args.model + \'_baseline_training_results.csv\'), \'a\') as f:\n        f.write(\'%03d,%05d,%0.6f,%0.5f,%0.2f\\n\' % (\n            (epoch + 1),\n            time.time() - begin_epoch,\n            state[\'train_loss\'],\n            state[\'test_loss\'],\n            100 - 100. * state[\'test_accuracy\'],\n        ))\n\n    # # print state with rounded decimals\n    # print({k: round(v, 4) if isinstance(v, float) else v for k, v in state.items()})\n\n    print(\'Epoch {0:3d} | Time {1:5d} | Train Loss {2:.4f} | Test Loss {3:.3f} | Test Error {4:.2f}\'.format(\n        (epoch + 1),\n        int(time.time() - begin_epoch),\n        state[\'train_loss\'],\n        state[\'test_loss\'],\n        100 - 100. * state[\'test_accuracy\'])\n    )\n'"
SVHN/oe_scratch.py,16,"b'# -*- coding: utf-8 -*-\nimport numpy as np\nimport os\nimport argparse\nimport time\nimport torch\nimport torch.nn as nn\nimport torch.backends.cudnn as cudnn\nimport torchvision.transforms as trn\nimport torchvision.datasets as dset\nimport torch.nn.functional as F\nfrom tqdm import tqdm\nfrom models.allconv import AllConvNet\nfrom models.wrn import WideResNet\n\nif __package__ is None:\n    import sys\n    from os import path\n\n    sys.path.append(path.dirname(path.dirname(path.abspath(__file__))))\n    import utils.svhn_loader as svhn\n    from utils.tinyimages_80mn_loader import TinyImages\n    from utils.validation_dataset import validation_split\n\nparser = argparse.ArgumentParser(description=\'Trains an SVHN Classifier with OE\',\n                                 formatter_class=argparse.ArgumentDefaultsHelpFormatter)\nparser.add_argument(\'--model\', \'-m\', type=str, default=\'allconv\',\n                    choices=[\'allconv\', \'wrn\'], help=\'Choose architecture.\')\nparser.add_argument(\'--calibration\', \'-c\', action=\'store_true\',\n                    help=\'Train a model to be used for calibration. This holds out some data for validation.\')\n# Optimization options\nparser.add_argument(\'--epochs\', \'-e\', type=int, default=20, help=\'Number of epochs to train.\')\nparser.add_argument(\'--learning_rate\', \'-lr\', type=float, default=0.01, help=\'The initial learning rate.\')\nparser.add_argument(\'--batch_size\', \'-b\', type=int, default=128, help=\'Batch size.\')\nparser.add_argument(\'--oe_batch_size\', type=int, default=256, help=\'Batch size.\')\nparser.add_argument(\'--test_bs\', type=int, default=200)\nparser.add_argument(\'--momentum\', type=float, default=0.9, help=\'Momentum.\')\nparser.add_argument(\'--decay\', \'-d\', type=float, default=0.0005, help=\'Weight decay (L2 penalty).\')\n# WRN Architecture\nparser.add_argument(\'--layers\', default=16, type=int, help=\'total number of layers\')\nparser.add_argument(\'--widen-factor\', default=4, type=int, help=\'widen factor\')\nparser.add_argument(\'--droprate\', default=0.4, type=float, help=\'dropout probability\')\n# Checkpoints\nparser.add_argument(\'--save\', \'-s\', type=str, default=\'./snapshots/oe_scratch\', help=\'Folder to save checkpoints.\')\nparser.add_argument(\'--load\', \'-l\', type=str, default=\'\', help=\'Checkpoint path to resume / test.\')\nparser.add_argument(\'--test\', \'-t\', action=\'store_true\', help=\'Test only flag.\')\n# Acceleration\nparser.add_argument(\'--ngpu\', type=int, default=1, help=\'0 = CPU.\')\nparser.add_argument(\'--prefetch\', type=int, default=2, help=\'Pre-fetching threads.\')\nargs = parser.parse_args()\n\nstate = {k: v for k, v in args._get_kwargs()}\nprint(state)\n\ntorch.manual_seed(1)\nnp.random.seed(1)\n\ntrain_data_in = svhn.SVHN(\'/share/data/vision-greg/svhn/\', split=\'train_and_extra\',\n                          transform=trn.ToTensor(), download=False)\ntest_data = svhn.SVHN(\'/share/data/vision-greg/svhn/\', split=\'test\',\n                      transform=trn.ToTensor(), download=False)\nnum_classes = 10\n\ncalib_indicator = \'\'\nif args.calibration:\n    train_data_in, val_data = validation_split(train_data_in, val_share=5000/604388.)\n    calib_indicator = \'calib_\'\n\ntiny_images = TinyImages(transform=trn.Compose(\n    [trn.ToTensor(), trn.ToPILImage(),\n     trn.RandomHorizontalFlip(), trn.ToTensor()]))\n\n\ntrain_loader_in = torch.utils.data.DataLoader(\n    train_data_in,\n    batch_size=args.batch_size, shuffle=True,\n    num_workers=args.prefetch, pin_memory=True)\n\ntrain_loader_out = torch.utils.data.DataLoader(\n    tiny_images,\n    batch_size=args.oe_batch_size, shuffle=False,\n    num_workers=args.prefetch, pin_memory=True)\n\ntest_loader = torch.utils.data.DataLoader(\n    test_data,\n    batch_size=args.batch_size, shuffle=False,\n    num_workers=args.prefetch, pin_memory=True)\n\n# Create model\nif args.model == \'allconv\':\n    net = AllConvNet(num_classes)\nelse:\n    net = WideResNet(args.layers, num_classes, args.widen_factor, dropRate=args.droprate)\n\nstart_epoch = 0\n\n# Restore model if desired\nif args.load != \'\':\n    for i in range(1000 - 1, -1, -1):\n        model_name = os.path.join(args.load, calib_indicator + args.model + \'_oe_scratch_epoch_\' + str(i) + \'.pt\')\n        if os.path.isfile(model_name):\n            net.load_state_dict(torch.load(model_name))\n            print(\'Model restored! Epoch:\', i)\n            start_epoch = i + 1\n            break\n    if start_epoch == 0:\n        assert False, ""could not resume""\n\nif args.ngpu > 1:\n    net = torch.nn.DataParallel(net, device_ids=list(range(args.ngpu)))\n\nif args.ngpu > 0:\n    net.cuda()\n    torch.cuda.manual_seed(1)\n\ncudnn.benchmark = True  # fire on all cylinders\n\noptimizer = torch.optim.SGD(\n    net.parameters(), state[\'learning_rate\'], momentum=state[\'momentum\'],\n    weight_decay=state[\'decay\'], nesterov=True)\n\n\ndef cosine_annealing(step, total_steps, lr_max, lr_min):\n    return lr_min + (lr_max - lr_min) * 0.5 * (\n            1 + np.cos(step / total_steps * np.pi))\n\n\nscheduler = torch.optim.lr_scheduler.LambdaLR(\n    optimizer,\n    lr_lambda=lambda step: cosine_annealing(\n        step,\n        args.epochs * len(train_loader_in),\n        1,  # since lr_lambda computes multiplicative factor\n        1e-6 / args.learning_rate))\n\n\n# /////////////// Training ///////////////\n\ndef train():\n    net.train()  # enter train mode\n    loss_avg = 0.0\n\n    # start at a random point of the outlier dataset; this induces more randomness without destroying locality\n    train_loader_out.dataset.offset = np.random.randint(len(train_loader_out.dataset))\n    for in_set, out_set in zip(train_loader_in, train_loader_out):\n        data = torch.cat((in_set[0], out_set[0]), 0)\n        target = in_set[1].long()\n\n        data, target = data.cuda(), target.cuda()\n\n        # forward\n        x = net(data)\n\n        # backward\n        scheduler.step()\n        optimizer.zero_grad()\n\n        loss = F.cross_entropy(x[:len(in_set[0])], target)\n        # cross-entropy from softmax distribution to uniform distribution\n        loss += 0.5 * -(x[len(in_set[0]):].mean(1) - torch.logsumexp(x[len(in_set[0]):], dim=1)).mean()\n\n        loss.backward()\n        optimizer.step()\n\n        # exponential moving average\n        loss_avg = loss_avg * 0.8 + float(loss) * 0.2\n\n    state[\'train_loss\'] = loss_avg\n\n\n# test function\ndef test():\n    net.eval()\n    loss_avg = 0.0\n    correct = 0\n    with torch.no_grad():\n        for data, target in test_loader:\n            data, target = data.cuda(), target.long().cuda()\n\n            # forward\n            output = net(data)\n            loss = F.cross_entropy(output, target)\n\n            # accuracy\n            pred = output.data.max(1)[1]\n            correct += pred.eq(target.data).sum().item()\n\n            # test loss average\n            loss_avg += float(loss.data)\n\n    state[\'test_loss\'] = loss_avg / len(test_loader)\n    state[\'test_accuracy\'] = correct / len(test_loader.dataset)\n\n\nif args.test:\n    test()\n    print(state)\n    exit()\n\n# Make save directory\nif not os.path.exists(args.save):\n    os.makedirs(args.save)\nif not os.path.isdir(args.save):\n    raise Exception(\'%s is not a dir\' % args.save)\n\nwith open(os.path.join(args.save, calib_indicator + args.model + \'_oe_scratch_training_results.csv\'), \'w\') as f:\n    f.write(\'epoch,time(s),train_loss,test_loss,test_error(%)\\n\')\n\nprint(\'Beginning Training\\n\')\n\n# Main loop\nfor epoch in range(start_epoch, args.epochs):\n    state[\'epoch\'] = epoch\n\n    begin_epoch = time.time()\n\n    train()\n    test()\n\n    # Save model\n    torch.save(net.state_dict(),\n               os.path.join(args.save, calib_indicator + args.model + \'_oe_scratch_epoch_\' + str(epoch) + \'.pt\'))\n    # Let us not waste space and delete the previous model\n    prev_path = os.path.join(args.save, calib_indicator + args.model + \'_oe_scratch_epoch_\' + str(epoch - 1) + \'.pt\')\n    if os.path.exists(prev_path): os.remove(prev_path)\n\n    # Show results\n\n    with open(os.path.join(args.save, calib_indicator + args.model + \'_oe_scratch_training_results.csv\'), \'a\') as f:\n        f.write(\'%03d,%05d,%0.6f,%0.5f,%0.2f\\n\' % (\n            (epoch + 1),\n            time.time() - begin_epoch,\n            state[\'train_loss\'],\n            state[\'test_loss\'],\n            100 - 100. * state[\'test_accuracy\'],\n        ))\n\n    # # print state with rounded decimals\n    # print({k: round(v, 4) if isinstance(v, float) else v for k, v in state.items()})\n\n    print(\'Epoch {0:3d} | Time {1:5d} | Train Loss {2:.4f} | Test Loss {3:.3f} | Test Error {4:.2f}\'.format(\n        (epoch + 1),\n        int(time.time() - begin_epoch),\n        state[\'train_loss\'],\n        state[\'test_loss\'],\n        100 - 100. * state[\'test_accuracy\'])\n    )\n'"
SVHN/oe_tune.py,16,"b'# -*- coding: utf-8 -*-\nimport numpy as np\nimport os\nimport argparse\nimport time\nimport torch\nimport torch.nn as nn\nimport torch.backends.cudnn as cudnn\nimport torchvision.transforms as trn\nimport torchvision.datasets as dset\nimport torch.nn.functional as F\nfrom tqdm import tqdm\nfrom models.allconv import AllConvNet\nfrom models.wrn import WideResNet\n\nif __package__ is None:\n    import sys\n    from os import path\n\n    sys.path.append(path.dirname(path.dirname(path.abspath(__file__))))\n    import utils.svhn_loader as svhn\n    from utils.tinyimages_80mn_loader import TinyImages\n    from utils.validation_dataset import validation_split\n\nparser = argparse.ArgumentParser(description=\'Tunes an SVHN Classifier with OE\',\n                                 formatter_class=argparse.ArgumentDefaultsHelpFormatter)\nparser.add_argument(\'--model\', \'-m\', type=str, default=\'allconv\',\n                    choices=[\'allconv\', \'wrn\'], help=\'Choose architecture.\')\nparser.add_argument(\'--calibration\', \'-c\', action=\'store_true\',\n                    help=\'Train a model to be used for calibration. This holds out some data for validation.\')\n# Optimization options\nparser.add_argument(\'--epochs\', \'-e\', type=int, default=5, help=\'Number of epochs to train.\')\nparser.add_argument(\'--learning_rate\', \'-lr\', type=float, default=0.0001, help=\'The initial learning rate.\')    # TODO\nparser.add_argument(\'--batch_size\', \'-b\', type=int, default=128, help=\'Batch size.\')\nparser.add_argument(\'--oe_batch_size\', type=int, default=256, help=\'Batch size.\')\nparser.add_argument(\'--test_bs\', type=int, default=200)\nparser.add_argument(\'--momentum\', type=float, default=0.9, help=\'Momentum.\')\nparser.add_argument(\'--decay\', \'-d\', type=float, default=0.0005, help=\'Weight decay (L2 penalty).\')\n# WRN Architecture\nparser.add_argument(\'--layers\', default=16, type=int, help=\'total number of layers\')\nparser.add_argument(\'--widen-factor\', default=4, type=int, help=\'widen factor\')\nparser.add_argument(\'--droprate\', default=0.4, type=float, help=\'dropout probability\')\n# Checkpoints\nparser.add_argument(\'--save\', \'-s\', type=str, default=\'./snapshots/oe_tune\', help=\'Folder to save checkpoints.\')\nparser.add_argument(\'--load\', \'-l\', type=str, default=\'./snapshots/baseline\', help=\'Checkpoint path to resume / test.\')\nparser.add_argument(\'--test\', \'-t\', action=\'store_true\', help=\'Test only flag.\')\n# Acceleration\nparser.add_argument(\'--ngpu\', type=int, default=1, help=\'0 = CPU.\')\nparser.add_argument(\'--prefetch\', type=int, default=2, help=\'Pre-fetching threads.\')\nargs = parser.parse_args()\n\nstate = {k: v for k, v in args._get_kwargs()}\nprint(state)\n\ntorch.manual_seed(1)\nnp.random.seed(1)\n\n\ntrain_data_in = svhn.SVHN(\'/share/data/vision-greg/svhn/\', split=\'train_and_extra\',\n                          transform=trn.ToTensor(), download=False)\ntest_data = svhn.SVHN(\'/share/data/vision-greg/svhn/\', split=\'test\',\n                      transform=trn.ToTensor(), download=False)\nnum_classes = 10\n\ncalib_indicator = \'\'\nif args.calibration:\n    train_data_in, val_data = validation_split(train_data_in, val_share=5000/604388.)\n    calib_indicator = \'calib_\'\n\ntiny_images = TinyImages(transform=trn.Compose(\n    [trn.ToTensor(), trn.ToPILImage(),\n     trn.RandomHorizontalFlip(), trn.ToTensor()]))\n\n\ntrain_loader_in = torch.utils.data.DataLoader(\n    train_data_in,\n    batch_size=args.batch_size, shuffle=True,\n    num_workers=args.prefetch, pin_memory=True)\n\ntrain_loader_out = torch.utils.data.DataLoader(\n    tiny_images,\n    batch_size=args.oe_batch_size, shuffle=False,\n    num_workers=args.prefetch, pin_memory=True)\n\ntest_loader = torch.utils.data.DataLoader(\n    test_data,\n    batch_size=args.batch_size, shuffle=False,\n    num_workers=args.prefetch, pin_memory=True)\n\n# Create model\nif args.model == \'allconv\':\n    net = AllConvNet(num_classes)\nelse:\n    net = WideResNet(args.layers, num_classes, args.widen_factor, dropRate=args.droprate)\n\n\n# Restore model\nmodel_found = False\nif args.load != \'\':\n    for i in range(1000 - 1, -1, -1):\n        model_name = os.path.join(args.load, calib_indicator + args.model + \'_baseline_epoch_\' + str(i) + \'.pt\')\n        if os.path.isfile(model_name):\n            net.load_state_dict(torch.load(model_name))\n            print(\'Model restored! Epoch:\', i)\n            model_found = True\n            break\n    if not model_found:\n        assert False, ""could not find model to restore""\n\nif args.ngpu > 1:\n    net = torch.nn.DataParallel(net, device_ids=list(range(args.ngpu)))\n\nif args.ngpu > 0:\n    net.cuda()\n    torch.cuda.manual_seed(1)\n\ncudnn.benchmark = True  # fire on all cylinders\n\noptimizer = torch.optim.SGD(\n    net.parameters(), state[\'learning_rate\'], momentum=state[\'momentum\'],\n    weight_decay=state[\'decay\'], nesterov=True)\n\n\ndef cosine_annealing(step, total_steps, lr_max, lr_min):\n    return lr_min + (lr_max - lr_min) * 0.5 * (\n            1 + np.cos(step / total_steps * np.pi))\n\n\nscheduler = torch.optim.lr_scheduler.LambdaLR(\n    optimizer,\n    lr_lambda=lambda step: cosine_annealing(\n        step,\n        args.epochs * len(train_loader_in),\n        1,  # since lr_lambda computes multiplicative factor\n        1e-6 / args.learning_rate))\n\n\n# /////////////// Training ///////////////\n\ndef train():\n    net.train()  # enter train mode\n    loss_avg = 0.0\n\n    # start at a random point of the outlier dataset; this induces more randomness without destroying locality\n    train_loader_out.dataset.offset = np.random.randint(len(train_loader_out.dataset))\n    for in_set, out_set in zip(train_loader_in, train_loader_out):\n        data = torch.cat((in_set[0], out_set[0]), 0)\n        target = in_set[1].long()\n\n        data, target = data.cuda(), target.long().cuda()\n\n        # forward\n        x = net(data)\n\n        # backward\n        scheduler.step()\n        optimizer.zero_grad()\n\n        loss = F.cross_entropy(x[:len(in_set[0])], target)\n        # cross-entropy from softmax distribution to uniform distribution\n        loss += 0.5 * -(x[len(in_set[0]):].mean(1) - torch.logsumexp(x[len(in_set[0]):], dim=1)).mean()\n\n        loss.backward()\n        optimizer.step()\n\n        # exponential moving average\n        loss_avg = loss_avg * 0.8 + float(loss) * 0.2\n\n    state[\'train_loss\'] = loss_avg\n\n\n# test function\ndef test():\n    net.eval()\n    loss_avg = 0.0\n    correct = 0\n    with torch.no_grad():\n        for data, target in test_loader:\n            data, target = data.cuda(), target.long().cuda()\n\n            # forward\n            output = net(data)\n            loss = F.cross_entropy(output, target)\n\n            # accuracy\n            pred = output.data.max(1)[1]\n            correct += pred.eq(target.data).sum().item()\n\n            # test loss average\n            loss_avg += float(loss.data)\n\n    state[\'test_loss\'] = loss_avg / len(test_loader)\n    state[\'test_accuracy\'] = correct / len(test_loader.dataset)\n\n\nif args.test:\n    test()\n    print(state)\n    exit()\n\n# Make save directory\nif not os.path.exists(args.save):\n    os.makedirs(args.save)\nif not os.path.isdir(args.save):\n    raise Exception(\'%s is not a dir\' % args.save)\n\nwith open(os.path.join(args.save, calib_indicator + args.model + \'_oe_tune_training_results.csv\'), \'w\') as f:\n    f.write(\'epoch,time(s),train_loss,test_loss,test_error(%)\\n\')\n\nprint(\'Beginning Training\\n\')\n\n# Main loop\nfor epoch in range(0, args.epochs):\n    state[\'epoch\'] = epoch\n\n    begin_epoch = time.time()\n\n    train()\n    test()\n\n    # Save model\n    torch.save(net.state_dict(),\n               os.path.join(args.save, calib_indicator + args.model + \'_oe_tune_epoch_\' + str(epoch) + \'.pt\'))\n    # Let us not waste space and delete the previous model\n    prev_path = os.path.join(args.save, calib_indicator + args.model + \'_oe_tune_epoch_\' + str(epoch - 1) + \'.pt\')\n    if os.path.exists(prev_path): os.remove(prev_path)\n\n    # Show results\n\n    with open(os.path.join(args.save, calib_indicator + args.model + \'_oe_tune_training_results.csv\'), \'a\') as f:\n        f.write(\'%03d,%05d,%0.6f,%0.5f,%0.2f\\n\' % (\n            (epoch + 1),\n            time.time() - begin_epoch,\n            state[\'train_loss\'],\n            state[\'test_loss\'],\n            100 - 100. * state[\'test_accuracy\'],\n        ))\n\n    # # print state with rounded decimals\n    # print({k: round(v, 4) if isinstance(v, float) else v for k, v in state.items()})\n\n    print(\'Epoch {0:3d} | Time {1:5d} | Train Loss {2:.4f} | Test Loss {3:.3f} | Test Error {4:.2f}\'.format(\n        (epoch + 1),\n        int(time.time() - begin_epoch),\n        state[\'train_loss\'],\n        state[\'test_loss\'],\n        100 - 100. * state[\'test_accuracy\'])\n    )\n'"
SVHN/test.py,49,"b'import numpy as np\nimport sys\nimport os\nimport pickle\nimport argparse\nimport torch\nimport torch.nn as nn\nimport torch.backends.cudnn as cudnn\nimport torchvision.transforms as trn\nimport torchvision.datasets as dset\nimport torch.nn.functional as F\nfrom models.allconv import AllConvNet\nfrom models.wrn import WideResNet\nfrom skimage.filters import gaussian as gblur\nfrom PIL import Image as PILImage\n\n# go through rigamaroo to do ...utils.display_results import show_performance\nif __package__ is None:\n    import sys\n    from os import path\n\n    sys.path.append(path.dirname(path.dirname(path.abspath(__file__))))\n    from utils.display_results import show_performance, get_measures, print_measures, print_measures_with_std\n    import utils.svhn_loader as svhn\n    import utils.lsun_loader as lsun_loader\n\nparser = argparse.ArgumentParser(description=\'Evaluates a SVHN OOD Detector\',\n                                 formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n# Setup\nparser.add_argument(\'--test_bs\', type=int, default=200)\nparser.add_argument(\'--num_to_avg\', type=int, default=1, help=\'Average measures across num_to_avg runs.\')\nparser.add_argument(\'--validate\', \'-v\', action=\'store_true\', help=\'Evaluate performance on validation distributions.\')\nparser.add_argument(\'--use_xent\', \'-x\', action=\'store_true\', help=\'Use cross entropy scoring instead of the MSP.\')\nparser.add_argument(\'--method_name\', \'-m\', type=str, default=\'allconv_baseline\', help=\'Method name.\')\n# Loading details\nparser.add_argument(\'--layers\', default=16, type=int, help=\'total number of layers\')\nparser.add_argument(\'--widen-factor\', default=4, type=int, help=\'widen factor\')\nparser.add_argument(\'--droprate\', default=0.4, type=float, help=\'dropout probability\')\nparser.add_argument(\'--load\', \'-l\', type=str, default=\'./snapshots\', help=\'Checkpoint path to resume / test.\')\nparser.add_argument(\'--ngpu\', type=int, default=1, help=\'0 = CPU.\')\nparser.add_argument(\'--prefetch\', type=int, default=2, help=\'Pre-fetching threads.\')\nargs = parser.parse_args()\n\n# torch.manual_seed(1)\n# np.random.seed(1)\n\ntest_data = svhn.SVHN(\'/share/data/vision-greg/svhn/\', split=\'test\',\n                      transform=trn.ToTensor(), download=False)\nnum_classes = 10\n\ntest_loader = torch.utils.data.DataLoader(\n    test_data, batch_size=args.test_bs, shuffle=False,\n    num_workers=args.prefetch, pin_memory=True)\n\n# Create model\nif \'allconv\' in args.method_name:\n    net = AllConvNet(num_classes)\nelse:\n    net = WideResNet(args.layers, num_classes, args.widen_factor, dropRate=args.droprate)\n\nstart_epoch = 0\n\n# Restore model\nif args.load != \'\':\n    for i in range(300 - 1, -1, -1):\n        if \'baseline\' in args.method_name:\n            subdir = \'baseline\'\n        elif \'oe_tune\' in args.method_name:\n            subdir = \'oe_tune\'\n        else:\n            subdir = \'oe_scratch\'\n\n        model_name = os.path.join(os.path.join(args.load, subdir), args.method_name + \'_epoch_\' + str(i) + \'.pt\')\n        if os.path.isfile(model_name):\n            net.load_state_dict(torch.load(model_name))\n            print(\'Model restored! Epoch:\', i)\n            start_epoch = i + 1\n            break\n    if start_epoch == 0:\n        assert False, ""could not resume""\n\nnet.eval()\n\nif args.ngpu > 1:\n    net = torch.nn.DataParallel(net, device_ids=list(range(args.ngpu)))\n\nif args.ngpu > 0:\n    net.cuda()\n    # torch.cuda.manual_seed(1)\n\ncudnn.benchmark = True  # fire on all cylinders\n\n# /////////////// Detection Prelims ///////////////\n\nood_num_examples = test_data.data.shape[0] // 5\nexpected_ap = ood_num_examples / (ood_num_examples + test_data.data.shape[0])\n\nconcat = lambda x: np.concatenate(x, axis=0)\nto_np = lambda x: x.data.cpu().numpy()\n\n\ndef get_ood_scores(loader, in_dist=False):\n    _score = []\n    _right_score = []\n    _wrong_score = []\n\n    with torch.no_grad():\n        for batch_idx, (data, target) in enumerate(loader):\n            if batch_idx >= ood_num_examples // args.test_bs and in_dist is False:\n                break\n\n            data = data.cuda()\n\n            output = net(data)\n            smax = to_np(F.softmax(output, dim=1))\n\n            if args.use_xent:\n                _score.append(to_np((output.mean(1) - torch.logsumexp(output, dim=1))))\n            else:\n                _score.append(-np.max(smax, axis=1))\n\n            if in_dist:\n                preds = np.argmax(smax, axis=1)\n                targets = target.numpy().squeeze()\n                right_indices = preds == targets\n                wrong_indices = np.invert(right_indices)\n\n                if args.use_xent:\n                    _right_score.append(to_np((output.mean(1) - torch.logsumexp(output, dim=1)))[right_indices])\n                    _wrong_score.append(to_np((output.mean(1) - torch.logsumexp(output, dim=1)))[wrong_indices])\n                else:\n                    _right_score.append(-np.max(smax[right_indices], axis=1))\n                    _wrong_score.append(-np.max(smax[wrong_indices], axis=1))\n    \n    if in_dist:\n        return concat(_score).copy(), concat(_right_score).copy(), concat(_wrong_score).copy()\n    else:\n        return concat(_score)[:ood_num_examples].copy()\n\n\nin_score, right_score, wrong_score = get_ood_scores(test_loader, in_dist=True)\n\nnum_right = len(right_score)\nnum_wrong = len(wrong_score)\nprint(\'Error Rate {:.2f}\'.format(100*num_wrong/(num_wrong + num_right)))\n\n# /////////////// End Detection Prelims ///////////////\n\nprint(\'\\nUsing SVHN as typical data\')\n\n# /////////////// Error Detection ///////////////\n\nprint(\'\\n\\nError Detection\')\nshow_performance(wrong_score, right_score, method_name=args.method_name)\n\n# /////////////// OOD Detection ///////////////\n\nauroc_list, aupr_list, fpr_list = [], [], []\n\n\ndef get_and_print_results(ood_loader, num_to_avg=args.num_to_avg):\n\n    aurocs, auprs, fprs = [], [], []\n    for _ in range(num_to_avg):\n        out_score = get_ood_scores(ood_loader)\n        measures = get_measures(out_score, in_score)\n        aurocs.append(measures[0]); auprs.append(measures[1]); fprs.append(measures[2])\n\n    auroc = np.mean(aurocs); aupr = np.mean(auprs); fpr = np.mean(fprs)\n    auroc_list.append(auroc); aupr_list.append(aupr); fpr_list.append(fpr)\n\n    if num_to_avg >= 5:\n        print_measures_with_std(aurocs, auprs, fprs, args.method_name)\n    else:\n        print_measures(auroc, aupr, fpr, args.method_name)\n\n\n# /////////////// Gaussian Noise ///////////////\n\ndummy_targets = torch.ones(ood_num_examples*args.num_to_avg)\nood_data = torch.from_numpy(\n    np.clip(np.random.normal(size=(ood_num_examples*args.num_to_avg, 3, 32, 32),\n                             loc=0.5, scale=0.5).astype(np.float32), 0, 1))\nood_data = torch.utils.data.TensorDataset(ood_data, dummy_targets)\nood_loader = torch.utils.data.DataLoader(ood_data, batch_size=args.test_bs, shuffle=True)\n\nprint(\'\\n\\nGaussian Noise (mu = sigma = 0.5) Detection\')\nget_and_print_results(ood_loader)\n\n# /////////////// Bernoulli Noise ///////////////\n\ndummy_targets = torch.ones(ood_num_examples*args.num_to_avg)\nood_data = torch.from_numpy(np.random.binomial(\n    n=1, p=0.5, size=(ood_num_examples*args.num_to_avg, 3, 32, 32)).astype(np.float32))\nood_data = torch.utils.data.TensorDataset(ood_data, dummy_targets)\nood_loader = torch.utils.data.DataLoader(ood_data, batch_size=args.test_bs, shuffle=True)\n\nprint(\'\\n\\nBernoulli Noise Detection\')\nget_and_print_results(ood_loader)\n\n# /////////////// Blob ///////////////\n\nood_data = np.float32(np.random.binomial(n=1, p=0.7, size=(ood_num_examples*args.num_to_avg, 32, 32, 3)))\nfor i in range(ood_num_examples*args.num_to_avg):\n    ood_data[i] = gblur(ood_data[i], sigma=1.5, multichannel=False)\n    ood_data[i][ood_data[i] < 0.75] = 0.0\n\ndummy_targets = torch.ones(ood_num_examples*args.num_to_avg)\nood_data = torch.from_numpy(ood_data.transpose((0,3,1,2)))\nood_data = torch.utils.data.TensorDataset(ood_data, dummy_targets)\nood_loader = torch.utils.data.DataLoader(ood_data, batch_size=args.test_bs, shuffle=True,\n                                         num_workers=args.prefetch, pin_memory=True)\n\nprint(\'\\n\\nBlob Detection\')\nget_and_print_results(ood_loader)\n\n# /////////////// Icons-50 ///////////////\n\nood_data = dset.ImageFolder(\'/share/data/vision-greg/DistortedImageNet/Icons-50\',\n                            transform=trn.Compose([trn.Resize((32, 32)), trn.ToTensor()]))\n\nfiltered_imgs = []\nfor img in ood_data.imgs:\n    if \'numbers\' not in img[0]:     # img[0] is image name\n        filtered_imgs.append(img)\nood_data.imgs = filtered_imgs\n\nood_loader = torch.utils.data.DataLoader(ood_data, batch_size=args.test_bs, shuffle=True)\n\n\nprint(\'\\n\\nIcons-50 Detection\')\nget_and_print_results(ood_loader)\n\n# /////////////// Textures ///////////////\n\nood_data = dset.ImageFolder(root=""/share/data/vision-greg2/users/dan/datasets/dtd/images"",\n                            transform=trn.Compose([trn.Resize(32), trn.CenterCrop(32), trn.ToTensor()]))\nood_loader = torch.utils.data.DataLoader(ood_data, batch_size=args.test_bs, shuffle=True,\n                                         num_workers=args.prefetch, pin_memory=True)\n\nprint(\'\\n\\nTexture Detection\')\nget_and_print_results(ood_loader)\n\n# /////////////// Places365 ///////////////\n\nood_data = dset.ImageFolder(root=""/share/data/vision-greg2/places365/test_subset"",\n                            transform=trn.Compose([trn.Resize(32), trn.CenterCrop(32), trn.ToTensor()]))\nood_loader = torch.utils.data.DataLoader(ood_data, batch_size=args.test_bs, shuffle=True,\n                                         num_workers=args.prefetch, pin_memory=True)\n\nprint(\'\\n\\nPlaces365 Detection\')\nget_and_print_results(ood_loader)\n\n# /////////////// LSUN ///////////////\n\nood_data = lsun_loader.LSUN(""/share/data/vision-greg2/users/dan/datasets/LSUN/lsun-master/data"", classes=\'test\',\n                            transform=trn.Compose([trn.Resize(32), trn.CenterCrop(32), trn.ToTensor()]))\nood_loader = torch.utils.data.DataLoader(ood_data, batch_size=args.test_bs, shuffle=True,\n                                         num_workers=args.prefetch, pin_memory=True)\n\nprint(\'\\n\\nLSUN Detection\')\nget_and_print_results(ood_loader)\n\n# /////////////// CIFAR data ///////////////\n\nood_data = dset.CIFAR10(\'/share/data/vision-greg/cifarpy\', train=False, transform=trn.ToTensor(), download=False)\nood_loader = torch.utils.data.DataLoader(ood_data, batch_size=args.test_bs, shuffle=True,\n                                         num_workers=args.prefetch, pin_memory=True)\n\nprint(\'\\n\\nCIFAR-10 Detection\')\nget_and_print_results(ood_loader)\n\n# /////////////// Street View Characters data ///////////////\n\nood_data = dset.ImageFolder(root=""/share/data/vision-greg2/users/dan/datasets/StreetLetters"",\n                            transform=trn.Compose([trn.Resize(32), trn.CenterCrop(32), trn.ToTensor()]))\nood_loader = torch.utils.data.DataLoader(ood_data, batch_size=args.test_bs, shuffle=True,\n                                         num_workers=args.prefetch, pin_memory=True)\n\nprint(\'\\n\\nStreet View Characters Detection\')\nget_and_print_results(ood_loader)\n\n# /////////////// Mean Results ///////////////\n\nprint(\'\\n\\nMean Test Results\')\nprint_measures(np.mean(auroc_list), np.mean(aupr_list), np.mean(fpr_list), method_name=args.method_name)\n\n\n# /////////////// OOD Detection of Validation Distributions ///////////////\n\nif args.validate is False:\n    exit()\n\nauroc_list, aupr_list, fpr_list = [], [], []\n\n# /////////////// Uniform Noise ///////////////\n\ndummy_targets = torch.ones(ood_num_examples*args.num_to_avg)\nood_data = torch.from_numpy(\n    np.random.uniform(size=(ood_num_examples*args.num_to_avg, 3, 32, 32),\n                      low=0.0, high=1.0).astype(np.float32))\nood_data = torch.utils.data.TensorDataset(ood_data, dummy_targets)\nood_loader = torch.utils.data.DataLoader(ood_data, batch_size=args.test_bs, shuffle=True)\n\nprint(\'\\n\\nUniform[0,1] Noise Detection\')\nget_and_print_results(ood_loader)\n\n# /////////////// Arithmetic Mean of Images ///////////////\n\n\nclass AvgOfPair(torch.utils.data.Dataset):\n    def __init__(self, dataset):\n        self.dataset = dataset\n        self.shuffle_indices = np.arange(len(dataset))\n        np.random.shuffle(self.shuffle_indices)\n\n    def __getitem__(self, i):\n        random_idx = np.random.choice(len(self.dataset))\n        while random_idx == i:\n            random_idx = np.random.choice(len(self.dataset))\n\n        return self.dataset[i][0]/2. + self.dataset[random_idx][0]/2., 0\n\n    def __len__(self):\n        return len(self.dataset)\n\n\nood_loader = torch.utils.data.DataLoader(\n    AvgOfPair(test_data), batch_size=args.test_bs, shuffle=True,\n    num_workers=args.prefetch, pin_memory=True)\n\nprint(\'\\n\\nArithmetic Mean of Random Image Pair Detection\')\nget_and_print_results(ood_loader)\n\n# /////////////// Geometric Mean of Images ///////////////\n\n\nclass GeomMeanOfPair(torch.utils.data.Dataset):\n    def __init__(self, dataset):\n        self.dataset = dataset\n        self.shuffle_indices = np.arange(len(dataset))\n        np.random.shuffle(self.shuffle_indices)\n\n    def __getitem__(self, i):\n        random_idx = np.random.choice(len(self.dataset))\n        while random_idx == i:\n            random_idx = np.random.choice(len(self.dataset))\n\n        return torch.sqrt(self.dataset[i][0] * self.dataset[random_idx][0]), 0\n\n    def __len__(self):\n        return len(self.dataset)\n\n\nood_loader = torch.utils.data.DataLoader(\n    GeomMeanOfPair(test_data), batch_size=args.test_bs, shuffle=True,\n    num_workers=args.prefetch, pin_memory=True)\n\nprint(\'\\n\\nGeometric Mean of Random Image Pair Detection\')\nget_and_print_results(ood_loader)\n\n# /////////////// Jigsaw Images ///////////////\n\nood_loader = torch.utils.data.DataLoader(test_data, batch_size=args.test_bs, shuffle=True,\n                                         num_workers=args.prefetch, pin_memory=True)\n\njigsaw = lambda x: torch.cat((\n    torch.cat((torch.cat((x[:, 8:16, :16], x[:, :8, :16]), 1),\n               x[:, 16:, :16]), 2),\n    torch.cat((x[:, 16:, 16:],\n               torch.cat((x[:, :16, 24:], x[:, :16, 16:24]), 2)), 2),\n), 1)\n\nood_loader.dataset.transform = trn.Compose([trn.ToTensor(), jigsaw])\n\nprint(\'\\n\\nJigsawed Images Detection\')\nget_and_print_results(ood_loader)\n\n# /////////////// Speckled Images ///////////////\n\nspeckle = lambda x: torch.clamp(x + x * torch.randn_like(x), 0, 1)\nood_loader.dataset.transform = trn.Compose([trn.ToTensor(), speckle])\n\nprint(\'\\n\\nSpeckle Noised Images Detection\')\nget_and_print_results(ood_loader)\n\n# /////////////// Pixelated Images ///////////////\n\npixelate = lambda x: x.resize((int(32 * 0.2), int(32 * 0.2)), PILImage.BOX).resize((32, 32), PILImage.BOX)\nood_loader.dataset.transform = trn.Compose([pixelate, trn.ToTensor()])\n\nprint(\'\\n\\nPixelate Detection\')\nget_and_print_results(ood_loader)\n\n# /////////////// Mirrored SVHN digits ///////////////\n\nidxs = test_data.targets\nvert_idxs = np.squeeze(np.logical_and(idxs != 3, np.logical_and(idxs != 0, np.logical_and(idxs != 1, idxs != 8))))\nvert_digits = test_data.data[vert_idxs][:, :, ::-1, :]\n\nhoriz_idxs = np.squeeze(np.logical_and(idxs != 0, np.logical_and(idxs != 1, idxs != 8)))\nhoriz_digits = test_data.data[horiz_idxs][:, :, :, ::-1]\n\nflipped_digits = concat((vert_digits, horiz_digits))\n\ndummy_targets = torch.ones(flipped_digits.shape[0])\nood_data = torch.from_numpy(flipped_digits.astype(np.float32) / 255)\nood_data = torch.utils.data.TensorDataset(ood_data, dummy_targets)\nood_loader = torch.utils.data.DataLoader(ood_data, batch_size=args.test_bs, shuffle=True,\n                                         num_workers=args.prefetch)\n\nprint(\'\\n\\nMirrored SVHN Digit Detection\')\nget_and_print_results(ood_loader)\n\n# /////////////// Mean Results ///////////////\n\nprint(\'\\n\\nMean Validation Results\')\nprint_measures(np.mean(auroc_list), np.mean(aupr_list), np.mean(fpr_list), method_name=args.method_name)\n'"
SVHN/test_calibration.py,47,"b'import numpy as np\nimport sys\nimport os\nimport pickle\nimport argparse\nimport torch\nimport torch.nn as nn\nimport torch.backends.cudnn as cudnn\nimport torchvision.transforms as trn\nimport torchvision.datasets as dset\nimport torch.nn.functional as F\nfrom models.allconv import AllConvNet\nfrom models.wrn import WideResNet\nfrom skimage.filters import gaussian as gblur\nfrom PIL import Image as PILImage\n\n# go through rigamaroo to do ...utils.display_results import show_performance\nif __package__ is None:\n    import sys\n    from os import path\n\n    sys.path.append(path.dirname(path.dirname(path.abspath(__file__))))\n    from utils.display_results import show_performance, get_measures, print_measures, print_measures_with_std\n    import utils.svhn_loader as svhn\n    import utils.lsun_loader as lsun_loader\n    from utils.validation_dataset import validation_split\n    from utils.calibration_tools import *\n\nparser = argparse.ArgumentParser(description=\'Evaluates a SVHN OOD Detector\',\n                                 formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n# Setup\nparser.add_argument(\'--test_bs\', type=int, default=200)\nparser.add_argument(\'--num_to_avg\', type=int, default=1, help=\'Average measures across num_to_avg runs.\')\nparser.add_argument(\'--validate\', \'-v\', action=\'store_true\', help=\'Evaluate performance on validation distributions.\')\nparser.add_argument(\'--method_name\', \'-m\', type=str, default=\'allconv_calib_baseline\', help=\'Method name.\')\nparser.add_argument(\'--use_01\', \'-z\', action=\'store_true\', help=\'Use 0-1 Posterior Rescaling.\')\n# Loading details\nparser.add_argument(\'--layers\', default=16, type=int, help=\'total number of layers\')\nparser.add_argument(\'--widen-factor\', default=4, type=int, help=\'widen factor\')\nparser.add_argument(\'--droprate\', default=0.4, type=float, help=\'dropout probability\')\nparser.add_argument(\'--load\', \'-l\', type=str, default=\'./snapshots\', help=\'Checkpoint path to resume / test.\')\nparser.add_argument(\'--ngpu\', type=int, default=1, help=\'0 = CPU.\')\nparser.add_argument(\'--prefetch\', type=int, default=2, help=\'Pre-fetching threads.\')\nargs = parser.parse_args()\n\n# torch.manual_seed(1)\n# np.random.seed(1)\n\ntrain_data = svhn.SVHN(\'/share/data/vision-greg/svhn/\', split=\'train_and_extra\',\n                       transform=trn.ToTensor(), download=False)\ntest_data = svhn.SVHN(\'/share/data/vision-greg/svhn/\', split=\'test\',\n                      transform=trn.ToTensor(), download=False)\nnum_classes = 10\n\ntrain_data, val_data = validation_split(train_data, val_share=5000/604388.)\n\nval_loader = torch.utils.data.DataLoader(\n    val_data, batch_size=args.test_bs, shuffle=False,\n    num_workers=args.prefetch, pin_memory=True)\ntest_loader = torch.utils.data.DataLoader(\n    test_data, batch_size=args.test_bs, shuffle=False,\n    num_workers=args.prefetch, pin_memory=True)\n\n# Create model\nif \'allconv\' in args.method_name:\n    net = AllConvNet(num_classes)\nelse:\n    net = WideResNet(args.layers, num_classes, args.widen_factor, dropRate=args.droprate)\n\nstart_epoch = 0\n\n# Restore model\nif args.load != \'\':\n    for i in range(300 - 1, -1, -1):\n        if \'baseline\' in args.method_name:\n            subdir = \'baseline\'\n        elif \'oe_tune\' in args.method_name:\n            subdir = \'oe_tune\'\n        else:\n            subdir = \'oe_scratch\'\n\n        model_name = os.path.join(os.path.join(args.load, subdir), args.method_name + \'_epoch_\' + str(i) + \'.pt\')\n        if os.path.isfile(model_name):\n            net.load_state_dict(torch.load(model_name))\n            print(\'Model restored! Epoch:\', i)\n            start_epoch = i + 1\n            break\n    if start_epoch == 0:\n        assert False, ""could not resume""\n\nnet.eval()\n\nif args.ngpu > 1:\n    net = torch.nn.DataParallel(net, device_ids=list(range(args.ngpu)))\n\nif args.ngpu > 0:\n    net.cuda()\n    # torch.cuda.manual_seed(1)\n\ncudnn.benchmark = True  # fire on all cylinders\n\n# /////////////// Calibration Prelims ///////////////\n\nood_num_examples = test_data.data.shape[0] // 5\nexpected_ap = ood_num_examples / (ood_num_examples + test_data.data.shape[0])\n\nconcat = lambda x: np.concatenate(x, axis=0)\nto_np = lambda x: x.data.cpu().numpy()\n\n\ndef get_net_results(data_loader, in_dist=False, t=1):\n    logits = []\n    confidence = []\n    correct = []\n\n    with torch.no_grad():\n        for batch_idx, (data, target) in enumerate(data_loader):\n            if batch_idx >= ood_num_examples // args.test_bs and in_dist is False:\n                break\n            data, target = data.cuda(), target.cuda().long()\n\n            output = net(data)\n\n            logits.extend(to_np(output).squeeze())\n\n            if args.use_01:\n                confidence.extend(to_np(\n                    (F.softmax(output/t, dim=1).max(1)[0] - 1./num_classes)/(1 - 1./num_classes)\n                ).squeeze().tolist())\n            else:\n                confidence.extend(to_np(F.softmax(output/t, dim=1).max(1)[0]).squeeze().tolist())\n\n            if in_dist:\n                pred = output.data.max(1)[1]\n                correct.extend(pred.eq(target).cpu().numpy().squeeze().tolist())\n\n    if in_dist:\n        return logits.copy(), confidence.copy(), correct.copy()\n    else:\n        return logits[:ood_num_examples].copy(), confidence[:ood_num_examples].copy()\n\n\nval_logits, val_confidence, val_correct = get_net_results(val_loader, in_dist=True)\n\nprint(\'\\nTuning Softmax Temperature\')\nval_labels = val_data.parent_ds.targets[val_data.offset:]\nt_star = tune_temp(val_logits, val_labels)\nprint(\'Softmax Temperature Tuned. Temperature is {:.3f}\'.format(t_star))\n\ntest_logits, test_confidence, test_correct = get_net_results(test_loader, in_dist=True, t=t_star)\n\nprint(\'Error Rate {:.2f}\'.format(100*(len(test_correct) - sum(test_correct))/len(test_correct)))\n\n# /////////////// End Calibration Prelims ///////////////\n\nprint(\'\\nUsing SVHN as typical data\')\n\n# /////////////// In-Distribution Calibration ///////////////\n\nprint(\'\\n\\nIn-Distribution Data\')\nshow_calibration_results(np.array(test_confidence), np.array(test_correct), method_name=args.method_name)\n\n# /////////////// OOD Calibration ///////////////\n\nrms_list, mad_list, sf1_list = [], [], []\n\n\ndef get_and_print_results(ood_loader, num_to_avg=args.num_to_avg):\n\n    rmss, mads, sf1s = [], [], []\n    for _ in range(num_to_avg):\n        out_logits, out_confidence = get_net_results(ood_loader, t=t_star)\n\n        measures = get_measures(\n            concat([out_confidence, test_confidence]),\n            concat([np.zeros(len(out_confidence)), test_correct]))\n\n        rmss.append(measures[0]); mads.append(measures[1]); sf1s.append(measures[2])\n\n    rms = np.mean(rmss); mad = np.mean(mads); sf1 = np.mean(sf1s)\n    rms_list.append(rms); mad_list.append(mad); sf1_list.append(sf1)\n\n    if num_to_avg >= 5:\n        print_measures_with_std(rmss, mads, sf1s, args.method_name)\n    else:\n        print_measures(rms, mad, sf1, args.method_name)\n\n\n# /////////////// Gaussian Noise ///////////////\n\ndummy_targets = torch.ones(ood_num_examples * args.num_to_avg)\nood_data = torch.from_numpy(\n    np.clip(np.random.normal(size=(ood_num_examples * args.num_to_avg, 3, 32, 32),\n                             loc=0.5, scale=0.5).astype(np.float32), 0, 1))\nood_data = torch.utils.data.TensorDataset(ood_data, dummy_targets)\nood_loader = torch.utils.data.DataLoader(ood_data, batch_size=args.test_bs, shuffle=True)\n\nprint(\'\\n\\nGaussian Noise (mu = sigma = 0.5) Calibration\')\nget_and_print_results(ood_loader)\n\n# /////////////// Bernoulli Noise ///////////////\n\ndummy_targets = torch.ones(ood_num_examples * args.num_to_avg)\nood_data = torch.from_numpy(np.random.binomial(\n    n=1, p=0.5, size=(ood_num_examples * args.num_to_avg, 3, 32, 32)).astype(np.float32))\nood_data = torch.utils.data.TensorDataset(ood_data, dummy_targets)\nood_loader = torch.utils.data.DataLoader(ood_data, batch_size=args.test_bs, shuffle=True)\n\nprint(\'\\n\\nBernoulli Noise Calibration\')\nget_and_print_results(ood_loader)\n\n# /////////////// Blob ///////////////\n\nood_data = np.float32(np.random.binomial(n=1, p=0.7, size=(ood_num_examples * args.num_to_avg, 32, 32, 3)))\nfor i in range(ood_num_examples * args.num_to_avg):\n    ood_data[i] = gblur(ood_data[i], sigma=1.5, multichannel=False)\n    ood_data[i][ood_data[i] < 0.75] = 0.0\n\ndummy_targets = torch.ones(ood_num_examples * args.num_to_avg)\nood_data = torch.from_numpy(ood_data.transpose((0, 3, 1, 2)))\nood_data = torch.utils.data.TensorDataset(ood_data, dummy_targets)\nood_loader = torch.utils.data.DataLoader(ood_data, batch_size=args.test_bs, shuffle=True,\n                                         num_workers=args.prefetch, pin_memory=True)\n\nprint(\'\\n\\nBlob Calibration\')\nget_and_print_results(ood_loader)\n\n# /////////////// Icons-50 ///////////////\n\nood_data = dset.ImageFolder(\'/share/data/vision-greg/DistortedImageNet/Icons-50\',\n                            transform=trn.Compose([trn.Resize((32, 32)), trn.ToTensor()]))\n\nfiltered_imgs = []\nfor img in ood_data.imgs:\n    if \'numbers\' not in img[0]:  # img[0] is image name\n        filtered_imgs.append(img)\nood_data.imgs = filtered_imgs\n\nood_loader = torch.utils.data.DataLoader(ood_data, batch_size=args.test_bs, shuffle=True)\n\nprint(\'\\n\\nIcons-50 Calibration\')\nget_and_print_results(ood_loader)\n\n# /////////////// Textures ///////////////\n\nood_data = dset.ImageFolder(root=""/share/data/vision-greg2/users/dan/datasets/dtd/images"",\n                            transform=trn.Compose([trn.Resize(32), trn.CenterCrop(32), trn.ToTensor()]))\nood_loader = torch.utils.data.DataLoader(ood_data, batch_size=args.test_bs, shuffle=True,\n                                         num_workers=args.prefetch, pin_memory=True)\n\nprint(\'\\n\\nTexture Calibration\')\nget_and_print_results(ood_loader)\n\n# /////////////// Places365 ///////////////\n\nood_data = dset.ImageFolder(root=""/share/data/vision-greg2/places365/test_subset"",\n                            transform=trn.Compose([trn.Resize(32), trn.CenterCrop(32), trn.ToTensor()]))\nood_loader = torch.utils.data.DataLoader(ood_data, batch_size=args.test_bs, shuffle=True,\n                                         num_workers=args.prefetch, pin_memory=True)\n\nprint(\'\\n\\nPlaces365 Calibration\')\nget_and_print_results(ood_loader)\n\n# /////////////// LSUN ///////////////\n\nood_data = lsun_loader.LSUN(""/share/data/vision-greg2/users/dan/datasets/LSUN/lsun-master/data"", classes=\'test\',\n                            transform=trn.Compose([trn.Resize(32), trn.CenterCrop(32), trn.ToTensor()]))\nood_loader = torch.utils.data.DataLoader(ood_data, batch_size=args.test_bs, shuffle=True,\n                                         num_workers=args.prefetch, pin_memory=True)\n\nprint(\'\\n\\nLSUN Calibration\')\nget_and_print_results(ood_loader)\n\n# /////////////// CIFAR data ///////////////\n\n\nood_data = dset.CIFAR10(\'/share/data/vision-greg/cifarpy\', train=False, transform=trn.ToTensor(), download=False)\nood_loader = torch.utils.data.DataLoader(ood_data, batch_size=args.test_bs, shuffle=True,\n                                         num_workers=args.prefetch, pin_memory=True)\n\nprint(\'\\n\\nCIFAR-10 Calibration\')\nget_and_print_results(ood_loader)\n\n# /////////////// Street View Characters data ///////////////\n\nood_data = dset.ImageFolder(root=""/share/data/vision-greg2/users/dan/datasets/StreetLetters"",\n                            transform=trn.Compose([trn.Resize(32), trn.CenterCrop(32), trn.ToTensor()]))\nood_loader = torch.utils.data.DataLoader(ood_data, batch_size=args.test_bs, shuffle=True,\n                                         num_workers=args.prefetch, pin_memory=True)\n\nprint(\'\\n\\nStreet View Characters Calibration\')\nget_and_print_results(ood_loader)\n\n# /////////////// Mean Results ///////////////\n\nprint(\'\\n\\nMean Test Results\')\nprint_measures(np.mean(rms_list), np.mean(mad_list), np.mean(sf1_list), method_name=args.method_name)\n\n# /////////////// OOD Detection of Validation Distributions ///////////////\n\nif args.validate is False:\n    exit()\n\nrms_list, mad_list, sf1_list = [], [], []\n\n# /////////////// Uniform Noise ///////////////\n\ndummy_targets = torch.ones(ood_num_examples * args.num_to_avg)\nood_data = torch.from_numpy(\n    np.random.uniform(size=(ood_num_examples * args.num_to_avg, 3, 32, 32),\n                      low=0.0, high=1.0).astype(np.float32))\nood_data = torch.utils.data.TensorDataset(ood_data, dummy_targets)\nood_loader = torch.utils.data.DataLoader(ood_data, batch_size=args.test_bs, shuffle=True)\n\nprint(\'\\n\\nUniform[0,1] Noise Calibration\')\nget_and_print_results(ood_loader)\n\n# /////////////// Arithmetic Mean of Images ///////////////\n\n\nclass AvgOfPair(torch.utils.data.Dataset):\n    def __init__(self, dataset):\n        self.dataset = dataset\n        self.shuffle_indices = np.arange(len(dataset))\n        np.random.shuffle(self.shuffle_indices)\n\n    def __getitem__(self, i):\n        random_idx = np.random.choice(len(self.dataset))\n        while random_idx == i:\n            random_idx = np.random.choice(len(self.dataset))\n\n        return self.dataset[i][0] / 2. + self.dataset[random_idx][0] / 2., 0\n\n    def __len__(self):\n        return len(self.dataset)\n\n\nood_loader = torch.utils.data.DataLoader(\n    AvgOfPair(test_data), batch_size=args.test_bs, shuffle=True,\n    num_workers=args.prefetch, pin_memory=True)\n\nprint(\'\\n\\nArithmetic Mean of Random Image Pair Calibration\')\nget_and_print_results(ood_loader)\n\n\n# /////////////// Geometric Mean of Images ///////////////\n\n\nclass GeomMeanOfPair(torch.utils.data.Dataset):\n    def __init__(self, dataset):\n        self.dataset = dataset\n        self.shuffle_indices = np.arange(len(dataset))\n        np.random.shuffle(self.shuffle_indices)\n\n    def __getitem__(self, i):\n        random_idx = np.random.choice(len(self.dataset))\n        while random_idx == i:\n            random_idx = np.random.choice(len(self.dataset))\n\n        return torch.sqrt(self.dataset[i][0] * self.dataset[random_idx][0]), 0\n\n    def __len__(self):\n        return len(self.dataset)\n\n\nood_loader = torch.utils.data.DataLoader(\n    GeomMeanOfPair(test_data), batch_size=args.test_bs, shuffle=True,\n    num_workers=args.prefetch, pin_memory=True)\n\nprint(\'\\n\\nGeometric Mean of Random Image Pair Calibration\')\nget_and_print_results(ood_loader)\n\n# /////////////// Jigsaw Images ///////////////\n\nood_loader = torch.utils.data.DataLoader(test_data, batch_size=args.test_bs, shuffle=True,\n                                         num_workers=args.prefetch, pin_memory=True)\n\njigsaw = lambda x: torch.cat((\n    torch.cat((torch.cat((x[:, 8:16, :16], x[:, :8, :16]), 1),\n               x[:, 16:, :16]), 2),\n    torch.cat((x[:, 16:, 16:],\n               torch.cat((x[:, :16, 24:], x[:, :16, 16:24]), 2)), 2),\n), 1)\n\nood_loader.dataset.transform = trn.Compose([trn.ToTensor(), jigsaw])\n\nprint(\'\\n\\nJigsawed Images Calibration\')\nget_and_print_results(ood_loader)\n\n# /////////////// Speckled Images ///////////////\n\nspeckle = lambda x: torch.clamp(x + x * torch.randn_like(x), 0, 1)\nood_loader.dataset.transform = trn.Compose([trn.ToTensor(), speckle])\n\nprint(\'\\n\\nSpeckle Noised Images Calibration\')\nget_and_print_results(ood_loader)\n\n# /////////////// Pixelated Images ///////////////\n\npixelate = lambda x: x.resize((int(32 * 0.2), int(32 * 0.2)), PILImage.BOX).resize((32, 32), PILImage.BOX)\nood_loader.dataset.transform = trn.Compose([pixelate, trn.ToTensor()])\n\nprint(\'\\n\\nPixelate Calibration\')\nget_and_print_results(ood_loader)\n\n# /////////////// Mirrored SVHN digits ///////////////\n\nidxs = test_data.targets\nvert_idxs = np.squeeze(np.logical_and(idxs != 3, np.logical_and(idxs != 0, np.logical_and(idxs != 1, idxs != 8))))\nvert_digits = test_data.data[vert_idxs][:, :, ::-1, :]\n\nhoriz_idxs = np.squeeze(np.logical_and(idxs != 0, np.logical_and(idxs != 1, idxs != 8)))\nhoriz_digits = test_data.data[horiz_idxs][:, :, :, ::-1]\n\nflipped_digits = concat((vert_digits, horiz_digits))\n\ndummy_targets = torch.ones(flipped_digits.shape[0])\nood_data = torch.from_numpy(flipped_digits.astype(np.float32) / 255)\nood_data = torch.utils.data.TensorDataset(ood_data, dummy_targets)\nood_loader = torch.utils.data.DataLoader(ood_data, batch_size=args.test_bs, shuffle=True,\n                                         num_workers=args.prefetch)\n\nprint(\'\\n\\nMirrored SVHN Digit Calibration\')\nget_and_print_results(ood_loader)\n\n# /////////////// Mean Results ///////////////\n\nprint(\'\\n\\nMean Validation Results\')\nprint_measures(np.mean(rms_list), np.mean(mad_list), np.mean(sf1_list), method_name=args.method_name)\n'"
TinyImageNet/baseline.py,13,"b'# -*- coding: utf-8 -*-\nimport numpy as np\nimport os\nimport argparse\nimport time\nimport torch\nimport torch.nn as nn\nimport torch.backends.cudnn as cudnn\nimport torchvision.transforms as trn\nimport torchvision.datasets as dset\nimport torch.nn.functional as F\nfrom tqdm import tqdm\nfrom models.allconv import AllConvNet\nfrom models.wrn import WideResNet\n\n# go through rigamaroo to do ...utils.display_results import show_performance\nif __package__ is None:\n    import sys\n    from os import path\n\n    sys.path.append(path.dirname(path.dirname(path.abspath(__file__))))\n    from utils.validation_dataset import validation_split_folder\n\nparser = argparse.ArgumentParser(description=\'Trains a Tiny ImageNet Classifier\',\n                                 formatter_class=argparse.ArgumentDefaultsHelpFormatter)\nparser.add_argument(\'--model\', \'-m\', type=str, default=\'allconv\',\n                    choices=[\'allconv\', \'wrn\'], help=\'Choose architecture.\')\nparser.add_argument(\'--calibration\', \'-c\', action=\'store_true\',\n                    help=\'Train a model to be used for calibration. This holds out some data for validation.\')\n# Optimization options\nparser.add_argument(\'--epochs\', \'-e\', type=int, default=100, help=\'Number of epochs to train.\')\nparser.add_argument(\'--learning_rate\', \'-lr\', type=float, default=0.1, help=\'The initial learning rate.\')\nparser.add_argument(\'--batch_size\', \'-b\', type=int, default=128, help=\'Batch size.\')\nparser.add_argument(\'--test_bs\', type=int, default=200)\nparser.add_argument(\'--momentum\', type=float, default=0.9, help=\'Momentum.\')\nparser.add_argument(\'--decay\', \'-d\', type=float, default=0.0005, help=\'Weight decay (L2 penalty).\')\n# WRN Architecture\nparser.add_argument(\'--layers\', default=40, type=int, help=\'total number of layers\')\nparser.add_argument(\'--widen-factor\', default=2, type=int, help=\'widen factor\')\nparser.add_argument(\'--droprate\', default=0.3, type=float, help=\'dropout probability\')\n# Checkpoints\nparser.add_argument(\'--save\', \'-s\', type=str, default=\'./snapshots/baseline\', help=\'Folder to save checkpoints.\')\nparser.add_argument(\'--load\', \'-l\', type=str, default=\'\', help=\'Checkpoint path to resume / test.\')\nparser.add_argument(\'--test\', \'-t\', action=\'store_true\', help=\'Test only flag.\')\n# Acceleration\nparser.add_argument(\'--ngpu\', type=int, default=1, help=\'0 = CPU.\')\nparser.add_argument(\'--prefetch\', type=int, default=4, help=\'Pre-fetching threads.\')\nargs = parser.parse_args()\n\nstate = {k: v for k, v in args._get_kwargs()}\nprint(state)\n\ntorch.manual_seed(1)\nnp.random.seed(1)\n\n# mean and standard deviation of channels of ImageNet images\nmean = [0.485, 0.456, 0.406]\nstd = [0.229, 0.224, 0.225]\n\ntrain_transform = trn.Compose([trn.RandomHorizontalFlip(), trn.RandomCrop(64, padding=8),\n                               trn.ToTensor(), trn.Normalize(mean, std)])\ntest_transform = trn.Compose([trn.ToTensor(), trn.Normalize(mean, std)])\n\ntrain_data = dset.ImageFolder(\n    root=""/share/data/vision-greg/tinyImageNet/tiny-imagenet-200/train"",\n    transform=train_transform)\ntest_data = dset.ImageFolder(\n    root=""/share/data/vision-greg/tinyImageNet/tiny-imagenet-200/val"",\n    transform=test_transform)\n\nnum_classes = 200\n\ncalib_indicator = \'\'\nif args.calibration:\n    train_data, val_data = validation_split_folder(train_data, val_share=0.1)\n    calib_indicator = \'calib_\'\n\ntrain_loader = torch.utils.data.DataLoader(\n    train_data, batch_size=args.batch_size, shuffle=True,\n    num_workers=args.prefetch, pin_memory=True)\ntest_loader = torch.utils.data.DataLoader(\n    test_data, batch_size=args.test_bs, shuffle=False,\n    num_workers=args.prefetch, pin_memory=True)\n\n# Create model\nif args.model == \'allconv\':\n    net = AllConvNet(num_classes)\nelse:\n    net = WideResNet(args.layers, num_classes, args.widen_factor, dropRate=args.droprate)\n\nstart_epoch = 0\n\n# Restore model if desired\nif args.load != \'\':\n    for i in range(1000 - 1, -1, -1):\n        model_name = os.path.join(args.load, calib_indicator + args.model + \'_baseline_epoch_\' + str(i) + \'.pt\')\n        if os.path.isfile(model_name):\n            net.load_state_dict(torch.load(model_name))\n            print(\'Model restored! Epoch:\', i)\n            start_epoch = i + 1\n            break\n    if start_epoch == 0:\n        assert False, ""could not resume""\n\nif args.ngpu > 1:\n    net = torch.nn.DataParallel(net, device_ids=list(range(args.ngpu)))\n\nif args.ngpu > 0:\n    net.cuda()\n    torch.cuda.manual_seed(1)\n\ncudnn.benchmark = True  # fire on all cylinders\n\noptimizer = torch.optim.SGD(\n    net.parameters(), state[\'learning_rate\'], momentum=state[\'momentum\'],\n    weight_decay=state[\'decay\'], nesterov=True)\n\n\ndef cosine_annealing(step, total_steps, lr_max, lr_min):\n    return lr_min + (lr_max - lr_min) * 0.5 * (\n            1 + np.cos(step / total_steps * np.pi))\n\n\nscheduler = torch.optim.lr_scheduler.LambdaLR(\n    optimizer,\n    lr_lambda=lambda step: cosine_annealing(\n        step,\n        args.epochs * len(train_loader),\n        1,  # since lr_lambda computes multiplicative factor\n        1e-6 / args.learning_rate))\n\n\n# /////////////// Training ///////////////\n\ndef train():\n    net.train()  # enter train mode\n    loss_avg = 0.0\n    for data, target in train_loader:\n        data, target = data.cuda(), target.cuda()\n\n        # forward\n        x = net(data)\n\n        # backward\n        scheduler.step()\n        optimizer.zero_grad()\n        loss = F.cross_entropy(x, target)\n        loss.backward()\n        optimizer.step()\n\n        # exponential moving average\n        loss_avg = loss_avg * 0.8 + float(loss) * 0.2\n\n    state[\'train_loss\'] = loss_avg\n\n\n# test function\ndef test():\n    net.eval()\n    loss_avg = 0.0\n    correct = 0\n    with torch.no_grad():\n        for data, target in test_loader:\n            data, target = data.cuda(), target.cuda()\n\n            # forward\n            output = net(data)\n            loss = F.cross_entropy(output, target)\n\n            # accuracy\n            pred = output.data.max(1)[1]\n            correct += pred.eq(target.data).sum().item()\n\n            # test loss average\n            loss_avg += float(loss.data)\n\n    state[\'test_loss\'] = loss_avg / len(test_loader)\n    state[\'test_accuracy\'] = correct / len(test_loader.dataset)\n\n\nif args.test:\n    test()\n    print(state)\n    exit()\n\n# Make save directory\nif not os.path.exists(args.save):\n    os.makedirs(args.save)\nif not os.path.isdir(args.save):\n    raise Exception(\'%s is not a dir\' % args.save)\n\nwith open(os.path.join(args.save, calib_indicator + args.model + \'_baseline_training_results.csv\'), \'w\') as f:\n    f.write(\'epoch,time(s),train_loss,test_loss,test_error(%)\\n\')\n\nprint(\'Beginning Training\\n\')\n\n# Main loop\nfor epoch in range(start_epoch, args.epochs):\n    state[\'epoch\'] = epoch\n\n    begin_epoch = time.time()\n\n    train()\n    test()\n\n    # Save model\n    torch.save(net.state_dict(),\n               os.path.join(args.save, calib_indicator + args.model + \'_baseline_epoch_\' + str(epoch) + \'.pt\'))\n    # Let us not waste space and delete the previous model\n    prev_path = os.path.join(args.save, calib_indicator + args.model + \'_baseline_epoch_\' + str(epoch - 1) + \'.pt\')\n    if os.path.exists(prev_path): os.remove(prev_path)\n\n    # Show results\n\n    with open(os.path.join(args.save, calib_indicator + args.model + \'_baseline_training_results.csv\'), \'a\') as f:\n        f.write(\'%03d,%05d,%0.6f,%0.5f,%0.2f\\n\' % (\n            (epoch + 1),\n            time.time() - begin_epoch,\n            state[\'train_loss\'],\n            state[\'test_loss\'],\n            100 - 100. * state[\'test_accuracy\'],\n        ))\n\n    # # print state with rounded decimals\n    # print({k: round(v, 4) if isinstance(v, float) else v for k, v in state.items()})\n\n    print(\'Epoch {0:3d} | Time {1:5d} | Train Loss {2:.4f} | Test Loss {3:.3f} | Test Error {4:.2f}\'.format(\n        (epoch + 1),\n        int(time.time() - begin_epoch),\n        state[\'train_loss\'],\n        state[\'test_loss\'],\n        100 - 100. * state[\'test_accuracy\'])\n    )\n'"
TinyImageNet/oe_scratch.py,16,"b'# -*- coding: utf-8 -*-\nimport numpy as np\nimport os\nimport pickle\nimport argparse\nimport time\nimport torch\nimport torch.nn as nn\nimport torch.backends.cudnn as cudnn\nimport torchvision.transforms as trn\nimport torchvision.datasets as dset\nimport torch.nn.functional as F\nfrom tqdm import tqdm\nfrom models.allconv import AllConvNet\nfrom models.wrn import WideResNet\n\nif __package__ is None:\n    import sys\n    from os import path\n\n    sys.path.append(path.dirname(path.dirname(path.abspath(__file__))))\n    from utils.tinyimages_80mn_loader import TinyImages\n    from utils.validation_dataset import validation_split_folder\n\nparser = argparse.ArgumentParser(description=\'Trains a Tiny ImageNet Classifier with OE\',\n                                 formatter_class=argparse.ArgumentDefaultsHelpFormatter)\nparser.add_argument(\'--model\', \'-m\', type=str, default=\'allconv\',\n                    choices=[\'allconv\', \'wrn\'], help=\'Choose architecture.\')\nparser.add_argument(\'--calibration\', \'-c\', action=\'store_true\',\n                    help=\'Train a model to be used for calibration. This holds out some data for validation.\')\n# Optimization options\nparser.add_argument(\'--epochs\', \'-e\', type=int, default=100, help=\'Number of epochs to train.\')\nparser.add_argument(\'--learning_rate\', \'-lr\', type=float, default=0.1, help=\'The initial learning rate.\')\nparser.add_argument(\'--batch_size\', \'-b\', type=int, default=128, help=\'Batch size.\')\nparser.add_argument(\'--oe_batch_size\', type=int, default=256, help=\'Batch size.\')\nparser.add_argument(\'--test_bs\', type=int, default=200)\nparser.add_argument(\'--momentum\', type=float, default=0.9, help=\'Momentum.\')\nparser.add_argument(\'--decay\', \'-d\', type=float, default=0.0005, help=\'Weight decay (L2 penalty).\')\n# WRN Architecture\nparser.add_argument(\'--layers\', default=40, type=int, help=\'total number of layers\')\nparser.add_argument(\'--widen-factor\', default=2, type=int, help=\'widen factor\')\nparser.add_argument(\'--droprate\', default=0.3, type=float, help=\'dropout probability\')\n# Checkpoints\nparser.add_argument(\'--save\', \'-s\', type=str, default=\'./snapshots/oe_scratch\', help=\'Folder to save checkpoints.\')\nparser.add_argument(\'--load\', \'-l\', type=str, default=\'\', help=\'Checkpoint path to resume / test.\')\nparser.add_argument(\'--test\', \'-t\', action=\'store_true\', help=\'Test only flag.\')\n# Acceleration\nparser.add_argument(\'--ngpu\', type=int, default=1, help=\'0 = CPU.\')\nparser.add_argument(\'--prefetch\', type=int, default=4, help=\'Pre-fetching threads.\')\nargs = parser.parse_args()\n\nstate = {k: v for k, v in args._get_kwargs()}\nprint(state)\n\ntorch.manual_seed(1)\nnp.random.seed(1)\n\n# mean and standard deviation of channels of ImageNet images\nmean = [0.485, 0.456, 0.406]\nstd = [0.229, 0.224, 0.225]\n\ntrain_transform = trn.Compose([trn.RandomHorizontalFlip(), trn.RandomCrop(64, padding=8),\n                               trn.ToTensor(), trn.Normalize(mean, std)])\ntest_transform = trn.Compose([trn.ToTensor(), trn.Normalize(mean, std)])\n\ntrain_data_in = dset.ImageFolder(\n    root=""/share/data/vision-greg/tinyImageNet/tiny-imagenet-200/train"",\n    transform=train_transform)\ntest_data = dset.ImageFolder(\n    root=""/share/data/vision-greg/tinyImageNet/tiny-imagenet-200/val"",\n    transform=test_transform)\n\nnum_classes = 200\n\ncalib_indicator = \'\'\nif args.calibration:\n    train_data_in, val_data = validation_split_folder(train_data_in, val_share=0.1)\n    calib_indicator = \'calib_\'\n\nprint(\'Loading ImageNet22k\')\nood_data = dset.ImageFolder(root=""/share/data/vision-greg/ImageNet/clsloc/images/val"",\n                            transform=trn.Compose([trn.Resize(64), trn.CenterCrop(64),\n                                                   trn.ToTensor(), trn.Normalize(mean, std)]))\nood_data.root = \'/share/data/vision-greg/ImageNet22k\'\nood_data.class_to_idx = pickle.load(open(ood_data.root + \'/class_to_idx.p\', ""rb""))\nood_data.classes = pickle.load(open(ood_data.root + \'/classes.p\', ""rb""))\nood_data.imgs = pickle.load(open(ood_data.root + \'/imgs.p\', ""rb""))\nprint(\'Loaded ImageNet22k\')\n\n\ntrain_loader_in = torch.utils.data.DataLoader(\n    train_data_in,\n    batch_size=args.batch_size, shuffle=True,\n    num_workers=args.prefetch, pin_memory=True)\n\ntrain_loader_out = torch.utils.data.DataLoader(\n    ood_data,\n    batch_size=args.oe_batch_size, shuffle=False,\n    num_workers=args.prefetch, pin_memory=True)\n\ntest_loader = torch.utils.data.DataLoader(\n    test_data,\n    batch_size=args.batch_size, shuffle=False,\n    num_workers=args.prefetch, pin_memory=True)\n\n# Create model\nif args.model == \'allconv\':\n    net = AllConvNet(num_classes)\nelse:\n    net = WideResNet(args.layers, num_classes, args.widen_factor, dropRate=args.droprate)\n\nstart_epoch = 0\n\n# Restore model if desired\nif args.load != \'\':\n    for i in range(1000 - 1, -1, -1):\n        model_name = os.path.join(args.load, calib_indicator + args.model + \'_oe_scratch_epoch_\' + str(i) + \'.pt\')\n        if os.path.isfile(model_name):\n            net.load_state_dict(torch.load(model_name))\n            print(\'Model restored! Epoch:\', i)\n            start_epoch = i + 1\n            break\n    if start_epoch == 0:\n        assert False, ""could not resume""\n\nif args.ngpu > 1:\n    net = torch.nn.DataParallel(net, device_ids=list(range(args.ngpu)))\n\nif args.ngpu > 0:\n    net.cuda()\n    torch.cuda.manual_seed(1)\n\ncudnn.benchmark = True  # fire on all cylinders\n\noptimizer = torch.optim.SGD(\n    net.parameters(), state[\'learning_rate\'], momentum=state[\'momentum\'],\n    weight_decay=state[\'decay\'], nesterov=True)\n\n\ndef cosine_annealing(step, total_steps, lr_max, lr_min):\n    return lr_min + (lr_max - lr_min) * 0.5 * (\n            1 + np.cos(step / total_steps * np.pi))\n\n\nscheduler = torch.optim.lr_scheduler.LambdaLR(\n    optimizer,\n    lr_lambda=lambda step: cosine_annealing(\n        step,\n        args.epochs * len(train_loader_in),\n        1,  # since lr_lambda computes multiplicative factor\n        1e-6 / args.learning_rate))\n\n\n# /////////////// Training ///////////////\n\ndef train():\n    net.train()  # enter train mode\n    loss_avg = 0.0\n\n    # start at a random point of the outlier dataset; this induces more randomness without destroying locality\n    train_loader_out.dataset.offset = np.random.randint(len(train_loader_out.dataset))\n    for in_set, out_set in zip(train_loader_in, train_loader_out):\n        data = torch.cat((in_set[0], out_set[0]), 0)\n        target = in_set[1]\n\n        data, target = data.cuda(), target.cuda()\n\n        # forward\n        x = net(data)\n\n        # backward\n        scheduler.step()\n        optimizer.zero_grad()\n\n        loss = F.cross_entropy(x[:len(in_set[0])], target)\n        # cross-entropy from softmax distribution to uniform distribution\n        loss += 0.5 * -(x[len(in_set[0]):].mean(1) - torch.logsumexp(x[len(in_set[0]):], dim=1)).mean()\n\n        loss.backward()\n        optimizer.step()\n\n        # exponential moving average\n        loss_avg = loss_avg * 0.8 + float(loss) * 0.2\n\n    state[\'train_loss\'] = loss_avg\n\n\n# test function\ndef test():\n    net.eval()\n    loss_avg = 0.0\n    correct = 0\n    with torch.no_grad():\n        for data, target in test_loader:\n            data, target = data.cuda(), target.cuda()\n\n            # forward\n            output = net(data)\n            loss = F.cross_entropy(output, target)\n\n            # accuracy\n            pred = output.data.max(1)[1]\n            correct += pred.eq(target.data).sum().item()\n\n            # test loss average\n            loss_avg += float(loss.data)\n\n    state[\'test_loss\'] = loss_avg / len(test_loader)\n    state[\'test_accuracy\'] = correct / len(test_loader.dataset)\n\n\nif args.test:\n    test()\n    print(state)\n    exit()\n\n# Make save directory\nif not os.path.exists(args.save):\n    os.makedirs(args.save)\nif not os.path.isdir(args.save):\n    raise Exception(\'%s is not a dir\' % args.save)\n\nwith open(os.path.join(args.save, calib_indicator + args.model + \'_oe_scratch_training_results.csv\'), \'w\') as f:\n    f.write(\'epoch,time(s),train_loss,test_loss,test_error(%)\\n\')\n\nprint(\'Beginning Training\\n\')\n\n# Main loop\nfor epoch in range(start_epoch, args.epochs):\n    state[\'epoch\'] = epoch\n\n    begin_epoch = time.time()\n\n    train()\n    test()\n\n    # Save model\n    torch.save(net.state_dict(),\n               os.path.join(args.save, calib_indicator + args.model + \'_oe_scratch_epoch_\' + str(epoch) + \'.pt\'))\n    # Let us not waste space and delete the previous model\n    prev_path = os.path.join(args.save, calib_indicator + args.model + \'_oe_scratch_epoch_\' + str(epoch - 1) + \'.pt\')\n    if os.path.exists(prev_path): os.remove(prev_path)\n\n    # Show results\n\n    with open(os.path.join(args.save, calib_indicator + args.model + \'_oe_scratch_training_results.csv\'), \'a\') as f:\n        f.write(\'%03d,%05d,%0.6f,%0.5f,%0.2f\\n\' % (\n            (epoch + 1),\n            time.time() - begin_epoch,\n            state[\'train_loss\'],\n            state[\'test_loss\'],\n            100 - 100. * state[\'test_accuracy\'],\n        ))\n\n    # # print state with rounded decimals\n    # print({k: round(v, 4) if isinstance(v, float) else v for k, v in state.items()})\n\n    print(\'Epoch {0:3d} | Time {1:5d} | Train Loss {2:.4f} | Test Loss {3:.3f} | Test Error {4:.2f}\'.format(\n        (epoch + 1),\n        int(time.time() - begin_epoch),\n        state[\'train_loss\'],\n        state[\'test_loss\'],\n        100 - 100. * state[\'test_accuracy\'])\n    )\n'"
TinyImageNet/oe_tune.py,16,"b'# -*- coding: utf-8 -*-\nimport numpy as np\nimport os\nimport pickle\nimport argparse\nimport time\nimport torch\nimport torch.nn as nn\nimport torch.backends.cudnn as cudnn\nimport torchvision.transforms as trn\nimport torchvision.datasets as dset\nimport torch.nn.functional as F\nfrom tqdm import tqdm\nfrom models.allconv import AllConvNet\nfrom models.wrn import WideResNet\n\nif __package__ is None:\n    import sys\n    from os import path\n\n    sys.path.append(path.dirname(path.dirname(path.abspath(__file__))))\n    from utils.tinyimages_80mn_loader import TinyImages\n    from utils.validation_dataset import validation_split_folder\n\nparser = argparse.ArgumentParser(description=\'Tunes a Tiny ImageNet Classifier  with OE\',\n                                 formatter_class=argparse.ArgumentDefaultsHelpFormatter)\nparser.add_argument(\'--model\', \'-m\', type=str, default=\'allconv\',\n                    choices=[\'allconv\', \'wrn\'], help=\'Choose architecture.\')\nparser.add_argument(\'--calibration\', \'-c\', action=\'store_true\',\n                    help=\'Train a model to be used for calibration. This holds out some data for validation.\')\n# Optimization options\nparser.add_argument(\'--epochs\', \'-e\', type=int, default=10, help=\'Number of epochs to train.\')\nparser.add_argument(\'--learning_rate\', \'-lr\', type=float, default=0.001, help=\'The initial learning rate.\')\nparser.add_argument(\'--batch_size\', \'-b\', type=int, default=128, help=\'Batch size.\')\nparser.add_argument(\'--oe_batch_size\', type=int, default=180, help=\'Batch size.\')   # as large as 1 GPU allows\nparser.add_argument(\'--test_bs\', type=int, default=200)\nparser.add_argument(\'--momentum\', type=float, default=0.9, help=\'Momentum.\')\nparser.add_argument(\'--decay\', \'-d\', type=float, default=0.0005, help=\'Weight decay (L2 penalty).\')\n# WRN Architecture\nparser.add_argument(\'--layers\', default=40, type=int, help=\'total number of layers\')\nparser.add_argument(\'--widen-factor\', default=2, type=int, help=\'widen factor\')\nparser.add_argument(\'--droprate\', default=0.3, type=float, help=\'dropout probability\')\n# Checkpoints\nparser.add_argument(\'--save\', \'-s\', type=str, default=\'./snapshots/oe_tune\', help=\'Folder to save checkpoints.\')\nparser.add_argument(\'--load\', \'-l\', type=str, default=\'./snapshots/baseline\', help=\'Checkpoint path to resume / test.\')\nparser.add_argument(\'--test\', \'-t\', action=\'store_true\', help=\'Test only flag.\')\n# Acceleration\nparser.add_argument(\'--ngpu\', type=int, default=1, help=\'0 = CPU.\')\nparser.add_argument(\'--prefetch\', type=int, default=4, help=\'Pre-fetching threads.\')\nargs = parser.parse_args()\n\nstate = {k: v for k, v in args._get_kwargs()}\nprint(state)\n\ntorch.manual_seed(1)\nnp.random.seed(1)\n\n# mean and standard deviation of channels of ImageNet images\nmean = [0.485, 0.456, 0.406]\nstd = [0.229, 0.224, 0.225]\n\ntrain_transform = trn.Compose([trn.RandomHorizontalFlip(), trn.RandomCrop(64, padding=8),\n                               trn.ToTensor(), trn.Normalize(mean, std)])\ntest_transform = trn.Compose([trn.ToTensor(), trn.Normalize(mean, std)])\n\ntrain_data_in = dset.ImageFolder(\n    root=""/share/data/vision-greg/tinyImageNet/tiny-imagenet-200/train"",\n    transform=train_transform)\ntest_data = dset.ImageFolder(\n    root=""/share/data/vision-greg/tinyImageNet/tiny-imagenet-200/val"",\n    transform=test_transform)\n\nnum_classes = 200\n\ncalib_indicator = \'\'\nif args.calibration:\n    train_data_in, val_data = validation_split_folder(train_data_in, val_share=0.1)\n    calib_indicator = \'calib_\'\n\nprint(\'Loading ImageNet22k\')\nood_data = dset.ImageFolder(root=""/share/data/vision-greg/ImageNet/clsloc/images/val"",\n                            transform=trn.Compose([trn.Resize(64), trn.CenterCrop(64),\n                                                   trn.ToTensor(), trn.Normalize(mean, std)]))\nood_data.root = \'/share/data/vision-greg/ImageNet22k\'\nood_data.class_to_idx = pickle.load(open(ood_data.root + \'/class_to_idx.p\', ""rb""))\nood_data.classes = pickle.load(open(ood_data.root + \'/classes.p\', ""rb""))\nood_data.imgs = pickle.load(open(ood_data.root + \'/imgs.p\', ""rb""))\nprint(\'Loaded ImageNet22k\')\n\ntrain_loader_in = torch.utils.data.DataLoader(\n    train_data_in,\n    batch_size=args.batch_size, shuffle=True,\n    num_workers=args.prefetch, pin_memory=True)\n\ntrain_loader_out = torch.utils.data.DataLoader(\n    ood_data,\n    batch_size=args.oe_batch_size, shuffle=False,\n    num_workers=args.prefetch, pin_memory=True)\n\ntest_loader = torch.utils.data.DataLoader(\n    test_data,\n    batch_size=args.batch_size, shuffle=False,\n    num_workers=args.prefetch, pin_memory=True)\n\n# Create model\nif args.model == \'allconv\':\n    net = AllConvNet(num_classes)\nelse:\n    net = WideResNet(args.layers, num_classes, args.widen_factor, dropRate=args.droprate)\n\n# Restore model\nmodel_found = False\nif args.load != \'\':\n    for i in range(1000 - 1, -1, -1):\n        model_name = os.path.join(args.load, calib_indicator + args.model + \'_baseline_epoch_\' + str(i) + \'.pt\')\n        if os.path.isfile(model_name):\n            net.load_state_dict(torch.load(model_name))\n            print(\'Model restored! Epoch:\', i)\n            model_found = True\n            break\n    if not model_found:\n        assert False, ""could not find model to restore""\n\nif args.ngpu > 1:\n    net = torch.nn.DataParallel(net, device_ids=list(range(args.ngpu)))\n\nif args.ngpu > 0:\n    net.cuda()\n    torch.cuda.manual_seed(1)\n\ncudnn.benchmark = True  # fire on all cylinders\n\noptimizer = torch.optim.SGD(\n    net.parameters(), state[\'learning_rate\'], momentum=state[\'momentum\'],\n    weight_decay=state[\'decay\'], nesterov=True)\n\n\ndef cosine_annealing(step, total_steps, lr_max, lr_min):\n    return lr_min + (lr_max - lr_min) * 0.5 * (\n            1 + np.cos(step / total_steps * np.pi))\n\n\nscheduler = torch.optim.lr_scheduler.LambdaLR(\n    optimizer,\n    lr_lambda=lambda step: cosine_annealing(\n        step,\n        args.epochs * len(train_loader_in),\n        1,  # since lr_lambda computes multiplicative factor\n        1e-6 / args.learning_rate))\n\n\n# /////////////// Training ///////////////\n\ndef train():\n    net.train()  # enter train mode\n    loss_avg = 0.0\n\n    # start at a random point of the outlier dataset; this induces more randomness without destroying locality\n    train_loader_out.dataset.offset = np.random.randint(len(train_loader_out.dataset))\n    for in_set, out_set in zip(train_loader_in, train_loader_out):\n        data = torch.cat((in_set[0], out_set[0]), 0)\n        target = in_set[1]\n\n        data, target = data.cuda(), target.cuda()\n\n        # forward\n        x = net(data)\n\n        # backward\n        scheduler.step()\n        optimizer.zero_grad()\n\n        loss = F.cross_entropy(x[:len(in_set[0])], target)\n        # cross-entropy from softmax distribution to uniform distribution\n        loss += 0.5 * -(x[len(in_set[0]):].mean(1) - torch.logsumexp(x[len(in_set[0]):], dim=1)).mean()\n\n        loss.backward()\n        optimizer.step()\n\n        # exponential moving average\n        loss_avg = loss_avg * 0.8 + float(loss) * 0.2\n\n    state[\'train_loss\'] = loss_avg\n\n\n# test function\ndef test():\n    net.eval()\n    loss_avg = 0.0\n    correct = 0\n    with torch.no_grad():\n        for data, target in test_loader:\n            data, target = data.cuda(), target.cuda()\n\n            # forward\n            output = net(data)\n            loss = F.cross_entropy(output, target)\n\n            # accuracy\n            pred = output.data.max(1)[1]\n            correct += pred.eq(target.data).sum().item()\n\n            # test loss average\n            loss_avg += float(loss.data)\n\n    state[\'test_loss\'] = loss_avg / len(test_loader)\n    state[\'test_accuracy\'] = correct / len(test_loader.dataset)\n\n\nif args.test:\n    test()\n    print(state)\n    exit()\n\n# Make save directory\nif not os.path.exists(args.save):\n    os.makedirs(args.save)\nif not os.path.isdir(args.save):\n    raise Exception(\'%s is not a dir\' % args.save)\n\nwith open(os.path.join(args.save, calib_indicator + args.model + \'_oe_tune_training_results.csv\'), \'w\') as f:\n    f.write(\'epoch,time(s),train_loss,test_loss,test_error(%)\\n\')\n\nprint(\'Beginning Training\\n\')\n\n# Main loop\nfor epoch in range(0, args.epochs):\n    state[\'epoch\'] = epoch\n\n    begin_epoch = time.time()\n\n    train()\n    test()\n\n    # Save model\n    torch.save(net.state_dict(),\n               os.path.join(args.save, calib_indicator + args.model + \'_oe_tune_epoch_\' + str(epoch) + \'.pt\'))\n    # Let us not waste space and delete the previous model\n    prev_path = os.path.join(args.save, calib_indicator + args.model + \'_oe_tune_epoch_\' + str(epoch - 1) + \'.pt\')\n    if os.path.exists(prev_path): os.remove(prev_path)\n\n    # Show results\n\n    with open(os.path.join(args.save, calib_indicator + args.model + \'_oe_tune_training_results.csv\'), \'a\') as f:\n        f.write(\'%03d,%05d,%0.6f,%0.5f,%0.2f\\n\' % (\n            (epoch + 1),\n            time.time() - begin_epoch,\n            state[\'train_loss\'],\n            state[\'test_loss\'],\n            100 - 100. * state[\'test_accuracy\'],\n        ))\n\n    # # print state with rounded decimals\n    # print({k: round(v, 4) if isinstance(v, float) else v for k, v in state.items()})\n\n    print(\'Epoch {0:3d} | Time {1:5d} | Train Loss {2:.4f} | Test Loss {3:.3f} | Test Error {4:.2f}\'.format(\n        (epoch + 1),\n        int(time.time() - begin_epoch),\n        state[\'train_loss\'],\n        state[\'test_loss\'],\n        100 - 100. * state[\'test_accuracy\'])\n    )\n'"
TinyImageNet/test.py,46,"b'import numpy as np\nimport sys\nimport os\nimport pickle\nimport argparse\nimport torch\nimport torch.nn as nn\nimport torch.backends.cudnn as cudnn\nimport torchvision.transforms as trn\nimport torchvision.datasets as dset\nimport torch.nn.functional as F\nfrom models.allconv import AllConvNet\nfrom models.wrn import WideResNet\nfrom skimage.filters import gaussian as gblur\nfrom PIL import Image as PILImage\n\n# go through rigamaroo to do ...utils.display_results import show_performance\nif __package__ is None:\n    import sys\n    from os import path\n\n    sys.path.append(path.dirname(path.dirname(path.abspath(__file__))))\n    from utils.display_results import show_performance, get_measures, print_measures, print_measures_with_std\n    import utils.svhn_loader as svhn\n    import utils.lsun_loader as lsun_loader\n\nparser = argparse.ArgumentParser(description=\'Evaluates a Tiny ImageNet OOD Detector\',\n                                 formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n# Setup\nparser.add_argument(\'--test_bs\', type=int, default=200)\nparser.add_argument(\'--num_to_avg\', type=int, default=1, help=\'Average measures across num_to_avg runs.\')\nparser.add_argument(\'--validate\', \'-v\', action=\'store_true\', help=\'Evaluate performance on validation distributions.\')\nparser.add_argument(\'--use_xent\', \'-x\', action=\'store_true\', help=\'Use cross entropy scoring instead of the MSP.\')\nparser.add_argument(\'--method_name\', \'-m\', type=str, default=\'allconv_baseline\', help=\'Method name.\')\n# Loading details\nparser.add_argument(\'--layers\', default=40, type=int, help=\'total number of layers\')\nparser.add_argument(\'--widen-factor\', default=2, type=int, help=\'widen factor\')\nparser.add_argument(\'--droprate\', default=0.3, type=float, help=\'dropout probability\')\nparser.add_argument(\'--load\', \'-l\', type=str, default=\'./snapshots\', help=\'Checkpoint path to resume / test.\')\nparser.add_argument(\'--ngpu\', type=int, default=1, help=\'0 = CPU.\')\nparser.add_argument(\'--prefetch\', type=int, default=4, help=\'Pre-fetching threads.\')\nargs = parser.parse_args()\n\n# torch.manual_seed(1)\n# np.random.seed(1)\n\n# mean and standard deviation of channels of ImageNet images\nmean = [0.485, 0.456, 0.406]\nstd = [0.229, 0.224, 0.225]\n\ntest_transform = trn.Compose([trn.ToTensor(), trn.Normalize(mean, std)])\n\ntest_data = dset.ImageFolder(\n    root=""/share/data/vision-greg/tinyImageNet/tiny-imagenet-200/val"",\n    transform=test_transform)\ntest_loader = torch.utils.data.DataLoader(test_data, batch_size=args.test_bs, shuffle=False,\n                                          num_workers=args.prefetch, pin_memory=True)\nnum_classes = 200\n\n# Create model\nif \'allconv\' in args.method_name:\n    net = AllConvNet(num_classes)\nelse:\n    net = WideResNet(args.layers, num_classes, args.widen_factor, dropRate=args.droprate)\n\nstart_epoch = 0\n\n# Restore model\nif args.load != \'\':\n    for i in range(1000 - 1, -1, -1):\n        if \'baseline\' in args.method_name:\n            subdir = \'baseline\'\n        elif \'oe_tune\' in args.method_name:\n            subdir = \'oe_tune\'\n        else:\n            subdir = \'oe_scratch\'\n\n        model_name = os.path.join(os.path.join(args.load, subdir), args.method_name + \'_epoch_\' + str(i) + \'.pt\')\n        if os.path.isfile(model_name):\n            net.load_state_dict(torch.load(model_name))\n            print(\'Model restored! Epoch:\', i)\n            start_epoch = i + 1\n            break\n    if start_epoch == 0:\n        assert False, ""could not resume""\n\nnet.eval()\n\nif args.ngpu > 1:\n    net = torch.nn.DataParallel(net, device_ids=list(range(args.ngpu)))\n\nif args.ngpu > 0:\n    net.cuda()\n    # torch.cuda.manual_seed(1)\n\ncudnn.benchmark = True  # fire on all cylinders\n\n# /////////////// Detection Prelims ///////////////\n\nood_num_examples = len(test_data) // 5\nexpected_ap = ood_num_examples / (ood_num_examples + len(test_data))\n\nconcat = lambda x: np.concatenate(x, axis=0)\nto_np = lambda x: x.data.cpu().numpy()\n\n\ndef get_ood_scores(loader, in_dist=False):\n    _score = []\n    _right_score = []\n    _wrong_score = []\n\n    with torch.no_grad():\n        for batch_idx, (data, target) in enumerate(loader):\n            if batch_idx >= ood_num_examples // args.test_bs and in_dist is False:\n                break\n\n            data = data.cuda()\n\n            output = net(data)\n            smax = to_np(F.softmax(output, dim=1))\n\n            if args.use_xent:\n                _score.append(to_np((output.mean(1) - torch.logsumexp(output, dim=1))))\n            else:\n                _score.append(-np.max(smax, axis=1))\n\n            if in_dist:\n                preds = np.argmax(smax, axis=1)\n                targets = target.numpy().squeeze()\n                right_indices = preds == targets\n                wrong_indices = np.invert(right_indices)\n\n                if args.use_xent:\n                    _right_score.append(to_np((output.mean(1) - torch.logsumexp(output, dim=1)))[right_indices])\n                    _wrong_score.append(to_np((output.mean(1) - torch.logsumexp(output, dim=1)))[wrong_indices])\n                else:\n                    _right_score.append(-np.max(smax[right_indices], axis=1))\n                    _wrong_score.append(-np.max(smax[wrong_indices], axis=1))\n\n    if in_dist:\n        return concat(_score).copy(), concat(_right_score).copy(), concat(_wrong_score).copy()\n    else:\n        return concat(_score)[:ood_num_examples].copy()\n\n\nin_score, right_score, wrong_score = get_ood_scores(test_loader, in_dist=True)\n\nnum_right = len(right_score)\nnum_wrong = len(wrong_score)\nprint(\'Error Rate {:.2f}\'.format(100 * num_wrong / (num_wrong + num_right)))\n\n# /////////////// End Detection Prelims ///////////////\n\nprint(\'\\nUsing TinyImageNet as typical data\')\n\n# /////////////// Error Detection ///////////////\n\nprint(\'\\n\\nError Detection\')\nshow_performance(wrong_score, right_score, method_name=args.method_name)\n\n# /////////////// OOD Detection ///////////////\nauroc_list, aupr_list, fpr_list = [], [], []\n\n\ndef get_and_print_results(ood_loader, num_to_avg=args.num_to_avg):\n\n    aurocs, auprs, fprs = [], [], []\n    for _ in range(num_to_avg):\n        out_score = get_ood_scores(ood_loader)\n        measures = get_measures(out_score, in_score)\n        aurocs.append(measures[0]); auprs.append(measures[1]); fprs.append(measures[2])\n\n    auroc = np.mean(aurocs); aupr = np.mean(auprs); fpr = np.mean(fprs)\n    auroc_list.append(auroc); aupr_list.append(aupr); fpr_list.append(fpr)\n\n    if num_to_avg >= 5:\n        print_measures_with_std(aurocs, auprs, fprs, args.method_name)\n    else:\n        print_measures(auroc, aupr, fpr, args.method_name)\n\n\n# /////////////// Gaussian Noise ///////////////\n\ndummy_targets = torch.ones(ood_num_examples * args.num_to_avg)\nood_data = torch.from_numpy(np.float32(np.clip(\n    np.random.normal(size=(ood_num_examples * args.num_to_avg, 3, 64, 64), scale=0.5), -1, 1)))\nood_data = torch.utils.data.TensorDataset(ood_data, dummy_targets)\nood_loader = torch.utils.data.DataLoader(ood_data, batch_size=args.test_bs, shuffle=True,\n                                         num_workers=args.prefetch, pin_memory=True)\n\nprint(\'\\n\\nGaussian Noise (sigma = 0.5) Detection\')\nget_and_print_results(ood_loader)\n\n# /////////////// Rademacher Noise ///////////////\n\ndummy_targets = torch.ones(ood_num_examples * args.num_to_avg)\nood_data = torch.from_numpy(np.random.binomial(\n    n=1, p=0.5, size=(ood_num_examples * args.num_to_avg, 3, 64, 64)).astype(np.float32)) * 2 - 1\nood_data = torch.utils.data.TensorDataset(ood_data, dummy_targets)\nood_loader = torch.utils.data.DataLoader(ood_data, batch_size=args.test_bs, shuffle=True)\n\nprint(\'\\n\\nRademacher Noise Detection\')\nget_and_print_results(ood_loader)\n\n# /////////////// Blob ///////////////\n\nood_data = np.float32(np.random.binomial(n=1, p=0.7, size=(ood_num_examples * args.num_to_avg, 64, 64, 3)))\nfor i in range(ood_num_examples * args.num_to_avg):\n    ood_data[i] = gblur(ood_data[i], sigma=2, multichannel=False)\n    ood_data[i][ood_data[i] < 0.75] = 0.0\n\ndummy_targets = torch.ones(ood_num_examples * args.num_to_avg)\nood_data = torch.from_numpy(ood_data.transpose((0, 3, 1, 2))) * 2 - 1\nood_data = torch.utils.data.TensorDataset(ood_data, dummy_targets)\nood_loader = torch.utils.data.DataLoader(ood_data, batch_size=args.test_bs, shuffle=True,\n                                         num_workers=args.prefetch, pin_memory=True)\n\nprint(\'\\n\\nBlob Detection\')\nget_and_print_results(ood_loader)\n\n# /////////////// Textures ///////////////\n\nood_data = dset.ImageFolder(root=""/share/data/vision-greg2/users/dan/datasets/dtd/images"",\n                            transform=trn.Compose([trn.Resize(64), trn.CenterCrop(64),\n                                                   trn.ToTensor(), trn.Normalize(mean, std)]))\nood_loader = torch.utils.data.DataLoader(ood_data, batch_size=args.test_bs, shuffle=True,\n                                         num_workers=args.prefetch, pin_memory=True)\n\nprint(\'\\n\\nTexture Detection\')\nget_and_print_results(ood_loader)\n\n# /////////////// SVHN ///////////////\n\nood_data = svhn.SVHN(root=\'/share/data/vision-greg/svhn/\', split=""test"",\n                     transform=trn.Compose([trn.Resize(64), trn.ToTensor(), trn.Normalize(mean, std)]), download=False)\nood_loader = torch.utils.data.DataLoader(ood_data, batch_size=args.test_bs, shuffle=True,\n                                         num_workers=args.prefetch, pin_memory=True)\n\nprint(\'\\n\\nSVHN Detection\')\nget_and_print_results(ood_loader)\n\n# /////////////// Places365 ///////////////\n\nood_data = dset.ImageFolder(root=""/share/data/vision-greg2/places365/test_subset"",\n                            transform=trn.Compose([trn.Resize(64), trn.CenterCrop(64),\n                                                   trn.ToTensor(), trn.Normalize(mean, std)]))\nood_loader = torch.utils.data.DataLoader(ood_data, batch_size=args.test_bs, shuffle=True,\n                                         num_workers=args.prefetch, pin_memory=True)\n\nprint(\'\\n\\nPlaces365 Detection\')\nget_and_print_results(ood_loader)\n\n# /////////////// LSUN ///////////////\n\nood_data = lsun_loader.LSUN(""/share/data/vision-greg2/users/dan/datasets/LSUN/lsun-master/data"", classes=\'test\',\n                            transform=trn.Compose([trn.Resize(64), trn.CenterCrop(64),\n                                                   trn.ToTensor(), trn.Normalize(mean, std)]))\nood_loader = torch.utils.data.DataLoader(ood_data, batch_size=args.test_bs, shuffle=True,\n                                         num_workers=args.prefetch, pin_memory=True)\n\nprint(\'\\n\\nLSUN Detection\')\nget_and_print_results(ood_loader)\n\n# /////////////// Held-out ImageNet Classes ///////////////\n\nood_data = dset.ImageFolder(\n    root=""/share/data/vision-greg2/users/dan/datasets/TinyImageNet/out/val"", transform=test_transform)\nood_loader = torch.utils.data.DataLoader(ood_data, batch_size=args.test_bs, shuffle=True,\n                                         num_workers=args.prefetch, pin_memory=True)\n\n\nprint(\'\\n\\nHeld-out ImageNet Classes Detection\')\nget_and_print_results(ood_loader)\n\n# /////////////// Mean Results ///////////////\n\nprint(\'\\n\\nMean Test Results\')\nprint_measures(np.mean(auroc_list), np.mean(aupr_list), np.mean(fpr_list), method_name=args.method_name)\n\n# /////////////// OOD Detection of Validation Distributions ///////////////\n\nif args.validate is False:\n    exit()\n\nauroc_list, aupr_list, fpr_list = [], [], []\n\n# /////////////// Uniform Noise ///////////////\n\ndummy_targets = torch.ones(ood_num_examples * args.num_to_avg)\nood_data = torch.from_numpy(\n    np.random.uniform(size=(ood_num_examples * args.num_to_avg, 3, 64, 64),\n                      low=-1.0, high=1.0).astype(np.float32))\nood_data = torch.utils.data.TensorDataset(ood_data, dummy_targets)\nood_loader = torch.utils.data.DataLoader(ood_data, batch_size=args.test_bs, shuffle=True)\n\nprint(\'\\n\\nUniform[-1,1] Noise Detection\')\nget_and_print_results(ood_loader)\n\n\n# /////////////// Arithmetic Mean of Images ///////////////\n\n\nclass AvgOfPair(torch.utils.data.Dataset):\n    def __init__(self, dataset):\n        self.dataset = dataset\n        self.shuffle_indices = np.arange(len(dataset))\n        np.random.shuffle(self.shuffle_indices)\n\n    def __getitem__(self, i):\n        random_idx = np.random.choice(len(self.dataset))\n        while random_idx == i:\n            random_idx = np.random.choice(len(self.dataset))\n\n        return self.dataset[i][0] / 2. + self.dataset[random_idx][0] / 2., 0\n\n    def __len__(self):\n        return len(self.dataset)\n\n\nood_data = dset.ImageFolder(\n    root=""/share/data/vision-greg2/users/dan/datasets/TinyImageNet/out/val"", transform=test_transform)\nood_loader = torch.utils.data.DataLoader(AvgOfPair(ood_data),\n                                         batch_size=args.test_bs, shuffle=True,\n                                         num_workers=args.prefetch, pin_memory=True)\n\nprint(\'\\n\\nArithmetic Mean of Random Image Pair Detection\')\nget_and_print_results(ood_loader)\n\n\n# /////////////// Geometric Mean of Images ///////////////\n\n\nclass GeomMeanOfPair(torch.utils.data.Dataset):\n    def __init__(self, dataset):\n        self.dataset = dataset\n        self.shuffle_indices = np.arange(len(dataset))\n        np.random.shuffle(self.shuffle_indices)\n\n    def __getitem__(self, i):\n        random_idx = np.random.choice(len(self.dataset))\n        while random_idx == i:\n            random_idx = np.random.choice(len(self.dataset))\n\n        return trn.Normalize(mean, std)(torch.sqrt(self.dataset[i][0] * self.dataset[random_idx][0])), 0\n\n    def __len__(self):\n        return len(self.dataset)\n\n\nood_data = dset.ImageFolder(\n    root=""/share/data/vision-greg2/users/dan/datasets/TinyImageNet/out/val"", transform=trn.ToTensor())\nood_loader = torch.utils.data.DataLoader(\n    GeomMeanOfPair(ood_data), batch_size=args.test_bs, shuffle=True,\n    num_workers=args.prefetch, pin_memory=True)\n\nprint(\'\\n\\nGeometric Mean of Random Image Pair Detection\')\nget_and_print_results(ood_loader)\n\n# /////////////// Jigsaw Images ///////////////\n\nood_loader = torch.utils.data.DataLoader(ood_data, batch_size=args.test_bs, shuffle=True,\n                                         num_workers=args.prefetch, pin_memory=True)\n\njigsaw = lambda x: torch.cat((\n    torch.cat((torch.cat((x[:, 16:32, :32], x[:, :16, :32]), 1),\n               x[:, 32:, :32]), 2),\n    torch.cat((x[:, 32:, 32:],\n               torch.cat((x[:, :32, 48:], x[:, :32, 32:48]), 2)), 2),\n), 1)\n\nood_loader.dataset.transform = trn.Compose([trn.ToTensor(), jigsaw, trn.Normalize(mean, std)])\n\nprint(\'\\n\\nJigsawed Images Detection\')\nget_and_print_results(ood_loader)\n\n# /////////////// Speckled Images ///////////////\n\nspeckle = lambda x: torch.clamp(x + x * torch.randn_like(x), 0, 1)\nood_loader.dataset.transform = trn.Compose([trn.ToTensor(), speckle, trn.Normalize(mean, std)])\n\nprint(\'\\n\\nSpeckle Noised Images Detection\')\nget_and_print_results(ood_loader)\n\n# /////////////// Pixelated Images ///////////////\n\npixelate = lambda x: x.resize((int(64 * 0.2), int(64 * 0.2)), PILImage.BOX).resize((64, 64), PILImage.BOX)\nood_loader.dataset.transform = trn.Compose([pixelate, trn.ToTensor(), trn.Normalize(mean, std)])\n\nprint(\'\\n\\nPixelate Detection\')\nget_and_print_results(ood_loader)\n\n# /////////////// RGB Ghosted/Shifted Images ///////////////\n\nrgb_shift = lambda x: torch.cat((x[1:2].index_select(2, torch.LongTensor([i for i in range(64 - 1, -1, -1)])),\n                                 x[2:, :, :], x[0:1, :, :]), 0)\nood_loader.dataset.transform = trn.Compose([trn.ToTensor(), rgb_shift, trn.Normalize(mean, std)])\n\nprint(\'\\n\\nRGB Ghosted/Shifted Image Detection\')\nget_and_print_results(ood_loader)\n\n# /////////////// Inverted Images ///////////////\n\n# not done on all channels to make image ood with higher probability\ninvert = lambda x: torch.cat((x[0:1, :, :], 1 - x[1:2, :, ], 1 - x[2:, :, :],), 0)\nood_loader.dataset.transform = trn.Compose([trn.ToTensor(), invert, trn.Normalize(mean, std)])\n\nprint(\'\\n\\nInverted Image Detection\')\nget_and_print_results(ood_loader)\n\n# /////////////// Mean Results ///////////////\n\nprint(\'\\n\\nMean Validation Results\')\nprint_measures(np.mean(auroc_list), np.mean(aupr_list), np.mean(fpr_list), method_name=args.method_name)\n'"
TinyImageNet/test_calibration.py,44,"b'import numpy as np\nimport sys\nimport os\nimport pickle\nimport argparse\nimport torch\nimport torch.nn as nn\nimport torch.backends.cudnn as cudnn\nimport torchvision.transforms as trn\nimport torchvision.datasets as dset\nimport torch.nn.functional as F\nfrom models.allconv import AllConvNet\nfrom models.wrn import WideResNet\nfrom skimage.filters import gaussian as gblur\nfrom PIL import Image as PILImage\n\n# go through rigamaroo to do ...utils.display_results import show_performance\nif __package__ is None:\n    import sys\n    from os import path\n\n    sys.path.append(path.dirname(path.dirname(path.abspath(__file__))))\n    from utils.display_results import show_performance, get_measures, print_measures, print_measures_with_std\n    import utils.svhn_loader as svhn\n    import utils.lsun_loader as lsun_loader\n    from utils.validation_dataset import validation_split_folder\n    from utils.calibration_tools import *\n\nparser = argparse.ArgumentParser(description=\'Evaluates a Tiny ImageNet OOD Detector\',\n                                 formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n# Setup\nparser.add_argument(\'--test_bs\', type=int, default=200)\nparser.add_argument(\'--num_to_avg\', type=int, default=1, help=\'Average measures across num_to_avg runs.\')\nparser.add_argument(\'--validate\', \'-v\', action=\'store_true\', help=\'Evaluate performance on validation distributions.\')\nparser.add_argument(\'--method_name\', \'-m\', type=str, default=\'allconv_calib_baseline\', help=\'Method name.\')\nparser.add_argument(\'--use_01\', \'-z\', action=\'store_true\', help=\'Use 0-1 Posterior Rescaling.\')\n# Loading details\nparser.add_argument(\'--layers\', default=40, type=int, help=\'total number of layers\')\nparser.add_argument(\'--widen-factor\', default=2, type=int, help=\'widen factor\')\nparser.add_argument(\'--droprate\', default=0.3, type=float, help=\'dropout probability\')\nparser.add_argument(\'--load\', \'-l\', type=str, default=\'./snapshots\', help=\'Checkpoint path to resume / test.\')\nparser.add_argument(\'--ngpu\', type=int, default=1, help=\'0 = CPU.\')\nparser.add_argument(\'--prefetch\', type=int, default=4, help=\'Pre-fetching threads.\')\nargs = parser.parse_args()\n\n# torch.manual_seed(1)\n# np.random.seed(1)\n\n# mean and standard deviation of channels of ImageNet images\nmean = [0.485, 0.456, 0.406]\nstd = [0.229, 0.224, 0.225]\n\ntest_transform = trn.Compose([trn.ToTensor(), trn.Normalize(mean, std)])\n\ntrain_data = dset.ImageFolder(\n    root=""/share/data/vision-greg/tinyImageNet/tiny-imagenet-200/train"",\n    transform=test_transform)\ntest_data = dset.ImageFolder(\n    root=""/share/data/vision-greg/tinyImageNet/tiny-imagenet-200/val"",\n    transform=test_transform)\n\ntrain_data, val_data = validation_split_folder(train_data, val_share=0.1)\nval_loader = torch.utils.data.DataLoader(val_data, batch_size=args.test_bs, shuffle=False,\n                                         num_workers=args.prefetch, pin_memory=True)\ntest_loader = torch.utils.data.DataLoader(test_data, batch_size=args.test_bs, shuffle=False,\n                                          num_workers=args.prefetch, pin_memory=True)\nnum_classes = 200\n\n# Create model\nif \'allconv\' in args.method_name:\n    net = AllConvNet(num_classes)\nelse:\n    net = WideResNet(args.layers, num_classes, args.widen_factor, dropRate=args.droprate)\n\nstart_epoch = 0\n\n# Restore model\nif args.load != \'\':\n    for i in range(1000 - 1, -1, -1):\n        if \'baseline\' in args.method_name:\n            subdir = \'baseline\'\n        elif \'oe_tune\' in args.method_name:\n            subdir = \'oe_tune\'\n        else:\n            subdir = \'oe_scratch\'\n\n        model_name = os.path.join(os.path.join(args.load, subdir), args.method_name + \'_epoch_\' + str(i) + \'.pt\')\n        if os.path.isfile(model_name):\n            net.load_state_dict(torch.load(model_name))\n            print(\'Model restored! Epoch:\', i)\n            start_epoch = i + 1\n            break\n    if start_epoch == 0:\n        assert False, ""could not resume""\n\nnet.eval()\n\nif args.ngpu > 1:\n    net = torch.nn.DataParallel(net, device_ids=list(range(args.ngpu)))\n\nif args.ngpu > 0:\n    net.cuda()\n    # torch.cuda.manual_seed(1)\n\ncudnn.benchmark = True  # fire on all cylinders\n\n# /////////////// Calibration Prelims ///////////////\n\nood_num_examples = len(test_data) // 5\nexpected_ap = ood_num_examples / (ood_num_examples + len(test_data))\n\nconcat = lambda x: np.concatenate(x, axis=0)\nto_np = lambda x: x.data.to(\'cpu\').numpy()\n\n\ndef get_net_results(data_loader, in_dist=False, t=1):\n    logits = []\n    confidence = []\n    correct = []\n    labels = []\n\n    with torch.no_grad():\n        for batch_idx, (data, target) in enumerate(data_loader):\n            if batch_idx >= ood_num_examples // args.test_bs and in_dist is False:\n                break\n            data, target = data.cuda(), target.cuda()\n\n            output = net(data)\n\n            logits.extend(to_np(output).squeeze())\n\n            if args.use_01:\n                confidence.extend(to_np(\n                    (F.softmax(output/t, dim=1).max(1)[0] - 1./num_classes)/(1 - 1./num_classes)\n                ).squeeze().tolist())\n            else:\n                confidence.extend(to_np(F.softmax(output/t, dim=1).max(1)[0]).squeeze().tolist())\n\n            if in_dist:\n                pred = output.data.max(1)[1]\n                correct.extend(pred.eq(target).to(\'cpu\').numpy().squeeze().tolist())\n                labels.extend(target.to(\'cpu\').numpy().squeeze().tolist())\n\n    if in_dist:\n        return logits.copy(), confidence.copy(), correct.copy(), labels.copy()\n    else:\n        return logits[:ood_num_examples].copy(), confidence[:ood_num_examples].copy()\n\n\nval_logits, val_confidence, val_correct, val_labels = get_net_results(val_loader, in_dist=True)\n\n# np.save(\'./logits_\'+args.method_name+\'.npy\', val_logits)\n# np.save(\'./labels_\'+args.method_name+\'.npy\', val_labels)\n# exit()\n\nprint(\'\\nTuning Softmax Temperature\')\nt_star = tune_temp(val_logits, val_labels)\n\nprint(\'Softmax Temperature Tuned. Temperature is {:.3f}\'.format(t_star))\n\ntest_logits, test_confidence, test_correct, _ = get_net_results(test_loader, in_dist=True, t=t_star)\n\nprint(\'Error Rate {:.2f}\'.format(100*(len(test_correct) - sum(test_correct))/len(test_correct)))\n\n# /////////////// End Calibration Prelims ///////////////\n\nprint(\'\\nUsing TinyImageNet as typical data\')\n\n# /////////////// In-Distribution Calibration ///////////////\n\nprint(\'\\n\\nIn-Distribution Data\')\nshow_calibration_results(np.array(test_confidence), np.array(test_correct), method_name=args.method_name)\n\n# /////////////// OOD Calibration ///////////////\nrms_list, mad_list, sf1_list = [], [], []\n\n\ndef get_and_print_results(ood_loader, num_to_avg=args.num_to_avg):\n\n    rmss, mads, sf1s = [], [], []\n    for _ in range(num_to_avg):\n        out_logits, out_confidence = get_net_results(ood_loader, t=t_star)\n\n        measures = get_measures(concat([out_confidence, test_confidence]),\n            concat([np.zeros(len(out_confidence)), test_correct]))\n\n        rmss.append(measures[0]); mads.append(measures[1]); sf1s.append(measures[2])\n\n    rms = np.mean(rmss); mad = np.mean(mads); sf1 = np.mean(sf1s)\n    rms_list.append(rms); mad_list.append(mad); sf1_list.append(sf1)\n\n    if num_to_avg >= 5:\n        print_measures_with_std(rmss, mads, sf1s, args.method_name)\n    else:\n        print_measures(rms, mad, sf1, args.method_name)\n\n\n# /////////////// Gaussian Noise ///////////////\n\ndummy_targets = torch.ones(ood_num_examples * args.num_to_avg)\nood_data = torch.from_numpy(np.float32(np.clip(\n    np.random.normal(size=(ood_num_examples * args.num_to_avg, 3, 64, 64), scale=0.5), -1, 1)))\nood_data = torch.utils.data.TensorDataset(ood_data, dummy_targets)\nood_loader = torch.utils.data.DataLoader(ood_data, batch_size=args.test_bs, shuffle=True,\n                                         num_workers=args.prefetch, pin_memory=True)\n\nprint(\'\\n\\nGaussian Noise (sigma = 0.5) Calibration\')\nget_and_print_results(ood_loader)\n\n# /////////////// Rademacher Noise ///////////////\n\ndummy_targets = torch.ones(ood_num_examples * args.num_to_avg)\nood_data = torch.from_numpy(np.random.binomial(\n    n=1, p=0.5, size=(ood_num_examples * args.num_to_avg, 3, 64, 64)).astype(np.float32)) * 2 - 1\nood_data = torch.utils.data.TensorDataset(ood_data, dummy_targets)\nood_loader = torch.utils.data.DataLoader(ood_data, batch_size=args.test_bs, shuffle=True)\n\nprint(\'\\n\\nRademacher Noise Calibration\')\nget_and_print_results(ood_loader)\n\n# /////////////// Blob ///////////////\n\nood_data = np.float32(np.random.binomial(n=1, p=0.7, size=(ood_num_examples * args.num_to_avg, 64, 64, 3)))\nfor i in range(ood_num_examples * args.num_to_avg):\n    ood_data[i] = gblur(ood_data[i], sigma=2, multichannel=False)\n    ood_data[i][ood_data[i] < 0.75] = 0.0\n\ndummy_targets = torch.ones(ood_num_examples * args.num_to_avg)\nood_data = torch.from_numpy(ood_data.transpose((0, 3, 1, 2))) * 2 - 1\nood_data = torch.utils.data.TensorDataset(ood_data, dummy_targets)\nood_loader = torch.utils.data.DataLoader(ood_data, batch_size=args.test_bs, shuffle=True,\n                                         num_workers=args.prefetch, pin_memory=True)\n\nprint(\'\\n\\nBlob Calibration\')\nget_and_print_results(ood_loader)\n\n# /////////////// Textures ///////////////\n\nood_data = dset.ImageFolder(root=""/share/data/vision-greg2/users/dan/datasets/dtd/images"",\n                            transform=trn.Compose([trn.Resize(64), trn.CenterCrop(64),\n                                                   trn.ToTensor(), trn.Normalize(mean, std)]))\nood_loader = torch.utils.data.DataLoader(ood_data, batch_size=args.test_bs, shuffle=True,\n                                         num_workers=args.prefetch, pin_memory=True)\n\nprint(\'\\n\\nTexture Calibration\')\nget_and_print_results(ood_loader)\n\n# /////////////// SVHN ///////////////\n\nood_data = svhn.SVHN(root=\'/share/data/vision-greg/svhn/\', split=""test"",\n                     transform=trn.Compose([trn.Resize(64), trn.ToTensor(), trn.Normalize(mean, std)]), download=False)\nood_loader = torch.utils.data.DataLoader(ood_data, batch_size=args.test_bs, shuffle=True,\n                                         num_workers=args.prefetch, pin_memory=True)\n\nprint(\'\\n\\nSVHN Calibration\')\nget_and_print_results(ood_loader)\n\n# /////////////// Places365 ///////////////\n\nood_data = dset.ImageFolder(root=""/share/data/vision-greg2/places365/test_subset"",\n                            transform=trn.Compose([trn.Resize(64), trn.CenterCrop(64),\n                                                   trn.ToTensor(), trn.Normalize(mean, std)]))\nood_loader = torch.utils.data.DataLoader(ood_data, batch_size=args.test_bs, shuffle=True,\n                                         num_workers=args.prefetch, pin_memory=True)\n\nprint(\'\\n\\nPlaces365 Calibration\')\nget_and_print_results(ood_loader)\n\n# /////////////// LSUN ///////////////\n\nood_data = lsun_loader.LSUN(""/share/data/vision-greg2/users/dan/datasets/LSUN/lsun-master/data"", classes=\'test\',\n                            transform=trn.Compose([trn.Resize(64), trn.CenterCrop(64),\n                                                   trn.ToTensor(), trn.Normalize(mean, std)]))\nood_loader = torch.utils.data.DataLoader(ood_data, batch_size=args.test_bs, shuffle=True,\n                                         num_workers=args.prefetch, pin_memory=True)\n\nprint(\'\\n\\nLSUN Calibration\')\nget_and_print_results(ood_loader)\n\n# /////////////// Held-out ImageNet Classes ///////////////\n\nood_data = dset.ImageFolder(\n    root=""/share/data/vision-greg2/users/dan/datasets/TinyImageNet/out/val"", transform=test_transform)\nood_loader = torch.utils.data.DataLoader(ood_data, batch_size=args.test_bs, shuffle=True,\n                                         num_workers=args.prefetch, pin_memory=True)\n\n\nprint(\'\\n\\nHeld-out ImageNet Classes Calibration\')\nget_and_print_results(ood_loader)\n\n# /////////////// Mean Results ///////////////\n\nprint(\'\\n\\nMean Test Results\')\nprint_measures(np.mean(rms_list), np.mean(mad_list), np.mean(sf1_list), method_name=args.method_name)\n\n# /////////////// OOD Calibration of Validation Distributions ///////////////\n\nif args.validate is False:\n    exit()\n\nrms_list, mad_list, sf1_list = [], [], []\n\n# /////////////// Uniform Noise ///////////////\n\ndummy_targets = torch.ones(ood_num_examples * args.num_to_avg)\nood_data = torch.from_numpy(\n    np.random.uniform(size=(ood_num_examples * args.num_to_avg, 3, 64, 64),\n                      low=-1.0, high=1.0).astype(np.float32))\nood_data = torch.utils.data.TensorDataset(ood_data, dummy_targets)\nood_loader = torch.utils.data.DataLoader(ood_data, batch_size=args.test_bs, shuffle=True)\n\nprint(\'\\n\\nUniform[-1,1] Noise Calibration\')\nget_and_print_results(ood_loader)\n\n# /////////////// Arithmetic Mean of Images ///////////////\n\n\nclass AvgOfPair(torch.utils.data.Dataset):\n    def __init__(self, dataset):\n        self.dataset = dataset\n        self.shuffle_indices = np.arange(len(dataset))\n        np.random.shuffle(self.shuffle_indices)\n\n    def __getitem__(self, i):\n        random_idx = np.random.choice(len(self.dataset))\n        while random_idx == i:\n            random_idx = np.random.choice(len(self.dataset))\n\n        return self.dataset[i][0] / 2. + self.dataset[random_idx][0] / 2., 0\n\n    def __len__(self):\n        return len(self.dataset)\n\n\nood_data = dset.ImageFolder(\n    root=""/share/data/vision-greg2/users/dan/datasets/TinyImageNet/out/val"", transform=test_transform)\nood_loader = torch.utils.data.DataLoader(AvgOfPair(ood_data),\n                                         batch_size=args.test_bs, shuffle=True,\n                                         num_workers=args.prefetch, pin_memory=True)\n\nprint(\'\\n\\nArithmetic Mean of Random Image Pair Calibration\')\nget_and_print_results(ood_loader)\n\n# /////////////// Geometric Mean of Images ///////////////\n\n\nclass GeomMeanOfPair(torch.utils.data.Dataset):\n    def __init__(self, dataset):\n        self.dataset = dataset\n        self.shuffle_indices = np.arange(len(dataset))\n        np.random.shuffle(self.shuffle_indices)\n\n    def __getitem__(self, i):\n        random_idx = np.random.choice(len(self.dataset))\n        while random_idx == i:\n            random_idx = np.random.choice(len(self.dataset))\n\n        return trn.Normalize(mean, std)(torch.sqrt(self.dataset[i][0] * self.dataset[random_idx][0])), 0\n\n    def __len__(self):\n        return len(self.dataset)\n\n\nood_data = dset.ImageFolder(\n    root=""/share/data/vision-greg2/users/dan/datasets/TinyImageNet/out/val"", transform=trn.ToTensor())\nood_loader = torch.utils.data.DataLoader(\n    GeomMeanOfPair(ood_data), batch_size=args.test_bs, shuffle=True,\n    num_workers=args.prefetch, pin_memory=True)\n\nprint(\'\\n\\nGeometric Mean of Random Image Pair Calibration\')\nget_and_print_results(ood_loader)\n\n# /////////////// Jigsaw Images ///////////////\n\nood_loader = torch.utils.data.DataLoader(ood_data, batch_size=args.test_bs, shuffle=True,\n                                         num_workers=args.prefetch, pin_memory=True)\n\njigsaw = lambda x: torch.cat((\n    torch.cat((torch.cat((x[:, 16:32, :32], x[:, :16, :32]), 1),\n               x[:, 32:, :32]), 2),\n    torch.cat((x[:, 32:, 32:],\n               torch.cat((x[:, :32, 48:], x[:, :32, 32:48]), 2)), 2),\n), 1)\n\nood_loader.dataset.transform = trn.Compose([trn.ToTensor(), jigsaw, trn.Normalize(mean, std)])\n\nprint(\'\\n\\nJigsawed Images Calibration\')\nget_and_print_results(ood_loader)\n\n# /////////////// Speckled Images ///////////////\n\nspeckle = lambda x: torch.clamp(x + x * torch.randn_like(x), 0, 1)\nood_loader.dataset.transform = trn.Compose([trn.ToTensor(), speckle, trn.Normalize(mean, std)])\n\nprint(\'\\n\\nSpeckle Noised Images Calibration\')\nget_and_print_results(ood_loader)\n\n# /////////////// Pixelated Images ///////////////\n\npixelate = lambda x: x.resize((int(64 * 0.2), int(64 * 0.2)), PILImage.BOX).resize((64, 64), PILImage.BOX)\nood_loader.dataset.transform = trn.Compose([pixelate, trn.ToTensor(), trn.Normalize(mean, std)])\n\nprint(\'\\n\\nPixelate Calibration\')\nget_and_print_results(ood_loader)\n\n# /////////////// RGB Ghosted/Shifted Images ///////////////\n\nrgb_shift = lambda x: torch.cat((x[1:2].index_select(2, torch.LongTensor([i for i in range(64 - 1, -1, -1)])),\n                                 x[2:, :, :], x[0:1, :, :]), 0)\nood_loader.dataset.transform = trn.Compose([trn.ToTensor(), rgb_shift, trn.Normalize(mean, std)])\n\nprint(\'\\n\\nRGB Ghosted/Shifted Image Calibration\')\nget_and_print_results(ood_loader)\n\n# /////////////// Inverted Images ///////////////\n\n# not done on all channels to make image ood with higher probability\ninvert = lambda x: torch.cat((x[0:1, :, :], 1 - x[1:2, :, ], 1 - x[2:, :, :],), 0)\nood_loader.dataset.transform = trn.Compose([trn.ToTensor(), invert, trn.Normalize(mean, std)])\n\nprint(\'\\n\\nInverted Image Calibration\')\nget_and_print_results(ood_loader)\n\n# /////////////// Mean Results ///////////////\n\nprint(\'\\n\\nMean Validation Results\')\nprint_measures(np.mean(rms_list), np.mean(mad_list), np.mean(sf1_list), method_name=args.method_name)\n'"
utils/calibration_tools.py,5,"b'import numpy as np\n\n\ndef calib_err(confidence, correct, p=\'2\', beta=100):\n    # beta is target bin size\n    idxs = np.argsort(confidence)\n    confidence = confidence[idxs]\n    correct = correct[idxs]\n    bins = [[i * beta, (i + 1) * beta] for i in range(len(confidence) // beta)]\n    bins[-1] = [bins[-1][0], len(confidence)]\n\n    cerr = 0\n    total_examples = len(confidence)\n    for i in range(len(bins) - 1):\n        bin_confidence = confidence[bins[i][0]:bins[i][1]]\n        bin_correct = correct[bins[i][0]:bins[i][1]]\n        num_examples_in_bin = len(bin_confidence)\n\n        if num_examples_in_bin > 0:\n            difference = np.abs(np.nanmean(bin_confidence) - np.nanmean(bin_correct))\n\n            if p == \'2\':\n                cerr += num_examples_in_bin / total_examples * np.square(difference)\n            elif p == \'1\':\n                cerr += num_examples_in_bin / total_examples * difference\n            elif p == \'infty\' or p == \'infinity\' or p == \'max\':\n                cerr = np.maximum(cerr, difference)\n            else:\n                assert False, ""p must be \'1\', \'2\', or \'infty\'""\n\n    if p == \'2\':\n        cerr = np.sqrt(cerr)\n\n    return cerr\n\n\ndef soft_f1(confidence, correct):\n    wrong = 1 - correct\n\n    # # the incorrectly classified samples are our interest\n    # # so they make the positive class\n    # tp_soft = np.sum((1 - confidence) * wrong)\n    # fp_soft = np.sum((1 - confidence) * correct)\n    # fn_soft = np.sum(confidence * wrong)\n\n    # return 2 * tp_soft / (2 * tp_soft + fn_soft + fp_soft)\n    return 2 * ((1 - confidence) * wrong).sum()/(1 - confidence + wrong).sum()\n\n\ndef tune_temp(logits, labels, binary_search=True, lower=0.2, upper=5.0, eps=0.0001):\n    logits = np.array(logits)\n\n    if binary_search:\n        import torch\n        import torch.nn.functional as F\n\n        logits = torch.FloatTensor(logits)\n        labels = torch.LongTensor(labels)\n        t_guess = torch.FloatTensor([0.5*(lower + upper)]).requires_grad_()\n\n        while upper - lower > eps:\n            if torch.autograd.grad(F.cross_entropy(logits / t_guess, labels), t_guess)[0] > 0:\n                upper = 0.5 * (lower + upper)\n            else:\n                lower = 0.5 * (lower + upper)\n            t_guess = t_guess * 0 + 0.5 * (lower + upper)\n\n        t = min([lower, 0.5 * (lower + upper), upper], key=lambda x: float(F.cross_entropy(logits / x, labels)))\n    else:\n        import cvxpy as cx\n\n        set_size = np.array(logits).shape[0]\n\n        t = cx.Variable()\n\n        expr = sum((cx.Minimize(cx.log_sum_exp(logits[i, :] * t) - logits[i, labels[i]] * t)\n                    for i in range(set_size)))\n        p = cx.Problem(expr, [lower <= t, t <= upper])\n\n        p.solve()   # p.solve(solver=cx.SCS)\n        t = 1 / t.value\n\n    return t\n\n\ndef get_measures(confidence, correct):\n    rms = calib_err(confidence, correct, p=\'2\')\n    mad = calib_err(confidence, correct, p=\'1\')\n    sf1 = soft_f1(confidence, correct)\n\n    return rms, mad, sf1\n\n\ndef print_measures(rms, mad, sf1, method_name=\'Baseline\'):\n    print(\'\\t\\t\\t\\t\\t\\t\\t\' + method_name)\n    print(\'RMS Calib Error (%): \\t\\t{:.2f}\'.format(100 * rms))\n    print(\'MAD Calib Error (%): \\t\\t{:.2f}\'.format(100 * mad))\n    print(\'Soft F1 Score (%):   \\t\\t{:.2f}\'.format(100 * sf1))\n\n\ndef print_measures_with_std(rmss, mads, sf1s, method_name=\'Baseline\'):\n    print(\'\\t\\t\\t\\t\\t\\t\\t\' + method_name)\n    print(\'RMS Calib Error (%): \\t\\t{:.2f}\\t+/- {:.2f}\'.format(100 * np.mean(rmss), 100 * np.std(rmss)))\n    print(\'MAD Calib Error (%): \\t\\t{:.2f}\\t+/- {:.2f}\'.format(100 * np.mean(mads), 100 * np.std(mads)))\n    print(\'Soft F1 Score (%):   \\t\\t{:.2f}\\t+/- {:.2f}\'.format(100 * np.mean(sf1s), 100 * np.std(sf1s)))\n\n\ndef show_calibration_results(confidence, correct, method_name=\'Baseline\'):\n\n    print(\'\\t\\t\\t\\t\' + method_name)\n    print(\'RMS Calib Error (%): \\t\\t{:.2f}\'.format(\n        100 * calib_err(confidence, correct, p=\'2\')))\n\n    print(\'MAD Calib Error (%): \\t\\t{:.2f}\'.format(\n        100 * calib_err(confidence, correct, p=\'1\')))\n\n    # print(\'Max Calib Error (%): \\t\\t{:.2f}\'.format(\n    #     100 * calib_err(confidence, correct, p=\'infty\')))\n\n    print(\'Soft F1-Score (%): \\t\\t{:.2f}\'.format(\n        100 * soft_f1(confidence, correct)))\n\n    # add error detection measures?\n'"
utils/cifar_resnet.py,4,"b'import math\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass BasicBlock(nn.Module):\n    def __init__(self, in_planes, out_planes, stride, dropRate=0.0):\n        super(BasicBlock, self).__init__()\n        self.bn1 = nn.BatchNorm2d(in_planes)\n        self.relu1 = nn.ReLU(inplace=True)\n        self.conv1 = nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n                               padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(out_planes)\n        self.relu2 = nn.ReLU(inplace=True)\n        self.conv2 = nn.Conv2d(out_planes, out_planes, kernel_size=3, stride=1,\n                               padding=1, bias=False)\n        self.droprate = dropRate\n        self.equalInOut = (in_planes == out_planes)\n        self.convShortcut = (not self.equalInOut) and nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride,\n                                                                padding=0, bias=False) or None\n\n    def forward(self, x):\n        if not self.equalInOut:\n            x = self.relu1(self.bn1(x))\n        else:\n            out = self.relu1(self.bn1(x))\n        if self.equalInOut:\n            out = self.relu2(self.bn2(self.conv1(out)))\n        else:\n            out = self.relu2(self.bn2(self.conv1(x)))\n        if self.droprate > 0:\n            out = F.dropout(out, p=self.droprate, training=self.training)\n        out = self.conv2(out)\n        if not self.equalInOut:\n            return torch.add(self.convShortcut(x), out)\n        else:\n            return torch.add(x, out)\n\n\nclass NetworkBlock(nn.Module):\n    def __init__(self, nb_layers, in_planes, out_planes, block, stride, dropRate=0.0):\n        super(NetworkBlock, self).__init__()\n        self.layer = self._make_layer(block, in_planes, out_planes, nb_layers, stride, dropRate)\n\n    def _make_layer(self, block, in_planes, out_planes, nb_layers, stride, dropRate):\n        layers = []\n        for i in range(nb_layers):\n            layers.append(block(i == 0 and in_planes or out_planes, out_planes, i == 0 and stride or 1, dropRate))\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        return self.layer(x)\n\n\nclass WideResNet(nn.Module):\n    def __init__(self, depth, num_classes, widen_factor=1, dropRate=0.0):\n        super(WideResNet, self).__init__()\n        nChannels = [16, 16 * widen_factor, 32 * widen_factor, 64 * widen_factor]\n        assert ((depth - 4) % 6 == 0)\n        n = (depth - 4) // 6\n        block = BasicBlock\n        # 1st conv before any network block\n        self.conv1 = nn.Conv2d(3, nChannels[0], kernel_size=3, stride=1,\n                               padding=1, bias=False)\n        # 1st block\n        self.block1 = NetworkBlock(n, nChannels[0], nChannels[1], block, 1, dropRate)\n        # 2nd block\n        self.block2 = NetworkBlock(n, nChannels[1], nChannels[2], block, 2, dropRate)\n        # 3rd block\n        self.block3 = NetworkBlock(n, nChannels[2], nChannels[3], block, 2, dropRate)\n        # global average pooling and classifier\n        self.bn1 = nn.BatchNorm2d(nChannels[3])\n        self.relu = nn.ReLU(inplace=True)\n        self.fc = nn.Linear(nChannels[3], num_classes)\n        self.nChannels = nChannels[3]\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n                m.weight.data.normal_(0, math.sqrt(2. / n))\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n            elif isinstance(m, nn.Linear):\n                m.bias.data.zero_()\n\n    def forward(self, x):\n        out = self.conv1(x)\n        out = self.block1(out)\n        out = self.block2(out)\n        out = self.block3(out)\n        out = self.relu(self.bn1(out))\n        out = F.avg_pool2d(out, 8)\n        out = out.view(-1, self.nChannels)\n        return self.fc(out)\n'"
utils/display_results.py,0,"b'import numpy as np\nimport sklearn.metrics as sk\n\nrecall_level_default = 0.95\n\n\ndef stable_cumsum(arr, rtol=1e-05, atol=1e-08):\n    """"""Use high precision for cumsum and check that final value matches sum\n    Parameters\n    ----------\n    arr : array-like\n        To be cumulatively summed as flat\n    rtol : float\n        Relative tolerance, see ``np.allclose``\n    atol : float\n        Absolute tolerance, see ``np.allclose``\n    """"""\n    out = np.cumsum(arr, dtype=np.float64)\n    expected = np.sum(arr, dtype=np.float64)\n    if not np.allclose(out[-1], expected, rtol=rtol, atol=atol):\n        raise RuntimeError(\'cumsum was found to be unstable: \'\n                           \'its last element does not correspond to sum\')\n    return out\n\n\ndef fpr_and_fdr_at_recall(y_true, y_score, recall_level=recall_level_default, pos_label=None):\n    classes = np.unique(y_true)\n    if (pos_label is None and\n            not (np.array_equal(classes, [0, 1]) or\n                     np.array_equal(classes, [-1, 1]) or\n                     np.array_equal(classes, [0]) or\n                     np.array_equal(classes, [-1]) or\n                     np.array_equal(classes, [1]))):\n        raise ValueError(""Data is not binary and pos_label is not specified"")\n    elif pos_label is None:\n        pos_label = 1.\n\n    # make y_true a boolean vector\n    y_true = (y_true == pos_label)\n\n    # sort scores and corresponding truth values\n    desc_score_indices = np.argsort(y_score, kind=""mergesort"")[::-1]\n    y_score = y_score[desc_score_indices]\n    y_true = y_true[desc_score_indices]\n\n    # y_score typically has many tied values. Here we extract\n    # the indices associated with the distinct values. We also\n    # concatenate a value for the end of the curve.\n    distinct_value_indices = np.where(np.diff(y_score))[0]\n    threshold_idxs = np.r_[distinct_value_indices, y_true.size - 1]\n\n    # accumulate the true positives with decreasing threshold\n    tps = stable_cumsum(y_true)[threshold_idxs]\n    fps = 1 + threshold_idxs - tps      # add one because of zero-based indexing\n\n    thresholds = y_score[threshold_idxs]\n\n    recall = tps / tps[-1]\n\n    last_ind = tps.searchsorted(tps[-1])\n    sl = slice(last_ind, None, -1)      # [last_ind::-1]\n    recall, fps, tps, thresholds = np.r_[recall[sl], 1], np.r_[fps[sl], 0], np.r_[tps[sl], 0], thresholds[sl]\n\n    cutoff = np.argmin(np.abs(recall - recall_level))\n\n    return fps[cutoff] / (np.sum(np.logical_not(y_true)))   # , fps[cutoff]/(fps[cutoff] + tps[cutoff])\n\n\ndef get_measures(_pos, _neg, recall_level=recall_level_default):\n    pos = np.array(_pos[:]).reshape((-1, 1))\n    neg = np.array(_neg[:]).reshape((-1, 1))\n    examples = np.squeeze(np.vstack((pos, neg)))\n    labels = np.zeros(len(examples), dtype=np.int32)\n    labels[:len(pos)] += 1\n\n    auroc = sk.roc_auc_score(labels, examples)\n    aupr = sk.average_precision_score(labels, examples)\n    fpr = fpr_and_fdr_at_recall(labels, examples, recall_level)\n\n    return auroc, aupr, fpr\n\n\ndef show_performance(pos, neg, method_name=\'Ours\', recall_level=recall_level_default):\n    \'\'\'\n    :param pos: 1\'s class, class to detect, outliers, or wrongly predicted\n    example scores\n    :param neg: 0\'s class scores\n    \'\'\'\n\n    auroc, aupr, fpr = get_measures(pos[:], neg[:], recall_level)\n\n    print(\'\\t\\t\\t\' + method_name)\n    print(\'FPR{:d}:\\t\\t\\t{:.2f}\'.format(int(100 * recall_level), 100 * fpr))\n    print(\'AUROC:\\t\\t\\t{:.2f}\'.format(100 * auroc))\n    print(\'AUPR:\\t\\t\\t{:.2f}\'.format(100 * aupr))\n    # print(\'FDR{:d}:\\t\\t\\t{:.2f}\'.format(int(100 * recall_level), 100 * fdr))\n\n\ndef print_measures(auroc, aupr, fpr, method_name=\'Ours\', recall_level=recall_level_default):\n    print(\'\\t\\t\\t\\t\' + method_name)\n    print(\'FPR{:d}:\\t\\t\\t{:.2f}\'.format(int(100 * recall_level), 100 * fpr))\n    print(\'AUROC: \\t\\t\\t{:.2f}\'.format(100 * auroc))\n    print(\'AUPR:  \\t\\t\\t{:.2f}\'.format(100 * aupr))\n\n\ndef print_measures_with_std(aurocs, auprs, fprs, method_name=\'Ours\', recall_level=recall_level_default):\n    print(\'\\t\\t\\t\\t\' + method_name)\n    print(\'FPR{:d}:\\t\\t\\t{:.2f}\\t+/- {:.2f}\'.format(int(100 * recall_level), 100 * np.mean(fprs), 100 * np.std(fprs)))\n    print(\'AUROC: \\t\\t\\t{:.2f}\\t+/- {:.2f}\'.format(100 * np.mean(aurocs), 100 * np.std(aurocs)))\n    print(\'AUPR:  \\t\\t\\t{:.2f}\\t+/- {:.2f}\'.format(100 * np.mean(auprs), 100 * np.std(auprs)))\n\n\ndef show_performance_comparison(pos_base, neg_base, pos_ours, neg_ours, baseline_name=\'Baseline\',\n                                method_name=\'Ours\', recall_level=recall_level_default):\n    \'\'\'\n    :param pos_base: 1\'s class, class to detect, outliers, or wrongly predicted\n    example scores from the baseline\n    :param neg_base: 0\'s class scores generated by the baseline\n    \'\'\'\n    auroc_base, aupr_base, fpr_base = get_measures(pos_base[:], neg_base[:], recall_level)\n    auroc_ours, aupr_ours, fpr_ours = get_measures(pos_ours[:], neg_ours[:], recall_level)\n\n    print(\'\\t\\t\\t\' + baseline_name + \'\\t\' + method_name)\n    print(\'FPR{:d}:\\t\\t\\t{:.2f}\\t\\t{:.2f}\'.format(\n        int(100 * recall_level), 100 * fpr_base, 100 * fpr_ours))\n    print(\'AUROC:\\t\\t\\t{:.2f}\\t\\t{:.2f}\'.format(\n        100 * auroc_base, 100 * auroc_ours))\n    print(\'AUPR:\\t\\t\\t{:.2f}\\t\\t{:.2f}\'.format(\n        100 * aupr_base, 100 * aupr_ours))\n    # print(\'FDR{:d}:\\t\\t\\t{:.2f}\\t\\t{:.2f}\'.format(\n    #     int(100 * recall_level), 100 * fdr_base, 100 * fdr_ours))\n'"
utils/lsun_loader.py,1,"b'import torch.utils.data as data\nfrom PIL import Image\nimport os\nimport os.path\nimport six\nimport string\nimport sys\n\nif sys.version_info[0] == 2:\n    import cPickle as pickle\nelse:\n    import pickle\n\n\nclass LSUNClass(data.Dataset):\n    def __init__(self, db_path, transform=None, target_transform=None):\n        import lmdb\n        self.db_path = db_path\n        self.env = lmdb.open(db_path, max_readers=1, readonly=True, lock=False,\n                             readahead=False, meminit=False)\n        with self.env.begin(write=False) as txn:\n            self.length = txn.stat()[\'entries\']\n        cache_file = \'_cache_\' + db_path.replace(\'/\', \'_\')\n        if os.path.isfile(cache_file):\n            self.keys = pickle.load(open(cache_file, ""rb""))\n        else:\n            with self.env.begin(write=False) as txn:\n                self.keys = [key for key, _ in txn.cursor()]\n            pickle.dump(self.keys, open(cache_file, ""wb""))\n        self.transform = transform\n        self.target_transform = target_transform\n\n    def __getitem__(self, index):\n        img, target = None, None\n        env = self.env\n        with env.begin(write=False) as txn:\n            imgbuf = txn.get(self.keys[index])\n\n        buf = six.BytesIO()\n        buf.write(imgbuf)\n        buf.seek(0)\n        img = Image.open(buf).convert(\'RGB\')\n\n        if self.transform is not None:\n            img = self.transform(img)\n\n        if self.target_transform is not None:\n            target = self.target_transform(target)\n\n        return img, target\n\n    def __len__(self):\n        return self.length\n\n    def __repr__(self):\n        return self.__class__.__name__ + \' (\' + self.db_path + \')\'\n\n\nclass LSUN(data.Dataset):\n    """"""\n    `LSUN <http://lsun.cs.princeton.edu>`_ dataset.\n\n    Args:\n        db_path (string): Root directory for the database files.\n        classes (string or list): One of {\'train\', \'val\', \'test\'} or a list of\n            categories to load. e,g. [\'bedroom_train\', \'church_train\'].\n        transform (callable, optional): A function/transform that  takes in an PIL image\n            and returns a transformed version. E.g, ``transforms.RandomCrop``\n        target_transform (callable, optional): A function/transform that takes in the\n            target and transforms it.\n    """"""\n\n    def __init__(self, db_path, classes=\'train\',\n                 transform=None, target_transform=None):\n        categories = [\'bedroom\', \'bridge\', \'church_outdoor\', \'classroom\',\n                      \'conference_room\', \'dining_room\', \'kitchen\',\n                      \'living_room\', \'restaurant\', \'tower\']\n        dset_opts = [\'train\', \'val\', \'test\']\n        self.db_path = db_path\n        if type(classes) == str and classes in dset_opts:\n            if classes == \'test\':\n                classes = [classes]\n            else:\n                classes = [c + \'_\' + classes for c in categories]\n        self.classes = classes\n\n        # for each class, create an LSUNClassDataset\n        self.dbs = []\n        for c in self.classes:\n            self.dbs.append(LSUNClass(\n                db_path=db_path + \'/\' + c + \'_lmdb\',\n                transform=transform))\n\n        self.indices = []\n        count = 0\n        for db in self.dbs:\n            count += len(db)\n            self.indices.append(count)\n\n        self.length = count\n        self.target_transform = target_transform\n\n    def __getitem__(self, index):\n        """"""\n        Args:\n            index (int): Index\n\n        Returns:\n            tuple: Tuple (image, target) where target is the index of the target category.\n        """"""\n        target = 0\n        sub = 0\n        for ind in self.indices:\n            if index < ind:\n                break\n            target += 1\n            sub = ind\n\n        db = self.dbs[target]\n        index = index - sub\n\n        if self.target_transform is not None:\n            target = self.target_transform(target)\n\n        img, _ = db[index]\n        return img, target\n\n    def __len__(self):\n        return self.length\n\n    def __repr__(self):\n        return self.__class__.__name__ + \' (\' + self.db_path + \')\'\n'"
utils/svhn_loader.py,1,"b'import torch.utils.data as data\nfrom PIL import Image\nimport os\nimport os.path\nimport numpy as np\n\n\nclass SVHN(data.Dataset):\n    url = """"\n    filename = """"\n    file_md5 = """"\n\n    split_list = {\n        \'train\': [""http://ufldl.stanford.edu/housenumbers/train_32x32.mat"",\n                  ""train_32x32.mat"", ""e26dedcc434d2e4c54c9b2d4a06d8373""],\n        \'test\': [""http://ufldl.stanford.edu/housenumbers/test_32x32.mat"",\n                 ""test_32x32.mat"", ""eb5a983be6a315427106f1b164d9cef3""],\n        \'extra\': [""http://ufldl.stanford.edu/housenumbers/extra_32x32.mat"",\n                  ""extra_32x32.mat"", ""a93ce644f1a588dc4d68dda5feec44a7""],\n        \'train_and_extra\': [\n                [""http://ufldl.stanford.edu/housenumbers/train_32x32.mat"",\n                 ""train_32x32.mat"", ""e26dedcc434d2e4c54c9b2d4a06d8373""],\n                [""http://ufldl.stanford.edu/housenumbers/extra_32x32.mat"",\n                 ""extra_32x32.mat"", ""a93ce644f1a588dc4d68dda5feec44a7""]]}\n\n    def __init__(self, root, split=\'train\',\n                 transform=None, target_transform=None, download=False):\n        self.root = root\n        self.transform = transform\n        self.target_transform = target_transform\n        self.split = split  # training set or test set or extra set\n\n        if self.split not in self.split_list:\n            raise ValueError(\'Wrong split entered! Please use split=""train"" \'\n                             \'or split=""extra"" or split=""test"" \'\n                             \'or split=""train_and_extra"" \')\n\n        if self.split == ""train_and_extra"":\n            self.url = self.split_list[split][0][0]\n            self.filename = self.split_list[split][0][1]\n            self.file_md5 = self.split_list[split][0][2]\n        else:\n            self.url = self.split_list[split][0]\n            self.filename = self.split_list[split][1]\n            self.file_md5 = self.split_list[split][2]\n\n        # import here rather than at top of file because this is\n        # an optional dependency for torchvision\n        import scipy.io as sio\n\n        # reading(loading) mat file as array\n        loaded_mat = sio.loadmat(os.path.join(root, self.filename))\n\n        if self.split == ""test"":\n            self.data = loaded_mat[\'X\']\n            self.targets = loaded_mat[\'y\']\n            # Note label 10 == 0 so modulo operator required\n            self.targets = (self.targets % 10).squeeze()    # convert to zero-based indexing\n            self.data = np.transpose(self.data, (3, 2, 0, 1))\n        else:\n            self.data = loaded_mat[\'X\']\n            self.targets = loaded_mat[\'y\']\n\n            if self.split == ""train_and_extra"":\n                extra_filename = self.split_list[split][1][1]\n                loaded_mat = sio.loadmat(os.path.join(root, extra_filename))\n                self.data = np.concatenate([self.data,\n                                                  loaded_mat[\'X\']], axis=3)\n                self.targets = np.vstack((self.targets,\n                                               loaded_mat[\'y\']))\n            # Note label 10 == 0 so modulo operator required\n            self.targets = (self.targets % 10).squeeze()    # convert to zero-based indexing\n            self.data = np.transpose(self.data, (3, 2, 0, 1))\n\n    def __getitem__(self, index):\n        if self.split == ""test"":\n            img, target = self.data[index], self.targets[index]\n        else:\n            img, target = self.data[index], self.targets[index]\n\n        # doing this so that it is consistent with all other datasets\n        # to return a PIL Image\n        img = Image.fromarray(np.transpose(img, (1, 2, 0)))\n\n        if self.transform is not None:\n            img = self.transform(img)\n\n        if self.target_transform is not None:\n            target = self.target_transform(target)\n\n        return img, target\n\n    def __len__(self):\n        if self.split == ""test"":\n            return len(self.data)\n        else:\n            return len(self.data)\n\n    def _check_integrity(self):\n        root = self.root\n        if self.split == ""train_and_extra"":\n            md5 = self.split_list[self.split][0][2]\n            fpath = os.path.join(root, self.filename)\n            train_integrity = check_integrity(fpath, md5)\n            extra_filename = self.split_list[self.split][1][1]\n            md5 = self.split_list[self.split][1][2]\n            fpath = os.path.join(root, extra_filename)\n            return check_integrity(fpath, md5) and train_integrity\n        else:\n            md5 = self.split_list[self.split][2]\n            fpath = os.path.join(root, self.filename)\n            return check_integrity(fpath, md5)\n\n    def download(self):\n        if self.split == ""train_and_extra"":\n            md5 = self.split_list[self.split][0][2]\n            download_url(self.url, self.root, self.filename, md5)\n            extra_filename = self.split_list[self.split][1][1]\n            md5 = self.split_list[self.split][1][2]\n            download_url(self.url, self.root, extra_filename, md5)\n        else:\n            md5 = self.split_list[self.split][2]\n            download_url(self.url, self.root, self.filename, md5)\n'"
utils/tiny_resnet.py,4,"b'import math\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass BasicBlock(nn.Module):\n    def __init__(self, in_planes, out_planes, stride, dropRate=0.0):\n        super(BasicBlock, self).__init__()\n        self.bn1 = nn.BatchNorm2d(in_planes)\n        self.relu1 = nn.ReLU(inplace=True)\n        self.conv1 = nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n                               padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(out_planes)\n        self.relu2 = nn.ReLU(inplace=True)\n        self.conv2 = nn.Conv2d(out_planes, out_planes, kernel_size=3, stride=1,\n                               padding=1, bias=False)\n        self.droprate = dropRate\n        self.equalInOut = (in_planes == out_planes)\n        self.convShortcut = (not self.equalInOut) and nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride,\n                                                                padding=0, bias=False) or None\n\n    def forward(self, x):\n        if not self.equalInOut:\n            x = self.relu1(self.bn1(x))\n        else:\n            out = self.relu1(self.bn1(x))\n        if self.equalInOut:\n            out = self.relu2(self.bn2(self.conv1(out)))\n        else:\n            out = self.relu2(self.bn2(self.conv1(x)))\n        if self.droprate > 0:\n            out = F.dropout(out, p=self.droprate, training=self.training)\n        out = self.conv2(out)\n        if not self.equalInOut:\n            return torch.add(self.convShortcut(x), out)\n        else:\n            return torch.add(x, out)\n\n\nclass NetworkBlock(nn.Module):\n    def __init__(self, nb_layers, in_planes, out_planes, block, stride, dropRate=0.0):\n        super(NetworkBlock, self).__init__()\n        self.layer = self._make_layer(block, in_planes, out_planes, nb_layers, stride, dropRate)\n\n    def _make_layer(self, block, in_planes, out_planes, nb_layers, stride, dropRate):\n        layers = []\n        for i in range(nb_layers):\n            layers.append(block(i == 0 and in_planes or out_planes, out_planes, i == 0 and stride or 1, dropRate))\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        return self.layer(x)\n\n\nclass WideResNet(nn.Module):\n    def __init__(self, depth, num_classes, widen_factor=1, dropRate=0.0):\n        super(WideResNet, self).__init__()\n        nChannels = [16, 16 * widen_factor, 32 * widen_factor, 64 * widen_factor]\n        assert ((depth - 4) % 6 == 0)\n        n = (depth - 4) // 6\n        block = BasicBlock\n        # 1st conv before any network block\n        self.conv1 = nn.Conv2d(3, nChannels[0], kernel_size=3, stride=1,\n                               padding=1, bias=False)\n        # 1st block\n        self.block1 = NetworkBlock(n, nChannels[0], nChannels[1], block, 1, dropRate)\n        # 2nd block\n        self.block2 = NetworkBlock(n, nChannels[1], nChannels[2], block, 2, dropRate)\n        # 3rd block\n        self.block3 = NetworkBlock(n, nChannels[2], nChannels[3], block, 2, dropRate)\n        # global average pooling and classifier\n        self.bn1 = nn.BatchNorm2d(nChannels[3])\n        self.relu = nn.ReLU(inplace=True)\n        self.fc = nn.Linear(nChannels[3], num_classes)\n        self.nChannels = nChannels[3]\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n                m.weight.data.normal_(0, math.sqrt(2. / n))\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n            elif isinstance(m, nn.Linear):\n                m.bias.data.zero_()\n\n    def forward(self, x):\n        out = self.conv1(x)\n        out = self.block1(out)\n        out = self.block2(out)\n        out = self.block3(out)\n        out = self.relu(self.bn1(out))\n        out = F.avg_pool2d(out, 16)\n        out = out.view(-1, self.nChannels)\n        return self.fc(out)\n'"
utils/tinyimages_80mn_loader.py,1,"b'import numpy as np\nimport torch\nfrom bisect import bisect_left\n\n\nclass TinyImages(torch.utils.data.Dataset):\n\n    def __init__(self, transform=None, exclude_cifar=True):\n\n        data_file = open(\'/share/data/vision-greg/80million/tiny_images.bin\', ""rb"")\n\n        def load_image(idx):\n            data_file.seek(idx * 3072)\n            data = data_file.read(3072)\n            return np.fromstring(data, dtype=\'uint8\').reshape(32, 32, 3, order=""F"")\n\n        self.load_image = load_image\n        self.offset = 0     # offset index\n\n        self.transform = transform\n        self.exclude_cifar = exclude_cifar\n\n        if exclude_cifar:\n            self.cifar_idxs = []\n            with open(\'/share/data/vision-greg2/OE/utils/80mn_cifar_idxs.txt\', \'r\') as idxs:\n                for idx in idxs:\n                    # indices in file take the 80mn database to start at 1, hence ""- 1""\n                    self.cifar_idxs.append(int(idx) - 1)\n\n            # hash table option\n            self.cifar_idxs = set(self.cifar_idxs)\n            self.in_cifar = lambda x: x in self.cifar_idxs\n\n            # bisection search option\n            # self.cifar_idxs = tuple(sorted(self.cifar_idxs))\n            #\n            # def binary_search(x, hi=len(self.cifar_idxs)):\n            #     pos = bisect_left(self.cifar_idxs, x, 0, hi)  # find insertion position\n            #     return True if pos != hi and self.cifar_idxs[pos] == x else False\n            #\n            # self.in_cifar = binary_search\n\n    def __getitem__(self, index):\n        index = (index + self.offset) % 79302016\n\n        if self.exclude_cifar:\n            while self.in_cifar(index):\n                index = np.random.randint(79302017)\n\n        img = self.load_image(index)\n        if self.transform is not None:\n            img = self.transform(img)\n\n        return img, 0  # 0 is the class\n\n    def __len__(self):\n        return 79302017\n'"
utils/validation_dataset.py,2,"b'import torch\nimport numpy as np\n\n\nclass PartialDataset(torch.utils.data.Dataset):\n    def __init__(self, parent_ds, offset, length):\n        self.parent_ds = parent_ds\n        self.offset = offset\n        self.length = length\n        assert len(parent_ds) >= offset + length, Exception(""Parent Dataset not long enough"")\n        super(PartialDataset, self).__init__()\n\n    def __len__(self):\n        return self.length\n\n    def __getitem__(self, i):\n        return self.parent_ds[i + self.offset]\n\n\ndef validation_split(dataset, val_share=0.1):\n    """"""\n       Split a (training and vaidation combined) dataset into training and validation.\n       Note that to be statistically sound, the items in the dataset should be statistically\n       independent (e.g. not sorted by class, not several instances of the same dataset that\n       could end up in either set).\n\n       inputs:\n          dataset:   (""training"") dataset to split into training and validation\n          val_share: fraction of validation data (should be 0<val_share<1, default: 0.1)\n       returns: input dataset split into test_ds, val_ds\n\n    """"""\n    val_offset = int(len(dataset) * (1 - val_share))\n    return PartialDataset(dataset, 0, val_offset), PartialDataset(dataset, val_offset, len(dataset) - val_offset)\n\n\nclass PartialFolder(torch.utils.data.Dataset):\n    def __init__(self, parent_ds, perm, length):\n        self.parent_ds = parent_ds\n        self.perm = perm\n        self.length = length\n        super(PartialFolder, self).__init__()\n\n    def __len__(self):\n        return self.length\n\n    def __getitem__(self, i):\n        return self.parent_ds[self.perm[i]]\n\n\ndef validation_split_folder(dataset, val_share=0.1):\n    """"""\n       Split a (training and vaidation combined) dataset into training and validation.\n       Note that to be statistically sound, the items in the dataset should be statistically\n       independent (e.g. not sorted by class, not several instances of the same dataset that\n       could end up in either set).\n\n       inputs:\n          dataset:   (""training"") dataset to split into training and validation\n          val_share: fraction of validation data (should be 0<val_share<1, default: 0.1)\n       returns: input dataset split into test_ds, val_ds\n\n    """"""\n    num_train = int(len(dataset) * (1 - val_share))\n    num_val = len(dataset) - num_train\n\n    perm = np.asarray(range(len(dataset)))\n    np.random.seed(0)\n    np.random.shuffle(perm)\n\n    train_perm, val_perm = perm[:num_train], perm[num_train:]\n\n    return PartialFolder(dataset, train_perm, num_train), PartialFolder(dataset, val_perm, num_val)\n'"
CIFAR/models/allconv.py,4,"b""import math\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass GELU(nn.Module):\n    def __init__(self):\n        super(GELU, self).__init__()\n\n    def forward(self, x):\n        return torch.sigmoid(1.702 * x) * x\n        # return 0.5 * x * (1 + torch.tanh(x * 0.7978845608 * (1 + 0.044715 * x * x)))\n\n\ndef make_layers(cfg):\n    layers = []\n    in_channels = 3\n    for v in cfg:\n        if v == 'Md':\n            layers += [nn.MaxPool2d(kernel_size=2, stride=2), nn.Dropout(p=0.5)]\n        elif v == 'A':\n            layers += [nn.AvgPool2d(kernel_size=8)]\n        elif v == 'NIN':\n            conv2d = nn.Conv2d(in_channels, in_channels, kernel_size=1, padding=1)\n            layers += [conv2d, nn.BatchNorm2d(in_channels), GELU()]      # nn.ReLU(inplace=True)]\n        elif v == 'nopad':\n            conv2d = nn.Conv2d(in_channels, in_channels, kernel_size=3, padding=0)\n            layers += [conv2d, nn.BatchNorm2d(in_channels), GELU()]      # nn.ReLU(inplace=True)]\n        else:\n            conv2d = nn.Conv2d(in_channels, v, kernel_size=3, padding=1)\n            layers += [conv2d, nn.BatchNorm2d(v), GELU()]    # nn.ReLU(inplace=True)]\n            in_channels = v\n    return nn.Sequential(*layers)\n\n\nclass AllConvNet(nn.Module):\n    def __init__(self, num_classes):\n        super(AllConvNet, self).__init__()\n\n        self.num_classes = num_classes\n\n        # if num_classes > 10:\n        #     self.width1, w1 = 128, 128\n        #     self.width2, w2 = 256, 256\n        # else:\n        #     self.width1, w1 = 96,  96\n        #     self.width2, w2 = 192, 192\n\n        self.width1, w1 = 96,  96\n        self.width2, w2 = 192, 192\n\n        self.features = make_layers([w1, w1, w1, 'Md', w2, w2, w2, 'Md', 'nopad', 'NIN', 'NIN', 'A'])\n        self.classifier = nn.Linear(self.width2, num_classes)\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n                m.weight.data.normal_(0, math.sqrt(2. / n))     # He initialization\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n            elif isinstance(m, nn.Linear):\n                m.bias.data.zero_()\n\n    def forward(self, x):\n        x = self.features(x)\n        x = x.view(x.size(0), -1)\n        x = self.classifier(x)\n        return x\n"""
CIFAR/models/wrn.py,4,"b'import math\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass BasicBlock(nn.Module):\n    def __init__(self, in_planes, out_planes, stride, dropRate=0.0):\n        super(BasicBlock, self).__init__()\n        self.bn1 = nn.BatchNorm2d(in_planes)\n        self.relu1 = nn.ReLU(inplace=True)\n        self.conv1 = nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n                               padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(out_planes)\n        self.relu2 = nn.ReLU(inplace=True)\n        self.conv2 = nn.Conv2d(out_planes, out_planes, kernel_size=3, stride=1,\n                               padding=1, bias=False)\n        self.droprate = dropRate\n        self.equalInOut = (in_planes == out_planes)\n        self.convShortcut = (not self.equalInOut) and nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride,\n                                                                padding=0, bias=False) or None\n\n    def forward(self, x):\n        if not self.equalInOut:\n            x = self.relu1(self.bn1(x))\n        else:\n            out = self.relu1(self.bn1(x))\n        if self.equalInOut:\n            out = self.relu2(self.bn2(self.conv1(out)))\n        else:\n            out = self.relu2(self.bn2(self.conv1(x)))\n        if self.droprate > 0:\n            out = F.dropout(out, p=self.droprate, training=self.training)\n        out = self.conv2(out)\n        if not self.equalInOut:\n            return torch.add(self.convShortcut(x), out)\n        else:\n            return torch.add(x, out)\n\n\nclass NetworkBlock(nn.Module):\n    def __init__(self, nb_layers, in_planes, out_planes, block, stride, dropRate=0.0):\n        super(NetworkBlock, self).__init__()\n        self.layer = self._make_layer(block, in_planes, out_planes, nb_layers, stride, dropRate)\n\n    def _make_layer(self, block, in_planes, out_planes, nb_layers, stride, dropRate):\n        layers = []\n        for i in range(nb_layers):\n            layers.append(block(i == 0 and in_planes or out_planes, out_planes, i == 0 and stride or 1, dropRate))\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        return self.layer(x)\n\n\nclass WideResNet(nn.Module):\n    def __init__(self, depth, num_classes, widen_factor=1, dropRate=0.0):\n        super(WideResNet, self).__init__()\n        nChannels = [16, 16 * widen_factor, 32 * widen_factor, 64 * widen_factor]\n        assert ((depth - 4) % 6 == 0)\n        n = (depth - 4) // 6\n        block = BasicBlock\n        # 1st conv before any network block\n        self.conv1 = nn.Conv2d(3, nChannels[0], kernel_size=3, stride=1,\n                               padding=1, bias=False)\n        # 1st block\n        self.block1 = NetworkBlock(n, nChannels[0], nChannels[1], block, 1, dropRate)\n        # 2nd block\n        self.block2 = NetworkBlock(n, nChannels[1], nChannels[2], block, 2, dropRate)\n        # 3rd block\n        self.block3 = NetworkBlock(n, nChannels[2], nChannels[3], block, 2, dropRate)\n        # global average pooling and classifier\n        self.bn1 = nn.BatchNorm2d(nChannels[3])\n        self.relu = nn.ReLU(inplace=True)\n        self.fc = nn.Linear(nChannels[3], num_classes)\n        self.nChannels = nChannels[3]\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n                m.weight.data.normal_(0, math.sqrt(2. / n))\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n            elif isinstance(m, nn.Linear):\n                m.bias.data.zero_()\n\n    def forward(self, x):\n        out = self.conv1(x)\n        out = self.block1(out)\n        out = self.block2(out)\n        out = self.block3(out)\n        out = self.relu(self.bn1(out))\n        out = F.avg_pool2d(out, 8)\n        out = out.view(-1, self.nChannels)\n        return self.fc(out)\n'"
MNIST/models/convnet.py,4,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\ndef gelu(x):\n    return torch.sigmoid(1.702 * x) * x\n    # return 0.5 * x * (1 + torch.tanh(x * 0.7978845608 * (1 + 0.044715 * x * x)))\n\n\nclass ConvNet(nn.Module):\n\n    def __init__(self):\n        super(ConvNet, self).__init__()\n        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n        self.conv2_drop = nn.Dropout2d()\n        self.fc1 = nn.Linear(320, 50)\n        self.fc2 = nn.Linear(50, 10)\n\n    def forward(self, x):\n        x = gelu(F.max_pool2d(self.conv1(x), 2))\n        x = gelu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n        x = x.view(-1, 320)\n        x = gelu(self.fc1(x))\n        # x = F.dropout(x)\n        return self.fc2(x)\n'"
NLP_classification/20newsgroups/reformat_20ng.py,0,"b""import numpy as np\nimport nltk.data\nimport os\nimport csv\n\ntrain_path = './orig_data/20ng-train-no-short.txt'\ntest_path = './orig_data/20ng-test-no-short.txt'\n\nclass_names = ['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware',\n\t\t\t   'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles',\n\t\t\t   'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med',\n\t\t\t   'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast',\n\t\t\t   'talk.politics.misc', 'talk.religion.misc']\n\ndef reformat(in_path, out_path):\n\twith open(in_path, 'r') as f:\n\t\tlines = f.readlines()\n\n\tnew_lines = [line.split() for line in lines]\n\tlabels = [line[0] for line in new_lines]  # extract label from line\n\tlabels = [class_names.index(label) for label in labels]  # switch to index\n\tdata = [' '.join(line[1:]) for line in new_lines]  # extract text from line\n\n\twith open(out_path, 'w') as myfile:\n\t\twr = csv.writer(myfile)\n\t\tfor label, inp in zip(labels, data):\n\t\t\twr.writerow([label, inp])\n\nreformat(train_path, './20ng-train.txt')\nreformat(test_path, './20ng-test.txt')"""
NLP_classification/multi30k/reformat_multi30k.py,0,"b'import numpy as np\nimport nltk.data\nimport os\nimport csv\n\n\ntrain_path = \'./orig_data/train.en\'\ntest_path = \'./orig_data/val.en\'\n\ndef reformat(in_path, out_path):\n\twith open(in_path, \'r\') as f:\n\t\tlines = f.readlines()\n\n\n\tnew_lines = []\n\n\tfor i in range(len(lines)):\n\t\tnew_line = lines[i].rstrip(\'\\n\')\n\t\tnew_line = "" \'s "".join(new_line.split(""\'s""))\n\t\tnew_line = "" \'d "".join(new_line.split(""\'d""))\n\t\tnew_line = "" ,"".join(new_line.split("",""))\n\t\tnew_line = "" n\'t "".join(new_line.split(""n\'t""))\n\t\tnew_line = \' "" \'.join(new_line.split(\'""\'))\n\t\tif len(new_line) == 0:\n\t\t\tcontinue\n\t\tif new_line[-1] in [\'?\', \'.\', \'!\']:\n\t\t\tnew_line = new_line[:-1] + \' \' + new_line[-1]\n\n\t\tnew_lines.append(new_line.lower())\n\n\twith open(out_path, \'w\') as myfile:\n\t\twr = csv.writer(myfile)\n\t\tfor line in new_lines:\n\t\t\twr.writerow([line])\n\nreformat(train_path, \'./train.txt\')\nreformat(test_path, \'./val.txt\')'"
NLP_classification/utils/calibration_tools.py,2,"b'import numpy as np\nimport cvxpy as cx\nimport torch\nimport torch.nn.functional as F\nfrom torch.autograd import Variable as V\n\n\ndef rms_calib_err(confidence, correct, p=\'2\', beta=100):\n    # beta is target bin size\n    idxs = np.argsort(confidence)\n    confidence = confidence[idxs]\n    correct = correct[idxs]\n    bins = [[i * beta, (i + 1) * beta] for i in range(len(confidence) // beta)]\n    bins[-1] = [bins[-1][0], len(confidence)]\n\n    cerr = 0\n    total_examples = len(confidence)\n    for i in range(len(bins) - 1):\n        bin_confidence = confidence[bins[i][0]:bins[i][1]]\n        bin_correct = correct[bins[i][0]:bins[i][1]]\n        num_examples_in_bin = len(bin_confidence)\n\n        if num_examples_in_bin > 0:\n            difference = np.abs(np.nanmean(bin_confidence) - np.nanmean(bin_correct))\n\n            if p == \'2\':\n                cerr += num_examples_in_bin / total_examples * np.square(difference)\n            elif p == \'1\':\n                cerr += num_examples_in_bin / total_examples * difference\n            elif p == \'infty\' or p == \'infinity\' or p == \'max\':\n                cerr = np.maximum(cerr, difference)\n            else:\n                assert False, ""p must be \'1\', \'2\', or \'infty\'""\n\n    if p == \'2\':\n        cerr = np.sqrt(cerr)\n\n    return cerr\n\n\ndef soft_f1(confidence, correct):\n    wrong = 1 - correct\n\n    # the incorrectly classified samples are our interest\n    # so they make the positive class\n    tp_soft = np.sum((1 - confidence) * wrong)\n    fp_soft = np.sum((1 - confidence) * correct)\n    fn_soft = np.sum(confidence * wrong)\n\n#    return 2 * tp_soft / (2 * tp_soft + fn_soft + fp_soft)\n    return 2 * ((1 - confidence) * wrong).sum()/(1 - confidence + wrong).sum()\n\n\ndef tune_temp(logits, labels, correct):\n    logits = np.array(logits)\n    set_size = np.array(logits).shape[0]\n\n    t = cx.Variable()\n\n    expr = sum([cx.Minimize(cx.log_sum_exp(logits[i, :] * t) - logits[i, labels[i]] * t)\n                for i in range(set_size)])\n    p = cx.Problem(expr, [0.25 <= t, t <= 4])\n    p.solve()\n\n    t = 1 / t.value\n\n    return t\n\n\narr = lambda x: np.array(x)\n\n\ndef show_calibration_results(confidence, correct, method_name=\'Baseline\'):\n\n    print(\'\\t\\t\\t\\t\' + method_name)\n    print(\'RMS Calib Error (%): \\t\\t{:.2f}\'.format(\n        100 * rms_calib_err(confidence, correct, p=\'2\')))\n\n    print(\'MAV Calib Error (%): \\t\\t{:.2f}\'.format(\n        100 * rms_calib_err(confidence, correct, p=\'1\')))\n\n    # print(\'Max Calib Error (%): \\t\\t{:.2f}\'.format(\n    #     100 * rms_calib_err(confidence, correct, p=\'infty\')))\n\n    print(\'Soft F1-Score (%): \\t\\t{:.2f}\'.format(\n        100 * soft_f1(confidence, correct)))\n\n    # add error detection measures?\n'"
NLP_classification/utils/cifar_resnet.py,4,"b'import math\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass BasicBlock(nn.Module):\n    def __init__(self, in_planes, out_planes, stride, dropRate=0.0):\n        super(BasicBlock, self).__init__()\n        self.bn1 = nn.BatchNorm2d(in_planes)\n        self.relu1 = nn.ReLU(inplace=True)\n        self.conv1 = nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n                               padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(out_planes)\n        self.relu2 = nn.ReLU(inplace=True)\n        self.conv2 = nn.Conv2d(out_planes, out_planes, kernel_size=3, stride=1,\n                               padding=1, bias=False)\n        self.droprate = dropRate\n        self.equalInOut = (in_planes == out_planes)\n        self.convShortcut = (not self.equalInOut) and nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride,\n                                                                padding=0, bias=False) or None\n\n    def forward(self, x):\n        if not self.equalInOut:\n            x = self.relu1(self.bn1(x))\n        else:\n            out = self.relu1(self.bn1(x))\n        if self.equalInOut:\n            out = self.relu2(self.bn2(self.conv1(out)))\n        else:\n            out = self.relu2(self.bn2(self.conv1(x)))\n        if self.droprate > 0:\n            out = F.dropout(out, p=self.droprate, training=self.training)\n        out = self.conv2(out)\n        if not self.equalInOut:\n            return torch.add(self.convShortcut(x), out)\n        else:\n            return torch.add(x, out)\n\n\nclass NetworkBlock(nn.Module):\n    def __init__(self, nb_layers, in_planes, out_planes, block, stride, dropRate=0.0):\n        super(NetworkBlock, self).__init__()\n        self.layer = self._make_layer(block, in_planes, out_planes, nb_layers, stride, dropRate)\n\n    def _make_layer(self, block, in_planes, out_planes, nb_layers, stride, dropRate):\n        layers = []\n        for i in range(nb_layers):\n            layers.append(block(i == 0 and in_planes or out_planes, out_planes, i == 0 and stride or 1, dropRate))\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        return self.layer(x)\n\n\nclass WideResNet(nn.Module):\n    def __init__(self, depth, num_classes, widen_factor=1, dropRate=0.0):\n        super(WideResNet, self).__init__()\n        nChannels = [16, 16 * widen_factor, 32 * widen_factor, 64 * widen_factor]\n        assert ((depth - 4) % 6 == 0)\n        n = (depth - 4) // 6\n        block = BasicBlock\n        # 1st conv before any network block\n        self.conv1 = nn.Conv2d(3, nChannels[0], kernel_size=3, stride=1,\n                               padding=1, bias=False)\n        # 1st block\n        self.block1 = NetworkBlock(n, nChannels[0], nChannels[1], block, 1, dropRate)\n        # 2nd block\n        self.block2 = NetworkBlock(n, nChannels[1], nChannels[2], block, 2, dropRate)\n        # 3rd block\n        self.block3 = NetworkBlock(n, nChannels[2], nChannels[3], block, 2, dropRate)\n        # global average pooling and classifier\n        self.bn1 = nn.BatchNorm2d(nChannels[3])\n        self.relu = nn.ReLU(inplace=True)\n        self.fc = nn.Linear(nChannels[3], num_classes)\n        self.nChannels = nChannels[3]\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n                m.weight.data.normal_(0, math.sqrt(2. / n))\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n            elif isinstance(m, nn.Linear):\n                m.bias.data.zero_()\n\n    def forward(self, x):\n        out = self.conv1(x)\n        out = self.block1(out)\n        out = self.block2(out)\n        out = self.block3(out)\n        out = self.relu(self.bn1(out))\n        out = F.avg_pool2d(out, 8)\n        out = out.view(-1, self.nChannels)\n        return self.fc(out)\n'"
NLP_classification/utils/display_results.py,0,"b'import numpy as np\nimport sklearn.metrics as sk\n\nrecall_level = 0.95\n\n\ndef stable_cumsum(arr, rtol=1e-05, atol=1e-08):\n    """"""Use high precision for cumsum and check that final value matches sum\n    Parameters\n    ----------\n    arr : array-like\n        To be cumulatively summed as flat\n    rtol : float\n        Relative tolerance, see ``np.allclose``\n    atol : float\n        Absolute tolerance, see ``np.allclose``\n    """"""\n    out = np.cumsum(arr, dtype=np.float64)\n    expected = np.sum(arr, dtype=np.float64)\n    if not np.allclose(out[-1], expected, rtol=rtol, atol=atol):\n        raise RuntimeError(\'cumsum was found to be unstable: \'\n                           \'its last element does not correspond to sum\')\n    return out\n\n\ndef fpr_and_fdr_at_recall(y_true, y_score, recall_level=recall_level,\n                          pos_label=None):\n    classes = np.unique(y_true)\n    if (pos_label is None and\n            not (np.array_equal(classes, [0, 1]) or\n                     np.array_equal(classes, [-1, 1]) or\n                     np.array_equal(classes, [0]) or\n                     np.array_equal(classes, [-1]) or\n                     np.array_equal(classes, [1]))):\n        raise ValueError(""Data is not binary and pos_label is not specified"")\n    elif pos_label is None:\n        pos_label = 1.\n\n    # make y_true a boolean vector\n    y_true = (y_true == pos_label)\n\n    # sort scores and corresponding truth values\n    desc_score_indices = np.argsort(y_score, kind=""mergesort"")[::-1]\n    y_score = y_score[desc_score_indices]\n    y_true = y_true[desc_score_indices]\n\n    # y_score typically has many tied values. Here we extract\n    # the indices associated with the distinct values. We also\n    # concatenate a value for the end of the curve.\n    distinct_value_indices = np.where(np.diff(y_score))[0]\n    threshold_idxs = np.r_[distinct_value_indices, y_true.size - 1]\n\n    # accumulate the true positives with decreasing threshold\n    tps = stable_cumsum(y_true)[threshold_idxs]\n    fps = 1 + threshold_idxs - tps      # add one because of zero-based indexing\n\n    thresholds = y_score[threshold_idxs]\n\n    recall = tps / tps[-1]\n\n    last_ind = tps.searchsorted(tps[-1])\n    sl = slice(last_ind, None, -1)      # [last_ind::-1]\n    recall, fps, tps, thresholds = np.r_[recall[sl], 1], np.r_[fps[sl], 0], np.r_[tps[sl], 0], thresholds[sl]\n\n    cutoff = np.argmin(np.abs(recall - recall_level))\n\n    return fps[cutoff] / (np.sum(np.logical_not(y_true))), fps[cutoff]/(fps[cutoff] + tps[cutoff])\n\n\ndef show_performance(pos, neg, expected_ap=1 / (1 + 10.), method_name=\'Ours\', recall_level=recall_level):\n    \'\'\'\n    :param pos: 1\'s class, class to detect, outliers, or wrongly predicted\n    example scores from the baseline\n    :param neg: 0\'s class scores generated by the baseline\n    :param expected_ap: this is changed from the default for failure detection\n    \'\'\'\n    pos = np.array(pos).reshape((-1, 1))\n    neg = np.array(neg).reshape((-1, 1))\n    examples = np.squeeze(np.vstack((pos, neg)))\n    labels = np.zeros(len(examples), dtype=np.int32)\n    labels[:len(pos)] += 1\n\n    auroc = sk.roc_auc_score(labels, examples)\n    aupr = sk.average_precision_score(labels, examples)\n    fpr, fdr = fpr_and_fdr_at_recall(labels, examples, recall_level)\n\n    print(\'\\t\\t\\t\' + method_name)\n    print(\'FPR{:d}:\\t\\t\\t{:.2f}\'.format(int(100 * recall_level), 100 * fpr))\n    print(\'AUROC:\\t\\t\\t{:.2f}\'.format(100 * auroc))\n    print(\'AUPR:\\t\\t\\t{:.2f}\'.format(100 * aupr))\n    # print(\'FDR{:d}:\\t\\t\\t{:.2f}\'.format(int(100 * recall_level), 100 * fdr))\n\n\ndef get_performance(pos, neg, expected_ap=1 / (1 + 10.), method_name=\'Ours\', recall_level=recall_level):\n    \'\'\'\n    :param pos: 1\'s class, class to detect, outliers, or wrongly predicted\n    example scores from the baseline\n    :param neg: 0\'s class scores generated by the baseline\n    :param expected_ap: this is changed from the default for failure detection\n    \'\'\'\n    pos = np.array(pos).reshape((-1, 1))\n    neg = np.array(neg).reshape((-1, 1))\n    examples = np.squeeze(np.vstack((pos, neg)))\n    labels = np.zeros(len(examples), dtype=np.int32)\n    labels[:len(pos)] += 1\n\n    auroc = sk.roc_auc_score(labels, examples)\n    aupr = sk.average_precision_score(labels, examples)\n    fpr, fdr = fpr_and_fdr_at_recall(labels, examples, recall_level)\n\n    # print(\'\\t\\t\\t\' + method_name)\n    # print(\'FPR{:d}:\\t\\t\\t{:.2f}\'.format(int(100 * recall_level), 100 * fpr))\n    # print(\'AUROC:\\t\\t\\t{:.2f}\'.format(100 * auroc))\n    # print(\'AUPR:\\t\\t\\t{:.2f}\'.format(100 * aupr))\n    # print(\'FDR{:d}:\\t\\t\\t{:.2f}\'.format(int(100 * recall_level), 100 * fdr))\n\n    return fpr, auroc, aupr\n\n\ndef show_performance_comparison(pos_base, neg_base, pos_ours, neg_ours, baseline_name=\'Baseline\',\n                                alternative_name=\'Ours\', expected_ap=1 / (1 + 10.), recall_level=recall_level):\n    \'\'\'\n    :param pos_base: 1\'s class, class to detect, outliers, or wrongly predicted\n    example scores from the baseline\n    :param neg_base: 0\'s class scores generated by the baseline\n    :param expected_ap: this is changed from the default for failure detection\n    \'\'\'\n    pos_base = np.array(pos_base).reshape((-1, 1))\n    neg_base = np.array(neg_base).reshape((-1, 1))\n    examples_base = np.squeeze(np.vstack((pos_base, neg_base)))\n    labels_base = np.zeros(len(examples_base), dtype=np.int32)\n    labels_base[:len(pos_base)] += 1\n\n    auroc_base = sk.roc_auc_score(labels_base, examples_base)\n    aupr_base = sk.average_precision_score(labels_base, examples_base)\n    fpr_base, fdr_base = fpr_and_fdr_at_recall(labels_base, examples_base)\n\n    del pos_base; del neg_base\n\n    pos_ours = np.array(pos_ours).reshape((-1, 1))\n    neg_ours = np.array(neg_ours).reshape((-1, 1))\n    examples_ours = np.squeeze(np.vstack((pos_ours, neg_ours)))\n    labels_ours = np.zeros(len(examples_ours), dtype=np.int32)\n    labels_ours[:len(pos_ours)] += 1\n\n    auroc_ours = sk.roc_auc_score(labels_ours, examples_ours)\n    aupr_ours = sk.average_precision_score(labels_ours, examples_ours)\n    fpr_ours, fdr_ours = fpr_and_fdr_at_recall(labels_ours, examples_ours)\n\n    print(\'\\t\\t\\t\' + baseline_name + \'\\t\' + alternative_name)\n    print(\'FPR{:d}:\\t\\t\\t{:.2f}\\t\\t{:.2f}\'.format(\n        int(100 * recall_level), 100 * fpr_base, 100 * fpr_ours))\n    print(\'AUROC:\\t\\t\\t{:.2f}\\t\\t{:.2f}\'.format(\n        100 * auroc_base, 100 * auroc_ours))\n    print(\'AUPR:\\t\\t\\t{:.2f}\\t\\t{:.2f}\'.format(\n        100 * aupr_base, 100 * aupr_ours))\n\n    # print(\'FDR{:d}:\\t\\t\\t{:.2f}\\t\\t{:.2f}\'.format(\n    #     int(100 * recall_level), 100 * fdr_base, 100 * fdr_ours))\n'"
NLP_classification/utils/log_sum_exp.py,2,"b'import torch\n\n\ndef log_sum_exp(value, dim=1, keepdim=False):\n    """"""Numerically stable implementation of the operation\n\n    value.exp().sum(dim, keepdim).log()\n    from https://github.com/pytorch/pytorch/issues/2591#issuecomment-338980717\n    """"""\n    m, _ = torch.max(value, dim=dim, keepdim=True)\n    value0 = value - m\n    if keepdim is False:\n        m = m.squeeze(dim)\n    return m + torch.log(torch.sum(torch.exp(value0), dim=dim, keepdim=keepdim))\n\n'"
NLP_classification/utils/lsun_loader.py,1,"b'import torch.utils.data as data\nfrom PIL import Image\nimport os\nimport os.path\nimport six\nimport string\nimport sys\n\nif sys.version_info[0] == 2:\n    import cPickle as pickle\nelse:\n    import pickle\n\n\nclass LSUNClass(data.Dataset):\n    def __init__(self, db_path, transform=None, target_transform=None):\n        import lmdb\n        self.db_path = db_path\n        self.env = lmdb.open(db_path, max_readers=1, readonly=True, lock=False,\n                             readahead=False, meminit=False)\n        with self.env.begin(write=False) as txn:\n            self.length = txn.stat()[\'entries\']\n        cache_file = \'_cache_\' + db_path.replace(\'/\', \'_\')\n        if os.path.isfile(cache_file):\n            self.keys = pickle.load(open(cache_file, ""rb""))\n        else:\n            with self.env.begin(write=False) as txn:\n                self.keys = [key for key, _ in txn.cursor()]\n            pickle.dump(self.keys, open(cache_file, ""wb""))\n        self.transform = transform\n        self.target_transform = target_transform\n\n    def __getitem__(self, index):\n        img, target = None, None\n        env = self.env\n        with env.begin(write=False) as txn:\n            imgbuf = txn.get(self.keys[index])\n\n        buf = six.BytesIO()\n        buf.write(imgbuf)\n        buf.seek(0)\n        img = Image.open(buf).convert(\'RGB\')\n\n        if self.transform is not None:\n            img = self.transform(img)\n\n        if self.target_transform is not None:\n            target = self.target_transform(target)\n\n        return img, target\n\n    def __len__(self):\n        return self.length\n\n    def __repr__(self):\n        return self.__class__.__name__ + \' (\' + self.db_path + \')\'\n\n\nclass LSUN(data.Dataset):\n    """"""\n    `LSUN <http://lsun.cs.princeton.edu>`_ dataset.\n\n    Args:\n        db_path (string): Root directory for the database files.\n        classes (string or list): One of {\'train\', \'val\', \'test\'} or a list of\n            categories to load. e,g. [\'bedroom_train\', \'church_train\'].\n        transform (callable, optional): A function/transform that  takes in an PIL image\n            and returns a transformed version. E.g, ``transforms.RandomCrop``\n        target_transform (callable, optional): A function/transform that takes in the\n            target and transforms it.\n    """"""\n\n    def __init__(self, db_path, classes=\'train\',\n                 transform=None, target_transform=None):\n        categories = [\'bedroom\', \'bridge\', \'church_outdoor\', \'classroom\',\n                      \'conference_room\', \'dining_room\', \'kitchen\',\n                      \'living_room\', \'restaurant\', \'tower\']\n        dset_opts = [\'train\', \'val\', \'test\']\n        self.db_path = db_path\n        if type(classes) == str and classes in dset_opts:\n            if classes == \'test\':\n                classes = [classes]\n            else:\n                classes = [c + \'_\' + classes for c in categories]\n        self.classes = classes\n\n        # for each class, create an LSUNClassDataset\n        self.dbs = []\n        for c in self.classes:\n            self.dbs.append(LSUNClass(\n                db_path=db_path + \'/\' + c + \'_lmdb\',\n                transform=transform))\n\n        self.indices = []\n        count = 0\n        for db in self.dbs:\n            count += len(db)\n            self.indices.append(count)\n\n        self.length = count\n        self.target_transform = target_transform\n\n    def __getitem__(self, index):\n        """"""\n        Args:\n            index (int): Index\n\n        Returns:\n            tuple: Tuple (image, target) where target is the index of the target category.\n        """"""\n        target = 0\n        sub = 0\n        for ind in self.indices:\n            if index < ind:\n                break\n            target += 1\n            sub = ind\n\n        db = self.dbs[target]\n        index = index - sub\n\n        if self.target_transform is not None:\n            target = self.target_transform(target)\n\n        img, _ = db[index]\n        return img, target\n\n    def __len__(self):\n        return self.length\n\n    def __repr__(self):\n        return self.__class__.__name__ + \' (\' + self.db_path + \')\'\n'"
NLP_classification/utils/svhn_loader.py,1,"b'import torch.utils.data as data\nfrom PIL import Image\nimport os\nimport os.path\nimport numpy as np\n\n\nclass SVHN(data.Dataset):\n    url = """"\n    filename = """"\n    file_md5 = """"\n\n    split_list = {\n        \'train\': [""http://ufldl.stanford.edu/housenumbers/train_32x32.mat"",\n                  ""train_32x32.mat"", ""e26dedcc434d2e4c54c9b2d4a06d8373""],\n        \'test\': [""http://ufldl.stanford.edu/housenumbers/test_32x32.mat"",\n                 ""test_32x32.mat"", ""eb5a983be6a315427106f1b164d9cef3""],\n        \'extra\': [""http://ufldl.stanford.edu/housenumbers/extra_32x32.mat"",\n                  ""extra_32x32.mat"", ""a93ce644f1a588dc4d68dda5feec44a7""],\n        \'train_and_extra\': [\n                [""http://ufldl.stanford.edu/housenumbers/train_32x32.mat"",\n                 ""train_32x32.mat"", ""e26dedcc434d2e4c54c9b2d4a06d8373""],\n                [""http://ufldl.stanford.edu/housenumbers/extra_32x32.mat"",\n                 ""extra_32x32.mat"", ""a93ce644f1a588dc4d68dda5feec44a7""]]}\n\n    def __init__(self, root, split=\'train\',\n                 transform=None, target_transform=None, download=False):\n        self.root = root\n        self.transform = transform\n        self.target_transform = target_transform\n        self.split = split  # training set or test set or extra set\n\n        if self.split not in self.split_list:\n            raise ValueError(\'Wrong split entered! Please use split=""train"" \'\n                             \'or split=""extra"" or split=""test"" \'\n                             \'or split=""train_and_extra"" \')\n\n        if self.split == ""train_and_extra"":\n            self.url = self.split_list[split][0][0]\n            self.filename = self.split_list[split][0][1]\n            self.file_md5 = self.split_list[split][0][2]\n        else:\n            self.url = self.split_list[split][0]\n            self.filename = self.split_list[split][1]\n            self.file_md5 = self.split_list[split][2]\n\n        # import here rather than at top of file because this is\n        # an optional dependency for torchvision\n        import scipy.io as sio\n\n        # reading(loading) mat file as array\n        loaded_mat = sio.loadmat(os.path.join(root, self.filename))\n\n        if self.split == ""test"":\n            self.test_data = loaded_mat[\'X\']\n            self.test_labels = loaded_mat[\'y\']\n            # Note label 10 == 0 so modulo operator required\n            self.test_labels %= 10    # convert to zero-based indexing\n            self.test_data = np.transpose(self.test_data, (3, 2, 0, 1))\n        else:\n            self.train_data = loaded_mat[\'X\']\n            self.train_labels = loaded_mat[\'y\']\n\n            if self.split == ""train_and_extra"":\n                extra_filename = self.split_list[split][1][1]\n                loaded_mat = sio.loadmat(os.path.join(root, extra_filename))\n                self.train_data = np.concatenate([self.train_data,\n                                                  loaded_mat[\'X\']], axis=3)\n                self.train_labels = np.vstack((self.train_labels,\n                                               loaded_mat[\'y\']))\n            # Note label 10 == 0 so modulo operator required\n            self.train_labels %= 10    # convert to zero-based indexing\n            self.train_data = np.transpose(self.train_data, (3, 2, 0, 1))\n\n    def __getitem__(self, index):\n        if self.split == ""test"":\n            img, target = self.test_data[index], self.test_labels[index]\n        else:\n            img, target = self.train_data[index], self.train_labels[index]\n\n        # doing this so that it is consistent with all other datasets\n        # to return a PIL Image\n        img = Image.fromarray(np.transpose(img, (1, 2, 0)))\n\n        if self.transform is not None:\n            img = self.transform(img)\n\n        if self.target_transform is not None:\n            target = self.target_transform(target)\n\n        return img, target\n\n    def __len__(self):\n        if self.split == ""test"":\n            return len(self.test_data)\n        else:\n            return len(self.train_data)\n\n    def _check_integrity(self):\n        root = self.root\n        if self.split == ""train_and_extra"":\n            md5 = self.split_list[self.split][0][2]\n            fpath = os.path.join(root, self.filename)\n            train_integrity = check_integrity(fpath, md5)\n            extra_filename = self.split_list[self.split][1][1]\n            md5 = self.split_list[self.split][1][2]\n            fpath = os.path.join(root, extra_filename)\n            return check_integrity(fpath, md5) and train_integrity\n        else:\n            md5 = self.split_list[self.split][2]\n            fpath = os.path.join(root, self.filename)\n            return check_integrity(fpath, md5)\n\n    def download(self):\n        if self.split == ""train_and_extra"":\n            md5 = self.split_list[self.split][0][2]\n            download_url(self.url, self.root, self.filename, md5)\n            extra_filename = self.split_list[self.split][1][1]\n            md5 = self.split_list[self.split][1][2]\n            download_url(self.url, self.root, extra_filename, md5)\n        else:\n            md5 = self.split_list[self.split][2]\n            download_url(self.url, self.root, self.filename, md5)\n'"
NLP_classification/utils/tiny_resnet.py,4,"b'import math\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass BasicBlock(nn.Module):\n    def __init__(self, in_planes, out_planes, stride, dropRate=0.0):\n        super(BasicBlock, self).__init__()\n        self.bn1 = nn.BatchNorm2d(in_planes)\n        self.relu1 = nn.ReLU(inplace=True)\n        self.conv1 = nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n                               padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(out_planes)\n        self.relu2 = nn.ReLU(inplace=True)\n        self.conv2 = nn.Conv2d(out_planes, out_planes, kernel_size=3, stride=1,\n                               padding=1, bias=False)\n        self.droprate = dropRate\n        self.equalInOut = (in_planes == out_planes)\n        self.convShortcut = (not self.equalInOut) and nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride,\n                                                                padding=0, bias=False) or None\n\n    def forward(self, x):\n        if not self.equalInOut:\n            x = self.relu1(self.bn1(x))\n        else:\n            out = self.relu1(self.bn1(x))\n        if self.equalInOut:\n            out = self.relu2(self.bn2(self.conv1(out)))\n        else:\n            out = self.relu2(self.bn2(self.conv1(x)))\n        if self.droprate > 0:\n            out = F.dropout(out, p=self.droprate, training=self.training)\n        out = self.conv2(out)\n        if not self.equalInOut:\n            return torch.add(self.convShortcut(x), out)\n        else:\n            return torch.add(x, out)\n\n\nclass NetworkBlock(nn.Module):\n    def __init__(self, nb_layers, in_planes, out_planes, block, stride, dropRate=0.0):\n        super(NetworkBlock, self).__init__()\n        self.layer = self._make_layer(block, in_planes, out_planes, nb_layers, stride, dropRate)\n\n    def _make_layer(self, block, in_planes, out_planes, nb_layers, stride, dropRate):\n        layers = []\n        for i in range(nb_layers):\n            layers.append(block(i == 0 and in_planes or out_planes, out_planes, i == 0 and stride or 1, dropRate))\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        return self.layer(x)\n\n\nclass WideResNet(nn.Module):\n    def __init__(self, depth, num_classes, widen_factor=1, dropRate=0.0):\n        super(WideResNet, self).__init__()\n        nChannels = [16, 16 * widen_factor, 32 * widen_factor, 64 * widen_factor]\n        assert ((depth - 4) % 6 == 0)\n        n = (depth - 4) // 6\n        block = BasicBlock\n        # 1st conv before any network block\n        self.conv1 = nn.Conv2d(3, nChannels[0], kernel_size=3, stride=1,\n                               padding=1, bias=False)\n        # 1st block\n        self.block1 = NetworkBlock(n, nChannels[0], nChannels[1], block, 1, dropRate)\n        # 2nd block\n        self.block2 = NetworkBlock(n, nChannels[1], nChannels[2], block, 2, dropRate)\n        # 3rd block\n        self.block3 = NetworkBlock(n, nChannels[2], nChannels[3], block, 2, dropRate)\n        # global average pooling and classifier\n        self.bn1 = nn.BatchNorm2d(nChannels[3])\n        self.relu = nn.ReLU(inplace=True)\n        self.fc = nn.Linear(nChannels[3], num_classes)\n        self.nChannels = nChannels[3]\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n                m.weight.data.normal_(0, math.sqrt(2. / n))\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n            elif isinstance(m, nn.Linear):\n                m.bias.data.zero_()\n\n    def forward(self, x):\n        out = self.conv1(x)\n        out = self.block1(out)\n        out = self.block2(out)\n        out = self.block3(out)\n        out = self.relu(self.bn1(out))\n        out = F.avg_pool2d(out, 16)\n        out = out.view(-1, self.nChannels)\n        return self.fc(out)\n'"
NLP_classification/utils/tinyimages_80mn_loader.py,1,"b'import numpy as np\nimport torch\nfrom bisect import bisect_left\n\n\nclass TinyImages(torch.utils.data.Dataset):\n\n    def __init__(self, transform=None, exclude_cifar=True):\n\n        data_file = open(\'/share/data/vision-greg/80million/tiny_images.bin\', ""rb"")\n\n        def load_image(idx):\n            data_file.seek(idx * 3072)\n            data = data_file.read(3072)\n            return np.fromstring(data, dtype=\'uint8\').reshape(32, 32, 3, order=""F"")\n\n        self.load_image = load_image\n\n        self.transform = transform\n        self.exclude_cifar = exclude_cifar\n\n        if exclude_cifar:\n            self.cifar_idxs = []\n            with open(\'/share/data/vision-greg2/OE/utils/80mn_cifar_idxs.txt\', \'r\') as idxs:\n                for idx in idxs:\n                    # indices in file take the 80mn database to start at 1, hence ""- 1""\n                    self.cifar_idxs.append(int(idx) - 1)\n            self.cifar_idxs = tuple(sorted(self.cifar_idxs))\n\n            def binary_search(x, hi=len(self.cifar_idxs)):\n                pos = bisect_left(self.cifar_idxs, x, 0, hi)  # find insertion position\n                return True if pos != hi and self.cifar_idxs[pos] == x else False\n\n            self.in_cifar = binary_search\n\n    def __getitem__(self, index):\n\n        if self.exclude_cifar:\n            while self.in_cifar(index):\n                index = np.random.randint(79302017)\n\n        img = self.load_image(index)\n        if self.transform is not None:\n            img = self.transform(img)\n\n        return img, 0  # 0 is the class\n\n    def __len__(self):\n        return 79302017\n'"
NLP_classification/wikitext_reformatted/reformat_wikitext.py,0,"b""import numpy as np\nimport nltk.data\nimport os\nimport csv\n\n# Usage: Uncomment wikitext-2 parts and comment wikitext-103 parts (at top and bottom) to switch.\n\n\n# wikitext_path = '../wikitext-2/wikitext-2/wiki.train.tokens'\nwikitext_path = '../wikitext-103/wikitext-103/wiki.train.tokens'\n\n# Get rid of headers\nwith open(wikitext_path, 'r') as f:\n\tlines = f.readlines()\n\nwith open('./tmp', 'w') as f:\n\tfor line in lines:\n\t\tif (line == ' \\n') or (line[:2] == ' ='):\n\t\t\tcontinue\n\n\t\tf.write(line + '\\n')\n\n# Separate sentences with NLTK\ntokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\nfp = open('./tmp')\ndata = fp.read()\nsentences = tokenizer.tokenize(data)\n\n# Remove sentences that tend to be poor quality\nnew_sentences = []\nfor sentence in sentences:\n\tif (sentence[0] == '@') or (sentence[0] == ' ') or (sentence[0] == '\\n') or (sentence[0] in ['0','1','2','3','4','5','6','7','8','9']):\n\t\tcontinue\n\tlength = len(sentence.split(' '))\n\tif length > 60 or length < 3:\n\t\tcontinue\n\tif '\\n' in sentence:\n\t\tcontinue\n\tnew_sentences.append(sentence)\n\nos.remove('./tmp')\n\n# Write output\n\n# with open('./wikitext_sentences', 'w') as f:\n# \tfor sentence in new_sentences:\n# \t\tf.write(sentence + '\\n')\n\nwith open('./wikitext103_sentences', 'w') as myfile:\n\twr = csv.writer(myfile)\n\tfor row in new_sentences:\n\t\twr.writerow([row])\n\n"""
NLP_classification/wmt16/concat_wmt_sentences.py,0,"b""import numpy as np\nimport os\n\nwmt16_en_de_path = './wmt16_en_de'\n\n# select the tokenized english test sets\nfiles_to_concat = []\nfor fname in os.listdir(wmt16_en_de_path):\n\tif '.tok.en' in fname:\n\t\tfiles_to_concat.append(fname)\n\n# convert each line in these files to lowercase, and concatenate them all together\nwith open('./wmt16_sentences', 'w') as f_out:\n\tfor fname in files_to_concat:\n\t\twith open(os.path.join(wmt16_en_de_path, fname), 'r') as f:\n\t\t\tlines = f.readlines()\n\t\t\tfor i, line in enumerate(lines):\n\t\t\t\tline_out = line.lower()\n\t\t\t\tf_out.write(line_out)\n"""
NLP_language_modeling/torchqrnn/__init__.py,0,"b'from .forget_mult import ForgetMult\nfrom .qrnn import QRNN, QRNNLayer\n'"
NLP_language_modeling/torchqrnn/forget_mult.py,17,"b'import math\nimport torch\nfrom torch.autograd import Variable\nfrom cupy.cuda import function\nfrom pynvrtc.compiler import Program\nfrom collections import namedtuple\n\n###\n\nkernel = \'\'\'\nextern ""C""\n__global__ void recurrent_forget_mult(float *dst, const float *f, const float *x, int SEQ, int BATCH, int HIDDEN)\n{\n  /*\n  Note: destination is assumed to be one timestep longer than f or x where dst[0] = h_{-1}\n  This means dst array has a separate index than that of f or x\n  */\n  int hid = blockIdx.x * blockDim.x + threadIdx.x;\n  int bid = blockIdx.y * blockDim.y + threadIdx.y;\n  if(hid >= HIDDEN || bid >= BATCH)\n     return;\n  //\n  for (int ts = 0 + 1; ts < SEQ + 1; ts++) {\n     // Good sanity check for debugging - only perform additions to a zeroed chunk of memory\n     // Addition seems atomic or near atomic - you should get incorrect answers if doubling up via threads\n     // Note: the index i needs to be offset by one as f[0] (f_t) is used for dst[1] (h_t) etc\n\n     // To move timesteps, we step HIDDEN * BATCH\n     // To move batches, we move HIDDEN\n     // To move neurons, we move +- 1\n     // Note: dst[dst_i] = ts * 100 + bid * 10 + hid; is useful for debugging\n\n     int i           = (ts - 1) * HIDDEN * BATCH + bid * HIDDEN + hid;\n     int dst_i       = (ts - 0) * HIDDEN * BATCH + bid * HIDDEN + hid;\n     int dst_iminus1 = (ts - 1) * HIDDEN * BATCH + bid * HIDDEN + hid;\n     dst[dst_i]      = f[i] * x[i];\n     dst[dst_i]      += (1 - f[i]) * dst[dst_iminus1];\n  }\n}\n\nextern ""C""\n__global__ void bwd_recurrent_forget_mult(const float *h, const float *f, const float *x, const float *gh, float *gf, float *gx, float *ghinit, int SEQ, int BATCH, int HIDDEN)\n{\n  /*\n  Note: h is assumed to be one timestep longer than f, x, gf, gx, or gh where dst[0] = h_{-1}\n  This means dst array has a separate index than that of f or x\n  */\n  int hid = blockIdx.x * blockDim.x + threadIdx.x;\n  int bid = blockIdx.y * blockDim.y + threadIdx.y;\n  if(hid >= HIDDEN || bid >= BATCH)\n     return;\n  //\n  double running_f = 0;\n  for (int ts = SEQ - 1 + 1; ts >= 0 + 1; ts--) {\n     int i           = (ts - 1) * HIDDEN * BATCH + bid * HIDDEN + hid;\n     int dst_i       = (ts - 0) * HIDDEN * BATCH + bid * HIDDEN + hid;\n     int dst_iminus1 = (ts - 1) * HIDDEN * BATCH + bid * HIDDEN + hid;\n     //\n     running_f       += gh[dst_iminus1];\n     // Gradient of X\n     gx[i]           = f[i] * running_f;\n     // Gradient of F\n     gf[i]           = (x[i] - h[dst_iminus1]) * running_f;\n     //\n     // The line below is likely more numerically stable than (1 - f[i]) * running_f;\n     running_f       = running_f - f[i] * running_f;\n  }\n  ghinit[bid * HIDDEN + hid] = running_f;\n}\n\'\'\'\n\n###\n\nclass CPUForgetMult(torch.nn.Module):\n    def __init__(self):\n        super(CPUForgetMult, self).__init__()\n\n    def forward(self, f, x, hidden_init=None):\n        result = []\n        ###\n        forgets = f.split(1, dim=0)\n        prev_h = hidden_init\n        for i, h in enumerate((f * x).split(1, dim=0)):\n            if prev_h is not None: h = h + (1 - forgets[i]) * prev_h\n            # h is (1, batch, hidden) when it needs to be (batch_hidden)\n            # Calling squeeze will result in badness if batch size is 1\n            h = h.view(h.size()[1:])\n            result.append(h)\n            prev_h = h\n        ###\n        return torch.stack(result)\n\n\nclass GPUForgetMult(torch.autograd.Function):\n    configured_gpus = {}\n    ptx = None\n    def __init__(self):\n        super(GPUForgetMult, self).__init__()\n\n    def compile(self):\n        if self.ptx is None:\n            program = Program(kernel.encode(), \'recurrent_forget_mult.cu\'.encode())\n            GPUForgetMult.ptx = program.compile()\n\n        if torch.cuda.current_device() not in GPUForgetMult.configured_gpus:\n            m = function.Module()\n            m.load(bytes(self.ptx.encode()))\n\n            self.forget_mult = m.get_function(\'recurrent_forget_mult\')\n            self.bwd_forget_mult = m.get_function(\'bwd_recurrent_forget_mult\')\n\n            Stream = namedtuple(\'Stream\', [\'ptr\'])\n            self.stream = Stream(ptr=torch.cuda.current_stream().cuda_stream)\n\n            GPUForgetMult.configured_gpus[torch.cuda.current_device()] = (self.forget_mult, self.bwd_forget_mult, self.stream)\n\n        self.forget_mult, self.bwd_forget_mult, self.stream = GPUForgetMult.configured_gpus[torch.cuda.current_device()]\n\n    def forward(self, f, x, hidden_init=None):\n        self.compile()\n        seq_size, batch_size, hidden_size = f.size()\n        result = f.new(seq_size + 1, batch_size, hidden_size)\n        # We only zero the result array (result[0]) if we don\'t set a hidden initial state\n        # All other values (result[1:]) are overwritten by default\n        if hidden_init is not None: result[0, :, :] = hidden_init\n        else: result = result.zero_()\n        ###\n        grid_hidden_size = min(hidden_size, 512)\n        grid = (math.ceil(hidden_size / grid_hidden_size), batch_size)\n        self.forget_mult(grid=grid, block=(grid_hidden_size, 1), args=[result.data_ptr(), f.data_ptr(), x.data_ptr(), seq_size, batch_size, hidden_size], stream=self.stream)\n        self.save_for_backward(f, x, hidden_init)\n        self.result = result\n        return result[1:, :, :]\n\n    def backward(self, grad_h):\n        self.compile()\n        f, x, hidden_init = self.saved_tensors\n        h = self.result\n        ###\n        seq_size, batch_size, hidden_size = f.size()\n        # Zeroing is not necessary as these will be overwritten\n        grad_f = f.new(*f.size())\n        grad_x = f.new(*f.size())\n        grad_h_init = f.new(batch_size, hidden_size)\n        ###\n        grid_hidden_size = min(hidden_size, 512)\n        grid = (math.ceil(hidden_size / grid_hidden_size), batch_size)\n        self.bwd_forget_mult(grid=grid, block=(grid_hidden_size, 1), args=[h.data_ptr(), f.data_ptr(), x.data_ptr(), grad_h.data_ptr(), grad_f.data_ptr(), grad_x.data_ptr(), grad_h_init.data_ptr(), seq_size, batch_size, hidden_size], stream=self.stream)\n        ###\n        if hidden_init is not None:\n            return grad_f, grad_x, grad_h_init\n        return grad_f, grad_x\n\n\nclass ForgetMult(torch.nn.Module):\n    r""""""ForgetMult computes a simple recurrent equation:\n    h_t = f_t * x_t + (1 - f_t) * h_{t-1}\n\n    This equation is equivalent to dynamic weighted averaging.\n\n    Inputs: X, hidden\n        - X (seq_len, batch, input_size): tensor containing the features of the input sequence.\n        - F (seq_len, batch, input_size): tensor containing the forget gate values, assumed in range [0, 1].\n        - hidden_init (batch, input_size): tensor containing the initial hidden state for the recurrence (h_{t-1}).\n        - use_cuda: If True, use the fast element-wise CUDA kernel for recurrence. If False, uses naive for loop. Default: True.\n    """"""\n\n    def __init__(self):\n        super(ForgetMult, self).__init__()\n\n    def forward(self, f, x, hidden_init=None, use_cuda=True):\n        # Use CUDA by default unless it\'s available\n        use_cuda = use_cuda and torch.cuda.is_available()\n        # Ensure the user is aware when ForgetMult is not GPU version as it\'s far faster\n        if use_cuda: assert f.is_cuda and x.is_cuda, \'GPU ForgetMult with fast element-wise CUDA kernel requested but tensors not on GPU\'\n        ###\n        # Avoiding \'RuntimeError: expected a Variable argument, but got NoneType\' when hidden_init is None\n        if hidden_init is None: return GPUForgetMult()(f, x) if use_cuda else CPUForgetMult()(f, x)\n        return GPUForgetMult()(f, x, hidden_init) if use_cuda else CPUForgetMult()(f, x, hidden_init)\n\n###\n\nif __name__ == \'__main__\':\n    seq, batch, hidden = 35, 20, 650\n    # Larger input (batch * seq * hidden) results in excessive memory for gradient check\n    seq, batch, hidden = 3, 7, 19\n    a      = Variable(torch.rand(seq, batch, hidden).cuda(), requires_grad=True)\n    forget = Variable(torch.rand(seq, batch, hidden).cuda(), requires_grad=True)\n    last_h = Variable(torch.rand(batch, hidden).cuda(), requires_grad=True)\n\n    #seq, batch, hidden = 4, 1, 1\n    #a = Variable(torch.Tensor([0.75, 0.5, 0.9, 0.8]).view(seq, batch, hidden).cuda(), requires_grad=True)\n    #forget = Variable(torch.Tensor([0.25, 0.25, 0.5, 0.4]).view(seq, batch, hidden).cuda(), requires_grad=True)\n    #last_h = Variable(torch.Tensor([0]).view(batch, hidden).cuda(), requires_grad=True)\n    #print(forget, a, last_h)\n\n    print(\'CUDA forget mult\')\n    print(\'=-=-\' * 5)\n\n    resulta = ForgetMult()(forget, a, last_h, use_cuda=True)\n    print(resulta.size())\n    loss = resulta.pow(2).sum()\n    loss.backward()\n\n    print(\'Result =\', loss.data[0])\n    print(\'X grad =\', a.grad.mean().data[0])\n    print(\'Forget grad =\', forget.grad.mean().data[0])\n    print(\'Last H grad =\', last_h.grad.mean().data[0])\n\n    x_grad_copy = a.grad.clone()\n\n    print()\n    print(\'CPU forget mult\')\n    print(\'=-=-\' * 5)\n\n    a.grad.data *= 0\n    forget.grad.data *= 0\n    last_h.grad.data *= 0\n\n    resultb = ForgetMult()(forget, a, last_h, use_cuda=False)\n    print(resultb.size())\n    loss = resultb.pow(2).sum()\n    loss.backward()\n\n    print(\'Result =\', loss.data[0])\n    print(\'X grad =\', a.grad.mean().data[0])\n    print(\'Forget grad =\', forget.grad.mean().data[0])\n    print(\'Last H grad =\', last_h.grad.mean().data[0])\n\n    ###\n\n    print()\n    print(\'=-=-\' * 5)\n    print(\'(Xgrad - Xgrad).sum() =\', (x_grad_copy - a.grad).sum().data[0])\n    print(\'Residual error for result\')\n    print(\'=-=-\' * 5)\n    residual = (resulta - resultb)\n    print(residual.abs().sum().data[0])\n \n    # Had to loosen gradient checking, potentially due to general floating point badness?\n    from torch.autograd import gradcheck\n    inputs = [forget, a, last_h]\n    test = gradcheck(ForgetMult(), inputs, eps=1e-4, atol=1e-2)\n    print(test)\n'"
NLP_language_modeling/torchqrnn/qrnn.py,13,"b'import torch\nfrom torch import nn\nfrom torch.autograd import Variable\n\nif __name__ == \'__main__\':\n    from forget_mult import ForgetMult\nelse:\n    from .forget_mult import ForgetMult\n\n\nclass QRNNLayer(nn.Module):\n    r""""""Applies a single layer Quasi-Recurrent Neural Network (QRNN) to an input sequence.\n\n    Args:\n        input_size: The number of expected features in the input x.\n        hidden_size: The number of features in the hidden state h. If not specified, the input size is used.\n        save_prev_x: Whether to store previous inputs for use in future convolutional windows (i.e. for a continuing sequence such as in language modeling). If true, you must call reset to remove cached previous values of x. Default: False.\n        window: Defines the size of the convolutional window (how many previous tokens to look when computing the QRNN values). Supports 1 and 2. Default: 1.\n        zoneout: Whether to apply zoneout (i.e. failing to update elements in the hidden state) to the hidden state updates. Default: 0.\n        output_gate: If True, performs QRNN-fo (applying an output gate to the output). If False, performs QRNN-f. Default: True.\n        use_cuda: If True, uses fast custom CUDA kernel. If False, uses naive for loop. Default: True.\n\n    Inputs: X, hidden\n        - X (seq_len, batch, input_size): tensor containing the features of the input sequence.\n        - hidden (batch, hidden_size): tensor containing the initial hidden state for the QRNN.\n\n    Outputs: output, h_n\n        - output (seq_len, batch, hidden_size): tensor containing the output of the QRNN for each timestep.\n        - h_n (batch, hidden_size): tensor containing the hidden state for t=seq_len\n    """"""\n\n    def __init__(self, input_size, hidden_size=None, save_prev_x=False, zoneout=0, window=1, output_gate=True, use_cuda=True):\n        super(QRNNLayer, self).__init__()\n\n        assert window in [1, 2], ""This QRNN implementation currently only handles convolutional window of size 1 or size 2""\n        self.window = window\n        self.input_size = input_size\n        self.hidden_size = hidden_size if hidden_size else input_size\n        self.zoneout = zoneout\n        self.save_prev_x = save_prev_x\n        self.prevX = None\n        self.output_gate = output_gate\n        self.use_cuda = use_cuda\n\n        # One large matmul with concat is faster than N small matmuls and no concat\n        self.linear = nn.Linear(self.window * self.input_size, 3 * self.hidden_size if self.output_gate else 2 * self.hidden_size)\n\n    def reset(self):\n        # If you are saving the previous value of x, you should call this when starting with a new state\n        self.prevX = None\n\n    def forward(self, X, hidden=None):\n        seq_len, batch_size, _ = X.size()\n\n        source = None\n        if self.window == 1:\n            source = X\n        elif self.window == 2:\n            # Construct the x_{t-1} tensor with optional x_{-1}, otherwise a zeroed out value for x_{-1}\n            Xm1 = []\n            Xm1.append(self.prevX if self.prevX is not None else X[:1, :, :] * 0)\n            # Note: in case of len(X) == 1, X[:-1, :, :] results in slicing of empty tensor == bad\n            if len(X) > 1:\n                Xm1.append(X[:-1, :, :])\n            Xm1 = torch.cat(Xm1, 0)\n            # Convert two (seq_len, batch_size, hidden) tensors to (seq_len, batch_size, 2 * hidden)\n            source = torch.cat([X, Xm1], 2)\n\n        # Matrix multiplication for the three outputs: Z, F, O\n        Y = self.linear(source)\n        # Convert the tensor back to (batch, seq_len, len([Z, F, O]) * hidden_size)\n        if self.output_gate:\n            Y = Y.view(seq_len, batch_size, 3 * self.hidden_size)\n            Z, F, O = Y.chunk(3, dim=2)\n        else:\n            Y = Y.view(seq_len, batch_size, 2 * self.hidden_size)\n            Z, F = Y.chunk(2, dim=2)\n        ###\n        Z = torch.nn.functional.tanh(Z)\n        F = torch.nn.functional.sigmoid(F)\n\n        # If zoneout is specified, we perform dropout on the forget gates in F\n        # If an element of F is zero, that means the corresponding neuron keeps the old value\n        if self.zoneout:\n            if self.training:\n                mask = Variable(F.data.new(*F.size()).bernoulli_(1 - self.zoneout), requires_grad=False)\n                F = F * mask\n            else:\n                F *= 1 - self.zoneout\n\n        # Ensure the memory is laid out as expected for the CUDA kernel\n        # This is a null op if the tensor is already contiguous\n        Z = Z.contiguous()\n        F = F.contiguous()\n        # The O gate doesn\'t need to be contiguous as it isn\'t used in the CUDA kernel\n\n        # Forget Mult\n        # For testing QRNN without ForgetMult CUDA kernel, C = Z * F may be useful\n        C = ForgetMult()(F, Z, hidden, use_cuda=self.use_cuda)\n\n        # Apply (potentially optional) output gate\n        if self.output_gate:\n            H = torch.nn.functional.sigmoid(O) * C\n        else:\n            H = C\n\n        # In an optimal world we may want to backprop to x_{t-1} but ...\n        if self.window > 1 and self.save_prev_x:\n            self.prevX = Variable(X[-1:, :, :].data, requires_grad=False)\n\n        return H, C[-1:, :, :]\n\n\nclass QRNN(torch.nn.Module):\n    r""""""Applies a multiple layer Quasi-Recurrent Neural Network (QRNN) to an input sequence.\n\n    Args:\n        input_size: The number of expected features in the input x.\n        hidden_size: The number of features in the hidden state h. If not specified, the input size is used.\n        num_layers: The number of QRNN layers to produce.\n        layers: List of preconstructed QRNN layers to use for the QRNN module (optional).\n        save_prev_x: Whether to store previous inputs for use in future convolutional windows (i.e. for a continuing sequence such as in language modeling). If true, you must call reset to remove cached previous values of x. Default: False.\n        window: Defines the size of the convolutional window (how many previous tokens to look when computing the QRNN values). Supports 1 and 2. Default: 1.\n        zoneout: Whether to apply zoneout (i.e. failing to update elements in the hidden state) to the hidden state updates. Default: 0.\n        output_gate: If True, performs QRNN-fo (applying an output gate to the output). If False, performs QRNN-f. Default: True.\n        use_cuda: If True, uses fast custom CUDA kernel. If False, uses naive for loop. Default: True.\n\n    Inputs: X, hidden\n        - X (seq_len, batch, input_size): tensor containing the features of the input sequence.\n        - hidden (layers, batch, hidden_size): tensor containing the initial hidden state for the QRNN.\n\n    Outputs: output, h_n\n        - output (seq_len, batch, hidden_size): tensor containing the output of the QRNN for each timestep.\n        - h_n (layers, batch, hidden_size): tensor containing the hidden state for t=seq_len\n    """"""\n\n    def __init__(self, input_size, hidden_size,\n                 num_layers=1, bias=True, batch_first=False,\n                 dropout=0, bidirectional=False, layers=None, **kwargs):\n        assert bidirectional == False, \'Bidirectional QRNN is not yet supported\'\n        assert batch_first == False, \'Batch first mode is not yet supported\'\n        assert bias == True, \'Removing underlying bias is not yet supported\'\n\n        super(QRNN, self).__init__()\n\n        self.layers = torch.nn.ModuleList(layers if layers else [QRNNLayer(input_size if l == 0 else hidden_size, hidden_size, **kwargs) for l in range(num_layers)])\n\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.num_layers = len(layers) if layers else num_layers\n        self.bias = bias\n        self.batch_first = batch_first\n        self.dropout = dropout\n        self.bidirectional = bidirectional\n\n    def reset(self):\n        r\'\'\'If your convolutional window is greater than 1, you must reset at the beginning of each new sequence\'\'\'\n        [layer.reset() for layer in self.layers]\n\n    def forward(self, input, hidden=None):\n        next_hidden = []\n\n        for i, layer in enumerate(self.layers):\n            input, hn = layer(input, None if hidden is None else hidden[i])\n            next_hidden.append(hn)\n\n            if self.dropout != 0 and i < len(self.layers) - 1:\n                input = torch.nn.functional.dropout(input, p=self.dropout, training=self.training, inplace=False)\n\n        next_hidden = torch.cat(next_hidden, 0).view(self.num_layers, *next_hidden[0].size()[-2:])\n\n        return input, next_hidden\n\n\nif __name__ == \'__main__\':\n    seq_len, batch_size, hidden_size, input_size = 7, 20, 256, 32\n    size = (seq_len, batch_size, input_size)\n    X = torch.autograd.Variable(torch.rand(size), requires_grad=True).cuda()\n    qrnn = QRNN(input_size, hidden_size, num_layers=2, dropout=0.4)\n    qrnn.cuda()\n    output, hidden = qrnn(X)\n    assert list(output.size()) == [7, 20, 256]\n    assert list(hidden.size()) == [2, 20, 256]\n\n    ###\n\n    seq_len, batch_size, hidden_size = 2, 2, 16\n    seq_len, batch_size, hidden_size = 35, 8, 32\n    size = (seq_len, batch_size, hidden_size)\n    X = Variable(torch.rand(size), requires_grad=True).cuda()\n    print(X.size())\n\n    qrnn = QRNNLayer(hidden_size, hidden_size)\n    qrnn.cuda()\n    Y, _ = qrnn(X)\n\n    qrnn.use_cuda = False\n    Z, _ = qrnn(X)\n\n    diff = (Y - Z).sum().data[0]\n    print(\'Total difference between QRNN(use_cuda=True) and QRNN(use_cuda=False) results:\', diff)\n    assert diff < 1e-5, \'CUDA and non-CUDA QRNN layers return different results\'\n\n    from torch.autograd import gradcheck\n    inputs = [X,]\n    test = gradcheck(QRNNLayer(hidden_size, hidden_size).cuda(), inputs)\n    print(test)\n'"
SVHN/models/allconv.py,4,"b""import math\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass GELU(nn.Module):\n    def __init__(self):\n        super(GELU, self).__init__()\n\n    def forward(self, x):\n        return torch.sigmoid(1.702 * x) * x\n        # return 0.5 * x * (1 + torch.tanh(x * 0.7978845608 * (1 + 0.044715 * x * x)))\n\n\ndef make_layers(cfg):\n    layers = []\n    in_channels = 3\n    for v in cfg:\n        if v == 'Md':\n            layers += [nn.MaxPool2d(kernel_size=2, stride=2), nn.Dropout(p=0.5)]\n        elif v == 'A':\n            layers += [nn.AvgPool2d(kernel_size=8)]\n        elif v == 'NIN':\n            conv2d = nn.Conv2d(in_channels, in_channels, kernel_size=1, padding=1)\n            layers += [conv2d, nn.BatchNorm2d(in_channels), GELU()]      # nn.ReLU(inplace=True)]\n        elif v == 'nopad':\n            conv2d = nn.Conv2d(in_channels, in_channels, kernel_size=3, padding=0)\n            layers += [conv2d, nn.BatchNorm2d(in_channels), GELU()]      # nn.ReLU(inplace=True)]\n        else:\n            conv2d = nn.Conv2d(in_channels, v, kernel_size=3, padding=1)\n            layers += [conv2d, nn.BatchNorm2d(v), GELU()]    # nn.ReLU(inplace=True)]\n            in_channels = v\n    return nn.Sequential(*layers)\n\n\nclass AllConvNet(nn.Module):\n    def __init__(self, num_classes):\n        super(AllConvNet, self).__init__()\n\n        self.num_classes = num_classes\n\n        # if num_classes > 10:\n        #     self.width1, w1 = 128, 128\n        #     self.width2, w2 = 256, 256\n        # else:\n        #     self.width1, w1 = 96,  96\n        #     self.width2, w2 = 192, 192\n\n        self.width1, w1 = 96,  96\n        self.width2, w2 = 192, 192\n\n        self.features = make_layers([w1, w1, w1, 'Md', w2, w2, w2, 'Md', 'nopad', 'NIN', 'NIN', 'A'])\n        self.classifier = nn.Linear(self.width2, num_classes)\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n                m.weight.data.normal_(0, math.sqrt(2. / n))     # He initialization\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n            elif isinstance(m, nn.Linear):\n                m.bias.data.zero_()\n\n    def forward(self, x):\n        x = self.features(x)\n        x = x.view(x.size(0), -1)\n        x = self.classifier(x)\n        return x\n"""
SVHN/models/wrn.py,4,"b'import math\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass BasicBlock(nn.Module):\n    def __init__(self, in_planes, out_planes, stride, dropRate=0.0):\n        super(BasicBlock, self).__init__()\n        self.bn1 = nn.BatchNorm2d(in_planes)\n        self.relu1 = nn.ReLU(inplace=True)\n        self.conv1 = nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n                               padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(out_planes)\n        self.relu2 = nn.ReLU(inplace=True)\n        self.conv2 = nn.Conv2d(out_planes, out_planes, kernel_size=3, stride=1,\n                               padding=1, bias=False)\n        self.droprate = dropRate\n        self.equalInOut = (in_planes == out_planes)\n        self.convShortcut = (not self.equalInOut) and nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride,\n                                                                padding=0, bias=False) or None\n\n    def forward(self, x):\n        if not self.equalInOut:\n            x = self.relu1(self.bn1(x))\n        else:\n            out = self.relu1(self.bn1(x))\n        if self.equalInOut:\n            out = self.relu2(self.bn2(self.conv1(out)))\n        else:\n            out = self.relu2(self.bn2(self.conv1(x)))\n        if self.droprate > 0:\n            out = F.dropout(out, p=self.droprate, training=self.training)\n        out = self.conv2(out)\n        if not self.equalInOut:\n            return torch.add(self.convShortcut(x), out)\n        else:\n            return torch.add(x, out)\n\n\nclass NetworkBlock(nn.Module):\n    def __init__(self, nb_layers, in_planes, out_planes, block, stride, dropRate=0.0):\n        super(NetworkBlock, self).__init__()\n        self.layer = self._make_layer(block, in_planes, out_planes, nb_layers, stride, dropRate)\n\n    def _make_layer(self, block, in_planes, out_planes, nb_layers, stride, dropRate):\n        layers = []\n        for i in range(nb_layers):\n            layers.append(block(i == 0 and in_planes or out_planes, out_planes, i == 0 and stride or 1, dropRate))\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        return self.layer(x)\n\n\nclass WideResNet(nn.Module):\n    def __init__(self, depth, num_classes, widen_factor=1, dropRate=0.0):\n        super(WideResNet, self).__init__()\n        nChannels = [16, 16 * widen_factor, 32 * widen_factor, 64 * widen_factor]\n        assert ((depth - 4) % 6 == 0)\n        n = (depth - 4) // 6\n        block = BasicBlock\n        # 1st conv before any network block\n        self.conv1 = nn.Conv2d(3, nChannels[0], kernel_size=3, stride=1,\n                               padding=1, bias=False)\n        # 1st block\n        self.block1 = NetworkBlock(n, nChannels[0], nChannels[1], block, 1, dropRate)\n        # 2nd block\n        self.block2 = NetworkBlock(n, nChannels[1], nChannels[2], block, 2, dropRate)\n        # 3rd block\n        self.block3 = NetworkBlock(n, nChannels[2], nChannels[3], block, 2, dropRate)\n        # global average pooling and classifier\n        self.bn1 = nn.BatchNorm2d(nChannels[3])\n        self.relu = nn.ReLU(inplace=True)\n        self.fc = nn.Linear(nChannels[3], num_classes)\n        self.nChannels = nChannels[3]\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n                m.weight.data.normal_(0, math.sqrt(2. / n))\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n            elif isinstance(m, nn.Linear):\n                m.bias.data.zero_()\n\n    def forward(self, x):\n        out = self.conv1(x)\n        out = self.block1(out)\n        out = self.block2(out)\n        out = self.block3(out)\n        out = self.relu(self.bn1(out))\n        out = F.avg_pool2d(out, 8)\n        out = out.view(-1, self.nChannels)\n        return self.fc(out)\n'"
TinyImageNet/Tiny_ImageNet_dataset/make_tiny_imagenet.py,0,"b'import argparse, os, os.path, glob, random, sys, json\nfrom collections import defaultdict\nfrom lxml import objectify\n\nfrom scipy.misc import imread, imsave, imresize\nfrom matplotlib.patches import Rectangle\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ntrain_anns_path = \'/share/data/vision-greg/ImageNetbbox/clsloc/train\'\ntrain_image_dir = \'/share/data/vision-greg/ImageNet_flat/clsloc/images/train\'\nval_anns_path = \'/share/data/vision-greg/ImageNetbbox/clsloc/val\'\nval_image_dir = \'/share/data/vision-greg/ImageNet_flat/clsloc/images/val\'\n\n\ndef get_synset_stats():\n  with open(\'./words.txt\') as f:\n    wnid_to_words = dict(line.strip().split(\'\\t\') for line in f)\n\n  wnids = os.listdir(train_anns_path)\n  wnid_to_stats = {wnid: {} for wnid in wnids}\n  for i, wnid in enumerate(wnids):\n    synset_dir = os.path.join(train_anns_path, wnid)\n    bbox_files = os.listdir(synset_dir)\n    bbox_files = [os.path.join(synset_dir, x) for x in bbox_files]\n\n    glob_str = \'%s_*.JPEG\' % wnid\n    img_files = glob.glob(os.path.join(train_image_dir, glob_str))\n\n    wnid_to_stats[wnid][\'bbox_files\'] = bbox_files\n    wnid_to_stats[wnid][\'img_files\'] = img_files\n    wnid_to_stats[wnid][\'num_imgs_train\'] = len(img_files)\n    wnid_to_stats[wnid][\'num_loc_train\'] = len(bbox_files)\n    wnid_to_stats[wnid][\'words\'] = wnid_to_words[wnid]\n\n    print(i, file=sys.stderr)\n    print(\'%d\\t%s\\t%s\\t%d\\t%d\' % (\n        i, wnid, wnid_to_words[wnid], len(bbox_files), len(img_files)))\n\n    \ndef parse_xml_file(filename):\n  with open(filename, \'r\') as f:\n    xml = f.read()\n  ann = objectify.fromstring(xml)\n  if ann.filename != \'%s\':\n    img_filename = \'%s.JPEG\' % ann.filename\n  else:\n    img_filename = filename[filename.rfind(\'/\')+1:-3] + \'JPEG\'\n  bbox = ann.object.bndbox\n  bbox = [bbox.xmin, bbox.ymin, bbox.xmax, bbox.ymax]\n  bbox = [int(x) for x in bbox]\n  name = str(ann.object.name)\n  return img_filename, bbox, name\n\n\ndef resize_image(img, size, bbox=None, crop=True, show=False):\n  """"""\n  Resize an image and its bounding box to a square.\n\n  img - A numpy array with pixel data for the image to resize.\n  size - Integer giving the height and width of the resized image.\n  bbox - Optionally, a list [xmin, ymin, xmax, ymax] giving the coordinates\n         of a bounding box in the original image.\n  crop - If true, center crop the original image before resizing; this avoids\n         distortion in images with nonunit aspect ratio, but may also crop out\n         part of the object.\n  show - If true, show the original and resized image and bounding box.\n\n  Returns:\n  If bbox was passed: (img_resized, bbox_resized)\n  otherwise: img_resized\n  """"""\n\n  def draw_rect(coords):\n    width = coords[2] - coords[0]\n    height = coords[3] - coords[1]\n    rect = Rectangle((coords[0], coords[1]), width, height, \n                     fill=False, linewidth=2.0, ec=\'green\')\n    plt.gca().add_patch(rect)\n\n  img_resized = img\n  if bbox is not None:\n    bbox_resized = [x for x in bbox]\n  if crop:\n    h, w = img.shape[0], img.shape[1]\n    if h > w:\n      h0 = (h - w) // 2\n      if bbox is not None:\n        bbox_resized[1] -= h0\n        bbox_resized[3] -= h0\n      img_resized = img[h0:h0+w, :]\n    elif w > h:\n      w0 = (w - h) // 2\n      if bbox is not None:\n        bbox_resized[0] -= w0\n        bbox_resized[2] -= w0\n      img_resized = img[:, w0:w0+h]\n\n  if bbox is not None:\n    h_ratio = float(size) / img_resized.shape[0]\n    w_ratio = float(size) / img_resized.shape[1]\n    ratios = [w_ratio, h_ratio, w_ratio, h_ratio]\n    bbox_resized = [int(1 + r * (x - 1)) for x, r in zip(bbox_resized, ratios)]\n    bbox_resized = np.clip(bbox_resized, 0, size - 1)\n  img_resized = imresize(img_resized, (size, size))\n\n  if show:\n    plt.subplot(1, 2, 1)\n    plt.imshow(img)\n    if bbox is not None:\n      draw_rect(bbox)\n    plt.subplot(1, 2, 2)\n    plt.imshow(img_resized)\n    if bbox is not None:\n      draw_rect(bbox_resized)\n    plt.show()\n\n  if bbox is None:\n    return img_resized\n  else:\n    return img_resized, bbox_resized\n\n\ndef write_data_in_synset_folders(part_data, part, out_dir, image_size):\n  part_dir = os.path.join(out_dir, part)\n  os.mkdir(part_dir)\n  num_wnids = len(part_data)\n  for i, (wnid, wnid_data) in enumerate(part_data.items()):\n    print(\'Writing images for synset %d / %d of %s\' % (i + 1, num_wnids, part))\n    wnid_dir = os.path.join(part_dir, wnid)\n    os.mkdir(wnid_dir)\n    image_dir = os.path.join(wnid_dir, \'images\')\n    os.mkdir(image_dir)\n    boxes_filename = os.path.join(wnid_dir, \'%s_boxes.txt\' % wnid)\n    boxes_file = open(boxes_filename, \'w\')\n    for i, (img_filename, bbox) in enumerate(wnid_data):\n      out_img_filename = \'%s_%d.JPEG\' % (wnid, i)\n      full_out_img_filename = os.path.join(image_dir, out_img_filename)\n      try: img = imread(img_filename[:58+9] + \'/\' + img_filename[58:], mode=\'RGB\')\n      except: img = imread(img_filename, mode=\'RGB\')\n\n      img_resized, bbox_resized = resize_image(img, image_size, bbox)\n\n      #if img_resized.mode != ""RGB"": img_resized = img_resized.convert(mode=""RGB"")\n      #if img_resized.shape != (64,64,3):\n      #  if img_resized.shape == (64,64):\n      #    img_resized = np.array([img_resized, img_resized, img_resized]).transpose((1, 2, 0))\n      #  elif img_resized.shape == (64,64,4):\n      #    print(img_resized)\n\n      imsave(full_out_img_filename, img_resized)\n      boxes_file.write(\'%s\\t%d\\t%d\\t%d\\t%d\\n\' % (out_img_filename,\n                       bbox_resized[0], bbox_resized[1], bbox_resized[2], bbox_resized[3]))\n    boxes_file.close()\n\n\ndef write_data_in_one_folder(part_data, part, out_dir, image_size):\n  part_dir = os.path.join(out_dir, part)\n  os.mkdir(part_dir)\n\n  # First flatten the part data so we can shuffle it\n  part_data_flat = []\n  for wnid, wnid_data in part_data.items():\n    for (img_filename, bbox) in wnid_data:\n      part_data_flat.append((wnid, img_filename, bbox))\n\n  random.shuffle(part_data_flat)\n  image_dir = os.path.join(part_dir, \'images\')\n  os.mkdir(image_dir)\n\n  annotations_filename = os.path.join(part_dir, \'%s_annotations.txt\' % part)\n  annotations_file = open(annotations_filename, \'w\')\n  for i, (wnid, img_filename, bbox) in enumerate(part_data_flat):\n    if i % 100 == 0:\n      print(\'Finished writing %d / %d %s images\' % (i, len(part_data_flat), part))\n    out_img_filename = \'%s_%s.JPEG\' % (part, i)\n    full_out_img_filename = os.path.join(image_dir, out_img_filename)\n    try: img = imread(img_filename[:58+9] + \'/\' + img_filename[58:], mode=\'RGB\')\n    except: img = imread(img_filename, mode=\'RGB\')\n\n    img_resized, bbox_resized = resize_image(img, image_size, bbox)\n\n    imsave(full_out_img_filename, img_resized)\n    annotations_file.write(\'%s\\t%s\\t%d\\t%d\\t%d\\t%d\\n\' % (\n        out_img_filename, wnid,\n        bbox_resized[0], bbox_resized[1], bbox_resized[2], bbox_resized[3]))\n  annotations_file.close()\n\n\ndef make_tiny_imagenet(wnids, num_train, num_val, out_dir, image_size=50, test=False):\n  if os.path.isdir(out_dir):\n    print(\'Output directory already exists\')\n    return\n\n  # dataset[\'train\'][\'n123\'][0] = (filename, (xmin, ymin, xmax, xmax))\n  # gives one example of an image and bbox for synset n123 of the training subset\n  dataset = defaultdict(lambda: defaultdict(list))\n  for i, wnid in enumerate(wnids):\n    print(\'Choosing train and val images for synset %d / %d\' % (i + 1, len(wnids)))\n\n    # TinyImagenet train and val images come from ILSVRC-2012 train images\n    train_synset_dir = os.path.join(train_anns_path, wnid)\n    orig_train_bbox_files = os.listdir(train_synset_dir)\n    orig_train_bbox_files = {os.path.join(train_synset_dir, x) for x in orig_train_bbox_files}\n\n    train_bbox_files = random.sample(orig_train_bbox_files, min(num_train, len(orig_train_bbox_files)))\n    orig_train_bbox_files -= set(train_bbox_files)\n    val_bbox_files = random.sample(orig_train_bbox_files, min(num_val, len(orig_train_bbox_files)))\n\n    for bbox_file in train_bbox_files:\n      img_filename, bbox, _ =  parse_xml_file(bbox_file)\n      img_filename = os.path.join(train_image_dir, img_filename)\n      dataset[\'train\'][wnid].append((img_filename, bbox))\n\n    for bbox_file in val_bbox_files:\n      img_filename, bbox, _ = parse_xml_file(bbox_file)\n      img_filename = os.path.join(train_image_dir, img_filename)\n      dataset[\'val\'][wnid].append((img_filename, bbox))\n    \n  # All the validation XML files are all mixed up in one folder, so we need to\n  # iterate over all of them. Since this takes forever, guard it behind a flag.\n  # The name field of the validation XML files gives the synset of that image.\n  if test:\n    val_xml_files = os.listdir(val_anns_path)\n    for i, val_xml_file in enumerate(val_xml_files):\n      if i % 200 == 0:\n        print(\'Processed %d / %d val xml files so far\' % (i, len(val_xml_files)))\n      val_xml_file = os.path.join(val_anns_path, val_xml_file)\n      img_filename, bbox, wnid = parse_xml_file(val_xml_file)\n      if wnid in wnids:\n        img_filename = os.path.join(val_image_dir, img_filename)\n        dataset[\'test\'][wnid].append((img_filename, bbox))\n\n  # Now that we have selected the images for the dataset, we need to actually\n  # create it on disk\n  os.mkdir(out_dir)\n  write_data_in_synset_folders(dataset[\'train\'], \'train\', out_dir, image_size)\n  write_data_in_one_folder(dataset[\'val\'], \'val\', out_dir, image_size)\n  write_data_in_one_folder(dataset[\'test\'], \'test\', out_dir, image_size)\n\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\'--wnid_file\', type=argparse.FileType(\'r\'))\nparser.add_argument(\'--num_train\', type=int, default=500)\nparser.add_argument(\'--num_val\', type=int, default=50)\nparser.add_argument(\'--image_size\', type=int, default=64)\nparser.add_argument(\'--out_dir\')\nargs = parser.parse_args()\n\nif __name__ == \'__main__\':\n  wnids = [line.strip() for line in args.wnid_file]\n  print(len(wnids))\n  # wnids = [\'n02108089\', \'n09428293\', \'n02113799\']\n  make_tiny_imagenet(wnids, args.num_train, args.num_val, args.out_dir, \n                     image_size=args.image_size, test=True)\n  sys.exit(0)\n\n  train_synsets = os.listdir(train_anns_path)\n\n  get_synset_stats()\n  sys.exit(0)\n'"
TinyImageNet/models/allconv.py,4,"b""import math\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass GELU(nn.Module):\n    def __init__(self):\n        super(GELU, self).__init__()\n\n    def forward(self, x):\n        return F.relu(x, inplace=True)\n        # return torch.sigmoid(1.702 * x) * x\n        # return 0.5 * x * (1 + torch.tanh(x * 0.7978845608 * (1 + 0.044715 * x * x)))\n\n\ndef make_layers(cfg):\n    layers = []\n    in_channels = 3\n    for v in cfg:\n        if v == 'Md':\n            layers += [nn.MaxPool2d(kernel_size=2, stride=2), nn.Dropout(p=0.5)]\n        elif v == 'A':\n            layers += [nn.AvgPool2d(kernel_size=8)]\n        elif v == 'NIN':\n            conv2d = nn.Conv2d(in_channels, in_channels, kernel_size=1, padding=1)\n            layers += [conv2d, nn.BatchNorm2d(in_channels), nn.ReLU(inplace=True)]\n        elif v == 'nopad':\n            conv2d = nn.Conv2d(in_channels, in_channels, kernel_size=3, padding=0)\n            layers += [conv2d, nn.BatchNorm2d(in_channels), nn.ReLU(inplace=True)]\n        else:\n            conv2d = nn.Conv2d(in_channels, v, kernel_size=3, padding=1)\n            layers += [conv2d, nn.BatchNorm2d(v), nn.ReLU(inplace=True)]\n            in_channels = v\n    return nn.Sequential(*layers)\n\n\nclass AllConvNet(nn.Module):\n    def __init__(self, num_classes):\n        super(AllConvNet, self).__init__()\n\n        self.num_classes = num_classes\n\n        # if num_classes > 10:\n        #     self.width1, w1 = 128, 128\n        #     self.width2, w2 = 256, 256\n        # else:\n        #     self.width1, w1 = 96,  96\n        #     self.width2, w2 = 192, 192\n\n        self.width1, w1 = 96,  96\n        self.width2, w2 = 192, 192\n\n        self.features = make_layers([w1, w1, w1, 'Md', w2, w2, w2, 'Md', 'nopad', 'NIN', 'NIN', 'A'])\n        self.classifier = nn.Linear(self.width2, num_classes)\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n                m.weight.data.normal_(0, math.sqrt(2. / n))     # He initialization\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n            elif isinstance(m, nn.Linear):\n                m.bias.data.zero_()\n\n    def forward(self, x):\n        x = self.features(x)\n        x = F.avg_pool2d(x, 2)\n        x = x.view(x.size(0), -1)\n        x = self.classifier(x)\n        return x\n\n"""
TinyImageNet/models/wrn.py,4,"b'import math\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass BasicBlock(nn.Module):\n    def __init__(self, in_planes, out_planes, stride, dropRate=0.0):\n        super(BasicBlock, self).__init__()\n        self.bn1 = nn.BatchNorm2d(in_planes)\n        self.relu1 = nn.ReLU(inplace=True)\n        self.conv1 = nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n                               padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(out_planes)\n        self.relu2 = nn.ReLU(inplace=True)\n        self.conv2 = nn.Conv2d(out_planes, out_planes, kernel_size=3, stride=1,\n                               padding=1, bias=False)\n        self.droprate = dropRate\n        self.equalInOut = (in_planes == out_planes)\n        self.convShortcut = (not self.equalInOut) and nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride,\n                                                                padding=0, bias=False) or None\n\n    def forward(self, x):\n        if not self.equalInOut:\n            x = self.relu1(self.bn1(x))\n        else:\n            out = self.relu1(self.bn1(x))\n        if self.equalInOut:\n            out = self.relu2(self.bn2(self.conv1(out)))\n        else:\n            out = self.relu2(self.bn2(self.conv1(x)))\n        if self.droprate > 0:\n            out = F.dropout(out, p=self.droprate, training=self.training)\n        out = self.conv2(out)\n        if not self.equalInOut:\n            return torch.add(self.convShortcut(x), out)\n        else:\n            return torch.add(x, out)\n\n\nclass NetworkBlock(nn.Module):\n    def __init__(self, nb_layers, in_planes, out_planes, block, stride, dropRate=0.0):\n        super(NetworkBlock, self).__init__()\n        self.layer = self._make_layer(block, in_planes, out_planes, nb_layers, stride, dropRate)\n\n    def _make_layer(self, block, in_planes, out_planes, nb_layers, stride, dropRate):\n        layers = []\n        for i in range(nb_layers):\n            layers.append(block(i == 0 and in_planes or out_planes, out_planes, i == 0 and stride or 1, dropRate))\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        return self.layer(x)\n\n\nclass WideResNet(nn.Module):\n    def __init__(self, depth, num_classes, widen_factor=1, dropRate=0.0):\n        super(WideResNet, self).__init__()\n        nChannels = [16, 16 * widen_factor, 32 * widen_factor, 64 * widen_factor]\n        assert ((depth - 4) % 6 == 0)\n        n = (depth - 4) // 6\n        block = BasicBlock\n        # 1st conv before any network block\n        self.conv1 = nn.Conv2d(3, nChannels[0], kernel_size=3, stride=1,\n                               padding=1, bias=False)\n        # 1st block\n        self.block1 = NetworkBlock(n, nChannels[0], nChannels[1], block, 1, dropRate)\n        # 2nd block\n        self.block2 = NetworkBlock(n, nChannels[1], nChannels[2], block, 2, dropRate)\n        # 3rd block\n        self.block3 = NetworkBlock(n, nChannels[2], nChannels[3], block, 2, dropRate)\n        # global average pooling and classifier\n        self.bn1 = nn.BatchNorm2d(nChannels[3])\n        self.relu = nn.ReLU(inplace=True)\n        self.fc = nn.Linear(nChannels[3], num_classes)\n        self.nChannels = nChannels[3]\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n                m.weight.data.normal_(0, math.sqrt(2. / n))\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n            elif isinstance(m, nn.Linear):\n                m.bias.data.zero_()\n\n    def forward(self, x):\n        out = self.conv1(x)\n        out = self.block1(out)\n        out = self.block2(out)\n        out = self.block3(out)\n        out = self.relu(self.bn1(out))\n        out = F.avg_pool2d(out, 16)\n        out = out.view(-1, self.nChannels)\n        return self.fc(out)\n'"
