file_path,api_count,code
ImageNet-C/condensenet_converted.py,3,"b""from __future__ import absolute_import\nfrom __future__ import unicode_literals\nfrom __future__ import print_function\nfrom __future__ import division\n\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\nimport math\nfrom layers import ShuffleLayer, Conv, CondenseConv, CondenseLinear\n\n__all__ = ['CondenseNet']\n\nclass _DenseLayer(nn.Module):\n    def __init__(self, in_channels, growth_rate, args):\n        super(_DenseLayer, self).__init__()\n        self.group_1x1 = args.group_1x1\n        self.group_3x3 = args.group_3x3\n        ### 1x1 conv i --> b*k\n        self.conv_1 = CondenseConv(in_channels, args.bottleneck * growth_rate,\n                                   kernel_size=1, groups=self.group_1x1)\n        ### 3x3 conv b*k-->k\n        self.conv_2 = Conv(args.bottleneck * growth_rate, growth_rate,\n                           kernel_size=3, padding=1, groups=self.group_3x3)\n\n    def forward(self, x):\n        x_ = x\n        x = self.conv_1(x)\n        x = self.conv_2(x)\n        return torch.cat([x_, x], 1)\n\n\nclass _DenseBlock(nn.Sequential):\n    def __init__(self, num_layers, in_channels, growth_rate, args):\n        super(_DenseBlock, self).__init__()\n        for i in range(num_layers):\n            layer = _DenseLayer(in_channels + i * growth_rate, growth_rate, args)\n            self.add_module('denselayer_%d' % (i + 1), layer)\n\n\nclass _Transition(nn.Module):\n    def __init__(self, in_channels, args):\n        super(_Transition, self).__init__()\n        self.pool = nn.AvgPool2d(kernel_size=2, stride=2)\n\n    def forward(self, x):\n        x = self.pool(x)\n        return x\n\n\nclass CondenseNet(nn.Module):\n    def __init__(self, args):\n\n        super(CondenseNet, self).__init__()\n\n        self.stages = args.stages\n        self.growth = args.growth\n        assert len(self.stages) == len(self.growth)\n        self.args = args\n        self.progress = 0.0\n        if args.data in ['cifar10', 'cifar100']:\n            self.init_stride = 1\n            self.pool_size = 8\n        else:\n            self.init_stride = 2\n            self.pool_size = 7\n\n        self.features = nn.Sequential()\n        ### Initial nChannels should be 3\n        self.num_features = 2 * self.growth[0]\n        ### Dense-block 1 (224x224)\n        self.features.add_module('init_conv', nn.Conv2d(3, self.num_features,\n                                                        kernel_size=3,\n                                                        stride=self.init_stride,\n                                                        padding=1,\n                                                        bias=False))\n        for i in range(len(self.stages)):\n            ### Dense-block i\n            self.add_block(i)\n        ### Linear layer\n        self.classifier = CondenseLinear(self.num_features, args.num_classes,\n                                         0.5)\n        ### initialize\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n                m.weight.data.normal_(0, math.sqrt(2. / n))\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n            elif isinstance(m, nn.Linear):\n                m.bias.data.zero_()\n\n    def add_block(self, i):\n        ### Check if ith is the last one\n        last = (i == len(self.stages) - 1)\n        block = _DenseBlock(\n            num_layers=self.stages[i],\n            in_channels=self.num_features,\n            growth_rate=self.growth[i],\n            args=self.args,\n        )\n        self.features.add_module('denseblock_%d' % (i + 1), block)\n        self.num_features += self.stages[i] * self.growth[i]\n        if not last:\n            trans = _Transition(in_channels=self.num_features,\n                                args=self.args)\n            self.features.add_module('transition_%d' % (i + 1), trans)\n        else:\n            self.features.add_module('norm_last',\n                                     nn.BatchNorm2d(self.num_features))\n            self.features.add_module('relu_last',\n                                     nn.ReLU(inplace=True))\n            self.features.add_module('pool_last',\n                                     nn.AvgPool2d(self.pool_size))\n\n    def forward(self, x, progress=None):\n        features = self.features(x)\n        out = features.view(features.size(0), -1)\n        out = self.classifier(out)\n        return out\n"""
ImageNet-C/densenet_cosine_264_k48.py,133,"b'\nimport torch\nimport torch.nn as nn\nimport torch.legacy.nn as lnn\n\nfrom functools import reduce\nfrom torch.autograd import Variable\n\nclass LambdaBase(nn.Sequential):\n    def __init__(self, fn, *args):\n        super(LambdaBase, self).__init__(*args)\n        self.lambda_func = fn\n\n    def forward_prepare(self, input):\n        output = []\n        for module in self._modules.values():\n            output.append(module(input))\n        return output if output else input\n\nclass Lambda(LambdaBase):\n    def forward(self, input):\n        return self.lambda_func(self.forward_prepare(input))\n\nclass LambdaMap(LambdaBase):\n    def forward(self, input):\n        return list(map(self.lambda_func,self.forward_prepare(input)))\n\nclass LambdaReduce(LambdaBase):\n    def forward(self, input):\n        return reduce(self.lambda_func,self.forward_prepare(input))\n\n\ndensenet_cosine_264_k48 = nn.Sequential( # Sequential,\n\tnn.Conv2d(3,96,(7, 7),(2, 2),(3, 3),1,1,bias=False),\n\tnn.BatchNorm2d(96),\n\tnn.ReLU(),\n\tnn.MaxPool2d((3, 3),(2, 2),(1, 1)),\n\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n\t\tLambda(lambda x: x), # Identity,\n\t\tnn.Sequential( # Sequential,\n\t\t\tnn.BatchNorm2d(96),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(96,192,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\tnn.BatchNorm2d(192),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(192,48,(3, 3),(1, 1),(1, 1),1,1,bias=False),\n\t\t),\n\t),\n\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n\t\tLambda(lambda x: x), # Identity,\n\t\tnn.Sequential( # Sequential,\n\t\t\tnn.BatchNorm2d(144),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(144,192,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\tnn.BatchNorm2d(192),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(192,48,(3, 3),(1, 1),(1, 1),1,1,bias=False),\n\t\t),\n\t),\n\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n\t\tLambda(lambda x: x), # Identity,\n\t\tnn.Sequential( # Sequential,\n\t\t\tnn.BatchNorm2d(192),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(192,192,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\tnn.BatchNorm2d(192),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(192,48,(3, 3),(1, 1),(1, 1),1,1,bias=False),\n\t\t),\n\t),\n\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n\t\tLambda(lambda x: x), # Identity,\n\t\tnn.Sequential( # Sequential,\n\t\t\tnn.BatchNorm2d(240),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(240,192,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\tnn.BatchNorm2d(192),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(192,48,(3, 3),(1, 1),(1, 1),1,1,bias=False),\n\t\t),\n\t),\n\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n\t\tLambda(lambda x: x), # Identity,\n\t\tnn.Sequential( # Sequential,\n\t\t\tnn.BatchNorm2d(288),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(288,192,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\tnn.BatchNorm2d(192),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(192,48,(3, 3),(1, 1),(1, 1),1,1,bias=False),\n\t\t),\n\t),\n\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n\t\tLambda(lambda x: x), # Identity,\n\t\tnn.Sequential( # Sequential,\n\t\t\tnn.BatchNorm2d(336),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(336,192,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\tnn.BatchNorm2d(192),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(192,48,(3, 3),(1, 1),(1, 1),1,1,bias=False),\n\t\t),\n\t),\n\tnn.BatchNorm2d(384),\n\tnn.ReLU(),\n\tnn.Conv2d(384,192,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\tnn.AvgPool2d((2, 2),(2, 2)),\n\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n\t\tLambda(lambda x: x), # Identity,\n\t\tnn.Sequential( # Sequential,\n\t\t\tnn.BatchNorm2d(192),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(192,192,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\tnn.BatchNorm2d(192),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(192,48,(3, 3),(1, 1),(1, 1),1,1,bias=False),\n\t\t),\n\t),\n\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n\t\tLambda(lambda x: x), # Identity,\n\t\tnn.Sequential( # Sequential,\n\t\t\tnn.BatchNorm2d(240),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(240,192,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\tnn.BatchNorm2d(192),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(192,48,(3, 3),(1, 1),(1, 1),1,1,bias=False),\n\t\t),\n\t),\n\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n\t\tLambda(lambda x: x), # Identity,\n\t\tnn.Sequential( # Sequential,\n\t\t\tnn.BatchNorm2d(288),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(288,192,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\tnn.BatchNorm2d(192),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(192,48,(3, 3),(1, 1),(1, 1),1,1,bias=False),\n\t\t),\n\t),\n\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n\t\tLambda(lambda x: x), # Identity,\n\t\tnn.Sequential( # Sequential,\n\t\t\tnn.BatchNorm2d(336),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(336,192,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\tnn.BatchNorm2d(192),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(192,48,(3, 3),(1, 1),(1, 1),1,1,bias=False),\n\t\t),\n\t),\n\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n\t\tLambda(lambda x: x), # Identity,\n\t\tnn.Sequential( # Sequential,\n\t\t\tnn.BatchNorm2d(384),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(384,192,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\tnn.BatchNorm2d(192),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(192,48,(3, 3),(1, 1),(1, 1),1,1,bias=False),\n\t\t),\n\t),\n\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n\t\tLambda(lambda x: x), # Identity,\n\t\tnn.Sequential( # Sequential,\n\t\t\tnn.BatchNorm2d(432),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(432,192,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\tnn.BatchNorm2d(192),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(192,48,(3, 3),(1, 1),(1, 1),1,1,bias=False),\n\t\t),\n\t),\n\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n\t\tLambda(lambda x: x), # Identity,\n\t\tnn.Sequential( # Sequential,\n\t\t\tnn.BatchNorm2d(480),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(480,192,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\tnn.BatchNorm2d(192),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(192,48,(3, 3),(1, 1),(1, 1),1,1,bias=False),\n\t\t),\n\t),\n\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n\t\tLambda(lambda x: x), # Identity,\n\t\tnn.Sequential( # Sequential,\n\t\t\tnn.BatchNorm2d(528),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(528,192,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\tnn.BatchNorm2d(192),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(192,48,(3, 3),(1, 1),(1, 1),1,1,bias=False),\n\t\t),\n\t),\n\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n\t\tLambda(lambda x: x), # Identity,\n\t\tnn.Sequential( # Sequential,\n\t\t\tnn.BatchNorm2d(576),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(576,192,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\tnn.BatchNorm2d(192),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(192,48,(3, 3),(1, 1),(1, 1),1,1,bias=False),\n\t\t),\n\t),\n\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n\t\tLambda(lambda x: x), # Identity,\n\t\tnn.Sequential( # Sequential,\n\t\t\tnn.BatchNorm2d(624),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(624,192,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\tnn.BatchNorm2d(192),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(192,48,(3, 3),(1, 1),(1, 1),1,1,bias=False),\n\t\t),\n\t),\n\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n\t\tLambda(lambda x: x), # Identity,\n\t\tnn.Sequential( # Sequential,\n\t\t\tnn.BatchNorm2d(672),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(672,192,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\tnn.BatchNorm2d(192),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(192,48,(3, 3),(1, 1),(1, 1),1,1,bias=False),\n\t\t),\n\t),\n\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n\t\tLambda(lambda x: x), # Identity,\n\t\tnn.Sequential( # Sequential,\n\t\t\tnn.BatchNorm2d(720),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(720,192,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\tnn.BatchNorm2d(192),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(192,48,(3, 3),(1, 1),(1, 1),1,1,bias=False),\n\t\t),\n\t),\n\tnn.BatchNorm2d(768),\n\tnn.ReLU(),\n\tnn.Conv2d(768,384,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\tnn.AvgPool2d((2, 2),(2, 2)),\n\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n\t\tLambda(lambda x: x), # Identity,\n\t\tnn.Sequential( # Sequential,\n\t\t\tnn.BatchNorm2d(384),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(384,192,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\tnn.BatchNorm2d(192),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(192,48,(3, 3),(1, 1),(1, 1),1,1,bias=False),\n\t\t),\n\t),\n\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n\t\tLambda(lambda x: x), # Identity,\n\t\tnn.Sequential( # Sequential,\n\t\t\tnn.BatchNorm2d(432),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(432,192,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\tnn.BatchNorm2d(192),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(192,48,(3, 3),(1, 1),(1, 1),1,1,bias=False),\n\t\t),\n\t),\n\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n\t\tLambda(lambda x: x), # Identity,\n\t\tnn.Sequential( # Sequential,\n\t\t\tnn.BatchNorm2d(480),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(480,192,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\tnn.BatchNorm2d(192),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(192,48,(3, 3),(1, 1),(1, 1),1,1,bias=False),\n\t\t),\n\t),\n\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n\t\tLambda(lambda x: x), # Identity,\n\t\tnn.Sequential( # Sequential,\n\t\t\tnn.BatchNorm2d(528),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(528,192,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\tnn.BatchNorm2d(192),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(192,48,(3, 3),(1, 1),(1, 1),1,1,bias=False),\n\t\t),\n\t),\n\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n\t\tLambda(lambda x: x), # Identity,\n\t\tnn.Sequential( # Sequential,\n\t\t\tnn.BatchNorm2d(576),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(576,192,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\tnn.BatchNorm2d(192),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(192,48,(3, 3),(1, 1),(1, 1),1,1,bias=False),\n\t\t),\n\t),\n\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n\t\tLambda(lambda x: x), # Identity,\n\t\tnn.Sequential( # Sequential,\n\t\t\tnn.BatchNorm2d(624),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(624,192,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\tnn.BatchNorm2d(192),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(192,48,(3, 3),(1, 1),(1, 1),1,1,bias=False),\n\t\t),\n\t),\n\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n\t\tLambda(lambda x: x), # Identity,\n\t\tnn.Sequential( # Sequential,\n\t\t\tnn.BatchNorm2d(672),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(672,192,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\tnn.BatchNorm2d(192),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(192,48,(3, 3),(1, 1),(1, 1),1,1,bias=False),\n\t\t),\n\t),\n\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n\t\tLambda(lambda x: x), # Identity,\n\t\tnn.Sequential( # Sequential,\n\t\t\tnn.BatchNorm2d(720),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(720,192,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\tnn.BatchNorm2d(192),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(192,48,(3, 3),(1, 1),(1, 1),1,1,bias=False),\n\t\t),\n\t),\n\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n\t\tLambda(lambda x: x), # Identity,\n\t\tnn.Sequential( # Sequential,\n\t\t\tnn.BatchNorm2d(768),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(768,192,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\tnn.BatchNorm2d(192),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(192,48,(3, 3),(1, 1),(1, 1),1,1,bias=False),\n\t\t),\n\t),\n\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n\t\tLambda(lambda x: x), # Identity,\n\t\tnn.Sequential( # Sequential,\n\t\t\tnn.BatchNorm2d(816),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(816,192,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\tnn.BatchNorm2d(192),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(192,48,(3, 3),(1, 1),(1, 1),1,1,bias=False),\n\t\t),\n\t),\n\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n\t\tLambda(lambda x: x), # Identity,\n\t\tnn.Sequential( # Sequential,\n\t\t\tnn.BatchNorm2d(864),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(864,192,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\tnn.BatchNorm2d(192),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(192,48,(3, 3),(1, 1),(1, 1),1,1,bias=False),\n\t\t),\n\t),\n\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n\t\tLambda(lambda x: x), # Identity,\n\t\tnn.Sequential( # Sequential,\n\t\t\tnn.BatchNorm2d(912),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(912,192,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\tnn.BatchNorm2d(192),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(192,48,(3, 3),(1, 1),(1, 1),1,1,bias=False),\n\t\t),\n\t),\n\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n\t\tLambda(lambda x: x), # Identity,\n\t\tnn.Sequential( # Sequential,\n\t\t\tnn.BatchNorm2d(960),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(960,192,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\tnn.BatchNorm2d(192),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(192,48,(3, 3),(1, 1),(1, 1),1,1,bias=False),\n\t\t),\n\t),\n\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n\t\tLambda(lambda x: x), # Identity,\n\t\tnn.Sequential( # Sequential,\n\t\t\tnn.BatchNorm2d(1008),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(1008,192,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\tnn.BatchNorm2d(192),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(192,48,(3, 3),(1, 1),(1, 1),1,1,bias=False),\n\t\t),\n\t),\n\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n\t\tLambda(lambda x: x), # Identity,\n\t\tnn.Sequential( # Sequential,\n\t\t\tnn.BatchNorm2d(1056),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(1056,192,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\tnn.BatchNorm2d(192),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(192,48,(3, 3),(1, 1),(1, 1),1,1,bias=False),\n\t\t),\n\t),\n\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n\t\tLambda(lambda x: x), # Identity,\n\t\tnn.Sequential( # Sequential,\n\t\t\tnn.BatchNorm2d(1104),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(1104,192,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\tnn.BatchNorm2d(192),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(192,48,(3, 3),(1, 1),(1, 1),1,1,bias=False),\n\t\t),\n\t),\n\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n\t\tLambda(lambda x: x), # Identity,\n\t\tnn.Sequential( # Sequential,\n\t\t\tnn.BatchNorm2d(1152),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(1152,192,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\tnn.BatchNorm2d(192),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(192,48,(3, 3),(1, 1),(1, 1),1,1,bias=False),\n\t\t),\n\t),\n\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n\t\tLambda(lambda x: x), # Identity,\n\t\tnn.Sequential( # Sequential,\n\t\t\tnn.BatchNorm2d(1200),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(1200,192,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\tnn.BatchNorm2d(192),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(192,48,(3, 3),(1, 1),(1, 1),1,1,bias=False),\n\t\t),\n\t),\n\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n\t\tLambda(lambda x: x), # Identity,\n\t\tnn.Sequential( # Sequential,\n\t\t\tnn.BatchNorm2d(1248),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(1248,192,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\tnn.BatchNorm2d(192),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(192,48,(3, 3),(1, 1),(1, 1),1,1,bias=False),\n\t\t),\n\t),\n\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n\t\tLambda(lambda x: x), # Identity,\n\t\tnn.Sequential( # Sequential,\n\t\t\tnn.BatchNorm2d(1296),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(1296,192,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\tnn.BatchNorm2d(192),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(192,48,(3, 3),(1, 1),(1, 1),1,1,bias=False),\n\t\t),\n\t),\n\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n\t\tLambda(lambda x: x), # Identity,\n\t\tnn.Sequential( # Sequential,\n\t\t\tnn.BatchNorm2d(1344),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(1344,192,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\tnn.BatchNorm2d(192),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(192,48,(3, 3),(1, 1),(1, 1),1,1,bias=False),\n\t\t),\n\t),\n\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n\t\tLambda(lambda x: x), # Identity,\n\t\tnn.Sequential( # Sequential,\n\t\t\tnn.BatchNorm2d(1392),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(1392,192,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\tnn.BatchNorm2d(192),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(192,48,(3, 3),(1, 1),(1, 1),1,1,bias=False),\n\t\t),\n\t),\n\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n\t\tLambda(lambda x: x), # Identity,\n\t\tnn.Sequential( # Sequential,\n\t\t\tnn.BatchNorm2d(1440),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(1440,192,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\tnn.BatchNorm2d(192),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(192,48,(3, 3),(1, 1),(1, 1),1,1,bias=False),\n\t\t),\n\t),\n\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n\t\tLambda(lambda x: x), # Identity,\n\t\tnn.Sequential( # Sequential,\n\t\t\tnn.BatchNorm2d(1488),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(1488,192,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\tnn.BatchNorm2d(192),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(192,48,(3, 3),(1, 1),(1, 1),1,1,bias=False),\n\t\t),\n\t),\n\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n\t\tLambda(lambda x: x), # Identity,\n\t\tnn.Sequential( # Sequential,\n\t\t\tnn.BatchNorm2d(1536),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(1536,192,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\tnn.BatchNorm2d(192),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(192,48,(3, 3),(1, 1),(1, 1),1,1,bias=False),\n\t\t),\n\t),\n\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n\t\tLambda(lambda x: x), # Identity,\n\t\tnn.Sequential( # Sequential,\n\t\t\tnn.BatchNorm2d(1584),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(1584,192,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\tnn.BatchNorm2d(192),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(192,48,(3, 3),(1, 1),(1, 1),1,1,bias=False),\n\t\t),\n\t),\n\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n\t\tLambda(lambda x: x), # Identity,\n\t\tnn.Sequential( # Sequential,\n\t\t\tnn.BatchNorm2d(1632),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(1632,192,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\tnn.BatchNorm2d(192),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(192,48,(3, 3),(1, 1),(1, 1),1,1,bias=False),\n\t\t),\n\t),\n\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n\t\tLambda(lambda x: x), # Identity,\n\t\tnn.Sequential( # Sequential,\n\t\t\tnn.BatchNorm2d(1680),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(1680,192,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\tnn.BatchNorm2d(192),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(192,48,(3, 3),(1, 1),(1, 1),1,1,bias=False),\n\t\t),\n\t),\n\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n\t\tLambda(lambda x: x), # Identity,\n\t\tnn.Sequential( # Sequential,\n\t\t\tnn.BatchNorm2d(1728),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(1728,192,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\tnn.BatchNorm2d(192),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(192,48,(3, 3),(1, 1),(1, 1),1,1,bias=False),\n\t\t),\n\t),\n\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n\t\tLambda(lambda x: x), # Identity,\n\t\tnn.Sequential( # Sequential,\n\t\t\tnn.BatchNorm2d(1776),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(1776,192,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\tnn.BatchNorm2d(192),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(192,48,(3, 3),(1, 1),(1, 1),1,1,bias=False),\n\t\t),\n\t),\n\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n\t\tLambda(lambda x: x), # Identity,\n\t\tnn.Sequential( # Sequential,\n\t\t\tnn.BatchNorm2d(1824),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(1824,192,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\tnn.BatchNorm2d(192),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(192,48,(3, 3),(1, 1),(1, 1),1,1,bias=False),\n\t\t),\n\t),\n\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n\t\tLambda(lambda x: x), # Identity,\n\t\tnn.Sequential( # Sequential,\n\t\t\tnn.BatchNorm2d(1872),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(1872,192,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\tnn.BatchNorm2d(192),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(192,48,(3, 3),(1, 1),(1, 1),1,1,bias=False),\n\t\t),\n\t),\n\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n\t\tLambda(lambda x: x), # Identity,\n\t\tnn.Sequential( # Sequential,\n\t\t\tnn.BatchNorm2d(1920),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(1920,192,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\tnn.BatchNorm2d(192),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(192,48,(3, 3),(1, 1),(1, 1),1,1,bias=False),\n\t\t),\n\t),\n\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n\t\tLambda(lambda x: x), # Identity,\n\t\tnn.Sequential( # Sequential,\n\t\t\tnn.BatchNorm2d(1968),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(1968,192,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\tnn.BatchNorm2d(192),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(192,48,(3, 3),(1, 1),(1, 1),1,1,bias=False),\n\t\t),\n\t),\n\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n\t\tLambda(lambda x: x), # Identity,\n\t\tnn.Sequential( # Sequential,\n\t\t\tnn.BatchNorm2d(2016),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(2016,192,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\tnn.BatchNorm2d(192),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(192,48,(3, 3),(1, 1),(1, 1),1,1,bias=False),\n\t\t),\n\t),\n\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n\t\tLambda(lambda x: x), # Identity,\n\t\tnn.Sequential( # Sequential,\n\t\t\tnn.BatchNorm2d(2064),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(2064,192,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\tnn.BatchNorm2d(192),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(192,48,(3, 3),(1, 1),(1, 1),1,1,bias=False),\n\t\t),\n\t),\n\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n\t\tLambda(lambda x: x), # Identity,\n\t\tnn.Sequential( # Sequential,\n\t\t\tnn.BatchNorm2d(2112),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(2112,192,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\tnn.BatchNorm2d(192),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(192,48,(3, 3),(1, 1),(1, 1),1,1,bias=False),\n\t\t),\n\t),\n\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n\t\tLambda(lambda x: x), # Identity,\n\t\tnn.Sequential( # Sequential,\n\t\t\tnn.BatchNorm2d(2160),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(2160,192,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\tnn.BatchNorm2d(192),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(192,48,(3, 3),(1, 1),(1, 1),1,1,bias=False),\n\t\t),\n\t),\n\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n\t\tLambda(lambda x: x), # Identity,\n\t\tnn.Sequential( # Sequential,\n\t\t\tnn.BatchNorm2d(2208),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(2208,192,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\tnn.BatchNorm2d(192),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(192,48,(3, 3),(1, 1),(1, 1),1,1,bias=False),\n\t\t),\n\t),\n\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n\t\tLambda(lambda x: x), # Identity,\n\t\tnn.Sequential( # Sequential,\n\t\t\tnn.BatchNorm2d(2256),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(2256,192,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\tnn.BatchNorm2d(192),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(192,48,(3, 3),(1, 1),(1, 1),1,1,bias=False),\n\t\t),\n\t),\n\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n\t\tLambda(lambda x: x), # Identity,\n\t\tnn.Sequential( # Sequential,\n\t\t\tnn.BatchNorm2d(2304),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(2304,192,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\tnn.BatchNorm2d(192),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(192,48,(3, 3),(1, 1),(1, 1),1,1,bias=False),\n\t\t),\n\t),\n\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n\t\tLambda(lambda x: x), # Identity,\n\t\tnn.Sequential( # Sequential,\n\t\t\tnn.BatchNorm2d(2352),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(2352,192,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\tnn.BatchNorm2d(192),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(192,48,(3, 3),(1, 1),(1, 1),1,1,bias=False),\n\t\t),\n\t),\n\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n\t\tLambda(lambda x: x), # Identity,\n\t\tnn.Sequential( # Sequential,\n\t\t\tnn.BatchNorm2d(2400),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(2400,192,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\tnn.BatchNorm2d(192),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(192,48,(3, 3),(1, 1),(1, 1),1,1,bias=False),\n\t\t),\n\t),\n\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n\t\tLambda(lambda x: x), # Identity,\n\t\tnn.Sequential( # Sequential,\n\t\t\tnn.BatchNorm2d(2448),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(2448,192,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\tnn.BatchNorm2d(192),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(192,48,(3, 3),(1, 1),(1, 1),1,1,bias=False),\n\t\t),\n\t),\n\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n\t\tLambda(lambda x: x), # Identity,\n\t\tnn.Sequential( # Sequential,\n\t\t\tnn.BatchNorm2d(2496),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(2496,192,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\tnn.BatchNorm2d(192),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(192,48,(3, 3),(1, 1),(1, 1),1,1,bias=False),\n\t\t),\n\t),\n\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n\t\tLambda(lambda x: x), # Identity,\n\t\tnn.Sequential( # Sequential,\n\t\t\tnn.BatchNorm2d(2544),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(2544,192,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\tnn.BatchNorm2d(192),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(192,48,(3, 3),(1, 1),(1, 1),1,1,bias=False),\n\t\t),\n\t),\n\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n\t\tLambda(lambda x: x), # Identity,\n\t\tnn.Sequential( # Sequential,\n\t\t\tnn.BatchNorm2d(2592),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(2592,192,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\tnn.BatchNorm2d(192),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(192,48,(3, 3),(1, 1),(1, 1),1,1,bias=False),\n\t\t),\n\t),\n\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n\t\tLambda(lambda x: x), # Identity,\n\t\tnn.Sequential( # Sequential,\n\t\t\tnn.BatchNorm2d(2640),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(2640,192,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\tnn.BatchNorm2d(192),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(192,48,(3, 3),(1, 1),(1, 1),1,1,bias=False),\n\t\t),\n\t),\n\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n\t\tLambda(lambda x: x), # Identity,\n\t\tnn.Sequential( # Sequential,\n\t\t\tnn.BatchNorm2d(2688),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(2688,192,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\tnn.BatchNorm2d(192),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(192,48,(3, 3),(1, 1),(1, 1),1,1,bias=False),\n\t\t),\n\t),\n\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n\t\tLambda(lambda x: x), # Identity,\n\t\tnn.Sequential( # Sequential,\n\t\t\tnn.BatchNorm2d(2736),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(2736,192,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\tnn.BatchNorm2d(192),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(192,48,(3, 3),(1, 1),(1, 1),1,1,bias=False),\n\t\t),\n\t),\n\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n\t\tLambda(lambda x: x), # Identity,\n\t\tnn.Sequential( # Sequential,\n\t\t\tnn.BatchNorm2d(2784),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(2784,192,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\tnn.BatchNorm2d(192),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(192,48,(3, 3),(1, 1),(1, 1),1,1,bias=False),\n\t\t),\n\t),\n\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n\t\tLambda(lambda x: x), # Identity,\n\t\tnn.Sequential( # Sequential,\n\t\t\tnn.BatchNorm2d(2832),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(2832,192,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\tnn.BatchNorm2d(192),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(192,48,(3, 3),(1, 1),(1, 1),1,1,bias=False),\n\t\t),\n\t),\n\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n\t\tLambda(lambda x: x), # Identity,\n\t\tnn.Sequential( # Sequential,\n\t\t\tnn.BatchNorm2d(2880),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(2880,192,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\tnn.BatchNorm2d(192),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(192,48,(3, 3),(1, 1),(1, 1),1,1,bias=False),\n\t\t),\n\t),\n\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n\t\tLambda(lambda x: x), # Identity,\n\t\tnn.Sequential( # Sequential,\n\t\t\tnn.BatchNorm2d(2928),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(2928,192,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\tnn.BatchNorm2d(192),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(192,48,(3, 3),(1, 1),(1, 1),1,1,bias=False),\n\t\t),\n\t),\n\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n\t\tLambda(lambda x: x), # Identity,\n\t\tnn.Sequential( # Sequential,\n\t\t\tnn.BatchNorm2d(2976),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(2976,192,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\tnn.BatchNorm2d(192),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(192,48,(3, 3),(1, 1),(1, 1),1,1,bias=False),\n\t\t),\n\t),\n\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n\t\tLambda(lambda x: x), # Identity,\n\t\tnn.Sequential( # Sequential,\n\t\t\tnn.BatchNorm2d(3024),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(3024,192,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\tnn.BatchNorm2d(192),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(192,48,(3, 3),(1, 1),(1, 1),1,1,bias=False),\n\t\t),\n\t),\n\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n\t\tLambda(lambda x: x), # Identity,\n\t\tnn.Sequential( # Sequential,\n\t\t\tnn.BatchNorm2d(3072),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(3072,192,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\tnn.BatchNorm2d(192),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(192,48,(3, 3),(1, 1),(1, 1),1,1,bias=False),\n\t\t),\n\t),\n\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n\t\tLambda(lambda x: x), # Identity,\n\t\tnn.Sequential( # Sequential,\n\t\t\tnn.BatchNorm2d(3120),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(3120,192,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\tnn.BatchNorm2d(192),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(192,48,(3, 3),(1, 1),(1, 1),1,1,bias=False),\n\t\t),\n\t),\n\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n\t\tLambda(lambda x: x), # Identity,\n\t\tnn.Sequential( # Sequential,\n\t\t\tnn.BatchNorm2d(3168),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(3168,192,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\tnn.BatchNorm2d(192),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(192,48,(3, 3),(1, 1),(1, 1),1,1,bias=False),\n\t\t),\n\t),\n\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n\t\tLambda(lambda x: x), # Identity,\n\t\tnn.Sequential( # Sequential,\n\t\t\tnn.BatchNorm2d(3216),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(3216,192,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\tnn.BatchNorm2d(192),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(192,48,(3, 3),(1, 1),(1, 1),1,1,bias=False),\n\t\t),\n\t),\n\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n\t\tLambda(lambda x: x), # Identity,\n\t\tnn.Sequential( # Sequential,\n\t\t\tnn.BatchNorm2d(3264),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(3264,192,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\tnn.BatchNorm2d(192),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(192,48,(3, 3),(1, 1),(1, 1),1,1,bias=False),\n\t\t),\n\t),\n\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n\t\tLambda(lambda x: x), # Identity,\n\t\tnn.Sequential( # Sequential,\n\t\t\tnn.BatchNorm2d(3312),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(3312,192,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\tnn.BatchNorm2d(192),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(192,48,(3, 3),(1, 1),(1, 1),1,1,bias=False),\n\t\t),\n\t),\n\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n\t\tLambda(lambda x: x), # Identity,\n\t\tnn.Sequential( # Sequential,\n\t\t\tnn.BatchNorm2d(3360),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(3360,192,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\tnn.BatchNorm2d(192),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(192,48,(3, 3),(1, 1),(1, 1),1,1,bias=False),\n\t\t),\n\t),\n\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n\t\tLambda(lambda x: x), # Identity,\n\t\tnn.Sequential( # Sequential,\n\t\t\tnn.BatchNorm2d(3408),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(3408,192,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\tnn.BatchNorm2d(192),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(192,48,(3, 3),(1, 1),(1, 1),1,1,bias=False),\n\t\t),\n\t),\n\tnn.BatchNorm2d(3456),\n\tnn.ReLU(),\n\tnn.Conv2d(3456,1728,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\tnn.AvgPool2d((2, 2),(2, 2)),\n\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n\t\tLambda(lambda x: x), # Identity,\n\t\tnn.Sequential( # Sequential,\n\t\t\tnn.BatchNorm2d(1728),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(1728,192,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\tnn.BatchNorm2d(192),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(192,48,(3, 3),(1, 1),(1, 1),1,1,bias=False),\n\t\t),\n\t),\n\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n\t\tLambda(lambda x: x), # Identity,\n\t\tnn.Sequential( # Sequential,\n\t\t\tnn.BatchNorm2d(1776),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(1776,192,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\tnn.BatchNorm2d(192),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(192,48,(3, 3),(1, 1),(1, 1),1,1,bias=False),\n\t\t),\n\t),\n\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n\t\tLambda(lambda x: x), # Identity,\n\t\tnn.Sequential( # Sequential,\n\t\t\tnn.BatchNorm2d(1824),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(1824,192,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\tnn.BatchNorm2d(192),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(192,48,(3, 3),(1, 1),(1, 1),1,1,bias=False),\n\t\t),\n\t),\n\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n\t\tLambda(lambda x: x), # Identity,\n\t\tnn.Sequential( # Sequential,\n\t\t\tnn.BatchNorm2d(1872),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(1872,192,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\tnn.BatchNorm2d(192),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(192,48,(3, 3),(1, 1),(1, 1),1,1,bias=False),\n\t\t),\n\t),\n\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n\t\tLambda(lambda x: x), # Identity,\n\t\tnn.Sequential( # Sequential,\n\t\t\tnn.BatchNorm2d(1920),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(1920,192,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\tnn.BatchNorm2d(192),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(192,48,(3, 3),(1, 1),(1, 1),1,1,bias=False),\n\t\t),\n\t),\n\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n\t\tLambda(lambda x: x), # Identity,\n\t\tnn.Sequential( # Sequential,\n\t\t\tnn.BatchNorm2d(1968),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(1968,192,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\tnn.BatchNorm2d(192),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(192,48,(3, 3),(1, 1),(1, 1),1,1,bias=False),\n\t\t),\n\t),\n\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n\t\tLambda(lambda x: x), # Identity,\n\t\tnn.Sequential( # Sequential,\n\t\t\tnn.BatchNorm2d(2016),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(2016,192,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\tnn.BatchNorm2d(192),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(192,48,(3, 3),(1, 1),(1, 1),1,1,bias=False),\n\t\t),\n\t),\n\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n\t\tLambda(lambda x: x), # Identity,\n\t\tnn.Sequential( # Sequential,\n\t\t\tnn.BatchNorm2d(2064),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(2064,192,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\tnn.BatchNorm2d(192),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(192,48,(3, 3),(1, 1),(1, 1),1,1,bias=False),\n\t\t),\n\t),\n\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n\t\tLambda(lambda x: x), # Identity,\n\t\tnn.Sequential( # Sequential,\n\t\t\tnn.BatchNorm2d(2112),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(2112,192,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\tnn.BatchNorm2d(192),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(192,48,(3, 3),(1, 1),(1, 1),1,1,bias=False),\n\t\t),\n\t),\n\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n\t\tLambda(lambda x: x), # Identity,\n\t\tnn.Sequential( # Sequential,\n\t\t\tnn.BatchNorm2d(2160),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(2160,192,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\tnn.BatchNorm2d(192),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(192,48,(3, 3),(1, 1),(1, 1),1,1,bias=False),\n\t\t),\n\t),\n\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n\t\tLambda(lambda x: x), # Identity,\n\t\tnn.Sequential( # Sequential,\n\t\t\tnn.BatchNorm2d(2208),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(2208,192,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\tnn.BatchNorm2d(192),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(192,48,(3, 3),(1, 1),(1, 1),1,1,bias=False),\n\t\t),\n\t),\n\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n\t\tLambda(lambda x: x), # Identity,\n\t\tnn.Sequential( # Sequential,\n\t\t\tnn.BatchNorm2d(2256),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(2256,192,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\tnn.BatchNorm2d(192),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(192,48,(3, 3),(1, 1),(1, 1),1,1,bias=False),\n\t\t),\n\t),\n\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n\t\tLambda(lambda x: x), # Identity,\n\t\tnn.Sequential( # Sequential,\n\t\t\tnn.BatchNorm2d(2304),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(2304,192,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\tnn.BatchNorm2d(192),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(192,48,(3, 3),(1, 1),(1, 1),1,1,bias=False),\n\t\t),\n\t),\n\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n\t\tLambda(lambda x: x), # Identity,\n\t\tnn.Sequential( # Sequential,\n\t\t\tnn.BatchNorm2d(2352),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(2352,192,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\tnn.BatchNorm2d(192),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(192,48,(3, 3),(1, 1),(1, 1),1,1,bias=False),\n\t\t),\n\t),\n\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n\t\tLambda(lambda x: x), # Identity,\n\t\tnn.Sequential( # Sequential,\n\t\t\tnn.BatchNorm2d(2400),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(2400,192,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\tnn.BatchNorm2d(192),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(192,48,(3, 3),(1, 1),(1, 1),1,1,bias=False),\n\t\t),\n\t),\n\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n\t\tLambda(lambda x: x), # Identity,\n\t\tnn.Sequential( # Sequential,\n\t\t\tnn.BatchNorm2d(2448),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(2448,192,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\tnn.BatchNorm2d(192),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(192,48,(3, 3),(1, 1),(1, 1),1,1,bias=False),\n\t\t),\n\t),\n\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n\t\tLambda(lambda x: x), # Identity,\n\t\tnn.Sequential( # Sequential,\n\t\t\tnn.BatchNorm2d(2496),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(2496,192,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\tnn.BatchNorm2d(192),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(192,48,(3, 3),(1, 1),(1, 1),1,1,bias=False),\n\t\t),\n\t),\n\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n\t\tLambda(lambda x: x), # Identity,\n\t\tnn.Sequential( # Sequential,\n\t\t\tnn.BatchNorm2d(2544),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(2544,192,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\tnn.BatchNorm2d(192),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(192,48,(3, 3),(1, 1),(1, 1),1,1,bias=False),\n\t\t),\n\t),\n\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n\t\tLambda(lambda x: x), # Identity,\n\t\tnn.Sequential( # Sequential,\n\t\t\tnn.BatchNorm2d(2592),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(2592,192,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\tnn.BatchNorm2d(192),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(192,48,(3, 3),(1, 1),(1, 1),1,1,bias=False),\n\t\t),\n\t),\n\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n\t\tLambda(lambda x: x), # Identity,\n\t\tnn.Sequential( # Sequential,\n\t\t\tnn.BatchNorm2d(2640),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(2640,192,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\tnn.BatchNorm2d(192),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(192,48,(3, 3),(1, 1),(1, 1),1,1,bias=False),\n\t\t),\n\t),\n\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n\t\tLambda(lambda x: x), # Identity,\n\t\tnn.Sequential( # Sequential,\n\t\t\tnn.BatchNorm2d(2688),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(2688,192,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\tnn.BatchNorm2d(192),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(192,48,(3, 3),(1, 1),(1, 1),1,1,bias=False),\n\t\t),\n\t),\n\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n\t\tLambda(lambda x: x), # Identity,\n\t\tnn.Sequential( # Sequential,\n\t\t\tnn.BatchNorm2d(2736),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(2736,192,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\tnn.BatchNorm2d(192),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(192,48,(3, 3),(1, 1),(1, 1),1,1,bias=False),\n\t\t),\n\t),\n\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n\t\tLambda(lambda x: x), # Identity,\n\t\tnn.Sequential( # Sequential,\n\t\t\tnn.BatchNorm2d(2784),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(2784,192,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\tnn.BatchNorm2d(192),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(192,48,(3, 3),(1, 1),(1, 1),1,1,bias=False),\n\t\t),\n\t),\n\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n\t\tLambda(lambda x: x), # Identity,\n\t\tnn.Sequential( # Sequential,\n\t\t\tnn.BatchNorm2d(2832),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(2832,192,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\tnn.BatchNorm2d(192),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(192,48,(3, 3),(1, 1),(1, 1),1,1,bias=False),\n\t\t),\n\t),\n\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n\t\tLambda(lambda x: x), # Identity,\n\t\tnn.Sequential( # Sequential,\n\t\t\tnn.BatchNorm2d(2880),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(2880,192,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\tnn.BatchNorm2d(192),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(192,48,(3, 3),(1, 1),(1, 1),1,1,bias=False),\n\t\t),\n\t),\n\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n\t\tLambda(lambda x: x), # Identity,\n\t\tnn.Sequential( # Sequential,\n\t\t\tnn.BatchNorm2d(2928),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(2928,192,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\tnn.BatchNorm2d(192),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(192,48,(3, 3),(1, 1),(1, 1),1,1,bias=False),\n\t\t),\n\t),\n\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n\t\tLambda(lambda x: x), # Identity,\n\t\tnn.Sequential( # Sequential,\n\t\t\tnn.BatchNorm2d(2976),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(2976,192,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\tnn.BatchNorm2d(192),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(192,48,(3, 3),(1, 1),(1, 1),1,1,bias=False),\n\t\t),\n\t),\n\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n\t\tLambda(lambda x: x), # Identity,\n\t\tnn.Sequential( # Sequential,\n\t\t\tnn.BatchNorm2d(3024),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(3024,192,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\tnn.BatchNorm2d(192),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(192,48,(3, 3),(1, 1),(1, 1),1,1,bias=False),\n\t\t),\n\t),\n\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n\t\tLambda(lambda x: x), # Identity,\n\t\tnn.Sequential( # Sequential,\n\t\t\tnn.BatchNorm2d(3072),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(3072,192,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\tnn.BatchNorm2d(192),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(192,48,(3, 3),(1, 1),(1, 1),1,1,bias=False),\n\t\t),\n\t),\n\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n\t\tLambda(lambda x: x), # Identity,\n\t\tnn.Sequential( # Sequential,\n\t\t\tnn.BatchNorm2d(3120),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(3120,192,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\tnn.BatchNorm2d(192),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(192,48,(3, 3),(1, 1),(1, 1),1,1,bias=False),\n\t\t),\n\t),\n\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n\t\tLambda(lambda x: x), # Identity,\n\t\tnn.Sequential( # Sequential,\n\t\t\tnn.BatchNorm2d(3168),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(3168,192,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\tnn.BatchNorm2d(192),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(192,48,(3, 3),(1, 1),(1, 1),1,1,bias=False),\n\t\t),\n\t),\n\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n\t\tLambda(lambda x: x), # Identity,\n\t\tnn.Sequential( # Sequential,\n\t\t\tnn.BatchNorm2d(3216),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(3216,192,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\tnn.BatchNorm2d(192),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(192,48,(3, 3),(1, 1),(1, 1),1,1,bias=False),\n\t\t),\n\t),\n\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n\t\tLambda(lambda x: x), # Identity,\n\t\tnn.Sequential( # Sequential,\n\t\t\tnn.BatchNorm2d(3264),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(3264,192,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\tnn.BatchNorm2d(192),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(192,48,(3, 3),(1, 1),(1, 1),1,1,bias=False),\n\t\t),\n\t),\n\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n\t\tLambda(lambda x: x), # Identity,\n\t\tnn.Sequential( # Sequential,\n\t\t\tnn.BatchNorm2d(3312),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(3312,192,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\tnn.BatchNorm2d(192),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(192,48,(3, 3),(1, 1),(1, 1),1,1,bias=False),\n\t\t),\n\t),\n\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n\t\tLambda(lambda x: x), # Identity,\n\t\tnn.Sequential( # Sequential,\n\t\t\tnn.BatchNorm2d(3360),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(3360,192,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\tnn.BatchNorm2d(192),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(192,48,(3, 3),(1, 1),(1, 1),1,1,bias=False),\n\t\t),\n\t),\n\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n\t\tLambda(lambda x: x), # Identity,\n\t\tnn.Sequential( # Sequential,\n\t\t\tnn.BatchNorm2d(3408),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(3408,192,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\tnn.BatchNorm2d(192),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(192,48,(3, 3),(1, 1),(1, 1),1,1,bias=False),\n\t\t),\n\t),\n\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n\t\tLambda(lambda x: x), # Identity,\n\t\tnn.Sequential( # Sequential,\n\t\t\tnn.BatchNorm2d(3456),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(3456,192,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\tnn.BatchNorm2d(192),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(192,48,(3, 3),(1, 1),(1, 1),1,1,bias=False),\n\t\t),\n\t),\n\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n\t\tLambda(lambda x: x), # Identity,\n\t\tnn.Sequential( # Sequential,\n\t\t\tnn.BatchNorm2d(3504),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(3504,192,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\tnn.BatchNorm2d(192),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(192,48,(3, 3),(1, 1),(1, 1),1,1,bias=False),\n\t\t),\n\t),\n\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n\t\tLambda(lambda x: x), # Identity,\n\t\tnn.Sequential( # Sequential,\n\t\t\tnn.BatchNorm2d(3552),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(3552,192,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\tnn.BatchNorm2d(192),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(192,48,(3, 3),(1, 1),(1, 1),1,1,bias=False),\n\t\t),\n\t),\n\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n\t\tLambda(lambda x: x), # Identity,\n\t\tnn.Sequential( # Sequential,\n\t\t\tnn.BatchNorm2d(3600),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(3600,192,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\tnn.BatchNorm2d(192),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(192,48,(3, 3),(1, 1),(1, 1),1,1,bias=False),\n\t\t),\n\t),\n\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n\t\tLambda(lambda x: x), # Identity,\n\t\tnn.Sequential( # Sequential,\n\t\t\tnn.BatchNorm2d(3648),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(3648,192,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\tnn.BatchNorm2d(192),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(192,48,(3, 3),(1, 1),(1, 1),1,1,bias=False),\n\t\t),\n\t),\n\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n\t\tLambda(lambda x: x), # Identity,\n\t\tnn.Sequential( # Sequential,\n\t\t\tnn.BatchNorm2d(3696),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(3696,192,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\tnn.BatchNorm2d(192),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(192,48,(3, 3),(1, 1),(1, 1),1,1,bias=False),\n\t\t),\n\t),\n\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n\t\tLambda(lambda x: x), # Identity,\n\t\tnn.Sequential( # Sequential,\n\t\t\tnn.BatchNorm2d(3744),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(3744,192,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\tnn.BatchNorm2d(192),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(192,48,(3, 3),(1, 1),(1, 1),1,1,bias=False),\n\t\t),\n\t),\n\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n\t\tLambda(lambda x: x), # Identity,\n\t\tnn.Sequential( # Sequential,\n\t\t\tnn.BatchNorm2d(3792),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(3792,192,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\tnn.BatchNorm2d(192),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(192,48,(3, 3),(1, 1),(1, 1),1,1,bias=False),\n\t\t),\n\t),\n\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n\t\tLambda(lambda x: x), # Identity,\n\t\tnn.Sequential( # Sequential,\n\t\t\tnn.BatchNorm2d(3840),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(3840,192,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\tnn.BatchNorm2d(192),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(192,48,(3, 3),(1, 1),(1, 1),1,1,bias=False),\n\t\t),\n\t),\n\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n\t\tLambda(lambda x: x), # Identity,\n\t\tnn.Sequential( # Sequential,\n\t\t\tnn.BatchNorm2d(3888),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(3888,192,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\tnn.BatchNorm2d(192),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(192,48,(3, 3),(1, 1),(1, 1),1,1,bias=False),\n\t\t),\n\t),\n\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n\t\tLambda(lambda x: x), # Identity,\n\t\tnn.Sequential( # Sequential,\n\t\t\tnn.BatchNorm2d(3936),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(3936,192,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\tnn.BatchNorm2d(192),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(192,48,(3, 3),(1, 1),(1, 1),1,1,bias=False),\n\t\t),\n\t),\n\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n\t\tLambda(lambda x: x), # Identity,\n\t\tnn.Sequential( # Sequential,\n\t\t\tnn.BatchNorm2d(3984),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(3984,192,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\tnn.BatchNorm2d(192),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(192,48,(3, 3),(1, 1),(1, 1),1,1,bias=False),\n\t\t),\n\t),\n\tnn.BatchNorm2d(4032),\n\tnn.ReLU(),\n\tnn.AvgPool2d((7, 7),(7, 7)),\n\tLambda(lambda x: x.view(x.size(0),-1)), # Reshape,\n\tnn.Sequential(Lambda(lambda x: x.view(1,-1) if 1==len(x.size()) else x ),nn.Linear(4032,1000)), # Linear,\n)'"
ImageNet-C/layers.py,15,"b'from __future__ import absolute_import\nfrom __future__ import unicode_literals\nfrom __future__ import print_function\nfrom __future__ import division\n\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\nimport torch.nn.functional as F\n\n\nclass LearnedGroupConv(nn.Module):\n    global_progress = 0.0\n    def __init__(self, in_channels, out_channels, kernel_size, stride=1,\n                 padding=0, dilation=1, groups=1, \n                 condense_factor=None, dropout_rate=0.):\n        super(LearnedGroupConv, self).__init__()\n        self.norm = nn.BatchNorm2d(in_channels)\n        self.relu = nn.ReLU(inplace=True)\n        self.dropout_rate = dropout_rate\n        if self.dropout_rate > 0:\n            self.drop = nn.Dropout(dropout_rate, inplace=False)\n        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride,\n                              padding, dilation, groups=1, bias=False)\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        self.groups = groups\n        self.condense_factor = condense_factor\n        if self.condense_factor is None:\n            self.condense_factor = self.groups\n        ### Parameters that should be carefully used\n        self.register_buffer(\'_count\', torch.zeros(1))\n        self.register_buffer(\'_stage\', torch.zeros(1))\n        self.register_buffer(\'_mask\', torch.ones(self.conv.weight.size()))\n        ### Check if arguments are valid\n        assert self.in_channels % self.groups == 0, ""group number can not be divided by input channels""\n        assert self.in_channels % self.condense_factor == 0, ""condensation factor can not be divided by input channels""\n        assert self.out_channels % self.groups == 0, ""group number can not be divided by output channels""\n\n    def forward(self, x):\n        self._check_drop()\n        x = self.norm(x)\n        x = self.relu(x)\n        if self.dropout_rate > 0:\n            x = self.drop(x)\n        ### Masked output\n        weight = self.conv.weight * self.mask\n        return F.conv2d(x, weight, None, self.conv.stride,\n                        self.conv.padding, self.conv.dilation, 1)\n\n    def _check_drop(self):\n        progress = LearnedGroupConv.global_progress\n        delta = 0\n        ### Get current stage\n        for i in range(self.condense_factor - 1):\n            if progress * 2 < (i + 1) / (self.condense_factor - 1):\n                stage = i\n                break\n        else:\n            stage = self.condense_factor - 1\n        ### Check for dropping\n        if not self._at_stage(stage):\n            self.stage = stage\n            delta = self.in_channels // self.condense_factor\n        if delta > 0:\n            self._dropping(delta)\n        return\n\n    def _dropping(self, delta):\n        weight = self.conv.weight * self.mask\n        ### Sum up all kernels\n        ### Assume only apply to 1x1 conv to speed up\n        assert weight.size()[-1] == 1\n        weight = weight.abs().squeeze()\n        assert weight.size()[0] == self.out_channels\n        assert weight.size()[1] == self.in_channels\n        d_out = self.out_channels // self.groups\n        ### Shuffle weight\n        weight = weight.view(d_out, self.groups, self.in_channels)\n        weight = weight.transpose(0, 1).contiguous()\n        weight = weight.view(self.out_channels, self.in_channels)\n        ### Sort and drop\n        for i in range(self.groups):\n            wi = weight[i * d_out:(i + 1) * d_out, :]\n            ### Take corresponding delta index\n            di = wi.sum(0).sort()[1][self.count:self.count + delta]\n            for d in di.data:\n                self._mask[i::self.groups, d, :, :].fill_(0)\n        self.count = self.count + delta\n\n    @property\n    def count(self):\n        return int(self._count[0])\n\n    @count.setter\n    def count(self, val):\n        self._count.fill_(val)\n\n    @property\n    def stage(self):\n        return int(self._stage[0])\n        \n    @stage.setter\n    def stage(self, val):\n        self._stage.fill_(val)\n\n    @property\n    def mask(self):\n        return Variable(self._mask)\n\n    def _at_stage(self, stage):\n        return (self._stage == stage).all()\n\n    @property\n    def lasso_loss(self):\n        if self._at_stage(self.groups - 1):\n            return 0\n        weight = self.conv.weight * self.mask\n        ### Assume only apply to 1x1 conv to speed up\n        assert weight.size()[-1] == 1\n        weight = weight.squeeze().pow(2)\n        d_out = self.out_channels // self.groups\n        ### Shuffle weight\n        weight = weight.view(d_out, self.groups, self.in_channels)\n        weight = weight.sum(0).clamp(min=1e-6).sqrt()\n        return weight.sum()\n\n\ndef ShuffleLayer(x, groups):\n    batchsize, num_channels, height, width = x.data.size()\n    channels_per_group = num_channels // groups\n    ### reshape\n    x = x.view(batchsize, groups,\n               channels_per_group, height, width)\n    ### transpose\n    x = torch.transpose(x, 1, 2).contiguous()\n    ### flatten\n    x = x.view(batchsize, -1, height, width)\n    return x\n\n\nclass CondensingLinear(nn.Module):\n    def __init__(self, model, drop_rate=0.5):\n        super(CondensingLinear, self).__init__()\n        self.in_features = int(model.in_features*drop_rate)\n        self.out_features = model.out_features\n        self.linear = nn.Linear(self.in_features, self.out_features)\n        self.register_buffer(\'index\', torch.LongTensor(self.in_features))\n        _, index = model.weight.data.abs().sum(0).sort()\n        index = index[model.in_features-self.in_features:]\n        self.linear.bias.data = model.bias.data.clone()\n        for i in range(self.in_features):\n            self.index[i] = index[i]\n            self.linear.weight.data[:, i] = model.weight.data[:, index[i]]\n\n    def forward(self, x):\n        x = torch.index_select(x, 1, Variable(self.index))\n        x = self.linear(x)\n        return x\n\n\nclass CondensingConv(nn.Module):\n    def __init__(self, model):\n        super(CondensingConv, self).__init__()\n        self.in_channels = model.conv.in_channels \\\n                         * model.groups // model.condense_factor\n        self.out_channels = model.conv.out_channels\n        self.groups = model.groups\n        self.condense_factor = model.condense_factor\n        self.norm = nn.BatchNorm2d(self.in_channels)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv = nn.Conv2d(self.in_channels, self.out_channels,\n                              kernel_size=model.conv.kernel_size,\n                              padding=model.conv.padding,\n                              groups=self.groups,\n                              bias=False,\n                              stride=model.conv.stride)\n        self.register_buffer(\'index\', torch.LongTensor(self.in_channels))\n        index = 0\n        mask = model._mask.mean(-1).mean(-1)\n        for i in range(self.groups):\n            for j in range(model.conv.in_channels):\n                if index < (self.in_channels // self.groups) * (i + 1) \\\n                         and mask[i, j] == 1:\n                    for k in range(self.out_channels // self.groups):\n                        idx_i = int(k + i * (self.out_channels // self.groups))\n                        idx_j = index % (self.in_channels // self.groups)\n                        self.conv.weight.data[idx_i, idx_j, :, :] = \\\n                            model.conv.weight.data[int(i + k * self.groups), j, :, :]\n                        self.norm.weight.data[index] = model.norm.weight.data[j]\n                        self.norm.bias.data[index] = model.norm.bias.data[j]\n                        self.norm.running_mean[index] = model.norm.running_mean[j]\n                        self.norm.running_var[index] = model.norm.running_var[j]\n                    self.index[index] = j\n                    index += 1\n\n    def forward(self, x):\n        x = torch.index_select(x, 1, Variable(self.index))\n        x = self.norm(x)\n        x = self.relu(x)\n        x = self.conv(x)\n        x = ShuffleLayer(x, self.groups)\n        return x\n\n\nclass CondenseLinear(nn.Module):\n    def __init__(self, in_features, out_features, drop_rate=0.5):\n        super(CondenseLinear, self).__init__()\n        self.in_features = int(in_features*drop_rate)\n        self.out_features = out_features\n        self.linear = nn.Linear(self.in_features, self.out_features)\n        self.register_buffer(\'index\', torch.LongTensor(self.in_features))\n\n    def forward(self, x):\n        x = torch.index_select(x, 1, Variable(self.index))\n        x = self.linear(x)\n        return x\n\n\nclass CondenseConv(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size, \n                 stride=1, padding=0, groups=1):\n        super(CondenseConv, self).__init__()\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        self.groups = groups\n        self.norm = nn.BatchNorm2d(self.in_channels)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv = nn.Conv2d(self.in_channels, self.out_channels,\n                              kernel_size=kernel_size,\n                              stride=stride,\n                              padding=padding,\n                              groups=self.groups,\n                              bias=False)\n        self.register_buffer(\'index\', torch.LongTensor(self.in_channels))\n        self.index.fill_(0)\n\n    def forward(self, x):\n        x = torch.index_select(x, 1, Variable(self.index))\n        x = self.norm(x)\n        x = self.relu(x)\n        x = self.conv(x)\n        x = ShuffleLayer(x, self.groups)\n        return x\n\n\nclass Conv(nn.Sequential):\n    def __init__(self, in_channels, out_channels, kernel_size, \n                 stride=1, padding=0, groups=1):\n        super(Conv, self).__init__()\n        self.add_module(\'norm\', nn.BatchNorm2d(in_channels))\n        self.add_module(\'relu\', nn.ReLU(inplace=True))\n        self.add_module(\'conv\', nn.Conv2d(in_channels, out_channels,\n                                          kernel_size=kernel_size,\n                                          stride=stride,\n                                          padding=padding, bias=False,\n                                          groups=groups))\n'"
ImageNet-C/test.py,30,"b'# -*- coding: utf-8 -*-\n\nimport argparse\nimport os\nimport time\nimport torch\nfrom torch.autograd import Variable as V\nimport torch.nn.functional as F\nimport torch.backends.cudnn as cudnn\nimport torchvision.datasets as dset\nimport torchvision.transforms as trn\nimport torchvision.models as models\nimport torch.utils.model_zoo as model_zoo\nimport numpy as np\nfrom resnext_50_32x4d import resnext_50_32x4d\nfrom resnext_101_32x4d import resnext_101_32x4d\nfrom resnext_101_64x4d import resnext_101_64x4d\nfrom densenet_cosine_264_k48 import densenet_cosine_264_k48\nfrom condensenet_converted import CondenseNet\n\nparser = argparse.ArgumentParser(description=\'Evaluates robustness of various nets on ImageNet\',\n                                 formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n# Architecture\nparser.add_argument(\'--model-name\', \'-m\', type=str,\n                    choices=[\'alexnet\', \'squeezenet1.0\', \'squeezenet1.1\', \'condensenet4\', \'condensenet8\',\n                             \'vgg11\', \'vgg\', \'vggbn\',\n                             \'densenet121\', \'densenet169\', \'densenet201\', \'densenet161\', \'densenet264\',\n                             \'resnet18\', \'resnet34\', \'resnet50\', \'resnet101\', \'resnet152\',\n                             \'resnext50\', \'resnext101\', \'resnext101_64\'])\n# Acceleration\nparser.add_argument(\'--ngpu\', type=int, default=1, help=\'0 = CPU.\')\nargs = parser.parse_args()\nprint(args)\n\n# /////////////// Model Setup ///////////////\n\nif args.model_name == \'alexnet\':\n    net = models.AlexNet()\n    net.load_state_dict(model_zoo.load_url(\'https://download.pytorch.org/models/alexnet-owt-4df8aa71.pth\',\n                                           model_dir=\'/share/data/lang/users/dan/.torch/models\'))\n    args.test_bs = 256\n\nelif args.model_name == \'squeezenet1.0\':\n    net = models.SqueezeNet(version=1.0)\n    net.load_state_dict(model_zoo.load_url(\'https://download.pytorch.org/models/squeezenet1_0-a815701f.pth\',\n                                           model_dir=\'/share/data/lang/users/dan/.torch/models\'))\n    args.test_bs = 256\n\nelif args.model_name == \'squeezenet1.1\':\n    net = models.SqueezeNet(version=1.1)\n    net.load_state_dict(model_zoo.load_url(\'https://download.pytorch.org/models/squeezenet1_1-f364aa15.pth\',\n                                           model_dir=\'/share/data/lang/users/dan/.torch/models\'))\n    args.test_bs = 256\n\nelif args.model_name == \'condensenet4\':\n    args.evaluate = True\n    args.stages = [4,6,8,10,8]\n    args.growth = [8,16,32,64,128]\n    args.data = \'imagenet\'\n    args.num_classes = 1000\n    args.bottleneck = 4\n    args.group_1x1 = 4\n    args.group_3x3 = 4\n    args.reduction = 0.5\n    args.condense_factor = 4\n    net = CondenseNet(args)\n    state_dict = torch.load(\'./converted_condensenet_4.pth\')[\'state_dict\']\n    for i in range(len(state_dict)):\n        name, v = state_dict.popitem(False)\n        state_dict[name[7:]] = v     # remove \'module.\' in key beginning\n    net.load_state_dict(state_dict)\n    args.test_bs = 256\n\nelif args.model_name == \'condensenet8\':\n    args.evaluate = True\n    args.stages = [4,6,8,10,8]\n    args.growth = [8,16,32,64,128]\n    args.data = \'imagenet\'\n    args.num_classes = 1000\n    args.bottleneck = 4\n    args.group_1x1 = 8\n    args.group_3x3 = 8\n    args.reduction = 0.5\n    args.condense_factor = 8\n    net = CondenseNet(args)\n    state_dict = torch.load(\'./converted_condensenet_8.pth\')[\'state_dict\']\n    for i in range(len(state_dict)):\n        name, v = state_dict.popitem(False)\n        state_dict[name[7:]] = v     # remove \'module.\' in key beginning\n    net.load_state_dict(state_dict)\n    args.test_bs = 256\n\nelif \'vgg\' in args.model_name:\n    if \'bn\' not in args.model_name:\n        net = models.vgg19()\n        net.load_state_dict(model_zoo.load_url(\'https://download.pytorch.org/models/vgg19-dcbb9e9d.pth\',\n                                               model_dir=\'/share/data/lang/users/dan/.torch/models\'))\n    elif \'11\' in args.model_name:\n        net = models.vgg11()\n        net.load_state_dict(model_zoo.load_url(\'https://download.pytorch.org/models/vgg11-bbd30ac9.pth\',\n                                               model_dir=\'/share/data/lang/users/dan/.torch/models\'))\n    else:\n        net = models.vgg19_bn()\n        net.load_state_dict(model_zoo.load_url(\'https://download.pytorch.org/models/vgg19_bn-c79401a0.pth\',\n                                               model_dir=\'/share/data/lang/users/dan/.torch/models\'))\n    args.test_bs = 64\n\nelif args.model_name == \'densenet121\':\n    net = models.densenet121()\n    net.load_state_dict(model_zoo.load_url(\'https://download.pytorch.org/models/densenet121-a639ec97.pth\',\n                                           model_dir=\'/share/data/lang/users/dan/.torch/models\'))\n    args.test_bs = 128\n\nelif args.model_name == \'densenet169\':\n    net = models.densenet169()\n    net.load_state_dict(model_zoo.load_url(\'https://download.pytorch.org/models/densenet169-6f0f7f60.pth\',\n                                           model_dir=\'/share/data/lang/users/dan/.torch/models\'))\n    args.test_bs = 128\n\nelif args.model_name == \'densenet201\':\n    net = models.densenet201()\n    net.load_state_dict(model_zoo.load_url(\'https://download.pytorch.org/models/densenet201-c1103571.pth\',\n                                           model_dir=\'/share/data/lang/users/dan/.torch/models\'))\n    args.test_bs = 64\n\nelif args.model_name == \'densenet161\':\n    net = models.densenet161()\n    net.load_state_dict(model_zoo.load_url(\'https://download.pytorch.org/models/densenet161-8d451a50.pth\',\n                                           model_dir=\'/share/data/lang/users/dan/.torch/models\'))\n    args.test_bs = 64\n\nelif args.model_name == \'densenet264\':\n    net = densenet_cosine_264_k48\n    net.load_state_dict(model_zoo.load_url(\'https://download.pytorch.org/models/densenet_cosine_264_k48.pth\',\n                                           model_dir=\'/share/data/lang/users/dan/.torch/models\'))\n    args.test_bs = 64\n\nelif args.model_name == \'resnet18\':\n    net = models.resnet18()\n    net.load_state_dict(model_zoo.load_url(\'https://download.pytorch.org/models/resnet18-5c106cde.pth\',\n                                           model_dir=\'/share/data/lang/users/dan/.torch/models\'))\n    args.test_bs = 256\n\nelif args.model_name == \'resnet34\':\n    net = models.resnet34()\n    net.load_state_dict(model_zoo.load_url(\'https://download.pytorch.org/models/resnet34-333f7ec4.pth\',\n                                           model_dir=\'/share/data/lang/users/dan/.torch/models\'))\n    args.test_bs = 128\n\nelif args.model_name == \'resnet50\':\n    net = models.resnet50()\n    net.load_state_dict(model_zoo.load_url(\'https://download.pytorch.org/models/resnet50-19c8e357.pth\',\n                                           model_dir=\'/share/data/lang/users/dan/.torch/models\'))\n    args.test_bs = 128\n\nelif args.model_name == \'resnet101\':\n    net = models.resnet101()\n    net.load_state_dict(model_zoo.load_url(\'https://download.pytorch.org/models/resnet101-5d3b4d8f.pth\',\n                                           model_dir=\'/share/data/lang/users/dan/.torch/models\'))\n    args.test_bs = 128\n\nelif args.model_name == \'resnet152\':\n    net = models.resnet152()\n    net.load_state_dict(model_zoo.load_url(\'https://download.pytorch.org/models/resnet152-b121ed2d.pth\',\n                                           model_dir=\'/share/data/lang/users/dan/.torch/models\'))\n    args.test_bs = 64\n\nelif args.model_name == \'resnext50\':\n    net = resnext_50_32x4d\n    net.load_state_dict(torch.load(\'/share/data/lang/users/dan/.torch/models/resnext_50_32x4d.pth\'))\n    args.test_bs = 64\n\nelif args.model_name == \'resnext101\':\n    net = resnext_101_32x4d\n    net.load_state_dict(torch.load(\'/share/data/lang/users/dan/.torch/models/resnext_101_32x4d.pth\'))\n    args.test_bs = 64\n\nelif args.model_name == \'resnext101_64\':\n    net = resnext_101_64x4d\n    net.load_state_dict(torch.load(\'/share/data/lang/users/dan/.torch/models/resnext_101_64x4d.pth\'))\n    args.test_bs = 64\n\nargs.prefetch = 4\n\nfor p in net.parameters():\n    p.volatile = True\n\nif args.ngpu > 1:\n    net = torch.nn.DataParallel(net, device_ids=list(range(args.ngpu)))\n\nif args.ngpu > 0:\n    net.cuda()\n\ntorch.manual_seed(1)\nnp.random.seed(1)\nif args.ngpu > 0:\n    torch.cuda.manual_seed(1)\n\nnet.eval()\ncudnn.benchmark = True  # fire on all cylinders\n\nprint(\'Model Loaded\')\n\n# /////////////// Data Loader ///////////////\n\nmean = [0.485, 0.456, 0.406]\nstd = [0.229, 0.224, 0.225]\n\nclean_loader = torch.utils.data.DataLoader(dset.ImageFolder(\n    root=""/share/data/vision-greg/ImageNet/clsloc/images/val"",\n    transform=trn.Compose([trn.Resize(256), trn.CenterCrop(224), trn.ToTensor(), trn.Normalize(mean, std)])),\n    batch_size=args.test_bs, shuffle=False, num_workers=args.prefetch, pin_memory=True)\n\n\n# /////////////// Further Setup ///////////////\n\ndef auc(errs):  # area under the distortion-error curve\n    area = 0\n    for i in range(1, len(errs)):\n        area += (errs[i] + errs[i - 1]) / 2\n    area /= len(errs) - 1\n    return area\n\n\n# correct = 0\n# for batch_idx, (data, target) in enumerate(clean_loader):\n#     data = V(data.cuda(), volatile=True)\n#\n#     output = net(data)\n#\n#     pred = output.data.max(1)[1]\n#     correct += pred.eq(target.cuda()).sum()\n#\n# clean_error = 1 - correct / len(clean_loader.dataset)\n# print(\'Clean dataset error (%): {:.2f}\'.format(100 * clean_error))\n\n\ndef show_performance(distortion_name):\n    errs = []\n\n    for severity in range(1, 6):\n        distorted_dataset = dset.ImageFolder(\n            root=\'/share/data/vision-greg/DistortedImageNet/JPEG/\' + distortion_name + \'/\' + str(severity),\n            transform=trn.Compose([trn.CenterCrop(224), trn.ToTensor(), trn.Normalize(mean, std)]))\n\n        distorted_dataset_loader = torch.utils.data.DataLoader(\n            distorted_dataset, batch_size=args.test_bs, shuffle=False, num_workers=args.prefetch, pin_memory=True)\n\n        correct = 0\n        for batch_idx, (data, target) in enumerate(distorted_dataset_loader):\n            data = V(data.cuda(), volatile=True)\n\n            output = net(data)\n\n            pred = output.data.max(1)[1]\n            correct += pred.eq(target.cuda()).sum()\n\n        errs.append(1 - 1.*correct / len(distorted_dataset))\n\n    print(\'\\n=Average\', tuple(errs))\n    return np.mean(errs)\n\n\n# /////////////// End Further Setup ///////////////\n\n\n# /////////////// Display Results ///////////////\nimport collections\n\nprint(\'\\nUsing ImageNet data\')\n\ndistortions = [\n    \'gaussian_noise\', \'shot_noise\', \'impulse_noise\',\n    \'defocus_blur\', \'glass_blur\', \'motion_blur\', \'zoom_blur\',\n    \'snow\', \'frost\', \'fog\', \'brightness\',\n    \'contrast\', \'elastic_transform\', \'pixelate\', \'jpeg_compression\',\n    \'speckle_noise\', \'gaussian_blur\', \'spatter\', \'saturate\'\n]\n\nerror_rates = []\nfor distortion_name in distortions:\n    rate = show_performance(distortion_name)\n    error_rates.append(rate)\n    print(\'Distortion: {:15s}  | CE (unnormalized) (%): {:.2f}\'.format(distortion_name, 100 * rate))\n\n\nprint(\'mCE (unnormalized by AlexNet errors) (%): {:.2f}\'.format(100 * np.mean(error_rates)))\n\n'"
ImageNet-P/cifar-p-eval.py,16,"b'# An example cifar-10/100-p evaluation script; some imports may not work out-of-the-box\n# -*- coding: utf-8 -*-\nimport numpy as np\nimport os\nimport argparse\nimport time\nimport torch\nimport torch.nn as nn\nimport torch.backends.cudnn as cudnn\nimport torchvision.transforms as trn\nimport torchvision.datasets as dset\nimport torch.nn.functional as F\nfrom tqdm import tqdm\nfrom models.wrn import WideResNet\nfrom models.resnext import resnext29\nfrom models.densenet import densenet\nfrom models.allconv import AllConvNet\n\n\nparser = argparse.ArgumentParser(description=\'Trains a CIFAR Classifier\',\n                                 formatter_class=argparse.ArgumentDefaultsHelpFormatter)\nparser.add_argument(\'--dataset\', \'-d\', type=str, default=\'cifar10\', choices=[\'cifar10\', \'cifar100\'],\n                    help=\'Choose between CIFAR-10, CIFAR-100.\')\nparser.add_argument(\'--model\', \'-m\', type=str, default=\'resnext\',\n                    choices=[\'wrn\', \'allconv\', \'densenet\', \'resnext\'], help=\'Choose architecture.\')\n# Optimization options\nparser.add_argument(\'--epochs\', \'-e\', type=int, default=100, help=\'Number of epochs to train.\')\nparser.add_argument(\'--learning_rate\', \'-lr\', type=float, default=0.1, help=\'The initial learning rate.\')\nparser.add_argument(\'--batch_size\', \'-b\', type=int, default=128, help=\'Batch size.\')\nparser.add_argument(\'--test_bs\', type=int, default=200)\nparser.add_argument(\'--momentum\', type=float, default=0.9, help=\'Momentum.\')\nparser.add_argument(\'--decay\', type=float, default=0.0005, help=\'Weight decay (L2 penalty).\')\n# WRN Architecture\nparser.add_argument(\'--layers\', default=40, type=int, help=\'total number of layers\')\nparser.add_argument(\'--widen-factor\', default=2, type=int, help=\'widen factor\')\nparser.add_argument(\'--droprate\', default=0.0, type=float, help=\'dropout probability\')\n# Checkpoints\nparser.add_argument(\'--save\', \'-s\', type=str, default=\'./snapshots/adv\', help=\'Folder to save checkpoints.\')\nparser.add_argument(\'--load\', \'-l\', type=str, default=\'./snapshots/augmix\', help=\'Checkpoint path to resume / test.\')\nparser.add_argument(\'--test\', \'-t\', action=\'store_true\', help=\'Test only flag.\')\n# Acceleration\nparser.add_argument(\'--ngpu\', type=int, default=1, help=\'0 = CPU.\')\nparser.add_argument(\'--prefetch\', type=int, default=2, help=\'Pre-fetching threads.\')\nargs = parser.parse_args()\n\nstate = {k: v for k, v in args._get_kwargs()}\nprint(state)\n\ntorch.manual_seed(1)\nnp.random.seed(1)\n\n# # mean and standard deviation of channels of CIFAR-10 images\n# mean = [x / 255 for x in [125.3, 123.0, 113.9]]\n# std = [x / 255 for x in [63.0, 62.1, 66.7]]\n\ntrain_transform = trn.Compose([trn.RandomHorizontalFlip(), trn.RandomCrop(32, padding=4),\n                               trn.ToTensor()])\ntest_transform = trn.Compose([trn.ToTensor()])\n\nif args.dataset == \'cifar10\':\n    train_data = dset.CIFAR10(\'~/datasets/cifarpy\', train=True, transform=train_transform)\n    test_data = dset.CIFAR10(\'~/datasets/cifarpy\', train=False, transform=test_transform)\n    num_classes = 10\nelse:\n    train_data = dset.CIFAR100(\'~/datasets/cifarpy\', train=True, transform=train_transform)\n    test_data = dset.CIFAR100(\'~/datasets/cifarpy\', train=False, transform=test_transform)\n    num_classes = 100\n\n\ntrain_loader = torch.utils.data.DataLoader(\n    train_data, batch_size=args.batch_size, shuffle=True,\n    num_workers=args.prefetch, pin_memory=True)\ntest_loader = torch.utils.data.DataLoader(\n    test_data, batch_size=args.test_bs, shuffle=False,\n    num_workers=args.prefetch, pin_memory=True)\n\n# Create model\nif args.model == \'densenet\':\n    args.decay = 0.0001\n    args.epochs = 200\n    net = densenet(num_classes=num_classes)\nelif args.model == \'wrn\':\n    net = WideResNet(args.layers, num_classes, args.widen_factor, dropRate=args.droprate)\nelif args.model == \'allconv\':\n    net = AllConvNet(num_classes)\nelif args.model == \'resnext\':\n    args.epochs = 200\n    net = resnext29(num_classes=num_classes)\n\nstate = {k: v for k, v in args._get_kwargs()}\nprint(state)\n\nstart_epoch = 0\n\n# Restore model if desired\nif args.load != \'\':\n    for i in range(1000 - 1, -1, -1):\n        model_name = os.path.join(args.load, args.dataset + \'_\' + args.model +\n                                  \'_baseline_epoch_\' + str(i) + \'.pt\')\n        if os.path.isfile(model_name):\n            net.load_state_dict(torch.load(model_name))\n            print(\'Model restored! Epoch:\', i)\n            start_epoch = i + 1\n            break\n        model_name = os.path.join(args.load, args.dataset + \'_\' + args.model + \'_\' + args.model +\n                                  \'_baseline_epoch_\' + str(i) + \'.pt\')\n        if os.path.isfile(model_name):\n            net.load_state_dict(torch.load(model_name))\n            print(\'Model restored! Epoch:\', i)\n            start_epoch = i + 1\n            break\n    if start_epoch == 0:\n        assert False, ""could not resume""\n\nif args.ngpu > 1:\n    net = torch.nn.DataParallel(net, device_ids=list(range(args.ngpu)))\n\nif args.ngpu > 0:\n    net.cuda()\n    torch.cuda.manual_seed(1)\n\ncudnn.benchmark = True  # fire on all cylinders\n\nnet.eval()\n\nconcat = lambda x: np.concatenate(x, axis=0)\nto_np = lambda x: x.data.to(\'cpu\').numpy()\n\ndef evaluate(loader):\n    confidence = []\n    correct = []\n\n    num_correct = 0\n    with torch.no_grad():\n        for data, target in loader:\n            data, target = data.cuda(), target.cuda()\n\n            output = net(2 * data - 1)\n\n            # accuracy\n            pred = output.data.max(1)[1]\n            num_correct += pred.eq(target.data).sum().item()\n\n            confidence.extend(to_np(F.softmax(output, dim=1).max(1)[0]).squeeze().tolist())\n            pred = output.data.max(1)[1]\n            correct.extend(pred.eq(target).to(\'cpu\').numpy().squeeze().tolist())\n\n    return num_correct / len(loader.dataset), np.array(confidence), np.array(correct)\n\n\nacc, test_confidence, test_correct = evaluate(test_loader)\nprint(\'Error\', 100 - 100. * acc)\nprint(\'RMS\', 100 * calib_err(test_confidence, test_correct, p=\'2\'))\n# print(\'AURRA\', 100 * aurra(test_confidence, test_correct))\n\n# /////////////// Stability Measurements ///////////////\n\nargs.difficulty = 1\nidentity = np.asarray(range(1, num_classes+1))\ncum_sum_top5 = np.cumsum(np.asarray([0] + [1] * 5 + [0] * (num_classes-1 - 5)))\nrecip = 1./identity\n\n\ndef dist(sigma, mode=\'top5\'):\n    if mode == \'top5\':\n        return np.sum(np.abs(cum_sum_top5[:5] - cum_sum_top5[sigma-1][:5]))\n    elif mode == \'zipf\':\n        return np.sum(np.abs(recip - recip[sigma-1])*recip)\n\n\ndef ranking_dist(ranks, noise_perturbation=False, mode=\'top5\'):\n    result = 0\n    step_size = 1 if noise_perturbation else args.difficulty\n\n    for vid_ranks in ranks:\n        result_for_vid = []\n\n        for i in range(step_size):\n            perm1 = vid_ranks[i]\n            perm1_inv = np.argsort(perm1)\n\n            for rank in vid_ranks[i::step_size][1:]:\n                perm2 = rank\n                result_for_vid.append(dist(perm2[perm1_inv], mode))\n                if not noise_perturbation:\n                    perm1 = perm2\n                    perm1_inv = np.argsort(perm1)\n\n        result += np.mean(result_for_vid) / len(ranks)\n\n    return result\n\n\ndef flip_prob(predictions, noise_perturbation=False):\n    result = 0\n    step_size = 1 if noise_perturbation else args.difficulty\n\n    for vid_preds in predictions:\n        result_for_vid = []\n\n        for i in range(step_size):\n            prev_pred = vid_preds[i]\n\n            for pred in vid_preds[i::step_size][1:]:\n                result_for_vid.append(int(prev_pred != pred))\n                if not noise_perturbation: prev_pred = pred\n\n        result += np.mean(result_for_vid) / len(predictions)\n\n    return result\n\n\n# /////////////// Get Results ///////////////\n\nfrom tqdm import tqdm\nfrom scipy.stats import rankdata\n\nc_p_dir =  \'CIFAR-10-P\' if num_classes == 10 else \'CIFAR-100-P\'\nc_p_dir = \'/home/hendrycks/datasets/\' + c_p_dir\n\n\ndummy_targets = torch.LongTensor(np.random.randint(0, num_classes, (10000,)))\n\nflip_list = []\nzipf_list = []\n\nfor p in [\'gaussian_noise\', \'shot_noise\', \'motion_blur\', \'zoom_blur\',\n          \'spatter\', \'brightness\', \'translate\', \'rotate\', \'tilt\', \'scale\']:\n    # ,\'speckle_noise\', \'gaussian_blur\', \'snow\', \'shear\']:\n    dataset = torch.from_numpy(np.float32(np.load(os.path.join(c_p_dir, p + \'.npy\')).transpose((0,1,4,2,3))))/255.\n\n    ood_data = torch.utils.data.TensorDataset(dataset, dummy_targets)\n\n    loader = torch.utils.data.DataLoader(\n        dataset, batch_size=25, shuffle=False, num_workers=2, pin_memory=True)\n\n    predictions, ranks = [], []\n\n    with torch.no_grad():\n\n        for data in loader:\n            num_vids = data.size(0)\n            data = data.view(-1,3,32,32).cuda()\n\n            output = net(data * 2 - 1)\n\n            for vid in output.view(num_vids, -1, num_classes):\n                predictions.append(vid.argmax(1).to(\'cpu\').numpy())\n                ranks.append([np.uint16(rankdata(-frame, method=\'ordinal\')) for frame in vid.to(\'cpu\').numpy()])\n\n        ranks = np.asarray(ranks)\n\n        # print(\'\\nComputing Metrics for\', p,)\n\n        current_flip = flip_prob(predictions, True if \'noise\' in p else False)\n        current_zipf = ranking_dist(ranks, True if \'noise\' in p else False, mode=\'zipf\')\n        flip_list.append(current_flip)\n        zipf_list.append(current_zipf)\n\n        print(\'\\n\' + p, \'Flipping Prob\')\n        print(current_flip)\n        # print(\'Top5 Distance\\t{:.5f}\'.format(ranking_dist(ranks, True if \'noise\' in p else False, mode=\'top5\')))\n        # print(\'Zipf Distance\\t{:.5f}\'.format(current_zipf))\n\nprint(flip_list)\nprint(\'\\nMean Flipping Prob\\t{:.5f}\'.format(np.mean(flip_list)))\n# print(\'Mean Zipf Distance\\t{:.5f}\'.format(np.mean(zipf_list)))\n'"
ImageNet-P/densenet_cosine_264_k48.py,133,"b'\nimport torch\nimport torch.nn as nn\nimport torch.legacy.nn as lnn\n\nfrom functools import reduce\nfrom torch.autograd import Variable\n\nclass LambdaBase(nn.Sequential):\n    def __init__(self, fn, *args):\n        super(LambdaBase, self).__init__(*args)\n        self.lambda_func = fn\n\n    def forward_prepare(self, input):\n        output = []\n        for module in self._modules.values():\n            output.append(module(input))\n        return output if output else input\n\nclass Lambda(LambdaBase):\n    def forward(self, input):\n        return self.lambda_func(self.forward_prepare(input))\n\nclass LambdaMap(LambdaBase):\n    def forward(self, input):\n        return list(map(self.lambda_func,self.forward_prepare(input)))\n\nclass LambdaReduce(LambdaBase):\n    def forward(self, input):\n        return reduce(self.lambda_func,self.forward_prepare(input))\n\n\ndensenet_cosine_264_k48 = nn.Sequential( # Sequential,\n\tnn.Conv2d(3,96,(7, 7),(2, 2),(3, 3),1,1,bias=False),\n\tnn.BatchNorm2d(96),\n\tnn.ReLU(),\n\tnn.MaxPool2d((3, 3),(2, 2),(1, 1)),\n\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n\t\tLambda(lambda x: x), # Identity,\n\t\tnn.Sequential( # Sequential,\n\t\t\tnn.BatchNorm2d(96),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(96,192,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\tnn.BatchNorm2d(192),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(192,48,(3, 3),(1, 1),(1, 1),1,1,bias=False),\n\t\t),\n\t),\n\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n\t\tLambda(lambda x: x), # Identity,\n\t\tnn.Sequential( # Sequential,\n\t\t\tnn.BatchNorm2d(144),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(144,192,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\tnn.BatchNorm2d(192),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(192,48,(3, 3),(1, 1),(1, 1),1,1,bias=False),\n\t\t),\n\t),\n\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n\t\tLambda(lambda x: x), # Identity,\n\t\tnn.Sequential( # Sequential,\n\t\t\tnn.BatchNorm2d(192),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(192,192,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\tnn.BatchNorm2d(192),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(192,48,(3, 3),(1, 1),(1, 1),1,1,bias=False),\n\t\t),\n\t),\n\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n\t\tLambda(lambda x: x), # Identity,\n\t\tnn.Sequential( # Sequential,\n\t\t\tnn.BatchNorm2d(240),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(240,192,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\tnn.BatchNorm2d(192),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(192,48,(3, 3),(1, 1),(1, 1),1,1,bias=False),\n\t\t),\n\t),\n\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n\t\tLambda(lambda x: x), # Identity,\n\t\tnn.Sequential( # Sequential,\n\t\t\tnn.BatchNorm2d(288),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(288,192,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\tnn.BatchNorm2d(192),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(192,48,(3, 3),(1, 1),(1, 1),1,1,bias=False),\n\t\t),\n\t),\n\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n\t\tLambda(lambda x: x), # Identity,\n\t\tnn.Sequential( # Sequential,\n\t\t\tnn.BatchNorm2d(336),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(336,192,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\tnn.BatchNorm2d(192),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(192,48,(3, 3),(1, 1),(1, 1),1,1,bias=False),\n\t\t),\n\t),\n\tnn.BatchNorm2d(384),\n\tnn.ReLU(),\n\tnn.Conv2d(384,192,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\tnn.AvgPool2d((2, 2),(2, 2)),\n\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n\t\tLambda(lambda x: x), # Identity,\n\t\tnn.Sequential( # Sequential,\n\t\t\tnn.BatchNorm2d(192),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(192,192,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\tnn.BatchNorm2d(192),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(192,48,(3, 3),(1, 1),(1, 1),1,1,bias=False),\n\t\t),\n\t),\n\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n\t\tLambda(lambda x: x), # Identity,\n\t\tnn.Sequential( # Sequential,\n\t\t\tnn.BatchNorm2d(240),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(240,192,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\tnn.BatchNorm2d(192),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(192,48,(3, 3),(1, 1),(1, 1),1,1,bias=False),\n\t\t),\n\t),\n\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n\t\tLambda(lambda x: x), # Identity,\n\t\tnn.Sequential( # Sequential,\n\t\t\tnn.BatchNorm2d(288),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(288,192,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\tnn.BatchNorm2d(192),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(192,48,(3, 3),(1, 1),(1, 1),1,1,bias=False),\n\t\t),\n\t),\n\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n\t\tLambda(lambda x: x), # Identity,\n\t\tnn.Sequential( # Sequential,\n\t\t\tnn.BatchNorm2d(336),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(336,192,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\tnn.BatchNorm2d(192),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(192,48,(3, 3),(1, 1),(1, 1),1,1,bias=False),\n\t\t),\n\t),\n\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n\t\tLambda(lambda x: x), # Identity,\n\t\tnn.Sequential( # Sequential,\n\t\t\tnn.BatchNorm2d(384),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(384,192,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\tnn.BatchNorm2d(192),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(192,48,(3, 3),(1, 1),(1, 1),1,1,bias=False),\n\t\t),\n\t),\n\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n\t\tLambda(lambda x: x), # Identity,\n\t\tnn.Sequential( # Sequential,\n\t\t\tnn.BatchNorm2d(432),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(432,192,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\tnn.BatchNorm2d(192),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(192,48,(3, 3),(1, 1),(1, 1),1,1,bias=False),\n\t\t),\n\t),\n\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n\t\tLambda(lambda x: x), # Identity,\n\t\tnn.Sequential( # Sequential,\n\t\t\tnn.BatchNorm2d(480),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(480,192,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\tnn.BatchNorm2d(192),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(192,48,(3, 3),(1, 1),(1, 1),1,1,bias=False),\n\t\t),\n\t),\n\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n\t\tLambda(lambda x: x), # Identity,\n\t\tnn.Sequential( # Sequential,\n\t\t\tnn.BatchNorm2d(528),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(528,192,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\tnn.BatchNorm2d(192),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(192,48,(3, 3),(1, 1),(1, 1),1,1,bias=False),\n\t\t),\n\t),\n\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n\t\tLambda(lambda x: x), # Identity,\n\t\tnn.Sequential( # Sequential,\n\t\t\tnn.BatchNorm2d(576),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(576,192,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\tnn.BatchNorm2d(192),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(192,48,(3, 3),(1, 1),(1, 1),1,1,bias=False),\n\t\t),\n\t),\n\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n\t\tLambda(lambda x: x), # Identity,\n\t\tnn.Sequential( # Sequential,\n\t\t\tnn.BatchNorm2d(624),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(624,192,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\tnn.BatchNorm2d(192),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(192,48,(3, 3),(1, 1),(1, 1),1,1,bias=False),\n\t\t),\n\t),\n\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n\t\tLambda(lambda x: x), # Identity,\n\t\tnn.Sequential( # Sequential,\n\t\t\tnn.BatchNorm2d(672),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(672,192,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\tnn.BatchNorm2d(192),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(192,48,(3, 3),(1, 1),(1, 1),1,1,bias=False),\n\t\t),\n\t),\n\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n\t\tLambda(lambda x: x), # Identity,\n\t\tnn.Sequential( # Sequential,\n\t\t\tnn.BatchNorm2d(720),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(720,192,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\tnn.BatchNorm2d(192),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(192,48,(3, 3),(1, 1),(1, 1),1,1,bias=False),\n\t\t),\n\t),\n\tnn.BatchNorm2d(768),\n\tnn.ReLU(),\n\tnn.Conv2d(768,384,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\tnn.AvgPool2d((2, 2),(2, 2)),\n\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n\t\tLambda(lambda x: x), # Identity,\n\t\tnn.Sequential( # Sequential,\n\t\t\tnn.BatchNorm2d(384),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(384,192,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\tnn.BatchNorm2d(192),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(192,48,(3, 3),(1, 1),(1, 1),1,1,bias=False),\n\t\t),\n\t),\n\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n\t\tLambda(lambda x: x), # Identity,\n\t\tnn.Sequential( # Sequential,\n\t\t\tnn.BatchNorm2d(432),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(432,192,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\tnn.BatchNorm2d(192),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(192,48,(3, 3),(1, 1),(1, 1),1,1,bias=False),\n\t\t),\n\t),\n\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n\t\tLambda(lambda x: x), # Identity,\n\t\tnn.Sequential( # Sequential,\n\t\t\tnn.BatchNorm2d(480),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(480,192,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\tnn.BatchNorm2d(192),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(192,48,(3, 3),(1, 1),(1, 1),1,1,bias=False),\n\t\t),\n\t),\n\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n\t\tLambda(lambda x: x), # Identity,\n\t\tnn.Sequential( # Sequential,\n\t\t\tnn.BatchNorm2d(528),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(528,192,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\tnn.BatchNorm2d(192),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(192,48,(3, 3),(1, 1),(1, 1),1,1,bias=False),\n\t\t),\n\t),\n\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n\t\tLambda(lambda x: x), # Identity,\n\t\tnn.Sequential( # Sequential,\n\t\t\tnn.BatchNorm2d(576),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(576,192,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\tnn.BatchNorm2d(192),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(192,48,(3, 3),(1, 1),(1, 1),1,1,bias=False),\n\t\t),\n\t),\n\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n\t\tLambda(lambda x: x), # Identity,\n\t\tnn.Sequential( # Sequential,\n\t\t\tnn.BatchNorm2d(624),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(624,192,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\tnn.BatchNorm2d(192),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(192,48,(3, 3),(1, 1),(1, 1),1,1,bias=False),\n\t\t),\n\t),\n\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n\t\tLambda(lambda x: x), # Identity,\n\t\tnn.Sequential( # Sequential,\n\t\t\tnn.BatchNorm2d(672),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(672,192,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\tnn.BatchNorm2d(192),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(192,48,(3, 3),(1, 1),(1, 1),1,1,bias=False),\n\t\t),\n\t),\n\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n\t\tLambda(lambda x: x), # Identity,\n\t\tnn.Sequential( # Sequential,\n\t\t\tnn.BatchNorm2d(720),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(720,192,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\tnn.BatchNorm2d(192),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(192,48,(3, 3),(1, 1),(1, 1),1,1,bias=False),\n\t\t),\n\t),\n\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n\t\tLambda(lambda x: x), # Identity,\n\t\tnn.Sequential( # Sequential,\n\t\t\tnn.BatchNorm2d(768),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(768,192,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\tnn.BatchNorm2d(192),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(192,48,(3, 3),(1, 1),(1, 1),1,1,bias=False),\n\t\t),\n\t),\n\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n\t\tLambda(lambda x: x), # Identity,\n\t\tnn.Sequential( # Sequential,\n\t\t\tnn.BatchNorm2d(816),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(816,192,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\tnn.BatchNorm2d(192),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(192,48,(3, 3),(1, 1),(1, 1),1,1,bias=False),\n\t\t),\n\t),\n\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n\t\tLambda(lambda x: x), # Identity,\n\t\tnn.Sequential( # Sequential,\n\t\t\tnn.BatchNorm2d(864),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(864,192,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\tnn.BatchNorm2d(192),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(192,48,(3, 3),(1, 1),(1, 1),1,1,bias=False),\n\t\t),\n\t),\n\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n\t\tLambda(lambda x: x), # Identity,\n\t\tnn.Sequential( # Sequential,\n\t\t\tnn.BatchNorm2d(912),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(912,192,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\tnn.BatchNorm2d(192),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(192,48,(3, 3),(1, 1),(1, 1),1,1,bias=False),\n\t\t),\n\t),\n\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n\t\tLambda(lambda x: x), # Identity,\n\t\tnn.Sequential( # Sequential,\n\t\t\tnn.BatchNorm2d(960),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(960,192,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\tnn.BatchNorm2d(192),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(192,48,(3, 3),(1, 1),(1, 1),1,1,bias=False),\n\t\t),\n\t),\n\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n\t\tLambda(lambda x: x), # Identity,\n\t\tnn.Sequential( # Sequential,\n\t\t\tnn.BatchNorm2d(1008),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(1008,192,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\tnn.BatchNorm2d(192),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(192,48,(3, 3),(1, 1),(1, 1),1,1,bias=False),\n\t\t),\n\t),\n\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n\t\tLambda(lambda x: x), # Identity,\n\t\tnn.Sequential( # Sequential,\n\t\t\tnn.BatchNorm2d(1056),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(1056,192,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\tnn.BatchNorm2d(192),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(192,48,(3, 3),(1, 1),(1, 1),1,1,bias=False),\n\t\t),\n\t),\n\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n\t\tLambda(lambda x: x), # Identity,\n\t\tnn.Sequential( # Sequential,\n\t\t\tnn.BatchNorm2d(1104),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(1104,192,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\tnn.BatchNorm2d(192),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(192,48,(3, 3),(1, 1),(1, 1),1,1,bias=False),\n\t\t),\n\t),\n\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n\t\tLambda(lambda x: x), # Identity,\n\t\tnn.Sequential( # Sequential,\n\t\t\tnn.BatchNorm2d(1152),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(1152,192,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\tnn.BatchNorm2d(192),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(192,48,(3, 3),(1, 1),(1, 1),1,1,bias=False),\n\t\t),\n\t),\n\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n\t\tLambda(lambda x: x), # Identity,\n\t\tnn.Sequential( # Sequential,\n\t\t\tnn.BatchNorm2d(1200),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(1200,192,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\tnn.BatchNorm2d(192),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(192,48,(3, 3),(1, 1),(1, 1),1,1,bias=False),\n\t\t),\n\t),\n\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n\t\tLambda(lambda x: x), # Identity,\n\t\tnn.Sequential( # Sequential,\n\t\t\tnn.BatchNorm2d(1248),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(1248,192,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\tnn.BatchNorm2d(192),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(192,48,(3, 3),(1, 1),(1, 1),1,1,bias=False),\n\t\t),\n\t),\n\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n\t\tLambda(lambda x: x), # Identity,\n\t\tnn.Sequential( # Sequential,\n\t\t\tnn.BatchNorm2d(1296),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(1296,192,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\tnn.BatchNorm2d(192),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(192,48,(3, 3),(1, 1),(1, 1),1,1,bias=False),\n\t\t),\n\t),\n\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n\t\tLambda(lambda x: x), # Identity,\n\t\tnn.Sequential( # Sequential,\n\t\t\tnn.BatchNorm2d(1344),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(1344,192,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\tnn.BatchNorm2d(192),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(192,48,(3, 3),(1, 1),(1, 1),1,1,bias=False),\n\t\t),\n\t),\n\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n\t\tLambda(lambda x: x), # Identity,\n\t\tnn.Sequential( # Sequential,\n\t\t\tnn.BatchNorm2d(1392),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(1392,192,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\tnn.BatchNorm2d(192),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(192,48,(3, 3),(1, 1),(1, 1),1,1,bias=False),\n\t\t),\n\t),\n\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n\t\tLambda(lambda x: x), # Identity,\n\t\tnn.Sequential( # Sequential,\n\t\t\tnn.BatchNorm2d(1440),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(1440,192,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\tnn.BatchNorm2d(192),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(192,48,(3, 3),(1, 1),(1, 1),1,1,bias=False),\n\t\t),\n\t),\n\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n\t\tLambda(lambda x: x), # Identity,\n\t\tnn.Sequential( # Sequential,\n\t\t\tnn.BatchNorm2d(1488),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(1488,192,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\tnn.BatchNorm2d(192),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(192,48,(3, 3),(1, 1),(1, 1),1,1,bias=False),\n\t\t),\n\t),\n\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n\t\tLambda(lambda x: x), # Identity,\n\t\tnn.Sequential( # Sequential,\n\t\t\tnn.BatchNorm2d(1536),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(1536,192,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\tnn.BatchNorm2d(192),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(192,48,(3, 3),(1, 1),(1, 1),1,1,bias=False),\n\t\t),\n\t),\n\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n\t\tLambda(lambda x: x), # Identity,\n\t\tnn.Sequential( # Sequential,\n\t\t\tnn.BatchNorm2d(1584),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(1584,192,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\tnn.BatchNorm2d(192),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(192,48,(3, 3),(1, 1),(1, 1),1,1,bias=False),\n\t\t),\n\t),\n\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n\t\tLambda(lambda x: x), # Identity,\n\t\tnn.Sequential( # Sequential,\n\t\t\tnn.BatchNorm2d(1632),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(1632,192,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\tnn.BatchNorm2d(192),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(192,48,(3, 3),(1, 1),(1, 1),1,1,bias=False),\n\t\t),\n\t),\n\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n\t\tLambda(lambda x: x), # Identity,\n\t\tnn.Sequential( # Sequential,\n\t\t\tnn.BatchNorm2d(1680),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(1680,192,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\tnn.BatchNorm2d(192),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(192,48,(3, 3),(1, 1),(1, 1),1,1,bias=False),\n\t\t),\n\t),\n\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n\t\tLambda(lambda x: x), # Identity,\n\t\tnn.Sequential( # Sequential,\n\t\t\tnn.BatchNorm2d(1728),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(1728,192,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\tnn.BatchNorm2d(192),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(192,48,(3, 3),(1, 1),(1, 1),1,1,bias=False),\n\t\t),\n\t),\n\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n\t\tLambda(lambda x: x), # Identity,\n\t\tnn.Sequential( # Sequential,\n\t\t\tnn.BatchNorm2d(1776),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(1776,192,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\tnn.BatchNorm2d(192),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(192,48,(3, 3),(1, 1),(1, 1),1,1,bias=False),\n\t\t),\n\t),\n\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n\t\tLambda(lambda x: x), # Identity,\n\t\tnn.Sequential( # Sequential,\n\t\t\tnn.BatchNorm2d(1824),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(1824,192,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\tnn.BatchNorm2d(192),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(192,48,(3, 3),(1, 1),(1, 1),1,1,bias=False),\n\t\t),\n\t),\n\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n\t\tLambda(lambda x: x), # Identity,\n\t\tnn.Sequential( # Sequential,\n\t\t\tnn.BatchNorm2d(1872),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(1872,192,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\tnn.BatchNorm2d(192),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(192,48,(3, 3),(1, 1),(1, 1),1,1,bias=False),\n\t\t),\n\t),\n\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n\t\tLambda(lambda x: x), # Identity,\n\t\tnn.Sequential( # Sequential,\n\t\t\tnn.BatchNorm2d(1920),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(1920,192,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\tnn.BatchNorm2d(192),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(192,48,(3, 3),(1, 1),(1, 1),1,1,bias=False),\n\t\t),\n\t),\n\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n\t\tLambda(lambda x: x), # Identity,\n\t\tnn.Sequential( # Sequential,\n\t\t\tnn.BatchNorm2d(1968),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(1968,192,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\tnn.BatchNorm2d(192),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(192,48,(3, 3),(1, 1),(1, 1),1,1,bias=False),\n\t\t),\n\t),\n\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n\t\tLambda(lambda x: x), # Identity,\n\t\tnn.Sequential( # Sequential,\n\t\t\tnn.BatchNorm2d(2016),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(2016,192,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\tnn.BatchNorm2d(192),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(192,48,(3, 3),(1, 1),(1, 1),1,1,bias=False),\n\t\t),\n\t),\n\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n\t\tLambda(lambda x: x), # Identity,\n\t\tnn.Sequential( # Sequential,\n\t\t\tnn.BatchNorm2d(2064),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(2064,192,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\tnn.BatchNorm2d(192),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(192,48,(3, 3),(1, 1),(1, 1),1,1,bias=False),\n\t\t),\n\t),\n\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n\t\tLambda(lambda x: x), # Identity,\n\t\tnn.Sequential( # Sequential,\n\t\t\tnn.BatchNorm2d(2112),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(2112,192,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\tnn.BatchNorm2d(192),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(192,48,(3, 3),(1, 1),(1, 1),1,1,bias=False),\n\t\t),\n\t),\n\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n\t\tLambda(lambda x: x), # Identity,\n\t\tnn.Sequential( # Sequential,\n\t\t\tnn.BatchNorm2d(2160),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(2160,192,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\tnn.BatchNorm2d(192),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(192,48,(3, 3),(1, 1),(1, 1),1,1,bias=False),\n\t\t),\n\t),\n\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n\t\tLambda(lambda x: x), # Identity,\n\t\tnn.Sequential( # Sequential,\n\t\t\tnn.BatchNorm2d(2208),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(2208,192,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\tnn.BatchNorm2d(192),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(192,48,(3, 3),(1, 1),(1, 1),1,1,bias=False),\n\t\t),\n\t),\n\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n\t\tLambda(lambda x: x), # Identity,\n\t\tnn.Sequential( # Sequential,\n\t\t\tnn.BatchNorm2d(2256),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(2256,192,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\tnn.BatchNorm2d(192),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(192,48,(3, 3),(1, 1),(1, 1),1,1,bias=False),\n\t\t),\n\t),\n\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n\t\tLambda(lambda x: x), # Identity,\n\t\tnn.Sequential( # Sequential,\n\t\t\tnn.BatchNorm2d(2304),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(2304,192,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\tnn.BatchNorm2d(192),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(192,48,(3, 3),(1, 1),(1, 1),1,1,bias=False),\n\t\t),\n\t),\n\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n\t\tLambda(lambda x: x), # Identity,\n\t\tnn.Sequential( # Sequential,\n\t\t\tnn.BatchNorm2d(2352),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(2352,192,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\tnn.BatchNorm2d(192),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(192,48,(3, 3),(1, 1),(1, 1),1,1,bias=False),\n\t\t),\n\t),\n\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n\t\tLambda(lambda x: x), # Identity,\n\t\tnn.Sequential( # Sequential,\n\t\t\tnn.BatchNorm2d(2400),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(2400,192,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\tnn.BatchNorm2d(192),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(192,48,(3, 3),(1, 1),(1, 1),1,1,bias=False),\n\t\t),\n\t),\n\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n\t\tLambda(lambda x: x), # Identity,\n\t\tnn.Sequential( # Sequential,\n\t\t\tnn.BatchNorm2d(2448),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(2448,192,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\tnn.BatchNorm2d(192),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(192,48,(3, 3),(1, 1),(1, 1),1,1,bias=False),\n\t\t),\n\t),\n\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n\t\tLambda(lambda x: x), # Identity,\n\t\tnn.Sequential( # Sequential,\n\t\t\tnn.BatchNorm2d(2496),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(2496,192,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\tnn.BatchNorm2d(192),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(192,48,(3, 3),(1, 1),(1, 1),1,1,bias=False),\n\t\t),\n\t),\n\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n\t\tLambda(lambda x: x), # Identity,\n\t\tnn.Sequential( # Sequential,\n\t\t\tnn.BatchNorm2d(2544),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(2544,192,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\tnn.BatchNorm2d(192),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(192,48,(3, 3),(1, 1),(1, 1),1,1,bias=False),\n\t\t),\n\t),\n\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n\t\tLambda(lambda x: x), # Identity,\n\t\tnn.Sequential( # Sequential,\n\t\t\tnn.BatchNorm2d(2592),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(2592,192,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\tnn.BatchNorm2d(192),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(192,48,(3, 3),(1, 1),(1, 1),1,1,bias=False),\n\t\t),\n\t),\n\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n\t\tLambda(lambda x: x), # Identity,\n\t\tnn.Sequential( # Sequential,\n\t\t\tnn.BatchNorm2d(2640),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(2640,192,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\tnn.BatchNorm2d(192),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(192,48,(3, 3),(1, 1),(1, 1),1,1,bias=False),\n\t\t),\n\t),\n\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n\t\tLambda(lambda x: x), # Identity,\n\t\tnn.Sequential( # Sequential,\n\t\t\tnn.BatchNorm2d(2688),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(2688,192,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\tnn.BatchNorm2d(192),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(192,48,(3, 3),(1, 1),(1, 1),1,1,bias=False),\n\t\t),\n\t),\n\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n\t\tLambda(lambda x: x), # Identity,\n\t\tnn.Sequential( # Sequential,\n\t\t\tnn.BatchNorm2d(2736),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(2736,192,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\tnn.BatchNorm2d(192),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(192,48,(3, 3),(1, 1),(1, 1),1,1,bias=False),\n\t\t),\n\t),\n\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n\t\tLambda(lambda x: x), # Identity,\n\t\tnn.Sequential( # Sequential,\n\t\t\tnn.BatchNorm2d(2784),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(2784,192,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\tnn.BatchNorm2d(192),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(192,48,(3, 3),(1, 1),(1, 1),1,1,bias=False),\n\t\t),\n\t),\n\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n\t\tLambda(lambda x: x), # Identity,\n\t\tnn.Sequential( # Sequential,\n\t\t\tnn.BatchNorm2d(2832),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(2832,192,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\tnn.BatchNorm2d(192),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(192,48,(3, 3),(1, 1),(1, 1),1,1,bias=False),\n\t\t),\n\t),\n\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n\t\tLambda(lambda x: x), # Identity,\n\t\tnn.Sequential( # Sequential,\n\t\t\tnn.BatchNorm2d(2880),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(2880,192,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\tnn.BatchNorm2d(192),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(192,48,(3, 3),(1, 1),(1, 1),1,1,bias=False),\n\t\t),\n\t),\n\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n\t\tLambda(lambda x: x), # Identity,\n\t\tnn.Sequential( # Sequential,\n\t\t\tnn.BatchNorm2d(2928),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(2928,192,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\tnn.BatchNorm2d(192),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(192,48,(3, 3),(1, 1),(1, 1),1,1,bias=False),\n\t\t),\n\t),\n\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n\t\tLambda(lambda x: x), # Identity,\n\t\tnn.Sequential( # Sequential,\n\t\t\tnn.BatchNorm2d(2976),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(2976,192,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\tnn.BatchNorm2d(192),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(192,48,(3, 3),(1, 1),(1, 1),1,1,bias=False),\n\t\t),\n\t),\n\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n\t\tLambda(lambda x: x), # Identity,\n\t\tnn.Sequential( # Sequential,\n\t\t\tnn.BatchNorm2d(3024),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(3024,192,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\tnn.BatchNorm2d(192),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(192,48,(3, 3),(1, 1),(1, 1),1,1,bias=False),\n\t\t),\n\t),\n\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n\t\tLambda(lambda x: x), # Identity,\n\t\tnn.Sequential( # Sequential,\n\t\t\tnn.BatchNorm2d(3072),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(3072,192,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\tnn.BatchNorm2d(192),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(192,48,(3, 3),(1, 1),(1, 1),1,1,bias=False),\n\t\t),\n\t),\n\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n\t\tLambda(lambda x: x), # Identity,\n\t\tnn.Sequential( # Sequential,\n\t\t\tnn.BatchNorm2d(3120),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(3120,192,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\tnn.BatchNorm2d(192),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(192,48,(3, 3),(1, 1),(1, 1),1,1,bias=False),\n\t\t),\n\t),\n\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n\t\tLambda(lambda x: x), # Identity,\n\t\tnn.Sequential( # Sequential,\n\t\t\tnn.BatchNorm2d(3168),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(3168,192,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\tnn.BatchNorm2d(192),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(192,48,(3, 3),(1, 1),(1, 1),1,1,bias=False),\n\t\t),\n\t),\n\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n\t\tLambda(lambda x: x), # Identity,\n\t\tnn.Sequential( # Sequential,\n\t\t\tnn.BatchNorm2d(3216),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(3216,192,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\tnn.BatchNorm2d(192),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(192,48,(3, 3),(1, 1),(1, 1),1,1,bias=False),\n\t\t),\n\t),\n\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n\t\tLambda(lambda x: x), # Identity,\n\t\tnn.Sequential( # Sequential,\n\t\t\tnn.BatchNorm2d(3264),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(3264,192,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\tnn.BatchNorm2d(192),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(192,48,(3, 3),(1, 1),(1, 1),1,1,bias=False),\n\t\t),\n\t),\n\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n\t\tLambda(lambda x: x), # Identity,\n\t\tnn.Sequential( # Sequential,\n\t\t\tnn.BatchNorm2d(3312),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(3312,192,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\tnn.BatchNorm2d(192),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(192,48,(3, 3),(1, 1),(1, 1),1,1,bias=False),\n\t\t),\n\t),\n\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n\t\tLambda(lambda x: x), # Identity,\n\t\tnn.Sequential( # Sequential,\n\t\t\tnn.BatchNorm2d(3360),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(3360,192,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\tnn.BatchNorm2d(192),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(192,48,(3, 3),(1, 1),(1, 1),1,1,bias=False),\n\t\t),\n\t),\n\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n\t\tLambda(lambda x: x), # Identity,\n\t\tnn.Sequential( # Sequential,\n\t\t\tnn.BatchNorm2d(3408),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(3408,192,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\tnn.BatchNorm2d(192),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(192,48,(3, 3),(1, 1),(1, 1),1,1,bias=False),\n\t\t),\n\t),\n\tnn.BatchNorm2d(3456),\n\tnn.ReLU(),\n\tnn.Conv2d(3456,1728,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\tnn.AvgPool2d((2, 2),(2, 2)),\n\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n\t\tLambda(lambda x: x), # Identity,\n\t\tnn.Sequential( # Sequential,\n\t\t\tnn.BatchNorm2d(1728),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(1728,192,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\tnn.BatchNorm2d(192),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(192,48,(3, 3),(1, 1),(1, 1),1,1,bias=False),\n\t\t),\n\t),\n\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n\t\tLambda(lambda x: x), # Identity,\n\t\tnn.Sequential( # Sequential,\n\t\t\tnn.BatchNorm2d(1776),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(1776,192,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\tnn.BatchNorm2d(192),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(192,48,(3, 3),(1, 1),(1, 1),1,1,bias=False),\n\t\t),\n\t),\n\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n\t\tLambda(lambda x: x), # Identity,\n\t\tnn.Sequential( # Sequential,\n\t\t\tnn.BatchNorm2d(1824),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(1824,192,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\tnn.BatchNorm2d(192),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(192,48,(3, 3),(1, 1),(1, 1),1,1,bias=False),\n\t\t),\n\t),\n\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n\t\tLambda(lambda x: x), # Identity,\n\t\tnn.Sequential( # Sequential,\n\t\t\tnn.BatchNorm2d(1872),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(1872,192,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\tnn.BatchNorm2d(192),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(192,48,(3, 3),(1, 1),(1, 1),1,1,bias=False),\n\t\t),\n\t),\n\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n\t\tLambda(lambda x: x), # Identity,\n\t\tnn.Sequential( # Sequential,\n\t\t\tnn.BatchNorm2d(1920),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(1920,192,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\tnn.BatchNorm2d(192),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(192,48,(3, 3),(1, 1),(1, 1),1,1,bias=False),\n\t\t),\n\t),\n\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n\t\tLambda(lambda x: x), # Identity,\n\t\tnn.Sequential( # Sequential,\n\t\t\tnn.BatchNorm2d(1968),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(1968,192,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\tnn.BatchNorm2d(192),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(192,48,(3, 3),(1, 1),(1, 1),1,1,bias=False),\n\t\t),\n\t),\n\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n\t\tLambda(lambda x: x), # Identity,\n\t\tnn.Sequential( # Sequential,\n\t\t\tnn.BatchNorm2d(2016),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(2016,192,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\tnn.BatchNorm2d(192),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(192,48,(3, 3),(1, 1),(1, 1),1,1,bias=False),\n\t\t),\n\t),\n\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n\t\tLambda(lambda x: x), # Identity,\n\t\tnn.Sequential( # Sequential,\n\t\t\tnn.BatchNorm2d(2064),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(2064,192,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\tnn.BatchNorm2d(192),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(192,48,(3, 3),(1, 1),(1, 1),1,1,bias=False),\n\t\t),\n\t),\n\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n\t\tLambda(lambda x: x), # Identity,\n\t\tnn.Sequential( # Sequential,\n\t\t\tnn.BatchNorm2d(2112),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(2112,192,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\tnn.BatchNorm2d(192),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(192,48,(3, 3),(1, 1),(1, 1),1,1,bias=False),\n\t\t),\n\t),\n\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n\t\tLambda(lambda x: x), # Identity,\n\t\tnn.Sequential( # Sequential,\n\t\t\tnn.BatchNorm2d(2160),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(2160,192,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\tnn.BatchNorm2d(192),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(192,48,(3, 3),(1, 1),(1, 1),1,1,bias=False),\n\t\t),\n\t),\n\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n\t\tLambda(lambda x: x), # Identity,\n\t\tnn.Sequential( # Sequential,\n\t\t\tnn.BatchNorm2d(2208),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(2208,192,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\tnn.BatchNorm2d(192),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(192,48,(3, 3),(1, 1),(1, 1),1,1,bias=False),\n\t\t),\n\t),\n\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n\t\tLambda(lambda x: x), # Identity,\n\t\tnn.Sequential( # Sequential,\n\t\t\tnn.BatchNorm2d(2256),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(2256,192,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\tnn.BatchNorm2d(192),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(192,48,(3, 3),(1, 1),(1, 1),1,1,bias=False),\n\t\t),\n\t),\n\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n\t\tLambda(lambda x: x), # Identity,\n\t\tnn.Sequential( # Sequential,\n\t\t\tnn.BatchNorm2d(2304),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(2304,192,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\tnn.BatchNorm2d(192),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(192,48,(3, 3),(1, 1),(1, 1),1,1,bias=False),\n\t\t),\n\t),\n\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n\t\tLambda(lambda x: x), # Identity,\n\t\tnn.Sequential( # Sequential,\n\t\t\tnn.BatchNorm2d(2352),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(2352,192,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\tnn.BatchNorm2d(192),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(192,48,(3, 3),(1, 1),(1, 1),1,1,bias=False),\n\t\t),\n\t),\n\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n\t\tLambda(lambda x: x), # Identity,\n\t\tnn.Sequential( # Sequential,\n\t\t\tnn.BatchNorm2d(2400),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(2400,192,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\tnn.BatchNorm2d(192),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(192,48,(3, 3),(1, 1),(1, 1),1,1,bias=False),\n\t\t),\n\t),\n\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n\t\tLambda(lambda x: x), # Identity,\n\t\tnn.Sequential( # Sequential,\n\t\t\tnn.BatchNorm2d(2448),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(2448,192,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\tnn.BatchNorm2d(192),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(192,48,(3, 3),(1, 1),(1, 1),1,1,bias=False),\n\t\t),\n\t),\n\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n\t\tLambda(lambda x: x), # Identity,\n\t\tnn.Sequential( # Sequential,\n\t\t\tnn.BatchNorm2d(2496),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(2496,192,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\tnn.BatchNorm2d(192),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(192,48,(3, 3),(1, 1),(1, 1),1,1,bias=False),\n\t\t),\n\t),\n\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n\t\tLambda(lambda x: x), # Identity,\n\t\tnn.Sequential( # Sequential,\n\t\t\tnn.BatchNorm2d(2544),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(2544,192,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\tnn.BatchNorm2d(192),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(192,48,(3, 3),(1, 1),(1, 1),1,1,bias=False),\n\t\t),\n\t),\n\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n\t\tLambda(lambda x: x), # Identity,\n\t\tnn.Sequential( # Sequential,\n\t\t\tnn.BatchNorm2d(2592),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(2592,192,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\tnn.BatchNorm2d(192),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(192,48,(3, 3),(1, 1),(1, 1),1,1,bias=False),\n\t\t),\n\t),\n\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n\t\tLambda(lambda x: x), # Identity,\n\t\tnn.Sequential( # Sequential,\n\t\t\tnn.BatchNorm2d(2640),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(2640,192,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\tnn.BatchNorm2d(192),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(192,48,(3, 3),(1, 1),(1, 1),1,1,bias=False),\n\t\t),\n\t),\n\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n\t\tLambda(lambda x: x), # Identity,\n\t\tnn.Sequential( # Sequential,\n\t\t\tnn.BatchNorm2d(2688),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(2688,192,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\tnn.BatchNorm2d(192),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(192,48,(3, 3),(1, 1),(1, 1),1,1,bias=False),\n\t\t),\n\t),\n\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n\t\tLambda(lambda x: x), # Identity,\n\t\tnn.Sequential( # Sequential,\n\t\t\tnn.BatchNorm2d(2736),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(2736,192,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\tnn.BatchNorm2d(192),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(192,48,(3, 3),(1, 1),(1, 1),1,1,bias=False),\n\t\t),\n\t),\n\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n\t\tLambda(lambda x: x), # Identity,\n\t\tnn.Sequential( # Sequential,\n\t\t\tnn.BatchNorm2d(2784),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(2784,192,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\tnn.BatchNorm2d(192),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(192,48,(3, 3),(1, 1),(1, 1),1,1,bias=False),\n\t\t),\n\t),\n\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n\t\tLambda(lambda x: x), # Identity,\n\t\tnn.Sequential( # Sequential,\n\t\t\tnn.BatchNorm2d(2832),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(2832,192,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\tnn.BatchNorm2d(192),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(192,48,(3, 3),(1, 1),(1, 1),1,1,bias=False),\n\t\t),\n\t),\n\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n\t\tLambda(lambda x: x), # Identity,\n\t\tnn.Sequential( # Sequential,\n\t\t\tnn.BatchNorm2d(2880),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(2880,192,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\tnn.BatchNorm2d(192),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(192,48,(3, 3),(1, 1),(1, 1),1,1,bias=False),\n\t\t),\n\t),\n\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n\t\tLambda(lambda x: x), # Identity,\n\t\tnn.Sequential( # Sequential,\n\t\t\tnn.BatchNorm2d(2928),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(2928,192,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\tnn.BatchNorm2d(192),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(192,48,(3, 3),(1, 1),(1, 1),1,1,bias=False),\n\t\t),\n\t),\n\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n\t\tLambda(lambda x: x), # Identity,\n\t\tnn.Sequential( # Sequential,\n\t\t\tnn.BatchNorm2d(2976),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(2976,192,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\tnn.BatchNorm2d(192),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(192,48,(3, 3),(1, 1),(1, 1),1,1,bias=False),\n\t\t),\n\t),\n\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n\t\tLambda(lambda x: x), # Identity,\n\t\tnn.Sequential( # Sequential,\n\t\t\tnn.BatchNorm2d(3024),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(3024,192,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\tnn.BatchNorm2d(192),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(192,48,(3, 3),(1, 1),(1, 1),1,1,bias=False),\n\t\t),\n\t),\n\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n\t\tLambda(lambda x: x), # Identity,\n\t\tnn.Sequential( # Sequential,\n\t\t\tnn.BatchNorm2d(3072),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(3072,192,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\tnn.BatchNorm2d(192),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(192,48,(3, 3),(1, 1),(1, 1),1,1,bias=False),\n\t\t),\n\t),\n\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n\t\tLambda(lambda x: x), # Identity,\n\t\tnn.Sequential( # Sequential,\n\t\t\tnn.BatchNorm2d(3120),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(3120,192,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\tnn.BatchNorm2d(192),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(192,48,(3, 3),(1, 1),(1, 1),1,1,bias=False),\n\t\t),\n\t),\n\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n\t\tLambda(lambda x: x), # Identity,\n\t\tnn.Sequential( # Sequential,\n\t\t\tnn.BatchNorm2d(3168),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(3168,192,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\tnn.BatchNorm2d(192),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(192,48,(3, 3),(1, 1),(1, 1),1,1,bias=False),\n\t\t),\n\t),\n\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n\t\tLambda(lambda x: x), # Identity,\n\t\tnn.Sequential( # Sequential,\n\t\t\tnn.BatchNorm2d(3216),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(3216,192,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\tnn.BatchNorm2d(192),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(192,48,(3, 3),(1, 1),(1, 1),1,1,bias=False),\n\t\t),\n\t),\n\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n\t\tLambda(lambda x: x), # Identity,\n\t\tnn.Sequential( # Sequential,\n\t\t\tnn.BatchNorm2d(3264),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(3264,192,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\tnn.BatchNorm2d(192),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(192,48,(3, 3),(1, 1),(1, 1),1,1,bias=False),\n\t\t),\n\t),\n\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n\t\tLambda(lambda x: x), # Identity,\n\t\tnn.Sequential( # Sequential,\n\t\t\tnn.BatchNorm2d(3312),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(3312,192,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\tnn.BatchNorm2d(192),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(192,48,(3, 3),(1, 1),(1, 1),1,1,bias=False),\n\t\t),\n\t),\n\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n\t\tLambda(lambda x: x), # Identity,\n\t\tnn.Sequential( # Sequential,\n\t\t\tnn.BatchNorm2d(3360),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(3360,192,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\tnn.BatchNorm2d(192),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(192,48,(3, 3),(1, 1),(1, 1),1,1,bias=False),\n\t\t),\n\t),\n\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n\t\tLambda(lambda x: x), # Identity,\n\t\tnn.Sequential( # Sequential,\n\t\t\tnn.BatchNorm2d(3408),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(3408,192,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\tnn.BatchNorm2d(192),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(192,48,(3, 3),(1, 1),(1, 1),1,1,bias=False),\n\t\t),\n\t),\n\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n\t\tLambda(lambda x: x), # Identity,\n\t\tnn.Sequential( # Sequential,\n\t\t\tnn.BatchNorm2d(3456),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(3456,192,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\tnn.BatchNorm2d(192),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(192,48,(3, 3),(1, 1),(1, 1),1,1,bias=False),\n\t\t),\n\t),\n\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n\t\tLambda(lambda x: x), # Identity,\n\t\tnn.Sequential( # Sequential,\n\t\t\tnn.BatchNorm2d(3504),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(3504,192,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\tnn.BatchNorm2d(192),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(192,48,(3, 3),(1, 1),(1, 1),1,1,bias=False),\n\t\t),\n\t),\n\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n\t\tLambda(lambda x: x), # Identity,\n\t\tnn.Sequential( # Sequential,\n\t\t\tnn.BatchNorm2d(3552),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(3552,192,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\tnn.BatchNorm2d(192),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(192,48,(3, 3),(1, 1),(1, 1),1,1,bias=False),\n\t\t),\n\t),\n\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n\t\tLambda(lambda x: x), # Identity,\n\t\tnn.Sequential( # Sequential,\n\t\t\tnn.BatchNorm2d(3600),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(3600,192,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\tnn.BatchNorm2d(192),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(192,48,(3, 3),(1, 1),(1, 1),1,1,bias=False),\n\t\t),\n\t),\n\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n\t\tLambda(lambda x: x), # Identity,\n\t\tnn.Sequential( # Sequential,\n\t\t\tnn.BatchNorm2d(3648),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(3648,192,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\tnn.BatchNorm2d(192),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(192,48,(3, 3),(1, 1),(1, 1),1,1,bias=False),\n\t\t),\n\t),\n\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n\t\tLambda(lambda x: x), # Identity,\n\t\tnn.Sequential( # Sequential,\n\t\t\tnn.BatchNorm2d(3696),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(3696,192,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\tnn.BatchNorm2d(192),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(192,48,(3, 3),(1, 1),(1, 1),1,1,bias=False),\n\t\t),\n\t),\n\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n\t\tLambda(lambda x: x), # Identity,\n\t\tnn.Sequential( # Sequential,\n\t\t\tnn.BatchNorm2d(3744),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(3744,192,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\tnn.BatchNorm2d(192),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(192,48,(3, 3),(1, 1),(1, 1),1,1,bias=False),\n\t\t),\n\t),\n\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n\t\tLambda(lambda x: x), # Identity,\n\t\tnn.Sequential( # Sequential,\n\t\t\tnn.BatchNorm2d(3792),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(3792,192,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\tnn.BatchNorm2d(192),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(192,48,(3, 3),(1, 1),(1, 1),1,1,bias=False),\n\t\t),\n\t),\n\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n\t\tLambda(lambda x: x), # Identity,\n\t\tnn.Sequential( # Sequential,\n\t\t\tnn.BatchNorm2d(3840),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(3840,192,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\tnn.BatchNorm2d(192),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(192,48,(3, 3),(1, 1),(1, 1),1,1,bias=False),\n\t\t),\n\t),\n\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n\t\tLambda(lambda x: x), # Identity,\n\t\tnn.Sequential( # Sequential,\n\t\t\tnn.BatchNorm2d(3888),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(3888,192,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\tnn.BatchNorm2d(192),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(192,48,(3, 3),(1, 1),(1, 1),1,1,bias=False),\n\t\t),\n\t),\n\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n\t\tLambda(lambda x: x), # Identity,\n\t\tnn.Sequential( # Sequential,\n\t\t\tnn.BatchNorm2d(3936),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(3936,192,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\tnn.BatchNorm2d(192),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(192,48,(3, 3),(1, 1),(1, 1),1,1,bias=False),\n\t\t),\n\t),\n\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n\t\tLambda(lambda x: x), # Identity,\n\t\tnn.Sequential( # Sequential,\n\t\t\tnn.BatchNorm2d(3984),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(3984,192,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\tnn.BatchNorm2d(192),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(192,48,(3, 3),(1, 1),(1, 1),1,1,bias=False),\n\t\t),\n\t),\n\tnn.BatchNorm2d(4032),\n\tnn.ReLU(),\n\tnn.AvgPool2d((7, 7),(7, 7)),\n\tLambda(lambda x: x.view(x.size(0),-1)), # Reshape,\n\tnn.Sequential(Lambda(lambda x: x.view(1,-1) if 1==len(x.size()) else x ),nn.Linear(4032,1000)), # Linear,\n)'"
ImageNet-P/resnext_101_32x4d.py,2,"b'\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\nfrom functools import reduce\n\nclass LambdaBase(nn.Sequential):\n    def __init__(self, fn, *args):\n        super(LambdaBase, self).__init__(*args)\n        self.lambda_func = fn\n\n    def forward_prepare(self, input):\n        output = []\n        for module in self._modules.values():\n            output.append(module(input))\n        return output if output else input\n\nclass Lambda(LambdaBase):\n    def forward(self, input):\n        return self.lambda_func(self.forward_prepare(input))\n\nclass LambdaMap(LambdaBase):\n    def forward(self, input):\n        return list(map(self.lambda_func,self.forward_prepare(input)))\n\nclass LambdaReduce(LambdaBase):\n    def forward(self, input):\n        return reduce(self.lambda_func,self.forward_prepare(input))\n\n\nresnext_101_32x4d = nn.Sequential( # Sequential,\n\tnn.Conv2d(3,64,(7, 7),(2, 2),(3, 3),1,1,bias=False),\n\tnn.BatchNorm2d(64),\n\tnn.ReLU(),\n\tnn.MaxPool2d((3, 3),(2, 2),(1, 1)),\n\tnn.Sequential( # Sequential,\n\t\tnn.Sequential( # Sequential,\n\t\t\tLambdaMap(lambda x: x, # ConcatTable,\n\t\t\t\tnn.Sequential( # Sequential,\n\t\t\t\t\tnn.Sequential( # Sequential,\n\t\t\t\t\t\tnn.Conv2d(64,128,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\t\t\t\tnn.BatchNorm2d(128),\n\t\t\t\t\t\tnn.ReLU(),\n\t\t\t\t\t\tnn.Conv2d(128,128,(3, 3),(1, 1),(1, 1),1,32,bias=False),\n\t\t\t\t\t\tnn.BatchNorm2d(128),\n\t\t\t\t\t\tnn.ReLU(),\n\t\t\t\t\t),\n\t\t\t\t\tnn.Conv2d(128,256,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\t\t\tnn.BatchNorm2d(256),\n\t\t\t\t),\n\t\t\t\tnn.Sequential( # Sequential,\n\t\t\t\t\tnn.Conv2d(64,256,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\t\t\tnn.BatchNorm2d(256),\n\t\t\t\t),\n\t\t\t),\n\t\t\tLambdaReduce(lambda x,y: x+y), # CAddTable,\n\t\t\tnn.ReLU(),\n\t\t),\n\t\tnn.Sequential( # Sequential,\n\t\t\tLambdaMap(lambda x: x, # ConcatTable,\n\t\t\t\tnn.Sequential( # Sequential,\n\t\t\t\t\tnn.Sequential( # Sequential,\n\t\t\t\t\t\tnn.Conv2d(256,128,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\t\t\t\tnn.BatchNorm2d(128),\n\t\t\t\t\t\tnn.ReLU(),\n\t\t\t\t\t\tnn.Conv2d(128,128,(3, 3),(1, 1),(1, 1),1,32,bias=False),\n\t\t\t\t\t\tnn.BatchNorm2d(128),\n\t\t\t\t\t\tnn.ReLU(),\n\t\t\t\t\t),\n\t\t\t\t\tnn.Conv2d(128,256,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\t\t\tnn.BatchNorm2d(256),\n\t\t\t\t),\n\t\t\t\tLambda(lambda x: x), # Identity,\n\t\t\t),\n\t\t\tLambdaReduce(lambda x,y: x+y), # CAddTable,\n\t\t\tnn.ReLU(),\n\t\t),\n\t\tnn.Sequential( # Sequential,\n\t\t\tLambdaMap(lambda x: x, # ConcatTable,\n\t\t\t\tnn.Sequential( # Sequential,\n\t\t\t\t\tnn.Sequential( # Sequential,\n\t\t\t\t\t\tnn.Conv2d(256,128,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\t\t\t\tnn.BatchNorm2d(128),\n\t\t\t\t\t\tnn.ReLU(),\n\t\t\t\t\t\tnn.Conv2d(128,128,(3, 3),(1, 1),(1, 1),1,32,bias=False),\n\t\t\t\t\t\tnn.BatchNorm2d(128),\n\t\t\t\t\t\tnn.ReLU(),\n\t\t\t\t\t),\n\t\t\t\t\tnn.Conv2d(128,256,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\t\t\tnn.BatchNorm2d(256),\n\t\t\t\t),\n\t\t\t\tLambda(lambda x: x), # Identity,\n\t\t\t),\n\t\t\tLambdaReduce(lambda x,y: x+y), # CAddTable,\n\t\t\tnn.ReLU(),\n\t\t),\n\t),\n\tnn.Sequential( # Sequential,\n\t\tnn.Sequential( # Sequential,\n\t\t\tLambdaMap(lambda x: x, # ConcatTable,\n\t\t\t\tnn.Sequential( # Sequential,\n\t\t\t\t\tnn.Sequential( # Sequential,\n\t\t\t\t\t\tnn.Conv2d(256,256,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\t\t\t\tnn.BatchNorm2d(256),\n\t\t\t\t\t\tnn.ReLU(),\n\t\t\t\t\t\tnn.Conv2d(256,256,(3, 3),(2, 2),(1, 1),1,32,bias=False),\n\t\t\t\t\t\tnn.BatchNorm2d(256),\n\t\t\t\t\t\tnn.ReLU(),\n\t\t\t\t\t),\n\t\t\t\t\tnn.Conv2d(256,512,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\t\t\tnn.BatchNorm2d(512),\n\t\t\t\t),\n\t\t\t\tnn.Sequential( # Sequential,\n\t\t\t\t\tnn.Conv2d(256,512,(1, 1),(2, 2),(0, 0),1,1,bias=False),\n\t\t\t\t\tnn.BatchNorm2d(512),\n\t\t\t\t),\n\t\t\t),\n\t\t\tLambdaReduce(lambda x,y: x+y), # CAddTable,\n\t\t\tnn.ReLU(),\n\t\t),\n\t\tnn.Sequential( # Sequential,\n\t\t\tLambdaMap(lambda x: x, # ConcatTable,\n\t\t\t\tnn.Sequential( # Sequential,\n\t\t\t\t\tnn.Sequential( # Sequential,\n\t\t\t\t\t\tnn.Conv2d(512,256,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\t\t\t\tnn.BatchNorm2d(256),\n\t\t\t\t\t\tnn.ReLU(),\n\t\t\t\t\t\tnn.Conv2d(256,256,(3, 3),(1, 1),(1, 1),1,32,bias=False),\n\t\t\t\t\t\tnn.BatchNorm2d(256),\n\t\t\t\t\t\tnn.ReLU(),\n\t\t\t\t\t),\n\t\t\t\t\tnn.Conv2d(256,512,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\t\t\tnn.BatchNorm2d(512),\n\t\t\t\t),\n\t\t\t\tLambda(lambda x: x), # Identity,\n\t\t\t),\n\t\t\tLambdaReduce(lambda x,y: x+y), # CAddTable,\n\t\t\tnn.ReLU(),\n\t\t),\n\t\tnn.Sequential( # Sequential,\n\t\t\tLambdaMap(lambda x: x, # ConcatTable,\n\t\t\t\tnn.Sequential( # Sequential,\n\t\t\t\t\tnn.Sequential( # Sequential,\n\t\t\t\t\t\tnn.Conv2d(512,256,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\t\t\t\tnn.BatchNorm2d(256),\n\t\t\t\t\t\tnn.ReLU(),\n\t\t\t\t\t\tnn.Conv2d(256,256,(3, 3),(1, 1),(1, 1),1,32,bias=False),\n\t\t\t\t\t\tnn.BatchNorm2d(256),\n\t\t\t\t\t\tnn.ReLU(),\n\t\t\t\t\t),\n\t\t\t\t\tnn.Conv2d(256,512,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\t\t\tnn.BatchNorm2d(512),\n\t\t\t\t),\n\t\t\t\tLambda(lambda x: x), # Identity,\n\t\t\t),\n\t\t\tLambdaReduce(lambda x,y: x+y), # CAddTable,\n\t\t\tnn.ReLU(),\n\t\t),\n\t\tnn.Sequential( # Sequential,\n\t\t\tLambdaMap(lambda x: x, # ConcatTable,\n\t\t\t\tnn.Sequential( # Sequential,\n\t\t\t\t\tnn.Sequential( # Sequential,\n\t\t\t\t\t\tnn.Conv2d(512,256,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\t\t\t\tnn.BatchNorm2d(256),\n\t\t\t\t\t\tnn.ReLU(),\n\t\t\t\t\t\tnn.Conv2d(256,256,(3, 3),(1, 1),(1, 1),1,32,bias=False),\n\t\t\t\t\t\tnn.BatchNorm2d(256),\n\t\t\t\t\t\tnn.ReLU(),\n\t\t\t\t\t),\n\t\t\t\t\tnn.Conv2d(256,512,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\t\t\tnn.BatchNorm2d(512),\n\t\t\t\t),\n\t\t\t\tLambda(lambda x: x), # Identity,\n\t\t\t),\n\t\t\tLambdaReduce(lambda x,y: x+y), # CAddTable,\n\t\t\tnn.ReLU(),\n\t\t),\n\t),\n\tnn.Sequential( # Sequential,\n\t\tnn.Sequential( # Sequential,\n\t\t\tLambdaMap(lambda x: x, # ConcatTable,\n\t\t\t\tnn.Sequential( # Sequential,\n\t\t\t\t\tnn.Sequential( # Sequential,\n\t\t\t\t\t\tnn.Conv2d(512,512,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\t\t\t\tnn.BatchNorm2d(512),\n\t\t\t\t\t\tnn.ReLU(),\n\t\t\t\t\t\tnn.Conv2d(512,512,(3, 3),(2, 2),(1, 1),1,32,bias=False),\n\t\t\t\t\t\tnn.BatchNorm2d(512),\n\t\t\t\t\t\tnn.ReLU(),\n\t\t\t\t\t),\n\t\t\t\t\tnn.Conv2d(512,1024,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\t\t\tnn.BatchNorm2d(1024),\n\t\t\t\t),\n\t\t\t\tnn.Sequential( # Sequential,\n\t\t\t\t\tnn.Conv2d(512,1024,(1, 1),(2, 2),(0, 0),1,1,bias=False),\n\t\t\t\t\tnn.BatchNorm2d(1024),\n\t\t\t\t),\n\t\t\t),\n\t\t\tLambdaReduce(lambda x,y: x+y), # CAddTable,\n\t\t\tnn.ReLU(),\n\t\t),\n\t\tnn.Sequential( # Sequential,\n\t\t\tLambdaMap(lambda x: x, # ConcatTable,\n\t\t\t\tnn.Sequential( # Sequential,\n\t\t\t\t\tnn.Sequential( # Sequential,\n\t\t\t\t\t\tnn.Conv2d(1024,512,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\t\t\t\tnn.BatchNorm2d(512),\n\t\t\t\t\t\tnn.ReLU(),\n\t\t\t\t\t\tnn.Conv2d(512,512,(3, 3),(1, 1),(1, 1),1,32,bias=False),\n\t\t\t\t\t\tnn.BatchNorm2d(512),\n\t\t\t\t\t\tnn.ReLU(),\n\t\t\t\t\t),\n\t\t\t\t\tnn.Conv2d(512,1024,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\t\t\tnn.BatchNorm2d(1024),\n\t\t\t\t),\n\t\t\t\tLambda(lambda x: x), # Identity,\n\t\t\t),\n\t\t\tLambdaReduce(lambda x,y: x+y), # CAddTable,\n\t\t\tnn.ReLU(),\n\t\t),\n\t\tnn.Sequential( # Sequential,\n\t\t\tLambdaMap(lambda x: x, # ConcatTable,\n\t\t\t\tnn.Sequential( # Sequential,\n\t\t\t\t\tnn.Sequential( # Sequential,\n\t\t\t\t\t\tnn.Conv2d(1024,512,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\t\t\t\tnn.BatchNorm2d(512),\n\t\t\t\t\t\tnn.ReLU(),\n\t\t\t\t\t\tnn.Conv2d(512,512,(3, 3),(1, 1),(1, 1),1,32,bias=False),\n\t\t\t\t\t\tnn.BatchNorm2d(512),\n\t\t\t\t\t\tnn.ReLU(),\n\t\t\t\t\t),\n\t\t\t\t\tnn.Conv2d(512,1024,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\t\t\tnn.BatchNorm2d(1024),\n\t\t\t\t),\n\t\t\t\tLambda(lambda x: x), # Identity,\n\t\t\t),\n\t\t\tLambdaReduce(lambda x,y: x+y), # CAddTable,\n\t\t\tnn.ReLU(),\n\t\t),\n\t\tnn.Sequential( # Sequential,\n\t\t\tLambdaMap(lambda x: x, # ConcatTable,\n\t\t\t\tnn.Sequential( # Sequential,\n\t\t\t\t\tnn.Sequential( # Sequential,\n\t\t\t\t\t\tnn.Conv2d(1024,512,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\t\t\t\tnn.BatchNorm2d(512),\n\t\t\t\t\t\tnn.ReLU(),\n\t\t\t\t\t\tnn.Conv2d(512,512,(3, 3),(1, 1),(1, 1),1,32,bias=False),\n\t\t\t\t\t\tnn.BatchNorm2d(512),\n\t\t\t\t\t\tnn.ReLU(),\n\t\t\t\t\t),\n\t\t\t\t\tnn.Conv2d(512,1024,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\t\t\tnn.BatchNorm2d(1024),\n\t\t\t\t),\n\t\t\t\tLambda(lambda x: x), # Identity,\n\t\t\t),\n\t\t\tLambdaReduce(lambda x,y: x+y), # CAddTable,\n\t\t\tnn.ReLU(),\n\t\t),\n\t\tnn.Sequential( # Sequential,\n\t\t\tLambdaMap(lambda x: x, # ConcatTable,\n\t\t\t\tnn.Sequential( # Sequential,\n\t\t\t\t\tnn.Sequential( # Sequential,\n\t\t\t\t\t\tnn.Conv2d(1024,512,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\t\t\t\tnn.BatchNorm2d(512),\n\t\t\t\t\t\tnn.ReLU(),\n\t\t\t\t\t\tnn.Conv2d(512,512,(3, 3),(1, 1),(1, 1),1,32,bias=False),\n\t\t\t\t\t\tnn.BatchNorm2d(512),\n\t\t\t\t\t\tnn.ReLU(),\n\t\t\t\t\t),\n\t\t\t\t\tnn.Conv2d(512,1024,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\t\t\tnn.BatchNorm2d(1024),\n\t\t\t\t),\n\t\t\t\tLambda(lambda x: x), # Identity,\n\t\t\t),\n\t\t\tLambdaReduce(lambda x,y: x+y), # CAddTable,\n\t\t\tnn.ReLU(),\n\t\t),\n\t\tnn.Sequential( # Sequential,\n\t\t\tLambdaMap(lambda x: x, # ConcatTable,\n\t\t\t\tnn.Sequential( # Sequential,\n\t\t\t\t\tnn.Sequential( # Sequential,\n\t\t\t\t\t\tnn.Conv2d(1024,512,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\t\t\t\tnn.BatchNorm2d(512),\n\t\t\t\t\t\tnn.ReLU(),\n\t\t\t\t\t\tnn.Conv2d(512,512,(3, 3),(1, 1),(1, 1),1,32,bias=False),\n\t\t\t\t\t\tnn.BatchNorm2d(512),\n\t\t\t\t\t\tnn.ReLU(),\n\t\t\t\t\t),\n\t\t\t\t\tnn.Conv2d(512,1024,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\t\t\tnn.BatchNorm2d(1024),\n\t\t\t\t),\n\t\t\t\tLambda(lambda x: x), # Identity,\n\t\t\t),\n\t\t\tLambdaReduce(lambda x,y: x+y), # CAddTable,\n\t\t\tnn.ReLU(),\n\t\t),\n\t\tnn.Sequential( # Sequential,\n\t\t\tLambdaMap(lambda x: x, # ConcatTable,\n\t\t\t\tnn.Sequential( # Sequential,\n\t\t\t\t\tnn.Sequential( # Sequential,\n\t\t\t\t\t\tnn.Conv2d(1024,512,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\t\t\t\tnn.BatchNorm2d(512),\n\t\t\t\t\t\tnn.ReLU(),\n\t\t\t\t\t\tnn.Conv2d(512,512,(3, 3),(1, 1),(1, 1),1,32,bias=False),\n\t\t\t\t\t\tnn.BatchNorm2d(512),\n\t\t\t\t\t\tnn.ReLU(),\n\t\t\t\t\t),\n\t\t\t\t\tnn.Conv2d(512,1024,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\t\t\tnn.BatchNorm2d(1024),\n\t\t\t\t),\n\t\t\t\tLambda(lambda x: x), # Identity,\n\t\t\t),\n\t\t\tLambdaReduce(lambda x,y: x+y), # CAddTable,\n\t\t\tnn.ReLU(),\n\t\t),\n\t\tnn.Sequential( # Sequential,\n\t\t\tLambdaMap(lambda x: x, # ConcatTable,\n\t\t\t\tnn.Sequential( # Sequential,\n\t\t\t\t\tnn.Sequential( # Sequential,\n\t\t\t\t\t\tnn.Conv2d(1024,512,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\t\t\t\tnn.BatchNorm2d(512),\n\t\t\t\t\t\tnn.ReLU(),\n\t\t\t\t\t\tnn.Conv2d(512,512,(3, 3),(1, 1),(1, 1),1,32,bias=False),\n\t\t\t\t\t\tnn.BatchNorm2d(512),\n\t\t\t\t\t\tnn.ReLU(),\n\t\t\t\t\t),\n\t\t\t\t\tnn.Conv2d(512,1024,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\t\t\tnn.BatchNorm2d(1024),\n\t\t\t\t),\n\t\t\t\tLambda(lambda x: x), # Identity,\n\t\t\t),\n\t\t\tLambdaReduce(lambda x,y: x+y), # CAddTable,\n\t\t\tnn.ReLU(),\n\t\t),\n\t\tnn.Sequential( # Sequential,\n\t\t\tLambdaMap(lambda x: x, # ConcatTable,\n\t\t\t\tnn.Sequential( # Sequential,\n\t\t\t\t\tnn.Sequential( # Sequential,\n\t\t\t\t\t\tnn.Conv2d(1024,512,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\t\t\t\tnn.BatchNorm2d(512),\n\t\t\t\t\t\tnn.ReLU(),\n\t\t\t\t\t\tnn.Conv2d(512,512,(3, 3),(1, 1),(1, 1),1,32,bias=False),\n\t\t\t\t\t\tnn.BatchNorm2d(512),\n\t\t\t\t\t\tnn.ReLU(),\n\t\t\t\t\t),\n\t\t\t\t\tnn.Conv2d(512,1024,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\t\t\tnn.BatchNorm2d(1024),\n\t\t\t\t),\n\t\t\t\tLambda(lambda x: x), # Identity,\n\t\t\t),\n\t\t\tLambdaReduce(lambda x,y: x+y), # CAddTable,\n\t\t\tnn.ReLU(),\n\t\t),\n\t\tnn.Sequential( # Sequential,\n\t\t\tLambdaMap(lambda x: x, # ConcatTable,\n\t\t\t\tnn.Sequential( # Sequential,\n\t\t\t\t\tnn.Sequential( # Sequential,\n\t\t\t\t\t\tnn.Conv2d(1024,512,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\t\t\t\tnn.BatchNorm2d(512),\n\t\t\t\t\t\tnn.ReLU(),\n\t\t\t\t\t\tnn.Conv2d(512,512,(3, 3),(1, 1),(1, 1),1,32,bias=False),\n\t\t\t\t\t\tnn.BatchNorm2d(512),\n\t\t\t\t\t\tnn.ReLU(),\n\t\t\t\t\t),\n\t\t\t\t\tnn.Conv2d(512,1024,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\t\t\tnn.BatchNorm2d(1024),\n\t\t\t\t),\n\t\t\t\tLambda(lambda x: x), # Identity,\n\t\t\t),\n\t\t\tLambdaReduce(lambda x,y: x+y), # CAddTable,\n\t\t\tnn.ReLU(),\n\t\t),\n\t\tnn.Sequential( # Sequential,\n\t\t\tLambdaMap(lambda x: x, # ConcatTable,\n\t\t\t\tnn.Sequential( # Sequential,\n\t\t\t\t\tnn.Sequential( # Sequential,\n\t\t\t\t\t\tnn.Conv2d(1024,512,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\t\t\t\tnn.BatchNorm2d(512),\n\t\t\t\t\t\tnn.ReLU(),\n\t\t\t\t\t\tnn.Conv2d(512,512,(3, 3),(1, 1),(1, 1),1,32,bias=False),\n\t\t\t\t\t\tnn.BatchNorm2d(512),\n\t\t\t\t\t\tnn.ReLU(),\n\t\t\t\t\t),\n\t\t\t\t\tnn.Conv2d(512,1024,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\t\t\tnn.BatchNorm2d(1024),\n\t\t\t\t),\n\t\t\t\tLambda(lambda x: x), # Identity,\n\t\t\t),\n\t\t\tLambdaReduce(lambda x,y: x+y), # CAddTable,\n\t\t\tnn.ReLU(),\n\t\t),\n\t\tnn.Sequential( # Sequential,\n\t\t\tLambdaMap(lambda x: x, # ConcatTable,\n\t\t\t\tnn.Sequential( # Sequential,\n\t\t\t\t\tnn.Sequential( # Sequential,\n\t\t\t\t\t\tnn.Conv2d(1024,512,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\t\t\t\tnn.BatchNorm2d(512),\n\t\t\t\t\t\tnn.ReLU(),\n\t\t\t\t\t\tnn.Conv2d(512,512,(3, 3),(1, 1),(1, 1),1,32,bias=False),\n\t\t\t\t\t\tnn.BatchNorm2d(512),\n\t\t\t\t\t\tnn.ReLU(),\n\t\t\t\t\t),\n\t\t\t\t\tnn.Conv2d(512,1024,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\t\t\tnn.BatchNorm2d(1024),\n\t\t\t\t),\n\t\t\t\tLambda(lambda x: x), # Identity,\n\t\t\t),\n\t\t\tLambdaReduce(lambda x,y: x+y), # CAddTable,\n\t\t\tnn.ReLU(),\n\t\t),\n\t\tnn.Sequential( # Sequential,\n\t\t\tLambdaMap(lambda x: x, # ConcatTable,\n\t\t\t\tnn.Sequential( # Sequential,\n\t\t\t\t\tnn.Sequential( # Sequential,\n\t\t\t\t\t\tnn.Conv2d(1024,512,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\t\t\t\tnn.BatchNorm2d(512),\n\t\t\t\t\t\tnn.ReLU(),\n\t\t\t\t\t\tnn.Conv2d(512,512,(3, 3),(1, 1),(1, 1),1,32,bias=False),\n\t\t\t\t\t\tnn.BatchNorm2d(512),\n\t\t\t\t\t\tnn.ReLU(),\n\t\t\t\t\t),\n\t\t\t\t\tnn.Conv2d(512,1024,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\t\t\tnn.BatchNorm2d(1024),\n\t\t\t\t),\n\t\t\t\tLambda(lambda x: x), # Identity,\n\t\t\t),\n\t\t\tLambdaReduce(lambda x,y: x+y), # CAddTable,\n\t\t\tnn.ReLU(),\n\t\t),\n\t\tnn.Sequential( # Sequential,\n\t\t\tLambdaMap(lambda x: x, # ConcatTable,\n\t\t\t\tnn.Sequential( # Sequential,\n\t\t\t\t\tnn.Sequential( # Sequential,\n\t\t\t\t\t\tnn.Conv2d(1024,512,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\t\t\t\tnn.BatchNorm2d(512),\n\t\t\t\t\t\tnn.ReLU(),\n\t\t\t\t\t\tnn.Conv2d(512,512,(3, 3),(1, 1),(1, 1),1,32,bias=False),\n\t\t\t\t\t\tnn.BatchNorm2d(512),\n\t\t\t\t\t\tnn.ReLU(),\n\t\t\t\t\t),\n\t\t\t\t\tnn.Conv2d(512,1024,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\t\t\tnn.BatchNorm2d(1024),\n\t\t\t\t),\n\t\t\t\tLambda(lambda x: x), # Identity,\n\t\t\t),\n\t\t\tLambdaReduce(lambda x,y: x+y), # CAddTable,\n\t\t\tnn.ReLU(),\n\t\t),\n\t\tnn.Sequential( # Sequential,\n\t\t\tLambdaMap(lambda x: x, # ConcatTable,\n\t\t\t\tnn.Sequential( # Sequential,\n\t\t\t\t\tnn.Sequential( # Sequential,\n\t\t\t\t\t\tnn.Conv2d(1024,512,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\t\t\t\tnn.BatchNorm2d(512),\n\t\t\t\t\t\tnn.ReLU(),\n\t\t\t\t\t\tnn.Conv2d(512,512,(3, 3),(1, 1),(1, 1),1,32,bias=False),\n\t\t\t\t\t\tnn.BatchNorm2d(512),\n\t\t\t\t\t\tnn.ReLU(),\n\t\t\t\t\t),\n\t\t\t\t\tnn.Conv2d(512,1024,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\t\t\tnn.BatchNorm2d(1024),\n\t\t\t\t),\n\t\t\t\tLambda(lambda x: x), # Identity,\n\t\t\t),\n\t\t\tLambdaReduce(lambda x,y: x+y), # CAddTable,\n\t\t\tnn.ReLU(),\n\t\t),\n\t\tnn.Sequential( # Sequential,\n\t\t\tLambdaMap(lambda x: x, # ConcatTable,\n\t\t\t\tnn.Sequential( # Sequential,\n\t\t\t\t\tnn.Sequential( # Sequential,\n\t\t\t\t\t\tnn.Conv2d(1024,512,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\t\t\t\tnn.BatchNorm2d(512),\n\t\t\t\t\t\tnn.ReLU(),\n\t\t\t\t\t\tnn.Conv2d(512,512,(3, 3),(1, 1),(1, 1),1,32,bias=False),\n\t\t\t\t\t\tnn.BatchNorm2d(512),\n\t\t\t\t\t\tnn.ReLU(),\n\t\t\t\t\t),\n\t\t\t\t\tnn.Conv2d(512,1024,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\t\t\tnn.BatchNorm2d(1024),\n\t\t\t\t),\n\t\t\t\tLambda(lambda x: x), # Identity,\n\t\t\t),\n\t\t\tLambdaReduce(lambda x,y: x+y), # CAddTable,\n\t\t\tnn.ReLU(),\n\t\t),\n\t\tnn.Sequential( # Sequential,\n\t\t\tLambdaMap(lambda x: x, # ConcatTable,\n\t\t\t\tnn.Sequential( # Sequential,\n\t\t\t\t\tnn.Sequential( # Sequential,\n\t\t\t\t\t\tnn.Conv2d(1024,512,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\t\t\t\tnn.BatchNorm2d(512),\n\t\t\t\t\t\tnn.ReLU(),\n\t\t\t\t\t\tnn.Conv2d(512,512,(3, 3),(1, 1),(1, 1),1,32,bias=False),\n\t\t\t\t\t\tnn.BatchNorm2d(512),\n\t\t\t\t\t\tnn.ReLU(),\n\t\t\t\t\t),\n\t\t\t\t\tnn.Conv2d(512,1024,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\t\t\tnn.BatchNorm2d(1024),\n\t\t\t\t),\n\t\t\t\tLambda(lambda x: x), # Identity,\n\t\t\t),\n\t\t\tLambdaReduce(lambda x,y: x+y), # CAddTable,\n\t\t\tnn.ReLU(),\n\t\t),\n\t\tnn.Sequential( # Sequential,\n\t\t\tLambdaMap(lambda x: x, # ConcatTable,\n\t\t\t\tnn.Sequential( # Sequential,\n\t\t\t\t\tnn.Sequential( # Sequential,\n\t\t\t\t\t\tnn.Conv2d(1024,512,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\t\t\t\tnn.BatchNorm2d(512),\n\t\t\t\t\t\tnn.ReLU(),\n\t\t\t\t\t\tnn.Conv2d(512,512,(3, 3),(1, 1),(1, 1),1,32,bias=False),\n\t\t\t\t\t\tnn.BatchNorm2d(512),\n\t\t\t\t\t\tnn.ReLU(),\n\t\t\t\t\t),\n\t\t\t\t\tnn.Conv2d(512,1024,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\t\t\tnn.BatchNorm2d(1024),\n\t\t\t\t),\n\t\t\t\tLambda(lambda x: x), # Identity,\n\t\t\t),\n\t\t\tLambdaReduce(lambda x,y: x+y), # CAddTable,\n\t\t\tnn.ReLU(),\n\t\t),\n\t\tnn.Sequential( # Sequential,\n\t\t\tLambdaMap(lambda x: x, # ConcatTable,\n\t\t\t\tnn.Sequential( # Sequential,\n\t\t\t\t\tnn.Sequential( # Sequential,\n\t\t\t\t\t\tnn.Conv2d(1024,512,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\t\t\t\tnn.BatchNorm2d(512),\n\t\t\t\t\t\tnn.ReLU(),\n\t\t\t\t\t\tnn.Conv2d(512,512,(3, 3),(1, 1),(1, 1),1,32,bias=False),\n\t\t\t\t\t\tnn.BatchNorm2d(512),\n\t\t\t\t\t\tnn.ReLU(),\n\t\t\t\t\t),\n\t\t\t\t\tnn.Conv2d(512,1024,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\t\t\tnn.BatchNorm2d(1024),\n\t\t\t\t),\n\t\t\t\tLambda(lambda x: x), # Identity,\n\t\t\t),\n\t\t\tLambdaReduce(lambda x,y: x+y), # CAddTable,\n\t\t\tnn.ReLU(),\n\t\t),\n\t\tnn.Sequential( # Sequential,\n\t\t\tLambdaMap(lambda x: x, # ConcatTable,\n\t\t\t\tnn.Sequential( # Sequential,\n\t\t\t\t\tnn.Sequential( # Sequential,\n\t\t\t\t\t\tnn.Conv2d(1024,512,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\t\t\t\tnn.BatchNorm2d(512),\n\t\t\t\t\t\tnn.ReLU(),\n\t\t\t\t\t\tnn.Conv2d(512,512,(3, 3),(1, 1),(1, 1),1,32,bias=False),\n\t\t\t\t\t\tnn.BatchNorm2d(512),\n\t\t\t\t\t\tnn.ReLU(),\n\t\t\t\t\t),\n\t\t\t\t\tnn.Conv2d(512,1024,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\t\t\tnn.BatchNorm2d(1024),\n\t\t\t\t),\n\t\t\t\tLambda(lambda x: x), # Identity,\n\t\t\t),\n\t\t\tLambdaReduce(lambda x,y: x+y), # CAddTable,\n\t\t\tnn.ReLU(),\n\t\t),\n\t\tnn.Sequential( # Sequential,\n\t\t\tLambdaMap(lambda x: x, # ConcatTable,\n\t\t\t\tnn.Sequential( # Sequential,\n\t\t\t\t\tnn.Sequential( # Sequential,\n\t\t\t\t\t\tnn.Conv2d(1024,512,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\t\t\t\tnn.BatchNorm2d(512),\n\t\t\t\t\t\tnn.ReLU(),\n\t\t\t\t\t\tnn.Conv2d(512,512,(3, 3),(1, 1),(1, 1),1,32,bias=False),\n\t\t\t\t\t\tnn.BatchNorm2d(512),\n\t\t\t\t\t\tnn.ReLU(),\n\t\t\t\t\t),\n\t\t\t\t\tnn.Conv2d(512,1024,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\t\t\tnn.BatchNorm2d(1024),\n\t\t\t\t),\n\t\t\t\tLambda(lambda x: x), # Identity,\n\t\t\t),\n\t\t\tLambdaReduce(lambda x,y: x+y), # CAddTable,\n\t\t\tnn.ReLU(),\n\t\t),\n\t\tnn.Sequential( # Sequential,\n\t\t\tLambdaMap(lambda x: x, # ConcatTable,\n\t\t\t\tnn.Sequential( # Sequential,\n\t\t\t\t\tnn.Sequential( # Sequential,\n\t\t\t\t\t\tnn.Conv2d(1024,512,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\t\t\t\tnn.BatchNorm2d(512),\n\t\t\t\t\t\tnn.ReLU(),\n\t\t\t\t\t\tnn.Conv2d(512,512,(3, 3),(1, 1),(1, 1),1,32,bias=False),\n\t\t\t\t\t\tnn.BatchNorm2d(512),\n\t\t\t\t\t\tnn.ReLU(),\n\t\t\t\t\t),\n\t\t\t\t\tnn.Conv2d(512,1024,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\t\t\tnn.BatchNorm2d(1024),\n\t\t\t\t),\n\t\t\t\tLambda(lambda x: x), # Identity,\n\t\t\t),\n\t\t\tLambdaReduce(lambda x,y: x+y), # CAddTable,\n\t\t\tnn.ReLU(),\n\t\t),\n\t\tnn.Sequential( # Sequential,\n\t\t\tLambdaMap(lambda x: x, # ConcatTable,\n\t\t\t\tnn.Sequential( # Sequential,\n\t\t\t\t\tnn.Sequential( # Sequential,\n\t\t\t\t\t\tnn.Conv2d(1024,512,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\t\t\t\tnn.BatchNorm2d(512),\n\t\t\t\t\t\tnn.ReLU(),\n\t\t\t\t\t\tnn.Conv2d(512,512,(3, 3),(1, 1),(1, 1),1,32,bias=False),\n\t\t\t\t\t\tnn.BatchNorm2d(512),\n\t\t\t\t\t\tnn.ReLU(),\n\t\t\t\t\t),\n\t\t\t\t\tnn.Conv2d(512,1024,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\t\t\tnn.BatchNorm2d(1024),\n\t\t\t\t),\n\t\t\t\tLambda(lambda x: x), # Identity,\n\t\t\t),\n\t\t\tLambdaReduce(lambda x,y: x+y), # CAddTable,\n\t\t\tnn.ReLU(),\n\t\t),\n\t),\n\tnn.Sequential( # Sequential,\n\t\tnn.Sequential( # Sequential,\n\t\t\tLambdaMap(lambda x: x, # ConcatTable,\n\t\t\t\tnn.Sequential( # Sequential,\n\t\t\t\t\tnn.Sequential( # Sequential,\n\t\t\t\t\t\tnn.Conv2d(1024,1024,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\t\t\t\tnn.BatchNorm2d(1024),\n\t\t\t\t\t\tnn.ReLU(),\n\t\t\t\t\t\tnn.Conv2d(1024,1024,(3, 3),(2, 2),(1, 1),1,32,bias=False),\n\t\t\t\t\t\tnn.BatchNorm2d(1024),\n\t\t\t\t\t\tnn.ReLU(),\n\t\t\t\t\t),\n\t\t\t\t\tnn.Conv2d(1024,2048,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\t\t\tnn.BatchNorm2d(2048),\n\t\t\t\t),\n\t\t\t\tnn.Sequential( # Sequential,\n\t\t\t\t\tnn.Conv2d(1024,2048,(1, 1),(2, 2),(0, 0),1,1,bias=False),\n\t\t\t\t\tnn.BatchNorm2d(2048),\n\t\t\t\t),\n\t\t\t),\n\t\t\tLambdaReduce(lambda x,y: x+y), # CAddTable,\n\t\t\tnn.ReLU(),\n\t\t),\n\t\tnn.Sequential( # Sequential,\n\t\t\tLambdaMap(lambda x: x, # ConcatTable,\n\t\t\t\tnn.Sequential( # Sequential,\n\t\t\t\t\tnn.Sequential( # Sequential,\n\t\t\t\t\t\tnn.Conv2d(2048,1024,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\t\t\t\tnn.BatchNorm2d(1024),\n\t\t\t\t\t\tnn.ReLU(),\n\t\t\t\t\t\tnn.Conv2d(1024,1024,(3, 3),(1, 1),(1, 1),1,32,bias=False),\n\t\t\t\t\t\tnn.BatchNorm2d(1024),\n\t\t\t\t\t\tnn.ReLU(),\n\t\t\t\t\t),\n\t\t\t\t\tnn.Conv2d(1024,2048,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\t\t\tnn.BatchNorm2d(2048),\n\t\t\t\t),\n\t\t\t\tLambda(lambda x: x), # Identity,\n\t\t\t),\n\t\t\tLambdaReduce(lambda x,y: x+y), # CAddTable,\n\t\t\tnn.ReLU(),\n\t\t),\n\t\tnn.Sequential( # Sequential,\n\t\t\tLambdaMap(lambda x: x, # ConcatTable,\n\t\t\t\tnn.Sequential( # Sequential,\n\t\t\t\t\tnn.Sequential( # Sequential,\n\t\t\t\t\t\tnn.Conv2d(2048,1024,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\t\t\t\tnn.BatchNorm2d(1024),\n\t\t\t\t\t\tnn.ReLU(),\n\t\t\t\t\t\tnn.Conv2d(1024,1024,(3, 3),(1, 1),(1, 1),1,32,bias=False),\n\t\t\t\t\t\tnn.BatchNorm2d(1024),\n\t\t\t\t\t\tnn.ReLU(),\n\t\t\t\t\t),\n\t\t\t\t\tnn.Conv2d(1024,2048,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\t\t\tnn.BatchNorm2d(2048),\n\t\t\t\t),\n\t\t\t\tLambda(lambda x: x), # Identity,\n\t\t\t),\n\t\t\tLambdaReduce(lambda x,y: x+y), # CAddTable,\n\t\t\tnn.ReLU(),\n\t\t),\n\t),\n\tnn.AvgPool2d((7, 7),(1, 1)),\n\tLambda(lambda x: x.view(x.size(0),-1)), # View,\n\tnn.Sequential(Lambda(lambda x: x.view(1,-1) if 1==len(x.size()) else x ),nn.Linear(2048,1000)), # Linear,\n)'"
ImageNet-P/resnext_101_64x4d.py,2,"b'\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\nfrom functools import reduce\n\nclass LambdaBase(nn.Sequential):\n    def __init__(self, fn, *args):\n        super(LambdaBase, self).__init__(*args)\n        self.lambda_func = fn\n\n    def forward_prepare(self, input):\n        output = []\n        for module in self._modules.values():\n            output.append(module(input))\n        return output if output else input\n\nclass Lambda(LambdaBase):\n    def forward(self, input):\n        return self.lambda_func(self.forward_prepare(input))\n\nclass LambdaMap(LambdaBase):\n    def forward(self, input):\n        return list(map(self.lambda_func,self.forward_prepare(input)))\n\nclass LambdaReduce(LambdaBase):\n    def forward(self, input):\n        return reduce(self.lambda_func,self.forward_prepare(input))\n\n\nresnext_101_64x4d = nn.Sequential( # Sequential,\n\tnn.Conv2d(3,64,(7, 7),(2, 2),(3, 3),1,1,bias=False),\n\tnn.BatchNorm2d(64),\n\tnn.ReLU(),\n\tnn.MaxPool2d((3, 3),(2, 2),(1, 1)),\n\tnn.Sequential( # Sequential,\n\t\tnn.Sequential( # Sequential,\n\t\t\tLambdaMap(lambda x: x, # ConcatTable,\n\t\t\t\tnn.Sequential( # Sequential,\n\t\t\t\t\tnn.Sequential( # Sequential,\n\t\t\t\t\t\tnn.Conv2d(64,256,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\t\t\t\tnn.BatchNorm2d(256),\n\t\t\t\t\t\tnn.ReLU(),\n\t\t\t\t\t\tnn.Conv2d(256,256,(3, 3),(1, 1),(1, 1),1,64,bias=False),\n\t\t\t\t\t\tnn.BatchNorm2d(256),\n\t\t\t\t\t\tnn.ReLU(),\n\t\t\t\t\t),\n\t\t\t\t\tnn.Conv2d(256,256,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\t\t\tnn.BatchNorm2d(256),\n\t\t\t\t),\n\t\t\t\tnn.Sequential( # Sequential,\n\t\t\t\t\tnn.Conv2d(64,256,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\t\t\tnn.BatchNorm2d(256),\n\t\t\t\t),\n\t\t\t),\n\t\t\tLambdaReduce(lambda x,y: x+y), # CAddTable,\n\t\t\tnn.ReLU(),\n\t\t),\n\t\tnn.Sequential( # Sequential,\n\t\t\tLambdaMap(lambda x: x, # ConcatTable,\n\t\t\t\tnn.Sequential( # Sequential,\n\t\t\t\t\tnn.Sequential( # Sequential,\n\t\t\t\t\t\tnn.Conv2d(256,256,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\t\t\t\tnn.BatchNorm2d(256),\n\t\t\t\t\t\tnn.ReLU(),\n\t\t\t\t\t\tnn.Conv2d(256,256,(3, 3),(1, 1),(1, 1),1,64,bias=False),\n\t\t\t\t\t\tnn.BatchNorm2d(256),\n\t\t\t\t\t\tnn.ReLU(),\n\t\t\t\t\t),\n\t\t\t\t\tnn.Conv2d(256,256,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\t\t\tnn.BatchNorm2d(256),\n\t\t\t\t),\n\t\t\t\tLambda(lambda x: x), # Identity,\n\t\t\t),\n\t\t\tLambdaReduce(lambda x,y: x+y), # CAddTable,\n\t\t\tnn.ReLU(),\n\t\t),\n\t\tnn.Sequential( # Sequential,\n\t\t\tLambdaMap(lambda x: x, # ConcatTable,\n\t\t\t\tnn.Sequential( # Sequential,\n\t\t\t\t\tnn.Sequential( # Sequential,\n\t\t\t\t\t\tnn.Conv2d(256,256,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\t\t\t\tnn.BatchNorm2d(256),\n\t\t\t\t\t\tnn.ReLU(),\n\t\t\t\t\t\tnn.Conv2d(256,256,(3, 3),(1, 1),(1, 1),1,64,bias=False),\n\t\t\t\t\t\tnn.BatchNorm2d(256),\n\t\t\t\t\t\tnn.ReLU(),\n\t\t\t\t\t),\n\t\t\t\t\tnn.Conv2d(256,256,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\t\t\tnn.BatchNorm2d(256),\n\t\t\t\t),\n\t\t\t\tLambda(lambda x: x), # Identity,\n\t\t\t),\n\t\t\tLambdaReduce(lambda x,y: x+y), # CAddTable,\n\t\t\tnn.ReLU(),\n\t\t),\n\t),\n\tnn.Sequential( # Sequential,\n\t\tnn.Sequential( # Sequential,\n\t\t\tLambdaMap(lambda x: x, # ConcatTable,\n\t\t\t\tnn.Sequential( # Sequential,\n\t\t\t\t\tnn.Sequential( # Sequential,\n\t\t\t\t\t\tnn.Conv2d(256,512,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\t\t\t\tnn.BatchNorm2d(512),\n\t\t\t\t\t\tnn.ReLU(),\n\t\t\t\t\t\tnn.Conv2d(512,512,(3, 3),(2, 2),(1, 1),1,64,bias=False),\n\t\t\t\t\t\tnn.BatchNorm2d(512),\n\t\t\t\t\t\tnn.ReLU(),\n\t\t\t\t\t),\n\t\t\t\t\tnn.Conv2d(512,512,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\t\t\tnn.BatchNorm2d(512),\n\t\t\t\t),\n\t\t\t\tnn.Sequential( # Sequential,\n\t\t\t\t\tnn.Conv2d(256,512,(1, 1),(2, 2),(0, 0),1,1,bias=False),\n\t\t\t\t\tnn.BatchNorm2d(512),\n\t\t\t\t),\n\t\t\t),\n\t\t\tLambdaReduce(lambda x,y: x+y), # CAddTable,\n\t\t\tnn.ReLU(),\n\t\t),\n\t\tnn.Sequential( # Sequential,\n\t\t\tLambdaMap(lambda x: x, # ConcatTable,\n\t\t\t\tnn.Sequential( # Sequential,\n\t\t\t\t\tnn.Sequential( # Sequential,\n\t\t\t\t\t\tnn.Conv2d(512,512,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\t\t\t\tnn.BatchNorm2d(512),\n\t\t\t\t\t\tnn.ReLU(),\n\t\t\t\t\t\tnn.Conv2d(512,512,(3, 3),(1, 1),(1, 1),1,64,bias=False),\n\t\t\t\t\t\tnn.BatchNorm2d(512),\n\t\t\t\t\t\tnn.ReLU(),\n\t\t\t\t\t),\n\t\t\t\t\tnn.Conv2d(512,512,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\t\t\tnn.BatchNorm2d(512),\n\t\t\t\t),\n\t\t\t\tLambda(lambda x: x), # Identity,\n\t\t\t),\n\t\t\tLambdaReduce(lambda x,y: x+y), # CAddTable,\n\t\t\tnn.ReLU(),\n\t\t),\n\t\tnn.Sequential( # Sequential,\n\t\t\tLambdaMap(lambda x: x, # ConcatTable,\n\t\t\t\tnn.Sequential( # Sequential,\n\t\t\t\t\tnn.Sequential( # Sequential,\n\t\t\t\t\t\tnn.Conv2d(512,512,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\t\t\t\tnn.BatchNorm2d(512),\n\t\t\t\t\t\tnn.ReLU(),\n\t\t\t\t\t\tnn.Conv2d(512,512,(3, 3),(1, 1),(1, 1),1,64,bias=False),\n\t\t\t\t\t\tnn.BatchNorm2d(512),\n\t\t\t\t\t\tnn.ReLU(),\n\t\t\t\t\t),\n\t\t\t\t\tnn.Conv2d(512,512,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\t\t\tnn.BatchNorm2d(512),\n\t\t\t\t),\n\t\t\t\tLambda(lambda x: x), # Identity,\n\t\t\t),\n\t\t\tLambdaReduce(lambda x,y: x+y), # CAddTable,\n\t\t\tnn.ReLU(),\n\t\t),\n\t\tnn.Sequential( # Sequential,\n\t\t\tLambdaMap(lambda x: x, # ConcatTable,\n\t\t\t\tnn.Sequential( # Sequential,\n\t\t\t\t\tnn.Sequential( # Sequential,\n\t\t\t\t\t\tnn.Conv2d(512,512,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\t\t\t\tnn.BatchNorm2d(512),\n\t\t\t\t\t\tnn.ReLU(),\n\t\t\t\t\t\tnn.Conv2d(512,512,(3, 3),(1, 1),(1, 1),1,64,bias=False),\n\t\t\t\t\t\tnn.BatchNorm2d(512),\n\t\t\t\t\t\tnn.ReLU(),\n\t\t\t\t\t),\n\t\t\t\t\tnn.Conv2d(512,512,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\t\t\tnn.BatchNorm2d(512),\n\t\t\t\t),\n\t\t\t\tLambda(lambda x: x), # Identity,\n\t\t\t),\n\t\t\tLambdaReduce(lambda x,y: x+y), # CAddTable,\n\t\t\tnn.ReLU(),\n\t\t),\n\t),\n\tnn.Sequential( # Sequential,\n\t\tnn.Sequential( # Sequential,\n\t\t\tLambdaMap(lambda x: x, # ConcatTable,\n\t\t\t\tnn.Sequential( # Sequential,\n\t\t\t\t\tnn.Sequential( # Sequential,\n\t\t\t\t\t\tnn.Conv2d(512,1024,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\t\t\t\tnn.BatchNorm2d(1024),\n\t\t\t\t\t\tnn.ReLU(),\n\t\t\t\t\t\tnn.Conv2d(1024,1024,(3, 3),(2, 2),(1, 1),1,64,bias=False),\n\t\t\t\t\t\tnn.BatchNorm2d(1024),\n\t\t\t\t\t\tnn.ReLU(),\n\t\t\t\t\t),\n\t\t\t\t\tnn.Conv2d(1024,1024,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\t\t\tnn.BatchNorm2d(1024),\n\t\t\t\t),\n\t\t\t\tnn.Sequential( # Sequential,\n\t\t\t\t\tnn.Conv2d(512,1024,(1, 1),(2, 2),(0, 0),1,1,bias=False),\n\t\t\t\t\tnn.BatchNorm2d(1024),\n\t\t\t\t),\n\t\t\t),\n\t\t\tLambdaReduce(lambda x,y: x+y), # CAddTable,\n\t\t\tnn.ReLU(),\n\t\t),\n\t\tnn.Sequential( # Sequential,\n\t\t\tLambdaMap(lambda x: x, # ConcatTable,\n\t\t\t\tnn.Sequential( # Sequential,\n\t\t\t\t\tnn.Sequential( # Sequential,\n\t\t\t\t\t\tnn.Conv2d(1024,1024,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\t\t\t\tnn.BatchNorm2d(1024),\n\t\t\t\t\t\tnn.ReLU(),\n\t\t\t\t\t\tnn.Conv2d(1024,1024,(3, 3),(1, 1),(1, 1),1,64,bias=False),\n\t\t\t\t\t\tnn.BatchNorm2d(1024),\n\t\t\t\t\t\tnn.ReLU(),\n\t\t\t\t\t),\n\t\t\t\t\tnn.Conv2d(1024,1024,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\t\t\tnn.BatchNorm2d(1024),\n\t\t\t\t),\n\t\t\t\tLambda(lambda x: x), # Identity,\n\t\t\t),\n\t\t\tLambdaReduce(lambda x,y: x+y), # CAddTable,\n\t\t\tnn.ReLU(),\n\t\t),\n\t\tnn.Sequential( # Sequential,\n\t\t\tLambdaMap(lambda x: x, # ConcatTable,\n\t\t\t\tnn.Sequential( # Sequential,\n\t\t\t\t\tnn.Sequential( # Sequential,\n\t\t\t\t\t\tnn.Conv2d(1024,1024,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\t\t\t\tnn.BatchNorm2d(1024),\n\t\t\t\t\t\tnn.ReLU(),\n\t\t\t\t\t\tnn.Conv2d(1024,1024,(3, 3),(1, 1),(1, 1),1,64,bias=False),\n\t\t\t\t\t\tnn.BatchNorm2d(1024),\n\t\t\t\t\t\tnn.ReLU(),\n\t\t\t\t\t),\n\t\t\t\t\tnn.Conv2d(1024,1024,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\t\t\tnn.BatchNorm2d(1024),\n\t\t\t\t),\n\t\t\t\tLambda(lambda x: x), # Identity,\n\t\t\t),\n\t\t\tLambdaReduce(lambda x,y: x+y), # CAddTable,\n\t\t\tnn.ReLU(),\n\t\t),\n\t\tnn.Sequential( # Sequential,\n\t\t\tLambdaMap(lambda x: x, # ConcatTable,\n\t\t\t\tnn.Sequential( # Sequential,\n\t\t\t\t\tnn.Sequential( # Sequential,\n\t\t\t\t\t\tnn.Conv2d(1024,1024,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\t\t\t\tnn.BatchNorm2d(1024),\n\t\t\t\t\t\tnn.ReLU(),\n\t\t\t\t\t\tnn.Conv2d(1024,1024,(3, 3),(1, 1),(1, 1),1,64,bias=False),\n\t\t\t\t\t\tnn.BatchNorm2d(1024),\n\t\t\t\t\t\tnn.ReLU(),\n\t\t\t\t\t),\n\t\t\t\t\tnn.Conv2d(1024,1024,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\t\t\tnn.BatchNorm2d(1024),\n\t\t\t\t),\n\t\t\t\tLambda(lambda x: x), # Identity,\n\t\t\t),\n\t\t\tLambdaReduce(lambda x,y: x+y), # CAddTable,\n\t\t\tnn.ReLU(),\n\t\t),\n\t\tnn.Sequential( # Sequential,\n\t\t\tLambdaMap(lambda x: x, # ConcatTable,\n\t\t\t\tnn.Sequential( # Sequential,\n\t\t\t\t\tnn.Sequential( # Sequential,\n\t\t\t\t\t\tnn.Conv2d(1024,1024,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\t\t\t\tnn.BatchNorm2d(1024),\n\t\t\t\t\t\tnn.ReLU(),\n\t\t\t\t\t\tnn.Conv2d(1024,1024,(3, 3),(1, 1),(1, 1),1,64,bias=False),\n\t\t\t\t\t\tnn.BatchNorm2d(1024),\n\t\t\t\t\t\tnn.ReLU(),\n\t\t\t\t\t),\n\t\t\t\t\tnn.Conv2d(1024,1024,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\t\t\tnn.BatchNorm2d(1024),\n\t\t\t\t),\n\t\t\t\tLambda(lambda x: x), # Identity,\n\t\t\t),\n\t\t\tLambdaReduce(lambda x,y: x+y), # CAddTable,\n\t\t\tnn.ReLU(),\n\t\t),\n\t\tnn.Sequential( # Sequential,\n\t\t\tLambdaMap(lambda x: x, # ConcatTable,\n\t\t\t\tnn.Sequential( # Sequential,\n\t\t\t\t\tnn.Sequential( # Sequential,\n\t\t\t\t\t\tnn.Conv2d(1024,1024,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\t\t\t\tnn.BatchNorm2d(1024),\n\t\t\t\t\t\tnn.ReLU(),\n\t\t\t\t\t\tnn.Conv2d(1024,1024,(3, 3),(1, 1),(1, 1),1,64,bias=False),\n\t\t\t\t\t\tnn.BatchNorm2d(1024),\n\t\t\t\t\t\tnn.ReLU(),\n\t\t\t\t\t),\n\t\t\t\t\tnn.Conv2d(1024,1024,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\t\t\tnn.BatchNorm2d(1024),\n\t\t\t\t),\n\t\t\t\tLambda(lambda x: x), # Identity,\n\t\t\t),\n\t\t\tLambdaReduce(lambda x,y: x+y), # CAddTable,\n\t\t\tnn.ReLU(),\n\t\t),\n\t\tnn.Sequential( # Sequential,\n\t\t\tLambdaMap(lambda x: x, # ConcatTable,\n\t\t\t\tnn.Sequential( # Sequential,\n\t\t\t\t\tnn.Sequential( # Sequential,\n\t\t\t\t\t\tnn.Conv2d(1024,1024,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\t\t\t\tnn.BatchNorm2d(1024),\n\t\t\t\t\t\tnn.ReLU(),\n\t\t\t\t\t\tnn.Conv2d(1024,1024,(3, 3),(1, 1),(1, 1),1,64,bias=False),\n\t\t\t\t\t\tnn.BatchNorm2d(1024),\n\t\t\t\t\t\tnn.ReLU(),\n\t\t\t\t\t),\n\t\t\t\t\tnn.Conv2d(1024,1024,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\t\t\tnn.BatchNorm2d(1024),\n\t\t\t\t),\n\t\t\t\tLambda(lambda x: x), # Identity,\n\t\t\t),\n\t\t\tLambdaReduce(lambda x,y: x+y), # CAddTable,\n\t\t\tnn.ReLU(),\n\t\t),\n\t\tnn.Sequential( # Sequential,\n\t\t\tLambdaMap(lambda x: x, # ConcatTable,\n\t\t\t\tnn.Sequential( # Sequential,\n\t\t\t\t\tnn.Sequential( # Sequential,\n\t\t\t\t\t\tnn.Conv2d(1024,1024,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\t\t\t\tnn.BatchNorm2d(1024),\n\t\t\t\t\t\tnn.ReLU(),\n\t\t\t\t\t\tnn.Conv2d(1024,1024,(3, 3),(1, 1),(1, 1),1,64,bias=False),\n\t\t\t\t\t\tnn.BatchNorm2d(1024),\n\t\t\t\t\t\tnn.ReLU(),\n\t\t\t\t\t),\n\t\t\t\t\tnn.Conv2d(1024,1024,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\t\t\tnn.BatchNorm2d(1024),\n\t\t\t\t),\n\t\t\t\tLambda(lambda x: x), # Identity,\n\t\t\t),\n\t\t\tLambdaReduce(lambda x,y: x+y), # CAddTable,\n\t\t\tnn.ReLU(),\n\t\t),\n\t\tnn.Sequential( # Sequential,\n\t\t\tLambdaMap(lambda x: x, # ConcatTable,\n\t\t\t\tnn.Sequential( # Sequential,\n\t\t\t\t\tnn.Sequential( # Sequential,\n\t\t\t\t\t\tnn.Conv2d(1024,1024,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\t\t\t\tnn.BatchNorm2d(1024),\n\t\t\t\t\t\tnn.ReLU(),\n\t\t\t\t\t\tnn.Conv2d(1024,1024,(3, 3),(1, 1),(1, 1),1,64,bias=False),\n\t\t\t\t\t\tnn.BatchNorm2d(1024),\n\t\t\t\t\t\tnn.ReLU(),\n\t\t\t\t\t),\n\t\t\t\t\tnn.Conv2d(1024,1024,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\t\t\tnn.BatchNorm2d(1024),\n\t\t\t\t),\n\t\t\t\tLambda(lambda x: x), # Identity,\n\t\t\t),\n\t\t\tLambdaReduce(lambda x,y: x+y), # CAddTable,\n\t\t\tnn.ReLU(),\n\t\t),\n\t\tnn.Sequential( # Sequential,\n\t\t\tLambdaMap(lambda x: x, # ConcatTable,\n\t\t\t\tnn.Sequential( # Sequential,\n\t\t\t\t\tnn.Sequential( # Sequential,\n\t\t\t\t\t\tnn.Conv2d(1024,1024,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\t\t\t\tnn.BatchNorm2d(1024),\n\t\t\t\t\t\tnn.ReLU(),\n\t\t\t\t\t\tnn.Conv2d(1024,1024,(3, 3),(1, 1),(1, 1),1,64,bias=False),\n\t\t\t\t\t\tnn.BatchNorm2d(1024),\n\t\t\t\t\t\tnn.ReLU(),\n\t\t\t\t\t),\n\t\t\t\t\tnn.Conv2d(1024,1024,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\t\t\tnn.BatchNorm2d(1024),\n\t\t\t\t),\n\t\t\t\tLambda(lambda x: x), # Identity,\n\t\t\t),\n\t\t\tLambdaReduce(lambda x,y: x+y), # CAddTable,\n\t\t\tnn.ReLU(),\n\t\t),\n\t\tnn.Sequential( # Sequential,\n\t\t\tLambdaMap(lambda x: x, # ConcatTable,\n\t\t\t\tnn.Sequential( # Sequential,\n\t\t\t\t\tnn.Sequential( # Sequential,\n\t\t\t\t\t\tnn.Conv2d(1024,1024,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\t\t\t\tnn.BatchNorm2d(1024),\n\t\t\t\t\t\tnn.ReLU(),\n\t\t\t\t\t\tnn.Conv2d(1024,1024,(3, 3),(1, 1),(1, 1),1,64,bias=False),\n\t\t\t\t\t\tnn.BatchNorm2d(1024),\n\t\t\t\t\t\tnn.ReLU(),\n\t\t\t\t\t),\n\t\t\t\t\tnn.Conv2d(1024,1024,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\t\t\tnn.BatchNorm2d(1024),\n\t\t\t\t),\n\t\t\t\tLambda(lambda x: x), # Identity,\n\t\t\t),\n\t\t\tLambdaReduce(lambda x,y: x+y), # CAddTable,\n\t\t\tnn.ReLU(),\n\t\t),\n\t\tnn.Sequential( # Sequential,\n\t\t\tLambdaMap(lambda x: x, # ConcatTable,\n\t\t\t\tnn.Sequential( # Sequential,\n\t\t\t\t\tnn.Sequential( # Sequential,\n\t\t\t\t\t\tnn.Conv2d(1024,1024,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\t\t\t\tnn.BatchNorm2d(1024),\n\t\t\t\t\t\tnn.ReLU(),\n\t\t\t\t\t\tnn.Conv2d(1024,1024,(3, 3),(1, 1),(1, 1),1,64,bias=False),\n\t\t\t\t\t\tnn.BatchNorm2d(1024),\n\t\t\t\t\t\tnn.ReLU(),\n\t\t\t\t\t),\n\t\t\t\t\tnn.Conv2d(1024,1024,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\t\t\tnn.BatchNorm2d(1024),\n\t\t\t\t),\n\t\t\t\tLambda(lambda x: x), # Identity,\n\t\t\t),\n\t\t\tLambdaReduce(lambda x,y: x+y), # CAddTable,\n\t\t\tnn.ReLU(),\n\t\t),\n\t\tnn.Sequential( # Sequential,\n\t\t\tLambdaMap(lambda x: x, # ConcatTable,\n\t\t\t\tnn.Sequential( # Sequential,\n\t\t\t\t\tnn.Sequential( # Sequential,\n\t\t\t\t\t\tnn.Conv2d(1024,1024,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\t\t\t\tnn.BatchNorm2d(1024),\n\t\t\t\t\t\tnn.ReLU(),\n\t\t\t\t\t\tnn.Conv2d(1024,1024,(3, 3),(1, 1),(1, 1),1,64,bias=False),\n\t\t\t\t\t\tnn.BatchNorm2d(1024),\n\t\t\t\t\t\tnn.ReLU(),\n\t\t\t\t\t),\n\t\t\t\t\tnn.Conv2d(1024,1024,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\t\t\tnn.BatchNorm2d(1024),\n\t\t\t\t),\n\t\t\t\tLambda(lambda x: x), # Identity,\n\t\t\t),\n\t\t\tLambdaReduce(lambda x,y: x+y), # CAddTable,\n\t\t\tnn.ReLU(),\n\t\t),\n\t\tnn.Sequential( # Sequential,\n\t\t\tLambdaMap(lambda x: x, # ConcatTable,\n\t\t\t\tnn.Sequential( # Sequential,\n\t\t\t\t\tnn.Sequential( # Sequential,\n\t\t\t\t\t\tnn.Conv2d(1024,1024,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\t\t\t\tnn.BatchNorm2d(1024),\n\t\t\t\t\t\tnn.ReLU(),\n\t\t\t\t\t\tnn.Conv2d(1024,1024,(3, 3),(1, 1),(1, 1),1,64,bias=False),\n\t\t\t\t\t\tnn.BatchNorm2d(1024),\n\t\t\t\t\t\tnn.ReLU(),\n\t\t\t\t\t),\n\t\t\t\t\tnn.Conv2d(1024,1024,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\t\t\tnn.BatchNorm2d(1024),\n\t\t\t\t),\n\t\t\t\tLambda(lambda x: x), # Identity,\n\t\t\t),\n\t\t\tLambdaReduce(lambda x,y: x+y), # CAddTable,\n\t\t\tnn.ReLU(),\n\t\t),\n\t\tnn.Sequential( # Sequential,\n\t\t\tLambdaMap(lambda x: x, # ConcatTable,\n\t\t\t\tnn.Sequential( # Sequential,\n\t\t\t\t\tnn.Sequential( # Sequential,\n\t\t\t\t\t\tnn.Conv2d(1024,1024,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\t\t\t\tnn.BatchNorm2d(1024),\n\t\t\t\t\t\tnn.ReLU(),\n\t\t\t\t\t\tnn.Conv2d(1024,1024,(3, 3),(1, 1),(1, 1),1,64,bias=False),\n\t\t\t\t\t\tnn.BatchNorm2d(1024),\n\t\t\t\t\t\tnn.ReLU(),\n\t\t\t\t\t),\n\t\t\t\t\tnn.Conv2d(1024,1024,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\t\t\tnn.BatchNorm2d(1024),\n\t\t\t\t),\n\t\t\t\tLambda(lambda x: x), # Identity,\n\t\t\t),\n\t\t\tLambdaReduce(lambda x,y: x+y), # CAddTable,\n\t\t\tnn.ReLU(),\n\t\t),\n\t\tnn.Sequential( # Sequential,\n\t\t\tLambdaMap(lambda x: x, # ConcatTable,\n\t\t\t\tnn.Sequential( # Sequential,\n\t\t\t\t\tnn.Sequential( # Sequential,\n\t\t\t\t\t\tnn.Conv2d(1024,1024,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\t\t\t\tnn.BatchNorm2d(1024),\n\t\t\t\t\t\tnn.ReLU(),\n\t\t\t\t\t\tnn.Conv2d(1024,1024,(3, 3),(1, 1),(1, 1),1,64,bias=False),\n\t\t\t\t\t\tnn.BatchNorm2d(1024),\n\t\t\t\t\t\tnn.ReLU(),\n\t\t\t\t\t),\n\t\t\t\t\tnn.Conv2d(1024,1024,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\t\t\tnn.BatchNorm2d(1024),\n\t\t\t\t),\n\t\t\t\tLambda(lambda x: x), # Identity,\n\t\t\t),\n\t\t\tLambdaReduce(lambda x,y: x+y), # CAddTable,\n\t\t\tnn.ReLU(),\n\t\t),\n\t\tnn.Sequential( # Sequential,\n\t\t\tLambdaMap(lambda x: x, # ConcatTable,\n\t\t\t\tnn.Sequential( # Sequential,\n\t\t\t\t\tnn.Sequential( # Sequential,\n\t\t\t\t\t\tnn.Conv2d(1024,1024,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\t\t\t\tnn.BatchNorm2d(1024),\n\t\t\t\t\t\tnn.ReLU(),\n\t\t\t\t\t\tnn.Conv2d(1024,1024,(3, 3),(1, 1),(1, 1),1,64,bias=False),\n\t\t\t\t\t\tnn.BatchNorm2d(1024),\n\t\t\t\t\t\tnn.ReLU(),\n\t\t\t\t\t),\n\t\t\t\t\tnn.Conv2d(1024,1024,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\t\t\tnn.BatchNorm2d(1024),\n\t\t\t\t),\n\t\t\t\tLambda(lambda x: x), # Identity,\n\t\t\t),\n\t\t\tLambdaReduce(lambda x,y: x+y), # CAddTable,\n\t\t\tnn.ReLU(),\n\t\t),\n\t\tnn.Sequential( # Sequential,\n\t\t\tLambdaMap(lambda x: x, # ConcatTable,\n\t\t\t\tnn.Sequential( # Sequential,\n\t\t\t\t\tnn.Sequential( # Sequential,\n\t\t\t\t\t\tnn.Conv2d(1024,1024,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\t\t\t\tnn.BatchNorm2d(1024),\n\t\t\t\t\t\tnn.ReLU(),\n\t\t\t\t\t\tnn.Conv2d(1024,1024,(3, 3),(1, 1),(1, 1),1,64,bias=False),\n\t\t\t\t\t\tnn.BatchNorm2d(1024),\n\t\t\t\t\t\tnn.ReLU(),\n\t\t\t\t\t),\n\t\t\t\t\tnn.Conv2d(1024,1024,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\t\t\tnn.BatchNorm2d(1024),\n\t\t\t\t),\n\t\t\t\tLambda(lambda x: x), # Identity,\n\t\t\t),\n\t\t\tLambdaReduce(lambda x,y: x+y), # CAddTable,\n\t\t\tnn.ReLU(),\n\t\t),\n\t\tnn.Sequential( # Sequential,\n\t\t\tLambdaMap(lambda x: x, # ConcatTable,\n\t\t\t\tnn.Sequential( # Sequential,\n\t\t\t\t\tnn.Sequential( # Sequential,\n\t\t\t\t\t\tnn.Conv2d(1024,1024,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\t\t\t\tnn.BatchNorm2d(1024),\n\t\t\t\t\t\tnn.ReLU(),\n\t\t\t\t\t\tnn.Conv2d(1024,1024,(3, 3),(1, 1),(1, 1),1,64,bias=False),\n\t\t\t\t\t\tnn.BatchNorm2d(1024),\n\t\t\t\t\t\tnn.ReLU(),\n\t\t\t\t\t),\n\t\t\t\t\tnn.Conv2d(1024,1024,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\t\t\tnn.BatchNorm2d(1024),\n\t\t\t\t),\n\t\t\t\tLambda(lambda x: x), # Identity,\n\t\t\t),\n\t\t\tLambdaReduce(lambda x,y: x+y), # CAddTable,\n\t\t\tnn.ReLU(),\n\t\t),\n\t\tnn.Sequential( # Sequential,\n\t\t\tLambdaMap(lambda x: x, # ConcatTable,\n\t\t\t\tnn.Sequential( # Sequential,\n\t\t\t\t\tnn.Sequential( # Sequential,\n\t\t\t\t\t\tnn.Conv2d(1024,1024,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\t\t\t\tnn.BatchNorm2d(1024),\n\t\t\t\t\t\tnn.ReLU(),\n\t\t\t\t\t\tnn.Conv2d(1024,1024,(3, 3),(1, 1),(1, 1),1,64,bias=False),\n\t\t\t\t\t\tnn.BatchNorm2d(1024),\n\t\t\t\t\t\tnn.ReLU(),\n\t\t\t\t\t),\n\t\t\t\t\tnn.Conv2d(1024,1024,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\t\t\tnn.BatchNorm2d(1024),\n\t\t\t\t),\n\t\t\t\tLambda(lambda x: x), # Identity,\n\t\t\t),\n\t\t\tLambdaReduce(lambda x,y: x+y), # CAddTable,\n\t\t\tnn.ReLU(),\n\t\t),\n\t\tnn.Sequential( # Sequential,\n\t\t\tLambdaMap(lambda x: x, # ConcatTable,\n\t\t\t\tnn.Sequential( # Sequential,\n\t\t\t\t\tnn.Sequential( # Sequential,\n\t\t\t\t\t\tnn.Conv2d(1024,1024,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\t\t\t\tnn.BatchNorm2d(1024),\n\t\t\t\t\t\tnn.ReLU(),\n\t\t\t\t\t\tnn.Conv2d(1024,1024,(3, 3),(1, 1),(1, 1),1,64,bias=False),\n\t\t\t\t\t\tnn.BatchNorm2d(1024),\n\t\t\t\t\t\tnn.ReLU(),\n\t\t\t\t\t),\n\t\t\t\t\tnn.Conv2d(1024,1024,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\t\t\tnn.BatchNorm2d(1024),\n\t\t\t\t),\n\t\t\t\tLambda(lambda x: x), # Identity,\n\t\t\t),\n\t\t\tLambdaReduce(lambda x,y: x+y), # CAddTable,\n\t\t\tnn.ReLU(),\n\t\t),\n\t\tnn.Sequential( # Sequential,\n\t\t\tLambdaMap(lambda x: x, # ConcatTable,\n\t\t\t\tnn.Sequential( # Sequential,\n\t\t\t\t\tnn.Sequential( # Sequential,\n\t\t\t\t\t\tnn.Conv2d(1024,1024,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\t\t\t\tnn.BatchNorm2d(1024),\n\t\t\t\t\t\tnn.ReLU(),\n\t\t\t\t\t\tnn.Conv2d(1024,1024,(3, 3),(1, 1),(1, 1),1,64,bias=False),\n\t\t\t\t\t\tnn.BatchNorm2d(1024),\n\t\t\t\t\t\tnn.ReLU(),\n\t\t\t\t\t),\n\t\t\t\t\tnn.Conv2d(1024,1024,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\t\t\tnn.BatchNorm2d(1024),\n\t\t\t\t),\n\t\t\t\tLambda(lambda x: x), # Identity,\n\t\t\t),\n\t\t\tLambdaReduce(lambda x,y: x+y), # CAddTable,\n\t\t\tnn.ReLU(),\n\t\t),\n\t\tnn.Sequential( # Sequential,\n\t\t\tLambdaMap(lambda x: x, # ConcatTable,\n\t\t\t\tnn.Sequential( # Sequential,\n\t\t\t\t\tnn.Sequential( # Sequential,\n\t\t\t\t\t\tnn.Conv2d(1024,1024,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\t\t\t\tnn.BatchNorm2d(1024),\n\t\t\t\t\t\tnn.ReLU(),\n\t\t\t\t\t\tnn.Conv2d(1024,1024,(3, 3),(1, 1),(1, 1),1,64,bias=False),\n\t\t\t\t\t\tnn.BatchNorm2d(1024),\n\t\t\t\t\t\tnn.ReLU(),\n\t\t\t\t\t),\n\t\t\t\t\tnn.Conv2d(1024,1024,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\t\t\tnn.BatchNorm2d(1024),\n\t\t\t\t),\n\t\t\t\tLambda(lambda x: x), # Identity,\n\t\t\t),\n\t\t\tLambdaReduce(lambda x,y: x+y), # CAddTable,\n\t\t\tnn.ReLU(),\n\t\t),\n\t),\n\tnn.Sequential( # Sequential,\n\t\tnn.Sequential( # Sequential,\n\t\t\tLambdaMap(lambda x: x, # ConcatTable,\n\t\t\t\tnn.Sequential( # Sequential,\n\t\t\t\t\tnn.Sequential( # Sequential,\n\t\t\t\t\t\tnn.Conv2d(1024,2048,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\t\t\t\tnn.BatchNorm2d(2048),\n\t\t\t\t\t\tnn.ReLU(),\n\t\t\t\t\t\tnn.Conv2d(2048,2048,(3, 3),(2, 2),(1, 1),1,64,bias=False),\n\t\t\t\t\t\tnn.BatchNorm2d(2048),\n\t\t\t\t\t\tnn.ReLU(),\n\t\t\t\t\t),\n\t\t\t\t\tnn.Conv2d(2048,2048,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\t\t\tnn.BatchNorm2d(2048),\n\t\t\t\t),\n\t\t\t\tnn.Sequential( # Sequential,\n\t\t\t\t\tnn.Conv2d(1024,2048,(1, 1),(2, 2),(0, 0),1,1,bias=False),\n\t\t\t\t\tnn.BatchNorm2d(2048),\n\t\t\t\t),\n\t\t\t),\n\t\t\tLambdaReduce(lambda x,y: x+y), # CAddTable,\n\t\t\tnn.ReLU(),\n\t\t),\n\t\tnn.Sequential( # Sequential,\n\t\t\tLambdaMap(lambda x: x, # ConcatTable,\n\t\t\t\tnn.Sequential( # Sequential,\n\t\t\t\t\tnn.Sequential( # Sequential,\n\t\t\t\t\t\tnn.Conv2d(2048,2048,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\t\t\t\tnn.BatchNorm2d(2048),\n\t\t\t\t\t\tnn.ReLU(),\n\t\t\t\t\t\tnn.Conv2d(2048,2048,(3, 3),(1, 1),(1, 1),1,64,bias=False),\n\t\t\t\t\t\tnn.BatchNorm2d(2048),\n\t\t\t\t\t\tnn.ReLU(),\n\t\t\t\t\t),\n\t\t\t\t\tnn.Conv2d(2048,2048,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\t\t\tnn.BatchNorm2d(2048),\n\t\t\t\t),\n\t\t\t\tLambda(lambda x: x), # Identity,\n\t\t\t),\n\t\t\tLambdaReduce(lambda x,y: x+y), # CAddTable,\n\t\t\tnn.ReLU(),\n\t\t),\n\t\tnn.Sequential( # Sequential,\n\t\t\tLambdaMap(lambda x: x, # ConcatTable,\n\t\t\t\tnn.Sequential( # Sequential,\n\t\t\t\t\tnn.Sequential( # Sequential,\n\t\t\t\t\t\tnn.Conv2d(2048,2048,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\t\t\t\tnn.BatchNorm2d(2048),\n\t\t\t\t\t\tnn.ReLU(),\n\t\t\t\t\t\tnn.Conv2d(2048,2048,(3, 3),(1, 1),(1, 1),1,64,bias=False),\n\t\t\t\t\t\tnn.BatchNorm2d(2048),\n\t\t\t\t\t\tnn.ReLU(),\n\t\t\t\t\t),\n\t\t\t\t\tnn.Conv2d(2048,2048,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\t\t\tnn.BatchNorm2d(2048),\n\t\t\t\t),\n\t\t\t\tLambda(lambda x: x), # Identity,\n\t\t\t),\n\t\t\tLambdaReduce(lambda x,y: x+y), # CAddTable,\n\t\t\tnn.ReLU(),\n\t\t),\n\t),\n\tnn.AvgPool2d((7, 7),(1, 1)),\n\tLambda(lambda x: x.view(x.size(0),-1)), # View,\n\tnn.Sequential(Lambda(lambda x: x.view(1,-1) if 1==len(x.size()) else x ),nn.Linear(2048,1000)), # Linear,\n)'"
ImageNet-P/resnext_50_32x4d.py,2,"b'\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\nfrom functools import reduce\n\nclass LambdaBase(nn.Sequential):\n    def __init__(self, fn, *args):\n        super(LambdaBase, self).__init__(*args)\n        self.lambda_func = fn\n\n    def forward_prepare(self, input):\n        output = []\n        for module in self._modules.values():\n            output.append(module(input))\n        return output if output else input\n\nclass Lambda(LambdaBase):\n    def forward(self, input):\n        return self.lambda_func(self.forward_prepare(input))\n\nclass LambdaMap(LambdaBase):\n    def forward(self, input):\n        return list(map(self.lambda_func,self.forward_prepare(input)))\n\nclass LambdaReduce(LambdaBase):\n    def forward(self, input):\n        return reduce(self.lambda_func,self.forward_prepare(input))\n\n\nresnext_50_32x4d = nn.Sequential( # Sequential,\n\tnn.Conv2d(3,64,(7, 7),(2, 2),(3, 3),1,1,bias=False),\n\tnn.BatchNorm2d(64),\n\tnn.ReLU(),\n\tnn.MaxPool2d((3, 3),(2, 2),(1, 1)),\n\tnn.Sequential( # Sequential,\n\t\tnn.Sequential( # Sequential,\n\t\t\tLambdaMap(lambda x: x, # ConcatTable,\n\t\t\t\tnn.Sequential( # Sequential,\n\t\t\t\t\tnn.Sequential( # Sequential,\n\t\t\t\t\t\tnn.Conv2d(64,128,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\t\t\t\tnn.BatchNorm2d(128),\n\t\t\t\t\t\tnn.ReLU(),\n\t\t\t\t\t\tnn.Conv2d(128,128,(3, 3),(1, 1),(1, 1),1,32,bias=False),\n\t\t\t\t\t\tnn.BatchNorm2d(128),\n\t\t\t\t\t\tnn.ReLU(),\n\t\t\t\t\t),\n\t\t\t\t\tnn.Conv2d(128,256,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\t\t\tnn.BatchNorm2d(256),\n\t\t\t\t),\n\t\t\t\tnn.Sequential( # Sequential,\n\t\t\t\t\tnn.Conv2d(64,256,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\t\t\tnn.BatchNorm2d(256),\n\t\t\t\t),\n\t\t\t),\n\t\t\tLambdaReduce(lambda x,y: x+y), # CAddTable,\n\t\t\tnn.ReLU(),\n\t\t),\n\t\tnn.Sequential( # Sequential,\n\t\t\tLambdaMap(lambda x: x, # ConcatTable,\n\t\t\t\tnn.Sequential( # Sequential,\n\t\t\t\t\tnn.Sequential( # Sequential,\n\t\t\t\t\t\tnn.Conv2d(256,128,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\t\t\t\tnn.BatchNorm2d(128),\n\t\t\t\t\t\tnn.ReLU(),\n\t\t\t\t\t\tnn.Conv2d(128,128,(3, 3),(1, 1),(1, 1),1,32,bias=False),\n\t\t\t\t\t\tnn.BatchNorm2d(128),\n\t\t\t\t\t\tnn.ReLU(),\n\t\t\t\t\t),\n\t\t\t\t\tnn.Conv2d(128,256,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\t\t\tnn.BatchNorm2d(256),\n\t\t\t\t),\n\t\t\t\tLambda(lambda x: x), # Identity,\n\t\t\t),\n\t\t\tLambdaReduce(lambda x,y: x+y), # CAddTable,\n\t\t\tnn.ReLU(),\n\t\t),\n\t\tnn.Sequential( # Sequential,\n\t\t\tLambdaMap(lambda x: x, # ConcatTable,\n\t\t\t\tnn.Sequential( # Sequential,\n\t\t\t\t\tnn.Sequential( # Sequential,\n\t\t\t\t\t\tnn.Conv2d(256,128,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\t\t\t\tnn.BatchNorm2d(128),\n\t\t\t\t\t\tnn.ReLU(),\n\t\t\t\t\t\tnn.Conv2d(128,128,(3, 3),(1, 1),(1, 1),1,32,bias=False),\n\t\t\t\t\t\tnn.BatchNorm2d(128),\n\t\t\t\t\t\tnn.ReLU(),\n\t\t\t\t\t),\n\t\t\t\t\tnn.Conv2d(128,256,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\t\t\tnn.BatchNorm2d(256),\n\t\t\t\t),\n\t\t\t\tLambda(lambda x: x), # Identity,\n\t\t\t),\n\t\t\tLambdaReduce(lambda x,y: x+y), # CAddTable,\n\t\t\tnn.ReLU(),\n\t\t),\n\t),\n\tnn.Sequential( # Sequential,\n\t\tnn.Sequential( # Sequential,\n\t\t\tLambdaMap(lambda x: x, # ConcatTable,\n\t\t\t\tnn.Sequential( # Sequential,\n\t\t\t\t\tnn.Sequential( # Sequential,\n\t\t\t\t\t\tnn.Conv2d(256,256,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\t\t\t\tnn.BatchNorm2d(256),\n\t\t\t\t\t\tnn.ReLU(),\n\t\t\t\t\t\tnn.Conv2d(256,256,(3, 3),(2, 2),(1, 1),1,32,bias=False),\n\t\t\t\t\t\tnn.BatchNorm2d(256),\n\t\t\t\t\t\tnn.ReLU(),\n\t\t\t\t\t),\n\t\t\t\t\tnn.Conv2d(256,512,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\t\t\tnn.BatchNorm2d(512),\n\t\t\t\t),\n\t\t\t\tnn.Sequential( # Sequential,\n\t\t\t\t\tnn.Conv2d(256,512,(1, 1),(2, 2),(0, 0),1,1,bias=False),\n\t\t\t\t\tnn.BatchNorm2d(512),\n\t\t\t\t),\n\t\t\t),\n\t\t\tLambdaReduce(lambda x,y: x+y), # CAddTable,\n\t\t\tnn.ReLU(),\n\t\t),\n\t\tnn.Sequential( # Sequential,\n\t\t\tLambdaMap(lambda x: x, # ConcatTable,\n\t\t\t\tnn.Sequential( # Sequential,\n\t\t\t\t\tnn.Sequential( # Sequential,\n\t\t\t\t\t\tnn.Conv2d(512,256,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\t\t\t\tnn.BatchNorm2d(256),\n\t\t\t\t\t\tnn.ReLU(),\n\t\t\t\t\t\tnn.Conv2d(256,256,(3, 3),(1, 1),(1, 1),1,32,bias=False),\n\t\t\t\t\t\tnn.BatchNorm2d(256),\n\t\t\t\t\t\tnn.ReLU(),\n\t\t\t\t\t),\n\t\t\t\t\tnn.Conv2d(256,512,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\t\t\tnn.BatchNorm2d(512),\n\t\t\t\t),\n\t\t\t\tLambda(lambda x: x), # Identity,\n\t\t\t),\n\t\t\tLambdaReduce(lambda x,y: x+y), # CAddTable,\n\t\t\tnn.ReLU(),\n\t\t),\n\t\tnn.Sequential( # Sequential,\n\t\t\tLambdaMap(lambda x: x, # ConcatTable,\n\t\t\t\tnn.Sequential( # Sequential,\n\t\t\t\t\tnn.Sequential( # Sequential,\n\t\t\t\t\t\tnn.Conv2d(512,256,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\t\t\t\tnn.BatchNorm2d(256),\n\t\t\t\t\t\tnn.ReLU(),\n\t\t\t\t\t\tnn.Conv2d(256,256,(3, 3),(1, 1),(1, 1),1,32,bias=False),\n\t\t\t\t\t\tnn.BatchNorm2d(256),\n\t\t\t\t\t\tnn.ReLU(),\n\t\t\t\t\t),\n\t\t\t\t\tnn.Conv2d(256,512,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\t\t\tnn.BatchNorm2d(512),\n\t\t\t\t),\n\t\t\t\tLambda(lambda x: x), # Identity,\n\t\t\t),\n\t\t\tLambdaReduce(lambda x,y: x+y), # CAddTable,\n\t\t\tnn.ReLU(),\n\t\t),\n\t\tnn.Sequential( # Sequential,\n\t\t\tLambdaMap(lambda x: x, # ConcatTable,\n\t\t\t\tnn.Sequential( # Sequential,\n\t\t\t\t\tnn.Sequential( # Sequential,\n\t\t\t\t\t\tnn.Conv2d(512,256,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\t\t\t\tnn.BatchNorm2d(256),\n\t\t\t\t\t\tnn.ReLU(),\n\t\t\t\t\t\tnn.Conv2d(256,256,(3, 3),(1, 1),(1, 1),1,32,bias=False),\n\t\t\t\t\t\tnn.BatchNorm2d(256),\n\t\t\t\t\t\tnn.ReLU(),\n\t\t\t\t\t),\n\t\t\t\t\tnn.Conv2d(256,512,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\t\t\tnn.BatchNorm2d(512),\n\t\t\t\t),\n\t\t\t\tLambda(lambda x: x), # Identity,\n\t\t\t),\n\t\t\tLambdaReduce(lambda x,y: x+y), # CAddTable,\n\t\t\tnn.ReLU(),\n\t\t),\n\t),\n\tnn.Sequential( # Sequential,\n\t\tnn.Sequential( # Sequential,\n\t\t\tLambdaMap(lambda x: x, # ConcatTable,\n\t\t\t\tnn.Sequential( # Sequential,\n\t\t\t\t\tnn.Sequential( # Sequential,\n\t\t\t\t\t\tnn.Conv2d(512,512,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\t\t\t\tnn.BatchNorm2d(512),\n\t\t\t\t\t\tnn.ReLU(),\n\t\t\t\t\t\tnn.Conv2d(512,512,(3, 3),(2, 2),(1, 1),1,32,bias=False),\n\t\t\t\t\t\tnn.BatchNorm2d(512),\n\t\t\t\t\t\tnn.ReLU(),\n\t\t\t\t\t),\n\t\t\t\t\tnn.Conv2d(512,1024,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\t\t\tnn.BatchNorm2d(1024),\n\t\t\t\t),\n\t\t\t\tnn.Sequential( # Sequential,\n\t\t\t\t\tnn.Conv2d(512,1024,(1, 1),(2, 2),(0, 0),1,1,bias=False),\n\t\t\t\t\tnn.BatchNorm2d(1024),\n\t\t\t\t),\n\t\t\t),\n\t\t\tLambdaReduce(lambda x,y: x+y), # CAddTable,\n\t\t\tnn.ReLU(),\n\t\t),\n\t\tnn.Sequential( # Sequential,\n\t\t\tLambdaMap(lambda x: x, # ConcatTable,\n\t\t\t\tnn.Sequential( # Sequential,\n\t\t\t\t\tnn.Sequential( # Sequential,\n\t\t\t\t\t\tnn.Conv2d(1024,512,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\t\t\t\tnn.BatchNorm2d(512),\n\t\t\t\t\t\tnn.ReLU(),\n\t\t\t\t\t\tnn.Conv2d(512,512,(3, 3),(1, 1),(1, 1),1,32,bias=False),\n\t\t\t\t\t\tnn.BatchNorm2d(512),\n\t\t\t\t\t\tnn.ReLU(),\n\t\t\t\t\t),\n\t\t\t\t\tnn.Conv2d(512,1024,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\t\t\tnn.BatchNorm2d(1024),\n\t\t\t\t),\n\t\t\t\tLambda(lambda x: x), # Identity,\n\t\t\t),\n\t\t\tLambdaReduce(lambda x,y: x+y), # CAddTable,\n\t\t\tnn.ReLU(),\n\t\t),\n\t\tnn.Sequential( # Sequential,\n\t\t\tLambdaMap(lambda x: x, # ConcatTable,\n\t\t\t\tnn.Sequential( # Sequential,\n\t\t\t\t\tnn.Sequential( # Sequential,\n\t\t\t\t\t\tnn.Conv2d(1024,512,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\t\t\t\tnn.BatchNorm2d(512),\n\t\t\t\t\t\tnn.ReLU(),\n\t\t\t\t\t\tnn.Conv2d(512,512,(3, 3),(1, 1),(1, 1),1,32,bias=False),\n\t\t\t\t\t\tnn.BatchNorm2d(512),\n\t\t\t\t\t\tnn.ReLU(),\n\t\t\t\t\t),\n\t\t\t\t\tnn.Conv2d(512,1024,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\t\t\tnn.BatchNorm2d(1024),\n\t\t\t\t),\n\t\t\t\tLambda(lambda x: x), # Identity,\n\t\t\t),\n\t\t\tLambdaReduce(lambda x,y: x+y), # CAddTable,\n\t\t\tnn.ReLU(),\n\t\t),\n\t\tnn.Sequential( # Sequential,\n\t\t\tLambdaMap(lambda x: x, # ConcatTable,\n\t\t\t\tnn.Sequential( # Sequential,\n\t\t\t\t\tnn.Sequential( # Sequential,\n\t\t\t\t\t\tnn.Conv2d(1024,512,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\t\t\t\tnn.BatchNorm2d(512),\n\t\t\t\t\t\tnn.ReLU(),\n\t\t\t\t\t\tnn.Conv2d(512,512,(3, 3),(1, 1),(1, 1),1,32,bias=False),\n\t\t\t\t\t\tnn.BatchNorm2d(512),\n\t\t\t\t\t\tnn.ReLU(),\n\t\t\t\t\t),\n\t\t\t\t\tnn.Conv2d(512,1024,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\t\t\tnn.BatchNorm2d(1024),\n\t\t\t\t),\n\t\t\t\tLambda(lambda x: x), # Identity,\n\t\t\t),\n\t\t\tLambdaReduce(lambda x,y: x+y), # CAddTable,\n\t\t\tnn.ReLU(),\n\t\t),\n\t\tnn.Sequential( # Sequential,\n\t\t\tLambdaMap(lambda x: x, # ConcatTable,\n\t\t\t\tnn.Sequential( # Sequential,\n\t\t\t\t\tnn.Sequential( # Sequential,\n\t\t\t\t\t\tnn.Conv2d(1024,512,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\t\t\t\tnn.BatchNorm2d(512),\n\t\t\t\t\t\tnn.ReLU(),\n\t\t\t\t\t\tnn.Conv2d(512,512,(3, 3),(1, 1),(1, 1),1,32,bias=False),\n\t\t\t\t\t\tnn.BatchNorm2d(512),\n\t\t\t\t\t\tnn.ReLU(),\n\t\t\t\t\t),\n\t\t\t\t\tnn.Conv2d(512,1024,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\t\t\tnn.BatchNorm2d(1024),\n\t\t\t\t),\n\t\t\t\tLambda(lambda x: x), # Identity,\n\t\t\t),\n\t\t\tLambdaReduce(lambda x,y: x+y), # CAddTable,\n\t\t\tnn.ReLU(),\n\t\t),\n\t\tnn.Sequential( # Sequential,\n\t\t\tLambdaMap(lambda x: x, # ConcatTable,\n\t\t\t\tnn.Sequential( # Sequential,\n\t\t\t\t\tnn.Sequential( # Sequential,\n\t\t\t\t\t\tnn.Conv2d(1024,512,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\t\t\t\tnn.BatchNorm2d(512),\n\t\t\t\t\t\tnn.ReLU(),\n\t\t\t\t\t\tnn.Conv2d(512,512,(3, 3),(1, 1),(1, 1),1,32,bias=False),\n\t\t\t\t\t\tnn.BatchNorm2d(512),\n\t\t\t\t\t\tnn.ReLU(),\n\t\t\t\t\t),\n\t\t\t\t\tnn.Conv2d(512,1024,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\t\t\tnn.BatchNorm2d(1024),\n\t\t\t\t),\n\t\t\t\tLambda(lambda x: x), # Identity,\n\t\t\t),\n\t\t\tLambdaReduce(lambda x,y: x+y), # CAddTable,\n\t\t\tnn.ReLU(),\n\t\t),\n\t),\n\tnn.Sequential( # Sequential,\n\t\tnn.Sequential( # Sequential,\n\t\t\tLambdaMap(lambda x: x, # ConcatTable,\n\t\t\t\tnn.Sequential( # Sequential,\n\t\t\t\t\tnn.Sequential( # Sequential,\n\t\t\t\t\t\tnn.Conv2d(1024,1024,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\t\t\t\tnn.BatchNorm2d(1024),\n\t\t\t\t\t\tnn.ReLU(),\n\t\t\t\t\t\tnn.Conv2d(1024,1024,(3, 3),(2, 2),(1, 1),1,32,bias=False),\n\t\t\t\t\t\tnn.BatchNorm2d(1024),\n\t\t\t\t\t\tnn.ReLU(),\n\t\t\t\t\t),\n\t\t\t\t\tnn.Conv2d(1024,2048,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\t\t\tnn.BatchNorm2d(2048),\n\t\t\t\t),\n\t\t\t\tnn.Sequential( # Sequential,\n\t\t\t\t\tnn.Conv2d(1024,2048,(1, 1),(2, 2),(0, 0),1,1,bias=False),\n\t\t\t\t\tnn.BatchNorm2d(2048),\n\t\t\t\t),\n\t\t\t),\n\t\t\tLambdaReduce(lambda x,y: x+y), # CAddTable,\n\t\t\tnn.ReLU(),\n\t\t),\n\t\tnn.Sequential( # Sequential,\n\t\t\tLambdaMap(lambda x: x, # ConcatTable,\n\t\t\t\tnn.Sequential( # Sequential,\n\t\t\t\t\tnn.Sequential( # Sequential,\n\t\t\t\t\t\tnn.Conv2d(2048,1024,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\t\t\t\tnn.BatchNorm2d(1024),\n\t\t\t\t\t\tnn.ReLU(),\n\t\t\t\t\t\tnn.Conv2d(1024,1024,(3, 3),(1, 1),(1, 1),1,32,bias=False),\n\t\t\t\t\t\tnn.BatchNorm2d(1024),\n\t\t\t\t\t\tnn.ReLU(),\n\t\t\t\t\t),\n\t\t\t\t\tnn.Conv2d(1024,2048,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\t\t\tnn.BatchNorm2d(2048),\n\t\t\t\t),\n\t\t\t\tLambda(lambda x: x), # Identity,\n\t\t\t),\n\t\t\tLambdaReduce(lambda x,y: x+y), # CAddTable,\n\t\t\tnn.ReLU(),\n\t\t),\n\t\tnn.Sequential( # Sequential,\n\t\t\tLambdaMap(lambda x: x, # ConcatTable,\n\t\t\t\tnn.Sequential( # Sequential,\n\t\t\t\t\tnn.Sequential( # Sequential,\n\t\t\t\t\t\tnn.Conv2d(2048,1024,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\t\t\t\tnn.BatchNorm2d(1024),\n\t\t\t\t\t\tnn.ReLU(),\n\t\t\t\t\t\tnn.Conv2d(1024,1024,(3, 3),(1, 1),(1, 1),1,32,bias=False),\n\t\t\t\t\t\tnn.BatchNorm2d(1024),\n\t\t\t\t\t\tnn.ReLU(),\n\t\t\t\t\t),\n\t\t\t\t\tnn.Conv2d(1024,2048,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n\t\t\t\t\tnn.BatchNorm2d(2048),\n\t\t\t\t),\n\t\t\t\tLambda(lambda x: x), # Identity,\n\t\t\t),\n\t\t\tLambdaReduce(lambda x,y: x+y), # CAddTable,\n\t\t\tnn.ReLU(),\n\t\t),\n\t),\n\tnn.AvgPool2d((7, 7),(1, 1)),\n\tLambda(lambda x: x.view(x.size(0),-1)), # View,\n\tnn.Sequential(Lambda(lambda x: x.view(1,-1) if 1==len(x.size()) else x ),nn.Linear(2048,1000)), # Linear,\n)'"
ImageNet-P/test.py,28,"b'import numpy as np\nimport argparse\nimport torch\nimport torch.backends.cudnn as cudnn\nimport torch.nn.functional as F\nimport torchvision.datasets as dset\nimport torchvision.transforms as trn\nimport torchvision.transforms.functional as trn_F\nimport torchvision.models as models\nimport torch.utils.model_zoo as model_zoo\nfrom resnext_50_32x4d import resnext_50_32x4d\nfrom resnext_101_32x4d import resnext_101_32x4d\nfrom resnext_101_64x4d import resnext_101_64x4d\nfrom scipy.stats import rankdata\n\nif __package__ is None:\n    import sys\n    from os import path\n\n    sys.path.append(path.dirname(path.dirname(path.abspath(__file__))))\n    from utils.video_loader import VideoFolder\n\nparser = argparse.ArgumentParser(description=\'Evaluates robustness of various nets on ImageNet\',\n                                 formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n# Architecture\nparser.add_argument(\'--model-name\', \'-m\', default=\'resnet18\', type=str,\n                    choices=[\'alexnet\', \'squeezenet1.1\', \'vgg11\', \'vgg19\', \'vggbn\',\n                             \'densenet121\', \'densenet169\', \'densenet201\', \'densenet161\',\n                             \'resnet18\', \'resnet34\', \'resnet50\', \'resnet101\', \'resnet152\',\n                             \'resnext50\', \'resnext101\', \'resnext101_64\'])\nparser.add_argument(\'--perturbation\', \'-p\', default=\'brightness\', type=str,\n                    choices=[\'gaussian_noise\', \'shot_noise\', \'motion_blur\', \'zoom_blur\',\n                             \'spatter\', \'brightness\', \'translate\', \'rotate\', \'tilt\', \'scale\',\n                             \'speckle_noise\', \'gaussian_blur\', \'snow\', \'shear\'])\nparser.add_argument(\'--difficulty\', \'-d\', type=int, default=1, choices=[1, 2, 3])\n# Acceleration\nparser.add_argument(\'--ngpu\', type=int, default=1, help=\'0 = CPU.\')\nargs = parser.parse_args()\nprint(args)\n\n# /////////////// Model Setup ///////////////\n\nif args.model_name == \'alexnet\':\n    net = models.AlexNet()\n    net.load_state_dict(model_zoo.load_url(\'https://download.pytorch.org/models/alexnet-owt-4df8aa71.pth\',\n                                           # model_dir=\'/share/data/lang/users/dan/.torch/models\'))\n                                           model_dir=\'/share/data/vision-greg2/pytorch_models/alexnet\'))\n    args.test_bs = 6\n\nelif args.model_name == \'squeezenet1.0\':\n    net = models.SqueezeNet(version=1.0)\n    net.load_state_dict(model_zoo.load_url(\'https://download.pytorch.org/models/squeezenet1_0-a815701f.pth\',\n                                           # model_dir=\'/share/data/lang/users/dan/.torch/models\'))\n                                           model_dir=\'/share/data/vision-greg2/pytorch_models/squeezenet\'))\n    args.test_bs = 6\n\nelif args.model_name == \'squeezenet1.1\':\n    net = models.SqueezeNet(version=1.1)\n    net.load_state_dict(model_zoo.load_url(\'https://download.pytorch.org/models/squeezenet1_1-f364aa15.pth\',\n                                           # model_dir=\'/share/data/lang/users/dan/.torch/models\'))\n                                           model_dir=\'/share/data/vision-greg2/pytorch_models/squeezenet\'))\n    args.test_bs = 6\n\nelif \'vgg\' in args.model_name:\n    if \'bn\' not in args.model_name and \'11\' not in args.model_name:\n        net = models.vgg19()\n        net.load_state_dict(model_zoo.load_url(\'https://download.pytorch.org/models/vgg19-dcbb9e9d.pth\',\n                                               # model_dir=\'/share/data/lang/users/dan/.torch/models\'))\n                                               model_dir=\'/share/data/vision-greg2/pytorch_models/vgg\'))\n    elif \'11\' in args.model_name:\n        net = models.vgg11()\n        net.load_state_dict(model_zoo.load_url(\'https://download.pytorch.org/models/vgg11-bbd30ac9.pth\',\n                                               # model_dir=\'/share/data/lang/users/dan/.torch/models\'))\n                                               model_dir=\'/share/data/vision-greg2/pytorch_models/vgg\'))\n    else:\n        net = models.vgg19_bn()\n        net.load_state_dict(model_zoo.load_url(\'https://download.pytorch.org/models/vgg19_bn-c79401a0.pth\',\n                                               # model_dir=\'/share/data/lang/users/dan/.torch/models\'))\n                                               model_dir=\'/share/data/vision-greg2/pytorch_models/vgg\'))\n    args.test_bs = 2\n\nelif args.model_name == \'densenet121\':\n    net = models.densenet121()\n\n    import re\n    # \'.\'s are no longer allowed in module names, but pervious _DenseLayer\n    # has keys \'norm.1\', \'relu.1\', \'conv.1\', \'norm.2\', \'relu.2\', \'conv.2\'.\n    # They are also in the checkpoints in model_urls.\n    # This pattern is used to find such keys.\n    pattern = re.compile(\n        r\'^(.*denselayer\\d+\\.(?:norm|relu|conv))\\.((?:[12])\\.(?:weight|bias|running_mean|running_var))$\')\n    state_dict = model_zoo.load_url(\'https://download.pytorch.org/models/densenet121-a639ec97.pth\',\n                                    model_dir=\'/share/data/vision-greg2/pytorch_models/densenet\')\n    for key in list(state_dict.keys()):\n        res = pattern.match(key)\n        if res:\n            new_key = res.group(1) + res.group(2)\n            state_dict[new_key] = state_dict[key]\n            del state_dict[key]\n\n    net.load_state_dict(state_dict)\n    args.test_bs = 5\n\nelif args.model_name == \'densenet161\':\n    net = models.densenet161()\n\n    import re\n    pattern = re.compile(\n        r\'^(.*denselayer\\d+\\.(?:norm|relu|conv))\\.((?:[12])\\.(?:weight|bias|running_mean|running_var))$\')\n    state_dict = model_zoo.load_url(\'https://download.pytorch.org/models/densenet161-8d451a50.pth\',\n                                    model_dir=\'/share/data/vision-greg2/pytorch_models/densenet\')\n    for key in list(state_dict.keys()):\n        res = pattern.match(key)\n        if res:\n            new_key = res.group(1) + res.group(2)\n            state_dict[new_key] = state_dict[key]\n            del state_dict[key]\n\n    net.load_state_dict(state_dict)\n\n    args.test_bs = 3\n\nelif args.model_name == \'resnet18\':\n    net = models.resnet18()\n    net.load_state_dict(model_zoo.load_url(\'https://download.pytorch.org/models/resnet18-5c106cde.pth\',\n                                           # model_dir=\'/share/data/lang/users/dan/.torch/models\'))\n                                           model_dir=\'/share/data/vision-greg2/pytorch_models/resnet\'))\n    args.test_bs = 5\n\nelif args.model_name == \'resnet34\':\n    net = models.resnet34()\n    net.load_state_dict(model_zoo.load_url(\'https://download.pytorch.org/models/resnet34-333f7ec4.pth\',\n                                           # model_dir=\'/share/data/lang/users/dan/.torch/models\'))\n                                           model_dir=\'/share/data/vision-greg2/pytorch_models/resnet\'))\n    args.test_bs = 4\n\nelif args.model_name == \'resnet50\':\n    net = models.resnet50()\n    net.load_state_dict(model_zoo.load_url(\'https://download.pytorch.org/models/resnet50-19c8e357.pth\',\n                                           # model_dir=\'/share/data/lang/users/dan/.torch/models\'))\n                                           model_dir=\'/share/data/vision-greg2/pytorch_models/resnet\'))\n    args.test_bs = 4\n\nelif args.model_name == \'resnet101\':\n    net = models.resnet101()\n    net.load_state_dict(model_zoo.load_url(\'https://download.pytorch.org/models/resnet101-5d3b4d8f.pth\',\n                                           # model_dir=\'/share/data/lang/users/dan/.torch/models\'))\n                                           model_dir=\'/share/data/vision-greg2/pytorch_models/resnet\'))\n    args.test_bs = 3\n\nelif args.model_name == \'resnet152\':\n    net = models.resnet152()\n    net.load_state_dict(model_zoo.load_url(\'https://download.pytorch.org/models/resnet152-b121ed2d.pth\',\n                                           # model_dir=\'/share/data/lang/users/dan/.torch/models\'))\n                                           model_dir=\'/share/data/vision-greg2/pytorch_models/resnet\'))\n    args.test_bs = 3\n\nelif args.model_name == \'resnext50\':\n    net = resnext_50_32x4d\n    # net.load_state_dict(torch.load(\'/share/data/lang/users/dan/.torch/models/resnext_50_32x4d.pth\'))\n    net.load_state_dict(torch.load(\'/share/data/vision-greg2/pytorch_models/resnext_50_32x4d.pth\'))\n    args.test_bs = 3\n\nelif args.model_name == \'resnext101\':\n    net = resnext_101_32x4d\n    # net.load_state_dict(torch.load(\'/share/data/lang/users/dan/.torch/models/resnext_101_32x4d.pth\'))\n    net.load_state_dict(torch.load(\'/share/data/vision-greg2/pytorch_models/resnext_101_32x4d.pth\'))\n    args.test_bs = 3\n\nelif args.model_name == \'resnext101_64\':\n    net = resnext_101_64x4d\n    # net.load_state_dict(torch.load(\'/share/data/lang/users/dan/.torch/models/resnext_101_64x4d.pth\'))\n    net.load_state_dict(torch.load(\'/share/data/vision-greg2/pytorch_models/resnext_101_64x4d.pth\'))\n    args.test_bs = 3\n\nargs.prefetch = 4\n\nif args.ngpu > 1:\n    net = torch.nn.DataParallel(net, device_ids=list(range(args.ngpu)))\n\nif args.ngpu > 0:\n    net.cuda()\n\ntorch.manual_seed(1)\nnp.random.seed(1)\nif args.ngpu > 0:\n    torch.cuda.manual_seed(1)\n\nnet.eval()\ncudnn.benchmark = True  # fire on all cylinders\n\nprint(\'Model Loaded\\n\')\n\n# /////////////// Data Loader ///////////////\nmean = [0.485, 0.456, 0.406]\nstd = [0.229, 0.224, 0.225]\n\nif args.difficulty > 1 and \'noise\' in args.perturbation:\n    loader = torch.utils.data.DataLoader(\n        VideoFolder(root=""/share/data/vision-greg2/users/dan/datasets/ImageNet-P/"" +\n                         args.perturbation + \'_\' + str(args.difficulty),\n                    transform=trn.Compose([trn.ToTensor(), trn.Normalize(mean, std)])),\n        batch_size=args.test_bs, shuffle=False, num_workers=5, pin_memory=True)\nelse:\n    loader = torch.utils.data.DataLoader(\n        VideoFolder(root=""/share/data/vision-greg2/users/dan/datasets/ImageNet-P/"" + args.perturbation,\n                    transform=trn.Compose([trn.ToTensor(), trn.Normalize(mean, std)])),\n        batch_size=args.test_bs, shuffle=False, num_workers=5, pin_memory=True)\n\nprint(\'Data Loaded\\n\')\n\n\n# /////////////// Stability Measurements ///////////////\n\nidentity = np.asarray(range(1, 1001))\ncum_sum_top5 = np.cumsum(np.asarray([0] + [1] * 5 + [0] * (999 - 5)))\nrecip = 1./identity\n\n# def top5_dist(sigma):\n#     result = 0\n#     for i in range(1,6):\n#         for j in range(min(sigma[i-1], i) + 1, max(sigma[i-1], i) + 1):\n#             if 1 <= j - 1 <= 5:\n#                 result += 1\n#     return result\n\ndef dist(sigma, mode=\'top5\'):\n    if mode == \'top5\':\n        return np.sum(np.abs(cum_sum_top5[:5] - cum_sum_top5[sigma-1][:5]))\n    elif mode == \'zipf\':\n        return np.sum(np.abs(recip - recip[sigma-1])*recip)\n\n\ndef ranking_dist(ranks, noise_perturbation=True if \'noise\' in args.perturbation else False, mode=\'top5\'):\n    result = 0\n    step_size = 1 if noise_perturbation else args.difficulty\n\n    for vid_ranks in ranks:\n        result_for_vid = []\n\n        for i in range(step_size):\n            perm1 = vid_ranks[i]\n            perm1_inv = np.argsort(perm1)\n\n            for rank in vid_ranks[i::step_size][1:]:\n                perm2 = rank\n                result_for_vid.append(dist(perm2[perm1_inv], mode))\n                if not noise_perturbation:\n                    perm1 = perm2\n                    perm1_inv = np.argsort(perm1)\n\n        result += np.mean(result_for_vid) / len(ranks)\n\n    return result\n\n\ndef flip_prob(predictions, noise_perturbation=True if \'noise\' in args.perturbation else False):\n    result = 0\n    step_size = 1 if noise_perturbation else args.difficulty\n\n    for vid_preds in predictions:\n        result_for_vid = []\n\n        for i in range(step_size):\n            prev_pred = vid_preds[i]\n\n            for pred in vid_preds[i::step_size][1:]:\n                result_for_vid.append(int(prev_pred != pred))\n                if not noise_perturbation: prev_pred = pred\n\n        result += np.mean(result_for_vid) / len(predictions)\n\n    return result\n\n\n# /////////////// Get Results ///////////////\n\nfrom tqdm import tqdm\n\npredictions, ranks = [], []\nwith torch.no_grad():\n\n    for data, target in loader:\n        num_vids = data.size(0)\n        data = data.view(-1,3,224,224).cuda()\n\n        output = net(data)\n\n        for vid in output.view(num_vids, -1, 1000):\n            predictions.append(vid.argmax(1).to(\'cpu\').numpy())\n            ranks.append([np.uint16(rankdata(-frame, method=\'ordinal\')) for frame in vid.to(\'cpu\').numpy()])\n\n\nranks = np.asarray(ranks)\n\nprint(\'Computing Metrics\\n\')\n\nprint(\'Flipping Prob\\t{:.5f}\'.format(flip_prob(predictions)))\nprint(\'Top5 Distance\\t{:.5f}\'.format(ranking_dist(ranks, mode=\'top5\')))\nprint(\'Zipf Distance\\t{:.5f}\'.format(ranking_dist(ranks, mode=\'zipf\')))\n\n'"
ImageNet-C/create_c/make_cifar_c.py,1,"b'# -*- coding: utf-8 -*-\n\nimport os\nfrom PIL import Image\nimport os.path\nimport time\nimport torch\nimport torchvision.datasets as dset\nimport torchvision.transforms as trn\nimport torch.utils.data as data\nimport numpy as np\n\nfrom PIL import Image\n\n\n# /////////////// Distortion Helpers ///////////////\n\nimport skimage as sk\nfrom skimage.filters import gaussian\nfrom io import BytesIO\nfrom wand.image import Image as WandImage\nfrom wand.api import library as wandlibrary\nimport wand.color as WandColor\nimport ctypes\nfrom PIL import Image as PILImage\nimport cv2\nfrom scipy.ndimage import zoom as scizoom\nfrom scipy.ndimage.interpolation import map_coordinates\nimport warnings\n\nwarnings.simplefilter(""ignore"", UserWarning)\n\n\ndef disk(radius, alias_blur=0.1, dtype=np.float32):\n    if radius <= 8:\n        L = np.arange(-8, 8 + 1)\n        ksize = (3, 3)\n    else:\n        L = np.arange(-radius, radius + 1)\n        ksize = (5, 5)\n    X, Y = np.meshgrid(L, L)\n    aliased_disk = np.array((X ** 2 + Y ** 2) <= radius ** 2, dtype=dtype)\n    aliased_disk /= np.sum(aliased_disk)\n\n    # supersample disk to antialias\n    return cv2.GaussianBlur(aliased_disk, ksize=ksize, sigmaX=alias_blur)\n\n\n# Tell Python about the C method\nwandlibrary.MagickMotionBlurImage.argtypes = (ctypes.c_void_p,  # wand\n                                              ctypes.c_double,  # radius\n                                              ctypes.c_double,  # sigma\n                                              ctypes.c_double)  # angle\n\n\n# Extend wand.image.Image class to include method signature\nclass MotionImage(WandImage):\n    def motion_blur(self, radius=0.0, sigma=0.0, angle=0.0):\n        wandlibrary.MagickMotionBlurImage(self.wand, radius, sigma, angle)\n\n\n# modification of https://github.com/FLHerne/mapgen/blob/master/diamondsquare.py\ndef plasma_fractal(mapsize=32, wibbledecay=3):\n    """"""\n    Generate a heightmap using diamond-square algorithm.\n    Return square 2d array, side length \'mapsize\', of floats in range 0-255.\n    \'mapsize\' must be a power of two.\n    """"""\n    assert (mapsize & (mapsize - 1) == 0)\n    maparray = np.empty((mapsize, mapsize), dtype=np.float_)\n    maparray[0, 0] = 0\n    stepsize = mapsize\n    wibble = 100\n\n    def wibbledmean(array):\n        return array / 4 + wibble * np.random.uniform(-wibble, wibble, array.shape)\n\n    def fillsquares():\n        """"""For each square of points stepsize apart,\n           calculate middle value as mean of points + wibble""""""\n        cornerref = maparray[0:mapsize:stepsize, 0:mapsize:stepsize]\n        squareaccum = cornerref + np.roll(cornerref, shift=-1, axis=0)\n        squareaccum += np.roll(squareaccum, shift=-1, axis=1)\n        maparray[stepsize // 2:mapsize:stepsize,\n        stepsize // 2:mapsize:stepsize] = wibbledmean(squareaccum)\n\n    def filldiamonds():\n        """"""For each diamond of points stepsize apart,\n           calculate middle value as mean of points + wibble""""""\n        mapsize = maparray.shape[0]\n        drgrid = maparray[stepsize // 2:mapsize:stepsize, stepsize // 2:mapsize:stepsize]\n        ulgrid = maparray[0:mapsize:stepsize, 0:mapsize:stepsize]\n        ldrsum = drgrid + np.roll(drgrid, 1, axis=0)\n        lulsum = ulgrid + np.roll(ulgrid, -1, axis=1)\n        ltsum = ldrsum + lulsum\n        maparray[0:mapsize:stepsize, stepsize // 2:mapsize:stepsize] = wibbledmean(ltsum)\n        tdrsum = drgrid + np.roll(drgrid, 1, axis=1)\n        tulsum = ulgrid + np.roll(ulgrid, -1, axis=0)\n        ttsum = tdrsum + tulsum\n        maparray[stepsize // 2:mapsize:stepsize, 0:mapsize:stepsize] = wibbledmean(ttsum)\n\n    while stepsize >= 2:\n        fillsquares()\n        filldiamonds()\n        stepsize //= 2\n        wibble /= wibbledecay\n\n    maparray -= maparray.min()\n    return maparray / maparray.max()\n\n\ndef clipped_zoom(img, zoom_factor):\n    h = img.shape[0]\n    # ceil crop height(= crop width)\n    ch = int(np.ceil(h / zoom_factor))\n\n    top = (h - ch) // 2\n    img = scizoom(img[top:top + ch, top:top + ch], (zoom_factor, zoom_factor, 1), order=1)\n    # trim off any extra pixels\n    trim_top = (img.shape[0] - h) // 2\n\n    return img[trim_top:trim_top + h, trim_top:trim_top + h]\n\n\n# /////////////// End Distortion Helpers ///////////////\n\n\n# /////////////// Distortions ///////////////\n\ndef gaussian_noise(x, severity=1):\n    c = [0.04, 0.06, .08, .09, .10][severity - 1]\n\n    x = np.array(x) / 255.\n    return np.clip(x + np.random.normal(size=x.shape, scale=c), 0, 1) * 255\n\n\ndef shot_noise(x, severity=1):\n    c = [500, 250, 100, 75, 50][severity - 1]\n\n    x = np.array(x) / 255.\n    return np.clip(np.random.poisson(x * c) / c, 0, 1) * 255\n\n\ndef impulse_noise(x, severity=1):\n    c = [.01, .02, .03, .05, .07][severity - 1]\n\n    x = sk.util.random_noise(np.array(x) / 255., mode=\'s&p\', amount=c)\n    return np.clip(x, 0, 1) * 255\n\n\ndef speckle_noise(x, severity=1):\n    c = [.06, .1, .12, .16, .2][severity - 1]\n\n    x = np.array(x) / 255.\n    return np.clip(x + x * np.random.normal(size=x.shape, scale=c), 0, 1) * 255\n\n\ndef gaussian_blur(x, severity=1):\n    c = [.4, .6, 0.7, .8, 1][severity - 1]\n\n    x = gaussian(np.array(x) / 255., sigma=c, multichannel=True)\n    return np.clip(x, 0, 1) * 255\n\n\ndef glass_blur(x, severity=1):\n    # sigma, max_delta, iterations\n    c = [(0.05,1,1), (0.25,1,1), (0.4,1,1), (0.25,1,2), (0.4,1,2)][severity - 1]\n\n    x = np.uint8(gaussian(np.array(x) / 255., sigma=c[0], multichannel=True) * 255)\n\n    # locally shuffle pixels\n    for i in range(c[2]):\n        for h in range(32 - c[1], c[1], -1):\n            for w in range(32 - c[1], c[1], -1):\n                dx, dy = np.random.randint(-c[1], c[1], size=(2,))\n                h_prime, w_prime = h + dy, w + dx\n                # swap\n                x[h, w], x[h_prime, w_prime] = x[h_prime, w_prime], x[h, w]\n\n    return np.clip(gaussian(x / 255., sigma=c[0], multichannel=True), 0, 1) * 255\n\n\ndef defocus_blur(x, severity=1):\n    c = [(0.3, 0.4), (0.4, 0.5), (0.5, 0.6), (1, 0.2), (1.5, 0.1)][severity - 1]\n\n    x = np.array(x) / 255.\n    kernel = disk(radius=c[0], alias_blur=c[1])\n\n    channels = []\n    for d in range(3):\n        channels.append(cv2.filter2D(x[:, :, d], -1, kernel))\n    channels = np.array(channels).transpose((1, 2, 0))  # 3x32x32 -> 32x32x3\n\n    return np.clip(channels, 0, 1) * 255\n\n\ndef motion_blur(x, severity=1):\n    c = [(6,1), (6,1.5), (6,2), (8,2), (9,2.5)][severity - 1]\n\n    output = BytesIO()\n    x.save(output, format=\'PNG\')\n    x = MotionImage(blob=output.getvalue())\n\n    x.motion_blur(radius=c[0], sigma=c[1], angle=np.random.uniform(-45, 45))\n\n    x = cv2.imdecode(np.fromstring(x.make_blob(), np.uint8),\n                     cv2.IMREAD_UNCHANGED)\n\n    if x.shape != (32, 32):\n        return np.clip(x[..., [2, 1, 0]], 0, 255)  # BGR to RGB\n    else:  # greyscale to RGB\n        return np.clip(np.array([x, x, x]).transpose((1, 2, 0)), 0, 255)\n\n\ndef zoom_blur(x, severity=1):\n    c = [np.arange(1, 1.06, 0.01), np.arange(1, 1.11, 0.01), np.arange(1, 1.16, 0.01),\n         np.arange(1, 1.21, 0.01), np.arange(1, 1.26, 0.01)][severity - 1]\n\n    x = (np.array(x) / 255.).astype(np.float32)\n    out = np.zeros_like(x)\n    for zoom_factor in c:\n        out += clipped_zoom(x, zoom_factor)\n\n    x = (x + out) / (len(c) + 1)\n    return np.clip(x, 0, 1) * 255\n\n\ndef fog(x, severity=1):\n    c = [(.2,3), (.5,3), (0.75,2.5), (1,2), (1.5,1.75)][severity - 1]\n\n    x = np.array(x) / 255.\n    max_val = x.max()\n    x += c[0] * plasma_fractal(wibbledecay=c[1])[:32, :32][..., np.newaxis]\n    return np.clip(x * max_val / (max_val + c[0]), 0, 1) * 255\n\n\ndef frost(x, severity=1):\n    c = [(1, 0.2), (1, 0.3), (0.9, 0.4), (0.85, 0.4), (0.75, 0.45)][severity - 1]\n    idx = np.random.randint(5)\n    filename = [\'./frost1.png\', \'./frost2.png\', \'./frost3.png\', \'./frost4.jpg\', \'./frost5.jpg\', \'./frost6.jpg\'][idx]\n    frost = cv2.imread(filename)\n    frost = cv2.resize(frost, (0, 0), fx=0.2, fy=0.2)\n    # randomly crop and convert to rgb\n    x_start, y_start = np.random.randint(0, frost.shape[0] - 32), np.random.randint(0, frost.shape[1] - 32)\n    frost = frost[x_start:x_start + 32, y_start:y_start + 32][..., [2, 1, 0]]\n\n    return np.clip(c[0] * np.array(x) + c[1] * frost, 0, 255)\n\n\ndef snow(x, severity=1):\n    c = [(0.1,0.2,1,0.6,8,3,0.95),\n         (0.1,0.2,1,0.5,10,4,0.9),\n         (0.15,0.3,1.75,0.55,10,4,0.9),\n         (0.25,0.3,2.25,0.6,12,6,0.85),\n         (0.3,0.3,1.25,0.65,14,12,0.8)][severity - 1]\n\n    x = np.array(x, dtype=np.float32) / 255.\n    snow_layer = np.random.normal(size=x.shape[:2], loc=c[0], scale=c[1])  # [:2] for monochrome\n\n    snow_layer = clipped_zoom(snow_layer[..., np.newaxis], c[2])\n    snow_layer[snow_layer < c[3]] = 0\n\n    snow_layer = PILImage.fromarray((np.clip(snow_layer.squeeze(), 0, 1) * 255).astype(np.uint8), mode=\'L\')\n    output = BytesIO()\n    snow_layer.save(output, format=\'PNG\')\n    snow_layer = MotionImage(blob=output.getvalue())\n\n    snow_layer.motion_blur(radius=c[4], sigma=c[5], angle=np.random.uniform(-135, -45))\n\n    snow_layer = cv2.imdecode(np.fromstring(snow_layer.make_blob(), np.uint8),\n                              cv2.IMREAD_UNCHANGED) / 255.\n    snow_layer = snow_layer[..., np.newaxis]\n\n    x = c[6] * x + (1 - c[6]) * np.maximum(x, cv2.cvtColor(x, cv2.COLOR_RGB2GRAY).reshape(32, 32, 1) * 1.5 + 0.5)\n    return np.clip(x + snow_layer + np.rot90(snow_layer, k=2), 0, 1) * 255\n\n\ndef spatter(x, severity=1):\n    c = [(0.62,0.1,0.7,0.7,0.5,0),\n         (0.65,0.1,0.8,0.7,0.5,0),\n         (0.65,0.3,1,0.69,0.5,0),\n         (0.65,0.1,0.7,0.69,0.6,1),\n         (0.65,0.1,0.5,0.68,0.6,1)][severity - 1]\n    x = np.array(x, dtype=np.float32) / 255.\n\n    liquid_layer = np.random.normal(size=x.shape[:2], loc=c[0], scale=c[1])\n\n    liquid_layer = gaussian(liquid_layer, sigma=c[2])\n    liquid_layer[liquid_layer < c[3]] = 0\n    if c[5] == 0:\n        liquid_layer = (liquid_layer * 255).astype(np.uint8)\n        dist = 255 - cv2.Canny(liquid_layer, 50, 150)\n        dist = cv2.distanceTransform(dist, cv2.DIST_L2, 5)\n        _, dist = cv2.threshold(dist, 20, 20, cv2.THRESH_TRUNC)\n        dist = cv2.blur(dist, (3, 3)).astype(np.uint8)\n        dist = cv2.equalizeHist(dist)\n        #     ker = np.array([[-1,-2,-3],[-2,0,0],[-3,0,1]], dtype=np.float32)\n        #     ker -= np.mean(ker)\n        ker = np.array([[-2, -1, 0], [-1, 1, 1], [0, 1, 2]])\n        dist = cv2.filter2D(dist, cv2.CV_8U, ker)\n        dist = cv2.blur(dist, (3, 3)).astype(np.float32)\n\n        m = cv2.cvtColor(liquid_layer * dist, cv2.COLOR_GRAY2BGRA)\n        m /= np.max(m, axis=(0, 1))\n        m *= c[4]\n\n        # water is pale turqouise\n        color = np.concatenate((175 / 255. * np.ones_like(m[..., :1]),\n                                238 / 255. * np.ones_like(m[..., :1]),\n                                238 / 255. * np.ones_like(m[..., :1])), axis=2)\n\n        color = cv2.cvtColor(color, cv2.COLOR_BGR2BGRA)\n        x = cv2.cvtColor(x, cv2.COLOR_BGR2BGRA)\n\n        return cv2.cvtColor(np.clip(x + m * color, 0, 1), cv2.COLOR_BGRA2BGR) * 255\n    else:\n        m = np.where(liquid_layer > c[3], 1, 0)\n        m = gaussian(m.astype(np.float32), sigma=c[4])\n        m[m < 0.8] = 0\n        #         m = np.abs(m) ** (1/c[4])\n\n        # mud brown\n        color = np.concatenate((63 / 255. * np.ones_like(x[..., :1]),\n                                42 / 255. * np.ones_like(x[..., :1]),\n                                20 / 255. * np.ones_like(x[..., :1])), axis=2)\n\n        color *= m[..., np.newaxis]\n        x *= (1 - m[..., np.newaxis])\n\n        return np.clip(x + color, 0, 1) * 255\n\n\ndef contrast(x, severity=1):\n    c = [.75, .5, .4, .3, 0.15][severity - 1]\n\n    x = np.array(x) / 255.\n    means = np.mean(x, axis=(0, 1), keepdims=True)\n    return np.clip((x - means) * c + means, 0, 1) * 255\n\n\ndef brightness(x, severity=1):\n    c = [.05, .1, .15, .2, .3][severity - 1]\n\n    x = np.array(x) / 255.\n    x = sk.color.rgb2hsv(x)\n    x[:, :, 2] = np.clip(x[:, :, 2] + c, 0, 1)\n    x = sk.color.hsv2rgb(x)\n\n    return np.clip(x, 0, 1) * 255\n\n\ndef saturate(x, severity=1):\n    c = [(0.3, 0), (0.1, 0), (1.5, 0), (2, 0.1), (2.5, 0.2)][severity - 1]\n\n    x = np.array(x) / 255.\n    x = sk.color.rgb2hsv(x)\n    x[:, :, 1] = np.clip(x[:, :, 1] * c[0] + c[1], 0, 1)\n    x = sk.color.hsv2rgb(x)\n\n    return np.clip(x, 0, 1) * 255\n\n\ndef jpeg_compression(x, severity=1):\n    c = [80, 65, 58, 50, 40][severity - 1]\n\n    output = BytesIO()\n    x.save(output, \'JPEG\', quality=c)\n    x = PILImage.open(output)\n\n    return x\n\n\ndef pixelate(x, severity=1):\n    c = [0.95, 0.9, 0.85, 0.75, 0.65][severity - 1]\n\n    x = x.resize((int(32 * c), int(32 * c)), PILImage.BOX)\n    x = x.resize((32, 32), PILImage.BOX)\n\n    return x\n\n\n# mod of https://gist.github.com/erniejunior/601cdf56d2b424757de5\ndef elastic_transform(image, severity=1):\n    IMSIZE = 32\n    c = [(IMSIZE*0, IMSIZE*0, IMSIZE*0.08),\n         (IMSIZE*0.05, IMSIZE*0.2, IMSIZE*0.07),\n         (IMSIZE*0.08, IMSIZE*0.06, IMSIZE*0.06),\n         (IMSIZE*0.1, IMSIZE*0.04, IMSIZE*0.05),\n         (IMSIZE*0.1, IMSIZE*0.03, IMSIZE*0.03)][severity - 1]\n\n    image = np.array(image, dtype=np.float32) / 255.\n    shape = image.shape\n    shape_size = shape[:2]\n\n    # random affine\n    center_square = np.float32(shape_size) // 2\n    square_size = min(shape_size) // 3\n    pts1 = np.float32([center_square + square_size,\n                       [center_square[0] + square_size, center_square[1] - square_size],\n                       center_square - square_size])\n    pts2 = pts1 + np.random.uniform(-c[2], c[2], size=pts1.shape).astype(np.float32)\n    M = cv2.getAffineTransform(pts1, pts2)\n    image = cv2.warpAffine(image, M, shape_size[::-1], borderMode=cv2.BORDER_REFLECT_101)\n\n    dx = (gaussian(np.random.uniform(-1, 1, size=shape[:2]),\n                   c[1], mode=\'reflect\', truncate=3) * c[0]).astype(np.float32)\n    dy = (gaussian(np.random.uniform(-1, 1, size=shape[:2]),\n                   c[1], mode=\'reflect\', truncate=3) * c[0]).astype(np.float32)\n    dx, dy = dx[..., np.newaxis], dy[..., np.newaxis]\n\n    x, y, z = np.meshgrid(np.arange(shape[1]), np.arange(shape[0]), np.arange(shape[2]))\n    indices = np.reshape(y + dy, (-1, 1)), np.reshape(x + dx, (-1, 1)), np.reshape(z, (-1, 1))\n    return np.clip(map_coordinates(image, indices, order=1, mode=\'reflect\').reshape(shape), 0, 1) * 255\n\n\n# /////////////// End Distortions ///////////////\n\nimport collections\n\nprint(\'Using CIFAR-10 data\')\n\nd = collections.OrderedDict()\nd[\'Gaussian Noise\'] = gaussian_noise\nd[\'Shot Noise\'] = shot_noise\nd[\'Impulse Noise\'] = impulse_noise\nd[\'Defocus Blur\'] = defocus_blur\nd[\'Glass Blur\'] = glass_blur\nd[\'Motion Blur\'] = motion_blur\nd[\'Zoom Blur\'] = zoom_blur\nd[\'Snow\'] = snow\nd[\'Frost\'] = frost\nd[\'Fog\'] = fog\nd[\'Brightness\'] = brightness\nd[\'Contrast\'] = contrast\nd[\'Elastic\'] = elastic_transform\nd[\'Pixelate\'] = pixelate\nd[\'JPEG\'] = jpeg_compression\n\nd[\'Speckle Noise\'] = speckle_noise\nd[\'Gaussian Blur\'] = gaussian_blur\nd[\'Spatter\'] = spatter\nd[\'Saturate\'] = saturate\n\n\ntest_data = dset.CIFAR10(\'/share/data/vision-greg/cifarpy\', train=False)\nconvert_img = trn.Compose([trn.ToTensor(), trn.ToPILImage()])\n\n\nfor method_name in d.keys():\n    print(\'Creating images for the corruption\', method_name)\n    cifar_c, labels = [], []\n\n    for severity in range(1,6):\n        corruption = lambda clean_img: d[method_name](clean_img, severity)\n\n        for img, label in zip(test_data.data, test_data.targets):\n            labels.append(label)\n            cifar_c.append(np.uint8(corruption(convert_img(img))))\n\n    np.save(\'/share/data/vision-greg2/users/dan/datasets/CIFAR-10-C/\' + d[method_name].__name__ + \'.npy\',\n            np.array(cifar_c).astype(np.uint8))\n\n    np.save(\'/share/data/vision-greg2/users/dan/datasets/CIFAR-10-C/labels.npy\',\n            np.array(labels).astype(np.uint8))\n\n'"
ImageNet-C/create_c/make_imagenet_64_c.py,2,"b'# -*- coding: utf-8 -*-\n\nimport os\nfrom PIL import Image\nimport os.path\nimport time\nimport torch\nimport torchvision.datasets as dset\nimport torchvision.transforms as trn\nimport torch.utils.data as data\nimport numpy as np\n\nfrom PIL import Image\n\n# /////////////// Data Loader ///////////////\n\n\nIMG_EXTENSIONS = [\'.jpg\', \'.jpeg\', \'.png\', \'.ppm\', \'.bmp\', \'.pgm\']\n\n\ndef is_image_file(filename):\n    """"""Checks if a file is an image.\n    Args:\n        filename (string): path to a file\n    Returns:\n        bool: True if the filename ends with a known image extension\n    """"""\n    filename_lower = filename.lower()\n    return any(filename_lower.endswith(ext) for ext in IMG_EXTENSIONS)\n\n\ndef find_classes(dir):\n    classes = [d for d in os.listdir(dir) if os.path.isdir(os.path.join(dir, d))]\n    classes.sort()\n    class_to_idx = {classes[i]: i for i in range(len(classes))}\n    return classes, class_to_idx\n\n\ndef make_dataset(dir, class_to_idx):\n    images = []\n    dir = os.path.expanduser(dir)\n    for target in sorted(os.listdir(dir)):\n        d = os.path.join(dir, target)\n        if not os.path.isdir(d):\n            continue\n\n        for root, _, fnames in sorted(os.walk(d)):\n            for fname in sorted(fnames):\n                if is_image_file(fname):\n                    path = os.path.join(root, fname)\n                    item = (path, class_to_idx[target])\n                    images.append(item)\n\n    return images\n\n\ndef pil_loader(path):\n    # open path as file to avoid ResourceWarning (https://github.com/python-pillow/Pillow/issues/835)\n    with open(path, \'rb\') as f:\n        img = Image.open(f)\n        return img.convert(\'RGB\')\n\n\ndef accimage_loader(path):\n    import accimage\n    try:\n        return accimage.Image(path)\n    except IOError:\n        # Potentially a decoding problem, fall back to PIL.Image\n        return pil_loader(path)\n\n\ndef default_loader(path):\n    from torchvision import get_image_backend\n    if get_image_backend() == \'accimage\':\n        return accimage_loader(path)\n    else:\n        return pil_loader(path)\n\n\nclass DistortImageFolder(data.Dataset):\n    def __init__(self, root, method, severity, transform=None, target_transform=None,\n                 loader=default_loader):\n        classes, class_to_idx = find_classes(root)\n        imgs = make_dataset(root, class_to_idx)\n        if len(imgs) == 0:\n            raise (RuntimeError(""Found 0 images in subfolders of: "" + root + ""\\n"" +\n                                ""Supported image extensions are: "" + "","".join(IMG_EXTENSIONS)))\n\n        self.root = root\n        self.method = method\n        self.severity = severity\n        self.imgs = imgs\n        self.classes = classes\n        self.class_to_idx = class_to_idx\n        self.idx_to_class = {v: k for k, v in class_to_idx.items()}\n        self.transform = transform\n        self.target_transform = target_transform\n        self.loader = loader\n\n    def __getitem__(self, index):\n        path, target = self.imgs[index]\n        img = self.loader(path)\n        if self.transform is not None:\n            img = self.transform(img)\n            img = self.method(img, self.severity)\n        if self.target_transform is not None:\n            target = self.target_transform(target)\n\n        save_path = \'/share/data/lang/users/dan/ImageNet-64x64-C/\' + self.method.__name__ + \\\n                    \'/\' + str(self.severity) + \'/\' + self.idx_to_class[target]\n\n        if not os.path.exists(save_path):\n            os.makedirs(save_path)\n\n        save_path += path[path.rindex(\'/\'):]\n\n        Image.fromarray(np.uint8(img)).save(save_path, quality=85, optimize=True)\n\n        return 0  # we do not care about returning the data\n\n    def __len__(self):\n        return len(self.imgs)\n\n\n# /////////////// Distortion Helpers ///////////////\n\nimport skimage as sk\nfrom skimage.filters import gaussian\nfrom io import BytesIO\nfrom wand.image import Image as WandImage\nfrom wand.api import library as wandlibrary\nimport wand.color as WandColor\nimport ctypes\nfrom PIL import Image as PILImage\nimport cv2\nfrom scipy.ndimage import zoom as scizoom\nfrom scipy.ndimage.interpolation import map_coordinates\nimport warnings\n\nwarnings.simplefilter(""ignore"", UserWarning)\n\n\ndef auc(errs):  # area under the alteration error curve\n    area = 0\n    for i in range(1, len(errs)):\n        area += (errs[i] + errs[i - 1]) / 2\n    area /= len(errs) - 1\n    return area\n\n\ndef disk(radius, alias_blur=0.1, dtype=np.float32):\n    if radius <= 8:\n        L = np.arange(-8, 8 + 1)\n        ksize = (3, 3)\n    else:\n        L = np.arange(-radius, radius + 1)\n        ksize = (5, 5)\n    X, Y = np.meshgrid(L, L)\n    aliased_disk = np.array((X ** 2 + Y ** 2) <= radius ** 2, dtype=dtype)\n    aliased_disk /= np.sum(aliased_disk)\n\n    # supersample disk to antialias\n    return cv2.GaussianBlur(aliased_disk, ksize=ksize, sigmaX=alias_blur)\n\n\n# Tell Python about the C method\nwandlibrary.MagickMotionBlurImage.argtypes = (ctypes.c_void_p,  # wand\n                                              ctypes.c_double,  # radius\n                                              ctypes.c_double,  # sigma\n                                              ctypes.c_double)  # angle\n\n\n# Extend wand.image.Image class to include method signature\nclass MotionImage(WandImage):\n    def motion_blur(self, radius=0.0, sigma=0.0, angle=0.0):\n        wandlibrary.MagickMotionBlurImage(self.wand, radius, sigma, angle)\n\n\n# modification of https://github.com/FLHerne/mapgen/blob/master/diamondsquare.py\ndef plasma_fractal(mapsize=64, wibbledecay=3):\n    """"""\n    Generate a heightmap using diamond-square algorithm.\n    Return square 2d array, side length \'mapsize\', of floats in range 0-255.\n    \'mapsize\' must be a power of two.\n    """"""\n    assert (mapsize & (mapsize - 1) == 0)\n    maparray = np.empty((mapsize, mapsize), dtype=np.float_)\n    maparray[0, 0] = 0\n    stepsize = mapsize\n    wibble = 100\n\n    def wibbledmean(array):\n        return array / 4 + wibble * np.random.uniform(-wibble, wibble, array.shape)\n\n    def fillsquares():\n        """"""For each square of points stepsize apart,\n           calculate middle value as mean of points + wibble""""""\n        cornerref = maparray[0:mapsize:stepsize, 0:mapsize:stepsize]\n        squareaccum = cornerref + np.roll(cornerref, shift=-1, axis=0)\n        squareaccum += np.roll(squareaccum, shift=-1, axis=1)\n        maparray[stepsize // 2:mapsize:stepsize,\n        stepsize // 2:mapsize:stepsize] = wibbledmean(squareaccum)\n\n    def filldiamonds():\n        """"""For each diamond of points stepsize apart,\n           calculate middle value as mean of points + wibble""""""\n        mapsize = maparray.shape[0]\n        drgrid = maparray[stepsize // 2:mapsize:stepsize, stepsize // 2:mapsize:stepsize]\n        ulgrid = maparray[0:mapsize:stepsize, 0:mapsize:stepsize]\n        ldrsum = drgrid + np.roll(drgrid, 1, axis=0)\n        lulsum = ulgrid + np.roll(ulgrid, -1, axis=1)\n        ltsum = ldrsum + lulsum\n        maparray[0:mapsize:stepsize, stepsize // 2:mapsize:stepsize] = wibbledmean(ltsum)\n        tdrsum = drgrid + np.roll(drgrid, 1, axis=1)\n        tulsum = ulgrid + np.roll(ulgrid, -1, axis=0)\n        ttsum = tdrsum + tulsum\n        maparray[stepsize // 2:mapsize:stepsize, 0:mapsize:stepsize] = wibbledmean(ttsum)\n\n    while stepsize >= 2:\n        fillsquares()\n        filldiamonds()\n        stepsize //= 2\n        wibble /= wibbledecay\n\n    maparray -= maparray.min()\n    return maparray / maparray.max()\n\n\ndef clipped_zoom(img, zoom_factor):\n    h = img.shape[0]\n    # ceil crop height(= crop width)\n    ch = int(np.ceil(h / zoom_factor))\n\n    top = (h - ch) // 2\n    img = scizoom(img[top:top + ch, top:top + ch], (zoom_factor, zoom_factor, 1), order=1)\n    # trim off any extra pixels\n    trim_top = (img.shape[0] - h) // 2\n\n    return img[trim_top:trim_top + h, trim_top:trim_top + h]\n\n\n# /////////////// End Distortion Helpers ///////////////\n\n\n# /////////////// Distortions ///////////////\n\ndef gaussian_noise(x, severity=1):\n    c = [0.04, 0.08, .12, .15, .18][severity - 1]\n\n    x = np.array(x) / 255.\n    return np.clip(x + np.random.normal(size=x.shape, scale=c), 0, 1) * 255\n\n\ndef shot_noise(x, severity=1):\n    c = [250, 100, 50, 30, 15][severity - 1]\n\n    x = np.array(x) / 255.\n    return np.clip(np.random.poisson(x * c) / c, 0, 1) * 255\n\n\ndef impulse_noise(x, severity=1):\n    c = [.01, .02, .05, .08, .14][severity - 1]\n\n    x = sk.util.random_noise(np.array(x) / 255., mode=\'s&p\', amount=c)\n    return np.clip(x, 0, 1) * 255\n\n\ndef speckle_noise(x, severity=1):\n    c = [.15, .2, 0.25, 0.3, 0.35][severity - 1]\n\n    x = np.array(x) / 255.\n    return np.clip(x + x * np.random.normal(size=x.shape, scale=c), 0, 1) * 255\n\n\ndef gaussian_blur(x, severity=1):\n    c = [.5, .75, 1, 1.25, 1.5][severity - 1]\n\n    x = gaussian(np.array(x) / 255., sigma=c, multichannel=True)\n    return np.clip(x, 0, 1) * 255\n\n\ndef glass_blur(x, severity=1):\n    # sigma, max_delta, iterations\n    c = [(0.1,1,1), (0.5,1,1), (0.6,1,2), (0.7,2,1), (0.9,2,2)][severity - 1]\n\n    x = np.uint8(gaussian(np.array(x) / 255., sigma=c[0], multichannel=True) * 255)\n\n    # locally shuffle pixels\n    for i in range(c[2]):\n        for h in range(64 - c[1], c[1], -1):\n            for w in range(64 - c[1], c[1], -1):\n                dx, dy = np.random.randint(-c[1], c[1], size=(2,))\n                h_prime, w_prime = h + dy, w + dx\n                # swap\n                x[h, w], x[h_prime, w_prime] = x[h_prime, w_prime], x[h, w]\n\n    return np.clip(gaussian(x / 255., sigma=c[0], multichannel=True), 0, 1) * 255\n\n\ndef defocus_blur(x, severity=1):\n    c = [(0.5, 0.6), (1, 0.1), (1.5, 0.1), (2.5, 0.01), (3, 0.1)][severity - 1]\n\n    x = np.array(x) / 255.\n    kernel = disk(radius=c[0], alias_blur=c[1])\n\n    channels = []\n    for d in range(3):\n        channels.append(cv2.filter2D(x[:, :, d], -1, kernel))\n    channels = np.array(channels).transpose((1, 2, 0))  # 3x64x64 -> 64x64x3\n\n    return np.clip(channels, 0, 1) * 255\n\n\ndef motion_blur(x, severity=1):\n    c = [(10,1), (10,1.5), (10,2), (10,2.5), (12,3)][severity - 1]\n\n    output = BytesIO()\n    x.save(output, format=\'PNG\')\n    x = MotionImage(blob=output.getvalue())\n\n    x.motion_blur(radius=c[0], sigma=c[1], angle=np.random.uniform(-45, 45))\n\n    x = cv2.imdecode(np.fromstring(x.make_blob(), np.uint8),\n                     cv2.IMREAD_UNCHANGED)\n\n    if x.shape != (64, 64):\n        return np.clip(x[..., [2, 1, 0]], 0, 255)  # BGR to RGB\n    else:  # greyscale to RGB\n        return np.clip(np.array([x, x, x]).transpose((1, 2, 0)), 0, 255)\n\n\ndef zoom_blur(x, severity=1):\n    c = [np.arange(1, 1.06, 0.01), np.arange(1, 1.11, 0.01), np.arange(1, 1.16, 0.01),\n         np.arange(1, 1.21, 0.01), np.arange(1, 1.26, 0.01)][severity - 1]\n\n    x = (np.array(x) / 255.).astype(np.float32)\n    out = np.zeros_like(x)\n    for zoom_factor in c:\n        out += clipped_zoom(x, zoom_factor)\n\n    x = (x + out) / (len(c) + 1)\n    return np.clip(x, 0, 1) * 255\n\n\ndef fog(x, severity=1):\n    c = [(.4,3), (.7,3), (1,2.5), (1.5,2), (2,1.75)][severity - 1]\n\n    x = np.array(x) / 255.\n    max_val = x.max()\n    x += c[0] * plasma_fractal(wibbledecay=c[1])[:64, :64][..., np.newaxis]\n    return np.clip(x * max_val / (max_val + c[0]), 0, 1) * 255\n\n\ndef frost(x, severity=1):\n    c = [(1, 0.3), (0.9, 0.4), (0.8, 0.45), (0.75, 0.5), (0.7, 0.55)][severity - 1]\n    idx = np.random.randint(5)\n    filename = [\'./frost1.png\', \'./frost2.png\', \'./frost3.png\', \'./frost4.jpg\', \'./frost5.jpg\', \'./frost6.jpg\'][idx]\n    frost = cv2.imread(filename)\n    frost = cv2.resize(frost, (0, 0), fx=0.3, fy=0.3)\n    # randomly crop and convert to rgb\n    x_start, y_start = np.random.randint(0, frost.shape[0] - 64), np.random.randint(0, frost.shape[1] - 64)\n    frost = frost[x_start:x_start + 64, y_start:y_start + 64][..., [2, 1, 0]]\n\n    return np.clip(c[0] * np.array(x) + c[1] * frost, 0, 255)\n\n\ndef snow(x, severity=1):\n    c = [(0.1,0.2,1,0.6,8,3,0.8),\n         (0.1,0.2,1,0.5,10,4,0.8),\n         (0.15,0.3,1.75,0.55,10,4,0.7),\n         (0.25,0.3,2.25,0.6,12,6,0.65),\n         (0.3,0.3,1.25,0.65,14,12,0.6)][severity - 1]\n\n    x = np.array(x, dtype=np.float32) / 255.\n    snow_layer = np.random.normal(size=x.shape[:2], loc=c[0], scale=c[1])  # [:2] for monochrome\n\n    snow_layer = clipped_zoom(snow_layer[..., np.newaxis], c[2])\n    snow_layer[snow_layer < c[3]] = 0\n\n    snow_layer = PILImage.fromarray((np.clip(snow_layer.squeeze(), 0, 1) * 255).astype(np.uint8), mode=\'L\')\n    output = BytesIO()\n    snow_layer.save(output, format=\'PNG\')\n    snow_layer = MotionImage(blob=output.getvalue())\n\n    snow_layer.motion_blur(radius=c[4], sigma=c[5], angle=np.random.uniform(-135, -45))\n\n    snow_layer = cv2.imdecode(np.fromstring(snow_layer.make_blob(), np.uint8),\n                              cv2.IMREAD_UNCHANGED) / 255.\n    snow_layer = snow_layer[..., np.newaxis]\n\n    x = c[6] * x + (1 - c[6]) * np.maximum(x, cv2.cvtColor(x, cv2.COLOR_RGB2GRAY).reshape(64, 64, 1) * 1.5 + 0.5)\n    return np.clip(x + snow_layer + np.rot90(snow_layer, k=2), 0, 1) * 255\n\n\ndef spatter(x, severity=1):\n    c = [(0.62,0.1,0.7,0.7,0.6,0),\n         (0.65,0.1,0.8,0.7,0.6,0),\n         (0.65,0.3,1,0.69,0.6,0),\n         (0.65,0.1,0.7,0.68,0.6,1),\n         (0.65,0.1,0.5,0.67,0.6,1)][severity - 1]\n    x = np.array(x, dtype=np.float32) / 255.\n\n    liquid_layer = np.random.normal(size=x.shape[:2], loc=c[0], scale=c[1])\n\n    liquid_layer = gaussian(liquid_layer, sigma=c[2])\n    liquid_layer[liquid_layer < c[3]] = 0\n    if c[5] == 0:\n        liquid_layer = (liquid_layer * 255).astype(np.uint8)\n        dist = 255 - cv2.Canny(liquid_layer, 50, 150)\n        dist = cv2.distanceTransform(dist, cv2.DIST_L2, 5)\n        _, dist = cv2.threshold(dist, 20, 20, cv2.THRESH_TRUNC)\n        dist = cv2.blur(dist, (3, 3)).astype(np.uint8)\n        dist = cv2.equalizeHist(dist)\n        #     ker = np.array([[-1,-2,-3],[-2,0,0],[-3,0,1]], dtype=np.float32)\n        #     ker -= np.mean(ker)\n        ker = np.array([[-2, -1, 0], [-1, 1, 1], [0, 1, 2]])\n        dist = cv2.filter2D(dist, cv2.CV_8U, ker)\n        dist = cv2.blur(dist, (3, 3)).astype(np.float32)\n\n        m = cv2.cvtColor(liquid_layer * dist, cv2.COLOR_GRAY2BGRA)\n        m /= np.max(m, axis=(0, 1))\n        m *= c[4]\n\n        # water is pale turqouise\n        color = np.concatenate((175 / 255. * np.ones_like(m[..., :1]),\n                                238 / 255. * np.ones_like(m[..., :1]),\n                                238 / 255. * np.ones_like(m[..., :1])), axis=2)\n\n        color = cv2.cvtColor(color, cv2.COLOR_BGR2BGRA)\n        x = cv2.cvtColor(x, cv2.COLOR_BGR2BGRA)\n\n        return cv2.cvtColor(np.clip(x + m * color, 0, 1), cv2.COLOR_BGRA2BGR) * 255\n    else:\n        m = np.where(liquid_layer > c[3], 1, 0)\n        m = gaussian(m.astype(np.float32), sigma=c[4])\n        m[m < 0.8] = 0\n        #         m = np.abs(m) ** (1/c[4])\n\n        # mud brown\n        color = np.concatenate((63 / 255. * np.ones_like(x[..., :1]),\n                                42 / 255. * np.ones_like(x[..., :1]),\n                                20 / 255. * np.ones_like(x[..., :1])), axis=2)\n\n        color *= m[..., np.newaxis]\n        x *= (1 - m[..., np.newaxis])\n\n        return np.clip(x + color, 0, 1) * 255\n\n\ndef contrast(x, severity=1):\n    c = [.4, .3, .2, .1, 0.05][severity - 1]\n\n    x = np.array(x) / 255.\n    means = np.mean(x, axis=(0, 1), keepdims=True)\n    return np.clip((x - means) * c + means, 0, 1) * 255\n\n\ndef brightness(x, severity=1):\n    c = [.1, .2, .3, .4, .5][severity - 1]\n\n    x = np.array(x) / 255.\n    x = sk.color.rgb2hsv(x)\n    x[:, :, 2] = np.clip(x[:, :, 2] + c, 0, 1)\n    x = sk.color.hsv2rgb(x)\n\n    return np.clip(x, 0, 1) * 255\n\n\ndef saturate(x, severity=1):\n    c = [(0.3, 0), (0.1, 0), (2, 0), (5, 0.1), (30, 0.2)][severity - 1]\n\n    x = np.array(x) / 255.\n    x = sk.color.rgb2hsv(x)\n    x[:, :, 1] = np.clip(x[:, :, 1] * c[0] + c[1], 0, 1)\n    x = sk.color.hsv2rgb(x)\n\n    return np.clip(x, 0, 1) * 255\n\n\ndef jpeg_compression(x, severity=1):\n    c = [65, 58, 50, 40, 25][severity - 1]\n\n    output = BytesIO()\n    x.save(output, \'JPEG\', quality=c)\n    x = PILImage.open(output)\n\n    return x\n\n\ndef pixelate(x, severity=1):\n    c = [0.9, 0.8, 0.7, 0.6, 0.5][severity - 1]\n\n    x = x.resize((int(64 * c), int(64 * c)), PILImage.BOX)\n    x = x.resize((64, 64), PILImage.BOX)\n\n    return x\n\n\n# mod of https://gist.github.com/erniejunior/601cdf56d2b424757de5\ndef elastic_transform(image, severity=1):\n    IMSIZE = 64\n    c = [(IMSIZE*0, IMSIZE*0, IMSIZE*0.08),\n         (IMSIZE*0.05, IMSIZE*0.3, IMSIZE*0.06),\n         (IMSIZE*0.1, IMSIZE*0.08, IMSIZE*0.06),\n         (IMSIZE*0.1, IMSIZE*0.03, IMSIZE*0.03),\n         (IMSIZE*0.16, IMSIZE*0.03, IMSIZE*0.02)][severity - 1]\n\n    image = np.array(image, dtype=np.float32) / 255.\n    shape = image.shape\n    shape_size = shape[:2]\n\n    # random affine\n    center_square = np.float32(shape_size) // 2\n    square_size = min(shape_size) // 3\n    pts1 = np.float32([center_square + square_size,\n                       [center_square[0] + square_size, center_square[1] - square_size],\n                       center_square - square_size])\n    pts2 = pts1 + np.random.uniform(-c[2], c[2], size=pts1.shape).astype(np.float32)\n    M = cv2.getAffineTransform(pts1, pts2)\n    image = cv2.warpAffine(image, M, shape_size[::-1], borderMode=cv2.BORDER_REFLECT_101)\n\n    dx = (gaussian(np.random.uniform(-1, 1, size=shape[:2]),\n                   c[1], mode=\'reflect\', truncate=3) * c[0]).astype(np.float32)\n    dy = (gaussian(np.random.uniform(-1, 1, size=shape[:2]),\n                   c[1], mode=\'reflect\', truncate=3) * c[0]).astype(np.float32)\n    dx, dy = dx[..., np.newaxis], dy[..., np.newaxis]\n\n    x, y, z = np.meshgrid(np.arange(shape[1]), np.arange(shape[0]), np.arange(shape[2]))\n    indices = np.reshape(y + dy, (-1, 1)), np.reshape(x + dx, (-1, 1)), np.reshape(z, (-1, 1))\n    return np.clip(map_coordinates(image, indices, order=1, mode=\'reflect\').reshape(shape), 0, 1) * 255\n\n\n# /////////////// End Distortions ///////////////\n\n\n# /////////////// Further Setup ///////////////\n\n\ndef save_distorted(method=gaussian_noise):\n    for severity in range(1, 6):\n        print(method.__name__, severity)\n        distorted_dataset = DistortImageFolder(\n            root=""/share/data/vision-greg/ImageNet/clsloc/images/val"",\n            method=method, severity=severity,\n            transform=trn.Compose([trn.Resize((64, 64))]))\n        distorted_dataset_loader = torch.utils.data.DataLoader(\n            distorted_dataset, batch_size=100, shuffle=False, num_workers=6)\n\n        for _ in distorted_dataset_loader: continue\n\n\n# /////////////// End Further Setup ///////////////\n\n\n# /////////////// Display Results ///////////////\nimport collections\n\nprint(\'\\nUsing ImageNet data\')\n\nd = collections.OrderedDict()\n# d[\'Gaussian Noise\'] = gaussian_noise\n# d[\'Shot Noise\'] = shot_noise\n# d[\'Impulse Noise\'] = impulse_noise\n# d[\'Defocus Blur\'] = defocus_blur\n# d[\'Glass Blur\'] = glass_blur\n# d[\'Motion Blur\'] = motion_blur\n# d[\'Zoom Blur\'] = zoom_blur\n# d[\'Snow\'] = snow\nd[\'Frost\'] = frost\nd[\'Fog\'] = fog\nd[\'Brightness\'] = brightness\nd[\'Contrast\'] = contrast\n# d[\'Elastic\'] = elastic_transform\n# d[\'Pixelate\'] = pixelate\n# d[\'JPEG\'] = jpeg_compression\n#\n# d[\'Speckle Noise\'] = speckle_noise\n# d[\'Gaussian Blur\'] = gaussian_blur\n# d[\'Spatter\'] = spatter\n# d[\'Saturate\'] = saturate\n\nfor method_name in d.keys():\n    save_distorted(d[method_name])\n'"
ImageNet-C/create_c/make_imagenet_c.py,3,"b'# -*- coding: utf-8 -*-\n\nimport os\nfrom PIL import Image\nimport os.path\nimport time\nimport torch\nimport torchvision.datasets as dset\nimport torchvision.transforms as trn\nimport torch.utils.data as data\nimport numpy as np\n\nfrom PIL import Image\n\n# /////////////// Data Loader ///////////////\n\n\nIMG_EXTENSIONS = [\'.jpg\', \'.jpeg\', \'.png\', \'.ppm\', \'.bmp\', \'.pgm\']\n\n\ndef is_image_file(filename):\n    """"""Checks if a file is an image.\n    Args:\n        filename (string): path to a file\n    Returns:\n        bool: True if the filename ends with a known image extension\n    """"""\n    filename_lower = filename.lower()\n    return any(filename_lower.endswith(ext) for ext in IMG_EXTENSIONS)\n\n\ndef find_classes(dir):\n    classes = [d for d in os.listdir(dir) if os.path.isdir(os.path.join(dir, d))]\n    classes.sort()\n    class_to_idx = {classes[i]: i for i in range(len(classes))}\n    return classes, class_to_idx\n\n\ndef make_dataset(dir, class_to_idx):\n    images = []\n    dir = os.path.expanduser(dir)\n    for target in sorted(os.listdir(dir)):\n        d = os.path.join(dir, target)\n        if not os.path.isdir(d):\n            continue\n\n        for root, _, fnames in sorted(os.walk(d)):\n            for fname in sorted(fnames):\n                if is_image_file(fname):\n                    path = os.path.join(root, fname)\n                    item = (path, class_to_idx[target])\n                    images.append(item)\n\n    return images\n\n\ndef pil_loader(path):\n    # open path as file to avoid ResourceWarning (https://github.com/python-pillow/Pillow/issues/835)\n    with open(path, \'rb\') as f:\n        img = Image.open(f)\n        return img.convert(\'RGB\')\n\n\ndef accimage_loader(path):\n    import accimage\n    try:\n        return accimage.Image(path)\n    except IOError:\n        # Potentially a decoding problem, fall back to PIL.Image\n        return pil_loader(path)\n\n\ndef default_loader(path):\n    from torchvision import get_image_backend\n    if get_image_backend() == \'accimage\':\n        return accimage_loader(path)\n    else:\n        return pil_loader(path)\n\n\nclass DistortImageFolder(data.Dataset):\n    def __init__(self, root, method, severity, transform=None, target_transform=None,\n                 loader=default_loader):\n        classes, class_to_idx = find_classes(root)\n        imgs = make_dataset(root, class_to_idx)\n        if len(imgs) == 0:\n            raise (RuntimeError(""Found 0 images in subfolders of: "" + root + ""\\n""\n                                                                             ""Supported image extensions are: "" + "","".join(\n                IMG_EXTENSIONS)))\n\n        self.root = root\n        self.method = method\n        self.severity = severity\n        self.imgs = imgs\n        self.classes = classes\n        self.class_to_idx = class_to_idx\n        self.idx_to_class = {v: k for k, v in class_to_idx.items()}\n        self.transform = transform\n        self.target_transform = target_transform\n        self.loader = loader\n\n    def __getitem__(self, index):\n        path, target = self.imgs[index]\n        img = self.loader(path)\n        if self.transform is not None:\n            img = self.transform(img)\n            img = self.method(img, self.severity)\n        if self.target_transform is not None:\n            target = self.target_transform(target)\n\n        save_path = \'/share/data/vision-greg/DistortedImageNet/JPEG/\' + self.method.__name__ + \\\n                    \'/\' + str(self.severity) + \'/\' + self.idx_to_class[target]\n\n        if not os.path.exists(save_path):\n            os.makedirs(save_path)\n\n        save_path += path[path.rindex(\'/\'):]\n\n        Image.fromarray(np.uint8(img)).save(save_path, quality=85, optimize=True)\n\n        return 0  # we do not care about returning the data\n\n    def __len__(self):\n        return len(self.imgs)\n\n\n# /////////////// Distortion Helpers ///////////////\n\nimport skimage as sk\nfrom skimage.filters import gaussian\nfrom io import BytesIO\nfrom wand.image import Image as WandImage\nfrom wand.api import library as wandlibrary\nimport wand.color as WandColor\nimport ctypes\nfrom PIL import Image as PILImage\nimport cv2\nfrom scipy.ndimage import zoom as scizoom\nfrom scipy.ndimage.interpolation import map_coordinates\nimport warnings\n\nwarnings.simplefilter(""ignore"", UserWarning)\n\n\ndef auc(errs):  # area under the alteration error curve\n    area = 0\n    for i in range(1, len(errs)):\n        area += (errs[i] + errs[i - 1]) / 2\n    area /= len(errs) - 1\n    return area\n\n\ndef disk(radius, alias_blur=0.1, dtype=np.float32):\n    if radius <= 8:\n        L = np.arange(-8, 8 + 1)\n        ksize = (3, 3)\n    else:\n        L = np.arange(-radius, radius + 1)\n        ksize = (5, 5)\n    X, Y = np.meshgrid(L, L)\n    aliased_disk = np.array((X ** 2 + Y ** 2) <= radius ** 2, dtype=dtype)\n    aliased_disk /= np.sum(aliased_disk)\n\n    # supersample disk to antialias\n    return cv2.GaussianBlur(aliased_disk, ksize=ksize, sigmaX=alias_blur)\n\n\n# Tell Python about the C method\nwandlibrary.MagickMotionBlurImage.argtypes = (ctypes.c_void_p,  # wand\n                                              ctypes.c_double,  # radius\n                                              ctypes.c_double,  # sigma\n                                              ctypes.c_double)  # angle\n\n\n# Extend wand.image.Image class to include method signature\nclass MotionImage(WandImage):\n    def motion_blur(self, radius=0.0, sigma=0.0, angle=0.0):\n        wandlibrary.MagickMotionBlurImage(self.wand, radius, sigma, angle)\n\n\n# modification of https://github.com/FLHerne/mapgen/blob/master/diamondsquare.py\ndef plasma_fractal(mapsize=256, wibbledecay=3):\n    """"""\n    Generate a heightmap using diamond-square algorithm.\n    Return square 2d array, side length \'mapsize\', of floats in range 0-255.\n    \'mapsize\' must be a power of two.\n    """"""\n    assert (mapsize & (mapsize - 1) == 0)\n    maparray = np.empty((mapsize, mapsize), dtype=np.float_)\n    maparray[0, 0] = 0\n    stepsize = mapsize\n    wibble = 100\n\n    def wibbledmean(array):\n        return array / 4 + wibble * np.random.uniform(-wibble, wibble, array.shape)\n\n    def fillsquares():\n        """"""For each square of points stepsize apart,\n           calculate middle value as mean of points + wibble""""""\n        cornerref = maparray[0:mapsize:stepsize, 0:mapsize:stepsize]\n        squareaccum = cornerref + np.roll(cornerref, shift=-1, axis=0)\n        squareaccum += np.roll(squareaccum, shift=-1, axis=1)\n        maparray[stepsize // 2:mapsize:stepsize,\n        stepsize // 2:mapsize:stepsize] = wibbledmean(squareaccum)\n\n    def filldiamonds():\n        """"""For each diamond of points stepsize apart,\n           calculate middle value as mean of points + wibble""""""\n        mapsize = maparray.shape[0]\n        drgrid = maparray[stepsize // 2:mapsize:stepsize, stepsize // 2:mapsize:stepsize]\n        ulgrid = maparray[0:mapsize:stepsize, 0:mapsize:stepsize]\n        ldrsum = drgrid + np.roll(drgrid, 1, axis=0)\n        lulsum = ulgrid + np.roll(ulgrid, -1, axis=1)\n        ltsum = ldrsum + lulsum\n        maparray[0:mapsize:stepsize, stepsize // 2:mapsize:stepsize] = wibbledmean(ltsum)\n        tdrsum = drgrid + np.roll(drgrid, 1, axis=1)\n        tulsum = ulgrid + np.roll(ulgrid, -1, axis=0)\n        ttsum = tdrsum + tulsum\n        maparray[stepsize // 2:mapsize:stepsize, 0:mapsize:stepsize] = wibbledmean(ttsum)\n\n    while stepsize >= 2:\n        fillsquares()\n        filldiamonds()\n        stepsize //= 2\n        wibble /= wibbledecay\n\n    maparray -= maparray.min()\n    return maparray / maparray.max()\n\n\ndef clipped_zoom(img, zoom_factor):\n    h = img.shape[0]\n    # ceil crop height(= crop width)\n    ch = int(np.ceil(h / zoom_factor))\n\n    top = (h - ch) // 2\n    img = scizoom(img[top:top + ch, top:top + ch], (zoom_factor, zoom_factor, 1), order=1)\n    # trim off any extra pixels\n    trim_top = (img.shape[0] - h) // 2\n\n    return img[trim_top:trim_top + h, trim_top:trim_top + h]\n\n\n# /////////////// End Distortion Helpers ///////////////\n\n\n# /////////////// Distortions ///////////////\n\ndef gaussian_noise(x, severity=1):\n    c = [.08, .12, 0.18, 0.26, 0.38][severity - 1]\n\n    x = np.array(x) / 255.\n    return np.clip(x + np.random.normal(size=x.shape, scale=c), 0, 1) * 255\n\n\ndef shot_noise(x, severity=1):\n    c = [60, 25, 12, 5, 3][severity - 1]\n\n    x = np.array(x) / 255.\n    return np.clip(np.random.poisson(x * c) / c, 0, 1) * 255\n\n\ndef impulse_noise(x, severity=1):\n    c = [.03, .06, .09, 0.17, 0.27][severity - 1]\n\n    x = sk.util.random_noise(np.array(x) / 255., mode=\'s&p\', amount=c)\n    return np.clip(x, 0, 1) * 255\n\n\ndef speckle_noise(x, severity=1):\n    c = [.15, .2, 0.35, 0.45, 0.6][severity - 1]\n\n    x = np.array(x) / 255.\n    return np.clip(x + x * np.random.normal(size=x.shape, scale=c), 0, 1) * 255\n\n\ndef fgsm(x, source_net, severity=1):\n    c = [8, 16, 32, 64, 128][severity - 1]\n\n    x = V(x, requires_grad=True)\n    logits = source_net(x)\n    source_net.zero_grad()\n    loss = F.cross_entropy(logits, V(logits.data.max(1)[1].squeeze_()), size_average=False)\n    loss.backward()\n\n    return standardize(torch.clamp(unstandardize(x.data) + c / 255. * unstandardize(torch.sign(x.grad.data)), 0, 1))\n\n\ndef gaussian_blur(x, severity=1):\n    c = [1, 2, 3, 4, 6][severity - 1]\n\n    x = gaussian(np.array(x) / 255., sigma=c, multichannel=True)\n    return np.clip(x, 0, 1) * 255\n\n\ndef glass_blur(x, severity=1):\n    # sigma, max_delta, iterations\n    c = [(0.7, 1, 2), (0.9, 2, 1), (1, 2, 3), (1.1, 3, 2), (1.5, 4, 2)][severity - 1]\n\n    x = np.uint8(gaussian(np.array(x) / 255., sigma=c[0], multichannel=True) * 255)\n\n    # locally shuffle pixels\n    for i in range(c[2]):\n        for h in range(224 - c[1], c[1], -1):\n            for w in range(224 - c[1], c[1], -1):\n                dx, dy = np.random.randint(-c[1], c[1], size=(2,))\n                h_prime, w_prime = h + dy, w + dx\n                # swap\n                x[h, w], x[h_prime, w_prime] = x[h_prime, w_prime], x[h, w]\n\n    return np.clip(gaussian(x / 255., sigma=c[0], multichannel=True), 0, 1) * 255\n\n\ndef defocus_blur(x, severity=1):\n    c = [(3, 0.1), (4, 0.5), (6, 0.5), (8, 0.5), (10, 0.5)][severity - 1]\n\n    x = np.array(x) / 255.\n    kernel = disk(radius=c[0], alias_blur=c[1])\n\n    channels = []\n    for d in range(3):\n        channels.append(cv2.filter2D(x[:, :, d], -1, kernel))\n    channels = np.array(channels).transpose((1, 2, 0))  # 3x224x224 -> 224x224x3\n\n    return np.clip(channels, 0, 1) * 255\n\n\ndef motion_blur(x, severity=1):\n    c = [(10, 3), (15, 5), (15, 8), (15, 12), (20, 15)][severity - 1]\n\n    output = BytesIO()\n    x.save(output, format=\'PNG\')\n    x = MotionImage(blob=output.getvalue())\n\n    x.motion_blur(radius=c[0], sigma=c[1], angle=np.random.uniform(-45, 45))\n\n    x = cv2.imdecode(np.fromstring(x.make_blob(), np.uint8),\n                     cv2.IMREAD_UNCHANGED)\n\n    if x.shape != (224, 224):\n        return np.clip(x[..., [2, 1, 0]], 0, 255)  # BGR to RGB\n    else:  # greyscale to RGB\n        return np.clip(np.array([x, x, x]).transpose((1, 2, 0)), 0, 255)\n\n\ndef zoom_blur(x, severity=1):\n    c = [np.arange(1, 1.11, 0.01),\n         np.arange(1, 1.16, 0.01),\n         np.arange(1, 1.21, 0.02),\n         np.arange(1, 1.26, 0.02),\n         np.arange(1, 1.31, 0.03)][severity - 1]\n\n    x = (np.array(x) / 255.).astype(np.float32)\n    out = np.zeros_like(x)\n    for zoom_factor in c:\n        out += clipped_zoom(x, zoom_factor)\n\n    x = (x + out) / (len(c) + 1)\n    return np.clip(x, 0, 1) * 255\n\n\n# def barrel(x, severity=1):\n#     c = [(0,0.03,0.03), (0.05,0.05,0.05), (0.1,0.1,0.1),\n#          (0.2,0.2,0.2), (0.1,0.3,0.6)][severity - 1]\n#\n#     output = BytesIO()\n#     x.save(output, format=\'PNG\')\n#\n#     x = WandImage(blob=output.getvalue())\n#     x.distort(\'barrel\', c)\n#\n#     x = cv2.imdecode(np.fromstring(x.make_blob(), np.uint8),\n#                      cv2.IMREAD_UNCHANGED)\n#\n#     if x.shape != (224, 224):\n#         return np.clip(x[..., [2, 1, 0]], 0, 255)  # BGR to RGB\n#     else:  # greyscale to RGB\n#         return np.clip(np.array([x, x, x]).transpose((1, 2, 0)), 0, 255)\n\n\ndef fog(x, severity=1):\n    c = [(1.5, 2), (2, 2), (2.5, 1.7), (2.5, 1.5), (3, 1.4)][severity - 1]\n\n    x = np.array(x) / 255.\n    max_val = x.max()\n    x += c[0] * plasma_fractal(wibbledecay=c[1])[:224, :224][..., np.newaxis]\n    return np.clip(x * max_val / (max_val + c[0]), 0, 1) * 255\n\n\ndef frost(x, severity=1):\n    c = [(1, 0.4),\n         (0.8, 0.6),\n         (0.7, 0.7),\n         (0.65, 0.7),\n         (0.6, 0.75)][severity - 1]\n    idx = np.random.randint(5)\n    filename = [\'./frost1.png\', \'./frost2.png\', \'./frost3.png\', \'./frost4.jpg\', \'./frost5.jpg\', \'./frost6.jpg\'][idx]\n    frost = cv2.imread(filename)\n    # randomly crop and convert to rgb\n    x_start, y_start = np.random.randint(0, frost.shape[0] - 224), np.random.randint(0, frost.shape[1] - 224)\n    frost = frost[x_start:x_start + 224, y_start:y_start + 224][..., [2, 1, 0]]\n\n    return np.clip(c[0] * np.array(x) + c[1] * frost, 0, 255)\n\n\ndef snow(x, severity=1):\n    c = [(0.1, 0.3, 3, 0.5, 10, 4, 0.8),\n         (0.2, 0.3, 2, 0.5, 12, 4, 0.7),\n         (0.55, 0.3, 4, 0.9, 12, 8, 0.7),\n         (0.55, 0.3, 4.5, 0.85, 12, 8, 0.65),\n         (0.55, 0.3, 2.5, 0.85, 12, 12, 0.55)][severity - 1]\n\n    x = np.array(x, dtype=np.float32) / 255.\n    snow_layer = np.random.normal(size=x.shape[:2], loc=c[0], scale=c[1])  # [:2] for monochrome\n\n    snow_layer = clipped_zoom(snow_layer[..., np.newaxis], c[2])\n    snow_layer[snow_layer < c[3]] = 0\n\n    snow_layer = PILImage.fromarray((np.clip(snow_layer.squeeze(), 0, 1) * 255).astype(np.uint8), mode=\'L\')\n    output = BytesIO()\n    snow_layer.save(output, format=\'PNG\')\n    snow_layer = MotionImage(blob=output.getvalue())\n\n    snow_layer.motion_blur(radius=c[4], sigma=c[5], angle=np.random.uniform(-135, -45))\n\n    snow_layer = cv2.imdecode(np.fromstring(snow_layer.make_blob(), np.uint8),\n                              cv2.IMREAD_UNCHANGED) / 255.\n    snow_layer = snow_layer[..., np.newaxis]\n\n    x = c[6] * x + (1 - c[6]) * np.maximum(x, cv2.cvtColor(x, cv2.COLOR_RGB2GRAY).reshape(224, 224, 1) * 1.5 + 0.5)\n    return np.clip(x + snow_layer + np.rot90(snow_layer, k=2), 0, 1) * 255\n\n\ndef spatter(x, severity=1):\n    c = [(0.65, 0.3, 4, 0.69, 0.6, 0),\n         (0.65, 0.3, 3, 0.68, 0.6, 0),\n         (0.65, 0.3, 2, 0.68, 0.5, 0),\n         (0.65, 0.3, 1, 0.65, 1.5, 1),\n         (0.67, 0.4, 1, 0.65, 1.5, 1)][severity - 1]\n    x = np.array(x, dtype=np.float32) / 255.\n\n    liquid_layer = np.random.normal(size=x.shape[:2], loc=c[0], scale=c[1])\n\n    liquid_layer = gaussian(liquid_layer, sigma=c[2])\n    liquid_layer[liquid_layer < c[3]] = 0\n    if c[5] == 0:\n        liquid_layer = (liquid_layer * 255).astype(np.uint8)\n        dist = 255 - cv2.Canny(liquid_layer, 50, 150)\n        dist = cv2.distanceTransform(dist, cv2.DIST_L2, 5)\n        _, dist = cv2.threshold(dist, 20, 20, cv2.THRESH_TRUNC)\n        dist = cv2.blur(dist, (3, 3)).astype(np.uint8)\n        dist = cv2.equalizeHist(dist)\n        #     ker = np.array([[-1,-2,-3],[-2,0,0],[-3,0,1]], dtype=np.float32)\n        #     ker -= np.mean(ker)\n        ker = np.array([[-2, -1, 0], [-1, 1, 1], [0, 1, 2]])\n        dist = cv2.filter2D(dist, cv2.CV_8U, ker)\n        dist = cv2.blur(dist, (3, 3)).astype(np.float32)\n\n        m = cv2.cvtColor(liquid_layer * dist, cv2.COLOR_GRAY2BGRA)\n        m /= np.max(m, axis=(0, 1))\n        m *= c[4]\n\n        # water is pale turqouise\n        color = np.concatenate((175 / 255. * np.ones_like(m[..., :1]),\n                                238 / 255. * np.ones_like(m[..., :1]),\n                                238 / 255. * np.ones_like(m[..., :1])), axis=2)\n\n        color = cv2.cvtColor(color, cv2.COLOR_BGR2BGRA)\n        x = cv2.cvtColor(x, cv2.COLOR_BGR2BGRA)\n\n        return cv2.cvtColor(np.clip(x + m * color, 0, 1), cv2.COLOR_BGRA2BGR) * 255\n    else:\n        m = np.where(liquid_layer > c[3], 1, 0)\n        m = gaussian(m.astype(np.float32), sigma=c[4])\n        m[m < 0.8] = 0\n        #         m = np.abs(m) ** (1/c[4])\n\n        # mud brown\n        color = np.concatenate((63 / 255. * np.ones_like(x[..., :1]),\n                                42 / 255. * np.ones_like(x[..., :1]),\n                                20 / 255. * np.ones_like(x[..., :1])), axis=2)\n\n        color *= m[..., np.newaxis]\n        x *= (1 - m[..., np.newaxis])\n\n        return np.clip(x + color, 0, 1) * 255\n\n\ndef contrast(x, severity=1):\n    c = [0.4, .3, .2, .1, .05][severity - 1]\n\n    x = np.array(x) / 255.\n    means = np.mean(x, axis=(0, 1), keepdims=True)\n    return np.clip((x - means) * c + means, 0, 1) * 255\n\n\ndef brightness(x, severity=1):\n    c = [.1, .2, .3, .4, .5][severity - 1]\n\n    x = np.array(x) / 255.\n    x = sk.color.rgb2hsv(x)\n    x[:, :, 2] = np.clip(x[:, :, 2] + c, 0, 1)\n    x = sk.color.hsv2rgb(x)\n\n    return np.clip(x, 0, 1) * 255\n\n\ndef saturate(x, severity=1):\n    c = [(0.3, 0), (0.1, 0), (2, 0), (5, 0.1), (20, 0.2)][severity - 1]\n\n    x = np.array(x) / 255.\n    x = sk.color.rgb2hsv(x)\n    x[:, :, 1] = np.clip(x[:, :, 1] * c[0] + c[1], 0, 1)\n    x = sk.color.hsv2rgb(x)\n\n    return np.clip(x, 0, 1) * 255\n\n\ndef jpeg_compression(x, severity=1):\n    c = [25, 18, 15, 10, 7][severity - 1]\n\n    output = BytesIO()\n    x.save(output, \'JPEG\', quality=c)\n    x = PILImage.open(output)\n\n    return x\n\n\ndef pixelate(x, severity=1):\n    c = [0.6, 0.5, 0.4, 0.3, 0.25][severity - 1]\n\n    x = x.resize((int(224 * c), int(224 * c)), PILImage.BOX)\n    x = x.resize((224, 224), PILImage.BOX)\n\n    return x\n\n\n# mod of https://gist.github.com/erniejunior/601cdf56d2b424757de5\ndef elastic_transform(image, severity=1):\n    c = [(244 * 2, 244 * 0.7, 244 * 0.1),   # 244 should have been 224, but ultimately nothing is incorrect\n         (244 * 2, 244 * 0.08, 244 * 0.2),\n         (244 * 0.05, 244 * 0.01, 244 * 0.02),\n         (244 * 0.07, 244 * 0.01, 244 * 0.02),\n         (244 * 0.12, 244 * 0.01, 244 * 0.02)][severity - 1]\n\n    image = np.array(image, dtype=np.float32) / 255.\n    shape = image.shape\n    shape_size = shape[:2]\n\n    # random affine\n    center_square = np.float32(shape_size) // 2\n    square_size = min(shape_size) // 3\n    pts1 = np.float32([center_square + square_size,\n                       [center_square[0] + square_size, center_square[1] - square_size],\n                       center_square - square_size])\n    pts2 = pts1 + np.random.uniform(-c[2], c[2], size=pts1.shape).astype(np.float32)\n    M = cv2.getAffineTransform(pts1, pts2)\n    image = cv2.warpAffine(image, M, shape_size[::-1], borderMode=cv2.BORDER_REFLECT_101)\n\n    dx = (gaussian(np.random.uniform(-1, 1, size=shape[:2]),\n                   c[1], mode=\'reflect\', truncate=3) * c[0]).astype(np.float32)\n    dy = (gaussian(np.random.uniform(-1, 1, size=shape[:2]),\n                   c[1], mode=\'reflect\', truncate=3) * c[0]).astype(np.float32)\n    dx, dy = dx[..., np.newaxis], dy[..., np.newaxis]\n\n    x, y, z = np.meshgrid(np.arange(shape[1]), np.arange(shape[0]), np.arange(shape[2]))\n    indices = np.reshape(y + dy, (-1, 1)), np.reshape(x + dx, (-1, 1)), np.reshape(z, (-1, 1))\n    return np.clip(map_coordinates(image, indices, order=1, mode=\'reflect\').reshape(shape), 0, 1) * 255\n\n\n# /////////////// End Distortions ///////////////\n\n\n# /////////////// Further Setup ///////////////\n\n\ndef save_distorted(method=gaussian_noise):\n    for severity in range(1, 6):\n        print(method.__name__, severity)\n        distorted_dataset = DistortImageFolder(\n            root=""/share/data/vision-greg/ImageNet/clsloc/images/val"",\n            method=method, severity=severity,\n            transform=trn.Compose([trn.Resize(256), trn.CenterCrop(224)]))\n        distorted_dataset_loader = torch.utils.data.DataLoader(\n            distorted_dataset, batch_size=100, shuffle=False, num_workers=4)\n\n        for _ in distorted_dataset_loader: continue\n\n\n# /////////////// End Further Setup ///////////////\n\n\n# /////////////// Display Results ///////////////\nimport collections\n\nprint(\'\\nUsing ImageNet data\')\n\nd = collections.OrderedDict()\nd[\'Gaussian Noise\'] = gaussian_noise\nd[\'Shot Noise\'] = shot_noise\nd[\'Impulse Noise\'] = impulse_noise\nd[\'Defocus Blur\'] = defocus_blur\nd[\'Glass Blur\'] = glass_blur\nd[\'Motion Blur\'] = motion_blur\nd[\'Zoom Blur\'] = zoom_blur\nd[\'Snow\'] = snow\nd[\'Frost\'] = frost\nd[\'Fog\'] = fog\nd[\'Brightness\'] = brightness\nd[\'Contrast\'] = contrast\nd[\'Elastic\'] = elastic_transform\nd[\'Pixelate\'] = pixelate\nd[\'JPEG\'] = jpeg_compression\n\nd[\'Speckle Noise\'] = speckle_noise\nd[\'Gaussian Blur\'] = gaussian_blur\nd[\'Spatter\'] = spatter\nd[\'Saturate\'] = saturate\n\nfor method_name in d.keys():\n    save_distorted(d[method_name])\n'"
ImageNet-C/create_c/make_imagenet_c_inception.py,2,"b'# -*- coding: utf-8 -*-\n\nimport os\nfrom PIL import Image\nimport os.path\nimport time\nimport torch\nimport torchvision.datasets as dset\nimport torchvision.transforms as trn\nimport torch.utils.data as data\nimport numpy as np\n\nfrom PIL import Image\nimport numbers\n\n# /////////////// Data Loader ///////////////\n\n\ndef resize(img, size, interpolation=Image.BILINEAR):\n    if isinstance(size, int):\n        w, h = img.size\n        if (w <= h and w == size) or (h <= w and h == size):\n            return img\n        if w < h:\n            ow = size\n            oh = int(size * h / w)\n            return img.resize((ow, oh), interpolation)\n        else:\n            oh = size\n            ow = int(size * w / h)\n            return img.resize((ow, oh), interpolation)\n    else:\n        return img.resize(size[::-1], interpolation)\n\n\ndef center_crop(img, output_size):\n    w, h = img.size\n    # Case of single value provided\n    if isinstance(output_size, numbers.Number):\n        # Float case: constraint for fraction must be from 0 to 1.0\n        if isinstance(output_size, float):\n            if not 0.0 < output_size <= 1.0:\n                raise ValueError(""Invalid float output size. Range is (0.0, 1.0]"")\n            output_size = (output_size, output_size)\n            th, tw = int(h * output_size[0]), int(w * output_size[1])\n        elif isinstance(output_size, int):\n            output_size = (output_size, output_size)\n            th, tw = output_size\n    # Case of tuple of values provided\n    else:\n        if isinstance(output_size[0], float):\n            th, tw = int(h * output_size[0]), int(w * output_size[1])\n        elif isinstance(output_size[0], int):\n            th, tw = output_size\n\n    i = int(round((h - th) / 2.))\n    j = int(round((w - tw) / 2.))\n    return img.crop((j, i, j + tw, i + th))\n\n\ndef resized_center_crop(img, scale=0.875, size=(299, 299), interpolation=Image.BILINEAR):\n    img = center_crop(img, scale)\n    img = resize(img, size, interpolation)\n\n    return img\n\n\nIMG_EXTENSIONS = [\'.jpg\', \'.jpeg\', \'.png\', \'.ppm\', \'.bmp\', \'.pgm\']\n\n\ndef is_image_file(filename):\n    """"""Checks if a file is an image.\n    Args:\n        filename (string): path to a file\n    Returns:\n        bool: True if the filename ends with a known image extension\n    """"""\n    filename_lower = filename.lower()\n    return any(filename_lower.endswith(ext) for ext in IMG_EXTENSIONS)\n\n\ndef find_classes(dir):\n    classes = [d for d in os.listdir(dir) if os.path.isdir(os.path.join(dir, d))]\n    classes.sort()\n    class_to_idx = {classes[i]: i for i in range(len(classes))}\n    return classes, class_to_idx\n\n\ndef make_dataset(dir, class_to_idx):\n    images = []\n    dir = os.path.expanduser(dir)\n    for target in sorted(os.listdir(dir)):\n        d = os.path.join(dir, target)\n        if not os.path.isdir(d):\n            continue\n\n        for root, _, fnames in sorted(os.walk(d)):\n            for fname in sorted(fnames):\n                if is_image_file(fname):\n                    path = os.path.join(root, fname)\n                    item = (path, class_to_idx[target])\n                    images.append(item)\n\n    return images\n\n\ndef pil_loader(path):\n    # open path as file to avoid ResourceWarning (https://github.com/python-pillow/Pillow/issues/835)\n    with open(path, \'rb\') as f:\n        img = Image.open(f)\n        return img.convert(\'RGB\')\n\n\ndef accimage_loader(path):\n    import accimage\n    try:\n        return accimage.Image(path)\n    except IOError:\n        # Potentially a decoding problem, fall back to PIL.Image\n        return pil_loader(path)\n\n\ndef default_loader(path):\n    from torchvision import get_image_backend\n    if get_image_backend() == \'accimage\':\n        return accimage_loader(path)\n    else:\n        return pil_loader(path)\n\n\nclass DistortImageFolder(data.Dataset):\n    def __init__(self, root, method, severity, transform=None, target_transform=None,\n                 loader=default_loader):\n        classes, class_to_idx = find_classes(root)\n        imgs = make_dataset(root, class_to_idx)\n        if len(imgs) == 0:\n            raise (RuntimeError(""Found 0 images in subfolders of: "" + root + ""\\n"" +\n                                ""Supported image extensions are: "" + "","".join(IMG_EXTENSIONS)))\n\n        self.root = root\n        self.method = method\n        self.severity = severity\n        self.imgs = imgs\n        self.classes = classes\n        self.class_to_idx = class_to_idx\n        self.idx_to_class = {v: k for k, v in class_to_idx.items()}\n        self.transform = transform\n        self.target_transform = target_transform\n        self.loader = loader\n\n    def __getitem__(self, index):\n        path, target = self.imgs[index]\n        img = self.loader(path)\n        if self.transform is not None:\n            img = self.transform(img)\n            img = self.method(img, self.severity)\n        if self.target_transform is not None:\n            target = self.target_transform(target)\n\n        save_path = \'/share/data/lang/users/dan/ImageNet-C-299/\' + self.method.__name__ + \\\n                    \'/\' + str(self.severity) + \'/\' + self.idx_to_class[target]\n\n        if not os.path.exists(save_path):\n            os.makedirs(save_path)\n\n        save_path += path[path.rindex(\'/\'):]\n\n        Image.fromarray(np.uint8(img)).save(save_path, quality=85, optimize=True)\n\n        return 0  # we do not care about returning the data\n\n    def __len__(self):\n        return len(self.imgs)\n\n\n# /////////////// Distortion Helpers ///////////////\n\nimport skimage as sk\nfrom skimage.filters import gaussian\nfrom io import BytesIO\nfrom wand.image import Image as WandImage\nfrom wand.api import library as wandlibrary\nimport wand.color as WandColor\nimport ctypes\nfrom PIL import Image as PILImage\nimport cv2\nfrom scipy.ndimage import zoom as scizoom\nfrom scipy.ndimage.interpolation import map_coordinates\nimport warnings\n\nwarnings.simplefilter(""ignore"", UserWarning)\n\n\ndef auc(errs):  # area under the alteration error curve\n    area = 0\n    for i in range(1, len(errs)):\n        area += (errs[i] + errs[i - 1]) / 2\n    area /= len(errs) - 1\n    return area\n\n\ndef disk(radius, alias_blur=0.1, dtype=np.float32):\n    if radius <= 8:\n        L = np.arange(-8, 8 + 1)\n        ksize = (3, 3)\n    else:\n        L = np.arange(-radius, radius + 1)\n        ksize = (5, 5)\n    X, Y = np.meshgrid(L, L)\n    aliased_disk = np.array((X ** 2 + Y ** 2) <= radius ** 2, dtype=dtype)\n    aliased_disk /= np.sum(aliased_disk)\n\n    # supersample disk to antialias\n    return cv2.GaussianBlur(aliased_disk, ksize=ksize, sigmaX=alias_blur)\n\n\n# Tell Python about the C method\nwandlibrary.MagickMotionBlurImage.argtypes = (ctypes.c_void_p,  # wand\n                                              ctypes.c_double,  # radius\n                                              ctypes.c_double,  # sigma\n                                              ctypes.c_double)  # angle\n\n\n# Extend wand.image.Image class to include method signature\nclass MotionImage(WandImage):\n    def motion_blur(self, radius=0.0, sigma=0.0, angle=0.0):\n        wandlibrary.MagickMotionBlurImage(self.wand, radius, sigma, angle)\n\n\n# modification of https://github.com/FLHerne/mapgen/blob/master/diamondsquare.py\ndef plasma_fractal(mapsize=512, wibbledecay=3):\n    """"""\n    Generate a heightmap using diamond-square algorithm.\n    Return square 2d array, side length \'mapsize\', of floats in range 0-255.\n    \'mapsize\' must be a power of two.\n    """"""\n    assert (mapsize & (mapsize - 1) == 0)\n    maparray = np.empty((mapsize, mapsize), dtype=np.float_)\n    maparray[0, 0] = 0\n    stepsize = mapsize\n    wibble = 100\n\n    def wibbledmean(array):\n        return array / 4 + wibble * np.random.uniform(-wibble, wibble, array.shape)\n\n    def fillsquares():\n        """"""For each square of points stepsize apart,\n           calculate middle value as mean of points + wibble""""""\n        cornerref = maparray[0:mapsize:stepsize, 0:mapsize:stepsize]\n        squareaccum = cornerref + np.roll(cornerref, shift=-1, axis=0)\n        squareaccum += np.roll(squareaccum, shift=-1, axis=1)\n        maparray[stepsize // 2:mapsize:stepsize,\n        stepsize // 2:mapsize:stepsize] = wibbledmean(squareaccum)\n\n    def filldiamonds():\n        """"""For each diamond of points stepsize apart,\n           calculate middle value as mean of points + wibble""""""\n        mapsize = maparray.shape[0]\n        drgrid = maparray[stepsize // 2:mapsize:stepsize, stepsize // 2:mapsize:stepsize]\n        ulgrid = maparray[0:mapsize:stepsize, 0:mapsize:stepsize]\n        ldrsum = drgrid + np.roll(drgrid, 1, axis=0)\n        lulsum = ulgrid + np.roll(ulgrid, -1, axis=1)\n        ltsum = ldrsum + lulsum\n        maparray[0:mapsize:stepsize, stepsize // 2:mapsize:stepsize] = wibbledmean(ltsum)\n        tdrsum = drgrid + np.roll(drgrid, 1, axis=1)\n        tulsum = ulgrid + np.roll(ulgrid, -1, axis=0)\n        ttsum = tdrsum + tulsum\n        maparray[stepsize // 2:mapsize:stepsize, 0:mapsize:stepsize] = wibbledmean(ttsum)\n\n    while stepsize >= 2:\n        fillsquares()\n        filldiamonds()\n        stepsize //= 2\n        wibble /= wibbledecay\n\n    maparray -= maparray.min()\n    return maparray / maparray.max()\n\n\ndef clipped_zoom(img, zoom_factor):\n    h = img.shape[0]\n    # ceil crop height(= crop width)\n    ch = int(np.ceil(h / zoom_factor))\n\n    top = (h - ch) // 2\n    img = scizoom(img[top:top + ch, top:top + ch], (zoom_factor, zoom_factor, 1), order=1)\n    # trim off any extra pixels\n    trim_top = (img.shape[0] - h) // 2\n\n    return img[trim_top:trim_top + h, trim_top:trim_top + h]\n\n\n# /////////////// End Distortion Helpers ///////////////\n\n\n# /////////////// Distortions ///////////////\n\ndef gaussian_noise(x, severity=1):\n    c = [.09, .13, 0.19, 0.27, 0.40][severity - 1]\n\n    x = np.array(x) / 255.\n    return np.clip(x + np.random.normal(size=x.shape, scale=c), 0, 1) * 255\n\n\ndef shot_noise(x, severity=1):\n    c = [50, 20, 10, 5, 3][severity - 1]\n\n    x = np.array(x) / 255.\n    return np.clip(np.random.poisson(x * c) / c, 0, 1) * 255\n\n\ndef impulse_noise(x, severity=1):\n    c = [.04, .07, .10, 0.18, 0.28][severity - 1]\n\n    x = sk.util.random_noise(np.array(x) / 255., mode=\'s&p\', amount=c)\n    return np.clip(x, 0, 1) * 255\n\n\ndef speckle_noise(x, severity=1):\n    c = [.16, .21, 0.36, 0.46, 0.61][severity - 1]\n\n    x = np.array(x) / 255.\n    return np.clip(x + x * np.random.normal(size=x.shape, scale=c), 0, 1) * 255\n\n\ndef gaussian_blur(x, severity=1):\n    c = [1.5, 2.5, 3.5, 4.5, 6.5][severity - 1]\n\n    x = gaussian(np.array(x) / 255., sigma=c, multichannel=True)\n    return np.clip(x, 0, 1) * 255\n\n\ndef glass_blur(x, severity=1):\n    # sigma, max_delta, iterations\n    c = [(0.8,1,3), (1,3,1), (1.1,2,4), (1.2,4,2), (1.6,5,2)][severity - 1]\n\n    x = np.uint8(gaussian(np.array(x) / 255., sigma=c[0], multichannel=True) * 255)\n\n    # locally shuffle pixels\n    for i in range(c[2]):\n        for h in range(299 - c[1], c[1], -1):\n            for w in range(299 - c[1], c[1], -1):\n                dx, dy = np.random.randint(-c[1], c[1], size=(2,))\n                h_prime, w_prime = h + dy, w + dx\n                # swap\n                x[h, w], x[h_prime, w_prime] = x[h_prime, w_prime], x[h, w]\n\n    return np.clip(gaussian(x / 255., sigma=c[0], multichannel=True), 0, 1) * 255\n\n\ndef defocus_blur(x, severity=1):\n    c = [(4, 0.1), (5,0.5), (7,0.5), (10,0.5), (12,0.5)][severity - 1]\n\n    x = np.array(x) / 255.\n    kernel = disk(radius=c[0], alias_blur=c[1])\n\n    channels = []\n    for d in range(3):\n        channels.append(cv2.filter2D(x[:, :, d], -1, kernel))\n    channels = np.array(channels).transpose((1, 2, 0))  # 3x299x299 -> 299x299x3\n\n    return np.clip(channels, 0, 1) * 255\n\n\ndef motion_blur(x, severity=1):\n    c = [(12,4), (17,6), (17, 9), (17,13), (22,16)][severity - 1]\n\n    output = BytesIO()\n    x.save(output, format=\'PNG\')\n    x = MotionImage(blob=output.getvalue())\n\n    x.motion_blur(radius=c[0], sigma=c[1], angle=np.random.uniform(-45, 45))\n\n    x = cv2.imdecode(np.fromstring(x.make_blob(), np.uint8),\n                     cv2.IMREAD_UNCHANGED)\n\n    if x.shape != (299, 299):\n        return np.clip(x[..., [2, 1, 0]], 0, 255)  # BGR to RGB\n    else:  # greyscale to RGB\n        return np.clip(np.array([x, x, x]).transpose((1, 2, 0)), 0, 255)\n\n\ndef zoom_blur(x, severity=1):\n    c = [np.arange(1, 1.11, 0.01),\n         np.arange(1, 1.16, 0.01),\n         np.arange(1, 1.21, 0.02),\n         np.arange(1, 1.26, 0.02),\n         np.arange(1, 1.31, 0.03)][severity - 1]\n\n    x = (np.array(x) / 255.).astype(np.float32)\n    out = np.zeros_like(x)\n    for zoom_factor in c:\n        out += clipped_zoom(x, zoom_factor)\n\n    x = (x + out) / (len(c) + 1)\n    return np.clip(x, 0, 1) * 255\n\n\ndef fog(x, severity=1):\n    c = [(1.5, 2), (2, 2), (2.5, 1.8), (2.5, 1.6), (3, 1.5)][severity - 1]\n\n    x = np.array(x) / 255.\n    max_val = x.max()\n    x += c[0] * plasma_fractal(wibbledecay=c[1])[:299, :299][..., np.newaxis]\n    return np.clip(x * max_val / (max_val + c[0]), 0, 1) * 255\n\n\ndef frost(x, severity=1):\n    c = [(1, 0.4), (0.8, 0.6), (0.7, 0.7), (0.65, 0.7), (0.6, 0.75)][severity - 1]\n    idx = np.random.randint(5)\n    filename = [\'./frost1.png\', \'./frost2.png\', \'./frost3.png\', \'./frost4.jpg\', \'./frost5.jpg\', \'./frost6.jpg\'][idx]\n    frost = cv2.imread(filename)\n    # randomly crop and convert to rgb\n    x_start, y_start = np.random.randint(0, frost.shape[0] - 299), np.random.randint(0, frost.shape[1] - 299)\n    frost = frost[x_start:x_start + 299, y_start:y_start + 299][..., [2, 1, 0]]\n\n    return np.clip(c[0] * np.array(x) + c[1] * frost, 0, 255)\n\n\ndef snow(x, severity=1):\n    c = [(0.1, 0.3, 3, 0.5, 10, 4, 0.8),\n         (0.2, 0.3, 2, 0.5, 12, 4, 0.7),\n         (0.55, 0.3, 4, 0.9, 12, 8, 0.7),\n         (0.55, 0.3, 4.5, 0.85, 12, 8, 0.65),\n         (0.55, 0.3, 2.5, 0.85, 12, 12, 0.55)][severity - 1]\n\n    x = np.array(x, dtype=np.float32) / 255.\n    snow_layer = np.random.normal(size=x.shape[:2], loc=c[0], scale=c[1])  # [:2] for monochrome\n\n    snow_layer = clipped_zoom(snow_layer[..., np.newaxis], c[2])\n    snow_layer[snow_layer < c[3]] = 0\n\n    snow_layer = PILImage.fromarray((np.clip(snow_layer.squeeze(), 0, 1) * 255).astype(np.uint8), mode=\'L\')\n    output = BytesIO()\n    snow_layer.save(output, format=\'PNG\')\n    snow_layer = MotionImage(blob=output.getvalue())\n\n    snow_layer.motion_blur(radius=c[4], sigma=c[5], angle=np.random.uniform(-135, -45))\n\n    snow_layer = cv2.imdecode(np.fromstring(snow_layer.make_blob(), np.uint8),\n                              cv2.IMREAD_UNCHANGED) / 255.\n    snow_layer = snow_layer[..., np.newaxis]\n\n    x = c[6] * x + (1 - c[6]) * np.maximum(x, cv2.cvtColor(x, cv2.COLOR_RGB2GRAY).reshape(299, 299, 1) * 1.5 + 0.5)\n    return np.clip(x + snow_layer + np.rot90(snow_layer, k=2), 0, 1) * 255\n\n\ndef spatter(x, severity=1):\n    c = [(0.65,0.3,4,0.69,0.9,0),\n         (0.65,0.3,3.5,0.68,0.9,0),\n         (0.65,0.3,3,0.68,0.8,0),\n         (0.65,0.3,1.2,0.65,1.8,1),\n         (0.67,0.4,1.2,0.65,1.8,1)][severity - 1]\n    x = np.array(x, dtype=np.float32) / 255.\n\n    liquid_layer = np.random.normal(size=x.shape[:2], loc=c[0], scale=c[1])\n\n    liquid_layer = gaussian(liquid_layer, sigma=c[2])\n    liquid_layer[liquid_layer < c[3]] = 0\n    if c[5] == 0:\n        liquid_layer = (liquid_layer * 255).astype(np.uint8)\n        dist = 255 - cv2.Canny(liquid_layer, 50, 150)\n        dist = cv2.distanceTransform(dist, cv2.DIST_L2, 5)\n        _, dist = cv2.threshold(dist, 20, 20, cv2.THRESH_TRUNC)\n        dist = cv2.blur(dist, (3, 3)).astype(np.uint8)\n        dist = cv2.equalizeHist(dist)\n        #     ker = np.array([[-1,-2,-3],[-2,0,0],[-3,0,1]], dtype=np.float32)\n        #     ker -= np.mean(ker)\n        ker = np.array([[-2, -1, 0], [-1, 1, 1], [0, 1, 2]])\n        dist = cv2.filter2D(dist, cv2.CV_8U, ker)\n        dist = cv2.blur(dist, (3, 3)).astype(np.float32)\n\n        m = cv2.cvtColor(liquid_layer * dist, cv2.COLOR_GRAY2BGRA)\n        m /= np.max(m, axis=(0, 1))\n        m *= c[4]\n\n        # water is pale turqouise\n        color = np.concatenate((175 / 255. * np.ones_like(m[..., :1]),\n                                238 / 255. * np.ones_like(m[..., :1]),\n                                238 / 255. * np.ones_like(m[..., :1])), axis=2)\n\n        color = cv2.cvtColor(color, cv2.COLOR_BGR2BGRA)\n        x = cv2.cvtColor(x, cv2.COLOR_BGR2BGRA)\n\n        return cv2.cvtColor(np.clip(x + m * color, 0, 1), cv2.COLOR_BGRA2BGR) * 255\n    else:\n        m = np.where(liquid_layer > c[3], 1, 0)\n        m = gaussian(m.astype(np.float32), sigma=c[4])\n        m[m < 0.8] = 0\n        #         m = np.abs(m) ** (1/c[4])\n\n        # mud brown\n        color = np.concatenate((63 / 255. * np.ones_like(x[..., :1]),\n                                42 / 255. * np.ones_like(x[..., :1]),\n                                20 / 255. * np.ones_like(x[..., :1])), axis=2)\n\n        color *= m[..., np.newaxis]\n        x *= (1 - m[..., np.newaxis])\n\n        return np.clip(x + color, 0, 1) * 255\n\n\ndef contrast(x, severity=1):\n    c = [0.4, .3, .2, .1, .05][severity - 1]\n\n    x = np.array(x) / 255.\n    means = np.mean(x, axis=(0, 1), keepdims=True)\n    return np.clip((x - means) * c + means, 0, 1) * 255\n\n\ndef brightness(x, severity=1):\n    c = [.1, .2, .3, .4, .5][severity - 1]\n\n    x = np.array(x) / 255.\n    x = sk.color.rgb2hsv(x)\n    x[:, :, 2] = np.clip(x[:, :, 2] + c, 0, 1)\n    x = sk.color.hsv2rgb(x)\n\n    return np.clip(x, 0, 1) * 255\n\n\ndef saturate(x, severity=1):\n    c = [(0.3, 0), (0.1, 0), (2, 0), (5, 0.1), (20, 0.2)][severity - 1]\n\n    x = np.array(x) / 255.\n    x = sk.color.rgb2hsv(x)\n    x[:, :, 1] = np.clip(x[:, :, 1] * c[0] + c[1], 0, 1)\n    x = sk.color.hsv2rgb(x)\n\n    return np.clip(x, 0, 1) * 255\n\n\ndef jpeg_compression(x, severity=1):\n    c = [23, 16, 13, 8, 5][severity - 1]\n\n    output = BytesIO()\n    x.save(output, \'JPEG\', quality=c)\n    x = PILImage.open(output)\n\n    return x\n\n\ndef pixelate(x, severity=1):\n    c = [0.5, 0.4, 0.3, 0.25, 0.2][severity - 1]\n\n    x = x.resize((int(299 * c), int(299 * c)), PILImage.BOX)\n    x = x.resize((299, 299), PILImage.BOX)\n\n    return x\n\n\n# mod of https://gist.github.com/erniejunior/601cdf56d2b424757de5\ndef elastic_transform(image, severity=1):\n    c = [(360 * 2, 360 * 0.7, 360 * 0.1),\n         (360 * 2, 360 * 0.08, 360 * 0.2),\n         (360 * 0.05, 360 * 0.01, 360 * 0.02),\n         (360 * 0.07, 360 * 0.01, 360 * 0.02),\n         (360 * 0.12, 360 * 0.01, 360 * 0.02)][severity - 1]\n\n    image = np.array(image, dtype=np.float32) / 255.\n    shape = image.shape\n    shape_size = shape[:2]\n\n    # random affine\n    center_square = np.float32(shape_size) // 2\n    square_size = min(shape_size) // 3\n    pts1 = np.float32([center_square + square_size,\n                       [center_square[0] + square_size, center_square[1] - square_size],\n                       center_square - square_size])\n    pts2 = pts1 + np.random.uniform(-c[2], c[2], size=pts1.shape).astype(np.float32)\n    M = cv2.getAffineTransform(pts1, pts2)\n    image = cv2.warpAffine(image, M, shape_size[::-1], borderMode=cv2.BORDER_REFLECT_101)\n\n    dx = (gaussian(np.random.uniform(-1, 1, size=shape[:2]),\n                   c[1], mode=\'reflect\', truncate=3) * c[0]).astype(np.float32)\n    dy = (gaussian(np.random.uniform(-1, 1, size=shape[:2]),\n                   c[1], mode=\'reflect\', truncate=3) * c[0]).astype(np.float32)\n    dx, dy = dx[..., np.newaxis], dy[..., np.newaxis]\n\n    x, y, z = np.meshgrid(np.arange(shape[1]), np.arange(shape[0]), np.arange(shape[2]))\n    indices = np.reshape(y + dy, (-1, 1)), np.reshape(x + dx, (-1, 1)), np.reshape(z, (-1, 1))\n    return np.clip(map_coordinates(image, indices, order=1, mode=\'reflect\').reshape(shape), 0, 1) * 255\n\n\n# /////////////// End Distortions ///////////////\n\n\n# /////////////// Further Setup ///////////////\n\n\ndef save_distorted(method=gaussian_noise):\n    for severity in range(1, 6):\n        print(method.__name__, severity)\n        distorted_dataset = DistortImageFolder(\n            root=""/share/data/vision-greg/ImageNet/clsloc/images/val"",\n            method=method, severity=severity,\n            transform=resized_center_crop)\n        distorted_dataset_loader = torch.utils.data.DataLoader(\n            distorted_dataset, batch_size=100, shuffle=False, num_workers=4)\n\n        for _ in distorted_dataset_loader: continue\n\n\n# /////////////// End Further Setup ///////////////\n\n\n# /////////////// Display Results ///////////////\nimport collections\n\nprint(\'\\nUsing ImageNet data\')\n\nd = collections.OrderedDict()\n# d[\'Gaussian Noise\'] = gaussian_noise\n# d[\'Shot Noise\'] = shot_noise\n# d[\'Impulse Noise\'] = impulse_noise\n# d[\'Defocus Blur\'] = defocus_blur\n# d[\'Glass Blur\'] = glass_blur\n# d[\'Motion Blur\'] = motion_blur\n# d[\'Zoom Blur\'] = zoom_blur\n# d[\'Snow\'] = snow\n# d[\'Frost\'] = frost\n# d[\'Fog\'] = fog\n# d[\'Brightness\'] = brightness\n# d[\'Contrast\'] = contrast\n# d[\'Elastic\'] = elastic_transform\n# d[\'Pixelate\'] = pixelate\n# d[\'JPEG\'] = jpeg_compression\n#\nd[\'Speckle Noise\'] = speckle_noise\nd[\'Gaussian Blur\'] = gaussian_blur\nd[\'Spatter\'] = spatter\nd[\'Saturate\'] = saturate\n\nfor method_name in d.keys():\n    save_distorted(d[method_name])\n'"
ImageNet-C/create_c/make_tinyimagenet_c.py,2,"b'# -*- coding: utf-8 -*-\n\nimport os\nfrom PIL import Image\nimport os.path\nimport time\nimport torch\nimport torchvision.datasets as dset\nimport torchvision.transforms as trn\nimport torch.utils.data as data\nimport numpy as np\n\nfrom PIL import Image\n\n# /////////////// Data Loader ///////////////\n\n\nIMG_EXTENSIONS = [\'.jpg\', \'.jpeg\', \'.png\', \'.ppm\', \'.bmp\', \'.pgm\']\n\n\ndef is_image_file(filename):\n    """"""Checks if a file is an image.\n    Args:\n        filename (string): path to a file\n    Returns:\n        bool: True if the filename ends with a known image extension\n    """"""\n    filename_lower = filename.lower()\n    return any(filename_lower.endswith(ext) for ext in IMG_EXTENSIONS)\n\n\ndef find_classes(dir):\n    classes = [d for d in os.listdir(dir) if os.path.isdir(os.path.join(dir, d))]\n    classes.sort()\n    class_to_idx = {classes[i]: i for i in range(len(classes))}\n    return classes, class_to_idx\n\n\ndef make_dataset(dir, class_to_idx):\n    images = []\n    dir = os.path.expanduser(dir)\n    for target in sorted(os.listdir(dir)):\n        d = os.path.join(dir, target)\n        if not os.path.isdir(d):\n            continue\n\n        for root, _, fnames in sorted(os.walk(d)):\n            for fname in sorted(fnames):\n                if is_image_file(fname):\n                    path = os.path.join(root, fname)\n                    item = (path, class_to_idx[target])\n                    images.append(item)\n\n    return images\n\n\ndef pil_loader(path):\n    # open path as file to avoid ResourceWarning (https://github.com/python-pillow/Pillow/issues/835)\n    with open(path, \'rb\') as f:\n        img = Image.open(f)\n        return img.convert(\'RGB\')\n\n\ndef accimage_loader(path):\n    import accimage\n    try:\n        return accimage.Image(path)\n    except IOError:\n        # Potentially a decoding problem, fall back to PIL.Image\n        return pil_loader(path)\n\n\ndef default_loader(path):\n    from torchvision import get_image_backend\n    if get_image_backend() == \'accimage\':\n        return accimage_loader(path)\n    else:\n        return pil_loader(path)\n\n\nclass DistortImageFolder(data.Dataset):\n    def __init__(self, root, method, severity, transform=None, target_transform=None,\n                 loader=default_loader):\n        classes, class_to_idx = find_classes(root)\n        imgs = make_dataset(root, class_to_idx)\n        if len(imgs) == 0:\n            raise (RuntimeError(""Found 0 images in subfolders of: "" + root + ""\\n"" +\n                                ""Supported image extensions are: "" + "","".join(IMG_EXTENSIONS)))\n\n        self.root = root\n        self.method = method\n        self.severity = severity\n        self.imgs = imgs\n        self.classes = classes\n        self.class_to_idx = class_to_idx\n        self.idx_to_class = {v: k for k, v in class_to_idx.items()}\n        self.transform = transform\n        self.target_transform = target_transform\n        self.loader = loader\n\n    def __getitem__(self, index):\n        path, target = self.imgs[index]\n        img = self.loader(path)\n        if self.transform is not None:\n            img = self.transform(img)\n            img = self.method(img, self.severity)\n        if self.target_transform is not None:\n            target = self.target_transform(target)\n\n        save_path = \'/share/data/lang/users/dan/Tiny-ImageNet-C/\' + self.method.__name__ + \\\n                    \'/\' + str(self.severity) + \'/\' + self.idx_to_class[target]\n\n        if not os.path.exists(save_path):\n            os.makedirs(save_path)\n\n        save_path += path[path.rindex(\'/\'):]\n\n        Image.fromarray(np.uint8(img)).save(save_path, quality=85, optimize=True)\n\n        return 0  # we do not care about returning the data\n\n    def __len__(self):\n        return len(self.imgs)\n\n\n# /////////////// Distortion Helpers ///////////////\n\nimport skimage as sk\nfrom skimage.filters import gaussian\nfrom io import BytesIO\nfrom wand.image import Image as WandImage\nfrom wand.api import library as wandlibrary\nimport wand.color as WandColor\nimport ctypes\nfrom PIL import Image as PILImage\nimport cv2\nfrom scipy.ndimage import zoom as scizoom\nfrom scipy.ndimage.interpolation import map_coordinates\nimport warnings\n\nwarnings.simplefilter(""ignore"", UserWarning)\n\n\ndef auc(errs):  # area under the alteration error curve\n    area = 0\n    for i in range(1, len(errs)):\n        area += (errs[i] + errs[i - 1]) / 2\n    area /= len(errs) - 1\n    return area\n\n\ndef disk(radius, alias_blur=0.1, dtype=np.float32):\n    if radius <= 8:\n        L = np.arange(-8, 8 + 1)\n        ksize = (3, 3)\n    else:\n        L = np.arange(-radius, radius + 1)\n        ksize = (5, 5)\n    X, Y = np.meshgrid(L, L)\n    aliased_disk = np.array((X ** 2 + Y ** 2) <= radius ** 2, dtype=dtype)\n    aliased_disk /= np.sum(aliased_disk)\n\n    # supersample disk to antialias\n    return cv2.GaussianBlur(aliased_disk, ksize=ksize, sigmaX=alias_blur)\n\n\n# Tell Python about the C method\nwandlibrary.MagickMotionBlurImage.argtypes = (ctypes.c_void_p,  # wand\n                                              ctypes.c_double,  # radius\n                                              ctypes.c_double,  # sigma\n                                              ctypes.c_double)  # angle\n\n\n# Extend wand.image.Image class to include method signature\nclass MotionImage(WandImage):\n    def motion_blur(self, radius=0.0, sigma=0.0, angle=0.0):\n        wandlibrary.MagickMotionBlurImage(self.wand, radius, sigma, angle)\n\n\n# modification of https://github.com/FLHerne/mapgen/blob/master/diamondsquare.py\ndef plasma_fractal(mapsize=64, wibbledecay=3):\n    """"""\n    Generate a heightmap using diamond-square algorithm.\n    Return square 2d array, side length \'mapsize\', of floats in range 0-255.\n    \'mapsize\' must be a power of two.\n    """"""\n    assert (mapsize & (mapsize - 1) == 0)\n    maparray = np.empty((mapsize, mapsize), dtype=np.float_)\n    maparray[0, 0] = 0\n    stepsize = mapsize\n    wibble = 100\n\n    def wibbledmean(array):\n        return array / 4 + wibble * np.random.uniform(-wibble, wibble, array.shape)\n\n    def fillsquares():\n        """"""For each square of points stepsize apart,\n           calculate middle value as mean of points + wibble""""""\n        cornerref = maparray[0:mapsize:stepsize, 0:mapsize:stepsize]\n        squareaccum = cornerref + np.roll(cornerref, shift=-1, axis=0)\n        squareaccum += np.roll(squareaccum, shift=-1, axis=1)\n        maparray[stepsize // 2:mapsize:stepsize,\n        stepsize // 2:mapsize:stepsize] = wibbledmean(squareaccum)\n\n    def filldiamonds():\n        """"""For each diamond of points stepsize apart,\n           calculate middle value as mean of points + wibble""""""\n        mapsize = maparray.shape[0]\n        drgrid = maparray[stepsize // 2:mapsize:stepsize, stepsize // 2:mapsize:stepsize]\n        ulgrid = maparray[0:mapsize:stepsize, 0:mapsize:stepsize]\n        ldrsum = drgrid + np.roll(drgrid, 1, axis=0)\n        lulsum = ulgrid + np.roll(ulgrid, -1, axis=1)\n        ltsum = ldrsum + lulsum\n        maparray[0:mapsize:stepsize, stepsize // 2:mapsize:stepsize] = wibbledmean(ltsum)\n        tdrsum = drgrid + np.roll(drgrid, 1, axis=1)\n        tulsum = ulgrid + np.roll(ulgrid, -1, axis=0)\n        ttsum = tdrsum + tulsum\n        maparray[stepsize // 2:mapsize:stepsize, 0:mapsize:stepsize] = wibbledmean(ttsum)\n\n    while stepsize >= 2:\n        fillsquares()\n        filldiamonds()\n        stepsize //= 2\n        wibble /= wibbledecay\n\n    maparray -= maparray.min()\n    return maparray / maparray.max()\n\n\ndef clipped_zoom(img, zoom_factor):\n    h = img.shape[0]\n    # ceil crop height(= crop width)\n    ch = int(np.ceil(h / zoom_factor))\n\n    top = (h - ch) // 2\n    img = scizoom(img[top:top + ch, top:top + ch], (zoom_factor, zoom_factor, 1), order=1)\n    # trim off any extra pixels\n    trim_top = (img.shape[0] - h) // 2\n\n    return img[trim_top:trim_top + h, trim_top:trim_top + h]\n\n\n# /////////////// End Distortion Helpers ///////////////\n\n\n# /////////////// Distortions ///////////////\n\ndef gaussian_noise(x, severity=1):\n    c = [0.04, 0.08, .12, .15, .18][severity - 1]\n\n    x = np.array(x) / 255.\n    return np.clip(x + np.random.normal(size=x.shape, scale=c), 0, 1) * 255\n\n\ndef shot_noise(x, severity=1):\n    c = [250, 100, 50, 30, 15][severity - 1]\n\n    x = np.array(x) / 255.\n    return np.clip(np.random.poisson(x * c) / c, 0, 1) * 255\n\n\ndef impulse_noise(x, severity=1):\n    c = [.01, .02, .05, .08, .14][severity - 1]\n\n    x = sk.util.random_noise(np.array(x) / 255., mode=\'s&p\', amount=c)\n    return np.clip(x, 0, 1) * 255\n\n\ndef speckle_noise(x, severity=1):\n    c = [.15, .2, 0.25, 0.3, 0.35][severity - 1]\n\n    x = np.array(x) / 255.\n    return np.clip(x + x * np.random.normal(size=x.shape, scale=c), 0, 1) * 255\n\n\ndef gaussian_blur(x, severity=1):\n    c = [.5, .75, 1, 1.25, 1.5][severity - 1]\n\n    x = gaussian(np.array(x) / 255., sigma=c, multichannel=True)\n    return np.clip(x, 0, 1) * 255\n\n\ndef glass_blur(x, severity=1):\n    # sigma, max_delta, iterations\n    c = [(0.1,1,1), (0.5,1,1), (0.6,1,2), (0.7,2,1), (0.9,2,2)][severity - 1]\n\n    x = np.uint8(gaussian(np.array(x) / 255., sigma=c[0], multichannel=True) * 255)\n\n    # locally shuffle pixels\n    for i in range(c[2]):\n        for h in range(64 - c[1], c[1], -1):\n            for w in range(64 - c[1], c[1], -1):\n                dx, dy = np.random.randint(-c[1], c[1], size=(2,))\n                h_prime, w_prime = h + dy, w + dx\n                # swap\n                x[h, w], x[h_prime, w_prime] = x[h_prime, w_prime], x[h, w]\n\n    return np.clip(gaussian(x / 255., sigma=c[0], multichannel=True), 0, 1) * 255\n\n\ndef defocus_blur(x, severity=1):\n    c = [(0.5, 0.6), (1, 0.1), (1.5, 0.1), (2.5, 0.01), (3, 0.1)][severity - 1]\n\n    x = np.array(x) / 255.\n    kernel = disk(radius=c[0], alias_blur=c[1])\n\n    channels = []\n    for d in range(3):\n        channels.append(cv2.filter2D(x[:, :, d], -1, kernel))\n    channels = np.array(channels).transpose((1, 2, 0))  # 3x64x64 -> 64x64x3\n\n    return np.clip(channels, 0, 1) * 255\n\n\ndef motion_blur(x, severity=1):\n    c = [(10,1), (10,1.5), (10,2), (10,2.5), (12,3)][severity - 1]\n\n    output = BytesIO()\n    x.save(output, format=\'PNG\')\n    x = MotionImage(blob=output.getvalue())\n\n    x.motion_blur(radius=c[0], sigma=c[1], angle=np.random.uniform(-45, 45))\n\n    x = cv2.imdecode(np.fromstring(x.make_blob(), np.uint8),\n                     cv2.IMREAD_UNCHANGED)\n\n    if x.shape != (64, 64):\n        return np.clip(x[..., [2, 1, 0]], 0, 255)  # BGR to RGB\n    else:  # greyscale to RGB\n        return np.clip(np.array([x, x, x]).transpose((1, 2, 0)), 0, 255)\n\n\ndef zoom_blur(x, severity=1):\n    c = [np.arange(1, 1.06, 0.01), np.arange(1, 1.11, 0.01), np.arange(1, 1.16, 0.01),\n         np.arange(1, 1.21, 0.01), np.arange(1, 1.26, 0.01)][severity - 1]\n\n    x = (np.array(x) / 255.).astype(np.float32)\n    out = np.zeros_like(x)\n    for zoom_factor in c:\n        out += clipped_zoom(x, zoom_factor)\n\n    x = (x + out) / (len(c) + 1)\n    return np.clip(x, 0, 1) * 255\n\n\ndef fog(x, severity=1):\n    c = [(.4,3), (.7,3), (1,2.5), (1.5,2), (2,1.75)][severity - 1]\n\n    x = np.array(x) / 255.\n    max_val = x.max()\n    x += c[0] * plasma_fractal(wibbledecay=c[1])[:64, :64][..., np.newaxis]\n    return np.clip(x * max_val / (max_val + c[0]), 0, 1) * 255\n\n\ndef frost(x, severity=1):\n    c = [(1, 0.3), (0.9, 0.4), (0.8, 0.45), (0.75, 0.5), (0.7, 0.55)][severity - 1]\n    idx = np.random.randint(5)\n    filename = [\'./frost1.png\', \'./frost2.png\', \'./frost3.png\', \'./frost4.jpg\', \'./frost5.jpg\', \'./frost6.jpg\'][idx]\n    frost = cv2.imread(filename)\n    frost = cv2.resize(frost, (0, 0), fx=0.3, fy=0.3)\n    # randomly crop and convert to rgb\n    x_start, y_start = np.random.randint(0, frost.shape[0] - 64), np.random.randint(0, frost.shape[1] - 64)\n    frost = frost[x_start:x_start + 64, y_start:y_start + 64][..., [2, 1, 0]]\n\n    return np.clip(c[0] * np.array(x) + c[1] * frost, 0, 255)\n\n\ndef snow(x, severity=1):\n    c = [(0.1,0.2,1,0.6,8,3,0.8),\n         (0.1,0.2,1,0.5,10,4,0.8),\n         (0.15,0.3,1.75,0.55,10,4,0.7),\n         (0.25,0.3,2.25,0.6,12,6,0.65),\n         (0.3,0.3,1.25,0.65,14,12,0.6)][severity - 1]\n\n    x = np.array(x, dtype=np.float32) / 255.\n    snow_layer = np.random.normal(size=x.shape[:2], loc=c[0], scale=c[1])  # [:2] for monochrome\n\n    snow_layer = clipped_zoom(snow_layer[..., np.newaxis], c[2])\n    snow_layer[snow_layer < c[3]] = 0\n\n    snow_layer = PILImage.fromarray((np.clip(snow_layer.squeeze(), 0, 1) * 255).astype(np.uint8), mode=\'L\')\n    output = BytesIO()\n    snow_layer.save(output, format=\'PNG\')\n    snow_layer = MotionImage(blob=output.getvalue())\n\n    snow_layer.motion_blur(radius=c[4], sigma=c[5], angle=np.random.uniform(-135, -45))\n\n    snow_layer = cv2.imdecode(np.fromstring(snow_layer.make_blob(), np.uint8),\n                              cv2.IMREAD_UNCHANGED) / 255.\n    snow_layer = snow_layer[..., np.newaxis]\n\n    x = c[6] * x + (1 - c[6]) * np.maximum(x, cv2.cvtColor(x, cv2.COLOR_RGB2GRAY).reshape(64, 64, 1) * 1.5 + 0.5)\n    return np.clip(x + snow_layer + np.rot90(snow_layer, k=2), 0, 1) * 255\n\n\ndef spatter(x, severity=1):\n    c = [(0.62,0.1,0.7,0.7,0.6,0),\n         (0.65,0.1,0.8,0.7,0.6,0),\n         (0.65,0.3,1,0.69,0.6,0),\n         (0.65,0.1,0.7,0.68,0.6,1),\n         (0.65,0.1,0.5,0.67,0.6,1)][severity - 1]\n    x = np.array(x, dtype=np.float32) / 255.\n\n    liquid_layer = np.random.normal(size=x.shape[:2], loc=c[0], scale=c[1])\n\n    liquid_layer = gaussian(liquid_layer, sigma=c[2])\n    liquid_layer[liquid_layer < c[3]] = 0\n    if c[5] == 0:\n        liquid_layer = (liquid_layer * 255).astype(np.uint8)\n        dist = 255 - cv2.Canny(liquid_layer, 50, 150)\n        dist = cv2.distanceTransform(dist, cv2.DIST_L2, 5)\n        _, dist = cv2.threshold(dist, 20, 20, cv2.THRESH_TRUNC)\n        dist = cv2.blur(dist, (3, 3)).astype(np.uint8)\n        dist = cv2.equalizeHist(dist)\n        #     ker = np.array([[-1,-2,-3],[-2,0,0],[-3,0,1]], dtype=np.float32)\n        #     ker -= np.mean(ker)\n        ker = np.array([[-2, -1, 0], [-1, 1, 1], [0, 1, 2]])\n        dist = cv2.filter2D(dist, cv2.CV_8U, ker)\n        dist = cv2.blur(dist, (3, 3)).astype(np.float32)\n\n        m = cv2.cvtColor(liquid_layer * dist, cv2.COLOR_GRAY2BGRA)\n        m /= np.max(m, axis=(0, 1))\n        m *= c[4]\n\n        # water is pale turqouise\n        color = np.concatenate((175 / 255. * np.ones_like(m[..., :1]),\n                                238 / 255. * np.ones_like(m[..., :1]),\n                                238 / 255. * np.ones_like(m[..., :1])), axis=2)\n\n        color = cv2.cvtColor(color, cv2.COLOR_BGR2BGRA)\n        x = cv2.cvtColor(x, cv2.COLOR_BGR2BGRA)\n\n        return cv2.cvtColor(np.clip(x + m * color, 0, 1), cv2.COLOR_BGRA2BGR) * 255\n    else:\n        m = np.where(liquid_layer > c[3], 1, 0)\n        m = gaussian(m.astype(np.float32), sigma=c[4])\n        m[m < 0.8] = 0\n        #         m = np.abs(m) ** (1/c[4])\n\n        # mud brown\n        color = np.concatenate((63 / 255. * np.ones_like(x[..., :1]),\n                                42 / 255. * np.ones_like(x[..., :1]),\n                                20 / 255. * np.ones_like(x[..., :1])), axis=2)\n\n        color *= m[..., np.newaxis]\n        x *= (1 - m[..., np.newaxis])\n\n        return np.clip(x + color, 0, 1) * 255\n\n\ndef contrast(x, severity=1):\n    c = [.4, .3, .2, .1, 0.05][severity - 1]\n\n    x = np.array(x) / 255.\n    means = np.mean(x, axis=(0, 1), keepdims=True)\n    return np.clip((x - means) * c + means, 0, 1) * 255\n\n\ndef brightness(x, severity=1):\n    c = [.1, .2, .3, .4, .5][severity - 1]\n\n    x = np.array(x) / 255.\n    x = sk.color.rgb2hsv(x)\n    x[:, :, 2] = np.clip(x[:, :, 2] + c, 0, 1)\n    x = sk.color.hsv2rgb(x)\n\n    return np.clip(x, 0, 1) * 255\n\n\ndef saturate(x, severity=1):\n    c = [(0.3, 0), (0.1, 0), (2, 0), (5, 0.1), (30, 0.2)][severity - 1]\n\n    x = np.array(x) / 255.\n    x = sk.color.rgb2hsv(x)\n    x[:, :, 1] = np.clip(x[:, :, 1] * c[0] + c[1], 0, 1)\n    x = sk.color.hsv2rgb(x)\n\n    return np.clip(x, 0, 1) * 255\n\n\ndef jpeg_compression(x, severity=1):\n    c = [65, 58, 50, 40, 25][severity - 1]\n\n    output = BytesIO()\n    x.save(output, \'JPEG\', quality=c)\n    x = PILImage.open(output)\n\n    return x\n\n\ndef pixelate(x, severity=1):\n    c = [0.9, 0.8, 0.7, 0.6, 0.5][severity - 1]\n\n    x = x.resize((int(64 * c), int(64 * c)), PILImage.BOX)\n    x = x.resize((64, 64), PILImage.BOX)\n\n    return x\n\n\n# mod of https://gist.github.com/erniejunior/601cdf56d2b424757de5\ndef elastic_transform(image, severity=1):\n    IMSIZE = 64\n    c = [(IMSIZE*0, IMSIZE*0, IMSIZE*0.08),\n         (IMSIZE*0.05, IMSIZE*0.3, IMSIZE*0.06),\n         (IMSIZE*0.1, IMSIZE*0.08, IMSIZE*0.06),\n         (IMSIZE*0.1, IMSIZE*0.03, IMSIZE*0.03),\n         (IMSIZE*0.16, IMSIZE*0.03, IMSIZE*0.02)][severity - 1]\n\n    image = np.array(image, dtype=np.float32) / 255.\n    shape = image.shape\n    shape_size = shape[:2]\n\n    # random affine\n    center_square = np.float32(shape_size) // 2\n    square_size = min(shape_size) // 3\n    pts1 = np.float32([center_square + square_size,\n                       [center_square[0] + square_size, center_square[1] - square_size],\n                       center_square - square_size])\n    pts2 = pts1 + np.random.uniform(-c[2], c[2], size=pts1.shape).astype(np.float32)\n    M = cv2.getAffineTransform(pts1, pts2)\n    image = cv2.warpAffine(image, M, shape_size[::-1], borderMode=cv2.BORDER_REFLECT_101)\n\n    dx = (gaussian(np.random.uniform(-1, 1, size=shape[:2]),\n                   c[1], mode=\'reflect\', truncate=3) * c[0]).astype(np.float32)\n    dy = (gaussian(np.random.uniform(-1, 1, size=shape[:2]),\n                   c[1], mode=\'reflect\', truncate=3) * c[0]).astype(np.float32)\n    dx, dy = dx[..., np.newaxis], dy[..., np.newaxis]\n\n    x, y, z = np.meshgrid(np.arange(shape[1]), np.arange(shape[0]), np.arange(shape[2]))\n    indices = np.reshape(y + dy, (-1, 1)), np.reshape(x + dx, (-1, 1)), np.reshape(z, (-1, 1))\n    return np.clip(map_coordinates(image, indices, order=1, mode=\'reflect\').reshape(shape), 0, 1) * 255\n\n\n# /////////////// End Distortions ///////////////\n\n\n# /////////////// Further Setup ///////////////\n\n\ndef save_distorted(method=gaussian_noise):\n    for severity in range(1, 6):\n        print(method.__name__, severity)\n        distorted_dataset = DistortImageFolder(\n            root=""./imagenet_val_bbox_crop/"",\n            method=method, severity=severity,\n            transform=trn.Compose([trn.Resize((64, 64))]))\n        distorted_dataset_loader = torch.utils.data.DataLoader(\n            distorted_dataset, batch_size=100, shuffle=False, num_workers=6)\n\n        for _ in distorted_dataset_loader: continue\n\n\n# /////////////// End Further Setup ///////////////\n\n\n# /////////////// Display Results ///////////////\nimport collections\n\nprint(\'\\nUsing ImageNet data\')\n\nd = collections.OrderedDict()\n# d[\'Gaussian Noise\'] = gaussian_noise\n# d[\'Shot Noise\'] = shot_noise\n# d[\'Impulse Noise\'] = impulse_noise\n# d[\'Defocus Blur\'] = defocus_blur\n# d[\'Glass Blur\'] = glass_blur\n# d[\'Motion Blur\'] = motion_blur\n# d[\'Zoom Blur\'] = zoom_blur\n# d[\'Snow\'] = snow\nd[\'Frost\'] = frost\n# d[\'Fog\'] = fog\n# d[\'Brightness\'] = brightness\n# d[\'Contrast\'] = contrast\n# d[\'Elastic\'] = elastic_transform\n# d[\'Pixelate\'] = pixelate\n# d[\'JPEG\'] = jpeg_compression\n\n# d[\'Speckle Noise\'] = speckle_noise\n# d[\'Gaussian Blur\'] = gaussian_blur\n# d[\'Spatter\'] = spatter\n# d[\'Saturate\'] = saturate\n\nfor method_name in d.keys():\n    save_distorted(d[method_name])\n'"
ImageNet-C/imagenet_c/setup.py,0,"b'import setuptools\nfrom glob import glob\n\nwith open(""README.md"", ""r"") as fh:\n    long_description = fh.read()\n\nsetuptools.setup(\n    name=""imagenet_c"",\n    version=""0.0.2"",\n    author=""Dan Hendrycks"",\n    author_email=""hendrycks@berkeley.edu"",\n    description=""Access to ImageNet-C corruption functions"",\n    long_description=long_description,\n    long_description_content_type=""text/markdown"",\n    url=""https://github.com/hendrycks/robustness/tree/master/ImageNet-C/imagenet_c"",\n    packages=setuptools.find_packages(),\n    package_data={\n        ""imagenet_c.frost"": [\n            ""frost1.png"", ""frost2.png"", ""frost3.png"",\n            ""frost4.jpg"", ""frost5.jpg"", ""frost6.jpg""\n        ],\n    },\n    install_requires=[\n        \'wand ~= 0.4\',\n        \'opencv-python ~= 3.4\',\n    ],\n    #include_package_data=True,\n    classifiers=[\n        ""Programming Language :: Python :: 3"",\n        ""License :: OSI Approved :: MIT License"",\n        ""Operating System :: OS Independent"",\n    ],\n)\n\n'"
ImageNet-P/create_p/make_cifar_p.py,3,"b""import os\nimport numpy as np\nimport torch\nimport torchvision.transforms as trn\nimport torchvision.transforms.functional as trn_F\nimport cv2\nfrom PIL import Image as PILImage\nimport skimage.color as skcolor\nfrom skimage.util import random_noise\nfrom wand.image import Image as WandImage\nfrom wand.api import library as wandlibrary\nfrom io import BytesIO\nimport ctypes\nfrom scipy.ndimage import zoom as scizoom\nfrom skimage.filters import gaussian\nfrom tempfile import gettempdir\nfrom shutil import rmtree\nimport torchvision.datasets as dset\n\n# Tell Python about the C method\nwandlibrary.MagickMotionBlurImage.argtypes = (ctypes.c_void_p,  # wand\n                                              ctypes.c_double,  # radius\n                                              ctypes.c_double,  # sigma\n                                              ctypes.c_double)  # angle\n\n\n# Extend wand.image.Image class to include method signature\nclass MotionImage(WandImage):\n    def motion_blur(self, radius=0.0, sigma=0.0, angle=0.0):\n        wandlibrary.MagickMotionBlurImage(self.wand, radius, sigma, angle)\n\n\ndef clipped_zoom(img, zoom_factor):\n    h = img.shape[0]\n    # ceil crop height(= crop width)\n    ch = int(np.ceil(h / zoom_factor))\n\n    top = (h - ch) // 2\n    img = scizoom(img[top:top + ch, top:top + ch], (zoom_factor, zoom_factor, 1), order=1)\n    # trim off any extra pixels\n    trim_top = (img.shape[0] - h) // 2\n\n    return img[trim_top:trim_top + h, trim_top:trim_top + h]\n\n\ndef brightness(_x, c=0.):\n    _x = np.array(_x, copy=True) / 255.\n    _x = skcolor.rgb2hsv(_x)\n    _x[:, :, 2] = np.clip(_x[:, :, 2] + c, 0, 1)\n    _x = skcolor.hsv2rgb(_x)\n\n    return np.uint8(_x * 255)\n\n\ndef to_numpy(image):\n    return np.uint8(image.numpy().transpose(1,2,0) * 255)\n\n\ntest_data = dset.CIFAR10('/share/data/vision-greg/cifarpy', train=False)\n\ncifar_p = []\nlabels = []\n\nfor img, label in zip(test_data.data, test_data.targets):\n    seq = []\n    labels.append(label)\n\n    # /////////////// Test Data ///////////////\n\n    # /////////////// Gaussian Noise Code ///////////////\n\n    x = trn.ToTensor()(img)\n    seq.append(img)\n\n    for i in range(1, 31):\n        z = to_numpy(torch.clamp(x + 0.02 * torch.randn_like(x), 0, 1))\n        seq.append(z)\n\n    # for i in range(1, 31):\n    #     z = torch.clamp(x + 0.04 * torch.randn_like(x), 0, 1)\n    #     seq.append(to_numpy(z))\n\n    # for i in range(1, 31):\n    #     z = torch.clamp(x + 0.06 * torch.randn_like(x), 0, 1)\n    #     seq.append(to_numpy(z))\n\n    # /////////////// End Gaussian Noise Code ///////////////\n\n    # /////////////// Shot Noise Code ///////////////\n\n    # x = img\n    # seq.append(img)\n\n    # for i in range(1, 31):\n    #     z = np.array(x, copy=True) / 255.\n    #     z = np.uint8(255 * np.clip(np.random.poisson(z * 700) / 700., 0, 1))\n    #     seq.append(z)\n\n    # for i in range(1, 31):\n    #     z = np.array(x, copy=True) / 255.\n    #     z = np.uint8(255 * np.clip(np.random.poisson(z * 400) / 400., 0, 1))\n    #     seq.append(z)\n\n    # for i in range(1, 31):\n    #     z = np.array(x, copy=True) / 255.\n    #     z = np.uint8(255 * np.clip(np.random.poisson(z * 200) / 200., 0, 1))\n    #     seq.append(z)\n\n    # /////////////// End Shot Noise Code ///////////////\n\n    # /////////////// Motion Blur Code ///////////////\n\n    # for i in range(0, 21):\n    #     z = PILImage.fromarray(img)\n    #     output = BytesIO()\n    #     z.save(output, format='PNG')\n    #     z = MotionImage(blob=output.getvalue())\n    #\n    #     z.motion_blur(radius=6, sigma=1.8, angle=(i - 20) * 9)\n    #\n    #     z = cv2.imdecode(np.fromstring(z.make_blob(), np.uint8),\n    #                      cv2.IMREAD_UNCHANGED)\n    #\n    #     if z.shape != (32, 32):\n    #         z = np.clip(z[..., [2, 1, 0]], 0, 255)  # BGR to RGB\n    #     else:  # grayscale to RGB\n    #         z = np.clip(np.array([z, z, z]).transpose((1, 2, 0)), 0, 255)\n    #\n    #     seq.append(np.uint8(z))\n\n    # /////////////// End Motion Blur Code ///////////////\n\n    # /////////////// Zoom Blur Code ///////////////\n\n    # seq.append(img)\n    # avg = trn.ToTensor()(img)\n    #\n    # for i in range(1, 31):\n    #     z = trn.CenterCrop(32)(trn_F.affine(PILImage.fromarray(img), angle=0, translate=(0, 0),\n    #                                         scale=1+0.004*i, shear=0, resample=PILImage.BILINEAR))\n    #     avg += trn.ToTensor()(z)\n    #     seq.append(np.array(trn.ToPILImage()(avg / (i + 1))))\n\n    # /////////////// End Zoom Blur Code ///////////////\n\n    # /////////////// Snow Code ///////////////\n\n    # x = np.array(img) / 255.\n    #\n    # snow_layer = np.random.normal(size=(32, 32), loc=0.05, scale=0.3)\n    #\n    # snow_layer = clipped_zoom(snow_layer[..., np.newaxis], 2)\n    # snow_layer[snow_layer < 0.5] = 0\n    #\n    # snow_layer = PILImage.fromarray((np.clip(snow_layer.squeeze(), 0, 1) * 255).astype(np.uint8), mode='L')\n    # output = BytesIO()\n    # snow_layer.save(output, format='PNG')\n    # output = output.getvalue()\n    #\n    # for i in range(0, 31):\n    #     moving_snow = MotionImage(blob=output)\n    #     moving_snow.motion_blur(radius=10, sigma=2, angle=i*4-150)\n    #\n    #     snow_layer = cv2.imdecode(np.fromstring(moving_snow.make_blob(), np.uint8),\n    #                               cv2.IMREAD_UNCHANGED) / 255.\n    #     snow_layer = snow_layer[..., np.newaxis]\n    #\n    #     z = 0.85 * x + (1 - 0.85) * np.maximum(\n    #         x, cv2.cvtColor(np.float32(x), cv2.COLOR_RGB2GRAY).reshape(32, 32, 1) * 1.5 + 0.5)\n    #\n    #     z = np.uint8(np.clip(z + snow_layer + np.rot90(snow_layer, k=2), 0, 1) * 255)\n    #\n    #     seq.append(z)\n\n    # /////////////// End Snow Code ///////////////\n\n    # /////////////// Brightness Code ///////////////\n\n    # x = PILImage.fromarray(img)\n    #\n    # for i in range(0, 31):\n    #     z = brightness(x, c=(i - 15) * 1.5 / 100.)\n    #     seq.append(z)\n\n    # /////////////// End Brightness Code ///////////////\n\n    # /////////////// Translate Code ///////////////\n\n    # x = PILImage.fromarray(img)\n    #\n    # for i in range(0,16):\n    #     z = trn_F.affine(x, angle=0, translate=(i-7, 0), scale=1, shear=0)\n    #     seq.append(np.array(z))\n\n    # /////////////// End Translate Code ///////////////\n\n    # /////////////// Rotate Code ///////////////\n\n    # x = PILImage.fromarray(img)\n    #\n    # for i in range(0, 31):\n    #     z = trn_F.affine(x, angle=i-15, translate=(0, 0),\n    #                      scale=1., shear=0, resample=PILImage.BILINEAR)\n    #     seq.append(np.array(z))\n\n    # /////////////// End Rotate Code ///////////////\n\n    # /////////////// Tilt Code ///////////////\n\n    # x = np.array(img)\n    # h, w = x.shape[0:2]\n    #\n    # for i in range(0, 31):\n    #     phi, theta = np.deg2rad(0.75*(i-15)), np.deg2rad(0.75*(i-15))\n    #\n    #     f = np.sqrt(w ** 2 + h ** 2)\n    #\n    #     P1 = np.array([[1, 0, -w / 2], [0, 1, -h / 2], [0, 0, 1], [0, 0, 1]])\n    #\n    #     RX = np.array([[1, 0, 0, 0], [0, np.cos(theta), -np.sin(theta), 0],\n    #                    [0, np.sin(theta), np.cos(theta), 0], [0, 0, 0, 1]])\n    #\n    #     RY = np.array([[np.cos(phi), 0, -np.sin(phi), 0], [0, 1, 0, 0],\n    #                    [np.sin(phi), 0, np.cos(phi), 0], [0, 0, 0, 1]])\n    #\n    #     T = np.array([[1, 0, 0, 0], [0, 1, 0, 0],\n    #                   [0, 0, 1, f], [0, 0, 0, 1]])\n    #\n    #     P2 = np.array([[f, 0, w / 2, 0], [0, f, h / 2, 0], [0, 0, 1, 0]])\n    #\n    #     mat = P2 @ T @ RX @ RY @ P1\n    #\n    #     z = cv2.warpPerspective(x, mat, (w, h))\n    #     seq.append(z)\n\n    # /////////////// End Tilt Code ///////////////\n\n    # /////////////// Scale Code ///////////////\n\n    # x = PILImage.fromarray(img)\n    #\n    # for i in range(0, 31):\n    #     z = trn.CenterCrop(32)(trn_F.affine(x, angle=0, translate=(0, 0),\n    #                                         scale=(i * 2 + 70) / 100., shear=0, resample=PILImage.BILINEAR))\n    #     seq.append(np.array(z))\n\n    # /////////////// End Scale Code ///////////////\n\n    # /////////////// Validation Data ///////////////\n\n    # /////////////// Speckle Noise Code ///////////////\n\n    # seq.append(img)\n    # x = np.array(img) / 255.\n\n    # for i in range(1, 31):\n    #     z = np.uint8(255 * np.clip(x + x * np.random.normal(size=x.shape, scale=0.05), 0, 1))\n    #     seq.append(z)\n\n    # for i in range(1, 31):\n    #     z = np.uint8(255 * np.clip(x + x * np.random.normal(size=x.shape, scale=0.07), 0, 1))\n    #     seq.append(z)\n\n    # for i in range(1, 31):\n    #     z = np.uint8(255 * np.clip(x + x * np.random.normal(size=x.shape, scale=0.1), 0, 1))\n    #     seq.append(z)\n\n    # /////////////// End Speckle Noise Code ///////////////\n\n    # /////////////// Gaussian Blur Code ///////////////\n\n    # for i in range(0, 31):\n    #     z = np.uint8(255*gaussian(np.array(img, copy=True)/255.,\n    #                               sigma=0.3 + 0.015*i, multichannel=True, truncate=7.0))\n    #     seq.append(z)\n\n    # /////////////// End Gaussian Blur Code ///////////////\n\n    # /////////////// Spatter Code ///////////////\n\n    # x = cv2.cvtColor(np.array(img, dtype=np.float32) / 255., cv2.COLOR_BGR2BGRA)\n    #\n    # liquid_layer = np.random.normal(size=x.shape[:2], loc=0.6, scale=0.265)\n    # liquid_layer = gaussian(liquid_layer, sigma=1.75)\n    # liquid_layer[liquid_layer < 0.7] = 0\n    #\n    # for i in range(0, 31):\n    #\n    #     liquid_layer_i = (liquid_layer * 255).astype(np.uint8)\n    #     dist = 255 - cv2.Canny(liquid_layer_i, 50, 150)\n    #     dist = cv2.distanceTransform(dist, cv2.DIST_L2, 5)\n    #     _, dist = cv2.threshold(dist, 20, 20, cv2.THRESH_TRUNC)\n    #     dist = cv2.blur(dist, (3, 3)).astype(np.uint8)\n    #     dist = cv2.equalizeHist(dist)\n    #     ker = np.array([[-2, -1, 0], [-1, 1, 1], [0, 1, 2]])\n    #     dist = cv2.filter2D(dist, cv2.CV_8U, ker)\n    #     dist = cv2.blur(dist, (3, 3)).astype(np.float32)\n    #\n    #     m = cv2.cvtColor(liquid_layer_i * dist, cv2.COLOR_GRAY2BGRA)\n    #     m /= np.max(m, axis=(0, 1))\n    #     m *= 0.6\n    #\n    #     # water is pale turqouise\n    #     color = np.concatenate((175 / 255. * np.ones_like(m[..., :1]),\n    #                             238 / 255. * np.ones_like(m[..., :1]),\n    #                             238 / 255. * np.ones_like(m[..., :1])), axis=2)\n    #\n    #     color = cv2.cvtColor(color, cv2.COLOR_BGR2BGRA)\n    #\n    #     z = np.uint8(cv2.cvtColor(np.clip(x + m * color, 0, 1), cv2.COLOR_BGRA2BGR) * 255)\n    #\n    #     liquid_layer = np.apply_along_axis(lambda mat:\n    #                                        np.convolve(mat, np.array([0.2, 0.8]), mode='same'),\n    #                                        axis=0, arr=liquid_layer)\n    #\n    #     seq.append(z)\n\n    # /////////////// End Spatter Code ///////////////\n\n    # /////////////// Shear Code ///////////////\n\n    # for i in range(0, 31):\n    #     z = trn.CenterCrop(32)(trn_F.affine(PILImage.fromarray(img), angle=0, translate=(0, 0),\n    #                                         scale=1., shear=i-15, resample=PILImage.BILINEAR))\n    #     seq.append(np.array(z))\n\n    # /////////////// End Shear Code ///////////////\n\n    cifar_p.append(seq)\n\n\nnp.save('/share/data/vision-greg2/users/dan/datasets/CIFAR-10-P/gaussian_noise.npy',\n        np.array(cifar_p).astype(np.uint8))\n"""
ImageNet-P/create_p/make_imagenet_64_p.py,3,"b'import os\nimport numpy as np\nimport torch\nimport torchvision.transforms as trn\nimport torchvision.transforms.functional as trn_F\nimport cv2\nfrom PIL import Image as PILImage\nimport skimage.color as skcolor\nfrom skimage.util import random_noise\nfrom wand.image import Image as WandImage\nfrom wand.api import library as wandlibrary\nfrom io import BytesIO\nimport ctypes\nfrom scipy.ndimage import zoom as scizoom\nfrom skimage.filters import gaussian\nfrom tempfile import gettempdir\nfrom shutil import rmtree\n\nimagenet_val_folder = \'/share/data/vision-greg/ImageNet/clsloc/images/val/\'\n\n# Tell Python about the C method\nwandlibrary.MagickMotionBlurImage.argtypes = (ctypes.c_void_p,  # wand\n                                              ctypes.c_double,  # radius\n                                              ctypes.c_double,  # sigma\n                                              ctypes.c_double)  # angle\n\n\n# Extend wand.image.Image class to include method signature\nclass MotionImage(WandImage):\n    def motion_blur(self, radius=0.0, sigma=0.0, angle=0.0):\n        wandlibrary.MagickMotionBlurImage(self.wand, radius, sigma, angle)\n\n\ndef clipped_zoom(img, zoom_factor):\n    h = img.shape[0]\n    # ceil crop height(= crop width)\n    ch = int(np.ceil(h / zoom_factor))\n\n    top = (h - ch) // 2\n    img = scizoom(img[top:top + ch, top:top + ch], (zoom_factor, zoom_factor, 1), order=1)\n    # trim off any extra pixels\n    trim_top = (img.shape[0] - h) // 2\n\n    return img[trim_top:trim_top + h, trim_top:trim_top + h]\n\n\ndef brightness(_x, c=0.):\n    _x = np.array(_x, copy=True) / 255.\n    _x = skcolor.rgb2hsv(_x)\n    _x[:, :, 2] = np.clip(_x[:, :, 2] + c, 0, 1)\n    _x = skcolor.hsv2rgb(_x)\n\n    return np.uint8(_x * 255)\n\n\nfor index, folder in enumerate(sorted(os.listdir(imagenet_val_folder))):\n    if index < 750 or index > 1000: continue\n    \n    for img_loc in os.listdir(imagenet_val_folder + folder):\n        orig_img = PILImage.open(imagenet_val_folder + folder + \'/\' + img_loc).convert(\'RGB\')\n        img = trn.Resize((64, 64))(orig_img)\n\n        # /////////////// Test Data ///////////////\n\n        # # /////////////// Gaussian Noise Code ///////////////\n        #\n        # tmp = os.path.join(gettempdir(), \'.{}\'.format(hash(os.times())))\n        # os.makedirs(tmp)\n        #\n        # x = trn.Compose([trn.CenterCrop(64), trn.ToTensor()])(img)\n        #\n        # z = trn.ToPILImage()(x)\n        # z.save(os.path.join(tmp, \'img0.png\'))\n        #\n        # # for i in range(1, 31):\n        # #     z = trn.ToPILImage()(torch.clamp(x + 0.025 * torch.randn_like(x), 0, 1))\n        # #     z.save(os.path.join(tmp, \'img\' + str(i) + \'.png\'))\n        # # save_dir = \'/share/data/vision-greg2/users/dan/datasets/ImageNet-64x64-P/gaussian_noise/\' + folder + \'/\'\n        #\n        # # for i in range(1, 31):\n        # #     z = trn.ToPILImage()(torch.clamp(x + 0.05 * torch.randn_like(x), 0, 1))\n        # #     z.save(os.path.join(tmp, \'img\' + str(i) + \'.png\'))\n        # # save_dir = \'/share/data/vision-greg2/users/dan/datasets/ImageNet-64x64-P/gaussian_noise_2/\' + folder + \'/\'\n        #\n        # # for i in range(1, 31):\n        # #     z = trn.ToPILImage()(torch.clamp(x + 0.075 * torch.randn_like(x), 0, 1))\n        # #     z.save(os.path.join(tmp, \'img\' + str(i) + \'.png\'))\n        # # save_dir = \'/share/data/vision-greg2/users/dan/datasets/ImageNet-64x64-P/gaussian_noise_3/\' + folder + \'/\'\n        #\n        # if not os.path.exists(save_dir):\n        #     os.makedirs(save_dir)\n        #\n        # os.system(""./ffmpeg/ffmpeg -r 1 -framerate 1 -i "" + os.path.join(tmp, \'img\') + \\\n        #           ""%01d.png -vcodec libx264 -tune grain -preset veryslow -y "" + \\\n        #           save_dir + img_loc[:-5] + "".mp4"")\n        #\n        # rmtree(tmp, ignore_errors=True)\n        #\n        # # /////////////// End Gaussian Noise Code ///////////////\n\n        # # /////////////// Shot Noise Code ///////////////\n        #\n        # tmp = os.path.join(gettempdir(), \'.{}\'.format(hash(os.times())))\n        # os.makedirs(tmp)\n        #\n        # x = trn.CenterCrop(64)(img)\n        #\n        # x.save(os.path.join(tmp, \'img0.png\'))\n        #\n        # # for i in range(1, 31):\n        # #     z = np.array(x, copy=True) / 255.\n        # #     z = PILImage.fromarray(\n        # #         np.uint8(255 * np.clip(np.random.poisson(z * 500) / 500., 0, 1)))\n        # #     z.save(os.path.join(tmp, \'img\' + str(i) + \'.png\'))\n        # # save_dir = \'/share/data/vision-greg2/users/dan/datasets/ImageNet-64x64-P/shot_noise/\' + folder + \'/\'\n        #\n        # # for i in range(1, 31):\n        # #     z = np.array(x, copy=True) / 255.\n        # #     z = PILImage.fromarray(\n        # #         np.uint8(255 * np.clip(np.random.poisson(z * 250) / 250., 0, 1)))\n        # #     z.save(os.path.join(tmp, \'img\' + str(i) + \'.png\'))\n        # # save_dir = \'/share/data/vision-greg2/users/dan/datasets/ImageNet-64x64-P/shot_noise_2/\' + folder + \'/\'\n        #\n        # # for i in range(1, 31):\n        # #     z = np.array(x, copy=True) / 255.\n        # #     z = PILImage.fromarray(\n        # #         np.uint8(255 * np.clip(np.random.poisson(z * 125) / 125., 0, 1)))\n        # #     z.save(os.path.join(tmp, \'img\' + str(i) + \'.png\'))\n        # # save_dir = \'/share/data/vision-greg2/users/dan/datasets/ImageNet-64x64-P/shot_noise_3/\' + folder + \'/\'\n        #\n        # if not os.path.exists(save_dir):\n        #     os.makedirs(save_dir)\n        #\n        # os.system(""./ffmpeg/ffmpeg -r 1 -framerate 1 -i "" + os.path.join(tmp, \'img\') + \\\n        #           ""%01d.png -vcodec libx264 -tune grain -preset veryslow -y "" + \\\n        #           save_dir + img_loc[:-5] + "".mp4"")\n        #\n        # rmtree(tmp, ignore_errors=True)\n        #\n        # # /////////////// End Shot Noise Code ///////////////\n\n        # /////////////// Motion Blur Code ///////////////\n\n        tmp = os.path.join(gettempdir(), \'.{}\'.format(hash(os.times())))\n        os.makedirs(tmp)\n\n        for i in range(0,31):\n\n            z = orig_img\n            output = BytesIO()\n            z.save(output, format=\'PNG\')\n            z = MotionImage(blob=output.getvalue())\n\n            z.motion_blur(radius=14, sigma=3, angle=(i-30)*5)\n\n            z = cv2.imdecode(np.fromstring(z.make_blob(), np.uint8),\n                             cv2.IMREAD_UNCHANGED)\n\n            if z.shape != (64, 64):\n                z = np.clip(z[..., [2, 1, 0]], 0, 255)  # BGR to RGB\n            else:  # grayscale to RGB\n                z = np.clip(np.array([z, z, z]).transpose((1, 2, 0)), 0, 255)\n\n            trn.Resize(64)(PILImage.fromarray(z)).save(os.path.join(tmp, \'img\' + str(i) + \'.png\'))\n\n        save_dir = \'/share/data/vision-greg2/users/dan/datasets/ImageNet-64x64-P/motion_blur/\' + folder + \'/\'\n\n        if not os.path.exists(save_dir):\n            os.makedirs(save_dir)\n\n        os.system(""./ffmpeg/ffmpeg -r 1 -framerate 1 -i "" + os.path.join(tmp, \'img\') + \\\n                  ""%01d.png -vcodec libx264 -crf 20 -preset veryslow -y "" + \\\n                  save_dir + img_loc[:-5] + "".mp4"")\n\n        rmtree(tmp, ignore_errors=True)\n\n        # /////////////// End Motion Blur Code ///////////////\n\n        # # /////////////// Zoom Blur Code ///////////////\n        #\n        # tmp = os.path.join(gettempdir(), \'.{}\'.format(hash(os.times())))\n        # os.makedirs(tmp)\n        #\n        # z = trn.CenterCrop(64)(img)\n        # avg = trn.ToTensor()(z)\n        # z.save(os.path.join(tmp, \'img0.png\'))\n        # for i in range(1, 31):\n        #     z = trn.CenterCrop(64)(trn_F.affine(img, angle=0, translate=(0, 0),\n        #                                         scale=1+0.004*i, shear=0, resample=PILImage.BILINEAR))\n        #     avg += trn.ToTensor()(z)\n        #     trn.ToPILImage()(avg / (i + 1)).save(os.path.join(tmp, \'img\' + str(i) + \'.png\'))\n        #\n        # save_dir = \'/share/data/vision-greg2/users/dan/datasets/ImageNet-64x64-P/zoom_blur/\' + folder + \'/\'\n        #\n        # if not os.path.exists(save_dir):\n        #     os.makedirs(save_dir)\n        #\n        # os.system(""./ffmpeg/ffmpeg -r 1 -framerate 1 -i "" +\n        #           os.path.join(tmp, \'img\') + ""%01d.png -vcodec libx264 -crf 20 -preset veryslow -y "" +\n        #           save_dir + img_loc[:-5] + "".mp4"")\n        #\n        # rmtree(tmp, ignore_errors=True)\n        #\n        # # /////////////// End Zoom Blur Code ///////////////\n\n        # # /////////////// Snow Code ///////////////\n        #\n        # tmp = os.path.join(gettempdir(), \'.{}\'.format(hash(os.times())))\n        # os.makedirs(tmp)\n        #\n        # x = trn.CenterCrop(64)(img)\n        # x = np.array(x) / 255.\n        #\n        # snow_layer = np.random.normal(size=(64, 64), loc=0.05, scale=0.28)\n        #\n        # snow_layer = clipped_zoom(snow_layer[..., np.newaxis], 2)\n        # snow_layer[snow_layer < 0.5] = 0\n        #\n        # snow_layer = PILImage.fromarray((np.clip(snow_layer.squeeze(), 0, 1) * 255).astype(np.uint8), mode=\'L\')\n        # output = BytesIO()\n        # snow_layer.save(output, format=\'PNG\')\n        # output = output.getvalue()\n        #\n        # for i in range(0, 31):\n        #     moving_snow = MotionImage(blob=output)\n        #     moving_snow.motion_blur(radius=10, sigma=2.5, angle=i*4-150)\n        #\n        #     snow_layer = cv2.imdecode(np.fromstring(moving_snow.make_blob(), np.uint8),\n        #                               cv2.IMREAD_UNCHANGED) / 255.\n        #     snow_layer = snow_layer[..., np.newaxis]\n        #\n        #     z = 0.85 * x + (1 - 0.85) * np.maximum(\n        #         x, cv2.cvtColor(np.float32(x), cv2.COLOR_RGB2GRAY).reshape(64, 64, 1) * 1.5 + 0.5)\n        #\n        #     z = np.uint8(np.clip(z + snow_layer + np.rot90(snow_layer, k=2), 0, 1) * 255)\n        #\n        #     PILImage.fromarray(z).save(os.path.join(tmp, \'img\' + str(i) + \'.png\'))\n        #\n        # save_dir = \'/share/data/vision-greg2/users/dan/datasets/ImageNet-64x64-P/snow/\' + folder + \'/\'\n        #\n        # if not os.path.exists(save_dir):\n        #     os.makedirs(save_dir)\n        #\n        # os.system(""./ffmpeg/ffmpeg -r 1 -framerate 1 -i "" + os.path.join(tmp, \'img\') + \\\n        #           ""%01d.png -vcodec libx264 -crf 20 -preset veryslow -y "" + \\\n        #           save_dir + img_loc[:-5] + "".mp4"")\n        #\n        # rmtree(tmp, ignore_errors=True)\n        #\n        # # /////////////// End Snow Code ///////////////\n\n        # # /////////////// Brightness Code ///////////////\n        #\n        # tmp = os.path.join(gettempdir(), \'.{}\'.format(hash(os.times())))\n        # os.makedirs(tmp)\n        #\n        # x = trn.CenterCrop(64)(img)\n        #\n        # for i in range(0, 31):\n        #     z = PILImage.fromarray(brightness(x, c=(i - 15) * 2 / 100.))\n        #     z.save(os.path.join(tmp, \'img\' + str(i) + \'.png\'))\n        #\n        # save_dir = \'/share/data/vision-greg2/users/dan/datasets/ImageNet-64x64-P/brightness/\' + folder + \'/\'\n        #\n        # if not os.path.exists(save_dir):\n        #     os.makedirs(save_dir)\n        #\n        # os.system(""./ffmpeg/ffmpeg -r 1 -framerate 1 -i "" + os.path.join(tmp, \'img\') + \\\n        #           ""%01d.png -vcodec libx264 -crf 20 -preset veryslow -y "" + \\\n        #           save_dir + img_loc[:-5] + "".mp4"")\n        #\n        # rmtree(tmp, ignore_errors=True)\n        #\n        # # /////////////// End Brightness Code ///////////////\n\n        # # /////////////// Translate Code ///////////////\n        #\n        # tmp = os.path.join(gettempdir(), \'.{}\'.format(hash(os.times())))\n        # os.makedirs(tmp)\n        #\n        # for i in range(0,31):\n        #     z = trn.CenterCrop(64)(trn_F.affine(img, angle=0, translate=(i-15, 0), scale=1, shear=0))\n        #     z.save(os.path.join(tmp, \'img\'+str(i)+\'.png\'))\n        #\n        # save_dir = \'/share/data/vision-greg2/users/dan/datasets/ImageNet-64x64-P/translate/\' + folder + \'/\'\n        #\n        # if not os.path.exists(save_dir):\n        #     os.makedirs(save_dir)\n        #\n        # os.system(""./ffmpeg/ffmpeg -r 1 -framerate 1 -i "" + os.path.join(tmp, \'img\') +\n        #           ""%01d.png -vcodec libx264 -crf 20 -preset veryslow -y "" +\n        #           save_dir + img_loc[:-5] + "".mp4"")\n        #\n        # rmtree(tmp, ignore_errors=True)\n        #\n        # # /////////////// End Translate Code ///////////////\n\n        # # /////////////// Rotate Code ///////////////\n        #\n        # tmp = os.path.join(gettempdir(), \'.{}\'.format(hash(os.times())))\n        # os.makedirs(tmp)\n        #\n        # for i in range(0, 31):\n        #     z = trn.CenterCrop(64)(trn_F.affine(img, angle=i-15, translate=(0, 0),\n        #                                         scale=1., shear=0, resample=PILImage.BILINEAR))\n        #     z.save(os.path.join(tmp, \'img\' + str(i) + \'.png\'))\n        #\n        # save_dir = \'/share/data/vision-greg2/users/dan/datasets/ImageNet-64x64-P/rotate/\' + folder + \'/\'\n        #\n        # if not os.path.exists(save_dir):\n        #     os.makedirs(save_dir)\n        #\n        # os.system(""./ffmpeg/ffmpeg -r 1 -framerate 1 -i "" +\n        #           os.path.join(tmp, \'img\') + ""%01d.png -vcodec libx264 -crf 20 -preset veryslow -y "" +\n        #           save_dir + img_loc[:-5] + "".mp4"")\n        #\n        # rmtree(tmp, ignore_errors=True)\n        #\n        # # /////////////// End Rotate Code ///////////////\n\n        # # /////////////// Tilt Code ///////////////\n        #\n        # tmp = os.path.join(gettempdir(), \'.{}\'.format(hash(os.times())))\n        # os.makedirs(tmp)\n        #\n        # x = np.array(img)\n        # h, w = x.shape[0:2]\n        #\n        # for i in range(0, 31):\n        #     phi, theta = np.deg2rad(i-15), np.deg2rad(i-15)\n        #\n        #     f = np.sqrt(w ** 2 + h ** 2)\n        #\n        #     P1 = np.array([[1, 0, -w / 2], [0, 1, -h / 2], [0, 0, 1], [0, 0, 1]])\n        #\n        #     RX = np.array([[1, 0, 0, 0], [0, np.cos(theta), -np.sin(theta), 0],\n        #                    [0, np.sin(theta), np.cos(theta), 0], [0, 0, 0, 1]])\n        #\n        #     RY = np.array([[np.cos(phi), 0, -np.sin(phi), 0], [0, 1, 0, 0],\n        #                    [np.sin(phi), 0, np.cos(phi), 0], [0, 0, 0, 1]])\n        #\n        #     T = np.array([[1, 0, 0, 0], [0, 1, 0, 0],\n        #                   [0, 0, 1, f], [0, 0, 0, 1]])\n        #\n        #     P2 = np.array([[f, 0, w / 2, 0], [0, f, h / 2, 0], [0, 0, 1, 0]])\n        #\n        #     mat = P2 @ T @ RX @ RY @ P1\n        #\n        #     z = trn.CenterCrop(64)(PILImage.fromarray(cv2.warpPerspective(x, mat, (w, h))))\n        #\n        #     z.save(os.path.join(tmp, \'img\' + str(i) + \'.png\'))\n        #\n        # save_dir = \'/share/data/vision-greg2/users/dan/datasets/ImageNet-64x64-P/tilt/\' + folder + \'/\'\n        #\n        # if not os.path.exists(save_dir):\n        #     os.makedirs(save_dir)\n        #\n        # os.system(""./ffmpeg/ffmpeg -r 1 -framerate 1 -i "" + os.path.join(tmp, \'img\') + \\\n        #           ""%01d.png -vcodec libx264 -crf 20 -preset veryslow -y "" + \\\n        #           save_dir + img_loc[:-5] + "".mp4"")\n        #\n        # rmtree(tmp, ignore_errors=True)\n        #\n        # # /////////////// End Tilt Code ///////////////\n\n        # # /////////////// Scale Code ///////////////\n        #\n        # tmp = os.path.join(gettempdir(), \'.{}\'.format(hash(os.times())))\n        # os.makedirs(tmp)\n        #\n        # for i in range(0, 31):\n        #     z = trn.CenterCrop(64)(trn_F.affine(img, angle=0, translate=(0, 0),\n        #                                         scale=(i * 2.5 + 40) / 100., shear=0, resample=PILImage.BILINEAR))\n        #     z.save(os.path.join(tmp, \'img\' + str(i) + \'.png\'))\n        #\n        # save_dir = \'/share/data/vision-greg2/users/dan/datasets/ImageNet-64x64-P/scale/\' + folder + \'/\'\n        #\n        # if not os.path.exists(save_dir):\n        #     os.makedirs(save_dir)\n        #\n        # os.system(""./ffmpeg/ffmpeg -r 1 -framerate 1 -i "" +\n        #           os.path.join(tmp, \'img\') + ""%01d.png -vcodec libx264 -crf 20 -preset veryslow -y "" +\n        #           save_dir + img_loc[:-5] + "".mp4"")\n        #\n        # rmtree(tmp, ignore_errors=True)\n        #\n        # # /////////////// End Scale Code ///////////////\n\n        # /////////////// Validation Data ///////////////\n\n        # # /////////////// Speckle Noise Code ///////////////\n        #\n        # tmp = os.path.join(gettempdir(), \'.{}\'.format(hash(os.times())))\n        # os.makedirs(tmp)\n        #\n        # x = trn.CenterCrop(64)(img)\n        #\n        # x.save(os.path.join(tmp, \'img0.png\'))\n        # x = np.array(x) / 255.\n        #\n        # # for i in range(1, 31):\n        # #     z = PILImage.fromarray(\n        # #         np.uint8(255 * np.clip(x + x * np.random.normal(size=x.shape, scale=0.05), 0, 1)))\n        # #     z.save(os.path.join(tmp, \'img\' + str(i) + \'.png\'))\n        # # save_dir = \'/share/data/vision-greg2/users/dan/datasets/ImageNet-64x64-P/speckle_noise/\' + folder + \'/\'\n        #\n        # # for i in range(1, 31):\n        # #     z = PILImage.fromarray(\n        # #         np.uint8(255 * np.clip(x + x * np.random.normal(size=x.shape, scale=0.1), 0, 1)))\n        # #     z.save(os.path.join(tmp, \'img\' + str(i) + \'.png\'))\n        # # save_dir = \'/share/data/vision-greg2/users/dan/datasets/ImageNet-64x64-P/speckle_noise_2/\' + folder + \'/\'\n        #\n        # # for i in range(1, 31):\n        # #     z = PILImage.fromarray(\n        # #         np.uint8(255 * np.clip(x + x * np.random.normal(size=x.shape, scale=0.15), 0, 1)))\n        # #     z.save(os.path.join(tmp, \'img\' + str(i) + \'.png\'))\n        # # save_dir = \'/share/data/vision-greg2/users/dan/datasets/ImageNet-64x64-P/speckle_noise_3/\' + folder + \'/\'\n        #\n        # if not os.path.exists(save_dir):\n        #     os.makedirs(save_dir)\n        #\n        # os.system(""./ffmpeg/ffmpeg -r 1 -framerate 1 -i "" + os.path.join(tmp, \'img\') + \\\n        #           ""%01d.png -vcodec libx264 -tune grain -preset veryslow -y "" + \\\n        #           save_dir + img_loc[:-5] + "".mp4"")\n        #\n        # rmtree(tmp, ignore_errors=True)\n        #\n        # # /////////////// End Speckle Noise Code ///////////////\n\n        # # /////////////// Gaussian Blur Code ///////////////\n        #\n        # tmp = os.path.join(gettempdir(), \'.{}\'.format(hash(os.times())))\n        # os.makedirs(tmp)\n        #\n        # x = trn.CenterCrop(64)(img)\n        # for i in range(0, 31):\n        #     z = PILImage.fromarray(\n        #         np.uint8(255*gaussian(np.array(x, copy=True)/255.,\n        #                               sigma=0.35 + 0.014*i, multichannel=True, truncate=7.0)))\n        #\n        #     z.save(os.path.join(tmp, \'img\' + str(i) + \'.png\'))\n        #\n        # save_dir = \'/share/data/vision-greg2/users/dan/datasets/ImageNet-64x64-P/gaussian_blur/\' + folder + \'/\'\n        #\n        # if not os.path.exists(save_dir):\n        #     os.makedirs(save_dir)\n        #\n        # os.system(""./ffmpeg/ffmpeg -r 1 -framerate 1 -i "" +\n        #           os.path.join(tmp, \'img\') + ""%01d.png -vcodec libx264 -crf 20 -preset veryslow -y "" +\n        #           save_dir + img_loc[:-5] + "".mp4"")\n        #\n        # rmtree(tmp, ignore_errors=True)\n        #\n        # # /////////////// End Gaussian Blur Code ///////////////\n\n        # # /////////////// Spatter Code ///////////////\n        #\n        # tmp = os.path.join(gettempdir(), \'.{}\'.format(hash(os.times())))\n        # os.makedirs(tmp)\n        #\n        # x = trn.CenterCrop(64)(img)\n        # x = cv2.cvtColor(np.array(x, dtype=np.float32) / 255., cv2.COLOR_BGR2BGRA)\n        #\n        # liquid_layer = np.random.normal(size=x.shape[:2], loc=0.6, scale=0.25)\n        # liquid_layer = gaussian(liquid_layer, sigma=1.75)\n        # liquid_layer[liquid_layer < 0.7] = 0\n        #\n        # for i in range(0, 31):\n        #\n        #     liquid_layer_i = (liquid_layer * 255).astype(np.uint8)\n        #     dist = 255 - cv2.Canny(liquid_layer_i, 50, 150)\n        #     dist = cv2.distanceTransform(dist, cv2.DIST_L2, 5)\n        #     _, dist = cv2.threshold(dist, 20, 20, cv2.THRESH_TRUNC)\n        #     dist = cv2.blur(dist, (3, 3)).astype(np.uint8)\n        #     dist = cv2.equalizeHist(dist)\n        #     ker = np.array([[-2, -1, 0], [-1, 1, 1], [0, 1, 2]])\n        #     dist = cv2.filter2D(dist, cv2.CV_8U, ker)\n        #     dist = cv2.blur(dist, (3, 3)).astype(np.float32)\n        #\n        #     m = cv2.cvtColor(liquid_layer_i * dist, cv2.COLOR_GRAY2BGRA)\n        #     m /= np.max(m, axis=(0, 1))\n        #     m *= 0.6\n        #\n        #     # water is pale turqouise\n        #     color = np.concatenate((175 / 255. * np.ones_like(m[..., :1]),\n        #                             238 / 255. * np.ones_like(m[..., :1]),\n        #                             238 / 255. * np.ones_like(m[..., :1])), axis=2)\n        #\n        #     color = cv2.cvtColor(color, cv2.COLOR_BGR2BGRA)\n        #\n        #     z = np.uint8(cv2.cvtColor(np.clip(x + m * color, 0, 1), cv2.COLOR_BGRA2BGR) * 255)\n        #\n        #     liquid_layer = np.apply_along_axis(lambda mat:\n        #                                        np.convolve(mat, np.array([0.2, 0.8]), mode=\'same\'),\n        #                                        axis=0, arr=liquid_layer)\n        #\n        #     PILImage.fromarray(z).save(os.path.join(tmp, \'img\' + str(i) + \'.png\'))\n        #\n        # save_dir = \'/share/data/vision-greg2/users/dan/datasets/ImageNet-64x64-P/spatter/\' + folder + \'/\'\n        #\n        # if not os.path.exists(save_dir):\n        #     os.makedirs(save_dir)\n        #\n        # os.system(""./ffmpeg/ffmpeg -r 1 -framerate 1 -i "" + os.path.join(tmp, \'img\') + \\\n        #           ""%01d.png -vcodec libx264 -crf 20 -preset veryslow -y "" + \\\n        #           save_dir + img_loc[:-5] + "".mp4"")\n        #\n        # rmtree(tmp, ignore_errors=True)\n        #\n        # # /////////////// End Spatter Code ///////////////\n\n        # # /////////////// Shear Code ///////////////\n        #\n        # tmp = os.path.join(gettempdir(), \'.{}\'.format(hash(os.times())))\n        # os.makedirs(tmp)\n        #\n        # for i in range(0, 31):\n        #     z = trn.CenterCrop(64)(trn_F.affine(img, angle=0, translate=(0, 0),\n        #                                          scale=1., shear=i-15, resample=PILImage.BILINEAR))\n        #     z.save(os.path.join(tmp, \'img\' + str(i) + \'.png\'))\n        #\n        # save_dir = \'/share/data/vision-greg2/users/dan/datasets/ImageNet-64x64-P/shear/\' + folder + \'/\'\n        #\n        # if not os.path.exists(save_dir):\n        #     os.makedirs(save_dir)\n        #\n        # os.system(""./ffmpeg/ffmpeg -r 1 -framerate 1 -i "" +\n        #           os.path.join(tmp, \'img\') + ""%01d.png -vcodec libx264 -crf 20 -preset veryslow -y "" +\n        #           save_dir + img_loc[:-5] + "".mp4"")\n        #\n        # rmtree(tmp, ignore_errors=True)\n        #\n        # # /////////////// End Shear Code ///////////////\n'"
ImageNet-P/create_p/make_imagenet_p.py,3,"b'import os\nimport numpy as np\nimport torch\nimport torchvision.transforms as trn\nimport torchvision.transforms.functional as trn_F\nimport cv2\nfrom PIL import Image as PILImage\nimport skimage.color as skcolor\nfrom skimage.util import random_noise\nfrom wand.image import Image as WandImage\nfrom wand.api import library as wandlibrary\nfrom io import BytesIO\nimport ctypes\nfrom scipy.ndimage import zoom as scizoom\nfrom skimage.filters import gaussian\nfrom tempfile import gettempdir\nfrom shutil import rmtree\n\nimagenet_val_folder = \'/share/data/vision-greg/ImageNet/clsloc/images/val/\'\n\n# Tell Python about the C method\nwandlibrary.MagickMotionBlurImage.argtypes = (ctypes.c_void_p,  # wand\n                                              ctypes.c_double,  # radius\n                                              ctypes.c_double,  # sigma\n                                              ctypes.c_double)  # angle\n\n\n# Extend wand.image.Image class to include method signature\nclass MotionImage(WandImage):\n    def motion_blur(self, radius=0.0, sigma=0.0, angle=0.0):\n        wandlibrary.MagickMotionBlurImage(self.wand, radius, sigma, angle)\n\n\ndef clipped_zoom(img, zoom_factor):\n    h = img.shape[0]\n    # ceil crop height(= crop width)\n    ch = int(np.ceil(h / zoom_factor))\n\n    top = (h - ch) // 2\n    img = scizoom(img[top:top + ch, top:top + ch], (zoom_factor, zoom_factor, 1), order=1)\n    # trim off any extra pixels\n    trim_top = (img.shape[0] - h) // 2\n\n    return img[trim_top:trim_top + h, trim_top:trim_top + h]\n\n\ndef brightness(_x, c=0.):\n    _x = np.array(_x, copy=True) / 255.\n    _x = skcolor.rgb2hsv(_x)\n    _x[:, :, 2] = np.clip(_x[:, :, 2] + c, 0, 1)\n    _x = skcolor.hsv2rgb(_x)\n\n    return np.uint8(_x * 255)\n\n\nfor index, folder in enumerate(sorted(os.listdir(imagenet_val_folder))):\n\n    for img_loc in os.listdir(imagenet_val_folder + folder):\n        img = trn.Resize(256)(PILImage.open(imagenet_val_folder + folder + \'/\' + img_loc)).convert(\'RGB\')\n\n        # /////////////// Test Data ///////////////\n\n        # # /////////////// Gaussian Noise Code ///////////////\n        #\n        # tmp = os.path.join(gettempdir(), \'.{}\'.format(hash(os.times())))\n        # os.makedirs(tmp)\n        #\n        # x = trn.Compose([trn.CenterCrop(224), trn.ToTensor()])(img)\n        #\n        # z = trn.ToPILImage()(x)\n        # z.save(os.path.join(tmp, \'img0.png\'))\n        #\n        # for i in range(1, 31):\n        #     z = trn.ToPILImage()(torch.clamp(x + 0.03 * torch.randn_like(x), 0, 1))\n        #     z.save(os.path.join(tmp, \'img\' + str(i) + \'.png\'))\n        # save_dir = \'/share/data/vision-greg2/users/dan/datasets/ImageNet-P/gaussian_noise/\' + folder + \'/\'\n        #\n        # # for i in range(1, 31):\n        # #     z = trn.ToPILImage()(torch.clamp(x + 0.05 * torch.randn_like(x), 0, 1))\n        # #     z.save(os.path.join(tmp, \'img\' + str(i) + \'.png\'))\n        # # save_dir = \'/share/data/vision-greg2/users/dan/datasets/ImageNet-P/gaussian_noise_2/\' + folder + \'/\'\n        #\n        # # for i in range(1, 31):\n        # #     z = trn.ToPILImage()(torch.clamp(x + 0.08 * torch.randn_like(x), 0, 1))\n        # #     z.save(os.path.join(tmp, \'img\' + str(i) + \'.png\'))\n        # # save_dir = \'/share/data/vision-greg2/users/dan/datasets/ImageNet-P/gaussian_noise_3/\' + folder + \'/\'\n        #\n        # if not os.path.exists(save_dir):\n        #     os.makedirs(save_dir)\n        #\n        # os.system(""./ffmpeg/ffmpeg -r 1 -framerate 1 -i "" + os.path.join(tmp, \'img\') + \\\n        #           ""%01d.png -vcodec libx264 -tune grain -preset veryslow -y "" + \\\n        #           save_dir + img_loc[:-5] + "".mp4"")\n        #\n        # rmtree(tmp, ignore_errors=True)\n        #\n        # # /////////////// End Gaussian Noise Code ///////////////\n\n        # /////////////// Shot Noise Code ///////////////\n\n        tmp = os.path.join(gettempdir(), \'.{}\'.format(hash(os.times())))\n        os.makedirs(tmp)\n\n        x = trn.CenterCrop(224)(img)\n\n        x.save(os.path.join(tmp, \'img0.png\'))\n\n        for i in range(1, 31):\n            z = np.array(x, copy=True) / 255.\n            z = PILImage.fromarray(np.uint8(255 * np.clip(np.random.poisson(z * 300) / 300., 0, 1)))\n            z.save(os.path.join(tmp, \'img\' + str(i) + \'.png\'))\n        save_dir = \'/share/data/vision-greg2/users/dan/datasets/ImageNet-P/shot_noise/\' + folder + \'/\'\n\n        # for i in range(1, 31):\n        #     z = np.array(x, copy=True) / 255.\n        #     z = PILImage.fromarray(np.uint8(255 * np.clip(np.random.poisson(z * 120) / 120., 0, 1)))\n        #     z.save(os.path.join(tmp, \'img\' + str(i) + \'.png\'))\n        # save_dir = \'/share/data/vision-greg2/users/dan/datasets/ImageNet-P/shot_noise_2/\' + folder + \'/\'\n\n        # for i in range(1, 31):\n        #     z = np.array(x, copy=True) / 255.\n        #     z = PILImage.fromarray(np.uint8(255 * np.clip(np.random.poisson(z * 60) / 60., 0, 1)))\n        #     z.save(os.path.join(tmp, \'img\' + str(i) + \'.png\'))\n        # save_dir = \'/share/data/vision-greg2/users/dan/datasets/ImageNet-P/shot_noise_3/\' + folder + \'/\'\n\n        if not os.path.exists(save_dir):\n            os.makedirs(save_dir)\n\n        os.system(""./ffmpeg/ffmpeg -r 1 -framerate 1 -i "" + os.path.join(tmp, \'img\') + \\\n                  ""%01d.png -vcodec libx264 -tune grain -preset veryslow -y "" + \\\n                  save_dir + img_loc[:-5] + "".mp4"")\n\n        rmtree(tmp, ignore_errors=True)\n\n        # /////////////// End Shot Noise Code ///////////////\n\n        # # /////////////// Motion Blur Code ///////////////\n        #\n        # tmp = os.path.join(gettempdir(), \'.{}\'.format(hash(os.times())))\n        # os.makedirs(tmp)\n        #\n        # for i in range(0,31):\n        #\n        #     z = trn.CenterCrop(224)(img)\n        #     output = BytesIO()\n        #     z.save(output, format=\'PNG\')\n        #     z = MotionImage(blob=output.getvalue())\n        #\n        #     z.motion_blur(radius=10, sigma=3, angle=(i-30)*4)\n        #\n        #     z = cv2.imdecode(np.fromstring(z.make_blob(), np.uint8),\n        #                      cv2.IMREAD_UNCHANGED)\n        #\n        #     if z.shape != (224, 224):\n        #         z = np.clip(z[..., [2, 1, 0]], 0, 255)  # BGR to RGB\n        #     else:  # grayscale to RGB\n        #         z = np.clip(np.array([z, z, z]).transpose((1, 2, 0)), 0, 255)\n        #\n        #     PILImage.fromarray(z).save(os.path.join(tmp, \'img\' + str(i) + \'.png\'))\n        #\n        # save_dir = \'/share/data/vision-greg2/users/dan/datasets/ImageNet-P/motion_blur/\' + folder + \'/\'\n        #\n        # if not os.path.exists(save_dir):\n        #     os.makedirs(save_dir)\n        #\n        # os.system(""./ffmpeg/ffmpeg -r 1 -framerate 1 -i "" + os.path.join(tmp, \'img\') + \\\n        #           ""%01d.png -vcodec libx264 -crf 20 -preset veryslow -y "" + \\\n        #           save_dir + img_loc[:-5] + "".mp4"")\n        #\n        # rmtree(tmp, ignore_errors=True)\n        #\n        # # /////////////// End Motion Blur Code ///////////////\n\n        # # /////////////// Zoom Blur Code ///////////////\n        #\n        # tmp = os.path.join(gettempdir(), \'.{}\'.format(hash(os.times())))\n        # os.makedirs(tmp)\n        #\n        # z = trn.CenterCrop(224)(img)\n        # avg = trn.ToTensor()(z)\n        # z.save(os.path.join(tmp, \'img0.png\'))\n        # for i in range(1, 31):\n        #     z = trn.CenterCrop(224)(trn_F.affine(img, angle=0, translate=(0, 0),\n        #                                          scale=1+0.004*i, shear=0, resample=PILImage.BILINEAR))\n        #     avg += trn.ToTensor()(z)\n        #     trn.ToPILImage()(avg / (i + 1)).save(os.path.join(tmp, \'img\' + str(i) + \'.png\'))\n        #\n        # save_dir = \'/share/data/vision-greg2/users/dan/datasets/ImageNet-P/zoom_blur/\' + folder + \'/\'\n        #\n        # if not os.path.exists(save_dir):\n        #     os.makedirs(save_dir)\n        #\n        # os.system(""./ffmpeg/ffmpeg -r 1 -framerate 1 -i "" +\n        #           os.path.join(tmp, \'img\') + ""%01d.png -vcodec libx264 -crf 20 -preset veryslow -y "" +\n        #           save_dir + img_loc[:-5] + "".mp4"")\n        #\n        # rmtree(tmp, ignore_errors=True)\n        #\n        # # /////////////// End Zoom Blur Code ///////////////\n\n        # # /////////////// Snow Code ///////////////\n        #\n        # tmp = os.path.join(gettempdir(), \'.{}\'.format(hash(os.times())))\n        # os.makedirs(tmp)\n        #\n        # x = trn.CenterCrop(224)(img)\n        # x = np.array(x) / 255.\n        #\n        # snow_layer = np.random.normal(size=(224, 224), loc=0.05, scale=0.3)\n        #\n        # snow_layer = clipped_zoom(snow_layer[..., np.newaxis], 3)\n        # snow_layer[snow_layer < 0.5] = 0\n        #\n        # snow_layer = PILImage.fromarray((np.clip(snow_layer.squeeze(), 0, 1) * 255).astype(np.uint8), mode=\'L\')\n        # output = BytesIO()\n        # snow_layer.save(output, format=\'PNG\')\n        # output = output.getvalue()\n        #\n        # for i in range(0, 31):\n        #     moving_snow = MotionImage(blob=output)\n        #     moving_snow.motion_blur(radius=14, sigma=4, angle=i*4-150)\n        #\n        #     snow_layer = cv2.imdecode(np.fromstring(moving_snow.make_blob(), np.uint8),\n        #                               cv2.IMREAD_UNCHANGED) / 255.\n        #     snow_layer = snow_layer[..., np.newaxis]\n        #\n        #     z = 0.85 * x + (1 - 0.85) * np.maximum(\n        #         x, cv2.cvtColor(np.float32(x), cv2.COLOR_RGB2GRAY).reshape(224, 224, 1) * 1.5 + 0.5)\n        #\n        #     z = np.uint8(np.clip(z + snow_layer + np.rot90(snow_layer, k=2), 0, 1) * 255)\n        #\n        #     PILImage.fromarray(z).save(os.path.join(tmp, \'img\' + str(i) + \'.png\'))\n        #\n        # save_dir = \'/share/data/vision-greg2/users/dan/datasets/ImageNet-P/snow/\' + folder + \'/\'\n        #\n        # if not os.path.exists(save_dir):\n        #     os.makedirs(save_dir)\n        #\n        # os.system(""./ffmpeg/ffmpeg -r 1 -framerate 1 -i "" + os.path.join(tmp, \'img\') + \\\n        #           ""%01d.png -vcodec libx264 -crf 20 -preset veryslow -y "" + \\\n        #           save_dir + img_loc[:-5] + "".mp4"")\n        #\n        # rmtree(tmp, ignore_errors=True)\n        #\n        # # /////////////// End Snow Code ///////////////\n\n        # # /////////////// Brightness Code ///////////////\n        #\n        # tmp = os.path.join(gettempdir(), \'.{}\'.format(hash(os.times())))\n        # os.makedirs(tmp)\n        #\n        # x = trn.CenterCrop(224)(img)\n        #\n        # for i in range(0, 31):\n        #     z = PILImage.fromarray(brightness(x, c=(i - 15) * 2 / 100.))\n        #     z.save(os.path.join(tmp, \'img\' + str(i) + \'.png\'))\n        #\n        # save_dir = \'/share/data/vision-greg2/users/dan/datasets/ImageNet-P/brightness/\' + folder + \'/\'\n        #\n        # if not os.path.exists(save_dir):\n        #     os.makedirs(save_dir)\n        #\n        # os.system(""./ffmpeg/ffmpeg -r 1 -framerate 1 -i "" + os.path.join(tmp, \'img\') + \\\n        #           ""%01d.png -vcodec libx264 -crf 20 -preset veryslow -y "" + \\\n        #           save_dir + img_loc[:-5] + "".mp4"")\n        #\n        # rmtree(tmp, ignore_errors=True)\n        #\n        # # /////////////// End Brightness Code ///////////////\n\n        # # /////////////// Translate Code ///////////////\n        #\n        # tmp = os.path.join(gettempdir(), \'.{}\'.format(hash(os.times())))\n        # os.makedirs(tmp)\n        #\n        # for i in range(0,41):\n        #     z = trn.CenterCrop(224)(trn_F.affine(img, angle=0, translate=(i-20, 0), scale=1, shear=0))\n        #     z.save(os.path.join(tmp, \'img\'+str(i)+\'.png\'))\n        #\n        # save_dir = \'/share/data/vision-greg2/users/dan/datasets/ImageNet-P/translate/\' + folder + \'/\'\n        #\n        # if not os.path.exists(save_dir):\n        #     os.makedirs(save_dir)\n        #\n        # os.system(""./ffmpeg/ffmpeg -r 1 -framerate 1 -i "" + os.path.join(tmp, \'img\') +\n        #           ""%01d.png -vcodec libx264 -crf 20 -preset veryslow -y "" +\n        #           save_dir + img_loc[:-5] + "".mp4"")\n        #\n        # rmtree(tmp, ignore_errors=True)\n        #\n        # # /////////////// End Translate Code ///////////////\n\n        # # /////////////// Rotate Code ///////////////\n        #\n        # tmp = os.path.join(gettempdir(), \'.{}\'.format(hash(os.times())))\n        # os.makedirs(tmp)\n        #\n        # for i in range(0, 31):\n        #     z = trn.CenterCrop(224)(trn_F.affine(img, angle=i-15, translate=(0, 0),\n        #                                          scale=1., shear=0, resample=PILImage.BILINEAR))\n        #     z.save(os.path.join(tmp, \'img\' + str(i) + \'.png\'))\n        #\n        # save_dir = \'/share/data/vision-greg2/users/dan/datasets/ImageNet-P/rotate/\' + folder + \'/\'\n        #\n        # if not os.path.exists(save_dir):\n        #     os.makedirs(save_dir)\n        #\n        # os.system(""./ffmpeg/ffmpeg -r 1 -framerate 1 -i "" +\n        #           os.path.join(tmp, \'img\') + ""%01d.png -vcodec libx264 -crf 20 -preset veryslow -y "" +\n        #           save_dir + img_loc[:-5] + "".mp4"")\n        #\n        # rmtree(tmp, ignore_errors=True)\n        #\n        # # /////////////// End Rotate Code ///////////////\n\n        # # /////////////// Tilt Code ///////////////\n        #\n        # tmp = os.path.join(gettempdir(), \'.{}\'.format(hash(os.times())))\n        # os.makedirs(tmp)\n        #\n        # x = np.array(img)\n        # h, w = x.shape[0:2]\n        #\n        # for i in range(0, 31):\n        #     phi, theta = np.deg2rad(i-15), np.deg2rad(i-15)\n        #\n        #     f = np.sqrt(w ** 2 + h ** 2)\n        #\n        #     P1 = np.array([[1, 0, -w / 2], [0, 1, -h / 2], [0, 0, 1], [0, 0, 1]])\n        #\n        #     RX = np.array([[1, 0, 0, 0], [0, np.cos(theta), -np.sin(theta), 0],\n        #                    [0, np.sin(theta), np.cos(theta), 0], [0, 0, 0, 1]])\n        #\n        #     RY = np.array([[np.cos(phi), 0, -np.sin(phi), 0], [0, 1, 0, 0],\n        #                    [np.sin(phi), 0, np.cos(phi), 0], [0, 0, 0, 1]])\n        #\n        #     T = np.array([[1, 0, 0, 0], [0, 1, 0, 0],\n        #                   [0, 0, 1, f], [0, 0, 0, 1]])\n        #\n        #     P2 = np.array([[f, 0, w / 2, 0], [0, f, h / 2, 0], [0, 0, 1, 0]])\n        #\n        #     mat = P2 @ T @ RX @ RY @ P1\n        #\n        #     z = trn.CenterCrop(224)(PILImage.fromarray(cv2.warpPerspective(x, mat, (w, h))))\n        #\n        #     z.save(os.path.join(tmp, \'img\' + str(i) + \'.png\'))\n        #\n        # save_dir = \'/share/data/vision-greg2/users/dan/datasets/ImageNet-P/tilt/\' + folder + \'/\'\n        #\n        # if not os.path.exists(save_dir):\n        #     os.makedirs(save_dir)\n        #\n        # os.system(""./ffmpeg/ffmpeg -r 1 -framerate 1 -i "" + os.path.join(tmp, \'img\') + \\\n        #           ""%01d.png -vcodec libx264 -crf 20 -preset veryslow -y "" + \\\n        #           save_dir + img_loc[:-5] + "".mp4"")\n        #\n        # rmtree(tmp, ignore_errors=True)\n        #\n        # # /////////////// End Tilt Code ///////////////\n\n        # # /////////////// Scale Code ///////////////\n        #\n        # tmp = os.path.join(gettempdir(), \'.{}\'.format(hash(os.times())))\n        # os.makedirs(tmp)\n        #\n        # for i in range(0, 31):\n        #     z = trn.CenterCrop(224)(trn_F.affine(img, angle=0, translate=(0, 0),\n        #                                          scale=(i * 2.5 + 40) / 100., shear=0, resample=PILImage.BILINEAR))\n        #     z.save(os.path.join(tmp, \'img\' + str(i) + \'.png\'))\n        #\n        # save_dir = \'/share/data/vision-greg2/users/dan/datasets/ImageNet-P/scale/\' + folder + \'/\'\n        #\n        # if not os.path.exists(save_dir):\n        #     os.makedirs(save_dir)\n        #\n        # os.system(""./ffmpeg/ffmpeg -r 1 -framerate 1 -i "" +\n        #           os.path.join(tmp, \'img\') + ""%01d.png -vcodec libx264 -crf 20 -preset veryslow -y "" +\n        #           save_dir + img_loc[:-5] + "".mp4"")\n        #\n        # rmtree(tmp, ignore_errors=True)\n        #\n        # # /////////////// End Scale Code ///////////////\n\n        # /////////////// Validation Data ///////////////\n\n        # # /////////////// Speckle Noise Code ///////////////\n        #\n        # tmp = os.path.join(gettempdir(), \'.{}\'.format(hash(os.times())))\n        # os.makedirs(tmp)\n        #\n        # x = trn.CenterCrop(224)(img)\n        #\n        # x.save(os.path.join(tmp, \'img0.png\'))\n        # x = np.array(x) / 255.\n        #\n        # for i in range(1, 31):\n        #     z = PILImage.fromarray(\n        #         np.uint8(255 * np.clip(x + x * np.random.normal(size=x.shape, scale=0.05), 0, 1)))\n        #     z.save(os.path.join(tmp, \'img\' + str(i) + \'.png\'))\n        # save_dir = \'/share/data/vision-greg2/users/dan/datasets/ImageNet-P/speckle_noise/\' + folder + \'/\'\n        #\n        # # for i in range(1, 31):\n        # #     z = PILImage.fromarray(\n        # #         np.uint8(255 * np.clip(x + x * np.random.normal(size=x.shape, scale=0.10), 0, 1)))\n        # #     z.save(os.path.join(tmp, \'img\' + str(i) + \'.png\'))\n        # # save_dir = \'/share/data/vision-greg2/users/dan/datasets/ImageNet-P/speckle_noise_2/\' + folder + \'/\'\n        #\n        # # for i in range(1, 31):\n        # #     z = PILImage.fromarray(\n        # #         np.uint8(255 * np.clip(x + x * np.random.normal(size=x.shape, scale=0.15), 0, 1)))\n        # #     z.save(os.path.join(tmp, \'img\' + str(i) + \'.png\'))\n        # # save_dir = \'/share/data/vision-greg2/users/dan/datasets/ImageNet-P/speckle_noise_3/\' + folder + \'/\'\n        #\n        # if not os.path.exists(save_dir):\n        #     os.makedirs(save_dir)\n        #\n        # os.system(""./ffmpeg/ffmpeg -r 1 -framerate 1 -i "" + os.path.join(tmp, \'img\') + \\\n        #           ""%01d.png -vcodec libx264 -tune grain -preset veryslow -y "" + \\\n        #           save_dir + img_loc[:-5] + "".mp4"")\n        #\n        # rmtree(tmp, ignore_errors=True)\n        #\n        # # /////////////// End Speckle Noise Code ///////////////\n\n        # # /////////////// Gaussian Blur Code ///////////////\n        #\n        # tmp = os.path.join(gettempdir(), \'.{}\'.format(hash(os.times())))\n        # os.makedirs(tmp)\n        #\n        # x = trn.CenterCrop(224)(img)\n        # for i in range(0, 31):\n        #     z = PILImage.fromarray(\n        #         np.uint8(255*gaussian(np.array(x, copy=True)/255.,\n        #                               sigma=0.25 + 0.035*i, multichannel=True, truncate=6.0)))\n        #\n        #     z.save(os.path.join(tmp, \'img\' + str(i) + \'.png\'))\n        #\n        # save_dir = \'/share/data/vision-greg2/users/dan/datasets/ImageNet-P/gaussian_blur/\' + folder + \'/\'\n        #\n        # if not os.path.exists(save_dir):\n        #     os.makedirs(save_dir)\n        #\n        # os.system(""./ffmpeg/ffmpeg -r 1 -framerate 1 -i "" +\n        #           os.path.join(tmp, \'img\') + ""%01d.png -vcodec libx264 -crf 20 -preset veryslow -y "" +\n        #           save_dir + img_loc[:-5] + "".mp4"")\n        #\n        # rmtree(tmp, ignore_errors=True)\n        #\n        # # /////////////// End Gaussian Blur Code ///////////////\n\n        # # /////////////// Spatter Code ///////////////\n        #\n        # tmp = os.path.join(gettempdir(), \'.{}\'.format(hash(os.times())))\n        # os.makedirs(tmp)\n        #\n        # x = trn.CenterCrop(224)(img)\n        # x = cv2.cvtColor(np.array(x, dtype=np.float32) / 255., cv2.COLOR_BGR2BGRA)\n        #\n        # liquid_layer = np.random.normal(size=x.shape[:2], loc=0.65, scale=0.27)\n        # liquid_layer = gaussian(liquid_layer, sigma=3.7)\n        # liquid_layer[liquid_layer < 0.69] = 0\n        #\n        # for i in range(0, 31):\n        #\n        #     liquid_layer_i = (liquid_layer * 255).astype(np.uint8)\n        #     dist = 255 - cv2.Canny(liquid_layer_i, 50, 150)\n        #     dist = cv2.distanceTransform(dist, cv2.DIST_L2, 5)\n        #     _, dist = cv2.threshold(dist, 20, 20, cv2.THRESH_TRUNC)\n        #     dist = cv2.blur(dist, (3, 3)).astype(np.uint8)\n        #     dist = cv2.equalizeHist(dist)\n        #     ker = np.array([[-2, -1, 0], [-1, 1, 1], [0, 1, 2]])\n        #     dist = cv2.filter2D(dist, cv2.CV_8U, ker)\n        #     dist = cv2.blur(dist, (3, 3)).astype(np.float32)\n        #\n        #     m = cv2.cvtColor(liquid_layer_i * dist, cv2.COLOR_GRAY2BGRA)\n        #     m /= np.max(m, axis=(0, 1))\n        #     m *= 0.6\n        #\n        #     # water is pale turqouise\n        #     color = np.concatenate((175 / 255. * np.ones_like(m[..., :1]),\n        #                             238 / 255. * np.ones_like(m[..., :1]),\n        #                             238 / 255. * np.ones_like(m[..., :1])), axis=2)\n        #\n        #     color = cv2.cvtColor(color, cv2.COLOR_BGR2BGRA)\n        #\n        #     z = np.uint8(cv2.cvtColor(np.clip(x + m * color, 0, 1), cv2.COLOR_BGRA2BGR) * 255)\n        #\n        #     liquid_layer = np.apply_along_axis(lambda mat:\n        #                                        np.convolve(mat, np.array([0.05, 0.1, 0.15, 0.7]), mode=\'same\'),\n        #                                        axis=0, arr=liquid_layer)\n        #\n        #     PILImage.fromarray(z).save(os.path.join(tmp, \'img\' + str(i) + \'.png\'))\n        #\n        # save_dir = \'/share/data/vision-greg2/users/dan/datasets/ImageNet-P/spatter/\' + folder + \'/\'\n        #\n        # if not os.path.exists(save_dir):\n        #     os.makedirs(save_dir)\n        #\n        # os.system(""./ffmpeg/ffmpeg -r 1 -framerate 1 -i "" + os.path.join(tmp, \'img\') + \\\n        #           ""%01d.png -vcodec libx264 -crf 20 -preset veryslow -y "" + \\\n        #           save_dir + img_loc[:-5] + "".mp4"")\n        #\n        # rmtree(tmp, ignore_errors=True)\n        #\n        # # /////////////// End Spatter Code ///////////////\n\n        # # /////////////// Shear Code ///////////////\n        #\n        # tmp = os.path.join(gettempdir(), \'.{}\'.format(hash(os.times())))\n        # os.makedirs(tmp)\n        #\n        # for i in range(0, 31):\n        #     z = trn.CenterCrop(224)(trn_F.affine(img, angle=0, translate=(0, 0),\n        #                                          scale=1., shear=i-15, resample=PILImage.BILINEAR))\n        #     z.save(os.path.join(tmp, \'img\' + str(i) + \'.png\'))\n        #\n        # save_dir = \'/share/data/vision-greg2/users/dan/datasets/ImageNet-P/shear/\' + folder + \'/\'\n        #\n        # if not os.path.exists(save_dir):\n        #     os.makedirs(save_dir)\n        #\n        # os.system(""./ffmpeg/ffmpeg -r 1 -framerate 1 -i "" +\n        #           os.path.join(tmp, \'img\') + ""%01d.png -vcodec libx264 -crf 20 -preset veryslow -y "" +\n        #           save_dir + img_loc[:-5] + "".mp4"")\n        #\n        # rmtree(tmp, ignore_errors=True)\n        #\n        # # /////////////// End Shear Code ///////////////\n'"
ImageNet-P/create_p/make_imagenet_p_inception.py,3,"b'import os\nimport numpy as np\nimport torch\nimport torchvision.transforms as trn\nimport torchvision.transforms.functional as trn_F\nimport cv2\nfrom PIL import Image as PILImage\nimport skimage.color as skcolor\nfrom skimage.util import random_noise\nimport numbers\nfrom wand.image import Image as WandImage\nfrom wand.api import library as wandlibrary\nfrom io import BytesIO\nimport ctypes\nfrom scipy.ndimage import zoom as scizoom\nfrom skimage.filters import gaussian\nfrom tempfile import gettempdir\nfrom shutil import rmtree\n\nimagenet_val_folder = \'/share/data/vision-greg/ImageNet/clsloc/images/val/\'\n\n# Tell Python about the C method\nwandlibrary.MagickMotionBlurImage.argtypes = (ctypes.c_void_p,  # wand\n                                              ctypes.c_double,  # radius\n                                              ctypes.c_double,  # sigma\n                                              ctypes.c_double)  # angle\n\n\n# Extend wand.image.Image class to include method signature\nclass MotionImage(WandImage):\n    def motion_blur(self, radius=0.0, sigma=0.0, angle=0.0):\n        wandlibrary.MagickMotionBlurImage(self.wand, radius, sigma, angle)\n\n\ndef clipped_zoom(img, zoom_factor):\n    h = img.shape[0]\n    # ceil crop height(= crop width)\n    ch = int(np.ceil(h / zoom_factor))\n\n    top = (h - ch) // 2\n    img = scizoom(img[top:top + ch, top:top + ch], (zoom_factor, zoom_factor, 1), order=1)\n    # trim off any extra pixels\n    trim_top = (img.shape[0] - h) // 2\n\n    return img[trim_top:trim_top + h, trim_top:trim_top + h]\n\n\ndef brightness(_x, c=0.):\n    _x = np.array(_x, copy=True) / 255.\n    _x = skcolor.rgb2hsv(_x)\n    _x[:, :, 2] = np.clip(_x[:, :, 2] + c, 0, 1)\n    _x = skcolor.hsv2rgb(_x)\n\n    return np.uint8(_x * 255)\n\n\ndef resize(img, size, interpolation=PILImage.BILINEAR):\n    if isinstance(size, int):\n        w, h = img.size\n        if (w <= h and w == size) or (h <= w and h == size):\n            return img\n        if w < h:\n            ow = size\n            oh = int(size * h / w)\n            return img.resize((ow, oh), interpolation)\n        else:\n            oh = size\n            ow = int(size * w / h)\n            return img.resize((ow, oh), interpolation)\n    else:\n        return img.resize(size[::-1], interpolation)\n\n\ndef center_crop(img, output_size):\n    w, h = img.size\n    # Case of single value provided\n    if isinstance(output_size, numbers.Number):\n        # Float case: constraint for fraction must be from 0 to 1.0\n        if isinstance(output_size, float):\n            if not 0.0 < output_size <= 1.0:\n                raise ValueError(""Invalid float output size. Range is (0.0, 1.0]"")\n            output_size = (output_size, output_size)\n            th, tw = int(h * output_size[0]), int(w * output_size[1])\n        elif isinstance(output_size, int):\n            output_size = (output_size, output_size)\n            th, tw = output_size\n    # Case of tuple of values provided\n    else:\n        if isinstance(output_size[0], float):\n            th, tw = int(h * output_size[0]), int(w * output_size[1])\n        elif isinstance(output_size[0], int):\n            th, tw = output_size\n\n    i = int(round((h - th) / 2.))\n    j = int(round((w - tw) / 2.))\n    return img.crop((j, i, j + tw, i + th))\n\n\ndef resized_center_crop(img, scale=0.875, size=(299, 299), interpolation=PILImage.BILINEAR):\n    img = center_crop(img, scale)\n    img = resize(img, size, interpolation)\n\n    return img\n\n\nfor index, folder in enumerate(sorted(os.listdir(imagenet_val_folder))):\n\n    for img_loc in os.listdir(imagenet_val_folder + folder):\n        img = PILImage.open(imagenet_val_folder + folder + \'/\' + img_loc).convert(\'RGB\')\n\n        # /////////////// Test Data ///////////////\n\n        # # /////////////// Gaussian Noise Code ///////////////\n        #\n        # tmp = os.path.join(gettempdir(), \'.{}\'.format(hash(os.times())))\n        # os.makedirs(tmp)\n        #\n        # x = trn.Compose([lambda image: resized_center_crop(image), trn.ToTensor()])(img)\n        #\n        # z = trn.ToPILImage()(x)\n        # z.save(os.path.join(tmp, \'img0.png\'))\n        #\n        # for i in range(1, 31):\n        #     z = trn.ToPILImage()(torch.clamp(x + 0.035 * torch.randn_like(x), 0, 1))  # 0.03 -> 0.035\n        #     z.save(os.path.join(tmp, \'img\' + str(i) + \'.png\'))\n        # save_dir = \'/share/data/lang/users/dan/ImageNet-P-299/gaussian_noise/\' + folder + \'/\'\n        #\n        # # for i in range(1, 31):\n        # #     z = trn.ToPILImage()(torch.clamp(x + 0.055 * torch.randn_like(x), 0, 1))\n        # #     z.save(os.path.join(tmp, \'img\' + str(i) + \'.png\'))\n        # # save_dir = \'/share/data/lang/users/dan/ImageNet-P-299/gaussian_noise_2/\' + folder + \'/\'\n        #\n        # # for i in range(1, 31):\n        # #     z = trn.ToPILImage()(torch.clamp(x + 0.085 * torch.randn_like(x), 0, 1))\n        # #     z.save(os.path.join(tmp, \'img\' + str(i) + \'.png\'))\n        # # save_dir = \'/share/data/lang/users/dan/ImageNet-P-299/gaussian_noise_3/\' + folder + \'/\'\n        #\n        # if not os.path.exists(save_dir):\n        #     os.makedirs(save_dir)\n        #\n        # os.system(""./ffmpeg/ffmpeg -r 1 -framerate 1 -i "" + os.path.join(tmp, \'img\') + \\\n        #           ""%01d.png -vcodec libx264 -tune grain -preset veryslow -y "" + \\\n        #           save_dir + img_loc[:-5] + "".mp4"")\n        #\n        # rmtree(tmp, ignore_errors=True)\n        #\n        # # /////////////// End Gaussian Noise Code ///////////////\n\n        # # /////////////// Shot Noise Code ///////////////\n        #\n        # tmp = os.path.join(gettempdir(), \'.{}\'.format(hash(os.times())))\n        # os.makedirs(tmp)\n        #\n        # x = resized_center_crop(img)\n        #\n        # x.save(os.path.join(tmp, \'img0.png\'))\n        #\n        # for i in range(1, 31):\n        #     z = np.array(x, copy=True) / 255.\n        #     z = PILImage.fromarray(\n        #         np.uint8(255 * np.clip(np.random.poisson(z * 250) / 250., 0, 1)))     # 300 -> 250\n        #     z.save(os.path.join(tmp, \'img\' + str(i) + \'.png\'))\n        # save_dir = \'/share/data/lang/users/dan/ImageNet-P-299/shot_noise/\' + folder + \'/\'\n        #\n        # # for i in range(1, 31):\n        # #     z = np.array(x, copy=True) / 255.\n        # #     z = PILImage.fromarray(\n        # #         np.uint8(255 * np.clip(np.random.poisson(z * 100) / 100., 0, 1)))     # 120 -> 100\n        # #     z.save(os.path.join(tmp, \'img\' + str(i) + \'.png\'))\n        # # save_dir = \'/share/data/lang/users/dan/ImageNet-P-299/shot_noise_2/\' + folder + \'/\'\n        #\n        # # for i in range(1, 31):\n        # #     z = np.array(x, copy=True) / 255.\n        # #     z = PILImage.fromarray(\n        # #         np.uint8(255 * np.clip(np.random.poisson(z * 50) / 50., 0, 1)))     # 60 -> 50\n        # #     z.save(os.path.join(tmp, \'img\' + str(i) + \'.png\'))\n        # # save_dir = \'/share/data/lang/users/dan/ImageNet-P-299/shot_noise_3/\' + folder + \'/\'\n        #\n        # if not os.path.exists(save_dir):\n        #     os.makedirs(save_dir)\n        #\n        # os.system(""./ffmpeg/ffmpeg -r 1 -framerate 1 -i "" + os.path.join(tmp, \'img\') + \\\n        #           ""%01d.png -vcodec libx264 -tune grain -preset veryslow -y "" + \\\n        #           save_dir + img_loc[:-5] + "".mp4"")\n        #\n        # rmtree(tmp, ignore_errors=True)\n        #\n        # # /////////////// End Shot Noise Code ///////////////\n\n        # # /////////////// Motion Blur Code ///////////////\n        #\n        # tmp = os.path.join(gettempdir(), \'.{}\'.format(hash(os.times())))\n        # os.makedirs(tmp)\n        #\n        # img_resized = resized_center_crop(img)\n        # for i in range(0,31):\n        #\n        #     z = img_resized.copy()\n        #     output = BytesIO()\n        #     z.save(output, format=\'PNG\')\n        #     z = MotionImage(blob=output.getvalue())\n        #\n        #     z.motion_blur(radius=13, sigma=3, angle=(i-30)*4)   # bumping from 10 to 13 since images are larger\n        #\n        #     z = cv2.imdecode(np.fromstring(z.make_blob(), np.uint8),\n        #                      cv2.IMREAD_UNCHANGED)\n        #\n        #     if z.shape != (299, 299):\n        #         z = np.clip(z[..., [2, 1, 0]], 0, 255)  # BGR to RGB\n        #     else:  # grayscale to RGB\n        #         z = np.clip(np.array([z, z, z]).transpose((1, 2, 0)), 0, 255)\n        #\n        #     PILImage.fromarray(z).save(os.path.join(tmp, \'img\' + str(i) + \'.png\'))\n        #\n        # save_dir = \'/share/data/lang/users/dan/ImageNet-P-299/motion_blur/\' + folder + \'/\'\n        #\n        # if not os.path.exists(save_dir):\n        #     os.makedirs(save_dir)\n        #\n        # os.system(""./ffmpeg/ffmpeg -r 1 -framerate 1 -i "" + os.path.join(tmp, \'img\') + \\\n        #           ""%01d.png -vcodec libx264 -crf 20 -preset veryslow -y "" + \\\n        #           save_dir + img_loc[:-5] + "".mp4"")\n        #\n        # rmtree(tmp, ignore_errors=True)\n        #\n        # # /////////////// End Motion Blur Code ///////////////\n\n        # # /////////////// Zoom Blur Code ///////////////\n        #\n        # tmp = os.path.join(gettempdir(), \'.{}\'.format(hash(os.times())))\n        # os.makedirs(tmp)\n        #\n        # img_cropped = resized_center_crop(img)\n        # z = img_cropped\n        # avg = trn.ToTensor()(z)\n        # z.save(os.path.join(tmp, \'img0.png\'))\n        # for i in range(1, 31):\n        #     z = trn.CenterCrop(299)(trn_F.affine(img_cropped, angle=0, translate=(0, 0),\n        #                                          scale=1+0.004*i, shear=0, resample=PILImage.BILINEAR))\n        #     avg += trn.ToTensor()(z)\n        #     trn.ToPILImage()(avg / (i + 1)).save(os.path.join(tmp, \'img\' + str(i) + \'.png\'))\n        #\n        # save_dir = \'/share/data/lang/users/dan/ImageNet-P-299/zoom_blur/\' + folder + \'/\'\n        #\n        # if not os.path.exists(save_dir):\n        #     os.makedirs(save_dir)\n        #\n        # os.system(""./ffmpeg/ffmpeg -r 1 -framerate 1 -i "" +\n        #           os.path.join(tmp, \'img\') + ""%01d.png -vcodec libx264 -crf 20 -preset veryslow -y "" +\n        #           save_dir + img_loc[:-5] + "".mp4"")\n        #\n        # rmtree(tmp, ignore_errors=True)\n        #\n        # # /////////////// End Zoom Blur Code ///////////////\n\n        # /////////////// Snow Code ///////////////\n\n        tmp = os.path.join(gettempdir(), \'.{}\'.format(hash(os.times())))\n        os.makedirs(tmp)\n\n        x = resized_center_crop(img)\n        x = np.array(x) / 255.\n\n        snow_layer = np.random.normal(size=(299, 299), loc=0.05, scale=0.3)\n\n        snow_layer = clipped_zoom(snow_layer[..., np.newaxis], 3)\n        snow_layer[snow_layer < 0.5] = 0\n\n        snow_layer = PILImage.fromarray((np.clip(snow_layer.squeeze(), 0, 1) * 255).astype(np.uint8), mode=\'L\')\n        output = BytesIO()\n        snow_layer.save(output, format=\'PNG\')\n        output = output.getvalue()\n\n        for i in range(0, 31):\n            moving_snow = MotionImage(blob=output)\n            moving_snow.motion_blur(radius=14, sigma=4, angle=i*4-150)\n\n            snow_layer = cv2.imdecode(np.fromstring(moving_snow.make_blob(), np.uint8),\n                                      cv2.IMREAD_UNCHANGED) / 255.\n            snow_layer = snow_layer[..., np.newaxis]\n\n            z = 0.85 * x + (1 - 0.85) * np.maximum(\n                x, cv2.cvtColor(np.float32(x), cv2.COLOR_RGB2GRAY).reshape(299, 299, 1) * 1.5 + 0.5)\n\n            z = np.uint8(np.clip(z + snow_layer + np.rot90(snow_layer, k=2), 0, 1) * 255)\n\n            PILImage.fromarray(z).save(os.path.join(tmp, \'img\' + str(i) + \'.png\'))\n\n        save_dir = \'/share/data/lang/users/dan/ImageNet-P-299/snow/\' + folder + \'/\'\n\n        if not os.path.exists(save_dir):\n            os.makedirs(save_dir)\n\n        os.system(""./ffmpeg/ffmpeg -r 1 -framerate 1 -i "" + os.path.join(tmp, \'img\') + \\\n                  ""%01d.png -vcodec libx264 -crf 20 -preset veryslow -y "" + \\\n                  save_dir + img_loc[:-5] + "".mp4"")\n\n        rmtree(tmp, ignore_errors=True)\n\n        # /////////////// End Snow Code ///////////////\n\n        # # /////////////// Brightness Code ///////////////\n        #\n        # tmp = os.path.join(gettempdir(), \'.{}\'.format(hash(os.times())))\n        # os.makedirs(tmp)\n        #\n        # x = resized_center_crop(img)\n        #\n        # for i in range(0, 31):\n        #     z = PILImage.fromarray(brightness(x, c=(i - 15) * 2 / 100.))\n        #     z.save(os.path.join(tmp, \'img\' + str(i) + \'.png\'))\n        #\n        # save_dir = \'/share/data/lang/users/dan/ImageNet-P-299/brightness/\' + folder + \'/\'\n        #\n        # if not os.path.exists(save_dir):\n        #     os.makedirs(save_dir)\n        #\n        # os.system(""./ffmpeg/ffmpeg -r 1 -framerate 1 -i "" + os.path.join(tmp, \'img\') + \\\n        #           ""%01d.png -vcodec libx264 -crf 20 -preset veryslow -y "" + \\\n        #           save_dir + img_loc[:-5] + "".mp4"")\n        #\n        # rmtree(tmp, ignore_errors=True)\n        #\n        # # /////////////// End Brightness Code ///////////////\n\n        # # /////////////// Translate Code ///////////////\n        #\n        # tmp = os.path.join(gettempdir(), \'.{}\'.format(hash(os.times())))\n        # os.makedirs(tmp)\n        #\n        # for i in range(0,41):\n        #     z = resized_center_crop(trn_F.affine(img, angle=0, translate=(i-20, 0), scale=1, shear=0))\n        #     z.save(os.path.join(tmp, \'img\'+str(i)+\'.png\'))\n        #\n        # save_dir = \'/share/data/lang/users/dan/ImageNet-P-299/translate/\' + folder + \'/\'\n        #\n        # if not os.path.exists(save_dir):\n        #     os.makedirs(save_dir)\n        #\n        # os.system(""./ffmpeg/ffmpeg -r 1 -framerate 1 -i "" + os.path.join(tmp, \'img\') +\n        #           ""%01d.png -vcodec libx264 -crf 20 -preset veryslow -y "" +\n        #           save_dir + img_loc[:-5] + "".mp4"")\n        #\n        # rmtree(tmp, ignore_errors=True)\n        #\n        # # /////////////// End Translate Code ///////////////\n\n        # # /////////////// Rotate Code ///////////////\n        #\n        # tmp = os.path.join(gettempdir(), \'.{}\'.format(hash(os.times())))\n        # os.makedirs(tmp)\n        #\n        # for i in range(0, 31):\n        #     z = resized_center_crop(trn_F.affine(img, angle=i-15, translate=(0, 0),\n        #                                          scale=1., shear=0, resample=PILImage.BILINEAR))\n        #     z.save(os.path.join(tmp, \'img\' + str(i) + \'.png\'))\n        #\n        # save_dir = \'/share/data/lang/users/dan/ImageNet-P-299/rotate/\' + folder + \'/\'\n        #\n        # if not os.path.exists(save_dir):\n        #     os.makedirs(save_dir)\n        #\n        # os.system(""./ffmpeg/ffmpeg -r 1 -framerate 1 -i "" +\n        #           os.path.join(tmp, \'img\') + ""%01d.png -vcodec libx264 -crf 20 -preset veryslow -y "" +\n        #           save_dir + img_loc[:-5] + "".mp4"")\n        #\n        # rmtree(tmp, ignore_errors=True)\n        #\n        # # /////////////// End Rotate Code ///////////////\n\n        # # /////////////// Tilt Code ///////////////\n        #\n        # tmp = os.path.join(gettempdir(), \'.{}\'.format(hash(os.times())))\n        # os.makedirs(tmp)\n        #\n        # x = np.array(img)\n        # h, w = x.shape[0:2]\n        #\n        # for i in range(0, 31):\n        #     phi, theta = np.deg2rad(i-15), np.deg2rad(i-15)\n        #\n        #     f = np.sqrt(w ** 2 + h ** 2)\n        #\n        #     P1 = np.array([[1, 0, -w / 2], [0, 1, -h / 2], [0, 0, 1], [0, 0, 1]])\n        #\n        #     RX = np.array([[1, 0, 0, 0], [0, np.cos(theta), -np.sin(theta), 0],\n        #                    [0, np.sin(theta), np.cos(theta), 0], [0, 0, 0, 1]])\n        #\n        #     RY = np.array([[np.cos(phi), 0, -np.sin(phi), 0], [0, 1, 0, 0],\n        #                    [np.sin(phi), 0, np.cos(phi), 0], [0, 0, 0, 1]])\n        #\n        #     T = np.array([[1, 0, 0, 0], [0, 1, 0, 0],\n        #                   [0, 0, 1, f], [0, 0, 0, 1]])\n        #\n        #     P2 = np.array([[f, 0, w / 2, 0], [0, f, h / 2, 0], [0, 0, 1, 0]])\n        #\n        #     mat = P2 @ T @ RX @ RY @ P1\n        #\n        #     z = resized_center_crop(PILImage.fromarray(cv2.warpPerspective(x, mat, (w, h))))\n        #\n        #     z.save(os.path.join(tmp, \'img\' + str(i) + \'.png\'))\n        #\n        # save_dir = \'/share/data/lang/users/dan/ImageNet-P-299/tilt/\' + folder + \'/\'\n        #\n        # if not os.path.exists(save_dir):\n        #     os.makedirs(save_dir)\n        #\n        # os.system(""./ffmpeg/ffmpeg -r 1 -framerate 1 -i "" + os.path.join(tmp, \'img\') + \\\n        #           ""%01d.png -vcodec libx264 -crf 20 -preset veryslow -y "" + \\\n        #           save_dir + img_loc[:-5] + "".mp4"")\n        #\n        # rmtree(tmp, ignore_errors=True)\n        #\n        # # /////////////// End Tilt Code ///////////////\n\n        # # /////////////// Scale Code ///////////////\n        #\n        # tmp = os.path.join(gettempdir(), \'.{}\'.format(hash(os.times())))\n        # os.makedirs(tmp)\n        #\n        # x = resized_center_crop(img)\n        # for i in range(0, 31):\n        #     z = trn.CenterCrop(299)(trn_F.affine(x, angle=0, translate=(0, 0),\n        #                                          scale=(i * 2.5 + 40) / 100., shear=0, resample=PILImage.BILINEAR))\n        #     z.save(os.path.join(tmp, \'img\' + str(i) + \'.png\'))\n        #\n        # save_dir = \'/share/data/lang/users/dan/ImageNet-P-299/scale/\' + folder + \'/\'\n        #\n        # if not os.path.exists(save_dir):\n        #     os.makedirs(save_dir)\n        #\n        # os.system(""./ffmpeg/ffmpeg -r 1 -framerate 1 -i "" +\n        #           os.path.join(tmp, \'img\') + ""%01d.png -vcodec libx264 -crf 20 -preset veryslow -y "" +\n        #           save_dir + img_loc[:-5] + "".mp4"")\n        #\n        # rmtree(tmp, ignore_errors=True)\n        #\n        # # /////////////// End Scale Code ///////////////\n\n        # # /////////////// Validation Data ///////////////\n\n        # # /////////////// Speckle Noise Code ///////////////\n        #\n        # tmp = os.path.join(gettempdir(), \'.{}\'.format(hash(os.times())))\n        # os.makedirs(tmp)\n        #\n        # x = resized_center_crop(img)\n        #\n        # x.save(os.path.join(tmp, \'img0.png\'))\n        # x = np.array(x) / 255.\n        #\n        # for i in range(1, 31):\n        #     z = PILImage.fromarray(\n        #         np.uint8(255 * np.clip(x + x * np.random.normal(size=x.shape, scale=0.06), 0, 1)))    # 0.05 -> 0.06\n        #     z.save(os.path.join(tmp, \'img\' + str(i) + \'.png\'))\n        # save_dir = \'/share/data/lang/users/dan/ImageNet-P-299/speckle_noise/\' + folder + \'/\'\n        #\n        # # for i in range(1, 31):\n        # #     z = PILImage.fromarray(\n        # #         np.uint8(255 * np.clip(x + x * np.random.normal(size=x.shape, scale=0.11), 0, 1)))\n        # #     z.save(os.path.join(tmp, \'img\' + str(i) + \'.png\'))\n        # # save_dir = \'/share/data/lang/users/dan/ImageNet-P-299/speckle_noise_2/\' + folder + \'/\'\n        #\n        # # for i in range(1, 31):\n        # #     z = PILImage.fromarray(\n        # #         np.uint8(255 * np.clip(x + x * np.random.normal(size=x.shape, scale=0.16), 0, 1)))\n        # #     z.save(os.path.join(tmp, \'img\' + str(i) + \'.png\'))\n        # # save_dir = \'/share/data/lang/users/dan/ImageNet-P-299/speckle_noise_3/\' + folder + \'/\'\n        #\n        # if not os.path.exists(save_dir):\n        #     os.makedirs(save_dir)\n        #\n        # os.system(""./ffmpeg/ffmpeg -r 1 -framerate 1 -i "" + os.path.join(tmp, \'img\') + \\\n        #           ""%01d.png -vcodec libx264 -tune grain -preset veryslow -y "" + \\\n        #           save_dir + img_loc[:-5] + "".mp4"")\n        #\n        # rmtree(tmp, ignore_errors=True)\n        #\n        # # /////////////// End Speckle Noise Code ///////////////\n\n        # # /////////////// Gaussian Blur Code ///////////////\n        #\n        # tmp = os.path.join(gettempdir(), \'.{}\'.format(hash(os.times())))\n        # os.makedirs(tmp)\n        #\n        # x = resized_center_crop(img)\n        # for i in range(0, 31):\n        #     # 0.25 -> 0.35, as the images are larger than 224\n        #     z = PILImage.fromarray(\n        #         np.uint8(255*gaussian(np.array(x, copy=True)/255.,\n        #                               sigma=0.35 + 0.035*i, multichannel=True, truncate=6.0)))\n        #\n        #     z.save(os.path.join(tmp, \'img\' + str(i) + \'.png\'))\n        #\n        # save_dir = \'/share/data/lang/users/dan/ImageNet-P-299/gaussian_blur/\' + folder + \'/\'\n        #\n        # if not os.path.exists(save_dir):\n        #     os.makedirs(save_dir)\n        #\n        # os.system(""./ffmpeg/ffmpeg -r 1 -framerate 1 -i "" +\n        #           os.path.join(tmp, \'img\') + ""%01d.png -vcodec libx264 -crf 20 -preset veryslow -y "" +\n        #           save_dir + img_loc[:-5] + "".mp4"")\n        #\n        # rmtree(tmp, ignore_errors=True)\n        #\n        # # /////////////// End Gaussian Blur Code ///////////////\n\n        # # /////////////// Spatter Code ///////////////\n        #\n        # tmp = os.path.join(gettempdir(), \'.{}\'.format(hash(os.times())))\n        # os.makedirs(tmp)\n        #\n        # x = resized_center_crop(img)\n        # x = cv2.cvtColor(np.array(x, dtype=np.float32) / 255., cv2.COLOR_BGR2BGRA)\n        #\n        # liquid_layer = np.random.normal(size=x.shape[:2], loc=0.65, scale=0.3)     # 0.27 -> 0.3\n        # liquid_layer = gaussian(liquid_layer, sigma=4)  # 3.7 -> 4\n        # liquid_layer[liquid_layer < 0.69] = 0\n        #\n        # for i in range(0, 31):\n        #\n        #     liquid_layer_i = (liquid_layer * 255).astype(np.uint8)\n        #     dist = 255 - cv2.Canny(liquid_layer_i, 50, 150)\n        #     dist = cv2.distanceTransform(dist, cv2.DIST_L2, 5)\n        #     _, dist = cv2.threshold(dist, 20, 20, cv2.THRESH_TRUNC)\n        #     dist = cv2.blur(dist, (3, 3)).astype(np.uint8)\n        #     dist = cv2.equalizeHist(dist)\n        #     ker = np.array([[-2, -1, 0], [-1, 1, 1], [0, 1, 2]])\n        #     dist = cv2.filter2D(dist, cv2.CV_8U, ker)\n        #     dist = cv2.blur(dist, (3, 3)).astype(np.float32)\n        #\n        #     m = cv2.cvtColor(liquid_layer_i * dist, cv2.COLOR_GRAY2BGRA)\n        #     m /= np.max(m, axis=(0, 1))\n        #     m *= 0.6\n        #\n        #     # water is pale turqouise\n        #     color = np.concatenate((175 / 255. * np.ones_like(m[..., :1]),\n        #                             238 / 255. * np.ones_like(m[..., :1]),\n        #                             238 / 255. * np.ones_like(m[..., :1])), axis=2)\n        #\n        #     color = cv2.cvtColor(color, cv2.COLOR_BGR2BGRA)\n        #\n        #     z = np.uint8(cv2.cvtColor(np.clip(x + m * color, 0, 1), cv2.COLOR_BGRA2BGR) * 255)\n        #\n        #     liquid_layer = np.apply_along_axis(lambda mat:\n        #                                        np.convolve(mat, np.array([0.05, 0.1, 0.15, 0.7]), mode=\'same\'),\n        #                                        axis=0, arr=liquid_layer)\n        #\n        #     PILImage.fromarray(z).save(os.path.join(tmp, \'img\' + str(i) + \'.png\'))\n        #\n        # save_dir = \'/share/data/lang/users/dan/ImageNet-P-299/spatter/\' + folder + \'/\'\n        #\n        # if not os.path.exists(save_dir):\n        #     os.makedirs(save_dir)\n        #\n        # os.system(""./ffmpeg/ffmpeg -r 1 -framerate 1 -i "" + os.path.join(tmp, \'img\') + \\\n        #           ""%01d.png -vcodec libx264 -crf 20 -preset veryslow -y "" + \\\n        #           save_dir + img_loc[:-5] + "".mp4"")\n        #\n        # rmtree(tmp, ignore_errors=True)\n        #\n        # # /////////////// End Spatter Code ///////////////\n\n        # # /////////////// Shear Code ///////////////\n        #\n        # tmp = os.path.join(gettempdir(), \'.{}\'.format(hash(os.times())))\n        # os.makedirs(tmp)\n        #\n        # for i in range(0, 31):\n        #     z = resized_center_crop(trn_F.affine(img, angle=0, translate=(0, 0),\n        #                                          scale=1., shear=i-15, resample=PILImage.BILINEAR))\n        #     z.save(os.path.join(tmp, \'img\' + str(i) + \'.png\'))\n        #\n        # save_dir = \'/share/data/lang/users/dan/ImageNet-P-299/shear/\' + folder + \'/\'\n        #\n        # if not os.path.exists(save_dir):\n        #     os.makedirs(save_dir)\n        #\n        # os.system(""./ffmpeg/ffmpeg -r 1 -framerate 1 -i "" +\n        #           os.path.join(tmp, \'img\') + ""%01d.png -vcodec libx264 -crf 20 -preset veryslow -y "" +\n        #           save_dir + img_loc[:-5] + "".mp4"")\n        #\n        # rmtree(tmp, ignore_errors=True)\n        #\n        # # /////////////// End Shear Code ///////////////\n'"
ImageNet-P/create_p/make_tinyimagenet_p.py,3,"b'import os\nimport numpy as np\nimport torch\nimport torchvision.transforms as trn\nimport torchvision.transforms.functional as trn_F\nimport cv2\nfrom PIL import Image as PILImage\nimport skimage.color as skcolor\nfrom skimage.util import random_noise\nfrom wand.image import Image as WandImage\nfrom wand.api import library as wandlibrary\nfrom io import BytesIO\nimport ctypes\nfrom scipy.ndimage import zoom as scizoom\nfrom skimage.filters import gaussian\nfrom tempfile import gettempdir\nfrom shutil import rmtree\n\nimagenet_val_folder = \'./imagenet_val_bbox_crop/\'\n\n# Tell Python about the C method\nwandlibrary.MagickMotionBlurImage.argtypes = (ctypes.c_void_p,  # wand\n                                              ctypes.c_double,  # radius\n                                              ctypes.c_double,  # sigma\n                                              ctypes.c_double)  # angle\n\n\n# Extend wand.image.Image class to include method signature\nclass MotionImage(WandImage):\n    def motion_blur(self, radius=0.0, sigma=0.0, angle=0.0):\n        wandlibrary.MagickMotionBlurImage(self.wand, radius, sigma, angle)\n\n\ndef clipped_zoom(img, zoom_factor):\n    h = img.shape[0]\n    # ceil crop height(= crop width)\n    ch = int(np.ceil(h / zoom_factor))\n\n    top = (h - ch) // 2\n    img = scizoom(img[top:top + ch, top:top + ch], (zoom_factor, zoom_factor, 1), order=1)\n    # trim off any extra pixels\n    trim_top = (img.shape[0] - h) // 2\n\n    return img[trim_top:trim_top + h, trim_top:trim_top + h]\n\n\ndef brightness(_x, c=0.):\n    _x = np.array(_x, copy=True) / 255.\n    _x = skcolor.rgb2hsv(_x)\n    _x[:, :, 2] = np.clip(_x[:, :, 2] + c, 0, 1)\n    _x = skcolor.hsv2rgb(_x)\n\n    return np.uint8(_x * 255)\n\n\nfor index, folder in enumerate(sorted(os.listdir(imagenet_val_folder))):\n\n    for img_loc in os.listdir(imagenet_val_folder + folder):\n        orig_img = PILImage.open(imagenet_val_folder + folder + \'/\' + img_loc).convert(\'RGB\')\n        img = trn.Resize(64)(orig_img)\n\n        # /////////////// Test Data ///////////////\n\n        # # /////////////// Gaussian Noise Code ///////////////\n        #\n        # tmp = os.path.join(gettempdir(), \'.{}\'.format(hash(os.times())))\n        # os.makedirs(tmp)\n        #\n        # x = trn.Compose([trn.CenterCrop(64), trn.ToTensor()])(img)\n        #\n        # z = trn.ToPILImage()(x)\n        # z.save(os.path.join(tmp, \'img0.png\'))\n        #\n        # for i in range(1, 31):\n        #     z = trn.ToPILImage()(torch.clamp(x + 0.025 * torch.randn_like(x), 0, 1))\n        #     z.save(os.path.join(tmp, \'img\' + str(i) + \'.png\'))\n        # save_dir = \'/share/data/vision-greg2/users/dan/datasets/TinyImageNet-P/gaussian_noise/\' + folder + \'/\'\n        #\n        # # for i in range(1, 31):\n        # #     z = trn.ToPILImage()(torch.clamp(x + 0.05 * torch.randn_like(x), 0, 1))\n        # #     z.save(os.path.join(tmp, \'img\' + str(i) + \'.png\'))\n        # # save_dir = \'/share/data/vision-greg2/users/dan/datasets/TinyImageNet-P/gaussian_noise_2/\' + folder + \'/\'\n        #\n        # # for i in range(1, 31):\n        # #     z = trn.ToPILImage()(torch.clamp(x + 0.075 * torch.randn_like(x), 0, 1))\n        # #     z.save(os.path.join(tmp, \'img\' + str(i) + \'.png\'))\n        # # save_dir = \'/share/data/vision-greg2/users/dan/datasets/TinyImageNet-P/gaussian_noise_3/\' + folder + \'/\'\n        #\n        # if not os.path.exists(save_dir):\n        #     os.makedirs(save_dir)\n        #\n        # os.system(""./ffmpeg/ffmpeg -r 1 -framerate 1 -i "" + os.path.join(tmp, \'img\') + \\\n        #           ""%01d.png -vcodec libx264 -tune grain -preset veryslow -y "" + \\\n        #           save_dir + img_loc[:-5] + "".mp4"")\n        #\n        # rmtree(tmp, ignore_errors=True)\n        #\n        # # /////////////// End Gaussian Noise Code ///////////////\n\n        # # /////////////// Shot Noise Code ///////////////\n        #\n        # tmp = os.path.join(gettempdir(), \'.{}\'.format(hash(os.times())))\n        # os.makedirs(tmp)\n        #\n        # x = trn.CenterCrop(64)(img)\n        #\n        # x.save(os.path.join(tmp, \'img0.png\'))\n        #\n        # for i in range(1, 31):\n        #     z = np.array(x, copy=True) / 255.\n        #     z = PILImage.fromarray(\n        #         np.uint8(255 * np.clip(np.random.poisson(z * 500) / 500., 0, 1)))\n        #     z.save(os.path.join(tmp, \'img\' + str(i) + \'.png\'))\n        # save_dir = \'/share/data/vision-greg2/users/dan/datasets/TinyImageNet-P/shot_noise/\' + folder + \'/\'\n        #\n        # # for i in range(1, 31):\n        # #     z = np.array(x, copy=True) / 255.\n        # #     z = PILImage.fromarray(\n        # #         np.uint8(255 * np.clip(np.random.poisson(z * 250) / 250., 0, 1)))\n        # #     z.save(os.path.join(tmp, \'img\' + str(i) + \'.png\'))\n        # # save_dir = \'/share/data/vision-greg2/users/dan/datasets/TinyImageNet-P/shot_noise_2/\' + folder + \'/\'\n        #\n        # # for i in range(1, 31):\n        # #     z = np.array(x, copy=True) / 255.\n        # #     z = PILImage.fromarray(\n        # #         np.uint8(255 * np.clip(np.random.poisson(z * 125) / 125., 0, 1)))\n        # #     z.save(os.path.join(tmp, \'img\' + str(i) + \'.png\'))\n        # # save_dir = \'/share/data/vision-greg2/users/dan/datasets/TinyImageNet-P/shot_noise_3/\' + folder + \'/\'\n        #\n        # if not os.path.exists(save_dir):\n        #     os.makedirs(save_dir)\n        #\n        # os.system(""./ffmpeg/ffmpeg -r 1 -framerate 1 -i "" + os.path.join(tmp, \'img\') + \\\n        #           ""%01d.png -vcodec libx264 -tune grain -preset veryslow -y "" + \\\n        #           save_dir + img_loc[:-5] + "".mp4"")\n        #\n        # rmtree(tmp, ignore_errors=True)\n        #\n        # # /////////////// End Shot Noise Code ///////////////\n\n        # /////////////// Motion Blur Code ///////////////\n\n        tmp = os.path.join(gettempdir(), \'.{}\'.format(hash(os.times())))\n        os.makedirs(tmp)\n\n        for i in range(0,31):\n\n            z = orig_img\n            output = BytesIO()\n            z.save(output, format=\'PNG\')\n            z = MotionImage(blob=output.getvalue())\n\n            z.motion_blur(radius=14, sigma=3, angle=(i-30)*5)\n\n            z = cv2.imdecode(np.fromstring(z.make_blob(), np.uint8),\n                             cv2.IMREAD_UNCHANGED)\n\n            if z.shape != (64, 64):\n                z = np.clip(z[..., [2, 1, 0]], 0, 255)  # BGR to RGB\n            else:  # grayscale to RGB\n                z = np.clip(np.array([z, z, z]).transpose((1, 2, 0)), 0, 255)\n\n            trn.Resize(64)(PILImage.fromarray(z)).save(os.path.join(tmp, \'img\' + str(i) + \'.png\'))\n\n        save_dir = \'/share/data/vision-greg2/users/dan/datasets/TinyImageNet-P/motion_blur/\' + folder + \'/\'\n\n        if not os.path.exists(save_dir):\n            os.makedirs(save_dir)\n\n        os.system(""./ffmpeg/ffmpeg -r 1 -framerate 1 -i "" + os.path.join(tmp, \'img\') + \\\n                  ""%01d.png -vcodec libx264 -crf 20 -preset veryslow -y "" + \\\n                  save_dir + img_loc[:-5] + "".mp4"")\n\n        rmtree(tmp, ignore_errors=True)\n\n        # /////////////// End Motion Blur Code ///////////////\n\n        # # /////////////// Zoom Blur Code ///////////////\n        #\n        # tmp = os.path.join(gettempdir(), \'.{}\'.format(hash(os.times())))\n        # os.makedirs(tmp)\n        #\n        # z = trn.CenterCrop(64)(img)\n        # avg = trn.ToTensor()(z)\n        # z.save(os.path.join(tmp, \'img0.png\'))\n        # for i in range(1, 31):\n        #     z = trn.CenterCrop(64)(trn_F.affine(img, angle=0, translate=(0, 0),\n        #                                         scale=1+0.004*i, shear=0, resample=PILImage.BILINEAR))\n        #     avg += trn.ToTensor()(z)\n        #     trn.ToPILImage()(avg / (i + 1)).save(os.path.join(tmp, \'img\' + str(i) + \'.png\'))\n        #\n        # save_dir = \'/share/data/vision-greg2/users/dan/datasets/TinyImageNet-P/zoom_blur/\' + folder + \'/\'\n        #\n        # if not os.path.exists(save_dir):\n        #     os.makedirs(save_dir)\n        #\n        # os.system(""./ffmpeg/ffmpeg -r 1 -framerate 1 -i "" +\n        #           os.path.join(tmp, \'img\') + ""%01d.png -vcodec libx264 -crf 20 -preset veryslow -y "" +\n        #           save_dir + img_loc[:-5] + "".mp4"")\n        #\n        # rmtree(tmp, ignore_errors=True)\n        #\n        # # /////////////// End Zoom Blur Code ///////////////\n\n        # # /////////////// Snow Code ///////////////\n        #\n        # tmp = os.path.join(gettempdir(), \'.{}\'.format(hash(os.times())))\n        # os.makedirs(tmp)\n        #\n        # x = trn.CenterCrop(64)(img)\n        # x = np.array(x) / 255.\n        #\n        # snow_layer = np.random.normal(size=(64, 64), loc=0.05, scale=0.28)\n        #\n        # snow_layer = clipped_zoom(snow_layer[..., np.newaxis], 2)\n        # snow_layer[snow_layer < 0.5] = 0\n        #\n        # snow_layer = PILImage.fromarray((np.clip(snow_layer.squeeze(), 0, 1) * 255).astype(np.uint8), mode=\'L\')\n        # output = BytesIO()\n        # snow_layer.save(output, format=\'PNG\')\n        # output = output.getvalue()\n        #\n        # for i in range(0, 31):\n        #     moving_snow = MotionImage(blob=output)\n        #     moving_snow.motion_blur(radius=10, sigma=2.5, angle=i*4-150)\n        #\n        #     snow_layer = cv2.imdecode(np.fromstring(moving_snow.make_blob(), np.uint8),\n        #                               cv2.IMREAD_UNCHANGED) / 255.\n        #     snow_layer = snow_layer[..., np.newaxis]\n        #\n        #     z = 0.85 * x + (1 - 0.85) * np.maximum(\n        #         x, cv2.cvtColor(np.float32(x), cv2.COLOR_RGB2GRAY).reshape(64, 64, 1) * 1.5 + 0.5)\n        #\n        #     z = np.uint8(np.clip(z + snow_layer + np.rot90(snow_layer, k=2), 0, 1) * 255)\n        #\n        #     PILImage.fromarray(z).save(os.path.join(tmp, \'img\' + str(i) + \'.png\'))\n        #\n        # save_dir = \'/share/data/vision-greg2/users/dan/datasets/TinyImageNet-P/snow/\' + folder + \'/\'\n        #\n        # if not os.path.exists(save_dir):\n        #     os.makedirs(save_dir)\n        #\n        # os.system(""./ffmpeg/ffmpeg -r 1 -framerate 1 -i "" + os.path.join(tmp, \'img\') + \\\n        #           ""%01d.png -vcodec libx264 -crf 20 -preset veryslow -y "" + \\\n        #           save_dir + img_loc[:-5] + "".mp4"")\n        #\n        # rmtree(tmp, ignore_errors=True)\n        #\n        # # /////////////// End Snow Code ///////////////\n\n        # # /////////////// Brightness Code ///////////////\n        #\n        # tmp = os.path.join(gettempdir(), \'.{}\'.format(hash(os.times())))\n        # os.makedirs(tmp)\n        #\n        # x = trn.CenterCrop(64)(img)\n        #\n        # for i in range(0, 31):\n        #     z = PILImage.fromarray(brightness(x, c=(i - 15) * 2 / 100.))\n        #     z.save(os.path.join(tmp, \'img\' + str(i) + \'.png\'))\n        #\n        # save_dir = \'/share/data/vision-greg2/users/dan/datasets/TinyImageNet-P/brightness/\' + folder + \'/\'\n        #\n        # if not os.path.exists(save_dir):\n        #     os.makedirs(save_dir)\n        #\n        # os.system(""./ffmpeg/ffmpeg -r 1 -framerate 1 -i "" + os.path.join(tmp, \'img\') + \\\n        #           ""%01d.png -vcodec libx264 -crf 20 -preset veryslow -y "" + \\\n        #           save_dir + img_loc[:-5] + "".mp4"")\n        #\n        # rmtree(tmp, ignore_errors=True)\n        #\n        # # /////////////// End Brightness Code ///////////////\n\n        # # /////////////// Translate Code ///////////////\n        #\n        # tmp = os.path.join(gettempdir(), \'.{}\'.format(hash(os.times())))\n        # os.makedirs(tmp)\n        #\n        # for i in range(0,31):\n        #     z = trn.CenterCrop(64)(trn_F.affine(img, angle=0, translate=(i-15, 0), scale=1, shear=0))\n        #     z.save(os.path.join(tmp, \'img\'+str(i)+\'.png\'))\n        #\n        # save_dir = \'/share/data/vision-greg2/users/dan/datasets/TinyImageNet-P/translate/\' + folder + \'/\'\n        #\n        # if not os.path.exists(save_dir):\n        #     os.makedirs(save_dir)\n        #\n        # os.system(""./ffmpeg/ffmpeg -r 1 -framerate 1 -i "" + os.path.join(tmp, \'img\') +\n        #           ""%01d.png -vcodec libx264 -crf 20 -preset veryslow -y "" +\n        #           save_dir + img_loc[:-5] + "".mp4"")\n        #\n        # rmtree(tmp, ignore_errors=True)\n        #\n        # # /////////////// End Translate Code ///////////////\n\n        # # /////////////// Rotate Code ///////////////\n        #\n        # tmp = os.path.join(gettempdir(), \'.{}\'.format(hash(os.times())))\n        # os.makedirs(tmp)\n        #\n        # for i in range(0, 31):\n        #     z = trn.CenterCrop(64)(trn_F.affine(img, angle=i-15, translate=(0, 0),\n        #                                         scale=1., shear=0, resample=PILImage.BILINEAR))\n        #     z.save(os.path.join(tmp, \'img\' + str(i) + \'.png\'))\n        #\n        # save_dir = \'/share/data/vision-greg2/users/dan/datasets/TinyImageNet-P/rotate/\' + folder + \'/\'\n        #\n        # if not os.path.exists(save_dir):\n        #     os.makedirs(save_dir)\n        #\n        # os.system(""./ffmpeg/ffmpeg -r 1 -framerate 1 -i "" +\n        #           os.path.join(tmp, \'img\') + ""%01d.png -vcodec libx264 -crf 20 -preset veryslow -y "" +\n        #           save_dir + img_loc[:-5] + "".mp4"")\n        #\n        # rmtree(tmp, ignore_errors=True)\n        #\n        # # /////////////// End Rotate Code ///////////////\n\n        # # /////////////// Tilt Code ///////////////\n        #\n        # tmp = os.path.join(gettempdir(), \'.{}\'.format(hash(os.times())))\n        # os.makedirs(tmp)\n        #\n        # x = np.array(img)\n        # h, w = x.shape[0:2]\n        #\n        # for i in range(0, 31):\n        #     phi, theta = np.deg2rad(i-15), np.deg2rad(i-15)\n        #\n        #     f = np.sqrt(w ** 2 + h ** 2)\n        #\n        #     P1 = np.array([[1, 0, -w / 2], [0, 1, -h / 2], [0, 0, 1], [0, 0, 1]])\n        #\n        #     RX = np.array([[1, 0, 0, 0], [0, np.cos(theta), -np.sin(theta), 0],\n        #                    [0, np.sin(theta), np.cos(theta), 0], [0, 0, 0, 1]])\n        #\n        #     RY = np.array([[np.cos(phi), 0, -np.sin(phi), 0], [0, 1, 0, 0],\n        #                    [np.sin(phi), 0, np.cos(phi), 0], [0, 0, 0, 1]])\n        #\n        #     T = np.array([[1, 0, 0, 0], [0, 1, 0, 0],\n        #                   [0, 0, 1, f], [0, 0, 0, 1]])\n        #\n        #     P2 = np.array([[f, 0, w / 2, 0], [0, f, h / 2, 0], [0, 0, 1, 0]])\n        #\n        #     mat = P2 @ T @ RX @ RY @ P1\n        #\n        #     z = trn.CenterCrop(64)(PILImage.fromarray(cv2.warpPerspective(x, mat, (w, h))))\n        #\n        #     z.save(os.path.join(tmp, \'img\' + str(i) + \'.png\'))\n        #\n        # save_dir = \'/share/data/vision-greg2/users/dan/datasets/TinyImageNet-P/tilt/\' + folder + \'/\'\n        #\n        # if not os.path.exists(save_dir):\n        #     os.makedirs(save_dir)\n        #\n        # os.system(""./ffmpeg/ffmpeg -r 1 -framerate 1 -i "" + os.path.join(tmp, \'img\') + \\\n        #           ""%01d.png -vcodec libx264 -crf 20 -preset veryslow -y "" + \\\n        #           save_dir + img_loc[:-5] + "".mp4"")\n        #\n        # rmtree(tmp, ignore_errors=True)\n        #\n        # # /////////////// End Tilt Code ///////////////\n\n        # # /////////////// Scale Code ///////////////\n        #\n        # tmp = os.path.join(gettempdir(), \'.{}\'.format(hash(os.times())))\n        # os.makedirs(tmp)\n        #\n        # for i in range(0, 31):\n        #     z = trn.CenterCrop(64)(trn_F.affine(img, angle=0, translate=(0, 0),\n        #                                         scale=(i * 2.5 + 40) / 100., shear=0, resample=PILImage.BILINEAR))\n        #     z.save(os.path.join(tmp, \'img\' + str(i) + \'.png\'))\n        #\n        # save_dir = \'/share/data/vision-greg2/users/dan/datasets/TinyImageNet-P/scale/\' + folder + \'/\'\n        #\n        # if not os.path.exists(save_dir):\n        #     os.makedirs(save_dir)\n        #\n        # os.system(""./ffmpeg/ffmpeg -r 1 -framerate 1 -i "" +\n        #           os.path.join(tmp, \'img\') + ""%01d.png -vcodec libx264 -crf 20 -preset veryslow -y "" +\n        #           save_dir + img_loc[:-5] + "".mp4"")\n        #\n        # rmtree(tmp, ignore_errors=True)\n        #\n        # # /////////////// End Scale Code ///////////////\n\n        # /////////////// Validation Data ///////////////\n\n        # # /////////////// Speckle Noise Code ///////////////\n        #\n        # tmp = os.path.join(gettempdir(), \'.{}\'.format(hash(os.times())))\n        # os.makedirs(tmp)\n        #\n        # x = trn.CenterCrop(64)(img)\n        #\n        # x.save(os.path.join(tmp, \'img0.png\'))\n        # x = np.array(x) / 255.\n        #\n        # for i in range(1, 31):\n        #     z = PILImage.fromarray(\n        #         np.uint8(255 * np.clip(x + x * np.random.normal(size=x.shape, scale=0.05), 0, 1)))\n        #     z.save(os.path.join(tmp, \'img\' + str(i) + \'.png\'))\n        # save_dir = \'/share/data/vision-greg2/users/dan/datasets/TinyImageNet-P/speckle_noise/\' + folder + \'/\'\n        #\n        # # for i in range(1, 31):\n        # #     z = PILImage.fromarray(\n        # #         np.uint8(255 * np.clip(x + x * np.random.normal(size=x.shape, scale=0.1), 0, 1)))\n        # #     z.save(os.path.join(tmp, \'img\' + str(i) + \'.png\'))\n        # # save_dir = \'/share/data/vision-greg2/users/dan/datasets/TinyImageNet-P/speckle_noise_2/\' + folder + \'/\'\n        #\n        # # for i in range(1, 31):\n        # #     z = PILImage.fromarray(\n        # #         np.uint8(255 * np.clip(x + x * np.random.normal(size=x.shape, scale=0.15), 0, 1)))\n        # #     z.save(os.path.join(tmp, \'img\' + str(i) + \'.png\'))\n        # # save_dir = \'/share/data/vision-greg2/users/dan/datasets/TinyImageNet-P/speckle_noise_3/\' + folder + \'/\'\n        #\n        # if not os.path.exists(save_dir):\n        #     os.makedirs(save_dir)\n        #\n        # os.system(""./ffmpeg/ffmpeg -r 1 -framerate 1 -i "" + os.path.join(tmp, \'img\') + \\\n        #           ""%01d.png -vcodec libx264 -tune grain -preset veryslow -y "" + \\\n        #           save_dir + img_loc[:-5] + "".mp4"")\n        #\n        # rmtree(tmp, ignore_errors=True)\n        #\n        # # /////////////// End Speckle Noise Code ///////////////\n\n        # # /////////////// Gaussian Blur Code ///////////////\n        #\n        # tmp = os.path.join(gettempdir(), \'.{}\'.format(hash(os.times())))\n        # os.makedirs(tmp)\n        #\n        # x = trn.CenterCrop(64)(img)\n        # for i in range(0, 31):\n        #     z = PILImage.fromarray(\n        #         np.uint8(255*gaussian(np.array(x, copy=True)/255.,\n        #                               sigma=0.35 + 0.014*i, multichannel=True, truncate=7.0)))\n        #\n        #     z.save(os.path.join(tmp, \'img\' + str(i) + \'.png\'))\n        #\n        # save_dir = \'/share/data/vision-greg2/users/dan/datasets/TinyImageNet-P/gaussian_blur/\' + folder + \'/\'\n        #\n        # if not os.path.exists(save_dir):\n        #     os.makedirs(save_dir)\n        #\n        # os.system(""./ffmpeg/ffmpeg -r 1 -framerate 1 -i "" +\n        #           os.path.join(tmp, \'img\') + ""%01d.png -vcodec libx264 -crf 20 -preset veryslow -y "" +\n        #           save_dir + img_loc[:-5] + "".mp4"")\n        #\n        # rmtree(tmp, ignore_errors=True)\n        #\n        # # /////////////// End Gaussian Blur Code ///////////////\n\n        # # /////////////// Spatter Code ///////////////\n        #\n        # tmp = os.path.join(gettempdir(), \'.{}\'.format(hash(os.times())))\n        # os.makedirs(tmp)\n        #\n        # x = trn.CenterCrop(64)(img)\n        # x = cv2.cvtColor(np.array(x, dtype=np.float32) / 255., cv2.COLOR_BGR2BGRA)\n        #\n        # liquid_layer = np.random.normal(size=x.shape[:2], loc=0.6, scale=0.25)\n        # liquid_layer = gaussian(liquid_layer, sigma=1.75)\n        # liquid_layer[liquid_layer < 0.7] = 0\n        #\n        # for i in range(0, 31):\n        #\n        #     liquid_layer_i = (liquid_layer * 255).astype(np.uint8)\n        #     dist = 255 - cv2.Canny(liquid_layer_i, 50, 150)\n        #     dist = cv2.distanceTransform(dist, cv2.DIST_L2, 5)\n        #     _, dist = cv2.threshold(dist, 20, 20, cv2.THRESH_TRUNC)\n        #     dist = cv2.blur(dist, (3, 3)).astype(np.uint8)\n        #     dist = cv2.equalizeHist(dist)\n        #     ker = np.array([[-2, -1, 0], [-1, 1, 1], [0, 1, 2]])\n        #     dist = cv2.filter2D(dist, cv2.CV_8U, ker)\n        #     dist = cv2.blur(dist, (3, 3)).astype(np.float32)\n        #\n        #     m = cv2.cvtColor(liquid_layer_i * dist, cv2.COLOR_GRAY2BGRA)\n        #     m /= np.max(m, axis=(0, 1))\n        #     m *= 0.6\n        #\n        #     # water is pale turqouise\n        #     color = np.concatenate((175 / 255. * np.ones_like(m[..., :1]),\n        #                             238 / 255. * np.ones_like(m[..., :1]),\n        #                             238 / 255. * np.ones_like(m[..., :1])), axis=2)\n        #\n        #     color = cv2.cvtColor(color, cv2.COLOR_BGR2BGRA)\n        #\n        #     z = np.uint8(cv2.cvtColor(np.clip(x + m * color, 0, 1), cv2.COLOR_BGRA2BGR) * 255)\n        #\n        #     liquid_layer = np.apply_along_axis(lambda mat:\n        #                                        np.convolve(mat, np.array([0.2, 0.8]), mode=\'same\'),\n        #                                        axis=0, arr=liquid_layer)\n        #\n        #     PILImage.fromarray(z).save(os.path.join(tmp, \'img\' + str(i) + \'.png\'))\n        #\n        # save_dir = \'/share/data/vision-greg2/users/dan/datasets/TinyImageNet-P/spatter/\' + folder + \'/\'\n        #\n        # if not os.path.exists(save_dir):\n        #     os.makedirs(save_dir)\n        #\n        # os.system(""./ffmpeg/ffmpeg -r 1 -framerate 1 -i "" + os.path.join(tmp, \'img\') + \\\n        #           ""%01d.png -vcodec libx264 -crf 20 -preset veryslow -y "" + \\\n        #           save_dir + img_loc[:-5] + "".mp4"")\n        #\n        # rmtree(tmp, ignore_errors=True)\n        #\n        # # /////////////// End Spatter Code ///////////////\n\n        # # /////////////// Shear Code ///////////////\n        #\n        # tmp = os.path.join(gettempdir(), \'.{}\'.format(hash(os.times())))\n        # os.makedirs(tmp)\n        #\n        # for i in range(0, 31):\n        #     z = trn.CenterCrop(64)(trn_F.affine(img, angle=0, translate=(0, 0),\n        #                                          scale=1., shear=i-15, resample=PILImage.BILINEAR))\n        #     z.save(os.path.join(tmp, \'img\' + str(i) + \'.png\'))\n        #\n        # save_dir = \'/share/data/vision-greg2/users/dan/datasets/TinyImageNet-P/shear/\' + folder + \'/\'\n        #\n        # if not os.path.exists(save_dir):\n        #     os.makedirs(save_dir)\n        #\n        # os.system(""./ffmpeg/ffmpeg -r 1 -framerate 1 -i "" +\n        #           os.path.join(tmp, \'img\') + ""%01d.png -vcodec libx264 -crf 20 -preset veryslow -y "" +\n        #           save_dir + img_loc[:-5] + "".mp4"")\n        #\n        # rmtree(tmp, ignore_errors=True)\n        #\n        # # /////////////// End Shear Code ///////////////\n'"
ImageNet-P/utils/video_loader.py,2,"b'import cv2\n# from skvideo.io import VideoCapture\n# import skvideo.io\nimport torch\nimport torch.utils.data as data\nfrom torchvision.datasets.folder import DatasetFolder\nfrom PIL import Image\n\nimport os\nimport os.path\nimport sys\n\n\nclass VideoFolder(DatasetFolder):\n\n    def __init__(self, root, transform=None, target_transform=None, loader=None):\n        super(VideoFolder, self).__init__(\n            root, loader, [\'.mp4\'], transform=transform, target_transform=target_transform)\n\n        self.vids = self.samples\n\n    def __getitem__(self, index):\n        """"""\n        Args:\n            index (int): Index\n        Returns:\n            tuple: (sample, target) where target is class_index of the target class.\n        """"""\n        path, target = self.samples[index]\n\n        # cap = VideoCapture(path)\n        cap = cv2.VideoCapture(path)\n\n        frames = []\n\n        while True:\n            # Capture frame-by-frame\n            ret, frame = cap.read()\n\n            if not ret: break\n\n            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n            frames.append(self.transform(Image.fromarray(frame)).unsqueeze(0))\n\n        cap.release()\n\n        return torch.cat(frames, 0), target\n'"
old/Icons-50/train.py,13,"b'# -*- coding: utf-8 -*-\nimport os\nimport numpy as np\nimport argparse\nimport math\nimport time\nimport torch\nimport torch.backends.cudnn as cudnn\nimport torchvision.transforms as trn\nimport torchvision.datasets as dset\nimport torch.nn.functional as F\nfrom models.augment import RandomErasing\n\nparser = argparse.ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter)\nparser.add_argument(\'--test-style\', type=str, default=\'microsoft\')\nparser.add_argument(\'--traditional\', action=\'store_true\', help=\'Test classification performance not robustness.\')\nparser.add_argument(\'--subtype\', action=\'store_true\', help=\'Test subtype robustness.\')\nparser.add_argument(\'--c100\', action=\'store_true\', help=\'Test classification performance on CIFAR-100 not robustness.\')\n# Optimization options\nparser.add_argument(\'--epochs\', \'-e\', type=int, default=50, help=\'Number of epochs to train.\')\nparser.add_argument(\'--learning_rate\', \'-lr\', type=float, default=0.1, help=\'The initial learning rate.\')\nparser.add_argument(\'--batch_size\', \'-b\', type=int, default=128, help=\'Batch size.\')\nparser.add_argument(\'--valid_size\', \'-v\', type=int, default=0, help=\'Number of validation examples to hold out.\')\nparser.add_argument(\'--test_bs\', type=int, default=250)\nparser.add_argument(\'--momentum\', type=float, default=0.9, help=\'Momentum.\')\nparser.add_argument(\'--decay\', \'-d\', type=float, default=0.0001, help=\'Weight decay (L2 penalty).\')\n# Checkpoints\nparser.add_argument(\'--save\', \'-s\', type=str, default=\'./snapshots\', help=\'Folder to save checkpoints.\')\nparser.add_argument(\'--load\', \'-l\', type=str, default=\'\', help=\'Checkpoint path to resume / test.\')\nparser.add_argument(\'--test\', \'-t\', action=\'store_true\', help=\'Test only flag.\')\n# Architecture\nparser.add_argument(\'--model\', default=\'resnext\', type=str)\n# Acceleration\nparser.add_argument(\'--ngpu\', type=int, default=1, help=\'0 = CPU.\')\nparser.add_argument(\'--prefetch\', type=int, default=2, help=\'Pre-fetching threads.\')\nargs = parser.parse_args()\nargs.dataset = \'icons\'\n\nstate = {k: v for k, v in args._get_kwargs()}\n\nprint(state)\n\n# set seeds\ntorch.manual_seed(1)\nnp.random.seed(1)\ntorch.cuda.manual_seed(1)\n\n\n# /////////////// Dataset Loading ///////////////\n\nif args.c100:\n    # mean and standard deviation of channels of CIFAR-10 images\n    mean = [x / 255 for x in [125.3, 123.0, 113.9]]\n    std = [x / 255 for x in [63.0, 62.1, 66.7]]\n\n    train_transform = trn.Compose([trn.RandomHorizontalFlip(), trn.RandomCrop(32, padding=4),\n                                   trn.ToTensor(), trn.Normalize(mean, std)])\n    test_transform = trn.Compose([trn.ToTensor(), trn.Normalize(mean, std)])\n\n    train_data = dset.CIFAR100(\'/share/data/vision-greg/cifarpy\', train=True, transform=train_transform, download=False)\n    test_data = dset.CIFAR100(\'/share/data/vision-greg/cifarpy\', train=False, transform=test_transform, download=False)\n    num_classes = 100\n\nelse:\n    train_data = dset.ImageFolder(\'/share/data/vision-greg/DistortedImageNet/Icons-50\',\n                                  transform=trn.Compose([trn.Resize((32, 32)), trn.RandomHorizontalFlip(),\n                                                         trn.RandomCrop(32, padding=4), trn.ToTensor(),\n                                                         # RandomErasing()\n                                                         ]))\n    test_data = dset.ImageFolder(\'/share/data/vision-greg/DistortedImageNet/Icons-50\',\n                                 transform=trn.Compose([trn.Resize((32, 32)), trn.ToTensor()]))\n    num_classes = 50\n\n    if args.traditional:\n        filtered_imgs = []\n        for img in train_data.samples:\n            img_name = img[0]\n            if \'_2\' not in img_name:\n                filtered_imgs.append(img)\n\n        train_data.samples = filtered_imgs[:]\n\n        filtered_imgs = []\n        for img in test_data.samples:\n            img_name = img[0]\n            if \'_2\' in img_name:\n                filtered_imgs.append(img)\n\n        test_data.samples = filtered_imgs[:]\n\n    elif args.subtype:\n        test_subclasses = (\n            ""small_airplane"", ""top_with_upwards_arrow_above"", ""soccer_ball"", ""duck"", ""hatching_chick"", ""crossed_swords"",\n            ""passenger_ship"", ""ledger"", ""books"", ""derelict_house_building"", ""convenience_store"", ""rabbit_face"",\n            ""cartwheel_type_6"", ""mantelpiece_clock"", ""watch"", ""sun_behind_cloud_with_rain"", ""wine_glass"",\n            ""face_throwing_a_kiss"", ""e_mail_symbol"", ""family_man_boy"", ""family_man_girl"", ""family_man_boy_boy"",\n            ""family_man_girl_boy"", ""family_man_girl_girl"", ""monorail"", ""leopard"", ""chequered_flag"", ""tulip"",\n            ""womans_sandal"",\n            ""victory_hand"", ""womans_hat"", ""broken_heart"", ""unified_ideograph_5408"", ""circled_ideograph_accept"",\n            ""closed_lock_with_key"", ""open_mailbox_with_lowered_flag"", ""shark"", ""military_medal"",\n            ""banknote_with_dollar_sign"",\n            ""monkey"", ""crescent_moon"", ""mount_fuji"", ""mobile_phone_off"", ""no_smoking_symbol"", ""glowing_star"",\n            ""evergreen_tree"",\n            ""umbrella_with_rain_drops"", ""racing_car"", ""factory_worker"", ""pencil""\n        )\n\n        filtered_imgs = []\n        for img in train_data.samples:\n            img_name = img[0]\n            in_test_subclass = False\n            for subclass in test_subclasses:\n                if subclass in img_name:\n                    in_test_subclass = True\n                    break\n            if in_test_subclass is False:\n                filtered_imgs.append(img)\n\n        train_data.samples = filtered_imgs[:]\n\n        filtered_imgs = []\n        for img in test_data.samples:\n            img_name = img[0]\n            in_test_subclass = False\n            for subclass in test_subclasses:\n                if subclass in img_name:\n                    in_test_subclass = True\n                    break\n            if in_test_subclass is True:\n                filtered_imgs.append(img)\n\n        test_data.samples = filtered_imgs[:]\n\n    else:\n        filtered_imgs = []\n        for img in train_data.samples:\n            img_name = img[0]\n            if args.test_style not in img_name:\n                filtered_imgs.append(img)\n\n        train_data.samples = filtered_imgs[:]\n\n        filtered_imgs = []\n        for img in test_data.samples:\n            img_name = img[0]\n            if args.test_style in img_name:\n                filtered_imgs.append(img)\n\n        test_data.samples = filtered_imgs[:]\n\ntrain_loader = torch.utils.data.DataLoader(\n    train_data, batch_size=args.batch_size, shuffle=True,\n    num_workers=args.prefetch, pin_memory=True)\ntest_loader = torch.utils.data.DataLoader(\n    test_data, batch_size=args.test_bs, shuffle=False,\n    num_workers=args.prefetch, pin_memory=True)\n\n# /////////////// Model Setup ///////////////\n\n# Create model\n\nif args.model == \'resnext\':\n    from models.resnext import ResNeXt\n    net = ResNeXt({\'input_shape\': (1,3,32,32), \'n_classes\': num_classes,\n                   \'base_channels\': 32, \'depth\': 29, \'cardinality\': 8})\nif \'shake\' in args.model:\n    from models.shake_shake import ResNeXt\n    net = ResNeXt({\'input_shape\': (1,3,32,32), \'n_classes\': num_classes,\n                   \'base_channels\': 96, \'depth\': 26, ""shake_forward"": True,\n                   ""shake_backward"": True, ""shake_image"": True})\n    args.epochs = 500\n    print(\'Overwriting epochs parameter; now the value is\', args.epochs)\nelif args.model == \'wrn\' or \'wide\' in args.model:\n    from models.wrn import WideResNet\n    net = WideResNet(16, num_classes, 4, dropRate=0.3)\n    # args.decay = 5e-4\n    # print(\'Overwriting decay parameter; now the value is\', args.decay)\nelif args.model == \'resnet\':\n    from models.resnet import ResNet\n    net = ResNet({\'input_shape\': (1,3,32,32), \'n_classes\': num_classes,\n                  \'base_channels\': 16, \'block_type\': \'basic\', \'depth\': 20})\nelif args.model == \'densenet\':\n    from models.densenet import DenseNet\n    net = DenseNet({\'input_shape\': (1,3,32,32), \'n_classes\': num_classes,\n                    ""depth"": 40, ""block_type"": ""bottleneck"", ""growth_rate"": 24,\n                    ""drop_rate"": 0.0, ""compression_rate"": 1})   # 1 is turns compression off\n\nstart_epoch = 0\n# Restore model\nif args.load != \'\':\n    for i in range(1000 - 1, -1, -1):\n        model_name = os.path.join(args.load, args.dataset + \'_epoch_\' + str(i) + \'.pt\')\n        if os.path.isfile(model_name):\n            net.load_state_dict(torch.load(model_name))\n            print(\'Model restored! Epoch:\', i)\n            start_epoch = i + 1\n            break\n    if start_epoch == 0:\n        assert False, ""could not resume""\n\n\nif args.ngpu > 1:\n    net = torch.nn.DataParallel(net, device_ids=list(range(args.ngpu)))\n\nif args.ngpu > 0:\n    net.cuda()\n    torch.cuda.manual_seed(1)\n\ncudnn.benchmark = True  # fire on all cylinders\n\noptimizer = torch.optim.SGD(net.parameters(),\n            state[\'learning_rate\'], momentum=state[\'momentum\'],\n            weight_decay=state[\'decay\'], nesterov=True)\n\n\ndef cosine_annealing(step, total_steps, lr_max, lr_min):\n    return lr_min + (lr_max - lr_min) * 0.5 * (\n        1 + np.cos(step / total_steps * np.pi))\n\n\nscheduler = torch.optim.lr_scheduler.LambdaLR(\n    optimizer,\n    lr_lambda=lambda step: cosine_annealing(\n        step,\n        args.epochs * len(train_loader),\n        1,  # since lr_lambda computes multiplicative factor\n        1e-6 / args.learning_rate))\n\n\n# /////////////// Training ///////////////\n\nfrom tqdm import tqdm\n\n\ndef train():\n    net.train()  # enter train mode\n    loss_avg = 0.0\n    for batch_idx, (data, target) in enumerate(train_loader):\n\n        data, target = data.requires_grad_().cuda(), target.requires_grad_().cuda()\n\n        # forward\n        x = net(data)\n\n        # backward\n        scheduler.step()\n        optimizer.zero_grad()\n        loss = F.cross_entropy(x, target)\n        loss.backward()\n        optimizer.step()\n\n        # exponential moving average\n        loss_avg = loss_avg * 0.8 + float(loss.data) * 0.2\n\n    state[\'train_loss\'] = loss_avg\n\n\n# test function\ndef test():\n    net.eval()\n    loss_avg = 0.0\n    correct = 0\n    with torch.no_grad():\n        for batch_idx, (data, target) in enumerate(test_loader):\n            data, target = data.cuda(), target.cuda()\n\n            # forward\n            output = net(data)\n            loss = F.cross_entropy(output, target)\n\n            # accuracy\n            pred = output.data.max(1)[1]\n            correct += pred.eq(target.data).sum().item()\n\n            # test loss average\n            loss_avg += float(loss.data)\n\n    state[\'test_loss\'] = loss_avg / len(test_loader)\n    state[\'test_accuracy\'] = correct / len(test_loader.dataset)\n\n\nif args.test:\n    test()\n    print(state)\n    exit()\n\n\n# Make save directory\nif not os.path.exists(args.save):\n    os.makedirs(args.save)\nif not os.path.isdir(args.save):\n    raise Exception(\'%s is not a dir\' % args.save)\n\nexperiment_indicator = \'\'\nif args.traditional:\n    experiment_indicator = \'_tradition\'\nelif args.c100:\n    experiment_indicator = \'_c100\'\nelif args.subtype:\n    experiment_indicator = \'_subtype\'\n\nwith open(os.path.join(args.save, args.model + experiment_indicator + \'_results.csv\'), \'w\') as f:\n    f.write(\'epoch,time(s),train_loss,test_loss,test_error(%)\\n\')\n\n\nprint(\'Beginning Training\\n\')\n\n# Main loop\nfor epoch in range(start_epoch, args.epochs):\n    state[\'epoch\'] = epoch\n\n    begin_epoch = time.time()\n\n    train()\n    test()\n\n    # # Save model\n    # torch.save(net.state_dict(),\n    #            os.path.join(args.save, args.dataset + \'_epoch_\' + str(epoch) + \'.pt\'))\n    # # Let us not waste space and delete the previous model\n    # try: os.remove(os.path.join(args.save, args.dataset + \'_epoch_\' + str(epoch - 1) + \'.pt\'))\n    # except: True\n\n    # Show results\n\n    with open(os.path.join(args.save, args.model + experiment_indicator + \'_results.csv\'), \'a\') as f:\n        f.write(\'%03d,%05d,%0.6f,%0.5f,%0.2f,\\n\' % (\n            (epoch + 1),\n            time.time() - begin_epoch,\n            state[\'train_loss\'],\n            state[\'test_loss\'],\n            100 - 100. * state[\'test_accuracy\'],\n        ))\n\n    print(\'Epoch {0:3d} | Time {1:5d} | Train Loss {2:.4f} | Test Loss {3:.3f} | Test Error {4:.2f}\'.format(\n              (epoch + 1),\n              int(time.time() - begin_epoch),\n              state[\'train_loss\'],\n              state[\'test_loss\'],\n              100 - 100. * state[\'test_accuracy\'])\n          )\n\n'"
ImageNet-C/imagenet_c/imagenet_c/__init__.py,0,"b'import numpy as np\nfrom PIL import Image\nfrom .corruptions import *\n\ncorruption_tuple = (gaussian_noise, shot_noise, impulse_noise, defocus_blur,\n                    glass_blur, motion_blur, zoom_blur, snow, frost, fog,\n                    brightness, contrast, elastic_transform, pixelate, jpeg_compression,\n                    speckle_noise, gaussian_blur, spatter, saturate)\n\ncorruption_dict = {corr_func.__name__: corr_func for corr_func in corruption_tuple}\n\n\ndef corrupt(x, severity=1, corruption_name=None, corruption_number=-1):\n    """"""\n    :param x: image to corrupt; a 224x224x3 numpy array in [0, 255]\n    :param severity: strength with which to corrupt x; an integer in [0, 5]\n    :param corruption_name: specifies which corruption function to call;\n    must be one of \'gaussian_noise\', \'shot_noise\', \'impulse_noise\', \'defocus_blur\',\n                    \'glass_blur\', \'motion_blur\', \'zoom_blur\', \'snow\', \'frost\', \'fog\',\n                    \'brightness\', \'contrast\', \'elastic_transform\', \'pixelate\', \'jpeg_compression\',\n                    \'speckle_noise\', \'gaussian_blur\', \'spatter\', \'saturate\';\n                    the last four are validation functions\n    :param corruption_number: the position of the corruption_name in the above list;\n    an integer in [0, 18]; useful for easy looping; 15, 16, 17, 18 are validation corruption numbers\n    :return: the image x corrupted by a corruption function at the given severity; same shape as input\n    """"""\n\n    if corruption_name:\n        x_corrupted = corruption_dict[corruption_name](Image.fromarray(x), severity)\n    elif corruption_number != -1:\n        x_corrupted = corruption_tuple[corruption_number](Image.fromarray(x), severity)\n    else:\n        raise ValueError(""Either corruption_name or corruption_number must be passed"")\n\n    return np.uint8(x_corrupted)\n'"
ImageNet-C/imagenet_c/imagenet_c/corruptions.py,1,"b'# -*- coding: utf-8 -*-\n\nimport numpy as np\nfrom PIL import Image\n\n# /////////////// Corruption Helpers ///////////////\n\nimport skimage as sk\nfrom skimage.filters import gaussian\nfrom io import BytesIO\nfrom wand.image import Image as WandImage\nfrom wand.api import library as wandlibrary\nimport wand.color as WandColor\nimport ctypes\nfrom PIL import Image as PILImage\nimport cv2\nfrom scipy.ndimage import zoom as scizoom\nfrom scipy.ndimage.interpolation import map_coordinates\nimport warnings\nimport os\nfrom pkg_resources import resource_filename\n\nwarnings.simplefilter(""ignore"", UserWarning)\n\n\ndef disk(radius, alias_blur=0.1, dtype=np.float32):\n    if radius <= 8:\n        L = np.arange(-8, 8 + 1)\n        ksize = (3, 3)\n    else:\n        L = np.arange(-radius, radius + 1)\n        ksize = (5, 5)\n    X, Y = np.meshgrid(L, L)\n    aliased_disk = np.array((X ** 2 + Y ** 2) <= radius ** 2, dtype=dtype)\n    aliased_disk /= np.sum(aliased_disk)\n\n    # supersample disk to antialias\n    return cv2.GaussianBlur(aliased_disk, ksize=ksize, sigmaX=alias_blur)\n\n\n# Tell Python about the C method\nwandlibrary.MagickMotionBlurImage.argtypes = (ctypes.c_void_p,  # wand\n                                              ctypes.c_double,  # radius\n                                              ctypes.c_double,  # sigma\n                                              ctypes.c_double)  # angle\n\n\n# Extend wand.image.Image class to include method signature\nclass MotionImage(WandImage):\n    def motion_blur(self, radius=0.0, sigma=0.0, angle=0.0):\n        wandlibrary.MagickMotionBlurImage(self.wand, radius, sigma, angle)\n\n\n# modification of https://github.com/FLHerne/mapgen/blob/master/diamondsquare.py\ndef plasma_fractal(mapsize=256, wibbledecay=3):\n    """"""\n    Generate a heightmap using diamond-square algorithm.\n    Return square 2d array, side length \'mapsize\', of floats in range 0-255.\n    \'mapsize\' must be a power of two.\n    """"""\n    assert (mapsize & (mapsize - 1) == 0)\n    maparray = np.empty((mapsize, mapsize), dtype=np.float_)\n    maparray[0, 0] = 0\n    stepsize = mapsize\n    wibble = 100\n\n    def wibbledmean(array):\n        return array / 4 + wibble * np.random.uniform(-wibble, wibble, array.shape)\n\n    def fillsquares():\n        """"""For each square of points stepsize apart,\n           calculate middle value as mean of points + wibble""""""\n        cornerref = maparray[0:mapsize:stepsize, 0:mapsize:stepsize]\n        squareaccum = cornerref + np.roll(cornerref, shift=-1, axis=0)\n        squareaccum += np.roll(squareaccum, shift=-1, axis=1)\n        maparray[stepsize // 2:mapsize:stepsize,\n        stepsize // 2:mapsize:stepsize] = wibbledmean(squareaccum)\n\n    def filldiamonds():\n        """"""For each diamond of points stepsize apart,\n           calculate middle value as mean of points + wibble""""""\n        mapsize = maparray.shape[0]\n        drgrid = maparray[stepsize // 2:mapsize:stepsize, stepsize // 2:mapsize:stepsize]\n        ulgrid = maparray[0:mapsize:stepsize, 0:mapsize:stepsize]\n        ldrsum = drgrid + np.roll(drgrid, 1, axis=0)\n        lulsum = ulgrid + np.roll(ulgrid, -1, axis=1)\n        ltsum = ldrsum + lulsum\n        maparray[0:mapsize:stepsize, stepsize // 2:mapsize:stepsize] = wibbledmean(ltsum)\n        tdrsum = drgrid + np.roll(drgrid, 1, axis=1)\n        tulsum = ulgrid + np.roll(ulgrid, -1, axis=0)\n        ttsum = tdrsum + tulsum\n        maparray[stepsize // 2:mapsize:stepsize, 0:mapsize:stepsize] = wibbledmean(ttsum)\n\n    while stepsize >= 2:\n        fillsquares()\n        filldiamonds()\n        stepsize //= 2\n        wibble /= wibbledecay\n\n    maparray -= maparray.min()\n    return maparray / maparray.max()\n\n\ndef clipped_zoom(img, zoom_factor):\n    h = img.shape[0]\n    # ceil crop height(= crop width)\n    ch = int(np.ceil(h / float(zoom_factor)))\n\n    top = (h - ch) // 2\n    img = scizoom(img[top:top + ch, top:top + ch], (zoom_factor, zoom_factor, 1), order=1)\n    # trim off any extra pixels\n    trim_top = (img.shape[0] - h) // 2\n\n    return img[trim_top:trim_top + h, trim_top:trim_top + h]\n\n\n# /////////////// End Corruption Helpers ///////////////\n\n\n# /////////////// Corruptions ///////////////\n\ndef gaussian_noise(x, severity=1):\n    c = [.08, .12, 0.18, 0.26, 0.38][severity - 1]\n\n    x = np.array(x) / 255.\n    return np.clip(x + np.random.normal(size=x.shape, scale=c), 0, 1) * 255\n\n\ndef shot_noise(x, severity=1):\n    c = [60, 25, 12, 5, 3][severity - 1]\n\n    x = np.array(x) / 255.\n    return np.clip(np.random.poisson(x * c) / float(c), 0, 1) * 255\n\n\ndef impulse_noise(x, severity=1):\n    c = [.03, .06, .09, 0.17, 0.27][severity - 1]\n\n    x = sk.util.random_noise(np.array(x) / 255., mode=\'s&p\', amount=c)\n    return np.clip(x, 0, 1) * 255\n\n\ndef speckle_noise(x, severity=1):\n    c = [.15, .2, 0.35, 0.45, 0.6][severity - 1]\n\n    x = np.array(x) / 255.\n    return np.clip(x + x * np.random.normal(size=x.shape, scale=c), 0, 1) * 255\n\n\ndef fgsm(x, source_net, severity=1):\n    c = [8, 16, 32, 64, 128][severity - 1]\n\n    x = V(x, requires_grad=True)\n    logits = source_net(x)\n    source_net.zero_grad()\n    loss = F.cross_entropy(logits, V(logits.data.max(1)[1].squeeze_()), size_average=False)\n    loss.backward()\n\n    return standardize(torch.clamp(unstandardize(x.data) + c / 255. * unstandardize(torch.sign(x.grad.data)), 0, 1))\n\n\ndef gaussian_blur(x, severity=1):\n    c = [1, 2, 3, 4, 6][severity - 1]\n\n    x = gaussian(np.array(x) / 255., sigma=c, multichannel=True)\n    return np.clip(x, 0, 1) * 255\n\n\ndef glass_blur(x, severity=1):\n    # sigma, max_delta, iterations\n    c = [(0.7, 1, 2), (0.9, 2, 1), (1, 2, 3), (1.1, 3, 2), (1.5, 4, 2)][severity - 1]\n\n    x = np.uint8(gaussian(np.array(x) / 255., sigma=c[0], multichannel=True) * 255)\n\n    # locally shuffle pixels\n    for i in range(c[2]):\n        for h in range(224 - c[1], c[1], -1):\n            for w in range(224 - c[1], c[1], -1):\n                dx, dy = np.random.randint(-c[1], c[1], size=(2,))\n                h_prime, w_prime = h + dy, w + dx\n                # swap\n                x[h, w], x[h_prime, w_prime] = x[h_prime, w_prime], x[h, w]\n\n    return np.clip(gaussian(x / 255., sigma=c[0], multichannel=True), 0, 1) * 255\n\n\ndef defocus_blur(x, severity=1):\n    c = [(3, 0.1), (4, 0.5), (6, 0.5), (8, 0.5), (10, 0.5)][severity - 1]\n\n    x = np.array(x) / 255.\n    kernel = disk(radius=c[0], alias_blur=c[1])\n\n    channels = []\n    for d in range(3):\n        channels.append(cv2.filter2D(x[:, :, d], -1, kernel))\n    channels = np.array(channels).transpose((1, 2, 0))  # 3x224x224 -> 224x224x3\n\n    return np.clip(channels, 0, 1) * 255\n\n\ndef motion_blur(x, severity=1):\n    c = [(10, 3), (15, 5), (15, 8), (15, 12), (20, 15)][severity - 1]\n\n    output = BytesIO()\n    x.save(output, format=\'PNG\')\n    x = MotionImage(blob=output.getvalue())\n\n    x.motion_blur(radius=c[0], sigma=c[1], angle=np.random.uniform(-45, 45))\n\n    x = cv2.imdecode(np.fromstring(x.make_blob(), np.uint8),\n                     cv2.IMREAD_UNCHANGED)\n\n    if x.shape != (224, 224):\n        return np.clip(x[..., [2, 1, 0]], 0, 255)  # BGR to RGB\n    else:  # greyscale to RGB\n        return np.clip(np.array([x, x, x]).transpose((1, 2, 0)), 0, 255)\n\n\ndef zoom_blur(x, severity=1):\n    c = [np.arange(1, 1.11, 0.01),\n         np.arange(1, 1.16, 0.01),\n         np.arange(1, 1.21, 0.02),\n         np.arange(1, 1.26, 0.02),\n         np.arange(1, 1.31, 0.03)][severity - 1]\n\n    x = (np.array(x) / 255.).astype(np.float32)\n    out = np.zeros_like(x)\n    for zoom_factor in c:\n        out += clipped_zoom(x, zoom_factor)\n\n    x = (x + out) / (len(c) + 1)\n    return np.clip(x, 0, 1) * 255\n\n\ndef fog(x, severity=1):\n    c = [(1.5, 2), (2., 2), (2.5, 1.7), (2.5, 1.5), (3., 1.4)][severity - 1]\n\n    x = np.array(x) / 255.\n    max_val = x.max()\n    x += c[0] * plasma_fractal(wibbledecay=c[1])[:224, :224][..., np.newaxis]\n    return np.clip(x * max_val / (max_val + c[0]), 0, 1) * 255\n\n\ndef frost(x, severity=1):\n    c = [(1, 0.4),\n         (0.8, 0.6),\n         (0.7, 0.7),\n         (0.65, 0.7),\n         (0.6, 0.75)][severity - 1]\n    idx = np.random.randint(5)\n    filename = [resource_filename(__name__, \'frost/frost1.png\'),\n                resource_filename(__name__, \'frost/frost2.png\'),\n                resource_filename(__name__, \'frost/frost3.png\'),\n                resource_filename(__name__, \'frost/frost4.jpg\'),\n                resource_filename(__name__, \'frost/frost5.jpg\'),\n                resource_filename(__name__, \'frost/frost6.jpg\')][idx]\n    frost = cv2.imread(filename)\n    # randomly crop and convert to rgb\n    x_start, y_start = np.random.randint(0, frost.shape[0] - 224), np.random.randint(0, frost.shape[1] - 224)\n    frost = frost[x_start:x_start + 224, y_start:y_start + 224][..., [2, 1, 0]]\n\n    return np.clip(c[0] * np.array(x) + c[1] * frost, 0, 255)\n\n\ndef snow(x, severity=1):\n    c = [(0.1, 0.3, 3, 0.5, 10, 4, 0.8),\n         (0.2, 0.3, 2, 0.5, 12, 4, 0.7),\n         (0.55, 0.3, 4, 0.9, 12, 8, 0.7),\n         (0.55, 0.3, 4.5, 0.85, 12, 8, 0.65),\n         (0.55, 0.3, 2.5, 0.85, 12, 12, 0.55)][severity - 1]\n\n    x = np.array(x, dtype=np.float32) / 255.\n    snow_layer = np.random.normal(size=x.shape[:2], loc=c[0], scale=c[1])  # [:2] for monochrome\n\n    snow_layer = clipped_zoom(snow_layer[..., np.newaxis], c[2])\n    snow_layer[snow_layer < c[3]] = 0\n\n    snow_layer = PILImage.fromarray((np.clip(snow_layer.squeeze(), 0, 1) * 255).astype(np.uint8), mode=\'L\')\n    output = BytesIO()\n    snow_layer.save(output, format=\'PNG\')\n    snow_layer = MotionImage(blob=output.getvalue())\n\n    snow_layer.motion_blur(radius=c[4], sigma=c[5], angle=np.random.uniform(-135, -45))\n\n    snow_layer = cv2.imdecode(np.fromstring(snow_layer.make_blob(), np.uint8),\n                              cv2.IMREAD_UNCHANGED) / 255.\n    snow_layer = snow_layer[..., np.newaxis]\n\n    x = c[6] * x + (1 - c[6]) * np.maximum(x, cv2.cvtColor(x, cv2.COLOR_RGB2GRAY).reshape(224, 224, 1) * 1.5 + 0.5)\n    return np.clip(x + snow_layer + np.rot90(snow_layer, k=2), 0, 1) * 255\n\n\ndef spatter(x, severity=1):\n    c = [(0.65, 0.3, 4, 0.69, 0.6, 0),\n         (0.65, 0.3, 3, 0.68, 0.6, 0),\n         (0.65, 0.3, 2, 0.68, 0.5, 0),\n         (0.65, 0.3, 1, 0.65, 1.5, 1),\n         (0.67, 0.4, 1, 0.65, 1.5, 1)][severity - 1]\n    x = np.array(x, dtype=np.float32) / 255.\n\n    liquid_layer = np.random.normal(size=x.shape[:2], loc=c[0], scale=c[1])\n\n    liquid_layer = gaussian(liquid_layer, sigma=c[2])\n    liquid_layer[liquid_layer < c[3]] = 0\n    if c[5] == 0:\n        liquid_layer = (liquid_layer * 255).astype(np.uint8)\n        dist = 255 - cv2.Canny(liquid_layer, 50, 150)\n        dist = cv2.distanceTransform(dist, cv2.DIST_L2, 5)\n        _, dist = cv2.threshold(dist, 20, 20, cv2.THRESH_TRUNC)\n        dist = cv2.blur(dist, (3, 3)).astype(np.uint8)\n        dist = cv2.equalizeHist(dist)\n        ker = np.array([[-2, -1, 0], [-1, 1, 1], [0, 1, 2]])\n        dist = cv2.filter2D(dist, cv2.CV_8U, ker)\n        dist = cv2.blur(dist, (3, 3)).astype(np.float32)\n\n        m = cv2.cvtColor(liquid_layer * dist, cv2.COLOR_GRAY2BGRA)\n        m /= np.max(m, axis=(0, 1))\n        m *= c[4]\n\n        # water is pale turqouise\n        color = np.concatenate((175 / 255. * np.ones_like(m[..., :1]),\n                                238 / 255. * np.ones_like(m[..., :1]),\n                                238 / 255. * np.ones_like(m[..., :1])), axis=2)\n\n        color = cv2.cvtColor(color, cv2.COLOR_BGR2BGRA)\n        x = cv2.cvtColor(x, cv2.COLOR_BGR2BGRA)\n\n        return cv2.cvtColor(np.clip(x + m * color, 0, 1), cv2.COLOR_BGRA2BGR) * 255\n    else:\n        m = np.where(liquid_layer > c[3], 1, 0)\n        m = gaussian(m.astype(np.float32), sigma=c[4])\n        m[m < 0.8] = 0\n\n        # mud brown\n        color = np.concatenate((63 / 255. * np.ones_like(x[..., :1]),\n                                42 / 255. * np.ones_like(x[..., :1]),\n                                20 / 255. * np.ones_like(x[..., :1])), axis=2)\n\n        color *= m[..., np.newaxis]\n        x *= (1 - m[..., np.newaxis])\n\n        return np.clip(x + color, 0, 1) * 255\n\n\ndef contrast(x, severity=1):\n    c = [0.4, .3, .2, .1, .05][severity - 1]\n\n    x = np.array(x) / 255.\n    means = np.mean(x, axis=(0, 1), keepdims=True)\n    return np.clip((x - means) * c + means, 0, 1) * 255\n\n\ndef brightness(x, severity=1):\n    c = [.1, .2, .3, .4, .5][severity - 1]\n\n    x = np.array(x) / 255.\n    x = sk.color.rgb2hsv(x)\n    x[:, :, 2] = np.clip(x[:, :, 2] + c, 0, 1)\n    x = sk.color.hsv2rgb(x)\n\n    return np.clip(x, 0, 1) * 255\n\n\ndef saturate(x, severity=1):\n    c = [(0.3, 0), (0.1, 0), (2, 0), (5, 0.1), (20, 0.2)][severity - 1]\n\n    x = np.array(x) / 255.\n    x = sk.color.rgb2hsv(x)\n    x[:, :, 1] = np.clip(x[:, :, 1] * c[0] + c[1], 0, 1)\n    x = sk.color.hsv2rgb(x)\n\n    return np.clip(x, 0, 1) * 255\n\n\ndef jpeg_compression(x, severity=1):\n    c = [25, 18, 15, 10, 7][severity - 1]\n\n    output = BytesIO()\n    x.save(output, \'JPEG\', quality=c)\n    x = PILImage.open(output)\n\n    return x\n\n\ndef pixelate(x, severity=1):\n    c = [0.6, 0.5, 0.4, 0.3, 0.25][severity - 1]\n\n    x = x.resize((int(224 * c), int(224 * c)), PILImage.BOX)\n    x = x.resize((224, 224), PILImage.BOX)\n\n    return x\n\n\n# mod of https://gist.github.com/erniejunior/601cdf56d2b424757de5\ndef elastic_transform(image, severity=1):\n    c = [(244 * 2, 244 * 0.7, 244 * 0.1),   # 244 should have been 224, but ultimately nothing is incorrect\n         (244 * 2, 244 * 0.08, 244 * 0.2),\n         (244 * 0.05, 244 * 0.01, 244 * 0.02),\n         (244 * 0.07, 244 * 0.01, 244 * 0.02),\n         (244 * 0.12, 244 * 0.01, 244 * 0.02)][severity - 1]\n\n    image = np.array(image, dtype=np.float32) / 255.\n    shape = image.shape\n    shape_size = shape[:2]\n\n    # random affine\n    center_square = np.float32(shape_size) // 2\n    square_size = min(shape_size) // 3\n    pts1 = np.float32([center_square + square_size,\n                       [center_square[0] + square_size, center_square[1] - square_size],\n                       center_square - square_size])\n    pts2 = pts1 + np.random.uniform(-c[2], c[2], size=pts1.shape).astype(np.float32)\n    M = cv2.getAffineTransform(pts1, pts2)\n    image = cv2.warpAffine(image, M, shape_size[::-1], borderMode=cv2.BORDER_REFLECT_101)\n\n    dx = (gaussian(np.random.uniform(-1, 1, size=shape[:2]),\n                   c[1], mode=\'reflect\', truncate=3) * c[0]).astype(np.float32)\n    dy = (gaussian(np.random.uniform(-1, 1, size=shape[:2]),\n                   c[1], mode=\'reflect\', truncate=3) * c[0]).astype(np.float32)\n    dx, dy = dx[..., np.newaxis], dy[..., np.newaxis]\n\n    x, y, z = np.meshgrid(np.arange(shape[1]), np.arange(shape[0]), np.arange(shape[2]))\n    indices = np.reshape(y + dy, (-1, 1)), np.reshape(x + dx, (-1, 1)), np.reshape(z, (-1, 1))\n    return np.clip(map_coordinates(image, indices, order=1, mode=\'reflect\').reshape(shape), 0, 1) * 255\n\n\n# /////////////// End Corruptions ///////////////\n'"
old/Icons-50/models/augment.py,0,"b""from torchvision.transforms import *\n\nfrom PIL import Image\nimport random\nimport math\nimport numpy as np\nimport torch\n\nclass RandomErasing(object):\n    '''\n    Class that performs Random Erasing in Random Erasing Data Augmentation by Zhong et al. \n    -------------------------------------------------------------------------------------\n    probability: The probability that the operation will be performed.\n    sl: min erasing area\n    sh: max erasing area\n    r1: min aspect ratio\n    mean: erasing value\n    -------------------------------------------------------------------------------------\n    '''\n    def __init__(self, probability = 0.5, sl = 0.02, sh = 0.4, r1 = 0.3, mean=[0.5, 0.5, 0.5]):\n        self.probability = probability\n        self.mean = mean\n        self.sl = sl\n        self.sh = sh\n        self.r1 = r1\n       \n    def __call__(self, img):\n\n        if random.uniform(0, 1) > self.probability:\n            return img\n\n        for attempt in range(100):\n            area = img.size()[1] * img.size()[2]\n       \n            target_area = random.uniform(self.sl, self.sh) * area\n            aspect_ratio = random.uniform(self.r1, 1/self.r1)\n\n            h = int(round(math.sqrt(target_area * aspect_ratio)))\n            w = int(round(math.sqrt(target_area / aspect_ratio)))\n\n            if w < img.size()[2] and h < img.size()[1]:\n                x1 = random.randint(0, img.size()[1] - h)\n                y1 = random.randint(0, img.size()[2] - w)\n                if img.size()[0] == 3:\n                    img[0, x1:x1+h, y1:y1+w] = self.mean[0]\n                    img[1, x1:x1+h, y1:y1+w] = self.mean[1]\n                    img[2, x1:x1+h, y1:y1+w] = self.mean[2]\n                else:\n                    img[0, x1:x1+h, y1:y1+w] = self.mean[0]\n                return img\n\n        return img\n\n"""
old/Icons-50/models/densenet.py,5,"b""# coding: utf-8\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\ndef initialize_weights(m):\n    if isinstance(m, nn.Conv2d):\n        nn.init.kaiming_normal_(m.weight.data, mode='fan_out')\n    elif isinstance(m, nn.BatchNorm2d):\n        m.weight.data.fill_(1)\n        m.bias.data.zero_()\n    elif isinstance(m, nn.Linear):\n        m.bias.data.zero_()\n\n\nclass BasicBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, drop_rate):\n        super(BasicBlock, self).__init__()\n\n        self.drop_rate = drop_rate\n\n        self.bn = nn.BatchNorm2d(in_channels)\n        self.conv = nn.Conv2d(\n            in_channels,\n            out_channels,\n            kernel_size=3,\n            stride=1,\n            padding=1,\n            bias=False)\n\n    def forward(self, x):\n        y = self.conv(F.relu(self.bn(x), inplace=True))\n        if self.drop_rate > 0:\n            y = F.dropout(\n                y, p=self.drop_rate, training=self.training, inplace=False)\n        return torch.cat([x, y], dim=1)\n\n\nclass BottleneckBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, drop_rate):\n        super(BottleneckBlock, self).__init__()\n\n        self.drop_rate = drop_rate\n\n        bottleneck_channels = out_channels * 4\n\n        self.bn1 = nn.BatchNorm2d(in_channels)\n        self.conv1 = nn.Conv2d(\n            in_channels,\n            bottleneck_channels,\n            kernel_size=1,\n            stride=1,\n            padding=0,\n            bias=False)\n\n        self.bn2 = nn.BatchNorm2d(bottleneck_channels)\n        self.conv2 = nn.Conv2d(\n            bottleneck_channels,\n            out_channels,\n            kernel_size=3,\n            stride=1,\n            padding=1,\n            bias=False)\n\n    def forward(self, x):\n        y = self.conv1(F.relu(self.bn1(x), inplace=True))\n        if self.drop_rate > 0:\n            y = F.dropout(\n                y, p=self.drop_rate, training=self.training, inplace=False)\n        y = self.conv2(F.relu(self.bn2(y), inplace=True))\n        if self.drop_rate > 0:\n            y = F.dropout(\n                y, p=self.drop_rate, training=self.training, inplace=False)\n        return torch.cat([x, y], dim=1)\n\n\nclass TransitionBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, drop_rate):\n        super(TransitionBlock, self).__init__()\n\n        self.drop_rate = drop_rate\n\n        self.bn = nn.BatchNorm2d(in_channels)\n        self.conv = nn.Conv2d(\n            in_channels,\n            out_channels,\n            kernel_size=1,\n            stride=1,\n            padding=0,\n            bias=False)\n\n    def forward(self, x):\n        x = self.conv(F.relu(self.bn(x), inplace=True))\n        if self.drop_rate > 0:\n            x = F.dropout(\n                x, p=self.drop_rate, training=self.training, inplace=False)\n        x = F.avg_pool2d(x, kernel_size=2, stride=2)\n        return x\n\n\nclass DenseNet(nn.Module):\n    def __init__(self, config):\n        super(DenseNet, self).__init__()\n\n        input_shape = config['input_shape']\n        n_classes = config['n_classes']\n\n        block_type = config['block_type']\n        depth = config['depth']\n        self.growth_rate = config['growth_rate']\n        self.drop_rate = config['drop_rate']\n        self.compression_rate = config['compression_rate']\n\n        assert block_type in ['basic', 'bottleneck']\n        if block_type == 'basic':\n            block = BasicBlock\n            n_blocks_per_stage = (depth - 4) // 3\n            assert n_blocks_per_stage * 3 + 4 == depth\n        else:\n            block = BottleneckBlock\n            n_blocks_per_stage = (depth - 4) // 6\n            assert n_blocks_per_stage * 6 + 4 == depth\n\n        in_channels = [2 * self.growth_rate]\n        for index in range(3):\n            denseblock_out_channels = int(\n                in_channels[-1] + n_blocks_per_stage * self.growth_rate)\n            if index < 2:\n                transitionblock_out_channels = int(\n                    denseblock_out_channels * self.compression_rate)\n            else:\n                transitionblock_out_channels = denseblock_out_channels\n            in_channels.append(transitionblock_out_channels)\n\n        self.conv = nn.Conv2d(\n            input_shape[1],\n            in_channels[0],\n            kernel_size=3,\n            stride=1,\n            padding=1,\n            bias=False)\n        self.stage1 = self._make_stage(in_channels[0], n_blocks_per_stage,\n                                       block, True)\n        self.stage2 = self._make_stage(in_channels[1], n_blocks_per_stage,\n                                       block, True)\n        self.stage3 = self._make_stage(in_channels[2], n_blocks_per_stage,\n                                       block, False)\n        self.bn = nn.BatchNorm2d(in_channels[3])\n\n        # compute conv feature size\n        self.feature_size = self._forward_conv(\n            torch.zeros(*input_shape)).view(-1).shape[0]\n\n        self.fc = nn.Linear(self.feature_size, n_classes)\n\n        # initialize weights\n        self.apply(initialize_weights)\n\n    def _make_stage(self, in_channels, n_blocks, block, add_transition_block):\n        stage = nn.Sequential()\n        for index in range(n_blocks):\n            stage.add_module('block{}'.format(index + 1),\n                             block(in_channels + index * self.growth_rate,\n                                   self.growth_rate, self.drop_rate))\n        if add_transition_block:\n            in_channels = int(in_channels + n_blocks * self.growth_rate)\n            out_channels = int(in_channels * self.compression_rate)\n            stage.add_module('transition',\n                             TransitionBlock(in_channels, out_channels,\n                                             self.drop_rate))\n        return stage\n\n    def _forward_conv(self, x):\n        x = self.conv(x)\n        x = self.stage1(x)\n        x = self.stage2(x)\n        x = self.stage3(x)\n        x = F.relu(self.bn(x), inplace=True)\n        x = F.adaptive_avg_pool2d(x, output_size=1)\n        return x\n\n    def forward(self, x):\n        x = self._forward_conv(x)\n        x = x.view(x.size(0), -1)\n        x = self.fc(x)\n        return x\n\n"""
old/Icons-50/models/msdnet.py,2,"b'import torch\nimport torch.nn as nn\nimport math\n\n\nclass _DynamicInputDenseBlock(nn.Module):\n\n    def __init__(self, conv_modules, debug):\n        super(_DynamicInputDenseBlock, self).__init__()\n        self.conv_modules = conv_modules\n        self.debug = debug\n\n    def forward(self, x):\n        """"""\n        Use the first element as raw input, and stream the rest of\n        the inputs through the list of modules, then apply concatenation.\n        expect x to be [identity, first input, second input, ..]\n        and len(x) - len(self.conv_modules) = 1 for identity\n\n        :param x: Input\n        :return: Concatenation of the input with 1 or more module outputs\n        """"""\n        if self.debug:\n            for i, t in enumerate(x):\n                print(""Current input size[{}]: {}"".format(i,\n                                                          t.size()))\n\n        # Init output\n        out = x[0]\n\n        # Apply all given modules and return output\n        for calc, m in enumerate(self.conv_modules):\n            out = torch.cat([out, m(x[calc + 1])], 1)\n\n            if self.debug:\n                print(""Working on input number: %s"" % calc)\n                print(""Added: "", m(x[calc + 1]).size())\n                print(""Current out size {}"".format(out.size()))\n\n        return out\n\n\nclass MSDLayer(nn.Module):\n\n    def __init__(self, in_channels, out_channels,\n                 in_scales, out_scales, orig_scales, args):\n        """"""\n        Creates a regular/transition MSDLayer. this layer uses DenseNet like concatenation on each scale,\n        and performs spatial reduction between scales. if input and output scales are different, than this\n        class creates a transition layer and the first layer (with the largest spatial size) is dropped.\n\n        :param current_channels: number of input channels\n        :param in_scales: number of input scales\n        :param out_scales: number of output scales\n        :param orig_scales: number of scales in the first layer of the MSDNet\n        :param args: other arguments\n        """"""\n        super(MSDLayer, self).__init__()\n\n        # Init vars\n        self.current_channels = in_channels\n        self.out_channels = out_channels\n        self.in_scales = in_scales\n        self.out_scales = out_scales\n        self.orig_scales = orig_scales\n        self.args = args\n        self.bottleneck = args.msd_bottleneck\n        self.bottleneck_factor = args.msd_bottleneck_factor\n        self.growth_factor = self.args.msd_growth_factor\n        self.debug = self.args.debug\n\n        # Define Conv2d/GCN params\n        self.use_gcn = args.msd_all_gcn\n        self.conv_l, self.ks, self.pad = get_conv_params(self.use_gcn, args)\n\n        # Calculate number of channels to drop and number of\n        # all dropped channels\n        self.to_drop = in_scales - out_scales\n        self.dropped = orig_scales - out_scales # Use this as an offset\n        self.subnets = self.get_subnets()\n\n    def get_subnets(self):\n        """"""\n        Builds the different scales of the MSD network layer.\n\n        :return: A list of scale modules\n        """"""\n        subnets = nn.ModuleList()\n\n        # If this is a transition layer\n        if self.to_drop:\n            # Create a reduced feature map for the first scale\n            # self.dropped > 0 since out_scales < in_scales < orig_scales\n            in_channels1 = self.current_channels *\\\n                          self.growth_factor[self.dropped - 1]\n            in_channels2 = self.current_channels *\\\n                           self.growth_factor[self.dropped]\n            out_channels = self.out_channels *\\\n                           self.growth_factor[self.dropped]\n            bn_width1 = self.bottleneck_factor[self.dropped - 1]\n            bn_width2 = self.bottleneck_factor[self.dropped]\n            subnets.append(self.build_down_densenet(in_channels1,\n                                                    in_channels2,\n                                                    out_channels,\n                                                    self.bottleneck,\n                                                    bn_width1,\n                                                    bn_width2))\n        else:\n            # Create a normal first scale\n            in_channels = self.current_channels *\\\n                          self.growth_factor[self.dropped]\n            out_channels = self.out_channels *\\\n                           self.growth_factor[self.dropped]\n            bn_width = self.bottleneck_factor[self.dropped]\n            subnets.append(self.build_densenet(in_channels,\n                                               out_channels,\n                                               self.bottleneck,\n                                               bn_width))\n\n\n        # Build second+ scales\n        for scale in range(1, self.out_scales):\n            in_channels1 = self.current_channels *\\\n                          self.growth_factor[self.dropped + scale - 1]\n            in_channels2 = self.current_channels *\\\n                           self.growth_factor[self.dropped + scale]\n            out_channels = self.out_channels *\\\n                           self.growth_factor[self.dropped + scale]\n            bn_width1 = self.bottleneck_factor[self.dropped + scale - 1]\n            bn_width2 = self.bottleneck_factor[self.dropped + scale]\n            subnets.append(self.build_down_densenet(in_channels1,\n                                                    in_channels2,\n                                                    out_channels,\n                                                    self.bottleneck,\n                                                    bn_width1,\n                                                    bn_width2))\n\n        return subnets\n\n    def build_down_densenet(self, in_channels1, in_channels2, out_channels,\n                            bottleneck, bn_width1, bn_width2):\n        """"""\n        Builds a scale sub-network for scales 2 and up.\n\n        :param in_channels1: number of same scale input channels\n        :param in_channels2: number of upper scale input channels\n        :param out_channels: number of output channels\n        :param bottleneck: A flag to perform a channel dimension bottleneck\n        :param bn_width1: The first input width of the bottleneck factor\n        :param bn_width2: The first input width of the bottleneck factor\n        :return: A scale module\n        """"""\n        conv_module1 = self.convolve(in_channels1, int(out_channels/2), \'down\',\n                                    bottleneck, bn_width1)\n        conv_module2 = self.convolve(in_channels2, int(out_channels/2), \'normal\',\n                                    bottleneck, bn_width2)\n        conv_modules = [conv_module1, conv_module2]\n        return _DynamicInputDenseBlock(nn.ModuleList(conv_modules),\n                                       self.debug)\n\n    def build_densenet(self, in_channels, out_channels, bottleneck, bn_width):\n        """"""\n        Builds a scale sub-network for the first layer\n\n        :param in_channels: number of input channels\n        :param out_channels: number of output channels\n        :param bottleneck: A flag to perform a channel dimension bottleneck\n        :param bn_width: The width of the bottleneck factor\n        :return: A scale module\n        """"""\n        conv_module = self.convolve(in_channels, out_channels, \'normal\',\n                                    bottleneck, bn_width)\n        return _DynamicInputDenseBlock(nn.ModuleList([conv_module]),\n                                       self.debug)\n\n    def convolve(self, in_channels, out_channels, conv_type,\n                 bottleneck, bn_width=4):\n        """"""\n        Doing the main convolution of a specific scale in the\n        MSD network\n\n        :param in_channels: number of input channels\n        :param out_channels: number of output channels\n        :param conv_type: convolution type\n        :param bottleneck: A flag to perform a channel dimension bottleneck\n        :param bn_width: The width of the bottleneck factor\n        :return: A Sequential module of the main convolution\n        """"""\n        conv = nn.Sequential()\n        tmp_channels = in_channels\n\n        # Bottleneck before the convolution\n        if bottleneck:\n            tmp_channels = int(min([in_channels, bn_width * out_channels]))\n            conv.add_module(\'Bottleneck_1x1\', nn.Conv2d(in_channels,\n                                                        tmp_channels,\n                                                        kernel_size=1,\n                                                        stride=1,\n                                                        padding=0))\n            conv.add_module(\'Bottleneck_BN\', nn.BatchNorm2d(tmp_channels))\n            conv.add_module(\'Bottleneck_ReLU\', nn.ReLU(inplace=True))\n        if conv_type == \'normal\':\n            conv.add_module(\'Spatial_forward\', self.conv_l(tmp_channels,\n                                                           out_channels,\n                                                           kernel_size=self.ks,\n                                                           stride=1,\n                                                           padding=self.pad))\n        elif conv_type == \'down\':\n            conv.add_module(\'Spatial_down\', self.conv_l(tmp_channels, out_channels,\n                                                        kernel_size=self.ks,\n                                                        stride=2,\n                                                        padding=self.pad))\n        else: # Leaving an option to change the main conv type\n            raise NotImplementedError\n\n        conv.add_module(\'BN_out\', nn.BatchNorm2d(out_channels))\n        conv.add_module(\'ReLU_out\', nn.ReLU(inplace=True))\n        return conv\n\n    def forward(self, x):\n        cur_input = []\n        outputs = []\n\n        # Prepare the different scales\' inputs of the\n        # current transition/regular layer\n        if self.to_drop: # Transition\n            for scale in range(0, self.out_scales):\n                last_same_scale = x[self.to_drop + scale]\n                last_upper_scale = x[self.to_drop + scale - 1]\n                cur_input.append([last_same_scale,\n                                  last_upper_scale,\n                                  last_same_scale])\n        else: # Regular\n\n            # Add first scale\'s input\n            cur_input.append([x[0], x[0]])\n\n            # Add second+ scales\' input\n            for scale in range(1, self.out_scales):\n                last_same_scale = x[scale]\n                last_upper_scale = x[scale - 1]\n                cur_input.append([last_same_scale,\n                                  last_upper_scale,\n                                  last_same_scale])\n\n        # Flow inputs in subnets and fill outputs\n        for scale in range(0, self.out_scales):\n            outputs.append(self.subnets[scale](cur_input[scale]))\n\n        return outputs\n\n\nclass MSDFirstLayer(nn.Module):\n\n    def __init__(self, in_channels, out_channels, num_scales, args):\n        """"""\n        Creates the first layer of the MSD network, which takes\n        an input tensor (image) and generates a list of size num_scales\n        with deeper features with smaller (spatial) dimensions.\n\n        :param in_channels: number of input channels to the first layer\n        :param out_channels: number of output channels in the first scale\n        :param num_scales: number of output scales in the first layer\n        :param args: other arguments\n        """"""\n        super(MSDFirstLayer, self).__init__()\n\n        # Init params\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        self.num_scales = num_scales\n        self.args = args\n        self.use_gcn = args.msd_gcn\n        self.conv_l, self.ks, self.pad = get_conv_params(self.use_gcn, args)\n        if self.use_gcn:\n            print(\'|          First layer with GCN           |\')\n        else:\n            print(\'|         First layer without GCN         |\')\n\n        self.subnets = self.create_modules()\n\n    def create_modules(self):\n\n        # Create first scale features\n        modules = nn.ModuleList()\n        if \'cifar\' in self.args.data:\n            current_channels = int(self.out_channels *\n                                   self.args.msd_growth_factor[0])\n\n            current_m = nn.Sequential(\n                self.conv_l(self.in_channels,\n                       current_channels, kernel_size=self.ks,\n                       stride=1, padding=self.pad),\n                nn.BatchNorm2d(current_channels),\n                nn.ReLU(inplace=True)\n            )\n            modules.append(current_m)\n        else:\n            raise NotImplementedError\n\n        # Create second scale features and down\n        for scale in range(1, self.num_scales):\n\n            # Calculate desired output channels\n            out_channels = int(self.out_channels *\n                               self.args.msd_growth_factor[scale])\n\n            # Use a strided convolution to create next scale features\n            current_m = nn.Sequential(\n                self.conv_l(current_channels, out_channels,\n                       kernel_size=self.ks,\n                       stride=2, padding=self.pad),\n                nn.BatchNorm2d(out_channels),\n                nn.ReLU(inplace=True)\n            )\n\n            # Use the output channels size for the next scale\n            current_channels = out_channels\n\n            # Append module\n            modules.append(current_m)\n\n        return modules\n\n    def forward(self, x):\n        output = [None] * self.num_scales\n        current_input = x\n        for scale in range(0, self.num_scales):\n\n            # Use upper scale as an input\n            if scale > 0:\n                current_input = output[scale-1]\n            output[scale] = self.subnets[scale](current_input)\n        return output\n\n\nclass Transition(nn.Sequential):\n\n    def __init__(self, channels_in, channels_out,\n                 out_scales, offset, growth_factor, args):\n        """"""\n        Performs 1x1 convolution to increase channels size after reducing a spatial size reduction\n        in transition layer.\n\n        :param channels_in: channels before the transition\n        :param channels_out: channels after reduction\n        :param out_scales: number of scales after the transition\n        :param offset: gap between original number of scales to out_scales\n        :param growth_factor: densenet channel growth factor\n        :return: A Parallel trainable array with the scales after channel\n                 reduction\n        """"""\n\n        super(Transition, self).__init__()\n        self.args = args\n\n        # Define a parallel stream for the different scales\n        self.scales = nn.ModuleList()\n        for i in range(0, out_scales):\n            cur_in = channels_in * growth_factor[offset + i]\n            cur_out = channels_out * growth_factor[offset + i]\n            self.scales.append(self.conv1x1(cur_in, cur_out))\n\n    def conv1x1(self, in_channels, out_channels):\n        """"""\n        Inner function to define the basic operation\n\n        :param in_channels: number of input channels\n        :param out_channels: number of output channels\n        :return: A Sequential module to perform 1x1 convolution\n        """"""\n        scale = nn.Sequential(\n            nn.Conv2d(in_channels, out_channels,\n                      kernel_size=1, stride=1, padding=0),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True)\n        )\n\n        return scale\n\n    def forward(self, x):\n        """"""\n        Propegate output through different scales.\n\n        :param x: input to the transition layer\n        :return: list of scales\' outputs\n        """"""\n        if self.args.debug:\n            print (""In transition forward!"")\n\n        output = []\n        for scale, scale_net in enumerate(self.scales):\n            if self.args.debug:\n                print (""Size of x[{}]: {}"".format(scale, x[scale].size()))\n                print (""scale_net[0]: {}"".format(scale_net[0]))\n            output.append(scale_net(x[scale]))\n\n        return output\n\n\nclass CifarClassifier(nn.Module):\n\n    def __init__(self, num_channels, num_classes):\n        """"""\n        Classifier of a cifar10/100 image.\n\n        :param num_channels: Number of input channels to the classifier\n        :param num_classes: Number of classes to classify\n        """"""\n\n        super(CifarClassifier, self).__init__()\n        self.inner_channels = 128\n\n        self.features = nn.Sequential(\n            nn.Conv2d(num_channels, self.inner_channels, kernel_size=3,\n                      stride=2, padding=1),\n            nn.BatchNorm2d(self.inner_channels),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(self.inner_channels, self.inner_channels, kernel_size=3,\n                      stride=2, padding=1),\n            nn.BatchNorm2d(self.inner_channels),\n            nn.ReLU(inplace=True),\n            nn.AvgPool2d(2, 2)\n        )\n\n        self.classifier = nn.Linear(self.inner_channels, num_classes)\n\n    def forward(self, x):\n        """"""\n        Drive features to classification.\n\n        :param x: Input of the lowest scale of the last layer of\n                  the last block\n        :return: Cifar object classification result\n        """"""\n\n        x = self.features(x)\n        x = x.view(x.size(0), self.inner_channels)\n        x = self.classifier(x)\n        return x\n\n\nclass GCN(nn.Module):\n\n    def __init__(self, in_channels, out_channels, kernel_size=7, stride=1, padding=1):\n        """"""\n        Global convolutional network module implementation\n\n        :param in_channels: number of input channels\n        :param out_channels: number of output channels\n        :param kernel_size: size of conv kernel\n        :param stride: stride to use in the conv parts\n        :param padding: padding to use in the conv parts\n        :param share_weights: use shared weights for every side of GCN\n        """"""\n        super(GCN, self).__init__()\n        self.conv_l1 = nn.Conv2d(in_channels, out_channels, kernel_size=(kernel_size, 1),\n                                 padding=(padding, 0), stride=(stride, 1))\n        self.conv_l2 = nn.Conv2d(out_channels, out_channels, kernel_size=(1, kernel_size),\n                                 padding=(0, padding), stride=(1, stride))\n        self.conv_r1 = nn.Conv2d(in_channels, out_channels, kernel_size=(1, kernel_size),\n                                 padding=(0, padding), stride=(1, stride))\n        self.conv_r2 = nn.Conv2d(out_channels, out_channels, kernel_size=(kernel_size, 1),\n                                 padding=(padding, 0), stride=(stride, 1))\n\n\n    def forward(self, x):\n\n        if GCN.share_weights:\n\n            # Prepare input and state\n            self.conv_l1.shared = 2\n            self.conv_l2.shared = 2\n            xt = x.transpose(2,3)\n\n            # Left convs\n            xl = self.conv_l1(x)\n            xl = self.conv_l2(xl)\n\n            # Right convs\n            xrt = self.conv_l1(xt)\n            xrt = self.conv_l2(xrt)\n            xr = xrt.transpose(2,3)\n        else:\n\n            # Left convs\n            xl = self.conv_l1(x)\n            xl = self.conv_l2(xl)\n\n            # Right convs\n            xr = self.conv_r1(x)\n            xr = self.conv_r2(xr)\n\n        return xl + xr\n\ndef get_conv_params(use_gcn, args):\n    """"""\n    Calculates and returns the convulotion parameters\n\n    :param use_gcn: flag to use GCN or not\n    :param args: user defined arguments\n    :return: convolution type, kernel size and padding\n    """"""\n\n    if use_gcn:\n        GCN.share_weights = args.msd_share_weights\n        conv_l = GCN\n        ks = args.msd_gcn_kernel\n    else:\n        conv_l = nn.Conv2d\n        ks = args.msd_kernel\n    pad = int(math.floor(ks / 2))\n    return conv_l, ks, pad\n\n\nclass MSDNet(nn.Module):\n\n    def __init__(self, args):\n        """"""\n        The main module for Multi Scale Dense Network.\n        It holds the different blocks with layers and classifiers of the MSDNet layers\n\n        :param args: Network argument\n        """"""\n\n        super(MSDNet, self).__init__()\n\n        # Init arguments\n        self.args = args\n        self.base = self.args.msd_base\n        self.step = self.args.msd_step\n        self.step_mode = self.args.msd_stepmode\n        self.msd_prune = self.args.msd_prune\n        self.num_blocks = self.args.msd_blocks\n        self.reduction_rate = self.args.reduction\n        self.growth = self.args.msd_growth\n        self.growth_factor = args.msd_growth_factor\n        self.bottleneck = self.args.msd_bottleneck\n        self.bottleneck_factor = args.msd_bottleneck_factor\n\n\n        # Set progress\n        if args.data in [\'cifar10\', \'cifar100\']:\n            self.image_channels = 3\n            self.num_channels = 32\n            self.num_scales = 3\n            self.num_classes = int(args.data.strip(\'cifar\'))\n        else:\n            raise NotImplementedError\n\n        # Init MultiScale graph and fill with Blocks and Classifiers\n        print(\'| MSDNet-Block {}-{}-{}\'.format(self.num_blocks,\n                                               self.step,\n                                               self.args.data))\n        (self.num_layers, self.steps) = self.calc_steps()\n\n        print(\'Building network with the steps: {}\'.format(self.steps))\n        self.cur_layer = 1\n        self.cur_transition_layer = 1\n        self.subnets = nn.ModuleList(self.build_modules(self.num_channels))\n\n        # initialize\n        for m in self.subnets:\n            self.init_weights(m)\n            if hasattr(m,\'__iter__\'):\n                for sub_m in m:\n                    self.init_weights(sub_m)\n\n    def init_weights(self, m):\n        if isinstance(m, nn.Conv2d):\n            n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n            m.weight.data.normal_(0, math.sqrt(2. / n))\n        elif isinstance(m, nn.BatchNorm2d):\n            m.weight.data.fill_(1)\n            m.bias.data.zero_()\n        elif isinstance(m, nn.Linear):\n            m.bias.data.zero_()\n\n    def calc_steps(self):\n        """"""Calculates the number of layers required in each\n        Block and the total number of layers, according to\n        the step and stepmod.\n\n        :return: number of total layers and list of layers/steps per blocks\n        """"""\n\n        # Init steps array\n        steps = [None]*self.num_blocks\n        steps[0] = num_layers = self.base\n\n        # Fill steps and num_layers\n        for i in range(1, self.num_blocks):\n\n            # Take even steps or calc next linear growth of a step\n            steps[i] = (self.step_mode == \'even\' and self.step) or \\\n                        self.step*(i-1)+1\n            num_layers += steps[i]\n\n        return num_layers, steps\n\n    def build_modules(self, num_channels):\n        """"""Builds all blocks and classifiers and add it\n        into an array in the order of the format:\n        [[block]*num_blocks [classifier]*num_blocks]\n        where the i\'th block corresponds to the (i+num_block) classifier.\n\n        :param num_channels: number of input channels\n        :return: An array with all blocks and classifiers\n        """"""\n\n        # Init the blocks & classifiers data structure\n        modules = [None] * self.num_blocks * 2\n        for i in range(0, self.num_blocks):\n            print (\'|-----------------Block {:0>2d}----------------|\'.format(i+1))\n\n            # Add block\n            modules[i], num_channels = self.create_block(num_channels, i)\n\n            # Calculate the last scale (smallest) channels size\n            channels_in_last_layer = num_channels *\\\n                                     self.growth_factor[self.num_scales]\n\n            # Add a classifier that belongs to the i\'th block\n            modules[i + self.num_blocks] = \\\n                CifarClassifier(channels_in_last_layer, self.num_classes)\n        return modules\n\n    def create_block(self, num_channels, block_num):\n        \'\'\'\n        :param num_channels: number of input channels to the block\n        :param block_num: the number of the block (among all blocks)\n        :return: A sequential container with steps[block_num] MSD layers\n        \'\'\'\n\n        block = nn.Sequential()\n\n        # Add the first layer if needed\n        if block_num == 0:\n            block.add_module(\'MSD_first\', MSDFirstLayer(self.image_channels,\n                                                        num_channels,\n                                                        self.num_scales,\n                                                        self.args))\n\n        # Add regular layers\n        current_channels = num_channels\n        for _ in range(0, self.steps[block_num]):\n\n            # Calculate in and out scales of the layer (use paper heuristics)\n            if self.msd_prune == \'max\':\n                interval = math.ceil(self.num_layers/\n                                      self.num_scales)\n                in_scales = int(self.num_scales - \\\n                            math.floor((max(0, self.cur_layer - 2))/interval))\n                out_scales = int(self.num_scales - \\\n                             math.floor((self.cur_layer - 1)/interval))\n            else:\n                raise NotImplementedError\n\n            self.print_layer(in_scales, out_scales)\n            self.cur_layer += 1\n\n            # Add an MSD layer\n            block.add_module(\'MSD_layer_{}\'.format(self.cur_layer - 1),\n                             MSDLayer(current_channels,\n                                      self.growth,\n                                      in_scales,\n                                      out_scales,\n                                      self.num_scales,\n                                      self.args))\n\n            # Increase number of channel (as in densenet pattern)\n            current_channels += self.growth\n\n            # Add a transition layer if required\n            if (self.msd_prune == \'max\' and in_scales > out_scales and\n                self.reduction_rate):\n\n                # Calculate scales transition and add a Transition layer\n                offset = self.num_scales - out_scales\n                new_channels = int(math.floor(current_channels*\n                                              self.reduction_rate))\n                block.add_module(\'Transition\', Transition(\n                    current_channels, new_channels, out_scales,\n                    offset, self.growth_factor, self.args))\n                print(\'|      Transition layer {} was added!      |\'.\n                      format(self.cur_transition_layer))\n                current_channels = new_channels\n\n                # Increment counters\n                self.cur_transition_layer += 1\n\n            elif self.msd_prune != \'max\':\n                raise NotImplementedError\n\n        return block, current_channels\n\n    def print_layer(self, in_scales, out_scales):\n        print(\'| Layer {:0>2d} input scales {} output scales {} |\'.\n              format(self.cur_layer, in_scales, out_scales))\n\n    def forward(self, x, progress=None):\n        """"""\n        Propagate Input image in all blocks of MSD layers and classifiers\n        and return a list of classifications\n\n        :param x: Input image / batch\n        :return: a list of classification outputs\n        """"""\n\n        outputs = [None] * self.num_blocks\n        cur_input = x\n        for block_num in range(0, self.num_blocks):\n\n            # Get the current block\'s output\n            if self.args.debug:\n                print("""")\n                print(""Forwarding to block %s:"" % str(block_num + 1))\n            block = self.subnets[block_num]\n            cur_input = block_output = block(cur_input)\n\n            # Classify and add current output\n            if self.args.debug:\n                print(""- Getting %s block\'s output"" % str(block_num + 1))\n                for s, b in enumerate(block_output):\n                    print(""- Output size of this block\'s scale {}: "".format(s),\n                          b.size())\n            class_output = \\\n                self.subnets[block_num+self.num_blocks](block_output[-1])\n            outputs[block_num] = class_output\n\n        return outputs\n\n\n'"
old/Icons-50/models/resnet.py,3,"b""# coding: utf-8\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\ndef initialize_weights(module):\n    if isinstance(module, nn.Conv2d):\n        nn.init.kaiming_normal_(module.weight.data, mode='fan_out')\n    elif isinstance(module, nn.BatchNorm2d):\n        module.weight.data.fill_(1)\n        module.bias.data.zero_()\n    elif isinstance(module, nn.Linear):\n        module.bias.data.zero_()\n\n\nclass BasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self, in_channels, out_channels, stride):\n        super(BasicBlock, self).__init__()\n\n        self.conv1 = nn.Conv2d(\n            in_channels,\n            out_channels,\n            kernel_size=3,\n            stride=stride,  # downsample with first conv\n            padding=1,\n            bias=False)\n        self.bn1 = nn.BatchNorm2d(out_channels)\n        self.conv2 = nn.Conv2d(\n            out_channels,\n            out_channels,\n            kernel_size=3,\n            stride=1,\n            padding=1,\n            bias=False)\n        self.bn2 = nn.BatchNorm2d(out_channels)\n\n        self.shortcut = nn.Sequential()\n        if in_channels != out_channels:\n            self.shortcut.add_module(\n                'conv',\n                nn.Conv2d(\n                    in_channels,\n                    out_channels,\n                    kernel_size=1,\n                    stride=stride,  # downsample\n                    padding=0,\n                    bias=False))\n            self.shortcut.add_module('bn', nn.BatchNorm2d(out_channels))  # BN\n\n    def forward(self, x):\n        y = F.relu(self.bn1(self.conv1(x)), inplace=True)\n        y = self.bn2(self.conv2(y))\n        y += self.shortcut(x)\n        y = F.relu(y, inplace=True)  # apply ReLU after addition\n        return y\n\n\nclass BottleneckBlock(nn.Module):\n    expansion = 4\n\n    def __init__(self, in_channels, out_channels, stride):\n        super(BottleneckBlock, self).__init__()\n\n        bottleneck_channels = out_channels // self.expansion\n\n        self.conv1 = nn.Conv2d(\n            in_channels,\n            bottleneck_channels,\n            kernel_size=1,\n            stride=1,\n            padding=0,\n            bias=False)\n        self.bn1 = nn.BatchNorm2d(bottleneck_channels)\n\n        self.conv2 = nn.Conv2d(\n            bottleneck_channels,\n            bottleneck_channels,\n            kernel_size=3,\n            stride=stride,  # downsample with 3x3 conv\n            padding=1,\n            bias=False)\n        self.bn2 = nn.BatchNorm2d(bottleneck_channels)\n\n        self.conv3 = nn.Conv2d(\n            bottleneck_channels,\n            out_channels,\n            kernel_size=1,\n            stride=1,\n            padding=0,\n            bias=False)\n        self.bn3 = nn.BatchNorm2d(out_channels)\n\n        self.shortcut = nn.Sequential()  # identity\n        if in_channels != out_channels:\n            self.shortcut.add_module(\n                'conv',\n                nn.Conv2d(\n                    in_channels,\n                    out_channels,\n                    kernel_size=1,\n                    stride=stride,  # downsample\n                    padding=0,\n                    bias=False))\n            self.shortcut.add_module('bn', nn.BatchNorm2d(out_channels))  # BN\n\n    def forward(self, x):\n        y = F.relu(self.bn1(self.conv1(x)), inplace=True)\n        y = F.relu(self.bn2(self.conv2(y)), inplace=True)\n        y = self.bn3(self.conv3(y))  # not apply ReLU\n        y += self.shortcut(x)\n        y = F.relu(y, inplace=True)  # apply ReLU after addition\n        return y\n\n\nclass ResNet(nn.Module):\n    def __init__(self, config):\n        super(ResNet, self).__init__()\n\n        input_shape = config['input_shape']\n        n_classes = config['n_classes']\n\n        base_channels = config['base_channels']\n        block_type = config['block_type']\n        depth = config['depth']\n\n        assert block_type in ['basic', 'bottleneck']\n        if block_type == 'basic':\n            block = BasicBlock\n            n_blocks_per_stage = (depth - 2) // 6\n            assert n_blocks_per_stage * 6 + 2 == depth\n        else:\n            block = BottleneckBlock\n            n_blocks_per_stage = (depth - 2) // 9\n            assert n_blocks_per_stage * 9 + 2 == depth\n\n        n_channels = [\n            base_channels,\n            base_channels * 2 * block.expansion,\n            base_channels * 4 * block.expansion\n        ]\n\n        self.conv = nn.Conv2d(\n            input_shape[1],\n            n_channels[0],\n            kernel_size=3,\n            stride=1,\n            padding=1,\n            bias=False)\n        self.bn = nn.BatchNorm2d(base_channels)\n\n        self.stage1 = self._make_stage(\n            n_channels[0], n_channels[0], n_blocks_per_stage, block, stride=1)\n        self.stage2 = self._make_stage(\n            n_channels[0], n_channels[1], n_blocks_per_stage, block, stride=2)\n        self.stage3 = self._make_stage(\n            n_channels[1], n_channels[2], n_blocks_per_stage, block, stride=2)\n\n        # compute conv feature size\n        self.feature_size = self._forward_conv(\n            torch.zeros(*input_shape)).view(-1).shape[0]\n\n        self.fc = nn.Linear(self.feature_size, n_classes)\n\n        # initialize weights\n        self.apply(initialize_weights)\n\n    def _make_stage(self, in_channels, out_channels, n_blocks, block, stride):\n        stage = nn.Sequential()\n        for index in range(n_blocks):\n            block_name = 'block{}'.format(index + 1)\n            if index == 0:\n                stage.add_module(block_name,\n                                 block(in_channels,\n                                       out_channels,\n                                       stride=stride))\n            else:\n                stage.add_module(block_name,\n                                 block(out_channels,\n                                       out_channels,\n                                       stride=1))\n        return stage\n\n    def _forward_conv(self, x):\n        x = F.relu(self.bn(self.conv(x)), inplace=True)\n        x = self.stage1(x)\n        x = self.stage2(x)\n        x = self.stage3(x)\n        x = F.adaptive_avg_pool2d(x, output_size=1)\n        return x\n\n    def forward(self, x):\n        x = self._forward_conv(x)\n        x = x.view(x.size(0), -1)\n        x = self.fc(x)\n        return x\n"""
old/Icons-50/models/resnext.py,4,"b""# coding: utf-8\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\ndef initialize_weights(m):\n    if isinstance(m, nn.Conv2d):\n        nn.init.kaiming_normal_(m.weight.data, mode='fan_out')\n    elif isinstance(m, nn.BatchNorm2d):\n        m.weight.data.fill_(1)\n        m.bias.data.zero_()\n    elif isinstance(m, nn.Linear):\n        m.bias.data.zero_()\n\n\nclass BottleneckBlock(nn.Module):\n    expansion = 4\n\n    def __init__(self, in_channels, out_channels, stride, cardinality):\n        super(BottleneckBlock, self).__init__()\n\n        bottleneck_channels = cardinality * out_channels // self.expansion\n\n        self.conv1 = nn.Conv2d(\n            in_channels,\n            bottleneck_channels,\n            kernel_size=1,\n            stride=1,\n            padding=0,\n            bias=False)\n        self.bn1 = nn.BatchNorm2d(bottleneck_channels)\n\n        self.conv2 = nn.Conv2d(\n            bottleneck_channels,\n            bottleneck_channels,\n            kernel_size=3,\n            stride=stride,  # downsample with 3x3 conv\n            padding=1,\n            groups=cardinality,\n            bias=False)\n        self.bn2 = nn.BatchNorm2d(bottleneck_channels)\n\n        self.conv3 = nn.Conv2d(\n            bottleneck_channels,\n            out_channels,\n            kernel_size=1,\n            stride=1,\n            padding=0,\n            bias=False)\n        self.bn3 = nn.BatchNorm2d(out_channels)\n\n        self.shortcut = nn.Sequential()  # identity\n        if in_channels != out_channels:\n            self.shortcut.add_module(\n                'conv',\n                nn.Conv2d(\n                    in_channels,\n                    out_channels,\n                    kernel_size=1,\n                    stride=stride,  # downsample\n                    padding=0,\n                    bias=False))\n            self.shortcut.add_module('bn', nn.BatchNorm2d(out_channels))  # BN\n\n    def forward(self, x):\n        y = F.relu(self.bn1(self.conv1(x)), inplace=True)\n        y = F.relu(self.bn2(self.conv2(y)), inplace=True)\n        y = self.bn3(self.conv3(y))  # not apply ReLU\n        y += self.shortcut(x)\n        y = F.relu(y, inplace=True)  # apply ReLU after addition\n        return y\n\n\nclass ResNeXt(nn.Module):\n    def __init__(self, config):\n        super(ResNeXt, self).__init__()\n\n        input_shape = config['input_shape']\n        n_classes = config['n_classes']\n\n        base_channels = config['base_channels']\n        depth = config['depth']\n        self.cardinality = config['cardinality']\n\n        n_blocks_per_stage = (depth - 2) // 9\n        assert n_blocks_per_stage * 9 + 2 == depth\n        block = BottleneckBlock\n\n        n_channels = [\n            base_channels,\n            base_channels * block.expansion,\n            base_channels * 2 * block.expansion,\n            base_channels * 4 * block.expansion\n        ]\n\n        self.conv = nn.Conv2d(\n            input_shape[1],\n            n_channels[0],\n            kernel_size=3,\n            stride=1,\n            padding=1,\n            bias=False)\n        self.bn = nn.BatchNorm2d(n_channels[0])\n\n        self.stage1 = self._make_stage(\n            n_channels[0], n_channels[1], n_blocks_per_stage, stride=1)\n        self.stage2 = self._make_stage(\n            n_channels[1], n_channels[2], n_blocks_per_stage, stride=2)\n        self.stage3 = self._make_stage(\n            n_channels[2], n_channels[3], n_blocks_per_stage, stride=2)\n\n        with torch.no_grad():\n            # compute conv feature size\n            self.feature_size = self._forward_conv(\n                torch.zeros(*input_shape)).view(-1).shape[0]\n\n        self.fc = nn.Linear(self.feature_size, n_classes)\n\n        # initialize weights\n        self.apply(initialize_weights)\n\n    def _make_stage(self, in_channels, out_channels, n_blocks, stride):\n        stage = nn.Sequential()\n        for index in range(n_blocks):\n            block_name = 'block{}'.format(index + 1)\n            if index == 0:\n                stage.add_module(\n                    block_name,\n                    BottleneckBlock(\n                        in_channels,\n                        out_channels,\n                        stride,  # downsample\n                        self.cardinality))\n            else:\n                stage.add_module(\n                    block_name,\n                    BottleneckBlock(\n                        out_channels,\n                        out_channels,\n                        1,  # no downsampling\n                        self.cardinality))\n        return stage\n\n    def _forward_conv(self, x):\n        x = F.relu(self.bn(self.conv(x)), inplace=True)\n        x = self.stage1(x)\n        x = self.stage2(x)\n        x = self.stage3(x)\n        x = F.adaptive_avg_pool2d(x, output_size=1)\n        return x\n\n    def forward(self, x):\n        x = self._forward_conv(x)\n        x = x.view(x.size(0), -1)\n        x = self.fc(x)\n        return x\n"""
old/Icons-50/models/shake_shake.py,11,"b""# coding: utf-8\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Function\n\n\nclass ShakeFunction(Function):\n    @staticmethod\n    def forward(ctx, x1, x2, alpha, beta):\n        ctx.save_for_backward(x1, x2, alpha, beta)\n\n        y = x1 * alpha.data + x2 * (1 - alpha.data)\n        return y\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        x1, x2, alpha, beta = ctx.saved_variables\n        grad_x1 = grad_x2 = grad_alpha = grad_beta = None\n\n        if ctx.needs_input_grad[0]:\n            grad_x1 = grad_output * beta\n        if ctx.needs_input_grad[1]:\n            grad_x2 = grad_output * (1 - beta)\n\n        return grad_x1, grad_x2, grad_alpha, grad_beta\n\n\nshake_function = ShakeFunction.apply\n\n\ndef get_alpha_beta(batch_size, shake_config, is_cuda):\n    forward_shake, backward_shake, shake_image = shake_config\n\n    if forward_shake and not shake_image:\n        alpha = torch.rand(1)\n    elif forward_shake and shake_image:\n        alpha = torch.rand(batch_size).view(batch_size, 1, 1, 1)\n    else:\n        alpha = torch.tensor(0.5)\n\n    if backward_shake and not shake_image:\n        beta = torch.rand(1)\n    elif backward_shake and shake_image:\n        beta = torch.rand(batch_size).view(batch_size, 1, 1, 1)\n    else:\n        beta = torch.tensor(0.5)\n\n    if is_cuda:\n        alpha, beta = alpha.cuda(), beta.cuda()\n\n    return alpha, beta\n\n\ndef initialize_weights(module):\n    if isinstance(module, nn.Conv2d):\n        nn.init.kaiming_normal(module.weight.data, mode='fan_out')\n    elif isinstance(module, nn.BatchNorm2d):\n        module.weight.data.fill_(1)\n        module.bias.data.zero_()\n    elif isinstance(module, nn.Linear):\n        module.bias.data.zero_()\n\n\nclass ResidualPath(nn.Module):\n    def __init__(self, in_channels, out_channels, stride):\n        super(ResidualPath, self).__init__()\n\n        self.conv1 = nn.Conv2d(\n            in_channels,\n            out_channels,\n            kernel_size=3,\n            stride=stride,\n            padding=1,\n            bias=False,\n        )\n        self.bn1 = nn.BatchNorm2d(out_channels)\n        self.conv2 = nn.Conv2d(\n            out_channels,\n            out_channels,\n            kernel_size=3,\n            stride=1,\n            padding=1,\n            bias=False)\n        self.bn2 = nn.BatchNorm2d(out_channels)\n\n    def forward(self, x):\n        x = F.relu(x, inplace=False)\n        x = F.relu(self.bn1(self.conv1(x)), inplace=False)\n        x = self.bn2(self.conv2(x))\n        return x\n\n\nclass DownsamplingShortcut(nn.Module):\n    def __init__(self, in_channels):\n        super(DownsamplingShortcut, self).__init__()\n        self.conv1 = nn.Conv2d(\n            in_channels,\n            in_channels,\n            kernel_size=1,\n            stride=1,\n            padding=0,\n            bias=False)\n        self.conv2 = nn.Conv2d(\n            in_channels,\n            in_channels,\n            kernel_size=1,\n            stride=1,\n            padding=0,\n            bias=False)\n        self.bn = nn.BatchNorm2d(in_channels * 2)\n\n    def forward(self, x):\n        x = F.relu(x, inplace=False)\n        y1 = F.avg_pool2d(x, kernel_size=1, stride=2, padding=0)\n        y1 = self.conv1(y1)\n\n        y2 = F.pad(x[:, :, 1:, 1:], (0, 1, 0, 1))\n        y2 = F.avg_pool2d(y2, kernel_size=1, stride=2, padding=0)\n        y2 = self.conv2(y2)\n\n        z = torch.cat([y1, y2], dim=1)\n        z = self.bn(z)\n\n        return z\n\n\nclass BasicBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, stride, shake_config):\n        super(BasicBlock, self).__init__()\n\n        self.shake_config = shake_config\n\n        self.residual_path1 = ResidualPath(in_channels, out_channels, stride)\n        self.residual_path2 = ResidualPath(in_channels, out_channels, stride)\n\n        self.shortcut = nn.Sequential()\n        if in_channels != out_channels:\n            self.shortcut.add_module('downsample',\n                                     DownsamplingShortcut(in_channels))\n\n    def forward(self, x):\n        x1 = self.residual_path1(x)\n        x2 = self.residual_path2(x)\n\n        if self.training:\n            shake_config = self.shake_config\n        else:\n            shake_config = (False, False, False)\n\n        alpha, beta = get_alpha_beta(x.size(0), shake_config, x.is_cuda)\n        y = shake_function(x1, x2, alpha, beta)\n\n        return self.shortcut(x) + y\n\n\nclass ResNeXt(nn.Module):\n    def __init__(self, config):\n        super(ResNeXt, self).__init__()\n\n        input_shape = config['input_shape']\n        n_classes = config['n_classes']\n\n        base_channels = config['base_channels']\n        depth = config['depth']\n        self.shake_config = (config['shake_forward'], config['shake_backward'],\n                             config['shake_image'])\n\n        block = BasicBlock\n        n_blocks_per_stage = (depth - 2) // 6\n        assert n_blocks_per_stage * 6 + 2 == depth\n\n        n_channels = [base_channels, base_channels * 2, base_channels * 4]\n\n        self.conv = nn.Conv2d(\n            input_shape[1],\n            n_channels[0],\n            kernel_size=3,\n            stride=1,\n            padding=1,\n            bias=False)\n        self.bn = nn.BatchNorm2d(base_channels)\n\n        self.stage1 = self._make_stage(\n            n_channels[0], n_channels[0], n_blocks_per_stage, block, stride=1)\n        self.stage2 = self._make_stage(\n            n_channels[0], n_channels[1], n_blocks_per_stage, block, stride=2)\n        self.stage3 = self._make_stage(\n            n_channels[1], n_channels[2], n_blocks_per_stage, block, stride=2)\n\n        # compute conv feature size\n        self.feature_size = self._forward_conv(\n            torch.zeros(*input_shape)).view(-1).shape[0]\n\n        self.fc = nn.Linear(self.feature_size, n_classes)\n\n        # initialize weights\n        self.apply(initialize_weights)\n\n    def _make_stage(self, in_channels, out_channels, n_blocks, block, stride):\n        stage = nn.Sequential()\n        for index in range(n_blocks):\n            block_name = 'block{}'.format(index + 1)\n            if index == 0:\n                stage.add_module(block_name,\n                                 block(\n                                     in_channels,\n                                     out_channels,\n                                     stride=stride,\n                                     shake_config=self.shake_config))\n            else:\n                stage.add_module(block_name,\n                                 block(\n                                     out_channels,\n                                     out_channels,\n                                     stride=1,\n                                     shake_config=self.shake_config))\n        return stage\n\n    def _forward_conv(self, x):\n        x = F.relu(self.bn(self.conv(x)), inplace=True)\n        x = self.stage1(x)\n        x = self.stage2(x)\n        x = self.stage3(x)\n        x = F.adaptive_avg_pool2d(x, output_size=1)\n        return x\n\n    def forward(self, x):\n        x = self._forward_conv(x)\n        x = x.view(x.size(0), -1)\n        x = self.fc(x)\n        return x\n\n"""
old/Icons-50/models/wrn.py,4,"b'import math\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass BasicBlock(nn.Module):\n    def __init__(self, in_planes, out_planes, stride, dropRate=0.0):\n        super(BasicBlock, self).__init__()\n        self.bn1 = nn.BatchNorm2d(in_planes)\n        self.relu1 = nn.ReLU(inplace=True)\n        self.conv1 = nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n                               padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(out_planes)\n        self.relu2 = nn.ReLU(inplace=True)\n        self.conv2 = nn.Conv2d(out_planes, out_planes, kernel_size=3, stride=1,\n                               padding=1, bias=False)\n        self.droprate = dropRate\n        self.equalInOut = (in_planes == out_planes)\n        self.convShortcut = (not self.equalInOut) and nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride,\n                                                                padding=0, bias=False) or None\n\n    def forward(self, x):\n        if not self.equalInOut:\n            x = self.relu1(self.bn1(x))\n        else:\n            out = self.relu1(self.bn1(x))\n        if self.equalInOut:\n            out = self.relu2(self.bn2(self.conv1(out)))\n        else:\n            out = self.relu2(self.bn2(self.conv1(x)))\n        if self.droprate > 0:\n            out = F.dropout(out, p=self.droprate, training=self.training)\n        out = self.conv2(out)\n        if not self.equalInOut:\n            return torch.add(self.convShortcut(x), out)\n        else:\n            return torch.add(x, out)\n\n\nclass NetworkBlock(nn.Module):\n    def __init__(self, nb_layers, in_planes, out_planes, block, stride, dropRate=0.0):\n        super(NetworkBlock, self).__init__()\n        self.layer = self._make_layer(block, in_planes, out_planes, nb_layers, stride, dropRate)\n\n    def _make_layer(self, block, in_planes, out_planes, nb_layers, stride, dropRate):\n        layers = []\n        for i in range(nb_layers):\n            layers.append(block(i == 0 and in_planes or out_planes, out_planes, i == 0 and stride or 1, dropRate))\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        return self.layer(x)\n\n\nclass WideResNet(nn.Module):\n    def __init__(self, depth, num_classes, widen_factor=1, dropRate=0.0):\n        super(WideResNet, self).__init__()\n        nChannels = [16, 16 * widen_factor, 32 * widen_factor, 64 * widen_factor]\n        assert ((depth - 4) % 6 == 0)\n        n = (depth - 4) // 6\n        block = BasicBlock\n        # 1st conv before any network block\n        self.conv1 = nn.Conv2d(3, nChannels[0], kernel_size=3, stride=1,\n                               padding=1, bias=False)\n        # 1st block\n        self.block1 = NetworkBlock(n, nChannels[0], nChannels[1], block, 1, dropRate)\n        # 2nd block\n        self.block2 = NetworkBlock(n, nChannels[1], nChannels[2], block, 2, dropRate)\n        # 3rd block\n        self.block3 = NetworkBlock(n, nChannels[2], nChannels[3], block, 2, dropRate)\n        # global average pooling and classifier\n        self.bn1 = nn.BatchNorm2d(nChannels[3])\n        self.relu = nn.ReLU(inplace=True)\n        self.fc = nn.Linear(nChannels[3], num_classes)\n        self.nChannels = nChannels[3]\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n                m.weight.data.normal_(0, math.sqrt(2. / n))\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n            elif isinstance(m, nn.Linear):\n                m.bias.data.zero_()\n\n    def forward(self, x):\n        out = self.conv1(x)\n        out = self.block1(out)\n        out = self.block2(out)\n        out = self.block3(out)\n        out = self.relu(self.bn1(out))\n        out = F.avg_pool2d(out, 8)\n        out = out.view(-1, self.nChannels)\n        return self.fc(out)\n'"
old/auxiliary/CIFAR100/train.py,15,"b'# -*- coding: utf-8 -*-\nimport numpy as np\nimport sys\nimport os\nimport pickle\nimport argparse\nimport math\nimport time\nimport torch\nimport torch.nn as nn\nimport torch.backends.cudnn as cudnn\nimport torchvision.transforms as trn\nimport torchvision.datasets as dset\nimport torch.nn.functional as F\nfrom torch.autograd import Variable as V\nfrom cifar_resnet import WideResNet\n\nparser = argparse.ArgumentParser(description=\'Trains a CIFAR-100 Classifier\',\n                                 formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n# Optimization options\nparser.add_argument(\'--epochs\', \'-e\', type=int, default=100, help=\'Number of epochs to train.\')\nparser.add_argument(\'--learning_rate\', \'-lr\', type=float, default=0.1, help=\'The initial learning rate.\')\nparser.add_argument(\'--batch_size\', \'-b\', type=int, default=128, help=\'Batch size.\')\nparser.add_argument(\'--test_bs\', type=int, default=200)\nparser.add_argument(\'--momentum\', \'-m\', type=float, default=0.9, help=\'Momentum.\')\nparser.add_argument(\'--decay\', \'-d\', type=float, default=0.0005, help=\'Weight decay (L2 penalty).\')\n# Checkpoints\nparser.add_argument(\'--save\', \'-s\', type=str, default=\'./snapshots\', help=\'Folder to save checkpoints.\')\nparser.add_argument(\'--load\', \'-l\', type=str, default=\'\', help=\'Checkpoint path to resume / test.\')\nparser.add_argument(\'--test\', \'-t\', action=\'store_true\', help=\'Test only flag.\')\n# Architecture\nparser.add_argument(\'--layers\', default=40, type=int, help=\'total number of layers (default: 28)\')\nparser.add_argument(\'--widen-factor\', default=2, type=int, help=\'widen factor (default: 10)\')\nparser.add_argument(\'--droprate\', default=0.3, type=float, help=\'dropout probability (default: 0.0)\')\n# Acceleration\nparser.add_argument(\'--ngpu\', type=int, default=1, help=\'0 = CPU.\')\nparser.add_argument(\'--prefetch\', type=int, default=4, help=\'Pre-fetching threads.\')\nargs = parser.parse_args()\nargs.dataset = \'cifar100\'\n\ntorch.manual_seed(1)\nnp.random.seed(1)\n\nstate = {k: v for k, v in args._get_kwargs()}\nstate[\'tt\'] = 0     # SGDR variable\nstate[\'init_learning_rate\'] = args.learning_rate\n\n\n# mean and standard deviation of channels of CIFAR-10 images\nmean = [x / 255 for x in [125.3, 123.0, 113.9]]\nstd = [x / 255 for x in [63.0, 62.1, 66.7]]\n\ntrain_transform = trn.Compose([trn.RandomHorizontalFlip(), trn.RandomCrop(32, padding=4),\n                               trn.ToTensor(), trn.Normalize(mean, std)])\ntest_transform = trn.Compose([trn.ToTensor(), trn.Normalize(mean, std)])\n\n\ntrain_data = dset.CIFAR100(\'/share/data/vision-greg/cifarpy\', train=True, transform=train_transform, download=False)\ntest_data = dset.CIFAR100(\'/share/data/vision-greg/cifarpy\', train=False, transform=test_transform, download=False)\ntest_data_out = dset.CIFAR100(\'/share/data/vision-greg/cifarpy\', train=False, transform=test_transform, download=False)\nnum_classes = 20\n\n# d = dict.fromkeys([i for i in range(20)])\n# for i in range(len(coarse)):\n#     if d[coarse[i]] is None: d[coarse[i]] = []\n#     if fine[i] not in d[coarse[i]]:\n#         d[coarse[i]].append(fine[i])\n\ncoarse_to_fine =\\\n    {0: [72, 4, 95, 30, 55], 1: [73, 32, 67, 91, 1], 2: [92, 70, 82, 54, 62],\n     3: [16, 61, 9, 10, 28], 4: [51, 0, 53, 57, 83], 5: [40, 39, 22, 87, 86],\n     6: [20, 25, 94, 84, 5], 7: [14, 24, 6, 7, 18], 8: [43, 97, 42, 3, 88],\n     9: [37, 17, 76, 12, 68], 10: [49, 33, 71, 23, 60], 11: [15, 21, 19, 31, 38],\n     12: [75, 63, 66, 64, 34], 13: [77, 26, 45, 99, 79], 14: [11, 2, 35, 46, 98],\n     15: [29, 93, 27, 78, 44], 16: [65, 50, 74, 36, 80], 17: [56, 52, 47, 59, 96],\n     18: [8, 58, 90, 13, 48], 19: [81, 69, 41, 89, 85]}\n\n# {v: k for k, v in coarse_to_fine.items()}\nfine_to_coarse = dict((v,k) for k in coarse_to_fine for v in coarse_to_fine[k])\n\ntrain_in_data = []\ntrain_in_labels = []\nfor i in range(len(train_data)):\n    fine = train_data.train_labels[i]\n\n    if coarse_to_fine[fine_to_coarse[fine]].index(fine) > 0:    # 0, 1, 2, 3\n        train_in_data.append(train_data.train_data[i])\n        train_in_labels.append(fine_to_coarse[fine])\n\ntrain_in_data = np.array(train_in_data)\ntrain_data.train_data = train_in_data\ntrain_data.train_labels = train_in_labels\n\n\ntest_in_data = []\ntest_in_labels = []\ntest_out_data = []\ntest_out_labels = []\nfor i in range(len(test_data)):\n    fine = test_data.test_labels[i]\n\n    if coarse_to_fine[fine_to_coarse[fine]].index(fine) > 0:\n        test_in_data.append(test_data.test_data[i])\n        test_in_labels.append(fine_to_coarse[fine])\n    else:\n        test_out_data.append(test_data.test_data[i])\n        test_out_labels.append(fine_to_coarse[fine])\n\ntest_in_data = np.array(test_in_data)\ntest_data.test_data = test_in_data\ntest_data.test_labels = test_in_labels\n\ntest_out_data = np.array(test_out_data)\ntest_data_out.test_data = test_out_data\ntest_data_out.test_labels = test_out_labels\n\n\ntrain_loader = torch.utils.data.DataLoader(\n    train_data, batch_size=args.batch_size, shuffle=True,\n    num_workers=args.prefetch, pin_memory=True)\ntest_loader = torch.utils.data.DataLoader(\n    test_data, batch_size=args.test_bs, shuffle=False,\n    num_workers=args.prefetch, pin_memory=True)\ntest_out_loader = torch.utils.data.DataLoader(\n    test_data_out, batch_size=args.test_bs, shuffle=False,\n    num_workers=args.prefetch, pin_memory=True)\n\nprint(\'Number of in examples:\',len(test_loader.dataset))\nprint(\'Number of out examples:\', len(test_out_loader.dataset))\n\n# Create model\nnet = WideResNet(args.layers, num_classes, args.widen_factor, dropRate=args.droprate)\n\nstart_epoch = 0\n\n# # Restore model\n# if args.load != \'\':\n#     for i in range(1000 - 1, -1, -1):\n#         model_name = os.path.join(args.load, args.dataset + \'_model_epoch\' + str(i) + \'.pytorch\')\n#         if os.path.isfile(model_name):\n#             net.load_state_dict(torch.load(model_name))\n#             print(\'Model restored! Epoch:\', i)\n#             start_epoch = i + 1\n#             break\n#     if start_epoch == 0:\n#         assert False, ""could not resume""\n\n\nif args.ngpu > 1:\n    net = torch.nn.DataParallel(net, device_ids=list(range(args.ngpu)))\n\nif args.ngpu > 0:\n    net.cuda()\n    torch.cuda.manual_seed(1)\n\ncudnn.benchmark = True  # fire on all cylinders\n\noptimizer = torch.optim.SGD(net.parameters(), state[\'learning_rate\'], momentum=state[\'momentum\'],\n                            weight_decay=state[\'decay\'], nesterov=True)\n\nfrom tqdm import tqdm\n\ndef train():\n    net.train()  # enter train mode\n    loss_avg = 0.0\n    for batch_idx, (data, target) in enumerate(train_loader):\n        t = torch.from_numpy(np.random.beta(1,1, size=data.size(0)).astype(np.float32)).view(-1,1,1,1)\n        perm = torch.from_numpy(np.random.permutation(data.size(0))).long()\n\n        data, target = V((t * data + (1 - t) * data[perm]).cuda()), V(target.cuda())\n\n        # forward\n        x = net(data)\n\n        # backward\n        optimizer.zero_grad()\n        t = V(t.view(-1).cuda())\n        loss = (t * F.cross_entropy(x, target, reduce=False)).mean() +\\\n               ((1 - t) * F.cross_entropy(x, target[V(perm.cuda())], reduce=False)).mean()\n        loss.backward()\n        optimizer.step()\n\n        # exponential moving average\n        loss_avg = loss_avg * 0.8 + loss.data[0] * 0.2\n\n        dt = math.pi / float(args.epochs)\n        state[\'tt\'] += float(dt) / (len(train_loader.dataset) / float(args.batch_size))\n        if state[\'tt\'] >= math.pi - 0.01:\n            state[\'tt\'] = math.pi - 0.01\n        curT = math.pi / 2.0 + state[\'tt\']\n        new_lr = args.learning_rate * (1.0 + math.sin(curT)) / 2.0  # lr_min = 0, lr_max = lr\n        state[\'learning_rate\'] = new_lr\n        for param_group in optimizer.param_groups:\n            param_group[\'lr\'] = state[\'learning_rate\']\n\n    state[\'train_loss\'] = loss_avg\n\n\n# test function\ndef test():\n    net.eval()\n    loss_avg = 0.0\n    correct = 0\n    for batch_idx, (data, target) in enumerate(test_loader):\n        data, target = V(data.cuda(), volatile=True), V(target.cuda(), volatile=True)\n\n        # forward\n        output = net(data)\n        loss = F.cross_entropy(output, target)\n\n        # accuracy\n        pred = output.data.max(1)[1]\n        correct += pred.eq(target.data).sum()\n\n        # test loss average\n        loss_avg += loss.data[0]\n\n    state[\'test_loss\'] = loss_avg / len(test_loader)\n    state[\'test_accuracy\'] = correct / len(test_loader.dataset)\n\n\ndef test_out():\n    net.eval()\n    loss_avg = 0.0\n    correct = 0\n    for batch_idx, (data, target) in enumerate(test_out_loader):\n        data, target = V(data.cuda(), volatile=True), V(target.cuda(), volatile=True)\n\n        # forward\n        output = net(data)\n        loss = F.cross_entropy(output, target)\n\n        # accuracy\n        pred = output.data.max(1)[1]\n        correct += pred.eq(target.data).sum()\n\n        # test loss average\n        loss_avg += loss.data[0]\n\n    state[\'test_out_loss\'] = loss_avg / len(test_out_loader)\n    state[\'test_out_accuracy\'] = correct / len(test_out_loader.dataset)\n\n\nif args.test:\n    test()\n    print(state)\n    exit()\n\nstate[\'learning_rate\'] = state[\'init_learning_rate\']\n\nprint(\'Beginning Training\')\n# Main loop\nbest_accuracy = 0.0\nfor epoch in range(start_epoch, args.epochs):\n    for param_group in optimizer.param_groups:\n        param_group[\'lr\'] = state[\'learning_rate\']\n    state[\'tt\'] = math.pi / float(args.epochs) * epoch\n\n    state[\'epoch\'] = epoch\n\n    begin_epoch = time.time()\n    train()\n    print(\'Epoch\', epoch, \'| Time Spent:\', round(time.time() - begin_epoch, 4))\n\n    test()\n    test_out()\n\n    torch.save(net.state_dict(),\n               os.path.join(args.save, args.dataset + \'_model_subclass_epoch\' + str(epoch) + \'.pytorch\'))\n    # Let us not waste space and delete the previous model\n    # We do not overwrite the model because we need the epoch number\n    try: os.remove(os.path.join(args.save, args.dataset + \'_model_subclass_epoch\' + str(epoch - 1) + \'.pytorch\'))\n    except: True\n\n    print(state)\n'"
old/auxiliary/ImageNet22K/test.py,20,"b'# -*- coding: utf-8 -*-\nimport numpy as np\nimport sys\nimport os\nimport pickle\nimport argparse\nimport math\nimport time\nimport torch\nimport torch.nn as nn\nimport torch.backends.cudnn as cudnn\nimport torchvision.transforms as trn\nimport torchvision.datasets as dset\nimport torch.nn.functional as F\nfrom torch.autograd import Variable as V\nimport torchvision.models as models\nimport torch.utils.model_zoo as model_zoo\nfrom resnext_101_64x4d import resnext_101_64x4d\n\nparser = argparse.ArgumentParser(description=\'Trains an ImageNet Superclass Classifier\',\n                                 formatter_class=argparse.ArgumentDefaultsHelpFormatter)\nparser.add_argument(\'--model-name\', \'-m\', type=str,\n                    choices=[\'alexnet\', \'squeezenet1.1\', \'vgg11\', \'vgg\', \'vggbn\', \'resnet18\', \'resnet50\', \'resnext\'])\n# Optimization options\nparser.add_argument(\'--test_bs\', type=int, default=100)\n# Checkpoints\nparser.add_argument(\'--load\', \'-l\', type=str, default=\'./snapshots\', help=\'Checkpoint path to resume / test.\')\nparser.add_argument(\'--test\', \'-t\', action=\'store_true\', help=\'Test only flag.\')\n# Acceleration\nparser.add_argument(\'--ngpu\', type=int, default=1, help=\'0 = CPU.\')\nparser.add_argument(\'--prefetch\', type=int, default=4, help=\'Pre-fetching threads.\')\nargs = parser.parse_args()\nstate = {k: v for k, v in args._get_kwargs()}\n\ntorch.manual_seed(1)\nnp.random.seed(1)\n\n\nim1k_classes = (\n    # amphibian n01627424\n    (\'n01641577\', \'n01644373\', \'n01644900\', \'n01629819\', \'n01630670\', \'n01631663\', \'n01632458\', \'n01632777\'),\n    # appliance n02729837\n    (\'n03483316\', \'n04179913\', \'n03584829\', \'n03297495\', \'n03761084\', \'n03259280\', \'n04111531\', \'n04442312\', \'n04542943\', \'n04517823\', \'n03207941\', \'n04070727\', \'n04554684\'),\n    # aquatic mammal n02062017\n    (\'n02074367\', \'n02077923\', \'n02071294\', \'n02066245\'),\n    # bird n01503061\n    (\'n01514668\', \'n01514859\', \'n01518878\', \'n01530575\', \'n01531178\', \'n01532829\', \'n01534433\', \'n01537544\', \'n01558993\', \'n01560419\', \'n01580077\', \'n01582220\', \'n01592084\', \'n01601694\', \'n01608432\', \'n01614925\', \'n01616318\', \'n01622779\', \'n01795545\', \'n01796340\', \'n01797886\', \'n01798484\', \'n01806143\', \'n01806567\', \'n01807496\', \'n01817953\', \'n01818515\', \'n01819313\', \'n01820546\', \'n01824575\', \'n01828970\', \'n01829413\', \'n01833805\', \'n01843065\', \'n01843383\', \'n01855672\', \'n01847000\', \'n01860187\', \'n02002556\', \'n02002724\', \'n02006656\', \'n02007558\', \'n02009912\', \'n02009229\', \'n02011460\', \'n02012849\', \'n02013706\', \'n02018207\', \'n02018795\', \'n02025239\', \'n02027492\', \'n02028035\', \'n02033041\', \'n02037110\', \'n02017213\', \'n02051845\', \'n02056570\', \'n02058221\'),\n    # bear n02131653\n    (\'n02132136\', \'n02134084\', \'n02133161\', \'n02134418\'),\n    # beverage n07881800\n    (\'n07920052\', \'n07892512\', \'n07932039\', \'n07930864\'),\n    # big cat n02127808\n    (\'n02128925\', \'n02129604\', \'n02128385\', \'n02128757\', \'n02129165\', \'n02130308\'),\n    # building n02913152\n    (\'n02793495\', \'n03457902\', \'n03877845\', \'n03781244\', \'n03661043\', \'n02727426\', \'n02859443\', \'n03028079\', \'n03788195\', \'n04346328\', \'n03956157\', \'n04081281\', \'n03032252\', \'n03529860\'),\n    # cat n02121620\n    (\'n02124075\', \'n02123394\', \'n02123159\', \'n02123597\', \'n02123045\', \'n02125311\', \'n02127052\'),\n    # clothing n03051540\n    (\'n03724870\', \'n04584207\', \'n03623198\', \'n02730930\', \'n04162706\', \'n02669723\', \'n04532106\', \'n02916936\', \'n04371430\', \'n03710721\', \'n02837789\', \'n04350905\', \'n03594734\', \'n03325584\', \'n04325704\', \'n02883205\', \'n04591157\', \'n02865351\', \'n03534580\', \'n03770439\', \'n03866082\', \'n04136333\', \'n03980874\', \'n03404251\', \'n04479046\', \'n03630383\', \'n04370456\', \'n02963159\', \'n03617480\', \'n02667093\', \'n03595614\', \'n02892767\', \'n03188531\', \'n03775071\', \'n03127747\', \'n03379051\', \'n02807133\', \'n03787032\', \'n04209133\', \'n02869837\', \'n02817516\', \'n03124170\', \'n04259630\', \'n03710637\', \'n04254777\', \'n03026506\', \'n03763968\', \'n03877472\', \'n03594734\', \'n03450230\', \'n02892767\'),\n    # dog n02084071\n    (\'n02100583\', \'n02100236\', \'n02100735\', \'n02101006\', \'n02100877\', \'n02102973\', \'n02102177\', \'n02102040\', \'n02101556\', \'n02101388\', \'n02102318\', \'n02102480\', \'n02099601\', \'n02099849\', \'n02099429\', \'n02099267\', \'n02099712\', \'n02096294\', \'n02095314\', \'n02098105\', \'n02095889\', \'n02095570\', \'n02096437\', \'n02096051\', \'n02098413\', \'n02094433\', \'n02098286\', \'n02097130\', \'n02097047\', \'n02097209\', \'n02093256\', \'n02093428\', \'n02094114\', \'n02096177\', \'n02093859\', \'n02097298\', \'n02096585\', \'n02093647\', \'n02093991\', \'n02097658\', \'n02094258\', \'n02097474\', \'n02093754\', \'n02090622\', \'n02090721\', \'n02092002\', \'n02089078\', \'n02089867\', \'n02089973\', \'n02092339\', \'n02091635\', \'n02088466\', \'n02091467\', \'n02091831\', \'n02088094\', \'n02091134\', \'n02091032\', \'n02088364\', \'n02088238\', \'n02088632\', \'n02090379\', \'n02091244\', \'n02087394\', \'n02110341\', \'n02113186\', \'n02113023\', \'n02113978\', \'n02111277\', \'n02113712\', \'n02113624\', \'n02113799\', \'n02110806\', \'n02111129\', \'n02112706\', \'n02110958\', \'n02109047\', \'n02105641\', \'n02106382\', \'n02106550\', \'n02105505\', \'n02106030\', \'n02106166\', \'n02105162\', \'n02105056\', \'n02105855\', \'n02105412\', \'n02105251\', \'n02106662\', \'n02104365\', \'n02107142\', \'n02110627\', \'n02107312\', \'n02104029\', \'n02110185\', \'n02110063\', \'n02108089\', \'n02108422\', \'n02109961\', \'n02108000\', \'n02107683\', \'n02107574\', \'n02107908\', \'n02109525\', \'n02108551\', \'n02108915\', \'n02112018\', \'n02112350\', \'n02112137\', \'n02111889\', \'n02111500\', \'n02086910\', \'n02086646\', \'n02086079\', \'n02085936\', \'n02087046\', \'n02085782\', \'n02086240\', \'n02085620\'),\n    # electronic equipment n03278248\n    (\'n03584254\', \'n03777754\', \'n03782006\', \'n03857828\', \'n04004767\', \'n03085013\', \'n03602883\', \'n02979186\', \'n02992529\', \'n03902125\', \'n03187595\', \'n04392985\', \'n02988304\'),\n    # fish n02512053\n    (\'n01496331\', \'n01498041\', \'n01484850\', \'n01491361\', \'n01494475\', \'n02514041\', \'n02536864\', \'n01440764\', \'n01443537\', \'n02526121\', \'n02536864\', \'n02606052\', \'n02607072\', \'n02643566\', \'n02655020\', \'n02640242\', \'n02641379\'),\n    # footwear n03380867\n    (\'n04133789\', \'n04120489\', \'n03680355\', \'n03124043\', \'n03047690\'),\n    # fruit n13134947\n    (\'n07742313\', \'n07745940\', \'n07747607\', \'n07749582\', \'n07753113\', \'n07753275\', \'n07753592\', \'n07754684\', \'n07760859\', \'n07768694\', \'n12267677\', \'n12620546\', \'n13133613\', \'n11879895\', \'n12144580\', \'n12768682\', \'n07742313\'),\n    # fungus n12992868\n    (\'n13052670\', \'n13044778\', \'n12985857\', \'n13040303\', \'n13037406\', \'n13054560\', \'n12998815\'),\n    # geological formation n09287968\n    (\'n09246464\', \'n09468604\', \'n09193705\', \'n09472597\', \'n09399592\', \'n09421951\', \'n09256479\', \'n09332890\', \'n09428293\', \'n09288635\'),\n    # hoofed animal n02370806\n    (\'n02391049\', \'n02389026\', \'n02437312\', \'n02412080\', \'n02423022\', \'n02422699\', \'n02422106\', \'n02415577\', \'n02417914\', \'n02410509\', \'n02408429\', \'n02403003\', \'n02398521\', \'n02437616\', \'n02396427\', \'n02397096\', \'n02395406\'),\n    # insect n02159955\n    (\'n02165105\', \'n02165456\', \'n02167151\', \'n02168699\', \'n02169497\', \'n02172182\', \'n02174001\', \'n02177972\', \'n02190166\', \'n02219486\', \'n02206856\', \'n02226429\', \'n02229544\', \'n02231487\', \'n02233338\', \'n02236044\', \'n02256656\', \'n02259212\', \'n02264363\', \'n02268443\', \'n02268853\', \'n02276258\', \'n02277742\', \'n02279972\', \'n02280649\', \'n02281406\', \'n02281787\'),\n    # musical instrument n03800933\n    (\'n02672831\', \'n03854065\', \'n03452741\', \'n04515003\', \'n03452741\', \'n04515003\', \'n03017168\', \'n03249569\', \'n03447721\', \'n03720891\', \'n03721384\', \'n04311174\', \'n03452741\', \'n04515003\', \'n02787622\', \'n02992211\', \'n04536866\', \'n03495258\', \'n02676566\', \'n03272010\', \'n03854065\', \'n03110669\', \'n03394916\', \'n04487394\', \'n02672831\', \'n03494278\', \'n03840681\', \'n03884397\', \'n02804610\', \'n03838899\', \'n04141076\', \'n03372029\'),\n    # primate n02469914\n    (\'n02500267\', \'n02497673\', \'n02483708\', \'n02483362\', \'n02480495\', \'n02481823\', \'n02480855\', \'n02488702\', \'n02484975\', \'n02489166\', \'n02486261\', \'n02486410\', \'n02487347\', \'n02488291\', \'n02493509\', \'n02494079\', \'n02493793\', \'n02492035\', \'n02492660\', \'n02490219\'),\n    # reptile n01661091\n    (\'n01664065\', \'n01665541\', \'n01667114\', \'n01667778\', \'n01669191\', \'n01675722\', \'n01677366\', \'n01682714\', \'n01685808\', \'n01687978\', \'n01688243\', \'n01689811\', \'n01692333\', \'n01693334\', \'n01694178\', \'n01695060\', \'n01704323\', \'n01698640\', \'n01697457\', \'n01728572\', \'n01728920\', \'n01729322\', \'n01729977\', \'n01734418\', \'n01735189\', \'n01737021\', \'n01739381\', \'n01740131\', \'n01742172\', \'n01744401\', \'n01748264\', \'n01749939\', \'n01751748\', \'n01753488\', \'n01755581\', \'n01756291\'),\n    # utensil n04516672\n    (\'n03133878\', \'n03400231\', \'n04596742\', \'n02939185\', \'n03063689\', \'n04398044\', \'n04270147\'),\n    # vegetable n07707451\n    (\'n07711569\', \'n07720875\', \'n07711569\', \'n07714571\', \'n07714990\', \'n07715103\', \'n07717410\', \'n07717556\', \'n07716358\', \'n07716906\', \'n07718472\', \'n07718747\', \'n07730033\', \'n07734744\'),\n    # vehicle n04576211\n    (\'n02835271\', \'n03792782\', \'n03393912\', \'n03895866\', \'n02797295\', \'n04204347\', \'n03791053\', \'n04389033\', \'n03384352\', \'n03272562\', \'n04310018\', \'n02704792\', \'n02701002\', \'n02814533\', \'n02930766\', \'n03100240\', \'n03594945\', \'n03670208\', \'n03770679\', \'n03777568\', \'n04037443\', \'n04285008\', \'n03444034\', \'n03445924\', \'n03785016\', \'n04252225\', \'n03345487\', \'n03417042\', \'n03930630\', \'n04461696\', \'n04467665\', \'n03796401\', \'n03770679\', \'n03977966\', \'n04065272\', \'n04335435\', \'n03478589\', \'n04389033\', \'n04252077\', \'n04465501\', \'n03776460\', \'n04482393\', \'n04509417\', \'n03538406\', \'n03599486\', \'n03868242\')\n)\n\n\n# this excludes ImageNet-1K classes\nim22k_classes = (\n    # amphibian n01627424\n    (\n    \'n01627424\', \'n01639765\', \'n01640846\', \'n01641206\', \'n01641391\', \'n01641739\', \'n01641930\', \'n01642097\', \'n01642257\',\n    \'n01642391\', \'n01642539\', \'n01642943\', \'n01643255\', \'n01643507\', \'n01643896\', \'n01645466\', \'n01645776\', \'n01646292\',\n    \'n01646388\', \'n01646555\', \'n01646648\', \'n01646802\', \'n01646902\', \'n01647033\', \'n01647180\', \'n01647303\', \'n01647466\',\n    \'n01647640\', \'n01648139\', \'n01648356\', \'n01648620\', \'n01649170\', \'n01649412\', \'n01649556\', \'n01649726\', \'n01650167\',\n    \'n01650690\', \'n01650901\', \'n01651059\', \'n01651285\', \'n01651487\', \'n01651641\', \'n01651778\', \'n01652026\', \'n01652297\',\n    \'n01653026\', \'n01653223\', \'n01653509\', \'n01653773\', \'n01654083\', \'n01654637\', \'n01654863\', \'n01628331\', \'n01628770\',\n    \'n01629276\', \'n01629962\', \'n01630148\', \'n01630284\', \'n01630901\', \'n01631175\', \'n01631354\', \'n01631512\', \'n01632047\',\n    \'n01632308\', \'n01632601\', \'n01632952\', \'n01633406\', \'n01633781\', \'n01634227\', \'n01634522\', \'n01635027\', \'n01635176\',\n    \'n01635480\', \'n01636127\', \'n01636352\', \'n01636510\', \'n01636829\', \'n01637112\', \'n01637338\', \'n01637615\', \'n01637932\',\n    \'n01638194\', \'n01638329\', \'n01638722\', \'n01639187\', \'n01655344\'),\n    # appliance n02729837\n    (\n    \'n02729837\', \'n03251766\', \'n03050655\', \'n03717285\', \'n04277826\', \'n04496726\', \'n04607242\', \'n03528263\', \'n04174101\',\n    \'n03150511\', \'n04309833\', \'n04475631\', \'n03620052\', \'n03063338\', \'n04219185\', \'n03212114\', \'n03378174\', \'n03543254\',\n    \'n03557692\', \'n03862676\', \'n02905036\', \'n03425241\', \'n04388473\', \'n04330340\', \'n03102371\', \'n03273740\', \'n03425595\',\n    \'n03991202\', \'n04003241\', \'n04280487\', \'n04442441\', \'n04488857\', \'n03534776\', \'n04580493\', \'n03050655\', \'n03717285\',\n    \'n04277826\', \'n04496726\', \'n04607242\', \'n03273740\', \'n03102654\', \'n03273913\', \'n03557590\', \'n03170635\'),\n    # aquatic mammal n02062017\n    (\n    \'n02062017\', \'n02073250\', \'n02073831\', \'n02074726\', \'n02075927\', \'n02076196\', \'n02076779\', \'n02078574\', \'n02078292\',\n    \'n02078738\', \'n02079005\', \'n02077658\', \'n02077787\', \'n02077152\', \'n02077384\', \'n02076402\', \'n02079389\', \'n02081060\',\n    \'n02079851\', \'n02080146\', \'n02080415\', \'n02080713\', \'n02081571\', \'n02081798\', \'n02081927\', \'n02062430\', \'n02062744\',\n    \'n02066707\', \'n02068974\', \'n02072040\', \'n02069701\', \'n02069974\', \'n02070174\', \'n02072798\', \'n02069412\', \'n02071028\',\n    \'n02070430\', \'n02070624\', \'n02070776\', \'n02071636\', \'n02067240\', \'n02068206\', \'n02068541\', \'n02067768\', \'n02067603\',\n    \'n02072493\', \'n02063224\', \'n02063662\', \'n02064000\', \'n02064816\', \'n02064338\', \'n02065026\', \'n02065407\', \'n02065263\',\n    \'n02065726\'),\n    # bird n01503061\n    (\n    \'n01503061\', \'n01503976\', \'n01514752\', \'n01514926\', \'n01515078\', \'n01515217\', \'n01515303\', \'n01516212\', \'n01517389\',\n    \'n01517565\', \'n01519563\', \'n01519873\', \'n01520576\', \'n01521399\', \'n01521756\', \'n01522450\', \'n01523105\', \'n01517966\',\n    \'n01524359\', \'n01525720\', \'n01526521\', \'n01526766\', \'n01527194\', \'n01527347\', \'n01527617\', \'n01527917\', \'n01528396\',\n    \'n01528654\', \'n01528845\', \'n01529672\', \'n01530439\', \'n01531344\', \'n01531512\', \'n01531639\', \'n01531811\', \'n01531971\',\n    \'n01532325\', \'n01532511\', \'n01533000\', \'n01533339\', \'n01533481\', \'n01533651\', \'n01533893\', \'n01534155\', \'n01534582\',\n    \'n01534762\', \'n01535140\', \'n01535469\', \'n01535690\', \'n01536035\', \'n01536186\', \'n01536334\', \'n01536644\', \'n01536780\',\n    \'n01537134\', \'n01537895\', \'n01538059\', \'n01538200\', \'n01538362\', \'n01538630\', \'n01540233\', \'n01540566\', \'n01540832\',\n    \'n01541102\', \'n01541386\', \'n01541760\', \'n01541922\', \'n01542433\', \'n01542168\', \'n01544704\', \'n01538955\', \'n01539272\',\n    \'n01542786\', \'n01543175\', \'n01543383\', \'n01543632\', \'n01543936\', \'n01544208\', \'n01544389\', \'n01555809\', \'n01556182\',\n    \'n01556514\', \'n01557185\', \'n01557962\', \'n01558149\', \'n01558307\', \'n01558461\', \'n01558594\', \'n01558765\', \'n01559160\',\n    \'n01559477\', \'n01559639\', \'n01559804\', \'n01560105\', \'n01560280\', \'n01560636\', \'n01560793\', \'n01560935\', \'n01561181\',\n    \'n01561452\', \'n01561732\', \'n01562014\', \'n01562265\', \'n01562451\', \'n01563128\', \'n01563449\', \'n01563746\', \'n01563945\',\n    \'n01564101\', \'n01564217\', \'n01564394\', \'n01564773\', \'n01565345\', \'n01565599\', \'n01565930\', \'n01566207\', \'n01564914\',\n    \'n01565078\', \'n01567133\', \'n01567678\', \'n01567879\', \'n01568132\', \'n01568294\', \'n01568720\', \'n01568892\', \'n01569060\',\n    \'n01569262\', \'n01569423\', \'n01569566\', \'n01569836\', \'n01569971\', \'n01570267\', \'n01570421\', \'n01570676\', \'n01570839\',\n    \'n01566645\', \'n01571410\', \'n01571904\', \'n01572328\', \'n01572489\', \'n01572654\', \'n01572782\', \'n01573074\', \'n01573240\',\n    \'n01573360\', \'n01573627\', \'n01573898\', \'n01574045\', \'n01574390\', \'n01574560\', \'n01574801\', \'n01575117\', \'n01575401\',\n    \'n01575745\', \'n01576076\', \'n01576358\', \'n01576695\', \'n01577035\', \'n01577458\', \'n01577659\', \'n01577941\', \'n01578180\',\n    \'n01578575\', \'n01579028\', \'n01579149\', \'n01579260\', \'n01579410\', \'n01579578\', \'n01579729\', \'n01580379\', \'n01580490\',\n    \'n01580772\', \'n01580870\', \'n01581166\', \'n01581434\', \'n01581730\', \'n01581874\', \'n01581984\', \'n01582398\', \'n01582498\',\n    \'n01582856\', \'n01583209\', \'n01583495\', \'n01583828\', \'n01586941\', \'n01587278\', \'n01587526\', \'n01587834\', \'n01588002\',\n    \'n01588431\', \'n01588725\', \'n01588996\', \'n01589286\', \'n01589718\', \'n01589893\', \'n01590220\', \'n01591005\', \'n01591123\',\n    \'n01591301\', \'n01591697\', \'n01592257\', \'n01592540\', \'n01592387\', \'n01592694\', \'n01593028\', \'n01593282\', \'n01593553\',\n    \'n01594004\', \'n01594372\', \'n01594787\', \'n01594968\', \'n01595168\', \'n01595450\', \'n01595624\', \'n01595974\', \'n01596273\',\n    \'n01596608\', \'n01597022\', \'n01597336\', \'n01597737\', \'n01597906\', \'n01598074\', \'n01598271\', \'n01598588\', \'n01598988\',\n    \'n01599159\', \'n01599269\', \'n01599388\', \'n01599556\', \'n01599741\', \'n01600085\', \'n01600341\', \'n01600657\', \'n01601410\',\n    \'n01601068\', \'n01602080\', \'n01602209\', \'n01602630\', \'n01602832\', \'n01603000\', \'n01603152\', \'n01603600\', \'n01603812\',\n    \'n01603953\', \'n01539573\', \'n01539925\', \'n01540090\', \'n01545574\', \'n01546039\', \'n01546506\', \'n01546921\', \'n01547832\',\n    \'n01548301\', \'n01548492\', \'n01548694\', \'n01548865\', \'n01549053\', \'n01549430\', \'n01549641\', \'n01549886\', \'n01550172\',\n    \'n01550761\', \'n01551080\', \'n01551300\', \'n01552034\', \'n01552333\', \'n01555305\', \'n01551711\', \'n01552813\', \'n01553142\',\n    \'n01553527\', \'n01553762\', \'n01554017\', \'n01554448\', \'n01555004\', \'n01584225\', \'n01584695\', \'n01584853\', \'n01585121\',\n    \'n01585287\', \'n01585422\', \'n01585715\', \'n01586020\', \'n01586374\', \'n01524761\', \'n01604330\', \'n01604968\', \'n01605630\',\n    \'n01606097\', \'n01606177\', \'n01606522\', \'n01606672\', \'n01606809\', \'n01606978\', \'n01607309\', \'n01607429\', \'n01607600\',\n    \'n01607812\', \'n01607962\', \'n01608265\', \'n01608814\', \'n01609062\', \'n01609391\', \'n01609751\', \'n01609956\', \'n01610100\',\n    \'n01610226\', \'n01610552\', \'n01610955\', \'n01611472\', \'n01611674\', \'n01611800\', \'n01611969\', \'n01612122\', \'n01612275\',\n    \'n01612476\', \'n01612628\', \'n01612955\', \'n01613177\', \'n01616086\', \'n01613294\', \'n01613807\', \'n01614038\', \'n01614343\',\n    \'n01614556\', \'n01615121\', \'n01615303\', \'n01615458\', \'n01615703\', \'n01616551\', \'n01616764\', \'n01617095\', \'n01617443\',\n    \'n01617766\', \'n01618082\', \'n01618922\', \'n01619310\', \'n01619536\', \'n01619835\', \'n01620135\', \'n01620414\', \'n01620735\',\n    \'n01618503\', \'n01621127\', \'n01621635\', \'n01622120\', \'n01622352\', \'n01622483\', \'n01622959\', \'n01623110\', \'n01623425\',\n    \'n01623615\', \'n01623706\', \'n01624115\', \'n01624212\', \'n01623880\', \'n01624305\', \'n01624537\', \'n01624833\', \'n01625121\',\n    \'n01625562\', \'n01789386\', \'n01789740\', \'n01790171\', \'n01790304\', \'n01790398\', \'n01790557\', \'n01790711\', \'n01790812\',\n    \'n01791625\', \'n01792042\', \'n01792158\', \'n01792429\', \'n01792530\', \'n01792640\', \'n01792808\', \'n01792955\', \'n01793085\',\n    \'n01793159\', \'n01793249\', \'n01793340\', \'n01793435\', \'n01793565\', \'n01793715\', \'n01791954\', \'n01794158\', \'n01794344\',\n    \'n01809106\', \'n01809371\', \'n01791107\', \'n01791314\', \'n01791388\', \'n01791463\', \'n01794651\', \'n01799302\', \'n01800195\',\n    \'n01799679\', \'n01800424\', \'n01800633\', \'n01801088\', \'n01801479\', \'n01801672\', \'n01801876\', \'n01802159\', \'n01809752\',\n    \'n01810700\', \'n01811243\', \'n01811909\', \'n01812187\', \'n01812337\', \'n01813385\', \'n01813532\', \'n01813658\', \'n01813948\',\n    \'n01814217\', \'n01812662\', \'n01812866\', \'n01813088\', \'n01814370\', \'n01814620\', \'n01814755\', \'n01814921\', \'n01815036\',\n    \'n01814549\', \'n01815270\', \'n01815601\', \'n01816017\', \'n01816140\', \'n01816474\', \'n02153203\', \'n01795088\', \'n01795735\',\n    \'n01795900\', \'n01796019\', \'n01796105\', \'n01796519\', \'n01796729\', \'n01797020\', \'n01797307\', \'n01797601\', \'n01798168\',\n    \'n01798706\', \'n01798839\', \'n01798979\', \'n01802721\', \'n01803078\', \'n01803362\', \'n01803641\', \'n01803893\', \'n01804163\',\n    \'n01805321\', \'n01805801\', \'n01806061\', \'n01806297\', \'n01806364\', \'n01806467\', \'n01807105\', \'n01804478\', \'n01804653\',\n    \'n01804921\', \'n01805070\', \'n01806847\', \'n01807828\', \'n01808140\', \'n01808291\', \'n01808596\', \'n01810268\', \'n01816887\',\n    \'n01817263\', \'n01817346\', \'n01818299\', \'n01818832\', \'n01819115\', \'n01819465\', \'n01819734\', \'n01820052\', \'n01820348\',\n    \'n01820801\', \'n01821076\', \'n01821203\', \'n01821554\', \'n01821869\', \'n01822300\', \'n01822602\', \'n01823013\', \'n01823414\',\n    \'n01823740\', \'n01824035\', \'n01824344\', \'n01824749\', \'n01825278\', \'n01825930\', \'n01826364\', \'n01826680\', \'n01826844\',\n    \'n01827403\', \'n01827793\', \'n01828096\', \'n01828556\', \'n01829869\', \'n01830042\', \'n01830479\', \'n01830915\', \'n01831360\',\n    \'n01831712\', \'n01832167\', \'n01832493\', \'n01832813\', \'n01833112\', \'n01833415\', \'n01834177\', \'n01834540\', \'n01835276\',\n    \'n01835769\', \'n01835918\', \'n01836087\', \'n01836673\', \'n01837072\', \'n01837526\', \'n01838038\', \'n01838598\', \'n01839086\',\n    \'n01839330\', \'n01839598\', \'n01839750\', \'n01839949\', \'n01840120\', \'n01840412\', \'n01840775\', \'n01841102\', \'n01841288\',\n    \'n01841441\', \'n01841679\', \'n01841943\', \'n01842235\', \'n01842504\', \'n01842788\', \'n01843719\', \'n01844231\', \'n01844551\',\n    \'n01844746\', \'n01844917\', \'n01845132\', \'n01860497\', \'n01860864\', \'n01861148\', \'n01861330\', \'n01845477\', \'n01856072\',\n    \'n01856155\', \'n01856380\', \'n01856553\', \'n01856890\', \'n01857079\', \'n01857325\', \'n01857512\', \'n01857632\', \'n01857851\',\n    \'n01846331\', \'n01847089\', \'n01847170\', \'n01847253\', \'n01847407\', \'n01847806\', \'n01847978\', \'n01848123\', \'n01848323\',\n    \'n01848453\', \'n01848555\', \'n01848648\', \'n01848840\', \'n01848976\', \'n01849157\', \'n01849466\', \'n01849676\', \'n01849863\',\n    \'n01850192\', \'n01850373\', \'n01850553\', \'n01850873\', \'n01851038\', \'n01851207\', \'n01851375\', \'n01851731\', \'n01851573\',\n    \'n01851895\', \'n01852142\', \'n01852329\', \'n01852400\', \'n01852671\', \'n01852861\', \'n01853195\', \'n01853498\', \'n01853870\',\n    \'n01854415\', \'n01858441\', \'n01858281\', \'n01858780\', \'n01858845\', \'n01858906\', \'n01859190\', \'n01859325\', \'n01859496\',\n    \'n01859689\', \'n01859852\', \'n01860002\', \'n02000954\', \'n02002075\', \'n02003037\', \'n02003204\', \'n02003577\', \'n02003839\',\n    \'n02004131\', \'n02004492\', \'n02004855\', \'n02005399\', \'n02005790\', \'n02006063\', \'n02006364\', \'n02007284\', \'n02006985\',\n    \'n02008041\', \'n02008497\', \'n02008643\', \'n02008796\', \'n02009380\', \'n02009508\', \'n02009750\', \'n02010272\', \'n02010453\',\n    \'n02010728\', \'n02011016\', \'n02011281\', \'n02011805\', \'n02011943\', \'n02012185\', \'n02013177\', \'n02013567\', \'n02014237\',\n    \'n02014524\', \'n02014941\', \'n02015357\', \'n02015554\', \'n02015797\', \'n02016066\', \'n02017725\', \'n02018027\', \'n02018368\',\n    \'n02019190\', \'n02019438\', \'n02019929\', \'n02020219\', \'n02020578\', \'n02021050\', \'n02021281\', \'n02022684\', \'n02023341\',\n    \'n02023855\', \'n02023992\', \'n02024185\', \'n02024479\', \'n02024763\', \'n02025043\', \'n02025389\', \'n02026059\', \'n02026948\',\n    \'n02027075\', \'n02027357\', \'n02027897\', \'n02028175\', \'n02028342\', \'n02028451\', \'n02028727\', \'n02028900\', \'n02029087\',\n    \'n02029378\', \'n02029706\', \'n02030035\', \'n02030224\', \'n02030287\', \'n02030837\', \'n02030568\', \'n02026629\', \'n02030996\',\n    \'n02031298\', \'n02031585\', \'n02031934\', \'n02032222\', \'n02032355\', \'n02032480\', \'n02032769\', \'n02033324\', \'n02033208\',\n    \'n02033561\', \'n02033779\', \'n02033882\', \'n02034129\', \'n02034295\', \'n02034661\', \'n02034971\', \'n02035210\', \'n02035402\',\n    \'n02035656\', \'n02036053\', \'n02036228\', \'n02036711\', \'n02037464\', \'n02037869\', \'n02038141\', \'n02038466\', \'n02038993\',\n    \'n02039171\', \'n02039780\', \'n02039497\', \'n02040266\', \'n02016358\', \'n02016659\', \'n02016816\', \'n02016956\', \'n02017475\',\n    \'n02021795\', \'n02040505\', \'n02041085\', \'n02043063\', \'n02043333\', \'n02041246\', \'n02041678\', \'n02041875\', \'n02042046\',\n    \'n02042180\', \'n02042472\', \'n02042759\', \'n02043808\', \'n02044178\', \'n02044778\', \'n02044908\', \'n02044517\', \'n02045369\',\n    \'n02045596\', \'n02045864\', \'n02046171\', \'n02046759\', \'n02046939\', \'n02047045\', \'n02047260\', \'n02047411\', \'n02047517\',\n    \'n02047614\', \'n02047975\', \'n02048115\', \'n02048353\', \'n02048698\', \'n02049088\', \'n02049532\', \'n02050004\', \'n02050313\',\n    \'n02050442\', \'n02050586\', \'n02050809\', \'n02051059\', \'n02051474\', \'n02052365\', \'n02052204\', \'n02052775\', \'n02053083\',\n    \'n02053425\', \'n02053584\', \'n02054036\', \'n02054502\', \'n02054711\', \'n02055107\', \'n02055658\', \'n02055803\', \'n02056228\',\n    \'n02056728\', \'n02057035\', \'n02057330\', \'n02057731\', \'n02057898\', \'n02058594\', \'n02058747\', \'n02059162\', \'n02059541\',\n    \'n02059852\', \'n02060133\', \'n02060411\', \'n02060569\', \'n02060889\', \'n02061217\', \'n02061560\', \'n02061853\', \'n02511730\'),\n    # bear n02131653\n    (\'n02131653\', \'n02133704\', \'n02132788\', \'n02132466\', \'n02132580\', \'n02132320\', \'n02133400\', \'n01322983\'),\n    # beverage n07881800\n    (\n    \'n07881800\', \'n07925966\', \'n07926346\', \'n07926250\', \'n07926442\', \'n07933274\', \'n07934373\', \'n07933652\', \'n07933799\',\n    \'n07933891\', \'n07934032\', \'n07934152\', \'n07934282\', \'n07913180\', \'n07929519\', \'n07920349\', \'n07731122\', \'n07731284\',\n    \'n07731436\', \'n07921239\', \'n07919665\', \'n07919572\', \'n07919441\', \'n07920872\', \'n07920222\', \'n07919787\', \'n07919894\',\n    \'n07920540\', \'n07920663\', \'n07929940\', \'n07926785\', \'n07922764\', \'n07890970\', \'n07891309\', \'n07883251\', \'n07883661\',\n    \'n07883384\', \'n07883510\', \'n07882420\', \'n07924033\', \'n07925116\', \'n07924366\', \'n07924443\', \'n07924747\', \'n07924560\',\n    \'n07924655\', \'n07924276\', \'n07924834\', \'n07924955\', \'n07914271\', \'n07891189\', \'n07921455\', \'n07921615\', \'n07921834\',\n    \'n07921948\', \'n07922041\', \'n07914128\', \'n07936263\', \'n07936093\', \'n07935737\', \'n07936745\', \'n14941787\', \'n07936979\',\n    \'n07937069\', \'n07936459\', \'n07936548\', \'n07914006\', \'n07927197\', \'n07928887\', \'n07927512\', \'n07927836\', \'n07928367\',\n    \'n07927931\', \'n07928696\', \'n07928790\', \'n07928163\', \'n07928264\', \'n07927716\', \'n07929172\', \'n07928578\', \'n07928998\',\n    \'n07928488\', \'n07884567\', \'n07886176\', \'n07885705\', \'n07922607\', \'n07905618\', \'n07886572\', \'n07890617\', \'n07886849\',\n    \'n07889510\', \'n07888465\', \'n07888816\', \'n07890068\', \'n07889814\', \'n07890352\', \'n07890540\', \'n07889990\', \'n07890226\',\n    \'n07887099\', \'n07887192\', \'n07887634\', \'n07887967\', \'n07889274\', \'n07888058\', \'n07887461\', \'n07888229\', \'n07887304\',\n    \'n07890750\', \'n07890890\', \'n07932323\', \'n07932454\', \'n07921615\', \'n07922147\', \'n07922512\', \'n07886463\', \'n07901587\',\n    \'n07902336\', \'n07904934\', \'n07905474\', \'n07903101\', \'n07903208\', \'n07904293\', \'n07903962\', \'n07904072\', \'n07902443\',\n    \'n07903731\', \'n07903841\', \'n07903643\', \'n07903543\', \'n07906284\', \'n07906718\', \'n07906572\', \'n07907429\', \'n07907831\',\n    \'n07907161\', \'n07907342\', \'n07907548\', \'n07909593\', \'n07906877\', \'n07902520\', \'n07902937\', \'n07904395\', \'n07904760\',\n    \'n07902698\', \'n07904637\', \'n07902799\', \'n07907037\', \'n07906111\', \'n07905038\', \'n07905296\', \'n07904865\', \'n07905386\',\n    \'n07905770\', \'n07905979\', \'n07907943\', \'n07910656\', \'n07909714\', \'n07908812\', \'n07908647\', \'n07908567\', \'n07908411\',\n    \'n07910245\', \'n07910379\', \'n07909504\', \'n07911249\', \'n07909811\', \'n07909954\', \'n07910048\', \'n07910152\', \'n07909593\',\n    \'n07908923\', \'n07909129\', \'n07910970\', \'n07910538\', \'n07911061\', \'n07909231\', \'n07909362\', \'n07910799\', \'n07925808\',\n    \'n07902121\', \'n07891726\', \'n07900225\', \'n07892813\', \'n07894551\', \'n07896893\', \'n07898443\', \'n07897975\', \'n07899769\',\n    \'n07898247\', \'n07899533\', \'n07895100\', \'n07897200\', \'n07897438\', \'n07897600\', \'n07894703\', \'n07899660\', \'n07895962\',\n    \'n07894799\', \'n07899899\', \'n07896765\', \'n07894451\', \'n07900406\', \'n07901355\', \'n07900825\', \'n07900616\', \'n07900734\',\n    \'n07901457\', \'n07900958\', \'n07899976\', \'n07893528\', \'n07893642\', \'n07893792\', \'n07896060\', \'n07896560\', \'n07897750\',\n    \'n07897865\', \'n07895710\', \'n07895839\', \'n07895435\', \'n07898117\', \'n07898333\', \'n07894965\', \'n07894102\', \'n07895595\',\n    \'n07894298\', \'n07896165\', \'n07897116\', \'n07896422\', \'n07892418\', \'n07893891\', \'n07894551\', \'n07894703\', \'n07894102\',\n    \'n07899108\', \'n07899434\', \'n07899292\', \'n07926920\', \'n07927070\', \'n07913300\', \'n07898745\', \'n07899003\', \'n07895237\',\n    \'n07895435\', \'n07898117\', \'n07894298\', \'n07893253\', \'n07896994\', \'n07893425\', \'n07896287\', \'n07898617\', \'n07896661\',\n    \'n07898895\', \'n07886057\', \'n07911371\', \'n07918879\', \'n07930554\', \'n07931280\', \'n07930205\', \'n07931001\', \'n07931096\',\n    \'n07931733\', \'n07930062\', \'n07931870\', \'n07912211\', \'n07913882\', \'n07917618\', \'n07917951\', \'n07917874\', \'n07917791\',\n    \'n07915491\', \'n07915094\', \'n07913774\', \'n07917507\', \'n07919165\', \'n07912093\', \'n07911677\', \'n07916041\', \'n07916319\',\n    \'n07917392\', \'n07915918\', \'n07915366\', \'n07931612\', \'n07913393\', \'n07913537\', \'n07916437\', \'n07914413\', \'n07914586\',\n    \'n07914686\', \'n07916582\', \'n07918028\', \'n07918193\', \'n07913644\', \'n07917133\', \'n07915618\', \'n07915800\', \'n07917272\',\n    \'n07931452\', \'n07916183\', \'n07915213\', \'n07918309\', \'n07914995\', \'n07930315\', \'n07914887\', \'n07918706\', \'n07930433\',\n    \'n07914777\', \'n07932614\', \'n07932762\', \'n07891433\', \'n07886317\', \'n07891095\', \'n07844042\', \'n07844786\', \'n07846143\',\n    \'n07846274\', \'n07845775\', \'n07921360\', \'n07846014\', \'n07845421\', \'n07847047\', \'n07846359\', \'n07845335\', \'n07846471\',\n    \'n07846557\', \'n07845702\', \'n07846688\', \'n07845495\', \'n07846802\', \'n07846938\', \'n07845863\', \'n07845166\', \'n07845571\',\n    \'n07919310\', \'n07933530\'),\n    # big cat n02127808\n    (\n    \'n02127808\', \'n02129991\', \'n02130545\', \'n02130925\', \'n02129923\', \'n02129837\', \'n01323068\', \'n02128598\', \'n02128669\',\n    \'n02129463\', \'n02129530\', \'n01322898\', \'n02130086\'),\n    # building n02913152\n    (\n    \'n02913152\', \'n02666943\', \'n02726681\', \'n04409384\', \'n02734725\', \'n02763604\', \'n02806992\', \'n02882190\', \'n02993546\',\n    \'n02940385\', \'n03078506\', \'n03089753\', \'n03097362\', \'n04077889\', \'n04175574\', \'n04177931\', \'n04343511\', \'n03007444\',\n    \'n03224893\', \'n03479397\', \'n03322570\', \'n03123809\', \'n04441662\', \'n03016389\', \'n04294879\', \'n03326371\', \'n03413428\',\n    \'n02977936\', \'n03430418\', \'n03449564\', \'n02956699\', \'n03005033\', \'n03121431\', \'n03152303\', \'n03203806\', \'n03093427\',\n    \'n03282295\', \'n04305210\', \'n04461437\', \'n03092166\', \'n13252672\', \'n03478756\', \'n03036022\', \'n03466839\', \'n03437184\',\n    \'n03698723\', \'n03479121\', \'n03479266\', \'n03542333\', \'n03541696\', \'n02961035\', \'n03561573\', \'n03989898\', \'n04097085\',\n    \'n03790755\', \'n03788498\', \'n04080705\', \'n04095109\', \'n04229737\', \'n08640531\', \'n08560295\', \'n08652376\', \'n03542605\',\n    \'n03544360\', \'n02814338\', \'n02857477\', \'n02820085\', \'n02919792\', \'n02932400\', \'n03686924\', \'n03002816\', \'n03007297\',\n    \'n03118969\', \'n03010915\', \'n03158186\', \'n04202142\', \'n04354026\', \'n04535252\', \'n04535370\', \'n03180865\', \'n03219483\',\n    \'n03257210\', \'n03322836\', \'n03428090\', \'n03685640\', \'n03465605\', \'n03474352\', \'n03685486\', \'n03685820\', \'n03367321\',\n    \'n03713151\', \'n03719053\', \'n03718458\', \'n03878066\', \'n04305323\', \'n04052658\', \'n04079244\', \'n03121040\', \'n03166514\',\n    \'n03718935\', \'n02695627\', \'n03892557\', \'n03439348\', \'n04073948\', \'n03099454\', \'n02667478\', \'n02667379\', \'n03396580\',\n    \'n03635032\', \'n04005197\', \'n04115256\', \'n02907873\', \'n04413969\', \'n04125541\', \'n04131368\', \'n04255899\', \'n04258438\',\n    \'n04465050\', \'n04535524\', \'n03545150\', \'n02806875\', \'n04350235\', \'n03121298\', \'n03333610\', \'n03736269\', \'n03837698\',\n    \'n04022708\', \'n04022866\', \'n04246731\', \'n03739518\', \'n03043274\', \'n03210552\', \'n03540595\', \'n03129471\', \'n03730334\',\n    \'n03762982\', \'n03333349\', \'n03770316\', \'n03785499\', \'n03130233\', \'n03402941\', \'n03839671\', \'n03842012\', \'n03859280\',\n    \'n03055857\', \'n03416489\', \'n02968074\', \'n03610524\', \'n03860404\', \'n04187547\', \'n03056288\', \'n04452757\', \'n04598318\',\n    \'n03872016\', \'n03953416\', \'n02833040\', \'n03007130\', \'n03006788\', \'n03633341\', \'n04214413\', \'n02667576\', \'n02801184\',\n    \'n02984061\', \'n03772077\', \'n02984203\', \'n03618982\', \'n03099622\', \'n03724756\', \'n04210390\', \'n04374735\', \'n04407435\',\n    \'n03602365\', \'n03884778\', \'n03999160\', \'n02844214\', \'n02892499\', \'n02897389\', \'n02935658\', \'n02936281\', \'n03155178\',\n    \'n03297644\', \'n03298089\', \'n02935891\', \'n02760099\', \'n02952485\', \'n02952674\', \'n03199647\', \'n03456548\', \'n03459914\',\n    \'n03497100\', \'n03697552\', \'n04111414\', \'n04307878\', \'n04398497\', \'n04081699\', \'n04093625\', \'n03558176\', \'n03557360\',\n    \'n04104500\', \'n02801047\', \'n04112654\', \'n04118635\', \'n04146050\', \'n03092314\', \'n03801671\', \'n02746978\', \'n03165616\',\n    \'n04217546\', \'n04233124\', \'n04343740\', \'n04395875\', \'n02823586\', \'n02910241\', \'n04018399\', \'n02696165\', \'n03393017\',\n    \'n04055700\', \'n07888378\', \'n04400109\', \'n04407686\', \'n04614655\', \'n04417809\', \'n03798982\', \'n03202481\', \'n03678729\',\n    \'n03801533\', \'n03849814\', \'n04581595\', \'n03726233\'),\n    # cat n02121620\n    (\n    \'n02121620\', \'n02121808\', \'n02122298\', \'n02123478\', \'n02122725\', \'n02124484\', \'n02124157\', \'n02122878\', \'n02123917\',\n    \'n02122510\', \'n02124313\', \'n02123242\', \'n02122430\', \'n02124623\', \'n02125081\', \'n02125872\', \'n02125010\', \'n02126787\',\n    \'n02126317\', \'n02125494\', \'n02125689\', \'n02126028\', \'n02126640\', \'n02126139\'),\n    # clothing n03051540\n    (\n    \'n03051540\', \'n04385079\', \'n02756098\', \'n03859958\', \'n04489695\', \'n02834506\', \'n03964611\', \'n03289985\', \'n04129766\',\n    \'n03114041\', \'n03113657\', \'n03320519\', \'n03340923\', \'n04355115\', \'n03786194\', \'n03114236\', \'n03206718\', \'n03320519\',\n    \'n03319457\', \'n03221059\', \'n02726017\', \'n03384891\', \'n02780704\', \'n03239054\', \'n03201638\', \'n03201776\', \'n04285803\',\n    \'n04120695\', \'n03472672\', \'n03476083\', \'n02683454\', \'n04459018\', \'n03917327\', \'n03538957\', \'n03263338\', \'n03113835\',\n    \'n02669534\', \'n04092168\', \'n03473966\', \'n03398153\', \'n03835941\', \'n03781467\', \'n03474167\', \'n02846141\', \'n03692379\',\n    \'n03692136\', \'n03692272\', \'n03692004\', \'n04243142\', \'n04600912\', \'n03863108\', \'n03655720\', \'n03815482\', \'n03068998\',\n    \'n03406759\', \'n04015204\', \'n04194127\', \'n03268645\', \'n03216402\', \'n03121897\', \'n03863262\', \'n03604763\', \'n04607640\',\n    \'n02752615\', \'n04429038\', \'n03015478\', \'n02841847\', \'n04423552\', \'n04001845\', \'n02720576\', \'n04266375\', \'n02863340\',\n    \'n02738859\', \'n03386870\', \'n04207903\', \'n03521771\', \'n02671780\', \'n02827606\', \'n04125853\', \'n04132465\', \'n02778294\',\n    \'n02972397\', \'n02786611\', \'n03527565\', \'n03781683\', \'n03405595\', \'n03859495\', \'n03450516\', \'n03434830\', \'n03010795\',\n    \'n03880129\', \'n02694966\', \'n04364994\', \'n03981340\', \'n03548930\', \'n02979516\', \'n04264233\', \'n04446162\', \'n02814774\',\n    \'n02742322\', \'n04552097\', \'n02855925\', \'n03419014\', \'n04332580\', \'n04488530\', \'n02922578\', \'n04531873\', \'n03608504\',\n    \'n04371563\', \'n04574067\', \'n04219580\', \'n02896294\', \'n03186199\', \'n03226538\', \'n04222470\', \'n03943920\', \'n02925519\',\n    \'n04427715\', \'n04504141\', \'n04615644\', \'n04233832\', \'n03885669\', \'n03657511\', \'n04046277\', \'n03789794\', \'n04123317\',\n    \'n03502331\', \'n03314884\', \'n03826039\', \'n04612159\', \'n02998841\', \'n04605572\', \'n04489008\', \'n03660124\', \'n04337287\',\n    \'n03877674\', \'n03600285\', \'n02896442\', \'n02902816\', \'n03970363\', \'n04491934\', \'n02910864\', \'n03107488\', \'n03688605\',\n    \'n03019434\', \'n03884554\', \'n04370288\', \'n04480303\', \'n03029296\', \'n04132158\', \'n04233715\', \'n03903733\', \'n04205318\',\n    \'n03653833\', \'n02831595\', \'n03543112\', \'n02825442\', \'n03357081\', \'n03745487\', \'n03487642\', \'n03450734\', \'n04143897\',\n    \'n03332173\', \'n03610992\', \'n03814817\', \'n03505504\', \'n03520493\', \'n03615655\', \'n02766168\', \'n04122262\', \'n04495698\',\n    \'n03719743\', \'n03900028\', \'n04060448\', \'n03797182\', \'n03648219\', \'n03816005\', \'n03815615\', \'n03845990\', \'n02847631\',\n    \'n04339191\', \'n03388323\', \'n03128085\', \'n02747063\', \'n03814727\', \'n03913930\', \'n04325804\', \'n04370774\', \'n04160261\',\n    \'n04570532\', \'n03605598\', \'n04104770\', \'n04230808\', \'n03454442\', \'n03617312\', \'n03429003\', \'n03205669\', \'n03732458\',\n    \'n02780815\', \'n03402511\', \'n03146777\', \'n03649003\', \'n03523506\', \'n03863923\', \'n03588216\', \'n03045337\', \'n03595055\',\n    \'n04497570\', \'n03472796\', \'n04361937\', \'n04378489\', \'n03619275\', \'n03607923\', \'n02956883\', \'n02936402\', \'n02923535\',\n    \'n03103904\', \'n03880032\', \'n02955767\', \'n04440597\', \'n03021228\', \'n03719560\', \'n03906789\', \'n03219859\', \'n04605446\',\n    \'n04445040\', \'n04445154\', \'n04186455\', \'n04173907\', \'n03998333\', \'n03849943\', \'n03057021\', \'n03703463\', \'n03770954\',\n    \'n04122492\', \'n04455579\', \'n04049405\', \'n03844815\', \'n02921406\', \'n03254046\', \'n03057841\', \'n03456665\', \'n04506402\',\n    \'n04365229\', \'n02957008\', \'n03589791\', \'n03238286\', \'n02867966\', \'n03595264\', \'n02788462\', \'n03221540\', \'n02864504\',\n    \'n04123026\', \'n03548320\', \'n03226375\', \'n03751269\', \'n04222307\', \'n03696909\', \'n02925385\', \'n02850358\', \'n03219966\',\n    \'n03720005\', \'n04368496\', \'n02820675\', \'n03902756\', \'n03891051\', \'n04230387\', \'n02937010\', \'n03301175\', \'n03829857\',\n    \'n03604536\', \'n03228254\', \'n04123448\', \'n03398228\', \'n04003359\', \'n04363777\', \'n04187970\', \'n02885233\', \'n04252560\',\n    \'n04357531\', \'n04367950\', \'n04370048\', \'n04021028\', \'n04502197\', \'n03655072\', \'n04269822\', \'n03410938\', \'n04027935\',\n    \'n03006903\', \'n04097866\', \'n02807616\', \'n03237992\', \'n04085574\', \'n04197391\', \'n04390577\', \'n03238879\', \'n04602956\',\n    \'n03978966\', \'n03476542\', \'n03163381\', \'n03629231\', \'n02943964\', \'n04502197\', \'n04172904\', \'n04508163\', \'n04223299\',\n    \'n02944146\', \'n04508489\', \'n02837887\', \'n04426427\', \'n02854739\', \'n03885028\', \'n02901114\', \'n03234164\', \'n04514241\',\n    \'n03387323\', \'n04103665\', \'n03112869\', \'n03885788\', \'n02863014\', \'n03013580\', \'n04508949\', \'n03688192\', \'n03673450\',\n    \'n04509171\', \'n03824381\', \'n04231905\', \'n02930214\', \'n03920737\', \'n03132776\', \'n03421324\', \'n03688707\', \'n03585875\',\n    \'n03404149\', \'n03540090\', \'n03456186\', \'n03490324\', \'n03796974\', \'n03441112\', \'n02811204\', \'n03429771\', \'n03447075\',\n    \'n03616979\', \'n03429682\', \'n03502509\', \'n03531281\', \'n03124474\', \'n02941845\', \'n03513137\', \'n04265428\', \'n04229107\',\n    \'n02811350\', \'n03504205\', \'n03492922\', \'n02954340\', \'n04129688\', \'n04228693\', \'n03103563\', \'n03061893\', \'n04387095\',\n    \'n02831237\', \'n03331077\', \'n04596116\', \'n03610682\', \'n02799323\', \'n03824284\', \'n02843909\', \'n04232153\', \'n03065243\',\n    \'n02816768\', \'n04612026\', \'n04556408\', \'n03607527\', \'n02776825\', \'n03776167\', \'n03420801\', \'n03049924\', \'n02941228\',\n    \'n03439631\', \'n04455048\', \'n04585318\', \'n03597317\', \'n04432203\', \'n03138669\', \'n03497657\', \'n04356595\', \'n03950899\',\n    \'n04354589\', \'n04354589\', \'n04248507\', \'n03984643\', \'n02987379\', \'n03256631\', \'n03061050\', \'n02834642\', \'n04482177\',\n    \'n04456011\', \'n02859184\', \'n04208582\', \'n02881757\', \'n03404360\', \'n02818135\', \'n04505888\', \'n04264361\', \'n02945964\',\n    \'n03237416\', \'n03766322\', \'n03931885\', \'n03046029\', \'n03937835\', \'n03028785\', \'n03170872\', \'n03325941\', \'n04441528\',\n    \'n03607186\', \'n04498389\', \'n04335693\', \'n03381126\', \'n03540267\', \'n04434932\', \'n03885904\', \'n02713218\', \'n04378956\',\n    \'n03622931\', \'n02736798\', \'n02752496\', \'n04323819\', \'n04360914\', \'n03622931\', \'n03836976\', \'n02874336\', \'n04532022\',\n    \'n04059157\', \'n04509592\', \'n03605504\', \'n04071393\', \'n03402188\', \'n03239259\', \'n03237212\', \'n03846234\', \'n02811719\',\n    \'n03615563\', \'n03324928\', \'n03825080\', \'n04235771\', \'n03824381\', \'n03824999\', \'n03625943\', \'n02728440\', \'n04603872\',\n    \'n03660124\', \'n04602956\', \'n03746330\', \'n02752615\', \'n02887489\', \'n03237416\', \'n04596852\', \'n03236735\', \'n03619196\',\n    \'n03788914\', \'n04197878\', \'n03604400\', \'n02936570\', \'n03013438\', \'n03062015\', \'n02781121\', \'n02898585\', \'n03201638\',\n    \'n04397645\', \'n04334105\', \'n03057724\', \'n03205574\', \'n04355511\', \'n03978815\', \'n03786096\', \'n04136161\', \'n03817647\',\n    \'n02908123\', \'n02944075\', \'n02697221\', \'n04453666\', \'n02861387\', \'n02926426\', \'n03480579\', \'n02854926\', \'n03418749\',\n    \'n03467254\', \'n04198562\', \'n03762238\', \'n04514241\', \'n03464053\', \'n04241249\', \'n03036469\', \'n03036341\', \'n03797264\'),\n    # dog n02084071\n    (\n    \'n02084071\', \'n02084732\', \'n02087122\', \'n02098550\', \'n02099997\', \'n02100399\', \'n02098806\', \'n02101108\', \'n02101670\',\n    \'n02102605\', \'n02102806\', \'n02101861\', \'n02103181\', \'n02098906\', \'n02099029\', \'n02089232\', \'n02089468\', \'n02092468\',\n    \'n02095050\', \'n02095212\', \'n02095412\', \'n02095727\', \'n02096756\', \'n02093056\', \'n02097786\', \'n02097967\', \'n02094562\',\n    \'n02094721\', \'n02094931\', \'n02087314\', \'n02087551\', \'n02090253\', \'n02090475\', \'n02088839\', \'n02088992\', \'n02089555\',\n    \'n02089725\', \'n02092173\', \'n02090827\', \'n02090129\', \'n02088745\', \'n02110532\', \'n02084861\', \'n02085118\', \'n02085019\',\n    \'n02112826\', \'n02085272\', \'n02113335\', \'n02113892\', \'n02112497\', \'n02103406\', \'n02109150\', \'n02109256\', \'n02104523\',\n    \'n02104882\', \'n02103841\', \'n02106966\', \'n02104280\', \'n02104184\', \'n02106854\', \'n02109687\', \'n02109811\', \'n02107420\',\n    \'n02108254\', \'n02109391\', \'n02108672\', \'n02111626\', \'n02085374\', \'n02086346\', \'n02086753\', \'n02086478\', \'n01322604\'),\n    # electronic equipment n03278248\n    (\n    \'n03278248\', \'n02757462\', \'n04077430\', \'n03517760\', \'n04315948\', \'n04546340\', \'n03436182\', \'n04030965\', \'n03584400\',\n    \'n04064213\', \'n04043411\', \'n04413419\', \'n04075291\', \'n02676670\', \'n03181293\', \'n03143400\', \'n04142731\', \'n03034405\',\n    \'n03775388\', \'n04176528\', \'n04060647\', \'n03205304\', \'n03447894\', \'n04042204\', \'n04137773\', \'n04043733\', \'n03144156\',\n    \'n03516996\', \'n03046921\', \'n04027367\', \'n04405907\', \'n04472726\', \'n04138131\', \'n04406552\', \'n03592931\', \'n04045085\',\n    \'n04269668\', \'n04405762\', \'n02705944\', \'n02872529\', \'n02756854\', \'n03724176\', \'n03424204\', \'n04405540\', \'n04404997\',\n    \'n02942349\', \'n02995345\', \'n03916720\', \'n03225777\', \'n04595285\', \'n03571942\', \'n02909285\', \'n03861048\', \'n03163973\',\n    \'n04143140\', \'n03781787\', \'n02962938\', \'n03278914\', \'n03963294\', \'n03293741\', \'n03656957\', \'n04392526\', \'n02979074\',\n    \'n04401088\', \'n03488438\', \'n03306869\', \'n04044498\', \'n03179910\', \'n04270371\', \'n03842377\'),\n    # fish n02512053\n    (\n    \'n02512053\', \'n01316579\', \'n02599958\', \'n02600298\', \'n02600503\', \'n02600798\', \'n01480516\', \'n01482071\', \'n01495701\',\n    \'n01497118\', \'n01497413\', \'n01497738\', \'n01498406\', \'n01498699\', \'n01498989\', \'n01499396\', \'n01499732\', \'n01500091\',\n    \'n01500854\', \'n01500476\', \'n01501160\', \'n01501641\', \'n01501777\', \'n01501948\', \'n01502101\', \'n01482330\', \'n01483021\',\n    \'n01483522\', \'n01483830\', \'n01484097\', \'n01484285\', \'n01484447\', \'n01484562\', \'n01485479\', \'n01486010\', \'n01486540\',\n    \'n01486838\', \'n01487506\', \'n01488038\', \'n01488918\', \'n01489501\', \'n01489709\', \'n01489920\', \'n01490112\', \'n01490360\',\n    \'n01490670\', \'n01491006\', \'n01491661\', \'n01491874\', \'n01492357\', \'n01492569\', \'n01492708\', \'n01492860\', \'n01493146\',\n    \'n01493541\', \'n01493829\', \'n01494041\', \'n01494757\', \'n01494882\', \'n01495006\', \'n01495493\', \'n01480880\', \'n01481331\',\n    \'n01481498\', \'n02512752\', \'n02512830\', \'n02512938\', \'n02513355\', \'n02530421\', \'n02530637\', \'n02530831\', \'n02530999\',\n    \'n02532028\', \'n02532272\', \'n02532451\', \'n02532602\', \'n02532918\', \'n02532786\', \'n02534734\', \'n02535163\', \'n02535258\',\n    \'n02535537\', \'n02535759\', \'n02536165\', \'n02536456\', \'n02537085\', \'n02537319\', \'n02537716\', \'n02537525\', \'n02538010\',\n    \'n02538216\', \'n02538985\', \'n02539424\', \'n02539573\', \'n02539894\', \'n02567334\', \'n02567633\', \'n02568087\', \'n02568447\',\n    \'n02568959\', \'n02569484\', \'n02569631\', \'n02569905\', \'n02570164\', \'n02586543\', \'n02587051\', \'n02587300\', \'n02587479\',\n    \'n02587618\', \'n02587877\', \'n02626762\', \'n02627037\', \'n02627292\', \'n02627532\', \'n02663849\', \'n02664285\', \'n02664642\',\n    \'n02665250\', \'n02513248\', \'n02513560\', \'n02513727\', \'n02530052\', \'n02530188\', \'n02535080\', \'n02513805\', \'n02513939\',\n    \'n02515214\', \'n02515713\', \'n02516188\', \'n02516776\', \'n02528163\', \'n02538985\', \'n02539424\', \'n02539573\', \'n02539894\',\n    \'n01429172\', \'n01438208\', \'n01438581\', \'n01439121\', \'n01439514\', \'n01439808\', \'n01441117\', \'n01441272\', \'n01441425\',\n    \'n01441910\', \'n01442450\', \'n01442710\', \'n01442972\', \'n01443243\', \'n01443831\', \'n01444339\', \'n01444783\', \'n01445429\',\n    \'n01445857\', \'n01446152\', \'n01446589\', \'n01446760\', \'n01447139\', \'n01447331\', \'n01447658\', \'n01447946\', \'n01448291\',\n    \'n01448594\', \'n01448951\', \'n01449374\', \'n01449712\', \'n01449980\', \'n02583567\', \'n02583890\', \'n02584145\', \'n02584449\',\n    \'n02517442\', \'n02517938\', \'n02518622\', \'n02518324\', \'n02519148\', \'n02519340\', \'n02519472\', \'n02519686\', \'n02519862\',\n    \'n02520147\', \'n02520525\', \'n02520810\', \'n02521646\', \'n02522399\', \'n02522637\', \'n02522722\', \'n02522866\', \'n02523427\',\n    \'n02523110\', \'n02523877\', \'n02524202\', \'n02524524\', \'n02524928\', \'n02524659\', \'n02525382\', \'n02525703\', \'n02526425\',\n    \'n02526818\', \'n02527057\', \'n02527271\', \'n02527622\', \'n02529293\', \'n02529772\', \'n02530421\', \'n02530637\', \'n02530831\',\n    \'n02530999\', \'n02532028\', \'n02532272\', \'n02532451\', \'n02532602\', \'n02532918\', \'n02532786\', \'n02531114\', \'n02531625\',\n    \'n02533209\', \'n02533545\', \'n02533834\', \'n02534165\', \'n02534559\', \'n02534734\', \'n02535163\', \'n02535258\', \'n02535537\',\n    \'n02535759\', \'n02536165\', \'n02536456\', \'n02537085\', \'n02537319\', \'n02537716\', \'n02537525\', \'n02538010\', \'n02538216\',\n    \'n02538406\', \'n02538562\', \'n02540412\', \'n02540983\', \'n02541257\', \'n02541687\', \'n02542017\', \'n02542432\', \'n02542958\',\n    \'n02543255\', \'n02543565\', \'n02544274\', \'n02545841\', \'n02546028\', \'n02546331\', \'n02546627\', \'n02547014\', \'n01454545\',\n    \'n01455778\', \'n01456137\', \'n01456454\', \'n01456756\', \'n01457082\', \'n01457407\', \'n01457852\', \'n02549989\', \'n02550203\',\n    \'n02550460\', \'n02550655\', \'n02551134\', \'n02551668\', \'n02552171\', \'n01450661\', \'n01450950\', \'n01451115\', \'n01451295\',\n    \'n01451426\', \'n01451863\', \'n01452345\', \'n01453087\', \'n01453475\', \'n01453742\', \'n01454856\', \'n01455461\', \'n01455317\',\n    \'n02547733\', \'n02548247\', \'n02548689\', \'n02548884\', \'n02549248\', \'n02549376\', \'n02554730\', \'n02599958\', \'n02600298\',\n    \'n02600503\', \'n02600798\', \'n02586543\', \'n02587051\', \'n02587300\', \'n02587479\', \'n02587618\', \'n02587877\', \'n02555863\',\n    \'n02556846\', \'n02557182\', \'n02557318\', \'n02557591\', \'n02557749\', \'n02558206\', \'n02558860\', \'n02559144\', \'n02559383\',\n    \'n02559862\', \'n02560110\', \'n02561108\', \'n02561381\', \'n02561514\', \'n02561661\', \'n02561803\', \'n02561937\', \'n02562315\',\n    \'n02562796\', \'n02562971\', \'n02563079\', \'n02563182\', \'n01440467\', \'n02563792\', \'n02563949\', \'n02563648\', \'n02564403\',\n    \'n02564720\', \'n02564935\', \'n02565072\', \'n02565324\', \'n02565573\', \'n02564270\', \'n02566109\', \'n02567334\', \'n02567633\',\n    \'n02568087\', \'n02568447\', \'n02568959\', \'n02566489\', \'n02566665\', \'n02570484\', \'n02570838\', \'n02571167\', \'n02571652\',\n    \'n02571810\', \'n02572196\', \'n02572484\', \'n02573249\', \'n02573704\', \'n02574271\', \'n02576223\', \'n02576575\', \'n02576906\',\n    \'n02577041\', \'n02577164\', \'n02577403\', \'n02577662\', \'n02577952\', \'n02578771\', \'n02578928\', \'n02579303\', \'n02578233\',\n    \'n02578454\', \'n02579557\', \'n02579762\', \'n02579928\', \'n02580336\', \'n02580679\', \'n02580830\', \'n02581108\', \'n02581482\',\n    \'n02581642\', \'n02581957\', \'n02582220\', \'n02582349\', \'n02585872\', \'n02586238\', \'n02588286\', \'n02588794\', \'n02588945\',\n    \'n02589062\', \'n02589196\', \'n02589316\', \'n02589623\', \'n02589796\', \'n02590094\', \'n02590495\', \'n02592055\', \'n02592371\',\n    \'n02593019\', \'n02590702\', \'n02582721\', \'n02590987\', \'n02591330\', \'n02592734\', \'n02593453\', \'n02593679\', \'n02591613\',\n    \'n02591911\', \'n02593191\', \'n02594250\', \'n02594942\', \'n02595056\', \'n02595339\', \'n02595702\', \'n02596067\', \'n02596252\',\n    \'n02596381\', \'n02596720\', \'n02597004\', \'n02598573\', \'n02598878\', \'n02597367\', \'n02597608\', \'n02597818\', \'n02597972\',\n    \'n02598134\', \'n02599052\', \'n02599557\', \'n02599347\', \'n02601344\', \'n02601767\', \'n02601921\', \'n02602059\', \'n02604157\',\n    \'n02604480\', \'n02604954\', \'n02605316\', \'n02605703\', \'n02605936\', \'n02606384\', \'n02606751\', \'n02607201\', \'n02607470\',\n    \'n02607862\', \'n02608284\', \'n02608547\', \'n02608860\', \'n02608996\', \'n02609302\', \'n02609823\', \'n02610066\', \'n02610373\',\n    \'n02610664\', \'n02610980\', \'n02611561\', \'n02611898\', \'n02612167\', \'n02613181\', \'n02613572\', \'n02613820\', \'n02614140\',\n    \'n02614482\', \'n02614653\', \'n02614978\', \'n02615298\', \'n02616128\', \'n02616397\', \'n02616851\', \'n02617537\', \'n02618094\',\n    \'n02619165\', \'n02619550\', \'n02619861\', \'n02620167\', \'n02620578\', \'n02621258\', \'n02621908\', \'n02622249\', \'n02622547\',\n    \'n02622712\', \'n02622955\', \'n02623445\', \'n02626762\', \'n02627037\', \'n02627292\', \'n02627532\', \'n02624167\', \'n02624551\',\n    \'n02624807\', \'n02624987\', \'n02625258\', \'n02625612\', \'n02627835\', \'n02628062\', \'n02628259\', \'n02628600\', \'n02629230\',\n    \'n02629716\', \'n02630281\', \'n02630615\', \'n02630739\', \'n02631041\', \'n02632039\', \'n02632494\', \'n02633422\', \'n02633677\',\n    \'n02633977\', \'n02634545\', \'n02635154\', \'n02635580\', \'n02636170\', \'n02636405\', \'n02636550\', \'n02636854\', \'n02637179\',\n    \'n02637475\', \'n02637977\', \'n02574910\', \'n02575325\', \'n02575590\', \'n02602405\', \'n02602760\', \'n02603317\', \'n02603540\',\n    \'n02618513\', \'n02618827\', \'n02642107\', \'n02553028\', \'n02642644\', \'n02643112\', \'n02643316\', \'n02643836\', \'n02644113\',\n    \'n02644360\', \'n02644501\', \'n02644665\', \'n02644817\', \'n02645538\', \'n02645691\', \'n02645953\', \'n02646667\', \'n02646892\',\n    \'n02648035\', \'n02648625\', \'n02648916\', \'n02649218\', \'n02649546\', \'n02650050\', \'n02650413\', \'n02650541\', \'n02651060\',\n    \'n02652132\', \'n02652668\', \'n02653145\', \'n02653497\', \'n02653786\', \'n02654112\', \'n02654425\', \'n02654745\', \'n02655523\',\n    \'n02655848\', \'n02656032\', \'n02656301\', \'n02656670\', \'n02656969\', \'n02657368\', \'n02663849\', \'n02664285\', \'n02664642\',\n    \'n02665250\', \'n02657694\', \'n02658079\', \'n02658531\', \'n02658811\', \'n02659176\', \'n02659478\', \'n02659808\', \'n02660091\',\n    \'n02660519\', \'n02660640\', \'n02660208\', \'n02661017\', \'n02661473\', \'n02661618\', \'n02662239\', \'n02662397\', \'n02662559\',\n    \'n02662825\', \'n02662993\', \'n02663211\', \'n02663485\', \'n02603862\', \'n02638596\', \'n02639087\', \'n02639605\', \'n02639922\',\n    \'n02640857\', \'n02640626\', \'n02556373\'),\n    # footwear n03380867\n    (\n    \'n03380867\', \'n04199027\', \'n03297103\', \'n04156411\', \'n03547530\', \'n04027706\', \'n03364008\', \'n04386664\', \'n03027625\',\n    \'n04239786\', \'n04122578\', \'n03090710\', \'n02904927\', \'n02713364\', \'n04593524\', \'n02938218\', \'n02855701\', \'n03411079\',\n    \'n04545748\', \'n03868406\', \'n04124370\', \'n02882894\', \'n02767147\', \'n04570118\', \'n03776877\', \'n03364156\', \'n03041449\',\n    \'n03472535\', \'n03967270\', \'n03025250\', \'n04022332\', \'n04272389\', \'n04546081\', \'n03361550\', \'n04241394\', \'n03798061\',\n    \'n02873733\', \'n02872752\', \'n04228581\', \'n04089666\', \'n03600475\', \'n03516844\', \'n03521544\', \'n04542715\', \'n02925666\',\n    \'n04116294\', \'n03865949\', \'n02735538\'),\n    # fruit n13134947\n    (\n    \'n13134947\', \'n07705931\', \'n07738105\', \'n07738224\', \'n07739035\', \'n07739125\', \'n07739344\', \'n07739506\', \'n07739923\',\n    \'n07740033\', \'n07740220\', \'n07740342\', \'n07740461\', \'n07740597\', \'n07740744\', \'n07740855\', \'n07740954\', \'n07741138\',\n    \'n07741235\', \'n07741357\', \'n07741461\', \'n07740115\', \'n07741623\', \'n07741706\', \'n07741804\', \'n07741888\', \'n07742012\',\n    \'n07742224\', \'n07742415\', \'n07742513\', \'n07742605\', \'n07742704\', \'n07743224\', \'n07743384\', \'n07743544\', \'n07743723\',\n    \'n07743902\', \'n07744057\', \'n07744246\', \'n07744430\', \'n07744559\', \'n07744682\', \'n07744811\', \'n07745046\', \'n07745197\',\n    \'n07745357\', \'n07745466\', \'n07745661\', \'n07746038\', \'n07746186\', \'n07746334\', \'n07767171\', \'n07746551\', \'n07746749\',\n    \'n07746910\', \'n07747055\', \'n07747811\', \'n07748753\', \'n07748912\', \'n07749095\', \'n07749192\', \'n07749312\', \'n07747951\',\n    \'n07748157\', \'n07748276\', \'n07748416\', \'n07749446\', \'n07749731\', \'n07749870\', \'n07749969\', \'n07750146\', \'n07750299\',\n    \'n07750449\', \'n07748574\', \'n07750872\', \'n07751004\', \'n07751148\', \'n07751280\', \'n07751451\', \'n07751737\', \'n07751858\',\n    \'n07751977\', \'n07752109\', \'n07752264\', \'n07752377\', \'n07752514\', \'n07752602\', \'n07752664\', \'n07752782\', \'n07752874\',\n    \'n07752966\', \'n07753448\', \'n07753743\', \'n07753980\', \'n07754155\', \'n07754279\', \'n07754451\', \'n07755262\', \'n07755411\',\n    \'n07755619\', \'n07755707\', \'n07755929\', \'n07756096\', \'n07756325\', \'n07756499\', \'n07756838\', \'n07756641\', \'n07756951\',\n    \'n07757132\', \'n07757312\', \'n07757511\', \'n07757602\', \'n07757753\', \'n07757874\', \'n07757990\', \'n07758125\', \'n07758260\',\n    \'n07758407\', \'n07758680\', \'n07759424\', \'n07759576\', \'n07759691\', \'n07758950\', \'n07759194\', \'n07759324\', \'n07759816\',\n    \'n07760070\', \'n07760153\', \'n07760297\', \'n07760395\', \'n07760501\', \'n07760673\', \'n07760755\', \'n07761141\', \'n07761309\',\n    \'n07761611\', \'n07761777\', \'n07761954\', \'n07762114\', \'n07762244\', \'n07762373\', \'n07762534\', \'n07762740\', \'n07762913\',\n    \'n07763107\', \'n07763290\', \'n07763483\', \'n07763629\', \'n07763792\', \'n07763987\', \'n07764155\', \'n07764315\', \'n07764486\',\n    \'n07764630\', \'n07764847\', \'n07765073\', \'n07765208\', \'n07765361\', \'n07765517\', \'n07765612\', \'n07765728\', \'n07765862\',\n    \'n07765999\', \'n07766173\', \'n07766409\', \'n07766530\', \'n07766723\', \'n07766891\', \'n07767002\', \'n07767847\', \'n07768068\',\n    \'n07768139\', \'n07768230\', \'n07768318\', \'n07768590\', \'n07768858\', \'n07769102\', \'n07769306\', \'n07769584\', \'n07769731\',\n    \'n07769886\', \'n07770034\', \'n07770180\', \'n07770439\', \'n11636835\', \'n11700279\', \'n12036067\', \'n12036226\', \'n12158031\',\n    \'n12815838\', \'n12162758\', \'n12193334\', \'n12301445\', \'n12642090\', \'n12644283\', \'n12647787\', \'n12650805\', \'n12658481\',\n    \'n12144313\', \'n13135692\', \'n13135832\', \'n07770571\', \'n07770763\', \'n07770869\', \'n07775197\', \'n07814634\', \'n07929351\',\n    \'n11685091\', \'n11689197\', \'n11689367\', \'n11689483\', \'n11689678\', \'n11689815\', \'n11689957\', \'n15086247\', \'n11946313\',\n    \'n12156819\', \'n11823305\', \'n12123648\', \'n12142357\', \'n12157056\', \'n12157179\', \'n12585373\', \'n12592839\', \'n12593341\',\n    \'n12696830\', \'n12928819\', \'n13136316\', \'n11750173\', \'n11766046\', \'n12487058\', \'n12493426\', \'n12532564\', \'n12576323\',\n    \'n13136556\', \'n07737081\', \'n07737594\', \'n07737745\', \'n07750586\', \'n07750736\', \'n07769465\', \'n07771082\', \'n07771212\',\n    \'n07771539\', \'n07771405\', \'n07771731\', \'n07771891\', \'n07772026\', \'n07772147\', \'n07772274\', \'n07772413\', \'n07772788\',\n    \'n07772935\', \'n07774182\', \'n07774295\', \'n07774596\', \'n07774719\', \'n07774842\', \'n07775050\', \'n11612235\', \'n12197601\',\n    \'n12280364\', \'n12590715\', \'n13136781\', \'n13137409\', \'n07743902\', \'n11723986\', \'n13137951\', \'n13137672\', \'n13137225\',\n    \'n13138308\', \'n07751004\', \'n07767344\', \'n07767709\', \'n07767549\', \'n13138658\', \'n07744811\', \'n13138155\', \'n13138842\',\n    \'n07739125\', \'n07739344\', \'n13139055\', \'n11748002\', \'n12515925\', \'n12544539\', \'n12560282\', \'n12560621\', \'n12561594\',\n    \'n12578916\', \'n11748811\', \'n11766432\', \'n12172364\', \'n13139321\', \'n13139482\', \'n13140367\', \'n13141415\', \'n13150378\',\n    \'n13150592\'),\n    # fungus n12992868\n    (\n    \'n12992868\', \'n13035241\', \'n13035707\', \'n13036312\', \'n13035925\', \'n13036116\', \'n13036804\', \'n12982468\', \'n12982590\',\n    \'n12985420\', \'n13079419\', \'n13079567\', \'n13001930\', \'n13023134\', \'n12987056\', \'n12987535\', \'n12988158\', \'n12988341\',\n    \'n12988572\', \'n12991184\', \'n12989938\', \'n12991837\', \'n12989007\', \'n12987423\', \'n12992177\', \'n12990597\', \'n13027879\',\n    \'n13081999\', \'n12979829\', \'n13077295\', \'n12963628\', \'n12980840\', \'n12981301\', \'n12981443\', \'n12981086\', \'n12969131\',\n    \'n12969670\', \'n12969425\', \'n12969927\', \'n13025647\', \'n13026015\', \'n13025854\', \'n13046669\', \'n13081229\', \'n13078021\',\n    \'n13002209\', \'n13079073\', \'n13015509\', \'n13060190\', \'n13062421\', \'n13061348\', \'n13061704\', \'n13061471\', \'n13061172\',\n    \'n13080306\', \'n12964920\', \'n12970733\', \'n13080866\', \'n13018906\', \'n13048447\', \'n12966945\', \'n13066129\', \'n13067672\',\n    \'n13067532\', \'n13068434\', \'n13067191\', \'n13067330\', \'n13066979\', \'n13068255\', \'n13068917\', \'n13069224\', \'n13068735\',\n    \'n13066448\', \'n12971400\', \'n12971804\', \'n12972136\', \'n13046130\', \'n13045210\', \'n13045975\', \'n12978076\', \'n12965626\',\n    \'n12965951\', \'n13034555\', \'n13045594\', \'n12985773\', \'n13042982\', \'n13041312\', \'n13040629\', \'n13040796\', \'n13082568\',\n    \'n13016076\', \'n13024012\', \'n12983873\', \'n13024500\', \'n13024653\', \'n12983961\', \'n12984489\', \'n12984595\', \'n12984267\',\n    \'n13028611\', \'n13029122\', \'n13031323\', \'n13029326\', \'n13031474\', \'n13029760\', \'n13031193\', \'n13030616\', \'n13029610\',\n    \'n13030852\', \'n13028937\', \'n12982915\', \'n13047862\', \'n13016289\', \'n13077033\', \'n12973791\', \'n12973937\', \'n12980080\',\n    \'n12973443\', \'n12981954\', \'n12995601\', \'n12966804\', \'n13063269\', \'n13065514\', \'n13065089\', \'n13064111\', \'n13064457\',\n    \'n13035389\', \'n13038577\', \'n13043926\', \'n13044375\', \'n13027557\', \'n12968136\', \'n12968309\', \'n13037585\', \'n13037805\',\n    \'n13038376\', \'n13038068\', \'n13038744\', \'n13069773\', \'n13054073\', \'n13039349\', \'n12970193\', \'n12970293\', \'n13042316\',\n    \'n13042134\', \'n13041943\', \'n12986227\', \'n13056799\', \'n13059298\', \'n13056135\', \'n13058272\', \'n13056349\', \'n13057054\',\n    \'n13055577\', \'n13057422\', \'n13058608\', \'n13058037\', \'n13055792\', \'n13060017\', \'n13055423\', \'n13055949\', \'n13057242\',\n    \'n13057639\', \'n13059657\', \'n13056607\', \'n12997654\', \'n13049953\', \'n13052931\', \'n13050940\', \'n13053608\', \'n13050705\',\n    \'n13050397\', \'n13051346\', \'n13052248\', \'n13052014\', \'n13011595\', \'n12997919\', \'n13032115\', \'n13033879\', \'n13032381\',\n    \'n13033577\', \'n13032618\', \'n13034062\', \'n13032923\', \'n13033134\', \'n13033396\', \'n13002750\', \'n13232106\', \'n13003254\',\n    \'n13001366\', \'n13008839\', \'n13004826\', \'n13075020\', \'n13009656\', \'n13009244\', \'n13020481\', \'n13003712\', \'n13003522\',\n    \'n13018407\', \'n13004640\', \'n13022210\', \'n13017240\', \'n13002925\', \'n13074814\', \'n13008157\', \'n13020964\', \'n13017979\',\n    \'n13232363\', \'n13017610\', \'n13076405\', \'n13012469\', \'n13076831\', \'n13017439\', \'n13007629\', \'n13232779\', \'n13020191\',\n    \'n13004423\', \'n13013534\', \'n13014097\', \'n13014581\', \'n13014741\', \'n13014879\', \'n13014409\', \'n13013965\', \'n13014265\',\n    \'n13006631\', \'n13005984\', \'n13010951\', \'n13008485\', \'n13021867\', \'n13000891\', \'n13013764\', \'n13021543\', \'n13017789\',\n    \'n13009429\', \'n13005329\', \'n13006171\', \'n13010694\', \'n13075441\', \'n13001206\', \'n13017102\', \'n13076041\', \'n13018088\',\n    \'n13021332\', \'n13018232\', \'n13070308\', \'n13072350\', \'n13072209\', \'n13070875\', \'n13072863\', \'n13072031\', \'n13073703\',\n    \'n13072528\', \'n13071815\', \'n13072706\', \'n13073055\', \'n13071371\', \'n13071553\', \'n13075684\', \'n13075847\', \'n13004992\',\n    \'n13021689\', \'n13231678\', \'n13001041\', \'n13009085\', \'n13008689\', \'n13001529\', \'n13012253\', \'n13231919\', \'n13019496\',\n    \'n13074619\', \'n13006894\', \'n13019835\', \'n13075272\', \'n13003061\', \'n13012973\', \'n13011221\', \'n13019643\', \'n13076643\',\n    \'n13008315\', \'n13021166\', \'n13007417\', \'n13015688\', \'n12974987\', \'n12975804\', \'n12976198\', \'n12976554\', \'n12983654\',\n    \'n12979316\', \'n13226871\', \'n13034788\', \'n12983048\'),\n    # geological formation n09287968\n    (\n    \'n09287968\', \'n09201998\', \'n09217230\', \'n09393524\', \'n09238926\', \'n09239302\', \'n09257843\', \'n09294877\', \'n09259025\',\n    \'n09398677\', \'n09264803\', \'n09266604\', \'n09283866\', \'n09309292\', \'n09289331\', \'n09194227\', \'n09255070\', \'n09396608\',\n    \'n09391774\', \'n09308572\', \'n09295210\', \'n09308743\', \'n09309046\', \'n09309168\', \'n09331251\', \'n09348460\', \'n09357447\',\n    \'n09362316\', \'n09366017\', \'n09215437\', \'n09245515\', \'n09421031\', \'n09457979\', \'n09330378\', \'n09376526\', \'n09186592\',\n    \'n09415671\', \'n09448690\', \'n09259219\', \'n09249155\', \'n09344324\', \'n09304750\', \'n09230041\', \'n09474765\', \'n09290350\',\n    \'n09214269\', \'n09226869\', \'n09268007\', \'n09402944\', \'n09422190\', \'n09425019\', \'n09454744\', \'n09398076\', \'n09403086\',\n    \'n09344198\', \'n09335809\', \'n09422631\', \'n09435739\', \'n09452291\', \'n09262690\', \'n09289596\', \'n09300306\', \'n09206896\',\n    \'n09269882\', \'n09474010\', \'n09305031\', \'n09375606\', \'n09405787\', \'n09290444\', \'n09295946\', \'n09233446\', \'n09410224\',\n    \'n09366317\', \'n09302616\', \'n09269341\', \'n09453008\', \'n09351905\', \'n09456207\', \'n09303008\', \'n09230202\', \'n09283405\',\n    \'n09326662\', \'n09199101\', \'n09327077\', \'n09357346\', \'n09459979\', \'n09359803\', \'n09218641\', \'n09362945\', \'n09396465\',\n    \'n09409512\', \'n09213434\', \'n09224725\', \'n09421799\', \'n09433312\', \'n09214060\', \'n09270735\', \'n09429630\', \'n09274305\',\n    \'n09337253\', \'n09219233\', \'n09406793\', \'n09210862\', \'n09214916\', \'n09411295\', \'n09452760\', \'n09376786\', \'n09403734\',\n    \'n09409752\', \'n09205509\', \'n09433442\', \'n08596076\', \'n09335693\', \'n09428628\', \'n09458269\', \'n09447666\', \'n09437454\',\n    \'n09206985\', \'n09466678\', \'n09213565\', \'n09475925\', \'n09415584\', \'n09233603\', \'n09248153\', \'n09265620\', \'n09269472\',\n    \'n09445008\', \'n09274152\', \'n09303528\', \'n09228055\', \'n09361517\', \'n09391644\', \'n09436444\', \'n09305898\', \'n09454153\',\n    \'n09470222\', \'n09472413\', \'n09344724\', \'n09231117\', \'n09474412\', \'n09476123\'),\n    # hoofed animal n02370806\n    (\n    \'n02370806\', \'n02373336\', \'n02374149\', \'n02390258\', \'n02391617\', \'n02390101\', \'n02389346\', \'n02389559\', \'n02389779\',\n    \'n02389865\', \'n02390015\', \'n02389943\', \'n02390454\', \'n02390834\', \'n02390938\', \'n02390640\', \'n02390738\', \'n02391234\',\n    \'n02391373\', \'n02391508\', \'n02374451\', \'n02382948\', \'n02385776\', \'n02385580\', \'n02383231\', \'n02385098\', \'n02388143\',\n    \'n02385214\', \'n02385676\', \'n02388276\', \'n02388453\', \'n02375862\', \'n02385898\', \'n02389261\', \'n02382204\', \'n02380335\',\n    \'n02382039\', \'n02380583\', \'n02380745\', \'n02381460\', \'n02381831\', \'n02381609\', \'n02386310\', \'n02386496\', \'n02386853\',\n    \'n02387452\', \'n02387346\', \'n02387887\', \'n02386968\', \'n02387093\', \'n02386746\', \'n02382338\', \'n02387254\', \'n02382132\',\n    \'n02375302\', \'n02388917\', \'n02388588\', \'n02376918\', \'n02377181\', \'n02377291\', \'n02377388\', \'n02387983\', \'n02380464\',\n    \'n02388832\', \'n02386014\', \'n02386224\', \'n02386141\', \'n02382437\', \'n02382850\', \'n02382750\', \'n02382635\', \'n02377480\',\n    \'n02377603\', \'n02389128\', \'n02387722\', \'n02375757\', \'n02377703\', \'n02379430\', \'n02381364\', \'n02378870\', \'n02379908\',\n    \'n02379081\', \'n02379743\', \'n02379630\', \'n02378415\', \'n02378625\', \'n02378755\', \'n02378541\', \'n02378299\', \'n02378969\',\n    \'n02381004\', \'n02380052\', \'n02381119\', \'n02379183\', \'n02381261\', \'n02379329\', \'n02378149\', \'n02388735\', \'n02375438\',\n    \'n02384741\', \'n02393580\', \'n02393807\', \'n02393940\', \'n02391994\', \'n02392824\', \'n02392434\', \'n02393161\', \'n02392555\',\n    \'n02394477\', \'n02438580\', \'n02397529\', \'n02397744\', \'n02397987\', \'n02437136\', \'n02437482\', \'n02399000\', \'n02429456\',\n    \'n02401031\', \'n02418064\', \'n02418465\', \'n02419336\', \'n02419056\', \'n02419634\', \'n02418770\', \'n02411206\', \'n02411705\',\n    \'n02412210\', \'n02413131\', \'n02413917\', \'n02414043\', \'n02414442\', \'n02413593\', \'n02414209\', \'n02414290\', \'n02413717\',\n    \'n02413824\', \'n02413484\', \'n02411999\', \'n02413050\', \'n02419796\', \'n02424085\', \'n02424695\', \'n02425228\', \'n02424909\',\n    \'n02423218\', \'n02423589\', \'n02423362\', \'n02421449\', \'n02425887\', \'n02421136\', \'n02421792\', \'n02427724\', \'n02427576\',\n    \'n02427470\', \'n02428349\', \'n02428508\', \'n02426813\', \'n02427032\', \'n02427183\', \'n02425086\', \'n02424305\', \'n02424589\',\n    \'n02424486\', \'n02420828\', \'n02422391\', \'n02428089\', \'n02426176\', \'n02420509\', \'n02426481\', \'n02425532\', \'n02414578\',\n    \'n02414763\', \'n02416104\', \'n02415130\', \'n02414904\', \'n02415435\', \'n02415829\', \'n02415253\', \'n02416519\', \'n02417534\',\n    \'n02417785\', \'n02417663\', \'n02417070\', \'n02417387\', \'n02417242\', \'n02416820\', \'n02416964\', \'n02416880\', \'n02410702\',\n    \'n02410900\', \'n02407959\', \'n02409202\', \'n02409508\', \'n02409038\', \'n02408817\', \'n02408660\', \'n02402010\', \'n02404573\',\n    \'n02404906\', \'n02402425\', \'n02403231\', \'n02403153\', \'n02403325\', \'n02405692\', \'n02406859\', \'n02403454\', \'n02405577\',\n    \'n02406952\', \'n02406046\', \'n02404186\', \'n02406174\', \'n02402175\', \'n02405101\', \'n02405302\', \'n02405440\', \'n02409870\',\n    \'n02428842\', \'n02430045\', \'n02431976\', \'n02430830\', \'n02435216\', \'n02432511\', \'n02432704\', \'n02435517\', \'n02434712\',\n    \'n02431122\', \'n02431542\', \'n02431337\', \'n02431441\', \'n02432291\', \'n02433318\', \'n02433925\', \'n02434190\', \'n02434415\',\n    \'n02431628\', \'n02430748\', \'n02432983\', \'n02431785\', \'n02434954\', \'n02433546\', \'n02433729\', \'n02435853\', \'n02436224\',\n    \'n02436353\', \'n02436645\', \'n02439033\', \'n02439398\', \'n02437971\', \'n02438272\', \'n02438173\', \'n02395003\', \'n02395931\',\n    \'n02396014\', \'n02396088\', \'n02396796\', \'n02396157\', \'n02372140\'),\n    # insect n02159955\n    (\n    \'n02159955\', \'n02160947\', \'n02161225\', \'n02161338\', \'n02161457\', \'n02161588\', \'n02162561\', \'n02163008\', \'n02163297\',\n    \'n02164464\', \'n02165877\', \'n02166229\', \'n02166567\', \'n02166826\', \'n02167505\', \'n02167820\', \'n02167944\', \'n02168245\',\n    \'n02168427\', \'n02169023\', \'n02169218\', \'n02169705\', \'n02169974\', \'n02170400\', \'n02170599\', \'n02170738\', \'n02170993\',\n    \'n02171164\', \'n02171453\', \'n02171869\', \'n02172518\', \'n02172678\', \'n02172761\', \'n02172870\', \'n02173113\', \'n02173373\',\n    \'n02173784\', \'n02174355\', \'n02174659\', \'n02175014\', \'n02175569\', \'n02175916\', \'n02176261\', \'n02176439\', \'n02176747\',\n    \'n02177196\', \'n02177506\', \'n02177775\', \'n02178411\', \'n02178717\', \'n02181235\', \'n02181724\', \'n02182045\', \'n02182355\',\n    \'n02182642\', \'n02182930\', \'n02179012\', \'n02179192\', \'n02179340\', \'n02180233\', \'n02179891\', \'n02180427\', \'n02180875\',\n    \'n02183096\', \'n02183507\', \'n02183857\', \'n02184473\', \'n02184589\', \'n02184720\', \'n02185167\', \'n02185481\', \'n02186153\',\n    \'n02186717\', \'n02187150\', \'n02187279\', \'n02187554\', \'n02187900\', \'n02188699\', \'n02189363\', \'n02189670\', \'n02190790\',\n    \'n02191273\', \'n02191773\', \'n02191979\', \'n02192252\', \'n02192513\', \'n02192814\', \'n02193009\', \'n02193163\', \'n02194249\',\n    \'n02194750\', \'n02195091\', \'n02195526\', \'n02195819\', \'n02199502\', \'n02196119\', \'n02196344\', \'n02196896\', \'n02197185\',\n    \'n02197689\', \'n02197877\', \'n02198532\', \'n02198859\', \'n02199170\', \'n02200198\', \'n02200630\', \'n02200850\', \'n02201000\',\n    \'n02201497\', \'n02201626\', \'n02202006\', \'n02202124\', \'n02202287\', \'n02202678\', \'n02203152\', \'n02203978\', \'n02204249\',\n    \'n02205673\', \'n02203592\', \'n02204722\', \'n02204907\', \'n02205219\', \'n02198129\', \'n02206270\', \'n02220055\', \'n02220225\',\n    \'n02220518\', \'n02220804\', \'n02221083\', \'n02221414\', \'n02221571\', \'n02221715\', \'n02221820\', \'n02222035\', \'n02222582\',\n    \'n02222321\', \'n02207179\', \'n02208280\', \'n02208498\', \'n02208848\', \'n02208979\', \'n02209111\', \'n02209354\', \'n02209624\',\n    \'n02209964\', \'n02210427\', \'n02210921\', \'n02211444\', \'n02211627\', \'n02211896\', \'n02212062\', \'n02212602\', \'n02212958\',\n    \'n02214096\', \'n02213107\', \'n02213239\', \'n02213663\', \'n02213788\', \'n02213543\', \'n02214341\', \'n02214499\', \'n02214773\',\n    \'n02215161\', \'n02215621\', \'n02215770\', \'n02216211\', \'n02216365\', \'n02216740\', \'n02214660\', \'n02217563\', \'n02218134\',\n    \'n02218371\', \'n02218713\', \'n02219015\', \'n02207449\', \'n02207805\', \'n02207647\', \'n02223266\', \'n02223520\', \'n02225798\',\n    \'n02224023\', \'n02224713\', \'n02225081\', \'n02226183\', \'n02226821\', \'n02226970\', \'n02227247\', \'n02227604\', \'n02227966\',\n    \'n02228341\', \'n02228697\', \'n02229156\', \'n02229765\', \'n02230023\', \'n02230187\', \'n02230480\', \'n02230634\', \'n02231052\',\n    \'n02231803\', \'n02232223\', \'n02233943\', \'n02234355\', \'n02234570\', \'n02234848\', \'n02235205\', \'n02236241\', \'n02236355\',\n    \'n02236896\', \'n02237424\', \'n02237581\', \'n02237868\', \'n02238235\', \'n02238358\', \'n02238594\', \'n02238887\', \'n02239192\',\n    \'n02239528\', \'n02239774\', \'n02240068\', \'n02240517\', \'n02241008\', \'n02241426\', \'n02241569\', \'n02241799\', \'n02242137\',\n    \'n02242455\', \'n02243209\', \'n02243562\', \'n02243878\', \'n02244173\', \'n02244515\', \'n02244797\', \'n02245111\', \'n02245443\',\n    \'n02246011\', \'n02246628\', \'n02246941\', \'n02247216\', \'n02247511\', \'n02247655\', \'n02248062\', \'n02248368\', \'n02248510\',\n    \'n02248887\', \'n02249134\', \'n02249515\', \'n02249809\', \'n02250280\', \'n02250822\', \'n02251067\', \'n02251233\', \'n02251593\',\n    \'n02251775\', \'n02252226\', \'n02252799\', \'n02252972\', \'n02253127\', \'n02253264\', \'n02253494\', \'n02253715\', \'n02253913\',\n    \'n02254246\', \'n02254697\', \'n02254901\', \'n02255023\', \'n02255391\', \'n02256172\', \'n02257003\', \'n02257284\', \'n02257715\',\n    \'n02257985\', \'n02258198\', \'n02258508\', \'n02258629\', \'n02259377\', \'n02259708\', \'n02259987\', \'n02260421\', \'n02260863\',\n    \'n02261063\', \'n02261419\', \'n02261757\', \'n02262178\', \'n02262449\', \'n02262803\', \'n02263378\', \'n02264021\', \'n02264885\',\n    \'n02265330\', \'n02266050\', \'n02266421\', \'n02266864\', \'n02267208\', \'n02267483\', \'n02268148\', \'n02269196\', \'n02269340\',\n    \'n02270011\', \'n02270200\', \'n02270945\', \'n02270623\', \'n02271222\', \'n02271570\', \'n02271897\', \'n02272286\', \'n02272552\',\n    \'n02272871\', \'n02273392\', \'n02274024\', \'n02274259\', \'n02274822\', \'n02275560\', \'n02275773\', \'n02276078\', \'n02276355\',\n    \'n02276749\', \'n02276902\', \'n02277094\', \'n02277268\', \'n02277422\', \'n02278024\', \'n02278210\', \'n02278463\', \'n02278839\',\n    \'n02278980\', \'n02279257\', \'n02279637\', \'n02280458\', \'n02281015\', \'n02281136\', \'n02281267\', \'n02282257\', \'n02282385\',\n    \'n02282553\', \'n02282903\', \'n02283077\', \'n02283201\', \'n02283617\', \'n02283951\', \'n02284224\', \'n02284611\', \'n02284884\',\n    \'n02285179\', \'n02285548\', \'n02286089\', \'n02286425\', \'n02286654\', \'n02287004\', \'n02287352\', \'n02287622\', \'n02288789\',\n    \'n02289307\', \'n02289610\', \'n02289988\', \'n02290340\', \'n02290664\', \'n02290870\', \'n02291220\', \'n02291572\', \'n02291748\',\n    \'n02292085\', \'n02292401\', \'n02292692\', \'n02293352\', \'n02293868\', \'n02294097\', \'n02294407\', \'n02295064\', \'n02295870\',\n    \'n02296021\', \'n02296276\', \'n02296612\', \'n02297294\', \'n02297819\', \'n02298095\', \'n02298541\', \'n02299039\', \'n02299378\',\n    \'n02299846\', \'n02300173\', \'n02300554\', \'n02301452\', \'n02301935\', \'n02302244\', \'n02302459\', \'n02303585\', \'n02305085\',\n    \'n02302969\', \'n02303284\', \'n02304036\', \'n02304432\', \'n02304657\', \'n02304797\', \'n02305407\', \'n02305636\', \'n02305929\',\n    \'n02306433\', \'n02306825\', \'n02307515\', \'n02307910\', \'n02308471\', \'n02308618\', \'n02307176\', \'n02312427\', \'n02312640\',\n    \'n02312912\', \'n02313008\', \'n02207345\'),\n    # musical instrument n03800933\n    (\n    \'n03800933\', \'n02795978\', \'n02803349\', \'n02803934\', \'n02804123\', \'n02804252\', \'n03301568\', \'n03512030\', \'n02940706\',\n    \'n03279153\', \'n03273551\', \'n04376400\', \'n04419642\', \'n03597916\', \'n03614532\', \'n04376400\', \'n02990758\', \'n03038870\',\n    \'n03039015\', \'n03496296\', \'n04278247\', \'n04537436\', \'n03928116\', \'n02766792\', \'n03086457\', \'n03738066\', \'n04278353\',\n    \'n03801353\', \'n03915437\', \'n03928116\', \'n02766792\', \'n03086457\', \'n03738066\', \'n04278353\', \'n02869249\', \'n02965529\',\n    \'n03483230\', \'n03157348\', \'n03518829\', \'n04614844\', \'n02803666\', \'n02869737\', \'n04249415\', \'n04382334\', \'n04387201\',\n    \'n04387400\', \'n04410086\', \'n04436542\', \'n03440682\', \'n03612965\', \'n03633632\', \'n04049753\', \'n04480853\', \'n04532831\',\n    \'n04338517\', \'n03038870\', \'n03039015\', \'n03496296\', \'n04278247\', \'n04537436\', \'n03928116\', \'n02766792\', \'n03086457\',\n    \'n03738066\', \'n04278353\', \'n02880546\', \'n02803934\', \'n04536153\', \'n04536465\', \'n04536595\', \'n04536765\', \'n04536335\',\n    \'n02700895\', \'n03465500\', \'n04330998\', \'n03025886\', \'n02776978\', \'n02682407\', \'n03699280\', \'n03698360\', \'n03716966\',\n    \'n03716887\', \'n03254862\', \'n03467517\', \'n02804123\', \'n03035832\', \'n03499907\', \'n04506289\', \'n03628215\', \'n04016846\',\n    \'n04132603\', \'n04224842\', \'n04615226\', \'n03254737\', \'n04586932\', \'n02891788\', \'n02804252\', \'n03301568\', \'n03512030\',\n    \'n02793089\', \'n02912894\', \'n04174500\', \'n03369276\', \'n04141198\', \'n04123123\', \'n03393324\', \'n02701730\', \'n03086670\',\n    \'n02786736\', \'n03494537\', \'n03609397\', \'n03854815\', \'n03254625\', \'n04067231\', \'n04542595\', \'n04263950\', \'n04542474\',\n    \'n04067143\', \'n03945615\', \'n02775483\', \'n03800371\', \'n03006626\', \'n03245724\', \'n03343354\', \'n03355468\', \'n03945459\',\n    \'n03912218\', \'n03950647\', \'n03989777\', \'n04579667\', \'n04598582\', \'n02817799\', \'n03228016\', \'n03096439\', \'n04410365\',\n    \'n03288742\', \'n03628831\', \'n03510866\', \'n03800485\', \'n03839172\', \'n03839276\', \'n04186624\', \'n03393199\', \'n04222847\',\n    \'n03037709\', \'n02803539\', \'n02803809\', \'n02834027\', \'n03537550\', \'n03334492\', \'n03831757\', \'n03929091\'),\n    # primate n02469914\n    (\n    \'n02469914\', \'n02496913\', \'n02499808\', \'n02499022\', \'n02498153\', \'n02499568\', \'n02499316\', \'n02498743\', \'n02500596\',\n    \'n02470238\', \'n02470709\', \'n02471300\', \'n02478875\', \'n02479332\', \'n02471762\', \'n02473983\', \'n02478239\', \'n02472293\',\n    \'n02472987\', \'n02474777\', \'n02475358\', \'n02475669\', \'n02473307\', \'n02473720\', \'n02473857\', \'n02475078\', \'n02474605\',\n    \'n02474110\', \'n10528148\', \'n02474282\', \'n02476219\', \'n02476870\', \'n02477187\', \'n02477329\', \'n02477516\', \'n02476567\',\n    \'n02477028\', \'n02477782\', \'n02473554\', \'n02501583\', \'n02502006\', \'n02501923\', \'n02496052\', \'n02470325\', \'n02470899\',\n    \'n02483092\', \'n02480153\', \'n02484322\', \'n02484473\', \'n02485988\', \'n02488894\', \'n02485225\', \'n02485688\', \'n02485371\',\n    \'n02485536\', \'n02486657\', \'n02487079\', \'n02486908\', \'n02487847\', \'n02488003\', \'n02487547\', \'n02487675\', \'n02488415\',\n    \'n02489589\', \'n02494383\', \'n02493224\', \'n02492948\', \'n02492356\', \'n02490597\', \'n02490811\', \'n02491107\'),\n    # reptile n01661091\n    (\n    \'n01661091\', \'n01661592\', \'n01662622\', \'n01662784\', \'n01663401\', \'n01663782\', \'n01664369\', \'n01664492\', \'n01664674\',\n    \'n01664990\', \'n01665932\', \'n01666585\', \'n01666228\', \'n01667432\', \'n01668091\', \'n01668436\', \'n01668665\', \'n01668892\',\n    \'n01669372\', \'n01669654\', \'n01670092\', \'n01670535\', \'n01670802\', \'n01671125\', \'n01671479\', \'n01671705\', \'n01672032\',\n    \'n01672432\', \'n01672611\', \'n01661818\', \'n01673282\', \'n01674216\', \'n01674464\', \'n01674990\', \'n01675352\', \'n01676755\',\n    \'n01677747\', \'n01678043\', \'n01678343\', \'n01678657\', \'n01679005\', \'n01679307\', \'n01679626\', \'n01679962\', \'n01680264\',\n    \'n01680478\', \'n01680655\', \'n01680813\', \'n01680983\', \'n01681328\', \'n01681653\', \'n01681940\', \'n01682172\', \'n01682435\',\n    \'n01683201\', \'n01683558\', \'n01684133\', \'n01684578\', \'n01684741\', \'n01685439\', \'n01686044\', \'n01686220\', \'n01686403\',\n    \'n01686609\', \'n01686808\', \'n01687128\', \'n01687290\', \'n01687665\', \'n01688961\', \'n01689081\', \'n01689411\', \'n01690149\',\n    \'n01690466\', \'n01691217\', \'n01691652\', \'n01691951\', \'n01692523\', \'n01692864\', \'n01693175\', \'n01693783\', \'n01694311\',\n    \'n01694709\', \'n01694955\', \'n01701551\', \'n01701859\', \'n01702256\', \'n01702479\', \'n01703011\', \'n01703161\', \'n01703569\',\n    \'n01704103\', \'n01704626\', \'n01705010\', \'n01705591\', \'n01705934\', \'n01707294\', \'n01708106\', \'n01708998\', \'n01709484\',\n    \'n01709876\', \'n01712008\', \'n01712752\', \'n01713170\', \'n01713764\', \'n01714231\', \'n01715888\', \'n01717016\', \'n01717229\',\n    \'n01717467\', \'n01718096\', \'n01718414\', \'n01710177\', \'n01711160\', \'n01722998\', \'n01723579\', \'n01724231\', \'n01724840\',\n    \'n01725086\', \'n01725713\', \'n01726203\', \'n01696633\', \'n01698434\', \'n01698782\', \'n01697178\', \'n01697611\', \'n01697749\',\n    \'n01697978\', \'n01699040\', \'n01699254\', \'n01699675\', \'n01726692\', \'n01727646\', \'n01728266\', \'n01729672\', \'n01730185\',\n    \'n01730307\', \'n01730563\', \'n01730812\', \'n01730960\', \'n01731137\', \'n01731277\', \'n01731545\', \'n01731764\', \'n01731941\',\n    \'n01732093\', \'n01732244\', \'n01732614\', \'n01732789\', \'n01732989\', \'n01733214\', \'n01733466\', \'n01733757\', \'n01733957\',\n    \'n01734104\', \'n01734637\', \'n01734808\', \'n01735439\', \'n01735577\', \'n01735728\', \'n01736032\', \'n01736375\', \'n01736796\',\n    \'n01737472\', \'n01737728\', \'n01737875\', \'n01738065\', \'n01738306\', \'n01738601\', \'n01738731\', \'n01739094\', \'n01739647\',\n    \'n01739871\', \'n01741232\', \'n01741442\', \'n01740551\', \'n01740885\', \'n01741562\', \'n01741943\', \'n01742447\', \'n01742821\',\n    \'n01743086\', \'n01743605\', \'n01743936\', \'n01744100\', \'n01744270\', \'n01744555\', \'n01745125\', \'n01745484\', \'n01745902\',\n    \'n01746191\', \'n01746359\', \'n01746952\', \'n01747285\', \'n01747589\', \'n01747885\', \'n01748389\', \'n01748686\', \'n01748906\',\n    \'n01749244\', \'n01749582\', \'n01749742\', \'n01750167\', \'n01750437\', \'n01750743\', \'n01751036\', \'n01751215\', \'n01751472\',\n    \'n01752165\', \'n01752585\', \'n01752736\', \'n01753032\', \'n01753180\', \'n01753959\', \'n01754370\', \'n01754533\', \'n01754876\',\n    \'n01755740\', \'n01755952\', \'n01756089\', \'n01756508\', \'n01756733\', \'n01756916\', \'n01757115\', \'n01757343\', \'n01757677\',\n    \'n01757901\', \'n01758141\', \'n01662060\', \'n01719403\', \'n01721174\', \'n01721898\', \'n01722670\'),\n    # utensil n04516672\n    (\n    \'n04516672\', \'n02997607\', \'n03262519\', \'n03173270\', \'n03317788\', \'n03713436\', \'n04414101\', \'n04414319\', \'n03984234\',\n    \'n03018209\', \'n02869155\', \'n03125588\', \'n04282992\', \'n03992703\', \'n02684248\', \'n03698226\', \'n04570214\', \'n04326676\',\n    \'n03104512\', \'n03403643\', \'n03621049\', \'n03012499\', \'n03101517\', \'n03101986\', \'n02805283\', \'n02999138\', \'n03101156\',\n    \'n03983712\', \'n03101796\', \'n03284981\', \'n03047799\', \'n03453231\', \'n03458422\', \'n03459328\', \'n03880531\', \'n03242390\',\n    \'n03271765\', \'n04275283\', \'n03846677\', \'n03900301\', \'n04097760\', \'n04138977\', \'n03226254\', \'n04317325\', \'n03972372\',\n    \'n03990474\', \'n03242506\', \'n03915118\', \'n03216562\', \'n03259401\', \'n03612814\', \'n04397768\', \'n03992975\', \'n04139140\',\n    \'n04324297\', \'n04516214\', \'n03064250\', \'n04132985\', \'n04399158\', \'n04229959\', \'n04309548\', \'n04500060\', \'n03352961\',\n    \'n03881305\', \'n03454885\', \'n03621377\', \'n03724417\', \'n03767966\', \'n03775199\', \'n02850732\', \'n03266371\', \'n03272940\',\n    \'n04578934\', \'n04088441\', \'n04103206\', \'n04293119\', \'n04059516\', \'n04396902\', \'n04175039\'),\n    # vegetable n07707451\n    (\n    \'n07707451\', \'n07708124\', \'n07708398\', \'n07708798\', \'n07709046\', \'n07724943\', \'n07725158\', \'n07726796\', \'n07727048\',\n    \'n07727140\', \'n07727252\', \'n07727377\', \'n07727458\', \'n07727578\', \'n07727868\', \'n07728053\', \'n07728181\', \'n07728284\',\n    \'n07728391\', \'n07728585\', \'n07728708\', \'n07728804\', \'n07729000\', \'n07729142\', \'n07729225\', \'n07729384\', \'n07729828\',\n    \'n07727741\', \'n07729485\', \'n07729926\', \'n07725255\', \'n07725376\', \'n07725531\', \'n07725789\', \'n07725888\', \'n07726009\',\n    \'n07725663\', \'n07726230\', \'n07726386\', \'n07726095\', \'n07726672\', \'n07709172\', \'n07709333\', \'n07709701\', \'n07719437\',\n    \'n07719616\', \'n07719756\', \'n07719980\', \'n07720277\', \'n07723330\', \'n07723559\', \'n07723753\', \'n07723968\', \'n07724078\',\n    \'n07724173\', \'n07724269\', \'n07724492\', \'n07724654\', \'n07724819\', \'n07730855\', \'n07731006\', \'n07731587\', \'n07731767\',\n    \'n07732747\', \'n07732904\', \'n07733005\', \'n07733124\', \'n07820036\', \'n07733217\', \'n07733712\', \'n07733847\', \'n07736256\',\n    \'n07736371\', \'n07736527\', \'n07736692\', \'n07710007\', \'n07710616\', \'n07710952\', \'n07711371\', \'n07711080\', \'n07711232\',\n    \'n07711799\', \'n07713074\', \'n07720442\', \'n07720615\', \'n07721018\', \'n07721118\', \'n07721195\', \'n07721325\', \'n07722052\',\n    \'n07721456\', \'n07721678\', \'n07721833\', \'n07721942\', \'n07734017\', \'n07734183\', \'n07734292\', \'n07734417\', \'n07734555\',\n    \'n07710283\', \'n07710616\', \'n07710952\', \'n07711371\', \'n07711907\', \'n07712063\', \'n07712267\', \'n07719058\', \'n07719839\',\n    \'n07720084\', \'n07720185\', \'n07730207\', \'n07730708\', \'n07735052\', \'n07735179\', \'n07735294\', \'n07735404\', \'n07735687\',\n    \'n07735803\', \'n07735981\', \'n07736087\', \'n07736813\', \'n07713267\', \'n07713395\', \'n07735687\', \'n07713763\', \'n07713895\',\n    \'n07714078\', \'n07714188\', \'n07714287\', \'n07714448\', \'n07714895\', \'n07714802\', \'n07715221\', \'n07715407\', \'n07733567\',\n    \'n07715561\', \'n07717070\', \'n07717714\', \'n07717858\', \'n07718068\', \'n07718195\', \'n07718329\', \'n07715721\', \'n07716034\',\n    \'n07716203\', \'n07716504\', \'n07716649\', \'n07716750\', \'n07718671\', \'n07718920\', \'n07719213\', \'n07719330\', \'n07722217\',\n    \'n07722390\', \'n07722485\', \'n07722666\', \'n07722763\', \'n07722888\', \'n07723177\', \'n07723039\', \'n07730406\', \'n07730562\',\n    \'n07733394\', \'n07735510\', \'n07736971\', \'n07768423\', \'n07817871\'),\n    # vehicle n04576211\n    (\n    \'n04576211\', \'n02766534\', \'n02804515\', \'n02834778\', \'n03853924\', \'n04026813\', \'n04126066\', \'n04524716\', \'n02869563\',\n    \'n02959942\', \'n02775039\', \'n02932523\', \'n03053976\', \'n02885108\', \'n04322692\', \'n02986066\', \'n03056097\', \'n03360731\',\n    \'n04070964\', \'n04389521\', \'n03465320\', \'n03483971\', \'n03710294\', \'n03200357\', \'n03828020\', \'n04020912\', \'n04236001\',\n    \'n04246855\', \'n04409279\', \'n03484083\', \'n02729222\', \'n03490119\', \'n03648431\', \'n04176068\', \'n04397027\', \'n03897634\',\n    \'n03538634\', \'n02968473\', \'n02794474\', \'n02907296\', \'n02909706\', \'n02912557\', \'n02931013\', \'n02966068\', \'n03002555\',\n    \'n03009111\', \'n03037590\', \'n03055670\', \'n04297098\', \'n03247351\', \'n03435991\', \'n03436656\', \'n03389889\', \'n03492087\',\n    \'n03638014\', \'n03989199\', \'n04302863\', \'n04365112\', \'n04486616\', \'n03009269\', \'n03669245\', \'n04353573\', \'n04103364\',\n    \'n04149374\', \'n04170037\', \'n03919096\', \'n04062807\', \'n04566561\', \'n02740533\', \'n03886053\', \'n02739889\', \'n02740061\',\n    \'n02740300\', \'n02749292\', \'n04389718\', \'n03684823\', \'n03025165\', \'n03193597\', \'n03193260\', \'n03193423\', \'n03585778\',\n    \'n03939565\', \'n04211219\', \'n04373428\', \'n04389854\', \'n04465358\', \'n03791235\', \'n04368695\', \'n02854630\', \'n02958343\',\n    \'n03404012\', \'n04201733\', \'n03472937\', \'n03079136\', \'n03119396\', \'n03141065\', \'n03881534\', \'n03268790\', \'n03421669\',\n    \'n03493219\', \'n03498781\', \'n03539103\', \'n03543394\', \'n02831335\', \'n03680512\', \'n03770085\', \'n03870105\', \'n04322801\',\n    \'n03342961\', \'n04097373\', \'n04166281\', \'n04285965\', \'n04302988\', \'n04347119\', \'n04459122\', \'n04516354\', \'n03389761\',\n    \'n03506880\', \'n03790512\', \'n03769722\', \'n04466871\', \'n04490091\', \'n03256166\', \'n03632852\', \'n03690473\', \'n04465666\',\n    \'n04474035\', \'n04520170\', \'n02871314\', \'n03173929\', \'n03648667\', \'n03764822\', \'n03884639\', \'n03896419\', \'n02946348\',\n    \'n04520382\', \'n03256788\', \'n03538300\', \'n04464852\', \'n03886053\', \'n02983507\', \'n04250599\', \'n04229007\', \'n02916179\',\n    \'n02712643\', \'n04225987\', \'n04467099\', \'n02946509\', \'n03904433\', \'n04543158\', \'n02787120\', \'n02970849\', \'n03217739\',\n    \'n03255899\', \'n04497249\', \'n03235979\', \'n03594010\', \'n03981924\', \'n04558059\', \'n04560502\', \'n03027505\', \'n03122295\',\n    \'n03765467\', \'n04543924\', \'n04563020\', \'n04543509\', \'n04571800\')\n)\n\nnum_classes = 25\n\n# mean and standard deviation of channels of ImageNet images\nmean = [0.485, 0.456, 0.406]\nstd = [0.229, 0.224, 0.225]\n\ntest_transform = trn.Compose([trn.Resize(256), trn.CenterCrop(224), trn.ToTensor(), trn.Normalize(mean, std)])\n\ntest_data_in = dset.ImageFolder(\n    root=""/share/data/vision-greg/ImageNet/clsloc/images/val"",\n    transform=test_transform)\n\n\nprint(\'Loading ImageNet-22K\')\n# some ImageNet-22K images are broken JPEGs\n\n\ndef my_collate(batch):\n    batch = list(filter(lambda x: x is not None, batch))\n    return torch.utils.data.dataloader.default_collate(batch)\n\n\nclass MyImageFolder(dset.ImageFolder):\n    __init__ = dset.ImageFolder.__init__\n\n    def __getitem__(self, index):\n        try:\n            return super(MyImageFolder, self).__getitem__(index)\n        except OSError:\n            pass\n\n\ntest_data_out = MyImageFolder(root=""/share/data/vision-greg/ImageNet/clsloc/images/val"",\n                              transform=test_transform)\ntest_data_out.root = \'/share/data/vision-greg/ImageNet22k\'\ntest_data_out.class_to_idx = pickle.load(open(test_data_out.root + \'/class_to_idx.p\', ""rb""))\ntest_data_out.classes = pickle.load(open(test_data_out.root + \'/classes.p\', ""rb""))\n# test_data_out.imgs = pickle.load(open(test_data_out.root + \'/imgs.p\', ""rb""))\nprint(\'Loaded ImageNet-22K\')\n\n\nprint(\'Filtering Data\')\nim1k_classes_flat = tuple(np.concatenate(im1k_classes))\ntest_data_in.imgs = [x for x in test_data_in.imgs if\n                     x[0][x[0].index(\'val/\') + 4:x[0].index(\'val/\') + 4 + len(\'n07718472\')] in im1k_classes_flat]\n\nin_idx_to_superidx = {}\nfor c in im1k_classes_flat:\n    for i in range(len(im1k_classes)):      # range(num_classes)\n        t = im1k_classes[i]\n        if c in t:\n            in_idx_to_superidx[test_data_in.class_to_idx[c]] = i\n            break\ntest_data_in.target_transform = lambda x: in_idx_to_superidx[x]\n\nim22k_classes_flat = tuple(np.concatenate(im22k_classes))\n# test_data_out.imgs = [x for x in test_data_out.imgs if\n#                       x[0][x[0].index(\'22k/images/\') + 11:x[0].index(\'22k/images/\') + 11 + len(\'n07718472\')]\n#                       in im22k_classes_flat]\ntest_data_out.imgs = pickle.load(open(\'./imgs.p\', ""rb""))\n\nout_idx_to_superidx = {}\nfor c in im22k_classes_flat:\n    for i in range(len(im22k_classes)):     # range(num_classes)\n        t = im22k_classes[i]\n        if c in t:\n            out_idx_to_superidx[test_data_out.class_to_idx[c]] = i\n            break\ntest_data_out.target_transform = lambda x: out_idx_to_superidx[x]\n\ntest_loader_in = torch.utils.data.DataLoader(\n    test_data_in, batch_size=args.test_bs, shuffle=False,\n    num_workers=args.prefetch, pin_memory=True)\ntest_loader_out = torch.utils.data.DataLoader(\n    test_data_out, batch_size=args.test_bs, shuffle=False,\n    num_workers=args.prefetch, pin_memory=True, collate_fn=my_collate)\n\n\nprint(\'Initializing Network\')\n\n\nclass FineTuneModel(nn.Module):\n    """"""\n    This freezes the weights of all layers except the last one.\n\n    Arguments:\n        original_model: Model to finetune\n        arch: Name of model architecture\n        num_classes: Number of classes to tune for\n    """"""\n\n    def __init__(self, original_model, arch, num_classes):\n        super(FineTuneModel, self).__init__()\n\n        if arch.startswith(\'alexnet\') or arch.startswith(\'vgg\'):\n            self.features = original_model.features\n            self.fc = nn.Sequential(*list(original_model.classifier.children())[:-1])\n            self.classifier = nn.Sequential(\n                nn.Linear(4096, num_classes)\n            )\n        elif arch.startswith(\'resnet\') or arch.startswith(\'resnext\'):\n            # Everything except the last linear layer\n            self.features = nn.Sequential(*list(original_model.children())[:-1])\n            if arch == \'resnet18\':\n                self.classifier = nn.Sequential(\n                    nn.Linear(512, num_classes)\n                )\n            else:\n                self.classifier = nn.Sequential(\n                    nn.Linear(2048, num_classes)\n                )\n        else:\n            raise (""Finetuning not supported on this architecture yet. Feel free to add"")\n\n        self.unfreeze(False)  # Freeze weights except last layer\n\n    def unfreeze(self, unfreeze):\n        # Freeze those weights\n        for p in self.features.parameters():\n            p.requires_grad = unfreeze\n        if hasattr(self, \'fc\'):\n            for p in self.fc.parameters():\n                p.requires_grad = unfreeze\n\n    def forward(self, x):\n        f = self.features(x)\n        if hasattr(self, \'fc\'):\n            f = f.view(f.size(0), -1)\n            f = self.fc(f)\n        f = f.view(f.size(0), -1)\n        y = self.classifier(f)\n        return y\n\n\nif args.model_name == \'alexnet\':\n    net = models.AlexNet()\n    net.load_state_dict(model_zoo.load_url(\'https://download.pytorch.org/models/alexnet-owt-4df8aa71.pth\',\n                                           model_dir=\'/share/data/lang/users/dan/.torch/models\'))\n    net = FineTuneModel(net, \'alexnet\', num_classes)\n\nelif args.model_name == \'squeezenet1.1\':\n    net = models.SqueezeNet(version=1.1)\n    net.load_state_dict(model_zoo.load_url(\'https://download.pytorch.org/models/squeezenet1_1-f364aa15.pth\',\n                                           model_dir=\'/share/data/lang/users/dan/.torch/models\'))\n    net.classifier = nn.Sequential(\n        nn.Dropout(p=0.5),\n        nn.Conv2d(512, num_classes, kernel_size=1),\n        nn.ReLU(inplace=True),\n        nn.AvgPool2d(13)\n    )\n    net.forward = lambda x: net.classifier(net.features(x)).view(x.size(0), num_classes)\n\n    for p in net.features.parameters():\n        p.requires_grad = False\n\nelif \'vgg\' in args.model_name:\n    if \'bn\' not in args.model_name:\n        net = models.vgg19()\n        net.load_state_dict(model_zoo.load_url(\'https://download.pytorch.org/models/vgg19-dcbb9e9d.pth\',\n                                               model_dir=\'/share/data/lang/users/dan/.torch/models\'))\n    elif \'11\' in args.model_name:\n        net = models.vgg11()\n        net.load_state_dict(model_zoo.load_url(\'https://download.pytorch.org/models/vgg11-bbd30ac9.pth\',\n                                               model_dir=\'/share/data/lang/users/dan/.torch/models\'))\n    else:\n        net = models.vgg19_bn()\n        net.load_state_dict(model_zoo.load_url(\'https://download.pytorch.org/models/vgg19_bn-c79401a0.pth\',\n                                               model_dir=\'/share/data/lang/users/dan/.torch/models\'))\n    net = FineTuneModel(net, \'vgg\', num_classes)\n\nelif args.model_name == \'resnet18\':\n    net = models.resnet18()\n    net.load_state_dict(model_zoo.load_url(\'https://download.pytorch.org/models/resnet18-5c106cde.pth\',\n                                           model_dir=\'/share/data/lang/users/dan/.torch/models\'))\n    net = FineTuneModel(net, \'resnet18\', num_classes)\n\nelif args.model_name == \'resnet50\':\n    net = models.resnet50()\n    net.load_state_dict(model_zoo.load_url(\'https://download.pytorch.org/models/resnet50-19c8e357.pth\',\n                                           model_dir=\'/share/data/lang/users/dan/.torch/models\'))\n    net = FineTuneModel(net, \'resnet50\', num_classes)\n\nelif args.model_name == \'resnext\':\n    net = resnext_101_64x4d\n    net.load_state_dict(torch.load(\'/share/data/lang/users/dan/.torch/models/resnext_101_64x4d.pth\'))\n    net = FineTuneModel(net, \'resnext\', num_classes)\n\nstart_epoch = 0\n# Restore model\nif args.load != \'\':\n    for i in range(1000 - 1, -1, -1):\n        model_name = os.path.join(args.load, args.model_name + \'_superclass_epoch\' + str(i) + \'.pth\')\n        if os.path.isfile(model_name):\n            snapshot = torch.load(model_name)\n            net.load_state_dict(snapshot[\'state_dict\'])\n            print(\'Model restored! Epoch:\', i)\n            start_epoch = i + 1\n            break\n    if start_epoch == 0:\n        assert False, ""could not resume""\n\n\nif args.ngpu > 1:\n    net = torch.nn.DataParallel(net, device_ids=list(range(args.ngpu)))\n\nif args.ngpu > 0:\n    net.cuda()\n    torch.cuda.manual_seed(1)\n\nfor p in net.parameters():\n    p.volatile = True\n\ncudnn.benchmark = True  # fire on all cylinders\n\n\n# test function\ndef test_in():\n    net.eval()\n    loss_avg = 0.0\n    correct = 0\n    for batch_idx, (data, target) in enumerate(test_loader_in):\n        data, target = V(data.cuda(), volatile=True), V(target.cuda(), volatile=True)\n\n        # forward\n        output = net(data)\n        loss = F.cross_entropy(output, target)\n\n        # accuracy\n        pred = output.data.max(1)[1]\n        correct += pred.eq(target.data).sum()\n\n        # test loss average\n        loss_avg += loss.data[0]\n\n    state[\'test_in_loss\'] = loss_avg / len(test_loader_in)\n    state[\'test_in_accuracy\'] = correct / len(test_loader_in.dataset)\n\n\n# test function\ndef test_out():\n    print(\'Testing unseen classes\')\n    net.eval()\n    correct = 0\n    for batch_idx, (data, target) in enumerate(test_loader_out):\n        data, target = V(data.cuda(), volatile=True), V(target.cuda(), volatile=True)\n\n        # forward\n        output = net(data)\n\n        # accuracy\n        pred = output.data.max(1)[1]\n        correct += pred.eq(target.data).sum()\n\n    state[\'test_out_accuracy\'] = correct / len(test_loader_out.dataset)\n\n\ntest_in()\ntest_out()\nprint(state)\n'"
old/auxiliary/ImageNet22K/train.py,23,"b'# -*- coding: utf-8 -*-\nimport numpy as np\nimport sys\nimport os\nimport pickle\nimport argparse\nimport math\nimport time\nimport torch\nimport torch.nn as nn\nimport torch.backends.cudnn as cudnn\nimport torchvision.transforms as trn\nimport torchvision.datasets as dset\nimport torch.nn.functional as F\nfrom torch.autograd import Variable as V\nimport torchvision.models as models\nimport torch.utils.model_zoo as model_zoo\nfrom resnext_101_64x4d import resnext_101_64x4d\n\nparser = argparse.ArgumentParser(description=\'Trains an ImageNet Superclass Classifier\',\n                                 formatter_class=argparse.ArgumentDefaultsHelpFormatter)\nparser.add_argument(\'--model-name\', \'-m\', type=str,\n                    choices=[\'alexnet\', \'squeezenet1.1\', \'vgg11\', \'vgg\', \'vggbn\',\n                             \'resnet18\', \'resnet50\', \'resnext\'])\n# Optimization options\nparser.add_argument(\'--epochs\', \'-e\', type=int, default=1, help=\'Number of epochs to train.\')\nparser.add_argument(\'--learning_rate\', \'-lr\', type=float, default=0.01, help=\'The initial learning rate.\')\nparser.add_argument(\'--batch_size\', \'-b\', type=int, default=128, help=\'Batch size.\')\nparser.add_argument(\'--test_bs\', type=int, default=200)\nparser.add_argument(\'--momentum\', type=float, default=0.9, help=\'Momentum.\')\nparser.add_argument(\'--decay\', \'-d\', type=float, default=0.0005, help=\'Weight decay (L2 penalty).\')\n# Checkpoints\nparser.add_argument(\'--save\', \'-s\', type=str, default=\'./snapshots\', help=\'Folder to save checkpoints.\')\nparser.add_argument(\'--load\', \'-l\', type=str, default=\'\', help=\'Checkpoint path to resume / test.\')\nparser.add_argument(\'--test\', \'-t\', action=\'store_true\', help=\'Test only flag.\')\n# Acceleration\nparser.add_argument(\'--ngpu\', type=int, default=1, help=\'0 = CPU.\')\nparser.add_argument(\'--prefetch\', type=int, default=4, help=\'Pre-fetching threads.\')\nargs = parser.parse_args()\n\ntorch.manual_seed(1)\nnp.random.seed(1)\n\nstate = {k: v for k, v in args._get_kwargs()}\nstate[\'tt\'] = 0     # SGDR variable\nstate[\'init_learning_rate\'] = args.learning_rate\n\n\nim1k_classes = (\n    # amphibian n01627424\n    (\'n01641577\', \'n01644373\', \'n01644900\', \'n01629819\', \'n01630670\', \'n01631663\', \'n01632458\', \'n01632777\'),\n    # appliance n02729837\n    (\'n03483316\', \'n04179913\', \'n03584829\', \'n03297495\', \'n03761084\', \'n03259280\', \'n04111531\', \'n04442312\', \'n04542943\', \'n04517823\', \'n03207941\', \'n04070727\', \'n04554684\'),\n    # aquatic mammal n02062017\n    (\'n02074367\', \'n02077923\', \'n02071294\', \'n02066245\'),\n    # bird n01503061\n    (\'n01514668\', \'n01514859\', \'n01518878\', \'n01530575\', \'n01531178\', \'n01532829\', \'n01534433\', \'n01537544\', \'n01558993\', \'n01560419\', \'n01580077\', \'n01582220\', \'n01592084\', \'n01601694\', \'n01608432\', \'n01614925\', \'n01616318\', \'n01622779\', \'n01795545\', \'n01796340\', \'n01797886\', \'n01798484\', \'n01806143\', \'n01806567\', \'n01807496\', \'n01817953\', \'n01818515\', \'n01819313\', \'n01820546\', \'n01824575\', \'n01828970\', \'n01829413\', \'n01833805\', \'n01843065\', \'n01843383\', \'n01855672\', \'n01847000\', \'n01860187\', \'n02002556\', \'n02002724\', \'n02006656\', \'n02007558\', \'n02009912\', \'n02009229\', \'n02011460\', \'n02012849\', \'n02013706\', \'n02018207\', \'n02018795\', \'n02025239\', \'n02027492\', \'n02028035\', \'n02033041\', \'n02037110\', \'n02017213\', \'n02051845\', \'n02056570\', \'n02058221\'),\n    # bear n02131653\n    (\'n02132136\', \'n02134084\', \'n02133161\', \'n02134418\'),\n    # beverage n07881800\n    (\'n07920052\', \'n07892512\', \'n07932039\', \'n07930864\'),\n    # big cat n02127808\n    (\'n02128925\', \'n02129604\', \'n02128385\', \'n02128757\', \'n02129165\', \'n02130308\'),\n    # building n02913152\n    (\'n02793495\', \'n03457902\', \'n03877845\', \'n03781244\', \'n03661043\', \'n02727426\', \'n02859443\', \'n03028079\', \'n03788195\', \'n04346328\', \'n03956157\', \'n04081281\', \'n03032252\', \'n03529860\'),\n    # cat n02121620\n    (\'n02124075\', \'n02123394\', \'n02123159\', \'n02123597\', \'n02123045\', \'n02125311\', \'n02127052\'),\n    # clothing n03051540\n    (\'n03724870\', \'n04584207\', \'n03623198\', \'n02730930\', \'n04162706\', \'n02669723\', \'n04532106\', \'n02916936\', \'n04371430\', \'n03710721\', \'n02837789\', \'n04350905\', \'n03594734\', \'n03325584\', \'n04325704\', \'n02883205\', \'n04591157\', \'n02865351\', \'n03534580\', \'n03770439\', \'n03866082\', \'n04136333\', \'n03980874\', \'n03404251\', \'n04479046\', \'n03630383\', \'n04370456\', \'n02963159\', \'n03617480\', \'n02667093\', \'n03595614\', \'n02892767\', \'n03188531\', \'n03775071\', \'n03127747\', \'n03379051\', \'n02807133\', \'n03787032\', \'n04209133\', \'n02869837\', \'n02817516\', \'n03124170\', \'n04259630\', \'n03710637\', \'n04254777\', \'n03026506\', \'n03763968\', \'n03877472\', \'n03594734\', \'n03450230\', \'n02892767\'),\n    # dog n02084071\n    (\'n02100583\', \'n02100236\', \'n02100735\', \'n02101006\', \'n02100877\', \'n02102973\', \'n02102177\', \'n02102040\', \'n02101556\', \'n02101388\', \'n02102318\', \'n02102480\', \'n02099601\', \'n02099849\', \'n02099429\', \'n02099267\', \'n02099712\', \'n02096294\', \'n02095314\', \'n02098105\', \'n02095889\', \'n02095570\', \'n02096437\', \'n02096051\', \'n02098413\', \'n02094433\', \'n02098286\', \'n02097130\', \'n02097047\', \'n02097209\', \'n02093256\', \'n02093428\', \'n02094114\', \'n02096177\', \'n02093859\', \'n02097298\', \'n02096585\', \'n02093647\', \'n02093991\', \'n02097658\', \'n02094258\', \'n02097474\', \'n02093754\', \'n02090622\', \'n02090721\', \'n02092002\', \'n02089078\', \'n02089867\', \'n02089973\', \'n02092339\', \'n02091635\', \'n02088466\', \'n02091467\', \'n02091831\', \'n02088094\', \'n02091134\', \'n02091032\', \'n02088364\', \'n02088238\', \'n02088632\', \'n02090379\', \'n02091244\', \'n02087394\', \'n02110341\', \'n02113186\', \'n02113023\', \'n02113978\', \'n02111277\', \'n02113712\', \'n02113624\', \'n02113799\', \'n02110806\', \'n02111129\', \'n02112706\', \'n02110958\', \'n02109047\', \'n02105641\', \'n02106382\', \'n02106550\', \'n02105505\', \'n02106030\', \'n02106166\', \'n02105162\', \'n02105056\', \'n02105855\', \'n02105412\', \'n02105251\', \'n02106662\', \'n02104365\', \'n02107142\', \'n02110627\', \'n02107312\', \'n02104029\', \'n02110185\', \'n02110063\', \'n02108089\', \'n02108422\', \'n02109961\', \'n02108000\', \'n02107683\', \'n02107574\', \'n02107908\', \'n02109525\', \'n02108551\', \'n02108915\', \'n02112018\', \'n02112350\', \'n02112137\', \'n02111889\', \'n02111500\', \'n02086910\', \'n02086646\', \'n02086079\', \'n02085936\', \'n02087046\', \'n02085782\', \'n02086240\', \'n02085620\'),\n    # electronic equipment n03278248\n    (\'n03584254\', \'n03777754\', \'n03782006\', \'n03857828\', \'n04004767\', \'n03085013\', \'n03602883\', \'n02979186\', \'n02992529\', \'n03902125\', \'n03187595\', \'n04392985\', \'n02988304\'),\n    # fish n02512053\n    (\'n01496331\', \'n01498041\', \'n01484850\', \'n01491361\', \'n01494475\', \'n02514041\', \'n02536864\', \'n01440764\', \'n01443537\', \'n02526121\', \'n02536864\', \'n02606052\', \'n02607072\', \'n02643566\', \'n02655020\', \'n02640242\', \'n02641379\'),\n    # footwear n03380867\n    (\'n04133789\', \'n04120489\', \'n03680355\', \'n03124043\', \'n03047690\'),\n    # fruit n13134947\n    (\'n07742313\', \'n07745940\', \'n07747607\', \'n07749582\', \'n07753113\', \'n07753275\', \'n07753592\', \'n07754684\', \'n07760859\', \'n07768694\', \'n12267677\', \'n12620546\', \'n13133613\', \'n11879895\', \'n12144580\', \'n12768682\', \'n07742313\'),\n    # fungus n12992868\n    (\'n13052670\', \'n13044778\', \'n12985857\', \'n13040303\', \'n13037406\', \'n13054560\', \'n12998815\'),\n    # geological formation n09287968\n    (\'n09246464\', \'n09468604\', \'n09193705\', \'n09472597\', \'n09399592\', \'n09421951\', \'n09256479\', \'n09332890\', \'n09428293\', \'n09288635\'),\n    # hoofed animal n02370806\n    (\'n02391049\', \'n02389026\', \'n02437312\', \'n02412080\', \'n02423022\', \'n02422699\', \'n02422106\', \'n02415577\', \'n02417914\', \'n02410509\', \'n02408429\', \'n02403003\', \'n02398521\', \'n02437616\', \'n02396427\', \'n02397096\', \'n02395406\'),\n    # insect n02159955\n    (\'n02165105\', \'n02165456\', \'n02167151\', \'n02168699\', \'n02169497\', \'n02172182\', \'n02174001\', \'n02177972\', \'n02190166\', \'n02219486\', \'n02206856\', \'n02226429\', \'n02229544\', \'n02231487\', \'n02233338\', \'n02236044\', \'n02256656\', \'n02259212\', \'n02264363\', \'n02268443\', \'n02268853\', \'n02276258\', \'n02277742\', \'n02279972\', \'n02280649\', \'n02281406\', \'n02281787\'),\n    # musical instrument n03800933\n    (\'n02672831\', \'n03854065\', \'n03452741\', \'n04515003\', \'n03452741\', \'n04515003\', \'n03017168\', \'n03249569\', \'n03447721\', \'n03720891\', \'n03721384\', \'n04311174\', \'n03452741\', \'n04515003\', \'n02787622\', \'n02992211\', \'n04536866\', \'n03495258\', \'n02676566\', \'n03272010\', \'n03854065\', \'n03110669\', \'n03394916\', \'n04487394\', \'n02672831\', \'n03494278\', \'n03840681\', \'n03884397\', \'n02804610\', \'n03838899\', \'n04141076\', \'n03372029\'),\n    # primate n02469914\n    (\'n02500267\', \'n02497673\', \'n02483708\', \'n02483362\', \'n02480495\', \'n02481823\', \'n02480855\', \'n02488702\', \'n02484975\', \'n02489166\', \'n02486261\', \'n02486410\', \'n02487347\', \'n02488291\', \'n02493509\', \'n02494079\', \'n02493793\', \'n02492035\', \'n02492660\', \'n02490219\'),\n    # reptile n01661091\n    (\'n01664065\', \'n01665541\', \'n01667114\', \'n01667778\', \'n01669191\', \'n01675722\', \'n01677366\', \'n01682714\', \'n01685808\', \'n01687978\', \'n01688243\', \'n01689811\', \'n01692333\', \'n01693334\', \'n01694178\', \'n01695060\', \'n01704323\', \'n01698640\', \'n01697457\', \'n01728572\', \'n01728920\', \'n01729322\', \'n01729977\', \'n01734418\', \'n01735189\', \'n01737021\', \'n01739381\', \'n01740131\', \'n01742172\', \'n01744401\', \'n01748264\', \'n01749939\', \'n01751748\', \'n01753488\', \'n01755581\', \'n01756291\'),\n    # utensil n04516672\n    (\'n03133878\', \'n03400231\', \'n04596742\', \'n02939185\', \'n03063689\', \'n04398044\', \'n04270147\'),\n    # vegetable n07707451\n    (\'n07711569\', \'n07720875\', \'n07711569\', \'n07714571\', \'n07714990\', \'n07715103\', \'n07717410\', \'n07717556\', \'n07716358\', \'n07716906\', \'n07718472\', \'n07718747\', \'n07730033\', \'n07734744\'),\n    # vehicle n04576211\n    (\'n02835271\', \'n03792782\', \'n03393912\', \'n03895866\', \'n02797295\', \'n04204347\', \'n03791053\', \'n04389033\', \'n03384352\', \'n03272562\', \'n04310018\', \'n02704792\', \'n02701002\', \'n02814533\', \'n02930766\', \'n03100240\', \'n03594945\', \'n03670208\', \'n03770679\', \'n03777568\', \'n04037443\', \'n04285008\', \'n03444034\', \'n03445924\', \'n03785016\', \'n04252225\', \'n03345487\', \'n03417042\', \'n03930630\', \'n04461696\', \'n04467665\', \'n03796401\', \'n03770679\', \'n03977966\', \'n04065272\', \'n04335435\', \'n03478589\', \'n04389033\', \'n04252077\', \'n04465501\', \'n03776460\', \'n04482393\', \'n04509417\', \'n03538406\', \'n03599486\', \'n03868242\')\n)\n\n# this excludes ImageNet-1K classes\n\n\nim22k_classes = (\n    # amphibian n01627424\n    (\n    \'n01627424\', \'n01639765\', \'n01640846\', \'n01641206\', \'n01641391\', \'n01641739\', \'n01641930\', \'n01642097\', \'n01642257\',\n    \'n01642391\', \'n01642539\', \'n01642943\', \'n01643255\', \'n01643507\', \'n01643896\', \'n01645466\', \'n01645776\', \'n01646292\',\n    \'n01646388\', \'n01646555\', \'n01646648\', \'n01646802\', \'n01646902\', \'n01647033\', \'n01647180\', \'n01647303\', \'n01647466\',\n    \'n01647640\', \'n01648139\', \'n01648356\', \'n01648620\', \'n01649170\', \'n01649412\', \'n01649556\', \'n01649726\', \'n01650167\',\n    \'n01650690\', \'n01650901\', \'n01651059\', \'n01651285\', \'n01651487\', \'n01651641\', \'n01651778\', \'n01652026\', \'n01652297\',\n    \'n01653026\', \'n01653223\', \'n01653509\', \'n01653773\', \'n01654083\', \'n01654637\', \'n01654863\', \'n01628331\', \'n01628770\',\n    \'n01629276\', \'n01629962\', \'n01630148\', \'n01630284\', \'n01630901\', \'n01631175\', \'n01631354\', \'n01631512\', \'n01632047\',\n    \'n01632308\', \'n01632601\', \'n01632952\', \'n01633406\', \'n01633781\', \'n01634227\', \'n01634522\', \'n01635027\', \'n01635176\',\n    \'n01635480\', \'n01636127\', \'n01636352\', \'n01636510\', \'n01636829\', \'n01637112\', \'n01637338\', \'n01637615\', \'n01637932\',\n    \'n01638194\', \'n01638329\', \'n01638722\', \'n01639187\', \'n01655344\'),\n    # appliance n02729837\n    (\n    \'n02729837\', \'n03251766\', \'n03050655\', \'n03717285\', \'n04277826\', \'n04496726\', \'n04607242\', \'n03528263\', \'n04174101\',\n    \'n03150511\', \'n04309833\', \'n04475631\', \'n03620052\', \'n03063338\', \'n04219185\', \'n03212114\', \'n03378174\', \'n03543254\',\n    \'n03557692\', \'n03862676\', \'n02905036\', \'n03425241\', \'n04388473\', \'n04330340\', \'n03102371\', \'n03273740\', \'n03425595\',\n    \'n03991202\', \'n04003241\', \'n04280487\', \'n04442441\', \'n04488857\', \'n03534776\', \'n04580493\', \'n03050655\', \'n03717285\',\n    \'n04277826\', \'n04496726\', \'n04607242\', \'n03273740\', \'n03102654\', \'n03273913\', \'n03557590\', \'n03170635\'),\n    # aquatic mammal n02062017\n    (\n    \'n02062017\', \'n02073250\', \'n02073831\', \'n02074726\', \'n02075927\', \'n02076196\', \'n02076779\', \'n02078574\', \'n02078292\',\n    \'n02078738\', \'n02079005\', \'n02077658\', \'n02077787\', \'n02077152\', \'n02077384\', \'n02076402\', \'n02079389\', \'n02081060\',\n    \'n02079851\', \'n02080146\', \'n02080415\', \'n02080713\', \'n02081571\', \'n02081798\', \'n02081927\', \'n02062430\', \'n02062744\',\n    \'n02066707\', \'n02068974\', \'n02072040\', \'n02069701\', \'n02069974\', \'n02070174\', \'n02072798\', \'n02069412\', \'n02071028\',\n    \'n02070430\', \'n02070624\', \'n02070776\', \'n02071636\', \'n02067240\', \'n02068206\', \'n02068541\', \'n02067768\', \'n02067603\',\n    \'n02072493\', \'n02063224\', \'n02063662\', \'n02064000\', \'n02064816\', \'n02064338\', \'n02065026\', \'n02065407\', \'n02065263\',\n    \'n02065726\'),\n    # bird n01503061\n    (\n    \'n01503061\', \'n01503976\', \'n01514752\', \'n01514926\', \'n01515078\', \'n01515217\', \'n01515303\', \'n01516212\', \'n01517389\',\n    \'n01517565\', \'n01519563\', \'n01519873\', \'n01520576\', \'n01521399\', \'n01521756\', \'n01522450\', \'n01523105\', \'n01517966\',\n    \'n01524359\', \'n01525720\', \'n01526521\', \'n01526766\', \'n01527194\', \'n01527347\', \'n01527617\', \'n01527917\', \'n01528396\',\n    \'n01528654\', \'n01528845\', \'n01529672\', \'n01530439\', \'n01531344\', \'n01531512\', \'n01531639\', \'n01531811\', \'n01531971\',\n    \'n01532325\', \'n01532511\', \'n01533000\', \'n01533339\', \'n01533481\', \'n01533651\', \'n01533893\', \'n01534155\', \'n01534582\',\n    \'n01534762\', \'n01535140\', \'n01535469\', \'n01535690\', \'n01536035\', \'n01536186\', \'n01536334\', \'n01536644\', \'n01536780\',\n    \'n01537134\', \'n01537895\', \'n01538059\', \'n01538200\', \'n01538362\', \'n01538630\', \'n01540233\', \'n01540566\', \'n01540832\',\n    \'n01541102\', \'n01541386\', \'n01541760\', \'n01541922\', \'n01542433\', \'n01542168\', \'n01544704\', \'n01538955\', \'n01539272\',\n    \'n01542786\', \'n01543175\', \'n01543383\', \'n01543632\', \'n01543936\', \'n01544208\', \'n01544389\', \'n01555809\', \'n01556182\',\n    \'n01556514\', \'n01557185\', \'n01557962\', \'n01558149\', \'n01558307\', \'n01558461\', \'n01558594\', \'n01558765\', \'n01559160\',\n    \'n01559477\', \'n01559639\', \'n01559804\', \'n01560105\', \'n01560280\', \'n01560636\', \'n01560793\', \'n01560935\', \'n01561181\',\n    \'n01561452\', \'n01561732\', \'n01562014\', \'n01562265\', \'n01562451\', \'n01563128\', \'n01563449\', \'n01563746\', \'n01563945\',\n    \'n01564101\', \'n01564217\', \'n01564394\', \'n01564773\', \'n01565345\', \'n01565599\', \'n01565930\', \'n01566207\', \'n01564914\',\n    \'n01565078\', \'n01567133\', \'n01567678\', \'n01567879\', \'n01568132\', \'n01568294\', \'n01568720\', \'n01568892\', \'n01569060\',\n    \'n01569262\', \'n01569423\', \'n01569566\', \'n01569836\', \'n01569971\', \'n01570267\', \'n01570421\', \'n01570676\', \'n01570839\',\n    \'n01566645\', \'n01571410\', \'n01571904\', \'n01572328\', \'n01572489\', \'n01572654\', \'n01572782\', \'n01573074\', \'n01573240\',\n    \'n01573360\', \'n01573627\', \'n01573898\', \'n01574045\', \'n01574390\', \'n01574560\', \'n01574801\', \'n01575117\', \'n01575401\',\n    \'n01575745\', \'n01576076\', \'n01576358\', \'n01576695\', \'n01577035\', \'n01577458\', \'n01577659\', \'n01577941\', \'n01578180\',\n    \'n01578575\', \'n01579028\', \'n01579149\', \'n01579260\', \'n01579410\', \'n01579578\', \'n01579729\', \'n01580379\', \'n01580490\',\n    \'n01580772\', \'n01580870\', \'n01581166\', \'n01581434\', \'n01581730\', \'n01581874\', \'n01581984\', \'n01582398\', \'n01582498\',\n    \'n01582856\', \'n01583209\', \'n01583495\', \'n01583828\', \'n01586941\', \'n01587278\', \'n01587526\', \'n01587834\', \'n01588002\',\n    \'n01588431\', \'n01588725\', \'n01588996\', \'n01589286\', \'n01589718\', \'n01589893\', \'n01590220\', \'n01591005\', \'n01591123\',\n    \'n01591301\', \'n01591697\', \'n01592257\', \'n01592540\', \'n01592387\', \'n01592694\', \'n01593028\', \'n01593282\', \'n01593553\',\n    \'n01594004\', \'n01594372\', \'n01594787\', \'n01594968\', \'n01595168\', \'n01595450\', \'n01595624\', \'n01595974\', \'n01596273\',\n    \'n01596608\', \'n01597022\', \'n01597336\', \'n01597737\', \'n01597906\', \'n01598074\', \'n01598271\', \'n01598588\', \'n01598988\',\n    \'n01599159\', \'n01599269\', \'n01599388\', \'n01599556\', \'n01599741\', \'n01600085\', \'n01600341\', \'n01600657\', \'n01601410\',\n    \'n01601068\', \'n01602080\', \'n01602209\', \'n01602630\', \'n01602832\', \'n01603000\', \'n01603152\', \'n01603600\', \'n01603812\',\n    \'n01603953\', \'n01539573\', \'n01539925\', \'n01540090\', \'n01545574\', \'n01546039\', \'n01546506\', \'n01546921\', \'n01547832\',\n    \'n01548301\', \'n01548492\', \'n01548694\', \'n01548865\', \'n01549053\', \'n01549430\', \'n01549641\', \'n01549886\', \'n01550172\',\n    \'n01550761\', \'n01551080\', \'n01551300\', \'n01552034\', \'n01552333\', \'n01555305\', \'n01551711\', \'n01552813\', \'n01553142\',\n    \'n01553527\', \'n01553762\', \'n01554017\', \'n01554448\', \'n01555004\', \'n01584225\', \'n01584695\', \'n01584853\', \'n01585121\',\n    \'n01585287\', \'n01585422\', \'n01585715\', \'n01586020\', \'n01586374\', \'n01524761\', \'n01604330\', \'n01604968\', \'n01605630\',\n    \'n01606097\', \'n01606177\', \'n01606522\', \'n01606672\', \'n01606809\', \'n01606978\', \'n01607309\', \'n01607429\', \'n01607600\',\n    \'n01607812\', \'n01607962\', \'n01608265\', \'n01608814\', \'n01609062\', \'n01609391\', \'n01609751\', \'n01609956\', \'n01610100\',\n    \'n01610226\', \'n01610552\', \'n01610955\', \'n01611472\', \'n01611674\', \'n01611800\', \'n01611969\', \'n01612122\', \'n01612275\',\n    \'n01612476\', \'n01612628\', \'n01612955\', \'n01613177\', \'n01616086\', \'n01613294\', \'n01613807\', \'n01614038\', \'n01614343\',\n    \'n01614556\', \'n01615121\', \'n01615303\', \'n01615458\', \'n01615703\', \'n01616551\', \'n01616764\', \'n01617095\', \'n01617443\',\n    \'n01617766\', \'n01618082\', \'n01618922\', \'n01619310\', \'n01619536\', \'n01619835\', \'n01620135\', \'n01620414\', \'n01620735\',\n    \'n01618503\', \'n01621127\', \'n01621635\', \'n01622120\', \'n01622352\', \'n01622483\', \'n01622959\', \'n01623110\', \'n01623425\',\n    \'n01623615\', \'n01623706\', \'n01624115\', \'n01624212\', \'n01623880\', \'n01624305\', \'n01624537\', \'n01624833\', \'n01625121\',\n    \'n01625562\', \'n01789386\', \'n01789740\', \'n01790171\', \'n01790304\', \'n01790398\', \'n01790557\', \'n01790711\', \'n01790812\',\n    \'n01791625\', \'n01792042\', \'n01792158\', \'n01792429\', \'n01792530\', \'n01792640\', \'n01792808\', \'n01792955\', \'n01793085\',\n    \'n01793159\', \'n01793249\', \'n01793340\', \'n01793435\', \'n01793565\', \'n01793715\', \'n01791954\', \'n01794158\', \'n01794344\',\n    \'n01809106\', \'n01809371\', \'n01791107\', \'n01791314\', \'n01791388\', \'n01791463\', \'n01794651\', \'n01799302\', \'n01800195\',\n    \'n01799679\', \'n01800424\', \'n01800633\', \'n01801088\', \'n01801479\', \'n01801672\', \'n01801876\', \'n01802159\', \'n01809752\',\n    \'n01810700\', \'n01811243\', \'n01811909\', \'n01812187\', \'n01812337\', \'n01813385\', \'n01813532\', \'n01813658\', \'n01813948\',\n    \'n01814217\', \'n01812662\', \'n01812866\', \'n01813088\', \'n01814370\', \'n01814620\', \'n01814755\', \'n01814921\', \'n01815036\',\n    \'n01814549\', \'n01815270\', \'n01815601\', \'n01816017\', \'n01816140\', \'n01816474\', \'n02153203\', \'n01795088\', \'n01795735\',\n    \'n01795900\', \'n01796019\', \'n01796105\', \'n01796519\', \'n01796729\', \'n01797020\', \'n01797307\', \'n01797601\', \'n01798168\',\n    \'n01798706\', \'n01798839\', \'n01798979\', \'n01802721\', \'n01803078\', \'n01803362\', \'n01803641\', \'n01803893\', \'n01804163\',\n    \'n01805321\', \'n01805801\', \'n01806061\', \'n01806297\', \'n01806364\', \'n01806467\', \'n01807105\', \'n01804478\', \'n01804653\',\n    \'n01804921\', \'n01805070\', \'n01806847\', \'n01807828\', \'n01808140\', \'n01808291\', \'n01808596\', \'n01810268\', \'n01816887\',\n    \'n01817263\', \'n01817346\', \'n01818299\', \'n01818832\', \'n01819115\', \'n01819465\', \'n01819734\', \'n01820052\', \'n01820348\',\n    \'n01820801\', \'n01821076\', \'n01821203\', \'n01821554\', \'n01821869\', \'n01822300\', \'n01822602\', \'n01823013\', \'n01823414\',\n    \'n01823740\', \'n01824035\', \'n01824344\', \'n01824749\', \'n01825278\', \'n01825930\', \'n01826364\', \'n01826680\', \'n01826844\',\n    \'n01827403\', \'n01827793\', \'n01828096\', \'n01828556\', \'n01829869\', \'n01830042\', \'n01830479\', \'n01830915\', \'n01831360\',\n    \'n01831712\', \'n01832167\', \'n01832493\', \'n01832813\', \'n01833112\', \'n01833415\', \'n01834177\', \'n01834540\', \'n01835276\',\n    \'n01835769\', \'n01835918\', \'n01836087\', \'n01836673\', \'n01837072\', \'n01837526\', \'n01838038\', \'n01838598\', \'n01839086\',\n    \'n01839330\', \'n01839598\', \'n01839750\', \'n01839949\', \'n01840120\', \'n01840412\', \'n01840775\', \'n01841102\', \'n01841288\',\n    \'n01841441\', \'n01841679\', \'n01841943\', \'n01842235\', \'n01842504\', \'n01842788\', \'n01843719\', \'n01844231\', \'n01844551\',\n    \'n01844746\', \'n01844917\', \'n01845132\', \'n01860497\', \'n01860864\', \'n01861148\', \'n01861330\', \'n01845477\', \'n01856072\',\n    \'n01856155\', \'n01856380\', \'n01856553\', \'n01856890\', \'n01857079\', \'n01857325\', \'n01857512\', \'n01857632\', \'n01857851\',\n    \'n01846331\', \'n01847089\', \'n01847170\', \'n01847253\', \'n01847407\', \'n01847806\', \'n01847978\', \'n01848123\', \'n01848323\',\n    \'n01848453\', \'n01848555\', \'n01848648\', \'n01848840\', \'n01848976\', \'n01849157\', \'n01849466\', \'n01849676\', \'n01849863\',\n    \'n01850192\', \'n01850373\', \'n01850553\', \'n01850873\', \'n01851038\', \'n01851207\', \'n01851375\', \'n01851731\', \'n01851573\',\n    \'n01851895\', \'n01852142\', \'n01852329\', \'n01852400\', \'n01852671\', \'n01852861\', \'n01853195\', \'n01853498\', \'n01853870\',\n    \'n01854415\', \'n01858441\', \'n01858281\', \'n01858780\', \'n01858845\', \'n01858906\', \'n01859190\', \'n01859325\', \'n01859496\',\n    \'n01859689\', \'n01859852\', \'n01860002\', \'n02000954\', \'n02002075\', \'n02003037\', \'n02003204\', \'n02003577\', \'n02003839\',\n    \'n02004131\', \'n02004492\', \'n02004855\', \'n02005399\', \'n02005790\', \'n02006063\', \'n02006364\', \'n02007284\', \'n02006985\',\n    \'n02008041\', \'n02008497\', \'n02008643\', \'n02008796\', \'n02009380\', \'n02009508\', \'n02009750\', \'n02010272\', \'n02010453\',\n    \'n02010728\', \'n02011016\', \'n02011281\', \'n02011805\', \'n02011943\', \'n02012185\', \'n02013177\', \'n02013567\', \'n02014237\',\n    \'n02014524\', \'n02014941\', \'n02015357\', \'n02015554\', \'n02015797\', \'n02016066\', \'n02017725\', \'n02018027\', \'n02018368\',\n    \'n02019190\', \'n02019438\', \'n02019929\', \'n02020219\', \'n02020578\', \'n02021050\', \'n02021281\', \'n02022684\', \'n02023341\',\n    \'n02023855\', \'n02023992\', \'n02024185\', \'n02024479\', \'n02024763\', \'n02025043\', \'n02025389\', \'n02026059\', \'n02026948\',\n    \'n02027075\', \'n02027357\', \'n02027897\', \'n02028175\', \'n02028342\', \'n02028451\', \'n02028727\', \'n02028900\', \'n02029087\',\n    \'n02029378\', \'n02029706\', \'n02030035\', \'n02030224\', \'n02030287\', \'n02030837\', \'n02030568\', \'n02026629\', \'n02030996\',\n    \'n02031298\', \'n02031585\', \'n02031934\', \'n02032222\', \'n02032355\', \'n02032480\', \'n02032769\', \'n02033324\', \'n02033208\',\n    \'n02033561\', \'n02033779\', \'n02033882\', \'n02034129\', \'n02034295\', \'n02034661\', \'n02034971\', \'n02035210\', \'n02035402\',\n    \'n02035656\', \'n02036053\', \'n02036228\', \'n02036711\', \'n02037464\', \'n02037869\', \'n02038141\', \'n02038466\', \'n02038993\',\n    \'n02039171\', \'n02039780\', \'n02039497\', \'n02040266\', \'n02016358\', \'n02016659\', \'n02016816\', \'n02016956\', \'n02017475\',\n    \'n02021795\', \'n02040505\', \'n02041085\', \'n02043063\', \'n02043333\', \'n02041246\', \'n02041678\', \'n02041875\', \'n02042046\',\n    \'n02042180\', \'n02042472\', \'n02042759\', \'n02043808\', \'n02044178\', \'n02044778\', \'n02044908\', \'n02044517\', \'n02045369\',\n    \'n02045596\', \'n02045864\', \'n02046171\', \'n02046759\', \'n02046939\', \'n02047045\', \'n02047260\', \'n02047411\', \'n02047517\',\n    \'n02047614\', \'n02047975\', \'n02048115\', \'n02048353\', \'n02048698\', \'n02049088\', \'n02049532\', \'n02050004\', \'n02050313\',\n    \'n02050442\', \'n02050586\', \'n02050809\', \'n02051059\', \'n02051474\', \'n02052365\', \'n02052204\', \'n02052775\', \'n02053083\',\n    \'n02053425\', \'n02053584\', \'n02054036\', \'n02054502\', \'n02054711\', \'n02055107\', \'n02055658\', \'n02055803\', \'n02056228\',\n    \'n02056728\', \'n02057035\', \'n02057330\', \'n02057731\', \'n02057898\', \'n02058594\', \'n02058747\', \'n02059162\', \'n02059541\',\n    \'n02059852\', \'n02060133\', \'n02060411\', \'n02060569\', \'n02060889\', \'n02061217\', \'n02061560\', \'n02061853\', \'n02511730\'),\n    # bear n02131653\n    (\'n02131653\', \'n02133704\', \'n02132788\', \'n02132466\', \'n02132580\', \'n02132320\', \'n02133400\', \'n01322983\'),\n    # beverage n07881800\n    (\n    \'n07881800\', \'n07925966\', \'n07926346\', \'n07926250\', \'n07926442\', \'n07933274\', \'n07934373\', \'n07933652\', \'n07933799\',\n    \'n07933891\', \'n07934032\', \'n07934152\', \'n07934282\', \'n07913180\', \'n07929519\', \'n07920349\', \'n07731122\', \'n07731284\',\n    \'n07731436\', \'n07921239\', \'n07919665\', \'n07919572\', \'n07919441\', \'n07920872\', \'n07920222\', \'n07919787\', \'n07919894\',\n    \'n07920540\', \'n07920663\', \'n07929940\', \'n07926785\', \'n07922764\', \'n07890970\', \'n07891309\', \'n07883251\', \'n07883661\',\n    \'n07883384\', \'n07883510\', \'n07882420\', \'n07924033\', \'n07925116\', \'n07924366\', \'n07924443\', \'n07924747\', \'n07924560\',\n    \'n07924655\', \'n07924276\', \'n07924834\', \'n07924955\', \'n07914271\', \'n07891189\', \'n07921455\', \'n07921615\', \'n07921834\',\n    \'n07921948\', \'n07922041\', \'n07914128\', \'n07936263\', \'n07936093\', \'n07935737\', \'n07936745\', \'n14941787\', \'n07936979\',\n    \'n07937069\', \'n07936459\', \'n07936548\', \'n07914006\', \'n07927197\', \'n07928887\', \'n07927512\', \'n07927836\', \'n07928367\',\n    \'n07927931\', \'n07928696\', \'n07928790\', \'n07928163\', \'n07928264\', \'n07927716\', \'n07929172\', \'n07928578\', \'n07928998\',\n    \'n07928488\', \'n07884567\', \'n07886176\', \'n07885705\', \'n07922607\', \'n07905618\', \'n07886572\', \'n07890617\', \'n07886849\',\n    \'n07889510\', \'n07888465\', \'n07888816\', \'n07890068\', \'n07889814\', \'n07890352\', \'n07890540\', \'n07889990\', \'n07890226\',\n    \'n07887099\', \'n07887192\', \'n07887634\', \'n07887967\', \'n07889274\', \'n07888058\', \'n07887461\', \'n07888229\', \'n07887304\',\n    \'n07890750\', \'n07890890\', \'n07932323\', \'n07932454\', \'n07921615\', \'n07922147\', \'n07922512\', \'n07886463\', \'n07901587\',\n    \'n07902336\', \'n07904934\', \'n07905474\', \'n07903101\', \'n07903208\', \'n07904293\', \'n07903962\', \'n07904072\', \'n07902443\',\n    \'n07903731\', \'n07903841\', \'n07903643\', \'n07903543\', \'n07906284\', \'n07906718\', \'n07906572\', \'n07907429\', \'n07907831\',\n    \'n07907161\', \'n07907342\', \'n07907548\', \'n07909593\', \'n07906877\', \'n07902520\', \'n07902937\', \'n07904395\', \'n07904760\',\n    \'n07902698\', \'n07904637\', \'n07902799\', \'n07907037\', \'n07906111\', \'n07905038\', \'n07905296\', \'n07904865\', \'n07905386\',\n    \'n07905770\', \'n07905979\', \'n07907943\', \'n07910656\', \'n07909714\', \'n07908812\', \'n07908647\', \'n07908567\', \'n07908411\',\n    \'n07910245\', \'n07910379\', \'n07909504\', \'n07911249\', \'n07909811\', \'n07909954\', \'n07910048\', \'n07910152\', \'n07909593\',\n    \'n07908923\', \'n07909129\', \'n07910970\', \'n07910538\', \'n07911061\', \'n07909231\', \'n07909362\', \'n07910799\', \'n07925808\',\n    \'n07902121\', \'n07891726\', \'n07900225\', \'n07892813\', \'n07894551\', \'n07896893\', \'n07898443\', \'n07897975\', \'n07899769\',\n    \'n07898247\', \'n07899533\', \'n07895100\', \'n07897200\', \'n07897438\', \'n07897600\', \'n07894703\', \'n07899660\', \'n07895962\',\n    \'n07894799\', \'n07899899\', \'n07896765\', \'n07894451\', \'n07900406\', \'n07901355\', \'n07900825\', \'n07900616\', \'n07900734\',\n    \'n07901457\', \'n07900958\', \'n07899976\', \'n07893528\', \'n07893642\', \'n07893792\', \'n07896060\', \'n07896560\', \'n07897750\',\n    \'n07897865\', \'n07895710\', \'n07895839\', \'n07895435\', \'n07898117\', \'n07898333\', \'n07894965\', \'n07894102\', \'n07895595\',\n    \'n07894298\', \'n07896165\', \'n07897116\', \'n07896422\', \'n07892418\', \'n07893891\', \'n07894551\', \'n07894703\', \'n07894102\',\n    \'n07899108\', \'n07899434\', \'n07899292\', \'n07926920\', \'n07927070\', \'n07913300\', \'n07898745\', \'n07899003\', \'n07895237\',\n    \'n07895435\', \'n07898117\', \'n07894298\', \'n07893253\', \'n07896994\', \'n07893425\', \'n07896287\', \'n07898617\', \'n07896661\',\n    \'n07898895\', \'n07886057\', \'n07911371\', \'n07918879\', \'n07930554\', \'n07931280\', \'n07930205\', \'n07931001\', \'n07931096\',\n    \'n07931733\', \'n07930062\', \'n07931870\', \'n07912211\', \'n07913882\', \'n07917618\', \'n07917951\', \'n07917874\', \'n07917791\',\n    \'n07915491\', \'n07915094\', \'n07913774\', \'n07917507\', \'n07919165\', \'n07912093\', \'n07911677\', \'n07916041\', \'n07916319\',\n    \'n07917392\', \'n07915918\', \'n07915366\', \'n07931612\', \'n07913393\', \'n07913537\', \'n07916437\', \'n07914413\', \'n07914586\',\n    \'n07914686\', \'n07916582\', \'n07918028\', \'n07918193\', \'n07913644\', \'n07917133\', \'n07915618\', \'n07915800\', \'n07917272\',\n    \'n07931452\', \'n07916183\', \'n07915213\', \'n07918309\', \'n07914995\', \'n07930315\', \'n07914887\', \'n07918706\', \'n07930433\',\n    \'n07914777\', \'n07932614\', \'n07932762\', \'n07891433\', \'n07886317\', \'n07891095\', \'n07844042\', \'n07844786\', \'n07846143\',\n    \'n07846274\', \'n07845775\', \'n07921360\', \'n07846014\', \'n07845421\', \'n07847047\', \'n07846359\', \'n07845335\', \'n07846471\',\n    \'n07846557\', \'n07845702\', \'n07846688\', \'n07845495\', \'n07846802\', \'n07846938\', \'n07845863\', \'n07845166\', \'n07845571\',\n    \'n07919310\', \'n07933530\'),\n    # big cat n02127808\n    (\n    \'n02127808\', \'n02129991\', \'n02130545\', \'n02130925\', \'n02129923\', \'n02129837\', \'n01323068\', \'n02128598\', \'n02128669\',\n    \'n02129463\', \'n02129530\', \'n01322898\', \'n02130086\'),\n    # building n02913152\n    (\n    \'n02913152\', \'n02666943\', \'n02726681\', \'n04409384\', \'n02734725\', \'n02763604\', \'n02806992\', \'n02882190\', \'n02993546\',\n    \'n02940385\', \'n03078506\', \'n03089753\', \'n03097362\', \'n04077889\', \'n04175574\', \'n04177931\', \'n04343511\', \'n03007444\',\n    \'n03224893\', \'n03479397\', \'n03322570\', \'n03123809\', \'n04441662\', \'n03016389\', \'n04294879\', \'n03326371\', \'n03413428\',\n    \'n02977936\', \'n03430418\', \'n03449564\', \'n02956699\', \'n03005033\', \'n03121431\', \'n03152303\', \'n03203806\', \'n03093427\',\n    \'n03282295\', \'n04305210\', \'n04461437\', \'n03092166\', \'n13252672\', \'n03478756\', \'n03036022\', \'n03466839\', \'n03437184\',\n    \'n03698723\', \'n03479121\', \'n03479266\', \'n03542333\', \'n03541696\', \'n02961035\', \'n03561573\', \'n03989898\', \'n04097085\',\n    \'n03790755\', \'n03788498\', \'n04080705\', \'n04095109\', \'n04229737\', \'n08640531\', \'n08560295\', \'n08652376\', \'n03542605\',\n    \'n03544360\', \'n02814338\', \'n02857477\', \'n02820085\', \'n02919792\', \'n02932400\', \'n03686924\', \'n03002816\', \'n03007297\',\n    \'n03118969\', \'n03010915\', \'n03158186\', \'n04202142\', \'n04354026\', \'n04535252\', \'n04535370\', \'n03180865\', \'n03219483\',\n    \'n03257210\', \'n03322836\', \'n03428090\', \'n03685640\', \'n03465605\', \'n03474352\', \'n03685486\', \'n03685820\', \'n03367321\',\n    \'n03713151\', \'n03719053\', \'n03718458\', \'n03878066\', \'n04305323\', \'n04052658\', \'n04079244\', \'n03121040\', \'n03166514\',\n    \'n03718935\', \'n02695627\', \'n03892557\', \'n03439348\', \'n04073948\', \'n03099454\', \'n02667478\', \'n02667379\', \'n03396580\',\n    \'n03635032\', \'n04005197\', \'n04115256\', \'n02907873\', \'n04413969\', \'n04125541\', \'n04131368\', \'n04255899\', \'n04258438\',\n    \'n04465050\', \'n04535524\', \'n03545150\', \'n02806875\', \'n04350235\', \'n03121298\', \'n03333610\', \'n03736269\', \'n03837698\',\n    \'n04022708\', \'n04022866\', \'n04246731\', \'n03739518\', \'n03043274\', \'n03210552\', \'n03540595\', \'n03129471\', \'n03730334\',\n    \'n03762982\', \'n03333349\', \'n03770316\', \'n03785499\', \'n03130233\', \'n03402941\', \'n03839671\', \'n03842012\', \'n03859280\',\n    \'n03055857\', \'n03416489\', \'n02968074\', \'n03610524\', \'n03860404\', \'n04187547\', \'n03056288\', \'n04452757\', \'n04598318\',\n    \'n03872016\', \'n03953416\', \'n02833040\', \'n03007130\', \'n03006788\', \'n03633341\', \'n04214413\', \'n02667576\', \'n02801184\',\n    \'n02984061\', \'n03772077\', \'n02984203\', \'n03618982\', \'n03099622\', \'n03724756\', \'n04210390\', \'n04374735\', \'n04407435\',\n    \'n03602365\', \'n03884778\', \'n03999160\', \'n02844214\', \'n02892499\', \'n02897389\', \'n02935658\', \'n02936281\', \'n03155178\',\n    \'n03297644\', \'n03298089\', \'n02935891\', \'n02760099\', \'n02952485\', \'n02952674\', \'n03199647\', \'n03456548\', \'n03459914\',\n    \'n03497100\', \'n03697552\', \'n04111414\', \'n04307878\', \'n04398497\', \'n04081699\', \'n04093625\', \'n03558176\', \'n03557360\',\n    \'n04104500\', \'n02801047\', \'n04112654\', \'n04118635\', \'n04146050\', \'n03092314\', \'n03801671\', \'n02746978\', \'n03165616\',\n    \'n04217546\', \'n04233124\', \'n04343740\', \'n04395875\', \'n02823586\', \'n02910241\', \'n04018399\', \'n02696165\', \'n03393017\',\n    \'n04055700\', \'n07888378\', \'n04400109\', \'n04407686\', \'n04614655\', \'n04417809\', \'n03798982\', \'n03202481\', \'n03678729\',\n    \'n03801533\', \'n03849814\', \'n04581595\', \'n03726233\'),\n    # cat n02121620\n    (\n    \'n02121620\', \'n02121808\', \'n02122298\', \'n02123478\', \'n02122725\', \'n02124484\', \'n02124157\', \'n02122878\', \'n02123917\',\n    \'n02122510\', \'n02124313\', \'n02123242\', \'n02122430\', \'n02124623\', \'n02125081\', \'n02125872\', \'n02125010\', \'n02126787\',\n    \'n02126317\', \'n02125494\', \'n02125689\', \'n02126028\', \'n02126640\', \'n02126139\'),\n    # clothing n03051540\n    (\n    \'n03051540\', \'n04385079\', \'n02756098\', \'n03859958\', \'n04489695\', \'n02834506\', \'n03964611\', \'n03289985\', \'n04129766\',\n    \'n03114041\', \'n03113657\', \'n03320519\', \'n03340923\', \'n04355115\', \'n03786194\', \'n03114236\', \'n03206718\', \'n03320519\',\n    \'n03319457\', \'n03221059\', \'n02726017\', \'n03384891\', \'n02780704\', \'n03239054\', \'n03201638\', \'n03201776\', \'n04285803\',\n    \'n04120695\', \'n03472672\', \'n03476083\', \'n02683454\', \'n04459018\', \'n03917327\', \'n03538957\', \'n03263338\', \'n03113835\',\n    \'n02669534\', \'n04092168\', \'n03473966\', \'n03398153\', \'n03835941\', \'n03781467\', \'n03474167\', \'n02846141\', \'n03692379\',\n    \'n03692136\', \'n03692272\', \'n03692004\', \'n04243142\', \'n04600912\', \'n03863108\', \'n03655720\', \'n03815482\', \'n03068998\',\n    \'n03406759\', \'n04015204\', \'n04194127\', \'n03268645\', \'n03216402\', \'n03121897\', \'n03863262\', \'n03604763\', \'n04607640\',\n    \'n02752615\', \'n04429038\', \'n03015478\', \'n02841847\', \'n04423552\', \'n04001845\', \'n02720576\', \'n04266375\', \'n02863340\',\n    \'n02738859\', \'n03386870\', \'n04207903\', \'n03521771\', \'n02671780\', \'n02827606\', \'n04125853\', \'n04132465\', \'n02778294\',\n    \'n02972397\', \'n02786611\', \'n03527565\', \'n03781683\', \'n03405595\', \'n03859495\', \'n03450516\', \'n03434830\', \'n03010795\',\n    \'n03880129\', \'n02694966\', \'n04364994\', \'n03981340\', \'n03548930\', \'n02979516\', \'n04264233\', \'n04446162\', \'n02814774\',\n    \'n02742322\', \'n04552097\', \'n02855925\', \'n03419014\', \'n04332580\', \'n04488530\', \'n02922578\', \'n04531873\', \'n03608504\',\n    \'n04371563\', \'n04574067\', \'n04219580\', \'n02896294\', \'n03186199\', \'n03226538\', \'n04222470\', \'n03943920\', \'n02925519\',\n    \'n04427715\', \'n04504141\', \'n04615644\', \'n04233832\', \'n03885669\', \'n03657511\', \'n04046277\', \'n03789794\', \'n04123317\',\n    \'n03502331\', \'n03314884\', \'n03826039\', \'n04612159\', \'n02998841\', \'n04605572\', \'n04489008\', \'n03660124\', \'n04337287\',\n    \'n03877674\', \'n03600285\', \'n02896442\', \'n02902816\', \'n03970363\', \'n04491934\', \'n02910864\', \'n03107488\', \'n03688605\',\n    \'n03019434\', \'n03884554\', \'n04370288\', \'n04480303\', \'n03029296\', \'n04132158\', \'n04233715\', \'n03903733\', \'n04205318\',\n    \'n03653833\', \'n02831595\', \'n03543112\', \'n02825442\', \'n03357081\', \'n03745487\', \'n03487642\', \'n03450734\', \'n04143897\',\n    \'n03332173\', \'n03610992\', \'n03814817\', \'n03505504\', \'n03520493\', \'n03615655\', \'n02766168\', \'n04122262\', \'n04495698\',\n    \'n03719743\', \'n03900028\', \'n04060448\', \'n03797182\', \'n03648219\', \'n03816005\', \'n03815615\', \'n03845990\', \'n02847631\',\n    \'n04339191\', \'n03388323\', \'n03128085\', \'n02747063\', \'n03814727\', \'n03913930\', \'n04325804\', \'n04370774\', \'n04160261\',\n    \'n04570532\', \'n03605598\', \'n04104770\', \'n04230808\', \'n03454442\', \'n03617312\', \'n03429003\', \'n03205669\', \'n03732458\',\n    \'n02780815\', \'n03402511\', \'n03146777\', \'n03649003\', \'n03523506\', \'n03863923\', \'n03588216\', \'n03045337\', \'n03595055\',\n    \'n04497570\', \'n03472796\', \'n04361937\', \'n04378489\', \'n03619275\', \'n03607923\', \'n02956883\', \'n02936402\', \'n02923535\',\n    \'n03103904\', \'n03880032\', \'n02955767\', \'n04440597\', \'n03021228\', \'n03719560\', \'n03906789\', \'n03219859\', \'n04605446\',\n    \'n04445040\', \'n04445154\', \'n04186455\', \'n04173907\', \'n03998333\', \'n03849943\', \'n03057021\', \'n03703463\', \'n03770954\',\n    \'n04122492\', \'n04455579\', \'n04049405\', \'n03844815\', \'n02921406\', \'n03254046\', \'n03057841\', \'n03456665\', \'n04506402\',\n    \'n04365229\', \'n02957008\', \'n03589791\', \'n03238286\', \'n02867966\', \'n03595264\', \'n02788462\', \'n03221540\', \'n02864504\',\n    \'n04123026\', \'n03548320\', \'n03226375\', \'n03751269\', \'n04222307\', \'n03696909\', \'n02925385\', \'n02850358\', \'n03219966\',\n    \'n03720005\', \'n04368496\', \'n02820675\', \'n03902756\', \'n03891051\', \'n04230387\', \'n02937010\', \'n03301175\', \'n03829857\',\n    \'n03604536\', \'n03228254\', \'n04123448\', \'n03398228\', \'n04003359\', \'n04363777\', \'n04187970\', \'n02885233\', \'n04252560\',\n    \'n04357531\', \'n04367950\', \'n04370048\', \'n04021028\', \'n04502197\', \'n03655072\', \'n04269822\', \'n03410938\', \'n04027935\',\n    \'n03006903\', \'n04097866\', \'n02807616\', \'n03237992\', \'n04085574\', \'n04197391\', \'n04390577\', \'n03238879\', \'n04602956\',\n    \'n03978966\', \'n03476542\', \'n03163381\', \'n03629231\', \'n02943964\', \'n04502197\', \'n04172904\', \'n04508163\', \'n04223299\',\n    \'n02944146\', \'n04508489\', \'n02837887\', \'n04426427\', \'n02854739\', \'n03885028\', \'n02901114\', \'n03234164\', \'n04514241\',\n    \'n03387323\', \'n04103665\', \'n03112869\', \'n03885788\', \'n02863014\', \'n03013580\', \'n04508949\', \'n03688192\', \'n03673450\',\n    \'n04509171\', \'n03824381\', \'n04231905\', \'n02930214\', \'n03920737\', \'n03132776\', \'n03421324\', \'n03688707\', \'n03585875\',\n    \'n03404149\', \'n03540090\', \'n03456186\', \'n03490324\', \'n03796974\', \'n03441112\', \'n02811204\', \'n03429771\', \'n03447075\',\n    \'n03616979\', \'n03429682\', \'n03502509\', \'n03531281\', \'n03124474\', \'n02941845\', \'n03513137\', \'n04265428\', \'n04229107\',\n    \'n02811350\', \'n03504205\', \'n03492922\', \'n02954340\', \'n04129688\', \'n04228693\', \'n03103563\', \'n03061893\', \'n04387095\',\n    \'n02831237\', \'n03331077\', \'n04596116\', \'n03610682\', \'n02799323\', \'n03824284\', \'n02843909\', \'n04232153\', \'n03065243\',\n    \'n02816768\', \'n04612026\', \'n04556408\', \'n03607527\', \'n02776825\', \'n03776167\', \'n03420801\', \'n03049924\', \'n02941228\',\n    \'n03439631\', \'n04455048\', \'n04585318\', \'n03597317\', \'n04432203\', \'n03138669\', \'n03497657\', \'n04356595\', \'n03950899\',\n    \'n04354589\', \'n04354589\', \'n04248507\', \'n03984643\', \'n02987379\', \'n03256631\', \'n03061050\', \'n02834642\', \'n04482177\',\n    \'n04456011\', \'n02859184\', \'n04208582\', \'n02881757\', \'n03404360\', \'n02818135\', \'n04505888\', \'n04264361\', \'n02945964\',\n    \'n03237416\', \'n03766322\', \'n03931885\', \'n03046029\', \'n03937835\', \'n03028785\', \'n03170872\', \'n03325941\', \'n04441528\',\n    \'n03607186\', \'n04498389\', \'n04335693\', \'n03381126\', \'n03540267\', \'n04434932\', \'n03885904\', \'n02713218\', \'n04378956\',\n    \'n03622931\', \'n02736798\', \'n02752496\', \'n04323819\', \'n04360914\', \'n03622931\', \'n03836976\', \'n02874336\', \'n04532022\',\n    \'n04059157\', \'n04509592\', \'n03605504\', \'n04071393\', \'n03402188\', \'n03239259\', \'n03237212\', \'n03846234\', \'n02811719\',\n    \'n03615563\', \'n03324928\', \'n03825080\', \'n04235771\', \'n03824381\', \'n03824999\', \'n03625943\', \'n02728440\', \'n04603872\',\n    \'n03660124\', \'n04602956\', \'n03746330\', \'n02752615\', \'n02887489\', \'n03237416\', \'n04596852\', \'n03236735\', \'n03619196\',\n    \'n03788914\', \'n04197878\', \'n03604400\', \'n02936570\', \'n03013438\', \'n03062015\', \'n02781121\', \'n02898585\', \'n03201638\',\n    \'n04397645\', \'n04334105\', \'n03057724\', \'n03205574\', \'n04355511\', \'n03978815\', \'n03786096\', \'n04136161\', \'n03817647\',\n    \'n02908123\', \'n02944075\', \'n02697221\', \'n04453666\', \'n02861387\', \'n02926426\', \'n03480579\', \'n02854926\', \'n03418749\',\n    \'n03467254\', \'n04198562\', \'n03762238\', \'n04514241\', \'n03464053\', \'n04241249\', \'n03036469\', \'n03036341\', \'n03797264\'),\n    # dog n02084071\n    (\n    \'n02084071\', \'n02084732\', \'n02087122\', \'n02098550\', \'n02099997\', \'n02100399\', \'n02098806\', \'n02101108\', \'n02101670\',\n    \'n02102605\', \'n02102806\', \'n02101861\', \'n02103181\', \'n02098906\', \'n02099029\', \'n02089232\', \'n02089468\', \'n02092468\',\n    \'n02095050\', \'n02095212\', \'n02095412\', \'n02095727\', \'n02096756\', \'n02093056\', \'n02097786\', \'n02097967\', \'n02094562\',\n    \'n02094721\', \'n02094931\', \'n02087314\', \'n02087551\', \'n02090253\', \'n02090475\', \'n02088839\', \'n02088992\', \'n02089555\',\n    \'n02089725\', \'n02092173\', \'n02090827\', \'n02090129\', \'n02088745\', \'n02110532\', \'n02084861\', \'n02085118\', \'n02085019\',\n    \'n02112826\', \'n02085272\', \'n02113335\', \'n02113892\', \'n02112497\', \'n02103406\', \'n02109150\', \'n02109256\', \'n02104523\',\n    \'n02104882\', \'n02103841\', \'n02106966\', \'n02104280\', \'n02104184\', \'n02106854\', \'n02109687\', \'n02109811\', \'n02107420\',\n    \'n02108254\', \'n02109391\', \'n02108672\', \'n02111626\', \'n02085374\', \'n02086346\', \'n02086753\', \'n02086478\', \'n01322604\'),\n    # electronic equipment n03278248\n    (\n    \'n03278248\', \'n02757462\', \'n04077430\', \'n03517760\', \'n04315948\', \'n04546340\', \'n03436182\', \'n04030965\', \'n03584400\',\n    \'n04064213\', \'n04043411\', \'n04413419\', \'n04075291\', \'n02676670\', \'n03181293\', \'n03143400\', \'n04142731\', \'n03034405\',\n    \'n03775388\', \'n04176528\', \'n04060647\', \'n03205304\', \'n03447894\', \'n04042204\', \'n04137773\', \'n04043733\', \'n03144156\',\n    \'n03516996\', \'n03046921\', \'n04027367\', \'n04405907\', \'n04472726\', \'n04138131\', \'n04406552\', \'n03592931\', \'n04045085\',\n    \'n04269668\', \'n04405762\', \'n02705944\', \'n02872529\', \'n02756854\', \'n03724176\', \'n03424204\', \'n04405540\', \'n04404997\',\n    \'n02942349\', \'n02995345\', \'n03916720\', \'n03225777\', \'n04595285\', \'n03571942\', \'n02909285\', \'n03861048\', \'n03163973\',\n    \'n04143140\', \'n03781787\', \'n02962938\', \'n03278914\', \'n03963294\', \'n03293741\', \'n03656957\', \'n04392526\', \'n02979074\',\n    \'n04401088\', \'n03488438\', \'n03306869\', \'n04044498\', \'n03179910\', \'n04270371\', \'n03842377\'),\n    # fish n02512053\n    (\n    \'n02512053\', \'n01316579\', \'n02599958\', \'n02600298\', \'n02600503\', \'n02600798\', \'n01480516\', \'n01482071\', \'n01495701\',\n    \'n01497118\', \'n01497413\', \'n01497738\', \'n01498406\', \'n01498699\', \'n01498989\', \'n01499396\', \'n01499732\', \'n01500091\',\n    \'n01500854\', \'n01500476\', \'n01501160\', \'n01501641\', \'n01501777\', \'n01501948\', \'n01502101\', \'n01482330\', \'n01483021\',\n    \'n01483522\', \'n01483830\', \'n01484097\', \'n01484285\', \'n01484447\', \'n01484562\', \'n01485479\', \'n01486010\', \'n01486540\',\n    \'n01486838\', \'n01487506\', \'n01488038\', \'n01488918\', \'n01489501\', \'n01489709\', \'n01489920\', \'n01490112\', \'n01490360\',\n    \'n01490670\', \'n01491006\', \'n01491661\', \'n01491874\', \'n01492357\', \'n01492569\', \'n01492708\', \'n01492860\', \'n01493146\',\n    \'n01493541\', \'n01493829\', \'n01494041\', \'n01494757\', \'n01494882\', \'n01495006\', \'n01495493\', \'n01480880\', \'n01481331\',\n    \'n01481498\', \'n02512752\', \'n02512830\', \'n02512938\', \'n02513355\', \'n02530421\', \'n02530637\', \'n02530831\', \'n02530999\',\n    \'n02532028\', \'n02532272\', \'n02532451\', \'n02532602\', \'n02532918\', \'n02532786\', \'n02534734\', \'n02535163\', \'n02535258\',\n    \'n02535537\', \'n02535759\', \'n02536165\', \'n02536456\', \'n02537085\', \'n02537319\', \'n02537716\', \'n02537525\', \'n02538010\',\n    \'n02538216\', \'n02538985\', \'n02539424\', \'n02539573\', \'n02539894\', \'n02567334\', \'n02567633\', \'n02568087\', \'n02568447\',\n    \'n02568959\', \'n02569484\', \'n02569631\', \'n02569905\', \'n02570164\', \'n02586543\', \'n02587051\', \'n02587300\', \'n02587479\',\n    \'n02587618\', \'n02587877\', \'n02626762\', \'n02627037\', \'n02627292\', \'n02627532\', \'n02663849\', \'n02664285\', \'n02664642\',\n    \'n02665250\', \'n02513248\', \'n02513560\', \'n02513727\', \'n02530052\', \'n02530188\', \'n02535080\', \'n02513805\', \'n02513939\',\n    \'n02515214\', \'n02515713\', \'n02516188\', \'n02516776\', \'n02528163\', \'n02538985\', \'n02539424\', \'n02539573\', \'n02539894\',\n    \'n01429172\', \'n01438208\', \'n01438581\', \'n01439121\', \'n01439514\', \'n01439808\', \'n01441117\', \'n01441272\', \'n01441425\',\n    \'n01441910\', \'n01442450\', \'n01442710\', \'n01442972\', \'n01443243\', \'n01443831\', \'n01444339\', \'n01444783\', \'n01445429\',\n    \'n01445857\', \'n01446152\', \'n01446589\', \'n01446760\', \'n01447139\', \'n01447331\', \'n01447658\', \'n01447946\', \'n01448291\',\n    \'n01448594\', \'n01448951\', \'n01449374\', \'n01449712\', \'n01449980\', \'n02583567\', \'n02583890\', \'n02584145\', \'n02584449\',\n    \'n02517442\', \'n02517938\', \'n02518622\', \'n02518324\', \'n02519148\', \'n02519340\', \'n02519472\', \'n02519686\', \'n02519862\',\n    \'n02520147\', \'n02520525\', \'n02520810\', \'n02521646\', \'n02522399\', \'n02522637\', \'n02522722\', \'n02522866\', \'n02523427\',\n    \'n02523110\', \'n02523877\', \'n02524202\', \'n02524524\', \'n02524928\', \'n02524659\', \'n02525382\', \'n02525703\', \'n02526425\',\n    \'n02526818\', \'n02527057\', \'n02527271\', \'n02527622\', \'n02529293\', \'n02529772\', \'n02530421\', \'n02530637\', \'n02530831\',\n    \'n02530999\', \'n02532028\', \'n02532272\', \'n02532451\', \'n02532602\', \'n02532918\', \'n02532786\', \'n02531114\', \'n02531625\',\n    \'n02533209\', \'n02533545\', \'n02533834\', \'n02534165\', \'n02534559\', \'n02534734\', \'n02535163\', \'n02535258\', \'n02535537\',\n    \'n02535759\', \'n02536165\', \'n02536456\', \'n02537085\', \'n02537319\', \'n02537716\', \'n02537525\', \'n02538010\', \'n02538216\',\n    \'n02538406\', \'n02538562\', \'n02540412\', \'n02540983\', \'n02541257\', \'n02541687\', \'n02542017\', \'n02542432\', \'n02542958\',\n    \'n02543255\', \'n02543565\', \'n02544274\', \'n02545841\', \'n02546028\', \'n02546331\', \'n02546627\', \'n02547014\', \'n01454545\',\n    \'n01455778\', \'n01456137\', \'n01456454\', \'n01456756\', \'n01457082\', \'n01457407\', \'n01457852\', \'n02549989\', \'n02550203\',\n    \'n02550460\', \'n02550655\', \'n02551134\', \'n02551668\', \'n02552171\', \'n01450661\', \'n01450950\', \'n01451115\', \'n01451295\',\n    \'n01451426\', \'n01451863\', \'n01452345\', \'n01453087\', \'n01453475\', \'n01453742\', \'n01454856\', \'n01455461\', \'n01455317\',\n    \'n02547733\', \'n02548247\', \'n02548689\', \'n02548884\', \'n02549248\', \'n02549376\', \'n02554730\', \'n02599958\', \'n02600298\',\n    \'n02600503\', \'n02600798\', \'n02586543\', \'n02587051\', \'n02587300\', \'n02587479\', \'n02587618\', \'n02587877\', \'n02555863\',\n    \'n02556846\', \'n02557182\', \'n02557318\', \'n02557591\', \'n02557749\', \'n02558206\', \'n02558860\', \'n02559144\', \'n02559383\',\n    \'n02559862\', \'n02560110\', \'n02561108\', \'n02561381\', \'n02561514\', \'n02561661\', \'n02561803\', \'n02561937\', \'n02562315\',\n    \'n02562796\', \'n02562971\', \'n02563079\', \'n02563182\', \'n01440467\', \'n02563792\', \'n02563949\', \'n02563648\', \'n02564403\',\n    \'n02564720\', \'n02564935\', \'n02565072\', \'n02565324\', \'n02565573\', \'n02564270\', \'n02566109\', \'n02567334\', \'n02567633\',\n    \'n02568087\', \'n02568447\', \'n02568959\', \'n02566489\', \'n02566665\', \'n02570484\', \'n02570838\', \'n02571167\', \'n02571652\',\n    \'n02571810\', \'n02572196\', \'n02572484\', \'n02573249\', \'n02573704\', \'n02574271\', \'n02576223\', \'n02576575\', \'n02576906\',\n    \'n02577041\', \'n02577164\', \'n02577403\', \'n02577662\', \'n02577952\', \'n02578771\', \'n02578928\', \'n02579303\', \'n02578233\',\n    \'n02578454\', \'n02579557\', \'n02579762\', \'n02579928\', \'n02580336\', \'n02580679\', \'n02580830\', \'n02581108\', \'n02581482\',\n    \'n02581642\', \'n02581957\', \'n02582220\', \'n02582349\', \'n02585872\', \'n02586238\', \'n02588286\', \'n02588794\', \'n02588945\',\n    \'n02589062\', \'n02589196\', \'n02589316\', \'n02589623\', \'n02589796\', \'n02590094\', \'n02590495\', \'n02592055\', \'n02592371\',\n    \'n02593019\', \'n02590702\', \'n02582721\', \'n02590987\', \'n02591330\', \'n02592734\', \'n02593453\', \'n02593679\', \'n02591613\',\n    \'n02591911\', \'n02593191\', \'n02594250\', \'n02594942\', \'n02595056\', \'n02595339\', \'n02595702\', \'n02596067\', \'n02596252\',\n    \'n02596381\', \'n02596720\', \'n02597004\', \'n02598573\', \'n02598878\', \'n02597367\', \'n02597608\', \'n02597818\', \'n02597972\',\n    \'n02598134\', \'n02599052\', \'n02599557\', \'n02599347\', \'n02601344\', \'n02601767\', \'n02601921\', \'n02602059\', \'n02604157\',\n    \'n02604480\', \'n02604954\', \'n02605316\', \'n02605703\', \'n02605936\', \'n02606384\', \'n02606751\', \'n02607201\', \'n02607470\',\n    \'n02607862\', \'n02608284\', \'n02608547\', \'n02608860\', \'n02608996\', \'n02609302\', \'n02609823\', \'n02610066\', \'n02610373\',\n    \'n02610664\', \'n02610980\', \'n02611561\', \'n02611898\', \'n02612167\', \'n02613181\', \'n02613572\', \'n02613820\', \'n02614140\',\n    \'n02614482\', \'n02614653\', \'n02614978\', \'n02615298\', \'n02616128\', \'n02616397\', \'n02616851\', \'n02617537\', \'n02618094\',\n    \'n02619165\', \'n02619550\', \'n02619861\', \'n02620167\', \'n02620578\', \'n02621258\', \'n02621908\', \'n02622249\', \'n02622547\',\n    \'n02622712\', \'n02622955\', \'n02623445\', \'n02626762\', \'n02627037\', \'n02627292\', \'n02627532\', \'n02624167\', \'n02624551\',\n    \'n02624807\', \'n02624987\', \'n02625258\', \'n02625612\', \'n02627835\', \'n02628062\', \'n02628259\', \'n02628600\', \'n02629230\',\n    \'n02629716\', \'n02630281\', \'n02630615\', \'n02630739\', \'n02631041\', \'n02632039\', \'n02632494\', \'n02633422\', \'n02633677\',\n    \'n02633977\', \'n02634545\', \'n02635154\', \'n02635580\', \'n02636170\', \'n02636405\', \'n02636550\', \'n02636854\', \'n02637179\',\n    \'n02637475\', \'n02637977\', \'n02574910\', \'n02575325\', \'n02575590\', \'n02602405\', \'n02602760\', \'n02603317\', \'n02603540\',\n    \'n02618513\', \'n02618827\', \'n02642107\', \'n02553028\', \'n02642644\', \'n02643112\', \'n02643316\', \'n02643836\', \'n02644113\',\n    \'n02644360\', \'n02644501\', \'n02644665\', \'n02644817\', \'n02645538\', \'n02645691\', \'n02645953\', \'n02646667\', \'n02646892\',\n    \'n02648035\', \'n02648625\', \'n02648916\', \'n02649218\', \'n02649546\', \'n02650050\', \'n02650413\', \'n02650541\', \'n02651060\',\n    \'n02652132\', \'n02652668\', \'n02653145\', \'n02653497\', \'n02653786\', \'n02654112\', \'n02654425\', \'n02654745\', \'n02655523\',\n    \'n02655848\', \'n02656032\', \'n02656301\', \'n02656670\', \'n02656969\', \'n02657368\', \'n02663849\', \'n02664285\', \'n02664642\',\n    \'n02665250\', \'n02657694\', \'n02658079\', \'n02658531\', \'n02658811\', \'n02659176\', \'n02659478\', \'n02659808\', \'n02660091\',\n    \'n02660519\', \'n02660640\', \'n02660208\', \'n02661017\', \'n02661473\', \'n02661618\', \'n02662239\', \'n02662397\', \'n02662559\',\n    \'n02662825\', \'n02662993\', \'n02663211\', \'n02663485\', \'n02603862\', \'n02638596\', \'n02639087\', \'n02639605\', \'n02639922\',\n    \'n02640857\', \'n02640626\', \'n02556373\'),\n    # footwear n03380867\n    (\n    \'n03380867\', \'n04199027\', \'n03297103\', \'n04156411\', \'n03547530\', \'n04027706\', \'n03364008\', \'n04386664\', \'n03027625\',\n    \'n04239786\', \'n04122578\', \'n03090710\', \'n02904927\', \'n02713364\', \'n04593524\', \'n02938218\', \'n02855701\', \'n03411079\',\n    \'n04545748\', \'n03868406\', \'n04124370\', \'n02882894\', \'n02767147\', \'n04570118\', \'n03776877\', \'n03364156\', \'n03041449\',\n    \'n03472535\', \'n03967270\', \'n03025250\', \'n04022332\', \'n04272389\', \'n04546081\', \'n03361550\', \'n04241394\', \'n03798061\',\n    \'n02873733\', \'n02872752\', \'n04228581\', \'n04089666\', \'n03600475\', \'n03516844\', \'n03521544\', \'n04542715\', \'n02925666\',\n    \'n04116294\', \'n03865949\', \'n02735538\'),\n    # fruit n13134947\n    (\n    \'n13134947\', \'n07705931\', \'n07738105\', \'n07738224\', \'n07739035\', \'n07739125\', \'n07739344\', \'n07739506\', \'n07739923\',\n    \'n07740033\', \'n07740220\', \'n07740342\', \'n07740461\', \'n07740597\', \'n07740744\', \'n07740855\', \'n07740954\', \'n07741138\',\n    \'n07741235\', \'n07741357\', \'n07741461\', \'n07740115\', \'n07741623\', \'n07741706\', \'n07741804\', \'n07741888\', \'n07742012\',\n    \'n07742224\', \'n07742415\', \'n07742513\', \'n07742605\', \'n07742704\', \'n07743224\', \'n07743384\', \'n07743544\', \'n07743723\',\n    \'n07743902\', \'n07744057\', \'n07744246\', \'n07744430\', \'n07744559\', \'n07744682\', \'n07744811\', \'n07745046\', \'n07745197\',\n    \'n07745357\', \'n07745466\', \'n07745661\', \'n07746038\', \'n07746186\', \'n07746334\', \'n07767171\', \'n07746551\', \'n07746749\',\n    \'n07746910\', \'n07747055\', \'n07747811\', \'n07748753\', \'n07748912\', \'n07749095\', \'n07749192\', \'n07749312\', \'n07747951\',\n    \'n07748157\', \'n07748276\', \'n07748416\', \'n07749446\', \'n07749731\', \'n07749870\', \'n07749969\', \'n07750146\', \'n07750299\',\n    \'n07750449\', \'n07748574\', \'n07750872\', \'n07751004\', \'n07751148\', \'n07751280\', \'n07751451\', \'n07751737\', \'n07751858\',\n    \'n07751977\', \'n07752109\', \'n07752264\', \'n07752377\', \'n07752514\', \'n07752602\', \'n07752664\', \'n07752782\', \'n07752874\',\n    \'n07752966\', \'n07753448\', \'n07753743\', \'n07753980\', \'n07754155\', \'n07754279\', \'n07754451\', \'n07755262\', \'n07755411\',\n    \'n07755619\', \'n07755707\', \'n07755929\', \'n07756096\', \'n07756325\', \'n07756499\', \'n07756838\', \'n07756641\', \'n07756951\',\n    \'n07757132\', \'n07757312\', \'n07757511\', \'n07757602\', \'n07757753\', \'n07757874\', \'n07757990\', \'n07758125\', \'n07758260\',\n    \'n07758407\', \'n07758680\', \'n07759424\', \'n07759576\', \'n07759691\', \'n07758950\', \'n07759194\', \'n07759324\', \'n07759816\',\n    \'n07760070\', \'n07760153\', \'n07760297\', \'n07760395\', \'n07760501\', \'n07760673\', \'n07760755\', \'n07761141\', \'n07761309\',\n    \'n07761611\', \'n07761777\', \'n07761954\', \'n07762114\', \'n07762244\', \'n07762373\', \'n07762534\', \'n07762740\', \'n07762913\',\n    \'n07763107\', \'n07763290\', \'n07763483\', \'n07763629\', \'n07763792\', \'n07763987\', \'n07764155\', \'n07764315\', \'n07764486\',\n    \'n07764630\', \'n07764847\', \'n07765073\', \'n07765208\', \'n07765361\', \'n07765517\', \'n07765612\', \'n07765728\', \'n07765862\',\n    \'n07765999\', \'n07766173\', \'n07766409\', \'n07766530\', \'n07766723\', \'n07766891\', \'n07767002\', \'n07767847\', \'n07768068\',\n    \'n07768139\', \'n07768230\', \'n07768318\', \'n07768590\', \'n07768858\', \'n07769102\', \'n07769306\', \'n07769584\', \'n07769731\',\n    \'n07769886\', \'n07770034\', \'n07770180\', \'n07770439\', \'n11636835\', \'n11700279\', \'n12036067\', \'n12036226\', \'n12158031\',\n    \'n12815838\', \'n12162758\', \'n12193334\', \'n12301445\', \'n12642090\', \'n12644283\', \'n12647787\', \'n12650805\', \'n12658481\',\n    \'n12144313\', \'n13135692\', \'n13135832\', \'n07770571\', \'n07770763\', \'n07770869\', \'n07775197\', \'n07814634\', \'n07929351\',\n    \'n11685091\', \'n11689197\', \'n11689367\', \'n11689483\', \'n11689678\', \'n11689815\', \'n11689957\', \'n15086247\', \'n11946313\',\n    \'n12156819\', \'n11823305\', \'n12123648\', \'n12142357\', \'n12157056\', \'n12157179\', \'n12585373\', \'n12592839\', \'n12593341\',\n    \'n12696830\', \'n12928819\', \'n13136316\', \'n11750173\', \'n11766046\', \'n12487058\', \'n12493426\', \'n12532564\', \'n12576323\',\n    \'n13136556\', \'n07737081\', \'n07737594\', \'n07737745\', \'n07750586\', \'n07750736\', \'n07769465\', \'n07771082\', \'n07771212\',\n    \'n07771539\', \'n07771405\', \'n07771731\', \'n07771891\', \'n07772026\', \'n07772147\', \'n07772274\', \'n07772413\', \'n07772788\',\n    \'n07772935\', \'n07774182\', \'n07774295\', \'n07774596\', \'n07774719\', \'n07774842\', \'n07775050\', \'n11612235\', \'n12197601\',\n    \'n12280364\', \'n12590715\', \'n13136781\', \'n13137409\', \'n07743902\', \'n11723986\', \'n13137951\', \'n13137672\', \'n13137225\',\n    \'n13138308\', \'n07751004\', \'n07767344\', \'n07767709\', \'n07767549\', \'n13138658\', \'n07744811\', \'n13138155\', \'n13138842\',\n    \'n07739125\', \'n07739344\', \'n13139055\', \'n11748002\', \'n12515925\', \'n12544539\', \'n12560282\', \'n12560621\', \'n12561594\',\n    \'n12578916\', \'n11748811\', \'n11766432\', \'n12172364\', \'n13139321\', \'n13139482\', \'n13140367\', \'n13141415\', \'n13150378\',\n    \'n13150592\'),\n    # fungus n12992868\n    (\n    \'n12992868\', \'n13035241\', \'n13035707\', \'n13036312\', \'n13035925\', \'n13036116\', \'n13036804\', \'n12982468\', \'n12982590\',\n    \'n12985420\', \'n13079419\', \'n13079567\', \'n13001930\', \'n13023134\', \'n12987056\', \'n12987535\', \'n12988158\', \'n12988341\',\n    \'n12988572\', \'n12991184\', \'n12989938\', \'n12991837\', \'n12989007\', \'n12987423\', \'n12992177\', \'n12990597\', \'n13027879\',\n    \'n13081999\', \'n12979829\', \'n13077295\', \'n12963628\', \'n12980840\', \'n12981301\', \'n12981443\', \'n12981086\', \'n12969131\',\n    \'n12969670\', \'n12969425\', \'n12969927\', \'n13025647\', \'n13026015\', \'n13025854\', \'n13046669\', \'n13081229\', \'n13078021\',\n    \'n13002209\', \'n13079073\', \'n13015509\', \'n13060190\', \'n13062421\', \'n13061348\', \'n13061704\', \'n13061471\', \'n13061172\',\n    \'n13080306\', \'n12964920\', \'n12970733\', \'n13080866\', \'n13018906\', \'n13048447\', \'n12966945\', \'n13066129\', \'n13067672\',\n    \'n13067532\', \'n13068434\', \'n13067191\', \'n13067330\', \'n13066979\', \'n13068255\', \'n13068917\', \'n13069224\', \'n13068735\',\n    \'n13066448\', \'n12971400\', \'n12971804\', \'n12972136\', \'n13046130\', \'n13045210\', \'n13045975\', \'n12978076\', \'n12965626\',\n    \'n12965951\', \'n13034555\', \'n13045594\', \'n12985773\', \'n13042982\', \'n13041312\', \'n13040629\', \'n13040796\', \'n13082568\',\n    \'n13016076\', \'n13024012\', \'n12983873\', \'n13024500\', \'n13024653\', \'n12983961\', \'n12984489\', \'n12984595\', \'n12984267\',\n    \'n13028611\', \'n13029122\', \'n13031323\', \'n13029326\', \'n13031474\', \'n13029760\', \'n13031193\', \'n13030616\', \'n13029610\',\n    \'n13030852\', \'n13028937\', \'n12982915\', \'n13047862\', \'n13016289\', \'n13077033\', \'n12973791\', \'n12973937\', \'n12980080\',\n    \'n12973443\', \'n12981954\', \'n12995601\', \'n12966804\', \'n13063269\', \'n13065514\', \'n13065089\', \'n13064111\', \'n13064457\',\n    \'n13035389\', \'n13038577\', \'n13043926\', \'n13044375\', \'n13027557\', \'n12968136\', \'n12968309\', \'n13037585\', \'n13037805\',\n    \'n13038376\', \'n13038068\', \'n13038744\', \'n13069773\', \'n13054073\', \'n13039349\', \'n12970193\', \'n12970293\', \'n13042316\',\n    \'n13042134\', \'n13041943\', \'n12986227\', \'n13056799\', \'n13059298\', \'n13056135\', \'n13058272\', \'n13056349\', \'n13057054\',\n    \'n13055577\', \'n13057422\', \'n13058608\', \'n13058037\', \'n13055792\', \'n13060017\', \'n13055423\', \'n13055949\', \'n13057242\',\n    \'n13057639\', \'n13059657\', \'n13056607\', \'n12997654\', \'n13049953\', \'n13052931\', \'n13050940\', \'n13053608\', \'n13050705\',\n    \'n13050397\', \'n13051346\', \'n13052248\', \'n13052014\', \'n13011595\', \'n12997919\', \'n13032115\', \'n13033879\', \'n13032381\',\n    \'n13033577\', \'n13032618\', \'n13034062\', \'n13032923\', \'n13033134\', \'n13033396\', \'n13002750\', \'n13232106\', \'n13003254\',\n    \'n13001366\', \'n13008839\', \'n13004826\', \'n13075020\', \'n13009656\', \'n13009244\', \'n13020481\', \'n13003712\', \'n13003522\',\n    \'n13018407\', \'n13004640\', \'n13022210\', \'n13017240\', \'n13002925\', \'n13074814\', \'n13008157\', \'n13020964\', \'n13017979\',\n    \'n13232363\', \'n13017610\', \'n13076405\', \'n13012469\', \'n13076831\', \'n13017439\', \'n13007629\', \'n13232779\', \'n13020191\',\n    \'n13004423\', \'n13013534\', \'n13014097\', \'n13014581\', \'n13014741\', \'n13014879\', \'n13014409\', \'n13013965\', \'n13014265\',\n    \'n13006631\', \'n13005984\', \'n13010951\', \'n13008485\', \'n13021867\', \'n13000891\', \'n13013764\', \'n13021543\', \'n13017789\',\n    \'n13009429\', \'n13005329\', \'n13006171\', \'n13010694\', \'n13075441\', \'n13001206\', \'n13017102\', \'n13076041\', \'n13018088\',\n    \'n13021332\', \'n13018232\', \'n13070308\', \'n13072350\', \'n13072209\', \'n13070875\', \'n13072863\', \'n13072031\', \'n13073703\',\n    \'n13072528\', \'n13071815\', \'n13072706\', \'n13073055\', \'n13071371\', \'n13071553\', \'n13075684\', \'n13075847\', \'n13004992\',\n    \'n13021689\', \'n13231678\', \'n13001041\', \'n13009085\', \'n13008689\', \'n13001529\', \'n13012253\', \'n13231919\', \'n13019496\',\n    \'n13074619\', \'n13006894\', \'n13019835\', \'n13075272\', \'n13003061\', \'n13012973\', \'n13011221\', \'n13019643\', \'n13076643\',\n    \'n13008315\', \'n13021166\', \'n13007417\', \'n13015688\', \'n12974987\', \'n12975804\', \'n12976198\', \'n12976554\', \'n12983654\',\n    \'n12979316\', \'n13226871\', \'n13034788\', \'n12983048\'),\n    # geological formation n09287968\n    (\n    \'n09287968\', \'n09201998\', \'n09217230\', \'n09393524\', \'n09238926\', \'n09239302\', \'n09257843\', \'n09294877\', \'n09259025\',\n    \'n09398677\', \'n09264803\', \'n09266604\', \'n09283866\', \'n09309292\', \'n09289331\', \'n09194227\', \'n09255070\', \'n09396608\',\n    \'n09391774\', \'n09308572\', \'n09295210\', \'n09308743\', \'n09309046\', \'n09309168\', \'n09331251\', \'n09348460\', \'n09357447\',\n    \'n09362316\', \'n09366017\', \'n09215437\', \'n09245515\', \'n09421031\', \'n09457979\', \'n09330378\', \'n09376526\', \'n09186592\',\n    \'n09415671\', \'n09448690\', \'n09259219\', \'n09249155\', \'n09344324\', \'n09304750\', \'n09230041\', \'n09474765\', \'n09290350\',\n    \'n09214269\', \'n09226869\', \'n09268007\', \'n09402944\', \'n09422190\', \'n09425019\', \'n09454744\', \'n09398076\', \'n09403086\',\n    \'n09344198\', \'n09335809\', \'n09422631\', \'n09435739\', \'n09452291\', \'n09262690\', \'n09289596\', \'n09300306\', \'n09206896\',\n    \'n09269882\', \'n09474010\', \'n09305031\', \'n09375606\', \'n09405787\', \'n09290444\', \'n09295946\', \'n09233446\', \'n09410224\',\n    \'n09366317\', \'n09302616\', \'n09269341\', \'n09453008\', \'n09351905\', \'n09456207\', \'n09303008\', \'n09230202\', \'n09283405\',\n    \'n09326662\', \'n09199101\', \'n09327077\', \'n09357346\', \'n09459979\', \'n09359803\', \'n09218641\', \'n09362945\', \'n09396465\',\n    \'n09409512\', \'n09213434\', \'n09224725\', \'n09421799\', \'n09433312\', \'n09214060\', \'n09270735\', \'n09429630\', \'n09274305\',\n    \'n09337253\', \'n09219233\', \'n09406793\', \'n09210862\', \'n09214916\', \'n09411295\', \'n09452760\', \'n09376786\', \'n09403734\',\n    \'n09409752\', \'n09205509\', \'n09433442\', \'n08596076\', \'n09335693\', \'n09428628\', \'n09458269\', \'n09447666\', \'n09437454\',\n    \'n09206985\', \'n09466678\', \'n09213565\', \'n09475925\', \'n09415584\', \'n09233603\', \'n09248153\', \'n09265620\', \'n09269472\',\n    \'n09445008\', \'n09274152\', \'n09303528\', \'n09228055\', \'n09361517\', \'n09391644\', \'n09436444\', \'n09305898\', \'n09454153\',\n    \'n09470222\', \'n09472413\', \'n09344724\', \'n09231117\', \'n09474412\', \'n09476123\'),\n    # hoofed animal n02370806\n    (\n    \'n02370806\', \'n02373336\', \'n02374149\', \'n02390258\', \'n02391617\', \'n02390101\', \'n02389346\', \'n02389559\', \'n02389779\',\n    \'n02389865\', \'n02390015\', \'n02389943\', \'n02390454\', \'n02390834\', \'n02390938\', \'n02390640\', \'n02390738\', \'n02391234\',\n    \'n02391373\', \'n02391508\', \'n02374451\', \'n02382948\', \'n02385776\', \'n02385580\', \'n02383231\', \'n02385098\', \'n02388143\',\n    \'n02385214\', \'n02385676\', \'n02388276\', \'n02388453\', \'n02375862\', \'n02385898\', \'n02389261\', \'n02382204\', \'n02380335\',\n    \'n02382039\', \'n02380583\', \'n02380745\', \'n02381460\', \'n02381831\', \'n02381609\', \'n02386310\', \'n02386496\', \'n02386853\',\n    \'n02387452\', \'n02387346\', \'n02387887\', \'n02386968\', \'n02387093\', \'n02386746\', \'n02382338\', \'n02387254\', \'n02382132\',\n    \'n02375302\', \'n02388917\', \'n02388588\', \'n02376918\', \'n02377181\', \'n02377291\', \'n02377388\', \'n02387983\', \'n02380464\',\n    \'n02388832\', \'n02386014\', \'n02386224\', \'n02386141\', \'n02382437\', \'n02382850\', \'n02382750\', \'n02382635\', \'n02377480\',\n    \'n02377603\', \'n02389128\', \'n02387722\', \'n02375757\', \'n02377703\', \'n02379430\', \'n02381364\', \'n02378870\', \'n02379908\',\n    \'n02379081\', \'n02379743\', \'n02379630\', \'n02378415\', \'n02378625\', \'n02378755\', \'n02378541\', \'n02378299\', \'n02378969\',\n    \'n02381004\', \'n02380052\', \'n02381119\', \'n02379183\', \'n02381261\', \'n02379329\', \'n02378149\', \'n02388735\', \'n02375438\',\n    \'n02384741\', \'n02393580\', \'n02393807\', \'n02393940\', \'n02391994\', \'n02392824\', \'n02392434\', \'n02393161\', \'n02392555\',\n    \'n02394477\', \'n02438580\', \'n02397529\', \'n02397744\', \'n02397987\', \'n02437136\', \'n02437482\', \'n02399000\', \'n02429456\',\n    \'n02401031\', \'n02418064\', \'n02418465\', \'n02419336\', \'n02419056\', \'n02419634\', \'n02418770\', \'n02411206\', \'n02411705\',\n    \'n02412210\', \'n02413131\', \'n02413917\', \'n02414043\', \'n02414442\', \'n02413593\', \'n02414209\', \'n02414290\', \'n02413717\',\n    \'n02413824\', \'n02413484\', \'n02411999\', \'n02413050\', \'n02419796\', \'n02424085\', \'n02424695\', \'n02425228\', \'n02424909\',\n    \'n02423218\', \'n02423589\', \'n02423362\', \'n02421449\', \'n02425887\', \'n02421136\', \'n02421792\', \'n02427724\', \'n02427576\',\n    \'n02427470\', \'n02428349\', \'n02428508\', \'n02426813\', \'n02427032\', \'n02427183\', \'n02425086\', \'n02424305\', \'n02424589\',\n    \'n02424486\', \'n02420828\', \'n02422391\', \'n02428089\', \'n02426176\', \'n02420509\', \'n02426481\', \'n02425532\', \'n02414578\',\n    \'n02414763\', \'n02416104\', \'n02415130\', \'n02414904\', \'n02415435\', \'n02415829\', \'n02415253\', \'n02416519\', \'n02417534\',\n    \'n02417785\', \'n02417663\', \'n02417070\', \'n02417387\', \'n02417242\', \'n02416820\', \'n02416964\', \'n02416880\', \'n02410702\',\n    \'n02410900\', \'n02407959\', \'n02409202\', \'n02409508\', \'n02409038\', \'n02408817\', \'n02408660\', \'n02402010\', \'n02404573\',\n    \'n02404906\', \'n02402425\', \'n02403231\', \'n02403153\', \'n02403325\', \'n02405692\', \'n02406859\', \'n02403454\', \'n02405577\',\n    \'n02406952\', \'n02406046\', \'n02404186\', \'n02406174\', \'n02402175\', \'n02405101\', \'n02405302\', \'n02405440\', \'n02409870\',\n    \'n02428842\', \'n02430045\', \'n02431976\', \'n02430830\', \'n02435216\', \'n02432511\', \'n02432704\', \'n02435517\', \'n02434712\',\n    \'n02431122\', \'n02431542\', \'n02431337\', \'n02431441\', \'n02432291\', \'n02433318\', \'n02433925\', \'n02434190\', \'n02434415\',\n    \'n02431628\', \'n02430748\', \'n02432983\', \'n02431785\', \'n02434954\', \'n02433546\', \'n02433729\', \'n02435853\', \'n02436224\',\n    \'n02436353\', \'n02436645\', \'n02439033\', \'n02439398\', \'n02437971\', \'n02438272\', \'n02438173\', \'n02395003\', \'n02395931\',\n    \'n02396014\', \'n02396088\', \'n02396796\', \'n02396157\', \'n02372140\'),\n    # insect n02159955\n    (\n    \'n02159955\', \'n02160947\', \'n02161225\', \'n02161338\', \'n02161457\', \'n02161588\', \'n02162561\', \'n02163008\', \'n02163297\',\n    \'n02164464\', \'n02165877\', \'n02166229\', \'n02166567\', \'n02166826\', \'n02167505\', \'n02167820\', \'n02167944\', \'n02168245\',\n    \'n02168427\', \'n02169023\', \'n02169218\', \'n02169705\', \'n02169974\', \'n02170400\', \'n02170599\', \'n02170738\', \'n02170993\',\n    \'n02171164\', \'n02171453\', \'n02171869\', \'n02172518\', \'n02172678\', \'n02172761\', \'n02172870\', \'n02173113\', \'n02173373\',\n    \'n02173784\', \'n02174355\', \'n02174659\', \'n02175014\', \'n02175569\', \'n02175916\', \'n02176261\', \'n02176439\', \'n02176747\',\n    \'n02177196\', \'n02177506\', \'n02177775\', \'n02178411\', \'n02178717\', \'n02181235\', \'n02181724\', \'n02182045\', \'n02182355\',\n    \'n02182642\', \'n02182930\', \'n02179012\', \'n02179192\', \'n02179340\', \'n02180233\', \'n02179891\', \'n02180427\', \'n02180875\',\n    \'n02183096\', \'n02183507\', \'n02183857\', \'n02184473\', \'n02184589\', \'n02184720\', \'n02185167\', \'n02185481\', \'n02186153\',\n    \'n02186717\', \'n02187150\', \'n02187279\', \'n02187554\', \'n02187900\', \'n02188699\', \'n02189363\', \'n02189670\', \'n02190790\',\n    \'n02191273\', \'n02191773\', \'n02191979\', \'n02192252\', \'n02192513\', \'n02192814\', \'n02193009\', \'n02193163\', \'n02194249\',\n    \'n02194750\', \'n02195091\', \'n02195526\', \'n02195819\', \'n02199502\', \'n02196119\', \'n02196344\', \'n02196896\', \'n02197185\',\n    \'n02197689\', \'n02197877\', \'n02198532\', \'n02198859\', \'n02199170\', \'n02200198\', \'n02200630\', \'n02200850\', \'n02201000\',\n    \'n02201497\', \'n02201626\', \'n02202006\', \'n02202124\', \'n02202287\', \'n02202678\', \'n02203152\', \'n02203978\', \'n02204249\',\n    \'n02205673\', \'n02203592\', \'n02204722\', \'n02204907\', \'n02205219\', \'n02198129\', \'n02206270\', \'n02220055\', \'n02220225\',\n    \'n02220518\', \'n02220804\', \'n02221083\', \'n02221414\', \'n02221571\', \'n02221715\', \'n02221820\', \'n02222035\', \'n02222582\',\n    \'n02222321\', \'n02207179\', \'n02208280\', \'n02208498\', \'n02208848\', \'n02208979\', \'n02209111\', \'n02209354\', \'n02209624\',\n    \'n02209964\', \'n02210427\', \'n02210921\', \'n02211444\', \'n02211627\', \'n02211896\', \'n02212062\', \'n02212602\', \'n02212958\',\n    \'n02214096\', \'n02213107\', \'n02213239\', \'n02213663\', \'n02213788\', \'n02213543\', \'n02214341\', \'n02214499\', \'n02214773\',\n    \'n02215161\', \'n02215621\', \'n02215770\', \'n02216211\', \'n02216365\', \'n02216740\', \'n02214660\', \'n02217563\', \'n02218134\',\n    \'n02218371\', \'n02218713\', \'n02219015\', \'n02207449\', \'n02207805\', \'n02207647\', \'n02223266\', \'n02223520\', \'n02225798\',\n    \'n02224023\', \'n02224713\', \'n02225081\', \'n02226183\', \'n02226821\', \'n02226970\', \'n02227247\', \'n02227604\', \'n02227966\',\n    \'n02228341\', \'n02228697\', \'n02229156\', \'n02229765\', \'n02230023\', \'n02230187\', \'n02230480\', \'n02230634\', \'n02231052\',\n    \'n02231803\', \'n02232223\', \'n02233943\', \'n02234355\', \'n02234570\', \'n02234848\', \'n02235205\', \'n02236241\', \'n02236355\',\n    \'n02236896\', \'n02237424\', \'n02237581\', \'n02237868\', \'n02238235\', \'n02238358\', \'n02238594\', \'n02238887\', \'n02239192\',\n    \'n02239528\', \'n02239774\', \'n02240068\', \'n02240517\', \'n02241008\', \'n02241426\', \'n02241569\', \'n02241799\', \'n02242137\',\n    \'n02242455\', \'n02243209\', \'n02243562\', \'n02243878\', \'n02244173\', \'n02244515\', \'n02244797\', \'n02245111\', \'n02245443\',\n    \'n02246011\', \'n02246628\', \'n02246941\', \'n02247216\', \'n02247511\', \'n02247655\', \'n02248062\', \'n02248368\', \'n02248510\',\n    \'n02248887\', \'n02249134\', \'n02249515\', \'n02249809\', \'n02250280\', \'n02250822\', \'n02251067\', \'n02251233\', \'n02251593\',\n    \'n02251775\', \'n02252226\', \'n02252799\', \'n02252972\', \'n02253127\', \'n02253264\', \'n02253494\', \'n02253715\', \'n02253913\',\n    \'n02254246\', \'n02254697\', \'n02254901\', \'n02255023\', \'n02255391\', \'n02256172\', \'n02257003\', \'n02257284\', \'n02257715\',\n    \'n02257985\', \'n02258198\', \'n02258508\', \'n02258629\', \'n02259377\', \'n02259708\', \'n02259987\', \'n02260421\', \'n02260863\',\n    \'n02261063\', \'n02261419\', \'n02261757\', \'n02262178\', \'n02262449\', \'n02262803\', \'n02263378\', \'n02264021\', \'n02264885\',\n    \'n02265330\', \'n02266050\', \'n02266421\', \'n02266864\', \'n02267208\', \'n02267483\', \'n02268148\', \'n02269196\', \'n02269340\',\n    \'n02270011\', \'n02270200\', \'n02270945\', \'n02270623\', \'n02271222\', \'n02271570\', \'n02271897\', \'n02272286\', \'n02272552\',\n    \'n02272871\', \'n02273392\', \'n02274024\', \'n02274259\', \'n02274822\', \'n02275560\', \'n02275773\', \'n02276078\', \'n02276355\',\n    \'n02276749\', \'n02276902\', \'n02277094\', \'n02277268\', \'n02277422\', \'n02278024\', \'n02278210\', \'n02278463\', \'n02278839\',\n    \'n02278980\', \'n02279257\', \'n02279637\', \'n02280458\', \'n02281015\', \'n02281136\', \'n02281267\', \'n02282257\', \'n02282385\',\n    \'n02282553\', \'n02282903\', \'n02283077\', \'n02283201\', \'n02283617\', \'n02283951\', \'n02284224\', \'n02284611\', \'n02284884\',\n    \'n02285179\', \'n02285548\', \'n02286089\', \'n02286425\', \'n02286654\', \'n02287004\', \'n02287352\', \'n02287622\', \'n02288789\',\n    \'n02289307\', \'n02289610\', \'n02289988\', \'n02290340\', \'n02290664\', \'n02290870\', \'n02291220\', \'n02291572\', \'n02291748\',\n    \'n02292085\', \'n02292401\', \'n02292692\', \'n02293352\', \'n02293868\', \'n02294097\', \'n02294407\', \'n02295064\', \'n02295870\',\n    \'n02296021\', \'n02296276\', \'n02296612\', \'n02297294\', \'n02297819\', \'n02298095\', \'n02298541\', \'n02299039\', \'n02299378\',\n    \'n02299846\', \'n02300173\', \'n02300554\', \'n02301452\', \'n02301935\', \'n02302244\', \'n02302459\', \'n02303585\', \'n02305085\',\n    \'n02302969\', \'n02303284\', \'n02304036\', \'n02304432\', \'n02304657\', \'n02304797\', \'n02305407\', \'n02305636\', \'n02305929\',\n    \'n02306433\', \'n02306825\', \'n02307515\', \'n02307910\', \'n02308471\', \'n02308618\', \'n02307176\', \'n02312427\', \'n02312640\',\n    \'n02312912\', \'n02313008\', \'n02207345\'),\n    # musical instrument n03800933\n    (\n    \'n03800933\', \'n02795978\', \'n02803349\', \'n02803934\', \'n02804123\', \'n02804252\', \'n03301568\', \'n03512030\', \'n02940706\',\n    \'n03279153\', \'n03273551\', \'n04376400\', \'n04419642\', \'n03597916\', \'n03614532\', \'n04376400\', \'n02990758\', \'n03038870\',\n    \'n03039015\', \'n03496296\', \'n04278247\', \'n04537436\', \'n03928116\', \'n02766792\', \'n03086457\', \'n03738066\', \'n04278353\',\n    \'n03801353\', \'n03915437\', \'n03928116\', \'n02766792\', \'n03086457\', \'n03738066\', \'n04278353\', \'n02869249\', \'n02965529\',\n    \'n03483230\', \'n03157348\', \'n03518829\', \'n04614844\', \'n02803666\', \'n02869737\', \'n04249415\', \'n04382334\', \'n04387201\',\n    \'n04387400\', \'n04410086\', \'n04436542\', \'n03440682\', \'n03612965\', \'n03633632\', \'n04049753\', \'n04480853\', \'n04532831\',\n    \'n04338517\', \'n03038870\', \'n03039015\', \'n03496296\', \'n04278247\', \'n04537436\', \'n03928116\', \'n02766792\', \'n03086457\',\n    \'n03738066\', \'n04278353\', \'n02880546\', \'n02803934\', \'n04536153\', \'n04536465\', \'n04536595\', \'n04536765\', \'n04536335\',\n    \'n02700895\', \'n03465500\', \'n04330998\', \'n03025886\', \'n02776978\', \'n02682407\', \'n03699280\', \'n03698360\', \'n03716966\',\n    \'n03716887\', \'n03254862\', \'n03467517\', \'n02804123\', \'n03035832\', \'n03499907\', \'n04506289\', \'n03628215\', \'n04016846\',\n    \'n04132603\', \'n04224842\', \'n04615226\', \'n03254737\', \'n04586932\', \'n02891788\', \'n02804252\', \'n03301568\', \'n03512030\',\n    \'n02793089\', \'n02912894\', \'n04174500\', \'n03369276\', \'n04141198\', \'n04123123\', \'n03393324\', \'n02701730\', \'n03086670\',\n    \'n02786736\', \'n03494537\', \'n03609397\', \'n03854815\', \'n03254625\', \'n04067231\', \'n04542595\', \'n04263950\', \'n04542474\',\n    \'n04067143\', \'n03945615\', \'n02775483\', \'n03800371\', \'n03006626\', \'n03245724\', \'n03343354\', \'n03355468\', \'n03945459\',\n    \'n03912218\', \'n03950647\', \'n03989777\', \'n04579667\', \'n04598582\', \'n02817799\', \'n03228016\', \'n03096439\', \'n04410365\',\n    \'n03288742\', \'n03628831\', \'n03510866\', \'n03800485\', \'n03839172\', \'n03839276\', \'n04186624\', \'n03393199\', \'n04222847\',\n    \'n03037709\', \'n02803539\', \'n02803809\', \'n02834027\', \'n03537550\', \'n03334492\', \'n03831757\', \'n03929091\'),\n    # primate n02469914\n    (\n    \'n02469914\', \'n02496913\', \'n02499808\', \'n02499022\', \'n02498153\', \'n02499568\', \'n02499316\', \'n02498743\', \'n02500596\',\n    \'n02470238\', \'n02470709\', \'n02471300\', \'n02478875\', \'n02479332\', \'n02471762\', \'n02473983\', \'n02478239\', \'n02472293\',\n    \'n02472987\', \'n02474777\', \'n02475358\', \'n02475669\', \'n02473307\', \'n02473720\', \'n02473857\', \'n02475078\', \'n02474605\',\n    \'n02474110\', \'n10528148\', \'n02474282\', \'n02476219\', \'n02476870\', \'n02477187\', \'n02477329\', \'n02477516\', \'n02476567\',\n    \'n02477028\', \'n02477782\', \'n02473554\', \'n02501583\', \'n02502006\', \'n02501923\', \'n02496052\', \'n02470325\', \'n02470899\',\n    \'n02483092\', \'n02480153\', \'n02484322\', \'n02484473\', \'n02485988\', \'n02488894\', \'n02485225\', \'n02485688\', \'n02485371\',\n    \'n02485536\', \'n02486657\', \'n02487079\', \'n02486908\', \'n02487847\', \'n02488003\', \'n02487547\', \'n02487675\', \'n02488415\',\n    \'n02489589\', \'n02494383\', \'n02493224\', \'n02492948\', \'n02492356\', \'n02490597\', \'n02490811\', \'n02491107\'),\n    # reptile n01661091\n    (\n    \'n01661091\', \'n01661592\', \'n01662622\', \'n01662784\', \'n01663401\', \'n01663782\', \'n01664369\', \'n01664492\', \'n01664674\',\n    \'n01664990\', \'n01665932\', \'n01666585\', \'n01666228\', \'n01667432\', \'n01668091\', \'n01668436\', \'n01668665\', \'n01668892\',\n    \'n01669372\', \'n01669654\', \'n01670092\', \'n01670535\', \'n01670802\', \'n01671125\', \'n01671479\', \'n01671705\', \'n01672032\',\n    \'n01672432\', \'n01672611\', \'n01661818\', \'n01673282\', \'n01674216\', \'n01674464\', \'n01674990\', \'n01675352\', \'n01676755\',\n    \'n01677747\', \'n01678043\', \'n01678343\', \'n01678657\', \'n01679005\', \'n01679307\', \'n01679626\', \'n01679962\', \'n01680264\',\n    \'n01680478\', \'n01680655\', \'n01680813\', \'n01680983\', \'n01681328\', \'n01681653\', \'n01681940\', \'n01682172\', \'n01682435\',\n    \'n01683201\', \'n01683558\', \'n01684133\', \'n01684578\', \'n01684741\', \'n01685439\', \'n01686044\', \'n01686220\', \'n01686403\',\n    \'n01686609\', \'n01686808\', \'n01687128\', \'n01687290\', \'n01687665\', \'n01688961\', \'n01689081\', \'n01689411\', \'n01690149\',\n    \'n01690466\', \'n01691217\', \'n01691652\', \'n01691951\', \'n01692523\', \'n01692864\', \'n01693175\', \'n01693783\', \'n01694311\',\n    \'n01694709\', \'n01694955\', \'n01701551\', \'n01701859\', \'n01702256\', \'n01702479\', \'n01703011\', \'n01703161\', \'n01703569\',\n    \'n01704103\', \'n01704626\', \'n01705010\', \'n01705591\', \'n01705934\', \'n01707294\', \'n01708106\', \'n01708998\', \'n01709484\',\n    \'n01709876\', \'n01712008\', \'n01712752\', \'n01713170\', \'n01713764\', \'n01714231\', \'n01715888\', \'n01717016\', \'n01717229\',\n    \'n01717467\', \'n01718096\', \'n01718414\', \'n01710177\', \'n01711160\', \'n01722998\', \'n01723579\', \'n01724231\', \'n01724840\',\n    \'n01725086\', \'n01725713\', \'n01726203\', \'n01696633\', \'n01698434\', \'n01698782\', \'n01697178\', \'n01697611\', \'n01697749\',\n    \'n01697978\', \'n01699040\', \'n01699254\', \'n01699675\', \'n01726692\', \'n01727646\', \'n01728266\', \'n01729672\', \'n01730185\',\n    \'n01730307\', \'n01730563\', \'n01730812\', \'n01730960\', \'n01731137\', \'n01731277\', \'n01731545\', \'n01731764\', \'n01731941\',\n    \'n01732093\', \'n01732244\', \'n01732614\', \'n01732789\', \'n01732989\', \'n01733214\', \'n01733466\', \'n01733757\', \'n01733957\',\n    \'n01734104\', \'n01734637\', \'n01734808\', \'n01735439\', \'n01735577\', \'n01735728\', \'n01736032\', \'n01736375\', \'n01736796\',\n    \'n01737472\', \'n01737728\', \'n01737875\', \'n01738065\', \'n01738306\', \'n01738601\', \'n01738731\', \'n01739094\', \'n01739647\',\n    \'n01739871\', \'n01741232\', \'n01741442\', \'n01740551\', \'n01740885\', \'n01741562\', \'n01741943\', \'n01742447\', \'n01742821\',\n    \'n01743086\', \'n01743605\', \'n01743936\', \'n01744100\', \'n01744270\', \'n01744555\', \'n01745125\', \'n01745484\', \'n01745902\',\n    \'n01746191\', \'n01746359\', \'n01746952\', \'n01747285\', \'n01747589\', \'n01747885\', \'n01748389\', \'n01748686\', \'n01748906\',\n    \'n01749244\', \'n01749582\', \'n01749742\', \'n01750167\', \'n01750437\', \'n01750743\', \'n01751036\', \'n01751215\', \'n01751472\',\n    \'n01752165\', \'n01752585\', \'n01752736\', \'n01753032\', \'n01753180\', \'n01753959\', \'n01754370\', \'n01754533\', \'n01754876\',\n    \'n01755740\', \'n01755952\', \'n01756089\', \'n01756508\', \'n01756733\', \'n01756916\', \'n01757115\', \'n01757343\', \'n01757677\',\n    \'n01757901\', \'n01758141\', \'n01662060\', \'n01719403\', \'n01721174\', \'n01721898\', \'n01722670\'),\n    # utensil n04516672\n    (\n    \'n04516672\', \'n02997607\', \'n03262519\', \'n03173270\', \'n03317788\', \'n03713436\', \'n04414101\', \'n04414319\', \'n03984234\',\n    \'n03018209\', \'n02869155\', \'n03125588\', \'n04282992\', \'n03992703\', \'n02684248\', \'n03698226\', \'n04570214\', \'n04326676\',\n    \'n03104512\', \'n03403643\', \'n03621049\', \'n03012499\', \'n03101517\', \'n03101986\', \'n02805283\', \'n02999138\', \'n03101156\',\n    \'n03983712\', \'n03101796\', \'n03284981\', \'n03047799\', \'n03453231\', \'n03458422\', \'n03459328\', \'n03880531\', \'n03242390\',\n    \'n03271765\', \'n04275283\', \'n03846677\', \'n03900301\', \'n04097760\', \'n04138977\', \'n03226254\', \'n04317325\', \'n03972372\',\n    \'n03990474\', \'n03242506\', \'n03915118\', \'n03216562\', \'n03259401\', \'n03612814\', \'n04397768\', \'n03992975\', \'n04139140\',\n    \'n04324297\', \'n04516214\', \'n03064250\', \'n04132985\', \'n04399158\', \'n04229959\', \'n04309548\', \'n04500060\', \'n03352961\',\n    \'n03881305\', \'n03454885\', \'n03621377\', \'n03724417\', \'n03767966\', \'n03775199\', \'n02850732\', \'n03266371\', \'n03272940\',\n    \'n04578934\', \'n04088441\', \'n04103206\', \'n04293119\', \'n04059516\', \'n04396902\', \'n04175039\'),\n    # vegetable n07707451\n    (\n    \'n07707451\', \'n07708124\', \'n07708398\', \'n07708798\', \'n07709046\', \'n07724943\', \'n07725158\', \'n07726796\', \'n07727048\',\n    \'n07727140\', \'n07727252\', \'n07727377\', \'n07727458\', \'n07727578\', \'n07727868\', \'n07728053\', \'n07728181\', \'n07728284\',\n    \'n07728391\', \'n07728585\', \'n07728708\', \'n07728804\', \'n07729000\', \'n07729142\', \'n07729225\', \'n07729384\', \'n07729828\',\n    \'n07727741\', \'n07729485\', \'n07729926\', \'n07725255\', \'n07725376\', \'n07725531\', \'n07725789\', \'n07725888\', \'n07726009\',\n    \'n07725663\', \'n07726230\', \'n07726386\', \'n07726095\', \'n07726672\', \'n07709172\', \'n07709333\', \'n07709701\', \'n07719437\',\n    \'n07719616\', \'n07719756\', \'n07719980\', \'n07720277\', \'n07723330\', \'n07723559\', \'n07723753\', \'n07723968\', \'n07724078\',\n    \'n07724173\', \'n07724269\', \'n07724492\', \'n07724654\', \'n07724819\', \'n07730855\', \'n07731006\', \'n07731587\', \'n07731767\',\n    \'n07732747\', \'n07732904\', \'n07733005\', \'n07733124\', \'n07820036\', \'n07733217\', \'n07733712\', \'n07733847\', \'n07736256\',\n    \'n07736371\', \'n07736527\', \'n07736692\', \'n07710007\', \'n07710616\', \'n07710952\', \'n07711371\', \'n07711080\', \'n07711232\',\n    \'n07711799\', \'n07713074\', \'n07720442\', \'n07720615\', \'n07721018\', \'n07721118\', \'n07721195\', \'n07721325\', \'n07722052\',\n    \'n07721456\', \'n07721678\', \'n07721833\', \'n07721942\', \'n07734017\', \'n07734183\', \'n07734292\', \'n07734417\', \'n07734555\',\n    \'n07710283\', \'n07710616\', \'n07710952\', \'n07711371\', \'n07711907\', \'n07712063\', \'n07712267\', \'n07719058\', \'n07719839\',\n    \'n07720084\', \'n07720185\', \'n07730207\', \'n07730708\', \'n07735052\', \'n07735179\', \'n07735294\', \'n07735404\', \'n07735687\',\n    \'n07735803\', \'n07735981\', \'n07736087\', \'n07736813\', \'n07713267\', \'n07713395\', \'n07735687\', \'n07713763\', \'n07713895\',\n    \'n07714078\', \'n07714188\', \'n07714287\', \'n07714448\', \'n07714895\', \'n07714802\', \'n07715221\', \'n07715407\', \'n07733567\',\n    \'n07715561\', \'n07717070\', \'n07717714\', \'n07717858\', \'n07718068\', \'n07718195\', \'n07718329\', \'n07715721\', \'n07716034\',\n    \'n07716203\', \'n07716504\', \'n07716649\', \'n07716750\', \'n07718671\', \'n07718920\', \'n07719213\', \'n07719330\', \'n07722217\',\n    \'n07722390\', \'n07722485\', \'n07722666\', \'n07722763\', \'n07722888\', \'n07723177\', \'n07723039\', \'n07730406\', \'n07730562\',\n    \'n07733394\', \'n07735510\', \'n07736971\', \'n07768423\', \'n07817871\'),\n    # vehicle n04576211\n    (\n    \'n04576211\', \'n02766534\', \'n02804515\', \'n02834778\', \'n03853924\', \'n04026813\', \'n04126066\', \'n04524716\', \'n02869563\',\n    \'n02959942\', \'n02775039\', \'n02932523\', \'n03053976\', \'n02885108\', \'n04322692\', \'n02986066\', \'n03056097\', \'n03360731\',\n    \'n04070964\', \'n04389521\', \'n03465320\', \'n03483971\', \'n03710294\', \'n03200357\', \'n03828020\', \'n04020912\', \'n04236001\',\n    \'n04246855\', \'n04409279\', \'n03484083\', \'n02729222\', \'n03490119\', \'n03648431\', \'n04176068\', \'n04397027\', \'n03897634\',\n    \'n03538634\', \'n02968473\', \'n02794474\', \'n02907296\', \'n02909706\', \'n02912557\', \'n02931013\', \'n02966068\', \'n03002555\',\n    \'n03009111\', \'n03037590\', \'n03055670\', \'n04297098\', \'n03247351\', \'n03435991\', \'n03436656\', \'n03389889\', \'n03492087\',\n    \'n03638014\', \'n03989199\', \'n04302863\', \'n04365112\', \'n04486616\', \'n03009269\', \'n03669245\', \'n04353573\', \'n04103364\',\n    \'n04149374\', \'n04170037\', \'n03919096\', \'n04062807\', \'n04566561\', \'n02740533\', \'n03886053\', \'n02739889\', \'n02740061\',\n    \'n02740300\', \'n02749292\', \'n04389718\', \'n03684823\', \'n03025165\', \'n03193597\', \'n03193260\', \'n03193423\', \'n03585778\',\n    \'n03939565\', \'n04211219\', \'n04373428\', \'n04389854\', \'n04465358\', \'n03791235\', \'n04368695\', \'n02854630\', \'n02958343\',\n    \'n03404012\', \'n04201733\', \'n03472937\', \'n03079136\', \'n03119396\', \'n03141065\', \'n03881534\', \'n03268790\', \'n03421669\',\n    \'n03493219\', \'n03498781\', \'n03539103\', \'n03543394\', \'n02831335\', \'n03680512\', \'n03770085\', \'n03870105\', \'n04322801\',\n    \'n03342961\', \'n04097373\', \'n04166281\', \'n04285965\', \'n04302988\', \'n04347119\', \'n04459122\', \'n04516354\', \'n03389761\',\n    \'n03506880\', \'n03790512\', \'n03769722\', \'n04466871\', \'n04490091\', \'n03256166\', \'n03632852\', \'n03690473\', \'n04465666\',\n    \'n04474035\', \'n04520170\', \'n02871314\', \'n03173929\', \'n03648667\', \'n03764822\', \'n03884639\', \'n03896419\', \'n02946348\',\n    \'n04520382\', \'n03256788\', \'n03538300\', \'n04464852\', \'n03886053\', \'n02983507\', \'n04250599\', \'n04229007\', \'n02916179\',\n    \'n02712643\', \'n04225987\', \'n04467099\', \'n02946509\', \'n03904433\', \'n04543158\', \'n02787120\', \'n02970849\', \'n03217739\',\n    \'n03255899\', \'n04497249\', \'n03235979\', \'n03594010\', \'n03981924\', \'n04558059\', \'n04560502\', \'n03027505\', \'n03122295\',\n    \'n03765467\', \'n04543924\', \'n04563020\', \'n04543509\', \'n04571800\')\n)\n\nnum_classes = 25\n\n# mean and standard deviation of channels of ImageNet images\nmean = [0.485, 0.456, 0.406]\nstd = [0.229, 0.224, 0.225]\n\ntrain_transform = trn.Compose([trn.Resize(256), trn.RandomCrop(224), trn.RandomHorizontalFlip(),\n                               trn.ToTensor(), trn.Normalize(mean, std)])\ntest_transform = trn.Compose([trn.Resize(256), trn.CenterCrop(224), trn.ToTensor(), trn.Normalize(mean, std)])\n\ntrain_data = dset.ImageFolder(\n    root=""/share/data/vision-greg/ImageNet/clsloc/images/train"",\n    transform=train_transform)\ntest_data_in = dset.ImageFolder(\n    root=""/share/data/vision-greg/ImageNet/clsloc/images/val"",\n    transform=test_transform)\n\n\n# print(\'Loading ImageNet-22K\')\n# # some ImageNet-22K images are broken JPEGs\n#\n#\n# def my_collate(batch):\n#     batch = list(filter(lambda x: x is not None, batch))\n#     return torch.utils.data.dataloader.default_collate(batch)\n#\n#\n# class MyImageFolder(dset.ImageFolder):\n#     __init__ = dset.ImageFolder.__init__\n#\n#     def __getitem__(self, index):\n#         try:\n#             return super(MyImageFolder, self).__getitem__(index)\n#         except Exception as e:\n#             print(e)\n#\n#\n# test_data_out = MyImageFolder(root=""/share/data/vision-greg/ImageNet/clsloc/images/val"",\n#                               transform=trn.Compose([trn.Resize(256), trn.CenterCrop(224),\n#                                                      trn.ToTensor(), trn.Normalize(mean, std)]))\n# test_data_out.root = \'/share/data/vision-greg/ImageNet22k\'\n# test_data_out.class_to_idx = pickle.load(open(test_data_out.root + \'/class_to_idx.p\', ""rb""))\n# test_data_out.classes = pickle.load(open(test_data_out.root + \'/classes.p\', ""rb""))\n# test_data_out.imgs = pickle.load(open(test_data_out.root + \'/imgs.p\', ""rb""))\n# print(\'Loaded ImageNet-22K\')\n\n\nprint(\'Filtering Data\')\nim1k_classes_flat = tuple(np.concatenate(im1k_classes))\ntrain_data.imgs = [x for x in train_data.imgs if\n                   x[0][x[0].index(\'train/\') + 6:x[0].index(\'train/\') + 6 + len(\'n07718472\')] in im1k_classes_flat]\ntest_data_in.imgs = [x for x in test_data_in.imgs if\n                     x[0][x[0].index(\'val/\') + 4:x[0].index(\'val/\') + 4 + len(\'n07718472\')] in im1k_classes_flat]\n\nin_idx_to_superidx = {}\nfor c in im1k_classes_flat:\n    for i in range(len(im1k_classes)):      # range(num_classes)\n        t = im1k_classes[i]\n        if c in t:\n            in_idx_to_superidx[train_data.class_to_idx[c]] = i\n            break\ntrain_data.target_transform = lambda x: in_idx_to_superidx[x]\ntest_data_in.target_transform = lambda x: in_idx_to_superidx[x]\n\n# im22k_classes_flat = tuple(np.concatenate(im22k_classes))\n# test_data_out.imgs = [x for x in test_data_out.imgs if\n#                       x[0][x[0].index(\'22k/images/\') + 11:x[0].index(\'22k/images/\') + 11 + len(\'n07718472\')]\n#                       in im22k_classes_flat]\n#\n# out_idx_to_superidx = {}\n# for c in im22k_classes_flat:\n#     for i in range(len(im22k_classes)):     # range(num_classes)\n#         t = im22k_classes[i]\n#         if c in t:\n#             out_idx_to_superidx[test_data_out.class_to_idx[c]] = i\n#             break\n# test_data_out.target_transform = lambda x: out_idx_to_superidx[x]\n\ntrain_loader = torch.utils.data.DataLoader(\n    train_data, batch_size=args.batch_size, shuffle=True,\n    num_workers=args.prefetch, pin_memory=True)\ntest_loader_in = torch.utils.data.DataLoader(\n    test_data_in, batch_size=args.test_bs, shuffle=False,\n    num_workers=args.prefetch, pin_memory=True)\n# test_loader_out = torch.utils.data.DataLoader(\n#     test_data_out, batch_size=args.test_bs, shuffle=False,\n#     num_workers=args.prefetch, pin_memory=True, collate_fn=my_collate)\n\n\nprint(\'Initializing Network\')\n\n\nclass FineTuneModel(nn.Module):\n    """"""\n    This freezes the weights of all layers except the last one.\n\n    Arguments:\n        original_model: Model to finetune\n        arch: Name of model architecture\n        num_classes: Number of classes to tune for\n    """"""\n\n    def __init__(self, original_model, arch, num_classes):\n        super(FineTuneModel, self).__init__()\n\n        if arch.startswith(\'alexnet\') or arch.startswith(\'vgg\'):\n            self.features = original_model.features\n            self.fc = nn.Sequential(*list(original_model.classifier.children())[:-1])\n            self.classifier = nn.Sequential(\n                nn.Linear(4096, num_classes)\n            )\n        elif arch.startswith(\'resnet\') or arch.startswith(\'resnext\'):\n            # Everything except the last linear layer\n            self.features = nn.Sequential(*list(original_model.children())[:-1])\n            if arch == \'resnet18\':\n                self.classifier = nn.Sequential(\n                    nn.Linear(512, num_classes)\n                )\n            else:\n                self.classifier = nn.Sequential(\n                    nn.Linear(2048, num_classes)\n                )\n        else:\n            raise (""Finetuning not supported on this architecture yet. Feel free to add"")\n\n        self.unfreeze(False)  # Freeze weights except last layer\n\n    def unfreeze(self, unfreeze):\n        # Freeze those weights\n        for p in self.features.parameters():\n            p.requires_grad = unfreeze\n        if hasattr(self, \'fc\'):\n            for p in self.fc.parameters():\n                p.requires_grad = unfreeze\n\n    def forward(self, x):\n        f = self.features(x)\n        if hasattr(self, \'fc\'):\n            f = f.view(f.size(0), -1)\n            f = self.fc(f)\n        f = f.view(f.size(0), -1)\n        y = self.classifier(f)\n        return y\n\n\nif args.model_name == \'alexnet\':\n    net = models.AlexNet()\n    net.load_state_dict(model_zoo.load_url(\'https://download.pytorch.org/models/alexnet-owt-4df8aa71.pth\',\n                                           model_dir=\'/share/data/lang/users/dan/.torch/models\'))\n    net = FineTuneModel(net, \'alexnet\', num_classes)\n\nelif args.model_name == \'squeezenet1.1\':\n    net = models.SqueezeNet(version=1.1)\n    net.load_state_dict(model_zoo.load_url(\'https://download.pytorch.org/models/squeezenet1_1-f364aa15.pth\',\n                                           model_dir=\'/share/data/lang/users/dan/.torch/models\'))\n\n    net.classifier = nn.Sequential(\n        nn.Dropout(p=0.5),\n        nn.Conv2d(512, num_classes, kernel_size=1),\n        nn.ReLU(inplace=True),\n        nn.AvgPool2d(13)\n    )\n    net.forward = lambda x: net.classifier(net.features(x)).view(x.size(0), num_classes)\n\n    for p in net.features.parameters():\n        p.requires_grad = False\n\nelif \'vgg\' in args.model_name:\n    if \'bn\' not in args.model_name:\n        net = models.vgg19()\n        net.load_state_dict(model_zoo.load_url(\'https://download.pytorch.org/models/vgg19-dcbb9e9d.pth\',\n                                               model_dir=\'/share/data/lang/users/dan/.torch/models\'))\n    elif \'11\' in args.model_name:\n        net = models.vgg11()\n        net.load_state_dict(model_zoo.load_url(\'https://download.pytorch.org/models/vgg11-bbd30ac9.pth\',\n                                               model_dir=\'/share/data/lang/users/dan/.torch/models\'))\n    else:\n        net = models.vgg19_bn()\n        net.load_state_dict(model_zoo.load_url(\'https://download.pytorch.org/models/vgg19_bn-c79401a0.pth\',\n                                               model_dir=\'/share/data/lang/users/dan/.torch/models\'))\n    net = FineTuneModel(net, \'vgg\', num_classes)\n\nelif args.model_name == \'resnet18\':\n    net = models.resnet18()\n    net.load_state_dict(model_zoo.load_url(\'https://download.pytorch.org/models/resnet18-5c106cde.pth\',\n                                           model_dir=\'/share/data/lang/users/dan/.torch/models\'))\n    net = FineTuneModel(net, \'resnet18\', num_classes)\n\nelif args.model_name == \'resnet50\':\n    net = models.resnet50()\n    net.load_state_dict(model_zoo.load_url(\'https://download.pytorch.org/models/resnet50-19c8e357.pth\',\n                                           model_dir=\'/share/data/lang/users/dan/.torch/models\'))\n    net = FineTuneModel(net, \'resnet50\', num_classes)\n\nelif args.model_name == \'resnext\':\n    net = resnext_101_64x4d\n    net.load_state_dict(torch.load(\'/share/data/lang/users/dan/.torch/models/resnext_101_64x4d.pth\'))\n    net = FineTuneModel(net, \'resnext\', num_classes)\n\nprint(args.model_name)\nprint(net)\n\noptimizer = torch.optim.SGD(filter(lambda p: p.requires_grad, net.parameters()),\n                            state[\'learning_rate\'], momentum=state[\'momentum\'],\n                            weight_decay=state[\'decay\'], nesterov=True)\n\nstart_epoch = 0\n# # Restore model\n# if args.load != \'\':\n#     for i in range(1000 - 1, -1, -1):\n#         model_name = os.path.join(args.load, \'superclass_model_epoch\' + str(i) + \'.pth\')\n#         if os.path.isfile(model_name):\n#             snapshot = torch.load(model_name)\n#             net.load_state_dict(checkpoint[\'state_dict\'])\n#             optimizer.load_state_dict(checkpoint[\'optimizer\'])\n#             print(\'Model restored! Epoch:\', i)\n#             start_epoch = i + 1\n#             break\n#     if start_epoch == 0:\n#         assert False, ""could not resume""\n\n\nif args.ngpu > 1:\n    net = torch.nn.DataParallel(net, device_ids=list(range(args.ngpu)))\n\nif args.ngpu > 0:\n    net.cuda()\n    torch.cuda.manual_seed(1)\n\ncudnn.benchmark = True  # fire on all cylinders\n\nfrom tqdm import tqdm\n\ndef train():\n    net.train()  # enter train mode\n    loss_avg = 0.0\n    for batch_idx, (data, target) in enumerate(train_loader):\n        data, target = V(data.cuda()), V(target.cuda())\n\n        # forward\n        x = net(data)\n\n        # backward\n        optimizer.zero_grad()\n        loss = F.cross_entropy(x, target)\n        loss.backward()\n        optimizer.step()\n\n        # exponential moving average\n        loss_avg = loss_avg * 0.8 + loss.data[0] * 0.2\n\n        dt = math.pi / float(args.epochs)\n        state[\'tt\'] += float(dt) / (len(train_loader.dataset) / float(args.batch_size))\n        if state[\'tt\'] >= math.pi - 0.01:\n            state[\'tt\'] = math.pi - 0.01\n        curT = math.pi / 2.0 + state[\'tt\']\n        new_lr = args.learning_rate * (1.0 + math.sin(curT)) / 2.0  # lr_min = 0, lr_max = lr\n        state[\'learning_rate\'] = new_lr\n        for param_group in optimizer.param_groups:\n            param_group[\'lr\'] = state[\'learning_rate\']\n\n    state[\'train_loss\'] = loss_avg\n\n\n# test function\ndef test_in():\n    net.eval()\n    loss_avg = 0.0\n    correct = 0\n    for batch_idx, (data, target) in enumerate(test_loader_in):\n        data, target = V(data.cuda(), volatile=True), V(target.cuda(), volatile=True)\n\n        # forward\n        output = net(data)\n        loss = F.cross_entropy(output, target)\n\n        # accuracy\n        pred = output.data.max(1)[1]\n        correct += pred.eq(target.data).sum()\n\n        # test loss average\n        loss_avg += loss.data[0]\n\n    state[\'test_in_loss\'] = loss_avg / len(test_loader_in)\n    state[\'test_in_accuracy\'] = correct / len(test_loader_in.dataset)\n\n\n# # test function\n# def test_out():\n#     net.eval()\n#     correct = 0\n#     for batch_idx, (data, target) in enumerate(test_loader_out):\n#         data, target = V(data.cuda(), volatile=True), V(target.cuda(), volatile=True)\n#\n#         # forward\n#         output = net(data)\n#\n#         # accuracy\n#         pred = output.data.max(1)[1]\n#         correct += pred.eq(target.data).sum()\n#\n#     state[\'test_out_accuracy\'] = correct / len(test_loader_out.dataset)\n\n\nif args.test:\n    test_in()\n    # test_out()\n    print(state)\n    exit()\n\nstate[\'learning_rate\'] = state[\'init_learning_rate\']\n\nprint(\'Beginning Training\')\n# Main loop\nbest_accuracy = 0.0\nfor epoch in range(start_epoch, args.epochs):\n    for param_group in optimizer.param_groups:\n        param_group[\'lr\'] = state[\'learning_rate\']\n    state[\'tt\'] = math.pi / float(args.epochs) * epoch\n\n    state[\'epoch\'] = epoch\n\n    begin_epoch = time.time()\n    train()\n    print(\'Epoch\', epoch, \'| Time Spent:\', round(time.time() - begin_epoch, 4))\n\n    torch.save({\n            \'state_dict\': net.state_dict(),\n            \'optimizer\': optimizer.state_dict()\n    }, os.path.join(args.save, args.model_name + \'_superclass_epoch\' + str(epoch) + \'.pth\'))\n    # Let us not waste space and delete the previous model\n    # We do not overwrite the model because we need the epoch number\n    try: os.remove(os.path.join(args.save, args.model_name + \'_superclass_epoch\' + str(epoch - 1) + \'.pth\'))\n    except: True\n\n    test_in()\n    # test_out()\n\n    print(state)\n'"
