file_path,api_count,code
attention.py,3,"b'import torch\nimport torch.nn as nn\nfrom torch.nn.utils.weight_norm import weight_norm\nfrom fc import FCNet\n\n\nclass Attention(nn.Module):\n    def __init__(self, v_dim, q_dim, num_hid):\n        super(Attention, self).__init__()\n        self.nonlinear = FCNet([v_dim + q_dim, num_hid])\n        self.linear = weight_norm(nn.Linear(num_hid, 1), dim=None)\n\n    def forward(self, v, q):\n        """"""\n        v: [batch, k, vdim]\n        q: [batch, qdim]\n        """"""\n        logits = self.logits(v, q)\n        w = nn.functional.softmax(logits, 1)\n        return w\n\n    def logits(self, v, q):\n        num_objs = v.size(1)\n        q = q.unsqueeze(1).repeat(1, num_objs, 1)\n        vq = torch.cat((v, q), 2)\n        joint_repr = self.nonlinear(vq)\n        logits = self.linear(joint_repr)\n        return logits\n\n\nclass NewAttention(nn.Module):\n    def __init__(self, v_dim, q_dim, num_hid, dropout=0.2):\n        super(NewAttention, self).__init__()\n\n        self.v_proj = FCNet([v_dim, num_hid])\n        self.q_proj = FCNet([q_dim, num_hid])\n        self.dropout = nn.Dropout(dropout)\n        self.linear = weight_norm(nn.Linear(q_dim, 1), dim=None)\n\n    def forward(self, v, q):\n        """"""\n        v: [batch, k, vdim]\n        q: [batch, qdim]\n        """"""\n        logits = self.logits(v, q)\n        w = nn.functional.softmax(logits, 1)\n        return w\n\n    def logits(self, v, q):\n        batch, k, _ = v.size()\n        v_proj = self.v_proj(v) # [batch, k, qdim]\n        q_proj = self.q_proj(q).unsqueeze(1).repeat(1, k, 1)\n        joint_repr = v_proj * q_proj\n        joint_repr = self.dropout(joint_repr)\n        logits = self.linear(joint_repr)\n        return logits\n'"
base_model.py,1,"b'import torch\nimport torch.nn as nn\nfrom attention import Attention, NewAttention\nfrom language_model import WordEmbedding, QuestionEmbedding\nfrom classifier import SimpleClassifier\nfrom fc import FCNet\n\n\nclass BaseModel(nn.Module):\n    def __init__(self, w_emb, q_emb, v_att, q_net, v_net, classifier):\n        super(BaseModel, self).__init__()\n        self.w_emb = w_emb\n        self.q_emb = q_emb\n        self.v_att = v_att\n        self.q_net = q_net\n        self.v_net = v_net\n        self.classifier = classifier\n\n    def forward(self, v, b, q, labels):\n        """"""Forward\n\n        v: [batch, num_objs, obj_dim]\n        b: [batch, num_objs, b_dim]\n        q: [batch_size, seq_length]\n\n        return: logits, not probs\n        """"""\n        w_emb = self.w_emb(q)\n        q_emb = self.q_emb(w_emb) # [batch, q_dim]\n\n        att = self.v_att(v, q_emb)\n        v_emb = (att * v).sum(1) # [batch, v_dim]\n\n        q_repr = self.q_net(q_emb)\n        v_repr = self.v_net(v_emb)\n        joint_repr = q_repr * v_repr\n        logits = self.classifier(joint_repr)\n        return logits\n\n\ndef build_baseline0(dataset, num_hid):\n    w_emb = WordEmbedding(dataset.dictionary.ntoken, 300, 0.0)\n    q_emb = QuestionEmbedding(300, num_hid, 1, False, 0.0)\n    v_att = Attention(dataset.v_dim, q_emb.num_hid, num_hid)\n    q_net = FCNet([num_hid, num_hid])\n    v_net = FCNet([dataset.v_dim, num_hid])\n    classifier = SimpleClassifier(\n        num_hid, 2 * num_hid, dataset.num_ans_candidates, 0.5)\n    return BaseModel(w_emb, q_emb, v_att, q_net, v_net, classifier)\n\n\ndef build_baseline0_newatt(dataset, num_hid):\n    w_emb = WordEmbedding(dataset.dictionary.ntoken, 300, 0.0)\n    q_emb = QuestionEmbedding(300, num_hid, 1, False, 0.0)\n    v_att = NewAttention(dataset.v_dim, q_emb.num_hid, num_hid)\n    q_net = FCNet([q_emb.num_hid, num_hid])\n    v_net = FCNet([dataset.v_dim, num_hid])\n    classifier = SimpleClassifier(\n        num_hid, num_hid * 2, dataset.num_ans_candidates, 0.5)\n    return BaseModel(w_emb, q_emb, v_att, q_net, v_net, classifier)\n'"
classifier.py,2,"b'import torch.nn as nn\nfrom torch.nn.utils.weight_norm import weight_norm\n\n\nclass SimpleClassifier(nn.Module):\n    def __init__(self, in_dim, hid_dim, out_dim, dropout):\n        super(SimpleClassifier, self).__init__()\n        layers = [\n            weight_norm(nn.Linear(in_dim, hid_dim), dim=None),\n            nn.ReLU(),\n            nn.Dropout(dropout, inplace=True),\n            weight_norm(nn.Linear(hid_dim, out_dim), dim=None)\n        ]\n        self.main = nn.Sequential(*layers)\n\n    def forward(self, x):\n        logits = self.main(x)\n        return logits\n'"
dataset.py,7,"b'from __future__ import print_function\nimport os\nimport json\nimport cPickle\nimport numpy as np\nimport utils\nimport h5py\nimport torch\nfrom torch.utils.data import Dataset\n\n\nclass Dictionary(object):\n    def __init__(self, word2idx=None, idx2word=None):\n        if word2idx is None:\n            word2idx = {}\n        if idx2word is None:\n            idx2word = []\n        self.word2idx = word2idx\n        self.idx2word = idx2word\n\n    @property\n    def ntoken(self):\n        return len(self.word2idx)\n\n    @property\n    def padding_idx(self):\n        return len(self.word2idx)\n\n    def tokenize(self, sentence, add_word):\n        sentence = sentence.lower()\n        sentence = sentence.replace(\',\', \'\').replace(\'?\', \'\').replace(\'\\\'s\', \' \\\'s\')\n        words = sentence.split()\n        tokens = []\n        if add_word:\n            for w in words:\n                tokens.append(self.add_word(w))\n        else:\n            for w in words:\n                tokens.append(self.word2idx[w])\n        return tokens\n\n    def dump_to_file(self, path):\n        cPickle.dump([self.word2idx, self.idx2word], open(path, \'wb\'))\n        print(\'dictionary dumped to %s\' % path)\n\n    @classmethod\n    def load_from_file(cls, path):\n        print(\'loading dictionary from %s\' % path)\n        word2idx, idx2word = cPickle.load(open(path, \'rb\'))\n        d = cls(word2idx, idx2word)\n        return d\n\n    def add_word(self, word):\n        if word not in self.word2idx:\n            self.idx2word.append(word)\n            self.word2idx[word] = len(self.idx2word) - 1\n        return self.word2idx[word]\n\n    def __len__(self):\n        return len(self.idx2word)\n\n\ndef _create_entry(img, question, answer):\n    answer.pop(\'image_id\')\n    answer.pop(\'question_id\')\n    entry = {\n        \'question_id\' : question[\'question_id\'],\n        \'image_id\'    : question[\'image_id\'],\n        \'image\'       : img,\n        \'question\'    : question[\'question\'],\n        \'answer\'      : answer}\n    return entry\n\n\ndef _load_dataset(dataroot, name, img_id2val):\n    """"""Load entries\n\n    img_id2val: dict {img_id -> val} val can be used to retrieve image or features\n    dataroot: root path of dataset\n    name: \'train\', \'val\'\n    """"""\n    question_path = os.path.join(\n        dataroot, \'v2_OpenEnded_mscoco_%s2014_questions.json\' % name)\n    questions = sorted(json.load(open(question_path))[\'questions\'],\n                       key=lambda x: x[\'question_id\'])\n    answer_path = os.path.join(dataroot, \'cache\', \'%s_target.pkl\' % name)\n    answers = cPickle.load(open(answer_path, \'rb\'))\n    answers = sorted(answers, key=lambda x: x[\'question_id\'])\n\n    utils.assert_eq(len(questions), len(answers))\n    entries = []\n    for question, answer in zip(questions, answers):\n        utils.assert_eq(question[\'question_id\'], answer[\'question_id\'])\n        utils.assert_eq(question[\'image_id\'], answer[\'image_id\'])\n        img_id = question[\'image_id\']\n        entries.append(_create_entry(img_id2val[img_id], question, answer))\n\n    return entries\n\n\nclass VQAFeatureDataset(Dataset):\n    def __init__(self, name, dictionary, dataroot=\'data\'):\n        super(VQAFeatureDataset, self).__init__()\n        assert name in [\'train\', \'val\']\n\n        ans2label_path = os.path.join(dataroot, \'cache\', \'trainval_ans2label.pkl\')\n        label2ans_path = os.path.join(dataroot, \'cache\', \'trainval_label2ans.pkl\')\n        self.ans2label = cPickle.load(open(ans2label_path, \'rb\'))\n        self.label2ans = cPickle.load(open(label2ans_path, \'rb\'))\n        self.num_ans_candidates = len(self.ans2label)\n\n        self.dictionary = dictionary\n\n        self.img_id2idx = cPickle.load(\n            open(os.path.join(dataroot, \'%s36_imgid2idx.pkl\' % name)))\n        print(\'loading features from h5 file\')\n        h5_path = os.path.join(dataroot, \'%s36.hdf5\' % name)\n        with h5py.File(h5_path, \'r\') as hf:\n            self.features = np.array(hf.get(\'image_features\'))\n            self.spatials = np.array(hf.get(\'spatial_features\'))\n\n        self.entries = _load_dataset(dataroot, name, self.img_id2idx)\n\n        self.tokenize()\n        self.tensorize()\n        self.v_dim = self.features.size(2)\n        self.s_dim = self.spatials.size(2)\n\n    def tokenize(self, max_length=14):\n        """"""Tokenizes the questions.\n\n        This will add q_token in each entry of the dataset.\n        -1 represent nil, and should be treated as padding_idx in embedding\n        """"""\n        for entry in self.entries:\n            tokens = self.dictionary.tokenize(entry[\'question\'], False)\n            tokens = tokens[:max_length]\n            if len(tokens) < max_length:\n                # Note here we pad in front of the sentence\n                padding = [self.dictionary.padding_idx] * (max_length - len(tokens))\n                tokens = padding + tokens\n            utils.assert_eq(len(tokens), max_length)\n            entry[\'q_token\'] = tokens\n\n    def tensorize(self):\n        self.features = torch.from_numpy(self.features)\n        self.spatials = torch.from_numpy(self.spatials)\n\n        for entry in self.entries:\n            question = torch.from_numpy(np.array(entry[\'q_token\']))\n            entry[\'q_token\'] = question\n\n            answer = entry[\'answer\']\n            labels = np.array(answer[\'labels\'])\n            scores = np.array(answer[\'scores\'], dtype=np.float32)\n            if len(labels):\n                labels = torch.from_numpy(labels)\n                scores = torch.from_numpy(scores)\n                entry[\'answer\'][\'labels\'] = labels\n                entry[\'answer\'][\'scores\'] = scores\n            else:\n                entry[\'answer\'][\'labels\'] = None\n                entry[\'answer\'][\'scores\'] = None\n\n    def __getitem__(self, index):\n        entry = self.entries[index]\n        features = self.features[entry[\'image\']]\n        spatials = self.spatials[entry[\'image\']]\n\n        question = entry[\'q_token\']\n        answer = entry[\'answer\']\n        labels = answer[\'labels\']\n        scores = answer[\'scores\']\n        target = torch.zeros(self.num_ans_candidates)\n        if labels is not None:\n            target.scatter_(0, labels, scores)\n\n        return features, spatials, question, target\n\n    def __len__(self):\n        return len(self.entries)\n'"
fc.py,2,"b'from __future__ import print_function\nimport torch.nn as nn\nfrom torch.nn.utils.weight_norm import weight_norm\n\n\nclass FCNet(nn.Module):\n    """"""Simple class for non-linear fully connect network\n    """"""\n    def __init__(self, dims):\n        super(FCNet, self).__init__()\n\n        layers = []\n        for i in range(len(dims)-2):\n            in_dim = dims[i]\n            out_dim = dims[i+1]\n            layers.append(weight_norm(nn.Linear(in_dim, out_dim), dim=None))\n            layers.append(nn.ReLU())\n        layers.append(weight_norm(nn.Linear(dims[-2], dims[-1]), dim=None))\n        layers.append(nn.ReLU())\n\n        self.main = nn.Sequential(*layers)\n\n    def forward(self, x):\n        return self.main(x)\n\n\nif __name__ == \'__main__\':\n    fc1 = FCNet([10, 20, 10])\n    print(fc1)\n\n    print(\'============\')\n    fc2 = FCNet([10, 20])\n    print(fc2)\n'"
language_model.py,4,"b'import torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\nimport numpy as np\n\n\nclass WordEmbedding(nn.Module):\n    """"""Word Embedding\n\n    The ntoken-th dim is used for padding_idx, which agrees *implicitly*\n    with the definition in Dictionary.\n    """"""\n    def __init__(self, ntoken, emb_dim, dropout):\n        super(WordEmbedding, self).__init__()\n        self.emb = nn.Embedding(ntoken+1, emb_dim, padding_idx=ntoken)\n        self.dropout = nn.Dropout(dropout)\n        self.ntoken = ntoken\n        self.emb_dim = emb_dim\n\n    def init_embedding(self, np_file):\n        weight_init = torch.from_numpy(np.load(np_file))\n        assert weight_init.shape == (self.ntoken, self.emb_dim)\n        self.emb.weight.data[:self.ntoken] = weight_init\n\n    def forward(self, x):\n        emb = self.emb(x)\n        emb = self.dropout(emb)\n        return emb\n\n\nclass QuestionEmbedding(nn.Module):\n    def __init__(self, in_dim, num_hid, nlayers, bidirect, dropout, rnn_type=\'GRU\'):\n        """"""Module for question embedding\n        """"""\n        super(QuestionEmbedding, self).__init__()\n        assert rnn_type == \'LSTM\' or rnn_type == \'GRU\'\n        rnn_cls = nn.LSTM if rnn_type == \'LSTM\' else nn.GRU\n\n        self.rnn = rnn_cls(\n            in_dim, num_hid, nlayers,\n            bidirectional=bidirect,\n            dropout=dropout,\n            batch_first=True)\n\n        self.in_dim = in_dim\n        self.num_hid = num_hid\n        self.nlayers = nlayers\n        self.rnn_type = rnn_type\n        self.ndirections = 1 + int(bidirect)\n\n    def init_hidden(self, batch):\n        # just to get the type of tensor\n        weight = next(self.parameters()).data\n        hid_shape = (self.nlayers * self.ndirections, batch, self.num_hid)\n        if self.rnn_type == \'LSTM\':\n            return (Variable(weight.new(*hid_shape).zero_()),\n                    Variable(weight.new(*hid_shape).zero_()))\n        else:\n            return Variable(weight.new(*hid_shape).zero_())\n\n    def forward(self, x):\n        # x: [batch, sequence, in_dim]\n        batch = x.size(0)\n        hidden = self.init_hidden(batch)\n        self.rnn.flatten_parameters()\n        output, hidden = self.rnn(x, hidden)\n\n        if self.ndirections == 1:\n            return output[:, -1]\n\n        forward_ = output[:, -1, :self.num_hid]\n        backward = output[:, 0, self.num_hid:]\n        return torch.cat((forward_, backward), dim=1)\n\n    def forward_all(self, x):\n        # x: [batch, sequence, in_dim]\n        batch = x.size(0)\n        hidden = self.init_hidden(batch)\n        self.rnn.flatten_parameters()\n        output, hidden = self.rnn(x, hidden)\n        return output\n'"
main.py,5,"b""import argparse\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader\nimport numpy as np\n\nfrom dataset import Dictionary, VQAFeatureDataset\nimport base_model\nfrom train import train\nimport utils\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--epochs', type=int, default=30)\n    parser.add_argument('--num_hid', type=int, default=1024)\n    parser.add_argument('--model', type=str, default='baseline0_newatt')\n    parser.add_argument('--output', type=str, default='saved_models/exp0')\n    parser.add_argument('--batch_size', type=int, default=512)\n    parser.add_argument('--seed', type=int, default=1111, help='random seed')\n    args = parser.parse_args()\n    return args\n\n\nif __name__ == '__main__':\n    args = parse_args()\n\n    torch.manual_seed(args.seed)\n    torch.cuda.manual_seed(args.seed)\n    torch.backends.cudnn.benchmark = True\n\n    dictionary = Dictionary.load_from_file('data/dictionary.pkl')\n    train_dset = VQAFeatureDataset('train', dictionary)\n    eval_dset = VQAFeatureDataset('val', dictionary)\n    batch_size = args.batch_size\n\n    constructor = 'build_%s' % args.model\n    model = getattr(base_model, constructor)(train_dset, args.num_hid).cuda()\n    model.w_emb.init_embedding('data/glove6b_init_300d.npy')\n\n    model = nn.DataParallel(model).cuda()\n\n    train_loader = DataLoader(train_dset, batch_size, shuffle=True, num_workers=1)\n    eval_loader =  DataLoader(eval_dset, batch_size, shuffle=True, num_workers=1)\n    train(model, train_loader, eval_loader, args.epochs, args.output)\n"""
train.py,6,"b""import os\nimport time\nimport torch\nimport torch.nn as nn\nimport utils\nfrom torch.autograd import Variable\n\n\ndef instance_bce_with_logits(logits, labels):\n    assert logits.dim() == 2\n\n    loss = nn.functional.binary_cross_entropy_with_logits(logits, labels)\n    loss *= labels.size(1)\n    return loss\n\n\ndef compute_score_with_logits(logits, labels):\n    logits = torch.max(logits, 1)[1].data # argmax\n    one_hots = torch.zeros(*labels.size()).cuda()\n    one_hots.scatter_(1, logits.view(-1, 1), 1)\n    scores = (one_hots * labels)\n    return scores\n\n\ndef train(model, train_loader, eval_loader, num_epochs, output):\n    utils.create_dir(output)\n    optim = torch.optim.Adamax(model.parameters())\n    logger = utils.Logger(os.path.join(output, 'log.txt'))\n    best_eval_score = 0\n\n    for epoch in range(num_epochs):\n        total_loss = 0\n        train_score = 0\n        t = time.time()\n\n        for i, (v, b, q, a) in enumerate(train_loader):\n            v = Variable(v).cuda()\n            b = Variable(b).cuda()\n            q = Variable(q).cuda()\n            a = Variable(a).cuda()\n\n            pred = model(v, b, q, a)\n            loss = instance_bce_with_logits(pred, a)\n            loss.backward()\n            nn.utils.clip_grad_norm(model.parameters(), 0.25)\n            optim.step()\n            optim.zero_grad()\n\n            batch_score = compute_score_with_logits(pred, a.data).sum()\n            total_loss += loss.data[0] * v.size(0)\n            train_score += batch_score\n\n        total_loss /= len(train_loader.dataset)\n        train_score = 100 * train_score / len(train_loader.dataset)\n        model.train(False)\n        eval_score, bound = evaluate(model, eval_loader)\n        model.train(True)\n\n        logger.write('epoch %d, time: %.2f' % (epoch, time.time()-t))\n        logger.write('\\ttrain_loss: %.2f, score: %.2f' % (total_loss, train_score))\n        logger.write('\\teval score: %.2f (%.2f)' % (100 * eval_score, 100 * bound))\n\n        if eval_score > best_eval_score:\n            model_path = os.path.join(output, 'model.pth')\n            torch.save(model.state_dict(), model_path)\n            best_eval_score = eval_score\n\n\ndef evaluate(model, dataloader):\n    score = 0\n    upper_bound = 0\n    num_data = 0\n    for v, b, q, a in iter(dataloader):\n        v = Variable(v, volatile=True).cuda()\n        b = Variable(b, volatile=True).cuda()\n        q = Variable(q, volatile=True).cuda()\n        pred = model(v, b, q, None)\n        batch_score = compute_score_with_logits(pred, a.cuda()).sum()\n        score += batch_score\n        upper_bound += (a.max(1)[0]).sum()\n        num_data += pred.size(0)\n\n    score = score / len(dataloader.dataset)\n    upper_bound = upper_bound / len(dataloader.dataset)\n    return score, upper_bound\n"""
utils.py,2,"b'from __future__ import print_function\n\nimport errno\nimport os\nimport numpy as np\nfrom PIL import Image\nimport torch\nimport torch.nn as nn\n\n\nEPS = 1e-7\n\n\ndef assert_eq(real, expected):\n    assert real == expected, \'%s (true) vs %s (expected)\' % (real, expected)\n\n\ndef assert_array_eq(real, expected):\n    assert (np.abs(real-expected) < EPS).all(), \\\n        \'%s (true) vs %s (expected)\' % (real, expected)\n\n\ndef load_folder(folder, suffix):\n    imgs = []\n    for f in sorted(os.listdir(folder)):\n        if f.endswith(suffix):\n            imgs.append(os.path.join(folder, f))\n    return imgs\n\n\ndef load_imageid(folder):\n    images = load_folder(folder, \'jpg\')\n    img_ids = set()\n    for img in images:\n        img_id = int(img.split(\'/\')[-1].split(\'.\')[0].split(\'_\')[-1])\n        img_ids.add(img_id)\n    return img_ids\n\n\ndef pil_loader(path):\n    with open(path, \'rb\') as f:\n        with Image.open(f) as img:\n            return img.convert(\'RGB\')\n\n\ndef weights_init(m):\n    """"""custom weights initialization.""""""\n    cname = m.__class__\n    if cname == nn.Linear or cname == nn.Conv2d or cname == nn.ConvTranspose2d:\n        m.weight.data.normal_(0.0, 0.02)\n    elif cname == nn.BatchNorm2d:\n        m.weight.data.normal_(1.0, 0.02)\n        m.bias.data.fill_(0)\n    else:\n        print(\'%s is not initialized.\' % cname)\n\n\ndef init_net(net, net_file):\n    if net_file:\n        net.load_state_dict(torch.load(net_file))\n    else:\n        net.apply(weights_init)\n\n\ndef create_dir(path):\n    if not os.path.exists(path):\n        try:\n            os.makedirs(path)\n        except OSError as exc:\n            if exc.errno != errno.EEXIST:\n                raise\n\n\nclass Logger(object):\n    def __init__(self, output_name):\n        dirname = os.path.dirname(output_name)\n        if not os.path.exists(dirname):\n            os.mkdir(dirname)\n\n        self.log_file = open(output_name, \'w\')\n        self.infos = {}\n\n    def append(self, key, val):\n        vals = self.infos.setdefault(key, [])\n        vals.append(val)\n\n    def log(self, extra_msg=\'\'):\n        msgs = [extra_msg]\n        for key, vals in self.infos.iteritems():\n            msgs.append(\'%s %.6f\' % (key, np.mean(vals)))\n        msg = \'\\n\'.join(msgs)\n        self.log_file.write(msg + \'\\n\')\n        self.log_file.flush()\n        self.infos = {}\n        return msg\n\n    def write(self, msg):\n        self.log_file.write(msg + \'\\n\')\n        self.log_file.flush()\n        print(msg)\n'"
tools/compute_softscore.py,0,"b'from __future__ import print_function\nimport os\nimport sys\nimport json\nimport numpy as np\nimport re\nimport cPickle\n\nsys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\nfrom dataset import Dictionary\nimport utils\n\n\ncontractions = {\n    ""aint"": ""ain\'t"", ""arent"": ""aren\'t"", ""cant"": ""can\'t"", ""couldve"":\n    ""could\'ve"", ""couldnt"": ""couldn\'t"", ""couldn\'tve"": ""couldn\'t\'ve"",\n    ""couldnt\'ve"": ""couldn\'t\'ve"", ""didnt"": ""didn\'t"", ""doesnt"":\n    ""doesn\'t"", ""dont"": ""don\'t"", ""hadnt"": ""hadn\'t"", ""hadnt\'ve"":\n    ""hadn\'t\'ve"", ""hadn\'tve"": ""hadn\'t\'ve"", ""hasnt"": ""hasn\'t"", ""havent"":\n    ""haven\'t"", ""hed"": ""he\'d"", ""hed\'ve"": ""he\'d\'ve"", ""he\'dve"":\n    ""he\'d\'ve"", ""hes"": ""he\'s"", ""howd"": ""how\'d"", ""howll"": ""how\'ll"",\n    ""hows"": ""how\'s"", ""Id\'ve"": ""I\'d\'ve"", ""I\'dve"": ""I\'d\'ve"", ""Im"":\n    ""I\'m"", ""Ive"": ""I\'ve"", ""isnt"": ""isn\'t"", ""itd"": ""it\'d"", ""itd\'ve"":\n    ""it\'d\'ve"", ""it\'dve"": ""it\'d\'ve"", ""itll"": ""it\'ll"", ""let\'s"": ""let\'s"",\n    ""maam"": ""ma\'am"", ""mightnt"": ""mightn\'t"", ""mightnt\'ve"":\n    ""mightn\'t\'ve"", ""mightn\'tve"": ""mightn\'t\'ve"", ""mightve"": ""might\'ve"",\n    ""mustnt"": ""mustn\'t"", ""mustve"": ""must\'ve"", ""neednt"": ""needn\'t"",\n    ""notve"": ""not\'ve"", ""oclock"": ""o\'clock"", ""oughtnt"": ""oughtn\'t"",\n    ""ow\'s\'at"": ""\'ow\'s\'at"", ""\'ows\'at"": ""\'ow\'s\'at"", ""\'ow\'sat"":\n    ""\'ow\'s\'at"", ""shant"": ""shan\'t"", ""shed\'ve"": ""she\'d\'ve"", ""she\'dve"":\n    ""she\'d\'ve"", ""she\'s"": ""she\'s"", ""shouldve"": ""should\'ve"", ""shouldnt"":\n    ""shouldn\'t"", ""shouldnt\'ve"": ""shouldn\'t\'ve"", ""shouldn\'tve"":\n    ""shouldn\'t\'ve"", ""somebody\'d"": ""somebodyd"", ""somebodyd\'ve"":\n    ""somebody\'d\'ve"", ""somebody\'dve"": ""somebody\'d\'ve"", ""somebodyll"":\n    ""somebody\'ll"", ""somebodys"": ""somebody\'s"", ""someoned"": ""someone\'d"",\n    ""someoned\'ve"": ""someone\'d\'ve"", ""someone\'dve"": ""someone\'d\'ve"",\n    ""someonell"": ""someone\'ll"", ""someones"": ""someone\'s"", ""somethingd"":\n    ""something\'d"", ""somethingd\'ve"": ""something\'d\'ve"", ""something\'dve"":\n    ""something\'d\'ve"", ""somethingll"": ""something\'ll"", ""thats"":\n    ""that\'s"", ""thered"": ""there\'d"", ""thered\'ve"": ""there\'d\'ve"",\n    ""there\'dve"": ""there\'d\'ve"", ""therere"": ""there\'re"", ""theres"":\n    ""there\'s"", ""theyd"": ""they\'d"", ""theyd\'ve"": ""they\'d\'ve"", ""they\'dve"":\n    ""they\'d\'ve"", ""theyll"": ""they\'ll"", ""theyre"": ""they\'re"", ""theyve"":\n    ""they\'ve"", ""twas"": ""\'twas"", ""wasnt"": ""wasn\'t"", ""wed\'ve"":\n    ""we\'d\'ve"", ""we\'dve"": ""we\'d\'ve"", ""weve"": ""we\'ve"", ""werent"":\n    ""weren\'t"", ""whatll"": ""what\'ll"", ""whatre"": ""what\'re"", ""whats"":\n    ""what\'s"", ""whatve"": ""what\'ve"", ""whens"": ""when\'s"", ""whered"":\n    ""where\'d"", ""wheres"": ""where\'s"", ""whereve"": ""where\'ve"", ""whod"":\n    ""who\'d"", ""whod\'ve"": ""who\'d\'ve"", ""who\'dve"": ""who\'d\'ve"", ""wholl"":\n    ""who\'ll"", ""whos"": ""who\'s"", ""whove"": ""who\'ve"", ""whyll"": ""why\'ll"",\n    ""whyre"": ""why\'re"", ""whys"": ""why\'s"", ""wont"": ""won\'t"", ""wouldve"":\n    ""would\'ve"", ""wouldnt"": ""wouldn\'t"", ""wouldnt\'ve"": ""wouldn\'t\'ve"",\n    ""wouldn\'tve"": ""wouldn\'t\'ve"", ""yall"": ""y\'all"", ""yall\'ll"":\n    ""y\'all\'ll"", ""y\'allll"": ""y\'all\'ll"", ""yall\'d\'ve"": ""y\'all\'d\'ve"",\n    ""y\'alld\'ve"": ""y\'all\'d\'ve"", ""y\'all\'dve"": ""y\'all\'d\'ve"", ""youd"":\n    ""you\'d"", ""youd\'ve"": ""you\'d\'ve"", ""you\'dve"": ""you\'d\'ve"", ""youll"":\n    ""you\'ll"", ""youre"": ""you\'re"", ""youve"": ""you\'ve""\n}\n\nmanual_map = { \'none\': \'0\',\n              \'zero\': \'0\',\n              \'one\': \'1\',\n              \'two\': \'2\',\n              \'three\': \'3\',\n              \'four\': \'4\',\n              \'five\': \'5\',\n              \'six\': \'6\',\n              \'seven\': \'7\',\n              \'eight\': \'8\',\n               \'nine\': \'9\',\n              \'ten\': \'10\'}\narticles = [\'a\', \'an\', \'the\']\nperiod_strip = re.compile(""(?!<=\\d)(\\.)(?!\\d)"")\ncomma_strip = re.compile(""(\\d)(\\,)(\\d)"")\npunct = [\';\', r""/"", \'[\', \']\', \'""\', \'{\', \'}\',\n                \'(\', \')\', \'=\', \'+\', \'\\\\\', \'_\', \'-\',\n                \'>\', \'<\', \'@\', \'`\', \',\', \'?\', \'!\']\n\n\ndef get_score(occurences):\n    if occurences == 0:\n        return 0\n    elif occurences == 1:\n        return 0.3\n    elif occurences == 2:\n        return 0.6\n    elif occurences == 3:\n        return 0.9\n    else:\n        return 1\n\n\ndef process_punctuation(inText):\n    outText = inText\n    for p in punct:\n        if (p + \' \' in inText or \' \' + p in inText) \\\n           or (re.search(comma_strip, inText) != None):\n            outText = outText.replace(p, \'\')\n        else:\n            outText = outText.replace(p, \' \')\n    outText = period_strip.sub("""", outText, re.UNICODE)\n    return outText\n\n\ndef process_digit_article(inText):\n    outText = []\n    tempText = inText.lower().split()\n    for word in tempText:\n        word = manual_map.setdefault(word, word)\n        if word not in articles:\n            outText.append(word)\n        else:\n            pass\n    for wordId, word in enumerate(outText):\n        if word in contractions:\n            outText[wordId] = contractions[word]\n    outText = \' \'.join(outText)\n    return outText\n\n\ndef multiple_replace(text, wordDict):\n    for key in wordDict:\n        text = text.replace(key, wordDict[key])\n    return text\n\n\ndef preprocess_answer(answer):\n    answer = process_digit_article(process_punctuation(answer))\n    answer = answer.replace(\',\', \'\')\n    return answer\n\n\ndef filter_answers(answers_dset, min_occurence):\n    """"""This will change the answer to preprocessed version\n    """"""\n    occurence = {}\n\n    for ans_entry in answers_dset:\n        answers = ans_entry[\'answers\']\n        gtruth = ans_entry[\'multiple_choice_answer\']\n        gtruth = preprocess_answer(gtruth)\n        if gtruth not in occurence:\n            occurence[gtruth] = set()\n        occurence[gtruth].add(ans_entry[\'question_id\'])\n    for answer in occurence.keys():\n        if len(occurence[answer]) < min_occurence:\n            occurence.pop(answer)\n\n    print(\'Num of answers that appear >= %d times: %d\' % (\n        min_occurence, len(occurence)))\n    return occurence\n\n\ndef create_ans2label(occurence, name, cache_root=\'data/cache\'):\n    """"""Note that this will also create label2ans.pkl at the same time\n\n    occurence: dict {answer -> whatever}\n    name: prefix of the output file\n    cache_root: str\n    """"""\n    ans2label = {}\n    label2ans = []\n    label = 0\n    for answer in occurence:\n        label2ans.append(answer)\n        ans2label[answer] = label\n        label += 1\n\n    utils.create_dir(cache_root)\n\n    cache_file = os.path.join(cache_root, name+\'_ans2label.pkl\')\n    cPickle.dump(ans2label, open(cache_file, \'wb\'))\n    cache_file = os.path.join(cache_root, name+\'_label2ans.pkl\')\n    cPickle.dump(label2ans, open(cache_file, \'wb\'))\n    return ans2label\n\n\ndef compute_target(answers_dset, ans2label, name, cache_root=\'data/cache\'):\n    """"""Augment answers_dset with soft score as label\n\n    ***answers_dset should be preprocessed***\n\n    Write result into a cache file\n    """"""\n    target = []\n    for ans_entry in answers_dset:\n        answers = ans_entry[\'answers\']\n        answer_count = {}\n        for answer in answers:\n            answer_ = answer[\'answer\']\n            answer_count[answer_] = answer_count.get(answer_, 0) + 1\n\n        labels = []\n        scores = []\n        for answer in answer_count:\n            if answer not in ans2label:\n                continue\n            labels.append(ans2label[answer])\n            score = get_score(answer_count[answer])\n            scores.append(score)\n\n        target.append({\n            \'question_id\': ans_entry[\'question_id\'],\n            \'image_id\': ans_entry[\'image_id\'],\n            \'labels\': labels,\n            \'scores\': scores\n        })\n\n    utils.create_dir(cache_root)\n    cache_file = os.path.join(cache_root, name+\'_target.pkl\')\n    cPickle.dump(target, open(cache_file, \'wb\'))\n    return target\n\n\ndef get_answer(qid, answers):\n    for ans in answers:\n        if ans[\'question_id\'] == qid:\n            return ans\n\n\ndef get_question(qid, questions):\n    for question in questions:\n        if question[\'question_id\'] == qid:\n            return question\n\n\nif __name__ == \'__main__\':\n    train_answer_file = \'data/v2_mscoco_train2014_annotations.json\'\n    train_answers = json.load(open(train_answer_file))[\'annotations\']\n\n    val_answer_file = \'data/v2_mscoco_val2014_annotations.json\'\n    val_answers = json.load(open(val_answer_file))[\'annotations\']\n\n    train_question_file = \'data/v2_OpenEnded_mscoco_train2014_questions.json\'\n    train_questions = json.load(open(train_question_file))[\'questions\']\n\n    val_question_file = \'data/v2_OpenEnded_mscoco_val2014_questions.json\'\n    val_questions = json.load(open(val_question_file))[\'questions\']\n\n    answers = train_answers + val_answers\n    occurence = filter_answers(answers, 9)\n    ans2label = create_ans2label(occurence, \'trainval\')\n    compute_target(train_answers, ans2label, \'train\')\n    compute_target(val_answers, ans2label, \'val\')\n'"
tools/create_dictionary.py,0,"b""from __future__ import print_function\nimport os\nimport sys\nimport json\nimport numpy as np\nsys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\nfrom dataset import Dictionary\n\n\ndef create_dictionary(dataroot):\n    dictionary = Dictionary()\n    questions = []\n    files = [\n        'v2_OpenEnded_mscoco_train2014_questions.json',\n        'v2_OpenEnded_mscoco_val2014_questions.json',\n        'v2_OpenEnded_mscoco_test2015_questions.json',\n        'v2_OpenEnded_mscoco_test-dev2015_questions.json'\n    ]\n    for path in files:\n        question_path = os.path.join(dataroot, path)\n        qs = json.load(open(question_path))['questions']\n        for q in qs:\n            dictionary.tokenize(q['question'], True)\n    return dictionary\n\n\ndef create_glove_embedding_init(idx2word, glove_file):\n    word2emb = {}\n    with open(glove_file, 'r') as f:\n        entries = f.readlines()\n    emb_dim = len(entries[0].split(' ')) - 1\n    print('embedding dim is %d' % emb_dim)\n    weights = np.zeros((len(idx2word), emb_dim), dtype=np.float32)\n\n    for entry in entries:\n        vals = entry.split(' ')\n        word = vals[0]\n        vals = map(float, vals[1:])\n        word2emb[word] = np.array(vals)\n    for idx, word in enumerate(idx2word):\n        if word not in word2emb:\n            continue\n        weights[idx] = word2emb[word]\n    return weights, word2emb\n\n\nif __name__ == '__main__':\n    d = create_dictionary('data')\n    d.dump_to_file('data/dictionary.pkl')\n\n    d = Dictionary.load_from_file('data/dictionary.pkl')\n    emb_dim = 300\n    glove_file = 'data/glove/glove.6B.%dd.txt' % emb_dim\n    weights, word2emb = create_glove_embedding_init(d.idx2word, glove_file)\n    np.save('data/glove6b_init_%dd.npy' % emb_dim, weights)\n"""
tools/detection_features_converter.py,0,"b'""""""\nReads in a tsv file with pre-trained bottom up attention features and\nstores it in HDF5 format.  Also store {image_id: feature_idx}\n as a pickle file.\n\nHierarchy of HDF5 file:\n\n{ \'image_features\': num_images x num_boxes x 2048 array of features\n  \'image_bb\': num_images x num_boxes x 4 array of bounding boxes }\n""""""\nfrom __future__ import print_function\n\nimport os\nimport sys\nsys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\n\nimport base64\nimport csv\nimport h5py\nimport cPickle\nimport numpy as np\nimport utils\n\n\ncsv.field_size_limit(sys.maxsize)\n\nFIELDNAMES = [\'image_id\', \'image_w\', \'image_h\', \'num_boxes\', \'boxes\', \'features\']\ninfile = \'data/trainval_36/trainval_resnet101_faster_rcnn_genome_36.tsv\'\ntrain_data_file = \'data/train36.hdf5\'\nval_data_file = \'data/val36.hdf5\'\ntrain_indices_file = \'data/train36_imgid2idx.pkl\'\nval_indices_file = \'data/val36_imgid2idx.pkl\'\ntrain_ids_file = \'data/train_ids.pkl\'\nval_ids_file = \'data/val_ids.pkl\'\n\nfeature_length = 2048\nnum_fixed_boxes = 36\n\n\nif __name__ == \'__main__\':\n    h_train = h5py.File(train_data_file, ""w"")\n    h_val = h5py.File(val_data_file, ""w"")\n\n    if os.path.exists(train_ids_file) and os.path.exists(val_ids_file):\n        train_imgids = cPickle.load(open(train_ids_file))\n        val_imgids = cPickle.load(open(val_ids_file))\n    else:\n        train_imgids = utils.load_imageid(\'data/train2014\')\n        val_imgids = utils.load_imageid(\'data/val2014\')\n        cPickle.dump(train_imgids, open(train_ids_file, \'wb\'))\n        cPickle.dump(val_imgids, open(val_ids_file, \'wb\'))\n\n    train_indices = {}\n    val_indices = {}\n\n    train_img_features = h_train.create_dataset(\n        \'image_features\', (len(train_imgids), num_fixed_boxes, feature_length), \'f\')\n    train_img_bb = h_train.create_dataset(\n        \'image_bb\', (len(train_imgids), num_fixed_boxes, 4), \'f\')\n    train_spatial_img_features = h_train.create_dataset(\n        \'spatial_features\', (len(train_imgids), num_fixed_boxes, 6), \'f\')\n\n    val_img_bb = h_val.create_dataset(\n        \'image_bb\', (len(val_imgids), num_fixed_boxes, 4), \'f\')\n    val_img_features = h_val.create_dataset(\n        \'image_features\', (len(val_imgids), num_fixed_boxes, feature_length), \'f\')\n    val_spatial_img_features = h_val.create_dataset(\n        \'spatial_features\', (len(val_imgids), num_fixed_boxes, 6), \'f\')\n\n    train_counter = 0\n    val_counter = 0\n\n    print(""reading tsv..."")\n    with open(infile, ""r+b"") as tsv_in_file:\n        reader = csv.DictReader(tsv_in_file, delimiter=\'\\t\', fieldnames=FIELDNAMES)\n        for item in reader:\n            item[\'num_boxes\'] = int(item[\'num_boxes\'])\n            image_id = int(item[\'image_id\'])\n            image_w = float(item[\'image_w\'])\n            image_h = float(item[\'image_h\'])\n            bboxes = np.frombuffer(\n                base64.decodestring(item[\'boxes\']),\n                dtype=np.float32).reshape((item[\'num_boxes\'], -1))\n\n            box_width = bboxes[:, 2] - bboxes[:, 0]\n            box_height = bboxes[:, 3] - bboxes[:, 1]\n            scaled_width = box_width / image_w\n            scaled_height = box_height / image_h\n            scaled_x = bboxes[:, 0] / image_w\n            scaled_y = bboxes[:, 1] / image_h\n\n            box_width = box_width[..., np.newaxis]\n            box_height = box_height[..., np.newaxis]\n            scaled_width = scaled_width[..., np.newaxis]\n            scaled_height = scaled_height[..., np.newaxis]\n            scaled_x = scaled_x[..., np.newaxis]\n            scaled_y = scaled_y[..., np.newaxis]\n\n            spatial_features = np.concatenate(\n                (scaled_x,\n                 scaled_y,\n                 scaled_x + scaled_width,\n                 scaled_y + scaled_height,\n                 scaled_width,\n                 scaled_height),\n                axis=1)\n\n            if image_id in train_imgids:\n                train_imgids.remove(image_id)\n                train_indices[image_id] = train_counter\n                train_img_bb[train_counter, :, :] = bboxes\n                train_img_features[train_counter, :, :] = np.frombuffer(\n                    base64.decodestring(item[\'features\']),\n                    dtype=np.float32).reshape((item[\'num_boxes\'], -1))\n                train_spatial_img_features[train_counter, :, :] = spatial_features\n                train_counter += 1\n            elif image_id in val_imgids:\n                val_imgids.remove(image_id)\n                val_indices[image_id] = val_counter\n                val_img_bb[val_counter, :, :] = bboxes\n                val_img_features[val_counter, :, :] = np.frombuffer(\n                    base64.decodestring(item[\'features\']),\n                    dtype=np.float32).reshape((item[\'num_boxes\'], -1))\n                val_spatial_img_features[val_counter, :, :] = spatial_features\n                val_counter += 1\n            else:\n                assert False, \'Unknown image id: %d\' % image_id\n\n    if len(train_imgids) != 0:\n        print(\'Warning: train_image_ids is not empty\')\n\n    if len(val_imgids) != 0:\n        print(\'Warning: val_image_ids is not empty\')\n\n    cPickle.dump(train_indices, open(train_indices_file, \'wb\'))\n    cPickle.dump(val_indices, open(val_indices_file, \'wb\'))\n    h_train.close()\n    h_val.close()\n    print(""done!"")\n'"
