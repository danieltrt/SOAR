file_path,api_count,code
evaluate_voc.py,7,"b'import argparse\nimport scipy\nfrom scipy import ndimage\nimport cv2\nimport numpy as np\nimport sys\nfrom collections import OrderedDict\nimport os\nfrom packaging import version\n\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\nimport torchvision.models as models\nimport torch.nn.functional as F\nfrom torch.utils import data, model_zoo\n\nfrom model.deeplab import Res_Deeplab\nfrom dataset.voc_dataset import VOCDataSet\n\nfrom PIL import Image\n\nimport matplotlib.pyplot as plt\n\nIMG_MEAN = np.array((104.00698793,116.66876762,122.67891434), dtype=np.float32)\n\nMODEL = \'DeepLab\'\nDATA_DIRECTORY = \'./dataset/VOC2012\'\nDATA_LIST_PATH = \'./dataset/voc_list/val.txt\'\nIGNORE_LABEL = 255\nNUM_CLASSES = 21\nNUM_STEPS = 1449 # Number of images in the validation set.\nRESTORE_FROM = \'http://vllab1.ucmerced.edu/~whung/adv-semi-seg/AdvSemiSegVOC0.125-8d75b3f1.pth\'\nPRETRAINED_MODEL = None\nSAVE_DIRECTORY = \'results\'\n\n\npretrianed_models_dict ={\'semi0.125\': \'http://vllab1.ucmerced.edu/~whung/adv-semi-seg/AdvSemiSegVOC0.125-03c6f81c.pth\',\n                         \'semi0.25\': \'http://vllab1.ucmerced.edu/~whung/adv-semi-seg/AdvSemiSegVOC0.25-473f8a14.pth\',\n                         \'semi0.5\': \'http://vllab1.ucmerced.edu/~whung/adv-semi-seg/AdvSemiSegVOC0.5-acf6a654.pth\',\n                         \'advFull\': \'http://vllab1.ucmerced.edu/~whung/adv-semi-seg/AdvSegVOCFull-92fbc7ee.pth\'}\n\n\ndef get_arguments():\n    """"""Parse all the arguments provided from the CLI.\n\n    Returns:\n      A list of parsed arguments.\n    """"""\n    parser = argparse.ArgumentParser(description=""VOC evaluation script"")\n    parser.add_argument(""--model"", type=str, default=MODEL,\n                        help=""available options : DeepLab/DRN"")\n    parser.add_argument(""--data-dir"", type=str, default=DATA_DIRECTORY,\n                        help=""Path to the directory containing the PASCAL VOC dataset."")\n    parser.add_argument(""--data-list"", type=str, default=DATA_LIST_PATH,\n                        help=""Path to the file listing the images in the dataset."")\n    parser.add_argument(""--ignore-label"", type=int, default=IGNORE_LABEL,\n                        help=""The index of the label to ignore during the training."")\n    parser.add_argument(""--num-classes"", type=int, default=NUM_CLASSES,\n                        help=""Number of classes to predict (including background)."")\n    parser.add_argument(""--restore-from"", type=str, default=RESTORE_FROM,\n                        help=""Where restore model parameters from."")\n    parser.add_argument(""--pretrained-model"", type=str, default=PRETRAINED_MODEL,\n                        help=""Where restore model parameters from."")\n    parser.add_argument(""--save-dir"", type=str, default=SAVE_DIRECTORY,\n                        help=""Directory to store results"")\n    parser.add_argument(""--gpu"", type=int, default=0,\n                        help=""choose gpu device."")\n    return parser.parse_args()\n\n\nclass VOCColorize(object):\n    def __init__(self, n=22):\n        self.cmap = color_map(22)\n        self.cmap = torch.from_numpy(self.cmap[:n])\n\n    def __call__(self, gray_image):\n        size = gray_image.shape\n        color_image = np.zeros((3, size[0], size[1]), dtype=np.uint8)\n\n        for label in range(0, len(self.cmap)):\n            mask = (label == gray_image)\n            color_image[0][mask] = self.cmap[label][0]\n            color_image[1][mask] = self.cmap[label][1]\n            color_image[2][mask] = self.cmap[label][2]\n\n        # handle void\n        mask = (255 == gray_image)\n        color_image[0][mask] = color_image[1][mask] = color_image[2][mask] = 255\n\n        return color_image\n\ndef color_map(N=256, normalized=False):\n    def bitget(byteval, idx):\n        return ((byteval & (1 << idx)) != 0)\n\n    dtype = \'float32\' if normalized else \'uint8\'\n    cmap = np.zeros((N, 3), dtype=dtype)\n    for i in range(N):\n        r = g = b = 0\n        c = i\n        for j in range(8):\n            r = r | (bitget(c, 0) << 7-j)\n            g = g | (bitget(c, 1) << 7-j)\n            b = b | (bitget(c, 2) << 7-j)\n            c = c >> 3\n\n        cmap[i] = np.array([r, g, b])\n\n    cmap = cmap/255 if normalized else cmap\n    return cmap\n\n\ndef get_iou(data_list, class_num, save_path=None):\n    from multiprocessing import Pool\n    from utils.metric import ConfusionMatrix\n\n    ConfM = ConfusionMatrix(class_num)\n    f = ConfM.generateM\n    pool = Pool()\n    m_list = pool.map(f, data_list)\n    pool.close()\n    pool.join()\n\n    for m in m_list:\n        ConfM.addM(m)\n\n    aveJ, j_list, M = ConfM.jaccard()\n\n    classes = np.array((\'background\',  # always index 0\n               \'aeroplane\', \'bicycle\', \'bird\', \'boat\',\n               \'bottle\', \'bus\', \'car\', \'cat\', \'chair\',\n                         \'cow\', \'diningtable\', \'dog\', \'horse\',\n                         \'motorbike\', \'person\', \'pottedplant\',\n                         \'sheep\', \'sofa\', \'train\', \'tvmonitor\'))\n\n    for i, iou in enumerate(j_list):\n        print(\'class {:2d} {:12} IU {:.2f}\'.format(i, classes[i], j_list[i]))\n\n\n    print(\'meanIOU: \' + str(aveJ) + \'\\n\')\n    if save_path:\n        with open(save_path, \'w\') as f:\n            for i, iou in enumerate(j_list):\n                f.write(\'class {:2d} {:12} IU {:.2f}\'.format(i, classes[i], j_list[i]) + \'\\n\')\n            f.write(\'meanIOU: \' + str(aveJ) + \'\\n\')\n\ndef show_all(gt, pred):\n    import matplotlib.pyplot as plt\n    from matplotlib import colors\n    from mpl_toolkits.axes_grid1 import make_axes_locatable\n\n    fig, axes = plt.subplots(1, 2)\n    ax1, ax2 = axes\n\n    classes = np.array((\'background\',  # always index 0\n               \'aeroplane\', \'bicycle\', \'bird\', \'boat\',\n               \'bottle\', \'bus\', \'car\', \'cat\', \'chair\',\n                         \'cow\', \'diningtable\', \'dog\', \'horse\',\n                         \'motorbike\', \'person\', \'pottedplant\',\n                         \'sheep\', \'sofa\', \'train\', \'tvmonitor\'))\n    colormap = [(0,0,0),(0.5,0,0),(0,0.5,0),(0.5,0.5,0),(0,0,0.5),(0.5,0,0.5),(0,0.5,0.5),\n                    (0.5,0.5,0.5),(0.25,0,0),(0.75,0,0),(0.25,0.5,0),(0.75,0.5,0),(0.25,0,0.5),\n                    (0.75,0,0.5),(0.25,0.5,0.5),(0.75,0.5,0.5),(0,0.25,0),(0.5,0.25,0),(0,0.75,0),\n                    (0.5,0.75,0),(0,0.25,0.5)]\n    cmap = colors.ListedColormap(colormap)\n    bounds=[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21]\n    norm = colors.BoundaryNorm(bounds, cmap.N)\n\n    ax1.set_title(\'gt\')\n    ax1.imshow(gt, cmap=cmap, norm=norm)\n\n    ax2.set_title(\'pred\')\n    ax2.imshow(pred, cmap=cmap, norm=norm)\n\n    plt.show()\n\ndef main():\n    """"""Create the model and start the evaluation process.""""""\n    args = get_arguments()\n\n    gpu0 = args.gpu\n\n    if not os.path.exists(args.save_dir):\n        os.makedirs(args.save_dir)\n\n\n    model = Res_Deeplab(num_classes=args.num_classes)\n\n    if args.pretrained_model != None:\n        args.restore_from = pretrianed_models_dict[args.pretrained_model]\n\n    if args.restore_from[:4] == \'http\' :\n        saved_state_dict = model_zoo.load_url(args.restore_from)\n    else:\n        saved_state_dict = torch.load(args.restore_from)\n    model.load_state_dict(saved_state_dict)\n\n    model.eval()\n    model.cuda(gpu0)\n\n    testloader = data.DataLoader(VOCDataSet(args.data_dir, args.data_list, crop_size=(505, 505), mean=IMG_MEAN, scale=False, mirror=False),\n                                    batch_size=1, shuffle=False, pin_memory=True)\n\n    if version.parse(torch.__version__) >= version.parse(\'0.4.0\'):\n        interp = nn.Upsample(size=(505, 505), mode=\'bilinear\', align_corners=True)\n    else:\n        interp = nn.Upsample(size=(505, 505), mode=\'bilinear\')\n    data_list = []\n\n    colorize = VOCColorize()\n\n    for index, batch in enumerate(testloader):\n        if index % 100 == 0:\n            print(\'%d processd\'%(index))\n        image, label, size, name = batch\n        size = size[0].numpy()\n        output = model(Variable(image, volatile=True).cuda(gpu0))\n        output = interp(output).cpu().data[0].numpy()\n\n        output = output[:,:size[0],:size[1]]\n        gt = np.asarray(label[0].numpy()[:size[0],:size[1]], dtype=np.int)\n\n        output = output.transpose(1,2,0)\n        output = np.asarray(np.argmax(output, axis=2), dtype=np.int)\n\n        filename = os.path.join(args.save_dir, \'{}.png\'.format(name[0]))\n        color_file = Image.fromarray(colorize(output).transpose(1, 2, 0), \'RGB\')\n        color_file.save(filename)\n\n        # show_all(gt, output)\n        data_list.append([gt.flatten(), output.flatten()])\n\n    filename = os.path.join(args.save_dir, \'result.txt\')\n    get_iou(data_list, args.num_classes, filename)\n\n\nif __name__ == \'__main__\':\n    main()\n'"
train.py,17,"b'import argparse\nimport cv2\nimport torch\nimport torch.nn as nn\nfrom torch.utils import data, model_zoo\nimport numpy as np\nimport pickle\nfrom torch.autograd import Variable\nimport torch.optim as optim\nimport torch.nn.functional as F\nimport scipy.misc\nimport torch.backends.cudnn as cudnn\nimport sys\nimport os\nimport os.path as osp\nimport pickle\nfrom packaging import version\n\nfrom model.deeplab import Res_Deeplab\nfrom model.discriminator import FCDiscriminator\nfrom utils.loss import CrossEntropy2d, BCEWithLogitsLoss2d\nfrom dataset.voc_dataset import VOCDataSet, VOCGTDataSet\n\n\n\nimport matplotlib.pyplot as plt\nimport random\nimport timeit\nstart = timeit.default_timer()\n\nIMG_MEAN = np.array((104.00698793,116.66876762,122.67891434), dtype=np.float32)\n\nMODEL = \'DeepLab\'\nBATCH_SIZE = 10\nITER_SIZE = 1\nNUM_WORKERS = 4\nDATA_DIRECTORY = \'./dataset/VOC2012\'\nDATA_LIST_PATH = \'./dataset/voc_list/train_aug.txt\'\nIGNORE_LABEL = 255\nINPUT_SIZE = \'321,321\'\nLEARNING_RATE = 2.5e-4\nMOMENTUM = 0.9\nNUM_CLASSES = 21\nNUM_STEPS = 20000\nPOWER = 0.9\nRANDOM_SEED = 1234\nRESTORE_FROM = \'http://vllab1.ucmerced.edu/~whung/adv-semi-seg/resnet101COCO-41f33a49.pth\'\nSAVE_NUM_IMAGES = 2\nSAVE_PRED_EVERY = 5000\nSNAPSHOT_DIR = \'./snapshots/\'\nWEIGHT_DECAY = 0.0005\n\nLEARNING_RATE_D = 1e-4\nLAMBDA_ADV_PRED = 0.1\n\nPARTIAL_DATA=0.5\n\nSEMI_START=5000\nLAMBDA_SEMI=0.1\nMASK_T=0.2\n\nLAMBDA_SEMI_ADV=0.001\nSEMI_START_ADV=0\nD_REMAIN=True\n\n\ndef get_arguments():\n    """"""Parse all the arguments provided from the CLI.\n\n    Returns:\n      A list of parsed arguments.\n    """"""\n    parser = argparse.ArgumentParser(description=""DeepLab-ResNet Network"")\n    parser.add_argument(""--model"", type=str, default=MODEL,\n                        help=""available options : DeepLab/DRN"")\n    parser.add_argument(""--batch-size"", type=int, default=BATCH_SIZE,\n                        help=""Number of images sent to the network in one step."")\n    parser.add_argument(""--iter-size"", type=int, default=ITER_SIZE,\n                        help=""Accumulate gradients for ITER_SIZE iterations."")\n    parser.add_argument(""--num-workers"", type=int, default=NUM_WORKERS,\n                        help=""number of workers for multithread dataloading."")\n    parser.add_argument(""--data-dir"", type=str, default=DATA_DIRECTORY,\n                        help=""Path to the directory containing the PASCAL VOC dataset."")\n    parser.add_argument(""--data-list"", type=str, default=DATA_LIST_PATH,\n                        help=""Path to the file listing the images in the dataset."")\n    parser.add_argument(""--partial-data"", type=float, default=PARTIAL_DATA,\n                        help=""The index of the label to ignore during the training."")\n    parser.add_argument(""--partial-id"", type=str, default=None,\n                        help=""restore partial id list"")\n    parser.add_argument(""--ignore-label"", type=int, default=IGNORE_LABEL,\n                        help=""The index of the label to ignore during the training."")\n    parser.add_argument(""--input-size"", type=str, default=INPUT_SIZE,\n                        help=""Comma-separated string with height and width of images."")\n    parser.add_argument(""--is-training"", action=""store_true"",\n                        help=""Whether to updates the running means and variances during the training."")\n    parser.add_argument(""--learning-rate"", type=float, default=LEARNING_RATE,\n                        help=""Base learning rate for training with polynomial decay."")\n    parser.add_argument(""--learning-rate-D"", type=float, default=LEARNING_RATE_D,\n                        help=""Base learning rate for discriminator."")\n    parser.add_argument(""--lambda-adv-pred"", type=float, default=LAMBDA_ADV_PRED,\n                        help=""lambda_adv for adversarial training."")\n    parser.add_argument(""--lambda-semi"", type=float, default=LAMBDA_SEMI,\n                        help=""lambda_semi for adversarial training."")\n    parser.add_argument(""--lambda-semi-adv"", type=float, default=LAMBDA_SEMI_ADV,\n                        help=""lambda_semi for adversarial training."")\n    parser.add_argument(""--mask-T"", type=float, default=MASK_T,\n                        help=""mask T for semi adversarial training."")\n    parser.add_argument(""--semi-start"", type=int, default=SEMI_START,\n                        help=""start semi learning after # iterations"")\n    parser.add_argument(""--semi-start-adv"", type=int, default=SEMI_START_ADV,\n                        help=""start semi learning after # iterations"")\n    parser.add_argument(""--D-remain"", type=bool, default=D_REMAIN,\n                        help=""Whether to train D with unlabeled data"")\n    parser.add_argument(""--momentum"", type=float, default=MOMENTUM,\n                        help=""Momentum component of the optimiser."")\n    parser.add_argument(""--not-restore-last"", action=""store_true"",\n                        help=""Whether to not restore last (FC) layers."")\n    parser.add_argument(""--num-classes"", type=int, default=NUM_CLASSES,\n                        help=""Number of classes to predict (including background)."")\n    parser.add_argument(""--num-steps"", type=int, default=NUM_STEPS,\n                        help=""Number of training steps."")\n    parser.add_argument(""--power"", type=float, default=POWER,\n                        help=""Decay parameter to compute the learning rate."")\n    parser.add_argument(""--random-mirror"", action=""store_true"",\n                        help=""Whether to randomly mirror the inputs during the training."")\n    parser.add_argument(""--random-scale"", action=""store_true"",\n                        help=""Whether to randomly scale the inputs during the training."")\n    parser.add_argument(""--random-seed"", type=int, default=RANDOM_SEED,\n                        help=""Random seed to have reproducible results."")\n    parser.add_argument(""--restore-from"", type=str, default=RESTORE_FROM,\n                        help=""Where restore model parameters from."")\n    parser.add_argument(""--restore-from-D"", type=str, default=None,\n                        help=""Where restore model parameters from."")\n    parser.add_argument(""--save-num-images"", type=int, default=SAVE_NUM_IMAGES,\n                        help=""How many images to save."")\n    parser.add_argument(""--save-pred-every"", type=int, default=SAVE_PRED_EVERY,\n                        help=""Save summaries and checkpoint every often."")\n    parser.add_argument(""--snapshot-dir"", type=str, default=SNAPSHOT_DIR,\n                        help=""Where to save snapshots of the model."")\n    parser.add_argument(""--weight-decay"", type=float, default=WEIGHT_DECAY,\n                        help=""Regularisation parameter for L2-loss."")\n    parser.add_argument(""--gpu"", type=int, default=0,\n                        help=""choose gpu device."")\n    return parser.parse_args()\n\nargs = get_arguments()\n\ndef loss_calc(pred, label, gpu):\n    """"""\n    This function returns cross entropy loss for semantic segmentation\n    """"""\n    # out shape batch_size x channels x h x w -> batch_size x channels x h x w\n    # label shape h x w x 1 x batch_size  -> batch_size x 1 x h x w\n    label = Variable(label.long()).cuda(gpu)\n    criterion = CrossEntropy2d().cuda(gpu)\n\n    return criterion(pred, label)\n\n\ndef lr_poly(base_lr, iter, max_iter, power):\n    return base_lr*((1-float(iter)/max_iter)**(power))\n\n\ndef adjust_learning_rate(optimizer, i_iter):\n    lr = lr_poly(args.learning_rate, i_iter, args.num_steps, args.power)\n    optimizer.param_groups[0][\'lr\'] = lr\n    if len(optimizer.param_groups) > 1 :\n        optimizer.param_groups[1][\'lr\'] = lr * 10\n\ndef adjust_learning_rate_D(optimizer, i_iter):\n    lr = lr_poly(args.learning_rate_D, i_iter, args.num_steps, args.power)\n    optimizer.param_groups[0][\'lr\'] = lr\n    if len(optimizer.param_groups) > 1 :\n        optimizer.param_groups[1][\'lr\'] = lr * 10\n\ndef one_hot(label):\n    label = label.numpy()\n    one_hot = np.zeros((label.shape[0], args.num_classes, label.shape[1], label.shape[2]), dtype=label.dtype)\n    for i in range(args.num_classes):\n        one_hot[:,i,...] = (label==i)\n    #handle ignore labels\n    return torch.FloatTensor(one_hot)\n\ndef make_D_label(label, ignore_mask):\n    ignore_mask = np.expand_dims(ignore_mask, axis=1)\n    D_label = np.ones(ignore_mask.shape)*label\n    D_label[ignore_mask] = 255\n    D_label = Variable(torch.FloatTensor(D_label)).cuda(args.gpu)\n\n    return D_label\n\n\ndef main():\n\n    h, w = map(int, args.input_size.split(\',\'))\n    input_size = (h, w)\n\n    cudnn.enabled = True\n    gpu = args.gpu\n\n    # create network\n    model = Res_Deeplab(num_classes=args.num_classes)\n\n    # load pretrained parameters\n    if args.restore_from[:4] == \'http\' :\n        saved_state_dict = model_zoo.load_url(args.restore_from)\n    else:\n        saved_state_dict = torch.load(args.restore_from)\n\n    # only copy the params that exist in current model (caffe-like)\n    new_params = model.state_dict().copy()\n    for name, param in new_params.items():\n        print name\n        if name in saved_state_dict and param.size() == saved_state_dict[name].size():\n            new_params[name].copy_(saved_state_dict[name])\n            print(\'copy {}\'.format(name))\n    model.load_state_dict(new_params)\n\n\n    model.train()\n    model.cuda(args.gpu)\n\n    cudnn.benchmark = True\n\n    # init D\n    model_D = FCDiscriminator(num_classes=args.num_classes)\n    if args.restore_from_D is not None:\n        model_D.load_state_dict(torch.load(args.restore_from_D))\n    model_D.train()\n    model_D.cuda(args.gpu)\n\n\n    if not os.path.exists(args.snapshot_dir):\n        os.makedirs(args.snapshot_dir)\n\n\n    train_dataset = VOCDataSet(args.data_dir, args.data_list, crop_size=input_size,\n                    scale=args.random_scale, mirror=args.random_mirror, mean=IMG_MEAN)\n\n    train_dataset_size = len(train_dataset)\n\n    train_gt_dataset = VOCGTDataSet(args.data_dir, args.data_list, crop_size=input_size,\n                       scale=args.random_scale, mirror=args.random_mirror, mean=IMG_MEAN)\n\n    if args.partial_data is None:\n        trainloader = data.DataLoader(train_dataset,\n                        batch_size=args.batch_size, shuffle=True, num_workers=5, pin_memory=True)\n\n        trainloader_gt = data.DataLoader(train_gt_dataset,\n                        batch_size=args.batch_size, shuffle=True, num_workers=5, pin_memory=True)\n    else:\n        #sample partial data\n        partial_size = int(args.partial_data * train_dataset_size)\n\n        if args.partial_id is not None:\n            train_ids = pickle.load(open(args.partial_id))\n            print(\'loading train ids from {}\'.format(args.partial_id))\n        else:\n            train_ids = range(train_dataset_size)\n            np.random.shuffle(train_ids)\n\n        pickle.dump(train_ids, open(osp.join(args.snapshot_dir, \'train_id.pkl\'), \'wb\'))\n\n        train_sampler = data.sampler.SubsetRandomSampler(train_ids[:partial_size])\n        train_remain_sampler = data.sampler.SubsetRandomSampler(train_ids[partial_size:])\n        train_gt_sampler = data.sampler.SubsetRandomSampler(train_ids[:partial_size])\n\n        trainloader = data.DataLoader(train_dataset,\n                        batch_size=args.batch_size, sampler=train_sampler, num_workers=3, pin_memory=True)\n        trainloader_remain = data.DataLoader(train_dataset,\n                        batch_size=args.batch_size, sampler=train_remain_sampler, num_workers=3, pin_memory=True)\n        trainloader_gt = data.DataLoader(train_gt_dataset,\n                        batch_size=args.batch_size, sampler=train_gt_sampler, num_workers=3, pin_memory=True)\n\n        trainloader_remain_iter = enumerate(trainloader_remain)\n\n\n    trainloader_iter = enumerate(trainloader)\n    trainloader_gt_iter = enumerate(trainloader_gt)\n\n\n    # implement model.optim_parameters(args) to handle different models\' lr setting\n\n    # optimizer for segmentation network\n    optimizer = optim.SGD(model.optim_parameters(args),\n                lr=args.learning_rate, momentum=args.momentum,weight_decay=args.weight_decay)\n    optimizer.zero_grad()\n\n    # optimizer for discriminator network\n    optimizer_D = optim.Adam(model_D.parameters(), lr=args.learning_rate_D, betas=(0.9,0.99))\n    optimizer_D.zero_grad()\n\n    # loss/ bilinear upsampling\n    bce_loss = BCEWithLogitsLoss2d()\n    interp = nn.Upsample(size=(input_size[1], input_size[0]), mode=\'bilinear\')\n\n    if version.parse(torch.__version__) >= version.parse(\'0.4.0\'):\n        interp = nn.Upsample(size=(input_size[1], input_size[0]), mode=\'bilinear\', align_corners=True)\n    else:\n        interp = nn.Upsample(size=(input_size[1], input_size[0]), mode=\'bilinear\')\n\n\n    # labels for adversarial training\n    pred_label = 0\n    gt_label = 1\n\n\n    for i_iter in range(args.num_steps):\n\n        loss_seg_value = 0\n        loss_adv_pred_value = 0\n        loss_D_value = 0\n        loss_semi_value = 0\n        loss_semi_adv_value = 0\n\n        optimizer.zero_grad()\n        adjust_learning_rate(optimizer, i_iter)\n        optimizer_D.zero_grad()\n        adjust_learning_rate_D(optimizer_D, i_iter)\n\n        for sub_i in range(args.iter_size):\n\n            # train G\n\n            # don\'t accumulate grads in D\n            for param in model_D.parameters():\n                param.requires_grad = False\n\n            # do semi first\n            if (args.lambda_semi > 0 or args.lambda_semi_adv > 0 ) and i_iter >= args.semi_start_adv :\n                try:\n                    _, batch = trainloader_remain_iter.next()\n                except:\n                    trainloader_remain_iter = enumerate(trainloader_remain)\n                    _, batch = trainloader_remain_iter.next()\n\n                # only access to img\n                images, _, _, _ = batch\n                images = Variable(images).cuda(args.gpu)\n\n\n                pred = interp(model(images))\n                pred_remain = pred.detach()\n\n                D_out = interp(model_D(F.softmax(pred)))\n                D_out_sigmoid = F.sigmoid(D_out).data.cpu().numpy().squeeze(axis=1)\n\n                ignore_mask_remain = np.zeros(D_out_sigmoid.shape).astype(np.bool)\n\n                loss_semi_adv = args.lambda_semi_adv * bce_loss(D_out, make_D_label(gt_label, ignore_mask_remain))\n                loss_semi_adv = loss_semi_adv/args.iter_size\n\n                #loss_semi_adv.backward()\n                loss_semi_adv_value += loss_semi_adv.data.cpu().numpy()[0]/args.lambda_semi_adv\n\n                if args.lambda_semi <= 0 or i_iter < args.semi_start:\n                    loss_semi_adv.backward()\n                    loss_semi_value = 0\n                else:\n                    # produce ignore mask\n                    semi_ignore_mask = (D_out_sigmoid < args.mask_T)\n\n                    semi_gt = pred.data.cpu().numpy().argmax(axis=1)\n                    semi_gt[semi_ignore_mask] = 255\n\n                    semi_ratio = 1.0 - float(semi_ignore_mask.sum())/semi_ignore_mask.size\n                    print(\'semi ratio: {:.4f}\'.format(semi_ratio))\n\n                    if semi_ratio == 0.0:\n                        loss_semi_value += 0\n                    else:\n                        semi_gt = torch.FloatTensor(semi_gt)\n\n                        loss_semi = args.lambda_semi * loss_calc(pred, semi_gt, args.gpu)\n                        loss_semi = loss_semi/args.iter_size\n                        loss_semi_value += loss_semi.data.cpu().numpy()[0]/args.lambda_semi\n                        loss_semi += loss_semi_adv\n                        loss_semi.backward()\n\n            else:\n                loss_semi = None\n                loss_semi_adv = None\n\n            # train with source\n\n            try:\n                _, batch = trainloader_iter.next()\n            except:\n                trainloader_iter = enumerate(trainloader)\n                _, batch = trainloader_iter.next()\n\n            images, labels, _, _ = batch\n            images = Variable(images).cuda(args.gpu)\n            ignore_mask = (labels.numpy() == 255)\n            pred = interp(model(images))\n\n            loss_seg = loss_calc(pred, labels, args.gpu)\n\n            D_out = interp(model_D(F.softmax(pred)))\n\n            loss_adv_pred = bce_loss(D_out, make_D_label(gt_label, ignore_mask))\n\n            loss = loss_seg + args.lambda_adv_pred * loss_adv_pred\n\n            # proper normalization\n            loss = loss/args.iter_size\n            loss.backward()\n            loss_seg_value += loss_seg.data.cpu().numpy()[0]/args.iter_size\n            loss_adv_pred_value += loss_adv_pred.data.cpu().numpy()[0]/args.iter_size\n\n\n            # train D\n\n            # bring back requires_grad\n            for param in model_D.parameters():\n                param.requires_grad = True\n\n            # train with pred\n            pred = pred.detach()\n\n            if args.D_remain:\n                pred = torch.cat((pred, pred_remain), 0)\n                ignore_mask = np.concatenate((ignore_mask,ignore_mask_remain), axis = 0)\n\n            D_out = interp(model_D(F.softmax(pred)))\n            loss_D = bce_loss(D_out, make_D_label(pred_label, ignore_mask))\n            loss_D = loss_D/args.iter_size/2\n            loss_D.backward()\n            loss_D_value += loss_D.data.cpu().numpy()[0]\n\n\n            # train with gt\n            # get gt labels\n            try:\n                _, batch = trainloader_gt_iter.next()\n            except:\n                trainloader_gt_iter = enumerate(trainloader_gt)\n                _, batch = trainloader_gt_iter.next()\n\n            _, labels_gt, _, _ = batch\n            D_gt_v = Variable(one_hot(labels_gt)).cuda(args.gpu)\n            ignore_mask_gt = (labels_gt.numpy() == 255)\n\n            D_out = interp(model_D(D_gt_v))\n            loss_D = bce_loss(D_out, make_D_label(gt_label, ignore_mask_gt))\n            loss_D = loss_D/args.iter_size/2\n            loss_D.backward()\n            loss_D_value += loss_D.data.cpu().numpy()[0]\n\n\n\n        optimizer.step()\n        optimizer_D.step()\n\n        print(\'exp = {}\'.format(args.snapshot_dir))\n        print(\'iter = {0:8d}/{1:8d}, loss_seg = {2:.3f}, loss_adv_p = {3:.3f}, loss_D = {4:.3f}, loss_semi = {5:.3f}, loss_semi_adv = {6:.3f}\'.format(i_iter, args.num_steps, loss_seg_value, loss_adv_pred_value, loss_D_value, loss_semi_value, loss_semi_adv_value))\n\n        if i_iter >= args.num_steps-1:\n            print \'save model ...\'\n            torch.save(model.state_dict(),osp.join(args.snapshot_dir, \'VOC_\'+str(args.num_steps)+\'.pth\'))\n            torch.save(model_D.state_dict(),osp.join(args.snapshot_dir, \'VOC_\'+str(args.num_steps)+\'_D.pth\'))\n            break\n\n        if i_iter % args.save_pred_every == 0 and i_iter!=0:\n            print \'taking snapshot ...\'\n            torch.save(model.state_dict(),osp.join(args.snapshot_dir, \'VOC_\'+str(i_iter)+\'.pth\'))\n            torch.save(model_D.state_dict(),osp.join(args.snapshot_dir, \'VOC_\'+str(i_iter)+\'_D.pth\'))\n\n    end = timeit.default_timer()\n    print end-start,\'seconds\'\n\nif __name__ == \'__main__\':\n    main()\n'"
dataset/__init__.py,0,b''
dataset/voc_dataset.py,1,"b'import os\nimport os.path as osp\nimport numpy as np\nimport random\nimport matplotlib.pyplot as plt\nimport collections\nimport torch\nimport torchvision\nimport cv2\nfrom torch.utils import data\nfrom PIL import Image\n\n\nclass VOCDataSet(data.Dataset):\n    def __init__(self, root, list_path, max_iters=None, crop_size=(321, 321), mean=(128, 128, 128), scale=True, mirror=True, ignore_label=255):\n        self.root = root\n        self.list_path = list_path\n        self.crop_h, self.crop_w = crop_size\n        self.scale = scale\n        self.ignore_label = ignore_label\n        self.mean = mean\n        self.is_mirror = mirror\n        self.img_ids = [i_id.strip() for i_id in open(list_path)]\n        if not max_iters==None:\n\t        self.img_ids = self.img_ids * int(np.ceil(float(max_iters) / len(self.img_ids)))\n        self.files = []\n        # for split in [""train"", ""trainval"", ""val""]:\n        for name in self.img_ids:\n            img_file = osp.join(self.root, ""JPEGImages/%s.jpg"" % name)\n            label_file = osp.join(self.root, ""SegmentationClassAug/%s.png"" % name)\n            self.files.append({\n                ""img"": img_file,\n                ""label"": label_file,\n                ""name"": name\n            })\n\n    def __len__(self):\n        return len(self.files)\n\n    def generate_scale_label(self, image, label):\n        f_scale = 0.5 + random.randint(0, 11) / 10.0\n        image = cv2.resize(image, None, fx=f_scale, fy=f_scale, interpolation = cv2.INTER_LINEAR)\n        label = cv2.resize(label, None, fx=f_scale, fy=f_scale, interpolation = cv2.INTER_NEAREST)\n        return image, label\n\n    def __getitem__(self, index):\n        datafiles = self.files[index]\n        image = cv2.imread(datafiles[""img""], cv2.IMREAD_COLOR)\n        label = cv2.imread(datafiles[""label""], cv2.IMREAD_GRAYSCALE)\n        size = image.shape\n        name = datafiles[""name""]\n        if self.scale:\n            image, label = self.generate_scale_label(image, label)\n        image = np.asarray(image, np.float32)\n        image -= self.mean\n        img_h, img_w = label.shape\n        pad_h = max(self.crop_h - img_h, 0)\n        pad_w = max(self.crop_w - img_w, 0)\n        if pad_h > 0 or pad_w > 0:\n            img_pad = cv2.copyMakeBorder(image, 0, pad_h, 0,\n                pad_w, cv2.BORDER_CONSTANT,\n                value=(0.0, 0.0, 0.0))\n            label_pad = cv2.copyMakeBorder(label, 0, pad_h, 0,\n                pad_w, cv2.BORDER_CONSTANT,\n                value=(self.ignore_label,))\n        else:\n            img_pad, label_pad = image, label\n\n        img_h, img_w = label_pad.shape\n        h_off = random.randint(0, img_h - self.crop_h)\n        w_off = random.randint(0, img_w - self.crop_w)\n        image = np.asarray(img_pad[h_off : h_off+self.crop_h, w_off : w_off+self.crop_w], np.float32)\n        label = np.asarray(label_pad[h_off : h_off+self.crop_h, w_off : w_off+self.crop_w], np.float32)\n        image = image[:, :, ::-1]  # change to BGR\n        image = image.transpose((2, 0, 1))\n        if self.is_mirror:\n            flip = np.random.choice(2) * 2 - 1\n            image = image[:, :, ::flip]\n            label = label[:, ::flip]\n\n        return image.copy(), label.copy(), np.array(size), name\n\n\nclass VOCGTDataSet(data.Dataset):\n    def __init__(self, root, list_path, max_iters=None, crop_size=(321, 321), mean=(128, 128, 128), scale=True, mirror=True, ignore_label=255):\n        self.root = root\n        self.list_path = list_path\n        self.crop_size = crop_size\n        self.crop_h, self.crop_w = crop_size\n        self.scale = scale\n        self.ignore_label = ignore_label\n        self.mean = mean\n        self.is_mirror = mirror\n        self.img_ids = [i_id.strip() for i_id in open(list_path)]\n        if not max_iters==None:\n            self.img_ids = self.img_ids * int(np.ceil(float(max_iters) / len(self.img_ids)))\n        self.files = []\n        for name in self.img_ids:\n            img_file = osp.join(self.root, ""JPEGImages/%s.jpg"" % name)\n            label_file = osp.join(self.root, ""SegmentationClassAug/%s.png"" % name)\n            self.files.append({\n                ""img"": img_file,\n                ""label"": label_file,\n                ""name"": name\n            })\n\n    def __len__(self):\n        return len(self.files)\n\n    def generate_scale_label(self, image, label):\n        f_scale = 0.5 + random.randint(0, 11) / 10.0\n        image = cv2.resize(image, None, fx=f_scale, fy=f_scale, interpolation = cv2.INTER_LINEAR)\n        label = cv2.resize(label, None, fx=f_scale, fy=f_scale, interpolation = cv2.INTER_NEAREST)\n        return image, label\n\n    def __getitem__(self, index):\n        datafiles = self.files[index]\n        image = cv2.imread(datafiles[""img""], cv2.IMREAD_COLOR)\n        label = cv2.imread(datafiles[""label""], cv2.IMREAD_GRAYSCALE)\n        size = image.shape\n        name = datafiles[""name""]\n\n        attempt = 0\n        while attempt < 10 :\n            if self.scale:\n                image, label = self.generate_scale_label(image, label)\n\n            img_h, img_w = label.shape\n            pad_h = max(self.crop_h - img_h, 0)\n            pad_w = max(self.crop_w - img_w, 0)\n            if pad_h > 0 or pad_w > 0:\n                attempt += 1\n                continue\n            else:\n                break\n\n        if attempt == 10 :\n            image = cv2.resize(image, self.crop_size, interpolation = cv2.INTER_LINEAR)\n            label = cv2.resize(label, self.crop_size, interpolation = cv2.INTER_NEAREST)\n\n\n        image = np.asarray(image, np.float32)\n        image -= self.mean\n\n        img_h, img_w = label.shape\n        h_off = random.randint(0, img_h - self.crop_h)\n        w_off = random.randint(0, img_w - self.crop_w)\n        image = np.asarray(image[h_off : h_off+self.crop_h, w_off : w_off+self.crop_w], np.float32)\n        label = np.asarray(label[h_off : h_off+self.crop_h, w_off : w_off+self.crop_w], np.float32)\n        image = image[:, :, ::-1]  # change to BGR\n        image = image.transpose((2, 0, 1))\n        if self.is_mirror:\n            flip = np.random.choice(2) * 2 - 1\n            image = image[:, :, ::flip]\n            label = label[:, ::flip]\n\n        return image.copy(), label.copy(), np.array(size), name\n\nclass VOCDataTestSet(data.Dataset):\n    def __init__(self, root, list_path, crop_size=(505, 505), mean=(128, 128, 128)):\n        self.root = root\n        self.list_path = list_path\n        self.crop_h, self.crop_w = crop_size\n        self.mean = mean\n        self.img_ids = [i_id.strip() for i_id in open(list_path)]\n        self.files = []\n        # for split in [""train"", ""trainval"", ""val""]:\n        for name in self.img_ids:\n            img_file = osp.join(self.root, ""JPEGImages/%s.jpg"" % name)\n            self.files.append({\n                ""img"": img_file\n            })\n\n    def __len__(self):\n        return len(self.files)\n\n    def __getitem__(self, index):\n        datafiles = self.files[index]\n        image = cv2.imread(datafiles[""img""], cv2.IMREAD_COLOR)\n        size = image.shape\n        name = osp.splitext(osp.basename(datafiles[""img""]))[0]\n        image = np.asarray(image, np.float32)\n        image -= self.mean\n\n        img_h, img_w, _ = image.shape\n        pad_h = max(self.crop_h - img_h, 0)\n        pad_w = max(self.crop_w - img_w, 0)\n        if pad_h > 0 or pad_w > 0:\n            image = cv2.copyMakeBorder(image, 0, pad_h, 0,\n                pad_w, cv2.BORDER_CONSTANT,\n                value=(0.0, 0.0, 0.0))\n        image = image.transpose((2, 0, 1))\n        return image, name, size\n\n\nif __name__ == \'__main__\':\n    dst = VOCDataSet(""./data"", is_transform=True)\n    trainloader = data.DataLoader(dst, batch_size=4)\n    for i, data in enumerate(trainloader):\n        imgs, labels = data\n        if i == 0:\n            img = torchvision.utils.make_grid(imgs).numpy()\n            img = np.transpose(img, (1, 2, 0))\n            img = img[:, :, ::-1]\n            plt.imshow(img)\n            plt.show()\n'"
model/__init__.py,0,b''
model/deeplab.py,2,"b'import torch.nn as nn\nimport math\nimport torch.utils.model_zoo as model_zoo\nimport torch\nimport numpy as np\naffine_par = True\n\n\ndef outS(i):\n    i = int(i)\n    i = (i+1)/2\n    i = int(np.ceil((i+1)/2.0))\n    i = (i+1)/2\n    return i\n\ndef conv3x3(in_planes, out_planes, stride=1):\n    ""3x3 convolution with padding""\n    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n                     padding=1, bias=False)\n\n\nclass BasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None):\n        super(BasicBlock, self).__init__()\n        self.conv1 = conv3x3(inplanes, planes, stride)\n        self.bn1 = nn.BatchNorm2d(planes, affine = affine_par)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = conv3x3(planes, planes)\n        self.bn2 = nn.BatchNorm2d(planes, affine = affine_par)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\n\nclass Bottleneck(nn.Module):\n    expansion = 4\n\n    def __init__(self, inplanes, planes, stride=1, dilation=1, downsample=None):\n        super(Bottleneck, self).__init__()\n        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, stride=stride, bias=False) # change\n        self.bn1 = nn.BatchNorm2d(planes,affine = affine_par)\n        for i in self.bn1.parameters():\n            i.requires_grad = False\n\n        padding = dilation\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, # change\n                               padding=padding, bias=False, dilation = dilation)\n        self.bn2 = nn.BatchNorm2d(planes,affine = affine_par)\n        for i in self.bn2.parameters():\n            i.requires_grad = False\n        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)\n        self.bn3 = nn.BatchNorm2d(planes * 4, affine = affine_par)\n        for i in self.bn3.parameters():\n            i.requires_grad = False\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = downsample\n        self.stride = stride\n\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\nclass Classifier_Module(nn.Module):\n\n    def __init__(self, dilation_series, padding_series, num_classes):\n        super(Classifier_Module, self).__init__()\n        self.conv2d_list = nn.ModuleList()\n        for dilation, padding in zip(dilation_series, padding_series):\n            self.conv2d_list.append(nn.Conv2d(2048, num_classes, kernel_size=3, stride=1, padding=padding, dilation=dilation, bias = True))\n\n        for m in self.conv2d_list:\n            m.weight.data.normal_(0, 0.01)\n\n    def forward(self, x):\n        out = self.conv2d_list[0](x)\n        for i in range(len(self.conv2d_list)-1):\n            out += self.conv2d_list[i+1](x)\n            return out\n\n\n\nclass ResNet(nn.Module):\n    def __init__(self, block, layers, num_classes):\n        self.inplanes = 64\n        super(ResNet, self).__init__()\n        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3,\n                               bias=False)\n        self.bn1 = nn.BatchNorm2d(64, affine = affine_par)\n        for i in self.bn1.parameters():\n            i.requires_grad = False\n        self.relu = nn.ReLU(inplace=True)\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1, ceil_mode=True) # change\n        self.layer1 = self._make_layer(block, 64, layers[0])\n        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n        self.layer3 = self._make_layer(block, 256, layers[2], stride=1, dilation=2)\n        self.layer4 = self._make_layer(block, 512, layers[3], stride=1, dilation=4)\n        self.layer5 = self._make_pred_layer(Classifier_Module, [6,12,18,24],[6,12,18,24],num_classes)\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n                m.weight.data.normal_(0, 0.01)\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n        #        for i in m.parameters():\n        #            i.requires_grad = False\n\n    def _make_layer(self, block, planes, blocks, stride=1, dilation=1):\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion or dilation == 2 or dilation == 4:\n            downsample = nn.Sequential(\n                nn.Conv2d(self.inplanes, planes * block.expansion,\n                          kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(planes * block.expansion,affine = affine_par))\n        for i in downsample._modules[\'1\'].parameters():\n            i.requires_grad = False\n        layers = []\n        layers.append(block(self.inplanes, planes, stride,dilation=dilation, downsample=downsample))\n        self.inplanes = planes * block.expansion\n        for i in range(1, blocks):\n            layers.append(block(self.inplanes, planes, dilation=dilation))\n\n        return nn.Sequential(*layers)\n    def _make_pred_layer(self,block, dilation_series, padding_series,num_classes):\n        return block(dilation_series,padding_series,num_classes)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n        x = self.layer5(x)\n\n        return x\n\n    def get_1x_lr_params_NOscale(self):\n        """"""\n        This generator returns all the parameters of the net except for \n        the last classification layer. Note that for each batchnorm layer, \n        requires_grad is set to False in deeplab_resnet.py, therefore this function does not return \n        any batchnorm parameter\n        """"""\n        b = []\n\n        b.append(self.conv1)\n        b.append(self.bn1)\n        b.append(self.layer1)\n        b.append(self.layer2)\n        b.append(self.layer3)\n        b.append(self.layer4)\n\n    \n        for i in range(len(b)):\n            for j in b[i].modules():\n                jj = 0\n                for k in j.parameters():\n                    jj+=1\n                    if k.requires_grad:\n                        yield k\n\n    def get_10x_lr_params(self):\n        """"""\n        This generator returns all the parameters for the last layer of the net,\n        which does the classification of pixel into classes\n        """"""\n        b = []\n        b.append(self.layer5.parameters())\n\n        for j in range(len(b)):\n            for i in b[j]:\n                yield i\n            \n\n\n    def optim_parameters(self, args):\n        return [{\'params\': self.get_1x_lr_params_NOscale(), \'lr\': args.learning_rate},\n                {\'params\': self.get_10x_lr_params(), \'lr\': 10*args.learning_rate}] \n\n\ndef Res_Deeplab(num_classes=21):\n    model = ResNet(Bottleneck,[3, 4, 23, 3], num_classes)\n    return model\n\n'"
model/discriminator.py,2,"b""import torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass FCDiscriminator(nn.Module):\n\n\tdef __init__(self, num_classes, ndf = 64):\n\t\tsuper(FCDiscriminator, self).__init__()\n\n\t\tself.conv1 = nn.Conv2d(num_classes, ndf, kernel_size=4, stride=2, padding=1)\n\t\tself.conv2 = nn.Conv2d(ndf, ndf*2, kernel_size=4, stride=2, padding=1)\n\t\tself.conv3 = nn.Conv2d(ndf*2, ndf*4, kernel_size=4, stride=2, padding=1)\n\t\tself.conv4 = nn.Conv2d(ndf*4, ndf*8, kernel_size=4, stride=2, padding=1)\n\t\tself.classifier = nn.Conv2d(ndf*8, 1, kernel_size=4, stride=2, padding=1)\n\n\t\tself.leaky_relu = nn.LeakyReLU(negative_slope=0.2, inplace=True)\n\t\t#self.up_sample = nn.Upsample(scale_factor=32, mode='bilinear')\n\t\t#self.sigmoid = nn.Sigmoid()\n\n\n\tdef forward(self, x):\n\t\tx = self.conv1(x)\n\t\tx = self.leaky_relu(x)\n\t\tx = self.conv2(x)\n\t\tx = self.leaky_relu(x)\n\t\tx = self.conv3(x)\n\t\tx = self.leaky_relu(x)\n\t\tx = self.conv4(x)\n\t\tx = self.leaky_relu(x)\n\t\tx = self.classifier(x)\n\t\t#x = self.up_sample(x)\n\t\t#x = self.sigmoid(x) \n\n\t\treturn x\n"""
utils/__init__.py,0,b''
utils/loss.py,5,"b'import torch\nimport torch.nn.functional as F\nimport torch.nn as nn\nfrom torch.autograd import Variable\n\nclass CrossEntropy2d(nn.Module):\n\n    def __init__(self, size_average=True, ignore_label=255):\n        super(CrossEntropy2d, self).__init__()\n        self.size_average = size_average\n        self.ignore_label = ignore_label\n\n    def forward(self, predict, target, weight=None):\n        """"""\n            Args:\n                predict:(n, c, h, w)\n                target:(n, h, w)\n                weight (Tensor, optional): a manual rescaling weight given to each class.\n                                           If given, has to be a Tensor of size ""nclasses""\n        """"""\n        assert not target.requires_grad\n        assert predict.dim() == 4\n        assert target.dim() == 3\n        assert predict.size(0) == target.size(0), ""{0} vs {1} "".format(predict.size(0), target.size(0))\n        assert predict.size(2) == target.size(1), ""{0} vs {1} "".format(predict.size(2), target.size(1))\n        assert predict.size(3) == target.size(2), ""{0} vs {1} "".format(predict.size(3), target.size(3))\n        n, c, h, w = predict.size()\n        target_mask = (target >= 0) * (target != self.ignore_label)\n        target = target[target_mask]\n        if not target.data.dim():\n            return Variable(torch.zeros(1))\n        predict = predict.transpose(1, 2).transpose(2, 3).contiguous()\n        predict = predict[target_mask.view(n, h, w, 1).repeat(1, 1, 1, c)].view(-1, c)\n        loss = F.cross_entropy(predict, target, weight=weight, size_average=self.size_average)\n        return loss\n\n\nclass BCEWithLogitsLoss2d(nn.Module):\n\n    def __init__(self, size_average=True, ignore_label=255):\n        super(BCEWithLogitsLoss2d, self).__init__()\n        self.size_average = size_average\n        self.ignore_label = ignore_label\n\n    def forward(self, predict, target, weight=None):\n        """"""\n            Args:\n                predict:(n, 1, h, w)\n                target:(n, 1, h, w)\n                weight (Tensor, optional): a manual rescaling weight given to each class.\n                                           If given, has to be a Tensor of size ""nclasses""\n        """"""\n        assert not target.requires_grad\n        assert predict.dim() == 4\n        assert target.dim() == 4\n        assert predict.size(0) == target.size(0), ""{0} vs {1} "".format(predict.size(0), target.size(0))\n        assert predict.size(2) == target.size(2), ""{0} vs {1} "".format(predict.size(2), target.size(2))\n        assert predict.size(3) == target.size(3), ""{0} vs {1} "".format(predict.size(3), target.size(3))\n        n, c, h, w = predict.size()\n        target_mask = (target >= 0) * (target != self.ignore_label)\n        target = target[target_mask]\n        if not target.data.dim():\n            return Variable(torch.zeros(1))\n        predict = predict[target_mask]\n        loss = F.binary_cross_entropy_with_logits(predict, target, weight=weight, size_average=self.size_average)\n        return loss\n'"
utils/metric.py,0,"b""import os, sys\nimport numpy as np\n\nfrom multiprocessing import Pool \nimport copy_reg\nimport types\ndef _pickle_method(m):\n    if m.im_self is None:\n        return getattr, (m.im_class, m.im_func.func_name)\n    else:\n        return getattr, (m.im_self, m.im_func.func_name)\n\ncopy_reg.pickle(types.MethodType, _pickle_method)\n\nclass ConfusionMatrix(object):\n\n    def __init__(self, nclass, classes=None):\n        self.nclass = nclass\n        self.classes = classes\n        self.M = np.zeros((nclass, nclass))\n\n    def add(self, gt, pred):\n        assert(np.max(pred) <= self.nclass)\n        assert(len(gt) == len(pred))\n        for i in range(len(gt)):\n            if not gt[i] == 255:\n                self.M[gt[i], pred[i]] += 1.0\n\n    def addM(self, matrix):\n        assert(matrix.shape == self.M.shape)\n        self.M += matrix\n\n    def __str__(self):\n        pass\n\n    def recall(self):\n        recall = 0.0\n        for i in xrange(self.nclass):\n            recall += self.M[i, i] / np.sum(self.M[:, i])\n\n        return recall/self.nclass\n\n    def accuracy(self):\n        accuracy = 0.0\n        for i in xrange(self.nclass):\n            accuracy += self.M[i, i] / np.sum(self.M[i, :])\n\n        return accuracy/self.nclass\n\n    def jaccard(self):\n        jaccard = 0.0\n        jaccard_perclass = []\n        for i in xrange(self.nclass):\n            if not self.M[i, i] == 0:\n                jaccard_perclass.append(self.M[i, i] / (np.sum(self.M[i, :]) + np.sum(self.M[:, i]) - self.M[i, i]))\n\n        return np.sum(jaccard_perclass)/len(jaccard_perclass), jaccard_perclass, self.M\n\n    def generateM(self, item):\n        gt, pred = item\n        m = np.zeros((self.nclass, self.nclass))\n        assert(len(gt) == len(pred))\n        for i in range(len(gt)):\n            if gt[i] < self.nclass: #and pred[i] < self.nclass:\n                m[gt[i], pred[i]] += 1.0\n        return m\n\n\nif __name__ == '__main__':\n    args = parse_args()\n\n    m_list = []\n    data_list = []\n    test_ids = [i.strip() for i in open(args.test_ids) if not i.strip() == '']\n    for index, img_id in enumerate(test_ids):\n        if index % 100 == 0:\n            print('%d processd'%(index))\n        pred_img_path = os.path.join(args.pred_dir, img_id+'.png')\n        gt_img_path = os.path.join(args.gt_dir, img_id+'.png')\n        pred = cv2.imread(pred_img_path, cv2.IMREAD_GRAYSCALE)\n        gt = cv2.imread(gt_img_path, cv2.IMREAD_GRAYSCALE)\n        # show_all(gt, pred)\n        data_list.append([gt.flatten(), pred.flatten()])\n\n    ConfM = ConfusionMatrix(args.class_num)\n    f = ConfM.generateM\n    pool = Pool() \n    m_list = pool.map(f, data_list)\n    pool.close() \n    pool.join() \n    \n    for m in m_list:\n        ConfM.addM(m)\n\n    aveJ, j_list, M = ConfM.jaccard()\n    with open(args.save_path, 'w') as f:\n        f.write('meanIOU: ' + str(aveJ) + '\\n')\n        f.write(str(j_list)+'\\n')\n        f.write(str(M)+'\\n')\n"""
