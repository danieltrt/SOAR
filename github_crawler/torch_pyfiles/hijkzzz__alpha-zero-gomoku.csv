file_path,api_count,code
config.py,0,"b""config = {\n    # gomoku\n    'n': 15,                                    # board size\n    'n_in_row': 5,                              # n in row\n\n    # mcts\n    'libtorch_use_gpu' : True,                  # libtorch use cuda\n    'num_mcts_threads': 4,                      # mcts threads number\n    'num_mcts_sims': 1600,                      # mcts simulation times\n    'c_puct': 5,                                # puct coeff\n    'c_virtual_loss': 3,                        # virtual loss coeff\n\n    # neural_network\n    'train_use_gpu' : True,                     # train neural network using cuda\n    'lr': 0.001,                                # learning rate\n    'l2': 0.0001,                               # L2\n    'num_channels': 256,                        # convolution neural network channel size\n    'num_layers' : 4,                           # residual layer number\n    'epochs': 1.5,                              # train epochs\n    'batch_size': 512,                          # batch size\n\n    # train\n    'num_iters': 10000,                         # train iterations\n    'num_eps': 10,                              # self play times in per iter\n    'num_train_threads': 10,                    # self play in parallel\n    'num_explore': 5,                           # explore step in a game\n    'temp': 1,                                  # temperature\n    'dirichlet_alpha': 0.3,                     # action noise in self play games\n    'update_threshold': 0.55,                   # update model threshold\n    'num_contest': 10,                          # new/old model compare times\n    'check_freq': 20,                           # test model frequency\n    'examples_buffer_max_len': 20,              # max length of examples buffer\n\n    # test\n    'human_color': 1                            # human player's color\n}\n\n# action size\nconfig['action_size'] = config['n'] ** 2\n"""
src/gomoku_gui.py,0,"b'# -*- coding: utf-8 -*-\nimport pygame\nimport os\nimport numpy as np\n\nclass GomokuGUI():\n    def __init__(self, n, human_color=1, fps=3):\n\n        # color, white for player 1, black for player -1\n        self.white = (255, 255, 255)\n        self.black = (0, 0, 0)\n        self.green = (0, 255, 0)\n\n        # screen\n        self.width = 700 if n > 10 else 500\n        self.height = 700 if n > 10 else 500\n\n        self.n = n\n        self.grid_width = self.width / (self.n + 3)\n        self.fps = fps\n\n        # human color\n        self.human_color = human_color\n\n        # reset status\n        self.reset_status()\n\n    def __del__(self):\n        # close window\n        self.is_running = False\n\n    # reset status\n    def reset_status(self):\n        self.board = np.zeros((self.n, self.n), dtype=int)\n        self.number = np.zeros((self.n, self.n), dtype=int)\n        self.k = 1 # step number\n\n        self.is_human = False\n        self.human_move = -1\n\n    # human play\n    def set_is_human(self, value=True):\n        self.is_human = value\n\n    def get_is_human(self):\n        return self.is_human\n\n    def get_human_move(self):\n        return self.human_move\n\n    def get_human_color(self):\n        return self.human_color\n\n    # execute move\n    def execute_move(self, color, move):\n        x, y = move // self.n, move % self.n\n        assert self.board[x][y] == 0\n\n        self.board[x][y] = color\n        self.number[x][y] = self.k\n        self.k += 1\n\n    # main loop\n    def loop(self):\n        # set running\n        self.is_running = True\n\n        # init\n        pygame.init()\n        self.screen = pygame.display.set_mode((self.width, self.height))\n        pygame.display.set_caption(""Gomoku"")\n\n        # timer\n        self.clock = pygame.time.Clock()\n\n        # background image\n        base_folder = os.path.dirname(__file__)\n        self.background_img = pygame.image.load(\n            os.path.join(base_folder, \'../assets/background.png\')).convert()\n\n        # font\n        self.font = pygame.font.SysFont(\'Arial\', 16)\n\n        while self.is_running:\n            # timer\n            self.clock.tick(self.fps)\n\n            # handle event\n            for event in pygame.event.get():\n                # close window\n                if event.type == pygame.QUIT:\n                    self.is_running = False\n\n                # human play\n                if self.is_human and event.type == pygame.MOUSEBUTTONDOWN:\n                    mouse_y, mouse_x = event.pos\n                    position = (int(mouse_x / self.grid_width + 0.5) - 2,\n                              int(mouse_y / self.grid_width + 0.5) - 2)\n\n                    if position[0] in range(0, self.n) and position[1] in range(0, self.n) \\\n                            and self.board[position[0]][position[1]] == 0:\n                        self.human_move = position[0] * self.n + position[1]\n                        self.execute_move(self.human_color, self.human_move)\n                        self.set_is_human(False)\n\n\n            # draw\n            self._draw_background()\n            self._draw_chessman()\n\n            # refresh\n            pygame.display.flip()\n\n    def _draw_background(self):\n        # load background\n        self.screen.blit(self.background_img, (0, 0))\n\n        # draw lines\n        rect_lines = [\n            ((self.grid_width, self.grid_width),\n             (self.grid_width, self.height - self.grid_width)),\n            ((self.grid_width, self.grid_width), (self.width - self.grid_width,\n                                                  self.grid_width)),\n            ((self.grid_width, self.height - self.grid_width),\n             (self.width - self.grid_width, self.height - self.grid_width)),\n            ((self.width - self.grid_width, self.grid_width),\n             (self.width - self.grid_width, self.height - self.grid_width)),\n        ]\n        for line in rect_lines:\n            pygame.draw.line(self.screen, self.black, line[0], line[1], 2)\n\n        # draw grid\n        for i in range(self.n):\n            pygame.draw.line(\n                self.screen, self.black,\n                (self.grid_width * (2 + i), self.grid_width),\n                (self.grid_width * (2 + i), self.height - self.grid_width))\n            pygame.draw.line(\n                self.screen, self.black,\n                (self.grid_width, self.grid_width * (2 + i)),\n                (self.height - self.grid_width, self.grid_width * (2 + i)))\n\n    def _draw_chessman(self):\n        # draw chessmen\n        for i in range(self.n):\n            for j in range(self.n):\n                if self.board[i][j] != 0:\n                    # circle\n                    position = (int(self.grid_width * (j + 2)),\n                              int(self.grid_width * (i + 2)))\n                    color = self.white if self.board[i][j] == 1 else self.black\n                    pygame.draw.circle(self.screen, color, position,\n                                       int(self.grid_width / 2.3))\n                    # text\n                    position = (position[0] - 8, position[1] - 8)\n                    color = self.white if self.board[i][j] == -1 else self.black\n                    text = self.font.render(str(self.number[i][j]), 3, color)\n                    self.screen.blit(text, position)\n'"
src/learner.py,1,"b'from collections import deque\nfrom os import path, mkdir\nimport threading\nimport time\nimport math\nimport numpy as np\nimport pickle\nimport concurrent.futures\nimport random\nfrom functools import reduce\n\nimport sys\nsys.path.append(\'../build\')\nfrom library import MCTS, Gomoku, NeuralNetwork\n\nfrom neural_network import NeuralNetWorkWrapper\nfrom gomoku_gui import GomokuGUI\n\ndef tuple_2d_to_numpy_2d(tuple_2d):\n    # help function\n    # convert type\n    res = [None] * len(tuple_2d)\n    for i, tuple_1d in enumerate(tuple_2d):\n        res[i] = list(tuple_1d)\n    return np.array(res)\n\n\nclass Leaner():\n    def __init__(self, config):\n        # see config.py\n        # gomoku\n        self.n = config[\'n\']\n        self.n_in_row = config[\'n_in_row\']\n        self.gomoku_gui = GomokuGUI(config[\'n\'], config[\'human_color\'])\n        self.action_size = config[\'action_size\']\n\n        # train\n        self.num_iters = config[\'num_iters\']\n        self.num_eps = config[\'num_eps\']\n        self.num_train_threads = config[\'num_train_threads\']\n        self.check_freq = config[\'check_freq\']\n        self.num_contest = config[\'num_contest\']\n        self.dirichlet_alpha = config[\'dirichlet_alpha\']\n        self.temp = config[\'temp\']\n        self.update_threshold = config[\'update_threshold\']\n        self.num_explore = config[\'num_explore\']\n\n        self.examples_buffer = deque([], maxlen=config[\'examples_buffer_max_len\'])\n\n        # mcts\n        self.num_mcts_sims = config[\'num_mcts_sims\']\n        self.c_puct = config[\'c_puct\']\n        self.c_virtual_loss = config[\'c_virtual_loss\']\n        self.num_mcts_threads = config[\'num_mcts_threads\']\n        self.libtorch_use_gpu = config[\'libtorch_use_gpu\']\n\n        # neural network\n        self.batch_size = config[\'batch_size\']\n        self.epochs = config[\'epochs\']\n        self.nnet = NeuralNetWorkWrapper(config[\'lr\'], config[\'l2\'], config[\'num_layers\'],\n                                         config[\'num_channels\'], config[\'n\'], self.action_size, config[\'train_use_gpu\'], self.libtorch_use_gpu)\n\n        # start gui\n        t = threading.Thread(target=self.gomoku_gui.loop)\n        t.start()\n\n    def learn(self):\n        # train the model by self play\n\n        if path.exists(path.join(\'models\', \'checkpoint.example\')):\n            print(""loading checkpoint..."")\n            self.nnet.load_model()\n            self.load_samples()\n        else:\n            # save torchscript\n            self.nnet.save_model()\n            self.nnet.save_model(\'models\', ""best_checkpoint"")\n\n        for itr in range(1, self.num_iters + 1):\n            print(""ITER :: {}"".format(itr))\n\n            # self play in parallel\n            libtorch = NeuralNetwork(\'./models/checkpoint.pt\',\n                                     self.libtorch_use_gpu, self.num_mcts_threads * self.num_train_threads)\n            itr_examples = []\n            with concurrent.futures.ThreadPoolExecutor(max_workers=self.num_train_threads) as executor:\n                futures = [executor.submit(self.self_play, 1 if itr % 2 else -1, libtorch, k == 1) for k in range(1, self.num_eps + 1)]\n                for k, f in enumerate(futures):\n                    examples = f.result()\n                    itr_examples += examples\n\n                    # decrease libtorch batch size\n                    remain = min(len(futures) - (k + 1), self.num_train_threads)\n                    libtorch.set_batch_size(max(remain * self.num_mcts_threads, 1))\n                    print(""EPS: {}, EXAMPLES: {}"".format(k + 1, len(examples)))\n\n            # release gpu memory\n            del libtorch\n\n            # prepare train data\n            self.examples_buffer.append(itr_examples)\n            train_data = reduce(lambda a, b : a + b, self.examples_buffer)\n            random.shuffle(train_data)\n\n            # train neural network\n            epochs = self.epochs * (len(itr_examples) + self.batch_size - 1) // self.batch_size\n            self.nnet.train(train_data, self.batch_size, int(epochs))\n            self.nnet.save_model()\n            self.save_samples()\n\n            # compare performance\n            if itr % self.check_freq == 0:\n                libtorch_current = NeuralNetwork(\'./models/checkpoint.pt\',\n                                         self.libtorch_use_gpu, self.num_mcts_threads * self.num_train_threads // 2)\n                libtorch_best = NeuralNetwork(\'./models/best_checkpoint.pt\',\n                                              self.libtorch_use_gpu, self.num_mcts_threads * self.num_train_threads // 2)\n\n                one_won, two_won, draws = self.contest(libtorch_current, libtorch_best, self.num_contest)\n                print(""NEW/PREV WINS : %d / %d ; DRAWS : %d"" % (one_won, two_won, draws))\n\n                if one_won + two_won > 0 and float(one_won) / (one_won + two_won) > self.update_threshold:\n                    print(\'ACCEPTING NEW MODEL\')\n                    self.nnet.save_model(\'models\', ""best_checkpoint"")\n                else:\n                    print(\'REJECTING NEW MODEL\')\n\n                # release gpu memory\n                del libtorch_current\n                del libtorch_best\n\n    def self_play(self, first_color, libtorch, show):\n        """"""\n        This function executes one episode of self-play, starting with player 1.\n        As the game is played, each turn is added as a training example to\n        train_examples. The game is played till the game ends. After the game\n        ends, the outcome of the game is used to assign values to each example\n        in train_examples.\n        """"""\n        train_examples = []\n\n        player1 = MCTS(libtorch, self.num_mcts_threads, self.c_puct,\n                    self.num_mcts_sims, self.c_virtual_loss, self.action_size)\n        player2 = MCTS(libtorch, self.num_mcts_threads, self.c_puct,\n            self.num_mcts_sims, self.c_virtual_loss, self.action_size)\n        players = [player2, None, player1]\n        player_index = 1\n\n        gomoku = Gomoku(self.n, self.n_in_row, first_color)\n\n        if show:\n            self.gomoku_gui.reset_status()\n\n        episode_step = 0\n        while True:\n            episode_step += 1\n            player = players[player_index + 1]\n\n            # get action prob\n            if episode_step <= self.num_explore:\n                prob = np.array(list(player.get_action_probs(gomoku, self.temp)))\n            else:\n                prob = np.array(list(player.get_action_probs(gomoku, 0)))\n\n            # generate sample\n            board = tuple_2d_to_numpy_2d(gomoku.get_board())\n            last_action = gomoku.get_last_move()\n            cur_player = gomoku.get_current_color()\n\n            sym = self.get_symmetries(board, prob, last_action)\n            for b, p, a in sym:\n                train_examples.append([b, a, cur_player, p])\n\n            # dirichlet noise\n            legal_moves = list(gomoku.get_legal_moves())\n            noise = 0.1 * np.random.dirichlet(self.dirichlet_alpha * np.ones(np.count_nonzero(legal_moves)))\n\n            prob = 0.9 * prob\n            j = 0\n            for i in range(len(prob)):\n                if legal_moves[i] == 1:\n                    prob[i] += noise[j]\n                    j += 1\n            prob /= np.sum(prob)\n\n            # execute move\n            action = np.random.choice(len(prob), p=prob)\n\n            if show:\n                self.gomoku_gui.execute_move(cur_player, action)\n            gomoku.execute_move(action)\n            player1.update_with_move(action)\n            player2.update_with_move(action)\n\n            # next player\n            player_index = -player_index\n\n            # is ended\n            ended, winner = gomoku.get_game_status()\n            if ended == 1:\n                # b, last_action, cur_player, p, v\n                return [(x[0], x[1], x[2], x[3], x[2] * winner) for x in train_examples]\n\n    def contest(self, network1, network2, num_contest):\n        """"""compare new and old model\n           Args: player1, player2 is neural network\n           Return: one_won, two_won, draws\n        """"""\n        one_won, two_won, draws = 0, 0, 0\n\n        with concurrent.futures.ThreadPoolExecutor(max_workers=self.num_train_threads) as executor:\n            futures = [executor.submit(\\\n                self._contest, network1, network2, 1 if k <= num_contest // 2 else -1, k == 1) for k in range(1, num_contest + 1)]\n            for f in futures:\n                winner = f.result()\n                if winner == 1:\n                    one_won += 1\n                elif winner == -1:\n                    two_won += 1\n                else:\n                    draws += 1\n\n        return one_won, two_won, draws\n\n    def _contest(self, network1, network2, first_player, show):\n        # create MCTS\n        player1 = MCTS(network1, self.num_mcts_threads, self.c_puct,\n            self.num_mcts_sims, self.c_virtual_loss, self.action_size)\n        player2 = MCTS(network2, self.num_mcts_threads, self.c_puct,\n                    self.num_mcts_sims, self.c_virtual_loss, self.action_size)\n\n        # prepare\n        players = [player2, None, player1]\n        player_index = first_player\n        gomoku = Gomoku(self.n, self.n_in_row, first_player)\n        if show:\n            self.gomoku_gui.reset_status()\n\n        # play\n        while True:\n            player = players[player_index + 1]\n\n            # select best move\n            prob = player.get_action_probs(gomoku)\n            best_move = int(np.argmax(np.array(list(prob))))\n\n            # execute move\n            gomoku.execute_move(best_move)\n            if show:\n                self.gomoku_gui.execute_move(player_index, best_move)\n\n            # check game status\n            ended, winner = gomoku.get_game_status()\n            if ended == 1:\n                return winner\n\n            # update search tree\n            player1.update_with_move(best_move)\n            player2.update_with_move(best_move)\n\n            # next player\n            player_index = -player_index\n\n    def get_symmetries(self, board, pi, last_action):\n        # mirror, rotational\n        assert(len(pi) == self.action_size)  # 1 for pass\n\n        pi_board = np.reshape(pi, (self.n, self.n))\n        last_action_board = np.zeros((self.n, self.n))\n        last_action_board[last_action // self.n][last_action % self.n] = 1\n        l = []\n\n        for i in range(1, 5):\n            for j in [True, False]:\n                newB = np.rot90(board, i)\n                newPi = np.rot90(pi_board, i)\n                newAction = np.rot90(last_action_board, i)\n                if j:\n                    newB = np.fliplr(newB)\n                    newPi = np.fliplr(newPi)\n                    newAction = np.fliplr(last_action_board)\n                l += [(newB, newPi.ravel(), np.argmax(newAction) if last_action != -1 else -1)]\n        return l\n\n    def play_with_human(self, human_first=True, checkpoint_name=""best_checkpoint""):\n        # load best model\n        libtorch_best = NeuralNetwork(\'./models/best_checkpoint.pt\', self.libtorch_use_gpu, 12)\n        mcts_best = MCTS(libtorch_best, self.num_mcts_threads * 3, \\\n             self.c_puct, self.num_mcts_sims * 6, self.c_virtual_loss, self.action_size)\n\n        # create gomoku game\n        human_color = self.gomoku_gui.get_human_color()\n        gomoku = Gomoku(self.n, self.n_in_row, human_color if human_first else -human_color)\n\n        players = [""alpha"", None, ""human""] if human_color == 1 else [""human"", None, ""alpha""]\n        player_index = human_color if human_first else -human_color\n\n        self.gomoku_gui.reset_status()\n\n        while True:\n            player = players[player_index + 1]\n\n            # select move\n            if player == ""alpha"":\n                prob = mcts_best.get_action_probs(gomoku)\n                best_move = int(np.argmax(np.array(list(prob))))\n                self.gomoku_gui.execute_move(player_index, best_move)\n            else:\n                self.gomoku_gui.set_is_human(True)\n                # wait human action\n                while self.gomoku_gui.get_is_human():\n                    time.sleep(0.1)\n                best_move = self.gomoku_gui.get_human_move()\n\n            # execute move\n            gomoku.execute_move(best_move)\n\n            # check game status\n            ended, winner = gomoku.get_game_status()\n            if ended == 1:\n                break\n\n            # update tree search\n            mcts_best.update_with_move(best_move)\n\n            # next player\n            player_index = -player_index\n\n        print(""HUMAN WIN"" if winner == human_color else ""ALPHA ZERO WIN"")\n\n    def load_samples(self, folder=""models"", filename=""checkpoint.example""):\n        """"""load self.examples_buffer\n        """"""\n\n        filepath = path.join(folder, filename)\n        with open(filepath, \'rb\') as f:\n            self.examples_buffer = pickle.load(f)\n\n    def save_samples(self, folder=""models"", filename=""checkpoint.example""):\n        """"""save self.examples_buffer\n        """"""\n\n        if not path.exists(folder):\n            mkdir(folder)\n\n        filepath = path.join(folder, filename)\n        with open(filepath, \'wb\') as f:\n            pickle.dump(self.examples_buffer, f, -1)\n'"
src/neural_network.py,18,"b'# -*- coding: utf-8 -*-\nimport sys\nimport os\nimport random\n\nimport torch\nimport torch.nn as nn\nfrom torch.optim import Adam\nfrom torch.autograd import Variable\nimport torch.nn.functional as F\n\nimport numpy as np\n\n\ndef conv3x3(in_channels, out_channels, stride=1):\n    # 3x3 convolution\n    return nn.Conv2d(in_channels, out_channels, kernel_size=3,\n                     stride=stride, padding=1, bias=False)\n\n\nclass ResidualBlock(nn.Module):\n    # Residual block\n    def __init__(self, in_channels, out_channels, stride=1):\n        super(ResidualBlock, self).__init__()\n        self.conv1 = conv3x3(in_channels, out_channels, stride)\n        self.bn1 = nn.BatchNorm2d(out_channels)\n        self.relu = nn.ReLU(inplace=True)\n\n        self.conv2 = conv3x3(out_channels, out_channels)\n        self.bn2 = nn.BatchNorm2d(out_channels)\n\n        self.downsample = False\n        if in_channels != out_channels or stride != 1:\n            self.downsample = True\n            self.downsample_conv = conv3x3(in_channels, out_channels, stride=stride)\n            self.downsample_bn = nn.BatchNorm2d(out_channels)\n\n    def forward(self, x):\n        residual = x\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n        out = self.conv2(out)\n        out = self.bn2(out)\n\n        if self.downsample:\n            residual = self.downsample_conv(residual)\n            residual = self.downsample_bn(residual)\n\n        out += residual\n        out = self.relu(out)\n        return out\n\n\nclass NeuralNetWork(nn.Module):\n    """"""Policy and Value Network\n    """"""\n\n    def __init__(self, num_layers, num_channels, n, action_size):\n        super(NeuralNetWork, self).__init__()\n\n        # residual block\n        res_list = [ResidualBlock(3, num_channels)] + [ResidualBlock(num_channels, num_channels) for _ in range(num_layers - 1)]\n        self.res_layers = nn.Sequential(*res_list)\n\n        # policy head\n        self.p_conv = nn.Conv2d(num_channels, 4, kernel_size=1, padding=0, bias=False)\n        self.p_bn = nn.BatchNorm2d(num_features=4)\n        self.relu = nn.ReLU(inplace=True)\n\n        self.p_fc = nn.Linear(4 * n ** 2, action_size)\n        self.log_softmax = nn.LogSoftmax(dim=1)\n\n        # value head\n        self.v_conv = nn.Conv2d(num_channels, 2, kernel_size=1, padding=0, bias=False)\n        self.v_bn = nn.BatchNorm2d(num_features=2)\n\n        self.v_fc1 = nn.Linear(2 * n ** 2, 256)\n        self.v_fc2 = nn.Linear(256, 1)\n        self.tanh = nn.Tanh()\n\n    def forward(self, inputs):\n        # residual block\n        out = self.res_layers(inputs)\n\n        # policy head\n        p = self.p_conv(out)\n        p = self.p_bn(p)\n        p = self.relu(p)\n\n        p = self.p_fc(p.view(p.size(0), -1))\n        p = self.log_softmax(p)\n\n        # value head\n        v = self.v_conv(out)\n        v = self.v_bn(v)\n        v = self.relu(v)\n\n        v = self.v_fc1(v.view(v.size(0), -1))\n        v = self.relu(v)\n        v = self.v_fc2(v)\n        v = self.tanh(v)\n\n        return p, v\n\n\nclass AlphaLoss(nn.Module):\n    """"""\n    Custom loss as defined in the paper :\n    (z - v) ** 2 --> MSE Loss\n    (-pi * logp) --> Cross Entropy Loss\n    z : self_play_winner\n    v : winner\n    pi : self_play_probas\n    p : probas\n\n    The loss is then averaged over the entire batch\n    """"""\n\n    def __init__(self):\n        super(AlphaLoss, self).__init__()\n\n    def forward(self, log_ps, vs, target_ps, target_vs):\n        value_loss = torch.mean(torch.pow(vs - target_vs, 2))\n        policy_loss = -torch.mean(torch.sum(target_ps * log_ps, 1))\n\n        return value_loss + policy_loss\n\n\nclass NeuralNetWorkWrapper():\n    """"""train and predict\n    """"""\n\n    def __init__(self, lr, l2, num_layers, num_channels, n, action_size, train_use_gpu=True, libtorch_use_gpu=True):\n        """""" init\n        """"""\n        self.lr = lr\n        self.l2 = l2\n        self.num_channels = num_channels\n        self.n = n\n\n        self.libtorch_use_gpu = libtorch_use_gpu\n        self.train_use_gpu = train_use_gpu\n\n        self.neural_network = NeuralNetWork(num_layers, num_channels, n, action_size)\n        if self.train_use_gpu:\n            self.neural_network.cuda()\n\n        self.optim = Adam(self.neural_network.parameters(), lr=self.lr, weight_decay=self.l2)\n        self.alpha_loss = AlphaLoss()\n\n    def train(self, example_buffer, batch_size, epochs):\n        """"""train neural network\n        """"""\n        for epo in range(1, epochs + 1):\n            self.neural_network.train()\n\n            # sample\n            train_data = random.sample(example_buffer, batch_size)\n\n            # extract train data\n            board_batch, last_action_batch, cur_player_batch, p_batch, v_batch = list(zip(*train_data))\n\n            state_batch = self._data_convert(board_batch, last_action_batch, cur_player_batch)\n            p_batch = torch.Tensor(p_batch).cuda() if self.train_use_gpu else torch.Tensor(p_batch)\n            v_batch = torch.Tensor(v_batch).unsqueeze(\n                1).cuda() if self.train_use_gpu else torch.Tensor(v_batch).unsqueeze(1)\n\n            # zero the parameter gradients\n            self.optim.zero_grad()\n\n            # forward + backward + optimize\n            log_ps, vs = self.neural_network(state_batch)\n            loss = self.alpha_loss(log_ps, vs, p_batch, v_batch)\n            loss.backward()\n\n            self.optim.step()\n\n            # calculate entropy\n            new_p, _ = self._infer(state_batch)\n\n            entropy = -np.mean(\n                np.sum(new_p * np.log(new_p + 1e-10), axis=1)\n            )\n\n            print(""EPOCH: {}, LOSS: {}, ENTROPY: {}"".format(epo, loss.item(), entropy))\n\n    def infer(self, feature_batch):\n        """"""predict p and v by raw input\n           return numpy\n        """"""\n        board_batch, last_action_batch, cur_player_batch = list(zip(*feature_batch))\n        states = self._data_convert(board_batch, last_action_batch, cur_player_batch)\n\n        self.neural_network.eval()\n        log_ps, vs = self.neural_network(states)\n\n        return np.exp(log_ps.cpu().detach().numpy()), vs.cpu().detach().numpy()\n\n    def _infer(self, state_batch):\n        """"""predict p and v by state\n           return numpy object\n        """"""\n\n        self.neural_network.eval()\n        log_ps, vs = self.neural_network(state_batch)\n\n        return np.exp(log_ps.cpu().detach().numpy()), vs.cpu().detach().numpy()\n\n    def _data_convert(self, board_batch, last_action_batch, cur_player_batch):\n        """"""convert data format\n           return tensor\n        """"""\n        n = self.n\n\n        board_batch = torch.Tensor(board_batch).unsqueeze(1)\n        state0 = (board_batch > 0).float()\n        state1 = (board_batch < 0).float()\n\n        state2 = torch.zeros((len(last_action_batch), 1, n, n)).float()\n\n        for i in range(len(board_batch)):\n            if cur_player_batch[i] == -1:\n                temp = state0[i].clone()\n                state0[i].copy_(state1[i])\n                state1[i].copy_(temp)\n\n            last_action = last_action_batch[i]\n            if last_action != -1:\n                x, y = last_action // self.n, last_action % self.n\n                state2[i][0][x][y] = 1\n\n        res =  torch.cat((state0, state1, state2), dim=1)\n        # res = torch.cat((state0, state1), dim=1)\n        return res.cuda() if self.train_use_gpu else res\n\n    def set_learning_rate(self, lr):\n        """"""set learning rate\n        """"""\n\n        for param_group in self.optim.param_groups:\n            param_group[\'lr\'] = lr\n\n    def load_model(self, folder=""models"", filename=""checkpoint""):\n        """"""load model from file\n        """"""\n\n        filepath = os.path.join(folder, filename)\n        state = torch.load(filepath)\n        self.neural_network.load_state_dict(state[\'network\'])\n        self.optim.load_state_dict(state[\'optim\'])\n\n    def save_model(self, folder=""models"", filename=""checkpoint""):\n        """"""save model to file\n        """"""\n\n        if not os.path.exists(folder):\n            os.mkdir(folder)\n\n        filepath = os.path.join(folder, filename)\n        state = {\'network\':self.neural_network.state_dict(), \'optim\':self.optim.state_dict()}\n        torch.save(state, filepath)\n\n        # save torchscript\n        filepath += \'.pt\'\n        self.neural_network.eval()\n\n        if self.libtorch_use_gpu:\n            self.neural_network.cuda()\n            example = torch.rand(1, 3, self.n, self.n).cuda()\n        else:\n            self.neural_network.cpu()\n            example = torch.rand(1, 3, self.n, self.n).cpu()\n\n        traced_script_module = torch.jit.trace(self.neural_network, example)\n        traced_script_module.save(filepath)\n\n        if self.train_use_gpu:\n            self.neural_network.cuda()\n        else:\n            self.neural_network.cpu()\n'"
test/gomoku_gui_test.py,0,"b'# -*- coding: utf-8 -*-\nimport sys\nsys.path.append(\'../src\')\n\nimport numpy as np\nimport threading\nimport gomoku_gui\n\n\nif __name__ == ""__main__"":\n    gomoku_gui = gomoku_gui.GomokuGUI(10)\n    t = threading.Thread(target=gomoku_gui.loop)\n    t.start()\n\n    # test\n    gomoku_gui.execute_move(1, 0)\n    gomoku_gui.execute_move(-1, 1)\n    gomoku_gui.execute_move(1, 2)\n    gomoku_gui.execute_move(-1, 3)\n    gomoku_gui.set_is_human(True)\n\n    t.join()\n'"
test/leaner_test.py,0,"b'# coding: utf-8\nimport sys\nsys.path.append(\'..\')\nsys.path.append(\'../src\')\n\nimport learner\nimport config\n\nif __name__ == ""__main__"":\n    if len(sys.argv) < 2 or sys.argv[1] not in [""train"", ""play""]:\n        print(""[USAGE] python leaner_test.py train|play"")\n        exit(1)\n\n    alpha_zero = learner.Leaner(config.config)\n\n    if sys.argv[1] == ""train"":\n        alpha_zero.learn()\n    elif sys.argv[1] == ""play"":\n        for i in range(10):\n            print(""GAME: {}"".format(i + 1))\n            alpha_zero.play_with_human(human_first=i % 2)\n'"
test/library_test.py,0,"b'# coding: utf-8\nimport os\nimport sys\nsys.path.append(\'../build\')\n\nfrom library import Gomoku, MCTS\nimport numpy as np\nimport time\n\n\nif __name__ == ""__main__"":\n    gomoku = Gomoku(15, 5, 1)\n    gomoku.execute_move(0 + 40)\n    gomoku.execute_move(99)\n    gomoku.execute_move(1 + 40)\n    gomoku.execute_move(98)\n    gomoku.execute_move(2 + 40)\n    gomoku.execute_move(97)\n    gomoku.execute_move(3 + 40)\n    gomoku.execute_move(96)\n\n    gomoku.display()\n\n    mcts = MCTS(""./models/checkpoint.pt"", 4, 2.5, 1600, 2.5, 225, True)\n\n    print(""RUNNING"")\n    while True:\n        time_start=time.time()\n        res = mcts.get_action_probs(gomoku, 1)\n        time_end=time.time()\n        print(\'get_action_probs\', time_end - time_start)\n\n        print(list(res))\n        best_action = int(np.argmax(np.array(list(res))))\n        print(best_action, res[best_action])\n\n        mcts.update_with_move(-1)\n'"
test/neural_network_test.py,2,"b'# -*- coding: utf-8 -*-\nimport sys\nsys.path.append(\'../src\')\nsys.path.append(\'../build\')\n\nimport torch\nfrom library import Gomoku\nimport neural_network\nimport numpy as np\n\n\ndef tuple_2d_to_numpy_2d(tuple_2d):\n    # help function\n    # convert type\n    res = [None] * len(tuple_2d)\n    for i, tuple_1d in enumerate(tuple_2d):\n        res[i] = list(tuple_1d)\n    return np.array(res)\n\n\nif __name__ == ""__main__"":\n    lr = 0.002\n    l2 = 0.0002\n    epochs = 5\n    num_layers = 4\n    num_channels = 128\n    n = 5\n    action_size = n ** 2\n\n    policy_value_net = neural_network.NeuralNetWorkWrapper(lr, l2, num_layers, num_channels, n, action_size)\n\n    # test data convert\n    board_batch = [[[1, 0, -1, 0, -1], [1, 0, -1, 0, -1], [1, 0, -1, 0, -1], [1, 0, -1, 0, -1], [1, 0, -1, 0, -1]],\n                   [[1, 0, -1, 0, -1], [1, 0, -1, 0, -1], [1, 0, -1, 0, -1], [1, 0, -1, 0, -1], [1, 0, -1, 0, -1]]]\n    last_action_batch = [-1, 0]\n    cur_player_batch = [1, -1]\n\n    state_batch = policy_value_net._data_convert(board_batch, last_action_batch, cur_player_batch)\n    print(\'state \\n\', state_batch)\n\n    # test loss\n    p_batch = torch.Tensor([[1 / 25 for _ in range(25)], [1 / 25 for _ in range(25)]]).cuda()\n    v_batch = torch.Tensor([0.5, 0.5]).cuda()\n\n    log_p, v = policy_value_net.neural_network(state_batch.cuda())\n    print(\'p, v \\n\', np.exp(log_p.cpu().detach().numpy()), v.cpu())\n\n    loss = policy_value_net.alpha_loss(log_p, v, p_batch, v_batch.unsqueeze(1))\n\n    print(\'loss \\n\', loss.cpu())\n\n    # test train\n    example_batch = list(zip(board_batch, last_action_batch, cur_player_batch,\n                             p_batch.cpu().numpy().tolist(), v_batch.cpu().numpy().tolist()))\n    print(\'train\\n\', example_batch)\n\n    policy_value_net.train(example_batch, len(example_batch), epochs)\n\n    # test infer\n    print(\'infer \\n\', policy_value_net.infer(list(zip(board_batch, last_action_batch, cur_player_batch))))\n\n    # test libtorch\n    nn = neural_network.NeuralNetWorkWrapper(lr, l2, 4, 256, 15, 225, True, True)\n    nn.save_model(folder=""models"", filename=""checkpoint"")\n    # nn.load_model(folder=""models"", filename=""checkpoint"")\n\n    gomoku = Gomoku(15, 5, 1)\n    gomoku.execute_move(3)\n    gomoku.execute_move(4)\n    gomoku.execute_move(6)\n    gomoku.execute_move(23)\n    gomoku.execute_move(8)\n    gomoku.execute_move(9)\n    gomoku.execute_move(78)\n    gomoku.execute_move(0)\n    gomoku.execute_move(17)\n    gomoku.execute_move(7)\n    gomoku.execute_move(19)\n    gomoku.execute_move(67)\n    gomoku.execute_move(60)\n    gomoku.execute_move(14)\n    gomoku.execute_move(11)\n    gomoku.execute_move(2)\n    gomoku.execute_move(99)\n    gomoku.execute_move(10)\n    gomoku.execute_move(1)\n    gomoku.execute_move(5)\n    gomoku.execute_move(18)\n    gomoku.execute_move(12)\n    gomoku.execute_move(15)\n\n    feature_batch = [(tuple_2d_to_numpy_2d(gomoku.get_board()), gomoku.get_last_move(), gomoku.get_current_color())]\n    print(feature_batch)\n    print(nn.infer(feature_batch))\n\n    gomoku.execute_move(24)\n    feature_batch = [(tuple_2d_to_numpy_2d(gomoku.get_board()), gomoku.get_last_move(), gomoku.get_current_color())]\n    print(feature_batch)\n    print(nn.infer(feature_batch))\n\n'"
