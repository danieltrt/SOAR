file_path,api_count,code
setup.py,0,"b""#! /usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n# Copyright 2019 Kyoto University (Hirofumi Inaguma)\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n\nimport io\nimport os\nfrom setuptools import find_packages\nfrom setuptools import setup\n\n\nrequirements = {\n    'install': [\n        'configargparse>=1.0',\n        'editdistance>=0.5.3',\n        'kaldiio>=2.15.1',\n        'matplotlib>=3.1.2',\n        'nltk>=3.4.5'\n        'pandas>=1.0.0',\n        'pyyaml>=5.3',\n        'seaborn>=0.10.0',\n        'sentencepiece>= 0.1.85',\n        'setproctitle>=1.1.10',\n        'tensorboardX>=2.0',\n        'tqdm>=4.42.0',\n        'torch==1.0.0',\n    ],\n    'setup': [\n\n    ],\n    'test': [\n\n    ],\n    'doc': [\n\n    ]\n}\ninstall_requires = requirements['install']\nsetup_requires = requirements['setup']\ntests_require = requirements['test']\nextras_require = {k: v for k, v in requirements.items()\n                  if k not in ['install', 'setup']}\n\ndirname = os.path.dirname(__file__)\nsetup(name='neural_sp',\n      version='0.1.0',\n      url='http://github.com/neural_sp/neural_sp',\n      author='Hirofumi Inaguma',\n      author_email='hiro.mhbc@gmail.com',\n      description='NeuralSP: Neural network based end-to-end Speech Processing toolkit',\n      long_description=io.open(os.path.join(dirname, 'README.md'),\n                               encoding='utf-8').read(),\n      license='Apache Software License',\n      packages=find_packages(include=['neural_sp*']),\n      install_requires=install_requires,\n      setup_requires=setup_requires,\n      tests_require=tests_require,\n      extras_require=extras_require,\n      classifiers=[\n          'Programming Language :: Python',\n          'Programming Language :: Python :: 2.7',\n          'Programming Language :: Python :: 3',\n          'Development Status :: 5 - Production/Stable',\n          'Intended Audience :: Science/Research',\n          'Operating System :: POSIX :: Linux',\n          'License :: OSI Approved :: Apache Software License',\n          'Topic :: Software Development :: Libraries :: Python Modules'],\n      )\n"""
neural_sp/__init__.py,0,b''
neural_sp/utils.py,0,"b'#! /usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n# Copyright 2018 Kyoto University (Hirofumi Inaguma)\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n\n""""""Unility functions for general purposes.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\n\n\ndef mkdir_join(path, *dir_name):\n    """"""Concatenate root path and 1 or more paths, and make a new direcory if the direcory does not exist.\n    Args:\n        path (str): path to a diretcory\n        dir_name (str): a direcory name\n    Returns:\n        path to the new directory\n    """"""\n    if not os.path.isdir(path):\n        os.mkdir(path)\n    for i in range(len(dir_name)):\n        # dir\n        if i < len(dir_name) - 1:\n            path = os.path.join(path, dir_name[i])\n            if not os.path.isdir(path):\n                os.mkdir(path)\n        elif \'.\' not in dir_name[i]:\n            path = os.path.join(path, dir_name[i])\n            if not os.path.isdir(path):\n                os.mkdir(path)\n        # file\n        else:\n            path = os.path.join(path, dir_name[i])\n    return path\n'"
utils/compute_oov_rate.py,0,"b'#! /usr/bin/env python\n# -*- coding: utf-8 -*-\n\n# Copyright 2018 Kyoto University (Hirofumi Inaguma)\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n\n""""""Compute OOV (out-of-vocabylary) rate.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport argparse\nimport codecs\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\'word_count\', type=str,\n                    help=\'word count file\')\nparser.add_argument(\'dict\', type=str,\n                    help=\'dictionary file\')\nparser.add_argument(\'set\', type=str,\n                    help=\'dataset\')\nargs = parser.parse_args()\n\n\ndef main():\n\n    vocab = set([])\n    with codecs.open(args.dict, \'r\', encoding=""utf-8"") as f:\n        vocab = set([])\n        for line in f:\n            v, idx = line.strip().split(\' \')\n            vocab.add(v)\n\n    n_oovs = 0\n    n_words = 0\n    with codecs.open(args.word_count, \'r\', encoding=""utf-8"") as f:\n        for line in f:\n            line = line.strip()\n            if len(line.split(\' \')) == 1:\n                # Skip empty line\n                continue\n            count, w = line.split(\' \')\n\n            # For swbd\n            if w == \'(%hesitation)\':\n                continue\n\n            n_words += int(count)\n            if w not in vocab:\n                n_oovs += int(count)\n\n    oov_rate = float(n_oovs * 100) / float(n_words)\n    print(""%s: %.3f%%"" % (args.set, oov_rate))\n\n\nif __name__ == \'__main__\':\n    main()\n'"
utils/make_tsv.py,0,"b'#! /usr/bin/env python\n# -*- coding: utf-8 -*-\n\n# Copyright 2018 Kyoto University (Hirofumi Inaguma)\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n\n""""""Make a dataset tsv file.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport argparse\nimport codecs\nfrom distutils.util import strtobool\nimport kaldiio\nimport os\nimport re\nimport sentencepiece as spm\nfrom tqdm import tqdm\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\'--feat\', type=str, default=\'\', nargs=\'?\',\n                    help=\'feats.scp file\')\nparser.add_argument(\'--utt2num_frames\', type=str, nargs=\'?\',\n                    help=\'utt2num_frames file\')\nparser.add_argument(\'--utt2spk\', type=str, nargs=\'?\',\n                    help=\'utt2spk file\')\nparser.add_argument(\'--dict\', type=str,\n                    help=\'dictionary file\')\nparser.add_argument(\'--text\', type=str,\n                    help=\'text file\')\nparser.add_argument(\'--unit\', type=str,\n                    choices=[\'word\', \'wp\', \'char\', \'phone\', \'word_char\'],\n                    help=\'token units\')\nparser.add_argument(\'--remove_space\', type=strtobool, default=False,\n                    help=\'\')\nparser.add_argument(\'--unk\', type=str, default=\'<unk>\',\n                    help=\'<unk> token\')\nparser.add_argument(\'--space\', type=str, default=\'<space>\',\n                    help=\'<space> token\')\nparser.add_argument(\'--nlsyms\', type=str, default=\'\', nargs=\'?\',\n                    help=\'path to non-linguistic symbols, e.g., [noise] etc.\')\nparser.add_argument(\'--wp_model\', type=str, default=False, nargs=\'?\',\n                    help=\'prefix of the wordpiece model\')\nparser.add_argument(\'--wp_nbest\', type=int, default=1, nargs=\'?\',\n                    help=\'\')\nparser.add_argument(\'--update\', action=\'store_true\',\n                    help=\'\')\nargs = parser.parse_args()\n\n\ndef main():\n\n    nlsyms = []\n    if args.nlsyms:\n        with codecs.open(args.nlsyms, \'r\', encoding=""utf-8"") as f:\n            for line in f:\n                nlsyms.append(line.strip())\n\n    utt2featpath = {}\n    if args.feat:\n        with codecs.open(args.feat, \'r\', encoding=""utf-8"") as f:\n            for line in f:\n                utt_id, feat_path = line.strip().split(\' \')\n                utt2featpath[utt_id] = feat_path\n\n    utt2num_frames = {}\n    if args.utt2num_frames and os.path.isfile(args.utt2num_frames):\n        with codecs.open(args.utt2num_frames, \'r\', encoding=""utf-8"") as f:\n            for line in f:\n                utt_id, xlen = line.strip().split(\' \')\n                utt2num_frames[utt_id] = int(xlen)\n\n    utt2spk = {}\n    if args.utt2spk and os.path.isfile(args.utt2spk):\n        with codecs.open(args.utt2spk, \'r\', encoding=""utf-8"") as f:\n            for line in f:\n                utt_id, speaker = line.strip().split(\' \')\n                utt2spk[str(utt_id)] = speaker\n\n    token2idx = {}\n    idx2token = {}\n    with codecs.open(args.dict, \'r\', encoding=""utf-8"") as f:\n        for line in f:\n            token, idx = line.strip().split(\' \')\n            token2idx[token] = str(idx)\n            idx2token[str(idx)] = token\n\n    if args.unit == \'wp\':\n        sp = spm.SentencePieceProcessor()\n        sp.Load(args.wp_model + \'.model\')\n\n    if not args.update:\n        print(\'utt_id\\tspeaker\\tfeat_path\\txlen\\txdim\\ttext\\ttoken_id\\tylen\\tydim\\tprev_utt\')\n\n    xdim = None\n    pbar = tqdm(total=len(codecs.open(args.text, \'r\', encoding=""utf-8"").readlines()))\n\n    # Sort by 1.session and 2.onset\n    if \'swbd\' in args.text and not args.update:\n        lines = [line.strip() for line in codecs.open(args.text, \'r\', encoding=""utf-8"")]\n        lines = sorted(lines, key=lambda x: (str(utt2spk[x.split(\' \')[0]]).split(\'-\')[0],\n                                             int(x.split(\' \')[0].split(\'_\')[-1].split(\'-\')[0])))\n    else:\n        lines = codecs.open(args.text, \'r\', encoding=""utf-8"")\n\n    for line in lines:\n        # Remove succesive spaces\n        line = re.sub(r\'[\\s]+\', \' \', line.strip())\n        utt_id = str(line.split(\' \')[0])\n        words = line.split(\' \')[1:]\n        if \'\' in words:\n            words.remove(\'\')\n        text = \' \'.join(words)\n\n        # Skip empty line\n        if text == \'\':\n            continue\n\n        if args.feat:\n            feat_path = utt2featpath[utt_id]\n            if utt_id in utt2num_frames.keys():\n                xlen = utt2num_frames[utt_id]\n            else:\n                xlen = kaldiio.load_mat(feat_path).shape[-2]\n            speaker = utt2spk[utt_id]\n\n            if not os.path.isfile(feat_path.split(\':\')[0]):\n                raise ValueError(\'There is no file: %s\' % feat_path)\n        else:\n            # dummy for LM\n            feat_path = \'\'\n            xlen = 0\n            speaker = \'\'\n\n        # Convert strings into the corresponding indices\n        token_ids = []\n        if args.unit in [\'word\', \'word_char\']:\n            for w in words:\n                if w in token2idx.keys():\n                    token_ids.append(token2idx[w])\n                else:\n                    # Replace with <unk>\n                    if args.unit == \'word_char\':\n                        for c in list(w):\n                            if c in token2idx.keys():\n                                token_ids.append(token2idx[c])\n                            else:\n                                token_ids.append(token2idx[args.unk])\n                    else:\n                        token_ids.append(token2idx[args.unk])\n\n        elif args.unit == \'wp\':\n            # Remove space before the first special symbol\n            wps = sp.EncodeAsPieces(text)\n            if wps[0] == \'\xe2\x96\x81\' and wps[1][0] == \'<\':\n                wps = wps[1:]\n\n            for wp in wps:\n                if wp in token2idx.keys():\n                    token_ids.append(token2idx[wp])\n                else:\n                    # Replace with <unk>\n                    token_ids.append(token2idx[args.unk])\n\n        elif args.unit == \'char\':\n            for i, w in enumerate(words):\n                if w in nlsyms:\n                    token_ids.append(token2idx[w])\n                else:\n                    for c in list(w):\n                        if c in token2idx.keys():\n                            token_ids.append(token2idx[c])\n                        else:\n                            # Replace with <unk>\n                            token_ids.append(token2idx[args.unk])\n\n                # Insert <space> mark\n                if not args.remove_space:\n                    if i < len(words) - 1:\n                        token_ids.append(token2idx[args.space])\n\n        elif args.unit == \'phone\':\n            for p in words:\n                token_ids.append(token2idx[p])\n\n        else:\n            raise ValueError(args.unit)\n        token_id = \' \'.join(token_ids)\n        ylen = len(token_ids)\n\n        if xdim is None:\n            if args.feat:\n                xdim = kaldiio.load_mat(feat_path).shape[-1]\n            else:\n                xdim = 0\n        ydim = len(token2idx.keys())\n\n        print(\'%s\\t%s\\t%s\\t%d\\t%d\\t%s\\t%s\\t%d\\t%d\' %\n              (utt_id, speaker, feat_path, xlen, xdim, text, token_id, ylen, ydim))\n\n        # data augmentation for wordpiece\n        if args.unit == \'wp\' and args.wp_nbest > 1:\n            raise NotImplementedError\n\n            for wp_i in sp.NBestEncodeAsPieces(text, args.wp_nbest)[1:]:\n                if wp_i in token2idx.keys():\n                    token_ids = token2idx[wp_i]\n                else:\n                    # Replace with <unk>\n                    token_ids = token2idx[args.unk]\n\n                token_id = \' \'.join(token_ids)\n                ylen = len(token_ids)\n\n                print(\'%s\\t%s\\t%s\\t%d\\t%d\\t%s\\t%s\\t%d\\t%d\' %\n                      (utt_id, speaker, feat_path, xlen, xdim, text, token_id, ylen, ydim))\n\n        pbar.update(1)\n\n\nif __name__ == \'__main__\':\n    main()\n'"
utils/map2phone.py,0,"b'#! /usr/bin/env python\n# -*- coding: utf-8 -*-\n\n# Copyright 2019 Kyoto University (Hirofumi Inaguma)\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n\n""""""Map transcription to phone sequence with a lexicon.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport argparse\nimport codecs\nimport re\nfrom tqdm import tqdm\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\'--text\', type=str,\n                    help=\'text file\')\nparser.add_argument(\'--lexicon\', type=str, default=\'\',\n                    help=\'path to lexicon\')\nargs = parser.parse_args()\n\n\ndef main():\n\n    word2phone = {}\n    with codecs.open(args.lexicon, \'r\', encoding=""utf-8"") as f:\n        for line in f:\n            word = line.strip().split(\' \')[0]\n            word = word.split(\'+\')[0]  # for CSJ\n            phone_seq = \' \'.join(line.strip().split(\' \')[1:])\n            word2phone[word] = phone_seq\n\n    utt_count = 0\n    with codecs.open(args.text, \'r\', encoding=""utf-8"") as f:\n        pbar = tqdm(total=len(codecs.open(args.text, \'r\', encoding=""utf-8"").readlines()))\n        for line in f:\n            # Remove succesive spaces\n            line = re.sub(r\'[\\s]+\', \' \', line.strip())\n            utt_id = line.split(\' \')[0]\n            words = line.split(\' \')[1:]\n            if \'\' in words:\n                words.remove(\'\')\n\n            phones = []\n            for w in words:\n                phones += word2phone[w].split()\n            text_phone = \' \'.join(phones)\n\n            print(\'%s %s\' % (utt_id, text_phone))\n            utt_count += 1\n            pbar.update(1)\n\n\nif __name__ == \'__main__\':\n    main()\n'"
utils/text2dict.py,0,"b'#! /usr/bin/env python\n# -*- coding: utf-8 -*-\n\n# Copyright 2018 Kyoto University (Hirofumi Inaguma)\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n\n""""""Make a dictionary file.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport argparse\nimport codecs\nfrom distutils.util import strtobool\nfrom tqdm import tqdm\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\'text\', type=str,\n                    help=\'path to text file\')\nparser.add_argument(\'--unit\', type=str,\n                    choices=[\'word\', \'wp\', \'char\', \'phone\', \'word_char\'],\n                    help=\'token units\')\nparser.add_argument(\'--vocab_size\', type=int, nargs=\'?\',\n                    help=\'the size of vocabulary for word and wordpiece.\')\nparser.add_argument(\'--remove_word_boundary\', action=\'store_false\',\n                    help=\'remove all whitespaces in the transcriptions\')\nparser.add_argument(\'--nlsyms\', type=str, default=False, nargs=\'?\',\n                    help=\'path to non-linguistic symbols, e.g., <NOISE> etc.\')\nparser.add_argument(\'--speed_perturb\', type=strtobool, default=False,\n                    help=\'use speed perturbation.\')\nargs = parser.parse_args()\n\n\n# TODO(hirofumi): python sentencepiece shows different behaviors from bash command.\n\ndef main():\n\n    nlsyms = []\n    if args.nlsyms:\n        with codecs.open(args.nlsyms, \'r\', encoding=""utf-8"") as f:\n            for line in f:\n                nlsyms.append(line.strip())\n\n    if args.unit == \'wp\':\n        raise ValueError(""Use spm_encode in the bash script instead of text2dict.py."")\n\n    word_dict = {}\n    token_set = set([])\n    with codecs.open(args.text, \'r\', encoding=""utf-8"") as f:\n        pbar = tqdm(total=len(codecs.open(args.text, \'r\', encoding=""utf-8"").readlines()))\n        for line in f:\n            line = line.strip()\n\n            if args.speed_perturb and \'sp1.0\' not in line:\n                pbar.update(1)\n                continue\n\n            words = line.split()[1:]\n            if \'\' in words:\n                words.remove(\'\')\n\n            # Remove special tokens\n            for nlsym in nlsyms:\n                # Include in the dictionary to sort by frequency\n                if args.unit in [\'word\', \'word_char\']:\n                    if nlsym not in word_dict.keys():\n                        word_dict[nlsym] = words.count(nlsym)\n                    else:\n                        word_dict[nlsym] += words.count(nlsym)\n\n                while True:\n                    if nlsym in words:\n                        words.remove(nlsym)\n                    else:\n                        break\n\n            text = \' \'.join(words)\n\n            if args.unit in [\'word\', \'word_char\']:\n                for w in words:\n                    # Count word frequency\n                    if w not in word_dict.keys():\n                        word_dict[w] = 1\n                    else:\n                        word_dict[w] += 1\n\n                if args.unit == \'word_char\':\n                    token_set |= set(list(text))\n\n            elif args.unit == \'char\':\n                # Remove whitespaces\n                if args.remove_word_boundary:\n                    text = text.replace(\' \', \'\')\n\n                token_set |= set(list(text))\n\n            elif args.unit == \'phone\':\n                token_set |= set(words)\n\n            else:\n                raise ValueError(args.unit)\n            pbar.update(1)\n\n    if args.unit in [\'word\']:\n        token_list = sorted(list(word_dict.keys()),\n                            key=lambda x: word_dict[x],\n                            reverse=True)[:args.vocab_size]\n        # NOTE: nlsyms are already included in the word_dict\n\n    elif args.unit == \'word_char\':\n        word_char_list = sorted(list(word_dict.keys()),\n                                key=lambda x: word_dict[x],\n                                reverse=True)[:args.vocab_size] + list(token_set)\n        token_list = sorted(list(set(word_char_list)))\n        # NOTE: nlsyms are already included in the word_dict\n\n    elif args.unit in [\'char\']:\n        token_list = sorted(nlsyms) + sorted(list(token_set))\n\n    elif args.unit == \'phone\':\n        token_list = sorted(list(token_set))\n        # NOTE: nlsyms are already included in the phone set\n\n    for t in token_list:\n        print(\'%s\' % t)\n\n\nif __name__ == \'__main__\':\n    main()\n'"
utils/trn2ctm.py,0,"b'#! /usr/bin/env python\n# -*- coding: utf-8 -*-\n\n# Copyright 2018 Kyoto University (Hirofumi Inaguma)\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n\n""Convert a trn file to a ctm file based on a stm segmentation.""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport argparse\nimport codecs\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\'trn\', type=str,\n                    help=\'trn file\')\nparser.add_argument(\'--stm\', type=str, default=\'\', nargs=\'?\',\n                    help=\'stm file (for Switchboard corpus)\')\nargs = parser.parse_args()\n\n\ndef main():\n\n    stm_segments = {}\n    if args.stm:\n        with codecs.open(args.stm, \'r\', encoding=""utf-8"") as f:\n            for line in f:\n                line = line.strip()\n\n                if line[0] == \';\':\n                    continue\n\n                speaker = line.split()[2]\n                start_t = float(line.split()[3])\n                end_t = float(line.split()[4])\n                if speaker not in stm_segments.keys():\n                    stm_segments[speaker] = {}\n                stm_segments[speaker][start_t] = end_t\n\n    with codecs.open(args.trn, \'r\', encoding=""utf-8"") as f:\n        for line in f:\n            line = line.strip()\n            words = line.split()[:-1]\n            speaker_utt_id = line.split()[-1].replace(\'(\', \'\').replace(\')\', \'\')\n            speaker = speaker_utt_id.split(\'-\')[0]\n            utt_id = \'-\'.join(speaker_utt_id.split(\'-\')[1:]).replace(\'-\', \'_\')\n            if args.stm:\n                channel = speaker.split(\'-\')[0].split(\'_\')[-1]  # A or B\n            else:\n                channel = \'1\'\n            start_f = utt_id.split(\'_\')[-2]\n            end_f = utt_id.split(\'_\')[-1]\n            start_t = round(int(start_f) / 100, 2)\n\n            if args.stm:\n                # Fix end time based on the stm file\n                if start_t in stm_segments[speaker].keys():\n                    end_t = stm_segments[speaker][start_t]\n                else:\n                    end_t = stm_segments[speaker][round(start_t + 0.01, 2)]\n            else:\n                end_t = round(int(end_f) / 100, 2)\n\n            duration_t = end_t - start_t\n            if len(words) > 0:\n                duration_t /= len(words)\n\n            confidence = 1  # Nist-1 manner in the ROVER paper\n\n            speaker_no_channel = speaker.replace(\'_A\', \'\').replace(\'_B\', \'\')\n\n            for w in words:\n                print(\'%s %s %.2f %.2f %s %.3f\' % (speaker_no_channel, channel, start_t, duration_t, w, confidence))\n                start_t += duration_t\n\n\nif __name__ == \'__main__\':\n    main()\n'"
neural_sp/bin/__init__.py,0,b''
neural_sp/bin/args_asr.py,0,"b'#! /usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n# Copyright 2018 Kyoto University (Hirofumi Inaguma)\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n\n""""""Args option for the ASR task.""""""\n\nimport configargparse\nfrom distutils.util import strtobool\nimport os\n\nfrom neural_sp.bin.train_utils import load_config\n\n\ndef parse_args_train(input_args):\n    parser = build_parser()\n    user_args, _ = parser.parse_known_args(input_args)\n\n    # register module specific arguments\n    parser = register_args_encoder(parser, user_args)\n    user_args, _ = parser.parse_known_args(input_args)  # to avoid args conflict\n    parser = register_args_decoder(parser, user_args)\n    user_args = parser.parse_args()\n    return user_args\n\n\ndef parse_args_eval(input_args):\n    parser = build_parser()\n    user_args, _ = parser.parse_known_args(input_args)\n\n    # Load a yaml config file\n    dir_name = os.path.dirname(user_args.recog_model[0])\n    conf_train = load_config(os.path.join(dir_name, \'conf.yml\'))\n\n    # register module specific arguments\n    user_args.enc_type = conf_train[\'enc_type\']\n    parser = register_args_encoder(parser, user_args)\n    user_args, _ = parser.parse_known_args(input_args)  # to avoid args conflict\n    user_args.dec_type = conf_train[\'dec_type\']  # to avoid overlap\n    parser = register_args_decoder(parser, user_args)\n    user_args = parser.parse_args()\n    # NOTE: If new args are registered after training the model, the default value will be set\n\n    # Overwrite config\n    for k, v in conf_train.items():\n        if \'recog\' not in k:\n            setattr(user_args, k, v)\n\n    return user_args, vars(user_args), dir_name\n\n\ndef register_args_encoder(parser, args):\n    if args.enc_type == \'tds\':\n        from neural_sp.models.seq2seq.encoders.tds import TDSEncoder as module\n    elif args.enc_type == \'gated_conv\':\n        from neural_sp.models.seq2seq.encoders.gated_conv import GatedConvEncoder as module\n    elif \'transformer\' in args.enc_type:\n        from neural_sp.models.seq2seq.encoders.transformer import TransformerEncoder as module\n    else:\n        from neural_sp.models.seq2seq.encoders.rnn import RNNEncoder as module\n    if hasattr(module, \'add_args\'):\n        parser = module.add_args(parser, args)\n    return parser\n\n\ndef register_args_decoder(parser, args):\n    if args.dec_type in [\'transformer\', \'transformer_xl\']:\n        from neural_sp.models.seq2seq.decoders.transformer import TransformerDecoder as module\n    elif args.dec_type == \'transformer_transducer\':\n        from neural_sp.models.seq2seq.decoders.transformer_transducer import TrasformerTransducer as module\n    elif args.dec_type in [\'lstm_transducer\', \'gru_transducer\']:\n        from neural_sp.models.seq2seq.decoders.rnn_transducer import RNNTransducer as module\n    elif args.dec_type == \'asg\':\n        from neural_sp.models.seq2seq.decoders.asg import ASGDecoder as module\n    else:\n        from neural_sp.models.seq2seq.decoders.las import RNNDecoder as module\n    if hasattr(module, \'add_args\'):\n        parser = module.add_args(parser, args)\n    return parser\n\n\ndef build_parser():\n    parser = configargparse.ArgumentParser(\n        config_file_parser_class=configargparse.YAMLConfigFileParser,\n        formatter_class=configargparse.ArgumentDefaultsHelpFormatter)\n    parser.add(\'--config\', is_config_file=True, help=\'config file path\')\n    parser.add(\'--config2\', is_config_file=True, default=False, nargs=\'?\',\n               help=\'another config file path to overwrite --config\')\n    # general\n    parser.add_argument(\'--corpus\', type=str,\n                        help=\'corpus name\')\n    parser.add_argument(\'--n_gpus\', type=int, default=1,\n                        help=\'number of GPUs (0 indicates CPU)\')\n    parser.add_argument(\'--cudnn_benchmark\', type=strtobool, default=True,\n                        help=\'use CuDNN benchmark mode\')\n    parser.add_argument(\'--model_save_dir\', type=str, default=False,\n                        help=\'directory to save a model\')\n    parser.add_argument(\'--resume\', type=str, default=False, nargs=\'?\',\n                        help=\'model path to resume training\')\n    parser.add_argument(\'--job_name\', type=str, default=False,\n                        help=\'job name\')\n    parser.add_argument(\'--stdout\', type=strtobool, default=False,\n                        help=\'print to standard output during training\')\n    # dataset\n    parser.add_argument(\'--train_set\', type=str,\n                        help=\'tsv file path for the training set\')\n    parser.add_argument(\'--train_set_sub1\', type=str, default=False,\n                        help=\'tsv file path for the training set for the 1st auxiliary task\')\n    parser.add_argument(\'--train_set_sub2\', type=str, default=False,\n                        help=\'tsv file path for the training set for the 2nd auxiliary task\')\n    parser.add_argument(\'--dev_set\', type=str,\n                        help=\'tsv file path for the development set\')\n    parser.add_argument(\'--dev_set_sub1\', type=str, default=False,\n                        help=\'tsv file path for the development set for the 1st auxiliary task\')\n    parser.add_argument(\'--dev_set_sub2\', type=str, default=False,\n                        help=\'tsv file path for the development set for the 2nd auxiliary task\')\n    parser.add_argument(\'--eval_sets\', type=str, default=[], nargs=\'+\',\n                        help=\'tsv file paths for the evaluation sets\')\n    parser.add_argument(\'--nlsyms\', type=str, default=False, nargs=\'?\',\n                        help=\'non-linguistic symbols file path\')\n    parser.add_argument(\'--dict\', type=str,\n                        help=\'dictionary file path\')\n    parser.add_argument(\'--dict_sub1\', type=str, default=False,\n                        help=\'dictionary file path for the 1st auxiliary task\')\n    parser.add_argument(\'--dict_sub2\', type=str, default=False,\n                        help=\'dictionary file path for the 2nd auxiliary task\')\n    parser.add_argument(\'--unit\', type=str, default=\'wp\',\n                        choices=[\'word\', \'wp\', \'char\', \'phone\', \'word_char\', \'char_space\'],\n                        help=\'output unit for the main task\')\n    parser.add_argument(\'--unit_sub1\', type=str, default=False,\n                        choices=[\'wp\', \'char\', \'phone\'],\n                        help=\'output unit for the 1st auxiliary task\')\n    parser.add_argument(\'--unit_sub2\', type=str, default=False,\n                        choices=[\'wp\', \'char\', \'phone\'],\n                        help=\'output unit for the 2nd auxiliary task\')\n    parser.add_argument(\'--wp_model\', type=str, default=False, nargs=\'?\',\n                        help=\'wordpiece model path for the main task\')\n    parser.add_argument(\'--wp_model_sub1\', type=str, default=False, nargs=\'?\',\n                        help=\'wordpiece model path for the 1st auxiliary task\')\n    parser.add_argument(\'--wp_model_sub2\', type=str, default=False, nargs=\'?\',\n                        help=\'wordpiece model path for the 2nd auxiliary task\')\n    # features\n    parser.add_argument(\'--input_type\', type=str, default=\'speech\',\n                        choices=[\'speech\', \'text\'],\n                        help=\'type of input features\')\n    parser.add_argument(\'--n_splices\', type=int, default=1,\n                        help=\'number of input frames to splice (both for left and right frames)\')\n    parser.add_argument(\'--n_stacks\', type=int, default=1,\n                        help=\'number of input frames to stack (frame stacking)\')\n    parser.add_argument(\'--n_skips\', type=int, default=1,\n                        help=\'number of input frames to skip\')\n    parser.add_argument(\'--max_n_frames\', type=int, default=2000,\n                        help=\'maximum number of input frames\')\n    parser.add_argument(\'--min_n_frames\', type=int, default=40,\n                        help=\'minimum number of input frames\')\n    parser.add_argument(\'--dynamic_batching\', type=strtobool, default=True,\n                        help=\'\')\n    parser.add_argument(\'--gaussian_noise\', type=strtobool, default=False,\n                        help=\'add Gaussian noise to input features\')\n    parser.add_argument(\'--weight_noise\', type=strtobool, default=False,\n                        help=\'add Gaussian noise to weight parameters\')\n    parser.add_argument(\'--sequence_summary_network\', type=strtobool, default=False,\n                        help=\'use sequence summary network\')\n    # topology (encoder)\n    parser.add_argument(\'--enc_type\', type=str, default=\'blstm\',\n                        choices=[\'blstm\', \'lstm\', \'bgru\', \'gru\',\n                                 \'conv_blstm\', \'conv_lstm\', \'conv_bgru\', \'conv_gru\',\n                                 \'transformer\', \'conv_transformer\',\n                                 \'conv\', \'tds\', \'gated_conv\'],\n                        help=\'type of the encoder\')\n    parser.add_argument(\'--enc_n_layers\', type=int, default=5,\n                        help=\'number of encoder RNN layers\')\n    parser.add_argument(\'--enc_n_layers_sub1\', type=int, default=0,\n                        help=\'number of encoder RNN layers in the 1st auxiliary task\')\n    parser.add_argument(\'--enc_n_layers_sub2\', type=int, default=0,\n                        help=\'number of encoder RNN layers in the 2nd auxiliary task\')\n    parser.add_argument(\'--subsample\', type=str, default=""1_1_1_1_1"",\n                        help=\'delimited list input\')\n    parser.add_argument(\'--subsample_type\', type=str, default=\'drop\',\n                        choices=[\'drop\', \'concat\', \'max_pool\', \'1dconv\'],\n                        help=\'type of subsampling in the encoder\')\n    # topology (decoder)\n    parser.add_argument(\'--dec_type\', type=str, default=\'lstm\',\n                        choices=[\'lstm\', \'gru\', \'transformer\', \'transformer_xl\',\n                                 \'lstm_transducer\', \'gru_transducer\', \'transformer_transducer\',\n                                 \'asg\'],\n                        help=\'type of the decoder\')\n    parser.add_argument(\'--dec_n_layers\', type=int, default=1,\n                        help=\'number of decoder RNN layers\')\n    parser.add_argument(\'--tie_embedding\', type=strtobool, default=False, nargs=\'?\',\n                        help=\'tie weights between an embedding matrix and a linear layer before the softmax layer\')\n    parser.add_argument(\'--ctc_fc_list\', type=str, default="""", nargs=\'?\',\n                        help=\'\')\n    parser.add_argument(\'--ctc_fc_list_sub1\', type=str, default="""", nargs=\'?\',\n                        help=\'\')\n    parser.add_argument(\'--ctc_fc_list_sub2\', type=str, default="""", nargs=\'?\',\n                        help=\'\')\n    # streaming decoder\n    parser.add_argument(\'--mocha_n_heads_mono\', type=int, default=1,\n                        help=\'number of heads for monotonic attention\')\n    parser.add_argument(\'--mocha_n_heads_chunk\', type=int, default=1,\n                        help=\'number of heads for chunkwise attention\')\n    parser.add_argument(\'--mocha_chunk_size\', type=int, default=0,\n                        help=\'chunk size for MoChA/MMA. -1 means infinite lookback.\')\n    parser.add_argument(\'--mocha_init_r\', type=float, default=-4,\n                        help=\'\')\n    parser.add_argument(\'--mocha_eps\', type=float, default=1e-6,\n                        help=\'\')\n    parser.add_argument(\'--mocha_std\', type=float, default=1.0,\n                        help=\'\')\n    parser.add_argument(\'--mocha_no_denominator\', type=strtobool, default=False,\n                        help=\'remove denominator (set to 1) in the alpha recurrence\')\n    parser.add_argument(\'--mocha_1dconv\', type=strtobool, default=False,\n                        help=\'1dconv for MoChA/MMA\')\n    parser.add_argument(\'--mocha_quantity_loss_weight\', type=float, default=0.0,\n                        help=\'quantity loss weight for MoChA/MMA\')\n    parser.add_argument(\'--mocha_latency_metric\', type=str, default=False,\n                        choices=[False, \'decot\', \'minlt\', \'ctc_sync\',\n                                 \'interval\', \'frame_dal\', \'ctc_dal\'],\n                        help=\'differentiable latency metric for MoChA/MMA\')\n    parser.add_argument(\'--mocha_latency_loss_weight\', type=float, default=0.0,\n                        help=\'latency loss weight for MoChA/MMA\')\n    # optimization\n    parser.add_argument(\'--batch_size\', type=int, default=50,\n                        help=\'mini-batch size\')\n    parser.add_argument(\'--optimizer\', type=str, default=\'adam\',\n                        choices=[\'adam\', \'adadelta\', \'adagrad\', \'sgd\', \'momentum\', \'nesterov\', \'noam\'],\n                        help=\'type of optimizer\')\n    parser.add_argument(\'--n_epochs\', type=int, default=25,\n                        help=\'number of epochs to train the model\')\n    parser.add_argument(\'--convert_to_sgd_epoch\', type=int, default=100,\n                        help=\'epoch to converto to SGD fine-tuning\')\n    parser.add_argument(\'--print_step\', type=int, default=200,\n                        help=\'print log per this value\')\n    parser.add_argument(\'--metric\', type=str, default=\'edit_distance\',\n                        choices=[\'edit_distance\', \'loss\', \'accuracy\', \'ppl\', \'bleu\', \'mse\'],\n                        help=\'metric for evaluation during training\')\n    parser.add_argument(\'--lr\', type=float, default=1e-3,\n                        help=\'initial learning rate\')\n    parser.add_argument(\'--lr_factor\', type=float, default=10.0,\n                        help=\'factor of learning rate for Transformer\')\n    parser.add_argument(\'--eps\', type=float, default=1e-6,\n                        help=\'epsilon parameter for Adadelta optimizer\')\n    parser.add_argument(\'--lr_decay_type\', type=str, default=\'always\',\n                        choices=[\'always\', \'metric\', \'warmup\'],\n                        help=\'type of learning rate decay\')\n    parser.add_argument(\'--lr_decay_start_epoch\', type=int, default=10,\n                        help=\'epoch to start to decay learning rate\')\n    parser.add_argument(\'--lr_decay_rate\', type=float, default=0.9,\n                        help=\'decay rate of learning rate\')\n    parser.add_argument(\'--lr_decay_patient_n_epochs\', type=int, default=0,\n                        help=\'number of epochs to tolerate learning rate decay when validation perfomance is not improved\')\n    parser.add_argument(\'--early_stop_patient_n_epochs\', type=int, default=5,\n                        help=\'number of epochs to tolerate stopping training when validation perfomance is not improved\')\n    parser.add_argument(\'--sort_stop_epoch\', type=int, default=10000,\n                        help=\'epoch to stop soring utterances by length\')\n    parser.add_argument(\'--sort_short2long\', type=strtobool, default=True,\n                        help=\'sort utterances in the ascending order\')\n    parser.add_argument(\'--shuffle_bucket\', type=strtobool, default=False,\n                        help=\'gather the similar length of utterances and shuffle them\')\n    parser.add_argument(\'--eval_start_epoch\', type=int, default=1,\n                        help=\'first epoch to start evalaution\')\n    parser.add_argument(\'--warmup_start_lr\', type=float, default=0,\n                        help=\'initial learning rate for learning rate warm up\')\n    parser.add_argument(\'--warmup_n_steps\', type=int, default=0,\n                        help=\'number of steps to warm up learing rate\')\n    parser.add_argument(\'--accum_grad_n_steps\', type=int, default=1,\n                        help=\'total number of steps to accumulate gradients\')\n    # initialization\n    parser.add_argument(\'--param_init\', type=float, default=0.1,\n                        help=\'\')\n    parser.add_argument(\'--asr_init\', type=str, default=False, nargs=\'?\',\n                        help=\'pre-trained seq2seq model path\')\n    parser.add_argument(\'--asr_init_enc_only\', type=strtobool, default=False,\n                        help=\'Initialize the encoder only\')\n    parser.add_argument(\'--freeze_encoder\', type=strtobool, default=False,\n                        help=\'freeze the encoder parameter\')\n    # regularization\n    parser.add_argument(\'--clip_grad_norm\', type=float, default=5.0,\n                        help=\'\')\n    parser.add_argument(\'--dropout_in\', type=float, default=0.0,\n                        help=\'dropout probability for the input\')\n    parser.add_argument(\'--dropout_enc\', type=float, default=0.0,\n                        help=\'dropout probability for the encoder\')\n    parser.add_argument(\'--dropout_dec\', type=float, default=0.0,\n                        help=\'dropout probability for the decoder\')\n    parser.add_argument(\'--dropout_emb\', type=float, default=0.0,\n                        help=\'dropout probability for the embedding\')\n    parser.add_argument(\'--dropout_att\', type=float, default=0.0,\n                        help=\'dropout probability for the attention weights\')\n    parser.add_argument(\'--weight_decay\', type=float, default=0,\n                        help=\'weight decay parameter\')\n    parser.add_argument(\'--ss_prob\', type=float, default=0.0,\n                        help=\'probability of scheduled sampling\')\n    parser.add_argument(\'--ss_type\', type=str, default=\'constant\',\n                        choices=[\'constant\', \'ramp\'],\n                        help=\'type of scheduled sampling\')\n    parser.add_argument(\'--lsm_prob\', type=float, default=0.0,\n                        help=\'probability of label smoothing\')\n    parser.add_argument(\'--ctc_lsm_prob\', type=float, default=0.0,\n                        help=\'probability of label smoothing for CTC\')\n    # SpecAugment\n    parser.add_argument(\'--freq_width\', type=int, default=27,\n                        help=\'width of frequency mask for SpecAugment\')\n    parser.add_argument(\'--n_freq_masks\', type=int, default=0,\n                        help=\'number of frequency masks for SpecAugment\')\n    parser.add_argument(\'--time_width\', type=int, default=70,\n                        help=\'width of time mask for SpecAugment\')\n    parser.add_argument(\'--n_time_masks\', type=int, default=0,\n                        help=\'number of time masks for SpecAugment\')\n    parser.add_argument(\'--time_width_upper\', type=float, default=0.2,\n                        help=\'\')\n    parser.add_argument(\'--adaptive_number_ratio\', type=float, default=0.0,\n                        help=\'adaptive multiplicity ratio for time masking\')\n    parser.add_argument(\'--adaptive_size_ratio\', type=float, default=0.0,\n                        help=\'adaptive size ratio for time masking\')\n    parser.add_argument(\'--max_n_time_masks\', type=int, default=20,\n                        help=\'maximum number of time masking\')\n    # MTL\n    parser.add_argument(\'--ctc_weight\', type=float, default=0.0,\n                        help=\'CTC loss weight for the main task\')\n    parser.add_argument(\'--ctc_weight_sub1\', type=float, default=0.0,\n                        help=\'CTC loss weight for the 1st auxiliary task\')\n    parser.add_argument(\'--ctc_weight_sub2\', type=float, default=0.0,\n                        help=\'CTC loss weight for the 2nd auxiliary task\')\n    parser.add_argument(\'--sub1_weight\', type=float, default=0.0,\n                        help=\'total loss weight for the 1st auxiliary task\')\n    parser.add_argument(\'--sub2_weight\', type=float, default=0.0,\n                        help=\'total loss weight for the 2nd auxiliary task\')\n    parser.add_argument(\'--mtl_per_batch\', type=strtobool, default=False, nargs=\'?\',\n                        help=\'change mini-batch per task\')\n    parser.add_argument(\'--task_specific_layer\', type=strtobool, default=False, nargs=\'?\',\n                        help=\'insert a task-specific encoder layer per task\')\n    # foroward-backward\n    parser.add_argument(\'--bwd_weight\', type=float, default=0.0,\n                        help=\'cross etnropy loss weight for the backward decoder in the main task\')\n    # cold fusion, LM initialization\n    parser.add_argument(\'--external_lm\', type=str, default=False, nargs=\'?\',\n                        help=\'LM path\')\n    parser.add_argument(\'--lm_fusion\', type=str, default=\'\',\n                        choices=[\'\', \'cold\', \'cold_prob\', \'deep\', \'cold_attention\'],\n                        help=\'type of LM fusion\')\n    parser.add_argument(\'--lm_init\', type=strtobool, default=False,\n                        help=\'initialize the decoder with the external LM\')\n    # contextualization\n    parser.add_argument(\'--discourse_aware\', type=strtobool, default=False, nargs=\'?\',\n                        help=\'carry over the last decoder state to the initial state in the next utterance\')\n    # MBR\n    parser.add_argument(\'--mbr_training\', type=strtobool, default=False,\n                        help=\'Minimum Bayes Risk (MBR) training\')\n    parser.add_argument(\'--mbr_ce_weight\', type=float, default=0.01,\n                        help=\'MBR loss weight for the main task\')\n    parser.add_argument(\'--mbr_nbest\', type=int, default=4,\n                        help=\'N-best for MBR training\')\n    parser.add_argument(\'--mbr_softmax_smoothing\', type=float, default=0.8,\n                        help=\'softmax smoothing (beta) for MBR training\')\n    # TransformerXL\n    parser.add_argument(\'--bptt\', type=int, default=0,\n                        help=\'number of tokens to truncate in TransformerXL decoder during training\')\n    parser.add_argument(\'--mem_len\', type=int, default=0,\n                        help=\'number of tokens for memory in TransformerXL decoder during training\')\n    # distillation related\n    parser.add_argument(\'--teacher\', default=False, nargs=\'?\',\n                        help=\'Teacher ASR model for knowledge distillation\')\n    parser.add_argument(\'--teacher_lm\', default=False, nargs=\'?\',\n                        help=\'Teacher LM for knowledge distillation\')\n    parser.add_argument(\'--distillation_weight\', type=float, default=0.1,\n                        help=\'soft label weight for knowledge distillation\')\n    # special label\n    parser.add_argument(\'--replace_sos\', type=strtobool, default=False,\n                        help=\'\')\n    # decoding parameters\n    parser.add_argument(\'--recog_stdout\', type=strtobool, default=False,\n                        help=\'print to standard output during evaluation\')\n    parser.add_argument(\'--recog_n_gpus\', type=int, default=0,\n                        help=\'number of GPUs (0 indicates CPU)\')\n    parser.add_argument(\'--recog_sets\', type=str, default=[], nargs=\'+\',\n                        help=\'tsv file paths for the evaluation sets\')\n    parser.add_argument(\'--recog_model\', type=str, default=False, nargs=\'+\',\n                        help=\'model path\')\n    parser.add_argument(\'--recog_model_bwd\', type=str, default=False, nargs=\'?\',\n                        help=\'model path in the reverse direction\')\n    parser.add_argument(\'--recog_dir\', type=str, default=False,\n                        help=\'directory to save decoding results\')\n    parser.add_argument(\'--recog_unit\', type=str, default=False, nargs=\'?\',\n                        choices=[\'word\', \'wp\', \'char\', \'phone\', \'word_char\', \'char_space\'],\n                        help=\'\')\n    parser.add_argument(\'--recog_metric\', type=str, default=\'edit_distance\',\n                        choices=[\'edit_distance\', \'loss\', \'accuracy\', \'ppl\', \'bleu\'],\n                        help=\'metric for evaluation\')\n    parser.add_argument(\'--recog_oracle\', type=strtobool, default=False,\n                        help=\'recognize by teacher-forcing\')\n    parser.add_argument(\'--recog_batch_size\', type=int, default=1,\n                        help=\'size of mini-batch in evaluation\')\n    parser.add_argument(\'--recog_beam_width\', type=int, default=1,\n                        help=\'size of beam\')\n    parser.add_argument(\'--recog_max_len_ratio\', type=float, default=1.0,\n                        help=\'\')\n    parser.add_argument(\'--recog_min_len_ratio\', type=float, default=0.0,\n                        help=\'\')\n    parser.add_argument(\'--recog_length_penalty\', type=float, default=0.0,\n                        help=\'length penalty\')\n    parser.add_argument(\'--recog_length_norm\', type=strtobool, default=False, nargs=\'?\',\n                        help=\'normalize score by hypothesis length\')\n    parser.add_argument(\'--recog_coverage_penalty\', type=float, default=0.0,\n                        help=\'coverage penalty\')\n    parser.add_argument(\'--recog_coverage_threshold\', type=float, default=0.0,\n                        help=\'coverage threshold\')\n    parser.add_argument(\'--recog_gnmt_decoding\', type=strtobool, default=False, nargs=\'?\',\n                        help=\'adopt Google NMT beam search decoding\')\n    parser.add_argument(\'--recog_eos_threshold\', type=float, default=1.5,\n                        help=\'threshold for emitting a EOS token\')\n    parser.add_argument(\'--recog_lm_weight\', type=float, default=0.0,\n                        help=\'weight of fisrt-path LM score\')\n    parser.add_argument(\'--recog_lm_second_weight\', type=float, default=0.0,\n                        help=\'weight of second-path LM score\')\n    parser.add_argument(\'--recog_lm_bwd_weight\', type=float, default=0.0,\n                        help=\'weight of second-path bakward LM score. \\\n                                  First-pass backward LM in case of synchronous bidirectional decoding.\')\n    parser.add_argument(\'--recog_ctc_weight\', type=float, default=0.0,\n                        help=\'weight of CTC score\')\n    parser.add_argument(\'--recog_lm\', type=str, default=False, nargs=\'?\',\n                        help=\'path to first path LM for shallow fusion\')\n    parser.add_argument(\'--recog_lm_second\', type=str, default=False, nargs=\'?\',\n                        help=\'path to second path LM for rescoring\')\n    parser.add_argument(\'--recog_lm_bwd\', type=str, default=False, nargs=\'?\',\n                        help=\'path to second path LM in the reverse direction for rescoring\')\n    parser.add_argument(\'--recog_resolving_unk\', type=strtobool, default=False,\n                        help=\'resolving UNK for the word-based model\')\n    parser.add_argument(\'--recog_fwd_bwd_attention\', type=strtobool, default=False,\n                        help=\'forward-backward attention decoding\')\n    parser.add_argument(\'--recog_bwd_attention\', type=strtobool, default=False,\n                        help=\'backward attention decoding\')\n    parser.add_argument(\'--recog_reverse_lm_rescoring\', type=strtobool, default=False,\n                        help=\'rescore with another LM in the reverse direction\')\n    parser.add_argument(\'--recog_asr_state_carry_over\', type=strtobool, default=False,\n                        help=\'carry over ASR decoder state\')\n    parser.add_argument(\'--recog_lm_state_carry_over\', type=strtobool, default=False,\n                        help=\'carry over LM state\')\n    parser.add_argument(\'--recog_softmax_smoothing\', type=float, default=1.0,\n                        help=\'softmax smoothing (beta) for diverse hypothesis generation\')\n    parser.add_argument(\'--recog_wordlm\', type=strtobool, default=False,\n                        help=\'\')\n    parser.add_argument(\'--recog_n_average\', type=int, default=1,\n                        help=\'number of models for the model averaging of Transformer\')\n    parser.add_argument(\'--recog_streaming\', type=strtobool, default=False,\n                        help=\'streaming decoding\')\n    parser.add_argument(\'--recog_chunk_sync\', type=strtobool, default=False,\n                        help=\'chunk-synchronous beam search decoding for MoChA\')\n    parser.add_argument(\'--recog_ctc_spike_forced_decoding\', type=strtobool, default=False,\n                        help=\'force MoChA to generate tokens corresponding to CTC spikes\')\n    parser.add_argument(\'--recog_ctc_vad\', type=strtobool, default=True,\n                        help=\'\')\n    parser.add_argument(\'--recog_ctc_vad_blank_threshold\', type=int, default=40,\n                        help=\'\')\n    parser.add_argument(\'--recog_ctc_vad_spike_threshold\', type=float, default=0.1,\n                        help=\'\')\n    parser.add_argument(\'--recog_ctc_vad_n_accum_frames\', type=float, default=4000,\n                        help=\'\')\n    parser.add_argument(\'--recog_mma_delay_threshold\', type=int, default=-1,\n                        help=\'delay threshold for MMA decoder\')\n    parser.add_argument(\'--recog_mem_len\', type=int, default=0,\n                        help=\'number of tokens for memory in TransformerXL decoder during evaluation\')\n    return parser\n'"
neural_sp/bin/args_lm.py,0,"b'#! /usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n# Copyright 2018 Kyoto University (Hirofumi Inaguma)\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n\n""""""Args option for the LM task.""""""\n\nimport configargparse\nfrom distutils.util import strtobool\nimport os\n\nfrom neural_sp.bin.train_utils import load_config\n\n\ndef parse_args_train(input_args):\n    parser = build_parser()\n    user_args, _ = parser.parse_known_args(input_args)\n\n    # register module specific arguments\n    parser = register_args_lm(parser, user_args)\n    user_args = parser.parse_args()\n    return user_args\n\n\ndef parse_args_eval(input_args):\n    parser = build_parser()\n    user_args, _ = parser.parse_known_args(input_args)\n\n    # Load a yaml config file\n    dir_name = os.path.dirname(user_args.recog_model[0])\n    conf_train = load_config(os.path.join(dir_name, \'conf.yml\'))\n\n    # register module specific arguments\n    user_args.lm_type = conf_train[\'lm_type\']\n    parser = register_args_lm(parser, user_args)\n    user_args = parser.parse_args()\n    # NOTE: If new args are registered after training the model, the default value will be set\n\n    # Overwrite config\n    for k, v in conf_train.items():\n        if \'recog\' not in k:\n            setattr(user_args, k, v)\n\n    return user_args, vars(user_args), dir_name\n\n\ndef register_args_lm(parser, args):\n    if \'gated_conv\' in args.lm_type:\n        from neural_sp.models.lm.gated_convlm import GatedConvLM as module\n    elif args.lm_type == \'transformer\':\n        from neural_sp.models.lm.transformerlm import TransformerLM as module\n    elif args.lm_type == \'transformer_xl\':\n        from neural_sp.models.lm.transformer_xl import TransformerXL as module\n    else:\n        from neural_sp.models.lm.rnnlm import RNNLM as module\n    if hasattr(module, \'add_args\'):\n        parser = module.add_args(parser, args)\n    return parser\n\n\ndef build_parser():\n    parser = configargparse.ArgumentParser(\n        config_file_parser_class=configargparse.YAMLConfigFileParser,\n        formatter_class=configargparse.ArgumentDefaultsHelpFormatter)\n    parser.add(\'--config\', is_config_file=True, help=\'config file path\')\n    # general\n    parser.add_argument(\'--corpus\', type=str,\n                        help=\'corpus name\')\n    parser.add_argument(\'--n_gpus\', type=int, default=1,\n                        help=\'number of GPUs (0 indicates CPU)\')\n    parser.add_argument(\'--cudnn_benchmark\', type=strtobool, default=True,\n                        help=\'use CuDNN benchmark mode\')\n    parser.add_argument(\'--model_save_dir\', type=str, default=False,\n                        help=\'directory to save a model\')\n    parser.add_argument(\'--resume\', type=str, default=False, nargs=\'?\',\n                        help=\'model path to resume training\')\n    parser.add_argument(\'--job_name\', type=str, default=False,\n                        help=\'job name\')\n    parser.add_argument(\'--stdout\', type=strtobool, default=False,\n                        help=\'print to standard output\')\n    parser.add_argument(\'--recog_stdout\', type=strtobool, default=False,\n                        help=\'print to standard output during evaluation\')\n    # dataset\n    parser.add_argument(\'--train_set\', type=str,\n                        help=\'tsv file path for the training set\')\n    parser.add_argument(\'--dev_set\', type=str,\n                        help=\'tsv file path for the development set\')\n    parser.add_argument(\'--eval_sets\', type=str, default=[], nargs=\'+\',\n                        help=\'tsv file paths for the evaluation sets\')\n    parser.add_argument(\'--nlsyms\', type=str, default=False, nargs=\'?\',\n                        help=\'non-linguistic symbols file path\')\n    parser.add_argument(\'--dict\', type=str,\n                        help=\'dictionary file path\')\n    parser.add_argument(\'--unit\', type=str, default=\'word\',\n                        choices=[\'word\', \'wp\', \'char\', \'word_char\'],\n                        help=\'output unit\')\n    parser.add_argument(\'--wp_model\', type=str, default=False, nargs=\'?\',\n                        help=\'wordpiece model path\')\n    # features\n    parser.add_argument(\'--min_n_tokens\', type=int, default=1,\n                        help=\'minimum number of input tokens\')\n    parser.add_argument(\'--dynamic_batching\', type=strtobool, default=False,\n                        help=\'\')\n    # topology\n    parser.add_argument(\'--lm_type\', type=str, default=\'lstm\',\n                        choices=[\'lstm\', \'gru\', \'gated_conv_custom\',\n                                 \'gated_conv_8\', \'gated_conv_8B\', \'gated_conv_9\',\n                                 \'gated_conv_13\', \'gated_conv_14\', \'gated_conv_14B\',\n                                 \'transformer\', \'transformer_xl\'],\n                        help=\'type of language model\')\n    parser.add_argument(\'--n_layers\', type=int, default=5,\n                        help=\'number of layers\')\n    parser.add_argument(\'--emb_dim\', type=int, default=1024,\n                        help=\'number of dimensions in the embedding layer\')\n    parser.add_argument(\'--n_units_null_context\', type=int, default=0, nargs=\'?\',\n                        help=\'\')\n    parser.add_argument(\'--tie_embedding\', type=strtobool, default=False, nargs=\'?\',\n                        help=\'tie input and output embedding\')\n    # optimization\n    parser.add_argument(\'--batch_size\', type=int, default=256,\n                        help=\'mini-batch size\')\n    parser.add_argument(\'--bptt\', type=int, default=200,\n                        help=\'BPTT length\')\n    parser.add_argument(\'--optimizer\', type=str, default=\'adam\',\n                        choices=[\'adam\', \'adadelta\', \'adagrad\', \'sgd\', \'momentum\', \'nesterov\', \'noam\'],\n                        help=\'type of optimizer\')\n    parser.add_argument(\'--n_epochs\', type=int, default=50,\n                        help=\'number of epochs to train the model\')\n    parser.add_argument(\'--convert_to_sgd_epoch\', type=int, default=100,\n                        help=\'epoch to converto to SGD fine-tuning\')\n    parser.add_argument(\'--print_step\', type=int, default=100,\n                        help=\'print log per this value\')\n    parser.add_argument(\'--lr\', type=float, default=1e-3,\n                        help=\'initial learning rate\')\n    parser.add_argument(\'--lr_factor\', type=float, default=10.0,\n                        help=\'factor of learning rate for Transformer\')\n    parser.add_argument(\'--eps\', type=float, default=1e-6,\n                        help=\'epsilon parameter for Adadelta optimizer\')\n    parser.add_argument(\'--lr_decay_type\', type=str, default=\'always\',\n                        choices=[\'always\', \'metric\', \'warmup\'],\n                        help=\'type of learning rate decay\')\n    parser.add_argument(\'--lr_decay_start_epoch\', type=int, default=10,\n                        help=\'epoch to start to decay learning rate\')\n    parser.add_argument(\'--lr_decay_rate\', type=float, default=0.9,\n                        help=\'decay rate of learning rate\')\n    parser.add_argument(\'--lr_decay_patient_n_epochs\', type=int, default=0,\n                        help=\'number of epochs to tolerate learning rate decay when validation perfomance is not improved\')\n    parser.add_argument(\'--early_stop_patient_n_epochs\', type=int, default=5,\n                        help=\'number of epochs to tolerate stopping training when validation perfomance is not improved\')\n    parser.add_argument(\'--sort_stop_epoch\', type=int, default=10000,\n                        help=\'epoch to stop soring utterances by length\')\n    parser.add_argument(\'--eval_start_epoch\', type=int, default=1,\n                        help=\'first epoch to start evalaution\')\n    parser.add_argument(\'--warmup_start_lr\', type=float, default=0,\n                        help=\'initial learning rate for learning rate warm up\')\n    parser.add_argument(\'--warmup_n_steps\', type=int, default=0,\n                        help=\'number of steps to warm up learing rate\')\n    parser.add_argument(\'--accum_grad_n_steps\', type=int, default=1,\n                        help=\'total number of steps to accumulate gradients\')\n    # initialization\n    parser.add_argument(\'--param_init\', type=float, default=0.1,\n                        help=\'\')\n    parser.add_argument(\'--pretrained_model\', type=str, default=False, nargs=\'?\',\n                        help=\'\')\n    # regularization\n    parser.add_argument(\'--clip_grad_norm\', type=float, default=5.0,\n                        help=\'\')\n    parser.add_argument(\'--dropout_in\', type=float, default=0.0,\n                        help=\'dropout probability for the input embedding layer\')\n    parser.add_argument(\'--dropout_hidden\', type=float, default=0.0,\n                        help=\'dropout probability for the hidden layers\')\n    parser.add_argument(\'--dropout_out\', type=float, default=0.0,\n                        help=\'dropout probability for the output layer\')\n    parser.add_argument(\'--weight_decay\', type=float, default=1e-6,\n                        help=\'\')\n    parser.add_argument(\'--lsm_prob\', type=float, default=0.0,\n                        help=\'probability of label smoothing\')\n    parser.add_argument(\'--logits_temp\', type=float, default=1.0,\n                        help=\'\')\n    parser.add_argument(\'--backward\', type=strtobool, default=False, nargs=\'?\',\n                        help=\'\')\n    parser.add_argument(\'--adaptive_softmax\', type=strtobool, default=False,\n                        help=\'use adaptive softmax\')\n    # contextualization\n    parser.add_argument(\'--shuffle\', type=strtobool, default=False, nargs=\'?\',\n                        help=\'shuffle utterances per epoch\')\n    parser.add_argument(\'--serialize\', type=strtobool, default=False, nargs=\'?\',\n                        help=\'serialize text according to onset in dialogue\')\n    # evaluation parameters\n    parser.add_argument(\'--recog_n_gpus\', type=int, default=0,\n                        help=\'number of GPUs (0 indicates CPU)\')\n    parser.add_argument(\'--recog_sets\', type=str, default=[], nargs=\'+\',\n                        help=\'tsv file paths for the evaluation sets\')\n    parser.add_argument(\'--recog_model\', type=str, default=False, nargs=\'+\',\n                        help=\'model path\')\n    parser.add_argument(\'--recog_dir\', type=str, default=False,\n                        help=\'directory to save decoding results\')\n    parser.add_argument(\'--recog_batch_size\', type=int, default=1,\n                        help=\'size of mini-batch in evaluation\')\n    parser.add_argument(\'--recog_n_average\', type=int, default=5,\n                        help=\'number of models for the model averaging of Transformer\')\n    parser.add_argument(\'--recog_n_caches\', type=int, default=0,\n                        help=\'number of tokens for cache\')\n    parser.add_argument(\'--recog_cache_theta\', type=float, default=0.2,\n                        help=\'theta paramter for cache\')\n    parser.add_argument(\'--recog_cache_lambda\', type=float, default=0.2,\n                        help=\'lambda paramter for cache\')\n    parser.add_argument(\'--recog_mem_len\', type=int, default=0,\n                        help=\'number of tokens for memory in TransformerXL during evaluation\')\n    return parser\n'"
neural_sp/bin/eval_utils.py,2,"b'#! /usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n# Copyright 2019 Kyoto University (Hirofumi Inaguma)\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n\n""""""Utility functions for evaluation.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport logging\nimport os\nimport torch\n\nlogger = logging.getLogger(__name__)\n\n\ndef average_checkpoints(model, best_model_path, n_average, topk_list=[]):\n    if n_average == 1:\n        return model\n\n    n_models = 0\n    checkpoint_avg = {\'model_state_dict\': None}\n    if len(topk_list) == 0:\n        epoch = int(best_model_path.split(\'model.epoch-\')[1])\n        topk_list = [(i, 0) for i in range(epoch, epoch - n_average - 1, -1)]\n    for ep, _ in topk_list:\n        if n_models == n_average:\n            break\n        checkpoint_path = best_model_path.split(\'model.epoch-\')[0] + \'model.epoch-\' + str(ep)\n        if os.path.isfile(checkpoint_path):\n            logger.info(""=> Loading checkpoint (epoch:%d): %s"" % (ep, checkpoint_path))\n            checkpoint = torch.load(checkpoint_path, map_location=lambda storage, loc: storage)\n            if checkpoint_avg[\'model_state_dict\'] is None:\n                # first checkpoint\n                checkpoint_avg[\'model_state_dict\'] = checkpoint[\'model_state_dict\']\n                n_models += 1\n                continue\n            for k, v in checkpoint[\'model_state_dict\'].items():\n                checkpoint_avg[\'model_state_dict\'][k] += v\n            n_models += 1\n\n    # take an average\n    logger.info(\'Take average for %d models\' % n_models)\n    for k, v in checkpoint_avg[\'model_state_dict\'].items():\n        checkpoint_avg[\'model_state_dict\'][k] /= n_models\n    model.load_state_dict(checkpoint_avg[\'model_state_dict\'])\n\n    # save as a new checkpoint\n    checkpoint_avg_path = best_model_path.split(\'model.epoch-\')[0] + \'model-avg\' + str(n_average)\n    if os.path.isfile(checkpoint_avg_path):\n        os.remove(checkpoint_avg_path)\n    torch.save(checkpoint_avg, checkpoint_avg_path)\n\n    return model\n'"
neural_sp/bin/model_name.py,0,"b'#! /usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n# Copyright 2019 Kyoto University (Hirofumi Inaguma)\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n\n""""""Set model name.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\n\nfrom neural_sp.bin.train_utils import load_config\n\n\ndef set_asr_model_name(args):\n    # encoder\n    dir_name = args.enc_type.replace(\'conv_\', \'\')\n    if args.conv_channels and len(args.conv_channels.split(\'_\')) > 0 and \'conv\' in args.enc_type:\n        tmp = dir_name\n        dir_name = \'conv\' + str(len(args.conv_channels.split(\'_\'))) + \'L\'\n        if args.conv_batch_norm:\n            dir_name += \'bn\'\n        if args.conv_layer_norm:\n            dir_name += \'ln\'\n        dir_name += tmp\n    if \'transformer\' in args.enc_type:\n        dir_name += str(args.transformer_d_model) + \'dmodel\'\n        dir_name += str(args.transformer_d_ff) + \'dff\'\n        if args.transformer_d_ff_bottleneck_dim > 0:\n            dir_name += str(args.transformer_d_ff_bottleneck_dim) + \'bn\'\n        dir_name += str(args.enc_n_layers) + \'L\'\n        dir_name += str(args.transformer_n_heads) + \'H\'\n        dir_name += \'pe\' + str(args.transformer_enc_pe_type)\n        if args.dropout_enc_layer > 0:\n            dir_name += \'droplayer\' + str(args.dropout_enc_layer)\n        if args.lc_chunk_size_left > 0 or args.lc_chunk_size_current > 0 or args.lc_chunk_size_right > 0:\n            dir_name += \'_chunkL\' + str(args.lc_chunk_size_left) + \'C\' + \\\n                str(args.lc_chunk_size_current) + \'R\' + str(args.lc_chunk_size_right)\n    else:\n        dir_name += str(args.enc_n_units) + \'H\'\n        if args.enc_n_projs > 0:\n            dir_name += str(args.enc_n_projs) + \'P\'\n        dir_name += str(args.enc_n_layers) + \'L\'\n        if args.bidirectional_sum_fwd_bwd:\n            dir_name += \'_sumfwdbwd\'\n        if args.lc_chunk_size_left > 0 or args.lc_chunk_size_right > 0:\n            dir_name += \'_chunkL\' + str(args.lc_chunk_size_left) + \'R\' + str(args.lc_chunk_size_right)\n    if args.n_stacks > 1:\n        dir_name += \'_stack\' + str(args.n_stacks)\n    else:\n        dir_name += \'_\' + args.subsample_type + str(args.subsample_factor)\n    if args.sequence_summary_network:\n        dir_name += \'_ssn\'\n\n    # decoder\n    if args.ctc_weight < 1:\n        dir_name += \'_\' + args.dec_type\n        if \'transformer\' in args.dec_type:\n            dir_name += str(args.transformer_d_model) + \'dmodel\'\n            dir_name += str(args.transformer_d_ff) + \'dff\'\n            if args.transformer_d_ff_bottleneck_dim > 0:\n                dir_name += str(args.transformer_d_ff_bottleneck_dim) + \'bn\'\n            dir_name += str(args.dec_n_layers) + \'L\'\n            dir_name += str(args.transformer_n_heads) + \'H\'\n            dir_name += \'pe\' + str(args.transformer_dec_pe_type)\n            dir_name += args.transformer_attn_type\n            if \'mocha\' in args.transformer_attn_type:\n                dir_name += \'_ma\' + str(args.mocha_n_heads_mono) + \'H\'\n                dir_name += \'_ca\' + str(args.mocha_n_heads_chunk) + \'H\'\n                dir_name += \'_w\' + str(args.mocha_chunk_size)\n                dir_name += \'_bias\' + str(args.mocha_init_r)\n                if args.mocha_no_denominator:\n                    dir_name += \'_denom1\'\n                if args.mocha_1dconv:\n                    dir_name += \'_1dconv\'\n                if args.mocha_quantity_loss_weight > 0:\n                    dir_name += \'_quantity\' + str(args.mocha_quantity_loss_weight)\n                if args.mocha_head_divergence_loss_weight > 0:\n                    dir_name += \'_headdiv\' + str(args.mocha_head_divergence_loss_weight)\n                if args.mocha_latency_metric:\n                    dir_name += \'_\' + args.mocha_latency_metric\n                    dir_name += str(args.mocha_latency_loss_weight)\n            if args.mocha_first_layer > 1:\n                dir_name += \'_from\' + str(args.mocha_first_layer) + \'L\'\n            if args.dropout_dec_layer > 0:\n                dir_name += \'droplayer\' + str(args.dropout_dec_layer)\n            if args.dropout_head > 0:\n                dir_name += \'drophead\' + str(args.dropout_head)\n        elif \'asg\' not in args.dec_type:\n            dir_name += str(args.dec_n_units) + \'H\'\n            if args.dec_n_projs > 0:\n                dir_name += str(args.dec_n_projs) + \'P\'\n            dir_name += str(args.dec_n_layers) + \'L\'\n            if \'transducer\' not in args.dec_type:\n                dir_name += \'_\' + args.attn_type\n                if args.attn_sigmoid:\n                    dir_name += \'_sig\'\n                if \'mocha\' in args.attn_type:\n                    dir_name += \'_w\' + str(args.mocha_chunk_size)\n                    if args.mocha_n_heads_mono > 1:\n                        dir_name += \'_ma\' + str(args.mocha_n_heads_mono) + \'H\'\n                    if args.mocha_no_denominator:\n                        dir_name += \'_denom1\'\n                    if args.mocha_1dconv:\n                        dir_name += \'_1dconv\'\n                    if args.attn_sharpening_factor:\n                        dir_name += \'_temp\' + str(args.attn_sharpening_factor)\n                    if args.mocha_quantity_loss_weight > 0:\n                        dir_name += \'_quantity\' + str(args.mocha_quantity_loss_weight)\n                elif args.attn_type == \'gmm\':\n                    dir_name += \'_mix\' + str(args.gmm_attn_n_mixtures)\n                if args.mocha_latency_metric:\n                    dir_name += \'_\' + args.mocha_latency_metric\n                    dir_name += str(args.mocha_latency_loss_weight)\n                if args.attn_n_heads > 1:\n                    dir_name += \'_head\' + str(args.attn_n_heads)\n        if args.tie_embedding:\n            dir_name += \'_tie\'\n\n    # optimization\n    dir_name += \'_\' + args.optimizer\n    if args.optimizer == \'noam\':\n        dir_name += \'_lr\' + str(args.lr_factor)\n    else:\n        dir_name += \'_lr\' + str(args.lr)\n    dir_name += \'_bs\' + str(args.batch_size)\n    # if args.shuffle_bucket:\n    #     dir_name += \'_bucket\'\n    # if \'transformer\' in args.enc_type or \'transformer\' in args.dec_type:\n    #     dir_name += \'_\' + args.transformer_param_init\n\n    # regularization\n    if args.ctc_weight < 1 and args.ss_prob > 0:\n        dir_name += \'_ss\' + str(args.ss_prob)\n    if args.lsm_prob > 0:\n        dir_name += \'_ls\' + str(args.lsm_prob)\n    if args.warmup_n_steps > 0:\n        dir_name += \'_warmup\' + str(args.warmup_n_steps)\n    if args.accum_grad_n_steps > 1:\n        dir_name += \'_accum\' + str(args.accum_grad_n_steps)\n\n    # LM integration\n    if args.lm_fusion:\n        dir_name += \'_\' + args.lm_fusion\n\n    # MTL\n    if args.mtl_per_batch:\n        if args.ctc_weight > 0:\n            dir_name += \'_\' + args.unit + \'ctc\'\n        if args.bwd_weight > 0:\n            dir_name += \'_\' + args.unit + \'bwd\'\n        for sub in [\'sub1\', \'sub2\']:\n            if getattr(args, \'train_set_\' + sub):\n                dir_name += \'_\' + getattr(args, \'unit_\' + sub) + str(getattr(args, \'vocab_\' + sub))\n                if getattr(args, \'ctc_weight_\' + sub) > 0:\n                    dir_name += \'ctc\'\n                if getattr(args, sub + \'_weight\') - getattr(args, \'ctc_weight_\' + sub) > 0:\n                    dir_name += \'fwd\'\n    else:\n        if args.ctc_weight > 0:\n            dir_name += \'_ctc\' + str(args.ctc_weight)\n        if args.bwd_weight > 0:\n            dir_name += \'_bwd\' + str(args.bwd_weight)\n        for sub in [\'sub1\', \'sub2\']:\n            if getattr(args, sub + \'_weight\') > 0:\n                dir_name += \'_\' + getattr(args, \'unit_\' + sub) + str(getattr(args, \'vocab_\' + sub))\n                if getattr(args, \'ctc_weight_\' + sub) > 0:\n                    dir_name += \'ctc\' + str(getattr(args, \'ctc_weight_\' + sub))\n                if getattr(args, sub + \'_weight\') - getattr(args, \'ctc_weight_\' + sub) > 0:\n                    dir_name += \'fwd\' + str(1 - getattr(args, sub + \'_weight\') - getattr(args, \'ctc_weight_\' + sub))\n    if args.task_specific_layer:\n        dir_name += \'_tsl\'\n\n    # SpecAugment\n    if args.n_freq_masks > 0:\n        dir_name += \'_\' + str(args.freq_width) + \'FM\' + str(args.n_freq_masks)\n    if args.n_time_masks > 0:\n        if args.adaptive_number_ratio > 0 or args.adaptive_size_ratio > 0:\n            dir_name += \'_pnum\' + str(args.adaptive_number_ratio)\n            dir_name += \'_psize\' + str(args.adaptive_size_ratio)\n        else:\n            dir_name += \'_\' + str(args.time_width) + \'TM\' + str(args.n_time_masks)\n    if args.weight_noise:\n        dir_name += \'_weightnoise\'\n\n    # contextualization\n    if args.discourse_aware:\n        dir_name += \'_discourse\'\n    if args.mem_len > 0:\n        dir_name += \'_mem\' + str(args.mem_len)\n    if args.bptt > 0:\n        dir_name += \'_bptt\' + str(args.bptt)\n\n    # Pre-training\n    if args.asr_init and os.path.isfile(args.asr_init):\n        conf_init = load_config(os.path.join(os.path.dirname(args.asr_init), \'conf.yml\'))\n        dir_name += \'_\' + conf_init[\'unit\'] + \'pt\'\n    if args.freeze_encoder:\n        dir_name += \'_encfreeze\'\n    if args.lm_init:\n        dir_name += \'_lminit\'\n\n    # knowledge distillation\n    if args.teacher:\n        dir_name += \'_KD\' + str(args.soft_label_weight)\n    if args.teacher_lm:\n        dir_name += \'_lmKD\' + str(args.soft_label_weight)\n\n    # MBR training\n    if args.mbr_training:\n        dir_name += \'_MBR\' + str(args.recog_beam_width) + \'best\'\n        dir_name += \'_ce\' + str(args.mbr_ce_weight) + \'_smooth\' + str(args.recog_softmax_smoothing)\n\n    if args.n_gpus > 1:\n        dir_name += \'_\' + str(args.n_gpus) + \'GPU\'\n    return dir_name\n\n\ndef set_lm_name(args):\n    dir_name = args.lm_type\n    if \'transformer\' in args.lm_type:\n        dir_name += str(args.transformer_d_model) + \'dmodel\'\n        dir_name += str(args.transformer_d_ff) + \'dff\'\n        dir_name += str(args.n_layers) + \'L\'\n        dir_name += str(args.transformer_n_heads) + \'H\'\n        if getattr(args, \'transformer_pe_type\', False):\n            dir_name += \'pe\' + str(args.transformer_pe_type)\n    elif \'gated_conv\' not in args.lm_type or args.lm_type == \'gated_conv_custom\':\n        dir_name += str(args.n_units) + \'H\'\n        dir_name += str(args.n_projs) + \'P\'\n        dir_name += str(args.n_layers) + \'L\'\n    if \'transformer\' not in args.lm_type:\n        dir_name += \'_emb\' + str(args.emb_dim)\n    dir_name += \'_\' + args.optimizer\n    if args.optimizer == \'noam\':\n        dir_name += \'_lr\' + str(args.lr_factor)\n    else:\n        dir_name += \'_lr\' + str(args.lr)\n    dir_name += \'_bs\' + str(args.batch_size)\n    dir_name += \'_bptt\' + str(args.bptt)\n    if getattr(args, \'mem_len\', 0) > 0:\n        dir_name += \'_mem\' + str(args.mem_len)\n    if getattr(args, \'zero_center_offset\', False):\n        dir_name += \'_zero_center\'\n    if args.tie_embedding:\n        dir_name += \'_tie\'\n    if \'lstm\' in args.lm_type or \'gru\' in args.lm_type:\n        if args.residual:\n            dir_name += \'_residual\'\n        if args.use_glu:\n            dir_name += \'_glu\'\n    if args.n_units_null_context > 0:\n        dir_name += \'_nullcv\' + str(args.n_units_null_context)\n\n    # regularization\n    dir_name += \'_dropI\' + str(args.dropout_in) + \'H\' + str(args.dropout_hidden)\n    if getattr(args, \'dropout_layer\', 0) > 0:\n        dir_name += \'Layer\' + str(args.dropout_layer)\n    if args.lsm_prob > 0:\n        dir_name += \'_ls\' + str(args.lsm_prob)\n    if args.warmup_n_steps > 0:\n        dir_name += \'_warmup\' + str(args.warmup_n_steps)\n    if args.accum_grad_n_steps > 1:\n        dir_name += \'_accum\' + str(args.accum_grad_n_steps)\n\n    if args.backward:\n        dir_name += \'_bwd\'\n    if args.shuffle:\n        dir_name += \'_shuffle\'\n    if args.serialize:\n        dir_name += \'_serialize\'\n    if args.min_n_tokens > 1:\n        dir_name += \'_\' + str(args.min_n_tokens) + \'tokens\'\n    if args.adaptive_softmax:\n        dir_name += \'_adaptiveSM\'\n    return dir_name\n'"
neural_sp/bin/plot_utils.py,0,"b'#! /usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n# Copyright 2018 Kyoto University (Hirofumi Inaguma)\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n\n""""""Plot attention weights & ctc probabilities.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport seaborn as sns\nimport numpy as np\nfrom matplotlib import pyplot as plt\nimport matplotlib\nmatplotlib.use(\'Agg\')\n\nplt.style.use(\'ggplot\')\nsns.set_style(""white"")\nblue = \'#4682B4\'\norange = \'#D2691E\'\ngreen = \'#006400\'\n\n# sns.set(font=\'IPAMincho\')\nsns.set(font=\'Noto Sans CJK JP\')\n\n\ndef plot_attention_weights(aw, tokens=[], spectrogram=None, ref=None,\n                           save_path=None, figsize=(20, 6),\n                           ctc_probs=None, ctc_topk_ids=None):\n    """"""Plot attention weights.\n\n    Args:\n        aw (np.ndarray): A tensor of size `[H, L, T]\n        tokens (list): hypothesis tokens\n        spectrogram (np.ndarray): A tensor of size `[T, feature_dim]`\n        ref (str): reference text\n        save_path (str): path to save a figure\n        figsize (tuple):\n\n    """"""\n    n_heads = aw.shape[0]\n    n_col = n_heads\n    if spectrogram is not None:\n        n_col += 1\n    if ctc_probs is not None:\n        n_col += 1\n    if n_heads > 1:\n        figsize = (20, 16)\n\n    plt.clf()\n    plt.figure(figsize=figsize)\n    # Plot attention weights\n    for h in range(1, n_heads + 1):\n        # plt.subplot(n_col, 1, h)\n        plt.subplot(n_col, 1, n_heads - h + 1)\n        sns.heatmap(aw[h - 1, :, :], cmap=\'viridis\',\n                    xticklabels=False,\n                    yticklabels=tokens if len(tokens) > 0 else False,\n                    cbar=False,\n                    cbar_kws={""orientation"": ""horizontal""})\n        plt.ylabel(u\'Output labels (\xe2\x86\x90)\', fontsize=12 if n_heads == 1 else 8)\n        plt.yticks(rotation=0, fontsize=6)\n\n    # Plot CTC propabilities for joint CTC-attention\n    if ctc_probs is not None:\n        plt.subplot(n_col, 1, n_heads + 1)\n        times_probs = np.arange(ctc_probs.shape[0])\n        for idx in set(ctc_topk_ids.reshape(-1).tolist()):\n            if idx == 0:\n                plt.plot(times_probs, ctc_probs[:, 0], \':\', label=\'<blank>\', color=\'grey\')\n            else:\n                plt.plot(times_probs, ctc_probs[:, idx])\n        plt.ylabel(\'CTC posteriors\', fontsize=12 if n_heads == 1 else 8)\n        plt.tick_params(labelbottom=False)\n        plt.yticks(list(range(0, 2, 1)))\n        plt.xlim(0, ctc_probs.shape[0])\n\n    # Plot spectrogram\n    if spectrogram is not None:\n        plt.subplot(n_col, 1, n_col)\n        plt.imshow(spectrogram.T, cmap=\'viridis\', aspect=\'auto\', origin=\'lower\')\n        plt.xlabel(u\'Time [msec]\', fontsize=12)\n        plt.ylabel(u\'Frequency bin\', fontsize=12 if n_heads == 1 else 8)\n        # plt.colorbar()\n        plt.grid(\'off\')\n\n    # if ref is not None:\n    #     plt.title(\'REF: \' + ref + \'\\n\' + \'HYP: \' + \' \'.join(tokens).replace(\'\xe2\x96\x81\', \' \'), fontsize=12)\n\n    # Save as a png file\n    if save_path is not None:\n        plt.savefig(save_path, dvi=100)\n\n    plt.close()\n\n\ndef plot_hierarchical_attention_weights(aw, aw_sub, tokens=[], tokens_sub=[],\n                                        spectrogram=None, ref=None,\n                                        save_path=None, figsize=(20, 6)):\n    """"""Plot attention weights for the hierarchical model.\n\n    Args:\n        spectrogram (np.ndarray): A tensor of size `[T, input_size]`\n        aw (np.ndarray): A tensor of size `[H, L, T]`\n        aw_sub (np.ndarray): A tensor of size `[H_sub, L_sub, T]`\n        tokens (list):\n        tokens_sub (list):\n        spectrogram (np.ndarray): A tensor of size `[T, feature_dim]`\n        ref (str):\n        save_path (str): path to save a figure\n        figsize (tuple):\n\n    """"""\n    plt.clf()\n    plt.figure(figsize=figsize)\n\n    if spectrogram is None:\n        plt.subplot(211)\n        sns.heatmap(aw[0], cmap=\'viridis\',\n                    xticklabels=False,\n                    yticklabels=tokens if len(tokens) > 0 else False)\n        plt.ylabel(u\'Output labels (main) (\xe2\x86\x90)\', fontsize=12)\n        plt.yticks(rotation=0, fontsize=6)\n\n        plt.subplot(212)\n        sns.heatmap(aw_sub[0], cmap=\'viridis\',\n                    xticklabels=False,\n                    yticklabels=tokens_sub if len(tokens_sub) > 0 else False)\n        plt.xlabel(u\'Time [sec]\', fontsize=12)\n        plt.ylabel(u\'Output labels (sub) (\xe2\x86\x90)\', fontsize=12)\n        plt.yticks(rotation=0, fontsize=6)\n    else:\n        plt.subplot(311)\n        sns.heatmap(aw[0], cmap=\'viridis\',\n                    xticklabels=False,\n                    yticklabels=tokens if len(tokens) > 0 else False)\n        plt.ylabel(u\'Output labels (main) (\xe2\x86\x90)\', fontsize=12)\n        plt.yticks(rotation=0, fontsize=6)\n\n        plt.subplot(312)\n        sns.heatmap(aw_sub[0], cmap=\'viridis\',\n                    xticklabels=False,\n                    yticklabels=tokens_sub if len(tokens_sub) > 0 else False)\n        plt.ylabel(u\'Output labels (sub) (\xe2\x86\x90)\', fontsize=12)\n        plt.yticks(rotation=0, fontsize=6)\n\n        # Plot spectrogram\n        plt.subplot(313)\n        plt.imshow(spectrogram.T, cmap=\'viridis\', aspect=\'auto\', origin=\'lower\')\n        plt.xlabel(u\'Time [msec]\', fontsize=12)\n        plt.ylabel(u\'Frequency bin\', fontsize=12)\n        plt.colorbar()\n        plt.grid(\'off\')\n\n    # Save as a png file\n    if save_path is not None:\n        plt.savefig(save_path, dvi=100)\n\n    plt.close()\n\n\ndef plot_ctc_probs(ctc_probs, topk_ids, subsample_factor, space=-1, hyp=\'\',\n                   spectrogram=None, save_path=None, figsize=(20, 6), topk=None):\n    """"""Plot CTC posteriors.\n\n    Args:\n        ctc_probs (np.ndarray): A tensor of size `[T, vocab]`\n        topk_ids ():\n        subsample_factor (int): the number of frames to stack\n        space (int): index for space mark\n        hyp (str):\n        save_path (str): path to save a figure\n        figsize (tuple):\n        topk (int):\n\n    """"""\n    plt.clf()\n    plt.figure(figsize=figsize)\n    n_frames = ctc_probs.shape[0]\n    times_probs = np.arange(n_frames) * subsample_factor / 100\n    if len(hyp) > 0:\n        plt.title(hyp)\n\n    plt.xlim([0, n_frames * subsample_factor / 100])\n    plt.ylim([0.05, 1.05])\n    plt.legend(loc=""upper right"", fontsize=12)\n\n    if spectrogram is None:\n        # NOTE: index 0 is reserved for blank\n        for idx in set(topk_ids.reshape(-1).tolist()):\n            if idx == 0:\n                plt.plot(times_probs, ctc_probs[:, 0], \':\', label=\'<blank>\', color=\'grey\')\n            elif idx == space:\n                plt.plot(times_probs, ctc_probs[:, space], label=\'<space>\', color=\'black\')\n            else:\n                plt.plot(times_probs, ctc_probs[:, idx])\n        plt.xlabel(u\'Time [sec]\', fontsize=12)\n        plt.ylabel(\'Posteriors\', fontsize=12)\n        plt.xticks(list(range(0, int(n_frames * subsample_factor / 100) + 1, 1)))\n        plt.yticks(list(range(0, 2, 1)))\n    else:\n        plt.subplot(211)\n        # NOTE: index 0 is reserved for blank\n        for idx in set(topk_ids.reshape(-1).tolist()):\n            if idx == 0:\n                plt.plot(times_probs, ctc_probs[:, 0], \':\', label=\'<blank>\', color=\'grey\')\n            elif idx == space:\n                plt.plot(times_probs, ctc_probs[:, space], label=\'<space>\', color=\'black\')\n            else:\n                plt.plot(times_probs, ctc_probs[:, idx])\n        plt.ylabel(\'Posteriors\', fontsize=12)\n        plt.tick_params(labelbottom=False)\n        plt.yticks(list(range(0, 2, 1)))\n\n        # Plot spectrogram\n        plt.subplot(212)\n        plt.imshow(spectrogram.T, cmap=\'viridis\', aspect=\'auto\', origin=\'lower\')\n        plt.xlabel(u\'Time [msec]\', fontsize=12)\n        plt.ylabel(u\'Frequency bin\', fontsize=12)\n        # plt.colorbar()\n        plt.grid(\'off\')\n\n    # Save as a png file\n    if save_path is not None:\n        plt.savefig(save_path, dvi=100)\n\n    plt.close()\n\n\ndef plot_hierarchical_ctc_probs(ctc_probs, topk_ids, ctc_probs_sub, topk_ids_sub,\n                                subsample_factor, space=-1, space_sub=-1, hyp=\'\', hyp_sub=\'\',\n                                spectrogram=None, save_path=None, figsize=(20, 6)):\n    """"""Plot CTC posteriors for the hierarchical model.\n\n    Args:\n        ctc_probs (np.ndarray): A tensor of size `[T, vocab]`\n        ctc_probs_sub (np.ndarray): A tensor of size `[T, num_classes_sub]`\n        n_frames (int):\n        subsample_factor (int):\n        save_path (str): path to save a figure\n        figsize (tuple):\n        space (int): index for space mark\n\n    """"""\n    # TODO(hirofumi): add spectrogram\n\n    plt.clf()\n    plt.figure(figsize=figsize)\n    n_frames = ctc_probs.shape[0]\n    times_probs = np.arange(n_frames) * subsample_factor / 100\n    if len(hyp) > 0:\n        plt.title(hyp)\n        # plt.title(hyp_sub)\n\n    # NOTE: index 0 is reserved for blank\n    plt.subplot(211)\n    plt.plot(times_probs, ctc_probs[:, 0], \':\', label=\'<blank>\', color=\'grey\')\n\n    # Plot only top-k\n    for idx in set(topk_ids.reshape(-1).tolist()):\n        if idx == 0:\n            plt.plot(times_probs, ctc_probs[:, 0], \':\', label=\'<blank>\', color=\'grey\')\n        elif idx == space:\n            plt.plot(times_probs, ctc_probs[:, space], label=\'<space>\', color=\'black\')\n        else:\n            plt.plot(times_probs, ctc_probs[:, idx])\n\n    plt.ylabel(\'Posteriors (Word)\', fontsize=12)\n    plt.xlim([0, n_frames * subsample_factor / 100])\n    plt.ylim([0.05, 1.05])\n    plt.xticks(list(range(0, int(n_frames * subsample_factor / 100) + 1, 1)))\n    plt.yticks(list(range(0, 2, 1)))\n    plt.legend(loc=""upper right"", fontsize=12)\n\n    plt.subplot(212)\n    plt.plot(times_probs, ctc_probs_sub[:, 0], \':\', label=\'<blank>\', color=\'grey\')\n    for idx in set(topk_ids_sub.reshape(-1).tolist()):\n        if idx == 0:\n            plt.plot(times_probs, ctc_probs_sub[:, 0], \':\', label=\'<blank>\', color=\'grey\')\n        elif idx == space_sub:\n            plt.plot(times_probs, ctc_probs_sub[:, space_sub], label=\'<space>\', color=\'black\')\n        else:\n            plt.plot(times_probs, ctc_probs_sub[:, idx])\n    plt.xlabel(u\'Time [sec]\', fontsize=12)\n    plt.ylabel(\'Posteriors (Char)\', fontsize=12)\n    plt.xlim([0, n_frames * subsample_factor / 100])\n    plt.ylim([0.05, 1.05])\n    plt.xticks(list(range(0, int(n_frames * subsample_factor / 100) + 1, 1)))\n    plt.yticks(list(range(0, 2, 1)))\n    plt.legend(loc=""upper right"", fontsize=12)\n\n    # Save as a png file\n    if save_path is not None:\n        plt.savefig(save_path, dvi=100)\n\n    plt.close()\n'"
neural_sp/bin/train_utils.py,3,"b'#! /usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n# Copyright 2018 Kyoto University (Hirofumi Inaguma)\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n\n""""""Utility functions for training.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport functools\nimport logging\nimport numpy as np\nimport os\nimport time\nimport torch\nimport yaml\n\nlogger = logging.getLogger(__name__)\n\n\ndef compute_susampling_factor(args):\n    """"""Register subsample factor to args.\n\n        Args:\n            args (Namespace):\n        Returns:\n            args (Namespace):\n\n    """"""\n    if args.resume:\n        return args\n\n    args.subsample_factor = 1\n    args.subsample_factor_sub1 = 1\n    args.subsample_factor_sub2 = 1\n    subsample = [int(s) for s in args.subsample.split(\'_\')]\n    if \'conv\' in args.enc_type and args.conv_poolings:\n        for p in args.conv_poolings.split(\'_\'):\n            args.subsample_factor *= int(p.split(\',\')[0].replace(\'(\', \'\'))\n    else:\n        args.subsample_factor = int(np.prod(subsample))\n    if args.train_set_sub1:\n        if \'conv\' in args.enc_type and args.conv_poolings:\n            args.subsample_factor_sub1 = args.subsample_factor * \\\n                int(np.prod(subsample[:args.enc_n_layers_sub1 - 1]))\n        else:\n            args.subsample_factor_sub1 = args.subsample_factor\n    if args.train_set_sub2:\n        if \'conv\' in args.enc_type and args.conv_poolings:\n            args.subsample_factor_sub2 = args.subsample_factor * \\\n                int(np.prod(subsample[:args.enc_n_layers_sub2 - 1]))\n        else:\n            args.subsample_factor_sub2 = args.subsample_factor\n\n    return args\n\n\ndef measure_time(func):\n    @functools.wraps(func)\n    def _measure_time(*args, **kwargs):\n        start = time.time()\n        func(*args, **kwargs)\n        elapse = time.time() - start\n        print(""Takes {} seconds."".format(elapse))\n    return _measure_time\n\n\ndef load_config(config_path):\n    """"""Load a configration yaml file.\n\n    Args:\n        config_path (str):\n    Returns:\n        params (dict):\n\n    """"""\n    with open(config_path, ""r"") as f:\n        conf = yaml.load(f, Loader=yaml.FullLoader)\n\n    params = conf[\'param\']\n    return params\n\n\ndef save_config(conf, save_path):\n    """"""Save a configuration file as a yaml file.\n\n    Args:\n        conf (dict):\n\n    """"""\n    with open(os.path.join(save_path), ""w"") as f:\n        f.write(yaml.dump({\'param\': conf}, default_flow_style=False))\n\n\ndef set_logger(save_path, stdout=False):\n    """"""Set logger.\n\n    Args:\n        save_path (str): path to save a log file\n        stdout (bool):\n\n    """"""\n    format = \'%(asctime)s %(name)s line:%(lineno)d %(levelname)s: %(message)s\'\n    logging.basicConfig(level=logging.DEBUG if stdout else logging.INFO,\n                        format=format,\n                        filename=save_path if not stdout else None)\n\n\ndef set_save_path(save_path):\n    """"""Change directory name to avoid name ovarlapping.\n\n    Args:\n        save_path (str):\n    Returns:\n        save_path_new (str):\n\n    """"""\n    # Reset model directory\n    model_idx = 0\n    save_path_new = save_path\n    while True:\n        if os.path.isfile(os.path.join(save_path_new, \'conf.yml\')):\n            # Training of the first model have not been finished yet\n            model_idx += 1\n            save_path_new = save_path + \'_\' + str(model_idx)\n        else:\n            break\n    if not os.path.isdir(save_path_new):\n        os.mkdir(save_path_new)\n    return save_path_new\n\n\ndef load_checkpoint(model, checkpoint_path, optimizer=None):\n    """"""Load checkpoint.\n\n    Args:\n        model (torch.nn.Module):\n        checkpoint_path (str): path to the saved model (model..epoch-*)\n        optimizer (LRScheduler): optimizer wrapped by LRScheduler class\n    Returns:\n        topk_list (list): list of (epoch, metric)\n\n    """"""\n    if not os.path.isfile(checkpoint_path):\n        raise ValueError(\'There is no checkpoint\')\n\n    if os.path.isfile(checkpoint_path):\n        checkpoint = torch.load(checkpoint_path, map_location=lambda storage, loc: storage)\n    else:\n        raise ValueError(""No checkpoint found at %s"" % checkpoint_path)\n\n    # Restore parameters\n    if \'avg\' not in checkpoint_path:\n        epoch = int(os.path.basename(checkpoint_path).split(\'-\')[-1]) - 1\n        logger.info(""=> Loading checkpoint (epoch:%d): %s"" % (epoch + 1, checkpoint_path))\n    else:\n        logger.info(""=> Loading checkpoint: %s"" % checkpoint_path)\n    try:\n        model.load_state_dict(checkpoint[\'model_state_dict\'])\n    except KeyError:\n        model.load_state_dict(checkpoint[\'state_dict\'])\n        checkpoint[\'model_state_dict\'] = checkpoint[\'state_dict\']\n        checkpoint[\'optimizer_state_dict\'] = checkpoint[\'optimizer\']\n        del checkpoint[\'state_dict\']\n        del checkpoint[\'optimizer\']\n        torch.save(checkpoint, checkpoint_path + \'.tmp\')\n\n    # Restore optimizer\n    if optimizer is not None:\n        optimizer.load_state_dict(checkpoint[\'optimizer_state_dict\'])\n        # NOTE: fix this later\n        optimizer.optimizer.param_groups[0][\'params\'] = []\n        for param_group in list(model.parameters()):\n            optimizer.optimizer.param_groups[0][\'params\'].append(param_group)\n    else:\n        logger.warning(\'Optimizer is not loaded.\')\n\n    if \'optimizer_state_dict\' in checkpoint.keys() and \'topk_list\' in checkpoint[\'optimizer_state_dict\'].keys():\n        topk_list = checkpoint[\'optimizer_state_dict\'][\'topk_list\']\n    else:\n        topk_list = []\n    return topk_list\n'"
neural_sp/datasets/__init__.py,0,b''
neural_sp/datasets/asr.py,0,"b'#! /usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n# Copyright 2018 Kyoto University (Hirofumi Inaguma)\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n\n""""""Base class for loading dataset for ASR.\n   In this class, all data will be loaded at each step.\n   You can use the multi-GPU version.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport codecs\nimport kaldiio\nimport numpy as np\nimport os\nimport pandas as pd\nimport random\n\nfrom neural_sp.datasets.token_converter.character import Char2idx\nfrom neural_sp.datasets.token_converter.character import Idx2char\nfrom neural_sp.datasets.token_converter.phone import Idx2phone\nfrom neural_sp.datasets.token_converter.phone import Phone2idx\nfrom neural_sp.datasets.token_converter.word import Idx2word\nfrom neural_sp.datasets.token_converter.word import Word2idx\nfrom neural_sp.datasets.token_converter.wordpiece import Idx2wp\nfrom neural_sp.datasets.token_converter.wordpiece import Wp2idx\n\nrandom.seed(1)\nnp.random.seed(1)\n\n\ndef count_vocab_size(dict_path):\n    vocab_count = 1  # for <blank>\n    with codecs.open(dict_path, \'r\', \'utf-8\') as f:\n        for line in f:\n            if line.strip() != \'\':\n                vocab_count += 1\n    return vocab_count\n\n\nclass Dataset(object):\n\n    def __init__(self, tsv_path, dict_path,\n                 unit, batch_size, nlsyms=False, n_epochs=1e10,\n                 is_test=False, min_n_frames=40, max_n_frames=2000,\n                 shuffle_bucket=False, sort_by=\'utt_id\',\n                 short2long=False, sort_stop_epoch=1000, dynamic_batching=False,\n                 ctc=False, subsample_factor=1, wp_model=False, corpus=\'\',\n                 tsv_path_sub1=False, dict_path_sub1=False, unit_sub1=False,\n                 wp_model_sub1=False, ctc_sub1=False, subsample_factor_sub1=1,\n                 tsv_path_sub2=False, dict_path_sub2=False, unit_sub2=False,\n                 wp_model_sub2=False, ctc_sub2=False, subsample_factor_sub2=1,\n                 discourse_aware=False):\n        """"""A class for loading dataset.\n\n        Args:\n            tsv_path (str): path to the dataset tsv file\n            dict_path (str): path to the dictionary\n            unit (str): word or wp or char or phone or word_char\n            batch_size (int): size of mini-batch\n            nlsyms (str): path to the non-linguistic symbols file\n            n_epochs (int): total epochs for training.\n            is_test (bool):\n            min_n_frames (int): exclude utterances shorter than this value\n            max_n_frames (int): exclude utterances longer than this value\n            shuffle_bucket (bool): gather the similar length of utterances and shuffle them\n            sort_by (str): sort all utterances in the ascending order\n                input: sort by input length\n                output: sort by output length\n                shuffle: shuffle all utterances\n            short2long (bool): sort utterances in the descending order\n            sort_stop_epoch (int): After sort_stop_epoch, training will revert\n                back to a random order\n            dynamic_batching (bool): change batch size dynamically in training\n            ctc (bool):\n            subsample_factor (int):\n            wp_model (): path to the word-piece model for sentencepiece\n            corpus (str): name of corpus\n            discourse_aware (bool):\n\n        """"""\n        super(Dataset, self).__init__()\n\n        self.epoch = 0\n        self.iteration = 0\n        self.offset = 0\n\n        self.set = os.path.basename(tsv_path).split(\'.\')[0]\n        self.is_test = is_test\n        self.unit = unit\n        self.unit_sub1 = unit_sub1\n        self.batch_size = batch_size\n        self.max_epoch = n_epochs\n        self.shuffle_bucket = shuffle_bucket\n        if shuffle_bucket:\n            assert sort_by in [\'input\', \'output\']\n        self.sort_stop_epoch = sort_stop_epoch\n        self.sort_by = sort_by\n        assert sort_by in [\'input\', \'output\', \'shuffle\', \'utt_id\']\n        self.dynamic_batching = dynamic_batching\n        self.corpus = corpus\n        self.discourse_aware = discourse_aware\n        if discourse_aware:\n            assert not is_test\n\n        self.vocab = count_vocab_size(dict_path)\n        self.eos = 2\n        self.pad = 3\n        # NOTE: reserved in advance\n\n        self.idx2token = []\n        self.token2idx = []\n\n        # Set index converter\n        if unit in [\'word\', \'word_char\']:\n            self.idx2token += [Idx2word(dict_path)]\n            self.token2idx += [Word2idx(dict_path, word_char_mix=(unit == \'word_char\'))]\n        elif unit == \'wp\':\n            self.idx2token += [Idx2wp(dict_path, wp_model)]\n            self.token2idx += [Wp2idx(dict_path, wp_model)]\n        elif unit == \'char\':\n            self.idx2token += [Idx2char(dict_path)]\n            self.token2idx += [Char2idx(dict_path, nlsyms=nlsyms)]\n        elif \'phone\' in unit:\n            self.idx2token += [Idx2phone(dict_path)]\n            self.token2idx += [Phone2idx(dict_path)]\n        else:\n            raise ValueError(unit)\n\n        for i in range(1, 3):\n            dict_path_sub = locals()[\'dict_path_sub\' + str(i)]\n            wp_model_sub = locals()[\'wp_model_sub\' + str(i)]\n            unit_sub = locals()[\'unit_sub\' + str(i)]\n            if dict_path_sub:\n                setattr(self, \'vocab_sub\' + str(i), count_vocab_size(dict_path_sub))\n\n                # Set index converter\n                if unit_sub:\n                    if unit_sub == \'wp\':\n                        self.idx2token += [Idx2wp(dict_path_sub, wp_model_sub)]\n                        self.token2idx += [Wp2idx(dict_path_sub, wp_model_sub)]\n                    elif unit_sub == \'char\':\n                        self.idx2token += [Idx2char(dict_path_sub)]\n                        self.token2idx += [Char2idx(dict_path_sub, nlsyms=nlsyms)]\n                    elif \'phone\' in unit_sub:\n                        self.idx2token += [Idx2phone(dict_path_sub)]\n                        self.token2idx += [Phone2idx(dict_path_sub)]\n                    else:\n                        raise ValueError(unit_sub)\n            else:\n                setattr(self, \'vocab_sub\' + str(i), -1)\n\n        # Load dataset tsv file\n        df = pd.read_csv(tsv_path, encoding=\'utf-8\', delimiter=\'\\t\')\n        df = df.loc[:, [\'utt_id\', \'speaker\', \'feat_path\',\n                        \'xlen\', \'xdim\', \'text\', \'token_id\', \'ylen\', \'ydim\']]\n        for i in range(1, 3):\n            if locals()[\'tsv_path_sub\' + str(i)]:\n                df_sub = pd.read_csv(locals()[\'tsv_path_sub\' + str(i)], encoding=\'utf-8\', delimiter=\'\\t\')\n                df_sub = df_sub.loc[:, [\'utt_id\', \'speaker\', \'feat_path\',\n                                        \'xlen\', \'xdim\', \'text\', \'token_id\', \'ylen\', \'ydim\']]\n                setattr(self, \'df_sub\' + str(i), df_sub)\n            else:\n                setattr(self, \'df_sub\' + str(i), None)\n        self.input_dim = kaldiio.load_mat(df[\'feat_path\'][0]).shape[-1]\n\n        # Remove inappropriate utterances\n        if is_test or discourse_aware:\n            print(\'Original utterance num: %d\' % len(df))\n            n_utts = len(df)\n            df = df[df.apply(lambda x: x[\'ylen\'] > 0, axis=1)]\n            print(\'Removed %d empty utterances\' % (n_utts - len(df)))\n        else:\n            print(\'Original utterance num: %d\' % len(df))\n            n_utts = len(df)\n            df = df[df.apply(lambda x: min_n_frames <= x[\n                \'xlen\'] <= max_n_frames, axis=1)]\n            df = df[df.apply(lambda x: x[\'ylen\'] > 0, axis=1)]\n            print(\'Removed %d utterances (threshold)\' % (n_utts - len(df)))\n\n            if ctc and subsample_factor > 1:\n                n_utts = len(df)\n                df = df[df.apply(lambda x: x[\'ylen\'] <= (x[\'xlen\'] // subsample_factor), axis=1)]\n                print(\'Removed %d utterances (for CTC)\' % (n_utts - len(df)))\n\n            for i in range(1, 3):\n                df_sub = getattr(self, \'df_sub\' + str(i))\n                ctc_sub = locals()[\'ctc_sub\' + str(i)]\n                subsample_factor_sub = locals()[\'subsample_factor_sub\' + str(i)]\n                if df_sub is not None:\n                    if ctc_sub and subsample_factor_sub > 1:\n                        df_sub = df_sub[df_sub.apply(\n                            lambda x: x[\'ylen\'] <= (x[\'xlen\'] // subsample_factor_sub), axis=1)]\n\n                    if len(df) != len(df_sub):\n                        n_utts = len(df)\n                        df = df.drop(df.index.difference(df_sub.index))\n                        print(\'Removed %d utterances (for CTC, sub%d)\' % (n_utts - len(df), i))\n                        for j in range(1, i + 1):\n                            setattr(self, \'df_sub\' + str(j),\n                                    getattr(self, \'df_sub\' + str(j)).drop(getattr(self, \'df_sub\' + str(j)).index.difference(df.index)))\n\n        if corpus == \'swbd\':\n            # 1. serialize\n            # df[\'session\'] = df[\'speaker\'].apply(lambda x: str(x).split(\'-\')[0])\n            # 2. not serialize\n            df[\'session\'] = df[\'speaker\'].apply(lambda x: str(x))\n        else:\n            df[\'session\'] = df[\'speaker\'].apply(lambda x: str(x))\n\n        # Sort tsv records\n        if discourse_aware:\n            # Sort by onset (start time)\n            df = df.assign(prev_utt=\'\')\n            df = df.assign(line_no=list(range(len(df))))\n            if corpus == \'swbd\':\n                df[\'onset\'] = df[\'utt_id\'].apply(lambda x: int(x.split(\'_\')[-1].split(\'-\')[0]))\n            elif corpus == \'csj\':\n                df[\'onset\'] = df[\'utt_id\'].apply(lambda x: int(x.split(\'_\')[1]))\n            elif corpus == \'tedlium2\':\n                df[\'onset\'] = df[\'utt_id\'].apply(lambda x: int(x.split(\'-\')[-2]))\n            else:\n                raise NotImplementedError(corpus)\n            df = df.sort_values(by=[\'session\', \'onset\'], ascending=True)\n\n            # Extract previous utterances\n            groups = df.groupby(\'session\').groups\n            df[\'prev_utt\'] = df.apply(\n                lambda x: [df.loc[i, \'line_no\']\n                           for i in groups[x[\'session\']] if df.loc[i, \'onset\'] < x[\'onset\']], axis=1)\n            df[\'n_prev_utt\'] = df.apply(lambda x: len(x[\'prev_utt\']), axis=1)\n            df[\'n_utt_in_session\'] = df.apply(\n                lambda x: len([i for i in groups[x[\'session\']]]), axis=1)\n            df = df.sort_values(by=[\'n_utt_in_session\'], ascending=short2long)\n\n            # NOTE: this is used only when LM is trained with seliarize: true\n            # if is_test and corpus == \'swbd\':\n            #     # Sort by onset\n            #     df[\'onset\'] = df[\'utt_id\'].apply(lambda x: int(x.split(\'_\')[-1].split(\'-\')[0]))\n            #     df = df.sort_values(by=[\'session\', \'onset\'], ascending=True)\n\n        elif not is_test:\n            if sort_by == \'input\':\n                df = df.sort_values(by=[\'xlen\'], ascending=short2long)\n            elif sort_by == \'output\':\n                df = df.sort_values(by=[\'ylen\'], ascending=short2long)\n            elif sort_by == \'shuffle\':\n                df = df.reindex(np.random.permutation(self.df.index))\n\n        # Re-indexing\n        if discourse_aware:\n            self.df = df\n            for i in range(1, 3):\n                if getattr(self, \'df_sub\' + str(i)) is not None:\n                    setattr(self, \'df_sub\' + str(i),\n                            getattr(self, \'df_sub\' + str(i)).reindex(df.index))\n        else:\n            self.df = df.reset_index()\n            for i in range(1, 3):\n                if getattr(self, \'df_sub\' + str(i)) is not None:\n                    setattr(self, \'df_sub\' + str(i),\n                            getattr(self, \'df_sub\' + str(i)).reindex(df.index).reset_index())\n\n        if discourse_aware:\n            self.df_indices_buckets = self.discourse_bucketing(batch_size)\n        elif shuffle_bucket:\n            self.df_indices_buckets = self.shuffle_bucketing(batch_size)\n        else:\n            self.df_indices = list(self.df.index)\n\n    def __len__(self):\n        return len(self.df)\n\n    @property\n    def epoch_detail(self):\n        """"""Percentage of the current epoch.""""""\n        return self.offset / len(self)\n\n    @property\n    def n_frames(self):\n        return self.df[\'xlen\'].sum()\n\n    def reset(self, batch_size=None):\n        """"""Reset data counter and offset.\n\n            Args:\n                batch_size (int): size of mini-batch\n\n        """"""\n        if batch_size is None:\n            batch_size = self.batch_size\n\n        if self.discourse_aware:\n            self.df_indices_buckets = self.discourse_bucketing(batch_size)\n        elif self.shuffle_bucket:\n            self.df_indices_buckets = self.shuffle_bucketing(batch_size)\n        else:\n            self.df_indices = list(self.df.index)\n        self.offset = 0\n\n    def next(self, batch_size=None):\n        """"""Generate each mini-batch.\n\n        Args:\n            batch_size (int): size of mini-batch\n        Returns:\n            mini_batch (dict):\n            is_new_epoch (bool): flag for the end of the current epoch\n\n        """"""\n        if batch_size is None:\n            batch_size = self.batch_size\n\n        if self.epoch >= self.max_epoch:\n            raise StopIteration\n\n        df_indices_mb, is_new_epoch = self.sample_index(batch_size)\n        mini_batch = self.make_mini_batch(df_indices_mb)\n\n        if is_new_epoch:\n            # shuffle the whole data\n            if self.epoch + 1 == self.sort_stop_epoch:\n                self.sort_by = \'shuffle\'\n                self.df = self.df.reindex(np.random.permutation(self.df.index))\n                for i in range(1, 3):\n                    if getattr(self, \'df_sub\' + str(i)) is not None:\n                        setattr(self, \'df_sub\' + str(i),\n                                getattr(self, \'df_sub\' + str(i)).reindex(self.df.index).reset_index())\n\n                # Re-indexing\n                self.df = self.df.reset_index()\n\n            self.reset()\n            self.epoch += 1\n\n        return mini_batch, is_new_epoch\n\n    def sample_index(self, batch_size):\n        """"""Sample data indices of mini-batch.\n\n        Args:\n            batch_size (int): size of mini-batch\n        Returns:\n            df_indices_mb (np.ndarray): indices of dataframe in the current mini-batch\n            is_new_epoch (bool): flag for the end of the current epoch\n\n        """"""\n        is_new_epoch = False\n\n        if self.discourse_aware:\n            df_indices_mb = self.df_indices_buckets.pop(0)\n            self.offset += len(df_indices_mb)\n            is_new_epoch = (len(self.df_indices_buckets) == 0)\n\n        elif self.shuffle_bucket:\n            df_indices_mb = self.df_indices_buckets.pop(0)\n            self.offset += len(df_indices_mb)\n            is_new_epoch = (len(self.df_indices_buckets) == 0)\n\n            # Shuffle uttrances in mini-batch\n            df_indices_mb = random.sample(df_indices_mb, len(df_indices_mb))\n        else:\n            if len(self.df_indices) > batch_size:\n                # Change batch size dynamically\n                min_xlen = self.df[self.offset:self.offset + 1][\'xlen\'].values[0]\n                min_ylen = self.df[self.offset:self.offset + 1][\'ylen\'].values[0]\n                batch_size = self.set_batch_size(batch_size, min_xlen, min_ylen)\n\n                df_indices_mb = list(self.df[self.offset:self.offset + batch_size].index)\n                self.offset += len(df_indices_mb)\n            else:\n                # Last mini-batch\n                df_indices_mb = self.df_indices[:]\n                self.offset = len(self)\n                is_new_epoch = True\n\n                # Change batch size dynamically\n                min_xlen = self.df[df_indices_mb[0]:df_indices_mb[0] + 1][\'xlen\'].values[0]\n                min_ylen = self.df[df_indices_mb[0]:df_indices_mb[0] + 1][\'ylen\'].values[0]\n                batch_size = self.set_batch_size(batch_size, min_xlen, min_ylen)\n\n                # Remove the rest\n                df_indices_mb = df_indices_mb[:batch_size]\n\n            # Shuffle uttrances in mini-batch\n            df_indices_mb = random.sample(df_indices_mb, len(df_indices_mb))\n\n            for i in df_indices_mb:\n                self.df_indices.remove(i)\n\n        return df_indices_mb, is_new_epoch\n\n    def make_mini_batch(self, df_indices_mb):\n        """"""Create mini-batch per step.\n\n        Args:\n            df_indices_mb (np.ndarray): indices of dataframe in the current mini-batch\n        Returns:\n            mini_batch_dict (dict):\n                xs (list): input data of size `[T, input_dim]`\n                xlens (list): lengths of xs\n                ys (list): reference labels in the main task of size `[L]`\n                ys_sub1 (list): reference labels in the 1st auxiliary task of size `[L_sub1]`\n                ys_sub2 (list): reference labels in the 2nd auxiliary task of size `[L_sub2]`\n                utt_ids (list): name of each utterance\n                speakers (list): name of each speaker\n                sessions (list): name of each session\n\n        """"""\n        # inputs\n        xs = [kaldiio.load_mat(self.df[\'feat_path\'][i]) for i in df_indices_mb]\n\n        # outputs\n        if self.is_test:\n            ys = [self.token2idx[0](self.df[\'text\'][i]) for i in df_indices_mb]\n        else:\n            ys = [list(map(int, str(self.df[\'token_id\'][i]).split())) for i in df_indices_mb]\n\n        ys_sub1 = []\n        if self.df_sub1 is not None:\n            ys_sub1 = [list(map(int, str(self.df_sub1[\'token_id\'][i]).split())) for i in df_indices_mb]\n        elif self.vocab_sub1 > 0 and not self.is_test:\n            ys_sub1 = [self.token2idx[1](self.df[\'text\'][i]) for i in df_indices_mb]\n\n        ys_sub2 = []\n        if self.df_sub2 is not None:\n            ys_sub2 = [list(map(int, str(self.df_sub2[\'token_id\'][i]).split())) for i in df_indices_mb]\n        elif self.vocab_sub2 > 0 and not self.is_test:\n            ys_sub2 = [self.token2idx[2](self.df[\'text\'][i]) for i in df_indices_mb]\n\n        mini_batch_dict = {\n            \'xs\': xs,\n            \'xlens\': [self.df[\'xlen\'][i] for i in df_indices_mb],\n            \'ys\': ys,\n            \'ys_sub1\': ys_sub1,\n            \'ys_sub2\': ys_sub2,\n            \'utt_ids\': [self.df[\'utt_id\'][i] for i in df_indices_mb],\n            \'speakers\': [self.df[\'speaker\'][i] for i in df_indices_mb],\n            \'sessions\': [self.df[\'session\'][i] for i in df_indices_mb],\n            \'text\': [self.df[\'text\'][i] for i in df_indices_mb],\n            \'feat_path\': [self.df[\'feat_path\'][i] for i in df_indices_mb],  # for plot\n        }\n        return mini_batch_dict\n\n    def set_batch_size(self, batch_size, min_xlen, min_ylen):\n        if not self.dynamic_batching:\n            return batch_size\n\n        if min_xlen <= 800:\n            pass\n        elif min_xlen <= 1600 or 80 < min_ylen <= 100:\n            batch_size //= 2\n        else:\n            batch_size //= 4\n\n        return max(1, batch_size)\n\n    def shuffle_bucketing(self, batch_size):\n        df_indices_buckets = []  # list of list\n        offset = 0\n        while True:\n            min_xlen = self.df[offset:offset + 1][\'xlen\'].values[0]\n            min_ylen = self.df[offset:offset + 1][\'ylen\'].values[0]\n            _batch_size = self.set_batch_size(batch_size, min_xlen, min_ylen)\n            df_indices_mb = list(self.df[offset:offset + _batch_size].index)\n            df_indices_buckets.append(df_indices_mb)\n            offset += len(df_indices_mb)\n            if offset + _batch_size >= len(self):\n                break\n\n        # shuffle buckets\n        random.shuffle(df_indices_buckets)\n        return df_indices_buckets\n\n    def discourse_bucketing(self, batch_size):\n        df_indices_buckets = []  # list of list\n        session_groups = [(k, v) for k, v in self.df.groupby(\'n_utt_in_session\').groups.items()]\n        if self.shuffle_bucket:\n            random.shuffle(session_groups)\n        for n_utt, ids in session_groups:\n            first_utt_ids = [i for i in ids if self.df[\'n_prev_utt\'][i] == 0]\n            for i in range(0, len(first_utt_ids), batch_size):\n                first_utt_ids_mb = first_utt_ids[i:i + batch_size]\n                for j in range(n_utt):\n                    df_indices_mb = [k + j for k in first_utt_ids_mb]\n                    df_indices_buckets.append(df_indices_mb)\n\n        return df_indices_buckets\n'"
neural_sp/datasets/lm.py,0,"b'#! /usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n# Copyright 2018 Kyoto University (Hirofumi Inaguma)\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n\n""""""Base class for loading dataset for language model.\n   In this class, all data will be loaded at each step.\n   You can use the multi-GPU version.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport logging\nimport numpy as np\nimport os\nimport pandas as pd\nimport random\n\nfrom neural_sp.datasets.asr import count_vocab_size\nfrom neural_sp.datasets.token_converter.character import Char2idx\nfrom neural_sp.datasets.token_converter.character import Idx2char\nfrom neural_sp.datasets.token_converter.phone import Idx2phone\nfrom neural_sp.datasets.token_converter.phone import Phone2idx\nfrom neural_sp.datasets.token_converter.word import Idx2word\nfrom neural_sp.datasets.token_converter.word import Word2idx\nfrom neural_sp.datasets.token_converter.wordpiece import Idx2wp\nfrom neural_sp.datasets.token_converter.wordpiece import Wp2idx\n\nrandom.seed(1)\nnp.random.seed(1)\n\nlogger = logging.getLogger(__name__)\n\n\nclass Dataset(object):\n\n    def __init__(self, tsv_path, dict_path,\n                 unit, batch_size, nlsyms=False, n_epochs=1e10,\n                 is_test=False, min_n_tokens=1,\n                 bptt=2, shuffle=False, backward=False, serialize=False,\n                 wp_model=None, corpus=\'\'):\n        """"""A class for loading dataset.\n\n        Args:\n            tsv_path (str): path to the dataset tsv file\n            dict_path (str): path to the dictionary\n            unit (str): word or wp or char or phone or word_char\n            batch_size (int): size of mini-batch\n            nlsyms (str): path to the non-linguistic symbols file\n            n_epochs (int): total epochs for training\n            is_test (bool):\n            min_n_tokens (int): exclude utterances shorter than this value\n            bptt (int): BPTT length\n            shuffle (bool): shuffle utterances per epoch.\n            backward (bool): flip all text in the corpus\n            serialize (bool): serialize text according to contexts in dialogue\n            wp_model (): path to the word-piece model for sentencepiece\n            corpus (str): name of corpus\n\n        """"""\n        super(Dataset, self).__init__()\n\n        self.epoch = 0\n        self.iteration = 0\n        self.offset = 0\n\n        self.set = os.path.basename(tsv_path).split(\'.\')[0]\n        self.is_test = is_test\n        self.unit = unit\n        self.batch_size = batch_size\n        self.bptt = bptt\n        self.sos = 2\n        self.eos = 2\n        self.max_epoch = n_epochs\n        self.shuffle = shuffle\n        self.backward = backward\n        self.vocab = count_vocab_size(dict_path)\n        assert bptt >= 2\n\n        self.idx2token = []\n        self.token2idx = []\n\n        # Set index converter\n        if unit in [\'word\', \'word_char\']:\n            self.idx2token += [Idx2word(dict_path)]\n            self.token2idx += [Word2idx(dict_path, word_char_mix=(unit == \'word_char\'))]\n        elif unit == \'wp\':\n            self.idx2token += [Idx2wp(dict_path, wp_model)]\n            self.token2idx += [Wp2idx(dict_path, wp_model)]\n        elif unit == \'char\':\n            self.idx2token += [Idx2char(dict_path)]\n            self.token2idx += [Char2idx(dict_path, nlsyms=nlsyms)]\n        elif \'phone\' in unit:\n            self.idx2token += [Idx2phone(dict_path)]\n            self.token2idx += [Phone2idx(dict_path)]\n        else:\n            raise ValueError(unit)\n\n        # Load dataset tsv file\n        self.df = pd.read_csv(tsv_path, encoding=\'utf-8\', delimiter=\'\\t\')\n        self.df = self.df.loc[:, [\'utt_id\', \'speaker\', \'feat_path\',\n                                  \'xlen\', \'xdim\', \'text\', \'token_id\', \'ylen\', \'ydim\']]\n\n        # Remove inappropriate utterances\n        if is_test:\n            print(\'Original utterance num: %d\' % len(self.df))\n            n_utts = len(self.df)\n            self.df = self.df[self.df.apply(lambda x: x[\'ylen\'] > 0, axis=1)]\n            print(\'Removed %d empty utterances\' % (n_utts - len(self.df)))\n        else:\n            print(\'Original utterance num: %d\' % len(self.df))\n            n_utts = len(self.df)\n            self.df = self.df[self.df.apply(lambda x: x[\'ylen\'] >= min_n_tokens, axis=1)]\n            print(\'Removed %d utterances (threshold)\' % (n_utts - len(self.df)))\n\n        # Sort tsv records\n        if shuffle:\n            assert not serialize\n            self.df = self.df.reindex(np.random.permutation(self.df.index))\n        elif serialize:\n            assert not shuffle\n            assert corpus == \'swbd\'\n            self.df[\'session\'] = self.df[\'speaker\'].apply(lambda x: str(x).split(\'-\')[0])\n            self.df[\'onset\'] = self.df[\'utt_id\'].apply(lambda x: int(x.split(\'_\')[-1].split(\'-\')[0]))\n            self.df = self.df.sort_values(by=[\'session\', \'onset\'], ascending=True)\n        else:\n            self.df = self.df.sort_values(by=\'utt_id\', ascending=True)\n\n        # Concatenate into a single sentence\n        self.concat_ids = self.concat_utterances(self.df)\n\n    def concat_utterances(self, df):\n        indices = list(df.index)\n        if self.backward:\n            indices = indices[::-1]\n        concat_ids = []\n        for i in indices:\n            assert df[\'token_id\'][i] != \'\'\n            concat_ids += [self.eos] + list(map(int, df[\'token_id\'][i].split()))\n        concat_ids += [self.eos]  # for the last sentence\n        # NOTE: <sos> and <eos> have the same index\n\n        # Reshape\n        n_utts = len(concat_ids)\n        concat_ids = concat_ids[:n_utts // self.batch_size * self.batch_size]\n        logger.info(\'Removed %d tokens / %d tokens\' % (n_utts - len(concat_ids), n_utts))\n        concat_ids = np.array(concat_ids).reshape((self.batch_size, -1))\n\n        return concat_ids\n\n    def __len__(self):\n        return len(self.concat_ids.reshape((-1,)))\n\n    @property\n    def epoch_detail(self):\n        """"""Percentage of the current epoch.""""""\n        return float(self.offset * self.batch_size) / len(self)\n\n    def reset(self):\n        """"""Reset data counter and offset.""""""\n        if self.shuffle:\n            self.df = self.df.reindex(np.random.permutation(self.df.index))\n            self.concat_ids = self.concat_utterances(self.df)\n        self.offset = 0\n\n    def next(self, batch_size=None, bptt=None):\n        """"""Generate each mini-batch.\n\n        Args:\n            batch_size (int): size of mini-batch\n            bptt (int): BPTT length\n        Returns:\n            ys (np.ndarray): target labels in the main task of size `[B, bptt]`\n            is_new_epoch (bool): flag for the end of the current epoch\n\n        """"""\n        if batch_size is None:\n            batch_size = self.batch_size\n        elif self.concat_ids.shape[0] != batch_size:\n            self.concat_ids = self.concat_ids.reshape((batch_size, -1))\n            # NOTE: only for the first iteration during evaluation\n\n        if bptt is None:\n            bptt = self.bptt\n\n        if self.epoch >= self.max_epoch:\n            raise StopIteration\n\n        ys = self.concat_ids[:, self.offset:self.offset + bptt]\n        self.offset += bptt - 1\n        # ys = self.concat_ids[:, self.offset:self.offset + (bptt + 1)]\n        # self.offset += (bptt + 1) - 1\n        # NOTE: the last token in ys must be feeded as inputs in the next mini-batch\n\n        is_new_epoch = False\n\n        # Last mini-batch\n        if (self.offset + 1) * batch_size >= len(self):\n            is_new_epoch = True\n            self.reset()\n            self.epoch += 1\n\n        return ys, is_new_epoch\n'"
neural_sp/evaluators/__init__.py,0,b''
neural_sp/evaluators/accuracy.py,0,"b'#! /usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n# Copyright 2020 Kyoto University (Hirofumi Inaguma)\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n\n""""""Evaluate a model by accuracy.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport logging\nfrom tqdm import tqdm\n\n\nlogger = logging.getLogger(__name__)\n\n\ndef eval_accuracy(models, dataset, batch_size=1, progressbar=False):\n    """"""Evaluate a Seq2seq by teacher-forcing accuracy.\n\n    Args:\n        models (list): models to evaluate\n        dataset (Dataset): evaluation dataset\n        batch_size (int): batch size\n        progressbar (bool): if True, visualize the progressbar\n    Returns:\n        accuracy (float): Average accuracy\n\n    """"""\n    # Reset data counter\n    dataset.reset()\n\n    total_acc = 0\n    n_tokens = 0\n    if progressbar:\n        pbar = tqdm(total=len(dataset))\n    while True:\n        batch, is_new_epoch = dataset.next(batch_size)\n        bs = len(batch[\'ys\'])\n        _, observation = models[0](batch, task=\'all\', is_eval=True)\n        n_tokens_b = sum([len(y) for y in batch[\'ys\']])\n        total_acc += observation[\'acc.att\'] * n_tokens_b\n        n_tokens += n_tokens_b\n\n        if progressbar:\n            pbar.update(bs)\n\n        if is_new_epoch:\n            break\n\n    if progressbar:\n        pbar.close()\n\n    # Reset data counters\n    dataset.reset()\n\n    accuracy = total_acc / n_tokens\n\n    logger.debug(\'Accuracy (%s): %.2f %%\' % (dataset.set, accuracy))\n\n    return accuracy\n'"
neural_sp/evaluators/character.py,0,"b'#! /usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n# Copyright 2018 Kyoto University (Hirofumi Inaguma)\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n\n""""""Evaluate the character-level model by WER & CER.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport logging\nfrom tqdm import tqdm\n\nfrom neural_sp.evaluators.edit_distance import compute_wer\nfrom neural_sp.utils import mkdir_join\n\nlogger = logging.getLogger(__name__)\n\n\ndef eval_char(models, dataset, recog_params, epoch,\n              recog_dir=None, streaming=False, progressbar=False, task_idx=0):\n    """"""Evaluate the character-level model by WER & CER.\n\n    Args:\n        models (list): models to evaluate\n        dataset (Dataset): evaluation dataset\n        recog_params (dict):\n        epoch (int):\n        recog_dir (str):\n        streaming (bool): streaming decoding for the session-level evaluation\n        progressbar (bool): visualize the progressbar\n        task_idx (int): the index of the target task in interest\n            0: main task\n            1: sub task\n            2: sub sub task\n    Returns:\n        wer (float): Word error rate\n        cer (float): Character error rate\n\n    """"""\n    # Reset data counter\n    dataset.reset()\n\n    if recog_dir is None:\n        recog_dir = \'decode_\' + dataset.set + \'_ep\' + str(epoch) + \'_beam\' + str(recog_params[\'recog_beam_width\'])\n        recog_dir += \'_lp\' + str(recog_params[\'recog_length_penalty\'])\n        recog_dir += \'_cp\' + str(recog_params[\'recog_coverage_penalty\'])\n        recog_dir += \'_\' + str(recog_params[\'recog_min_len_ratio\']) + \'_\' + str(recog_params[\'recog_max_len_ratio\'])\n        recog_dir += \'_lm\' + str(recog_params[\'recog_lm_weight\'])\n\n        ref_trn_save_path = mkdir_join(models[0].save_path, recog_dir, \'ref.trn\')\n        hyp_trn_save_path = mkdir_join(models[0].save_path, recog_dir, \'hyp.trn\')\n    else:\n        ref_trn_save_path = mkdir_join(recog_dir, \'ref.trn\')\n        hyp_trn_save_path = mkdir_join(recog_dir, \'hyp.trn\')\n\n    wer, cer = 0, 0\n    n_sub_w, n_ins_w, n_del_w = 0, 0, 0\n    n_sub_c, n_ins_c, n_del_c = 0, 0, 0\n    n_word, n_char = 0, 0\n    if progressbar:\n        pbar = tqdm(total=len(dataset))\n\n    if task_idx == 0:\n        task = \'ys\'\n    elif task_idx == 1:\n        task = \'ys_sub1\'\n    elif task_idx == 2:\n        task = \'ys_sub2\'\n    elif task_idx == 3:\n        task = \'ys_sub3\'\n\n    with open(hyp_trn_save_path, \'w\') as f_hyp, open(ref_trn_save_path, \'w\') as f_ref:\n        while True:\n            batch, is_new_epoch = dataset.next(recog_params[\'recog_batch_size\'])\n            if streaming or recog_params[\'recog_chunk_sync\']:\n                best_hyps_id, _ = models[0].decode_streaming(\n                    batch[\'xs\'], recog_params, dataset.idx2token[0],\n                    exclude_eos=True)\n            else:\n                best_hyps_id, _ = models[0].decode(\n                    batch[\'xs\'], recog_params,\n                    idx2token=dataset.idx2token[task_idx] if progressbar else None,\n                    exclude_eos=True,\n                    refs_id=batch[\'ys\'] if task_idx == 0 else batch[\'ys_sub\' + str(task_idx)],\n                    utt_ids=batch[\'utt_ids\'],\n                    speakers=batch[\'sessions\' if dataset.corpus == \'swbd\' else \'speakers\'],\n                    task=task,\n                    ensemble_models=models[1:] if len(models) > 1 else [])\n\n            for b in range(len(batch[\'xs\'])):\n                ref = batch[\'text\'][b]\n                hyp = dataset.idx2token[task_idx](best_hyps_id[b])\n\n                # Write to trn\n                speaker = str(batch[\'speakers\'][b]).replace(\'-\', \'_\')\n                if streaming:\n                    utt_id = str(batch[\'utt_ids\'][b]) + \'_0000000_0000001\'\n                else:\n                    utt_id = str(batch[\'utt_ids\'][b])\n                f_ref.write(ref + \' (\' + speaker + \'-\' + utt_id + \')\\n\')\n                f_hyp.write(hyp + \' (\' + speaker + \'-\' + utt_id + \')\\n\')\n                logger.debug(\'utt-id: %s\' % utt_id)\n                logger.debug(\'Ref: %s\' % ref)\n                logger.debug(\'Hyp: %s\' % hyp)\n                logger.debug(\'-\' * 150)\n\n                if not streaming:\n                    if (\'char\' in dataset.unit and \'nowb\' not in dataset.unit) or (task_idx > 0 and dataset.unit_sub1 == \'char\'):\n                        # Compute WER\n                        wer_b, sub_b, ins_b, del_b = compute_wer(ref=ref.split(\' \'),\n                                                                 hyp=hyp.split(\' \'),\n                                                                 normalize=False)\n                        wer += wer_b\n                        n_sub_w += sub_b\n                        n_ins_w += ins_b\n                        n_del_w += del_b\n                        n_word += len(ref.split(\' \'))\n\n                    # Compute CER\n                    if dataset.corpus == \'csj\':\n                        ref = ref.replace(\' \', \'\')\n                        hyp = hyp.replace(\' \', \'\')\n                    cer_b, sub_b, ins_b, del_b = compute_wer(ref=list(ref),\n                                                             hyp=list(hyp),\n                                                             normalize=False)\n                    cer += cer_b\n                    n_sub_c += sub_b\n                    n_ins_c += ins_b\n                    n_del_c += del_b\n                    n_char += len(ref)\n\n                if progressbar:\n                    pbar.update(1)\n\n            if is_new_epoch:\n                break\n\n    if progressbar:\n        pbar.close()\n\n    # Reset data counters\n    dataset.reset()\n\n    if not streaming:\n        if (\'char\' in dataset.unit and \'nowb\' not in dataset.unit) or (task_idx > 0 and dataset.unit_sub1 == \'char\'):\n            wer /= n_word\n            n_sub_w /= n_word\n            n_ins_w /= n_word\n            n_del_w /= n_word\n        else:\n            wer = n_sub_w = n_ins_w = n_del_w = 0\n\n        cer /= n_char\n        n_sub_c /= n_char\n        n_ins_c /= n_char\n        n_del_c /= n_char\n\n    logger.debug(\'WER (%s): %.2f %%\' % (dataset.set, wer))\n    logger.debug(\'SUB: %.2f / INS: %.2f / DEL: %.2f\' % (n_sub_w, n_ins_w, n_del_w))\n    logger.debug(\'CER (%s): %.2f %%\' % (dataset.set, cer))\n    logger.debug(\'SUB: %.2f / INS: %.2f / DEL: %.2f\' % (n_sub_c, n_ins_c, n_del_c))\n\n    return wer, cer\n'"
neural_sp/evaluators/edit_distance.py,0,"b'#! /usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n# Copyright 2018 Kyoto University (Hirofumi Inaguma)\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n\n""""""Functions for computing edit distance.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\n# import Levenshtein as lev  # TODO(hirofumi): install\nimport numpy as np\n\n\ndef compute_per(ref, hyp, normalize=False):\n    """"""Compute Phone Error Rate.\n\n    Args:\n        ref (list): phones in the reference transcript\n        hyp (list): phones in the predicted transcript\n        normalize (bool, optional): if True, divide by the length of ref\n    Returns:\n        per (float): Phone Error Rate between ref and hyp\n\n    """"""\n    # Build mapping of phone to index\n    phone_set = set(ref + hyp)\n    phone2char = dict(zip(phone_set, range(len(phone_set))))\n\n    # Map phones to a single char array\n    # NOTE: Levenshtein packages only accepts strings\n    phones_ref = [chr(phone2char[p]) for p in ref]\n    phones_hyp = [chr(phone2char[p]) for p in hyp]\n\n    per = lev.distance(\'\'.join(phones_ref), \'\'.join(phones_hyp))\n    if normalize:\n        per /= len(ref)\n    return per * 100\n\n\ndef compute_cer(ref, hyp, normalize=False):\n    """"""Compute Character Error Rate.\n\n    Args:\n        ref (str): a sentence without spaces\n        hyp (str): a sentence without spaces\n        normalize (bool, optional): if True, divide by the length of ref\n    Returns:\n        cer (float): Character Error Rate between ref and hyp\n\n    """"""\n    cer = lev.distance(hyp, ref)\n    if normalize:\n        cer /= len(list(ref))\n    return cer * 100\n\n\ndef compute_wer(ref, hyp, normalize=False):\n    """"""Compute Word Error Rate.\n\n        [Reference]\n            https://martin-thoma.com/word-error-rate-calculation/\n    Args:\n        ref (list): words in the reference transcript\n        hyp (list): words in the predicted transcript\n        normalize (bool, optional): if True, divide by the length of ref\n    Returns:\n        wer (float): Word Error Rate between ref and hyp\n        n_sub (int): the number of substitution\n        n_ins (int): the number of insertion\n        n_del (int): the number of deletion\n\n    """"""\n    # Initialisation\n    d = np.zeros((len(ref) + 1) * (len(hyp) + 1), dtype=np.uint16)\n    d = d.reshape((len(ref) + 1, len(hyp) + 1))\n    for i in range(len(ref) + 1):\n        for j in range(len(hyp) + 1):\n            if i == 0:\n                d[0][j] = j\n            elif j == 0:\n                d[i][0] = i\n\n    # Computation\n    for i in range(1, len(ref) + 1):\n        for j in range(1, len(hyp) + 1):\n            if ref[i - 1] == hyp[j - 1]:\n                d[i][j] = d[i - 1][j - 1]\n            else:\n                sub_tmp = d[i - 1][j - 1] + 1\n                ins_tmp = d[i][j - 1] + 1\n                del_tmp = d[i - 1][j] + 1\n                d[i][j] = min(sub_tmp, ins_tmp, del_tmp)\n\n    wer = d[len(ref)][len(hyp)]\n\n    # Find out the manipulation steps\n    x = len(ref)\n    y = len(hyp)\n    error_list = []\n    while True:\n        if x == 0 and y == 0:\n            break\n        else:\n            if x > 0 and y > 0:\n                if d[x][y] == d[x - 1][y - 1] and ref[x - 1] == hyp[y - 1]:\n                    error_list.append(""C"")\n                    x = x - 1\n                    y = y - 1\n                elif d[x][y] == d[x][y - 1] + 1:\n                    error_list.append(""I"")\n                    y = y - 1\n                elif d[x][y] == d[x - 1][y - 1] + 1:\n                    error_list.append(""S"")\n                    x = x - 1\n                    y = y - 1\n                else:\n                    error_list.append(""D"")\n                    x = x - 1\n            elif x == 0 and y > 0:\n                if d[x][y] == d[x][y - 1] + 1:\n                    error_list.append(""I"")\n                    y = y - 1\n                else:\n                    error_list.append(""D"")\n                    x = x - 1\n            elif y == 0 and x > 0:\n                error_list.append(""D"")\n                x = x - 1\n            else:\n                raise ValueError\n\n    n_sub = error_list.count(""S"")\n    n_ins = error_list.count(""I"")\n    n_del = error_list.count(""D"")\n    n_cor = error_list.count(""C"")\n\n    assert wer == (n_sub + n_ins + n_del)\n    assert n_cor == (len(ref) - n_sub - n_del)\n\n    if normalize:\n        wer /= len(ref)\n\n    return wer * 100, n_sub * 100, n_ins * 100, n_del * 100\n\n\ndef wer_align(ref, hyp, normalize=False, double_byte=False):\n    """"""Compute Word Error Rate.\n\n        [Reference]\n            https://github.com/zszyellow/WER-in-python\n    Args:\n        ref (list): words in the reference transcript\n        hyp (list): words in the predicted transcript\n        normalize (bool, optional): if True, divide by the length of ref\n        double_byte (bool):\n    Returns:\n        wer (float): Word Error Rate between ref and hyp\n        n_sub (int): the number of substitution error\n        n_ins (int): the number of insertion error\n        n_del (int): the number of deletion error\n\n    """"""\n    space_char = ""\xe3\x80\x80"" if double_byte else "" ""\n    s_char = ""\xef\xbc\xb3"" if double_byte else ""S""\n    i_char = ""\xef\xbc\xa9"" if double_byte else ""I""\n    d_char = ""\xef\xbc\xa4"" if double_byte else ""D""\n\n    # Build the matrix\n    d = np.zeros((len(ref) + 1) * (len(hyp) + 1),\n                 dtype=np.uint8).reshape((len(ref) + 1, len(hyp) + 1))\n    for i in range(len(ref) + 1):\n        for j in range(len(hyp) + 1):\n            if i == 0:\n                d[0][j] = j\n            elif j == 0:\n                d[i][0] = i\n    for i in range(1, len(ref) + 1):\n        for j in range(1, len(hyp) + 1):\n            if ref[i - 1] == hyp[j - 1]:\n                d[i][j] = d[i - 1][j - 1]\n            else:\n                n_sub = d[i - 1][j - 1] + 1\n                n_ins = d[i][j - 1] + 1\n                n_del = d[i - 1][j] + 1\n                d[i][j] = min(n_sub, n_ins, n_del)\n    wer = float(d[len(ref)][len(hyp)])\n\n    # Find out the manipulation steps\n    x = len(ref)\n    y = len(hyp)\n    error_list = []\n    while True:\n        if x == 0 and y == 0:\n            break\n        else:\n            if x > 0 and y > 0:\n                if d[x][y] == d[x - 1][y - 1] and ref[x - 1] == hyp[y - 1]:\n                    error_list.append(""C"")\n                    x = x - 1\n                    y = y - 1\n                elif d[x][y] == d[x][y - 1] + 1:\n                    error_list.append(""I"")\n                    y = y - 1\n                elif d[x][y] == d[x - 1][y - 1] + 1:\n                    error_list.append(""S"")\n                    x = x - 1\n                    y = y - 1\n                else:\n                    error_list.append(""D"")\n                    x = x - 1\n            elif x == 0 and y > 0:\n                if d[x][y] == d[x][y - 1] + 1:\n                    error_list.append(""I"")\n                    y = y - 1\n                else:\n                    error_list.append(""D"")\n                    x = x - 1\n            elif y == 0 and x > 0:\n                error_list.append(""D"")\n                x = x - 1\n            else:\n                raise ValueError\n    error_list = error_list[::-1]\n\n    # Print the result in aligned way\n    print(""REF: "", end=\'\')\n    for i in range(len(error_list)):\n        if error_list[i] == ""I"":\n            count = 0\n            for j in range(i):\n                if error_list[j] == ""D"":\n                    count += 1\n            index = i - count\n            print(space_char * (len(hyp[index])), end=\' \')\n        elif error_list[i] == ""S"":\n            count1 = 0\n            for j in range(i):\n                if error_list[j] == ""I"":\n                    count1 += 1\n            index1 = i - count1\n            count2 = 0\n            for j in range(i):\n                if error_list[j] == ""D"":\n                    count2 += 1\n            index2 = i - count2\n            if len(ref[index1]) < len(hyp[index2]):\n                print(ref[index1] + space_char * (len(hyp[index2]) - len(ref[index1])), end=\' \')\n            else:\n                print(ref[index1], end=\' \')\n        else:\n            count = 0\n            for j in range(i):\n                if error_list[j] == ""I"":\n                    count += 1\n            index = i - count\n            print(ref[index], end=\' \')\n\n    print(""\\nHYP: "", end=\'\')\n    for i in range(len(error_list)):\n        if error_list[i] == ""D"":\n            count = 0\n            for j in range(i):\n                if error_list[j] == ""I"":\n                    count += 1\n            index = i - count\n            print(space_char * (len(ref[index])), end=\' \')\n        elif error_list[i] == ""S"":\n            count1 = 0\n            for j in range(i):\n                if error_list[j] == ""I"":\n                    count1 += 1\n            index1 = i - count1\n            count2 = 0\n            for j in range(i):\n                if error_list[j] == ""D"":\n                    count2 += 1\n            index2 = i - count2\n            if len(ref[index1]) > len(hyp[index2]):\n                print(hyp[index2] + space_char * (len(ref[index1]) - len(hyp[index2])), end=\' \')\n            else:\n                print(hyp[index2], end=\' \')\n        else:\n            count = 0\n            for j in range(i):\n                if error_list[j] == ""D"":\n                    count += 1\n            index = i - count\n            print(hyp[index], end=\' \')\n\n    print(""\\nEVA: "", end=\'\')\n    for i in range(len(error_list)):\n        if error_list[i] == ""D"":\n            count = 0\n            for j in range(i):\n                if error_list[j] == ""I"":\n                    count += 1\n            index = i - count\n            print(d_char + space_char * (len(ref[index]) - 1), end=\' \')\n        elif error_list[i] == ""I"":\n            count = 0\n            for j in range(i):\n                if error_list[j] == ""D"":\n                    count += 1\n            index = i - count\n            print(i_char + space_char * (len(hyp[index]) - 1), end=\' \')\n        elif error_list[i] == ""S"":\n            count1 = 0\n            for j in range(i):\n                if error_list[j] == ""I"":\n                    count1 += 1\n            index1 = i - count1\n            count2 = 0\n            for j in range(i):\n                if error_list[j] == ""D"":\n                    count2 += 1\n            index2 = i - count2\n            if len(ref[index1]) > len(hyp[index2]):\n                print(s_char + space_char * (len(ref[index1]) - 1), end=\' \')\n            else:\n                print(s_char + space_char * (len(hyp[index2]) - 1), end=\' \')\n        else:\n            count = 0\n            for j in range(i):\n                if error_list[j] == ""I"":\n                    count += 1\n            index = i - count\n            print(space_char * (len(ref[index])), end=\' \')\n\n    n_sub = error_list.count(""S"")\n    n_ins = error_list.count(""I"")\n    n_del = error_list.count(""D"")\n    n_cor = error_list.count(""C"")\n\n    assert wer == (n_sub + n_ins + n_del)\n    assert n_cor == (len(ref) - n_sub - n_del)\n\n    if normalize:\n        wer /= len(ref)\n\n    return wer * 100, n_sub * 100, n_ins * 100, n_del * 100\n'"
neural_sp/evaluators/phone.py,0,"b'#! /usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n# Copyright 2018 Kyoto University (Hirofumi Inaguma)\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n\n""""""Evaluate a phene-level model by PER.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport logging\nfrom tqdm import tqdm\n\nfrom neural_sp.evaluators.edit_distance import compute_wer\nfrom neural_sp.utils import mkdir_join\n\nlogger = logging.getLogger(__name__)\n\n\ndef eval_phone(models, dataset, recog_params, epoch,\n               recog_dir=None, streaming=False, progressbar=False):\n    """"""Evaluate a phone-level model by PER.\n\n    Args:\n        models (list): models to evaluate\n        dataset (Dataset): evaluation dataset\n        recog_params (dict):\n        epoch (int):\n        recog_dir (str):\n        streaming (bool): streaming decoding for the session-level evaluation\n        progressbar (bool): visualize the progressbar\n    Returns:\n        per (float): Phone error rate\n\n    """"""\n    # Reset data counter\n    dataset.reset()\n\n    if recog_dir is None:\n        recog_dir = \'decode_\' + dataset.set + \'_ep\' + str(epoch) + \'_beam\' + str(recog_params[\'recog_beam_width\'])\n        recog_dir += \'_lp\' + str(recog_params[\'recog_length_penalty\'])\n        recog_dir += \'_cp\' + str(recog_params[\'recog_coverage_penalty\'])\n        recog_dir += \'_\' + str(recog_params[\'recog_min_len_ratio\']) + \'_\' + str(recog_params[\'recog_max_len_ratio\'])\n\n        ref_trn_save_path = mkdir_join(models[0].save_path, recog_dir, \'ref.trn\')\n        hyp_trn_save_path = mkdir_join(models[0].save_path, recog_dir, \'hyp.trn\')\n    else:\n        ref_trn_save_path = mkdir_join(recog_dir, \'ref.trn\')\n        hyp_trn_save_path = mkdir_join(recog_dir, \'hyp.trn\')\n\n    per = 0\n    n_sub, n_ins, n_del = 0, 0, 0\n    n_phone = 0\n    if progressbar:\n        pbar = tqdm(total=len(dataset))\n\n    with open(hyp_trn_save_path, \'w\') as f_hyp, open(ref_trn_save_path, \'w\') as f_ref:\n        while True:\n            batch, is_new_epoch = dataset.next(recog_params[\'recog_batch_size\'])\n            if streaming or recog_params[\'recog_chunk_sync\']:\n                best_hyps_id, _ = models[0].decode_streaming(\n                    batch[\'xs\'], recog_params, dataset.idx2token[0],\n                    exclude_eos=True)\n            else:\n                best_hyps_id, _ = models[0].decode(\n                    batch[\'xs\'], recog_params,\n                    idx2token=dataset.idx2token[0] if progressbar else None,\n                    exclude_eos=True,\n                    refs_id=batch[\'ys\'],\n                    utt_ids=batch[\'utt_ids\'],\n                    speakers=batch[\'sessions\' if dataset.corpus == \'swbd\' else \'speakers\'],\n                    ensemble_models=models[1:] if len(models) > 1 else [])\n\n            for b in range(len(batch[\'xs\'])):\n                ref = batch[\'text\'][b]\n                hyp = dataset.idx2token[0](best_hyps_id[b])\n\n                # Write to trn\n                speaker = str(batch[\'speakers\'][b]).replace(\'-\', \'_\')\n                if streaming:\n                    utt_id = str(batch[\'utt_ids\'][b]) + \'_0000000_0000001\'\n                else:\n                    utt_id = str(batch[\'utt_ids\'][b])\n                f_ref.write(ref + \' (\' + speaker + \'-\' + utt_id + \')\\n\')\n                f_hyp.write(hyp + \' (\' + speaker + \'-\' + utt_id + \')\\n\')\n                logger.debug(\'utt-id: %s\' % utt_id)\n                logger.debug(\'Ref: %s\' % ref)\n                logger.debug(\'Hyp: %s\' % hyp)\n                logger.debug(\'-\' * 150)\n\n                if not streaming:\n                    # Compute PER\n                    per_b, sub_b, ins_b, del_b = compute_wer(ref=ref.split(\' \'),\n                                                             hyp=hyp.split(\' \'),\n                                                             normalize=False)\n                    per += per_b\n                    n_sub += sub_b\n                    n_ins += ins_b\n                    n_del += del_b\n                    n_phone += len(ref.split(\' \'))\n\n                if progressbar:\n                    pbar.update(1)\n\n            if is_new_epoch:\n                break\n\n    if progressbar:\n        pbar.close()\n\n    # Reset data counters\n    dataset.reset()\n\n    if not streaming:\n        per /= n_phone\n        n_sub /= n_phone\n        n_ins /= n_phone\n        n_del /= n_phone\n\n    logger.debug(\'PER (%s): %.2f %%\' % (dataset.set, per))\n    logger.debug(\'SUB: %.2f / INS: %.2f / DEL: %.2f\' % (n_sub, n_ins, n_del))\n\n    return per\n'"
neural_sp/evaluators/ppl.py,0,"b'#! /usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n# Copyright 2018 Kyoto University (Hirofumi Inaguma)\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n\n""""""Evaluate a RNNLM by perplexity.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport logging\nimport numpy as np\nfrom tqdm import tqdm\n\nfrom neural_sp.models.lm.gated_convlm import GatedConvLM\nfrom neural_sp.models.lm.rnnlm import RNNLM\nfrom neural_sp.models.lm.transformerlm import TransformerLM\n\nlogger = logging.getLogger(__name__)\n\n\ndef eval_ppl(models, dataset, batch_size=1, bptt=None,\n             n_caches=0, progressbar=False):\n    """"""Evaluate a Seq2seq or (RNN/GatedConv)LM by perprexity and loss.\n\n    Args:\n        models (list): models to evaluate\n        dataset (Dataset): evaluation dataset\n        batch_size (int): batch size\n        bptt (int): BPTT length\n        n_caches (int):\n        progressbar (bool): if True, visualize the progressbar\n    Returns:\n        ppl (float): Average perplexity\n        loss (float): Average loss\n\n    """"""\n    # Reset data counter\n    dataset.reset()\n\n    is_lm = False\n    if isinstance(models[0], RNNLM) or isinstance(models[0], GatedConvLM) or isinstance(models[0], TransformerLM):\n        is_lm = True\n\n    total_loss = 0\n    n_tokens = 0\n    hidden = None  # for RNNLM\n    if progressbar:\n        pbar = tqdm(total=len(dataset))\n    while True:\n        if is_lm:\n            ys, is_new_epoch = dataset.next(batch_size, bptt)\n            bs, time = ys.shape[:2]\n            if n_caches > 0:\n                assert isinstance(models[0], RNNLM)\n                # NOTE: cache is not supported for GatedConvLM/TransformerLM now\n                for t in range(time - 1):\n                    loss, hidden = models[0](ys[:, t:t + 2], hidden, is_eval=True, n_caches=n_caches)[:2]\n                    total_loss += loss.item() * bs\n                    n_tokens += bs\n\n                    if progressbar:\n                        pbar.update(bs)\n            else:\n                loss, hidden = models[0](ys, hidden, is_eval=True)[:2]\n                total_loss += loss.item() * bs * (time - 1)\n                n_tokens += bs * (time - 1)\n\n                if progressbar:\n                    pbar.update(bs * (time - 1))\n        else:\n            batch, is_new_epoch = dataset.next(batch_size)\n            bs = len(batch[\'ys\'])\n            loss, _ = models[0](batch, task=\'all\', is_eval=True)\n            total_loss += loss.item() * bs\n            n_tokens += sum([len(y) for y in batch[\'ys\']])\n            # NOTE: loss is divided by batch size in the ASR model\n\n            if progressbar:\n                pbar.update(bs)\n\n        if is_new_epoch:\n            break\n\n    if progressbar:\n        pbar.close()\n\n    # Reset data counters\n    dataset.reset()\n\n    avg_loss = total_loss / n_tokens\n    ppl = np.exp(avg_loss)\n\n    logger.debug(\'PPL (%s): %.2f %%\' % (dataset.set, ppl))\n    logger.debug(\'Loss (%s): %.2f %%\' % (dataset.set, avg_loss))\n\n    return ppl, avg_loss\n'"
neural_sp/evaluators/resolving_unk.py,0,"b'#! /usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n# Copyright 2018 Kyoto University (Hirofumi Inaguma)\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n\n""""""Resolve UNK tokens words from the character-based model.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\n\n\ndef resolve_unk(hyp_word, best_hyps_char, aw_word, aw_char, idx2char,\n                subsample_factor_word, subsample_factor_char):\n    """"""Revolving UNK.\n\n    Args:\n        hyp_word ():\n        best_hyps_char ():\n        aw_word ():\n        aw_char ():\n        idx2char ():\n        subsample_factor_word (int):\n        subsample_factor_char (int):\n    Returns:\n        hyp_no_unk (str):\n\n    """"""\n    oov_info = []\n    # [[id_oov_0, t_sub_0], ...]\n\n    diff_time_resolution = subsample_factor_word // subsample_factor_char\n\n    if diff_time_resolution > 1:\n        assert diff_time_resolution == 2\n        aw_char1 = aw_char[:, ::diff_time_resolution]\n        aw_char1 = aw_char1[:, :aw_word.shape[1]]\n        aw_char2 = aw_char[:, 1::diff_time_resolution]\n        aw_char2 = aw_char2[:, :aw_word.shape[1]]\n        aw_char = (aw_char1 + aw_char2) / 2\n\n    # Store places for <unk>\n    for offset, w in enumerate(hyp_word.split(\' \')):\n        if w == \'<unk>\':\n            oov_info.append([offset, -1])\n\n    # Point to characters\n    for i in range(len(oov_info)):\n        max_attn_overlap = 0\n        for t_char in range(len(aw_char)):\n            # print(np.sum(aw_word[oov_info[i][0]] * aw_char[t_char]))\n            if np.sum(aw_word[oov_info[i][0]] * aw_char[t_char]) > max_attn_overlap:\n                # Check if the correcsponding character is space\n                max_char = idx2char(best_hyps_char[t_char: t_char + 1])\n                if max_char == \' \':\n                    continue\n\n                max_attn_overlap = np.sum(\n                    aw_word[oov_info[i][0]] * aw_char[t_char])\n                oov_info[i][1] = t_char\n\n    hyp_no_unk = \'\'\n    n_oovs = 0\n    for offset, w in enumerate(hyp_word.split(\' \')):\n        if w == \'<unk>\':\n            t_char = oov_info[n_oovs][1]\n            covered_word = idx2char(best_hyps_char[t_char: t_char + 1])\n\n            # Search until space (forward pass)\n            fwd = 1\n            while True:\n                if t_char - fwd < 0:\n                    break\n                elif idx2char(best_hyps_char[t_char - fwd: t_char - fwd + 1]) not in [\' \', \'>\']:\n                    covered_word = idx2char(best_hyps_char[t_char - fwd: t_char - fwd + 1]) + covered_word\n                    fwd += 1\n                else:\n                    break\n\n            # Search until space (backward pass)\n            bwd = 1\n            while True:\n                if t_char + bwd > len(best_hyps_char) - 1:\n                    break\n                elif idx2char(best_hyps_char[t_char + bwd: t_char + bwd + 1]) not in [\' \', \'>\']:\n                    covered_word += idx2char(best_hyps_char[t_char + bwd: t_char + bwd + 1])\n                    bwd += 1\n                else:\n                    break\n\n            if offset == 0:\n                # First word in a sentence\n                hyp_no_unk += \'***\' + covered_word + \'***\'\n            else:\n                hyp_no_unk += \' ***\' + covered_word + \'***\'\n            n_oovs += 1\n        else:\n            hyp_no_unk += \' \' + w\n\n    if hyp_no_unk[0] == \' \':\n        hyp_no_unk = hyp_no_unk[1:]\n\n    return hyp_no_unk\n'"
neural_sp/evaluators/word.py,0,"b'#! /usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n# Copyright 2018 Kyoto University (Hirofumi Inaguma)\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n\n""""""Evaluate the word-level model by WER.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport copy\nimport logging\nimport numpy as np\nfrom tqdm import tqdm\n\nfrom neural_sp.evaluators.edit_distance import compute_wer\nfrom neural_sp.evaluators.resolving_unk import resolve_unk\nfrom neural_sp.utils import mkdir_join\n\nlogger = logging.getLogger(__name__)\n\n\ndef eval_word(models, dataset, recog_params, epoch,\n              recog_dir=None, streaming=False, progressbar=False):\n    """"""Evaluate the word-level model by WER.\n\n    Args:\n        models (list): models to evaluate\n        dataset (Dataset): evaluation dataset\n        recog_params (dict):\n        epoch (int):\n        recog_dir (str):\n        streaming (bool): streaming decoding for the session-level evaluation\n        progressbar (bool): visualize the progressbar\n    Returns:\n        wer (float): Word error rate\n        cer (float): Character error rate\n        n_oov_total (int): totol number of OOV\n\n    """"""\n    # Reset data counter\n    dataset.reset()\n\n    if recog_dir is None:\n        recog_dir = \'decode_\' + dataset.set + \'_ep\' + str(epoch) + \'_beam\' + str(recog_params[\'recog_beam_width\'])\n        recog_dir += \'_lp\' + str(recog_params[\'recog_length_penalty\'])\n        recog_dir += \'_cp\' + str(recog_params[\'recog_coverage_penalty\'])\n        recog_dir += \'_\' + str(recog_params[\'recog_min_len_ratio\']) + \'_\' + str(recog_params[\'recog_max_len_ratio\'])\n        recog_dir += \'_lm\' + str(recog_params[\'recog_lm_weight\'])\n\n        ref_trn_save_path = mkdir_join(models[0].save_path, recog_dir, \'ref.trn\')\n        hyp_trn_save_path = mkdir_join(models[0].save_path, recog_dir, \'hyp.trn\')\n    else:\n        ref_trn_save_path = mkdir_join(recog_dir, \'ref.trn\')\n        hyp_trn_save_path = mkdir_join(recog_dir, \'hyp.trn\')\n\n    wer, cer = 0, 0\n    n_sub_w, n_ins_w, n_del_w = 0, 0, 0\n    n_sub_c, n_ins_c, n_del_c = 0, 0, 0\n    n_word, n_char = 0, 0\n    n_oov_total = 0\n    if progressbar:\n        pbar = tqdm(total=len(dataset))\n\n    with open(hyp_trn_save_path, \'w\') as f_hyp, open(ref_trn_save_path, \'w\') as f_ref:\n        while True:\n            batch, is_new_epoch = dataset.next(recog_params[\'recog_batch_size\'])\n            if streaming or recog_params[\'recog_chunk_sync\']:\n                best_hyps_id, _ = models[0].decode_streaming(\n                    batch[\'xs\'], recog_params, dataset.idx2token[0],\n                    exclude_eos=True)\n            else:\n                best_hyps_id, aws = models[0].decode(\n                    batch[\'xs\'], recog_params,\n                    idx2token=dataset.idx2token[0] if progressbar else None,\n                    exclude_eos=True,\n                    refs_id=batch[\'ys\'],\n                    utt_ids=batch[\'utt_ids\'],\n                    speakers=batch[\'sessions\' if dataset.corpus == \'swbd\' else \'speakers\'],\n                    ensemble_models=models[1:] if len(models) > 1 else [])\n\n            for b in range(len(batch[\'xs\'])):\n                ref = batch[\'text\'][b]\n                hyp = dataset.idx2token[0](best_hyps_id[b])\n\n                n_oov_total += hyp.count(\'<unk>\')\n\n                # Resolving UNK\n                if recog_params[\'recog_resolving_unk\'] and \'<unk>\' in hyp:\n                    recog_params_char = copy.deepcopy(recog_params)\n                    recog_params_char[\'recog_lm_weight\'] = 0\n                    recog_params_char[\'recog_beam_width\'] = 1\n                    best_hyps_id_char, aw_char = models[0].decode(\n                        batch[\'xs\'][b:b + 1], recog_params_char,\n                        idx2token=dataset.idx2token[1] if progressbar else None,\n                        exclude_eos=True,\n                        refs_id=batch[\'ys_sub1\'],\n                        utt_ids=batch[\'utt_ids\'],\n                        speakers=batch[\'sessions\'] if dataset.corpus == \'swbd\' else batch[\'speakers\'],\n                        task=\'ys_sub1\')\n                    # TODO(hirofumi): support ys_sub2 and ys_sub3\n\n                    assert not streaming\n\n                    hyp = resolve_unk(\n                        hyp, best_hyps_id_char[0], aws[b], aw_char[0], dataset.idx2token[1],\n                        subsample_factor_word=np.prod(models[0].subsample),\n                        subsample_factor_char=np.prod(models[0].subsample[:models[0].enc_n_layers_sub1 - 1]))\n                    logger.debug(\'Hyp (after OOV resolution): %s\' % hyp)\n                    hyp = hyp.replace(\'*\', \'\')\n\n                    # Compute CER\n                    ref_char = ref\n                    hyp_char = hyp\n                    if dataset.corpus == \'csj\':\n                        ref_char = ref.replace(\' \', \'\')\n                        hyp_char = hyp.replace(\' \', \'\')\n                    cer_b, sub_b, ins_b, del_b = compute_wer(ref=list(ref_char),\n                                                             hyp=list(hyp_char),\n                                                             normalize=False)\n                    cer += cer_b\n                    n_sub_c += sub_b\n                    n_ins_c += ins_b\n                    n_del_c += del_b\n                    n_char += len(ref_char)\n\n                # Write to trn\n                speaker = str(batch[\'speakers\'][b]).replace(\'-\', \'_\')\n                if streaming:\n                    utt_id = str(batch[\'utt_ids\'][b]) + \'_0000000_0000001\'\n                else:\n                    utt_id = str(batch[\'utt_ids\'][b])\n                f_ref.write(ref + \' (\' + speaker + \'-\' + utt_id + \')\\n\')\n                f_hyp.write(hyp + \' (\' + speaker + \'-\' + utt_id + \')\\n\')\n                logger.debug(\'utt-id: %s\' % utt_id)\n                logger.debug(\'Ref: %s\' % ref)\n                logger.debug(\'Hyp: %s\' % hyp)\n                logger.debug(\'-\' * 150)\n\n                if not streaming:\n                    # Compute WER\n                    wer_b, sub_b, ins_b, del_b = compute_wer(ref=ref.split(\' \'),\n                                                             hyp=hyp.split(\' \'),\n                                                             normalize=False)\n                    wer += wer_b\n                    n_sub_w += sub_b\n                    n_ins_w += ins_b\n                    n_del_w += del_b\n                    n_word += len(ref.split(\' \'))\n\n                if progressbar:\n                    pbar.update(1)\n\n            if is_new_epoch:\n                break\n\n    if progressbar:\n        pbar.close()\n\n    # Reset data counters\n    dataset.reset()\n\n    if not streaming:\n        wer /= n_word\n        n_sub_w /= n_word\n        n_ins_w /= n_word\n        n_del_w /= n_word\n\n        if n_char > 0:\n            cer /= n_char\n            n_sub_c /= n_char\n            n_ins_c /= n_char\n            n_del_c /= n_char\n\n    logger.debug(\'WER (%s): %.2f %%\' % (dataset.set, wer))\n    logger.debug(\'SUB: %.2f / INS: %.2f / DEL: %.2f\' % (n_sub_w, n_ins_w, n_del_w))\n    logger.debug(\'CER (%s): %.2f %%\' % (dataset.set, cer))\n    logger.debug(\'SUB: %.2f / INS: %.2f / DEL: %.2f\' % (n_sub_c, n_ins_c, n_del_c))\n    logger.debug(\'OOV (total): %d\' % (n_oov_total))\n\n    return wer, cer, n_oov_total\n'"
neural_sp/evaluators/wordpiece.py,0,"b'#! /usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n# Copyright 2018 Kyoto University (Hirofumi Inaguma)\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n\n""""""Evaluate the wordpiece-level model by WER.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport logging\nfrom tqdm import tqdm\n\nfrom neural_sp.evaluators.edit_distance import compute_wer\nfrom neural_sp.utils import mkdir_join\n\nlogger = logging.getLogger(__name__)\n\n\ndef eval_wordpiece(models, dataset, recog_params, epoch,\n                   recog_dir=None, streaming=False, progressbar=False,\n                   fine_grained=False):\n    """"""Evaluate the wordpiece-level model by WER.\n\n    Args:\n        models (list): models to evaluate\n        dataset (Dataset): evaluation dataset\n        recog_params (dict):\n        epoch (int):\n        recog_dir (str):\n        streaming (bool): streaming decoding for the session-level evaluation\n        progressbar (bool): visualize the progressbar\n        fine_grained (bool): calculate fine-grained WER distributions based on input lengths\n    Returns:\n        wer (float): Word error rate\n        cer (float): Character error rate\n\n    """"""\n    # Reset data counter\n    dataset.reset()\n\n    if recog_dir is None:\n        recog_dir = \'decode_\' + dataset.set + \'_ep\' + str(epoch) + \'_beam\' + str(recog_params[\'recog_beam_width\'])\n        recog_dir += \'_lp\' + str(recog_params[\'recog_length_penalty\'])\n        recog_dir += \'_cp\' + str(recog_params[\'recog_coverage_penalty\'])\n        recog_dir += \'_\' + str(recog_params[\'recog_min_len_ratio\']) + \'_\' + str(recog_params[\'recog_max_len_ratio\'])\n        recog_dir += \'_lm\' + str(recog_params[\'recog_lm_weight\'])\n\n        ref_trn_save_path = mkdir_join(models[0].save_path, recog_dir, \'ref.trn\')\n        hyp_trn_save_path = mkdir_join(models[0].save_path, recog_dir, \'hyp.trn\')\n    else:\n        ref_trn_save_path = mkdir_join(recog_dir, \'ref.trn\')\n        hyp_trn_save_path = mkdir_join(recog_dir, \'hyp.trn\')\n\n    wer, cer = 0, 0\n    n_sub_w, n_ins_w, n_del_w = 0, 0, 0\n    n_sub_c, n_ins_c, n_del_c = 0, 0, 0\n    n_word, n_char = 0, 0\n    if progressbar:\n        pbar = tqdm(total=len(dataset))\n\n    # calculate WER distribution based on input lengths\n    wer_dist = {}\n\n    with open(hyp_trn_save_path, \'w\') as f_hyp, open(ref_trn_save_path, \'w\') as f_ref:\n        while True:\n            batch, is_new_epoch = dataset.next(recog_params[\'recog_batch_size\'])\n            if streaming or recog_params[\'recog_chunk_sync\']:\n                best_hyps_id, _ = models[0].decode_streaming(\n                    batch[\'xs\'], recog_params, dataset.idx2token[0],\n                    exclude_eos=True)\n            else:\n                best_hyps_id, _ = models[0].decode(\n                    batch[\'xs\'], recog_params,\n                    idx2token=dataset.idx2token[0] if progressbar else None,\n                    exclude_eos=True,\n                    refs_id=batch[\'ys\'],\n                    utt_ids=batch[\'utt_ids\'],\n                    speakers=batch[\'sessions\' if dataset.corpus == \'swbd\' else \'speakers\'],\n                    ensemble_models=models[1:] if len(models) > 1 else [])\n\n            for b in range(len(batch[\'xs\'])):\n                ref = batch[\'text\'][b]\n                if ref[0] == \'<\':\n                    ref = ref.split(\'>\')[1]\n                hyp = dataset.idx2token[0](best_hyps_id[b])\n\n                # Write to trn\n                speaker = str(batch[\'speakers\'][b]).replace(\'-\', \'_\')\n                if streaming:\n                    utt_id = str(batch[\'utt_ids\'][b]) + \'_0000000_0000001\'\n                else:\n                    utt_id = str(batch[\'utt_ids\'][b])\n                f_ref.write(ref + \' (\' + speaker + \'-\' + utt_id + \')\\n\')\n                f_hyp.write(hyp + \' (\' + speaker + \'-\' + utt_id + \')\\n\')\n                logger.debug(\'utt-id: %s\' % utt_id)\n                logger.debug(\'Ref: %s\' % ref)\n                logger.debug(\'Hyp: %s\' % hyp)\n                logger.debug(\'-\' * 150)\n\n                if not streaming:\n                    # Compute WER\n                    wer_b, sub_b, ins_b, del_b = compute_wer(ref=ref.split(\' \'),\n                                                             hyp=hyp.split(\' \'),\n                                                             normalize=False)\n                    wer += wer_b\n                    n_sub_w += sub_b\n                    n_ins_w += ins_b\n                    n_del_w += del_b\n                    n_word += len(ref.split(\' \'))\n\n                    if fine_grained:\n                        xlen_bin = (batch[\'xlens\'][b] // 200 + 1) * 200\n                        if xlen_bin in wer_dist.keys():\n                            wer_dist[xlen_bin] += [wer_b / 100]\n                        else:\n                            wer_dist[xlen_bin] = [wer_b / 100]\n\n                    # Compute CER\n                    if dataset.corpus == \'csj\':\n                        ref = ref.replace(\' \', \'\')\n                        hyp = hyp.replace(\' \', \'\')\n                    cer_b, sub_b, ins_b, del_b = compute_wer(ref=list(ref),\n                                                             hyp=list(hyp),\n                                                             normalize=False)\n                    cer += cer_b\n                    n_sub_c += sub_b\n                    n_ins_c += ins_b\n                    n_del_c += del_b\n                    n_char += len(ref)\n\n                if progressbar:\n                    pbar.update(1)\n\n            if is_new_epoch:\n                break\n\n    if progressbar:\n        pbar.close()\n\n    # Reset data counters\n    dataset.reset()\n\n    if not streaming:\n        wer /= n_word\n        n_sub_w /= n_word\n        n_ins_w /= n_word\n        n_del_w /= n_word\n\n        cer /= n_char\n        n_sub_c /= n_char\n        n_ins_c /= n_char\n        n_del_c /= n_char\n\n        if fine_grained:\n            for len_bin, wers in sorted(wer_dist.items(), key=lambda x: x[0]):\n                logger.info(\'  WER (%s): %.2f %% (%d)\' % (dataset.set, sum(wers) / len(wers), len_bin))\n\n    logger.debug(\'WER (%s): %.2f %%\' % (dataset.set, wer))\n    logger.debug(\'SUB: %.2f / INS: %.2f / DEL: %.2f\' % (n_sub_w, n_ins_w, n_del_w))\n    logger.debug(\'CER (%s): %.2f %%\' % (dataset.set, cer))\n    logger.debug(\'SUB: %.2f / INS: %.2f / DEL: %.2f\' % (n_sub_c, n_ins_c, n_del_c))\n\n    return wer, cer\n'"
neural_sp/evaluators/wordpiece_bleu.py,0,"b'#! /usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n# Copyright 2020 Kyoto University (Hirofumi Inaguma)\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n\n""""""Evaluate the wordpiece-level model by BLEU.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport logging\nfrom tqdm import tqdm\nfrom nltk.translate.bleu_score import corpus_bleu, sentence_bleu\n\nfrom neural_sp.utils import mkdir_join\n\nlogger = logging.getLogger(__name__)\n\n\ndef eval_wordpiece_bleu(models, dataset, recog_params, epoch,\n                        recog_dir=None, streaming=False, progressbar=False,\n                        fine_grained=False):\n    """"""Evaluate the wordpiece-level model by BLEU.\n\n    Args:\n        models (list): models to evaluate\n        dataset (Dataset): evaluation dataset\n        recog_params (dict):\n        epoch (int):\n        recog_dir (str):\n        streaming (bool): streaming decoding for the session-level evaluation\n        progressbar (bool): visualize the progressbar\n        fine_grained (bool): calculate fine-grained BLEU distributions based on input lengths\n    Returns:\n        bleu (float): 4-gram BLEU\n\n    """"""\n    # Reset data counter\n    dataset.reset()\n\n    if recog_dir is None:\n        recog_dir = \'decode_\' + dataset.set + \'_ep\' + str(epoch) + \'_beam\' + str(recog_params[\'recog_beam_width\'])\n        recog_dir += \'_lp\' + str(recog_params[\'recog_length_penalty\'])\n        recog_dir += \'_cp\' + str(recog_params[\'recog_coverage_penalty\'])\n        recog_dir += \'_\' + str(recog_params[\'recog_min_len_ratio\']) + \'_\' + str(recog_params[\'recog_max_len_ratio\'])\n        recog_dir += \'_lm\' + str(recog_params[\'recog_lm_weight\'])\n\n        ref_trn_save_path = mkdir_join(models[0].save_path, recog_dir, \'ref.trn\')\n        hyp_trn_save_path = mkdir_join(models[0].save_path, recog_dir, \'hyp.trn\')\n    else:\n        ref_trn_save_path = mkdir_join(recog_dir, \'ref.trn\')\n        hyp_trn_save_path = mkdir_join(recog_dir, \'hyp.trn\')\n\n    s_bleu = 0\n    n_sentence = 0\n    if progressbar:\n        pbar = tqdm(total=len(dataset))\n\n    # calculate sentence-level BLEU distribution based on input lengths\n    s_bleu_dist = {}\n\n    list_of_references = []\n    hypotheses = []\n    with open(hyp_trn_save_path, \'w\') as f_hyp, open(ref_trn_save_path, \'w\') as f_ref:\n        while True:\n            batch, is_new_epoch = dataset.next(recog_params[\'recog_batch_size\'])\n            if streaming or recog_params[\'recog_chunk_sync\']:\n                best_hyps_id, _ = models[0].decode_streaming(\n                    batch[\'xs\'], recog_params, dataset.idx2token[0],\n                    exclude_eos=True)\n            else:\n                best_hyps_id, _ = models[0].decode(\n                    batch[\'xs\'], recog_params,\n                    idx2token=dataset.idx2token[0] if progressbar else None,\n                    exclude_eos=True,\n                    refs_id=batch[\'ys\'],\n                    utt_ids=batch[\'utt_ids\'],\n                    speakers=batch[\'sessions\' if dataset.corpus == \'swbd\' else \'speakers\'],\n                    ensemble_models=models[1:] if len(models) > 1 else [])\n\n            for b in range(len(batch[\'xs\'])):\n                ref = batch[\'text\'][b]\n                if ref[0] == \'<\':\n                    ref = ref.split(\'>\')[1]\n                hyp = dataset.idx2token[0](best_hyps_id[b])\n\n                # Write to trn\n                # speaker = str(batch[\'speakers\'][b]).replace(\'-\', \'_\')\n                if streaming:\n                    utt_id = str(batch[\'utt_ids\'][b]) + \'_0000000_0000001\'\n                else:\n                    utt_id = str(batch[\'utt_ids\'][b])\n                f_ref.write(ref + \'\\n\')\n                f_hyp.write(hyp + \'\\n\')\n                logger.debug(\'utt-id: %s\' % utt_id)\n                logger.debug(\'Ref: %s\' % ref)\n                logger.debug(\'Hyp: %s\' % hyp)\n                logger.debug(\'-\' * 150)\n\n                if not streaming:\n                    list_of_references += [[ref.split(\' \')]]\n                    hypotheses += [hyp.split(\' \')]\n                    n_sentence += 1\n\n                    # Compute sentence-level BLEU\n                    if fine_grained:\n                        s_bleu_b = sentence_bleu([ref.split(\' \')], hyp.split(\' \'))\n                        s_bleu += s_bleu_b * 100\n\n                        xlen_bin = (batch[\'xlens\'][b] // 200 + 1) * 200\n                        if xlen_bin in s_bleu_dist.keys():\n                            s_bleu_dist[xlen_bin] += [s_bleu_b / 100]\n                        else:\n                            s_bleu_dist[xlen_bin] = [s_bleu_b / 100]\n\n                if progressbar:\n                    pbar.update(1)\n\n            if is_new_epoch:\n                break\n\n    if progressbar:\n        pbar.close()\n\n    # Reset data counters\n    dataset.reset()\n\n    c_bleu = corpus_bleu(list_of_references, hypotheses) * 100\n    if not streaming and fine_grained:\n        s_bleu /= n_sentence\n        for len_bin, s_bleus in sorted(s_bleu_dist.items(), key=lambda x: x[0]):\n            logger.info(\'  sentence-level BLEU (%s): %.2f %% (%d)\' %\n                        (dataset.set, sum(s_bleus) / len(s_bleus), len_bin))\n\n    logger.debug(\'Corpus-level BLEU (%s): %.2f %%\' % (dataset.set, c_bleu))\n\n    return c_bleu\n'"
neural_sp/models/__init__.py,0,b''
neural_sp/models/base.py,12,"b'#! /usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n# Copyright 2018 Kyoto University (Hirofumi Inaguma)\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n\n""""""Base class for all models.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport logging\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.nn.utils import vector_to_parameters, parameters_to_vector\n\nnp.random.seed(1)\n\nlogger = logging.getLogger(__name__)\n\n\nclass ModelBase(nn.Module):\n    """"""A base class for all models. All models have to inherit this class.""""""\n\n    def __init__(self, *args, **kwargs):\n\n        super().__init__()\n        logger.info(\'Overriding ModelBase class.\')\n\n    @property\n    def torch_version(self):\n        return float(\'.\'.join(torch.__version__.split(\'.\')[:2]))\n\n    @property\n    def num_params_dict(self):\n        if not hasattr(self, \'_nparams_dict\'):\n            self._nparams_dict = {}\n            for n, p in self.named_parameters():\n                self._nparams_dict[n] = p.view(-1).size(0)\n        return self._nparams_dict\n\n    @property\n    def total_parameters(self):\n        if not hasattr(self, \'_nparams\'):\n            self._nparams = 0\n            for n, p in self.named_parameters():\n                self._nparams += p.view(-1).size(0)\n        return self._nparams\n\n    @property\n    def use_cuda(self):\n        return torch.cuda.is_available()\n\n    @property\n    def device_id(self):\n        return torch.cuda.device_of(next(self.parameters())).idx\n\n    def init_forget_gate_bias_with_one(self):\n        """"""Initialize bias in forget gate with 1. See detail in\n\n            https://discuss.pytorch.org/t/set-forget-gate-bias-of-lstm/1745\n\n        """"""\n        for n, p in self.named_parameters():\n            if p.dim() == 1 and \'bias_ih\' in n:\n                dim = p.size(0)\n                start, end = dim // 4, dim // 2\n                p.data[start:end].fill_(1.)\n                logger.info(\'Initialize %s with 1 (bias in forget gate)\' % (n))\n\n    def add_weight_noise(self, std=0.075):\n        """"""Add variational weight noise to weight parametesr.\n\n        Args:\n            std (float): standard deviation\n\n        """"""\n        with torch.no_grad():\n            param_vector = parameters_to_vector(self.parameters())\n            normal_dist = torch.distributions.Normal(loc=torch.tensor([0.]), scale=torch.tensor([std]))\n            noise = normal_dist.sample(param_vector.size())\n            if self.device_id >= 0:\n                noise = noise.cuda(self.device_id)\n            param_vector.add_(noise[0])\n        vector_to_parameters(param_vector, self.parameters())\n\n    def cudnn_setting(self, deterministic=False, benchmark=True):\n        """"""CuDNN setting.\n\n        Args:\n            deterministic (bool):\n            benchmark (bool):\n\n        """"""\n        assert self.use_cuda\n        if benchmark:\n            torch.backends.cudnn.benchmark = True\n        elif deterministic:\n            torch.backends.cudnn.enabled = False\n            # NOTE: this is slower than GPU mode.\n        logger.info(""torch.backends.cudnn.benchmark: %s"" % torch.backends.cudnn.benchmark)\n        logger.info(""torch.backends.cudnn.enabled: %s"" % torch.backends.cudnn.enabled)\n'"
neural_sp/models/criterion.py,16,"b'#! /usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n# Copyright 2018 Kyoto University (Hirofumi Inaguma)\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n\n""""""Criterions.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport math\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\n\n\nclass MBR(torch.autograd.Function):\n    """"""Minimum Bayes Risk (MBR) training.\n\n    Args:\n        vocab (int): number of nodes in softmax layer\n\n    """"""\n    @staticmethod\n    def forward(ctx, log_probs, hyps, exp_risk, grad):\n        """"""Forward pass.\n\n        Args:\n            log_probs (FloatTensor): `[N_best, L, vocab]`\n            hyps (LongTensor): `[N_best, L]`\n            exp_risk (FloatTensor): `[1]` (for forward)\n            grad (FloatTensor): `[1]` (for backward)\n        Returns:\n            loss (FloatTensor): `[1]`\n\n        """"""\n        device_id = torch.cuda.device_of(log_probs).idx\n        onehot = torch.eye(log_probs.size(-1)).cuda(device_id)[hyps]\n        grads = grad * onehot  # mask out other classes\n        log_probs = log_probs.requires_grad_()\n        ctx.save_for_backward(log_probs, grads)\n        return exp_risk\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        input, grads, = ctx.saved_tensors\n        input.grad = grads\n        return input, None, None, None\n\n\ndef cross_entropy_lsm(logits, ys, lsm_prob, ignore_index, training, normalize_length=False):\n    """"""Compute cross entropy loss for label smoothing of sequence-to-sequence models.\n\n    Args:\n        logits (FloatTensor): `[B, T, vocab]`\n        ys (LongTensor): Indices of labels. `[B, L]`\n        lsm_prob (float): label smoothing probability\n        ignore_index (int): index for padding\n        normalize_length (bool): normalize XE loss by target sequence length\n    Returns:\n        loss_mean (FloatTensor): `[1]`\n        ppl (float): perplexity\n\n    """"""\n    bs, _, vocab = logits.size()\n    ys = ys.view(-1)\n    logits = logits.view((-1, logits.size(2)))\n\n    if lsm_prob == 0 or not training:\n        loss = F.cross_entropy(logits, ys,\n                               ignore_index=ignore_index, reduction=\'mean\')\n        ppl = np.exp(loss.item())\n        if not normalize_length:\n            loss *= (ys != ignore_index).sum() / bs\n    else:\n        with torch.no_grad():\n            target_dist = logits.new_zeros(logits.size())\n            target_dist.fill_(lsm_prob / (vocab - 1))\n            mask = (ys == ignore_index)\n            ys_masked = ys.masked_fill(mask, 0)\n            target_dist.scatter_(1, ys_masked.unsqueeze(1), 1 - lsm_prob)\n\n        log_probs = torch.log_softmax(logits, dim=-1)\n        loss_sum = -torch.mul(target_dist, log_probs)\n        n_tokens = len(ys) - mask.sum().item()\n        denom = n_tokens if normalize_length else bs\n        loss = loss_sum.masked_fill(mask.unsqueeze(1), 0).sum() / denom\n\n        ppl = np.exp(loss.item()) if normalize_length else np.exp(loss.item() * bs / n_tokens)\n\n    return loss, ppl\n\n\ndef distillation(logits_student, logits_teacher, ylens, temperature=5.0):\n    """"""Compute cross entropy loss for knowledge distillation of sequence-to-sequence models.\n\n    Args:\n        logits_student (FloatTensor): `[B, T, vocab]`\n        logits_teacher (FloatTensor): `[B, T, vocab]`\n        ylens (IntTensor): `[B]`\n        temperature (float):\n    Returns:\n        loss_mean (FloatTensor): `[1]`\n\n    """"""\n    bs, _, vocab = logits_student.size()\n\n    log_probs_student = torch.log_softmax(logits_student, dim=-1)\n    probs_teacher = torch.softmax(logits_teacher / temperature, dim=-1).data\n    loss = -torch.mul(probs_teacher, log_probs_student)\n    loss_mean = np.sum([loss[b, :ylens[b], :].sum() for b in range(bs)]) / ylens.sum()\n    return loss_mean\n\n\ndef kldiv_lsm_ctc(logits, ylens):\n    """"""Compute KL divergence loss for label smoothing of CTC and Transducer models.\n\n    Args:\n        logits (FloatTensor): `[B, T, vocab]`\n        ylens (IntTensor): `[B]`\n    Returns:\n        loss_mean (FloatTensor): `[1]`\n\n    """"""\n    bs, _, vocab = logits.size()\n\n    log_uniform = logits.new_zeros(logits.size()).fill_(math.log(1 / (vocab - 1)))\n    probs = torch.softmax(logits, dim=-1)\n    log_probs = torch.log_softmax(logits, dim=-1)\n    loss = torch.mul(probs, log_probs - log_uniform)\n    loss_mean = np.sum([loss[b, :ylens[b], :].sum() for b in range(bs)]) / ylens.sum()\n    # assert loss_mean >= 0\n    return loss_mean\n\n\ndef focal_loss(logits, ys, ylens, alpha, gamma):\n    """"""Compute focal loss.\n\n    Args:\n        logits (FloatTensor): `[B, T, vocab]`\n        ys (LongTensor): Indices of labels. `[B, L]`\n        ylens (IntTensor): `[B]`\n        alpha (float):\n        gamma (float):\n    Returns:\n        loss_mean (FloatTensor): `[1]`\n\n    """"""\n    bs = ys.size(0)\n\n    log_probs = torch.log_softmax(logits, dim=-1)\n    probs_inv = -torch.softmax(logits, dim=-1) + 1\n    loss = -alpha * torch.mul(torch.pow(probs_inv, gamma), log_probs)\n    loss_mean = np.sum([loss[b, :ylens[b], :].sum() for b in range(bs)]) / ylens.sum()\n    return loss_mean\n'"
neural_sp/models/data_parallel.py,2,"b'#! /usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n# Copyright 2018 Kyoto University (Hirofumi Inaguma)\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n\n""""""Custom class for data parallel training.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom torch.nn import DataParallel\nfrom torch.nn.parallel.scatter_gather import gather\n\n\nclass CustomDataParallel(DataParallel):\n\n    def __init__(self, module, device_ids=None, output_device=None, dim=0):\n        super(CustomDataParallel, self).__init__(module, device_ids, output_device, dim)\n\n    def gather(self, outputs, output_device):\n        n_returns = len(outputs[0])\n        n_gpus = len(outputs)\n        if n_returns == 2:\n            losses = [output[0] for output in outputs]\n            observation_mean = {}\n            for output in outputs:\n                for k, v in output[1].items():\n                    if v is None:\n                        continue\n                    if k not in observation_mean.keys():\n                        observation_mean[k] = v\n                    else:\n                        observation_mean[k] += v\n                observation_mean = {k: v / n_gpus for k, v in observation_mean.items()}\n            return gather(losses, output_device, dim=self.dim).mean(), observation_mean\n        else:\n            raise ValueError(n_returns)\n'"
neural_sp/models/torch_utils.py,11,"b'#! /usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n# Copyright 2018 Kyoto University (Hirofumi Inaguma)\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n\n""""""Utility functions.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport copy\nimport numpy as np\nimport torch\n\n\ndef repeat(module, n_layers):\n    return torch.nn.ModuleList([copy.deepcopy(module) for _ in range(n_layers)])\n\n\ndef tensor2np(x):\n    """"""Convert torch.Tensor to np.ndarray.\n\n    Args:\n        x (Tensor):\n    Returns:\n        np.ndarray\n\n    """"""\n    return x.cpu().numpy()\n\n\ndef np2tensor(array, device_id=-1):\n    """"""Convert form np.ndarray to torch.Tensor.\n\n    Args:\n        array (np.ndarray): A tensor of any sizes\n        device_id (int): ht index of the device\n    Returns:\n        tensor (FloatTensor/IntTensor/LongTensor):\n\n    """"""\n    tensor = torch.from_numpy(array)\n    if device_id >= 0:\n        tensor = tensor.cuda(device_id)\n    return tensor\n\n\ndef pad_list(xs, pad_value=0., pad_left=False):\n    """"""Convert list of Tensors to a single Tensor with padding.\n\n    Args:\n        xs (list): A list of length `[B]`, which concains Tensors of size `[T, input_size]`\n        pad_value (float):\n        pad_left (bool):\n    Returns:\n        xs_pad (FloatTensor): `[B, T, input_size]`\n\n    """"""\n    bs = len(xs)\n    max_time = max(x.size(0) for x in xs)\n    xs_pad = xs[0].new_zeros(bs, max_time, * xs[0].size()[1:]).fill_(pad_value)\n    for b in range(bs):\n        if len(xs[b]) == 0:\n            continue\n        if pad_left:\n            xs_pad[b, -xs[b].size(0):] = xs[b]\n        else:\n            xs_pad[b, :xs[b].size(0)] = xs[b]\n    return xs_pad\n\n\ndef make_pad_mask(seq_lens, device_id=-1):\n    """"""Make mask for padding.\n\n    Args:\n        seq_lens (IntTensor): `[B]`\n        device_id (int):\n    Returns:\n        mask (IntTensor): `[B, T]`\n\n    """"""\n    bs = seq_lens.size(0)\n    max_time = max(seq_lens)\n\n    seq_range = torch.arange(0, max_time, dtype=torch.int32)\n    seq_range_expand = seq_range.unsqueeze(0).expand(bs, max_time)\n    seq_length_expand = seq_range_expand.new(seq_lens).unsqueeze(-1)\n    mask = seq_range_expand < seq_length_expand\n\n    if device_id >= 0:\n        mask = mask.cuda(device_id)\n    return mask\n\n\ndef append_sos_eos(xs, ys, sos, eos, pad, bwd=False, replace_sos=False):\n    """"""Append <sos> and <eos> and return padded sequences.\n\n    Args:\n        xs (Tensor): for GPU id extraction\n        ys (list): A list of length `[B]`, which contains a list of size `[L]`\n        sos (int):\n        eos (int):\n        pad (int):\n        bwd (bool):\n        replace_sos (bool):\n    Returns:\n        ys_in (LongTensor): `[B, L]`\n        ys_out (LongTensor): `[B, L]`\n        ylens (IntTensor): `[B]`\n\n    """"""\n    device_id = torch.cuda.device_of(xs.data).idx\n    _eos = xs.new_zeros(1).fill_(eos).long()\n    ys = [np2tensor(np.fromiter(y[::-1] if bwd else y, dtype=np.int64),\n                    device_id) for y in ys]\n    if replace_sos:\n        ylens = np2tensor(np.fromiter([y[1:].size(0) + 1 for y in ys], dtype=np.int32))  # +1 for <eos>\n        ys_in = pad_list([y for y in ys], pad)\n        ys_out = pad_list([torch.cat([y[1:], _eos], dim=0) for y in ys], pad)\n    else:\n        _sos = xs.new_zeros(1).fill_(sos).long()\n        ylens = np2tensor(np.fromiter([y.size(0) + 1 for y in ys], dtype=np.int32))  # +1 for <eos>\n        ys_in = pad_list([torch.cat([_sos, y], dim=0) for y in ys], pad)\n        ys_out = pad_list([torch.cat([y, _eos], dim=0) for y in ys], pad)\n    return ys_in, ys_out, ylens\n\n\ndef compute_accuracy(logits, ys_ref, pad):\n    """"""Compute teacher-forcing accuracy.\n\n    Args:\n        logits (FloatTensor): `[B, T, vocab]`\n        ys_ref (LongTensor): `[B, T]`\n        pad (int): index for padding\n    Returns:\n        acc (float): teacher-forcing accuracy\n\n    """"""\n    pad_pred = logits.view(ys_ref.size(0), ys_ref.size(1), logits.size(-1)).argmax(2)\n    mask = ys_ref != pad\n    numerator = torch.sum(pad_pred.masked_select(mask) == ys_ref.masked_select(mask))\n    denominator = torch.sum(mask)\n    acc = float(numerator) * 100 / float(denominator)\n    return acc\n'"
neural_sp/trainers/__init__.py,0,b''
neural_sp/trainers/lr_scheduler.py,4,"b'#! /usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n# Copyright 2019 Kyoto University (Hirofumi Inaguma)\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n\n""""""Learning rate scheduler.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom glob import glob\nimport logging\nimport os\nimport torch\n\nfrom neural_sp.trainers.optimizer import set_optimizer\n\nlogger = logging.getLogger(__name__)\n\n\nclass LRScheduler(object):\n    """"""Learning rate scheduler (wrapper for optimizer).\n\n    Args:\n        optimizer (torch.optim): optimizer\n        base_lr (float): maximum of learning rate\n        decay_type (str): always/metric\n            always: decay per epoch regardless of validation metric\n            metric: decay if validation metric is not improved\n        decay_start_epoch (int): the epoch to start decay\n        decay_rate (float): the rate to decay the current learning rate\n        decay_patient_n_epochs (int): decay learning rate if results have not been\n            improved for \'decay_patient_n_epochs\'\n        early_stop_patient_n_epochs (int): number of epochs to tolerate stopping training\n            when validation perfomance is not improved\n        lower_better (bool): If True, the lower, the better.\n                             If False, the higher, the better.\n        warmup_start_lr (float): initial learning rate for warmup\n        warmup_n_steps (int): steps for learning rate warmup\n        model_size (int): d_model\n        factor (float): factor of learning rate for Transformer\n        noam (bool): schedule for Transformer\n        save_checkpoints_topk (int): save top-k checkpoints\n\n    """"""\n\n    def __init__(self, optimizer, base_lr, decay_type, decay_start_epoch, decay_rate,\n                 decay_patient_n_epochs=0, early_stop_patient_n_epochs=-1, lower_better=True,\n                 warmup_start_lr=0, warmup_n_steps=0,\n                 model_size=0, factor=1, noam=False, save_checkpoints_topk=1):\n\n        self.optimizer = optimizer\n        self.noam = noam\n\n        self._step = 0\n        self._epoch = 0\n\n        # for warmup\n        if noam:\n            self.decay_type = \'warmup\'\n            assert warmup_n_steps > 0\n            self.base_lr = factor * (model_size ** -0.5)\n        else:\n            self.base_lr = base_lr\n        self.warmup_start_lr = warmup_start_lr\n        self.warmup_n_steps = warmup_n_steps\n        self.lr = self.base_lr\n\n        # for decay\n        self.lower_better = lower_better\n        self.decay_type = decay_type\n        self.decay_start_epoch = decay_start_epoch\n        self.decay_rate = decay_rate\n        self.decay_patient_n_epochs = decay_patient_n_epochs\n        self.not_improved_n_epochs = 0\n        self.early_stop_patient_n_epochs = early_stop_patient_n_epochs\n\n        # for performance monotoring\n        self._is_topk = False\n        self.topk = save_checkpoints_topk\n        assert save_checkpoints_topk >= 1\n        self.topk_list = []\n\n    @property\n    def n_steps(self):\n        return self._step\n\n    @property\n    def n_epochs(self):\n        return self._epoch\n\n    @property\n    def is_topk(self):\n        return self._is_topk\n\n    @property\n    def is_early_stop(self):\n        return self.not_improved_n_epochs >= self.early_stop_patient_n_epochs\n\n    def step(self):\n        self._step += 1\n        self.optimizer.step()\n        if self.noam:\n            self._noam_lr()\n        else:\n            self._warmup_lr()\n\n    def zero_grad(self):\n        self.optimizer.zero_grad()\n\n    def _noam_lr(self):\n        """"""Warm up and decay learning rate per step based on Transformer.""""""\n        self.lr = self.base_lr * min(self._step ** (-0.5),\n                                     self._step * (self.warmup_n_steps ** (-1.5)))\n        self._update_lr()\n\n    def _warmup_lr(self):\n        """"""Warm up learning rate per step by incresing linearly.""""""\n        if self.warmup_n_steps > 0 and self._step <= self.warmup_n_steps:\n            self.lr = (self.base_lr - self.warmup_start_lr) / \\\n                self.warmup_n_steps * self._step + self.warmup_start_lr\n            self._update_lr()\n\n    def epoch(self, metric=None):\n        """"""Decay learning rate per epoch.\n\n        Args:\n            metric: (float): A metric to evaluate\n\n        """"""\n        self._epoch += 1\n        self._is_topk = False\n        is_best = False\n\n        if not self.lower_better:\n            metric *= -1\n\n        if metric is not None:\n            if len(self.topk_list) < self.topk or metric < self.topk_list[-1][1]:\n                topk = sum([v < metric for (ep, v) in self.topk_list]) + 1\n                logger.info(\'||||| Top-%d Score |||||\' % topk)\n                self.topk_list.append((self.n_epochs, metric))\n                self.topk_list = sorted(self.topk_list, key=lambda x: x[1])[:self.topk]\n                self._is_topk = True\n                is_best = topk == 1\n                for k, (ep, v) in enumerate(self.topk_list):\n                    logger.info(\'----- Top-%d: epoch%d (%.3f)\' % (k + 1, ep, v))\n\n        if not self.noam and self._epoch >= self.decay_start_epoch:\n            if self.decay_type == \'metric\':\n                if is_best:\n                    # Improved\n                    self.not_improved_n_epochs = 0\n                elif self.not_improved_n_epochs < self.decay_patient_n_epochs:\n                    # Not improved, but learning rate is not decayed\n                    self.not_improved_n_epochs += 1\n                else:\n                    # Not improved, and learning rate is decayed\n                    self.not_improved_n_epochs = 0\n                    self.lr *= self.decay_rate\n                    self._update_lr()\n                    logger.info(\'Epoch %d: reducing learning rate to %.7f\'\n                                % (self._epoch, self.lr))\n            elif self.decay_type == \'always\':\n                self.lr *= self.decay_rate\n                self._update_lr()\n                logger.info(\'Epoch %d: reducing learning rate to %.7f\'\n                            % (self._epoch, self.lr))\n\n    def _update_lr(self):\n        """"""Reduce learning rate.""""""\n        for param_group in self.optimizer.param_groups:\n            if isinstance(self.optimizer, torch.optim.Adadelta):\n                param_group[\'eps\'] = self.lr\n            else:\n                param_group[\'lr\'] = self.lr\n\n    def save_checkpoint(self, model, save_path, remove_old=True, epoch_detail=None):\n        """"""Save checkpoint.\n\n        Args:\n            model (torch.nn.Module):\n            save_path (str): path to the directory to save a model\n            optimizer (LRScheduler): optimizer wrapped by LRScheduler class\n            remove_old (bool): if True, all checkpoints\n                worse than the top-k ones are deleted\n            epoch_detail (float): fine-grained epoch (used for MBR training)\n\n        """"""\n        if epoch_detail is None:\n            epoch_detail = self.n_epochs\n        model_path = os.path.join(save_path, \'model.epoch-\' + str(epoch_detail))\n\n        # Remove old checkpoints\n        if remove_old:\n            for path in glob(os.path.join(save_path, \'model.epoch-*\')):\n                if \'model.epoch-avg\' in path:\n                    continue\n                epoch = int(path.split(\'-\')[-1])\n                if epoch not in [ep for (ep, v) in self.topk_list]:\n                    os.remove(path)\n\n        # Save parameters, optimizer, step index etc.\n        checkpoint = {\n            ""model_state_dict"": model.module.state_dict(),\n            ""optimizer_state_dict"": self.state_dict(),  # LRScheduler class\n        }\n        torch.save(checkpoint, model_path)\n\n        logger.info(""=> Saved checkpoint (epoch:%s): %s"" % (str(epoch_detail), model_path))\n\n    def state_dict(self):\n        """"""Returns the state of the scheduler as a :class:`dict`.\n\n        It contains an entry for every variable in self.__dict__ which\n        is not the optimizer.\n\n        """"""\n        dict = {key: value for key, value in self.__dict__.items()}\n        dict[\'optimizer_state_dict\'] = self.optimizer.state_dict()\n        return dict\n\n    def load_state_dict(self, state_dict):\n        """"""Loads the schedulers state.\n\n        Arguments:\n            state_dict (dict): scheduler state. Should be an object returned\n                from a call to :meth:`state_dict`.\n\n        """"""\n        self.__dict__.update({k: v for k, v in state_dict.items() if k != \'optimizer_state_dict\'})\n        self.optimizer.load_state_dict(state_dict[\'optimizer_state_dict\'])\n\n    def convert_to_sgd(self, model, lr, weight_decay, decay_type, decay_rate):\n        self.lr = lr\n        self.decay_type = decay_type\n        self.decay_rate = decay_rate\n        self.noam = False\n\n        # weight_decay = self.optimizer.defaults[\'weight_decay\']\n        self.optimizer = set_optimizer(model, \'sgd\', lr, weight_decay)\n        logger.info(\'========== Convert to SGD ==========\')\n'"
neural_sp/trainers/optimizer.py,9,"b'#! /usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n# Copyright 2019 Kyoto University (Hirofumi Inaguma)\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n\n""""""Select optimizer.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport logging\nimport torch\n\nlogger = logging.getLogger(__name__)\n\n\ndef set_optimizer(model, optimizer, lr, weight_decay=0.):\n    """"""Set optimizer.\n\n    Args:\n        model (): model class\n        optimizer (str): name of optimizer\n        lr (float): learning rate\n        weight_decay (float): L2 penalty for weight decay\n\n    Returns:\n        opt (torch.optim): optimizer\n\n    """"""\n    parameters = [p for p in model.parameters() if p.requires_grad]\n    logger.info(""===== Freezed parameters ====="")\n    for n in [n for n, p in model.named_parameters() if not p.requires_grad]:\n        logger.info(""%s"" % n)\n\n    if optimizer == \'sgd\':\n        opt = torch.optim.SGD(parameters,\n                              lr=lr,\n                              weight_decay=weight_decay,\n                              nesterov=False)\n    elif optimizer == \'momentum\':\n        opt = torch.optim.SGD(parameters,\n                              lr=lr,\n                              momentum=0.9,\n                              weight_decay=weight_decay,\n                              nesterov=False)\n    elif optimizer == \'nesterov\':\n        opt = torch.optim.SGD(parameters,\n                              lr=lr,\n                              #  momentum=0.9,\n                              momentum=0.99,\n                              weight_decay=weight_decay,\n                              nesterov=True)\n    elif optimizer == \'adadelta\':\n        opt = torch.optim.Adadelta(parameters,\n                                   rho=0.9,  # pytorch default\n                                   # rho=0.95,  # chainer default\n                                   # eps=1e-8,  # pytorch default\n                                   # eps=1e-6,  # chainer default\n                                   eps=lr,\n                                   weight_decay=weight_decay)\n\n    elif optimizer == \'adam\':\n        opt = torch.optim.Adam(parameters,\n                               lr=lr,\n                               weight_decay=weight_decay)\n\n    elif optimizer == \'noam\':\n        opt = torch.optim.Adam(parameters,\n                               lr=0,\n                               betas=(0.9, 0.98),\n                               eps=1e-09,\n                               weight_decay=weight_decay)\n\n    elif optimizer == \'adagrad\':\n        opt = torch.optim.Adagrad(parameters,\n                                  lr=lr,\n                                  weight_decay=weight_decay)\n\n    elif optimizer == \'rmsprop\':\n        opt = torch.optim.RMSprop(parameters,\n                                  lr=lr,\n                                  weight_decay=weight_decay)\n\n    else:\n        raise NotImplementedError(optimizer)\n\n    return opt\n'"
neural_sp/trainers/reporter.py,0,"b'#! /usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n# Copyright 2018 Kyoto University (Hirofumi Inaguma)\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n\n""""""Reporter during training.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom tensorboardX import SummaryWriter\nimport seaborn as sns\nimport os\nimport numpy as np\nfrom matplotlib import pyplot as plt\nimport logging\nimport matplotlib\nmatplotlib.use(\'Agg\')\n\nplt.style.use(\'ggplot\')\ngrey = \'#878f99\'\nblue = \'#4682B4\'\norange = \'#D2691E\'\ngreen = \'#82b74b\'\n\nlogger = logging.getLogger(__name__)\n\n\nclass Reporter(object):\n    """"""""Report loss, accuracy etc. during training.\n\n    Args:\n        save_path (str):\n\n    """"""\n\n    def __init__(self, save_path):\n        self.save_path = save_path\n\n        # tensorboard\n        self.tf_writer = SummaryWriter(save_path)\n\n        # report per step\n        self._step = 0\n        self.obsv_train = {\'loss\': {}, \'acc\': {}, \'ppl\': {}}\n        self.obsv_train_local = {\'loss\': {}, \'acc\': {}, \'ppl\': {}}\n        self.obsv_dev = {\'loss\': {}, \'acc\': {}, \'ppl\': {}}\n        self.steps = []\n\n        # report per epoch\n        self._epoch = 0\n        self.obsv_eval = []\n        self.epochs = []\n\n    def add(self, observation, is_eval=False):\n        """"""Restore values per step.\n\n        Args:\n            observation (dict):\n            is_eval (bool):\n\n        """"""\n        for k, v in observation.items():\n            if v is None:\n                continue\n            metric, name = k.split(\'.\')\n            # NOTE: metric: loss, acc, ppl\n\n            if v == float(""inf"") or v == -float(""inf""):\n                logger.warning(""WARNING: received an inf %s for %s."" % (metric, k))\n\n            if not is_eval:\n                if name not in self.obsv_train_local[metric].keys():\n                    self.obsv_train_local[metric][name] = []\n                self.obsv_train_local[metric][name].append(v)\n            else:\n                # avarage for training\n                if name not in self.obsv_train[metric].keys():\n                    self.obsv_train[metric][name] = []\n                self.obsv_train[metric][name].append(\n                    np.mean(self.obsv_train_local[metric][name]))\n                logger.info(\'%s (train): %.3f\' % (k, np.mean(self.obsv_train_local[metric][name])))\n\n                if name not in self.obsv_dev[metric].keys():\n                    self.obsv_dev[metric][name] = []\n                self.obsv_dev[metric][name].append(v)\n                logger.info(\'%s (dev): %.3f\' % (k, v))\n\n            if is_eval:\n                self.add_tensorboard_scalar(\'train\' + \'/\' + metric + \'/\' + name, v)\n            else:\n                self.add_tensorboard_scalar(\'dev\' + \'/\' + metric + \'/\' + name, v)\n\n    def add_tensorboard_scalar(self, key, value):\n        """"""Add scalar value to tensorboard.""""""\n        self.tf_writer.add_scalar(key, value, self._step)\n\n    def add_tensorboard_histogram(self, key, value):\n        """"""Add histogram value to tensorboard.""""""\n        self.tf_writer.add_histogram(key, value, self._step)\n\n    def step(self, is_eval=False):\n        self._step += 1\n        if is_eval:\n            self.steps.append(self._step)\n\n            # reset\n            self.obsv_train_local = {\'loss\': {}, \'acc\': {}, \'ppl\': {}}\n\n    def epoch(self, metric=None, name=\'wer\'):\n        self._epoch += 1\n        if metric is None:\n            return\n        self.epochs.append(self._epoch)\n\n        # register\n        self.obsv_eval.append(metric)\n\n        plt.clf()\n        plt.plot(self.epochs, self.obsv_eval, orange,\n                 label=\'dev\', linestyle=\'-\')\n        plt.xlabel(\'epoch\', fontsize=12)\n        plt.ylabel(name, fontsize=12)\n        plt.ylim([0, min(100, max(self.obsv_eval) + 1)])\n        plt.legend(loc=""upper right"", fontsize=12)\n        if os.path.isfile(os.path.join(self.save_path, name + "".png"")):\n            os.remove(os.path.join(self.save_path, name + "".png""))\n        plt.savefig(os.path.join(self.save_path, name + "".png""), dvi=500)\n\n    def snapshot(self):\n        # linestyles = [\'solid\', \'dashed\', \'dotted\', \'dashdotdotted\']\n        linestyles = [\'-\', \'--\', \'-.\', \':\', \':\', \':\', \':\', \':\', \':\', \':\', \':\', \':\']\n        for metric in self.obsv_train.keys():\n            plt.clf()\n            upper = 0\n            for i, (k, v) in enumerate(sorted(self.obsv_train[metric].items())):\n                # skip non-observed values\n                if np.mean(self.obsv_train[metric][k]) == 0:\n                    continue\n\n                plt.plot(self.steps, self.obsv_train[metric][k], blue,\n                         label=k + "" (train)"", linestyle=linestyles[i])\n                plt.plot(self.steps, self.obsv_dev[metric][k], orange,\n                         label=k + "" (dev)"", linestyle=linestyles[i])\n                upper = max(upper, max(self.obsv_train[metric][k]))\n                upper = max(upper, max(self.obsv_dev[metric][k]))\n\n                # Save as csv file\n                if os.path.isfile(os.path.join(self.save_path, metric + \'-\' + k + "".csv"")):\n                    os.remove(os.path.join(self.save_path, metric + \'-\' + k + "".csv""))\n                loss_graph = np.column_stack(\n                    (self.steps, self.obsv_train[metric][k], self.obsv_dev[metric][k]))\n                np.savetxt(os.path.join(self.save_path, metric + \'-\' + k + "".csv""), loss_graph, delimiter="","")\n\n            upper = min(upper + 10, 300)\n\n            plt.xlabel(\'step\', fontsize=12)\n            plt.ylabel(metric, fontsize=12)\n            plt.ylim([0, upper])\n            plt.legend(loc=""upper right"", fontsize=12)\n            if os.path.isfile(os.path.join(self.save_path, metric + "".png"")):\n                os.remove(os.path.join(self.save_path, metric + "".png""))\n            plt.savefig(os.path.join(self.save_path, metric + "".png""), dvi=500)\n'"
test/decoders/test_las_decoder.py,1,"b'#! /usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n""""""Test for attention-based RNN decoder.""""""\n\nimport importlib\nimport numpy as np\nimport pytest\nimport torch\n\nfrom neural_sp.models.torch_utils import np2tensor\nfrom neural_sp.models.torch_utils import pad_list\n\n\nENC_N_UNITS = 64\nVOCAB = 10\n\n\ndef make_args(**kwargs):\n    args = dict(\n        special_symbols={\'blank\': 0, \'unk\': 1, \'eos\': 2, \'pad\': 3},\n        enc_n_units=ENC_N_UNITS,\n        attn_type=\'location\',\n        rnn_type=\'lstm\',\n        n_units=64,\n        n_projs=0,\n        n_layers=2,\n        bottleneck_dim=32,\n        emb_dim=16,\n        vocab=VOCAB,\n        tie_embedding=False,\n        attn_dim=128,\n        attn_sharpening_factor=1.0,\n        attn_sigmoid_smoothing=False,\n        attn_conv_out_channels=10,\n        attn_conv_kernel_size=201,\n        attn_n_heads=1,\n        dropout=0.1,\n        dropout_emb=0.1,\n        dropout_att=0.1,\n        lsm_prob=0.0,\n        ss_prob=0.0,\n        ss_type=\'constant\',\n        ctc_weight=0.0,\n        ctc_lsm_prob=0.1,\n        ctc_fc_list=\'128_128\',\n        mbr_training=False,\n        mbr_ce_weight=0.01,\n        external_lm=None,\n        lm_fusion=\'\',\n        lm_init=False,\n        backward=False,\n        global_weight=1.0,\n        mtl_per_batch=False,\n        param_init=0.1,\n        mocha_chunk_size=4,\n        mocha_n_heads_mono=1,\n        mocha_init_r=-4,\n        mocha_eps=1e-6,\n        mocha_std=1.0,\n        mocha_no_denominator=False,\n        mocha_1dconv=False,\n        mocha_quantity_loss_weight=0.0,\n        latency_metric=False,\n        latency_loss_weight=0.0,\n        gmm_attn_n_mixtures=1,\n        replace_sos=False,\n        distillation_weight=0.0,\n        discourse_aware=False\n    )\n    args.update(kwargs)\n    return args\n\n\n@pytest.mark.parametrize(\n    ""args"", [\n        # RNN type\n        ({\'rnn_type\': \'lstm\', \'n_layers\': 1}),\n        ({\'rnn_type\': \'lstm\', \'n_layers\': 2}),\n        ({\'rnn_type\': \'gru\', \'n_layers\': 1}),\n        ({\'rnn_type\': \'gru\', \'n_layers\': 2}),\n        # attention\n        ({\'attn_type\': \'add\'}),\n        ({\'attn_type\': \'dot\'}),\n        ({\'attn_type\': \'luong_dot\'}),\n        ({\'attn_type\': \'luong_general\'}),\n        ({\'attn_type\': \'luong_concat\'}),\n        ({\'attn_type\': \'gmm\', \'gmm_attn_n_mixtures\': 5}),\n        # multihead attention\n        ({\'attn_type\': \'add\', \'attn_n_heads\': 4}),\n        # MoChA\n        ({\'attn_type\': \'mocha\', \'mocha_chunk_size\': 1}),\n        ({\'attn_type\': \'mocha\', \'mocha_chunk_size\': 4}),\n        ({\'attn_type\': \'mocha\', \'mocha_chunk_size\': 4, \'mocha_no_denominator\': True}),\n        ({\'attn_type\': \'mocha\', \'mocha_chunk_size\': 4, \'mocha_1dconv\': True}),\n        ({\'attn_type\': \'mocha\', \'mocha_chunk_size\': 4, \'mocha_quantity_loss_weight\': 1.0}),\n        # ({\'attn_type\': \'mocha\', \'mocha_chunk_size\': 4,\n        #   \'ctc_weight\': 0.5, \'latency_metric\': \'ctc_sync\', \'latency_loss_weight\': 1.0}),\n        # ({\'attn_type\': \'mocha\', \'mocha_chunk_size\': 4, \'mocha_quantity_loss_weight\': 1.0,\n        #   \'ctc_weight\': 0.5, \'latency_metric\': \'ctc_sync\', \'latency_loss_weight\': 1.0}),\n        ({\'attn_type\': \'mocha\', \'mocha_chunk_size\': 1, \'mocha_n_heads_mono\': 4}),\n        ({\'attn_type\': \'mocha\', \'mocha_chunk_size\': 4, \'mocha_n_heads_mono\': 4}),\n        # CTC\n        ({\'ctc_weight\': 0.5}),\n        ({\'ctc_weight\': 1.0}),\n        ({\'ctc_weight\': 1.0, \'ctc_lsm_prob\': 0.0}),\n        # forward-backward decoder\n        ({\'backward\': True}),\n        ({\'backward\': True, \'ctc_weight\': 0.5}),\n        ({\'backward\': True, \'ctc_weight\': 1.0}),\n        # others\n        ({\'tie_embedding\': True, \'bottleneck_dim\': 64, \'emb_dim\': 64}),\n        ({\'lsm_prob\': 0.1}),\n        ({\'ss_prob\': 0.2}),\n        # RNNLM init\n        # LM integration\n    ]\n)\ndef test_forward(args):\n    args = make_args(**args)\n\n    batch_size = 4\n    emax = 40\n    device_id = -1\n    eouts = np.random.randn(batch_size, emax, ENC_N_UNITS).astype(np.float32)\n    elens = torch.IntTensor([len(x) for x in eouts])\n    eouts = pad_list([np2tensor(x, device_id).float() for x in eouts], 0.)\n\n    ylens = [4, 5, 3, 7]\n    ys = [np.random.randint(0, VOCAB, ylen).astype(np.int32) for ylen in ylens]\n\n    module = importlib.import_module(\'neural_sp.models.seq2seq.decoders.las\')\n    dec = module.RNNDecoder(**args)\n    loss, observation = dec(eouts, elens, ys, task=\'all\')\n    assert loss.dim() == 1\n    assert loss.size(0) == 1\n    assert loss.item() >= 0\n    assert isinstance(observation, dict)\n'"
test/decoders/test_rnn_transducer_decoder.py,1,"b'#! /usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n""""""Test for RNN Transducer.""""""\n\nimport importlib\nimport numpy as np\nimport pytest\nimport torch\n\nfrom neural_sp.models.torch_utils import np2tensor\nfrom neural_sp.models.torch_utils import pad_list\n\n\nENC_N_UNITS = 64\nVOCAB = 10\n\n\ndef make_args(**kwargs):\n    args = dict(\n        special_symbols={\'blank\': 0, \'unk\': 1, \'eos\': 2, \'pad\': 3},\n        enc_n_units=ENC_N_UNITS,\n        rnn_type=\'lstm_transducer\',\n        n_units=64,\n        n_projs=0,\n        n_layers=2,\n        bottleneck_dim=32,\n        emb_dim=16,\n        vocab=VOCAB,\n        dropout=0.1,\n        dropout_emb=0.1,\n        lsm_prob=0.0,\n        ctc_weight=0.0,\n        ctc_lsm_prob=0.1,\n        ctc_fc_list=\'128_128\',\n        external_lm=None,\n        global_weight=1.0,\n        mtl_per_batch=False,\n        param_init=0.1,\n    )\n    args.update(kwargs)\n    return args\n\n\n@pytest.mark.parametrize(\n    ""args"", [\n        # RNN type\n        ({\'rnn_type\': \'lstm_transducer\', \'n_layers\': 1}),\n        ({\'rnn_type\': \'lstm_transducer\', \'n_layers\': 2}),\n        ({\'rnn_type\': \'gru_transducer\', \'n_layers\': 1}),\n        ({\'rnn_type\': \'gru_transducer\', \'n_layers\': 2}),\n        # CTC\n        ({\'ctc_weight\': 0.5}),\n        ({\'ctc_weight\': 1.0}),\n        ({\'ctc_weight\': 1.0, \'ctc_lsm_prob\': 0.0}),\n    ]\n)\ndef test_forward(args):\n    args = make_args(**args)\n\n    batch_size = 4\n    emax = 40\n    device_id = -1\n    eouts = np.random.randn(batch_size, emax, ENC_N_UNITS).astype(np.float32)\n    elens = torch.IntTensor([len(x) for x in eouts])\n    eouts = pad_list([np2tensor(x, device_id).float() for x in eouts], 0.)\n\n    ylens = [4, 5, 3, 7]\n    ys = [np.random.randint(0, VOCAB, ylen).astype(np.int32) for ylen in ylens]\n\n    module = importlib.import_module(\'neural_sp.models.seq2seq.decoders.rnn_transducer\')\n    dec = module.RNNTransducer(**args)\n    loss, observation = dec(eouts, elens, ys, task=\'all\')\n    assert loss.dim() == 1\n    assert loss.size(0) == 1\n    assert loss.item() >= 0\n    assert isinstance(observation, dict)\n'"
test/decoders/test_transformer_decoder.py,1,"b'#! /usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n""""""Test for Transformer decoder.""""""\n\nimport importlib\nimport numpy as np\nimport pytest\nimport torch\n\nfrom neural_sp.models.torch_utils import np2tensor\nfrom neural_sp.models.torch_utils import pad_list\n\n\nENC_N_UNITS = 64\nVOCAB = 10\n\n\ndef make_args(**kwargs):\n    args = dict(\n        special_symbols={\'blank\': 0, \'unk\': 1, \'eos\': 2, \'pad\': 3},\n        enc_n_units=ENC_N_UNITS,\n        attn_type=\'scaled_dot\',\n        n_heads=4,\n        n_layers=6,\n        d_model=64,\n        d_ff=256,\n        d_ff_bottleneck_dim=0,\n        layer_norm_eps=1e-12,\n        ffn_activation=\'relu\',\n        pe_type=\'add\',\n        vocab=VOCAB,\n        tie_embedding=False,\n        dropout=0.1,\n        dropout_emb=0.1,\n        dropout_att=0.1,\n        dropout_layer=0.0,\n        dropout_head=0.0,\n        lsm_prob=0.0,\n        ctc_weight=0.0,\n        ctc_lsm_prob=0.1,\n        ctc_fc_list=\'128_128\',\n        backward=False,\n        global_weight=1.0,\n        mtl_per_batch=False,\n        param_init=\'xavier_uniform\',\n        memory_transformer=False,\n        mem_len=0,\n        mocha_chunk_size=4,\n        mocha_n_heads_mono=1,\n        mocha_n_heads_chunk=1,\n        mocha_init_r=-4,\n        mocha_eps=1e-6,\n        mocha_std=1.0,\n        mocha_no_denominator=False,\n        mocha_1dconv=False,\n        mocha_quantity_loss_weight=0.0,\n        mocha_head_divergence_loss_weight=0.0,\n        latency_metric=False,\n        latency_loss_weight=0.0,\n        mocha_first_layer=1,\n        share_chunkwise_attention=False,\n        external_lm=None,\n        lm_fusion=\'\',\n        # lm_init=False,\n    )\n    args.update(kwargs)\n    return args\n\n\n@pytest.mark.parametrize(\n    ""args"", [\n        # head\n        ({\'n_heads\': 1}),\n        ({\'n_heads\': 4}),\n        # positional encoding\n        ({\'pe_type\': \'none\'}),\n        ({\'pe_type\': \'1dconv3L\'}),\n        # activation\n        ({\'ffn_activation\': \'relu\'}),\n        ({\'ffn_activation\': \'gelu\'}),\n        # ({\'ffn_activation\': \'glu\'}),\n        ({\'ffn_activation\': \'swish\'}),\n        # MMA\n        ({\'attn_type\': \'mocha\', \'mocha_chunk_size\': 1, \'mocha_n_heads_mono\': 1}),\n        ({\'attn_type\': \'mocha\', \'mocha_chunk_size\': 1, \'mocha_n_heads_mono\': 4}),\n        ({\'attn_type\': \'mocha\', \'mocha_chunk_size\': 4, \'mocha_n_heads_mono\': 1, \'mocha_n_heads_chunk\': 1}),\n        ({\'attn_type\': \'mocha\', \'mocha_chunk_size\': 4, \'mocha_n_heads_mono\': 4, \'mocha_n_heads_chunk\': 1}),\n        ({\'attn_type\': \'mocha\', \'mocha_chunk_size\': 4, \'mocha_n_heads_mono\': 1, \'mocha_n_heads_chunk\': 4}),\n        ({\'attn_type\': \'mocha\', \'mocha_chunk_size\': 4, \'mocha_n_heads_mono\': 4, \'mocha_n_heads_chunk\': 4}),\n        ({\'attn_type\': \'mocha\', \'mocha_chunk_size\': 4, \'mocha_n_heads_mono\': 4, \'mocha_n_heads_chunk\': 4,\n          \'share_chunkwise_attention\': True}),\n        # MMA + HeadDrop\n        ({\'attn_type\': \'mocha\', \'dropout_head\': 0.1, \'mocha_chunk_size\': 1, \'mocha_n_heads_mono\': 1}),\n        ({\'attn_type\': \'mocha\', \'dropout_head\': 0.1, \'mocha_chunk_size\': 1, \'mocha_n_heads_mono\': 4}),\n        ({\'attn_type\': \'mocha\', \'dropout_head\': 0.1, \'mocha_chunk_size\': 4, \'mocha_n_heads_mono\': 1, \'mocha_n_heads_chunk\': 1}),\n        ({\'attn_type\': \'mocha\', \'dropout_head\': 0.1, \'mocha_chunk_size\': 4, \'mocha_n_heads_mono\': 4, \'mocha_n_heads_chunk\': 1}),\n        ({\'attn_type\': \'mocha\', \'dropout_head\': 0.1, \'mocha_chunk_size\': 4, \'mocha_n_heads_mono\': 1, \'mocha_n_heads_chunk\': 4}),\n        ({\'attn_type\': \'mocha\', \'dropout_head\': 0.1, \'mocha_chunk_size\': 4, \'mocha_n_heads_mono\': 4, \'mocha_n_heads_chunk\': 4}),\n        # regularization\n        ({\'lsm_prob\': 0.1}),\n        ({\'dropout_layer\': 0.1}),\n        ({\'dropout_head\': 0.1}),\n        ({\'tie_embedding\': True}),\n        # CTC\n        ({\'ctc_weight\': 0.5}),\n        ({\'ctc_weight\': 1.0}),\n        ({\'ctc_weight\': 1.0, \'ctc_lsm_prob\': 0.0}),\n        # forward-backward decoder\n        ({\'backward\': True}),\n        ({\'backward\': True, \'ctc_weight\': 0.5}),\n        ({\'backward\': True, \'ctc_weight\': 1.0}),\n        # bottleneck\n        ({\'d_ff_bottleneck_dim\': 256}),\n        # RNNLM init\n        # LM integration\n    ]\n)\ndef test_forward(args):\n    args = make_args(**args)\n\n    batch_size = 4\n    emax = 40\n    device_id = -1\n    eouts = np.random.randn(batch_size, emax, ENC_N_UNITS).astype(np.float32)\n    elens = torch.IntTensor([len(x) for x in eouts])\n    eouts = pad_list([np2tensor(x, device_id).float() for x in eouts], 0.)\n\n    ylens = [4, 5, 3, 7]\n    ys = [np.random.randint(0, VOCAB, ylen).astype(np.int32) for ylen in ylens]\n\n    module = importlib.import_module(\'neural_sp.models.seq2seq.decoders.transformer\')\n    dec = module.TransformerDecoder(**args)\n    loss, observation = dec(eouts, elens, ys, task=\'all\')\n    assert loss.dim() == 1\n    assert loss.size(0) == 1\n    assert loss.item() >= 0\n    assert isinstance(observation, dict)\n'"
test/encoders/test_conv_encoder.py,2,"b'#! /usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n""""""Test for CNN encoder.""""""\n\nimport importlib\nimport numpy as np\nimport pytest\nimport torch\n\nfrom neural_sp.models.torch_utils import np2tensor\nfrom neural_sp.models.torch_utils import pad_list\n\n\ndef make_args_2d(**kwargs):\n    args = dict(\n        input_dim=80,\n        in_channel=1,\n        channels=""32_32_32"",\n        kernel_sizes=""(3,3)_(3,3)_(3,3)"",\n        strides=""(1,1)_(1,1)_(1,1)"",\n        poolings=""(2,2)_(2,2)_(2,2)"",\n        dropout=0.1,\n        batch_norm=True,\n        layer_norm=True,\n        residual=True,\n        bottleneck_dim=0,\n        param_init=0.1,\n        layer_norm_eps=1e-12\n    )\n    args.update(kwargs)\n    return args\n\n\n@pytest.mark.parametrize(\n    ""args"", [\n        # subsample4\n        ({\'channels\': ""32_32"", \'kernel_sizes\': ""(3,3)_(3,3)"",\n          \'strides\': ""(1,1)_(1,1)"", \'poolings\': ""(2, 2)_(2, 2)""}),\n        ({\'channels\': ""32_32"", \'kernel_sizes\': ""(3,3)_(3,3)"",\n          \'strides\': ""(1,1)_(1,1)"", \'poolings\': ""(2, 2)_(2, 1)""}),\n        # ({\'channels\': ""32_32"", \'kernel_sizes\': ""(3,3)_(3,3)"",\n        #   \'strides\': ""(1,1)_(1,1)"", \'poolings\': ""(2, 2)_(1, 2)""}),\n        ({\'channels\': ""32_32"", \'kernel_sizes\': ""(3,3)_(3,3)"",\n          \'strides\': ""(1,1)_(1,1)"", \'poolings\': ""(1, 1)_(1, 1)""}),\n        # subsample8\n        ({\'channels\': ""32_32_32"", \'kernel_sizes\': ""(3,3)_(3,3)_(3,3)"",\n          \'poolings\': ""(2, 2)_(2, 2)_(2, 2)""}),\n        ({\'channels\': ""32_32_32"", \'kernel_sizes\': ""(3,3)_(3,3)_(3,3)"",\n          \'poolings\': ""(2, 2)_(2, 2)_(2, 1)""}),\n        ({\'channels\': ""32_32_32"", \'kernel_sizes\': ""(3,3)_(3,3)_(3,3)"",\n          \'poolings\': ""(2, 2)_(2, 1)_(2, 1)""}),\n        # ({\'channels\': ""32_32_32"", \'kernel_sizes\': ""(3,3)_(3,3)_(3,3)"",\n        #   \'poolings\': ""(2, 2)_(2, 2)_(1, 2)""}),\n        # ({\'channels\': ""32_32_32"", \'kernel_sizes\': ""(3,3)_(3,3)_(3,3)"",\n        #   \'poolings\': ""(2, 2)_(1, 2)_(1, 2)""}),\n        ({\'channels\': ""32_32_32"", \'kernel_sizes\': ""(3,3)_(3,3)_(3,3)"",\n          \'poolings\': ""(1, 1)_(1, 1)_(1, 1)""}),\n        # bottleneck\n        # ({\'bottleneck_dim\': 128}),\n    ]\n)\ndef test_forward_2d(args):\n    args = make_args_2d(**args)\n\n    batch_size = 4\n    xmaxs = [40, 45]\n    device_id = -1\n    module = importlib.import_module(\'neural_sp.models.seq2seq.encoders.conv\')\n    (channels, kernel_sizes, strides, poolings), is_1dconv = module.parse_cnn_config(\n        args[\'channels\'], args[\'kernel_sizes\'],\n        args[\'strides\'], args[\'poolings\'])\n    assert not is_1dconv\n    enc = module.ConvEncoder(**args)\n    for xmax in xmaxs:\n        xs = np.random.randn(batch_size, xmax, args[\'input_dim\']).astype(np.float32)\n        xlens = torch.IntTensor([len(x) for x in xs])\n        xs = pad_list([np2tensor(x, device_id).float() for x in xs], 0.)\n        xs, xlens = enc(xs, xlens)\n\n        assert xs.size(0) == batch_size, xs.size()\n        assert xs.size(1) == xlens[0], xs.size()\n\n\ndef make_args_1d(**kwargs):\n    args = dict(\n        input_dim=80,\n        in_channel=1,\n        channels=""32_32_32"",\n        kernel_sizes=""3_3_3"",\n        strides=""1_1_1"",\n        poolings=""2_2_2"",\n        dropout=0.1,\n        batch_norm=False,\n        layer_norm=True,\n        residual=True,\n        bottleneck_dim=0,\n        param_init=0.1,\n        layer_norm_eps=1e-12\n    )\n    args.update(kwargs)\n    return args\n\n\n@pytest.mark.parametrize(\n    ""args"", [\n        # subsample4\n        ({\'channels\': ""32_32"", \'kernel_sizes\': ""3_3"",\n          \'strides\': ""1_1"", \'poolings\': ""2_2""}),\n        ({\'channels\': ""32_32"", \'kernel_sizes\': ""3_3"",\n          \'strides\': ""1_1"", \'poolings\': ""2_1""}),\n        ({\'channels\': ""32_32"", \'kernel_sizes\': ""3_3"",\n          \'strides\': ""1_1"", \'poolings\': ""1_1""}),\n        # subsample8\n        ({\'channels\': ""32_32_32"", \'kernel_sizes\': ""3_3_3"",\n          \'poolings\': ""2_2_2""}),\n        ({\'channels\': ""32_32_32"", \'kernel_sizes\': ""3_3_3"",\n          \'poolings\': ""2_2_1""}),\n        ({\'channels\': ""32_32_32"", \'kernel_sizes\': ""3_3_3"",\n            \'poolings\': ""2_1_1""}),\n        ({\'channels\': ""32_32_32"", \'kernel_sizes\': ""3_3_3"",\n          \'poolings\': ""1_1_1""}),\n        # bottleneck\n        # ({\'bottleneck_dim\': 128}),\n    ]\n)\ndef test_forward_1d(args):\n    args = make_args_1d(**args)\n\n    batch_size = 4\n    xmaxs = [40, 45]\n    device_id = -1\n    module = importlib.import_module(\'neural_sp.models.seq2seq.encoders.conv\')\n    (channels, kernel_sizes, strides, poolings), is_1dconv = module.parse_cnn_config(\n        args[\'channels\'], args[\'kernel_sizes\'],\n        args[\'strides\'], args[\'poolings\'])\n    assert is_1dconv\n    enc = module.ConvEncoder(**args)\n    for xmax in xmaxs:\n        xs = np.random.randn(batch_size, xmax, args[\'input_dim\']).astype(np.float32)\n        xlens = torch.IntTensor([len(x) for x in xs])\n        xs = pad_list([np2tensor(x, device_id).float() for x in xs], 0.)\n        xs, xlens = enc(xs, xlens)\n\n        assert xs.size(0) == batch_size, xs.size()\n        assert xs.size(1) == xlens[0], xs.size()\n'"
test/encoders/test_rnn_encoder.py,1,"b'#! /usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n""""""Test for RNN encoder.""""""\n\nimport importlib\nimport numpy as np\nimport pytest\nimport torch\n\nfrom neural_sp.models.torch_utils import np2tensor\nfrom neural_sp.models.torch_utils import pad_list\n\n\ndef make_args(**kwargs):\n    args = dict(\n        input_dim=80,\n        rnn_type=\'blstm\',\n        n_units=128,\n        n_projs=0,\n        last_proj_dim=0,\n        n_layers=5,\n        n_layers_sub1=0,\n        n_layers_sub2=0,\n        dropout_in=0.1,\n        dropout=0.1,\n        subsample=""1_1_1_1_1"",\n        subsample_type=\'drop\',\n        n_stacks=1,\n        n_splices=1,\n        conv_in_channel=1,\n        conv_channels=""32_32"",\n        conv_kernel_sizes=""(3,3)_(3,3)"",\n        conv_strides=""(1,1)_(1,1)"",\n        conv_poolings=""(2,2)_(2,2)"",\n        conv_batch_norm=False,\n        conv_layer_norm=False,\n        conv_bottleneck_dim=0,\n        bidirectional_sum_fwd_bwd=False,\n        task_specific_layer=False,\n        param_init=0.1,\n        chunk_size_left=-1,\n        chunk_size_right=-1\n    )\n    args.update(kwargs)\n    return args\n\n\n@pytest.mark.parametrize(\n    ""args"", [\n        # RNN type\n        ({\'rnn_type\': \'blstm\'}),\n        ({\'rnn_type\': \'bgru\'}),\n        ({\'rnn_type\': \'lstm\'}),\n        ({\'rnn_type\': \'lstm\'}),\n        ({\'rnn_type\': \'gru\'}),\n        # 2dCNN-RNN\n        ({\'rnn_type\': \'conv_blstm\'}),\n        ({\'rnn_type\': \'conv_blstm\', \'input_dim\': 240, \'conv_in_channel\': 3}),\n        ({\'rnn_type\': \'conv_bgru\'}),\n        ({\'rnn_type\': \'conv_gru\'}),\n        # 1dCNN-RNN\n        ({\'rnn_type\': \'conv_blstm\',\n          \'conv_kernel_sizes\': ""3_3"", \'conv_strides\': ""1_1"", \'conv_poolings\': ""2_2"", }),\n        ({\'rnn_type\': \'conv_blstm\',\n          \'conv_kernel_sizes\': ""3_3"", \'conv_strides\': ""1_1"", \'conv_poolings\': ""2_2"",\n          \'input_dim\': 240, \'conv_in_channel\': 3}),\n        ({\'rnn_type\': \'conv_bgru\',\n          \'conv_kernel_sizes\': ""3_3"", \'conv_strides\': ""1_1"", \'conv_poolings\': ""2_2"", }),\n        ({\'rnn_type\': \'conv_gru\',\n          \'conv_kernel_sizes\': ""3_3"", \'conv_strides\': ""1_1"", \'conv_poolings\': ""2_2"", }),\n        # normalization\n        ({\'rnn_type\': \'conv_blstm\', \'conv_batch_norm\': True}),\n        ({\'rnn_type\': \'conv_blstm\', \'conv_layer_norm\': True}),\n        # subsampling\n        ({\'rnn_type\': \'blstm\', \'subsample\': ""1_2_2_1_1"", \'subsample_type\': \'drop\'}),\n        ({\'rnn_type\': \'blstm\', \'subsample\': ""1_2_2_1_1"", \'subsample_type\': \'concat\'}),\n        ({\'rnn_type\': \'blstm\', \'subsample\': ""1_2_2_1_1"", \'subsample_type\': \'max_pool\'}),\n        ({\'rnn_type\': \'blstm\', \'subsample\': ""1_2_2_1_1"", \'subsample_type\': \'1dconv\'}),\n        ({\'rnn_type\': \'blstm\', \'subsample\': ""1_2_2_1_1"", \'subsample_type\': \'drop\',\n          \'bidirectional_sum_fwd_bwd\': True}),\n        # ({\'rnn_type\': \'blstm\', \'subsample\': ""1_2_2_1_1"", \'subsample_type\': \'concat\',\n        #   \'bidirectional_sum_fwd_bwd\': True}),\n        ({\'rnn_type\': \'blstm\', \'subsample\': ""1_2_2_1_1"", \'subsample_type\': \'max_pool\',\n          \'bidirectional_sum_fwd_bwd\': True}),\n        # ({\'rnn_type\': \'blstm\', \'subsample\': ""1_2_2_1_1"", \'subsample_type\': \'1dconv\',\n        #   \'bidirectional_sum_fwd_bwd\': True}),\n        # projection\n        ({\'rnn_type\': \'blstm\', \'n_projs\': 64}),\n        ({\'rnn_type\': \'lstm\', \'n_projs\': 64}),\n        ({\'rnn_type\': \'blstm\', \'bidirectional_sum_fwd_bwd\': True}),\n        # ({\'rnn_type\': \'blstm\', \'n_projs\': 64, \'bidirectional_sum_fwd_bwd\': True}),\n        ({\'rnn_type\': \'blstm\', \'last_proj_dim\': 256}),\n        ({\'rnn_type\': \'blstm\', \'n_projs\': 64, \'last_proj_dim\': 256}),\n        ({\'rnn_type\': \'lstm\', \'n_projs\': 64, \'last_proj_dim\': 256}),\n        ({\'rnn_type\': \'blstm\', \'bidirectional_sum_fwd_bwd\': True, \'last_proj_dim\': 256}),\n        # ({\'rnn_type\': \'blstm\', \'n_projs\': 64, \'bidirectional_sum_fwd_bwd\': True, \'last_proj_dim\': 256}),\n        # LC-BLSTM\n        ({\'rnn_type\': \'blstm\', \'chunk_size_left\': -1, \'chunk_size_right\': 40}),\n        ({\'rnn_type\': \'blstm\', \'chunk_size_left\': 40, \'chunk_size_right\': 40}),\n        ({\'rnn_type\': \'blstm\', \'bidirectional_sum_fwd_bwd\': True, \'chunk_size_left\': -1, \'chunk_size_right\': 40}),\n        ({\'rnn_type\': \'blstm\', \'bidirectional_sum_fwd_bwd\': True, \'chunk_size_left\': 40, \'chunk_size_right\': 40}),\n        ({\'rnn_type\': \'blstm\', \'bidirectional_sum_fwd_bwd\': True, \'chunk_size_left\': 40, \'chunk_size_right\': 40,\n          \'conv_poolings\': ""(2,1)_(2,1)""}),\n        ({\'rnn_type\': \'blstm\', \'bidirectional_sum_fwd_bwd\': True, \'chunk_size_left\': 40, \'chunk_size_right\': 40,\n          \'conv_poolings\': ""(1,2)_(1,2)""}),\n        ({\'rnn_type\': \'blstm\', \'bidirectional_sum_fwd_bwd\': True, \'chunk_size_left\': 40, \'chunk_size_right\': 40,\n          \'conv_poolings\': ""(1,1)_(1,1)""}),\n        # Multi-task\n        ({\'rnn_type\': \'blstm\', \'n_layers_sub1\': 4}),\n        ({\'rnn_type\': \'blstm\', \'n_layers_sub1\': 4, \'task_specific_layer\': True}),\n        ({\'rnn_type\': \'blstm\', \'n_layers_sub1\': 4, \'n_layers_sub2\': 3}),\n        ({\'rnn_type\': \'blstm\', \'n_layers_sub1\': 4, \'n_layers_sub2\': 3, \'task_specific_layer\': True}),\n    ]\n)\ndef test_forward(args):\n    args = make_args(**args)\n\n    batch_size = 4\n    xmaxs = [40, 45] if args[\'chunk_size_left\'] == -1 else [1600, 1655]\n    device_id = -1\n    module = importlib.import_module(\'neural_sp.models.seq2seq.encoders.rnn\')\n    enc = module.RNNEncoder(**args)\n    for xmax in xmaxs:\n        xs = np.random.randn(batch_size, xmax, args[\'input_dim\']).astype(np.float32)\n        xlens = torch.IntTensor([len(x) for x in xs])\n        xs = pad_list([np2tensor(x, device_id).float() for x in xs], 0.)\n        enc_out_dict = enc(xs, xlens, task=\'all\')\n\n        assert enc_out_dict[\'ys\'][\'xs\'].size(0) == batch_size, xs.size()\n        assert enc_out_dict[\'ys\'][\'xs\'].size(1) == enc_out_dict[\'ys\'][\'xlens\'][0], xs.size()\n        if args[\'n_layers_sub1\'] > 0:\n            assert enc_out_dict[\'ys_sub1\'][\'xs\'].size(0) == batch_size, xs.size()\n            assert enc_out_dict[\'ys_sub1\'][\'xs\'].size(1) == enc_out_dict[\'ys_sub1\'][\'xlens\'][0], xs.size()\n        if args[\'n_layers_sub2\'] > 0:\n            assert enc_out_dict[\'ys_sub2\'][\'xs\'].size(0) == batch_size, xs.size()\n            assert enc_out_dict[\'ys_sub2\'][\'xs\'].size(1) == enc_out_dict[\'ys_sub2\'][\'xlens\'][0], xs.size()\n'"
test/encoders/test_transformer_encoder.py,1,"b'#! /usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n""""""Test for Transformer encoder.""""""\n\nimport importlib\nimport numpy as np\nimport pytest\nimport torch\n\nfrom neural_sp.models.torch_utils import np2tensor\nfrom neural_sp.models.torch_utils import pad_list\n\n\ndef make_args(**kwargs):\n    args = dict(\n        input_dim=80,\n        enc_type=\'transformer\',\n        n_heads=4,\n        n_layers=6,\n        n_layers_sub1=0,\n        n_layers_sub2=0,\n        d_model=64,\n        d_ff=256,\n        d_ff_bottleneck_dim=0,\n        last_proj_dim=0,\n        pe_type=\'none\',\n        layer_norm_eps=1e-12,\n        ffn_activation=\'relu\',\n        dropout_in=0.1,\n        dropout=0.1,\n        dropout_att=0.1,\n        dropout_layer=0.1,\n        n_stacks=1,\n        n_splices=1,\n        conv_in_channel=1,\n        conv_channels=""32_32"",\n        conv_kernel_sizes=""(3,3)_(3,3)"",\n        conv_strides=""(1,1)_(1,1)"",\n        conv_poolings=""(2,2)_(2,2)"",\n        conv_batch_norm=False,\n        conv_layer_norm=False,\n        conv_bottleneck_dim=0,\n        conv_param_init=0.1,\n        task_specific_layer=False,\n        param_init=\'xavier_uniform\',\n        chunk_size_left=-1,\n        chunk_size_current=-1,\n        chunk_size_right=-1\n    )\n    args.update(kwargs)\n    return args\n\n\n@pytest.mark.parametrize(\n    ""args"", [\n        ({\'enc_type\': \'transformer\'}),\n        # 2dCNN-Transformer\n        ({\'enc_type\': \'conv_transformer\'}),\n        ({\'enc_type\': \'conv_transformer\', \'input_dim\': 240, \'conv_in_channel\': 3}),\n        # 1dCNN-Transformer\n        ({\'enc_type\': \'conv_transformer\',\n          \'conv_kernel_sizes\': ""3_3"", \'conv_strides\': ""1_1"", \'conv_poolings\': ""2_2""}),\n        ({\'enc_type\': \'conv_transformer\',\n          \'conv_kernel_sizes\': ""3_3"", \'conv_strides\': ""1_1"", \'conv_poolings\': ""2_2"",\n          \'input_dim\': 240, \'conv_in_channel\': 3}),\n        # positional encoding\n        ({\'pe_type\': \'add\'}),\n        ({\'pe_type\': \'relative\'}),\n        # normalization\n        ({\'enc_type\': \'conv_transformer\', \'conv_batch_norm\': True}),\n        ({\'enc_type\': \'conv_transformer\', \'conv_layer_norm\': True}),\n        # projection\n        ({\'enc_type\': \'conv_transformer\', \'last_proj_dim\': 256}),\n        # LC-Transformer\n        ({\'enc_type\': \'transformer\', \'chunk_size_left\': 96, \'chunk_size_current\': 64, \'chunk_size_right\': 32}),\n        ({\'enc_type\': \'transformer\', \'chunk_size_left\': 64, \'chunk_size_current\': 128, \'chunk_size_right\': 64}),\n        ({\'enc_type\': \'transformer\', \'chunk_size_left\': 64, \'chunk_size_current\': 128, \'chunk_size_right\': 64,\n          \'pe_type\': \'relative\'}),\n        # Multi-task\n        ({\'enc_type\': \'transformer\', \'n_layers_sub1\': 4}),\n        ({\'enc_type\': \'transformer\', \'n_layers_sub1\': 4, \'task_specific_layer\': True}),\n        ({\'enc_type\': \'transformer\', \'n_layers_sub1\': 4, \'n_layers_sub2\': 3}),\n        ({\'enc_type\': \'transformer\', \'n_layers_sub1\': 4, \'n_layers_sub2\': 3, \'task_specific_layer\': True}),\n        # bottleneck\n        ({\'d_ff_bottleneck_dim\': 128}),\n    ]\n)\ndef test_forward(args):\n    args = make_args(**args)\n\n    batch_size = 4\n    xmaxs = [40, 45] if args[\'chunk_size_left\'] == -1 else [1600, 1655]\n    device_id = -1\n    module = importlib.import_module(\'neural_sp.models.seq2seq.encoders.transformer\')\n    enc = module.TransformerEncoder(**args)\n    for xmax in xmaxs:\n        xs = np.random.randn(batch_size, xmax, args[\'input_dim\']).astype(np.float32)\n        xlens = torch.IntTensor([len(x) for x in xs])\n        xs = pad_list([np2tensor(x, device_id).float() for x in xs], 0.)\n        enc_out_dict = enc(xs, xlens, task=\'all\')\n\n        assert enc_out_dict[\'ys\'][\'xs\'].size(0) == batch_size, xs.size()\n        assert enc_out_dict[\'ys\'][\'xs\'].size(1) == enc_out_dict[\'ys\'][\'xlens\'][0], xs.size()\n        if args[\'n_layers_sub1\'] > 0:\n            assert enc_out_dict[\'ys_sub1\'][\'xs\'].size(0) == batch_size, xs.size()\n            assert enc_out_dict[\'ys_sub1\'][\'xs\'].size(1) == enc_out_dict[\'ys_sub1\'][\'xlens\'][0], xs.size()\n        if args[\'n_layers_sub2\'] > 0:\n            assert enc_out_dict[\'ys_sub2\'][\'xs\'].size(0) == batch_size, xs.size()\n            assert enc_out_dict[\'ys_sub2\'][\'xs\'].size(1) == enc_out_dict[\'ys_sub2\'][\'xlens\'][0], xs.size()\n'"
test/lm/test_rnnlm.py,0,"b'#! /usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n""""""Test for RNNLM.""""""\n\nimport argparse\nimport importlib\nimport numpy as np\nimport pytest\n\n\nENC_N_UNITS = 64\nVOCAB = 100\n\n\ndef make_args(**kwargs):\n    args = dict(\n        lm_type=\'lstm\',\n        n_units=64,\n        n_projs=0,\n        n_layers=2,\n        residual=False,\n        use_glu=False,\n        n_units_null_context=0,\n        bottleneck_dim=32,\n        emb_dim=16,\n        vocab=VOCAB,\n        dropout_in=0.1,\n        dropout_hidden=0.1,\n        # dropout_out=0.1,\n        lsm_prob=0.0,\n        param_init=0.1,\n        adaptive_softmax=False,\n        tie_embedding=False,\n    )\n    args.update(kwargs)\n    return argparse.Namespace(**args)\n\n\n@pytest.mark.parametrize(\n    ""args"", [\n        # RNN type\n        ({\'lm_type\': \'lstm\', \'n_layers\': 1}),\n        ({\'lm_type\': \'lstm\', \'n_layers\': 2}),\n        ({\'lm_type\': \'gru\', \'n_layers\': 1}),\n        ({\'lm_type\': \'gru\', \'n_layers\': 2}),\n        # projection\n        ({\'n_projs\': 32}),\n        # regularization\n        ({\'lsm_prob\': 0.1}),\n        ({\'residual\': True}),\n        ({\'n_units_null_context\': 32}),\n        ({\'use_glu\': True}),\n        ({\'use_glu\': True, \'residual\': True}),\n        ({\'use_glu\': True, \'residual\': True, \'n_units_null_context\': 32}),\n        # embedding\n        ({\'adaptive_softmax\': True}),\n        ({\'tie_embedding\': True}),\n    ]\n)\ndef test_forward(args):\n    args = make_args(**args)\n\n    ylens = [4, 5, 3, 7] * 200\n    ys = [np.random.randint(0, VOCAB, ylen).astype(np.int64) for ylen in ylens]\n\n    module = importlib.import_module(\'neural_sp.models.lm.rnnlm\')\n    lm = module.RNNLM(args)\n    loss, state, observation = lm(ys, state=None, n_caches=0)\n    # assert loss.dim() == 1, loss\n    # assert loss.size(0) == 1, loss\n    assert loss.item() >= 0\n    assert isinstance(observation, dict)\n'"
test/lm/test_transformer_xl_lm.py,0,"b'#! /usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n""""""Test for Transformer-XL LM.""""""\n\nimport argparse\nimport importlib\nimport numpy as np\nimport pytest\n\n\nENC_N_UNITS = 64\nVOCAB = 100\n\n\ndef make_args(**kwargs):\n    args = dict(\n        lm_type=\'transformer\',\n        transformer_attn_type=\'scaled_dot\',\n        transformer_n_heads=4,\n        n_layers=2,\n        transformer_d_model=64,\n        transformer_d_ff=256,\n        transformer_layer_norm_eps=1e-12,\n        transformer_ffn_activation=\'relu\',\n        transformer_pe_type=\'add\',\n        vocab=VOCAB,\n        dropout_in=0.1,\n        dropout_hidden=0.1,\n        dropout_att=0.1,\n        dropout_layer=0.0,\n        # dropout_out=0.1,\n        lsm_prob=0.0,\n        transformer_param_init=\'xavier_uniform\',\n        bptt=200,\n        mem_len=100,\n        recog_mem_len=1000,\n        zero_center_offset=False,\n        adaptive_softmax=False,\n        tie_embedding=False,\n    )\n    args.update(kwargs)\n    return argparse.Namespace(**args)\n\n\n@pytest.mark.parametrize(\n    ""args"", [\n        # head\n        ({\'transformer_n_heads\': 1}),\n        ({\'transformer_n_heads\': 4}),\n        # positional encoding\n        ({\'pe_type\': \'none\'}),\n        ({\'pe_type\': \'1dconv3L\'}),\n        # activation\n        ({\'ffn_activation\': \'relu\'}),\n        ({\'ffn_activation\': \'gelu\'}),\n        # ({\'ffn_activation\': \'glu\'}),\n        # regularization\n        ({\'lsm_prob\': 0.1}),\n        ({\'dropout_layer\': 0.1}),\n        ({\'tie_embedding\': True}),\n        # embedding\n        ({\'adaptive_softmax\': True}),\n        # memory\n        ({\'mem_len\': 0}),\n        ({\'recog_mem_len\': 0}),\n        ({\'mem_len\': 0, \'recog_mem_len\': 0}),\n        ({\'zero_center_offset\': True}),\n    ]\n)\ndef test_forward(args):\n    args = make_args(**args)\n\n    ylens = [4, 5, 3, 7] * 200\n    ys = [np.random.randint(0, VOCAB, ylen).astype(np.int64) for ylen in ylens]\n\n    module = importlib.import_module(\'neural_sp.models.lm.transformer_xl\')\n    lm = module.TransformerXL(args)\n    loss, state, observation = lm(ys, state=None, n_caches=0)\n    # assert loss.dim() == 1, loss\n    # assert loss.size(0) == 1, loss\n    assert loss.item() >= 0\n    assert isinstance(observation, dict)\n'"
test/lm/test_transformerlm.py,0,"b'#! /usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n""""""Test for TransformerLM.""""""\n\nimport argparse\nimport importlib\nimport numpy as np\nimport pytest\n\n\nENC_N_UNITS = 64\nVOCAB = 100\n\n\ndef make_args(**kwargs):\n    args = dict(\n        lm_type=\'transformer\',\n        transformer_attn_type=\'scaled_dot\',\n        transformer_n_heads=4,\n        n_layers=2,\n        transformer_d_model=64,\n        transformer_d_ff=256,\n        transformer_layer_norm_eps=1e-12,\n        transformer_ffn_activation=\'relu\',\n        transformer_pe_type=\'add\',\n        vocab=VOCAB,\n        dropout_in=0.1,\n        dropout_hidden=0.1,\n        dropout_att=0.1,\n        dropout_layer=0.0,\n        # dropout_out=0.1,\n        lsm_prob=0.0,\n        transformer_param_init=\'xavier_uniform\',\n        mem_len=0,\n        recog_mem_len=0,\n        adaptive_softmax=False,\n        tie_embedding=False,\n    )\n    args.update(kwargs)\n    return argparse.Namespace(**args)\n\n\n@pytest.mark.parametrize(\n    ""args"", [\n        # head\n        ({\'transformer_n_heads\': 1}),\n        ({\'transformer_n_heads\': 4}),\n        # positional encoding\n        ({\'pe_type\': \'none\'}),\n        ({\'pe_type\': \'1dconv3L\'}),\n        # activation\n        ({\'ffn_activation\': \'relu\'}),\n        ({\'ffn_activation\': \'gelu\'}),\n        # ({\'ffn_activation\': \'glu\'}),\n        # regularization\n        ({\'lsm_prob\': 0.1}),\n        ({\'dropout_layer\': 0.1}),\n        ({\'tie_embedding\': True}),\n        # embedding\n        ({\'adaptive_softmax\': True}),\n        # memory\n        ({\'mem_len\': 5}),\n        ({\'recog_mem_len\': 5}),\n        ({\'mem_len\': 5, \'recog_mem_len\': 5}),\n    ]\n)\ndef test_forward(args):\n    args = make_args(**args)\n\n    ylens = [4, 5, 3, 7] * 200\n    ys = [np.random.randint(0, VOCAB, ylen).astype(np.int64) for ylen in ylens]\n\n    module = importlib.import_module(\'neural_sp.models.lm.transformerlm\')\n    lm = module.TransformerLM(args)\n    loss, state, observation = lm(ys, state=None, n_caches=0)\n    # assert loss.dim() == 1, loss\n    # assert loss.size(0) == 1, loss\n    assert loss.item() >= 0\n    assert isinstance(observation, dict)\n'"
test/modules/test_attention.py,4,"b'#! /usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n""""""Test for single-head atteniton.""""""\n\nimport importlib\nimport pytest\nimport torch\n\n\ndef make_args(**kwargs):\n    args = dict(\n        kdim=32,\n        qdim=32,\n        adim=16,\n        atype=\'location\',\n        sharpening_factor=1,\n        sigmoid_smoothing=False,\n        conv_out_channels=10,\n        conv_kernel_size=201,\n        dropout=0.1,\n        lookahead=2\n    )\n    args.update(kwargs)\n    return args\n\n\n@pytest.mark.parametrize(\n    ""args"", [\n        # attention type\n        ({\'atype\': \'location\'}),\n        ({\'atype\': \'add\'}),\n        ({\'atype\': \'dot\'}),\n        ({\'atype\': \'luong_dot\'}),\n        ({\'atype\': \'luong_general\'}),\n        ({\'atype\': \'luong_concat\'}),\n        # others\n        ({\'sharpening_factor\': 2.0}),\n        ({\'sigmoid_smoothing\': True}),\n        ({\'sigmoid_smoothing\': True}),\n    ]\n)\ndef test_forward(args):\n    args = make_args(**args)\n\n    batch_size = 4\n    klen = 40\n    qlen = 5\n    key = torch.FloatTensor(batch_size, klen, args[\'kdim\'])\n    value = torch.FloatTensor(batch_size, klen, args[\'kdim\'])\n    query = torch.FloatTensor(batch_size, qlen, args[\'qdim\'])\n    src_mask = torch.ones(batch_size, 1, klen).byte()\n\n    module = importlib.import_module(\'neural_sp.models.modules.attention\')\n    attention = module.AttentionMechanism(**args)\n    attention.train()\n    aws = None\n    for i in range(qlen):\n        out = attention(key, value, query[:, i:i + 1], mask=src_mask, aw_prev=aws,\n                        mode=\'parallel\', cache=True)\n        assert len(out) == 3\n        cv, aws, _ = out\n        assert cv.size() == (batch_size, 1, value.size(2))\n        assert aws.size() == (batch_size, 1, 1, klen)\n'"
test/modules/test_gmm_attention.py,4,"b'#! /usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n""""""Test for GMM atteniton.""""""\n\nimport importlib\nimport pytest\nimport torch\n\n\ndef make_args(**kwargs):\n    args = dict(\n        kdim=32,\n        qdim=32,\n        adim=16,\n        n_mixtures=5,\n    )\n    args.update(kwargs)\n    return args\n\n\n@pytest.mark.parametrize(\n    ""args"", [\n        ({\'n_mixtures\': 1}),\n        ({\'n_mixtures\': 4}),\n    ]\n)\ndef test_forward(args):\n    args = make_args(**args)\n\n    batch_size = 4\n    klen = 40\n    qlen = 5\n    key = torch.FloatTensor(batch_size, klen, args[\'kdim\'])\n    value = torch.FloatTensor(batch_size, klen, args[\'kdim\'])\n    query = torch.FloatTensor(batch_size, qlen, args[\'qdim\'])\n    src_mask = torch.ones(batch_size, 1, klen).byte()\n\n    module = importlib.import_module(\'neural_sp.models.modules.gmm_attention\')\n    attention = module.GMMAttention(**args)\n    attention.train()\n    aws = None\n    for i in range(qlen):\n        out = attention(key, value, query[:, i:i + 1], mask=src_mask, aw_prev=aws,\n                        mode=\'parallel\', cache=True)\n        assert len(out) == 3\n        cv, aws, _ = out\n        assert cv.size() == (batch_size, 1, value.size(2))\n        assert aws.size() == (batch_size, 1, 1, klen)\n'"
test/modules/test_mocha.py,7,"b'#! /usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n""""""Test for monotonic chunkwise atteniton (MoChA).""""""\n\nimport importlib\nimport pytest\nimport torch\n\n\ndef make_args(**kwargs):\n    args = dict(\n        kdim=32,\n        qdim=32,\n        adim=16,\n        odim=32,\n        atype=\'add\',\n        chunk_size=1,\n        n_heads_mono=1,\n        n_heads_chunk=1,\n        conv1d=False,\n        init_r=-4,\n        eps=1e-6,\n        noise_std=1.0,\n        no_denominator=False,\n        sharpening_factor=1.0,\n        dropout=0.1,\n        dropout_head=0.,\n        bias=True,\n        param_init=\'\',\n        decot=False,\n        lookahead=2,\n        share_chunkwise_attention=True\n    )\n    args.update(kwargs)\n    return args\n\n\n@pytest.mark.parametrize(\n    ""args"", [\n        # hard monotonic attention\n        ({\'n_heads_mono\': 1, \'chunk_size\': 1}),\n        ({\'n_heads_mono\': 1, \'chunk_size\': 1, \'conv1d\': True}),\n        ({\'n_heads_mono\': 1, \'chunk_size\': 1, \'no_denominator\': True}),\n        ({\'n_heads_mono\': 1, \'chunk_size\': 1, \'bias\': False}),\n        # mocha\n        ({\'n_heads_mono\': 1, \'chunk_size\': 4}),\n        # MMA\n        ({\'n_heads_mono\': 4, \'n_heads_chunk\': 1, \'chunk_size\': 1, \'atype\': \'scaled_dot\'}),\n        ({\'n_heads_mono\': 4, \'n_heads_chunk\': 1, \'chunk_size\': 4, \'atype\': \'scaled_dot\'}),\n        ({\'n_heads_mono\': 1, \'n_heads_chunk\': 4, \'chunk_size\': 4, \'atype\': \'scaled_dot\'}),\n        ({\'n_heads_mono\': 4, \'n_heads_chunk\': 4, \'chunk_size\': 4, \'atype\': \'scaled_dot\'}),\n        ({\'n_heads_mono\': 4, \'n_heads_chunk\': 1, \'chunk_size\': 4, \'atype\': \'scaled_dot\',\n          \'share_chunkwise_attention\': False}),\n        ({\'n_heads_mono\': 4, \'n_heads_chunk\': 4, \'chunk_size\': 4, \'atype\': \'scaled_dot\',\n          \'share_chunkwise_attention\': False}),\n        # HeadDrop\n        ({\'n_heads_mono\': 4, \'n_heads_chunk\': 1, \'chunk_size\': 1, \'atype\': \'scaled_dot\',\n          \'dropout_head\': 0.5}),\n        ({\'n_heads_mono\': 4, \'n_heads_chunk\': 1, \'chunk_size\': 4, \'atype\': \'scaled_dot\',\n          \'dropout_head\': 0.5}),\n        ({\'n_heads_mono\': 1, \'n_heads_chunk\': 4, \'chunk_size\': 4, \'atype\': \'scaled_dot\',\n          \'dropout_head\': 0.5}),\n        ({\'n_heads_mono\': 4, \'n_heads_chunk\': 4, \'chunk_size\': 4, \'atype\': \'scaled_dot\',\n          \'dropout_head\': 0.5}),\n    ]\n)\ndef test_forward_soft_parallel(args):\n    args = make_args(**args)\n\n    batch_size = 4\n    klen = 40\n    qlen = 5\n    key = torch.FloatTensor(batch_size, klen, args[\'kdim\'])\n    value = torch.FloatTensor(batch_size, klen, args[\'kdim\'])\n    query = torch.FloatTensor(batch_size, qlen, args[\'qdim\'])\n    src_mask = torch.ones(batch_size, 1, klen).byte()\n\n    module = importlib.import_module(\'neural_sp.models.modules.mocha\')\n    attention = module.MoChA(**args)\n    attention.train()\n    alpha = None\n    for i in range(qlen):\n        out = attention(key, value, query[:, i:i + 1], mask=src_mask, aw_prev=alpha,\n                        mode=\'parallel\', cache=True)\n        assert len(out) == 3\n        cv, alpha, beta = out\n        assert cv.size() == (batch_size, 1, value.size(2))\n        assert alpha.size() == (batch_size, args[\'n_heads_mono\'], 1, klen)\n        if args[\'chunk_size\'] > 1:\n            assert beta is not None\n            assert beta.size() == (batch_size, args[\'n_heads_mono\'] * args[\'n_heads_chunk\'], 1, klen)\n\n\n@pytest.mark.parametrize(\n    ""args"", [\n        # hard monotonic attention\n        ({\'n_heads_mono\': 1, \'chunk_size\': 1}),\n        ({\'n_heads_mono\': 1, \'chunk_size\': 1, \'conv1d\': True}),\n        ({\'n_heads_mono\': 1, \'chunk_size\': 1, \'no_denominator\': True}),\n        ({\'n_heads_mono\': 1, \'chunk_size\': 1, \'bias\': False}),\n        # mocha\n        ({\'n_heads_mono\': 1, \'chunk_size\': 4}),\n        # MMA\n        ({\'n_heads_mono\': 4, \'n_heads_chunk\': 1, \'chunk_size\': 1, \'atype\': \'scaled_dot\'}),\n        ({\'n_heads_mono\': 4, \'n_heads_chunk\': 1, \'chunk_size\': 4, \'atype\': \'scaled_dot\'}),\n        ({\'n_heads_mono\': 1, \'n_heads_chunk\': 4, \'chunk_size\': 4, \'atype\': \'scaled_dot\'}),\n        ({\'n_heads_mono\': 4, \'n_heads_chunk\': 4, \'chunk_size\': 4, \'atype\': \'scaled_dot\'}),\n        ({\'n_heads_mono\': 4, \'n_heads_chunk\': 1, \'chunk_size\': 4, \'atype\': \'scaled_dot\',\n          \'share_chunkwise_attention\': False}),\n        ({\'n_heads_mono\': 4, \'n_heads_chunk\': 4, \'chunk_size\': 4, \'atype\': \'scaled_dot\',\n          \'share_chunkwise_attention\': False}),\n    ]\n)\ndef test_forward_hard(args):\n    args = make_args(**args)\n\n    batch_size = 4\n    klen = 40\n    qlen = 5\n    key = torch.FloatTensor(batch_size, klen, args[\'kdim\'])\n    value = torch.FloatTensor(batch_size, klen, args[\'kdim\'])\n    query = torch.FloatTensor(batch_size, qlen, args[\'qdim\'])\n\n    module = importlib.import_module(\'neural_sp.models.modules.mocha\')\n    mocha = module.MoChA(**args)\n    mocha.eval()\n    alpha = None\n    for i in range(qlen):\n        out = mocha(key, value, query[:, i:i + 1], mask=None, aw_prev=alpha,\n                    mode=\'hard\', cache=False, eps_wait=-1,\n                    efficient_decoding=False)\n        assert len(out) == 3\n        cv, alpha, beta = out\n        assert cv.size() == (batch_size, 1, value.size(2))\n        assert alpha.size() == (batch_size, args[\'n_heads_mono\'], 1, klen)\n        if args[\'chunk_size\'] > 1:\n            assert beta is not None\n            assert beta.size() == (batch_size, args[\'n_heads_mono\'] * args[\'n_heads_chunk\'], 1, klen)\n'"
test/modules/test_multihead_attention.py,4,"b'#! /usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n""""""Test for multihead atteniton.""""""\n\nimport importlib\nimport pytest\nimport torch\n\n\ndef make_args(**kwargs):\n    args = dict(\n        kdim=32,\n        qdim=32,\n        adim=16,\n        odim=32,\n        atype=\'scaled_dot\',\n        n_heads=4,\n        dropout=0.1,\n        dropout_head=0.,\n        bias=True,\n        param_init=\'xavier_uniform\'\n    )\n    args.update(kwargs)\n    return args\n\n\n@pytest.mark.parametrize(\n    ""args"", [\n        ({\'n_heads\': 1}),\n        ({\'n_heads\': 1, \'atype\': \'add\'}),\n        ({\'n_heads\': 4}),\n        ({\'n_heads\': 4, \'atype\': \'add\'}),\n        ({\'dropout_head\': 0.5}),\n        ({\'bias\': False}),\n    ]\n)\ndef test_forward(args):\n    args = make_args(**args)\n\n    batch_size = 4\n    klen = 40\n    qlen = 5\n    key = torch.FloatTensor(batch_size, klen, args[\'kdim\'])\n    value = torch.FloatTensor(batch_size, klen, args[\'kdim\'])\n    query = torch.FloatTensor(batch_size, qlen, args[\'qdim\'])\n    src_mask = torch.ones(batch_size, 1, klen).byte()\n\n    module = importlib.import_module(\'neural_sp.models.modules.multihead_attention\')\n    attention = module.MultiheadAttentionMechanism(**args)\n    attention.train()\n    aws = None\n    for i in range(qlen):\n        out = attention(key, value, query[:, i:i + 1], mask=src_mask, aw_prev=aws,\n                        mode=\'parallel\', cache=True)\n        assert len(out) == 3\n        cv, aws, _ = out\n        assert cv.size() == (batch_size, 1, value.size(2))\n        assert aws.size() == (batch_size, args[\'n_heads\'], 1, klen)\n'"
test/modules/test_pointwise_feed_forward.py,1,"b'#! /usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n""""""Test for positionwise fully-connected feed-forward neural network (FFN).""""""\n\nimport importlib\nimport pytest\nimport torch\n\n\ndef make_args(**kwargs):\n    args = dict(\n        d_model=32,\n        d_ff=128,\n        dropout=0.1,\n        activation=\'relu\',\n        param_init=\'xavier_uniform\',\n        bottleneck_dim=0\n    )\n    args.update(kwargs)\n    return args\n\n\n@pytest.mark.parametrize(\n    ""args"", [\n        # activation\n        ({\'activation\': \'relu\'}),\n        ({\'activation\': \'gelu\'}),\n        ({\'activation\': \'gelu_accurate\'}),\n        ({\'activation\': \'glu\'}),\n        ({\'activation\': \'swish\'}),\n        # bottleneck\n        ({\'bottleneck_dim\': 16}),\n    ]\n)\ndef test_forward(args):\n    args = make_args(**args)\n\n    batch_size = 4\n    max_len = 40\n    ffn_in = torch.FloatTensor(batch_size, max_len, args[\'d_model\'])\n\n    module = importlib.import_module(\'neural_sp.models.modules.positionwise_feed_forward\')\n    ffn = module.PositionwiseFeedForward(**args)\n\n    ffn_out = ffn(ffn_in)\n    assert ffn_in.size() == ffn_out.size()\n'"
test/modules/test_relative_multihead_attention.py,9,"b'#! /usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n""""""Test for relative multihead atteniton.""""""\n\nimport importlib\nimport pytest\nimport torch\n\n\ndef make_args(**kwargs):\n    args = dict(\n        kdim=32,\n        qdim=32,\n        adim=16,\n        odim=32,\n        n_heads=4,\n        dropout=0.1,\n        bias=True,\n        param_init=\'xavier_uniform\'\n    )\n    args.update(kwargs)\n    return args\n\n\n@pytest.mark.parametrize(\n    ""args, learnable"", [\n        ({\'n_heads\': 1}, False),\n        ({\'n_heads\': 1}, True),\n        ({\'n_heads\': 4}, False),\n        ({\'n_heads\': 4}, True),\n        ({\'bias\': False}, False),\n    ]\n)\ndef test_forward(args, learnable):\n    args = make_args(**args)\n\n    batch_size = 4\n    device_id = -1\n    klen = 40\n    mlen = 20\n    qlen = 5\n    key = torch.FloatTensor(batch_size, klen, args[\'kdim\'])\n    memory = torch.FloatTensor(batch_size, mlen, args[\'kdim\'])\n    query = torch.FloatTensor(batch_size, qlen, args[\'qdim\'])\n\n    # Create the self-attention mask\n    causal_mask = torch.ones(qlen, klen + mlen).byte()\n    causal_mask = torch.tril(causal_mask, diagonal=0 + mlen, out=causal_mask).unsqueeze(0)\n    causal_mask = causal_mask.repeat([batch_size, 1, 1])  # `[B, qlen, klen+mlen]`\n    # src_mask = torch.ones(batch_size, 1, klen + mlen).byte()\n\n    module_embedding = importlib.import_module(\'neural_sp.models.modules.positional_embedding\')\n    pos_emb = module_embedding.XLPositionalEmbedding(args[\'kdim\'], args[\'dropout\'])\n\n    if learnable:\n        u = torch.nn.Parameter(torch.Tensor(args[\'n_heads\'], args[\'adim\'] // args[\'n_heads\']))\n        v = torch.nn.Parameter(torch.Tensor(args[\'n_heads\'], args[\'adim\'] // args[\'n_heads\']))\n    else:\n        u, v = None, None\n\n    module_mha = importlib.import_module(\'neural_sp.models.modules.relative_multihead_attention\')\n    attention = module_mha.RelativeMultiheadAttentionMechanism(**args)\n    attention.train()\n    aws = None\n    for i in range(qlen):\n        pos_idxs = torch.arange(klen + mlen - 1, -1, -1.0, dtype=torch.float)\n        pos_embs = pos_emb(pos_idxs, device_id)\n\n        out = attention(key, query[:, i:i + 1], memory, mask=causal_mask[:, i:i + 1],\n                        pos_embs=pos_embs, u=u, v=v)\n        assert len(out) == 2\n        cv, aws = out\n        assert cv.size() == (batch_size, 1, memory.size(2))\n        assert aws.size() == (batch_size, args[\'n_heads\'], 1, klen + mlen)\n'"
neural_sp/bin/asr/__init__.py,0,b''
neural_sp/bin/asr/eval.py,0,"b'#! /usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n# Copyright 2018 Kyoto University (Hirofumi Inaguma)\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n\n""""""Evaluate the ASR model.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport argparse\nimport copy\nimport logging\nimport os\nimport sys\nimport time\n\nfrom neural_sp.bin.args_asr import parse_args_eval\nfrom neural_sp.bin.eval_utils import average_checkpoints\nfrom neural_sp.bin.train_utils import load_checkpoint\nfrom neural_sp.bin.train_utils import load_config\nfrom neural_sp.bin.train_utils import set_logger\nfrom neural_sp.datasets.asr import Dataset\nfrom neural_sp.evaluators.accuracy import eval_accuracy\nfrom neural_sp.evaluators.character import eval_char\nfrom neural_sp.evaluators.phone import eval_phone\nfrom neural_sp.evaluators.ppl import eval_ppl\nfrom neural_sp.evaluators.word import eval_word\nfrom neural_sp.evaluators.wordpiece import eval_wordpiece\nfrom neural_sp.evaluators.wordpiece_bleu import eval_wordpiece_bleu\nfrom neural_sp.models.lm.build import build_lm\nfrom neural_sp.models.seq2seq.speech2text import Speech2Text\n\nlogger = logging.getLogger(__name__)\n\n\ndef main():\n\n    # Load configuration\n    args, recog_params, dir_name = parse_args_eval(sys.argv[1:])\n\n    # Setting for logging\n    if os.path.isfile(os.path.join(args.recog_dir, \'decode.log\')):\n        os.remove(os.path.join(args.recog_dir, \'decode.log\'))\n    set_logger(os.path.join(args.recog_dir, \'decode.log\'), stdout=args.recog_stdout)\n\n    wer_avg, cer_avg, per_avg = 0, 0, 0\n    ppl_avg, loss_avg = 0, 0\n    acc_avg = 0\n    bleu_avg = 0\n    for i, s in enumerate(args.recog_sets):\n        # Load dataset\n        dataset = Dataset(corpus=args.corpus,\n                          tsv_path=s,\n                          dict_path=os.path.join(dir_name, \'dict.txt\'),\n                          dict_path_sub1=os.path.join(dir_name, \'dict_sub1.txt\') if os.path.isfile(\n                              os.path.join(dir_name, \'dict_sub1.txt\')) else False,\n                          dict_path_sub2=os.path.join(dir_name, \'dict_sub2.txt\') if os.path.isfile(\n                              os.path.join(dir_name, \'dict_sub2.txt\')) else False,\n                          nlsyms=os.path.join(dir_name, \'nlsyms.txt\'),\n                          wp_model=os.path.join(dir_name, \'wp.model\'),\n                          wp_model_sub1=os.path.join(dir_name, \'wp_sub1.model\'),\n                          wp_model_sub2=os.path.join(dir_name, \'wp_sub2.model\'),\n                          unit=args.unit,\n                          unit_sub1=args.unit_sub1,\n                          unit_sub2=args.unit_sub2,\n                          batch_size=args.recog_batch_size,\n                          is_test=True)\n\n        if i == 0:\n            # Load the ASR model\n            model = Speech2Text(args, dir_name)\n            epoch = int(args.recog_model[0].split(\'-\')[-1])\n            if args.recog_n_average > 1:\n                # Model averaging for Transformer\n                # topk_list = load_checkpoint(model, args.recog_model[0])\n                model = average_checkpoints(model, args.recog_model[0],\n                                            # topk_list=topk_list,\n                                            n_average=args.recog_n_average)\n            else:\n                load_checkpoint(model, args.recog_model[0])\n\n            # Ensemble (different models)\n            ensemble_models = [model]\n            if len(args.recog_model) > 1:\n                for recog_model_e in args.recog_model[1:]:\n                    conf_e = load_config(os.path.join(os.path.dirname(recog_model_e), \'conf.yml\'))\n                    args_e = copy.deepcopy(args)\n                    for k, v in conf_e.items():\n                        if \'recog\' not in k:\n                            setattr(args_e, k, v)\n                    model_e = Speech2Text(args_e)\n                    load_checkpoint(model_e, recog_model_e)\n                    if args.recog_n_gpus >= 1:\n                        model_e.cuda()\n                    ensemble_models += [model_e]\n\n            # Load the LM for shallow fusion\n            if not args.lm_fusion:\n                # first path\n                if args.recog_lm is not None and args.recog_lm_weight > 0:\n                    conf_lm = load_config(os.path.join(os.path.dirname(args.recog_lm), \'conf.yml\'))\n                    args_lm = argparse.Namespace()\n                    for k, v in conf_lm.items():\n                        setattr(args_lm, k, v)\n                    args_lm.recog_mem_len = args.recog_mem_len\n                    lm = build_lm(args_lm, wordlm=args.recog_wordlm,\n                                  lm_dict_path=os.path.join(os.path.dirname(args.recog_lm), \'dict.txt\'),\n                                  asr_dict_path=os.path.join(dir_name, \'dict.txt\'))\n                    load_checkpoint(lm, args.recog_lm)\n                    if args_lm.backward:\n                        model.lm_bwd = lm\n                    else:\n                        model.lm_fwd = lm\n\n                # second path (forward)\n                if args.recog_lm_second is not None and args.recog_lm_second_weight > 0:\n                    conf_lm_second = load_config(os.path.join(os.path.dirname(args.recog_lm_second), \'conf.yml\'))\n                    args_lm_second = argparse.Namespace()\n                    for k, v in conf_lm_second.items():\n                        setattr(args_lm_second, k, v)\n                    args_lm_second.recog_mem_len = args.recog_mem_len\n                    lm_second = build_lm(args_lm_second)\n                    load_checkpoint(lm_second, args.recog_lm_second)\n                    model.lm_second = lm_second\n\n                # second path (bakward)\n                if args.recog_lm_bwd is not None and args.recog_lm_bwd_weight > 0:\n                    conf_lm = load_config(os.path.join(os.path.dirname(args.recog_lm_bwd), \'conf.yml\'))\n                    args_lm_bwd = argparse.Namespace()\n                    for k, v in conf_lm.items():\n                        setattr(args_lm_bwd, k, v)\n                    args_lm_bwd.recog_mem_len = args.recog_mem_len\n                    lm_bwd = build_lm(args_lm_bwd)\n                    load_checkpoint(lm_bwd, args.recog_lm_bwd)\n                    model.lm_bwd = lm_bwd\n\n            if not args.recog_unit:\n                args.recog_unit = args.unit\n\n            logger.info(\'recog unit: %s\' % args.recog_unit)\n            logger.info(\'recog metric: %s\' % args.recog_metric)\n            logger.info(\'recog oracle: %s\' % args.recog_oracle)\n            logger.info(\'epoch: %d\' % epoch)\n            logger.info(\'batch size: %d\' % args.recog_batch_size)\n            logger.info(\'beam width: %d\' % args.recog_beam_width)\n            logger.info(\'min length ratio: %.3f\' % args.recog_min_len_ratio)\n            logger.info(\'max length ratio: %.3f\' % args.recog_max_len_ratio)\n            logger.info(\'length penalty: %.3f\' % args.recog_length_penalty)\n            logger.info(\'length norm: %s\' % args.recog_length_norm)\n            logger.info(\'coverage penalty: %.3f\' % args.recog_coverage_penalty)\n            logger.info(\'coverage threshold: %.3f\' % args.recog_coverage_threshold)\n            logger.info(\'CTC weight: %.3f\' % args.recog_ctc_weight)\n            logger.info(\'fist LM path: %s\' % args.recog_lm)\n            logger.info(\'second LM path: %s\' % args.recog_lm_second)\n            logger.info(\'backward LM path: %s\' % args.recog_lm_bwd)\n            logger.info(\'LM weight (first-pass): %.3f\' % args.recog_lm_weight)\n            logger.info(\'LM weight (second-pass): %.3f\' % args.recog_lm_second_weight)\n            logger.info(\'LM weight (backward): %.3f\' % args.recog_lm_bwd_weight)\n            logger.info(\'GNMT: %s\' % args.recog_gnmt_decoding)\n            logger.info(\'forward-backward attention: %s\' % args.recog_fwd_bwd_attention)\n            logger.info(\'resolving UNK: %s\' % args.recog_resolving_unk)\n            logger.info(\'ensemble: %d\' % (len(ensemble_models)))\n            logger.info(\'ASR decoder state carry over: %s\' % (args.recog_asr_state_carry_over))\n            logger.info(\'LM state carry over: %s\' % (args.recog_lm_state_carry_over))\n            logger.info(\'model average (Transformer): %d\' % (args.recog_n_average))\n\n            # GPU setting\n            if args.recog_n_gpus >= 1:\n                model.cudnn_setting(deterministic=True, benchmark=False)\n                model.cuda()\n\n        start_time = time.time()\n\n        if args.recog_metric == \'edit_distance\':\n            if args.recog_unit in [\'word\', \'word_char\']:\n                wer, cer, _ = eval_word(ensemble_models, dataset, recog_params,\n                                        epoch=epoch - 1,\n                                        recog_dir=args.recog_dir,\n                                        progressbar=True)\n                wer_avg += wer\n                cer_avg += cer\n            elif args.recog_unit == \'wp\':\n                wer, cer = eval_wordpiece(ensemble_models, dataset, recog_params,\n                                          epoch=epoch - 1,\n                                          recog_dir=args.recog_dir,\n                                          streaming=args.recog_streaming,\n                                          progressbar=True,\n                                          fine_grained=True)\n                wer_avg += wer\n                cer_avg += cer\n            elif \'char\' in args.recog_unit:\n                wer, cer = eval_char(ensemble_models, dataset, recog_params,\n                                     epoch=epoch - 1,\n                                     recog_dir=args.recog_dir,\n                                     progressbar=True,\n                                     task_idx=0)\n                #  task_idx=1 if args.recog_unit and \'char\' in args.recog_unit else 0)\n                wer_avg += wer\n                cer_avg += cer\n            elif \'phone\' in args.recog_unit:\n                per = eval_phone(ensemble_models, dataset, recog_params,\n                                 epoch=epoch - 1,\n                                 recog_dir=args.recog_dir,\n                                 progressbar=True)\n                per_avg += per\n            else:\n                raise ValueError(args.recog_unit)\n        elif args.recog_metric in [\'ppl\', \'loss\']:\n            ppl, loss = eval_ppl(ensemble_models, dataset, progressbar=True)\n            ppl_avg += ppl\n            loss_avg += loss\n        elif args.recog_metric == \'accuracy\':\n            acc_avg += eval_accuracy(ensemble_models, dataset, progressbar=True)\n        elif args.recog_metric == \'bleu\':\n            bleu = eval_wordpiece_bleu(ensemble_models, dataset, recog_params,\n                                       epoch=epoch - 1,\n                                       recog_dir=args.recog_dir,\n                                       streaming=args.recog_streaming,\n                                       progressbar=True,\n                                       fine_grained=True)\n            bleu_avg += bleu\n        else:\n            raise NotImplementedError(args.recog_metric)\n        elasped_time = time.time() - start_time\n        logger.info(\'Elasped time: %.3f [sec]\' % elasped_time)\n        logger.info(\'RTF: %.3f\' % (elasped_time / (dataset.n_frames * 0.01)))\n\n    if args.recog_metric == \'edit_distance\':\n        if \'phone\' in args.recog_unit:\n            logger.info(\'PER (avg.): %.2f %%\\n\' % (per_avg / len(args.recog_sets)))\n        else:\n            logger.info(\'WER / CER (avg.): %.2f / %.2f %%\\n\' %\n                        (wer_avg / len(args.recog_sets), cer_avg / len(args.recog_sets)))\n    elif args.recog_metric in [\'ppl\', \'loss\']:\n        logger.info(\'PPL (avg.): %.2f\\n\' % (ppl_avg / len(args.recog_sets)))\n        print(\'PPL (avg.): %.3f\' % (ppl_avg / len(args.recog_sets)))\n        logger.info(\'Loss (avg.): %.2f\\n\' % (loss_avg / len(args.recog_sets)))\n        print(\'Loss (avg.): %.3f\' % (loss_avg / len(args.recog_sets)))\n    elif args.recog_metric == \'accuracy\':\n        logger.info(\'Accuracy (avg.): %.2f\\n\' % (acc_avg / len(args.recog_sets)))\n        print(\'Accuracy (avg.): %.3f\' % (acc_avg / len(args.recog_sets)))\n    elif args.recog_metric == \'bleu\':\n        logger.info(\'BLEU (avg.): %.2f\\n\' % (bleu / len(args.recog_sets)))\n        print(\'BLEU (avg.): %.3f\' % (bleu / len(args.recog_sets)))\n\n\nif __name__ == \'__main__\':\n    main()\n'"
neural_sp/bin/asr/plot_attention.py,0,"b'#! /usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n# Copyright 2018 Kyoto University (Hirofumi Inaguma)\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n\n""""""Plot attention weights of the attention model.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport argparse\nimport copy\nimport logging\nimport os\nimport shutil\nimport sys\n\nfrom neural_sp.bin.args_asr import parse_args_eval\nfrom neural_sp.bin.eval_utils import average_checkpoints\nfrom neural_sp.bin.plot_utils import plot_attention_weights\nfrom neural_sp.bin.train_utils import (\n    load_checkpoint,\n    load_config,\n    set_logger\n)\nfrom neural_sp.datasets.asr import Dataset\nfrom neural_sp.models.lm.build import build_lm\nfrom neural_sp.models.seq2seq.speech2text import Speech2Text\nfrom neural_sp.utils import mkdir_join\n\nlogger = logging.getLogger(__name__)\n\n\ndef main():\n\n    # Load configuration\n    args, recog_params, dir_name = parse_args_eval(sys.argv[1:])\n\n    # Setting for logging\n    if os.path.isfile(os.path.join(args.recog_dir, \'plot.log\')):\n        os.remove(os.path.join(args.recog_dir, \'plot.log\'))\n    set_logger(os.path.join(args.recog_dir, \'plot.log\'), stdout=args.recog_stdout)\n\n    for i, s in enumerate(args.recog_sets):\n        # Load dataset\n        dataset = Dataset(corpus=args.corpus,\n                          tsv_path=s,\n                          dict_path=os.path.join(dir_name, \'dict.txt\'),\n                          dict_path_sub1=os.path.join(dir_name, \'dict_sub1.txt\') if os.path.isfile(\n                              os.path.join(dir_name, \'dict_sub1.txt\')) else False,\n                          nlsyms=args.nlsyms,\n                          wp_model=os.path.join(dir_name, \'wp.model\'),\n                          unit=args.unit,\n                          unit_sub1=args.unit_sub1,\n                          batch_size=args.recog_batch_size,\n                          is_test=True)\n\n        if i == 0:\n            # Load the ASR model\n            model = Speech2Text(args, dir_name)\n            epoch = int(args.recog_model[0].split(\'-\')[-1])\n            if args.recog_n_average > 1:\n                # Model averaging for Transformer\n                model = average_checkpoints(model, args.recog_model[0],\n                                            n_average=args.recog_n_average)\n            else:\n                load_checkpoint(model, args.recog_model[0])\n\n            # Ensemble (different models)\n            ensemble_models = [model]\n            if len(args.recog_model) > 1:\n                for recog_model_e in args.recog_model[1:]:\n                    conf_e = load_config(os.path.join(os.path.dirname(recog_model_e), \'conf.yml\'))\n                    args_e = copy.deepcopy(args)\n                    for k, v in conf_e.items():\n                        if \'recog\' not in k:\n                            setattr(args_e, k, v)\n                    model_e = Speech2Text(args_e)\n                    load_checkpoint(model_e, recog_model_e)\n                    if args.recog_n_gpus >= 1:\n                        model_e.cuda()\n                    ensemble_models += [model_e]\n\n            # Load the LM for shallow fusion\n            if not args.lm_fusion:\n                # first path\n                if args.recog_lm is not None and args.recog_lm_weight > 0:\n                    conf_lm = load_config(os.path.join(os.path.dirname(args.recog_lm), \'conf.yml\'))\n                    args_lm = argparse.Namespace()\n                    for k, v in conf_lm.items():\n                        setattr(args_lm, k, v)\n                    lm = build_lm(args_lm)\n                    load_checkpoint(lm, args.recog_lm)\n                    if args_lm.backward:\n                        model.lm_bwd = lm\n                    else:\n                        model.lm_fwd = lm\n                # NOTE: only support for first path\n\n            if not args.recog_unit:\n                args.recog_unit = args.unit\n\n            logger.info(\'recog unit: %s\' % args.recog_unit)\n            logger.info(\'recog oracle: %s\' % args.recog_oracle)\n            logger.info(\'epoch: %d\' % epoch)\n            logger.info(\'batch size: %d\' % args.recog_batch_size)\n            logger.info(\'beam width: %d\' % args.recog_beam_width)\n            logger.info(\'min length ratio: %.3f\' % args.recog_min_len_ratio)\n            logger.info(\'max length ratio: %.3f\' % args.recog_max_len_ratio)\n            logger.info(\'length penalty: %.3f\' % args.recog_length_penalty)\n            logger.info(\'length norm: %s\' % args.recog_length_norm)\n            logger.info(\'coverage penalty: %.3f\' % args.recog_coverage_penalty)\n            logger.info(\'coverage threshold: %.3f\' % args.recog_coverage_threshold)\n            logger.info(\'CTC weight: %.3f\' % args.recog_ctc_weight)\n            logger.info(\'fist LM path: %s\' % args.recog_lm)\n            logger.info(\'LM weight: %.3f\' % args.recog_lm_weight)\n            logger.info(\'GNMT: %s\' % args.recog_gnmt_decoding)\n            logger.info(\'forward-backward attention: %s\' % args.recog_fwd_bwd_attention)\n            logger.info(\'resolving UNK: %s\' % args.recog_resolving_unk)\n            logger.info(\'ensemble: %d\' % (len(ensemble_models)))\n            logger.info(\'ASR decoder state carry over: %s\' % (args.recog_asr_state_carry_over))\n            logger.info(\'LM state carry over: %s\' % (args.recog_lm_state_carry_over))\n            logger.info(\'model average (Transformer): %d\' % (args.recog_n_average))\n\n            # GPU setting\n            if args.recog_n_gpus >= 1:\n                model.cudnn_setting(deterministic=True, benchmark=False)\n                model.cuda()\n\n        save_path = mkdir_join(args.recog_dir, \'att_weights\')\n\n        # Clean directory\n        if save_path is not None and os.path.isdir(save_path):\n            shutil.rmtree(save_path)\n            os.mkdir(save_path)\n\n        while True:\n            batch, is_new_epoch = dataset.next(recog_params[\'recog_batch_size\'])\n            best_hyps_id, aws = model.decode(\n                batch[\'xs\'], recog_params, dataset.idx2token[0],\n                exclude_eos=False,\n                refs_id=batch[\'ys\'],\n                ensemble_models=ensemble_models[1:] if len(ensemble_models) > 1 else [],\n                speakers=batch[\'sessions\'] if dataset.corpus == \'swbd\' else batch[\'speakers\'])\n\n            # Get CTC probs\n            ctc_probs, topk_ids = None, None\n            if args.ctc_weight > 0:\n                ctc_probs, topk_ids, xlens = model.get_ctc_probs(\n                    batch[\'xs\'], temperature=1, topk=min(100, model.vocab))\n                # NOTE: ctc_probs: \'[B, T, topk]\'\n\n            if model.bwd_weight > 0.5:\n                # Reverse the order\n                best_hyps_id = [hyp[::-1] for hyp in best_hyps_id]\n                aws = [aw[:, ::-1] for aw in aws]\n\n            for b in range(len(batch[\'xs\'])):\n                tokens = dataset.idx2token[0](best_hyps_id[b], return_list=True)\n                spk = batch[\'speakers\'][b]\n\n                plot_attention_weights(\n                    aws[b][:, :len(tokens)], tokens,\n                    spectrogram=batch[\'xs\'][b][:, :dataset.input_dim] if args.input_type == \'speech\' else None,\n                    ref=batch[\'text\'][b].lower(),\n                    save_path=mkdir_join(save_path, spk, batch[\'utt_ids\'][b] + \'.png\'),\n                    figsize=(20, 8),\n                    ctc_probs=ctc_probs[b, :xlens[b]] if ctc_probs is not None else None,\n                    ctc_topk_ids=topk_ids[b] if topk_ids is not None else None)\n\n                if model.bwd_weight > 0.5:\n                    hyp = \' \'.join(tokens[::-1])\n                else:\n                    hyp = \' \'.join(tokens)\n                logger.info(\'utt-id: %s\' % batch[\'utt_ids\'][b])\n                logger.info(\'Ref: %s\' % batch[\'text\'][b].lower())\n                logger.info(\'Hyp: %s\' % hyp)\n                logger.info(\'-\' * 50)\n\n            if is_new_epoch:\n                break\n\n\nif __name__ == \'__main__\':\n    main()\n'"
neural_sp/bin/asr/plot_ctc.py,0,"b'#! /usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n# Copyright 2018 Kyoto University (Hirofumi Inaguma)\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n\n""""""Plot the CTC posteriors.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport logging\nimport os\nimport shutil\nimport sys\n\nfrom neural_sp.bin.args_asr import parse_args_eval\nfrom neural_sp.bin.eval_utils import average_checkpoints\nfrom neural_sp.bin.plot_utils import plot_ctc_probs\nfrom neural_sp.bin.train_utils import (\n    load_checkpoint,\n    set_logger\n)\nfrom neural_sp.datasets.asr import Dataset\nfrom neural_sp.models.seq2seq.speech2text import Speech2Text\nfrom neural_sp.utils import mkdir_join\n\nlogger = logging.getLogger(__name__)\n\n\ndef main():\n\n    # Load configuration\n    args, recog_params, dir_name = parse_args_eval(sys.argv[1:])\n\n    # Setting for logging\n    if os.path.isfile(os.path.join(args.recog_dir, \'plot.log\')):\n        os.remove(os.path.join(args.recog_dir, \'plot.log\'))\n    set_logger(os.path.join(args.recog_dir, \'plot.log\'), stdout=args.recog_stdout)\n\n    for i, s in enumerate(args.recog_sets):\n        # Load dataset\n        dataset = Dataset(corpus=args.corpus,\n                          tsv_path=s,\n                          dict_path=os.path.join(dir_name, \'dict.txt\'),\n                          dict_path_sub1=os.path.join(dir_name, \'dict_sub1.txt\') if os.path.isfile(\n                              os.path.join(dir_name, \'dict_sub1.txt\')) else False,\n                          nlsyms=args.nlsyms,\n                          wp_model=os.path.join(dir_name, \'wp.model\'),\n                          unit=args.unit,\n                          unit_sub1=args.unit_sub1,\n                          batch_size=args.recog_batch_size,\n                          is_test=True)\n\n        if i == 0:\n            # Load the ASR model\n            model = Speech2Text(args, dir_name)\n            epoch = int(args.recog_model[0].split(\'-\')[-1])\n            if args.recog_n_average > 1:\n                # Model averaging for Transformer\n                model = average_checkpoints(model, args.recog_model[0],\n                                            n_average=args.recog_n_average)\n            else:\n                load_checkpoint(model, args.recog_model[0])\n\n            if not args.recog_unit:\n                args.recog_unit = args.unit\n\n            logger.info(\'recog unit: %s\' % args.recog_unit)\n            logger.info(\'epoch: %d\' % epoch)\n            logger.info(\'batch size: %d\' % args.recog_batch_size)\n\n            # GPU setting\n            if args.recog_n_gpus >= 1:\n                model.cudnn_setting(deterministic=True, benchmark=False)\n                model.cuda()\n\n        save_path = mkdir_join(args.recog_dir, \'ctc_probs\')\n\n        # Clean directory\n        if save_path is not None and os.path.isdir(save_path):\n            shutil.rmtree(save_path)\n            os.mkdir(save_path)\n\n        while True:\n            batch, is_new_epoch = dataset.next(recog_params[\'recog_batch_size\'])\n            best_hyps_id, _ = model.decode(batch[\'xs\'], recog_params)\n\n            # Get CTC probs\n            ctc_probs, topk_ids, xlens = model.get_ctc_probs(\n                batch[\'xs\'], temperature=1, topk=min(100, model.vocab))\n            # NOTE: ctc_probs: \'[B, T, topk]\'\n\n            for b in range(len(batch[\'xs\'])):\n                tokens = dataset.idx2token[0](best_hyps_id[b], return_list=True)\n                spk = batch[\'speakers\'][b]\n\n                plot_ctc_probs(\n                    ctc_probs[b, :xlens[b]], topk_ids[b],\n                    subsample_factor=args.subsample_factor,\n                    spectrogram=batch[\'xs\'][b][:, :dataset.input_dim],\n                    save_path=mkdir_join(save_path, spk, batch[\'utt_ids\'][b] + \'.png\'),\n                    figsize=(20, 8))\n\n                hyp = \' \'.join(tokens)\n                logger.info(\'utt-id: %s\' % batch[\'utt_ids\'][b])\n                logger.info(\'Ref: %s\' % batch[\'text\'][b].lower())\n                logger.info(\'Hyp: %s\' % hyp)\n                logger.info(\'-\' * 50)\n\n            if is_new_epoch:\n                break\n\n\nif __name__ == \'__main__\':\n    main()\n'"
neural_sp/bin/asr/train.py,3,"b'#! /usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n# Copyright 2018 Kyoto University (Hirofumi Inaguma)\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n\n""""""Train the ASR model.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport argparse\nimport copy\nimport cProfile\nimport logging\nimport os\nfrom setproctitle import setproctitle\nimport shutil\nimport sys\nimport time\nimport torch\nfrom tqdm import tqdm\n\nfrom neural_sp.bin.args_asr import parse_args_train\nfrom neural_sp.bin.model_name import set_asr_model_name\nfrom neural_sp.bin.train_utils import (\n    compute_susampling_factor,\n    load_checkpoint,\n    load_config,\n    save_config,\n    set_logger,\n    set_save_path\n)\nfrom neural_sp.datasets.asr import Dataset\nfrom neural_sp.evaluators.accuracy import eval_accuracy\nfrom neural_sp.evaluators.character import eval_char\nfrom neural_sp.evaluators.phone import eval_phone\nfrom neural_sp.evaluators.ppl import eval_ppl\nfrom neural_sp.evaluators.word import eval_word\nfrom neural_sp.evaluators.wordpiece import eval_wordpiece\nfrom neural_sp.evaluators.wordpiece_bleu import eval_wordpiece_bleu\nfrom neural_sp.models.data_parallel import CustomDataParallel\nfrom neural_sp.models.lm.build import build_lm\nfrom neural_sp.models.seq2seq.speech2text import Speech2Text\nfrom neural_sp.trainers.lr_scheduler import LRScheduler\nfrom neural_sp.trainers.optimizer import set_optimizer\nfrom neural_sp.trainers.reporter import Reporter\nfrom neural_sp.utils import mkdir_join\n\ntorch.manual_seed(1)\ntorch.cuda.manual_seed_all(1)\n\nlogger = logging.getLogger(__name__)\n\n\ndef main():\n\n    args = parse_args_train(sys.argv[1:])\n    args_init = copy.deepcopy(args)\n    args_teacher = copy.deepcopy(args)\n\n    # Load a conf file\n    if args.resume:\n        conf = load_config(os.path.join(os.path.dirname(args.resume), \'conf.yml\'))\n        for k, v in conf.items():\n            if k != \'resume\':\n                setattr(args, k, v)\n    recog_params = vars(args)\n\n    args = compute_susampling_factor(args)\n\n    # Load dataset\n    train_set = Dataset(corpus=args.corpus,\n                        tsv_path=args.train_set,\n                        tsv_path_sub1=args.train_set_sub1,\n                        tsv_path_sub2=args.train_set_sub2,\n                        dict_path=args.dict,\n                        dict_path_sub1=args.dict_sub1,\n                        dict_path_sub2=args.dict_sub2,\n                        nlsyms=args.nlsyms,\n                        unit=args.unit,\n                        unit_sub1=args.unit_sub1,\n                        unit_sub2=args.unit_sub2,\n                        wp_model=args.wp_model,\n                        wp_model_sub1=args.wp_model_sub1,\n                        wp_model_sub2=args.wp_model_sub2,\n                        batch_size=args.batch_size * args.n_gpus,\n                        n_epochs=args.n_epochs,\n                        min_n_frames=args.min_n_frames,\n                        max_n_frames=args.max_n_frames,\n                        shuffle_bucket=args.shuffle_bucket,\n                        sort_by=\'input\',\n                        short2long=args.sort_short2long,\n                        sort_stop_epoch=args.sort_stop_epoch,\n                        dynamic_batching=args.dynamic_batching,\n                        ctc=args.ctc_weight > 0,\n                        ctc_sub1=args.ctc_weight_sub1 > 0,\n                        ctc_sub2=args.ctc_weight_sub2 > 0,\n                        subsample_factor=args.subsample_factor,\n                        subsample_factor_sub1=args.subsample_factor_sub1,\n                        subsample_factor_sub2=args.subsample_factor_sub2,\n                        discourse_aware=args.discourse_aware)\n    dev_set = Dataset(corpus=args.corpus,\n                      tsv_path=args.dev_set,\n                      tsv_path_sub1=args.dev_set_sub1,\n                      tsv_path_sub2=args.dev_set_sub2,\n                      dict_path=args.dict,\n                      dict_path_sub1=args.dict_sub1,\n                      dict_path_sub2=args.dict_sub2,\n                      nlsyms=args.nlsyms,\n                      unit=args.unit,\n                      unit_sub1=args.unit_sub1,\n                      unit_sub2=args.unit_sub2,\n                      wp_model=args.wp_model,\n                      wp_model_sub1=args.wp_model_sub1,\n                      wp_model_sub2=args.wp_model_sub2,\n                      batch_size=args.batch_size * args.n_gpus,\n                      min_n_frames=args.min_n_frames,\n                      max_n_frames=args.max_n_frames,\n                      ctc=args.ctc_weight > 0,\n                      ctc_sub1=args.ctc_weight_sub1 > 0,\n                      ctc_sub2=args.ctc_weight_sub2 > 0,\n                      subsample_factor=args.subsample_factor,\n                      subsample_factor_sub1=args.subsample_factor_sub1,\n                      subsample_factor_sub2=args.subsample_factor_sub2)\n    eval_sets = [Dataset(corpus=args.corpus,\n                         tsv_path=s,\n                         dict_path=args.dict,\n                         nlsyms=args.nlsyms,\n                         unit=args.unit,\n                         wp_model=args.wp_model,\n                         batch_size=1,\n                         is_test=True) for s in args.eval_sets]\n\n    args.vocab = train_set.vocab\n    args.vocab_sub1 = train_set.vocab_sub1\n    args.vocab_sub2 = train_set.vocab_sub2\n    args.input_dim = train_set.input_dim\n\n    # Set save path\n    if args.resume:\n        save_path = os.path.dirname(args.resume)\n        dir_name = os.path.basename(save_path)\n    else:\n        dir_name = set_asr_model_name(args)\n        if args.mbr_training:\n            assert args.asr_init\n            save_path = mkdir_join(os.path.dirname(args.asr_init), dir_name)\n        else:\n            save_path = mkdir_join(args.model_save_dir, \'_\'.join(\n                os.path.basename(args.train_set).split(\'.\')[:-1]), dir_name)\n        save_path = set_save_path(save_path)  # avoid overwriting\n\n    # Set logger\n    set_logger(os.path.join(save_path, \'train.log\'), stdout=args.stdout)\n\n    # Load a LM conf file for LM fusion & LM initialization\n    if not args.resume and args.external_lm:\n        lm_conf = load_config(os.path.join(os.path.dirname(args.external_lm), \'conf.yml\'))\n        args.lm_conf = argparse.Namespace()\n        for k, v in lm_conf.items():\n            setattr(args.lm_conf, k, v)\n        assert args.unit == args.lm_conf.unit\n        assert args.vocab == args.lm_conf.vocab\n\n    # Model setting\n    model = Speech2Text(args, save_path, train_set.idx2token[0])\n\n    if not args.resume:\n        # Save the conf file as a yaml file\n        save_config(vars(args), os.path.join(save_path, \'conf.yml\'))\n        if args.external_lm:\n            save_config(args.lm_conf, os.path.join(save_path, \'conf_lm.yml\'))\n\n        # Save the nlsyms, dictionar, and wp_model\n        if args.nlsyms:\n            shutil.copy(args.nlsyms, os.path.join(save_path, \'nlsyms.txt\'))\n        for sub in [\'\', \'_sub1\', \'_sub2\']:\n            if getattr(args, \'dict\' + sub):\n                shutil.copy(getattr(args, \'dict\' + sub), os.path.join(save_path, \'dict\' + sub + \'.txt\'))\n            if getattr(args, \'unit\' + sub) == \'wp\':\n                shutil.copy(getattr(args, \'wp_model\' + sub), os.path.join(save_path, \'wp\' + sub + \'.model\'))\n\n        for k, v in sorted(vars(args).items(), key=lambda x: x[0]):\n            logger.info(\'%s: %s\' % (k, str(v)))\n\n        # Count total parameters\n        for n in sorted(list(model.num_params_dict.keys())):\n            n_params = model.num_params_dict[n]\n            logger.info(""%s %d"" % (n, n_params))\n        logger.info(""Total %.2f M parameters"" % (model.total_parameters / 1000000))\n        logger.info(model)\n\n        # Initialize with pre-trained model\'s parameters\n        if args.asr_init:\n            # Load the ASR model (full model)\n            assert os.path.isfile(args.asr_init), \'There is no checkpoint.\'\n            conf_init = load_config(os.path.join(os.path.dirname(args.asr_init), \'conf.yml\'))\n            for k, v in conf_init.items():\n                setattr(args_init, k, v)\n            model_init = Speech2Text(args_init)\n            load_checkpoint(model_init, args.asr_init)\n\n            # Overwrite parameters\n            param_dict = dict(model_init.named_parameters())\n            for n, p in model.named_parameters():\n                if n in param_dict.keys() and p.size() == param_dict[n].size():\n                    if args.asr_init_enc_only and \'enc\' not in n:\n                        continue\n                    p.data = param_dict[n].data\n                    logger.info(\'Overwrite %s\' % n)\n\n    # Set optimizer\n    resume_epoch = 0\n    if args.resume:\n        resume_epoch = int(args.resume.split(\'-\')[-1])\n        optimizer = set_optimizer(model, \'sgd\' if resume_epoch > args.convert_to_sgd_epoch else args.optimizer,\n                                  args.lr, args.weight_decay)\n    else:\n        optimizer = set_optimizer(model, args.optimizer, args.lr, args.weight_decay)\n\n    # Wrap optimizer by learning rate scheduler\n    is_transformer = \'former\' in args.enc_type or args.dec_type == \'former\'\n    optimizer = LRScheduler(optimizer, args.lr,\n                            decay_type=args.lr_decay_type,\n                            decay_start_epoch=args.lr_decay_start_epoch,\n                            decay_rate=args.lr_decay_rate,\n                            decay_patient_n_epochs=args.lr_decay_patient_n_epochs,\n                            early_stop_patient_n_epochs=args.early_stop_patient_n_epochs,\n                            lower_better=args.metric not in [\'accuracy\', \'bleu\'],\n                            warmup_start_lr=args.warmup_start_lr,\n                            warmup_n_steps=args.warmup_n_steps,\n                            model_size=getattr(args, \'transformer_d_model\', 0),\n                            factor=args.lr_factor,\n                            noam=is_transformer,\n                            save_checkpoints_topk=10 if is_transformer else 1)\n\n    if args.resume:\n        # Restore the last saved model\n        load_checkpoint(model, args.resume, optimizer)\n\n        # Resume between convert_to_sgd_epoch -1 and convert_to_sgd_epoch\n        if resume_epoch == args.convert_to_sgd_epoch:\n            optimizer.convert_to_sgd(model, args.lr, args.weight_decay,\n                                     decay_type=\'always\', decay_rate=0.5)\n\n    # Load the teacher ASR model\n    teacher = None\n    if args.teacher:\n        assert os.path.isfile(args.teacher), \'There is no checkpoint.\'\n        conf_teacher = load_config(os.path.join(os.path.dirname(args.teacher), \'conf.yml\'))\n        for k, v in conf_teacher.items():\n            setattr(args_teacher, k, v)\n        # Setting for knowledge distillation\n        args_teacher.ss_prob = 0\n        args.lsm_prob = 0\n        teacher = Speech2Text(args_teacher)\n        load_checkpoint(teacher, args.teacher)\n\n    # Load the teacher LM\n    teacher_lm = None\n    if args.teacher_lm:\n        assert os.path.isfile(args.teacher_lm), \'There is no checkpoint.\'\n        conf_lm = load_config(os.path.join(os.path.dirname(args.teacher_lm), \'conf.yml\'))\n        args_lm = argparse.Namespace()\n        for k, v in conf_lm.items():\n            setattr(args_lm, k, v)\n        teacher_lm = build_lm(args_lm)\n        load_checkpoint(teacher_lm, args.teacher_lm)\n\n    # GPU setting\n    if args.n_gpus >= 1:\n        model.cudnn_setting(deterministic=not (is_transformer or args.cudnn_benchmark),\n                            benchmark=args.cudnn_benchmark)\n        model = CustomDataParallel(model, device_ids=list(range(0, args.n_gpus)))\n        model.cuda()\n        if teacher is not None:\n            teacher.cuda()\n        if teacher_lm is not None:\n            teacher_lm.cuda()\n\n    # Set process name\n    logger.info(\'PID: %s\' % os.getpid())\n    logger.info(\'USERNAME: %s\' % os.uname()[1])\n    setproctitle(args.job_name if args.job_name else dir_name)\n\n    # Set reporter\n    reporter = Reporter(save_path)\n\n    if args.mtl_per_batch:\n        # NOTE: from easier to harder tasks\n        tasks = []\n        if 1 - args.bwd_weight - args.ctc_weight - args.sub1_weight - args.sub2_weight > 0:\n            tasks += [\'ys\']\n        if args.bwd_weight > 0:\n            tasks = [\'ys.bwd\'] + tasks\n        if args.ctc_weight > 0:\n            tasks = [\'ys.ctc\'] + tasks\n        if args.mbr_weight > 0:\n            tasks = [\'ys.mbr\'] + tasks\n        for sub in [\'sub1\', \'sub2\']:\n            if getattr(args, \'train_set_\' + sub):\n                if getattr(args, sub + \'_weight\') - getattr(args, \'ctc_weight_\' + sub) > 0:\n                    tasks = [\'ys_\' + sub] + tasks\n                if getattr(args, \'ctc_weight_\' + sub) > 0:\n                    tasks = [\'ys_\' + sub + \'.ctc\'] + tasks\n    else:\n        tasks = [\'all\']\n\n    start_time_train = time.time()\n    start_time_epoch = time.time()\n    start_time_step = time.time()\n    pbar_epoch = tqdm(total=len(train_set))\n    accum_n_steps = 0\n    n_steps = optimizer.n_steps * args.accum_grad_n_steps\n    epoch_detail_prev = 0\n    session_prev = None\n    while True:\n        # Compute loss in the training set\n        batch_train, is_new_epoch = train_set.next()\n        if args.discourse_aware and batch_train[\'sessions\'][0] != session_prev:\n            model.module.reset_session()\n        session_prev = batch_train[\'sessions\'][0]\n        accum_n_steps += 1\n\n        # Change mini-batch depending on task\n        for task in tasks:\n            loss, observation = model(batch_train, task,\n                                      teacher=teacher, teacher_lm=teacher_lm)\n            reporter.add(observation)\n            loss.backward()\n            loss.detach()  # Trancate the graph\n            if accum_n_steps >= args.accum_grad_n_steps:\n                if args.clip_grad_norm > 0:\n                    total_norm = torch.nn.utils.clip_grad_norm_(\n                        model.module.parameters(), args.clip_grad_norm)\n                    reporter.add_tensorboard_scalar(\'total_norm\', total_norm)\n                optimizer.step()\n                optimizer.zero_grad()\n                accum_n_steps = 0\n            loss_train = loss.item()\n            del loss\n\n        reporter.add_tensorboard_scalar(\'learning_rate\', optimizer.lr)\n        # NOTE: loss/acc/ppl are already added in the model\n        reporter.step()\n        pbar_epoch.update(len(batch_train[\'utt_ids\']))\n        n_steps += 1\n        # NOTE: n_steps is different from the step counter in Noam Optimizer\n\n        if n_steps % args.print_step == 0:\n            # Compute loss in the dev set\n            batch_dev = dev_set.next(batch_size=1 if \'transducer\' in args.dec_type else None)[0]\n            # Change mini-batch depending on task\n            for task in tasks:\n                loss, observation = model(batch_dev, task, is_eval=True)\n                reporter.add(observation, is_eval=True)\n                loss_dev = loss.item()\n                del loss\n            reporter.step(is_eval=True)\n\n            duration_step = time.time() - start_time_step\n            if args.input_type == \'speech\':\n                xlen = max(len(x) for x in batch_train[\'xs\'])\n                ylen = max(len(y) for y in batch_train[\'ys\'])\n            elif args.input_type == \'text\':\n                xlen = max(len(x) for x in batch_train[\'ys\'])\n                ylen = max(len(y) for y in batch_train[\'ys_sub1\'])\n            logger.info(""step:%d(ep:%.2f) loss:%.3f(%.3f)/lr:%.7f/bs:%d/xlen:%d/ylen:%d (%.2f min)"" %\n                        (n_steps, optimizer.n_epochs + train_set.epoch_detail,\n                         loss_train, loss_dev,\n                         optimizer.lr, len(batch_train[\'utt_ids\']),\n                         xlen, ylen, duration_step / 60))\n            start_time_step = time.time()\n\n        # Save fugures of loss and accuracy\n        if n_steps % (args.print_step * 10) == 0:\n            reporter.snapshot()\n            model.module.plot_attention()\n\n        # Ealuate model every 0.1 epoch during MBR training\n        if args.mbr_training:\n            if int(train_set.epoch_detail * 10) != int(epoch_detail_prev * 10):\n                # dev\n                evaluate([model.module], dev_set, recog_params, args,\n                         int(train_set.epoch_detail * 10) / 10, logger)\n                # Save the model\n                optimizer.save_checkpoint(model, save_path, remove_old=False,\n                                          epoch_detail=train_set.epoch_detail)\n            epoch_detail_prev = train_set.epoch_detail\n\n        # Save checkpoint and evaluate model per epoch\n        if is_new_epoch:\n            duration_epoch = time.time() - start_time_epoch\n            logger.info(\'========== EPOCH:%d (%.2f min) ==========\' %\n                        (optimizer.n_epochs + 1, duration_epoch / 60))\n\n            if optimizer.n_epochs + 1 < args.eval_start_epoch:\n                optimizer.epoch()  # lr decay\n                reporter.epoch()  # plot\n\n                # Save the model\n                optimizer.save_checkpoint(model, save_path, remove_old=not is_transformer)\n            else:\n                start_time_eval = time.time()\n                # dev\n                metric_dev = evaluate([model.module], dev_set, recog_params, args,\n                                      optimizer.n_epochs + 1, logger)\n                optimizer.epoch(metric_dev)  # lr decay\n                reporter.epoch(metric_dev, name=args.metric)  # plot\n\n                if optimizer.is_topk or is_transformer:\n                    # Save the model\n                    optimizer.save_checkpoint(model, save_path, remove_old=not is_transformer)\n\n                    # test\n                    if optimizer.is_topk:\n                        for eval_set in eval_sets:\n                            evaluate([model.module], eval_set, recog_params, args,\n                                     optimizer.n_epochs, logger)\n\n                duration_eval = time.time() - start_time_eval\n                logger.info(\'Evaluation time: %.2f min\' % (duration_eval / 60))\n\n                # Early stopping\n                if optimizer.is_early_stop:\n                    break\n\n                # Convert to fine-tuning stage\n                if optimizer.n_epochs == args.convert_to_sgd_epoch:\n                    optimizer.convert_to_sgd(model, args.lr, args.weight_decay,\n                                             decay_type=\'always\', decay_rate=0.5)\n\n            pbar_epoch = tqdm(total=len(train_set))\n            session_prev = None\n\n            if optimizer.n_epochs >= args.n_epochs:\n                break\n            # if args.ss_prob > 0:\n            #     model.module.scheduled_sampling_trigger()\n\n            start_time_step = time.time()\n            start_time_epoch = time.time()\n\n    duration_train = time.time() - start_time_train\n    logger.info(\'Total time: %.2f hour\' % (duration_train / 3600))\n\n    reporter.tf_writer.close()\n    pbar_epoch.close()\n\n    return save_path\n\n\ndef evaluate(models, dataset, recog_params, args, epoch, logger):\n    if args.metric == \'edit_distance\':\n        if args.unit in [\'word\', \'word_char\']:\n            metric = eval_word(models, dataset, recog_params, epoch=epoch)[0]\n            logger.info(\'WER (%s, ep:%d): %.2f %%\' % (dataset.set, epoch, metric))\n        elif args.unit == \'wp\':\n            metric, cer = eval_wordpiece(models, dataset, recog_params, epoch=epoch)\n            logger.info(\'WER (%s, ep:%d): %.2f %%\' % (dataset.set, epoch, metric))\n            logger.info(\'CER (%s, ep:%d): %.2f %%\' % (dataset.set, epoch, cer))\n        elif \'char\' in args.unit:\n            wer, cer = eval_char(models, dataset, recog_params, epoch=epoch)\n            logger.info(\'WER (%s, ep:%d): %.2f %%\' % (dataset.set, epoch, wer))\n            logger.info(\'CER (%s, ep:%d): %.2f %%\' % (dataset.set, epoch, cer))\n            if dataset.corpus in [\'aishell1\']:\n                metric = cer\n            else:\n                metric = wer\n        elif \'phone\' in args.unit:\n            metric = eval_phone(models, dataset, recog_params, epoch=epoch)\n            logger.info(\'PER (%s, ep:%d): %.2f %%\' % (dataset.set, epoch, metric))\n    elif args.metric == \'ppl\':\n        metric = eval_ppl(models, dataset, batch_size=args.batch_size)[0]\n        logger.info(\'PPL (%s, ep:%d): %.3f\' % (dataset.set, epoch, metric))\n    elif args.metric == \'loss\':\n        metric = eval_ppl(models, dataset, batch_size=args.batch_size)[1]\n        logger.info(\'Loss (%s, ep:%d): %.5f\' % (dataset.set, epoch, metric))\n    elif args.metric == \'accuracy\':\n        metric = eval_accuracy(models, dataset, batch_size=args.batch_size)\n        logger.info(\'Accuracy (%s, ep:%d): %.3f\' % (dataset.set, epoch, metric))\n    elif args.metric == \'bleu\':\n        metric = eval_wordpiece_bleu(models, dataset, recog_params, epoch=epoch)\n        logger.info(\'BLEU (%s, ep:%d): %.3f\' % (dataset.set, epoch, metric))\n    else:\n        raise NotImplementedError(args.metric)\n    return metric\n\n\nif __name__ == \'__main__\':\n    # Setting for profiling\n    pr = cProfile.Profile()\n    save_path = pr.runcall(main)\n    pr.dump_stats(os.path.join(save_path, \'train.profile\'))\n'"
neural_sp/bin/lm/__init__.py,0,b''
neural_sp/bin/lm/eval.py,0,"b'#! /usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n# Copyright 2019 Kyoto University (Hirofumi Inaguma)\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n\n""""""Evaluate the LM.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport logging\nimport os\nimport sys\nimport time\n\nfrom neural_sp.bin.args_lm import parse_args_eval\nfrom neural_sp.bin.train_utils import (\n    load_checkpoint,\n    set_logger\n)\nfrom neural_sp.datasets.lm import Dataset\nfrom neural_sp.evaluators.ppl import eval_ppl\nfrom neural_sp.models.lm.build import build_lm\n\nlogger = logging.getLogger(__name__)\n\n\ndef main():\n\n    # Load configuration\n    args, _, dir_name = parse_args_eval(sys.argv[1:])\n\n    # Setting for logging\n    if os.path.isfile(os.path.join(args.recog_dir, \'decode.log\')):\n        os.remove(os.path.join(args.recog_dir, \'decode.log\'))\n    set_logger(os.path.join(args.recog_dir, \'decode.log\'), stdout=args.recog_stdout)\n\n    ppl_avg = 0\n    for i, s in enumerate(args.recog_sets):\n        # Load dataset\n        dataset = Dataset(corpus=args.corpus,\n                          tsv_path=s,\n                          dict_path=os.path.join(dir_name, \'dict.txt\'),\n                          wp_model=os.path.join(dir_name, \'wp.model\'),\n                          unit=args.unit,\n                          batch_size=args.recog_batch_size,\n                          bptt=args.bptt,\n                          backward=args.backward,\n                          serialize=args.serialize,\n                          is_test=True)\n\n        if i == 0:\n            # Load the LM\n            model = build_lm(args)\n            load_checkpoint(model, args.recog_model[0])\n            epoch = int(args.recog_model[0].split(\'-\')[-1])\n            # NOTE: model averaging is not helpful for LM\n\n            logger.info(\'epoch: %d\' % epoch)\n            logger.info(\'batch size: %d\' % args.recog_batch_size)\n            logger.info(\'BPTT: %d\' % (args.bptt))\n            logger.info(\'cache size: %d\' % (args.recog_n_caches))\n            logger.info(\'cache theta: %.3f\' % (args.recog_cache_theta))\n            logger.info(\'cache lambda: %.3f\' % (args.recog_cache_lambda))\n            logger.info(\'model average (Transformer): %d\' % (args.recog_n_average))\n            model.cache_theta = args.recog_cache_theta\n            model.cache_lambda = args.recog_cache_lambda\n\n            # GPU setting\n            if args.recog_n_gpus > 0:\n                model.cuda()\n\n        start_time = time.time()\n\n        ppl, _ = eval_ppl([model], dataset, batch_size=1, bptt=args.bptt,\n                          n_caches=args.recog_n_caches, progressbar=True)\n        ppl_avg += ppl\n        print(\'PPL (%s): %.2f\' % (dataset.set, ppl))\n        logger.info(\'Elasped time: %.2f [sec]:\' % (time.time() - start_time))\n\n    logger.info(\'PPL (avg.): %.2f\\n\' % (ppl_avg / len(args.recog_sets)))\n\n\nif __name__ == \'__main__\':\n    main()\n'"
neural_sp/bin/lm/plot_cache.py,0,"b'#! /usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n# Copyright 2019 Kyoto University (Hirofumi Inaguma)\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n\n""""""Plot cache distributions of RNNLM.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport logging\nimport numpy as np\nimport os\nimport shutil\nimport sys\n\nfrom neural_sp.bin.args_lm import parse_args_eval\nfrom neural_sp.bin.eval_utils import average_checkpoints\nfrom neural_sp.bin.plot_utils import plot_cache_weights\nfrom neural_sp.bin.train_utils import (\n    load_checkpoint,\n    set_logger\n)\nfrom neural_sp.datasets.lm import Dataset\nfrom neural_sp.models.lm.build import build_lm\nfrom neural_sp.utils import mkdir_join\n\nlogger = logging.getLogger(__name__)\n\n\ndef main():\n\n    # Load configuration\n    args, _, dir_name = parse_args_eval(sys.argv[1:])\n\n    # Setting for logging\n    if os.path.isfile(os.path.join(args.recog_dir, \'plot.log\')):\n        os.remove(os.path.join(args.recog_dir, \'plot.log\'))\n    set_logger(os.path.join(args.recog_dir, \'plot.log\'), stdout=args.recog_stdout)\n\n    for i, s in enumerate(args.recog_sets):\n        # Load dataset\n        dataset = Dataset(corpus=args.corpus,\n                          tsv_path=s,\n                          dict_path=os.path.join(dir_name, \'dict.txt\'),\n                          wp_model=os.path.join(dir_name, \'wp.model\'),\n                          unit=args.unit,\n                          batch_size=args.recog_batch_size,\n                          bptt=args.bptt,\n                          backward=args.backward,\n                          serialize=args.serialize,\n                          is_test=True)\n\n        if i == 0:\n            # Load the LM\n            model = build_lm(args, dir_name)\n            topk_list = load_checkpoint(model, args.recog_model[0])\n            epoch = int(args.recog_model[0].split(\'-\')[-1])\n\n            # Model averaging for Transformer\n            if args.lm_type == \'transformer\':\n                model = average_checkpoints(model, args.recog_model[0],\n                                            n_average=args.recog_n_average,\n                                            topk_list=topk_list)\n\n            logger.info(\'epoch: %d\' % (epoch - 1))\n            logger.info(\'batch size: %d\' % args.recog_batch_size)\n            # logger.info(\'recog unit: %s\' % args.recog_unit)\n            # logger.info(\'ensemble: %d\' % (len(ensemble_models)))\n            logger.info(\'BPTT: %d\' % (args.bptt))\n            logger.info(\'cache size: %d\' % (args.recog_n_caches))\n            logger.info(\'cache theta: %.3f\' % (args.recog_cache_theta))\n            logger.info(\'cache lambda: %.3f\' % (args.recog_cache_lambda))\n            model.cache_theta = args.recog_cache_theta\n            model.cache_lambda = args.recog_cache_lambda\n\n            # GPU setting\n            model.cuda()\n\n        assert args.recog_n_caches > 0\n        save_path = mkdir_join(args.recog_dir, \'cache\')\n\n        # Clean directory\n        if save_path is not None and os.path.isdir(save_path):\n            shutil.rmtree(save_path)\n            os.mkdir(save_path)\n\n        hidden = None\n        fig_count = 0\n        toknen_count = 0\n        n_tokens = args.recog_n_caches\n        while True:\n            ys, is_new_epoch = dataset.next()\n\n            for t in range(ys.shape[1] - 1):\n                loss, hidden = model(ys[:, t:t + 2], hidden, is_eval=True, n_caches=args.recog_n_caches)[:2]\n\n                if len(model.cache_attn) > 0:\n                    if toknen_count == n_tokens:\n                        tokens_keys = dataset.idx2token[0](model.cache_ids[:args.recog_n_caches], return_list=True)\n                        tokens_query = dataset.idx2token[0](model.cache_ids[-n_tokens:], return_list=True)\n\n                        # Slide attention matrix\n                        n_keys = len(tokens_keys)\n                        n_queries = len(tokens_query)\n                        cache_probs = np.zeros((n_keys, n_queries))  # `[n_keys, n_queries]`\n                        mask = np.zeros((n_keys, n_queries))\n                        for i, aw in enumerate(model.cache_attn[-n_tokens:]):\n                            cache_probs[:(n_keys - n_queries + i + 1), i] = aw[0, -(n_keys - n_queries + i + 1):]\n                            mask[(n_keys - n_queries + i + 1):, i] = 1\n\n                        plot_cache_weights(\n                            cache_probs,\n                            keys=tokens_keys,\n                            queries=tokens_query,\n                            save_path=mkdir_join(save_path, str(fig_count) + \'.png\'),\n                            figsize=(40, 16),\n                            mask=mask)\n                        toknen_count = 0\n                        fig_count += 1\n                    else:\n                        toknen_count += 1\n\n            if is_new_epoch:\n                break\n\n\nif __name__ == \'__main__\':\n    main()\n'"
neural_sp/bin/lm/train.py,3,"b'#! /usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n# Copyright 2018 Kyoto University (Hirofumi Inaguma)\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n\n""""""Train the LM.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport cProfile\nimport logging\nimport os\nfrom setproctitle import setproctitle\nimport shutil\nimport sys\nimport time\nimport torch\nfrom tqdm import tqdm\n\nfrom neural_sp.bin.args_lm import parse_args_train\nfrom neural_sp.bin.model_name import set_lm_name\nfrom neural_sp.bin.train_utils import (\n    load_checkpoint,\n    load_config,\n    save_config,\n    set_logger,\n    set_save_path\n)\nfrom neural_sp.datasets.lm import Dataset\nfrom neural_sp.evaluators.ppl import eval_ppl\nfrom neural_sp.models.data_parallel import CustomDataParallel\nfrom neural_sp.models.lm.build import build_lm\nfrom neural_sp.trainers.lr_scheduler import LRScheduler\nfrom neural_sp.trainers.optimizer import set_optimizer\nfrom neural_sp.trainers.reporter import Reporter\nfrom neural_sp.utils import mkdir_join\n\ntorch.manual_seed(1)\ntorch.cuda.manual_seed_all(1)\n\nlogger = logging.getLogger(__name__)\n\n\ndef main():\n\n    args = parse_args_train(sys.argv[1:])\n\n    # Load a conf file\n    if args.resume:\n        conf = load_config(os.path.join(os.path.dirname(args.resume), \'conf.yml\'))\n        for k, v in conf.items():\n            if k != \'resume\':\n                setattr(args, k, v)\n\n    # Load dataset\n    train_set = Dataset(corpus=args.corpus,\n                        tsv_path=args.train_set,\n                        dict_path=args.dict,\n                        nlsyms=args.nlsyms,\n                        unit=args.unit,\n                        wp_model=args.wp_model,\n                        batch_size=args.batch_size * args.n_gpus,\n                        n_epochs=args.n_epochs,\n                        min_n_tokens=args.min_n_tokens,\n                        bptt=args.bptt,\n                        shuffle=args.shuffle,\n                        backward=args.backward,\n                        serialize=args.serialize)\n    dev_set = Dataset(corpus=args.corpus,\n                      tsv_path=args.dev_set,\n                      dict_path=args.dict,\n                      nlsyms=args.nlsyms,\n                      unit=args.unit,\n                      wp_model=args.wp_model,\n                      batch_size=args.batch_size * args.n_gpus,\n                      bptt=args.bptt,\n                      backward=args.backward,\n                      serialize=args.serialize)\n    eval_sets = [Dataset(corpus=args.corpus,\n                         tsv_path=s,\n                         dict_path=args.dict,\n                         nlsyms=args.nlsyms,\n                         unit=args.unit,\n                         wp_model=args.wp_model,\n                         batch_size=1,\n                         bptt=args.bptt,\n                         backward=args.backward,\n                         serialize=args.serialize) for s in args.eval_sets]\n\n    args.vocab = train_set.vocab\n\n    # Set save path\n    if args.resume:\n        save_path = os.path.dirname(args.resume)\n        dir_name = os.path.basename(save_path)\n    else:\n        dir_name = set_lm_name(args)\n        save_path = mkdir_join(args.model_save_dir, \'_\'.join(\n            os.path.basename(args.train_set).split(\'.\')[:-1]), dir_name)\n        save_path = set_save_path(save_path)  # avoid overwriting\n\n    # Set logger\n    set_logger(os.path.join(save_path, \'train.log\'), stdout=args.stdout)\n\n    # Model setting\n    model = build_lm(args, save_path)\n\n    if not args.resume:\n        # Save the conf file as a yaml file\n        save_config(vars(args), os.path.join(save_path, \'conf.yml\'))\n\n        # Save the nlsyms, dictionar, and wp_model\n        if args.nlsyms:\n            shutil.copy(args.nlsyms, os.path.join(save_path, \'nlsyms.txt\'))\n        shutil.copy(args.dict, os.path.join(save_path, \'dict.txt\'))\n        if args.unit == \'wp\':\n            shutil.copy(args.wp_model, os.path.join(save_path, \'wp.model\'))\n\n        for k, v in sorted(vars(args).items(), key=lambda x: x[0]):\n            logger.info(\'%s: %s\' % (k, str(v)))\n\n        # Count total parameters\n        for n in sorted(list(model.num_params_dict.keys())):\n            n_params = model.num_params_dict[n]\n            logger.info(""%s %d"" % (n, n_params))\n        logger.info(""Total %.2f M parameters"" % (model.total_parameters / 1000000))\n        logger.info(model)\n\n    # Set optimizer\n    resume_epoch = 0\n    if args.resume:\n        epoch = int(args.resume.split(\'-\')[-1])\n        optimizer = set_optimizer(model, \'sgd\' if epoch > args.convert_to_sgd_epoch else args.optimizer,\n                                  args.lr, args.weight_decay)\n    else:\n        optimizer = set_optimizer(model, args.optimizer, args.lr, args.weight_decay)\n\n    # Wrap optimizer by learning rate scheduler\n    is_transformer = args.lm_type in [\'transformer\', \'transformer_xl\']\n    optimizer = LRScheduler(optimizer, args.lr,\n                            decay_type=args.lr_decay_type,\n                            decay_start_epoch=args.lr_decay_start_epoch,\n                            decay_rate=args.lr_decay_rate,\n                            decay_patient_n_epochs=args.lr_decay_patient_n_epochs,\n                            early_stop_patient_n_epochs=args.early_stop_patient_n_epochs,\n                            warmup_start_lr=args.warmup_start_lr,\n                            warmup_n_steps=args.warmup_n_steps,\n                            model_size=getattr(args, \'transformer_d_model\', 0),\n                            factor=args.lr_factor,\n                            noam=is_transformer,\n                            save_checkpoints_topk=1)\n\n    if args.resume:\n        # Restore the last saved model\n        load_checkpoint(model, args.resume, optimizer)\n\n        # Resume between convert_to_sgd_epoch -1 and convert_to_sgd_epoch\n        if resume_epoch == args.convert_to_sgd_epoch:\n            optimizer.convert_to_sgd(model, args.lr, args.weight_decay,\n                                     decay_type=\'always\', decay_rate=0.5)\n\n    # GPU setting\n    if args.n_gpus >= 1:\n        model.cudnn_setting(deterministic=not (is_transformer or args.cudnn_benchmark),\n                            benchmark=args.cudnn_benchmark)\n        model = CustomDataParallel(model, device_ids=list(range(0, args.n_gpus)))\n        model.cuda()\n\n    # Set process name\n    logger.info(\'PID: %s\' % os.getpid())\n    logger.info(\'USERNAME: %s\' % os.uname()[1])\n    setproctitle(args.job_name if args.job_name else dir_name)\n\n    # Set reporter\n    reporter = Reporter(save_path)\n\n    hidden = None\n    start_time_train = time.time()\n    start_time_epoch = time.time()\n    start_time_step = time.time()\n    pbar_epoch = tqdm(total=len(train_set))\n    accum_n_steps = 0\n    n_steps = optimizer.n_steps * args.accum_grad_n_steps\n    while True:\n        # Compute loss in the training set\n        ys_train, is_new_epoch = train_set.next()\n        accum_n_steps += 1\n\n        loss, hidden, observation = model(ys_train, hidden)\n        reporter.add(observation)\n        loss.backward()\n        loss.detach()  # Trancate the graph\n        if args.accum_grad_n_steps == 1 or accum_n_steps >= args.accum_grad_n_steps:\n            if args.clip_grad_norm > 0:\n                total_norm = torch.nn.utils.clip_grad_norm_(\n                    model.module.parameters(), args.clip_grad_norm)\n                reporter.add_tensorboard_scalar(\'total_norm\', total_norm)\n            optimizer.step()\n            optimizer.zero_grad()\n            accum_n_steps = 0\n        loss_train = loss.item()\n        del loss\n        hidden = model.module.repackage_state(hidden)\n        reporter.add_tensorboard_scalar(\'learning_rate\', optimizer.lr)\n        # NOTE: loss/acc/ppl are already added in the model\n        reporter.step()\n        pbar_epoch.update(ys_train.shape[0] * (ys_train.shape[1] - 1))\n        n_steps += 1\n\n        if n_steps % args.print_step == 0:\n            # Compute loss in the dev set\n            ys_dev = dev_set.next(bptt=args.bptt)[0]\n            loss, _, observation = model(ys_dev, None, is_eval=True)\n            reporter.add(observation, is_eval=True)\n            loss_dev = loss.item()\n            del loss\n            reporter.step(is_eval=True)\n\n            duration_step = time.time() - start_time_step\n            logger.info(""step:%d(ep:%.2f) loss:%.3f(%.3f)/lr:%.5f/bs:%d (%.2f min)"" %\n                        (n_steps, optimizer.n_epochs + train_set.epoch_detail,\n                         loss_train, loss_dev,\n                         optimizer.lr, ys_train.shape[0], duration_step / 60))\n            start_time_step = time.time()\n\n        # Save fugures of loss and accuracy\n        if n_steps % (args.print_step * 10) == 0:\n            reporter.snapshot()\n            model.module.plot_attention()\n\n        # Save checkpoint and evaluate model per epoch\n        if is_new_epoch:\n            duration_epoch = time.time() - start_time_epoch\n            logger.info(\'========== EPOCH:%d (%.2f min) ==========\' %\n                        (optimizer.n_epochs + 1, duration_epoch / 60))\n\n            if optimizer.n_epochs + 1 < args.eval_start_epoch:\n                optimizer.epoch()  # lr decay\n                reporter.epoch()  # plot\n\n                # Save the model\n                optimizer.save_checkpoint(model, save_path, remove_old=True)\n            else:\n                start_time_eval = time.time()\n                # dev\n                model.module.reset_length(args.bptt)\n                ppl_dev, _ = eval_ppl([model.module], dev_set,\n                                      batch_size=1, bptt=args.bptt)\n                model.module.reset_length(args.bptt)\n                optimizer.epoch(ppl_dev)  # lr decay\n                reporter.epoch(ppl_dev, name=\'perplexity\')  # plot\n                logger.info(\'PPL (%s, ep:%d): %.2f\' %\n                            (dev_set.set, optimizer.n_epochs, ppl_dev))\n\n                if optimizer.is_topk:\n                    # Save the model\n                    optimizer.save_checkpoint(model, save_path, remove_old=True)\n\n                    # test\n                    ppl_test_avg = 0.\n                    for eval_set in eval_sets:\n                        model.module.reset_length(args.bptt)\n                        ppl_test, _ = eval_ppl([model.module], eval_set,\n                                               batch_size=1, bptt=args.bptt)\n                        model.module.reset_length(args.bptt)\n                        logger.info(\'PPL (%s, ep:%d): %.2f\' %\n                                    (eval_set.set, optimizer.n_epochs, ppl_test))\n                        ppl_test_avg += ppl_test\n                    if len(eval_sets) > 0:\n                        logger.info(\'PPL (avg., ep:%d): %.2f\' %\n                                    (optimizer.n_epochs, ppl_test_avg / len(eval_sets)))\n\n                duration_eval = time.time() - start_time_eval\n                logger.info(\'Evaluation time: %.2f min\' % (duration_eval / 60))\n\n                # Early stopping\n                if optimizer.is_early_stop:\n                    break\n\n                # Convert to fine-tuning stage\n                if optimizer.n_epochs == args.convert_to_sgd_epoch:\n                    optimizer.convert_to_sgd(model, args.lr, args.weight_decay,\n                                             decay_type=\'always\', decay_rate=0.5)\n\n            pbar_epoch = tqdm(total=len(train_set))\n\n            if optimizer.n_epochs == args.n_epochs:\n                break\n\n            start_time_step = time.time()\n            start_time_epoch = time.time()\n\n    duration_train = time.time() - start_time_train\n    logger.info(\'Total time: %.2f hour\' % (duration_train / 3600))\n\n    reporter.tf_writer.close()\n    pbar_epoch.close()\n\n    return save_path\n\n\nif __name__ == \'__main__\':\n    # Setting for profiling\n    pr = cProfile.Profile()\n    save_path = pr.runcall(main)\n    pr.dump_stats(os.path.join(save_path, \'train.profile\'))\n'"
neural_sp/datasets/token_converter/__init__.py,0,b''
neural_sp/datasets/token_converter/character.py,0,"b'#! /usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n# Copyright 2018 Kyoto University (Hirofumi Inaguma)\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n\n""""""Character-level token <-> index converter.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport codecs\nimport os\n\n\nclass Char2idx(object):\n    """"""Class for converting character sequence into indices.\n\n    Args:\n        dict_path (str): path to a dictionary file\n        remove_list (list): characters to ignore\n\n    """"""\n\n    def __init__(self, dict_path, nlsyms=False, remove_space=False, remove_list=[]):\n        self.remove_space = remove_space\n        self.remove_list = remove_list\n\n        # Load a dictionary file\n        self.token2idx = {\'<blank>\': 0}\n        with codecs.open(dict_path, \'r\', \'utf-8\') as f:\n            for line in f:\n                c, idx = line.strip().split(\' \')\n                if c in remove_list:\n                    continue\n                self.token2idx[c] = int(idx)\n        self.vocab = len(self.token2idx.keys())\n\n        self.nlsyms_list = []\n        if nlsyms and os.path.isfile(nlsyms):\n            with codecs.open(nlsyms, \'r\', \'utf-8\') as f:\n                for line in f:\n                    self.nlsyms_list.append(line.strip())\n\n    def __call__(self, text):\n        """"""Convert character sequence into indices.\n\n        Args:\n            text (str): character sequence\n        Returns:\n            token_ids (list): character indices\n\n        """"""\n        token_ids = []\n        words = text.replace(\' \', \'<space>\').split(\'<space>\')\n        for i,  w in enumerate(words):\n            if w in self.nlsyms_list:\n                token_ids.append(self.token2idx[w])\n            else:\n                for c in list(w):\n                    if c in self.token2idx.keys():\n                        token_ids.append(self.token2idx[c])\n                    else:\n                        # Replace with <unk>\n                        token_ids.append(self.token2idx[\'<unk>\'])\n                        # NOTE: OOV handling is prepared for Japanese and Chinese\n\n            if not self.remove_space:\n                if i < len(words) - 1:\n                    token_ids.append(self.token2idx[\'<space>\'])\n        return token_ids\n\n\nclass Idx2char(object):\n    """"""Class for converting indices into character sequence.\n\n    Args:\n        dict_path (str): path to a dictionary file\n        remove_list (list): characters to ignore\n\n    """"""\n\n    def __init__(self, dict_path,  remove_list=[]):\n        self.remove_list = remove_list\n\n        # Load a dictionary file\n        self.idx2token = {0: \'<blank>\'}\n        with codecs.open(dict_path, \'r\', \'utf-8\') as f:\n            for line in f:\n                c, idx = line.strip().split(\' \')\n                if c in remove_list:\n                    continue\n                self.idx2token[int(idx)] = c\n        self.vocab = len(self.idx2token.keys())\n        # for synchronous bidirectional attention\n        self.idx2token[self.vocab] = \'<l2r>\'\n        self.idx2token[self.vocab + 1] = \'<r2l>\'\n        self.idx2token[self.vocab + 2] = \'<null>\'\n\n    def __call__(self, token_ids, return_list=False):\n        """"""Convert indices into character sequence.\n\n        Args:\n            token_ids (np.ndarray or list): character indices\n            return_list (bool): if True, return list of characters\n        Returns:\n            text (str): character sequence\n                or\n            characters (list): list of characters\n\n        """"""\n        characters = list(map(lambda c: self.idx2token[c], token_ids))\n        if return_list:\n            return characters\n        return \'\'.join(characters).replace(\'<space>\', \' \')\n'"
neural_sp/datasets/token_converter/phone.py,0,"b'#! /usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n# Copyright 2018 Kyoto University (Hirofumi Inaguma)\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n\n""""""Phone-level token <-> index converter.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport codecs\n\n\nclass Phone2idx(object):\n    """"""Class for converting phone sequence to indices.\n\n    Args:\n        dict_path (str): path to a dictionary file\n        remove_list (list): phones to ingore\n\n    """"""\n\n    def __init__(self, dict_path, remove_list=[]):\n        # Load a dictionary file\n        self.token2idx = {\'<blank>\': 0}\n        with codecs.open(dict_path, \'r\', \'utf-8\') as f:\n            for line in f:\n                p, idx = line.strip().split(\' \')\n                if p in remove_list:\n                    continue\n                self.token2idx[p] = int(idx)\n        self.vocab = len(self.token2idx.keys())\n\n    def __call__(self, text):\n        """"""Convert phone sequence to indices.\n\n        Args:\n            text (str): phone sequence divided by spaces\n        Returns:\n            token_ids (list): phone indices\n\n        """"""\n        phones = text.split(\' \')\n        token_ids = list(map(lambda p: self.token2idx[p], phones))\n        return token_ids\n\n\nclass Idx2phone(object):\n    """"""Class for converting indices to phone sequence.\n\n    Args:\n        dict_path (str): path to a dictionary file\n        remove_list (list): phones to ingore\n\n    """"""\n\n    def __init__(self, dict_path, remove_list=[]):\n        # Load a dictionary file\n        self.idx2token = {0: \'<blank>\'}\n        with codecs.open(dict_path, \'r\', \'utf-8\') as f:\n            for line in f:\n                p, idx = line.strip().split(\' \')\n                if p in remove_list:\n                    continue\n                self.idx2token[int(idx)] = p\n        self.vocab = len(self.idx2token.keys())\n        # for synchronous bidirectional attention\n        self.idx2token[self.vocab] = \'<l2r>\'\n        self.idx2token[self.vocab + 1] = \'<r2l>\'\n        self.idx2token[self.vocab + 2] = \'<null>\'\n\n    def __call__(self, token_ids, return_list=False):\n        """"""Convert indices to phone sequence.\n\n        Args:\n            token_ids (list): phone indices\n            return_list (bool): if True, return list of phones\n        Returns:\n            text (str): phone sequence divided by spaces\n                or\n            phones (list): list of phones\n\n        """"""\n        phones = list(map(lambda i: self.idx2token[i], token_ids))\n        if return_list:\n            return phones\n        return \' \'.join(phones)\n'"
neural_sp/datasets/token_converter/word.py,0,"b'#! /usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n# Copyright 2018 Kyoto University (Hirofumi Inaguma)\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n\n""""""Word-level token <-> index converter.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport codecs\n\n\nclass Word2idx(object):\n    """"""Class for converting word sequence into indices.\n\n    Args:\n        dict_path (str): path to a dictionary file\n        word_char_mix (bool):\n\n    """"""\n\n    def __init__(self, dict_path, word_char_mix=False):\n        self.word_char_mix = word_char_mix\n\n        # Load a dictionary file\n        self.token2idx = {\'<blank>\': 0}\n        with codecs.open(dict_path, \'r\', \'utf-8\') as f:\n            for line in f:\n                w, idx = line.strip().split(\' \')\n                self.token2idx[w] = int(idx)\n        self.vocab = len(self.token2idx.keys())\n\n    def __call__(self, text):\n        """"""Convert word sequence into indices.\n\n        Args:\n            text (str): word sequence\n        Returns:\n            token_ids (list): word indices\n\n        """"""\n        token_ids = []\n        words = text.split(\' \')\n        for w in words:\n            if w in self.token2idx.keys():\n                token_ids.append(self.token2idx[w])\n            else:\n                # Replace with <unk>\n                if self.word_char_mix:\n                    for c in list(w):\n                        if c in self.token2idx.keys():\n                            token_ids.append(self.token2idx[c])\n                        else:\n                            token_ids.append(self.token2idx[\'<unk>\'])\n                else:\n                    token_ids.append(self.token2idx[\'<unk>\'])\n        return token_ids\n\n\nclass Idx2word(object):\n    """"""Class for converting indices into word sequence.\n\n    Args:\n        dict_path (str): path to a dictionary file\n\n    """"""\n\n    def __init__(self, dict_path):\n        # Load a dictionary file\n        self.idx2token = {0: \'<blank>\'}\n        with codecs.open(dict_path, \'r\', \'utf-8\') as f:\n            for line in f:\n                w, idx = line.strip().split(\' \')\n                self.idx2token[int(idx)] = w\n        self.vocab = len(self.idx2token.keys())\n        # for synchronous bidirectional attention\n        self.idx2token[self.vocab] = \'<l2r>\'\n        self.idx2token[self.vocab + 1] = \'<r2l>\'\n        self.idx2token[self.vocab + 2] = \'<null>\'\n\n    def __call__(self, token_ids, return_list=False):\n        """"""Convert indices into word sequence.\n\n        Args:\n            token_ids (np.ndarray or list): word indices\n            return_list (bool): if True, return list of words\n        Returns:\n            text (str): word sequence\n                or\n            words (list): list of words\n\n        """"""\n        words = list(map(lambda w: self.idx2token[w], token_ids))\n        if return_list:\n            return words\n        return \' \'.join(words)\n\n\nclass Char2word(object):\n    """"""Class for converting character indices into the signle word index.\n\n    Args:\n        dict_path_word (str): path to a word dictionary file\n        dict_path_char (str): path to a character dictionary file\n\n    """"""\n\n    def __init__(self, dict_path_word, dict_path_char):\n        # Load a word dictionary file\n        self.word2idx = {}\n        with codecs.open(dict_path_word, \'r\', \'utf-8\') as f:\n            for line in f:\n                w, idx = line.strip().split(\' \')\n                self.word2idx[w] = int(idx)\n\n        # Load a character dictionary file\n        self.idx2char = {}\n        with codecs.open(dict_path_char, \'r\', \'utf-8\') as f:\n            for line in f:\n                c, idx = line.strip().split(\' \')\n                self.idx2char[int(idx)] = c\n\n    def __call__(self, char_ids):\n        """"""Convert character indices into the single word index.\n\n        Args:\n            char_ids (np.ndarray or list): character indices corresponding to a single word\n        Returns:\n            word_idx (int): a single word index\n\n        """"""\n        # char ids -> text\n        single_word = \'\'.join(list(map(lambda i: self.idx2char[i], char_ids)))\n\n        # text -> word idx\n        if single_word in self.word2idx.keys():\n            word_idx = self.word2idx[single_word]\n        else:\n            word_idx = self.word2idx[\'<unk>\']\n        return word_idx\n\n\nclass Word2char(object):\n    """"""Class for converting a word index into the character indices.\n\n    Args:\n        dict_path_word (str): path to a dictionary file of words\n        dict_path_char (str): path to a dictionary file of characters\n\n    """"""\n\n    def __init__(self, dict_path_word, dict_path_char):\n        # Load a word dictionary file\n        self.idx2word = {}\n        with codecs.open(dict_path_word, \'r\', \'utf-8\') as f:\n            for line in f:\n                w, idx = line.strip().split(\' \')\n                self.idx2word[int(idx)] = w\n\n        # Load a character dictionary file\n        self.char2idx = {}\n        with codecs.open(dict_path_char, \'r\', \'utf-8\') as f:\n            for line in f:\n                c, idx = line.strip().split(\' \')\n                self.char2idx[c] = int(idx)\n\n    def __call__(self, word_idx):\n        """"""Convert a word index into character indices.\n\n        Args:\n            word_idx (int): a single word index\n        Returns:\n            char_indices (list): character indices\n\n        """"""\n        # word idx -> text\n        single_word = self.idx2word[word_idx]\n\n        # text -> char ids\n        char_indices = list(map(lambda c: self.char2idx[c], list(single_word)))\n        return char_indices\n'"
neural_sp/datasets/token_converter/wordpiece.py,0,"b'#! /usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n# Copyright 2018 Kyoto University (Hirofumi Inaguma)\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n\n""""""Wordpiece-level token <-> index converter.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport codecs\nimport sentencepiece as spm\n\n\nclass Wp2idx(object):\n    """"""Class for converting word-piece sequence into indices.\n\n    Args:\n        dict_path (str): path to a dictionary file\n        wp_model ():\n\n    """"""\n\n    def __init__(self, dict_path, wp_model):\n        # Load a dictionary file\n        self.token2idx = {\'<blank>\': 0}\n        with codecs.open(dict_path, \'r\', \'utf-8\') as f:\n            for line in f:\n                wp, idx = line.strip().split(\' \')\n                self.token2idx[wp] = int(idx)\n        self.vocab = len(self.token2idx.keys())\n\n        self.sp = spm.SentencePieceProcessor()\n        self.sp.Load(wp_model)\n\n    def __call__(self, text):\n        """"""Convert word-piece sequence into indices.\n\n        Args:\n            text (str): word-piece sequence\n        Returns:\n            token_ids (list): word-piece indices\n\n        """"""\n        # Remove space before the first special symbol\n        wps = self.sp.EncodeAsPieces(text)\n        if wps[0] == \'\xe2\x96\x81\' and wps[1][0] == \'<\':\n            wps = wps[1:]\n\n        token_ids = []\n        for wp in wps:\n            if wp in self.token2idx.keys():\n                token_ids.append(self.token2idx[wp])\n            else:\n                # Replace with <unk>\n                token_ids.append(self.token2idx[\'<unk>\'])\n        return token_ids\n\n\nclass Idx2wp(object):\n    """"""Class for converting indices into word-piece sequence.\n\n    Args:\n        dict_path (str): path to a dictionary file\n\n    """"""\n\n    def __init__(self, dict_path, wp_model):\n        # Load a dictionary file\n        self.idx2token = {0: \'<blank>\'}\n        with codecs.open(dict_path, \'r\', \'utf-8\') as f:\n            for line in f:\n                wp, idx = line.strip().split(\' \')\n                self.idx2token[int(idx)] = wp\n        self.vocab = len(self.idx2token.keys())\n        # for synchronous bidirectional attention\n        self.idx2token[self.vocab] = \'<l2r>\'\n        self.idx2token[self.vocab + 1] = \'<r2l>\'\n        self.idx2token[self.vocab + 2] = \'<null>\'\n\n        self.sp = spm.SentencePieceProcessor()\n        self.sp.Load(wp_model)\n\n    def __call__(self, token_ids, return_list=False):\n        """"""Convert indices into word-piece sequence.\n\n        Args:\n            token_ids (np.ndarray or list): word-piece indices\n            return_list (bool): if True, return list of words\n        Returns:\n            text (str): word-piece sequence\n                or\n            wordpieces (list): list of words\n\n        """"""\n        if len(token_ids) == 0:\n            return \'\'\n        wordpieces = list(map(lambda wp: self.idx2token[wp], token_ids))\n        if return_list:\n            return wordpieces\n        return self.sp.DecodePieces(wordpieces)\n'"
neural_sp/models/lm/__init__.py,0,b''
neural_sp/models/lm/build.py,0,"b'#! /usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n# Copyright 2019 Kyoto University (Hirofumi Inaguma)\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n\n""""""Select a language model""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\n\ndef build_lm(args, save_path=None, wordlm=False, lm_dict_path=None, asr_dict_path=None):\n    """"""Select LM class.\n\n    Args:\n        args ():\n        save_path (str):\n        wordlm (bool):\n        lm_dict_path (dict):\n        asr_dict_path (dict):\n    Returns:\n        lm ():\n\n    """"""\n    if \'gated_conv\' in args.lm_type:\n        from neural_sp.models.lm.gated_convlm import GatedConvLM\n        lm = GatedConvLM(args, save_path)\n    elif args.lm_type == \'transformer\':\n        from neural_sp.models.lm.transformerlm import TransformerLM\n        lm = TransformerLM(args, save_path)\n    elif args.lm_type == \'transformer_xl\':\n        from neural_sp.models.lm.transformer_xl import TransformerXL\n        lm = TransformerXL(args, save_path)\n    else:\n        from neural_sp.models.lm.rnnlm import RNNLM\n        lm = RNNLM(args, save_path)\n    return lm\n'"
neural_sp/models/lm/gated_convlm.py,1,"b'#! /usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n# Copyright 2019 Kyoto University (Hirofumi Inaguma)\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n\n""""""Gated convolutional neural network language model with Gated Linear Units (GLU).""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom collections import OrderedDict\nimport logging\nimport torch.nn as nn\n\nfrom neural_sp.models.lm.lm_base import LMBase\nfrom neural_sp.models.modules.glu import ConvGLUBlock\n\nlogger = logging.getLogger(__name__)\n\n\nclass GatedConvLM(LMBase):\n    """"""Gated convolutional neural network language model with Gated Linear Units (GLU).""""""\n\n    def __init__(self, args, save_path=None):\n\n        super(LMBase, self).__init__()\n        logger.info(self.__class__.__name__)\n\n        self.lm_type = args.lm_type\n        self.save_path = save_path\n\n        self.emb_dim = args.emb_dim\n        self.n_units = args.n_units\n        self.n_layers = args.n_layers\n        self.lsm_prob = args.lsm_prob\n\n        self.vocab = args.vocab\n        self.eos = 2\n        self.pad = 3\n        # NOTE: reserved in advance\n\n        # for cache\n        self.cache_theta = 0.2  # smoothing parameter\n        self.cache_lambda = 0.2  # cache weight\n        self.cache_ids = []\n        self.cache_keys = []\n        self.cache_attn = []\n\n        self.embed = nn.Embedding(self.vocab, args.emb_dim, padding_idx=self.pad)\n        self.dropout_embed = nn.Dropout(p=args.dropout_in)\n\n        model_size = args.lm_type.replace(\'gated_conv_\', \'\')\n\n        blocks = OrderedDict()\n        dropout = args.dropout_hidden\n        if model_size == \'custom\':\n            blocks[\'conv1\'] = ConvGLUBlock(args.kernel_size, args.emb_dim, args.n_units,\n                                           bottlececk_dim=args.n_projs,\n                                           dropout=dropout)\n            for l in range(args.n_layers - 1):\n                blocks[\'conv%d\' % (l + 2)] = ConvGLUBlock(args.kernel_size, args.n_units, args.n_units,\n                                                          bottlececk_dim=args.n_projs,\n                                                          dropout=dropout)\n            last_dim = args.n_units\n\n        elif model_size == \'8\':\n            blocks[\'conv1\'] = ConvGLUBlock(4, args.emb_dim, 900, dropout=dropout)\n            for i in range(1, 8, 1):\n                blocks[\'conv2-%d\' % i] = ConvGLUBlock(4, 900, 900, dropout=dropout)\n            last_dim = 900\n\n        elif model_size == \'8B\':\n            blocks[\'conv1\'] = ConvGLUBlock(1, args.emb_dim, 512, dropout=dropout)\n            for i in range(1, 4, 1):\n                blocks[\'conv2-%d\' % i] = ConvGLUBlock(5, 512, 512,\n                                                      bottlececk_dim=128,\n                                                      dropout=dropout)\n            for i in range(1, 4, 1):\n                blocks[\'conv3-%d\' % i] = ConvGLUBlock(5, 512, 512,\n                                                      bottlececk_dim=256,\n                                                      dropout=dropout)\n            blocks[\'conv4\'] = ConvGLUBlock(1, 512, 2048,\n                                           bottlececk_dim=1024,\n                                           dropout=dropout)\n            last_dim = 2048\n\n        elif model_size == \'9\':\n            blocks[\'conv1\'] = ConvGLUBlock(4, args.emb_dim, 807, dropout=dropout)\n            for i in range(1, 4, 1):\n                blocks[\'conv2-%d-1\' % i] = ConvGLUBlock(4, 807, 807, dropout=dropout)\n                blocks[\'conv2-%d-2\' % i] = ConvGLUBlock(4, 807, 807, dropout=dropout)\n            last_dim = 807\n\n        elif model_size == \'13\':\n            blocks[\'conv1\'] = ConvGLUBlock(4, args.emb_dim, 1268, dropout=dropout)\n            for i in range(1, 13, 1):\n                blocks[\'conv2-%d\' % i] = ConvGLUBlock(4, 1268, 1268, dropout=dropout)\n            last_dim = 1268\n\n        elif model_size == \'14\':\n            for i in range(1, 4, 1):\n                blocks[\'conv1-%d\' % i] = ConvGLUBlock(6, args.emb_dim if i == 1 else 850, 850,\n                                                      dropout=dropout)\n            blocks[\'conv2\'] = ConvGLUBlock(1, 850, 850, dropout=dropout)\n            for i in range(1, 5, 1):\n                blocks[\'conv3-%d\' % i] = ConvGLUBlock(5, 850, 850, dropout=dropout)\n            blocks[\'conv4\'] = ConvGLUBlock(1, 850, 850, dropout=dropout)\n            for i in range(1, 4, 1):\n                blocks[\'conv5-%d\' % i] = ConvGLUBlock(4, 850, 850, dropout=dropout)\n            blocks[\'conv6\'] = ConvGLUBlock(4, 850, 1024, dropout=dropout)\n            blocks[\'conv7\'] = ConvGLUBlock(4, 1024, 2048, dropout=dropout)\n            last_dim = 2048\n\n        elif model_size == \'14B\':\n            blocks[\'conv1\'] = ConvGLUBlock(5, args.emb_dim, 512, dropout=dropout)\n            for i in range(1, 4, 1):\n                blocks[\'conv2-%d\' % i] = ConvGLUBlock(5, 512, 512,\n                                                      bottlececk_dim=128,\n                                                      dropout=dropout)\n            for i in range(1, 4, 1):\n                blocks[\'conv3-%d\' % i] = ConvGLUBlock(5, 512 if i == 1 else 1024, 1024,\n                                                      bottlececk_dim=512,\n                                                      dropout=dropout)\n            for i in range(1, 7, 1):\n                blocks[\'conv4-%d\' % i] = ConvGLUBlock(5, 1024 if i == 1 else 2048, 2048,\n                                                      bottlececk_dim=1024,\n                                                      dropout=dropout)\n            blocks[\'conv5\'] = ConvGLUBlock(5, 2048, 4096,\n                                           bottlececk_dim=1024,\n                                           dropout=dropout)\n            last_dim = 4096\n\n        else:\n            raise NotImplementedError(model_size)\n\n        self.blocks = nn.Sequential(blocks)\n\n        if args.adaptive_softmax:\n            self.adaptive_softmax = nn.AdaptiveLogSoftmaxWithLoss(\n                last_dim, self.vocab,\n                # cutoffs=[self.vocab // 10, 3 * self.vocab // 10],\n                cutoffs=[self.vocab // 25, self.vocab // 5],\n                div_value=4.0)\n            self.output = None\n        else:\n            self.adaptive_softmax = None\n            self.output = nn.Linear(last_dim, self.vocab)\n            if args.tie_embedding:\n                if args.n_units != args.emb_dim:\n                    raise ValueError(\'When using the tied flag, n_units must be equal to emb_dim.\')\n                self.output.weight = self.embed.weight\n\n        self.reset_parameters(args.param_init)\n\n    @staticmethod\n    def add_args(parser, args):\n        """"""Add arguments.""""""\n        group = parser.add_argument_group(""GatedConv LM"")\n        group.add_argument(\'--n_units\', type=int, default=1024,\n                           help=\'number of units in each layer\')\n        group.add_argument(\'--kernel_size\', type=int, default=4,\n                           help=\'kernel size for GatedConvLM\')\n        return parser\n\n    def reset_parameters(self, param_init):\n        """"""Initialize parameters with kaiming_uniform style.""""""\n        logger.info(\'===== Initialize %s =====\' % self.__class__.__name__)\n        for n, p in self.named_parameters():\n            if p.dim() == 1:\n                nn.init.constant_(p, 0.)  # bias\n                logger.info(\'Initialize %s with %s / %.3f\' % (n, \'constant\', 0.))\n            elif p.dim() in [2, 4]:\n                nn.init.kaiming_uniform_(p, mode=\'fan_in\', nonlinearity=\'relu\')\n                # nn.init.kaiming_normal_(p, mode=\'fan_in\', nonlinearity=\'relu\')\n                logger.info(\'Initialize %s with %s / %.3f\' % (n, \'kaiming_uniform\', param_init))\n            else:\n                raise ValueError(n)\n\n    def decode(self, ys, state=None, mems=None, incremental=False):\n        """"""Decode function.\n\n        Args:\n            ys (LongTensor): `[B, L]`\n            state: dummy interfance for RNNLM\n            cache: dummy interfance for TransformerLM/TransformerXL\n            incremental: dummy interfance for TransformerLM/TransformerXL\n        Returns:\n            logits (FloatTensor): `[B, L, vocab]`\n            out (FloatTensor): `[B, L, d_model]` (for cache)\n            new_cache: dummy interfance for RNNLM/TransformerLM/TransformerXL\n            new_mems: dummy interfance for TransformerXL\n\n        """"""\n        out = self.dropout_embed(self.embed(ys.long()))\n        bs, max_ylen = out.size()[:2]\n\n        # NOTE: consider embed_dim as in_ch\n        out = out.unsqueeze(3)\n        out = self.blocks(out.transpose(2, 1))  # [B, out_ch, T, 1]\n        out = out.transpose(2, 1).contiguous()  # `[B, T, out_ch, 1]`\n        out = out.squeeze(3)\n        if self.adaptive_softmax is None:\n            logits = self.output(out)\n        else:\n            logits = out\n\n        return logits, out, None\n'"
neural_sp/models/lm/lm_base.py,6,"b'#! /usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n# Copyright 2019 Kyoto University (Hirofumi Inaguma)\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n\n""""""Base class for language models.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport logging\nimport numpy as np\nimport torch\n\nfrom neural_sp.models.base import ModelBase\nfrom neural_sp.models.criterion import cross_entropy_lsm\nfrom neural_sp.models.torch_utils import compute_accuracy\nfrom neural_sp.models.torch_utils import np2tensor\nfrom neural_sp.models.torch_utils import pad_list\n\nlogger = logging.getLogger(__name__)\n\n\nclass LMBase(ModelBase):\n    """"""Base class for language models.""""""\n\n    def __init__(self, args):\n\n        super(ModelBase, self).__init__()\n        logger.info(self.__class__.__name__)\n        logger.info(\'Overriding LMBase class.\')\n\n    def reset_parameters(self, param_init):\n        raise NotImplementedError\n\n    def forward(self, ys, state=None, is_eval=False, n_caches=0,\n                ylens=[], predict_last=False):\n        """"""Forward computation.\n\n        Args:\n            ys (list): length `B`, each of which contains arrays of size `[L]`\n            state (tuple or list):\n            is_eval (bool): if True, the history will not be saved.\n                This should be used in inference model for memory efficiency.\n            n_caches (int): number of cached states\n            ylens (list): not used\n            predict_last (bool): used for TransformerLM and GatedConvLM\n        Returns:\n            loss (FloatTensor): `[1]`\n            new_state (tuple or list):\n            observation (dict):\n\n        """"""\n        if is_eval:\n            self.eval()\n            with torch.no_grad():\n                loss, state, observation = self._forward(ys, state, n_caches, predict_last)\n        else:\n            self.train()\n            loss, state, observation = self._forward(ys, state)\n        return loss, state, observation\n\n    def _forward(self, ys, state, n_caches=0, predict_last=False):\n        ys = [np2tensor(y, self.device_id) for y in ys]  # <eos> is included\n        ys = pad_list(ys, self.pad)\n        ys_in, ys_out = ys[:, :-1], ys[:, 1:]\n\n        logits, out, new_state = self.decode(ys_in, state=state, mems=state)\n        # NOTE: state=state is used for RNNLM while mems=state is used for TransformerXL.\n        # TransformerLM ignores both of them.\n\n        if predict_last:\n            ys_out = ys_out[:, -1].unsqueeze(1)\n            logits = logits[:, -1].unsqueeze(1)\n\n        # Compute XE sequence loss\n        if n_caches > 0 and len(self.cache_ids) > 0:\n            assert ys_out.size(1) == 1\n            assert ys_out.size(0) == 1\n            if self.adaptive_softmax is None:\n                probs = torch.softmax(logits, dim=-1)\n            else:\n                probs = self.adaptive_softmax.log_prob(logits).exp()\n            cache_probs = probs.new_zeros(probs.size())\n\n            # Truncate cache\n            self.cache_ids = self.cache_ids[-n_caches:]  # list of `[B, 1]`\n            self.cache_keys = self.cache_keys[-n_caches:]  # list of `[B, 1, n_units]`\n\n            # Compute inner-product over caches\n            cache_attn = torch.softmax(self.cache_theta * torch.matmul(\n                torch.cat(self.cache_keys, dim=1), out.transpose(2, 1)).squeeze(2), dim=1)\n\n            # For visualization\n            if len(self.cache_ids) == n_caches:\n                self.cache_attn += [cache_attn.cpu().numpy()]\n                self.cache_attn = self.cache_attn[-n_caches:]\n\n            # Sum all probabilities\n            for offset, idx in enumerate(self.cache_ids):\n                cache_probs[:, :, idx] += cache_attn[:, offset]\n            probs = (1 - self.cache_lambda) * probs + self.cache_lambda * cache_probs\n            loss = -torch.log(probs[:, :, ys_out[:, -1]])\n        else:\n            if self.adaptive_softmax is None:\n                loss, ppl = cross_entropy_lsm(logits, ys_out.contiguous(),\n                                              self.lsm_prob, self.pad, self.training,\n                                              normalize_length=True)\n            else:\n                loss = self.adaptive_softmax(logits.view((-1, logits.size(2))),\n                                             ys_out.contiguous().view(-1)).loss\n                ppl = np.exp(loss.item())\n\n        if n_caches > 0:\n            # Register to cache\n            self.cache_ids += [ys_out[0, -1].item()]\n            self.cache_keys += [out]\n\n        # Compute token-level accuracy in teacher-forcing\n        if self.adaptive_softmax is None:\n            acc = compute_accuracy(logits, ys_out, pad=self.pad)\n        else:\n            acc = compute_accuracy(self.adaptive_softmax.log_prob(\n                logits.view((-1, logits.size(2)))), ys_out, pad=self.pad)\n\n        observation = {\'loss.lm\': loss.item(), \'acc.lm\': acc, \'ppl.lm\': ppl}\n        return loss, new_state, observation\n\n    def repackage_state(self, state):\n        return state\n\n    def reset_length(self, mem_len):\n        # for TransformerXL\n        self.mem_len = mem_len\n\n    def decode(self, ys, state=None, mems=None, incremental=False):\n        raise NotImplementedError\n\n    def predict(self, ys, state=None, mems=None, cache=None):\n        """"""Precict function for ASR.\n\n        Args:\n            ys (LongTensor): `[B, L]`\n            state:\n                - RNNLM: dict\n                    hxs (FloatTensor): `[n_layers, B, n_units]`\n                    cxs (FloatTensor): `[n_layers, B, n_units]`\n                - TransformerLM (LongTensor): `[B, L]`\n                - TransformerXL (list): length `n_layers + 1`, each of which contains a tensor`[B, L, d_model]`\n            mems (list):\n            cache (list):\n        Returns:\n            lmout (FloatTensor): `[B, L, vocab]`, used for LM integration such as cold fusion\n            state:\n                - RNNLM: dict\n                    hxs (FloatTensor): `[n_layers, B, n_units]`\n                    cxs (FloatTensor): `[n_layers, B, n_units]`\n                - TransformerLM (LongTensor): `[B, L]`\n                - TransformerXL (list): length `n_layers + 1`, each of which contains a tensor`[B, L, d_model]`\n            log_probs (FloatTensor): `[B, L, vocab]`\n\n        """"""\n        logits, lmout, new_state = self.decode(ys, state, mems=mems, cache=cache,\n                                               incremental=True)\n        log_probs = torch.log_softmax(logits, dim=-1)\n        return lmout, new_state, log_probs\n\n    def plot_attention(self):\n        # raise NotImplementedError\n        pass\n'"
neural_sp/models/lm/rnnlm.py,5,"b'#! /usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n# Copyright 2018 Kyoto University (Hirofumi Inaguma)\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n\n""""""Recurrent neural network language model (RNNLM).""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom distutils.util import strtobool\nimport logging\nimport torch\nimport torch.nn as nn\n\nfrom neural_sp.models.lm.lm_base import LMBase\nfrom neural_sp.models.modules.glu import LinearGLUBlock\nfrom neural_sp.models.torch_utils import repeat\n\nlogger = logging.getLogger(__name__)\n\n\nclass RNNLM(LMBase):\n    """"""RNN language model.""""""\n\n    def __init__(self, args, save_path=None):\n\n        super(LMBase, self).__init__()\n        logger.info(self.__class__.__name__)\n\n        self.lm_type = args.lm_type\n        self.save_path = save_path\n\n        self.emb_dim = args.emb_dim\n        self.rnn_type = args.lm_type\n        assert args.lm_type in [\'lstm\', \'gru\']\n        self.n_units = args.n_units\n        self.n_projs = args.n_projs\n        self.n_layers = args.n_layers\n        self.residual = args.residual\n        self.n_units_cv = args.n_units_null_context\n        self.lsm_prob = args.lsm_prob\n\n        self.vocab = args.vocab\n        self.eos = 2\n        self.pad = 3\n        # NOTE: reserved in advance\n\n        # for cache\n        self.cache_theta = 0.2  # smoothing parameter\n        self.cache_lambda = 0.2  # cache weight\n        self.cache_ids = []\n        self.cache_keys = []\n        self.cache_attn = []\n\n        self.embed = nn.Embedding(self.vocab, args.emb_dim, padding_idx=self.pad)\n        self.dropout_embed = nn.Dropout(p=args.dropout_in)\n\n        rnn = nn.LSTM if args.lm_type == \'lstm\' else nn.GRU\n        self.rnn = nn.ModuleList()\n        self.dropout = nn.Dropout(p=args.dropout_hidden)\n        if args.n_projs > 0:\n            self.proj = repeat(nn.Linear(args.n_units, args.n_projs), args.n_layers)\n        rnn_idim = args.emb_dim + args.n_units_null_context\n        for _ in range(args.n_layers):\n            self.rnn += [rnn(rnn_idim, args.n_units, 1, batch_first=True)]\n            rnn_idim = args.n_units\n            if args.n_projs > 0:\n                rnn_idim = args.n_projs\n\n        self.glu = None\n        if args.use_glu:\n            self.glu = LinearGLUBlock(rnn_idim)\n\n        self._odim = rnn_idim\n\n        self.adaptive_softmax = None\n        self.output_proj = None\n        self.output = None\n        if args.adaptive_softmax:\n            self.adaptive_softmax = nn.AdaptiveLogSoftmaxWithLoss(\n                rnn_idim, self.vocab,\n                # cutoffs=[self.vocab // 10, 3 * self.vocab // 10],\n                cutoffs=[self.vocab // 25, self.vocab // 5],\n                div_value=4.0)\n        elif args.tie_embedding:\n            if rnn_idim != args.emb_dim:\n                self.output_proj = nn.Linear(rnn_idim, args.emb_dim)\n                rnn_idim = args.emb_dim\n                self._odim = rnn_idim\n            self.output = nn.Linear(rnn_idim, self.vocab)\n            self.output.weight = self.embed.weight\n        else:\n            self.output = nn.Linear(rnn_idim, self.vocab)\n\n        self.reset_parameters(args.param_init)\n\n        # Initialize bias in forget gate with 1\n        # self.init_forget_gate_bias_with_one()\n\n    @property\n    def output_dim(self):\n        return self._odim\n\n    @staticmethod\n    def add_args(parser, args):\n        """"""Add arguments.""""""\n        group = parser.add_argument_group(""RNNLM"")\n        group.add_argument(\'--n_units\', type=int, default=1024,\n                           help=\'number of units in each layer\')\n        group.add_argument(\'--n_projs\', type=int, default=0,\n                           help=\'number of units in the projection layer\')\n        group.add_argument(\'--residual\', type=strtobool, default=False,\n                           help=\'\')\n        group.add_argument(\'--use_glu\', type=strtobool, default=False,\n                           help=\'use Gated Linear Unit (GLU) for fully-connected layers\')\n        return parser\n\n    def reset_parameters(self, param_init):\n        """"""Initialize parameters with uniform distribution.""""""\n        logger.info(\'===== Initialize %s =====\' % self.__class__.__name__)\n        for n, p in self.named_parameters():\n            if p.dim() == 1:\n                nn.init.constant_(p, 0.)  # bias\n                logger.info(\'Initialize %s with %s / %.3f\' % (n, \'constant\', 0.))\n            elif p.dim() == 2:\n                nn.init.uniform_(p, a=-param_init, b=param_init)\n                logger.info(\'Initialize %s with %s / %.3f\' % (n, \'uniform\', param_init))\n            else:\n                raise ValueError(n)\n\n    def decode(self, ys, state, mems=None, cache=None, incremental=False):\n        """"""Decode function.\n\n        Args:\n            ys (FloatTensor): `[B, L]`\n            state (dict):\n                hxs (FloatTensor): `[n_layers, B, n_units]`\n                cxs (FloatTensor): `[n_layers, B, n_units]`\n            cache: dummy interfance for TransformerLM/TransformerXL\n            incremental: dummy interfance for TransformerLM/TransformerXL\n        Returns:\n            logits (FloatTensor): `[B, L, vocab]`\n            ys_emb (FloatTensor): `[B, L, n_units]` (for cache)\n            new_state (dict):\n                hxs (FloatTensor): `[n_layers, B, n_units]`\n                cxs (FloatTensor): `[n_layers, B, n_units]`\n            new_mems: dummy interfance for TransformerXL\n\n        """"""\n        bs, ymax = ys.size()\n        ys_emb = self.dropout_embed(self.embed(ys.long()))\n\n        if state is None:\n            state = self.zero_state(bs)\n        new_state = {\'hxs\': None, \'cxs\': None}\n\n        # for ASR decoder pre-training\n        if self.n_units_cv > 0:\n            cv = ys.new_zeros(bs, ymax, self.n_units_cv).float()\n            ys_emb = torch.cat([ys_emb, cv], dim=-1)\n\n        residual = None\n        new_hxs, new_cxs = [], []\n        for lth in range(self.n_layers):\n            self.rnn[lth].flatten_parameters()  # for multi-GPUs\n\n            # Path through RNN\n            if self.rnn_type == \'lstm\':\n                ys_emb, (h, c) = self.rnn[lth](ys_emb, hx=(state[\'hxs\'][lth:lth + 1],\n                                                           state[\'cxs\'][lth:lth + 1]))\n                new_cxs.append(c)\n            elif self.rnn_type == \'gru\':\n                ys_emb, h = self.rnn[lth](ys_emb, hx=state[\'hxs\'][lth:lth + 1])\n            new_hxs.append(h)\n            ys_emb = self.dropout(ys_emb)\n            if self.n_projs > 0:\n                ys_emb = torch.tanh(self.proj[lth](ys_emb))\n\n            # Residual connection\n            if self.residual and lth > 0:\n                ys_emb = ys_emb + residual\n            residual = ys_emb\n            # NOTE: Exclude residual connection from the raw inputs\n\n        # Repackage\n        new_state[\'hxs\'] = torch.cat(new_hxs, dim=0)\n        if self.rnn_type == \'lstm\':\n            new_state[\'cxs\'] = torch.cat(new_cxs, dim=0)\n\n        if self.glu is not None:\n            if self.residual:\n                residual = ys_emb\n            ys_emb = self.glu(ys_emb)\n            if self.residual:\n                ys_emb = ys_emb + residual\n\n        if self.adaptive_softmax is None:\n            if self.output_proj is not None:\n                ys_emb = self.output_proj(ys_emb)\n            logits = self.output(ys_emb)\n        else:\n            logits = ys_emb\n\n        return logits, ys_emb, new_state\n\n    def zero_state(self, batch_size):\n        """"""Initialize hidden state.\n\n        Args:\n            batch_size (int): batch size\n        Returns:\n            state (dict):\n                hxs (FloatTensor): `[n_layers, B, n_units]`\n                cxs (FloatTensor): `[n_layers, B, n_units]`\n\n        """"""\n        w = next(self.parameters())\n        state = {\'hxs\': None, \'cxs\': None}\n        state[\'hxs\'] = w.new_zeros(self.n_layers, batch_size, self.n_units)\n        if self.rnn_type == \'lstm\':\n            state[\'cxs\'] = w.new_zeros(self.n_layers, batch_size, self.n_units)\n        return state\n\n    def repackage_state(self, state):\n        """"""Wraps hidden states in new Tensors, to detach them from their history.\n\n        Args:\n            state (dict):\n                hxs (FloatTensor): `[n_layers, B, n_units]`\n                cxs (FloatTensor): `[n_layers, B, n_units]`\n        Returns:\n            state (dict):\n                hxs (FloatTensor): `[n_layers, B, n_units]`\n                cxs (FloatTensor): `[n_layers, B, n_units]`\n\n        """"""\n        state[\'hxs\'] = state[\'hxs\'].detach()\n        if self.rnn_type == \'lstm\':\n            state[\'cxs\'] = state[\'cxs\'].detach()\n        return state\n'"
neural_sp/models/lm/transformer_xl.py,10,"b'#! /usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n# Copyright 2020 Kyoto University (Hirofumi Inaguma)\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n\n""""""TransformerXL language model.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport copy\nfrom distutils.util import strtobool\nimport logging\nimport math\nimport os\nimport random\nimport shutil\nimport torch\nimport torch.nn as nn\n\nfrom neural_sp.models.lm.lm_base import LMBase\nfrom neural_sp.models.modules.initialization import init_like_transformer_xl\nfrom neural_sp.models.modules.positional_embedding import XLPositionalEmbedding\nfrom neural_sp.models.modules.transformer import TransformerDecoderBlock\nfrom neural_sp.models.torch_utils import tensor2np\nfrom neural_sp.utils import mkdir_join\n\nimport matplotlib\nmatplotlib.use(\'Agg\')\n\nrandom.seed(1)\n\nlogger = logging.getLogger(__name__)\n\n\nclass TransformerXL(LMBase):\n    """"""TransformerXL language model.""""""\n\n    def __init__(self, args, save_path=None):\n\n        super(LMBase, self).__init__()\n        logger.info(self.__class__.__name__)\n\n        self.lm_type = args.lm_type\n        self.save_path = save_path\n\n        self.d_model = args.transformer_d_model\n        self.n_layers = args.n_layers\n        self.n_heads = args.transformer_n_heads\n        self.lsm_prob = args.lsm_prob\n\n        if args.mem_len > 0:\n            self.mem_len = args.mem_len\n        else:\n            self.mem_len = args.bptt\n        if args.recog_mem_len > 0:\n            self.mem_len = args.recog_mem_len\n        self.zero_center_offset = args.zero_center_offset\n\n        self.vocab = args.vocab\n        self.eos = 2\n        self.pad = 3\n        # NOTE: reserved in advance\n\n        # for cache\n        self.cache_theta = 0.2  # smoothing parameter\n        self.cache_lambda = 0.2  # cache weight\n        self.cache_ids = []\n        self.cache_keys = []\n        self.cache_attn = []\n\n        # positional embedding\n        self.pos_emb = XLPositionalEmbedding(self.d_model, args.dropout_in)\n        self.u = nn.Parameter(torch.Tensor(self.n_heads, self.d_model // self.n_heads))\n        self.v = nn.Parameter(torch.Tensor(self.n_heads, self.d_model // self.n_heads))\n        # NOTE: u and v are global parameters\n\n        self.embed = nn.Embedding(self.vocab, self.d_model, padding_idx=self.pad)\n        self.scale = math.sqrt(self.d_model)  # for token embedding\n        self.dropout_emb = nn.Dropout(p=args.dropout_in)  # for token embedding\n        self.layers = nn.ModuleList([copy.deepcopy(TransformerDecoderBlock(\n            self.d_model, args.transformer_d_ff, \'scaled_dot\',\n            self.n_heads, args.dropout_hidden, args.dropout_att, args.dropout_layer,\n            args.transformer_layer_norm_eps, args.transformer_ffn_activation, args.transformer_param_init,\n            src_tgt_attention=False, memory_transformer=True)) for lth in range(self.n_layers)])\n        self.norm_out = nn.LayerNorm(self.d_model, eps=args.transformer_layer_norm_eps)\n\n        self.adaptive_softmax = None\n        self.output = None\n        if args.adaptive_softmax:\n            self.adaptive_softmax = nn.AdaptiveLogSoftmaxWithLoss(\n                self.d_model, self.vocab,\n                cutoffs=[round(self.vocab / 15), 3 * round(self.vocab / 15)],\n                # cutoffs=[self.vocab // 25, 3 * self.vocab // 5],\n                div_value=4.0)\n        else:\n            self.output = nn.Linear(self.d_model, self.vocab)\n            if args.tie_embedding:\n                self.output.weight = self.embed.weight\n\n        self.reset_parameters()\n\n    @property\n    def output_dim(self):\n        return self.d_model\n\n    @staticmethod\n    def add_args(parser, args):\n        """"""Add arguments.""""""\n        group = parser.add_argument_group(""Transformer-XL LM"")\n        group.add_argument(\'--transformer_d_model\', type=int, default=256,\n                           help=\'number of units in the MHA layer\')\n        group.add_argument(\'--transformer_d_ff\', type=int, default=2048,\n                           help=\'number of units in the FFN layer\')\n        # group.add_argument(\'--transformer_d_ff_bottleneck_dim\', type=int, default=0,\n        #                    help=\'bottleneck dimension in the FFN layer\')\n        group.add_argument(\'--transformer_n_heads\', type=int, default=4,\n                           help=\'number of heads in the MHA layer\')\n        group.add_argument(\'--transformer_layer_norm_eps\', type=float, default=1e-12,\n                           help=\'epsilon value for layer normalization\')\n        group.add_argument(\'--transformer_ffn_activation\', type=str, default=\'relu\',\n                           choices=[\'relu\', \'gelu\', \'gelu_accurate\', \'glu\', \'swish\'],\n                           help=\'nonlinear activation for the FFN layer\')\n        group.add_argument(\'--transformer_param_init\', type=str, default=\'xavier_uniform\',\n                           choices=[\'xavier_uniform\', \'pytorch\'],\n                           help=\'parameter initializatin\')\n        group.add_argument(\'--dropout_att\', type=float, default=0.1,\n                           help=\'dropout probability for the attention weights\')\n        group.add_argument(\'--dropout_layer\', type=float, default=0.0,\n                           help=\'LayerDrop probability for Transformer layers\')\n        # XL specific\n        group.add_argument(\'--mem_len\', type=int, default=0,\n                           help=\'number of tokens for memory in TransformerXL during training\')\n        group.add_argument(\'--zero_center_offset\', type=strtobool, default=False,\n                           help=\'set the offset right after memory to zero (accept negaitve indices)\')\n        return parser\n\n    def reset_parameters(self):\n        """"""Initialize parameters with normal distribution.""""""\n        logging.info(\'===== Initialize %s with normal distribution =====\' % self.__class__.__name__)\n        for n, p in self.named_parameters():\n            init_like_transformer_xl(n, p, std=0.02)\n\n    def init_memory(self):\n        """"""Initialize memory.""""""\n        if self.device_id >= 0:\n            return [torch.empty(0, dtype=torch.float).cuda(self.device_id)\n                    for _ in range(self.n_layers)]\n        else:\n            return [torch.empty(0, dtype=torch.float)\n                    for _ in range(self.n_layers)]\n\n    def update_memory(self, memory_prev, hidden_states):\n        """"""Update memory.\n\n        Args:\n            memory_prev (list): length `n_layers`, each of which contains `[B, mlen, d_model]`\n            hidden_states (list): length `n_layers`, each of which contains `[B, L, d_model]`\n        Returns:\n            new_mems (list): length `n_layers`, each of which contains `[B, mlen, d_model]`\n\n        """"""\n        if memory_prev is None:\n            memory_prev = self.init_memory()  # 0-th to L-1-th layer\n        assert len(hidden_states) == len(memory_prev)\n        mlen = memory_prev[0].size(1) if memory_prev[0].dim() > 1 else 0\n        qlen = hidden_states[0].size(1)\n\n        # There are `mlen + qlen` steps that can be cached into mems\n        # For the next step, the last `ext_len` of the `qlen` tokens\n        # will be used as the extended context. Hence, we only cache\n        # the tokens from `mlen + qlen - self.ext_len - self.mem_len`\n        # to `mlen + qlen - self.ext_len`.\n        with torch.no_grad():\n            new_mems = []\n            end_idx = mlen + qlen\n            start_idx = max(0, end_idx - self.mem_len)\n            for m, h in zip(memory_prev, hidden_states):\n                cat = torch.cat([m, h], dim=1)  # `[B, mlen + qlen, d_model]`\n                new_mems.append(cat[:, start_idx:end_idx].detach())  # `[B, self.mem_len, d_model]`\n\n        return new_mems\n\n    def decode(self, ys, state=None, mems=None, cache=None, incremental=False):\n        """"""Decode function.\n\n        Args:\n            ys (LongTensor): `[B, L]`\n            state (list): dummy interfance for RNNLM\n            mems (list): length `n_layers`, each of which contains a FloatTensor `[B, mlen, d_model]`\n            cache (list): length `L`, each of which contains a FloatTensor `[B, L-1, d_model]`\n            incremental (bool): ASR decoding mode\n        Returns:\n            logits (FloatTensor): `[B, L, vocab]`\n            out (FloatTensor): `[B, L, d_model]`\n            new_cache (list): length `n_layers`, each of which contains a FloatTensor `[B, L, d_model]`\n\n        """"""\n        # for ASR decoding\n        if cache is None:\n            cache = [None] * self.n_layers  # 1-th to L-th layer\n\n        if mems is None:\n            mems = self.init_memory()\n            mlen = 0\n        else:\n            mlen = mems[0].size(1)\n\n        bs, ylen = ys.size()[:2]\n        if incremental and cache[0] is not None:\n            ylen = cache[0].size(1) + 1\n\n        # Create the self-attention mask\n        causal_mask = ys.new_ones(ylen, ylen + mlen).byte()\n        causal_mask = torch.tril(causal_mask, diagonal=0 + mlen, out=causal_mask).unsqueeze(0)\n        causal_mask = causal_mask.repeat([bs, 1, 1])  # `[B, L, L+mlen]`\n\n        out = self.dropout_emb(self.embed(ys.long()) * self.scale)\n        # NOTE: TransformerXL does not use positional encoding in the token embedding\n        if self.zero_center_offset:\n            pos_idxs = torch.arange(mlen - 1, -ylen - 1, -1.0, dtype=torch.float)\n        else:\n            pos_idxs = torch.arange(ylen + mlen - 1, -1, -1.0, dtype=torch.float)\n        pos_embs = self.pos_emb(pos_idxs, self.device_id)\n\n        new_mems = [None] * self.n_layers\n        new_cache = [None] * self.n_layers\n        hidden_states = [out]\n        for lth, (mem, layer) in enumerate(zip(mems, self.layers)):\n            if incremental and mlen > 0 and mem.size(0) != bs:\n                mem = mem.repeat([bs, 1, 1])\n            out, yy_aws = layer(out, causal_mask, cache=cache[lth],\n                                pos_embs=pos_embs, memory=mem,\n                                u=self.u, v=self.v)[:2]\n            if incremental:\n                new_cache[lth] = out\n            elif lth < self.n_layers - 1:\n                hidden_states.append(out)\n                # NOTE: outputs from the last layer is not used for memory\n            if not self.training and yy_aws is not None:\n                setattr(self, \'yy_aws_layer%d\' % lth, tensor2np(yy_aws))\n        out = self.norm_out(out)\n        if self.adaptive_softmax is None:\n            logits = self.output(out)\n        else:\n            logits = out\n\n        if incremental:\n            # NOTE: do not update memory here during ASR decoding\n            return logits, out, new_cache\n        else:\n            # Update memory\n            new_mems = self.update_memory(mems, hidden_states)\n            return logits, out, new_mems\n\n    def plot_attention(self, n_cols=4):\n        """"""Plot attention for each head in all layers.""""""\n        from matplotlib import pyplot as plt\n        from matplotlib.ticker import MaxNLocator\n\n        save_path = mkdir_join(self.save_path, \'att_weights\')\n\n        # Clean directory\n        if save_path is not None and os.path.isdir(save_path):\n            shutil.rmtree(save_path)\n            os.mkdir(save_path)\n\n        for lth in range(self.n_layers):\n            if not hasattr(self, \'yy_aws_layer%d\' % lth):\n                continue\n\n            yy_aws = getattr(self, \'yy_aws_layer%d\' % lth)\n\n            plt.clf()\n            fig, axes = plt.subplots(self.n_heads // n_cols, n_cols, figsize=(20, 8))\n            for h in range(self.n_heads):\n                if self.n_heads > n_cols:\n                    ax = axes[h // n_cols, h % n_cols]\n                else:\n                    ax = axes[h]\n                ax.imshow(yy_aws[-1, h, :, :], aspect=""auto"")\n                ax.grid(False)\n                ax.set_xlabel(""Input (head%d)"" % h)\n                ax.set_ylabel(""Output (head%d)"" % h)\n                ax.xaxis.set_major_locator(MaxNLocator(integer=True))\n                ax.yaxis.set_major_locator(MaxNLocator(integer=True))\n\n            fig.tight_layout()\n            fig.savefig(os.path.join(save_path, \'layer%d.png\' % (lth)), dvi=500)\n            plt.close()\n'"
neural_sp/models/lm/transformerlm.py,6,"b'#! /usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n# Copyright 2019 Kyoto University (Hirofumi Inaguma)\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n\n""""""Transformer language model.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport copy\nimport logging\nimport os\nimport random\nimport shutil\nimport torch\nimport torch.nn as nn\n\nfrom neural_sp.models.lm.lm_base import LMBase\nfrom neural_sp.models.modules.positional_embedding import PositionalEncoding\nfrom neural_sp.models.modules.transformer import TransformerDecoderBlock\nfrom neural_sp.models.torch_utils import tensor2np\nfrom neural_sp.utils import mkdir_join\n\nimport matplotlib\nmatplotlib.use(\'Agg\')\n\nrandom.seed(1)\n\nlogger = logging.getLogger(__name__)\n\n\nclass TransformerLM(LMBase):\n    """"""Transformer language model.""""""\n\n    def __init__(self, args, save_path=None):\n\n        super(LMBase, self).__init__()\n        logger.info(self.__class__.__name__)\n\n        self.lm_type = args.lm_type\n        self.save_path = save_path\n\n        self.d_model = args.transformer_d_model\n        self.n_layers = args.n_layers\n        self.n_heads = args.transformer_n_heads\n        self.lsm_prob = args.lsm_prob\n        self.tie_embedding = args.tie_embedding\n\n        self.mem_len = args.mem_len\n        if args.recog_mem_len > 0:\n            self.mem_len = args.recog_mem_len\n\n        self.vocab = args.vocab\n        self.eos = 2\n        self.pad = 3\n        # NOTE: reserved in advance\n\n        # for cache\n        self.cache_theta = 0.2  # smoothing parameter\n        self.cache_lambda = 0.2  # cache weight\n        self.cache_ids = []\n        self.cache_keys = []\n        self.cache_attn = []\n\n        self.embed = nn.Embedding(self.vocab, self.d_model, padding_idx=self.pad)\n        self.pos_enc = PositionalEncoding(self.d_model, args.dropout_in, args.transformer_pe_type,\n                                          args.transformer_param_init)\n        self.layers = nn.ModuleList([copy.deepcopy(TransformerDecoderBlock(\n            self.d_model, args.transformer_d_ff, \'scaled_dot\',\n            self.n_heads, args.dropout_hidden, args.dropout_att, args.dropout_layer,\n            args.transformer_layer_norm_eps, args.transformer_ffn_activation, args.transformer_param_init,\n            src_tgt_attention=False)) for lth in range(self.n_layers)])\n        self.norm_out = nn.LayerNorm(self.d_model, eps=args.transformer_layer_norm_eps)\n\n        self.adaptive_softmax = None\n        self.output = None\n        if args.adaptive_softmax:\n            self.adaptive_softmax = nn.AdaptiveLogSoftmaxWithLoss(\n                self.d_model, self.vocab,\n                cutoffs=[round(self.vocab / 15), 3 * round(self.vocab / 15)],\n                # cutoffs=[self.vocab // 25, 3 * self.vocab // 5],\n                div_value=4.0)\n        else:\n            self.output = nn.Linear(self.d_model, self.vocab)\n            if args.tie_embedding:\n                self.output.weight = self.embed.weight\n\n        self.reset_parameters()\n\n    @property\n    def output_dim(self):\n        return self.d_model\n\n    @staticmethod\n    def add_args(parser, args):\n        """"""Add arguments.""""""\n        group = parser.add_argument_group(""Transformer LM"")\n        group.add_argument(\'--transformer_d_model\', type=int, default=256,\n                           help=\'number of units in the MHA layer\')\n        group.add_argument(\'--transformer_d_ff\', type=int, default=2048,\n                           help=\'number of units in the FFN layer\')\n        # group.add_argument(\'--transformer_d_ff_bottleneck_dim\', type=int, default=0,\n        #                    help=\'bottleneck dimension in the FFN layer\')\n        group.add_argument(\'--transformer_n_heads\', type=int, default=4,\n                           help=\'number of heads in the MHA layer\')\n        group.add_argument(\'--transformer_pe_type\', type=str, default=\'add\',\n                           choices=[\'add\', \'concat\', \'none\', \'1dconv3L\'],\n                           help=\'type of positional encoding\')\n        group.add_argument(\'--transformer_layer_norm_eps\', type=float, default=1e-12,\n                           help=\'epsilon value for layer normalization\')\n        group.add_argument(\'--transformer_ffn_activation\', type=str, default=\'relu\',\n                           choices=[\'relu\', \'gelu\', \'gelu_accurate\', \'glu\', \'swish\'],\n                           help=\'nonlinear activation for the FFN layer\')\n        group.add_argument(\'--transformer_param_init\', type=str, default=\'xavier_uniform\',\n                           choices=[\'xavier_uniform\', \'pytorch\'],\n                           help=\'parameter initializatin\')\n        group.add_argument(\'--dropout_att\', type=float, default=0.1,\n                           help=\'dropout probability for the attention weights\')\n        group.add_argument(\'--dropout_layer\', type=float, default=0.0,\n                           help=\'LayerDrop probability for Transformer layers\')\n        # memory\n        group.add_argument(\'--mem_len\', type=int, default=0,\n                           help=\'number of tokens for memory in TransformerXL during training\')\n        return parser\n\n    def reset_parameters(self):\n        """"""Initialize parameters with Xavier uniform distribution.""""""\n        logging.info(\'===== Initialize %s =====\' % self.__class__.__name__)\n        # see https://github.com/pytorch/fairseq/blob/master/fairseq/models/transformer.py\n        # embedding\n        nn.init.normal_(self.embed.weight, mean=0., std=self.d_model**-0.5)\n        nn.init.constant_(self.embed.weight[self.pad], 0)\n        # output layer\n        if self.output is not None and not self.tie_embedding:\n            nn.init.xavier_uniform_(self.output.weight)\n            nn.init.constant_(self.output.bias, 0.)\n            # nn.init.normal_(self.embed.weight, mean=0., std=self.d_model**-0.5)\n\n    def init_memory(self):\n        """"""Initialize memory.""""""\n        if self.device_id >= 0:\n            return [torch.empty(0, dtype=torch.float).cuda(self.device_id)\n                    for _ in range(self.n_layers)]\n        else:\n            return [torch.empty(0, dtype=torch.float)\n                    for _ in range(self.n_layers)]\n\n    def update_memory(self, memory_prev, hidden_states):\n        """"""Update memory.\n\n        Args:\n            memory_prev (list): length `n_layers`, each of which contains `[B, mlen, d_model]`\n            hidden_states (list): length `n_layers`, each of which contains `[B, L, d_model]`\n        Returns:\n            new_mems (list): length `n_layers`, each of which contains `[B, mlen, d_model]`\n\n        """"""\n        if memory_prev is None:\n            memory_prev = self.init_memory()  # 0-th to L-1-th layer\n        assert len(hidden_states) == len(memory_prev)\n        mlen = memory_prev[0].size(1) if memory_prev[0].dim() > 1 else 0\n        qlen = hidden_states[0].size(1)\n\n        # There are `mlen + qlen` steps that can be cached into mems\n        # For the next step, the last `ext_len` of the `qlen` tokens\n        # will be used as the extended context. Hence, we only cache\n        # the tokens from `mlen + qlen - self.ext_len - self.mem_len`\n        # to `mlen + qlen - self.ext_len`.\n        with torch.no_grad():\n            new_mems = []\n            end_idx = mlen + qlen\n            start_idx = max(0, end_idx - self.mem_len)\n            for m, h in zip(memory_prev, hidden_states):\n                cat = torch.cat([m, h], dim=1)  # `[B, mlen + qlen, d_model]`\n                new_mems.append(cat[:, start_idx:end_idx].detach())  # `[B, self.mem_len, d_model]`\n        return new_mems\n\n    def decode(self, ys, state=None, mems=None, cache=None, incremental=False):\n        """"""Decode function.\n\n        Args:\n            ys (LongTensor): `[B, L]`\n            state (list): dummy interfance for RNNLM\n            mems (list): length `n_layers`, each of which contains a FloatTensor `[B, mlen, d_model]`\n            cache (list): length `L`, each of which contains a FloatTensor `[B, L-1, d_model]`\n            incremental (bool): ASR decoding mode\n        Returns:\n            logits (FloatTensor): `[B, L, vocab]`\n            out (FloatTensor): `[B, L, d_model]`\n            new_cache (list): length `n_layers`, each of which contains a FloatTensor `[B, L, d_model]`\n\n        """"""\n        # for ASR decoding\n        if cache is None:\n            cache = [None] * self.n_layers  # 1-th to L-th layer\n\n        if mems is None:\n            mems = self.init_memory()\n\n        # Create the self-attention mask\n        bs, ylen = ys.size()[:2]\n        if incremental and cache[0] is not None:\n            ylen = cache[0].size(1) + 1\n        causal_mask = ys.new_ones(ylen, ylen).byte()\n        causal_mask = torch.tril(causal_mask, diagonal=0, out=causal_mask).unsqueeze(0)\n        causal_mask = causal_mask.repeat([bs, 1, 1])\n\n        out = self.pos_enc(self.embed(ys.long()))\n\n        new_mems = [None] * self.n_layers\n        new_cache = [None] * self.n_layers\n        hidden_states = [out]\n        for lth, (mem, layer) in enumerate(zip(mems, self.layers)):\n            out, yy_aws = layer(out, causal_mask, cache=cache[lth], memory=mem)[:2]\n            if incremental:\n                new_cache[lth] = out\n            elif lth < self.n_layers - 1:\n                hidden_states.append(out)\n                # NOTE: outputs from the last layer is not used for memory\n            if not self.training and yy_aws is not None:\n                setattr(self, \'yy_aws_layer%d\' % lth, tensor2np(yy_aws))\n        out = self.norm_out(out)\n        if self.adaptive_softmax is None:\n            logits = self.output(out)\n        else:\n            logits = out\n\n        if incremental:\n            # NOTE: do not update memory here during ASR decoding\n            return logits, out, new_cache\n        elif self.mem_len > 0:\n            # Update memory\n            new_mems = self.update_memory(mems, hidden_states)\n            return logits, out, new_mems\n        else:\n            return logits, out, mems\n\n    def plot_attention(self, n_cols=4):\n        """"""Plot attention for each head in all layers.""""""\n        from matplotlib import pyplot as plt\n        from matplotlib.ticker import MaxNLocator\n\n        save_path = mkdir_join(self.save_path, \'att_weights\')\n\n        # Clean directory\n        if save_path is not None and os.path.isdir(save_path):\n            shutil.rmtree(save_path)\n            os.mkdir(save_path)\n\n        for lth in range(self.n_layers):\n            if not hasattr(self, \'yy_aws_layer%d\' % lth):\n                continue\n\n            yy_aws = getattr(self, \'yy_aws_layer%d\' % lth)\n\n            plt.clf()\n            fig, axes = plt.subplots(self.n_heads // n_cols, n_cols, figsize=(20, 8))\n            for h in range(self.n_heads):\n                if self.n_heads > n_cols:\n                    ax = axes[h // n_cols, h % n_cols]\n                else:\n                    ax = axes[h]\n                ax.imshow(yy_aws[-1, h, :, :], aspect=""auto"")\n                ax.grid(False)\n                ax.set_xlabel(""Input (head%d)"" % h)\n                ax.set_ylabel(""Output (head%d)"" % h)\n                ax.xaxis.set_major_locator(MaxNLocator(integer=True))\n                ax.yaxis.set_major_locator(MaxNLocator(integer=True))\n\n            fig.tight_layout()\n            fig.savefig(os.path.join(save_path, \'layer%d.png\' % (lth)), dvi=500)\n            plt.close()\n'"
neural_sp/models/modules/__init__.py,0,b''
neural_sp/models/modules/attention.py,9,"b'#! /usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n# Copyright 2018 Kyoto University (Hirofumi Inaguma)\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n\n""""""Single-head attention layer.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\n\nNEG_INF = float(np.finfo(np.float32).min)\n\n\nclass AttentionMechanism(nn.Module):\n    """"""Single-head attention layer.\n\n    Args:\n        kdim (int): dimension of key\n        qdim (int): dimension of query\n        atype (str): type of attention mechanisms\n        adim: (int) dimension of the attention space\n        sharpening_factor (float): sharpening factor in the softmax layer\n            for attention weights\n        sigmoid_smoothing (bool): replace the softmax layer for attention weights\n            with the sigmoid function\n        conv_out_channels (int): number of channles of conv outputs.\n            This is used for location-based attention.\n        conv_kernel_size (int): size of kernel.\n            This must be the odd number.\n        dropout (float): dropout probability for attention weights\n        lookahead (int): lookahead frames for triggered attention\n\n    """"""\n\n    def __init__(self, kdim, qdim, adim, atype,\n                 sharpening_factor=1, sigmoid_smoothing=False,\n                 conv_out_channels=10, conv_kernel_size=201, dropout=0.,\n                 lookahead=2):\n\n        super(AttentionMechanism, self).__init__()\n\n        assert conv_kernel_size % 2 == 1, ""Kernel size should be odd for \'same\' conv.""\n        self.atype = atype\n        self.adim = adim\n        self.sharpening_factor = sharpening_factor\n        self.sigmoid_smoothing = sigmoid_smoothing\n        self.n_heads = 1\n        self.lookahead = lookahead\n        self.reset()\n\n        # attention dropout applied after the softmax layer\n        self.dropout = nn.Dropout(p=dropout)\n\n        if atype == \'no\':\n            raise NotImplementedError\n            # NOTE: sequence-to-sequence without attetnion (use the last state as a context vector)\n\n        elif atype in [\'add\', \'triggered_attention\']:\n            self.w_key = nn.Linear(kdim, adim)\n            self.w_query = nn.Linear(qdim, adim, bias=False)\n            self.v = nn.Linear(adim, 1, bias=False)\n\n        elif atype == \'location\':\n            self.w_key = nn.Linear(kdim, adim)\n            self.w_query = nn.Linear(qdim, adim, bias=False)\n            self.w_conv = nn.Linear(conv_out_channels, adim, bias=False)\n            self.conv = nn.Conv2d(in_channels=1,\n                                  out_channels=conv_out_channels,\n                                  kernel_size=(1, conv_kernel_size),\n                                  stride=1,\n                                  padding=(0, (conv_kernel_size - 1) // 2),\n                                  bias=False)\n            self.v = nn.Linear(adim, 1, bias=False)\n\n        elif atype == \'dot\':\n            self.w_key = nn.Linear(kdim, adim, bias=False)\n            self.w_query = nn.Linear(qdim, adim, bias=False)\n\n        elif atype == \'luong_dot\':\n            assert kdim == qdim\n            # NOTE: no additional parameters\n\n        elif atype == \'luong_general\':\n            self.w_key = nn.Linear(kdim, qdim, bias=False)\n\n        elif atype == \'luong_concat\':\n            self.w = nn.Linear(kdim + qdim, adim, bias=False)\n            self.v = nn.Linear(adim, 1, bias=False)\n\n        else:\n            raise ValueError(atype)\n\n    def reset(self):\n        self.key = None\n        self.mask = None\n\n    def forward(self, key, value, query, mask=None, aw_prev=None,\n                cache=False, mode=\'\', trigger_point=None):\n        """"""Forward pass.\n\n        Args:\n            key (FloatTensor): `[B, klen, kdim]`\n            klens (IntTensor): `[B]`\n            value (FloatTensor): `[B, klen, vdim]`\n            query (FloatTensor): `[B, 1, qdim]`\n            mask (ByteTensor): `[B, qlen, klen]`\n            aw_prev (FloatTensor): `[B, 1 (H), 1 (qlen), klen]`\n            cache (bool): cache key and mask\n            mode: dummy interface for MoChA\n            trigger_point (IntTensor): `[B]`\n        Returns:\n            cv (FloatTensor): `[B, 1, vdim]`\n            aw (FloatTensor): `[B, 1 (H), 1 (qlen), klen]`\n            beta: dummy interface for MoChA\n\n        """"""\n        bs, klen = key.size()[:2]\n        qlen = query.size(1)\n\n        if aw_prev is None:\n            aw_prev = key.new_zeros(bs, 1, klen)\n        else:\n            aw_prev = aw_prev.squeeze(1)  # remove head dimension\n\n        # Pre-computation of encoder-side features for computing scores\n        if self.key is None or not cache:\n            if self.atype in [\'add\', \'trigerred_attention\',\n                              \'location\', \'dot\', \'luong_general\']:\n                self.key = self.w_key(key)\n            else:\n                self.key = key\n            self.mask = mask\n            if mask is not None:\n                assert self.mask.size() == (bs, 1, klen), (self.mask.size(), (bs, 1, klen))\n\n        # for batch beam search decoding\n        if self.key.size(0) != query.size(0):\n            self.key = self.key[0:1, :, :].repeat([query.size(0), 1, 1])\n\n        if self.atype == \'no\':\n            raise NotImplementedError\n\n        elif self.atype in [\'add\', \'triggered_attention\']:\n            tmp = self.key.unsqueeze(1) + self.w_query(query).unsqueeze(2)\n            e = self.v(torch.tanh(tmp)).squeeze(3)\n\n        elif self.atype == \'location\':\n            conv_feat = self.conv(aw_prev.unsqueeze(1)).squeeze(2)  # `[B, ch, klen]`\n            conv_feat = conv_feat.transpose(2, 1).contiguous().unsqueeze(1)  # `[B, 1, klen, ch]`\n            tmp = self.key.unsqueeze(1) + self.w_query(query).unsqueeze(2)\n            e = self.v(torch.tanh(tmp + self.w_conv(conv_feat))).squeeze(3)\n\n        elif self.atype == \'dot\':\n            e = torch.bmm(self.w_query(query), self.key.transpose(2, 1))\n\n        elif self.atype in [\'luong_dot\', \'luong_general\']:\n            e = torch.bmm(query, self.key.transpose(2, 1))\n\n        elif self.atype == \'luong_concat\':\n            query = query.repeat([1, klen, 1])\n            e = self.v(torch.tanh(self.w(torch.cat([self.key, query], dim=-1)))).transpose(2, 1)\n        assert e.size() == (bs, qlen, klen), (e.size(), (bs, qlen, klen))\n\n        # Mask the right part from the trigger point\n        if self.atype == \'triggered_attention\':\n            assert trigger_point is not None\n            for b in range(bs):\n                e[b, :, trigger_point[b] + self.lookahead + 1:] = NEG_INF\n\n        # Compute attention weights, context vector\n        if self.mask is not None:\n            e = e.masked_fill_(self.mask == 0, NEG_INF)\n        if self.sigmoid_smoothing:\n            aw = torch.sigmoid(e) / torch.sigmoid(e).sum(-1).unsqueeze(-1)\n        else:\n            aw = torch.softmax(e * self.sharpening_factor, dim=-1)\n        aw = self.dropout(aw)\n        cv = torch.bmm(aw, value)\n\n        return cv, aw.unsqueeze(1), None\n'"
neural_sp/models/modules/causal_conv.py,1,"b'#! /usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n# Copyright 2020 Kyoto University (Hirofumi Inaguma)\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n\n""""""Dilated causal convolution.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport logging\nimport torch.nn as nn\n\nlogger = logging.getLogger(__name__)\n\n\nclass CausalConv1d(nn.Module):\n    """"""1D dilated causal convolution.""""""\n\n    def __init__(self, in_channels, out_channels, kernel_size, dilation=1):\n        super(CausalConv1d, self).__init__()\n        self.padding = (kernel_size - 1) * dilation\n        self.conv1d = nn.Conv1d(in_channels, out_channels, kernel_size,\n                                padding=self.padding, dilation=dilation)\n\n    def forward(self, xs):\n        """"""Forward computation.\n\n        Args:\n            xs (FloatTensor): `[B, T, in_channels]`\n        Returns:\n            xs (FloatTensor): `[B, T, out_channels]`\n\n        """"""\n        xs = xs.transpose(2, 1)\n        xs = self.conv1d(xs)\n        if self.padding != 0:\n            xs = xs[:, :, :-self.padding]\n        xs = xs.transpose(2, 1).contiguous()\n        return xs\n'"
neural_sp/models/modules/cif.py,3,"b'# ! /usr/bin/env python\n# -*- coding: utf-8 -*-\n\n# Copyright 2019 Kyoto University (Hirofumi Inaguma)\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n\nimport torch\nimport torch.nn as nn\n\nfrom neural_sp.models.modules.linear import Linear\n\n\nclass CIF(nn.Module):\n    """"""docstring for CIF.""""""\n\n    def __init__(self, enc_dim, conv_out_channels, conv_kernel_size,\n                 threshold=0.9):\n        super(CIF, self).__init__()\n\n        self.threshold = threshold\n        self.channel = conv_out_channels\n        self.n_heads = 1\n\n        self.conv = nn.Conv1d(in_channels=enc_dim,\n                              out_channels=conv_out_channels,\n                              kernel_size=conv_kernel_size * 2 + 1,\n                              stride=1,\n                              padding=conv_kernel_size)\n        self.proj = Linear(conv_out_channels, 1)\n\n    def forward(self, eouts, elens, ylens=None, max_len=200):\n        """"""\n\n        Args:\n            eouts (FloatTensor): `[B, T, enc_dim]`\n            elens (IntTensor): `[B]`\n            ylens (IntTensor): `[B]`\n            max_len (int): the maximum length of target sequence\n        Returns:\n            eouts_fired (FloatTensor): `[B, T, enc_dim]`\n            alpha (FloatTensor): `[B, T]`\n            aws (FloatTensor): `[B, 1 (head), L. T]`\n\n        """"""\n        bs, xtime, enc_dim = eouts.size()\n\n        # 1d conv\n        conv_feat = self.conv(eouts.transpose(2, 1))  # `[B, channel, kmax]`\n        conv_feat = conv_feat.transpose(2, 1)\n        alpha = torch.sigmoid(self.proj(conv_feat)).squeeze(2)  # `[B, kmax]`\n\n        # normalization\n        if ylens is not None:\n            alpha_norm = alpha / alpha.sum(1).unsqueeze(1) * ylens.unsqueeze(1)\n        else:\n            alpha_norm = alpha\n\n        if ylens is not None:\n            max_len = ylens.max().int()\n        eouts_fired = eouts.new_zeros(bs, max_len + 1, enc_dim)\n        aws = eouts.new_zeros(bs, 1, max_len + 1, xtime)\n        n_tokens = torch.zeros(bs, dtype=torch.int32)\n        state = eouts.new_zeros(bs, self.channel)\n        alpha_accum = eouts.new_zeros(bs)\n        for t in range(xtime):\n            alpha_accum += alpha_norm[:, t]\n\n            for b in range(bs):\n                # skip the padding region\n                if t > elens[b] - 1:\n                    continue\n                # skip all-fired utterance\n                if ylens is not None and n_tokens[b] >= ylens[b].item():\n                    continue\n                if alpha_accum[b] >= self.threshold:\n                    # fire\n                    ak1 = 1 - alpha_accum[b]\n                    ak2 = alpha_norm[b, t] - ak1\n                    aws[b, 0, n_tokens[b], t] += ak1\n                    eouts_fired[b, n_tokens[b]] = state[b] + ak1 * eouts[b, t]\n                    n_tokens[b] += 1\n                    # Carry over to the next frame\n                    state[b] = ak2 * eouts[b, t]\n                    alpha_accum[b] = ak2\n                    aws[b, 0, n_tokens[b], t] += ak2\n                else:\n                    # Carry over to the next frame\n                    state[b] += alpha_norm[b, t] * eouts[b, t]\n                    aws[b, 0, n_tokens[b], t] += alpha_norm[b, t]\n\n            # tail of target sequence\n            if ylens is None and t == elens[b] - 1:\n                if alpha_accum[b] >= 0.5:\n                    n_tokens[b] += 1\n                    eouts_fired[b, n_tokens[b]] = state[b]\n\n        # truncate\n        eouts_fired = eouts_fired[:, :max_len]\n        aws = aws[:, :max_len]\n\n        return eouts_fired, alpha, aws\n'"
neural_sp/models/modules/gelu.py,4,"b'#! /usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n# Copyright 2019 Kyoto University (Hirofumi Inaguma)\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n\n""""""Gaussian Error Linear Units (GELU) activation.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport math\nimport torch\n\n\n# [reference] https://github.com/pytorch/fairseq/blob/e75cff5f2c1d62f12dc911e0bf420025eb1a4e33/fairseq/modules/gelu.py\ndef gelu_accurate(x):\n    if not hasattr(gelu_accurate, ""_a""):\n        gelu_accurate._a = math.sqrt(2 / math.pi)\n    return 0.5 * x * (1 + torch.tanh(gelu_accurate._a * (x + 0.044715 * torch.pow(x, 3))))\n\n\ndef gelu(x):\n    if hasattr(torch.nn.functional, \'gelu\'):\n        return torch.nn.functional.gelu(x.float()).type_as(x)\n    else:\n        return x * 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))\n'"
neural_sp/models/modules/glu.py,2,"b'#! /usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n# Copyright 2019 Kyoto University (Hirofumi Inaguma)\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n\n""""""Gated Linear Units (GLU) block.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom collections import OrderedDict\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass LinearGLUBlock(nn.Module):\n    """"""A linear GLU block.\n\n    Args:\n        size (int): input and output dimension\n\n    """"""\n\n    def __init__(self, size):\n        super().__init__()\n\n        self.fc = nn.Linear(size, size * 2)\n\n    def forward(self, xs):\n        return F.glu(self.fc(xs), dim=-1)\n\n\nclass ConvGLUBlock(nn.Module):\n    """"""A convolutional GLU block.\n\n    Args:\n        kernel_size (int): kernel size\n        in_ch (int): number of input channels\n        out_ch (int): number of output channels\n        bottlececk_dim (int): dimension of the bottleneck layers for computational efficiency\n        dropout (float): dropout probability\n\n    """"""\n\n    def __init__(self, kernel_size, in_ch, out_ch, bottlececk_dim=0, dropout=0.):\n        super().__init__()\n\n        self.conv_residual = None\n        if in_ch != out_ch:\n            self.conv_residual = nn.utils.weight_norm(\n                nn.Conv2d(in_channels=in_ch,\n                          out_channels=out_ch,\n                          kernel_size=(1, 1)), name=\'weight\', dim=0)\n            self.dropout_residual = nn.Dropout(p=dropout)\n\n        self.pad_left = nn.ConstantPad2d((0, 0, kernel_size - 1, 0), 0)\n\n        layers = OrderedDict()\n        if bottlececk_dim == 0:\n            layers[\'conv\'] = nn.utils.weight_norm(\n                nn.Conv2d(in_channels=in_ch,\n                          out_channels=out_ch * 2,\n                          kernel_size=(kernel_size, 1)), name=\'weight\', dim=0)\n            layers[\'dropout\'] = nn.Dropout(p=dropout)\n            layers[\'glu\'] = nn.GLU()\n\n        elif bottlececk_dim > 0:\n            layers[\'conv_in\'] = nn.utils.weight_norm(\n                nn.Conv2d(in_channels=in_ch,\n                          out_channels=bottlececk_dim,\n                          kernel_size=(1, 1)), name=\'weight\', dim=0)\n            layers[\'dropout_in\'] = nn.Dropout(p=dropout)\n            layers[\'conv_bottleneck\'] = nn.utils.weight_norm(\n                nn.Conv2d(in_channels=bottlececk_dim,\n                          out_channels=bottlececk_dim,\n                          kernel_size=(kernel_size, 1)), name=\'weight\', dim=0)\n            layers[\'dropout\'] = nn.Dropout(p=dropout)\n            layers[\'glu\'] = nn.GLU()\n            layers[\'conv_out\'] = nn.utils.weight_norm(\n                nn.Conv2d(in_channels=bottlececk_dim,\n                          out_channels=out_ch * 2,\n                          kernel_size=(1, 1)), name=\'weight\', dim=0)\n            layers[\'dropout_out\'] = nn.Dropout(p=dropout)\n\n        self.layers = nn.Sequential(layers)\n\n    def forward(self, xs):\n        """"""Forward computation.\n\n        Args:\n            xs (FloatTensor): `[B, in_ch, T, feat_dim]`\n        Returns:\n            out (FloatTensor): `[B, out_ch, T, feat_dim]`\n\n        """"""\n        residual = xs\n        if self.conv_residual is not None:\n            residual = self.dropout_residual(self.conv_residual(residual))\n        xs = self.pad_left(xs)  # `[B, embed_dim, T+kernel-1, 1]`\n        xs = self.layers(xs)  # `[B, out_ch * 2, T ,1]`\n        xs = xs + residual\n        return xs\n'"
neural_sp/models/modules/gmm_attention.py,9,"b'#! /usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n# Copyright 2019 Kyoto University (Hirofumi Inaguma)\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n\n""""""GMM attention.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport math\nimport numpy as np\nimport torch\nimport torch.nn as nn\n\nNEG_INF = float(np.finfo(np.float32).min)\n\n\nclass GMMAttention(nn.Module):\n    def __init__(self, kdim, qdim, adim, n_mixtures, vfloor=1e-6):\n        """"""GMM attention.\n\n        Args:\n            kdim (int): dimension of key\n            qdim (int): dimension of query\n            adim: (int) dimension of the attention layer\n            n_mixtures (int): number of mixtures\n            vfloor (float):\n\n        """"""\n        super(GMMAttention, self).__init__()\n\n        self.n_mix = n_mixtures\n        self.n_heads = 1  # dummy for attention plot\n        self.vfloor = vfloor\n        self.mask = None\n        self.myu = None\n\n        self.ffn_gamma = nn.Linear(qdim, n_mixtures)\n        self.ffn_beta = nn.Linear(qdim, n_mixtures)\n        self.ffn_kappa = nn.Linear(qdim, n_mixtures)\n\n    def reset(self):\n        self.mask = None\n        self.myu = None\n\n    def forward(self, key, value, query, mask=None, aw_prev=None,\n                cache=False, mode=\'\', trigger_point=None):\n        """"""Soft monotonic attention during training.\n\n        Args:\n            key (FloatTensor): `[B, klen, kdim]`\n            value (FloatTensor): `[B, klen, vdim]`\n            query (FloatTensor): `[B, 1, qdim]`\n            mask (ByteTensor): `[B, qmax, klen]`\n            aw_prev (FloatTensor): `[B, klen, 1]`\n            cache (bool): cache key and mask\n            mode: dummy interface for MoChA\n            trigger_point: dummy interface for MoChA\n        Returns:\n            cv (FloatTensor): `[B, 1, vdim]`\n            alpha (FloatTensor): `[B, klen, 1]`\n            beta: dummy interface for MoChA\n\n        """"""\n        bs, klen = key.size()[:2]\n\n        if self.myu is None:\n            myu_prev = key.new_zeros(bs, 1, self.n_mix)\n        else:\n            myu_prev = self.myu\n\n        self.mask = mask\n        if self.mask is None:\n            assert self.mask.size() == (bs, 1, klen), (self.mask.size(), (bs, 1, klen))\n\n        w = torch.softmax(self.ffn_gamma(query), dim=-1)  # `[B, 1, n_mix]`\n        v = torch.exp(self.ffn_beta(query))  # `[B, 1, n_mix]`\n        myu = torch.exp(self.ffn_kappa(query)) + myu_prev  # `[B, 1, n_mix]`\n        self.myu = myu  # register for the next step\n\n        # Compute attention weights\n        js = torch.arange(klen).unsqueeze(0).unsqueeze(2).repeat([bs, 1, self.n_mix]).float()\n        device_id = torch.cuda.device_of(next(self.parameters())).idx\n        if device_id >= 0:\n            js = js.cuda(device_id).float()\n        numerator = torch.exp(-torch.pow(js - myu, 2) / (2 * v + self.vfloor))\n        denominator = torch.pow(2 * math.pi * v + self.vfloor, 0.5)\n        aw = w * numerator / denominator  # `[B, klen, n_mix]`\n        aw = aw.sum(2).unsqueeze(1)  # `[B, 1, klen]`\n\n        # Compute context vector\n        if self.mask is not None:\n            aw = aw.masked_fill_(self.mask == 0, NEG_INF)\n        cv = torch.bmm(aw, value)\n\n        return cv, aw.unsqueeze(2), None\n'"
neural_sp/models/modules/initialization.py,1,"b'#! /usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n# Copyright 2020 Kyoto University (Hirofumi Inaguma)\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n\n""""""Parameter initialization.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport logging\nimport math\nimport torch.nn as nn\n\nlogger = logging.getLogger(__name__)\n\n\ndef init_like_transformer_xl(n, p, std):\n    # https://github.com/kimiyoung/transformer-xl/blob/44781ed21dbaec88b280f74d9ae2877f52b492a5/pytorch/train.py\n    if \'norm\' in n and \'weight\' in n:\n        assert p.dim() == 1\n        nn.init.normal_(p, 1.0, std)  # layer normalization\n        logger.info(\'Initialize %s with %s / (1.0, %.3f)\' % (n, \'normal\', std))\n    elif p.dim() == 1:\n        nn.init.constant_(p, 0.)  # bias\n        logger.info(\'Initialize %s with %s / %.3f\' % (n, \'constant\', 0.))\n    elif p.dim() == 2:\n        nn.init.normal_(p, mean=0, std=std)\n        logger.info(\'Initialize %s with %s / (0.0, %.3f)\' % (n, \'normal\', std))\n    else:\n        raise ValueError(n)\n\n\ndef init_with_xavier_uniform(n, p):\n    if p.dim() == 1:\n        nn.init.constant_(p, 0.)  # bias\n        logger.info(\'Initialize %s with %s / %.3f\' % (n, \'constant\', 0.))\n    elif p.dim() in [2, 3]:\n        nn.init.xavier_uniform_(p)  # linear layer\n        logger.info(\'Initialize %s with %s\' % (n, \'xavier_uniform\'))\n    else:\n        raise ValueError(n)\n\n\ndef init_with_lecun_normal(n, p, param_init):\n    if p.dim() == 1:\n        nn.init.constant_(p, 0.)  # bias\n        logger.info(\'Initialize %s with %s / %.3f\' % (n, \'constant\', 0.))\n    elif p.dim() == 2:\n        fan_in = p.size(1)\n        nn.init.normal_(p, mean=0., std=1. / math.sqrt(fan_in))  # linear weight\n        logger.info(\'Initialize %s with %s / %.3f\' % (n, \'lecun\', param_init))\n    elif p.dim() == 3:\n        fan_in = p.size(1) * p[0][0].numel()\n        nn.init.normal_(p, mean=0., std=1. / math.sqrt(fan_in))  # 1d conv weight\n        logger.info(\'Initialize %s with %s / %.3f\' % (n, \'lecun\', param_init))\n    elif p.dim() == 4:\n        fan_in = p.size(1) * p[0][0].numel()\n        nn.init.normal_(p, mean=0., std=1. / math.sqrt(fan_in))  # 2d conv weight\n        logger.info(\'Initialize %s with %s / %.3f\' % (n, \'lecun\', param_init))\n    else:\n        raise ValueError(n)\n'"
neural_sp/models/modules/mocha.py,35,"b'#! /usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n# Copyright 2019 Kyoto University (Hirofumi Inaguma)\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n\n""""""Monotonic (multihead) chunkwise atteniton.""""""\n\n# [reference]\n# https://github.com/j-min/MoChA-pytorch/blob/94b54a7fa13e4ac6dc255b509dd0febc8c0a0ee6/attention.py\n\nimport logging\nimport math\nimport numpy as np\nimport random\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom neural_sp.models.modules.causal_conv import CausalConv1d\nfrom neural_sp.models.modules.initialization import init_with_xavier_uniform\n\nrandom.seed(1)\n\nNEG_INF = float(np.finfo(np.float32).min)\n\nlogger = logging.getLogger(__name__)\n\n\nclass MonotonicEnergy(nn.Module):\n    def __init__(self, kdim, qdim, adim, atype, n_heads, init_r,\n                 bias=True, param_init=\'\', conv1d=False, conv_kernel_size=5):\n        """"""Energy function for the monotonic attenion.\n\n        Args:\n            kdim (int): dimension of key\n            qdim (int): dimension of quary\n            adim (int): dimension of attention space\n            atype (str): type of attention mechanism\n            n_heads (int): number of monotonic attention heads\n            init_r (int): initial value for offset r\n            bias (bool): use bias term in linear layers\n            param_init (str): parameter initialization method\n            conv1d (bool): use 1D causal convolution for energy calculation\n            conv_kernel_size (int): kernel size for 1D convolution\n\n        """"""\n        super().__init__()\n\n        assert conv_kernel_size % 2 == 1, ""Kernel size should be odd for \'same\' conv.""\n        self.key = None\n        self.mask = None\n\n        self.atype = atype\n        assert adim % n_heads == 0\n        self.d_k = adim // n_heads\n        self.n_heads = n_heads\n        self.scale = math.sqrt(adim)\n\n        if atype == \'add\':\n            self.w_key = nn.Linear(kdim, adim)\n            self.v = nn.Linear(adim, n_heads, bias=False)\n            self.w_query = nn.Linear(qdim, adim, bias=False)\n        elif atype == \'scaled_dot\':\n            self.w_key = nn.Linear(kdim, adim, bias=bias)\n            self.w_query = nn.Linear(qdim, adim, bias=bias)\n        else:\n            raise NotImplementedError(atype)\n\n        self.r = nn.Parameter(torch.Tensor([init_r]))\n        logger.info(\'init_r is initialized with %d\' % init_r)\n\n        self.conv1d = None\n        if conv1d:\n            self.conv1d = CausalConv1d(in_channels=kdim,\n                                       out_channels=kdim,\n                                       kernel_size=conv_kernel_size)\n            # padding=(conv_kernel_size - 1) // 2\n\n        if atype == \'add\':\n            self.v = nn.utils.weight_norm(self.v, name=\'weight\', dim=0)\n            # initialization\n            self.v.weight_g.data = torch.Tensor([1 / adim]).sqrt()\n        elif atype == \'scaled_dot\':\n            if param_init == \'xavier_uniform\':\n                self.reset_parameters(bias)\n\n    def reset_parameters(self, bias):\n        """"""Initialize parameters with Xavier uniform distribution.""""""\n        logger.info(\'===== Initialize %s with Xavier uniform distribution =====\' % self.__class__.__name__)\n        # NOTE: see https://github.com/pytorch/fairseq/blob/master/fairseq/modules/multihead_attention.py\n        nn.init.xavier_uniform_(self.w_key.weight, gain=1 / math.sqrt(2))\n        nn.init.xavier_uniform_(self.w_query.weight, gain=1 / math.sqrt(2))\n        if bias:\n            nn.init.constant_(self.w_key.bias, 0.)\n            nn.init.constant_(self.w_query.bias, 0.)\n        if self.conv1d is not None:\n            logger.info(\'===== Initialize %s with Xavier uniform distribution =====\' % self.conv1d.__class__.__name__)\n            for n, p in self.conv1d.named_parameters():\n                init_with_xavier_uniform(n, p)\n\n    def reset(self):\n        self.key = None\n        self.mask = None\n\n    def forward(self, key, query, mask, cache=False, boundary_leftmost=0):\n        """"""Compute monotonic energy.\n\n        Args:\n            key (FloatTensor): `[B, klen, kdim]`\n            query (FloatTensor): `[B, qlen, qdim]`\n            mask (ByteTensor): `[B, qlen, klen]`\n            cache (bool): cache key and mask\n        Returns:\n            e (FloatTensor): `[B, H_ma, qlen, klen]`\n\n        """"""\n        bs, klen, kdim = key.size()\n        qlen = query.size(1)\n\n        # Pre-computation of encoder-side features for computing scores\n        if self.key is None or not cache:\n            # 1d conv\n            if self.conv1d is not None:\n                key = torch.relu(self.conv1d(key))\n            key = self.w_key(key).view(bs, -1, self.n_heads, self.d_k)\n            self.key = key.transpose(2, 1).contiguous()  # `[B, H_ma, klen, d_k]`\n            self.mask = mask\n            if mask is not None:\n                self.mask = self.mask.unsqueeze(1).repeat([1, self.n_heads, 1, 1])  # `[B, H_ma, qlen, klen]`\n                assert self.mask.size() == (bs, self.n_heads, qlen, klen), \\\n                    (self.mask.size(), (bs, self.n_heads, qlen, klen))\n\n        query = self.w_query(query).view(bs, -1, self.n_heads, self.d_k)\n        query = query.transpose(2, 1).contiguous()  # `[B, H_ma, qlen, d_k]`\n        m = self.mask\n\n        if self.atype == \'add\':\n            k = self.key.unsqueeze(2)  # `[B, H_ma, 1, klen, d_k]`\n            # Truncate encoder memories\n            if boundary_leftmost > 0:\n                k = k[:, :, :, boundary_leftmost:]\n                klen = k.size(3)\n                if m is not None:\n                    m = m[:, :, :, boundary_leftmost:]\n            e = torch.relu(k + query.unsqueeze(3))  # `[B, H_ma, qlen, klen, d_k]`\n            e = e.permute(0, 2, 3, 1, 4).contiguous().view(bs, qlen, klen, -1)\n            e = self.v(e).permute(0, 3, 1, 2)  # `[B, qlen, klen, H_ma]`\n        elif self.atype == \'scaled_dot\':\n            k = self.key.transpose(3, 2)\n            e = torch.matmul(query, k) / self.scale\n\n        if self.r is not None:\n            e = e + self.r\n        if m is not None:\n            e = e.masked_fill_(m == 0, NEG_INF)\n        assert e.size() == (bs, self.n_heads, qlen, klen), \\\n            (e.size(), (bs, self.n_heads, qlen, klen))\n        return e\n\n\nclass ChunkEnergy(nn.Module):\n    def __init__(self, kdim, qdim, adim, atype, n_heads=1,\n                 bias=True, param_init=\'\'):\n        """"""Energy function for the chunkwise attention.\n\n        Args:\n            kdim (int): dimension of key\n            qdim (int): dimension of quary\n            adim (int): dimension of attention space\n            atype (str): type of attention mechanism\n            n_heads (int): number of chunkwise attention heads\n            bias (bool): use bias term in linear layers\n            param_init (str): parameter initialization method\n\n        """"""\n        super().__init__()\n\n        self.key = None\n        self.mask = None\n\n        self.atype = atype\n        assert adim % n_heads == 0\n        self.d_k = adim // n_heads\n        self.n_heads = n_heads\n        self.scale = math.sqrt(adim)\n\n        if atype == \'add\':\n            self.w_key = nn.Linear(kdim, adim)\n            self.w_query = nn.Linear(qdim, adim, bias=False)\n            self.v = nn.Linear(adim, n_heads, bias=False)\n        elif atype == \'scaled_dot\':\n            self.w_key = nn.Linear(kdim, adim, bias=bias)\n            self.w_query = nn.Linear(qdim, adim, bias=bias)\n            if param_init == \'xavier_uniform\':\n                self.reset_parameters(bias)\n        else:\n            raise NotImplementedError(atype)\n\n    def reset_parameters(self, bias):\n        """"""Initialize parameters with Xavier uniform distribution.""""""\n        logger.info(\'===== Initialize %s with Xavier uniform distribution =====\' % self.__class__.__name__)\n        # NOTE: see https://github.com/pytorch/fairseq/blob/master/fairseq/modules/multihead_attention.py\n        nn.init.xavier_uniform_(self.w_key.weight, gain=1 / math.sqrt(2))\n        nn.init.xavier_uniform_(self.w_query.weight, gain=1 / math.sqrt(2))\n        if bias:\n            nn.init.constant_(self.w_key.bias, 0.)\n            nn.init.constant_(self.w_query.bias, 0.)\n\n    def reset(self):\n        self.key = None\n        self.mask = None\n\n    def forward(self, key, query, mask, cache=False,\n                boundary_leftmost=0, boundary_rightmost=10e6):\n        """"""Compute chunkwise energy.\n\n        Args:\n            key (FloatTensor): `[B, klen, kdim]`\n            query (FloatTensor): `[B, qlen, qdim]`\n            mask (ByteTensor): `[B, qlen, klen]`\n            cache (bool): cache key and mask\n        Returns:\n            e (FloatTensor): `[B, H_ca, qlen, klen]`\n\n        """"""\n        bs, klen, kdim = key.size()\n        qlen = query.size(1)\n\n        # Pre-computation of encoder-side features for computing scores\n        if self.key is None or not cache:\n            key = self.w_key(key).view(bs, -1, self.n_heads, self.d_k)\n            self.key = key.transpose(2, 1).contiguous()  # `[B, H_ca, klen, d_k]`\n            self.mask = mask\n            if mask is not None:\n                self.mask = self.mask.unsqueeze(1).repeat([1, self.n_heads, 1, 1])  # `[B, H_ca, qlen, klen]`\n                assert self.mask.size() == (bs, self.n_heads, qlen, klen), \\\n                    (self.mask.size(), (bs, self.n_heads, qlen, klen))\n\n        query = self.w_query(query).view(bs, -1, self.n_heads, self.d_k)\n        query = query.transpose(2, 1).contiguous()  # `[B, H_ca, qlen, d_k]`\n        m = self.mask\n\n        if self.atype == \'add\':\n            k = self.key.unsqueeze(2)  # `[B, H_ca, 1, klen, d_k]`\n            # Truncate\n            k = k[:, :, :, boundary_leftmost:boundary_rightmost]\n            klen = k.size(3)\n            if m is not None:\n                m = m[:, :, :, boundary_leftmost:boundary_rightmost]\n\n            r = torch.relu(k + query.unsqueeze(3))  # `[B, H_ca, qlen, klen, d_k]`\n            r = r.permute(0, 2, 3, 1, 4).contiguous().view(bs, qlen, klen, -1)  # `[B, qlen, klen, H_ca * d_k]`\n            r = self.v(r).permute(0, 3, 1, 2).contiguous()  # `[B, H_ca, qlen, klen]`\n        elif self.atype == \'scaled_dot\':\n            k = self.key.transpose(3, 2)\n            r = torch.matmul(query, k) / self.scale\n\n        if m is not None:\n            r = r.masked_fill_(m == 0, NEG_INF)\n        assert r.size() == (bs, self.n_heads, qlen, klen), \\\n            (r.size(), (bs, self.n_heads, qlen, klen))\n        return r\n\n\nclass MoChA(nn.Module):\n    def __init__(self, kdim, qdim, adim, odim, atype, chunk_size,\n                 n_heads_mono=1, n_heads_chunk=1,\n                 conv1d=False, init_r=-4, eps=1e-6, noise_std=1.0,\n                 no_denominator=False, sharpening_factor=1.0,\n                 dropout=0., dropout_head=0., bias=True, param_init=\'\',\n                 decot=False, lookahead=2, share_chunkwise_attention=False):\n        """"""Monotonic (multihead) chunkwise attention.\n\n            if chunk_size == 1, this is equivalent to Hard monotonic attention\n                ""Online and Linear-Time Attention by Enforcing Monotonic Alignment"" (ICML 2017)\n                    https://arxiv.org/abs/1704.00784\n            if chunk_size > 1, this is equivalent to monotonic chunkwise attention (MoChA)\n                ""Monotonic Chunkwise Attention"" (ICLR 2018)\n                    https://openreview.net/forum?id=Hko85plCW\n            if chunk_size == -1, this is equivalent to Monotonic infinite lookback attention (Milk)\n                ""Monotonic Infinite Lookback Attention for Simultaneous Machine Translation"" (ACL 2019)\n                    https://arxiv.org/abs/1906.05218\n            if chunk_size == 1 and n_heads_mono>1, this is equivalent to Monotonic Multihead Attention (MMA)-hard\n                ""Monotonic Multihead Attention"" (ICLR 2020)\n                    https://openreview.net/forum?id=Hyg96gBKPS\n            if chunk_size == -1 and n_heads_mono>1, this is equivalent to Monotonic Multihead Attention (MMA)-Ilk\n                ""Monotonic Multihead Attention"" (ICLR 2020)\n                    https://openreview.net/forum?id=Hyg96gBKPS\n\n        Args:\n            kdim (int): dimension of key\n            qdim (int): dimension of query\n            adim: (int) dimension of the attention layer\n            odim: (int) dimension of output\n            atype (str): type of attention mechanism\n            chunk_size (int): window size for chunkwise attention\n            n_heads_mono (int): number of heads for monotonic attention\n            n_heads_chunk (int): number of heads for chunkwise attention\n            conv1d (bool): apply 1d convolution for energy calculation\n            init_r (int): initial value for parameter \'r\' used for monotonic attention\n            eps (float): epsilon parameter to avoid zero division\n            noise_std (float): standard deviation for input noise\n            no_denominator (bool): set the denominator to 1 in the alpha recurrence\n            sharpening_factor (float): sharping factor for beta calculation\n            dropout (float): dropout probability for attention weights\n            dropout_head (float): HeadDrop probability\n            bias (bool): use bias term in linear layers\n            param_init (str): parameter initialization method\n            decot (bool): delay constrainted training (DeCoT)\n            lookahead (int): lookahead frames for DeCoT\n            share_chunkwise_attention (int): share CA heads among MA heads\n\n        """"""\n        super(MoChA, self).__init__()\n\n        self.atype = atype\n        assert adim % (n_heads_mono * n_heads_chunk) == 0\n        self.d_k = adim // (n_heads_mono * n_heads_chunk)\n\n        self.w = chunk_size\n        self.milk = (chunk_size == -1)\n        self.n_heads = n_heads_mono\n        self.n_heads_mono = n_heads_mono\n        self.n_heads_chunk = n_heads_chunk\n        self.eps = eps\n        self.noise_std = noise_std\n        self.no_denom = no_denominator\n        self.sharpening_factor = sharpening_factor\n        self.decot = decot\n        self.lookahead = lookahead\n        self.share_chunkwise_attention = share_chunkwise_attention\n\n        self.monotonic_energy = MonotonicEnergy(kdim, qdim, adim, atype,\n                                                n_heads_mono, init_r,\n                                                bias, param_init, conv1d=conv1d)\n        self.chunk_energy = ChunkEnergy(kdim, qdim, adim, atype,\n                                        n_heads_chunk if share_chunkwise_attention else n_heads_mono * n_heads_chunk,\n                                        bias, param_init) if chunk_size > 1 or self.milk else None\n        if n_heads_mono * n_heads_chunk > 1:\n            self.w_value = nn.Linear(kdim, adim, bias=bias)\n            self.w_out = nn.Linear(adim, odim, bias=bias)\n            if param_init == \'xavier_uniform\':\n                self.reset_parameters(bias)\n\n        # attention dropout\n        self.dropout_attn = nn.Dropout(p=dropout)  # for beta\n        self.dropout_head = dropout_head\n\n        self.bd_offset = 0\n\n    def reset_parameters(self, bias):\n        """"""Initialize parameters with Xavier uniform distribution.""""""\n        logger.info(\'===== Initialize %s with Xavier uniform distribution =====\' % self.__class__.__name__)\n        # NOTE: see https://github.com/pytorch/fairseq/blob/master/fairseq/modules/multihead_attention.py\n        nn.init.xavier_uniform_(self.w_value.weight, gain=1 / math.sqrt(2))\n        if bias:\n            nn.init.constant_(self.w_value.bias, 0.)\n\n        nn.init.xavier_uniform_(self.w_out.weight)\n        if bias:\n            nn.init.constant_(self.w_out.bias, 0.)\n\n    def reset(self):\n        self.monotonic_energy.reset()\n        if self.chunk_energy is not None:\n            self.chunk_energy.reset()\n        self.bd_offset = 0\n\n    def forward(self, key, value, query, mask=None, aw_prev=None,\n                mode=\'hard\', cache=False, trigger_point=None,\n                eps_wait=-1, efficient_decoding=False):\n        """"""Forward pass.\n\n        Args:\n            key (FloatTensor): `[B, klen, kdim]`\n            value (FloatTensor): `[B, klen, vdim]`\n            query (FloatTensor): `[B, qlen, qdim]`\n            mask (ByteTensor): `[B, qlen, klen]`\n            aw_prev (FloatTensor): `[B, H, 1, klen]`\n            mode (str): recursive/parallel/hard\n            cache (bool): cache key and mask\n            trigger_point (IntTensor): `[B]`\n            eps_wait (int): wait time delay for head-synchronous decoding in MMA\n        Returns:\n            cv (FloatTensor): `[B, qlen, vdim]`\n            alpha (FloatTensor): `[B, H_ma, qlen, klen]`\n            beta (FloatTensor): `[B, H_ma * H_ca, qlen, klen]`\n\n        """"""\n        bs, klen = key.size()[:2]\n        qlen = query.size(1)\n\n        if aw_prev is None:\n            # aw_prev = [1, 0, 0 ... 0]\n            aw_prev = key.new_zeros(bs, self.n_heads_mono, 1, klen)\n            aw_prev[:, :, :, 0:1] = key.new_ones(bs, self.n_heads_mono, 1, 1)\n\n        # Compute monotonic energy\n        e_mono = self.monotonic_energy(key, query, mask, cache=cache,\n                                       boundary_leftmost=self.bd_offset)  # `[B, H_ma, qlen, klen]`\n        assert e_mono.size(3) + self.bd_offset == key.size(1)\n\n        if mode == \'recursive\':  # training\n            p_choose = torch.sigmoid(add_gaussian_noise(e_mono, self.noise_std))  # `[B, H_ma, qlen, klen]`\n            alpha = []\n            for i in range(qlen):\n                # Compute [1, 1 - p_choose[0], 1 - p_choose[1], ..., 1 - p_choose[-2]]\n                shifted_1mp_choose = torch.cat([key.new_ones(bs, self.n_heads_mono, 1, 1),\n                                                1 - p_choose[:, :, i:i + 1, :-1]], dim=-1)\n                # Compute attention distribution recursively as\n                # q_j = (1 - p_choose_j) * q_(j-1) + aw_prev_j\n                # alpha_j = p_choose_j * q_j\n                q = key.new_zeros(bs, self.n_heads_mono, 1, klen + 1)\n                for j in range(klen):\n                    q[:, :, i:i + 1, j + 1] = shifted_1mp_choose[:, :, i:i + 1, j].clone() * q[:, :, i:i + 1, j].clone() + \\\n                        aw_prev[:, :, :, j].clone()\n                aw_prev = p_choose[:, :, i:i + 1] * q[:, :, i:i + 1, 1:]  # `[B, H_ma, 1, klen]`\n                alpha.append(aw_prev)\n            alpha = torch.cat(alpha, dim=2) if qlen > 1 else alpha[-1]  # `[B, H_ma, qlen, klen]`\n            alpha_masked = alpha.clone()\n\n        elif mode == \'parallel\':  # training\n            p_choose = torch.sigmoid(add_gaussian_noise(e_mono, self.noise_std))  # `[B, H_ma, qlen, klen]`\n            # safe_cumprod computes cumprod in logspace with numeric checks\n            cumprod_1mp_choose = safe_cumprod(1 - p_choose, eps=self.eps)  # `[B, H_ma, qlen, klen]`\n            # Compute recurrence relation solution\n            alpha = []\n            for i in range(qlen):\n                denom = 1 if self.no_denom else torch.clamp(cumprod_1mp_choose[:, :, i:i + 1], min=self.eps, max=1.0)\n                aw_prev = p_choose[:, :, i:i + 1] * cumprod_1mp_choose[:, :, i:i + 1] * torch.cumsum(\n                    aw_prev / denom, dim=-1)  # `[B, H_ma, 1, klen]`\n                # Mask the right part from the trigger point\n                if self.decot and trigger_point is not None:\n                    for b in range(bs):\n                        aw_prev[b, :, :, trigger_point[b] + self.lookahead + 1:] = 0\n                alpha.append(aw_prev)\n\n            alpha = torch.cat(alpha, dim=2) if qlen > 1 else alpha[-1]  # `[B, H_ma, qlen, klen]`\n            alpha_masked = alpha.clone()\n\n            # mask out each head independently (HeadDrop)\n            if self.dropout_head > 0 and self.training:\n                n_effective_heads = self.n_heads_mono\n                head_mask = alpha.new_ones(alpha.size()).byte()\n                for h in range(self.n_heads_mono):\n                    if random.random() < self.dropout_head:\n                        head_mask[:, h] = 0\n                        n_effective_heads -= 1\n                alpha_masked = alpha_masked.masked_fill_(head_mask == 0, 0)\n                # Normalization\n                if n_effective_heads > 0:\n                    alpha_masked = alpha_masked * (self.n_heads_mono / n_effective_heads)\n\n        elif mode == \'hard\':  # inference\n            assert qlen == 1\n            assert not self.training\n            if self.n_heads_mono == 1:\n                # assert aw_prev.sum() > 0\n                p_choose_i = (torch.sigmoid(e_mono) >= 0.5).float()[:, :, 0:1]\n                # Attend when monotonic energy is above threshold (Sigmoid > 0.5)\n                # Remove any probabilities before the index chosen at the last time step\n                p_choose_i *= torch.cumsum(\n                    aw_prev[:, :, 0:1, -e_mono.size(3):], dim=-1)  # `[B, H_ma, 1 (qlen), klen]`\n                # Now, use exclusive cumprod to remove probabilities after the first\n                # chosen index, like so:\n                # p_choose_i                        = [0, 0, 0, 1, 1, 0, 1, 1]\n                # 1 - p_choose_i                    = [1, 1, 1, 0, 0, 1, 0, 0]\n                # exclusive_cumprod(1 - p_choose_i) = [1, 1, 1, 1, 0, 0, 0, 0]\n                # alpha: product of above           = [0, 0, 0, 1, 0, 0, 0, 0]\n                alpha = p_choose_i * exclusive_cumprod(1 - p_choose_i)  # `[B, H_ma, 1 (qlen), klen]`\n            else:\n                p_choose_i = (torch.sigmoid(e_mono) >= 0.5).float()[:, :, 0:1]\n                # Attend when monotonic energy is above threshold (Sigmoid > 0.5)\n                # Remove any probabilities before the index chosen at the last time step\n                p_choose_i *= torch.cumsum(aw_prev[:, :, 0:1], dim=-1)  # `[B, H_ma, 1 (qlen), klen]`\n                # Now, use exclusive cumprod to remove probabilities after the first\n                # chosen index, like so:\n                # p_choose_i                        = [0, 0, 0, 1, 1, 0, 1, 1]\n                # 1 - p_choose_i                    = [1, 1, 1, 0, 0, 1, 0, 0]\n                # exclusive_cumprod(1 - p_choose_i) = [1, 1, 1, 1, 0, 0, 0, 0]\n                # alpha: product of above           = [0, 0, 0, 1, 0, 0, 0, 0]\n                alpha = p_choose_i * exclusive_cumprod(1 - p_choose_i)  # `[B, H_ma, 1 (qlen), klen]`\n\n            if eps_wait > 0:\n                for b in range(bs):\n                    # no boundary until the last frame for all heads\n                    if alpha[b].sum() == 0:\n                        continue\n\n                    leftmost = alpha[b, :, 0].nonzero()[:, -1].min().item()\n                    rightmost = alpha[b, :, 0].nonzero()[:, -1].max().item()\n                    for h in range(self.n_heads_mono):\n                        # no bondary at the h-th head\n                        if alpha[b, h, 0].sum().item() == 0:\n                            alpha[b, h, 0, min(rightmost, leftmost + eps_wait)] = 1\n                            continue\n\n                        # surpass acceptable latency\n                        if alpha[b, h, 0].nonzero()[:, -1].min().item() >= leftmost + eps_wait:\n                            alpha[b, h, 0, :] = 0  # reset\n                            alpha[b, h, 0, leftmost + eps_wait] = 1\n\n            alpha_masked = alpha.clone()\n\n        else:\n            raise ValueError(""mode must be \'recursive\', \'parallel\', or \'hard\'."")\n\n        # Compute chunk energy\n        beta = None\n        if self.w > 1 or self.milk:\n            bd_leftmost = 0\n            bd_rightmost = klen - 1 - self.bd_offset\n            if efficient_decoding and mode == \'hard\' and alpha.sum() > 0:\n                bd_leftmost = alpha[:, :, 0].nonzero()[:, -1].min().item()\n                bd_rightmost = alpha[:, :, 0].nonzero()[:, -1].max().item()\n                if bd_leftmost == bd_rightmost:\n                    alpha_masked = alpha_masked[:, :, :, bd_leftmost:bd_leftmost + 1]\n                else:\n                    alpha_masked = alpha_masked[:, :, :, bd_leftmost:bd_rightmost]\n\n            e_chunk = self.chunk_energy(key, query, mask, cache=cache,\n                                        boundary_leftmost=max(0, self.bd_offset + bd_leftmost - self.w + 1),\n                                        boundary_rightmost=self.bd_offset + bd_rightmost + 1)  # `[B, (H_ma*)H_ca, qlen, ken]`\n\n            # padding\n            additional = e_chunk.size(3) - alpha_masked.size(3)\n            if efficient_decoding and mode == \'hard\':\n                alpha = torch.cat([alpha.new_zeros(bs, alpha.size(1), 1, klen - alpha.size(3)), alpha], dim=3)\n                if additional > 0:\n                    alpha_masked = torch.cat([alpha_masked.new_zeros(bs, alpha_masked.size(1), 1, additional),\n                                              alpha_masked], dim=3)\n\n            if mode == \'hard\':\n                beta = hard_chunkwise_attention(alpha_masked, e_chunk, mask, self.w,\n                                                self.n_heads_chunk, self.sharpening_factor,\n                                                self.share_chunkwise_attention)\n\n            else:\n                beta = efficient_chunkwise_attention(alpha_masked, e_chunk, mask, self.w,\n                                                     self.n_heads_chunk, self.sharpening_factor,\n                                                     self.share_chunkwise_attention)\n            beta = self.dropout_attn(beta)  # `[B, H_ma * H_ca, qlen, klen]`\n\n            if efficient_decoding and mode == \'hard\':\n                value = value[:, max(0, self.bd_offset + bd_leftmost - self.w + 1):self.bd_offset + bd_rightmost + 1]\n\n        # Update after calculating beta\n        bd_offset_old = self.bd_offset\n        if efficient_decoding and mode == \'hard\' and alpha.sum() > 0:\n            self.bd_offset += alpha[:, :, 0, self.bd_offset:].nonzero()[:, -1].min().item()\n\n        # Compute context vector\n        if self.n_heads_mono * self.n_heads_chunk > 1:\n            value = self.w_value(value).view(bs, -1, self.n_heads_mono * self.n_heads_chunk, self.d_k)\n            value = value.transpose(2, 1).contiguous()  # `[B, H_ma * H_ca, klen, d_k]`\n            if self.w == 1:\n                cv = torch.matmul(alpha, value)  # `[B, H_ma, qlen, d_k]`\n            else:\n                cv = torch.matmul(beta, value)  # `[B, H_ma * H_ca, qlen, d_k]`\n            cv = cv.transpose(2, 1).contiguous().view(bs, -1, self.n_heads_mono * self.n_heads_chunk * self.d_k)\n            cv = self.w_out(cv)  # `[B, qlen, adim]`\n        else:\n            if self.w == 1:\n                cv = torch.bmm(alpha.squeeze(1), value)  # `[B, 1, adim]`\n            else:\n                cv = torch.bmm(beta.squeeze(1), value)  # `[B, 1, adim]`\n\n        assert alpha.size() == (bs, self.n_heads_mono, qlen, klen), \\\n            (alpha.size(), (bs, self.n_heads_mono, qlen, klen))\n        if self.w > 1 or self.milk:\n            _w = max(1, (bd_offset_old + bd_rightmost + 1) - max(0, bd_offset_old + bd_leftmost - self.w + 1))\n            # assert beta.size() == (bs, self.n_heads_mono * self.n_heads_chunk, qlen, e_chunk.size(3) + additional), \\\n            #     (beta.size(), (bs, self.n_heads_mono * self.n_heads_chunk, qlen, e_chunk.size(3) + additional))\n            assert beta.size() == (bs, self.n_heads_mono * self.n_heads_chunk, qlen, _w), \\\n                (beta.size(), (bs, self.n_heads_mono * self.n_heads_chunk, qlen, _w))\n            # TODO: padding for beta\n        return cv, alpha, beta\n\n\ndef add_gaussian_noise(xs, std):\n    """"""Additive gaussian nosie to encourage discreteness.""""""\n    noise = xs.new_zeros(xs.size()).normal_(std=std)\n    return xs + noise\n\n\ndef safe_cumprod(x, eps):\n    """"""Numerically stable cumulative product by cumulative sum in log-space.\n        Args:\n            x (FloatTensor): `[B, H, qlen, klen]`\n        Returns:\n            x (FloatTensor): `[B, H, qlen, klen]`\n\n    """"""\n    return torch.exp(exclusive_cumsum(torch.log(torch.clamp(x, min=eps, max=1.0))))\n\n\ndef exclusive_cumsum(x):\n    """"""Exclusive cumulative summation [a, b, c] => [0, a, a + b].\n\n        Args:\n            x (FloatTensor): `[B, H, qlen, klen]`\n        Returns:\n            x (FloatTensor): `[B, H, qlen, klen]`\n\n    """"""\n    return torch.cumsum(torch.cat([x.new_zeros(x.size(0), x.size(1), x.size(2), 1),\n                                   x[:, :, :, :-1]], dim=-1), dim=-1)\n\n\ndef exclusive_cumprod(x):\n    """"""Exclusive cumulative product [a, b, c] => [1, a, a * b].\n\n        Args:\n            x (FloatTensor): `[B, H, qlen, klen]`\n        Returns:\n            x (FloatTensor): `[B, H, qlen, klen]`\n\n    """"""\n    return torch.cumprod(torch.cat([x.new_ones(x.size(0), x.size(1), x.size(2), 1),\n                                    x[:, :, :, :-1]], dim=-1), dim=-1)\n\n\ndef moving_sum(x, back, forward):\n    """"""Compute the moving sum of x over a chunk_size with the provided bounds.\n\n    Args:\n        x (FloatTensor): `[B, H_ma, H_ca, qlen, klen]`\n        back (int):\n        forward (int):\n\n    Returns:\n        x_sum (FloatTensor): `[B, H_ma, H_ca, qlen, klen]`\n\n    """"""\n    bs, n_heads_mono, n_heads_chunk, qlen, klen = x.size()\n    x = x.view(-1, klen)\n    # Moving sum is computed as a carefully-padded 1D convolution with ones\n    x_padded = F.pad(x, pad=[back, forward])  # `[B * H_ma * H_ca * qlen, back + klen + forward]`\n    # Add a ""channel"" dimension\n    x_padded = x_padded.unsqueeze(1)\n    # Construct filters\n    filters = x.new_ones(1, 1, back + forward + 1)\n    x_sum = F.conv1d(x_padded, filters)\n    x_sum = x_sum.squeeze(1).view(bs, n_heads_mono, n_heads_chunk, qlen, -1)\n    return x_sum\n\n\ndef efficient_chunkwise_attention(alpha, u, mask, chunk_size, n_heads_chunk,\n                                  sharpening_factor, share_chunkwise_attention):\n    """"""Compute chunkwise attention efficiently by clipping logits at training time.\n\n    Args:\n        alpha (FloatTensor): `[B, H_ma, qlen, klen]`\n        u (FloatTensor): `[B, (H_ma*)H_ca, qlen, klen]`\n        mask (ByteTensor): `[B, qlen, klen]`\n        chunk_size (int): window size for chunkwise attention\n        n_heads_chunk (int): number of chunkwise attention heads\n        sharpening_factor (float): sharping factor for beta calculation\n        share_chunkwise_attention (int): share CA heads among MA heads\n    Returns:\n        beta (FloatTensor): `[B, H_ma * H_ca, qlen, klen]`\n\n    """"""\n    bs, n_heads_mono, qlen, klen = alpha.size()\n    alpha = alpha.unsqueeze(2)  # `[B, H_ma, 1, qlen, klen]`\n    u = u.unsqueeze(1)  # `[B, 1, (H_ma*)H_ca, qlen, klen]`\n    if n_heads_chunk > 1:\n        alpha = alpha.repeat([1, 1, n_heads_chunk, 1, 1])\n    if n_heads_mono > 1 and not share_chunkwise_attention:\n        u = u.view(bs, n_heads_mono, n_heads_chunk, qlen, klen)\n    # Shift logits to avoid overflow\n    u -= torch.max(u, dim=-1, keepdim=True)[0]\n    # Limit the range for numerical stability\n    softmax_exp = torch.clamp(torch.exp(u), min=1e-5)\n    # Compute chunkwise softmax denominators\n    if chunk_size == -1:\n        # infinite lookback attention\n        # inner_items = alpha * sharpening_factor / torch.cumsum(softmax_exp, dim=-1)\n        # beta = softmax_exp * torch.cumsum(inner_items.flip(dims=[-1]), dim=-1).flip(dims=[-1])\n        # beta = beta.masked_fill(mask.unsqueeze(1), 0)\n        # beta = beta / beta.sum(dim=-1, keepdim=True)\n\n        softmax_denominators = torch.cumsum(softmax_exp, dim=-1)\n        # Compute \\beta_{i, :}. emit_probs are \\alpha_{i, :}.\n        beta = softmax_exp * moving_sum(alpha * sharpening_factor / softmax_denominators,\n                                        back=0, forward=klen - 1)\n    else:\n        softmax_denominators = moving_sum(softmax_exp,\n                                          back=chunk_size - 1, forward=0)\n        # Compute \\beta_{i, :}. emit_probs are \\alpha_{i, :}.\n        beta = softmax_exp * moving_sum(alpha * sharpening_factor / softmax_denominators,\n                                        back=0, forward=chunk_size - 1)\n    return beta.view(bs, -1, qlen, klen)\n\n\ndef hard_chunkwise_attention(alpha, u, mask, chunk_size, n_heads_chunk,\n                             sharpening_factor, share_chunkwise_attention):\n    """"""Compute chunkwise attention over hard attention at test time.\n\n    Args:\n        alpha (FloatTensor): `[B, H_ma, qlen, klen]`\n        u (FloatTensor): `[B, (H_ma*)H_ca, qlen, klen]`\n        mask (ByteTensor): `[B, qlen, klen]`\n        chunk_size (int): window size for chunkwise attention\n        n_heads_chunk (int): number of chunkwise attention heads\n        sharpening_factor (float): sharping factor for beta calculation\n        share_chunkwise_attention (int): share CA heads among MA heads\n    Returns:\n        beta (FloatTensor): `[B, H_ma * H_ca, qlen, klen]`\n\n    """"""\n    bs, n_heads_mono, qlen, klen = alpha.size()\n    alpha = alpha.unsqueeze(2)   # `[B, H_ma, 1, qlen, klen]`\n    u = u.unsqueeze(1)  # `[B, 1, (H_ma*)H_ca, qlen, klen]`\n    if n_heads_chunk > 1:\n        alpha = alpha.repeat([1, 1, n_heads_chunk, 1, 1])\n    if n_heads_mono > 1:\n        if share_chunkwise_attention:\n            u = u.repeat([1, n_heads_mono, 1, 1, 1])\n        else:\n            u = u.view(bs, n_heads_mono, n_heads_chunk, qlen, klen)\n\n    mask = alpha.clone().byte()  # `[B, H_ma, H_ca, qlen, klen]`\n    for b in range(bs):\n        for h in range(n_heads_mono):\n            if alpha[b, h, 0, 0].sum() > 0:\n                boundary = alpha[b, h, 0, 0].nonzero()[:, -1].min().item()\n                if chunk_size == -1:\n                    # infinite lookback attention\n                    mask[b, h, :, 0, 0:boundary + 1] = 1\n                else:\n                    mask[b, h, :, 0, max(0, boundary - chunk_size + 1):boundary + 1] = 1\n\n    u = u.masked_fill(mask == 0, NEG_INF)\n    beta = torch.softmax(u, dim=-1)\n    return beta.view(bs, -1, qlen, klen)\n'"
neural_sp/models/modules/multihead_attention.py,5,"b'#! /usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n# Copyright 2018 Kyoto University (Hirofumi Inaguma)\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n\n""""""Multi-head attention layer.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport logging\nimport math\nimport numpy as np\nimport random\nimport torch\nimport torch.nn as nn\n\nrandom.seed(1)\n\nNEG_INF = float(np.finfo(np.float32).min)\n\nlogger = logging.getLogger(__name__)\n\n\nclass MultiheadAttentionMechanism(nn.Module):\n    """"""Multi-headed attention layer.\n\n    Args:\n        kdim (int): dimension of key\n        qdim (int): dimension of query\n        adim: (int) dimension of the attention space\n        odim: (int) dimension of output\n        n_heads (int): number of heads\n        dropout (float): dropout probability for attenion weights\n        dropout_head (float): HeadDrop probability\n        atype (str): type of attention mechanism\n        bias (bool): use bias term in linear layers\n        param_init (str): parameter initialization method\n\n    """"""\n\n    def __init__(self, kdim, qdim, adim, odim, n_heads, dropout, dropout_head=0.,\n                 atype=\'scaled_dot\', bias=True, param_init=\'\'):\n        super(MultiheadAttentionMechanism, self).__init__()\n\n        self.atype = atype\n        assert adim % n_heads == 0\n        self.d_k = adim // n_heads\n        self.n_heads = n_heads\n        self.scale = math.sqrt(self.d_k)\n        self.reset()\n\n        self.dropout_attn = nn.Dropout(p=dropout)\n        self.dropout_head = dropout_head\n\n        if atype == \'scaled_dot\':\n            # for Transformer\n            self.w_key = nn.Linear(kdim, adim, bias=bias)\n            self.w_value = nn.Linear(kdim, adim, bias=bias)\n            self.w_query = nn.Linear(qdim, adim, bias=bias)\n        elif atype == \'add\':\n            # for LAS\n            self.w_key = nn.Linear(kdim, adim, bias=bias)\n            self.w_value = nn.Linear(kdim, adim, bias=bias)\n            self.w_query = nn.Linear(qdim, adim, bias=bias)\n            self.v = nn.Linear(adim, n_heads, bias=bias)\n        else:\n            raise NotImplementedError(atype)\n\n        self.w_out = nn.Linear(adim, odim, bias=bias)\n\n        if param_init == \'xavier_uniform\':\n            self.reset_parameters(bias)\n\n    def reset_parameters(self, bias):\n        """"""Initialize parameters with Xavier uniform distribution.""""""\n        logger.info(\'===== Initialize %s with Xavier uniform distribution =====\' % self.__class__.__name__)\n        # NOTE: see https://github.com/pytorch/fairseq/blob/master/fairseq/modules/multihead_attention.py\n        nn.init.xavier_uniform_(self.w_key.weight, gain=1 / math.sqrt(2))\n        nn.init.xavier_uniform_(self.w_value.weight, gain=1 / math.sqrt(2))\n        nn.init.xavier_uniform_(self.w_query.weight, gain=1 / math.sqrt(2))\n        if bias:\n            nn.init.constant_(self.w_key.bias, 0.)\n            nn.init.constant_(self.w_value.bias, 0.)\n            nn.init.constant_(self.w_query.bias, 0.)\n\n        nn.init.xavier_uniform_(self.w_out.weight)\n        if bias:\n            nn.init.constant_(self.w_out.bias, 0.)\n\n    def reset(self):\n        self.key = None\n        self.value = None\n        self.mask = None\n\n    def forward(self, key, value, query, mask, aw_prev=None,\n                cache=False, mode=\'\', trigger_point=None,\n                eps_wait=-1):\n        """"""Forward pass.\n\n        Args:\n            key (FloatTensor): `[B, klen, kdim]`\n            value (FloatTensor): `[B, klen, vdim]`\n            query (FloatTensor): `[B, qlen, qdim]`\n            mask (ByteTensor): `[B, qlen, klen]`\n            aw_prev: dummy interface\n            cache (bool): cache key, value, and mask\n            mode: dummy interface for MoChA\n            trigger_point: dummy interface for MoChA\n            eps_wait: dummy interface for MMA\n        Returns:\n            cv (FloatTensor): `[B, qlen, vdim]`\n            aw (FloatTensor): `[B, H, qlen, klen]`\n            beta: dummy interface for MoChA\n\n        """"""\n        bs, klen = key.size()[: 2]\n        qlen = query.size(1)\n\n        if self.key is None or not cache:\n            self.key = self.w_key(key).view(bs, -1, self.n_heads, self.d_k)  # `[B, klen, H, d_k]`\n            self.value = self.w_value(value).view(bs, -1, self.n_heads, self.d_k)  # `[B, klen, H, d_k]`\n            self.mask = mask\n            if self.mask is not None:\n                self.mask = self.mask.unsqueeze(3).repeat([1, 1, 1, self.n_heads])\n                assert self.mask.size() == (bs, qlen, klen, self.n_heads), \\\n                    (self.mask.size(), (bs, qlen, klen, self.n_heads))\n\n        query = self.w_query(query).view(bs, -1, self.n_heads, self.d_k)  # `[B, qlen, H, d_k]`\n\n        if self.atype == \'scaled_dot\':\n            e = torch.einsum(""bihd,bjhd->bijh"", (query, self.key)) / self.scale  # `[B, qlen, klen, H]`\n        elif self.atype == \'add\':\n            key = self.key.unsqueeze(1)  # `[B, 1, klen, H, d_k]`\n            query = query.unsqueeze(2)  # `[B, qlen, 1, H, d_k]`\n            tmp = torch.tanh(key + query).view(bs, qlen, klen, -1)  # `[B, qlen, klen, H, d_k]`\n            e = self.v(tmp)  # `[B, qlen, klen, H]`\n\n        # Compute attention weights\n        if self.mask is not None:\n            e = e.masked_fill_(self.mask == 0, NEG_INF)  # `[B, qlen, klen, H]`\n        aw = torch.softmax(e, dim=2)\n        aw = self.dropout_attn(aw)\n        aw_masked = aw.clone()\n\n        # mask out each head independently (HeadDrop)\n        if self.dropout_head > 0 and self.training:\n            n_effective_heads = self.n_heads\n            head_mask = aw.new_ones(aw.size()).byte()  # `[B, qlen, klen, H]`\n            for h in range(self.n_heads):\n                if random.random() < self.dropout_head:\n                    head_mask[:, :, :, h] = 0\n                    n_effective_heads -= 1\n            aw_masked = aw_masked.masked_fill_(head_mask == 0, 0)\n            # Normalization\n            if n_effective_heads > 0:\n                aw_masked = aw_masked * (self.n_heads / n_effective_heads)\n\n        cv = torch.einsum(""bijh,bjhd->bihd"", (aw_masked, self.value))  # `[B, qlen, H, d_k]`\n        cv = cv.contiguous().view(bs, -1, self.n_heads * self.d_k)  # `[B, qlen, H * d_k]`\n        cv = self.w_out(cv)\n        aw = aw.permute(0, 3, 1, 2)  # `[B, H, qlen, klen]`\n\n        return cv, aw, None\n'"
neural_sp/models/modules/positional_embedding.py,10,"b'#! /usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n# Copyright 2020 Kyoto University (Hirofumi Inaguma)\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n\n""""""Positional Embeddings.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport copy\nimport logging\nimport math\nimport numpy as np\nimport torch\nimport torch.nn as nn\n\nfrom neural_sp.models.modules.causal_conv import CausalConv1d\nfrom neural_sp.models.modules.initialization import init_with_xavier_uniform\n\n\nNEG_INF = float(np.finfo(np.float32).min)\n\nlogger = logging.getLogger(__name__)\n\n\nclass PositionalEncoding(nn.Module):\n    """"""Positional encoding for Transformer.\n\n    Args:\n        d_model (int): dimension of MultiheadAttentionMechanism\n        dropout (float): dropout probability\n        pe_type (str): type of positional encoding\n        param_init (str): parameter initialization method\n        max_len (int):\n        conv_kernel_size (int):\n        layer_norm_eps (float):\n\n    """"""\n\n    def __init__(self, d_model, dropout, pe_type, param_init, max_len=5000,\n                 conv_kernel_size=3, layer_norm_eps=1e-12):\n        super(PositionalEncoding, self).__init__()\n\n        self.d_model = d_model\n        self.pe_type = pe_type\n        self.scale = math.sqrt(self.d_model)\n\n        if \'1dconv\' in pe_type:\n            causal_conv1d = CausalConv1d(in_channels=d_model,\n                                         out_channels=d_model,\n                                         kernel_size=conv_kernel_size)\n            layers = []\n            conv_nlayers = int(pe_type.replace(\'1dconv\', \'\')[0])\n            for l in range(conv_nlayers):\n                layers.append(copy.deepcopy(causal_conv1d))\n                layers.append(nn.LayerNorm(d_model, eps=layer_norm_eps))\n                layers.append(nn.ReLU())\n                layers.append(nn.Dropout(p=dropout))\n            self.pe = nn.Sequential(*layers)\n\n            if param_init == \'xavier_uniform\':\n                self.reset_parameters()\n\n        elif pe_type != \'none\':\n            # Compute the positional encodings once in log space.\n            pe = torch.zeros(max_len, d_model, dtype=torch.float32)\n            position = torch.arange(0, max_len, dtype=torch.float32).unsqueeze(1)\n            div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model))\n            pe[:, 0::2] = torch.sin(position * div_term)\n            pe[:, 1::2] = torch.cos(position * div_term)\n            pe = pe.unsqueeze(0)  # for batch dimension\n            self.register_buffer(\'pe\', pe)\n            self.dropout = nn.Dropout(p=dropout)\n\n        logger.info(\'Positional encoding: %s\' % pe_type)\n\n    def reset_parameters(self):\n        """"""Initialize parameters with Xavier uniform distribution.""""""\n        logger.info(\'===== Initialize %s with Xavier uniform distribution =====\' % self.__class__.__name__)\n        for layer in self.pe:\n            if isinstance(layer, CausalConv1d):\n                for n, p in layer.named_parameters():\n                    init_with_xavier_uniform(n, p)\n\n    def forward(self, xs, scale=True):\n        """"""Forward computation.\n\n        Args:\n            xs (FloatTensor): `[B, T, d_model]`\n        Returns:\n            xs (FloatTensor): `[B, T, d_model]`\n\n        """"""\n        if scale:\n            xs = xs * self.scale\n        # NOTE: xs is an embedding without been scaled\n\n        if self.pe_type == \'none\':\n            return xs\n        elif self.pe_type == \'add\':\n            xs = xs + self.pe[:, :xs.size(1)]\n            xs = self.dropout(xs)\n        elif self.pe_type == \'concat\':\n            xs = torch.cat([xs, self.pe[:, :xs.size(1)]], dim=-1)\n            xs = self.dropout(xs)\n        elif \'1dconv\' in self.pe_type:\n            xs = self.pe(xs)\n        else:\n            raise NotImplementedError(self.pe_type)\n        return xs\n\n\nclass XLPositionalEmbedding(nn.Module):\n    def __init__(self, d_model, dropout):\n        """"""Positional embedding for TransformerXL.""""""\n        super().__init__()\n        self.d_model = d_model\n        inv_freq = 1 / (10000 ** (torch.arange(0.0, d_model, 2.0) / d_model))\n        self.register_buffer(""inv_freq"", inv_freq)\n\n        self.dropout = nn.Dropout(p=dropout)\n\n    def forward(self, positions, device_id):\n        """"""Forward computation.\n\n        Args:\n            positions (LongTensor): `[L]`\n        Returns:\n            pos_emb (LongTensor): `[L, 1, d_model]`\n\n        """"""\n        if device_id >= 0:\n            positions = positions.cuda(device_id)\n        # outer product\n        sinusoid_inp = torch.einsum(""i,j->ij"", positions.float(), self.inv_freq)\n        pos_emb = torch.cat([sinusoid_inp.sin(), sinusoid_inp.cos()], dim=-1)\n        pos_emb = self.dropout(pos_emb)\n        return pos_emb.unsqueeze(1)\n'"
neural_sp/models/modules/positionwise_feed_forward.py,2,"b'#! /usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n# Copyright 2020 Kyoto University (Hirofumi Inaguma)\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n\n""""""Positionwise fully-connected feed-forward neural network (FFN).""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport logging\nimport random\nimport torch\nimport torch.nn as nn\n\nfrom neural_sp.models.modules.gelu import gelu, gelu_accurate\nfrom neural_sp.models.modules.glu import LinearGLUBlock\nfrom neural_sp.models.modules.swish import Swish\n\nrandom.seed(1)\n\nlogger = logging.getLogger(__name__)\n\n\nclass PositionwiseFeedForward(nn.Module):\n    """"""Positionwise fully-connected feed-forward neural network (FFN) layer.\n\n    Args:\n        d_model (int): input and output dimension\n        d_ff (int): hidden dimension\n        dropout (float): dropout probability\n        activation (str): non-linear activation function\n        param_init (str): parameter initialization method\n        bottleneck_dim (int): bottleneck dimension for low-rank FFN\n\n    """"""\n\n    def __init__(self, d_model, d_ff, dropout, activation, param_init,\n                 bottleneck_dim=0):\n        super(PositionwiseFeedForward, self).__init__()\n\n        self.bottleneck_dim = bottleneck_dim\n        if bottleneck_dim > 0:\n            self.w_1_e = nn.Linear(d_model, bottleneck_dim)\n            self.w_1_d = nn.Linear(bottleneck_dim, d_ff)\n            self.w_2_e = nn.Linear(d_ff, bottleneck_dim)\n            self.w_2_d = nn.Linear(bottleneck_dim, d_model)\n        else:\n            self.w_1 = nn.Linear(d_model, d_ff)\n            self.w_2 = nn.Linear(d_ff, d_model)\n\n        self.dropout = nn.Dropout(p=dropout)\n\n        if activation == \'relu\':\n            self.activation = torch.relu\n        elif activation == \'gelu\':\n            self.activation = lambda x: gelu(x)\n        elif activation == \'gelu_accurate\':\n            self.activation = lambda x: gelu_accurate(x)\n        elif activation == \'glu\':\n            self.activation = LinearGLUBlock(d_ff)\n        elif activation == \'swish\':\n            self.activation = Swish()\n        else:\n            raise NotImplementedError(activation)\n        logger.info(\'FFN activation: %s\' % activation)\n\n        if param_init == \'xavier_uniform\':\n            self.reset_parameters()\n\n    def reset_parameters(self):\n        """"""Initialize parameters with Xavier uniform distribution.""""""\n        logger.info(\'===== Initialize %s with Xavier uniform distribution =====\' % self.__class__.__name__)\n        for n, p in self.named_parameters():\n            if p.dim() == 1:\n                nn.init.constant_(p, 0.)\n            elif p.dim() == 2:\n                nn.init.xavier_uniform_(p)\n            else:\n                raise ValueError(n)\n\n    def forward(self, xs):\n        """"""Forward computation.\n\n        Args:\n            xs (FloatTensor): `[B, T, d_model]`\n        Returns:\n            xs (FloatTensor): `[B, T, d_model]`\n\n        """"""\n        if self.bottleneck_dim > 0:\n            return self.w_2_d(self.w_2_e(self.dropout(self.activation(self.w_1_d(self.w_1_e(xs))))))\n        else:\n            return self.w_2(self.dropout(self.activation(self.w_1(xs))))\n'"
neural_sp/models/modules/relative_multihead_attention.py,9,"b'#! /usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n# Copyright 2020 Kyoto University (Hirofumi Inaguma)\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n\n""""""Relative multi-head attention layer for TransformerXL.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport logging\nimport math\nimport numpy as np\nimport torch\nimport torch.nn as nn\n\n\nNEG_INF = float(np.finfo(np.float32).min)\n\nlogger = logging.getLogger(__name__)\n\n\nclass RelativeMultiheadAttentionMechanism(nn.Module):\n    """"""Relative multi-head attention layer for TransformerXL.\n\n    Args:\n        kdim (int): dimension of key\n        qdim (int): dimension of query\n        adim: (int) dimension of the attention space\n        odim: (int) dimension of output\n        n_heads (int): number of heads\n        dropout (float): dropout probability for attenion weights\n        bias (bool): use bias term in linear layers\n        param_init (str): parameter initialization method\n\n    """"""\n\n    def __init__(self, kdim, qdim, adim, odim, n_heads, dropout,\n                 bias=True, param_init=\'\'):\n        super(RelativeMultiheadAttentionMechanism, self).__init__()\n\n        assert adim % n_heads == 0\n        self.d_k = adim // n_heads\n        self.n_heads = n_heads\n        self.scale = math.sqrt(self.d_k)\n\n        # attention dropout applied AFTER the softmax layer\n        self.dropout = nn.Dropout(p=dropout)\n\n        self.w_key = nn.Linear(kdim, adim, bias=bias)\n        self.w_value = nn.Linear(kdim, adim, bias=bias)\n        self.w_query = nn.Linear(qdim, adim, bias=bias)\n        self.w_position = nn.Linear(qdim, adim, bias=bias)\n        # TODO: fix later\n        self.w_out = nn.Linear(adim, odim, bias=bias)\n\n        if param_init == \'xavier_uniform\':\n            self.reset_parameters(bias)\n\n    def reset_parameters(self, bias):\n        """"""Initialize parameters with Xavier uniform distribution.""""""\n        logger.info(\'===== Initialize %s with Xavier uniform distribution =====\' % self.__class__.__name__)\n        # NOTE: see https://github.com/pytorch/fairseq/blob/master/fairseq/modules/multihead_attention.py\n        nn.init.xavier_uniform_(self.w_key.weight, gain=1 / math.sqrt(2))\n        nn.init.xavier_uniform_(self.w_value.weight, gain=1 / math.sqrt(2))\n        nn.init.xavier_uniform_(self.w_query.weight, gain=1 / math.sqrt(2))\n        if bias:\n            nn.init.constant_(self.w_key.bias, 0.)\n            nn.init.constant_(self.w_value.bias, 0.)\n            nn.init.constant_(self.w_query.bias, 0.)\n\n        nn.init.xavier_uniform_(self.w_out.weight)\n        if bias:\n            nn.init.constant_(self.w_out.bias, 0.)\n\n    def _rel_shift(self, xs):\n        """"""Calculate relative positional attention efficiently.\n\n        Args:\n            xs (FloatTensor): `[B, qlen, klen, H]`\n        Returns:\n            xs_shifted (FloatTensor): `[B, qlen, klen, H]`\n\n        """"""\n        bs, qlen, klen, n_heads = xs.size()\n        # `[qlen, klen, B, H]` -> `[B, qlen, klen, H]`\n        xs = xs.permute(1, 2, 0, 3).contiguous().view(qlen, klen, bs * n_heads)\n\n        zero_pad = xs.new_zeros((qlen, 1, bs * n_heads))\n        xs_shifted = (torch.cat([zero_pad, xs], dim=1)\n                      .view(klen + 1, qlen, bs * n_heads)[1:]\n                      .view_as(xs))\n        return xs_shifted.view(qlen, klen, bs, n_heads).permute(2, 0, 1, 3)\n\n    def forward(self, key, query, memory, pos_embs, mask, u=None, v=None):\n        """"""Forward computation.\n\n        Args:\n            key (FloatTensor): `[B, klen, kdim]`\n            query (FloatTensor): `[B, qlen, qdim]`\n            memory (FloatTensor): `[B, mlen, d_model]`\n            mask (ByteTensor): `[B, qlen, klen+mlen]`\n            pos_embs (LongTensor): `[qlen, 1, d_model]`\n            u (nn.Parameter): `[H, d_k]`\n            v (nn.Parameter): `[H, d_k]`\n        Returns:\n            cv (FloatTensor): `[B, qlen, vdim]`\n            aw (FloatTensor): `[B, H, qlen, klen+mlen]`\n\n        """"""\n        bs, qlen = query.size()[: 2]\n        klen = key.size(1)\n        mlen = memory.size(1) if memory is not None and memory.dim() > 1 else 0\n        if mlen > 0:\n            key = torch.cat([memory, key], dim=1)\n\n        value = self.w_value(key).view(bs, -1, self.n_heads, self.d_k)  # `[B, klen+mlen, H, d_k]`\n        key = self.w_key(key).view(bs, -1, self.n_heads, self.d_k)  # `[B, klen+mlen, H, d_k]`\n        if mask is not None:\n            mask = mask.unsqueeze(3).repeat([1, 1, 1, self.n_heads])\n            assert mask.size() == (bs, qlen, mlen + klen, self.n_heads), \\\n                (mask.size(), (bs, qlen, klen + mlen, self.n_heads))\n\n        query = self.w_query(query).view(bs, -1, self.n_heads, self.d_k)  # `[B, qlen, H, d_k]`\n        pos_embs = self.w_position(pos_embs)\n        pos_embs = pos_embs.view(-1, self.n_heads, self.d_k)  # `[qlen, H, d_k]`\n\n        # content-based attention term: (a) + (c)\n        if u is not None:\n            AC = torch.einsum(""bihd,bjhd->bijh"", ((query + u[None, None]), key))  # `[B, qlen, klen+mlen, H]`\n        else:\n            AC = torch.einsum(""bihd,bjhd->bijh"", (query, key))  # `[B, qlen, klen+mlen, H]`\n\n        # position-based attention term: (b) + (d)\n        if v is not None:\n            BD = torch.einsum(""bihd,jhd->bijh"", ((query + v[None, None]), pos_embs))  # `[B, qlen, klen+mlen, H]`\n        else:\n            BD = torch.einsum(""bihd,jhd->bijh"", (query, pos_embs))  # `[B, qlen, klen+mlen, H]`\n\n        # Compute positional attention efficiently\n        BD = self._rel_shift(BD)\n\n        # the attention is the sum of content-based and position-based attention\n        e = (AC + BD) / self.scale  # `[B, qlen, klen+mlen, H]`\n\n        # Compute attention weights\n        if mask is not None:\n            e = e.masked_fill_(mask == 0, NEG_INF)  # `[B, qlen, klen+mlen, H]`\n        aw = torch.softmax(e, dim=2)\n        aw = self.dropout(aw)  # `[B, qlen, klen+mlen, H]`\n        cv = torch.einsum(""bijh,bjhd->bihd"", (aw, value))  # `[B, qlen, H, d_k]`\n        cv = cv.contiguous().view(bs, -1, self.n_heads * self.d_k)  # `[B, qlen, H * d_k]`\n        cv = self.w_out(cv)\n        aw = aw.permute(0, 3, 1, 2)  # `[B, H, qlen, klen+mlen]`\n\n        return cv, aw\n'"
neural_sp/models/modules/swish.py,2,"b'#! /usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n# Copyright 2020 Kyoto University (Hirofumi Inaguma)\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n\n""""""Swish activation.\n   See details in https://arxiv.org/abs/1710.05941.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport torch\n\n\nclass Swish(torch.nn.Module):\n    def forward(self, x):\n        return x * torch.sigmoid(x)\n'"
neural_sp/models/modules/sync_bidir_multihead_attention.py,19,"b'#! /usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n# Copyright 2020 Kyoto University (Hirofumi Inaguma)\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n\n""""""Synchronous bidirectional multi-head attention layer.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport logging\nimport math\nimport numpy as np\nimport torch\nimport torch.nn as nn\n\nNEG_INF = float(np.finfo(np.float32).min)\n\nlogger = logging.getLogger(__name__)\n\n\nclass SyncBidirMultiheadAttentionMechanism(nn.Module):\n    """"""Multi-headed attention layer.\n\n    Args:\n        kdim (int): dimension of key\n        qdim (int): dimension of query\n        adim: (int) dimension of the attention space\n        n_heads (int): number of heads\n        dropout (float): dropout probability\n        bias (bool): use bias term in linear layers\n        atype (str): type of attention mechanisms\n        param_init (str): parameter initialization method\n        future_weight (float):\n\n    """"""\n\n    def __init__(self, kdim, qdim, adim, n_heads, dropout,\n                 atype=\'scaled_dot\', bias=True, param_init=\'\', future_weight=0.1):\n        super(SyncBidirMultiheadAttentionMechanism, self).__init__()\n\n        self.atype = atype\n        assert adim % n_heads == 0\n        self.d_k = adim // n_heads\n        self.n_heads = n_heads\n        self.scale = math.sqrt(self.d_k)\n        self.future_weight = future_weight\n        self.reset()\n\n        # attention dropout applied AFTER the softmax layer\n        self.dropout = nn.Dropout(p=dropout)\n\n        if atype == \'scaled_dot\':\n            # for Transformer\n            self.w_key = nn.Linear(kdim, adim, bias=bias)\n            self.w_value = nn.Linear(kdim, adim, bias=bias)\n            self.w_query = nn.Linear(qdim, adim, bias=bias)\n        elif atype == \'add\':\n            # for LAS\n            self.w_key = nn.Linear(kdim, adim, bias=bias)\n            self.w_value = nn.Linear(kdim, adim, bias=bias)\n            self.w_query = nn.Linear(qdim, adim, bias=bias)\n            self.v = nn.Linear(adim, n_heads, bias=bias)\n        else:\n            raise NotImplementedError(atype)\n\n        self.w_out = nn.Linear(adim, kdim, bias=bias)\n\n        if param_init == \'xavier_uniform\':\n            self.reset_parameters(bias)\n\n    def reset_parameters(self, bias):\n        """"""Initialize parameters with Xavier uniform distribution.""""""\n        logger.info(\'===== Initialize %s with Xavier uniform distribution =====\' % self.__class__.__name__)\n        # NOTE: see https://github.com/pytorch/fairseq/blob/master/fairseq/modules/multihead_attention.py\n        nn.init.xavier_uniform_(self.w_key.weight, gain=1 / math.sqrt(2))\n        nn.init.xavier_uniform_(self.w_value.weight, gain=1 / math.sqrt(2))\n        nn.init.xavier_uniform_(self.w_query.weight, gain=1 / math.sqrt(2))\n        if bias:\n            nn.init.constant_(self.w_key.bias, 0.)\n            nn.init.constant_(self.w_value.bias, 0.)\n            nn.init.constant_(self.w_query.bias, 0.)\n\n        nn.init.xavier_uniform_(self.w_out.weight)\n        if bias:\n            nn.init.constant_(self.w_out.bias, 0.)\n\n    def reset(self):\n        self.key_fwd = None\n        self.key_bwd = None\n        self.value_fwd = None\n        self.value_bwd = None\n        self.tgt_mask = None\n        self.identity_mask = None\n\n    def forward(self, key_fwd, value_fwd, query_fwd,\n                key_bwd, value_bwd, query_bwd,\n                tgt_mask, identity_mask,\n                mode=\'\', cache=True, trigger_point=None):\n        """"""Forward computation.\n\n        Args:\n            key_fwd (FloatTensor): `[B, klen, kdim]`\n            value_fwd (FloatTensor): `[B, klen, vdim]`\n            query_fwd (FloatTensor): `[B, qlen, qdim]`\n            key_bwd (FloatTensor): `[B, klen, kdim]`\n            value_bwd (FloatTensor): `[B, klen, vdim]`\n            query_bwd (FloatTensor): `[B, qlen, qdim]`\n            tgt_mask (ByteTensor): `[B, qlen, klen]`\n            identity_mask (ByteTensor): `[B, qlen, klen]`\n            mode: dummy interface for MoChA\n            cache (bool): cache key, value, and tgt_mask\n            trigger_point (IntTensor): dummy\n        Returns:\n            cv_fwd (FloatTensor): `[B, qlen, vdim]`\n            cv_bwd (FloatTensor): `[B, qlen, vdim]`\n            aw_fwd_h (FloatTensor): `[B, H, qlen, klen]`\n            aw_fwd_f (FloatTensor): `[B, H, qlen, klen]`\n            aw_bwd_h (FloatTensor): `[B, H, qlen, klen]`\n            aw_bwd_f (FloatTensor): `[B, H, qlen, klen]`\n\n        """"""\n        bs, klen = key_fwd.size()[: 2]\n        qlen = query_fwd.size(1)\n\n        if self.key_fwd is None or not cache:\n            key_fwd = self.w_key(key_fwd).view(bs, -1, self.n_heads, self.d_k)\n            self.key_fwd = key_fwd.transpose(2, 1).contiguous()      # `[B, H, klen, d_k]`\n            value_fwd = self.w_value(value_fwd).view(bs, -1, self.n_heads, self.d_k)\n            self.value_fwd = value_fwd.transpose(2, 1).contiguous()  # `[B, H, klen, d_k]`\n            self.tgt_mask = tgt_mask\n            self.identity_mask = identity_mask\n            if tgt_mask is not None:\n                self.tgt_mask = tgt_mask.unsqueeze(1).repeat([1, self.n_heads, 1, 1])\n                assert self.tgt_mask.size() == (bs, self.n_heads, qlen, klen)\n            if identity_mask is not None:\n                self.identity_mask = identity_mask.unsqueeze(1).repeat([1, self.n_heads, 1, 1])\n                assert self.identity_mask.size() == (bs, self.n_heads, qlen, klen)\n        if self.key_bwd is None or not cache:\n            key_bwd = self.w_key(key_bwd).view(bs, -1, self.n_heads, self.d_k)\n            self.key_bwd = key_bwd.transpose(2, 1).contiguous()  # `[B, H, klen, d_k]`\n            value_bwd = self.w_value(value_bwd).view(bs, -1, self.n_heads, self.d_k)\n            self.value_bwd = value_bwd.transpose(2, 1).contiguous()  # `[B, H, klen, d_k]`\n\n        query_fwd = self.w_query(query_fwd).view(bs, -1, self.n_heads, self.d_k)\n        query_fwd = query_fwd.transpose(2, 1).contiguous()  # `[B, H, qlen, d_k]`\n        query_bwd = self.w_query(query_bwd).view(bs, -1, self.n_heads, self.d_k)\n        query_bwd = query_bwd.transpose(2, 1).contiguous()  # `[B, H, qlen, d_k]`\n\n        if self.atype == \'scaled_dot\':\n            e_fwd_h = torch.matmul(query_fwd, self.key_fwd.transpose(3, 2)) / self.scale\n            e_fwd_f = torch.matmul(query_fwd, self.key_bwd.transpose(3, 2)) / self.scale\n            e_bwd_h = torch.matmul(query_bwd, self.key_bwd.transpose(3, 2)) / self.scale\n            e_bwd_f = torch.matmul(query_bwd, self.key_fwd.transpose(3, 2)) / self.scale\n        elif self.atype == \'add\':\n            e_fwd_h = torch.tanh(self.key_fwd.unsqueeze(2) + query_fwd.unsqueeze(3))\n            e_fwd_h = e_fwd_h.permute(0, 2, 3, 1, 4).contiguous().view(bs, qlen, klen, -1)\n            e_fwd_h = self.v(e_fwd_h).permute(0, 3, 1, 2)\n            e_fwd_f = torch.tanh(self.key_bwd.unsqueeze(2) + query_fwd.unsqueeze(3))\n            e_fwd_f = e_fwd_f.permute(0, 2, 3, 1, 4).contiguous().view(bs, qlen, klen, -1)\n            e_fwd_f = self.v(e_fwd_f).permute(0, 3, 1, 2)\n            e_bwd_h = torch.tanh(self.key_bwd.unsqueeze(2) + query_bwd.unsqueeze(3))\n            e_bwd_h = e_bwd_h.permute(0, 2, 3, 1, 4).contiguous().view(bs, qlen, klen, -1)\n            e_bwd_h = self.v(e_bwd_h).permute(0, 3, 1, 2)\n            e_bwd_f = torch.tanh(self.key_fwd.unsqueeze(2) + query_bwd.unsqueeze(3))\n            e_bwd_f = e_bwd_f.permute(0, 2, 3, 1, 4).contiguous().view(bs, qlen, klen, -1)\n            e_bwd_f = self.v(e_bwd_f).permute(0, 3, 1, 2)\n\n        # Compute attention weights\n        if self.tgt_mask is not None:\n            e_fwd_h = e_fwd_h.masked_fill_(self.tgt_mask == 0, NEG_INF)  # `[B, H, qlen, klen]`\n            e_bwd_h = e_bwd_h.masked_fill_(self.tgt_mask == 0, NEG_INF)  # `[B, H, qlen, klen]`\n        if self.identity_mask is not None:\n            e_fwd_f = e_fwd_f.masked_fill_(self.identity_mask == 0, NEG_INF)  # `[B, H, qlen, klen]`\n            e_bwd_f = e_bwd_f.masked_fill_(self.identity_mask == 0, NEG_INF)  # `[B, H, qlen, klen]`\n        aw_fwd_h = self.dropout(torch.softmax(e_fwd_h, dim=-1))\n        aw_fwd_f = self.dropout(torch.softmax(e_fwd_f, dim=-1))\n        aw_bwd_h = self.dropout(torch.softmax(e_bwd_h, dim=-1))\n        aw_bwd_f = self.dropout(torch.softmax(e_bwd_f, dim=-1))\n\n        cv_fwd_h = torch.matmul(aw_fwd_h, self.value_fwd)  # `[B, H, qlen, d_k]`\n        cv_fwd_f = torch.matmul(aw_fwd_f, self.value_bwd)  # `[B, H, qlen, d_k]`\n        cv_bwd_h = torch.matmul(aw_bwd_h, self.value_bwd)  # `[B, H, qlen, d_k]`\n        cv_bwd_f = torch.matmul(aw_bwd_f, self.value_fwd)  # `[B, H, qlen, d_k]`\n\n        cv_fwd_h = cv_fwd_h.transpose(2, 1).contiguous().view(bs, -1, self.n_heads * self.d_k)\n        cv_fwd_h = self.w_out(cv_fwd_h)\n        cv_fwd_f = cv_fwd_f.transpose(2, 1).contiguous().view(bs, -1, self.n_heads * self.d_k)\n        cv_fwd_f = self.w_out(cv_fwd_f)\n        cv_bwd_h = cv_bwd_h.transpose(2, 1).contiguous().view(bs, -1, self.n_heads * self.d_k)\n        cv_bwd_h = self.w_out(cv_bwd_h)\n        cv_bwd_f = cv_bwd_f.transpose(2, 1).contiguous().view(bs, -1, self.n_heads * self.d_k)\n        cv_bwd_f = self.w_out(cv_bwd_f)\n\n        # merge history and future information\n        cv_fwd = cv_fwd_h + self.future_weight * torch.tanh(cv_fwd_f)\n        cv_bwd = cv_bwd_h + self.future_weight * torch.tanh(cv_bwd_f)\n\n        return cv_fwd, cv_bwd, aw_fwd_h, aw_fwd_f, aw_bwd_h, aw_bwd_f\n'"
neural_sp/models/modules/transformer.py,6,"b'#! /usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n# Copyright 2019 Kyoto University (Hirofumi Inaguma)\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n\n""""""Transformer blocks.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport logging\nimport random\nimport torch\nimport torch.nn as nn\n\nfrom neural_sp.models.modules.mocha import MoChA\nfrom neural_sp.models.modules.multihead_attention import MultiheadAttentionMechanism as MHA\nfrom neural_sp.models.modules.positionwise_feed_forward import PositionwiseFeedForward as FFN\nfrom neural_sp.models.modules.relative_multihead_attention import RelativeMultiheadAttentionMechanism as RelMHA\n\nrandom.seed(1)\n\nlogger = logging.getLogger(__name__)\n\n\nclass TransformerDecoderBlock(nn.Module):\n    """"""A single layer of the Transformer decoder.\n\n        Args:\n            d_model (int): input dimension of MultiheadAttentionMechanism and PositionwiseFeedForward\n            d_ff (int): hidden dimension of PositionwiseFeedForward\n            atype (str): type of attention mechanism\n            n_heads (int): number of heads for multi-head attention\n            dropout (float): dropout probabilities for linear layers\n            dropout_att (float): dropout probabilities for attention probabilities\n            dropout_layer (float): LayerDrop probability\n            dropout_head (float): HeadDrop probability\n            layer_norm_eps (float): epsilon parameter for layer normalization\n            ffn_activation (str): nonolinear function for PositionwiseFeedForward\n            param_init (str): parameter initialization method\n            src_tgt_attention (bool): if False, ignore source-target attention\n            memory_transformer (bool): TransformerXL decoder\n            mocha_chunk_size (int): chunk size for MoChA. -1 means infinite lookback.\n            mocha_n_heads_mono (int): number of heads for monotonic attention\n            mocha_n_heads_chunk (int): number of heads for chunkwise attention\n            mocha_init_r (int):\n            mocha_eps (float):\n            mocha_std (float):\n            mocha_no_denominator (bool):\n            mocha_1dconv (bool):\n            lm_fusion (bool):\n            d_ff_bottleneck_dim (int): bottleneck dimension for the light-weight FFN layer\n            share_chunkwise_attention (bool):\n\n    """"""\n\n    def __init__(self, d_model, d_ff, atype, n_heads,\n                 dropout, dropout_att, dropout_layer,\n                 layer_norm_eps, ffn_activation, param_init,\n                 src_tgt_attention=True, memory_transformer=False,\n                 mocha_chunk_size=0, mocha_n_heads_mono=1, mocha_n_heads_chunk=1,\n                 mocha_init_r=2, mocha_eps=1e-6, mocha_std=1.0,\n                 mocha_no_denominator=False, mocha_1dconv=False,\n                 dropout_head=0, lm_fusion=False, d_ff_bottleneck_dim=0,\n                 share_chunkwise_attention=False):\n        super(TransformerDecoderBlock, self).__init__()\n\n        self.atype = atype\n        self.n_heads = n_heads\n        self.src_tgt_attention = src_tgt_attention\n        self.memory_transformer = memory_transformer\n\n        # self-attention\n        self.norm1 = nn.LayerNorm(d_model, eps=layer_norm_eps)\n        mha = RelMHA if memory_transformer else MHA\n        self.self_attn = mha(kdim=d_model,\n                             qdim=d_model,\n                             adim=d_model,\n                             odim=d_model,\n                             n_heads=n_heads,\n                             dropout=dropout_att,\n                             param_init=param_init)\n\n        # attention over encoder stacks\n        if src_tgt_attention:\n            self.norm2 = nn.LayerNorm(d_model, eps=layer_norm_eps)\n            if \'mocha\' in atype:\n                self.n_heads = mocha_n_heads_mono\n                self.src_attn = MoChA(kdim=d_model,\n                                      qdim=d_model,\n                                      adim=d_model,\n                                      odim=d_model,\n                                      atype=\'scaled_dot\',\n                                      chunk_size=mocha_chunk_size,\n                                      n_heads_mono=mocha_n_heads_mono,\n                                      n_heads_chunk=mocha_n_heads_chunk,\n                                      init_r=mocha_init_r,\n                                      eps=mocha_eps,\n                                      noise_std=mocha_std,\n                                      no_denominator=mocha_no_denominator,\n                                      conv1d=mocha_1dconv,\n                                      dropout=dropout_att,\n                                      dropout_head=dropout_head,\n                                      param_init=param_init,\n                                      share_chunkwise_attention=share_chunkwise_attention)\n            else:\n                self.src_attn = MHA(kdim=d_model,\n                                    qdim=d_model,\n                                    adim=d_model,\n                                    odim=d_model,\n                                    n_heads=n_heads,\n                                    dropout=dropout_att,\n                                    param_init=param_init)\n\n        # position-wise feed-forward\n        self.norm3 = nn.LayerNorm(d_model, eps=layer_norm_eps)\n        self.feed_forward = FFN(d_model, d_ff, dropout, ffn_activation, param_init,\n                                d_ff_bottleneck_dim)\n\n        self.dropout = nn.Dropout(p=dropout)\n        self.dropout_layer = dropout_layer\n\n        # LM fusion\n        self.lm_fusion = lm_fusion\n        if lm_fusion:\n            self.norm_lm = nn.LayerNorm(d_model, eps=layer_norm_eps)\n            # NOTE: LM should be projected to d_model in advance\n            self.linear_lm_feat = nn.Linear(d_model, d_model)\n            self.linear_lm_gate = nn.Linear(d_model * 2, d_model)\n            self.linear_lm_fusion = nn.Linear(d_model * 2, d_model)\n            if \'attention\' in lm_fusion:\n                self.lm_attn = MHA(kdim=d_model,\n                                   qdim=d_model,\n                                   adim=d_model,\n                                   odim=d_model,\n                                   n_heads=n_heads,\n                                   dropout=dropout_att,\n                                   param_init=param_init)\n\n    def forward(self, ys, yy_mask, xs=None, xy_mask=None, cache=None,\n                xy_aws_prev=None, mode=\'hard\', lmout=None,\n                pos_embs=None, memory=None, u=None, v=None,\n                eps_wait=-1):\n        """"""Transformer decoder forward pass.\n\n        Args:\n            ys (FloatTensor): `[B, L, d_model]`\n            yy_mask (ByteTensor): `[B, L (query), L (key)]`\n            xs (FloatTensor): encoder outputs. `[B, T, d_model]`\n            xy_mask (ByteTensor): `[B, L, T]`\n            cache (FloatTensor): `[B, L-1, d_model]`\n            xy_aws_prev (FloatTensor): `[B, H, L, T]`\n            mode (str):\n            lmout (FloatTensor): `[B, L, d_model]`\n            pos_embs (LongTensor): `[L, 1, d_model]`\n            memory (FloatTensor): `[B, L_prev, d_model]`\n            u (FloatTensor): global parameter for TransformerXL\n            v (FloatTensor): global parameter for TransformerXL\n            eps_wait (int): wait time delay for head-synchronous decoding in MMA\n        Returns:\n            out (FloatTensor): `[B, L, d_model]`\n            yy_aws (FloatTensor)`[B, H, L, L]`\n            xy_aws (FloatTensor): `[B, H, L, T]`\n            xy_aws_beta (FloatTensor): `[B, H, L, T]`\n\n        """"""\n        if self.dropout_layer > 0 and self.training and random.random() >= self.dropout_layer:\n            xy_aws = None\n            if self.src_tgt_attention:\n                bs, qlen, klen = xy_mask.size()\n                xy_aws = ys.new_zeros(bs, self.n_heads, qlen, klen)\n            return ys, None, xy_aws, None, None\n\n        residual = ys\n        ys = self.norm1(ys)\n\n        if cache is not None:\n            ys_q = ys[:, -1:]\n            residual = residual[:, -1:]\n            yy_mask = yy_mask[:, -1:]\n        else:\n            ys_q = ys\n\n        # self-attention\n        yy_aws = None\n        if self.memory_transformer:\n            if cache is not None:\n                pos_embs = pos_embs[-ys_q.size(1):]\n            out, yy_aws = self.self_attn(ys, ys_q, memory, pos_embs, yy_mask, u, v)\n        else:\n            out, yy_aws, _ = self.self_attn(ys, ys, ys_q, mask=yy_mask)  # k/v/q\n        out = self.dropout(out) + residual\n\n        # attention over encoder stacks\n        xy_aws, xy_aws_beta = None, None\n        if self.src_tgt_attention:\n            residual = out\n            out = self.norm2(out)\n            out, xy_aws, xy_aws_beta = self.src_attn(xs, xs, out, mask=xy_mask,  # k/v/q\n                                                     aw_prev=xy_aws_prev, mode=mode,\n                                                     eps_wait=eps_wait)\n            out = self.dropout(out) + residual\n\n        # LM integration\n        yy_aws_lm = None\n        if self.lm_fusion:\n            residual = out\n            out = self.norm_lm(out)\n            lmout = self.linear_lm_feat(lmout)\n\n            # attention over LM outputs\n            if \'attention\' in self.lm_fusion:\n                out, yy_aws_lm, _ = self.lm_attn(lmout, lmout, out, mask=yy_mask)  # k/v/q\n\n            gate = torch.sigmoid(self.linear_lm_gate(torch.cat([out, lmout], dim=-1)))\n            gated_lmout = gate * lmout\n            out = self.linear_lm_fusion(torch.cat([out, gated_lmout], dim=-1))\n            out = self.dropout(out) + residual\n\n        # position-wise feed-forward\n        residual = out\n        out = self.norm3(out)\n        out = self.feed_forward(out)\n        out = self.dropout(out) + residual\n\n        if cache is not None:\n            out = torch.cat([cache, out], dim=1)\n\n        return out, yy_aws, xy_aws, xy_aws_beta, yy_aws_lm\n\n\nclass SyncBidirTransformerDecoderBlock(nn.Module):\n    """"""A single layer of the synchronous bidirectional Transformer decoder.\n\n        Args:\n            d_model (int): input dimension of MultiheadAttentionMechanism and PositionwiseFeedForward\n            d_ff (int): hidden dimension of PositionwiseFeedForward\n            n_heads (int): number of heads for multi-head attention\n            dropout (float): dropout probabilities for linear layers\n            dropout_att (float): dropout probabilities for attention probabilities\n            dropout_layer (float): LayerDrop probability\n            layer_norm_eps (float): epsilon parameter for layer normalization\n            ffn_activation (str): nonolinear function for PositionwiseFeedForward\n            param_init (str): parameter initialization method\n\n    """"""\n\n    def __init__(self, d_model, d_ff, n_heads,\n                 dropout, dropout_att, dropout_layer,\n                 layer_norm_eps, ffn_activation, param_init):\n        super(SyncBidirTransformerDecoderBlock, self).__init__()\n\n        self.n_heads = n_heads\n\n        # synchronous bidirectional attention\n        self.norm1 = nn.LayerNorm(d_model, eps=layer_norm_eps)\n        from neural_sp.models.modules.sync_bidir_multihead_attention import SyncBidirMultiheadAttentionMechanism as SyncBidirMHA\n        self.self_attn = SyncBidirMHA(kdim=d_model,\n                                      qdim=d_model,\n                                      adim=d_model,\n                                      n_heads=n_heads,\n                                      dropout=dropout_att,\n                                      param_init=param_init)\n\n        # attention over encoder stacks\n        self.norm2 = nn.LayerNorm(d_model, eps=layer_norm_eps)\n        self.src_attn = MHA(kdim=d_model,\n                            qdim=d_model,\n                            adim=d_model,\n                            odim=d_model,\n                            n_heads=n_heads,\n                            dropout=dropout_att,\n                            param_init=param_init)\n\n        # position-wise feed-forward\n        self.norm3 = nn.LayerNorm(d_model, eps=layer_norm_eps)\n        self.feed_forward = FFN(d_model, d_ff, dropout, ffn_activation, param_init)\n\n        self.dropout = nn.Dropout(p=dropout)\n        # self.dropout_layer = dropout_layer\n\n    def forward(self, ys, ys_bwd, yy_mask, identity_mask, xs, xy_mask,\n                cache=None, cache_bwd=None):\n        """"""Synchronous bidirectional Transformer decoder forward pass.\n\n        Args:\n            ys (FloatTensor): `[B, L, d_model]`\n            ys_bwd (FloatTensor): `[B, L, d_model]`\n            yy_mask (ByteTensor): `[B, L, L]`\n            identity_mask (ByteTensor): `[B, L, L]`\n            xs (FloatTensor): encoder outputs. `[B, T, d_model]`\n            xy_mask (ByteTensor): `[B, L, T]`\n            cache (FloatTensor): `[B, L-1, d_model]`\n            cache_bwd (FloatTensor): `[B, L-1, d_model]`\n        Returns:\n            out (FloatTensor): `[B, L, d_model]`\n            yy_aws_h (FloatTensor)`[B, L, L]`\n            yy_aws_f (FloatTensor)`[B, L, L]`\n            yy_aws_bwd_h (FloatTensor)`[B, L, L]`\n            yy_aws_bwd_f (FloatTensor)`[B, L, L]`\n            xy_aws (FloatTensor): `[B, L, T]`\n            xy_aws_bwd (FloatTensor): `[B, L, T]`\n\n        """"""\n        residual = ys\n        residual_bwd = ys_bwd\n        ys = self.norm1(ys)\n        ys_bwd = self.norm1(ys_bwd)\n\n        if cache is not None:\n            assert cache_bwd is not None\n            ys_q = ys[:, -1:]\n            ys_bwd_q = ys_bwd[:, -1:]\n            residual = residual[:, -1:]\n            residual_bwd = residual_bwd[:, -1:]\n            yy_mask = yy_mask[:, -1:]\n        else:\n            ys_q = ys\n            ys_bwd_q = ys_bwd\n\n        # synchronous bidirectional attention\n        out, out_bwd, yy_aws_h, yy_aws_f, yy_aws_bwd_h, yy_aws_bwd_f = self.self_attn(\n            ys, ys, ys_q,  # k/v/q\n            ys_bwd, ys_bwd, ys_bwd_q,  # k/v/q\n            tgt_mask=yy_mask, identity_mask=identity_mask)\n        out = self.dropout(out) + residual\n        out_bwd = self.dropout(out_bwd) + residual_bwd\n\n        # attention over encoder stacks\n        # fwd\n        residual = out\n        out = self.norm2(out)\n        out, xy_aws, _ = self.src_attn(xs, xs, out, mask=xy_mask)  # k/v/q\n        out = self.dropout(out) + residual\n        # bwd\n        residual_bwd = out_bwd\n        out_bwd = self.norm2(out_bwd)\n        out_bwd, xy_aws_bwd, _ = self.src_attn(xs, xs, out_bwd, mask=xy_mask)  # k/v/q\n        out_bwd = self.dropout(out_bwd) + residual_bwd\n\n        # position-wise feed-forward\n        # fwd\n        residual = out\n        out = self.norm3(out)\n        out = self.feed_forward(out)\n        out = self.dropout(out) + residual\n        # bwd\n        residual_bwd = out_bwd\n        out_bwd = self.norm3(out_bwd)\n        out_bwd = self.feed_forward(out_bwd)\n        out_bwd = self.dropout(out_bwd) + residual_bwd\n\n        if cache is not None:\n            out = torch.cat([cache, out], dim=1)\n            out_bwd = torch.cat([cache_bwd, out_bwd], dim=1)\n\n        return out, out_bwd, yy_aws_h, yy_aws_f, yy_aws_bwd_h, yy_aws_bwd_f, xy_aws, xy_aws_bwd\n'"
neural_sp/models/modules/zoneout.py,1,"b'#! /usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n# Copyright 2019 Kyoto University (Hirofumi Inaguma)\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n\n""""""Zoneout regularization.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport torch.nn as nn\n\n\nclass ZoneoutCell(nn.Module):\n\n    def __init__(self, cell, zoneout_prob_h, zoneout_prob_c):\n        super(ZoneoutCell, self).__init__()\n        self.cell = cell\n        self.hidden_size = cell.hidden_size\n\n        if not isinstance(cell, nn.RNNCellBase):\n            raise TypeError(""The cell is not a LSTMCell or GRUCell!"")\n\n        if isinstance(cell, nn.LSTMCell):\n            self.prob = (zoneout_prob_h, zoneout_prob_c)\n        else:\n            self.prob = zoneout_prob_h\n\n    def forward(self, inputs, state):\n        return self.zoneout(state, self.cell(inputs, state), self.prob)\n\n    def zoneout(self, state, next_state, prob):\n        if isinstance(state, tuple):\n            return (self.zoneout(state[0], next_state[0], prob[0]),\n                    self.zoneout(state[1], next_state[1], prob[1]))\n        mask = state.new(state.size()).bernoulli_(prob)\n        if self.training:\n            return mask * next_state + (1 - mask) * state\n        else:\n            return prob * next_state + (1 - prob) * state\n\n\ndef zoneout_wrapper(cell, zoneout_prob_h=0, zoneout_prob_c=0):\n    if zoneout_prob_h > 0 or zoneout_prob_c > 0:\n        return ZoneoutCell(cell, zoneout_prob_h, zoneout_prob_c)\n    else:\n        return cell\n'"
neural_sp/models/seq2seq/__init__.py,0,b''
neural_sp/models/seq2seq/speech2text.py,18,"b'#! /usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n# Copyright 2018 Kyoto University (Hirofumi Inaguma)\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n\n""""""Speech to text sequence-to-sequence model.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport copy\nimport logging\nimport numpy as np\nimport random\nimport torch\nimport torch.nn as nn\n\nfrom neural_sp.bin.train_utils import load_checkpoint\nfrom neural_sp.models.base import ModelBase\nfrom neural_sp.models.lm.rnnlm import RNNLM\nfrom neural_sp.models.seq2seq.decoders.build import build_decoder\nfrom neural_sp.models.seq2seq.decoders.fwd_bwd_attention import fwd_bwd_attention\nfrom neural_sp.models.seq2seq.decoders.rnn_transducer import RNNTransducer\nfrom neural_sp.models.seq2seq.encoders.build import build_encoder\nfrom neural_sp.models.seq2seq.frontends.frame_stacking import stack_frame\nfrom neural_sp.models.seq2seq.frontends.gaussian_noise import add_gaussian_noise\nfrom neural_sp.models.seq2seq.frontends.sequence_summary import SequenceSummaryNetwork\nfrom neural_sp.models.seq2seq.frontends.spec_augment import SpecAugment\nfrom neural_sp.models.seq2seq.frontends.splicing import splice\nfrom neural_sp.models.torch_utils import np2tensor\nfrom neural_sp.models.torch_utils import tensor2np\nfrom neural_sp.models.torch_utils import pad_list\n\nrandom.seed(1)\n\nlogger = logging.getLogger(__name__)\n\n\nclass Speech2Text(ModelBase):\n    """"""Speech to text sequence-to-sequence model.""""""\n\n    def __init__(self, args, save_path=None, idx2token=None):\n\n        super(ModelBase, self).__init__()\n\n        self.save_path = save_path\n\n        # for encoder, decoder\n        self.input_type = args.input_type\n        self.input_dim = args.input_dim\n        self.enc_type = args.enc_type\n        self.dec_type = args.dec_type\n\n        # for OOV resolution\n        self.enc_n_layers = args.enc_n_layers\n        self.enc_n_layers_sub1 = args.enc_n_layers_sub1\n        self.subsample = [int(s) for s in args.subsample.split(\'_\')]\n\n        # for decoder\n        self.vocab = args.vocab\n        self.vocab_sub1 = args.vocab_sub1\n        self.vocab_sub2 = args.vocab_sub2\n        self.blank = 0\n        self.unk = 1\n        self.eos = 2\n        self.pad = 3\n        # NOTE: reserved in advance\n\n        # for the sub tasks\n        self.main_weight = 1 - args.sub1_weight - args.sub2_weight\n        self.sub1_weight = args.sub1_weight\n        self.sub2_weight = args.sub2_weight\n        self.mtl_per_batch = args.mtl_per_batch\n        self.task_specific_layer = args.task_specific_layer\n\n        # for CTC\n        self.ctc_weight = min(args.ctc_weight, self.main_weight)\n        self.ctc_weight_sub1 = min(args.ctc_weight_sub1, self.sub1_weight)\n        self.ctc_weight_sub2 = min(args.ctc_weight_sub2, self.sub2_weight)\n\n        # for backward decoder\n        self.bwd_weight = min(args.bwd_weight, self.main_weight)\n        self.fwd_weight = self.main_weight - self.bwd_weight - self.ctc_weight\n        self.fwd_weight_sub1 = self.sub1_weight - self.ctc_weight_sub1\n        self.fwd_weight_sub2 = self.sub2_weight - self.ctc_weight_sub2\n\n        # for MBR\n        self.mbr_training = args.mbr_training\n        self.recog_params = vars(args)\n        self.idx2token = idx2token\n\n        # for discourse-aware model\n        self.utt_id_prev = None\n\n        # Feature extraction\n        self.gaussian_noise = args.gaussian_noise\n        self.n_stacks = args.n_stacks\n        self.n_skips = args.n_skips\n        self.n_splices = args.n_splices\n        self.use_specaug = args.n_freq_masks > 0 or args.n_time_masks > 0\n        self.specaug = None\n        self.weight_noise = args.weight_noise\n        if self.use_specaug:\n            assert args.n_stacks == 1 and args.n_skips == 1\n            assert args.n_splices == 1\n            self.specaug = SpecAugment(F=args.freq_width,\n                                       T=args.time_width,\n                                       n_freq_masks=args.n_freq_masks,\n                                       n_time_masks=args.n_time_masks,\n                                       p=args.time_width_upper,\n                                       adaptive_number_ratio=args.adaptive_number_ratio,\n                                       adaptive_size_ratio=args.adaptive_size_ratio,\n                                       max_n_time_masks=args.max_n_time_masks)\n\n        # Frontend\n        self.ssn = None\n        if args.sequence_summary_network:\n            assert args.input_type == \'speech\'\n            self.ssn = SequenceSummaryNetwork(args.input_dim,\n                                              n_units=512,\n                                              n_layers=3,\n                                              bottleneck_dim=100,\n                                              dropout=0,\n                                              param_init=args.param_init)\n\n        # Encoder\n        self.enc = build_encoder(args)\n        if args.freeze_encoder:\n            for p in self.enc.parameters():\n                p.requires_grad = False\n\n        # main task\n        external_lm = None\n        directions = []\n        if self.fwd_weight > 0 or (self.bwd_weight == 0 and self.ctc_weight > 0):\n            directions.append(\'fwd\')\n        if self.bwd_weight > 0:\n            directions.append(\'bwd\')\n        for dir in directions:\n            # Load the LM for LM fusion and decoder initialization\n            if args.external_lm and dir == \'fwd\':\n                external_lm = RNNLM(args.lm_conf)\n                load_checkpoint(external_lm, args.external_lm)\n                # freeze LM parameters\n                for n, p in external_lm.named_parameters():\n                    p.requires_grad = False\n\n            # Decoder\n            special_symbols = {\n                \'blank\': self.blank,\n                \'unk\': self.unk,\n                \'eos\': self.eos,\n                \'pad\': self.pad,\n            }\n            dec = build_decoder(args, special_symbols, self.enc.output_dim,\n                                args.vocab,\n                                self.ctc_weight,\n                                args.ctc_fc_list,\n                                self.main_weight - self.bwd_weight if dir == \'fwd\' else self.bwd_weight,\n                                external_lm)\n            setattr(self, \'dec_\' + dir, dec)\n\n        # sub task\n        for sub in [\'sub1\', \'sub2\']:\n            if getattr(self, sub + \'_weight\') > 0:\n                dec_sub = build_decoder(args, special_symbols, self.enc.output_dim,\n                                        getattr(self, \'vocab_\' + sub),\n                                        getattr(self, \'ctc_weight_\' + sub),\n                                        getattr(args, \'ctc_fc_list_\' + sub),\n                                        getattr(self, sub + \'_weight\'),\n                                        external_lm)\n                setattr(self, \'dec_fwd_\' + sub, dec_sub)\n\n        if args.input_type == \'text\':\n            if args.vocab == args.vocab_sub1:\n                # Share the embedding layer between input and output\n                self.embed = dec.embed\n            else:\n                self.embed = nn.Embedding(args.vocab_sub1, args.emb_dim,\n                                          padding_idx=self.pad)\n                self.dropout_emb = nn.Dropout(p=args.dropout_emb)\n\n        # Initialize bias in forget gate with 1\n        # self.init_forget_gate_bias_with_one()\n\n        # Fix all parameters except for the gating parts in deep fusion\n        if args.lm_fusion == \'deep\' and external_lm is not None:\n            for n, p in self.named_parameters():\n                if \'output\' in n or \'output_bn\' in n or \'linear\' in n:\n                    p.requires_grad = True\n                else:\n                    p.requires_grad = False\n\n    def scheduled_sampling_trigger(self):\n        # main task\n        for dir in [\'fwd\', \'bwd\']:\n            if hasattr(self, \'dec_\' + dir):\n                getattr(self, \'dec_\' + dir).start_scheduled_sampling()\n\n        # sub task\n        for sub in [\'sub1\', \'sub2\']:\n            if hasattr(self, \'dec_fwd_\' + sub):\n                getattr(self, \'dec_fwd_\' + sub).start_scheduled_sampling()\n\n    def mocha_quantity_loss_trigger(self):\n        # main task only now\n        if hasattr(self, \'dec_fwd\'):\n            getattr(self, \'dec_fwd\').start_mocha_quantity_loss()\n\n    def reset_session(self):\n        # main task\n        for dir in [\'fwd\', \'bwd\']:\n            if hasattr(self, \'dec_\' + dir):\n                getattr(self, \'dec_\' + dir).reset_session()\n\n        # sub task\n        for sub in [\'sub1\', \'sub2\']:\n            if hasattr(self, \'dec_fwd_\' + sub):\n                getattr(self, \'dec_fwd_\' + sub).reset_session()\n\n    def forward(self, batch, task=\'all\', is_eval=False, teacher=None, teacher_lm=None):\n        """"""Forward computation.\n\n        Args:\n            batch (dict):\n                xs (list): input data of size `[T, input_dim]`\n                xlens (list): lengths of each element in xs\n                ys (list): reference labels in the main task of size `[L]`\n                ys_sub1 (list): reference labels in the 1st auxiliary task of size `[L_sub1]`\n                ys_sub2 (list): reference labels in the 2nd auxiliary task of size `[L_sub2]`\n                utt_ids (list): name of utterances\n                speakers (list): name of speakers\n            task (str): all/ys*/ys_sub*\n            is_eval (bool): evaluation mode\n                This should be used in inference model for memory efficiency.\n            teacher (Speech2Text): used for knowledge distillation from ASR\n            teacher_lm (RNNLM): used for knowledge distillation from LM\n        Returns:\n            loss (FloatTensor): `[1]`\n            observation (dict):\n\n        """"""\n        if is_eval:\n            self.eval()\n            with torch.no_grad():\n                loss, observation = self._forward(batch, task)\n        else:\n            self.train()\n            loss, observation = self._forward(batch, task, teacher, teacher_lm)\n\n        return loss, observation\n\n    def _forward(self, batch, task, teacher=None, teacher_lm=None):\n        # Encode input features\n        if self.input_type == \'speech\':\n            if self.mtl_per_batch:\n                eout_dict = self.encode(batch[\'xs\'], task)\n            else:\n                eout_dict = self.encode(batch[\'xs\'], \'all\')\n        else:\n            eout_dict = self.encode(batch[\'ys_sub1\'])\n\n        observation = {}\n        loss = torch.zeros((1,), dtype=torch.float32)\n        if self.device_id >= 0:\n            loss = loss.cuda(self.device_id)\n\n        # for the forward decoder in the main task\n        if (self.fwd_weight > 0 or (self.bwd_weight == 0 and self.ctc_weight > 0) or self.mbr_training) and task in [\'all\', \'ys\', \'ys.ctc\', \'ys.mbr\']:\n            teacher_logits = None\n            if teacher is not None:\n                teacher.eval()\n                teacher_logits = teacher.generate_logits(batch)\n                # TODO(hirofumi): label smoothing, scheduled sampling, dropout?\n            elif teacher_lm is not None:\n                teacher_lm.eval()\n                teacher_logits = self.generate_lm_logits(batch[\'ys\'], lm=teacher_lm)\n\n            loss_fwd, obs_fwd = self.dec_fwd(eout_dict[\'ys\'][\'xs\'], eout_dict[\'ys\'][\'xlens\'],\n                                             batch[\'ys\'], task,\n                                             teacher_logits, self.recog_params, self.idx2token)\n            loss += loss_fwd\n            if isinstance(self.dec_fwd, RNNTransducer):\n                observation[\'loss.transducer\'] = obs_fwd[\'loss_transducer\']\n            else:\n                observation[\'acc.att\'] = obs_fwd[\'acc_att\']\n                observation[\'ppl.att\'] = obs_fwd[\'ppl_att\']\n                observation[\'loss.att\'] = obs_fwd[\'loss_att\']\n                observation[\'loss.mbr\'] = obs_fwd[\'loss_mbr\']\n                if \'loss_quantity\' not in obs_fwd.keys():\n                    obs_fwd[\'loss_quantity\'] = None\n                observation[\'loss.quantity\'] = obs_fwd[\'loss_quantity\']\n\n                if \'loss_latency\' not in obs_fwd.keys():\n                    obs_fwd[\'loss_latency\'] = None\n                observation[\'loss.latency\'] = obs_fwd[\'loss_latency\']\n\n            observation[\'loss.ctc\'] = obs_fwd[\'loss_ctc\']\n\n        # for the backward decoder in the main task\n        if self.bwd_weight > 0 and task in [\'all\', \'ys.bwd\']:\n            loss_bwd, obs_bwd = self.dec_bwd(eout_dict[\'ys\'][\'xs\'], eout_dict[\'ys\'][\'xlens\'], batch[\'ys\'], task)\n            loss += loss_bwd\n            observation[\'loss.att-bwd\'] = obs_bwd[\'loss_att\']\n            observation[\'acc.att-bwd\'] = obs_bwd[\'acc_att\']\n            observation[\'ppl.att-bwd\'] = obs_bwd[\'ppl_att\']\n            observation[\'loss.ctc-bwd\'] = obs_bwd[\'loss_ctc\']\n\n        # only fwd for sub tasks\n        for sub in [\'sub1\', \'sub2\']:\n            # for the forward decoder in the sub tasks\n            if (getattr(self, \'fwd_weight_\' + sub) > 0 or getattr(self, \'ctc_weight_\' + sub) > 0) and task in [\'all\', \'ys_\' + sub, \'ys_\' + sub + \'.ctc\']:\n                loss_sub, obs_fwd_sub = getattr(self, \'dec_fwd_\' + sub)(\n                    eout_dict[\'ys_\' + sub][\'xs\'], eout_dict[\'ys_\' + sub][\'xlens\'],\n                    batch[\'ys_\' + sub], task)\n                loss += loss_sub\n                if isinstance(getattr(self, \'dec_fwd_\' + sub), RNNTransducer):\n                    observation[\'loss.transducer-\' + sub] = obs_fwd_sub[\'loss_transducer\']\n                else:\n                    observation[\'loss.att-\' + sub] = obs_fwd_sub[\'loss_att\']\n                    observation[\'acc.att-\' + sub] = obs_fwd_sub[\'acc_att\']\n                    observation[\'ppl.att-\' + sub] = obs_fwd_sub[\'ppl_att\']\n                observation[\'loss.ctc-\' + sub] = obs_fwd_sub[\'loss_ctc\']\n\n        return loss, observation\n\n    def generate_logits(self, batch, temperature=1.0):\n        # Encode input features\n        if self.input_type == \'speech\':\n            eout_dict = self.encode(batch[\'xs\'], task=\'ys\')\n        else:\n            eout_dict = self.encode(batch[\'ys_sub1\'], task=\'ys\')\n\n        # for the forward decoder in the main task\n        logits = self.dec_fwd.forward_att(\n            eout_dict[\'ys\'][\'xs\'], eout_dict[\'ys\'][\'xlens\'], batch[\'ys\'],\n            return_logits=True)\n        return logits\n\n    def generate_lm_logits(self, ys, lm, temperature=5.0):\n        # Append <sos> and <eos>\n        eos = next(lm.parameters()).new_zeros(1).fill_(self.eos).long()\n        ys = [np2tensor(np.fromiter(y, dtype=np.int64), self.device_id)for y in ys]\n        ys_in = pad_list([torch.cat([eos, y], dim=0) for y in ys], self.pad)\n        lmout, _ = lm.decode(ys_in, None)\n        logits = lm.output(lmout)\n        return logits\n\n    def encode(self, xs, task=\'all\', use_cache=False, streaming=False):\n        """"""Encode acoustic or text features.\n\n        Args:\n            xs (list): A list of length `[B]`, which contains Tensor of size `[T, input_dim]`\n            task (str): all/ys*/ys_sub1*/ys_sub2*\n            use_cache (bool): use the cached forward encoder state in the previous chunk as the initial state\n            streaming (bool): streaming encoding\n        Returns:\n            eout_dict (dict):\n\n        """"""\n        if self.input_type == \'speech\':\n            # Frame stacking\n            if self.n_stacks > 1:\n                xs = [stack_frame(x, self.n_stacks, self.n_skips) for x in xs]\n\n            # Splicing\n            if self.n_splices > 1:\n                xs = [splice(x, self.n_splices, self.n_stacks) for x in xs]\n            xlens = torch.IntTensor([len(x) for x in xs])\n\n            xs = pad_list([np2tensor(x, self.device_id).float() for x in xs], 0.)\n\n            # SpecAugment\n            if self.use_specaug and self.training:\n                xs = self.specaug(xs)\n                if self.weight_noise:\n                    self.add_weight_noise(std=0.075)\n\n            # Gaussian noise injection\n            if self.gaussian_noise:\n                xs = add_gaussian_noise(xs)\n\n            # Sequence summary network\n            if self.ssn is not None:\n                xs += self.ssn(xs, xlens)\n\n        elif self.input_type == \'text\':\n            xlens = torch.IntTensor([len(x) for x in xs])\n            xs = [np2tensor(np.fromiter(x, dtype=np.int64), self.device_id) for x in xs]\n            xs = pad_list(xs, self.pad)\n            xs = self.dropout_emb(self.embed(xs))\n            # TODO(hirofumi): fix for Transformer\n\n        # encoder\n        eout_dict = self.enc(xs, xlens, task.split(\'.\')[0], use_cache, streaming)\n\n        if self.main_weight < 1 and self.enc_type in [\'conv\', \'tds\', \'gated_conv\', \'transformer\', \'conv_transformer\']:\n            for sub in [\'sub1\', \'sub2\']:\n                eout_dict[\'ys_\' + sub][\'xs\'] = eout_dict[\'ys\'][\'xs\'].clone()\n                eout_dict[\'ys_\' + sub][\'xlens\'] = eout_dict[\'ys\'][\'xlens\'][:]\n\n        return eout_dict\n\n    def get_ctc_probs(self, xs, task=\'ys\', temperature=1, topk=None):\n        self.eval()\n        with torch.no_grad():\n            eout_dict = self.encode(xs, task)\n            dir = \'fwd\' if self.fwd_weight >= self.bwd_weight else \'bwd\'\n            if task == \'ys_sub1\':\n                dir += \'_sub1\'\n            elif task == \'ys_sub2\':\n                dir += \'_sub2\'\n\n            if task == \'ys\':\n                assert self.ctc_weight > 0\n            elif task == \'ys_sub1\':\n                assert self.ctc_weight_sub1 > 0\n            elif task == \'ys_sub2\':\n                assert self.ctc_weight_sub2 > 0\n            ctc_probs, indices_topk = getattr(self, \'dec_\' + dir).ctc_probs_topk(\n                eout_dict[task][\'xs\'], temperature, topk)\n            return tensor2np(ctc_probs), tensor2np(indices_topk), eout_dict[task][\'xlens\']\n\n    def plot_attention(self):\n        if \'transformer\' in self.enc_type:\n            self.enc._plot_attention(self.save_path)\n        if \'transformer\' in self.dec_type or \'las\' in self.dec_type:\n            self.dec_fwd._plot_attention(self.save_path)\n\n    def decode_streaming(self, xs, params, idx2token, exclude_eos=False, task=\'ys\'):\n        # check configurations\n        assert task == \'ys\'\n        assert self.input_type == \'speech\'\n        assert self.ctc_weight > 0\n        assert self.fwd_weight > 0\n        assert len(xs) == 1  # batch size\n        # assert params[\'recog_length_norm\']\n        global_params = copy.deepcopy(params)\n        global_params[\'recog_max_len_ratio\'] = 1.0\n\n        # hyper parameters\n        ctc_vad = params[\'recog_ctc_vad\']\n        BLANK_THRESHOLD = params[\'recog_ctc_vad_blank_threshold\']\n        SPIKE_THRESHOLD = params[\'recog_ctc_vad_spike_threshold\']\n        MAX_N_ACCUM_FRAMES = params[\'recog_ctc_vad_n_accum_frames\']\n\n        N_l = self.enc.chunk_size_left\n        N_r = self.enc.chunk_size_right\n        if N_l == 0 and N_r == 0:\n            # N_l = params[\'lc_chunk_size_left\']  # for unidirectional encoder\n            N_l = 40\n        factor = self.enc.subsampling_factor\n        BLANK_THRESHOLD /= factor\n        x_whole = xs[0]  # `[T, input_dim]`\n        if self.enc.conv is not None:\n            self.enc.turn_off_ceil_mode(self.enc)\n\n        self.eval()\n        with torch.no_grad():\n            lm = getattr(self, \'lm_fwd\', None)\n            lm_second = getattr(self, \'lm_second\', None)\n\n            eout_chunks = []\n            ctc_probs_chunks = []\n            t = 0  # global time offset\n            n_blanks = 0\n            n_accum_frames = 0\n            boundary_offset = -1  # boudnary offset in each chunk (after subsampling)\n            is_reset = True   # for the first step\n            hyps = None\n            best_hyp_id_stream = []\n            while True:\n                # Encode input features chunk by chunk\n                if self.enc.conv is not None:\n                    x_chunk = x_whole[max(0, t - 1):t + (N_l + N_r) + 3]\n                else:\n                    x_chunk = x_whole[t:t + (N_l + N_r)]\n                is_last_chunk = t + N_l >= len(x_whole) - 1\n                eout_dict_chunk = self.encode([x_chunk], task, use_cache=not is_reset, streaming=True)\n                eout_chunk = eout_dict_chunk[task][\'xs\']\n                boundary_offset = -1  # reset\n                is_reset = False  # detect the first boundary in the same chunk\n                n_accum_frames += eout_chunk.size(1) * factor\n\n                # CTC-based VAD\n                ctc_log_probs_chunk = None\n                if ctc_vad:\n                    ctc_probs_chunk = self.dec_fwd.ctc_probs(eout_chunk)\n                    if params[\'recog_ctc_weight\'] > 0:\n                        ctc_log_probs_chunk = torch.log(ctc_probs_chunk)\n\n                    # Segmentation strategy 1:\n                    # If any segmentation points are not found in the current chunk,\n                    # encoder states will be carried over to the next chunk.\n                    # Otherwise, the current chunk is segmented at the point where\n                    # n_blanks surpasses the threshold.\n                    if n_accum_frames >= MAX_N_ACCUM_FRAMES:\n                        _, topk_ids_chunk = torch.topk(ctc_probs_chunk, k=1, dim=-1, largest=True, sorted=True)\n                        ctc_probs_chunks.append(ctc_probs_chunk)\n                        for j in range(ctc_probs_chunk.size(1)):\n                            if topk_ids_chunk[0, j, 0] == self.blank:\n                                n_blanks += 1\n                                # print(\'CTC (T:%d): <blank>\' % (t + j * factor))\n                            elif ctc_probs_chunk[0, j, topk_ids_chunk[0, j, 0]] < SPIKE_THRESHOLD:\n                                n_blanks += 1\n                                # print(\'CTC (T:%d): <blank>\' % (t + j * factor))\n                            else:\n                                n_blanks = 0\n                                # print(\'CTC (T:%d): %s\' % (t + j * factor,\n                                #                           idx2token([topk_ids_chunk[0, j, 0].item()])))\n                            if not is_reset and n_blanks > BLANK_THRESHOLD:\n                                boundary_offset = j  # select the most right blank offset\n                                is_reset = True\n\n                # Truncate the most right frames\n                if is_reset and not is_last_chunk:\n                    eout_chunk = eout_chunk[:, :boundary_offset + 1]\n                eout_chunks.append(eout_chunk)\n\n                # Chunk-synchronous attention decoding\n                if params[\'recog_chunk_sync\']:\n                    end_hyps, hyps, aws_seg = self.dec_fwd.beam_search_chunk_sync(\n                        eout_chunk, params, idx2token, lm,\n                        ctc_log_probs=ctc_log_probs_chunk, hyps=hyps,\n                        state_carry_over=False,\n                        ignore_eos=self.enc.rnn_type in [\'lstm\', \'conv_lstm\'])\n                    merged_hyps = sorted(end_hyps + hyps, key=lambda x: x[\'score\'], reverse=True)\n                    best_hyp_id_prefix = np.array(merged_hyps[0][\'hyp\'][1:])\n                    if len(best_hyp_id_prefix) > 0 and best_hyp_id_prefix[-1] == self.eos:\n                        # reset beam if <eos> is generated from the best hypothesis\n                        best_hyp_id_prefix = best_hyp_id_prefix[:-1]  # exclude <eos>\n                        # Segmentation strategy 2:\n                        # If <eos> is emitted from the decoder (not CTC),\n                        # the current chunk is segmented.\n                        if not is_reset:\n                            boundary_offset = eout_chunk.size(1) - 1\n                            is_reset = True\n                    # print(\'\\rSync MoChA (T:%d, offset:%d, blank:%d frames): %s\' %\n                    #       (t + eout_chunk.size(1) * factor,\n                    #        self.dec_fwd.n_frames * factor,\n                    #        n_blanks * factor, idx2token(best_hyp_id_prefix)), end=\'\')\n                    # print(\'\\rSync MoChA (T:%d, offset:%d, blank:%d frames): %s\' %\n                    #       (t + eout_chunk.size(1) * factor,\n                    #        self.dec_fwd.n_frames * factor,\n                    #        n_blanks * factor, idx2token(best_hyp_id_prefix)))\n                    # print(\'-\' * 50)\n\n                if is_reset:\n                    # Global decoding over the segmented region\n                    if not params[\'recog_chunk_sync\']:\n                        eout = torch.cat(eout_chunks, dim=1)\n                        elens = torch.IntTensor([eout.size(1)])\n                        ctc_log_probs = None\n                        if params[\'recog_ctc_weight\'] > 0:\n                            ctc_log_probs = torch.log(self.dec_fwd.ctc_probs(eout))\n                        nbest_hyps_id_offline, _, _ = self.dec_fwd.beam_search(\n                            eout, elens, global_params, idx2token, lm, lm_second,\n                            ctc_log_probs=ctc_log_probs)\n                        # print(\'Offline MoChA (T:%d): %s\' %\n                        #       (t + eout_chunk.size(1) * factor,\n                        #        idx2token(nbest_hyps_id_offline[0][0])))\n                    eout = torch.cat(eout_chunks, dim=1)\n                    elens = torch.IntTensor([eout.size(1)])\n                    ctc_log_probs = None\n                    nbest_hyps_id_offline, _, _ = self.dec_fwd.beam_search(\n                        eout, elens, global_params, idx2token, lm, lm_second,\n                        ctc_log_probs=ctc_log_probs)\n                    # print(\'Offline MoChA (T:%d): %s\' %\n                    #       (t + eout_chunk.size(1) * factor,\n                    #        idx2token(nbest_hyps_id_offline[0][0])))\n\n                    # pick up the best hyp from ended and active hypotheses\n                    if not params[\'recog_chunk_sync\']:\n                        if len(nbest_hyps_id_offline[0][0]) > 0:\n                            best_hyp_id_stream.extend(nbest_hyps_id_offline[0][0])\n                    else:\n                        if len(best_hyp_id_prefix) > 0:\n                            best_hyp_id_stream.extend(best_hyp_id_prefix)\n                        # print(\'Final Sync MoChA (T:%d, segment:%d frames): %s\' %\n                        #       (t + eout_chunk.size(1) * factor,\n                        #        self.dec_fwd.n_frames * factor,\n                        #        idx2token(best_hyp_id_prefix)))\n                        # print(\'-\' * 50)\n                        # for test\n                        # eos_hyp = np.zeros(1, dtype=np.int32)\n                        # eos_hyp[0] = self.eos\n                        # best_hyp_id_stream.extend(eos_hyp)\n\n                    # reset\n                    eout_chunks = []\n                    ctc_probs_chunks = []\n                    n_blanks = 0\n                    n_accum_frames = 0\n                    hyps = None\n\n                    # next chunk will start from the frame next to the boundary\n                    if not is_last_chunk and 0 <= boundary_offset * factor < N_l - 1:\n                        t -= x_chunk[(boundary_offset + 1) * factor:N_l].shape[0]\n                        self.dec_fwd.n_frames -= x_chunk[(boundary_offset + 1) * factor:N_l].shape[0] // factor\n                        # print(\'Back %d frames\' % (x_chunk[(boundary_offset + 1) * factor:N_l].shape[0]))\n\n                t += N_l\n                if is_last_chunk:\n                    break\n\n            # Global decoding over the last chunk\n            if not params[\'recog_chunk_sync\'] and len(eout_chunks) > 0:\n                eout = torch.cat(eout_chunks, dim=1)\n                elens = torch.IntTensor([eout.size(1)])\n                nbest_hyps_id_offline, _, _ = self.dec_fwd.beam_search(\n                    eout, elens, global_params, idx2token, lm, lm_second, None)\n                # print(\'MoChA: \' + idx2token(nbest_hyps_id_offline[0][0]))\n                # print(\'*\' * 50)\n                if len(nbest_hyps_id_offline[0][0]) > 0:\n                    best_hyp_id_stream.extend(nbest_hyps_id_offline[0][0])\n\n            # pick up the best hyp\n            if not is_reset and params[\'recog_chunk_sync\'] and len(best_hyp_id_prefix) > 0:\n                best_hyp_id_stream.extend(best_hyp_id_prefix)\n\n            if len(best_hyp_id_stream) > 0:\n                return [np.stack(best_hyp_id_stream, axis=0)], [None]\n            else:\n                return [[]], [None]\n\n    def streamable(self):\n        return getattr(self.dec_fwd, \'streamable\', False)\n\n    def quantity_rate(self):\n        return getattr(self.dec_fwd, \'quantity_rate\', 1.0)\n\n    def last_success_frame_ratio(self):\n        return getattr(self.dec_fwd, \'last_success_frame_ratio\', 0)\n\n    def decode(self, xs, params, idx2token, exclude_eos=False,\n               refs_id=None, refs=None, utt_ids=None, speakers=None,\n               task=\'ys\', ensemble_models=[]):\n        """"""Decoding in the inference stage.\n\n        Args:\n            xs (list): A list of length `[B]`, which contains arrays of size `[T, input_dim]`\n            params (dict): hyper-parameters for decoding\n                beam_width (int): the size of beam\n                min_len_ratio (float):\n                max_len_ratio (float):\n                len_penalty (float): length penalty\n                cov_penalty (float): coverage penalty\n                cov_threshold (float): threshold for coverage penalty\n                lm_weight (float): the weight of RNNLM score\n                resolving_unk (bool): not used (to make compatible)\n                fwd_bwd_attention (bool):\n            idx2token (): converter from index to token\n            exclude_eos (bool): exclude <eos> from best_hyps_id\n            refs_id (list): gold token IDs to compute log likelihood\n            refs (list): gold transcriptions\n            utt_ids (list):\n            speakers (list):\n            task (str): ys* or ys_sub1* or ys_sub2*\n            ensemble_models (list): list of Speech2Text classes\n        Returns:\n            best_hyps_id (list): A list of length `[B]`, which contains arrays of size `[L]`\n            aws (list): A list of length `[B]`, which contains arrays of size `[L, T, n_heads]`\n\n        """"""\n        if task.split(\'.\')[0] == \'ys\':\n            dir = \'bwd\' if self.bwd_weight > 0 and params[\'recog_bwd_attention\'] else \'fwd\'\n        elif task.split(\'.\')[0] == \'ys_sub1\':\n            dir = \'fwd_sub1\'\n        elif task.split(\'.\')[0] == \'ys_sub2\':\n            dir = \'fwd_sub2\'\n        else:\n            raise ValueError(task)\n\n        if utt_ids is not None:\n            if self.utt_id_prev != utt_ids[0]:\n                self.reset_session()\n            self.utt_id_prev = utt_ids[0]\n\n        self.eval()\n        with torch.no_grad():\n            # Encode input features\n            if self.input_type == \'speech\' and self.mtl_per_batch and \'bwd\' in dir:\n                eout_dict = self.encode(xs, task)\n            else:\n                eout_dict = self.encode(xs, task)\n\n            # CTC\n            if (self.fwd_weight == 0 and self.bwd_weight == 0) or (self.ctc_weight > 0 and params[\'recog_ctc_weight\'] == 1):\n                lm = getattr(self, \'lm_\' + dir, None)\n                lm_second = getattr(self, \'lm_second\', None)\n                lm_second_bwd = None  # TODO\n\n                best_hyps_id = getattr(self, \'dec_\' + dir).decode_ctc(\n                    eout_dict[task][\'xs\'], eout_dict[task][\'xlens\'], params, idx2token,\n                    lm, lm_second, lm_second_bwd, 1, refs_id, utt_ids, speakers)\n                return best_hyps_id, None\n\n            # Attention\n            elif params[\'recog_beam_width\'] == 1 and not params[\'recog_fwd_bwd_attention\']:\n                best_hyps_id, aws = getattr(self, \'dec_\' + dir).greedy(\n                    eout_dict[task][\'xs\'], eout_dict[task][\'xlens\'],\n                    params[\'recog_max_len_ratio\'], idx2token,\n                    exclude_eos, refs_id, utt_ids, speakers)\n            else:\n                assert params[\'recog_batch_size\'] == 1\n\n                ctc_log_probs = None\n                if params[\'recog_ctc_weight\'] > 0:\n                    ctc_log_probs = self.dec_fwd.ctc_log_probs(eout_dict[task][\'xs\'])\n\n                # forward-backward decoding\n                if params[\'recog_fwd_bwd_attention\']:\n                    lm_fwd = getattr(self, \'lm_fwd\', None)\n                    lm_bwd = getattr(self, \'lm_bwd\', None)\n\n                    # forward decoder\n                    nbest_hyps_id_fwd, aws_fwd, scores_fwd = self.dec_fwd.beam_search(\n                        eout_dict[task][\'xs\'], eout_dict[task][\'xlens\'],\n                        params, idx2token, lm_fwd, None, lm_bwd, ctc_log_probs,\n                        params[\'recog_beam_width\'], False, refs_id, utt_ids, speakers)\n\n                    # backward decoder\n                    nbest_hyps_id_bwd, aws_bwd, scores_bwd, _ = self.dec_bwd.beam_search(\n                        eout_dict[task][\'xs\'], eout_dict[task][\'xlens\'],\n                        params, idx2token, lm_bwd, None, lm_fwd, ctc_log_probs,\n                        params[\'recog_beam_width\'], False, refs_id, utt_ids, speakers)\n\n                    # forward-backward attention\n                    best_hyps_id = fwd_bwd_attention(\n                        nbest_hyps_id_fwd, aws_fwd, scores_fwd,\n                        nbest_hyps_id_bwd, aws_bwd, scores_bwd,\n                        self.eos, params[\'recog_gnmt_decoding\'], params[\'recog_length_penalty\'],\n                        idx2token, refs_id)\n                    aws = None\n                else:\n                    # ensemble\n                    ensmbl_eouts, ensmbl_elens, ensmbl_decs = [], [], []\n                    if len(ensemble_models) > 0:\n                        for i_e, model in enumerate(ensemble_models):\n                            if model.input_type == \'speech\' and model.mtl_per_batch and \'bwd\' in dir:\n                                enc_outs_e = model.encode(xs, task)\n                            else:\n                                enc_outs_e = model.encode(xs, task)\n                            ensmbl_eouts += [enc_outs_e[task][\'xs\']]\n                            ensmbl_elens += [enc_outs_e[task][\'xlens\']]\n                            ensmbl_decs += [getattr(model, \'dec_\' + dir)]\n                            # NOTE: only support for the main task now\n\n                    lm = getattr(self, \'lm_\' + dir, None)\n                    lm_second = getattr(self, \'lm_second\', None)\n                    lm_bwd = getattr(self, \'lm_bwd\' if dir == \'fwd\' else \'lm_bwd\', None)\n\n                    nbest_hyps_id, aws, scores = getattr(self, \'dec_\' + dir).beam_search(\n                        eout_dict[task][\'xs\'], eout_dict[task][\'xlens\'],\n                        params, idx2token, lm, lm_second, lm_bwd, ctc_log_probs,\n                        1, exclude_eos, refs_id, utt_ids, speakers,\n                        ensmbl_eouts, ensmbl_elens, ensmbl_decs)\n                    best_hyps_id = [hyp[0] for hyp in nbest_hyps_id]\n\n            return best_hyps_id, aws\n'"
examples/csj/s5/local/remove_disfluency.py,0,"b'#! /usr/bin/env python\n# -*- coding: utf-8 -*-\n\n# Copyright 2019 Kyoto University (Hirofumi Inaguma)\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n\n""""""Remove filler and disfluency based on POS tag.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport argparse\nimport codecs\nimport re\nfrom tqdm import tqdm\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\'text\', type=str,\n                    help=\'path to text file\')\nargs = parser.parse_args()\n\n\ndef main():\n\n    with codecs.open(args.text, \'r\', encoding=""utf-8"") as f:\n        pbar = tqdm(total=len(open(args.text).readlines()))\n        for line in f:\n            line = line.strip()\n\n            utt_id = line.split()[0]\n            words = line.split()[1:]\n\n            # Remove filler and disfluency\n            text = \' \'.join([\'\' if \'\xe8\xa8\x80\xe3\x81\x84\xe3\x82\x88\xe3\x81\xa9\xe3\x81\xbf\' in w or \'\xe6\x84\x9f\xe5\x8b\x95\xe8\xa9\x9e\' in w else w for w in words])\n\n            # Remove conseccutive spaces\n            text = re.sub(r\'[\\s]+\', \' \', text)\n\n            # Remove the first and last spaces\n            if len(text) > 0 and text[0] == \' \':\n                text = text[1:]\n            if len(text) > 0 and text[-1] == \' \':\n                text = text[:-1]\n\n            if len(text) > 0:\n                print(utt_id + \' \' + text)\n            pbar.update(1)\n\n\nif __name__ == \'__main__\':\n    main()\n'"
examples/csj/s5/local/remove_pos.py,0,"b'#! /usr/bin/env python\n# -*- coding: utf-8 -*-\n\n# Copyright 2018 Kyoto University (Hirofumi Inaguma)\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n\n""""""Remove POS tag.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport argparse\nimport codecs\nimport re\nfrom tqdm import tqdm\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\'text\', type=str,\n                    help=\'path to text file\')\nargs = parser.parse_args()\n\n\ndef main():\n\n    with codecs.open(args.text, \'r\', encoding=""utf-8"") as f:\n        pbar = tqdm(total=len(open(args.text).readlines()))\n        for line in f:\n            line = line.strip()\n\n            utt_id = line.split()[0]\n            words = line.split()[1:]\n\n            # Remove POS tag\n            text = \' \'.join(list(map(lambda x: x.split(\'+\')[0], words)))\n\n            # Remove <sp> (short pause)\n            text = text.replace(\'<sp>\', \'\')\n\n            # Remove conseccutive spaces\n            text = re.sub(r\'[\\s]+\', \' \', text)\n\n            # Remove the first and last spaces\n            if len(text) > 0 and text[0] == \' \':\n                text = text[1:]\n            if len(text) > 0 and text[-1] == \' \':\n                text = text[:-1]\n\n            print(utt_id + \' \' + text)\n            pbar.update(1)\n\n\nif __name__ == \'__main__\':\n    main()\n'"
examples/swbd/s5c/local/format_acronyms_dict.py,0,"b'#!/usr/bin/env python\n\n# Copyright 2015  Minhua Wu\n# Apache 2.0\n\n# convert acronyms in swbd dict to fisher convention\n# IBM to i._b._m.\n# BBC to b._b._c.\n# BBCs to b._b._c.s\n# BBC\'s to b._b._c.\'s\n\nimport argparse\nimport re\n__author__ = \'Minhua Wu\'\n\nparser = argparse.ArgumentParser(description=\'format acronyms to a._b._c.\')\nparser.add_argument(\'-i\', \'--input\', help=\'Input lexicon\', required=True)\nparser.add_argument(\'-o\', \'--output\', help=\'Output lexicon\', required=True)\nparser.add_argument(\n    \'-L\', \'--Letter\', help=\'Input single letter pronunciation\', required=True)\nparser.add_argument(\n    \'-M\', \'--Map\', help=\'Output acronyms mapping\', required=True)\nargs = parser.parse_args()\n\n\nfin_lex = open(args.input, ""r"")\nfin_Letter = open(args.Letter, ""r"")\nfout_lex = open(args.output, ""w"")\nfout_map = open(args.Map, ""w"")\n\n# Initialise single letter dictionary\ndict_letter = {}\nfor single_letter_lex in fin_Letter:\n    items = single_letter_lex.split()\n    dict_letter[items[0]] = single_letter_lex[len(items[0]) + 1:].strip()\nfin_Letter.close()\n# print dict_letter\n\nfor lex in fin_lex:\n    items = lex.split()\n    word = items[0]\n    lexicon = lex[len(items[0]) + 1:].strip()\n    # find acronyms from words with only letters and \'\n    pre_match = re.match(r\'^[A-Za-z]+$|^[A-Za-z]+\\\'s$|^[A-Za-z]+s$\', word)\n    if pre_match:\n        # find if words in the form of xxx\'s is acronym\n        if word[-2:] == \'\\\'s\' and (lexicon[-1] == \'s\' or lexicon[-1] == \'z\'):\n            actual_word = word[:-2]\n            actual_lexicon = lexicon[:-2]\n            acronym_lexicon = """"\n            for l in actual_word:\n                acronym_lexicon = acronym_lexicon + \\\n                    dict_letter[l.upper()] + "" ""\n            if acronym_lexicon.strip() == actual_lexicon:\n                acronym_mapped = """"\n                acronym_mapped_back = """"\n                for l in actual_word[:-1]:\n                    acronym_mapped = acronym_mapped + l.lower() + \'._\'\n                    acronym_mapped_back = acronym_mapped_back + l.lower() + \' \'\n                acronym_mapped = acronym_mapped + \\\n                    actual_word[-1].lower() + \'.\\\'s\'\n                acronym_mapped_back = acronym_mapped_back + \\\n                    actual_word[-1].lower() + \'\\\'s\'\n                fout_map.write(word + \'\\t\' + acronym_mapped +\n                               \'\\t\' + acronym_mapped_back + \'\\n\')\n                fout_lex.write(acronym_mapped + \' \' + lexicon + \'\\n\')\n            else:\n                fout_lex.write(lex)\n\n        # find if words in the form of xxxs is acronym\n        elif word[-1] == \'s\' and (lexicon[-1] == \'s\' or lexicon[-1] == \'z\'):\n            actual_word = word[:-1]\n            actual_lexicon = lexicon[:-2]\n            acronym_lexicon = """"\n            for l in actual_word:\n                acronym_lexicon = acronym_lexicon + \\\n                    dict_letter[l.upper()] + "" ""\n            if acronym_lexicon.strip() == actual_lexicon:\n                acronym_mapped = """"\n                acronym_mapped_back = """"\n                for l in actual_word[:-1]:\n                    acronym_mapped = acronym_mapped + l.lower() + \'._\'\n                    acronym_mapped_back = acronym_mapped_back + l.lower() + \' \'\n                acronym_mapped = acronym_mapped + \\\n                    actual_word[-1].lower() + \'.s\'\n                acronym_mapped_back = acronym_mapped_back + \\\n                    actual_word[-1].lower() + \'\\\'s\'\n                fout_map.write(word + \'\\t\' + acronym_mapped +\n                               \'\\t\' + acronym_mapped_back + \'\\n\')\n                fout_lex.write(acronym_mapped + \' \' + lexicon + \'\\n\')\n            else:\n                fout_lex.write(lex)\n\n        # find if words in the form of xxx (not ended with \'s or s) is acronym\n        elif word.find(\'\\\'\') == -1 and word[-1] != \'s\':\n            acronym_lexicon = """"\n            for l in word:\n                acronym_lexicon = acronym_lexicon + \\\n                    dict_letter[l.upper()] + "" ""\n            if acronym_lexicon.strip() == lexicon:\n                acronym_mapped = """"\n                acronym_mapped_back = """"\n                for l in word[:-1]:\n                    acronym_mapped = acronym_mapped + l.lower() + \'._\'\n                    acronym_mapped_back = acronym_mapped_back + l.lower() + \' \'\n                acronym_mapped = acronym_mapped + word[-1].lower() + \'.\'\n                acronym_mapped_back = acronym_mapped_back + word[-1].lower()\n                fout_map.write(word + \'\\t\' + acronym_mapped +\n                               \'\\t\' + acronym_mapped_back + \'\\n\')\n                fout_lex.write(acronym_mapped + \' \' + lexicon + \'\\n\')\n            else:\n                fout_lex.write(lex)\n        else:\n            fout_lex.write(lex)\n\n    else:\n        fout_lex.write(lex)\n'"
examples/swbd/s5c/local/format_acronyms_dict_fisher_swbd.py,0,"b'#!/usr/bin/env python\n\n# Copyright 2015  Minhua Wu\n# Apache 2.0\n\n# convert acronyms in swbd dict to fisher convention\n# IBM to i._b._m.\n# BBC to b._b._c.\n# BBCs to b._b._c.s\n# BBC\'s to b._b._c.\'s\n\nimport argparse\nimport re\n__author__ = \'Minhua Wu\'\n\nparser = argparse.ArgumentParser(description=\'format acronyms to a._b._c.\')\nparser.add_argument(\'-i\', \'--input\', help=\'Input lexicon\', required=True)\nparser.add_argument(\'-o1\', \'--output1\',\n                    help=\'Output acronym lexicon(mapped)\', required=True)\nparser.add_argument(\'-o2\', \'--output2\',\n                    help=\'Output acronym lexicon(original)\', required=True)\nparser.add_argument(\n    \'-L\', \'--Letter\', help=\'Input single letter pronunciation\', required=True)\nparser.add_argument(\n    \'-M\', \'--Map\', help=\'Output acronyms mapping\', required=True)\nargs = parser.parse_args()\n\n\nfin_lex = open(args.input, ""r"")\nfin_Letter = open(args.Letter, ""r"")\nfout_lex = open(args.output1, ""w"")\nfout_lex_ori = open(args.output2, ""w"")\nfout_map = open(args.Map, ""w"")\n\n# Initialise single letter dictionary\ndict_letter = {}\nfor single_letter_lex in fin_Letter:\n    items = single_letter_lex.split()\n    dict_letter[items[0]] = single_letter_lex[len(items[0]) + 1:].strip()\nfin_Letter.close()\n# print dict_letter\n\nfor lex in fin_lex:\n    items = lex.split()\n    word = items[0]\n    lexicon = lex[len(items[0]) + 1:].strip()\n    # find acronyms from words with only letters and \'\n    pre_match = re.match(r\'^[A-Za-z]+$|^[A-Za-z]+\\\'s$|^[A-Za-z]+s$\', word)\n    if pre_match:\n        # find if words in the form of xxx\'s is acronym\n        if word[-2:] == \'\\\'s\' and (lexicon[-1] == \'s\' or lexicon[-1] == \'z\'):\n            actual_word = word[:-2]\n            actual_lexicon = lexicon[:-2]\n            acronym_lexicon = """"\n            for l in actual_word:\n                acronym_lexicon = acronym_lexicon + \\\n                    dict_letter[l.upper()] + "" ""\n            if acronym_lexicon.strip() == actual_lexicon:\n                acronym_mapped = """"\n                for l in actual_word[:-1]:\n                    acronym_mapped = acronym_mapped + l.lower() + \'._\'\n                acronym_mapped = acronym_mapped + \\\n                    actual_word[-1].lower() + \'.\\\'s\'\n                fout_map.write(word + \'\\t\' + acronym_mapped + \'\\n\')\n                fout_lex.write(acronym_mapped + \' \' + lexicon + \'\\n\')\n                fout_lex_ori.write(word + \' \' + lexicon + \'\\n\')\n            else:\n                continue\n\n        # find if words in the form of xxxs is acronym\n        elif word[-1] == \'s\' and (lexicon[-1] == \'s\' or lexicon[-1] == \'z\'):\n            actual_word = word[:-1]\n            actual_lexicon = lexicon[:-2]\n            acronym_lexicon = """"\n            for l in actual_word:\n                acronym_lexicon = acronym_lexicon + \\\n                    dict_letter[l.upper()] + "" ""\n            if acronym_lexicon.strip() == actual_lexicon:\n                acronym_mapped = """"\n                for l in actual_word[:-1]:\n                    acronym_mapped = acronym_mapped + l.lower() + \'._\'\n                acronym_mapped = acronym_mapped + \\\n                    actual_word[-1].lower() + \'.s\'\n                fout_map.write(word + \'\\t\' + acronym_mapped + \'\\n\')\n                fout_lex.write(acronym_mapped + \' \' + lexicon + \'\\n\')\n                fout_lex_ori.write(word + \' \' + lexicon + \'\\n\')\n            else:\n                continue\n\n        # find if words in the form of xxx (not ended with \'s or s) is acronym\n        elif word.find(\'\\\'\') == -1 and word[-1] != \'s\':\n            acronym_lexicon = """"\n            for l in word:\n                acronym_lexicon = acronym_lexicon + \\\n                    dict_letter[l.upper()] + "" ""\n            if acronym_lexicon.strip() == lexicon:\n                acronym_mapped = """"\n                for l in word[:-1]:\n                    acronym_mapped = acronym_mapped + l.lower() + \'._\'\n                acronym_mapped = acronym_mapped + word[-1].lower() + \'.\'\n                fout_map.write(word + \'\\t\' + acronym_mapped + \'\\n\')\n                fout_lex.write(acronym_mapped + \' \' + lexicon + \'\\n\')\n                fout_lex_ori.write(word + \' \' + lexicon + \'\\n\')\n            else:\n                continue\n        else:\n            continue\n'"
examples/swbd/s5c/local/map_acronyms_ctm.py,0,"b'#!/usr/bin/env python\n\n# Copyright 2015  Minhua Wu\n# Apache 2.0\n\n# convert acronyms in swbd decode result\n# e.g. convert things like en_4156 B 414.26 0.65 u._c._l._a. to\n# en_4156 B 414.26 0.16 u\n# en_4156 B 414.42 0.16 c\n# en_4156 B 414.58 0.16 l\n# en_4156 B 414.74 0.17 a\n\nimport argparse\nimport re\n__author__ = \'Minhua Wu\'\n\nparser = argparse.ArgumentParser(\n    description=\'convert acronyms back to eval2000 format\')\nparser.add_argument(\'-i\', \'--input\', help=\'Input ctm file \', required=True)\nparser.add_argument(\'-o\', \'--output\', help=\'Output ctm file\', required=True)\nparser.add_argument(\'-M\', \'--Map\', help=\'Input acronyms map\', required=True)\nargs = parser.parse_args()\n\nif args.input == \'-\':\n    args.input = \'/dev/stdin\'\nif args.output == \'-\':\n    args.output = \'/dev/stdout\'\n\ndict_acronym_back = {}\nfin_map = open(args.Map, ""r"")\nfor line in fin_map:\n    items = line.split(\'\\t\')\n    dict_acronym_back[items[1]] = items[2]\n\nfin_map.close()\n\nfin = open(args.input, ""r"")\nfout = open(args.output, ""w"")\n\nfor line in fin:\n    items = line.split()\n\n    if items[4] in dict_acronym_back.keys():\n        letters = dict_acronym_back[items[4]].split()\n        acronym_period = round(float(items[3]), 2)\n        letter_slot = round(acronym_period / len(letters), 2)\n        time_start = round(float(items[2]), 2)\n        for l in letters[:-1]:\n            time = "" %.2f %.2f "" % (time_start, letter_slot)\n            fout.write(\' \'.join(items[:2]) + time + l + ""\\n"")\n            time_start = time_start + letter_slot\n        last_slot = acronym_period - letter_slot * (len(letters) - 1)\n        time = "" %.2f %.2f "" % (time_start, last_slot)\n        fout.write(\' \'.join(items[:2]) + time + letters[-1] + ""\\n"")\n    else:\n        fout.write(line)\n'"
examples/swbd/s5c/local/map_acronyms_transcripts.py,0,"b'#!/usr/bin/env python\n\n# Copyright 2015  Minhua Wu\n# Apache 2.0\n\n# convert acronyms in swbd transcript to fisher convention\n# accoring to first two columns in the input acronyms mapping\n\nimport argparse\nimport re\n__author__ = \'Minhua Wu\'\n\nparser = argparse.ArgumentParser(description=\'format acronyms to a._b._c.\')\nparser.add_argument(\'-i\', \'--input\', help=\'Input transcripts\', required=True)\nparser.add_argument(\'-o\', \'--output\', help=\'Output transcripts\', required=True)\nparser.add_argument(\n    \'-M\', \'--Map\', help=\'Input acronyms mapping\', required=True)\nargs = parser.parse_args()\n\nfin_map = open(args.Map, ""r"")\ndict_acronym = {}\ndict_acronym_noi = {}  # Mapping of acronyms without I, i\nfor pair in fin_map:\n    items = pair.split(\'\\t\')\n    dict_acronym[items[0]] = items[1]\n    dict_acronym_noi[items[0]] = items[1]\nfin_map.close()\ndel dict_acronym_noi[\'I\']\ndel dict_acronym_noi[\'i\']\n\n\nfin_trans = open(args.input, ""r"")\nfout_trans = open(args.output, ""w"")\nfor line in fin_trans:\n    items = line.split()\n    L = len(items)\n    # First pass mapping to map I as part of acronym\n    for i in range(L):\n        if items[i] == \'I\':\n            x = 0\n            while(i - 1 - x >= 0 and re.match(r\'^[A-Z]$\', items[i - 1 - x])):\n                x += 1\n\n            y = 0\n            while(i + 1 + y < L and re.match(r\'^[A-Z]$\', items[i + 1 + y])):\n                y += 1\n\n            if x + y > 0:\n                for bias in range(-x, y + 1):\n                    items[i + bias] = dict_acronym[items[i + bias]]\n\n    # Second pass mapping (not mapping \'i\' and \'I\')\n    for i in range(len(items)):\n        if items[i] in dict_acronym_noi.keys():\n            items[i] = dict_acronym_noi[items[i]]\n    sentence = \' \'.join(items[1:])\n    fout_trans.write(items[0] + \' \' + sentence.lower() + \'\\n\')\n\nfin_trans.close()\nfout_trans.close()\n'"
examples/swbd/s5c/local/remove_disfluency.py,0,"b'#! /usr/bin/env python\n# -*- coding: utf-8 -*-\n\n# Copyright 2019 Kyoto University (Hirofumi Inaguma)\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n\n""""""Remove filler and disfluency based on POS tag.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport argparse\nimport codecs\nimport re\nfrom tqdm import tqdm\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\'text\', type=str,\n                    help=\'path to text file\')\nargs = parser.parse_args()\n\n\nrepeat_2gram_exception = [\'bye bye\']\n\n\ndef main():\n\n    with codecs.open(args.text, \'r\', encoding=""utf-8"") as f:\n        pbar = tqdm(total=len(open(args.text).readlines()))\n        for line in f:\n            line = line.strip()\n\n            utt_id = line.split()[0]\n            text = \' \'.join(line.split()[1:])\n\n            # Remove repeats words (without interventing words) at first\n            w_prev = \'\'\n            w_prev_2gram = []\n            words = []\n            for w in text.split(\' \'):\n                if w_prev + \' \' + w in \' \'.join(w_prev_2gram):\n                    words = words[:-1]  # remove the last word\n                elif w != w_prev or (w_prev + \' \' + w in repeat_2gram_exception):\n                    words.append(w)\n\n                w_prev_2gram.append(w)\n                w_prev_2gram = w_prev_2gram[-3:]  # pruning\n                w_prev = w\n            text = \' \'.join(words)\n\n            # Remove noise\n            text = re.sub(r\'\\[noise\\]\', \' \', text)\n            text = re.sub(r\'\\[laughter\\]\', \' \', text)\n            text = re.sub(r\'\\[vocalized-noise\\]\', \' \', text)\n\n            # Remove filled pause (filler and backchannel)\n            for w in [\'uh-huh\', \'um-hum\', \'mhm\', \'mmhm\', \'mm-hm\', \'mm-huh\', \'huh-uh\',\n                      \'uhhuh\', \'uhuh\',\n                      \'uh\', \'um\', \'eh\', \'mm\', \'hm\', \'ah\', \'huh\', \'ha\', \'er\', \'oof\', \'hee\', \'ach\', \'eee\', \'ew\',\n                      \'you know\', \'i mean\']:\n                text = re.sub(r\'\\A%s\\s+\' % w, \' \', text)  # start\n                text = re.sub(r\'\\s+%s\\Z\' % w, \' \', text)  # end\n                text = re.sub(r\'\\A%s\\Z\' % w, \' \', text)  # single\n                text = re.sub(r\'\\s+%s\\s+\' % w, \' \', text)  # middle\n\n            # Remove fragment (partial words)\n            text = re.sub(r\'\\A([^\\s]+-)\\s+\', \' \', text)  # start\n            text = re.sub(r\'\\s+([^\\s]+-)\\Z\', \' \', text)  # end\n            text = re.sub(r\'\\A([^\\s]+-)\\Z\', \' \', text)  # single\n            text = re.sub(r\'\\s+([^\\s]+-)\\s+\', \' \', text)  # middle\n\n            text = re.sub(r\'\\A(-[^\\s]+)\\s+\', \' \', text)  # start\n            text = re.sub(r\'\\s+(-[^\\s]+)\\Z\', \' \', text)  # end\n            text = re.sub(r\'\\A(-[^\\s]+)\\Z\', \' \', text)  # single\n            text = re.sub(r\'\\s+(-[^\\s]+)\\s+\', \' \', text)  # middle\n\n            # Remove conseccutive spaces\n            text = re.sub(r\'[\\s]+\', \' \', text)\n\n            # Remove the first and last spaces\n            if len(text) > 0 and text[0] == \' \':\n                text = text[1:]\n            if len(text) > 0 and text[-1] == \' \':\n                text = text[:-1]\n\n            if len(text) > 0:\n                print(utt_id + \' \' + text)\n            pbar.update(1)\n\n\nif __name__ == \'__main__\':\n    main()\n'"
examples/tedlium/s5_r2/local/join_suffix.py,0,"b'#!/usr/bin/env python\n#\n# Copyright  2014  Nickolay V. Shmyrev\n#            2016  Johns Hopkins University (author: Daniel Povey)\n# Apache 2.0\n\n\nfrom __future__ import print_function\nimport sys\nfrom codecs import open\n\n# This script joins together pairs of split-up words like ""you \'re"" -> ""you\'re"".\n# The TEDLIUM transcripts are normalized in a way that\'s not traditional for\n# speech recognition.\n\nfor line in sys.stdin:\n    items = line.split()\n    new_items = []\n    i = 1\n    while i < len(items):\n        if i < len(items) - 1 and items[i+1][0] == \'\\\'\':\n            new_items.append(items[i] + items[i+1])\n            i = i + 1\n        else:\n            new_items.append(items[i])\n        i = i + 1\n    print(items[0] + \' \' + \' \'.join(new_items))\n'"
examples/tedlium/s5_r3/local/join_suffix.py,0,"b'#!/usr/bin/env python\n#\n# Copyright  2014  Nickolay V. Shmyrev\n#            2016  Johns Hopkins University (author: Daniel Povey)\n# Apache 2.0\n\n\nfrom __future__ import print_function\nimport sys\nfrom codecs import open\n\n# This script joins together pairs of split-up words like ""you \'re"" -> ""you\'re"".\n# The TEDLIUM transcripts are normalized in a way that\'s not traditional for\n# speech recognition.\n\nfor line in sys.stdin:\n    items = line.split()\n    new_items = []\n    i = 1\n    while i < len(items):\n        if i < len(items) - 1 and items[i+1][0] == \'\\\'\':\n            new_items.append(items[i] + items[i+1])\n            i = i + 1\n        else:\n            new_items.append(items[i])\n        i = i + 1\n    print(items[0] + \' \' + \' \'.join(new_items))\n'"
neural_sp/models/seq2seq/decoders/__init__.py,0,b''
neural_sp/models/seq2seq/decoders/beam_search.py,3,"b'#! /usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n# Copyright 2020 Kyoto University (Hirofumi Inaguma)\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n\n""""""Utility funcitons for beam search decoding.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\n# import logging\n# import math\n# import numpy as np\n# import os\n# import random\n# import shutil\nimport torch\n# import torch.nn as nn\n\nfrom neural_sp.models.torch_utils import tensor2np\n\n\nclass BeamSearch(object):\n    def __init__(self, beam_width, eos, ctc_weight, device_id, beam_width_bwd=0):\n\n        super(BeamSearch, self).__init__()\n\n        self.beam_width = beam_width\n        self.beam_width_bwd = beam_width_bwd\n        self.eos = eos\n        self.device_id = device_id\n\n        self.ctc_weight = ctc_weight\n        # self.lm_weight = lm_weight\n\n    def remove_complete_hyp(self, hyps_sorted, end_hyps, prune=True, backward=False):\n        new_hyps = []\n        is_finish = False\n        for hyp in hyps_sorted:\n            if not backward and len(hyp[\'hyp\']) > 1 and hyp[\'hyp\'][-1] == self.eos:\n                end_hyps += [hyp]\n            elif backward and len(hyp[\'hyp_bwd\']) > 1 and hyp[\'hyp_bwd\'][-1] == self.eos:\n                end_hyps += [hyp]\n            else:\n                new_hyps += [hyp]\n        if len(end_hyps) >= self.beam_width + self.beam_width_bwd:\n            if prune:\n                end_hyps = end_hyps[:self.beam_width + self.beam_width_bwd]\n            is_finish = True\n        return new_hyps, end_hyps, is_finish\n\n    def add_ctc_score(self, hyp, topk_ids, ctc_state, total_scores_topk,\n                      ctc_prefix_scorer, new_chunk=False, backward=False):\n        beam_width = self.beam_width_bwd if backward else self.beam_width\n        if ctc_prefix_scorer is None:\n            return None, topk_ids.new_zeros(beam_width), total_scores_topk\n\n        ctc_scores, new_ctc_states = ctc_prefix_scorer(hyp, tensor2np(topk_ids[0]), ctc_state,\n                                                       new_chunk=new_chunk)\n        total_scores_ctc = torch.from_numpy(ctc_scores)\n        if self.device_id >= 0:\n            total_scores_ctc = total_scores_ctc.cuda(self.device_id)\n        total_scores_topk += total_scores_ctc * self.ctc_weight\n        # Sort again\n        total_scores_topk, joint_ids_topk = torch.topk(\n            total_scores_topk, k=beam_width, dim=1, largest=True, sorted=True)\n        topk_ids = topk_ids[:, joint_ids_topk[0]]\n        new_ctc_states = new_ctc_states[joint_ids_topk[0].cpu().numpy()]\n        return new_ctc_states, total_scores_ctc, total_scores_topk\n\n    def add_lm_score(self):\n        raise NotImplementedError\n'"
neural_sp/models/seq2seq/decoders/build.py,0,"b'#! /usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n# Copyright 2019 Kyoto University (Hirofumi Inaguma)\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n\n""""""Select an decoder network.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\n\ndef build_decoder(args, special_symbols, enc_n_units, vocab,\n                  ctc_weight, ctc_fc_list, global_weight, external_lm=None):\n    if args.dec_type in [\'transformer\', \'transformer_xl\']:\n        from neural_sp.models.seq2seq.decoders.transformer import TransformerDecoder\n        decoder = TransformerDecoder(\n            special_symbols=special_symbols,\n            enc_n_units=enc_n_units,\n            attn_type=args.transformer_attn_type,\n            n_heads=args.transformer_n_heads,\n            n_layers=args.dec_n_layers,\n            d_model=args.transformer_d_model,\n            d_ff=args.transformer_d_ff,\n            d_ff_bottleneck_dim=getattr(args, \'transformer_d_ff_bottleneck_dim\', 0),\n            layer_norm_eps=args.transformer_layer_norm_eps,\n            ffn_activation=args.transformer_ffn_activation,\n            pe_type=args.transformer_dec_pe_type,\n            vocab=vocab,\n            tie_embedding=args.tie_embedding,\n            dropout=args.dropout_dec,\n            dropout_emb=args.dropout_emb,\n            dropout_att=args.dropout_att,\n            dropout_layer=args.dropout_dec_layer,\n            dropout_head=args.dropout_head,\n            lsm_prob=args.lsm_prob,\n            ctc_weight=ctc_weight,\n            ctc_lsm_prob=args.ctc_lsm_prob,\n            ctc_fc_list=ctc_fc_list,\n            backward=(dir == \'bwd\'),\n            global_weight=global_weight,\n            mtl_per_batch=args.mtl_per_batch,\n            param_init=args.transformer_param_init,\n            memory_transformer=args.dec_type == \'transformer_xl\',\n            mem_len=args.mem_len,\n            mocha_chunk_size=args.mocha_chunk_size,\n            mocha_n_heads_mono=args.mocha_n_heads_mono,\n            mocha_n_heads_chunk=args.mocha_n_heads_chunk,\n            mocha_init_r=args.mocha_init_r,\n            mocha_eps=args.mocha_eps,\n            mocha_std=args.mocha_std,\n            mocha_no_denominator=args.mocha_no_denominator,\n            mocha_1dconv=args.mocha_1dconv,\n            mocha_quantity_loss_weight=args.mocha_quantity_loss_weight,\n            mocha_head_divergence_loss_weight=args.mocha_head_divergence_loss_weight,\n            latency_metric=args.mocha_latency_metric,\n            latency_loss_weight=args.mocha_latency_loss_weight,\n            mocha_first_layer=args.mocha_first_layer,\n            share_chunkwise_attention=getattr(args, \'share_chunkwise_attention\', False),\n            external_lm=external_lm,\n            lm_fusion=args.lm_fusion)\n\n    elif args.dec_type in [\'lstm_transducer\', \'gru_transducer\']:\n        from neural_sp.models.seq2seq.decoders.rnn_transducer import RNNTransducer\n        decoder = RNNTransducer(\n            special_symbols=special_symbols,\n            enc_n_units=enc_n_units,\n            rnn_type=args.dec_type,\n            n_units=args.dec_n_units,\n            n_projs=args.dec_n_projs,\n            n_layers=args.dec_n_layers,\n            bottleneck_dim=args.dec_bottleneck_dim,\n            emb_dim=args.emb_dim,\n            vocab=vocab,\n            dropout=args.dropout_dec,\n            dropout_emb=args.dropout_emb,\n            lsm_prob=args.lsm_prob,\n            ctc_weight=ctc_weight,\n            ctc_lsm_prob=args.ctc_lsm_prob,\n            ctc_fc_list=ctc_fc_list,\n            external_lm=external_lm if args.lm_init else None,\n            global_weight=global_weight,\n            mtl_per_batch=args.mtl_per_batch,\n            param_init=args.param_init)\n\n    else:\n        from neural_sp.models.seq2seq.decoders.las import RNNDecoder\n        decoder = RNNDecoder(\n            special_symbols=special_symbols,\n            enc_n_units=enc_n_units,\n            rnn_type=args.dec_type,\n            n_units=args.dec_n_units,\n            n_projs=args.dec_n_projs,\n            n_layers=args.dec_n_layers,\n            bottleneck_dim=args.dec_bottleneck_dim,\n            emb_dim=args.emb_dim,\n            vocab=vocab,\n            tie_embedding=args.tie_embedding,\n            attn_type=args.attn_type,\n            attn_dim=args.attn_dim,\n            attn_sharpening_factor=args.attn_sharpening_factor,\n            attn_sigmoid_smoothing=args.attn_sigmoid,\n            attn_conv_out_channels=args.attn_conv_n_channels,\n            attn_conv_kernel_size=args.attn_conv_width,\n            attn_n_heads=args.attn_n_heads,\n            dropout=args.dropout_dec,\n            dropout_emb=args.dropout_emb,\n            dropout_att=args.dropout_att,\n            lsm_prob=args.lsm_prob,\n            ss_prob=args.ss_prob,\n            ss_type=args.ss_type,\n            ctc_weight=ctc_weight,\n            ctc_lsm_prob=args.ctc_lsm_prob,\n            ctc_fc_list=ctc_fc_list,\n            mbr_training=args.mbr_training,\n            mbr_ce_weight=args.mbr_ce_weight,\n            external_lm=external_lm,\n            lm_fusion=args.lm_fusion,\n            lm_init=args.lm_init,\n            backward=(dir == \'bwd\'),\n            global_weight=global_weight,\n            mtl_per_batch=args.mtl_per_batch,\n            param_init=args.param_init,\n            mocha_chunk_size=args.mocha_chunk_size,\n            mocha_n_heads_mono=args.mocha_n_heads_mono,\n            mocha_init_r=args.mocha_init_r,\n            mocha_eps=args.mocha_eps,\n            mocha_std=args.mocha_std,\n            mocha_no_denominator=args.mocha_no_denominator,\n            mocha_1dconv=args.mocha_1dconv,\n            mocha_quantity_loss_weight=args.mocha_quantity_loss_weight,\n            latency_metric=args.mocha_latency_metric,\n            latency_loss_weight=args.mocha_latency_loss_weight,\n            gmm_attn_n_mixtures=args.gmm_attn_n_mixtures,\n            replace_sos=args.replace_sos,\n            distillation_weight=args.distillation_weight,\n            discourse_aware=args.discourse_aware)\n\n    return decoder\n'"
neural_sp/models/seq2seq/decoders/ctc.py,28,"b'#! /usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n# Copyright 2019 Kyoto University (Hirofumi Inaguma)\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n\n""""""CTC decoder.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom collections import OrderedDict\nfrom itertools import groupby\nimport logging\nimport numpy as np\nimport random\nimport torch\nimport torch.nn as nn\n\nfrom neural_sp.models.criterion import kldiv_lsm_ctc\nfrom neural_sp.models.seq2seq.decoders.decoder_base import DecoderBase\nfrom neural_sp.models.torch_utils import make_pad_mask\nfrom neural_sp.models.torch_utils import np2tensor\nfrom neural_sp.models.torch_utils import pad_list\nfrom neural_sp.models.torch_utils import tensor2np\n\nrandom.seed(1)\n\n# LOG_0 = float(np.finfo(np.float32).min)\nLOG_0 = -1e10\nLOG_1 = 0\n\nlogger = logging.getLogger(__name__)\n\n\nclass CTC(DecoderBase):\n    """"""Connectionist temporal classificaiton (CTC).\n\n    Args:\n        eos (int): index for <eos> (shared with <sos>)\n        blank (int): index for <blank>\n        enc_n_units (int):\n        vocab (int): number of nodes in softmax layer\n        dropout (float): dropout probability for the RNN layer\n        lsm_prob (float): label smoothing probability\n        fc_list (list):\n        param_init (float): parameter initialization method\n        backward (bool): flip the output sequence\n\n    """"""\n\n    def __init__(self,\n                 eos,\n                 blank,\n                 enc_n_units,\n                 vocab,\n                 dropout=0.,\n                 lsm_prob=0.,\n                 fc_list=None,\n                 param_init=0.1,\n                 backward=False):\n\n        super(CTC, self).__init__()\n\n        self.eos = eos\n        self.blank = blank\n        self.vocab = vocab\n        self.lsm_prob = lsm_prob\n        self.bwd = backward\n\n        self.space = -1  # TODO(hirofumi): fix later\n\n        # Fully-connected layers before the softmax\n        if fc_list is not None and len(fc_list) > 0:\n            _fc_list = [int(fc) for fc in fc_list.split(\'_\')]\n            fc_layers = OrderedDict()\n            for i in range(len(_fc_list)):\n                input_dim = enc_n_units if i == 0 else _fc_list[i - 1]\n                fc_layers[\'fc\' + str(i)] = nn.Linear(input_dim, _fc_list[i])\n                fc_layers[\'dropout\' + str(i)] = nn.Dropout(p=dropout)\n            fc_layers[\'fc\' + str(len(_fc_list))] = nn.Linear(_fc_list[-1], vocab)\n            self.output = nn.Sequential(fc_layers)\n        else:\n            self.output = nn.Linear(enc_n_units, vocab)\n\n        import warpctc_pytorch\n        self.warpctc_loss = warpctc_pytorch.CTCLoss(size_average=True)\n\n        self.forced_aligner = CTCForcedAligner()\n\n    def forward(self, eouts, elens, ys, forced_align=False):\n        """"""Compute CTC loss.\n\n        Args:\n            eouts (FloatTensor): `[B, T, dec_n_units]`\n            elens (list): A list of length B\n            ys (list): A list of length B, which contains a list of size `[L]`\n        Returns:\n            loss (FloatTensor): `[B, L, vocab]`\n\n        """"""\n        # Concatenate all elements in ys for warpctc_pytorch\n        ylens = np2tensor(np.fromiter([len(y) for y in ys], dtype=np.int32))\n        ys_ctc = torch.cat([np2tensor(np.fromiter(y[::-1] if self.bwd else y, dtype=np.int32))\n                            for y in ys], dim=0)\n        # NOTE: do not copy to GPUs here\n\n        # Compute CTC loss\n        logits = self.output(eouts)\n        loss = self.warpctc_loss(logits.transpose(1, 0),  # time-major\n                                 ys_ctc, elens.cpu(), ylens)\n        # NOTE: ctc loss has already been normalized by bs\n        # NOTE: index 0 is reserved for blank in warpctc_pytorch\n        if self.device_id >= 0:\n            loss = loss.cuda(self.device_id)\n\n        # Label smoothing for CTC\n        if self.lsm_prob > 0:\n            loss = loss * (1 - self.lsm_prob) + kldiv_lsm_ctc(logits, elens) * self.lsm_prob\n\n        trigger_points = None\n        if forced_align:\n            ys = [np2tensor(np.fromiter(y, dtype=np.int64), self.device_id) for y in ys]\n            ys_in_pad = pad_list(ys, 0)  # pad by zero\n            trigger_points = self.forced_aligner.align(logits.clone(), elens, ys_in_pad, ylens)\n\n        return loss, trigger_points\n\n    def trigger_points(self, eouts, elens):\n        """"""Extract trigger points.\n\n        Args:\n            eouts (FloatTensor): `[B, T, enc_n_units]`\n            elens (IntTensor): `[B]`\n        Returns:\n            hyps (IntTensor): `[B, L]`\n\n        """"""\n        bs, xmax, _ = eouts.size()\n        log_probs = torch.log_softmax(self.output(eouts), dim=-1)\n        best_paths = log_probs.argmax(-1)  # `[B, L]`\n\n        hyps = []\n        for b in range(bs):\n            indices = [best_paths[b, t].item() for t in range(elens[b])]\n\n            # Step 1. Collapse repeated labels\n            collapsed_indices = [x[0] for x in groupby(indices)]\n\n            # Step 2. Remove all blank labels\n            best_hyp = [x for x in filter(lambda x: x != self.blank, collapsed_indices)]\n            hyps.append(best_hyp)\n\n        ymax = max([len(h) for h in hyps])\n\n        # pick up trigger points\n        trigger_points = log_probs.new_zeros((bs, ymax + 1), dtype=torch.int32)  # +1 for <eos>\n        for b in range(bs):\n            n_triggers = 0\n            for t in range(elens[b]):\n                token_idx = best_paths[b, t]\n\n                if token_idx == self.blank:\n                    continue\n                if not (t == 0 or token_idx != best_paths[b, t - 1]):\n                    continue\n\n                # NOTE: select the most left trigger points\n                trigger_points[b, n_triggers] = t\n                n_triggers += 1\n\n        return trigger_points\n\n    def greedy(self, eouts, elens):\n        """"""Greedy decoding.\n\n        Args:\n            eouts (FloatTensor): `[B, T, enc_n_units]`\n            elens (np.ndarray): `[B]`\n        Returns:\n            hyps (np.ndarray): Best path hypothesis. `[B, L]`\n\n        """"""\n        log_probs = torch.log_softmax(self.output(eouts), dim=-1)\n        best_paths = log_probs.argmax(-1)  # `[B, L]`\n\n        hyps = []\n        for b in range(eouts.size(0)):\n            indices = [best_paths[b, t].item() for t in range(elens[b])]\n\n            # Step 1. Collapse repeated labels\n            collapsed_indices = [x[0] for x in groupby(indices)]\n\n            # Step 2. Remove all blank labels\n            best_hyp = [x for x in filter(lambda x: x != self.blank, collapsed_indices)]\n            hyps.append(np.array(best_hyp))\n\n        return np.array(hyps)\n\n    def beam_search(self, eouts, elens, params, idx2token,\n                    lm=None, lm_second=None, lm_second_rev=None,\n                    nbest=1, refs_id=None, utt_ids=None, speakers=None):\n        """"""Beam search decoding.\n\n        Args:\n            eouts (FloatTensor): `[B, T, enc_n_units]`\n            elens (list): A list of length B\n            params (dict):\n                recog_beam_width (int): size of beam\n                recog_length_penalty (float): length penalty\n                recog_lm_weight (float): weight of first path LM score\n                recog_lm_second_weight (float): weight of second path LM score\n                recog_lm_bwd_weight (float): weight of second path backward LM score\n            idx2token (): converter from index to token\n            lm: firsh path LM\n            lm_second: second path LM\n            lm_second_rev: secoding path backward LM\n            nbest (int):\n            refs_id (list): reference list\n            utt_ids (list): utterance id list\n            speakers (list): speaker list\n        Returns:\n            best_hyps (list): Best path hypothesis. `[B, L]`\n\n        """"""\n        bs = eouts.size(0)\n\n        beam_width = params[\'recog_beam_width\']\n        lp_weight = params[\'recog_length_penalty\']\n        lm_weight = params[\'recog_lm_weight\']\n        lm_weight_second = params[\'recog_lm_second_weight\']\n        lm_weight_second_bwd = params[\'recog_lm_bwd_weight\']\n\n        if lm is not None:\n            assert lm_weight > 0\n            lm.eval()\n        if lm_second is not None:\n            assert lm_weight_second > 0\n            lm_second.eval()\n\n        best_hyps = []\n        log_probs = torch.log_softmax(self.output(eouts), dim=-1)\n        for b in range(bs):\n            # Elements in the beam are (prefix, (p_b, p_no_blank))\n            # Initialize the beam with the empty sequence, a probability of\n            # 1 for ending in blank and zero for ending in non-blank (in log space).\n            beam = [{\'hyp\': [self.eos],  # <eos> is used for LM\n                     \'p_b\': LOG_1,\n                     \'p_nb\': LOG_0,\n                     \'score_lm\': LOG_1,\n                     \'lmstate\': None}]\n\n            for t in range(elens[b]):\n                new_beam = []\n\n                # Pick up the top-k scores\n                log_probs_topk, topk_ids = torch.topk(\n                    log_probs[b:b + 1, t], k=min(beam_width, self.vocab), dim=-1, largest=True, sorted=True)\n\n                for i_beam in range(len(beam)):\n                    hyp = beam[i_beam][\'hyp\'][:]\n                    p_b = beam[i_beam][\'p_b\']\n                    p_nb = beam[i_beam][\'p_nb\']\n                    score_lm = beam[i_beam][\'score_lm\']\n\n                    # case 1. hyp is not extended\n                    new_p_b = np.logaddexp(p_b + log_probs[b, t, self.blank].item(),\n                                           p_nb + log_probs[b, t, self.blank].item())\n                    if len(hyp) > 1:\n                        new_p_nb = p_nb + log_probs[b, t, hyp[-1]].item()\n                    else:\n                        new_p_nb = LOG_0\n                    score_ctc = np.logaddexp(new_p_b, new_p_nb)\n                    score_lp = len(hyp[1:]) * lp_weight\n                    new_beam.append({\'hyp\': hyp,\n                                     \'score\': score_ctc + score_lm + score_lp,\n                                     \'p_b\': new_p_b,\n                                     \'p_nb\': new_p_nb,\n                                     \'score_ctc\': score_ctc,\n                                     \'score_lm\': score_lm,\n                                     \'score_lp\': score_lp,\n                                     \'lmstate\': beam[i_beam][\'lmstate\']})\n\n                    # Update LM states for shallow fusion\n                    if lm_weight > 0 and lm is not None:\n                        _, lmstate, lm_log_probs = lm.predict(\n                            eouts.new_zeros(1, 1).fill_(hyp[-1]), beam[i_beam][\'lmstate\'])\n                    else:\n                        lmstate = None\n\n                    # case 2. hyp is extended\n                    new_p_b = LOG_0\n                    for c in tensor2np(topk_ids)[0]:\n                        p_t = log_probs[b, t, c].item()\n\n                        if c == self.blank:\n                            continue\n\n                        c_prev = hyp[-1] if len(hyp) > 1 else None\n                        if c == c_prev:\n                            new_p_nb = p_b + p_t\n                            # TODO(hirofumi): apply character LM here\n                        else:\n                            new_p_nb = np.logaddexp(p_b + p_t, p_nb + p_t)\n                            # TODO(hirofumi): apply character LM here\n                            if c == self.space:\n                                pass\n                                # TODO(hirofumi): apply word LM here\n\n                        score_ctc = np.logaddexp(new_p_b, new_p_nb)\n                        score_lp = (len(hyp[1:]) + 1) * lp_weight\n                        if lm_weight > 0 and lm is not None:\n                            local_score_lm = lm_log_probs[0, 0, c].item() * lm_weight\n                            score_lm += local_score_lm\n                        new_beam.append({\'hyp\': hyp + [c],\n                                         \'score\': score_ctc + score_lm + score_lp,\n                                         \'p_b\': new_p_b,\n                                         \'p_nb\': new_p_nb,\n                                         \'score_ctc\': score_ctc,\n                                         \'score_lm\': score_lm,\n                                         \'score_lp\': score_lp,\n                                         \'lmstate\': lmstate})\n\n                # Pruning\n                beam = sorted(new_beam, key=lambda x: x[\'score\'], reverse=True)[:beam_width]\n\n            # Rescoing lattice\n            if lm_second is not None:\n                new_beam = []\n                for i_beam in range(len(beam)):\n                    ys = [np2tensor(np.fromiter(beam[i_beam][\'hyp\'], dtype=np.int64), self.device_id)]\n                    ys_pad = pad_list(ys, lm_second.pad)\n                    _, _, lm_log_probs = lm_second.predict(ys_pad, None)\n                    score_ctc = np.logaddexp(beam[i_beam][\'p_b\'], beam[i_beam][\'p_nb\'])\n                    score_lm = lm_log_probs.sum() * lm_weight_second\n                    score_lp = len(beam[i_beam][\'hyp\'][1:]) * lp_weight\n                    new_beam.append({\'hyp\': beam[i_beam][\'hyp\'],\n                                     \'score\': score_ctc + score_lm + score_lp,\n                                     \'score_ctc\': score_ctc,\n                                     \'score_lp\': score_lp,\n                                     \'score_lm\': score_lm})\n                beam = sorted(new_beam, key=lambda x: x[\'score\'], reverse=True)\n\n            best_hyps.append(np.array(beam[0][\'hyp\'][1:]))\n\n            if utt_ids is not None:\n                logger.info(\'Utt-id: %s\' % utt_ids[b])\n            if refs_id is not None and self.vocab == idx2token.vocab:\n                logger.info(\'Ref: %s\' % idx2token(refs_id[b]))\n            logger.info(\'Hyp: %s\' % idx2token(beam[0][\'hyp\'][1:]))\n            logger.info(\'log prob (hyp): %.7f\' % beam[0][\'score\'])\n            logger.info(\'log prob (CTC): %.7f\' % beam[0][\'score_ctc\'])\n            logger.info(\'log prob (lp): %.7f\' % beam[0][\'score_lp\'])\n            if lm is not None:\n                logger.info(\'log prob (hyp, lm): %.7f\' % (beam[0][\'score_lm\']))\n\n        return np.array(best_hyps)\n\n\ndef _label_to_path(labels, blank):\n    path = labels.new_zeros(labels.size(0), labels.size(1) * 2 + 1).fill_(blank).long()\n    path[:, 1::2] = labels\n    return path\n\n\ndef _flip_path(path, path_lens):\n    """"""Flips label sequence.\n    This function rotates a label sequence and flips it.\n    ``path[b, t]`` stores a label at time ``t`` in ``b``-th batch.\n    The rotated matrix ``r`` is defined as\n    ``r[b, t] = path[b, t + path_lens[b]]``\n    .. ::\n       a b c d .     . a b c d    d c b a .\n       e f . . .  -> . . . e f -> f e . . .\n       g h i j k     g h i j k    k j i h g\n\n    Args:\n        path (FloatTensor): `[B, 2*L+1]`\n        path_lens (LongTensor): `[B]`\n    Returns:\n        FloatTensor: `[B, 2*L+1]`\n\n    """"""\n    bs = path.size(0)\n    max_path_len = path.size(1)\n    rotate = (torch.arange(max_path_len) + path_lens[:, None]) % max_path_len\n    return torch.flip(path[torch.arange(bs, dtype=torch.int64)[:, None], rotate], dims=[1])\n\n\ndef _flip_label_probability(log_probs, xlens):\n    """"""Flips a label probability matrix.\n    This function rotates a label probability matrix and flips it.\n    ``log_probs[i, b, l]`` stores log probability of label ``l`` at ``i``-th\n    input in ``b``-th batch.\n    The rotated matrix ``r`` is defined as\n    ``r[i, b, l] = log_probs[i + xlens[b], b, l]``\n\n    Args:\n        cum_log_prob (FloatTensor): `[T, B, vocab]`\n        xlens (LongTensor): `[B]`\n    Returns:\n        FloatTensor: `[T, B, vocab]`\n\n    """"""\n    xmax, bs, vocab = log_probs.size()\n    rotate = (torch.arange(xmax, dtype=torch.int64)[:, None] + xlens) % xmax\n    return torch.flip(log_probs[rotate[:, :, None],\n                                torch.arange(bs, dtype=torch.int64)[None, :, None],\n                                torch.arange(vocab, dtype=torch.int64)[None, None, :]], dims=[0])\n\n\ndef _flip_path_probability(cum_log_prob, xlens, path_lens):\n    """"""Flips a path probability matrix.\n    This function returns a path probability matrix and flips it.\n    ``cum_log_prob[i, b, t]`` stores log probability at ``i``-th input and\n    at time ``t`` in a output sequence in ``b``-th batch.\n    The rotated matrix ``r`` is defined as\n    ``r[i, j, k] = cum_log_prob[i + xlens[j], j, k + path_lens[j]]``\n\n    Args:\n        cum_log_prob (FloatTensor): `[T, B, 2*L+1]`\n        xlens (LongTensor): `[B]`\n        path_lens (LongTensor): `[B]`\n    Returns:\n        FloatTensor: `[T, B, 2*L+1]`\n\n    """"""\n    xmax, bs, max_path_len = cum_log_prob.size()\n    rotate_input = ((torch.arange(xmax, dtype=torch.int64)[:, None] + xlens) % xmax)\n    rotate_label = ((torch.arange(max_path_len, dtype=torch.int64) + path_lens[:, None]) % max_path_len)\n    return torch.flip(cum_log_prob[rotate_input[:, :, None],\n                                   torch.arange(bs, dtype=torch.int64)[None, :, None],\n                                   rotate_label], dims=[0, 2])\n\n\nclass CTCForcedAligner(object):\n    def __init__(self, blank=0):\n        self.blank = blank\n        self.log0 = LOG_0\n\n    def _computes_transition(self, prev_log_prob, path, path_lens, cum_log_prob, y, skip_accum=False):\n        bs, max_path_len = path.size()\n        mat = prev_log_prob.new_zeros(3, bs, max_path_len).fill_(self.log0)\n        mat[0, :, :] = prev_log_prob\n        mat[1, :, 1:] = prev_log_prob[:, :-1]\n        mat[2, :, 2:] = prev_log_prob[:, :-2]\n        # disable transition between the same symbols\n        # (including blank-to-blank)\n        same_transition = (path[:, :-2] == path[:, 2:])\n        mat[2, :, 2:][same_transition] = self.log0\n        log_prob = torch.logsumexp(mat, dim=0)\n        outside = torch.arange(max_path_len, dtype=torch.int64) >= path_lens.unsqueeze(1)\n        log_prob[outside] = self.log0\n        if not skip_accum:\n            cum_log_prob += log_prob\n        batch_index = torch.arange(bs, dtype=torch.int64).unsqueeze(1)\n        log_prob += y[batch_index, path]\n        return log_prob\n\n    def align(self, logits, elens, ys, ylens):\n        bs, xmax, vocab = logits.size()\n\n        # zero padding\n        device_id = torch.cuda.device_of(logits).idx\n        mask = make_pad_mask(elens, device_id)\n        mask = mask.unsqueeze(2).repeat([1, 1, vocab])\n        logits = logits.masked_fill_(mask == 0, self.log0)\n        log_probs = torch.log_softmax(logits, dim=-1).transpose(0, 1)  # `[T, B, vocab]`\n\n        path = _label_to_path(ys, self.blank)\n        path_lens = 2 * ylens.long() + 1\n\n        ymax = ys.size(1)\n        max_path_len = path.size(1)\n        assert ys.size() == (bs, ymax), ys.size()\n        assert path.size() == (bs, ymax * 2 + 1)\n\n        alpha = log_probs.new_zeros(bs, max_path_len).fill_(self.log0)\n        alpha[:, 0] = LOG_1\n        beta = alpha.clone()\n        gamma = alpha.clone()\n\n        batch_index = torch.arange(bs, dtype=torch.int64).unsqueeze(1)\n        seq_index = torch.arange(xmax, dtype=torch.int64).unsqueeze(1).unsqueeze(2)\n        log_probs_fwd_bwd = log_probs[seq_index, batch_index, path]\n\n        # forward algorithm\n        for t in range(xmax):\n            alpha = self._computes_transition(alpha, path, path_lens, log_probs_fwd_bwd[t], log_probs[t])\n\n        # backward algorithm\n        r_path = _flip_path(path, path_lens)\n        log_probs_inv = _flip_label_probability(log_probs, elens.long())  # `[T, B, vocab]`\n        log_probs_fwd_bwd = _flip_path_probability(log_probs_fwd_bwd, elens.long(), path_lens)  # `[T, B, 2*L+1]`\n        for t in range(xmax):\n            beta = self._computes_transition(beta, r_path, path_lens, log_probs_fwd_bwd[t], log_probs_inv[t])\n\n        # pick up the best CTC path\n        best_lattices = log_probs.new_zeros((bs, xmax), dtype=torch.int64)\n\n        # forward algorithm\n        log_probs_fwd_bwd = _flip_path_probability(log_probs_fwd_bwd, elens.long(), path_lens)\n        for t in range(xmax):\n            gamma = self._computes_transition(gamma, path, path_lens, log_probs_fwd_bwd[t], log_probs[t],\n                                              skip_accum=True)\n\n            # select paths where gamma is valid\n            log_probs_fwd_bwd[t] = log_probs_fwd_bwd[t].masked_fill_(gamma == self.log0, self.log0)\n\n            # pick up the best lattice\n            offsets = log_probs_fwd_bwd[t].argmax(1)\n            for b in range(bs):\n                if t <= elens[b] - 1:\n                    token_idx = path[b, offsets[b]]\n                    best_lattices[b, t] = token_idx\n\n            # remove the rest of paths\n            gamma = log_probs.new_zeros(bs, max_path_len).fill_(self.log0)\n            for b in range(bs):\n                gamma[b, offsets[b]] = LOG_1\n\n        # pick up trigger points\n        trigger_lattices = torch.zeros((bs, xmax), dtype=torch.int64)\n        trigger_points = log_probs.new_zeros((bs, ymax + 1), dtype=torch.int32)  # +1 for <eos>\n        for b in range(bs):\n            n_triggers = 0\n            trigger_points[b, ylens[b]] = elens[b] - 1  # for <eos>\n            for t in range(elens[b]):\n                token_idx = best_lattices[b, t]\n                if token_idx == self.blank:\n                    continue\n                if not (t == 0 or token_idx != best_lattices[b, t - 1]):\n                    continue\n\n                # NOTE: select the most left trigger points\n                trigger_lattices[b, t] = token_idx\n                trigger_points[b, n_triggers] = t\n                n_triggers += 1\n\n        # print(trigger_points[0])\n        # print(trigger_lattices[0])\n        # print(ys[0])\n\n        assert ylens.sum() == (trigger_lattices != 0).sum()\n        return trigger_points\n\n\nclass CTCPrefixScore(object):\n    """"""Compute CTC label sequence scores.\n\n    which is based on Algorithm 2 in WATANABE et al.\n    ""HYBRID CTC/ATTENTION ARCHITECTURE FOR END-TO-END SPEECH RECOGNITION,""\n    but extended to efficiently compute the probablities of multiple labels\n    simultaneously\n\n    [Reference]:\n        https://github.com/espnet/espnet\n    """"""\n\n    def __init__(self, log_probs, blank, eos, truncate=False):\n        """"""\n        Args:\n            log_probs (np.ndarray):\n            blank (int): index of <blank>\n            eos (int): index of <eos>\n            truncate (bool): restart prefix search from the previous CTC spike\n\n        """"""\n        self.blank = blank\n        self.eos = eos\n        self.xlen_prev = 0\n        self.xlen = len(log_probs)\n        self.log_probs = log_probs\n        self.log0 = LOG_0\n\n        self.truncate = truncate\n        self.offset = 0\n\n    def initial_state(self):\n        """"""Obtain an initial CTC state\n\n        Returns:\n            ctc_states (np.ndarray): `[T, 2]`\n\n        """"""\n        # initial CTC state is made of a frame x 2 tensor that corresponds to\n        # r_t^n(<sos>) and r_t^b(<sos>), where 0 and 1 of axis=1 represent\n        # superscripts n and b (non-blank and blank), respectively.\n        r = np.full((self.xlen, 2), self.log0, dtype=np.float32)\n        r[0, 1] = self.log_probs[0, self.blank]\n        for i in range(1, self.xlen):\n            r[i, 1] = r[i - 1, 1] + self.log_probs[i, self.blank]\n        return r\n\n    def register_new_chunk(self, log_probs_chunk):\n        self.xlen_prev = self.xlen\n        self.log_probs = np.concatenate([self.log_probs, log_probs_chunk], axis=0)\n        self.xlen = len(self.log_probs)\n\n    def __call__(self, hyp, cs, r_prev, new_chunk=False):\n        """"""Compute CTC prefix scores for next labels.\n\n        Args:\n            hyp (list): prefix label sequence\n            cs (np.ndarray): array of next labels. A tensor of size `[beam_width]`\n            r_prev (np.ndarray): previous CTC state `[T, 2]`\n        Returns:\n            ctc_scores (np.ndarray): `[beam_width]`\n            ctc_states (np.ndarray): `[beam_width, T, 2]`\n\n        """"""\n        beam_width = len(cs)\n\n        # initialize CTC states\n        ylen = len(hyp) - 1  # ignore sos\n        # new CTC states are prepared as a frame x (n or b) x n_labels tensor\n        # that corresponds to r_t^n(h) and r_t^b(h).\n        r = np.ndarray((self.xlen, 2, beam_width), dtype=np.float32)\n        xs = self.log_probs[:, cs]\n        if ylen == 0:\n            r[0, 0] = xs[0]\n            r[0, 1] = self.log0\n        else:\n            r[ylen - 1] = self.log0\n\n        # Initialize CTC state for the new chunk\n        if new_chunk and self.xlen_prev > 0:\n            xlen_prev = r_prev.shape[0]\n            r_new = np.full((self.xlen - xlen_prev, 2), self.log0, dtype=np.float32)\n            r_new[0, 1] = r_prev[xlen_prev - 1, 1] + self.log_probs[xlen_prev, self.blank]\n            for i in range(xlen_prev + 1, self.xlen):\n                r_new[i - xlen_prev, 1] = r_new[i - xlen_prev - 1, 1] + self.log_probs[i, self.blank]\n            r_prev = np.concatenate([r_prev, r_new], axis=0)\n\n        # prepare forward probabilities for the last label\n        r_sum = np.logaddexp(r_prev[:, 0], r_prev[:, 1])  # log(r_t^n(g) + r_t^b(g))\n        last = hyp[-1]\n        if ylen > 0 and last in cs:\n            log_phi = np.ndarray((self.xlen, beam_width), dtype=np.float32)\n            for k in range(beam_width):\n                log_phi[:, k] = r_sum if cs[k] != last else r_prev[:, 1]\n        else:\n            log_phi = r_sum  # `[T]`\n\n        # compute forward probabilities log(r_t^n(h)), log(r_t^b(h)),\n        # and log prefix probabilites log(psi)\n        start = max(ylen, 1)\n        log_psi = r[start - 1, 0]\n        for t in range(start, self.xlen):\n            # non-blank\n            r[t, 0] = np.logaddexp(r[t - 1, 0], log_phi[t - 1]) + xs[t]\n            # blank\n            r[t, 1] = np.logaddexp(r[t - 1, 0], r[t - 1, 1]) + self.log_probs[t, self.blank]\n            log_psi = np.logaddexp(log_psi, log_phi[t - 1] + xs[t])\n\n        # get P(...eos|X) that ends with the prefix itself\n        eos_pos = np.where(cs == self.eos)[0]\n        if len(eos_pos) > 0:\n            log_psi[eos_pos] = r_sum[-1]  # log(r_T^n(g) + r_T^b(g))\n\n        # return the log prefix probability and CTC states, where the label axis\n        # of the CTC states is moved to the first axis to slice it easily\n        return log_psi, np.rollaxis(r, 2)\n'"
neural_sp/models/seq2seq/decoders/decoder_base.py,5,"b'#! /usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n# Copyright 2019 Kyoto University (Hirofumi Inaguma)\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n\n""""""Base class for decoders.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport logging\nimport numpy as np\nimport torch\n\nfrom neural_sp.models.base import ModelBase\nfrom neural_sp.models.torch_utils import np2tensor\nfrom neural_sp.models.torch_utils import pad_list\n\nlogger = logging.getLogger(__name__)\n\n\nclass DecoderBase(ModelBase):\n    """"""Base class for decoders.""""""\n\n    def __init__(self):\n\n        super(ModelBase, self).__init__()\n\n        logger.info(\'Overriding DecoderBase class.\')\n\n    @property\n    def device_id(self):\n        return torch.cuda.device_of(next(self.parameters()).data).idx\n\n    def reset_parameters(self, param_init):\n        raise NotImplementedError\n\n    def reset_session(self):\n        self.new_session = True\n\n    def greedy(self, eouts, elens, max_len_ratio):\n        raise NotImplementedError\n\n    def beam_search(self, eouts, elens, params, idx2token):\n        raise NotImplementedError\n\n    def _plot_attention(self):\n        raise NotImplementedError\n\n    def decode_ctc(self, eouts, elens, params, idx2token,\n                   lm=None, lm_2nd=None, lm_2nd_rev=None,\n                   nbest=1, refs_id=None, utt_ids=None, speakers=None):\n        """"""Decoding with CTC scores in the inference stage.\n\n        Args:\n            eouts (FloatTensor): `[B, T, enc_units]`\n            elens (IntTensor): `[B]`\n            params (dict):\n                recog_beam_width (int): size of beam\n                recog_length_penalty (float): length penalty\n                recog_lm_weight (float): weight of first path LM score\n                recog_lm_second_weight (float): weight of second path LM score\n                recog_lm_rev_weight (float): weight of second path backward LM score\n            lm: firsh path LM\n            lm_2nd: second path LM\n            lm_2nd_rev: secoding path backward LM\n        Returns:\n            probs (FloatTensor): `[B, T, vocab]`\n            topk_ids (LongTensor): `[B, T, topk]`\n            best_hyps (list): A list of length `[B]`, which contains arrays of size `[L]`\n\n        """"""\n        if params[\'recog_beam_width\'] == 1:\n            best_hyps = self.ctc.greedy(eouts, elens)\n        else:\n            best_hyps = self.ctc.beam_search(eouts, elens, params, idx2token,\n                                             lm, lm_2nd, lm_2nd_rev,\n                                             nbest, refs_id, utt_ids, speakers)\n        return best_hyps\n\n    def ctc_probs(self, eouts, temperature=1.):\n        """"""Return CTC probabilities.\n\n        Args:\n            eouts (FloatTensor): `[B, T, enc_units]`\n        Returns:\n            probs (FloatTensor): `[B, T, vocab]`\n\n        """"""\n        return torch.softmax(self.ctc.output(eouts) / temperature, dim=-1)\n\n    def ctc_log_probs(self, eouts, temperature=1.):\n        """"""Return log-scale CTC probabilities.\n\n        Args:\n            eouts (FloatTensor): `[B, T, enc_units]`\n        Returns:\n            log_probs (FloatTensor): `[B, T, vocab]`\n\n        """"""\n        return torch.log_softmax(self.ctc.output(eouts) / temperature, dim=-1)\n\n    def ctc_probs_topk(self, eouts, temperature=1., topk=None):\n        """"""Get CTC top-K probabilities.\n\n        Args:\n            eouts (FloatTensor): `[B, T, enc_units]`\n            temperature (float): softmax temperature\n            topk (int):\n        Returns:\n            probs (FloatTensor): `[B, T, vocab]`\n            topk_ids (LongTensor): `[B, T, topk]`\n\n        """"""\n        probs = torch.softmax(self.ctc.output(eouts) / temperature, dim=-1)\n        if topk is None:\n            topk = probs.size(-1)\n        _, topk_ids = torch.topk(probs, k=topk, dim=-1, largest=True, sorted=True)\n        return probs, topk_ids\n\n    def lm_rescoring(self, hyps, lm, lm_weight, reverse=False, tag=\'\'):\n        for i in range(len(hyps)):\n            ys = hyps[i][\'hyp\']  # include <sos>\n            if reverse:\n                ys = ys[::-1]\n\n            ys = [np2tensor(np.fromiter(ys, dtype=np.int64), self.device_id)]\n            ys_in = pad_list([y[:-1] for y in ys], -1)  # `[1, L-1]`\n            ys_out = pad_list([y[1:] for y in ys], -1)  # `[1, L-1]`\n\n            lmout, lmstate, scores_lm = lm.predict(ys_in, None)\n            score_lm = sum([scores_lm[0, t, ys_out[0, t]] for t in range(ys_out.size(1))])\n            score_lm /= ys_out.size(1)\n\n            hyps[i][\'score\'] += score_lm * lm_weight\n            hyps[i][\'score_lm_\' + tag] = score_lm\n'"
neural_sp/models/seq2seq/decoders/fwd_bwd_attention.py,0,"b'#! /usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n# Copyright 2018 Kyoto University (Hirofumi Inaguma)\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n\n""""""Forward-backward attention decoding.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport logging\n\nlogger = logging.getLogger(__name__)\n\n\ndef fwd_bwd_attention(nbest_hyps_fwd, aws_fwd, scores_fwd,\n                      nbest_hyps_bwd, aws_bwd, scores_bwd,\n                      eos, gnmt_decoding, lp_weight, idx2token, refs_id, flip=False):\n    """"""Decoding with the forward and backward attention-based decoders.\n\n    Args:\n        nbest_hyps_fwd (list): A list of length `[B]`, which contains list of n hypotheses\n        aws_fwd (list): A list of length `[B]`, which contains arrays of size `[L, T]`\n        scores_fwd (list):\n        nbest_hyps_bwd (list):\n        aws_bwd (list):\n        scores_bwd (list):\n        eos (int):\n        gnmt_decoding (float):\n        lp_weight (float):\n        idx2token (): converter from index to token\n        refs_id ():\n        flip (bool): flip the encoder indices\n    Returns:\n\n    """"""\n    bs = len(nbest_hyps_fwd)\n    nbest = len(nbest_hyps_fwd[0])\n\n    best_hyps = []\n    for b in range(bs):\n        max_time = len(aws_fwd[b][0])\n\n        merged = []\n        for n in range(nbest):\n            # forward\n            if len(nbest_hyps_fwd[b][n]) > 1:\n                if nbest_hyps_fwd[b][n][-1] == eos:\n                    merged.append({\'hyp\': nbest_hyps_fwd[b][n][:-1],\n                                   \'score\': scores_fwd[b][n][-2]})\n                    # NOTE: remove eos probability\n                else:\n                    merged.append({\'hyp\': nbest_hyps_fwd[b][n],\n                                   \'score\': scores_fwd[b][n][-1]})\n            else:\n                # <eos> only\n                logger.info(nbest_hyps_fwd[b][n])\n\n            # backward\n            if len(nbest_hyps_bwd[b][n]) > 1:\n                if nbest_hyps_bwd[b][n][0] == eos:\n                    merged.append({\'hyp\': nbest_hyps_bwd[b][n][1:],\n                                   \'score\': scores_bwd[b][n][1]})\n                    # NOTE: remove eos probability\n                else:\n                    merged.append({\'hyp\': nbest_hyps_bwd[b][n],\n                                   \'score\': scores_bwd[b][n][0]})\n            else:\n                # <eos> only\n                logger.info(nbest_hyps_bwd[b][n])\n\n        for n_f in range(nbest):\n            for n_b in range(nbest):\n                for i_f in range(len(aws_fwd[b][n_f]) - 1):\n                    for i_b in range(len(aws_bwd[b][n_b]) - 1):\n                        if flip:\n                            # the encoder is not shared between forward and backward decoders\n                            t_prev = max_time - aws_bwd[b][n_b][i_b + 1].argmax(-2)\n                            t_curr = aws_fwd[b][n_f][i_f].argmax(-2)\n                            t_next = max_time - aws_bwd[b][n_b][i_b - 1].argmax(-2)\n                        else:\n                            t_prev = aws_bwd[b][n_b][i_b + 1].argmax(-2)\n                            t_curr = aws_fwd[b][n_f][i_f].argmax(-2)\n                            t_next = aws_bwd[b][n_b][i_b - 1].argmax(-2)\n\n                        # the same token at the same time\n                        if t_curr >= t_prev and t_curr <= t_next and nbest_hyps_fwd[b][n_f][i_f] == nbest_hyps_bwd[b][n_b][i_b]:\n                            new_hyp = nbest_hyps_fwd[b][n_f][:i_f + 1].tolist() + \\\n                                nbest_hyps_bwd[b][n_b][i_b + 1:].tolist()\n                            score_curr_fwd = scores_fwd[b][n_f][i_f] - scores_fwd[b][n_f][i_f - 1]\n                            score_curr_bwd = scores_bwd[b][n_b][i_b] - scores_bwd[b][n_b][i_b + 1]\n                            score_curr = max(score_curr_fwd, score_curr_bwd)\n                            new_score = scores_fwd[b][n_f][i_f - 1] + scores_bwd[b][n_b][i_b + 1] + score_curr\n                            merged.append({\'hyp\': new_hyp, \'score\': new_score})\n\n                            logger.info(\'time matching\')\n                            if refs_id is not None:\n                                logger.info(\'Ref: %s\' % idx2token(refs_id[b]))\n                            logger.info(\'hyp (fwd): %s\' % idx2token(nbest_hyps_fwd[b][n_f]))\n                            logger.info(\'hyp (bwd): %s\' % idx2token(nbest_hyps_bwd[b][n_b]))\n                            logger.info(\'hyp (fwd-bwd): %s\' % idx2token(new_hyp))\n                            logger.info(\'log prob (fwd): %.3f\' % scores_fwd[b][n_f][-1])\n                            logger.info(\'log prob (bwd): %.3f\' % scores_bwd[b][n_b][0])\n                            logger.info(\'log prob (fwd-bwd): %.3f\' % new_score)\n\n        merged = sorted(merged, key=lambda x: x[\'score\'], reverse=True)\n        best_hyps.append(merged[0][\'hyp\'])\n\n    return best_hyps\n'"
neural_sp/models/seq2seq/decoders/las.py,73,"b'#! /usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n# Copyright 2018 Kyoto University (Hirofumi Inaguma)\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n\n""""""RNN decoder for Listen Attend and Spell (LAS) model (including CTC loss calculation).""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom distutils.util import strtobool\nimport logging\nimport math\nimport numpy as np\nimport os\nimport random\nimport shutil\nimport torch\nimport torch.nn as nn\n\nfrom neural_sp.evaluators.edit_distance import compute_wer\nfrom neural_sp.models.criterion import cross_entropy_lsm\nfrom neural_sp.models.criterion import distillation\nfrom neural_sp.models.criterion import MBR\n# from neural_sp.models.criterion import minimum_bayes_risk\nfrom neural_sp.models.lm.rnnlm import RNNLM\nfrom neural_sp.models.lm.transformerlm import TransformerLM\nfrom neural_sp.models.lm.transformer_xl import TransformerXL\nfrom neural_sp.models.modules.gmm_attention import GMMAttention\nfrom neural_sp.models.modules.mocha import MoChA\nfrom neural_sp.models.modules.multihead_attention import MultiheadAttentionMechanism\nfrom neural_sp.models.modules.attention import AttentionMechanism\nfrom neural_sp.models.seq2seq.decoders.beam_search import BeamSearch\nfrom neural_sp.models.seq2seq.decoders.ctc import CTC\nfrom neural_sp.models.seq2seq.decoders.ctc import CTCPrefixScore\nfrom neural_sp.models.seq2seq.decoders.decoder_base import DecoderBase\nfrom neural_sp.models.torch_utils import append_sos_eos\nfrom neural_sp.models.torch_utils import compute_accuracy\nfrom neural_sp.models.torch_utils import make_pad_mask\nfrom neural_sp.models.torch_utils import repeat\nfrom neural_sp.models.torch_utils import pad_list\nfrom neural_sp.models.torch_utils import np2tensor\nfrom neural_sp.models.torch_utils import tensor2np\nfrom neural_sp.utils import mkdir_join\n\nimport matplotlib\nmatplotlib.use(\'Agg\')\n\nrandom.seed(1)\n\nlogger = logging.getLogger(__name__)\n\n\nclass RNNDecoder(DecoderBase):\n    """"""RNN decoder.\n\n    Args:\n        special_symbols (dict):\n            eos (int): index for <eos> (shared with <sos>)\n            unk (int): index for <unk>\n            pad (int): index for <pad>\n            blank (int): index for <blank>\n        enc_n_units (int): number of units of the encoder outputs\n        attn_type (str): type of attention mechanism\n        rnn_type (str): lstm/gru\n        n_units (int): number of units in each RNN layer\n        n_projs (int): number of units in each projection layer\n        n_layers (int): number of RNN layers\n        bottleneck_dim (int): dimension of the bottleneck layer before the softmax layer for label generation\n        emb_dim (int): dimension of the embedding in target spaces.\n        vocab (int): number of nodes in softmax layer\n        tie_embedding (bool): tie parameters of the embedding and output layers\n        attn_dim (int): dimension of attention space\n        attn_sharpening_factor (float):\n        attn_sigmoid_smoothing (bool):\n        attn_conv_out_channels (int):\n        attn_conv_kernel_size (int):\n        attn_n_heads (int): number of attention heads\n        dropout (float): dropout probability for the RNN layer\n        dropout_emb (float): dropout probability for the embedding layer\n        dropout_att (float): dropout probability for attention distributions\n        lsm_prob (float): label smoothing probability\n        ss_prob (float): scheduled sampling probability\n        ss_type (str): constant/ramp\n        ctc_weight (float): CTC loss weight\n        ctc_lsm_prob (float): label smoothing probability for CTC\n        ctc_fc_list (list):\n        mbr_training (bool): MBR training\n        mbr_ce_weight (float): CE weight for regularization during MBR training\n        external_lm (RNNLM):\n        lm_fusion (str): type of LM fusion\n        lm_init (bool):\n        backward (bool): decode in the backward order\n        global_weight (float):\n        mtl_per_batch (bool): change mini-batch per task for multi-task training\n        param_init (float):\n        mocha_chunk_size (int): chunk size for MoChA\n        mocha_n_heads_mono (int):\n        mocha_init_r (int):\n        mocha_eps (float):\n        mocha_std (float):\n        mocha_no_denominator (bool):\n        mocha_1dconv (bool): 1dconv for MoChA\n        mocha_quantity_loss_weight (float):\n        latency_metric (str): latency metric\n        latency_loss_weight (float):\n        gmm_attn_n_mixtures (int): number of mixtures for GMM attention\n        replace_sos (bool): replace <sos> with special tokens\n        distillation_weight (float): soft label weight for knowledge distillation\n        discourse_aware (str): state_carry_over\n\n    """"""\n\n    def __init__(self, special_symbols,\n                 enc_n_units, attn_type, rnn_type, n_units, n_projs, n_layers,\n                 bottleneck_dim, emb_dim, vocab, tie_embedding,\n                 attn_dim, attn_sharpening_factor, attn_sigmoid_smoothing,\n                 attn_conv_out_channels, attn_conv_kernel_size, attn_n_heads,\n                 dropout, dropout_emb, dropout_att,\n                 lsm_prob, ss_prob, ss_type,\n                 ctc_weight, ctc_lsm_prob, ctc_fc_list,\n                 mbr_training, mbr_ce_weight,\n                 external_lm, lm_fusion, lm_init,\n                 backward, global_weight, mtl_per_batch, param_init,\n                 mocha_chunk_size, mocha_n_heads_mono,\n                 mocha_init_r, mocha_eps, mocha_std, mocha_no_denominator,\n                 mocha_1dconv, mocha_quantity_loss_weight,\n                 latency_metric, latency_loss_weight,\n                 gmm_attn_n_mixtures, replace_sos, distillation_weight, discourse_aware):\n\n        super(RNNDecoder, self).__init__()\n\n        self.eos = special_symbols[\'eos\']\n        self.unk = special_symbols[\'unk\']\n        self.pad = special_symbols[\'pad\']\n        self.blank = special_symbols[\'blank\']\n        self.vocab = vocab\n        self.attn_type = attn_type\n        self.rnn_type = rnn_type\n        assert rnn_type in [\'lstm\', \'gru\']\n        self.enc_n_units = enc_n_units\n        self.dec_n_units = n_units\n        self.n_projs = n_projs\n        self.n_layers = n_layers\n        self.lsm_prob = lsm_prob\n        self.ss_prob = ss_prob\n        self.ss_type = ss_type\n        if ss_type == \'constant\':\n            self._ss_prob = ss_prob\n        elif ss_type == \'ramp\':\n            self._ss_prob = 0  # for curriculum\n        self.att_weight = global_weight - ctc_weight\n        self.ctc_weight = ctc_weight\n        self.lm_fusion = lm_fusion\n        self.bwd = backward\n        self.mtl_per_batch = mtl_per_batch\n        self.replace_sos = replace_sos\n        self.distillation_weight = distillation_weight\n\n        # for mocha and triggered attention\n        self.quantity_loss_weight = mocha_quantity_loss_weight\n        self._quantity_loss_weight = mocha_quantity_loss_weight  # for curriculum\n        self.latency_metric = latency_metric\n        self.latency_loss_weight = latency_loss_weight\n        self.ctc_trigger = (self.latency_metric in [\'ctc_sync\', \'ctc_dal\'] or attn_type == \'triggered_attention\')\n        if self.ctc_trigger:\n            assert 0 < self.ctc_weight < 1\n\n        # for MBR training\n        self.mbr_ce_weight = mbr_ce_weight\n        self.mbr = MBR.apply if mbr_training else None\n\n        # for contextualization\n        self.discourse_aware = discourse_aware\n        self.dstate_prev = None\n        self.new_session = False\n\n        self.prev_spk = \'\'\n        self.dstates_final = None\n        self.lmstate_final = None\n        self.lmmemory = None\n\n        # for attention plot\n        self.aws_dict = {}\n        self.data_dict = {}\n\n        if ctc_weight > 0:\n            self.ctc = CTC(eos=self.eos,\n                           blank=self.blank,\n                           enc_n_units=enc_n_units,\n                           vocab=vocab,\n                           dropout=dropout,\n                           lsm_prob=ctc_lsm_prob,\n                           fc_list=ctc_fc_list,\n                           param_init=param_init)\n\n        if self.att_weight > 0:\n            # Attention layer\n            qdim = n_units if n_projs == 0 else n_projs\n            if attn_type == \'mocha\':\n                assert attn_n_heads == 1\n                self.score = MoChA(enc_n_units, qdim, attn_dim, enc_n_units,\n                                   atype=\'add\',\n                                   chunk_size=mocha_chunk_size,\n                                   n_heads_mono=mocha_n_heads_mono,\n                                   init_r=mocha_init_r,\n                                   eps=mocha_eps,\n                                   noise_std=mocha_std,\n                                   no_denominator=mocha_no_denominator,\n                                   conv1d=mocha_1dconv,\n                                   sharpening_factor=attn_sharpening_factor,\n                                   decot=latency_metric == \'decot\',\n                                   lookahead=2)\n            elif attn_type == \'gmm\':\n                self.score = GMMAttention(enc_n_units, qdim, attn_dim,\n                                          n_mixtures=gmm_attn_n_mixtures)\n            else:\n                if attn_n_heads > 1:\n                    assert attn_type == \'add\'\n                    self.score = MultiheadAttentionMechanism(\n                        enc_n_units, qdim, attn_dim, enc_n_units,\n                        n_heads=attn_n_heads,\n                        dropout=dropout_att,\n                        atype=\'add\')\n                else:\n                    self.score = AttentionMechanism(\n                        enc_n_units, qdim, attn_dim, attn_type,\n                        sharpening_factor=attn_sharpening_factor,\n                        sigmoid_smoothing=attn_sigmoid_smoothing,\n                        conv_out_channels=attn_conv_out_channels,\n                        conv_kernel_size=attn_conv_kernel_size,\n                        dropout=dropout_att,\n                        lookahead=2)\n\n            # Decoder\n            self.rnn = nn.ModuleList()\n            cell = nn.LSTMCell if rnn_type == \'lstm\' else nn.GRUCell\n            if self.n_projs > 0:\n                self.proj = repeat(nn.Linear(n_units, n_projs), n_layers)\n            self.dropout = nn.Dropout(p=dropout)\n            dec_odim = enc_n_units + emb_dim\n            for l in range(n_layers):\n                self.rnn += [cell(dec_odim, n_units)]\n                dec_odim = n_units\n                if self.n_projs > 0:\n                    dec_odim = n_projs\n\n            # LM fusion\n            if external_lm is not None and lm_fusion:\n                self.linear_dec_feat = nn.Linear(dec_odim + enc_n_units, n_units)\n                if lm_fusion in [\'cold\', \'deep\']:\n                    self.linear_lm_feat = nn.Linear(external_lm.output_dim, n_units)\n                    self.linear_lm_gate = nn.Linear(n_units * 2, n_units)\n                elif lm_fusion == \'cold_prob\':\n                    self.linear_lm_feat = nn.Linear(external_lm.vocab, n_units)\n                    self.linear_lm_gate = nn.Linear(n_units * 2, n_units)\n                else:\n                    raise ValueError(lm_fusion)\n                self.output_bn = nn.Linear(n_units * 2, bottleneck_dim)\n            else:\n                self.output_bn = nn.Linear(dec_odim + enc_n_units, bottleneck_dim)\n\n            self.embed = nn.Embedding(vocab, emb_dim, padding_idx=self.pad)\n            self.dropout_emb = nn.Dropout(p=dropout_emb)\n            assert bottleneck_dim > 0, \'bottleneck_dim must be larger than zero.\'\n            self.output = nn.Linear(bottleneck_dim, vocab)\n            if tie_embedding:\n                if emb_dim != bottleneck_dim:\n                    raise ValueError(\'When using the tied flag, n_units must be equal to emb_dim.\')\n                self.output.weight = self.embed.weight\n\n        self.reset_parameters(param_init)\n\n        # resister the external LM\n        self.lm = external_lm\n\n        # decoder initialization with pre-trained LM\n        if lm_init:\n            assert lm_init.vocab == vocab\n            assert lm_init.n_units == n_units\n            assert lm_init.emb_dim == emb_dim\n            logger.info(\'===== Initialize the decoder with pre-trained RNNLM\')\n            assert lm_init.n_projs == 0  # TODO(hirofumi): fix later\n            assert lm_init.n_units_null_context == enc_n_units\n\n            # RNN\n            for l in range(lm_init.n_layers):\n                for n, p in lm_init.rnn[l].named_parameters():\n                    assert getattr(self.rnn[l], n).size() == p.size()\n                    getattr(self.rnn[l], n).data = p.data\n                    logger.info(\'Overwrite %s\' % n)\n\n            # embedding\n            assert self.embed.weight.size() == lm_init.embed.weight.size()\n            self.embed.weight.data = lm_init.embed.weight.data\n            logger.info(\'Overwrite %s\' % \'embed.weight\')\n\n    @staticmethod\n    def add_args(parser, args):\n        """"""Add arguments.""""""\n        group = parser.add_argument_group(""LAS decoder"")\n        # common (LAS/RNN-T)\n        if not hasattr(args, \'dec_n_units\'):\n            group.add_argument(\'--dec_n_units\', type=int, default=512,\n                               help=\'number of units in each decoder RNN layer\')\n            group.add_argument(\'--dec_n_projs\', type=int, default=0,\n                               help=\'number of units in the projection layer after each decoder RNN layer\')\n            group.add_argument(\'--dec_bottleneck_dim\', type=int, default=1024,\n                               help=\'number of dimensions of the bottleneck layer before the softmax layer\')\n            group.add_argument(\'--emb_dim\', type=int, default=512,\n                               help=\'number of dimensions in the embedding layer\')\n        # attention\n        group.add_argument(\'--attn_type\', type=str, default=\'location\',\n                           choices=[\'no\', \'location\', \'add\', \'dot\',\n                                    \'luong_dot\', \'luong_general\', \'luong_concat\',\n                                    \'mocha\', \'gmm\', \'cif\', \'triggered_attention\'],\n                           help=\'type of attention mechasnism for RNN decoder\')\n        group.add_argument(\'--attn_dim\', type=int, default=128,\n                           help=\'dimension of the attention layer\')\n        group.add_argument(\'--attn_n_heads\', type=int, default=1,\n                           help=\'number of heads in the attention layer\')\n        group.add_argument(\'--attn_sharpening_factor\', type=float, default=1.0,\n                           help=\'sharpening factor\')\n        group.add_argument(\'--attn_conv_n_channels\', type=int, default=10,\n                           help=\'\')\n        group.add_argument(\'--attn_conv_width\', type=int, default=201,\n                           help=\'\')\n        group.add_argument(\'--attn_sigmoid\', type=strtobool, default=False, nargs=\'?\',\n                           help=\'\')\n        group.add_argument(\'--gmm_attn_n_mixtures\', type=int, default=5,\n                           help=\'number of mixtures for GMM attention\')\n        return parser\n\n    def reset_parameters(self, param_init):\n        """"""Initialize parameters with uniform distribution.""""""\n        logger.info(\'===== Initialize %s with uniform distribution =====\' % self.__class__.__name__)\n        for n, p in self.named_parameters():\n            if \'score.monotonic_energy.v.weight_g\' in n or \'score.monotonic_energy.r\' in n:\n                logger.info(\'Skip initialization of %s\' % n)\n                continue\n            if \'score.chunk_energy.v.weight_g\' in n or \'score.chunk_energy.r\' in n:\n                logger.info(\'Skip initialization of %s\' % n)\n                continue\n\n            if p.dim() == 1:\n                if \'linear_lm_gate.fc.bias\' in n:\n                    # Initialize bias in gating with -1 for cold fusion\n                    nn.init.constant_(p, -1.)  # bias\n                    logger.info(\'Initialize %s with %s / %.3f\' % (n, \'constant\', -1.))\n                else:\n                    nn.init.constant_(p, 0.)  # bias\n                    logger.info(\'Initialize %s with %s / %.3f\' % (n, \'constant\', 0.))\n            elif p.dim() in [2, 3, 4]:\n                nn.init.uniform_(p, a=-param_init, b=param_init)\n                logger.info(\'Initialize %s with %s / %.3f\' % (n, \'uniform\', param_init))\n            else:\n                raise ValueError(n)\n\n    def start_scheduled_sampling(self):\n        self._ss_prob = self.ss_prob\n\n    def forward(self, eouts, elens, ys, task=\'all\',\n                teacher_logits=None, recog_params={}, idx2token=None):\n        """"""Forward computation.\n\n        Args:\n            eouts (FloatTensor): `[B, T, enc_n_units]`\n            elens (IntTensor): `[B]`\n            ys (list): length `B`, each of which contains a list of size `[L]`\n            task (str): all/ys*/ys_sub*\n            teacher_logits (FloatTensor): `[B, L, vocab]`\n            recog_params (dict): parameters for MBR training\n            idx2token ():\n        Returns:\n            loss (FloatTensor): `[1]`\n            observation (dict):\n\n        """"""\n        observation = {\'loss\': None, \'loss_att\': None, \'loss_ctc\': None, \'loss_mbr\': None,\n                       \'acc_att\': None, \'ppl_att\': None}\n        loss = eouts.new_zeros(1)\n\n        # CTC loss\n        trigger_points = None\n        if self.ctc_weight > 0 and (task == \'all\' or \'ctc\' in task):\n            forced_align = (self.ctc_trigger and self.training) or self.attn_type == \'triggered_attention\'\n            loss_ctc, trigger_points = self.ctc(eouts, elens, ys, forced_align=forced_align)\n            observation[\'loss_ctc\'] = loss_ctc.item()\n            if self.mtl_per_batch:\n                loss += loss_ctc\n            else:\n                loss += loss_ctc * self.ctc_weight\n\n        # XE loss\n        if self.att_weight > 0 and (task == \'all\' or \'ctc\' not in task) and self.mbr is None:\n            loss_att, acc_att, ppl_att, loss_quantity, loss_latency = self.forward_att(\n                eouts, elens, ys, teacher_logits=teacher_logits,\n                trigger_points=trigger_points)\n            observation[\'loss_att\'] = loss_att.item()\n            observation[\'acc_att\'] = acc_att\n            observation[\'ppl_att\'] = ppl_att\n            if self.attn_type == \'mocha\':\n                if self._quantity_loss_weight > 0:\n                    loss_att += loss_quantity * self._quantity_loss_weight\n                observation[\'loss_quantity\'] = loss_quantity.item()\n            if self.latency_metric:\n                observation[\'loss_latency\'] = loss_latency.item() if self.training else 0\n                if self.latency_metric != \'decot\' and self.latency_loss_weight > 0:\n                    loss_att += loss_latency * self.latency_loss_weight\n            if self.mtl_per_batch:\n                loss += loss_att\n            else:\n                loss += loss_att * self.att_weight\n\n        # MBR loss\n        if self.mbr is not None and (task == \'all\' or \'mbr\' not in task):\n            N_best = recog_params[\'recog_beam_width\']\n            alpha = 1.0\n            assert N_best >= 2\n            loss_mbr = 0.\n            loss_ce = 0.\n            bs = eouts.size(0)\n            for b in range(bs):\n                # 1. beam search\n                self.eval()\n                with torch.no_grad():\n                    nbest_hyps_id, _, log_scores = self.beam_search(\n                        eouts[b:b + 1], elens[b:b + 1], params=recog_params,\n                        nbest=N_best, exclude_eos=True)\n                nbest_hyps_id_b = [np.fromiter(y, dtype=np.int64) for y in nbest_hyps_id[0]]\n                log_scores_b = np2tensor(np.array(log_scores[0], dtype=np.float32), self.device_id)\n                scores_b_norm = torch.softmax(alpha * log_scores_b, dim=-1)  # `[N_best]`\n                # print((scores_b_norm * 100).int())\n\n                # 2. calculate expected WER\n                wers_b = np2tensor(np.array([\n                    compute_wer(ref=idx2token(ys[b]).split(\' \'),\n                                hyp=idx2token(nbest_hyps_id_b[n]).split(\' \'))[0] / 100\n                    for n in range(N_best)], dtype=np.float32), self.device_id)\n                exp_wer_b = (scores_b_norm * wers_b).sum()\n                grad_b = (scores_b_norm * (wers_b - exp_wer_b)).sum()\n                # print(wers_b)\n                # print(exp_wer_b)\n                # print(scores_b_norm * (wers_b - exp_wer_b))\n                # print(grad_b)\n\n                # 3. forward pass (teacher-forcing with hypotheses)\n                self.train()\n                logits_b = self.forward_mbr(eouts[b:b + 1].repeat([N_best, 1, 1]),\n                                            elens[b:b + 1].repeat([N_best]),\n                                            nbest_hyps_id_b)\n                log_probs_b = torch.log_softmax(logits_b, dim=-1)  # `[nbest, L, vocab]`\n\n                # 4. backward pass (attach gradient)\n                _eos = eouts.new_zeros(1).fill_(self.eos).long()\n                nbest_hyps_id_b_pad = pad_list([torch.cat([np2tensor(y, self.device_id), _eos], dim=0)\n                                                for y in nbest_hyps_id_b], self.pad)\n                loss_mbr += self.mbr(log_probs_b, nbest_hyps_id_b_pad, exp_wer_b, grad_b)\n\n                # 4. calculate MBR loss\n                # loss_mbr_b, scores_b = minimum_bayes_risk(log_probs_b, nbest_hyps_id_b, wers_b,\n                #                                           self.eos, self.pad)\n                # loss_mbr += loss_mbr_b\n\n                # 5. CE loss regularization\n                loss_ce += self.forward_att(eouts[b:b + 1], elens[b:b + 1], ys[b:b + 1])[0]\n\n                # ys_out_b = append_sos_eos(eouts[b:b + 1], [ys[b]], self.eos, self.eos, self.pad)[1]\n                # ys_out_b = ys_out_b.repeat([N_best, 1])\n                # # NOTE: truncate to match the lengths\n                # ymax = min(logits_b.size(1), ys_out_b.size(1))\n                # logits_b = logits_b[:, :ymax].contiguous()\n                # ys_out_b = ys_out_b[:, :ymax].contiguous()\n                # for k in range(N_best):\n                #     loss_ce_k = cross_entropy_lsm(logits_b, ys_out_b, 0, self.pad, self.training)[0]\n                #     loss_ce += loss_ce_k * scores_b_norm[k]\n\n            # NOTE: MBR loss is accumlated over N-best and mini-batch\n            loss = loss_mbr + loss_ce * self.mbr_ce_weight\n            observation[\'loss_mbr\'] = loss_mbr.item()\n            observation[\'loss_att\'] = loss_ce.item()\n\n        observation[\'loss\'] = loss.item()\n        return loss, observation\n\n    def forward_mbr(self, eouts, elens, ys_hyp):\n        """"""Compute XE loss for the attention-based decoder.\n\n        Args:\n            eouts (FloatTensor): `[N_best, T, enc_n_units]`\n            elens (IntTensor): `[N_best]`\n            ys_hyp (list): length `N_best`, each of which contains a list of size `[L]`\n        Returns:\n            logits (FloatTensor): `[N_best, L, vocab]`\n\n        """"""\n        bs, xmax = eouts.size()[:2]\n\n        # Append <sos> and <eos>\n        ys_in, ys_out, ylens = append_sos_eos(eouts, ys_hyp, self.eos, self.eos, self.pad)\n\n        # Initialization\n        dstates = self.zero_state(bs)\n        cv = eouts.new_zeros(bs, 1, self.enc_n_units)\n        self.score.reset()\n        aw, aws = None, []\n        betas = []\n        lmout, lmstate = None, None\n\n        ys_emb = self.dropout_emb(self.embed(ys_in))\n        src_mask = make_pad_mask(elens, self.device_id).unsqueeze(1)  # `[B, 1, T]`\n        logits = []\n        for t in range(ys_in.size(1)):\n            is_sample = t > 0 and self._ss_prob > 0 and random.random() < self._ss_prob\n\n            # Update LM states for LM fusion\n            if self.lm is not None:\n                y_lm = self.output(logits[-1]).detach().argmax(-1) if is_sample else ys_in[:, t:t + 1]\n                lmout, lmstate, _ = self.lm.predict(y_lm, lmstate)\n\n            # Recurrency -> Score -> Generate\n            y_emb = self.dropout_emb(self.embed(\n                self.output(logits[-1]).detach().argmax(-1))) if is_sample else ys_emb[:, t:t + 1]\n            dstates, cv, aw, attn_v, beta = self.decode_step(\n                eouts, dstates, cv, y_emb, src_mask, aw, lmout, mode=\'parallel\')\n            aws.append(aw)  # `[B, H, 1, T]`\n            if beta is not None:\n                betas.append(beta)  # `[B, H, 1, T]`\n            logits.append(attn_v)\n\n        # for attention plot\n        with torch.no_grad():\n            aws = torch.cat(aws, dim=2)  # `[B, H, L, T]`\n            self.data_dict[\'elens\'] = tensor2np(elens)\n            self.data_dict[\'ylens\'] = tensor2np(ylens)\n            self.data_dict[\'ys\'] = tensor2np(ys_out)\n            self.aws_dict[\'xy_aws\'] = tensor2np(aws)\n            if len(betas) > 0:\n                betas = torch.cat(betas, dim=2)  # `[B, H, L, T]`\n                self.aws_dict[\'xy_aws_beta\'] = tensor2np(betas)\n\n        logits = self.output(torch.cat(logits, dim=1))\n        return logits\n\n    def forward_att(self, eouts, elens, ys,\n                    return_logits=False, teacher_logits=None, trigger_points=None):\n        """"""Compute XE loss for the attention-based decoder.\n\n        Args:\n            eouts (FloatTensor): `[B, T, enc_n_units]`\n            elens (IntTensor): `[B]`\n            ys (list): length `B`, each of which contains a list of size `[L]`\n            return_logits (bool): return logits for knowledge distillation\n            teacher_logits (FloatTensor): `[B, L, vocab]`\n            trigger_points (IntTensor): `[B, T]`\n        Returns:\n            loss (FloatTensor): `[1]`\n            acc (float): accuracy for token prediction\n            ppl (float): perplexity\n            loss_quantity (FloatTensor): `[1]`\n            loss_latency (FloatTensor): `[1]`\n\n        """"""\n        bs, xmax = eouts.size()[:2]\n\n        # Append <sos> and <eos>\n        ys_in, ys_out, ylens = append_sos_eos(eouts, ys, self.eos, self.eos, self.pad, self.bwd)\n        ymax = ys_in.size(1)\n\n        # Initialization\n        dstates = self.zero_state(bs)\n        if self.training:\n            if self.discourse_aware and not self.new_session:\n                dstates = {\'dstate\': (self.dstate_prev[\'hxs\'], self.dstate_prev[\'cxs\'])}\n            self.dstate_prev = {\'hxs\': [None] * bs, \'cxs\': [None] * bs}\n            self.new_session = False\n        cv = eouts.new_zeros(bs, 1, self.enc_n_units)\n        self.score.reset()\n        aw, aws = None, []\n        betas = []\n        lmout, lmstate = None, None\n\n        ys_emb = self.dropout_emb(self.embed(ys_in))\n        src_mask = make_pad_mask(elens, self.device_id).unsqueeze(1)  # `[B, 1, T]`\n        tgt_mask = (ys_out != self.pad).unsqueeze(2)  # `[B, L, 1]`\n        logits = []\n        for t in range(ymax):\n            is_sample = t > 0 and self._ss_prob > 0 and random.random() < self._ss_prob\n\n            # Update LM states for LM fusion\n            if self.lm is not None:\n                self.lm.eval()\n                with torch.no_grad():\n                    y_lm = self.output(logits[-1]).detach().argmax(-1) if is_sample else ys_in[:, t:t + 1]\n                    lmout, lmstate, _ = self.lm.predict(y_lm, lmstate)\n\n            # Recurrency -> Score -> Generate\n            y_emb = self.dropout_emb(self.embed(\n                self.output(logits[-1]).detach().argmax(-1))) if is_sample else ys_emb[:, t:t + 1]\n            dstates, cv, aw, attn_v, beta = self.decode_step(\n                eouts, dstates, cv, y_emb, src_mask, aw, lmout, mode=\'parallel\',\n                trigger_point=trigger_points[:, t] if trigger_points is not None else None)\n            aws.append(aw)  # `[B, H, 1, T]`\n            if beta is not None:\n                betas.append(beta)  # `[B, H, 1, T]`\n            logits.append(attn_v)\n\n            if self.training and self.discourse_aware:\n                for b in [b for b, ylen in enumerate(ylens.tolist()) if t == ylen - 1]:\n                    self.dstate_prev[\'hxs\'][b] = dstates[\'dstate\'][0][:, b:b + 1].detach()\n                    if self.rnn_type == \'lstm\':\n                        self.dstate_prev[\'cxs\'][b] = dstates[\'dstate\'][1][:, b:b + 1].detach()\n\n        if self.training and self.discourse_aware:\n            if bs > 1:\n                self.dstate_prev[\'hxs\'] = torch.cat(self.dstate_prev[\'hxs\'], dim=1)\n                if self.rnn_type == \'lstm\':\n                    self.dstate_prev[\'cxs\'] = torch.cat(self.dstate_prev[\'cxs\'], dim=1)\n            else:\n                self.dstate_prev[\'hxs\'] = self.dstate_prev[\'hxs\'][0]\n                if self.rnn_type == \'lstm\':\n                    self.dstate_prev[\'cxs\'] = self.dstate_prev[\'cxs\'][0]\n\n        logits = self.output(torch.cat(logits, dim=1))\n\n        # for knowledge distillation\n        if return_logits:\n            return logits\n\n        # for attention plot\n        aws = torch.cat(aws, dim=2)  # `[B, H, L, T]`\n        if not self.training:\n            self.data_dict[\'elens\'] = tensor2np(elens)\n            self.data_dict[\'ylens\'] = tensor2np(ylens)\n            self.data_dict[\'ys\'] = tensor2np(ys_out)\n            self.aws_dict[\'xy_aws\'] = tensor2np(aws)\n            if len(betas) > 0:\n                betas = torch.cat(betas, dim=2)  # `[B, H, L, T]`\n                self.aws_dict[\'xy_aws_beta\'] = tensor2np(betas)\n\n        n_heads = aws.size(1)  # mono\n\n        # Compute XE sequence loss (+ label smoothing)\n        loss, ppl = cross_entropy_lsm(logits, ys_out, self.lsm_prob, self.pad, self.training)\n\n        # Attention padding\n        if self.attn_type == \'mocha\' or trigger_points is not None:\n            aws = aws.masked_fill_(tgt_mask.unsqueeze(1).repeat([1, n_heads, 1, 1]) == 0, 0)\n            # NOTE: attention padding is quite effective for quantity loss\n\n        # Quantity loss\n        loss_quantity = 0.\n        if self.attn_type == \'mocha\':\n            # Average over all heads\n            n_tokens_pred = aws.sum(3).sum(2).sum(1) / n_heads  # `[B]`\n            n_tokens_ref = tgt_mask.squeeze(2).sum(1).float()  # `[B]`\n            # NOTE: count <eos> tokens\n            loss_quantity = torch.mean(torch.abs(n_tokens_pred - n_tokens_ref))\n\n        # Latency loss\n        loss_latency = 0.\n        if self.latency_metric == \'interval\':\n            assert trigger_points is None\n            assert aws.size(1) == 1  # TODO: extend to multi-head\n            aws_prev = torch.cat([aws.new_zeros(aws.size())[:, :, -1:], aws.clone()[:, :, :-1]], dim=2)\n            aws_mat = aws_prev.unsqueeze(3) * aws.unsqueeze(4)  # `[B, H, L, T, T]`\n            delay_mat = aws.new_ones(xmax, xmax).float()\n            delay_mat = torch.tril(delay_mat, diagonal=-1, out=delay_mat)\n            delay_mat = torch.cumsum(delay_mat, dim=-2).unsqueeze(0)\n            delay_mat = delay_mat.unsqueeze(1).unsqueeze(2).expand_as(aws_mat)\n            loss_latency = torch.pow((aws_mat * delay_mat).sum(-1), 2).sum(-1)\n            loss_latency = torch.mean(loss_latency.squeeze(1))\n        elif trigger_points is not None:\n            js = torch.arange(xmax, dtype=torch.float)\n            if self.device_id >= 0:\n                js = js.cuda(self.device_id)\n            js = js.repeat([bs, n_heads, ymax, 1])\n            exp_trigger_points = (js * aws).sum(3)  # `[B, H, L]`\n            trigger_points = trigger_points.float().unsqueeze(1)  # `[B, 1, L]`\n            if self.device_id >= 0:\n                trigger_points = trigger_points.cuda(self.device_id)\n            if self.latency_metric == \'ctc_sync\':\n                loss_latency = torch.abs(exp_trigger_points - trigger_points)  # `[B, H, L]`\n            elif self.latency_metric == \'ctc_dal\':\n                loss_latency = torch.abs(exp_trigger_points - trigger_points)  # `[B, H, L]`\n            else:\n                raise NotImplementedError(self.latency_metric)\n            # NOTE: trigger_points are padded with 0\n            loss_latency = loss_latency.sum() / ylens.sum()\n\n        # Knowledge distillation\n        if teacher_logits is not None:\n            kl_loss = distillation(logits, teacher_logits, ylens, temperature=5.0)\n            loss = loss * (1 - self.distillation_weight) + kl_loss * self.distillation_weight\n\n        # Compute token-level accuracy in teacher-forcing\n        acc = compute_accuracy(logits, ys_out, self.pad)\n\n        return loss, acc, ppl, loss_quantity, loss_latency\n\n    def decode_step(self, eouts, dstates, cv, y_emb, mask, aw, lmout,\n                    mode=\'hard\', trigger_point=None, cache=True):\n        dstates = self.recurrency(torch.cat([y_emb, cv], dim=-1), dstates[\'dstate\'])\n        cv, aw, beta = self.score(eouts, eouts, dstates[\'dout_score\'], mask, aw,\n                                  cache=cache, mode=mode, trigger_point=trigger_point)\n        attn_v = self.generate(cv, dstates[\'dout_gen\'], lmout)\n        return dstates, cv, aw, attn_v, beta\n\n    def zero_state(self, bs):\n        """"""Initialize decoder state.\n\n        Args:\n            bs (int): batch size\n        Returns:\n            dstates (dict):\n                dout (FloatTensor): `[B, 1, dec_n_units]`\n                dstate (tuple): A tuple of (hxs, cxs)\n                    hxs (FloatTensor): `[n_layers, B, dec_n_units]`\n                    cxs (FloatTensor): `[n_layers, B, dec_n_units]`\n\n        """"""\n        dstates = {\'dstate\': None}\n        w = next(self.parameters())\n        hxs = w.new_zeros(self.n_layers, bs, self.dec_n_units)\n        cxs = w.new_zeros(self.n_layers, bs, self.dec_n_units) if self.rnn_type == \'lstm\' else None\n        dstates[\'dstate\'] = (hxs, cxs)\n        return dstates\n\n    def recurrency(self, inputs, dstate):\n        """"""Recurrency function.\n\n        Args:\n            inputs (FloatTensor): `[B, 1, emb_dim + enc_n_units]`\n            dstate (tuple): A tuple of (hxs, cxs)\n        Returns:\n            new_dstates (dict):\n                dout_score (FloatTensor): `[B, 1, dec_n_units]`\n                dout_gen (FloatTensor): `[B, 1, dec_n_units]`\n                dstate (tuple): A tuple of (hxs, cxs)\n                    hxs (FloatTensor): `[n_layers, B, dec_n_units]`\n                    cxs (FloatTensor): `[n_layers, B, dec_n_units]`\n\n        """"""\n        hxs, cxs = dstate\n        dout = inputs.squeeze(1)\n\n        new_dstates = {\'dout_score\': None,  # for attention scoring\n                       \'dout_gen\': None,  # for token generation\n                       \'dstate\': None}\n\n        new_hxs, new_cxs = [], []\n        for lth in range(self.n_layers):\n            if self.rnn_type == \'lstm\':\n                h, c = self.rnn[lth](dout, (hxs[lth], cxs[lth]))\n                new_cxs.append(c)\n            elif self.rnn_type == \'gru\':\n                h = self.rnn[lth](dout, hxs[lth])\n            new_hxs.append(h)\n            dout = self.dropout(h)\n            if self.n_projs > 0:\n                dout = torch.tanh(self.proj[lth](dout))\n            # use output in the first layer for attention scoring\n            if lth == 0:\n                new_dstates[\'dout_score\'] = dout.unsqueeze(1)\n        new_hxs = torch.stack(new_hxs, dim=0)\n        if self.rnn_type == \'lstm\':\n            new_cxs = torch.stack(new_cxs, dim=0)\n\n        # use oupput in the the last layer for label generation\n        new_dstates[\'dout_gen\'] = dout.unsqueeze(1)\n        new_dstates[\'dstate\'] = (new_hxs, new_cxs)\n        return new_dstates\n\n    def generate(self, cv, dout, lmout):\n        """"""Generate function.\n\n        Args:\n            cv (FloatTensor): `[B, 1, enc_n_units]`\n            dout (FloatTensor): `[B, 1, dec_n_units]`\n            lmout (FloatTensor): `[B, 1, lm_n_units]`\n        Returns:\n            attn_v (FloatTensor): `[B, 1, vocab]`\n\n        """"""\n        gated_lmout = None\n        if self.lm is not None:\n            # LM fusion\n            dec_feat = self.linear_dec_feat(torch.cat([dout, cv], dim=-1))\n\n            if self.lm_fusion in [\'cold\', \'deep\']:\n                lmout = self.linear_lm_feat(lmout)\n                gate = torch.sigmoid(self.linear_lm_gate(torch.cat([dec_feat, lmout], dim=-1)))\n                gated_lmout = gate * lmout\n            elif self.lm_fusion == \'cold_prob\':\n                lmout = self.linear_lm_feat(self.lm.output(lmout))\n                gate = torch.sigmoid(self.linear_lm_gate(torch.cat([dec_feat, lmout], dim=-1)))\n                gated_lmout = gate * lmout\n\n            out = self.output_bn(torch.cat([dec_feat, gated_lmout], dim=-1))\n        else:\n            out = self.output_bn(torch.cat([dout, cv], dim=-1))\n        attn_v = torch.tanh(out)\n        return attn_v\n\n    def _plot_attention(self, save_path, n_cols=1):\n        """"""Plot attention for each head.""""""\n        if self.att_weight == 0:\n            return 0\n\n        from matplotlib import pyplot as plt\n        from matplotlib.ticker import MaxNLocator\n\n        _save_path = mkdir_join(save_path, \'dec_att_weights\')\n\n        # Clean directory\n        if _save_path is not None and os.path.isdir(_save_path):\n            shutil.rmtree(_save_path)\n            os.mkdir(_save_path)\n\n        elens = self.data_dict[\'elens\']\n        ylens = self.data_dict[\'ylens\']\n        # ys = self.data_dict[\'ys\']\n\n        for k, aw in self.aws_dict.items():\n            plt.clf()\n            n_heads = aw.shape[1]\n            n_cols_tmp = 1 if n_heads == 1 else n_cols\n            fig, axes = plt.subplots(max(1, n_heads // n_cols_tmp), n_cols_tmp,\n                                     figsize=(20, 8), squeeze=False)\n            for h in range(n_heads):\n                ax = axes[h // n_cols_tmp, h % n_cols_tmp]\n                ax.imshow(aw[-1, h, :ylens[-1], :elens[-1]], aspect=""auto"")\n                ax.grid(False)\n                ax.set_xlabel(""Input (head%d)"" % h)\n                ax.set_ylabel(""Output (head%d)"" % h)\n                ax.xaxis.set_major_locator(MaxNLocator(integer=True))\n                ax.yaxis.set_major_locator(MaxNLocator(integer=True))\n                # ax.set_yticks(np.linspace(0, ylens[-1] - 1, ylens[-1]))\n                # ax.set_yticks(np.linspace(0, ylens[-1] - 1, 1), minor=True)\n                # ax.set_yticklabels(ys + [\'\'])\n\n            fig.tight_layout()\n            fig.savefig(os.path.join(_save_path, \'%s.png\' % k), dvi=500)\n            plt.close()\n\n    def greedy(self, eouts, elens, max_len_ratio, idx2token,\n               exclude_eos=False, refs_id=None, utt_ids=None, speakers=None,\n               trigger_points=None):\n        """"""Greedy decoding.\n\n        Args:\n            eouts (FloatTensor): `[B, T, enc_units]`\n            elens (IntTensor): `[B]`\n            max_len_ratio (int): maximum sequence length of tokens\n            idx2token (): converter from index to token\n            exclude_eos (bool): exclude <eos> from hypothesis\n            refs_id (list): reference list\n            utt_ids (list): utterance id list\n            speakers (list): speaker list\n            trigger_points (IntTensor): `[B, T]`\n        Returns:\n            hyps (list): length `B`, each of which contains arrays of size `[L]`\n            aws (list): length `B`, each of which contains arrays of size `[H, L, T]`\n\n        """"""\n        bs, xmax, _ = eouts.size()\n\n        # Initialization\n        dstates = self.zero_state(bs)\n        if self.discourse_aware and not self.new_session:\n            dstates = {\'dstate\': (self.dstate_prev[\'hxs\'], self.dstate_prev[\'cxs\'])}\n        self.dstate_prev = {\'hxs\': [None] * bs, \'cxs\': [None] * bs}\n        self.new_session = False\n        cv = eouts.new_zeros(bs, 1, self.enc_n_units)\n        self.score.reset()\n        aw = None\n        lmout, lmstate = None, None\n        y = eouts.new_zeros(bs, 1).fill_(refs_id[0][0] if self.replace_sos else self.eos).long()\n\n        # Create the attention mask\n        src_mask = make_pad_mask(elens, self.device_id).unsqueeze(1)  # `[B, 1, T]`\n\n        if self.attn_type == \'triggered_attention\':\n            assert trigger_points is not None\n\n        hyps_batch, aws_batch = [], []\n        ylens = torch.zeros(bs).int()\n        eos_flags = [False] * bs\n        ymax = int(math.floor(xmax * max_len_ratio)) + 1\n        for t in range(ymax):\n            # Update LM states for LM fusion\n            if self.lm is not None:\n                lmout, lmstate = self.lm.decode(self.lm(y), lmstate)\n\n            # Recurrency -> Score -> Generate\n            y_emb = self.dropout_emb(self.embed(y))\n            dstates, cv, aw, attn_v, _ = self.decode_step(\n                eouts, dstates, cv, y_emb, src_mask, aw, lmout,\n                trigger_point=trigger_points[:, t] if trigger_points is not None else None)\n            aws_batch += [aw]  # `[B, H, 1, T]`\n\n            # Pick up 1-best\n            y = self.output(attn_v).argmax(-1)\n            hyps_batch += [y]\n\n            # Count lengths of hypotheses\n            for b in range(bs):\n                if not eos_flags[b]:\n                    if y[b].item() == self.eos:\n                        eos_flags[b] = True\n                        if self.discourse_aware:\n                            self.dstate_prev[\'hxs\'][b] = dstates[\'dstate\'][0][:, b:b + 1]\n                            if self.rnn_type == \'lstm\':\n                                self.dstate_prev[\'cxs\'][b] = dstates[\'dstate\'][1][:, b:b + 1]\n                    ylens[b] += 1  # include <eos>\n\n            # Break if <eos> is outputed in all mini-batch\n            if sum(eos_flags) == bs:\n                break\n            if t == ymax - 1:\n                break\n\n        # ASR state carry over\n        if self.discourse_aware:\n            if bs > 1:\n                self.dstate_prev[\'hxs\'] = torch.cat(self.dstate_prev[\'hxs\'], dim=1)\n                if self.rnn_type == \'lstm\':\n                    self.dstate_prev[\'cxs\'] = torch.cat(self.dstate_prev[\'cxs\'], dim=1)\n            else:\n                self.dstate_prev[\'hxs\'] = self.dstate_prev[\'hxs\']\n                if self.rnn_type == \'lstm\':\n                    self.dstate_prev[\'cxs\'] = self.dstate_prev[\'cxs\']\n\n        # LM state carry over\n        self.lmstate_final = lmstate\n\n        # Concatenate in L dimension\n        hyps_batch = tensor2np(torch.cat(hyps_batch, dim=1))\n        aws_batch = tensor2np(torch.cat(aws_batch, dim=2))  # `[B, H, L, T]`\n\n        # Truncate by the first <eos> (<sos> in case of the backward decoder)\n        if self.bwd:\n            # Reverse the order\n            hyps = [hyps_batch[b, :ylens[b]][::-1] for b in range(bs)]\n            aws = [aws_batch[b, :, :ylens[b]][::-1] for b in range(bs)]\n        else:\n            hyps = [hyps_batch[b, :ylens[b]] for b in range(bs)]\n            aws = [aws_batch[b, :, :ylens[b]] for b in range(bs)]\n\n        # Exclude <eos> (<sos> in case of the backward decoder)\n        if exclude_eos:\n            if self.bwd:\n                hyps = [hyps[b][1:] if eos_flags[b] else hyps[b] for b in range(bs)]\n            else:\n                hyps = [hyps[b][:-1] if eos_flags[b] else hyps[b] for b in range(bs)]\n\n        if idx2token is not None:\n            for b in range(bs):\n                if utt_ids is not None:\n                    logger.debug(\'Utt-id: %s\' % utt_ids[b])\n                if refs_id is not None and self.vocab == idx2token.vocab:\n                    logger.debug(\'Ref: %s\' % idx2token(refs_id[b]))\n                if self.bwd:\n                    logger.debug(\'Hyp: %s\' % idx2token(hyps[b][::-1]))\n                else:\n                    logger.debug(\'Hyp: %s\' % idx2token(hyps[b]))\n\n        return hyps, aws\n\n    def beam_search(self, eouts, elens, params, idx2token=None,\n                    lm=None, lm_second=None, lm_second_bwd=None, ctc_log_probs=None,\n                    nbest=1, exclude_eos=False,\n                    refs_id=None, utt_ids=None, speakers=None,\n                    ensmbl_eouts=None, ensmbl_elens=None, ensmbl_decs=[], cache_states=True):\n        """"""Beam search decoding.\n\n        Args:\n            eouts (FloatTensor): `[B, T, enc_n_units]`\n            elens (IntTensor): `[B]`\n            params (dict): hyperparameters for decoding\n            idx2token (): converter from index to token\n            lm: firsh path LM\n            lm_second: second path LM\n            lm_second_bwd: secoding path backward LM\n            ctc_log_probs (FloatTensor):\n            nbest (int):\n            exclude_eos (bool): exclude <eos> from hypothesis\n            refs_id (list): reference list\n            utt_ids (list): utterance id list\n            speakers (list): speaker list\n            ensmbl_eouts (list): list of FloatTensor\n            ensmbl_elens (list) list of list\n            ensmbl_decs (list): list of torch.nn.Module\n            cache_states (bool): cache TransformerLM/TransformerXL states for fast decoding\n        Returns:\n            nbest_hyps_idx (list): length `B`, each of which contains list of N hypotheses\n            aws (list): length `B`, each of which contains arrays of size `[H, L, T]`\n            scores (list):\n\n        """"""\n        bs, xmax, _ = eouts.size()\n        n_models = len(ensmbl_decs) + 1\n\n        beam_width = params[\'recog_beam_width\']\n        assert 1 <= nbest <= beam_width\n        ctc_weight = params[\'recog_ctc_weight\']\n        max_len_ratio = params[\'recog_max_len_ratio\']\n        min_len_ratio = params[\'recog_min_len_ratio\']\n        lp_weight = params[\'recog_length_penalty\']\n        cp_weight = params[\'recog_coverage_penalty\']\n        cp_threshold = params[\'recog_coverage_threshold\']\n        length_norm = params[\'recog_length_norm\']\n        lm_weight = params[\'recog_lm_weight\']\n        lm_weight_second = params[\'recog_lm_second_weight\']\n        lm_weight_second_bwd = params[\'recog_lm_bwd_weight\']\n        gnmt_decoding = params[\'recog_gnmt_decoding\']\n        eos_threshold = params[\'recog_eos_threshold\']\n        asr_state_CO = params[\'recog_asr_state_carry_over\']\n        lm_state_CO = params[\'recog_lm_state_carry_over\']\n        softmax_smoothing = params[\'recog_softmax_smoothing\']\n\n        if lm is not None:\n            assert lm_weight > 0\n            lm.eval()\n        if lm_second is not None:\n            assert lm_weight_second > 0\n            lm_second.eval()\n        if lm_second_bwd is not None:\n            assert lm_weight_second_bwd > 0\n            lm_second_bwd.eval()\n        trfm_lm = isinstance(lm, TransformerLM) or isinstance(lm, TransformerXL)\n\n        if ctc_log_probs is not None:\n            assert ctc_weight > 0\n            ctc_log_probs = tensor2np(ctc_log_probs)\n\n        nbest_hyps_idx, aws, scores = [], [], []\n        eos_flags = []\n        for b in range(bs):\n            # Initialization per utterance\n            self.score.reset()\n            dstates = self.zero_state(1)\n            lmstate = None\n            ys = eouts.new_zeros(1, 1).fill_(self.eos).long()  # for TransformerLM/TransformerXL\n\n            # For joint CTC-Attention decoding\n            ctc_prefix_scorer = None\n            if ctc_log_probs is not None:\n                if self.bwd:\n                    ctc_prefix_scorer = CTCPrefixScore(ctc_log_probs[b][::-1], self.blank, self.eos)\n                else:\n                    ctc_prefix_scorer = CTCPrefixScore(ctc_log_probs[b], self.blank, self.eos)\n\n            # Ensemble initialization\n            ensmbl_dstate, ensmbl_cv = [], []\n            if n_models > 1:\n                for dec in ensmbl_decs:\n                    ensmbl_dstate += [dec.zero_state(1)]\n                    ensmbl_cv += [eouts.new_zeros(1, 1, dec.enc_n_units)]\n                    dec.score.reset()\n\n            if speakers is not None:\n                if speakers[b] == self.prev_spk:\n                    if asr_state_CO:\n                        dstates = self.dstates_final\n                    if lm_state_CO:\n                        if isinstance(lm, RNNLM):\n                            lmstate = self.lmstate_final\n                        elif isinstance(lm, TransformerLM):\n                            ys_prev = self.lmstate_final\n                            # Re-encode past tokens here\n                            _, lmstate, _ = lm.predict(ys_prev)\n                            ys = torch.cat([ys_prev, ys], dim=1)\n                        # elif isinstance(lm, TransformerXL):\n                        #     ys_prev = self.lmstate_final\n                        #     # Re-encode past tokens here\n                        #     _, lmstate, _ = lm.predict(ys_prev, mems=self.lmmemory)\n                        #     ys = torch.cat([ys_prev, ys], dim=1)\n                else:\n                    self.dstates_final = None  # reset\n                    self.lmstate_final = None  # reset\n                    self.lmmemory = None  # reset\n                self.prev_spk = speakers[b]\n\n            helper = BeamSearch(beam_width, self.eos, ctc_weight, self.device_id)\n\n            end_hyps = []\n            hyps = [{\'hyp\': [self.eos],\n                     \'ys\': ys,\n                     \'score\': 0.,\n                     \'score_att\': 0.,\n                     \'score_ctc\': 0.,\n                     \'score_lm\': 0.,\n                     \'dstates\': dstates,\n                     \'cv\': eouts.new_zeros(1, 1, self.enc_n_units),\n                     \'aws\': [None],\n                     \'lmstate\': lmstate,\n                     \'ensmbl_dstate\': ensmbl_dstate,\n                     \'ensmbl_cv\': ensmbl_cv,\n                     \'ensmbl_aws\':[[None]] * (n_models - 1),\n                     \'ctc_state\': ctc_prefix_scorer.initial_state() if ctc_prefix_scorer is not None else None}]\n            ymax = int(math.floor(elens[b] * max_len_ratio)) + 1\n            for t in range(ymax):\n                # batchfy all hypotheses for batch decoding\n                y = eouts.new_zeros(len(hyps), 1).long()\n                for j, beam in enumerate(hyps):\n                    if self.replace_sos and t == 0:\n                        prev_idx = refs_id[0][0]\n                    else:\n                        prev_idx = beam[\'hyp\'][-1]\n                    y[j, 0] = prev_idx\n                cv = torch.cat([beam[\'cv\'] for beam in hyps], dim=0)\n                aw = torch.cat([beam[\'aws\'][-1] for beam in hyps], dim=0) if t > 0 else None\n                hxs = torch.cat([beam[\'dstates\'][\'dstate\'][0] for beam in hyps], dim=1)\n                if self.rnn_type == \'lstm\':\n                    cxs = torch.cat([beam[\'dstates\'][\'dstate\'][1] for beam in hyps], dim=1)\n                dstates = {\'dstate\': (hxs, cxs)}\n\n                # Update LM states for LM fusion\n                lmout, lmstate, scores_lm = None, None, None\n                if lm is not None or self.lm is not None:\n                    if trfm_lm:\n                        ys = eouts.new_zeros(len(hyps), beam[\'ys\'].size(1)).long()\n                        for j, cand in enumerate(hyps):\n                            ys[j, :] = cand[\'ys\']\n                    else:\n                        ys = y\n\n                    if t > 0 or (t == 0 and trfm_lm and lm_state_CO and self.lmstate_final is not None):\n                        if isinstance(lm, RNNLM):\n                            lmstate = {\'hxs\': torch.cat([beam[\'lmstate\'][\'hxs\'] for beam in hyps], dim=1),\n                                       \'cxs\': torch.cat([beam[\'lmstate\'][\'cxs\'] for beam in hyps], dim=1)}\n                        elif trfm_lm:\n                            if isinstance(lm, TransformerLM):\n                                lmstate = [torch.cat([beam[\'lmstate\'][l] for beam in hyps], dim=0)\n                                           for l in range(lm.n_layers)]\n                            elif t > 0:\n                                lmstate = [torch.cat([beam[\'lmstate\'][l] for beam in hyps], dim=0)\n                                           for l in range(lm.n_layers)]\n\n                    if self.lm is not None:  # cold/deep fusion\n                        lmout, lmstate, scores_lm = self.lm.predict(y, lmstate)\n                    elif lm is not None:  # shallow fusion\n                        lmout, lmstate, scores_lm = lm.predict(ys, lmstate,\n                                                               mems=self.lmmemory,\n                                                               cache=lmstate if cache_states else None)\n\n                # for the main model\n                dstates, cv, aw, attn_v, _ = self.decode_step(\n                    eouts[b:b + 1, :elens[b]].repeat([cv.size(0), 1, 1]),\n                    dstates, cv, self.dropout_emb(self.embed(y)), None, aw, lmout)\n                probs = torch.softmax(self.output(attn_v).squeeze(1) * softmax_smoothing, dim=1)\n\n                # for the ensemble\n                ensmbl_dstate, ensmbl_cv, ensmbl_aws = [], [], []\n                if n_models > 1:\n                    for i_e, dec in enumerate(ensmbl_decs):\n                        cv_e = torch.cat([beam[\'ensmbl_cv\'][i_e] for beam in hyps], dim=0)\n                        aw_e = torch.cat([beam[\'ensmbl_aws\'][i_e][-1] for beam in hyps], dim=0) if t > 0 else None\n                        hxs_e = torch.cat([beam[\'ensmbl_dstate\'][i_e][\'dstate\'][0] for beam in hyps], dim=1)\n                        if self.rnn_type == \'lstm\':\n                            cxs_e = torch.cat([beam[\'dstates\'][i_e][\'dstate\'][1] for beam in hyps], dim=1)\n                        dstates_e = {\'dstate\': (hxs_e, cxs_e)}\n\n                        dstate_e, cv_e, aw_e, attn_v_e, _ = dec.decode_step(\n                            ensmbl_eouts[i_e][b:b + 1, :ensmbl_elens[i_e][b]].repeat([cv_e.size(0), 1, 1]),\n                            dstates_e, cv_e, dec.dropout_emb(dec.embed(y)), None, aw_e, lmout)\n\n                        ensmbl_dstate += [{\'dstate\': (beam[\'dstates\'][i_e][\'dstate\'][0][:, j:j + 1],\n                                                      beam[\'dstates\'][i_e][\'dstate\'][1][:, j:j + 1])}]\n                        ensmbl_cv += [cv_e[j:j + 1]]\n                        ensmbl_aws += [beam[\'ensmbl_aws\'][i_e] + [aw_e[j:j + 1]]]\n                        probs += torch.softmax(dec.output(attn_v_e).squeeze(1), dim=1)\n                        # NOTE: sum in the probability scale (not log-scale)\n\n                # Ensemble\n                scores_att = torch.log(probs / n_models)\n\n                new_hyps = []\n                for j, beam in enumerate(hyps):\n                    # Attention scores\n                    total_scores_att = beam[\'score_att\'] + scores_att[j:j + 1]\n                    total_scores = total_scores_att * (1 - ctc_weight)\n\n                    # Add LM score <after> top-K selection\n                    total_scores_topk, topk_ids = torch.topk(\n                        total_scores, k=beam_width, dim=1, largest=True, sorted=True)\n                    if lm is not None:\n                        total_scores_lm = beam[\'score_lm\'] + scores_lm[j, -1, topk_ids[0]]\n                        total_scores_topk += total_scores_lm * lm_weight\n                    else:\n                        total_scores_lm = eouts.new_zeros(beam_width)\n\n                    # Add length penalty\n                    if lp_weight > 0:\n                        if gnmt_decoding:\n                            lp = math.pow(6 + len(beam[\'hyp\'][1:]), lp_weight) / math.pow(6, lp_weight)\n                            total_scores_topk /= lp\n                        else:\n                            total_scores_topk += (len(beam[\'hyp\'][1:]) + 1) * lp_weight\n\n                    # Add coverage penalty\n                    if cp_weight > 0:\n                        aw_mat = torch.cat(beam[\'aws\'][1:] + [aw[j:j + 1]], dim=2)  # `[B, H, L, T]`\n                        aw_mat = aw_mat[:, 0, :, :]  # `[B, L, T]`\n                        if gnmt_decoding:\n                            aw_mat = torch.log(aw_mat.sum(-1))\n                            cp = torch.where(aw_mat < 0, aw_mat, aw_mat.new_zeros(aw_mat.size())).sum()\n                            # TODO(hirofumi): mask by elens[b]\n                            total_scores_topk += cp * cp_weight\n                        else:\n                            # Recompute converage penalty at each step\n                            if cp_threshold == 0:\n                                cp = aw_mat.sum() / self.score.n_heads\n                            else:\n                                cp = torch.where(aw_mat > cp_threshold, aw_mat,\n                                                 aw_mat.new_zeros(aw_mat.size())).sum() / self.score.n_heads\n                            total_scores_topk += cp * cp_weight\n                    else:\n                        cp = 0.\n\n                    # Add CTC score\n                    new_ctc_states, total_scores_ctc, total_scores_topk = helper.add_ctc_score(\n                        beam[\'hyp\'], topk_ids, beam[\'ctc_state\'],\n                        total_scores_topk, ctc_prefix_scorer)\n\n                    for k in range(beam_width):\n                        idx = topk_ids[0, k].item()\n                        length_norm_factor = 1.\n                        if length_norm:\n                            length_norm_factor = len(beam[\'hyp\'][1:]) + 1\n                        total_score = total_scores_topk[0, k].item() / length_norm_factor\n\n                        if idx == self.eos:\n                            # Exclude short hypotheses\n                            if len(beam[\'hyp\']) - 1 < elens[b] * min_len_ratio:\n                                continue\n                            # EOS threshold\n                            max_score_no_eos = scores_att[j, :idx].max(0)[0].item()\n                            max_score_no_eos = max(max_score_no_eos, scores_att[j, idx + 1:].max(0)[0].item())\n                            if scores_att[j, idx].item() <= eos_threshold * max_score_no_eos:\n                                continue\n\n                        new_lmstate = None\n                        if lmstate is not None:\n                            if isinstance(lm, RNNLM):\n                                new_lmstate = {\'hxs\': lmstate[\'hxs\'][:, j:j + 1],\n                                               \'cxs\': lmstate[\'cxs\'][:, j:j + 1]}\n                            elif trfm_lm:\n                                new_lmstate = [lmstate_l[j:j + 1] for lmstate_l in lmstate]\n                            else:\n                                raise ValueError(type(lm))\n\n                        ys = torch.cat([beam[\'ys\'], eouts.new_zeros(1, 1).fill_(idx).long()], dim=-1)\n\n                        new_hyps.append(\n                            {\'hyp\': beam[\'hyp\'] + [idx],\n                             \'ys\': ys,\n                             \'score\': total_score,\n                             \'score_att\': total_scores_att[0, idx].item(),\n                             \'score_cp\': cp,\n                             \'score_ctc\': total_scores_ctc[k].item(),\n                             \'score_lm\': total_scores_lm[k].item(),\n                             \'dstates\': {\'dstate\': (dstates[\'dstate\'][0][:, j:j + 1],\n                                                    dstates[\'dstate\'][1][:, j:j + 1])},\n                             \'cv\': cv[j:j + 1],\n                             \'aws\': beam[\'aws\'] + [aw[j:j + 1]],\n                             \'lmstate\': new_lmstate,\n                             \'ctc_state\': new_ctc_states[k] if ctc_prefix_scorer is not None else None,\n                             \'ensmbl_dstate\': ensmbl_dstate,\n                             \'ensmbl_cv\': ensmbl_cv,\n                             \'ensmbl_aws\': ensmbl_aws})\n\n                # Local pruning\n                new_hyps_sorted = sorted(new_hyps, key=lambda x: x[\'score\'], reverse=True)[:beam_width]\n\n                # Remove complete hypotheses\n                new_hyps, end_hyps, is_finish = helper.remove_complete_hyp(new_hyps_sorted, end_hyps)\n                hyps = new_hyps[:]\n                if is_finish:\n                    break\n\n            # Global pruning\n            if len(end_hyps) == 0:\n                end_hyps = hyps[:]\n            elif len(end_hyps) < nbest and nbest > 1:\n                end_hyps.extend(hyps[:nbest - len(end_hyps)])\n\n            # forward second path LM rescoring\n            if lm_second is not None:\n                self.lm_rescoring(end_hyps, lm_second, lm_weight_second, tag=\'second\')\n\n            # backward secodn path LM rescoring\n            if lm_second_bwd is not None:\n                self.lm_rescoring(end_hyps, lm_second_bwd, lm_weight_second_bwd, tag=\'second_bwd\')\n\n            # Sort by score\n            end_hyps = sorted(end_hyps, key=lambda x: x[\'score\'], reverse=True)\n\n            if idx2token is not None:\n                if utt_ids is not None:\n                    logger.info(\'Utt-id: %s\' % utt_ids[b])\n                assert self.vocab == idx2token.vocab\n                logger.info(\'=\' * 200)\n                for k in range(len(end_hyps)):\n                    if refs_id is not None:\n                        logger.info(\'Ref: %s\' % idx2token(refs_id[b]))\n                    logger.info(\'Hyp: %s\' % idx2token(\n                        end_hyps[k][\'hyp\'][1:][::-1] if self.bwd else end_hyps[k][\'hyp\'][1:]))\n                    logger.info(\'log prob (hyp): %.7f\' % end_hyps[k][\'score\'])\n                    logger.info(\'log prob (hyp, att): %.7f\' % (end_hyps[k][\'score_att\'] * (1 - ctc_weight)))\n                    logger.info(\'log prob (hyp, cp): %.7f\' % (end_hyps[k][\'score_cp\'] * cp_weight))\n                    if ctc_prefix_scorer is not None:\n                        logger.info(\'log prob (hyp, ctc): %.7f\' % (end_hyps[k][\'score_ctc\'] * ctc_weight))\n                    if lm is not None:\n                        logger.info(\'log prob (hyp, first-path lm): %.7f\' % (end_hyps[k][\'score_lm\'] * lm_weight))\n                    if lm_second is not None:\n                        logger.info(\'log prob (hyp, second-path lm): %.7f\' %\n                                    (end_hyps[k][\'score_lm_second\'] * lm_weight_second))\n                    if lm_second_bwd is not None:\n                        logger.info(\'log prob (hyp, second-path lm, reverse): %.7f\' %\n                                    (end_hyps[k][\'score_lm_second_rev\'] * lm_weight_second_bwd))\n                    logger.info(\'-\' * 50)\n\n            # N-best list\n            if self.bwd:\n                # Reverse the order\n                nbest_hyps_idx += [[np.array(end_hyps[n][\'hyp\'][1:][::-1]) for n in range(nbest)]]\n                aws += [tensor2np(torch.cat(end_hyps[0][\'aws\'][1:][::-1], dim=2).squeeze(0))]\n            else:\n                nbest_hyps_idx += [[np.array(end_hyps[n][\'hyp\'][1:]) for n in range(nbest)]]\n                aws += [tensor2np(torch.cat(end_hyps[0][\'aws\'][1:], dim=2).squeeze(0))]\n            if length_norm:\n                scores += [[end_hyps[n][\'score_att\'] / len(end_hyps[n][\'hyp\'][1:]) for n in range(nbest)]]\n            else:\n                scores += [[end_hyps[n][\'score_att\'] for n in range(nbest)]]\n\n            # Check <eos>\n            eos_flags.append([(end_hyps[n][\'hyp\'][-1] == self.eos) for n in range(nbest)])\n\n        # Exclude <eos> (<sos> in case of the backward decoder)\n        if exclude_eos:\n            if self.bwd:\n                nbest_hyps_idx = [[nbest_hyps_idx[b][n][1:] if eos_flags[b][n]\n                                   else nbest_hyps_idx[b][n] for n in range(nbest)] for b in range(bs)]\n            else:\n                nbest_hyps_idx = [[nbest_hyps_idx[b][n][:-1] if eos_flags[b][n]\n                                   else nbest_hyps_idx[b][n] for n in range(nbest)] for b in range(bs)]\n\n        # Store ASR/LM state\n        self.dstates_final = end_hyps[0][\'dstates\']\n        if isinstance(lm, RNNLM):\n            self.lmstate_final = end_hyps[0][\'lmstate\']\n        elif trfm_lm:\n            if isinstance(lm, TransformerXL):\n                self.lmmemory = lm.update_memory(self.lmmemory, end_hyps[0][\'lmstate\'])\n                logging.info(\'Memory: %d\' % self.lmmemory[0].size(1))\n            else:\n                ys = end_hyps[0][\'ys\']\n                # Exclude the last state corresponding to <eos>\n                if ys[0, -1].item() == self.eos:\n                    ys = ys[:, :-1]\n                ys = ys[:, -lm.mem_len:]  # Truncate by BPTT length\n            self.lmstate_final = ys\n\n        return nbest_hyps_idx, aws, scores\n\n    def beam_search_chunk_sync(self, eouts_c, params, idx2token,\n                               lm=None, ctc_log_probs=None,\n                               hyps=False, state_carry_over=False, ignore_eos=False):\n        bs, chunk_size, _ = eouts_c.size()\n        assert bs == 1\n        assert self.attn_type == \'mocha\'\n\n        beam_width = params[\'recog_beam_width\']\n        # beam_width_second = params[\'recog_beam_width\']\n        ctc_weight = params[\'recog_ctc_weight\']\n        max_len_ratio = params[\'recog_max_len_ratio\']\n        lp_weight = params[\'recog_length_penalty\']\n        length_norm = params[\'recog_length_norm\']\n        lm_weight = params[\'recog_lm_weight\']\n        eos_threshold = params[\'recog_eos_threshold\']\n\n        if lm is not None:\n            assert lm_weight > 0\n            lm.eval()\n\n        # Initialization per utterance\n        self.score.reset()\n        dstates = self.zero_state(1)\n        lmstate = None\n        ctc_state = None\n\n        # For joint CTC-Attention decoding\n        self.ctc_prefix_scorer = None\n        if ctc_log_probs is not None:\n            assert ctc_weight > 0\n            ctc_log_probs = tensor2np(ctc_log_probs)\n            if hyps is None:\n                # first chunk\n                self.ctc_prefix_scorer = CTCPrefixScore(ctc_log_probs[0], self.blank, self.eos)\n            else:\n                self.ctc_prefix_scorer.register_new_chunk(ctc_log_probs[0])\n            ctc_state = self.ctc_prefix_scorer.initial_state()\n        # TODO: add truncated version\n\n        if state_carry_over:\n            dstates = self.dstates_final\n            if isinstance(lm, RNNLM):\n                lmstate = self.lmstate_final\n\n        helper = BeamSearch(beam_width, self.eos, ctc_weight, self.device_id)\n\n        end_hyps = []\n        hyps_nobd = []\n        if hyps is None:\n            self.n_frames = 0\n            self.chunk_size = eouts_c.size(1)\n            hyps = [{\'hyp\': [self.eos],\n                     \'score\': 0.,\n                     \'score_att\': 0.,\n                     \'score_ctc\': 0.,\n                     \'score_lm\': 0.,\n                     \'dstates\': dstates,\n                     \'cv\': eouts_c.new_zeros(1, 1, self.enc_n_units),\n                     \'aws\': [None],\n                     \'lmstate\': lmstate,\n                     \'ctc_state\': ctc_state,\n                     \'no_boundary\': False}]\n        else:\n            for h in hyps:\n                h[\'no_boundary\'] = False\n\n        ymax = int(math.floor(eouts_c.size(1) * max_len_ratio)) + 1\n        for t in range(ymax):\n            # finish if no additional decision boundary is found in all candidates\n            if len(hyps) == 0:\n                break\n            if t > 0 and sum([cand[\'no_boundary\'] for cand in hyps]) == len(hyps):\n                break\n\n            # ignore hypotheses with no boundary from batch decoding\n            new_hyps = []\n            hyps_filtered = []\n            for j, beam in enumerate(hyps):\n                # no decision boundary found in this chunk\n                if beam[\'no_boundary\']:\n                    new_hyps.append(beam.copy())\n                else:\n                    hyps_filtered.append(beam.copy())\n            if len(hyps_filtered) == 0:\n                break\n            hyps = hyps_filtered[:]\n\n            # preprocess for batch decoding\n            y = eouts_c.new_zeros(len(hyps), 1).long()\n            for j, beam in enumerate(hyps):\n                y[j, 0] = beam[\'hyp\'][-1]\n            cv = torch.cat([beam[\'cv\'] for beam in hyps], dim=0)\n            aw = torch.cat([beam[\'aws\'][-1] for beam in hyps], dim=0) if t > 0 else None\n            hxs = torch.cat([beam[\'dstates\'][\'dstate\'][0] for beam in hyps], dim=1)\n            if self.rnn_type == \'lstm\':\n                cxs = torch.cat([beam[\'dstates\'][\'dstate\'][1] for beam in hyps], dim=1)\n            dstates = {\'dstate\': (hxs, cxs)}\n\n            # Update LM states for LM fusion\n            lmout, lmstate, scores_lm = None, None, None\n            if lm is not None or self.lm is not None:\n                if beam[\'lmstate\'] is not None:\n                    lm_hxs = torch.cat([beam[\'lmstate\'][\'hxs\'] for beam in hyps], dim=1)\n                    lm_cxs = torch.cat([beam[\'lmstate\'][\'cxs\'] for beam in hyps], dim=1)\n                    lmstate = {\'hxs\': lm_hxs, \'cxs\': lm_cxs}\n                if self.lm is not None:  # cold/deep fusion\n                    lmout, lmstate, scores_lm = self.lm.predict(y, lmstate)\n                elif lm is not None:  # shallow fusion\n                    lmout, lmstate, scores_lm = lm.predict(y, lmstate)\n\n            dstates, cv, aw, attn_v, _ = self.decode_step(\n                eouts_c[0:1].repeat([cv.size(0), 1, 1]),\n                dstates, cv, self.dropout_emb(self.embed(y)), None, aw, lmout, cache=False)\n            scores_att = torch.log_softmax(self.output(attn_v).squeeze(1), dim=1)\n\n            for j, beam in enumerate(hyps):\n                # no decision boundary found in this chunk for j-th utterance\n                no_boundary = aw[j].sum().item() == 0\n                if no_boundary:\n                    beam[\'aws\'][-1] = eouts_c.new_zeros(eouts_c.size(0), 1, 1, eouts_c.size(1))\n                    # NOTE: the case where the first token in the current chunk is <eos>\n                    beam[\'no_boundary\'] = True\n                    new_hyps.append(beam.copy())  # this is important to remove repeated hyps\n\n                # Attention scores\n                total_scores_att = beam[\'score_att\'] + scores_att[j:j + 1]\n                total_scores = total_scores_att * (1 - ctc_weight)\n\n                # Add LM score <after> top-K selection\n                total_scores_topk, topk_ids = torch.topk(\n                    total_scores, k=beam_width, dim=1, largest=True, sorted=True)\n                if lm is not None:\n                    total_scores_lm = beam[\'score_lm\'] + scores_lm[j, -1, topk_ids[0]]\n                    total_scores_topk += total_scores_lm * lm_weight\n                else:\n                    total_scores_lm = eouts_c.new_zeros(beam_width)\n\n                # Add length penalty\n                total_scores_topk += (len(beam[\'hyp\'][1:]) + 1) * lp_weight\n\n                # Add CTC score\n                new_ctc_states, total_scores_ctc, total_scores_topk = helper.add_ctc_score(\n                    beam[\'hyp\'], topk_ids, beam[\'ctc_state\'],\n                    total_scores_topk, self.ctc_prefix_scorer, new_chunk=(t == 0))\n\n                for k in range(beam_width):\n                    idx = topk_ids[0, k].item()\n                    if no_boundary and idx != self.eos:\n                        continue\n                    length_norm_factor = len(beam[\'hyp\'][1:]) + 1 if length_norm else 1\n                    total_score = total_scores_topk[0, k].item() / length_norm_factor\n\n                    if idx == self.eos:\n                        if ignore_eos:\n                            # NOTE: for unidirectional encoder\n                            beam[\'aws\'][-1] = eouts_c.new_zeros(eouts_c.size(0), 1, 1, eouts_c.size(1))\n                            beam[\'no_boundary\'] = True\n                            new_hyps.append(beam.copy())\n                            continue\n\n                        # EOS threshold\n                        max_score_no_eos = scores_att[j, :idx].max(0)[0].item()\n                        max_score_no_eos = max(max_score_no_eos, scores_att[j, idx + 1:].max(0)[0].item())\n                        if scores_att[j, idx].item() <= eos_threshold * max_score_no_eos:\n                            continue\n\n                    new_hyps.append(\n                        {\'hyp\': beam[\'hyp\'] + [idx],\n                         \'score\': total_score,\n                         \'score_att\': total_scores_att[0, idx].item(),\n                         \'score_ctc\': total_scores_ctc[k].item(),\n                         \'score_lm\': total_scores_lm[k].item(),\n                         \'dstates\': {\'dstate\': (dstates[\'dstate\'][0][:, j:j + 1], dstates[\'dstate\'][1][:, j:j + 1])},\n                         \'cv\': cv[j:j + 1],\n                         \'aws\': beam[\'aws\'] + [aw[j:j + 1]],\n                         \'lmstate\': {\'hxs\': lmstate[\'hxs\'][:, j:j + 1],\n                                     \'cxs\': lmstate[\'cxs\'][:, j:j + 1]} if lmstate is not None else None,\n                         \'ctc_state\': new_ctc_states[k] if self.ctc_prefix_scorer is not None else None,\n                         \'no_boundary\': no_boundary})\n\n            # Local pruning\n            new_hyps_sorted = sorted(new_hyps, key=lambda x: x[\'score\'], reverse=True)\n            hyps_nobd += [hyp for hyp in new_hyps_sorted[beam_width:] if hyp[\'no_boundary\']]\n\n            # Remove complete hypotheses\n            new_hyps, end_hyps, is_finish = helper.remove_complete_hyp(new_hyps_sorted[:beam_width], end_hyps)\n            hyps = new_hyps[:]\n            if is_finish:\n                break\n\n        # Global pruning\n        hyps_nobd_sorted = sorted(hyps_nobd, key=lambda x: x[\'score\'], reverse=True)\n        hyps = (hyps[:] + hyps_nobd_sorted)[:beam_width]\n\n        # Sort by score\n        if len(end_hyps) > 0:\n            end_hyps = sorted(end_hyps, key=lambda x: x[\'score\'], reverse=True)\n\n        merged_hyps = sorted(end_hyps + hyps, key=lambda x: x[\'score\'], reverse=True)[:beam_width]\n        if idx2token is not None:\n            logger.info(\'=\' * 200)\n            for k in range(len(merged_hyps)):\n                logger.info(\'Hyp: %s\' % idx2token(merged_hyps[k][\'hyp\'][1:]))\n                logger.info(\'log prob (hyp): %.7f\' % merged_hyps[k][\'score\'])\n                logger.info(\'log prob (hyp, att): %.7f\' % (merged_hyps[k][\'score_att\'] * (1 - ctc_weight)))\n                if self.ctc_prefix_scorer is not None:\n                    logger.info(\'log prob (hyp, ctc): %.7f\' % (merged_hyps[k][\'score_ctc\'] * ctc_weight))\n                if lm is not None:\n                    logger.info(\'log prob (hyp, first-path lm): %.7f\' % (merged_hyps[k][\'score_lm\'] * lm_weight))\n                logger.info(\'-\' * 50)\n\n        aws = None\n\n        # Store ASR/LM state\n        if len(end_hyps) > 0:\n            self.dstates_final = end_hyps[0][\'dstates\']\n            self.lmstate_final = end_hyps[0][\'lmstate\']\n\n        self.n_frames += eouts_c.size(1)\n\n        return end_hyps, hyps, aws\n'"
neural_sp/models/seq2seq/decoders/rnn_transducer.py,15,"b'#! /usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n# Copyright 2019 Kyoto University (Hirofumi Inaguma)\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n\n""""""RNN transducer.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom collections import OrderedDict\nimport logging\nimport numpy as np\nimport random\nimport torch\nimport torch.nn as nn\n\nfrom neural_sp.models.lm.rnnlm import RNNLM\nfrom neural_sp.models.seq2seq.decoders.beam_search import BeamSearch\nfrom neural_sp.models.seq2seq.decoders.ctc import CTC\nfrom neural_sp.models.seq2seq.decoders.ctc import CTCPrefixScore\nfrom neural_sp.models.seq2seq.decoders.decoder_base import DecoderBase\nfrom neural_sp.models.torch_utils import np2tensor\nfrom neural_sp.models.torch_utils import pad_list\nfrom neural_sp.models.torch_utils import repeat\nfrom neural_sp.models.torch_utils import tensor2np\n\nrandom.seed(1)\n\nLOG_0 = float(np.finfo(np.float32).min)\nLOG_1 = 0\n\nlogger = logging.getLogger(__name__)\n\n\nclass RNNTransducer(DecoderBase):\n    """"""RNN transducer.\n\n    Args:\n        special_symbols (dict):\n            eos (int): index for <eos> (shared with <sos>)\n            unk (int): index for <unk>\n            pad (int): index for <pad>\n            blank (int): index for <blank>\n        enc_n_units (int):\n        rnn_type (str): lstm_transducer or gru_transducer\n        n_units (int): number of units in each RNN layer\n        n_projs (int): number of units in each projection layer\n        n_layers (int): number of RNN layers\n        bottleneck_dim (int): dimension of the bottleneck layer before the softmax layer for label generation\n        emb_dim (int): dimension of the embedding in target spaces.\n        vocab (int): number of nodes in softmax layer\n        dropout (float): dropout probability for the RNN layer\n        dropout_emb (float): dropout probability for the embedding layer\n        lsm_prob (float): label smoothing probability\n        ctc_weight (float):\n        ctc_lsm_prob (float): label smoothing probability for CTC\n        ctc_fc_list (list):\n        external_lm (RNNLM):\n        global_weight (float):\n        mtl_per_batch (bool):\n        param_init (str): parameter initialization method\n\n    """"""\n\n    def __init__(self, special_symbols,\n                 enc_n_units, rnn_type, n_units, n_projs, n_layers,\n                 bottleneck_dim,\n                 emb_dim,\n                 vocab,\n                 dropout=0.,\n                 dropout_emb=0.,\n                 lsm_prob=0.,\n                 ctc_weight=0.,\n                 ctc_lsm_prob=0.,\n                 ctc_fc_list=[],\n                 external_lm=None,\n                 global_weight=1.,\n                 mtl_per_batch=False,\n                 param_init=0.1):\n\n        super(RNNTransducer, self).__init__()\n\n        self.eos = special_symbols[\'eos\']\n        self.unk = special_symbols[\'unk\']\n        self.pad = special_symbols[\'pad\']\n        self.blank = special_symbols[\'blank\']\n        self.vocab = vocab\n        self.rnn_type = rnn_type\n        assert rnn_type in [\'lstm_transducer\', \'gru_transducer\']\n        self.enc_n_units = enc_n_units\n        self.dec_n_units = n_units\n        self.n_projs = n_projs\n        self.n_layers = n_layers\n        self.lsm_prob = lsm_prob\n        self.ctc_weight = ctc_weight\n        self.global_weight = global_weight\n        self.mtl_per_batch = mtl_per_batch\n\n        # for cache\n        self.prev_spk = \'\'\n        self.lmstate_final = None\n        self.state_cache = OrderedDict()\n\n        if ctc_weight > 0:\n            self.ctc = CTC(eos=self.eos,\n                           blank=self.blank,\n                           enc_n_units=enc_n_units,\n                           vocab=vocab,\n                           dropout=dropout,\n                           lsm_prob=ctc_lsm_prob,\n                           fc_list=ctc_fc_list,\n                           param_init=0.1)\n\n        if ctc_weight < global_weight:\n            # import warprnnt_pytorch\n            # self.warprnnt_loss = warprnnt_pytorch.RNNTLoss()\n\n            # Prediction network\n            rnn_l = nn.LSTM if rnn_type == \'lstm_transducer\' else nn.GRU\n            self.rnn = nn.ModuleList()\n            self.dropout = nn.Dropout(p=dropout)\n            if n_projs > 0:\n                self.proj = repeat(nn.Linear(n_units, n_projs), n_layers)\n            dec_idim = emb_dim\n            for l in range(n_layers):\n                self.rnn += [rnn_l(dec_idim, n_units, 1, batch_first=True)]\n                dec_idim = n_projs if n_projs > 0 else n_units\n\n            self.embed = nn.Embedding(vocab, emb_dim, padding_idx=self.pad)\n            self.dropout_emb = nn.Dropout(p=dropout_emb)\n\n            # Joint network\n            self.w_enc = nn.Linear(enc_n_units, bottleneck_dim)\n            self.w_dec = nn.Linear(dec_idim, bottleneck_dim, bias=False)\n            self.output = nn.Linear(bottleneck_dim, vocab)\n\n        self.reset_parameters(param_init)\n\n        # prediction network initialization with pre-trained LM\n        if external_lm is not None:\n            assert external_lm.vocab == vocab\n            assert external_lm.n_units == n_units\n            assert external_lm.n_projs == n_projs\n            assert external_lm.n_layers == n_layers\n            param_dict = dict(external_lm.named_parameters())\n            for n, p in self.named_parameters():\n                if n in param_dict.keys() and p.size() == param_dict[n].size():\n                    if \'output\' in n:\n                        continue\n                    p.data = param_dict[n].data\n                    logger.info(\'Overwrite %s\' % n)\n\n    @staticmethod\n    def add_args(parser, args):\n        """"""Add arguments.""""""\n        group = parser.add_argument_group(""RNN-T decoder"")\n        # common (LAS/RNN-T)\n        if not hasattr(args, \'dec_n_units\'):\n            group.add_argument(\'--dec_n_units\', type=int, default=512,\n                               help=\'number of units in each decoder RNN layer\')\n            group.add_argument(\'--dec_n_projs\', type=int, default=0,\n                               help=\'number of units in the projection layer after each decoder RNN layer\')\n            group.add_argument(\'--dec_bottleneck_dim\', type=int, default=1024,\n                               help=\'number of dimensions of the bottleneck layer before the softmax layer\')\n            group.add_argument(\'--emb_dim\', type=int, default=512,\n                               help=\'number of dimensions in the embedding layer\')\n        return parser\n\n    def reset_parameters(self, param_init):\n        """"""Initialize parameters with uniform distribution.""""""\n        logger.info(\'===== Initialize %s with uniform distribution =====\' % self.__class__.__name__)\n        for n, p in self.named_parameters():\n            if p.dim() == 1:\n                nn.init.constant_(p, 0.)  # bias\n                logger.info(\'Initialize %s with %s / %.3f\' % (n, \'constant\', 0.))\n            elif p.dim() in [2, 4]:\n                nn.init.uniform_(p, a=-param_init, b=param_init)\n                logger.info(\'Initialize %s with %s / %.3f\' % (n, \'uniform\', param_init))\n            else:\n                raise ValueError(n)\n\n    def start_scheduled_sampling(self):\n        self._ss_prob = 0.\n\n    def forward(self, eouts, elens, ys, task=\'all\',\n                teacher_logits=None, recog_params={}, idx2token=None):\n        """"""Forward computation.\n\n        Args:\n            eouts (FloatTensor): `[B, T, enc_n_units]`\n            elens (IntTensor): `[B]`\n            ys (list): length `B`, each of which contains a list of size `[L]`\n            task (str): all/ys*/ys_sub*\n            teacher_logits (FloatTensor): `[B, L, vocab]`\n            recog_params (dict): parameters for MBR training\n            idx2token ():\n        Returns:\n            loss (FloatTensor): `[1]`\n            observation (dict):\n\n        """"""\n        observation = {\'loss\': None, \'loss_transducer\': None, \'loss_ctc\': None, \'loss_mbr\': None}\n        loss = eouts.new_zeros((1,))\n\n        # CTC loss\n        if self.ctc_weight > 0 and (task == \'all\' or \'ctc\' in task):\n            loss_ctc, _ = self.ctc(eouts, elens, ys)\n            observation[\'loss_ctc\'] = loss_ctc.item()\n            if self.mtl_per_batch:\n                loss += loss_ctc\n            else:\n                loss += loss_ctc * self.ctc_weight\n\n        # XE loss\n        if self.global_weight - self.ctc_weight > 0 and (task == \'all\' or \'ctc\' not in task):\n            loss_transducer = self.forward_transducer(eouts, elens, ys)\n            observation[\'loss_transducer\'] = loss_transducer.item()\n            if self.mtl_per_batch:\n                loss += loss_transducer\n            else:\n                loss += loss_transducer * (self.global_weight - self.ctc_weight)\n\n        observation[\'loss\'] = loss.item()\n        return loss, observation\n\n    def forward_transducer(self, eouts, elens, ys):\n        """"""Compute RNN-T loss.\n\n        Args:\n            eouts (FloatTensor): `[B, T, enc_n_units]`\n            elens (IntTensor): `[B]`\n            ys (list): length `B`, each of which contains a list of size `[L]`\n        Returns:\n            loss (FloatTensor): `[1]`\n\n        """"""\n        # Append <sos> and <eos>\n        eos = eouts.new_zeros(1).fill_(self.eos).long()\n        _ys = [np2tensor(np.fromiter(y, dtype=np.int64), self.device_id) for y in ys]\n        ylens = np2tensor(np.fromiter([y.size(0) for y in _ys], dtype=np.int32))\n        ys_in = pad_list([torch.cat([eos, y], dim=0) for y in _ys], self.pad)\n        ys_out = pad_list(_ys, self.blank)\n\n        # Update prediction network\n        ys_emb = self.dropout_emb(self.embed(ys_in))\n        dout, _ = self.recurrency(ys_emb, None)\n\n        # Compute output distribution\n        logits = self.joint(eouts, dout)\n\n        # Compute Transducer loss\n        log_probs = torch.log_softmax(logits, dim=-1)\n        assert log_probs.size(2) == ys_out.size(1) + 1\n        if self.device_id >= 0:\n            ys_out = ys_out.cuda(self.device_id)\n            elens = elens.cuda(self.device_id)\n            ylens = ylens.cuda(self.device_id)\n            import warp_rnnt\n            loss = warp_rnnt.rnnt_loss(log_probs, ys_out.int(), elens, ylens,\n                                       average_frames=False,\n                                       reduction=\'mean\',\n                                       gather=False)\n        else:\n            import warprnnt_pytorch\n            self.warprnnt_loss = warprnnt_pytorch.RNNTLoss()\n            loss = self.warprnnt_loss(log_probs, ys_out.int(), elens, ylens)\n            # NOTE: Transducer loss has already been normalized by bs\n            # NOTE: index 0 is reserved for blank in warprnnt_pytorch\n\n        return loss\n\n    def joint(self, eouts, douts):\n        """"""Combine encoder outputs and prediction network outputs.\n\n        Args:\n            eouts (FloatTensor): `[B, T, enc_n_units]`\n            douts (FloatTensor): `[B, L, dec_n_units]`\n        Returns:\n            out (FloatTensor): `[B, T, L, vocab]`\n\n        """"""\n        eouts = eouts.unsqueeze(2)  # `[B, T, 1, enc_n_units]`\n        douts = douts.unsqueeze(1)  # `[B, 1, L, dec_n_units]`\n        out = torch.tanh(self.w_enc(eouts) + self.w_dec(douts))\n        out = self.output(out)\n        return out\n\n    def recurrency(self, ys_emb, dstate):\n        """"""Update prediction network.\n\n        Args:\n            ys_emb (FloatTensor): `[B, L, emb_dim]`\n            dstate (dict):\n                hxs (FloatTensor): `[n_layers, B, dec_n_units]`\n                cxs (FloatTensor): `[n_layers, B, dec_n_units]`\n        Returns:\n            dout (FloatTensor): `[B, L, emb_dim]`\n            new_dstate (dict):\n                hxs (FloatTensor): `[n_layers, B, dec_n_units]`\n                cxs (FloatTensor): `[n_layers, B, dec_n_units]`\n\n        """"""\n        if dstate is None:\n            dstate = self.zero_state(ys_emb.size(0))\n        new_dstate = {\'hxs\': None, \'cxs\': None}\n\n        new_hxs, new_cxs = [], []\n        for l in range(self.n_layers):\n            if self.rnn_type == \'lstm_transducer\':\n                ys_emb, (h, c) = self.rnn[l](ys_emb, hx=(dstate[\'hxs\'][l:l + 1],\n                                                         dstate[\'cxs\'][l:l + 1]))\n                new_cxs.append(c)\n            elif self.rnn_type == \'gru_transducer\':\n                ys_emb, h = self.rnn[l](ys_emb, hx=dstate[\'hxs\'][l:l + 1])\n            new_hxs.append(h)\n            ys_emb = self.dropout(ys_emb)\n            if self.n_projs > 0:\n                ys_emb = torch.tanh(self.proj[l](ys_emb))\n\n        # Repackage\n        new_dstate[\'hxs\'] = torch.cat(new_hxs, dim=0)\n        if self.rnn_type == \'lstm_transducer\':\n            new_dstate[\'cxs\'] = torch.cat(new_cxs, dim=0)\n\n        return ys_emb, new_dstate\n\n    def zero_state(self, batch_size):\n        """"""Initialize hidden states.\n\n        Args:\n            batch_size (int): batch size\n        Returns:\n            zero_state (dict):\n                hxs (FloatTensor): `[n_layers, B, dec_n_units]`\n                cxs (FloatTensor): `[n_layers, B, dec_n_units]`\n\n        """"""\n        w = next(self.parameters())\n        zero_state = {\'hxs\': None, \'cxs\': None}\n        zero_state[\'hxs\'] = w.new_zeros(self.n_layers, batch_size, self.dec_n_units)\n        if self.rnn_type == \'lstm_transducer\':\n            zero_state[\'cxs\'] = w.new_zeros(self.n_layers, batch_size, self.dec_n_units)\n        return zero_state\n\n    def greedy(self, eouts, elens, max_len_ratio, idx2token,\n               exclude_eos=False, refs_id=None, utt_ids=None, speakers=None):\n        """"""Greedy decoding.\n\n        Args:\n            eouts (FloatTensor): `[B, T, enc_units]`\n            elens (IntTensor): `[B]`\n            max_len_ratio (int): maximum sequence length of tokens\n            idx2token (): converter from index to token\n            exclude_eos (bool): exclude <eos> from hypothesis\n            refs_id (list): reference list\n            utt_ids (list): utterance id list\n            speakers (list): speaker list\n        Returns:\n            hyps (list): length `B`, each of which contains arrays of size `[L]`\n            aw: dummy\n\n        """"""\n        bs = eouts.size(0)\n\n        hyps = []\n        for b in range(bs):\n            hyp_b = []\n            # Initialization\n            y = eouts.new_zeros(1, 1).fill_(self.eos).long()\n            y_emb = self.dropout_emb(self.embed(y))\n            dout, dstate = self.recurrency(y_emb, None)\n\n            for t in range(elens[b]):\n                # Pick up 1-best per frame\n                out = self.joint(eouts[b:b + 1, t:t + 1], dout)\n                y = out.squeeze(2).argmax(-1)\n                idx = y[0].item()\n\n                # Update prediction network only when predicting non-blank labels\n                if idx != self.blank:\n                    hyp_b += [idx]\n                    y_emb = self.dropout_emb(self.embed(y))\n                    dout, dstate = self.recurrency(y_emb, dstate)\n\n            hyps += [hyp_b]\n\n        for b in range(bs):\n            if utt_ids is not None:\n                logger.debug(\'Utt-id: %s\' % utt_ids[b])\n            if refs_id is not None and self.vocab == idx2token.vocab:\n                logger.debug(\'Ref: %s\' % idx2token(refs_id[b]))\n            logger.debug(\'Hyp: %s\' % idx2token(hyps[b]))\n\n        return hyps, None\n\n    def beam_search(self, eouts, elens, params, idx2token=None,\n                    lm=None, lm_second=None, lm_second_bwd=None, ctc_log_probs=None,\n                    nbest=1, exclude_eos=False,\n                    refs_id=None, utt_ids=None, speakers=None,\n                    ensmbl_eouts=None, ensmbl_elens=None, ensmbl_decs=[]):\n        """"""Beam search decoding.\n\n        Args:\n            eouts (FloatTensor): `[B, T, enc_n_units]`\n            elens (IntTensor): `[B]`\n            params (dict): hyperparameters for decoding\n            idx2token (): converter from index to token\n            lm: firsh path LM\n            lm_second: second path LM\n            lm_second_bwd: secoding path backward LM\n            ctc_log_probs (FloatTensor):\n            nbest (int):\n            exclude_eos (bool): exclude <eos> from hypothesis\n            refs_id (list): reference list\n            utt_ids (list): utterance id list\n            speakers (list): speaker list\n            ensmbl_eouts (list): list of FloatTensor\n            ensmbl_elens (list) list of list\n            ensmbl_decs (list): list of torch.nn.Module\n        Returns:\n            nbest_hyps_idx (list): length `B`, each of which contains list of N hypotheses\n            aws: dummy\n            scores: dummy\n\n        """"""\n        bs = eouts.size(0)\n\n        beam_width = params[\'recog_beam_width\']\n        ctc_weight = params[\'recog_ctc_weight\']\n        lm_weight = params[\'recog_lm_weight\']\n        lm_weight_second = params[\'recog_lm_second_weight\']\n        lm_weight_second_bwd = params[\'recog_lm_bwd_weight\']\n        asr_state_carry_over = params[\'recog_asr_state_carry_over\']\n        lm_state_carry_over = params[\'recog_lm_state_carry_over\']\n\n        if lm is not None:\n            assert lm_weight > 0\n            lm.eval()\n        if lm_second is not None:\n            assert lm_weight_second > 0\n            lm_second.eval()\n        if lm_second_bwd is not None:\n            assert lm_weight_second_bwd > 0\n            lm_second_bwd.eval()\n\n        if ctc_log_probs is not None:\n            assert ctc_weight > 0\n            ctc_log_probs = tensor2np(ctc_log_probs)\n\n        nbest_hyps_idx = []\n        eos_flags = []\n        for b in range(bs):\n            # Initialization per utterance\n            y = eouts.new_zeros(bs, 1).fill_(self.eos).long()\n            y_emb = self.dropout_emb(self.embed(y))\n            dout, dstate = self.recurrency(y_emb, None)\n            lmstate = None\n\n            # For joint CTC-Attention decoding\n            ctc_prefix_scorer = None\n            if ctc_log_probs is not None:\n                ctc_prefix_scorer = CTCPrefixScore(ctc_log_probs[b], self.blank, self.eos)\n\n            if speakers is not None:\n                if speakers[b] == self.prev_spk:\n                    if lm_state_carry_over and isinstance(lm, RNNLM):\n                        lmstate = self.lmstate_final\n                self.prev_spk = speakers[b]\n\n            helper = BeamSearch(beam_width, self.eos, ctc_weight, self.device_id)\n\n            end_hyps = []\n            hyps = [{\'hyp\': [self.eos],\n                     \'ref_id\': [self.eos],\n                     \'score\': 0.,\n                     \'score_rnnt\': 0.,\n                     \'score_lm\': 0.,\n                     \'score_ctc\': 0.,\n                     \'dout\': dout,\n                     \'dstate\': dstate,\n                     \'lmstate\': lmstate,\n                     \'ctc_state\': ctc_prefix_scorer.initial_state() if ctc_prefix_scorer is not None else None}]\n            for t in range(elens[b]):\n                # preprocess for batch decoding\n                douts = torch.cat([beam[\'dout\'] for beam in hyps], dim=0)\n                outs = self.joint(eouts[b:b + 1, t:t + 1].repeat([douts.size(0), 1, 1]), douts)\n                scores_rnnt = torch.log_softmax(outs.squeeze(2).squeeze(1), dim=-1)\n\n                # Update LM states for shallow fusion\n                y = eouts.new_zeros(len(hyps), 1).long()\n                for j, beam in enumerate(hyps):\n                    y[j, 0] = beam[\'hyp\'][-1]\n                lmstate, scores_lm = None, None\n                if lm is not None:\n                    if hyps[0][\'lmstate\'] is not None:\n                        lm_hxs = torch.cat([beam[\'lmstate\'][\'hxs\'] for beam in hyps], dim=1)\n                        lm_cxs = torch.cat([beam[\'lmstate\'][\'cxs\'] for beam in hyps], dim=1)\n                        lmstate = {\'hxs\': lm_hxs, \'cxs\': lm_cxs}\n                    lmout, lmstate, scores_lm = lm.predict(y, lmstate)\n\n                new_hyps = []\n                for j, beam in enumerate(hyps):\n                    dout = douts[j:j + 1]\n                    dstate = beam[\'dstate\']\n                    lmstate = beam[\'lmstate\']\n\n                    # Attention scores\n                    total_scores_rnnt = beam[\'score_rnnt\'] + scores_rnnt[j:j + 1]\n                    total_scores = total_scores_rnnt * (1 - ctc_weight)\n\n                    # Add LM score <after> top-K selection\n                    total_scores_topk, topk_ids = torch.topk(\n                        total_scores, k=beam_width, dim=-1, largest=True, sorted=True)\n                    if lm is not None:\n                        total_scores_lm = beam[\'score_lm\'] + scores_lm[j, -1, topk_ids[0]]\n                        total_scores_topk += total_scores_lm * lm_weight\n                    else:\n                        total_scores_lm = eouts.new_zeros(beam_width)\n\n                    # Add CTC score\n                    new_ctc_states, total_scores_ctc, total_scores_topk = helper.add_ctc_score(\n                        beam[\'hyp\'], topk_ids, beam[\'ctc_state\'],\n                        total_scores_topk, ctc_prefix_scorer)\n\n                    for k in range(beam_width):\n                        idx = topk_ids[0, k].item()\n\n                        if idx == self.blank:\n                            beam[\'score\'] = total_scores_topk[0, k].item()\n                            beam[\'score_rnnt\'] = total_scores_topk[0, k].item()\n                            new_hyps.append(beam.copy())\n                            continue\n\n                        # skip blank-dominant frames\n                        # if total_scores_topk[0, self.blank].item() > 0.7:\n                        #     continue\n\n                        # Update prediction network only when predicting non-blank labels\n                        hyp_id = beam[\'hyp\'] + [idx]\n                        hyp_str = \' \'.join(list(map(str, hyp_id)))\n                        # if hyp_str in self.state_cache.keys():\n                        #     # from cache\n                        #     dout = self.state_cache[hyp_str][\'dout\']\n                        #     new_dstate = self.state_cache[hyp_str][\'dstate\']\n                        #     lmstate = self.state_cache[hyp_str][\'lmstate\']\n                        # else:\n                        y = eouts.new_zeros(1, 1).fill_(idx).long()\n                        y_emb = self.dropout_emb(self.embed(y))\n                        dout, new_dstate = self.recurrency(y_emb, dstate)\n\n                        # store in cache\n                        self.state_cache[hyp_str] = {\n                            \'dout\': dout,\n                            \'dstate\': new_dstate,\n                            \'lmstate\': {\'hxs\': lmstate[\'hxs\'][:, j:j + 1],\n                                        \'cxs\': lmstate[\'cxs\'][:, j:j + 1]} if lmstate is not None else None,\n                        }\n\n                        new_hyps.append({\'hyp\': hyp_id,\n                                         \'score\': total_scores_topk[0, k].item(),\n                                         \'score_rnnt\': total_scores_rnnt[0, idx].item(),\n                                         \'score_ctc\': total_scores_ctc[k].item(),\n                                         \'score_lm\': total_scores_lm[k].item(),\n                                         \'dout\': dout,\n                                         \'dstate\': new_dstate,\n                                         \'lmstate\': {\'hxs\': lmstate[\'hxs\'][:, j:j + 1],\n                                                     \'cxs\': lmstate[\'cxs\'][:, j:j + 1]} if lmstate is not None else None,\n                                         \'ctc_state\': new_ctc_states[k] if ctc_prefix_scorer is not None else None})\n\n                # Merge hypotheses having the same token sequences\n                new_hyps_merged = {}\n                for beam in new_hyps:\n                    hyp_str = \' \'.join(list(map(str, beam[\'hyp\'])))\n                    if hyp_str not in new_hyps_merged.keys():\n                        new_hyps_merged[hyp_str] = beam\n                    elif hyp_str in new_hyps_merged.keys():\n                        if beam[\'score\'] > new_hyps_merged[hyp_str][\'score\']:\n                            new_hyps_merged[hyp_str] = beam\n                new_hyps = [v for v in new_hyps_merged.values()]\n\n                # Local pruning\n                new_hyps_tmp = sorted(new_hyps, key=lambda x: x[\'score\'], reverse=True)[:beam_width]\n\n                # Remove complete hypotheses\n                new_hyps = []\n                for hyp in new_hyps_tmp:\n                    new_hyps += [hyp]\n                if len(end_hyps) >= beam_width:\n                    end_hyps = end_hyps[:beam_width]\n                    break\n                hyps = new_hyps[:]\n\n            # Global pruning\n            if len(end_hyps) == 0:\n                end_hyps = hyps[:]\n            elif len(end_hyps) < nbest and nbest > 1:\n                end_hyps.extend(hyps[:nbest - len(end_hyps)])\n\n            # forward second path LM rescoring\n            if lm_second is not None:\n                self.lm_rescoring(end_hyps, lm_second, lm_weight_second, tag=\'second\')\n\n            # backward secodn path LM rescoring\n            if lm_second_bwd is not None:\n                self.lm_rescoring(end_hyps, lm_second_bwd, lm_weight_second_bwd, tag=\'second_rev\')\n\n            end_hyps = sorted(end_hyps, key=lambda x: x[\'score\'], reverse=True)\n\n            # Reset state cache\n            self.state_cache = OrderedDict()\n\n            if idx2token is not None:\n                if utt_ids is not None:\n                    logger.info(\'Utt-id: %s\' % utt_ids[b])\n                logger.info(\'=\' * 200)\n                for k in range(len(end_hyps)):\n                    if refs_id is not None and self.vocab == idx2token.vocab:\n                        logger.info(\'Ref: %s\' % idx2token(refs_id[b]))\n                    logger.info(\'Hyp: %s\' % idx2token(end_hyps[k][\'hyp\'][1:]))\n                    logger.info(\'log prob (hyp): %.7f\' % end_hyps[k][\'score\'])\n                    if ctc_log_probs is not None:\n                        logger.info(\'log prob (hyp, ctc): %.7f\' % (end_hyps[k][\'score_ctc\']))\n                    if lm is not None:\n                        logger.info(\'log prob (hyp, lm): %.7f\' % (end_hyps[k][\'score_lm\']))\n                    logger.info(\'-\' * 50)\n\n            # N-best list\n            nbest_hyps_idx += [[np.array(end_hyps[n][\'hyp\'][1:]) for n in range(nbest)]]\n\n            # Check <eos>\n            eos_flags.append([(end_hyps[n][\'hyp\'][-1] == self.eos) for n in range(nbest)])\n\n        return nbest_hyps_idx, None, None\n'"
neural_sp/models/seq2seq/decoders/transformer.py,33,"b'#! /usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n# Copyright 2019 Kyoto University (Hirofumi Inaguma)\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n\n""""""Transformer decoder (including CTC loss calculation).""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport copy\nfrom distutils.util import strtobool\nimport logging\nimport math\nimport numpy as np\nimport os\nimport random\nimport shutil\nimport torch\nimport torch.nn as nn\n\nfrom neural_sp.models.criterion import cross_entropy_lsm\nfrom neural_sp.models.lm.rnnlm import RNNLM\nfrom neural_sp.models.modules.initialization import init_like_transformer_xl\nfrom neural_sp.models.modules.positional_embedding import PositionalEncoding\nfrom neural_sp.models.modules.positional_embedding import XLPositionalEmbedding\nfrom neural_sp.models.modules.transformer import TransformerDecoderBlock\nfrom neural_sp.models.seq2seq.decoders.beam_search import BeamSearch\nfrom neural_sp.models.seq2seq.decoders.ctc import CTC\nfrom neural_sp.models.seq2seq.decoders.ctc import CTCPrefixScore\nfrom neural_sp.models.seq2seq.decoders.decoder_base import DecoderBase\nfrom neural_sp.models.torch_utils import append_sos_eos\nfrom neural_sp.models.torch_utils import compute_accuracy\nfrom neural_sp.models.torch_utils import make_pad_mask\nfrom neural_sp.models.torch_utils import tensor2np\nfrom neural_sp.utils import mkdir_join\n\nimport matplotlib\nmatplotlib.use(\'Agg\')\n\nrandom.seed(1)\n\nlogger = logging.getLogger(__name__)\n\n\nclass TransformerDecoder(DecoderBase):\n    """"""Transformer decoder.\n\n    Args:\n        special_symbols (dict):\n            eos (int): index for <eos> (shared with <sos>)\n            unk (int): index for <unk>\n            pad (int): index for <pad>\n            blank (int): index for <blank>\n        enc_n_units (int): number of units of the encoder outputs\n        attn_type (str): type of attention mechanism\n        n_heads (int): number of attention heads\n        n_layers (int): number of self-attention layers\n        d_model (int): dimension of MultiheadAttentionMechanism\n        d_ff (int): dimension of PositionwiseFeedForward\n        d_ff_bottleneck_dim (int): bottleneck dimension for the light-weight FFN layer\n        pe_type (str): type of positional encoding\n        layer_norm_eps (float): epsilon value for layer normalization\n        ffn_activation (str): nonolinear function for PositionwiseFeedForward\n        vocab (int): number of nodes in softmax layer\n        tie_embedding (bool): tie parameters of the embedding and output layers\n        dropout (float): dropout probability for linear layers\n        dropout_emb (float): dropout probability for the embedding layer\n        dropout_att (float): dropout probability for attention distributions\n        dropout_layer (float): LayerDrop probability for layers\n        dropout_head (float): HeadDrop probability for attention heads\n        lsm_prob (float): label smoothing probability\n        ctc_weight (float):\n        ctc_lsm_prob (float): label smoothing probability for CTC\n        ctc_fc_list (list):\n        backward (bool): decode in the backward order\n        global_weight (float):\n        mtl_per_batch (bool):\n        param_init (str): parameter initialization method\n        memory_transformer (bool): TransformerXL decoder\n        mem_len (int):\n        mocha_chunk_size (int):\n        mocha_n_heads_mono (int):\n        mocha_n_heads_chunk (int):\n        mocha_init_r (int):\n        mocha_eps (float):\n        mocha_std (float):\n        mocha_no_denominator (bool):\n        mocha_1dconv (bool): 1dconv for MoChA\n        mocha_quantity_loss_weight (float):\n        mocha_head_divergence_loss_weight (float):\n        latency_metric (str):\n        latency_loss_weight (float):\n        mocha_first_layer (int):\n        share_chunkwise_attention (bool):\n        external_lm (RNNLM):\n        lm_fusion (str):\n\n    """"""\n\n    def __init__(self, special_symbols,\n                 enc_n_units, attn_type, n_heads, n_layers,\n                 d_model, d_ff, d_ff_bottleneck_dim,\n                 pe_type, layer_norm_eps, ffn_activation,\n                 vocab, tie_embedding,\n                 dropout, dropout_emb, dropout_att, dropout_layer, dropout_head,\n                 lsm_prob, ctc_weight, ctc_lsm_prob, ctc_fc_list, backward,\n                 global_weight, mtl_per_batch, param_init,\n                 memory_transformer, mem_len,\n                 mocha_chunk_size, mocha_n_heads_mono, mocha_n_heads_chunk,\n                 mocha_init_r, mocha_eps, mocha_std,\n                 mocha_no_denominator, mocha_1dconv,\n                 mocha_quantity_loss_weight, mocha_head_divergence_loss_weight,\n                 latency_metric, latency_loss_weight,\n                 mocha_first_layer, share_chunkwise_attention,\n                 external_lm, lm_fusion):\n\n        super(TransformerDecoder, self).__init__()\n\n        self.eos = special_symbols[\'eos\']\n        self.unk = special_symbols[\'unk\']\n        self.pad = special_symbols[\'pad\']\n        self.blank = special_symbols[\'blank\']\n        self.vocab = vocab\n        self.enc_n_units = enc_n_units\n        self.d_model = d_model\n        self.n_layers = n_layers\n        self.n_heads = n_heads\n        self.pe_type = pe_type\n        self.lsm_prob = lsm_prob\n        self.ctc_weight = ctc_weight\n        self.bwd = backward\n        self.global_weight = global_weight\n        self.mtl_per_batch = mtl_per_batch\n\n        self.prev_spk = \'\'\n        self.lmstate_final = None\n\n        # for TransformerXL decoder\n        self.memory_transformer = memory_transformer\n        self.mem_len = mem_len\n        if memory_transformer:\n            assert pe_type == \'none\'\n\n        # for attention plot\n        self.aws_dict = {}\n        self.data_dict = {}\n\n        # for MMA\n        self.attn_type = attn_type\n        self.quantity_loss_weight = mocha_quantity_loss_weight\n        self._quantity_loss_weight = 0  # for curriculum\n        self.mocha_first_layer = mocha_first_layer\n\n        self.headdiv_loss_weight = mocha_head_divergence_loss_weight\n        self.latency_metric = latency_metric\n        self.latency_loss_weight = latency_loss_weight\n        self.ctc_trigger = (self.latency_metric in [\'ctc_sync\'])\n        if self.ctc_trigger:\n            assert 0 < self.ctc_weight < 1\n\n        if ctc_weight > 0:\n            self.ctc = CTC(eos=self.eos,\n                           blank=self.blank,\n                           enc_n_units=enc_n_units,\n                           vocab=self.vocab,\n                           dropout=dropout,\n                           lsm_prob=ctc_lsm_prob,\n                           fc_list=ctc_fc_list,\n                           param_init=0.1,\n                           backward=backward)\n\n        if ctc_weight < global_weight:\n            # token embedding\n            self.embed = nn.Embedding(self.vocab, d_model, padding_idx=self.pad)\n            self.pos_enc = PositionalEncoding(d_model, dropout_emb, pe_type, param_init)\n            # positional embedding\n            self.u = None\n            self.v = None\n            if memory_transformer:\n                self.scale = math.sqrt(d_model)  # for token embedding\n                self.dropout_emb = nn.Dropout(p=dropout_emb)  # for token embedding\n                self.pos_emb = XLPositionalEmbedding(d_model, dropout_emb)\n                if self.mem_len > 0:\n                    self.u = nn.Parameter(torch.Tensor(self.n_heads, self.d_model // self.n_heads))\n                    self.v = nn.Parameter(torch.Tensor(self.n_heads, self.d_model // self.n_heads))\n                    # NOTE: u and v are global parameters\n            # self-attention\n            self.layers = nn.ModuleList([copy.deepcopy(TransformerDecoderBlock(\n                d_model, d_ff, attn_type, n_heads, dropout, dropout_att, dropout_layer,\n                layer_norm_eps, ffn_activation, param_init,\n                src_tgt_attention=False if lth < mocha_first_layer - 1 else True,\n                memory_transformer=memory_transformer,\n                mocha_chunk_size=mocha_chunk_size,\n                mocha_n_heads_mono=mocha_n_heads_mono,\n                mocha_n_heads_chunk=mocha_n_heads_chunk,\n                mocha_init_r=mocha_init_r,\n                mocha_eps=mocha_eps,\n                mocha_std=mocha_std,\n                mocha_no_denominator=mocha_no_denominator,\n                mocha_1dconv=mocha_1dconv,\n                dropout_head=dropout_head,\n                lm_fusion=lm_fusion,\n                d_ff_bottleneck_dim=d_ff_bottleneck_dim,\n                share_chunkwise_attention=share_chunkwise_attention)) for lth in range(n_layers)])\n            self.norm_out = nn.LayerNorm(d_model, eps=layer_norm_eps)\n            self.output = nn.Linear(d_model, self.vocab)\n            if tie_embedding:\n                self.output.weight = self.embed.weight\n\n            self.lm = external_lm\n            if external_lm is not None:\n                self.lm_output_proj = nn.Linear(external_lm.output_dim, d_model)\n\n            self.reset_parameters(param_init)\n\n    @staticmethod\n    def add_args(parser, args):\n        """"""Add arguments.""""""\n        group = parser.add_argument_group(""Transformer decoder"")\n        # Transformer common\n        if not hasattr(args, \'transformer_d_model\'):\n            group.add_argument(\'--transformer_d_model\', type=int, default=256,\n                               help=\'number of units in the MHA layer\')\n        if not hasattr(args, \'transformer_d_ff\'):\n            group.add_argument(\'--transformer_d_ff\', type=int, default=2048,\n                               help=\'number of units in the FFN layer\')\n        if not hasattr(args, \'transformer_d_ff_bottleneck_dim\'):\n            group.add_argument(\'--transformer_d_ff_bottleneck_dim\', type=int, default=0,\n                               help=\'bottleneck dimension in the FFN layer\')\n        if not hasattr(args, \'transformer_n_heads\'):\n            group.add_argument(\'--transformer_n_heads\', type=int, default=4,\n                               help=\'number of heads in the MHA layer\')\n        if not hasattr(args, \'transformer_layer_norm_eps\'):\n            group.add_argument(\'--transformer_layer_norm_eps\', type=float, default=1e-12,\n                               help=\'epsilon value for layer normalization\')\n        if not hasattr(args, \'transformer_ffn_activation\'):\n            group.add_argument(\'--transformer_ffn_activation\', type=str, default=\'relu\',\n                               choices=[\'relu\', \'gelu\', \'gelu_accurate\', \'glu\', \'swish\'],\n                               help=\'nonlinear activation for the FFN layer\')\n        if not hasattr(args, \'transformer_param_init\'):\n            group.add_argument(\'--transformer_param_init\', type=str, default=\'xavier_uniform\',\n                               choices=[\'xavier_uniform\', \'pytorch\'],\n                               help=\'parameter initializatin\')\n        # Transformer decoder specific\n        group.add_argument(\'--transformer_attn_type\', type=str, default=\'scaled_dot\',\n                           choices=[\'scaled_dot\', \'add\', \'average\',\n                                    \'mocha\', \'sync_bidir\', \'sync_bidir_half\'],\n                           help=\'type of attention mechasnism for Transformer decoder\')\n        group.add_argument(\'--transformer_dec_pe_type\', type=str, default=\'add\',\n                           choices=[\'add\', \'concat\', \'none\', \'1dconv3L\'],\n                           help=\'type of positional encoding for the Transformer decoder\')\n        group.add_argument(\'--dropout_dec_layer\', type=float, default=0.0,\n                           help=\'LayerDrop probability for Transformer decoder layers\')\n        group.add_argument(\'--dropout_head\', type=float, default=0.0,\n                           help=\'HeadDrop probability for masking out a head in the Transformer decoder\')\n        # streaming\n        group.add_argument(\'--mocha_first_layer\', type=int, default=1,\n                           help=\'the initial layer to have a MMA function\')\n        group.add_argument(\'--mocha_head_divergence_loss_weight\', type=float, default=0.0,\n                           help=\'head divergence loss weight for MMA\')\n        group.add_argument(\'--share_chunkwise_attention\', type=strtobool, default=False,\n                           help=\'share chunkwise attention heads among monotonic attention heads in the same layer\')\n\n        return parser\n\n    def reset_parameters(self, param_init):\n        """"""Initialize parameters.""""""\n        if self.memory_transformer:\n            logger.info(\'===== Initialize %s with normal distribution =====\' % self.__class__.__name__)\n            for n, p in self.named_parameters():\n                if \'conv\' in n:\n                    continue\n                init_like_transformer_xl(n, p, std=0.02)\n\n        elif param_init == \'xavier_uniform\':\n            logger.info(\'===== Initialize %s with Xavier uniform distribution =====\' % self.__class__.__name__)\n            # see https://github.com/pytorch/fairseq/blob/master/fairseq/models/transformer.py\n            # embedding\n            nn.init.normal_(self.embed.weight, mean=0., std=self.d_model**-0.5)\n            nn.init.constant_(self.embed.weight[self.pad], 0.)\n            # output layer\n            nn.init.xavier_uniform_(self.output.weight)\n            # nn.init.normal_(self.output.weight, mean=0., std=self.d_model**-0.5)\n            nn.init.constant_(self.output.bias, 0.)\n\n    def init_memory(self):\n        """"""Initialize memory.""""""\n        if self.device_id >= 0:\n            return [torch.empty(0, dtype=torch.float).cuda(self.device_id)\n                    for _ in range(self.n_layers)]\n        else:\n            return [torch.empty(0, dtype=torch.float)\n                    for _ in range(self.n_layers)]\n\n    def update_memory(self, memory_prev, hidden_states):\n        """"""Update memory.\n\n        Args:\n            memory_prev (list): length `n_layers`, each of which contains [B, L_prev, d_model]`\n            hidden_states (list): length `n_layers`, each of which contains [B, L, d_model]`\n        Returns:\n            new_mems (list): length `n_layers`, each of which contains `[B, mlen, d_model]`\n\n        """"""\n        # if memory_prev is None:\n        #     memory_prev = self.init_memory()  # 0-th to L-1-th layer\n        assert len(hidden_states) == len(memory_prev)\n        mlen = memory_prev[0].size(1) if memory_prev[0].dim() > 1 else 0\n        qlen = hidden_states[0].size(1)\n\n        # There are `mlen + qlen` steps that can be cached into mems\n        # For the next step, the last `ext_len` of the `qlen` tokens\n        # will be used as the extended context. Hence, we only cache\n        # the tokens from `mlen + qlen - self.ext_len - self.mem_len`\n        # to `mlen + qlen - self.ext_len`.\n        with torch.no_grad():\n            new_mems = []\n            end_idx = mlen + qlen\n            start_idx = max(0, end_idx - self.mem_len)\n            for m, h in zip(memory_prev, hidden_states):\n                cat = torch.cat([m, h], dim=1)  # `[B, mlen + qlen, d_model]`\n                new_mems.append(cat[:, start_idx:end_idx].detach())  # `[B, self.mem_len, d_model]`\n\n        return new_mems\n\n    def forward(self, eouts, elens, ys, task=\'all\',\n                teacher_logits=None, recog_params={}, idx2token=None):\n        """"""Forward computation.\n\n        Args:\n            eouts (FloatTensor): `[B, T, d_model]`\n            elens (IntTensor): `[B]`\n            ys (list): length `B`, each of which contains a list of size `[L]`\n            task (str): all/ys*/ys_sub*\n            teacher_logits (FloatTensor): `[B, L, vocab]`\n            recog_params (dict): parameters for MBR training\n            idx2token ():\n        Returns:\n            loss (FloatTensor): `[1]`\n            observation (dict):\n\n        """"""\n        observation = {\'loss\': None, \'loss_att\': None, \'loss_ctc\': None, \'loss_mbr\': None,\n                       \'acc_att\': None, \'ppl_att\': None}\n        loss = eouts.new_zeros((1,))\n\n        # CTC loss\n        trigger_points = None\n        if self.ctc_weight > 0 and (task == \'all\' or \'ctc\' in task):\n            forced_align = (self.ctc_trigger and self.training) or self.attn_type == \'triggered_attention\'\n            loss_ctc, trigger_points = self.ctc(eouts, elens, ys, forced_align=forced_align)\n            observation[\'loss_ctc\'] = loss_ctc.item()\n            if self.mtl_per_batch:\n                loss += loss_ctc\n            else:\n                loss += loss_ctc * self.ctc_weight\n\n        # XE loss\n        if self.global_weight - self.ctc_weight > 0 and (task == \'all\' or \'ctc\' not in task):\n            loss_att, acc_att, ppl_att, losses_auxiliary = self.forward_att(\n                eouts, elens, ys, trigger_points=trigger_points)\n            observation[\'loss_att\'] = loss_att.item()\n            observation[\'acc_att\'] = acc_att\n            observation[\'ppl_att\'] = ppl_att\n            if \'mocha\' in self.attn_type:\n                if self._quantity_loss_weight > 0:\n                    loss_att += losses_auxiliary[\'loss_quantity\'] * self._quantity_loss_weight\n                observation[\'loss_quantity\'] = losses_auxiliary[\'loss_quantity\'].item()\n            if self.headdiv_loss_weight > 0 or trigger_points is not None:\n                loss_att += losses_auxiliary[\'loss_headdiv\'] * self.headdiv_loss_weight\n                observation[\'loss_headdiv\'] = losses_auxiliary[\'loss_headdiv\'].item()\n            if self.latency_metric:\n                observation[\'loss_latency\'] = losses_auxiliary[\'loss_latency\'].item() if self.training else 0\n                if self.latency_metric != \'decot\' and self.latency_loss_weight > 0:\n                    loss_att += losses_auxiliary[\'loss_latency\'] * self.latency_loss_weight\n            if self.mtl_per_batch:\n                loss += loss_att\n            else:\n                loss += loss_att * (self.global_weight - self.ctc_weight)\n\n        observation[\'loss\'] = loss.item()\n        return loss, observation\n\n    def forward_att(self, eouts, elens, ys,\n                    return_logits=False, teacher_logits=None, trigger_points=None):\n        """"""Compute XE loss for the Transformer decoder.\n\n        Args:\n            eouts (FloatTensor): `[B, T, d_model]`\n            elens (IntTensor): `[B]`\n            ys (list): length `B`, each of which contains a list of size `[L]`\n            return_logits (bool): return logits for knowledge distillation\n            teacher_logits (FloatTensor): `[B, L, vocab]`\n            trigger_points (IntTensor): `[B, T]`\n        Returns:\n            loss (FloatTensor): `[1]`\n            acc (float): accuracy for token prediction\n            ppl (float): perplexity\n            loss_quantity (FloatTensor): `[1]`\n            loss_headdiv (FloatTensor): `[1]`\n            loss_latency (FloatTensor): `[1]`\n\n        """"""\n        # Append <sos> and <eos>\n        ys_in, ys_out, ylens = append_sos_eos(eouts, ys, self.eos, self.eos, self.pad, self.bwd)\n        if not self.training:\n            self.data_dict[\'elens\'] = tensor2np(elens)\n            self.data_dict[\'ylens\'] = tensor2np(ylens)\n            self.data_dict[\'ys\'] = tensor2np(ys_out)\n\n        # Create target self-attention mask\n        xmax = eouts.size(1)\n        bs, ymax = ys_in.size()[:2]\n        mlen = 0\n        tgt_mask = (ys_out != self.pad).unsqueeze(1).repeat([1, ymax, 1])\n        causal_mask = tgt_mask.new_ones(ymax, ymax).byte()\n        causal_mask = torch.tril(causal_mask, diagonal=0 + mlen, out=causal_mask).unsqueeze(0)\n        tgt_mask = tgt_mask & causal_mask  # `[B, L (query), L (key)]`\n\n        # Create source-target mask\n        src_mask = make_pad_mask(elens, self.device_id).unsqueeze(1).repeat([1, ymax, 1])  # `[B, L, T]`\n\n        # external LM integration\n        lmout = None\n        if self.lm is not None:\n            self.lm.eval()\n            with torch.no_grad():\n                lmout, lmstate, _ = self.lm.predict(ys_in, None)\n            lmout = self.lm_output_proj(lmout)\n\n        out = self.pos_enc(self.embed(ys_in))  # scaled\n\n        mems = self.init_memory()\n        pos_embs = None\n        if self.memory_transformer:\n            out = self.dropout_emb(out)\n            # NOTE: TransformerXL does not use positional encoding in the token embedding\n            # adopt zero-centered offset\n            pos_idxs = torch.arange(mlen - 1, -ymax - 1, -1.0, dtype=torch.float)\n            pos_embs = self.pos_emb(pos_idxs, self.device_id)\n\n        hidden_states = [out]\n        xy_aws_layers = []\n        for lth, (mem, layer) in enumerate(zip(mems, self.layers)):\n            out, yy_aws, xy_aws, xy_aws_beta, yy_aws_lm = layer(\n                out, tgt_mask, eouts, src_mask, mode=\'parallel\', lmout=lmout,\n                pos_embs=pos_embs, memory=mem, u=self.u, v=self.v)\n            if lth < self.n_layers - 1:\n                hidden_states.append(out)\n                # NOTE: outputs from the last layer is not used for momory\n            # Attention padding\n            if xy_aws is not None and \'mocha\' in self.attn_type:\n                tgt_mask_v2 = (ys_out != self.pad).unsqueeze(1).unsqueeze(3)  # `[B, 1, L, 1]`\n                xy_aws = xy_aws.masked_fill_(tgt_mask_v2.repeat([1, xy_aws.size(1), 1, xmax]) == 0, 0)\n                # NOTE: attention padding is quite effective for quantity loss\n                xy_aws_layers.append(xy_aws.clone())\n            if not self.training:\n                if yy_aws is not None:\n                    self.aws_dict[\'yy_aws_layer%d\' % lth] = tensor2np(yy_aws)\n                if xy_aws is not None:\n                    self.aws_dict[\'xy_aws_layer%d\' % lth] = tensor2np(xy_aws)\n                if xy_aws_beta is not None:\n                    self.aws_dict[\'xy_aws_beta_layer%d\' % lth] = tensor2np(xy_aws_beta)\n                if yy_aws_lm is not None:\n                    self.aws_dict[\'yy_aws_lm_layer%d\' % lth] = tensor2np(yy_aws_lm)\n        logits = self.output(self.norm_out(out))\n\n        # for knowledge distillation\n        if return_logits:\n            return logits\n\n        # Compute XE loss (+ label smoothing)\n        loss, ppl = cross_entropy_lsm(logits, ys_out, self.lsm_prob, self.pad, self.training)\n        losses_auxiliary = {}\n\n        # Quantity loss\n        losses_auxiliary[\'loss_quantity\'] = 0.\n        if \'mocha\' in self.attn_type:\n            # Average over all heads across all layers\n            n_tokens_ref = tgt_mask[:, -1, :].sum(1).float()  # `[B]`\n            # NOTE: count <eos> tokens\n            n_tokens_pred = sum([torch.abs(aws.sum(3).sum(2).sum(1) / aws.size(1))\n                                 for aws in xy_aws_layers])  # `[B]`\n            n_tokens_pred /= len(xy_aws_layers)\n            losses_auxiliary[\'loss_quantity\'] = torch.mean(torch.abs(n_tokens_pred - n_tokens_ref))\n\n        # Compute token-level accuracy in teacher-forcing\n        acc = compute_accuracy(logits, ys_out, self.pad)\n\n        return loss, acc, ppl, losses_auxiliary\n\n    def _plot_attention(self, save_path, n_cols=2):\n        """"""Plot attention for each head in all layers.""""""\n        from matplotlib import pyplot as plt\n        from matplotlib.ticker import MaxNLocator\n\n        _save_path = mkdir_join(save_path, \'dec_att_weights\')\n\n        # Clean directory\n        if _save_path is not None and os.path.isdir(_save_path):\n            shutil.rmtree(_save_path)\n            os.mkdir(_save_path)\n\n        for k, aw in self.aws_dict.items():\n            elens = self.data_dict[\'elens\']\n            ylens = self.data_dict[\'ylens\']\n            # ys = self.data_dict[\'ys\']\n\n            plt.clf()\n            n_heads = aw.shape[1]\n            n_cols_tmp = 1 if n_heads == 1 else n_cols * max(1, n_heads // 4)\n            fig, axes = plt.subplots(max(1, n_heads // n_cols_tmp), n_cols_tmp,\n                                     figsize=(20 * max(1, n_heads // 4), 8), squeeze=False)\n            for h in range(n_heads):\n                ax = axes[h // n_cols_tmp, h % n_cols_tmp]\n                if \'xy\' in k:\n                    ax.imshow(aw[-1, h, :ylens[-1], :elens[-1]], aspect=""auto"")\n                else:\n                    ax.imshow(aw[-1, h, :ylens[-1], :ylens[-1]], aspect=""auto"")\n                ax.grid(False)\n                ax.set_xlabel(""Input (head%d)"" % h)\n                ax.set_ylabel(""Output (head%d)"" % h)\n                ax.xaxis.set_major_locator(MaxNLocator(integer=True))\n                ax.yaxis.set_major_locator(MaxNLocator(integer=True))\n                # ax.set_yticks(np.linspace(0, ylens[-1] - 1, ylens[-1]))\n                # ax.set_yticks(np.linspace(0, ylens[-1] - 1, 1), minor=True)\n                # ax.set_yticklabels(ys + [\'\'])\n\n            fig.tight_layout()\n            fig.savefig(os.path.join(_save_path, \'%s.png\' % k), dvi=500)\n            plt.close()\n\n    def greedy(self, eouts, elens, max_len_ratio, idx2token,\n               exclude_eos=False, refs_id=None, utt_ids=None, speakers=None,\n               cache_states=True):\n        """"""Greedy decoding.\n\n        Args:\n            eouts (FloatTensor): `[B, T, enc_units]`\n            elens (IntTensor): `[B]`\n            max_len_ratio (int): maximum sequence length of tokens\n            idx2token (): converter from index to token\n            exclude_eos (bool): exclude <eos> from hypothesis\n            refs_id (list): reference list\n            utt_ids (list): utterance id list\n            speakers (list): speaker list\n            cache_states (bool):\n        Returns:\n            hyps (list): length `B`, each of which contains arrays of size `[L]`\n            aw (list): length `B`, each of which contains arrays of size `[L, T]`\n\n        """"""\n        bs, xtime = eouts.size()[:2]\n        ys = eouts.new_zeros(bs, 1).fill_(self.eos).long()\n\n        cache = [None] * self.n_layers\n\n        hyps_batch = []\n        ylens = torch.zeros(bs).int()\n        eos_flags = [False] * bs\n        ymax = int(math.floor(xtime * max_len_ratio)) + 1\n        for t in range(ymax):\n            causal_mask = eouts.new_ones(t + 1, t + 1).byte()\n            causal_mask = torch.tril(causal_mask, out=causal_mask).unsqueeze(0)\n\n            new_cache = [None] * self.n_layers\n            out = self.pos_enc(self.embed(ys))  # scaled\n            for lth, layer in enumerate(self.layers):\n                out, _, xy_aws, _, _ = layer(out, causal_mask, eouts, None, cache=cache[lth])\n                new_cache[lth] = out\n\n            if cache_states:\n                cache = new_cache[:]\n\n            # Pick up 1-best\n            y = self.output(self.norm_out(out))[:, -1:].argmax(-1)\n            hyps_batch += [y]\n\n            # Count lengths of hypotheses\n            for b in range(bs):\n                if not eos_flags[b]:\n                    if y[b].item() == self.eos:\n                        eos_flags[b] = True\n                    ylens[b] += 1  # include <eos>\n\n            # Break if <eos> is outputed in all mini-batch\n            if sum(eos_flags) == bs:\n                break\n            if t == ymax - 1:\n                break\n\n            ys = torch.cat([ys, y], dim=-1)\n\n        # Concatenate in L dimension\n        hyps_batch = tensor2np(torch.cat(hyps_batch, dim=1))\n        xy_aws = tensor2np(xy_aws.transpose(1, 2).transpose(2, 3))\n\n        # Truncate by the first <eos> (<sos> in case of the backward decoder)\n        if self.bwd:\n            # Reverse the order\n            hyps = [hyps_batch[b, :ylens[b]][::-1] for b in range(bs)]\n            aws = [xy_aws[b, :, :ylens[b]][::-1] for b in range(bs)]\n        else:\n            hyps = [hyps_batch[b, :ylens[b]] for b in range(bs)]\n            aws = [xy_aws[b, :, :ylens[b]] for b in range(bs)]\n\n        # Exclude <eos> (<sos> in case of the backward decoder)\n        if exclude_eos:\n            if self.bwd:\n                hyps = [hyps[b][1:] if eos_flags[b] else hyps[b] for b in range(bs)]\n            else:\n                hyps = [hyps[b][:-1] if eos_flags[b] else hyps[b] for b in range(bs)]\n\n        for b in range(bs):\n            if utt_ids is not None:\n                logger.debug(\'Utt-id: %s\' % utt_ids[b])\n            if refs_id is not None and self.vocab == idx2token.vocab:\n                logger.debug(\'Ref: %s\' % idx2token(refs_id[b]))\n            if self.bwd:\n                logger.debug(\'Hyp: %s\' % idx2token(hyps[b][::-1]))\n            else:\n                logger.debug(\'Hyp: %s\' % idx2token(hyps[b]))\n\n        return hyps, aws\n\n    def beam_search(self, eouts, elens, params, idx2token=None,\n                    lm=None, lm_second=None, lm_bwd=None, ctc_log_probs=None,\n                    nbest=1, exclude_eos=False,\n                    refs_id=None, utt_ids=None, speakers=None,\n                    ensmbl_eouts=None, ensmbl_elens=None, ensmbl_decs=[], cache_states=True):\n        """"""Beam search decoding.\n\n        Args:\n            eouts (FloatTensor): `[B, T, d_model]`\n            elens (IntTensor): `[B]`\n            params (dict): hyperparameters for decoding\n            idx2token (): converter from index to token\n            lm: firsh path LM\n            lm_second: second path LM\n            lm_bwd: first/secoding path backward LM\n            ctc_log_probs (FloatTensor):\n            nbest (int):\n            exclude_eos (bool): exclude <eos> from hypothesis\n            refs_id (list): reference list\n            utt_ids (list): utterance id list\n            speakers (list): speaker list\n            ensmbl_eouts (list): list of FloatTensor\n            ensmbl_elens (list) list of list\n            ensmbl_decs (list): list of torch.nn.Module\n            cache_states (bool): cache decoder states for fast decoding\n        Returns:\n            nbest_hyps_idx (list): length `B`, each of which contains list of N hypotheses\n            aws (list): length `B`, each of which contains arrays of size `[H, L, T]`\n            scores (list):\n\n        """"""\n        bs, xmax, _ = eouts.size()\n        n_models = len(ensmbl_decs) + 1\n\n        beam_width = params[\'recog_beam_width\']\n        assert 1 <= nbest <= beam_width\n        ctc_weight = params[\'recog_ctc_weight\']\n        max_len_ratio = params[\'recog_max_len_ratio\']\n        min_len_ratio = params[\'recog_min_len_ratio\']\n        lp_weight = params[\'recog_length_penalty\']\n        length_norm = params[\'recog_length_norm\']\n        lm_weight = params[\'recog_lm_weight\']\n        lm_weight_second = params[\'recog_lm_second_weight\']\n        lm_weight_bwd = params[\'recog_lm_bwd_weight\']\n        eos_threshold = params[\'recog_eos_threshold\']\n        lm_state_carry_over = params[\'recog_lm_state_carry_over\']\n        softmax_smoothing = params[\'recog_softmax_smoothing\']\n        eps_wait = params[\'recog_mma_delay_threshold\']\n\n        if lm is not None:\n            assert lm_weight > 0\n            lm.eval()\n        if lm_second is not None:\n            assert lm_weight_second > 0\n            lm_second.eval()\n        if lm_bwd is not None:\n            assert lm_weight_bwd > 0\n            lm_bwd.eval()\n\n        if ctc_log_probs is not None:\n            assert ctc_weight > 0\n            ctc_log_probs = tensor2np(ctc_log_probs)\n\n        nbest_hyps_idx, aws, scores = [], [], []\n        eos_flags = []\n        for b in range(bs):\n            # Initialization per utterance\n            lmstate = None\n            ys = eouts.new_zeros(1, 1).fill_(self.eos).long()\n\n            # For joint CTC-Attention decoding\n            ctc_prefix_scorer = None\n            if ctc_log_probs is not None:\n                if self.bwd:\n                    ctc_prefix_scorer = CTCPrefixScore(ctc_log_probs[b][::-1], self.blank, self.eos)\n                else:\n                    ctc_prefix_scorer = CTCPrefixScore(ctc_log_probs[b], self.blank, self.eos)\n\n            if speakers is not None:\n                if speakers[b] == self.prev_spk:\n                    if lm_state_carry_over and isinstance(lm, RNNLM):\n                        lmstate = self.lmstate_final\n                self.prev_spk = speakers[b]\n\n            helper = BeamSearch(beam_width, self.eos, ctc_weight, self.device_id)\n\n            end_hyps = []\n            ymax = int(math.floor(elens[b] * max_len_ratio)) + 1\n            hyps = [{\'hyp\': [self.eos],\n                     \'ys\': ys,\n                     \'cache\': None,\n                     \'score\': 0.,\n                     \'score_attn\': 0.,\n                     \'score_ctc\': 0.,\n                     \'score_lm\': 0.,\n                     \'aws\': [None],\n                     \'lmstate\': lmstate,\n                     \'ensmbl_aws\':[[None]] * (n_models - 1),\n                     \'ctc_state\': ctc_prefix_scorer.initial_state() if ctc_prefix_scorer is not None else None,\n                     \'streamable\': True,\n                     \'streaming_failed_point\': 1000}]\n            streamable_global = True\n            for t in range(ymax):\n                # batchfy all hypotheses for batch decoding\n                cache = [None] * self.n_layers\n                if cache_states and t > 0:\n                    for lth in range(self.n_layers):\n                        cache[lth] = torch.cat([beam[\'cache\'][lth] for beam in hyps], dim=0)\n                ys = eouts.new_zeros(len(hyps), t + 1).long()\n                for j, beam in enumerate(hyps):\n                    ys[j, :] = beam[\'ys\']\n                if t > 0:\n                    xy_aws_prev = torch.cat([beam[\'aws\'][-1] for beam in hyps], dim=0)  # `[B, n_layers, H_ma, 1, klen]`\n                else:\n                    xy_aws_prev = None\n\n                # Update LM states for shallow fusion\n                lmstate, scores_lm = None, None\n                if lm is not None:\n                    if hyps[0][\'lmstate\'] is not None:\n                        lm_hxs = torch.cat([beam[\'lmstate\'][\'hxs\'] for beam in hyps], dim=1)\n                        lm_cxs = torch.cat([beam[\'lmstate\'][\'cxs\'] for beam in hyps], dim=1)\n                        lmstate = {\'hxs\': lm_hxs, \'cxs\': lm_cxs}\n                    y = ys[:, -1:].clone()  # NOTE: this is important\n                    _, lmstate, scores_lm = lm.predict(y, lmstate)\n\n                # for the main model\n                causal_mask = eouts.new_ones(t + 1, t + 1).byte()\n                causal_mask = torch.tril(causal_mask, out=causal_mask).unsqueeze(0).repeat([ys.size(0), 1, 1])\n\n                out = self.pos_enc(self.embed(ys))  # scaled\n\n                mlen = 0  # TODO: fix later\n                if self.memory_transformer:\n                    # NOTE: TransformerXL does not use positional encoding in the token embedding\n                    mems = self.init_memory()\n                    # adopt zero-centered offset\n                    pos_idxs = torch.arange(mlen - 1, -(t + 1) - 1, -1.0, dtype=torch.float)\n                    pos_embs = self.pos_emb(pos_idxs, self.device_id)\n                    out = self.dropout_emb(out)\n                    hidden_states = [out]\n\n                n_heads_total = 0\n                eouts_b = eouts[b:b + 1, :elens[b]].repeat([ys.size(0), 1, 1])\n                new_cache = [None] * self.n_layers\n                xy_aws_all_layers = []\n                xy_aws = None\n                lth_s = self.mocha_first_layer - 1\n                for lth, layer in enumerate(self.layers):\n                    if self.memory_transformer:\n                        out, _, xy_aws, _, _ = layer(\n                            out, causal_mask, eouts_b, None,\n                            cache=cache[lth],\n                            pos_embs=pos_embs, memory=mems[lth], u=self.u, v=self.v)\n                        hidden_states.append(out)\n                    else:\n                        out, _, xy_aws, _, _ = layer(\n                            out, causal_mask, eouts_b, None,\n                            cache=cache[lth],\n                            xy_aws_prev=xy_aws_prev[:, lth - lth_s] if lth >= lth_s and t > 0 else None,\n                            eps_wait=eps_wait)\n\n                    new_cache[lth] = out\n                    if xy_aws is not None:\n                        xy_aws_all_layers.append(xy_aws)\n                logits = self.output(self.norm_out(out))\n                probs = torch.softmax(logits[:, -1] * softmax_smoothing, dim=1)\n                xy_aws_all_layers = torch.stack(xy_aws_all_layers, dim=1)  # `[B, H, n_layers, L, T]`\n\n                # for the ensemble\n                ensmbl_new_cache = []\n                if n_models > 1:\n                    # Ensemble initialization\n                    # ensmbl_cache = []\n                    # cache_e = [None] * self.n_layers\n                    # if cache_states and t > 0:\n                    #     for l in range(self.n_layers):\n                    #         cache_e[l] = torch.cat([beam[\'ensmbl_cache\'][l] for beam in hyps], dim=0)\n                    for i_e, dec in enumerate(ensmbl_decs):\n                        out_e = dec.pos_enc(dec.embed(ys))  # scaled\n                        eouts_e = ensmbl_eouts[i_e][b:b + 1, :elens[b]].repeat([ys.size(0), 1, 1])\n                        new_cache_e = [None] * dec.n_layers\n                        for l in range(dec.n_layers):\n                            out_e, _, xy_aws_e, _, _ = dec.layers[l](out_e, causal_mask, eouts_e, None,\n                                                                     cache=cache[lth])\n                            new_cache_e[l] = out_e\n                        ensmbl_new_cache.append(new_cache_e)\n                        logits_e = dec.output(dec.norm_out(out_e))\n                        probs += torch.softmax(logits_e[:, -1] * softmax_smoothing, dim=1)\n                        # NOTE: sum in the probability scale (not log-scale)\n\n                # Ensemble in log-scale\n                scores_attn = torch.log(probs) / n_models\n\n                new_hyps = []\n                for j, beam in enumerate(hyps):\n                    # Attention scores\n                    total_scores_attn = beam[\'score_attn\'] + scores_attn[j:j + 1]\n                    total_scores = total_scores_attn * (1 - ctc_weight)\n\n                    # Add LM score <before> top-K selection\n                    if lm is not None:\n                        total_scores_lm = beam[\'score_lm\'] + scores_lm[j:j + 1, -1]\n                        total_scores += total_scores_lm * lm_weight\n                    else:\n                        total_scores_lm = eouts.new_zeros(1, self.vocab)\n\n                    total_scores_topk, topk_ids = torch.topk(\n                        total_scores, k=beam_width, dim=1, largest=True, sorted=True)\n\n                    # Add length penalty\n                    if lp_weight > 0:\n                        total_scores_topk += (len(beam[\'hyp\'][1:]) + 1) * lp_weight\n\n                    # Add CTC score\n                    new_ctc_states, total_scores_ctc, total_scores_topk = helper.add_ctc_score(\n                        beam[\'hyp\'], topk_ids, beam[\'ctc_state\'],\n                        total_scores_topk, ctc_prefix_scorer)\n\n                    new_aws = beam[\'aws\'] + [xy_aws_all_layers[j:j + 1, :, :, -1:]]\n                    aws_j = torch.cat(new_aws[1:], dim=3)  # `[1, H, n_layers, L, T]`\n                    streaming_failed_point = beam[\'streaming_failed_point\']\n\n                    # forward direction\n                    for k in range(beam_width):\n                        idx = topk_ids[0, k].item()\n                        length_norm_factor = len(beam[\'hyp\'][1:]) + 1 if length_norm else 1\n                        total_scores_topk /= length_norm_factor\n\n                        if idx == self.eos:\n                            # Exclude short hypotheses\n                            if len(beam[\'hyp\']) - 1 < elens[b] * min_len_ratio:\n                                continue\n                            # EOS threshold\n                            max_score_no_eos = scores_attn[j, :idx].max(0)[0].item()\n                            max_score_no_eos = max(max_score_no_eos, scores_attn[j, idx + 1:].max(0)[0].item())\n                            if scores_attn[j, idx].item() <= eos_threshold * max_score_no_eos:\n                                continue\n\n                        quantity_rate = 1.\n                        if \'mocha\' in self.attn_type:\n                            n_tokens_hyp_k = t + 1\n                            n_quantity_k = aws_j[:, :, :, :n_tokens_hyp_k].int().sum().item()\n                            quantity_diff = n_tokens_hyp_k * n_heads_total - n_quantity_k\n\n                            if quantity_diff != 0:\n                                if idx == self.eos:\n                                    n_tokens_hyp_k -= 1  # NOTE: do not count <eos> for streamability\n                                    n_quantity_k = aws_j[:, :, :, :n_tokens_hyp_k].int().sum().item()\n                                else:\n                                    streamable_global = False\n                                if n_tokens_hyp_k * n_heads_total == 0:\n                                    quantity_rate = 0\n                                else:\n                                    quantity_rate = n_quantity_k / (n_tokens_hyp_k * n_heads_total)\n\n                            if beam[\'streamable\'] and not streamable_global:\n                                streaming_failed_point = t\n\n                        new_hyps.append(\n                            {\'hyp\': beam[\'hyp\'] + [idx],\n                             \'ys\': torch.cat([beam[\'ys\'], eouts.new_zeros(1, 1).fill_(idx).long()], dim=-1),\n                             \'cache\': [new_cache_l[j:j + 1] for new_cache_l in new_cache] if cache_states else cache,\n                             \'score\': total_scores_topk[0, k].item(),\n                             \'score_attn\': total_scores_attn[0, idx].item(),\n                             \'score_ctc\': total_scores_ctc[k].item(),\n                             \'score_lm\': total_scores_lm[0, idx].item(),\n                             \'aws\': new_aws,\n                             \'lmstate\': {\'hxs\': lmstate[\'hxs\'][:, j:j + 1],\n                                         \'cxs\': lmstate[\'cxs\'][:, j:j + 1]} if lmstate is not None else None,\n                             \'ctc_state\': new_ctc_states[k] if ctc_prefix_scorer is not None else None,\n                             \'ensmbl_cache\': ensmbl_new_cache,\n                             \'streamable\': streamable_global,\n                             \'streaming_failed_point\': streaming_failed_point,\n                             \'quantity_rate\': quantity_rate})\n\n                # Local pruning\n                new_hyps_sorted = sorted(new_hyps, key=lambda x: x[\'score\'], reverse=True)[:beam_width]\n\n                # Remove complete hypotheses\n                new_hyps, end_hyps, is_finish = helper.remove_complete_hyp(\n                    new_hyps_sorted, end_hyps, prune=True)\n                hyps = new_hyps[:]\n                if is_finish:\n                    break\n\n            # Global pruning\n            if len(end_hyps) == 0:\n                end_hyps = hyps[:]\n            elif len(end_hyps) < nbest and nbest > 1:\n                end_hyps.extend(hyps[:nbest - len(end_hyps)])\n\n            # forward second path LM rescoring\n            if lm_second is not None:\n                self.lm_rescoring(end_hyps, lm_second, lm_weight_second, tag=\'second\')\n\n            # backward secodn path LM rescoring\n            if lm_bwd is not None and lm_weight_bwd > 0:\n                self.lm_rescoring(end_hyps, lm_bwd, lm_weight_bwd, tag=\'second_bwd\')\n\n            # Sort by score\n            end_hyps = sorted(end_hyps, key=lambda x: x[\'score\'], reverse=True)\n\n            for j in range(len(end_hyps[0][\'aws\'][1:])):\n                tmp = end_hyps[0][\'aws\'][j + 1]\n                end_hyps[0][\'aws\'][j + 1] = tmp.view(1, -1, tmp.size(-2), tmp.size(-1))\n\n            # metrics for streaming infernece\n            self.streamable = end_hyps[0][\'streamable\']\n            self.quantity_rate = end_hyps[0][\'quantity_rate\']\n            self.last_success_frame_ratio = None\n\n            if idx2token is not None:\n                if utt_ids is not None:\n                    logger.info(\'Utt-id: %s\' % utt_ids[b])\n                assert self.vocab == idx2token.vocab\n                logger.info(\'=\' * 200)\n                for k in range(len(end_hyps)):\n                    if refs_id is not None:\n                        logger.info(\'Ref: %s\' % idx2token(refs_id[b]))\n                    logger.info(\'Hyp: %s\' % idx2token(\n                        end_hyps[k][\'hyp\'][1:][::-1] if self.bwd else end_hyps[k][\'hyp\'][1:]))\n                    logger.info(\'num tokens (hyp): %d\' % len(end_hyps[k][\'hyp\'][1:]))\n                    logger.info(\'log prob (hyp): %.7f\' % end_hyps[k][\'score\'])\n                    logger.info(\'log prob (hyp, att): %.7f\' % (end_hyps[k][\'score_attn\'] * (1 - ctc_weight)))\n                    if ctc_prefix_scorer is not None:\n                        logger.info(\'log prob (hyp, ctc): %.7f\' % (end_hyps[k][\'score_ctc\'] * ctc_weight))\n                    if lm is not None:\n                        logger.info(\'log prob (hyp, first-path lm): %.7f\' % (end_hyps[k][\'score_lm\'] * lm_weight))\n                    if lm_second is not None:\n                        logger.info(\'log prob (hyp, second-path lm): %.7f\' %\n                                    (end_hyps[k][\'score_lm_second\'] * lm_weight_second))\n                    if lm_bwd is not None:\n                        logger.info(\'log prob (hyp, second-path lm-bwd): %.7f\' %\n                                    (end_hyps[k][\'score_lm_second_bwd\'] * lm_weight_bwd))\n                    if \'mocha\' in self.attn_type:\n                        logger.info(\'streamable: %s\' % end_hyps[k][\'streamable\'])\n                        logger.info(\'streaming failed point: %d\' % (end_hyps[k][\'streaming_failed_point\'] + 1))\n                        logger.info(\'quantity rate [%%]: %.2f\' % (end_hyps[k][\'quantity_rate\'] * 100))\n                    logger.info(\'-\' * 50)\n\n                if \'mocha\' in self.attn_type and end_hyps[0][\'streaming_failed_point\'] < 1000:\n                    assert not self.streamable\n                    aws_last_success = end_hyps[0][\'aws\'][1:][end_hyps[0][\'streaming_failed_point\'] - 1]\n                    rightmost_frame = max(0, aws_last_success[0, :, 0].nonzero()[:, -1].max().item()) + 1\n                    frame_ratio = rightmost_frame * 100 / xmax\n                    self.last_success_frame_ratio = frame_ratio\n                    logger.info(\'streaming last success frame ratio: %.2f\' % frame_ratio)\n\n            # N-best list\n            if self.bwd:\n                # Reverse the order\n                nbest_hyps_idx += [[np.array(end_hyps[n][\'hyp\'][1:][::-1]) for n in range(nbest)]]\n                aws += [tensor2np(torch.cat(end_hyps[0][\'aws\'][1:][::-1], dim=2).squeeze(0))]\n            else:\n                nbest_hyps_idx += [[np.array(end_hyps[n][\'hyp\'][1:]) for n in range(nbest)]]\n                aws += [tensor2np(torch.cat(end_hyps[0][\'aws\'][1:], dim=2).squeeze(0))]\n            scores += [[end_hyps[n][\'score_attn\'] for n in range(nbest)]]\n\n            # Check <eos>\n            eos_flags.append([(end_hyps[n][\'hyp\'][-1] == self.eos) for n in range(nbest)])\n\n        # Exclude <eos> (<sos> in case of the backward decoder)\n        if exclude_eos:\n            if self.bwd:\n                nbest_hyps_idx = [[nbest_hyps_idx[b][n][1:] if eos_flags[b][n]\n                                   else nbest_hyps_idx[b][n] for n in range(nbest)] for b in range(bs)]\n            else:\n                nbest_hyps_idx = [[nbest_hyps_idx[b][n][:-1] if eos_flags[b][n]\n                                   else nbest_hyps_idx[b][n] for n in range(nbest)] for b in range(bs)]\n\n        # Store ASR/LM state\n        if len(end_hyps) > 0:\n            self.lmstate_final = end_hyps[0][\'lmstate\']\n\n        return nbest_hyps_idx, aws, scores\n'"
neural_sp/models/seq2seq/encoders/__init__.py,0,b''
neural_sp/models/seq2seq/encoders/build.py,0,"b'#! /usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n# Copyright 2019 Kyoto University (Hirofumi Inaguma)\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n\n""""""Select an encoder network.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\n\ndef build_encoder(args):\n\n    if args.enc_type == \'tds\':\n        from neural_sp.models.seq2seq.encoders.tds import TDSEncoder\n        raise ValueError\n        encoder = TDSEncoder(\n            input_dim=args.input_dim * args.n_stacks,\n            in_channel=args.conv_in_channel,\n            channels=args.conv_channels,\n            kernel_sizes=args.conv_kernel_sizes,\n            dropout=args.dropout_enc,\n            bottleneck_dim=args.transformer_d_model if \'transformer\' in args.dec_type else args.dec_n_units)\n\n    elif args.enc_type == \'gated_conv\':\n        from neural_sp.models.seq2seq.encoders.gated_conv import GatedConvEncoder\n        raise ValueError\n        encoder = GatedConvEncoder(\n            input_dim=args.input_dim * args.n_stacks,\n            in_channel=args.conv_in_channel,\n            channels=args.conv_channels,\n            kernel_sizes=args.conv_kernel_sizes,\n            dropout=args.dropout_enc,\n            bottleneck_dim=args.transformer_d_model if \'transformer\' in args.dec_type else args.dec_n_units,\n            param_init=args.param_init)\n\n    elif \'transformer\' in args.enc_type:\n        from neural_sp.models.seq2seq.encoders.transformer import TransformerEncoder\n        encoder = TransformerEncoder(\n            input_dim=args.input_dim if args.input_type == \'speech\' else args.emb_dim,\n            enc_type=args.enc_type,\n            n_heads=args.transformer_n_heads,\n            n_layers=args.enc_n_layers,\n            n_layers_sub1=args.enc_n_layers_sub1,\n            n_layers_sub2=args.enc_n_layers_sub2,\n            d_model=args.transformer_d_model,\n            d_ff=args.transformer_d_ff,\n            d_ff_bottleneck_dim=getattr(args, \'transformer_d_ff_bottleneck_dim\', 0),\n            last_proj_dim=args.transformer_d_model if \'transformer\' in args.dec_type else 0,\n            pe_type=args.transformer_enc_pe_type,\n            layer_norm_eps=args.transformer_layer_norm_eps,\n            ffn_activation=args.transformer_ffn_activation,\n            dropout_in=args.dropout_in,\n            dropout=args.dropout_enc,\n            dropout_att=args.dropout_att,\n            dropout_layer=args.dropout_enc_layer,\n            n_stacks=args.n_stacks,\n            n_splices=args.n_splices,\n            conv_in_channel=args.conv_in_channel,\n            conv_channels=args.conv_channels,\n            conv_kernel_sizes=args.conv_kernel_sizes,\n            conv_strides=args.conv_strides,\n            conv_poolings=args.conv_poolings,\n            conv_batch_norm=args.conv_batch_norm,\n            conv_layer_norm=args.conv_layer_norm,\n            conv_bottleneck_dim=args.conv_bottleneck_dim,\n            conv_param_init=args.param_init,\n            task_specific_layer=args.task_specific_layer,\n            param_init=args.transformer_param_init,\n            chunk_size_left=args.lc_chunk_size_left,\n            chunk_size_current=args.lc_chunk_size_current,\n            chunk_size_right=args.lc_chunk_size_right)\n\n    else:\n        from neural_sp.models.seq2seq.encoders.rnn import RNNEncoder\n        encoder = RNNEncoder(\n            input_dim=args.input_dim if args.input_type == \'speech\' else args.emb_dim,\n            rnn_type=args.enc_type,\n            n_units=args.enc_n_units,\n            n_projs=args.enc_n_projs,\n            last_proj_dim=args.transformer_d_model if \'transformer\' in args.dec_type else 0,\n            n_layers=args.enc_n_layers,\n            n_layers_sub1=args.enc_n_layers_sub1,\n            n_layers_sub2=args.enc_n_layers_sub2,\n            dropout_in=args.dropout_in,\n            dropout=args.dropout_enc,\n            subsample=args.subsample,\n            subsample_type=args.subsample_type,\n            n_stacks=args.n_stacks,\n            n_splices=args.n_splices,\n            conv_in_channel=args.conv_in_channel,\n            conv_channels=args.conv_channels,\n            conv_kernel_sizes=args.conv_kernel_sizes,\n            conv_strides=args.conv_strides,\n            conv_poolings=args.conv_poolings,\n            conv_batch_norm=args.conv_batch_norm,\n            conv_layer_norm=args.conv_layer_norm,\n            conv_bottleneck_dim=args.conv_bottleneck_dim,\n            bidirectional_sum_fwd_bwd=args.bidirectional_sum_fwd_bwd,\n            task_specific_layer=args.task_specific_layer,\n            param_init=args.param_init,\n            chunk_size_left=args.lc_chunk_size_left,\n            chunk_size_right=args.lc_chunk_size_right)\n        # NOTE: pure Conv/TDS/GatedConv encoders are also included\n\n    return encoder\n'"
neural_sp/models/seq2seq/encoders/conv.py,7,"b'#! /usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n# Copyright 2018 Kyoto University (Hirofumi Inaguma)\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n\n""""""CNN encoder.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom distutils.util import strtobool\nimport logging\nimport math\nimport numpy as np\nimport torch\nimport torch.nn as nn\n\nfrom neural_sp.models.modules.initialization import init_with_lecun_normal\nfrom neural_sp.models.seq2seq.encoders.encoder_base import EncoderBase\n\nlogger = logging.getLogger(__name__)\n\n\nclass ConvEncoder(EncoderBase):\n    """"""CNN encoder.\n\n    Args:\n        input_dim (int): dimension of input features (freq * channel)\n        in_channel (int): number of channels of input features\n        channels (list): number of channles in CNN blocks\n        kernel_sizes (list): size of kernels in CNN blocks\n        strides (list): strides in CNN blocks\n        poolings (list): size of poolings in CNN blocks\n        dropout (float): probability to drop nodes in hidden-hidden connection\n        batch_norm (bool): apply batch normalization\n        layer_norm (bool): apply layer normalization\n        residual (bool): add residual connections\n        bottleneck_dim (int): dimension of the bridge layer after the last layer\n        param_init (float): model initialization parameter\n        layer_norm_eps (float):\n\n    """"""\n\n    def __init__(self, input_dim, in_channel, channels,\n                 kernel_sizes, strides, poolings,\n                 dropout, batch_norm, layer_norm, residual,\n                 bottleneck_dim, param_init, layer_norm_eps=1e-12):\n\n        super(ConvEncoder, self).__init__()\n\n        (channels, kernel_sizes, strides, poolings), is_1dconv = parse_cnn_config(\n            channels, kernel_sizes, strides, poolings)\n\n        self.is_1dconv = is_1dconv\n        self.in_channel = in_channel\n        assert input_dim % in_channel == 0\n        self.input_freq = input_dim // in_channel\n        self.residual = residual\n        self.bridge = None\n\n        assert len(channels) > 0\n        assert len(channels) == len(kernel_sizes) == len(strides) == len(poolings)\n\n        self.layers = nn.ModuleList()\n        C_i = input_dim if is_1dconv else in_channel\n        in_freq = self.input_freq\n        for lth in range(len(channels)):\n            if is_1dconv:\n                block = Conv1dBlock(in_channel=C_i,\n                                    out_channel=channels[lth],\n                                    kernel_size=kernel_sizes[lth],  # T\n                                    stride=strides[lth],  # T\n                                    pooling=poolings[lth],  # T\n                                    dropout=dropout,\n                                    batch_norm=batch_norm,\n                                    layer_norm=layer_norm,\n                                    layer_norm_eps=layer_norm_eps,\n                                    residual=residual)\n            else:\n                block = Conv2dBlock(input_dim=in_freq,\n                                    in_channel=C_i,\n                                    out_channel=channels[lth],\n                                    kernel_size=kernel_sizes[lth],  # (T,F)\n                                    stride=strides[lth],  # (T,F)\n                                    pooling=poolings[lth],  # (T,F)\n                                    dropout=dropout,\n                                    batch_norm=batch_norm,\n                                    layer_norm=layer_norm,\n                                    layer_norm_eps=layer_norm_eps,\n                                    residual=residual)\n            self.layers += [block]\n            in_freq = block.output_dim\n            C_i = channels[lth]\n\n        self._odim = C_i if is_1dconv else int(C_i * in_freq)\n\n        if bottleneck_dim > 0:\n            self.bridge = nn.Linear(self._odim, bottleneck_dim)\n            self._odim = bottleneck_dim\n\n        # calculate subsampling factor\n        self._factor = 1\n        if poolings:\n            for p in poolings:\n                self._factor *= p if is_1dconv else p[0]\n\n        self.reset_parameters(param_init)\n\n    @staticmethod\n    def add_args(parser, args):\n        """"""Add arguments.""""""\n        group = parser.add_argument_group(""CNN encoder"")\n        group.add_argument(\'--conv_in_channel\', type=int, default=1,\n                           help=\'input dimension of the first CNN block\')\n        group.add_argument(\'--conv_channels\', type=str, default="""",\n                           help=\'delimited list of channles in each CNN block\')\n        group.add_argument(\'--conv_kernel_sizes\', type=str, default="""",\n                           help=\'delimited list of kernel sizes in each CNN block\')\n        group.add_argument(\'--conv_strides\', type=str, default="""",\n                           help=\'delimited list of strides in each CNN block\')\n        group.add_argument(\'--conv_poolings\', type=str, default="""",\n                           help=\'delimited list of poolings in each CNN block\')\n        group.add_argument(\'--conv_batch_norm\', type=strtobool, default=False,\n                           help=\'apply batch normalization in each CNN block\')\n        group.add_argument(\'--conv_layer_norm\', type=strtobool, default=False,\n                           help=\'apply layer normalization in each CNN block\')\n        group.add_argument(\'--conv_bottleneck_dim\', type=int, default=0,\n                           help=\'dimension of the bottleneck layer between CNN and the subsequent RNN/Transformer layers\')\n        return parser\n\n    def reset_parameters(self, param_init):\n        """"""Initialize parameters with lecun style.""""""\n        logger.info(\'===== Initialize %s with lecun style =====\' % self.__class__.__name__)\n        for n, p in self.named_parameters():\n            init_with_lecun_normal(n, p, param_init)\n\n    def forward(self, xs, xlens):\n        """"""Forward computation.\n\n        Args:\n            xs (FloatTensor): `[B, T, F]`\n            xlens (list): A list of length `[B]`\n        Returns:\n            xs (FloatTensor): `[B, T\', F\']`\n            xlens (list): A list of length `[B]`\n\n        """"""\n        B, T, F = xs.size()\n        C_i = self.in_channel\n        if not self.is_1dconv:\n            xs = xs.view(B, T, C_i, F // C_i).contiguous().transpose(2, 1)  # `[B, C_i, T, F // C_i]`\n\n        for block in self.layers:\n            xs, xlens = block(xs, xlens)\n        if not self.is_1dconv:\n            B, C_o, T, F = xs.size()\n            xs = xs.transpose(2, 1).contiguous().view(B, T, -1)  # `[B, T\', C_o * F\']`\n\n        # Bridge layer\n        if self.bridge is not None:\n            xs = self.bridge(xs)\n\n        return xs, xlens\n\n\nclass Conv1dBlock(EncoderBase):\n    """"""1d-CNN block.""""""\n\n    def __init__(self, in_channel, out_channel,\n                 kernel_size, stride, pooling,\n                 dropout, batch_norm, layer_norm, layer_norm_eps, residual):\n\n        super(Conv1dBlock, self).__init__()\n\n        self.batch_norm = batch_norm\n        self.layer_norm = layer_norm\n        self.residual = residual\n        self.dropout = nn.Dropout(p=dropout)\n\n        # 1st layer\n        self.conv1 = nn.Conv1d(in_channels=in_channel,\n                               out_channels=out_channel,\n                               kernel_size=kernel_size,\n                               stride=stride,\n                               padding=1)\n        self._odim = update_lens_1d([in_channel], self.conv1)[0]\n        self.batch_norm1 = nn.BatchNorm1d(out_channel) if batch_norm else lambda x: x\n        self.layer_norm1 = nn.LayerNorm(out_channel,\n                                        eps=layer_norm_eps) if layer_norm else lambda x: x\n\n        # 2nd layer\n        self.conv2 = nn.Conv1d(in_channels=out_channel,\n                               out_channels=out_channel,\n                               kernel_size=kernel_size,\n                               stride=stride,\n                               padding=1)\n        self._odim = update_lens_1d([self._odim], self.conv2)[0]\n        self.batch_norm2 = nn.BatchNorm1d(out_channel) if batch_norm else lambda x: x\n        self.layer_norm2 = nn.LayerNorm(out_channel,\n                                        eps=layer_norm_eps) if layer_norm else lambda x: x\n\n        # Max Pooling\n        self.pool = None\n        if pooling > 1:\n            self.pool = nn.MaxPool1d(kernel_size=pooling,\n                                     stride=pooling,\n                                     padding=0,\n                                     ceil_mode=True)\n            # NOTE: If ceil_mode is False, remove last feature when the dimension of features are odd.\n            self._odim = update_lens_1d([self._odim], self.pool)[0].item()\n            if self._odim % 2 != 0:\n                self._odim = (self._odim // 2) * 2\n                # TODO(hirofumi0810): more efficient way?\n\n    def forward(self, xs, xlens):\n        """"""Forward computation.\n\n        Args:\n            xs (FloatTensor): `[B, T, F]`\n            xlens (IntTensor): `[B]`\n        Returns:\n            xs (FloatTensor): `[B, T\', F\']`\n            xlens (IntTensor): `[B]`\n\n        """"""\n        residual = xs\n\n        xs = xs.transpose(2, 1)\n        xs = self.conv1(xs)\n        xs = xs.transpose(2, 1)\n        xs = self.batch_norm1(xs)\n        xs = self.layer_norm1(xs)\n        xs = torch.relu(xs)\n        xs = self.dropout(xs)\n        xlens = update_lens_1d(xlens, self.conv1)\n\n        xs = xs.transpose(2, 1)\n        xs = self.conv2(xs)\n        xs = xs.transpose(2, 1)\n        xs = self.batch_norm2(xs)\n        xs = self.layer_norm2(xs)\n        if self.residual and xs.size() == residual.size():\n            xs += residual  # NOTE: this is the same place as in ResNet\n        xs = torch.relu(xs)\n        xs = self.dropout(xs)\n        xlens = update_lens_1d(xlens, self.conv2)\n\n        if self.pool is not None:\n            xs = self.pool(xs)\n            xlens = update_lens_1d(xlens, self.pool)\n\n        return xs, xlens\n\n\nclass Conv2dBlock(EncoderBase):\n    """"""2d-CNN block.""""""\n\n    def __init__(self, input_dim, in_channel, out_channel,\n                 kernel_size, stride, pooling,\n                 dropout, batch_norm, layer_norm, layer_norm_eps, residual):\n\n        super(Conv2dBlock, self).__init__()\n\n        self.batch_norm = batch_norm\n        self.layer_norm = layer_norm\n        self.residual = residual\n        self.dropout = nn.Dropout(p=dropout)\n\n        # 1st layer\n        self.conv1 = nn.Conv2d(in_channels=in_channel,\n                               out_channels=out_channel,\n                               kernel_size=tuple(kernel_size),\n                               stride=tuple(stride),\n                               padding=(1, 1))\n        self._odim = update_lens_2d([input_dim], self.conv1, dim=1)[0]\n        self.batch_norm1 = nn.BatchNorm2d(out_channel) if batch_norm else lambda x: x\n        self.layer_norm1 = LayerNorm2D(out_channel * self._odim.item(),\n                                       eps=layer_norm_eps) if layer_norm else lambda x: x\n\n        # 2nd layer\n        self.conv2 = nn.Conv2d(in_channels=out_channel,\n                               out_channels=out_channel,\n                               kernel_size=tuple(kernel_size),\n                               stride=tuple(stride),\n                               padding=(1, 1))\n        self._odim = update_lens_2d([self._odim], self.conv2, dim=1)[0]\n        self.batch_norm2 = nn.BatchNorm2d(out_channel) if batch_norm else lambda x: x\n        self.layer_norm2 = LayerNorm2D(out_channel * self._odim.item(),\n                                       eps=layer_norm_eps) if layer_norm else lambda x: x\n\n        # Max Pooling\n        self.pool = None\n        if len(pooling) > 0 and np.prod(pooling) > 1:\n            self.pool = nn.MaxPool2d(kernel_size=tuple(pooling),\n                                     stride=tuple(pooling),\n                                     padding=(0, 0),\n                                     ceil_mode=True)\n            # NOTE: If ceil_mode is False, remove last feature when the dimension of features are odd.\n            self._odim = update_lens_2d([self._odim], self.pool, dim=1)[0].item()\n            if self._odim % 2 != 0:\n                self._odim = (self._odim // 2) * 2\n                # TODO(hirofumi0810): more efficient way?\n\n    def forward(self, xs, xlens):\n        """"""Forward computation.\n\n        Args:\n            xs (FloatTensor): `[B, C_i, T, F]`\n            xlens (IntTensor): `[B]`\n        Returns:\n            xs (FloatTensor): `[B, C_o, T\', F\']`\n            xlens (IntTensor): `[B]`\n\n        """"""\n        residual = xs\n\n        xs = self.conv1(xs)\n        xs = self.batch_norm1(xs)\n        xs = self.layer_norm1(xs)\n        xs = torch.relu(xs)\n        xs = self.dropout(xs)\n        xlens = update_lens_2d(xlens, self.conv1, dim=0)\n\n        xs = self.conv2(xs)\n        xs = self.batch_norm2(xs)\n        xs = self.layer_norm2(xs)\n        if self.residual and xs.size() == residual.size():\n            xs += residual  # NOTE: this is the same place as in ResNet\n        xs = torch.relu(xs)\n        xs = self.dropout(xs)\n        xlens = update_lens_2d(xlens, self.conv2, dim=0)\n\n        if self.pool is not None:\n            xs = self.pool(xs)\n            xlens = update_lens_2d(xlens, self.pool, dim=0)\n\n        return xs, xlens\n\n\nclass LayerNorm2D(nn.Module):\n    """"""Layer normalization for CNN outputs.""""""\n\n    def __init__(self, dim, eps=1e-12):\n\n        super(LayerNorm2D, self).__init__()\n        self.norm = nn.LayerNorm(dim, eps=eps)\n\n    def forward(self, xs):\n        """"""Forward computation.\n\n        Args:\n            xs (FloatTensor): `[B, C, T, F]`\n        Returns:\n            xs (FloatTensor): `[B, C, T, F]`\n\n        """"""\n        B, C, T, F = xs.size()\n        xs = xs.transpose(2, 1).contiguous().view(B, T, C * F)\n        xs = self.norm(xs)\n        xs = xs.view(B, T, C, F).transpose(2, 1)\n        return xs\n\n\ndef update_lens_1d(seq_lens, layer, device_id=-1):\n    """"""Update lenghts (frequency or time).\n\n    Args:\n        seq_lens (list or IntTensor):\n        layer (nn.Conv1d or nn.MaxPool1d):\n        device_id (int):\n    Returns:\n        seq_lens (IntTensor):\n\n    """"""\n    if seq_lens is None:\n        return seq_lens\n    assert type(layer) in [nn.Conv1d, nn.MaxPool1d]\n    seq_lens = [_update_1d(seq_len, layer) for seq_len in seq_lens]\n    seq_lens = torch.IntTensor(seq_lens)\n    if device_id >= 0:\n        seq_lens = seq_lens.cuda(device_id)\n    return seq_lens\n\n\ndef _update_1d(seq_len, layer):\n    if type(layer) == nn.MaxPool1d and layer.ceil_mode:\n        return math.ceil(\n            (seq_len + 1 + 2 * layer.padding[0] - (layer.kernel_size[0] - 1) - 1) / layer.stride[0] + 1)\n    else:\n        return math.floor(\n            (seq_len + 2 * layer.padding[0] - (layer.kernel_size[0] - 1) - 1) / layer.stride[0] + 1)\n\n\ndef update_lens_2d(seq_lens, layer, dim=0, device_id=-1):\n    """"""Update lenghts (frequency or time).\n\n    Args:\n        seq_lens (list or IntTensor):\n        layer (nn.Conv2d or nn.MaxPool2d):\n        dim (int):\n        device_id (int):\n    Returns:\n        seq_lens (IntTensor):\n\n    """"""\n    if seq_lens is None:\n        return seq_lens\n    assert type(layer) in [nn.Conv2d, nn.MaxPool2d]\n    seq_lens = [_update_2d(seq_len, layer, dim) for seq_len in seq_lens]\n    seq_lens = torch.IntTensor(seq_lens)\n    if device_id >= 0:\n        seq_lens = seq_lens.cuda(device_id)\n    return seq_lens\n\n\ndef _update_2d(seq_len, layer, dim):\n    if type(layer) == nn.MaxPool2d and layer.ceil_mode:\n        return math.ceil(\n            (seq_len + 1 + 2 * layer.padding[dim] - (layer.kernel_size[dim] - 1) - 1) / layer.stride[dim] + 1)\n    else:\n        return math.floor(\n            (seq_len + 2 * layer.padding[dim] - (layer.kernel_size[dim] - 1) - 1) / layer.stride[dim] + 1)\n\n\ndef parse_cnn_config(channels, kernel_sizes, strides, poolings):\n    _channels, _kernel_sizes, _strides, _poolings = [], [], [], []\n    is_1dconv = \'(\' not in kernel_sizes\n    if len(channels) > 0:\n        _channels = [int(c) for c in channels.split(\'_\')]\n    if len(kernel_sizes) > 0:\n        if is_1dconv:\n            _kernel_sizes = [int(c) for c in kernel_sizes.split(\'_\')]\n        else:\n            _kernel_sizes = [[int(c.split(\',\')[0].replace(\'(\', \'\')),\n                              int(c.split(\',\')[1].replace(\')\', \'\'))] for c in kernel_sizes.split(\'_\')]\n    if len(strides) > 0:\n        if is_1dconv:\n            assert \'(\' not in _strides and \')\' not in _strides\n            _strides = [int(s) for s in strides.split(\'_\')]\n        else:\n            _strides = [[int(s.split(\',\')[0].replace(\'(\', \'\')),\n                         int(s.split(\',\')[1].replace(\')\', \'\'))] for s in strides.split(\'_\')]\n    if len(poolings) > 0:\n        if is_1dconv:\n            assert \'(\' not in poolings and \')\' not in poolings\n            _poolings = [int(p) for p in strides.split(\'_\')]\n        else:\n            _poolings = [[int(p.split(\',\')[0].replace(\'(\', \'\')),\n                          int(p.split(\',\')[1].replace(\')\', \'\'))] for p in poolings.split(\'_\')]\n    return (_channels, _kernel_sizes, _strides, _poolings), is_1dconv\n'"
neural_sp/models/seq2seq/encoders/encoder_base.py,3,"b'#! /usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n# Copyright 2019 Kyoto University (Hirofumi Inaguma)\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n\n""""""Base class for encoders.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport logging\nimport os\nimport shutil\nimport torch\n\nfrom neural_sp.models.base import ModelBase\nfrom neural_sp.utils import mkdir_join\n\nimport matplotlib\nmatplotlib.use(\'Agg\')\n\nlogger = logging.getLogger(__name__)\n\n\nclass EncoderBase(ModelBase):\n    """"""Base class for encoders.""""""\n\n    def __init__(self):\n\n        super(ModelBase, self).__init__()\n        logger.info(\'Overriding EncoderBase class.\')\n\n    @property\n    def device_id(self):\n        return torch.cuda.device_of(next(self.parameters()).data).idx\n\n    @property\n    def output_dim(self):\n        return self._odim\n\n    @property\n    def subsampling_factor(self):\n        return self._factor\n\n    def reset_parameters(self, param_init):\n        raise NotImplementedError\n\n    def forward(self, xs, xlens, task):\n        raise NotImplementedError\n\n    def turn_off_ceil_mode(self, encoder):\n        if isinstance(encoder, torch.nn.Module):\n            for name, module in encoder.named_children():\n                if isinstance(module, torch.nn.MaxPool2d):\n                    module.ceil_mode = False\n                    logging.debug(\'Turn off ceil_mode in %s.\' % name)\n                else:\n                    self.turn_off_ceil_mode(module)\n\n    def _plot_attention(self, save_path, n_cols=2):\n        """"""Plot attention for each head in all layers.""""""\n        from matplotlib import pyplot as plt\n        from matplotlib.ticker import MaxNLocator\n\n        _save_path = mkdir_join(save_path, \'enc_att_weights\')\n\n        # Clean directory\n        if _save_path is not None and os.path.isdir(_save_path):\n            shutil.rmtree(_save_path)\n            os.mkdir(_save_path)\n\n        for k, aw in self.aws_dict.items():\n            elens = self.data_dict[\'elens\']\n\n            plt.clf()\n            n_heads = aw.shape[1]\n            n_cols_tmp = 1 if n_heads == 1 else n_cols\n            fig, axes = plt.subplots(max(1, n_heads // n_cols_tmp), n_cols_tmp,\n                                     figsize=(20, 8), squeeze=False)\n            for h in range(n_heads):\n                ax = axes[h // n_cols_tmp, h % n_cols_tmp]\n                ax.imshow(aw[-1, h, :elens[-1], :elens[-1]], aspect=""auto"")\n                ax.grid(False)\n                ax.set_xlabel(""Input (head%d)"" % h)\n                ax.set_ylabel(""Output (head%d)"" % h)\n                ax.xaxis.set_major_locator(MaxNLocator(integer=True))\n                ax.yaxis.set_major_locator(MaxNLocator(integer=True))\n\n            fig.tight_layout()\n            fig.savefig(os.path.join(_save_path, \'%s.png\' % k), dvi=500)\n            plt.close()\n'"
neural_sp/models/seq2seq/encoders/gated_conv.py,2,"b'#! /usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n# Copyright 2019 Kyoto University (Hirofumi Inaguma)\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n\n""""""Gated convolutional neural netwrok encoder.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom collections import OrderedDict\nimport logging\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom neural_sp.models.modules.glu import ConvGLUBlock\nfrom neural_sp.models.seq2seq.encoders.conv import parse_cnn_config\nfrom neural_sp.models.seq2seq.encoders.encoder_base import EncoderBase\n\nlogger = logging.getLogger(__name__)\n\n\nclass GatedConvEncoder(EncoderBase):\n    """"""Gated convolutional neural netwrok encoder.\n\n    Args:\n        input_dim (int) dimension of input features (freq * channel)\n        in_channel (int) number of channels of input features\n        channels (list) number of channles in TDS layers\n        kernel_sizes (list) size of kernels in TDS layers\n        strides (list): strides in TDS layers\n        poolings (list) size of poolings in TDS layers\n        dropout (float) probability to drop nodes in hidden-hidden connection\n        batch_norm (bool): if True, apply batch normalization\n        bottleneck_dim (int): dimension of the bottleneck layer after the last layer\n        param_init (float): model initialization parameter\n\n    """"""\n\n    def __init__(self,\n                 input_dim,\n                 in_channel,\n                 channels,\n                 kernel_sizes,\n                 dropout,\n                 bottleneck_dim=0,\n                 param_init=0.1):\n\n        super(GatedConvEncoder, self).__init__()\n\n        (channels, kernel_sizes, _, _), _ = parse_cnn_config(channels, kernel_sizes, \'\', \'\')\n\n        self.in_channel = in_channel\n        assert input_dim % in_channel == 0\n        self.input_freq = input_dim // in_channel\n        self.bridge = None\n\n        assert len(channels) > 0\n        assert len(channels) == len(kernel_sizes)\n\n        layers = OrderedDict()\n        for l in range(len(channels)):\n            layers[\'conv%d\' % l] = ConvGLUBlock(kernel_sizes[l][0], input_dim, channels[l],\n                                                weight_norm=True,\n                                                dropout=0.2)\n            input_dim = channels[l]\n\n        # weight normalization + GLU for the last fully-connected layer\n        self.fc_glu = nn.utils.weight_norm(nn.Linear(input_dim, input_dim * 2),\n                                           name=\'weight\', dim=0)\n\n        self._odim = int(input_dim)\n\n        if bottleneck_dim > 0:\n            self.bridge = nn.Linear(self._odim, bottleneck_dim)\n            self._odim = bottleneck_dim\n\n        self.layers = nn.Sequential(layers)\n\n        self._factor = 1\n\n        self.reset_parameters(param_init)\n\n    def reset_parameters(self, param_init):\n        """"""Initialize parameters with kaiming_uniform style.""""""\n        logger.info(\'===== Initialize %s with kaiming_uniform style =====\' % self.__class__.__name__)\n        for n, p in self.named_parameters():\n            if p.dim() == 1:\n                nn.init.constant_(p, 0.)  # bias\n                logger.info(\'Initialize %s with %s / %.3f\' % (n, \'constant\', 0.))\n            elif p.dim() in [2, 4]:\n                nn.init.kaiming_uniform_(p, mode=\'fan_in\', nonlinearity=\'relu\')\n                # nn.init.kaiming_normal_(p, mode=\'fan_in\', nonlinearity=\'relu\')\n                logger.info(\'Initialize %s with %s / %.3f\' % (n, \'kaiming_uniform\', param_init))\n            else:\n                raise ValueError(n)\n\n    def forward(self, xs, xlens):\n        """"""Forward computation.\n\n        Args:\n            xs (FloatTensor): `[B, T, input_dim (+\xce\x94, \xce\x94\xce\x94)]`\n            xlens (list): A list of length `[B]`\n        Returns:\n            xs (FloatTensor): `[B, T\', out_ch * feat_dim]`\n            xlens (list): A list of length `[B]`\n\n        """"""\n        bs, time, input_dim = xs.size()\n        xs = xs.transpose(2, 1).unsqueeze(3)  # `[B, in_ch (input_dim), T, 1]`\n\n        xs = self.layers(xs)  # `[B, out_ch, T, 1]`\n        bs, out_ch, time, freq = xs.size()\n        xs = xs.transpose(2, 1).contiguous().view(bs, time, -1)  # `[B, T, out_ch * feat_dim]`\n\n        # weight normalization + GLU for the last fully-connected layer\n        xs = F.glu(self.fc_glu(xs), dim=2)\n\n        # Bridge layer\n        if self.bridge is not None:\n            xs = self.bridge(xs)\n\n        # NOTE: no subsampling is conducted\n\n        return xs, xlens\n'"
neural_sp/models/seq2seq/encoders/rnn.py,23,"b'#! /usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n# Copyright 2018 Kyoto University (Hirofumi Inaguma)\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n\n""""""(Hierarchical) RNN encoder.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom distutils.util import strtobool\nimport logging\nimport math\nimport numpy as np\nimport torch\nimport torch.nn as nn\n\nfrom torch.nn.utils.rnn import pack_padded_sequence\nfrom torch.nn.utils.rnn import pad_packed_sequence\n\nfrom neural_sp.models.seq2seq.encoders.conv import ConvEncoder\nfrom neural_sp.models.seq2seq.encoders.encoder_base import EncoderBase\nfrom neural_sp.models.seq2seq.encoders.gated_conv import GatedConvEncoder\nfrom neural_sp.models.seq2seq.encoders.tds import TDSEncoder\n\n\nlogger = logging.getLogger(__name__)\n\n\nclass RNNEncoder(EncoderBase):\n    """"""RNN encoder.\n\n    Args:\n        input_dim (int): dimension of input features (freq * channel)\n        rnn_type (str): type of encoder (including pure CNN layers)\n        n_units (int): number of units in each layer\n        n_projs (int): number of units in each projection layer\n        last_proj_dim (int): dimension of the last projection layer\n        n_layers (int): number of layers\n        n_layers_sub1 (int): number of layers in the 1st auxiliary task\n        n_layers_sub2 (int): number of layers in the 2nd auxiliary task\n        dropout_in (float): dropout probability for input-hidden connection\n        dropout (float): dropout probability for hidden-hidden connection\n        subsample (list): subsample in the corresponding RNN layers\n            ex.) [1, 2, 2, 1] means that subsample is conducted in the 2nd and 3rd layers.\n        subsample_type (str): drop/concat/max_pool\n        n_stacks (int): number of frames to stack\n        n_splices (int): number of frames to splice\n        conv_in_channel (int): number of channels of input features\n        conv_channels (int): number of channles in the CNN blocks\n        conv_kernel_sizes (list): size of kernels in the CNN blocks\n        conv_strides (list): number of strides in the CNN blocks\n        conv_poolings (list): size of poolings in the CNN blocks\n        conv_batch_norm (bool): apply batch normalization only in the CNN blocks\n        conv_layer_norm (bool): apply layer normalization only in the CNN blocks\n        conv_bottleneck_dim (int): dimension of the bottleneck layer between CNN and RNN layers\n        bidirectional_sum_fwd_bwd (bool): sum up forward and backward outputs for demiension reduction\n        task_specific_layer (bool): add a task specific layer for each sub task\n        param_init (float): model initialization parameter\n        chunk_size_left (int): left chunk size for latency-controlled bidirectional encoder\n        chunk_size_right (int): right chunk size for latency-controlled bidirectional encoder\n\n    """"""\n\n    def __init__(self, input_dim, rnn_type, n_units, n_projs, last_proj_dim,\n                 n_layers, n_layers_sub1, n_layers_sub2,\n                 dropout_in, dropout,\n                 subsample, subsample_type, n_stacks, n_splices,\n                 conv_in_channel, conv_channels, conv_kernel_sizes, conv_strides, conv_poolings,\n                 conv_batch_norm, conv_layer_norm, conv_bottleneck_dim,\n                 bidirectional_sum_fwd_bwd, task_specific_layer, param_init,\n                 chunk_size_left, chunk_size_right):\n\n        super(RNNEncoder, self).__init__()\n\n        # parse subsample\n        subsample_list = [1] * n_layers\n        for lth, s in enumerate(list(map(int, subsample.split(\'_\')[:n_layers]))):\n            subsample_list[lth] = s\n\n        if len(subsample_list) > 0 and len(subsample_list) != n_layers:\n            raise ValueError(\'subsample must be the same size as n_layers. n_layers: %d, subsample: %s\' %\n                             (n_layers, subsample_list))\n        if n_layers_sub1 < 0 or (n_layers_sub1 > 1 and n_layers < n_layers_sub1):\n            raise ValueError(\'Set n_layers_sub1 between 1 to n_layers. n_layers: %d, n_layers_sub1: %d\' %\n                             (n_layers, n_layers_sub1))\n        if n_layers_sub2 < 0 or (n_layers_sub2 > 1 and n_layers_sub1 < n_layers_sub2):\n            raise ValueError(\'Set n_layers_sub2 between 1 to n_layers_sub1. n_layers_sub1: %d, n_layers_sub2: %d\' %\n                             (n_layers_sub1, n_layers_sub2))\n\n        self.rnn_type = rnn_type\n        self.bidirectional = True if (\'blstm\' in rnn_type or \'bgru\' in rnn_type) else False\n        self.n_units = n_units\n        self.n_dirs = 2 if self.bidirectional else 1\n        self.n_layers = n_layers\n        self.bidir_sum = bidirectional_sum_fwd_bwd\n\n        # for latency-controlled\n        self.latency_controlled = chunk_size_left > 0 or chunk_size_right > 0\n        self.chunk_size_left = chunk_size_left\n        self.chunk_size_right = chunk_size_right\n        if self.latency_controlled:\n            assert n_layers_sub2 == 0\n\n        # for hierarchical encoder\n        self.n_layers_sub1 = n_layers_sub1\n        self.n_layers_sub2 = n_layers_sub2\n        self.task_specific_layer = task_specific_layer\n\n        # for bridge layers\n        self.bridge = None\n        self.bridge_sub1 = None\n        self.bridge_sub2 = None\n\n        # Dropout for input-hidden connection\n        self.dropout_in = nn.Dropout(p=dropout_in)\n\n        if rnn_type == \'tds\':\n            self.conv = TDSEncoder(input_dim=input_dim * n_stacks,\n                                   in_channel=conv_in_channel,\n                                   channels=conv_channels,\n                                   kernel_sizes=conv_kernel_sizes,\n                                   dropout=dropout,\n                                   bottleneck_dim=last_proj_dim)\n        elif rnn_type == \'gated_conv\':\n            self.conv = GatedConvEncoder(input_dim=input_dim * n_stacks,\n                                         in_channel=conv_in_channel,\n                                         channels=conv_channels,\n                                         kernel_sizes=conv_kernel_sizes,\n                                         dropout=dropout,\n                                         bottleneck_dim=last_proj_dim,\n                                         param_init=param_init)\n\n        elif \'conv\' in rnn_type:\n            assert n_stacks == 1 and n_splices == 1\n            self.conv = ConvEncoder(input_dim,\n                                    in_channel=conv_in_channel,\n                                    channels=conv_channels,\n                                    kernel_sizes=conv_kernel_sizes,\n                                    strides=conv_strides,\n                                    poolings=conv_poolings,\n                                    dropout=0.,\n                                    batch_norm=conv_batch_norm,\n                                    layer_norm=conv_layer_norm,\n                                    residual=False,\n                                    bottleneck_dim=conv_bottleneck_dim,\n                                    param_init=param_init)\n        else:\n            self.conv = None\n\n        if self.conv is None:\n            self._odim = input_dim * n_splices * n_stacks\n        else:\n            self._odim = self.conv.output_dim\n            subsample_list = [1] * self.n_layers\n            logger.warning(\'Subsampling is automatically ignored because CNN layers are used before RNN layers.\')\n\n        self.padding = Padding(bidirectional_sum_fwd_bwd=bidirectional_sum_fwd_bwd)\n\n        if rnn_type not in [\'conv\', \'tds\', \'gated_conv\']:\n            self.rnn = nn.ModuleList()\n            if self.latency_controlled:\n                self.rnn_bwd = nn.ModuleList()\n            self.dropout = nn.Dropout(p=dropout)\n            self.proj = None\n            if n_projs > 0:\n                self.proj = nn.ModuleList()\n\n            # subsample\n            self.subsample_layer = None\n            if subsample_type == \'max_pool\' and np.prod(subsample_list) > 1:\n                self.subsample_layer = nn.ModuleList([MaxpoolSubsampler(subsample_list[lth])\n                                                      for lth in range(n_layers)])\n            elif subsample_type == \'concat\' and np.prod(subsample_list) > 1:\n                self.subsample_layer = nn.ModuleList([ConcatSubsampler(subsample_list[lth], n_units * self.n_dirs)\n                                                      for lth in range(n_layers)])\n            elif subsample_type == \'drop\' and np.prod(subsample_list) > 1:\n                self.subsample_layer = nn.ModuleList([DropSubsampler(subsample_list[lth])\n                                                      for lth in range(n_layers)])\n            elif subsample_type == \'1dconv\' and np.prod(subsample_list) > 1:\n                self.subsample_layer = nn.ModuleList([Conv1dSubsampler(subsample_list[lth], n_units * self.n_dirs)\n                                                      for lth in range(n_layers)])\n\n            for lth in range(n_layers):\n                if \'lstm\' in rnn_type:\n                    rnn_i = nn.LSTM\n                elif \'gru\' in rnn_type:\n                    rnn_i = nn.GRU\n                else:\n                    raise ValueError(\'rnn_type must be ""(conv_)(b)lstm"" or ""(conv_)(b)gru"".\')\n\n                if self.latency_controlled:\n                    self.rnn += [rnn_i(self._odim, n_units, 1, batch_first=True)]\n                    self.rnn_bwd += [rnn_i(self._odim, n_units, 1, batch_first=True)]\n                else:\n                    self.rnn += [rnn_i(self._odim, n_units, 1, batch_first=True,\n                                       bidirectional=self.bidirectional)]\n                self._odim = n_units if bidirectional_sum_fwd_bwd else n_units * self.n_dirs\n\n                # Projection layer\n                if self.proj is not None:\n                    if lth != n_layers - 1:\n                        self.proj += [nn.Linear(n_units * self.n_dirs, n_projs)]\n                        self._odim = n_projs\n\n                # Task specific layer\n                if lth == n_layers_sub1 - 1 and task_specific_layer:\n                    assert not self.latency_controlled\n                    self.rnn_sub1 = rnn_i(self._odim, n_units, 1,\n                                          batch_first=True,\n                                          bidirectional=self.bidirectional)\n                    if last_proj_dim > 0 and last_proj_dim != self.output_dim:\n                        self.bridge_sub1 = nn.Linear(n_units, last_proj_dim)\n                if lth == n_layers_sub2 - 1 and task_specific_layer:\n                    assert not self.latency_controlled\n                    self.rnn_sub2 = rnn_i(self._odim, n_units, 1,\n                                          batch_first=True,\n                                          bidirectional=self.bidirectional)\n                    if last_proj_dim > 0 and last_proj_dim != self.output_dim:\n                        self.bridge_sub2 = nn.Linear(n_units, last_proj_dim)\n\n            if last_proj_dim > 0 and last_proj_dim != self.output_dim:\n                self.bridge = nn.Linear(self._odim, last_proj_dim)\n                self._odim = last_proj_dim\n\n        # calculate subsampling factor\n        self._factor = 1\n        if self.conv is not None:\n            self._factor *= self.conv.subsampling_factor\n        self._factor *= np.prod(subsample_list)\n\n        self.reset_parameters(param_init)\n\n        # for streaming inference\n        self.reset_cache()\n\n    @staticmethod\n    def add_args(parser, args):\n        group = parser.add_argument_group(""RNN encoder"")\n        if \'conv\' in args.enc_type:\n            parser = ConvEncoder.add_args(parser, args)\n        group.add_argument(\'--enc_n_units\', type=int, default=512,\n                           help=\'number of units in each encoder RNN layer\')\n        group.add_argument(\'--enc_n_projs\', type=int, default=0,\n                           help=\'number of units in the projection layer after each encoder RNN layer\')\n        group.add_argument(\'--bidirectional_sum_fwd_bwd\', type=strtobool, default=False,\n                           help=\'sum forward and backward RNN outputs for dimension reduction\')\n        # streaming\n        group.add_argument(\'--lc_chunk_size_left\', type=int, default=0,\n                           help=\'left chunk size for latency-controlled RNN encoder\')\n        group.add_argument(\'--lc_chunk_size_right\', type=int, default=0,\n                           help=\'right chunk size for latency-controlled RNN encoder\')\n        return parser\n\n    def reset_parameters(self, param_init):\n        """"""Initialize parameters with uniform distribution.""""""\n        logger.info(\'===== Initialize %s with uniform distribution =====\' % self.__class__.__name__)\n        for n, p in self.named_parameters():\n            if \'conv\' in n or \'tds\' in n or \'gated_conv\' in n:\n                continue  # for CNN layers before RNN layers\n            if p.dim() == 1:\n                nn.init.constant_(p, 0.)  # bias\n                logger.info(\'Initialize %s with %s / %.3f\' % (n, \'constant\', 0.))\n            elif p.dim() in [2, 4]:\n                nn.init.uniform_(p, a=-param_init, b=param_init)\n                logger.info(\'Initialize %s with %s / %.3f\' % (n, \'uniform\', param_init))\n            else:\n                raise ValueError(n)\n\n    def reset_cache(self):\n        self.fwd_states = [None] * self.n_layers\n        logger.debug(\'Reset cache.\')\n\n    def forward(self, xs, xlens, task, use_cache=False, streaming=False):\n        """"""Forward computation.\n\n        Args:\n            xs (FloatTensor): `[B, T, input_dim]`\n            xlens (list): A list of length `[B]`\n            task (str): all or ys or ys_sub1 or ys_sub2\n            use_cache (bool): use the cached forward encoder state in the previous chunk as the initial state\n            streaming (bool): streaming encoding\n        Returns:\n            eouts (dict):\n                xs (FloatTensor): `[B, T // prod(subsample), n_units (*2)]`\n                xlens (IntTensor): `[B]`\n                xs_sub1 (FloatTensor): `[B, T // prod(subsample), n_units (*2)]`\n                xlens_sub1 (IntTensor): `[B]`\n                xs_sub2 (FloatTensor): `[B, T // prod(subsample), n_units (*2)]`\n                xlens_sub2 (IntTensor): `[B]`\n\n        """"""\n        eouts = {\'ys\': {\'xs\': None, \'xlens\': None},\n                 \'ys_sub1\': {\'xs\': None, \'xlens\': None},\n                 \'ys_sub2\': {\'xs\': None, \'xlens\': None}}\n\n        # Sort by lenghts in the descending order for pack_padded_sequence\n        if not self.latency_controlled:\n            xlens, perm_ids = torch.IntTensor(xlens).sort(0, descending=True)\n            xs = xs[perm_ids]\n            _, perm_ids_unsort = perm_ids.sort()\n\n        # Dropout for inputs-hidden connection\n        xs = self.dropout_in(xs)\n\n        # Path through CNN blocks before RNN layers\n        if self.conv is not None:\n            xs, xlens = self.conv(xs, xlens)\n            if self.rnn_type in [\'conv\', \'tds\', \'gated_conv\']:\n                eouts[\'ys\'][\'xs\'] = xs\n                eouts[\'ys\'][\'xlens\'] = xlens\n                return eouts\n\n        if not use_cache:\n            self.reset_cache()\n\n        if self.latency_controlled:\n            # Flip the layer and time loop\n            xs, xlens, xs_sub1 = self._forward_streaming(xs, xlens, streaming)\n            xlens_sub1 = xlens.clone()\n        else:\n            for lth in range(self.n_layers):\n                self.rnn[lth].flatten_parameters()  # for multi-GPUs\n                xs, self.fwd_states[lth] = self.padding(xs, xlens, self.rnn[lth],\n                                                        prev_state=self.fwd_states[lth])\n                xs = self.dropout(xs)\n\n                # Pick up outputs in the sub task before the projection layer\n                if lth == self.n_layers_sub1 - 1:\n                    xs_sub1, xlens_sub1 = self.sub_module(xs, xlens, perm_ids_unsort, \'sub1\')\n                    if task == \'ys_sub1\':\n                        eouts[task][\'xs\'], eouts[task][\'xlens\'] = xs_sub1, xlens_sub1\n                        return eouts\n                if lth == self.n_layers_sub2 - 1:\n                    xs_sub2, xlens_sub2 = self.sub_module(xs, xlens, perm_ids_unsort, \'sub2\')\n                    if task == \'ys_sub2\':\n                        eouts[task][\'xs\'], eouts[task][\'xlens\'] = xs_sub2, xlens_sub2\n                        return eouts\n\n                # NOTE: Exclude the last layer\n                if lth != self.n_layers - 1:\n                    # Projection layer -> Subsampling\n                    if self.proj is not None:\n                        xs = torch.tanh(self.proj[lth](xs))\n                    if self.subsample_layer is not None:\n                        xs, xlens = self.subsample_layer[lth](xs, xlens)\n\n        # Bridge layer\n        if self.bridge is not None:\n            xs = self.bridge(xs)\n\n        # Unsort\n        if not self.latency_controlled:\n            xs = xs[perm_ids_unsort]\n            xlens = xlens[perm_ids_unsort]\n\n        if task in [\'all\', \'ys\']:\n            eouts[\'ys\'][\'xs\'], eouts[\'ys\'][\'xlens\'] = xs, xlens\n        if self.n_layers_sub1 >= 1 and task == \'all\':\n            eouts[\'ys_sub1\'][\'xs\'], eouts[\'ys_sub1\'][\'xlens\'] = xs_sub1, xlens_sub1\n        if self.n_layers_sub2 >= 1 and task == \'all\':\n            eouts[\'ys_sub2\'][\'xs\'], eouts[\'ys_sub2\'][\'xlens\'] = xs_sub2, xlens_sub2\n        return eouts\n\n    def _forward_streaming(self, xs, xlens, streaming, task=\'all\'):\n        """"""Streaming encoding for the latency-controlled bidirectional encoder.\n\n        Args:\n            xs (FloatTensor): `[B, T, n_units]`\n        Returns:\n            xs (FloatTensor): `[B, T, n_units]`\n\n        """"""\n        N_l = self.chunk_size_left // self.subsampling_factor\n        N_r = self.chunk_size_right // self.subsampling_factor\n\n        xs_sub1 = None\n\n        # full context BPTT\n        if N_l < 0:\n            for lth in range(self.n_layers):\n                self.rnn[lth].flatten_parameters()  # for multi-GPUs\n                self.rnn_bwd[lth].flatten_parameters()  # for multi-GPUs\n                # bwd\n                xs_bwd = torch.flip(xs, dims=[1])\n                xs_bwd, _ = self.rnn_bwd[lth](xs_bwd, hx=None)\n                xs_bwd = torch.flip(xs_bwd, dims=[1])\n                # fwd\n                xs_fwd, _ = self.rnn[lth](xs, hx=None)\n                if self.bidir_sum:\n                    xs = xs_fwd + xs_bwd\n                else:\n                    xs = torch.cat([xs_fwd, xs_bwd], dim=-1)\n                xs = self.dropout(xs)\n\n                # Pick up outputs in the sub task before the projection layer\n                if lth == self.n_layers_sub1 - 1:\n                    xs_sub1 = xs.clone()\n                    if self.bridge_sub1 is not None:\n                        xs_sub1 = self.bridge_sub1(xs_sub1)\n                    if task == \'ys_sub1\':\n                        return None, xlens, xs_sub1\n\n                # NOTE: Exclude the last layer\n                if lth != self.n_layers - 1:\n                    # Projection layer -> Subsampling\n                    if self.proj is not None:\n                        xs = torch.tanh(self.proj[lth](xs))\n                    if self.subsample_layer is not None:\n                        xs, xlens = self.subsample_layer[lth](xs, xlens)\n\n            return xs, xlens, xs_sub1\n\n        bs, xmax, input_dim = xs.size()\n        n_chunks = 1 if streaming else math.ceil(xmax / N_l)\n        xlens = torch.IntTensor(bs).fill_(N_l if streaming else xmax)\n\n        xs_chunks = []\n        xs_chunks_sub1 = []\n        for t in range(0, N_l * n_chunks, N_l):\n            xs_chunk = xs[:, t:t + (N_l + N_r)]\n            for lth in range(self.n_layers):\n                self.rnn[lth].flatten_parameters()  # for multi-GPUs\n                self.rnn_bwd[lth].flatten_parameters()  # for multi-GPUs\n                # bwd\n                xs_chunk_bwd = torch.flip(xs_chunk, dims=[1])\n                xs_chunk_bwd, _ = self.rnn_bwd[lth](xs_chunk_bwd, hx=None)\n                xs_chunk_bwd = torch.flip(xs_chunk_bwd, dims=[1])  # `[B, N_l+N_r, n_units]`\n                # fwd\n                if xs_chunk.size(1) <= N_l:\n                    xs_chunk_fwd, self.fwd_states[lth] = self.rnn[lth](xs_chunk, hx=self.fwd_states[lth])\n                else:\n                    xs_chunk_fwd1, self.fwd_states[lth] = self.rnn[lth](xs_chunk[:, :N_l], hx=self.fwd_states[lth])\n                    xs_chunk_fwd2, _ = self.rnn[lth](xs_chunk[:, N_l:], hx=self.fwd_states[lth])\n                    xs_chunk_fwd = torch.cat([xs_chunk_fwd1, xs_chunk_fwd2], dim=1)  # `[B, N_l+N_r, n_units]`\n                    # NOTE: xs_chunk_fwd2 is for xs_chunk_bwd in the next layer\n                if self.bidir_sum:\n                    xs_chunk = xs_chunk_fwd + xs_chunk_bwd\n                else:\n                    xs_chunk = torch.cat([xs_chunk_fwd, xs_chunk_bwd], dim=-1)\n                xs_chunk = self.dropout(xs_chunk)\n\n                # Pick up outputs in the sub task before the projection layer\n                if lth == self.n_layers_sub1 - 1:\n                    xs_chunk_sub1 = xs_chunk.clone()\n                    if self.bridge_sub1 is not None:\n                        xs_chunk_sub1 = self.bridge_sub1(xs_chunk_sub1)\n                    if task == \'ys_sub1\':\n                        return None, xlens, xs_chunk_sub1\n\n                # NOTE: Exclude the last layer\n                if lth != self.n_layers - 1:\n                    # Projection layer -> Subsampling\n                    if self.proj is not None:\n                        xs_chunk = torch.tanh(self.proj[lth](xs_chunk))\n                    if self.subsample_layer is not None:\n                        xs_chunk, xlens = self.subsample_layer[lth](xs_chunk, xlens)\n\n            xs_chunks.append(xs_chunk[:, :N_l])\n            if self.n_layers_sub1 > 0:\n                xs_chunks_sub1.append(xs_chunk_sub1[:, :N_l])\n        xs = torch.cat(xs_chunks, dim=1)\n        if self.n_layers_sub1 > 0:\n            xs_sub1 = torch.cat(xs_chunks_sub1, dim=1)\n\n        return xs, xlens, xs_sub1\n\n    def sub_module(self, xs, xlens, perm_ids_unsort, module=\'sub1\'):\n        if self.task_specific_layer:\n            getattr(self, \'rnn_\' + module).flatten_parameters()  # for multi-GPUs\n            xs_sub, _ = self.padding(xs, xlens, getattr(self, \'rnn_\' + module))\n            xs_sub = self.dropout(xs_sub)\n        else:\n            xs_sub = xs.clone()[perm_ids_unsort]\n        if getattr(self, \'bridge_\' + module) is not None:\n            xs_sub = getattr(self, \'bridge_\' + module)(xs_sub)\n        xlens_sub = xlens[perm_ids_unsort]\n        return xs_sub, xlens_sub\n\n\nclass Padding(nn.Module):\n    """"""Padding variable length of sequences.""""""\n\n    def __init__(self, bidirectional_sum_fwd_bwd):\n        super(Padding, self).__init__()\n        self.bidir_sum = bidirectional_sum_fwd_bwd\n\n    def forward(self, xs, xlens, rnn, prev_state=None):\n        xs = pack_padded_sequence(xs, xlens.tolist(), batch_first=True)\n        xs, state = rnn(xs, hx=prev_state)\n        xs = pad_packed_sequence(xs, batch_first=True)[0]\n        if self.bidir_sum:\n            assert rnn.bidirectional\n            half = xs.size(-1) // 2\n            xs = xs[:, :, :half] + xs[:, :, half:]\n        return xs, state\n\n\nclass MaxpoolSubsampler(nn.Module):\n    """"""Subsample by max-pooling input frames.""""""\n\n    def __init__(self, factor):\n        super(MaxpoolSubsampler, self).__init__()\n\n        self.factor = factor\n        if factor > 1:\n            self.max_pool = nn.MaxPool1d(1, stride=factor, ceil_mode=True)\n\n    def forward(self, xs, xlens):\n        if self.factor == 1:\n            return xs, xlens\n\n        xs = self.max_pool(xs.transpose(2, 1)).transpose(2, 1).contiguous()\n\n        xlens //= self.factor\n        return xs, xlens\n\n\nclass Conv1dSubsampler(nn.Module):\n    """"""Subsample by 1d convolution and max-pooling.""""""\n\n    def __init__(self, factor, n_units, conv_kernel_size=5):\n        super(Conv1dSubsampler, self).__init__()\n\n        assert conv_kernel_size % 2 == 1, ""Kernel size should be odd for \'same\' conv.""\n        self.factor = factor\n        if factor > 1:\n            self.conv1d = nn.Conv1d(in_channels=n_units,\n                                    out_channels=n_units,\n                                    kernel_size=conv_kernel_size,\n                                    stride=1,\n                                    padding=(conv_kernel_size - 1) // 2)\n            self.max_pool = nn.MaxPool1d(1, stride=factor, ceil_mode=True)\n\n    def forward(self, xs, xlens):\n        if self.factor == 1:\n            return xs, xlens\n\n        xs = torch.relu(self.conv1d(xs.transpose(2, 1)))\n        xs = self.max_pool(xs).transpose(2, 1).contiguous()\n\n        xlens //= self.factor\n        return xs, xlens\n\n\nclass DropSubsampler(nn.Module):\n    """"""Subsample by droping input frames.""""""\n\n    def __init__(self, factor):\n        super(DropSubsampler, self).__init__()\n\n        self.factor = factor\n\n    def forward(self, xs, xlens):\n        if self.factor == 1:\n            return xs, xlens\n\n        xs = xs[:, ::self.factor, :]\n\n        xlens = [max(1, (i + self.factor - 1) // self.factor) for i in xlens]\n        xlens = torch.IntTensor(xlens)\n        return xs, xlens\n\n\nclass ConcatSubsampler(nn.Module):\n    """"""Subsample by concatenating successive input frames.""""""\n\n    def __init__(self, factor, n_units):\n        super(ConcatSubsampler, self).__init__()\n\n        self.factor = factor\n        if factor > 1:\n            self.proj = nn.Linear(n_units * factor, n_units)\n\n    def forward(self, xs, xlens):\n        if self.factor == 1:\n            return xs, xlens\n\n        xs = xs.transpose(1, 0).contiguous()\n        xs = [torch.cat([xs[t - r:t - r + 1] for r in range(self.factor - 1, -1, -1)], dim=-1)\n              for t in range(xs.size(0)) if (t + 1) % self.factor == 0]\n        xs = torch.cat(xs, dim=0).transpose(1, 0)\n        # NOTE: Exclude the last frames if the length is not divisible\n\n        xs = torch.relu(self.proj(xs))\n        xlens //= self.factor\n        return xs, xlens\n\n\nclass NiN(nn.Module):\n    """"""Network in network.""""""\n\n    def __init__(self, dim):\n        super(NiN, self).__init__()\n\n        self.conv = nn.Conv2d(in_channels=dim,\n                              out_channels=dim,\n                              kernel_size=1,\n                              stride=1,\n                              padding=0)\n        self.batch_norm = nn.BatchNorm2d(dim)\n\n    def forward(self, xs):\n        # 1*1 conv + batch normalization + ReLU\n        xs = xs.contiguous().transpose(2, 1).unsqueeze(3)  # `[B, n_unis (*2), T, 1]`\n        # NOTE: consider feature dimension as input channel\n        xs = torch.relu(self.batch_norm(self.conv(xs)))  # `[B, n_unis (*2), T, 1]`\n        xs = xs.transpose(2, 1).squeeze(3)  # `[B, T, n_unis (*2)]`\n        return xs\n'"
neural_sp/models/seq2seq/encoders/tds.py,4,"b'#! /usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n# Copyright 2019 Kyoto University (Hirofumi Inaguma)\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n\n""""""TDS encoder.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom collections import OrderedDict\nimport logging\nimport math\nimport torch\nimport torch.nn as nn\n\nfrom neural_sp.models.seq2seq.encoders.conv import parse_cnn_config\nfrom neural_sp.models.seq2seq.encoders.encoder_base import EncoderBase\n\nlogger = logging.getLogger(__name__)\n\n\nclass TDSEncoder(EncoderBase):\n    """"""TDS (tim-depth separable convolutional) encoder.\n\n    Args:\n        input_dim (int) dimension of input features (freq * channel)\n        in_channel (int) number of channels of input features\n        channels (list) number of channles in TDS layers\n        kernel_sizes (list) size of kernels in TDS layers\n        strides (list): strides in TDS layers\n        poolings (list) size of poolings in TDS layers\n        dropout (float) probability to drop nodes in hidden-hidden connection\n        batch_norm (bool): if True, apply batch normalization\n        bottleneck_dim (int): dimension of the bottleneck layer after the last layer\n\n    """"""\n\n    def __init__(self,\n                 input_dim,\n                 in_channel,\n                 channels,\n                 kernel_sizes,\n                 dropout,\n                 bottleneck_dim=0):\n\n        super(TDSEncoder, self).__init__()\n\n        (channels, kernel_sizes, _, _), _ = parse_cnn_config(channels, kernel_sizes, \'\', \'\')\n\n        self.in_channel = in_channel\n        assert input_dim % in_channel == 0\n        self.input_freq = input_dim // in_channel\n        self.bridge = None\n\n        assert len(channels) > 0\n        assert len(channels) == len(kernel_sizes)\n\n        layers = OrderedDict()\n        C_i = in_channel\n        in_freq = self.input_freq\n        for lth in range(len(channels)):\n            # subsample\n            if C_i != channels[lth]:\n                layers[\'subsample%d\' % lth] = SubsampelBlock(in_channel=C_i,\n                                                             out_channel=channels[lth],\n                                                             in_freq=in_freq,\n                                                             dropout=dropout)\n\n            # Conv\n            layers[\'tds%d_block%d\' % (channels[lth], lth)] = TDSBlock(channel=channels[lth],\n                                                                      kernel_size=kernel_sizes[lth][0],\n                                                                      in_freq=in_freq,\n                                                                      dropout=dropout)\n\n            C_i = channels[lth]\n\n        self._odim = int(C_i * in_freq)\n\n        if bottleneck_dim > 0:\n            self.bridge = nn.Linear(self._odim, bottleneck_dim)\n            self._odim = bottleneck_dim\n\n        self.layers = nn.Sequential(layers)\n\n        self._factor = 8\n\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        """"""Initialize parameters with uniform distribution.""""""\n        logger.info(\'===== Initialize %s with uniform distribution =====\' % self.__class__.__name__)\n        for n, p in self.named_parameters():\n            if p.dim() == 1:\n                nn.init.constant_(p, 0.)  # bias\n                logger.info(\'Initialize %s with %s / %.3f\' % (n, \'constant\', 0.))\n            elif p.dim() == 2:\n                fan_in = p.size(1)\n                nn.init.uniform_(p, a=-math.sqrt(4 / fan_in), b=math.sqrt(4 / fan_in))  # linear weight\n                logger.info(\'Initialize %s with %s / %.3f\' % (n, \'uniform\', math.sqrt(4 / fan_in)))\n            elif p.dim() == 4:\n                fan_in = p.size(1) * p[0][0].numel()\n                nn.init.uniform_(p, a=-math.sqrt(4 / fan_in), b=math.sqrt(4 / fan_in))  # conv weight\n                logger.info(\'Initialize %s with %s / %.3f\' % (n, \'uniform\', math.sqrt(4 / fan_in)))\n            else:\n                raise ValueError(n)\n\n    def forward(self, xs, xlens):\n        """"""Forward computation.\n\n        Args:\n            xs (FloatTensor): `[B, T, F]`\n            xlens (list): A list of length `[B]`\n        Returns:\n            xs (FloatTensor): `[B, T\', C_o * F]`\n            xlens (list): A list of length `[B]`\n\n        """"""\n        B, T, F = xs.size()\n        xs = xs.contiguous().view(B, T, self.in_channel, F // self.in_channel).transpose(2, 1)\n        # `[B, C_i, T, F // C_i]`\n\n        xs = self.layers(xs)  # `[B, C_o, T, F]`\n        B, C_o, T, F = xs.size()\n        xs = xs.transpose(2, 1).contiguous().view(B, T, -1)  # `[B, T, C_o * F]`\n\n        # Bridge layer\n        if self.bridge is not None:\n            xs = self.bridge(xs)\n\n        # Update xlens\n        xlens //= 8\n\n        return xs, xlens\n\n\nclass TDSBlock(nn.Module):\n    """"""TDS block.\n\n    Args:\n        channel (int):\n        kernel_size (int):\n        in_freq (int):\n        dropout (float):\n\n    """"""\n\n    def __init__(self, channel, kernel_size, in_freq, dropout):\n        super().__init__()\n\n        self.channel = channel\n        self.in_freq = in_freq\n\n        self.dropout = nn.Dropout(p=dropout)\n\n        self.conv2d = nn.Conv2d(in_channels=channel,\n                                out_channels=channel,\n                                kernel_size=(kernel_size, 1),\n                                stride=(1, 1),\n                                padding=(kernel_size // 2, 0),\n                                groups=channel)  # depthwise\n        self.norm1 = nn.LayerNorm(in_freq * channel, eps=1e-12)\n\n        # second block\n        self.conv1d_1 = nn.Conv2d(in_channels=in_freq * channel,\n                                  out_channels=in_freq * channel,\n                                  kernel_size=1,\n                                  stride=1,\n                                  padding=0)\n        self.conv1d_2 = nn.Conv2d(in_channels=in_freq * channel,\n                                  out_channels=in_freq * channel,\n                                  kernel_size=1,\n                                  stride=1,\n                                  padding=0)\n        self.norm2 = nn.LayerNorm(in_freq * channel, eps=1e-12)\n\n    def forward(self, xs):\n        """"""Forward computation.\n        Args:\n            xs (FloatTensor): `[B, C_i, T, F]`\n        Returns:\n            out (FloatTensor): `[B, C_o, T, F]`\n\n        """"""\n        B, C_i, T, F = xs.size()\n\n        # first block\n        residual = xs\n        xs = self.dropout(torch.relu(self.conv2d(xs)))\n        raise ValueError(xs.size())\n        xs = xs + residual  # `[B, C_o, T, F]`\n\n        # layer normalization\n        B, C_o, T, F = xs.size()\n        xs = xs.transpose(2, 1).contiguous().view(B, T, -1)  # `[B, T, C_o * F]`\n        xs = self.norm1(xs)\n        xs = xs.contiguous().transpose(2, 1).unsqueeze(3)  # `[B, C_o * F, T, 1]`\n\n        # second block\n        residual = xs\n        self.dropout(torch.relu(self.conv1d_1(xs)))\n        xs = self.dropout(self.conv1d_2(xs)) + residual  # `[B, C_o * F, T, 1]`\n\n        # layer normalization\n        xs = xs.unsqueeze(3)  # `[B, C_o * F, T]`\n        xs = xs.transpose(2, 1).contiguous().view(B, T, -1)  # `[B, T, C_o * F]`\n        xs = self.norm2(xs)\n        xs = xs.view(B, T, C_o, F).contiguous().transpose(2, 1)\n\n        return xs\n\n\nclass SubsampelBlock(nn.Module):\n    def __init__(self, in_channel, out_channel, in_freq, dropout):\n        super().__init__()\n\n        self.conv1d = nn.Conv2d(in_channels=in_channel,\n                                out_channels=out_channel,\n                                kernel_size=(2, 1),\n                                stride=(2, 1),\n                                padding=(0, 0))\n        self.dropout = nn.Dropout(p=dropout)\n        self.norm = nn.LayerNorm(in_freq * out_channel, eps=1e-12)\n\n    def forward(self, xs):\n        """"""Forward computation.\n        Args:\n            xs (FloatTensor): `[B, C_i, T, F]`\n        Returns:\n            out (FloatTensor): `[B, C_o, T, F]`\n\n        """"""\n        bs, _, time, _ = xs.size()\n\n        xs = self.dropout(torch.relu(self.conv1d(xs)))\n\n        # layer normalization\n        bs, C_o, time, F = xs.size()\n        xs = xs.transpose(2, 1).contiguous().view(bs, time, -1)  # `[B, T, C_o * F]`\n        xs = self.norm(xs)\n        xs = xs.view(bs, time, C_o, F).contiguous().transpose(2, 1)\n\n        return xs\n'"
neural_sp/models/seq2seq/encoders/transformer.py,10,"b'#! /usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n# Copyright 2019 Kyoto University (Hirofumi Inaguma)\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n\n""""""Transformer encoder.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport copy\nimport logging\nimport math\nimport random\nimport torch\nimport torch.nn as nn\n\nfrom neural_sp.models.modules.initialization import init_like_transformer_xl\nfrom neural_sp.models.modules.multihead_attention import MultiheadAttentionMechanism as MHA\nfrom neural_sp.models.modules.positional_embedding import PositionalEncoding\nfrom neural_sp.models.modules.positional_embedding import XLPositionalEmbedding\nfrom neural_sp.models.modules.positionwise_feed_forward import PositionwiseFeedForward as FFN\nfrom neural_sp.models.modules.relative_multihead_attention import RelativeMultiheadAttentionMechanism as RelMHA\nfrom neural_sp.models.seq2seq.encoders.conv import ConvEncoder\nfrom neural_sp.models.seq2seq.encoders.encoder_base import EncoderBase\nfrom neural_sp.models.torch_utils import make_pad_mask\nfrom neural_sp.models.torch_utils import tensor2np\n\nrandom.seed(1)\n\nlogger = logging.getLogger(__name__)\n\n\nclass TransformerEncoder(EncoderBase):\n    """"""Transformer encoder.\n\n    Args:\n        input_dim (int): dimension of input features (freq * channel)\n        enc_type (str): type of encoder\n        n_heads (int): number of heads for multi-head attention\n        n_layers (int): number of blocks\n        n_layers_sub1 (int): number of layers in the 1st auxiliary task\n        n_layers_sub2 (int): number of layers in the 2nd auxiliary task\n        d_model (int): dimension of MultiheadAttentionMechanism\n        d_ff (int): dimension of PositionwiseFeedForward\n        d_ff_bottleneck_dim (int): bottleneck dimension for the light-weight FFN layer\n        last_proj_dim (int): dimension of the last projection layer\n        pe_type (str): type of positional encoding\n        layer_norm_eps (float): epsilon value for layer normalization\n        ffn_activation (str): nonolinear function for PositionwiseFeedForward\n        dropout_in (float): dropout probability for input-hidden connection\n        dropout (float): dropout probabilities for linear layers\n        dropout_att (float): dropout probabilities for attention distributions\n        dropout_layer (float): LayerDrop probability for layers\n        n_stacks (int): number of frames to stack\n        n_splices (int): frames to splice. Default is 1 frame.\n        conv_in_channel (int): number of channels of input features\n        conv_channels (int): number of channles in the CNN blocks\n        conv_kernel_sizes (list): size of kernels in the CNN blocks\n        conv_strides (list): number of strides in the CNN blocks\n        conv_poolings (list): size of poolings in the CNN blocks\n        conv_batch_norm (bool): apply batch normalization only in the CNN blocks\n        conv_layer_norm (bool): apply layer normalization only in the CNN blocks\n        conv_bottleneck_dim (int): dimension of the bottleneck layer between CNN and self-attention layers\n        conv_param_init (float): only for CNN layers before Transformer layers\n        task_specific_layer (bool): add a task specific layer for each sub task\n        param_init (str): parameter initialization method\n        chunk_size_left (int): left chunk size for time-restricted Transformer encoder\n        chunk_size_current (int): current chunk size for time-restricted Transformer encoder\n        chunk_size_right (int): right chunk size for time-restricted Transformer encoder\n\n    """"""\n\n    def __init__(self, input_dim, enc_type, n_heads,\n                 n_layers, n_layers_sub1, n_layers_sub2,\n                 d_model, d_ff, d_ff_bottleneck_dim, last_proj_dim,\n                 pe_type, layer_norm_eps, ffn_activation,\n                 dropout_in, dropout, dropout_att, dropout_layer,\n                 n_stacks, n_splices,\n                 conv_in_channel, conv_channels, conv_kernel_sizes, conv_strides, conv_poolings,\n                 conv_batch_norm, conv_layer_norm, conv_bottleneck_dim, conv_param_init,\n                 task_specific_layer, param_init,\n                 chunk_size_left, chunk_size_current, chunk_size_right):\n\n        super(TransformerEncoder, self).__init__()\n\n        if n_layers_sub1 < 0 or (n_layers_sub1 > 1 and n_layers < n_layers_sub1):\n            raise ValueError(\'Set n_layers_sub1 between 1 to n_layers.\')\n        if n_layers_sub2 < 0 or (n_layers_sub2 > 1 and n_layers_sub1 < n_layers_sub2):\n            raise ValueError(\'Set n_layers_sub2 between 1 to n_layers_sub1.\')\n\n        self.d_model = d_model\n        self.n_layers = n_layers\n        self.n_heads = n_heads\n        self.pe_type = pe_type\n\n        # for streaming TransformerXL encoder\n        self.chunk_size_left = chunk_size_left\n        self.chunk_size_current = chunk_size_current\n        self.chunk_size_right = chunk_size_right\n        self.latency_controlled = chunk_size_left > 0 or chunk_size_current > 0 or chunk_size_right > 0\n        self.memory_transformer = (\'transformer_xl\' in enc_type)\n        self.mem_len = chunk_size_left\n        self.scale = math.sqrt(d_model)\n        if self.memory_transformer:\n            assert pe_type == \'relative\'\n            assert chunk_size_left > 0\n            assert chunk_size_current > 0\n\n        # for hierarchical encoder\n        self.n_layers_sub1 = n_layers_sub1\n        self.n_layers_sub2 = n_layers_sub2\n        self.task_specific_layer = task_specific_layer\n\n        # for bridge layers\n        self.bridge = None\n        self.bridge_sub1 = None\n        self.bridge_sub2 = None\n\n        # for attention plot\n        self.aws_dict = {}\n        self.data_dict = {}\n\n        # Setting for CNNs\n        if conv_channels:\n            assert n_stacks == 1 and n_splices == 1\n            self.conv = ConvEncoder(input_dim,\n                                    in_channel=conv_in_channel,\n                                    channels=conv_channels,\n                                    kernel_sizes=conv_kernel_sizes,\n                                    strides=conv_strides,\n                                    poolings=conv_poolings,\n                                    dropout=0.,\n                                    batch_norm=conv_batch_norm,\n                                    layer_norm=conv_layer_norm,\n                                    layer_norm_eps=layer_norm_eps,\n                                    residual=False,\n                                    bottleneck_dim=d_model,\n                                    param_init=conv_param_init)\n            self._odim = self.conv.output_dim\n        else:\n            self.conv = None\n            self._odim = input_dim * n_splices * n_stacks\n            self.embed = nn.Linear(self._odim, d_model)\n\n        # calculate subsampling factor\n        self._factor = 1\n        if self.conv is not None:\n            self._factor *= self.conv.subsampling_factor\n\n        self.pos_emb = None\n        self.u = None\n        self.v = None\n        if self.memory_transformer:\n            self.pos_emb = XLPositionalEmbedding(d_model, dropout)\n            self.u = nn.Parameter(torch.Tensor(self.n_heads, self.d_model // self.n_heads))\n            self.v = nn.Parameter(torch.Tensor(self.n_heads, self.d_model // self.n_heads))\n            # NOTE: u and v are global parameters\n        elif pe_type == \'relative\':\n            self.pos_emb = XLPositionalEmbedding(d_model, dropout)  # TODO: dropout_in?\n        else:\n            self.pos_enc = PositionalEncoding(d_model, dropout_in, pe_type, param_init)\n\n        self.layers = nn.ModuleList([copy.deepcopy(TransformerEncoderBlock(\n            d_model, d_ff, n_heads, dropout, dropout_att, dropout_layer,\n            layer_norm_eps, ffn_activation, param_init,\n            relative_attention=self.pos_emb is not None,\n            d_ff_bottleneck_dim=d_ff_bottleneck_dim))\n            for _ in range(n_layers)])\n        self.norm_out = nn.LayerNorm(d_model, eps=layer_norm_eps)\n\n        self._odim = d_model\n\n        if n_layers_sub1 > 0:\n            if task_specific_layer:\n                self.layer_sub1 = TransformerEncoderBlock(\n                    d_model, d_ff, n_heads, dropout, dropout_att, dropout_layer,\n                    layer_norm_eps, ffn_activation, param_init,\n                    d_ff_bottleneck_dim=d_ff_bottleneck_dim)\n            self.norm_out_sub1 = nn.LayerNorm(d_model, eps=layer_norm_eps)\n            if last_proj_dim > 0 and last_proj_dim != self.output_dim:\n                self.bridge_sub1 = nn.Linear(self._odim, last_proj_dim)\n\n        if n_layers_sub2 > 0:\n            if task_specific_layer:\n                self.layer_sub2 = TransformerEncoderBlock(\n                    d_model, d_ff, n_heads, dropout, dropout_att, dropout_layer,\n                    layer_norm_eps, ffn_activation, param_init,\n                    d_ff_bottleneck_dim=d_ff_bottleneck_dim)\n            self.norm_out_sub2 = nn.LayerNorm(d_model, eps=layer_norm_eps)\n            if last_proj_dim > 0 and last_proj_dim != self.output_dim:\n                self.bridge_sub2 = nn.Linear(self._odim, last_proj_dim)\n\n        if last_proj_dim > 0 and last_proj_dim != self.output_dim:\n            self.bridge = nn.Linear(self._odim, last_proj_dim)\n            self._odim = last_proj_dim\n\n        self.reset_parameters(param_init)\n\n    @staticmethod\n    def add_args(parser, args):\n        """"""Add arguments.""""""\n        group = parser.add_argument_group(""Transformer encoder"")\n        if \'conv\' in args.enc_type:\n            parser = ConvEncoder.add_args(parser, args)\n        # Transformer common\n        if not hasattr(args, \'transformer_d_model\'):\n            group.add_argument(\'--transformer_d_model\', type=int, default=256,\n                               help=\'number of units in the MHA layer\')\n        if not hasattr(args, \'transformer_d_ff\'):\n            group.add_argument(\'--transformer_d_ff\', type=int, default=2048,\n                               help=\'number of units in the FFN layer\')\n        if not hasattr(args, \'transformer_d_ff_bottleneck_dim\'):\n            group.add_argument(\'--transformer_d_ff_bottleneck_dim\', type=int, default=0,\n                               help=\'bottleneck dimension in the FFN layer\')\n        if not hasattr(args, \'transformer_n_heads\'):\n            group.add_argument(\'--transformer_n_heads\', type=int, default=4,\n                               help=\'number of heads in the MHA layer\')\n        if not hasattr(args, \'transformer_layer_norm_eps\'):\n            group.add_argument(\'--transformer_layer_norm_eps\', type=float, default=1e-12,\n                               help=\'epsilon value for layer normalization\')\n        if not hasattr(args, \'transformer_ffn_activation\'):\n            group.add_argument(\'--transformer_ffn_activation\', type=str, default=\'relu\',\n                               choices=[\'relu\', \'gelu\', \'gelu_accurate\', \'glu\', \'swish\'],\n                               help=\'nonlinear activation for the FFN layer\')\n        if not hasattr(args, \'transformer_param_init\'):\n            group.add_argument(\'--transformer_param_init\', type=str, default=\'xavier_uniform\',\n                               choices=[\'xavier_uniform\', \'pytorch\'],\n                               help=\'parameter initializatin\')\n        # NOTE: These checks are important to avoid conflict with args in Transformer decoder\n\n        # Transformer encoder specific\n        group.add_argument(\'--transformer_enc_pe_type\', type=str, default=\'add\',\n                           choices=[\'add\', \'concat\', \'none\', \'relative\'],\n                           help=\'type of positional encoding for the Transformer encoder\')\n        group.add_argument(\'--dropout_enc_layer\', type=float, default=0.0,\n                           help=\'LayerDrop probability for Transformer encoder layers\')\n        # streaming\n        group.add_argument(\'--lc_chunk_size_left\', type=int, default=0,\n                           help=\'left chunk size for latency-controlled Transformer encoder\')\n        group.add_argument(\'--lc_chunk_size_current\', type=int, default=0,\n                           help=\'current chunk size (and hop size) for latency-controlled Transformer encoder\')\n        group.add_argument(\'--lc_chunk_size_right\', type=int, default=0,\n                           help=\'right chunk size for latency-controlled Transformer encoder\')\n        return parser\n\n    def reset_parameters(self, param_init):\n        """"""Initialize parameters.""""""\n        if self.memory_transformer:\n            logger.info(\'===== Initialize %s with normal distribution =====\' % self.__class__.__name__)\n            for n, p in self.named_parameters():\n                if \'conv\' in n:\n                    continue\n                init_like_transformer_xl(n, p, std=0.02)\n\n        elif param_init == \'xavier_uniform\':\n            logger.info(\'===== Initialize %s with Xavier uniform distribution =====\' % self.__class__.__name__)\n            if self.conv is None:\n                nn.init.xavier_uniform_(self.embed.weight)\n                nn.init.constant_(self.embed.bias, 0.)\n            if self.bridge is not None:\n                nn.init.xavier_uniform_(self.bridge.weight)\n                nn.init.constant_(self.bridge.bias, 0.)\n            if self.bridge_sub1 is not None:\n                nn.init.xavier_uniform_(self.bridge_sub1.weight)\n                nn.init.constant_(self.bridge_sub1.bias, 0.)\n            if self.bridge_sub2 is not None:\n                nn.init.xavier_uniform_(self.bridge_sub2.weight)\n                nn.init.constant_(self.bridge_sub2.bias, 0.)\n\n    def init_memory(self):\n        """"""Initialize memory.""""""\n        if self.device_id >= 0:\n            return [torch.empty(0, dtype=torch.float).cuda(self.device_id)\n                    for _ in range(self.n_layers)]\n        else:\n            return [torch.empty(0, dtype=torch.float)\n                    for _ in range(self.n_layers)]\n\n    def update_memory(self, memory_prev, hidden_states):\n        """"""Update memory.\n\n        Args:\n            memory_prev (list): length `n_layers`, each of which contains [B, L_prev, d_model]`\n            hidden_states (list): length `n_layers`, each of which contains [B, L, d_model]`\n        Returns:\n            new_mems (list): length `n_layers`, each of which contains `[B, mlen, d_model]`\n\n        """"""\n        assert len(hidden_states) == len(memory_prev)\n        mlen = memory_prev[0].size(1) if memory_prev[0].dim() > 1 else 0\n        qlen = hidden_states[0].size(1)\n\n        # There are `mlen + qlen` steps that can be cached into mems\n        # For the next step, the last `ext_len` of the `qlen` tokens\n        # will be used as the extended context. Hence, we only cache\n        # the tokens from `mlen + qlen - self.ext_len - self.mem_len`\n        # to `mlen + qlen - self.ext_len`.\n        with torch.no_grad():\n            new_mems = []\n            end_idx = mlen + qlen\n            start_idx = max(0, end_idx - (self.mem_len // self.subsampling_factor))\n            for m, h in zip(memory_prev, hidden_states):\n                cat = torch.cat([m, h], dim=1)  # `[B, mlen + qlen, d_model]`\n                new_mems.append(cat[:, start_idx:end_idx].detach())  # `[B, self.mem_len, d_model]`\n\n        return new_mems\n\n    def forward(self, xs, xlens, task, use_cache=False, streaming=False):\n        """"""Forward computation.\n\n        Args:\n            xs (FloatTensor): `[B, T, input_dim]`\n            xlens (list): `[B]`\n            task (str): not supported now\n            use_cache (bool):\n            streaming (bool): streaming encoding\n        Returns:\n            eouts (dict):\n                xs (FloatTensor): `[B, T, d_model]`\n                xlens (list): `[B]`\n\n        """"""\n        eouts = {\'ys\': {\'xs\': None, \'xlens\': None},\n                 \'ys_sub1\': {\'xs\': None, \'xlens\': None},\n                 \'ys_sub2\': {\'xs\': None, \'xlens\': None}}\n\n        N_l = self.chunk_size_left\n        N_c = self.chunk_size_current\n        N_r = self.chunk_size_right\n\n        if self.latency_controlled:\n            bs, xmax, idim = xs.size()\n            n_blocks = xmax // N_c\n            if xmax % N_c != 0:\n                n_blocks += 1\n            xs_tmp = xs.new_zeros(bs, n_blocks, N_l + N_c + N_r, idim)\n            xs_pad = torch.cat([xs.new_zeros(bs, N_l, idim),\n                                xs,\n                                xs.new_zeros(bs, N_r, idim)], dim=1)\n            for blc_id, t in enumerate(range(N_l, N_l + xmax, N_c)):\n                xs_chunk = xs_pad[:, t - N_l:t + (N_c + N_r)]\n                xs_tmp[:, blc_id, :xs_chunk.size(1), :] = xs_chunk\n            xs = xs_tmp.view(bs * n_blocks, N_l + N_c + N_r, idim)\n\n        if self.conv is None:\n            xs = self.embed(xs)\n        else:\n            # Path through CNN blocks\n            xs, xlens = self.conv(xs, xlens)\n\n        if not self.training:\n            self.data_dict[\'elens\'] = tensor2np(xlens)\n\n        if self.latency_controlled:\n            # streaming Transformer encoder\n            N_l = max(0, N_l // self.subsampling_factor)\n            N_c = N_c // self.subsampling_factor\n\n            emax = xmax // self.subsampling_factor\n            if xmax % self.subsampling_factor != 0:\n                emax += 1\n\n            pos_embs = None\n            if self.pe_type == \'relative\':\n                xs = xs * self.scale\n                pos_idxs = torch.arange(xs.size(1) - 1, -1, -1.0, dtype=torch.float)\n                pos_embs = self.pos_emb(pos_idxs, self.device_id)\n            else:\n                xs = self.pos_enc(xs, scale=True)\n\n            xx_mask = None  # NOTE: no mask\n            for lth, layer in enumerate(self.layers):\n                xs, xx_aws = layer(xs, xx_mask, pos_embs=pos_embs)\n                if not self.training:\n                    n_heads = xx_aws.size(1)\n                    xx_aws = xx_aws[:, :, N_l:N_l + N_c, N_l:N_l + N_c]\n                    xx_aws = xx_aws.view(bs, n_blocks, n_heads, N_c, N_c)\n                    xx_aws_center = xx_aws.new_zeros(bs, n_heads, emax, emax)\n                    for blc_id in range(n_blocks):\n                        offset = blc_id * N_c\n                        emax_blc = xx_aws_center[:, :, offset:offset + N_c].size(2)\n                        xx_aws_chunk = xx_aws[:, blc_id, :, :emax_blc, :emax_blc]\n                        xx_aws_center[:, :, offset:offset + N_c, offset:offset + N_c] = xx_aws_chunk\n                    self.aws_dict[\'xx_aws_layer%d\' % lth] = tensor2np(xx_aws_center)\n\n            # Extract the center region\n            xs = xs[:, N_l:N_l + N_c]  # `[B * n_blocks, N_c // subsampling_factor, d_model]`\n            xs = xs.contiguous().view(bs, -1, xs.size(2))\n            xs = xs[:, :emax]\n\n        else:\n            bs, xmax, idim = xs.size()\n\n            pos_embs = None\n            if self.pe_type == \'relative\':\n                xs = xs * self.scale\n                pos_idxs = torch.arange(xmax - 1, -1, -1.0, dtype=torch.float)\n                pos_embs = self.pos_emb(pos_idxs, self.device_id)\n            else:\n                xs = self.pos_enc(xs, scale=True)\n\n            # Create the self-attention mask\n            xx_mask = make_pad_mask(xlens, self.device_id).unsqueeze(2).repeat([1, 1, xmax])\n\n            for lth, layer in enumerate(self.layers):\n                xs, xx_aws = layer(xs, xx_mask, pos_embs=pos_embs)\n                if not self.training:\n                    self.aws_dict[\'xx_aws_layer%d\' % lth] = tensor2np(xx_aws)\n\n                # Pick up outputs in the sub task before the projection layer\n                if lth == self.n_layers_sub1 - 1:\n                    xs_sub1 = self.layer_sub1(xs, xx_mask)[0] if self.task_specific_layer else xs.clone()\n                    xs_sub1 = self.norm_out_sub1(xs_sub1)\n                    if self.bridge_sub1 is not None:\n                        xs_sub1 = self.bridge_sub1(xs_sub1)\n                    if task == \'ys_sub1\':\n                        eouts[task][\'xs\'], eouts[task][\'xlens\'] = xs_sub1, xlens\n                        return eouts\n                if lth == self.n_layers_sub2 - 1:\n                    xs_sub2 = self.layer_sub2(xs, xx_mask)[0] if self.task_specific_layer else xs.clone()\n                    xs_sub2 = self.norm_out_sub2(xs_sub2)\n                    if self.bridge_sub2 is not None:\n                        xs_sub2 = self.bridge_sub2(xs_sub2)\n                    if task == \'ys_sub2\':\n                        eouts[task][\'xs\'], eouts[task][\'xlens\'] = xs_sub2, xlens\n                        return eouts\n\n        xs = self.norm_out(xs)\n\n        # Bridge layer\n        if self.bridge is not None:\n            xs = self.bridge(xs)\n\n        if task in [\'all\', \'ys\']:\n            eouts[\'ys\'][\'xs\'], eouts[\'ys\'][\'xlens\'] = xs, xlens\n        if self.n_layers_sub1 >= 1 and task == \'all\':\n            eouts[\'ys_sub1\'][\'xs\'], eouts[\'ys_sub1\'][\'xlens\'] = xs_sub1, xlens\n        if self.n_layers_sub2 >= 1 and task == \'all\':\n            eouts[\'ys_sub2\'][\'xs\'], eouts[\'ys_sub2\'][\'xlens\'] = xs_sub2, xlens\n        return eouts\n\n\nclass TransformerEncoderBlock(nn.Module):\n    """"""A single layer of the Transformer encoder.\n\n    Args:\n        d_model (int): input dimension of MultiheadAttentionMechanism and PositionwiseFeedForward\n        d_ff (int): hidden dimension of PositionwiseFeedForward\n        n_heads (int): number of heads for multi-head attention\n        dropout (float): dropout probabilities for linear layers\n        dropout_att (float): dropout probabilities for attention distributions\n        dropout_layer (float): LayerDrop probability\n        layer_norm_eps (float): epsilon parameter for layer normalization\n        ffn_activation (str): nonolinear function for PositionwiseFeedForward\n        param_init (str): parameter initialization method\n        relative_attention (bool): relative postional encoding\n        d_ff_bottleneck_dim (int): bottleneck dimension for the light-weight FFN layer\n\n    """"""\n\n    def __init__(self, d_model, d_ff, n_heads,\n                 dropout, dropout_att, dropout_layer,\n                 layer_norm_eps, ffn_activation, param_init,\n                 relative_attention=False, d_ff_bottleneck_dim=0):\n        super(TransformerEncoderBlock, self).__init__()\n\n        self.n_heads = n_heads\n        self.relative_attention = relative_attention\n\n        # self-attention\n        self.norm1 = nn.LayerNorm(d_model, eps=layer_norm_eps)\n        mha = RelMHA if relative_attention else MHA\n        self.self_attn = mha(kdim=d_model,\n                             qdim=d_model,\n                             adim=d_model,\n                             odim=d_model,\n                             n_heads=n_heads,\n                             dropout=dropout_att,\n                             param_init=param_init)\n\n        # position-wise feed-forward\n        self.norm2 = nn.LayerNorm(d_model, eps=layer_norm_eps)\n        self.feed_forward = FFN(d_model, d_ff, dropout, ffn_activation, param_init,\n                                d_ff_bottleneck_dim)\n\n        self.dropout = nn.Dropout(dropout)\n        self.dropout_layer = dropout_layer\n\n    def forward(self, xs, xx_mask=None, pos_embs=None, memory=None, u=None, v=None):\n        """"""Transformer encoder layer definition.\n\n        Args:\n            xs (FloatTensor): `[B, T, d_model]`\n            xx_mask (ByteTensor): `[B, T, T]`\n            pos_embs (LongTensor): `[L, 1, d_model]`\n            memory (FloatTensor): `[B, mlen, d_model]`\n            u (FloatTensor): global parameter for relative positional embedding\n            v (FloatTensor): global parameter for relative positional embedding\n        Returns:\n            xs (FloatTensor): `[B, T, d_model]`\n            xx_aws (FloatTensor): `[B, H, T, T]`\n\n        """"""\n        if self.dropout_layer > 0 and self.training and random.random() >= self.dropout_layer:\n            return xs, None\n\n        # self-attention\n        residual = xs\n        xs = self.norm1(xs)\n        if self.relative_attention:\n            xs, xx_aws = self.self_attn(xs, xs, memory, pos_embs, xx_mask, u, v)  # k/q/m\n        else:\n            xs, xx_aws, _ = self.self_attn(xs, xs, xs, mask=xx_mask)  # k/v/q\n        xs = self.dropout(xs) + residual\n\n        # position-wise feed-forward\n        residual = xs\n        xs = self.norm2(xs)\n        xs = self.feed_forward(xs)\n        xs = self.dropout(xs) + residual\n\n        return xs, xx_aws\n'"
neural_sp/models/seq2seq/frontends/__init__.py,0,b''
neural_sp/models/seq2seq/frontends/frame_stacking.py,0,"b'#! /usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n# Copyright 2018 Kyoto University (Hirofumi Inaguma)\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n\n""""""Frame stacking.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\n\n\ndef stack_frame(feat, n_stacks, n_skips, dtype=np.float32):\n    """"""Stack & skip some frames. This implementation is based on\n\n       https://arxiv.org/abs/1507.06947.\n           Sak, Ha\xc5\x9fim, et al.\n           ""Fast and accurate recurrent neural network acoustic models for speech recognition.""\n           arXiv preprint arXiv:1507.06947 (2015).\n\n    Args:\n        feat (list): `[T, input_dim]`\n        n_stacks (int): the number of frames to stack\n        n_skips (int): the number of frames to skip\n        dtype ():\n    Returns:\n        stacked_feat (np.ndarray): `[floor(T / n_skips), input_dim * n_stacks]`\n\n    """"""\n    if n_stacks == 1 and n_stacks == 1:\n        return feat\n\n    if n_stacks < n_skips:\n        raise ValueError(\'n_skips must be less than n_stacks.\')\n\n    n_frames, input_dim = feat.shape\n    n_frames_new = (n_frames + 1) // n_skips\n\n    stacked_feat = np.zeros((n_frames_new, input_dim * n_stacks), dtype=dtype)\n    stack_count = 0\n    stack = []\n    for t, frame_t in enumerate(feat):\n        if t == len(feat) - 1:  # final frame\n            # Stack the final frame\n            stack.append(frame_t)\n\n            while stack_count != int(n_frames_new):\n                # Concatenate stacked frames\n                for i in range(len(stack)):\n                    stacked_feat[stack_count][input_dim * i:input_dim * (i + 1)] = stack[i]\n                stack_count += 1\n\n                # Delete some frames to skip\n                for _ in range(n_skips):\n                    if len(stack) != 0:\n                        stack.pop(0)\n\n        elif len(stack) < n_stacks:  # first & middle frames\n            # Stack some frames until stack is filled\n            stack.append(frame_t)\n\n        if len(stack) == n_stacks:\n            # Concatenate stacked frames\n            for i in range(n_stacks):\n                stacked_feat[stack_count][input_dim * i:input_dim * (i + 1)] = stack[i]\n            stack_count += 1\n\n            # Delete some frames to skip\n            for _ in range(n_skips):\n                stack.pop(0)\n\n    return stacked_feat\n'"
neural_sp/models/seq2seq/frontends/gaussian_noise.py,1,"b'#! /usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n# Copyright 2019 Kyoto University (Hirofumi Inaguma)\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n\n""""""Add Gaussian noise to input features.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport torch\n\n\ndef add_gaussian_noise(xs):\n    noise = torch.normal(torch.zeros(xs.shape[-1]), 0.075)\n    if xs.is_cuda:\n        noise = noise.cuda()\n    xs.data += noise\n    return xs\n'"
neural_sp/models/seq2seq/frontends/sequence_summary.py,3,"b'#! /usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n# Copyright 2019 Kyoto University (Hirofumi Inaguma)\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n\n""""""Sequence summary network.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport logging\nimport torch\nimport torch.nn as nn\n\nfrom neural_sp.models.torch_utils import make_pad_mask\n\nlogger = logging.getLogger(__name__)\n\n\nclass SequenceSummaryNetwork(nn.Module):\n    """"""Sequence summary network.\n\n    Args:\n        input_dim (int): dimension of input features\n        n_units (int):\n        n_layers (int):\n        bottleneck_dim (int): dimension of the last bottleneck layer\n        dropout (float): dropout probability\n        param_init (float):\n\n    """"""\n\n    def __init__(self,\n                 input_dim,\n                 n_units,\n                 n_layers,\n                 bottleneck_dim,\n                 dropout,\n                 param_init=0.1):\n\n        super(SequenceSummaryNetwork, self).__init__()\n\n        self.n_layers = n_layers\n\n        self.ssn = nn.ModuleList()\n        self.ssn += [nn.Linear(input_dim, n_units, bias=False)]\n        self.ssn += [nn.Dropout(p=dropout)]\n        for lth in range(1, n_layers - 1):\n            self.ssn += [nn.Linear(n_units, bottleneck_dim if lth == n_layers - 2 else n_units,\n                                   bias=False)]\n            self.ssn += [nn.Dropout(p=dropout)]\n        self.p = nn.Linear(bottleneck_dim, input_dim, bias=False)\n\n        self.reset_parameters(param_init)\n\n    def reset_parameters(self, param_init):\n        """"""Initialize parameters with uniform distribution.""""""\n        logger.info(\'===== Initialize %s with uniform distribution =====\' % self.__class__.__name__)\n        for n, p in self.named_parameters():\n            if p.dim() == 1:\n                nn.init.constant_(p, 0.)  # bias\n                logger.info(\'Initialize %s with %s / %.3f\' % (n, \'constant\', 0.))\n            elif p.dim() == 2:\n                nn.init.uniform_(p, a=-param_init, b=param_init)\n                logger.info(\'Initialize %s with %s / %.3f\' % (n, \'uniform\', param_init))\n            else:\n                raise ValueError(n)\n\n    def forward(self, xs, xlens):\n        """"""Forward computation.\n\n        Args:\n            xs (FloatTensor): `[B, T, input_dim (+\xce\x94, \xce\x94\xce\x94)]`\n            xlens (IntTensor): `[B]`\n        Returns:\n            xs (FloatTensor): `[B, T\', input_dim]`\n\n        """"""\n        bs, time = xs.size()[:2]\n\n        s = xs.clone()\n        for lth in range(self.n_layers - 1):\n            s = torch.tanh(self.ssn[lth](s))\n        s = self.ssn[self.n_layers - 1](s)  # `[B, T, input_dim]`\n\n        # padding\n        device_id = torch.cuda.device_of(next(self.parameters())).idx\n        mask = make_pad_mask(xlens, device_id).unsqueeze(2)\n        s = s.masked_fill_(mask == 0, 0)\n\n        # time average\n        s = s.sum(1) / xlens.float().cuda(device_id).unsqueeze(1)\n        xs = xs + self.p(s).unsqueeze(1)\n        return xs\n'"
neural_sp/models/seq2seq/frontends/spec_augment.py,0,"b'#! /usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n# Copyright 2019 Kyoto University (Hirofumi Inaguma)\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n\n""""""SpecAugment data augmentation.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\n\n\nclass SpecAugment(object):\n    """"""SpecAugment class.\n\n    Args:\n        F (int): parameter for frequency masking\n        T (int): parameter for time masking\n        n_freq_masks (int): number of frequency masks\n        n_time_masks (int): number of time masks\n        W (int): parameter for time warping\n        p (float): parameter for upperbound of the time mask\n        adaptive_number_ratio (float): adaptive multiplicity ratio for time masking\n        adaptive_size_ratio (float): adaptive size ratio for time masking\n        max_n_time_masks (int): maximum number of time masking\n\n    """"""\n\n    def __init__(self, F, T, n_freq_masks, n_time_masks, p=1.0, W=40,\n                 adaptive_number_ratio=0, adaptive_size_ratio=0,\n                 max_n_time_masks=20):\n\n        super(SpecAugment, self).__init__()\n\n        self.W = W\n        self.F = F\n        self.T = T\n        self.n_freq_masks = n_freq_masks\n        self.n_time_masks = n_time_masks\n        self.p = p\n\n        # adaptive SpecAugment\n        self.adaptive_number_ratio = adaptive_number_ratio\n        self.adaptive_size_ratio = adaptive_size_ratio\n        self.max_n_time_masks = max_n_time_masks\n\n        self._freq_mask = None\n        self._time_mask = None\n\n    def librispeech_basic(self):\n        self.W = 80\n        self.F = 27\n        self.T = 100\n        self.n_freq_masks = 1\n        self.n_time_masks = 1\n        self.p = 1.0\n\n    def librispeech_double(self):\n        self.W = 80\n        self.F = 27\n        self.T = 100\n        self.n_freq_masks = 2\n        self.n_time_masks = 2\n        self.p = 1.0\n\n    def switchboard_mild(self):\n        self.W = 40\n        self.F = 15\n        self.T = 70\n        self.n_freq_masks = 2\n        self.n_time_masks = 2\n        self.p = 0.2\n\n    def switchboard_strong(self):\n        self.W = 40\n        self.F = 27\n        self.T = 70\n        self.n_freq_masks = 2\n        self.n_time_masks = 2\n        self.p = 0.2\n\n    @property\n    def freq_mask(self):\n        return self._freq_mask\n\n    @property\n    def time_mask(self):\n        return self._time_mask\n\n    def __call__(self, xs):\n        """"""\n        Args:\n            xs (FloatTensor): `[B, T, F]`\n        Returns:\n            xs (FloatTensor): `[B, T, F]`\n\n        """"""\n        # xs = self.time_warp(xs)\n        xs = self.mask_freq(xs)\n        xs = self.mask_time(xs)\n        return xs\n\n    def time_warp(xs, W=40):\n        raise NotImplementedError\n\n    def mask_freq(self, xs, replace_with_zero=False):\n        n_bins = xs.size(-1)\n        for i in range(0, self.n_freq_masks):\n            f = int(np.random.uniform(low=0, high=self.F))\n            f_0 = int(np.random.uniform(low=0, high=n_bins - f))\n            xs[:, :, f_0:f_0 + f] = 0\n            assert f_0 <= f_0 + f\n            self._freq_mask = (f_0, f_0 + f)\n        return xs\n\n    def mask_time(self, xs, replace_with_zero=False):\n        n_frames = xs.size(1)\n        if self.adaptive_number_ratio > 0:\n            n_masks = int(n_frames * self.adaptive_number_ratio)\n            n_masks = min(n_masks, self.max_n_time_masks)\n        else:\n            n_masks = self.n_time_masks\n        if self.adaptive_size_ratio > 0:\n            T = self.adaptive_size_ratio * n_frames\n        else:\n            T = self.T\n        for i in range(n_masks):\n            t = int(np.random.uniform(low=0, high=T))\n            t = min(t, int(n_frames * self.p))\n            t_0 = int(np.random.uniform(low=0, high=n_frames - t))\n            xs[:, t_0:t_0 + t] = 0\n            assert t_0 <= t_0 + t\n            self._time_mask = (t_0, t_0 + t)\n        return xs\n'"
neural_sp/models/seq2seq/frontends/splicing.py,0,"b'#! /usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n# Copyright 2018 Kyoto University (Hirofumi Inaguma)\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n\n""""""Splice data.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\n\n\ndef splice(feat, n_splices=1, n_stacks=1, dtype=np.float32):\n    """"""Splice input data. This is expected to be used for CNN-like encoder.\n\n    Args:\n        feat (np.ndarray): A tensor of size\n            `[T, input_dim (freq * 3 * n_stacks)]\'\n        n_splices (int): frames to n_splices. Default is 1 frame.\n            ex.) if n_splices == 11\n                [t-5, ..., t-1, t, t+1, ..., t+5] (total 11 frames)\n        n_stacks (int): the number of frames to stack\n        dtype ():\n    Returns:\n        feat_splice (np.ndarray): A tensor of size\n            `[T, freq * (n_splices * n_stacks) * 3 (static + \xce\x94 + \xce\x94\xce\x94)]`\n\n    """"""\n    assert isinstance(feat, np.ndarray), \'feat should be np.ndarray.\'\n    assert len(feat.shape) == 2, \'feat must be 2 demension.\'\n    assert feat.shape[-1] % 3 == 0\n\n    if n_splices == 1:\n        return feat\n\n    max_xlen, input_dim = feat.shape\n    freq = (input_dim // 3) // n_stacks\n    feat_splice = np.zeros((max_xlen, freq * (n_splices * n_stacks) * 3), dtype=dtype)\n\n    for i_time in range(max_xlen):\n        spliced_frames = np.zeros((n_splices * n_stacks, freq, 3))\n        for i_splice in range(0, n_splices, 1):\n            if i_time <= n_splices - 1 and i_splice < n_splices - i_time:\n                # copy the first frame to left side (padding left frames)\n                copy_frame = feat[0]\n            elif max_xlen - n_splices <= i_time and i_time + (i_splice - n_splices) > max_xlen - 1:\n                # copy the last frame to right side (padding right frames)\n                copy_frame = feat[-1]\n            else:\n                copy_frame = feat[i_time + (i_splice - n_splices)]\n\n            # `[freq * 3 * n_stacks]` -> `[freq, 3, n_stacks]`\n            copy_frame = copy_frame.reshape((freq, 3, n_stacks))\n\n            # `[freq, 3, n_stacks]` -> `[n_stacks, freq, 3]`\n            copy_frame = np.transpose(copy_frame, (2, 0, 1))\n\n            spliced_frames[i_splice: i_splice + n_stacks] = copy_frame\n\n        # `[n_splices * n_stacks, freq, 3] -> `[freq, n_splices * n_stacks, 3]`\n        spliced_frames = np.transpose(spliced_frames, (1, 0, 2))\n\n        feat_splice[i_time] = spliced_frames.reshape((freq * (n_splices * n_stacks) * 3))\n\n    return feat_splice\n'"
examples/librispeech/s5/local/lm/python/pre_filter.py,0,"b'#!/usr/bin/env python\n\n# Copyright 2014 Vassil Panayotov\n# Apache 2.0\n\n# Removes common strings, that are not helpful for language modeling purposes,\n# from the Project Gutenberg\'s texts\n\nimport argparse\nimport re\nimport sys\n\nroman_number = [re.compile(\'^\\s*[_LXVI]+(\\.)?\\s*$\'), ]\n\nsq_brackets = [re.compile(\'(.*)(\\[.+\\])(.*)\', re.IGNORECASE)]\n\npipes = [re.compile(\'^\\s*\\|.*\\|\\s*$\')]\n\nnon_word = [re.compile(\'^\\W+$\')]\n\nchapter = [re.compile(\'^\\s*((Chapter)|(Volume)|(Canto)).*[LXIV0-9]+.*$\', re.IGNORECASE),]\n\ncontents = [re.compile(\'CONTENTS\'),\n            re.compile(\'^.*((\\s{2,50})|([\\t]+))[0-9]+\\s*$\'),\n            re.compile(\'^\\s*((I+[:.]+)|(I?[LXV]+I*([\\.:])?))\\s+.*\')]\n\ndebug = None\n\ndef parse_opts():\n    parser = argparse.ArgumentParser(\n        description=\'Strips unhelpful, from LM viewpoint, strings from PG texts\',\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n    parser.add_argument(\'--debug\', default=False, action=\'store_true\',\n                        help=\'Debug info - e.g. showing the lines that were stripped\')\n\n    parser.add_argument(\'in_text\', type=str, help=\'Input text file\')\n    parser.add_argument(\'out_text\', type=str, help=\'Filtered output text file\')\n    opts = parser.parse_args()\n    global debug\n    debug = opts.debug\n    return opts\n\ndef debug_log(lines, idx, context=2):\n    if debug:\n        start = max(0, idx - context)\n        end = min(len(lines), idx + context + 1)\n        sys.stderr.write(\'\\n\'.join(\'> %s\' % l for l in lines[start:end]) + \'\\n\\n\')\n\ndef match(regexes, line):\n    for r in regexes:\n        if r.match(line) is not None:\n            return True\n    return False\n\ndef empty_lines(lines, index, extent):\n    """"""\n    If the \'extent\' is negative, the function checks the preceding lines\n    """"""\n    if extent > 0:\n        start = min(index + 1, len(lines)-1)\n        end = min(index + extent + 1, len(lines))\n    else:\n        start = max(0, index + extent)\n        end = max(0, index)\n    for l in lines[start:end]:\n        if len(l) > 0:\n            return False\n    return True\n\nif __name__ == \'__main__\':\n    opts = parse_opts()\n\n    with open(opts.in_text) as in_text:\n        in_lines = [l.strip() for l in in_text.readlines()]\n\n    out_lines = list()\n    for i, l in enumerate(in_lines):\n        if len(l) == 0:\n            continue\n\n        # Roman numeral alone in a line, surrounded by empty lines\n        if match(roman_number, l) and empty_lines(in_lines, i, -1) and empty_lines(in_lines, i, 1):\n            #print \'matched roman\'\n            debug_log(in_lines, i)\n            continue\n\n        if match(chapter, l) and (empty_lines(in_lines, i, -1) or empty_lines(in_lines, i, 1)):\n            #print \'matched chapter\'\n            debug_log(in_lines, i)\n            continue\n\n        if match(non_word, l):\n            debug_log(in_lines, i)\n            continue\n\n        if match(contents, l):\n            #print \'matched contents\'\n            debug_log(in_lines, i)\n            continue\n\n        if match(pipes, l):\n            debug_log(in_lines, i)\n            continue\n\n        if match(sq_brackets, l):\n            debug_log(in_lines, i)\n            l = sq_brackets[0].sub(r\'\\1\\3\', l)\n\n        out_lines.append(l)\n\n    with open(opts.out_text, \'w\') as out_text:\n        out_text.write(\'\\n\'.join(out_lines) + \'\\n\')\n        '"
examples/librispeech/s5/local/lm/python/text_post_process.py,0,"b'#!/usr/bin/env python\n\n# Copyright 2014 Vassil Panayotov\n# Apache 2.0\n\n# [This script was taken verbatim from the LibriVox alignment setup]\n\n# Post processes the .opl file produced by \'nsw_expand\':\n# - removes the non-word tokens\n# - corrects likely wrong normalizations (e.g. Sun -> Sunday)\n# - splits the sentences into separate lines\n\nimport sys, argparse\nimport re\n\ndef parse_args():\n    parser = argparse.ArgumentParser(\n        description=""Post-process an .opl file into plain text"")\n    parser.add_argument(\'--max-sent-len\', type=int, default=600,\n                        help=""The maximum allowed # of words per sentence"")\n    parser.add_argument(\'--abort-long-sent\', type=bool, default=False,\n                        help=\'If True and a sentence longer than ""max-sent-len"" detected\' +\\\n                             \'exit with error code 1. If False, just split the long sentences.\')\n    parser.add_argument(\'--sent-end-marker\', type=str, default=""DOTDOTDOT"")\n    parser.add_argument(""in_text"", type=str, help=""Input text"")\n    parser.add_argument(""out_text"", type=str, help=""Output text"")\n    parser.add_argument(""sent_bounds"", type=str,\n                        help=""A file that will contain a comma separated list of numbers, s.t. if"" +\n                             ""i is in this list, then there is a sententence break after token i"")\n    return parser.parse_args()\n\n\nif __name__ == \'__main__\':\n    opts = parse_args()\n    with open(opts.in_text) as src, \\\n         open(opts.out_text, \'w\') as dst, \\\n         open(opts.sent_bounds, \'w\') as bounds:\n        corrections = 0\n        lines = list()\n        current_line = list()\n        sent_bounds = list()\n        n_tokens = 0\n        for opl_line in src:\n            start_scan = 3\n            opl_line = opl_line.upper()\n            opl_tokens = opl_line.split()\n            if opl_tokens[0] == opts.sent_end_marker.upper():\n                sent_bounds.append(n_tokens - 1)\n                if len(current_line) > opts.max_sent_len:\n                    if opts.abort_long_sent:\n                        sys.stderr.write(\'ERROR: Too long sentence - aborting!\\n\')\n                        sys.exit(1)\n                    else:\n                        sys.stderr.write(\'WARNING: Too long sentence - splitting ...\\n\')\n                        sent_start = 0\n                        while sent_start < len(current_line):\n                            lines.append(\' \'.join(current_line[sent_start:\\\n                                                  sent_start + opts.max_sent_len]))\n                            sent_start += opts.max_sent_len\n                else:\n                    lines.append(\' \'.join(current_line))\n                current_line = list()\n                continue\n            if len(opl_tokens) >= 4 and opl_tokens[3] == \'SUNDAY\' and opl_tokens[1] == \'EXPN\':\n                corrections += 1\n                n_tokens += 1\n                start_scan = 4\n                current_line.append(\'SUN\')\n            for i in xrange(start_scan, len(opl_tokens)):\n                m = re.match(""^[A-Z]+\\\'?[A-Z\\\']*$"", opl_tokens[i])\n                if m is not None:\n                    n_tokens += 1\n                    current_line.append(opl_tokens[i])\n                #else:\n                #    sys.stderr.write(\'rejected: %s\\n\' % opl_tokens[i])\n        sys.stderr.write(\'Corrected tokens: %d\\n\' % corrections)\n        dst.write(\'\\n\'.join(lines) + \'\\n\')\n        bounds.write(\',\'.join([str(t) for t in sent_bounds]))\n'"
examples/librispeech/s5/local/lm/python/text_pre_process.py,0,"b'#!/usr/bin/env python\n\n# Copyright 2014 Vassil Panayotov\n# Apache 2.0\n\n# [This script was taken verbatim from the alignment scripts]\n\n# Pre-process a book\'s text before passing it to Festival for normalization\n# of the non-standard words\n# Basically it does the following:\n# 1) Convert the non-ASCII characters to their closest ASCII equivalent.\n# 2) Convert Roman numerals to their decimal representation (do we really need this?)\n# 3) Segments the original file into utterances and puts a special token at the\n#    end of each sentence, to make possible to recover them after NSW normalization\n\nimport argparse\nimport codecs, unicodedata\nimport re\nimport nltk\n\ndef parse_args():\n    parser = argparse.ArgumentParser(description=""Pre-process a book\'s text"")\n    parser.add_argument(""--in-encoding"", type=str, default=""utf-8"",\n                        help=""Encoding to use when reading the input text"")\n    parser.add_argument(""--out-encoding"", type=str, default=""ascii"",\n                        help=""Encoding to use when writing the output text"")\n    parser.add_argument(\'--sent-end-marker\', type=str, default=""DOTDOTDOT"")\n    parser.add_argument(""in_text"", type=str, help=""Input text"")\n    parser.add_argument(""out_text"", type=str, help=""Output text"")\n    return parser.parse_args()\n\n# http://rosettacode.org/wiki/Roman_numerals/Decode#Python\n_rdecode = dict(zip(\'XVI\', (10, 5, 1)))\ndef decode(roman):\n    result = 0\n    for r, r1 in zip(roman, roman[1:]):\n        rd, rd1 = _rdecode[r], _rdecode[r1]\n        result += -rd if rd < rd1 else rd\n    return result + _rdecode[roman[-1]]\n\ndef convert_roman(text):\n    """"""\n    Uses heuristics to decide whether to convert a string that looks like a\n    roman numeral to decimal number.\n    """"""\n    lines = re.split(\'\\r?\\n\', text)\n    new_lines = list()\n    for i, l in enumerate(lines):\n        m = re.match(\'^(\\s*C((hapter)|(HAPTER))\\s+)(([IVX]+)|([ivx]+))(.*)\', l)\n        if m is not None:\n            new_line = ""%s%s%s"" % (m.group(1), decode(m.group(5).upper()), m.group(8))\n            new_lines.append(new_line)\n            continue\n        m = re.match(\'^(\\s*)(([IVX]+)|([ivx]+))([\\s\\.]+[A-Z].*)\', l)\n        if m is not None:\n            new_line = ""%s%s%s"" % (m.group(1), decode(m.group(2).upper()), m.group(5))\n            new_lines.append(new_line)\n            continue\n        new_lines.append(l)\n    return \'\\n\'.join(new_lines)\n\ndef segment_sentences(text, sent_marker):\n    punkt = nltk.data.load(\'tokenizers/punkt/english.pickle\')\n    sents = punkt.tokenize(text)\n    line_sents = [re.sub(\'\\r?\\n\', \' \', s) for s in sents]\n    line_sep = \' %s \\n\' % sent_marker\n    return (line_sep.join(line_sents) + sent_marker)\n\ndef pre_segment(text):\n    """"""\n    The segmentation at the start of the chapters is not ideal - e.g. Chapter\n    number and title are lumped together into a long \'sentence\'.\n    This routine tries to mitigate this by putting a dot at the end of each line\n    followed by 1 or more empty lines.\n    """"""\n    lines = text.split(\'\\n\')\n    out_text = list()\n    punkt = set([\'?\', \'!\', \'.\'])\n    for i, l in enumerate(lines[:-2]):\n        if len(l.strip()) != 0 and l.strip()[-1] not in punkt and\\\n           len(lines[i+1].strip()) == 0: #  and len(lines[i+2].strip()) == 0:\n            out_text.append(l + \'.\')\n        else:\n            out_text.append(l)\n    return \'\\n\'.join(out_text)\n\nif __name__ == \'__main__\':\n    opts = parse_args()\n    with codecs.open(opts.in_text, \'r\', opts.in_encoding, errors=\'ignore\') as src:\n        text_in = src.read()\n\n    text = unicodedata.normalize(\n                \'NFKD\', text_in).encode(opts.out_encoding, \'ignore\')\n    text = convert_roman(text)\n    text = pre_segment(text)\n    text = segment_sentences(text, opts.sent_end_marker)\n\n    with open(opts.out_text, \'w\') as dst:\n        dst.write(text)\n\n\n'"
