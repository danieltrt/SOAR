file_path,api_count,code
interact.py,4,"b'import time\nimport argparse\nimport torch\nimport msgpack\nfrom drqa.model import DocReaderModel\nfrom drqa.utils import str2bool\nfrom prepro import annotate, to_id, init\nfrom train import BatchGen\n\n""""""\nThis script serves as a template to be modified to suit all possible testing environments, including and not limited \nto files (json, xml, csv, ...), web service, databases and so on.\nTo change this script to batch model, simply modify line 70 from ""BatchGen([model_in], batch_size=1, ...)"" to \n""BatchGen([model_in_1, model_in_2, ...], batch_size=batch_size, ...)"".\n""""""\n\nparser = argparse.ArgumentParser(\n    description=\'Interact with document reader model.\'\n)\nparser.add_argument(\'--model-file\', default=\'models/best_model.pt\',\n                    help=\'path to model file\')\nparser.add_argument(""--cuda"", type=str2bool, nargs=\'?\',\n                    const=True, default=torch.cuda.is_available(),\n                    help=\'whether to use GPU acceleration.\')\nargs = parser.parse_args()\n\n\nif args.cuda:\n    checkpoint = torch.load(args.model_file)\nelse:\n    checkpoint = torch.load(args.model_file, map_location=lambda storage, loc: storage)\n\nstate_dict = checkpoint[\'state_dict\']\nopt = checkpoint[\'config\']\nwith open(\'SQuAD/meta.msgpack\', \'rb\') as f:\n    meta = msgpack.load(f, encoding=\'utf8\')\nembedding = torch.Tensor(meta[\'embedding\'])\nopt[\'pretrained_words\'] = True\nopt[\'vocab_size\'] = embedding.size(0)\nopt[\'embedding_dim\'] = embedding.size(1)\nopt[\'pos_size\'] = len(meta[\'vocab_tag\'])\nopt[\'ner_size\'] = len(meta[\'vocab_ent\'])\nopt[\'cuda\'] = args.cuda\nBatchGen.pos_size = opt[\'pos_size\']\nBatchGen.ner_size = opt[\'ner_size\']\nmodel = DocReaderModel(opt, embedding, state_dict)\nw2id = {w: i for i, w in enumerate(meta[\'vocab\'])}\ntag2id = {w: i for i, w in enumerate(meta[\'vocab_tag\'])}\nent2id = {w: i for i, w in enumerate(meta[\'vocab_ent\'])}\ninit()\n\nwhile True:\n    id_ = 0\n    try:\n        while True:\n            evidence = input(\'Evidence: \')\n            if evidence.strip():\n                break\n        while True:\n            question = input(\'Question: \')\n            if question.strip():\n                break\n    except EOFError:\n        print()\n        break\n    id_ += 1\n    start_time = time.time()\n    annotated = annotate((\'interact-{}\'.format(id_), evidence, question), meta[\'wv_cased\'])\n    model_in = to_id(annotated, w2id, tag2id, ent2id)\n    model_in = next(iter(BatchGen([model_in], batch_size=1, gpu=args.cuda, evaluation=True)))\n    prediction = model.predict(model_in)[0]\n    end_time = time.time()\n    print(\'Answer: {}\'.format(prediction))\n    print(\'Time: {:.4f}s\'.format(end_time - start_time))\n'"
prepro.py,0,"b'import re\nimport json\nimport spacy\nimport msgpack\nimport unicodedata\nimport numpy as np\nimport argparse\nimport collections\nimport multiprocessing\nfrom multiprocessing import Pool\nfrom tqdm import tqdm\nfrom functools import partial\nfrom drqa.utils import str2bool\nimport logging\n\n\ndef main():\n    args, log = setup()\n\n    train = flatten_json(args.trn_file, \'train\')\n    dev = flatten_json(args.dev_file, \'dev\')\n    log.info(\'json data flattened.\')\n\n    # tokenize & annotate\n    with Pool(args.threads, initializer=init) as p:\n        annotate_ = partial(annotate, wv_cased=args.wv_cased)\n        train = list(tqdm(p.imap(annotate_, train, chunksize=args.batch_size), total=len(train), desc=\'train\'))\n        dev = list(tqdm(p.imap(annotate_, dev, chunksize=args.batch_size), total=len(dev), desc=\'dev  \'))\n    train = list(map(index_answer, train))\n    initial_len = len(train)\n    train = list(filter(lambda x: x[-1] is not None, train))\n    log.info(\'drop {} inconsistent samples.\'.format(initial_len - len(train)))\n    log.info(\'tokens generated\')\n\n    # load vocabulary from word vector files\n    wv_vocab = set()\n    with open(args.wv_file) as f:\n        for line in f:\n            token = normalize_text(line.rstrip().split(\' \')[0])\n            wv_vocab.add(token)\n    log.info(\'glove vocab loaded.\')\n\n    # build vocabulary\n    full = train + dev\n    vocab, counter = build_vocab([row[5] for row in full], [row[1] for row in full], wv_vocab, args.sort_all)\n    total = sum(counter.values())\n    matched = sum(counter[t] for t in vocab)\n    log.info(\'vocab coverage {1}/{0} | OOV occurrence {2}/{3} ({4:.4f}%)\'.format(\n        len(counter), len(vocab), (total - matched), total, (total - matched) / total * 100))\n    counter_tag = collections.Counter(w for row in full for w in row[3])\n    vocab_tag = sorted(counter_tag, key=counter_tag.get, reverse=True)\n    counter_ent = collections.Counter(w for row in full for w in row[4])\n    vocab_ent = sorted(counter_ent, key=counter_ent.get, reverse=True)\n    w2id = {w: i for i, w in enumerate(vocab)}\n    tag2id = {w: i for i, w in enumerate(vocab_tag)}\n    ent2id = {w: i for i, w in enumerate(vocab_ent)}\n    log.info(\'Vocabulary size: {}\'.format(len(vocab)))\n    log.info(\'Found {} POS tags.\'.format(len(vocab_tag)))\n    log.info(\'Found {} entity tags: {}\'.format(len(vocab_ent), vocab_ent))\n\n    to_id_ = partial(to_id, w2id=w2id, tag2id=tag2id, ent2id=ent2id)\n    train = list(map(to_id_, train))\n    dev = list(map(to_id_, dev))\n    log.info(\'converted to ids.\')\n\n    vocab_size = len(vocab)\n    embeddings = np.zeros((vocab_size, args.wv_dim))\n    embed_counts = np.zeros(vocab_size)\n    embed_counts[:2] = 1  # PADDING & UNK\n    with open(args.wv_file) as f:\n        for line in f:\n            elems = line.rstrip().split(\' \')\n            token = normalize_text(elems[0])\n            if token in w2id:\n                word_id = w2id[token]\n                embed_counts[word_id] += 1\n                embeddings[word_id] += [float(v) for v in elems[1:]]\n    embeddings /= embed_counts.reshape((-1, 1))\n    log.info(\'got embedding matrix.\')\n\n    meta = {\n        \'vocab\': vocab,\n        \'vocab_tag\': vocab_tag,\n        \'vocab_ent\': vocab_ent,\n        \'embedding\': embeddings.tolist(),\n        \'wv_cased\': args.wv_cased,\n    }\n    with open(\'SQuAD/meta.msgpack\', \'wb\') as f:\n        msgpack.dump(meta, f)\n    result = {\n        \'train\': train,\n        \'dev\': dev\n    }\n    # train: id, context_id, context_features, tag_id, ent_id,\n    #        question_id, context, context_token_span, answer_start, answer_end\n    # dev:   id, context_id, context_features, tag_id, ent_id,\n    #        question_id, context, context_token_span, answer\n    with open(\'SQuAD/data.msgpack\', \'wb\') as f:\n        msgpack.dump(result, f)\n    if args.sample_size:\n        sample = {\n            \'train\': train[:args.sample_size],\n            \'dev\': dev[:args.sample_size]\n        }\n        with open(\'SQuAD/sample.msgpack\', \'wb\') as f:\n            msgpack.dump(sample, f)\n    log.info(\'saved to disk.\')\n\ndef setup():\n    parser = argparse.ArgumentParser(\n        description=\'Preprocessing data files, about 10 minitues to run.\'\n    )\n    parser.add_argument(\'--trn_file\', default=\'SQuAD/train-v1.1.json\',\n                        help=\'path to train file.\')\n    parser.add_argument(\'--dev_file\', default=\'SQuAD/dev-v1.1.json\',\n                        help=\'path to dev file.\')\n    parser.add_argument(\'--wv_file\', default=\'glove/glove.840B.300d.txt\',\n                        help=\'path to word vector file.\')\n    parser.add_argument(\'--wv_dim\', type=int, default=300,\n                        help=\'word vector dimension.\')\n    parser.add_argument(\'--wv_cased\', type=str2bool, nargs=\'?\',\n                        const=True, default=True,\n                        help=\'treat the words as cased or not.\')\n    parser.add_argument(\'--sort_all\', action=\'store_true\',\n                        help=\'sort the vocabulary by frequencies of all words. \'\n                             \'Otherwise consider question words first.\')\n    parser.add_argument(\'--sample_size\', type=int, default=0,\n                        help=\'size of sample data (for debugging).\')\n    parser.add_argument(\'--threads\', type=int, default=min(multiprocessing.cpu_count(), 16),\n                        help=\'number of threads for preprocessing.\')\n    parser.add_argument(\'--batch_size\', type=int, default=64,\n                        help=\'batch size for multiprocess tokenizing and tagging.\')\n    args = parser.parse_args()\n\n    logging.basicConfig(format=\'%(asctime)s %(message)s\', level=logging.DEBUG,\n                        datefmt=\'%m/%d/%Y %I:%M:%S\')\n    log = logging.getLogger(__name__)\n    log.info(vars(args))\n    log.info(\'start data preparing...\')\n\n    return args, log\n\ndef flatten_json(data_file, mode):\n    """"""Flatten each article in training data.""""""\n    with open(data_file) as f:\n        data = json.load(f)[\'data\']\n    rows = []\n    for article in data:\n        for paragraph in article[\'paragraphs\']:\n            context = paragraph[\'context\']\n            for qa in paragraph[\'qas\']:\n                id_, question, answers = qa[\'id\'], qa[\'question\'], qa[\'answers\']\n                if mode == \'train\':\n                    answer = answers[0][\'text\']  # in training data there\'s only one answer\n                    answer_start = answers[0][\'answer_start\']\n                    answer_end = answer_start + len(answer)\n                    rows.append((id_, context, question, answer, answer_start, answer_end))\n                else:  # mode == \'dev\'\n                    answers = [a[\'text\'] for a in answers]\n                    rows.append((id_, context, question, answers))\n    return rows\n\n\ndef clean_spaces(text):\n    """"""normalize spaces in a string.""""""\n    text = re.sub(r\'\\s\', \' \', text)\n    return text\n\n\ndef normalize_text(text):\n    return unicodedata.normalize(\'NFD\', text)\n\n\nnlp = None\n\n\ndef init():\n    """"""initialize spacy in each process""""""\n    global nlp\n    nlp = spacy.load(\'en\', parser=False)\n\n\ndef annotate(row, wv_cased):\n    global nlp\n    id_, context, question = row[:3]\n    q_doc = nlp(clean_spaces(question))\n    c_doc = nlp(clean_spaces(context))\n    question_tokens = [normalize_text(w.text) for w in q_doc]\n    context_tokens = [normalize_text(w.text) for w in c_doc]\n    question_tokens_lower = [w.lower() for w in question_tokens]\n    context_tokens_lower = [w.lower() for w in context_tokens]\n    context_token_span = [(w.idx, w.idx + len(w.text)) for w in c_doc]\n    context_tags = [w.tag_ for w in c_doc]\n    context_ents = [w.ent_type_ for w in c_doc]\n    question_lemma = {w.lemma_ if w.lemma_ != \'-PRON-\' else w.text.lower() for w in q_doc}\n    question_tokens_set = set(question_tokens)\n    question_tokens_lower_set = set(question_tokens_lower)\n    match_origin = [w in question_tokens_set for w in context_tokens]\n    match_lower = [w in question_tokens_lower_set for w in context_tokens_lower]\n    match_lemma = [(w.lemma_ if w.lemma_ != \'-PRON-\' else w.text.lower()) in question_lemma for w in c_doc]\n    # term frequency in document\n    counter_ = collections.Counter(context_tokens_lower)\n    total = len(context_tokens_lower)\n    context_tf = [counter_[w] / total for w in context_tokens_lower]\n    context_features = list(zip(match_origin, match_lower, match_lemma, context_tf))\n    if not wv_cased:\n        context_tokens = context_tokens_lower\n        question_tokens = question_tokens_lower\n    return (id_, context_tokens, context_features, context_tags, context_ents,\n            question_tokens, context, context_token_span) + row[3:]\n\n\ndef index_answer(row):\n    token_span = row[-4]\n    starts, ends = zip(*token_span)\n    answer_start = row[-2]\n    answer_end = row[-1]\n    try:\n        return row[:-3] + (starts.index(answer_start), ends.index(answer_end))\n    except ValueError:\n        return row[:-3] + (None, None)\n\n\ndef build_vocab(questions, contexts, wv_vocab, sort_all=False):\n    """"""\n    Build vocabulary sorted by global word frequency, or consider frequencies in questions first,\n    which is controlled by `args.sort_all`.\n    """"""\n    if sort_all:\n        counter = collections.Counter(w for doc in questions + contexts for w in doc)\n        vocab = sorted([t for t in counter if t in wv_vocab], key=counter.get, reverse=True)\n    else:\n        counter_q = collections.Counter(w for doc in questions for w in doc)\n        counter_c = collections.Counter(w for doc in contexts for w in doc)\n        counter = counter_c + counter_q\n        vocab = sorted([t for t in counter_q if t in wv_vocab], key=counter_q.get, reverse=True)\n        vocab += sorted([t for t in counter_c.keys() - counter_q.keys() if t in wv_vocab],\n                        key=counter.get, reverse=True)\n    vocab.insert(0, ""<PAD>"")\n    vocab.insert(1, ""<UNK>"")\n    return vocab, counter\n\n\ndef to_id(row, w2id, tag2id, ent2id, unk_id=1):\n    context_tokens = row[1]\n    context_features = row[2]\n    context_tags = row[3]\n    context_ents = row[4]\n    question_tokens = row[5]\n    question_ids = [w2id[w] if w in w2id else unk_id for w in question_tokens]\n    context_ids = [w2id[w] if w in w2id else unk_id for w in context_tokens]\n    tag_ids = [tag2id[w] for w in context_tags]\n    ent_ids = [ent2id[w] for w in context_ents]\n    return (row[0], context_ids, context_features, tag_ids, ent_ids, question_ids) + row[6:]\n\n\nif __name__ == \'__main__\':\n    main()\n'"
train.py,19,"b'import re\nimport os\nimport sys\nimport math\nimport random\nimport string\nimport logging\nimport argparse\nfrom shutil import copyfile\nfrom datetime import datetime\nfrom collections import Counter\nimport torch\nimport msgpack\nfrom drqa.model import DocReaderModel\nfrom drqa.utils import str2bool\n\n\ndef main():\n    args, log = setup()\n    log.info(\'[Program starts. Loading data...]\')\n    train, dev, dev_y, embedding, opt = load_data(vars(args))\n    log.info(opt)\n    log.info(\'[Data loaded.]\')\n    if args.save_dawn_logs:\n        dawn_start = datetime.now()\n        log.info(\'dawn_entry: epoch\\tf1Score\\thours\')\n\n    if args.resume:\n        log.info(\'[loading previous model...]\')\n        checkpoint = torch.load(os.path.join(args.model_dir, args.resume))\n        if args.resume_options:\n            opt = checkpoint[\'config\']\n        state_dict = checkpoint[\'state_dict\']\n        model = DocReaderModel(opt, embedding, state_dict)\n        epoch_0 = checkpoint[\'epoch\'] + 1\n        # synchronize random seed\n        random.setstate(checkpoint[\'random_state\'])\n        torch.random.set_rng_state(checkpoint[\'torch_state\'])\n        if args.cuda:\n            torch.cuda.set_rng_state(checkpoint[\'torch_cuda_state\'])\n        if args.reduce_lr:\n            lr_decay(model.optimizer, lr_decay=args.reduce_lr)\n            log.info(\'[learning rate reduced by {}]\'.format(args.reduce_lr))\n        batches = BatchGen(dev, batch_size=args.batch_size, evaluation=True, gpu=args.cuda)\n        predictions = []\n        for i, batch in enumerate(batches):\n            predictions.extend(model.predict(batch))\n            log.debug(\'> evaluating [{}/{}]\'.format(i, len(batches)))\n        em, f1 = score(predictions, dev_y)\n        log.info(""[dev EM: {} F1: {}]"".format(em, f1))\n        if math.fabs(em - checkpoint[\'em\']) > 1e-3 or math.fabs(f1 - checkpoint[\'f1\']) > 1e-3:\n            log.info(\'Inconsistent: recorded EM: {} F1: {}\'.format(checkpoint[\'em\'], checkpoint[\'f1\']))\n            log.error(\'Error loading model: current code is inconsistent with code used to train the previous model.\')\n            exit(1)\n        best_val_score = checkpoint[\'best_eval\']\n    else:\n        model = DocReaderModel(opt, embedding)\n        epoch_0 = 1\n        best_val_score = 0.0\n\n    for epoch in range(epoch_0, epoch_0 + args.epochs):\n        log.warning(\'Epoch {}\'.format(epoch))\n        # train\n        batches = BatchGen(train, batch_size=args.batch_size, gpu=args.cuda)\n        start = datetime.now()\n        for i, batch in enumerate(batches):\n            model.update(batch)\n            if i % args.log_per_updates == 0:\n                log.info(\'> epoch [{0:2}] updates[{1:6}] train loss[{2:.5f}] remaining[{3}]\'.format(\n                    epoch, model.updates, model.train_loss.value,\n                    str((datetime.now() - start) / (i + 1) * (len(batches) - i - 1)).split(\'.\')[0]))\n        log.debug(\'\\n\')\n        # eval\n        batches = BatchGen(dev, batch_size=args.batch_size, evaluation=True, gpu=args.cuda)\n        predictions = []\n        for i, batch in enumerate(batches):\n            predictions.extend(model.predict(batch))\n            log.debug(\'> evaluating [{}/{}]\'.format(i, len(batches)))\n        em, f1 = score(predictions, dev_y)\n        log.warning(""dev EM: {} F1: {}"".format(em, f1))\n        if args.save_dawn_logs:\n            time_diff = datetime.now() - dawn_start\n            log.warning(""dawn_entry: {}\\t{}\\t{}"".format(epoch, f1/100.0, float(time_diff.total_seconds() / 3600.0)))\n        # save\n        if not args.save_last_only or epoch == epoch_0 + args.epochs - 1:\n            model_file = os.path.join(args.model_dir, \'checkpoint_epoch_{}.pt\'.format(epoch))\n            model.save(model_file, epoch, [em, f1, best_val_score])\n            if f1 > best_val_score:\n                best_val_score = f1\n                copyfile(\n                    model_file,\n                    os.path.join(args.model_dir, \'best_model.pt\'))\n                log.info(\'[new best model saved.]\')\n\n\ndef setup():\n    parser = argparse.ArgumentParser(\n        description=\'Train a Document Reader model.\'\n    )\n    # system\n    parser.add_argument(\'--log_per_updates\', type=int, default=3,\n                        help=\'log model loss per x updates (mini-batches).\')\n    parser.add_argument(\'--data_file\', default=\'SQuAD/data.msgpack\',\n                        help=\'path to preprocessed data file.\')\n    parser.add_argument(\'--model_dir\', default=\'models\',\n                        help=\'path to store saved models.\')\n    parser.add_argument(\'--save_last_only\', action=\'store_true\',\n                        help=\'only save the final models.\')\n    parser.add_argument(\'--save_dawn_logs\', action=\'store_true\',\n                        help=\'append dawnbench log entries prefixed with dawn_entry:\')\n    parser.add_argument(\'--seed\', type=int, default=1013,\n                        help=\'random seed for data shuffling, dropout, etc.\')\n    parser.add_argument(""--cuda"", type=str2bool, nargs=\'?\',\n                        const=True, default=torch.cuda.is_available(),\n                        help=\'whether to use GPU acceleration.\')\n    # training\n    parser.add_argument(\'-e\', \'--epochs\', type=int, default=40)\n    parser.add_argument(\'-bs\', \'--batch_size\', type=int, default=32)\n    parser.add_argument(\'-rs\', \'--resume\', default=\'best_model.pt\',\n                        help=\'previous model file name (in `model_dir`). \'\n                             \'e.g. ""checkpoint_epoch_11.pt""\')\n    parser.add_argument(\'-ro\', \'--resume_options\', action=\'store_true\',\n                        help=\'use previous model options, ignore the cli and defaults.\')\n    parser.add_argument(\'-rlr\', \'--reduce_lr\', type=float, default=0.,\n                        help=\'reduce initial (resumed) learning rate by this factor.\')\n    parser.add_argument(\'-op\', \'--optimizer\', default=\'adamax\',\n                        help=\'supported optimizer: adamax, sgd\')\n    parser.add_argument(\'-gc\', \'--grad_clipping\', type=float, default=10)\n    parser.add_argument(\'-wd\', \'--weight_decay\', type=float, default=0)\n    parser.add_argument(\'-lr\', \'--learning_rate\', type=float, default=0.1,\n                        help=\'only applied to SGD.\')\n    parser.add_argument(\'-mm\', \'--momentum\', type=float, default=0,\n                        help=\'only applied to SGD.\')\n    parser.add_argument(\'-tp\', \'--tune_partial\', type=int, default=1000,\n                        help=\'finetune top-x embeddings.\')\n    parser.add_argument(\'--fix_embeddings\', action=\'store_true\',\n                        help=\'if true, `tune_partial` will be ignored.\')\n    parser.add_argument(\'--rnn_padding\', action=\'store_true\',\n                        help=\'perform rnn padding (much slower but more accurate).\')\n    # model\n    parser.add_argument(\'--question_merge\', default=\'self_attn\')\n    parser.add_argument(\'--doc_layers\', type=int, default=3)\n    parser.add_argument(\'--question_layers\', type=int, default=3)\n    parser.add_argument(\'--hidden_size\', type=int, default=128)\n    parser.add_argument(\'--num_features\', type=int, default=4)\n    parser.add_argument(\'--pos\', type=str2bool, nargs=\'?\', const=True, default=True,\n                        help=\'use pos tags as a feature.\')\n    parser.add_argument(\'--ner\', type=str2bool, nargs=\'?\', const=True, default=True,\n                        help=\'use named entity tags as a feature.\')\n    parser.add_argument(\'--use_qemb\', type=str2bool, nargs=\'?\', const=True, default=True)\n    parser.add_argument(\'--concat_rnn_layers\', type=str2bool, nargs=\'?\',\n                        const=True, default=True)\n    parser.add_argument(\'--dropout_emb\', type=float, default=0.4)\n    parser.add_argument(\'--dropout_rnn\', type=float, default=0.4)\n    parser.add_argument(\'--dropout_rnn_output\', type=str2bool, nargs=\'?\',\n                        const=True, default=True)\n    parser.add_argument(\'--max_len\', type=int, default=15)\n    parser.add_argument(\'--rnn_type\', default=\'lstm\',\n                        help=\'supported types: rnn, gru, lstm\')\n\n    args = parser.parse_args()\n\n    # set model dir\n    model_dir = args.model_dir\n    os.makedirs(model_dir, exist_ok=True)\n    args.model_dir = os.path.abspath(model_dir)\n\n    if args.resume == \'best_model.pt\' and not os.path.exists(os.path.join(args.model_dir, args.resume)):\n        # means we\'re starting fresh\n        args.resume = \'\'\n\n    # set random seed\n    random.seed(args.seed)\n    torch.manual_seed(args.seed)\n    if args.cuda:\n        torch.cuda.manual_seed(args.seed)\n\n    # setup logger\n    class ProgressHandler(logging.Handler):\n        def __init__(self, level=logging.NOTSET):\n            super().__init__(level)\n\n        def emit(self, record):\n            log_entry = self.format(record)\n            if record.message.startswith(\'> \'):\n                sys.stdout.write(\'{}\\r\'.format(log_entry.rstrip()))\n                sys.stdout.flush()\n            else:\n                sys.stdout.write(\'{}\\n\'.format(log_entry))\n\n    log = logging.getLogger(__name__)\n    log.setLevel(logging.DEBUG)\n    fh = logging.FileHandler(os.path.join(args.model_dir, \'log.txt\'))\n    fh.setLevel(logging.INFO)\n    ch = ProgressHandler()\n    ch.setLevel(logging.DEBUG)\n    formatter = logging.Formatter(fmt=\'%(asctime)s %(message)s\', datefmt=\'%m/%d/%Y %I:%M:%S\')\n    fh.setFormatter(formatter)\n    ch.setFormatter(formatter)\n    log.addHandler(fh)\n    log.addHandler(ch)\n\n    return args, log\n\n\ndef lr_decay(optimizer, lr_decay):\n    for param_group in optimizer.param_groups:\n        param_group[\'lr\'] *= lr_decay\n    return optimizer\n\n\ndef load_data(opt):\n    with open(\'SQuAD/meta.msgpack\', \'rb\') as f:\n        meta = msgpack.load(f, encoding=\'utf8\')\n    embedding = torch.Tensor(meta[\'embedding\'])\n    opt[\'pretrained_words\'] = True\n    opt[\'vocab_size\'] = embedding.size(0)\n    opt[\'embedding_dim\'] = embedding.size(1)\n    opt[\'pos_size\'] = len(meta[\'vocab_tag\'])\n    opt[\'ner_size\'] = len(meta[\'vocab_ent\'])\n    BatchGen.pos_size = opt[\'pos_size\']\n    BatchGen.ner_size = opt[\'ner_size\']\n    with open(opt[\'data_file\'], \'rb\') as f:\n        data = msgpack.load(f, encoding=\'utf8\')\n    train = data[\'train\']\n    data[\'dev\'].sort(key=lambda x: len(x[1]))\n    dev = [x[:-1] for x in data[\'dev\']]\n    dev_y = [x[-1] for x in data[\'dev\']]\n    return train, dev, dev_y, embedding, opt\n\n\nclass BatchGen:\n    pos_size = None\n    ner_size = None\n\n    def __init__(self, data, batch_size, gpu, evaluation=False):\n        """"""\n        input:\n            data - list of lists\n            batch_size - int\n        """"""\n        self.batch_size = batch_size\n        self.eval = evaluation\n        self.gpu = gpu\n\n        # sort by len\n        data = sorted(data, key=lambda x: len(x[1]))\n        # chunk into batches\n        data = [data[i:i + batch_size] for i in range(0, len(data), batch_size)]\n\n        # shuffle\n        if not evaluation:\n            random.shuffle(data)\n\n        self.data = data\n\n    def __len__(self):\n        return len(self.data)\n\n    def __iter__(self):\n        for batch in self.data:\n            batch_size = len(batch)\n            batch = list(zip(*batch))\n            if self.eval:\n                assert len(batch) == 8\n            else:\n                assert len(batch) == 10\n\n            context_len = max(len(x) for x in batch[1])\n            context_id = torch.LongTensor(batch_size, context_len).fill_(0)\n            for i, doc in enumerate(batch[1]):\n                context_id[i, :len(doc)] = torch.LongTensor(doc)\n\n            feature_len = len(batch[2][0][0])\n\n            context_feature = torch.Tensor(batch_size, context_len, feature_len).fill_(0)\n            for i, doc in enumerate(batch[2]):\n                for j, feature in enumerate(doc):\n                    context_feature[i, j, :] = torch.Tensor(feature)\n\n            context_tag = torch.Tensor(batch_size, context_len, self.pos_size).fill_(0)\n            for i, doc in enumerate(batch[3]):\n                for j, tag in enumerate(doc):\n                    context_tag[i, j, tag] = 1\n\n            context_ent = torch.Tensor(batch_size, context_len, self.ner_size).fill_(0)\n            for i, doc in enumerate(batch[4]):\n                for j, ent in enumerate(doc):\n                    context_ent[i, j, ent] = 1\n\n            question_len = max(len(x) for x in batch[5])\n            question_id = torch.LongTensor(batch_size, question_len).fill_(0)\n            for i, doc in enumerate(batch[5]):\n                question_id[i, :len(doc)] = torch.LongTensor(doc)\n\n            context_mask = torch.eq(context_id, 0)\n            question_mask = torch.eq(question_id, 0)\n            text = list(batch[6])\n            span = list(batch[7])\n            if not self.eval:\n                y_s = torch.LongTensor(batch[8])\n                y_e = torch.LongTensor(batch[9])\n            if self.gpu:\n                context_id = context_id.pin_memory()\n                context_feature = context_feature.pin_memory()\n                context_tag = context_tag.pin_memory()\n                context_ent = context_ent.pin_memory()\n                context_mask = context_mask.pin_memory()\n                question_id = question_id.pin_memory()\n                question_mask = question_mask.pin_memory()\n            if self.eval:\n                yield (context_id, context_feature, context_tag, context_ent, context_mask,\n                       question_id, question_mask, text, span)\n            else:\n                yield (context_id, context_feature, context_tag, context_ent, context_mask,\n                       question_id, question_mask, y_s, y_e, text, span)\n\n\ndef _normalize_answer(s):\n    def remove_articles(text):\n        return re.sub(r\'\\b(a|an|the)\\b\', \' \', text)\n\n    def white_space_fix(text):\n        return \' \'.join(text.split())\n\n    def remove_punc(text):\n        exclude = set(string.punctuation)\n        return \'\'.join(ch for ch in text if ch not in exclude)\n\n    def lower(text):\n        return text.lower()\n\n    return white_space_fix(remove_articles(remove_punc(lower(s))))\n\n\ndef _exact_match(pred, answers):\n    if pred is None or answers is None:\n        return False\n    pred = _normalize_answer(pred)\n    for a in answers:\n        if pred == _normalize_answer(a):\n            return True\n    return False\n\n\ndef _f1_score(pred, answers):\n    def _score(g_tokens, a_tokens):\n        common = Counter(g_tokens) & Counter(a_tokens)\n        num_same = sum(common.values())\n        if num_same == 0:\n            return 0\n        precision = 1. * num_same / len(g_tokens)\n        recall = 1. * num_same / len(a_tokens)\n        f1 = (2 * precision * recall) / (precision + recall)\n        return f1\n\n    if pred is None or answers is None:\n        return 0\n    g_tokens = _normalize_answer(pred).split()\n    scores = [_score(g_tokens, _normalize_answer(a).split()) for a in answers]\n    return max(scores)\n\n\ndef score(pred, truth):\n    assert len(pred) == len(truth)\n    f1 = em = total = 0\n    for p, t in zip(pred, truth):\n        total += 1\n        em += _exact_match(p, t)\n        f1 += _f1_score(p, t)\n    em = 100. * em / total\n    f1 = 100. * f1 / total\n    return em, f1\n\n\nif __name__ == \'__main__\':\n    main()\n\n'"
drqa/layers.py,10,"b'# Copyright (c) 2017-present, Facebook, Inc.\n# All rights reserved.\n# This source code is licensed under the BSD-style license found in the\n# LICENSE file in the root directory of this source tree. An additional grant\n# of patent rights can be found in the PATENTS file in the same directory.\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\n\n# No modification is made to this file.\n# Origin: https://github.com/facebookresearch/ParlAI/tree/master/parlai/agents/drqa\n\n# ------------------------------------------------------------------------------\n# Modules\n# ------------------------------------------------------------------------------\n\n\nclass StackedBRNN(nn.Module):\n    def __init__(self, input_size, hidden_size, num_layers,\n                 dropout_rate=0, dropout_output=False, rnn_type=nn.LSTM,\n                 concat_layers=False, padding=False):\n        super(StackedBRNN, self).__init__()\n        self.padding = padding\n        self.dropout_output = dropout_output\n        self.dropout_rate = dropout_rate\n        self.num_layers = num_layers\n        self.concat_layers = concat_layers\n        self.rnns = nn.ModuleList()\n        for i in range(num_layers):\n            input_size = input_size if i == 0 else 2 * hidden_size\n            self.rnns.append(rnn_type(input_size, hidden_size,\n                                      num_layers=1,\n                                      bidirectional=True))\n\n    def forward(self, x, x_mask):\n        """"""Can choose to either handle or ignore variable length sequences.\n        Always handle padding in eval.\n        """"""\n        # No padding necessary.\n        if x_mask.data.sum() == 0:\n            return self._forward_unpadded(x, x_mask)\n        # Pad if we care or if its during eval.\n        if self.padding or not self.training:\n            return self._forward_padded(x, x_mask)\n        # We don\'t care.\n        return self._forward_unpadded(x, x_mask)\n\n    def _forward_unpadded(self, x, x_mask):\n        """"""Faster encoding that ignores any padding.""""""\n        # Transpose batch and sequence dims\n        x = x.transpose(0, 1)\n\n        # Encode all layers\n        outputs = [x]\n        for i in range(self.num_layers):\n            rnn_input = outputs[-1]\n\n            # Apply dropout to hidden input\n            if self.dropout_rate > 0:\n                rnn_input = F.dropout(rnn_input,\n                                      p=self.dropout_rate,\n                                      training=self.training)\n            # Forward\n            rnn_output = self.rnns[i](rnn_input)[0]\n            outputs.append(rnn_output)\n\n        # Concat hidden layers\n        if self.concat_layers:\n            output = torch.cat(outputs[1:], 2)\n        else:\n            output = outputs[-1]\n\n        # Transpose back\n        output = output.transpose(0, 1)\n\n        # Dropout on output layer\n        if self.dropout_output and self.dropout_rate > 0:\n            output = F.dropout(output,\n                               p=self.dropout_rate,\n                               training=self.training)\n        return output\n\n    def _forward_padded(self, x, x_mask):\n        """"""Slower (significantly), but more precise,\n        encoding that handles padding.""""""\n        # Compute sorted sequence lengths\n        lengths = x_mask.data.eq(0).long().sum(1).squeeze()\n        _, idx_sort = torch.sort(lengths, dim=0, descending=True)\n        _, idx_unsort = torch.sort(idx_sort, dim=0)\n\n        lengths = list(lengths[idx_sort])\n        idx_sort = Variable(idx_sort)\n        idx_unsort = Variable(idx_unsort)\n\n        # Sort x\n        x = x.index_select(0, idx_sort)\n\n        # Transpose batch and sequence dims\n        x = x.transpose(0, 1)\n\n        # Pack it up\n        rnn_input = nn.utils.rnn.pack_padded_sequence(x, lengths)\n\n        # Encode all layers\n        outputs = [rnn_input]\n        for i in range(self.num_layers):\n            rnn_input = outputs[-1]\n\n            # Apply dropout to input\n            if self.dropout_rate > 0:\n                dropout_input = F.dropout(rnn_input.data,\n                                          p=self.dropout_rate,\n                                          training=self.training)\n                rnn_input = nn.utils.rnn.PackedSequence(dropout_input,\n                                                        rnn_input.batch_sizes)\n            outputs.append(self.rnns[i](rnn_input)[0])\n\n        # Unpack everything\n        for i, o in enumerate(outputs[1:], 1):\n            outputs[i] = nn.utils.rnn.pad_packed_sequence(o)[0]\n\n        # Concat hidden layers or take final\n        if self.concat_layers:\n            output = torch.cat(outputs[1:], 2)\n        else:\n            output = outputs[-1]\n\n        # Transpose and unsort\n        output = output.transpose(0, 1)\n        output = output.index_select(0, idx_unsort)\n\n        # Pad up to original batch sequence length\n        if output.size(1) != x_mask.size(1):\n            padding = torch.zeros(output.size(0),\n                                  x_mask.size(1) - output.size(1),\n                                  output.size(2)).type(output.data.type())\n            output = torch.cat([output, Variable(padding)], 1)\n\n        # Dropout on output layer\n        if self.dropout_output and self.dropout_rate > 0:\n            output = F.dropout(output,\n                               p=self.dropout_rate,\n                               training=self.training)\n        return output\n\n\nclass SeqAttnMatch(nn.Module):\n    """"""Given sequences X and Y, match sequence Y to each element in X.\n    * o_i = sum(alpha_j * y_j) for i in X\n    * alpha_j = softmax(y_j * x_i)\n    """"""\n    def __init__(self, input_size, identity=False):\n        super(SeqAttnMatch, self).__init__()\n        if not identity:\n            self.linear = nn.Linear(input_size, input_size)\n        else:\n            self.linear = None\n\n    def forward(self, x, y, y_mask):\n        """"""Input shapes:\n            x = batch * len1 * h\n            y = batch * len2 * h\n            y_mask = batch * len2\n        Output shapes:\n            matched_seq = batch * len1 * h\n        """"""\n        # Project vectors\n        if self.linear:\n            x_proj = self.linear(x.view(-1, x.size(2))).view(x.size())\n            x_proj = F.relu(x_proj)\n            y_proj = self.linear(y.view(-1, y.size(2))).view(y.size())\n            y_proj = F.relu(y_proj)\n        else:\n            x_proj = x\n            y_proj = y\n\n        # Compute scores\n        scores = x_proj.bmm(y_proj.transpose(2, 1))\n\n        # Mask padding\n        y_mask = y_mask.unsqueeze(1).expand(scores.size())\n        scores.data.masked_fill_(y_mask.data, -float(\'inf\'))\n\n        # Normalize with softmax\n        alpha_flat = F.softmax(scores.view(-1, y.size(1)), dim=1)\n        alpha = alpha_flat.view(-1, x.size(1), y.size(1))\n\n        # Take weighted average\n        matched_seq = alpha.bmm(y)\n        return matched_seq\n\n\nclass BilinearSeqAttn(nn.Module):\n    """"""A bilinear attention layer over a sequence X w.r.t y:\n    * o_i = softmax(x_i\'Wy) for x_i in X.\n\n    Optionally don\'t normalize output weights.\n    """"""\n    def __init__(self, x_size, y_size, identity=False):\n        super(BilinearSeqAttn, self).__init__()\n        if not identity:\n            self.linear = nn.Linear(y_size, x_size)\n        else:\n            self.linear = None\n\n    def forward(self, x, y, x_mask):\n        """"""\n        x = batch * len * h1\n        y = batch * h2\n        x_mask = batch * len\n        """"""\n        Wy = self.linear(y) if self.linear is not None else y\n        xWy = x.bmm(Wy.unsqueeze(2)).squeeze(2)\n        xWy.data.masked_fill_(x_mask.data, -float(\'inf\'))\n        if self.training:\n            # In training we output log-softmax for NLL\n            alpha = F.log_softmax(xWy, dim=1)\n        else:\n            # ...Otherwise 0-1 probabilities\n            alpha = F.softmax(xWy, dim=1)\n        return alpha\n\n\nclass LinearSeqAttn(nn.Module):\n    """"""Self attention over a sequence:\n    * o_i = softmax(Wx_i) for x_i in X.\n    """"""\n    def __init__(self, input_size):\n        super(LinearSeqAttn, self).__init__()\n        self.linear = nn.Linear(input_size, 1)\n\n    def forward(self, x, x_mask):\n        """"""\n        x = batch * len * hdim\n        x_mask = batch * len\n        """"""\n        x_flat = x.contiguous().view(-1, x.size(-1))\n        scores = self.linear(x_flat).view(x.size(0), x.size(1))\n        scores.data.masked_fill_(x_mask.data, -float(\'inf\'))\n        alpha = F.softmax(scores, dim=1)\n        return alpha\n\n\n# ------------------------------------------------------------------------------\n# Functional\n# ------------------------------------------------------------------------------\n\n\ndef uniform_weights(x, x_mask):\n    """"""Return uniform weights over non-masked input.""""""\n    alpha = Variable(torch.ones(x.size(0), x.size(1)))\n    if x.data.is_cuda:\n        alpha = alpha.cuda()\n    alpha = alpha * x_mask.eq(0).float()\n    alpha = alpha / alpha.sum(1).expand(alpha.size())\n    return alpha\n\n\ndef weighted_avg(x, weights):\n    """"""x = batch * len * d\n    weights = batch * len\n    """"""\n    return weights.unsqueeze(1).bmm(x).squeeze(1)\n'"
drqa/model.py,10,"b'# Copyright (c) 2017-present, Facebook, Inc.\n# All rights reserved.\n# This source code is licensed under the BSD-style license found in the\n# LICENSE file in the root directory of this source tree. An additional grant\n# of patent rights can be found in the PATENTS file in the same directory.\nimport random\nimport torch\nimport torch.optim as optim\nimport torch.nn.functional as F\nimport numpy as np\nimport logging\n\nfrom torch.autograd import Variable\nfrom .utils import AverageMeter\nfrom .rnn_reader import RnnDocReader\n\n# Modification:\n#   - change the logger name\n#   - save & load ""state_dict""s of optimizer and loss meter\n#   - save all random seeds\n#   - change the dimension of inputs (for POS and NER features)\n#   - remove ""reset parameters"" and use a gradient hook for gradient masking\n# Origin: https://github.com/facebookresearch/ParlAI/tree/master/parlai/agents/drqa\n\nlogger = logging.getLogger(__name__)\n\n\nclass DocReaderModel(object):\n    """"""High level model that handles intializing the underlying network\n    architecture, saving, updating examples, and predicting examples.\n    """"""\n\n    def __init__(self, opt, embedding=None, state_dict=None):\n        # Book-keeping.\n        self.opt = opt\n        self.device = torch.cuda.current_device() if opt[\'cuda\'] else torch.device(\'cpu\')\n        self.updates = state_dict[\'updates\'] if state_dict else 0\n        self.train_loss = AverageMeter()\n        if state_dict:\n            self.train_loss.load(state_dict[\'loss\'])\n\n        # Building network.\n        self.network = RnnDocReader(opt, embedding=embedding)\n        if state_dict:\n            new_state = set(self.network.state_dict().keys())\n            for k in list(state_dict[\'network\'].keys()):\n                if k not in new_state:\n                    del state_dict[\'network\'][k]\n            self.network.load_state_dict(state_dict[\'network\'])\n        self.network.to(self.device)\n\n        # Building optimizer.\n        self.opt_state_dict = state_dict[\'optimizer\'] if state_dict else None\n        self.build_optimizer()\n\n    def build_optimizer(self):\n        parameters = [p for p in self.network.parameters() if p.requires_grad]\n        if self.opt[\'optimizer\'] == \'sgd\':\n            self.optimizer = optim.SGD(parameters, self.opt[\'learning_rate\'],\n                                       momentum=self.opt[\'momentum\'],\n                                       weight_decay=self.opt[\'weight_decay\'])\n        elif self.opt[\'optimizer\'] == \'adamax\':\n            self.optimizer = optim.Adamax(parameters,\n                                          weight_decay=self.opt[\'weight_decay\'])\n        else:\n            raise RuntimeError(\'Unsupported optimizer: %s\' % self.opt[\'optimizer\'])\n        if self.opt_state_dict:\n            self.optimizer.load_state_dict(self.opt_state_dict)\n\n    def update(self, ex):\n        # Train mode\n        self.network.train()\n\n        # Transfer to GPU\n        inputs = [e.to(self.device) for e in ex[:7]]\n        target_s = ex[7].to(self.device)\n        target_e = ex[8].to(self.device)\n\n        # Run forward\n        score_s, score_e = self.network(*inputs)\n\n        # Compute loss and accuracies\n        loss = F.nll_loss(score_s, target_s) + F.nll_loss(score_e, target_e)\n        self.train_loss.update(loss.item())\n\n        # Clear gradients and run backward\n        self.optimizer.zero_grad()\n        loss.backward()\n\n        # Clip gradients\n        torch.nn.utils.clip_grad_norm_(self.network.parameters(),\n                                      self.opt[\'grad_clipping\'])\n\n        # Update parameters\n        self.optimizer.step()\n        self.updates += 1\n\n    def predict(self, ex):\n        # Eval mode\n        self.network.eval()\n\n        # Transfer to GPU\n        if self.opt[\'cuda\']:\n            inputs = [Variable(e.cuda()) for e in ex[:7]]\n        else:\n            inputs = [Variable(e) for e in ex[:7]]\n\n        # Run forward\n        with torch.no_grad():\n            score_s, score_e = self.network(*inputs)\n\n        # Transfer to CPU/normal tensors for numpy ops\n        score_s = score_s.data.cpu()\n        score_e = score_e.data.cpu()\n\n        # Get argmax text spans\n        text = ex[-2]\n        spans = ex[-1]\n        predictions = []\n        max_len = self.opt[\'max_len\'] or score_s.size(1)\n        for i in range(score_s.size(0)):\n            scores = torch.ger(score_s[i], score_e[i])\n            scores.triu_().tril_(max_len - 1)\n            scores = scores.numpy()\n            s_idx, e_idx = np.unravel_index(np.argmax(scores), scores.shape)\n            s_offset, e_offset = spans[i][s_idx][0], spans[i][e_idx][1]\n            predictions.append(text[i][s_offset:e_offset])\n\n        return predictions\n\n    def save(self, filename, epoch, scores):\n        em, f1, best_eval = scores\n        params = {\n            \'state_dict\': {\n                \'network\': self.network.state_dict(),\n                \'optimizer\': self.optimizer.state_dict(),\n                \'updates\': self.updates,\n                \'loss\': self.train_loss.state_dict()\n            },\n            \'config\': self.opt,\n            \'epoch\': epoch,\n            \'em\': em,\n            \'f1\': f1,\n            \'best_eval\': best_eval,\n            \'random_state\': random.getstate(),\n            \'torch_state\': torch.random.get_rng_state(),\n            \'torch_cuda_state\': torch.cuda.get_rng_state()\n        }\n        try:\n            torch.save(params, filename)\n            logger.info(\'model saved to {}\'.format(filename))\n        except BaseException:\n            logger.warning(\'[ WARN: Saving failed... continuing anyway. ]\')\n'"
drqa/rnn_reader.py,2,"b'# Copyright (c) 2017-present, Facebook, Inc.\n# All rights reserved.\n# This source code is licensed under the BSD-style license found in the\n# LICENSE file in the root directory of this source tree. An additional grant\n# of patent rights can be found in the PATENTS file in the same directory.\nimport torch\nimport torch.nn as nn\nfrom . import layers\n\n# Modification:\n#   - add \'pos\' and \'ner\' features.\n#   - use gradient hook (instead of tensor copying) for gradient masking\n# Origin: https://github.com/facebookresearch/ParlAI/tree/master/parlai/agents/drqa\n\n\nclass RnnDocReader(nn.Module):\n    """"""Network for the Document Reader module of DrQA.""""""\n    RNN_TYPES = {\'lstm\': nn.LSTM, \'gru\': nn.GRU, \'rnn\': nn.RNN}\n\n    def __init__(self, opt, padding_idx=0, embedding=None):\n        super(RnnDocReader, self).__init__()\n        # Store config\n        self.opt = opt\n\n        # Word embeddings\n        if opt[\'pretrained_words\']:\n            assert embedding is not None\n            self.embedding = nn.Embedding.from_pretrained(embedding, freeze=False)\n            if opt[\'fix_embeddings\']:\n                assert opt[\'tune_partial\'] == 0\n                self.embedding.weight.requires_grad = False\n            elif opt[\'tune_partial\'] > 0:\n                assert opt[\'tune_partial\'] + 2 < embedding.size(0)\n                offset = self.opt[\'tune_partial\'] + 2\n\n                def embedding_hook(grad, offset=offset):\n                    grad[offset:] = 0\n                    return grad\n\n                self.embedding.weight.register_hook(embedding_hook)\n\n        else:  # random initialized\n            self.embedding = nn.Embedding(opt[\'vocab_size\'],\n                                          opt[\'embedding_dim\'],\n                                          padding_idx=padding_idx)\n        # Projection for attention weighted question\n        if opt[\'use_qemb\']:\n            self.qemb_match = layers.SeqAttnMatch(opt[\'embedding_dim\'])\n\n        # Input size to RNN: word emb + question emb + manual features\n        doc_input_size = opt[\'embedding_dim\'] + opt[\'num_features\']\n        if opt[\'use_qemb\']:\n            doc_input_size += opt[\'embedding_dim\']\n        if opt[\'pos\']:\n            doc_input_size += opt[\'pos_size\']\n        if opt[\'ner\']:\n            doc_input_size += opt[\'ner_size\']\n\n        # RNN document encoder\n        self.doc_rnn = layers.StackedBRNN(\n            input_size=doc_input_size,\n            hidden_size=opt[\'hidden_size\'],\n            num_layers=opt[\'doc_layers\'],\n            dropout_rate=opt[\'dropout_rnn\'],\n            dropout_output=opt[\'dropout_rnn_output\'],\n            concat_layers=opt[\'concat_rnn_layers\'],\n            rnn_type=self.RNN_TYPES[opt[\'rnn_type\']],\n            padding=opt[\'rnn_padding\'],\n        )\n\n        # RNN question encoder\n        self.question_rnn = layers.StackedBRNN(\n            input_size=opt[\'embedding_dim\'],\n            hidden_size=opt[\'hidden_size\'],\n            num_layers=opt[\'question_layers\'],\n            dropout_rate=opt[\'dropout_rnn\'],\n            dropout_output=opt[\'dropout_rnn_output\'],\n            concat_layers=opt[\'concat_rnn_layers\'],\n            rnn_type=self.RNN_TYPES[opt[\'rnn_type\']],\n            padding=opt[\'rnn_padding\'],\n        )\n\n        # Output sizes of rnn encoders\n        doc_hidden_size = 2 * opt[\'hidden_size\']\n        question_hidden_size = 2 * opt[\'hidden_size\']\n        if opt[\'concat_rnn_layers\']:\n            doc_hidden_size *= opt[\'doc_layers\']\n            question_hidden_size *= opt[\'question_layers\']\n\n        # Question merging\n        if opt[\'question_merge\'] not in [\'avg\', \'self_attn\']:\n            raise NotImplementedError(\'question_merge = %s\' % opt[\'question_merge\'])\n        if opt[\'question_merge\'] == \'self_attn\':\n            self.self_attn = layers.LinearSeqAttn(question_hidden_size)\n\n        # Bilinear attention for span start/end\n        self.start_attn = layers.BilinearSeqAttn(\n            doc_hidden_size,\n            question_hidden_size,\n        )\n        self.end_attn = layers.BilinearSeqAttn(\n            doc_hidden_size,\n            question_hidden_size,\n        )\n\n    def forward(self, x1, x1_f, x1_pos, x1_ner, x1_mask, x2, x2_mask):\n        """"""Inputs:\n        x1 = document word indices             [batch * len_d]\n        x1_f = document word features indices  [batch * len_d * nfeat]\n        x1_pos = document POS tags             [batch * len_d]\n        x1_ner = document entity tags          [batch * len_d]\n        x1_mask = document padding mask        [batch * len_d]\n        x2 = question word indices             [batch * len_q]\n        x2_mask = question padding mask        [batch * len_q]\n        """"""\n        # Embed both document and question\n        x1_emb = self.embedding(x1)\n        x2_emb = self.embedding(x2)\n\n        # Dropout on embeddings\n        if self.opt[\'dropout_emb\'] > 0:\n            x1_emb = nn.functional.dropout(x1_emb, p=self.opt[\'dropout_emb\'],\n                                           training=self.training)\n            x2_emb = nn.functional.dropout(x2_emb, p=self.opt[\'dropout_emb\'],\n                                           training=self.training)\n\n        drnn_input_list = [x1_emb, x1_f]\n        # Add attention-weighted question representation\n        if self.opt[\'use_qemb\']:\n            x2_weighted_emb = self.qemb_match(x1_emb, x2_emb, x2_mask)\n            drnn_input_list.append(x2_weighted_emb)\n        if self.opt[\'pos\']:\n            drnn_input_list.append(x1_pos)\n        if self.opt[\'ner\']:\n            drnn_input_list.append(x1_ner)\n        drnn_input = torch.cat(drnn_input_list, 2)\n        # Encode document with RNN\n        doc_hiddens = self.doc_rnn(drnn_input, x1_mask)\n\n        # Encode question with RNN + merge hiddens\n        question_hiddens = self.question_rnn(x2_emb, x2_mask)\n        if self.opt[\'question_merge\'] == \'avg\':\n            q_merge_weights = layers.uniform_weights(question_hiddens, x2_mask)\n        elif self.opt[\'question_merge\'] == \'self_attn\':\n            q_merge_weights = self.self_attn(question_hiddens, x2_mask)\n        question_hidden = layers.weighted_avg(question_hiddens, q_merge_weights)\n\n        # Predict start and end positions\n        start_scores = self.start_attn(doc_hiddens, question_hidden, x1_mask)\n        end_scores = self.end_attn(doc_hiddens, question_hidden, x1_mask)\n        return start_scores, end_scores\n'"
drqa/utils.py,0,"b'import argparse\n\n\nclass AverageMeter(object):\n    """"""Keep exponential weighted averages.""""""\n    def __init__(self, beta=0.99):\n        self.beta = beta\n        self.moment = 0\n        self.value = 0\n        self.t = 0\n\n    def state_dict(self):\n        return vars(self)\n\n    def load(self, state_dict):\n        for k, v in state_dict.items():\n            self.__setattr__(k, v)\n\n    def update(self, val):\n        self.t += 1\n        self.moment = self.beta * self.moment + (1 - self.beta) * val\n        # bias correction\n        self.value = self.moment / (1 - self.beta ** self.t)\n\n\ndef str2bool(v):\n    if v.lower() in (\'yes\', \'true\', \'t\', \'y\', \'1\'):\n        return True\n    elif v.lower() in (\'no\', \'false\', \'f\', \'n\', \'0\'):\n        return False\n    else:\n        raise argparse.ArgumentTypeError(\'Boolean value expected.\')\n\n'"
