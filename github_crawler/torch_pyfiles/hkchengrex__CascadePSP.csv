file_path,api_count,code
eval.py,4,"b""import torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader\nimport progressbar\nimport cv2\n\nfrom models.psp.pspnet import PSPNet\nfrom dataset import OfflineDataset, SplitTransformDataset\nfrom util.image_saver import tensor_to_im, tensor_to_gray_im, tensor_to_seg\nfrom util.hyper_para import HyperParameters\nfrom eval_helper import process_high_res_im, process_im_single_pass\n\nimport os\nfrom os import path\nfrom argparse import ArgumentParser\nimport time\n\n\nclass Parser():\n    def parse(self):\n        self.default = HyperParameters()\n        self.default.parse(unknown_arg_ok=True)\n\n        parser = ArgumentParser()\n\n        parser.add_argument('--dir', help='Directory with testing images')\n        parser.add_argument('--model', help='Pretrained model')\n        parser.add_argument('--output', help='Output directory')\n\n        parser.add_argument('--global_only', help='Global step only', action='store_true')\n\n        parser.add_argument('--L', help='Parameter L used in the paper', type=int, default=900)\n        parser.add_argument('--stride', help='stride', type=int, default=450)\n\n        parser.add_argument('--clear', help='Clear pytorch cache?', action='store_true')\n\n        parser.add_argument('--ade', help='Test on ADE dataset?', action='store_true')\n\n        args, _ = parser.parse_known_args()\n        self.args = vars(args)\n\n    def __getitem__(self, key):\n        if key in self.args:\n            return self.args[key]\n        else:\n            return self.default[key]\n\n    def __str__(self):\n        return str(self.args)\n\n# Parse command line arguments\npara = Parser()\npara.parse()\nprint('Hyperparameters: ', para)\n\n# Construct model\nmodel = nn.DataParallel(PSPNet(sizes=(1, 2, 3, 6), psp_size=2048, deep_features_size=1024, backend='resnet50').cuda())\nmodel.load_state_dict(torch.load(para['model']))\n\nbatch_size = 1\n\nif para['ade']:\n    val_dataset = SplitTransformDataset(para['dir'], need_name=True, perturb=False, img_suffix='_im.jpg')\nelse:\n    val_dataset = OfflineDataset(para['dir'], need_name=True, resize=False, do_crop=False)\nval_loader = DataLoader(val_dataset, batch_size, shuffle=False, num_workers=2)\n\nos.makedirs(para['output'], exist_ok=True)\n\nepoch_start_time = time.time()\nmodel = model.eval()\nwith torch.no_grad():\n    for im, seg, gt, name in progressbar.progressbar(val_loader):\n        im, seg, gt = im, seg, gt\n\n        if para['global_only']:\n            if para['ade']:\n                # GTs of small objects in ADE are too coarse -- less upsampling is better\n                images = process_im_single_pass(model, im, seg, 224, para)\n            else:\n                images = process_im_single_pass(model, im, seg, para['L'], para)\n        else:\n            images = process_high_res_im(model, im, seg, para, name, aggre_device='cuda:0')\n\n        images['im'] = im\n        images['seg'] = seg\n        images['gt'] = gt\n\n        # Suppress close-to-zero segmentation input\n        for b in range(seg.shape[0]):\n            if (seg[b]+1).sum() < 2:\n                images['pred_224'][b] = 0\n\n        # Save output images\n        for i in range(im.shape[0]):\n            cv2.imwrite(path.join(para['output'], '%s_im.png' % (name[i]))\n                ,cv2.cvtColor(tensor_to_im(im[i]), cv2.COLOR_RGB2BGR))\n            cv2.imwrite(path.join(para['output'], '%s_seg.png' % (name[i]))\n                ,tensor_to_seg(images['seg'][i]))\n            cv2.imwrite(path.join(para['output'], '%s_gt.png' % (name[i]))\n                ,tensor_to_gray_im(gt[i]))\n            cv2.imwrite(path.join(para['output'], '%s_mask.png' % (name[i]))\n                ,tensor_to_gray_im(images['pred_224'][i]))\n\nprint('Time taken: %.1f s' % (time.time() - epoch_start_time))"""
eval_helper.py,11,"b'import torch\nimport torch.nn.functional as F\n\nfrom util.util import resize_max_side\n\n\ndef safe_forward(model, im, seg, inter_s8=None, inter_s4=None):\n    """"""\n    Slightly pads the input image such that its length is a multiple of 8\n    """"""\n    b, _, ph, pw = seg.shape\n    if (ph % 8 != 0) or (pw % 8 != 0):\n        newH = ((ph//8+1)*8)\n        newW = ((pw//8+1)*8)\n        p_im = torch.zeros(b, 3, newH, newW).cuda()\n        p_seg = torch.zeros(b, 1, newH, newW).cuda() - 1\n\n        p_im[:,:,0:ph,0:pw] = im\n        p_seg[:,:,0:ph,0:pw] = seg\n        im = p_im\n        seg = p_seg\n\n        if inter_s8 is not None:\n            p_inter_s8 = torch.zeros(b, 1, newH, newW).cuda() - 1\n            p_inter_s8[:,:,0:ph,0:pw] = inter_s8\n            inter_s8 = p_inter_s8\n        if inter_s4 is not None:\n            p_inter_s4 = torch.zeros(b, 1, newH, newW).cuda() - 1\n            p_inter_s4[:,:,0:ph,0:pw] = inter_s4\n            inter_s4 = p_inter_s4\n\n    images = model(im, seg, inter_s8, inter_s4)\n    return_im = {}\n\n    for key in [\'pred_224\', \'pred_28_3\', \'pred_56_2\']:\n        return_im[key] = images[key][:,:,0:ph,0:pw]\n    del images\n\n    return return_im\n\ndef process_high_res_im(model, im, seg, para, name=None, aggre_device=\'cpu:0\'):\n\n    im = im.to(aggre_device)\n    seg = seg.to(aggre_device)\n\n    max_L = para[\'L\']\n    stride = para[\'stride\']\n\n    _, _, h, w = seg.shape\n\n    """"""\n    Global Step\n    """"""\n    if max(h, w) > max_L:\n        im_small = resize_max_side(im, max_L, \'area\')\n        seg_small = resize_max_side(seg, max_L, \'area\')\n    else:\n        im_small = im\n        seg_small = seg\n\n    images = safe_forward(model, im_small, seg_small)\n\n    pred_224 = images[\'pred_224\'].to(aggre_device)\n    pred_56 = images[\'pred_56_2\'].to(aggre_device)\n\n    # del images\n    if para[\'clear\']:\n        torch.cuda.empty_cache()\n    \n    """"""\n    Local step\n    """"""\n\n    for new_size in [max(h, w)]:\n        im_small = resize_max_side(im, new_size, \'area\')\n        seg_small = resize_max_side(seg, new_size, \'area\')\n        _, _, h, w = seg_small.shape\n\n        combined_224 = torch.zeros_like(seg_small)\n        combined_weight = torch.zeros_like(seg_small)\n\n        r_pred_224 = (F.interpolate(pred_224, size=(h, w), mode=\'bilinear\', align_corners=False)>0.5).float()*2-1\n        r_pred_56 = F.interpolate(pred_56, size=(h, w), mode=\'bilinear\', align_corners=False)*2-1\n\n        padding = 16\n        step_size = stride - padding*2\n        step_len  = max_L\n\n        used_start_idx = {}\n        for x_idx in range((w)//step_size+1):\n            for y_idx in range((h)//step_size+1):\n\n                start_x = x_idx * step_size\n                start_y = y_idx * step_size\n                end_x = start_x + step_len\n                end_y = start_y + step_len\n\n                # Shift when required\n                if end_y > h:\n                    end_y = h\n                    start_y = h - step_len\n                if end_x > w:\n                    end_x = w\n                    start_x = w - step_len\n\n                # Bound x/y range\n                start_x = max(0, start_x)\n                start_y = max(0, start_y)\n                end_x = min(w, end_x)\n                end_y = min(h, end_y)\n\n                # The same crop might appear twice due to bounding/shifting\n                start_idx = start_y*w + start_x\n                if start_idx in used_start_idx:\n                    continue\n                else:\n                    used_start_idx[start_idx] = True\n                \n                # Take crop\n                im_part = im_small[:,:,start_y:end_y, start_x:end_x]\n                seg_224_part = r_pred_224[:,:,start_y:end_y, start_x:end_x]\n                seg_56_part = r_pred_56[:,:,start_y:end_y, start_x:end_x]\n\n                # Skip when it is not an interesting crop anyway\n                seg_part_norm = (seg_224_part>0).float()\n                high_thres = 0.9\n                low_thres = 0.1\n                if (seg_part_norm.mean() > high_thres) or (seg_part_norm.mean() < low_thres):\n                    continue\n                grid_images = safe_forward(model, im_part, seg_224_part, seg_56_part)\n                grid_pred_224 = grid_images[\'pred_224\'].to(aggre_device)\n\n                # Padding\n                pred_sx = pred_sy = 0\n                pred_ex = step_len\n                pred_ey = step_len\n\n                if start_x != 0:\n                    start_x += padding\n                    pred_sx += padding\n                if start_y != 0:\n                    start_y += padding\n                    pred_sy += padding\n                if end_x != w:\n                    end_x -= padding\n                    pred_ex -= padding\n                if end_y != h:\n                    end_y -= padding\n                    pred_ey -= padding\n\n                combined_224[:,:,start_y:end_y, start_x:end_x] += grid_pred_224[:,:,pred_sy:pred_ey,pred_sx:pred_ex]\n\n                del grid_pred_224\n\n                if para[\'clear\']:\n                    torch.cuda.empty_cache()\n\n                # Used for averaging\n                combined_weight[:,:,start_y:end_y, start_x:end_x] += 1\n\n        # Final full resolution output\n        seg_norm = (r_pred_224/2+0.5)\n        pred_224 = combined_224 / combined_weight\n        pred_224 = torch.where(combined_weight==0, seg_norm, pred_224)\n\n    _, _, h, w = seg.shape\n    images = {}\n    images[\'pred_224\'] = F.interpolate(pred_224, size=(h, w), mode=\'bilinear\', align_corners=False)\n\n    if para[\'clear\']:\n        torch.cuda.empty_cache()\n\n    return images\n\n\ndef process_im_single_pass(model, im, seg, min_size, para):\n    """"""\n    A single pass version, aka global step only.\n    """"""\n\n    max_size = para[\'L\']\n\n    _, _, h, w = im.shape\n    if max(h, w) < min_size:\n        im = resize_max_side(im, min_size, \'bicubic\')\n        seg = resize_max_side(seg, min_size, \'bilinear\')\n\n    if max(h, w) > max_size:\n        im = resize_max_side(im, max_size, \'area\')\n        seg = resize_max_side(seg, max_size, \'area\')\n\n    images = safe_forward(model, im, seg)\n\n    if max(h, w) < min_size:\n        images[\'pred_224\'] = F.interpolate(images[\'pred_224\'], size=(h, w), mode=\'area\')\n    elif max(h, w) > max_size:\n        images[\'pred_224\'] = F.interpolate(images[\'pred_224\'], size=(h, w), mode=\'bilinear\', align_corners=False)\n\n    return images\n'"
eval_memory_usage.py,6,"b""import torch\n\nfrom models.psp.pspnet import PSPNet\n\nimport sys\n\n# Construct model\nmodel = PSPNet(sizes=(1, 2, 3, 6), psp_size=2048, deep_features_size=1024, backend='resnet50').cuda()\n\nL = int(sys.argv[1])\nbatch_size = 1\n\ndef safe_forward(model, im, seg):\n\n    b, _, ph, pw = seg.shape\n    if (ph % 8 != 0) or (pw % 8 != 0):\n        newH = ((ph//8+1)*8)\n        newW = ((pw//8+1)*8)\n        p_im = torch.zeros(b, 3, newH, newW).cuda()\n        p_seg = torch.zeros(b, 1, newH, newW).cuda() - 1\n\n        p_im[:,:,0:ph,0:pw] = im\n        p_seg[:,:,0:ph,0:pw] = seg\n        im = p_im\n        seg = p_seg\n\n    images = model(im, seg)\n\n    return images\n\nwith torch.no_grad():\n    for _ in range(10):\n        im = torch.zeros((1, 3, L, L)).cuda()\n        seg = torch.zeros((1, 1, L, L)).cuda()\n        images = safe_forward(model, im, seg)\n\n        print(torch.cuda.max_memory_allocated()/1024/1024/1024)\n\n        del im\n        del seg\n        del images\n"""
eval_post.py,0,"b'import numpy as np\nfrom PIL import Image\nimport progressbar\n\nfrom util.compute_boundary_acc import compute_boundary_acc\nfrom util.file_buffer import FileBuffer\n\nfrom argparse import ArgumentParser\nimport os\nimport re\n\n\nparser = ArgumentParser()\n\nparser.add_argument(\'--dir\', help=\'Directory with image, gt, and mask\')\n\nparser.add_argument(\'--output\', help=\'Output of temp results\',\n        default=None)\n\nargs = parser.parse_args()\n\ndef get_iu(seg, gt):\n    intersection = np.count_nonzero(seg & gt)\n    union = np.count_nonzero(seg | gt)\n    \n    return intersection, union \n\ntotal_new_i = 0\ntotal_new_u = 0\ntotal_old_i = 0\ntotal_old_u = 0\n\ntotal_old_correct_pixels = 0\ntotal_new_correct_pixels = 0\ntotal_num_pixels = 0\n\ntotal_num_images = 0\ntotal_seg_acc = 0\ntotal_mask_acc = 0\n\nsmall_objects = 0\n\nall_h = 0\nall_w = 0\nall_max = 0\n\nall_gts = [gt for gt in os.listdir(args.dir) if \'_gt.png\' in gt]\nfile_buffer = FileBuffer(os.path.join(args.dir, \'results_post.txt\'))\n\nif args.output is not None:\n    os.makedirs(args.output, exist_ok=True)\n\nfor gt_name in progressbar.progressbar(all_gts):\n    \n    gt = np.array(Image.open(os.path.join(args.dir, gt_name)\n                                ).convert(\'L\'))\n\n    seg = np.array(Image.open(os.path.join(args.dir, gt_name.replace(\'_gt\', \'_seg\'))\n                                ).convert(\'L\'))\n\n    mask_im = Image.open(os.path.join(args.dir, gt_name.replace(\'_gt\', \'_mask\'))\n                                ).convert(\'L\')\n    mask = seg.copy()\n    this_class = int(re.search(r\'\\d+\', gt_name[::-1]).group()[::-1]) - 1\n\n    rmin = cmin = 0\n    rmax, cmax = seg.shape\n\n    all_h += rmax\n    all_w += cmax\n    all_max += max(rmax, cmax)\n\n    mask_h, mask_w = mask.shape\n    if mask_h != cmax:\n        mask = np.array(mask_im.resize((cmax, rmax), Image.BILINEAR))\n\n    if seg.sum() < 32*32:\n        # Reject small objects, just copy input\n        small_objects += 1\n    else:\n        if (cmax==cmin) or (rmax==rmin):\n            # Should not happen. Check the input in this case.\n            print(gt_name, this_class)\n            continue\n        class_mask_prob = np.array(mask_im.resize((cmax-cmin, rmax-rmin), Image.BILINEAR))\n        mask[rmin:rmax, cmin:cmax] = class_mask_prob\n\n    """"""\n    Compute IoU and boundary accuracy\n    """"""\n    gt = gt > 128\n    seg = seg > 128\n    mask = mask > 128\n\n    old_i, old_u = get_iu(gt, seg)\n    new_i, new_u = get_iu(gt, mask)\n\n    total_new_i += new_i\n    total_new_u += new_u\n    total_old_i += old_i\n    total_old_u += old_u\n\n    seg_acc, mask_acc = compute_boundary_acc(gt, seg, mask)\n    total_seg_acc += seg_acc\n    total_mask_acc += mask_acc\n    total_num_images += 1\n    \n    if args.output is not None:\n        gt = Image.fromarray(gt)\n        seg = Image.fromarray(seg)\n        mask = Image.fromarray(mask)\n\n        gt.save(os.path.join(args.output, gt_name))\n        seg.save(os.path.join(args.output, gt_name.replace(\'_gt.png\', \'_seg.png\')))\n        mask.save(os.path.join(args.output, gt_name.replace(\'_gt.png\', \'_mask.png\')))\n\nnew_iou = total_new_i/total_new_u\nold_iou = total_old_i/total_old_u\nnew_mba = total_mask_acc/total_num_images\nold_mba = total_seg_acc/total_num_images\n\nfile_buffer.write(\'New IoU  : \', new_iou)\nfile_buffer.write(\'Old IoU  : \', old_iou)\nfile_buffer.write(\'IoU Delta: \', new_iou-old_iou)\n\nfile_buffer.write(\'New mBA  : \', new_mba)\nfile_buffer.write(\'Old mBA  : \', old_mba)\nfile_buffer.write(\'mBA Delta: \', new_mba-old_mba)\n\nfile_buffer.write(\'Avg. H+W  : \', (all_h+all_w)/total_num_images)\nfile_buffer.write(\'Avg. Max(H,W) : \', all_max/total_num_images)\n\nfile_buffer.write(\'Number of small objects: \', small_objects)\n'"
eval_post_ade.py,0,"b""import numpy as np\nfrom PIL import Image\nimport progressbar\n\nfrom util.compute_boundary_acc import compute_boundary_acc_multi_class\nfrom util.file_buffer import FileBuffer\nfrom dataset.make_bb_trans import get_bb_position, scale_bb_by\n\nfrom argparse import ArgumentParser\nimport glob\nimport os\nimport re\nfrom pathlib import Path\nfrom shutil import copyfile\n\ndef color_map(N=256, normalized=False):\n    def bitget(byteval, idx):\n        return ((byteval & (1 << idx)) != 0)\n\n    dtype = 'float32' if normalized else 'uint8'\n    cmap = np.zeros((N, 3), dtype=dtype)\n    for i in range(N):\n        r = g = b = 0\n        c = i\n        for j in range(8):\n            r = r | (bitget(c, 0) << 7-j)\n            g = g | (bitget(c, 1) << 7-j)\n            b = b | (bitget(c, 2) << 7-j)\n            c = c >> 3\n\n        cmap[i] = np.array([r, g, b])\n\n    cmap = cmap/255 if normalized else cmap\n    return cmap\n\nparser = ArgumentParser()\n\nparser.add_argument('--mask_dir', help='Directory with all the _mask.png outputs',\n        default=os.path.join('./output/ade_output'))\n\nparser.add_argument('--gt_dir', help='Directory with original size GT images (in P mode)',\n        default=os.path.join('./data/ADE/annotations'))\n\nparser.add_argument('--seg_dir', help='Directory with original size input segmentation images (in L mode)',\n        default=os.path.join('./data/ADE/inputs'))\n\nparser.add_argument('--split_dir', help='Directory with the processed split dataset',\n        default=os.path.join('./data/ADE/split_ss'))\n\n# Optional\nparser.add_argument('--im_dir', help='Directory with original size input images (in RGB mode)',\n        default=os.path.join('.', './data/ADE/images'))\n\nparser.add_argument('--output', help='Output of temp results',\n        default=None)\n\nargs = parser.parse_args()\n\ndef get_iu(seg, gt):\n    intersection = np.count_nonzero(seg & gt)\n    union = np.count_nonzero(seg | gt)\n    \n    return intersection, union \n\ntotal_old_correct_pixels = 0\ntotal_new_correct_pixels = 0\ntotal_num_pixels = 0\n\ntotal_seg_mba = 0\ntotal_mask_mba = 0\ntotal_num_images = 0\n\nsmall_objects = 0\n\nnum_classes = 150\n\nnew_class_i = [0] * num_classes\nnew_class_u = [0] * num_classes\nold_class_i = [0] * num_classes\nold_class_u = [0] * num_classes\nedge_class_pixel = [0] * num_classes\nold_gd_class_pixel = [0] * num_classes\nnew_gd_class_pixel = [0] * num_classes\n\nall_gts = os.listdir(args.seg_dir)\nmask_path = Path(args.mask_dir)\n\nif args.output is not None:\n    os.makedirs(args.output, exist_ok=True)\n    file_buffer = FileBuffer(os.path.join(args.output, 'results_post.txt'))\n\nfor gt_name in progressbar.progressbar(all_gts):\n    \n    gt = np.array(Image.open(os.path.join(args.gt_dir, gt_name)\n                                ).convert('P'))\n\n    seg = np.array(Image.open(os.path.join(args.seg_dir, gt_name)\n                                ).convert('L'))\n\n    # We pick the highest confidence class label for overlapping region\n    mask = seg.copy()\n    confidence = np.zeros_like(gt) + 0.5\n    keep = False\n    for mask_name in mask_path.glob(gt_name[:-4] + '*mask*'):\n        class_mask_prob = np.array(Image.open(mask_name).convert('L')).astype('float') / 255\n        class_string = re.search(r'\\d+.\\d+', mask_name.name[::-1]).group()[::-1]\n        this_class = int(class_string.split('.')[0])\n        class_seg = np.array(\n                Image.open(\n                    os.path.join(args.split_dir, mask_name.name.replace('mask', 'seg'))\n                ).convert('L')\n            ).astype('float') / 255\n\n        try:\n            rmin, rmax, cmin, cmax = get_bb_position(class_seg)\n            rmin, rmax, cmin, cmax = scale_bb_by(rmin, rmax, cmin, cmax, seg.shape[0], seg.shape[1], 0.25, 0.25)\n        except:\n            # Sometimes we cannot get a proper bounding box\n            rmin = cmin = 0\n            rmax, cmax = seg.shape\n\n        if (cmax==cmin) or (rmax==rmin):\n            print(gt_name, this_class)\n            continue\n        class_mask_prob = np.array(Image.fromarray(class_mask_prob).resize((cmax-cmin, rmax-rmin), Image.BILINEAR))\n\n        background_classes = [1,2,3,4,6,7,10,12,14,17,22,26,27,29,30,47,49,52,53,55,60,61,62,69,80,85,92,95,97,102,106,110,114,129,141]\n        if this_class in background_classes:\n            class_mask_prob = class_mask_prob * 0.51\n\n        # Record the current higher confidence level for each pixel\n        mask[rmin:rmax, cmin:cmax] = np.where(class_mask_prob>confidence[rmin:rmax, cmin:cmax], \n                                                this_class, mask[rmin:rmax, cmin:cmax])\n        confidence[rmin:rmax, cmin:cmax] = np.maximum(confidence[rmin:rmax, cmin:cmax], class_mask_prob)\n\n    total_classes = np.union1d(np.unique(gt), np.unique(seg))\n    seg[gt==0] = 0\n    mask[gt==0] = 0\n    total_classes = total_classes[1:] # Remove background class\n    # Shift background class to -1\n    total_classes -= 1\n\n    for c in total_classes:\n        gt_class = (gt == (c+1))\n        seg_class = (seg == (c+1))\n        mask_class = (mask == (c+1))\n\n        old_i, old_u = get_iu(gt_class, seg_class)\n        new_i, new_u = get_iu(gt_class, mask_class)\n\n        total_old_correct_pixels += old_i\n        total_new_correct_pixels += new_i\n        total_num_pixels += gt_class.sum()\n\n        new_class_i[c] += new_i\n        new_class_u[c] += new_u\n        old_class_i[c] += old_i\n        old_class_u[c] += old_u\n\n    seg_acc, mask_acc = compute_boundary_acc_multi_class(gt, seg, mask)\n    total_seg_mba += seg_acc\n    total_mask_mba += mask_acc\n    total_num_images += 1\n\n    if args.output is not None and keep:\n        gt = Image.fromarray(gt,mode='P')\n        seg = Image.fromarray(seg,mode='P')\n        mask = Image.fromarray(mask,mode='P')\n        gt.putpalette(color_map())\n        seg.putpalette(color_map())\n        mask.putpalette(color_map())\n\n        gt.save(os.path.join(args.output, gt_name.replace('.png', '_gt.png')))\n        seg.save(os.path.join(args.output, gt_name.replace('.png', '_seg.png')))\n        mask.save(os.path.join(args.output, gt_name.replace('.png', '_mask.png')))\n\n        if args.im_dir is not None:\n            copyfile(os.path.join(args.im_dir, gt_name.replace('.png','.jpg')), \n            os.path.join(args.output, gt_name.replace('.png','.jpg')))\n\nfile_buffer.write('New pixel accuracy: ', total_new_correct_pixels / total_num_pixels)\nfile_buffer.write('Old pixel accuracy: ', total_old_correct_pixels / total_num_pixels)\n\nfile_buffer.write('Number of small objects: ', small_objects)\n\nfile_buffer.write('Now giving class information')\n\nnew_class_iou = [0] * num_classes\nold_class_iou = [0] * num_classes\nnew_class_boundary = [0] * num_classes\nold_class_boundary = [0] * num_classes\n\nprint('\\nNew IOUs: ')\nfor i in range(num_classes):\n    new_class_iou[i] = new_class_i[i] / (new_class_u[i] + 1e-6)\n    print('%.3f' % (new_class_iou[i]), end=' ')\n\nprint('\\nOld IOUs: ')\nfor i in range(num_classes):\n    old_class_iou[i] = old_class_i[i] / (old_class_u[i] + 1e-6)\n    print('%.3f' % (old_class_iou[i]), end=' ')\n\nfile_buffer.write()\nfile_buffer.write('Average over classes')\n\nold_miou = np.array(old_class_iou).mean()\nnew_miou = np.array(new_class_iou).mean()\nold_mba = total_seg_mba/total_num_images\nnew_mba = total_mask_mba/total_num_images\n\nfile_buffer.write('Old mIoU    : ', old_miou)\nfile_buffer.write('New mIoU    : ', new_miou)\nfile_buffer.write('mIoU Delta  : ', new_miou - old_miou)\nfile_buffer.write('Old mBA     : ', old_mba)\nfile_buffer.write('New mBA     : ', new_mba)\nfile_buffer.write('mBA Delta   : ', new_mba - old_mba)\n"""
train.py,5,"b""import numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch import optim\nfrom torch.utils.data import DataLoader, ConcatDataset\n\nfrom models.psp.pspnet import PSPNet\nfrom models.sobel_op import SobelComputer\nfrom dataset import OnlineTransformDataset\nfrom util.logger import BoardLogger\nfrom util.model_saver import ModelSaver\nfrom util.hyper_para import HyperParameters\nfrom util.log_integrator import Integrator\nfrom util.metrics_compute import compute_loss_and_metrics, iou_hooks_to_be_used\nfrom util.image_saver import vis_prediction\n\nimport time\nimport os\nimport datetime\n\ntorch.backends.cudnn.benchmark = True\n\n# Parse command line arguments\npara = HyperParameters()\npara.parse()\n\n# Logging\nif para['id'].lower() != 'null':\n    long_id = '%s_%s' % (para['id'],datetime.datetime.now().strftime('%Y-%m-%d_%H:%M:%S'))\nelse:\n    long_id = None\nlogger = BoardLogger(long_id)\nlogger.log_string('hyperpara', str(para))\n\nprint('CUDA Device count: ', torch.cuda.device_count())\n\n# Construct model\nmodel = PSPNet(sizes=(1, 2, 3, 6), psp_size=2048, deep_features_size=1024, backend='resnet50')\nmodel = nn.DataParallel(\n        model.cuda(), device_ids=[0,1]\n    )\n\nif para['load'] is not None:\n    model.load_state_dict(torch.load(para['load']))\noptimizer = optim.Adam(model.parameters(), lr=para['lr'], weight_decay=para['weight_decay'])\n\n\nduts_tr_dir = os.path.join('data', 'DUTS-TR')\nduts_te_dir = os.path.join('data', 'DUTS-TE')\necssd_dir = os.path.join('data', 'ecssd')\nmsra_dir = os.path.join('data', 'MSRA_10K')\n\nfss_dataset = OnlineTransformDataset(os.path.join('data', 'fss'), method=0, perturb=True)\nduts_tr_dataset = OnlineTransformDataset(duts_tr_dir, method=1, perturb=True)\nduts_te_dataset = OnlineTransformDataset(duts_te_dir, method=1, perturb=True)\necssd_dataset = OnlineTransformDataset(ecssd_dir, method=1, perturb=True)\nmsra_dataset = OnlineTransformDataset(msra_dir, method=1, perturb=True)\n\nprint('FSS dataset size: ', len(fss_dataset))\nprint('DUTS-TR dataset size: ', len(duts_tr_dataset))\nprint('DUTS-TE dataset size: ', len(duts_te_dataset))\nprint('ECSSD dataset size: ', len(ecssd_dataset))\nprint('MSRA-10K dataset size: ', len(msra_dataset))\n\ntrain_dataset = ConcatDataset([fss_dataset, duts_tr_dataset, duts_te_dataset, ecssd_dataset, msra_dataset])\n\nprint('Total training size: ', len(train_dataset))\n\n# For randomness: https://github.com/pytorch/pytorch/issues/5059\ndef worker_init_fn(worker_id): \n    np.random.seed(np.random.get_state()[1][0] + worker_id)\n\n# Dataloaders, multi-process data loading\ntrain_loader = DataLoader(train_dataset, para['batch_size'], shuffle=True, num_workers=8,\n                            worker_init_fn=worker_init_fn, drop_last=True, pin_memory=True)\n\nsobel_compute = SobelComputer()\n\n# Learning rate decay scheduling\nscheduler = optim.lr_scheduler.MultiStepLR(optimizer, para['steps'], para['gamma'])\n\nsaver = ModelSaver(long_id)\nreport_interval = 50\nsave_im_interval = 800\n\ntotal_epoch = int(para['iterations']/len(train_loader) + 0.5)\nprint('Actual training epoch: ', total_epoch)\n\ntrain_integrator = Integrator(logger)\ntrain_integrator.add_hook(iou_hooks_to_be_used)\ntotal_iter = 0\nlast_time = 0\nfor e in range(total_epoch):\n    np.random.seed() # reset seed\n    epoch_start_time = time.time()\n\n    # Train loop\n    model = model.train()\n    for im, seg, gt in train_loader:\n        im, seg, gt = im.cuda(), seg.cuda(), gt.cuda()\n\n        total_iter += 1\n        if total_iter % 5000 == 0:\n            saver.save_model(model, total_iter)\n\n        images = model(im, seg)\n\n        images['im'] = im\n        images['seg'] = seg\n        images['gt'] = gt\n\n        sobel_compute.compute_edges(images)\n\n        loss_and_metrics = compute_loss_and_metrics(images, para)\n        train_integrator.add_dict(loss_and_metrics)\n\n        optimizer.zero_grad()\n        (loss_and_metrics['total_loss']).backward()\n        optimizer.step()\n\n        if total_iter % report_interval == 0:\n            logger.log_scalar('train/lr', scheduler.get_lr()[0], total_iter)\n            train_integrator.finalize('train', total_iter)\n            train_integrator.reset_except_hooks()\n\n        # Need to put step AFTER get_lr() for correct logging, see issue #22107 in PyTorch\n        scheduler.step()\n\n        if total_iter % save_im_interval == 0:\n            predict_vis = vis_prediction(images)\n            logger.log_cv2('train/predict', predict_vis, total_iter)\n\n# Final save!\nsaver.save_model(model, total_iter)\n"""
dataset/__init__.py,0,b'from .online_dataset import OnlineTransformDataset\nfrom .offline_dataset import OfflineDataset\nfrom .split_dataset import SplitTransformDataset'
dataset/make_bb_trans.py,0,"b'import numpy as np\n\ndef is_bb_overlap(rmin, rmax, cmin, cmax, \n                    crmin, crmax, ccmin, ccmax):\n    \n    is_y_overlap = (rmax > crmin) and (crmax > rmin)\n    is_x_overlap = (cmax > ccmin) and (ccmax > cmin)\n\n    return is_x_overlap and is_y_overlap\n\ndef get_bb_position(mask):\n    mask = mask > 0.5\n    rows = np.any(mask, axis=1)\n    cols = np.any(mask, axis=0)\n    rmin, rmax = np.where(rows)[0][[0, -1]]\n    cmin, cmax = np.where(cols)[0][[0, -1]]\n\n    # y_min, y_max, x_min, x_max\n    return rmin, rmax, cmin, cmax\n\ndef scale_bb_by(rmin, rmax, cmin, cmax, im_height, im_width, h_scale, w_scale):\n    height = rmax - rmin\n    width = cmax - cmin\n\n    rmin -= h_scale * height / 2\n    rmax += h_scale * height / 2\n    cmin -= w_scale * width / 2\n    cmax += w_scale * width / 2\n\n    rmin = int(max(0, rmin))\n    rmax = int(min(im_height-1, rmax))\n    cmin = int(max(0, cmin))\n    cmax = int(min(im_width-1, cmax))\n\n    # Prevent negative width/height\n    rmax = max(rmin, rmax)\n    cmax = max(cmin, cmax)\n\n    return rmin, rmax, cmin, cmax \n'"
dataset/offline_dataset.py,1,"b'import os\nfrom os import path\nfrom torch.utils.data.dataset import Dataset\nfrom torchvision import transforms, utils\nfrom torchvision.transforms import functional\nfrom PIL import Image\nimport numpy as np\nimport progressbar\n\nfrom dataset.make_bb_trans import *\n\nclass OfflineDataset(Dataset):\n    def __init__(self, root, in_memory=False, need_name=False, resize=False, do_crop=False):\n        self.root = root\n        self.need_name = need_name\n        self.resize = resize\n        self.do_crop = do_crop\n        self.in_memory = in_memory\n\n        imgs = os.listdir(root)\n        imgs = sorted(imgs)\n\n        """"""\n        There are three kinds of files: _im.png, _seg.png, _gt.png\n        """"""\n        im_list = [im for im in imgs if \'im\' in im[-7:].lower()]\n\n        self.im_list = [path.join(root, im) for im in im_list]\n\n        print(\'%d images found\' % len(self.im_list))\n\n        # Make up some transforms\n        self.im_transform = transforms.Compose([\n            transforms.ToTensor(),\n            transforms.Normalize(\n                mean=[0.485, 0.456, 0.406],\n                std=[0.229, 0.224, 0.225]\n            ),\n        ])\n\n        self.gt_transform = transforms.Compose([\n            transforms.ToTensor(),\n        ])\n\n        self.seg_transform = transforms.Compose([\n            transforms.ToTensor(),\n            transforms.Normalize(\n                mean=[0.5],\n                std=[0.5]\n            ),\n        ])\n\n        if self.resize:\n            self.resize_bi = lambda x: x.resize((224, 224), Image.BILINEAR)\n            self.resize_nr = lambda x: x.resize((224, 224), Image.NEAREST)\n        else:\n            self.resize_bi = lambda x: x\n            self.resize_nr = lambda x: x\n\n        if self.in_memory:\n            print(\'Loading things into memory\')\n            self.images = []\n            self.gts = []\n            self.segs = []\n            for im in progressbar.progressbar(self.im_list):\n                image, seg, gt = self.load_tuple(im)\n\n                self.images.append(image)\n                self.segs.append(seg)\n                self.gts.append(gt)\n        \n    def load_tuple(self, im):\n        seg = Image.open(im[:-7]+\'_seg.png\').convert(\'L\')\n        crop_lambda = self.get_crop_lambda(seg)\n\n        image = self.resize_bi(crop_lambda(Image.open(im).convert(\'RGB\')))\n        gt = self.resize_bi(crop_lambda(Image.open(im[:-7]+\'_gt.png\').convert(\'L\')))\n        seg = self.resize_bi(crop_lambda(Image.open(im[:-7]+\'_seg.png\').convert(\'L\')))\n\n        return image, seg, gt\n\n    def get_crop_lambda(self, seg):\n        if self.do_crop:\n            seg = np.array(seg)\n            h, w = seg.shape\n            try:\n                bb = get_bb_position(seg)\n                rmin, rmax, cmin, cmax = scale_bb_by(*bb, h, w, 0.15, 0.15)\n                return lambda x: functional.crop(x, rmin, cmin, rmax-rmin, cmax-cmin)\n            except:\n                return lambda x: x\n        else:\n            return lambda x: x\n\n    def __getitem__(self, idx):\n        if self.in_memory:\n            im = self.images[idx]\n            gt = self.gts[idx]\n            seg = self.segs[idx]\n        else:\n            im, seg, gt = self.load_tuple(self.im_list[idx])\n\n        im = self.im_transform(im)\n        gt = self.gt_transform(gt)\n        seg = self.seg_transform(seg)\n\n        if self.need_name:\n            return im, seg, gt, os.path.basename(self.im_list[idx][:-7])\n        else:\n            return im, seg, gt\n\n    def __len__(self):\n        return len(self.im_list)\n        \nif __name__ == \'__main__\':\n    o = OfflineDataset(\'data/val_static\')\n'"
dataset/online_dataset.py,1,"b'import os\nfrom os import path\nimport warnings\n\nfrom torch.utils.data.dataset import Dataset\nfrom torchvision import transforms, utils\nfrom PIL import Image\nimport numpy as np\nimport random\nimport util.boundary_modification as boundary_modification\n\nseg_normalization = transforms.Normalize(\n                mean=[0.5],\n                std=[0.5]\n            )\n\nclass OnlineTransformDataset(Dataset):\n    """"""\n    Method 0 - FSS style (class/1.jpg class/1.png)\n    Method 1 - Others style (XXX.jpg XXX.png)\n    """"""\n    def __init__(self, root, need_name=False, method=0, perturb=True):\n        self.root = root\n        self.need_name = need_name\n        self.method = method\n\n        if method == 0:\n            # Get images\n            self.im_list = []\n            classes = os.listdir(self.root)\n            for c in classes:\n                imgs = os.listdir(path.join(root, c))\n                jpg_list = [im for im in imgs if \'jpg\' in im[-3:].lower()]\n                unmatched = any([im.replace(\'.jpg\', \'.png\') not in imgs for im in jpg_list])\n\n                if unmatched:\n                    print(\'Number of image/gt unmatch in class \', c)\n                    print(\'The whole class is ignored\', len(jpg_list))\n\n                    warnings.warn(\'Dataset unmatch error\')\n                else:\n                    joint_list = [path.join(root, c, im) for im in jpg_list]\n                    self.im_list.extend(joint_list)\n\n        elif method == 1:\n            self.im_list = [path.join(self.root, im) for im in os.listdir(self.root) if \'.jpg\' in im]\n\n        print(\'%d images found\' % len(self.im_list))\n\n        if perturb:\n            # Make up some transforms\n            self.bilinear_dual_transform = transforms.Compose([\n                transforms.RandomCrop((224, 224), pad_if_needed=True),\n                transforms.RandomHorizontalFlip(),\n            ])\n\n            self.bilinear_dual_transform_im = transforms.Compose([\n                transforms.RandomCrop((224, 224), pad_if_needed=True),\n                transforms.RandomHorizontalFlip(),\n            ])\n\n            self.im_transform = transforms.Compose([\n                transforms.ColorJitter(0.2, 0.05, 0.05, 0),\n                transforms.RandomGrayscale(),\n                transforms.ToTensor(),\n                transforms.Normalize(\n                    mean=[0.485, 0.456, 0.406],\n                    std=[0.229, 0.224, 0.225]\n                ),\n            ])\n        else:\n            # Make up some transforms\n            self.bilinear_dual_transform = transforms.Compose([\n                transforms.Resize(224, interpolation=Image.BILINEAR), \n                transforms.CenterCrop(224),\n            ])\n\n            self.im_transform = transforms.Compose([\n                transforms.ToTensor(),\n                transforms.Normalize(\n                    mean=[0.485, 0.456, 0.406],\n                    std=[0.229, 0.224, 0.225]\n                ),\n            ])\n\n        self.gt_transform = transforms.Compose([\n            transforms.ToTensor(),\n        ])\n\n        self.seg_transform = transforms.Compose([\n            transforms.ToTensor(),\n            seg_normalization,\n        ])\n\n    def __getitem__(self, idx):\n        im = Image.open(self.im_list[idx]).convert(\'RGB\')\n\n        if self.method == 0:\n            gt = Image.open(self.im_list[idx][:-3]+\'png\').convert(\'L\')\n        else:\n            gt = Image.open(self.im_list[idx].replace(\'.jpg\',\'.png\')).convert(\'L\')\n\n        seed = np.random.randint(2147483647)\n\n        random.seed(seed)\n        im = self.bilinear_dual_transform_im(im)\n\n        random.seed(seed)\n        gt = self.bilinear_dual_transform(gt)\n\n        iou_max = 1.0\n        iou_min = 0.8\n        iou_target = np.random.rand()*(iou_max-iou_min) + iou_min\n        seg = boundary_modification.modify_boundary((np.array(gt)>0.5).astype(\'uint8\')*255, iou_target=iou_target)\n\n        im = self.im_transform(im)\n        gt = self.gt_transform(gt)\n        seg = self.seg_transform(seg)\n\n        if self.need_name:\n            return im, seg, gt, os.path.basename(self.im_list[idx][:-4])\n        else:\n            return im, seg, gt\n\n    def __len__(self):\n        return len(self.im_list)\n\nif __name__ == \'__main__\':\n    o = OnlineTransformDataset(\'data/train\')\n    o = OnlineTransformDataset(\'data/val\')\n'"
dataset/split_dataset.py,1,"b""import os\nfrom torch.utils.data.dataset import Dataset\nfrom torchvision import transforms, utils\nfrom PIL import Image\nimport numpy as np\nimport progressbar\n\nfrom dataset.make_bb_trans import *\nimport util.boundary_modification as boundary_modification\n\nseg_normalization = transforms.Normalize(\n                mean=[0.5],\n                std=[0.5]\n            )\n\nclass SplitTransformDataset(Dataset):\n    def __init__(self, root, in_memory=False, need_name=False, perturb=True, img_suffix='_im.jpg'):\n        self.root = root\n        self.need_name = need_name\n        self.in_memory = in_memory\n        self.perturb = perturb\n        self.img_suffix = img_suffix\n\n        imgs = os.listdir(self.root)\n\n        self.im_list = [im for im in imgs if '_im' in im]\n        self.gt_list = [im for im in imgs if '_gt' in im]\n\n        print('%d ground truths found' % len(self.gt_list))\n\n        if perturb:\n            # Make up some transforms\n            self.im_transform = transforms.Compose([\n                transforms.ColorJitter(0.2, 0.2, 0.2, 0.2),\n                transforms.RandomGrayscale(),\n                # transforms.Resize((224, 224), interpolation=Image.BILINEAR),\n                transforms.ToTensor(),\n                transforms.Normalize(\n                    mean=[0.485, 0.456, 0.406],\n                    std=[0.229, 0.224, 0.225]\n                ),\n            ])\n        else:\n            # Make up some transforms\n            self.im_transform = transforms.Compose([\n                # transforms.Resize((224, 224), interpolation=Image.BILINEAR),\n                transforms.ToTensor(),\n                transforms.Normalize(\n                    mean=[0.485, 0.456, 0.406],\n                    std=[0.229, 0.224, 0.225]\n                ),\n            ])\n\n        self.gt_transform = transforms.Compose([\n            # transforms.Resize((224, 224), interpolation=Image.NEAREST),\n            transforms.ToTensor(),\n        ])\n\n        self.seg_transform = transforms.Compose([\n            # transforms.Resize((224, 224), interpolation=Image.BILINEAR),\n            transforms.ToTensor(),\n            seg_normalization,\n        ])\n\n        # Map ground truths to images\n        self.gt_to_im = []\n        for im in self.gt_list:\n            # Find the second last underscore and remove from there to get basename\n            end_idx = im[:-8].rfind('_')\n            self.gt_to_im.append(im[:end_idx])\n\n        if self.in_memory:\n            self.images = {}\n            for im in progressbar.progressbar(self.im_list):\n                # Remove img_suffix, indexing might be faster but well..\n                self.images[im.replace(self.img_suffix, '')] = Image.open(self.join_path(im)).convert('RGB')\n            print('Images loaded to memory.')\n\n            self.gts = []\n            for im in progressbar.progressbar(self.gt_list):\n                self.gts.append(Image.open(self.join_path(im)).convert('L'))\n            print('Ground truths loaded to memory')\n\n            if not self.perturb:\n                self.segs = []\n                for im in progressbar.progressbar(self.gt_list):\n                    self.segs.append(Image.open(self.join_path(im.replace('_gt', '_seg'))).convert('L'))\n                print('Input segmentations loaded to memory')\n\n    def join_path(self, im):\n        return os.path.join(self.root, im)\n\n    def __getitem__(self, idx):\n        if self.in_memory:\n            gt = self.gts[idx]\n            im = self.images[self.gt_to_im[idx]]\n            if not self.perturb:\n                seg = self.segs[idx]\n        else:\n            gt = Image.open(self.join_path(self.gt_list[idx])).convert('L')\n            im = Image.open(self.join_path(self.gt_to_im[idx]+self.img_suffix)).convert('RGB')\n            if not self.perturb:\n                seg = Image.open(self.join_path(self.gt_list[idx].replace('_gt', '_seg'))).convert('L')\n\n        # Get bounding box from ground truth\n        if self.perturb:\n            im_width, im_height = gt.size # PIL inverted width/height\n            try:\n                bb_pos = get_bb_position(np.array(gt))\n                bb_pos = mod_bb(*bb_pos, im_height, im_width, 0.1, 0.1)\n                rmin, rmax, cmin, cmax = scale_bb_by(*bb_pos, im_height, im_width, 0.25, 0.25)\n            except:\n                print('Failed to get bounding box')\n                rmin = cmin = 0\n                rmax = im_height\n                cmax = im_width\n        else:\n            im_width, im_height = seg.size # PIL inverted width/height\n            try:\n                bb_pos = get_bb_position(np.array(seg))\n                rmin, rmax, cmin, cmax = scale_bb_by(*bb_pos, im_height, im_width, 0.25, 0.25)\n            except:\n                print('Failed to get bounding box')\n                rmin = cmin = 0\n                rmax = im_height\n                cmax = im_width\n\n        # If no GT then we ha ha ha\n        if (rmax-rmin==0 or cmax-cmin==0):\n            print('No GT, no cropping is done.')\n            crop_lambda = lambda x: x\n        else:\n            crop_lambda = lambda x: transforms.functional.crop(x, rmin, cmin, rmax-rmin, cmax-cmin)\n\n        im = crop_lambda(im)\n        gt = crop_lambda(gt)\n\n        if self.perturb:\n            iou_max = 1.0\n            iou_min = 0.7\n            iou_target = np.random.rand()*(iou_max-iou_min) + iou_min\n            seg = boundary_modification.modify_boundary((np.array(gt)>0.5).astype('uint8')*255, iou_target=iou_target)\n            seg = Image.fromarray(seg)\n        else:\n            seg = crop_lambda(seg)\n\n        im = self.im_transform(im)\n        gt = self.gt_transform(gt)\n        seg = self.seg_transform(seg)\n\n        if self.need_name:\n            return im, seg, gt, os.path.basename(self.gt_list[idx][:-7])\n        else:\n            return im, seg, gt\n\n    def __len__(self):\n        return len(self.gt_list)\n"""
models/__init__.py,0,b''
models/sobel_op.py,4,"b""import torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\nimport numpy as np\n\nclass SobelOperator(nn.Module):\n    def __init__(self, epsilon):\n        super().__init__()\n        self.epsilon = epsilon\n\n        x_kernel = np.array([[1, 0, -1], [2, 0, -2], [1, 0, -1]])/4\n        self.conv_x = nn.Conv2d(1, 1, kernel_size=3, stride=1, padding=1, bias=False)\n        self.conv_x.weight.data = torch.tensor(x_kernel).unsqueeze(0).unsqueeze(0).float().cuda()\n        self.conv_x.weight.requires_grad = False\n\n        y_kernel = np.array([[1, 2, 1], [0, 0, 0], [-1, -2, -1]])/4\n        self.conv_y = nn.Conv2d(1, 1, kernel_size=3, stride=1, padding=1, bias=False)\n        self.conv_y.weight.data = torch.tensor(y_kernel).unsqueeze(0).unsqueeze(0).float().cuda()\n        self.conv_y.weight.requires_grad = False\n\n    def forward(self, x):\n\n        b, c, h, w = x.shape\n        if c > 1:\n            x = x.view(b*c, 1, h, w)\n\n        x = F.avg_pool2d(x, kernel_size=3, stride=1, padding=1)\n\n        grad_x = self.conv_x(x)\n        grad_y = self.conv_y(x)\n        \n        x = torch.sqrt(grad_x ** 2 + grad_y ** 2 + self.epsilon)\n\n        x = x.view(b, c, h, w)\n\n        return x\n\nclass SobelComputer:\n    def __init__(self):\n        self.sobel = SobelOperator(1e-4)\n\n    def compute_edges(self, images):\n        images['gt_sobel'] = self.sobel(images['gt'])\n        images['pred_sobel'] = self.sobel(images['pred_224'])"""
scripts/__init__.py,0,b''
scripts/download_training_dataset.py,0,"b'import os\nfrom shutil import copyfile, copytree\nimport glob\n\nos.system(""rm -r ../tmp_download_files"")\n\nos.makedirs(""../tmp_download_files"", exist_ok=True)\n\n# MSRA10K\nos.system(""wget -P ../tmp_download_files http://mftp.mmcheng.net/Data/MSRA10K_Imgs_GT.zip"")\n# ECSSD_url\nos.system(\n    ""wget -P ../tmp_download_files http://www.cse.cuhk.edu.hk/leojia/projects/hsaliency/data/ECSSD/ground_truth_mask.zip"")\nos.system(""wget -P ../tmp_download_files http://www.cse.cuhk.edu.hk/leojia/projects/hsaliency/data/ECSSD/images.zip"")\n# FSS1000\nos.system(\n    ""wget --load-cookies /tmp/cookies.txt \\""https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate \'https://docs.google.com/uc?export=download&id=16TgqOeI_0P41Eh3jWQlxlRXG9KIqtMgI\' -O- | sed -rn \'s/.*confirm=([0-9A-Za-z_]+).*/\\\\1\\\\n/p\')&id=16TgqOeI_0P41Eh3jWQlxlRXG9KIqtMgI\\"" -O ../tmp_download_files/fewshot_data.zip && rm -rf /tmp/cookies.txt"")\n# DUT-OMRON ========== Link is not working???\nos.system(""wget -P ../tmp_download_files http://saliencydetection.net/duts/download/DUTS-TR.zip"")\nos.system(""wget -P ../tmp_download_files http://saliencydetection.net/duts/download/DUTS-TE.zip"")\n\n# Unzip everything\nos.system(""unzip ../tmp_download_files/MSRA10K_Imgs_GT.zip -d ../tmp_download_files"")\nos.system(""unzip ../tmp_download_files/images.zip -d ../tmp_download_files"")\nos.system(""unzip ../tmp_download_files/ground_truth_mask.zip -d ../tmp_download_files"")\nos.system(""unzip ../tmp_download_files/fewshot_data.zip -d ../tmp_download_files"")\n\nos.makedirs(""../tmp_download_files/DUTS"", exist_ok=True)\nos.system(""unzip ../tmp_download_files/DUTS-TR.zip -d ../tmp_download_files/DUTS"")\nos.system(""unzip ../tmp_download_files/DUTS-TE.zip -d ../tmp_download_files/DUTS"")\n\n# Move to data folder\nos.makedirs(""../data/DUTS-TE"", exist_ok=True)\nos.makedirs(""../data/DUTS-TR"", exist_ok=True)\n\nfor file in glob.glob(""../tmp_download_files/DUTS/DUTS-TE/*/*""):\n    copyfile(file, ""../data/DUTS-TE/"" + os.path.basename(file))\n\nfor file in glob.glob(""../tmp_download_files/DUTS/DUTS-TR/*/*""):\n    copyfile(file, ""../data/DUTS-TR/"" + os.path.basename(file))\n\nos.makedirs(""../data/fss"", exist_ok=True)\nfor cl in os.listdir(""../tmp_download_files/fewshot_data/fewshot_data""):\n    copytree(""../tmp_download_files/fewshot_data/fewshot_data/"" + cl, ""../data/fss/"" + cl)\n\nos.makedirs(""../data/ecssd"", exist_ok=True)\nfor gt in glob.glob(""../tmp_download_files/images/*""):\n    copyfile(gt, ""../data/ecssd/{}"".format(os.path.basename(gt)))\nfor gt in glob.glob(""../training_dataset/ground_truth_mask/*""):\n    copyfile(gt, ""../data/ecssd/{}"".format(os.path.basename(gt)))\n\nos.makedirs(""../data/MSRA_10K"", exist_ok=True)\nfor gt in glob.glob(""../tmp_download_files/MSRA10K_Imgs_GT/Imgs/*""):\n    copyfile(gt, ""../data/MSRA_10K/{}"".format(os.path.basename(gt)))\n\n# Deleted temp files\nos.system(""rm -r ../tmp_download_files"")'"
segmentation-refinement/setup.py,0,"b'import setuptools\n\nwith open(""README.md"", ""r"") as fh:\n    long_description = fh.read()\n\nsetuptools.setup(\n    name=""segmentation-refinement"",\n    version=""0.2"",\n    author=""Rex Cheng"",\n    author_email=""hkchengrex@gmail.com"",\n    description=""Deep learning based segmentation refinement system."",\n    long_description=long_description,\n    long_description_content_type=""text/markdown"",\n    url=""https://github.com/hkchengrex/CascadePSP"",\n    packages=setuptools.find_packages(),\n    classifiers=[\n        ""Programming Language :: Python :: 3"",\n        ""License :: OSI Approved :: MIT License"",\n        ""Operating System :: OS Independent"",\n    ],\n    python_requires=\'>=3.6\',\n    install_requires=[\'torch\', \'torchvision\'],\n)'"
segmentation-refinement/test.py,0,"b""import cv2\nimport time\nimport matplotlib.pyplot as plt\nimport segmentation_refinement as refine\nimage = cv2.imread('test/aeroplane.jpg')\nmask = cv2.imread('test/aeroplane.png', cv2.IMREAD_GRAYSCALE)\n\n# model_path can also be specified here\n# This step takes some time to load the model\nrefiner = refine.Refiner(device='cuda:0') # device can also be 'cpu'\n\n# Fast - Global step only.\n# Smaller L -> Less memory usage; faster in fast mode.\noutput = refiner.refine(image, mask, fast=False, L=900) \n\nplt.imshow(output)\nplt.show()\n"""
util/__init__.py,0,b''
util/boundary_modification.py,0,"b""import cv2\nimport numpy as np\nimport random\nimport math\n\ntry:\n    from util.de_transform import perturb_seg\nexcept:\n    from de_transform import perturb_seg\n\n\ndef modify_boundary(image, regional_sample_rate=0.1, sample_rate=0.1, move_rate=0.0, iou_target = 0.8):\n    # modifies boundary of the given mask.\n    # remove consecutive vertice of the boundary by regional sample rate\n    # ->\n    # remove any vertice by sample rate\n    # ->\n    # move vertice by distance between vertice and center of the mask by move rate. \n    # input: np array of size [H,W] image\n    # output: same shape as input\n    \n    #get boundaries\n    _, contours, _ = cv2.findContours(image, cv2.RETR_LIST, cv2.CHAIN_APPROX_NONE)\n\n    #only modified contours is needed actually. \n    sampled_contours = []   \n    modified_contours = [] \n\n    for contour in contours:\n        if contour.shape[0] < 10:\n            continue\n        M = cv2.moments(contour)\n\n        #remove region of contour\n        number_of_vertices = contour.shape[0]\n        number_of_removes = int(number_of_vertices * regional_sample_rate)\n        \n        idx_dist = []\n        for i in range(number_of_vertices-number_of_removes):\n            idx_dist.append([i, np.sum((contour[i] - contour[i+number_of_removes])**2)])\n            \n        idx_dist = sorted(idx_dist, key=lambda x:x[1])\n        \n        remove_start = random.choice(idx_dist[:math.ceil(0.1*len(idx_dist))])[0]\n        \n       #remove_start = random.randrange(0, number_of_vertices-number_of_removes, 1)\n        new_contour = np.concatenate([contour[:remove_start], contour[remove_start+number_of_removes:]], axis=0)\n        contour = new_contour\n        \n\n        #sample contours\n        number_of_vertices = contour.shape[0]\n        indices = random.sample(range(number_of_vertices), int(number_of_vertices * sample_rate))\n        indices.sort()\n        sampled_contour = contour[indices]\n        sampled_contours.append(sampled_contour)\n\n        modified_contour = np.copy(sampled_contour)\n        if (M['m00'] != 0):\n            center = round(M['m10'] / M['m00']), round(M['m01'] / M['m00'])\n\n            #modify contours\n            for idx, coor in enumerate(modified_contour):\n\n                change = np.random.normal(0,move_rate) # 0.1 means change position of vertex to 10 percent farther from center\n                x,y = coor[0]\n                new_x = x + (x-center[0]) * change\n                new_y = y + (y-center[1]) * change\n\n                modified_contour[idx] = [new_x,new_y]\n        modified_contours.append(modified_contour)\n        \n\n    #draw boundary\n    gt = np.copy(image)\n    image = np.zeros_like(image)\n\n    modified_contours = [cont for cont in modified_contours if len(cont) > 0]\n    if len(modified_contours) == 0:\n        image = gt.copy()\n    else:\n        image = cv2.drawContours(image, modified_contours, -1, (255, 0, 0), -1)\n\n    image = perturb_seg(image, iou_target)\n    \n    return image\n\n"""
util/compute_boundary_acc.py,0,"b'import numpy as np\nimport cv2\n\ndef get_disk_kernel(radius):\n    return cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (radius*2+1, radius*2+1))\n\ndef compute_boundary_acc(gt, seg, mask):\n\n    gt = gt.astype(np.uint8)\n    seg = seg.astype(np.uint8)\n    mask = mask.astype(np.uint8)\n\n    h, w = gt.shape\n\n    min_radius = 1\n    max_radius = (w+h)/300\n    num_steps = 5\n\n    seg_acc = [None] * num_steps\n    mask_acc = [None] * num_steps\n\n    for i in range(num_steps):\n        curr_radius = min_radius + int((max_radius-min_radius)/num_steps*i)\n\n        kernel = get_disk_kernel(curr_radius)\n        boundary_region = cv2.morphologyEx(gt, cv2.MORPH_GRADIENT, kernel) > 0\n\n        gt_in_bound = gt[boundary_region]\n        seg_in_bound = seg[boundary_region]\n        mask_in_bound = mask[boundary_region]\n\n        num_edge_pixels = (boundary_region).sum()\n        num_seg_gd_pix = ((gt_in_bound) * (seg_in_bound) + (1-gt_in_bound) * (1-seg_in_bound)).sum()\n        num_mask_gd_pix = ((gt_in_bound) * (mask_in_bound) + (1-gt_in_bound) * (1-mask_in_bound)).sum()\n\n        seg_acc[i] = num_seg_gd_pix / num_edge_pixels\n        mask_acc[i] = num_mask_gd_pix / num_edge_pixels\n\n    return sum(seg_acc)/num_steps, sum(mask_acc)/num_steps\n\ndef compute_boundary_acc_multi_class(gt, seg, mask):\n    h, w = gt.shape\n\n    min_radius = 1\n    max_radius = (w+h)/300\n    num_steps = 5\n\n    seg_acc = [None] * num_steps\n    mask_acc = [None] * num_steps\n\n    classes = np.unique(gt)\n\n    for i in range(num_steps):\n        curr_radius = min_radius + int((max_radius-min_radius)/num_steps*i)\n\n        kernel = get_disk_kernel(curr_radius)\n\n        boundary_region = np.zeros_like(gt)\n        for c in classes:\n            # Skip void\n            if c == 0:\n                continue\n\n            gt_class = (gt == c).astype(np.uint8)\n            class_bound = cv2.morphologyEx(gt_class, cv2.MORPH_GRADIENT, kernel)\n            boundary_region += class_bound\n\n        boundary_region = boundary_region > 0\n\n        gt_in_bound = gt[boundary_region]\n        seg_in_bound = seg[boundary_region]\n        mask_in_bound = mask[boundary_region]\n\n        void_count = (gt_in_bound == 0).sum()\n\n        num_edge_pixels = (boundary_region).sum()\n        num_seg_gd_pix = (gt_in_bound == seg_in_bound).sum()\n        num_mask_gd_pix = (gt_in_bound == mask_in_bound).sum()\n\n        seg_acc[i] = (num_seg_gd_pix-void_count) / (num_edge_pixels-void_count)\n        mask_acc[i] = (num_mask_gd_pix-void_count) / (num_edge_pixels-void_count)\n\n    return sum(seg_acc)/num_steps, sum(mask_acc)/num_steps'"
util/de_transform.py,0,"b""import cv2\n\nimport numpy as np\n\ndef get_random_structure(size):\n    choice = np.random.randint(4)\n\n    if choice == 1:\n        return cv2.getStructuringElement(cv2.MORPH_RECT, (size, size))\n    elif choice == 2:\n        return cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (size, size))\n    elif choice == 3:\n        return cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (size, size//2))\n    elif choice == 4:\n        return cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (size//2, size))\n\ndef random_dilate(seg, min=3, max=10):\n    size = np.random.randint(min, max)\n    kernel = get_random_structure(size)\n    seg = cv2.dilate(seg,kernel,iterations = 1)\n    return seg\n\ndef random_erode(seg, min=3, max=10):\n    size = np.random.randint(min, max)\n    kernel = get_random_structure(size)\n    seg = cv2.erode(seg,kernel,iterations = 1)\n    return seg\n\ndef compute_iou(seg, gt):\n    intersection = seg*gt\n    union = seg+gt\n    return (np.count_nonzero(intersection) + 1e-6) / (np.count_nonzero(union) + 1e-6)\n\ndef perturb_seg(gt, iou_target=0.6):\n    h, w = gt.shape\n    seg = gt.copy()\n\n    _, seg = cv2.threshold(seg, 127, 255, 0)\n\n    # Rare case\n    if h <= 2 or w <= 2:\n        print('GT too small, returning original')\n        return seg\n\n    # Do a bunch of random operations\n    for _ in range(250):\n        for _ in range(4):\n            lx, ly = np.random.randint(w), np.random.randint(h)\n            lw, lh = np.random.randint(lx+1,w+1), np.random.randint(ly+1,h+1)\n\n            # Randomly set one pixel to 1/0. With the following dilate/erode, we can create holes/external regions\n            if np.random.rand() < 0.25:\n                cx = int((lx + lw) / 2)\n                cy = int((ly + lh) / 2)\n                seg[cy, cx] = np.random.randint(2) * 255\n\n            if np.random.rand() < 0.5:\n                seg[ly:lh, lx:lw] = random_dilate(seg[ly:lh, lx:lw])\n            else:\n                seg[ly:lh, lx:lw] = random_erode(seg[ly:lh, lx:lw])\n\n        if compute_iou(seg, gt) < iou_target:\n            break\n\n    return seg\n"""
util/file_buffer.py,0,"b""class FileBuffer:\n\n   def __init__(self, file):\n       self.file = open(file, 'w')\n\n   def write(self, *argv):\n       print(*argv, file=self.file)\n       print(*argv)\n"""
util/hyper_para.py,0,"b""from argparse import ArgumentParser\n\nclass HyperParameters():\n    def parse(self, unknown_arg_ok=False):\n        parser = ArgumentParser()\n\n        # Generic learning parameters\n        parser.add_argument('-i', '--iterations', help='Number of training iterations', default=4.5e4, type=int)\n        parser.add_argument('-b', '--batch_size', help='Batch size', default=12, type=int)\n        parser.add_argument('--lr', help='Initial learning rate', default=2.25e-4, type=float)\n        parser.add_argument('--steps', help='Iteration at which learning rate is decayed by gamma', default=[22500, 37500], type=int, nargs='*')\n        parser.add_argument('--gamma', help='Gamma used in learning rate decay', default=0.1, type=float)\n        parser.add_argument('--weight_decay', help='Weight decay', default=1e-4, type=float)\n\n        # same decay applied to discriminator\n        parser.add_argument('--load', help='Path to pretrained model if available')\n\n        parser.add_argument('--ce_weight', help='Weight of CE loss function for each iteration',\n            nargs=6, default=[0.0, 1.0, 0.5, 1.0, 1.0, 0.5], type=float)\n        parser.add_argument('--l1_weight', help='Weight of L1 loss function for each iteration',\n            nargs=6, default=[1.0, 0.0, 0.25, 0.0, 0.0, 0.25], type=float)\n        parser.add_argument('--l2_weight', help='Weight of L2 loss function for each iteration',\n            nargs=6, default=[1.0, 0.0, 0.25, 0.0, 0.0, 0.25], type=float)\n        parser.add_argument('--grad_weight', help='Weight of the gradient loss', default=5, type=float)\n\n        # Logging information, this one is positional and mandatory\n        parser.add_argument('id', help='Experiment UNIQUE id, use NULL to disable logging to tensorboard')\n\n        if unknown_arg_ok:\n            args, _ = parser.parse_known_args()\n            self.args = vars(args)\n        else:\n            self.args = vars(parser.parse_args())\n\n    def __getitem__(self, key):\n        return self.args[key]\n\n    def __str__(self):\n        return str(self.args)\n\n"""
util/image_saver.py,0,"b'import cv2\nimport numpy as np\n\nimport torchvision.transforms as transforms\n\ninv_im_trans = transforms.Normalize(\n                mean=[-0.485/0.229, -0.456/0.224, -0.406/0.225],\n                std=[1/0.229, 1/0.224, 1/0.225])\n\ninv_seg_trans = transforms.Normalize(\n    mean=[-0.5/0.5],\n    std=[1/0.5])\n\ndef tensor_to_numpy(image):\n    image_np = (image.numpy() * 255).astype(\'uint8\')\n    return image_np\n\ndef tensor_to_np_float(image):\n    image_np = image.numpy().astype(\'float32\')\n    return image_np\n\ndef detach_to_cpu(x):\n    return x.detach().cpu()\n\ndef transpose_np(x):\n    return np.transpose(x, [1,2,0])\n\ndef tensor_to_gray_im(x):\n    x = detach_to_cpu(x)\n    x = tensor_to_numpy(x)\n    x = transpose_np(x)\n    return x\n\ndef tensor_to_seg(x):\n    x = detach_to_cpu(x)\n    x = inv_seg_trans(x)\n    x = tensor_to_numpy(x)\n    x = transpose_np(x)\n    return x\n\ndef tensor_to_im(x):\n    x = detach_to_cpu(x)\n    x = inv_im_trans(x)\n    x = tensor_to_numpy(x)\n    x = transpose_np(x)\n    return x\n\n# Predefined key <-> caption dict\nkey_captions = {\n    \'im\': \'Image\', \n    \'gt\': \'GT\', \n    \'seg\': \'Input\', \n    \'error_map\': \'Error map\',\n}\nfor k in [\'28\', \'56\', \'224\']:\n    key_captions[\'pred_\' + k] = \'Ours-%sx%s\' % (k, k)\n    key_captions[\'pred_\' + k + \'_overlay\'] = \'%sx%s\' % (k, k)\n\n""""""\nReturn an image array with captions\nkeys in dictionary will be used as caption if not provided\nvalues should contain lists of cv2 images\n""""""\ndef get_image_array(images, grid_shape, captions={}):\n    w, h = grid_shape\n    cate_counts = len(images)\n    rows_counts = len(next(iter(images.values())))\n\n    font = cv2.FONT_HERSHEY_SIMPLEX\n\n    output_image = np.zeros([h*(rows_counts+1), w*cate_counts, 3], dtype=np.uint8)\n    col_cnt = 0\n    for k, v in images.items():\n\n        # Default as key value itself\n        caption = captions.get(k, k)\n\n        # Handles new line character\n        y0, dy = h-10-len(caption.split(\'\\n\'))*40, 40\n        for i, line in enumerate(caption.split(\'\\n\')):\n            y = y0 + i*dy\n            cv2.putText(output_image, line, (col_cnt*w, y),\n                     font, 0.8, (255,255,255), 2, cv2.LINE_AA)\n\n        # Put images\n        for row_cnt, img in enumerate(v):\n            im_shape = img.shape\n            if len(im_shape) == 2:\n                img = img[..., np.newaxis]\n\n            img = (img * 255).astype(\'uint8\')\n\n            output_image[(row_cnt+1)*h:(row_cnt+2)*h,\n                         col_cnt*w:(col_cnt+1)*w, :] = img\n            \n        col_cnt += 1\n\n    return output_image\n\n""""""\nCreate an image array, transform each image separately as needed\nWill only put images in req_keys\n""""""\ndef pool_images(images, req_keys, row_cnt=10):\n    req_images = {}\n\n    def base_transform(im):\n        im = tensor_to_np_float(im)\n        im = im.transpose((1, 2, 0))\n\n        # Resize\n        if im.shape[1] != 224:\n            im = cv2.resize(im, (224, 224), interpolation=cv2.INTER_NEAREST)\n\n        if len(im.shape) == 2:\n            im = im[..., np.newaxis]\n\n        return im\n\n    second_pass_keys = []\n    for k in req_keys:\n\n        if \'overlay\' in k: \n            # Run overlay in the second pass, skip for now\n            second_pass_keys.append(k)\n\n            # Make sure the base key information is transformed\n            base_key = k.replace(\'_overlay\', \'\')\n            if base_key in req_keys:\n                continue\n            else:\n                k = base_key\n\n        req_images[k] = []\n\n        images[k] = detach_to_cpu(images[k])\n        for i in range(min(row_cnt, len(images[k]))):\n\n            im = images[k][i]\n\n            # Handles inverse transform\n            if k in [\'im\']:\n                im = inv_im_trans(images[k][i])\n            elif k in [\'seg\']:\n                im = inv_seg_trans(images[k][i])\n\n            # Now we are all numpy array\n            im = base_transform(im)\n\n            req_images[k].append(im)\n\n    # Handle overlay images in the second pass\n    for k in second_pass_keys:\n        req_images[k] = []\n        base_key = k.replace(\'_overlay\', \'\')\n        for i in range(min(row_cnt, len(images[base_key]))):\n\n            # If overlay\n            im = req_images[base_key][i]\n            raw = req_images[\'im\'][i]\n\n            im = im.clip(0, 1)\n\n            # Just red overlay\n            im = (raw*0.5 + 0.5 * (raw * (1-im) \n                    + im * (np.array([1,0,0],dtype=np.float32)\n                    .reshape([1,1,3]))))\n            \n            req_images[k].append(im)\n    \n    # Remove all temp items\n    output_images = {}\n    for k in req_keys:\n        output_images[k] = req_images[k]\n\n    return get_image_array(output_images, (224, 224), key_captions)\n\n# Return cv2 image, directly usable for saving\ndef vis_prediction(images):\n\n    keys = [\'im\', \'seg\', \'gt\', \'pred_28\', \'pred_28_2\', \'pred_56\', \'pred_28_3\', \'pred_56_2\', \'pred_224\', \'pred_224_overlay\']\n\n    return pool_images(images, keys)\n'"
util/log_integrator.py,0,"b'""""""\nIntegrate numerical values for some iterations\nTypically used for loss computation\nJust call finalize and create a new Integrator when you want to display \n""""""\nclass Integrator:\n    def __init__(self, logger):\n        self.values = {}\n        self.counts = {}\n        self.hooks  = [] # List is used here to maintain insertion order\n\n        self.logger = logger\n\n    def add_tensor(self, key, tensor):\n        if key not in self.values:\n            self.counts[key] = 1\n            if type(tensor) == float or type(tensor) == int:\n                self.values[key] = tensor\n            else:\n                self.values[key] = tensor.mean().item()\n        else:\n            self.counts[key] += 1\n            if type(tensor) == float or type(tensor) == int:\n                self.values[key] += tensor\n            else:\n                self.values[key] += tensor.mean().item()\n\n    def add_dict(self, tensor_dict):\n        for k, v in tensor_dict.items():\n            self.add_tensor(k, v)\n\n    def add_hook(self, hook):\n        """"""\n        Adds a custom hook, i.e. compute new metrics using values in the dict\n        The hook takes the dict as argument, and returns a (k, v) tuple\n        """"""\n        if type(hook) == list:\n            self.hooks.extend(hook)\n        else:\n            self.hooks.append(hook)\n\n    def reset_except_hooks(self):\n        self.values = {}\n        self.counts = {}\n\n    # Average and output the metrics\n    def finalize(self, prefix, iter, f=None):\n\n        for hook in self.hooks:\n            k, v = hook(self.values)\n            self.add_tensor(k, v)\n\n        for k, v in self.values.items():\n            avg = v / self.counts[k]\n\n            self.logger.log_metrics(prefix, k, avg, iter, f)\n\n'"
util/logger.py,1,"b'import torchvision.transforms as transforms\n\nimport os\n\nfrom torch.utils.tensorboard import SummaryWriter\n# import git\nimport warnings\n\ndef tensor_to_numpy(image):\n    image_np = (image.numpy() * 255).astype(\'uint8\')\n    return image_np\n\ndef detach_to_cpu(x):\n    return x.detach().cpu()\n\ndef fix_width_trunc(x):\n    return (\'{:.9s}\'.format(\'{:0.9f}\'.format(x)))\n\nclass BoardLogger:\n    def __init__(self, id):\n\n        if id is None:\n            self.no_log = True\n            warnings.warn(\'Logging has been disbaled.\')\n        else:\n            self.no_log = False\n\n            self.inv_im_trans = transforms.Normalize(\n                mean=[-0.485/0.229, -0.456/0.224, -0.406/0.225],\n                std=[1/0.229, 1/0.224, 1/0.225])\n\n            self.inv_seg_trans = transforms.Normalize(\n                mean=[-0.5/0.5],\n                std=[1/0.5])\n\n            log_path = os.path.join(\'.\', \'log\', \'%s\' % id)\n            self.logger = SummaryWriter(log_path)\n\n        # repo = git.Repo(""."")\n        # self.log_string(\'git\', str(repo.active_branch) + \' \' + str(repo.head.commit.hexsha))\n\n    def log_scalar(self, tag, x, step):\n        if self.no_log:\n            warnings.warn(\'Logging has been disabled.\')\n            return\n        self.logger.add_scalar(tag, x, step)\n\n    def log_metrics(self, l1_tag, l2_tag, val, step, f=None):\n        tag = l1_tag + \'/\' + l2_tag\n        text = \'It {:8d} [{:5s}] [{:19s}]: {:s}\'.format(step, l1_tag.upper(), l2_tag, fix_width_trunc(val))\n        print(text)\n        if f is not None:\n            f.write(text + \'\\n\')\n            f.flush()\n        self.log_scalar(tag, val, step)\n\n    def log_im(self, tag, x, step):\n        if self.no_log:\n            warnings.warn(\'Logging has been disabled.\')\n            return\n        x = detach_to_cpu(x)\n        x = self.inv_im_trans(x)\n        x = tensor_to_numpy(x)\n        self.logger.add_image(tag, x, step)\n\n    def log_cv2(self, tag, x, step):\n        if self.no_log:\n            warnings.warn(\'Logging has been disabled.\')\n            return\n        x = x.transpose((2, 0, 1))\n        self.logger.add_image(tag, x, step)\n\n    def log_seg(self, tag, x, step):\n        if self.no_log:\n            warnings.warn(\'Logging has been disabled.\')\n            return\n        x = detach_to_cpu(x)\n        x = self.inv_seg_trans(x)\n        x = tensor_to_numpy(x)\n        self.logger.add_image(tag, x, step)\n\n    def log_gray(self, tag, x, step):\n        if self.no_log:\n            warnings.warn(\'Logging has been disabled.\')\n            return\n        x = detach_to_cpu(x)\n        x = tensor_to_numpy(x)\n        self.logger.add_image(tag, x, step)\n\n    def log_string(self, tag, x):\n        print(tag, x)\n        if self.no_log:\n            warnings.warn(\'Logging has been disabled.\')\n            return\n        self.logger.add_text(tag, x)\n\n    def log_total(self, tag, im, gt, seg, pred, step):\n        \n        if self.no_log:\n            warnings.warn(\'Logging has been disabled.\')\n            return\n        \n        row_cnt = min(10, im.shape[0])\n        w = im.shape[2]\n        h = im.shape[3]\n        \n        output_image = np.zeros([3, w*row_cnt, h*5], dtype=np.uint8)\n        \n        for i in range(row_cnt):\n            im_ = tensor_to_numpy(self.inv_im_trans(detach_to_cpu(im[i])))\n            gt_ = tensor_to_numpy(detach_to_cpu(gt[i]))\n            seg_ = tensor_to_numpy(self.inv_seg_trans(detach_to_cpu(seg[i])))\n            pred_ = tensor_to_numpy(detach_to_cpu(pred[i]))\n            \n            output_image[:, i * w : (i+1) * w, 0 : h] = im_\n            output_image[:, i * w : (i+1) * w, h : 2*h] = gt_\n            output_image[:, i * w : (i+1) * w, 2*h : 3*h] = seg_\n            output_image[:, i * w : (i+1) * w, 3*h : 4*h] = pred_\n            output_image[:, i * w : (i+1) * w, 4*h : 5*h] = im_*0.5 + 0.5 * (im_ * (1-(pred_/255)) + (pred_/255) * (np.array([255,0,0],dtype=np.uint8).reshape([1,3,1,1])))\n            \n        self.logger.add_image(tag, output_image, step)\n'"
util/metrics_compute.py,1,"b'import torch.nn.functional as F\n\nfrom util.util import compute_tensor_iu\n\ndef get_new_iou_hook(values, size):\n    return \'iou/new_iou_%s\'%size, values[\'iou/new_i_%s\'%size]/values[\'iou/new_u_%s\'%size]\n\ndef get_orig_iou_hook(values):\n    return \'iou/orig_iou\', values[\'iou/orig_i\']/values[\'iou/orig_u\']\n\ndef get_iou_gain(values, size):\n    return \'iou/iou_gain_%s\'%size, values[\'iou/new_iou_%s\'%size] - values[\'iou/orig_iou\']\n\niou_hooks_to_be_used = [\n        get_orig_iou_hook,\n        lambda x: get_new_iou_hook(x, \'224\'), lambda x: get_iou_gain(x, \'224\'),\n        lambda x: get_new_iou_hook(x, \'56\'), lambda x: get_iou_gain(x, \'56\'),\n        lambda x: get_new_iou_hook(x, \'28\'), lambda x: get_iou_gain(x, \'28\'),\n        lambda x: get_new_iou_hook(x, \'28_2\'), lambda x: get_iou_gain(x, \'28_2\'),\n        lambda x: get_new_iou_hook(x, \'28_3\'), lambda x: get_iou_gain(x, \'28_3\'),\n        lambda x: get_new_iou_hook(x, \'56_2\'), lambda x: get_iou_gain(x, \'56_2\'),\n    ]\n\niou_hooks_final_only = [\n    get_orig_iou_hook,\n    lambda x: get_new_iou_hook(x, \'224\'), lambda x: get_iou_gain(x, \'224\'),\n]\n\n# Compute common loss and metric for generator only\ndef compute_loss_and_metrics(images, para, detailed=True, need_loss=True, has_lower_res=True):\n\n    """"""\n    This part compute loss and metrics for the generator\n    """"""\n\n    loss_and_metrics = {}\n\n    gt = images[\'gt\']\n    seg = images[\'seg\']\n\n    pred_224 = images[\'pred_224\']\n    if has_lower_res:\n        pred_28 = images[\'pred_28\']\n        pred_56 = images[\'pred_56\']\n        pred_28_2 = images[\'pred_28_2\']\n        pred_28_3 = images[\'pred_28_3\']\n        pred_56_2 = images[\'pred_56_2\']\n\n    if need_loss:\n        # Loss weights\n        ce_weights = para[\'ce_weight\']\n        l1_weights = para[\'l1_weight\']\n        l2_weights = para[\'l2_weight\']\n\n        # temp holder for losses at different scale\n        ce_loss = [0] * 6\n        l1_loss = [0] * 6\n        l2_loss = [0] * 6\n        loss = [0] * 6\n\n        ce_loss[0] = F.binary_cross_entropy_with_logits(images[\'out_224\'], (gt>0.5).float())\n        if has_lower_res:\n            ce_loss[1] = F.binary_cross_entropy_with_logits(images[\'out_28\'], (gt>0.5).float())\n            ce_loss[2] = F.binary_cross_entropy_with_logits(images[\'out_56\'], (gt>0.5).float())\n            ce_loss[3] = F.binary_cross_entropy_with_logits(images[\'out_28_2\'], (gt>0.5).float())\n            ce_loss[4] = F.binary_cross_entropy_with_logits(images[\'out_28_3\'], (gt>0.5).float())\n            ce_loss[5] = F.binary_cross_entropy_with_logits(images[\'out_56_2\'], (gt>0.5).float())\n\n        l1_loss[0] = F.l1_loss(pred_224, gt)\n        if has_lower_res:\n            l2_loss[0] = F.mse_loss(pred_224, gt)\n            l1_loss[1] = F.l1_loss(pred_28, gt)\n            l2_loss[1] = F.mse_loss(pred_28, gt)\n            l1_loss[2] = F.l1_loss(pred_56, gt)\n            l2_loss[2] = F.mse_loss(pred_56, gt)\n\n        if has_lower_res:\n            l1_loss[3] = F.l1_loss(pred_28_2, gt)\n            l2_loss[3] = F.mse_loss(pred_28_2, gt)\n            l1_loss[4] = F.l1_loss(pred_28_3, gt)\n            l2_loss[4] = F.mse_loss(pred_28_3, gt)\n            l1_loss[5] = F.l1_loss(pred_56_2, gt)\n            l2_loss[5] = F.mse_loss(pred_56_2, gt)\n\n        loss_and_metrics[\'grad_loss\'] = F.l1_loss(images[\'gt_sobel\'], images[\'pred_sobel\'])\n\n        # Weighted loss for different levels\n        for i in range(6):\n            loss[i] = ce_loss[i] * ce_weights[i] + \\\n                    l1_loss[i] * l1_weights[i] + \\\n                    l2_loss[i] * l2_weights[i]\n        \n        loss[0] += loss_and_metrics[\'grad_loss\'] * para[\'grad_weight\']\n\n    """"""\n    Compute IOU stats\n    """"""\n    orig_total_i, orig_total_u = compute_tensor_iu(seg>0.5, gt>0.5)\n    loss_and_metrics[\'iou/orig_i\'] = orig_total_i\n    loss_and_metrics[\'iou/orig_u\'] = orig_total_u\n\n    new_total_i, new_total_u = compute_tensor_iu(pred_224>0.5, gt>0.5)\n    loss_and_metrics[\'iou/new_i_224\'] = new_total_i\n    loss_and_metrics[\'iou/new_u_224\'] = new_total_u\n\n    if has_lower_res:\n        new_total_i, new_total_u = compute_tensor_iu(pred_56>0.5, gt>0.5)\n        loss_and_metrics[\'iou/new_i_56\'] = new_total_i\n        loss_and_metrics[\'iou/new_u_56\'] = new_total_u\n\n        new_total_i, new_total_u = compute_tensor_iu(pred_28>0.5, gt>0.5)\n        loss_and_metrics[\'iou/new_i_28\'] = new_total_i\n        loss_and_metrics[\'iou/new_u_28\'] = new_total_u\n\n        new_total_i, new_total_u = compute_tensor_iu(pred_28_2>0.5, gt>0.5)\n        loss_and_metrics[\'iou/new_i_28_2\'] = new_total_i\n        loss_and_metrics[\'iou/new_u_28_2\'] = new_total_u\n\n        new_total_i, new_total_u = compute_tensor_iu(pred_28_3>0.5, gt>0.5)\n        loss_and_metrics[\'iou/new_i_28_3\'] = new_total_i\n        loss_and_metrics[\'iou/new_u_28_3\'] = new_total_u\n\n        new_total_i, new_total_u = compute_tensor_iu(pred_56_2>0.5, gt>0.5)\n        loss_and_metrics[\'iou/new_i_56_2\'] = new_total_i\n        loss_and_metrics[\'iou/new_u_56_2\'] = new_total_u\n        \n    """"""\n    All done.\n    Now gather everything in a dict for logging\n    """"""\n\n    if need_loss:\n        loss_and_metrics[\'total_loss\'] = 0\n        for i in range(6):\n            loss_and_metrics[\'ce_loss/s_%d\'%i] = ce_loss[i]\n            loss_and_metrics[\'l1_loss/s_%d\'%i] = l1_loss[i]\n            loss_and_metrics[\'l2_loss/s_%d\'%i] = l2_loss[i]\n            loss_and_metrics[\'loss/s_%d\'%i] = loss[i]\n\n            loss_and_metrics[\'total_loss\'] += loss[i]\n\n    return loss_and_metrics\n\n'"
util/model_saver.py,1,"b""import os\nimport torch\n\nclass ModelSaver:\n    def __init__(self, id):\n\n        if id is None:\n            self.no_log = True\n            print('Saving has been disbaled.')\n        else:\n            self.no_log = False\n\n            self.save_path = os.path.join('.', 'weights', '%s' % id )\n\n    def save_model(self, model, step):\n        if self.no_log:\n            print('Saving has been disabled.')\n            return\n\n        os.makedirs(self.save_path, exist_ok=True)\n\n        model_path = os.path.join(self.save_path, 'model_%s' % step)\n        torch.save(model.state_dict(), model_path)\n        print('Model saved to %s.' % model_path)\n"""
util/util.py,1,"b""from torch.nn import functional as F\n\ndef compute_tensor_iu(seg, gt):\n    seg = seg.squeeze(1)\n    gt = gt.squeeze(1)\n    \n    intersection = (seg & gt).float().sum()\n    union = (seg | gt).float().sum()\n\n    return intersection, union\n\ndef compute_tensor_iou(seg, gt):\n    seg = seg.squeeze(1)\n    gt = gt.squeeze(1)\n    \n    intersection = (seg & gt).float().sum((1, 2))\n    union = (seg | gt).float().sum((1, 2))\n    \n    iou = (intersection + 1e-6) / (union + 1e-6)\n    \n    return iou \n\ndef resize_min_side(im, size, method):\n    h, w = im.shape[-2:]\n    min_side = min(h, w)\n    ratio = size / min_side\n    if method == 'bilinear':\n        return F.interpolate(im, scale_factor=ratio, mode=method, align_corners=False)\n    else:\n        return F.interpolate(im, scale_factor=ratio, mode=method)\n\ndef resize_max_side(im, size, method):\n    h, w = im.shape[-2:]\n    max_side = max(h, w)\n    ratio = size / max_side\n    if method in ['bilinear', 'bicubic']:\n        return F.interpolate(im, scale_factor=ratio, mode=method, align_corners=False)\n    else:\n        return F.interpolate(im, scale_factor=ratio, mode=method)\n"""
models/psp/__init__.py,0,b''
models/psp/extractors.py,10,"b""from collections import OrderedDict\nimport math\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils import model_zoo\nfrom torchvision.models.densenet import densenet121, densenet161\nfrom torchvision.models.squeezenet import squeezenet1_1\n\nfrom models.sync_batchnorm import SynchronizedBatchNorm2d\n\ndef load_weights_sequential(target, source_state):\n    \n    new_dict = OrderedDict()\n    # for (k1, v1), (k2, v2) in zip(target.state_dict().items(), source_state.items()):\n    #     print(k1, v1.shape, k2, v2.shape)\n    #     new_dict[k1] = v2\n\n    for k1, v1 in target.state_dict().items():\n        if not 'num_batches_tracked' in k1:\n            tar_v = source_state[k1]\n\n            if v1.shape != tar_v.shape:\n                # Init the new segmentation channel with zeros\n                # print(v1.shape, tar_v.shape)\n                c, _, w, h = v1.shape\n                tar_v = torch.cat([\n                    tar_v, \n                    torch.zeros((c,3,w,h)),\n                ], 1)\n\n            new_dict[k1] = tar_v\n\n    target.load_state_dict(new_dict)\n\n'''\n    Implementation of dilated ResNet-101 with deep supervision. Downsampling is changed to 8x\n'''\nmodel_urls = {\n    'resnet18': 'https://download.pytorch.org/models/resnet18-5c106cde.pth',\n    'resnet34': 'https://download.pytorch.org/models/resnet34-333f7ec4.pth',\n    'resnet50': 'https://download.pytorch.org/models/resnet50-19c8e357.pth',\n    'resnet101': 'https://download.pytorch.org/models/resnet101-5d3b4d8f.pth',\n    'resnet152': 'https://download.pytorch.org/models/resnet152-b121ed2d.pth',\n}\n\n\ndef conv3x3(in_planes, out_planes, stride=1, dilation=1):\n    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n                     padding=dilation, dilation=dilation, bias=False)\n\n\nclass BasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None, dilation=1):\n        super(BasicBlock, self).__init__()\n        self.conv1 = conv3x3(inplanes, planes, stride=stride, dilation=dilation)\n        self.bn1 = SynchronizedBatchNorm2d(planes)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = conv3x3(planes, planes, stride=1, dilation=dilation)\n        self.bn2 = SynchronizedBatchNorm2d(planes)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\n\nclass Bottleneck(nn.Module):\n    expansion = 4\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None, dilation=1):\n        super(Bottleneck, self).__init__()\n        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n        self.bn1 = SynchronizedBatchNorm2d(planes)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, dilation=dilation,\n                               padding=dilation, bias=False)\n        self.bn2 = SynchronizedBatchNorm2d(planes)\n        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)\n        self.bn3 = SynchronizedBatchNorm2d(planes * 4)\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\n\nclass ResNet(nn.Module):\n    def __init__(self, block, layers=(3, 4, 23, 3)):\n        self.inplanes = 64\n        super(ResNet, self).__init__()\n        self.conv1 = nn.Conv2d(6, 64, kernel_size=7, stride=2, padding=3,\n                               bias=False)\n        self.bn1 = SynchronizedBatchNorm2d(64)\n        self.relu = nn.ReLU(inplace=True)\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n        self.layer1 = self._make_layer(block, 64, layers[0])\n        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n        self.layer3 = self._make_layer(block, 256, layers[2], stride=1, dilation=2)\n        self.layer4 = self._make_layer(block, 512, layers[3], stride=1, dilation=4)\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n                m.weight.data.normal_(0, math.sqrt(2. / n))\n            elif isinstance(m, SynchronizedBatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n\n    def _make_layer(self, block, planes, blocks, stride=1, dilation=1):\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                nn.Conv2d(self.inplanes, planes * block.expansion,\n                          kernel_size=1, stride=stride, bias=False),\n                SynchronizedBatchNorm2d(planes * block.expansion),\n            )\n\n        layers = [block(self.inplanes, planes, stride, downsample)]\n        self.inplanes = planes * block.expansion\n        for i in range(1, blocks):\n            layers.append(block(self.inplanes, planes, dilation=dilation))\n\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        x_1 = self.conv1(x)  # /2\n        x = self.bn1(x_1)\n        x = self.relu(x)\n        x = self.maxpool(x)  # /2\n\n        x_2 = self.layer1(x)\n        x = self.layer2(x_2)   # /2\n        x = self.layer3(x)\n        x = self.layer4(x)\n\n        return x, x_1, x_2\n\n\ndef resnet50(pretrained=True):\n    model = ResNet(Bottleneck, [3, 4, 6, 3])\n    if pretrained:\n        load_weights_sequential(model, model_zoo.load_url(model_urls['resnet50']))\n    return model\n\n"""
models/psp/pspnet.py,16,"b'import torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\nfrom models.psp import extractors\nfrom models.sync_batchnorm import SynchronizedBatchNorm2d\n\n\nclass PSPModule(nn.Module):\n    def __init__(self, features, out_features=1024, sizes=(1, 2, 3, 6)):\n        super().__init__()\n        self.stages = []\n        self.stages = nn.ModuleList([self._make_stage(features, size) for size in sizes])\n        self.bottleneck = nn.Conv2d(features * (len(sizes) + 1), out_features, kernel_size=1)\n        self.relu = nn.ReLU(inplace=True)\n\n    def _make_stage(self, features, size):\n        prior = nn.AdaptiveAvgPool2d(output_size=(size, size))\n        conv = nn.Conv2d(features, features, kernel_size=1, bias=False)\n        return nn.Sequential(prior, conv)\n\n    def forward(self, feats):\n        h, w = feats.size(2), feats.size(3)\n        set_priors = [F.interpolate(input=stage(feats), size=(h, w), mode=\'bilinear\', align_corners=False) for stage in self.stages]\n        priors = set_priors + [feats]\n        bottle = self.bottleneck(torch.cat(priors, 1))\n        return self.relu(bottle)\n\n\nclass PSPUpsample(nn.Module):\n    def __init__(self, x_channels, in_channels, out_channels):\n        super().__init__()\n        self.conv = nn.Sequential(\n            SynchronizedBatchNorm2d(in_channels),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(in_channels, out_channels, 3, padding=1),\n            SynchronizedBatchNorm2d(out_channels),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(out_channels, out_channels, 3, padding=1),\n        )\n\n        self.conv2 = nn.Sequential(\n            SynchronizedBatchNorm2d(out_channels),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(out_channels, out_channels, 3, padding=1),\n            SynchronizedBatchNorm2d(out_channels),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(out_channels, out_channels, 3, padding=1),\n        )\n\n        self.shortcut = nn.Conv2d(x_channels, out_channels, kernel_size=1)\n\n    def forward(self, x, up):\n        x = F.interpolate(input=x, scale_factor=2, mode=\'bilinear\', align_corners=False)\n\n        p = self.conv(torch.cat([x, up], 1))\n        sc = self.shortcut(x)\n\n        p = p + sc\n\n        p2 = self.conv2(p)\n\n        return p + p2\n\n\nclass PSPNet(nn.Module):\n    def __init__(self, sizes=(1, 2, 3, 6), psp_size=2048, deep_features_size=1024, backend=\'resnet34\',\n                 pretrained=True):\n        super().__init__()\n        self.feats = getattr(extractors, backend)(pretrained)\n        self.psp = PSPModule(psp_size, 1024, sizes)\n\n        self.up_1 = PSPUpsample(1024, 1024+256, 512)\n        self.up_2 = PSPUpsample(512, 512+64, 256)\n        self.up_3 = PSPUpsample(256, 256+3, 32)\n        \n        self.final_28 = nn.Sequential(\n            nn.Conv2d(1024, 32, kernel_size=1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(32, 1, kernel_size=1),\n        )\n\n        self.final_56 = nn.Sequential(\n            nn.Conv2d(512, 32, kernel_size=1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(32, 1, kernel_size=1),\n        )\n\n        self.final_11 = nn.Conv2d(32+3, 32, kernel_size=1)\n        self.final_21 = nn.Conv2d(32, 1, kernel_size=1)\n\n    def forward(self, x, seg, inter_s8=None, inter_s4=None):\n\n        images = {}\n\n        """"""\n        First iteration, s8 output\n        """"""\n        if inter_s8 is None:\n            p = torch.cat((x, seg, seg, seg), 1)\n\n            f, f_1, f_2 = self.feats(p) \n            p = self.psp(f)\n\n            inter_s8 = self.final_28(p)\n            r_inter_s8 = F.interpolate(inter_s8, scale_factor=8, mode=\'bilinear\', align_corners=False)\n            r_inter_tanh_s8 = torch.tanh(r_inter_s8)\n\n            images[\'pred_28\'] = torch.sigmoid(r_inter_s8)\n            images[\'out_28\'] = r_inter_s8\n        else:\n            r_inter_tanh_s8 = inter_s8\n\n        """"""\n        Second iteration, s8 output\n        """"""\n        if inter_s4 is None:\n            p = torch.cat((x, seg, r_inter_tanh_s8, r_inter_tanh_s8), 1)\n\n            f, f_1, f_2 = self.feats(p) \n            p = self.psp(f)\n            inter_s8_2 = self.final_28(p)\n            r_inter_s8_2 = F.interpolate(inter_s8_2, scale_factor=8, mode=\'bilinear\', align_corners=False)\n            r_inter_tanh_s8_2 = torch.tanh(r_inter_s8_2)\n\n            p = self.up_1(p, f_2)\n\n            inter_s4 = self.final_56(p)\n            r_inter_s4 = F.interpolate(inter_s4, scale_factor=4, mode=\'bilinear\', align_corners=False)\n            r_inter_tanh_s4 = torch.tanh(r_inter_s4)\n\n            images[\'pred_28_2\'] = torch.sigmoid(r_inter_s8_2)\n            images[\'out_28_2\'] = r_inter_s8_2\n            images[\'pred_56\'] = torch.sigmoid(r_inter_s4)\n            images[\'out_56\'] = r_inter_s4\n        else:\n            r_inter_tanh_s8_2 = inter_s8\n            r_inter_tanh_s4 = inter_s4\n\n        """"""\n        Third iteration, s1 output\n        """"""\n        p = torch.cat((x, seg, r_inter_tanh_s8_2, r_inter_tanh_s4), 1)\n\n        f, f_1, f_2 = self.feats(p) \n        p = self.psp(f)\n        inter_s8_3 = self.final_28(p)\n        r_inter_s8_3 = F.interpolate(inter_s8_3, scale_factor=8, mode=\'bilinear\', align_corners=False)\n\n        p = self.up_1(p, f_2)\n        inter_s4_2 = self.final_56(p)\n        r_inter_s4_2 = F.interpolate(inter_s4_2, scale_factor=4, mode=\'bilinear\', align_corners=False)\n        p = self.up_2(p, f_1)\n        p = self.up_3(p, x)\n\n\n        """"""\n        Final output\n        """"""\n        p = F.relu(self.final_11(torch.cat([p, x], 1)), inplace=True)\n        p = self.final_21(p)\n\n        pred_224 = torch.sigmoid(p)\n\n        images[\'pred_224\'] = pred_224\n        images[\'out_224\'] = p\n        images[\'pred_28_3\'] = torch.sigmoid(r_inter_s8_3)\n        images[\'pred_56_2\'] = torch.sigmoid(r_inter_s4_2)\n        images[\'out_28_3\'] = r_inter_s8_3\n        images[\'out_56_2\'] = r_inter_s4_2\n\n        return images\n'"
models/sync_batchnorm/__init__.py,0,"b'# -*- coding: utf-8 -*-\n# File   : __init__.py\n# Author : Jiayuan Mao\n# Email  : maojiayuan@gmail.com\n# Date   : 27/01/2018\n#\n# This file is part of Synchronized-BatchNorm-PyTorch.\n# https://github.com/vacancy/Synchronized-BatchNorm-PyTorch\n# Distributed under MIT License.\n\nfrom .batchnorm import SynchronizedBatchNorm1d, SynchronizedBatchNorm2d, SynchronizedBatchNorm3d\nfrom .replicate import DataParallelWithCallback, patch_replication_callback'"
models/sync_batchnorm/batchnorm.py,6,"b'# -*- coding: utf-8 -*-\n# File   : batchnorm.py\n# Author : Jiayuan Mao\n# Email  : maojiayuan@gmail.com\n# Date   : 27/01/2018\n#\n# This file is part of Synchronized-BatchNorm-PyTorch.\n# https://github.com/vacancy/Synchronized-BatchNorm-PyTorch\n# Distributed under MIT License.\n\nimport collections\n\nimport torch\nimport torch.nn.functional as F\n\nfrom torch.nn.modules.batchnorm import _BatchNorm\nfrom torch.nn.parallel._functions import ReduceAddCoalesced, Broadcast\n\nfrom .comm import SyncMaster\n\n__all__ = [\'SynchronizedBatchNorm1d\', \'SynchronizedBatchNorm2d\', \'SynchronizedBatchNorm3d\']\n\n\ndef _sum_ft(tensor):\n    """"""sum over the first and last dimention""""""\n    return tensor.sum(dim=0).sum(dim=-1)\n\n\ndef _unsqueeze_ft(tensor):\n    """"""add new dementions at the front and the tail""""""\n    return tensor.unsqueeze(0).unsqueeze(-1)\n\n\n_ChildMessage = collections.namedtuple(\'_ChildMessage\', [\'sum\', \'ssum\', \'sum_size\'])\n_MasterMessage = collections.namedtuple(\'_MasterMessage\', [\'sum\', \'inv_std\'])\n\n\nclass _SynchronizedBatchNorm(_BatchNorm):\n    def __init__(self, num_features, eps=1e-5, momentum=0.1, affine=True):\n        super(_SynchronizedBatchNorm, self).__init__(num_features, eps=eps, momentum=momentum, affine=affine)\n\n        self._sync_master = SyncMaster(self._data_parallel_master)\n\n        self._is_parallel = False\n        self._parallel_id = None\n        self._slave_pipe = None\n\n    def forward(self, input):\n        # If it is not parallel computation or is in evaluation mode, use PyTorch\'s implementation.\n        if not (self._is_parallel and self.training):\n            return F.batch_norm(\n                input, self.running_mean, self.running_var, self.weight, self.bias,\n                self.training, self.momentum, self.eps)\n\n        # Resize the input to (B, C, -1).\n        input_shape = input.size()\n        input = input.view(input.size(0), self.num_features, -1)\n\n        # Compute the sum and square-sum.\n        sum_size = input.size(0) * input.size(2)\n        input_sum = _sum_ft(input)\n        input_ssum = _sum_ft(input ** 2)\n\n        # Reduce-and-broadcast the statistics.\n        if self._parallel_id == 0:\n            mean, inv_std = self._sync_master.run_master(_ChildMessage(input_sum, input_ssum, sum_size))\n        else:\n            mean, inv_std = self._slave_pipe.run_slave(_ChildMessage(input_sum, input_ssum, sum_size))\n\n        # Compute the output.\n        if self.affine:\n            # MJY:: Fuse the multiplication for speed.\n            output = (input - _unsqueeze_ft(mean)) * _unsqueeze_ft(inv_std * self.weight) + _unsqueeze_ft(self.bias)\n        else:\n            output = (input - _unsqueeze_ft(mean)) * _unsqueeze_ft(inv_std)\n\n        # Reshape it.\n        return output.view(input_shape)\n\n    def __data_parallel_replicate__(self, ctx, copy_id):\n        self._is_parallel = True\n        self._parallel_id = copy_id\n\n        # parallel_id == 0 means master device.\n        if self._parallel_id == 0:\n            ctx.sync_master = self._sync_master\n        else:\n            self._slave_pipe = ctx.sync_master.register_slave(copy_id)\n\n    def _data_parallel_master(self, intermediates):\n        """"""Reduce the sum and square-sum, compute the statistics, and broadcast it.""""""\n\n        # Always using same ""device order"" makes the ReduceAdd operation faster.\n        # Thanks to:: Tete Xiao (http://tetexiao.com/)\n        intermediates = sorted(intermediates, key=lambda i: i[1].sum.get_device())\n\n        to_reduce = [i[1][:2] for i in intermediates]\n        to_reduce = [j for i in to_reduce for j in i]  # flatten\n        target_gpus = [i[1].sum.get_device() for i in intermediates]\n\n        sum_size = sum([i[1].sum_size for i in intermediates])\n        sum_, ssum = ReduceAddCoalesced.apply(target_gpus[0], 2, *to_reduce)\n        mean, inv_std = self._compute_mean_std(sum_, ssum, sum_size)\n\n        broadcasted = Broadcast.apply(target_gpus, mean, inv_std)\n\n        outputs = []\n        for i, rec in enumerate(intermediates):\n            outputs.append((rec[0], _MasterMessage(*broadcasted[i * 2:i * 2 + 2])))\n\n        return outputs\n\n    def _compute_mean_std(self, sum_, ssum, size):\n        """"""Compute the mean and standard-deviation with sum and square-sum. This method\n        also maintains the moving average on the master device.""""""\n        assert size > 1, \'BatchNorm computes unbiased standard-deviation, which requires size > 1.\'\n        mean = sum_ / size\n        sumvar = ssum - sum_ * mean\n        unbias_var = sumvar / (size - 1)\n        bias_var = sumvar / size\n\n        self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * mean.data\n        self.running_var = (1 - self.momentum) * self.running_var + self.momentum * unbias_var.data\n\n        return mean, bias_var.clamp(self.eps) ** -0.5\n\n\nclass SynchronizedBatchNorm1d(_SynchronizedBatchNorm):\n    r""""""Applies Synchronized Batch Normalization over a 2d or 3d input that is seen as a\n    mini-batch.\n    .. math::\n        y = \\frac{x - mean[x]}{ \\sqrt{Var[x] + \\epsilon}} * gamma + beta\n    This module differs from the built-in PyTorch BatchNorm1d as the mean and\n    standard-deviation are reduced across all devices during training.\n    For example, when one uses `nn.DataParallel` to wrap the network during\n    training, PyTorch\'s implementation normalize the tensor on each device using\n    the statistics only on that device, which accelerated the computation and\n    is also easy to implement, but the statistics might be inaccurate.\n    Instead, in this synchronized version, the statistics will be computed\n    over all training samples distributed on multiple devices.\n\n    Note that, for one-GPU or CPU-only case, this module behaves exactly same\n    as the built-in PyTorch implementation.\n    The mean and standard-deviation are calculated per-dimension over\n    the mini-batches and gamma and beta are learnable parameter vectors\n    of size C (where C is the input size).\n    During training, this layer keeps a running estimate of its computed mean\n    and variance. The running sum is kept with a default momentum of 0.1.\n    During evaluation, this running mean/variance is used for normalization.\n    Because the BatchNorm is done over the `C` dimension, computing statistics\n    on `(N, L)` slices, it\'s common terminology to call this Temporal BatchNorm\n    Args:\n        num_features: num_features from an expected input of size\n            `batch_size x num_features [x width]`\n        eps: a value added to the denominator for numerical stability.\n            Default: 1e-5\n        momentum: the value used for the running_mean and running_var\n            computation. Default: 0.1\n        affine: a boolean value that when set to ``True``, gives the layer learnable\n            affine parameters. Default: ``True``\n    Shape:\n        - Input: :math:`(N, C)` or :math:`(N, C, L)`\n        - Output: :math:`(N, C)` or :math:`(N, C, L)` (same shape as input)\n    Examples:\n        >>> # With Learnable Parameters\n        >>> m = SynchronizedBatchNorm1d(100)\n        >>> # Without Learnable Parameters\n        >>> m = SynchronizedBatchNorm1d(100, affine=False)\n        >>> input = torch.autograd.Variable(torch.randn(20, 100))\n        >>> output = m(input)\n    """"""\n\n    def _check_input_dim(self, input):\n        if input.dim() != 2 and input.dim() != 3:\n            raise ValueError(\'expected 2D or 3D input (got {}D input)\'\n                             .format(input.dim()))\n        super(SynchronizedBatchNorm1d, self)._check_input_dim(input)\n\n\nclass SynchronizedBatchNorm2d(_SynchronizedBatchNorm):\n    r""""""Applies Batch Normalization over a 4d input that is seen as a mini-batch\n    of 3d inputs\n    .. math::\n        y = \\frac{x - mean[x]}{ \\sqrt{Var[x] + \\epsilon}} * gamma + beta\n    This module differs from the built-in PyTorch BatchNorm2d as the mean and\n    standard-deviation are reduced across all devices during training.\n    For example, when one uses `nn.DataParallel` to wrap the network during\n    training, PyTorch\'s implementation normalize the tensor on each device using\n    the statistics only on that device, which accelerated the computation and\n    is also easy to implement, but the statistics might be inaccurate.\n    Instead, in this synchronized version, the statistics will be computed\n    over all training samples distributed on multiple devices.\n\n    Note that, for one-GPU or CPU-only case, this module behaves exactly same\n    as the built-in PyTorch implementation.\n    The mean and standard-deviation are calculated per-dimension over\n    the mini-batches and gamma and beta are learnable parameter vectors\n    of size C (where C is the input size).\n    During training, this layer keeps a running estimate of its computed mean\n    and variance. The running sum is kept with a default momentum of 0.1.\n    During evaluation, this running mean/variance is used for normalization.\n    Because the BatchNorm is done over the `C` dimension, computing statistics\n    on `(N, H, W)` slices, it\'s common terminology to call this Spatial BatchNorm\n    Args:\n        num_features: num_features from an expected input of\n            size batch_size x num_features x height x width\n        eps: a value added to the denominator for numerical stability.\n            Default: 1e-5\n        momentum: the value used for the running_mean and running_var\n            computation. Default: 0.1\n        affine: a boolean value that when set to ``True``, gives the layer learnable\n            affine parameters. Default: ``True``\n    Shape:\n        - Input: :math:`(N, C, H, W)`\n        - Output: :math:`(N, C, H, W)` (same shape as input)\n    Examples:\n        >>> # With Learnable Parameters\n        >>> m = SynchronizedBatchNorm2d(100)\n        >>> # Without Learnable Parameters\n        >>> m = SynchronizedBatchNorm2d(100, affine=False)\n        >>> input = torch.autograd.Variable(torch.randn(20, 100, 35, 45))\n        >>> output = m(input)\n    """"""\n\n    def _check_input_dim(self, input):\n        if input.dim() != 4:\n            raise ValueError(\'expected 4D input (got {}D input)\'\n                             .format(input.dim()))\n        super(SynchronizedBatchNorm2d, self)._check_input_dim(input)\n\n\nclass SynchronizedBatchNorm3d(_SynchronizedBatchNorm):\n    r""""""Applies Batch Normalization over a 5d input that is seen as a mini-batch\n    of 4d inputs\n    .. math::\n        y = \\frac{x - mean[x]}{ \\sqrt{Var[x] + \\epsilon}} * gamma + beta\n    This module differs from the built-in PyTorch BatchNorm3d as the mean and\n    standard-deviation are reduced across all devices during training.\n    For example, when one uses `nn.DataParallel` to wrap the network during\n    training, PyTorch\'s implementation normalize the tensor on each device using\n    the statistics only on that device, which accelerated the computation and\n    is also easy to implement, but the statistics might be inaccurate.\n    Instead, in this synchronized version, the statistics will be computed\n    over all training samples distributed on multiple devices.\n\n    Note that, for one-GPU or CPU-only case, this module behaves exactly same\n    as the built-in PyTorch implementation.\n    The mean and standard-deviation are calculated per-dimension over\n    the mini-batches and gamma and beta are learnable parameter vectors\n    of size C (where C is the input size).\n    During training, this layer keeps a running estimate of its computed mean\n    and variance. The running sum is kept with a default momentum of 0.1.\n    During evaluation, this running mean/variance is used for normalization.\n    Because the BatchNorm is done over the `C` dimension, computing statistics\n    on `(N, D, H, W)` slices, it\'s common terminology to call this Volumetric BatchNorm\n    or Spatio-temporal BatchNorm\n    Args:\n        num_features: num_features from an expected input of\n            size batch_size x num_features x depth x height x width\n        eps: a value added to the denominator for numerical stability.\n            Default: 1e-5\n        momentum: the value used for the running_mean and running_var\n            computation. Default: 0.1\n        affine: a boolean value that when set to ``True``, gives the layer learnable\n            affine parameters. Default: ``True``\n    Shape:\n        - Input: :math:`(N, C, D, H, W)`\n        - Output: :math:`(N, C, D, H, W)` (same shape as input)\n    Examples:\n        >>> # With Learnable Parameters\n        >>> m = SynchronizedBatchNorm3d(100)\n        >>> # Without Learnable Parameters\n        >>> m = SynchronizedBatchNorm3d(100, affine=False)\n        >>> input = torch.autograd.Variable(torch.randn(20, 100, 35, 45, 10))\n        >>> output = m(input)\n    """"""\n\n    def _check_input_dim(self, input):\n        if input.dim() != 5:\n            raise ValueError(\'expected 5D input (got {}D input)\'\n                             .format(input.dim()))\n        super(SynchronizedBatchNorm3d, self)._check_input_dim(input)'"
models/sync_batchnorm/comm.py,0,"b'# -*- coding: utf-8 -*-\n# File   : comm.py\n# Author : Jiayuan Mao\n# Email  : maojiayuan@gmail.com\n# Date   : 27/01/2018\n#\n# This file is part of Synchronized-BatchNorm-PyTorch.\n# https://github.com/vacancy/Synchronized-BatchNorm-PyTorch\n# Distributed under MIT License.\n\nimport queue\nimport collections\nimport threading\n\n__all__ = [\'FutureResult\', \'SlavePipe\', \'SyncMaster\']\n\n\nclass FutureResult(object):\n    """"""A thread-safe future implementation. Used only as one-to-one pipe.""""""\n\n    def __init__(self):\n        self._result = None\n        self._lock = threading.Lock()\n        self._cond = threading.Condition(self._lock)\n\n    def put(self, result):\n        with self._lock:\n            assert self._result is None, \'Previous result has\\\'t been fetched.\'\n            self._result = result\n            self._cond.notify()\n\n    def get(self):\n        with self._lock:\n            if self._result is None:\n                self._cond.wait()\n\n            res = self._result\n            self._result = None\n            return res\n\n\n_MasterRegistry = collections.namedtuple(\'MasterRegistry\', [\'result\'])\n_SlavePipeBase = collections.namedtuple(\'_SlavePipeBase\', [\'identifier\', \'queue\', \'result\'])\n\n\nclass SlavePipe(_SlavePipeBase):\n    """"""Pipe for master-slave communication.""""""\n\n    def run_slave(self, msg):\n        self.queue.put((self.identifier, msg))\n        ret = self.result.get()\n        self.queue.put(True)\n        return ret\n\n\nclass SyncMaster(object):\n    """"""An abstract `SyncMaster` object.\n    - During the replication, as the data parallel will trigger an callback of each module, all slave devices should\n    call `register(id)` and obtain an `SlavePipe` to communicate with the master.\n    - During the forward pass, master device invokes `run_master`, all messages from slave devices will be collected,\n    and passed to a registered callback.\n    - After receiving the messages, the master device should gather the information and determine to message passed\n    back to each slave devices.\n    """"""\n\n    def __init__(self, master_callback):\n        """"""\n        Args:\n            master_callback: a callback to be invoked after having collected messages from slave devices.\n        """"""\n        self._master_callback = master_callback\n        self._queue = queue.Queue()\n        self._registry = collections.OrderedDict()\n        self._activated = False\n\n    def __getstate__(self):\n        return {\'master_callback\': self._master_callback}\n\n    def __setstate__(self, state):\n        self.__init__(state[\'master_callback\'])\n\n    def register_slave(self, identifier):\n        """"""\n        Register an slave device.\n        Args:\n            identifier: an identifier, usually is the device id.\n        Returns: a `SlavePipe` object which can be used to communicate with the master device.\n        """"""\n        if self._activated:\n            assert self._queue.empty(), \'Queue is not clean before next initialization.\'\n            self._activated = False\n            self._registry.clear()\n        future = FutureResult()\n        self._registry[identifier] = _MasterRegistry(future)\n        return SlavePipe(identifier, self._queue, future)\n\n    def run_master(self, master_msg):\n        """"""\n        Main entry for the master device in each forward pass.\n        The messages were first collected from each devices (including the master device), and then\n        an callback will be invoked to compute the message to be sent back to each devices\n        (including the master device).\n        Args:\n            master_msg: the message that the master want to send to itself. This will be placed as the first\n            message when calling `master_callback`. For detailed usage, see `_SynchronizedBatchNorm` for an example.\n        Returns: the message to be sent back to the master device.\n        """"""\n        self._activated = True\n\n        intermediates = [(0, master_msg)]\n        for i in range(self.nr_slaves):\n            intermediates.append(self._queue.get())\n\n        results = self._master_callback(intermediates)\n        assert results[0][0] == 0, \'The first result should belongs to the master.\'\n\n        for i, res in results:\n            if i == 0:\n                continue\n            self._registry[i].result.put(res)\n\n        for i in range(self.nr_slaves):\n            assert self._queue.get() is True\n\n        return results[0][1]\n\n    @property\n    def nr_slaves(self):\n        return len(self._registry)\n'"
models/sync_batchnorm/replicate.py,1,"b'# -*- coding: utf-8 -*-\n# File   : replicate.py\n# Author : Jiayuan Mao\n# Email  : maojiayuan@gmail.com\n# Date   : 27/01/2018\n#\n# This file is part of Synchronized-BatchNorm-PyTorch.\n# https://github.com/vacancy/Synchronized-BatchNorm-PyTorch\n# Distributed under MIT License.\n\nimport functools\n\nfrom torch.nn.parallel.data_parallel import DataParallel\n\n__all__ = [\n    \'CallbackContext\',\n    \'execute_replication_callbacks\',\n    \'DataParallelWithCallback\',\n    \'patch_replication_callback\'\n]\n\n\nclass CallbackContext(object):\n    pass\n\n\ndef execute_replication_callbacks(modules):\n    """"""\n    Execute an replication callback `__data_parallel_replicate__` on each module created by original replication.\n    The callback will be invoked with arguments `__data_parallel_replicate__(ctx, copy_id)`\n    Note that, as all modules are isomorphism, we assign each sub-module with a context\n    (shared among multiple copies of this module on different devices).\n    Through this context, different copies can share some information.\n    We guarantee that the callback on the master copy (the first copy) will be called ahead of calling the callback\n    of any slave copies.\n    """"""\n    master_copy = modules[0]\n    nr_modules = len(list(master_copy.modules()))\n    ctxs = [CallbackContext() for _ in range(nr_modules)]\n\n    for i, module in enumerate(modules):\n        for j, m in enumerate(module.modules()):\n            if hasattr(m, \'__data_parallel_replicate__\'):\n                m.__data_parallel_replicate__(ctxs[j], i)\n\n\nclass DataParallelWithCallback(DataParallel):\n    """"""\n    Data Parallel with a replication callback.\n    An replication callback `__data_parallel_replicate__` of each module will be invoked after being created by\n    original `replicate` function.\n    The callback will be invoked with arguments `__data_parallel_replicate__(ctx, copy_id)`\n    Examples:\n        > sync_bn = SynchronizedBatchNorm1d(10, eps=1e-5, affine=False)\n        > sync_bn = DataParallelWithCallback(sync_bn, device_ids=[0, 1])\n        # sync_bn.__data_parallel_replicate__ will be invoked.\n    """"""\n\n    def replicate(self, module, device_ids):\n        modules = super(DataParallelWithCallback, self).replicate(module, device_ids)\n        execute_replication_callbacks(modules)\n        return modules\n\n\ndef patch_replication_callback(data_parallel):\n    """"""\n    Monkey-patch an existing `DataParallel` object. Add the replication callback.\n    Useful when you have customized `DataParallel` implementation.\n    Examples:\n        > sync_bn = SynchronizedBatchNorm1d(10, eps=1e-5, affine=False)\n        > sync_bn = DataParallel(sync_bn, device_ids=[0, 1])\n        > patch_replication_callback(sync_bn)\n        # this is equivalent to\n        > sync_bn = SynchronizedBatchNorm1d(10, eps=1e-5, affine=False)\n        > sync_bn = DataParallelWithCallback(sync_bn, device_ids=[0, 1])\n    """"""\n\n    assert isinstance(data_parallel, DataParallel)\n\n    old_replicate = data_parallel.replicate\n\n    @functools.wraps(old_replicate)\n    def new_replicate(module, device_ids):\n        modules = old_replicate(module, device_ids)\n        execute_replication_callbacks(modules)\n        return modules\n\n    data_parallel.replicate = new_replicate'"
models/sync_batchnorm/unittest.py,1,"b""# -*- coding: utf-8 -*-\n# File   : unittest.py\n# Author : Jiayuan Mao\n# Email  : maojiayuan@gmail.com\n# Date   : 27/01/2018\n#\n# This file is part of Synchronized-BatchNorm-PyTorch.\n# https://github.com/vacancy/Synchronized-BatchNorm-PyTorch\n# Distributed under MIT License.\n\nimport unittest\n\nimport numpy as np\nfrom torch.autograd import Variable\n\n\ndef as_numpy(v):\n    if isinstance(v, Variable):\n        v = v.data\n    return v.cpu().numpy()\n\n\nclass TorchTestCase(unittest.TestCase):\n    def assertTensorClose(self, a, b, atol=1e-3, rtol=1e-3):\n        npa, npb = as_numpy(a), as_numpy(b)\n        self.assertTrue(\n                np.allclose(npa, npb, atol=atol),\n                'Tensor close check failed\\n{}\\n{}\\nadiff={}, rdiff={}'.format(a, b, np.abs(npa - npb).max(), np.abs((npa - npb) / np.fmax(npa, 1e-5)).max())\n        )\n"""
scripts/BIG/binary_mask_negate.py,0,"b'import cv2\nimport sys\n\nimA = cv2.imread(sys.argv[1])\nimB = cv2.imread(sys.argv[2])\nout = sys.argv[3]\n\n\nimC = imA - imB\ncv2.imwrite(out, imC)\n\n'"
scripts/BIG/convert_binary.py,0,"b""import cv2\nimport sys\n\nim = cv2.imread(sys.argv[1])\nif len(im.shape) > 2:\n    im = im.sum(2)\n    im = (im > 0).astype('uint8') * 255\n\n    cv2.imwrite(sys.argv[1], im)\n\n"""
scripts/BIG/convert_deeplab_outputs.py,0,"b""import os\nimport sys\n\nimport cv2\nimport numpy as np\n\nclasses = {\n    'aeroplane': 1, \n    'bicycle': 2, \n    'bird': 3, \n    'boat': 4, \n    'bottle': 5, \n    'bus': 6, \n    'car': 7, \n    'cat': 8, \n    'chair': 9, \n    'cow': 10, \n    'diningtable': 11, \n    'dog': 12, \n    'horse': 13, \n    'motorbike': 14, \n    'person': 15, \n    'pottedplant': 16, \n    'sheep': 17, \n    'sofa': 18, \n    'train': 19, \n    'tv': 20, \n}\n\nroot = sys.argv[1]\n\nim_list = os.listdir(root)\nim_list = [f for f in im_list if '_im.jpg' in f]\n\nfor im_name in im_list:\n    im = cv2.imread(os.path.join(root, im_name))\n    h, w, _ = im.shape\n    print(h, w)\n\n    seg_name = im_name.replace('_im.jpg', '_seg.png')\n    seg = cv2.imread(os.path.join(root, seg_name))\n\n    print(np.unique(seg))\n\n    for k, v in classes.items():\n        if k in seg_name:\n            selected_class = v\n            print(seg_name, ', Selected: ', k, v)\n            break\n\n    seg_class = (seg==selected_class).astype('float32')\n    seg_class = cv2.resize(seg_class, (w, h), interpolation=cv2.INTER_CUBIC)\n\n    seg_class = (seg_class>0.5).astype('uint8') * 255\n\n    cv2.imwrite(os.path.join(root, seg_name), seg_class)\n"""
scripts/BIG/convert_refinenet_output.py,0,"b""import os\nimport sys\n\nimport cv2\nimport numpy as np\n\nimport h5py\n\nclasses = {\n    'aeroplane': 1, \n    'bicycle': 2, \n    'bird': 3, \n    'boat': 4, \n    'bottle': 5, \n    'bus': 6, \n    'car': 7, \n    'cat': 8, \n    'chair': 9, \n    'cow': 10, \n    'diningtable': 11, \n    'dog': 12, \n    'horse': 13, \n    'motorbike': 14, \n    'person': 15, \n    'pottedplant': 16, \n    'sheep': 17, \n    'sofa': 18, \n    'train': 19, \n    'tv': 20, \n}\n\nroot = sys.argv[1]\n\nim_list = os.listdir(root)\nim_list = [f for f in im_list if '_im.jpg' in f]\n\nfor im_name in im_list:\n    im = cv2.imread(os.path.join(root, im_name))\n    h, w, _ = im.shape\n    print(h, w)\n\n    mat_name = im_name.replace('_im.jpg', '.mat')\n\n    with h5py.File(os.path.join(root, mat_name), 'r') as mat:\n\n        seg = mat['data_obj']['mask_data']\n        seg = np.array(seg).T\n\n        for k, v in classes.items():\n            if k in mat_name:\n                selected_class = v\n                print(mat_name, ', Selected: ', k, v)\n                break\n\n        seg_class = (seg==selected_class).astype('float32')\n        seg_class = cv2.resize(seg_class, (w, h), interpolation=cv2.INTER_CUBIC)\n\n        seg_class = (seg_class>0.5).astype('uint8') * 255\n\n        seg_name = im_name.replace('_im.jpg', '_seg.png')\n        cv2.imwrite(os.path.join(root, seg_name), seg_class)\n"""
scripts/PASCAL_FINE/convert_deeplab_outputs.py,0,"b""import os\nimport sys\n\nimport cv2\nimport numpy as np\n\nroot = sys.argv[1]\nseg_root = sys.argv[2]\n\nim_list = os.listdir(root)\nim_list = [f for f in im_list if '_im.png' in f]\n\nfor im_name in im_list:\n    im = cv2.imread(os.path.join(root, im_name))\n    h, w, _ = im.shape\n    print(h, w)\n\n    seg_name = im_name.replace('_im.png', '_seg.png')\n    print(im_name[:-10])\n    seg = cv2.imread(os.path.join(seg_root, im_name[:-10]+'.png'))\n\n    selected_class = int(im_name[-9:-7])\n    print(np.unique(seg), selected_class)\n\n    seg_class = (seg==selected_class).astype('float32')\n    seg_class = cv2.resize(seg_class, (w, h), interpolation=cv2.INTER_CUBIC)\n\n    seg_class = (seg_class>0.5).astype('uint8') * 255\n\n    cv2.imwrite(os.path.join(root, seg_name), seg_class)\n"""
scripts/PASCAL_FINE/convert_psp_outputs.py,0,"b""import os\nimport sys\n\nimport cv2\nimport numpy as np\n\nroot = sys.argv[1]\n\nim_list = os.listdir(root)\nim_list = [f for f in im_list if '_im.png' in f]\n\nfor im_name in im_list:\n    im = cv2.imread(os.path.join(root, im_name))\n    h, w, _ = im.shape\n    print(h, w)\n\n    seg_name = im_name.replace('_im.png', '_seg.png')\n    seg = cv2.imread(os.path.join(root, seg_name))\n\n    selected_class = int(im_name[-9:-7])\n    print(np.unique(seg), selected_class)\n\n    seg_class = (seg==selected_class).astype('float32')\n    seg_class = cv2.resize(seg_class, (w, h), interpolation=cv2.INTER_CUBIC)\n\n    seg_class = (seg_class>0.5).astype('uint8') * 255\n\n    cv2.imwrite(os.path.join(root, seg_name), seg_class)\n"""
scripts/PASCAL_FINE/convert_refinenet_output.py,0,"b""import os\nimport sys\n\nimport cv2\nimport numpy as np\n\nimport h5py\n\nroot = sys.argv[1]\n\nim_list = os.listdir(root)\nim_list = [f for f in im_list if '_im.png' in f]\n\nfor im_name in im_list:\n    im = cv2.imread(os.path.join(root, im_name))\n    h, w, _ = im.shape\n    print(h, w)\n\n    mat_name = im_name.replace('_im.png', '.mat')\n    with h5py.File(os.path.join(root, mat_name), 'r') as mat:\n\n        seg = mat['data_obj']['mask_data']\n        seg = np.array(seg).T\n\n        selected_class = int(im_name[-9:-7])\n        print(np.unique(seg), selected_class)\n\n        seg_class = (seg==selected_class).astype('float32')\n        seg_class = cv2.resize(seg_class, (w, h), interpolation=cv2.INTER_CUBIC)\n\n        seg_class = (seg_class>0.5).astype('uint8') * 255\n\n        seg_name = im_name.replace('_im.png', '_seg.png')\n        cv2.imwrite(os.path.join(root, seg_name), seg_class)\n"""
scripts/ade20K/ade_expand_inst.py,0,"b""import os\nimport sys\nfrom shutil import copyfile\n\nfrom PIL import Image\nimport numpy as np\nimport cv2\n\nimport progressbar\n\nimg_path = sys.argv[1]\ngt_path = sys.argv[2]\nseg_path = sys.argv[3]\nout_path = sys.argv[4]\n\nseg_list = os.listdir(seg_path)\n\nos.makedirs(out_path, exist_ok=True)\n\ndef get_disk_kernel(size):\n    r = size // 2\n\n    y, x = np.ogrid[-r:size-r, -r:size-r]\n    mask = x*x + y*y <= r*r\n\n    array = np.zeros((size, size)).astype('uint8')\n    array[mask] = 1\n    return array\n\ndisc_kernel = get_disk_kernel(10)\nfor im_idx, seg in enumerate(progressbar.progressbar(seg_list)):\n    name = os.path.basename(seg)[:-4]\n\n    im_full_path = os.path.join(img_path, name+'.jpg')\n\n    seg_img = Image.open(os.path.join(seg_path, seg)).convert('L')\n    gt_img = Image.open(os.path.join(gt_path, seg)).convert('L')\n\n    seg_img = np.array(seg_img)\n    gt_img = np.array(gt_img)\n\n    seg_classes = np.unique(seg_img)\n    gt_classes = np.unique(gt_img)\n\n    all_classes = np.union1d(seg_classes, gt_classes)\n\n    seg_written = False\n    for c in all_classes:\n        class_seg = (seg_img == c).astype('uint8')\n        class_gt  = (gt_img == c).astype('uint8')\n\n        # Remove small overall parts\n        if class_seg.sum() <= 32*32:\n            continue\n\n        class_seg_dilated = cv2.dilate(class_seg, disc_kernel)\n        _, components_map = cv2.connectedComponents(class_seg_dilated, connectivity=8)\n        components = np.unique(components_map)\n        components = components[components!=0] # Remove zero, the background class\n\n        for comp in components:\n            comp_map = (components_map == comp).astype('uint8')\n            # Similar to a closing operator, we don't want to include extra regions\n            comp_map = cv2.erode(comp_map, disc_kernel)\n\n            if comp_map.sum() <= 32*32:\n                continue\n\n            # Masking\n            comp_seg = (comp_map * class_seg) * 255\n            comp_gt  = (comp_map * class_gt) * 255\n\n            seg_written = True\n            cv2.imwrite(os.path.join(out_path, name + '_%d.%d_seg.png' % (c, comp)), comp_seg)\n            cv2.imwrite(os.path.join(out_path, name + '_%d.%d_gt.png' % (c, comp)), comp_gt)\n\n    if seg_written:\n        copyfile(im_full_path, os.path.join(out_path, name + '_im.jpg'))\n"""
scripts/ade20K/all_plus_one.py,0,"b'import os\nimport sys\n\nimport cv2\n\nroot = sys.argv[1]\n\nmask_list = os.listdir(root)\n\nfor mask_name in mask_list:\n    mask = cv2.imread(os.path.join(root, mask_name))\n    cv2.imwrite(os.path.join(root, mask_name), mask+1)\n'"
scripts/ade20K/convert_refinenet_output.py,0,"b""import os\nimport sys\n\nimport cv2\nimport numpy as np\n\nimport h5py\n\nim_root = sys.argv[1]\nmask_root = sys.argv[2]\n\nim_list = os.listdir(im_root)\nim_list = [f for f in im_list]\n\nfor im_name in im_list:\n    im = cv2.imread(os.path.join(im_root, im_name))\n    h, w, _ = im.shape\n    print(h, w)\n\n    mat_name = im_name.replace('.jpg', '.mat')\n    with h5py.File(os.path.join(mask_root, mat_name), 'r') as mat:\n\n        seg = mat['data_obj']['mask_data']\n        seg = np.array(seg).T\n\n        seg = cv2.resize(seg, (w, h), interpolation=cv2.INTER_LINEAR)\n\n        seg_name = im_name.replace('.jpg', '.png')\n        cv2.imwrite(os.path.join(mask_root, seg_name), seg)\n"""
segmentation-refinement/segmentation_refinement/__init__.py,0,b'from segmentation_refinement.main import Refiner'
segmentation-refinement/segmentation_refinement/download.py,0,"b'import requests\n\ndef download_file_from_google_drive(id, destination):\n    URL = ""https://docs.google.com/uc?export=download""\n\n    session = requests.Session()\n\n    response = session.get(URL, params = { \'id\' : id }, stream = True)\n    token = get_confirm_token(response)\n\n    if token:\n        params = { \'id\' : id, \'confirm\' : token }\n        response = session.get(URL, params = params, stream = True)\n\n    save_response_content(response, destination)    \n\ndef get_confirm_token(response):\n    for key, value in response.cookies.items():\n        if key.startswith(\'download_warning\'):\n            return value\n\n    return None\n\ndef save_response_content(response, destination):\n    CHUNK_SIZE = 32768\n\n    with open(destination, ""wb"") as f:\n        for chunk in response.iter_content(CHUNK_SIZE):\n            if chunk: # filter out keep-alive new chunks\n                f.write(chunk)\n'"
segmentation-refinement/segmentation_refinement/eval_helper.py,8,"b'import torch\nimport torch.nn.functional as F\n\ndef resize_max_side(im, size, method):\n    h, w = im.shape[-2:]\n    max_side = max(h, w)\n    ratio = size / max_side\n    if method in [\'bilinear\', \'bicubic\']:\n        return F.interpolate(im, scale_factor=ratio, mode=method, align_corners=False)\n    else:\n        return F.interpolate(im, scale_factor=ratio, mode=method)\n\ndef safe_forward(model, im, seg, inter_s8=None, inter_s4=None):\n    """"""\n    Slightly pads the input image such that its length is a multiple of 8\n    """"""\n    b, _, ph, pw = seg.shape\n    if (ph % 8 != 0) or (pw % 8 != 0):\n        newH = ((ph//8+1)*8)\n        newW = ((pw//8+1)*8)\n        p_im = torch.zeros(b, 3, newH, newW, device=im.device)\n        p_seg = torch.zeros(b, 1, newH, newW, device=im.device) - 1\n\n        p_im[:,:,0:ph,0:pw] = im\n        p_seg[:,:,0:ph,0:pw] = seg\n        im = p_im\n        seg = p_seg\n\n        if inter_s8 is not None:\n            p_inter_s8 = torch.zeros(b, 1, newH, newW, device=im.device) - 1\n            p_inter_s8[:,:,0:ph,0:pw] = inter_s8\n            inter_s8 = p_inter_s8\n        if inter_s4 is not None:\n            p_inter_s4 = torch.zeros(b, 1, newH, newW, device=im.device) - 1\n            p_inter_s4[:,:,0:ph,0:pw] = inter_s4\n            inter_s4 = p_inter_s4\n\n    images = model(im, seg, inter_s8, inter_s4)\n    return_im = {}\n\n    for key in [\'pred_224\', \'pred_28_3\', \'pred_56_2\']:\n        return_im[key] = images[key][:,:,0:ph,0:pw]\n    del images\n\n    return return_im\n\ndef process_high_res_im(model, im, seg, L=900):\n\n    stride = L//2\n\n    _, _, h, w = seg.shape\n\n    """"""\n    Global Step\n    """"""\n    if max(h, w) > L:\n        im_small = resize_max_side(im, L, \'area\')\n        seg_small = resize_max_side(seg, L, \'area\')\n    elif max(h, w) < L:\n        im_small = resize_max_side(im, L, \'bicubic\')\n        seg_small = resize_max_side(seg, L, \'bilinear\')\n    else:\n        im_small = im\n        seg_small = seg\n\n    images = safe_forward(model, im_small, seg_small)\n\n    pred_224 = images[\'pred_224\']\n    pred_56 = images[\'pred_56_2\']\n    \n    """"""\n    Local step\n    """"""\n\n    for new_size in [max(h, w)]:\n        im_small = resize_max_side(im, new_size, \'area\')\n        seg_small = resize_max_side(seg, new_size, \'area\')\n        _, _, h, w = seg_small.shape\n\n        combined_224 = torch.zeros_like(seg_small)\n        combined_weight = torch.zeros_like(seg_small)\n\n        r_pred_224 = (F.interpolate(pred_224, size=(h, w), mode=\'bilinear\', align_corners=False)>0.5).float()*2-1\n        r_pred_56 = F.interpolate(pred_56, size=(h, w), mode=\'bilinear\', align_corners=False)*2-1\n\n        padding = 16\n        step_size = stride - padding*2\n        step_len  = L\n\n        used_start_idx = {}\n        for x_idx in range((w)//step_size+1):\n            for y_idx in range((h)//step_size+1):\n\n                start_x = x_idx * step_size\n                start_y = y_idx * step_size\n                end_x = start_x + step_len\n                end_y = start_y + step_len\n\n                # Shift when required\n                if end_y > h:\n                    end_y = h\n                    start_y = h - step_len\n                if end_x > w:\n                    end_x = w\n                    start_x = w - step_len\n\n                # Bound x/y range\n                start_x = max(0, start_x)\n                start_y = max(0, start_y)\n                end_x = min(w, end_x)\n                end_y = min(h, end_y)\n\n                # The same crop might appear twice due to bounding/shifting\n                start_idx = start_y*w + start_x\n                if start_idx in used_start_idx:\n                    continue\n                else:\n                    used_start_idx[start_idx] = True\n                \n                # Take crop\n                im_part = im_small[:,:,start_y:end_y, start_x:end_x]\n                seg_224_part = r_pred_224[:,:,start_y:end_y, start_x:end_x]\n                seg_56_part = r_pred_56[:,:,start_y:end_y, start_x:end_x]\n\n                # Skip when it is not an interesting crop anyway\n                seg_part_norm = (seg_224_part>0).float()\n                high_thres = 0.9\n                low_thres = 0.1\n                if (seg_part_norm.mean() > high_thres) or (seg_part_norm.mean() < low_thres):\n                    continue\n                grid_images = safe_forward(model, im_part, seg_224_part, seg_56_part)\n                grid_pred_224 = grid_images[\'pred_224\']\n\n                # Padding\n                pred_sx = pred_sy = 0\n                pred_ex = step_len\n                pred_ey = step_len\n\n                if start_x != 0:\n                    start_x += padding\n                    pred_sx += padding\n                if start_y != 0:\n                    start_y += padding\n                    pred_sy += padding\n                if end_x != w:\n                    end_x -= padding\n                    pred_ex -= padding\n                if end_y != h:\n                    end_y -= padding\n                    pred_ey -= padding\n\n                combined_224[:,:,start_y:end_y, start_x:end_x] += grid_pred_224[:,:,pred_sy:pred_ey,pred_sx:pred_ex]\n\n                del grid_pred_224\n\n                # Used for averaging\n                combined_weight[:,:,start_y:end_y, start_x:end_x] += 1\n\n        # Final full resolution output\n        seg_norm = (r_pred_224/2+0.5)\n        pred_224 = combined_224 / combined_weight\n        pred_224 = torch.where(combined_weight==0, seg_norm, pred_224)\n\n    _, _, h, w = seg.shape\n    images = {}\n    images[\'pred_224\'] = F.interpolate(pred_224, size=(h, w), mode=\'bilinear\', align_corners=True)\n\n    return images[\'pred_224\']\n\n\ndef process_im_single_pass(model, im, seg, L=900):\n    """"""\n    A single pass version, aka global step only.\n    """"""\n\n    _, _, h, w = im.shape\n    if max(h, w) < L:\n        im = resize_max_side(im, L, \'bicubic\')\n        seg = resize_max_side(seg, L, \'bilinear\')\n\n    if max(h, w) > L:\n        im = resize_max_side(im, L, \'area\')\n        seg = resize_max_side(seg, L, \'area\')\n\n    images = safe_forward(model, im, seg)\n\n    if max(h, w) < L:\n        images[\'pred_224\'] = F.interpolate(images[\'pred_224\'], size=(h, w), mode=\'area\')\n    elif max(h, w) > L:\n        images[\'pred_224\'] = F.interpolate(images[\'pred_224\'], size=(h, w), mode=\'bilinear\', align_corners=True)\n\n    return images[\'pred_224\']\n'"
segmentation-refinement/segmentation_refinement/main.py,2,"b'import os\n\nimport numpy as np\nimport torch\nfrom torchvision import transforms\n\nfrom segmentation_refinement.models.psp.pspnet import RefinementModule\nfrom segmentation_refinement.eval_helper import process_high_res_im, process_im_single_pass\nfrom segmentation_refinement.download import download_file_from_google_drive\n\n\nclass Refiner:\n    def __init__(self, device=\'cpu\', model_folder=None):\n        """"""\n        Initialize the segmentation refinement model.\n        device can be \'cpu\' or \'cuda\'\n        model_folder specifies the folder in which the model will be downloaded and stored. Defaulted in ~/.segmentation-refinement.\n        """"""\n        self.model = RefinementModule()\n        self.device = device\n        if model_folder is None:\n            model_folder = os.path.expanduser(""~/.segmentation-refinement"")\n\n        if not os.path.exists(model_folder):\n            os.makedirs(model_folder, exist_ok=True)\n\n        model_path = os.path.join(model_folder, \'model\')\n        if not os.path.exists(model_path):\n            print(\'Downloading the model file into: %s...\' % model_path)\n            download_file_from_google_drive(\'103nLN1JQCs2yASkna0HqfioYZO7MA_J9\', model_path)\n\n        model_dict = torch.load(model_path)\n        new_dict = {}\n        for k, v in model_dict.items():\n            name = k[7:] # Remove module. from dataparallel\n            new_dict[name] = v\n        self.model.load_state_dict(new_dict)\n        self.model.eval().to(device)\n\n        self.im_transform = transforms.Compose([\n            transforms.ToTensor(),\n            transforms.Normalize(\n                mean=[0.485, 0.456, 0.406],\n                std=[0.229, 0.224, 0.225]\n            ),\n        ])\n\n        self.seg_transform = transforms.Compose([\n            transforms.ToTensor(),\n            transforms.Normalize(\n                mean=[0.5],\n                std=[0.5]\n            ),\n        ])\n\n    def refine(self, image, mask, fast=False, L=900):\n        with torch.no_grad():\n            """"""\n            Refines an input segmentation mask of the image.\n\n            image should be of size [H, W, 3]. Range 0~255.\n            Mask should be of size [H, W] or [H, W, 1]. Range 0~255. We will make the mask binary by thresholding at 127.\n            Fast mode - Use the global step only. Default: False. The speedup is more significant for high resolution images.\n            L - Hyperparameter. Setting a lower value reduces memory usage. In fast mode, a lower L will make it runs faster as well.\n            """"""\n            image = self.im_transform(image).unsqueeze(0).to(self.device)\n            mask = self.seg_transform((mask>127).astype(np.uint8)*255).unsqueeze(0).to(self.device)\n            if len(mask.shape) < 4:\n                mask = mask.unsqueeze(0)\n\n            if fast:\n                output = process_im_single_pass(self.model, image, mask, L)\n            else:\n                output = process_high_res_im(self.model, image, mask, L)\n\n            return (output[0,0].cpu().numpy()*255).astype(\'uint8\')\n'"
segmentation-refinement/segmentation_refinement/models/__init__.py,0,b''
segmentation-refinement/segmentation_refinement/models/psp/__init__.py,0,b''
segmentation-refinement/segmentation_refinement/models/psp/extractors.py,2,"b'from collections import OrderedDict\nimport math\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\ndef conv3x3(in_planes, out_planes, stride=1, dilation=1):\n    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n                     padding=dilation, dilation=dilation, bias=False)\n\n\nclass Bottleneck(nn.Module):\n    expansion = 4\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None, dilation=1):\n        super(Bottleneck, self).__init__()\n        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, dilation=dilation,\n                               padding=dilation, bias=False)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)\n        self.bn3 = nn.BatchNorm2d(planes * 4)\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\n\nclass ResNet(nn.Module):\n    def __init__(self, block, layers=(3, 4, 23, 3)):\n        self.inplanes = 64\n        super(ResNet, self).__init__()\n        self.conv1 = nn.Conv2d(6, 64, kernel_size=7, stride=2, padding=3,\n                               bias=False)\n        self.bn1 = nn.BatchNorm2d(64)\n        self.relu = nn.ReLU(inplace=True)\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n        self.layer1 = self._make_layer(block, 64, layers[0])\n        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n        self.layer3 = self._make_layer(block, 256, layers[2], stride=1, dilation=2)\n        self.layer4 = self._make_layer(block, 512, layers[3], stride=1, dilation=4)\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n                m.weight.data.normal_(0, math.sqrt(2. / n))\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n\n    def _make_layer(self, block, planes, blocks, stride=1, dilation=1):\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                nn.Conv2d(self.inplanes, planes * block.expansion,\n                          kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(planes * block.expansion),\n            )\n\n        layers = [block(self.inplanes, planes, stride, downsample)]\n        self.inplanes = planes * block.expansion\n        for i in range(1, blocks):\n            layers.append(block(self.inplanes, planes, dilation=dilation))\n\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        x_1 = self.conv1(x)  # /2\n        x = self.bn1(x_1)\n        x = self.relu(x)\n        x = self.maxpool(x)  # /2\n\n        x_2 = self.layer1(x)\n        x = self.layer2(x_2)   # /2\n        x = self.layer3(x)\n        x = self.layer4(x)\n\n        return x, x_1, x_2\n\n\ndef resnet50():\n    model = ResNet(Bottleneck, [3, 4, 6, 3])\n    return model\n\n'"
segmentation-refinement/segmentation_refinement/models/psp/pspnet.py,16,"b'import torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\nfrom segmentation_refinement.models.psp import extractors\n\n\nclass PSPModule(nn.Module):\n    def __init__(self, features, out_features=1024, sizes=(1, 2, 3, 6)):\n        super().__init__()\n        self.stages = []\n        self.stages = nn.ModuleList([self._make_stage(features, size) for size in sizes])\n        self.bottleneck = nn.Conv2d(features * (len(sizes) + 1), out_features, kernel_size=1)\n        self.relu = nn.ReLU(inplace=True)\n\n    def _make_stage(self, features, size):\n        prior = nn.AdaptiveAvgPool2d(output_size=(size, size))\n        conv = nn.Conv2d(features, features, kernel_size=1, bias=False)\n        return nn.Sequential(prior, conv)\n\n    def forward(self, feats):\n        h, w = feats.size(2), feats.size(3)\n        set_priors = [F.interpolate(input=stage(feats), size=(h, w), mode=\'bilinear\', align_corners=False) for stage in self.stages]\n        priors = set_priors + [feats]\n        bottle = self.bottleneck(torch.cat(priors, 1))\n        return self.relu(bottle)\n\n\nclass PSPUpsample(nn.Module):\n    def __init__(self, x_channels, in_channels, out_channels):\n        super().__init__()\n        self.conv = nn.Sequential(\n            nn.BatchNorm2d(in_channels),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(in_channels, out_channels, 3, padding=1),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(out_channels, out_channels, 3, padding=1),\n        )\n\n        self.conv2 = nn.Sequential(\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(out_channels, out_channels, 3, padding=1),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(out_channels, out_channels, 3, padding=1),\n        )\n\n        self.shortcut = nn.Conv2d(x_channels, out_channels, kernel_size=1)\n\n    def forward(self, x, up):\n        x = F.interpolate(input=x, scale_factor=2, mode=\'bilinear\', align_corners=False)\n\n        p = self.conv(torch.cat([x, up], 1))\n        sc = self.shortcut(x)\n\n        p = p + sc\n\n        p2 = self.conv2(p)\n\n        return p + p2\n\n\nclass RefinementModule(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n        self.feats = extractors.resnet50()\n        self.psp = PSPModule(2048, 1024, (1, 2, 3, 6))\n\n        self.up_1 = PSPUpsample(1024, 1024+256, 512)\n        self.up_2 = PSPUpsample(512, 512+64, 256)\n        self.up_3 = PSPUpsample(256, 256+3, 32)\n        \n        self.final_28 = nn.Sequential(\n            nn.Conv2d(1024, 32, kernel_size=1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(32, 1, kernel_size=1),\n        )\n\n        self.final_56 = nn.Sequential(\n            nn.Conv2d(512, 32, kernel_size=1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(32, 1, kernel_size=1),\n        )\n\n        self.final_11 = nn.Conv2d(32+3, 32, kernel_size=1)\n        self.final_21 = nn.Conv2d(32, 1, kernel_size=1)\n\n    def forward(self, x, seg, inter_s8=None, inter_s4=None):\n\n        images = {}\n\n        """"""\n        First iteration, s8 output\n        """"""\n        if inter_s8 is None:\n            p = torch.cat((x, seg, seg, seg), 1)\n\n            f, f_1, f_2 = self.feats(p) \n            p = self.psp(f)\n\n            inter_s8 = self.final_28(p)\n            r_inter_s8 = F.interpolate(inter_s8, scale_factor=8, mode=\'bilinear\', align_corners=False)\n            r_inter_tanh_s8 = torch.tanh(r_inter_s8)\n\n            images[\'pred_28\'] = torch.sigmoid(r_inter_s8)\n            images[\'out_28\'] = r_inter_s8\n        else:\n            r_inter_tanh_s8 = inter_s8\n\n        """"""\n        Second iteration, s8 output\n        """"""\n        if inter_s4 is None:\n            p = torch.cat((x, seg, r_inter_tanh_s8, r_inter_tanh_s8), 1)\n\n            f, f_1, f_2 = self.feats(p) \n            p = self.psp(f)\n            inter_s8_2 = self.final_28(p)\n            r_inter_s8_2 = F.interpolate(inter_s8_2, scale_factor=8, mode=\'bilinear\', align_corners=False)\n            r_inter_tanh_s8_2 = torch.tanh(r_inter_s8_2)\n\n            p = self.up_1(p, f_2)\n\n            inter_s4 = self.final_56(p)\n            r_inter_s4 = F.interpolate(inter_s4, scale_factor=4, mode=\'bilinear\', align_corners=False)\n            r_inter_tanh_s4 = torch.tanh(r_inter_s4)\n\n            images[\'pred_28_2\'] = torch.sigmoid(r_inter_s8_2)\n            images[\'out_28_2\'] = r_inter_s8_2\n            images[\'pred_56\'] = torch.sigmoid(r_inter_s4)\n            images[\'out_56\'] = r_inter_s4\n        else:\n            r_inter_tanh_s8_2 = inter_s8\n            r_inter_tanh_s4 = inter_s4\n\n        """"""\n        Third iteration, s1 output\n        """"""\n        p = torch.cat((x, seg, r_inter_tanh_s8_2, r_inter_tanh_s4), 1)\n\n        f, f_1, f_2 = self.feats(p) \n        p = self.psp(f)\n        inter_s8_3 = self.final_28(p)\n        r_inter_s8_3 = F.interpolate(inter_s8_3, scale_factor=8, mode=\'bilinear\', align_corners=False)\n\n        p = self.up_1(p, f_2)\n        inter_s4_2 = self.final_56(p)\n        r_inter_s4_2 = F.interpolate(inter_s4_2, scale_factor=4, mode=\'bilinear\', align_corners=False)\n        p = self.up_2(p, f_1)\n        p = self.up_3(p, x)\n\n\n        """"""\n        Final output\n        """"""\n        p = F.relu(self.final_11(torch.cat([p, x], 1)), inplace=True)\n        p = self.final_21(p)\n\n        pred_224 = torch.sigmoid(p)\n\n        images[\'pred_224\'] = pred_224\n        images[\'out_224\'] = p\n        images[\'pred_28_3\'] = torch.sigmoid(r_inter_s8_3)\n        images[\'pred_56_2\'] = torch.sigmoid(r_inter_s4_2)\n        images[\'out_28_3\'] = r_inter_s8_3\n        images[\'out_56_2\'] = r_inter_s4_2\n\n        return images\n'"
