file_path,api_count,code
main.py,17,"b'#!/usr/bin/env python3\n\nimport json\nimport models\nimport utils\nimport argparse,random,logging,numpy,os\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\nfrom torch.autograd import Variable\nfrom torch.utils.data import DataLoader\nfrom torch.nn.utils import clip_grad_norm\nfrom time import time\nfrom tqdm import tqdm\n\nlogging.basicConfig(level=logging.INFO, format=\'%(asctime)s [INFO] %(message)s\')\nparser = argparse.ArgumentParser(description=\'extractive summary\')\n# model\nparser.add_argument(\'-save_dir\',type=str,default=\'checkpoints/\')\nparser.add_argument(\'-embed_dim\',type=int,default=100)\nparser.add_argument(\'-embed_num\',type=int,default=100)\nparser.add_argument(\'-pos_dim\',type=int,default=50)\nparser.add_argument(\'-pos_num\',type=int,default=100)\nparser.add_argument(\'-seg_num\',type=int,default=10)\nparser.add_argument(\'-kernel_num\',type=int,default=100)\nparser.add_argument(\'-kernel_sizes\',type=str,default=\'3,4,5\')\nparser.add_argument(\'-model\',type=str,default=\'RNN_RNN\')\nparser.add_argument(\'-hidden_size\',type=int,default=200)\n# train\nparser.add_argument(\'-lr\',type=float,default=1e-3)\nparser.add_argument(\'-batch_size\',type=int,default=32)\nparser.add_argument(\'-epochs\',type=int,default=5)\nparser.add_argument(\'-seed\',type=int,default=1)\nparser.add_argument(\'-train_dir\',type=str,default=\'data/train.json\')\nparser.add_argument(\'-val_dir\',type=str,default=\'data/val.json\')\nparser.add_argument(\'-embedding\',type=str,default=\'data/embedding.npz\')\nparser.add_argument(\'-word2id\',type=str,default=\'data/word2id.json\')\nparser.add_argument(\'-report_every\',type=int,default=1500)\nparser.add_argument(\'-seq_trunc\',type=int,default=50)\nparser.add_argument(\'-max_norm\',type=float,default=1.0)\n# test\nparser.add_argument(\'-load_dir\',type=str,default=\'checkpoints/RNN_RNN_seed_1.pt\')\nparser.add_argument(\'-test_dir\',type=str,default=\'data/test.json\')\nparser.add_argument(\'-ref\',type=str,default=\'outputs/ref\')\nparser.add_argument(\'-hyp\',type=str,default=\'outputs/hyp\')\nparser.add_argument(\'-filename\',type=str,default=\'x.txt\') # TextFile to be summarized\nparser.add_argument(\'-topk\',type=int,default=15)\n# device\nparser.add_argument(\'-device\',type=int)\n# option\nparser.add_argument(\'-test\',action=\'store_true\')\nparser.add_argument(\'-debug\',action=\'store_true\')\nparser.add_argument(\'-predict\',action=\'store_true\')\nargs = parser.parse_args()\nuse_gpu = args.device is not None\n\nif torch.cuda.is_available() and not use_gpu:\n    print(""WARNING: You have a CUDA device, should run with -device 0"")\n\n# set cuda device and seed\nif use_gpu:\n    torch.cuda.set_device(args.device)\ntorch.cuda.manual_seed(args.seed)\ntorch.manual_seed(args.seed)\nrandom.seed(args.seed)\nnumpy.random.seed(args.seed) \n    \ndef eval(net,vocab,data_iter,criterion):\n    net.eval()\n    total_loss = 0\n    batch_num = 0\n    for batch in data_iter:\n        features,targets,_,doc_lens = vocab.make_features(batch)\n        features,targets = Variable(features), Variable(targets.float())\n        if use_gpu:\n            features = features.cuda()\n            targets = targets.cuda()\n        probs = net(features,doc_lens)\n        loss = criterion(probs,targets)\n        total_loss += loss.data[0]\n        batch_num += 1\n    loss = total_loss / batch_num\n    net.train()\n    return loss\n\ndef train():\n    logging.info(\'Loading vocab,train and val dataset.Wait a second,please\')\n    \n    embed = torch.Tensor(np.load(args.embedding)[\'embedding\'])\n    with open(args.word2id) as f:\n        word2id = json.load(f)\n    vocab = utils.Vocab(embed, word2id)\n\n    with open(args.train_dir) as f:\n        examples = [json.loads(line) for line in f]\n    train_dataset = utils.Dataset(examples)\n\n    with open(args.val_dir) as f:\n        examples = [json.loads(line) for line in f]\n    val_dataset = utils.Dataset(examples)\n\n    # update args\n    args.embed_num = embed.size(0)\n    args.embed_dim = embed.size(1)\n    args.kernel_sizes = [int(ks) for ks in args.kernel_sizes.split(\',\')]\n    # build model\n    net = getattr(models,args.model)(args,embed)\n    if use_gpu:\n        net.cuda()\n    # load dataset\n    train_iter = DataLoader(dataset=train_dataset,\n            batch_size=args.batch_size,\n            shuffle=True)\n    val_iter = DataLoader(dataset=val_dataset,\n            batch_size=args.batch_size,\n            shuffle=False)\n    # loss function\n    criterion = nn.BCELoss()\n    # model info\n    print(net)\n    params = sum(p.numel() for p in list(net.parameters())) / 1e6\n    print(\'#Params: %.1fM\' % (params))\n    \n    min_loss = float(\'inf\')\n    optimizer = torch.optim.Adam(net.parameters(),lr=args.lr)\n    net.train()\n    \n    t1 = time() \n    for epoch in range(1,args.epochs+1):\n        for i,batch in enumerate(train_iter):\n            features,targets,_,doc_lens = vocab.make_features(batch)\n            features,targets = Variable(features), Variable(targets.float())\n            if use_gpu:\n                features = features.cuda()\n                targets = targets.cuda()\n            probs = net(features,doc_lens)\n            loss = criterion(probs,targets)\n            optimizer.zero_grad()\n            loss.backward()\n            clip_grad_norm(net.parameters(), args.max_norm)\n            optimizer.step()\n            if args.debug:\n                print(\'Batch ID:%d Loss:%f\' %(i,loss.data[0]))\n                continue\n            if i % args.report_every == 0:\n                cur_loss = eval(net,vocab,val_iter,criterion)\n                if cur_loss < min_loss:\n                    min_loss = cur_loss\n                    best_path = net.save()\n                logging.info(\'Epoch: %2d Min_Val_Loss: %f Cur_Val_Loss: %f\'\n                        % (epoch,min_loss,cur_loss))\n    t2 = time()\n    logging.info(\'Total Cost:%f h\'%((t2-t1)/3600))\n\ndef test():\n     \n    embed = torch.Tensor(np.load(args.embedding)[\'embedding\'])\n    with open(args.word2id) as f:\n        word2id = json.load(f)\n    vocab = utils.Vocab(embed, word2id)\n\n    with open(args.test_dir) as f:\n        examples = [json.loads(line) for line in f]\n    test_dataset = utils.Dataset(examples)\n\n    test_iter = DataLoader(dataset=test_dataset,\n                            batch_size=args.batch_size,\n                            shuffle=False)\n    if use_gpu:\n        checkpoint = torch.load(args.load_dir)\n    else:\n        checkpoint = torch.load(args.load_dir, map_location=lambda storage, loc: storage)\n\n    # checkpoint[\'args\'][\'device\'] saves the device used as train time\n    # if at test time, we are using a CPU, we must override device to None\n    if not use_gpu:\n        checkpoint[\'args\'].device = None\n    net = getattr(models,checkpoint[\'args\'].model)(checkpoint[\'args\'])\n    net.load_state_dict(checkpoint[\'model\'])\n    if use_gpu:\n        net.cuda()\n    net.eval()\n    \n    doc_num = len(test_dataset)\n    time_cost = 0\n    file_id = 1\n    for batch in tqdm(test_iter):\n        features,_,summaries,doc_lens = vocab.make_features(batch)\n        t1 = time()\n        if use_gpu:\n            probs = net(Variable(features).cuda(), doc_lens)\n        else:\n            probs = net(Variable(features), doc_lens)\n        t2 = time()\n        time_cost += t2 - t1\n        start = 0\n        for doc_id,doc_len in enumerate(doc_lens):\n            stop = start + doc_len\n            prob = probs[start:stop]\n            topk = min(args.topk,doc_len)\n            topk_indices = prob.topk(topk)[1].cpu().data.numpy()\n            topk_indices.sort()\n            doc = batch[\'doc\'][doc_id].split(\'\\n\')[:doc_len]\n            hyp = [doc[index] for index in topk_indices]\n            ref = summaries[doc_id]\n            with open(os.path.join(args.ref,str(file_id)+\'.txt\'), \'w\') as f:\n                f.write(ref)\n            with open(os.path.join(args.hyp,str(file_id)+\'.txt\'), \'w\') as f:\n                f.write(\'\\n\'.join(hyp))\n            start = stop\n            file_id = file_id + 1\n    print(\'Speed: %.2f docs / s\' % (doc_num / time_cost))\n\n\ndef predict(examples):\n    embed = torch.Tensor(np.load(args.embedding)[\'embedding\'])\n    with open(args.word2id) as f:\n        word2id = json.load(f)\n    vocab = utils.Vocab(embed, word2id)\n    pred_dataset = utils.Dataset(examples)\n\n    pred_iter = DataLoader(dataset=pred_dataset,\n                            batch_size=args.batch_size,\n                            shuffle=False)\n    if use_gpu:\n        checkpoint = torch.load(args.load_dir)\n    else:\n        checkpoint = torch.load(args.load_dir, map_location=lambda storage, loc: storage)\n\n    # checkpoint[\'args\'][\'device\'] saves the device used as train time\n    # if at test time, we are using a CPU, we must override device to None\n    if not use_gpu:\n        checkpoint[\'args\'].device = None\n    net = getattr(models,checkpoint[\'args\'].model)(checkpoint[\'args\'])\n    net.load_state_dict(checkpoint[\'model\'])\n    if use_gpu:\n        net.cuda()\n    net.eval()\n    \n    doc_num = len(pred_dataset)\n    time_cost = 0\n    file_id = 1\n    for batch in tqdm(pred_iter):\n        features, doc_lens = vocab.make_predict_features(batch)\n        t1 = time()\n        if use_gpu:\n            probs = net(Variable(features).cuda(), doc_lens)\n        else:\n            probs = net(Variable(features), doc_lens)\n        t2 = time()\n        time_cost += t2 - t1\n        start = 0\n        for doc_id,doc_len in enumerate(doc_lens):\n            stop = start + doc_len\n            prob = probs[start:stop]\n            topk = min(args.topk,doc_len)\n            topk_indices = prob.topk(topk)[1].cpu().data.numpy()\n            topk_indices.sort()\n            doc = batch[doc_id].split(\'. \')[:doc_len]\n            hyp = [doc[index] for index in topk_indices]\n            with open(os.path.join(args.hyp,str(file_id)+\'.txt\'), \'w\') as f:\n                f.write(\'. \'.join(hyp))\n            start = stop\n            file_id = file_id + 1\n    print(\'Speed: %.2f docs / s\' % (doc_num / time_cost))\n\nif __name__==\'__main__\':\n    if args.test:\n        test()\n    elif args.predict:\n        with open(args.filename) as file:\n            bod = [file.read()]\n        predict(bod)\n    else:\n        train()\n'"
preprocess.py,0,"b'#!/usr/bin/env python3\n\nimport argparse\nimport json\nimport numpy as np\nfrom collections import OrderedDict\nfrom glob import glob\nfrom time import time\nfrom multiprocessing import Pool,cpu_count\nfrom itertools import chain\n\ndef build_vocab(args):\n    print(\'start building vocab\')\n\n    PAD_IDX = 0\n    UNK_IDX = 1\n    PAD_TOKEN = \'PAD_TOKEN\'\n    UNK_TOKEN = \'UNK_TOKEN\'\n    \n    f = open(args.embed)\n    embed_dim = int(next(f).split()[1])\n\n    word2id = OrderedDict()\n    \n    word2id[PAD_TOKEN] = PAD_IDX\n    word2id[UNK_TOKEN] = UNK_IDX\n    \n    embed_list = []\n    # fill PAD and UNK vector\n    embed_list.append([0 for _ in range(embed_dim)])\n    embed_list.append([0 for _ in range(embed_dim)])\n    \n    # build Vocab\n    for line in f:\n        tokens = line.split()\n        word = tokens[:-1*embed_dim][0]\n        vector = [float(num) for num in tokens[-1*embed_dim:]]\n        embed_list.append(vector)\n        word2id[word] = len(word2id)\n    f.close()\n    embed = np.array(embed_list,dtype=np.float32)\n    np.savez_compressed(file=args.vocab, embedding=embed)\n    with open(args.word2id,\'w\') as f:\n        json.dump(word2id,f)\n\ndef worker(files):\n    examples = []\n    for f in files:\n        parts = open(f,encoding=\'latin-1\').read().split(\'\\n\\n\')\n        try:\n            entities = { line.strip().split(\':\')[0]:line.strip().split(\':\')[1].lower() for line in parts[-1].split(\'\\n\')}\n        except:\n            continue\n        sents,labels,summaries = [],[],[]\n        # content\n        for line in parts[1].strip().split(\'\\n\'):\n            content, label = line.split(\'\\t\\t\\t\')\n            tokens = content.strip().split()\n            for i,token in enumerate(tokens):\n                if token in entities:\n                    tokens[i] = entities[token]\n            label = \'1\' if label == \'1\' else \'0\'\n            sents.append(\' \'.join(tokens))\n            labels.append(label)\n        # summary\n        for line in parts[2].strip().split(\'\\n\'):\n            tokens = line.strip().split()\n            for i, token in enumerate(tokens):\n                if token in entities:\n                    tokens[i] = entities[token]\n            line = \' \'.join(tokens).replace(\'*\',\'\')\n            summaries.append(line)\n        ex = {\'doc\':\'\\n\'.join(sents),\'labels\':\'\\n\'.join(labels),\'summaries\':\'\\n\'.join(summaries)}\n        examples.append(ex)\n    return examples\n\ndef build_dataset(args):\n    t1 = time()\n    \n    print(\'start building dataset\')\n    if args.worker_num == 1 and cpu_count() > 1:\n        print(\'[INFO] There are %d CPUs in your device, please increase -worker_num to speed up\' % (cpu_count()))\n        print(""       It\'s a IO intensive application, so 2~10 may be a good choise"")\n\n    files = glob(args.source_dir)\n    data_num = len(files)\n    group_size = data_num // args.worker_num\n    groups = []\n    for i in range(args.worker_num):\n        if i == args.worker_num - 1:\n            groups.append(files[i*group_size : ])\n        else:\n            groups.append(files[i*group_size : (i+1)*group_size])\n    p = Pool(processes=args.worker_num)\n    multi_res = [p.apply_async(worker,(fs,)) for fs in groups]\n    res = [res.get() for res in multi_res]\n    \n    with open(args.target_dir, \'w\') as f:\n        for row in chain(*res):\n            f.write(json.dumps(row, ensure_ascii=False) + ""\\n"")\n\n    t2 = time()\n    print(\'Time Cost : %.1f seconds\' % (t2 - t1))\nif __name__ == \'__main__\':\n\n    parser = argparse.ArgumentParser()\n    \n    parser.add_argument(\'-build_vocab\',action=\'store_true\')\n    parser.add_argument(\'-embed\', type=str, default=\'data/100.w2v\')\n    parser.add_argument(\'-vocab\', type=str, default=\'data/embedding.npz\')\n    parser.add_argument(\'-word2id\',type=str,default=\'data/word2id.json\')\n\n    parser.add_argument(\'-worker_num\',type=int,default=1)\n    parser.add_argument(\'-source_dir\', type=str, default=\'data/neuralsum/dailymail/validation/*\')\n    parser.add_argument(\'-target_dir\', type=str, default=\'data/val.json\')\n\n    args = parser.parse_args()\n    \n    if args.build_vocab:\n        build_vocab(args)\n    else:\n        build_dataset(args)\n'"
models/Attention.py,10,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\n\nclass Attention(nn.Module):\n    r""""""\n    Applies an attention mechanism on the query features from the decoder.\n\n    .. math::\n            \\begin{array}{ll}\n            x = context*query \\\\\n            attn_scores = exp(x_i) / sum_j exp(x_j) \\\\\n            attn_out = attn * context\n            \\end{array}\n\n    Args:\n        dim(int): The number of expected features in the query\n\n    Inputs: query, context\n        - **query** (batch, query_len, dimensions): tensor containing the query features from the decoder.\n        - **context** (batch, input_len, dimensions): tensor containing features of the encoded input sequence.\n\n    Outputs: query, attn\n        - **query** (batch, query_len, dimensions): tensor containing the attended query features from the decoder.\n        - **attn** (batch, query_len, input_len): tensor containing attention weights.\n\n    Attributes:\n        mask (torch.Tensor, optional): applies a :math:`-inf` to the indices specified in the `Tensor`.\n\n    """"""\n    def __init__(self):\n        super(Attention, self).__init__()\n        self.mask = None\n\n    def set_mask(self, mask):\n        """"""\n        Sets indices to be masked\n\n        Args:\n            mask (torch.Tensor): tensor containing indices to be masked\n        """"""\n        self.mask = mask\n    \n    """"""\n        - query   (batch, query_len, dimensions): tensor containing the query features from the decoder.\n        - context (batch, input_len, dimensions): tensor containing features of the encoded input sequence.\n    """"""\n    def forward(self, query, context):\n        batch_size = query.size(0)\n        dim = query.size(2)\n        in_len = context.size(1)\n        # (batch, query_len, dim) * (batch, in_len, dim) -> (batch, query_len, in_len)\n        attn = torch.bmm(query, context.transpose(1, 2))\n        if self.mask is not None:\n            attn.data.masked_fill_(self.mask, -float(\'inf\'))\n        attn_scores = F.softmax(attn.view(-1, in_len),dim=1).view(batch_size, -1, in_len)\n\n        # (batch, query_len, in_len) * (batch, in_len, dim) -> (batch, query_len, dim)\n        attn_out = torch.bmm(attn_scores, context)\n\n        return attn_out, attn_scores\n\nif __name__ == \'__main__\':\n    torch.manual_seed(1)\n    attention = Attention()\n    context = Variable(torch.randn(10, 20, 4))\n    query = Variable(torch.randn(10, 1, 4))\n    query, attn = attention(query, context)\n    print(query)\n'"
models/AttnRNN.py,15,"b""#!/usr/bin/env python\n#coding:utf8\nfrom .BasicModule import BasicModule\nimport torch\nimport numpy as np\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom .Attention import Attention\nfrom torch.autograd import Variable\n\nclass AttnRNN(BasicModule):\n    def __init__(self, args, embed=None):\n        super(AttnRNN,self).__init__(args)\n        self.model_name = 'AttnRNN'\n        self.args = args\n        \n        V = args.embed_num\n        D = args.embed_dim\n        H = args.hidden_size\n        S = args.seg_num\n\n        P_V = args.pos_num\n        P_D = args.pos_dim\n        self.abs_pos_embed = nn.Embedding(P_V,P_D)\n        self.rel_pos_embed = nn.Embedding(S,P_D)\n        self.embed = nn.Embedding(V,D,padding_idx=0)\n        if embed is not None:\n            self.embed.weight.data.copy_(embed)\n\n        self.attn = Attention()\n        self.word_query = nn.Parameter(torch.randn(1,1,2*H))\n        self.sent_query = nn.Parameter(torch.randn(1,1,2*H))\n\n        self.word_RNN = nn.GRU(\n                        input_size = D,\n                        hidden_size = H,\n                        batch_first = True,\n                        bidirectional = True\n                        )\n        self.sent_RNN = nn.GRU(\n                        input_size = 2*H,\n                        hidden_size = H,\n                        batch_first = True,\n                        bidirectional = True\n                        )\n               \n        self.fc = nn.Linear(2*H,2*H)\n\n        # Parameters of Classification Layer\n        self.content = nn.Linear(2*H,1,bias=False)\n        self.salience = nn.Bilinear(2*H,2*H,1,bias=False)\n        self.novelty = nn.Bilinear(2*H,2*H,1,bias=False)\n        self.abs_pos = nn.Linear(P_D,1,bias=False)\n        self.rel_pos = nn.Linear(P_D,1,bias=False)\n        self.bias = nn.Parameter(torch.FloatTensor(1).uniform_(-0.1,0.1))\n    def forward(self,x,doc_lens):\n        N = x.size(0)\n        L = x.size(1)\n        B = len(doc_lens)\n        H = self.args.hidden_size\n        word_mask = torch.ones_like(x) - torch.sign(x)\n        word_mask = word_mask.data.type(torch.cuda.ByteTensor).view(N,1,L)\n        \n        x = self.embed(x)                                # (N,L,D)\n        x,_ = self.word_RNN(x)\n        \n        # attention\n        query = self.word_query.expand(N,-1,-1).contiguous()\n        self.attn.set_mask(word_mask)\n        word_out = self.attn(query,x)[0].squeeze(1)      # (N,2*H)\n\n        x = self.pad_doc(word_out,doc_lens)\n        # sent level GRU\n        sent_out = self.sent_RNN(x)[0]                                           # (B,max_doc_len,2*H)\n        #docs = self.avg_pool1d(sent_out,doc_lens)                               # (B,2*H)\n        max_doc_len = max(doc_lens)\n        mask = torch.ones(B,max_doc_len)\n        for i in range(B):\n            for j in range(doc_lens[i]):\n                mask[i][j] = 0\n        sent_mask = mask.type(torch.cuda.ByteTensor).view(B,1,max_doc_len)\n        \n        # attention\n        query = self.sent_query.expand(B,-1,-1).contiguous()\n        self.attn.set_mask(sent_mask)\n        docs = self.attn(query,x)[0].squeeze(1)      # (B,2*H)\n        probs = []\n        for index,doc_len in enumerate(doc_lens):\n            valid_hidden = sent_out[index,:doc_len,:]                            # (doc_len,2*H)\n            doc = F.tanh(self.fc(docs[index])).unsqueeze(0)\n            s = Variable(torch.zeros(1,2*H))\n            if self.args.device is not None:\n                s = s.cuda()\n            for position, h in enumerate(valid_hidden):\n                h = h.view(1, -1)                                                # (1,2*H)\n                # get position embeddings\n                abs_index = Variable(torch.LongTensor([[position]]))\n                if self.args.device is not None:\n                    abs_index = abs_index.cuda()\n                abs_features = self.abs_pos_embed(abs_index).squeeze(0)\n                \n                rel_index = int(round((position + 1) * 9.0 / doc_len))\n                rel_index = Variable(torch.LongTensor([[rel_index]]))\n                if self.args.device is not None:\n                    rel_index = rel_index.cuda()\n                rel_features = self.rel_pos_embed(rel_index).squeeze(0)\n                \n                # classification layer\n                content = self.content(h) \n                salience = self.salience(h,doc)\n                novelty = -1 * self.novelty(h,F.tanh(s))\n                abs_p = self.abs_pos(abs_features)\n                rel_p = self.rel_pos(rel_features)\n                prob = F.sigmoid(content + salience + novelty + abs_p + rel_p + self.bias)\n                s = s + torch.mm(prob,h)\n                #print position,F.sigmoid(abs_p + rel_p)\n                probs.append(prob)\n        return torch.cat(probs).squeeze()\n"""
models/BasicModule.py,8,"b""import torch\nfrom torch.autograd import Variable\nclass BasicModule(torch.nn.Module):\n\n    def __init__(self, args):\n        super(BasicModule,self).__init__()\n        self.args = args\n        self.model_name = str(type(self))\n\n    def pad_doc(self,words_out,doc_lens):\n        pad_dim = words_out.size(1)\n        max_doc_len = max(doc_lens)\n        sent_input = []\n        start = 0\n        for doc_len in doc_lens:\n            stop = start + doc_len\n            valid = words_out[start:stop]                                       # (doc_len,2*H)\n            start = stop\n            if doc_len == max_doc_len:\n                sent_input.append(valid.unsqueeze(0))\n            else:\n                pad = Variable(torch.zeros(max_doc_len-doc_len,pad_dim))\n                if self.args.device is not None:\n                    pad = pad.cuda()\n                sent_input.append(torch.cat([valid,pad]).unsqueeze(0))          # (1,max_len,2*H)\n        sent_input = torch.cat(sent_input,dim=0)                                # (B,max_len,2*H)\n        return sent_input\n    \n    def save(self):\n        checkpoint = {'model':self.state_dict(), 'args': self.args}\n        best_path = '%s%s_seed_%d.pt' % (self.args.save_dir,self.model_name,self.args.seed)\n        torch.save(checkpoint,best_path)\n\n        return best_path\n\n    def load(self, best_path):\n        if self.args.device is not None:\n            data = torch.load(best_path)['model']\n        else:\n            data = torch.load(best_path, map_location=lambda storage, loc: storage)['model']\n        self.load_state_dict(data)\n        if self.args.device is not None:\n            return self.cuda()\n        else:\n            return self\n"""
models/CNN_RNN.py,15,"b""from .BasicModule import BasicModule\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\n\nclass CNN_RNN(BasicModule):\n    def __init__(self, args, embed=None):\n        super(CNN_RNN,self).__init__(args)\n        self.model_name = 'CNN_RNN'\n        self.args = args\n        \n        Ks = args.kernel_sizes\n        Ci = args.embed_dim\n        Co = args.kernel_num\n        V = args.embed_num\n        D = args.embed_dim\n        H = args.hidden_size\n        S = args.seg_num\n        P_V = args.pos_num\n        P_D = args.pos_dim\n        self.abs_pos_embed = nn.Embedding(P_V,P_D)\n        self.rel_pos_embed = nn.Embedding(S,P_D)\n        self.embed = nn.Embedding(V,D,padding_idx=0)\n        if embed is not None:\n            self.embed.weight.data.copy_(embed)\n\n        self.convs = nn.ModuleList([ nn.Sequential(\n                                            nn.Conv1d(Ci,Co,K),\n                                            nn.BatchNorm1d(Co),\n                                            nn.LeakyReLU(inplace=True),\n\n                                            nn.Conv1d(Co,Co,K),\n                                            nn.BatchNorm1d(Co),\n                                            nn.LeakyReLU(inplace=True)\n                                     )\n                                    for K in Ks])\n        self.sent_RNN = nn.GRU(\n                        input_size = Co * len(Ks),\n                        hidden_size = H,\n                        batch_first = True,\n                        bidirectional = True\n                        )\n        self.fc = nn.Sequential(\n                nn.Linear(2*H,2*H),\n                nn.BatchNorm1d(2*H),\n                nn.Tanh()\n                )\n        # Parameters of Classification Layer\n        self.content = nn.Linear(2*H,1,bias=False)\n        self.salience = nn.Bilinear(2*H,2*H,1,bias=False)\n        self.novelty = nn.Bilinear(2*H,2*H,1,bias=False)\n        self.abs_pos = nn.Linear(P_D,1,bias=False)\n        self.rel_pos = nn.Linear(P_D,1,bias=False)\n        self.bias = nn.Parameter(torch.FloatTensor(1).uniform_(-0.1,0.1))\n\n    def max_pool1d(self,x,seq_lens):\n        # x:[N,L,O_in]\n        out = []\n        for index,t in enumerate(x):\n            t = t[:seq_lens[index],:]\n            t = torch.t(t).unsqueeze(0)\n            out.append(F.max_pool1d(t,t.size(2)))\n        \n        out = torch.cat(out).squeeze(2)\n        return out\n    def avg_pool1d(self,x,seq_lens):\n        # x:[N,L,O_in]\n        out = []\n        for index,t in enumerate(x):\n            t = t[:seq_lens[index],:]\n            t = torch.t(t).unsqueeze(0)\n            out.append(F.avg_pool1d(t,t.size(2)))\n        \n        out = torch.cat(out).squeeze(2)\n        return out\n    def forward(self,x,doc_lens):\n        sent_lens = torch.sum(torch.sign(x),dim=1).data \n        H = self.args.hidden_size\n        x = self.embed(x)                                                       # (N,L,D)\n        # word level GRU\n        x = [conv(x.permute(0,2,1)) for conv in self.convs]\n        x = [F.max_pool1d(i, i.size(2)).squeeze(2) for i in x]\n        x = torch.cat(x,1)\n        # make sent features(pad with zeros)\n        x = self.pad_doc(x,doc_lens)\n\n        # sent level GRU\n        sent_out = self.sent_RNN(x)[0]                                           # (B,max_doc_len,2*H)\n        docs = self.max_pool1d(sent_out,doc_lens)                                # (B,2*H)\n        docs = self.fc(docs)\n        probs = []\n        for index,doc_len in enumerate(doc_lens):\n            valid_hidden = sent_out[index,:doc_len,:]                            # (doc_len,2*H)\n            doc = docs[index].unsqueeze(0)\n            s = Variable(torch.zeros(1,2*H))\n            if self.args.device is not None:\n                s = s.cuda()\n            for position, h in enumerate(valid_hidden):\n                h = h.view(1, -1)                                                # (1,2*H)\n                # get position embeddings\n                abs_index = Variable(torch.LongTensor([[position]]))\n                if self.args.device is not None:\n                    abs_index = abs_index.cuda()\n                abs_features = self.abs_pos_embed(abs_index).squeeze(0)\n                \n                rel_index = int(round((position + 1) * 9.0 / doc_len))\n                rel_index = Variable(torch.LongTensor([[rel_index]]))\n                if self.args.device is not None:\n                    rel_index = rel_index.cuda()\n                rel_features = self.rel_pos_embed(rel_index).squeeze(0)\n                \n                # classification layer\n                content = self.content(h) \n                salience = self.salience(h,doc)\n                novelty = -1 * self.novelty(h,F.tanh(s))\n                abs_p = self.abs_pos(abs_features)\n                rel_p = self.rel_pos(rel_features)\n                prob = F.sigmoid(content + salience + novelty + abs_p + rel_p + self.bias)\n                s = s + torch.mm(prob,h)\n                probs.append(prob)\n        return torch.cat(probs).squeeze()\n"""
models/RNN_RNN.py,14,"b""from .BasicModule import BasicModule\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\n\nclass RNN_RNN(BasicModule):\n    def __init__(self, args, embed=None):\n        super(RNN_RNN, self).__init__(args)\n        self.model_name = 'RNN_RNN'\n        self.args = args\n        \n        V = args.embed_num\n        D = args.embed_dim\n        H = args.hidden_size\n        S = args.seg_num\n        P_V = args.pos_num\n        P_D = args.pos_dim\n        self.abs_pos_embed = nn.Embedding(P_V,P_D)\n        self.rel_pos_embed = nn.Embedding(S,P_D)\n        self.embed = nn.Embedding(V,D,padding_idx=0)\n        if embed is not None:\n            self.embed.weight.data.copy_(embed)\n\n        self.word_RNN = nn.GRU(\n                        input_size = D,\n                        hidden_size = H,\n                        batch_first = True,\n                        bidirectional = True\n                        )\n        self.sent_RNN = nn.GRU(\n                        input_size = 2*H,\n                        hidden_size = H,\n                        batch_first = True,\n                        bidirectional = True\n                        )\n        self.fc = nn.Linear(2*H,2*H)\n\n        # Parameters of Classification Layer\n        self.content = nn.Linear(2*H,1,bias=False)\n        self.salience = nn.Bilinear(2*H,2*H,1,bias=False)\n        self.novelty = nn.Bilinear(2*H,2*H,1,bias=False)\n        self.abs_pos = nn.Linear(P_D,1,bias=False)\n        self.rel_pos = nn.Linear(P_D,1,bias=False)\n        self.bias = nn.Parameter(torch.FloatTensor(1).uniform_(-0.1,0.1))\n\n    def max_pool1d(self,x,seq_lens):\n        # x:[N,L,O_in]\n        out = []\n        for index,t in enumerate(x):\n            t = t[:seq_lens[index],:]\n            t = torch.t(t).unsqueeze(0)\n            out.append(F.max_pool1d(t,t.size(2)))\n        \n        out = torch.cat(out).squeeze(2)\n        return out\n    def avg_pool1d(self,x,seq_lens):\n        # x:[N,L,O_in]\n        out = []\n        for index,t in enumerate(x):\n            t = t[:seq_lens[index],:]\n            t = torch.t(t).unsqueeze(0)\n            out.append(F.avg_pool1d(t,t.size(2)))\n        \n        out = torch.cat(out).squeeze(2)\n        return out\n    def forward(self,x,doc_lens):\n        sent_lens = torch.sum(torch.sign(x),dim=1).data \n        x = self.embed(x)                                                      # (N,L,D)\n        # word level GRU\n        H = self.args.hidden_size\n        x = self.word_RNN(x)[0]                                                 # (N,2*H,L)\n        #word_out = self.avg_pool1d(x,sent_lens)\n        word_out = self.max_pool1d(x,sent_lens)\n        # make sent features(pad with zeros)\n        x = self.pad_doc(word_out,doc_lens)\n\n        # sent level GRU\n        sent_out = self.sent_RNN(x)[0]                                           # (B,max_doc_len,2*H)\n        #docs = self.avg_pool1d(sent_out,doc_lens)                               # (B,2*H)\n        docs = self.max_pool1d(sent_out,doc_lens)                                # (B,2*H)\n        probs = []\n        for index,doc_len in enumerate(doc_lens):\n            valid_hidden = sent_out[index,:doc_len,:]                            # (doc_len,2*H)\n            doc = F.tanh(self.fc(docs[index])).unsqueeze(0)\n            s = Variable(torch.zeros(1,2*H))\n            if self.args.device is not None:\n                s = s.cuda()\n            for position, h in enumerate(valid_hidden):\n                h = h.view(1, -1)                                                # (1,2*H)\n                # get position embeddings\n                abs_index = Variable(torch.LongTensor([[position]]))\n                if self.args.device is not None:\n                    abs_index = abs_index.cuda()\n                abs_features = self.abs_pos_embed(abs_index).squeeze(0)\n                \n                rel_index = int(round((position + 1) * 9.0 / doc_len))\n                rel_index = Variable(torch.LongTensor([[rel_index]]))\n                if self.args.device is not None:\n                    rel_index = rel_index.cuda()\n                rel_features = self.rel_pos_embed(rel_index).squeeze(0)\n                \n                # classification layer\n                content = self.content(h) \n                salience = self.salience(h,doc)\n                novelty = -1 * self.novelty(h,F.tanh(s))\n                abs_p = self.abs_pos(abs_features)\n                rel_p = self.rel_pos(rel_features)\n                prob = F.sigmoid(content + salience + novelty + abs_p + rel_p + self.bias)\n                s = s + torch.mm(prob,h)\n                probs.append(prob)\n        return torch.cat(probs).squeeze()\n"""
models/__init__.py,0,b'from .BasicModule import BasicModule\nfrom .RNN_RNN import RNN_RNN\nfrom .CNN_RNN import CNN_RNN\nfrom .AttnRNN import AttnRNN\n'
outputs/eval.py,0,"b""#!/usr/bin/env python3\n\nimport os\nfrom pyrouge import Rouge155\n\ndef remove_broken_files():\n    error_id = []\n    for f in os.listdir('ref'):\n        try:\n            open('ref/' + f).read()\n        except:\n            error_id.append(f)\n    for f in os.listdir('hyp'):\n        try:\n            open('hyp/' + f).read()\n        except:\n            error_id.append(f)\n    error_set = set(error_id)\n    for f in error_set:\n        os.remove('ref/' + f)\n        os.remove('hyp/' + f)\n\ndef rouge():\n    r = Rouge155()\n    r.home_dir = '.'\n    r.system_dir = 'hyp'\n    r.model_dir =  'ref'\n\n    r.system_filename_pattern = '(\\d+).txt'\n    r.model_filename_pattern = '#ID#.txt'\n\n    command = '-e /YOUR/PATH/TO/ROUGE-1.5.5/data -a -c 95 -m -n 2 -b 75'\n    output = r.convert_and_evaluate(rouge_args=command)\n    print(output)\n\nif __name__ == '__main__':\n    remove_broken_files()\n    rouge()\n"""
utils/Dataset.py,2,"b""import csv\nimport torch\nimport torch.utils.data as data\nfrom torch.autograd import Variable\nfrom .Vocab import Vocab\nimport numpy as np\n\nclass Dataset(data.Dataset):\n    def __init__(self, examples):\n        super(Dataset,self).__init__()\n        # data: {'sents':xxxx,'labels':'xxxx', 'summaries':[1,0]}\n        self.examples = examples \n        self.training = False\n    def train(self):\n        self.training = True\n        return self\n    def test(self):\n        self.training = False\n        return self\n    def shuffle(self,words):\n        np.random.shuffle(words)\n        return ' '.join(words)\n    def dropout(self,words,p=0.3):\n        l = len(words)\n        drop_index = np.random.choice(l,int(l*p))\n        keep_words = [words[i] for i in range(l) if i not in drop_index]\n        return ' '.join(keep_words)\n    def __getitem__(self, idx):\n        ex = self.examples[idx]\n        return ex\n        #words = ex['sents'].split()\n        #guess = np.random.random()\n\n        #if self.training:\n        #    if guess > 0.5:\n        #        sents = self.dropout(words,p=0.3)\n        #    else:\n        #        sents = self.shuffle(words)\n        #else:\n        #    sents = ex['sents']\n        #return {'id':ex['id'],'sents':sents,'labels':ex['labels']}\n        \n    def __len__(self):\n        return len(self.examples)\n"""
utils/Vocab.py,3,"b""import torch\n\nclass Vocab():\n    def __init__(self,embed,word2id):\n        self.embed = embed\n        self.word2id = word2id\n        self.id2word = {v:k for k,v in word2id.items()}\n        assert len(self.word2id) == len(self.id2word)\n        self.PAD_IDX = 0\n        self.UNK_IDX = 1\n        self.PAD_TOKEN = 'PAD_TOKEN'\n        self.UNK_TOKEN = 'UNK_TOKEN'\n    \n    def __len__(self):\n        return len(word2id)\n\n    def i2w(self,idx):\n        return self.id2word[idx]\n    def w2i(self,w):\n        if w in self.word2id:\n            return self.word2id[w]\n        else:\n            return self.UNK_IDX\n    \n    def make_features(self,batch,sent_trunc=50,doc_trunc=100,split_token='\\n'):\n        sents_list,targets,doc_lens = [],[],[]\n        # trunc document\n        for doc,label in zip(batch['doc'],batch['labels']):\n            sents = doc.split(split_token)\n            labels = label.split(split_token)\n            labels = [int(l) for l in labels]\n            max_sent_num = min(doc_trunc,len(sents))\n            sents = sents[:max_sent_num]\n            labels = labels[:max_sent_num]\n            sents_list += sents\n            targets += labels\n            doc_lens.append(len(sents))\n        # trunc or pad sent\n        max_sent_len = 0\n        batch_sents = []\n        for sent in sents_list:\n            words = sent.split()\n            if len(words) > sent_trunc:\n                words = words[:sent_trunc]\n            max_sent_len = len(words) if len(words) > max_sent_len else max_sent_len\n            batch_sents.append(words)\n        \n        features = []\n        for sent in batch_sents:\n            feature = [self.w2i(w) for w in sent] + [self.PAD_IDX for _ in range(max_sent_len-len(sent))]\n            features.append(feature)\n        \n        features = torch.LongTensor(features)    \n        targets = torch.LongTensor(targets)\n        summaries = batch['summaries']\n\n        return features,targets,summaries,doc_lens\n\n    def make_predict_features(self, batch, sent_trunc=150, doc_trunc=100, split_token='. '):\n        sents_list, doc_lens = [],[]\n        for doc in batch:\n            sents = doc.split(split_token)\n            max_sent_num = min(doc_trunc,len(sents))\n            sents = sents[:max_sent_num]\n            sents_list += sents\n            doc_lens.append(len(sents))\n        # trunc or pad sent\n        max_sent_len = 0\n        batch_sents = []\n        for sent in sents_list:\n            words = sent.split()\n            if len(words) > sent_trunc:\n                words = words[:sent_trunc]\n            max_sent_len = len(words) if len(words) > max_sent_len else max_sent_len\n            batch_sents.append(words)\n\n        features = []\n        for sent in batch_sents:\n            feature = [self.w2i(w) for w in sent] + [self.PAD_IDX for _ in range(max_sent_len-len(sent))]\n            features.append(feature)\n\n        features = torch.LongTensor(features)\n\n        return features, doc_lens"""
utils/__init__.py,0,b'from .Dataset import Dataset\nfrom .Vocab import Vocab\n'
