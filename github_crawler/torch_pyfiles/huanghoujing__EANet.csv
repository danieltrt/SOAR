file_path,api_count,code
package/__init__.py,0,b''
package/config/__init__.py,0,b''
package/config/default.py,1,"b'""""""This is the config for GlobalPool.\nSome config may be reconfigured and new items may be added at run time.""""""\n\nfrom __future__ import print_function\nfrom easydict import EasyDict\n\ncfg = EasyDict()\n\ncfg.model = EasyDict()\n\ncfg.model.backbone = EasyDict()\ncfg.model.backbone.name = \'resnet50\'\ncfg.model.backbone.last_conv_stride = 1\ncfg.model.backbone.pretrained = True\ncfg.model.backbone.pretrained_model_dir = \'imagenet_model\'\n\ncfg.model.pool_type = \'GlobalPool\'  # [\'GlobalPool\', \'PCBPool\', \'PAPool\']\ncfg.model.max_or_avg = \'max\'\ncfg.model.em_dim = 512  #\ncfg.model.num_parts = 1  #\ncfg.model.use_ps = False  #\n\ncfg.model.ps_head = EasyDict()\ncfg.model.ps_head.mid_c = 256\ncfg.model.ps_head.num_classes = 8\n\ncfg.dataset = EasyDict()\ncfg.dataset.root = \'dataset\'\n\ncfg.dataset.im = EasyDict()\ncfg.dataset.im.h_w = (256, 128)  # final size for network input\n# https://pytorch.org/docs/master/torchvision/models.html#torchvision-models\ncfg.dataset.im.mean = [0.486, 0.459, 0.408]\ncfg.dataset.im.std = [0.229, 0.224, 0.225]\n\ncfg.dataset.use_pap_mask = False  #\ncfg.dataset.pap_mask = EasyDict()\ncfg.dataset.pap_mask.h_w = (24, 8)  # final size for masking\ncfg.dataset.pap_mask.type = \'PAP_9P\'\n\ncfg.dataset.use_ps_label = False  #\ncfg.dataset.ps_label = EasyDict()\ncfg.dataset.ps_label.h_w = (48, 16)  # final size for calculating loss\n\n# Note that cfg.dataset.train.* will not be accessed directly. Intended behavior e.g.\n#     from package.utils.cfg import transfer_items\n#     transfer_items(cfg.dataset.train, cfg.dataset)\n#     print(cfg.dataset.transform_list)\n# Similar for cfg.dataset.test.*, cfg.dataloader.train.*, cfg.dataloader.test.*\ncfg.dataset.train = EasyDict()\ncfg.dataset.train.name = \'market1501\'  # [\'market1501\', \'cuhk03_np_detected_jpg\', \'duke\']\ncfg.dataset.train.split = \'train\'  #\ncfg.dataset.train.transform_list = [\'hflip\', \'resize\']\n\ncfg.dataset.cd_train = EasyDict()\ncfg.dataset.cd_train.name = \'duke\'  #\ncfg.dataset.cd_train.split = \'train\'  #\ncfg.dataset.cd_train.transform_list = [\'hflip\', \'resize\']\n\ncfg.dataset.test = EasyDict()\ncfg.dataset.test.names = [\'market1501\', \'cuhk03_np_detected_jpg\', \'duke\']\nif hasattr(cfg.dataset.test, \'query_splits\'):\n    assert len(cfg.dataset.test.query_splits) == len(cfg.dataset.test.names), ""If cfg.dataset.test.query_splits is defined, it should be set for each test set.""\ncfg.dataset.test.transform_list = [\'resize\']\n\ncfg.dataloader = EasyDict()\ncfg.dataloader.num_workers = 2\n\ncfg.dataloader.train = EasyDict()\ncfg.dataloader.train.batch_type = \'random\'\ncfg.dataloader.train.batch_size = 32\ncfg.dataloader.train.drop_last = True\n\ncfg.dataloader.cd_train = EasyDict()\ncfg.dataloader.cd_train.batch_type = \'random\'\ncfg.dataloader.cd_train.batch_size = 32\ncfg.dataloader.cd_train.drop_last = True\n\ncfg.dataloader.test = EasyDict()\ncfg.dataloader.test.batch_type = \'seq\'\ncfg.dataloader.test.batch_size = 32\ncfg.dataloader.test.drop_last = False\n\ncfg.dataloader.pk = EasyDict()\ncfg.dataloader.pk.k = 4\n\ncfg.eval = EasyDict()\ncfg.eval.forward_type = \'reid\'\ncfg.eval.chunk_size = 1000\ncfg.eval.separate_camera_set = False\ncfg.eval.single_gallery_shot = False\ncfg.eval.first_match_break = True\ncfg.eval.score_prefix = \'\'\n\ncfg.train = EasyDict()\n\ncfg.id_loss = EasyDict()\ncfg.id_loss.name = \'idL\'\ncfg.id_loss.weight = 1  #\ncfg.id_loss.use = cfg.id_loss.weight > 0\n\ncfg.tri_loss = EasyDict()\ncfg.tri_loss.name = \'triL\'\ncfg.tri_loss.weight = 0  #\ncfg.tri_loss.use = cfg.tri_loss.weight > 0\ncfg.tri_loss.margin = 0.3\ncfg.tri_loss.dist_type = \'euclidean\'\ncfg.tri_loss.hard_type = \'tri_hard\'\ncfg.tri_loss.norm_by_num_of_effective_triplets = False\n\n# source domain ps loss\ncfg.src_ps_loss = EasyDict()\ncfg.src_ps_loss.name = \'psL\'\ncfg.src_ps_loss.weight = 0  #\ncfg.src_ps_loss.use = cfg.src_ps_loss.weight > 0\ncfg.src_ps_loss.normalize_size = True\ncfg.src_ps_loss.num_classes = cfg.model.ps_head.num_classes\n\n# cross-domain (COCO/target domain) ps loss\ncfg.cd_ps_loss = EasyDict()\ncfg.cd_ps_loss.name = \'cd_psL\'\ncfg.cd_ps_loss.weight = 0  #\ncfg.cd_ps_loss.use = cfg.cd_ps_loss.weight > 0\ncfg.cd_ps_loss.normalize_size = True\ncfg.cd_ps_loss.num_classes = cfg.model.ps_head.num_classes\n\ncfg.log = EasyDict()\ncfg.log.use_tensorboard = True\n\ncfg.optim = EasyDict()\ncfg.optim.optimizer = \'sgd\'\n\ncfg.optim.sgd = EasyDict()\ncfg.optim.sgd.momentum = 0.9\ncfg.optim.sgd.nesterov = False\n\ncfg.optim.weight_decay = 5e-4\ncfg.optim.ft_lr = 0.01  #\ncfg.optim.new_params_lr = 0.02  #\ncfg.optim.lr_decay_epochs = (25, 50)  #\ncfg.optim.normal_epochs = 60  # Not including warmup/pretrain\ncfg.optim.warmup_epochs = 0\ncfg.optim.warmup = cfg.optim.warmup_epochs > 0\ncfg.optim.warmup_init_lr = 0\ncfg.optim.pretrain_new_params_epochs = 0  #\ncfg.optim.pretrain_new_params = cfg.optim.pretrain_new_params_epochs > 0\ncfg.optim.epochs_per_val = 5\ncfg.optim.steps_per_log = 50\ncfg.optim.trial_run = False  #\ncfg.optim.phase = \'normal\'  # [pretrain, warmup, normal], may be re-configured in code\ncfg.optim.resume = False\ncfg.optim.cft = False  #\ncfg.optim.cft_iters = 1  #\ncfg.optim.cft_rho = 8e-4\ncfg.only_test = False  #\ncfg.only_infer = False  #'"
package/data/__init__.py,0,b''
package/data/create_dataset.py,0,"b""from .datasets.market1501 import Market1501\nfrom .datasets.cuhk03_np_detected_png import CUHK03NpDetectedPng\nfrom .datasets.cuhk03_np_detected_jpg import CUHK03NpDetectedJpg\nfrom .datasets.duke import DukeMTMCreID\nfrom .datasets.coco import COCO\nfrom .datasets.msmt17 import MSMT17\nfrom .datasets.partial_reid import PartialREID\nfrom .datasets.partial_ilids import PartialiLIDs\n\n\n__factory = {\n        'market1501': Market1501,\n        'cuhk03_np_detected_png': CUHK03NpDetectedPng,\n        'cuhk03_np_detected_jpg': CUHK03NpDetectedJpg,\n        'duke': DukeMTMCreID,\n        'coco': COCO,\n        'msmt17': MSMT17,\n        'partial_reid': PartialREID,\n        'partial_ilids': PartialiLIDs,\n    }\n\n\ndataset_shortcut = {\n    'market1501': 'M',\n    'cuhk03_np_detected_png': 'C',\n    'cuhk03_np_detected_jpg': 'C',\n    'duke': 'D',\n    'msmt17': 'MS',\n    'partial_reid': 'PR',\n    'partial_ilids': 'PI',\n}\n\n\ndef create_dataset(cfg, samples=None):\n    return __factory[cfg.name](cfg, samples=samples)\n"""
package/data/dataloader.py,2,"b""from torch.utils.data import DataLoader as TorchDataLoader\nfrom torch.utils.data import SequentialSampler, RandomSampler\nfrom .create_dataset import create_dataset\nfrom .random_identity_sampler import RandomIdentitySampler\n\n\ndef create_dataloader(cfg, dataset_cfg, samples=None):\n    dataset = create_dataset(dataset_cfg, samples=samples)\n    if cfg.batch_type == 'seq':\n        sampler = SequentialSampler(dataset)\n    elif cfg.batch_type == 'random':\n        sampler = RandomSampler(dataset)\n    elif cfg.batch_type == 'pk':\n        sampler = RandomIdentitySampler(dataset, cfg.pk.k)\n    else:\n        raise NotImplementedError\n    loader = TorchDataLoader(\n        dataset,\n        batch_size=cfg.batch_size,\n        sampler=sampler,\n        num_workers=cfg.num_workers,\n        pin_memory=True,\n        drop_last=cfg.drop_last,\n    )\n    return loader\n"""
package/data/dataset.py,1,"b'from __future__ import print_function\nimport os.path as osp\nfrom PIL import Image\nimport numpy as np\nfrom copy import deepcopy\nfrom torch.utils.data import Dataset as TorchDataset\nfrom .transform import transform\nfrom ..utils.file import load_pickle\n\n\nclass Dataset(TorchDataset):\n    """"""Args:\n        samples: None or a list of dicts; samples[i] has key \'im_path\' and optional \'label\', \'cam\'.\n    """"""\n    has_pap_mask = None\n    has_ps_label = None\n    im_root = None\n    split_spec = None\n\n    def __init__(self, cfg, samples=None):\n        self.cfg = cfg\n        self.root = osp.join(cfg.root, cfg.name)\n        if samples is None:\n            self.samples = self.load_split()\n        else:\n            self.samples = samples\n            cfg.split = \'None\'\n        print(self.summary)\n\n    def __len__(self):\n        return len(self.samples)\n\n    def __getitem__(self, index):\n        cfg = self.cfg\n        # Deepcopy to inherit all meta items\n        sample = deepcopy(self.samples[index])\n        im_path = sample[\'im_path\']\n        sample[\'im\'] = self.get_im(im_path)\n        # TODO: Precisely set use_pap_mask and use_ps_label at outside scope at run time\n        if self.has_pap_mask and cfg.use_pap_mask:\n            sample[\'pap_mask\'] = self.get_pap_mask(im_path)\n        if self.has_ps_label and cfg.use_ps_label:\n            sample[\'ps_label\'] = self.get_ps_label(im_path)\n        transform(sample, cfg)\n        return sample\n\n    def save_split(self, spec, save_path):\n        raise NotImplementedError\n\n    def load_split(self):\n        cfg = self.cfg\n        save_path = osp.join(self.root, cfg.split + \'.pkl\')\n        self.save_split(self.split_spec[cfg.split], save_path)\n        samples = load_pickle(save_path)\n        return samples\n\n    def get_im(self, im_path):\n        return Image.open(osp.join(self.root, im_path)).convert(""RGB"")\n\n    def _get_kpt_key(self, im_path):\n        raise NotImplementedError\n\n    def get_pap_mask(self, im_path):\n        # Place here to avoid circular import, since\n        #     dataset.py imports kpt_to_pap_mask.py,\n        #     kpt_to_pap_mask.py imports coco.py,\n        #     coco.py imports dataset.py\n        # About circular import, this is an excellent tutorial: https://stackabuse.com/python-circular-imports\n        from .kpt_to_pap_mask import gen_pap_masks\n        cfg = self.cfg\n        if not hasattr(self, \'im_path_to_kpt\'):\n            self.im_path_to_kpt = load_pickle(osp.join(self.root, \'im_path_to_kpt.pkl\'))\n        key = self._get_kpt_key(im_path)\n        kpt = self.im_path_to_kpt[key][\'kpt\']\n        kpt[:, 2] = (kpt[:, 2] > 0.1).astype(np.float)\n        pap_mask, _ = gen_pap_masks(self.im_path_to_kpt[key][\'im_h_w\'], cfg.pap_mask.h_w, kpt, mask_type=cfg.pap_mask.type)\n        return pap_mask\n\n    def _get_ps_label_path(self, im_path):\n        raise NotImplementedError\n\n    def get_ps_label(self, im_path):\n        cfg = self.cfg\n        ps_label = Image.open(self._get_ps_label_path(im_path))\n        return ps_label\n\n    # Use property (instead of setting it in self.__init__) in case self.samples is changed after initialization.\n    @property\n    def num_samples(self):\n        return len(self.samples)\n\n    @property\n    def num_ids(self):\n        return len(set([s[\'label\'] for s in self.samples])) if \'label\' in self.samples[0] else -1\n\n    @property\n    def num_cams(self):\n        return len(set([s[\'cam\'] for s in self.samples])) if \'cam\' in self.samples[0] else -1\n\n    @property\n    def summary(self):\n        summary = [\'=\' * 25]\n        summary += [self.__class__.__name__]\n        summary += [\'=\' * 25]\n        summary += [\'   split: {}\'.format(self.cfg.split)]\n        summary += [\'# images: {}\'.format(self.num_samples)]\n        summary += [\'   # ids: {}\'.format(self.num_ids)]\n        summary += [\'  # cams: {}\'.format(self.num_cams)]\n        summary = \'\\n\'.join(summary) + \'\\n\'\n        return summary\n'"
package/data/kpt_to_pap_mask.py,0,"b'from __future__ import print_function\nimport numpy as np\nfrom .datasets.coco import kpt_name_to_ind\n\n""""""\nSome naming shortcuts:\n    kpt: keypoint\n\nVisibility:\n    0: invisible\n    1: visible\n""""""\n\n\ndef fuse_y(kpts, l_name, r_name, fuse=np.mean):\n    l_kpt = kpts[kpt_name_to_ind[l_name]]\n    r_kpt = kpts[kpt_name_to_ind[r_name]]\n    if (l_kpt[2] == 1) and (r_kpt[2] == 1):\n        y = fuse([l_kpt[1], r_kpt[1]])\n        visible = True\n    elif (l_kpt[2] == 0) and (r_kpt[2] == 1):\n        y = r_kpt[1]\n        visible = True\n    elif (l_kpt[2] == 1) and (r_kpt[2] == 0):\n        y = l_kpt[1]\n        visible = True\n    else:\n        y = 0\n        visible = False\n    return y, visible\n\n\nPARTS_DICT = {\n    \'PAP_6P\': (\'HEAD\', \'UPPER_TORSO\', \'LOWER_TORSO\', \'UPPER_LEG\', \'LOWER_LEG\', \'SHOES\'),\n    \'PAP_9P\': (\'HEAD\', \'UPPER_TORSO\', \'LOWER_TORSO\', \'UPPER_LEG\', \'LOWER_LEG\', \'SHOES\', \'UPPER_HALF\', \'LOWER_HALF\', \'WHOLE\'),\n}\n# When training with random part occlusion, don\'t occlude these parts.\nPARTS_DONT_DROP_DICT = {\n    \'PAP_6P\': [],\n    \'PAP_9P\': [\'WHOLE\', ],\n}\n\n\ndef gen_pap_masks(im_h_w, h_w, kpts, mask_type=\'PAP_9P\'):\n    """"""Generate pap masks for one image.\n    Args:\n        im_h_w: size of image\n        h_w: size of pap masks\n        kpts: np array or a list with shape [num_kpts, 3], kpts[i] is (x, y, visibility)\n    Returns:\n        masks: numpy array (float32) with shape [num_masks, h, w]\n        visible: numpy array (float32) with shape [num_masks]. If some keypoints are invisible, related parts may be unavailable.\n    """"""\n    parts = PARTS_DICT[mask_type]\n    kpts = np.array(kpts)\n    assert len(kpts.shape) == 2\n    assert kpts.shape[1] == 3\n    P = kpts.shape[0]\n    H, W = h_w\n    masks = []\n    visible = []\n\n    def _to_ind(y):\n        return min(H, max(0, int(1. * H * y / im_h_w[0] + 0.5)))\n\n    def _to_ind_x(x):\n        return min(W, max(0, int(1. * W * x / im_h_w[1] + 0.5)))\n\n    def _gen_mask(y1, y2, v, part_name=\'\', debug=False):\n        if debug and v and y1 >= y2:\n            print(\'[Warning][{:15}] y1 {:2d}, y2 {:2d}, v {}\'.format(part_name, y1, y2, v))\n        v = v and (y1 < y2)\n        m = np.zeros([H, W], dtype=np.float32)\n        # Return 0 mask if invisible\n        if v:\n            m[y1:y2] = 1\n        masks.append(m)\n        visible.append(v)\n        return m, v\n\n    # Exclude upper background\n    def _gen_HEAD_mask():\n        return _gen_mask(max(0, shoulder_y - int(np.ceil(H * 0.25))), shoulder_y, shoulder_v, part_name=\'HEAD\')\n\n    def _gen_UPPER_TORSO_mask():\n        return _gen_mask(shoulder_y, shoulder_hip_mid_y, shoulder_hip_mid_v, part_name=\'UPPER_TORSO\')\n\n    def _gen_LOWER_TORSO_mask():\n        return _gen_mask(shoulder_hip_mid_y, hip_y, shoulder_hip_mid_v, part_name=\'LOWER_TORSO\')\n\n    def _gen_UPPER_LEG_mask():\n        return _gen_mask(hip_y, knee_y, hip_v and knee_v, part_name=\'UPPER_LEG\')\n\n    # Here LOWER_LEG is without shoes\n    def _gen_LOWER_LEG_mask():\n        if knee_v and ankle_v:\n            return _gen_mask(knee_y, ankle_y, True, part_name=\'LOWER_LEG\')\n        elif knee_v:\n            return _gen_mask(knee_y, min(H, knee_y + int(np.ceil(H * 0.25))), True, part_name=\'LOWER_LEG\')\n        else:\n            return _gen_mask(0, H, False, part_name=\'LOWER_LEG\')\n\n    def _gen_circle_mask(h_w, pt, radius):\n        """"""A circle region.\n        pt: head center point\n        """"""\n        h, w = h_w\n        x, y = pt\n        xv, yv = np.arange(w)[np.newaxis, :], np.arange(h)[:, np.newaxis]\n        return ((xv - x) ** 2 + (yv - y) ** 2 <= radius ** 2).astype(np.float32)\n\n    def _gen_SHOES_mask():\n        radius = int(0.5 * H / 6)\n        left_ankle_kpt = kpts[kpt_name_to_ind[\'left_ankle\']]\n        left_shoes_mask = _gen_circle_mask((H, W), (_to_ind_x(left_ankle_kpt[0]), _to_ind(left_ankle_kpt[1])), radius) if left_ankle_kpt[2] == 1 else np.zeros([H, W], dtype=np.float32)\n        right_ankle_kpt = kpts[kpt_name_to_ind[\'right_ankle\']]\n        right_shoes_mask = _gen_circle_mask((H, W), (_to_ind_x(right_ankle_kpt[0]), _to_ind(right_ankle_kpt[1])), radius) if right_ankle_kpt[2] == 1 else np.zeros([H, W], dtype=np.float32)\n        shoes_mask = np.logical_or(left_shoes_mask, right_shoes_mask).astype(np.float32)\n        masks.append(shoes_mask)\n        visible.append(ankle_v)\n        return shoes_mask, ankle_v\n\n    def _gen_UPPER_HALF_mask():\n        return _gen_mask(0, hip_y, hip_v, part_name=\'UPPER_HALF\')\n\n    # If knee and ankle are both invisible, view lower half as invisible\n    def _gen_LOWER_HALF_mask():\n        return _gen_mask(hip_y, H, hip_v and (knee_v or ankle_v), part_name=\'LOWER_HALF\')\n\n    def _gen_WHOLE_mask():\n        return _gen_mask(0, H, True, part_name=\'WHOLE\')\n\n    shoulder_y, shoulder_v = fuse_y(kpts, \'left_shoulder\', \'right_shoulder\')\n    hip_y, hip_v = fuse_y(kpts, \'left_hip\', \'right_hip\')\n    knee_y, knee_v = fuse_y(kpts, \'left_knee\', \'right_knee\')\n    ankle_y, ankle_v = fuse_y(kpts, \'left_ankle\', \'right_ankle\')\n    shoulder_hip_mid_y, shoulder_hip_mid_v = (shoulder_y + hip_y) * 0.5, shoulder_v and hip_v\n    shoulder_y, hip_y, knee_y, shoulder_hip_mid_y, ankle_y = _to_ind(shoulder_y), _to_ind(hip_y), _to_ind(knee_y), _to_ind(shoulder_hip_mid_y), _to_ind(ankle_y)\n    for p in parts:\n        eval(\'_gen_{}_mask()\'.format(p))\n    masks = np.array(masks, dtype=np.float32)\n    visible = np.array(visible, dtype=np.float32)\n    return masks, visible\n'"
package/data/multitask_dataloader.py,2,"b'""""""Refer to `torch.utils.data.dataloader`, there is a `_DataLoaderIter` and a\n`DataLoader` class. To combine multi dataloaders for multi-task learning, and\nto mimic the `for batch in dataloader:` syntax, we should also implement two\nclasses `_MTDataLoaderIter` and `MTDataLoader`.""""""\n\nfrom __future__ import print_function\n\n\nclass _MTDataLoaderIter(object):\n    """"""mt_loader: a MTDataLoader object""""""\n    def __init__(self, mt_loader):\n        self.mt_loader = mt_loader\n        self.loader_iters = [iter(loader) for loader in self.mt_loader.loaders]\n\n    def __iter__(self):\n        return self\n\n    def __next__(self):\n        if self.mt_loader.ref_shortest_loader:\n            # When the shortest loader (the one with minimum number of batches)\n            # terminates, this iterator will terminates.\n            # The `StopIteration` raised inside that shortest loader\'s `__next__`\n            # method will in turn gets out of this `__next__` method.\n            batches = [loader_iter.next() for loader_iter in self.loader_iters]\n        else:\n            batches = []\n            for idx, loader_iter in enumerate(self.loader_iters):\n                try:\n                    batch = loader_iter.next()\n                except StopIteration:\n                    # If it\'s the specified loader terminates, we terminate\n                    if idx == self.mt_loader.ref_loader_idx:\n                        raise StopIteration\n                    # Otherwise, start a new epoch for the current loader\n                    self.loader_iters[idx] = iter(self.mt_loader.loaders[idx])\n                    batch = self.loader_iters[idx].next()\n                batches.append(batch)\n        return self.mt_loader.combine_batch(batches)\n\n    # Python 2 compatibility\n    next = __next__\n\n    def __len__(self):\n        return len(self.mt_loader)\n\n\nclass MTDataLoader(object):\n    """"""Different dataloaders have different lengths, the length of the multi-task\n    loader (i.e. when to terminate) can be defined by the shortest loader, or by\n    the specified loader.\n    Args:\n        loaders: a list/tuple of pytorch DataLoader objects\n    """"""\n\n    def __init__(self, loaders, ref_shortest_loader=False, ref_loader_idx=0):\n        assert ref_loader_idx in range(len(loaders))\n        self.loaders = loaders\n        self.ref_shortest_loader = ref_shortest_loader\n        self.ref_loader_idx = ref_loader_idx\n\n    def __iter__(self):\n        return _MTDataLoaderIter(self)\n\n    def __len__(self):\n        lengths = [len(loader) for loader in self.loaders]\n        return min(lengths) if self.ref_shortest_loader else lengths[self.ref_loader_idx]\n\n    # Customize the behavior of combining batches here.\n    def combine_batch(self, batches):\n        return batches\n\n\nif __name__ == \'__main__\':\n    import torchvision\n    from torch.utils.data import DataLoader\n    from torchvision import transforms\n\n    loader1 = DataLoader(\n        torchvision.datasets.FakeData(\n            size=100,\n            image_size=(3, 8, 8),\n            num_classes=10,\n            transform=transforms.Compose([\n                transforms.ToTensor(),\n            ])\n        ),\n        batch_size=4,\n        shuffle=True\n    )\n\n    loader2 = DataLoader(\n        torchvision.datasets.FakeData(\n            size=80,\n            image_size=(3, 8, 8),\n            num_classes=10,\n            transform=transforms.Compose([\n                transforms.ToTensor(),\n            ])\n        ),\n        batch_size=8,\n        shuffle=True\n    )\n\n    loader3 = DataLoader(\n        torchvision.datasets.FakeData(\n            size=40,\n            image_size=(3, 8, 8),\n            num_classes=10,\n            transform=transforms.Compose([\n                transforms.ToTensor(),\n            ])\n        ),\n        batch_size=2,\n        shuffle=True\n    )\n\n    def loop_through_mt_loader(loader):\n        """"""A use case of iterating through mt_loader.""""""\n        for i, batches in enumerate(loader):\n            # print some info\n            if i in [0, 1]:\n                for j, b in enumerate(batches):\n                    data, target = b\n                    print(\'MT Batch Idx {}, Sub Batch Idx {}, Data Type {}, Size {}, Target Type {}, Size {}\'.format(i, j, data.dtype, data.size(), target.dtype, target.size()))\n        print(\'NO. MT Batches At Termination: {}\'.format(i + 1))\n\n    print(\'{}\\n MTDataLoader Length Refer to Shortest Loader\\n{}\'.format(\'*\' * 80, \'*\' * 80))\n    mt_loader = MTDataLoader([loader1, loader2, loader3], ref_shortest_loader=True)\n    for i in range(3):\n        print(\'{}\\nLoop Through MTDataLoader #{}\\n{}\'.format(\'-\' * 40, i + 1, \'-\' * 40))\n        loop_through_mt_loader(mt_loader)\n    print(\'Each Loader Length:\', \', \'.join([str(len(l)) for l in mt_loader.loaders]))\n    print(\'MTDataLoader Length:\', len(mt_loader))\n\n    ref_loader_idx = 0\n    print(\'{}\\n MTDataLoader Length Refer to Loader idx {} \\n{}\'.format(\'*\' * 80, ref_loader_idx, \'*\' * 80))\n    mt_loader = MTDataLoader([loader1, loader2, loader3], ref_shortest_loader=False, ref_loader_idx=ref_loader_idx)\n    for i in range(3):\n        print(\'{}\\nLoop Through MTDataLoader #{}\\n{}\'.format(\'-\' * 40, i + 1, \'-\' * 40))\n        loop_through_mt_loader(mt_loader)\n    print(\'Each Loader Length:\', \', \'.join([str(len(l)) for l in mt_loader.loaders]))\n    print(\'MTDataLoader Length:\', len(mt_loader))\n\n    ref_loader_idx = 2\n    print(\'{}\\n MTDataLoader Length Refer to Loader idx {} \\n{}\'.format(\'*\' * 80, ref_loader_idx, \'*\' * 80))\n    mt_loader = MTDataLoader([loader1, loader2, loader3], ref_shortest_loader=False, ref_loader_idx=ref_loader_idx)\n    for i in range(3):\n        print(\'{}\\nLoop Through MTDataLoader #{}\\n{}\'.format(\'-\' * 40, i + 1, \'-\' * 40))\n        loop_through_mt_loader(mt_loader)\n    print(\'Each Loader Length:\', \', \'.join([str(len(l)) for l in mt_loader.loaders]))\n    print(\'MTDataLoader Length:\', len(mt_loader))\n'"
package/data/random_identity_sampler.py,2,"b'from __future__ import absolute_import\nfrom collections import defaultdict\n\nimport numpy as np\nimport torch\nfrom torch.utils.data import Sampler\n\n\nclass RandomIdentitySampler(Sampler):\n    """"""Modified from https://github.com/Cysu/open-reid/blob/master/reid/utils/data/sampler.py""""""\n    def __init__(self, data_source, k=1):\n        self.data_source = data_source\n        self.k = k\n        self.index_dic = defaultdict(list)\n        for index, sample in enumerate(data_source):\n            self.index_dic[sample[\'label\']].append(index)\n        self.pids = list(self.index_dic.keys())\n        self.num_pids = len(self.pids)\n\n    def __len__(self):\n        return self.num_pids * self.k\n\n    def __iter__(self):\n        indices = torch.randperm(self.num_pids)\n        ret = []\n        for i in indices:\n            pid = self.pids[i]\n            t = self.index_dic[pid]\n            if len(t) >= self.k:\n                t = np.random.choice(t, size=self.k, replace=False)\n            else:\n                t = np.random.choice(t, size=self.k, replace=True)\n            ret.extend(t)\n        return iter(ret)\n'"
package/data/transform.py,2,"b'import torch\nimport torchvision.transforms.functional as F\nimport random\nimport numpy as np\nfrom PIL import Image\nimport cv2\n\n""""""We expect a list `cfg.transform_list`. The types specified in this list \nwill be applied sequentially. Each type name corresponds to a function name in \nthis file, so you have to implement the function w.r.t. your custom type. \nThe function head should be `FUNC_NAME(in_dict, cfg)`, and it should modify `in_dict`\nin place.\nThe transform list allows us to apply optional transforms in any order, while custom\nfunctions allow us to perform sync transformation for images and all labels.\n""""""\n\n\ndef hflip(in_dict, cfg):\n    # Tricky!! random.random() can not reproduce the score of np.random.random(),\n    # dropping ~1% for both Market1501 and Duke GlobalPool.\n    # if random.random() < 0.5:\n    if np.random.random() < 0.5:\n        in_dict[\'im\'] = F.hflip(in_dict[\'im\'])\n        if \'pap_mask\' in in_dict:\n            in_dict[\'pap_mask\'] = in_dict[\'pap_mask\'][:, :, ::-1]\n        if \'ps_label\' in in_dict:\n            in_dict[\'ps_label\'] = F.hflip(in_dict[\'ps_label\'])\n\n\ndef resize_3d_np_array(maps, resize_h_w, interpolation):\n    """"""maps: np array with shape [C, H, W], dtype is not restricted""""""\n    return np.stack([cv2.resize(m, tuple(resize_h_w[::-1]), interpolation=interpolation) for m in maps])\n\n\n# def may_resize(im, resize_h_w, interpolation):\n#     """"""im: PIL image""""""\n#     resize_w_h = tuple(resize_h_w[::-1])\n#     if im.size != resize_w_h:\n#         # Refer to\n#         #     from PIL import Image\n#         #     Image.Image.resize()\n#         im = im.resize(resize_w_h, resample=interpolation)\n#     return im\n\n\n# Resize image using PIL.Image.Image.resize()\n# def resize(in_dict, cfg):\n#     in_dict[\'im\'] = may_resize(in_dict[\'im\'], cfg.im.h_w, Image.BILINEAR)\n#     if \'pap_mask\' in in_dict:\n#         in_dict[\'pap_mask\'] = resize_3d_np_array(in_dict[\'pap_mask\'], cfg.pap_mask.h_w, cv2.INTER_NEAREST)\n#     if \'ps_label\' in in_dict:\n#         in_dict[\'ps_label\'] = may_resize(in_dict[\'ps_label\'], cfg.ps_label.h_w, Image.NEAREST)\n\n\n# Resize image using cv2.resize()\ndef resize(in_dict, cfg):\n    in_dict[\'im\'] = Image.fromarray(cv2.resize(np.array(in_dict[\'im\']), tuple(cfg.im.h_w[::-1]), interpolation=cv2.INTER_LINEAR))\n    if \'pap_mask\' in in_dict:\n        in_dict[\'pap_mask\'] = resize_3d_np_array(in_dict[\'pap_mask\'], cfg.pap_mask.h_w, cv2.INTER_NEAREST)\n    if \'ps_label\' in in_dict:\n        in_dict[\'ps_label\'] = Image.fromarray(cv2.resize(np.array(in_dict[\'ps_label\']), tuple(cfg.ps_label.h_w[::-1]), cv2.INTER_NEAREST), mode=\'L\')\n\n\ndef to_tensor(in_dict, cfg):\n    in_dict[\'im\'] = F.to_tensor(in_dict[\'im\'])\n    in_dict[\'im\'] = F.normalize(in_dict[\'im\'], cfg.im.mean, cfg.im.std)\n    if \'pap_mask\' in in_dict:\n        in_dict[\'pap_mask\'] = torch.from_numpy(in_dict[\'pap_mask\']).float()\n    if \'ps_label\' in in_dict:\n        in_dict[\'ps_label\'] = torch.from_numpy(np.array(in_dict[\'ps_label\'])).long()\n\n\ndef transform(in_dict, cfg):\n    for t in cfg.transform_list:\n        eval(\'{}(in_dict, cfg)\'.format(t))\n    to_tensor(in_dict, cfg)\n    return in_dict\n'"
package/eval/__init__.py,0,b''
package/eval/eval_dataloader.py,0,"b'from __future__ import print_function\nfrom .extract_feat import extract_dataloader_feat\nfrom .eval_feat import eval_feat\n\n\ndef _print_stat(dic):\n    print(\'=> Eval Statistics:\')\n    print(\'\\tdic.keys():\', dic.keys())\n    print(""\\tdic[\'q_feat\'].shape:"", dic[\'q_feat\'].shape)\n    print(""\\tdic[\'q_label\'].shape:"", dic[\'q_label\'].shape)\n    print(""\\tdic[\'q_cam\'].shape:"", dic[\'q_cam\'].shape)\n    print(""\\tdic[\'g_feat\'].shape:"", dic[\'g_feat\'].shape)\n    print(""\\tdic[\'g_label\'].shape:"", dic[\'g_label\'].shape)\n    print(""\\tdic[\'g_cam\'].shape:"", dic[\'g_cam\'].shape)\n\n\ndef eval_dataloader(model, q_loader, g_loader, cfg):\n    q_feat_dict = extract_dataloader_feat(model, q_loader, cfg)\n    g_feat_dict = extract_dataloader_feat(model, g_loader, cfg)\n    dic = {\n        \'q_feat\': q_feat_dict[\'feat\'],\n        \'q_label\': q_feat_dict[\'label\'],\n        \'q_cam\': q_feat_dict[\'cam\'],\n        \'g_feat\': g_feat_dict[\'feat\'],\n        \'g_label\': g_feat_dict[\'label\'],\n        \'g_cam\': g_feat_dict[\'cam\'],\n    }\n    if \'visible\' in q_feat_dict:\n        dic[\'q_visible\'] = q_feat_dict[\'visible\']\n    if \'visible\' in g_feat_dict:\n        dic[\'g_visible\'] = g_feat_dict[\'visible\']\n    _print_stat(dic)\n    return eval_feat(dic, cfg)\n'"
package/eval/eval_feat.py,0,"b""from __future__ import print_function\nimport numpy as np\nfrom .metric import cmc, mean_ap\nfrom .np_distance import compute_dist, compute_dist_with_visibility\nfrom ..utils.log import score_str\n\n\ndef get_scores_str(mAP, cmc_scores, score_prefix):\n    return score_prefix + '[mAP: {}], [cmc1: {}], [cmc5: {}], [cmc10: {}]'.format(\n        score_str(mAP), score_str(cmc_scores[0]), score_str(cmc_scores[4]), score_str(cmc_scores[9]))\n\n\ndef eval_feat(dic, cfg):\n    mAP = 0\n    cmc_scores = 0\n    num = 0\n    # Split query set into chunks, so that computing distance matrix does not run out of memory.\n    chunk_size = cfg.chunk_size\n    num_chunks = int(np.ceil(1. * len(dic['q_feat']) / chunk_size))\n    for i in range(num_chunks):\n        st_ind = i * chunk_size\n        end_ind = min(i * chunk_size + chunk_size, len(dic['q_feat']))\n        if 'q_visible' in dic:\n            dist_mat = compute_dist_with_visibility(dic['q_feat'][st_ind:end_ind], dic['g_feat'], dic['q_visible'][st_ind:end_ind], dic['g_visible'])\n        else:\n            dist_mat = compute_dist(dic['q_feat'][st_ind:end_ind], dic['g_feat'])\n        # Modifying distance matrix may be used in the future.\n        if 'add_dist_mat' in dic:\n            add_dist_mat_ = dic['add_dist_mat'][st_ind:end_ind]\n            assert add_dist_mat_.shape == dist_mat.shape\n            dist_mat += add_dist_mat_\n        input_kwargs = dict(\n            distmat=dist_mat,\n            query_ids=dic['q_label'][st_ind:end_ind],\n            gallery_ids=dic['g_label'],\n            query_cams=dic['q_cam'][st_ind:end_ind],\n            gallery_cams=dic['g_cam'],\n        )\n        # Compute mean AP\n        mAP_ = mean_ap(**input_kwargs)\n        # Compute CMC scores\n        cmc_scores_ = cmc(\n            separate_camera_set=cfg.separate_camera_set,\n            single_gallery_shot=cfg.single_gallery_shot,\n            first_match_break=cfg.first_match_break,\n            topk=10,\n            **input_kwargs\n        )\n        n = end_ind - st_ind\n        mAP += mAP_ * n\n        cmc_scores += cmc_scores_ * n\n        num += n\n    mAP /= num\n    cmc_scores /= num\n    scores_str = get_scores_str(mAP, cmc_scores, cfg.score_prefix)\n    print(scores_str)\n    return {\n        'mAP': mAP,\n        'cmc_scores': cmc_scores,\n        'scores_str': scores_str,\n    }\n"""
package/eval/extract_feat.py,2,"b""from __future__ import print_function\nfrom tqdm import tqdm\nimport torch\nfrom .torch_distance import normalize\nfrom ..utils.misc import concat_dict_list\nfrom ..utils.torch_utils import recursive_to_device\n\n\ndef extract_batch_feat(model, in_dict, cfg):\n    model.eval()\n    with torch.no_grad():\n        in_dict = recursive_to_device(in_dict, cfg.device)\n        out_dict = model(in_dict, forward_type=cfg.forward_type)\n        out_dict['feat_list'] = [normalize(f) for f in out_dict['feat_list']]\n        feat = torch.cat(out_dict['feat_list'], 1)\n        feat = feat.cpu().numpy()\n        ret_dict = {\n            'im_path': in_dict['im_path'],\n            'feat': feat,\n        }\n        if 'label' in in_dict:\n            ret_dict['label'] = in_dict['label'].cpu().numpy()\n        if 'cam' in in_dict:\n            ret_dict['cam'] = in_dict['cam'].cpu().numpy()\n        if 'visible' in out_dict:\n            ret_dict['visible'] = out_dict['visible'].cpu().numpy()\n    return ret_dict\n\n\ndef extract_dataloader_feat(model, loader, cfg):\n    dict_list = []\n    for batch in tqdm(loader, desc='Extract Feature', miniters=20, ncols=120, unit=' batches'):\n        feat_dict = extract_batch_feat(model, batch, cfg)\n        dict_list.append(feat_dict)\n    ret_dict = concat_dict_list(dict_list)\n    return ret_dict\n"""
package/eval/metric.py,0,"b'""""""Modified from Tong Xiao\'s open-reid (https://github.com/Cysu/open-reid) \nreid/evaluation_metrics/ranking.py. Modifications: \n1) Only accepts numpy data input, no torch is involved.\n1) Here results of each query can be returned.\n2) In the single-gallery-shot evaluation case, the time of repeats is changed \n   from 10 to 100.\n""""""\nfrom __future__ import absolute_import\nfrom collections import defaultdict\n\nimport numpy as np\nfrom sklearn.metrics import average_precision_score\n\n\ndef _unique_sample(ids_dict, num):\n    mask = np.zeros(num, dtype=np.bool)\n    for _, indices in ids_dict.items():\n        i = np.random.choice(indices)\n        mask[i] = True\n    return mask\n\n\ndef cmc(\n        distmat,\n        query_ids=None,\n        gallery_ids=None,\n        query_cams=None,\n        gallery_cams=None,\n        topk=100,\n        separate_camera_set=False,\n        single_gallery_shot=False,\n        first_match_break=False,\n        average=True):\n    """"""\n    Args:\n        distmat: numpy array with shape [num_query, num_gallery], the\n            pairwise distance between query and gallery samples\n        query_ids: numpy array with shape [num_query]\n        gallery_ids: numpy array with shape [num_gallery]\n        query_cams: numpy array with shape [num_query]\n        gallery_cams: numpy array with shape [num_gallery]\n        average: whether to average the results across queries\n    Returns:\n        If `average` is `False`:\n            ret: numpy array with shape [num_query, topk]\n            is_valid_query: numpy array with shape [num_query], containing 0\'s and\n                1\'s, whether each query is valid or not\n        If `average` is `True`:\n            numpy array with shape [topk]\n    """"""\n    # Ensure numpy array\n    assert isinstance(distmat, np.ndarray)\n    assert isinstance(query_ids, np.ndarray)\n    assert isinstance(gallery_ids, np.ndarray)\n    assert isinstance(query_cams, np.ndarray)\n    assert isinstance(gallery_cams, np.ndarray)\n\n    m, n = distmat.shape\n    # Sort and find correct matches\n    indices = np.argsort(distmat, axis=1)\n    matches = (gallery_ids[indices] == query_ids[:, np.newaxis])\n    # Compute CMC for each query\n    ret = np.zeros([m, topk])\n    is_valid_query = np.zeros(m)\n    num_valid_queries = 0\n    for i in range(m):\n        # Filter out the same id and same camera\n        valid = ((gallery_ids[indices[i]] != query_ids[i]) |\n                 (gallery_cams[indices[i]] != query_cams[i]))\n        if separate_camera_set:\n            # Filter out samples from same camera\n            valid &= (gallery_cams[indices[i]] != query_cams[i])\n        if not np.any(matches[i, valid]): continue\n        is_valid_query[i] = 1\n        if single_gallery_shot:\n            repeat = 100\n            gids = gallery_ids[indices[i][valid]]\n            inds = np.where(valid)[0]\n            ids_dict = defaultdict(list)\n            for j, x in zip(inds, gids):\n                ids_dict[x].append(j)\n        else:\n            repeat = 1\n        for _ in range(repeat):\n            if single_gallery_shot:\n                # Randomly choose one instance for each id\n                sampled = (valid & _unique_sample(ids_dict, len(valid)))\n                index = np.nonzero(matches[i, sampled])[0]\n            else:\n                index = np.nonzero(matches[i, valid])[0]\n            delta = 1. / (len(index) * repeat)\n            for j, k in enumerate(index):\n                if k - j >= topk: break\n                if first_match_break:\n                    ret[i, k - j] += 1\n                    break\n                ret[i, k - j] += delta\n        num_valid_queries += 1\n    if num_valid_queries == 0:\n        raise RuntimeError(""No valid query"")\n    ret = ret.cumsum(axis=1)\n    if average:\n        return np.sum(ret, axis=0) / num_valid_queries\n    return ret, is_valid_query\n\n\ndef mean_ap(\n        distmat,\n        query_ids=None,\n        gallery_ids=None,\n        query_cams=None,\n        gallery_cams=None,\n        average=True):\n    """"""\n    Args:\n        distmat: numpy array with shape [num_query, num_gallery], the\n            pairwise distance between query and gallery samples\n        query_ids: numpy array with shape [num_query]\n        gallery_ids: numpy array with shape [num_gallery]\n        query_cams: numpy array with shape [num_query]\n        gallery_cams: numpy array with shape [num_gallery]\n        average: whether to average the results across queries\n    Returns:\n        If `average` is `False`:\n            ret: numpy array with shape [num_query]\n            is_valid_query: numpy array with shape [num_query], containing 0\'s and\n                1\'s, whether each query is valid or not\n        If `average` is `True`:\n            a scalar\n    """"""\n\n    # -------------------------------------------------------------------------\n    # The behavior of method `sklearn.average_precision` has changed since version\n    # 0.19.\n    # Version 0.18.1 has same results as Matlab evaluation code by Zhun Zhong\n    # (https://github.com/zhunzhong07/person-re-ranking/\n    # blob/master/evaluation/utils/evaluation.m) and by Liang Zheng\n    # (http://www.liangzheng.org/Project/project_reid.html).\n    # My current awkward solution is sticking to this older version.\n    import sklearn\n    cur_version = sklearn.__version__\n    required_version = \'0.18.1\'\n    if cur_version != required_version:\n        print(\'User Warning: Version {} is required for package scikit-learn, \'\n              \'your current version is {}. \'\n              \'As a result, the mAP score may not be totally correct. \'\n              \'You can try `pip uninstall scikit-learn` \'\n              \'and then `pip install scikit-learn=={}`\'.format(\n            required_version, cur_version, required_version))\n    # -------------------------------------------------------------------------\n\n    # Ensure numpy array\n    assert isinstance(distmat, np.ndarray)\n    assert isinstance(query_ids, np.ndarray)\n    assert isinstance(gallery_ids, np.ndarray)\n    assert isinstance(query_cams, np.ndarray)\n    assert isinstance(gallery_cams, np.ndarray)\n\n    m, n = distmat.shape\n\n    # Sort and find correct matches\n    indices = np.argsort(distmat, axis=1)\n    matches = (gallery_ids[indices] == query_ids[:, np.newaxis])\n    # Compute AP for each query\n    aps = np.zeros(m)\n    is_valid_query = np.zeros(m)\n    for i in range(m):\n        # Filter out the same id and same camera\n        valid = ((gallery_ids[indices[i]] != query_ids[i]) |\n                 (gallery_cams[indices[i]] != query_cams[i]))\n        y_true = matches[i, valid]\n        y_score = -distmat[i][indices[i]][valid]\n        if not np.any(y_true): continue\n        is_valid_query[i] = 1\n        aps[i] = average_precision_score(y_true, y_score)\n    if len(aps) == 0:\n        raise RuntimeError(""No valid query"")\n    if average:\n        return float(np.sum(aps)) / np.sum(is_valid_query)\n    return aps, is_valid_query\n'"
package/eval/np_distance.py,0,"b'""""""Numpy version of euclidean distance, etc.\nNotice the input/output shape of methods, so that you can better understand\nthe meaning of these methods.""""""\nimport numpy as np\n\n\ndef normalize(nparray, order=2, axis=0):\n    """"""Normalize a N-D numpy array along the specified axis.""""""\n    norm = np.linalg.norm(nparray, ord=order, axis=axis, keepdims=True)\n    return nparray / (norm + np.finfo(np.float32).eps)\n\n\ndef compute_dist(array1, array2, dist_type=\'cosine\', cos_to_normalize=True):\n    """"""Compute the euclidean or cosine distance of all pairs.\n    Args:\n        array1: numpy array with shape [m1, n]\n        array2: numpy array with shape [m2, n]\n        dist_type: one of [\'cosine\', \'euclidean\']\n    Returns:\n        dist: numpy array with shape [m1, m2]\n    """"""\n    if dist_type == \'cosine\':\n        if cos_to_normalize:\n            array1 = normalize(array1, axis=1)\n            array2 = normalize(array2, axis=1)\n        dist = - np.matmul(array1, array2.T)\n        # Turn distance into positive value\n        dist += 1\n    elif dist_type == \'euclidean\':\n        # shape [m1, 1]\n        square1 = np.sum(np.square(array1), axis=1)[..., np.newaxis]\n        # shape [1, m2]\n        square2 = np.sum(np.square(array2), axis=1)[np.newaxis, ...]\n        dist = - 2 * np.matmul(array1, array2.T) + square1 + square2\n        dist[dist < 0] = 0\n        # Print(\'Debug why there is warning in np.sqrt\')\n        # np.seterr(all=\'raise\')\n        # for x in dist.flatten():\n        #     try:\n        #         np.sqrt(x)\n        #     except:\n        #         print(x)\n        # Setting `out=dist` saves 1x memory size of `dist`\n        np.sqrt(dist, out=dist)\n    else:\n        raise NotImplementedError\n    return dist\n\n\ndef compute_dist_with_visibility(array1, array2, vis1, vis2, dist_type=\'cosine\', avg_by_vis_num=True):\n    """"""Compute the euclidean or cosine distance of all pairs, considering part visibility.\n    In this version, if a query image does not has some part, don\'t calculate distance for this part.\n    If a query has one part that gallery does not have, we can optionally set the part distance to some\n    prior value, e.g. the mean distance of this part.\n    Args:\n        array1: numpy array with shape [m1, n]\n        array2: numpy array with shape [m2, n]\n        vis1: numpy array with shape [m1, p], p is num_parts\n        vis2: numpy array with shape [m2, p], p is num_parts\n        dist_type: one of [\'cosine\', \'euclidean\']\n        avg_by_vis_num: for each <query_image, gallery_image> distance, average the\n            summed distance by the number of visible parts in query_image\n    Returns:\n        dist: numpy array with shape [m1, m2]\n    """"""\n    err_msg = ""array1.shape = {}, vis1.shape = {}, array2.shape = {}, vis2.shape = {}""\\\n        .format(array1.shape, vis1.shape, array2.shape, vis2.shape)\n    assert array1.shape[0] == vis1.shape[0], err_msg\n    assert array2.shape[0] == vis2.shape[0], err_msg\n    assert vis1.shape[1] == vis2.shape[1], err_msg\n    assert array1.shape[1] % vis1.shape[1] == 0, err_msg\n    assert array2.shape[1] % vis2.shape[1] == 0, err_msg\n    m1 = array1.shape[0]\n    m2 = array2.shape[0]\n    p = vis1.shape[1]\n    d = int(array1.shape[1] / vis1.shape[1])\n\n    array1 = array1.reshape([m1, p, d])\n    array2 = array2.reshape([m2, p, d])\n    dist = 0\n    for i in range(p):\n        # [m1, m2]\n        dist_ = compute_dist(array1[:, i, :], array2[:, i, :], dist_type=dist_type)\n        q_invisible = vis1[:, i][:, np.newaxis].repeat(m2, 1) == 0\n        dist_[q_invisible] = 0\n        dist += dist_\n    if avg_by_vis_num:\n        dist /= (np.sum(vis1, axis=1, keepdims=True) + 1e-8)\n    if dist_type == \'cosine\':\n        # Turn distance into positive value\n        dist += 1\n    return dist\n'"
package/eval/torch_distance.py,4,"b'import torch\n\n\ndef normalize(x, axis=-1):\n    """"""Normalizing to unit length along the specified dimension.\n    Args:\n        x: pytorch tensor\n    Returns:\n        x: pytorch tensor, same shape as input\n    """"""\n    x = 1. * x / (torch.norm(x, 2, axis, keepdim=True).expand_as(x) + 1e-12)\n    return x\n\n\ndef euclidean_dist(x, y):\n    """"""\n    Args:\n        x: pytorch tensor, with shape [m, d]\n        y: pytorch tensor, with shape [n, d]\n    Returns:\n        dist: pytorch tensor, with shape [m, n]\n    """"""\n    m, n = x.size(0), y.size(0)\n    xx = torch.pow(x, 2).sum(1, keepdim=True).expand(m, n)\n    yy = torch.pow(y, 2).sum(1, keepdim=True).expand(n, m).t()\n    dist = xx + yy\n    dist.addmm_(1, -2, x, y.t())\n    dist = dist.clamp(min=1e-12).sqrt()  # for numerical stability\n    return dist\n\n\ndef compute_dist(array1, array2, dist_type=\'cosine\', cos_to_normalize=True):\n    """"""\n    Args:\n        array1: pytorch tensor, with shape [m, d]\n        array2: pytorch tensor, with shape [n, d]\n    Returns:\n        dist: pytorch tensor, with shape [m, n]\n    """"""\n    if dist_type == \'cosine\':\n        if cos_to_normalize:\n            array1 = normalize(array1, axis=1)\n            array2 = normalize(array2, axis=1)\n        dist = - torch.mm(array1, array2.t())\n        # Turn distance into positive value\n        dist += 1\n    elif dist_type == \'euclidean\':\n        dist = euclidean_dist(array1, array2)\n    else:\n        raise NotImplementedError\n    return dist\n'"
package/loss/__init__.py,0,b''
package/loss/id_loss.py,2,"b""from __future__ import print_function\nimport torch\nfrom .loss import Loss\nfrom ..utils.meter import RecentAverageMeter as Meter\n\n\nclass IDLoss(Loss):\n    def __init__(self, cfg, tb_writer=None):\n        super(IDLoss, self).__init__(cfg, tb_writer=tb_writer)\n        self.criterion = torch.nn.CrossEntropyLoss(reduction='none')\n\n    def __call__(self, batch, pred, step=0, **kwargs):\n        cfg = self.cfg\n\n        # Calculation\n        if 'visible' in pred:\n            loss_list = [(self.criterion(logits, batch['label']) * pred['visible'][:, i]).sum()\n                         / (pred['visible'][:, i].sum() + 1e-12)\n                         for i, logits in enumerate(pred['logits_list'])]\n        else:\n            loss_list = [self.criterion(logits, batch['label']).mean() for logits in pred['logits_list']]\n        # New version of pytorch allow stacking 0-dim tensors, but not concatenating.\n        loss = torch.stack(loss_list).sum()\n\n        # Meter\n        if cfg.name not in self.meter_dict:\n            self.meter_dict[cfg.name] = Meter(name=cfg.name)\n        self.meter_dict[cfg.name].update(loss.item())\n        if len(loss_list) > 1:\n            part_fmt = '#{}'\n            for i in range(len(loss_list)):\n                if part_fmt.format(i + 1) not in self.meter_dict:\n                    self.meter_dict[part_fmt.format(i + 1)] = Meter(name=part_fmt.format(i + 1))\n                self.meter_dict[part_fmt.format(i + 1)].update(loss_list[i].item())\n\n        # Tensorboard\n        if self.tb_writer is not None:\n            self.tb_writer.add_scalars(cfg.name, {cfg.name: self.meter_dict[cfg.name].avg}, step)\n            if len(loss_list) > 1:\n                self.tb_writer.add_scalars('Part ID Losses', {part_fmt.format(i + 1): self.meter_dict[part_fmt.format(i + 1)].avg for i in range(len(loss_list))}, step)\n\n        # Scale by loss weight\n        loss *= cfg.weight\n\n        return {'loss': loss}\n"""
package/loss/loss.py,0,"b'from collections import OrderedDict\n\n\nclass Loss(object):\n    """"""Base class for calculating loss and managing log.\n    """"""\n    def __init__(self, cfg, tb_writer=None):\n        self.cfg = cfg\n        self.meter_dict = OrderedDict()\n        self.tb_writer = tb_writer\n\n    def reset_meters(self):\n        for k, v in self.meter_dict.items():\n            v.reset()\n\n    def __call__(self, batch, pred, step=0, **kwargs):\n        """"""Return a dict, whose keys at least include [\'loss\']. In some cases, e.g.\n        for saving GPU memory, you may perform backward in this function. In this case,\n        the returned loss should be a python scalar, so that the outer logic will not\n        perform backward.""""""\n        pass\n'"
package/loss/ps_loss.py,1,"b""from __future__ import print_function\nimport torch\nfrom .loss import Loss\nfrom ..utils.meter import RecentAverageMeter as Meter\n\n\nclass PSLoss(Loss):\n    def __init__(self, cfg, tb_writer=None):\n        super(PSLoss, self).__init__(cfg, tb_writer=tb_writer)\n        self.criterion = torch.nn.CrossEntropyLoss(reduction='none' if cfg.normalize_size else 'mean')\n\n    # TODO: Pytorch newer versions support high-dimension CrossEntropyLoss, so no need to reshape pred and label.\n    def __call__(self, batch, pred, step=0, **kwargs):\n        cfg = self.cfg\n\n        # Calculation\n        ps_pred = pred['ps_pred']\n        ps_label = batch['ps_label']\n        N, C, H, W = ps_pred.size()\n        assert ps_label.size() == (N, H, W)\n        # shape [N, C, H, W] -> [N, H, W, C] -> [NHW, C]\n        ps_pred = ps_pred.permute(0, 2, 3, 1).contiguous().view(-1, C)\n        # shape [N, H, W] -> [NHW]\n        ps_label = ps_label.view(N * H * W).detach()\n        loss = self.criterion(ps_pred, ps_label)\n        # Calculate each class avg loss and then average across classes, to compensate for classes that have few pixels\n        if cfg.normalize_size:\n            loss_ = 0\n            cur_batch_n_classes = 0\n            for i in range(cfg.num_classes):\n                loss_i = loss[ps_label == i]\n                if loss_i.numel() > 0:\n                    loss_ += loss_i.mean()\n                    cur_batch_n_classes += 1\n            loss_ /= (cur_batch_n_classes + 1e-8)\n            loss = loss_\n\n        # Meter\n        if cfg.name not in self.meter_dict:\n            self.meter_dict[cfg.name] = Meter(name=cfg.name)\n        self.meter_dict[cfg.name].update(loss.item())\n\n        # Tensorboard\n        if self.tb_writer is not None:\n            self.tb_writer.add_scalars(cfg.name, {cfg.name: self.meter_dict[cfg.name].avg}, step)\n\n        # Scale by loss weight\n        loss *= cfg.weight\n\n        return {'loss': loss}\n"""
package/loss/triplet_loss.py,8,"b'from __future__ import print_function\nfrom collections import OrderedDict\nimport torch\nimport torch.nn as nn\nfrom ..eval.torch_distance import compute_dist\nfrom ..utils.meter import RecentAverageMeter as Meter\nfrom .loss import Loss\n\n\nclass _TripletLoss(object):\n    """"""Reference:\n        https://github.com/Cysu/open-reid\n        In Defense of the Triplet Loss for Person Re-Identification\n    """"""\n    def __init__(self, margin=None):\n        self.margin = margin\n        if margin is not None:\n            self.ranking_loss = nn.MarginRankingLoss(margin=margin)\n        else:\n            self.ranking_loss = nn.SoftMarginLoss()\n\n    def __call__(self, dist_ap, dist_an):\n        """"""\n        Args:\n          dist_ap: pytorch tensor, distance between anchor and positive sample, shape [N]\n          dist_an: pytorch tensor, distance between anchor and negative sample, shape [N]\n        Returns:\n          loss: pytorch scalar\n        """"""\n        y = torch.ones_like(dist_ap)\n        if self.margin is not None:\n            loss = self.ranking_loss(dist_an, dist_ap, y)\n        else:\n            loss = self.ranking_loss(dist_an - dist_ap, y)\n        return loss\n\n\ndef construct_triplets(dist_mat, labels, hard_type=\'tri_hard\'):\n    """"""Construct triplets inside a batch.\n    Args:\n        dist_mat: pytorch tensor, pair wise distance between samples, shape [N, N]\n        labels: pytorch LongTensor, with shape [N]\n    Returns:\n        dist_ap: pytorch tensor, distance(anchor, positive); shape [M]\n        dist_an: pytorch tensor, distance(anchor, negative); shape [M]\n    NOTE: Only consider PK batch, so we can cope with all anchors in parallel.\n    """"""\n    assert len(dist_mat.size()) == 2\n    assert dist_mat.size(0) == dist_mat.size(1)\n    N = dist_mat.size(0)\n\n    # shape [N, N]\n    is_self = labels.new().resize_(N, N).copy_(torch.eye(N)).byte()\n    is_pos = labels.expand(N, N).eq(labels.expand(N, N).t())\n    K = is_pos.sum(1)[0]\n    P = int(N / K)\n    assert P * K == N, ""P * K = {}, N = {}"".format(P * K, N)\n    is_pos = ~ is_self & is_pos\n    is_neg = labels.expand(N, N).ne(labels.expand(N, N).t())\n\n    if hard_type == \'semi\':\n        dist_ap = dist_mat[is_pos].contiguous().view(-1)\n        dist_an, relative_n_inds = torch.min(dist_mat[is_neg].contiguous().view(N, -1), 1, keepdim=True)\n        dist_an = dist_an.expand(N, K - 1).contiguous().view(-1)\n    elif hard_type == \'all\':\n        dist_ap = dist_mat[is_pos].contiguous().view(N, K - 1).unsqueeze(-1).expand(N, K - 1, P * K - K).contiguous().view(-1)\n        dist_an = dist_mat[is_neg].contiguous().view(N, P * K - K).unsqueeze(1).expand(N, K - 1, P * K - K).contiguous().view(-1)\n    elif hard_type == \'tri_hard\':\n        # `dist_ap` means distance(anchor, positive); both `dist_ap` and `relative_p_inds` with shape [N]\n        dist_ap, relative_p_inds = torch.max(dist_mat[is_pos].contiguous().view(N, -1), 1, keepdim=False)\n        # `dist_an` means distance(anchor, negative); both `dist_an` and `relative_n_inds` with shape [N]\n        dist_an, relative_n_inds = torch.min(dist_mat[is_neg].contiguous().view(N, -1), 1, keepdim=False)\n    else:\n        raise NotImplementedError\n\n    # print(""dist_ap.size() {}, dist_an.size() {}, N {}, P {}, K {}"".format(dist_ap.size(), dist_an.size(), N, P, K))\n    assert dist_ap.size() == dist_an.size(), ""dist_ap.size() {}, dist_an.size() {}"".format(dist_ap.size(), dist_an.size())\n    return dist_ap, dist_an\n\n\nclass TripletLoss(Loss):\n    def __init__(self, cfg, tb_writer=None):\n        super(TripletLoss, self).__init__(cfg, tb_writer=tb_writer)\n        self.tri_loss_obj = _TripletLoss(margin=cfg.margin)\n\n    def calculate(self, feat, labels, hard_type=None):\n        """"""\n        Args:\n            feat: pytorch tensor, shape [N, C]\n            labels: pytorch LongTensor, with shape [N]\n            hard_type: can be dynamically set to different types during training, for hybrid or curriculum learning\n        Returns:\n            loss: pytorch scalar\n            ==================\n            For Debugging, etc\n            ==================\n            dist_ap: pytorch tensor, distance(anchor, positive); shape [N]\n            dist_an: pytorch tensor, distance(anchor, negative); shape [N]\n            dist_mat: pytorch tensor, pairwise euclidean distance; shape [N, N]\n        """"""\n        cfg = self.cfg\n        dist_mat = compute_dist(feat, feat, dist_type=cfg.dist_type)\n        if hard_type is None:\n            hard_type = cfg.hard_type\n        dist_ap, dist_an = construct_triplets(dist_mat, labels, hard_type=hard_type)\n        loss = self.tri_loss_obj(dist_ap, dist_an)\n        if cfg.norm_by_num_of_effective_triplets:\n            sm = (dist_an > dist_ap + cfg.margin).float().mean().item()\n            loss *= 1. / (1 - sm + 1e-8)\n        return {\'loss\': loss, \'dist_ap\': dist_ap, \'dist_an\': dist_an}\n\n    def __call__(self, batch, pred, step=0, hard_type=None):\n        # NOTE: Here is only a trial implementation for PAP-9P*\n\n        # Calculation\n        cfg = self.cfg\n        res1 = self.calculate(torch.cat(pred[\'feat_list\'][:6], 1), batch[\'label\'], hard_type=hard_type)\n        res2 = self.calculate(torch.cat(pred[\'feat_list\'][7:9], 1), batch[\'label\'], hard_type=hard_type)\n        res3 = self.calculate(pred[\'feat_list\'][9], batch[\'label\'], hard_type=hard_type)\n        loss = res1[\'loss\'] + res2[\'loss\'] + res3[\'loss\']\n\n        # Meter\n        if cfg.name not in self.meter_dict:\n            self.meter_dict[cfg.name] = Meter(name=cfg.name)\n        self.meter_dict[cfg.name].update(loss.item())\n        dist_an = res1[\'dist_an\']\n        dist_ap = res1[\'dist_ap\']\n        # precision\n        key = \'prec\'\n        if key not in self.meter_dict:\n            self.meter_dict[key] = Meter(name=key, fmt=\'{:.2%}\')\n        self.meter_dict[key].update((dist_an > dist_ap).float().mean().item())\n        # the proportion of triplets that satisfy margin\n        key = \'sm\'\n        if key not in self.meter_dict:\n            self.meter_dict[key] = Meter(name=key, fmt=\'{:.2%}\')\n        self.meter_dict[key].update((dist_an > dist_ap + cfg.margin).float().mean().item())\n        # average (anchor, positive) distance\n        key = \'d_ap\'\n        if key not in self.meter_dict:\n            self.meter_dict[key] = Meter(name=key)\n        self.meter_dict[key].update(dist_ap.mean().item())\n        # average (anchor, negative) distance\n        key = \'d_an\'\n        if key not in self.meter_dict:\n            self.meter_dict[key] = Meter(name=key)\n        self.meter_dict[key].update(dist_an.mean().item())\n\n        # Tensorboard\n        if self.tb_writer is not None:\n            for key in [cfg.name, \'prec\', \'sm\', \'d_ap\', \'d_an\']:\n                self.tb_writer.add_scalars(key, {key: self.meter_dict[key].avg}, step)\n\n        # Scale by loss weight\n        loss *= cfg.weight\n\n        return {\'loss\': loss}\n'"
package/model/__init__.py,0,b''
package/model/backbone.py,0,"b""from resnet import get_resnet\n\n\nbackbone_factory = {\n    'resnet18': get_resnet,\n    'resnet34': get_resnet,\n    'resnet50': get_resnet,\n    'resnet101': get_resnet,\n    'resnet152': get_resnet,\n}\n\n\ndef create_backbone(cfg):\n    return backbone_factory[cfg.name](cfg)\n"""
package/model/base_model.py,1,"b'import torch.nn as nn\n\n\nclass BaseModel(nn.Module):\n\n    def get_ft_and_new_params(self, *args, **kwargs):\n        """"""Get finetune and new parameters, mostly for creating optimizer.\n        Return two lists.""""""\n        return [], list(self.parameters())\n\n    def get_ft_and_new_modules(self, *args, **kwargs):\n        """"""Get finetune and new modules, mostly for setting train/eval mode.\n        Return two lists.""""""\n        return [], list(self.modules())\n\n    def set_train_mode(self, *args, **kwargs):\n        """"""Set model to train mode for model training, some layers can be fixed and set to eval mode.""""""\n        self.train()\n'"
package/model/global_pool.py,1,"b""import torch.nn as nn\n\n\nclass GlobalPool(object):\n    def __init__(self, cfg):\n        self.pool = nn.AdaptiveAvgPool2d(1) if cfg.max_or_avg == 'avg' else nn.AdaptiveMaxPool2d(1)\n\n    def __call__(self, in_dict):\n        feat = self.pool(in_dict['feat'])\n        feat = feat.view(feat.size(0), -1)\n        out_dict = {'feat_list': [feat]}\n        return out_dict\n"""
package/model/model.py,1,"b'from __future__ import print_function\nfrom itertools import chain\nimport torch\nimport torch.nn as nn\nfrom .base_model import BaseModel\nfrom .backbone import create_backbone\nfrom .pa_pool import PAPool\nfrom .pcb_pool import PCBPool\nfrom .global_pool import GlobalPool\nfrom .ps_head import PartSegHead\nfrom ..utils.model import create_embedding\nfrom ..utils.model import init_classifier\n\n\nclass Model(BaseModel):\n    def __init__(self, cfg):\n        super(Model, self).__init__()\n        self.cfg = cfg\n        self.backbone = create_backbone(cfg.backbone)\n        self.pool = eval(\'{}(cfg)\'.format(cfg.pool_type))\n        self.create_em_list()\n        if hasattr(cfg, \'num_classes\') and cfg.num_classes > 0:\n            self.create_cls_list()\n        if cfg.use_ps:\n            cfg.ps_head.in_c = self.backbone.out_c\n            self.ps_head = PartSegHead(cfg.ps_head)\n        print(\'Model Structure:\\n{}\'.format(self))\n\n    def create_em_list(self):\n        cfg = self.cfg\n        self.em_list = nn.ModuleList([create_embedding(self.backbone.out_c, cfg.em_dim) for _ in range(cfg.num_parts)])\n\n    def create_cls_list(self):\n        cfg = self.cfg\n        self.cls_list = nn.ModuleList([nn.Linear(cfg.em_dim, cfg.num_classes) for _ in range(cfg.num_parts)])\n        ori_w = self.cls_list[0].weight.view(-1).detach().numpy().copy()\n        self.cls_list.apply(init_classifier)\n        new_w = self.cls_list[0].weight.view(-1).detach().numpy().copy()\n        import numpy as np\n        if np.array_equal(ori_w, new_w):\n            from ..utils.log import array_str\n            print(\'!!!!!! Warning: Model Weight Not Changed After Init !!!!!\')\n            print(\'Original Weight [:20]:\\n\\t{}\'.format(array_str(ori_w[:20], fmt=\'{:.6f}\')))\n            print(\'New Weight [:20]:\\n\\t{}\'.format(array_str(new_w[:20], fmt=\'{:.6f}\')))\n\n    def get_ft_and_new_params(self, cft=False):\n        """"""cft: Clustering and Fine Tuning""""""\n        ft_modules, new_modules = self.get_ft_and_new_modules(cft=cft)\n        ft_params = list(chain.from_iterable([list(m.parameters()) for m in ft_modules]))\n        new_params = list(chain.from_iterable([list(m.parameters()) for m in new_modules]))\n        return ft_params, new_params\n\n    def get_ft_and_new_modules(self, cft=False):\n        if cft:\n            ft_modules = [self.backbone, self.em_list]\n            if hasattr(self, \'ps_head\'):\n                ft_modules += [self.ps_head]\n            new_modules = [self.cls_list] if hasattr(self, \'cls_list\') else []\n        else:\n            ft_modules = [self.backbone]\n            new_modules = [self.em_list]\n            if hasattr(self, \'cls_list\'):\n                new_modules += [self.cls_list]\n            if hasattr(self, \'ps_head\'):\n                new_modules += [self.ps_head]\n        return ft_modules, new_modules\n\n    def set_train_mode(self, cft=False, fix_ft_layers=False):\n        self.train()\n        if fix_ft_layers:\n            for m in self.get_ft_and_new_modules(cft=cft)[0]:\n                m.eval()\n\n    def backbone_forward(self, in_dict):\n        return self.backbone(in_dict[\'im\'])\n\n    def reid_forward(self, in_dict):\n        # print(\'=====> in_dict.keys() entering reid_forward():\', in_dict.keys())\n        pool_out_dict = self.pool(in_dict)\n        feat_list = [em(f) for em, f in zip(self.em_list, pool_out_dict[\'feat_list\'])]\n        out_dict = {\n            \'feat_list\': feat_list,\n        }\n        if hasattr(self, \'cls_list\'):\n            logits_list = [cls(f) for cls, f in zip(self.cls_list, feat_list)]\n            out_dict[\'logits_list\'] = logits_list\n        if \'visible\' in pool_out_dict:\n            out_dict[\'visible\'] = pool_out_dict[\'visible\']\n        return out_dict\n\n    def ps_forward(self, in_dict):\n        return self.ps_head(in_dict[\'feat\'])\n\n    def forward(self, in_dict, forward_type=\'reid\'):\n        in_dict[\'feat\'] = self.backbone_forward(in_dict)\n        if forward_type == \'reid\':\n            out_dict = self.reid_forward(in_dict)\n        elif forward_type == \'ps\':\n            out_dict = {\'ps_pred\': self.ps_forward(in_dict)}\n        elif forward_type == \'ps_reid_parallel\':\n            out_dict = self.reid_forward(in_dict)\n            out_dict[\'ps_pred\'] = self.ps_forward(in_dict)\n        elif forward_type == \'ps_reid_serial\':\n            ps_pred = self.ps_forward(in_dict)\n            # Generate pap masks from ps_pred\n            in_dict[\'pap_mask\'] = gen_pap_mask_from_ps_pred(ps_pred)\n            out_dict = self.reid_forward(in_dict)\n            out_dict[\'ps_pred\'] = ps_pred\n        else:\n            raise ValueError(\'Error forward_type {}\'.format(forward_type))\n        return out_dict\n'"
package/model/pa_pool.py,1,"b'import torch.nn.functional as F\n\n\ndef pa_avg_pool(in_dict):\n    """"""Mask weighted avg pooling.\n    Args:\n        feat: pytorch tensor, with shape [N, C, H, W]\n        mask: pytorch tensor, with shape [N, pC, pH, pW]\n    Returns:\n        feat_list: a list (length = pC) of pytorch tensors with shape [N, C]\n        visible: pytorch tensor with shape [N, pC]\n    """"""\n    feat = in_dict[\'feat\']\n    mask = in_dict[\'pap_mask\']\n    N, C, H, W = feat.size()\n    N, pC, pH, pW = mask.size()\n    # 1 * [N, C, pH, pW] -> [N, 1, C, pH, pW] -> [N, pC, C, pH, pW]\n    feat = feat.unsqueeze(1).expand((N, pC, C, pH, pW))\n    # [N, pC]\n    visible = (mask.sum(-1).sum(-1) != 0).float()\n    # [N, pC, 1, pH, pW] -> [N, pC, C, pH, pW]\n    mask = mask.unsqueeze(2).expand((N, pC, C, pH, pW))\n    # [N, pC, C]\n    feat = (feat * mask).sum(-1).sum(-1) / (mask.sum(-1).sum(-1) + 1e-12)\n    # pC * [N, C]\n    feat_list = list(feat.transpose(0, 1))\n    out_dict = {\'feat_list\': feat_list, \'visible\': visible}\n    return out_dict\n\n\ndef pa_max_pool(in_dict):\n    """"""Implement `local max pooling` as `masking + global max pooling`.\n    Args:\n        feat: pytorch tensor, with shape [N, C, H, W]\n        mask: pytorch tensor, with shape [N, pC, pH, pW]\n    Returns:\n        feat_list: a list (length = pC) of pytorch tensors with shape [N, C]\n        visible: pytorch tensor with shape [N, pC]\n    NOTE:\n        The implementation of `masking + global max pooling` is only equivalent\n        to `local max pooling` when feature values are non-negative, which holds\n        for ResNet that has ReLU as final operation of all blocks.\n    """"""\n    feat = in_dict[\'feat\']\n    mask = in_dict[\'pap_mask\']\n    N, C, H, W = feat.size()\n    N, pC, pH, pW = mask.size()\n    feat_list = []\n    for i in range(pC):\n        # [N, C, pH, pW]\n        m = mask[:, i, :, :].unsqueeze(1).expand_as(feat)\n        # [N, C]\n        local_feat = F.adaptive_max_pool2d(feat * m, 1).view(N, -1)\n        feat_list.append(local_feat)\n    # [N, pC]\n    visible = (mask.sum(-1).sum(-1) != 0).float()\n    out_dict = {\'feat_list\': feat_list, \'visible\': visible}\n    return out_dict\n\n\nclass PAPool(object):\n    def __init__(self, cfg):\n        self.pool = pa_avg_pool if cfg.max_or_avg == \'avg\' else pa_max_pool\n\n    def __call__(self, *args, **kwargs):\n        return self.pool(*args, **kwargs)\n'"
package/model/pcb_pool.py,1,"b""import torch.nn as nn\n\n\nclass PCBPool(object):\n    def __init__(self, cfg):\n        self.cfg = cfg\n        self.pool = nn.AdaptiveAvgPool2d(1) if cfg.max_or_avg == 'avg' else nn.AdaptiveMaxPool2d(1)\n\n    def __call__(self, in_dict):\n        feat = in_dict['feat']\n        assert feat.size(2) % self.cfg.num_parts == 0\n        stripe_h = int(feat.size(2) / self.cfg.num_parts)\n        feat_list = []\n        for i in range(self.cfg.num_parts):\n            # shape [N, C]\n            local_feat = self.pool(feat[:, :, i * stripe_h: (i + 1) * stripe_h, :])\n            # shape [N, C]\n            local_feat = local_feat.view(local_feat.size(0), -1)\n            feat_list.append(local_feat)\n        out_dict = {'feat_list': feat_list}\n        return out_dict\n"""
package/model/ps_head.py,1,"b'import torch.nn as nn\n\n\nclass PartSegHead(nn.Module):\n    def __init__(self, cfg):\n        super(PartSegHead, self).__init__()\n        self.deconv = nn.ConvTranspose2d(\n            in_channels=cfg.in_c,\n            out_channels=cfg.mid_c,\n            kernel_size=3,\n            stride=2,\n            padding=1,\n            output_padding=1,\n            bias=False,\n        )\n        self.bn = nn.BatchNorm2d(cfg.mid_c)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv = nn.Conv2d(\n            in_channels=cfg.mid_c,\n            out_channels=cfg.num_classes,\n            kernel_size=1,\n            stride=1,\n            padding=0,\n        )\n        nn.init.normal_(self.deconv.weight, std=0.001)\n        nn.init.normal_(self.conv.weight, std=0.001)\n        nn.init.constant_(self.conv.bias, 0)\n\n    def forward(self, x):\n        x = self.conv(self.relu(self.bn(self.deconv(x))))\n        return x\n'"
package/model/resnet.py,7,"b'from __future__ import print_function\nimport os.path as osp\nimport torch.nn as nn\nimport torch.utils.model_zoo as model_zoo\nfrom collections import namedtuple\nfrom ..utils.torch_utils import load_state_dict\n\n\nmodel_urls = {\n    \'resnet18\': \'https://download.pytorch.org/models/resnet18-5c106cde.pth\',\n    \'resnet34\': \'https://download.pytorch.org/models/resnet34-333f7ec4.pth\',\n    \'resnet50\': \'https://download.pytorch.org/models/resnet50-19c8e357.pth\',\n    \'resnet101\': \'https://download.pytorch.org/models/resnet101-5d3b4d8f.pth\',\n    \'resnet152\': \'https://download.pytorch.org/models/resnet152-b121ed2d.pth\',\n}\n\n\ndef conv3x3(in_planes, out_planes, stride=1):\n    """"""3x3 convolution with padding""""""\n    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n                     padding=1, bias=False)\n\n\ndef conv1x1(in_planes, out_planes, stride=1):\n    """"""1x1 convolution""""""\n    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n\n\nclass BasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None):\n        super(BasicBlock, self).__init__()\n        self.conv1 = conv3x3(inplanes, planes, stride)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = conv3x3(planes, planes)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\n\nclass Bottleneck(nn.Module):\n    expansion = 4\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None):\n        super(Bottleneck, self).__init__()\n        self.conv1 = conv1x1(inplanes, planes)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.conv2 = conv3x3(planes, planes, stride)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.conv3 = conv1x1(planes, planes * self.expansion)\n        self.bn3 = nn.BatchNorm2d(planes * self.expansion)\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\n\nclass ResNet(nn.Module):\n\n    def __init__(self, block, layers, cfg):\n        super(ResNet, self).__init__()\n        self.inplanes = 64\n        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3,\n                               bias=False)\n        self.bn1 = nn.BatchNorm2d(64)\n        self.relu = nn.ReLU(inplace=True)\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n        self.layer1 = self._make_layer(block, 64, layers[0])\n        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n        self.layer4 = self._make_layer(block, 512, layers[3], stride=cfg.last_conv_stride)\n        self.out_c = 512 * block.expansion\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight, mode=\'fan_out\', nonlinearity=\'relu\')\n            elif isinstance(m, nn.BatchNorm2d):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n\n    def _make_layer(self, block, planes, blocks, stride=1):\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                conv1x1(self.inplanes, planes * block.expansion, stride),\n                nn.BatchNorm2d(planes * block.expansion),\n            )\n\n        layers = []\n        layers.append(block(self.inplanes, planes, stride, downsample))\n        self.inplanes = planes * block.expansion\n        for _ in range(1, blocks):\n            layers.append(block(self.inplanes, planes))\n\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n\n        return x\n\n\nArchCfg = namedtuple(\'ArchCfg\', [\'block\', \'layers\'])\narch_dict = {\n    \'resnet18\': ArchCfg(BasicBlock, [2, 2, 2, 2]),\n    \'resnet34\': ArchCfg(BasicBlock, [3, 4, 6, 3]),\n    \'resnet50\': ArchCfg(Bottleneck, [3, 4, 6, 3]),\n    \'resnet101\': ArchCfg(Bottleneck, [3, 4, 23, 3]),\n    \'resnet152\': ArchCfg(Bottleneck, [3, 8, 36, 3]),\n}\n\n\ndef get_resnet(cfg):\n    model = ResNet(arch_dict[cfg.name].block, arch_dict[cfg.name].layers, cfg)\n    if cfg.pretrained:\n        state_dict = model_zoo.load_url(model_urls[cfg.name], model_dir=cfg.pretrained_model_dir)\n        load_state_dict(model, state_dict)\n        print(\'=> Loaded ImageNet Model: {}\'.format(osp.join(cfg.pretrained_model_dir, osp.basename(model_urls[cfg.name]))))\n    return model\n'"
package/optim/__init__.py,0,b''
package/optim/cft_trainer.py,0,"b'from __future__ import print_function\nimport numpy as np\nfrom sklearn.cluster import DBSCAN\nfrom .eanet_trainer import EANetTrainer\nfrom ..eval.extract_feat import extract_dataloader_feat\nfrom ..eval.np_distance import compute_dist, compute_dist_with_visibility\n\n\nclass CFTTrainer(EANetTrainer):\n    """"""Clustring and Finetuning.""""""\n\n    def extract_feat(self):\n        cfg = self.cfg\n        extract_feat_loader = self.create_dataloader(mode=\'test\', name=cfg.dataset.train.name, split=cfg.dataset.train.split)\n        feat_dict = extract_dataloader_feat(self.model, extract_feat_loader, cfg.eval)\n        return feat_dict\n\n    def compute_dist(self, dic):\n        if \'visible\' in dic:\n            dist_mat = compute_dist_with_visibility(dic[\'feat\'], dic[\'feat\'], dic[\'visible\'], dic[\'visible\'])\n        else:\n            dist_mat = compute_dist(dic[\'feat\'], dic[\'feat\'])\n        return dist_mat\n\n    def estimate_label(self, dist_mat):\n        cfg = self.cfg\n        print(\'dist_mat.min(), dist_mat.max(), dist_mat.mean(): \', dist_mat[dist_mat > 1e-6].min(), dist_mat[dist_mat > 1e-6].max(), dist_mat[dist_mat > 1e-6].mean())\n        tri_mat = np.triu(dist_mat, 1)  # tri_mat.dim=2\n        tri_mat = tri_mat[np.nonzero(tri_mat)]  # tri_mat.dim=1\n        tri_mat = np.sort(tri_mat, axis=None)\n        top_num = np.round(cfg.optim.cft_rho * tri_mat.size).astype(int)\n        # NOTE: DBSCAN eps requires positive value\n        eps = tri_mat[:top_num].mean()\n        print(\'eps in cluster: {:.3f}\'.format(eps))\n        cluster = DBSCAN(eps=eps, min_samples=4, metric=\'precomputed\', n_jobs=8)\n        labels = cluster.fit_predict(dist_mat)\n        return labels\n\n    def select_pseudo_samples(self, feat_dict, labels):\n        samples = []\n        for idx, l in enumerate(labels):\n            if l != -1:\n                sample = {k: v[idx] for k, v in feat_dict.items()}\n                sample[\'label\'] = l\n                del sample[\'cam\']\n                samples.append(sample)\n        return samples\n\n    def cft_one_iter(self):\n        cfg = self.cfg.optim\n        feat_dict = self.extract_feat()\n        dist_mat = self.compute_dist(feat_dict)\n        labels = self.estimate_label(dist_mat)\n        num_labels = len(set(labels)) - 1\n        assert num_labels > 1, ""No Pseudo Labels Available!""\n        print(\'NO. Pseudo Labels: {}\'.format(num_labels))\n        samples = self.select_pseudo_samples(feat_dict, labels)\n        for phase in [\'pretrain\', \'normal\']:\n            cfg.phase = phase\n            cfg.dont_test = phase == \'pretrain\'\n            self.init_trainer(samples=samples)\n            self.load_items(model=True)\n            self.train()\n\n    def cft_total_iters(self):\n        cfg = self.cfg\n        # Test direct transfer of source domain -> target domain.\n        # NOTE: ckpt.pth should be placed under current exp dir.\n        self.load_items(model=True)\n        self.test()\n        for i in range(cfg.optim.cft_iters):\n            print(\'*\' * 50 + \'\\n\' + \'Clustering and Finetuning Iter {}\'.format(i + 1) + \'\\n\' + \'*\' * 50)\n            self.cft_one_iter()\n\n\nif __name__ == \'__main__\':\n    from ..utils import init_path\n    trainer = CFTTrainer()\n    if trainer.cfg.only_test:\n        trainer.test()\n    else:\n        trainer.cft_total_iters()\n'"
package/optim/eanet_trainer.py,3,"b'from __future__ import print_function\nfrom collections import OrderedDict\nfrom copy import deepcopy\nimport torch\nfrom torch.optim.lr_scheduler import MultiStepLR\nfrom .reid_trainer import ReIDTrainer\nfrom ..model.model import Model\nfrom ..utils.torch_utils import may_data_parallel\nfrom ..utils.torch_utils import recursive_to_device\nfrom .optimizer import create_optimizer\nfrom .lr_scheduler import WarmupLR\nfrom ..data.multitask_dataloader import MTDataLoader\nfrom ..loss.triplet_loss import TripletLoss\nfrom ..loss.id_loss import IDLoss\nfrom ..loss.ps_loss import PSLoss\n\n\nclass EANetTrainer(ReIDTrainer):\n\n    def create_train_loader(self, samples=None):\n        cfg = self.cfg\n        self.train_loader = self.create_dataloader(mode=\'train\', samples=samples)\n        if cfg.cd_ps_loss.use:\n            self.cd_train_loader = self.create_dataloader(mode=\'cd_train\', samples=samples)\n            self.train_loader = MTDataLoader([self.train_loader, self.cd_train_loader], ref_loader_idx=0)\n\n    def create_model(self):\n        if hasattr(self, \'train_loader\'):\n            reid_loader = self.train_loader.loaders[0] if self.cfg.cd_ps_loss.use else self.train_loader\n            self.cfg.model.num_classes = reid_loader.dataset.num_ids\n        self.model = Model(deepcopy(self.cfg.model))\n        self.model = may_data_parallel(self.model)\n        self.model.to(self.device)\n\n    def set_model_to_train_mode(self):\n        cfg = self.cfg.optim\n        self.model.set_train_mode(cft=cfg.cft, fix_ft_layers=cfg.phase == \'pretrain\')\n\n    def create_optimizer(self):\n        cfg = self.cfg.optim\n        ft_params, new_params = self.model.get_ft_and_new_params(cft=cfg.cft)\n        if cfg.phase == \'pretrain\':\n            assert len(new_params) > 0, ""No new params to pretrain!""\n            param_groups = [{\'params\': new_params, \'lr\': cfg.new_params_lr}]\n        else:\n            param_groups = [{\'params\': ft_params, \'lr\': cfg.ft_lr}]\n            # Some model may not have new params\n            if len(new_params) > 0:\n                param_groups += [{\'params\': new_params, \'lr\': cfg.new_params_lr}]\n        self.optimizer = create_optimizer(param_groups, cfg)\n        recursive_to_device(self.optimizer.state_dict(), self.device)\n\n    def create_lr_scheduler(self):\n        cfg = self.cfg.optim\n        if cfg.phase == \'normal\':\n            cfg.lr_decay_steps = [len(self.train_loader) * ep for ep in cfg.lr_decay_epochs]\n            cfg.epochs = cfg.normal_epochs\n            self.lr_scheduler = MultiStepLR(self.optimizer, cfg.lr_decay_steps)\n        elif cfg.phase == \'warmup\':\n            cfg.warmup_steps = cfg.warmup_epochs * len(self.train_loader)\n            cfg.epochs = cfg.warmup_epochs\n            self.lr_scheduler = WarmupLR(self.optimizer, cfg.warmup_steps)\n        elif cfg.phase == \'pretrain\':\n            cfg.pretrain_new_params_steps = cfg.pretrain_new_params_epochs * len(self.train_loader)\n            cfg.epochs = cfg.pretrain_new_params_epochs\n            self.lr_scheduler = None\n        else:\n            raise ValueError(\'Invalid phase {}\'.format(cfg.phase))\n\n    def create_loss_funcs(self):\n        cfg = self.cfg\n        self.loss_funcs = OrderedDict()\n        if cfg.id_loss.use:\n            self.loss_funcs[cfg.id_loss.name] = IDLoss(cfg.id_loss, self.tb_writer)\n        if cfg.tri_loss.use:\n            self.loss_funcs[cfg.tri_loss.name] = TripletLoss(cfg.tri_loss, self.tb_writer)\n        if cfg.src_ps_loss.use:\n            self.loss_funcs[cfg.src_ps_loss.name] = PSLoss(cfg.src_ps_loss, self.tb_writer)\n        if cfg.cd_ps_loss.use:\n            self.loss_funcs[cfg.cd_ps_loss.name] = PSLoss(cfg.cd_ps_loss, self.tb_writer)\n\n    # NOTE: To save GPU memory, our multi-domain training requires\n    # [1st batch: source-domain forward and backward]-\n    # [2nd batch: cross-domain forward and backward]-\n    # [update model]\n    # So the following three-step framework is not strictly followed.\n    #     pred = self.train_forward(batch)\n    #     loss = self.criterion(batch, pred)\n    #     loss.backward()\n    def train_forward(self, batch):\n        cfg = self.cfg\n        batch = recursive_to_device(batch, self.device)\n        if cfg.cd_ps_loss.use:\n            reid_batch, cd_ps_batch = batch\n        else:\n            reid_batch = batch\n        # Source Loss\n        loss = 0\n        pred = self.model.forward(reid_batch, forward_type=\'ps_reid_parallel\' if cfg.src_ps_loss.use else \'reid\')\n        for loss_cfg in [cfg.id_loss, cfg.tri_loss, cfg.src_ps_loss]:\n            if loss_cfg.use:\n                loss += self.loss_funcs[loss_cfg.name](reid_batch, pred, step=self.trainer.current_step)[\'loss\']\n        if isinstance(loss, torch.Tensor):\n            loss.backward()\n        # Cross-Domain Loss\n        if cfg.cd_ps_loss.use:\n            pred = self.model.forward(cd_ps_batch, forward_type=\'ps\')\n            loss = self.loss_funcs[cfg.cd_ps_loss.name](cd_ps_batch, pred, step=self.trainer.current_step)[\'loss\']\n            if isinstance(loss, torch.Tensor):\n                loss.backward()\n\n    def criterion(self, batch, pred):\n        return 0\n\n    def train_phases(self):\n        cfg = self.cfg.optim\n        if cfg.warmup:\n            cfg.phase = \'warmup\'\n            cfg.dont_test = True\n            self.init_trainer()\n            self.train()\n            cfg.phase = \'normal\'\n            cfg.dont_test = False\n            self.init_trainer()\n            self.load_items(model=True, optimizer=True)\n        self.train()\n\n\nif __name__ == \'__main__\':\n    from ..utils import init_path\n    trainer = EANetTrainer()\n    if trainer.cfg.only_test:\n        trainer.test()\n    else:\n        trainer.train_phases()\n'"
package/optim/lr_scheduler.py,1,"b'from torch.optim.lr_scheduler import _LRScheduler\n\n""""""The get_lr method of _LRScheduler can be interpreted as a math function that \ntakes parameter `self.last_epoch` and returns the current lr.\nNote that `epoch` in pytorch lr scheduler is only local naming, which in fact means\none time of [calling **LR.step()] instead of [going over the whole dataset].\n""""""\n\n\nclass WarmupLR(_LRScheduler):\n    def __init__(self, optimizer, epochs, last_epoch=-1):\n        super(WarmupLR, self).__init__(optimizer, last_epoch)\n        self.epochs = epochs\n        self.final_lrs = [group[\'warmup_final_lr\'] for group in optimizer.param_groups]\n\n    def get_lr(self):\n        """"""A linear function, increasing from base_lr to final_lr.""""""\n        return [base_lr + 1. * self.last_epoch * (final_lr - base_lr) / self.epochs\n                for base_lr, final_lr in zip(self.base_lrs, self.final_lrs)]\n'"
package/optim/optimizer.py,1,"b""import torch.optim as optim\n\n\ndef create_optimizer(param_groups, cfg):\n    if cfg.optimizer == 'sgd':\n        optim_class = optim.SGD\n    elif cfg.optimizer == 'adam':\n        optim_class = optim.Adam\n    else:\n        raise NotImplementedError('Unsupported Optimizer {}'.format(cfg.optimizer))\n    optim_kwargs = dict(weight_decay=cfg.weight_decay)\n    if cfg.optimizer == 'sgd':\n        optim_kwargs['momentum'] = cfg.sgd.momentum\n        optim_kwargs['nesterov'] = cfg.sgd.nesterov\n    return optim_class(param_groups, **optim_kwargs)\n"""
package/optim/reid_trainer.py,2,"b'""""""Manager for common ReID training and optional testing.\nTODO: Separate into a trainer (model, optim, lr) and an evaluator (containing model, eval)\n""""""\nfrom __future__ import print_function\nimport argparse\nimport time\nfrom collections import OrderedDict\nimport os\nimport os.path as osp\nfrom copy import deepcopy\nfrom PIL import Image\nfrom tqdm import tqdm\nfrom torch.nn.parallel import DataParallel\nimport torchvision.transforms.functional as F\n\nfrom ..utils.misc import import_file\nfrom ..utils.file import copy_to\nfrom ..utils.cfg import transfer_items\nfrom ..utils.cfg import overwrite_cfg_file\nfrom ..utils.torch_utils import get_default_device\nfrom ..utils.torch_utils import load_ckpt, save_ckpt\nfrom ..utils.torch_utils import get_optim_lr_str\nfrom ..utils.log import ReDirectSTD\nfrom ..utils.log import time_str as t_str\nfrom ..utils.log import join_str\nfrom ..data.dataloader import create_dataloader\nfrom ..data.create_dataset import dataset_shortcut as d_sc\nfrom ..data.transform import transform\nfrom ..utils.log import score_str as s_str\nfrom ..utils.log import write_to_file\nfrom ..utils.misc import concat_dict_list\nfrom .trainer import Trainer\nfrom ..eval.eval_dataloader import eval_dataloader\nfrom ..eval.extract_feat import extract_batch_feat\nfrom ..eval.extract_feat import extract_dataloader_feat\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'--exp_dir\', type=str, default=\'None\', help=\'[Optional] Directory to store experiment output, including log files and model checkpoint, etc.\')\n    parser.add_argument(\'--cfg_file\', type=str, default=\'None\', help=\'A configuration file.\')\n    parser.add_argument(\'--ow_file\', type=str, default=\'None\', help=\'[Optional] A text file, each line being an item to overwrite the cfg_file.\')\n    parser.add_argument(\'--ow_str\', type=str, default=\'None\', help=""""""[Optional] Items to overwrite the cfg_file. E.g. ""cfg.dataset.train.name = \'market1501\'; cfg.model.em_dim = 256"" """""")\n    args, _ = parser.parse_known_args()\n    return args\n\n\nclass ReIDTrainer(object):\n    """"""Note: This class does not inherit but contains Trainer.""""""\n    def __init__(self, args=None):\n        self.init_cfg(args=args)\n        self.init_log()\n        self.init_device()\n        if self.cfg.only_test:\n            self.init_eval()\n        elif self.cfg.only_infer:\n            self.init_infer()\n        else:\n            self.init_trainer()\n            self.init_eval()\n\n    def init_cfg(self, args=None):\n        """"""args can be parsed from command line, or provided by function caller.""""""\n        if args is None:\n            args = parse_args()\n        exp_dir = args.exp_dir\n        if exp_dir == \'None\':\n            exp_dir = \'exp/\' + osp.splitext(osp.basename(args.cfg_file))[0]\n        # Copy the config file to exp_dir, and then overwrite any configurations provided in ow_file and ow_str\n        cfg_file = osp.join(exp_dir, osp.basename(args.cfg_file))\n        copy_to(args.cfg_file, cfg_file)\n        if args.ow_file != \'None\':\n            # print(\'ow_file is: {}\'.format(args.ow_file))\n            overwrite_cfg_file(cfg_file, ow_file=args.ow_file)\n        if args.ow_str != \'None\':\n            # print(\'ow_str is: {}\'.format(args.ow_str))\n            overwrite_cfg_file(cfg_file, ow_str=args.ow_str)\n        self.cfg = import_file(cfg_file).cfg\n        # Tricky! EasyDict.__setattr__ will transform tuple into list!\n        # print(\'=====> type(cfg.dataset.pap_mask.h_w):\', type(self.cfg.dataset.pap_mask.h_w))\n        self.cfg.log.exp_dir = exp_dir\n\n    def init_log(self):\n        cfg = self.cfg.log\n        # Redirect logs to both console and file.\n        time_str = t_str()\n        ReDirectSTD(osp.join(cfg.exp_dir, \'stdout_{}.txt\'.format(time_str)), \'stdout\', True)\n        ReDirectSTD(osp.join(cfg.exp_dir, \'stderr_{}.txt\'.format(time_str)), \'stderr\', True)\n        print(\'=> Experiment Output Directory: {}\'.format(self.cfg.log.exp_dir))\n        import torch\n        print(\'[PYTORCH VERSION]:\', torch.__version__)\n        cfg.ckpt_file = osp.join(cfg.exp_dir, \'ckpt.pth\')\n        if cfg.use_tensorboard:\n            from tensorboardX import SummaryWriter\n            self.tb_writer = SummaryWriter(log_dir=osp.join(cfg.exp_dir, \'tensorboard\'))\n        else:\n            self.tb_writer = None\n        cfg.score_file = osp.join(cfg.exp_dir, \'score_{}.txt\'.format(time_str))\n\n    def init_device(self):\n        self.device = get_default_device()\n        self.cfg.eval.device = self.device\n\n    def init_trainer(self, samples=None):\n        cfg = self.cfg\n        self.create_train_loader(samples=samples)\n        self.create_model()\n        self.create_optimizer()\n        self.create_lr_scheduler()\n        self.create_loss_funcs()\n        self.trainer = Trainer(self.train_loader, self.train_forward, self.criterion, self.optimizer, self.lr_scheduler,\n                               steps_per_log=cfg.optim.steps_per_log, print_step_log=self.print_log)\n        self.ckpt_objects = {\'model\': self.model, \'optimizer\': self.optimizer, \'lr_scheduler\': self.lr_scheduler}\n        if cfg.optim.resume:\n            self.resume()\n\n    def init_eval(self):\n        if self.cfg.only_test:\n            self.create_model()\n            self.load_items(model=True)\n        self.create_test_loaders()\n\n    def init_infer(self):\n        self.create_model()\n        self.load_items(model=True)\n\n    def load_items(self, model=False, optimizer=False, lr_scheduler=False):\n        """"""To allow flexible multi-stage training.""""""\n        cfg = self.cfg.log\n        objects = {}\n        if model:\n            objects[\'model\'] = self.model\n        if optimizer:\n            objects[\'optimizer\'] = self.optimizer\n        if lr_scheduler:\n            objects[\'lr_scheduler\'] = self.lr_scheduler\n        load_ckpt(objects, cfg.ckpt_file, strict=False)\n\n    def resume(self):\n        """"""This method is ONLY used for resuming training after program breakdown.\n        self.cfg.optim.resume is also ONLY used for this purpose.\n        For finetuning or changing training phase, manually call self.load_items(**kwargs).""""""\n        cfg = self.cfg.log\n        resume_ep, score = load_ckpt(self.ckpt_objects, cfg.ckpt_file)\n        self.trainer.current_ep = resume_ep\n        self.trainer.current_step = resume_ep * len(self.train_loader)\n\n    def create_dataloader(self, mode=None, name=None, split=None, samples=None):\n        """"""Dynamically create any split of any dataset, with dynamic mode. E.g. you can even\n        create a train split with eval mode, for extracting train set features.""""""\n        cfg = self.cfg\n        assert mode in [\'train\', \'cd_train\', \'test\']\n        transfer_items(getattr(cfg.dataset, mode), cfg.dataset)\n        transfer_items(getattr(cfg.dataloader, mode), cfg.dataloader)\n        if name is not None:\n            cfg.dataset.name = name\n        if split is not None:\n            cfg.dataset.split = split\n        # NOTE: `transfer_items`, `cfg.dataset.name = name` etc change cfg in place.\n        # Deepcopy prevents next call of `self.create_dataloader` from modifying the\n        # cfg stored in the previous dataset.\n        dataloader = create_dataloader(deepcopy(cfg.dataloader), deepcopy(cfg.dataset), samples=samples)\n        return dataloader\n\n    def create_test_loaders(self):\n        cfg = self.cfg\n        self.test_loaders = OrderedDict()\n        for i, name in enumerate(cfg.dataset.test.names):\n            q_split = cfg.dataset.test.query_splits[i] if hasattr(cfg.dataset.test, \'query_splits\') else \'query\'\n            self.test_loaders[name] = {\n                \'query\': self.create_dataloader(mode=\'test\', name=name, split=q_split),\n                \'gallery\': self.create_dataloader(mode=\'test\', name=name, split=\'gallery\')\n            }\n\n    def test(self):\n        cfg = self.cfg\n        score_strs = []\n        score_summary = []\n        for test_name, loader_dict in self.test_loaders.items():\n            cfg.eval.test_feat_cache_file = osp.join(cfg.log.exp_dir, \'{}_to_{}_feat_cache.pkl\'.format(d_sc[cfg.dataset.train.name], d_sc[test_name]))\n            cfg.eval.score_prefix = \'{} -> {}\'.format(d_sc[cfg.dataset.train.name], d_sc[test_name]).ljust(12)\n            score_dict = eval_dataloader(self.model_for_eval, loader_dict[\'query\'], loader_dict[\'gallery\'], deepcopy(cfg.eval))\n            score_strs.append(score_dict[\'scores_str\'])\n            score_summary.append(""{}->{}: {} ({})"".format(d_sc[cfg.dataset.train.name], d_sc[test_name], s_str(score_dict[\'cmc_scores\'][0]).replace(\'%\', \'\'), s_str(score_dict[\'mAP\']).replace(\'%\', \'\')))\n        score_str = join_str(score_strs, \'\\n\')\n        score_summary = (\'Epoch {}\'.format(self.trainer.current_ep) if hasattr(self, \'trainer\') else \'Test\').ljust(12) + \', \'.join(score_summary) + \'\\n\'\n        write_to_file(cfg.log.score_file, score_summary, append=True)\n        return score_str\n\n    def infer_one_im(self, im=None, im_path=None, pap_mask=None, squeeze=True):\n        """"""\n        Args:\n            im: an image, numpy array (uint8) with shape [H, W, 3], same format as `Image.open(im_path).convert(""RGB"")`. Exclusive to `im_path`.\n            im_path: an image path. Exclusive to `im`.\n            pap_mask: None, or numpy array (float32) with shape [num_masks, h, w]\n        Returns:\n            dic[\'im_path\']: a list (length=1) if squeeze=False, otherwise a string\n            dic[\'feat\']: numpy array, with shape [1, d] if squeeze=False, otherwise [d]\n            optional dic[\'visible\']: numpy array, with shape [1, num_parts] if squeeze=False, otherwise [num_parts]\n        """"""\n        cfg = self.cfg\n        transfer_items(getattr(cfg.dataset, \'test\'), cfg.dataset)\n        dic = {\'im_path\': im_path}\n        if im is None:\n            dic[\'im\'] = Image.open(im_path).convert(""RGB"")\n        else:\n            assert F._is_pil_image(im), ""Image should be PIL Image. Got {}"".format(type(im))\n            assert len(im.size) == 3, ""Image should be 3-dimensional. Got size {}"".format(im.size)\n            assert im.size[2] == 3, ""Image should be transformed to have 3 channels. Got size {}"".format(im.size)\n            dic[\'im\'] = im\n        if pap_mask is not None:\n            dic[\'pap_mask\'] = pap_mask\n        transform(dic, cfg.dataset)\n\n        # Add the batch dimension\n        dic[\'im_path\'] = [dic[\'im_path\']]\n        dic[\'im\'] = dic[\'im\'].unsqueeze(0)\n        if pap_mask is not None:\n            dic[\'pap_mask\'] = dic[\'pap_mask\'].unsqueeze(0)\n\n        dic = extract_batch_feat(self.model_for_eval, dic, deepcopy(cfg.eval))\n        if squeeze:\n            dic[\'im_path\'] = dic[\'im_path\'][0]\n            dic[\'feat\'] = dic[\'feat\'][0]\n            if \'visible\' in dic:\n                dic[\'visible\'] = dic[\'visible\'][0]\n        return dic\n\n    def infer_im_list(self, im_paths, get_pap_mask=None):\n        """"""\n        Args:\n            im_paths: a list of image paths\n            get_pap_mask: None, or a function taking image path and returns pap mask\n        Returns:\n            ret_dict[\'im_path\']: a list of image paths\n            ret_dict[\'feat\']: numpy array, with shape [num_images, d]\n            optional ret_dict[\'visible\']: numpy array, with shape [num_images, num_parts]\n        """"""\n        dict_list = []\n        for im_path in tqdm(im_paths, desc=\'Extract Feature\', miniters=20, ncols=120, unit=\' images\'):\n            pap_mask = get_pap_mask(im_path) if get_pap_mask is not None else None\n            feat_dict = self.infer_one_im(im_path=im_path, pap_mask=pap_mask, squeeze=False)\n            dict_list.append(feat_dict)\n        ret_dict = concat_dict_list(dict_list)\n        return ret_dict\n\n    def infer_dataloader(self, loader):\n        """"""\n        Args:\n            loader: a dataloader created by self.create_dataloader\n        Returns:\n            ret_dict[\'im_path\']: a list of image paths\n            ret_dict[\'feat\']: numpy array, with shape [num_images, d]\n            optional ret_dict[\'visible\']: numpy array, with shape [num_images, num_parts]\n            optional ret_dict[\'label\']: numpy array, with shape [num_images]\n            optional ret_dict[\'cam\']: numpy array, with shape [num_images]\n        """"""\n        cfg = self.cfg\n        ret_dict = extract_dataloader_feat(self.model_for_eval, loader, deepcopy(cfg.eval))\n        return ret_dict\n\n    def create_train_loader(self):\n        # Create self.train_loader\n        raise NotImplementedError\n\n    def create_model(self):\n        # Create self.model, then\n        #     from package.utils.torch_utils import may_data_parallel\n        #     self.model = may_data_parallel(self.model)\n        #     self.model.to(self.device)\n        raise NotImplementedError\n\n    @property\n    def model_for_eval(self):\n        # Due to an abnormal bug, I decide not to use DataParallel during testing.\n        # The bug case: total im 15913, batch size 32, 15913 % 32 = 9, it\'s ok to use 2 gpus,\n        # but when I used 4 gpus, it threw error at the last batch: [line 83, in parallel_apply\n        # , ... TypeError: forward() takes at least 2 arguments (2 given)]\n        return self.model.module if isinstance(self.model, DataParallel) else self.model\n\n    def set_model_to_train_mode(self):\n        # Default is self.model.train()\n        raise NotImplementedError\n\n    def create_optimizer(self):\n        # Create self.optimizer, then\n        #     recursive_to_device(self.optimizer.state_dict(), self.device)\n        # self.optimizer.to(self.device) # One day, there may be this function in official pytorch\n        raise NotImplementedError\n\n    def create_lr_scheduler(self):\n        # Create self.lr_scheduler, self.epochs.\n        # self.lr_scheduler can be set to None\n        # TODO: a better place to set cfg.epochs?\n        raise NotImplementedError\n\n    def create_loss_funcs(self):\n        # Create self.loss_funcs, an OrderedDict\n        raise NotImplementedError\n\n    def train_forward(self, batch):\n        # pred = self.train_forward(batch)\n        raise NotImplementedError\n\n    def criterion(self, batch, pred):\n        # loss = self.criterion(batch, pred)\n        raise NotImplementedError\n\n    def get_log(self):\n        time_log = \'Ep {}, Step {}, {:.2f}s\'.format(self.trainer.current_ep + 1, self.trainer.current_step + 1, time.time() - self.ep_st)\n        lr_log = \'lr {}\'.format(get_optim_lr_str(self.optimizer))\n        meter_log = join_str([m.avg_str for lf in self.loss_funcs.values() for m in lf.meter_dict.values()], \', \')\n        log = join_str([time_log, lr_log, meter_log], \', \')\n        return log\n\n    def print_log(self):\n        print(self.get_log())\n\n    def may_test(self):\n        cfg = self.cfg.optim\n        score_str = \'\'\n        # You can force not testing by manually setting dont_test=True.\n        if not hasattr(cfg, \'dont_test\') or not cfg.dont_test:\n            if (self.trainer.current_ep % cfg.epochs_per_val == 0) or (self.trainer.current_ep == cfg.epochs) or cfg.trial_run:\n                score_str = self.test()\n        return score_str\n\n    def may_save_ckpt(self, score_str):\n        cfg = self.cfg\n        if not cfg.optim.trial_run:\n            save_ckpt(self.ckpt_objects, self.trainer.current_ep, score_str, cfg.log.ckpt_file)\n\n    def train(self):\n        cfg = self.cfg.optim\n        for _ in range(self.trainer.current_ep, cfg.epochs):\n            self.ep_st = time.time()\n            self.set_model_to_train_mode()\n            self.trainer.train_one_epoch(trial_run_steps=3 if cfg.trial_run else None)\n            score_str = self.may_test()\n            self.may_save_ckpt(score_str)\n'"
package/optim/trainer.py,1,"b""from __future__ import print_function\nimport torch\n\n\nclass Trainer(object):\n    def __init__(self, train_loader, train_forward, criterion, optimizer, lr_scheduler,\n                 steps_per_log=1, print_step_log=None, eps_per_log=1, print_ep_log=None):\n        self.train_loader = train_loader\n        self.train_forward = train_forward\n        self.criterion = criterion\n        self.optimizer = optimizer\n        self.lr_scheduler = lr_scheduler\n        self.steps_per_log = steps_per_log\n        self.print_step_log = print_step_log\n        self.eps_per_log = eps_per_log\n        self.print_ep_log = print_ep_log\n        self.current_step = 0  # will NOT be reset between epochs\n        self.current_ep = 0\n\n    def train_one_step(self, batch):\n        if self.lr_scheduler is not None:\n            self.lr_scheduler.step()\n        self.optimizer.zero_grad()\n        pred = self.train_forward(batch)\n        loss = self.criterion(batch, pred)\n        # some/all backward may be done in self.criterion/self.train_forward\n        if isinstance(loss, torch.Tensor):\n            loss.backward()\n        self.optimizer.step()\n        if ((self.current_step + 1) % self.steps_per_log == 0) and (self.print_step_log is not None):\n            self.print_step_log()\n        self.current_step += 1\n\n    def train_one_epoch(self, trial_run_steps=None):\n        for i, batch in enumerate(self.train_loader):\n            self.train_one_step(batch)\n            # 'Trial Run' to quickly make sure there is no error\n            if (trial_run_steps is not None) and (i + 1 >= trial_run_steps):\n                break\n        if ((self.current_ep + 1) % self.eps_per_log == 0) and (self.print_ep_log is not None):\n            self.print_ep_log()\n        self.current_ep += 1\n\n    def reset_counters(self):\n        self.current_step = 0\n        self.current_ep = 0\n"""
package/utils/__init__.py,0,b''
package/utils/arg_parser.py,0,"b'def str2bool(v):\n    """"""From https://github.com/amdegroot/ssd.pytorch""""""\n    return v.lower() in (""yes"", ""true"", ""t"", ""1"")\n\n\nclass CommaSeparatedSeq(object):\n    def __init__(self, seq_class=tuple, func=int):\n        self.seq_class = seq_class\n        self.func = func\n\n    def __call__(self, s):\n        return self.seq_class([self.func(i) for i in s.split(\',\')])\n'"
package/utils/cfg.py,0,"b'import re\n\n\ndef transfer_items(src, dest):\n    for k, v in src.items():\n        dest[k] = src[k]\n\n\ndef overwrite_cfg_file(cfg_file, ow_str=\'None\', ow_file=\'None\', new_cfg_file=\'None\'):\n    """"""Overwrite some items of a EasyDict defined config file.\n    Args:\n        cfg_file: The original config file\n        ow_str: Mutually exclusive to ow_file. Specify the new items (separated by \';\') to overwrite.\n            E.g. ""cfg.model = \'ResNet-50\'; cfg.im_mean = (0.5, 0.5, 0.5)"".\n        ow_file: A text file, each line being a new item.\n        new_cfg_file: Where to write the updated config. If \'None\', overwrite the original file.\n    """"""\n    with open(cfg_file, \'r\') as f:\n        lines = f.readlines()\n    if ow_str != \'None\':\n        cfgs = ow_str.split(\';\')\n        cfgs = [cfg.strip() for cfg in cfgs if cfg.strip()]\n    else:\n        with open(ow_file, \'r\') as f:\n            cfgs = f.readlines()\n        # Skip empty or comment lines\n        cfgs = [cfg.strip() for cfg in cfgs if cfg.strip() and not cfg.strip().startswith(\'#\')]\n    for cfg in cfgs:\n        key, value = cfg.split(\'=\')\n        key = key.strip()\n        value = value.strip()\n        pattern = r\'{}\\s*=\\s*(.*?)(\\s*)(#.*)?(\\n|$)\'.format(key.replace(\'.\', \'\\.\'))\n        def func(x):\n            # print(r\'=====> {} groups, x.groups(): {}\'.format(len(x.groups()), x.groups()))\n            # x.group(index), index starts from 1\n            # x.group(index) may be `None`\n            # x.group(4) is either \'\\n\' or \'\'\n            return \'{} = {}\'.format(key, value) + (x.group(2) or \'\') + (x.group(3) or \'\') + x.group(4)\n        new_lines = []\n        for line in lines:\n            # Skip empty or comment lines\n            if not line.strip() or line.strip().startswith(\'#\'):\n                new_lines.append(line)\n                continue\n            line = re.sub(pattern, func, line)\n            new_lines.append(line)\n        lines = new_lines\n    if new_cfg_file == \'None\':\n        new_cfg_file = cfg_file\n    with open(new_cfg_file, \'w\') as f:\n        # f.writelines(lines)  # Same effect\n        f.write(\'\'.join(lines))'"
package/utils/file.py,0,"b'import os\nimport os.path as osp\nimport shutil\nimport json\nimport numpy as np\nimport glob\nimport sys\nif sys.version_info[0] == 2:\n    import cPickle as pickle\nelse:\n    import pickle\n\n\ndef may_make_dir(path):\n    """"""\n    Args:\n        path: a dir, e.g. result of `osp.dirname()`\n    Note:\n        `osp.exists(\'\')` returns `False`, while `osp.exists(\'.\')` returns `True`!\n    """"""\n    # This clause has mistakes:\n    # if path is None or \'\':\n\n    if path in [None, \'\']:\n        return\n    if not osp.exists(path):\n        os.makedirs(path)\n\n\ndef load_pickle(path, verbose=True):\n    """"""Check and load pickle object.\n    According to this post: https://stackoverflow.com/a/41733927, cPickle and\n    disabling garbage collector helps with loading speed.""""""\n    assert osp.exists(path), ""File not exists: {}"".format(path)\n    # gc.disable()\n    with open(path, \'rb\') as f:\n        ret = pickle.load(f)\n    # gc.enable()\n    if verbose:\n        print(\'Loaded pickle file {}\'.format(path))\n    return ret\n\n\ndef save_pickle(obj, path, verbose=True):\n    """"""Create dir and save file.""""""\n    may_make_dir(osp.dirname(osp.abspath(path)))\n    with open(path, \'wb\') as f:\n        pickle.dump(obj, f, protocol=2)\n    if verbose:\n        print(\'Pickle file saved to {}\'.format(path))\n\n\ndef load_json(path):\n    """"""Check and load json file.""""""\n    assert osp.exists(path), ""Json file not exists: {}"".format(path)\n    with open(path, \'r\') as f:\n        ret = json.load(f)\n    print(\'Loaded json file {}\'.format(path))\n    return ret\n\n\ndef save_json(obj, path):\n    """"""Create dir and save file.""""""\n    may_make_dir(osp.dirname(osp.abspath(path)))\n    with open(path, \'w\') as f:\n        json.dump(obj, f)\n    print(\'Json file saved to {}\'.format(path))\n\n\ndef read_lines(file):\n    with open(file) as f:\n        lines = f.readlines()\n        lines = [l.strip() for l in lines if l.strip()]\n    return lines\n\n\ndef copy_to(p1, p2):\n    # Only when the copy can go on without error do we create destination dir.\n    if osp.exists(p1):\n        may_make_dir(osp.dirname(p2))\n    shutil.copy(p1, p2)\n\n\ndef get_files_by_pattern(root, pattern=\'a/b/*.ext\', strip_root=False):\n    """"""Optionally to only return matched sub paths.""""""\n    ret = glob.glob(osp.join(root, pattern))\n    if strip_root:\n        ret = [r[len(root) + 1:] for r in ret]\n    return ret\n\n\ndef walkdir(folder, exts=None, sub_path=False, abs_path=False):\n    """"""Walk through each files in a directory.\n    Reference: https://github.com/tqdm/tqdm/wiki/How-to-make-a-great-Progress-Bar\n    Args:\n        exts: file extensions, e.g. \'.jpg\', or [\'.jpg\'] or [\'.jpg\', \'.png\']\n        sub_path: whether to exclude `folder` in the resulting paths, remaining sub paths\n        abs_path: whether to return absolute paths\n    """"""\n    if isinstance(exts, str):\n        exts = [exts]\n    for dirpath, dirs, files in os.walk(folder):\n        for filename in files:\n            if (exts is None) or (os.path.splitext(filename)[1] in exts):\n                path = os.path.join(dirpath, filename)\n                if sub_path:\n                    path = path[len(folder) + 1:]\n                elif abs_path:\n                    path = os.path.abspath(path)\n                yield path\n\n\ndef strip_root(path):\n    """"""a/b/c -> b/c""""""\n    sep = os.sep\n    path = sep.join(path.split(sep)[1:])\n    return path\n'"
package/utils/image.py,0,"b'import numpy as np\nfrom PIL import Image\nimport cv2\nfrom os.path import dirname as ospdn\nfrom .file import may_make_dir\n\n\ndef make_im_grid(ims, n_rows, n_cols, space, pad_val):\n    """"""Make a grid of images with space in between.\n    Args:\n      ims: a list of [3, im_h, im_w] images\n      n_rows: num of rows\n      n_cols: num of columns\n      space: the num of pixels between two images\n      pad_val: scalar, or numpy array with shape [3]; the color of the space\n    Returns:\n      ret_im: a numpy array with shape [3, H, W]\n    """"""\n    assert (ims[0].ndim == 3) and (ims[0].shape[0] == 3)\n    if (n_rows is None) and (n_cols is None):\n        n_cols = int(np.ceil(np.sqrt(len(ims))))\n        n_rows = int(np.ceil(1. * len(ims) / n_cols))\n    else:\n        assert len(ims) <= n_rows * n_cols\n    h, w = ims[0].shape[1:]\n    H = h * n_rows + space * (n_rows - 1)\n    W = w * n_cols + space * (n_cols - 1)\n    if isinstance(pad_val, np.ndarray):\n        # reshape to [3, 1, 1]\n        pad_val = pad_val.flatten()[:, np.newaxis, np.newaxis]\n    ret_im = (np.ones([3, H, W]) * pad_val).astype(ims[0].dtype)\n    for n, im in enumerate(ims):\n        r = n // n_cols\n        c = n % n_cols\n        h1 = r * (h + space)\n        h2 = r * (h + space) + h\n        w1 = c * (w + space)\n        w2 = c * (w + space) + w\n        ret_im[:, h1:h2, w1:w2] = im\n    return ret_im\n\n\ndef read_im(im_path, convert_rgb=True, resize_h_w=(128, 64), transpose=True):\n    im = Image.open(im_path)\n    if convert_rgb:\n        # shape [H, W, 3]\n        im = im.convert(""RGB"")\n    im = np.asarray(im)\n    if resize_h_w is not None and (im.shape[0], im.shape[1]) != resize_h_w:\n        im = cv2.resize(im, resize_h_w[::-1], interpolation=cv2.INTER_LINEAR)\n    if transpose:\n        # shape [3, H, W]\n        im = im.transpose(2, 0, 1)\n    return im\n\n\ndef save_im(im, save_path, transpose=False, check_bound=False):\n    """"""\n    im: (1) shape [3, H, W], transpose should be True\n        (2) shape [H, W, 3], transpose should be False\n        (3) shape [H, W], transpose should be False\n    """"""\n    may_make_dir(ospdn(save_path))\n    if transpose:\n        im = im.transpose(1, 2, 0)\n    if check_bound:\n        im = im.clip(0, 255)\n    im = im.astype(np.uint8)\n    mode = \'L\' if len(im.shape) == 2 else \'RGB\'\n    im = Image.fromarray(im, mode=mode)\n    im.save(save_path)\n\n\ndef heatmap_to_color_im(\n    hmap,\n    normalize=False, min_max_val=None,\n    resize=False, resize_w_h=None,\n    transpose=False\n):\n    """"""\n    Args:\n        hmap: a numpy array with shape [h, w]\n        normalize: whether to normalize the value to range [0, 1]. If `False`,\n            make sure that `hmap` has been in range [0, 1]\n    Return:\n        hmap: shape [h, w, 3] if transpose=False, shape [3, h, w] if transpose=True, with value in range [0, 255], uint8""""""\n    if resize:\n        hmap = cv2.resize(hmap, tuple(resize_w_h), interpolation=cv2.INTER_LINEAR)\n    # normalize to interval [0, 1]\n    if normalize:\n        if min_max_val is None:\n            min_v, max_v = np.min(hmap), np.max(hmap)\n        else:\n            min_v, max_v = min_max_val\n        hmap = (hmap - min_v) / (float(max_v - min_v) + 1e-8)\n    # The `cv2.applyColorMap(gray_im, cv2.COLORMAP_JET)` maps 0 to RED and 1\n    # to BLUE, not normal. So rectify it.\n    hmap = 1 - hmap\n    hmap = (hmap * 255).clip(0, 255).astype(np.uint8)\n    # print(hmap.shape, hmap.dtype, np.min(hmap), np.max(hmap))\n    hmap = cv2.applyColorMap(hmap, cv2.COLORMAP_JET)\n    if transpose:\n        hmap = hmap.transpose(2, 0, 1)\n    return hmap\n\n\ndef restore_im(im, std, mean, transpose=False, resize_w_h=None):\n    """"""Invert the normalization process.\n    Args:\n        im: normalized im with shape [3, h, w]\n    Returns:\n        im: shape [h, w, 3] if transpose=True, shape [3, h, w] if transpose=False, with value in range [0, 255], uint8\n    """"""\n    im = im * np.array(std)[:, np.newaxis, np.newaxis]\n    im = im + np.array(mean)[:, np.newaxis, np.newaxis]\n    im = (im * 255).clip(0, 255).astype(np.uint8)\n    if resize_w_h is not None:\n        im = cv2.resize(im.transpose(1, 2, 0), tuple(resize_w_h), interpolation=cv2.INTER_LINEAR).transpose(2, 0, 1)\n    if transpose:\n        im = np.transpose(im, [1, 2, 0])\n    return im\n'"
package/utils/init_path.py,1,"b'from __future__ import print_function\nimport sys\nimport os\nfrom os.path import abspath as ospap\nfrom os.path import dirname as ospdn\n\n\ndef get_conda_paths():\n    return [p for p in sys.path if (\'conda\' in p)]\n\n\ndef get_non_conda_paths():\n    return [p for p in sys.path if (\'conda\' not in p)]\n\n\ndef mv_conda_paths_to_front():\n    """"""When using conda, we move all conda package paths to the front.\n    Otherwise, some paths (e.g. $HOME/.local/python*) may be interrupting.""""""\n    sys.path = get_conda_paths() + get_non_conda_paths()\n\n\ndef insert_package_path():\n    """"""This init_path.py should be placed at `${PROJECT_DIR}/package/utils/init_path.py`.\n    Here we add `${PROJECT_DIR}/package` to `sys.path`.\n    If you want to move this file to other places, remember to change the following path to insert.\n    """"""\n    sys.path.insert(0, ospdn(ospdn(ospdn(ospap(__file__)))))\n\n\ndef init_path():\n    print(\'[PYTHONPATH]:\\n\\t{}\'.format(os.environ.get(\'PYTHONPATH\', \'\')))\n    print(\'[Original sys.path]:\\n\\t{}\'.format(\'\\n\\t\'.join(sys.path)))\n    mv_conda_paths_to_front()\n    insert_package_path()\n    print(\'[Final sys.path]:\\n\\t{}\'.format(\'\\n\\t\'.join(sys.path)))\n    # import torch\n    # print(\'[PYTORCH VERSION]:\', torch.__version__)\n\n\ninit_path()'"
package/utils/log.py,0,"b'import sys\nimport os\nimport os.path as osp\nimport datetime\nfrom .file import may_make_dir\n\n\nclass ReDirectSTD(object):\n    """"""Modified from Tong Xiao\'s `Logger` in open-reid.\n    This class overwrites sys.stdout or sys.stderr, so that console logs can\n    also be written to file.\n    Args:\n      fpath: file path\n      console: one of [\'stdout\', \'stderr\']\n      immediately_visible: If `False`, the file is opened only once and closed\n        after exiting. In this case, the message written to file may not be\n        immediately visible (Because the file handle is occupied by the\n        program?). If `True`, each writing operation of the console will\n        open, write to, and close the file. If your program has tons of writing\n        operations, the cost of opening and closing file may be obvious. (?)\n    Usage example:\n      `ReDirectSTD(\'stdout.txt\', \'stdout\', False)`\n      `ReDirectSTD(\'stderr.txt\', \'stderr\', False)`\n    NOTE: File will be deleted if already existing. Log dir and file is created\n      lazily -- if no message is written, the dir and file will not be created.\n    """"""\n\n    def __init__(self, fpath=None, console=\'stdout\', immediately_visible=False):\n        assert console in [\'stdout\', \'stderr\']\n        self.console = sys.stdout if console == \'stdout\' else sys.stderr\n        self.file = fpath\n        self.f = None\n        self.immediately_visible = immediately_visible\n        if fpath is not None:\n            # Remove existing log file.\n            if osp.exists(fpath):\n                os.remove(fpath)\n\n        # Overwrite\n        if console == \'stdout\':\n            sys.stdout = self\n        else:\n            sys.stderr = self\n\n    def __del__(self):\n        self.close()\n\n    def __enter__(self):\n        pass\n\n    def __exit__(self, *args):\n        self.close()\n\n    def write(self, msg):\n        self.console.write(msg)\n        if self.file is not None:\n            may_make_dir(os.path.dirname(osp.abspath(self.file)))\n            if self.immediately_visible:\n                with open(self.file, \'a\') as f:\n                    f.write(msg)\n            else:\n                if self.f is None:\n                    self.f = open(self.file, \'w\')\n                self.f.write(msg)\n        self.flush()\n\n    def flush(self):\n        self.console.flush()\n        if self.f is not None:\n            self.f.flush()\n            import os\n            os.fsync(self.f.fileno())\n\n    def close(self):\n        self.console.close()\n        if self.f is not None:\n            self.f.close()\n\n\ndef time_str(fmt=None):\n    if fmt is None:\n        fmt = \'%Y-%m-%d_%H-%M-%S\'\n    return datetime.datetime.today().strftime(fmt)\n\n\ndef array_str(array, fmt=\'{:.2f}\', sep=\', \', with_boundary=True):\n    """"""String of a 1-D tuple, list, or numpy array containing digits.""""""\n    ret = sep.join([fmt.format(float(x)) for x in array])\n    if with_boundary:\n        ret = \'[\' + ret + \']\'\n    return ret\n\n\ndef array_2d_str(array, fmt=\'{:.2f}\', sep=\', \', row_sep=\'\\n\', with_boundary=True):\n    """"""String of a 2-D tuple, list, or numpy array containing digits.""""""\n    ret = row_sep.join([array_str(x, fmt=fmt, sep=sep, with_boundary=with_boundary) for x in array])\n    if with_boundary:\n        ret = \'[\' + ret + \']\'\n    return ret\n\n\ndef tight_float_str(x, fmt=\'{:.4f}\'):\n    return fmt.format(x).rstrip(\'0\').rstrip(\'.\')\n\n\ndef score_str(x):\n    return \'{:5.1%}\'.format(x).rjust(6)\n\n\ndef join_str(sequence, sep):\n    sequence = [s for s in sequence if s != \'\']\n    return sep.join(sequence)\n\n\ndef write_to_file(file, msg, append=True):\n    with open(file, \'a\' if append else \'w\') as f:\n        f.write(msg)\n'"
package/utils/meter.py,0,"b'class AverageMeter(object):\n    """"""Modified from https://github.com/Cysu/open-reid.\n    Computes and stores the average and current value""""""\n\n    def __init__(self, name=\'\', fmt=\'{:.4f}\'):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n        self.name = name\n        self.fmt = fmt\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = float(self.sum) / (self.count + 1e-20)\n\n    @property\n    def val_str(self):\n        return self.name + \' \' + self.fmt.format(self.val)\n\n    @property\n    def avg_str(self):\n        return self.name + \' \' + self.fmt.format(self.avg)\n\n\nclass RunningAverageMeter(object):\n    """"""Computes and stores the running average and current value""""""\n\n    def __init__(self, hist=0.99, name=\'\', fmt=\'{:.4f}\'):\n        self.val = None\n        self.avg = None\n        self.hist = hist\n        self.name = name\n        self.fmt = fmt\n\n    def reset(self):\n        self.val = None\n        self.avg = None\n\n    def update(self, val):\n        if self.avg is None:\n            self.avg = val\n        else:\n            self.avg = self.avg * self.hist + val * (1 - self.hist)\n        self.val = val\n\n    @property\n    def val_str(self):\n        return self.name + \' \' + self.fmt.format(self.val)\n\n    @property\n    def avg_str(self):\n        return self.name + \' \' + self.fmt.format(self.avg)\n\n\nclass RecentAverageMeter(object):\n    """"""Stores and computes the average of recent values.""""""\n\n    def __init__(self, hist_size=100, name=\'\', fmt=\'{:.4f}\'):\n        self.hist_size = hist_size\n        self.fifo = []\n        self.val = 0\n        self.name = name\n        self.fmt = fmt\n\n    def reset(self):\n        self.fifo = []\n        self.val = 0\n\n    def update(self, val):\n        self.val = val\n        self.fifo.append(val)\n        if len(self.fifo) > self.hist_size:\n            del self.fifo[0]\n\n    @property\n    def avg(self):\n        assert len(self.fifo) > 0\n        return float(sum(self.fifo)) / len(self.fifo)\n\n    @property\n    def val_str(self):\n        return self.name + \' \' + self.fmt.format(self.val)\n\n    @property\n    def avg_str(self):\n        return self.name + \' \' + self.fmt.format(self.avg)'"
package/utils/misc.py,0,"b'import numpy as np\nimport itertools\n\n\ndef concat_dict_list(dict_list):\n    ret_dict = {}\n    keys = dict_list[0].keys()\n    for k in keys:\n        if isinstance(dict_list[0][k], list):\n            ret_dict[k] = list(itertools.chain.from_iterable([dict_[k] for dict_ in dict_list]))\n        elif isinstance(dict_list[0][k], np.ndarray):\n            ret_dict[k] = np.concatenate([dict_[k] for dict_ in dict_list])\n        else:\n            raise NotImplementedError\n    return ret_dict\n\n\ndef import_file(path):\n    import sys, importlib\n    import os.path as osp\n    path_to_insert = osp.dirname(osp.abspath(osp.expanduser(path)))\n    sys.path.insert(0, path_to_insert)\n    imported = importlib.import_module(osp.splitext(osp.basename(path))[0])\n    # NOTE: sys.path may be modified inside the imported file. If path_to_insert\n    # is not added to sys.path at any index inside the imported file, this remove()\n    # can exactly cancel the previous insert(). Otherwise, the element order inside\n    # sys.path may not be desired by the imported file.\n    sys.path.remove(path_to_insert)\n    return imported\n'"
package/utils/model.py,1,"b'import torch.nn as nn\n\n\ndef init_classifier(m):\n    if isinstance(m, nn.Linear):\n        nn.init.normal_(m.weight, std=0.001)\n        if m.bias is not None:\n            nn.init.constant_(m.bias, 0)\n\n\ndef create_embedding(in_dim=None, out_dim=None):\n    layers = [\n        nn.Linear(in_dim, out_dim),\n        nn.BatchNorm1d(out_dim),\n        nn.ReLU(inplace=True)\n    ]\n    return nn.Sequential(*layers)'"
package/utils/rank_list.py,0,"b'import numpy as np\nfrom .image import read_im, save_im, make_im_grid\n\n\ndef add_border(im, border_width, value):\n    """"""Add color border around an image. The resulting image size is not changed.\n    Args:\n        im: numpy array with shape [3, im_h, im_w]\n        border_width: scalar, measured in pixel\n        value: scalar, or numpy array with shape [3]; the color of the border\n    Returns:\n        im: numpy array with shape [3, im_h, im_w]\n    """"""\n    assert (im.ndim == 3) and (im.shape[0] == 3)\n    im = np.copy(im)\n\n    if isinstance(value, np.ndarray):\n        # reshape to [3, 1, 1]\n        value = value.flatten()[:, np.newaxis, np.newaxis]\n    im[:, :border_width, :] = value\n    im[:, -border_width:, :] = value\n    im[:, :, :border_width] = value\n    im[:, :, -border_width:] = value\n\n    return im\n\n\ndef get_rank_list(dist_vec, q_id=None, q_cam=None, g_ids=None, g_cams=None,\n                  rank_list_size=10, skip_same_id_same_cam=True, id_aware=False):\n    """"""Get the ranking list of a query image\n    Args:\n        dist_vec: a numpy array with shape [num_gallery_images], the distance between the query image and all gallery images\n        q_id: optional, a scalar, query id\n        q_cam: optional, a scalar, query camera\n        g_ids: optional, a numpy array with shape [num_gallery_images], gallery ids\n        g_cams: optional, a numpy array with shape [num_gallery_images], gallery cameras\n        rank_list_size: a scalar, the number of images to show in a rank list\n        skip_same_id_same_cam: Skip gallery images with same id and same camera as query\n        id_aware: whether q_id, q_cam, g_ids, g_cams are known\n    Returns:\n        rank_list: a list, the indices of gallery images to show\n        same_id: None, or a list, len(same_id) = rank_list, whether each ranked image is with same id as query\n    """"""\n    sort_inds = np.argsort(dist_vec)\n\n    if not id_aware:\n        rank_list = sort_inds[:rank_list_size]\n        return rank_list, None\n\n    assert q_id is not None\n    assert q_cam is not None\n    assert g_ids is not None\n    assert g_cams is not None\n\n    rank_list = []\n    same_id = []\n    i = 0\n    for ind in sort_inds:\n        g_id, g_cam = g_ids[ind], g_cams[ind]\n        # Skip gallery images with same id and same camera as query\n        if (q_id == g_id) and (q_cam == g_cam) and skip_same_id_same_cam:\n            continue\n        same_id.append(q_id == g_id)\n        rank_list.append(ind)\n        i += 1\n        if i >= rank_list_size:\n            break\n    return rank_list, same_id\n\n\ndef save_rank_list_to_im(rank_list, q_im_path, g_im_paths, save_path, same_id=None, resize_h_w=(128, 64)):\n    """"""Save a query and its rank list as an image.\n    Args:\n        rank_list: a list, the indices of gallery images to show\n        q_im_path: query image path\n        g_im_paths: ALL gallery image paths\n        save_path: path to save the query and its rank list as an image\n        same_id: optional, a list, len(same_id) = rank_list, whether each ranked image is with same id as query\n        resize_h_w: size of each image in the rank list\n    """"""\n    ims = [read_im(q_im_path, convert_rgb=True, resize_h_w=resize_h_w, transpose=True)]\n    for i, ind in enumerate(rank_list):\n        im = read_im(g_im_paths[ind], convert_rgb=True, resize_h_w=resize_h_w, transpose=True)\n        if same_id is not None:\n            # Add green boundary to true positive, red to false positive\n            color = np.array([0, 255, 0]) if same_id[i] else np.array([255, 0, 0])\n            im = add_border(im, 3, color)\n        ims.append(im)\n    im = make_im_grid(ims, 1, len(rank_list) + 1, 8, 255)\n    save_im(im, save_path, transpose=True)\n'"
package/utils/torch_utils.py,21,"b'from __future__ import print_function\nimport os.path as osp\nimport torch\nfrom torch.nn.parallel import DataParallel\nfrom .file import may_make_dir\n\n\ndef get_default_device():\n    """"""Get default device for `*.to(device)`.""""""\n    device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")\n    return device\n\n\ndef recursive_to_device(input, device):\n    """"""NOTE: If input is dict/list/tuple, it is changed in place.""""""\n    if isinstance(input, torch.Tensor):\n        # print(\'=> IS torch.Tensor\')\n        # print(\'=> input.device before to_device: {}\'.format(input.device))\n        input = input.to(device)\n        # print(\'=> input.device after to_device: {}\'.format(input.device))\n    elif isinstance(input, dict):\n        for k, v in input.items():\n            input[k] = recursive_to_device(v, device)\n    elif isinstance(input, (list, tuple)):\n        input = [recursive_to_device(v, device) for v in input]\n    return input\n\n\nclass TransparentDataParallel(DataParallel):\n\n    def __getattr__(self, name):\n        """"""Forward attribute access to its wrapped module.""""""\n        try:\n            return super(TransparentDataParallel, self).__getattr__(name)\n        except AttributeError:\n            return getattr(self.module, name)\n\n    def state_dict(self, *args, **kwargs):\n        """"""We only save/load state_dict of the wrapped model. This allows loading\n        state_dict of a DataParallelSD model into a non-DataParallel model.""""""\n        return self.module.state_dict(*args, **kwargs)\n\n    def load_state_dict(self, *args, **kwargs):\n        return self.module.load_state_dict(*args, **kwargs)\n\n\ndef may_data_parallel(model):\n    """"""When there is no more than one gpu, don\'t wrap the model, for more\n    flexibility in forward function.""""""\n    if torch.cuda.device_count() > 1:\n        model = TransparentDataParallel(model)\n    return model\n\n\ndef load_state_dict(model, src_state_dict, fold_bnt=True):\n    """"""Copy parameters and buffers from `src_state_dict` into `model` and its\n    descendants. The `src_state_dict.keys()` NEED NOT exactly match\n    `model.state_dict().keys()`. For dict key mismatch, just\n    skip it; for copying error, just output warnings and proceed.\n\n    Arguments:\n        model: A torch.nn.Module object.\n        src_state_dict (dict): A dict containing parameters and persistent buffers.\n    Note:\n        This is modified from torch.nn.modules.module.load_state_dict(), to make\n        the warnings and errors more detailed.\n    """"""\n    from torch.nn import Parameter\n\n    dest_state_dict = model.state_dict()\n    for name, param in src_state_dict.items():\n        if name not in dest_state_dict:\n            continue\n        if isinstance(param, Parameter):\n            # backwards compatibility for serialized parameters\n            param = param.data\n        try:\n            dest_state_dict[name].copy_(param)\n        except Exception, msg:\n            print(""Warning: Error occurs when copying \'{}\': {}"".format(name, str(msg)))\n\n    # New version of BN has buffer `num_batches_tracked`, which is not used\n    # for normal BN, so we fold all these missing keys into one line\n    def _fold_nbt(keys):\n        nbt_keys = [s for s in keys if s.endswith(\'.num_batches_tracked\')]\n        if len(nbt_keys) > 0:\n            keys = [s for s in keys if not s.endswith(\'.num_batches_tracked\')] + [\'num_batches_tracked  x{}\'.format(len(nbt_keys))]\n        return keys\n\n    src_missing = set(dest_state_dict.keys()) - set(src_state_dict.keys())\n    if len(src_missing) > 0:\n        print(""Keys not found in source state_dict: "")\n        if fold_bnt:\n            src_missing = _fold_nbt(src_missing)\n        for n in src_missing:\n            print(\'\\t\', n)\n\n    dest_missing = set(src_state_dict.keys()) - set(dest_state_dict.keys())\n    if len(dest_missing) > 0:\n        print(""Keys not found in destination state_dict: "")\n        if fold_bnt:\n            dest_missing = _fold_nbt(dest_missing)\n        for n in dest_missing:\n            print(\'\\t\', n)\n\n\ndef load_ckpt(objects, ckpt_file, strict=True):\n    """"""Load state_dict\'s of modules/optimizers/lr_schedulers from file.\n    Args:\n        objects: A dict, which values are either\n            torch.nn.optimizer\n            or torch.nn.Module\n            or torch.optim.lr_scheduler._LRScheduler\n            or None\n        ckpt_file: The file path.\n    """"""\n    assert osp.exists(ckpt_file), ""ckpt_file {} does not exist!"".format(ckpt_file)\n    assert osp.isfile(ckpt_file), ""ckpt_file {} is not file!"".format(ckpt_file)\n    ckpt = torch.load(ckpt_file, map_location=(lambda storage, loc: storage))\n    for name, obj in objects.items():\n        if obj is not None:\n            # Only nn.Module.load_state_dict has this keyword argument\n            if not isinstance(obj, torch.nn.Module) or strict:\n                obj.load_state_dict(ckpt[\'state_dicts\'][name])\n            else:\n                load_state_dict(obj, ckpt[\'state_dicts\'][name])\n    objects_str = \', \'.join(objects.keys())\n    msg = \'=> Loaded [{}] from {}, epoch {}, score:\\n{}\'.format(objects_str, ckpt_file, ckpt[\'epoch\'], ckpt[\'score\'])\n    print(msg)\n    return ckpt[\'epoch\'], ckpt[\'score\']\n\n\ndef save_ckpt(objects, epoch, score, ckpt_file):\n    """"""Save state_dict\'s of modules/optimizers/lr_schedulers to file.\n    Args:\n        objects: A dict, which members are either\n            torch.nn.optimizer\n            or torch.nn.Module\n            or torch.optim.lr_scheduler._LRScheduler\n            or None\n        epoch: the current epoch number\n        score: the performance of current model\n        ckpt_file: The file path.\n    Note:\n        torch.save() reserves device type and id of tensors to save, so when\n        loading ckpt, you have to inform torch.load() to load these tensors to\n        cpu or your desired gpu, if you change devices.\n    """"""\n    state_dicts = {name: obj.state_dict() for name, obj in objects.items() if obj is not None}\n    ckpt = dict(state_dicts=state_dicts,\n                epoch=epoch,\n                score=score)\n    may_make_dir(osp.dirname(ckpt_file))\n    torch.save(ckpt, ckpt_file)\n    msg = \'=> Checkpoint Saved to {}\'.format(ckpt_file)\n    print(msg)\n\n\ndef only_keep_model(ori_ckpt_path, new_ckpt_path):\n    """"""Remove optimizer and lr scheduler in the checkpoint, reducing file size.""""""\n    ckpt = torch.load(ori_ckpt_path, map_location=(lambda storage, loc: storage))\n    ckpt[\'state_dicts\'] = {\'model\': ckpt[\'state_dicts\'][\'model\']}\n    may_make_dir(osp.dirname(new_ckpt_path))\n    torch.save(ckpt, new_ckpt_path)\n    print(\'=> Removed optimizer and lr scheduler of ckpt {} and save it to {}\'.format(ori_ckpt_path, new_ckpt_path))\n\n\ndef get_optim_lr_str(optimizer):\n    lr_strs = [\'{:.6f}\'.format(g[\'lr\']).rstrip(\'0\') for g in optimizer.param_groups]\n    lr_str = \', \'.join(lr_strs)\n    return lr_str\n'"
script/exp/infer_dataloader_example.py,0,"b'""""""Example of eanet_trainer.infer_dataloader(loader).\npython script/exp/infer_dataloader_example.py\n""""""\nfrom __future__ import print_function\n\n\nimport sys\nsys.path.insert(0, \'.\')\nfrom copy import deepcopy\nfrom package.optim.eanet_trainer import EANetTrainer\nfrom package.eval.eval_dataloader import eval_feat\n\n\ndef _print_stat(dic):\n    print(\'=> Eval Statistics:\')\n    print(\'\\tdic.keys():\', dic.keys())\n    print(""\\tdic[\'q_feat\'].shape:"", dic[\'q_feat\'].shape)\n    print(""\\tdic[\'q_label\'].shape:"", dic[\'q_label\'].shape)\n    print(""\\tdic[\'q_cam\'].shape:"", dic[\'q_cam\'].shape)\n    print(""\\tdic[\'g_feat\'].shape:"", dic[\'g_feat\'].shape)\n    print(""\\tdic[\'g_label\'].shape:"", dic[\'g_label\'].shape)\n    print(""\\tdic[\'g_cam\'].shape:"", dic[\'g_cam\'].shape)\n\n\ndef main():\n    from easydict import EasyDict\n\n    args = EasyDict()\n    args.exp_dir = \'exp/try_pcb_trained_on_market1501_for_reid_feature\'  # There should be the corresponding `ckpt.pth` in it\n    args.cfg_file = \'package/config/default.py\'  # Set this `${EANet_PROJECT_DIR}`\n    args.ow_file = \'paper_configs/PCB.txt\'  # Set this `${EANet_PROJECT_DIR}`\n    args.ow_str = ""cfg.only_infer = True""\n\n    eanet_trainer = EANetTrainer(args=args)\n    q_loader = eanet_trainer.create_dataloader(\'test\', name=\'market1501\', split=\'query\')\n    g_loader = eanet_trainer.create_dataloader(\'test\', name=\'market1501\', split=\'gallery\')\n    q_feat = eanet_trainer.infer_dataloader(q_loader)\n    g_feat = eanet_trainer.infer_dataloader(g_loader)\n    dic = {\n        \'q_feat\': q_feat[\'feat\'],\n        \'q_label\': q_feat[\'label\'],\n        \'q_cam\': q_feat[\'cam\'],\n        \'g_feat\': g_feat[\'feat\'],\n        \'g_label\': g_feat[\'label\'],\n        \'g_cam\': g_feat[\'cam\'],\n    }\n    if \'visible\' in q_feat:\n        dic[\'q_visible\'] = q_feat[\'visible\']\n    if \'visible\' in g_feat:\n        dic[\'g_visible\'] = g_feat[\'visible\']\n    _print_stat(dic)\n    return eval_feat(dic, deepcopy(eanet_trainer.cfg.eval))\n\n\nif __name__ == \'__main__\':\n    main()\n'"
script/exp/remove_optim_lr_s_in_ckpt.py,0,"b'""""""Remove optimizer and lr scheduler in checkpoint to reduce file size.""""""\nfrom __future__ import print_function\n\n\nimport sys\nsys.path.insert(0, \'.\')\nfrom package.utils.file import get_files_by_pattern\nfrom package.utils.torch_utils import only_keep_model\n\n\nori_ckpt_paths = get_files_by_pattern(\'exp/eanet/test_paper_models\', \'*/*/*.pth\')\nnew_ckpt_paths = ori_ckpt_paths\n\nfor ori_path, new_path in zip(ori_ckpt_paths, new_ckpt_paths):\n    only_keep_model(ori_path, new_path)\n'"
script/exp/visualize_rank_list.py,0,"b'""""""Visualize retrieving results. This script demonstrates how to use a trained model for inference.""""""\nfrom __future__ import print_function\n\n\nimport sys\nsys.path.insert(0, \'.\')\nimport os\nimport argparse\nimport numpy as np\nfrom package.utils.file import walkdir\nfrom package.utils.arg_parser import str2bool\nfrom package.optim.eanet_trainer import EANetTrainer\nfrom package.eval.np_distance import compute_dist, compute_dist_with_visibility\nfrom package.utils.rank_list import get_rank_list, save_rank_list_to_im\n\n# NOTE: If you use id and camera info, you have to define parse_im_path for your images\n# E.g.\nfrom package.data.datasets.market1501 import Market1501\nparse_im_path = Market1501.parse_im_path\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'--q_im_dir\', type=str, default=\'None\', help=\'Directory of query images.\')\n    parser.add_argument(\'--g_im_dir\', type=str, default=\'None\', help=\'Directory of gallery images.\')\n    parser.add_argument(\'--num_queries\', type=int, default=16, help=\'How many query images to visualize.\')\n    parser.add_argument(\'--rank_list_size\', type=int, default=10, help=\'How many top gallery images to visualize for each query.\')\n    parser.add_argument(\'--save_dir\', type=str, default=\'None\', help=\'Where to save visualization result.\')\n    parser.add_argument(\'--id_aware\', type=str2bool, default=False,\n                        help=\'Whether id and camera info are known for your images. If known, each gallery image in \'\n                             \'rank list will have either green or red boundary, denoting same or different id as query.\')\n    parser.add_argument(\'--pap_mask_provided\', type=str2bool, default=False,\n                        help=\'Do you provide pap_mask or not. If not, all PAP based models are not available \'\n                             \'for inference, and you can only use GlobalPool and PCB.\')\n    args, _ = parser.parse_known_args()\n    return args\n\n\ndef main(args):\n    q_im_paths = sorted(list(walkdir(args.q_im_dir, exts=[\'.jpg\', \'.png\'])))\n    q_im_paths = np.random.RandomState(1).choice(q_im_paths, args.num_queries)\n    g_im_paths = sorted(list(walkdir(args.g_im_dir, exts=[\'.jpg\', \'.png\'])))\n    trainer = EANetTrainer()\n    ###################\n    # NOTE: If you want to use Part Aligned Pooling, you have to provide pap_mask. E.g.\n    market1501_loader = trainer.create_dataloader(\'test\', \'market1501\', \'query\')\n    def get_pap_mask(im_path):\n        return market1501_loader.dataset.get_pap_mask(\'/\'.join(im_path.split(\'/\')[2:]))\n    ###################\n    q_feat = trainer.infer_im_list(q_im_paths, get_pap_mask=get_pap_mask if args.pap_mask_provided else None)\n    g_feat = trainer.infer_im_list(g_im_paths, get_pap_mask=get_pap_mask if args.pap_mask_provided else None)\n    if \'visible\' in q_feat:\n        dist_mat = compute_dist_with_visibility(q_feat[\'feat\'], g_feat[\'feat\'], q_feat[\'visible\'], g_feat[\'visible\'])\n    else:\n        dist_mat = compute_dist(q_feat[\'feat\'], g_feat[\'feat\'])\n    g_ids, g_cams = zip(*[parse_im_path(g_im_path) for g_im_path in g_im_paths]) if args.id_aware else (None, None)\n    for i, (dist_vec, q_im_path) in enumerate(zip(dist_mat, q_im_paths)):\n        q_id, q_cam = parse_im_path(q_im_path) if args.id_aware else (None, None)\n        rank_list, same_id = get_rank_list(dist_vec, q_id, q_cam, g_ids, g_cams, args.rank_list_size, id_aware=args.id_aware)\n        save_rank_list_to_im(rank_list, q_im_path, g_im_paths, os.path.join(args.save_dir, q_im_path.replace(\'/\', \'_\')), same_id=same_id)\n\n\nif __name__ == \'__main__\':\n    main(parse_args())\n'"
package/data/datasets/__init__.py,0,b''
package/data/datasets/coco.py,0,"b'from __future__ import print_function\nimport os.path as osp\nfrom os.path import basename as ospbn\nimport numpy as np\nimport itertools\nfrom ..dataset import Dataset\nfrom ...utils.file import get_files_by_pattern\nfrom ...utils.file import save_pickle, load_pickle\n\n\nkpt_names = [\n    \'nose\',\n    \'left_eye\', \'right_eye\',\n    \'left_ear\', \'right_ear\',\n    \'left_shoulder\', \'right_shoulder\',\n    \'left_elbow\', \'right_elbow\',\n    \'left_wrist\', \'right_wrist\',\n    \'left_hip\', \'right_hip\',\n    \'left_knee\', \'right_knee\',\n    \'left_ankle\', \'right_ankle\'\n]\nkpt_name_to_ind = dict(zip(kpt_names, range(len(kpt_names))))\n\n\nclass COCO(Dataset):\n    has_pap_mask = False\n    has_ps_label = True\n    im_root = \'images\'\n    split_spec = {\n        \'train\': {\'patterns\': [\'{}/train/*.jpg\'.format(im_root)]},\n        \'val\': {\'patterns\': [\'{}/val/*.jpg\'.format(im_root)]},\n        \'train_market1501_style\': {\'patterns\': [\'{}/train/*.jpg\'.format(im_root), \'{}/train_market1501_style/*.jpg\'.format(im_root)]},  # Original COCO images are also used in paper\'s PAP-StC-PS\n        \'train_cuhk03_style\': {\'patterns\': [\'{}/train/*.jpg\'.format(im_root), \'{}/train_cuhk03_style/*.jpg\'.format(im_root)]},\n        \'train_duke_style\': {\'patterns\': [\'{}/train/*.jpg\'.format(im_root), \'{}/train_duke_style/*.jpg\'.format(im_root)]},\n    }\n\n    def get_pap_mask(self, im_path):\n        err_msg = \'If you want to generate pap mask from keypoint for COCO, you have to \' \\\n                  \'deal with the file structure difference between im_path_to_kpt.pkl and \' \\\n                  \'im_name_to_kpt.pkl. So implement it here.\'\n        raise NotImplementedError(err_msg)\n\n    def ratio_ok(self, kpts, im_h_w):\n        """"""Select those samples (1) with nearly full body and (2) in upright pose, so that\n        we can lazily resize them as for ReID images without over distorting body ratio.\n        """"""\n        def _any_visible(kpts, kpt_names):\n            inds = [kpt_name_to_ind[n] for n in kpt_names]\n            kpts = kpts[inds]\n            return np.max(kpts[:, 2]) > 0\n        main_groups = [[\'nose\', \'left_eye\', \'right_eye\', \'left_ear\', \'right_ear\'],\n                       [\'left_shoulder\', \'right_shoulder\'],\n                       [\'left_hip\', \'right_hip\'],\n                       [\'left_knee\', \'right_knee\'],\n                       [\'left_ankle\', \'right_ankle\'],\n                       ]\n        # n_visible_kpts = np.sum(kpts[:, 2] > 0)\n        n_visible_groups = np.sum([_any_visible(kpts, g) for g in main_groups])\n        h, w = im_h_w\n        return (n_visible_groups >= 4) and (1. * h / w > 1.8)\n\n    def filter_ratio(self, im_paths):\n        im_name_to_kpt = load_pickle(osp.join(self.root, \'im_name_to_kpt.pkl\'))\n        im_name_to_h_w = load_pickle(osp.join(self.root, \'im_name_to_h_w.pkl\'))\n        im_paths = [imp for imp in im_paths if self.ratio_ok(im_name_to_kpt[ospbn(imp)], im_name_to_h_w[ospbn(imp)])]\n        return im_paths\n\n    def save_split(self, spec, save_path):\n        cfg = self.cfg\n        im_paths = sorted(itertools.chain.from_iterable(\n            [get_files_by_pattern(self.root, pattern=p, strip_root=True)\n             for p in spec[\'patterns\']]\n        ))\n        # TODO: Don\'t filter in the future, so that we can perform standard segmentation training.\n        print(\'Before filtering: Split {} of {} has {} images\'.format(cfg.split, self.__class__.__name__, len(im_paths)))\n        im_paths = self.filter_ratio(im_paths)\n        print(\'After filtering: Split {} of {} has {} images\'.format(cfg.split, self.__class__.__name__, len(im_paths)))\n        samples = [{\'im_path\': im_path} for im_path in im_paths]\n        save_pickle(samples, save_path)\n\n    def _get_ps_label_path(self, im_path):\n        cfg = self.cfg\n        path = im_path\n        # Style transferred COCO images use the same ps labels as original COCO images.\n        splits = [\'train_market1501_style\', \'train_cuhk03_style\', \'train_duke_style\']\n        for split in splits:\n            path = path.replace(split, \'train\')\n        path = path.replace(self.im_root, \'masks_7_parts\')\n        path = path.replace(\'.jpg\', \'.png\')\n        path = osp.join(self.root, path)\n        return path\n'"
package/data/datasets/cuhk03_np_detected_jpg.py,0,"b'import os.path as osp\nfrom PIL import Image\nimport numpy as np\nfrom scipy.misc import imsave\nfrom tqdm import tqdm\nimport shutil\nfrom ...utils.file import walkdir\nfrom ...utils.file import may_make_dir\nfrom .market1501 import Market1501\n\n\nclass CUHK03NpDetectedJpg(Market1501):\n    has_pap_mask = True\n    has_ps_label = True\n    png_im_root = \'cuhk03-np\'\n    im_root = \'cuhk03-np-jpg\'\n    split_spec = {\n        \'train\': {\'pattern\': \'{}/detected/bounding_box_train/*.jpg\'.format(im_root), \'map_label\': True},\n        \'query\': {\'pattern\': \'{}/detected/query/*.jpg\'.format(im_root), \'map_label\': False},\n        \'gallery\': {\'pattern\': \'{}/detected/bounding_box_test/*.jpg\'.format(im_root), \'map_label\': False},\n    }\n\n    def __init__(self, cfg, samples=None):\n        self.root = osp.join(cfg.root, cfg.name)\n        self._save_png_as_jpg()\n        super(CUHK03NpDetectedJpg, self).__init__(cfg, samples=samples)\n\n    def _get_kpt_key(self, im_path):\n        return im_path\n\n    def _get_ps_label_path(self, im_path):\n        return osp.join(self.root, im_path.replace(self.im_root, self.im_root + \'_ps_label\').replace(\'.jpg\', \'.png\'))\n\n    def _save_png_as_jpg(self):\n        png_im_dir = osp.join(self.root, self.png_im_root)\n        jpg_im_dir = osp.join(self.root, self.im_root)\n        assert osp.exists(png_im_dir), ""The PNG image dir {} should be placed inside {}"".format(png_im_dir, self.root)\n        png_paths = list(walkdir(png_im_dir, \'.png\'))\n        if osp.exists(jpg_im_dir):\n            jpg_paths = list(walkdir(jpg_im_dir, \'.jpg\'))\n            if len(png_paths) == len(jpg_paths):\n                print(\'=> Found same number of JPG images. Skip transforming PNG to JPG.\')\n                return\n            else:\n                shutil.rmtree(jpg_im_dir)\n                print(\'=> JPG image dir exists but does not contain same number of images. So it is removed and would be re-generated.\')\n        # CUHK03 contains 14097 detected + 14096 labeled images = 28193\n        for png_path in tqdm(png_paths, desc=\'PNG->JPG\', miniters=2000, ncols=120, unit=\' images\'):\n            jpg_path = png_path.replace(self.png_im_root, self.im_root).replace(\'.png\', \'.jpg\')\n            may_make_dir(osp.dirname(jpg_path))\n            imsave(jpg_path, np.array(Image.open(png_path)))\n'"
package/data/datasets/cuhk03_np_detected_png.py,0,"b""import os.path as osp\nfrom .market1501 import Market1501\n\n\nclass CUHK03NpDetectedPng(Market1501):\n    # Will be available in the future\n    has_pap_mask = False\n    has_ps_label = False\n    im_root = 'cuhk03-np'\n    split_spec = {\n        'train': {'pattern': '{}/detected/bounding_box_train/*.png'.format(im_root), 'map_label': True},\n        'query': {'pattern': '{}/detected/query/*.png'.format(im_root), 'map_label': False},\n        'gallery': {'pattern': '{}/detected/bounding_box_test/*.png'.format(im_root), 'map_label': False},\n    }\n\n    def _get_kpt_key(self, im_path):\n        return im_path\n\n    def _get_ps_label_path(self, im_path):\n        return osp.join(self.root, im_path.replace(self.im_root, self.im_root + '_ps_label'))\n"""
package/data/datasets/duke.py,0,"b""import os.path as osp\nfrom .market1501 import Market1501\n\n\nclass DukeMTMCreID(Market1501):\n    has_pap_mask = True\n    has_ps_label = True\n    im_root = 'DukeMTMC-reID'\n    split_spec = {\n        'train': {'pattern': '{}/bounding_box_train/*.jpg'.format(im_root), 'map_label': True},\n        'query': {'pattern': '{}/query/*.jpg'.format(im_root), 'map_label': False},\n        'gallery': {'pattern': '{}/bounding_box_test/*.jpg'.format(im_root), 'map_label': False},\n        'train_market1501_style': {'pattern': 'bounding_box_train_market1501_style/*.jpg', 'map_label': True},\n    }\n\n    def _get_kpt_key(self, im_path):\n        cfg = self.cfg\n        key = im_path\n        if cfg.split == 'train_market1501_style':\n            key = '{}/bounding_box_train/{}'.format(self.im_root, osp.basename(im_path))\n        return key\n\n    def _get_ps_label_path(self, im_path):\n        cfg = self.cfg\n        if cfg.split == 'train_market1501_style':\n            path = '{}_ps_label/bounding_box_train/{}'.format(self.im_root, osp.basename(im_path))\n        else:\n            path = im_path.replace(self.im_root, self.im_root + '_ps_label')\n        path = path.replace('.jpg', '.png')\n        path = osp.join(self.root, path)\n        return path\n"""
package/data/datasets/market1501.py,0,"b'import os.path as osp\nfrom ..dataset import Dataset\nfrom ...utils.file import get_files_by_pattern\nfrom ...utils.file import save_pickle\n\n\nclass Market1501(Dataset):\n    has_pap_mask = True\n    has_ps_label = True\n    im_root = \'Market-1501-v15.09.15\'\n    split_spec = {\n            \'train\': {\'pattern\': \'{}/bounding_box_train/*.jpg\'.format(im_root), \'map_label\': True},\n            \'query\': {\'pattern\': \'{}/query/*.jpg\'.format(im_root), \'map_label\': False},\n            \'gallery\': {\'pattern\': \'{}/bounding_box_test/*.jpg\'.format(im_root), \'map_label\': False},\n            \'train_duke_style\': {\'pattern\': \'bounding_box_train_duke_style/*.jpg\', \'map_label\': True},\n    }\n\n    def _get_kpt_key(self, im_path):\n        cfg = self.cfg\n        key = im_path\n        if cfg.split == \'train_duke_style\':\n            key = \'{}/bounding_box_train/{}\'.format(self.im_root, osp.basename(im_path))\n        return key\n\n    def _get_ps_label_path(self, im_path):\n        cfg = self.cfg\n        if cfg.split == \'train_duke_style\':\n            path = \'{}_ps_label/bounding_box_train/{}\'.format(self.im_root, osp.basename(im_path))\n        else:\n            path = im_path.replace(self.im_root, self.im_root + \'_ps_label\')\n        path = path.replace(\'.jpg\', \'.png\')\n        path = osp.join(self.root, path)\n        return path\n\n    @staticmethod\n    def parse_im_path(im_path):\n        im_name = osp.basename(im_path)\n        id = -1 if im_name.startswith(\'-1\') else int(im_name[:4])\n        cam = int(im_name[4]) if im_name.startswith(\'-1\') else int(im_name[6])\n        return id, cam\n\n    # TODO: make this extensible for MSMT17 and others.\n    def save_split(self, spec, save_path):\n        cfg = self.cfg\n        im_paths = sorted(get_files_by_pattern(self.root, pattern=spec[\'pattern\'], strip_root=True))\n        assert len(im_paths) > 0, ""There are {} images for split [{}] of dataset [{}]. Please place your dataset in right position.""\\\n            .format(len(im_paths), cfg.split, self.__class__.__name__)\n        ids, cams = zip(*[self.parse_im_path(p) for p in im_paths])\n        # Filter out id -1 which is officially not used\n        im_paths, ids, cams = zip(*[(im_path, id, cam) for im_path, id, cam in zip(im_paths, ids, cams) if id != -1])\n        if spec[\'map_label\']:\n            unique_ids = sorted(list(set(ids)))\n            ids2labels = dict(zip(unique_ids, range(len(unique_ids))))\n            labels = [ids2labels[id] for id in ids]\n        else:\n            labels = ids\n        samples = [{\'im_path\': im_path, \'label\': label, \'cam\': cam} for im_path, label, cam in zip(im_paths, labels, cams)]\n        save_pickle(samples, save_path)\n'"
package/data/datasets/msmt17.py,0,"b'import os.path as osp\nfrom ..dataset import Dataset\nfrom ...utils.file import load_pickle, save_pickle\nfrom ...utils.file import read_lines\n\n\nclass MSMT17(Dataset):\n    has_pap_mask = True\n    has_ps_label = True\n    im_root = \'MSMT17_V1\'\n\n    def _get_im_paths(self, list_file, dir_name):\n        lines = read_lines(osp.join(self.root, self.im_root, list_file))\n        im_paths = sorted([osp.join(self.im_root, dir_name, l.split(\' \')[0]) for l in lines])\n        return im_paths\n\n    def _gen_split_spec(self):\n        split_spec = {\n            \'train\': {\'im_paths\': self._get_im_paths(\'list_train.txt\', \'train\') + self._get_im_paths(\'list_val.txt\', \'train\'), \'map_label\': True},\n            \'query\': {\'im_paths\': self._get_im_paths(\'list_query.txt\', \'test\'), \'map_label\': False},\n            \'gallery\': {\'im_paths\': self._get_im_paths(\'list_gallery.txt\', \'test\'), \'map_label\': False},\n        }\n        return split_spec\n\n    def _get_kpt_key(self, im_path):\n        return im_path\n\n    def _get_ps_label_path(self, im_path):\n        return osp.join(self.root, im_path.replace(self.im_root, self.im_root + \'_ps_label\').replace(\'.jpg\', \'.png\'))\n\n    @staticmethod\n    def parse_im_path(im_path):\n        im_name = osp.basename(im_path)\n        id = int(im_name[:4])\n        cam = int(im_name[9:11])\n        return id, cam\n\n    def save_split(self, spec, save_path):\n        cfg = self.cfg\n        im_paths = spec[\'im_paths\']\n        assert len(im_paths) > 0, ""There are {} images for split [{}] of dataset [{}]. Please place your dataset in right position."" \\\n            .format(len(im_paths), cfg.split, self.__class__.__name__)\n        ids, cams = zip(*[self.parse_im_path(p) for p in im_paths])\n        if spec[\'map_label\']:\n            unique_ids = sorted(list(set(ids)))\n            ids2labels = dict(zip(unique_ids, range(len(unique_ids))))\n            labels = [ids2labels[id] for id in ids]\n        else:\n            labels = ids\n        samples = [{\'im_path\': im_path, \'label\': label, \'cam\': cam} for im_path, label, cam in zip(im_paths, labels, cams)]\n        save_pickle(samples, save_path)\n\n    def load_split(self):\n        cfg = self.cfg\n        self.split_spec = self._gen_split_spec()\n        save_path = osp.join(self.root, cfg.split + \'.pkl\')\n        self.save_split(self.split_spec[cfg.split], save_path)\n        samples = load_pickle(save_path)\n        return samples\n'"
package/data/datasets/partial_ilids.py,0,"b'import os.path as osp\nfrom ..dataset import Dataset\nfrom ...utils.file import get_files_by_pattern\nfrom ...utils.file import save_pickle\n\n\nclass PartialiLIDs(Dataset):\n    # Will be available in the future\n    has_pap_mask = False\n    has_ps_label = False\n    im_root = \'Partial_iLIDS\'\n    split_spec = {\n            \'query\': {\'pattern\': \'{}/Probe/*.jpg\'.format(im_root), \'map_label\': False},\n            \'gallery\': {\'pattern\': \'{}/Gallery/*.jpg\'.format(im_root), \'map_label\': False},\n    }\n\n    def _get_kpt_key(self, im_path):\n        return im_path\n\n    def _get_ps_label_path(self, im_path):\n        return osp.join(self.root, im_path.replace(self.im_root, self.im_root + \'_ps_label\').replace(\'.jpg\', \'.png\'))\n\n    @staticmethod\n    def parse_im_path(im_path):\n        im_name = osp.basename(im_path)\n        id = int(osp.splitext(im_name)[0])\n        return id\n\n    def save_split(self, spec, save_path):\n        cfg = self.cfg\n        im_paths = sorted(get_files_by_pattern(self.root, pattern=spec[\'pattern\'], strip_root=True))\n        assert len(im_paths) > 0, ""There are {} images for split [{}] of dataset [{}]. Please place your dataset in right position."" \\\n            .format(len(im_paths), cfg.split, self.__class__.__name__)\n        # The dataset does not annotate camera. Here we manually set query camera to 0, gallery to 1, to satisfy the testing code.\n        ids = [self.parse_im_path(p) for p in im_paths]\n        cams = [0 if cfg.split == \'query\' else 1 for _ in im_paths]\n        if spec[\'map_label\']:\n            unique_ids = sorted(list(set(ids)))\n            ids2labels = dict(zip(unique_ids, range(len(unique_ids))))\n            labels = [ids2labels[id] for id in ids]\n        else:\n            labels = ids\n        samples = [{\'im_path\': im_path, \'label\': label, \'cam\': cam} for im_path, label, cam in zip(im_paths, labels, cams)]\n        save_pickle(samples, save_path)\n'"
package/data/datasets/partial_reid.py,0,"b'import os.path as osp\nfrom ..dataset import Dataset\nfrom ...utils.file import get_files_by_pattern\nfrom ...utils.file import save_pickle\n\n\nclass PartialREID(Dataset):\n    # Will be available in the future\n    has_pap_mask = False\n    has_ps_label = False\n    im_root = \'Partial-REID_Dataset\'\n    split_spec = {\n            \'occ_query\': {\'pattern\': \'{}/occluded_body_images/*.jpg\'.format(im_root), \'map_label\': False},\n            \'partial_query\': {\'pattern\': \'{}/partial_body_images/*.jpg\'.format(im_root), \'map_label\': False},\n            # Paper ""Deep Spatial Feature Reconstruction for Partial Person Re-Identification: Alignment-Free Approach"" uses cropped query images\n            \'query\': {\'pattern\': \'{}/partial_body_images/*.jpg\'.format(im_root), \'map_label\': False},\n            \'gallery\': {\'pattern\': \'{}/whole_body_images/*.jpg\'.format(im_root), \'map_label\': False},\n    }\n\n    def _get_kpt_key(self, im_path):\n        return im_path\n\n    def _get_ps_label_path(self, im_path):\n        return osp.join(self.root, im_path.replace(self.im_root, self.im_root + \'_ps_label\').replace(\'.jpg\', \'.png\'))\n\n    @staticmethod\n    def parse_im_path(im_path):\n        im_name = osp.basename(im_path)\n        id = int(im_name[:3])\n        return id\n\n    def save_split(self, spec, save_path):\n        cfg = self.cfg\n        im_paths = sorted(get_files_by_pattern(self.root, pattern=spec[\'pattern\'], strip_root=True))\n        assert len(im_paths) > 0, ""There are {} images for split [{}] of dataset [{}]. Please place your dataset in right position."" \\\n            .format(len(im_paths), cfg.split, self.__class__.__name__)\n        # The dataset does not annotate camera. Here we manually set query camera to 0, gallery to 1, to satisfy the testing code.\n        ids = [self.parse_im_path(p) for p in im_paths]\n        cams = [0 if cfg.split in [\'query\', \'occ_query\', \'partial_query\'] else 1 for _ in im_paths]\n        if spec[\'map_label\']:\n            unique_ids = sorted(list(set(ids)))\n            ids2labels = dict(zip(unique_ids, range(len(unique_ids))))\n            labels = [ids2labels[id] for id in ids]\n        else:\n            labels = ids\n        samples = [{\'im_path\': im_path, \'label\': label, \'cam\': cam} for im_path, label, cam in zip(im_paths, labels, cams)]\n        save_pickle(samples, save_path)\n'"
