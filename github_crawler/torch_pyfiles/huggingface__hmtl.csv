file_path,api_count,code
evaluate.py,2,"b'# coding: utf-8\n\n""""""\nThe ``evaluate.py`` file can be used to\nevaluate a trained model against a dataset\nand report any metrics calculated by the model.\nIt requires a configuration file and a directory in\nwhich to write the results.\n\n.. code-block:: bash\n\n   $ python evaluate.py --help\n    usage: evaluate.py [-h] -s SERIALIZATION_DIR [-g]\n\n    optional arguments:\n    -h, --help            show this help message and exit\n    -s SERIALIZATION_DIR, --serialization_dir SERIALIZATION_DIR\n                            Directory in which to save the model and its logs.\n    -g, --gold_mentions   Whether or not evaluate using gold mentions in\n                            coreference\n""""""\n\nimport argparse\nimport os\nimport json\nimport itertools\nimport re\nfrom copy import deepcopy\nimport tqdm\nfrom typing import List, Dict, Any, Iterable\nimport torch\n\nfrom allennlp.models.model import Model\nfrom allennlp.data import Instance\nfrom allennlp.data.iterators import DataIterator\nfrom allennlp.common.checks import check_for_gpu\nfrom allennlp.common.params import Params\nfrom allennlp.nn import util\nfrom allennlp.data import Vocabulary\n\nfrom hmtl.tasks import Task\nfrom hmtl.common import create_and_set_iterators\n\nimport logging\n\nlogging.basicConfig(\n    format=""%(asctime)s - %(levelname)s - %(name)s -   %(message)s"", datefmt=""%m/%d/%Y %H:%M:%S"", level=logging.INFO\n)\nlogger = logging.getLogger(__name__)\n\n\ndef evaluate(\n    model: Model, instances: Iterable[Instance], task_name: str, data_iterator: DataIterator, cuda_device: int\n) -> Dict[str, Any]:\n    """"""\n    Evaluate a model for a particular task (usually after training).\n    \n    Parameters\n    ----------\n    model : ``allennlp.models.model.Model``, required\n        The model to evaluate\n    instances : ``Iterable[Instance]``, required\n        The (usually test) dataset on which to evalute the model.\n    task_name : ``str``, required\n        The name of the task on which evaluate the model.\n    data_iterator : ``DataIterator``\n        Iterator that go through the dataset.\n    cuda_device : ``int``\n        Cuda device to use.\n        \n    Returns\n    -------\n    metrics :  ``Dict[str, Any]``\n        A dictionary containing the metrics on the evaluated dataset.\n    """"""\n    check_for_gpu(cuda_device)\n    with torch.no_grad():\n        model.eval()\n\n        iterator = data_iterator(instances, num_epochs=1, shuffle=False)\n        logger.info(""Iterating over dataset"")\n        generator_tqdm = tqdm.tqdm(iterator, total=data_iterator.get_num_batches(instances))\n\n        eval_loss = 0\n        nb_batches = 0\n        for batch in generator_tqdm:\n            batch = util.move_to_device(batch, cuda_device)\n            nb_batches += 1\n\n            eval_output_dict = model.forward(task_name=task_name, tensor_batch=batch)\n            loss = eval_output_dict[""loss""]\n            eval_loss += loss.item()\n            metrics = model.get_metrics(task_name=task_name)\n            metrics[""loss""] = float(eval_loss / nb_batches)\n\n            description = "", "".join([""%s: %.2f"" % (name, value) for name, value in metrics.items()]) + "" ||""\n            generator_tqdm.set_description(description, refresh=False)\n\n        metrics = model.get_metrics(task_name=task_name, reset=True, full=True)\n        metrics[""loss""] = float(eval_loss / nb_batches)\n        return metrics\n\n\nif __name__ == ""__main__"":\n    ### Evaluate from args ###\n\n    # Parse arguments\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        ""-s"", ""--serialization_dir"", required=True, help=""Directory in which to save the model and its logs."", type=str\n    )\n    parser.add_argument(\n        ""-g"",\n        ""--gold_mentions"",\n        action=""store_true"",\n        required=False,\n        default=False,\n        help=""Whether or not evaluate using gold mentions in coreference"",\n    )\n    args = parser.parse_args()\n\n    params = Params.from_file(params_file=os.path.join(args.serialization_dir, ""config.json""))\n\n    ### Instantiate tasks ###\n    task_list = []\n    task_keys = [key for key in params.keys() if re.search(""^task_"", key)]\n\n    for key in task_keys:\n        logger.info(""Creating %s"", key)\n        task_params = params.pop(key)\n        task_description = task_params.pop(""task_description"")\n        task_data_params = task_params.pop(""data_params"")\n\n        task = Task.from_params(params=task_description)\n        task_list.append(task)\n\n        _, _ = task.load_data_from_params(params=task_data_params)\n\n    ### Load Vocabulary from files ###\n    vocab = Vocabulary.from_files(os.path.join(args.serialization_dir, ""vocabulary""))\n    logger.info(""Vocabulary loaded"")\n\n    ### Load the data iterators ###\n    task_list = create_and_set_iterators(params=params, task_list=task_list, vocab=vocab)\n\n    ### Regularization\t###\n    regularizer = None\n\n    ### Create model ###\n    model_params = params.pop(""model"")\n    model = Model.from_params(vocab=vocab, params=model_params, regularizer=regularizer)\n\n    ### Real evaluation ###\n    cuda_device = params.pop(""multi_task_trainer"").pop_int(""cuda_device"", -1)\n\n    metrics = {task._name: {} for task in task_list}\n    for task in task_list:\n        if not task._evaluate_on_test:\n            continue\n\n        logger.info(""Task %s will be evaluated using the best epoch weights."", task._name)\n        assert (\n            task._test_data is not None\n        ), ""Task {} wants to be evaluated on test dataset but no there is no test data loaded."".format(task._name)\n\n        logger.info(""Loading the best epoch weights for task %s"", task._name)\n        best_model_state_path = os.path.join(args.serialization_dir, ""best_{}.th"".format(task._name))\n        best_model_state = torch.load(best_model_state_path)\n        best_model = model\n        best_model.load_state_dict(state_dict=best_model_state)\n\n        test_metric_dict = {}\n\n        for pair_task in task_list:\n            if not pair_task._evaluate_on_test:\n                continue\n\n            logger.info(""Pair task %s is evaluated with the best model for %s"", pair_task._name, task._name)\n            test_metric_dict[pair_task._name] = {}\n            test_metrics = evaluate(\n                model=best_model,\n                task_name=pair_task._name,\n                instances=pair_task._test_data,\n                data_iterator=pair_task._data_iterator,\n                cuda_device=cuda_device,\n            )\n\n            for metric_name, value in test_metrics.items():\n                test_metric_dict[pair_task._name][metric_name] = value\n\n        metrics[task._name][""test""] = deepcopy(test_metric_dict)\n        logger.info(""Finished evaluation of task %s."", task._name)\n\n    metrics_json = json.dumps(metrics, indent=2)\n    with open(os.path.join(args.serialization_dir, ""evaluate_metrics.json""), ""w"") as metrics_file:\n        metrics_file.write(metrics_json)\n\n    logger.info(""Metrics: %s"", metrics_json)\n'"
fine_tune.py,1,"b'# coding: utf-8\n\n""""""\nThe ``fine_tune.py`` file is used to continue training (or `fine-tune`) a model on a `different\ndataset` than the one it was originally trained on.  It requires a saved model archive file, a path\nto the data you will continue training with, and a directory in which to write the results.\n\n. code-block:: bash\n\n   $ python fine_tune.py --help\n    usage: fine_tune.py [-h] -s SERIALIZATION_DIR -c CONFIG_FILE_PATH -p\n                        PRETRAINED_DIR -m PRETRAINED_MODEL_NAME\n\n    optional arguments:\n    -h, --help            show this help message and exit\n    -s SERIALIZATION_DIR, --serialization_dir SERIALIZATION_DIR\n                            Directory in which to save the model and its logs.\n    -c CONFIG_FILE_PATH, --config_file_path CONFIG_FILE_PATH\n                            Path to parameter file describing the new multi-tasked\n                            model to be fine-tuned.\n    -p PRETRAINED_DIR, --pretrained_dir PRETRAINED_DIR\n                            Directory in which was saved the pre-trained model.\n    -m PRETRAINED_MODEL_NAME, --pretrained_model_name PRETRAINED_MODEL_NAME\n                            Name of the weight file for the pretrained model to\n                            fine-tune in the ``pretrained_dir``.\n""""""\n\nimport argparse\nimport itertools\nimport os\nimport json\nimport re\nfrom copy import deepcopy\nimport torch\nfrom typing import List, Dict, Any, Tuple\nimport logging\n\nlogging.basicConfig(\n    format=""%(asctime)s - %(levelname)s - %(name)s -   %(message)s"", datefmt=""%m/%d/%Y %H:%M:%S"", level=logging.INFO\n)\n\nfrom hmtl.tasks import Task\nfrom hmtl.training.multi_task_trainer import MultiTaskTrainer\nfrom hmtl.common import create_and_set_iterators\nfrom evaluate import evaluate\nfrom train import train_model\n\nfrom allennlp.models.model import Model\nfrom allennlp.data import Vocabulary\nfrom allennlp.data.iterators import DataIterator\nfrom allennlp.commands.train import create_serialization_dir\nfrom allennlp.common.params import Params\nfrom allennlp.common.checks import ConfigurationError\nfrom allennlp.nn import RegularizerApplicator\n\nlogger = logging.getLogger(__name__)\n\n\nif __name__ == ""__main__"":\n    # Parse arguments\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        ""-s"", ""--serialization_dir"", required=True, help=""Directory in which to save the model and its logs."", type=str\n    )\n    parser.add_argument(\n        ""-c"",\n        ""--config_file_path"",\n        required=True,\n        help=""Path to parameter file describing the new multi-tasked model to be fine-tuned."",\n        type=str,\n    )\n    parser.add_argument(\n        ""-p"", ""--pretrained_dir"", required=True, help=""Directory in which was saved the pre-trained model."", type=str\n    )\n    parser.add_argument(\n        ""-m"",\n        ""--pretrained_model_name"",\n        required=True,\n        help=""Name of the weight file for the pretrained model to fine-tune in the ``pretrained_dir``."",\n        type=str,\n    )\n    args = parser.parse_args()\n\n    params = Params.from_file(params_file=args.config_file_path)\n    serialization_dir = args.serialization_dir\n    create_serialization_dir(params, serialization_dir, False)\n\n    serialization_params = deepcopy(params).as_dict(quiet=True)\n    with open(os.path.join(serialization_dir, ""config.json""), ""w"") as param_file:\n        json.dump(serialization_params, param_file, indent=4)\n\n    ### Instantiate tasks ###\n    task_list = []\n    task_keys = [key for key in params.keys() if re.search(""^task_"", key)]\n\n    for key in task_keys:\n        logger.info(""Creating %s"", key)\n        task_params = params.pop(key)\n        task_description = task_params.pop(""task_description"")\n        task_data_params = task_params.pop(""data_params"")\n\n        task = Task.from_params(params=task_description)\n        task_list.append(task)\n\n        _, _ = task.load_data_from_params(params=task_data_params)\n\n    ### Load Vocabulary from files and save it to the new serialization_dir ###\n    # PLEASE NOTE that here, we suppose that the vocabulary is the same for the pre-trained model\n    # and the model to fine-tune. The most noticeable implication of this hypothesis is that the label specs\n    # between the two datasets (for pre-training and for fine-tuning) are exactly the same.\n    vocab = Vocabulary.from_files(os.path.join(args.pretrained_dir, ""vocabulary""))\n    logger.info(""Vocabulary loaded from %s"", os.path.join(args.pretrained_dir, ""vocabulary""))\n\n    vocab.save_to_files(os.path.join(serialization_dir, ""vocabulary""))\n    logger.info(""Save vocabulary to file %s"", os.path.join(serialization_dir, ""vocabulary""))\n\n    ### Load the data iterators for each task ###\n    task_list = create_and_set_iterators(params=params, task_list=task_list, vocab=vocab)\n\n    ### Load Regularizations\t###\n    regularizer = RegularizerApplicator.from_params(params.pop(""regularizer"", []))\n\n    ### Create model ###\n    model_params = params.pop(""model"")\n    model = Model.from_params(vocab=vocab, params=model_params, regularizer=regularizer)\n\n    logger.info(""Loading the pretrained model from %s"", os.path.join(args.pretrained_dir, args.pretrained_model_name))\n    try:\n        pretrained_model_state_path = os.path.join(args.pretrained_dir, args.pretrained_model_name)\n        pretrained_model_state = torch.load(pretrained_model_state_path)\n        model.load_state_dict(state_dict=pretrained_model_state)\n    except:\n        raise ConfigurationError(\n            ""It appears that the configuration of the pretrained model and ""\n            ""the model to fine-tune are not compatible. ""\n            ""Please check the compatibility of the encoders and taggers in the ""\n            ""config files.""\n        )\n\n    ### Create multi-task trainer ###\n    multi_task_trainer_params = params.pop(""multi_task_trainer"")\n    trainer = MultiTaskTrainer.from_params(\n        model=model, task_list=task_list, serialization_dir=serialization_dir, params=multi_task_trainer_params\n    )\n\n    ### Launch training ###\n    metrics = train_model(multi_task_trainer=trainer, recover=False)\n    if metrics is not None:\n        logging.info(""Fine-tuning is finished ! Let\'s have a drink. It\'s on the house !"")\n'"
html_senteval.py,3,"b'# coding: utf-8\n\n""""""\nA quick and simple script for evaluating the embeddings throught the HTML model/hierarchy\nusing SentEval.\n""""""\n\n\nfrom __future__ import absolute_import, division, unicode_literals\n\nimport sys\nimport io\nimport numpy as np\nimport logging\nimport re\n\n# Set PATHs\nPATH_TO_SENTEVAL = \'./SentEval/\'\nPATH_TO_DATA = \'./SentEval/data\'\nsys.path.insert(0, PATH_TO_SENTEVAL)\nimport senteval\n\nimport os\nimport torch\nimport argparse\n\nfrom allennlp.common.params import Params\nfrom allennlp.data.token_indexers import TokenIndexer\nfrom allennlp.data import Token, Instance, Vocabulary\nfrom allennlp.data.dataset import Batch\nfrom allennlp.data.fields import TextField\nfrom allennlp.nn import util\nfrom allennlp.models.model import Model\n\nimport hmtl\n\n\ndef text_to_instance(sent, token_indexers):\n    text = TextField([Token(word) for word in sent], token_indexers = token_indexers)\n    instance = Instance({""text"": text})\n    return instance\n\ndef sentences_to_indexed_batch(sentences, token_indexers):\n    instances = [text_to_instance(sent, token_indexers) for sent in sentences]\n    batch = Batch(instances)\n    batch.index_instances(vocab)\n    return batch \t\n    \ndef compute_embds_from_layer(model, model_layer_name, batch):\n    batch_tensor = batch.as_tensor_dict(batch.get_padding_lengths())\n    text = batch_tensor[""text""]\n    text_mask = util.get_text_field_mask(text)\n    \n    if model_layer_name == ""text_field_embedder"":\n        embds_text_field_embedder = model._text_field_embedder(text)\n        embds = embds_text_field_embedder\n        \n    if model_layer_name == ""encoder_ner"":\n        embds_text_field_embedder = model._text_field_embedder(text)\n        embds_encoder_ner = model._encoder_ner(embds_text_field_embedder, text_mask)\n        embds = embds_encoder_ner\n        \n    if model_layer_name == ""encoder_emd"":\n        embds_text_field_embedder = model._shortcut_text_field_embedder(text)\n        embds_encoder_emd = model._encoder_emd(embds_text_field_embedder, text_mask)\n        embds = embds_encoder_emd\n        \n    if model_layer_name == ""encoder_relation"":\n        embds_text_field_embedder = model._shortcut_text_field_embedder_relation(text)\n        embds_encoder_relation = model._encoder_relation(embds_text_field_embedder, text_mask)\n        embds = embds_encoder_relation\n    \n    if model_layer_name == ""encoder_coref"":\n        embds_text_field_embedder = model._shortcut_text_field_embedder_coref(text)\n        embds_encoder_coref = model._encoder_coref(embds_text_field_embedder, text_mask)\n        embds = embds_encoder_coref\n    \n    emds_size = embds.size(2)\n    expanded_text_mask = torch.cat([text_mask.unsqueeze(-1)]*emds_size, dim = -1)\n        \n    embds_sum = (embds*expanded_text_mask.float()).sum(dim = 1)\n    normalization = torch.cat([(1/text_mask.float().sum(-1)).unsqueeze(-1)]*emds_size, dim = -1)\n    computed_embds = (embds_sum*normalization)\n    \n    return computed_embds.detach().numpy()\n\n\n# SentEval prepare and batcher\ndef prepare(params, samples):\n    return\n\ndef batcher(params, batch):\n    batch = sentences_to_indexed_batch(batch, token_index)\n    embds = compute_embds_from_layer(model, args.layer_name, batch)\n    return embds\n\n\n# Set params for SentEval\nparams_senteval = {\'task_path\': PATH_TO_DATA, \'usepytorch\': True, \'kfold\': 5}\nparams_senteval[\'classifier\'] = {\'nhid\': 0, \'optim\': \'rmsprop\', \'batch_size\': 128,\n                                 \'tenacity\': 3, \'epoch_size\': 2}\n\n\n# Set up logger\nlogging.basicConfig(format = \'%(asctime)s - %(levelname)s - %(name)s -   %(message)s\', \n                    datefmt = \'%m/%d/%Y %H:%M:%S\',\n                    level = logging.INFO)\nlogger = logging.getLogger(__name__)\n\n\nif __name__ == ""__main__"":\t\n    parser = argparse.ArgumentParser()\n    parser.add_argument(""-s"",\n                        ""--serialization_dir"",\n                        required = True,\n                        help = ""Directory from which to load the pretrained model."", \n                        type = str)\n    parser.add_argument(""-t"",\n                        ""--task"",\n                        required = False,\n                        default = ""ner"",\n                        help = ""Name of the task to load."", \n                        type = str)\t\n    parser.add_argument(""-l"",\n                        ""--layer_name"",\n                        required = False,\n                        default = ""text_field_embedder"",\n                        help = ""Name of encoder/embedding layer of the model"", \n                        type = str)\t\t\t\t\t\t\t\t\t\t\n    args = parser.parse_args()\n    \n    \n    serialization_dir = args.serialization_dir\n\n    params = Params.from_file(params_file = os.path.join(args.serialization_dir, ""config.json""))\n    logging.info(""Parameters loaded from %s"", os.path.join(serialization_dir, ""config.json""))\n    \n    ### Load Vocabulary from files ###\n    logging.info(""Loading Vocavulary from %s"", os.path.join(serialization_dir, ""vocabulary""))\n    vocab = Vocabulary.from_files(os.path.join(args.serialization_dir, ""vocabulary""))\n    logger.info(""Vocabulary loaded"")\n    \n    ### Create model ###\n    model_params = params.pop(""model"")\n    model = Model.from_params(vocab = vocab, params = model_params, regularizer = None)\n    best_model_state_path = os.path.join(serialization_dir, ""best_{}.th"".format(args.task))\n    best_model_state = torch.load(best_model_state_path)\n    model.load_state_dict(state_dict = best_model_state)\n    \n    ### Create token indexer ###\n    token_index = {}\n    task_keys = [key for key in params.keys() if re.search(""^task_"", key)] \n    token_indexer_params = params.pop(task_keys[-1]).pop(""data_params"").pop(""dataset_reader"").pop(""token_indexers"")\n    for name, indexer_params in token_indexer_params.items(): \n        token_index[name] = TokenIndexer.from_params(indexer_params) \n    \n    params_senteval[\'encoder\'] = model\n    \n    se = senteval.engine.SE(params_senteval, batcher, prepare)\n    transfer_tasks = [\'Length\', \'WordContent\', \'Depth\', \'TopConstituents\',\n                      \'BigramShift\', \'Tense\', \'SubjNumber\', \'ObjNumber\',\n                      \'OddManOut\', \'CoordinationInversion\']\n    results = se.eval(transfer_tasks)\n    \n    print(results)\n    logging.info(""SentEval(uation) Finished"")\n'"
train.py,1,"b'# coding: utf-8\n\n""""""\nThe ``train.py`` file can be used to train a model.\nIt requires a configuration file and a directory in\nwhich to write the results.\n\n.. code-block:: bash\n\n   $ python train.py --help\n    usage: train.py [-h] -s SERIALIZATION_DIR -c CONFIG_FILE_PATH [-r]\n\n    optional arguments:\n    -h, --help            show this help message and exit\n    -s SERIALIZATION_DIR, --serialization_dir SERIALIZATION_DIR\n                            Directory in which to save the model and its logs.\n    -c CONFIG_FILE_PATH, --config_file_path CONFIG_FILE_PATH\n                            Path to parameter file describing the multi-tasked\n                            model to be trained.\n    -r, --recover         Recover a previous training from the state in\n                            serialization_dir.\n""""""\n\nimport argparse\nimport itertools\nimport os\nimport json\nimport re\nfrom copy import deepcopy\nimport torch\nimport logging\nfrom typing import List, Dict, Any, Tuple\n\nlogging.basicConfig(\n    format=""%(asctime)s - %(levelname)s - %(name)s -   %(message)s"", datefmt=""%m/%d/%Y %H:%M:%S"", level=logging.INFO\n)\n\nfrom hmtl.tasks import Task\nfrom hmtl.training.multi_task_trainer import MultiTaskTrainer\nfrom hmtl.common import create_and_set_iterators\nfrom evaluate import evaluate\n\nfrom allennlp.models.model import Model\nfrom allennlp.data import Vocabulary\nfrom allennlp.data.iterators import DataIterator\nfrom allennlp.commands.train import create_serialization_dir\nfrom allennlp.common.params import Params\nfrom allennlp.nn import RegularizerApplicator\n\nlogger = logging.getLogger(__name__)\n\n\ndef tasks_and_vocab_from_params(params: Params, serialization_dir: str) -> Tuple[List[Task], Vocabulary]:\n    """"""\n    Load each of the tasks in the model from the ``params`` file\n    and load the datasets associated with each of these task.\n    Create the vocavulary from ``params`` using the concatenation of the ``datasets_for_vocab_creation``\n    from each of the task specific dataset.\n    \n    Parameters\n    ----------\n    params: ``Params``\n        A parameter object specifing an experiment.\n    serialization_dir: ``str``\n        Directory in which to save the model and its logs.\n    Returns\n    -------\n    task_list: ``List[Task]``\n        A list containing the tasks of the model to train.\n    vocab: ``Vocabulary``\n        The vocabulary fitted on the datasets_for_vocab_creation.\n    """"""\n    ### Instantiate the different tasks ###\n    task_list = []\n    instances_for_vocab_creation = itertools.chain()\n    datasets_for_vocab_creation = {}\n    task_keys = [key for key in params.keys() if re.search(""^task_"", key)]\n\n    for key in task_keys:\n        logger.info(""Creating %s"", key)\n        task_params = params.pop(key)\n        task_description = task_params.pop(""task_description"")\n        task_data_params = task_params.pop(""data_params"")\n\n        task = Task.from_params(params=task_description)\n        task_list.append(task)\n\n        task_instances_for_vocab, task_datasets_for_vocab = task.load_data_from_params(params=task_data_params)\n        instances_for_vocab_creation = itertools.chain(instances_for_vocab_creation, task_instances_for_vocab)\n        datasets_for_vocab_creation[task._name] = task_datasets_for_vocab\n\n    ### Create and save the vocabulary ###\n    for task_name, task_dataset_list in datasets_for_vocab_creation.items():\n        logger.info(""Creating a vocabulary using %s data from %s."", "", "".join(task_dataset_list), task_name)\n\n    logger.info(""Fitting vocabulary from dataset"")\n    vocab = Vocabulary.from_params(params.pop(""vocabulary"", {}), instances_for_vocab_creation)\n\n    vocab.save_to_files(os.path.join(serialization_dir, ""vocabulary""))\n    logger.info(""Vocabulary saved to %s"", os.path.join(serialization_dir, ""vocabulary""))\n\n    return task_list, vocab\n\n\ndef train_model(multi_task_trainer: MultiTaskTrainer, recover: bool = False) -> Dict[str, Any]:\n    """"""\n    Launching the training of the multi-task model.\n    \n\xc2\xa0\xc2\xa0\xc2\xa0\xc2\xa0Parameters\n\xc2\xa0\xc2\xa0\xc2\xa0\xc2\xa0----------\n    multi_task_trainer: ``MultiTaskTrainer``\n\xc2\xa0\xc2\xa0\xc2\xa0\xc2\xa0\xc2\xa0\xc2\xa0\xc2\xa0\xc2\xa0A trainer (similar to allennlp.training.trainer.Trainer) that can handle multi-task training.\n    recover : ``bool``, optional (default=False)\n        If ``True``, we will try to recover a training run from an existing serialization\n        directory.  This is only intended for use when something actually crashed during the middle\n        of a run.  For continuing training a model on new data, see the ``fine-tune`` command.\n                \xc2\xa0\xc2\xa0\xc2\xa0\xc2\xa0\n\xc2\xa0\xc2\xa0\xc2\xa0\xc2\xa0Returns\n\xc2\xa0\xc2\xa0\xc2\xa0\xc2\xa0-------\n    metrics: ``Dict[str, Any]\n        The different metrics summarizing the training of the model.\n        It includes the validation and test (if necessary) metrics.\n    """"""\n    ### Train the multi-task model ###\n    metrics = multi_task_trainer.train(recover=recover)\n\n    task_list = multi_task_trainer._task_list\n    serialization_dir = multi_task_trainer._serialization_dir\n    model = multi_task_trainer._model\n\n    ### Evaluate the model on test data if necessary ###\n    # This is a multi-task learning framework, the best validation metrics for one task are not necessarily\n    # obtained from the same epoch for all the tasks, one epoch begin equal to N forward+backward passes,\n    # where N is the total number of batches in all the training sets.\n    # We evaluate each of the best model for each task (based on the validation metrics) for all the other tasks (which have a test set).\n    for task in task_list:\n        if not task._evaluate_on_test:\n            continue\n\n        logger.info(""Task %s will be evaluated using the best epoch weights."", task._name)\n        assert (\n            task._test_data is not None\n        ), ""Task {} wants to be evaluated on test dataset but no there is no test data loaded."".format(task._name)\n\n        logger.info(""Loading the best epoch weights for task %s"", task._name)\n        best_model_state_path = os.path.join(serialization_dir, ""best_{}.th"".format(task._name))\n        best_model_state = torch.load(best_model_state_path)\n        best_model = model\n        best_model.load_state_dict(state_dict=best_model_state)\n\n        test_metric_dict = {}\n\n        for pair_task in task_list:\n            if not pair_task._evaluate_on_test:\n                continue\n\n            logger.info(""Pair task %s is evaluated with the best model for %s"", pair_task._name, task._name)\n            test_metric_dict[pair_task._name] = {}\n            test_metrics = evaluate(\n                model=best_model,\n                task_name=pair_task._name,\n                instances=pair_task._test_data,\n                data_iterator=pair_task._data_iterator,\n                cuda_device=multi_task_trainer._cuda_device,\n            )\n\n            for metric_name, value in test_metrics.items():\n                test_metric_dict[pair_task._name][metric_name] = value\n\n        metrics[task._name][""test""] = deepcopy(test_metric_dict)\n        logger.info(""Finished evaluation of task %s."", task._name)\n\n    ### Dump validation and possibly test metrics ###\n    metrics_json = json.dumps(metrics, indent=2)\n    with open(os.path.join(serialization_dir, ""metrics.json""), ""w"") as metrics_file:\n        metrics_file.write(metrics_json)\n    logger.info(""Metrics: %s"", metrics_json)\n\n    return metrics\n\n\nif __name__ == ""__main__"":\n    # Parse arguments\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        ""-s"", ""--serialization_dir"", required=True, help=""Directory in which to save the model and its logs."", type=str\n    )\n    parser.add_argument(\n        ""-c"",\n        ""--config_file_path"",\n        required=True,\n        help=""Path to parameter file describing the multi-tasked model to be trained."",\n        type=str,\n    )\n    parser.add_argument(\n        ""-r"",\n        ""--recover"",\n        action=""store_true"",\n        default=False,\n        help=""Recover a previous training from the state in serialization_dir."",\n    )\n    args = parser.parse_args()\n\n    params = Params.from_file(params_file=args.config_file_path)\n    serialization_dir = args.serialization_dir\n    create_serialization_dir(params, serialization_dir, args.recover)\n\n    serialization_params = deepcopy(params).as_dict(quiet=True)\n    with open(os.path.join(serialization_dir, ""config.json""), ""w"") as param_file:\n        json.dump(serialization_params, param_file, indent=4)\n\n    ### Instantiate the different tasks from the param file, load datasets and create vocabulary ###\n    tasks, vocab = tasks_and_vocab_from_params(params=params, serialization_dir=serialization_dir)\n\n    ### Load the data iterators for each task ###\n    tasks = create_and_set_iterators(params=params, task_list=tasks, vocab=vocab)\n\n    ### Load Regularizations ###\n    regularizer = RegularizerApplicator.from_params(params.pop(""regularizer"", []))\n\n    ### Create model ###\n    model_params = params.pop(""model"")\n    model = Model.from_params(vocab=vocab, params=model_params, regularizer=regularizer)\n\n    ### Create multi-task trainer ###\n    multi_task_trainer_params = params.pop(""multi_task_trainer"")\n    trainer = MultiTaskTrainer.from_params(\n        model=model, task_list=tasks, serialization_dir=serialization_dir, params=multi_task_trainer_params\n    )\n\n    ### Launch training ###\n    metrics = train_model(multi_task_trainer=trainer, recover=args.recover)\n    if metrics is not None:\n        logging.info(""Training is finished ! Let\'s have a drink. It\'s on the house !"")\n'"
demo/hmtlPredictor.py,12,"b'# coding: utf-8\n\nimport os\nimport argparse\nfrom typing import List, Dict, Any, Iterable\nimport torch\nimport torch.nn.functional as F\nimport math\nimport spacy\nimport re\nfrom emoji import UNICODE_EMOJI\n\nfrom allennlp.models.model import Model\nfrom allennlp.common.params import Params\nfrom allennlp.data import Vocabulary, Token, Instance\nfrom allennlp.data.token_indexers import TokenIndexer\nfrom allennlp.data.fields import TextField, Field, ListField, SpanField\nfrom allennlp.data.dataset import Batch\nfrom allennlp.data.dataset_readers.dataset_utils import enumerate_spans\nfrom allennlp.nn import util\n\nimport sys\n\nsys.path.append(""../"")\nimport hmtl\nfrom predictionFormatter import predictionFormatter\n\nimport logging\n\nlogging.basicConfig(\n    format=""%(asctime)s - %(levelname)s - %(name)s -   %(message)s"", datefmt=""%m/%d/%Y %H:%M:%S"", level=logging.INFO\n)\nlogger = logging.getLogger(__name__)\n\n\ntorch.set_num_threads(1)\n\nMAX_STRING_SIZE = 500\nCOREF_MAX_SPAN_WIDTH = 8\n\n\ndef is_only_emoji(text):\n    """"""\n    Test if an incoming text is only composed of emojis.\n    """"""\n    result = True\n    for c in text:\n        if c not in UNICODE_EMOJI:\n            result = False\n            break\n    return result\n\n\ndef filter_messages(input_text, sent):\n    """"""\n    Filter messages which are either too long or only emojis.\n    """"""\n    # Filter messages which are not long enough or too long\n    if len(input_text) >= MAX_STRING_SIZE:\n        return True\n\n    # Filter messages which are only emojis\n    if is_only_emoji(input_text):\n        return True\n\n    return False\n\n\ndef create_instance(sentence, vocab, token_indexers):\n    """"""\n    Create an batch tensor from the input sentence.\n    """"""\n    text = TextField([Token(word) for word in sentence], token_indexers=token_indexers)\n\n    spans = []\n    for start, end in enumerate_spans(sentence, offset=0, max_span_width=COREF_MAX_SPAN_WIDTH):\n        spans.append(SpanField(start, end, text))\n    span_field = ListField(spans)\n\n    instance = Instance({""tokens"": text, ""spans"": span_field})\n\n    instances = [instance]\n    batch = Batch(instances)\n    batch.index_instances(vocab)\n    batch_tensor = batch.as_tensor_dict(batch.get_padding_lengths())\n\n    return batch_tensor\n\n\ndef load_model(model_name=""conll_full_elmo""):\n    """"""\n    Load both vocabulary and model and create and instance of\n    HMTL full model.\n    """"""\n    if model_name not in [""conll_small_elmo"", ""conll_medium_elmo"", ""conll_full_elmo""]:\n        raise ValueError(f""{model_name} is not a valid name of model."")\n    serialization_dir = ""model_dumps"" + ""/"" + model_name\n    params = Params.from_file(params_file=os.path.join(serialization_dir, ""config.json""))\n\n    # Load TokenIndexer\n    task_keys = [key for key in params.keys() if re.search(""^task_"", key)]\n    token_indexer_params = params.pop(task_keys[-1]).pop(""data_params"").pop(""dataset_reader"").pop(""token_indexers"")\n    # see https://github.com/allenai/allennlp/issues/181 for better syntax\n    token_indexers = {}\n    for name, indexer_params in token_indexer_params.items():\n        token_indexers[name] = TokenIndexer.from_params(indexer_params)\n\n    # Load the vocabulary\n    logger.info(""Loading Vocavulary from %s"", os.path.join(serialization_dir, ""vocabulary""))\n    vocab = Vocabulary.from_files(os.path.join(serialization_dir, ""vocabulary""))\n    logger.info(""Vocabulary loaded"")\n\n    # Create model and load weights\n    model_params = params.pop(""model"")\n    model = Model.from_params(vocab=vocab, params=model_params, regularizer=None)\n    model_state_path = os.path.join(serialization_dir, ""weights.th"")\n    model_state = torch.load(model_state_path, map_location=""cpu"")\n    model.load_state_dict(state_dict=model_state)\n\n    return model, vocab, token_indexers\n\n\nclass HMTLPredictor:\n    """"""\n    Predictor class for HMTL full model.\n    """"""\n\n    def __init__(self, model_name=""conll_full_elmo""):\n        model, vocab, token_indexers = load_model(model_name=model_name)\n        self.model = model\n        self.vocab = vocab\n        self.token_indexers = token_indexers\n        self.formatter = predictionFormatter()\n        self.nlp = spacy.load(""en_core_web_sm"")\n\n    def predict(self, input_text, raw_format=False):\n        """"""\n        Take an input text and compute its prediction with HMTL model.\n        If sentence is 2 tokens or less, coreference is not called.\n        """"""\n        with torch.no_grad():\n            self.model.eval()\n\n            ### Prepare batch ###\n            input_text = input_text.strip()\n            sent, sent_char_offset, doc = self.parse_text(input_text=input_text)\n\n            message_filtered = filter_messages(input_text=input_text, sent=sent)\n\n            if message_filtered:\n                final_output = self.fallback_prediction(input_text=input_text, sent=sent)\n            else:\n                if len(sent) < 3:\n                    required_tasks = [""ner"", ""emd"", ""relation""]\n                else:\n                    required_tasks = [""ner"", ""emd"", ""relation"", ""coref""]\n\n                batch_tensor = create_instance(sentence=sent, vocab=self.vocab, token_indexers=self.token_indexers)\n                final_output = self.inference(batch=batch_tensor, required_tasks=required_tasks)\n                final_output[""tokenized_text""] = sent\n\n                if ""coref"" not in final_output.keys():\n                    final_output[""coref""] = [[]]\n\n            if not raw_format:\n                final_output = self.formatter.format(final_output, sent_char_offset, input_text)\n                final_output = self.formatter.expand(final_output, doc)\n\n            return message_filtered, final_output\n\n    # def inference(self,\n    #             tensor_batch,\n    #             task_name: str = ""emd""):\n    #     # pylint: disable=arguments-differ\n\n    #     tagger = getattr(self, ""_tagger_%s"" % task_name)\n    #     output = tagger.forward(**tensor_batch)\n\n    #     decoding_dict = tagger.decode(output)\n    #     return decoding_dict\n\n    def inference(self, batch, required_tasks):\n        """"""\n        Fast inference of HMTL.\n        """"""\n        # pylint: disable=arguments-differ\n\n        final_output = {}\n\n        ### Fast inference of NER ###\n        output_ner, embedded_text_input_base, encoded_text_ner, mask = self.inference_ner(batch)\n        decoding_dict_ner = self.decode(task_output=output_ner, task_name=""ner"")\n        final_output[""ner""] = decoding_dict_ner[""tags""]\n\n        ### Fast inference of EMD ###\n        output_emd, _, encoded_text_emd, mask = self.inference_emd(embedded_text_input_base, encoded_text_ner, mask)\n        decoding_dict_emd = self.decode(task_output=output_emd, task_name=""emd"")\n        final_output[""emd""] = decoding_dict_emd[""tags""]\n\n        ### Fast inference of Relation ###\n        output_relation, embedded_text_input_relation, mask = self.inference_relation(\n            embedded_text_input_base, encoded_text_emd, mask\n        )\n        decoding_dict_relation = self.decode(task_output=output_relation, task_name=""relation"")\n        final_output[""relation""] = decoding_dict_relation[""decoded_predictions""]\n\n        ### Fast inference of Coreference ##\n        if ""coref"" in required_tasks:\n            output_coref = self.inference_coref(batch, embedded_text_input_relation, mask)\n            decoding_dict_coref = self.decode(task_output=output_coref, task_name=""coref"")\n            final_output[""coref""] = decoding_dict_coref[""clusters""]\n\n        return final_output\n\n    def inference_ner(self, batch):\n        submodel = self.model._tagger_ner\n\n        ### Fast inference of NER ###\n        tokens = batch[""tokens""]\n        embedded_text_input_base = submodel.text_field_embedder(tokens)\n        mask = util.get_text_field_mask(tokens)\n\n        encoded_text_ner = submodel.encoder(embedded_text_input_base, mask)\n\n        logits = submodel.tag_projection_layer(encoded_text_ner)\n        best_paths = submodel.crf.viterbi_tags(logits, mask)\n\n        predicted_tags = [x for x, y in best_paths]\n\n        output = {""tags"": predicted_tags}\n\n        return output, embedded_text_input_base, encoded_text_ner, mask\n\n    def inference_emd(self, embedded_text_input_base, encoded_text_ner, mask):\n        submodel = self.model._tagger_emd\n\n        ### Fast inference of EMD ###\n        embedded_text_input_emd = torch.cat([embedded_text_input_base, encoded_text_ner], dim=-1)\n\n        encoded_text_emd = submodel.encoder(embedded_text_input_emd, mask)\n\n        logits = submodel.tag_projection_layer(encoded_text_emd)\n        best_paths = submodel.crf.viterbi_tags(logits, mask)\n\n        predicted_tags = [x for x, y in best_paths]\n\n        output = {""tags"": predicted_tags}\n\n        return output, embedded_text_input_emd, encoded_text_emd, mask\n\n    def inference_relation(self, embedded_text_input_base, encoded_text_emd, mask):\n        submodel = self.model._tagger_relation\n\n        ### Fast inference of Relation ###\n        embedded_text_input_relation = torch.cat([embedded_text_input_base, encoded_text_emd], dim=-1)\n\n        encoded_text_relation = submodel._context_layer(embedded_text_input_relation, mask)\n\n        left = torch.matmul(encoded_text_relation, submodel._U)\n        right = torch.matmul(encoded_text_relation, submodel._W)\n        left = left.permute(1, 0, 2)\n        left = left.unsqueeze(3)\n        right = right.permute(0, 2, 1)\n        right = right.unsqueeze(0)\n        B = left + right\n        B = B.permute(1, 0, 3, 2)\n\n        outer_sum_bias = B + submodel._b\n        if submodel._activation == ""relu"":\n            activated_outer_sum_bias = F.relu(outer_sum_bias)\n        elif submodel._activation == ""tanh"":\n            activated_outer_sum_bias = F.tanh(outer_sum_bias)\n\n        relation_scores = torch.matmul(activated_outer_sum_bias, submodel._V)\n        relation_sigmoid_scores = torch.sigmoid(relation_scores)\n        predicted_relations = torch.round(relation_sigmoid_scores)\n\n        output = {""predicted_relations"": predicted_relations, ""relation_sigmoid_scores"": relation_sigmoid_scores}\n\n        return output, embedded_text_input_relation, mask\n\n    def inference_coref(self, batch, embedded_text_input_relation, mask):\n        submodel = self.model._tagger_coref\n\n        ### Fast inference of coreference ###\n        spans = batch[""spans""]\n\n        document_length = mask.size(1)\n        num_spans = spans.size(1)\n\n        span_mask = (spans[:, :, 0] >= 0).squeeze(-1).float()\n        spans = F.relu(spans.float()).long()\n\n        encoded_text_coref = submodel._context_layer(embedded_text_input_relation, mask)\n        endpoint_span_embeddings = submodel._endpoint_span_extractor(encoded_text_coref, spans)\n        attended_span_embeddings = submodel._attentive_span_extractor(embedded_text_input_relation, spans)\n\n        span_embeddings = torch.cat([endpoint_span_embeddings, attended_span_embeddings], -1)\n        num_spans_to_keep = int(math.floor(submodel._spans_per_word * document_length))\n\n        (top_span_embeddings, top_span_mask, top_span_indices, top_span_mention_scores) = submodel._mention_pruner(\n            span_embeddings, span_mask, num_spans_to_keep\n        )\n        top_span_mask = top_span_mask.unsqueeze(-1)\n        flat_top_span_indices = util.flatten_and_batch_shift_indices(top_span_indices, num_spans)\n        top_spans = util.batched_index_select(spans, top_span_indices, flat_top_span_indices)\n\n        max_antecedents = min(submodel._max_antecedents, num_spans_to_keep)\n\n        valid_antecedent_indices, valid_antecedent_offsets, valid_antecedent_log_mask = submodel._generate_valid_antecedents(\n            num_spans_to_keep, max_antecedents, util.get_device_of(mask)\n        )\n        candidate_antecedent_embeddings = util.flattened_index_select(top_span_embeddings, valid_antecedent_indices)\n\n        candidate_antecedent_mention_scores = util.flattened_index_select(\n            top_span_mention_scores, valid_antecedent_indices\n        ).squeeze(-1)\n        span_pair_embeddings = submodel._compute_span_pair_embeddings(\n            top_span_embeddings, candidate_antecedent_embeddings, valid_antecedent_offsets\n        )\n        coreference_scores = submodel._compute_coreference_scores(\n            span_pair_embeddings,\n            top_span_mention_scores,\n            candidate_antecedent_mention_scores,\n            valid_antecedent_log_mask,\n        )\n\n        _, predicted_antecedents = coreference_scores.max(2)\n        predicted_antecedents -= 1\n\n        output_dict = {\n            ""top_spans"": top_spans,\n            ""antecedent_indices"": valid_antecedent_indices,\n            ""predicted_antecedents"": predicted_antecedents,\n        }\n\n        return output_dict\n\n    def decode(self, task_output, task_name: str = ""ner""):\n        """"""\n        Decode the predictions.\n        """"""\n        tagger = getattr(self.model, ""_tagger_%s"" % task_name)\n        return tagger.decode(task_output)\n\n    def parse_text(self, input_text):\n        """"""\n        Tokenized the input sentence, extract the tokens and their first character index in the sentence.\n        """"""\n        doc = self.nlp(input_text)\n        sent = [word.string.strip() for word in doc]\n        sent_char_offset = [word.idx for word in doc]\n        return sent, sent_char_offset, doc\n\n    def fallback_prediction(self, input_text, sent):\n        """"""\n        If message is filtered (message is too long or emoji), return a default API output.\n        """"""\n        return {\n            ""text"": input_text,\n            ""tokenized_text"": sent,\n            ""ner"": [[]],\n            ""emd"": [[]],\n            ""relation"": [[]],\n            ""relation_debug"": [[]],\n            ""coref"": [[]],\n        }\n\n\nif __name__ == ""__main__"":\n    # Parse arguments\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        ""-m"", ""--model_name"", default=""conll_full_elmo"", required=False, type=str, help=""Name of the model to use.""\n    )\n    args = parser.parse_args()\n\n    hmtl = HMTLPredictor(model_name=args.model_name)\n\n    input_text = ""Her sister used to swim with Barack Obama. He is not bad, but she is better. I like watching her.""\n    output = hmtl.predict(input_text=input_text)\n    print(output)\n'"
demo/predictionFormatter.py,0,"b'# coding: utf-8\n\nfrom allennlp.data.dataset_readers.dataset_utils.span_utils import bioul_tags_to_spans\nimport copy\n\n\ndef find_indices(lst, condition):\n    """"""\n    Find the indices of elements in a list `lst` that match a condition.\n    """"""\n    return [i for i, elem in enumerate(lst) if condition(elem)]\n\n\ndef expand_arg(relation, arg_nb, n_c, formatted_relation):\n    """"""\n    Update formatted_relation to take into account the possible expansion of relation arguments.\n    """"""\n    arg_str = ""arg{}"".format(arg_nb)\n    arg = (relation[arg_str + ""_begin_char""], relation[arg_str + ""_end_char""], relation[arg_str + ""_text""])\n    if arg in n_c.keys():\n        formatted_relation[arg_str + ""_begin_char""] = n_c[arg].start_char\n        formatted_relation[arg_str + ""_end_char""] = n_c[arg].end_char\n\n        formatted_relation[arg_str + ""_begin_token""] = n_c[arg].start\n        formatted_relation[arg_str + ""_end_token""] = n_c[arg].end - 1\n\n        formatted_relation[arg_str + ""_text""] = n_c[arg].text\n    else:\n        formatted_relation[arg_str + ""_begin_token""] = relation.get(arg_str + ""_index"")\n        formatted_relation[arg_str + ""_end_token""] = relation.get(arg_str + ""_index"")\n\n\ndef check_overlapping(relation, formatted_relation):\n    """"""\n    Check if there is no overlapping between the two expanded arguments of a relation.\n    If there is an overlap, we drop the expansion for the relation.\n    """"""\n    arg1_b, arg1_e = formatted_relation[""arg1_begin_char""], formatted_relation[""arg1_end_char""]\n    arg2_b, arg2_e = formatted_relation[""arg2_begin_char""], formatted_relation[""arg2_end_char""]\n\n    overlap = False or (arg1_b < arg2_e and arg1_e >= arg2_b) or (arg2_b < arg1_e and arg2_e >= arg1_b)\n\n    if overlap:\n        arg1_i, arg2_i = relation[""arg1_index""], relation[""arg2_index""]\n        relation[""arg1_begin_token""], relation[""arg1_end_token""] = arg1_i, arg1_i\n        relation[""arg2_begin_token""], relation[""arg2_end_token""] = arg2_i, arg2_i\n        return relation\n    else:\n        return formatted_relation\n\n\nclass predictionFormatter:\n    """"""\n    A class that format the prediction returned by HMTL model.\n    If necessary, it also expands\n    """"""\n\n    def __init__(self):\n        pass\n\n    def format(self, predictions, sent_char_offset, input_text):\n        tokenized_text = predictions[""tokenized_text""]\n        predicted_tasks = predictions.keys()\n\n        formatted_predictions = {}\n        formatted_predictions[""tokenized_text""] = tokenized_text\n\n        ### Format NER and EMD ###\n        for task_name in [""ner"", ""emd""]:\n            if task_name in predicted_tasks:\n                decoded_bioul = []\n                assert len(predictions[task_name]) == 1\n\n                spans = bioul_tags_to_spans(predictions[task_name][0])\n                for tag, (begin, end) in spans:\n                    entity = {\n                        ""type"": tag,\n                        ""begin_token"": begin,\n                        ""end_token"": end,\n                        ""begin_char"": sent_char_offset[begin],\n                        ""end_char"": sent_char_offset[end] + len(tokenized_text[end]),\n                        ""tokenized_text"": tokenized_text[begin : (end + 1)],\n                        ""text"": input_text[\n                            sent_char_offset[begin] : (sent_char_offset[end] + len(tokenized_text[end]))\n                        ],\n                    }\n                    decoded_bioul.append(entity)\n\n                formatted_predictions[task_name] = decoded_bioul\n\n        ### Format Relation ###\n        if ""relation"" in predicted_tasks:\n            decoded_relation_arcs = []\n            assert len(predictions[""relation""]) == 1\n\n            for i, relation in enumerate(predictions[""relation""][0]):\n                indices = find_indices(relation, lambda x: x != ""*"")\n                for ind in indices:\n                    tag = relation[ind]\n                    if tag[:4] == ""ARG1"":\n                        arg1_index, arg1_text = ind, tokenized_text[ind]\n                    if tag[:4] == ""ARG2"":\n                        arg2_index, arg2_text = ind, tokenized_text[ind]\n                rel = {\n                    ""type"": tag[5:],\n                    ""arg1_index"": arg1_index,\n                    ""arg1_text"": arg1_text,\n                    ""arg1_begin_char"": sent_char_offset[arg1_index],\n                    ""arg1_end_char"": sent_char_offset[arg1_index] + len(arg1_text),\n                    ""arg2_index"": arg2_index,\n                    ""arg2_text"": arg2_text,\n                    ""arg2_begin_char"": sent_char_offset[arg2_index],\n                    ""arg2_end_char"": sent_char_offset[arg2_index] + len(arg2_text),\n                }\n                decoded_relation_arcs.append(rel)\n\n            formatted_predictions[""relation_arcs""] = decoded_relation_arcs\n\n        ### Format Coreference ###\n        if ""coref"" in predicted_tasks:\n            decoded_coref_arcs = []\n            decoded_coref_clusters = []\n            assert len(predictions[""coref""]) == 1\n\n            for cluster in predictions[""coref""][0]:\n                ## Format the clusters\n                decoded_cluster = []\n                for mention in cluster:\n                    begin, end = mention\n                    m = {\n                        ""begin"": begin,\n                        ""end"": end,\n                        ""begin_char"": sent_char_offset[begin],\n                        ""end_char"": sent_char_offset[end] + len(tokenized_text[end]),\n                        ""tokenized_text"": tokenized_text[begin : (end + 1)],\n                        ""text"": input_text[\n                            sent_char_offset[begin] : (sent_char_offset[end] + len(tokenized_text[end]))\n                        ],\n                    }\n                    decoded_cluster.append(m)\n                decoded_coref_clusters.append(decoded_cluster)\n\n                ## Format the arcs\n                for i in range(len(cluster) - 1):\n                    mention1_begin, mention1_end = cluster[i]\n                    mention2_begin, mention2_end = cluster[i + 1]\n                    coref_arc = {\n                        ""mention1_begin"": mention1_begin,\n                        ""mention1_end"": mention1_end,\n                        ""mention1_begin_char"": sent_char_offset[mention1_begin],\n                        ""mention1_end_char"": sent_char_offset[mention1_end] + len(tokenized_text[mention1_end]),\n                        ""tokenized_text1"": tokenized_text[mention1_begin : (mention1_end + 1)],\n                        ""text1"": input_text[\n                            sent_char_offset[mention1_begin] : (\n                                sent_char_offset[mention1_end] + len(tokenized_text[mention1_end])\n                            )\n                        ],\n                        ""mention2_begin"": mention2_begin,\n                        ""mention2_end"": mention2_end,\n                        ""mention2_begin_char"": sent_char_offset[mention2_begin],\n                        ""mention2_end_char"": sent_char_offset[mention2_end] + len(tokenized_text[mention2_end]),\n                        ""tokenized_text2"": tokenized_text[mention2_begin : (mention2_end + 1)],\n                        ""text2"": input_text[\n                            sent_char_offset[mention2_begin] : (\n                                sent_char_offset[mention2_end] + len(tokenized_text[mention2_end])\n                            )\n                        ],\n                    }\n                    decoded_coref_arcs.append(coref_arc)\n\n            formatted_predictions[""coref_arcs""] = decoded_coref_arcs\n            formatted_predictions[""coref_clusters""] = decoded_coref_clusters\n\n        return formatted_predictions\n\n    def expand_relations(self, predictions, doc):\n        """"""\n        HMTL predicts the relation between the last head tokens.\n        This is a simple heuristic to expand relations using a dependecy tree.\n        """"""\n        if ""relation_arcs"" in predictions:\n            predictions[""relation_arcs_expanded""] = []\n            noun_chunks = {}\n            for chunk in doc.noun_chunks:\n                noun_chunks[(chunk.root.idx, chunk.root.idx + len(chunk.root.text), chunk.root.text)] = chunk\n\n            for relation in predictions[""relation_arcs""]:\n                formatted_relation = copy.deepcopy(relation)\n\n                expand_arg(relation, 1, noun_chunks, formatted_relation)\n                expand_arg(relation, 2, noun_chunks, formatted_relation)\n                formatted_relation = check_overlapping(relation, formatted_relation)\n\n                del formatted_relation[""arg1_index""], formatted_relation[""arg2_index""]\n\n                predictions[""relation_arcs_expanded""].append(formatted_relation)\n\n        return predictions\n\n    def expand_emd(self, predictions, doc):\n        """"""\n        HMTL predicts the heads of a mention.\n        Simple heuristic to expand entity mentions using a dependecy tree.\n        """"""\n        if ""emd"" in predictions:\n            noun_chunks = {}\n            for chunk in doc.noun_chunks:\n                noun_chunks[(chunk.root.idx, chunk.root.idx + len(chunk.root.text), chunk.root.text)] = chunk\n\n            predictions[""emd_expanded""] = []\n            for emd in predictions[""emd""]:\n                expanded_emd = copy.deepcopy(emd)\n                id_ = (emd[""begin_char""], emd[""end_char""], emd[""text""])\n\n                if id_ in noun_chunks.keys():\n                    expanded_emd[""begin_char""] = noun_chunks[id_].start_char\n                    expanded_emd[""end_char""] = noun_chunks[id_].end_char\n\n                    expanded_emd[""begin_token""] = noun_chunks[id_].start\n                    expanded_emd[""end_token""] = noun_chunks[id_].end - 1\n\n                    expanded_emd[""text""] = noun_chunks[id_].text\n                    expanded_emd[""tokenized_text""] = [token.text for token in noun_chunks[id_]]\n\n                predictions[""emd_expanded""].append(expanded_emd)\n\n        return predictions\n\n    def expand(self, predictions, doc):\n        """"""\n        Perform both EMD and Relation expansion\n        """"""\n        predictions = self.expand_relations(predictions, doc)\n        predictions = self.expand_emd(predictions, doc)\n\n        return predictions\n'"
demo/server.py,0,"b'# coding: utf-8\n\nimport json\nimport falcon\nimport sys\nfrom statsd import StatsClient\n\nfrom hmtlPredictor import HMTLPredictor\n\nSTATSD = StatsClient()\n\n\nclass AllResource(object):\n    def __init__(self, model_name=""conll_full_elmo"", mode=""demo""):\n        self.jmd = HMTLPredictor(model_name=model_name)\n        self.mode = mode\n        print(f""Server loaded with model {model_name}"")\n        self.response = None\n\n    def on_get(self, req, resp):\n        self.response = {}\n        text = req.get_param(""text"")  # Input text\n        STATSD.incr(f""huggingNLP-{self.mode}.msg"")\n\n        raw_format = req.get_param_as_bool(""raw"", required=False, blank_as_true=False)  # Non-formatted output\n        raw_format = False if raw_format is None else raw_format\n\n        self.response[""text""] = text\n\n        if text is not None:\n            STATSD.incr(f""huggingNLP-{self.mode}.not-empty-msg"")\n            with STATSD.timer(f""huggingNLP-{self.mode}.timing.inference""):\n                message_filtered, model_prediction = self.jmd.predict(text, raw_format=raw_format)\n                for key, value in model_prediction.items():\n                    self.response[key] = value\n\n                if message_filtered:\n                    STATSD.incr(f""huggingNLP-{self.mode}.filtered-msg"")\n        else:\n            STATSD.incr(f""huggingNLP-{self.mode}.empty-msg"")\n\n        resp.body = json.dumps(self.response)\n        resp.content_type = ""application/json""\n        resp.append_header(""Access-Control-Allow-Origin"", ""*"")\n        resp.status = falcon.HTTP_200\n\n\ndef build_app(model_name=""conll_full_elmo"", mode=""demo""):\n    APP = falcon.API()\n    APP.req_options.auto_parse_qs_csv = False\n    ALL_RESOURCE = AllResource(model_name=model_name, mode=mode)\n    APP.add_route(""/jmd"", ALL_RESOURCE)\n    return APP\n'"
hmtl/__init__.py,0,b'# coding: utf-8\n\nfrom hmtl.dataset_readers import *\nfrom hmtl.modules import *\nfrom hmtl.models import *\nfrom hmtl.tasks import *\nfrom hmtl.training import *\n'
hmtl/common/__init__.py,0,b'# coding: utf-8\n\nfrom hmtl.common.util import create_and_set_iterators\n'
hmtl/common/util.py,0,"b'# coding: utf-8\n\n""""""\nVarious utilities that don\'t fit anwhere else.\n""""""\n\nfrom typing import List, Dict, Any, Tuple\n\nfrom allennlp.common.params import Params\nfrom allennlp.data import Vocabulary\nfrom allennlp.data.iterators import DataIterator\n\nfrom hmtl.tasks import Task\n\n\ndef create_and_set_iterators(params: Params, task_list: List[Task], vocab: Vocabulary) -> List[Task]:\n    """"""\n    Each task/dataset can have its own specific data iterator. If not precised,\n    we use a shared/common data iterator.\n    \n    Parameters\n    ----------\n    params: ``Params``\n        A parameter object specifing an experiment.\n    task_list: ``List[Task]``\n        A list containing the tasks of the model to train.\n        \n    Returns\n    -------\n    task_list: ``List[Task]``\n        The list containing the tasks of the model to train, where each task has a new attribute: the data iterator.\n    """"""\n    ### Charge default iterator ###\n    iterators_params = params.pop(""iterators"")\n\n    default_iterator_params = iterators_params.pop(""iterator"")\n    default_iterator = DataIterator.from_params(default_iterator_params)\n    default_iterator.index_with(vocab)\n\n    ### Charge dataset specific iterators ###\n    for task in task_list:\n        specific_iterator_params = iterators_params.pop(""iterator_"" + task._name, None)\n        if specific_iterator_params is not None:\n            specific_iterator = DataIterator.from_params(specific_iterator_params)\n            specific_iterator.index_with(vocab)\n            task.set_data_iterator(specific_iterator)\n        else:\n            task.set_data_iterator(default_iterator)\n\n    return task_list\n'"
hmtl/dataset_readers/__init__.py,0,b'# coding: utf-8\n\nfrom hmtl.dataset_readers.ner_ontonotes import NerOntonotesReader\nfrom hmtl.dataset_readers.mention_ace import MentionACEReader\nfrom hmtl.dataset_readers.relation_ace import RelationACEReader\nfrom hmtl.dataset_readers.coref_ace import CorefACEReader\n'
hmtl/dataset_readers/coref_ace.py,0,"b'# coding: utf-8\n\nimport logging\nimport collections\nfrom typing import Any, Dict, List, Optional, Tuple, DefaultDict, Set\n\nfrom overrides import overrides\n\nfrom allennlp.common import Params\nfrom allennlp.common.file_utils import cached_path\nfrom allennlp.data.dataset_readers.dataset_reader import DatasetReader\nfrom allennlp.data.fields import Field, ListField, TextField, SpanField, MetadataField, SequenceLabelField\nfrom allennlp.data.instance import Instance\nfrom allennlp.data.tokenizers import Token\nfrom allennlp.data.token_indexers import SingleIdTokenIndexer, TokenIndexer\nfrom allennlp.data.dataset_readers.dataset_utils import enumerate_spans\n\nfrom hmtl.dataset_readers.dataset_utils import ACE\n\nlogger = logging.getLogger(__name__)  # pylint: disable=invalid-name\n\n\ndef canonicalize_clusters(clusters: DefaultDict[int, List[Tuple[int, int]]]) -> List[List[Tuple[int, int]]]:\n    """"""\n    The CoNLL 2012 data includes 2 annotatated spans which are identical,\n    but have different ids. This checks all clusters for spans which are\n    identical, and if it finds any, merges the clusters containing the\n    identical spans.\n    """"""\n    merged_clusters: List[Set[Tuple[int, int]]] = []\n    for cluster in clusters.values():\n        cluster_with_overlapping_mention = None\n        for mention in cluster:\n            # Look at clusters we have already processed to\n            # see if they contain a mention in the current\n            # cluster for comparison.\n            for cluster2 in merged_clusters:\n                if mention in cluster2:\n                    # first cluster in merged clusters\n                    # which contains this mention.\n                    cluster_with_overlapping_mention = cluster2\n                    break\n            # Already encountered overlap - no need to keep looking.\n            if cluster_with_overlapping_mention is not None:\n                break\n        if cluster_with_overlapping_mention is not None:\n            # Merge cluster we are currently processing into\n            # the cluster in the processed list.\n            cluster_with_overlapping_mention.update(cluster)\n        else:\n            merged_clusters.append(set(cluster))\n    return [list(c) for c in merged_clusters]\n\n\n@DatasetReader.register(""coref_ace"")\nclass CorefACEReader(DatasetReader):\n    """"""\n    A dataset reader to read the coref clusters from an ACE dataset\n    previously pre-procesed to fit the CoNLL-coreference format.\n\n    Parameters\n    ----------\n    max_span_width: ``int``, required.\n        The maximum width of candidate spans to consider.\n    token_indexers : ``Dict[str, TokenIndexer]``, optional\n        This is used to index the words in the document.  See :class:`TokenIndexer`.\n        Default is ``{""tokens"": SingleIdTokenIndexer()}``.\n    lazy : ``bool``, optional (default = False)\n        Whether or not the dataset should be loaded in lazy way. \n    """"""\n\n    def __init__(self, max_span_width: int, token_indexers: Dict[str, TokenIndexer] = None, lazy: bool = False) -> None:\n        super().__init__(lazy)\n        self._max_span_width = max_span_width\n        self._token_indexers = token_indexers or {""tokens"": SingleIdTokenIndexer()}\n\n    @overrides\n    def _read(self, file_path: str):\n        # if `file_path` is a URL, redirect to the cache\n        file_path = cached_path(file_path)\n\n        ace_reader = ACE()\n        for sentences in ace_reader.dataset_document_iterator(file_path):\n            clusters: DefaultDict[int, List[Tuple[int, int]]] = collections.defaultdict(list)\n\n            total_tokens = 0\n            for sentence in sentences:\n                for typed_span in sentence.coref_spans:\n                    # Coref annotations are on a _per sentence_\n                    # basis, so we need to adjust them to be relative\n                    # to the length of the document.\n                    span_id, (start, end) = typed_span\n                    clusters[span_id].append((start + total_tokens, end + total_tokens))\n                total_tokens += len(sentence.words)\n\n            canonical_clusters = canonicalize_clusters(clusters)\n            yield self.text_to_instance([s.words for s in sentences], canonical_clusters)\n\n    @overrides\n    def text_to_instance(\n        self, sentences: List[List[str]], gold_clusters: Optional[List[List[Tuple[int, int]]]] = None  # type: ignore\n    ) -> Instance:\n        # pylint: disable=arguments-differ\n        """"""\n        Parameters\n        ----------\n        sentences : ``List[List[str]]``, required.\n            A list of lists representing the tokenised words and sentences in the document.\n        gold_clusters : ``Optional[List[List[Tuple[int, int]]]]``, optional (default = None)\n            A list of all clusters in the document, represented as word spans. Each cluster\n            contains some number of spans, which can be nested and overlap, but will never\n            exactly match between clusters.\n\n        Returns\n        -------\n        An ``Instance`` containing the following ``Fields``:\n            text : ``TextField``\n                The text of the full document.\n            spans : ``ListField[SpanField]``\n                A ListField containing the spans represented as ``SpanFields``\n                with respect to the document text.\n            span_labels : ``SequenceLabelField``, optional\n                The id of the cluster which each possible span belongs to, or -1 if it does\n                 not belong to a cluster. As these labels have variable length (it depends on\n                 how many spans we are considering), we represent this a as a ``SequenceLabelField``\n                 with respect to the ``spans ``ListField``.\n        """"""\n        flattened_sentences = [self._normalize_word(word) for sentence in sentences for word in sentence]\n\n        metadata: Dict[str, Any] = {""original_text"": flattened_sentences}\n        if gold_clusters is not None:\n            metadata[""clusters""] = gold_clusters\n\n        text_field = TextField([Token(word) for word in flattened_sentences], self._token_indexers)\n\n        cluster_dict = {}\n        if gold_clusters is not None:\n            for cluster_id, cluster in enumerate(gold_clusters):\n                for mention in cluster:\n                    cluster_dict[tuple(mention)] = cluster_id\n\n        spans: List[Field] = []\n        span_labels: Optional[List[int]] = [] if gold_clusters is not None else None\n\n        sentence_offset = 0\n        for sentence in sentences:\n            for start, end in enumerate_spans(sentence, offset=sentence_offset, max_span_width=self._max_span_width):\n                if span_labels is not None:\n                    if (start, end) in cluster_dict:\n                        span_labels.append(cluster_dict[(start, end)])\n                    else:\n                        span_labels.append(-1)\n\n                spans.append(SpanField(start, end, text_field))\n            sentence_offset += len(sentence)\n\n        span_field = ListField(spans)\n        metadata_field = MetadataField(metadata)\n\n        fields: Dict[str, Field] = {""text"": text_field, ""spans"": span_field, ""metadata"": metadata_field}\n        if span_labels is not None:\n            fields[""span_labels""] = SequenceLabelField(span_labels, span_field)\n\n        return Instance(fields)\n\n    @staticmethod\n    def _normalize_word(word):\n        if word == ""/."" or word == ""/?"":\n            return word[1:]\n        else:\n            return word\n'"
hmtl/dataset_readers/mention_ace.py,0,"b'# coding: utf-8\n\nimport logging\nfrom typing import Dict, List, Iterable, Iterator\n\nfrom overrides import overrides\nimport codecs\n\nfrom allennlp.common import Params\nfrom allennlp.common.checks import ConfigurationError\nfrom allennlp.common.file_utils import cached_path\nfrom allennlp.data.dataset_readers.dataset_reader import DatasetReader\nfrom allennlp.data.dataset_readers.dataset_utils import iob1_to_bioul\nfrom allennlp.data.fields import Field, TextField, SequenceLabelField\nfrom allennlp.data.instance import Instance\nfrom allennlp.data.token_indexers import SingleIdTokenIndexer, TokenIndexer\nfrom allennlp.data.tokenizers import Token\nfrom allennlp.data.dataset_readers.dataset_utils import Ontonotes, OntonotesSentence\n\nfrom hmtl.dataset_readers.dataset_utils import ACE, ACESentence\n\nlogger = logging.getLogger(__name__)  # pylint: disable=invalid-name\n\n\n@DatasetReader.register(""mention_ace"")\nclass MentionACEReader(DatasetReader):\n    """"""\n    A dataset reader to read the Entity Mention Tags from an ACE dataset\n    previously pre-procesed to fit the CoNll-NER format.\n    """"""\n\n    def __init__(\n        self,\n        token_indexers: Dict[str, TokenIndexer] = None,\n        label_namespace: str = ""ace_mention_labels"",\n        lazy: bool = False,\n    ) -> None:\n        super().__init__(lazy)\n        self._token_indexers = token_indexers or {""tokens"": SingleIdTokenIndexer()}\n        self._label_namespace = label_namespace\n\n    @staticmethod\n    def _sentence_iterate(ace_reader: ACE, file_path: str) -> Iterable[ACESentence]:\n        for conll_file in ace_reader.dataset_path_iterator(file_path):\n            yield from ace_reader.sentence_iterator(conll_file)\n\n    @overrides\n    def _read(self, file_path: str):\n        file_path = cached_path(file_path)  # if `file_path` is a URL, redirect to the cache\n        ace_reader = ACE()\n        logger.info(""Reading ACE Mention instances from dataset files at: %s"", file_path)\n\n        for sentence in self._sentence_iterate(ace_reader, file_path):\n            tokens = [Token(t) for t in sentence.words]\n            if not sentence.mention_tags:\n                tags = [""O"" for _ in tokens]\n            else:\n                tags = sentence.mention_tags\n\n            yield self.text_to_instance(tokens, tags)\n\n    def text_to_instance(self, tokens: List[Token], tags: List[str] = None) -> Instance:\n        # pylint: disable=arguments-differ\n        fields: Dict[str, Field] = {}\n        text_field = TextField(tokens, token_indexers=self._token_indexers)\n        fields[""tokens""] = text_field\n        if tags:\n            fields[""tags""] = SequenceLabelField(\n                labels=tags, sequence_field=text_field, label_namespace=self._label_namespace\n            )\n        return Instance(fields)\n'"
hmtl/dataset_readers/ner_ontonotes.py,0,"b'# coding: utf-8\n\nimport logging\nfrom typing import Dict, List, Iterable\n\nfrom overrides import overrides\n\nfrom allennlp.common import Params\nfrom allennlp.common.checks import ConfigurationError\nfrom allennlp.common.file_utils import cached_path\nfrom allennlp.data.dataset_readers.dataset_reader import DatasetReader\nfrom allennlp.data.dataset_readers.dataset_utils import iob1_to_bioul\nfrom allennlp.data.fields import Field, TextField, SequenceLabelField\nfrom allennlp.data.instance import Instance\nfrom allennlp.data.token_indexers import SingleIdTokenIndexer, TokenIndexer\nfrom allennlp.data.tokenizers import Token\nfrom allennlp.data.dataset_readers.dataset_utils import Ontonotes, OntonotesSentence\n\n\nlogger = logging.getLogger(__name__)  # pylint: disable=invalid-name\n\n\n@DatasetReader.register(""ner_ontonotes"")\nclass NerOntonotesReader(DatasetReader):\n    """"""\n    An ``allennlp.data.dataset_readers.dataset_reader.DatasetReader`` for reading\n    NER annotations in CoNll-formatted OntoNotes dataset.\n    \n    NB: This DatasetReader was implemented before the current implementation of \n    ``OntonotesNamedEntityRecognition`` in AllenNLP. It is thought doing pretty much the same thing.\n    \n    Parameters\n    ----------\n    token_indexers : ``Dict[str, TokenIndexer]``, optional (default=``{""tokens"": SingleIdTokenIndexer()}``)\n        We use this to define the input representation for the text.  See :class:`TokenIndexer`.\n        Map a token to an id.\n    domain_identifier : ``str``, optional (default = None)\n        The subdomain to load. If None is specified, the whole dataset is loaded.\n    label_namespace : ``str``, optional (default = ""ontonotes_ner_labels"")\n        The tag/label namespace for the task/dataset considered.\n    lazy : ``bool``, optional (default = False)\n        Whether or not the dataset should be loaded in lazy way. \n        Refer to https://github.com/allenai/allennlp/blob/master/tutorials/getting_started/laziness.md\n        for more details about lazyness.\n    coding_scheme: ``str``, optional (default=``IOB1``)\n        Specifies the coding scheme for ``ner_labels`` and ``chunk_labels``.\n        Valid options are ``IOB1`` and ``BIOUL``.  The ``IOB1`` default maintains\n        the original IOB1 scheme in the CoNLL data.\n        In the IOB1 scheme, I is a token inside a span, O is a token outside\n        a span and B is the beginning of span immediately following another\n        span of the same type.\n    """"""\n\n    def __init__(\n        self,\n        token_indexers: Dict[str, TokenIndexer] = None,\n        domain_identifier: str = None,\n        label_namespace: str = ""ontonotes_ner_labels"",\n        lazy: bool = False,\n        coding_scheme: str = ""IOB1"",\n    ) -> None:\n        super().__init__(lazy)\n        self._token_indexers = token_indexers or {""tokens"": SingleIdTokenIndexer()}\n        self._domain_identifier = domain_identifier\n        self._label_namespace = label_namespace\n        self._coding_scheme = coding_scheme\n        if coding_scheme not in (""IOB1"", ""BIOUL""):\n            raise ConfigurationError(""unknown coding_scheme: {}"".format(coding_scheme))\n\n    @overrides\n    def _read(self, file_path: str):\n        file_path = cached_path(file_path)  # if `file_path` is a URL, redirect to the cache\n        ontonotes_reader = Ontonotes()\n        logger.info(""Reading NER instances from dataset files at: %s"", file_path)\n        if self._domain_identifier is not None:\n            logger.info(""Filtering to only include file paths containing the %s domain"", self._domain_identifier)\n\n        for sentence in self._ontonotes_subset(ontonotes_reader, file_path, self._domain_identifier):\n            tokens = [Token(t) for t in sentence.words]\n            if not sentence.named_entities:\n                tags = [""O"" for _ in tokens]\n            else:\n                tags = sentence.named_entities\n\n            if self._coding_scheme == ""BIOUL"":\n                tags = iob1_to_bioul(tags)\n\n            yield self.text_to_instance(tokens, tags)\n\n    @staticmethod\n    def _ontonotes_subset(\n        ontonotes_reader: Ontonotes, file_path: str, domain_identifier: str\n    ) -> Iterable[OntonotesSentence]:\n        for conll_file in ontonotes_reader.dataset_path_iterator(file_path):\n            yield from ontonotes_reader.sentence_iterator(conll_file)\n\n    def text_to_instance(self, tokens: List[Token], tags: List[str] = None) -> Instance:\n        # pylint: disable=arguments-differ\n        fields: Dict[str, Field] = {}\n        text_field = TextField(tokens, token_indexers=self._token_indexers)\n        fields[""tokens""] = text_field\n        if tags:\n            fields[""tags""] = SequenceLabelField(\n                labels=tags, sequence_field=text_field, label_namespace=self._label_namespace\n            )\n        return Instance(fields)\n'"
hmtl/dataset_readers/relation_ace.py,0,"b'# coding: utf-8\n\nimport logging\nfrom typing import Dict, List, Iterable, Iterator\n\nfrom overrides import overrides\nimport codecs\n\nfrom allennlp.common import Params\nfrom allennlp.common.checks import ConfigurationError\nfrom allennlp.common.file_utils import cached_path\nfrom allennlp.data.dataset_readers.dataset_reader import DatasetReader\nfrom allennlp.data.dataset_readers.dataset_utils import iob1_to_bioul\nfrom allennlp.data.fields import Field, TextField, SequenceLabelField, ListField\nfrom allennlp.data.instance import Instance\nfrom allennlp.data.token_indexers import SingleIdTokenIndexer, TokenIndexer\nfrom allennlp.data.tokenizers import Token\nfrom allennlp.data.dataset_readers.dataset_utils import Ontonotes, OntonotesSentence\n\nfrom hmtl.dataset_readers.dataset_utils import ACE, ACESentence\n\n# from hmtl.fields import MultipleSequenceLabelField\n\nlogger = logging.getLogger(__name__)  # pylint: disable=invalid-name\n\n\n@DatasetReader.register(""relation_ace"")\nclass RelationACEReader(DatasetReader):\n    """"""\n    A dataset reader to read the relations links from an ACE dataset\n    previously pre-procesed to fit the CoNLL-SRL format.\n    """"""\n\n    def __init__(\n        self,\n        token_indexers: Dict[str, TokenIndexer] = None,\n        label_namespace: str = ""relation_ace_labels"",\n        lazy: bool = False,\n    ) -> None:\n        super().__init__(lazy)\n        self._token_indexers = token_indexers or {""tokens"": SingleIdTokenIndexer()}\n        self._label_namespace = label_namespace\n\n    @staticmethod\n    def _sentence_iterate(ace_reader: ACE, file_path: str) -> Iterable[ACESentence]:\n        for conll_file in ace_reader.dataset_path_iterator(file_path):\n            yield from ace_reader.sentence_iterator(conll_file)\n\n    @overrides\n    def _read(self, file_path: str):\n        file_path = cached_path(file_path)  # if `file_path` is a URL, redirect to the cache\n        ace_reader = ACE()\n        logger.info(""Reading Relation labels from dataset files at: %s"", file_path)\n\n        for sentence in self._sentence_iterate(ace_reader, file_path):\n            tokens = [Token(t) for t in sentence.words]\n\n            if sentence.relations == []:\n                relations = None\n                continue\n            else:\n                relations = sentence.last_head_token_relations\n                yield self.text_to_instance(tokens, relations)\n\n    def text_to_instance(self, tokens: List[Token], relations=None) -> Instance:\n        # pylint: disable=arguments-differ\n        fields: Dict[str, Field] = {}\n        text_field = TextField(tokens, token_indexers=self._token_indexers)\n        fields[""text""] = text_field\n        if relations is not None:\n            field_list = []\n            for relation in relations:\n                field_list.append(\n                    SequenceLabelField(\n                        labels=relation, sequence_field=text_field, label_namespace=self._label_namespace\n                    )\n                )\n            fields[""relations""] = ListField(field_list=field_list)\n        return Instance(fields)\n'"
hmtl/models/__init__.py,0,b'# coding: utf-8\n\nfrom hmtl.models.coref_custom import CoreferenceCustom\nfrom hmtl.models.relation_extraction import RelationExtractor\n\n# Single Module\nfrom hmtl.models.layerNer import LayerNer\nfrom hmtl.models.layerRelation import LayerRelation\nfrom hmtl.models.layerCoref import LayerCoref\n\n# Two modules\nfrom hmtl.models.layerNerEmd import LayerNerEmd\nfrom hmtl.models.layerEmdRelation import LayerEmdRelation\nfrom hmtl.models.layerEmdCoref import LayerEmdCoref\n\n# Three modules\nfrom hmtl.models.layerNerEmdCoref import LayerNerEmdCoref\nfrom hmtl.models.layerNerEmdRelation import LayerNerEmdRelation\n\n# Four modules\nfrom hmtl.models.hmtl import HMTL\n'
hmtl/models/coref_custom.py,10,"b'import logging\nimport math\nfrom typing import Any, Dict, List, Optional, Tuple\n\nimport torch\nimport torch.nn.functional as F\nfrom overrides import overrides\n\nfrom allennlp.data import Vocabulary\nfrom allennlp.models.model import Model\nfrom allennlp.modules.token_embedders import Embedding\nfrom allennlp.modules import FeedForward\nfrom allennlp.modules import Seq2SeqEncoder, TimeDistributed, TextFieldEmbedder, SpanPruner\nfrom allennlp.modules.span_extractors import SelfAttentiveSpanExtractor, EndpointSpanExtractor\nfrom allennlp.nn import util, InitializerApplicator, RegularizerApplicator\nfrom allennlp.training.metrics import MentionRecall, ConllCorefScores\nfrom allennlp.models.coreference_resolution import CoreferenceResolver\n\nfrom hmtl.training.metrics import ConllCorefFullScores\n\nlogger = logging.getLogger(__name__)  # pylint: disable=invalid-name\n\n\nclass CoreferenceCustom(CoreferenceResolver):\n    """"""\n    This class implements a marginally modified version of ``allennlp.models.coreference_resolution.CoreferenceResolver``\n    which is an implementation of the model of Lee et al., 2017.\n    The two modifications are:\n        1/ Replacing the scorer to be able to get the 3 detailled coreference metrics (B3, MUC, CEAFE),\n        and not only their average.\n        2/ Give the possibility to evaluate with the gold mentions: the model first predict mentions that MIGHT\n        be part of a coreference cluster, and in second time predict the coreference clusters for theses mentions.\n        We leave the possibility of replacing predicting the possible mentions \n        with the gold mentions in evaluation.\n    """"""\n\n    def __init__(\n        self,\n        vocab: Vocabulary,\n        text_field_embedder: TextFieldEmbedder,\n        context_layer: Seq2SeqEncoder,\n        mention_feedforward: FeedForward,\n        antecedent_feedforward: FeedForward,\n        feature_size: int,\n        max_span_width: int,\n        spans_per_word: float,\n        max_antecedents: int,\n        lexical_dropout: float = 0.2,\n        initializer: InitializerApplicator = InitializerApplicator(),\n        regularizer: Optional[RegularizerApplicator] = None,\n        eval_on_gold_mentions: bool = False,\n    ) -> None:\n        super(CoreferenceCustom, self).__init__(\n            vocab=vocab,\n            text_field_embedder=text_field_embedder,\n            context_layer=context_layer,\n            mention_feedforward=mention_feedforward,\n            antecedent_feedforward=antecedent_feedforward,\n            feature_size=feature_size,\n            max_span_width=max_span_width,\n            spans_per_word=spans_per_word,\n            max_antecedents=max_antecedents,\n            lexical_dropout=lexical_dropout,\n            initializer=initializer,\n            regularizer=regularizer,\n        )\n\n        self._conll_coref_scores = ConllCorefFullScores()\n        self._eval_on_gold_mentions = eval_on_gold_mentions\n\n        if self._eval_on_gold_mentions:\n            self._use_gold_mentions = False\n        else:\n            self._use_gold_mentions = None\n\n    @overrides\n    def get_metrics(self, reset: bool = False, full: bool = False):\n        mention_recall = self._mention_recall.get_metric(reset=reset)\n        metrics = self._conll_coref_scores.get_metric(reset=reset, full=full)\n        metrics[""mention_recall""] = mention_recall\n\n        return metrics\n\n    @overrides\n    def forward(\n        self,  # type: ignore\n        text: Dict[str, torch.LongTensor],\n        spans: torch.IntTensor,\n        span_labels: torch.IntTensor = None,\n        metadata: List[Dict[str, Any]] = None,\n    ) -> Dict[str, torch.Tensor]:\n        # pylint: disable=arguments-differ\n\n        # Shape: (batch_size, document_length, embedding_size)\n        text_embeddings = self._lexical_dropout(self._text_field_embedder(text))\n\n        document_length = text_embeddings.size(1)\n\n        # Shape: (batch_size, document_length)\n        text_mask = util.get_text_field_mask(text).float()\n\n        # Shape: (batch_size, num_spans)\n        if self._use_gold_mentions:\n            if text_embeddings.is_cuda:\n                device = torch.device(""cuda"")\n            else:\n                device = torch.device(""cpu"")\n\n            s = [\n                torch.as_tensor(pair, dtype=torch.long, device=device)\n                for cluster in metadata[0][""clusters""]\n                for pair in cluster\n            ]\n            gm = torch.stack(s, dim=0).unsqueeze(0).unsqueeze(1)\n\n            span_mask = spans.unsqueeze(2) - gm\n            span_mask = (span_mask[:, :, :, 0] == 0) + (span_mask[:, :, :, 1] == 0)\n            span_mask, _ = (span_mask == 2).max(-1)\n            num_spans = span_mask.sum().item()\n            span_mask = span_mask.float()\n        else:\n            span_mask = (spans[:, :, 0] >= 0).squeeze(-1).float()\n            num_spans = spans.size(1)\n        # Shape: (batch_size, num_spans, 2)\n        spans = F.relu(spans.float()).long()\n\n        # Shape: (batch_size, document_length, encoding_dim)\n        contextualized_embeddings = self._context_layer(text_embeddings, text_mask)\n        # Shape: (batch_size, num_spans, 2 * encoding_dim + feature_size)\n        endpoint_span_embeddings = self._endpoint_span_extractor(contextualized_embeddings, spans)\n        # Shape: (batch_size, num_spans, emebedding_size)\n        attended_span_embeddings = self._attentive_span_extractor(text_embeddings, spans)\n\n        # Shape: (batch_size, num_spans, emebedding_size + 2 * encoding_dim + feature_size)\n        span_embeddings = torch.cat([endpoint_span_embeddings, attended_span_embeddings], -1)\n\n        # Prune based on mention scores.\n        num_spans_to_keep = int(math.floor(self._spans_per_word * document_length))\n\n        (top_span_embeddings, top_span_mask, top_span_indices, top_span_mention_scores) = self._mention_pruner(\n            span_embeddings, span_mask, num_spans_to_keep\n        )\n        top_span_mask = top_span_mask.unsqueeze(-1)\n        # Shape: (batch_size * num_spans_to_keep)\n        flat_top_span_indices = util.flatten_and_batch_shift_indices(top_span_indices, num_spans)\n\n        # Compute final predictions for which spans to consider as mentions.\n        # Shape: (batch_size, num_spans_to_keep, 2)\n        top_spans = util.batched_index_select(spans, top_span_indices, flat_top_span_indices)\n\n        # Compute indices for antecedent spans to consider.\n        max_antecedents = min(self._max_antecedents, num_spans_to_keep)\n\n        # Shapes:\n        # (num_spans_to_keep, max_antecedents),\n        # (1, max_antecedents),\n        # (1, num_spans_to_keep, max_antecedents)\n        valid_antecedent_indices, valid_antecedent_offsets, valid_antecedent_log_mask = self._generate_valid_antecedents(\n            num_spans_to_keep, max_antecedents, util.get_device_of(text_mask)\n        )\n        # Select tensors relating to the antecedent spans.\n        # Shape: (batch_size, num_spans_to_keep, max_antecedents, embedding_size)\n        candidate_antecedent_embeddings = util.flattened_index_select(top_span_embeddings, valid_antecedent_indices)\n\n        # Shape: (batch_size, num_spans_to_keep, max_antecedents)\n        candidate_antecedent_mention_scores = util.flattened_index_select(\n            top_span_mention_scores, valid_antecedent_indices\n        ).squeeze(-1)\n        # Compute antecedent scores.\n        # Shape: (batch_size, num_spans_to_keep, max_antecedents, embedding_size)\n        span_pair_embeddings = self._compute_span_pair_embeddings(\n            top_span_embeddings, candidate_antecedent_embeddings, valid_antecedent_offsets\n        )\n        # Shape: (batch_size, num_spans_to_keep, 1 + max_antecedents)\n        coreference_scores = self._compute_coreference_scores(\n            span_pair_embeddings,\n            top_span_mention_scores,\n            candidate_antecedent_mention_scores,\n            valid_antecedent_log_mask,\n        )\n\n        # Shape: (batch_size, num_spans_to_keep)\n        _, predicted_antecedents = coreference_scores.max(2)\n        predicted_antecedents -= 1\n\n        output_dict = {\n            ""top_spans"": top_spans,\n            ""antecedent_indices"": valid_antecedent_indices,\n            ""predicted_antecedents"": predicted_antecedents,\n        }\n        if span_labels is not None:\n            # Find the gold labels for the spans which we kept.\n            pruned_gold_labels = util.batched_index_select(\n                span_labels.unsqueeze(-1), top_span_indices, flat_top_span_indices\n            )\n\n            antecedent_labels = util.flattened_index_select(pruned_gold_labels, valid_antecedent_indices).squeeze(-1)\n            antecedent_labels += valid_antecedent_log_mask.long()\n\n            # Compute labels.\n            # Shape: (batch_size, num_spans_to_keep, max_antecedents + 1)\n            gold_antecedent_labels = self._compute_antecedent_gold_labels(pruned_gold_labels, antecedent_labels)\n            coreference_log_probs = util.last_dim_log_softmax(coreference_scores, top_span_mask)\n            correct_antecedent_log_probs = coreference_log_probs + gold_antecedent_labels.log()\n            negative_marginal_log_likelihood = -util.logsumexp(correct_antecedent_log_probs).sum()\n\n            self._mention_recall(top_spans, metadata)\n            self._conll_coref_scores(top_spans, valid_antecedent_indices, predicted_antecedents, metadata)\n\n            output_dict[""loss""] = negative_marginal_log_likelihood\n\n        if metadata is not None:\n            output_dict[""document""] = [x[""original_text""] for x in metadata]\n        return output_dict\n'"
hmtl/models/hmtl.py,1,"b'# coding: utf-8\n\nimport os\nimport sys\nimport logging\nfrom typing import Dict\nfrom overrides import overrides\n\nimport torch\n\nfrom allennlp.common import Params\nfrom allennlp.common.checks import ConfigurationError\nfrom allennlp.data import Vocabulary\nfrom allennlp.models.model import Model\nfrom allennlp.modules import Seq2SeqEncoder, TextFieldEmbedder\nfrom allennlp.nn import RegularizerApplicator, InitializerApplicator\nfrom allennlp.modules.text_field_embedders import BasicTextFieldEmbedder\nfrom allennlp.modules import FeedForward\nfrom allennlp.models.crf_tagger import CrfTagger\n\nfrom hmtl.modules.text_field_embedders import ShortcutConnectTextFieldEmbedder\nfrom hmtl.models.relation_extraction import RelationExtractor\nfrom hmtl.models import CoreferenceCustom\n\nlogger = logging.getLogger(__name__)  # pylint: disable=invalid-name\n\n\n@Model.register(""hmtl"")\nclass HMTL(Model):\n    """"""\n    A class that implement the full HMTL model.\n    \n    Parameters\n    ----------\n    vocab: ``allennlp.data.Vocabulary``, required.\n        The vocabulary fitted on the data.\n    params: ``allennlp.common.Params``, required\n        Configuration parameters for the multi-task model.\n    regularizer: ``allennlp.nn.RegularizerApplicator``, optional (default = None)\n        A reguralizer to apply to the model\'s layers.\n    """"""\n\n    def __init__(self, vocab: Vocabulary, params: Params, regularizer: RegularizerApplicator = None):\n\n        super(HMTL, self).__init__(vocab=vocab, regularizer=regularizer)\n\n        # Base text Field Embedder\n        text_field_embedder_params = params.pop(""text_field_embedder"")\n        text_field_embedder = BasicTextFieldEmbedder.from_params(vocab=vocab, params=text_field_embedder_params)\n        self._text_field_embedder = text_field_embedder\n\n        ############\n        # NER Stuffs\n        ############\n        ner_params = params.pop(""ner"")\n\n        # Encoder\n        encoder_ner_params = ner_params.pop(""encoder"")\n        encoder_ner = Seq2SeqEncoder.from_params(encoder_ner_params)\n        self._encoder_ner = encoder_ner\n\n        # Tagger NER - CRF Tagger\n        tagger_ner_params = ner_params.pop(""tagger"")\n        tagger_ner = CrfTagger(\n            vocab=vocab,\n            text_field_embedder=self._text_field_embedder,\n            encoder=self._encoder_ner,\n            label_namespace=tagger_ner_params.pop(""label_namespace"", ""labels""),\n            constraint_type=tagger_ner_params.pop(""constraint_type"", None),\n            dropout=tagger_ner_params.pop(""dropout"", None),\n            regularizer=regularizer,\n        )\n        self._tagger_ner = tagger_ner\n\n        ############\n        # EMD Stuffs\n        ############\n        emd_params = params.pop(""emd"")\n\n        # Encoder\n        encoder_emd_params = emd_params.pop(""encoder"")\n        encoder_emd = Seq2SeqEncoder.from_params(encoder_emd_params)\n        self._encoder_emd = encoder_emd\n\n        shortcut_text_field_embedder = ShortcutConnectTextFieldEmbedder(\n            base_text_field_embedder=self._text_field_embedder, previous_encoders=[self._encoder_ner]\n        )\n        self._shortcut_text_field_embedder = shortcut_text_field_embedder\n\n        # Tagger: EMD - CRF Tagger\n        tagger_emd_params = emd_params.pop(""tagger"")\n        tagger_emd = CrfTagger(\n            vocab=vocab,\n            text_field_embedder=self._shortcut_text_field_embedder,\n            encoder=self._encoder_emd,\n            label_namespace=tagger_emd_params.pop(""label_namespace"", ""labels""),\n            constraint_type=tagger_emd_params.pop(""constraint_type"", None),\n            dropout=tagger_ner_params.pop(""dropout"", None),\n            regularizer=regularizer,\n        )\n        self._tagger_emd = tagger_emd\n\n        ############################\n        # Relation Extraction Stuffs\n        ############################\n        relation_params = params.pop(""relation"")\n\n        # Encoder\n        encoder_relation_params = relation_params.pop(""encoder"")\n        encoder_relation = Seq2SeqEncoder.from_params(encoder_relation_params)\n        self._encoder_relation = encoder_relation\n\n        shortcut_text_field_embedder_relation = ShortcutConnectTextFieldEmbedder(\n            base_text_field_embedder=self._text_field_embedder, previous_encoders=[self._encoder_ner, self._encoder_emd]\n        )\n        self._shortcut_text_field_embedder_relation = shortcut_text_field_embedder_relation\n\n        # Tagger: Relation\n        tagger_relation_params = relation_params.pop(""tagger"")\n        tagger_relation = RelationExtractor(\n            vocab=vocab,\n            text_field_embedder=self._shortcut_text_field_embedder_relation,\n            context_layer=self._encoder_relation,\n            d=tagger_relation_params.pop_int(""d""),\n            l=tagger_relation_params.pop_int(""l""),\n            n_classes=tagger_relation_params.pop(""n_classes""),\n            activation=tagger_relation_params.pop(""activation""),\n        )\n        self._tagger_relation = tagger_relation\n\n        ##############\n        # Coref Stuffs\n        ##############\n        coref_params = params.pop(""coref"")\n\n        # Encoder\n        encoder_coref_params = coref_params.pop(""encoder"")\n        encoder_coref = Seq2SeqEncoder.from_params(encoder_coref_params)\n        self._encoder_coref = encoder_coref\n\n        shortcut_text_field_embedder_coref = ShortcutConnectTextFieldEmbedder(\n            base_text_field_embedder=self._text_field_embedder, previous_encoders=[self._encoder_ner, self._encoder_emd]\n        )\n        self._shortcut_text_field_embedder_coref = shortcut_text_field_embedder_coref\n\n        # Tagger: Coreference\n        tagger_coref_params = coref_params.pop(""tagger"")\n        eval_on_gold_mentions = tagger_coref_params.pop_bool(""eval_on_gold_mentions"", False)\n        init_params = tagger_coref_params.pop(""initializer"", None)\n        initializer = (\n            InitializerApplicator.from_params(init_params) if init_params is not None else InitializerApplicator()\n        )\n\n        tagger_coref = CoreferenceCustom(\n            vocab=vocab,\n            text_field_embedder=self._shortcut_text_field_embedder_coref,\n            context_layer=self._encoder_coref,\n            mention_feedforward=FeedForward.from_params(tagger_coref_params.pop(""mention_feedforward"")),\n            antecedent_feedforward=FeedForward.from_params(tagger_coref_params.pop(""antecedent_feedforward"")),\n            feature_size=tagger_coref_params.pop_int(""feature_size""),\n            max_span_width=tagger_coref_params.pop_int(""max_span_width""),\n            spans_per_word=tagger_coref_params.pop_float(""spans_per_word""),\n            max_antecedents=tagger_coref_params.pop_int(""max_antecedents""),\n            lexical_dropout=tagger_coref_params.pop_float(""lexical_dropout"", 0.2),\n            initializer=initializer,\n            regularizer=regularizer,\n            eval_on_gold_mentions=eval_on_gold_mentions,\n        )\n        self._tagger_coref = tagger_coref\n        if eval_on_gold_mentions:\n            self._tagger_coref._eval_on_gold_mentions = True\n\n        logger.info(""Multi-Task Learning Model has been instantiated."")\n\n    @overrides\n    def forward(self, tensor_batch, for_training: bool = False, task_name: str = ""ner"") -> Dict[str, torch.Tensor]:\n        # pylint: disable=arguments-differ\n\n        tagger = getattr(self, ""_tagger_%s"" % task_name)\n\n        if task_name == ""coref"" and tagger._eval_on_gold_mentions:\n            if for_training:\n                tagger._use_gold_mentions = False\n            else:\n                tagger._use_gold_mentions = True\n\n        return tagger.forward(**tensor_batch)\n\n    @overrides\n    def get_metrics(self, task_name: str, reset: bool = False, full: bool = False) -> Dict[str, float]:\n\n        task_tagger = getattr(self, ""_tagger_"" + task_name)\n        if full and task_name == ""coref"":\n            return task_tagger.get_metrics(reset=reset, full=full)\n        else:\n            return task_tagger.get_metrics(reset)\n\n    @classmethod\n    def from_params(cls, vocab: Vocabulary, params: Params, regularizer: RegularizerApplicator) -> ""HMTL"":\n        return cls(vocab=vocab, params=params, regularizer=regularizer)\n'"
hmtl/models/layerCoref.py,1,"b'# coding: utf-8\n\nimport os\nimport sys\nimport logging\nfrom typing import Dict\nfrom overrides import overrides\n\nimport torch\n\nfrom allennlp.common import Params\nfrom allennlp.common.checks import ConfigurationError\nfrom allennlp.data import Vocabulary\nfrom allennlp.models.model import Model\nfrom allennlp.modules import Seq2SeqEncoder, TextFieldEmbedder\nfrom allennlp.nn import RegularizerApplicator, InitializerApplicator\nfrom allennlp.modules.text_field_embedders import BasicTextFieldEmbedder\nfrom allennlp.modules import FeedForward\nfrom allennlp.models.crf_tagger import CrfTagger\n\nfrom hmtl.modules.text_field_embedders import ShortcutConnectTextFieldEmbedder\nfrom hmtl.models.relation_extraction import RelationExtractor\nfrom hmtl.models import CoreferenceCustom\n\nlogger = logging.getLogger(__name__)  # pylint: disable=invalid-name\n\n\n@Model.register(""coref_custom"")\nclass LayerCoref(Model):\n    """"""\n    A class that implement the one task of HMTL model: Coref (Lee et al).\n    \n    Parameters\n    ----------\n    vocab: ``allennlp.data.Vocabulary``, required.\n        The vocabulary fitted on the data.\n    params: ``allennlp.common.Params``, required\n        Configuration parameters for the multi-task model.\n    regularizer: ``allennlp.nn.RegularizerApplicator``, optional (default = None)\n        A reguralizer to apply to the model\'s layers.\n    """"""\n\n    def __init__(self, vocab: Vocabulary, params: Params, regularizer: RegularizerApplicator = None):\n\n        super(LayerCoref, self).__init__(vocab=vocab, regularizer=regularizer)\n\n        # Base text Field Embedder\n        text_field_embedder_params = params.pop(""text_field_embedder"")\n        text_field_embedder = BasicTextFieldEmbedder.from_params(vocab=vocab, params=text_field_embedder_params)\n        self._text_field_embedder = text_field_embedder\n\n        ##############\n        # Coref Stuffs\n        ##############\n        coref_params = params.pop(""coref"")\n\n        # Encoder\n        encoder_coref_params = coref_params.pop(""encoder"")\n        encoder_coref = Seq2SeqEncoder.from_params(encoder_coref_params)\n        self._encoder_coref = encoder_coref\n\n        # Tagger: Coreference\n        tagger_coref_params = coref_params.pop(""tagger"")\n        eval_on_gold_mentions = tagger_coref_params.pop_bool(""eval_on_gold_mentions"", False)\n        init_params = tagger_coref_params.pop(""initializer"", None)\n        initializer = (\n            InitializerApplicator.from_params(init_params) if init_params is not None else InitializerApplicator()\n        )\n\n        tagger_coref = CoreferenceCustom(\n            vocab=vocab,\n            text_field_embedder=self._text_field_embedder,\n            context_layer=self._encoder_coref,\n            mention_feedforward=FeedForward.from_params(tagger_coref_params.pop(""mention_feedforward"")),\n            antecedent_feedforward=FeedForward.from_params(tagger_coref_params.pop(""antecedent_feedforward"")),\n            feature_size=tagger_coref_params.pop_int(""feature_size""),\n            max_span_width=tagger_coref_params.pop_int(""max_span_width""),\n            spans_per_word=tagger_coref_params.pop_float(""spans_per_word""),\n            max_antecedents=tagger_coref_params.pop_int(""max_antecedents""),\n            lexical_dropout=tagger_coref_params.pop_float(""lexical_dropout"", 0.2),\n            initializer=initializer,\n            regularizer=regularizer,\n            eval_on_gold_mentions=eval_on_gold_mentions,\n        )\n        self._tagger_coref = tagger_coref\n        if eval_on_gold_mentions:\n            self._tagger_coref._eval_on_gold_mentions = True\n\n        logger.info(""Multi-Task Learning Model has been instantiated."")\n\n    @overrides\n    def forward(self, tensor_batch, for_training: bool = False, task_name: str = ""coref"") -> Dict[str, torch.Tensor]:\n        # pylint: disable=arguments-differ\n\n        tagger = getattr(self, ""_tagger_%s"" % task_name)\n\n        if task_name == ""coref"" and tagger._eval_on_gold_mentions:\n            if for_training:\n                tagger._use_gold_mentions = False\n            else:\n                tagger._use_gold_mentions = True\n\n        return tagger.forward(**tensor_batch)\n\n    @overrides\n    def get_metrics(self, task_name: str, reset: bool = False, full: bool = False) -> Dict[str, float]:\n\n        task_tagger = getattr(self, ""_tagger_"" + task_name)\n        if full and task_name == ""coref"":\n            return task_tagger.get_metrics(reset=reset, full=full)\n        else:\n            return task_tagger.get_metrics(reset)\n\n    @classmethod\n    def from_params(cls, vocab: Vocabulary, params: Params, regularizer: RegularizerApplicator) -> ""LayerCoref"":\n        return cls(vocab=vocab, params=params, regularizer=regularizer)\n'"
hmtl/models/layerEmdCoref.py,1,"b'# coding: utf-8\n\nimport os\nimport sys\nimport logging\nfrom typing import Dict\nfrom overrides import overrides\n\nimport torch\n\nfrom allennlp.common import Params\nfrom allennlp.data import Vocabulary\nfrom allennlp.models.model import Model\nfrom allennlp.modules import Seq2SeqEncoder, TextFieldEmbedder\nfrom allennlp.nn import RegularizerApplicator, InitializerApplicator\nfrom allennlp.modules.text_field_embedders import BasicTextFieldEmbedder\nfrom allennlp.modules import FeedForward\nfrom allennlp.models.crf_tagger import CrfTagger\n\nfrom hmtl.modules.text_field_embedders import ShortcutConnectTextFieldEmbedder\nfrom hmtl.models import CoreferenceCustom\n\nlogger = logging.getLogger(__name__)  # pylint: disable=invalid-name\n\n\n@Model.register(""emd_coref"")\nclass LayerEmdCoref(Model):\n    """"""\n    A class that implement two tasks of HMTL model: EMD (CRF Tagger) and Coref (Lee et al., 2017).\n    \n    Parameters\n    ----------\n    vocab: ``allennlp.data.Vocabulary``, required.\n        The vocabulary fitted on the data.\n    params: ``allennlp.common.Params``, required\n        Configuration parameters for the multi-task model.\n    regularizer: ``allennlp.nn.RegularizerApplicator``, optional (default = None)\n        A reguralizer to apply to the model\'s layers.\n    """"""\n\n    def __init__(self, vocab: Vocabulary, params: Params, regularizer: RegularizerApplicator = None):\n\n        super(LayerEmdCoref, self).__init__(vocab=vocab, regularizer=regularizer)\n\n        # Base text Field Embedder\n        text_field_embedder_params = params.pop(""text_field_embedder"")\n        text_field_embedder = BasicTextFieldEmbedder.from_params(vocab=vocab, params=text_field_embedder_params)\n        self._text_field_embedder = text_field_embedder\n\n        ############\n        # EMD Stuffs\n        ############\n        emd_params = params.pop(""emd"")\n\n        # Encoder\n        encoder_emd_params = emd_params.pop(""encoder"")\n        encoder_emd = Seq2SeqEncoder.from_params(encoder_emd_params)\n        self._encoder_emd = encoder_emd\n\n        # Tagger EMD - CRF Tagger\n        tagger_emd_params = emd_params.pop(""tagger"")\n        tagger_emd = CrfTagger(\n            vocab=vocab,\n            text_field_embedder=self._text_field_embedder,\n            encoder=self._encoder_emd,\n            label_namespace=tagger_emd_params.pop(""label_namespace"", ""labels""),\n            constraint_type=tagger_emd_params.pop(""constraint_type"", None),\n            dropout=tagger_emd_params.pop(""dropout"", None),\n            regularizer=regularizer,\n        )\n        self._tagger_emd = tagger_emd\n\n        ##############\n        # Coref Stuffs\n        ##############\n        coref_params = params.pop(""coref"")\n\n        # Encoder\n        encoder_coref_params = coref_params.pop(""encoder"")\n        encoder_coref = Seq2SeqEncoder.from_params(encoder_coref_params)\n        self._encoder_coref = encoder_coref\n\n        shortcut_text_field_embedder_coref = ShortcutConnectTextFieldEmbedder(\n            base_text_field_embedder=self._text_field_embedder, previous_encoders=[self._encoder_emd]\n        )\n        self._shortcut_text_field_embedder_coref = shortcut_text_field_embedder_coref\n\n        # Tagger: Coreference\n        tagger_coref_params = coref_params.pop(""tagger"")\n        eval_on_gold_mentions = tagger_coref_params.pop_bool(""eval_on_gold_mentions"", False)\n        init_params = tagger_coref_params.pop(""initializer"", None)\n        initializer = (\n            InitializerApplicator.from_params(init_params) if init_params is not None else InitializerApplicator()\n        )\n\n        tagger_coref = CoreferenceCustom(\n            vocab=vocab,\n            text_field_embedder=self._shortcut_text_field_embedder_coref,\n            context_layer=self._encoder_coref,\n            mention_feedforward=FeedForward.from_params(tagger_coref_params.pop(""mention_feedforward"")),\n            antecedent_feedforward=FeedForward.from_params(tagger_coref_params.pop(""antecedent_feedforward"")),\n            feature_size=tagger_coref_params.pop_int(""feature_size""),\n            max_span_width=tagger_coref_params.pop_int(""max_span_width""),\n            spans_per_word=tagger_coref_params.pop_float(""spans_per_word""),\n            max_antecedents=tagger_coref_params.pop_int(""max_antecedents""),\n            lexical_dropout=tagger_coref_params.pop_float(""lexical_dropout"", 0.2),\n            initializer=initializer,\n            regularizer=regularizer,\n            eval_on_gold_mentions=eval_on_gold_mentions,\n        )\n        self._tagger_coref = tagger_coref\n        if eval_on_gold_mentions:\n            self._tagger_coref._eval_on_gold_mentions = True\n\n        logger.info(""Multi-Task Learning Model has been instantiated."")\n\n    @overrides\n    def forward(self, tensor_batch, for_training: bool = False, task_name: str = ""emd"") -> Dict[str, torch.Tensor]:\n        # pylint: disable=arguments-differ\n        """"""\n        Special case for forward: for coreference, we can use gold mentions to predict the clusters\n        during evaluation (not during training).\n        """"""\n\n        tagger = getattr(self, ""_tagger_%s"" % task_name)\n\n        if task_name == ""coref"" and tagger._eval_on_gold_mentions:\n            if for_training:\n                tagger._use_gold_mentions = False\n            else:\n                tagger._use_gold_mentions = True\n\n        return tagger.forward(**tensor_batch)\n\n    @overrides\n    def get_metrics(self, task_name: str = ""emd"", reset: bool = False, full: bool = False) -> Dict[str, float]:\n\n        task_tagger = getattr(self, ""_tagger_"" + task_name)\n        if full and task_name == ""coref"":\n            return task_tagger.get_metrics(reset=reset, full=full)\n        else:\n            return task_tagger.get_metrics(reset=reset)\n\n    @classmethod\n    def from_params(cls, vocab: Vocabulary, params: Params, regularizer: RegularizerApplicator) -> ""LayerEmdCoref"":\n        return cls(vocab=vocab, params=params, regularizer=regularizer)\n'"
hmtl/models/layerEmdRelation.py,1,"b'# coding: utf-8\n\nimport os\nimport sys\nimport logging\nfrom typing import Dict\nfrom overrides import overrides\n\nimport torch\n\nfrom allennlp.common import Params\nfrom allennlp.data import Vocabulary\nfrom allennlp.models.model import Model\nfrom allennlp.modules import Seq2SeqEncoder, TextFieldEmbedder\nfrom allennlp.nn import RegularizerApplicator\nfrom allennlp.modules.text_field_embedders import BasicTextFieldEmbedder\nfrom allennlp.models.crf_tagger import CrfTagger\n\nfrom hmtl.modules.text_field_embedders import ShortcutConnectTextFieldEmbedder\nfrom hmtl.models.relation_extraction import RelationExtractor\n\nlogger = logging.getLogger(__name__)  # pylint: disable=invalid-name\n\n\n@Model.register(""emd_relation"")\nclass LayerEmdRelation(Model):\n    """"""\n    A class that implement three tasks of HMTL model: EMD (CRF Tagger) and Relation Extraction.\n    \n    Parameters\n    ----------\n    vocab: ``allennlp.data.Vocabulary``, required.\n        The vocabulary fitted on the data.\n    params: ``allennlp.common.Params``, required\n        Configuration parameters for the multi-task model.\n    regularizer: ``allennlp.nn.RegularizerApplicator``, optional (default = None)\n        A reguralizer to apply to the model\'s layers.\n    """"""\n\n    def __init__(self, vocab: Vocabulary, params: Params, regularizer: RegularizerApplicator = None):\n\n        super(LayerEmdRelation, self).__init__(vocab=vocab, regularizer=regularizer)\n\n        # Base text Field Embedder\n        text_field_embedder_params = params.pop(""text_field_embedder"")\n        text_field_embedder = BasicTextFieldEmbedder.from_params(vocab=vocab, params=text_field_embedder_params)\n        self._text_field_embedder = text_field_embedder\n\n        ############\n        # EMD Stuffs\n        ############\n        emd_params = params.pop(""emd"")\n\n        # Encoder\n        encoder_emd_params = emd_params.pop(""encoder"")\n        encoder_emd = Seq2SeqEncoder.from_params(encoder_emd_params)\n        self._encoder_emd = encoder_emd\n\n        # Tagger EMD - CRF Tagger\n        tagger_emd_params = emd_params.pop(""tagger"")\n        tagger_emd = CrfTagger(\n            vocab=vocab,\n            text_field_embedder=self._text_field_embedder,\n            encoder=self._encoder_emd,\n            label_namespace=tagger_emd_params.pop(""label_namespace"", ""labels""),\n            constraint_type=tagger_emd_params.pop(""constraint_type"", None),\n            dropout=tagger_emd_params.pop(""dropout"", None),\n            regularizer=regularizer,\n        )\n        self._tagger_emd = tagger_emd\n\n        ############################\n        # Relation Extraction Stuffs\n        ############################\n        relation_params = params.pop(""relation"")\n\n        # Encoder\n        encoder_relation_params = relation_params.pop(""encoder"")\n        encoder_relation = Seq2SeqEncoder.from_params(encoder_relation_params)\n        self._encoder_relation = encoder_relation\n\n        shortcut_text_field_embedder_relation = ShortcutConnectTextFieldEmbedder(\n            base_text_field_embedder=self._text_field_embedder, previous_encoders=[self._encoder_emd]\n        )\n        self._shortcut_text_field_embedder_relation = shortcut_text_field_embedder_relation\n\n        # Tagger: Relation\n        tagger_relation_params = relation_params.pop(""tagger"")\n        tagger_relation = RelationExtractor(\n            vocab=vocab,\n            text_field_embedder=self._shortcut_text_field_embedder_relation,\n            context_layer=self._encoder_relation,\n            d=tagger_relation_params.pop_int(""d""),\n            l=tagger_relation_params.pop_int(""l""),\n            n_classes=tagger_relation_params.pop(""n_classes""),\n            activation=tagger_relation_params.pop(""activation""),\n        )\n        self._tagger_relation = tagger_relation\n\n        logger.info(""Multi-Task Learning Model has been instantiated."")\n\n    @overrides\n    def forward(self, tensor_batch, for_training: bool = False, task_name: str = ""ner"") -> Dict[str, torch.Tensor]:\n        # pylint: disable=arguments-differ\n\n        tagger = getattr(self, ""_tagger_%s"" % task_name)\n        return tagger.forward(**tensor_batch)\n\n    @overrides\n    def get_metrics(self, task_name: str, reset: bool = False, full: bool = False) -> Dict[str, float]:\n\n        task_tagger = getattr(self, ""_tagger_"" + task_name)\n        return task_tagger.get_metrics(reset)\n\n    @classmethod\n    def from_params(cls, vocab: Vocabulary, params: Params, regularizer: RegularizerApplicator) -> ""LayerEmdRelation"":\n        return cls(vocab=vocab, params=params, regularizer=regularizer)\n'"
hmtl/models/layerNer.py,1,"b'# coding: utf-8\n\nimport os\nimport sys\nimport logging\nfrom typing import Dict\nfrom overrides import overrides\n\nimport torch\n\nfrom allennlp.common import Params\nfrom allennlp.data import Vocabulary\nfrom allennlp.models.model import Model\nfrom allennlp.modules import Seq2SeqEncoder, TextFieldEmbedder\nfrom allennlp.nn import RegularizerApplicator\nfrom allennlp.modules.text_field_embedders import BasicTextFieldEmbedder\nfrom allennlp.models.crf_tagger import CrfTagger\n\nlogger = logging.getLogger(__name__)  # pylint: disable=invalid-name\n\n\n@Model.register(""ner"")\nclass LayerNer(Model):\n    """"""\n    A class that implement the first task of HMTL model: NER (CRF Tagger).\n    \n    Parameters\n    ----------\n    vocab: ``allennlp.data.Vocabulary``, required.\n        The vocabulary fitted on the data.\n    params: ``allennlp.common.Params``, required\n        Configuration parameters for the multi-task model.\n    regularizer: ``allennlp.nn.RegularizerApplicator``, optional (default = None)\n        A reguralizer to apply to the model\'s layers.\n    """"""\n\n    def __init__(self, vocab: Vocabulary, params: Params, regularizer: RegularizerApplicator = None):\n\n        super(LayerNer, self).__init__(vocab=vocab, regularizer=regularizer)\n\n        # Base Text Field Embedder\n        text_field_embedder_params = params.pop(""text_field_embedder"")\n        text_field_embedder = BasicTextFieldEmbedder.from_params(vocab=vocab, params=text_field_embedder_params)\n        self._text_field_embedder = text_field_embedder\n\n        ############\n        # NER Stuffs\n        ############\n        ner_params = params.pop(""ner"")\n\n        # Encoder\n        encoder_ner_params = ner_params.pop(""encoder"")\n        encoder_ner = Seq2SeqEncoder.from_params(encoder_ner_params)\n        self._encoder_ner = encoder_ner\n\n        # Tagger NER - CRF Tagger\n        tagger_ner_params = ner_params.pop(""tagger"")\n        tagger_ner = CrfTagger(\n            vocab=vocab,\n            text_field_embedder=self._text_field_embedder,\n            encoder=self._encoder_ner,\n            label_namespace=tagger_ner_params.pop(""label_namespace"", ""labels""),\n            constraint_type=tagger_ner_params.pop(""constraint_type"", None),\n            dropout=tagger_ner_params.pop(""dropout"", None),\n            regularizer=regularizer,\n        )\n        self._tagger_ner = tagger_ner\n\n        logger.info(""Multi-Task Learning Model has been instantiated."")\n\n    @overrides\n    def forward(self, tensor_batch, for_training: bool = False, task_name: str = ""ner"") -> Dict[str, torch.Tensor]:\n        # pylint: disable=arguments-differ\n\n        tagger = getattr(self, ""_tagger_%s"" % task_name)\n        return tagger.forward(**tensor_batch)\n\n    @overrides\n    def get_metrics(self, task_name: str = ""ner"", reset: bool = False, full: bool = False) -> Dict[str, float]:\n\n        task_tagger = getattr(self, ""_tagger_"" + task_name)\n        return task_tagger.get_metrics(reset=reset)\n\n    @classmethod\n    def from_params(cls, vocab: Vocabulary, params: Params, regularizer: RegularizerApplicator) -> ""LayerNer"":\n        return cls(vocab=vocab, params=params, regularizer=regularizer)\n'"
hmtl/models/layerNerEmd.py,1,"b'# coding: utf-8\n\nimport os\nimport sys\nimport logging\nfrom typing import Dict\nfrom overrides import overrides\n\nimport torch\n\nfrom allennlp.common import Params\nfrom allennlp.data import Vocabulary\nfrom allennlp.models.model import Model\nfrom allennlp.modules import Seq2SeqEncoder, TextFieldEmbedder\nfrom allennlp.nn import RegularizerApplicator\nfrom allennlp.modules.text_field_embedders import BasicTextFieldEmbedder\nfrom allennlp.models.crf_tagger import CrfTagger\n\nfrom hmtl.modules.text_field_embedders import ShortcutConnectTextFieldEmbedder\n\nlogger = logging.getLogger(__name__)  # pylint: disable=invalid-name\n\n\n@Model.register(""ner_emd"")\nclass LayerNerEmd(Model):\n    """"""\n    A class that implement two tasks of HMTL model: NER (CRF Tagger) and EMD (CRF Tagger).\n    \n    Parameters\n    ----------\n    vocab: ``allennlp.data.Vocabulary``, required.\n        The vocabulary fitted on the data.\n    params: ``allennlp.common.Params``, required\n        Configuration parameters for the multi-task model.\n    regularizer: ``allennlp.nn.RegularizerApplicator``, optional (default = None)\n        A reguralizer to apply to the model\'s layers.\n    """"""\n\n    def __init__(self, vocab: Vocabulary, params: Params, regularizer: RegularizerApplicator = None):\n\n        super(LayerNerEmd, self).__init__(vocab=vocab, regularizer=regularizer)\n\n        # Base text Field Embedder\n        text_field_embedder_params = params.pop(""text_field_embedder"")\n        text_field_embedder = BasicTextFieldEmbedder.from_params(vocab=vocab, params=text_field_embedder_params)\n        self._text_field_embedder = text_field_embedder\n\n        ############\n        # NER Stuffs\n        ############\n        ner_params = params.pop(""ner"")\n\n        # Encoder\n        encoder_ner_params = ner_params.pop(""encoder"")\n        encoder_ner = Seq2SeqEncoder.from_params(encoder_ner_params)\n        self._encoder_ner = encoder_ner\n\n        # Tagger NER - CRF Tagger\n        tagger_ner_params = ner_params.pop(""tagger"")\n        tagger_ner = CrfTagger(\n            vocab=vocab,\n            text_field_embedder=self._text_field_embedder,\n            encoder=self._encoder_ner,\n            label_namespace=tagger_ner_params.pop(""label_namespace"", ""labels""),\n            constraint_type=tagger_ner_params.pop(""constraint_type"", None),\n            dropout=tagger_ner_params.pop(""dropout"", None),\n            regularizer=regularizer,\n        )\n        self._tagger_ner = tagger_ner\n\n        ############\n        # EMD Stuffs\n        ############\n        emd_params = params.pop(""emd"")\n\n        # Encoder\n        encoder_emd_params = emd_params.pop(""encoder"")\n        encoder_emd = Seq2SeqEncoder.from_params(encoder_emd_params)\n        self._encoder_emd = encoder_emd\n\n        shortcut_text_field_embedder = ShortcutConnectTextFieldEmbedder(\n            base_text_field_embedder=self._text_field_embedder, previous_encoders=[self._encoder_ner]\n        )\n        self._shortcut_text_field_embedder = shortcut_text_field_embedder\n\n        # Tagger: EMD - CRF Tagger\n        tagger_emd_params = emd_params.pop(""tagger"")\n        tagger_emd = CrfTagger(\n            vocab=vocab,\n            text_field_embedder=self._shortcut_text_field_embedder,\n            encoder=self._encoder_emd,\n            label_namespace=tagger_emd_params.pop(""label_namespace"", ""labels""),\n            constraint_type=tagger_emd_params.pop(""constraint_type"", None),\n            dropout=tagger_ner_params.pop(""dropout"", None),\n            regularizer=regularizer,\n        )\n        self._tagger_emd = tagger_emd\n\n        logger.info(""Multi-Task Learning Model has been instantiated."")\n\n    @overrides\n    def forward(self, tensor_batch, for_training: bool = False, task_name: str = ""ner"") -> Dict[str, torch.Tensor]:\n        # pylint: disable=arguments-differ\n\n        tagger = getattr(self, ""_tagger_%s"" % task_name)\n        return tagger.forward(**tensor_batch)\n\n    @overrides\n    def get_metrics(self, task_name: str, reset: bool = False, full: bool = False) -> Dict[str, float]:\n\n        task_tagger = getattr(self, ""_tagger_"" + task_name)\n        return task_tagger.get_metrics(reset)\n\n    @classmethod\n    def from_params(cls, vocab: Vocabulary, params: Params, regularizer: RegularizerApplicator) -> ""LayerNerEmd"":\n        return cls(vocab=vocab, params=params, regularizer=regularizer)\n'"
hmtl/models/layerNerEmdCoref.py,1,"b'# coding: utf-8\n\nimport os\nimport sys\nimport logging\nfrom typing import Dict\nfrom overrides import overrides\n\nimport torch\n\nfrom allennlp.common import Params\nfrom allennlp.data import Vocabulary\nfrom allennlp.models.model import Model\nfrom allennlp.modules import Seq2SeqEncoder, TextFieldEmbedder\nfrom allennlp.nn import RegularizerApplicator, InitializerApplicator\nfrom allennlp.modules.text_field_embedders import BasicTextFieldEmbedder\nfrom allennlp.modules import FeedForward\nfrom allennlp.models.crf_tagger import CrfTagger\n\nfrom hmtl.modules.text_field_embedders import ShortcutConnectTextFieldEmbedder\nfrom hmtl.models import CoreferenceCustom\n\nlogger = logging.getLogger(__name__)  # pylint: disable=invalid-name\n\n\n@Model.register(""ner_emd_coref"")\nclass LayerNerEmdCoref(Model):\n    """"""\n    A class that implement three tasks of HMTL model: NER (CRF Tagger), EMD (CRF Tagger) and Coreference Resolution.\n    \n    Parameters\n    ----------\n    vocab: ``allennlp.data.Vocabulary``, required.\n        The vocabulary fitted on the data.\n    params: ``allennlp.common.Params``, required\n        Configuration parameters for the multi-task model.\n    regularizer: ``allennlp.nn.RegularizerApplicator``, optional (default = None)\n        A reguralizer to apply to the model\'s layers.\n    """"""\n\n    def __init__(self, vocab: Vocabulary, params: Params, regularizer: RegularizerApplicator = None):\n\n        super(LayerNerEmdCoref, self).__init__(vocab=vocab, regularizer=regularizer)\n\n        # Base text Field Embedder\n        text_field_embedder_params = params.pop(""text_field_embedder"")\n        text_field_embedder = BasicTextFieldEmbedder.from_params(vocab=vocab, params=text_field_embedder_params)\n        self._text_field_embedder = text_field_embedder\n\n        ############\n        # NER Stuffs\n        ############\n        ner_params = params.pop(""ner"")\n\n        # Encoder\n        encoder_ner_params = ner_params.pop(""encoder"")\n        encoder_ner = Seq2SeqEncoder.from_params(encoder_ner_params)\n        self._encoder_ner = encoder_ner\n\n        # Tagger NER - CRF Tagger\n        tagger_ner_params = ner_params.pop(""tagger"")\n        tagger_ner = CrfTagger(\n            vocab=vocab,\n            text_field_embedder=self._text_field_embedder,\n            encoder=self._encoder_ner,\n            label_namespace=tagger_ner_params.pop(""label_namespace"", ""labels""),\n            constraint_type=tagger_ner_params.pop(""constraint_type"", None),\n            dropout=tagger_ner_params.pop(""dropout"", None),\n            regularizer=regularizer,\n        )\n        self._tagger_ner = tagger_ner\n\n        ############\n        # EMD Stuffs\n        ############\n        emd_params = params.pop(""emd"")\n\n        # Encoder\n        encoder_emd_params = emd_params.pop(""encoder"")\n        encoder_emd = Seq2SeqEncoder.from_params(encoder_emd_params)\n        self._encoder_emd = encoder_emd\n\n        shortcut_text_field_embedder = ShortcutConnectTextFieldEmbedder(\n            base_text_field_embedder=self._text_field_embedder, previous_encoders=[self._encoder_ner]\n        )\n        self._shortcut_text_field_embedder = shortcut_text_field_embedder\n\n        # Tagger: EMD - CRF Tagger\n        tagger_emd_params = emd_params.pop(""tagger"")\n        tagger_emd = CrfTagger(\n            vocab=vocab,\n            text_field_embedder=self._shortcut_text_field_embedder,\n            encoder=self._encoder_emd,\n            label_namespace=tagger_emd_params.pop(""label_namespace"", ""labels""),\n            constraint_type=tagger_emd_params.pop(""constraint_type"", None),\n            dropout=tagger_ner_params.pop(""dropout"", None),\n            regularizer=regularizer,\n        )\n        self._tagger_emd = tagger_emd\n\n        ##############\n        # Coref Stuffs\n        ##############\n        coref_params = params.pop(""coref"")\n\n        # Encoder\n        encoder_coref_params = coref_params.pop(""encoder"")\n        encoder_coref = Seq2SeqEncoder.from_params(encoder_coref_params)\n        self._encoder_coref = encoder_coref\n\n        shortcut_text_field_embedder_coref = ShortcutConnectTextFieldEmbedder(\n            base_text_field_embedder=self._text_field_embedder, previous_encoders=[self._encoder_ner, self._encoder_emd]\n        )\n        self._shortcut_text_field_embedder_coref = shortcut_text_field_embedder_coref\n\n        # Tagger: Coreference\n        tagger_coref_params = coref_params.pop(""tagger"")\n        eval_on_gold_mentions = tagger_coref_params.pop_bool(""eval_on_gold_mentions"", False)\n        init_params = tagger_coref_params.pop(""initializer"", None)\n        initializer = (\n            InitializerApplicator.from_params(init_params) if init_params is not None else InitializerApplicator()\n        )\n\n        tagger_coref = CoreferenceCustom(\n            vocab=vocab,\n            text_field_embedder=self._shortcut_text_field_embedder_coref,\n            context_layer=self._encoder_coref,\n            mention_feedforward=FeedForward.from_params(tagger_coref_params.pop(""mention_feedforward"")),\n            antecedent_feedforward=FeedForward.from_params(tagger_coref_params.pop(""antecedent_feedforward"")),\n            feature_size=tagger_coref_params.pop_int(""feature_size""),\n            max_span_width=tagger_coref_params.pop_int(""max_span_width""),\n            spans_per_word=tagger_coref_params.pop_float(""spans_per_word""),\n            max_antecedents=tagger_coref_params.pop_int(""max_antecedents""),\n            lexical_dropout=tagger_coref_params.pop_float(""lexical_dropout"", 0.2),\n            initializer=initializer,\n            regularizer=regularizer,\n            eval_on_gold_mentions=eval_on_gold_mentions,\n        )\n        self._tagger_coref = tagger_coref\n        if eval_on_gold_mentions:\n            self._tagger_coref._eval_on_gold_mentions = True\n\n        logger.info(""Multi-Task Learning Model has been instantiated."")\n\n    @overrides\n    def forward(self, tensor_batch, for_training: bool = False, task_name: str = ""ner"") -> Dict[str, torch.Tensor]:\n        # pylint: disable=arguments-differ\n        """"""\n        Special case for forward: for coreference, we can use gold mentions to predict the clusters\n        during evaluation (not during training).\n        """"""\n\n        tagger = getattr(self, ""_tagger_%s"" % task_name)\n\n        if task_name == ""coref"" and tagger._eval_on_gold_mentions:\n            if for_training:\n                tagger._use_gold_mentions = False\n            else:\n                tagger._use_gold_mentions = True\n\n        return tagger.forward(**tensor_batch)\n\n    @overrides\n    def get_metrics(self, task_name: str, reset: bool = False, full: bool = False) -> Dict[str, float]:\n\n        task_tagger = getattr(self, ""_tagger_"" + task_name)\n        if full and task_name == ""coref"":\n            return task_tagger.get_metrics(reset=reset, full=full)\n        else:\n            return task_tagger.get_metrics(reset)\n\n    @classmethod\n    def from_params(cls, vocab: Vocabulary, params: Params, regularizer: RegularizerApplicator) -> ""LayerNerEmdCoref"":\n        return cls(vocab=vocab, params=params, regularizer=regularizer)\n'"
hmtl/models/layerNerEmdRelation.py,1,"b'# coding: utf-8\n\nimport os\nimport sys\nimport logging\nfrom typing import Dict\nfrom overrides import overrides\n\nimport torch\n\nfrom allennlp.common import Params\nfrom allennlp.data import Vocabulary\nfrom allennlp.models.model import Model\nfrom allennlp.modules import Seq2SeqEncoder, TextFieldEmbedder\nfrom allennlp.nn import RegularizerApplicator\nfrom allennlp.modules.text_field_embedders import BasicTextFieldEmbedder\nfrom allennlp.models.crf_tagger import CrfTagger\n\nfrom hmtl.modules.text_field_embedders import ShortcutConnectTextFieldEmbedder\nfrom hmtl.models.relation_extraction import RelationExtractor\n\nlogger = logging.getLogger(__name__)  # pylint: disable=invalid-name\n\n\n@Model.register(""ner_emd_relation"")\nclass LayerNerEmdRelation(Model):\n    """"""\n    A class that implement three tasks of HMTL model: NER (CRF Tagger), EMD (CRF Tagger) and Relation Extraction.\n    \n    Parameters\n    ----------\n    vocab: ``allennlp.data.Vocabulary``, required.\n        The vocabulary fitted on the data.\n    params: ``allennlp.common.Params``, required\n        Configuration parameters for the multi-task model.\n    regularizer: ``allennlp.nn.RegularizerApplicator``, optional (default = None)\n        A reguralizer to apply to the model\'s layers.\n    """"""\n\n    def __init__(self, vocab: Vocabulary, params: Params, regularizer: RegularizerApplicator = None):\n\n        super(LayerNerEmdRelation, self).__init__(vocab=vocab, regularizer=regularizer)\n\n        # Base text Field Embedder\n        text_field_embedder_params = params.pop(""text_field_embedder"")\n        text_field_embedder = BasicTextFieldEmbedder.from_params(vocab=vocab, params=text_field_embedder_params)\n        self._text_field_embedder = text_field_embedder\n\n        ############\n        # NER Stuffs\n        ############\n        ner_params = params.pop(""ner"")\n\n        # Encoder\n        encoder_ner_params = ner_params.pop(""encoder"")\n        encoder_ner = Seq2SeqEncoder.from_params(encoder_ner_params)\n        self._encoder_ner = encoder_ner\n\n        # Tagger NER - CRF Tagger\n        tagger_ner_params = ner_params.pop(""tagger"")\n        tagger_ner = CrfTagger(\n            vocab=vocab,\n            text_field_embedder=self._text_field_embedder,\n            encoder=self._encoder_ner,\n            label_namespace=tagger_ner_params.pop(""label_namespace"", ""labels""),\n            constraint_type=tagger_ner_params.pop(""constraint_type"", None),\n            dropout=tagger_ner_params.pop(""dropout"", None),\n            regularizer=regularizer,\n        )\n        self._tagger_ner = tagger_ner\n\n        ############\n        # EMD Stuffs\n        ############\n        emd_params = params.pop(""emd"")\n\n        # Encoder\n        encoder_emd_params = emd_params.pop(""encoder"")\n        encoder_emd = Seq2SeqEncoder.from_params(encoder_emd_params)\n        self._encoder_emd = encoder_emd\n\n        shortcut_text_field_embedder = ShortcutConnectTextFieldEmbedder(\n            base_text_field_embedder=self._text_field_embedder, previous_encoders=[self._encoder_ner]\n        )\n        self._shortcut_text_field_embedder = shortcut_text_field_embedder\n\n        # Tagger: EMD - CRF Tagger\n        tagger_emd_params = emd_params.pop(""tagger"")\n        tagger_emd = CrfTagger(\n            vocab=vocab,\n            text_field_embedder=self._shortcut_text_field_embedder,\n            encoder=self._encoder_emd,\n            label_namespace=tagger_emd_params.pop(""label_namespace"", ""labels""),\n            constraint_type=tagger_emd_params.pop(""constraint_type"", None),\n            dropout=tagger_ner_params.pop(""dropout"", None),\n            regularizer=regularizer,\n        )\n        self._tagger_emd = tagger_emd\n\n        ############################\n        # Relation Extraction Stuffs\n        ############################\n        relation_params = params.pop(""relation"")\n\n        # Encoder\n        encoder_relation_params = relation_params.pop(""encoder"")\n        encoder_relation = Seq2SeqEncoder.from_params(encoder_relation_params)\n        self._encoder_relation = encoder_relation\n\n        shortcut_text_field_embedder_relation = ShortcutConnectTextFieldEmbedder(\n            base_text_field_embedder=self._text_field_embedder, previous_encoders=[self._encoder_ner, self._encoder_emd]\n        )\n        self._shortcut_text_field_embedder_relation = shortcut_text_field_embedder_relation\n\n        # Tagger: Relation\n        tagger_relation_params = relation_params.pop(""tagger"")\n        tagger_relation = RelationExtractor(\n            vocab=vocab,\n            text_field_embedder=self._shortcut_text_field_embedder_relation,\n            context_layer=self._encoder_relation,\n            d=tagger_relation_params.pop_int(""d""),\n            l=tagger_relation_params.pop_int(""l""),\n            n_classes=tagger_relation_params.pop(""n_classes""),\n            activation=tagger_relation_params.pop(""activation""),\n        )\n        self._tagger_relation = tagger_relation\n\n        logger.info(""Multi-Task Learning Model has been instantiated."")\n\n    @overrides\n    def forward(self, tensor_batch, for_training: bool = False, task_name: str = ""ner"") -> Dict[str, torch.Tensor]:\n        # pylint: disable=arguments-differ\n\n        tagger = getattr(self, ""_tagger_%s"" % task_name)\n        return tagger.forward(**tensor_batch)\n\n    @overrides\n    def get_metrics(self, task_name: str, reset: bool = False, full: bool = False) -> Dict[str, float]:\n\n        task_tagger = getattr(self, ""_tagger_"" + task_name)\n        return task_tagger.get_metrics(reset)\n\n    @classmethod\n    def from_params(\n        cls, vocab: Vocabulary, params: Params, regularizer: RegularizerApplicator\n    ) -> ""LayerNerEmdRelation"":\n        return cls(vocab=vocab, params=params, regularizer=regularizer)\n'"
hmtl/models/layerRelation.py,1,"b'# coding: utf-8\n\nimport os\nimport sys\nimport logging\nfrom typing import Dict\nfrom overrides import overrides\n\nimport torch\n\nfrom allennlp.common import Params\nfrom allennlp.data import Vocabulary\nfrom allennlp.models.model import Model\nfrom allennlp.modules import Seq2SeqEncoder, TextFieldEmbedder\nfrom allennlp.nn import RegularizerApplicator\nfrom allennlp.modules.text_field_embedders import BasicTextFieldEmbedder\n\nfrom hmtl.models.relation_extraction import RelationExtractor\n\nlogger = logging.getLogger(__name__)  # pylint: disable=invalid-name\n\n\n@Model.register(""relation"")\nclass LayerRelation(Model):\n    """"""\n    A class that implement one task of HMTL model: Relation Extraction.\n    \n    Parameters\n    ----------\n    vocab: ``allennlp.data.Vocabulary``, required.\n        The vocabulary fitted on the data.\n    params: ``allennlp.common.Params``, required\n        Configuration parameters for the multi-task model.\n    regularizer: ``allennlp.nn.RegularizerApplicator``, optional (default = None)\n        A reguralizer to apply to the model\'s layers.\n    """"""\n\n    def __init__(self, vocab: Vocabulary, params: Params, regularizer: RegularizerApplicator = None):\n\n        super(LayerRelation, self).__init__(vocab=vocab, regularizer=regularizer)\n\n        # Base text Field Embedder\n        text_field_embedder_params = params.pop(""text_field_embedder"")\n        text_field_embedder = BasicTextFieldEmbedder.from_params(vocab=vocab, params=text_field_embedder_params)\n        self._text_field_embedder = text_field_embedder\n\n        ############################\n        # Relation Extraction Stuffs\n        ############################\n        relation_params = params.pop(""relation"")\n\n        # Encoder\n        encoder_relation_params = relation_params.pop(""encoder"")\n        encoder_relation = Seq2SeqEncoder.from_params(encoder_relation_params)\n        self._encoder_relation = encoder_relation\n\n        # Tagger: Relation\n        tagger_relation_params = relation_params.pop(""tagger"")\n        tagger_relation = RelationExtractor(\n            vocab=vocab,\n            text_field_embedder=self._text_field_embedder,\n            context_layer=self._encoder_relation,\n            d=tagger_relation_params.pop_int(""d""),\n            l=tagger_relation_params.pop_int(""l""),\n            n_classes=tagger_relation_params.pop(""n_classes""),\n            activation=tagger_relation_params.pop(""activation""),\n        )\n        self._tagger_relation = tagger_relation\n\n        logger.info(""Multi-Task Learning Model has been instantiated."")\n\n    @overrides\n    def forward(self, tensor_batch, for_training: bool = False, task_name: str = ""relation"") -> Dict[str, torch.Tensor]:\n        # pylint: disable=arguments-differ\n\n        tagger = getattr(self, ""_tagger_%s"" % task_name)\n        return tagger.forward(**tensor_batch)\n\n    @overrides\n    def get_metrics(self, task_name: str = ""relation"", reset: bool = False, full: bool = False) -> Dict[str, float]:\n\n        task_tagger = getattr(self, ""_tagger_"" + task_name)\n        return task_tagger.get_metrics(reset)\n\n    @classmethod\n    def from_params(cls, vocab: Vocabulary, params: Params, regularizer: RegularizerApplicator) -> ""LayerRelation"":\n        return cls(vocab=vocab, params=params, regularizer=regularizer)\n'"
hmtl/models/relation_extraction.py,18,"b'# coding: utf-8\n\nimport logging\nimport math\nfrom typing import Any, Dict, List, Optional, Tuple\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable  # from torch.nn.parameter import Parameter, Variable\n\nfrom overrides import overrides\n\nfrom allennlp.common import Params\nfrom allennlp.data import Vocabulary\nfrom allennlp.models.model import Model\nfrom allennlp.modules import Seq2SeqEncoder, TextFieldEmbedder\nfrom allennlp.modules.span_extractors import SelfAttentiveSpanExtractor, EndpointSpanExtractor\nfrom allennlp.nn import util\n\nfrom hmtl.training.metrics import RelationF1Measure\n\nlogger = logging.getLogger(__name__)  # pylint: disable=invalid-name\n\n\n# Mapping specific to the dataset used in our setting (ACE2005)\n# Please adapt it if necessary\nrel_type_2_idx = {""ORG-AFF"": 0, ""PHYS"": 1, ""ART"": 2, ""PER-SOC"": 3, ""PART-WHOLE"": 4, ""GEN-AFF"": 5}\nidx_2_rel_type = {value: key for key, value in rel_type_2_idx.items()}\n\n\n@Model.register(""relation_extractor"")\nclass RelationExtractor(Model):\n    """"""\n\tA class containing the scoring model for relation extraction.\n\tIt is derived the model proposed by Bekoulis G. in \n\t""Joint entity recognition and relation extraction as a multi-head selection problem""\n\thttps://bekou.github.io/papers/eswa2018b/bekoulis_eswa_2018b.pdf\n\t\n\tParameters\n\t----------\n\tvocab: ``allennlp.data.Vocabulary``, required.\n        The vocabulary fitted on the data.\n\ttext_field_embedder : ``TextFieldEmbedder``, required\n        Used to embed the ``text`` ``TextField`` we get as input to the model.\n    context_layer : ``Seq2SeqEncoder``, required\n        This layer incorporates contextual information for each word in the document.\n\td: ``int``, required\n\t\tThe (half) dimension of embedding given\tby the encoder context_layer.\n\tl: ``int``, required\n\t\tThe dimension of the relation extractor scorer embedding.\n\tn_classes: ``int``, required\n\t\tThe number of different possible relation classes.\n\tactivation: ``str``, optional (default = ""relu"")\n\t\tNon-linear activation function for the scorer. Can be either ""relu"" or ""tanh"".\n\tlabel_namespace: ``str``, optional (default = ""relation_ace_labels"")\n\t\tThe namespace for the labels of the task of relation extraction.\n\t""""""\n\n    def __init__(\n        self,\n        vocab: Vocabulary,\n        text_field_embedder: TextFieldEmbedder,\n        context_layer: Seq2SeqEncoder,\n        d: int,\n        l: int,\n        n_classes: int,\n        activation: str = ""relu"",\n        label_namespace: str = ""relation_ace_labels"",\n    ) -> None:\n        super(RelationExtractor, self).__init__(vocab)\n\n        self._U = nn.Parameter(torch.Tensor(2 * d, l))\n        self._W = nn.Parameter(torch.Tensor(2 * d, l))\n        self._V = nn.Parameter(torch.Tensor(l, n_classes))\n        self._b = nn.Parameter(torch.Tensor(l))\n\n        self.init_weights()\n\n        self._n_classes = n_classes\n        self._activation = activation\n\n        self._text_field_embedder = text_field_embedder\n        self._context_layer = context_layer\n\n        self._label_namespace = label_namespace\n\n        self._relation_metric = RelationF1Measure()\n\n        self._loss_fn = nn.BCEWithLogitsLoss()\n\n    def init_weights(self) -> None:\n        """"""\n\t\tInitialization for the weights of the model.\n\t\t""""""\n        nn.init.kaiming_normal_(self._U)\n        nn.init.kaiming_normal_(self._W)\n        nn.init.kaiming_normal_(self._V)\n\n        nn.init.normal_(self._b)\n\n    def multi_class_cross_entropy_loss(self, scores, labels, mask):\n        """"""\n\t\tCompute the loss from\n\t\t""""""\n        # Compute the mask before computing the loss\n        # Transform the mask that is at the sentence level (#Size: n_batches x padded_document_length)\n        # to a suitable format for the relation labels level\n        padded_document_length = mask.size(1)\n        mask = mask.float()  # Size: n_batches x padded_document_length\n        squared_mask = torch.stack([e.view(padded_document_length, 1) * e for e in mask], dim=0)\n        squared_mask = squared_mask.unsqueeze(-1).repeat(\n            1, 1, 1, self._n_classes\n        )  # Size: n_batches x padded_document_length x padded_document_length x n_classes\n\n        # The scores (and gold labels) are flattened before using\n        # the binary cross entropy loss.\n        # We thus transform\n        flat_size = scores.size()\n        scores = scores * squared_mask  # Size: n_batches x padded_document_length x padded_document_length x n_classes\n        scores_flat = scores.view(\n            flat_size[0], flat_size[1], flat_size[2] * self._n_classes\n        )  # Size: n_batches x padded_document_length x (padded_document_length x n_classes)\n        labels = labels * squared_mask  # Size: n_batches x padded_document_length x padded_document_length x n_classes\n        labels_flat = labels.view(\n            flat_size[0], flat_size[1], flat_size[2] * self._n_classes\n        )  # Size: n_batches x padded_document_length x (padded_document_length x n_classes)\n\n        loss = self._loss_fn(scores_flat, labels_flat)\n\n        # Amplify the loss to actually see something...\n        return 100 * loss\n\n    @overrides\n    def forward(self, text: Dict[str, torch.LongTensor], relations: torch.IntTensor = None) -> Dict[str, torch.Tensor]:\n        # pylint: disable=arguments-differ\n        """"""\n\t\tForward pass of the model.\n\t\tCompute the predictions and the loss (if labels are available).\n\t\t\n\t\tParameters:\n\t\t----------\n\t\ttext: Dict[str, torch.LongTensor]\n\t\t\tThe input sentences which have transformed into indexes (integers) according to a mapping token:str -> token:int\n\t\trelations: torch.IntTensor\n\t\t\tThe gold relations to predict.\n\t\t""""""\n\n        # Text field embedder map the token:int to their word embedding representation token:embedding (whatever these embeddings are).\n        text_embeddings = self._text_field_embedder(text)\n        # Compute the mask from the text: 1 if there is actually a word in the corresponding sentence, 0 if it has been padded.\n        mask = util.get_text_field_mask(text)  # Size: batch_size x padded_document_length\n\n        # Compute the contextualized representation from the word embeddings.\n        # Usually, _context_layer is a Seq2seq model such as LSTM\n        encoded_text = self._context_layer(\n            text_embeddings, mask\n        )  # Size: batch_size x padded_document_length x lstm_output_size\n\n        ###### Relation Scorer ##############\n        # Compute the relation scores\n        left = torch.matmul(encoded_text, self._U)  # Size: batch_size x padded_document_length x l\n        right = torch.matmul(encoded_text, self._W)  # Size: batch_size x padded_document_length x l\n\n        left = left.permute(1, 0, 2)\n        left = left.unsqueeze(3)\n        right = right.permute(0, 2, 1)\n        right = right.unsqueeze(0)\n\n        B = left + right\n        B = B.permute(1, 0, 3, 2)  # Size: batch_size x padded_document_length x padded_document_length x l\n\n        outer_sum_bias = B + self._b  # Size: batch_size x padded_document_length x padded_document_length x l\n        if self._activation == ""relu"":\n            activated_outer_sum_bias = F.relu(outer_sum_bias)\n        elif self._activation == ""tanh"":\n            activated_outer_sum_bias = F.tanh(outer_sum_bias)\n\n        relation_scores = torch.matmul(\n            activated_outer_sum_bias, self._V\n        )  # Size: batch_size x padded_document_length x padded_document_length x n_classes\n        #################################################################\n\n        batch_size, padded_document_length = mask.size()\n\n        relation_sigmoid_scores = torch.sigmoid(\n            relation_scores\n        )  # F.sigmoid(relation_scores) #Size: batch_size x padded_document_length x padded_document_length x n_classes\n\n        # predicted_relations[l, i, j, k] == 1 iif we predict a relation k with ARG1==i, ARG2==j in the l-th sentence of the batch\n        predicted_relations = torch.round(\n            relation_sigmoid_scores\n        )  # Size: batch_size x padded_document_length x padded_document_length x n_classes\n\n        output_dict = {\n            ""relation_sigmoid_scores"": relation_sigmoid_scores,\n            ""predicted_relations"": predicted_relations,\n            ""mask"": mask,\n        }\n\n        if relations is not None:\n            # Reformat the gold relations before computing the loss\n            # Size: batch_size x padded_document_length x padded_document_length x n_classes\n            # gold_relations[l, i, j, k] == 1 iif we predict a relation k with ARG1==i, ARG2==j in the l-th sentence of the batch\n            gold_relations = torch.zeros(batch_size, padded_document_length, padded_document_length, self._n_classes)\n\n            for exple_idx, exple_tags in enumerate(relations):  # going through the batch\n                # rel is a list of list containing the current sentence in the batch\n                # each sublist in rel is of size padded_document_length\n                # and encodes a relation in the sentence where the two non zeros elements\n                # indicate the two words arguments AND the relation type between these two words.\n                for rel in exple_tags:\n                    # relations have been padded, so for each sentence in the batch there are\n                    # max_nb_of_relations_in_batch_for_one_sentence relations ie (number of sublist such as rel)\n                    # The padded relations are simply list of size padded_document_length filled with 0.\n                    if rel.sum().item() == 0:\n                        continue\n\n                    for idx in rel.nonzero():\n                        label_srt = self.vocab.get_token_from_index(rel[idx].item(), self._label_namespace)\n                        arg, rel_type = label_srt.split(""_"")\n                        if arg == ""ARG1"":\n                            x = idx.data[0]\n                        else:\n                            y = idx.data[0]\n\n                    gold_relations[exple_idx, x, y, rel_type_2_idx[rel_type]] = 1\n\n                    # GPU support\n            if text_embeddings.is_cuda:\n                gold_relations = gold_relations.cuda()\n\n            # Compute the loss\n            output_dict[""loss""] = self.multi_class_cross_entropy_loss(\n                scores=relation_scores, labels=gold_relations, mask=mask\n            )\n\n            # Compute the metrics with the predictions.\n            self._relation_metric(predictions=predicted_relations, gold_labels=gold_relations, mask=mask)\n\n        return output_dict\n\n    @overrides\n    def decode(self, output_dict: Dict[str, torch.Tensor]) -> Dict[str, Any]:\n        """"""\n\t\tDecode the predictions\n\t\t""""""\n        decoded_predictions = []\n\n        for instance_tags in output_dict[""predicted_relations""]:\n            sentence_length = instance_tags.size(0)\n            decoded_relations = []\n\n            for arg1, arg2, rel_type_idx in instance_tags.nonzero().data:\n                relation = [""*""] * sentence_length\n                rel_type = idx_2_rel_type[rel_type_idx.item()]\n                relation[arg1] = ""ARG1_"" + rel_type\n                relation[arg2] = ""ARG2_"" + rel_type\n                decoded_relations.append(relation)\n\n            decoded_predictions.append(decoded_relations)\n\n        output_dict[""decoded_predictions""] = decoded_predictions\n\n        return output_dict\n\n    @overrides\n    def get_metrics(self, reset: bool = False) -> Dict[str, float]:\n        """"""\n\t\tCompute the metrics for relation: precision, recall and f1.\n\t\tA relation is considered correct if we can correctly predict the last word of ARG1, the last word of ARG2 and the relation type.\n\t\t""""""\n        metric_dict = self._relation_metric.get_metric(reset=reset)\n        return {x: y for x, y in metric_dict.items() if ""overall"" in x}\n'"
hmtl/modules/__init__.py,0,b'# coding: utf-8\n\nfrom hmtl.modules import seq2seq_encoders\nfrom hmtl.modules import text_field_embedders\n'
hmtl/tasks/__init__.py,0,b'# coding: utf-8\n\nfrom hmtl.tasks.task import Task\n'
hmtl/tasks/task.py,0,"b'# coding: utf-8\n\nfrom typing import List\nfrom allennlp.common import Params\nfrom allennlp.commands.train import datasets_from_params\nfrom allennlp.data.iterators import DataIterator\nfrom allennlp.common.checks import ConfigurationError\n\n\nclass Task:\n    """"""\n    A class to encapsulate the necessary informations (and datasets)\n    about each task.\n    \n    Parameters\n    ----------\n    name : ``str``, required\n        The name of the task.\n    validation_metric_name : ``str``, required\n        The name of the validation metric to use to monitor training\n        and select the best epoch.\n    validation_metric_decreases : ``bool``, required\n        Whether or not the validation metric should decrease for improvement.\n    evaluate_on_test : ``bool`, optional (default = False)\n        Whether or not the task should be evaluated on the test set at the end of the training.\n    """"""\n\n    def __init__(\n        self, name: str, validation_metric_name: str, validation_metric_decreases: bool, evaluate_on_test: bool = False\n    ) -> None:\n        self._name = name\n\n        self._train_data = None\n        self._validation_data = None\n        self._test_data = None\n        self._evaluate_on_test = evaluate_on_test\n\n        self._val_metric = validation_metric_name\n        self._val_metric_decreases = validation_metric_decreases\n\n        self._data_iterator = None\n\n    def set_data_iterator(self, data_iterator: DataIterator):\n        if data_iterator is not None:\n            self._data_iterator = data_iterator\n        else:\n            ConfigurationError(f""data_iterator cannot be None in set_iterator - Task name: {self._name}"")\n\n    def load_data_from_params(self, params: Params):\n        all_datasets = datasets_from_params(params)\n        datasets_for_vocab_creation = set(params.pop(""datasets_for_vocab_creation"", all_datasets))\n\n        for dataset in datasets_for_vocab_creation:\n            if dataset not in all_datasets:\n                raise ConfigurationError(f""invalid \'dataset_for_vocab_creation\' {dataset}"")\n\n        instances_for_vocab_creation = (\n            instance\n            for key, dataset in all_datasets.items()\n            for instance in dataset\n            if key in datasets_for_vocab_creation\n        )\n\n        self._instances_for_vocab_creation = instances_for_vocab_creation\n        self._datasets_for_vocab_creation = datasets_for_vocab_creation\n\n        if ""train"" in all_datasets.keys():\n            self._train_data = all_datasets[""train""]\n            self._tr_instances = sum(1 for e in self._train_data)  # This is horrible if lazy iterator (Iterable)\n        if ""validation"" in all_datasets.keys():\n            self._validation_data = all_datasets[""validation""]\n            self._val_instances = sum(1 for e in self._validation_data)  # This is horrible if lazy iterator (Iterable)\n        if ""test"" in all_datasets.keys():\n            self._test_data = all_datasets[""test""]\n            self._test_instances = sum(1 for e in self._test_data)  # This is horrible if lazy iterator (Iterable)\n\n        # If trying to evaluate on test set, make sure the dataset is loaded\n        if self._evaluate_on_test:\n            assert self._test_data is not None\n\n        # return instances_for_vocab_creation, datasets_for_vocab_creation, all_datasets\n        return instances_for_vocab_creation, datasets_for_vocab_creation\n\n    @classmethod\n    def from_params(cls, params: Params) -> ""Task"":\n        task_name = params.pop(""task_name"", ""ner"")\n        validation_metric_name = params.pop(""validation_metric_name"", ""f1-measure-overall"")\n        validation_metric_decreases = params.pop_bool(""validation_metric_decreases"", False)\n        evaluate_on_test = params.pop_bool(""evaluate_on_test"", False)\n\n        params.assert_empty(cls.__name__)\n        return cls(\n            name=task_name,\n            validation_metric_name=validation_metric_name,\n            validation_metric_decreases=validation_metric_decreases,\n            evaluate_on_test=evaluate_on_test,\n        )\n'"
hmtl/training/__init__.py,0,b'# coding: utf-8\n\nfrom hmtl.training.sampler_multi_task_trainer import SamplerMultiTaskTrainer\n'
hmtl/training/multi_task_trainer.py,7,"b'# coding: utf-8\n\nimport os\nimport math\nimport time\nfrom copy import deepcopy\nimport random\nimport logging\nimport itertools\nimport shutil\nfrom tensorboardX import SummaryWriter\n\nfrom typing import List, Optional, Dict, Any, Tuple\n\nimport torch\nimport torch.optim.lr_scheduler\nimport tqdm\n\nfrom allennlp.common import Params\nfrom allennlp.common.checks import ConfigurationError, check_for_gpu\nfrom allennlp.common.util import peak_memory_mb, gpu_memory_mb\nfrom allennlp.nn.util import device_mapping, move_to_device\nfrom allennlp.training.learning_rate_schedulers import LearningRateScheduler\nfrom allennlp.training.optimizers import Optimizer\nfrom allennlp.training.trainer import sparse_clip_norm, TensorboardWriter\nfrom allennlp.models.model import Model\nfrom allennlp.common.registrable import Registrable\n\n\nfrom hmtl.tasks import Task\n\nlogger = logging.getLogger(__name__)  # pylint: disable=invalid-name\n\n\nclass MultiTaskTrainer(Registrable):\n    def __init__(\n        self,\n        model: Model,\n        task_list: List[Task],\n        optimizer_params: Params,\n        lr_scheduler_params: Params,\n        patience: Optional[int] = None,\n        num_epochs: int = 20,\n        serialization_dir: str = None,\n        cuda_device: int = -1,\n        grad_norm: Optional[float] = None,\n        grad_clipping: Optional[float] = None,\n        min_lr: float = 0.00001,\n        no_tqdm: bool = False,\n        summary_interval: int = 50,\n        log_parameter_statistics: bool = False,\n        log_gradient_statistics: bool = False,\n    ):\n        """""" \n        Parameters\n        ----------\n        model: ``Model``, required.\n            An AllenNLP model to be optimized. Pytorch Modules can also be optimized if\n            their ``forward`` method returns a dictionary with a ""loss"" key, containing a\n            scalar tensor representing the loss function to be optimized.\n        iterator: ``DataIterator``, required.\n            A method for iterating over a ``Dataset``, yielding padded indexed batches.\n        patience: Optional[int] > 0, optional (default=None)\n            Number of epochs to be patient before early stopping: the training is stopped\n            after ``patience`` epochs with no improvement. If given, it must be ``> 0``.\n            If None, early stopping is disabled.\n        num_epochs: int, optional (default = 20)\n            Number of training epochs.\n        serialization_dir: str, optional (default=None)\n            Path to directory for saving and loading model files. Models will not be saved if\n            this parameter is not passed.\n        cuda_device: int, optional (default = -1)\n            An integer specifying the CUDA device to use. If -1, the CPU is used.\n            Multi-gpu training is not currently supported, but will be once the\n            Pytorch DataParallel API stabilises.\n        grad_norm: float, optional, (default = None).\n            If provided, gradient norms will be rescaled to have a maximum of this value.\n        grad_clipping : float, optional (default = None).\n            If provided, gradients will be clipped `during the backward pass` to have an (absolute)\n            maximum of this value.  If you are getting ``NaNs`` in your gradients during training\n            that are not solved by using ``grad_norm``, you may need this.\n        no_tqdm : bool, optional (default=False)\n            We use ``tqdm`` for logging, which will print a nice progress bar that updates in place\n            after every batch.  This is nice if you\'re running training on a local shell, but can\n            cause problems with log files from, e.g., a docker image running on kubernetes.  If\n            ``no_tqdm`` is ``True``, we will not use tqdm, and instead log batch statistics using\n            ``logger.info``.\n        """"""\n        self._model = model\n        parameters_to_train = [(n, p) for n, p in self._model.named_parameters() if p.requires_grad]\n\n        self._task_list = task_list\n        self._n_tasks = len(self._task_list)\n\n        self._optimizer_params = optimizer_params\n        self._optimizers = {}\n        self._lr_scheduler_params = lr_scheduler_params\n        self._schedulers = {}\n        for task in self._task_list:\n            task_name = task._name\n            self._optimizers[task_name] = Optimizer.from_params(\n                model_parameters=parameters_to_train, params=deepcopy(optimizer_params)\n            )\n            self._schedulers[task_name] = LearningRateScheduler.from_params(\n                optimizer=self._optimizers[task_name], params=deepcopy(lr_scheduler_params)\n            )\n\n        self._serialization_dir = serialization_dir\n\n        self._patience = patience\n        self._num_epochs = num_epochs\n        self._cuda_device = cuda_device\n        if self._cuda_device >= 0:\n            check_for_gpu(self._cuda_device)\n            self._model = self._model.cuda(self._cuda_device)\n        self._grad_norm = grad_norm\n        self._grad_clipping = grad_clipping\n        self._min_lr = min_lr\n\n        self._task_infos = None\n        self._metric_infos = None\n\n        self._tr_generators = None\n        self._no_tqdm = no_tqdm\n\n        self._summary_interval = summary_interval  # num batches between logging to tensorboard\n        self._log_parameter_statistics = log_parameter_statistics\n        self._log_gradient_statistics = log_gradient_statistics\n        self._global_step = 0\n        train_log = SummaryWriter(os.path.join(self._serialization_dir, ""log"", ""train""))\n        validation_log = SummaryWriter(os.path.join(self._serialization_dir, ""log"", ""validation""))\n        self._tensorboard = TensorboardWriter(train_log=train_log, validation_log=validation_log)\n\n    def train(\n        self,\n        # tasks: List[Task],\n        # params: Params,\n        recover: bool = False,\n    ):\n\n        raise NotImplementedError\n\n    def _check_history(self, metric_history: List[float], cur_score: float, should_decrease: bool = False):\n        """"""\n        Given a task, the history of the performance on that task,\n        and the current score, check if current score is\n        best so far and if out of patience.\n        \n        Parameters\n        ----------\n        metric_history: List[float], required\n        cur_score: float, required\n        should_decrease: bool, default = False\n            Wheter or not the validation metric should increase while training.\n            For instance, the bigger the f1 score is, the better it is -> should_decrease = False\n            \n        Returns\n        -------\n        best_so_far: bool\n            Whether or not the current epoch is the best so far in terms of the speicified validation metric.\n        out_of_patience: bool\n            Whether or not the training for this specific task should stop (patience parameter).\n        """"""\n        patience = self._patience + 1\n        best_fn = min if should_decrease else max\n        best_score = best_fn(metric_history)\n        if best_score == cur_score:\n            best_so_far = metric_history.index(best_score) == len(metric_history) - 1\n        else:\n            best_so_far = False\n\n        out_of_patience = False\n        if len(metric_history) > patience:\n            if should_decrease:\n                out_of_patience = max(metric_history[-patience:]) <= cur_score\n            else:\n                out_of_patience = min(metric_history[-patience:]) >= cur_score\n\n        if best_so_far and out_of_patience:  # then something is up\n            print(""Something is up"")\n\n        return best_so_far, out_of_patience\n\n    def _forward(self, tensor_batch: torch.Tensor, for_training: bool = False, task: Task = None):\n        if task is not None:\n            tensor_batch = move_to_device(tensor_batch, self._cuda_device)\n            output_dict = self._model.forward(\n                task_name=task._name, tensor_batch=tensor_batch, for_training=for_training\n            )\n            if for_training:\n                try:\n                    loss = output_dict[""loss""]\n                    loss += self._model.get_regularization_penalty()\n                except KeyError:\n                    raise RuntimeError(\n                        ""The model you are trying to optimize does not contain a""\n                        "" `loss` key in the output of model.forward(inputs).""\n                    )\n            return output_dict\n        else:\n            raise ConfigurationError(""Cannot call forward through task `None`"")\n\n    def _get_metrics(self, task: Task, reset: bool = False):\n        task_tagger = getattr(self._model, ""_tagger_"" + task._name)\n        return task_tagger.get_metrics(reset)\n\n    def _description_from_metrics(self, metrics: Dict[str, float]):\n        # pylint: disable=no-self-use\n        return "", "".join([""%s: %.4f"" % (name, value) for name, value in metrics.items()]) + "" ||""\n\n    def _rescale_gradients(self) -> Optional[float]:\n        """"""\n        Performs gradient rescaling. Is a no-op if gradient rescaling is not enabled.\n        """"""\n        if self._grad_norm:\n            parameters_to_clip = [p for p in self._model.parameters() if p.grad is not None]\n            return sparse_clip_norm(parameters_to_clip, self._grad_norm)\n        return None\n\n    def _enable_gradient_clipping(self) -> None:\n        if self._grad_clipping is not None:\n            # Pylint is unable to tell that we\'re in the case that _grad_clipping is not None...\n            # pylint: disable=invalid-unary-operand-type\n            clip_function = lambda grad: grad.clamp(-self._grad_clipping, self._grad_clipping)\n            for parameter in self._model.parameters():\n                if parameter.requires_grad:\n                    parameter.register_hook(clip_function)\n\n    def _save_checkpoint(self, epoch: int, should_stop: bool) -> None:\n        """"""\n        Save the current states (model, training, optimizers, metrics and tasks).\n        \n        Parameters\n        ----------\n        epoch: int, required.\n            The epoch of training.\n        should_stop: bool, required\n            Wheter or not the training is finished.\n        should_save_model: bool, optional (default = True)\n            Whether or not the model state should be saved.\n        """"""\n        ### Saving training state ###\n        training_state = {\n            ""epoch"": epoch,\n            ""should_stop"": should_stop,\n            ""metric_infos"": self._metric_infos,\n            ""task_infos"": self._task_infos,\n            ""schedulers"": {},\n            ""optimizers"": {},\n        }\n\n        if self._optimizers is not None:\n            for task_name, optimizer in self._optimizers.items():\n                training_state[""optimizers""][task_name] = optimizer.state_dict()\n        if self._schedulers is not None:\n            for task_name, scheduler in self._schedulers.items():\n                training_state[""schedulers""][task_name] = scheduler.lr_scheduler.state_dict()\n\n        training_path = os.path.join(self._serialization_dir, ""training_state.th"")\n        torch.save(training_state, training_path)\n        logger.info(""Checkpoint - Saved training state to %s"", training_path)\n\n        ### Saving model state ###\n        model_path = os.path.join(self._serialization_dir, ""model_state.th"")\n        model_state = self._model.state_dict()\n        torch.save(model_state, model_path)\n        logger.info(""Checkpoint - Saved model state to %s"", model_path)\n\n        ### Saving best models for each task ###\n        for task_name, infos in self._metric_infos.items():\n            best_epoch, _ = infos[""best""]\n            if best_epoch == epoch:\n                logger.info(""Checkpoint - Best validation performance so far for %s task"", task_name)\n                logger.info(""Checkpoint - Copying weights to \'%s/best_%s.th\'."", self._serialization_dir, task_name)\n                shutil.copyfile(model_path, os.path.join(self._serialization_dir, ""best_{}.th"".format(task_name)))\n\n    def find_latest_checkpoint(self) -> Tuple[str, str]:\n        """"""\n        Return the location of the latest model and training state files.\n        If there isn\'t a valid checkpoint then return None.\n        """"""\n        have_checkpoint = (\n            self._serialization_dir is not None\n            and any(""model_state"" in x for x in os.listdir(self._serialization_dir))\n            and any(""training_state"" in x for x in os.listdir(self._serialization_dir))\n        )\n\n        if not have_checkpoint:\n            return None\n\n        model_path = os.path.join(self._serialization_dir, ""model_state.th"")\n        training_state_path = os.path.join(self._serialization_dir, ""training_state.th"")\n\n        return (model_path, training_state_path)\n\n    def _restore_checkpoint(self):\n        """"""\n        Restores a model from a serialization_dir to the last saved checkpoint.\n        This includes an epoch count, optimizer state, a model state, a task state and\n        a metric state. All are of which are serialized separately. \n        This function should only be used to continue training -\n        if you wish to load a model for inference/load parts of a model into a new\n        computation graph, you should use the native Pytorch functions:\n        `` model.load_state_dict(torch.load(""/path/to/model/weights.th""))``\n\n        Returns\n        -------\n        epoch: int, \n            The epoch at which to resume training.\n        should_stop: bool\n            Whether or not the training should already by stopped.\n        """"""\n\n        latest_checkpoint = self.find_latest_checkpoint()\n\n        if not self._serialization_dir:\n            raise ConfigurationError(\n                ""`serialization_dir` not specified - cannot "" ""restore a model without a directory path.""\n            )\n        if latest_checkpoint is None:\n            raise ConfigurationError(\n                ""Cannot restore model because one of""\n                ""`model_state.th` or `training_state.th` is not in directory path.""\n            )\n\n        model_path, training_state_path = latest_checkpoint\n\n        # Load the parameters onto CPU, then transfer to GPU.\n        # This avoids potential OOM on GPU for large models that\n        # load parameters onto GPU then make a new GPU copy into the parameter\n        # buffer. The GPU transfer happens implicitly in load_state_dict.\n        model_state = torch.load(model_path, map_location=device_mapping(-1))\n        training_state = torch.load(training_state_path, map_location=device_mapping(-1))\n\n        # Load model\n        self._model.load_state_dict(model_state)\n        logger.info(""Checkpoint - Model loaded from %s"", model_path)\n\n        # Load optimizers\n        for task_name, optimizers_state in training_state[""optimizers""].items():\n            self._optimizers[task_name].load_state_dict(optimizers_state)\n        logger.info(""Checkpoint - Optimizers loaded from %s"", training_state_path)\n\n        # Load schedulers\n        for task_name, scheduler_state in training_state[""schedulers""].items():\n            self._schedulers[task_name].lr_scheduler.load_state_dict(scheduler_state)\n        logger.info(""Checkpoint - Learning rate schedulers loaded from %s"", training_state_path)\n\n        self._metric_infos = training_state[""metric_infos""]\n        self._task_infos = training_state[""task_infos""]\n        logger.info(""Checkpoint - Task infos loaded from %s"", training_state_path)\n        logger.info(""Checkpoint - Metric infos loaded from %s"", training_state_path)\n\n        n_epoch, should_stop = training_state[""epoch""], training_state[""should_stop""]\n\n        return n_epoch + 1, should_stop\n\n    @classmethod\n    def from_params(\n        cls, model: Model, task_list: List[Task], serialization_dir: str, params: Params\n    ) -> ""MultiTaskTrainer"":\n        """"""\n        Static method that constructs the multi task trainer described by ``params``.\n        """"""\n        choice = params.pop_choice(""type"", cls.list_available())\n        return cls.by_name(choice).from_params(\n            model=model, task_list=task_list, serialization_dir=serialization_dir, params=params\n        )\n'"
hmtl/training/sampler_multi_task_trainer.py,1,"b'# coding: utf-8\n\n# A modified version of the trainer showcased in GLUE: https://github.com/nyu-mll/GLUE-baselines\n\nimport os\nimport math\nimport time\nfrom copy import deepcopy\nimport random\nimport logging\nimport itertools\nimport shutil\nfrom tensorboardX import SummaryWriter\nimport numpy as np\n\nfrom typing import List, Optional, Dict, Any\nfrom overrides import overrides\n\nimport torch\nimport torch.optim.lr_scheduler\nimport tqdm\n\nfrom allennlp.common import Params\nfrom allennlp.common.checks import ConfigurationError, check_for_gpu\nfrom allennlp.common.util import peak_memory_mb, gpu_memory_mb\nfrom allennlp.nn.util import device_mapping\nfrom allennlp.data.iterators import DataIterator\nfrom allennlp.training.learning_rate_schedulers import LearningRateScheduler\nfrom allennlp.training.optimizers import Optimizer\nfrom allennlp.training.trainer import sparse_clip_norm, TensorboardWriter\nfrom allennlp.models.model import Model\n\nfrom hmtl.tasks import Task\nfrom hmtl.training.multi_task_trainer import MultiTaskTrainer\n\n\nlogger = logging.getLogger(__name__)  # pylint: disable=invalid-name\n\n\n@MultiTaskTrainer.register(""sampler_multi_task_trainer"")\nclass SamplerMultiTaskTrainer(MultiTaskTrainer):\n    def __init__(\n        self,\n        model: Model,\n        task_list: List[Task],\n        optimizer_params: Params,\n        lr_scheduler_params: Params,\n        patience: Optional[int] = None,\n        num_epochs: int = 20,\n        serialization_dir: str = None,\n        cuda_device: int = -1,\n        grad_norm: Optional[float] = None,\n        grad_clipping: Optional[float] = None,\n        min_lr: float = 0.00001,\n        no_tqdm: bool = False,\n        summary_interval: int = 50,\n        log_parameter_statistics: bool = False,\n        log_gradient_statistics: bool = False,\n        sampling_method: str = ""proportional"",\n    ):\n\n        if sampling_method not in [""uniform"", ""proportional""]:\n            raise ConfigurationError(f""Sampling method ({sampling_method}) must be `uniform` or `proportional`."")\n\n        self._sampling_method = sampling_method\n        super(SamplerMultiTaskTrainer, self).__init__(\n            model=model,\n            task_list=task_list,\n            optimizer_params=optimizer_params,\n            lr_scheduler_params=lr_scheduler_params,\n            patience=patience,\n            num_epochs=num_epochs,\n            serialization_dir=serialization_dir,\n            cuda_device=cuda_device,\n            grad_norm=grad_norm,\n            grad_clipping=grad_clipping,\n            min_lr=min_lr,\n            no_tqdm=no_tqdm,\n            summary_interval=summary_interval,\n            log_parameter_statistics=log_parameter_statistics,\n            log_gradient_statistics=log_gradient_statistics,\n        )\n\n    @overrides\n    def train(self, recover: bool = False):\n        """"""\n        Train the different task_list, save the different checkpoints and metrics,\n        and save the model at the end of training while logging the training details.\n        \n        The metrics through the training are stored in dictionaries with the following structure:\n        \n        all_metrics - Dict[str, str]\n            task_name: val_metric\n\n        metric_infos (Dict[])\n            task_name (Dict[str, diverse]\n                val_metric (str): name (str)\n                hist (str): history_of_the_val_metric (List[float])\n                stopped (str): training_is_stopped (bool)\n                best (str): best_epoch_for_val_metric (Tuple(int, Dict))  \n\n        all_tr_metrics (Dict[str, Dict[str, float]])\n            task_name (Dict[str, float])\n                metric_name (str): value (float)\n                loss: value (float)\t\t\n\n        all_val_metrics (Dict[str, Dict[str, float]])\n            task_name (Dict[str, float])\n                metric_name (str): value (float)\n                loss (str): value (float)\n        \n        Parameters\n        ----------\n        task_list: List[Task], required\n            A list containing the tasks to train.\n        params: Params, required\n            Training parameters\n        recover: bool, required\n            Whether or not training should be recovered from a previous training.\n\n        Returns\n        -------\n        return_dict: Dict\n            A dictionary summarizing the training and the metrics for the best epochs for each task.\n        """"""\n        training_start_time = time.time()\n\n        if recover:\n            try:\n                n_epoch, should_stop = self._restore_checkpoint()\n                logger.info(""Loaded model from checkpoint. Starting at epoch %d"", n_epoch)\n            except RuntimeError:\n                raise ConfigurationError(\n                    ""Could not recover training from the checkpoint.  Did you mean to output to ""\n                    ""a different serialization directory or delete the existing serialization ""\n                    ""directory?""\n                )\n        else:\n            n_epoch, should_stop = 0, False\n\n            ### Store all the necessary informations and attributes about the tasks ###\n            task_infos = {task._name: {} for task in self._task_list}\n            for task_idx, task in enumerate(self._task_list):\n                task_info = task_infos[task._name]\n\n                # Store statistiscs on training and validation batches\n                data_iterator = task._data_iterator\n                n_tr_batches = data_iterator.get_num_batches(task._train_data)\n                n_val_batches = data_iterator.get_num_batches(task._validation_data)\n                task_info[""n_tr_batches""] = n_tr_batches\n                task_info[""n_val_batches""] = n_val_batches\n\n                # Create counter for number of batches trained during the whole\n                # training for this specific task\n                task_info[""total_n_batches_trained""] = 0\n\n                task_info[""last_log""] = time.time()  # Time of last logging\n            self._task_infos = task_infos\n\n            ### Bookkeeping the validation metrics ###\n            metric_infos = {\n                task._name: {\n                    ""val_metric"": task._val_metric,\n                    ""hist"": [],\n                    ""is_out_of_patience"": False,\n                    ""min_lr_hit"": False,\n                    ""best"": (-1, {}),\n                }\n                for task in self._task_list\n            }\n            self._metric_infos = metric_infos\n\n        ### Write log ###\n        total_n_tr_batches = 0  # The total number of training batches across all the datasets.\n        for task_name, info in self._task_infos.items():\n            total_n_tr_batches += info[""n_tr_batches""]\n            logger.info(""Task %s:"", task_name)\n            logger.info(""\\t%d training batches"", info[""n_tr_batches""])\n            logger.info(""\\t%d validation batches"", info[""n_val_batches""])\n\n        ### Create the training generators/iterators tqdm ###\n        self._tr_generators = {}\n        for task in self._task_list:\n            data_iterator = task._data_iterator\n            tr_generator = data_iterator(task._train_data, num_epochs=None)\n            self._tr_generators[task._name] = tr_generator\n\n        ### Create sampling probability distribution ###\n        if self._sampling_method == ""uniform"":\n            sampling_prob = [float(1 / self._n_tasks)] * self._n_tasks\n        elif self._sampling_method == ""proportional"":\n            sampling_prob = [float(info[""n_tr_batches""] / total_n_tr_batches) for info in self._task_infos.values()]\n\n        ### Enable gradient clipping ###\n        # Only if self._grad_clipping is specified\n        self._enable_gradient_clipping()\n\n        ### Setup is ready. Training of the model can begin ###\n        logger.info(""Set up ready. Beginning training/validation."")\n\n        ### Begin Training of the model ###\n        while not should_stop:\n            # Train one epoch (training pass + validation pass)\n\n            self._model.train()  # Set the model to ""train"" mode.\n\n            ### Log Infos: current epoch count and CPU/GPU usage ###\n            logger.info("""")\n            logger.info(""Epoch %d/%d - Begin"", n_epoch, self._num_epochs - 1)\n            logger.info(f""Peak CPU memory usage MB: {peak_memory_mb()}"")\n            for gpu, memory in gpu_memory_mb().items():\n                logger.info(f""GPU {gpu} memory usage MB: {memory}"")\n\n            logger.info(""Training - Begin"")\n\n            ### Reset training and trained batches counter before new training epoch ###\n            for _, task_info in self._task_infos.items():\n                task_info[""tr_loss_cum""] = 0.0\n                task_info[""n_batches_trained_this_epoch""] = 0\n            all_tr_metrics = {}  # BUG TO COMPLETE COMMENT TO MAKE IT MORE CLEAR\n\n            ### Start training epoch ###\n            epoch_tqdm = tqdm.tqdm(range(total_n_tr_batches), total=total_n_tr_batches)\n            for _ in epoch_tqdm:\n                task_idx = np.argmax(np.random.multinomial(1, sampling_prob))\n                task = self._task_list[task_idx]\n                task_info = self._task_infos[task._name]\n\n                ### One forward + backward pass ###\n\n                # Call next batch to train\n                batch = next(self._tr_generators[task._name])\n                task_info[""n_batches_trained_this_epoch""] += 1\n\n                # Load optimizer\n                optimizer = self._optimizers[task._name]\n                optimizer.zero_grad()\n\n                # Get the loss for this batch\n                output_dict = self._forward(tensor_batch=batch, task=task, for_training=True)\n                assert ""loss"" in output_dict, ""Model must return a dict containing a \'loss\' key""\n                loss = output_dict[""loss""]\n                loss.backward()\n                task_info[""tr_loss_cum""] += loss.item()\n\n                # Gradient rescaling if self._grad_norm is specified\n                self._rescale_gradients()\n\n                # Take an optimization step\n                optimizer.step()\n\n                ### Get metrics for all progress so far, update tqdm, display description ###\n                task_metrics = self._get_metrics(task=task)\n                task_metrics[""loss""] = float(\n                    task_info[""tr_loss_cum""] / (task_info[""n_batches_trained_this_epoch""] + 0.000_001)\n                )\n                description = self._description_from_metrics(task_metrics)\n                epoch_tqdm.set_description(task._name + "", "" + description)\n\n                ### Tensorboard logging: Training detailled metrics, parameters and gradients ###\n                if self._global_step % self._summary_interval == 0:\n                    # Metrics\n                    for metric_name, value in task_metrics.items():\n                        self._tensorboard.add_train_scalar(\n                            name=""training_details/"" + task._name + ""/"" + metric_name,\n                            value=value,\n                            global_step=self._global_step,\n                        )\n                    # Parameters and Gradients\n                    for param_name, param in self._model.named_parameters():\n                        if self._log_parameter_statistics:\n                            self._tensorboard.add_train_scalar(\n                                name=""parameter_mean/"" + param_name,\n                                value=param.data.mean(),\n                                global_step=self._global_step,\n                            )\n                            self._tensorboard.add_train_scalar(\n                                name=""parameter_std/"" + param_name,\n                                value=param.data.std(),\n                                global_step=self._global_step,\n                            )\n                        if param.grad is None:\n                            continue\n                        if self._log_gradient_statistics:\n                            self._tensorboard.add_train_scalar(\n                                name=""grad_mean/"" + param_name,\n                                value=param.grad.data.mean(),\n                                global_step=self._global_step,\n                            )\n                            self._tensorboard.add_train_scalar(\n                                name=""grad_std/"" + param_name,\n                                value=param.grad.data.std(),\n                                global_step=self._global_step,\n                            )\n                self._global_step += 1\n\n            ### Bookkeeping all the training metrics for all the tasks on the training epoch that just finished ###\n            for task in self._task_list:\n                task_info = self._task_infos[task._name]\n\n                task_info[""total_n_batches_trained""] += task_info[""n_batches_trained_this_epoch""]\n                task_info[""last_log""] = time.time()\n\n                task_metrics = self._get_metrics(task=task, reset=True)\n                if task._name not in all_tr_metrics:\n                    all_tr_metrics[task._name] = {}\n                for name, value in task_metrics.items():\n                    all_tr_metrics[task._name][name] = value\n                all_tr_metrics[task._name][""loss""] = float(\n                    task_info[""tr_loss_cum""] / (task_info[""n_batches_trained_this_epoch""] + 0.000_000_01)\n                )\n\n                # Tensorboard - Training metrics for this epoch\n                self._tensorboard.add_train_scalar(\n                    name=""training_proportions/"" + task._name,\n                    value=task_info[""n_batches_trained_this_epoch""],\n                    global_step=n_epoch,\n                )\n                for metric_name, value in all_tr_metrics[task._name].items():\n                    self._tensorboard.add_train_scalar(\n                        name=""task_"" + task._name + ""/"" + metric_name, value=value, global_step=n_epoch\n                    )\n\n            logger.info(""Train - End"")\n\n            ### Begin validation of the model ###\n            logger.info(""Validation - Begin"")\n            all_val_metrics = {}\n\n            self._model.eval()  # Set the model into evaluation mode\n\n            for task_idx, task in enumerate(self._task_list):\n                logger.info(""Validation - Task %d/%d: %s"", task_idx + 1, self._n_tasks, task._name)\n\n                val_loss = 0.0\n                n_batches_val_this_epoch_this_task = 0\n                n_val_batches = self._task_infos[task._name][""n_val_batches""]\n                scheduler = self._schedulers[task._name]\n\n                # Create tqdm generator for current task\'s validation\n                data_iterator = task._data_iterator\n                val_generator = data_iterator(task._validation_data, num_epochs=1, shuffle=False)\n                val_generator_tqdm = tqdm.tqdm(val_generator, total=n_val_batches)\n\n                # Iterate over each validation batch for this task\n                for batch in val_generator_tqdm:\n                    n_batches_val_this_epoch_this_task += 1\n\n                    # Get the loss\n                    val_output_dict = self._forward(batch, task=task, for_training=False)\n                    loss = val_output_dict[""loss""]\n                    val_loss += loss.item()\n\n                    # Get metrics for all progress so far, update tqdm, display description\n                    task_metrics = self._get_metrics(task=task)\n                    task_metrics[""loss""] = float(val_loss / n_batches_val_this_epoch_this_task)\n                    description = self._description_from_metrics(task_metrics)\n                    val_generator_tqdm.set_description(description)\n\n                # Get task validation metrics and store them in all_val_metrics\n                task_metrics = self._get_metrics(task=task, reset=True)\n                if task._name not in all_val_metrics:\n                    all_val_metrics[task._name] = {}\n                for name, value in task_metrics.items():\n                    all_val_metrics[task._name][name] = value\n                all_val_metrics[task._name][""loss""] = float(val_loss / n_batches_val_this_epoch_this_task)\n\n                # Tensorboard - Validation metrics for this epoch\n                for metric_name, value in all_val_metrics[task._name].items():\n                    self._tensorboard.add_validation_scalar(\n                        name=""task_"" + task._name + ""/"" + metric_name, value=value, global_step=n_epoch\n                    )\n\n                ### Perform a patience check and update the history of validation metric for this task ###\n                this_epoch_val_metric = all_val_metrics[task._name][task._val_metric]\n                metric_history = self._metric_infos[task._name][""hist""]\n\n                metric_history.append(this_epoch_val_metric)\n                is_best_so_far, out_of_patience = self._check_history(\n                    metric_history=metric_history,\n                    cur_score=this_epoch_val_metric,\n                    should_decrease=task._val_metric_decreases,\n                )\n\n                if is_best_so_far:\n                    logger.info(""Best model found for %s."", task._name)\n                    self._metric_infos[task._name][""best""] = (n_epoch, all_val_metrics)\n                if out_of_patience and not self._metric_infos[task._name][""is_out_of_patience""]:\n                    self._metric_infos[task._name][""is_out_of_patience""] = True\n                    logger.info(""Task %s is out of patience and vote to stop the training."", task._name)\n\n                # The LRScheduler API is agnostic to whether your schedule requires a validation metric -\n                # if it doesn\'t, the validation metric passed here is ignored.\n                scheduler.step(this_epoch_val_metric, n_epoch)\n\n            logger.info(""Validation - End"")\n\n            ### Print all training and validation metrics for this epoch ###\n            logger.info(""***** Epoch %d/%d Statistics *****"", n_epoch, self._num_epochs - 1)\n            for task in self._task_list:\n                logger.info(""Statistic: %s"", task._name)\n                logger.info(\n                    ""\\tTraining - %s: %3d"",\n                    ""Nb batches trained"",\n                    self._task_infos[task._name][""n_batches_trained_this_epoch""],\n                )\n                for metric_name, value in all_tr_metrics[task._name].items():\n                    logger.info(""\\tTraining - %s: %3f"", metric_name, value)\n                for metric_name, value in all_val_metrics[task._name].items():\n                    logger.info(""\\tValidation - %s: %3f"", metric_name, value)\n            logger.info(""**********"")\n\n            ### Check to see if should stop ###\n            stop_tr, stop_val = True, True\n\n            for task in self._task_list:\n                # task_info = self._task_infos[task._name]\n                if self._optimizers[task._name].param_groups[0][""lr""] < self._min_lr:\n                    logger.info(""Minimum lr hit on %s."", task._name)\n                    logger.info(""Task %s vote to stop training."", task._name)\n                    metric_infos[task._name][""min_lr_hit""] = True\n                stop_tr = stop_tr and self._metric_infos[task._name][""min_lr_hit""]\n                stop_val = stop_val and self._metric_infos[task._name][""is_out_of_patience""]\n\n            if stop_tr:\n                should_stop = True\n                logging.info(""All tasks hit minimum lr. Stopping training."")\n            if stop_val:\n                should_stop = True\n                logging.info(""All metrics ran out of patience. Stopping training."")\n            if n_epoch >= self._num_epochs - 1:\n                should_stop = True\n                logging.info(""Maximum number of epoch hit. Stopping training."")\n\n            self._save_checkpoint(n_epoch, should_stop)\n\n            ### Update n_epoch ###\n            # One epoch = doing N (forward + backward) pass where N is the total number of training batches.\n            n_epoch += 1\n\n        ### Summarize training at the end ###\n        logging.info(""***** Training is finished *****"")\n        logging.info(""Stopped training after %d epochs"", n_epoch)\n        return_metrics = {}\n        for task_name, task_info in self._task_infos.items():\n            nb_epoch_trained = int(task_info[""total_n_batches_trained""] / task_info[""n_tr_batches""])\n            logging.info(\n                ""Trained %s for %d batches ~= %d epochs"",\n                task_name,\n                task_info[""total_n_batches_trained""],\n                nb_epoch_trained,\n            )\n            return_metrics[task_name] = {\n                ""best_epoch"": self._metric_infos[task_name][""best""][0],\n                ""nb_epoch_trained"": nb_epoch_trained,\n                ""best_epoch_val_metrics"": self._metric_infos[task_name][""best""][1],\n            }\n\n        training_elapsed_time = time.time() - training_start_time\n        return_metrics[""training_duration""] = time.strftime(""%d:%H:%M:%S"", time.gmtime(training_elapsed_time))\n        return_metrics[""nb_epoch_trained""] = n_epoch\n\n        return return_metrics\n\n    @classmethod\n    def from_params(\n        cls, model: Model, task_list: List[Task], serialization_dir: str, params: Params\n    ) -> ""SamplerMultiTaskTrainer"":\n        """""" Generator multi-task trainer from parameters.  """"""\n\n        optimizer_params = params.pop(""optimizer"")\n        lr_scheduler_params = params.pop(""scheduler"")\n        patience = params.pop_int(""patience"", 2)\n        num_epochs = params.pop_int(""num_epochs"", 20)\n        cuda_device = params.pop_int(""cuda_device"", -1)\n        grad_norm = params.pop_float(""grad_norm"", None)\n        grad_clipping = params.pop_float(""grad_clipping"", None)\n        min_lr = params.pop_float(""min_lr"", 0.00001)\n        no_tqdm = params.pop_bool(""no_tqdm"", False)\n        summary_interval = params.pop(""sumarry_interval"", 50)\n        log_parameter_statistics = params.pop(""log_parameter_statistics"", False)\n        log_gradient_statistics = params.pop(""log_gradient_statistics"", False)\n        sampling_method = params.pop(""sampling_method"", ""proportional"")\n\n        params.assert_empty(cls.__name__)\n        return SamplerMultiTaskTrainer(\n            model=model,\n            task_list=task_list,\n            optimizer_params=optimizer_params,\n            lr_scheduler_params=lr_scheduler_params,\n            patience=patience,\n            num_epochs=num_epochs,\n            serialization_dir=serialization_dir,\n            cuda_device=cuda_device,\n            grad_norm=grad_norm,\n            grad_clipping=grad_clipping,\n            min_lr=min_lr,\n            no_tqdm=no_tqdm,\n            summary_interval=summary_interval,\n            log_parameter_statistics=log_parameter_statistics,\n            log_gradient_statistics=log_gradient_statistics,\n            sampling_method=sampling_method,\n        )\n'"
hmtl/dataset_readers/dataset_utils/__init__.py,0,"b'# coding: utf-8\n\nfrom hmtl.dataset_readers.dataset_utils.ace import ACE, ACESentence\n'"
hmtl/dataset_readers/dataset_utils/ace.py,0,"b'# coding: utf-8\n\nfrom typing import DefaultDict, List, Optional, Iterator, Set, Tuple\nfrom collections import defaultdict\nimport codecs\nimport os\nimport logging\n\nfrom allennlp.data.dataset_readers.dataset_utils import iob1_to_bioul\n\nfrom nltk import Tree\n\nlogger = logging.getLogger(__name__)  # pylint: disable=invalid-name\n\nTypedSpan = Tuple[int, Tuple[int, int]]  # pylint: disable=invalid-name\nTypedStringSpan = Tuple[str, Tuple[int, int]]  # pylint: disable=invalid-name\n\n\nclass ACESentence:\n    """"""\n    A class representing the annotations available for a single ACE CONLL-formatted sentence.\n\n    Parameters\n    ----------\n    words : ``List[str]``\n        This is the tokens as segmented/tokenized with spayc.\n    mention_tags : ``List[str]``\n        The BIO tags for Entity Mention Detection in the sentence.\n    relations : ``List[Tuple[str, List[str]]]``\n        The relations tags for Relation Extraction in the sentence.\n    last_head_token_relations : ``List[Tuple[str, List[str]]]``\n        The relations tags between last tokens for ARG1 and ARG2 for Relation Extraction in the sentence.\n    coref_spans : ``Set[TypedSpan]``\n        The spans for entity mentions involved in coreference resolution within the sentence.\n        Each element is a tuple composed of (cluster_id, (start_index, end_index)). Indices\n        are `inclusive`.\n    """"""\n\n    def __init__(\n        self,\n        words: List[str],\n        mention_tags: List[str],\n        relations: List[Tuple[str, List[str]]],\n        last_head_token_relations: List[Tuple[str, List[str]]],\n        coref_spans: Set[TypedSpan],\n    ):\n        self.words = words\n        self.mention_tags = mention_tags\n        self.relations = relations\n        self.last_head_token_relations = last_head_token_relations\n        self.coref_spans = coref_spans\n\n\nclass ACE:\n    """"""\n    This DatasetReader is designed to read in the ACE (2005 or 2004) which\n    have been previously formatted in the format used by the CoNLL format\n    (see for instance OntoNotes dataset).\n    """"""\n\n    def dataset_iterator(self, file_path: str) -> Iterator[ACESentence]:\n        """"""\n        An iterator over the entire dataset, yielding all sentences processed.\n        """"""\n        for conll_file in self.dataset_path_iterator(file_path):\n            yield from self.sentence_iterator(conll_file)\n\n    @staticmethod\n    def dataset_path_iterator(file_path: str) -> Iterator[str]:\n        """"""\n        An iterator returning file_paths in a directory\n        containing CONLL-formatted files.\n        """"""\n        logger.info(""Reading ACE CONLL-like sentences from dataset files at: %s"", file_path)\n        for root, _, files in list(os.walk(file_path)):\n            for data_file in files:\n                if not data_file.endswith(""like_conll""):\n                    continue\n\n                yield os.path.join(root, data_file)\n\n    def dataset_document_iterator(self, file_path: str) -> Iterator[List[ACESentence]]:\n        """"""\n        An iterator over CONLL-formatted files which yields documents, regardless\n        of the number of document annotations in a particular file.\n        """"""\n        with codecs.open(file_path, ""r"", encoding=""utf8"") as open_file:\n            conll_rows = []\n            document: List[ACESentence] = []\n            for line in open_file:\n                line = line.strip()\n                if line != """" and not line.startswith(""#""):\n                    # Non-empty line. Collect the annotation.\n                    conll_rows.append(line)\n                else:\n                    if conll_rows:\n                        document.append(self._conll_rows_to_sentence(conll_rows))\n                        conll_rows = []\n                if line.startswith(""#end document""):\n                    yield document\n                    document = []\n            if document:\n                # Collect any stragglers or files which might not\n                # have the \'#end document\' format for the end of the file.\n                yield document\n\n    def sentence_iterator(self, file_path: str) -> Iterator[ACESentence]:\n        """"""\n        An iterator over the sentences in an individual CONLL formatted file.\n        """"""\n        for document in self.dataset_document_iterator(file_path):\n            for sentence in document:\n                yield sentence\n\n    def _conll_rows_to_sentence(self, conll_rows: List[str]) -> ACESentence:\n        sentence: List[str] = []\n        mention_tags: List[str] = []\n\n        span_labels: List[List[str]] = []\n        current_span_labels: List[str] = []\n\n        # Cluster id -> List of (start_index, end_index) spans.\n        clusters: DefaultDict[int, List[Tuple[int, int]]] = defaultdict(list)\n        # Cluster id -> List of start_indices which are open for this id.\n        coref_stacks: DefaultDict[int, List[int]] = defaultdict(list)\n\n        for index, row in enumerate(conll_rows):\n            conll_components = row.split()\n\n            word = conll_components[1]\n\n            if not span_labels:\n                span_labels = [[] for _ in conll_components[2:-1]]\n                current_span_labels = [None for _ in conll_components[2:-1]]\n            self._process_span_annotations_for_word(\n                annotations=conll_components[2:-1], span_labels=span_labels, current_span_labels=current_span_labels\n            )\n\n            # Process coref\n            self._process_coref_span_annotations_for_word(conll_components[-1], index, clusters, coref_stacks)\n\n            sentence.append(word)\n\n        mention_tags = iob1_to_bioul(span_labels[0])\n\n        # Process coref clusters\n        coref_span_tuples: Set[TypedSpan] = {\n            (cluster_id, span) for cluster_id, span_list in clusters.items() for span in span_list\n        }\n\n        # Reformat the labels to only keep the the last token of the head\n        # Cf paper, we model relation between last tokens of heads.\n        last_head_token_relations = []\n        bioul_relations = []\n\n        for relation_frame in span_labels[1:]:\n            bioul_relation_frame = iob1_to_bioul(relation_frame)\n\n            reformatted_frame = []\n            for annotation in bioul_relation_frame:\n                if annotation[:2] in [""L-"", ""U-""]:\n                    reformatted_frame.append(annotation[2:])\n                else:\n                    reformatted_frame.append(""*"")\n\n            last_head_token_relations.append(reformatted_frame)\n            bioul_relations.append(bioul_relation_frame)\n\n        return ACESentence(sentence, mention_tags, bioul_relations, last_head_token_relations, coref_span_tuples)\n\n    @staticmethod\n    def _process_mention_tags(annotations: List[str]):\n        """"""\n        Read and pre-process the entity mention tags as a formatted in CoNll-NER-style.\n        """"""\n        labels = []\n        current_span_label = None\n        for annotation in annotations:\n            label = annotation.strip(""()*"")\n            if ""("" in annotation:\n                bio_label = ""B-"" + label\n                current_span_label = label\n            elif current_span_label is not None:\n                bio_label = ""I-"" + current_span_label\n            else:\n                bio_label = ""O""\n            if "")"" in annotation:\n                current_span_label = None\n            labels.append(bio_label)\n        return labels\n\n    @staticmethod\n    def _process_span_annotations_for_word(\n        annotations: List[str], span_labels: List[List[str]], current_span_labels: List[Optional[str]]\n    ) -> None:\n        """"""\n        Given a sequence of different label types for a single word and the current\n        span label we are inside, compute the BIO tag for each label and append to a list.\n\n        Parameters\n        ----------\n        annotations: ``List[str]``\n            A list of labels to compute BIO tags for.\n        span_labels : ``List[List[str]]``\n            A list of lists, one for each annotation, to incrementally collect\n            the BIO tags for a sequence.\n        current_span_labels : ``List[Optional[str]]``\n            The currently open span per annotation type, or ``None`` if there is no open span.\n        """"""\n        for annotation_index, annotation in enumerate(annotations):\n            # strip all bracketing information to\n            # get the actual propbank label.\n            label = annotation.strip(""()*"")\n\n            if ""("" in annotation:\n                # Entering into a span for a particular semantic role label.\n                # We append the label and set the current span for this annotation.\n                bio_label = ""B-"" + label\n                span_labels[annotation_index].append(bio_label)\n                current_span_labels[annotation_index] = label\n            elif current_span_labels[annotation_index] is not None:\n                # If there\'s no \'(\' token, but the current_span_label is not None,\n                # then we are inside a span.\n                bio_label = ""I-"" + current_span_labels[annotation_index]\n                span_labels[annotation_index].append(bio_label)\n            else:\n                # We\'re outside a span.\n                span_labels[annotation_index].append(""O"")\n            # Exiting a span, so we reset the current span label for this annotation.\n            if "")"" in annotation:\n                current_span_labels[annotation_index] = None\n\n    @staticmethod\n    def _process_coref_span_annotations_for_word(\n        label: str,\n        word_index: int,\n        clusters: DefaultDict[int, List[Tuple[int, int]]],\n        coref_stacks: DefaultDict[int, List[int]],\n    ) -> None:\n        """"""\n        For a given coref label, add it to a currently open span(s), complete a span(s) or\n        ignore it, if it is outside of all spans. This method mutates the clusters and coref_stacks\n        dictionaries.\n\n        Parameters\n        ----------\n        label : ``str``\n            The coref label for this word.\n        word_index : ``int``\n            The word index into the sentence.\n        clusters : ``DefaultDict[int, List[Tuple[int, int]]]``\n            A dictionary mapping cluster ids to lists of inclusive spans into the\n            sentence.\n        coref_stacks: ``DefaultDict[int, List[int]]``\n            Stacks for each cluster id to hold the start indices of active spans (spans\n            which we are inside of when processing a given word). Spans with the same id\n            can be nested, which is why we collect these opening spans on a stack, e.g:\n\n            [Greg, the baker who referred to [himself]_ID1 as \'the bread man\']_ID1\n        """"""\n        if label != ""-"":\n            for segment in label.split(""|""):\n                # The conll representation of coref spans allows spans to\n                # overlap. If spans end or begin at the same word, they are\n                # separated by a ""|"".\n                if segment[0] == ""("":\n                    # The span begins at this word.\n                    if segment[-1] == "")"":\n                        # The span begins and ends at this word (single word span).\n                        cluster_id = int(segment[1:-1])\n                        clusters[cluster_id].append((word_index, word_index))\n                    else:\n                        # The span is starting, so we record the index of the word.\n                        cluster_id = int(segment[1:])\n                        coref_stacks[cluster_id].append(word_index)\n                else:\n                    # The span for this id is ending, but didn\'t start at this word.\n                    # Retrieve the start index from the document state and\n                    # add the span to the clusters for this id.\n                    cluster_id = int(segment[:-1])\n                    start = coref_stacks[cluster_id].pop()\n                    clusters[cluster_id].append((start, word_index))\n'"
hmtl/modules/seq2seq_encoders/__init__.py,0,b'# coding: utf-8\n\nfrom hmtl.modules.seq2seq_encoders.stacked_gru import StackedGRU\n'
hmtl/modules/seq2seq_encoders/stacked_gru.py,6,"b'# coding: utf-8\n\nfrom typing import List\n\nfrom overrides import overrides\nimport torch\nfrom torch.nn import Dropout, Linear\nfrom torch.nn import GRU\n\nfrom allennlp.nn.util import last_dim_softmax, weighted_sum\nfrom allennlp.modules.seq2seq_encoders.seq2seq_encoder import Seq2SeqEncoder\nfrom allennlp.common.params import Params\n\n\n@Seq2SeqEncoder.register(""stacked_gru"")\nclass StackedGRU(Seq2SeqEncoder):\n    # pylint: disable=line-too-long\n    """"""\n    This class implements a multiple layer GRU (RNN).\n    The specificity of this implementation compared to the default one in allennlp\n    (``allennlp.modules.seq2seq_encoders.Seq2SeqEncoder``) is the ability to\n    specify differents hidden state size for each layer of the in the\n    multiple-stacked-layers-GRU.\n    Optionally, different dropouts can be individually specified for each layer of the encoder.\n\n    Parameters\n    ----------\n    input_dim : ``int``, required.\n        The size of the last dimension of the input tensor.\n    hidden_sizes : ``List[int]``, required.\n        The hidden state sizes of each layer of the stacked-GRU.\n    num_layers : ``int``, required.\n        The number of layers to stack in the encoder.\n    bidirectional : ``bool``, required\n        Wheter or not the layers should be bidirectional.\n    dropouts : ``List[float]``, optional (default = None).\n        The dropout probabilities applied to each layer. The length of this list should\n        be equal to the number of layers ``num_layers``.\n    """"""\n\n    def __init__(\n        self,\n        input_dim: int,\n        hidden_sizes: List[int],\n        num_layers: int,\n        bidirectional: bool,\n        dropouts: List[float] = None,\n    ) -> None:\n        super(StackedGRU, self).__init__()\n\n        self._input_dim = input_dim\n        self._hidden_sizes = hidden_sizes\n        self._num_layers = num_layers\n        self._bidirectional = bidirectional\n        self._dropouts = [0.0] * num_layers if dropouts is None else dropouts\n\n        if len(self._hidden_sizes) != self._num_layers:\n            raise ValueError(\n                f""Number of layers ({self._num_layers}) must be equal to the length of hidden state size list ({len(self._hidden_sizes)})""\n            )\n        if len(self._dropouts) != self._num_layers:\n            raise ValueError(\n                f""Number of layers ({self._num_layers}) must be equal to the legnth of drouput rates list ({len(self._dropouts)})""\n            )\n\n        self._output_dim = hidden_sizes[-1]\n        if self._bidirectional:\n            self._output_dim *= 2\n\n        self._gru_layers: List[GRU] = []\n        for k in range(self._num_layers):\n            input_size = self._input_dim if k == 0 else self._hidden_sizes[k - 1]\n            if self._bidirectional and (k != 0):\n                input_size *= 2\n\n            gru_layer = GRU(\n                input_size=input_size,\n                hidden_size=self._hidden_sizes[k],\n                dropout=self._dropouts[k],\n                num_layers=1,\n                bidirectional=self._bidirectional,\n            )\n            self.add_module(f""gru_{k}"", gru_layer)\n            self._gru_layers.append(gru_layer)\n\n    def get_input_dim(self):\n        return self._input_dim\n\n    def get_output_dim(self):\n        return self._output_dim\n\n    @overrides\n    def is_bidirectional(self):\n        return self._bidirectional\n\n    @overrides\n    def forward(\n        self, inputs: torch.Tensor, mask: torch.LongTensor = None  # pylint: disable=arguments-differ\n    ) -> torch.FloatTensor:\n        """"""\n        Parameters\n        ----------\n        inputs : ``torch.FloatTensor``, required.\n            A tensor of shape (batch_size, timesteps, input_dim)\n        mask : ``torch.FloatTensor``, optional (default = None).\n            A tensor of shape (batch_size, timesteps).\n\n        Returns\n        -------\n        A tensor of shape (batch_size, timesteps, output_projection_dim),\n        where output_projection_dim = input_dim by default.\n        """"""\n        gru = self._gru_layers[0]\n        outputs, _ = gru(inputs)\n\n        for k in range(1, self._num_layers):\n            gru = self._gru_layers[k]\n            next_outputs, _ = gru(outputs)\n            outputs = next_outputs\n\n        return outputs\n\n    @classmethod\n    def from_params(cls, params: Params) -> ""StackedGRU"":\n        input_dim = params.pop_int(""input_dim"")\n        hidden_sizes = params.pop(""hidden_sizes"")\n        dropouts = params.pop(""dropouts"", None)\n        num_layers = params.pop_int(""num_layers"")\n        bidirectional = params.pop_bool(""bidirectional"")\n        params.assert_empty(cls.__name__)\n\n        return cls(\n            input_dim=input_dim,\n            hidden_sizes=hidden_sizes,\n            num_layers=num_layers,\n            bidirectional=bidirectional,\n            dropouts=dropouts,\n        )\n'"
hmtl/modules/text_field_embedders/__init__.py,0,b'# coding: utf-8\n\nfrom hmtl.modules.text_field_embedders.shortcut_connect_text_field_embedder import ShortcutConnectTextFieldEmbedder\n'
hmtl/modules/text_field_embedders/shortcut_connect_text_field_embedder.py,3,"b'# coding: utf-8\n\nfrom typing import Dict, List\n\nimport torch\nfrom overrides import overrides\n\nfrom allennlp.modules.text_field_embedders.text_field_embedder import TextFieldEmbedder\nfrom allennlp.modules.seq2seq_encoders.seq2seq_encoder import Seq2SeqEncoder\nimport allennlp.nn.util as util\n\n\n@TextFieldEmbedder.register(""shortcut_connect_text_field_embedder"")\nclass ShortcutConnectTextFieldEmbedder(TextFieldEmbedder):\n    """"""\n    This class implement a specific text field embedder that benefits from the output of \n    a ``allennlp.modules.seq2seq_encoders.seq2seq_encoder.Seq2SeqEncoder``.\n    It simply concatenate two embeddings vectors: the one from the previous_encoder \n    (an ``allennlp.modules.seq2seq_encoders.seq2seq_encoder.Seq2SeqEncoder``)  and\n    the one from the base_text_field_embedder \n    (an ``allennlp.modules.text_field_embedders.text_field_embedder.TextFieldEmbedder``).\n    The latter actually computes the word representation and explains the name of this class\n    ""ShortcutConnectTextFieldEmbedder"": it will feed the input of a ``Seq2SeqEncoder`` \n    with the output of the previous_encoder and the output of the base_text_field_embedder,\n    the connection with base_text_field_embedder actually circumventing the previous_encoder.\n    \n    Parameters\n    ----------\n    base_text_field_embedder : ``TextFieldEmbedder``, required\n        The text field embedder that computes the word representation at the base of the model.\n    previous_encoder : ``Seq2SeqEncoder``, required\n        The previous seq2seqencoder.\n    """"""\n\n    def __init__(self, base_text_field_embedder: TextFieldEmbedder, previous_encoders: List[Seq2SeqEncoder]) -> None:\n        super(ShortcutConnectTextFieldEmbedder, self).__init__()\n        self._base_text_field_embedder = base_text_field_embedder\n        self._previous_encoders = previous_encoders\n\n    @overrides\n    def get_output_dim(self) -> int:\n        output_dim = 0\n        output_dim += self._base_text_field_embedder.get_output_dim()\n        output_dim += self._previous_encoders[-1].get_output_dim()\n\n        return output_dim\n\n    @overrides\n    def forward(self, text_field_input: Dict[str, torch.Tensor], num_wrapping_dims: int = 0) -> torch.Tensor:\n        text_field_embeddings = self._base_text_field_embedder.forward(text_field_input, num_wrapping_dims)\n        base_representation = text_field_embeddings\n        mask = util.get_text_field_mask(text_field_input)\n\n        for encoder in self._previous_encoders:\n            text_field_embeddings = encoder(text_field_embeddings, mask)\n            text_field_embeddings = torch.cat([base_representation, text_field_embeddings], dim=-1)\n\n        return torch.cat([text_field_embeddings], dim=-1)\n'"
hmtl/training/metrics/__init__.py,0,b'# coding: utf-8\n\nfrom hmtl.training.metrics.relation_f1_measure import RelationF1Measure\nfrom hmtl.training.metrics.conll_coref_full_scores import ConllCorefFullScores\n'
hmtl/training/metrics/conll_coref_full_scores.py,0,"b'from overrides import overrides\n\nfrom allennlp.training.metrics import ConllCorefScores\n\n\nclass ConllCorefFullScores(ConllCorefScores):\n    """"""\n    This is marginal modification of the class ``allennlp.training.metrics.metric.ConllCorefScores``.\n    It leaves the possibility to get the 3 detailled coreference metrics (B3, MUC, CEAFE),\n    and not only their average.\n    """"""\n\n    def __init__(self) -> None:\n        super(ConllCorefFullScores, self).__init__()\n\n    @overrides\n    def get_metric(self, reset: bool = False, full: bool = False):\n        full_metrics = {}\n        if full:\n            for e in self.scorers:\n                metric_name = e.metric.__name__\n                full_metrics[metric_name] = {\n                    ""precision"": e.get_precision(),\n                    ""recall"": e.get_recall(),\n                    ""f1_score"": e.get_f1(),\n                }\n\n        metrics = (lambda e: e.get_precision(), lambda e: e.get_recall(), lambda e: e.get_f1())\n        precision, recall, f1_score = tuple(\n            sum(metric(e) for e in self.scorers) / len(self.scorers) for metric in metrics\n        )\n\n        full_metrics[""coref_precision""] = precision\n        full_metrics[""coref_recall""] = recall\n        full_metrics[""coref_f1""] = f1_score\n\n        if reset:\n            self.reset()\n\n        return full_metrics\n'"
hmtl/training/metrics/relation_f1_measure.py,6,"b'from typing import Dict, List, Optional, Set\nfrom collections import defaultdict\n\nimport torch\n\nfrom allennlp.common.checks import ConfigurationError\nfrom allennlp.nn.util import get_lengths_from_binary_sequence_mask  # , ones_like\nfrom allennlp.data.vocabulary import Vocabulary\nfrom allennlp.training.metrics.metric import Metric\n\n\n@Metric.register(""relation_f1"")\nclass RelationF1Measure(Metric):\n    """"""\n    """"""\n\n    def __init__(self) -> None:\n        """"""\n        A class for computing the metrics specific to relation extraction.\n        We consider a relation correct if we correctly predict the last of the head of the two arguments and the relation type.\n        """"""\n        self._true_positives: int = 0\n        self._false_positives: int = 0\n        self._false_negatives: int = 0\n\n    def __call__(self, predictions: torch.Tensor, gold_labels: torch.Tensor, mask: Optional[torch.Tensor] = None):\n        """"""\n        Update the TP, FP and FN counters.\n        \n        Parameters\n        ----------\n        predictions : ``torch.Tensor``, required.\n            A tensor of predictions of shape (batch_size, sequence_length, num_classes).\n        gold_labels : ``torch.Tensor``, required.\n            A tensor of integer class label of shape (batch_size, sequence_length). It must be the same\n            shape as the ``predictions`` tensor without the ``num_classes`` dimension.\n        mask: ``torch.Tensor``, optional (default = None).\n            A masking tensor the same size as ``gold_labels``.\n        """"""\n        if mask is None:\n            mask = torch.ones_like(gold_labels)  # ones_like(gold_labels)\n        # Get the data from the Variables.\n        predictions, gold_labels, mask = self.unwrap_to_tensors(predictions, gold_labels, mask)\n\n        if gold_labels.size() != predictions.size():\n            raise ConfigurationError(""Predictions and gold labels don\'t have the same size."")\n\n        # Apply mask\n        # Compute the mask before computing the loss\n        # Transform the mask that is at the sentence level (#Size: n_batches x padded_document_length)\n        # to a suitable format for the relation labels level\n        _, padded_document_length, _, n_classes = predictions.size()\n        mask = mask.float()\n        squared_mask = torch.stack([e.view(padded_document_length, 1) * e for e in mask], dim=0)\n        squared_mask = squared_mask.unsqueeze(-1).repeat(\n            1, 1, 1, n_classes\n        )  # Size: n_batches x padded_document_length x padded_document_length x n_classes\n\n        gold_labels = gold_labels.cpu()\n\n        predictions = (\n            predictions * squared_mask\n        )  # Size: n_batches x padded_document_length x padded_document_length x n_classes\n        gold_labels = (\n            gold_labels * squared_mask\n        )  # Size: n_batches x padded_document_length x padded_document_length x n_classes\n\n        # Iterate over timesteps in batch.\n        batch_size = gold_labels.size(0)\n        for i in range(batch_size):\n            flattened_predictions = predictions[i].view(-1).nonzero().cpu().numpy()\n            flattened_gold_labels = gold_labels[i].view(-1).nonzero().cpu().numpy()\n\n            for prediction in flattened_predictions:\n                if prediction in flattened_gold_labels:\n                    self._true_positives += 1\n                else:\n                    self._false_positives += 1\n            for gold in flattened_gold_labels:\n                if gold not in flattened_predictions:\n                    self._false_negatives += 1\n\n    def get_metric(self, reset: bool = False):\n        """"""\n        Get the metrics and reset the counters if necessary.\n        """"""\n        all_metrics = {}\n\n        # Compute the precision, recall and f1 for all spans jointly.\n        precision, recall, f1_measure = self._compute_metrics(\n            self._true_positives, self._false_positives, self._false_negatives\n        )\n        all_metrics[""precision-overall""] = precision\n        all_metrics[""recall-overall""] = recall\n        all_metrics[""f1-measure-overall""] = f1_measure\n        if reset:\n            self.reset()\n        return all_metrics\n\n    @staticmethod\n    def _compute_metrics(true_positives: int, false_positives: int, false_negatives: int):\n        precision = float(true_positives) / float(true_positives + false_positives + 1e-13)\n        recall = float(true_positives) / float(true_positives + false_negatives + 1e-13)\n        f1_measure = 2.0 * ((precision * recall) / (precision + recall + 1e-13))\n        return precision, recall, f1_measure\n\n    def reset(self):\n        self._true_positives = 0\n        self._false_positives = 0\n        self._false_negatives = 0\n'"
