file_path,api_count,code
setup.py,0,"b'#!/usr/bin/env python\n""""""\nSimple check list from AllenNLP repo: https://github.com/allenai/allennlp/blob/master/setup.py\n\nTo create the package for pypi.\n\n1. Change the version in __init__.py and setup.py.\n\n2. Commit these changes with the message: ""Release: VERSION""\n\n3. Add a tag in git to mark the release: ""git tag VERSION -m\'Adds tag VERSION for pypi\' ""\n   Push the tag to git: git push --tags origin master\n\n4. Build both the sources and the wheel. Do not change anything in setup.py between\n   creating the wheel and the source distribution (obviously).\n\n   For the wheel, run: ""python setup.py bdist_wheel"" in the top level allennlp directory.\n   (this will build a wheel for the python version you use to build it - make sure you use python 3.x).\n\n   For the sources, run: ""python setup.py sdist""\n   You should now have a /dist directory with both .whl and .tar.gz source versions of allennlp.\n\n5. Check that everything looks correct by uploading the package to the pypi test server:\n\n   twine upload dist/* -r pypitest\n   (pypi suggest using twine as other methods upload files via plaintext.)\n\n   Check that you can install it in a virtualenv by running:\n   pip install -i https://testpypi.python.org/pypi neuralcoref\n\n6. Upload the final version to actual pypi:\n   twine upload dist/* -r pypi\n\n7. Copy the release notes from RELEASE.md to the tag in github once everything is looking hunky-dory.\n\n""""""\nimport os\nimport subprocess\nimport sys\nimport contextlib\nfrom distutils.command.build_ext import build_ext\nfrom distutils.sysconfig import get_python_inc\nimport distutils.util\nfrom distutils import ccompiler, msvccompiler\nfrom setuptools import Extension, setup, find_packages\n\ndef is_new_osx():\n    """"""Check whether we\'re on OSX >= 10.10""""""\n    name = distutils.util.get_platform()\n    if sys.platform != ""darwin"":\n        return False\n    elif name.startswith(""macosx-10""):\n        minor_version = int(name.split(""-"")[1].split(""."")[1])\n        if minor_version >= 7:\n            return True\n        else:\n            return False\n    else:\n        return False\n\n\nPACKAGE_DATA = {\'\': [\'*.pyx\', \'*.pxd\'],\n                \'\': [\'*.h\'],}\n\n\nPACKAGES = find_packages()\n\n\nMOD_NAMES = [\'neuralcoref.neuralcoref\']\n\n\n\nCOMPILE_OPTIONS = {\n    ""msvc"": [""/Ox"", ""/EHsc""],\n    ""mingw32"": [""-O2"", ""-Wno-strict-prototypes"", ""-Wno-unused-function""],\n    ""other"": [""-O2"", ""-Wno-strict-prototypes"", ""-Wno-unused-function""],\n}\n\n\nLINK_OPTIONS = {""msvc"": [], ""mingw32"": [], ""other"": []}\n\n\nif is_new_osx():\n    # On Mac, use libc++ because Apple deprecated use of\n    # libstdc\n    COMPILE_OPTIONS[""other""].append(""-stdlib=libc++"")\n    LINK_OPTIONS[""other""].append(""-lc++"")\n    # g++ (used by unix compiler on mac) links to libstdc++ as a default lib.\n    # See: https://stackoverflow.com/questions/1653047/avoid-linking-to-libstdc\n    LINK_OPTIONS[""other""].append(""-nodefaultlibs"")\n\n\nUSE_OPENMP_DEFAULT = ""0"" if sys.platform != ""darwin"" else None\nif os.environ.get(""USE_OPENMP"", USE_OPENMP_DEFAULT) == ""1"":\n    if sys.platform == ""darwin"":\n        COMPILE_OPTIONS[""other""].append(""-fopenmp"")\n        LINK_OPTIONS[""other""].append(""-fopenmp"")\n        PACKAGE_DATA[""spacy.platform.darwin.lib""] = [""*.dylib""]\n        PACKAGES.append(""spacy.platform.darwin.lib"")\n\n    elif sys.platform == ""win32"":\n        COMPILE_OPTIONS[""msvc""].append(""/openmp"")\n\n    else:\n        COMPILE_OPTIONS[""other""].append(""-fopenmp"")\n        LINK_OPTIONS[""other""].append(""-fopenmp"")\n\n\n# By subclassing build_extensions we have the actual compiler that will be used which is really known only after finalize_options\n# http://stackoverflow.com/questions/724664/python-distutils-how-to-get-a-compiler-that-is-going-to-be-used\nclass build_ext_options:\n    def build_options(self):\n        for e in self.extensions:\n            e.extra_compile_args += COMPILE_OPTIONS.get(\n                self.compiler.compiler_type, COMPILE_OPTIONS[""other""]\n            )\n        for e in self.extensions:\n            e.extra_link_args += LINK_OPTIONS.get(\n                self.compiler.compiler_type, LINK_OPTIONS[""other""]\n            )\n\n\nclass build_ext_subclass(build_ext, build_ext_options):\n    def build_extensions(self):\n        build_ext_options.build_options(self)\n        build_ext.build_extensions(self)\n\n\n# def is_installed(requirement):\n#     try:\n#         pkg_resources.require(requirement)\n#     except pkg_resources.ResolutionError:\n#         return False\n#     else:\n#         return True\n\n# if not is_installed(\'numpy>=1.11.0\') or not is_installed(\'spacy>=2.1.0\'):\n#     print(textwrap.dedent(""""""\n#             Error: requirements needs to be installed first.\n#             You can install them via:\n#             $ pip install -r requirements.txt\n#             """"""), file=sys.stderr)\n#     exit(1)\n\n@contextlib.contextmanager\ndef chdir(new_dir):\n    old_dir = os.getcwd()\n    try:\n        os.chdir(new_dir)\n        sys.path.insert(0, new_dir)\n        yield\n    finally:\n        del sys.path[0]\n        os.chdir(old_dir)\n\n\ndef generate_cython(root, source):\n    print(\'Cythonizing sources\')\n    p = subprocess.call([sys.executable,\n                         os.path.join(root, \'bin\', \'cythonize.py\'),\n                         source], env=os.environ)\n    if p != 0:\n        raise RuntimeError(\'Running cythonize failed\')\n\n\ndef is_source_release(path):\n    return os.path.exists(os.path.join(path, \'PKG-INFO\'))\n\n\ndef setup_package():\n    root = os.path.abspath(os.path.dirname(__file__))\n    with chdir(root):\n        if not is_source_release(root):\n            generate_cython(root, \'neuralcoref\')\n\n        include_dirs = [\n            get_python_inc(plat_specific=True),\n            os.path.join(root, \'include\')]\n\n        if (ccompiler.new_compiler().compiler_type == \'msvc\'\n            and msvccompiler.get_build_version() == 9):\n            include_dirs.append(os.path.join(root, \'include\', \'msvc9\'))\n\n        ext_modules = []\n        for mod_name in MOD_NAMES:\n            mod_path = mod_name.replace(\'.\', \'/\') + \'.cpp\'\n            extra_link_args = []\n            # ???\n            # Imported from patch from @mikepb\n            # See Issue #267. Running blind here...\n            if sys.platform == \'darwin\':\n                dylib_path = [\'..\' for _ in range(mod_name.count(\'.\'))]\n                dylib_path = \'/\'.join(dylib_path)\n                dylib_path = \'@loader_path/%s/neuralcoref/platform/darwin/lib\' % dylib_path\n                extra_link_args.append(\'-Wl,-rpath,%s\' % dylib_path)\n            ext_modules.append(\n                Extension(mod_name, [mod_path],\n                    language=\'c++\', include_dirs=include_dirs,\n                    extra_link_args=extra_link_args))\n\n        setup(name=\'neuralcoref\',\n            version=\'4.0\',\n            description=""Coreference Resolution in spaCy with Neural Networks"",\n            url=\'https://github.com/huggingface/neuralcoref\',\n            author=\'Thomas Wolf\',\n            author_email=\'thomwolf@gmail.com\',\n            ext_modules=ext_modules,\n            classifiers=[\n                \'Development Status :: 3 - Alpha\',\n                \'Environment :: Console\',\n                \'Intended Audience :: Developers\',\n                ""Intended Audience :: Science/Research"",\n                ""License :: OSI Approved :: MIT License"",\n                ""Operating System :: POSIX :: Linux"",\n                ""Operating System :: MacOS :: MacOS X"",\n                ""Operating System :: Microsoft :: Windows"",\n                ""Programming Language :: Cython"",\n                ""Programming Language :: Python :: 3.6"",\n                ""Programming Language :: Python :: 3.7"",\n                ""Programming Language :: Python :: 3.8"",\n                ""Topic :: Scientific/Engineering"",\n            ],\n            install_requires=[\n                ""numpy>=1.15.0"",\n                ""boto3"",\n                ""requests>=2.13.0,<3.0.0"",\n                ""spacy>=2.1.0""],\n            setup_requires=[\'wheel\', \'spacy>=2.1.0\'],\n            python_requires="">=3.6"",\n            packages=PACKAGES,\n            package_data=PACKAGE_DATA,\n            keywords=\'NLP chatbots coreference resolution\',\n            license=\'MIT\',\n            zip_safe=False,\n            platforms=\'any\',\n            cmdclass={""build_ext"": build_ext_subclass})\n\nif __name__ == \'__main__\':\n    setup_package()\n'"
bin/cythonize.py,0,"b'#!/usr/bin/env python\n"""""" cythonize.py\n\nCythonize pyx files into C++ files as needed.\n\nUsage: cythonize.py [root]\n\nChecks pyx files to see if they have been changed relative to their\ncorresponding C++ files. If they have, then runs cython on these files to\nrecreate the C++ files.\n\nAdditionally, checks pxd files and setup.py if they have been changed. If\nthey have, rebuilds everything.\n\nChange detection based on file hashes stored in JSON format.\n\nFor now, this script should be run by developers when changing Cython files\nand the resulting C++ files checked in, so that end-users (and Python-only\ndevelopers) do not get the Cython dependencies.\n\nBased upon:\n\nhttps://raw.github.com/dagss/private-scipy-refactor/cythonize/cythonize.py\nhttps://raw.githubusercontent.com/numpy/numpy/master/tools/cythonize.py\n\nNote: this script does not check any of the dependent C++ libraries.\n""""""\n\nimport os\nimport sys\nimport json\nimport hashlib\nimport subprocess\nimport argparse\n\n\nHASH_FILE = ""cythonize.json""\n\n\ndef process_pyx(fromfile, tofile):\n    print(""Processing %s"" % fromfile)\n    try:\n        from Cython.Compiler.Version import version as cython_version\n        from distutils.version import LooseVersion\n\n        if LooseVersion(cython_version) < LooseVersion(""0.19""):\n            raise Exception(""Require Cython >= 0.19"")\n\n    except ImportError:\n        pass\n\n    flags = [""--fast-fail""]\n    if tofile.endswith("".cpp""):\n        flags += [""--cplus""]\n\n    try:\n        try:\n            r = subprocess.call(\n                [""cython""] + flags + [""-o"", tofile, fromfile], env=os.environ\n            )  # See Issue #791\n            if r != 0:\n                raise Exception(""Cython failed"")\n        except OSError:\n            # There are ways of installing Cython that don\'t result in a cython\n            # executable on the path, see gh-2397.\n            r = subprocess.call(\n                [\n                    sys.executable,\n                    ""-c"",\n                    ""import sys; from Cython.Compiler.Main import ""\n                    ""setuptools_main as main; sys.exit(main())"",\n                ]\n                + flags\n                + [""-o"", tofile, fromfile]\n            )\n            if r != 0:\n                raise Exception(""Cython failed"")\n    except OSError:\n        raise OSError(""Cython needs to be installed"")\n\n\ndef preserve_cwd(path, func, *args):\n    orig_cwd = os.getcwd()\n    try:\n        os.chdir(path)\n        func(*args)\n    finally:\n        os.chdir(orig_cwd)\n\n\ndef load_hashes(filename):\n    try:\n        return json.load(open(filename))\n    except (ValueError, IOError):\n        return {}\n\n\ndef save_hashes(hash_db, filename):\n    with open(filename, ""w"") as f:\n        f.write(json.dumps(hash_db))\n\n\ndef get_hash(path):\n    return hashlib.md5(open(path, ""rb"").read()).hexdigest()\n\n\ndef hash_changed(base, path, db):\n    full_path = os.path.normpath(os.path.join(base, path))\n    return not get_hash(full_path) == db.get(full_path)\n\n\ndef hash_add(base, path, db):\n    full_path = os.path.normpath(os.path.join(base, path))\n    db[full_path] = get_hash(full_path)\n\n\ndef process(base, filename, db):\n    root, ext = os.path.splitext(filename)\n    if ext in ["".pyx"", "".cpp""]:\n        if hash_changed(base, filename, db) or not os.path.isfile(\n            os.path.join(base, root + "".cpp"")\n        ):\n            preserve_cwd(base, process_pyx, root + "".pyx"", root + "".cpp"")\n            hash_add(base, root + "".cpp"", db)\n            hash_add(base, root + "".pyx"", db)\n\n\ndef check_changes(root, db):\n    res = False\n    new_db = {}\n\n    setup_filename = ""setup.py""\n    hash_add(""."", setup_filename, new_db)\n    if hash_changed(""."", setup_filename, db):\n        res = True\n\n    for base, _, files in os.walk(root):\n        for filename in files:\n            if filename.endswith("".pxd""):\n                hash_add(base, filename, new_db)\n                if hash_changed(base, filename, db):\n                    res = True\n\n    if res:\n        db.clear()\n        db.update(new_db)\n    return res\n\n\ndef run(root):\n    db = load_hashes(HASH_FILE)\n\n    try:\n        check_changes(root, db)\n        for base, _, files in os.walk(root):\n            for filename in files:\n                process(base, filename, db)\n    finally:\n        save_hashes(db, HASH_FILE)\n\n\nif __name__ == ""__main__"":\n    parser = argparse.ArgumentParser(\n        description=""Cythonize pyx files into C++ files as needed""\n    )\n    parser.add_argument(""root"", help=""root directory"")\n    args = parser.parse_args()\n    run(args.root)\n'"
examples/server.py,0,"b'#!/usr/bin/env python\n""""""Coreference resolution server example.\nA simple server serving the coreference system.\n""""""\n\nimport json\nfrom wsgiref.simple_server import make_server\nimport falcon\nimport spacy\nimport neuralcoref\n\n# Python 3\nunicode_ = str\n\n\nclass AllResource(object):\n    def __init__(self):\n        self.nlp = spacy.load(""en"")\n        neuralcoref.add_to_pipe(self.nlp)\n        print(""Server loaded"")\n        self.response = None\n\n    def on_get(self, req, resp):\n        self.response = {}\n\n        text_param = req.get_param_as_list(""text"")\n        print(""text: "", text_param)\n        if text_param is not None:\n            text = "","".join(text_param) if isinstance(text_param, list) else text_param\n            text = unicode_(text)\n            doc = self.nlp(text)\n            if doc._.has_coref:\n                mentions = [\n                    {\n                        ""start"": mention.start_char,\n                        ""end"": mention.end_char,\n                        ""text"": mention.text,\n                        ""resolved"": cluster.main.text,\n                    }\n                    for cluster in doc._.coref_clusters\n                    for mention in cluster.mentions\n                ]\n                clusters = list(\n                    list(span.text for span in cluster)\n                    for cluster in doc._.coref_clusters\n                )\n                resolved = doc._.coref_resolved\n                self.response[""mentions""] = mentions\n                self.response[""clusters""] = clusters\n                self.response[""resolved""] = resolved\n\n        resp.body = json.dumps(self.response)\n        resp.content_type = ""application/json""\n        resp.append_header(""Access-Control-Allow-Origin"", ""*"")\n        resp.status = falcon.HTTP_200\n\n\nif __name__ == ""__main__"":\n    RESSOURCE = AllResource()\n    APP = falcon.API()\n    APP.add_route(""/"", RESSOURCE)\n    HTTPD = make_server(""0.0.0.0"", 8000, APP)\n    HTTPD.serve_forever()\n'"
neuralcoref/__init__.py,0,"b'import os\nimport tarfile\nimport logging\n\n# Filter Cython warnings that would force everybody to re-compile from source (like https://github.com/numpy/numpy/pull/432).\nimport warnings\n\nwarnings.filterwarnings(""ignore"", message=""spacy.strings.StringStore size changed"")\n\nfrom neuralcoref.neuralcoref import NeuralCoref\nfrom neuralcoref.file_utils import (\n    NEURALCOREF_MODEL_URL,\n    NEURALCOREF_MODEL_PATH,\n    NEURALCOREF_CACHE,\n    cached_path,\n)\n\n__all__ = [""NeuralCoref"", ""add_to_pipe""]\n__version__ = ""4.1.0""\n\nlogger = logging.getLogger(__name__)\n\nif os.path.exists(NEURALCOREF_MODEL_PATH) and os.path.exists(\n    os.path.join(NEURALCOREF_MODEL_PATH, ""cfg"")\n):\n    logger.info(f""Loading model from {NEURALCOREF_MODEL_PATH}"")\n    local_model = cached_path(NEURALCOREF_MODEL_PATH)\nelse:\n    if not os.path.exists(NEURALCOREF_MODEL_PATH):\n        os.makedirs(NEURALCOREF_MODEL_PATH, exist_ok=True)\n    logger.info(f""Getting model from {NEURALCOREF_MODEL_URL} or cache"")\n    downloaded_model = cached_path(NEURALCOREF_MODEL_URL)\n\n    logger.info(\n        f""extracting archive file {downloaded_model} to dir {NEURALCOREF_MODEL_PATH}""\n    )\n    with tarfile.open(downloaded_model, ""r:gz"") as archive:\n        archive.extractall(NEURALCOREF_CACHE)\n\n\ndef add_to_pipe(nlp, **kwargs):\n    coref = NeuralCoref(nlp.vocab, **kwargs)\n    nlp.add_pipe(coref, name=""neuralcoref"")\n    return nlp\n'"
neuralcoref/file_utils.py,0,"b'""""""\nUtilities for working with the local dataset cache.\nThis file is adapted from the AllenNLP library at https://github.com/allenai/allennlp\nCopyright by the AllenNLP authors.\n""""""\n\nimport json\nimport logging\nimport os\nimport shutil\nimport tempfile\nfrom functools import wraps\nfrom hashlib import sha256\nfrom io import open\n\nimport boto3\nimport requests\nfrom botocore.exceptions import ClientError\nfrom tqdm import tqdm\n\ntry:\n    from urllib.parse import urlparse\nexcept ImportError:\n    from urlparse import urlparse\n\nNEURALCOREF_CACHE = os.getenv(\n    ""NEURALCOREF_CACHE"", os.path.join(os.path.expanduser(""~""), "".neuralcoref_cache"")\n)\n\nNEURALCOREF_MODEL_URL = (\n    ""https://s3.amazonaws.com/models.huggingface.co/neuralcoref/neuralcoref.tar.gz""\n)\nNEURALCOREF_MODEL_PATH = os.path.join(str(NEURALCOREF_CACHE), ""neuralcoref"")\n\nlogger = logging.getLogger(__name__)  # pylint: disable=invalid-name\n\n\ndef url_to_filename(url, etag=None):\n    """"""\n    Convert `url` into a hashed filename in a repeatable way.\n    If `etag` is specified, append its hash to the url\'s, delimited\n    by a period.\n    """"""\n    url_bytes = url.encode(""utf-8"")\n    url_hash = sha256(url_bytes)\n    filename = url_hash.hexdigest()\n\n    if etag:\n        etag_bytes = etag.encode(""utf-8"")\n        etag_hash = sha256(etag_bytes)\n        filename += ""."" + etag_hash.hexdigest()\n\n    return filename\n\n\ndef filename_to_url(filename, cache_dir=None):\n    """"""\n    Return the url and etag (which may be ``None``) stored for `filename`.\n    Raise ``EnvironmentError`` if `filename` or its stored metadata do not exist.\n    """"""\n    if cache_dir is None:\n        cache_dir = NEURALCOREF_CACHE\n\n    cache_path = os.path.join(cache_dir, filename)\n    if not os.path.exists(cache_path):\n        raise EnvironmentError(f""file {cache_path} not found"")\n\n    meta_path = cache_path + "".json""\n    if not os.path.exists(meta_path):\n        raise EnvironmentError(f""file {meta_path} not found"")\n\n    with open(meta_path, encoding=""utf-8"") as meta_file:\n        metadata = json.load(meta_file)\n    url = metadata[""url""]\n    etag = metadata[""etag""]\n\n    return url, etag\n\n\ndef cached_path(url_or_filename, cache_dir=None):\n    """"""\n    Given something that might be a URL (or might be a local path),\n    determine which. If it\'s a URL, download the file and cache it, and\n    return the path to the cached file. If it\'s already a local path,\n    make sure the file exists and then return the path.\n    """"""\n    if cache_dir is None:\n        cache_dir = NEURALCOREF_CACHE\n\n    parsed = urlparse(url_or_filename)\n\n    if parsed.scheme in (""http"", ""https"", ""s3""):\n        # URL, so get it from the cache (downloading if necessary)\n        return get_from_cache(url_or_filename, cache_dir)\n    elif os.path.exists(url_or_filename):\n        # File, and it exists.\n        return url_or_filename\n    elif parsed.scheme == """":\n        # File, but it doesn\'t exist.\n        raise EnvironmentError(f""file {url_or_filename} not found"")\n    else:\n        # Something unknown\n        raise ValueError(\n            f""unable to parse {url_or_filename} as a URL or as a local path""\n        )\n\n\ndef split_s3_path(url):\n    """"""Split a full s3 path into the bucket name and path.""""""\n    parsed = urlparse(url)\n    if not parsed.netloc or not parsed.path:\n        raise ValueError(f""bad s3 path {url}"")\n    bucket_name = parsed.netloc\n    s3_path = parsed.path\n    # Remove \'/\' at beginning of path.\n    if s3_path.startswith(""/""):\n        s3_path = s3_path[1:]\n    return bucket_name, s3_path\n\n\ndef s3_request(func):\n    """"""\n    Wrapper function for s3 requests in order to create more helpful error\n    messages.\n    """"""\n\n    @wraps(func)\n    def wrapper(url, *args, **kwargs):\n        try:\n            return func(url, *args, **kwargs)\n        except ClientError as exc:\n            if int(exc.response[""Error""][""Code""]) == 404:\n                raise EnvironmentError(f""file {url} not found"")\n            else:\n                raise\n\n    return wrapper\n\n\n@s3_request\ndef s3_etag(url):\n    """"""Check ETag on S3 object.""""""\n    s3_resource = boto3.resource(""s3"")\n    bucket_name, s3_path = split_s3_path(url)\n    s3_object = s3_resource.Object(bucket_name, s3_path)\n    return s3_object.e_tag\n\n\n@s3_request\ndef s3_get(url, temp_file):\n    """"""Pull a file directly from S3.""""""\n    s3_resource = boto3.resource(""s3"")\n    bucket_name, s3_path = split_s3_path(url)\n    s3_resource.Bucket(bucket_name).download_fileobj(s3_path, temp_file)\n\n\ndef http_get(url, temp_file):\n    req = requests.get(url, stream=True)\n    content_length = req.headers.get(""Content-Length"")\n    total = int(content_length) if content_length is not None else None\n    progress = tqdm(unit=""B"", total=total)\n    for chunk in req.iter_content(chunk_size=1024):\n        if chunk:  # filter out keep-alive new chunks\n            progress.update(len(chunk))\n            temp_file.write(chunk)\n    progress.close()\n\n\ndef get_from_cache(url, cache_dir=None):\n    """"""\n    Given a URL, look for the corresponding dataset in the local cache.\n    If it\'s not there, download it. Then return the path to the cached file.\n    """"""\n    if cache_dir is None:\n        cache_dir = NEURALCOREF_CACHE\n\n    if not os.path.exists(cache_dir):\n        os.makedirs(cache_dir)\n\n    # Get eTag to add to filename, if it exists.\n    if url.startswith(""s3://""):\n        etag = s3_etag(url)\n    else:\n        response = requests.head(url, allow_redirects=True)\n        if response.status_code != 200:\n            raise IOError(\n                f""HEAD request failed for url {url} with status code {response.status_code}""\n            )\n        etag = response.headers.get(""ETag"")\n\n    filename = url_to_filename(url, etag)\n\n    # get cache path to put the file\n    cache_path = os.path.join(cache_dir, filename)\n\n    if not os.path.exists(cache_path):\n        # Download to temporary file, then copy to cache dir once finished.\n        # Otherwise you get corrupt cache entries if the download gets interrupted.\n        with tempfile.NamedTemporaryFile() as temp_file:\n            logger.info(""%s not found in cache, downloading to %s"", url, temp_file.name)\n\n            # GET file object\n            if url.startswith(""s3://""):\n                s3_get(url, temp_file)\n            else:\n                http_get(url, temp_file)\n\n            # we are copying the file before closing it, so flush to avoid truncation\n            temp_file.flush()\n            # shutil.copyfileobj() starts at the current position, so go to the start\n            temp_file.seek(0)\n\n            logger.info(""copying %s to cache at %s"", temp_file.name, cache_path)\n            with open(cache_path, ""wb"") as cache_file:\n                shutil.copyfileobj(temp_file, cache_file)\n\n            logger.info(""creating metadata file for %s"", cache_path)\n            meta = {""url"": url, ""etag"": etag}\n            meta_path = cache_path + "".json""\n            with open(meta_path, ""w"", encoding=""utf-8"") as meta_file:\n                json.dump(meta, meta_file)\n\n            logger.info(""removing temp file %s"", temp_file.name)\n\n    return cache_path\n\n\ndef read_set_from_file(filename):\n    """"""\n    Extract a de-duped collection (set) of text from a file.\n    Expected file format is one item per line.\n    """"""\n    collection = set()\n    with open(filename, ""r"", encoding=""utf-8"") as file_:\n        for line in file_:\n            collection.add(line.rstrip())\n    return collection\n\n\ndef get_file_extension(path, dot=True, lower=True):\n    ext = os.path.splitext(path)[1]\n    ext = ext if dot else ext[1:]\n    return ext.lower() if lower else ext\n'"
neuralcoref/tests/__init__.py,0,b''
neuralcoref/tests/test_neuralcoref.py,0,"b'import spacy\nfrom ..__init__ import add_to_pipe\n\n\ndef test_add_pipe():\n    nlp = spacy.lang.en.English()\n    add_to_pipe(nlp)\n    assert ""neuralcoref"" in nlp.pipe_names\n'"
neuralcoref/train/__init__.py,0,b''
neuralcoref/train/algorithm.py,0,"b'# cython: profile=True\n# cython: infer_types=True\n""""""Coref resolution""""""\n\nimport os\nimport spacy\nimport numpy as np\n\nfrom neuralcoref.train.utils import PACKAGE_DIRECTORY, SIZE_SINGLE_IN\nfrom neuralcoref.train.compat import unicode_\nfrom neuralcoref.train.document import Document, MENTION_TYPE, NO_COREF_LIST\n\n#######################\n##### UTILITIES #######\n\nMAX_FOLLOW_UP = 50\n\n#######################\n###### CLASSES ########\n\n\nclass Model(object):\n    """"""\n    Coreference neural model\n    """"""\n\n    def __init__(self, model_path):\n        weights, biases = [], []\n        for file in sorted(os.listdir(model_path)):\n            if file.startswith(""single_mention_weights""):\n                w = np.load(os.path.join(model_path, file))\n                weights.append(w)\n            if file.startswith(""single_mention_bias""):\n                w = np.load(os.path.join(model_path, file))\n                biases.append(w)\n        self.single_mention_model = list(zip(weights, biases))\n        weights, biases = [], []\n        for file in sorted(os.listdir(model_path)):\n            if file.startswith(""pair_mentions_weights""):\n                w = np.load(os.path.join(model_path, file))\n                weights.append(w)\n            if file.startswith(""pair_mentions_bias""):\n                w = np.load(os.path.join(model_path, file))\n                biases.append(w)\n        self.pair_mentions_model = list(zip(weights, biases))\n\n    def _score(self, features, layers):\n        for weights, bias in layers:\n            # print(""features"", features.shape)\n            features = np.matmul(weights, features) + bias\n            if weights.shape[0] > 1:\n                features = np.maximum(features, 0)  # ReLU\n        return np.sum(features, axis=0)\n\n    def get_multiple_single_score(self, first_layer_input):\n        return self._score(first_layer_input, self.single_mention_model)\n\n    def get_multiple_pair_score(self, first_layer_input):\n        return self._score(first_layer_input, self.pair_mentions_model)\n\n\nclass Coref(object):\n    """"""\n    Main coreference resolution algorithm\n    """"""\n\n    def __init__(\n        self,\n        nlp=None,\n        greedyness=0.5,\n        max_dist=50,\n        max_dist_match=500,\n        conll=None,\n        blacklist=True,\n        debug=False,\n    ):\n        self.greedyness = greedyness\n        self.max_dist = max_dist\n        self.max_dist_match = max_dist_match\n        self.debug = debug\n        model_path = os.path.join(\n            PACKAGE_DIRECTORY, ""weights/conll/"" if conll is not None else ""weights/""\n        )\n        model_path = os.path.join(PACKAGE_DIRECTORY, ""weights/"")\n        print(""Loading neuralcoref model from"", model_path)\n        self.coref_model = Model(model_path)\n        if nlp is None:\n            print(""Loading spacy model"")\n            try:\n                spacy.info(""en_core_web_sm"")\n                model = ""en_core_web_sm""\n            except IOError:\n                print(""No spacy 2 model detected, using spacy1 \'en\' model"")\n                spacy.info(""en"")\n                model = ""en""\n            nlp = spacy.load(model)\n        self.data = Document(\n            nlp, conll=conll, blacklist=blacklist, model_path=model_path\n        )\n        self.clusters = {}\n        self.mention_to_cluster = []\n        self.mentions_single_scores = {}\n        self.mentions_pairs_scores = {}\n\n    ###################################\n    #### ENTITY CLUSTERS FUNCTIONS ####\n    ###################################\n\n    def _prepare_clusters(self):\n        """"""\n        Clean up and prepare one cluster for each mention\n        """"""\n        self.mention_to_cluster = list(range(len(self.data.mentions)))\n        self.clusters = dict((i, [i]) for i in self.mention_to_cluster)\n        self.mentions_single_scores = {}\n        self.mentions_pairs_scores = {}\n        for mention in self.mention_to_cluster:\n            self.mentions_single_scores[mention] = None\n            self.mentions_pairs_scores[mention] = {}\n\n    def _merge_coreference_clusters(self, ant_idx, mention_idx):\n        """"""\n        Merge two clusters together\n        """"""\n        if self.mention_to_cluster[ant_idx] == self.mention_to_cluster[mention_idx]:\n            return\n\n        remove_id = self.mention_to_cluster[ant_idx]\n        keep_id = self.mention_to_cluster[mention_idx]\n        for idx in self.clusters[remove_id]:\n            self.mention_to_cluster[idx] = keep_id\n            self.clusters[keep_id].append(idx)\n\n        del self.clusters[remove_id]\n\n    def remove_singletons_clusters(self):\n        remove_id = []\n        for key, mentions in self.clusters.items():\n            if len(mentions) == 1:\n                remove_id.append(key)\n                self.mention_to_cluster[key] = None\n        for rem in remove_id:\n            del self.clusters[rem]\n\n    def display_clusters(self):\n        """"""\n        Print clusters informations\n        """"""\n        print(self.clusters)\n        for key, mentions in self.clusters.items():\n            print(\n                ""cluster"",\n                key,\n                ""("",\n                "", "".join(unicode_(self.data[m]) for m in mentions),\n                "")"",\n            )\n\n    ###################################\n    ####### MAIN COREF FUNCTIONS ######\n    ###################################\n\n    def run_coref_on_mentions(self, mentions):\n        """"""\n        Run the coreference model on a mentions list\n        """"""\n        best_ant = {}\n        best_score = {}\n        n_ant = 0\n        inp = np.empty((SIZE_SINGLE_IN, len(mentions)))\n        for i, mention_idx in enumerate(mentions):\n            mention = self.data[mention_idx]\n            print(""mention.embedding"", mention.embedding.shape)\n            inp[: len(mention.embedding), i] = mention.embedding\n            inp[: len(mention.embedding), i] = mention.features\n            inp[: len(mention.embedding), i] = self.data.genre\n        score = self.coref_model.get_multiple_single_score(inp.T)\n        for mention_idx, s in zip(mentions, score):\n            self.mentions_single_scores[mention_idx] = s\n            best_score[mention_idx] = s - 50 * (self.greedyness - 0.5)\n\n        for mention_idx, ant_list in self.data.get_candidate_pairs(\n            mentions, self.max_dist, self.max_dist_match\n        ):\n            if len(ant_list) == 0:\n                continue\n            inp_l = []\n            for ant_idx in ant_list:\n                mention = self.data[mention_idx]\n                antecedent = self.data[ant_idx]\n                feats_, pwf = self.data.get_pair_mentions_features(antecedent, mention)\n                inp_l.append(pwf)\n            inp = np.stack(inp_l, axis=0)\n            score = self.coref_model.get_multiple_pair_score(inp.T)\n            for ant_idx, s in zip(ant_list, score):\n                self.mentions_pairs_scores[mention_idx][ant_idx] = s\n                if s > best_score[mention_idx]:\n                    best_score[mention_idx] = s\n                    best_ant[mention_idx] = ant_idx\n            if mention_idx in best_ant:\n                n_ant += 1\n                self._merge_coreference_clusters(best_ant[mention_idx], mention_idx)\n        return (n_ant, best_ant)\n\n    def run_coref_on_utterances(\n        self, last_utterances_added=False, follow_chains=True, debug=False\n    ):\n        """""" Run the coreference model on some utterances\n\n        Arg:\n            last_utterances_added: run the coreference model over the last utterances added to the data\n            follow_chains: follow coreference chains over previous utterances\n        """"""\n        if debug:\n            print(""== run_coref_on_utterances == start"")\n        self._prepare_clusters()\n        if debug:\n            self.display_clusters()\n        mentions = list(\n            self.data.get_candidate_mentions(\n                last_utterances_added=last_utterances_added\n            )\n        )\n        n_ant, antecedents = self.run_coref_on_mentions(mentions)\n        mentions = antecedents.values()\n        if follow_chains and last_utterances_added and n_ant > 0:\n            i = 0\n            while i < MAX_FOLLOW_UP:\n                i += 1\n                n_ant, antecedents = self.run_coref_on_mentions(mentions)\n                mentions = antecedents.values()\n                if n_ant == 0:\n                    break\n        if debug:\n            self.display_clusters()\n        if debug:\n            print(""== run_coref_on_utterances == end"")\n\n    def one_shot_coref(\n        self,\n        utterances,\n        utterances_speakers_id=None,\n        context=None,\n        context_speakers_id=None,\n        speakers_names=None,\n    ):\n        """""" Clear history, load a list of utterances and an optional context and run the coreference model on them\n\n        Arg:\n        - `utterances` : iterator or list of string corresponding to successive utterances (in a dialogue) or sentences.\n            Can be a single string for non-dialogue text.\n        - `utterances_speakers_id=None` : iterator or list of speaker id for each utterance (in the case of a dialogue).\n            - if not provided, assume two speakers speaking alternatively.\n            - if utterances and utterances_speaker are not of the same length padded with None\n        - `context=None` : iterator or list of string corresponding to additionnal utterances/sentences sent prior to `utterances`. Coreferences are not computed for the mentions identified in `context`. The mentions in `context` are only used as possible antecedents to mentions in `uterrance`. Reduce the computations when we are only interested in resolving coreference in the last sentences/utterances.\n        - `context_speakers_id=None` : same as `utterances_speakers_id` for `context`. \n        - `speakers_names=None` : dictionnary of list of acceptable speaker names (strings) for speaker_id in `utterances_speakers_id` and `context_speakers_id`\n        Return:\n            clusters of entities with coreference resolved\n        """"""\n        self.data.set_utterances(context, context_speakers_id, speakers_names)\n        self.continuous_coref(utterances, utterances_speakers_id, speakers_names)\n        return self.get_clusters()\n\n    def continuous_coref(\n        self, utterances, utterances_speakers_id=None, speakers_names=None\n    ):\n        """"""\n        Only resolve coreferences for the mentions in the utterances\n        (but use the mentions in previously loaded utterances as possible antecedents)\n        Arg:\n            utterances : iterator or list of string corresponding to successive utterances\n            utterances_speaker : iterator or list of speaker id for each utterance.\n                If not provided, assume two speakers speaking alternatively.\n                if utterances and utterances_speaker are not of the same length padded with None\n            speakers_names : dictionnary of list of acceptable speaker names for each speaker id\n        Return:\n            clusters of entities with coreference resolved\n        """"""\n        self.data.add_utterances(utterances, utterances_speakers_id, speakers_names)\n        self.run_coref_on_utterances(last_utterances_added=True, follow_chains=True)\n        return self.get_clusters()\n\n    ###################################\n    ###### INFORMATION RETRIEVAL ######\n    ###################################\n\n    def get_utterances(self, last_utterances_added=True):\n        """""" Retrieve the list of parsed uterrances""""""\n        if last_utterances_added and len(self.data.last_utterances_loaded):\n            return [\n                self.data.utterances[idx] for idx in self.data.last_utterances_loaded\n            ]\n        else:\n            return self.data.utterances\n\n    def get_resolved_utterances(self, last_utterances_added=True, blacklist=True):\n        """""" Return a list of utterrances text where the """"""\n        coreferences = self.get_most_representative(last_utterances_added, blacklist)\n        resolved_utterances = []\n        for utt in self.get_utterances(last_utterances_added=last_utterances_added):\n            resolved_utt = """"\n            in_coref = None\n            for token in utt:\n                if in_coref is None:\n                    for coref_original, coref_replace in coreferences.items():\n                        if coref_original[0] == token:\n                            in_coref = coref_original\n                            resolved_utt += coref_replace.text.lower()\n                            break\n                    if in_coref is None:\n                        resolved_utt += token.text_with_ws\n                if in_coref is not None and token == in_coref[-1]:\n                    resolved_utt += (\n                        "" "" if token.whitespace_ and resolved_utt[-1] is not "" "" else """"\n                    )\n                    in_coref = None\n            resolved_utterances.append(resolved_utt)\n        return resolved_utterances\n\n    def get_mentions(self):\n        """""" Retrieve the list of mentions""""""\n        return self.data.mentions\n\n    def get_scores(self):\n        """""" Retrieve scores for single mentions and pair of mentions""""""\n        return {\n            ""single_scores"": self.mentions_single_scores,\n            ""pair_scores"": self.mentions_pairs_scores,\n        }\n\n    def get_clusters(self, remove_singletons=False, blacklist=False):\n        """""" Retrieve cleaned clusters""""""\n        clusters = self.clusters\n        mention_to_cluster = self.mention_to_cluster\n        remove_id = []\n        if blacklist:\n            for key, mentions in clusters.items():\n                cleaned_list = []\n                for mention_idx in mentions:\n                    mention = self.data.mentions[mention_idx]\n                    if mention.lower_ not in NO_COREF_LIST:\n                        cleaned_list.append(mention_idx)\n                clusters[key] = cleaned_list\n            # Also clean up keys so we can build coref chains in self.get_most_representative\n            added = {}\n            for key, mentions in clusters.items():\n                if self.data.mentions[key].lower_ in NO_COREF_LIST:\n                    remove_id.append(key)\n                    mention_to_cluster[key] = None\n                    if mentions:\n                        added[mentions[0]] = mentions\n            for rem in remove_id:\n                del clusters[rem]\n            clusters.update(added)\n\n        if remove_singletons:\n            remove_id = []\n            for key, mentions in clusters.items():\n                if len(mentions) == 1:\n                    remove_id.append(key)\n                    mention_to_cluster[key] = None\n            for rem in remove_id:\n                del clusters[rem]\n\n        return clusters, mention_to_cluster\n\n    def get_most_representative(self, last_utterances_added=True, blacklist=True):\n        """"""\n        Find a most representative mention for each cluster\n\n        Return:\n            Dictionnary of {original_mention: most_representative_resolved_mention, ...}\n        """"""\n        clusters, _ = self.get_clusters(remove_singletons=True, blacklist=blacklist)\n        coreferences = {}\n        for key in self.data.get_candidate_mentions(\n            last_utterances_added=last_utterances_added\n        ):\n            if self.mention_to_cluster[key] is None:\n                continue\n            mentions = clusters.get(self.mention_to_cluster[key], None)\n            if mentions is None:\n                continue\n            representative = self.data.mentions[key]\n            for mention_idx in mentions[1:]:\n                mention = self.data.mentions[mention_idx]\n                if mention.mention_type is not representative.mention_type:\n                    if mention.mention_type == MENTION_TYPE[""PROPER""] or (\n                        mention.mention_type == MENTION_TYPE[""NOMINAL""]\n                        and representative.mention_type == MENTION_TYPE[""PRONOMINAL""]\n                    ):\n                        coreferences[self.data.mentions[key]] = mention\n                        representative = mention\n\n        return coreferences\n'"
neuralcoref/train/compat.py,0,"b'import sys\n\nis_windows = sys.platform.startswith(""win"")\nis_linux = sys.platform.startswith(""linux"")\nis_osx = sys.platform == ""darwin""\n\n\n# Python 3 is default, Python 2 is not supported anymore\nunicode_ = str\nbytes_ = bytes\nstring_types = (bytes, str)\nchr_ = chr\n\n\ndef unicode_to_bytes(s, encoding=""utf8"", errors=""strict""):\n    return s.encode(encoding=encoding, errors=errors)\n\n\ndef bytes_to_unicode(b, encoding=""utf8"", errors=""strict""):\n    return b.decode(encoding=encoding, errors=errors)\n'"
neuralcoref/train/conllparser.py,0,"b'""""""Conll parser""""""\n\nimport re\nimport argparse\nimport time\nimport os\nimport io\nimport pickle\n\nimport spacy\n\nimport numpy as np\n\nfrom tqdm import tqdm\n\nfrom neuralcoref.train.compat import unicode_\nfrom neuralcoref.train.document import (\n    Mention,\n    Document,\n    Speaker,\n    EmbeddingExtractor,\n    MISSING_WORD,\n    extract_mentions_spans,\n)\nfrom neuralcoref.train.utils import parallel_process\n\nPACKAGE_DIRECTORY = os.path.dirname(os.path.abspath(__file__))\nREMOVED_CHAR = [""/"", ""%"", ""*""]\nNORMALIZE_DICT = {\n    ""/."": ""."",\n    ""/?"": ""?"",\n    ""-LRB-"": ""("",\n    ""-RRB-"": "")"",\n    ""-LCB-"": ""{"",\n    ""-RCB-"": ""}"",\n    ""-LSB-"": ""["",\n    ""-RSB-"": ""]"",\n}\n\nCONLL_GENRES = {""bc"": 0, ""bn"": 1, ""mz"": 2, ""nw"": 3, ""pt"": 4, ""tc"": 5, ""wb"": 6}\n\nFEATURES_NAMES = [\n    ""mentions_features"",  # 0\n    ""mentions_labels"",  # 1\n    ""mentions_pairs_length"",  # 2\n    ""mentions_pairs_start_index"",  # 3\n    ""mentions_spans"",  # 4\n    ""mentions_words"",  # 5\n    ""pairs_ant_index"",  # 6\n    ""pairs_features"",  # 7\n    ""pairs_labels"",  # 8\n    ""locations"",  # 9\n    ""conll_tokens"",  # 10\n    ""spacy_lookup"",  # 11\n    ""doc"",  # 12\n]\n\nMISSED_MENTIONS_FILE = os.path.join(\n    PACKAGE_DIRECTORY, ""test_mentions_identification.txt""\n)\nSENTENCES_PATH = os.path.join(PACKAGE_DIRECTORY, ""test_sentences.txt"")\n\n###################\n### UTILITIES #####\n\n\ndef clean_token(token):\n    cleaned_token = token\n    if cleaned_token in NORMALIZE_DICT:\n        cleaned_token = NORMALIZE_DICT[cleaned_token]\n    if cleaned_token not in REMOVED_CHAR:\n        for char in REMOVED_CHAR:\n            cleaned_token = cleaned_token.replace(char, """")\n    if len(cleaned_token) == 0:\n        cleaned_token = "",""\n    return cleaned_token\n\n\ndef mention_words_idx(embed_extractor, mention, debug=False):\n    # index of the word in the tuned embeddings no need for normalizing,\n    # it is already performed in set_mentions_features()\n    # We take them in the tuned vocabulary which is a smaller voc tailored from conll\n    words = []\n    for _, w in sorted(mention.words_embeddings_.items()):\n        if w not in embed_extractor.tun_idx:\n            if debug:\n                print(\n                    ""No matching tokens in tuned voc for word "",\n                    w,\n                    ""surrounding or inside mention"",\n                    mention,\n                )\n            words.append(MISSING_WORD)\n        else:\n            words.append(w)\n    return [embed_extractor.tun_idx[w] for w in words]\n\n\ndef check_numpy_array(feature, array, n_mentions_list, compressed=True):\n    for n_mentions in n_mentions_list:\n        if feature == FEATURES_NAMES[0]:\n            assert array.shape[0] == len(n_mentions)\n            if compressed:\n                assert np.array_equiv(\n                    array[:, 3], np.array([len(n_mentions)] * len(n_mentions))\n                )\n                assert np.max(array[:, 2]) == len(n_mentions) - 1\n                assert np.min(array[:, 2]) == 0\n        elif feature == FEATURES_NAMES[1]:\n            assert array.shape[0] == len(n_mentions)\n        elif feature == FEATURES_NAMES[2]:\n            assert array.shape[0] == len(n_mentions)\n            assert np.array_equiv(array[:, 0], np.array(list(range(len(n_mentions)))))\n        elif feature == FEATURES_NAMES[3]:\n            assert array.shape[0] == len(n_mentions)\n            assert np.array_equiv(\n                array[:, 0], np.array([p * (p - 1) / 2 for p in range(len(n_mentions))])\n            )\n        elif feature == FEATURES_NAMES[4]:\n            assert array.shape[0] == len(n_mentions)\n        elif feature == FEATURES_NAMES[5]:\n            assert array.shape[0] == len(n_mentions)\n        elif feature == FEATURES_NAMES[6]:\n            assert array.shape[0] == len(n_mentions) * (len(n_mentions) - 1) / 2\n            assert np.max(array) == len(n_mentions) - 2\n        elif feature == FEATURES_NAMES[7]:\n            if compressed:\n                assert array.shape[0] == len(n_mentions) * (len(n_mentions) - 1) / 2\n                assert np.max(array[:, 7]) == len(n_mentions) - 2\n                assert np.min(array[:, 7]) == 0\n        elif feature == FEATURES_NAMES[8]:\n            assert array.shape[0] == len(n_mentions) * (len(n_mentions) - 1) / 2\n\n\n###############################################################################################\n### PARALLEL FCT (has to be at top-level of the module to be pickled for multiprocessing) #####\ndef load_file(full_name, debug=False):\n    """"""\n    load a *._conll file\n    Input: full_name: path to the file\n    Output: list of tuples for each conll doc in the file, where the tuple contains:\n        (utts_text ([str]): list of the utterances in the document\n         utts_tokens ([[str]]): list of the tokens (conll words) in the document\n         utts_corefs: list of coref objects (dicts) with the following properties:\n            coref[\'label\']: id of the coreference cluster,\n            coref[\'start\']: start index (index of first token in the utterance),\n            coref[\'end\': end index (index of last token in the utterance).\n         utts_speakers ([str]): list of the speaker associated to each utterances in the document\n         name (str): name of the document\n         part (str): part of the document\n        )\n    """"""\n    docs = []\n    with io.open(full_name, ""rt"", encoding=""utf-8"", errors=""strict"") as f:\n        lines = list(f)  # .readlines()\n        utts_text = []\n        utts_tokens = []\n        utts_corefs = []\n        utts_speakers = []\n        tokens = []\n        corefs = []\n        index = 0\n        speaker = """"\n        name = """"\n        part = """"\n        for li, line in enumerate(lines):\n            cols = line.split()\n            if debug:\n                print(""line"", li, ""cols:"", cols)\n            # End of utterance\n            if len(cols) == 0:\n                if tokens:\n                    if debug:\n                        print(""End of utterance"")\n                    utts_text.append("""".join(t + "" "" for t in tokens))\n                    utts_tokens.append(tokens)\n                    utts_speakers.append(speaker)\n                    utts_corefs.append(corefs)\n                    tokens = []\n                    corefs = []\n                    index = 0\n                    speaker = """"\n                    continue\n            # End of doc\n            elif len(cols) == 2:\n                if debug:\n                    print(""End of doc"")\n                if cols[0] == ""#end"":\n                    if debug:\n                        print(""Saving doc"")\n                    docs.append(\n                        (utts_text, utts_tokens, utts_corefs, utts_speakers, name, part)\n                    )\n                    utts_text = []\n                    utts_tokens = []\n                    utts_corefs = []\n                    utts_speakers = []\n                else:\n                    raise ValueError(""Error on end line "" + line)\n            # New doc\n            elif len(cols) == 5:\n                if debug:\n                    print(""New doc"")\n                if cols[0] == ""#begin"":\n                    name = re.match(r""\\((.*)\\);"", cols[2]).group(1)\n                    try:\n                        part = cols[4]\n                    except ValueError:\n                        print(""Error parsing document part "" + line)\n                    if debug:\n                        print(""New doc"", name, part, name[:2])\n                    tokens = []\n                    corefs = []\n                    index = 0\n                else:\n                    raise ValueError(""Error on begin line "" + line)\n            # Inside utterance\n            elif len(cols) > 7:\n                if debug:\n                    print(""Inside utterance"")\n                assert cols[0] == name and int(cols[1]) == int(part), (\n                    ""Doc name or part error "" + line\n                )\n                assert int(cols[2]) == index, ""Index error on "" + line\n                if speaker:\n                    assert cols[9] == speaker, ""Speaker changed in "" + line + speaker\n                else:\n                    speaker = cols[9]\n                    if debug:\n                        print(""speaker"", speaker)\n                if cols[-1] != ""-"":\n                    coref_expr = cols[-1].split(""|"")\n                    if debug:\n                        print(""coref_expr"", coref_expr)\n                    if not coref_expr:\n                        raise ValueError(""Coref expression empty "" + line)\n                    for tok in coref_expr:\n                        if debug:\n                            print(""coref tok"", tok)\n                        try:\n                            match = re.match(r""^(\\(?)(\\d+)(\\)?)$"", tok)\n                        except:\n                            print(""error getting coreferences for line "" + line)\n                        assert match is not None, (\n                            ""Error parsing coref "" + tok + "" in "" + line\n                        )\n                        num = match.group(2)\n                        assert num is not """", (\n                            ""Error parsing coref "" + tok + "" in "" + line\n                        )\n                        if match.group(1) == ""("":\n                            if debug:\n                                print(""New coref"", num)\n                            corefs.append({""label"": num, ""start"": index, ""end"": None})\n                        if match.group(3) == "")"":\n                            j = None\n                            for i in range(len(corefs) - 1, -1, -1):\n                                if debug:\n                                    print(""i"", i)\n                                if (\n                                    corefs[i][""label""] == num\n                                    and corefs[i][""end""] is None\n                                ):\n                                    j = i\n                                    break\n                            assert j is not None, ""coref closing error "" + line\n                            if debug:\n                                print(""End coref"", num)\n                            corefs[j][""end""] = index\n                tokens.append(clean_token(cols[3]))\n                index += 1\n            else:\n                raise ValueError(""Line not standard "" + line)\n    return docs\n\n\ndef set_feats(doc):\n    doc.set_mentions_features()\n\n\ndef get_feats(doc, i):\n    return doc.get_feature_array(doc_id=i)\n\n\ndef gather_feats(gathering_array, array, feat_name, pairs_ant_index, pairs_start_index):\n    if gathering_array is None:\n        gathering_array = array\n    else:\n        if feat_name == FEATURES_NAMES[6]:\n            array = [a + pairs_ant_index for a in array]\n        elif feat_name == FEATURES_NAMES[3]:\n            array = [a + pairs_start_index for a in array]\n        gathering_array += array\n    return feat_name, gathering_array\n\n\ndef read_file(full_name):\n    doc = """"\n    with io.open(full_name, ""rt"", encoding=""utf-8"", errors=""strict"") as f:\n        doc = f.read()\n    return doc\n\n\n###################\n### ConllDoc #####\n\n\nclass ConllDoc(Document):\n    def __init__(self, name, part, *args, **kwargs):\n        self.name = name\n        self.part = part\n        self.feature_matrix = {}\n        self.conll_tokens = []\n        self.conll_lookup = []\n        self.gold_corefs = []\n        self.missed_gold = []\n        super(ConllDoc, self).__init__(*args, **kwargs)\n\n    def get_conll_spacy_lookup(self, conll_tokens, spacy_tokens, debug=False):\n        """"""\n        Compute a look up table between spacy tokens (from spacy tokenizer)\n        and conll pre-tokenized tokens\n        Output: list[conll_index] => list of associated spacy tokens (assume spacy tokenizer has a finer granularity)\n        """"""\n        lookup = []\n        c_iter = (t for t in conll_tokens)\n        s_iter = enumerate(t for t in spacy_tokens)\n        i, s_tok = next(s_iter)\n        for c_tok in c_iter:\n            # if debug: print(""conll"", c_tok, ""spacy"", s_tok, ""index"", i)\n            c_lookup = []\n            while i is not None and len(c_tok) and c_tok.startswith(s_tok.text):\n                c_lookup.append(i)\n                c_tok = c_tok[len(s_tok) :]\n                i, s_tok = next(s_iter, (None, None))\n                if debug and len(c_tok):\n                    print(""eating token: conll"", c_tok, ""spacy"", s_tok, ""index"", i)\n            assert len(c_lookup), ""Unmatched conll and spacy tokens""\n            lookup.append(c_lookup)\n        return lookup\n\n    def add_conll_utterance(\n        self, parsed, tokens, corefs, speaker_id, use_gold_mentions, debug=False\n    ):\n        conll_lookup = self.get_conll_spacy_lookup(tokens, parsed)\n        self.conll_tokens.append(tokens)\n        self.conll_lookup.append(conll_lookup)\n        # Convert conll tokens coref index in spacy tokens indexes\n        identified_gold = [False] * len(corefs)\n        for coref in corefs:\n            missing_values = [key for key in [\'label\', \'start\', \'end\', ] if coref.get(key, None) is None]\n            if missing_values:\n                found_values = {key: coref[key] for key in [\'label\', \'start\', \'end\'] if coref.get(key, None) is not None}\n                raise Exception(f""Coref {self.name} with fields {found_values} has empty values for the keys {missing_values}."")\n\n            coref[""start""] = conll_lookup[coref[""start""]][0]\n            coref[""end""] = conll_lookup[coref[""end""]][-1]\n\n        if speaker_id not in self.speakers:\n            speaker_name = speaker_id.split(""_"")\n            if debug:\n                print(""New speaker: "", speaker_id, ""name: "", speaker_name)\n            self.speakers[speaker_id] = Speaker(speaker_id, speaker_name)\n        if use_gold_mentions:\n            for coref in corefs:\n                # print(""coref[\'label\']"", coref[\'label\'])\n                # print(""coref text"",parsed[coref[\'start\']:coref[\'end\']+1])\n                mention = Mention(\n                    parsed[coref[""start""] : coref[""end""] + 1],\n                    len(self.mentions),\n                    len(self.utterances),\n                    self.n_sents,\n                    speaker=self.speakers[speaker_id],\n                    gold_label=coref[""label""],\n                )\n                self.mentions.append(mention)\n                # print(""mention: "", mention, ""label"", mention.gold_label)\n        else:\n            mentions_spans = extract_mentions_spans(\n                doc=parsed, blacklist=self.blacklist\n            )\n            self._process_mentions(\n                mentions_spans,\n                len(self.utterances),\n                self.n_sents,\n                self.speakers[speaker_id],\n            )\n\n            # Assign a gold label to mentions which have one\n            if debug:\n                print(""Check corefs"", corefs)\n            for i, coref in enumerate(corefs):\n                for m in self.mentions:\n                    if m.utterance_index != len(self.utterances):\n                        continue\n                    # if debug: print(""Checking mention"", m, m.utterance_index, m.start, m.end)\n                    if coref[""start""] == m.start and coref[""end""] == m.end - 1:\n                        m.gold_label = coref[""label""]\n                        identified_gold[i] = True\n                        # if debug: print(""Gold mention found:"", m, coref[\'label\'])\n            for found, coref in zip(identified_gold, corefs):\n                if not found:\n                    self.missed_gold.append(\n                        [\n                            self.name,\n                            self.part,\n                            str(len(self.utterances)),\n                            parsed.text,\n                            parsed[coref[""start""] : coref[""end""] + 1].text,\n                        ]\n                    )\n                    if debug:\n                        print(\n                            ""\xe2\x9d\x84\xef\xb8\x8f gold mention not in predicted mentions"",\n                            coref,\n                            parsed[coref[""start""] : coref[""end""] + 1],\n                        )\n        self.utterances.append(parsed)\n        self.gold_corefs.append(corefs)\n        self.utterances_speaker.append(self.speakers[speaker_id])\n        self.n_sents += len(list(parsed.sents))\n\n    def get_single_mention_features_conll(self, mention, compressed=True):\n        """""" Compressed or not single mention features""""""\n        if not compressed:\n            _, features = self.get_single_mention_features(mention)\n            return features[np.newaxis, :]\n        feat_l = [\n            mention.features_[""01_MentionType""],\n            mention.features_[""02_MentionLength""],\n            mention.index,\n            len(self.mentions),\n            mention.features_[""04_IsMentionNested""],\n            self.genre_,\n        ]\n        return feat_l\n\n    def get_pair_mentions_features_conll(self, m1, m2, compressed=True):\n        """""" Compressed or not single mention features""""""\n        if not compressed:\n            _, features = self.get_pair_mentions_features(m1, m2)\n            return features[np.newaxis, :]\n        features_, _ = self.get_pair_mentions_features(m1, m2)\n        feat_l = [\n            features_[""00_SameSpeaker""],\n            features_[""01_AntMatchMentionSpeaker""],\n            features_[""02_MentionMatchSpeaker""],\n            features_[""03_HeadsAgree""],\n            features_[""04_ExactStringMatch""],\n            features_[""05_RelaxedStringMatch""],\n            features_[""06_SentenceDistance""],\n            features_[""07_MentionDistance""],\n            features_[""08_Overlapping""],\n        ]\n        return feat_l\n\n    def get_feature_array(self, doc_id, feature=None, compressed=True, debug=False):\n        """"""\n        Prepare feature array:\n            mentions_spans: (N, S)\n            mentions_words: (N, W)\n            mentions_features: (N, Fs)\n            mentions_labels: (N, 1)\n            mentions_pairs_start_index: (N, 1) index of beggining of pair list in pair_labels\n            mentions_pairs_length: (N, 1) number of pairs (i.e. nb of antecedents) for each mention\n            pairs_features: (P, Fp)\n            pairs_labels: (P, 1)\n            pairs_ant_idx: (P, 1) => indexes of antecedents mention for each pair (mention index in doc)\n        """"""\n        if not self.mentions:\n            if debug:\n                print(""No mention in this doc !"")\n            return {}\n        if debug:\n            print(""\xf0\x9f\x9b\x8e features matrices"")\n        mentions_spans = []\n        mentions_words = []\n        mentions_features = []\n        pairs_ant_idx = []\n        pairs_features = []\n        pairs_labels = []\n        mentions_labels = []\n        mentions_pairs_start = []\n        mentions_pairs_length = []\n        mentions_location = []\n        n_mentions = 0\n        total_pairs = 0\n        if debug:\n            print(""mentions"", self.mentions, str([m.gold_label for m in self.mentions]))\n        for mention_idx, antecedents_idx in list(\n            self.get_candidate_pairs(max_distance=None, max_distance_with_match=None)\n        ):\n            n_mentions += 1\n            mention = self.mentions[mention_idx]\n            mentions_spans.append(mention.spans_embeddings)\n            w_idx = mention_words_idx(self.embed_extractor, mention)\n            if w_idx is None:\n                print(""error in"", self.name, self.part, mention.utterance_index)\n            mentions_words.append(w_idx)\n            mentions_features.append(\n                self.get_single_mention_features_conll(mention, compressed)\n            )\n            mentions_location.append(\n                [\n                    mention.start,\n                    mention.end,\n                    mention.utterance_index,\n                    mention_idx,\n                    doc_id,\n                ]\n            )\n            ants = [self.mentions[ant_idx] for ant_idx in antecedents_idx]\n            no_antecedent = (\n                not any(ant.gold_label == mention.gold_label for ant in ants)\n                or mention.gold_label is None\n            )\n            if antecedents_idx:\n                pairs_ant_idx += [idx for idx in antecedents_idx]\n                pairs_features += [\n                    self.get_pair_mentions_features_conll(ant, mention, compressed)\n                    for ant in ants\n                ]\n                ant_labels = (\n                    [0 for ant in ants]\n                    if no_antecedent\n                    else [\n                        1 if ant.gold_label == mention.gold_label else 0 for ant in ants\n                    ]\n                )\n                pairs_labels += ant_labels\n            mentions_labels.append(1 if no_antecedent else 0)\n            mentions_pairs_start.append(total_pairs)\n            total_pairs += len(ants)\n            mentions_pairs_length.append(len(ants))\n\n        out_dict = {\n            FEATURES_NAMES[0]: mentions_features,\n            FEATURES_NAMES[1]: mentions_labels,\n            FEATURES_NAMES[2]: mentions_pairs_length,\n            FEATURES_NAMES[3]: mentions_pairs_start,\n            FEATURES_NAMES[4]: mentions_spans,\n            FEATURES_NAMES[5]: mentions_words,\n            FEATURES_NAMES[6]: pairs_ant_idx if pairs_ant_idx else None,\n            FEATURES_NAMES[7]: pairs_features if pairs_features else None,\n            FEATURES_NAMES[8]: pairs_labels if pairs_labels else None,\n            FEATURES_NAMES[9]: [mentions_location],\n            FEATURES_NAMES[10]: [self.conll_tokens],\n            FEATURES_NAMES[11]: [self.conll_lookup],\n            FEATURES_NAMES[12]: [\n                {\n                    ""name"": self.name,\n                    ""part"": self.part,\n                    ""utterances"": list(str(u) for u in self.utterances),\n                    ""mentions"": list(str(m) for m in self.mentions),\n                }\n            ],\n        }\n        if debug:\n            print(""\xf0\x9f\x9a\x98 Summary"")\n            for k, v in out_dict.items():\n                print(k, len(v))\n        return n_mentions, total_pairs, out_dict\n\n\n###################\n### ConllCorpus #####\nclass ConllCorpus(object):\n    def __init__(\n        self,\n        n_jobs=4,\n        embed_path=PACKAGE_DIRECTORY + ""/weights/"",\n        gold_mentions=False,\n        blacklist=False,\n    ):\n        self.n_jobs = n_jobs\n        self.features = {}\n        self.utts_text = []\n        self.utts_tokens = []\n        self.utts_corefs = []\n        self.utts_speakers = []\n        self.utts_doc_idx = []\n        self.docs_names = []\n        self.docs = []\n        if embed_path is not None:\n            self.embed_extractor = EmbeddingExtractor(embed_path)\n        self.trainable_embed = []\n        self.trainable_voc = []\n        self.gold_mentions = gold_mentions\n        self.blacklist = blacklist\n\n    def check_words_in_embeddings_voc(self, embedding, tuned=True, debug=False):\n        print(""\xf0\x9f\x8c\x8b Checking if words are in embedding voc"")\n        if tuned:\n            embed_voc = embedding.tun_idx\n        else:\n            embed_voc = embedding.stat_idx\n        missing_words = []\n        missing_words_sents = []\n        missing_words_doc = []\n        for doc in self.docs:\n            # if debug: print(""Checking doc"", doc.name, doc.part)\n            for sent in doc.utterances:\n                # if debug: print(sent.text)\n                for word in sent:\n                    w = embedding.normalize_word(word)\n                    # if debug: print(w)\n                    if w not in embed_voc:\n                        missing_words.append(w)\n                        missing_words_sents.append(sent.text)\n                        missing_words_doc.append(doc.name + doc.part)\n                        if debug:\n                            out_str = (\n                                ""No matching tokens in tuned voc for ""\n                                + w\n                                + "" in sentence ""\n                                + sent.text\n                                + "" in doc ""\n                                + doc.name\n                                + doc.part\n                            )\n                            print(out_str)\n        return missing_words, missing_words_sents, missing_words_doc\n\n    def test_sentences_words(self, save_file, debug=False):\n        print(""\xf0\x9f\x8c\x8b Saving sentence list"")\n        with io.open(save_file, ""w"", encoding=""utf-8"") as f:\n            if debug:\n                print(""Sentences saved in"", save_file)\n            for doc in self.docs:\n                out_str = ""#begin document ("" + doc.name + ""); part "" + doc.part + ""\\n""\n                f.write(out_str)\n                for sent in doc.utterances:\n                    f.write(sent.text + ""\\n"")\n                out_str = ""#end document\\n\\n""\n                f.write(out_str)\n\n    def save_sentences(self, save_file, debug=False):\n        print(""\xf0\x9f\x8c\x8b Saving sentence list"")\n        with io.open(save_file, ""w"", encoding=""utf-8"") as f:\n            if debug:\n                print(""Sentences saved in"", save_file)\n            for doc in self.docs:\n                out_str = ""#begin document ("" + doc.name + ""); part "" + doc.part + ""\\n""\n                f.write(out_str)\n                for sent in doc.utterances:\n                    f.write(sent.text + ""\\n"")\n                out_str = ""#end document\\n\\n""\n                f.write(out_str)\n\n    def build_key_file(self, data_path, key_file, debug=False):\n        print(""\xf0\x9f\x8c\x8b Building key file from corpus"")\n        print(""Saving in"", key_file)\n        # Create a pool of processes. By default, one is created for each CPU in your machine.\n        with io.open(key_file, ""w"", encoding=""utf-8"") as kf:\n            if debug:\n                print(""Key file saved in"", key_file)\n            for dirpath, _, filenames in os.walk(data_path):\n                print(""In"", dirpath)\n                file_list = [\n                    os.path.join(dirpath, f)\n                    for f in filenames\n                    if f.endswith("".v4_auto_conll"") or f.endswith("".v4_gold_conll"")\n                ]\n                cleaned_file_list = []\n                for f in file_list:\n                    fn = f.split(""."")\n                    if fn[1] == ""v4_auto_conll"":\n                        gold = fn[0] + ""."" + ""v4_gold_conll""\n                        if gold not in file_list:\n                            cleaned_file_list.append(f)\n                    else:\n                        cleaned_file_list.append(f)\n                # self.load_file(file_list[0])\n                doc_list = parallel_process(cleaned_file_list, read_file)\n                for doc in doc_list:\n                    kf.write(doc)\n\n    def list_undetected_mentions(self, data_path, save_file, debug=True):\n        self.read_corpus(data_path)\n        print(""\xf0\x9f\x8c\x8b Listing undetected mentions"")\n        with io.open(save_file, ""w"", encoding=""utf-8"") as out_file:\n            for doc in tqdm(self.docs):\n                for name, part, utt_i, utt, coref in doc.missed_gold:\n                    out_str = name + ""\\t"" + part + ""\\t"" + utt_i + \'\\t""\' + utt + \'""\\n\'\n                    out_str += coref + ""\\n""\n                    out_file.write(out_str)\n                    if debug:\n                        print(out_str)\n\n    def read_corpus(self, data_path, model=None, debug=False):\n        print(""\xf0\x9f\x8c\x8b Reading files"")\n        for dirpath, _, filenames in os.walk(data_path):\n            print(""In"", dirpath, os.path.abspath(dirpath))\n            file_list = [\n                os.path.join(dirpath, f)\n                for f in filenames\n                if f.endswith("".v4_auto_conll"") or f.endswith("".v4_gold_conll"")\n            ]\n            cleaned_file_list = []\n            for f in file_list:\n                fn = f.split(""."")\n                if fn[1] == ""v4_auto_conll"":\n                    gold = fn[0] + ""."" + ""v4_gold_conll""\n                    if gold not in file_list:\n                        cleaned_file_list.append(f)\n                else:\n                    cleaned_file_list.append(f)\n            doc_list = parallel_process(cleaned_file_list, load_file)\n            for docs in doc_list:  # executor.map(self.load_file, cleaned_file_list):\n                for (\n                    utts_text,\n                    utt_tokens,\n                    utts_corefs,\n                    utts_speakers,\n                    name,\n                    part,\n                ) in docs:\n                    if debug:\n                        print(""Imported"", name)\n                        print(""utts_text"", utts_text)\n                        print(""utt_tokens"", utt_tokens)\n                        print(""utts_corefs"", utts_corefs)\n                        print(""utts_speakers"", utts_speakers)\n                        print(""name, part"", name, part)\n                    self.utts_text += utts_text\n                    self.utts_tokens += utt_tokens\n                    self.utts_corefs += utts_corefs\n                    self.utts_speakers += utts_speakers\n                    self.utts_doc_idx += [len(self.docs_names)] * len(utts_text)\n                    self.docs_names.append((name, part))\n        print(""utts_text size"", len(self.utts_text))\n        print(""utts_tokens size"", len(self.utts_tokens))\n        print(""utts_corefs size"", len(self.utts_corefs))\n        print(""utts_speakers size"", len(self.utts_speakers))\n        print(""utts_doc_idx size"", len(self.utts_doc_idx))\n        print(""\xf0\x9f\x8c\x8b Building docs"")\n        for name, part in self.docs_names:\n            self.docs.append(\n                ConllDoc(\n                    name=name,\n                    part=part,\n                    nlp=None,\n                    blacklist=self.blacklist,\n                    consider_speakers=True,\n                    embedding_extractor=self.embed_extractor,\n                    conll=CONLL_GENRES[name[:2]],\n                )\n            )\n        print(""\xf0\x9f\x8c\x8b Loading spacy model"")\n\n        if model is None:\n            model_options = [""en_core_web_lg"", ""en_core_web_md"", ""en_core_web_sm"", ""en""]\n            for model_option in model_options:\n                if not model:\n                    try:\n                        spacy.info(model_option)\n                        model = model_option\n                        print(""Loading model"", model_option)\n                    except:\n                        print(""Could not detect model"", model_option)\n            if not model:\n                print(""Could not detect any suitable English model"")\n                return\n        else:\n            spacy.info(model)\n            print(""Loading model"", model)\n        nlp = spacy.load(model)\n        print(\n            ""\xf0\x9f\x8c\x8b Parsing utterances and filling docs with use_gold_mentions=""\n            + (str(bool(self.gold_mentions)))\n        )\n        doc_iter = (s for s in self.utts_text)\n        for utt_tuple in tqdm(\n            zip(\n                nlp.pipe(doc_iter),\n                self.utts_tokens,\n                self.utts_corefs,\n                self.utts_speakers,\n                self.utts_doc_idx,\n            )\n        ):\n            spacy_tokens, conll_tokens, corefs, speaker, doc_id = utt_tuple\n            if debug:\n                print(unicode_(self.docs_names[doc_id]), ""-"", spacy_tokens)\n            doc = spacy_tokens\n            if debug:\n                out_str = (\n                    ""utterance ""\n                    + unicode_(doc)\n                    + "" corefs ""\n                    + unicode_(corefs)\n                    + "" speaker ""\n                    + unicode_(speaker)\n                    + ""doc_id""\n                    + unicode_(doc_id)\n                )\n                print(out_str.encode(""utf-8""))\n            self.docs[doc_id].add_conll_utterance(\n                doc, conll_tokens, corefs, speaker, use_gold_mentions=self.gold_mentions\n            )\n\n    def build_and_gather_multiple_arrays(self, save_path):\n        print(f""\xf0\x9f\x8c\x8b Extracting mentions features with {self.n_jobs} job(s)"")\n        parallel_process(self.docs, set_feats, n_jobs=self.n_jobs)\n\n        print(f""\xf0\x9f\x8c\x8b Building and gathering array with {self.n_jobs} job(s)"")\n        arr = [{""doc"": doc, ""i"": i} for i, doc in enumerate(self.docs)]\n        arrays_dicts = parallel_process(\n            arr, get_feats, use_kwargs=True, n_jobs=self.n_jobs\n        )\n        gathering_dict = dict((feat, None) for feat in FEATURES_NAMES)\n        n_mentions_list = []\n        pairs_ant_index = 0\n        pairs_start_index = 0\n        for npaidx in tqdm(range(len(arrays_dicts))):\n            try:\n                n, p, arrays_dict = arrays_dicts[npaidx]\n            except:\n                # empty array dict, cannot extract the dict values for this doc\n                continue\n\n            for f in FEATURES_NAMES:\n                if gathering_dict[f] is None:\n                    gathering_dict[f] = arrays_dict[f]\n                else:\n                    if f == FEATURES_NAMES[6]:\n                        array = [a + pairs_ant_index for a in arrays_dict[f]]\n                    elif f == FEATURES_NAMES[3]:\n                        array = [a + pairs_start_index for a in arrays_dict[f]]\n                    else:\n                        array = arrays_dict[f]\n                    gathering_dict[f] += array\n            pairs_ant_index += n\n            pairs_start_index += p\n            n_mentions_list.append(n)\n\n        for feature in FEATURES_NAMES[:9]:\n            feature_data = gathering_dict[feature]\n            if not feature_data:\n                print(""No data for"", feature)\n                continue\n            print(""Building numpy array for"", feature, ""length"", len(feature_data))\n            if feature != ""mentions_spans"":\n                array = np.array(feature_data)\n                if array.ndim == 1:\n                    array = np.expand_dims(array, axis=1)\n            else:\n                array = np.stack(feature_data)\n            # check_numpy_array(feature, array, n_mentions_list)\n            print(""Saving numpy"", feature, ""size"", array.shape)\n            np.save(save_path + feature, array)\n        for feature in FEATURES_NAMES[9:]:\n            feature_data = gathering_dict[feature]\n            if feature_data:\n                print(""Saving pickle"", feature, ""size"", len(feature_data))\n                with open(save_path + feature + "".bin"", ""wb"") as fp:\n                    pickle.dump(feature_data, fp)\n\n    def save_vocabulary(self, save_path, debug=False):\n        def _vocabulary_to_file(path, vocabulary):\n            print(""\xf0\x9f\x8c\x8b Saving vocabulary"")\n            with io.open(path, ""w"", encoding=""utf-8"") as f:\n                if debug:\n                    print(f""voc saved in {path}, length: {len(vocabulary)}"")\n                for w in tunable_voc:\n                    f.write(w + ""\\n"")\n\n        print(""\xf0\x9f\x8c\x8b Building tunable vocabulary matrix from static vocabulary"")\n        tunable_voc = self.embed_extractor.tun_voc\n        _vocabulary_to_file(\n            path=save_path + ""tuned_word_vocabulary.txt"", vocabulary=tunable_voc\n        )\n\n        static_voc = self.embed_extractor.stat_voc\n        _vocabulary_to_file(\n            path=save_path + ""static_word_vocabulary.txt"", vocabulary=static_voc\n        )\n\n        tuned_word_embeddings = np.vstack(\n            [self.embed_extractor.get_stat_word(w)[1] for w in tunable_voc]\n        )\n        print(""Saving tunable voc, size:"", tuned_word_embeddings.shape)\n        np.save(save_path + ""tuned_word_embeddings"", tuned_word_embeddings)\n\n        static_word_embeddings = np.vstack(\n            [self.embed_extractor.static_embeddings[w] for w in static_voc]\n        )\n        print(""Saving static voc, size:"", static_word_embeddings.shape)\n        np.save(save_path + ""static_word_embeddings"", static_word_embeddings)\n\n\nif __name__ == ""__main__"":\n    DIR_PATH = os.path.dirname(os.path.realpath(__file__))\n    parser = argparse.ArgumentParser(\n        description=""Training the neural coreference model""\n    )\n    parser.add_argument(\n        ""--function"",\n        type=str,\n        default=""all"",\n        help=\'Function (""all"", ""key"", ""parse"", ""find_undetected"")\',\n    )\n    parser.add_argument(\n        ""--path"", type=str, default=DIR_PATH + ""/data/"", help=""Path to the dataset""\n    )\n    parser.add_argument(\n        ""--key"", type=str, help=""Path to an optional key file for scoring""\n    )\n    parser.add_argument(\n        ""--n_jobs"", type=int, default=1, help=""Number of parallel jobs (default 1)""\n    )\n    parser.add_argument(\n        ""--gold_mentions"",\n        type=int,\n        default=0,\n        help=""Use gold mentions (1) or not (0, default)"",\n    )\n    parser.add_argument(\n        ""--blacklist"", type=int, default=0, help=""Use blacklist (1) or not (0, default)""\n    )\n    parser.add_argument(""--spacy_model"", type=str, default=None, help=""model name"")\n    args = parser.parse_args()\n    if args.key is None:\n        args.key = args.path + ""/key.txt""\n    CORPUS = ConllCorpus(\n        n_jobs=args.n_jobs, gold_mentions=args.gold_mentions, blacklist=args.blacklist\n    )\n    if args.function == ""parse"" or args.function == ""all"":\n        SAVE_DIR = args.path + ""/numpy/""\n        if not os.path.exists(SAVE_DIR):\n            os.makedirs(SAVE_DIR)\n        else:\n            if os.listdir(SAVE_DIR):\n                print(""There are already data in"", SAVE_DIR)\n                print(""Erasing"")\n                for file in os.listdir(SAVE_DIR):\n                    print(file)\n                    os.remove(SAVE_DIR + file)\n        start_time = time.time()\n        CORPUS.read_corpus(args.path, model=args.spacy_model)\n        print(""=> read_corpus time elapsed"", time.time() - start_time)\n        if not CORPUS.docs:\n            print(""Could not parse any valid docs"")\n        else:\n            start_time2 = time.time()\n            CORPUS.build_and_gather_multiple_arrays(SAVE_DIR)\n            print(\n                ""=> build_and_gather_multiple_arrays time elapsed"",\n                time.time() - start_time2,\n            )\n            start_time2 = time.time()\n            CORPUS.save_vocabulary(SAVE_DIR)\n            print(""=> save_vocabulary time elapsed"", time.time() - start_time2)\n            print(""=> total time elapsed"", time.time() - start_time)\n    if args.function == ""key"" or args.function == ""all"":\n        CORPUS.build_key_file(args.path, args.key)\n    if args.function == ""find_undetected"":\n        CORPUS.list_undetected_mentions(\n            args.path, args.path + ""/undetected_mentions.txt""\n        )\n'"
neuralcoref/train/dataset.py,34,"b'""""""Conll training algorithm""""""\n\nimport os\nimport io\nimport numpy as np\n\nimport torch\nimport torch.utils.data\n\nfrom torch.utils.data.sampler import Sampler\nfrom torch.utils.data import Dataset\n\nfrom neuralcoref.train.utils import (\n    encode_distance,\n    BATCH_SIZE_PATH,\n    SIZE_FP,\n    SIZE_FP_COMPRESSED,\n    SIZE_FS,\n    SIZE_FS_COMPRESSED,\n    SIZE_GENRE,\n    SIZE_PAIR_IN,\n    SIZE_SINGLE_IN,\n)\nfrom neuralcoref.train.conllparser import FEATURES_NAMES\n\n\ndef load_embeddings_from_file(name):\n    print(""loading"", name + ""_embeddings.npy"")\n    embed = torch.from_numpy(np.load(name + ""_embeddings.npy"")).float()\n    print(embed.size())\n    print(""loading"", name + ""_vocabulary.txt"")\n    with io.open(name + ""_vocabulary.txt"", ""r"", encoding=""utf-8"") as f:\n        voc = [line.strip() for line in f]\n    return embed, voc\n\n\nclass _DictionaryDataLoader(object):\n    def __init__(self, dict_object, order):\n        self.dict_object = dict_object\n        self.order = order\n\n    def __len__(self):\n        return len(self.dict_object[self.order[0]])\n\n    def __getitem__(self, idx):\n        if isinstance(idx, slice):\n            data = []\n            for i in range(\n                idx.start, idx.stop, idx.step if idx.step is not None else 1\n            ):\n                temp_data = []\n                for key in self.order:\n                    temp_data.append(self.dict_object[key][i])\n                data.append(temp_data)\n\n        else:\n            data = []\n            for key in self.order:\n                data.append(self.dict_object[key][idx])\n\n        return data\n\n\nclass NCDataset(Dataset):\n    def __init__(self, data_path, params, no_targets=False):\n        print(""\xf0\x9f\x8f\x9d Loading Dataset at"", data_path)\n        self.costs = params.costs\n        self.no_targets = no_targets\n        # Load files\n        datas = {}\n        if not os.listdir(data_path):\n            raise ValueError(""Empty data_path"")\n        numpy_files_found = False\n        print(""Reading "", end="""")\n        for file_name in os.listdir(data_path):\n            if not "".npy"" in file_name:\n                continue\n            numpy_files_found = True\n            print(file_name, end="", "")\n            datas[file_name.split(""."")[0]] = np.load(\n                data_path + file_name, mmap_mode=""r"" if params.lazy else None\n            )\n        if not numpy_files_found:\n            raise ValueError(f""Can\'t find numpy files in {data_path}"")\n\n        # Gather arrays in two lists of tuples for mention and pairs\n        if not params.lazy:\n            self.mentions = list(\n                zip(\n                    *(\n                        arr\n                        for key, arr in sorted(datas.items())\n                        if key.startswith(""mentions"")\n                    )\n                )\n            )\n            self.pairs = list(\n                zip(\n                    *(\n                        arr\n                        for key, arr in sorted(datas.items())\n                        if key.startswith(""pairs"")\n                    )\n                )\n            )\n        else:\n            self.mentions = _DictionaryDataLoader(\n                datas,\n                order=(\n                    ""mentions_features"",\n                    ""mentions_labels"",\n                    ""mentions_pairs_length"",\n                    ""mentions_pairs_start_index"",\n                    ""mentions_spans"",\n                    ""mentions_words"",\n                ),\n            )\n            self.pairs = _DictionaryDataLoader(\n                datas, order=(""pairs_ant_index"", ""pairs_features"", ""pairs_labels"")\n            )\n\n        self.mentions_pair_length = datas[FEATURES_NAMES[2]]\n        assert [arr.shape[0] for arr in self.mentions[0]] == [\n            6,\n            1,\n            1,\n            1,\n            250,\n            8,\n        ]  # Cf order of FEATURES_NAMES in conllparser.py\n        assert [arr.shape[0] for arr in self.pairs[0]] == [\n            1,\n            9,\n            1,\n        ]  # Cf order of FEATURES_NAMES in conllparser.py\n\n    def __len__(self):\n        return len(self.mentions)\n\n    def __getitem__(self, mention_idx, debug=False):\n        """"""\n        Return:\n            Definitions:\n                P is the number of antecedent per mention (number of pairs for the mention)\n                S = 250 is the size of the span vector (averaged word embeddings)\n                W = 8 is the number of words in a mention (tuned embeddings)\n                Fp = 70 is the number of features for a pair of mention\n                Fs = 24 is the number of features of a single mention\n\n            if there are some pairs:\n                inputs = (spans, words, features, ant_spans, ant_words, ana_spans, ana_words, pairs_features)\n                targets = (labels, costs, true_ants, false_ants)\n            else:\n                inputs = (spans, words, features)\n                targets = (labels, costs, true_ants)\n\n            inputs: Tuple of\n                spans => (S,)\n                words => (W,)\n                features => (Fs,)\n                + if there are potential antecedents (P > 0):\n                    ant_spans => (P, S) or nothing if no pairs\n                    ant_words => (P, W) or nothing if no pairs\n                    ana_spans => (P, S) or nothing if no pairs\n                    ana_words => (P, W) or nothing if no pairs\n                    pair_features => (P, Fp) or nothing if no pairs\n\n            targets: Tuple of\n                labels => (P+1,)\n                costs => (P+1,)\n                true_ant => (P+1,)\n                + if there are potential antecedents (P > 0):\n                    false_ant => (P+1,)\n\n        """"""\n        features_raw, label, pairs_length, pairs_start_index, spans, words = self.mentions[\n            mention_idx\n        ]\n        pairs_start_index = pairs_start_index.item()\n        pairs_length = pairs_length.item()\n\n        # Build features array (float) from raw features (int)\n        assert features_raw.shape[0] == SIZE_FS_COMPRESSED\n        features = np.zeros((SIZE_FS,))\n        features[features_raw[0]] = 1\n        features[4:15] = encode_distance(features_raw[1])\n        features[15] = features_raw[2].astype(float) / features_raw[3].astype(float)\n        features[16] = features_raw[4]\n        features[features_raw[5] + 17] = 1\n\n        if pairs_length == 0:\n            spans = torch.from_numpy(spans).float()\n            words = torch.from_numpy(words)\n            features = torch.from_numpy(features).float()\n            inputs = (spans, words, features)\n            if self.no_targets:\n                return inputs\n            true_ant = torch.zeros(1).long()  # zeros = indices of true ant\n            costs = torch.from_numpy((1 - label) * self.costs[""FN""]).float()\n            label = torch.from_numpy(label).float()\n            targets = (label, costs, true_ant)\n            if debug:\n                print(""inputs shapes: "", [a.size() for a in inputs])\n                print(""targets shapes: "", [a.size() for a in targets])\n            return inputs, targets\n\n        start = pairs_start_index\n        end = pairs_start_index + pairs_length\n        pairs = self.pairs[start:end]\n        assert len(pairs) == pairs_length\n        assert (\n            len(pairs[0]) == 3\n        )  # pair[i] = (pairs_ant_index, pairs_features, pairs_labels)\n        pairs_ant_index, pairs_features_raw, pairs_labels = list(zip(*pairs))\n\n        pairs_features_raw = np.stack(pairs_features_raw)\n        pairs_labels = np.squeeze(np.stack(pairs_labels), axis=1)\n\n        # Build pair features array (float) from raw features (int)\n        assert pairs_features_raw[0, :].shape[0] == SIZE_FP_COMPRESSED\n        pairs_features = np.zeros((len(pairs_ant_index), SIZE_FP))\n        pairs_features[:, 0:6] = pairs_features_raw[:, 0:6]\n        pairs_features[:, 6:17] = encode_distance(pairs_features_raw[:, 6])\n        pairs_features[:, 17:28] = encode_distance(pairs_features_raw[:, 7])\n        pairs_features[:, 28] = pairs_features_raw[:, 8]\n        # prepare antecent features\n        ant_features_raw = np.concatenate(\n            [self.mentions[idx.item()][0][np.newaxis, :] for idx in pairs_ant_index]\n        )\n        ant_features = np.zeros((pairs_length, SIZE_FS - SIZE_GENRE))\n        ant_features[:, ant_features_raw[:, 0]] = 1\n        ant_features[:, 4:15] = encode_distance(ant_features_raw[:, 1])\n        ant_features[:, 15] = ant_features_raw[:, 2].astype(float) / ant_features_raw[\n            :, 3\n        ].astype(float)\n        ant_features[:, 16] = ant_features_raw[:, 4]\n        pairs_features[:, 29:46] = ant_features\n        # Here we keep the genre\n        ana_features = np.tile(features, (pairs_length, 1))\n        pairs_features[:, 46:] = ana_features\n\n        ant_spans = np.concatenate(\n            [self.mentions[idx.item()][4][np.newaxis, :] for idx in pairs_ant_index]\n        )\n        ant_words = np.concatenate(\n            [self.mentions[idx.item()][5][np.newaxis, :] for idx in pairs_ant_index]\n        )\n        ana_spans = np.tile(spans, (pairs_length, 1))\n        ana_words = np.tile(words, (pairs_length, 1))\n        ant_spans = torch.from_numpy(ant_spans).float()\n        ant_words = torch.from_numpy(ant_words)\n        ana_spans = torch.from_numpy(ana_spans).float()\n        ana_words = torch.from_numpy(ana_words)\n        pairs_features = torch.from_numpy(pairs_features).float()\n\n        labels_stack = np.concatenate((pairs_labels, label), axis=0)\n        assert labels_stack.shape == (pairs_length + 1,)\n        labels = torch.from_numpy(labels_stack).float()\n\n        spans = torch.from_numpy(spans).float()\n        words = torch.from_numpy(words)\n        features = torch.from_numpy(features).float()\n\n        inputs = (\n            spans,\n            words,\n            features,\n            ant_spans,\n            ant_words,\n            ana_spans,\n            ana_words,\n            pairs_features,\n        )\n\n        if self.no_targets:\n            return inputs\n\n        if label == 0:\n            costs = np.concatenate(\n                (self.costs[""WL""] * (1 - pairs_labels), [self.costs[""FN""]])\n            )  # Inverse labels: 1=>0, 0=>1\n        else:\n            costs = np.concatenate((self.costs[""FL""] * np.ones_like(pairs_labels), [0]))\n        assert costs.shape == (pairs_length + 1,)\n        costs = torch.from_numpy(costs).float()\n\n        true_ants_unpad = np.flatnonzero(labels_stack)\n        if len(true_ants_unpad) == 0:\n            raise ValueError(""Error: no True antecedent for mention"")\n        true_ants = np.pad(\n            true_ants_unpad, (0, len(pairs_labels) + 1 - len(true_ants_unpad)), ""edge""\n        )\n        assert true_ants.shape == (pairs_length + 1,)\n        true_ants = torch.from_numpy(true_ants).long()\n\n        false_ants_unpad = np.flatnonzero(1 - labels_stack)\n        assert len(false_ants_unpad) != 0\n        false_ants = np.pad(\n            false_ants_unpad, (0, len(pairs_labels) + 1 - len(false_ants_unpad)), ""edge""\n        )\n        assert false_ants.shape == (pairs_length + 1,)\n        false_ants = torch.from_numpy(false_ants).long()\n\n        targets = (labels, costs, true_ants, false_ants)\n        if debug:\n            print(""Mention"", mention_idx)\n            print(""inputs shapes: "", [a.size() for a in inputs])\n            print(""targets shapes: "", [a.size() for a in targets])\n        return inputs, targets\n\n\nclass NCBatchSampler(Sampler):\n    """"""A Batch sampler to group mentions in batches with close number of pairs to be padded together\n    """"""\n\n    def __init__(\n        self, mentions_pairs_length, batchsize=600, shuffle=False, debug=False\n    ):\n        """""" Create and feed batches of mentions having close number of antecedents\n            The batch are padded and collated by the padder_collate function\n\n        # Arguments:\n            mentions_pairs_length array of shape (N, 1): list/array of the number of pairs for each mention\n            batchsize: Number of pairs of each batch will be capped at this\n        """"""\n        self.shuffle = shuffle\n        num_mentions = len(mentions_pairs_length)\n        mentions_lengths = np.concatenate(\n            [\n                mentions_pairs_length,\n                np.arange(0, num_mentions, 1, dtype=int)[:, np.newaxis],\n            ],\n            axis=1,\n        )\n        sorted_lengths = mentions_lengths[mentions_lengths[:, 0].argsort()]\n        print(""Preparing batches \xf0\x9f\x93\x9a"")\n\n        self.batches = []\n        self.batches_pairs = []\n        self.batches_size = []\n        batch = []\n        n_pairs = []\n        num = 0\n        for length, mention_idx in sorted_lengths:\n            if num > batchsize or (\n                num == len(batch) and length != 0\n            ):  # We keep the no_pairs batches pure\n                if debug:\n                    print(\n                        ""Added batch number"",\n                        len(self.batches),\n                        ""with"",\n                        len(batch),\n                        ""mentions and"",\n                        num,\n                        ""pairs"",\n                    )\n                self.batches.append(batch)\n                self.batches_size.append(\n                    num\n                )  # We don\'t count the max 7 additional mentions that are repeated\n                self.batches_pairs.append(n_pairs)\n\n                # Start a new batch\n                batch = [mention_idx]\n                n_pairs = [length]\n                num = (\n                    length + 1\n                )  # +1 since we also have the single mention to add to the number of pairs\n            else:\n                num += length + 1\n                batch.append(mention_idx)\n                n_pairs.append(length)\n\n        # Complete and store the last batch\n        if debug:\n            print(\n                ""Added batch number"",\n                len(self.batches),\n                ""with"",\n                len(batch),\n                ""mentions and"",\n                num,\n                ""pairs"",\n            )\n        self.batches.append(batch)\n        self.batches_size.append(num)\n        self.batches_pairs.append(n_pairs)\n        self.n_pairs = sum(sum(p) for p in self.batches_pairs)\n        self.n_mentions = sum(len(b) for b in self.batches)\n        self.n_batches = len(self.batches)\n        self.pairs_per_batch = float(self.n_pairs) / self.n_batches\n        self.mentions_per_batch = float(self.n_mentions) / self.n_batches\n        print(\n            ""Dataset has:"",\n            self.n_batches,\n            ""batches,"",\n            self.n_mentions,\n            ""mentions,"",\n            self.n_pairs,\n            ""pairs"",\n        )\n\n    def get_batch_info(self):\n        return self.batches, self.batches_pairs\n\n    def save_batch_sizes(self, save_file=BATCH_SIZE_PATH, debug=False):\n        print(""\xf0\x9f\x8c\x8b Saving sizes of batches"")\n        with io.open(save_file, ""w"", encoding=""utf-8"") as f:\n            if debug:\n                print(""Batch sizes saved in"", save_file)\n            for batch, size in zip(self.batches, self.batches_size):\n                out_str = str(len(batch)) + ""\\t"" + str(size) + ""\\n""\n                f.write(out_str)\n\n    def __iter__(self):\n        if self.shuffle:\n            np.random.shuffle(self.batches)\n        for batch in self.batches:\n            yield batch\n\n    def __len__(self):\n        return self.n_batches\n\n\ndef padder_collate(batch, debug=False):\n    """""" Puts each data field into a tensor with outer dimension batch size\n        Pad variable length input tensors and add a weight tensor to the target\n    """"""\n    transposed_inputs = tuple(zip(*batch))\n    if len(transposed_inputs) == 2:\n        inputs, targets = transposed_inputs\n        transposed_inputs = tuple(zip(*inputs))\n        transposed_targets = tuple(zip(*targets))\n    else:\n        transposed_targets = None\n\n    max_pairs = (\n        max(len(t) for t in transposed_inputs[3]) if len(transposed_inputs) == 8 else 0\n    )  # Get max nb of pairs (batch are sorted by nb of pairs)\n    if max_pairs > 0:\n        out_inputs = []\n        out_targets = []\n        for t_inp in transposed_inputs:\n            if len(t_inp[0].shape) == 2:\n                out_inputs.append(\n                    torch.stack(\n                        [\n                            torch.cat([t, t.new(max_pairs - len(t), len(t[0])).zero_()])\n                            if len(t) != max_pairs\n                            else t\n                            for t in t_inp\n                        ],\n                        0,\n                    )\n                )\n            else:\n                out_inputs.append(torch.stack(t_inp, 0))\n        if transposed_targets is not None:\n            for i, t_targ in enumerate(\n                transposed_targets\n            ):  # 0:labels, 1:costs, 2:true_ants, 3:false_ants\n                if i == 2 or i == 3:\n                    if debug:\n                        print(""collate before"", t_targ)\n                    # shift the antecedent index associated to single anaphores (last)\n                    t_targ = tuple(\n                        t.masked_fill_(torch.eq(t, len(t) - 1), max_pairs)\n                        for t in t_targ\n                    )\n                    if debug:\n                        print(""collate after"", t_targ)\n                out_targets.append(\n                    torch.stack(\n                        [\n                            torch.cat(\n                                [\n                                    t[:-1] if len(t) > 2 else t.new(1).fill_(t[0]),\n                                    t.new(max_pairs + 1 - len(t)).fill_(t[0]),\n                                    t.new(1).fill_(t[-1]),\n                                ]\n                            )\n                            if len(t) != max_pairs + 1\n                            else t\n                            for t in t_targ\n                        ],\n                        0,\n                    )\n                )\n\n            t_costs = transposed_targets[\n                1\n            ]  # We build the weights from the costs to have a float Tensor\n            out_targets.append(\n                torch.stack(\n                    [\n                        torch.cat(\n                            [\n                                t.new(len(t) - 1).fill_(1),\n                                t.new(max_pairs + 1 - len(t)).zero_(),\n                                t.new(1).fill_(1),\n                            ]\n                        )\n                        if len(t) != max_pairs + 1\n                        else t.new(max_pairs + 1).fill_(1)\n                        for t in t_costs\n                    ],\n                    0,\n                )\n            )\n        else:\n            # Remark this mask is the inverse of the weights in the above target (used for evaluation masking)\n            t_base = transposed_inputs[3]\n            out_targets = torch.stack(\n                [\n                    torch.cat(\n                        [\n                            t.new(len(t) - 1).zero_().bool(),\n                            t.new(max_pairs + 1 - len(t)).fill_(1).bool(),\n                            t.new(1).zero_().bool(),\n                        ]\n                    )\n                    if len(t) != max_pairs + 1\n                    else t.new(max_pairs + 1).zero_().bool()\n                    for t in t_base\n                ],\n                0,\n            )\n    else:\n        out_inputs = [torch.stack(t_inp, 0) for t_inp in transposed_inputs]\n        if transposed_targets is not None:\n            out_targets = [torch.stack(t_targ, 0) for t_targ in transposed_targets]\n            out_targets.append(out_targets[1].new(len(out_targets[1]), 1).fill_(1))\n        else:\n            out_targets = out_inputs[0].new(len(out_inputs[0]), 1).zero_().bool()\n    return (out_inputs, out_targets)\n'"
neuralcoref/train/document.py,0,"b'""""""data models and pre-processing for the coref algorithm""""""\n\nimport re\nimport io\nfrom six import string_types, integer_types\nfrom spacy.tokens import Span, Token\n\nfrom neuralcoref.train.compat import unicode_\nfrom neuralcoref.train.utils import encode_distance, parallel_process\n\ntry:\n    from itertools import izip_longest as zip_longest\nexcept ImportError:  # will be 3.x series\n    from itertools import zip_longest\n\nimport spacy\nimport numpy as np\n\n#########################\n####### UTILITIES #######\n#########################\n\nNO_COREF_LIST = [""i"", ""me"", ""my"", ""you"", ""your""]\n\nMENTION_TYPE = {""PRONOMINAL"": 0, ""NOMINAL"": 1, ""PROPER"": 2, ""LIST"": 3}\nMENTION_LABEL = {0: ""PRONOMINAL"", 1: ""NOMINAL"", 2: ""PROPER"", 3: ""LIST""}\n\nPROPERS_TAGS = [""NN"", ""NNS"", ""NNP"", ""NNPS""]\nACCEPTED_ENTS = [\n    ""PERSON"",\n    ""NORP"",\n    ""FACILITY"",\n    ""ORG"",\n    ""GPE"",\n    ""LOC"",\n    ""PRODUCT"",\n    ""EVENT"",\n    ""WORK_OF_ART"",\n    ""LANGUAGE"",\n]\nWHITESPACE_PATTERN = r""\\s+|_+""\nUNKNOWN_WORD = ""*UNK*""\nMISSING_WORD = ""<missing>""\nMAX_ITER = 100\n\n#########################\n## MENTION EXTRACTION ###\n#########################\n\n\ndef extract_mentions_spans(doc, blacklist, debug=False):\n    """"""\n    Extract potential mentions from a spacy parsed Doc\n    """"""\n    if debug:\n        print(""===== doc ====:"", doc)\n    for c in doc:\n        if debug:\n            print(\n                ""\xf0\x9f\x9a\xa7 span search:"",\n                c,\n                ""head:"",\n                c.head,\n                ""tag:"",\n                c.tag_,\n                ""pos:"",\n                c.pos_,\n                ""dep:"",\n                c.dep_,\n            )\n    # Named entities\n    mentions_spans = list(ent for ent in doc.ents if ent.label_ in ACCEPTED_ENTS)\n\n    if debug:\n        print(""==-- ents:"", list(((ent, ent.label_) for ent in mentions_spans)))\n    for spans in parallel_process(\n        [{""doc"": doc, ""span"": sent, ""blacklist"": blacklist} for sent in doc.sents],\n        _extract_from_sent,\n        use_kwargs=True,\n        front_num=0,\n    ):\n        mentions_spans = mentions_spans + spans\n    spans_set = set()\n    cleaned_mentions_spans = []\n    for spans in mentions_spans:\n        if spans.end > spans.start and (spans.start, spans.end) not in spans_set:\n            cleaned_mentions_spans.append(spans)\n            spans_set.add((spans.start, spans.end))\n\n    return cleaned_mentions_spans\n\n\ndef _extract_from_sent(doc, span, blacklist=True, debug=False):\n    """"""\n    Extract Pronouns and Noun phrases mentions from a spacy Span\n    """"""\n    keep_tags = re.compile(r""N.*|PRP.*|DT|IN"")\n    leave_dep = [""det"", ""compound"", ""appos""]\n    keep_dep = [""nsubj"", ""dobj"", ""iobj"", ""pobj""]\n    nsubj_or_dep = [""nsubj"", ""dep""]\n    conj_or_prep = [""conj"", ""prep""]\n    remove_pos = [""CCONJ"", ""INTJ"", ""ADP""]\n    lower_not_end = [""\'s"", "","", ""."", ""!"", ""?"", "":"", "";""]\n\n    # Utility to remove bad endings\n    def cleanup_endings(left, right, token):\n        minchild_idx = min(left + [token.i]) if left else token.i\n        maxchild_idx = max(right + [token.i]) if right else token.i\n        # Clean up endings and begginging\n        while maxchild_idx >= minchild_idx and (\n            doc[maxchild_idx].pos_ in remove_pos\n            or doc[maxchild_idx].lower_ in lower_not_end\n        ):\n            if debug:\n                print(\n                    ""Removing last token"",\n                    doc[maxchild_idx].lower_,\n                    doc[maxchild_idx].tag_,\n                )\n            maxchild_idx -= (\n                1\n            )  # We don\'t want mentions finishing with \'s or conjunctions/punctuation\n        while minchild_idx <= maxchild_idx and (\n            doc[minchild_idx].pos_ in remove_pos\n            or doc[minchild_idx].lower_ in lower_not_end\n        ):\n            if debug:\n                print(\n                    ""Removing first token"",\n                    doc[minchild_idx].lower_,\n                    doc[minchild_idx].tag_,\n                )\n            minchild_idx += (\n                1\n            )  # We don\'t want mentions starting with \'s or conjunctions/punctuation\n        return minchild_idx, maxchild_idx + 1\n\n    mentions_spans = []\n    for token in span:\n        if debug:\n            print(\n                ""\xf0\x9f\x9a\x80 tok:"",\n                token,\n                ""tok.tag_:"",\n                token.tag_,\n                ""tok.pos_:"",\n                token.pos_,\n                ""tok.dep_:"",\n                token.dep_,\n            )\n\n        if blacklist and token.lower_ in NO_COREF_LIST:\n            if debug:\n                print(""token in no_coref_list"")\n            continue\n        if (\n            not keep_tags.match(token.tag_) or token.dep_ in leave_dep\n        ) and not token.dep_ in keep_dep:\n            if debug:\n                print(""not pronoun or no right dependency"")\n            continue\n\n        # pronoun\n        if re.match(r""PRP.*"", token.tag_):\n            if debug:\n                print(""PRP"")\n            endIdx = token.i + 1\n\n            span = doc[token.i : endIdx]\n            if debug:\n                print(""==-- PRP store:"", span)\n            mentions_spans.append(span)\n\n            # when pronoun is a part of conjunction (e.g., you and I)\n            if token.n_rights > 0 or token.n_lefts > 0:\n                span = doc[token.left_edge.i : token.right_edge.i + 1]\n                if debug:\n                    print(""==-- in conj store:"", span)\n                mentions_spans.append(span)\n            continue\n\n        # Add NP mention\n        if debug:\n            print(""NP or IN:"", token.lower_)\n            if token.tag_ == ""IN"":\n                print(""IN tag"")\n        # Take care of \'s\n        if token.lower_ == ""\'s"":\n            if debug:\n                print(""\'s detected"")\n            h = token.head\n            j = 0\n            while h.head.i != h.i and j < MAX_ITER:\n                if debug:\n                    print(""token head:"", h, h.dep_, ""head:"", h.head)\n                    print(id(h.head), id(h))\n                if h.dep_ == ""nsubj"":\n                    minchild_idx = min(\n                        (\n                            c.left_edge.i\n                            for c in doc\n                            if c.head.i == h.head.i and c.dep_ in nsubj_or_dep\n                        ),\n                        default=token.i,\n                    )\n                    maxchild_idx = max(\n                        (\n                            c.right_edge.i\n                            for c in doc\n                            if c.head.i == h.head.i and c.dep_ in nsubj_or_dep\n                        ),\n                        default=token.i,\n                    )\n                    if debug:\n                        print(""\'s\', i1:"", doc[minchild_idx], "" i2:"", doc[maxchild_idx])\n                    span = doc[minchild_idx : maxchild_idx + 1]\n                    if debug:\n                        print(""==-- \'s\' store:"", span)\n                    mentions_spans.append(span)\n                    break\n                h = h.head\n                j += 1\n            assert j != MAX_ITER\n            continue\n\n        # clean up\n        for c in doc:\n            if debug and c.head.i == token.i:\n                print(""\xf0\x9f\x9a\xa7 token in span:"", c, ""- head & dep:"", c.head, c.dep_)\n        left = list(c.left_edge.i for c in doc if c.head.i == token.i)\n        right = list(c.right_edge.i for c in doc if c.head.i == token.i)\n        if (\n            token.tag_ == ""IN""\n            and token.dep_ == ""mark""\n            and len(left) == 0\n            and len(right) == 0\n        ):\n            left = list(c.left_edge.i for c in doc if c.head.i == token.head.i)\n            right = list(c.right_edge.i for c in doc if c.head.i == token.head.i)\n        if debug:\n            print(""left side:"", left)\n            print(""right side:"", right)\n            minchild_idx = min(left) if left else token.i\n            maxchild_idx = max(right) if right else token.i\n            print(""full span:"", doc[minchild_idx : maxchild_idx + 1])\n        start, end = cleanup_endings(left, right, token)\n        if start == end:\n            continue\n        if doc[start].lower_ == ""\'s"":\n            continue  # we probably already have stored this mention\n        span = doc[start:end]\n        if debug:\n            print(""cleaned endings span:"", doc[start:end])\n            print(""==-- full span store:"", span)\n        mentions_spans.append(span)\n        if debug and token.tag_ == ""IN"":\n            print(""IN tag"")\n        if any(tok.dep_ in conj_or_prep for tok in span):\n            if debug:\n                print(""Conjunction found, storing first element separately"")\n            for c in doc:\n                if c.head.i == token.i and c.dep_ not in conj_or_prep:\n                    if debug:\n                        print(""left no conj:"", c, ""dep & edge:"", c.dep_, c.left_edge)\n                    if debug:\n                        print(""right no conj:"", c, ""dep & edge:"", c.dep_, c.right_edge)\n            left_no_conj = list(\n                c.left_edge.i\n                for c in doc\n                if c.head.i == token.i and c.dep_ not in conj_or_prep\n            )\n            right_no_conj = list(\n                c.right_edge.i\n                for c in doc\n                if c.head.i == token.i and c.dep_ not in conj_or_prep\n            )\n            if debug:\n                print(""left side no conj:"", [doc[i] for i in left_no_conj])\n            if debug:\n                print(""right side no conj:"", [doc[i] for i in right_no_conj])\n            start, end = cleanup_endings(left_no_conj, right_no_conj, token)\n            if start == end:\n                continue\n            span = doc[start:end]\n            if debug:\n                print(""==-- full span store:"", span)\n            mentions_spans.append(span)\n    if debug:\n        print(""mentions_spans inside"", mentions_spans)\n    return mentions_spans\n\n\n#########################\n####### CLASSES #########\n\n\nclass Mention(spacy.tokens.Span):\n    """"""\n    A mention (possible anaphor) inherite from spacy Span class with additional informations\n    """"""\n\n    def __new__(\n        cls,\n        span,\n        mention_index,\n        utterance_index,\n        utterance_start_sent,\n        speaker=None,\n        gold_label=None,\n        *args,\n        **kwargs,\n    ):\n        # We need to override __new__ see http://cython.readthedocs.io/en/latest/src/userguide/special_methods.html\n        obj = spacy.tokens.Span.__new__(\n            cls, span.doc, span.start, span.end, *args, **kwargs\n        )\n        return obj\n\n    def __init__(\n        self,\n        span,\n        mention_index,\n        utterance_index,\n        utterances_start_sent,\n        speaker=None,\n        gold_label=None,\n    ):\n        """"""\n        Arguments:\n            span (spaCy Span): the spaCy span from which creating the Mention object\n            mention_index (int): index of the Mention in the Document\n            utterance_index (int): index of the utterance of the Mention in the Document\n            utterances_start_sent (int): index of the first sentence of the utterance of the Mention in the Document\n                (an utterance can comprise several sentences)\n            speaker (Speaker): the speaker of the mention\n            gold_label (anything): a gold label associated to the Mention (for training)\n        """"""\n        self.index = mention_index\n        self.utterance_index = utterance_index\n        self.utterances_sent = utterances_start_sent + self._get_doc_sent_number()\n        self.speaker = speaker\n        self.gold_label = gold_label\n        self.spans_embeddings = None\n        self.words_embeddings = None\n        self.features = None\n\n        self.features_ = None\n        self.spans_embeddings_ = None\n        self.words_embeddings_ = None\n\n        self.mention_type = self._get_type()\n        self.propers = set(self.content_words)\n        self.entity_label = self._get_entity_label()\n        self.in_entities = self._get_in_entities()\n\n    def _get_entity_label(self):\n        """""" Label of a detected named entity the Mention is nested in if any""""""\n        for ent in self.doc.ents:\n            if ent.start <= self.start and self.end <= ent.end:\n                return ent.label\n        return None\n\n    def _get_in_entities(self):\n        """""" Is the Mention nested in a detected named entity""""""\n        return self.entity_label is not None\n\n    def _get_type(self):\n        """""" Find the type of the Span """"""\n        conj = [""CC"", "",""]\n        prp = [""PRP"", ""PRP$""]\n        proper = [""NNP"", ""NNPS""]\n        if any(t.tag_ in conj and t.ent_type_ not in ACCEPTED_ENTS for t in self):\n            mention_type = MENTION_TYPE[""LIST""]\n        elif self.root.tag_ in prp:\n            mention_type = MENTION_TYPE[""PRONOMINAL""]\n        elif self.root.ent_type_ in ACCEPTED_ENTS or self.root.tag_ in proper:\n            mention_type = MENTION_TYPE[""PROPER""]\n        else:\n            mention_type = MENTION_TYPE[""NOMINAL""]\n        return mention_type\n\n    def _get_doc_sent_number(self):\n        """""" Index of the sentence of the Mention in the current utterance""""""\n        for i, s in enumerate(self.doc.sents):\n            if s == self.sent:\n                return i\n        return None\n\n    @property\n    def content_words(self):\n        """""" Returns an iterator of nouns/proper nouns in the Mention """"""\n        return (tok.lower_ for tok in self if tok.tag_ in PROPERS_TAGS)\n\n    @property\n    def embedding(self):\n        return np.concatenate([self.spans_embeddings, self.words_embeddings], axis=0)\n\n    def heads_agree(self, mention2):\n        """""" Does the root of the Mention match the root of another Mention/Span""""""\n        # we allow same-type NEs to not match perfectly,\n        # but rather one could be included in the other, e.g., ""George"" -> ""George Bush""\n        if (\n            self.in_entities\n            and mention2.in_entities\n            and self.entity_label == mention2.entity_label\n            and (\n                self.root.lower_ in mention2.lower_\n                or mention2.root.lower_ in self.lower_\n            )\n        ):\n            return True\n        return self.root.lower_ == mention2.root.lower_\n\n    def exact_match(self, mention2):\n        """""" Does the Mention lowercase text matches another Mention/Span lowercase text""""""\n        return self.lower_ == mention2.lower_\n\n    def relaxed_match(self, mention2):\n        """""" Does the nouns/proper nous in the Mention match another Mention/Span nouns/propers""""""\n        return not self.propers.isdisjoint(mention2.propers)\n\n    def speaker_match_mention(self, mention2):\n        # To take care of sentences like \'Larry said, ""San Francisco is a city.""\': (id(Larry), id(San Francisco))\n        # if document.speakerPairs.contains(new Pair<>(mention.mentionID, ant.mentionID)):\n        #    return True\n        if self.speaker is not None:\n            return self.speaker.speaker_matches_mention(mention2, strict_match=False)\n        return False\n\n\nclass Speaker(object):\n    """"""\n    A speaker with its names, list of mentions and matching test functions\n    """"""\n\n    def __init__(self, speaker_id, speaker_names=None):\n        self.mentions = []\n        self.speaker_id = speaker_id\n        if speaker_names is None:\n            self.speaker_names = [unicode_(speaker_id)]\n        elif isinstance(speaker_names, string_types):\n            self.speaker_names = [speaker_names]\n        elif len(speaker_names) > 1:\n            self.speaker_names = speaker_names\n        else:\n            self.speaker_names = unicode_(speaker_names)\n        self.speaker_tokens = [\n            tok.lower()\n            for s in self.speaker_names\n            for tok in re.split(WHITESPACE_PATTERN, s)\n        ]\n\n    def __str__(self):\n        return f""{self.speaker_id} <names> {self.speaker_names}""\n\n    def add_mention(self, mention):\n        """""" Add a Mention of the Speaker""""""\n        self.mentions.append(mention)\n\n    def contain_mention(self, mention):\n        """""" Does the Speaker contains a Mention""""""\n        return mention in self.mentions\n\n    def contain_string(self, string):\n        """""" Does the Speaker names contains a string""""""\n        return any(\n            re.sub(WHITESPACE_PATTERN, """", string.lower())\n            == re.sub(WHITESPACE_PATTERN, """", s.lower())\n            for s in self.speaker_names\n        )\n\n    def contain_token(self, token):\n        """""" Does the Speaker names contains a token (word)""""""\n        return any(token.lower() == tok.lower() for tok in self.speaker_tokens)\n\n    def speaker_matches_mention(self, mention, strict_match=False):\n        """""" Does a Mention matches the speaker names""""""\n        # Got info about this speaker\n        if self.contain_mention(mention):\n            return True\n        if strict_match:\n            if self.contain_string(mention):\n                self.mentions.append(mention)\n                return True\n        else:\n            # test each token in the speaker names\n            if not mention.root.tag_.startswith(""NNP""):\n                return False\n            if self.contain_token(mention.root.lower_):\n                self.mentions.append(mention)\n                return True\n        return False\n\n\nclass EmbeddingExtractor:\n    """"""\n    Compute words embedding features for mentions\n    """"""\n\n    def __init__(self, pretrained_model_path):\n        _, self.static_embeddings, self.stat_idx, self.stat_voc = self.load_embeddings_from_file(\n            pretrained_model_path + ""static_word""\n        )\n        _, self.tuned_embeddings, self.tun_idx, self.tun_voc = self.load_embeddings_from_file(\n            pretrained_model_path + ""tuned_word""\n        )\n        self.fallback = self.static_embeddings.get(UNKNOWN_WORD)\n\n        self.shape = self.static_embeddings[UNKNOWN_WORD].shape\n        shape2 = self.tuned_embeddings[UNKNOWN_WORD].shape\n        assert self.shape == shape2\n\n    @staticmethod\n    def load_embeddings_from_file(name):\n        print(""Loading embeddings from"", name)\n        embeddings = {}\n        voc_to_idx = {}\n        idx_to_voc = []\n        mat = np.load(name + ""_embeddings.npy"")\n        average_mean = np.average(mat, axis=0, weights=np.sum(mat, axis=1))\n        with io.open(name + ""_vocabulary.txt"", ""r"", encoding=""utf-8"") as f:\n            for i, line in enumerate(f):\n                embeddings[line.strip()] = mat[i, :]\n                voc_to_idx[line.strip()] = i\n                idx_to_voc.append(line.strip())\n        return average_mean, embeddings, voc_to_idx, idx_to_voc\n\n    @staticmethod\n    def normalize_word(w):\n        if w is None:\n            return MISSING_WORD\n        return re.sub(r""\\d"", ""0"", w.lower_)\n\n    def get_document_embedding(self, utterances_list):\n        """""" Embedding for the document """"""\n        #    We could also use this: embed_vector = np.copy(self.average_mean)#np.zeros(self.shape)\n        #    return embed_vector\n        embed_vector = np.zeros(self.shape)\n        for utt in utterances_list:\n            _, utt_embed = self.get_average_embedding(utt)\n            embed_vector += utt_embed\n        return embed_vector / max(len(utterances_list), 1)\n\n    def get_stat_word(self, word):\n        if word in self.static_embeddings:\n            return word, self.static_embeddings.get(word)\n        else:\n            return UNKNOWN_WORD, self.fallback\n\n    def get_word_embedding(self, word, static=False):\n        """""" Embedding for a single word (tuned if possible, otherwise static) """"""\n        norm_word = self.normalize_word(word)\n        if static:\n            return self.get_stat_word(norm_word)\n        else:\n            if norm_word in self.tuned_embeddings:\n                return norm_word, self.tuned_embeddings.get(norm_word)\n            else:\n                return self.get_stat_word(norm_word)\n\n    def get_word_in_sentence(self, word_idx, sentence):\n        """""" Embedding for a word in a sentence """"""\n        if word_idx < sentence.start or word_idx >= sentence.end:\n            return self.get_word_embedding(None)\n        return self.get_word_embedding(sentence.doc[word_idx])\n\n    def get_average_embedding(self, token_list):\n        """""" Embedding for a list of words """"""\n        embed_vector = np.zeros(\n            self.shape\n        )  # We could also use np.copy(self.average_mean)\n        word_list = []\n        for tok in token_list:\n            if tok.lower_ not in [""."", ""!"", ""?""]:\n                word, embed = self.get_word_embedding(tok, static=True)\n                embed_vector += embed\n                word_list.append(word)\n        return word_list, (embed_vector / max(len(word_list), 1))\n\n    def get_mention_embeddings(self, mention, doc_embedding):\n        """""" Get span (averaged) and word (single) embeddings of a mention """"""\n        st = mention.sent\n        mention_lefts = mention.doc[max(mention.start - 5, st.start) : mention.start]\n        mention_rights = mention.doc[mention.end : min(mention.end + 5, st.end)]\n        head = mention.root.head\n        spans = [\n            self.get_average_embedding(mention),\n            self.get_average_embedding(mention_lefts),\n            self.get_average_embedding(mention_rights),\n            self.get_average_embedding(st),\n            (unicode_(doc_embedding[0:8]) + ""..."", doc_embedding),\n        ]\n        words = [\n            self.get_word_embedding(mention.root),\n            self.get_word_embedding(mention[0]),\n            self.get_word_embedding(mention[-1]),\n            self.get_word_in_sentence(mention.start - 1, st),\n            self.get_word_in_sentence(mention.end, st),\n            self.get_word_in_sentence(mention.start - 2, st),\n            self.get_word_in_sentence(mention.end + 1, st),\n            self.get_word_embedding(head),\n        ]\n        spans_embeddings_ = {\n            ""00_Mention"": spans[0][0],\n            ""01_MentionLeft"": spans[1][0],\n            ""02_MentionRight"": spans[2][0],\n            ""03_Sentence"": spans[3][0],\n            ""04_Doc"": spans[4][0],\n        }\n        words_embeddings_ = {\n            ""00_MentionHead"": words[0][0],\n            ""01_MentionFirstWord"": words[1][0],\n            ""02_MentionLastWord"": words[2][0],\n            ""03_PreviousWord"": words[3][0],\n            ""04_NextWord"": words[4][0],\n            ""05_SecondPreviousWord"": words[5][0],\n            ""06_SecondNextWord"": words[6][0],\n            ""07_MentionRootHead"": words[7][0],\n        }\n        return (\n            spans_embeddings_,\n            words_embeddings_,\n            np.concatenate([em[1] for em in spans], axis=0),\n            np.concatenate([em[1] for em in words], axis=0),\n        )\n\n\nclass Document(object):\n    """"""\n    Main data class: encapsulate list of utterances, mentions and speakers\n    Process utterances to extract mentions and pre-compute mentions features\n    """"""\n\n    def __init__(\n        self,\n        nlp,\n        utterances=None,\n        utterances_speaker=None,\n        speakers_names=None,\n        blacklist=False,\n        consider_speakers=False,\n        model_path=None,\n        embedding_extractor=None,\n        conll=None,\n        debug=False,\n    ):\n        """"""\n        Arguments:\n            nlp (spaCy Language Class): A spaCy Language Class for processing the text input\n            utterances: utterance(s) to load already see self.add_utterances()\n            utterances_speaker: speaker(s) of utterance(s) to load already see self.add_utterances()\n            speakers_names: speaker(s) of utterance(s) to load already see self.add_utterances()\n            blacklist (boolean): use a list of term for which coreference is not preformed\n            consider_speakers (boolean): consider speakers informations\n            pretrained_model_path (string): Path to a folder with pretrained word embeddings\n            embedding_extractor (EmbeddingExtractor): Use a pre-loaded word embeddings extractor\n            conll (string): If training on coNLL data: identifier of the document type\n            debug (boolean): print debug informations\n        """"""\n        self.nlp = nlp\n        self.blacklist = blacklist\n        self.utterances = []\n        self.utterances_speaker = []\n        self.last_utterances_loaded = []\n        self.mentions = []\n        self.speakers = {}\n        self.n_sents = 0\n        self.debug = debug\n        self.consider_speakers = consider_speakers or conll is not None\n\n        self.genre_, self.genre = self.set_genre(conll)\n\n        if model_path is not None and embedding_extractor is None:\n            self.embed_extractor = EmbeddingExtractor(model_path)\n        elif embedding_extractor is not None:\n            self.embed_extractor = embedding_extractor\n        else:\n            self.embed_extractor = None\n\n        if utterances:\n            self.add_utterances(utterances, utterances_speaker, speakers_names)\n\n    def set_genre(self, conll):\n        if conll is not None:\n            genre = np.zeros((7,))\n            genre[conll] = 1\n        else:\n            genre = np.array(0, ndmin=1, copy=False)\n        return conll, genre\n\n    def __str__(self):\n        formatted = ""\\n "".join(\n            unicode_(i) + "" "" + unicode_(s)\n            for i, s in zip(self.utterances, self.utterances_speaker)\n        )\n        mentions = ""\\n "".join(\n            unicode_(i) + "" "" + unicode_(i.speaker) for i in self.mentions\n        )\n        return f""<utterances, speakers> \\n {formatted}\\n<mentions> \\n {mentions}""\n\n    def __len__(self):\n        """""" Return the number of mentions (not utterances) since it is what we really care about """"""\n        return len(self.mentions)\n\n    def __getitem__(self, key):\n        """""" Return a specific mention (not utterance) """"""\n        return self.mentions[key]\n\n    def __iter__(self):\n        """""" Iterate over mentions (not utterances) """"""\n        for mention in self.mentions:\n            yield mention\n\n    #######################################\n    ###### UTERANCE LOADING FUNCTIONS #####\n    #######################################\n\n    def set_utterances(self, utterances, utterances_speaker=None, speakers_names=None):\n        self.utterances = []\n        self.utterances_speaker = []\n        self.mentions = []\n        self.speakers = {}\n        self.n_sents = 0\n        if utterances:\n            self.add_utterances(utterances, utterances_speaker, speakers_names)\n\n    def add_utterances(self, utterances, utterances_speaker=None, speakers_names=None):\n        """"""\n        Add utterances to the utterance list and build mention list for these utterances\n\n        Arg:\n            utterances : iterator or list of string corresponding to successive utterances\n            utterances_speaker : iterator or list of speaker id for each utterance.\n                If not provided, assume two speakers speaking alternatively.\n                if utterances and utterances_speaker are not of the same length padded with None\n            speakers_names : dictionnary of list of acceptable speaker names for each speaker id\n        Return:\n            List of indexes of added utterances in the docs\n        """"""\n        if self.debug:\n            print(""Adding utterances"", utterances)\n        if isinstance(utterances, string_types):\n            utterances = [utterances]\n        if utterances_speaker is None:\n            if self.debug:\n                print(""No utterance speaker indication"")\n            a = -1\n            if self.utterances_speaker and isinstance(\n                self.utterances_speaker[-1].speaker_id, integer_types\n            ):\n                a = self.utterances_speaker[-1].speaker_id\n            utterances_speaker = ((i + a + 1) % 2 for i in range(len(utterances)))\n        utterances_index = []\n        utt_start = len(self.utterances)\n        docs = list(self.nlp.pipe(utterances))\n        m_spans = list(\n            extract_mentions_spans(doc, blacklist=self.blacklist) for doc in docs\n        )\n        for utt_index, (doc, m_spans, speaker_id) in enumerate(\n            zip_longest(docs, m_spans, utterances_speaker)\n        ):\n            if speaker_id not in self.speakers:\n                speaker_name = (\n                    speakers_names.get(speaker_id, None) if speakers_names else None\n                )\n                self.speakers[speaker_id] = Speaker(speaker_id, speaker_name)\n            self._process_mentions(\n                m_spans, utt_start + utt_index, self.n_sents, self.speakers[speaker_id]\n            )\n            utterances_index.append(utt_start + utt_index)\n            self.utterances.append(doc)\n            self.utterances_speaker.append(self.speakers[speaker_id])\n            self.n_sents += len(list(doc.sents))\n\n        self.set_mentions_features()\n        self.last_utterances_loaded = utterances_index\n\n    ###################################\n    ## FEATURES MENTIONS EXTRACTION ###\n    ###################################\n\n    def _process_mentions(self, mentions_spans, utterance_index, n_sents, speaker):\n        """"""\n        Process mentions in a spacy doc (an utterance)\n        """"""\n        processed_spans = sorted(\n            (m for m in mentions_spans), key=lambda m: (m.root.i, m.start)\n        )\n        n_mentions = len(self.mentions)\n        for mention_index, span in enumerate(processed_spans):\n            self.mentions.append(\n                Mention(\n                    span, mention_index + n_mentions, utterance_index, n_sents, speaker\n                )\n            )\n\n    def set_mentions_features(self):\n        """"""\n        Compute features for the extracted mentions\n        """"""\n        doc_embedding = (\n            self.embed_extractor.get_document_embedding(self.utterances)\n            if self.embed_extractor is not None\n            else None\n        )\n        for mention in self.mentions:\n            one_hot_type = np.zeros((4,))\n            one_hot_type[mention.mention_type] = 1\n            features_ = {\n                ""01_MentionType"": mention.mention_type,\n                ""02_MentionLength"": len(mention) - 1,\n                ""03_MentionNormLocation"": (mention.index) / len(self.mentions),\n                ""04_IsMentionNested"": 1\n                if any(\n                    (\n                        m is not mention\n                        and m.utterances_sent == mention.utterances_sent\n                        and m.start <= mention.start\n                        and mention.end <= m.end\n                    )\n                    for m in self.mentions\n                )\n                else 0,\n            }\n            features = np.concatenate(\n                [\n                    one_hot_type,\n                    encode_distance(features_[""02_MentionLength""]),\n                    np.array(features_[""03_MentionNormLocation""], ndmin=1, copy=False),\n                    np.array(features_[""04_IsMentionNested""], ndmin=1, copy=False),\n                ],\n                axis=0,\n            )\n            (\n                spans_embeddings_,\n                words_embeddings_,\n                spans_embeddings,\n                words_embeddings,\n            ) = self.embed_extractor.get_mention_embeddings(mention, doc_embedding)\n            mention.features_ = features_\n            mention.features = features\n            mention.spans_embeddings = spans_embeddings\n            mention.spans_embeddings_ = spans_embeddings_\n            mention.words_embeddings = words_embeddings\n            mention.words_embeddings_ = words_embeddings_\n\n    def get_single_mention_features(self, mention):\n        """""" Features for anaphoricity test (signle mention features + genre if conll)""""""\n        features_ = mention.features_\n        features_[""DocGenre""] = self.genre_\n        return (features_, np.concatenate([mention.features, self.genre], axis=0))\n\n    def get_pair_mentions_features(self, m1, m2):\n        """""" Features for pair of mentions (same speakers, speaker mentioned, string match)""""""\n        features_ = {\n            ""00_SameSpeaker"": 1\n            if self.consider_speakers and m1.speaker == m2.speaker\n            else 0,\n            ""01_AntMatchMentionSpeaker"": 1\n            if self.consider_speakers and m2.speaker_match_mention(m1)\n            else 0,\n            ""02_MentionMatchSpeaker"": 1\n            if self.consider_speakers and m1.speaker_match_mention(m2)\n            else 0,\n            ""03_HeadsAgree"": 1 if m1.heads_agree(m2) else 0,\n            ""04_ExactStringMatch"": 1 if m1.exact_match(m2) else 0,\n            ""05_RelaxedStringMatch"": 1 if m1.relaxed_match(m2) else 0,\n            ""06_SentenceDistance"": m2.utterances_sent - m1.utterances_sent,\n            ""07_MentionDistance"": m2.index - m1.index - 1,\n            ""08_Overlapping"": 1\n            if (m1.utterances_sent == m2.utterances_sent and m1.end > m2.start)\n            else 0,\n            ""09_M1Features"": m1.features_,\n            ""10_M2Features"": m2.features_,\n            ""11_DocGenre"": self.genre_,\n        }\n        pairwise_features = [\n            np.array(\n                [\n                    features_[""00_SameSpeaker""],\n                    features_[""01_AntMatchMentionSpeaker""],\n                    features_[""02_MentionMatchSpeaker""],\n                    features_[""03_HeadsAgree""],\n                    features_[""04_ExactStringMatch""],\n                    features_[""05_RelaxedStringMatch""],\n                ]\n            ),\n            encode_distance(features_[""06_SentenceDistance""]),\n            encode_distance(features_[""07_MentionDistance""]),\n            np.array(features_[""08_Overlapping""], ndmin=1),\n            m1.features,\n            m2.features,\n            self.genre,\n        ]\n        return (features_, np.concatenate(pairwise_features, axis=0))\n\n    ###################################\n    ###### ITERATOR OVER MENTIONS #####\n    ###################################\n\n    def get_candidate_mentions(self, last_utterances_added=False):\n        """"""\n        Return iterator over indexes of mentions in a list of utterances if specified\n        """"""\n        if last_utterances_added:\n            for i, mention in enumerate(self.mentions):\n                if self.debug:\n                    print(""\xf0\x9f\xa4\xa3"", i, mention, ""utterance index"", mention.utterance_index)\n                if mention.utterance_index in self.last_utterances_loaded:\n                    yield i\n        else:\n            iterator = range(len(self.mentions))\n            for i in iterator:\n                yield i\n\n    def get_candidate_pairs(\n        self, mentions=None, max_distance=50, max_distance_with_match=500, debug=False\n    ):\n        """"""\n        Yield tuples of mentions, dictionnary of candidate antecedents for the mention\n\n        Arg:\n            mentions: an iterator over mention indexes (as returned by get_candidate_mentions)\n            max_mention_distance : max distance between a mention and its antecedent\n            max_mention_distance_string_match : max distance between a mention and\n                its antecedent when there is a proper noun match\n        """"""\n        if mentions is None:\n            mentions = range(len(self.mentions))\n        if debug:\n            print(""get_candidate_pairs: mentions"", mentions)\n\n        if max_distance_with_match is not None:\n            word_to_mentions = {}\n            for i in mentions:\n                for tok in self.mentions[i].content_words:\n                    if not tok in word_to_mentions:\n                        word_to_mentions[tok] = [i]\n                    else:\n                        word_to_mentions[tok].append(i)\n\n        for i in mentions:\n            antecedents = (\n                set(range(i))\n                if max_distance is None\n                else set(range(max(0, i - max_distance), i))\n            )\n            if debug:\n                print(""antecedents"", antecedents)\n            if max_distance_with_match is not None:\n                for tok in self.mentions[i].content_words:\n                    with_string_match = word_to_mentions.get(tok, None)\n                    for match_idx in with_string_match:\n                        if match_idx < i and match_idx >= i - max_distance_with_match:\n                            antecedents.add(match_idx)\n            yield i, antecedents\n\n\ndef mention_detection_debug(sentence):\n    print(""\xf0\x9f\x8c\x8b Loading spacy model"")\n    try:\n        spacy.info(""en_core_web_sm"")\n        model = ""en_core_web_sm""\n    except IOError:\n        print(""No spacy 2 model detected, using spacy1 \'en\' model"")\n        spacy.info(""en"")\n        model = ""en""\n    nlp = spacy.load(model)\n    doc = nlp(sentence.decode(""utf-8""))\n    mentions = extract_mentions_spans(doc, blacklist=False, debug=True)\n    for mention in mentions:\n        print(mention)\n\n\nif __name__ == ""__main__"":\n    import sys\n\n    if len(sys.argv) > 1:\n        sent = sys.argv[1]\n        mention_detection_debug(sent)\n    else:\n        mention_detection_debug(""My sister has a dog. She loves him."")\n'"
neuralcoref/train/evaluator.py,2,"b'""""""Conll Evaluation - Scoring""""""\n\nimport os\nimport subprocess\nimport io\nimport pickle\n\nimport torch\nfrom torch.utils.data import DataLoader\n\nfrom neuralcoref.train.conllparser import FEATURES_NAMES\nfrom neuralcoref.train.dataset import NCBatchSampler, padder_collate\nfrom neuralcoref.train.compat import unicode_\n\nPACKAGE_DIRECTORY = os.path.dirname(os.path.abspath(__file__))\n\nOUT_PATH = os.path.join(PACKAGE_DIRECTORY, ""test_corefs.txt"")  # fernandes.txt"")#\nALL_MENTIONS_PATH = os.path.join(PACKAGE_DIRECTORY, ""test_mentions.txt"")\n# KEY_PATH = os.path.join(PACKAGE_DIRECTORY, ""conll-2012-test-test-key.txt"")\nSCORING_SCRIPT = os.path.join(PACKAGE_DIRECTORY, ""scorer_wrapper.pl"")\n\nMETRICS = [""muc"", ""bcub"", ""ceafm"", ""ceafe"", ""blanc""]\nCONLL_METRICS = [""muc"", ""bcub"", ""ceafe""]\n\n\nclass ConllEvaluator(object):\n    def __init__(self, model, dataset, test_data_path, test_key_file, embed_path, args):\n        """""" Evaluate the pytorch model that is currently being build\n            We take the embedding vocabulary currently being trained\n        """"""\n        self.test_key_file = test_key_file\n        self.cuda = args.cuda\n        self.model = model\n        batch_sampler = NCBatchSampler(\n            dataset.mentions_pair_length, batchsize=args.batchsize, shuffle=False\n        )\n        self.dataloader = DataLoader(\n            dataset,\n            collate_fn=padder_collate,\n            batch_sampler=batch_sampler,\n            num_workers=args.numworkers,\n            pin_memory=args.cuda,\n        )\n        self.mentions_idx, self.n_pairs = batch_sampler.get_batch_info()\n        self.load_meta(test_data_path)\n\n    def load_meta(self, test_data_path):\n        # Load meta files\n        datas = {}\n        if not os.listdir(test_data_path):\n            raise ValueError(""Empty test_data_path"")\n        bin_files_found = False\n        print(""Reading "", end="""")\n        for file_name in os.listdir(test_data_path):\n            if "".bin"" not in file_name:\n                continue\n            bin_files_found = True\n            print(file_name, end="", "")\n            with open(test_data_path + file_name, ""rb"") as f:\n                datas[file_name.split(""."")[0]] = pickle.load(f)\n        if not bin_files_found:\n            raise ValueError(f""Can\'t find bin files in {test_data_path}"")\n        print(""Done"")\n        self.m_loc = datas[FEATURES_NAMES[9]]\n        self.tokens = datas[FEATURES_NAMES[10]]\n        self.lookup = datas[FEATURES_NAMES[11]]\n        self.docs = datas[FEATURES_NAMES[12]]\n        self.flat_m_idx = list(\n            (doc_i, m_i) for doc_i, l in enumerate(self.m_loc) for m_i in range(len(l))\n        )\n\n    ###########################\n    #### CLUSTER FUNCTIONS ####\n    ###########################\n\n    def _prepare_clusters(self):\n        """"""\n        Clean up and prepare one cluster for each mention\n        """"""\n        self.mention_to_cluster = list(\n            list(range(len(doc_mentions))) for doc_mentions in self.m_loc\n        )\n        self.clusters = list(\n            dict((i, [i]) for i in doc_mentions)\n            for doc_mentions in self.mention_to_cluster\n        )\n\n    def _merge_coreference_clusters(self, ant_flat_idx, mention_flat_idx):\n        """"""\n        Merge two clusters together\n        """"""\n        doc_idx, ant_idx = self.flat_m_idx[ant_flat_idx]\n        doc_idx2, mention_idx = self.flat_m_idx[mention_flat_idx]\n        assert doc_idx2 == doc_idx\n        if (\n            self.mention_to_cluster[doc_idx][ant_idx]\n            == self.mention_to_cluster[doc_idx][mention_idx]\n        ):\n            return\n        remove_id = self.mention_to_cluster[doc_idx][ant_idx]\n        keep_id = self.mention_to_cluster[doc_idx][mention_idx]\n        for idx in self.clusters[doc_idx][remove_id]:\n            self.mention_to_cluster[doc_idx][idx] = keep_id\n            self.clusters[doc_idx][keep_id].append(idx)\n        del self.clusters[doc_idx][remove_id]\n\n    def remove_singletons_clusters(self, debug=False):\n        for doc_idx in range(len(self.docs)):\n            remove_id = []\n            kept = False\n            for key, mentions in self.clusters[doc_idx].items():\n                if len(mentions) == 1:\n                    remove_id.append(key)\n                    self.mention_to_cluster[doc_idx][key] = None\n                else:\n                    kept = True\n                    if debug:\n                        l = list(self.m_loc[doc_idx][m][3] for m in mentions)\n                        print(""Cluster found"", key)\n                        print(\n                            ""Corefs:"",\n                            ""|"".join(\n                                str(self.docs[doc_idx][""mentions""][m_idx])\n                                + "" (""\n                                + str(m_idx)\n                                + "")""\n                                for m_idx in l\n                            ),\n                        )\n            if not kept and debug:\n                print(""\xe2\x9d\x84\xef\xb8\x8f No coreference found"")\n            for rem in remove_id:\n                del self.clusters[doc_idx][rem]\n\n    def display_clusters(self, doc_idx=None):\n        """"""\n        Print clusters informations\n        """"""\n        doc_it = range(len(self.docs)) if doc_idx is None else [doc_idx]\n        for d_i in doc_it:\n            print(\n                ""Clusters in doc:"",\n                doc_it,\n                self.docs[d_i][""name""],\n                self.docs[d_i][""part""],\n            )\n            print(self.clusters[d_i])\n            for key, mentions in self.clusters[d_i].items():\n                l = list(self.m_loc[d_i][m][3] for m in mentions)\n                print(\n                    ""cluster"",\n                    key,\n                    ""("",\n                    "", "".join(self.docs[d_i][""mentions""][m_idx] for m_idx in l),\n                    "")"",\n                )\n\n    ########################\n    #### MAIN FUNCTIONS ####\n    ########################\n    def get_max_score(self, batch, debug=False):\n        inputs, mask = batch\n        if self.cuda:\n            inputs = tuple(i.cuda() for i in inputs)\n            mask = mask.cuda()\n        self.model.eval()\n        with torch.no_grad():\n            scores = self.model(inputs, concat_axis=1)\n            scores.masked_fill_(mask, -float(""Inf""))\n            _, max_idx = scores.max(\n                dim=1\n            )  # We may want to weight the single score with coref.greedyness\n        if debug:\n            print(""Max_idx"", max_idx)\n        return scores.cpu().numpy(), max_idx.cpu().numpy()\n\n    def test_model(self):\n        print(""\xf0\x9f\x8c\x8b Test evaluator / print all mentions"")\n        self.build_test_file(out_path=ALL_MENTIONS_PATH, print_all_mentions=True)\n        self.get_score(file_path=ALL_MENTIONS_PATH)\n\n    def build_test_file(\n        self,\n        out_path=OUT_PATH,\n        remove_singleton=True,\n        print_all_mentions=False,\n        debug=None,\n    ):\n        """""" Build a test file to supply to the coreference scoring perl script\n        """"""\n        print(""\xf0\x9f\x8c\x8b Building test file"")\n        self._prepare_clusters()\n        self.dataloader.dataset.no_targets = True\n        if not print_all_mentions:\n            print(""\xf0\x9f\x8c\x8b Build coreference clusters"")\n            for sample_batched, mentions_idx, n_pairs_l in zip(\n                self.dataloader, self.mentions_idx, self.n_pairs\n            ):\n                scores, max_i = self.get_max_score(sample_batched)\n                for m_idx, ind, n_pairs in zip(mentions_idx, max_i, n_pairs_l):\n                    if (\n                        ind < n_pairs\n                    ):  # the single score is not the highest, we have a match !\n                        prev_idx = m_idx - n_pairs + ind\n                        if debug is not None and (\n                            debug == -1 or debug == prev_idx or debug == m_idx\n                        ):\n                            m1_doc, m1_idx = self.flat_m_idx[m_idx]\n                            m1 = self.docs[m1_doc][""mentions""][m1_idx]\n                            m2_doc, m2_idx = self.flat_m_idx[prev_idx]\n                            m2 = self.docs[m2_doc][""mentions""][m2_idx]\n                            print(\n                                ""We have a match between:"",\n                                m1,\n                                ""("" + str(m1_idx) + "")"",\n                                ""and:"",\n                                m2,\n                                ""("" + str(m2_idx) + "")"",\n                            )\n                        self._merge_coreference_clusters(prev_idx, m_idx)\n            if remove_singleton:\n                self.remove_singletons_clusters()\n        self.dataloader.dataset.no_targets = False\n\n        print(""\xf0\x9f\x8c\x8b Construct test file"")\n        out_str = """"\n        for doc, d_tokens, d_lookup, d_m_loc, d_m_to_c in zip(\n            self.docs, self.tokens, self.lookup, self.m_loc, self.mention_to_cluster\n        ):\n            out_str += (\n                ""#begin document ("" + doc[""name""] + ""); part "" + doc[""part""] + ""\\n""\n            )\n            for utt_idx, (c_tokens, c_lookup) in enumerate(zip(d_tokens, d_lookup)):\n                for i, (token, lookup) in enumerate(zip(c_tokens, c_lookup)):\n                    out_coref = """"\n                    for m_str, mention, mention_cluster in zip(\n                        doc[""mentions""], d_m_loc, d_m_to_c\n                    ):\n                        m_start, m_end, m_utt, m_idx, m_doc = mention\n                        if mention_cluster is None:\n                            pass\n                        elif m_utt == utt_idx:\n                            if m_start in lookup:\n                                out_coref += ""|"" if out_coref else """"\n                                out_coref += ""("" + unicode_(mention_cluster)\n                                if (m_end - 1) in lookup:\n                                    out_coref += "")""\n                                else:\n                                    out_coref += """"\n                            elif (m_end - 1) in lookup:\n                                out_coref += ""|"" if out_coref else """"\n                                out_coref += unicode_(mention_cluster) + "")""\n                    out_line = (\n                        doc[""name""]\n                        + "" ""\n                        + doc[""part""]\n                        + "" ""\n                        + unicode_(i)\n                        + "" ""\n                        + token\n                        + "" ""\n                    )\n                    out_line += ""-"" if len(out_coref) == 0 else out_coref\n                    out_str += out_line + ""\\n""\n                out_str += ""\\n""\n            out_str += ""#end document\\n""\n\n        # Write test file\n        print(""Writing in"", out_path)\n        with io.open(out_path, ""w"", encoding=""utf-8"") as out_file:\n            out_file.write(out_str)\n\n    def get_score(self, file_path=OUT_PATH, debug=False):\n        """""" Call the coreference scoring perl script on the created test file\n        """"""\n        print(""\xf0\x9f\x8c\x8b Computing score"")\n        score = {}\n        ident = None\n        for metric_name in CONLL_METRICS:\n            if debug:\n                print(""Computing metric:"", metric_name)\n            try:\n                scorer_out = subprocess.check_output(\n                    [\n                        ""perl"",\n                        SCORING_SCRIPT,\n                        metric_name,\n                        self.test_key_file,\n                        file_path,\n                    ],\n                    stderr=subprocess.STDOUT,\n                    encoding=""utf-8"",\n                )\n            except subprocess.CalledProcessError as err:\n                print(""Error during the scoring"")\n                print(err)\n                print(err.output)\n                raise\n            if debug:\n                print(""scorer_out"", scorer_out)\n            value, ident = scorer_out.split(""\\n"")[-2], scorer_out.split(""\\n"")[-1]\n            if debug:\n                print(""value"", value, ""identification"", ident)\n            NR, DR, NP, DP = [float(x) for x in value.split("" "")]\n            ident_NR, ident_DR, ident_NP, ident_DP = [\n                float(x) for x in ident.split("" "")\n            ]\n            precision = NP / DP if DP else 0\n            recall = NR / DR if DR else 0\n            F1 = (\n                2 * precision * recall / (precision + recall)\n                if precision + recall > 0\n                else 0\n            )\n            ident_precision = ident_NP / ident_DP if ident_DP else 0\n            ident_recall = ident_NR / ident_DR if ident_DR else 0\n            ident_F1 = (\n                2 * ident_precision * ident_recall / (ident_precision + ident_recall)\n                if ident_precision + ident_recall > 0\n                else 0\n            )\n            score[metric_name] = (precision, recall, F1)\n            ident = (\n                ident_precision,\n                ident_recall,\n                ident_F1,\n                ident_NR,\n                ident_DR,\n                ident_NP,\n                ident_DP,\n            )\n        F1_conll = sum([score[metric][2] for metric in CONLL_METRICS]) / len(\n            CONLL_METRICS\n        )\n        print(\n            ""Mention identification recall"",\n            ident[1],\n            ""<= Detected mentions"",\n            ident[3],\n            ""True mentions"",\n            ident[4],\n        )\n        print(""Scores"", score)\n        print(""F1_conll"", F1_conll)\n        return score, F1_conll, ident\n'"
neuralcoref/train/learn.py,23,"b'""""""Conll training algorithm""""""\n\nimport os\nimport time\nimport argparse\nimport socket\nfrom datetime import datetime\nimport numpy as np\n\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\nfrom torch.optim import RMSprop\nfrom torch.utils.data import DataLoader\nfrom tensorboardX import SummaryWriter\n\nfrom neuralcoref.train.model import Model\nfrom neuralcoref.train.dataset import (\n    NCDataset,\n    NCBatchSampler,\n    load_embeddings_from_file,\n    padder_collate,\n    SIZE_PAIR_IN,\n    SIZE_SINGLE_IN,\n)\nfrom neuralcoref.train.utils import SIZE_EMBEDDING\nfrom neuralcoref.train.evaluator import ConllEvaluator\n\nPACKAGE_DIRECTORY = os.path.dirname(os.path.abspath(__file__))\nSTAGES = [""allpairs"", ""toppairs"", ""ranking""]\n\n\ndef clipped_sigmoid(inputs):\n    epsilon = 1.0e-7\n    return torch.sigmoid(inputs).clamp(epsilon, 1.0 - epsilon)\n\n\ndef get_all_pairs_loss(n):\n    def all_pair_loss(scores, targets):\n        """""" All pairs and single mentions probabilistic loss\n        """"""\n        labels = targets[0]\n        weights = targets[4].data if len(targets) == 5 else None\n        loss_op = nn.BCEWithLogitsLoss(weight=weights, reduction=""sum"")\n        loss = loss_op(scores, labels)\n        return loss / n\n\n    return all_pair_loss\n\n\ndef get_top_pair_loss(n):\n    def top_pair_loss(scores, targets, debug=False):\n        """""" Top pairs (best true and best mistaken) and single mention probabilistic loss\n        """"""\n        true_ants = targets[2]\n        false_ants = targets[3] if len(targets) == 5 else None\n        s_scores = clipped_sigmoid(scores)\n        true_pairs = torch.gather(s_scores, 1, true_ants)\n        top_true, top_true_arg = torch.log(true_pairs).max(\n            dim=1\n        )  # max(log(p)), p=sigmoid(s)\n        if debug:\n            print(""true_pairs"", true_pairs.data)\n            print(""top_true"", top_true.data)\n            print(""top_true_arg"", top_true_arg.data)\n        out_score = torch.sum(top_true).neg()\n        if (\n            false_ants is not None\n        ):  # We have no false antecedents when there are no pairs\n            false_pairs = torch.gather(s_scores, 1, false_ants)\n            top_false, _ = torch.log(1 - false_pairs).min(\n                dim=1\n            )  # min(log(1-p)), p=sigmoid(s)\n            out_score = out_score + torch.sum(top_false).neg()\n        return out_score / n\n\n    return top_pair_loss\n\n\ndef get_ranking_loss(n):\n    def ranking_loss(scores, targets):\n        """""" Slack-rescaled max margin loss\n        """"""\n        costs = targets[1]\n        true_ants = targets[2]\n        weights = targets[4] if len(targets) == 5 else None\n        true_ant_score = torch.gather(scores, 1, true_ants)\n        top_true, _ = true_ant_score.max(dim=1)\n        tmp_loss = scores.add(1).add(\n            top_true.unsqueeze(1).neg()\n        )  # 1 + scores - top_true\n        if weights is not None:\n            tmp_loss = tmp_loss.mul(weights)\n        tmp_loss = tmp_loss.mul(costs)\n        loss, _ = tmp_loss.max(dim=1)\n        out_score = torch.sum(loss)\n        return out_score / n\n\n    return ranking_loss\n\n\ndef decrease_lr(optim_func, factor=0.1, min_lrs=0, eps=0, verbose=True):\n    for i, param_group in enumerate(optim_func.param_groups):\n        old_lr = float(param_group[""lr""])\n        new_lr = max(old_lr * factor, min_lrs)\n        if old_lr - new_lr > eps:\n            param_group[""lr""] = new_lr\n            if verbose:\n                print(f""Reducing learning rate"" "" of group {i} to {new_lr:.4e}."")\n    return new_lr\n\n\ndef load_model(model, path):\n    print(""\xe2\x9b\x84\xef\xb8\x8f Reloading model from"", path)\n    model.load_state_dict(\n        torch.load(path)\n        if args.cuda\n        else torch.load(path, map_location=lambda storage, loc: storage)\n    )\n\n\ndef run_model(args):\n    print(\n        ""Training for"",\n        args.all_pairs_epoch,\n        args.top_pairs_epoch,\n        args.ranking_epoch,\n        ""epochs"",\n    )\n    # Tensorboard server\n    writer = SummaryWriter()\n\n    # Load datasets and embeddings\n    embed_path = args.weights if args.weights is not None else args.train\n    tensor_embeddings, voc = load_embeddings_from_file(embed_path + ""tuned_word"")\n    dataset = NCDataset(args.train, args)\n    eval_dataset = NCDataset(args.eval, args)\n    print(""Vocabulary:"", len(voc))\n\n    # Construct model\n    print(""\xf0\x9f\x8f\x9d Build model"")\n    model = Model(\n        len(voc),\n        SIZE_EMBEDDING,\n        args.h1,\n        args.h2,\n        args.h3,\n        SIZE_PAIR_IN,\n        SIZE_SINGLE_IN,\n    )\n    model.load_embeddings(tensor_embeddings)\n    if args.cuda:\n        model.cuda()\n    if args.weights is not None:\n        print(""\xf0\x9f\x8f\x9d Loading pre-trained weights"")\n        model.load_weights(args.weights)\n    if args.checkpoint_file is not None:\n        print(""\xe2\x9b\x84\xef\xb8\x8f Loading model from"", args.checkpoint_file)\n        model.load_state_dict(\n            torch.load(args.checkpoint_file)\n            if args.cuda\n            else torch.load(\n                args.checkpoint_file, map_location=lambda storage, loc: storage\n            )\n        )\n\n    print(""\xf0\x9f\x8f\x9d Loading conll evaluator"")\n    eval_evaluator = ConllEvaluator(\n        model, eval_dataset, args.eval, args.evalkey, embed_path, args\n    )\n    train_evaluator = ConllEvaluator(\n        model, dataset, args.train, args.trainkey, embed_path, args\n    )\n    print(""\xf0\x9f\x8f\x9d Testing evaluator and getting first eval score"")\n    eval_evaluator.test_model()\n    start_time = time.time()\n    eval_evaluator.build_test_file()\n    score, f1_conll, ident = eval_evaluator.get_score()\n    elapsed = time.time() - start_time\n    print(f""|| s/evaluation {elapsed:5.2f}"")\n    writer.add_scalar(""eval/"" + ""F1_conll"", f1_conll, 0)\n\n    # Preparing dataloader\n    print(""\xf0\x9f\x8f\x9d Preparing dataloader"")\n    print(\n        ""Dataloader parameters: batchsize"",\n        args.batchsize,\n        ""numworkers"",\n        args.numworkers,\n    )\n    batch_sampler = NCBatchSampler(\n        dataset.mentions_pair_length, shuffle=True, batchsize=args.batchsize\n    )\n    dataloader = DataLoader(\n        dataset,\n        collate_fn=padder_collate,\n        batch_sampler=batch_sampler,\n        num_workers=args.numworkers,\n        pin_memory=args.cuda,\n    )\n    mentions_idx, n_pairs = batch_sampler.get_batch_info()\n\n    print(""\xf0\x9f\x8f\x9d Start training"")\n    g_step = 0\n    start_from = (\n        args.startstep\n        if args.startstep is not None and args.startstage is not None\n        else 0\n    )\n\n    def run_epochs(\n        start_epoch, end_epoch, loss_func, optim_func, save_name, lr, g_step, debug=None\n    ):\n        best_model_path = args.save_path + ""best_model"" + save_name\n        start_time_all = time.time()\n        best_f1_conll = 0\n        lower_eval = 0\n        for epoch in range(start_epoch, end_epoch):\n            """""" Run an epoch """"""\n            print(f""\xf0\x9f\x9a\x98 {save_name} Epoch {epoch:d}"")\n            model.train()\n            start_time_log = time.time()\n            start_time_epoch = time.time()\n            epoch_loss = 0\n            for batch_i, (m_idx, n_pairs_l, batch) in enumerate(\n                zip(mentions_idx, n_pairs, dataloader)\n            ):\n                if debug is not None and (debug == -1 or debug in m_idx):\n                    l = list(dataset.flat_m_loc[m][2:] for m in m_idx)\n                    print(\n                        ""\xf0\x9f\x8f\x94 Batch"",\n                        batch_i,\n                        ""m_idx:"",\n                        ""|"".join(str(i) for i in m_idx),\n                        ""mentions:"",\n                        ""|"".join(dataset.docs[d][""mentions""][i] for u, i, d in l),\n                    )\n                    print(""Batch n_pairs:"", ""|"".join(str(p) for p in n_pairs_l))\n                inputs, targets = batch\n                inputs = tuple(Variable(inp, requires_grad=False) for inp in inputs)\n                targets = tuple(Variable(tar, requires_grad=False) for tar in targets)\n                if args.cuda:\n                    inputs = tuple(i.cuda() for i in inputs)\n                    targets = tuple(t.cuda() for t in targets)\n                scores = model(inputs)\n                if debug is not None and (debug == -1 or debug in m_idx):\n                    print(\n                        ""Scores:\\n""\n                        + ""\\n"".join(\n                            ""|"".join(str(s) for s in s_l)\n                            for s_l in scores.data.cpu().numpy()\n                        )\n                    )\n                    print(\n                        ""Labels:\\n""\n                        + ""\\n"".join(\n                            ""|"".join(str(s) for s in s_l)\n                            for s_l in targets[0].data.cpu().numpy()\n                        )\n                    )\n                loss = loss_func(scores, targets)\n                if debug is not None and (debug == -1 or debug in m_idx):\n                    print(""Loss"", loss.item())\n                # Zero gradients, perform a backward pass, and update the weights.\n                optim_func.zero_grad()\n                loss.backward()\n                epoch_loss += loss.item()\n                optim_func.step()\n                writer.add_scalar(""train/"" + save_name + ""_loss"", loss.item(), g_step)\n                writer.add_scalar(""meta/"" + ""lr"", lr, g_step)\n                writer.add_scalar(""meta/"" + ""stage"", STAGES.index(save_name), g_step)\n                g_step += 1\n                if batch_i % args.log_interval == 0 and batch_i > 0:\n                    elapsed = time.time() - start_time_log\n                    lr = optim_func.param_groups[0][""lr""]\n                    ea = elapsed * 1000 / args.log_interval\n                    li = loss.item()\n                    print(\n                        f""| epoch {epoch:3d} | {batch_i:5d}/{len(dataloader):5d} batches | ""\n                        f""lr {lr:.2e} | ms/batch {ea:5.2f} | ""\n                        f""loss {li:.2e}""\n                    )\n                    start_time_log = time.time()\n            elapsed_all = time.time() - start_time_all\n            elapsed_epoch = time.time() - start_time_epoch\n            ep = elapsed_epoch / 60\n            ea = (\n                elapsed_all\n                / 3600\n                * float(end_epoch - epoch)\n                / float(epoch - start_epoch + 1)\n            )\n            print(\n                f""|| min/epoch {ep:5.2f} | est. remaining time (h) {ea:5.2f} | loss {epoch_loss:.2e}""\n            )\n            writer.add_scalar(""epoch/"" + ""loss"", epoch_loss, g_step)\n            if epoch % args.conll_train_interval == 0:\n                start_time = time.time()\n                train_evaluator.build_test_file()\n                score, f1_conll, ident = train_evaluator.get_score()\n                elapsed = time.time() - start_time\n                ep = elapsed_epoch / 60\n                print(f""|| min/train evaluation {ep:5.2f} | F1_conll {f1_conll:5.2f}"")\n                writer.add_scalar(""epoch/"" + ""F1_conll"", f1_conll, g_step)\n            if epoch % args.conll_eval_interval == 0:\n                start_time = time.time()\n                eval_evaluator.build_test_file()\n                score, f1_conll, ident = eval_evaluator.get_score()\n                elapsed = time.time() - start_time\n                ep = elapsed_epoch / 60\n                print(f""|| min/evaluation {ep:5.2f}"")\n                writer.add_scalar(""eval/"" + ""F1_conll"", f1_conll, g_step)\n                g_step += 1\n                save_path = args.save_path + save_name + ""_"" + str(epoch)\n                torch.save(model.state_dict(), save_path)\n                if f1_conll > best_f1_conll:\n                    best_f1_conll = f1_conll\n                    torch.save(model.state_dict(), best_model_path)\n                    lower_eval = 0\n                elif args.on_eval_decrease != ""nothing"":\n                    print(""Evaluation metric decreases"")\n                    lower_eval += 1\n                    if lower_eval >= args.patience:\n                        if (\n                            args.on_eval_decrease == ""divide_lr""\n                            or args.on_eval_decrease == ""divide_then_next""\n                        ):\n                            print(""reload best model and decrease lr"")\n                            load_model(model, best_model_path)\n                            lr = decrease_lr(optim_func)\n                        if args.on_eval_decrease == ""next_stage"" or lr <= args.min_lr:\n                            print(""Switch to next stage"")\n                            break\n        # Save last step\n        start_time = time.time()\n        eval_evaluator.build_test_file()\n        score, f1_conll, ident = eval_evaluator.get_score()\n        elapsed = time.time() - start_time\n        ep = elapsed / 60\n        print(f""|| min/evaluation {ep:5.2f}"")\n        writer.add_scalar(""eval/"" + ""F1_conll"", f1_conll, g_step)\n        g_step += 1\n        save_path = args.save_path + save_name + ""_"" + str(epoch)\n        torch.save(model.state_dict(), save_path)\n        load_model(model, best_model_path)\n        return g_step\n\n    if args.startstage is None or args.startstage == ""allpairs"":\n        optimizer = RMSprop(\n            model.parameters(), lr=args.all_pairs_lr, weight_decay=args.all_pairs_l2\n        )\n        loss_func = get_all_pairs_loss(batch_sampler.pairs_per_batch)\n        g_step = run_epochs(\n            start_from,\n            args.all_pairs_epoch,\n            loss_func,\n            optimizer,\n            ""allpairs"",\n            args.all_pairs_lr,\n            g_step,\n        )\n        start_from = 0\n\n    if args.startstage is None or args.startstage in [""allpairs"", ""toppairs""]:\n        optimizer = RMSprop(\n            model.parameters(), lr=args.top_pairs_lr, weight_decay=args.top_pairs_l2\n        )\n        loss_func = get_top_pair_loss(10 * batch_sampler.mentions_per_batch)\n        g_step = run_epochs(\n            start_from,\n            args.top_pairs_epoch,\n            loss_func,\n            optimizer,\n            ""toppairs"",\n            args.top_pairs_lr,\n            g_step,\n        )\n        start_from = 0\n\n    if args.startstage is None or args.startstage in [\n        ""ranking"",\n        ""allpairs"",\n        ""toppairs"",\n    ]:\n        optimizer = RMSprop(\n            model.parameters(), lr=args.ranking_lr, weight_decay=args.ranking_l2\n        )\n        loss_func = get_ranking_loss(batch_sampler.mentions_per_batch)\n        g_step = run_epochs(\n            start_from,\n            args.ranking_epoch,\n            loss_func,\n            optimizer,\n            ""ranking"",\n            args.ranking_lr,\n            g_step,\n        )\n\n\nif __name__ == ""__main__"":\n    DIR_PATH = os.path.dirname(os.path.realpath(__file__))\n    parser = argparse.ArgumentParser(\n        description=""Training the neural coreference model""\n    )\n    parser.add_argument(\n        ""--train"",\n        type=str,\n        default=DIR_PATH + ""/data/"",\n        help=""Path to the train dataset"",\n    )\n    parser.add_argument(\n        ""--eval"", type=str, default=DIR_PATH + ""/data/"", help=""Path to the eval dataset""\n    )\n    parser.add_argument(\n        ""--evalkey"", type=str, help=""Path to an optional key file for scoring""\n    )\n    parser.add_argument(\n        ""--weights"",\n        type=str,\n        help=""Path to pre-trained weights (if you only want to test the scoring for e.g.)"",\n    )\n    parser.add_argument(\n        ""--batchsize"",\n        type=int,\n        default=20000,\n        help=""Size of a batch in total number of pairs"",\n    )\n    parser.add_argument(\n        ""--numworkers"",\n        type=int,\n        default=8,\n        help=""Number of workers for loading batches"",\n    )\n    parser.add_argument(\n        ""--startstage"",\n        type=str,\n        help=\'Start from a specific stage (""allpairs"", ""toppairs"", ""ranking"")\',\n    )\n    parser.add_argument(""--startstep"", type=int, help=""Start from a specific step"")\n    parser.add_argument(\n        ""--checkpoint_file"",\n        type=str,\n        help=""Start from a previously saved checkpoint file"",\n    )\n    parser.add_argument(\n        ""--log_interval"", type=int, default=10, help=""test every X mini-batches""\n    )\n    parser.add_argument(\n        ""--conll_eval_interval"",\n        type=int,\n        default=10,\n        help=""evaluate eval F1 conll every X epochs"",\n    )\n    parser.add_argument(\n        ""--conll_train_interval"",\n        type=int,\n        default=20,\n        help=""evaluate train F1 conll every X epochs"",\n    )\n    parser.add_argument(""--seed"", type=int, default=1111, help=""random seed"")\n    parser.add_argument(""--costfn"", type=float, default=0.8, help=""cost of false new"")\n    parser.add_argument(""--costfl"", type=float, default=0.4, help=""cost of false link"")\n    parser.add_argument(""--costwl"", type=float, default=1.0, help=""cost of wrong link"")\n    parser.add_argument(\n        ""--h1"", type=int, default=1000, help=""number of hidden unit on layer 1""\n    )\n    parser.add_argument(\n        ""--h2"", type=int, default=500, help=""number of hidden unit on layer 2""\n    )\n    parser.add_argument(\n        ""--h3"", type=int, default=500, help=""number of hidden unit on layer 3""\n    )\n    parser.add_argument(\n        ""--all_pairs_epoch"",\n        type=int,\n        default=200,\n        help=""number of epochs for all-pairs pre-training"",\n    )\n    parser.add_argument(\n        ""--top_pairs_epoch"",\n        type=int,\n        default=200,\n        help=""number of epochs for top-pairs pre-training"",\n    )\n    parser.add_argument(\n        ""--ranking_epoch"",\n        type=int,\n        default=200,\n        help=""number of epochs for ranking training"",\n    )\n    parser.add_argument(\n        ""--all_pairs_lr"",\n        type=float,\n        default=2e-4,\n        help=""all pairs pre-training learning rate"",\n    )\n    parser.add_argument(\n        ""--top_pairs_lr"",\n        type=float,\n        default=2e-4,\n        help=""top pairs pre-training learning rate"",\n    )\n    parser.add_argument(\n        ""--ranking_lr"", type=float, default=2e-6, help=""ranking training learning rate""\n    )\n    parser.add_argument(\n        ""--all_pairs_l2"",\n        type=float,\n        default=1e-6,\n        help=""all pairs pre-training l2 regularization"",\n    )\n    parser.add_argument(\n        ""--top_pairs_l2"",\n        type=float,\n        default=1e-5,\n        help=""top pairs pre-training l2 regularization"",\n    )\n    parser.add_argument(\n        ""--ranking_l2"",\n        type=float,\n        default=1e-5,\n        help=""ranking training l2 regularization"",\n    )\n    parser.add_argument(\n        ""--patience"",\n        type=int,\n        default=3,\n        help=""patience (epochs) before considering evaluation has decreased"",\n    )\n    parser.add_argument(""--min_lr"", type=float, default=2e-8, help=""min learning rate"")\n    parser.add_argument(\n        ""--on_eval_decrease"",\n        type=str,\n        default=""nothing"",\n        help=\'What to do when evaluation decreases (""nothing"", ""divide_lr"", ""next_stage"", ""divide_then_next"")\',\n    )\n    parser.add_argument(\n        ""--lazy"",\n        type=int,\n        default=1,\n        choices=(0, 1),\n        help=""Use lazy loading (1, default) or not (0) while loading the npy files"",\n    )\n    args = parser.parse_args()\n    args.costs = {""FN"": args.costfn, ""FL"": args.costfl, ""WL"": args.costwl}\n    args.lazy = bool(args.lazy)\n    current_time = datetime.now().strftime(""%b%d_%H-%M-%S"")\n    args.save_path = os.path.join(\n        PACKAGE_DIRECTORY,\n        ""checkpoints"",\n        current_time + ""_"" + socket.gethostname() + ""_"",\n    )\n\n    np.random.seed(args.seed)\n    torch.manual_seed(args.seed)\n    args.cuda = torch.cuda.is_available()\n    if args.cuda:\n        torch.cuda.manual_seed(args.seed)\n\n    args.evalkey = args.evalkey if args.evalkey is not None else args.eval + ""/key.txt""\n    args.trainkey = args.train + ""/key.txt""\n    args.train = args.train + ""/numpy/""\n    args.eval = args.eval + ""/numpy/""\n    print(args)\n    run_model(args)\n'"
neuralcoref/train/model.py,14,"b'""""""Conll training algorithm""""""\n\nimport os\nimport numpy as np\n\nimport torch\nimport torch.nn as nn\nimport torch.utils.data\n\n\nclass Model(nn.Module):\n    def __init__(\n        self, vocab_size, embedding_dim, H1, H2, H3, D_pair_in, D_single_in, dropout=0.5\n    ):\n        super(Model, self).__init__()\n        self.word_embeds = nn.Embedding(vocab_size, embedding_dim)\n        self.drop = nn.Dropout(dropout)\n        self.pair_top = nn.Sequential(\n            nn.Linear(D_pair_in, H1),\n            nn.ReLU(),\n            nn.Dropout(dropout),\n            nn.Linear(H1, H2),\n            nn.ReLU(),\n            nn.Dropout(dropout),\n            nn.Linear(H2, H3),\n            nn.ReLU(),\n            nn.Dropout(dropout),\n            nn.Linear(H3, 1),\n            nn.Linear(1, 1),\n        )\n        self.single_top = nn.Sequential(\n            nn.Linear(D_single_in, H1),\n            nn.ReLU(),\n            nn.Dropout(dropout),\n            nn.Linear(H1, H2),\n            nn.ReLU(),\n            nn.Dropout(dropout),\n            nn.Linear(H2, H3),\n            nn.ReLU(),\n            nn.Dropout(dropout),\n            nn.Linear(H3, 1),\n            nn.Linear(1, 1),\n        )\n        self.init_weights()\n\n    def init_weights(self):\n        w = (param.data for name, param in self.named_parameters() if ""weight"" in name)\n        b = (param.data for name, param in self.named_parameters() if ""bias"" in name)\n        nn.init.uniform_(self.word_embeds.weight.data, a=-0.5, b=0.5)\n        for t in w:\n            nn.init.xavier_uniform_(t)\n        for t in b:\n            nn.init.constant_(t, 0)\n\n    def load_embeddings(self, preloaded_weights):\n        self.word_embeds.weight = nn.Parameter(preloaded_weights)\n\n    def load_weights(self, weights_path):\n        print(""Loading weights"")\n        single_layers_weights, single_layers_biases = [], []\n        for f in sorted(os.listdir(weights_path)):\n            if f.startswith(""single_mention_weights""):\n                single_layers_weights.append(np.load(os.path.join(weights_path, f)))\n            if f.startswith(""single_mention_bias""):\n                single_layers_biases.append(np.load(os.path.join(weights_path, f)))\n        top_single_linear = (\n            layer for layer in self.single_top if isinstance(layer, nn.Linear)\n        )\n        for w, b, layer in zip(\n            single_layers_weights, single_layers_biases, top_single_linear\n        ):\n            layer.weight = nn.Parameter(torch.from_numpy(w).float())\n            layer.bias = nn.Parameter(torch.from_numpy(b).float().squeeze())\n        pair_layers_weights, pair_layers_biases = [], []\n        for f in sorted(os.listdir(weights_path)):\n            if f.startswith(""pair_mentions_weights""):\n                pair_layers_weights.append(np.load(os.path.join(weights_path, f)))\n            if f.startswith(""pair_mentions_bias""):\n                pair_layers_biases.append(np.load(os.path.join(weights_path, f)))\n        top_pair_linear = (\n            layer for layer in self.pair_top if isinstance(layer, nn.Linear)\n        )\n        for w, b, layer in zip(\n            pair_layers_weights, pair_layers_biases, top_pair_linear\n        ):\n            layer.weight = nn.Parameter(torch.from_numpy(w).float())\n            layer.bias = nn.Parameter(torch.from_numpy(b).float().squeeze())\n\n    def forward(self, inputs, concat_axis=1):\n        pairs = len(inputs) == 8\n        if pairs:\n            spans, words, single_features, ant_spans, ant_words, ana_spans, ana_words, pair_features = (\n                inputs\n            )\n        else:\n            spans, words, single_features = inputs\n        words = words.type(torch.LongTensor)\n        if torch.cuda.is_available():\n            words = words.cuda()\n        embed_words = self.drop(self.word_embeds(words).view(words.size()[0], -1))\n        single_input = torch.cat([spans, embed_words, single_features], 1)\n        single_scores = self.single_top(single_input)\n        if pairs:\n            batchsize, pairs_num, _ = ana_spans.size()\n            ant_words_long = ant_words.view(batchsize, -1).type(torch.LongTensor)\n            ana_words_long = ana_words.view(batchsize, -1).type(torch.LongTensor)\n            if torch.cuda.is_available():\n                ant_words_long = ant_words_long.cuda()\n                ana_words_long = ana_words_long.cuda()\n            ant_embed_words = self.drop(\n                self.word_embeds(ant_words_long).view(batchsize, pairs_num, -1)\n            )\n            ana_embed_words = self.drop(\n                self.word_embeds(ana_words_long).view(batchsize, pairs_num, -1)\n            )\n            pair_input = torch.cat(\n                [ant_spans, ant_embed_words, ana_spans, ana_embed_words, pair_features],\n                2,\n            )\n            pair_scores = self.pair_top(pair_input).squeeze(dim=2)\n            total_scores = torch.cat([pair_scores, single_scores], concat_axis)\n        return total_scores if pairs else single_scores\n'"
neuralcoref/train/utils.py,0,"b'""""""Utils""""""\n\n\nfrom concurrent.futures import ThreadPoolExecutor, as_completed\nimport os\nimport numpy as np\nfrom tqdm import tqdm\n\nPACKAGE_DIRECTORY = os.path.dirname(os.path.abspath(__file__))\nBATCH_SIZE_PATH = os.path.join(\n    PACKAGE_DIRECTORY, ""test_batch_size.txt""\n)  # fernandes.txt"")#\n\nSIZE_SPAN = 250  # size of the span vector (averaged word embeddings)\nSIZE_WORD = 8  # number of words in a mention (tuned embeddings)\nSIZE_EMBEDDING = 50  # size of the words embeddings\nSIZE_FP = 70  # number of features for a pair of mention\nSIZE_FP_COMPRESSED = (\n    9\n)  # size of the features for a pair of mentions as stored in numpy arrays\nSIZE_FS = 24  # number of features of a single mention\nSIZE_FS_COMPRESSED = 6  # size of the features for a mention as stored in numpy arrays\nSIZE_GENRE = 7  # Size of the genre one-hot array\nSIZE_MENTION_EMBEDDING = (\n    SIZE_SPAN + SIZE_WORD * SIZE_EMBEDDING\n)  # A mention embeddings (span + words vectors)\nSIZE_SNGL_FEATS = SIZE_FS - SIZE_GENRE\nSIZE_PAIR_FEATS = SIZE_FP - SIZE_GENRE\nSIZE_SNGL_IN_NO_GENRE = SIZE_MENTION_EMBEDDING + SIZE_SNGL_FEATS\nSIZE_PAIR_IN_NO_GENRE = 2 * SIZE_MENTION_EMBEDDING + SIZE_PAIR_FEATS\n\nSIZE_PAIR_IN = (\n    2 * SIZE_MENTION_EMBEDDING + SIZE_FP\n)  # Input to the mentions pair neural network\nSIZE_SINGLE_IN = (\n    SIZE_MENTION_EMBEDDING + SIZE_FS\n)  # Input to the single mention neural network\n\nDISTANCE_BINS = list(range(5)) + [5] * 3 + [6] * 8 + [7] * 16 + [8] * 32\nBINS_NUM = float(len(DISTANCE_BINS))\nMAX_BINS = DISTANCE_BINS[-1] + 1\n\n\ndef encode_distance(x):\n    """""" Encode an integer or an array of integers as a (bined) one-hot numpy array """"""\n\n    def _encode_distance(d):\n        """""" Encode an integer as a (bined) one-hot numpy array """"""\n        dist_vect = np.zeros((11,), dtype=""float32"")\n        if d < 64:\n            dist_vect[DISTANCE_BINS[d]] = 1\n        else:\n            dist_vect[9] = 1\n        dist_vect[10] = min(float(d), BINS_NUM) / BINS_NUM\n        return dist_vect\n\n    if isinstance(x, np.ndarray):\n        arr_l = [_encode_distance(y)[np.newaxis, :] for y in x]\n        out_arr = np.concatenate(arr_l)\n    else:\n        out_arr = _encode_distance(x)\n    return out_arr\n\n\ndef parallel_process(array, function, n_jobs=16, use_kwargs=False, front_num=10):\n    """"""\n        A parallel version of the map function with a progress bar. \n\n        Args:\n            array (array-like): An array to iterate over.\n            function (function): A python function to apply to the elements of array\n            n_jobs (int, default=16): The number of cores to use\n            use_kwargs (boolean, default=False): Whether to consider the elements of array as dictionaries of \n                keyword arguments to function \n            front_num (int, default=3): The number of iterations to run serially before kicking off the parallel job. \n                Useful for catching bugs\n        Returns:\n            [function(array[0]), function(array[1]), ...]\n    """"""\n    # We run the first few iterations serially to catch bugs\n    if front_num > 0:\n        front = [\n            function(**a) if use_kwargs else function(a) for a in array[:front_num]\n        ]\n    else:\n        front = []\n    # If we set n_jobs to 1, just run a list comprehension. This is useful for benchmarking and debugging.\n    if n_jobs == 1:\n        return front + [\n            function(**a) if use_kwargs else function(a)\n            for a in tqdm(array[front_num:])\n        ]\n    # Assemble the workers\n    with ThreadPoolExecutor(max_workers=n_jobs) as pool:\n        # Pass the elements of array into function\n        if use_kwargs:\n            futures = [pool.submit(function, **a) for a in array[front_num:]]\n        else:\n            futures = [pool.submit(function, a) for a in array[front_num:]]\n        kwargs = {\n            ""total"": len(futures),\n            ""unit"": ""it"",\n            ""unit_scale"": True,\n            ""leave"": True,\n        }\n        # #Print out the progress as tasks complete\n        # for _ in tqdm(as_completed(futures), **kwargs):\n        #     pass\n    out = []\n    # Get the results from the futures.\n    for future in futures:  # tqdm(futures):\n        try:\n            out.append(future.result())\n        except Exception as e:\n            out.append(e)\n    return front + out\n'"
