file_path,api_count,code
analysis.py,0,"b""import os\nimport json\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.metrics import accuracy_score\n\nfrom datasets import _rocstories\n\ndef rocstories(data_dir, pred_path, log_path):\n    preds = pd.read_csv(pred_path, delimiter='\\t')['prediction'].values.tolist()\n    _, _, _, labels = _rocstories(os.path.join(data_dir, 'cloze_test_test__spring2016 - cloze_test_ALL_test.csv'))\n    test_accuracy = accuracy_score(labels, preds)*100.\n    logs = [json.loads(line) for line in open(log_path)][1:]\n    best_validation_index = np.argmax([log['va_acc'] for log in logs])\n    valid_accuracy = logs[best_validation_index]['va_acc']\n    print('ROCStories Valid Accuracy: %.2f'%(valid_accuracy))\n    print('ROCStories Test Accuracy:  %.2f'%(test_accuracy))\n"""
datasets.py,0,"b""import os\nimport csv\nimport numpy as np\n\nfrom tqdm import tqdm\n\nfrom sklearn.utils import shuffle\nfrom sklearn.model_selection import train_test_split\n\nseed = 3535999445\n\ndef _rocstories(path):\n    with open(path, encoding='utf_8') as f:\n        f = csv.reader(f)\n        st = []\n        ct1 = []\n        ct2 = []\n        y = []\n        for i, line in enumerate(tqdm(list(f), ncols=80, leave=False)):\n            if i > 0:\n                s = ' '.join(line[1:5])\n                c1 = line[5]\n                c2 = line[6]\n                st.append(s)\n                ct1.append(c1)\n                ct2.append(c2)\n                y.append(int(line[-1])-1)\n        return st, ct1, ct2, y\n\ndef rocstories(data_dir, n_train=1497, n_valid=374):\n    storys, comps1, comps2, ys = _rocstories(os.path.join(data_dir, 'cloze_test_val__spring2016 - cloze_test_ALL_val.csv'))\n    teX1, teX2, teX3, _ = _rocstories(os.path.join(data_dir, 'cloze_test_test__spring2016 - cloze_test_ALL_test.csv'))\n    tr_storys, va_storys, tr_comps1, va_comps1, tr_comps2, va_comps2, tr_ys, va_ys = train_test_split(storys, comps1, comps2, ys, test_size=n_valid, random_state=seed)\n    trX1, trX2, trX3 = [], [], []\n    trY = []\n    for s, c1, c2, y in zip(tr_storys, tr_comps1, tr_comps2, tr_ys):\n        trX1.append(s)\n        trX2.append(c1)\n        trX3.append(c2)\n        trY.append(y)\n\n    vaX1, vaX2, vaX3 = [], [], []\n    vaY = []\n    for s, c1, c2, y in zip(va_storys, va_comps1, va_comps2, va_ys):\n        vaX1.append(s)\n        vaX2.append(c1)\n        vaX3.append(c2)\n        vaY.append(y)\n    trY = np.asarray(trY, dtype=np.int32)\n    vaY = np.asarray(vaY, dtype=np.int32)\n    return (trX1, trX2, trX3, trY), (vaX1, vaX2, vaX3, vaY), (teX1, teX2, teX3)\n"""
generate.py,10,"b'import argparse\nimport os\nimport random\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\n\nfrom model_pytorch import LMModel, load_openai_pretrained_model\nfrom text_utils import TextEncoder\n\n\ndef make_batch(X):\n    X = np.array(X)\n    assert X.ndim in [1, 2]\n    if X.ndim == 1:\n        X = np.expand_dims(X, axis=0)\n    pos_enc = np.arange(n_vocab + n_special, n_vocab + n_special + X.shape[-1])\n    pos_enc = np.expand_dims(pos_enc, axis=0)\n    batch = np.stack([X, pos_enc], axis=-1)\n    batch = torch.tensor(batch, dtype=torch.long).to(device)\n    return batch\n\ndef append_batch(X, next_idx):\n    next_pos = X[:, -1:, 1] + 1\n    next_x = torch.cat((next_idx, next_pos), -1).unsqueeze(1)\n    return torch.cat((X, next_x), 1)\n\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'--desc\', type=str, help=""Description"")\n    parser.add_argument(\'--dataset\', type=str)\n    parser.add_argument(\'--log_dir\', type=str, default=\'log/\')\n    parser.add_argument(\'--save_dir\', type=str, default=\'save/\')\n    parser.add_argument(\'--data_dir\', type=str, default=\'data/\')\n    parser.add_argument(\'--submission_dir\', type=str, default=\'submission/\')\n    parser.add_argument(\'--submit\', action=\'store_true\')\n    parser.add_argument(\'--analysis\', action=\'store_true\')\n    parser.add_argument(\'--seed\', type=int, default=42)\n    parser.add_argument(\'--n_iter\', type=int, default=3)\n    parser.add_argument(\'--n_batch\', type=int, default=8)\n    parser.add_argument(\'--max_grad_norm\', type=int, default=1)\n    parser.add_argument(\'--lr\', type=float, default=6.25e-5)\n    parser.add_argument(\'--lr_warmup\', type=float, default=0.002)\n    parser.add_argument(\'--n_ctx\', type=int, default=512)\n    parser.add_argument(\'--n_embd\', type=int, default=768)\n    parser.add_argument(\'--n_head\', type=int, default=12)\n    parser.add_argument(\'--n_layer\', type=int, default=12)\n    parser.add_argument(\'--embd_pdrop\', type=float, default=0.1)\n    parser.add_argument(\'--attn_pdrop\', type=float, default=0.1)\n    parser.add_argument(\'--resid_pdrop\', type=float, default=0.1)\n    parser.add_argument(\'--clf_pdrop\', type=float, default=0.1)\n    parser.add_argument(\'--l2\', type=float, default=0.01)\n    parser.add_argument(\'--vector_l2\', action=\'store_true\')\n    parser.add_argument(\'--opt\', type=str, default=\'adam\')\n    parser.add_argument(\'--afn\', type=str, default=\'gelu\')\n    parser.add_argument(\'--lr_schedule\', type=str, default=\'warmup_linear\')\n    parser.add_argument(\'--encoder_path\', type=str, default=\'model/encoder_bpe_40000.json\')\n    parser.add_argument(\'--bpe_path\', type=str, default=\'model/vocab_40000.bpe\')\n    parser.add_argument(\'--n_transfer\', type=int, default=12)\n    parser.add_argument(\'--lm_coef\', type=float, default=0.5)\n    parser.add_argument(\'--b1\', type=float, default=0.9)\n    parser.add_argument(\'--b2\', type=float, default=0.999)\n    parser.add_argument(\'--e\', type=float, default=1e-8)\n    parser.add_argument(\'--n_valid\', type=int, default=374)\n    parser.add_argument(\'--gen_len\', type=int, default=20)\n    parser.add_argument(\'--topk\', type=int, default=10)\n\n    args = parser.parse_args()\n    print(args)\n\n    random.seed(args.seed)\n    np.random.seed(args.seed)\n    torch.manual_seed(args.seed)\n    torch.cuda.manual_seed_all(args.seed)\n\n    # Constants\n    submit = args.submit\n    dataset = args.dataset\n    n_ctx = args.n_ctx\n    save_dir = args.save_dir\n    desc = args.desc\n    data_dir = args.data_dir\n    log_dir = args.log_dir\n    submission_dir = args.submission_dir\n\n    device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")\n    n_gpu = torch.cuda.device_count()\n    print(""device"", device, ""n_gpu"", n_gpu)\n\n    text_encoder = TextEncoder(args.encoder_path, args.bpe_path)\n    encoder = text_encoder.encoder\n    n_vocab = len(text_encoder.encoder)\n\n    n_special = 0   # XD: useless for language modeling task\n    vocab = n_vocab + n_special + n_ctx\n\n    lm_model = LMModel(args, vocab, n_ctx, return_probs=True)\n    load_openai_pretrained_model(lm_model.transformer, n_ctx=n_ctx, n_special=n_special)\n    lm_model.to(device)\n\n    lm_model.eval()\n\n    text = input(\'Input some beginning words:\')\n    while text != \'q\':\n        X = text_encoder.encode([text,])\n        XMB = make_batch(X)\n\n        for _ in range(args.gen_len):\n            lm_probs = lm_model(XMB)\n            if args.topk == 0:\n                next_idx = torch.multinomial(lm_probs[:, -1, :], 1)\n            else:\n                values, indices = lm_probs[:, -1, :].topk(args.topk)\n                next_idx = indices.gather(-1, torch.multinomial(values, 1))\n            next_token = text_encoder.decoder[next_idx.item()].replace(\'</w>\', \'\')\n            print(next_token, end=\' \')\n            XMB = append_batch(XMB, next_idx)\n\n        print()\n        text = input(\'Input some beginning words:\')'"
loss.py,2,"b'import torch\n\nclass MultipleChoiceLossCompute:\n    ""A Loss compute and train function for multiple choice tasks.""\n\n    def __init__(self, lm_criterion, clf_criterion, lm_coef, opt=None):\n        self.lm_criterion = lm_criterion\n        self.clf_criterion = clf_criterion\n        self.lm_coef = lm_coef\n        self.opt = opt\n\n    def __call__(self, X, Y, M, clf_logits, lm_logits=None, only_return_losses=False):\n        # Language modeling loss\n        if lm_logits is not None:\n            x_shifted = X[:, :, 1:, 0].contiguous().view(-1)  # Shape: 252\n            M = M.view(-1, M.size(2))\n            lm_losses = self.lm_criterion(lm_logits, x_shifted)\n            lm_losses = lm_losses.view(X.size(0) * X.size(1), X.size(2) - 1)\n            lm_losses = lm_losses * M[:, 1:]\n            lm_losses = lm_losses.sum(1) / torch.sum(M[:, 1:], 1)\n        # Classification loss\n        clf_losses = self.clf_criterion(clf_logits, Y)\n        if only_return_losses:\n            return (clf_losses, lm_losses) if lm_logits is not None else clf_losses\n\n        if self.lm_coef > 0 and lm_logits is not None:\n            train_loss = clf_losses.sum() + self.lm_coef * lm_losses.sum()\n        else:\n            train_loss = clf_losses.sum()\n        train_loss.backward()\n        if self.opt is not None:\n            self.opt.step()\n            self.opt.zero_grad()\n        return train_loss.item()\n\nclass ClassificationLossCompute:\n    ""A Loss compute and train function for classification tasks.""\n\n    def __init__(self, lm_criterion, clf_criterion, lm_coef, opt=None):\n        self.lm_criterion  = lm_criterion\n        self.clf_criterion = clf_criterion\n        self.lm_coef       = lm_coef\n        self.opt           = opt\n\n    def __call__(self, X, Y, M, clf_logits, lm_logits=None, only_return_losses=False):\n        # Language modeling loss\n        if lm_logits is not None:\n            x_shifted = X[:, 1:, 0].contiguous().view(-1)\n            M         = M.view(-1, M.size(-1))\n            lm_losses = self.lm_criterion(lm_logits, x_shifted)\n            lm_losses = lm_losses.view(X.size(0), X.size(-2) - 1)\n            lm_losses = lm_losses * M[:, 1:]\n            lm_losses = lm_losses.sum(1) / torch.sum(M[:, 1:], 1)\n        # Classification loss\n        clf_losses = self.clf_criterion(clf_logits, Y)\n        if only_return_losses:\n            return (clf_losses, lm_losses) if lm_logits is not None else clf_losses\n\n        if self.lm_coef > 0 and lm_logits is not None:\n            train_loss = clf_losses.sum() + self.lm_coef * lm_losses.sum()\n        else:\n            train_loss = clf_losses.sum()\n        train_loss.backward()\n        if self.opt is not None:\n            self.opt.step()\n            self.opt.zero_grad()\n        return train_loss.item()\n\n# TODO Implement a LossCompute class for similiraty tasks.\n'"
model_pytorch.py,17,"b'import copy\nimport json\nimport math\nimport re\nimport collections\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.nn.parameter import Parameter\n\n\ndef gelu(x):\n    return 0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3))))\n\n\ndef swish(x):\n    return x * torch.sigmoid(x)\n\n\nACT_FNS = {\n    \'relu\': nn.ReLU,\n    \'swish\': swish,\n    \'gelu\': gelu\n}\n\n\nclass LayerNorm(nn.Module):\n    ""Construct a layernorm module in the OpenAI style (epsilon inside the square root).""\n\n    def __init__(self, n_state, e=1e-5):\n        super(LayerNorm, self).__init__()\n        self.g = nn.Parameter(torch.ones(n_state))\n        self.b = nn.Parameter(torch.zeros(n_state))\n        self.e = e\n\n    def forward(self, x):\n        u = x.mean(-1, keepdim=True)\n        s = (x - u).pow(2).mean(-1, keepdim=True)\n        x = (x - u) / torch.sqrt(s + self.e)\n        return self.g * x + self.b\n\n\nclass Conv1D(nn.Module):\n    def __init__(self, nf, rf, nx):\n        super(Conv1D, self).__init__()\n        self.rf = rf\n        self.nf = nf\n        if rf == 1:  # faster 1x1 conv\n            w = torch.empty(nx, nf)\n            nn.init.normal_(w, std=0.02)\n            self.w = Parameter(w)\n            self.b = Parameter(torch.zeros(nf))\n        else:  # was used to train LM\n            raise NotImplementedError\n\n    def forward(self, x):\n        if self.rf == 1:\n            size_out = x.size()[:-1] + (self.nf,)\n            x = torch.addmm(self.b, x.view(-1, x.size(-1)), self.w)\n            x = x.view(*size_out)\n        else:\n            raise NotImplementedError\n        return x\n\n\nclass Attention(nn.Module):\n    def __init__(self, nx, n_ctx, cfg, scale=False):\n        super(Attention, self).__init__()\n        n_state = nx  # in Attention: n_state=768 (nx=n_embd)\n        # [switch nx => n_state from Block to Attention to keep identical to TF implem]\n        assert n_state % cfg.n_head == 0\n        self.register_buffer(\'b\', torch.tril(torch.ones(n_ctx, n_ctx)).view(1, 1, n_ctx, n_ctx))\n        self.n_head = cfg.n_head\n        self.split_size = n_state\n        self.scale = scale\n        self.c_attn = Conv1D(n_state * 3, 1, nx)\n        self.c_proj = Conv1D(n_state, 1, nx)\n        self.attn_dropout = nn.Dropout(cfg.attn_pdrop)\n        self.resid_dropout = nn.Dropout(cfg.resid_pdrop)\n\n    def _attn(self, q, k, v):\n        w = torch.matmul(q, k)\n        if self.scale:\n            w = w / math.sqrt(v.size(-1))\n        # w = w * self.b + -1e9 * (1 - self.b)  # TF implem method: mask_attn_weights\n        # XD: self.b may be larger than w, so we need to crop it\n        b = self.b[:, :, :w.size(-2), :w.size(-1)]\n        w = w * b + -1e9 * (1 - b)\n\n        w = nn.Softmax(dim=-1)(w)\n        w = self.attn_dropout(w)\n        return torch.matmul(w, v)\n\n    def merge_heads(self, x):\n        x = x.permute(0, 2, 1, 3).contiguous()\n        new_x_shape = x.size()[:-2] + (x.size(-2) * x.size(-1),)\n        return x.view(*new_x_shape)  # in Tensorflow implem: fct merge_states\n\n    def split_heads(self, x, k=False):\n        new_x_shape = x.size()[:-1] + (self.n_head, x.size(-1) // self.n_head)\n        x = x.view(*new_x_shape)  # in Tensorflow implem: fct split_states\n        if k:\n            return x.permute(0, 2, 3, 1)\n        else:\n            return x.permute(0, 2, 1, 3)\n\n    def forward(self, x):\n        x = self.c_attn(x)\n        query, key, value = x.split(self.split_size, dim=2)\n        query = self.split_heads(query)\n        key = self.split_heads(key, k=True)\n        value = self.split_heads(value)\n        a = self._attn(query, key, value)\n        a = self.merge_heads(a)\n        a = self.c_proj(a)\n        a = self.resid_dropout(a)\n        return a\n\n\nclass MLP(nn.Module):\n    def __init__(self, n_state, cfg):  # in MLP: n_state=3072 (4 * n_embd)\n        super(MLP, self).__init__()\n        nx = cfg.n_embd\n        self.c_fc = Conv1D(n_state, 1, nx)\n        self.c_proj = Conv1D(nx, 1, n_state)\n        self.act = ACT_FNS[cfg.afn]\n        self.dropout = nn.Dropout(cfg.resid_pdrop)\n\n    def forward(self, x):\n        h = self.act(self.c_fc(x))\n        h2 = self.c_proj(h)\n        return self.dropout(h2)\n\n\nclass Block(nn.Module):\n    def __init__(self, n_ctx, cfg, scale=False):\n        super(Block, self).__init__()\n        nx = cfg.n_embd\n        self.attn = Attention(nx, n_ctx, cfg, scale)\n        self.ln_1 = LayerNorm(nx)\n        self.mlp = MLP(4 * nx, cfg)\n        self.ln_2 = LayerNorm(nx)\n\n    def forward(self, x):\n        a = self.attn(x)\n        n = self.ln_1(x + a)\n        m = self.mlp(n)\n        h = self.ln_2(n + m)\n        return h\n\n\nclass TransformerModel(nn.Module):\n    """""" Transformer model """"""\n\n    def __init__(self, cfg, vocab=40990, n_ctx=512):\n        super(TransformerModel, self).__init__()\n        self.vocab = vocab\n        self.embed = nn.Embedding(vocab, cfg.n_embd)\n        self.drop = nn.Dropout(cfg.embd_pdrop)\n        block = Block(n_ctx, cfg, scale=True)\n        self.h = nn.ModuleList([copy.deepcopy(block) for _ in range(cfg.n_layer)])\n\n        nn.init.normal_(self.embed.weight, std=0.02)\n\n    def forward(self, x):\n        x = x.view(-1, x.size(-2), x.size(-1))\n        e = self.drop(self.embed(x))\n        # Add the position information to the input embeddings\n        h = e.sum(dim=2)\n        for block in self.h:\n            h = block(h)\n        return h\n\n\nclass LMHead(nn.Module):\n    """""" Language Model Head for the transformer """"""\n\n    def __init__(self, model, cfg, trunc_and_reshape=True):\n        super(LMHead, self).__init__()\n        self.n_embd = cfg.n_embd\n        embed_shape = model.embed.weight.shape\n        self.decoder = nn.Linear(embed_shape[1], embed_shape[0], bias=False)\n        self.decoder.weight = model.embed.weight # Tied weights\n        self.trunc_and_reshape = trunc_and_reshape  # XD\n\n    def forward(self, h):\n        # Truncated Language modeling logits (we remove the last token)\n        h_trunc = h[:, :-1].contiguous().view(-1, self.n_embd) \\\n            if self.trunc_and_reshape else h  # XD\n        lm_logits = self.decoder(h_trunc)\n        return lm_logits\n\n\nclass MultipleChoiceHead(nn.Module):\n    """""" Classifier Head for the transformer """"""\n\n    def __init__(self, clf_token, cfg):\n        super(MultipleChoiceHead, self).__init__()\n        self.n_embd = cfg.n_embd\n        self.clf_token = clf_token\n        self.dropout = nn.Dropout2d(cfg.clf_pdrop)  # To reproduce the noise_shape parameter of TF implementation\n        self.linear = nn.Linear(cfg.n_embd, 1)\n\n        nn.init.normal_(self.linear.weight, std = 0.02)\n        nn.init.normal_(self.linear.bias, 0)\n\n    def forward(self, h, x):\n        # Classification logits\n        clf_h = h.view(-1, self.n_embd)\n        flat = x[..., 0].contiguous().view(-1)\n        clf_h = clf_h[flat == self.clf_token, :]\n        clf_h = clf_h.view(-1, x.size(1), self.n_embd, 1)\n        # This double transposition is there to replicate the behavior\n        # of the noise_shape argument in the tensorflow\n        # implementation.  For more details, see\n        # https://github.com/huggingface/pytorch-openai-transformer-lm/issues/11\n        clf_h = self.dropout(clf_h.transpose(1, 2)).transpose(1, 2)\n        clf_h = clf_h.contiguous().view(-1, self.n_embd)\n        clf_logits = self.linear(clf_h)\n\n        return clf_logits.view(-1, x.size(1))\n\n\nclass ClfHead(nn.Module):\n    """"""Classification Head for the transformer\n\n    TODO: test this class.""""""\n    def __init__(self, clf_token, cfg, n_class):\n        super(ClfHead, self).__init__()\n        self.n_embd = cfg.n_embd\n        self.clf_token = clf_token\n        self.dropout = nn.Dropout(cfg.clf_pdrop)\n        self.linear = nn.Linear(cfg.n_embd, n_class)\n\n        nn.init.normal_(self.linear.weight, std = 0.02)\n        nn.init.normal_(self.linear.bias, 0)\n\n    def forward(self, h, x):\n        clf_h = h.view(-1, self.n_embd)\n        flat = x[..., 0].contiguous().view(-1)\n        clf_h = clf_h[flat == self.clf_token, :]\n        clf_h = self.dropout(clf_h)\n        clf_logits = self.linear(clf_h)\n\n        return clf_logits\n\nclass SimilarityHead(nn.Module):\n    """""" Similarity Head for the transformer\n\n        TODO: test this class.""""""\n    def __init__(self, clf_token, cfg):\n        super(SimilarityHead, self).__init__()\n        self.n_embd = cfg.n_embd\n        self.clf_token = clf_token\n        self.dropout = nn.Dropout(cfg.clf_pdrop)\n        self.linear = nn.Linear(cfg.n_embd, 1)\n\n        nn.init.normal_(self.linear.weight, std = 0.02)\n        nn.init.normal_(self.linear.bias, 0)\n\n    def forward(self, h, x):\n        sim_h = h.view(-1, self.n_embd)\n        flat = x[..., 0].contiguous().view(-1)\n        sim_h = sim_h[flat == self.clf_token, :]\n        sim_h = self.dropout(sim_h)\n        sim_h = sim_h.sum(dim = 1)\n        sim_logits = self.linear(sim_h)\n\n        return sim_logits\n\n\n# XD\nclass LMModel(nn.Module):\n    """""" Transformer with language model head only """"""\n    def __init__(self, cfg, vocab=40990, n_ctx=512, return_probs=False):\n        super(LMModel, self).__init__()\n        self.transformer = TransformerModel(cfg, vocab=vocab, n_ctx=n_ctx)\n        self.lm_head = LMHead(self.transformer, cfg, trunc_and_reshape=False)\n        self.return_probs = return_probs\n        if self.return_probs:\n            pos_emb_mask = torch.zeros(1, 1, vocab)\n            pos_emb_mask[:, :, -n_ctx:] = -1e12\n            self.register_buffer(\'pos_emb_mask\', pos_emb_mask)\n\n\n    def forward(self, x):\n        h = self.transformer(x)\n        lm_logits = self.lm_head(h)\n        if self.return_probs:\n            lm_logits = F.softmax(lm_logits + self.pos_emb_mask, dim=-1)\n        return lm_logits\n\n\nclass DoubleHeadModel(nn.Module):\n    """""" Transformer with language model and task specific heads """"""\n    def __init__(self, cfg, clf_token, task_head_type, vocab=40990, n_ctx=512):\n        super(DoubleHeadModel, self).__init__()\n        self.transformer = TransformerModel(cfg, vocab=vocab, n_ctx=n_ctx)\n        self.lm_head = LMHead(self.transformer, cfg)\n        if isinstance(task_head_type, str):\n            if task_head_type == \'multiple_choice\':\n                self.task_head = MultipleChoiceHead(clf_token, cfg)\n            elif task_head_type == \'similarity\':\n                self.task_head = SimilarityHead(clf_token, cfg)\n            elif task_head_type == \'inference\':\n                # the three classes correspond to entailment, contradiction and neutral.\n                self.task_head = ClfHead(clf_token, cfg, 3)\n            else:\n                raise ValueError(""task_head_type is expected to be \'multiple_choice\' ""\n                                 ""\'similarity\', \'inference\' or (\'classification\', n_class) ""\n                                 f""got {task_head_type}."")\n        elif isinstance(task_head_type, collections.abc.Sequence) and len(task_head_type) == 2 and \\\n             task_head_type[0] == \'classification\':\n            n_class = task_head_type[1]\n            self.task_head = ClfHead(clf_token, cfg, n_class)\n        else:\n            raise ValueError(""task_head_type is expected to be \'multiple_choice\' ""\n                             ""\'similarity\', \'inference\' or (\'classification\', n_class) ""\n                             f""got {task_head_type}."")\n\n    def forward(self, x):\n        h = self.transformer(x)\n        lm_logits = self.lm_head(h)\n        task_logits = self.task_head(h, x)\n\n        return lm_logits, task_logits\n\n\ndef load_openai_pretrained_model(model, n_ctx=-1, n_special=-1, n_transfer=12, n_embd=768, path=\'./model/\',\n                                 path_names=\'./\'):\n    # Load weights from TF model\n    print(""Loading weights..."")\n    names = json.load(open(path_names + \'parameters_names.json\'))\n    shapes = json.load(open(path + \'params_shapes.json\'))\n    offsets = np.cumsum([np.prod(shape) for shape in shapes])\n    init_params = [np.load(path + \'params_{}.npy\'.format(n)) for n in range(10)]\n    init_params = np.split(np.concatenate(init_params, 0), offsets)[:-1]\n    init_params = [param.reshape(shape) for param, shape in zip(init_params, shapes)]\n    if n_ctx > 0:\n        init_params[0] = init_params[0][:n_ctx]\n    if n_special > 0:\n        init_params[0] = np.concatenate(\n            [init_params[1],\n             (np.random.randn(n_special, n_embd) * 0.02).astype(np.float32),\n             init_params[0]\n             ], 0)\n    else:\n        init_params[0] = np.concatenate(\n            [init_params[1],\n             init_params[0]\n             ], 0)\n    del init_params[1]\n    if n_transfer == -1:\n        n_transfer = 0\n    else:\n        n_transfer = 1 + n_transfer * 12\n    init_params = [arr.squeeze() for arr in init_params]\n\n    try:\n        assert model.embed.weight.shape == init_params[0].shape\n    except AssertionError as e:\n        e.args += (model.embed.weight.shape, init_params[0].shape)\n        raise\n\n    model.embed.weight.data = torch.from_numpy(init_params[0])\n\n    for name, ip in zip(names[1:n_transfer], init_params[1:n_transfer]):\n        name = name[6:]  # skip ""model/""\n        assert name[-2:] == "":0""\n        name = name[:-2]\n        name = name.split(\'/\')\n        pointer = model\n        for m_name in name:\n            if re.fullmatch(r\'[A-Za-z]+\\d+\', m_name):\n                l = re.split(r\'(\\d+)\', m_name)\n            else:\n                l = [m_name]\n            pointer = getattr(pointer, l[0])\n            if len(l) >= 2:\n                num = int(l[1])\n                pointer = pointer[num]\n        try:\n            assert pointer.shape == ip.shape\n        except AssertionError as e:\n            e.args += (pointer.shape, ip.shape)\n            raise\n        pointer.data = torch.from_numpy(ip)\n\n\nclass dotdict(dict):\n    """"""dot.notation access to dictionary attributes""""""\n    __getattr__ = dict.get\n    __setattr__ = dict.__setitem__\n    __delattr__ = dict.__delitem__\n\n\nDEFAULT_CONFIG = dotdict({\n    \'n_embd\': 768,\n    \'n_head\': 12,\n    \'n_layer\': 12,\n    \'embd_pdrop\': 0.1,\n    \'attn_pdrop\': 0.1,\n    \'resid_pdrop\': 0.1,\n    \'afn\': \'gelu\',\n    \'clf_pdrop\': 0.1})\n'"
opt.py,5,"b'import math\nimport torch\nfrom torch.optim import Optimizer\nfrom torch.nn.utils import clip_grad_norm_\n\ndef warmup_cosine(x, warmup=0.002):\n    s = 1 if x <= warmup else 0\n    return s*(x/warmup) + (1-s)*(0.5 * (1 + torch.cos(math.pi * x)))\n\ndef warmup_constant(x, warmup=0.002):\n    s = 1 if x <= warmup else 0\n    return s*(x/warmup) + (1-s)*1\n\ndef warmup_linear(x, warmup=0.002):\n    s = 1 if x <= warmup else 0\n    return (s*(x/warmup) + (1-s))*(1-x)\n\nSCHEDULES = {\n    \'warmup_cosine\':warmup_cosine,\n    \'warmup_constant\':warmup_constant,\n    \'warmup_linear\':warmup_linear,\n}\n\n\nclass OpenAIAdam(Optimizer):\n    """"""Implements Open AI version of Adam algorithm with weight decay fix.\n    """"""\n    def __init__(self, params, lr, schedule, warmup, t_total,\n                 b1=0.9, b2=0.999, e=1e-8, l2=0,\n                 vector_l2=False, max_grad_norm=-1, **kwargs):\n        if not 0.0 <= lr:\n            raise ValueError(""Invalid learning rate: {}"".format(lr))\n        if schedule not in SCHEDULES:\n            raise ValueError(""Invalid schedule parameter: {}"".format(schedule))\n        if not 0 <= warmup:\n            raise ValueError(""Invalid warmup: {}"".format(warmup))\n        if not 0.0 <= b1 < 1.0:\n            raise ValueError(""Invalid b1 parameter: {}"".format(b1))\n        if not 0.0 <= b2 < 1.0:\n            raise ValueError(""Invalid b2 parameter: {}"".format(b2))\n        if not 0.0 <= e:\n            raise ValueError(""Invalid epsilon value: {}"".format(e))\n        defaults = dict(lr=lr, schedule=schedule, warmup=warmup, t_total=t_total,\n                        b1=b1, b2=b2, e=e, l2=l2, vector_l2=vector_l2,\n                        max_grad_norm=max_grad_norm)\n        super(OpenAIAdam, self).__init__(params, defaults)\n\n    def step(self, closure=None):\n        """"""Performs a single optimization step.\n\n        Arguments:\n            closure (callable, optional): A closure that reevaluates the model\n                and returns the loss.\n        """"""\n        loss = None\n        if closure is not None:\n            loss = closure()\n\n        for group in self.param_groups:\n            for p in group[\'params\']:\n                if p.grad is None:\n                    continue\n                grad = p.grad.data\n                if grad.is_sparse:\n                    raise RuntimeError(\'Adam does not support sparse gradients, please consider SparseAdam instead\')\n\n                state = self.state[p]\n\n                # State initialization\n                if len(state) == 0:\n                    state[\'step\'] = 0\n                    # Exponential moving average of gradient values\n                    state[\'exp_avg\'] = torch.zeros_like(p.data)\n                    # Exponential moving average of squared gradient values\n                    state[\'exp_avg_sq\'] = torch.zeros_like(p.data)\n\n                exp_avg, exp_avg_sq = state[\'exp_avg\'], state[\'exp_avg_sq\']\n                beta1, beta2 = group[\'b1\'], group[\'b2\']\n\n                state[\'step\'] += 1\n\n                # Add grad clipping\n                if group[\'max_grad_norm\'] > 0:\n                    clip_grad_norm_(p, group[\'max_grad_norm\'])\n\n                # Decay the first and second moment running average coefficient\n                exp_avg.mul_(beta1).add_(1 - beta1, grad)\n                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n                denom = exp_avg_sq.sqrt().add_(group[\'e\'])\n\n                bias_correction1 = 1 - beta1 ** state[\'step\']\n                bias_correction2 = 1 - beta2 ** state[\'step\']\n\n                schedule_fct = SCHEDULES[group[\'schedule\']]\n                lr_scheduled = group[\'lr\'] * schedule_fct(state[\'step\']/group[\'t_total\'], group[\'warmup\'])\n                step_size = lr_scheduled * math.sqrt(bias_correction2) / bias_correction1\n\n                p.data.addcdiv_(-step_size, exp_avg, denom)\n\n                # Add weight decay at the end (fixed version)\n                if (len(p.size()) > 1 or group[\'vector_l2\']) and group[\'l2\'] > 0:\n                    p.data.add_(-lr_scheduled * group[\'l2\'], p.data)\n\n        return loss\n'"
text_utils.py,0,"b'import re\nimport ftfy\nimport json\nimport spacy\n\nfrom tqdm import tqdm\n\ndef get_pairs(word):\n    """"""\n    Return set of symbol pairs in a word.\n    word is represented as tuple of symbols (symbols being variable-length strings)\n    """"""\n    pairs = set()\n    prev_char = word[0]\n    for char in word[1:]:\n        pairs.add((prev_char, char))\n        prev_char = char\n    return pairs\n\ndef text_standardize(text):\n    """"""\n    fixes some issues the spacy tokenizer had on books corpus\n    also does some whitespace standardization\n    """"""\n    text = text.replace(\'\xe2\x80\x94\', \'-\')\n    text = text.replace(\'\xe2\x80\x93\', \'-\')\n    text = text.replace(\'\xe2\x80\x95\', \'-\')\n    text = text.replace(\'\xe2\x80\xa6\', \'...\')\n    text = text.replace(\'\xc2\xb4\', ""\'"")\n    text = re.sub(r\'\'\'(-+|~+|!+|""+|;+|\\?+|\\++|,+|\\)+|\\(+|\\\\+|\\/+|\\*+|\\[+|\\]+|}+|{+|\\|+|_+)\'\'\', r\' \\1 \', text)\n    text = re.sub(r\'\\s*\\n\\s*\', \' \\n \', text)\n    text = re.sub(r\'[^\\S\\n]+\', \' \', text)\n    return text.strip()\n\nclass TextEncoder(object):\n    """"""\n    mostly a wrapper for a public python bpe tokenizer\n    """"""\n\n    def __init__(self, encoder_path, bpe_path):\n        self.nlp = spacy.load(\'en\', disable=[\'parser\', \'tagger\', \'ner\', \'textcat\'])\n        self.encoder = json.load(open(encoder_path))\n        self.decoder = {v:k for k,v in self.encoder.items()}\n        merges = open(bpe_path, encoding=\'utf-8\').read().split(\'\\n\')[1:-1]\n        merges = [tuple(merge.split()) for merge in merges]\n        self.bpe_ranks = dict(zip(merges, range(len(merges))))\n        self.cache = {}\n\n    def bpe(self, token):\n        word = tuple(token[:-1]) + ( token[-1] + \'</w>\',)\n        if token in self.cache:\n            return self.cache[token]\n        pairs = get_pairs(word)\n\n        if not pairs:\n            return token+\'</w>\'\n\n        while True:\n            bigram = min(pairs, key = lambda pair: self.bpe_ranks.get(pair, float(\'inf\')))\n            if bigram not in self.bpe_ranks:\n                break\n            first, second = bigram\n            new_word = []\n            i = 0\n            while i < len(word):\n                try:\n                    j = word.index(first, i)\n                    new_word.extend(word[i:j])\n                    i = j\n                except:\n                    new_word.extend(word[i:])\n                    break\n\n                if word[i] == first and i < len(word)-1 and word[i+1] == second:\n                    new_word.append(first+second)\n                    i += 2\n                else:\n                    new_word.append(word[i])\n                    i += 1\n            new_word = tuple(new_word)\n            word = new_word\n            if len(word) == 1:\n                break\n            else:\n                pairs = get_pairs(word)\n        word = \' \'.join(word)\n        if word == \'\\n  </w>\':\n            word = \'\\n</w>\'\n        self.cache[token] = word\n        return word\n\n    def encode(self, texts, verbose=True):\n        texts_tokens = []\n        if verbose:\n            for text in tqdm(texts, ncols=80, leave=False):\n                text = self.nlp(text_standardize(ftfy.fix_text(text)))\n                text_tokens = []\n                for token in text:\n                    text_tokens.extend([self.encoder.get(t, 0) for t in self.bpe(token.text.lower()).split(\' \')])\n                texts_tokens.append(text_tokens)\n        else:\n            for text in texts:\n                text = self.nlp(text_standardize(ftfy.fix_text(text)))\n                text_tokens = []\n                for token in text:\n                    text_tokens.extend([self.encoder.get(t, 0) for t in self.bpe(token.text.lower()).split(\' \')])\n                texts_tokens.append(text_tokens)\n        return texts_tokens\n'"
train.py,18,"b'import argparse\nimport os\nimport random\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.utils import shuffle\n\nfrom analysis import rocstories as rocstories_analysis\nfrom datasets import rocstories\nfrom model_pytorch import DoubleHeadModel, load_openai_pretrained_model\nfrom opt import OpenAIAdam\nfrom text_utils import TextEncoder\nfrom utils import (encode_dataset, iter_data,\n                   ResultLogger, make_path)\nfrom loss import MultipleChoiceLossCompute\n\ndef transform_roc(X1, X2, X3):\n    n_batch = len(X1)\n    xmb = np.zeros((n_batch, 2, n_ctx, 2), dtype=np.int32)\n    mmb = np.zeros((n_batch, 2, n_ctx), dtype=np.float32)\n    start = encoder[\'_start_\']\n    delimiter = encoder[\'_delimiter_\']\n    for i, (x1, x2, x3), in enumerate(zip(X1, X2, X3)):\n        x12 = [start] + x1[:max_len] + [delimiter] + x2[:max_len] + [clf_token]\n        x13 = [start] + x1[:max_len] + [delimiter] + x3[:max_len] + [clf_token]\n        l12 = len(x12)\n        l13 = len(x13)\n        xmb[i, 0, :l12, 0] = x12\n        xmb[i, 1, :l13, 0] = x13\n        mmb[i, 0, :l12] = 1\n        mmb[i, 1, :l13] = 1\n    # Position information that is added to the input embeddings in the TransformerModel\n    xmb[:, :, :, 1] = np.arange(n_vocab + n_special, n_vocab + n_special + n_ctx)\n    return xmb, mmb\n\n\ndef iter_apply(Xs, Ms, Ys):\n    # fns = [lambda x: np.concatenate(x, 0), lambda x: float(np.sum(x))]\n    logits = []\n    cost = 0\n    with torch.no_grad():\n        dh_model.eval()\n        for xmb, mmb, ymb in iter_data(Xs, Ms, Ys, n_batch=n_batch_train, truncate=False, verbose=True):\n            n = len(xmb)\n            XMB = torch.tensor(xmb, dtype=torch.long).to(device)\n            YMB = torch.tensor(ymb, dtype=torch.long).to(device)\n            MMB = torch.tensor(mmb).to(device)\n            _, clf_logits = dh_model(XMB)\n            clf_logits *= n\n            clf_losses = compute_loss_fct(XMB, YMB, MMB, clf_logits, only_return_losses=True)\n            clf_losses *= n\n            logits.append(clf_logits.to(""cpu"").numpy())\n            cost += clf_losses.sum().item()\n        logits = np.concatenate(logits, 0)\n    return logits, cost\n\n\ndef iter_predict(Xs, Ms):\n    logits = []\n    with torch.no_grad():\n        dh_model.eval()\n        for xmb, mmb in iter_data(Xs, Ms, n_batch=n_batch_train, truncate=False, verbose=True):\n            n = len(xmb)\n            XMB = torch.tensor(xmb, dtype=torch.long).to(device)\n            MMB = torch.tensor(mmb).to(device)\n            _, clf_logits = dh_model(XMB)\n            logits.append(clf_logits.to(""cpu"").numpy())\n    logits = np.concatenate(logits, 0)\n    return logits\n\n\ndef log(save_dir, desc):\n    global best_score\n    print(""Logging"")\n    tr_logits, tr_cost = iter_apply(trX[:n_valid], trM[:n_valid], trY[:n_valid])\n    va_logits, va_cost = iter_apply(vaX, vaM, vaY)\n    tr_cost = tr_cost / len(trY[:n_valid])\n    va_cost = va_cost / n_valid\n    tr_acc = accuracy_score(trY[:n_valid], np.argmax(tr_logits, 1)) * 100.\n    va_acc = accuracy_score(vaY, np.argmax(va_logits, 1)) * 100.\n    logger.log(n_epochs=n_epochs, n_updates=n_updates, tr_cost=tr_cost, va_cost=va_cost, tr_acc=tr_acc, va_acc=va_acc)\n    print(\'%d %d %.3f %.3f %.2f %.2f\' % (n_epochs, n_updates, tr_cost, va_cost, tr_acc, va_acc))\n    if submit:\n        score = va_acc\n        if score > best_score:\n            best_score = score\n            path = os.path.join(save_dir, desc, \'best_params\')\n            torch.save(dh_model.state_dict(), make_path(path))\n\n\ndef predict(dataset, submission_dir):\n    filename = filenames[dataset]\n    pred_fn = pred_fns[dataset]\n    label_decoder = label_decoders[dataset]\n    predictions = pred_fn(iter_predict(teX, teM))\n    if label_decoder is not None:\n        predictions = [label_decoder[prediction] for prediction in predictions]\n    path = os.path.join(submission_dir, filename)\n    os.makedirs(os.path.dirname(path), exist_ok=True)\n    with open(path, \'w\') as f:\n        f.write(\'{}\\t{}\\n\'.format(\'index\', \'prediction\'))\n        for i, prediction in enumerate(predictions):\n            f.write(\'{}\\t{}\\n\'.format(i, prediction))\n\n\ndef run_epoch():\n    for xmb, mmb, ymb in iter_data(*shuffle(trX, trM, trYt, random_state=np.random),\n                                   n_batch=n_batch_train, truncate=True, verbose=True):\n        global n_updates\n        dh_model.train()\n        XMB = torch.tensor(xmb, dtype=torch.long).to(device)\n        YMB = torch.tensor(ymb, dtype=torch.long).to(device)\n        MMB = torch.tensor(mmb).to(device)\n        lm_logits, clf_logits = dh_model(XMB)\n        compute_loss_fct(XMB, YMB, MMB, clf_logits, lm_logits)\n        n_updates += 1\n        if n_updates in [1000, 2000, 4000, 8000, 16000, 32000] and n_epochs == 0:\n            log(save_dir, desc)\n\n\nargmax = lambda x: np.argmax(x, 1)\n\npred_fns = {\n    \'rocstories\': argmax,\n}\n\nfilenames = {\n    \'rocstories\': \'ROCStories.tsv\',\n}\n\nlabel_decoders = {\n    \'rocstories\': None,\n}\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'--desc\', type=str, help=""Description"")\n    parser.add_argument(\'--dataset\', type=str)\n    parser.add_argument(\'--log_dir\', type=str, default=\'log/\')\n    parser.add_argument(\'--save_dir\', type=str, default=\'save/\')\n    parser.add_argument(\'--data_dir\', type=str, default=\'data/\')\n    parser.add_argument(\'--submission_dir\', type=str, default=\'submission/\')\n    parser.add_argument(\'--submit\', action=\'store_true\')\n    parser.add_argument(\'--analysis\', action=\'store_true\')\n    parser.add_argument(\'--seed\', type=int, default=42)\n    parser.add_argument(\'--n_iter\', type=int, default=3)\n    parser.add_argument(\'--n_batch\', type=int, default=8)\n    parser.add_argument(\'--max_grad_norm\', type=int, default=1)\n    parser.add_argument(\'--lr\', type=float, default=6.25e-5)\n    parser.add_argument(\'--lr_warmup\', type=float, default=0.002)\n    parser.add_argument(\'--n_ctx\', type=int, default=512)\n    parser.add_argument(\'--n_embd\', type=int, default=768)\n    parser.add_argument(\'--n_head\', type=int, default=12)\n    parser.add_argument(\'--n_layer\', type=int, default=12)\n    parser.add_argument(\'--embd_pdrop\', type=float, default=0.1)\n    parser.add_argument(\'--attn_pdrop\', type=float, default=0.1)\n    parser.add_argument(\'--resid_pdrop\', type=float, default=0.1)\n    parser.add_argument(\'--clf_pdrop\', type=float, default=0.1)\n    parser.add_argument(\'--l2\', type=float, default=0.01)\n    parser.add_argument(\'--vector_l2\', action=\'store_true\')\n    parser.add_argument(\'--opt\', type=str, default=\'adam\')\n    parser.add_argument(\'--afn\', type=str, default=\'gelu\')\n    parser.add_argument(\'--lr_schedule\', type=str, default=\'warmup_linear\')\n    parser.add_argument(\'--encoder_path\', type=str, default=\'model/encoder_bpe_40000.json\')\n    parser.add_argument(\'--bpe_path\', type=str, default=\'model/vocab_40000.bpe\')\n    parser.add_argument(\'--n_transfer\', type=int, default=12)\n    parser.add_argument(\'--lm_coef\', type=float, default=0.5)\n    parser.add_argument(\'--b1\', type=float, default=0.9)\n    parser.add_argument(\'--b2\', type=float, default=0.999)\n    parser.add_argument(\'--e\', type=float, default=1e-8)\n    parser.add_argument(\'--n_valid\', type=int, default=374)\n\n    args = parser.parse_args()\n    print(args)\n\n    random.seed(args.seed)\n    np.random.seed(args.seed)\n    torch.manual_seed(args.seed)\n    torch.cuda.manual_seed_all(args.seed)\n\n    # Constants\n    submit = args.submit\n    dataset = args.dataset\n    n_ctx = args.n_ctx\n    save_dir = args.save_dir\n    desc = args.desc\n    data_dir = args.data_dir\n    log_dir = args.log_dir\n    submission_dir = args.submission_dir\n\n    device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")\n    n_gpu = torch.cuda.device_count()\n    print(""device"", device, ""n_gpu"", n_gpu)\n\n    logger = ResultLogger(path=os.path.join(log_dir, \'{}.jsonl\'.format(desc)), **args.__dict__)\n    text_encoder = TextEncoder(args.encoder_path, args.bpe_path)\n    encoder = text_encoder.encoder\n    n_vocab = len(text_encoder.encoder)\n\n    print(""Encoding dataset..."")\n    ((trX1, trX2, trX3, trY),\n     (vaX1, vaX2, vaX3, vaY),\n     (teX1, teX2, teX3)) = encode_dataset(*rocstories(data_dir, n_valid=args.n_valid),\n                                          encoder=text_encoder)\n    encoder[\'_start_\'] = len(encoder)\n    encoder[\'_delimiter_\'] = len(encoder)\n    encoder[\'_classify_\'] = len(encoder)\n    clf_token = encoder[\'_classify_\']\n    n_special = 3\n    max_len = n_ctx // 2 - 2\n    n_ctx = min(max(\n        [len(x1[:max_len]) + max(len(x2[:max_len]),\n                                 len(x3[:max_len])) for x1, x2, x3 in zip(trX1, trX2, trX3)]\n        + [len(x1[:max_len]) + max(len(x2[:max_len]),\n                                   len(x3[:max_len])) for x1, x2, x3 in zip(vaX1, vaX2, vaX3)]\n        + [len(x1[:max_len]) + max(len(x2[:max_len]),\n                                   len(x3[:max_len])) for x1, x2, x3 in zip(teX1, teX2, teX3)]\n        ) + 3, n_ctx)\n    vocab = n_vocab + n_special + n_ctx\n    trX, trM = transform_roc(trX1, trX2, trX3)\n    vaX, vaM = transform_roc(vaX1, vaX2, vaX3)\n    if submit:\n        teX, teM = transform_roc(teX1, teX2, teX3)\n\n    n_train = len(trY)\n    n_valid = len(vaY)\n    n_batch_train = args.n_batch * max(n_gpu, 1)\n    n_updates_total = (n_train // n_batch_train) * args.n_iter\n\n    dh_model = DoubleHeadModel(args, clf_token, \'multiple_choice\', vocab, n_ctx)\n\n    criterion = nn.CrossEntropyLoss(reduce=False)\n    model_opt = OpenAIAdam(dh_model.parameters(),\n                           lr=args.lr,\n                           schedule=args.lr_schedule,\n                           warmup=args.lr_warmup,\n                           t_total=n_updates_total,\n                           b1=args.b1,\n                           b2=args.b2,\n                           e=args.e,\n                           l2=args.l2,\n                           vector_l2=args.vector_l2,\n                           max_grad_norm=args.max_grad_norm)\n    compute_loss_fct = MultipleChoiceLossCompute(criterion,\n                                                 criterion,\n                                                 args.lm_coef,\n                                                 model_opt)\n    load_openai_pretrained_model(dh_model.transformer, n_ctx=n_ctx, n_special=n_special)\n\n    dh_model.to(device)\n    dh_model = nn.DataParallel(dh_model)\n\n    n_updates = 0\n    n_epochs = 0\n    if dataset != \'stsb\':\n        trYt = trY\n    if submit:\n        path = os.path.join(save_dir, desc, \'best_params\')\n        torch.save(dh_model.state_dict(), make_path(path))\n    best_score = 0\n    for i in range(args.n_iter):\n        print(""running epoch"", i)\n        run_epoch()\n        n_epochs += 1\n        log(save_dir, desc)\n    if submit:\n        path = os.path.join(save_dir, desc, \'best_params\')\n        dh_model.load_state_dict(torch.load(path))\n        predict(dataset, args.submission_dir)\n        if args.analysis:\n            rocstories_analysis(data_dir, os.path.join(args.submission_dir, \'ROCStories.tsv\'),\n                                os.path.join(log_dir, \'rocstories.jsonl\'))\n'"
utils.py,0,"b'import os\nimport sys\nimport json\nimport time\nfrom functools import partial\nimport numpy as np\n# import tensorflow as tf\n# from tensorflow.python.framework import function\nfrom tqdm import tqdm\n\ndef encode_dataset(*splits, encoder):\n    encoded_splits = []\n    for split in splits:\n        fields = []\n        for field in split:\n            if isinstance(field[0], str):\n                field = encoder.encode(field)\n            fields.append(field)\n        encoded_splits.append(fields)\n    return encoded_splits\n\ndef stsb_label_encoding(labels, nclass=6):\n    """"""\n    Label encoding from Tree LSTM paper (Tai, Socher, Manning)\n    """"""\n    Y = np.zeros((len(labels), nclass)).astype(np.float32)\n    for j, y in enumerate(labels):\n        for i in range(nclass):\n            if i == np.floor(y) + 1:\n                Y[j,i] = y - np.floor(y)\n            if i == np.floor(y):\n                Y[j,i] = np.floor(y) - y + 1\n    return Y\n\ndef np_softmax(x, t=1):\n    x = x/t\n    x = x - np.max(x, axis=-1, keepdims=True)\n    ex = np.exp(x)\n    return ex/np.sum(ex, axis=-1, keepdims=True)\n\ndef make_path(f):\n    d = os.path.dirname(f)\n    if d and not os.path.exists(d):\n        os.makedirs(d)\n    return f\n\ndef _identity_init(shape, dtype, partition_info, scale):\n    n = shape[-1]\n    w = np.eye(n)*scale\n    if len([s for s in shape if s != 1]) == 2:\n        w = w.reshape(shape)\n    return w.astype(np.float32)\n\ndef identity_init(scale=1.0):\n    return partial(_identity_init, scale=scale)\n\ndef _np_init(shape, dtype, partition_info, w):\n    return w\n\ndef np_init(w):\n    return partial(_np_init, w=w)\n\nclass ResultLogger(object):\n    def __init__(self, path, *args, **kwargs):\n        if \'time\' not in kwargs:\n            kwargs[\'time\'] = time.time()\n        self.f_log = open(make_path(path), \'w\')\n        self.f_log.write(json.dumps(kwargs)+\'\\n\')\n\n    def log(self, **kwargs):\n        if \'time\' not in kwargs:\n            kwargs[\'time\'] = time.time()\n        self.f_log.write(json.dumps(kwargs)+\'\\n\')\n        self.f_log.flush()\n\n    def close(self):\n        self.f_log.close()\n\ndef flatten(outer):\n    return [el for inner in outer for el in inner]\n\ndef remove_none(l):\n    return [e for e in l if e is not None]\n\ndef iter_data(*datas, n_batch=128, truncate=False, verbose=False, max_batches=float(""inf"")):\n    n = len(datas[0])\n    if truncate:\n        n = (n//n_batch)*n_batch\n    n = min(n, max_batches*n_batch)\n    n_batches = 0\n    if verbose:\n        f = sys.stderr\n    else:\n        f = open(os.devnull, \'w\')\n    for i in tqdm(range(0, n, n_batch), total=n//n_batch, file=f, ncols=80, leave=False):\n        if n_batches >= max_batches: raise StopIteration\n        if len(datas) == 1:\n            yield datas[0][i:i+n_batch]\n        else:\n            yield (d[i:i+n_batch] for d in datas)\n        n_batches += 1\n'"
