file_path,api_count,code
setup.py,0,"b""from setuptools import setup\n\nsetup(\n    name='torchmoji',\n    version='1.0',\n    packages=['torchmoji'],\n    description='torchMoji',\n    include_package_data=True,\n    install_requires=[\n        'emoji==0.4.5',\n        'numpy==1.13.1',\n        'scipy==0.19.1',\n        'scikit-learn==0.19.0',\n        'text-unidecode==1.0',\n    ],\n)\n"""
examples/__init__.py,0,b''
examples/create_twitter_vocab.py,0,"b'"""""" Creates a vocabulary from a tsv file.\n""""""\n\nimport codecs\nimport example_helper\nfrom torchmoji.create_vocab import VocabBuilder\nfrom torchmoji.word_generator import TweetWordGenerator\n\nwith codecs.open(\'../../twitterdata/tweets.2016-09-01\', \'rU\', \'utf-8\') as stream:\n    wg = TweetWordGenerator(stream)\n    vb = VocabBuilder(wg)\n    vb.count_all_words()\n    vb.save_vocab()\n'"
examples/dataset_split.py,0,"b""'''\nSplit a given dataset into three different datasets: training, validation and\ntesting.\n\nThis is achieved by splitting the given list of sentences into three separate\nlists according to either a given ratio (e.g. [0.7, 0.1, 0.2]) or by an\nexplicit enumeration. The sentences are also tokenised using the given\nvocabulary.\n\nAlso splits a given list of dictionaries containing information about\neach sentence.\n\nAn additional parameter can be set 'extend_with', which will extend the given\nvocabulary with up to 'extend_with' tokens, taken from the training dataset.\n'''\nfrom __future__ import print_function, unicode_literals\nimport example_helper\nimport json\n\nfrom torchmoji.sentence_tokenizer import SentenceTokenizer\n\nDATASET = [\n    'I am sentence 0',\n    'I am sentence 1',\n    'I am sentence 2',\n    'I am sentence 3',\n    'I am sentence 4',\n    'I am sentence 5',\n    'I am sentence 6',\n    'I am sentence 7',\n    'I am sentence 8',\n    'I am sentence 9 newword',\n    ]\n\nINFO_DICTS = [\n    {'label': 'sentence 0'},\n    {'label': 'sentence 1'},\n    {'label': 'sentence 2'},\n    {'label': 'sentence 3'},\n    {'label': 'sentence 4'},\n    {'label': 'sentence 5'},\n    {'label': 'sentence 6'},\n    {'label': 'sentence 7'},\n    {'label': 'sentence 8'},\n    {'label': 'sentence 9'},\n    ]\n\nwith open('../model/vocabulary.json', 'r') as f:\n    vocab = json.load(f)\nst = SentenceTokenizer(vocab, 30)\n\n# Split using the default split ratio\nprint(st.split_train_val_test(DATASET, INFO_DICTS))\n\n# Split explicitly\nprint(st.split_train_val_test(DATASET,\n                              INFO_DICTS,\n                              [[0, 1, 2, 4, 9], [5, 6], [7, 8, 3]],\n                              extend_with=1))\n"""
examples/encode_texts.py,0,"b'# -*- coding: utf-8 -*-\n\n"""""" Use torchMoji to encode texts into emotional feature vectors.\n""""""\nfrom __future__ import print_function, division, unicode_literals\nimport json\n\nfrom torchmoji.sentence_tokenizer import SentenceTokenizer\nfrom torchmoji.model_def import torchmoji_feature_encoding\nfrom torchmoji.global_variables import PRETRAINED_PATH, VOCAB_PATH\n\nTEST_SENTENCES = [\'I love mom\\\'s cooking\',\n                  \'I love how you never reply back..\',\n                  \'I love cruising with my homies\',\n                  \'I love messing with yo mind!!\',\n                  \'I love you and now you\\\'re just gone..\',\n                  \'This is shit\',\n                  \'This is the shit\']\n\nmaxlen = 30\nbatch_size = 32\n\nprint(\'Tokenizing using dictionary from {}\'.format(VOCAB_PATH))\nwith open(VOCAB_PATH, \'r\') as f:\n    vocabulary = json.load(f)\nst = SentenceTokenizer(vocabulary, maxlen)\ntokenized, _, _ = st.tokenize_sentences(TEST_SENTENCES)\n\nprint(\'Loading model from {}.\'.format(PRETRAINED_PATH))\nmodel = torchmoji_feature_encoding(PRETRAINED_PATH)\nprint(model)\n\nprint(\'Encoding texts..\')\nencoding = model(tokenized)\n\nprint(\'First 5 dimensions for sentence: {}\'.format(TEST_SENTENCES[0]))\nprint(encoding[0,:5])\n\n# Now you could visualize the encodings to see differences,\n# run a logistic regression classifier on top,\n# or basically anything you\'d like to do.'"
examples/example_helper.py,0,"b'"""""" Module import helper.\nModifies PATH in order to allow us to import the torchmoji directory.\n""""""\nimport sys\nfrom os.path import abspath, dirname\nsys.path.insert(0, dirname(dirname(abspath(__file__))))\n'"
examples/finetune_insults_chain-thaw.py,0,"b'""""""Finetuning example.\n\nTrains the torchMoji model on the kaggle insults dataset, using the \'chain-thaw\'\nfinetuning method and the accuracy metric. See the blog post at\nhttps://medium.com/@bjarkefelbo/what-can-we-learn-from-emojis-6beb165a5ea0\nfor more information. Note that results may differ a bit due to slight\nchanges in preprocessing and train/val/test split.\n\nThe \'chain-thaw\' method does the following:\n0) Load all weights except for the softmax layer. Extend the embedding layer if\n   necessary, initialising the new weights with random values.\n1) Freeze every layer except the last (softmax) layer and train it.\n2) Freeze every layer except the first layer and train it.\n3) Freeze every layer except the second etc., until the second last layer.\n4) Unfreeze all layers and train entire model.\n""""""\n\nfrom __future__ import print_function\nimport example_helper\nimport json\nfrom torchmoji.model_def import torchmoji_transfer\nfrom torchmoji.global_variables import PRETRAINED_PATH\nfrom torchmoji.finetuning import (\n     load_benchmark,\n     finetune)\n\n\nDATASET_PATH = \'../data/kaggle-insults/raw.pickle\'\nnb_classes = 2\n\nwith open(\'../model/vocabulary.json\', \'r\') as f:\n    vocab = json.load(f)\n\n# Load dataset. Extend the existing vocabulary with up to 10000 tokens from\n# the training dataset.\ndata = load_benchmark(DATASET_PATH, vocab, extend_with=10000)\n\n# Set up model and finetune. Note that we have to extend the embedding layer\n# with the number of tokens added to the vocabulary.\nmodel = torchmoji_transfer(nb_classes, PRETRAINED_PATH, extend_embedding=data[\'added\'])\nprint(model)\nmodel, acc = finetune(model, data[\'texts\'], data[\'labels\'], nb_classes,\n                      data[\'batch_size\'], method=\'chain-thaw\')\nprint(\'Acc: {}\'.format(acc))\n'"
examples/finetune_semeval_class-avg_f1.py,0,"b'""""""Finetuning example.\n\nTrains the torchMoji model on the SemEval emotion dataset, using the \'last\'\nfinetuning method and the class average F1 metric.\n\nThe \'last\' method does the following:\n0) Load all weights except for the softmax layer. Do not add tokens to the\n   vocabulary and do not extend the embedding layer.\n1) Freeze all layers except for the softmax layer.\n2) Train.\n\nThe class average F1 metric does the following:\n1) For each class, relabel the dataset into binary classification\n   (belongs to/does not belong to this class).\n2) Calculate F1 score for each class.\n3) Compute the average of all F1 scores.\n""""""\n\nfrom __future__ import print_function\nimport example_helper\nimport json\nfrom torchmoji.finetuning import load_benchmark\nfrom torchmoji.class_avg_finetuning import class_avg_finetune\nfrom torchmoji.model_def import torchmoji_transfer\nfrom torchmoji.global_variables import PRETRAINED_PATH\n\nDATASET_PATH = \'../data/SE0714/raw.pickle\'\nnb_classes = 3\n\nwith open(\'../model/vocabulary.json\', \'r\') as f:\n    vocab = json.load(f)\n\n\n# Load dataset. Extend the existing vocabulary with up to 10000 tokens from\n# the training dataset.\ndata = load_benchmark(DATASET_PATH, vocab, extend_with=10000)\n\n# Set up model and finetune. Note that we have to extend the embedding layer\n# with the number of tokens added to the vocabulary.\n#\n# Also note that when using class average F1 to evaluate, the model has to be\n# defined with two classes, since the model will be trained for each class\n# separately.\nmodel = torchmoji_transfer(2, PRETRAINED_PATH, extend_embedding=data[\'added\'])\nprint(model)\n\n# For finetuning however, pass in the actual number of classes.\nmodel, f1 = class_avg_finetune(model, data[\'texts\'], data[\'labels\'],\n                                nb_classes, data[\'batch_size\'], method=\'last\')\nprint(\'F1: {}\'.format(f1))\n'"
examples/finetune_youtube_last.py,0,"b'""""""Finetuning example.\n\nTrains the torchMoji model on the SS-Youtube dataset, using the \'last\'\nfinetuning method and the accuracy metric.\n\nThe \'last\' method does the following:\n0) Load all weights except for the softmax layer. Do not add tokens to the\n   vocabulary and do not extend the embedding layer.\n1) Freeze all layers except for the softmax layer.\n2) Train.\n""""""\n\nfrom __future__ import print_function\nimport example_helper\nimport json\nfrom torchmoji.model_def import torchmoji_transfer\nfrom torchmoji.global_variables import PRETRAINED_PATH, VOCAB_PATH, ROOT_PATH\nfrom torchmoji.finetuning import (\n     load_benchmark,\n     finetune)\n\nDATASET_PATH = \'{}/data/SS-Youtube/raw.pickle\'.format(ROOT_PATH)\nnb_classes = 2\n\nwith open(VOCAB_PATH, \'r\') as f:\n    vocab = json.load(f)\n\n# Load dataset.\ndata = load_benchmark(DATASET_PATH, vocab)\n\n# Set up model and finetune\nmodel = torchmoji_transfer(nb_classes, PRETRAINED_PATH)\nprint(model)\nmodel, acc = finetune(model, data[\'texts\'], data[\'labels\'], nb_classes, data[\'batch_size\'], method=\'last\')\nprint(\'Acc: {}\'.format(acc))\n'"
examples/score_texts_emojis.py,0,"b'# -*- coding: utf-8 -*-\n\n"""""" Use torchMoji to score texts for emoji distribution.\n\nThe resulting emoji ids (0-63) correspond to the mapping\nin emoji_overview.png file at the root of the torchMoji repo.\n\nWrites the result to a csv file.\n""""""\nfrom __future__ import print_function, division, unicode_literals\nimport example_helper\nimport json\nimport csv\nimport numpy as np\n\nfrom torchmoji.sentence_tokenizer import SentenceTokenizer\nfrom torchmoji.model_def import torchmoji_emojis\nfrom torchmoji.global_variables import PRETRAINED_PATH, VOCAB_PATH\n\nOUTPUT_PATH = \'test_sentences.csv\'\n\nTEST_SENTENCES = [\'I love mom\\\'s cooking\',\n                  \'I love how you never reply back..\',\n                  \'I love cruising with my homies\',\n                  \'I love messing with yo mind!!\',\n                  \'I love you and now you\\\'re just gone..\',\n                  \'This is shit\',\n                  \'This is the shit\']\n\n\ndef top_elements(array, k):\n    ind = np.argpartition(array, -k)[-k:]\n    return ind[np.argsort(array[ind])][::-1]\n\nmaxlen = 30\n\nprint(\'Tokenizing using dictionary from {}\'.format(VOCAB_PATH))\nwith open(VOCAB_PATH, \'r\') as f:\n    vocabulary = json.load(f)\n\nst = SentenceTokenizer(vocabulary, maxlen)\n\nprint(\'Loading model from {}.\'.format(PRETRAINED_PATH))\nmodel = torchmoji_emojis(PRETRAINED_PATH)\nprint(model)\nprint(\'Running predictions.\')\ntokenized, _, _ = st.tokenize_sentences(TEST_SENTENCES)\nprob = model(tokenized)\n\nfor prob in [prob]:\n    # Find top emojis for each sentence. Emoji ids (0-63)\n    # correspond to the mapping in emoji_overview.png\n    # at the root of the torchMoji repo.\n    print(\'Writing results to {}\'.format(OUTPUT_PATH))\n    scores = []\n    for i, t in enumerate(TEST_SENTENCES):\n        t_tokens = tokenized[i]\n        t_score = [t]\n        t_prob = prob[i]\n        ind_top = top_elements(t_prob, 5)\n        t_score.append(sum(t_prob[ind_top]))\n        t_score.extend(ind_top)\n        t_score.extend([t_prob[ind] for ind in ind_top])\n        scores.append(t_score)\n        print(t_score)\n\n    with open(OUTPUT_PATH, \'w\') as csvfile:\n        writer = csv.writer(csvfile, delimiter=str(\',\'), lineterminator=\'\\n\')\n        writer.writerow([\'Text\', \'Top5%\',\n                        \'Emoji_1\', \'Emoji_2\', \'Emoji_3\', \'Emoji_4\', \'Emoji_5\',\n                        \'Pct_1\', \'Pct_2\', \'Pct_3\', \'Pct_4\', \'Pct_5\'])\n        for i, row in enumerate(scores):\n            try:\n                writer.writerow(row)\n            except:\n                print(""Exception at row {}!"".format(i))\n'"
examples/text_emojize.py,0,"b'# -*- coding: utf-8 -*-\n\n"""""" Use torchMoji to predict emojis from a single text input\n""""""\n\nfrom __future__ import print_function, division, unicode_literals\nimport example_helper\nimport json\nimport csv\nimport argparse\n\nimport numpy as np\nimport emoji\n\nfrom torchmoji.sentence_tokenizer import SentenceTokenizer\nfrom torchmoji.model_def import torchmoji_emojis\nfrom torchmoji.global_variables import PRETRAINED_PATH, VOCAB_PATH\n\n# Emoji map in emoji_overview.png\nEMOJIS = "":joy: :unamused: :weary: :sob: :heart_eyes: \\\n:pensive: :ok_hand: :blush: :heart: :smirk: \\\n:grin: :notes: :flushed: :100: :sleeping: \\\n:relieved: :relaxed: :raised_hands: :two_hearts: :expressionless: \\\n:sweat_smile: :pray: :confused: :kissing_heart: :heartbeat: \\\n:neutral_face: :information_desk_person: :disappointed: :see_no_evil: :tired_face: \\\n:v: :sunglasses: :rage: :thumbsup: :cry: \\\n:sleepy: :yum: :triumph: :hand: :mask: \\\n:clap: :eyes: :gun: :persevere: :smiling_imp: \\\n:sweat: :broken_heart: :yellow_heart: :musical_note: :speak_no_evil: \\\n:wink: :skull: :confounded: :smile: :stuck_out_tongue_winking_eye: \\\n:angry: :no_good: :muscle: :facepunch: :purple_heart: \\\n:sparkling_heart: :blue_heart: :grimacing: :sparkles:"".split(\' \')\n\ndef top_elements(array, k):\n    ind = np.argpartition(array, -k)[-k:]\n    return ind[np.argsort(array[ind])][::-1]\n\nif __name__ == ""__main__"":\n    argparser = argparse.ArgumentParser()\n    argparser.add_argument(\'--text\', type=str, required=True, help=""Input text to emojize"")\n    argparser.add_argument(\'--maxlen\', type=int, default=30, help=""Max length of input text"")\n    args = argparser.parse_args()\n\n    # Tokenizing using dictionary\n    with open(VOCAB_PATH, \'r\') as f:\n        vocabulary = json.load(f)\n\n    st = SentenceTokenizer(vocabulary, args.maxlen)\n\n    # Loading model\n    model = torchmoji_emojis(PRETRAINED_PATH)\n    # Running predictions\n    tokenized, _, _ = st.tokenize_sentences([args.text])\n    # Get sentence probability\n    prob = model(tokenized)[0]\n\n    # Top emoji id\n    emoji_ids = top_elements(prob, 5)\n\n    # map to emojis\n    emojis = map(lambda x: EMOJIS[x], emoji_ids)\n\n    print(emoji.emojize(""{} {}"".format(args.text,\' \'.join(emojis)), use_aliases=True))\n'"
examples/tokenize_dataset.py,0,"b'""""""\nTake a given list of sentences and turn it into a numpy array, where each\nnumber corresponds to a word. Padding is used (number 0) to ensure fixed length\nof sentences.\n""""""\n\nfrom __future__ import print_function, unicode_literals\nimport example_helper\nimport json\nfrom torchmoji.sentence_tokenizer import SentenceTokenizer\n\nwith open(\'../model/vocabulary.json\', \'r\') as f:\n    vocabulary = json.load(f)\n\nst = SentenceTokenizer(vocabulary, 30)\ntest_sentences = [\n    \'\\u2014 -- \\u203c !!\\U0001F602\',\n    \'Hello world!\',\n    \'This is a sample tweet #example\',\n    ]\n\ntokens, infos, stats = st.tokenize_sentences(test_sentences)\n\nprint(tokens)\nprint(infos)\nprint(stats)\n'"
examples/vocab_extension.py,0,"b'""""""\nExtend the given vocabulary using dataset-specific words.\n\n1. First create a vocabulary for the specific dataset.\n2. Find all words not in our vocabulary, but in the dataset vocabulary.\n3. Take top X (default=1000) of these words and add them to the vocabulary.\n4. Save this combined vocabulary and embedding matrix, which can now be used.\n""""""\n\nfrom __future__ import print_function, unicode_literals\nimport example_helper\nimport json\nfrom torchmoji.create_vocab import extend_vocab, VocabBuilder\nfrom torchmoji.word_generator import WordGenerator\n\nnew_words = [\'#zzzzaaazzz\', \'newword\', \'newword\']\nword_gen = WordGenerator(new_words)\nvb = VocabBuilder(word_gen)\nvb.count_all_words()\n\nwith open(\'../model/vocabulary.json\') as f:\n    vocab = json.load(f)\n\nprint(len(vocab))\nprint(vb.word_counts)\nextend_vocab(vocab, vb, max_tokens=1)\n\n# \'newword\' should be added because it\'s more frequent in the given vocab\nprint(vocab[\'newword\'])\nprint(len(vocab))\n'"
scripts/analyze_all_results.py,0,"b""from __future__ import print_function\n\n# allow us to import the codebase directory\nimport sys\nimport glob\nimport numpy as np\nfrom os.path import dirname, abspath\nsys.path.insert(0, dirname(dirname(abspath(__file__))))\n\nDATASETS = ['SE0714', 'Olympic', 'PsychExp', 'SS-Twitter', 'SS-Youtube',\n            'SCv1', 'SV2-GEN'] # 'SE1604' excluded due to Twitter's ToS\n\ndef get_results(dset):\n    METHOD = 'last'\n    RESULTS_DIR = 'results/'\n    RESULT_PATHS = glob.glob('{}/{}_{}_*_results.txt'.format(RESULTS_DIR, dset, METHOD))\n    assert len(RESULT_PATHS)\n\n    scores = []\n    for path in RESULT_PATHS:\n        with open(path) as f:\n            score = f.readline().split(':')[1]\n        scores.append(float(score))\n\n    average = np.mean(scores)\n    maximum = max(scores)\n    minimum = min(scores)\n    std = np.std(scores)\n\n    print('Dataset: {}'.format(dset))\n    print('Method:  {}'.format(METHOD))\n    print('Number of results: {}'.format(len(scores)))\n    print('--------------------------')\n    print('Average: {}'.format(average))\n    print('Maximum: {}'.format(maximum))\n    print('Minimum: {}'.format(minimum))\n    print('Standard deviaton: {}'.format(std))\n\nfor dset in DATASETS:\n    get_results(dset)\n"""
scripts/analyze_results.py,0,"b""from __future__ import print_function\n\nimport sys\nimport glob\nimport numpy as np\n\nDATASET = 'SS-Twitter' # 'SE1604' excluded due to Twitter's ToS\nMETHOD = 'new'\n\n# Optional usage: analyze_results.py <dataset> <method>\nif len(sys.argv) == 3:\n    DATASET = sys.argv[1]\n    METHOD = sys.argv[2]\n\nRESULTS_DIR = 'results/'\nRESULT_PATHS = glob.glob('{}/{}_{}_*_results.txt'.format(RESULTS_DIR, DATASET, METHOD))\n\nif not RESULT_PATHS:\n    print('Could not find results for \\'{}\\' using \\'{}\\' in directory \\'{}\\'.'.format(DATASET, METHOD, RESULTS_DIR))\nelse:\n    scores = []\n    for path in RESULT_PATHS:\n        with open(path) as f:\n            score = f.readline().split(':')[1]\n        scores.append(float(score))\n\n    average = np.mean(scores)\n    maximum = max(scores)\n    minimum = min(scores)\n    std = np.std(scores)\n\n    print('Dataset: {}'.format(DATASET))\n    print('Method:  {}'.format(METHOD))\n    print('Number of results: {}'.format(len(scores)))\n    print('--------------------------')\n    print('Average: {}'.format(average))\n    print('Maximum: {}'.format(maximum))\n    print('Minimum: {}'.format(minimum))\n    print('Standard deviaton: {}'.format(std))\n"""
scripts/calculate_coverages.py,0,"b'from __future__ import print_function\nimport pickle\nimport json\nimport csv\nimport sys\nfrom io import open\n\n# Allow us to import the torchmoji directory\nfrom os.path import dirname, abspath\nsys.path.insert(0, dirname(dirname(abspath(__file__))))\n\nfrom torchmoji.sentence_tokenizer import SentenceTokenizer, coverage\n\ntry:\n    unicode        # Python 2\nexcept NameError:\n    unicode = str  # Python 3\n\nIS_PYTHON2 = int(sys.version[0]) == 2\n\nOUTPUT_PATH = \'coverage.csv\'\nDATASET_PATHS = [\n    \'../data/Olympic/raw.pickle\',\n    \'../data/PsychExp/raw.pickle\',\n    \'../data/SCv1/raw.pickle\',\n    \'../data/SCv2-GEN/raw.pickle\',\n    \'../data/SE0714/raw.pickle\',\n    #\'../data/SE1604/raw.pickle\', # Excluded due to Twitter\'s ToS\n    \'../data/SS-Twitter/raw.pickle\',\n    \'../data/SS-Youtube/raw.pickle\',\n    ]\n\nwith open(\'../model/vocabulary.json\', \'r\') as f:\n    vocab = json.load(f)\n\nresults = []\nfor p in DATASET_PATHS:\n    coverage_result = [p]\n    print(\'Calculating coverage for {}\'.format(p))\n    with open(p, \'rb\') as f:\n        if IS_PYTHON2:\n            s = pickle.load(f)\n        else:\n            s = pickle.load(f, fix_imports=True)\n\n    # Decode data\n    try:\n        s[\'texts\'] = [unicode(x) for x in s[\'texts\']]\n    except UnicodeDecodeError:\n        s[\'texts\'] = [x.decode(\'utf-8\') for x in s[\'texts\']]\n\n    # Own\n    st = SentenceTokenizer({}, 30)\n    tests, dicts, _ = st.split_train_val_test(s[\'texts\'], s[\'info\'],\n                                              [s[\'train_ind\'],\n                                               s[\'val_ind\'],\n                                               s[\'test_ind\']],\n                                              extend_with=10000)\n    coverage_result.append(coverage(tests[2]))\n\n    # Last\n    st = SentenceTokenizer(vocab, 30)\n    tests, dicts, _ = st.split_train_val_test(s[\'texts\'], s[\'info\'],\n                                              [s[\'train_ind\'],\n                                               s[\'val_ind\'],\n                                               s[\'test_ind\']],\n                                              extend_with=0)\n    coverage_result.append(coverage(tests[2]))\n\n    # Full\n    st = SentenceTokenizer(vocab, 30)\n    tests, dicts, _ = st.split_train_val_test(s[\'texts\'], s[\'info\'],\n                                              [s[\'train_ind\'],\n                                               s[\'val_ind\'],\n                                               s[\'test_ind\']],\n                                              extend_with=10000)\n    coverage_result.append(coverage(tests[2]))\n\n    results.append(coverage_result)\n\nwith open(OUTPUT_PATH, \'wb\') as csvfile:\n    writer = csv.writer(csvfile, delimiter=\'\\t\', lineterminator=\'\\n\')\n    writer.writerow([\'Dataset\', \'Own\', \'Last\', \'Full\'])\n    for i, row in enumerate(results):\n        try:\n            writer.writerow(row)\n        except:\n            print(""Exception at row {}!"".format(i))\n\nprint(\'Saved to {}\'.format(OUTPUT_PATH))\n'"
scripts/convert_all_datasets.py,0,"b""from __future__ import print_function\n\nimport json\nimport math\nimport pickle\nimport sys\nfrom io import open\nimport numpy as np\nfrom os.path import abspath, dirname\nsys.path.insert(0, dirname(dirname(abspath(__file__))))\n\nfrom torchmoji.word_generator import WordGenerator\nfrom torchmoji.create_vocab import VocabBuilder\nfrom torchmoji.sentence_tokenizer import SentenceTokenizer, extend_vocab, coverage\nfrom torchmoji.tokenizer import tokenize\n\ntry:\n    unicode        # Python 2\nexcept NameError:\n    unicode = str  # Python 3\n\nIS_PYTHON2 = int(sys.version[0]) == 2\n\nDATASETS = [\n    'Olympic',\n    'PsychExp',\n    'SCv1',\n    'SCv2-GEN',\n    'SE0714',\n    #'SE1604', # Excluded due to Twitter's ToS\n    'SS-Twitter',\n    'SS-Youtube',\n    ]\n\nDIR = '../data'\nFILENAME_RAW = 'raw.pickle'\nFILENAME_OWN = 'own_vocab.pickle'\nFILENAME_OUR = 'twitter_vocab.pickle'\nFILENAME_COMBINED = 'combined_vocab.pickle'\n\n\ndef roundup(x):\n    return int(math.ceil(x / 10.0)) * 10\n\n\ndef format_pickle(dset, train_texts, val_texts, test_texts, train_labels, val_labels, test_labels):\n    return {'dataset': dset,\n            'train_texts': train_texts,\n            'val_texts': val_texts,\n            'test_texts': test_texts,\n            'train_labels': train_labels,\n            'val_labels': val_labels,\n            'test_labels': test_labels}\n\ndef convert_dataset(filepath, extend_with, vocab):\n    print('-- Generating {} '.format(filepath))\n    sys.stdout.flush()\n    st = SentenceTokenizer(vocab, maxlen)\n    tokenized, dicts, _ = st.split_train_val_test(texts,\n                                                  labels,\n                                                  [data['train_ind'],\n                                                   data['val_ind'],\n                                                   data['test_ind']],\n                                                  extend_with=extend_with)\n    pick = format_pickle(dset, tokenized[0], tokenized[1], tokenized[2],\n                        dicts[0], dicts[1], dicts[2])\n    with open(filepath, 'w') as f:\n        pickle.dump(pick, f)\n    cover = coverage(tokenized[2])\n\n    print('     done. Coverage: {}'.format(cover))\n\nwith open('../model/vocabulary.json', 'r') as f:\n    vocab = json.load(f)\n\nfor dset in DATASETS:\n    print('Converting {}'.format(dset))\n\n    PATH_RAW = '{}/{}/{}'.format(DIR, dset, FILENAME_RAW)\n    PATH_OWN = '{}/{}/{}'.format(DIR, dset, FILENAME_OWN)\n    PATH_OUR = '{}/{}/{}'.format(DIR, dset, FILENAME_OUR)\n    PATH_COMBINED = '{}/{}/{}'.format(DIR, dset, FILENAME_COMBINED)\n\n    with open(PATH_RAW, 'rb') as dataset:\n        if IS_PYTHON2:\n            data = pickle.load(dataset)\n        else:\n            data = pickle.load(dataset, fix_imports=True)\n\n    # Decode data\n    try:\n        texts = [unicode(x) for x in data['texts']]\n    except UnicodeDecodeError:\n        texts = [x.decode('utf-8') for x in data['texts']]\n\n    wg = WordGenerator(texts)\n    vb = VocabBuilder(wg)\n    vb.count_all_words()\n\n    # Calculate max length of sequences considered\n    # Adjust batch_size accordingly to prevent GPU overflow\n    lengths = [len(tokenize(t)) for t in texts]\n    maxlen = roundup(np.percentile(lengths, 80.0))\n\n    # Extract labels\n    labels = [x['label'] for x in data['info']]\n\n    convert_dataset(PATH_OWN, 50000, {})\n    convert_dataset(PATH_OUR, 0, vocab)\n    convert_dataset(PATH_COMBINED, 10000, vocab)\n"""
scripts/download_weights.py,0,"b'from __future__ import print_function\nimport os\nfrom subprocess import call\nfrom builtins import input\n\ncurr_folder = os.path.basename(os.path.normpath(os.getcwd()))\n\nweights_filename = \'pytorch_model.bin\'\nweights_folder = \'model\'\nweights_path = \'{}/{}\'.format(weights_folder, weights_filename)\nif curr_folder == \'scripts\':\n    weights_path = \'../\' + weights_path\nweights_download_link = \'https://www.dropbox.com/s/q8lax9ary32c7t9/pytorch_model.bin?dl=0#\'\n\n\nMB_FACTOR = float(1<<20)\n\ndef prompt():\n    while True:\n        valid = {\n            \'y\': True,\n            \'ye\': True,\n            \'yes\': True,\n            \'n\': False,\n            \'no\': False,\n        }\n        choice = input().lower()\n        if choice in valid:\n            return valid[choice]\n        else:\n            print(\'Please respond with \\\'y\\\' or \\\'n\\\' (or \\\'yes\\\' or \\\'no\\\')\')\n\ndownload = True\nif os.path.exists(weights_path):\n    print(\'Weight file already exists at {}. Would you like to redownload it anyway? [y/n]\'.format(weights_path))\n    download = prompt()\n    already_exists = True\nelse:\n    already_exists = False\n\nif download:\n    print(\'About to download the pretrained weights file from {}\'.format(weights_download_link))\n    if already_exists == False:\n        print(\'The size of the file is roughly 85MB. Continue? [y/n]\')\n    else:\n        os.unlink(weights_path)\n\n    if already_exists or prompt():\n        print(\'Downloading...\')\n\n        #urllib.urlretrieve(weights_download_link, weights_path)\n        #with open(weights_path,\'wb\') as f:\n        #    f.write(requests.get(weights_download_link).content)\n\n        # downloading using wget due to issues with urlretrieve and requests\n        sys_call = \'wget {} -O {}\'.format(weights_download_link, os.path.abspath(weights_path))\n        print(""Running system call: {}"".format(sys_call))\n        call(sys_call, shell=True)\n\n        if os.path.getsize(weights_path) / MB_FACTOR < 80:\n            raise ValueError(""Download finished, but the resulting file is too small! "" +\n                             ""It\\\'s only {} bytes."".format(os.path.getsize(weights_path)))\n        print(\'Downloaded weights to {}\'.format(weights_path))\nelse:\n    print(\'Exiting.\')\n'"
scripts/finetune_dataset.py,0,"b'"""""" Finetuning example.\n""""""\nfrom __future__ import print_function\nimport sys\nimport numpy as np\nfrom os.path import abspath, dirname\nsys.path.insert(0, dirname(dirname(abspath(__file__))))\n\nimport json\nimport math\nfrom torchmoji.model_def import torchmoji_transfer\nfrom torchmoji.global_variables import PRETRAINED_PATH, VOCAB_PATH\nfrom torchmoji.finetuning import (\n     load_benchmark,\n     finetune)\nfrom torchmoji.class_avg_finetuning import class_avg_finetune\n\ndef roundup(x):\n    return int(math.ceil(x / 10.0)) * 10\n\n\n# Format: (dataset_name,\n#          path_to_dataset,\n#          nb_classes,\n#          use_f1_score)\nDATASETS = [\n     #(\'SE0714\', \'../data/SE0714/raw.pickle\', 3, True),\n     #(\'Olympic\', \'../data/Olympic/raw.pickle\', 4, True),\n     #(\'PsychExp\', \'../data/PsychExp/raw.pickle\', 7, True),\n     #(\'SS-Twitter\', \'../data/SS-Twitter/raw.pickle\', 2, False),\n     (\'SS-Youtube\', \'../data/SS-Youtube/raw.pickle\', 2, False),\n     #(\'SE1604\', \'../data/SE1604/raw.pickle\', 3, False), # Excluded due to Twitter\'s ToS\n     #(\'SCv1\', \'../data/SCv1/raw.pickle\', 2, True),\n     #(\'SCv2-GEN\', \'../data/SCv2-GEN/raw.pickle\', 2, True)\n      ]\n\nRESULTS_DIR = \'results\'\n\n# \'new\' | \'last\' | \'full\' | \'chain-thaw\'\nFINETUNE_METHOD = \'last\'\nVERBOSE = 1\n\nnb_tokens = 50000\nnb_epochs = 1000\nepoch_size = 1000\n\nwith open(VOCAB_PATH, \'r\') as f:\n    vocab = json.load(f)\n\nfor rerun_iter in range(5):\n    for p in DATASETS:\n\n        # debugging\n        assert len(vocab) == nb_tokens\n\n        dset = p[0]\n        path = p[1]\n        nb_classes = p[2]\n        use_f1_score = p[3]\n\n        if FINETUNE_METHOD == \'last\':\n            extend_with = 0\n        elif FINETUNE_METHOD in [\'new\', \'full\', \'chain-thaw\']:\n            extend_with = 10000\n        else:\n            raise ValueError(\'Finetuning method not recognised!\')\n\n        # Load dataset.\n        data = load_benchmark(path, vocab, extend_with=extend_with)\n\n        (X_train, y_train) = (data[\'texts\'][0], data[\'labels\'][0])\n        (X_val, y_val) = (data[\'texts\'][1], data[\'labels\'][1])\n        (X_test, y_test) = (data[\'texts\'][2], data[\'labels\'][2])\n\n        weight_path = PRETRAINED_PATH if FINETUNE_METHOD != \'new\' else None\n        nb_model_classes = 2 if use_f1_score else nb_classes\n        model = torchmoji_transfer(\n                    nb_model_classes,\n                    weight_path,\n                    extend_embedding=data[\'added\'])\n        print(model)\n\n        # Training\n        print(\'Training: {}\'.format(path))\n        if use_f1_score:\n            model, result = class_avg_finetune(model, data[\'texts\'],\n                                               data[\'labels\'],\n                                               nb_classes, data[\'batch_size\'],\n                                               FINETUNE_METHOD,\n                                               verbose=VERBOSE)\n        else:\n            model, result = finetune(model, data[\'texts\'], data[\'labels\'],\n                                     nb_classes, data[\'batch_size\'],\n                                     FINETUNE_METHOD, metric=\'acc\',\n                                     verbose=VERBOSE)\n\n        # Write results\n        if use_f1_score:\n            print(\'Overall F1 score (dset = {}): {}\'.format(dset, result))\n            with open(\'{}/{}_{}_{}_results.txt\'.\n                      format(RESULTS_DIR, dset, FINETUNE_METHOD, rerun_iter),\n                      ""w"") as f:\n                f.write(""F1: {}\\n"".format(result))\n        else:\n            print(\'Test accuracy (dset = {}): {}\'.format(dset, result))\n            with open(\'{}/{}_{}_{}_results.txt\'.\n                      format(RESULTS_DIR, dset, FINETUNE_METHOD, rerun_iter),\n                      ""w"") as f:\n                f.write(""Acc: {}\\n"".format(result))\n'"
tests/test_finetuning.py,1,"b'from __future__ import absolute_import, print_function, division, unicode_literals\n\nimport test_helper\n\nfrom nose.plugins.attrib import attr\nimport json\nimport numpy as np\n\nfrom torchmoji.class_avg_finetuning import relabel\nfrom torchmoji.sentence_tokenizer import SentenceTokenizer\n\nfrom torchmoji.finetuning import (\n    calculate_batchsize_maxlen,\n    freeze_layers,\n    change_trainable,\n    finetune,\n    load_benchmark\n    )\nfrom torchmoji.model_def import (\n    torchmoji_transfer,\n    torchmoji_feature_encoding,\n    torchmoji_emojis\n    )\nfrom torchmoji.global_variables import (\n    PRETRAINED_PATH,\n    NB_TOKENS,\n    VOCAB_PATH,\n    ROOT_PATH\n    )\n\n\ndef test_calculate_batchsize_maxlen():\n    """""" Batch size and max length are calculated properly.\n    """"""\n    texts = [\'a b c d\',\n             \'e f g h i\']\n    batch_size, maxlen = calculate_batchsize_maxlen(texts)\n\n    assert batch_size == 250\n    assert maxlen == 10, maxlen\n\n\ndef test_freeze_layers():\n    """""" Correct layers are frozen.\n    """"""\n    model = torchmoji_transfer(5)\n    keyword = \'output_layer\'\n\n    model = freeze_layers(model, unfrozen_keyword=keyword)\n\n    for name, module in model.named_children():\n        trainable = keyword.lower() in name.lower()\n        assert all(p.requires_grad == trainable for p in module.parameters())\n\n\ndef test_change_trainable():\n    """""" change_trainable() changes trainability of layers.\n    """"""\n    model = torchmoji_transfer(5)\n    change_trainable(model.embed, False)\n    assert not any(p.requires_grad for p in model.embed.parameters())\n    change_trainable(model.embed, True)\n    assert all(p.requires_grad for p in model.embed.parameters())\n\n\ndef test_torchmoji_transfer_extend_embedding():\n    """""" Defining torchmoji with extension.\n    """"""\n    extend_with = 50\n    model = torchmoji_transfer(5, weight_path=PRETRAINED_PATH,\n                              extend_embedding=extend_with)\n    embedding_layer = model.embed\n    assert embedding_layer.weight.size()[0] == NB_TOKENS + extend_with\n\n\ndef test_torchmoji_return_attention():\n    seq_tensor = np.array([[1]])\n    # test the output of the normal model\n    model = torchmoji_emojis(weight_path=PRETRAINED_PATH)\n    # check correct number of outputs\n    assert len(model(seq_tensor)) == 1\n    # repeat above described tests when returning attention weights\n    model = torchmoji_emojis(weight_path=PRETRAINED_PATH, return_attention=True)\n    assert len(model(seq_tensor)) == 2\n\n\ndef test_relabel():\n    """""" relabel() works with multi-class labels.\n    """"""\n    nb_classes = 3\n    inputs = np.array([\n        [True, False, False],\n        [False, True, False],\n        [True, False, True],\n    ])\n    expected_0 = np.array([True, False, True])\n    expected_1 = np.array([False, True, False])\n    expected_2 = np.array([False, False, True])\n\n    assert np.array_equal(relabel(inputs, 0, nb_classes), expected_0)\n    assert np.array_equal(relabel(inputs, 1, nb_classes), expected_1)\n    assert np.array_equal(relabel(inputs, 2, nb_classes), expected_2)\n\n\ndef test_relabel_binary():\n    """""" relabel() works with binary classification (no changes to labels)\n    """"""\n    nb_classes = 2\n    inputs = np.array([True, False, False])\n\n    assert np.array_equal(relabel(inputs, 0, nb_classes), inputs)\n\n\n@attr(\'slow\')\ndef test_finetune_full():\n    """""" finetuning using \'full\'.\n    """"""\n    DATASET_PATH = ROOT_PATH+\'/data/SS-Youtube/raw.pickle\'\n    nb_classes = 2\n    # Keras and pyTorch implementation of the Adam optimizer are slightly different and change a bit the results\n    # We reduce the min accuracy needed here to pass the test\n    # See e.g. https://discuss.pytorch.org/t/suboptimal-convergence-when-compared-with-tensorflow-model/5099/11\n    min_acc = 0.68\n\n    with open(VOCAB_PATH, \'r\') as f:\n        vocab = json.load(f)\n\n    data = load_benchmark(DATASET_PATH, vocab, extend_with=10000)\n    print(\'Loading pyTorch model from {}.\'.format(PRETRAINED_PATH))\n    model = torchmoji_transfer(nb_classes, PRETRAINED_PATH, extend_embedding=data[\'added\'])\n    print(model)\n    model, acc = finetune(model, data[\'texts\'], data[\'labels\'], nb_classes,\n                          data[\'batch_size\'], method=\'full\', nb_epochs=1)\n\n    print(""Finetune full SS-Youtube 1 epoch acc: {}"".format(acc))\n    assert acc >= min_acc\n\n\n@attr(\'slow\')\ndef test_finetune_last():\n    """""" finetuning using \'last\'.\n    """"""\n    dataset_path = ROOT_PATH + \'/data/SS-Youtube/raw.pickle\'\n    nb_classes = 2\n    min_acc = 0.68\n\n    with open(VOCAB_PATH, \'r\') as f:\n        vocab = json.load(f)\n\n    data = load_benchmark(dataset_path, vocab)\n    print(\'Loading model from {}.\'.format(PRETRAINED_PATH))\n    model = torchmoji_transfer(nb_classes, PRETRAINED_PATH)\n    print(model)\n    model, acc = finetune(model, data[\'texts\'], data[\'labels\'], nb_classes,\n                          data[\'batch_size\'], method=\'last\', nb_epochs=1)\n\n    print(""Finetune last SS-Youtube 1 epoch acc: {}"".format(acc))\n\n    assert acc >= min_acc\n\n\ndef test_score_emoji():\n    """""" Emoji predictions make sense.\n    """"""\n    test_sentences = [\n        \'I love mom\\\'s cooking\',\n        \'I love how you never reply back..\',\n        \'I love cruising with my homies\',\n        \'I love messing with yo mind!!\',\n        \'I love you and now you\\\'re just gone..\',\n        \'This is shit\',\n        \'This is the shit\'\n    ]\n\n    expected = [\n        np.array([36,  4,  8, 16, 47]),\n        np.array([1, 19, 55, 25, 46]),\n        np.array([31,  6, 30, 15, 13]),\n        np.array([54, 44,  9, 50, 49]),\n        np.array([46,  5, 27, 35, 34]),\n        np.array([55, 32, 27,  1, 37]),\n        np.array([48, 11,  6, 31,  9])\n    ]\n\n    def top_elements(array, k):\n        ind = np.argpartition(array, -k)[-k:]\n        return ind[np.argsort(array[ind])][::-1]\n\n    # Initialize by loading dictionary and tokenize texts\n    with open(VOCAB_PATH, \'r\') as f:\n        vocabulary = json.load(f)\n\n    st = SentenceTokenizer(vocabulary, 30)\n    tokens, _, _ = st.tokenize_sentences(test_sentences)\n\n    # Load model and run\n    model = torchmoji_emojis(weight_path=PRETRAINED_PATH)\n    prob = model(tokens)\n\n    # Find top emojis for each sentence\n    for i, t_prob in enumerate(list(prob)):\n        assert np.array_equal(top_elements(t_prob, 5), expected[i])\n\n\ndef test_encode_texts():\n    """""" Text encoding is stable.\n    """"""\n\n    TEST_SENTENCES = [\'I love mom\\\'s cooking\',\n                      \'I love how you never reply back..\',\n                      \'I love cruising with my homies\',\n                      \'I love messing with yo mind!!\',\n                      \'I love you and now you\\\'re just gone..\',\n                      \'This is shit\',\n                      \'This is the shit\']\n\n\n    maxlen = 30\n    batch_size = 32\n\n    with open(VOCAB_PATH, \'r\') as f:\n        vocabulary = json.load(f)\n\n    st = SentenceTokenizer(vocabulary, maxlen)\n\n    print(\'Loading model from {}.\'.format(PRETRAINED_PATH))\n    model = torchmoji_feature_encoding(PRETRAINED_PATH)\n    print(model)\n    tokenized, _, _ = st.tokenize_sentences(TEST_SENTENCES)\n    encoding = model(tokenized)\n\n    avg_across_sentences = np.around(np.mean(encoding, axis=0)[:5], 3)\n    assert np.allclose(avg_across_sentences, np.array([-0.023, 0.021, -0.037, -0.001, -0.005]))\n\ntest_encode_texts()'"
tests/test_helper.py,0,"b'"""""" Module import helper.\nModifies PATH in order to allow us to import the torchmoji directory.\n""""""\nimport sys\nfrom os.path import abspath, dirname\nsys.path.insert(0, dirname(dirname(abspath(__file__))))\n'"
tests/test_sentence_tokenizer.py,0,"b'from __future__ import absolute_import, print_function, division, unicode_literals\nimport test_helper\nimport json\n\nfrom torchmoji.sentence_tokenizer import SentenceTokenizer\n\nsentences = [\'A\', \'B\', \'C\', \'D\', \'E\', \'F\', \'G\', \'H\', \'I\', \'J\']\n\ndicts = [\n    {\'label\': 0},\n    {\'label\': 1},\n    {\'label\': 2},\n    {\'label\': 3},\n    {\'label\': 4},\n    {\'label\': 5},\n    {\'label\': 6},\n    {\'label\': 7},\n    {\'label\': 8},\n    {\'label\': 9},\n    ]\n\ntrain_ind = [0, 5, 3, 6, 8]\nval_ind = [9, 2, 1]\ntest_ind = [4, 7]\n\nwith open(\'../model/vocabulary.json\', \'r\') as f:\n    vocab = json.load(f)\n\ndef test_dataset_split_parameter():\n    """""" Dataset is split in the desired ratios\n    """"""\n    split_parameter = [0.7, 0.1, 0.2]\n    st = SentenceTokenizer(vocab, 30)\n\n    result, result_dicts, _ = st.split_train_val_test(sentences, dicts,\n                                               split_parameter, extend_with=0)\n    train = result[0]\n    val = result[1]\n    test = result[2]\n\n    train_dicts = result_dicts[0]\n    val_dicts = result_dicts[1]\n    test_dicts = result_dicts[2]\n\n    assert len(train) == len(sentences) * split_parameter[0]\n    assert len(val) == len(sentences) * split_parameter[1]\n    assert len(test) == len(sentences) * split_parameter[2]\n\n    assert len(train_dicts) == len(dicts) * split_parameter[0]\n    assert len(val_dicts) == len(dicts) * split_parameter[1]\n    assert len(test_dicts) == len(dicts) * split_parameter[2]\n\ndef test_dataset_split_explicit():\n    """""" Dataset is split according to given indices\n    """"""\n    split_parameter = [train_ind, val_ind, test_ind]\n    st = SentenceTokenizer(vocab, 30)\n    tokenized, _, _ = st.tokenize_sentences(sentences)\n\n    result, result_dicts, added = st.split_train_val_test(sentences, dicts, split_parameter, extend_with=0)\n    train = result[0]\n    val = result[1]\n    test = result[2]\n\n    train_dicts = result_dicts[0]\n    val_dicts = result_dicts[1]\n    test_dicts = result_dicts[2]\n\n    tokenized = tokenized\n\n    for i, sentence in enumerate(sentences):\n        if i in train_ind:\n            assert tokenized[i] in train\n            assert dicts[i] in train_dicts\n        elif i in val_ind:\n            assert tokenized[i] in val\n            assert dicts[i] in val_dicts\n        elif i in test_ind:\n            assert tokenized[i] in test\n            assert dicts[i] in test_dicts\n\n    assert len(train) == len(train_ind)\n    assert len(val) == len(val_ind)\n    assert len(test) == len(test_ind)\n    assert len(train_dicts) == len(train_ind)\n    assert len(val_dicts) == len(val_ind)\n    assert len(test_dicts) == len(test_ind)\n\ndef test_id_to_sentence():\n    """"""Tokenizing and converting back preserves the input.\n    """"""\n    vb = {\'CUSTOM_MASK\': 0,\n          \'aasdf\': 1000,\n          \'basdf\': 2000}\n\n    sentence = \'aasdf basdf basdf basdf\'\n    st = SentenceTokenizer(vb, 30)\n    token, _, _ = st.tokenize_sentences([sentence])\n    assert st.to_sentence(token[0]) == sentence\n\ndef test_id_to_sentence_with_unknown():\n    """"""Tokenizing and converting back preserves the input, except for unknowns.\n    """"""\n    vb = {\'CUSTOM_MASK\': 0,\n          \'CUSTOM_UNKNOWN\': 1,\n          \'aasdf\': 1000,\n          \'basdf\': 2000}\n\n    sentence = \'aasdf basdf ccc\'\n    expected = \'aasdf basdf CUSTOM_UNKNOWN\'\n    st = SentenceTokenizer(vb, 30)\n    token, _, _ = st.tokenize_sentences([sentence])\n    assert st.to_sentence(token[0]) == expected\n'"
tests/test_tokenizer.py,0,"b'# -*- coding: utf-8 -*-\n"""""" Tokenization tests.\n""""""\nfrom __future__ import absolute_import, print_function, division, unicode_literals\n\nimport sys\nfrom nose.tools import nottest\nfrom os.path import dirname, abspath\nsys.path.append(dirname(dirname(abspath(__file__))))\nfrom torchmoji.tokenizer import tokenize\n\nTESTS_NORMAL = [\n    (\'200K words!\', [\'200\', \'K\', \'words\', \'!\']),\n]\n\nTESTS_EMOJIS = [\n    (\'i \\U0001f496 you to the moon and back\',\n     [\'i\', \'\\U0001f496\', \'you\', \'to\', \'the\', \'moon\', \'and\', \'back\']),\n    (""i\\U0001f496you to the \\u2605\'s and back"",\n     [\'i\', \'\\U0001f496\', \'you\', \'to\', \'the\',\n      \'\\u2605\', ""\'"", \'s\', \'and\', \'back\']),\n    (\'~<3~\', [\'~\', \'<3\', \'~\']),\n    (\'<333\', [\'<333\']),\n    (\':-)\', [\':-)\']),\n    (\'>:-(\', [\'>:-(\']),\n    (\'\\u266b\\u266a\\u2605\\u2606\\u2665\\u2764\\u2661\',\n     [\'\\u266b\', \'\\u266a\', \'\\u2605\', \'\\u2606\',\n      \'\\u2665\', \'\\u2764\', \'\\u2661\']),\n]\n\nTESTS_URLS = [\n    (\'www.sample.com\', [\'www.sample.com\']),\n    (\'http://endless.horse\', [\'http://endless.horse\']),\n    (\'https://github.mit.ed\', [\'https://github.mit.ed\']),\n]\n\nTESTS_TWITTER = [\n    (\'#blacklivesmatter\', [\'#blacklivesmatter\']),\n    (\'#99_percent.\', [\'#99_percent\', \'.\']),\n    (\'the#99%\', [\'the\', \'#99\', \'%\']),\n    (\'@golden_zenith\', [\'@golden_zenith\']),\n    (\'@99_percent\', [\'@99_percent\']),\n    (\'latte-express@mit.ed\', [\'latte-express@mit.ed\']),\n]\n\nTESTS_PHONE_NUMS = [\n    (\'518)528-0252\', [\'518\', \')\', \'528\', \'-\', \'0252\']),\n    (\'1200-0221-0234\', [\'1200\', \'-\', \'0221\', \'-\', \'0234\']),\n    (\'1200.0221.0234\', [\'1200\', \'.\', \'0221\', \'.\', \'0234\']),\n]\n\nTESTS_DATETIME = [\n    (\'15:00\', [\'15\', \':\', \'00\']),\n    (\'2:00pm\', [\'2\', \':\', \'00\', \'pm\']),\n    (\'9/14/16\', [\'9\', \'/\', \'14\', \'/\', \'16\']),\n]\n\nTESTS_CURRENCIES = [\n    (\'517.933\\xa3\', [\'517\', \'.\', \'933\', \'\\xa3\']),\n    (\'$517.87\', [\'$\', \'517\', \'.\', \'87\']),\n    (\'1201.6598\', [\'1201\', \'.\', \'6598\']),\n    (\'120,6\', [\'120\', \',\', \'6\']),\n    (\'10,00\\u20ac\', [\'10\', \',\', \'00\', \'\\u20ac\']),\n    (\'1,000\', [\'1\', \',\', \'000\']),\n    (\'1200pesos\', [\'1200\', \'pesos\']),\n]\n\nTESTS_NUM_SYM = [\n    (\'5162f\', [\'5162\', \'f\']),\n    (\'f5162\', [\'f\', \'5162\']),\n    (\'1203(\', [\'1203\', \'(\']),\n    (\'(1203)\', [\'(\', \'1203\', \')\']),\n    (\'1200/\', [\'1200\', \'/\']),\n    (\'1200+\', [\'1200\', \'+\']),\n    (\'1202o-east\', [\'1202\', \'o-east\']),\n    (\'1200r\', [\'1200\', \'r\']),\n    (\'1200-1400\', [\'1200\', \'-\', \'1400\']),\n    (\'120/today\', [\'120\', \'/\', \'today\']),\n    (\'today/120\', [\'today\', \'/\', \'120\']),\n    (\'120/5\', [\'120\', \'/\', \'5\']),\n    (""120\'/5"", [\'120\', ""\'"", \'/\', \'5\']),\n    (\'120/5pro\', [\'120\', \'/\', \'5\', \'pro\']),\n    (""1200\'s,)"", [\'1200\', ""\'"", \'s\', \',\', \')\']),\n    (\'120.76.218.207\', [\'120\', \'.\', \'76\', \'.\', \'218\', \'.\', \'207\']),\n]\n\nTESTS_PUNCTUATION = [\n    (""don\'\'t"", [\'don\', ""\'\'"", \'t\']),\n    (""don\'tcha"", [""don\'tcha""]),\n    (\'no?!?!;\', [\'no\', \'?\', \'!\', \'?\', \'!\', \';\']),\n    (\'no??!!..\', [\'no\', \'??\', \'!!\', \'..\']),\n    (\'a.m.\', [\'a.m.\']),\n    (\'.s.u\', [\'.\', \'s\', \'.\', \'u\']),\n    (\'!!i..n__\', [\'!!\', \'i\', \'..\', \'n\', \'__\']),\n    (\'lv(<3)w(3>)u Mr.!\', [\'lv\', \'(\', \'<3\', \')\', \'w\', \'(\', \'3\',\n                            \'>\', \')\', \'u\', \'Mr.\', \'!\']),\n    (\'-->\', [\'--\', \'>\']),\n    (\'->\', [\'-\', \'>\']),\n    (\'<-\', [\'<\', \'-\']),\n    (\'<--\', [\'<\', \'--\']),\n    (\'hello (@person)\', [\'hello\', \'(\', \'@person\', \')\']),\n]\n\n\ndef test_normal():\n    """""" Normal/combined usage.\n    """"""\n    test_base(TESTS_NORMAL)\n\n\ndef test_emojis():\n    """""" Tokenizing emojis/emoticons/decorations.\n    """"""\n    test_base(TESTS_EMOJIS)\n\n\ndef test_urls():\n    """""" Tokenizing URLs.\n    """"""\n    test_base(TESTS_URLS)\n\n\ndef test_twitter():\n    """""" Tokenizing hashtags, mentions and emails.\n    """"""\n    test_base(TESTS_TWITTER)\n\n\ndef test_phone_nums():\n    """""" Tokenizing phone numbers.\n    """"""\n    test_base(TESTS_PHONE_NUMS)\n\n\ndef test_datetime():\n    """""" Tokenizing dates and times.\n    """"""\n    test_base(TESTS_DATETIME)\n\n\ndef test_currencies():\n    """""" Tokenizing currencies.\n    """"""\n    test_base(TESTS_CURRENCIES)\n\n\ndef test_num_sym():\n    """""" Tokenizing combinations of numbers and symbols.\n    """"""\n    test_base(TESTS_NUM_SYM)\n\n\ndef test_punctuation():\n    """""" Tokenizing punctuation and contractions.\n    """"""\n    test_base(TESTS_PUNCTUATION)\n\n\n@nottest\ndef test_base(tests):\n    """""" Base function for running tests.\n    """"""\n    for (test, expected) in tests:\n        actual = tokenize(test)\n        assert actual == expected, \\\n            ""Tokenization of \\\'{}\\\' failed, expected: {}, actual: {}""\\\n            .format(test, expected, actual)\n'"
tests/test_word_generator.py,0,"b'# -*- coding: utf-8 -*-\nimport sys\nfrom os.path import dirname, abspath\nsys.path.append(dirname(dirname(abspath(__file__))))\nfrom nose.tools import raises\nfrom torchmoji.word_generator import WordGenerator\n\nIS_PYTHON2 = int(sys.version[0]) == 2\n\n@raises(ValueError)\ndef test_only_unicode_accepted():\n    """""" Non-Unicode strings raise a ValueError.\n        In Python 3 all string are Unicode\n    """"""\n    if not IS_PYTHON2:\n        raise ValueError(""You are using python 3 so this test should always pass"")\n\n    sentences = [\n        u\'Hello world\',\n        u\'I am unicode\',\n        \'I am not unicode\',\n        ]\n\n    wg = WordGenerator(sentences)\n    for w in wg:\n        pass\n\n\ndef test_unicode_sentences_ignored_if_set():\n    """""" Strings with Unicode characters tokenize to empty array if they\'re not allowed.\n    """"""\n    sentence = [u\'Dobr\xc3\xbd den, jak se m\xc3\xa1\xc5\xa1?\']\n    wg = WordGenerator(sentence, allow_unicode_text=False)\n    assert wg.get_words(sentence[0]) == []\n\n\ndef test_check_ascii():\n    """""" check_ascii recognises ASCII words properly.\n        In Python 3 all string are Unicode\n    """"""\n    if not IS_PYTHON2:\n        return\n\n    wg = WordGenerator([])\n    assert wg.check_ascii(\'ASCII\')\n    assert not wg.check_ascii(\'\xc5\xa1\xc4\x8d\xc5\x99\xc5\xbe\xc3\xbd\xc3\xa1\')\n    assert not wg.check_ascii(\'\xe2\x9d\xa4 \xe2\x98\x80 \xe2\x98\x86 \xe2\x98\x82 \xe2\x98\xbb \xe2\x99\x9e \xe2\x98\xaf \xe2\x98\xad \xe2\x98\xa2\')\n\n\ndef test_convert_unicode_word():\n    """""" convert_unicode_word converts Unicode words correctly.\n    """"""\n    wg = WordGenerator([], allow_unicode_text=True)\n\n    result = wg.convert_unicode_word(u\'\xc4\x8d\')\n    assert result == (True, u\'\\u010d\'), \'{}\'.format(result)\n\n\ndef test_convert_unicode_word_ignores_if_set():\n    """""" convert_unicode_word ignores Unicode words if set.\n    """"""\n    wg = WordGenerator([], allow_unicode_text=False)\n\n    result = wg.convert_unicode_word(u\'\xc4\x8d\')\n    assert result == (False, \'\'), \'{}\'.format(result)\n\n\ndef test_convert_unicode_chars():\n    """""" convert_unicode_word correctly converts accented characters.\n    """"""\n    wg = WordGenerator([], allow_unicode_text=True)\n    result = wg.convert_unicode_word(u\'\xc4\x9b\xc5\xa1\xc4\x8d\xc5\x99\xc5\xbe\xc3\xbd\xc3\xa1\xc3\xad\xc3\xa9\')\n    assert result == (True, u\'\\u011b\\u0161\\u010d\\u0159\\u017e\\xfd\\xe1\\xed\\xe9\'), \'{}\'.format(result)\n'"
torchmoji/__init__.py,0,b''
torchmoji/attlayer.py,8,"b'# -*- coding: utf-8 -*-\n"""""" Define the Attention Layer of the model.\n""""""\n\nfrom __future__ import print_function, division\n\nimport torch\n\nfrom torch.autograd import Variable\nfrom torch.nn import Module\nfrom torch.nn.parameter import Parameter\n\nclass Attention(Module):\n    """"""\n    Computes a weighted average of the different channels across timesteps.\n    Uses 1 parameter pr. channel to compute the attention value for a single timestep.\n    """"""\n\n    def __init__(self, attention_size, return_attention=False):\n        """""" Initialize the attention layer\n\n        # Arguments:\n            attention_size: Size of the attention vector.\n            return_attention: If true, output will include the weight for each input token\n                              used for the prediction\n\n        """"""\n        super(Attention, self).__init__()\n        self.return_attention = return_attention\n        self.attention_size = attention_size\n        self.attention_vector = Parameter(torch.FloatTensor(attention_size))\n        self.attention_vector.data.normal_(std=0.05) # Initialize attention vector\n\n    def __repr__(self):\n        s = \'{name}({attention_size}, return attention={return_attention})\'\n        return s.format(name=self.__class__.__name__, **self.__dict__)\n\n    def forward(self, inputs, input_lengths):\n        """""" Forward pass.\n\n        # Arguments:\n            inputs (Torch.Variable): Tensor of input sequences\n            input_lengths (torch.LongTensor): Lengths of the sequences\n\n        # Return:\n            Tuple with (representations and attentions if self.return_attention else None).\n        """"""\n        logits = inputs.matmul(self.attention_vector)\n        unnorm_ai = (logits - logits.max()).exp()\n\n        # Compute a mask for the attention on the padded sequences\n        # See e.g. https://discuss.pytorch.org/t/self-attention-on-words-and-masking/5671/5\n        max_len = unnorm_ai.size(1)\n        idxes = torch.arange(0, max_len, out=torch.LongTensor(max_len)).unsqueeze(0)\n        mask = Variable((idxes < input_lengths.unsqueeze(1)).float())\n\n        # apply mask and renormalize attention scores (weights)\n        masked_weights = unnorm_ai * mask\n        att_sums = masked_weights.sum(dim=1, keepdim=True)  # sums per sequence\n        attentions = masked_weights.div(att_sums)\n\n        # apply attention weights\n        weighted = torch.mul(inputs, attentions.unsqueeze(-1).expand_as(inputs))\n\n        # get the final fixed vector representations of the sentences\n        representations = weighted.sum(dim=1)\n\n        return (representations, attentions if self.return_attention else None)\n'"
torchmoji/class_avg_finetuning.py,7,"b'# -*- coding: utf-8 -*-\n"""""" Class average finetuning functions. Before using any of these finetuning\n    functions, ensure that the model is set up with nb_classes=2.\n""""""\nfrom __future__ import print_function\n\nimport uuid\nfrom time import sleep\nimport numpy as np\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\n\nfrom torchmoji.global_variables import (\n    FINETUNING_METHODS,\n    WEIGHTS_DIR)\nfrom torchmoji.finetuning import (\n    freeze_layers,\n    get_data_loader,\n    fit_model,\n    train_by_chain_thaw,\n    find_f1_threshold)\n\ndef relabel(y, current_label_nr, nb_classes):\n    """""" Makes a binary classification for a specific class in a\n        multi-class dataset.\n\n    # Arguments:\n        y: Outputs to be relabelled.\n        current_label_nr: Current label number.\n        nb_classes: Total number of classes.\n\n    # Returns:\n        Relabelled outputs of a given multi-class dataset into a binary\n        classification dataset.\n    """"""\n\n    # Handling binary classification\n    if nb_classes == 2 and len(y.shape) == 1:\n        return y\n\n    y_new = np.zeros(len(y))\n    y_cut = y[:, current_label_nr]\n    label_pos = np.where(y_cut == 1)[0]\n    y_new[label_pos] = 1\n    return y_new\n\n\ndef class_avg_finetune(model, texts, labels, nb_classes, batch_size,\n                       method, epoch_size=5000, nb_epochs=1000, embed_l2=1E-6,\n                       verbose=True):\n    """""" Compiles and finetunes the given model.\n\n    # Arguments:\n        model: Model to be finetuned\n        texts: List of three lists, containing tokenized inputs for training,\n            validation and testing (in that order).\n        labels: List of three lists, containing labels for training,\n            validation and testing (in that order).\n        nb_classes: Number of classes in the dataset.\n        batch_size: Batch size.\n        method: Finetuning method to be used. For available methods, see\n            FINETUNING_METHODS in global_variables.py. Note that the model\n            should be defined accordingly (see docstring for torchmoji_transfer())\n        epoch_size: Number of samples in an epoch.\n        nb_epochs: Number of epochs. Doesn\'t matter much as early stopping is used.\n        embed_l2: L2 regularization for the embedding layer.\n        verbose: Verbosity flag.\n\n    # Returns:\n        Model after finetuning,\n        score after finetuning using the class average F1 metric.\n    """"""\n\n    if method not in FINETUNING_METHODS:\n        raise ValueError(\'ERROR (class_avg_tune_trainable): \'\n                         \'Invalid method parameter. \'\n                         \'Available options: {}\'.format(FINETUNING_METHODS))\n\n    (X_train, y_train) = (texts[0], labels[0])\n    (X_val, y_val) = (texts[1], labels[1])\n    (X_test, y_test) = (texts[2], labels[2])\n\n    checkpoint_path = \'{}/torchmoji-checkpoint-{}.bin\' \\\n                      .format(WEIGHTS_DIR, str(uuid.uuid4()))\n\n    f1_init_path = \'{}/torchmoji-f1-init-{}.bin\' \\\n                   .format(WEIGHTS_DIR, str(uuid.uuid4()))\n\n    if method in [\'last\', \'new\']:\n        lr = 0.001\n    elif method in [\'full\', \'chain-thaw\']:\n        lr = 0.0001\n\n    loss_op = nn.BCEWithLogitsLoss()\n\n    # Freeze layers if using last\n    if method == \'last\':\n        model = freeze_layers(model, unfrozen_keyword=\'output_layer\')\n\n    # Define optimizer, for chain-thaw we define it later (after freezing)\n    if method == \'last\':\n        adam = optim.Adam((p for p in model.parameters() if p.requires_grad), lr=lr)\n    elif method in [\'full\', \'new\']:\n        # Add L2 regulation on embeddings only\n        special_params = [id(p) for p in model.embed.parameters()]\n        base_params = [p for p in model.parameters() if id(p) not in special_params and p.requires_grad]\n        embed_parameters = [p for p in model.parameters() if id(p) in special_params and p.requires_grad]\n        adam = optim.Adam([\n            {\'params\': base_params},\n            {\'params\': embed_parameters, \'weight_decay\': embed_l2},\n            ], lr=lr)\n\n    # Training\n    if verbose:\n        print(\'Method:  {}\'.format(method))\n        print(\'Classes: {}\'.format(nb_classes))\n\n    if method == \'chain-thaw\':\n        result = class_avg_chainthaw(model, nb_classes=nb_classes,\n                                     loss_op=loss_op,\n                                     train=(X_train, y_train),\n                                     val=(X_val, y_val),\n                                     test=(X_test, y_test),\n                                     batch_size=batch_size,\n                                     epoch_size=epoch_size,\n                                     nb_epochs=nb_epochs,\n                                     checkpoint_weight_path=checkpoint_path,\n                                     f1_init_weight_path=f1_init_path,\n                                     verbose=verbose)\n    else:\n        result = class_avg_tune_trainable(model, nb_classes=nb_classes,\n                                          loss_op=loss_op,\n                                          optim_op=adam,\n                                          train=(X_train, y_train),\n                                          val=(X_val, y_val),\n                                          test=(X_test, y_test),\n                                          epoch_size=epoch_size,\n                                          nb_epochs=nb_epochs,\n                                          batch_size=batch_size,\n                                          init_weight_path=f1_init_path,\n                                          checkpoint_weight_path=checkpoint_path,\n                                          verbose=verbose)\n    return model, result\n\n\ndef prepare_labels(y_train, y_val, y_test, iter_i, nb_classes):\n    # Relabel into binary classification\n    y_train_new = relabel(y_train, iter_i, nb_classes)\n    y_val_new = relabel(y_val, iter_i, nb_classes)\n    y_test_new = relabel(y_test, iter_i, nb_classes)\n    return y_train_new, y_val_new, y_test_new\n\ndef prepare_generators(X_train, y_train_new, X_val, y_val_new, batch_size, epoch_size):\n    # Create sample generators\n    # Make a fixed validation set to avoid fluctuations in validation\n    train_gen = get_data_loader(X_train, y_train_new, batch_size,\n                                extended_batch_sampler=True)\n    val_gen = get_data_loader(X_val, y_val_new, epoch_size,\n                              extended_batch_sampler=True)\n    X_val_resamp, y_val_resamp = next(iter(val_gen))\n    return train_gen, X_val_resamp, y_val_resamp\n\n\ndef class_avg_tune_trainable(model, nb_classes, loss_op, optim_op, train, val, test,\n                             epoch_size, nb_epochs, batch_size,\n                             init_weight_path, checkpoint_weight_path, patience=5,\n                             verbose=True):\n    """""" Finetunes the given model using the F1 measure.\n\n    # Arguments:\n        model: Model to be finetuned.\n        nb_classes: Number of classes in the given dataset.\n        train: Training data, given as a tuple of (inputs, outputs)\n        val: Validation data, given as a tuple of (inputs, outputs)\n        test: Testing data, given as a tuple of (inputs, outputs)\n        epoch_size: Number of samples in an epoch.\n        nb_epochs: Number of epochs.\n        batch_size: Batch size.\n        init_weight_path: Filepath where weights will be initially saved before\n            training each class. This file will be rewritten by the function.\n        checkpoint_weight_path: Filepath where weights will be checkpointed to\n            during training. This file will be rewritten by the function.\n        verbose: Verbosity flag.\n\n    # Returns:\n        F1 score of the trained model\n    """"""\n    total_f1 = 0\n    nb_iter = nb_classes if nb_classes > 2 else 1\n\n    # Unpack args\n    X_train, y_train = train\n    X_val, y_val = val\n    X_test, y_test = test\n\n    # Save and reload initial weights after running for\n    # each class to avoid learning across classes\n    torch.save(model.state_dict(), init_weight_path)\n    for i in range(nb_iter):\n        if verbose:\n            print(\'Iteration number {}/{}\'.format(i+1, nb_iter))\n\n        model.load_state_dict(torch.load(init_weight_path))\n        y_train_new, y_val_new, y_test_new = prepare_labels(y_train, y_val,\n                                                            y_test, i, nb_classes)\n        train_gen, X_val_resamp, y_val_resamp = \\\n            prepare_generators(X_train, y_train_new, X_val, y_val_new,\n                               batch_size, epoch_size)\n\n        if verbose:\n            print(""Training.."")\n        fit_model(model, loss_op, optim_op, train_gen, [(X_val_resamp, y_val_resamp)],\n                  nb_epochs, checkpoint_weight_path, patience, verbose=0)\n\n        # Reload the best weights found to avoid overfitting\n        # Wait a bit to allow proper closing of weights file\n        sleep(1)\n        model.load_state_dict(torch.load(checkpoint_weight_path))\n\n        # Evaluate\n        y_pred_val = model(X_val).cpu().numpy()\n        y_pred_test = model(X_test).cpu().numpy()\n\n        f1_test, best_t = find_f1_threshold(y_val_new, y_pred_val,\n                                            y_test_new, y_pred_test)\n        if verbose:\n            print(\'f1_test: {}\'.format(f1_test))\n            print(\'best_t:  {}\'.format(best_t))\n        total_f1 += f1_test\n\n    return total_f1 / nb_iter\n\n\ndef class_avg_chainthaw(model, nb_classes, loss_op, train, val, test, batch_size,\n                        epoch_size, nb_epochs, checkpoint_weight_path,\n                        f1_init_weight_path, patience=5,\n                        initial_lr=0.001, next_lr=0.0001, verbose=True):\n    """""" Finetunes given model using chain-thaw and evaluates using F1.\n        For a dataset with multiple classes, the model is trained once for\n        each class, relabeling those classes into a binary classification task.\n        The result is an average of all F1 scores for each class.\n\n    # Arguments:\n        model: Model to be finetuned.\n        nb_classes: Number of classes in the given dataset.\n        train: Training data, given as a tuple of (inputs, outputs)\n        val: Validation data, given as a tuple of (inputs, outputs)\n        test: Testing data, given as a tuple of (inputs, outputs)\n        batch_size: Batch size.\n        loss: Loss function to be used during training.\n        epoch_size: Number of samples in an epoch.\n        nb_epochs: Number of epochs.\n        checkpoint_weight_path: Filepath where weights will be checkpointed to\n            during training. This file will be rewritten by the function.\n        f1_init_weight_path: Filepath where weights will be saved to and\n            reloaded from before training each class. This ensures that\n            each class is trained independently. This file will be rewritten.\n        initial_lr: Initial learning rate. Will only be used for the first\n            training step (i.e. the softmax layer)\n        next_lr: Learning rate for every subsequent step.\n        seed: Random number generator seed.\n        verbose: Verbosity flag.\n\n    # Returns:\n        Averaged F1 score.\n    """"""\n\n    # Unpack args\n    X_train, y_train = train\n    X_val, y_val = val\n    X_test, y_test = test\n\n    total_f1 = 0\n    nb_iter = nb_classes if nb_classes > 2 else 1\n\n    torch.save(model.state_dict(), f1_init_weight_path)\n\n    for i in range(nb_iter):\n        if verbose:\n            print(\'Iteration number {}/{}\'.format(i+1, nb_iter))\n\n        model.load_state_dict(torch.load(f1_init_weight_path))\n        y_train_new, y_val_new, y_test_new = prepare_labels(y_train, y_val,\n                                                            y_test, i, nb_classes)\n        train_gen, X_val_resamp, y_val_resamp = \\\n                prepare_generators(X_train, y_train_new, X_val, y_val_new,\n                                   batch_size, epoch_size)\n\n        if verbose:\n            print(""Training.."")\n\n        # Train using chain-thaw\n        train_by_chain_thaw(model=model, train_gen=train_gen,\n                            val_gen=[(X_val_resamp, y_val_resamp)],\n                            loss_op=loss_op, patience=patience,\n                            nb_epochs=nb_epochs,\n                            checkpoint_path=checkpoint_weight_path,\n                            initial_lr=initial_lr, next_lr=next_lr,\n                            verbose=verbose)\n\n        # Evaluate\n        y_pred_val = model(X_val).cpu().numpy()\n        y_pred_test = model(X_test).cpu().numpy()\n\n        f1_test, best_t = find_f1_threshold(y_val_new, y_pred_val,\n                                            y_test_new, y_pred_test)\n\n        if verbose:\n            print(\'f1_test: {}\'.format(f1_test))\n            print(\'best_t:  {}\'.format(best_t))\n        total_f1 += f1_test\n\n    return total_f1 / nb_iter\n'"
torchmoji/create_vocab.py,0,"b'# -*- coding: utf-8 -*-\nfrom __future__ import print_function, division\n\nimport glob\nimport json\nimport uuid\nfrom copy import deepcopy\nfrom collections import defaultdict, OrderedDict\nimport numpy as np\n\nfrom torchmoji.filter_utils import is_special_token\nfrom torchmoji.word_generator import WordGenerator\nfrom torchmoji.global_variables import SPECIAL_TOKENS, VOCAB_PATH\n\nclass VocabBuilder():\n    """""" Create vocabulary with words extracted from sentences as fed from a\n        word generator.\n    """"""\n    def __init__(self, word_gen):\n        # initialize any new key with value of 0\n        self.word_counts = defaultdict(lambda: 0, {})\n        self.word_length_limit=30\n\n        for token in SPECIAL_TOKENS:\n            assert len(token) < self.word_length_limit\n            self.word_counts[token] = 0\n        self.word_gen = word_gen\n\n    def count_words_in_sentence(self, words):\n        """""" Generates word counts for all tokens in the given sentence.\n\n        # Arguments:\n            words: Tokenized sentence whose words should be counted.\n        """"""\n        for word in words:\n            if 0 < len(word) and len(word) <= self.word_length_limit:\n                try:\n                    self.word_counts[word] += 1\n                except KeyError:\n                    self.word_counts[word] = 1\n\n    def save_vocab(self, path=None):\n        """""" Saves the vocabulary into a file.\n\n        # Arguments:\n            path: Where the vocabulary should be saved. If not specified, a\n                  randomly generated filename is used instead.\n        """"""\n        dtype = ([(\'word\',\'|S{}\'.format(self.word_length_limit)),(\'count\',\'int\')])\n        np_dict = np.array(self.word_counts.items(), dtype=dtype)\n\n        # sort from highest to lowest frequency\n        np_dict[::-1].sort(order=\'count\')\n        data = np_dict\n\n        if path is None:\n            path = str(uuid.uuid4())\n\n        np.savez_compressed(path, data=data)\n        print(""Saved dict to {}"".format(path))\n\n    def get_next_word(self):\n        """""" Returns next tokenized sentence from the word geneerator.\n\n        # Returns:\n            List of strings, representing the next tokenized sentence.\n        """"""\n        return self.word_gen.__iter__().next()\n\n    def count_all_words(self):\n        """""" Generates word counts for all words in all sentences of the word\n            generator.\n        """"""\n        for words, _ in self.word_gen:\n            self.count_words_in_sentence(words)\n\nclass MasterVocab():\n    """""" Combines vocabularies.\n    """"""\n    def __init__(self):\n\n        # initialize custom tokens\n        self.master_vocab = {}\n\n    def populate_master_vocab(self, vocab_path, min_words=1, force_appearance=None):\n        """""" Populates the master vocabulary using all vocabularies found in the\n            given path. Vocabularies should be named *.npz. Expects the\n            vocabularies to be numpy arrays with counts. Normalizes the counts\n            and combines them.\n\n        # Arguments:\n            vocab_path: Path containing vocabularies to be combined.\n            min_words: Minimum amount of occurences a word must have in order\n                to be included in the master vocabulary.\n            force_appearance: Optional vocabulary filename that will be added\n                to the master vocabulary no matter what. This vocabulary must\n                be present in vocab_path.\n        """"""\n\n        paths = glob.glob(vocab_path + \'*.npz\')\n        sizes = {path: 0 for path in paths}\n        dicts = {path: {} for path in paths}\n\n        # set up and get sizes of individual dictionaries\n        for path in paths:\n            np_data = np.load(path)[\'data\']\n\n            for entry in np_data:\n                word, count = entry\n                if count < min_words:\n                    continue\n                if is_special_token(word):\n                    continue\n                dicts[path][word] = count\n\n            sizes[path] = sum(dicts[path].values())\n            print(\'Overall word count for {} -> {}\'.format(path, sizes[path]))\n            print(\'Overall word number for {} -> {}\'.format(path, len(dicts[path])))\n\n        vocab_of_max_size = max(sizes, key=sizes.get)\n        max_size = sizes[vocab_of_max_size]\n        print(\'Min: {}, {}, {}\'.format(sizes, vocab_of_max_size, max_size))\n\n        # can force one vocabulary to always be present\n        if force_appearance is not None:\n            force_appearance_path = [p for p in paths if force_appearance in p][0]\n            force_appearance_vocab = deepcopy(dicts[force_appearance_path])\n            print(force_appearance_path)\n        else:\n            force_appearance_path, force_appearance_vocab = None, None\n\n        # normalize word counts before inserting into master dict\n        for path in paths:\n            normalization_factor = max_size / sizes[path]\n            print(\'Norm factor for path {} -> {}\'.format(path, normalization_factor))\n\n            for word in dicts[path]:\n                if is_special_token(word):\n                    print(""SPECIAL - "", word)\n                    continue\n                normalized_count = dicts[path][word] * normalization_factor\n\n                # can force one vocabulary to always be present\n                if force_appearance_vocab is not None:\n                    try:\n                        force_word_count = force_appearance_vocab[word]\n                    except KeyError:\n                        continue\n                    #if force_word_count < 5:\n                        #continue\n\n                if word in self.master_vocab:\n                    self.master_vocab[word] += normalized_count\n                else:\n                    self.master_vocab[word] = normalized_count\n\n        print(\'Size of master_dict {}\'.format(len(self.master_vocab)))\n        print(""Hashes for master dict: {}"".format(\n            len([w for w in self.master_vocab if \'#\' in w[0]])))\n\n    def save_vocab(self, path_count, path_vocab, word_limit=100000):\n        """""" Saves the master vocabulary into a file.\n        """"""\n\n        # reserve space for 10 special tokens\n        words = OrderedDict()\n        for token in SPECIAL_TOKENS:\n            # store -1 instead of np.inf, which can overflow\n            words[token] = -1\n\n        # sort words by frequency\n        desc_order = OrderedDict(sorted(self.master_vocab.items(),\n                                 key=lambda kv: kv[1], reverse=True))\n        words.update(desc_order)\n\n        # use encoding of up to 30 characters (no token conversions)\n        # use float to store large numbers (we don\'t care about precision loss)\n        np_vocab = np.array(words.items(),\n                            dtype=([(\'word\',\'|S30\'),(\'count\',\'float\')]))\n\n        # output count for debugging\n        counts = np_vocab[:word_limit]\n        np.savez_compressed(path_count, counts=counts)\n\n        # output the index of each word for easy lookup\n        final_words = OrderedDict()\n        for i, w in enumerate(words.keys()[:word_limit]):\n            final_words.update({w:i})\n        with open(path_vocab, \'w\') as f:\n            f.write(json.dumps(final_words, indent=4, separators=(\',\', \': \')))\n\n\ndef all_words_in_sentences(sentences):\n    """""" Extracts all unique words from a given list of sentences.\n\n    # Arguments:\n        sentences: List or word generator of sentences to be processed.\n\n    # Returns:\n        List of all unique words contained in the given sentences.\n    """"""\n    vocab = []\n    if isinstance(sentences, WordGenerator):\n        sentences = [s for s, _ in sentences]\n\n    for sentence in sentences:\n        for word in sentence:\n            if word not in vocab:\n                vocab.append(word)\n\n    return vocab\n\n\ndef extend_vocab_in_file(vocab, max_tokens=10000, vocab_path=VOCAB_PATH):\n    """""" Extends JSON-formatted vocabulary with words from vocab that are not\n        present in the current vocabulary. Adds up to max_tokens words.\n        Overwrites file in vocab_path.\n\n    # Arguments:\n        new_vocab: Vocabulary to be added. MUST have word_counts populated, i.e.\n            must have run count_all_words() previously.\n        max_tokens: Maximum number of words to be added.\n        vocab_path: Path to the vocabulary json which is to be extended.\n    """"""\n    try:\n        with open(vocab_path, \'r\') as f:\n            current_vocab = json.load(f)\n    except IOError:\n        print(\'Vocabulary file not found, expected at \' + vocab_path)\n        return\n\n    extend_vocab(current_vocab, vocab, max_tokens)\n\n    # Save back to file\n    with open(vocab_path, \'w\') as f:\n        json.dump(current_vocab, f, sort_keys=True, indent=4, separators=(\',\',\': \'))\n\n\ndef extend_vocab(current_vocab, new_vocab, max_tokens=10000):\n    """""" Extends current vocabulary with words from vocab that are not\n        present in the current vocabulary. Adds up to max_tokens words.\n\n    # Arguments:\n        current_vocab: Current dictionary of tokens.\n        new_vocab: Vocabulary to be added. MUST have word_counts populated, i.e.\n            must have run count_all_words() previously.\n        max_tokens: Maximum number of words to be added.\n\n    # Returns:\n        How many new tokens have been added.\n    """"""\n    if max_tokens < 0:\n        max_tokens = 10000\n\n    words = OrderedDict()\n\n    # sort words by frequency\n    desc_order = OrderedDict(sorted(new_vocab.word_counts.items(),\n                                key=lambda kv: kv[1], reverse=True))\n    words.update(desc_order)\n\n    base_index = len(current_vocab.keys())\n    added = 0\n    for word in words:\n        if added >= max_tokens:\n            break\n        if word not in current_vocab.keys():\n            current_vocab[word] = base_index + added\n            added += 1\n\n    return added\n'"
torchmoji/filter_input.py,0,"b'# -*- coding: utf-8 -*-\nfrom __future__ import print_function, division\nimport codecs\nimport csv\nimport numpy as np\nfrom emoji import UNICODE_EMOJI\n\ndef read_english(path=""english_words.txt"", add_emojis=True):\n    # read english words for filtering (includes emojis as part of set)\n    english = set()\n    with codecs.open(path, ""r"", ""utf-8"") as f:\n        for line in f:\n            line = line.strip().lower().replace(\'\\n\', \'\')\n            if len(line):\n                english.add(line)\n    if add_emojis:\n        for e in UNICODE_EMOJI:\n            english.add(e)\n    return english\n\ndef read_wanted_emojis(path=""wanted_emojis.csv""):\n    emojis = []\n    with open(path, \'rb\') as f:\n        reader = csv.reader(f)\n        for line in reader:\n            line = line[0].strip().replace(\'\\n\', \'\')\n            line = line.decode(\'unicode-escape\')\n            emojis.append(line)\n    return emojis\n\ndef read_non_english_users(path=""unwanted_users.npz""):\n    try:\n        neu_set = set(np.load(path)[\'userids\'])\n    except IOError:\n        neu_set = set()\n    return neu_set\n'"
torchmoji/filter_utils.py,0,"b'\n# -*- coding: utf-8 -*-\nfrom __future__ import print_function, division, unicode_literals\nimport sys\nimport re\nimport string\nimport emoji\nfrom itertools import groupby\n\nimport numpy as np\nfrom torchmoji.tokenizer import RE_MENTION, RE_URL\nfrom torchmoji.global_variables import SPECIAL_TOKENS\n\ntry:\n    unichr        # Python 2\nexcept NameError:\n    unichr = chr  # Python 3\n\n\nAtMentionRegex = re.compile(RE_MENTION)\nurlRegex = re.compile(RE_URL)\n\n# from http://bit.ly/2rdjgjE (UTF-8 encodings and Unicode chars)\nVARIATION_SELECTORS = [ \'\\ufe00\',\n                        \'\\ufe01\',\n                        \'\\ufe02\',\n                        \'\\ufe03\',\n                        \'\\ufe04\',\n                        \'\\ufe05\',\n                        \'\\ufe06\',\n                        \'\\ufe07\',\n                        \'\\ufe08\',\n                        \'\\ufe09\',\n                        \'\\ufe0a\',\n                        \'\\ufe0b\',\n                        \'\\ufe0c\',\n                        \'\\ufe0d\',\n                        \'\\ufe0e\',\n                        \'\\ufe0f\']\n\n# from https://stackoverflow.com/questions/92438/stripping-non-printable-characters-from-a-string-in-python\nALL_CHARS = (unichr(i) for i in range(sys.maxunicode))\nCONTROL_CHARS = \'\'.join(map(unichr, list(range(0,32)) + list(range(127,160))))\nCONTROL_CHAR_REGEX = re.compile(\'[%s]\' % re.escape(CONTROL_CHARS))\n\ndef is_special_token(word):\n    equal = False\n    for spec in SPECIAL_TOKENS:\n        if word == spec:\n            equal = True\n            break\n    return equal\n\ndef mostly_english(words, english, pct_eng_short=0.5, pct_eng_long=0.6, ignore_special_tokens=True, min_length=2):\n    """""" Ensure text meets threshold for containing English words """"""\n\n    n_words = 0\n    n_english = 0\n\n    if english is None:\n        return True, 0, 0\n\n    for w in words:\n        if len(w) < min_length:\n            continue\n        if punct_word(w):\n            continue\n        if ignore_special_tokens and is_special_token(w):\n            continue\n        n_words += 1\n        if w in english:\n            n_english += 1\n\n    if n_words < 2:\n        return True, n_words, n_english\n    if n_words < 5:\n        valid_english = n_english >= n_words * pct_eng_short\n    else:\n        valid_english = n_english >= n_words * pct_eng_long\n    return valid_english, n_words, n_english\n\ndef correct_length(words, min_words, max_words, ignore_special_tokens=True):\n    """""" Ensure text meets threshold for containing English words\n        and that it\'s within the min and max words limits. """"""\n\n    if min_words is None:\n        min_words = 0\n\n    if max_words is None:\n        max_words = 99999\n\n    n_words = 0\n    for w in words:\n        if punct_word(w):\n            continue\n        if ignore_special_tokens and is_special_token(w):\n            continue\n        n_words += 1\n    valid = min_words <= n_words and n_words <= max_words\n    return valid\n\ndef punct_word(word, punctuation=string.punctuation):\n    return all([True if c in punctuation else False for c in word])\n\ndef load_non_english_user_set():\n    non_english_user_set = set(np.load(\'uids.npz\')[\'data\'])\n    return non_english_user_set\n\ndef non_english_user(userid, non_english_user_set):\n    neu_found = int(userid) in non_english_user_set\n    return neu_found\n\ndef separate_emojis_and_text(text):\n    emoji_chars = []\n    non_emoji_chars = []\n    for c in text:\n        if c in emoji.UNICODE_EMOJI:\n            emoji_chars.append(c)\n        else:\n            non_emoji_chars.append(c)\n    return \'\'.join(emoji_chars), \'\'.join(non_emoji_chars)\n\ndef extract_emojis(text, wanted_emojis):\n    text = remove_variation_selectors(text)\n    return [c for c in text if c in wanted_emojis]\n\ndef remove_variation_selectors(text):\n    """""" Remove styling glyph variants for Unicode characters.\n        For instance, remove skin color from emojis.\n    """"""\n    for var in VARIATION_SELECTORS:\n        text = text.replace(var, \'\')\n    return text\n\ndef shorten_word(word):\n    """""" Shorten groupings of 3+ identical consecutive chars to 2, e.g. \'!!!!\' --> \'!!\'\n    """"""\n\n    # only shorten ASCII words\n    try:\n        word.decode(\'ascii\')\n    except (UnicodeDecodeError, UnicodeEncodeError, AttributeError) as e:\n        return word\n\n    # must have at least 3 char to be shortened\n    if len(word) < 3:\n        return word\n\n    # find groups of 3+ consecutive letters\n    letter_groups = [list(g) for k, g in groupby(word)]\n    triple_or_more = [\'\'.join(g) for g in letter_groups if len(g) >= 3]\n    if len(triple_or_more) == 0:\n        return word\n\n    # replace letters to find the short word\n    short_word = word\n    for trip in triple_or_more:\n        short_word = short_word.replace(trip, trip[0]*2)\n\n    return short_word\n\ndef detect_special_tokens(word):\n    try:\n        int(word)\n        word = SPECIAL_TOKENS[4]\n    except ValueError:\n        if AtMentionRegex.findall(word):\n            word = SPECIAL_TOKENS[2]\n        elif urlRegex.findall(word):\n            word = SPECIAL_TOKENS[3]\n    return word\n\ndef process_word(word):\n    """""" Shortening and converting the word to a special token if relevant.\n    """"""\n    word = shorten_word(word)\n    word = detect_special_tokens(word)\n    return word\n\ndef remove_control_chars(text):\n    return CONTROL_CHAR_REGEX.sub(\'\', text)\n\ndef convert_nonbreaking_space(text):\n    # ugly hack handling non-breaking space no matter how badly it\'s been encoded in the input\n    for r in [\'\\\\\\\\xc2\', \'\\\\xc2\', \'\\xc2\', \'\\\\\\\\xa0\', \'\\\\xa0\', \'\\xa0\']:\n        text = text.replace(r, \' \')\n    return text\n\ndef convert_linebreaks(text):\n    # ugly hack handling non-breaking space no matter how badly it\'s been encoded in the input\n    # space around to ensure proper tokenization\n    for r in [\'\\\\\\\\n\', \'\\\\n\', \'\\n\', \'\\\\\\\\r\', \'\\\\r\', \'\\r\', \'<br>\']:\n        text = text.replace(r, \' \' + SPECIAL_TOKENS[5] + \' \')\n    return text\n'"
torchmoji/finetuning.py,18,"b'# -*- coding: utf-8 -*-\n"""""" Finetuning functions for doing transfer learning to new datasets.\n""""""\nfrom __future__ import print_function\n\nimport uuid\nfrom time import sleep\nfrom io import open\n\nimport math\nimport pickle\nimport numpy as np\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom sklearn.metrics import accuracy_score\nfrom torch.autograd import Variable\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.utils.data.sampler import BatchSampler, SequentialSampler\nfrom torch.nn.utils import clip_grad_norm\n\nfrom sklearn.metrics import f1_score\n\nfrom torchmoji.global_variables import (FINETUNING_METHODS,\n                                               FINETUNING_METRICS,\n                                               WEIGHTS_DIR)\nfrom torchmoji.tokenizer import tokenize\nfrom torchmoji.sentence_tokenizer import SentenceTokenizer\n\ntry:\n    unicode\n    IS_PYTHON2 = True\nexcept NameError:\n    unicode = str\n    IS_PYTHON2 = False\n\n\ndef load_benchmark(path, vocab, extend_with=0):\n    """""" Loads the given benchmark dataset.\n\n        Tokenizes the texts using the provided vocabulary, extending it with\n        words from the training dataset if extend_with > 0. Splits them into\n        three lists: training, validation and testing (in that order).\n\n        Also calculates the maximum length of the texts and the\n        suggested batch_size.\n\n    # Arguments:\n        path: Path to the dataset to be loaded.\n        vocab: Vocabulary to be used for tokenizing texts.\n        extend_with: If > 0, the vocabulary will be extended with up to\n            extend_with tokens from the training set before tokenizing.\n\n    # Returns:\n        A dictionary with the following fields:\n            texts: List of three lists, containing tokenized inputs for\n                training, validation and testing (in that order).\n            labels: List of three lists, containing labels for training,\n                validation and testing (in that order).\n            added: Number of tokens added to the vocabulary.\n            batch_size: Batch size.\n            maxlen: Maximum length of an input.\n    """"""\n    # Pre-processing dataset\n    with open(path, \'rb\') as dataset:\n        if IS_PYTHON2:\n            data = pickle.load(dataset)\n        else:\n            data = pickle.load(dataset, fix_imports=True)\n\n    # Decode data\n    try:\n        texts = [unicode(x) for x in data[\'texts\']]\n    except UnicodeDecodeError:\n        texts = [x.decode(\'utf-8\') for x in data[\'texts\']]\n\n    # Extract labels\n    labels = [x[\'label\'] for x in data[\'info\']]\n\n    batch_size, maxlen = calculate_batchsize_maxlen(texts)\n\n    st = SentenceTokenizer(vocab, maxlen)\n\n    # Split up dataset. Extend the existing vocabulary with up to extend_with\n    # tokens from the training dataset.\n    texts, labels, added = st.split_train_val_test(texts,\n                                                   labels,\n                                                   [data[\'train_ind\'],\n                                                    data[\'val_ind\'],\n                                                    data[\'test_ind\']],\n                                                   extend_with=extend_with)\n    return {\'texts\': texts,\n            \'labels\': labels,\n            \'added\': added,\n            \'batch_size\': batch_size,\n            \'maxlen\': maxlen}\n\n\ndef calculate_batchsize_maxlen(texts):\n    """""" Calculates the maximum length in the provided texts and a suitable\n        batch size. Rounds up maxlen to the nearest multiple of ten.\n\n    # Arguments:\n        texts: List of inputs.\n\n    # Returns:\n        Batch size,\n        max length\n    """"""\n    def roundup(x):\n        return int(math.ceil(x / 10.0)) * 10\n\n    # Calculate max length of sequences considered\n    # Adjust batch_size accordingly to prevent GPU overflow\n    lengths = [len(tokenize(t)) for t in texts]\n    maxlen = roundup(np.percentile(lengths, 80.0))\n    batch_size = 250 if maxlen <= 100 else 50\n    return batch_size, maxlen\n\n\n\ndef freeze_layers(model, unfrozen_types=[], unfrozen_keyword=None):\n    """""" Freezes all layers in the given model, except for ones that are\n        explicitly specified to not be frozen.\n\n    # Arguments:\n        model: Model whose layers should be modified.\n        unfrozen_types: List of layer types which shouldn\'t be frozen.\n        unfrozen_keyword: Name keywords of layers that shouldn\'t be frozen.\n\n    # Returns:\n        Model with the selected layers frozen.\n    """"""\n    # Get trainable modules\n    trainable_modules = [(n, m) for n, m in model.named_children() if len([id(p) for p in m.parameters()]) !=  0]\n    for name, module in trainable_modules:\n        trainable = (any(typ in str(module) for typ in unfrozen_types) or\n                     (unfrozen_keyword is not None and unfrozen_keyword.lower() in name.lower()))\n        change_trainable(module, trainable, verbose=False)\n    return model\n\n\ndef change_trainable(module, trainable, verbose=False):\n    """""" Helper method that freezes or unfreezes a given layer.\n\n    # Arguments:\n        module: Module to be modified.\n        trainable: Whether the layer should be frozen or unfrozen.\n        verbose: Verbosity flag.\n    """"""\n    \n    if verbose: print(\'Changing MODULE\', module, \'to trainable =\', trainable)\n    for name, param in module.named_parameters():\n        if verbose: print(\'Setting weight\', name, \'to trainable =\', trainable)\n        param.requires_grad = trainable\n\n    if verbose:\n        action = \'Unfroze\' if trainable else \'Froze\'\n        if verbose: print(""{} {}"".format(action, module))\n\n\ndef find_f1_threshold(model, val_gen, test_gen, average=\'binary\'):\n    """""" Choose a threshold for F1 based on the validation dataset\n        (see https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4442797/\n        for details on why to find another threshold than simply 0.5)\n\n    # Arguments:\n        model: pyTorch model\n        val_gen: Validation set dataloader.\n        test_gen: Testing set dataloader.\n\n    # Returns:\n        F1 score for the given data and\n        the corresponding F1 threshold\n    """"""\n    thresholds = np.arange(0.01, 0.5, step=0.01)\n    f1_scores = []\n\n    model.eval()\n    val_out = [(y, model(X)) for X, y in val_gen]\n    y_val, y_pred_val = (list(t) for t in zip(*val_out))\n\n    test_out = [(y, model(X)) for X, y in test_gen]\n    y_test, y_pred_test = (list(t) for t in zip(*val_out))\n\n    for t in thresholds:\n        y_pred_val_ind = (y_pred_val > t)\n        f1_val = f1_score(y_val, y_pred_val_ind, average=average)\n        f1_scores.append(f1_val)\n\n    best_t = thresholds[np.argmax(f1_scores)]\n    y_pred_ind = (y_pred_test > best_t)\n    f1_test = f1_score(y_test, y_pred_ind, average=average)\n    return f1_test, best_t\n\n\ndef finetune(model, texts, labels, nb_classes, batch_size, method,\n             metric=\'acc\', epoch_size=5000, nb_epochs=1000, embed_l2=1E-6,\n             verbose=1):\n    """""" Compiles and finetunes the given pytorch model.\n\n    # Arguments:\n        model: Model to be finetuned\n        texts: List of three lists, containing tokenized inputs for training,\n            validation and testing (in that order).\n        labels: List of three lists, containing labels for training,\n            validation and testing (in that order).\n        nb_classes: Number of classes in the dataset.\n        batch_size: Batch size.\n        method: Finetuning method to be used. For available methods, see\n            FINETUNING_METHODS in global_variables.py.\n        metric: Evaluation metric to be used. For available metrics, see\n            FINETUNING_METRICS in global_variables.py.\n        epoch_size: Number of samples in an epoch.\n        nb_epochs: Number of epochs. Doesn\'t matter much as early stopping is used.\n        embed_l2: L2 regularization for the embedding layer.\n        verbose: Verbosity flag.\n\n    # Returns:\n        Model after finetuning,\n        score after finetuning using the provided metric.\n    """"""\n\n    if method not in FINETUNING_METHODS:\n        raise ValueError(\'ERROR (finetune): Invalid method parameter. \'\n                         \'Available options: {}\'.format(FINETUNING_METHODS))\n    if metric not in FINETUNING_METRICS:\n        raise ValueError(\'ERROR (finetune): Invalid metric parameter. \'\n                         \'Available options: {}\'.format(FINETUNING_METRICS))\n\n    train_gen = get_data_loader(texts[0], labels[0], batch_size,\n                                extended_batch_sampler=True, epoch_size=epoch_size)\n    val_gen = get_data_loader(texts[1], labels[1], batch_size,\n                              extended_batch_sampler=False)\n    test_gen = get_data_loader(texts[2], labels[2], batch_size,\n                              extended_batch_sampler=False)\n\n    checkpoint_path = \'{}/torchmoji-checkpoint-{}.bin\' \\\n                      .format(WEIGHTS_DIR, str(uuid.uuid4()))\n\n    if method in [\'last\', \'new\']:\n        lr = 0.001\n    elif method in [\'full\', \'chain-thaw\']:\n        lr = 0.0001\n\n    loss_op = nn.BCEWithLogitsLoss() if nb_classes <= 2 \\\n         else nn.CrossEntropyLoss()\n\n    # Freeze layers if using last\n    if method == \'last\':\n        model = freeze_layers(model, unfrozen_keyword=\'output_layer\')\n\n    # Define optimizer, for chain-thaw we define it later (after freezing)\n    if method == \'last\':\n        adam = optim.Adam((p for p in model.parameters() if p.requires_grad), lr=lr)\n    elif method in [\'full\', \'new\']:\n        # Add L2 regulation on embeddings only\n        embed_params_id = [id(p) for p in model.embed.parameters()]\n        output_layer_params_id = [id(p) for p in model.output_layer.parameters()]\n        base_params = [p for p in model.parameters()\n                       if id(p) not in embed_params_id and id(p) not in output_layer_params_id and p.requires_grad]\n        embed_params = [p for p in model.parameters() if id(p) in embed_params_id and p.requires_grad]\n        output_layer_params = [p for p in model.parameters() if id(p) in output_layer_params_id and p.requires_grad]\n        adam = optim.Adam([\n            {\'params\': base_params},\n            {\'params\': embed_params, \'weight_decay\': embed_l2},\n            {\'params\': output_layer_params, \'lr\': 0.001},\n            ], lr=lr)\n\n    # Training\n    if verbose:\n        print(\'Method:  {}\'.format(method))\n        print(\'Metric:  {}\'.format(metric))\n        print(\'Classes: {}\'.format(nb_classes))\n\n    if method == \'chain-thaw\':\n        result = chain_thaw(model, train_gen, val_gen, test_gen, nb_epochs, checkpoint_path, loss_op, embed_l2=embed_l2,\n                            evaluate=metric, verbose=verbose)\n    else:\n        result = tune_trainable(model, loss_op, adam, train_gen, val_gen, test_gen, nb_epochs, checkpoint_path,\n                                evaluate=metric, verbose=verbose)\n    return model, result\n\n\ndef tune_trainable(model, loss_op, optim_op, train_gen, val_gen, test_gen,\n                   nb_epochs, checkpoint_path, patience=5, evaluate=\'acc\',\n                   verbose=2):\n    """""" Finetunes the given model using the accuracy measure.\n\n    # Arguments:\n        model: Model to be finetuned.\n        nb_classes: Number of classes in the given dataset.\n        train: Training data, given as a tuple of (inputs, outputs)\n        val: Validation data, given as a tuple of (inputs, outputs)\n        test: Testing data, given as a tuple of (inputs, outputs)\n        epoch_size: Number of samples in an epoch.\n        nb_epochs: Number of epochs.\n        batch_size: Batch size.\n        checkpoint_weight_path: Filepath where weights will be checkpointed to\n            during training. This file will be rewritten by the function.\n        patience: Patience for callback methods.\n        evaluate: Evaluation method to use. Can be \'acc\' or \'weighted_f1\'.\n        verbose: Verbosity flag.\n\n    # Returns:\n        Accuracy of the trained model, ONLY if \'evaluate\' is set.\n    """"""\n    if verbose:\n        print(""Trainable weights: {}"".format([n for n, p in model.named_parameters() if p.requires_grad]))\n        print(""Training..."")\n        if evaluate == \'acc\':\n            print(""Evaluation on test set prior training:"", evaluate_using_acc(model, test_gen))\n        elif evaluate == \'weighted_f1\':\n            print(""Evaluation on test set prior training:"", evaluate_using_weighted_f1(model, test_gen, val_gen))\n\n    fit_model(model, loss_op, optim_op, train_gen, val_gen, nb_epochs, checkpoint_path, patience)\n\n    # Reload the best weights found to avoid overfitting\n    # Wait a bit to allow proper closing of weights file\n    sleep(1)\n    model.load_state_dict(torch.load(checkpoint_path))\n    if verbose >= 2:\n        print(""Loaded weights from {}"".format(checkpoint_path))\n\n    if evaluate == \'acc\':\n        return evaluate_using_acc(model, test_gen)\n    elif evaluate == \'weighted_f1\':\n        return evaluate_using_weighted_f1(model, test_gen, val_gen)\n\n\ndef evaluate_using_weighted_f1(model, test_gen, val_gen):\n    """""" Evaluation function using macro weighted F1 score.\n\n    # Arguments:\n        model: Model to be evaluated.\n        X_test: Inputs of the testing set.\n        y_test: Outputs of the testing set.\n        X_val: Inputs of the validation set.\n        y_val: Outputs of the validation set.\n        batch_size: Batch size.\n\n    # Returns:\n        Weighted F1 score of the given model.\n    """"""\n    # Evaluate on test and val data\n    f1_test, _ = find_f1_threshold(model, test_gen, val_gen, average=\'weighted_f1\')\n    return f1_test\n\n\ndef evaluate_using_acc(model, test_gen):\n    """""" Evaluation function using accuracy.\n\n    # Arguments:\n        model: Model to be evaluated.\n        test_gen: Testing data iterator (DataLoader)\n\n    # Returns:\n        Accuracy of the given model.\n    """"""\n\n    # Validate on test_data\n    model.eval()\n    accs = []\n    for i, data in enumerate(test_gen):\n        x, y = data\n        outs = model(x)\n        if model.nb_classes > 2:\n            pred = torch.max(outs, 1)[1]\n            acc = accuracy_score(y.squeeze().numpy(), pred.squeeze().numpy())\n        else:\n            pred = (outs >= 0).long()\n            acc = (pred == y).double().sum() / len(pred)\n        accs.append(acc)\n    return np.mean(accs)\n\n\ndef chain_thaw(model, train_gen, val_gen, test_gen, nb_epochs, checkpoint_path, loss_op,\n               patience=5, initial_lr=0.001, next_lr=0.0001, embed_l2=1E-6, evaluate=\'acc\', verbose=1):\n    """""" Finetunes given model using chain-thaw and evaluates using accuracy.\n\n    # Arguments:\n        model: Model to be finetuned.\n        train: Training data, given as a tuple of (inputs, outputs)\n        val: Validation data, given as a tuple of (inputs, outputs)\n        test: Testing data, given as a tuple of (inputs, outputs)\n        batch_size: Batch size.\n        loss: Loss function to be used during training.\n        epoch_size: Number of samples in an epoch.\n        nb_epochs: Number of epochs.\n        checkpoint_weight_path: Filepath where weights will be checkpointed to\n            during training. This file will be rewritten by the function.\n        initial_lr: Initial learning rate. Will only be used for the first\n            training step (i.e. the output_layer layer)\n        next_lr: Learning rate for every subsequent step.\n        seed: Random number generator seed.\n        verbose: Verbosity flag.\n        evaluate: Evaluation method to use. Can be \'acc\' or \'weighted_f1\'.\n\n    # Returns:\n        Accuracy of the finetuned model.\n    """"""\n    if verbose:\n        print(\'Training..\')\n\n    # Train using chain-thaw\n    train_by_chain_thaw(model, train_gen, val_gen, loss_op, patience, nb_epochs, checkpoint_path,\n                        initial_lr, next_lr, embed_l2, verbose)\n\n    if evaluate == \'acc\':\n        return evaluate_using_acc(model, test_gen)\n    elif evaluate == \'weighted_f1\':\n        return evaluate_using_weighted_f1(model, test_gen, val_gen)\n\n\ndef train_by_chain_thaw(model, train_gen, val_gen, loss_op, patience, nb_epochs, checkpoint_path,\n                        initial_lr=0.001, next_lr=0.0001, embed_l2=1E-6, verbose=1):\n    """""" Finetunes model using the chain-thaw method.\n\n    This is done as follows:\n    1) Freeze every layer except the last (output_layer) layer and train it.\n    2) Freeze every layer except the first layer and train it.\n    3) Freeze every layer except the second etc., until the second last layer.\n    4) Unfreeze all layers and train entire model.\n\n    # Arguments:\n        model: Model to be trained.\n        train_gen: Training sample generator.\n        val_data: Validation data.\n        loss: Loss function to be used.\n        finetuning_args: Training early stopping and checkpoint saving parameters\n        epoch_size: Number of samples in an epoch.\n        nb_epochs: Number of epochs.\n        checkpoint_weight_path: Where weight checkpoints should be saved.\n        batch_size: Batch size.\n        initial_lr: Initial learning rate. Will only be used for the first\n            training step (i.e. the output_layer layer)\n        next_lr: Learning rate for every subsequent step.\n        verbose: Verbosity flag.\n    """"""\n    # Get trainable layers\n    layers = [m for m in model.children() if len([id(p) for p in m.parameters()]) !=  0]\n\n    # Bring last layer to front\n    layers.insert(0, layers.pop(len(layers) - 1))\n\n    # Add None to the end to signify finetuning all layers\n    layers.append(None)\n\n    lr = None\n    # Finetune each layer one by one and finetune all of them at once\n    # at the end\n    for layer in layers:\n        if lr is None:\n            lr = initial_lr\n        elif lr == initial_lr:\n            lr = next_lr\n\n        # Freeze all except current layer\n        for _layer in layers:\n            if _layer is not None:\n                trainable = _layer == layer or layer is None\n                change_trainable(_layer, trainable=trainable, verbose=False)\n\n        # Verify we froze the right layers\n        for _layer in model.children():\n            assert all(p.requires_grad == (_layer == layer) for p in _layer.parameters()) or layer is None\n\n        if verbose:\n            if layer is None:\n                print(\'Finetuning all layers\')\n            else:\n                print(\'Finetuning {}\'.format(layer))\n\n        special_params = [id(p) for p in model.embed.parameters()]\n        base_params = [p for p in model.parameters() if id(p) not in special_params and p.requires_grad]\n        embed_parameters = [p for p in model.parameters() if id(p) in special_params and p.requires_grad]\n        adam = optim.Adam([\n            {\'params\': base_params},\n            {\'params\': embed_parameters, \'weight_decay\': embed_l2},\n            ], lr=lr)\n\n        fit_model(model, loss_op, adam, train_gen, val_gen, nb_epochs,\n                  checkpoint_path, patience)\n\n        # Reload the best weights found to avoid overfitting\n        # Wait a bit to allow proper closing of weights file\n        sleep(1)\n        model.load_state_dict(torch.load(checkpoint_path))\n        if verbose >= 2:\n            print(""Loaded weights from {}"".format(checkpoint_path))\n\n\ndef calc_loss(loss_op, pred, yv):\n    if type(loss_op) is nn.CrossEntropyLoss:\n        return loss_op(pred.squeeze(), yv.squeeze())\n    else:\n        return loss_op(pred.squeeze(), yv.squeeze().float())\n\n\ndef fit_model(model, loss_op, optim_op, train_gen, val_gen, epochs,\n              checkpoint_path, patience):\n    """""" Analog to Keras fit_generator function.\n\n    # Arguments:\n        model: Model to be finetuned.\n        loss_op: loss operation (BCEWithLogitsLoss or CrossEntropy for e.g.)\n        optim_op: optimization operation (Adam e.g.)\n        train_gen: Training data iterator (DataLoader)\n        val_gen: Validation data iterator (DataLoader)\n        epochs: Number of epochs.\n        checkpoint_path: Filepath where weights will be checkpointed to\n            during training. This file will be rewritten by the function.\n        patience: Patience for callback methods.\n        verbose: Verbosity flag.\n\n    # Returns:\n        Accuracy of the trained model, ONLY if \'evaluate\' is set.\n    """"""\n    # Save original checkpoint\n    torch.save(model.state_dict(), checkpoint_path)\n\n    model.eval()\n    best_loss = np.mean([calc_loss(loss_op, model(Variable(xv)), Variable(yv)).data.cpu().numpy()[0] for xv, yv in val_gen])\n    print(""original val loss"", best_loss)\n\n    epoch_without_impr = 0\n    for epoch in range(epochs):\n        for i, data in enumerate(train_gen):\n            X_train, y_train = data\n            X_train = Variable(X_train, requires_grad=False)\n            y_train = Variable(y_train, requires_grad=False)\n            model.train()\n            optim_op.zero_grad()\n            output = model(X_train)\n            loss = calc_loss(loss_op, output, y_train)\n            loss.backward()\n            clip_grad_norm(model.parameters(), 1)\n            optim_op.step()\n\n            acc = evaluate_using_acc(model, [(X_train.data, y_train.data)])\n            print(""== Epoch"", epoch, ""step"", i, ""train loss"", loss.data.cpu().numpy()[0], ""train acc"", acc)\n\n        model.eval()\n        acc = evaluate_using_acc(model, val_gen)\n        print(""val acc"", acc)\n\n        val_loss = np.mean([calc_loss(loss_op, model(Variable(xv)), Variable(yv)).data.cpu().numpy()[0] for xv, yv in val_gen])\n        print(""val loss"", val_loss)\n        if best_loss is not None and val_loss >= best_loss:\n            epoch_without_impr += 1\n            print(\'No improvement over previous best loss: \', best_loss)\n\n        # Save checkpoint\n        if best_loss is None or val_loss < best_loss:\n            best_loss = val_loss\n            torch.save(model.state_dict(), checkpoint_path)\n            print(\'Saving model at\', checkpoint_path)\n\n        # Early stopping\n        if epoch_without_impr >= patience:\n            break\n\ndef get_data_loader(X_in, y_in, batch_size, extended_batch_sampler=True, epoch_size=25000, upsample=False, seed=42):\n    """""" Returns a dataloader that enables larger epochs on small datasets and\n        has upsampling functionality.\n\n    # Arguments:\n        X_in: Inputs of the given dataset.\n        y_in: Outputs of the given dataset.\n        batch_size: Batch size.\n        epoch_size: Number of samples in an epoch.\n        upsample: Whether upsampling should be done. This flag should only be\n            set on binary class problems.\n\n    # Returns:\n        DataLoader.\n    """"""\n    dataset = DeepMojiDataset(X_in, y_in)\n\n    if extended_batch_sampler:\n        batch_sampler = DeepMojiBatchSampler(y_in, batch_size, epoch_size=epoch_size, upsample=upsample, seed=seed)\n    else:\n        batch_sampler = BatchSampler(SequentialSampler(y_in), batch_size, drop_last=False)\n\n    return DataLoader(dataset, batch_sampler=batch_sampler, num_workers=0)\n\nclass DeepMojiDataset(Dataset):\n    """""" A simple Dataset class.\n\n    # Arguments:\n        X_in: Inputs of the given dataset.\n        y_in: Outputs of the given dataset.\n    \n    # __getitem__ output:\n        (torch.LongTensor, torch.LongTensor)\n    """"""\n    def __init__(self, X_in, y_in):\n        # Check if we have Torch.LongTensor inputs (assume Numpy array otherwise)\n        if not isinstance(X_in, torch.LongTensor):\n            X_in = torch.from_numpy(X_in.astype(\'int64\')).long()\n        if not isinstance(y_in, torch.LongTensor):\n            y_in = torch.from_numpy(y_in.astype(\'int64\')).long()\n\n        self.X_in = torch.split(X_in, 1, dim=0)\n        self.y_in = torch.split(y_in, 1, dim=0)\n\n    def __len__(self):\n        return len(self.X_in)\n\n    def __getitem__(self, idx):\n        return self.X_in[idx].squeeze(), self.y_in[idx].squeeze()\n\nclass DeepMojiBatchSampler(object):\n    """"""A Batch sampler that enables larger epochs on small datasets and\n        has upsampling functionality.\n\n    # Arguments:\n        y_in: Labels of the dataset.\n        batch_size: Batch size.\n        epoch_size: Number of samples in an epoch.\n        upsample: Whether upsampling should be done. This flag should only be\n            set on binary class problems.\n        seed: Random number generator seed.\n\n    # __iter__ output:\n        iterator of lists (batches) of indices in the dataset\n    """"""\n\n    def __init__(self, y_in, batch_size, epoch_size, upsample, seed):\n        self.batch_size = batch_size\n        self.epoch_size = epoch_size\n        self.upsample = upsample\n\n        np.random.seed(seed)\n\n        if upsample:\n            # Should only be used on binary class problems\n            assert len(y_in.shape) == 1\n            neg = np.where(y_in.numpy() == 0)[0]\n            pos = np.where(y_in.numpy() == 1)[0]\n            assert epoch_size % 2 == 0\n            samples_pr_class = int(epoch_size / 2)\n        else:\n            ind = range(len(y_in))\n\n        if not upsample:\n            # Randomly sample observations in a balanced way\n            self.sample_ind = np.random.choice(ind, epoch_size, replace=True)\n        else:\n            # Randomly sample observations in a balanced way\n            sample_neg = np.random.choice(neg, samples_pr_class, replace=True)\n            sample_pos = np.random.choice(pos, samples_pr_class, replace=True)\n            concat_ind = np.concatenate((sample_neg, sample_pos), axis=0)\n\n            # Shuffle to avoid labels being in specific order\n            # (all negative then positive)\n            p = np.random.permutation(len(concat_ind))\n            self.sample_ind = concat_ind[p]\n\n            label_dist = np.mean(y_in.numpy()[self.sample_ind])\n            assert(label_dist > 0.45)\n            assert(label_dist < 0.55)\n\n    def __iter__(self):\n        # Hand-off data using batch_size\n        for i in range(int(self.epoch_size/self.batch_size)):\n            start = i * self.batch_size\n            end = min(start + self.batch_size, self.epoch_size)\n            yield self.sample_ind[start:end]\n\n    def __len__(self):\n        # Take care of the last (maybe incomplete) batch\n        return (self.epoch_size + self.batch_size - 1) // self.batch_size\n'"
torchmoji/global_variables.py,0,"b'# -*- coding: utf-8 -*-\n"""""" Global variables.\n""""""\nimport tempfile\nfrom os.path import abspath, dirname\n\n# The ordering of these special tokens matter\n# blank tokens can be used for new purposes\n# Tokenizer should be updated if special token prefix is changed\nSPECIAL_PREFIX = \'CUSTOM_\'\nSPECIAL_TOKENS = [\'CUSTOM_MASK\',\n                  \'CUSTOM_UNKNOWN\',\n                  \'CUSTOM_AT\',\n                  \'CUSTOM_URL\',\n                  \'CUSTOM_NUMBER\',\n                  \'CUSTOM_BREAK\']\nSPECIAL_TOKENS.extend([\'{}BLANK_{}\'.format(SPECIAL_PREFIX, i) for i in range(6, 10)])\n\nROOT_PATH = dirname(dirname(abspath(__file__)))\nVOCAB_PATH = \'{}/model/vocabulary.json\'.format(ROOT_PATH)\nPRETRAINED_PATH = \'{}/model/pytorch_model.bin\'.format(ROOT_PATH)\n\nWEIGHTS_DIR = tempfile.mkdtemp()\n\nNB_TOKENS = 50000\nNB_EMOJI_CLASSES = 64\nFINETUNING_METHODS = [\'last\', \'full\', \'new\', \'chain-thaw\']\nFINETUNING_METRICS = [\'acc\', \'weighted\']\n'"
torchmoji/lstm.py,19,"b'# -*- coding: utf-8 -*-\n"""""" Implement a pyTorch LSTM with hard sigmoid reccurent activation functions.\n    Adapted from the non-cuda variant of pyTorch LSTM at\n    https://github.com/pytorch/pytorch/blob/master/torch/nn/_functions/rnn.py\n""""""\n\nfrom __future__ import print_function, division\nimport math\nimport torch\n\nfrom torch.nn import Module\nfrom torch.nn.parameter import Parameter\nfrom torch.nn.utils.rnn import PackedSequence\nimport torch.nn.functional as F\n\nclass LSTMHardSigmoid(Module):\n\n    def __init__(self, input_size, hidden_size,\n                 num_layers=1, bias=True, batch_first=False,\n                 dropout=0, bidirectional=False):\n        super(LSTMHardSigmoid, self).__init__()\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        self.bias = bias\n        self.batch_first = batch_first\n        self.dropout = dropout\n        self.dropout_state = {}\n        self.bidirectional = bidirectional\n        num_directions = 2 if bidirectional else 1\n\n        gate_size = 4 * hidden_size\n\n        self._all_weights = []\n        for layer in range(num_layers):\n            for direction in range(num_directions):\n                layer_input_size = input_size if layer == 0 else hidden_size * num_directions\n\n                w_ih = Parameter(torch.Tensor(gate_size, layer_input_size))\n                w_hh = Parameter(torch.Tensor(gate_size, hidden_size))\n                b_ih = Parameter(torch.Tensor(gate_size))\n                b_hh = Parameter(torch.Tensor(gate_size))\n                layer_params = (w_ih, w_hh, b_ih, b_hh)\n\n                suffix = \'_reverse\' if direction == 1 else \'\'\n                param_names = [\'weight_ih_l{}{}\', \'weight_hh_l{}{}\']\n                if bias:\n                    param_names += [\'bias_ih_l{}{}\', \'bias_hh_l{}{}\']\n                param_names = [x.format(layer, suffix) for x in param_names]\n\n                for name, param in zip(param_names, layer_params):\n                    setattr(self, name, param)\n                self._all_weights.append(param_names)\n\n        self.flatten_parameters()\n        self.reset_parameters()\n\n    def flatten_parameters(self):\n        """"""Resets parameter data pointer so that they can use faster code paths.\n\n        Right now, this is a no-op wince we don\'t use CUDA acceleration.\n        """"""\n        self._data_ptrs = []\n\n    def _apply(self, fn):\n        ret = super(LSTMHardSigmoid, self)._apply(fn)\n        self.flatten_parameters()\n        return ret\n\n    def reset_parameters(self):\n        stdv = 1.0 / math.sqrt(self.hidden_size)\n        for weight in self.parameters():\n            weight.data.uniform_(-stdv, stdv)\n\n    def forward(self, input, hx=None):\n        is_packed = isinstance(input, PackedSequence)\n        if is_packed:\n            input, batch_sizes = input\n            max_batch_size = batch_sizes[0]\n        else:\n            batch_sizes = None\n            max_batch_size = input.size(0) if self.batch_first else input.size(1)\n\n        if hx is None:\n            num_directions = 2 if self.bidirectional else 1\n            hx = torch.autograd.Variable(input.data.new(self.num_layers *\n                                                        num_directions,\n                                                        max_batch_size,\n                                                        self.hidden_size).zero_(), requires_grad=False)\n            hx = (hx, hx)\n\n        has_flat_weights = list(p.data.data_ptr() for p in self.parameters()) == self._data_ptrs\n        if has_flat_weights:\n            first_data = next(self.parameters()).data\n            assert first_data.storage().size() == self._param_buf_size\n            flat_weight = first_data.new().set_(first_data.storage(), 0, torch.Size([self._param_buf_size]))\n        else:\n            flat_weight = None\n        func = AutogradRNN(\n            self.input_size,\n            self.hidden_size,\n            num_layers=self.num_layers,\n            batch_first=self.batch_first,\n            dropout=self.dropout,\n            train=self.training,\n            bidirectional=self.bidirectional,\n            batch_sizes=batch_sizes,\n            dropout_state=self.dropout_state,\n            flat_weight=flat_weight\n        )\n        output, hidden = func(input, self.all_weights, hx)\n        if is_packed:\n            output = PackedSequence(output, batch_sizes)\n        return output, hidden\n\n    def __repr__(self):\n        s = \'{name}({input_size}, {hidden_size}\'\n        if self.num_layers != 1:\n            s += \', num_layers={num_layers}\'\n        if self.bias is not True:\n            s += \', bias={bias}\'\n        if self.batch_first is not False:\n            s += \', batch_first={batch_first}\'\n        if self.dropout != 0:\n            s += \', dropout={dropout}\'\n        if self.bidirectional is not False:\n            s += \', bidirectional={bidirectional}\'\n        s += \')\'\n        return s.format(name=self.__class__.__name__, **self.__dict__)\n\n    def __setstate__(self, d):\n        super(LSTMHardSigmoid, self).__setstate__(d)\n        self.__dict__.setdefault(\'_data_ptrs\', [])\n        if \'all_weights\' in d:\n            self._all_weights = d[\'all_weights\']\n        if isinstance(self._all_weights[0][0], str):\n            return\n        num_layers = self.num_layers\n        num_directions = 2 if self.bidirectional else 1\n        self._all_weights = []\n        for layer in range(num_layers):\n            for direction in range(num_directions):\n                suffix = \'_reverse\' if direction == 1 else \'\'\n                weights = [\'weight_ih_l{}{}\', \'weight_hh_l{}{}\', \'bias_ih_l{}{}\', \'bias_hh_l{}{}\']\n                weights = [x.format(layer, suffix) for x in weights]\n                if self.bias:\n                    self._all_weights += [weights]\n                else:\n                    self._all_weights += [weights[:2]]\n\n    @property\n    def all_weights(self):\n        return [[getattr(self, weight) for weight in weights] for weights in self._all_weights]\n\ndef AutogradRNN(input_size, hidden_size, num_layers=1, batch_first=False,\n                dropout=0, train=True, bidirectional=False, batch_sizes=None,\n                dropout_state=None, flat_weight=None):\n\n    cell = LSTMCell\n\n    if batch_sizes is None:\n        rec_factory = Recurrent\n    else:\n        rec_factory = variable_recurrent_factory(batch_sizes)\n\n    if bidirectional:\n        layer = (rec_factory(cell), rec_factory(cell, reverse=True))\n    else:\n        layer = (rec_factory(cell),)\n\n    func = StackedRNN(layer,\n                      num_layers,\n                      True,\n                      dropout=dropout,\n                      train=train)\n\n    def forward(input, weight, hidden):\n        if batch_first and batch_sizes is None:\n            input = input.transpose(0, 1)\n\n        nexth, output = func(input, hidden, weight)\n\n        if batch_first and batch_sizes is None:\n            output = output.transpose(0, 1)\n\n        return output, nexth\n\n    return forward\n\ndef Recurrent(inner, reverse=False):\n    def forward(input, hidden, weight):\n        output = []\n        steps = range(input.size(0) - 1, -1, -1) if reverse else range(input.size(0))\n        for i in steps:\n            hidden = inner(input[i], hidden, *weight)\n            # hack to handle LSTM\n            output.append(hidden[0] if isinstance(hidden, tuple) else hidden)\n\n        if reverse:\n            output.reverse()\n        output = torch.cat(output, 0).view(input.size(0), *output[0].size())\n\n        return hidden, output\n\n    return forward\n\n\ndef variable_recurrent_factory(batch_sizes):\n    def fac(inner, reverse=False):\n        if reverse:\n            return VariableRecurrentReverse(batch_sizes, inner)\n        else:\n            return VariableRecurrent(batch_sizes, inner)\n    return fac\n\ndef VariableRecurrent(batch_sizes, inner):\n    def forward(input, hidden, weight):\n        output = []\n        input_offset = 0\n        last_batch_size = batch_sizes[0]\n        hiddens = []\n        flat_hidden = not isinstance(hidden, tuple)\n        if flat_hidden:\n            hidden = (hidden,)\n        for batch_size in batch_sizes:\n            step_input = input[input_offset:input_offset + batch_size]\n            input_offset += batch_size\n\n            dec = last_batch_size - batch_size\n            if dec > 0:\n                hiddens.append(tuple(h[-dec:] for h in hidden))\n                hidden = tuple(h[:-dec] for h in hidden)\n            last_batch_size = batch_size\n\n            if flat_hidden:\n                hidden = (inner(step_input, hidden[0], *weight),)\n            else:\n                hidden = inner(step_input, hidden, *weight)\n\n            output.append(hidden[0])\n        hiddens.append(hidden)\n        hiddens.reverse()\n\n        hidden = tuple(torch.cat(h, 0) for h in zip(*hiddens))\n        assert hidden[0].size(0) == batch_sizes[0]\n        if flat_hidden:\n            hidden = hidden[0]\n        output = torch.cat(output, 0)\n\n        return hidden, output\n\n    return forward\n\n\ndef VariableRecurrentReverse(batch_sizes, inner):\n    def forward(input, hidden, weight):\n        output = []\n        input_offset = input.size(0)\n        last_batch_size = batch_sizes[-1]\n        initial_hidden = hidden\n        flat_hidden = not isinstance(hidden, tuple)\n        if flat_hidden:\n            hidden = (hidden,)\n            initial_hidden = (initial_hidden,)\n        hidden = tuple(h[:batch_sizes[-1]] for h in hidden)\n        for batch_size in reversed(batch_sizes):\n            inc = batch_size - last_batch_size\n            if inc > 0:\n                hidden = tuple(torch.cat((h, ih[last_batch_size:batch_size]), 0)\n                               for h, ih in zip(hidden, initial_hidden))\n            last_batch_size = batch_size\n            step_input = input[input_offset - batch_size:input_offset]\n            input_offset -= batch_size\n\n            if flat_hidden:\n                hidden = (inner(step_input, hidden[0], *weight),)\n            else:\n                hidden = inner(step_input, hidden, *weight)\n            output.append(hidden[0])\n\n        output.reverse()\n        output = torch.cat(output, 0)\n        if flat_hidden:\n            hidden = hidden[0]\n        return hidden, output\n\n    return forward\n\ndef StackedRNN(inners, num_layers, lstm=False, dropout=0, train=True):\n\n    num_directions = len(inners)\n    total_layers = num_layers * num_directions\n\n    def forward(input, hidden, weight):\n        assert(len(weight) == total_layers)\n        next_hidden = []\n\n        if lstm:\n            hidden = list(zip(*hidden))\n\n        for i in range(num_layers):\n            all_output = []\n            for j, inner in enumerate(inners):\n                l = i * num_directions + j\n\n                hy, output = inner(input, hidden[l], weight[l])\n                next_hidden.append(hy)\n                all_output.append(output)\n\n            input = torch.cat(all_output, input.dim() - 1)\n\n            if dropout != 0 and i < num_layers - 1:\n                input = F.dropout(input, p=dropout, training=train, inplace=False)\n\n        if lstm:\n            next_h, next_c = zip(*next_hidden)\n            next_hidden = (\n                torch.cat(next_h, 0).view(total_layers, *next_h[0].size()),\n                torch.cat(next_c, 0).view(total_layers, *next_c[0].size())\n            )\n        else:\n            next_hidden = torch.cat(next_hidden, 0).view(\n                total_layers, *next_hidden[0].size())\n\n        return next_hidden, input\n\n    return forward\n\ndef LSTMCell(input, hidden, w_ih, w_hh, b_ih=None, b_hh=None):\n    """"""\n    A modified LSTM cell with hard sigmoid activation on the input, forget and output gates.\n    """"""\n    hx, cx = hidden\n    gates = F.linear(input, w_ih, b_ih) + F.linear(hx, w_hh, b_hh)\n\n    ingate, forgetgate, cellgate, outgate = gates.chunk(4, 1)\n\n    ingate = hard_sigmoid(ingate)\n    forgetgate = hard_sigmoid(forgetgate)\n    cellgate = F.tanh(cellgate)\n    outgate = hard_sigmoid(outgate)\n\n    cy = (forgetgate * cx) + (ingate * cellgate)\n    hy = outgate * F.tanh(cy)\n\n    return hy, cy\n\ndef hard_sigmoid(x):\n    """"""\n    Computes element-wise hard sigmoid of x.\n    See e.g. https://github.com/Theano/Theano/blob/master/theano/tensor/nnet/sigm.py#L279\n    """"""\n    x = (0.2 * x) + 0.5\n    x = F.threshold(-x, -1, -1)\n    x = F.threshold(-x, 0, 0)\n    return x\n'"
torchmoji/model_def.py,10,"b'# -*- coding: utf-8 -*-\n"""""" Model definition functions and weight loading.\n""""""\n\nfrom __future__ import print_function, division, unicode_literals\n\nfrom os.path import exists\n\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\nfrom torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence, PackedSequence\n\nfrom torchmoji.lstm import LSTMHardSigmoid\nfrom torchmoji.attlayer import Attention\nfrom torchmoji.global_variables import NB_TOKENS, NB_EMOJI_CLASSES\n\n\ndef torchmoji_feature_encoding(weight_path, return_attention=False):\n    """""" Loads the pretrained torchMoji model for extracting features\n        from the penultimate feature layer. In this way, it transforms\n        the text into its emotional encoding.\n\n    # Arguments:\n        weight_path: Path to model weights to be loaded.\n        return_attention: If true, output will include weight of each input token\n            used for the prediction\n\n    # Returns:\n        Pretrained model for encoding text into feature vectors.\n    """"""\n\n    model = TorchMoji(nb_classes=None,\n                     nb_tokens=NB_TOKENS,\n                     feature_output=True,\n                     return_attention=return_attention)\n    load_specific_weights(model, weight_path, exclude_names=[\'output_layer\'])\n    return model\n\n\ndef torchmoji_emojis(weight_path, return_attention=False):\n    """""" Loads the pretrained torchMoji model for extracting features\n        from the penultimate feature layer. In this way, it transforms\n        the text into its emotional encoding.\n\n    # Arguments:\n        weight_path: Path to model weights to be loaded.\n        return_attention: If true, output will include weight of each input token\n            used for the prediction\n\n    # Returns:\n        Pretrained model for encoding text into feature vectors.\n    """"""\n\n    model = TorchMoji(nb_classes=NB_EMOJI_CLASSES,\n                     nb_tokens=NB_TOKENS,\n                     return_attention=return_attention)\n    model.load_state_dict(torch.load(weight_path))\n    return model\n\n\ndef torchmoji_transfer(nb_classes, weight_path=None, extend_embedding=0,\n                      embed_dropout_rate=0.1, final_dropout_rate=0.5):\n    """""" Loads the pretrained torchMoji model for finetuning/transfer learning.\n        Does not load weights for the softmax layer.\n\n        Note that if you are planning to use class average F1 for evaluation,\n        nb_classes should be set to 2 instead of the actual number of classes\n        in the dataset, since binary classification will be performed on each\n        class individually.\n\n        Note that for the \'new\' method, weight_path should be left as None.\n\n    # Arguments:\n        nb_classes: Number of classes in the dataset.\n        weight_path: Path to model weights to be loaded.\n        extend_embedding: Number of tokens that have been added to the\n            vocabulary on top of NB_TOKENS. If this number is larger than 0,\n            the embedding layer\'s dimensions are adjusted accordingly, with the\n            additional weights being set to random values.\n        embed_dropout_rate: Dropout rate for the embedding layer.\n        final_dropout_rate: Dropout rate for the final Softmax layer.\n\n    # Returns:\n        Model with the given parameters.\n    """"""\n\n    model = TorchMoji(nb_classes=nb_classes,\n                     nb_tokens=NB_TOKENS + extend_embedding,\n                     embed_dropout_rate=embed_dropout_rate,\n                     final_dropout_rate=final_dropout_rate,\n                     output_logits=True)\n    if weight_path is not None:\n        load_specific_weights(model, weight_path,\n                              exclude_names=[\'output_layer\'],\n                              extend_embedding=extend_embedding)\n    return model\n\n\nclass TorchMoji(nn.Module):\n    def __init__(self, nb_classes, nb_tokens, feature_output=False, output_logits=False,\n                 embed_dropout_rate=0, final_dropout_rate=0, return_attention=False):\n        """"""\n        torchMoji model.\n        IMPORTANT: The model is loaded in evaluation mode by default (self.eval())\n\n        # Arguments:\n            nb_classes: Number of classes in the dataset.\n            nb_tokens: Number of tokens in the dataset (i.e. vocabulary size).\n            feature_output: If True the model returns the penultimate\n                            feature vector rather than Softmax probabilities\n                            (defaults to False).\n            output_logits:  If True the model returns logits rather than probabilities\n                            (defaults to False).\n            embed_dropout_rate: Dropout rate for the embedding layer.\n            final_dropout_rate: Dropout rate for the final Softmax layer.\n            return_attention: If True the model also returns attention weights over the sentence\n                              (defaults to False).\n        """"""\n        super(TorchMoji, self).__init__()\n\n        embedding_dim = 256\n        hidden_size = 512\n        attention_size = 4 * hidden_size + embedding_dim\n\n        self.feature_output = feature_output\n        self.embed_dropout_rate = embed_dropout_rate\n        self.final_dropout_rate = final_dropout_rate\n        self.return_attention = return_attention\n        self.hidden_size = hidden_size\n        self.output_logits = output_logits\n        self.nb_classes = nb_classes\n\n        self.add_module(\'embed\', nn.Embedding(nb_tokens, embedding_dim))\n        # dropout2D: embedding channels are dropped out instead of words\n        # many exampels in the datasets contain few words that losing one or more words can alter the emotions completely\n        self.add_module(\'embed_dropout\', nn.Dropout2d(embed_dropout_rate))\n        self.add_module(\'lstm_0\', LSTMHardSigmoid(embedding_dim, hidden_size, batch_first=True, bidirectional=True))\n        self.add_module(\'lstm_1\', LSTMHardSigmoid(hidden_size*2, hidden_size, batch_first=True, bidirectional=True))\n        self.add_module(\'attention_layer\', Attention(attention_size=attention_size, return_attention=return_attention))\n        if not feature_output:\n            self.add_module(\'final_dropout\', nn.Dropout(final_dropout_rate))\n            if output_logits:\n                self.add_module(\'output_layer\', nn.Sequential(nn.Linear(attention_size, nb_classes if self.nb_classes > 2 else 1)))\n            else:\n                self.add_module(\'output_layer\', nn.Sequential(nn.Linear(attention_size, nb_classes if self.nb_classes > 2 else 1),\n                                                              nn.Softmax() if self.nb_classes > 2 else nn.Sigmoid()))\n        self.init_weights()\n        # Put model in evaluation mode by default\n        self.eval()\n\n    def init_weights(self):\n        """"""\n        Here we reproduce Keras default initialization weights for consistency with Keras version\n        """"""\n        ih = (param.data for name, param in self.named_parameters() if \'weight_ih\' in name)\n        hh = (param.data for name, param in self.named_parameters() if \'weight_hh\' in name)\n        b = (param.data for name, param in self.named_parameters() if \'bias\' in name)\n        nn.init.uniform(self.embed.weight.data, a=-0.5, b=0.5)\n        for t in ih:\n            nn.init.xavier_uniform(t)\n        for t in hh:\n            nn.init.orthogonal(t)\n        for t in b:\n            nn.init.constant(t, 0)\n        if not self.feature_output:\n            nn.init.xavier_uniform(self.output_layer[0].weight.data)\n\n    def forward(self, input_seqs):\n        """""" Forward pass.\n\n        # Arguments:\n            input_seqs: Can be one of Numpy array, Torch.LongTensor, Torch.Variable, Torch.PackedSequence.\n\n        # Return:\n            Same format as input format (except for PackedSequence returned as Variable).\n        """"""\n        # Check if we have Torch.LongTensor inputs or not Torch.Variable (assume Numpy array in this case), take note to return same format\n        return_numpy = False\n        return_tensor = False\n        if isinstance(input_seqs, (torch.LongTensor, torch.cuda.LongTensor)):\n            input_seqs = Variable(input_seqs)\n            return_tensor = True\n        elif not isinstance(input_seqs, Variable):\n            input_seqs = Variable(torch.from_numpy(input_seqs.astype(\'int64\')).long())\n            return_numpy = True\n\n        # If we don\'t have a packed inputs, let\'s pack it\n        reorder_output = False\n        if not isinstance(input_seqs, PackedSequence):\n            ho = self.lstm_0.weight_hh_l0.data.new(2, input_seqs.size()[0], self.hidden_size).zero_()\n            co = self.lstm_0.weight_hh_l0.data.new(2, input_seqs.size()[0], self.hidden_size).zero_()\n\n            # Reorder batch by sequence length\n            input_lengths = torch.LongTensor([torch.max(input_seqs[i, :].data.nonzero()) + 1 for i in range(input_seqs.size()[0])])\n            input_lengths, perm_idx = input_lengths.sort(0, descending=True)\n            input_seqs = input_seqs[perm_idx][:, :input_lengths.max()]\n\n            # Pack sequence and work on data tensor to reduce embeddings/dropout computations\n            packed_input = pack_padded_sequence(input_seqs, input_lengths.cpu().numpy(), batch_first=True)\n            reorder_output = True\n        else:\n            ho = self.lstm_0.weight_hh_l0.data.data.new(2, input_seqs.size()[0], self.hidden_size).zero_()\n            co = self.lstm_0.weight_hh_l0.data.data.new(2, input_seqs.size()[0], self.hidden_size).zero_()\n            input_lengths = input_seqs.batch_sizes\n            packed_input = input_seqs\n\n        hidden = (Variable(ho, requires_grad=False), Variable(co, requires_grad=False))\n\n        # Embed with an activation function to bound the values of the embeddings\n        x = self.embed(packed_input.data)\n        x = nn.Tanh()(x)\n\n        # pyTorch 2D dropout2d operate on axis 1 which is fine for us\n        x = self.embed_dropout(x)\n\n        # Update packed sequence data for RNN\n        packed_input = PackedSequence(x, packed_input.batch_sizes)\n\n        # skip-connection from embedding to output eases gradient-flow and allows access to lower-level features\n        # ordering of the way the merge is done is important for consistency with the pretrained model\n        lstm_0_output, _ = self.lstm_0(packed_input, hidden)\n        lstm_1_output, _ = self.lstm_1(lstm_0_output, hidden)\n\n        # Update packed sequence data for attention layer\n        packed_input = PackedSequence(torch.cat((lstm_1_output.data,\n                                                 lstm_0_output.data,\n                                                 packed_input.data), dim=1),\n                                      packed_input.batch_sizes)\n\n        input_seqs, _ = pad_packed_sequence(packed_input, batch_first=True)\n\n        x, att_weights = self.attention_layer(input_seqs, input_lengths)\n\n        # output class probabilities or penultimate feature vector\n        if not self.feature_output:\n            x = self.final_dropout(x)\n            outputs = self.output_layer(x)\n        else:\n            outputs = x\n\n        # Reorder output if needed\n        if reorder_output:\n            reorered = Variable(outputs.data.new(outputs.size()))\n            reorered[perm_idx] = outputs\n            outputs = reorered\n\n        # Adapt return format if needed\n        if return_tensor:\n            outputs = outputs.data\n        if return_numpy:\n            outputs = outputs.data.numpy()\n\n        if self.return_attention:\n            return outputs, att_weights\n        else:\n            return outputs\n\n\ndef load_specific_weights(model, weight_path, exclude_names=[], extend_embedding=0, verbose=True):\n    """""" Loads model weights from the given file path, excluding any\n        given layers.\n\n    # Arguments:\n        model: Model whose weights should be loaded.\n        weight_path: Path to file containing model weights.\n        exclude_names: List of layer names whose weights should not be loaded.\n        extend_embedding: Number of new words being added to vocabulary.\n        verbose: Verbosity flag.\n\n    # Raises:\n        ValueError if the file at weight_path does not exist.\n    """"""\n    if not exists(weight_path):\n        raise ValueError(\'ERROR (load_weights): The weights file at {} does \'\n                         \'not exist. Refer to the README for instructions.\'\n                         .format(weight_path))\n\n    if extend_embedding and \'embed\' in exclude_names:\n        raise ValueError(\'ERROR (load_weights): Cannot extend a vocabulary \'\n                         \'without loading the embedding weights.\')\n\n    # Copy only weights from the temporary model that are wanted\n    # for the specific task (e.g. the Softmax is often ignored)\n    weights = torch.load(weight_path)\n    for key, weight in weights.items():\n        if any(excluded in key for excluded in exclude_names):\n            if verbose:\n                print(\'Ignoring weights for {}\'.format(key))\n            continue\n\n        try:\n            model_w = model.state_dict()[key]\n        except KeyError:\n            raise KeyError(""Weights had parameters {},"".format(key)\n                           + "" but could not find this parameters in model."")\n\n        if verbose:\n            print(\'Loading weights for {}\'.format(key))\n\n        # extend embedding layer to allow new randomly initialized words\n        # if requested. Otherwise, just load the weights for the layer.\n        if \'embed\' in key and extend_embedding > 0:\n            weight = torch.cat((weight, model_w[NB_TOKENS:, :]), dim=0)\n            if verbose:\n                print(\'Extended vocabulary for embedding layer \' +\n                      \'from {} to {} tokens.\'.format(\n                        NB_TOKENS, NB_TOKENS + extend_embedding))\n        try:\n            model_w.copy_(weight)\n        except:\n            print(\'While copying the weigths named {}, whose dimensions in the model are\'\n                  \' {} and whose dimensions in the saved file are {}, ...\'.format(\n                        key, model_w.size(), weight.size()))\n            raise\n'"
torchmoji/sentence_tokenizer.py,1,"b'# -*- coding: utf-8 -*-\n\'\'\'\nProvides functionality for converting a given list of tokens (words) into\nnumbers, according to the given vocabulary.\n\'\'\'\nfrom __future__ import print_function, division, unicode_literals\n\nimport numbers\nimport numpy as np\n\nfrom torchmoji.create_vocab import extend_vocab, VocabBuilder\nfrom torchmoji.word_generator import WordGenerator\nfrom torchmoji.global_variables import SPECIAL_TOKENS\n\n# import torch\n\nfrom sklearn.model_selection import train_test_split\n\nfrom copy import deepcopy\n\nclass SentenceTokenizer():\n    """""" Create numpy array of tokens corresponding to input sentences.\n        The vocabulary can include Unicode tokens.\n    """"""\n    def __init__(self, vocabulary, fixed_length, custom_wordgen=None,\n                 ignore_sentences_with_only_custom=False, masking_value=0,\n                 unknown_value=1):\n        """""" Needs a dictionary as input for the vocabulary.\n        """"""\n\n        if len(vocabulary) > np.iinfo(\'uint16\').max:\n            raise ValueError(\'Dictionary is too big ({} tokens) for the numpy \'\n                             \'datatypes used (max limit={}). Reduce vocabulary\'\n                             \' or adjust code accordingly!\'\n                             .format(len(vocabulary), np.iinfo(\'uint16\').max))\n\n        # Shouldn\'t be able to modify the given vocabulary\n        self.vocabulary = deepcopy(vocabulary)\n        self.fixed_length = fixed_length\n        self.ignore_sentences_with_only_custom = ignore_sentences_with_only_custom\n        self.masking_value = masking_value\n        self.unknown_value = unknown_value\n\n        # Initialized with an empty stream of sentences that must then be fed\n        # to the generator at a later point for reusability.\n        # A custom word generator can be used for domain-specific filtering etc\n        if custom_wordgen is not None:\n            assert custom_wordgen.stream is None\n            self.wordgen = custom_wordgen\n            self.uses_custom_wordgen = True\n        else:\n            self.wordgen = WordGenerator(None, allow_unicode_text=True,\n                                         ignore_emojis=False,\n                                         remove_variation_selectors=True,\n                                         break_replacement=True)\n            self.uses_custom_wordgen = False\n\n    def tokenize_sentences(self, sentences, reset_stats=True, max_sentences=None):\n        """""" Converts a given list of sentences into a numpy array according to\n            its vocabulary.\n\n        # Arguments:\n            sentences: List of sentences to be tokenized.\n            reset_stats: Whether the word generator\'s stats should be reset.\n            max_sentences: Maximum length of sentences. Must be set if the\n                length cannot be inferred from the input.\n\n        # Returns:\n            Numpy array of the tokenization sentences with masking,\n            infos,\n            stats\n\n        # Raises:\n            ValueError: When maximum length is not set and cannot be inferred.\n        """"""\n\n        if max_sentences is None and not hasattr(sentences, \'__len__\'):\n            raise ValueError(\'Either you must provide an array with a length\'\n                             \'attribute (e.g. a list) or specify the maximum \'\n                             \'length yourself using `max_sentences`!\')\n        n_sentences = (max_sentences if max_sentences is not None\n                       else len(sentences))\n\n        if self.masking_value == 0:\n            tokens = np.zeros((n_sentences, self.fixed_length), dtype=\'uint16\')\n        else:\n            tokens = (np.ones((n_sentences, self.fixed_length), dtype=\'uint16\')\n                      * self.masking_value)\n\n        if reset_stats:\n            self.wordgen.reset_stats()\n\n        # With a custom word generator info can be extracted from each\n        # sentence (e.g. labels)\n        infos = []\n\n        # Returns words as strings and then map them to vocabulary\n        self.wordgen.stream = sentences\n        next_insert = 0\n        n_ignored_unknowns = 0\n        for s_words, s_info in self.wordgen:\n            s_tokens = self.find_tokens(s_words)\n\n            if (self.ignore_sentences_with_only_custom and\n                np.all([True if t < len(SPECIAL_TOKENS)\n                        else False for t in s_tokens])):\n                n_ignored_unknowns += 1\n                continue\n            if len(s_tokens) > self.fixed_length:\n                s_tokens = s_tokens[:self.fixed_length]\n            tokens[next_insert,:len(s_tokens)] = s_tokens\n            infos.append(s_info)\n            next_insert += 1\n\n        # For standard word generators all sentences should be tokenized\n        # this is not necessarily the case for custom wordgenerators as they\n        # may filter the sentences etc.\n        if not self.uses_custom_wordgen and not self.ignore_sentences_with_only_custom:\n            assert len(sentences) == next_insert\n        else:\n            # adjust based on actual tokens received\n            tokens = tokens[:next_insert]\n            infos = infos[:next_insert]\n\n        return tokens, infos, self.wordgen.stats\n\n    def find_tokens(self, words):\n        assert len(words) > 0\n        tokens = []\n        for w in words:\n            try:\n                tokens.append(self.vocabulary[w])\n            except KeyError:\n                tokens.append(self.unknown_value)\n        return tokens\n\n    def split_train_val_test(self, sentences, info_dicts,\n                             split_parameter=[0.7, 0.1, 0.2], extend_with=0):\n        """""" Splits given sentences into three different datasets: training,\n            validation and testing.\n\n        # Arguments:\n            sentences: The sentences to be tokenized.\n            info_dicts: A list of dicts that contain information about each\n                sentence (e.g. a label).\n            split_parameter: A parameter for deciding the splits between the\n                three different datasets. If instead of being passed three\n                values, three lists are passed, then these will be used to\n                specify which observation belong to which dataset.\n            extend_with: An optional parameter. If > 0 then this is the number\n                of tokens added to the vocabulary from this dataset. The\n                expanded vocab will be generated using only the training set,\n                but is applied to all three sets.\n\n        # Returns:\n            List of three lists of tokenized sentences,\n\n            List of three corresponding dictionaries with information,\n\n            How many tokens have been added to the vocab. Make sure to extend\n            the embedding layer of the model accordingly.\n        """"""\n\n        # If passed three lists, use those directly\n        if isinstance(split_parameter, list) and \\\n                all(isinstance(x, list) for x in split_parameter) and \\\n                len(split_parameter) == 3:\n\n            # Helper function to verify provided indices are numbers in range\n            def verify_indices(inds):\n                return list(filter(lambda i: isinstance(i, numbers.Number)\n                            and i < len(sentences), inds))\n\n            ind_train = verify_indices(split_parameter[0])\n            ind_val = verify_indices(split_parameter[1])\n            ind_test = verify_indices(split_parameter[2])\n        else:\n            # Split sentences and dicts\n            ind = list(range(len(sentences)))\n            ind_train, ind_test = train_test_split(ind, test_size=split_parameter[2])\n            ind_train, ind_val = train_test_split(ind_train, test_size=split_parameter[1])\n\n        # Map indices to data\n        train = np.array([sentences[x] for x in ind_train])\n        test = np.array([sentences[x] for x in ind_test])\n        val = np.array([sentences[x] for x in ind_val])\n\n        info_train = np.array([info_dicts[x] for x in ind_train])\n        info_test = np.array([info_dicts[x] for x in ind_test])\n        info_val = np.array([info_dicts[x] for x in ind_val])\n\n        added = 0\n        # Extend vocabulary with training set tokens\n        if extend_with > 0:\n            wg = WordGenerator(train)\n            vb = VocabBuilder(wg)\n            vb.count_all_words()\n            added = extend_vocab(self.vocabulary, vb, max_tokens=extend_with)\n\n        # Wrap results\n        result = [self.tokenize_sentences(s)[0] for s in [train, val, test]]\n        result_infos = [info_train, info_val, info_test]\n        # if type(result_infos[0][0]) in [np.double, np.float, np.int64, np.int32, np.uint8]:\n        #     result_infos = [torch.from_numpy(label).long() for label in result_infos]\n\n        return result, result_infos, added\n\n    def to_sentence(self, sentence_idx):\n        """""" Converts a tokenized sentence back to a list of words.\n\n        # Arguments:\n            sentence_idx: List of numbers, representing a tokenized sentence\n                given the current vocabulary.\n\n        # Returns:\n            String created by converting all numbers back to words and joined\n            together with spaces.\n        """"""\n        # Have to recalculate the mappings in case the vocab was extended.\n        ind_to_word = {ind: word for word, ind in self.vocabulary.items()}\n\n        sentence_as_list = [ind_to_word[x] for x in sentence_idx]\n        cleaned_list = [x for x in sentence_as_list if x != \'CUSTOM_MASK\']\n        return "" "".join(cleaned_list)\n\n\ndef coverage(dataset, verbose=False):\n    """""" Computes the percentage of words in a given dataset that are unknown.\n\n    # Arguments:\n        dataset: Tokenized dataset to be checked.\n        verbose: Verbosity flag.\n\n    # Returns:\n        Percentage of unknown tokens.\n    """"""\n    n_total = np.count_nonzero(dataset)\n    n_unknown = np.sum(dataset == 1)\n    coverage = 1.0 - float(n_unknown) / n_total\n\n    if verbose:\n        print(""Unknown words: {}"".format(n_unknown))\n        print(""Total words: {}"".format(n_total))\n        print(""Coverage: {}"".format(coverage))\n    return coverage\n'"
torchmoji/tokenizer.py,0,"b'# -*- coding: utf-8 -*-\n\'\'\'\nSplits up a Unicode string into a list of tokens.\nRecognises:\n- Abbreviations\n- URLs\n- Emails\n- #hashtags\n- @mentions\n- emojis\n- emoticons (limited support)\n\nMultiple consecutive symbols are also treated as a single token.\n\'\'\'\nfrom __future__ import absolute_import, division, print_function, unicode_literals\n\nimport re\n\n# Basic patterns.\nRE_NUM = r\'[0-9]+\'\nRE_WORD = r\'[a-zA-Z]+\'\nRE_WHITESPACE = r\'\\s+\'\nRE_ANY = r\'.\'\n\n# Combined words such as \'red-haired\' or \'CUSTOM_TOKEN\'\nRE_COMB = r\'[a-zA-Z]+[-_][a-zA-Z]+\'\n\n# English-specific patterns\nRE_CONTRACTIONS = RE_WORD + r\'\\\'\' + RE_WORD\n\nTITLES = [\n    r\'Mr\\.\',\n    r\'Ms\\.\',\n    r\'Mrs\\.\',\n    r\'Dr\\.\',\n    r\'Prof\\.\',\n    ]\n# Ensure case insensitivity\nRE_TITLES = r\'|\'.join([r\'(?i)\' + t for t in TITLES])\n\n# Symbols have to be created as separate patterns in order to match consecutive\n# identical symbols.\nSYMBOLS = r\'()<!?.,/\\\'\\""-_=\\\\\xc2\xa7|\xc2\xb4\xcb\x87\xc2\xb0[]<>{}~$^&*;:%+\\xa3\xe2\x82\xac`\'\nRE_SYMBOL = r\'|\'.join([re.escape(s) + r\'+\' for s in SYMBOLS])\n\n# Hash symbols and at symbols have to be defined separately in order to not\n# clash with hashtags and mentions if there are multiple - i.e.\n# ##hello -> [\'#\', \'#hello\'] instead of [\'##\', \'hello\']\nSPECIAL_SYMBOLS = r\'|#+(?=#[a-zA-Z0-9_]+)|@+(?=@[a-zA-Z0-9_]+)|#+|@+\'\nRE_SYMBOL += SPECIAL_SYMBOLS\n\nRE_ABBREVIATIONS = r\'\\b(?<!\\.)(?:[A-Za-z]\\.){2,}\'\n\n# Twitter-specific patterns\nRE_HASHTAG = r\'#[a-zA-Z0-9_]+\'\nRE_MENTION = r\'@[a-zA-Z0-9_]+\'\n\nRE_URL = r\'(?:https?://|www\\.)(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+\'\nRE_EMAIL = r\'\\b[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+\\b\'\n\n# Emoticons and emojis\nRE_HEART = r\'(?:<+/?3+)+\'\nEMOTICONS_START = [\n    r\'>:\',\n    r\':\',\n    r\'=\',\n    r\';\',\n    ]\nEMOTICONS_MID = [\n    r\'-\',\n    r\',\',\n    r\'^\',\n    \'\\\'\',\n    \'\\""\',\n    ]\nEMOTICONS_END = [\n    r\'D\',\n    r\'d\',\n    r\'p\',\n    r\'P\',\n    r\'v\',\n    r\')\',\n    r\'o\',\n    r\'O\',\n    r\'(\',\n    r\'3\',\n    r\'/\',\n    r\'|\',\n    \'\\\\\',\n    ]\nEMOTICONS_EXTRA = [\n    r\'-_-\',\n    r\'x_x\',\n    r\'^_^\',\n    r\'o.o\',\n    r\'o_o\',\n    r\'(:\',\n    r\'):\',\n    r\');\',\n    r\'(;\',\n    ]\n\nRE_EMOTICON = r\'|\'.join([re.escape(s) for s in EMOTICONS_EXTRA])\nfor s in EMOTICONS_START:\n    for m in EMOTICONS_MID:\n        for e in EMOTICONS_END:\n            RE_EMOTICON += \'|{0}{1}?{2}+\'.format(re.escape(s), re.escape(m), re.escape(e))\n\n# requires ucs4 in python2.7 or python3+\n# RE_EMOJI = r""""""[\\U0001F300-\\U0001F64F\\U0001F680-\\U0001F6FF\\u2600-\\u26FF\\u2700-\\u27BF]""""""\n# safe for all python\nRE_EMOJI = r""""""\\ud83c[\\udf00-\\udfff]|\\ud83d[\\udc00-\\ude4f\\ude80-\\udeff]|[\\u2600-\\u26FF\\u2700-\\u27BF]""""""\n\n# List of matched token patterns, ordered from most specific to least specific.\nTOKENS = [\n    RE_URL,\n    RE_EMAIL,\n    RE_COMB,\n    RE_HASHTAG,\n    RE_MENTION,\n    RE_HEART,\n    RE_EMOTICON,\n    RE_CONTRACTIONS,\n    RE_TITLES,\n    RE_ABBREVIATIONS,\n    RE_NUM,\n    RE_WORD,\n    RE_SYMBOL,\n    RE_EMOJI,\n    RE_ANY\n    ]\n\n# List of ignored token patterns\nIGNORED = [\n    RE_WHITESPACE\n    ]\n\n# Final pattern\nRE_PATTERN = re.compile(r\'|\'.join(IGNORED) + r\'|(\' + r\'|\'.join(TOKENS) + r\')\',\n                        re.UNICODE)\n\n\ndef tokenize(text):\n    \'\'\'Splits given input string into a list of tokens.\n\n    # Arguments:\n        text: Input string to be tokenized.\n\n    # Returns:\n        List of strings (tokens).\n    \'\'\'\n    result = RE_PATTERN.findall(text)\n\n    # Remove empty strings\n    result = [t for t in result if t.strip()]\n    return result\n'"
torchmoji/word_generator.py,0,"b'# -*- coding: utf-8 -*-\n\'\'\' Extracts lists of words from a given input to be used for later vocabulary\n    generation or for creating tokenized datasets.\n    Supports functionality for handling different file types and\n    filtering/processing of this input.\n\'\'\'\n\nfrom __future__ import division, print_function, unicode_literals\n\nimport re\nimport unicodedata\nimport numpy as np\nfrom text_unidecode import unidecode\n\nfrom torchmoji.tokenizer import RE_MENTION, tokenize\nfrom torchmoji.filter_utils import (convert_linebreaks,\n                                           convert_nonbreaking_space,\n                                           correct_length,\n                                           extract_emojis,\n                                           mostly_english,\n                                           non_english_user,\n                                           process_word,\n                                           punct_word,\n                                           remove_control_chars,\n                                           remove_variation_selectors,\n                                           separate_emojis_and_text)\n\ntry:\n    unicode        # Python 2\nexcept NameError:\n    unicode = str  # Python 3\n\n# Only catch retweets in the beginning of the tweet as those are the\n# automatically added ones.\n# We do not want to remove tweets like ""Omg.. please RT this!!""\nRETWEETS_RE = re.compile(r\'^[rR][tT]\')\n\n# Use fast and less precise regex for removing tweets with URLs\n# It doesn\'t matter too much if a few tweets with URL\'s make it through\nURLS_RE = re.compile(r\'https?://|www\\.\')\n\nMENTION_RE = re.compile(RE_MENTION)\nALLOWED_CONVERTED_UNICODE_PUNCTUATION = """"""!""#$\'()+,-.:;<=>?@`~""""""\n\n\nclass WordGenerator():\n    \'\'\' Cleanses input and converts into words. Needs all sentences to be in\n        Unicode format. Has subclasses that read sentences differently based on\n        file type.\n\n    Takes a generator as input. This can be from e.g. a file.\n    unicode_handling in [\'ignore_sentence\', \'convert_punctuation\', \'allow\']\n    unicode_handling in [\'ignore_emoji\', \'ignore_sentence\', \'allow\']\n    \'\'\'\n    def __init__(self, stream, allow_unicode_text=False, ignore_emojis=True,\n                 remove_variation_selectors=True, break_replacement=True):\n        self.stream = stream\n        self.allow_unicode_text = allow_unicode_text\n        self.remove_variation_selectors = remove_variation_selectors\n        self.ignore_emojis = ignore_emojis\n        self.break_replacement = break_replacement\n        self.reset_stats()\n\n    def get_words(self, sentence):\n        """""" Tokenizes a sentence into individual words.\n            Converts Unicode punctuation into ASCII if that option is set.\n            Ignores sentences with Unicode if that option is set.\n            Returns an empty list of words if the sentence has Unicode and\n            that is not allowed.\n        """"""\n\n        if not isinstance(sentence, unicode):\n            raise ValueError(""All sentences should be Unicode-encoded!"")\n        sentence = sentence.strip().lower()\n\n        if self.break_replacement:\n            sentence = convert_linebreaks(sentence)\n\n        if self.remove_variation_selectors:\n            sentence = remove_variation_selectors(sentence)\n\n        # Split into words using simple whitespace splitting and convert\n        # Unicode. This is done to prevent word splitting issues with\n        # twokenize and Unicode\n        words = sentence.split()\n        converted_words = []\n        for w in words:\n            accept_sentence, c_w = self.convert_unicode_word(w)\n            # Unicode word detected and not allowed\n            if not accept_sentence:\n                return []\n            else:\n                converted_words.append(c_w)\n        sentence = \' \'.join(converted_words)\n\n        words = tokenize(sentence)\n        words = [process_word(w) for w in words]\n        return words\n\n    def check_ascii(self, word):\n        """""" Returns whether a word is ASCII """"""\n\n        try:\n            word.decode(\'ascii\')\n            return True\n        except (UnicodeDecodeError, UnicodeEncodeError, AttributeError):\n            return False\n\n    def convert_unicode_punctuation(self, word):\n        word_converted_punct = []\n        for c in word:\n            decoded_c = unidecode(c).lower()\n            if len(decoded_c) == 0:\n                # Cannot decode to anything reasonable\n                word_converted_punct.append(c)\n            else:\n                # Check if all punctuation and therefore fine\n                # to include unidecoded version\n                allowed_punct = punct_word(\n                        decoded_c,\n                        punctuation=ALLOWED_CONVERTED_UNICODE_PUNCTUATION)\n\n                if allowed_punct:\n                    word_converted_punct.append(decoded_c)\n                else:\n                    word_converted_punct.append(c)\n        return \'\'.join(word_converted_punct)\n\n    def convert_unicode_word(self, word):\n        """""" Converts Unicode words to ASCII using unidecode. If Unicode is not\n            allowed (set as a variable during initialization), then only\n            punctuation that can be converted to ASCII will be allowed.\n        """"""\n        if self.check_ascii(word):\n            return True, word\n\n        # First we ensure that the Unicode is normalized so it\'s\n        # always a single character.\n        word = unicodedata.normalize(""NFKC"", word)\n\n        # Convert Unicode punctuation to ASCII equivalent. We want\n        # e.g. ""\\u203c"" (double exclamation mark) to be treated the same\n        # as ""!!"" no matter if we allow other Unicode characters or not.\n        word = self.convert_unicode_punctuation(word)\n\n        if self.ignore_emojis:\n            _, word = separate_emojis_and_text(word)\n\n        # If conversion of punctuation and removal of emojis took care\n        # of all the Unicode or if we allow Unicode then everything is fine\n        if self.check_ascii(word) or self.allow_unicode_text:\n            return True, word\n        else:\n            # Sometimes we might want to simply ignore Unicode sentences\n            # (e.g. for vocabulary creation). This is another way to prevent\n            # ""polution"" of strange Unicode tokens from low quality datasets\n            return False, \'\'\n\n    def data_preprocess_filtering(self, line, iter_i):\n        """""" To be overridden with specific preprocessing/filtering behavior\n            if desired.\n\n            Returns a boolean of whether the line should be accepted and the\n            preprocessed text.\n\n            Runs prior to tokenization.\n        """"""\n        return True, line, {}\n\n    def data_postprocess_filtering(self, words, iter_i):\n        """""" To be overridden with specific postprocessing/filtering behavior\n            if desired.\n\n            Returns a boolean of whether the line should be accepted and the\n            postprocessed text.\n\n            Runs after tokenization.\n        """"""\n        return True, words, {}\n\n    def extract_valid_sentence_words(self, line):\n        """""" Line may either a string of a list of strings depending on how\n            the stream is being parsed.\n            Domain-specific processing and filtering can be done both prior to\n            and after tokenization.\n            Custom information about the line can be extracted during the\n            processing phases and returned as a dict.\n        """"""\n\n        info = {}\n\n        pre_valid, pre_line, pre_info = \\\n            self.data_preprocess_filtering(line, self.stats[\'total\'])\n        info.update(pre_info)\n        if not pre_valid:\n            self.stats[\'pretokenization_filtered\'] += 1\n            return False, [], info\n\n        words = self.get_words(pre_line)\n        if len(words) == 0:\n            self.stats[\'unicode_filtered\'] += 1\n            return False, [], info\n\n        post_valid, post_words, post_info = \\\n            self.data_postprocess_filtering(words, self.stats[\'total\'])\n        info.update(post_info)\n        if not post_valid:\n            self.stats[\'posttokenization_filtered\'] += 1\n        return post_valid, post_words, info\n\n    def generate_array_from_input(self):\n        sentences = []\n        for words in self:\n            sentences.append(words)\n        return sentences\n\n    def reset_stats(self):\n        self.stats = {\'pretokenization_filtered\': 0,\n                      \'unicode_filtered\': 0,\n                      \'posttokenization_filtered\': 0,\n                      \'total\': 0,\n                      \'valid\': 0}\n\n    def __iter__(self):\n        if self.stream is None:\n            raise ValueError(""Stream should be set before iterating over it!"")\n\n        for line in self.stream:\n            valid, words, info = self.extract_valid_sentence_words(line)\n\n            # Words may be filtered away due to unidecode etc.\n            # In that case the words should not be passed on.\n            if valid and len(words):\n                self.stats[\'valid\'] += 1\n                yield words, info\n\n            self.stats[\'total\'] += 1\n\n\nclass TweetWordGenerator(WordGenerator):\n    \'\'\' Returns np array or generator of ASCII sentences for given tweet input.\n        Any file opening/closing should be handled outside of this class.\n    \'\'\'\n    def __init__(self, stream, wanted_emojis=None, english_words=None,\n                 non_english_user_set=None, allow_unicode_text=False,\n                 ignore_retweets=True, ignore_url_tweets=True,\n                 ignore_mention_tweets=False):\n\n        self.wanted_emojis = wanted_emojis\n        self.english_words = english_words\n        self.non_english_user_set = non_english_user_set\n        self.ignore_retweets = ignore_retweets\n        self.ignore_url_tweets = ignore_url_tweets\n        self.ignore_mention_tweets = ignore_mention_tweets\n        WordGenerator.__init__(self, stream,\n                               allow_unicode_text=allow_unicode_text)\n\n    def validated_tweet(self, data):\n        \'\'\' A bunch of checks to determine whether the tweet is valid.\n            Also returns emojis contained by the tweet.\n        \'\'\'\n\n        # Ordering of validations is important for speed\n        # If it passes all checks, then the tweet is validated for usage\n\n        # Skips incomplete tweets\n        if len(data) <= 9:\n            return False, []\n\n        text = data[9]\n\n        if self.ignore_retweets and RETWEETS_RE.search(text):\n            return False, []\n\n        if self.ignore_url_tweets and URLS_RE.search(text):\n            return False, []\n\n        if self.ignore_mention_tweets and MENTION_RE.search(text):\n            return False, []\n\n        if self.wanted_emojis is not None:\n            uniq_emojis = np.unique(extract_emojis(text, self.wanted_emojis))\n            if len(uniq_emojis) == 0:\n                return False, []\n        else:\n            uniq_emojis = []\n\n        if self.non_english_user_set is not None and \\\n           non_english_user(data[1], self.non_english_user_set):\n            return False, []\n        return True, uniq_emojis\n\n    def data_preprocess_filtering(self, line, iter_i):\n        fields = line.strip().split(""\\t"")\n        valid, emojis = self.validated_tweet(fields)\n        text = fields[9].replace(\'\\\\n\', \'\') \\\n                        .replace(\'\\\\r\', \'\') \\\n                        .replace(\'&amp\', \'&\') if valid else \'\'\n        return valid, text, {\'emojis\': emojis}\n\n    def data_postprocess_filtering(self, words, iter_i):\n        valid_length = correct_length(words, 1, None)\n        valid_english, n_words, n_english = mostly_english(words,\n                                                           self.english_words)\n        if valid_length and valid_english:\n            return True, words, {\'length\': len(words),\n                                 \'n_normal_words\': n_words,\n                                 \'n_english\': n_english}\n        else:\n            return False, [], {\'length\': len(words),\n                               \'n_normal_words\': n_words,\n                               \'n_english\': n_english}\n'"
