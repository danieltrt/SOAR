file_path,api_count,code
setup.py,0,"b""import os\nfrom setuptools import setup\n\nHYPERDASH_PACKAGE = 'hyperdash'\nCLI_PACKAGE = 'hyperdash_cli'\n\nVERSION_FILE = 'VERSION'\nPYPI_README_FILE = 'PYPI_README.rst'\n\nwith open(os.path.join(HYPERDASH_PACKAGE, VERSION_FILE), 'r') as version_file:\n    VERSION = version_file.read().strip()\n\nwith open(os.path.join(HYPERDASH_PACKAGE, PYPI_README_FILE), 'r') as readme_file:\n    LONG_DESCRIPTION = readme_file.read().strip()\n\nsetup(\n    name=HYPERDASH_PACKAGE,\n    packages=[HYPERDASH_PACKAGE, CLI_PACKAGE],\n    install_requires=[\n        'requests',\n        'six>=1.10.0',\n        'python-slugify',\n    ],\n    entry_points={\n        'console_scripts': [\n            'hyperdash = hyperdash_cli.cli:main',\n            'hd = hyperdash_cli.cli:main',\n            ]\n    },\n    package_data={'': [VERSION_FILE, PYPI_README_FILE]},\n    version=VERSION,\n    description='Hyperdash.io CLI and SDK',\n    long_description=LONG_DESCRIPTION,\n    author='Hyperdash',\n    author_email='support@hyperdash.io',\n    url='https://hyperdash.io',\n)\n"""
hyperdash/__init__.py,0,b'from .monitor import monitor\nfrom .jupyter import IPythonMagicsWrapper as IPythonMagicsWrapper\nfrom .experiment import Experiment\n\n# No-op just to make import nicer\ndef monitor_cell():\n  pass\n\ntry:\n  ip = get_ipython()\n  ip.register_magics(IPythonMagicsWrapper)\nexcept NameError:\n  pass\n'
hyperdash/client.py,0,"b'import time\n\nimport numbers\nimport six\nimport json\n\nfrom .sdk_message import create_metric_message\nfrom .sdk_message import create_param_message\n\n\nclass HDClient:\n    def __init__(self, logger, server_manager, sdk_run_uuid):\n        self.logger = logger\n        self._server_manager = server_manager\n        self._sdk_run_uuid = sdk_run_uuid\n        # Keeps track of which parameters have been seen before\n        # so we can prevent duplicates\n        self._seen_params = set()\n        # Keeps track of how many iterators have been created\n        # so we can give them distinct names\n        self._iter_num = 0\n        # Keep track of the last time we saw a metric so we can\n        # limit how often the are emitted\n        self._last_seen_metrics = {}\n\n    def metric(self, name, value, log=True):\n        """"""Emit a datapoint for a named timeseries.\n\n        Optional log parameter controls whether the metric is\n        logged / printed to STDOUT.\n        """"""\n        return self._metric(name, time.time(), value, log, False)\n\n    # This is gross, but be careful when modifying this functions signature as its used by the\n    # CLI in the tensorboard command.\n    def _metric(self, name, current_time, value, log=True, is_internal=False, sample_frequency_per_second=1):\n        assert isinstance(value, numbers.Real), ""value must be a real number.""\n        assert isinstance(name, six.string_types)\n        assert isinstance(sample_frequency_per_second, numbers.Real), ""sample_frequency_per_second must be a real number.""\n        assert value is not None and name is not None and sample_frequency_per_second is not None, ""value and name and sample_frequency_per_second must not be None.""\n        # We\'ve already determined its a real number, but some objects that satisfy the real number\n        # constraint (like numpy numbers) are not JSON serializable unless converted.\n        value = float(value)\n\n        last_seen_at = self._last_seen_metrics.get(name, None)\n        if last_seen_at and (current_time - last_seen_at < (1.0/float(sample_frequency_per_second))):\n            # Not enough time has elapsed since the last time this metric was emitted\n            return\n\n        message = create_metric_message(\n            self._sdk_run_uuid, name, current_time, value, is_internal)\n        self._server_manager.put_buf(message)\n        self._last_seen_metrics[name] = current_time\n        if log:\n            self.logger.info(""| {0}: {1:10f} |"".format(name, value))\n\n    def param(self, name, val, log=True):\n        """"""Associate a hyperparameter with the given experiment.\n\n        Optional log parameter controls whether the hyperparameter\n        is logged / printed to STDOUT.\n        """"""\n        return self._param(name, val, log, False)\n\n    def _param(self, name, val, log=True, is_internal=False):\n        assert isinstance(name, six.string_types), ""name must be a string.""\n        # Make sure its JSON serializable\n        try:\n            json.dumps(val)\n        except TypeError:\n            # If its not, see if its a number\n            if isinstance(val, numbers.Real):\n                val = float(val)\n            else:\n            # Otherwise, just convert it to a string\n                val = str(val)\n        assert name not in self._seen_params, ""hyperparameters should be unique and not reused""\n\n        params = {}\n        params[name] = val\n        message = create_param_message(self._sdk_run_uuid, params, is_internal)\n        self._server_manager.put_buf(message)\n        self._seen_params.add(name)\n        if log:\n            self.logger.info(""{{ {}: {} }}"".format(name, val))\n        return val\n\n    def iter(self, n, log=True):\n        """"""Returns an iterator with the specified number of iterations.\n\n        The iter method automatically associated the number of iterations\n        with the experiment, as well as emits timeseries data for each\n        iteration so that progress can be monitored.\n        """"""\n        i = 0\n        # Capture the existing iterator number\n        iter_num = self._iter_num\n        # Increment the iterator number for subsequent calls\n        self._iter_num += 1\n        self._param(""hd_iter_{}_epochs"".format(iter_num),\n                    n, log=False, is_internal=True)\n        while i < n:\n            if log:\n                self.logger.info(""| Iteration {} of {} |"".format(i, n - 1))\n            self._metric(\n                ""hd_iter_{}"".format(iter_num), time.time(), i, log=False, is_internal=True)\n            yield i\n            i += 1\n\n    def end(self):\n        self.logger.warning(""end() call is unneccessary while using decorator syntax."")'"
hyperdash/code_runner.py,0,"b'# Python 2/3 compatibility\nfrom __future__ import absolute_import, division, print_function, unicode_literals\n\nfrom datetime import datetime\nfrom inspect import getargspec\nfrom threading import Lock\nfrom traceback import format_exc\n\n\n# Python 2/3 compatibility\n__metaclass__ = type\n\n\nclass CodeRunner:\n\n    def __init__(self, f, hd_client, parent_logger, *args, **kwargs):\n        self.logger = parent_logger.getChild(__name__)\n        self.hd_client = hd_client\n        self.f = self.wrap(f, *args, **kwargs)\n        self.done = False\n        self.exited_cleanly = True\n        self.return_val = None\n        self.exception = None\n        self.lock = Lock()\n        self.start_time = None\n        self.end_time = None\n\n    def wrap(self, f, *args, **kwargs):\n        arg_spec = getargspec(f)\n        # Make sure function signature can handle injected hyperdash object\n        if ""exp"" in arg_spec.args or arg_spec.keywords:\n            # TODO: Inject in constructor instead of instantiating here\n            kwargs[""exp""] = self.hd_client\n\n        def wrapped():\n            # TODO: Error handling\n            return_val = None\n            try:\n                self.start_time = datetime.now()\n                return_val = f(*args, **kwargs)\n                self.end_time = datetime.now()\n            except Exception as e:\n                self.logger.error(format_exc())\n                with self.lock:\n                    self.exited_cleanly = False\n                    self.exception = e\n            finally:\n                with self.lock:\n                    self.done = True\n                    self.return_val = return_val\n        return wrapped\n\n    def run(self):\n        self.f()\n\n    def is_done(self):\n        with self.lock:\n            return self.exited_cleanly, self.done\n\n    def get_return_val(self):\n        with self.lock:\n            return self.return_val\n\n    def get_exception(self):\n        with self.lock:\n            return self.exception\n\n    def should_run_as_thread(self):\n        return True\n\n    def get_start_and_end_time(self):\n        with self.lock:\n            return self.start_time, self.end_time\n'"
hyperdash/commands.py,0,"b'STOP = ""stop""\n'"
hyperdash/constants.py,0,"b'import os\nimport sys\n\nimport six\n\nfrom slugify import slugify\n\nAUTH_KEY_NAME = ""x-hyperdash-auth""\nVERSION_KEY_NAME = ""x-hyperdash-version""\nAPI_KEY_NAME = ""x-hyperdash-api""\nHTTP_ENDPOINT = ""/api/v1/sdk/http""\nCACHE_API_KEY_FOR_SECONDS = 300\n# 20 KiB\nMAX_LOG_SIZE_BYTES = 20480\n\nAPI_NAME_MONITOR = ""monitor""\nAPI_NAME_EXPERIMENT = ""experiment""\nAPI_NAME_CLI_RUN = ""cli_run""\nAPI_NAME_CLI_PIPE = ""cli_pipe""\nAPI_NAME_CLI_TENSORBOARD = ""cli_tensorboard""\nAPI_NAME_JUPYTER = ""jupyter""\n\ndef get_base_http_url():\n    return six.text_type(os.environ.get(\n        ""HYPERDASH_SERVER"",\n        ""https://hyperdash.io"",\n    ))\n\n\ndef get_http_url():\n    return get_base_http_url() + HTTP_ENDPOINT\n\n\ndef get_hyperdash_json_paths():\n    return [\n        path for\n        path in\n        [get_hyperdash_json_home_path(), get_hyperdash_json_local_path()]\n        if path\n    ]\n\n\ndef get_hyperdash_home_path():\n    return os.path.join(os.path.expanduser(""~""), "".hyperdash"")\n\n\ndef get_hyperdash_json_home_path():\n    return os.path.join(get_hyperdash_home_path(), ""hyperdash.json"")\n\n\ndef get_hyperdash_logs_home_path():\n    return os.path.join(get_hyperdash_home_path(), ""logs"")\n\n\ndef get_hyperdash_logs_home_path_for_job(job):\n    return os.path.join(get_hyperdash_logs_home_path(), slugify(job))\n\n\ndef get_hyperdash_local_path():\n    main = sys.modules[""__main__""]\n    if not hasattr(main, ""__file__""):\n        return None\n\n    main_file_path = os.path.abspath(main.__file__)\n    return os.path.dirname(main_file_path)\n\n\ndef get_hyperdash_json_local_path():\n    local_path = get_hyperdash_local_path()\n    if not local_path:\n        return None\n    return os.path.join(local_path, ""hyperdash.json"")\n\n\ndef get_hyperdash_version():\n    """"""Return the version of Hyperdash as a string.""""""\n    with open(os.path.join(os.path.dirname(__file__), ""VERSION""), ""r"") as f:\n        return f.read().strip()\n'"
hyperdash/experiment.py,0,"b'from __future__ import absolute_import, division, print_function, unicode_literals\n\nimport sys\nimport uuid\nimport threading\nfrom threading import Lock\n\nfrom datetime import datetime\n\nfrom six.moves.queue import Queue\n\nfrom .client import HDClient\nfrom .constants import API_NAME_EXPERIMENT\nfrom .constants import API_NAME_CLI_TENSORBOARD\nfrom .monitor import monitor\nfrom .io_buffer import IOBuffer\nfrom .server_manager import ServerManagerHTTP\nfrom .hyper_dash import HyperDash\nfrom .utils import get_logger\n\n# Python 2/3 compatibility\n__metaclass__ = type\n\nKERAS = ""keras""\n\n\nclass ExperimentRunner:\n    """"""\n        No-op class for reusing CodeRunner architecture\n    """"""\n    def __init__(\n        self,\n        done=False,\n        exit_cleanly=True,\n    ):\n        self.done = done\n        self.lock = Lock()\n        self.exit_cleanly = exit_cleanly\n        self.start_time = None\n        self.end_time = None\n\n    def is_done(self):\n        with self.lock:\n            return self.exit_cleanly, self.done\n\n    def get_return_val(self):\n        with self.lock:\n            return None\n\n    def get_exception(self):\n        with self.lock:\n            return None\n        \n    def should_run_as_thread(self):\n        return False\n\n    def get_start_and_end_time(self):\n        return self.start_time, self.end_time\n\n    def _set_start_time(self, start_time):\n        self.start_time = start_time\n\n    def _set_end_time(self, end_time):\n        self.end_time = end_time\n\nclass Experiment:\n    """"""Experiment records hyperparameters and metrics. The recorded values\n    are sent to the Hyperdash server.\n\n    Example:\n      exp = Experiment(""MNIST"")\n      exp.param(""batch size"", 32)\n    """"""\n    _api_name = API_NAME_EXPERIMENT\n\n    def __init__(\n        self,\n        model_name,\n        api_key_getter=None,\n        capture_io=True,\n    ):\n        """"""Initialize the HyperDash class.\n\n        args:\n            1) model_name: Name of the model. Experiment number will autoincrement. \n            2) capture_io: Should save stdout/stderror to log file and upload it to Hyperdash.\n        """"""\n        self.model_name = model_name\n        self.callbacks = Callbacks(self)\n        self._experiment_runner = ExperimentRunner()\n        self.lock = Lock()\n\n        # Create a UUID to uniquely identify this run from the SDK\'s point of view\n        current_sdk_run_uuid = str(uuid.uuid4())\n\n        # Capture STDOUT/STDERR before they\'re modified\n        self._old_out, self._old_err = sys.stdout, sys.stderr\n\n        # Buffers to which to redirect output so we can capture it\n        out = [IOBuffer(), IOBuffer()]\n\n        self._logger = get_logger(model_name, current_sdk_run_uuid, out[0])\n\n        if capture_io:\n            # Redirect STDOUT/STDERR to buffers\n            sys.stdout, sys.stderr = out\n\n        server_manager = ServerManagerHTTP(api_key_getter, self._logger, self._api_name)\n        self._hd_client = HDClient(self._logger, server_manager, current_sdk_run_uuid)\n        self._hd = HyperDash(\n            model_name,\n            current_sdk_run_uuid,\n            server_manager,\n            out,\n            (self._old_out, self._old_err,),\n            self._logger,\n            self._experiment_runner,\n        )\n\n        # Channel to update once experiment has finished running\n        # Syncs with the seperate hyperdash messaging loop thread\n        self.done_chan = Queue()\n        def run():\n            self._experiment_runner._set_start_time(datetime.now())\n            self._hd.run()\n            self._experiment_runner._set_end_time(datetime.now())\n            self.done_chan.put(True)\n        exp_thread = threading.Thread(target=run)\n        exp_thread.daemon = True\n        exp_thread.start()\n        self._ended = False\n\n    def metric(self, name, value, log=True):\n        if self._ended:\n            self._logger.warn(""Cannot send metric {}, experiment ended. Please start a new experiment."".format(name))\n            return\n        return self._hd_client.metric(name, value, log)\n\n    def param(self, name, value, log=True):\n        if self._ended:\n            self._logger.warn(""Cannot send param {}, experiment ended. Please start a new experiment."".format(name))\n            return\n        return self._hd_client.param(name, value, log)\n\n    def iter(self, n, log=True):\n        if self._ended:\n            self._logger.warn(""Cannot iterate, experiment ended. Please start a new experiment."")\n            return\n        return self._hd_client.iter(n, log)\n\n    def end(self):\n        if self._ended:\n            return\n\n        self._ended = True\n        with self.lock:\n            sys.stdout, sys.stderr = self._old_out, self._old_err\n            self._experiment_runner.exit_cleanly = True\n            self._experiment_runner.done = True\n\n        # Makes sure the experiment runner has cleaned up fully    \n        self.done_chan.get(block=True, timeout=None)\n    """"""\n    For selective logging while capture_io is disabled\n    \n    Main use case is if you output large amounts of text to STDOUT\n    but only want a subset saved to logs\n    """"""\n    def log(self, string):\n        self._logger.info(string)\n\n\nclass Callbacks:\n    """"""Callbacks is a container class for 3rd-party library callbacks.\n   \n    An instance of Experiment is injected so that the callbacks can emit\n    metrics/logs/parameters on behalf of an experiment.\n    """"""\n    def __init__(self, exp):\n        self._exp = exp\n        self._callbacks = {}\n\n    @property\n    def keras(self):\n        """"""\n        Returns an object that implements the Keras Callback interface.\n\n        This method initializes the Keras callback lazily to to prevent\n        any possible import issues from affecting users who don\'t use it,\n        as well as prevent it from importing Keras/tensorflow and all of\n        their accompanying baggage unnecessarily in the case that they\n        happened to be installed, but the user is not using them.\n        """"""\n        cb = self._callbacks.get(KERAS)\n        # Keras is not importable\n        if cb is False:\n            return None\n        # If this is the first time, try and import Keras\n        if not cb:\n            # Check if Keras is installed and fallback gracefully\n            try:\n                from keras.callbacks import Callback as KerasCallback\n                class _KerasCallback(KerasCallback):\n                    """"""_KerasCallback implement KerasCallback using an injected Experiment.\n                    \n                    # TODO: Decide if we want to handle the additional callbacks:\n                    # 1) on_epoch_begin\n                    # 2) on_batch_begin\n                    # 3) on_batch_end\n                    # 4) on_train_begin\n                    # 5) on_train_end\n                    """"""\n                    def __init__(self, exp):\n                        super(_KerasCallback, self).__init__()\n                        self._exp = exp\n                    \n                    def on_epoch_end(self, epoch, logs=None):\n                        if not logs:\n                            logs = {}\n                        val_acc = logs.get(""val_acc"")\n                        val_loss = logs.get(""val_loss"")\n\n                        if val_acc is not None:\n                            self._exp.metric(""val_acc"", val_acc)\n                        if val_loss is not None:\n                            self._exp.metric(""val_loss"", val_loss)\n                cb = _KerasCallback(self._exp)\n                self._callbacks[KERAS] = cb\n                return cb\n            except ImportError:\n                # Mark Keras as unimportable for future calls                \n                self._callbacks[KERAS] = False\n                return None\n        return cb\n\n\n# Version of Experiment with a different name for use internally, should not be used directly by consumers\nclass _TensorboardExperiment(Experiment):\n    _api_name = API_NAME_CLI_TENSORBOARD\n\n    def __init__(self, *args, **kwargs):\n        Experiment.__init__(self, *args, **kwargs)\n'"
hyperdash/hyper_dash.py,0,"b'# Python 2/3 compatibility\nfrom __future__ import absolute_import, division, print_function, unicode_literals\nfrom threading import Thread\n\nimport datetime\nimport json\nimport os\nimport sys\nimport time\nimport uuid\n\nfrom six.moves.queue import Queue\nfrom six import PY2\nfrom slugify import slugify\n\nfrom .code_runner import CodeRunner\nfrom .constants import get_hyperdash_logs_home_path\nfrom .constants import get_hyperdash_logs_home_path_for_job\nfrom .constants import MAX_LOG_SIZE_BYTES\nfrom .sdk_message import create_run_started_message\nfrom .sdk_message import create_run_ended_message\nfrom .sdk_message import create_log_message\nfrom .utils import human_readable_duration\n\n# Python 2/3 compatibility\n__metaclass__ = type\n\n\nINFO_LEVEL = \'INFO\'\nERROR_LEVEL = \'ERROR\'\n\n\nclass HyperDash:\n    """"""HyperDash monitors a job and manages capturing IO / server comms.\n\n    This class is designed to be run in its own thread and contains an instance\n    of runner (which is running the job) and server_manager (for talking\n    to the server.)\n    """"""\n\n    def __init__(\n        self,\n        job_name,\n        current_sdk_run_uuid,\n        server_manager,\n        io_bufs,\n        std_streams,\n        parent_logger,\n        runner,\n    ):\n        """"""Initialize the HyperDash class.\n\n        args:\n            1) job_name: Name of the current running job\n            2) current_sdk_run_uuid: UUID of current run\n            3) runner: Instance of CodeRunner or ExperimentRunner\n            4) server_manager: ServerManager instance\n            5) io_bufs: Tuple in the form of (StringIO(), StringIO(),)\n            6) std_streams: Tuple in the form of (StdOut, StdErr)\n        """"""\n        self.job_name = job_name\n        self.current_sdk_run_uuid = current_sdk_run_uuid\n        self.runner = runner\n        self.server_manager = server_manager\n        self.out_buf, self.err_buf = io_bufs\n        self.std_out, self.std_err = std_streams\n        self.programmatic_exit = False\n        self.shutdown_network_channel = Queue()\n        self.shutdown_main_channel = Queue()\n\n        # Used to keep track of the current position in the IO buffers for data\n        # that has been sent to STDOUT/STDERR and the logfile\n        self.out_buf_offset = 0\n        self.err_buf_offset = 0\n\n        # Used to keep track of the current position in the IO buffers for data\n        # that has been sent to the ServerManager. We separate the local/server\n        # offsets because in the case of the user\'s code frequently flushing, we\n        # want terminal/logs to update extremely quickly, but a small delay in\n        # sending data to the server is acceptable so that more data can be batched\n        # together. I.E if the user\'s code flushes 1000 times per second, we want\n        # to capture that in realtime locally, but only want to send one message to\n        # the server with the cumulative output of those 1000 flushes for the one\n        # second period.\n        self.server_out_buf_offset = 0\n        self.server_err_buf_offset = 0\n\n        self.time_since_last_server_capture = time.time()\n\n        # Create run_start message before doing any other setup work to make sure that the\n        # run_started message always precedes any other messages\n        self.server_manager.put_buf(\n            create_run_started_message(\n                self.current_sdk_run_uuid, self.job_name),\n        )\n\n        def on_stdout_flush():\n            self.capture_io()\n            self.std_out.flush()\n            self.flush_log_file()\n\n        def on_stderr_flush():\n            self.capture_io()\n            self.std_err.flush()\n            self.flush_log_file()\n\n        self.out_buf.set_on_flush(on_stdout_flush)\n        self.err_buf.set_on_flush(on_stderr_flush)\n\n        self.logger = parent_logger.getChild(__name__)\n        self.log_file, self.log_file_path = self.open_log_file()\n        if not self.log_file:\n            self.logger.error(\n                ""Could not create logs file. Logs will not be stored locally."")\n\n    def open_log_file(self):\n        log_folder = get_hyperdash_logs_home_path()\n\n        # Make sure logs directory exists (/logs)\n        if not os.path.exists(log_folder):\n            try:\n                os.makedirs(log_folder)\n            except OSError as exc:\n                if exc.errno != errno.EEXIST:\n                    pass\n                return None, None\n\n        job_log_folder = get_hyperdash_logs_home_path_for_job(self.job_name)\n        # Make sure directory for job exists in log directory (/logs/<JOB_NAME>)\n        if not os.path.exists(job_log_folder):\n            try:\n                os.makedirs(job_log_folder)\n            except OSError as exc:\n                if exc.errno != errno.EEXIST:\n                    pass\n                return None, None\n\n        # Create new log file for run\n        try:\n            iso = slugify(datetime.datetime.now().isoformat())\n            logfile_name = ""{}_{}.log"".format(slugify(self.job_name), iso)\n            logfile_path = os.path.join(job_log_folder, logfile_name)\n            return open(logfile_path, ""a""), logfile_path\n        except IOError:\n            return None, None\n\n    # Capture all IO for terminal/log file since we last checked\n    def capture_io(self, force_server_capture=False):\n        current_time = time.time()\n        should_send_to_server_manager = (\n            ((current_time - self.time_since_last_server_capture) > 1) or\n            force_server_capture\n        )\n        if should_send_to_server_manager:\n            self.time_since_last_server_capture = current_time\n\n        self.out_buf.acquire()\n        out = self.out_buf.getvalue()\n        # Local\n        len_out = len(out) - self.out_buf_offset\n        if len_out != 0:\n            self.print_out(out[self.out_buf_offset:])\n        self.out_buf_offset += len_out\n        # Server\n        len_out_server = len(out) - self.server_out_buf_offset\n        if len_out_server != 0 and should_send_to_server_manager:\n            self.send_print_to_server_manager(\n                out[self.server_out_buf_offset:], INFO_LEVEL)\n            self.server_out_buf_offset += len_out_server\n        self.out_buf.release()\n\n        self.err_buf.acquire()\n        err = self.err_buf.getvalue()\n        # Local\n        len_err = len(err) - self.err_buf_offset\n        if len_err != 0:\n            self.print_err(err[self.err_buf_offset:])\n        self.err_buf_offset += len_err\n        # Server\n        len_err_server = len(err) - self.server_err_buf_offset\n        if len_err_server != 0 and should_send_to_server_manager:\n            self.send_print_to_server_manager(\n                err[self.server_err_buf_offset:], ERROR_LEVEL)\n            self.server_err_buf_offset += len_err_server\n        self.err_buf.release()\n\n    def print_out(self, s):\n        self.std_out.write(s)\n        self.write_to_log_file(s)\n\n    def print_err(self, s):\n        self.std_err.write(s)\n        self.write_to_log_file(s)\n\n    # In the case that the amount of data is large, we chunk it into\n    # several smaller messages\n    def send_print_to_server_manager(self, s, level):\n        offset = 0\n        while offset < len(s):\n            chunk_size = min(len(s) - offset, MAX_LOG_SIZE_BYTES)\n            message = create_log_message(\n                self.current_sdk_run_uuid, level, s[offset:offset + chunk_size])\n            self.server_manager.put_buf(message)\n            offset += chunk_size\n\n    def write_to_log_file(self, s):\n        if self.log_file:\n            if PY2:\n                self.log_file.write(s.encode(""utf-8""))\n            else:\n                self.log_file.write(s)\n\n    def flush_log_file(self):\n        if self.log_file:\n            self.log_file.flush()\n\n    def cleanup(self, exit_status):\n        self.print_completion_message()\n        self.capture_io(force_server_capture=True)\n        self.server_manager.put_buf(\n            create_run_ended_message(self.current_sdk_run_uuid, exit_status),\n        )\n        self.flush_log_file()\n        self.shutdown_network_channel.put(True)\n\n    def sudden_cleanup(self):\n        self.print_completion_message()\n        # Send what we can to local log\n        self.capture_io()\n        self.flush_log_file()\n\n        # Make a best-effort attempt to notify server that the run was\n        # canceled by the user, but don\'t wait for all messages to\n        # be flushed to server so we don\'t hang the user\'s terminal.\n        self.server_manager.send_message(\n            create_run_ended_message(\n                self.current_sdk_run_uuid, ""user_canceled""),\n            raise_exceptions=False,\n            timeout_seconds=1,\n        )\n        # Prevent the network thread from continuing to run in the background\n        # even if SystemExit is caught\n        self.shutdown_network_channel.put(True)\n\n    def print_completion_message(self):\n        start_time, end_time = self.runner.get_start_and_end_time()\n        # end_time is not set in the case that the user canceled the run\n        if not end_time:\n            end_time = datetime.datetime.now()\n\n        if self.log_file_path:\n            self.logger.info(""This run of {} ran for {} and logs are available locally at: {}"".format(\n                self.job_name,\n                human_readable_duration(start_time, end_time),\n                self.log_file_path,\n            ))\n\n    def run(self):\n        """"""\n        run_http works using three separate threads:\n            1) runner thread which runs the user\'s code (if using CodeRunner)\n            2) network_thread which does blocking I/O with the server\n            3) event_loop thread which runs the SDK\'s main event loop (this is\n               just the main thread)\n\n        We require the event loop and network loop to be in separate threads\n        because otherwise slow responses from the server could inhibit the\n        SDK\'s event loop causing weird behavior like delayed logs in the user\'s\n        terminal.\n\n        Once all threads are running, the event_loop thread will periodically\n        check the I/O buffers to see if any new logs have appeared, and if so,\n        it will send them to the server manager\'s outgoing buffer.\n\n        The network_loop thread will periodically check its outgoing buffer, and\n        if it finds any messages in there, it will send them all to the server.\n\n        Cleanup is the responsibility of the event_loop. With every tick of the\n        event_loop, we check to see if the user\'s code has completed running. If\n        it has, the event_loop will capture any remaining I/O and store that in\n        the ServerManager\'s outgoing buf, as well as store a message indicating\n        that the run is complete and its final exit status. Finally, the\n        event_loop thread will push a message into the shutdown_network_channel which\n        will indicate to the network_loop that it should finish sending any\n        pending messages and then exit. The event_loop thread will then block\n        until it receives a message on the shutdown_main_channel.\n\n        At the next tick of the network_loop, the shutdown_network_channel will no longer\n        be empty, and the network loop will try and fire off any remaining messages\n        in the ServerManager\'s buffer to the server, and then put a message in the\n        shutdown_main_channel.\n\n        The main event_loop which has been blocked until now on the shutdown_main_channel\n        will now return, and the program will exit cleanly.\n        """"""\n        def network_loop():\n            while True:\n                if self.shutdown_network_channel.qsize() != 0:\n                    self.server_manager.cleanup(self.current_sdk_run_uuid)\n                    self.shutdown_main_channel.put(True)\n                    return\n                else:\n                    self.server_manager.tick(self.current_sdk_run_uuid)\n                    time.sleep(1)\n\n        network_thread = Thread(target=network_loop)\n        # Daemonize so they don\'t impede shutdown if the user\n        # keyboard interrupts\n        network_thread.daemon = True\n        network_thread.start()\n\n        # Create thread for running code if using CLI or decorator\n        if self.runner.should_run_as_thread():\n            code_thread = Thread(target=self.runner.run)\n            code_thread.daemon = True\n            code_thread.start()\n            \n        # Event loop\n        while True:\n            try:\n                self.capture_io()\n                exited_cleanly, is_done = self.runner.is_done()\n                if is_done:\n                    self.programmatic_exit = True\n                    if exited_cleanly:\n                        self.cleanup(""success"")\n                        # Block until network loop says its done\n                        self.shutdown_main_channel.get(\n                            block=True, timeout=None)\n                        return self.runner.get_return_val()\n                    else:\n                        self.cleanup(""failure"")\n                        # Block until network loop says its done\n                        self.shutdown_main_channel.get(\n                            block=True, timeout=None)\n                        raise self.runner.get_exception()\n\n                time.sleep(1)\n            # Handle Ctrl+C\n            except (KeyboardInterrupt, SystemExit):\n                self.sudden_cleanup()\n                # code_thread and network_thread are daemons so they won\'t impede this\n                sys.exit(130)\n'"
hyperdash/io_buffer.py,0,"b'from io import StringIO\nfrom threading import RLock\n\nfrom six import PY2\n\n\ndef noop():\n    pass\n\n\nclass IOBuffer:\n    def __init__(self, on_flush=noop):\n        self.buf = StringIO()\n        self.on_flush = on_flush\n        self.lock = RLock()\n\n    # Wrap the write method so the buffer can handle inputs other than strings\n    # Otherwise it would fail with calls like: print(1) or print(<SOME_OBJECT>)\n    def write(self, input):\n        # Writes happen in other threads so we explicitly guard against them inside the class\n        with self.lock:\n            if PY2:\n                self.buf.write(input.decode(""utf-8"", ""ignore""))\n                return\n            self.buf.write(input)\n\n    def getvalue(self):\n        return self.buf.getvalue()\n\n    def close(self):\n        self.buf.close()\n\n    def flush(self):\n        self.on_flush()\n\n    def set_on_flush(self, on_flush):\n        self.on_flush = on_flush\n\n    def acquire(self):\n        self.lock.acquire()\n\n    def release(self):\n        self.lock.release()\n\n    # Implement the sys.stdout interface\n    def isatty(self):\n        return True\n'"
hyperdash/jupyter.py,0,"b'from six import PY2\n\nfrom .constants import API_NAME_JUPYTER\nfrom .monitor import _monitor\n\n# Handle situation where IPython is not in the local environment OR\n# it is available, but we\'re not running in the context of an IPython\n# notebook.\ntry:\n    from IPython.core.magic import cell_magic\n    from IPython.core.magic import magics_class\n    from IPython.core.magic import Magics\n    from IPython.core.magic import needs_local_scope\n\n    # Syntax for exec changes between Python2/3 (in Python2 its a statement\n    # and in Python3 its a function) so we have to wrap it like this to\n    # prevent pre-runtime syntax errors\n    if PY2:\n        from .jupyter_2_exec import wrapped_exec\n    else:\n        from .jupyter_3_exec import wrapped_exec\n\n    @magics_class\n    class IPythonMagicsWrapper(Magics):\n        @needs_local_scope\n        @cell_magic\n        def monitor_cell(self, line, cell, local_ns=None):\n            if line is None or line == """":\n                return ""ERROR: Please provide a valid model name. Ex. %%monitor_cell dogs vs. cats""\n\n            @_monitor(line, api_key_getter=None, capture_io=True, api_name=API_NAME_JUPYTER)\n            def wrapped():\n                wrapped_exec(cell, self.shell.user_ns, local_ns)\n            wrapped()\n\nexcept ImportError:\n    class IPythonMagicsWrapper:\n        pass\n'"
hyperdash/jupyter_2_exec.py,0,"b'def wrapped_exec(cell, global_ns, local_ns):\n  exec cell in global_ns, local_ns\n'"
hyperdash/jupyter_3_exec.py,0,"b'def wrapped_exec(cell, global_ns, local_ns):\n  exec(cell, global_ns, local_ns)\n'"
hyperdash/monitor.py,0,"b'# Python 2/3 compatibility\nfrom __future__ import absolute_import, division, print_function, unicode_literals\n\nimport sys\nimport uuid\n\nfrom .client import HDClient\nfrom .code_runner import CodeRunner\nfrom .constants import API_NAME_MONITOR\nfrom .hyper_dash import HyperDash\nfrom .io_buffer import IOBuffer\nfrom .server_manager import ServerManagerHTTP\nfrom .utils import get_logger\n\n\ndef monitor(model_name, api_key_getter=None, capture_io=True):\n    return _monitor(model_name, api_key_getter, capture_io, API_NAME_MONITOR)\n\n\ndef _monitor(model_name, api_key_getter, capture_io, api_name):\n    def _monitor(f):\n        def monitored(*args, **kwargs):\n            # Create a UUID to uniquely identify this run from the SDK\'s point of view\n            current_sdk_run_uuid = str(uuid.uuid4())\n\n            # Capture STDOUT/STDERR before they\'re modified\n            old_out, old_err = sys.stdout, sys.stderr\n \n            # Buffers to which to redirect output so we can capture it\n            out = [IOBuffer(), IOBuffer()]\n\n            logger = get_logger(model_name, current_sdk_run_uuid, out[0])\n\n            if capture_io:\n                # Redirect STDOUT/STDERR to buffers\n                sys.stdout, sys.stderr = out\n\n            if not hasattr(f, \'callcount\'):\n                f.callcount = 0\n            if f.callcount >= 1:\n                raise Exception(\n                    ""Hyperdash does not support recursive functions!"")\n            else:\n                f.callcount += 1\n            try:\n                server_manager = ServerManagerHTTP(api_key_getter, logger, api_name)\n                hd_client = HDClient(logger, server_manager, current_sdk_run_uuid)\n                code_runner = CodeRunner(f, hd_client, logger, *args, **kwargs)\n                hyper_dash = HyperDash(\n                    model_name,\n                    current_sdk_run_uuid,\n                    server_manager,\n                    out,\n                    (old_out, old_err,),\n                    logger,\n                    code_runner,\n                )\n                return_val = hyper_dash.run()\n                f.callcount -= 1\n                return return_val\n            # Prevent uncaught exceptions from silently being swallowed\n            except Exception:\n                raise\n            finally:\n                # Cleanup\n                sys.stdout, sys.stderr = old_out, old_err\n        return monitored\n    return _monitor\n'"
hyperdash/sdk_message.py,0,"b'import json\nimport time\nimport uuid\n\n\nTYPE_LOG = \'log\'\nTYPE_STARTED = \'run_started\'\nTYPE_ENDED = \'run_ended\'\nTYPE_HEARTBEAT = \'heartbeat\'\nTYPE_METRIC = \'metric\'\nTYPE_PARAM = \'param\'\n\n\ndef create_metric_message(sdk_run_uuid, name, timestamp, value, is_internal):\n    return create_sdk_message(\n        sdk_run_uuid,\n        TYPE_METRIC,\n        {\n            \'name\': name,\n            # This timestamp is separate from the timestamp that is associated with each SDK\n            # message as this one represents when the metric was emitted, not when the message\n            # was constructed (can be different I.E in the case where we\'re parsing stale\n            # tensorboard files)\n            \'timestamp\': int(timestamp * 1000),\n            \'value\': value,\n            \'is_internal\': is_internal,\n        }\n    )\n\n\ndef create_param_message(sdk_run_uuid, params, is_internal):\n    return create_sdk_message(\n        sdk_run_uuid,\n        TYPE_PARAM,\n        {\n            \'params\': params,\n            \'is_internal\': is_internal,\n        }\n    )\n\n\ndef create_log_message(sdk_run_uuid, level, body):\n    return create_sdk_message(\n        sdk_run_uuid,\n        TYPE_LOG,\n        {\n            \'uuid\': str(uuid.uuid4()),\n            \'level\': level,\n            \'body\': body,\n        }\n    )\n\n\ndef create_run_started_message(sdk_run_uuid, job_name):\n    return create_sdk_message(\n        sdk_run_uuid,\n        TYPE_STARTED,\n        {\n            \'job_name\': job_name,\n        },\n    )\n\n\ndef create_run_ended_message(sdk_run_uuid, final_status):\n    return create_sdk_message(\n        sdk_run_uuid,\n        TYPE_ENDED,\n        {\'final_status\': final_status},\n    )\n\n\ndef create_heartbeat_message(sdk_run_uuid):\n    return create_sdk_message(sdk_run_uuid, TYPE_HEARTBEAT, {})\n\n\ndef create_sdk_message(sdk_run_uuid, type_str, payload):\n    """"""Create a structured message for the server.""""""\n    return json.dumps({\n        \'type\': type_str,\n        \'timestamp\': int(time.time() * 1000),\n        \'sdk_run_uuid\': sdk_run_uuid,\n        \'payload\': payload,\n    })\n'"
hyperdash/server_manager.py,0,"b'# Python 2/3 compatibility\nfrom __future__ import absolute_import, division, print_function, unicode_literals\n\nimport json\nimport os\nimport sys\nimport time\n\nfrom collections import deque\nfrom traceback import format_exc\n\nfrom requests.exceptions import BaseHTTPError\nfrom requests import Request\nfrom requests import Session as HTTPSession\n\nfrom .constants import API_KEY_NAME\nfrom .constants import AUTH_KEY_NAME\nfrom .constants import CACHE_API_KEY_FOR_SECONDS\nfrom .constants import get_hyperdash_json_paths\nfrom .constants import get_http_url\nfrom .constants import get_hyperdash_version\nfrom .constants import VERSION_KEY_NAME\nfrom .sdk_message import create_heartbeat_message\n\n\n# Python 2/3 compatibility\n__metaclass__ = type\n\n\nclass ServerManagerBase():\n    # TODO: Check type\n    def put_buf(self, m):\n        self.out_buf.append(m)\n\n    def tick(self, sdk_run_uuid):\n        raise NotImplementedError()\n\n    def send_message(self, message, raise_exceptions=True, **kwargs):\n        raise NotImplementedError()\n\n    def get_api_key(self):\n        cur_time = time.time()\n\n        # Use cached API key if available\n        if self.fetched_api_key_at and cur_time - self.fetched_api_key_at < CACHE_API_KEY_FOR_SECONDS:\n            return self.api_key\n\n        # Set this now regardless of the outcome to make sure it runs\n        self.fetched_api_key_at = cur_time\n\n        # If they provided a custom function, just use that\n        if self.custom_api_key_getter:\n            api_key = self.custom_api_key_getter()\n            is_py3 = sys.version_info[0] == 3\n            if is_py3:\n                string_types = (str,)\n            else:\n                string_types = (basestring,)\n            if not isinstance(api_key, string_types):\n                self.log_error_once(\n                    ""custom_api_key_getter returned non-string value"")\n            self.api_key = api_key\n            return api_key\n\n        # Otherwise check for hyperdash.json and HYPERDASH_API_KEY env variable\n        from_file = self.get_api_key_from_file()\n        from_env = self.get_api_key_from_env()\n\n        if not (from_file or from_env):\n            self.log_error_once(\n                ""Unable to detect API key in hyperdash.json or HYPERDASH_API_KEY environment variable"")\n\n        if from_file and from_env:\n            self.log_error_once(\n                ""Found API key in hyperdash.json AND HYPERDASH_API_KEY environment variable. Environment variable will take precedence."")\n\n        self.api_key = from_env or from_file\n        return self.api_key\n\n    def get_api_key_from_file(self):\n        parsed = None\n        for path in get_hyperdash_json_paths():\n            try:\n                with open(path, ""r"") as f:\n                    try:\n                        parsed = json.load(f)\n                    except ValueError:\n                        self.log_error_once(""hyperdash.json is not valid JSON"")\n                        return None\n            except IOError:\n                continue\n\n        return parsed.get(\'api_key\') if parsed else None\n\n    def get_api_key_from_env(self):\n        return os.environ.get(""HYPERDASH_API_KEY"")\n\n    def log_error_once(self, message):\n        if message in self.logged_errors:\n            return\n        self.logger.error(message)\n        self.logged_errors.add(message)\n\n    def should_send_heartbeat(self):\n        return (\n            len(self.out_buf) == 0 and\n            self.last_message_sent_at and\n            # TODO: Constantize/config\n            time.time() - self.last_message_sent_at >= 5\n        )\n\n    def cleanup(self, sdk_run_uuid):\n        raise NotImplementedError()\n\n    def __init__(self, custom_api_key_getter, parent_logger, api_name):\n        self.out_buf = deque()\n        self.in_buf = deque()\n        self.logger = parent_logger.getChild(__name__)\n        self.custom_api_key_getter = custom_api_key_getter\n        self.logged_errors = set()\n        self.unauthorized = False\n        self.api_key = None\n        self.fetched_api_key_at = None\n        self.last_message_sent_at = None\n        self.version = get_hyperdash_version()\n        self.api_name = api_name\n\n\nclass ServerManagerHTTP(ServerManagerBase):\n\n    def tick(self, sdk_run_uuid):\n        if self.unauthorized:\n            return False\n\n        # If there are no messages to be sent, check if we\n        # need to send a heartbeat\n        if self.should_send_heartbeat():\n            try:\n                self.send_message(create_heartbeat_message(sdk_run_uuid))\n            except BaseHTTPError as e:\n                self.log_error_once(\n                    ""Unable to send heartbeat due to connection issues: {}"".format(\n                        e),\n                )\n                return False\n            except Exception as e:\n                self.logger.debug(e)\n                self.log_error_once(""Unable to send heartbeat message"")\n                return False\n\n        # TODO: Move while loop out of tick function\n        while True:\n            try:\n                message = self.out_buf.popleft()\n            # Empty\n            except IndexError:\n                # Clean exit\n                return True\n\n            sent_successfully = False\n            is_poison_pill = False\n            try:\n                res = self.send_message(message)\n                if res.status_code != 200:\n                    # TODO: Server should return better error message\n                    err_code = res.json()[""code""]\n                    if err_code == ""api_key_requred"":\n                        self.unauthorized = True\n                    self.log_error_once(\n                        ""Error from Hyperdash server: {}"".format(err_code))\n                    # Status code 400 indicates there is something malformed\n                    # about the message. Mark it as poison so we don\'t keep\n                    # retrying.\n                    if res.status_code == 400:\n                        is_poison_pill = True\n                else:\n                    sent_successfully = True\n            except BaseHTTPError as e:\n                self.log_error_once(\n                    ""Unable to send message due to connection issues: {}"".format(\n                        e),\n                )\n            except Exception as e:\n                self.logger.debug(format_exc())\n                self.log_error_once(\n                    ""Unable to communicate with Hyperdash servers"")\n\n            if sent_successfully is not True and not is_poison_pill:\n                # Re-enque so message is not lost\n                self.out_buf.appendleft(message)\n                return False\n\n    def send_message(self, message, raise_exceptions=True, timeout_seconds=5):\n        try:\n            return self.s.post(\n                get_http_url(),\n                json=json.loads(message),\n                headers={\n                    AUTH_KEY_NAME: self.get_api_key(),\n                    VERSION_KEY_NAME: self.version,\n                    API_KEY_NAME: self.api_name,\n                },\n                timeout=timeout_seconds,\n            )\n        except Exception:\n            if raise_exceptions:\n                raise\n        finally:\n            self.last_message_sent_at = time.time()\n\n    def cleanup(self, sdk_run_uuid):\n        # Try to flush any remaining messages\n        return self.tick(sdk_run_uuid)\n\n    def __init__(self, custom_api_key_getter, parent_logger, api_name):\n        ServerManagerBase.__init__(self, custom_api_key_getter, parent_logger, api_name)\n        # TODO: Keep alive\n        # TODO: Timeout\n        self.s = HTTPSession()\n'"
hyperdash/utils.py,0,"b'import datetime\nimport logging\n\ndef get_logger(model_name, current_sdk_run_uuid, stdout_buffer):\n   # Include the model_name/UUID in the logger name to make\n    # sure that its always distinct, even if multiple runs\n    # of the same model are happening at the same time in\n    # different threads\n    logger = logging.getLogger(\n        ""{}-{}"".format(model_name, current_sdk_run_uuid))\n    # Remove any existing log handlers so it doesn\'t double log\n    logger.handlers = []\n    # Don\'t propagate to the root logger\n    logger.propagate = False\n    logger.setLevel(logging.INFO)\n    logger.addHandler(logging.StreamHandler(stdout_buffer))\n    return logger\n\n\ndef human_readable_duration(start_time, end_time):\n    return str(datetime.timedelta(seconds=(end_time-start_time).seconds))'"
hyperdash_cli/__init__.py,0,b'from .cli import github\nfrom .cli import signup\nfrom .cli import login\nfrom .cli import keys\nfrom .cli import run\nfrom .cli import pipe\nfrom .cli import version\nfrom .cli import tensorboard\n'
hyperdash_cli/__main__.py,0,b'from .cli import main\n\nmain()\n'
hyperdash_cli/cli-runner.py,0,"b""from .cli import main\n\n\nif __name__ == '__main__':\n    main()\n"""
hyperdash_cli/cli.py,0,"b'import argparse\nfrom contextlib import closing\nimport errno\nfrom getpass import getpass\nimport os\nimport subprocess\nimport time\nfrom threading import Thread\nimport json\nimport signal\nimport socket\nimport sys\nimport webbrowser\n\nimport requests\n\nfrom six.moves import input\nfrom six.moves import xrange\nfrom six.moves.queue import Queue\nfrom six.moves.urllib.parse import urlparse, parse_qs, urlencode\nfrom six.moves import BaseHTTPServer\n\nfrom six import PY2\n\nfrom hyperdash.constants import API_NAME_CLI_PIPE\nfrom hyperdash.constants import API_NAME_CLI_RUN\nfrom hyperdash.constants import get_hyperdash_json_home_path\nfrom hyperdash.constants import get_hyperdash_json_paths\nfrom hyperdash.constants import get_hyperdash_version\nfrom hyperdash.experiment import _TensorboardExperiment\nfrom hyperdash import monitor\nfrom hyperdash.monitor import _monitor\n\nfrom .constants import get_base_url\nfrom .constants import get_base_http_url\nfrom .constants import GITHUB_OAUTH_START\nfrom .constants import THREADING_TIMEOUT_MAX\nfrom .constants import LOOPBACK\n\n\ndef signup(args=None):\n    if not (args.email or args.github):\n        print(""To signup with your email address, run `hd signup --email`. Alternatively, you can signup via Github by running `hd signup --github"")\n        return\n\n    if args.email:\n        _signup_email(args)\n        return\n    \n    if args.github:\n        github(args)\n        return\n\n\ndef _signup_email(args):\n    email = get_input(""Email address: "")\n    password = get_input(""Password (8 characters or more): "", True)\n\n    print(""Trying to sign you up now..."")\n    try:\n        res = post_json(""/users"", {\n            ""email"": email,\n            ""password"": password,\n            ""client"": ""cli"",\n        })\n    except Exception as e:\n        print(""Sorry we were unable to sign you up, please try again."")\n        return\n\n    res_body = res.json()\n    if res.status_code != 200:\n        message = res_body.get(""message"")\n        if message:\n            print(message)\n            return\n\n    print(""Congratulations on signing up!"")\n    api_key = res_body[""api_key""]\n    print(""Your API key is: {}"".format(api_key))\n\n    write_hyperdash_json_file({\n        ""api_key"": api_key\n    })\n    print(""""""\n        We stored your API key in {}\n        and we\'ll use that as the default for future jobs.\n\n        If you want to see Hyperdash in action, run `hyperdash demo`\n        and then install our mobile app to monitor your job in realtime.\n    """""".format(get_hyperdash_json_home_path())\n          )\n\n    _login(email, password)\n\n\ndef demo(args=None):\n    from_file = get_api_key_from_file()\n    from_env = get_api_key_from_env()\n    api_key = from_env or from_file\n\n    if not api_key:\n        print(""""""\n            `hyperdash demo` requires a Hyperdash API key. Try setting your API key in the\n            HYPERDASH_API_KEY environment variable, or in a hyperdash.json file in the local\n            directory or your user\'s home directory with the following format:\n\n            {\n                ""api_key"": ""<YOUR_API_KEY>""\n            }\n        """""")\n        return\n\n    print(""""""\nRunning the following program:\n\n    from hyperdash import Experiment\n    exp = Experiment(""Dogs vs. Cats"")\n\n    # Parameters\n    estimators = exp.param(""Estimators"", 500)\n    epochs = exp.param(""Epochs"", 5)\n    batch = exp.param(""Batch Size"", 64)\n\n    for epoch in xrange(1, epochs + 1):\n        accuracy = 1. - 1./epoch\n        loss = float(epochs - epoch)/epochs\n        print(""Training model (epoch {})"".format(epoch))\n        time.sleep(1)\n\n        # Metrics\n        exp.metric(""Accuracy"", accuracy)\n        exp.metric(""Loss"", loss)\n\n    exp.end()\n    """""")\n    from hyperdash import Experiment\n    exp = Experiment(""Dogs vs. Cats"")\n\n    # Parameters\n    estimators = exp.param(""Estimators"", 500)\n    epochs = exp.param(""Epochs"", 5)\n    batch = exp.param(""Batch Size"", 64)\n\n    for epoch in xrange(epochs):\n        print(""Training model (epoch {})"".format(epoch))\n\n        accuracy = 1. - 1./(epoch + 1)\n        loss = float(epochs - epoch)/(epochs + 1)\n\n        # Metrics\n        exp.metric(""Accuracy"", accuracy)\n        exp.metric(""Loss"", loss)\n\n        time.sleep(1)\n\n    exp.end()\n\n\ndef github(args=None):\n    port = _find_available_port()\n    if not port:\n        print(""Github sign in requires an open port, please open port 3000."")\n\n    # Signal when the HTTP server has started\n    server_started_queue = Queue()\n    # Signal when we have the access token\n    access_token_queue = Queue()\n\n    # Server that we will run in the background to accept a post-OAuth redirect from\n    # the Hyperdash server which will contain the user\'s access token\n    def start_server():\n        class OAuthRedirectHandler(BaseHTTPServer.BaseHTTPRequestHandler):\n            def do_GET(self):\n                parsed_path = urlparse(self.path)\n                query = parse_qs(parsed_path.query)\n                access_token = query[""access_token""][0] if ""access_token"" in query else None\n                if not access_token:\n                    print(""Something went wrong! Please try again."")\n                    sys.exit()\n                print(""Access token auto-detected!"")\n                access_token_queue.put(access_token)\n                # Redirect user\'s browser\n                self.send_response(301)\n                self.send_header(""Location"",""{}/{}"".format(get_base_http_url(), ""/oauth/github/success""))\n                self.end_headers()\n            # Silence logs\n            def log_message(self, _format, *args):\n                return\n\n        server = BaseHTTPServer.HTTPServer((LOOPBACK, port), OAuthRedirectHandler)\n        server_started_queue.put(True)\n        server.handle_request()\n    \n    server_thread = Thread(target=start_server)\n    # Prevent server_thread from preventing program shutdown\n    server_thread.setDaemon(True)\n    server_thread.start()\n\n    url = ""{}/{}"".format(get_base_http_url(), GITHUB_OAUTH_START)\n    auto_login_query_args = {\n        ""state"": ""client_cli_auto:{}"".format(port),\n    }\n    auto_login_url = ""{}?{}"".format(url, urlencode(auto_login_query_args))\n    \n    # Copy\n    manual_login_query_args = dict(auto_login_query_args)\n    manual_login_query_args[""state""] = ""client_cli_manual""\n    manual_login_url = ""{}?{}"".format(url, urlencode(manual_login_query_args))\n\n    print(""Opening browser, please wait. If something goes wrong, press CTRL+C to cancel."")\n    print(""\\033[1m SSH\'d into a remote machine, or just don\'t have access to a browser? Open this link in any browser and then copy/paste the provided access token: \\033[4m{}\\033[0m \\033[0m"".format(manual_login_url))\n\n    # If the user doesn\'t have programatic access to a browser, then we need to give them\n    # the option of opening a URL manually and copy-pasting the access token into the CLI.\n    # We spin this up in a separate thread so that it doesn\'t block the happy path where\n    # the browser is available and we\'re able to auto-detect the access token\n    manual_entry_thread_started_queue = Queue()\n    def manual_entry():\n        print(""Waiting for Github OAuth to complete."")\n        print(""If something goes wrong, press CTRL+C to cancel."")        \n        manual_entry_thread_started_queue.put(True)\n        access_token = get_input(""Access token: "")\n        access_token_queue.put(access_token)\n            \n    manual_entry_thread = Thread(target=manual_entry)\n    # Prevent manual_entry_thread from preventing program shutdown\n    manual_entry_thread.setDaemon(True)\n    manual_entry_thread.start()\n\n    # Wait until the server and manual entry threads have started before opening the\n    # user\'s browser to prevent a race condition where the Hyperdash server\n    # redirects with an access token but the Python server isn\'t ready yet.\n    # \n    # Also, we set the timeout to THREADING_TIMEOUT_MAX because without a timeout,\n    # the .get() call on the queue can not be interrupted with CTRL+C.\n    server_started_queue.get(block=True, timeout=THREADING_TIMEOUT_MAX)\n    manual_entry_thread_started_queue.get(block=True, timeout=THREADING_TIMEOUT_MAX)\n    # Blocks until browser opens, but doesn\'t wait for user to close it\n    webbrowser.open_new_tab(auto_login_url)\n\n\n    # Wait for the Hyperdash server to redirect with the access token to our embedded\n    # server, or for the user to manually enter an access token. Whichever happens\n    # first.\n    access_token = access_token_queue.get(block=True, timeout=THREADING_TIMEOUT_MAX)\n    # Use the access token to retrieve the user\'s API key and store a valid\n    # hyperdash.json file\n    success, default_api_key = _after_access_token_login(access_token)\n    if success:\n        print(""Successfully logged in! We also installed: {} as your default API key"".format(\n            default_api_key))\n\n\ndef _find_available_port():\n    for cur_port in xrange(3000, 9000):\n        is_open = _is_port_open(LOOPBACK, cur_port)\n        if is_open:\n            return cur_port\n    return None\n\n\ndef _is_port_open(host, port):\n    try:\n        sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n        sock.settimeout(2)\n        sock.bind((host, port))\n        sock.listen(5)\n        sock.close()\n    except (socket.error, socket.timeout):\n        return False\n    return True\n\n\ndef login(args=None):\n    if not (args.email or args.github):\n        print(""To login with your email address, run `hd login --email`. Alternatively, you can login via Github by running `hd login --github"")\n        return\n\n    if args.email:\n        email = get_input(""Email address: "")\n        password = get_input(""Password: "", True)\n        success, default_api_key = _login(email, password)\n        if success:\n            print(""Successfully logged in! We also installed: {} as your default API key"".format(\n                default_api_key))\n        return\n    \n    if args.github:\n        github(args)\n        return\n\n\ndef _login(email, password):\n    try:\n        response = post_json(""/sessions"", {\n            ""email"": email,\n            ""password"": password,\n        })\n    except Exception as e:\n        print(""Sorry we were unable to log you in, please try again."")\n        return False, None\n\n    response_body = response.json()\n    if response.status_code != 200:\n        message = response_body.get(""message"")\n        if message:\n            print(message)\n            return False, None\n\n    access_token = response_body[""access_token""]\n    return _after_access_token_login(access_token)\n\n\ndef _after_access_token_login(access_token):\n    config = {""access_token"": access_token}\n\n    # Add API key if available\n    api_keys = get_api_keys(access_token)\n    if api_keys and len(api_keys) > 0:\n        default_api_key = api_keys[0]\n        config[""api_key""] = default_api_key\n    else:\n        print(""Login failure: We were unable to retrieve your default API key."")\n        return False, None\n\n    write_hyperdash_json_file(config)\n    return True, default_api_key\n\n\ndef get_api_keys(access_token):\n    try:\n        res = get_json(""/users/api_keys"", headers={\n            ""authorization"": access_token,\n        })\n        res_json = res.json()\n        if res.status_code != 200:\n            message = res_json.get(""message"")\n            if message:\n                print(message)\n            return None\n        return res_json.get(""api_keys"")\n    except Exception as e:\n        print(""Sorry we were unable to retrieve your API keys, please try again."")\n        return None\n\n\ndef keys(args=None):\n    from_file = get_access_token_from_file()\n    from_env = get_access_token_from_env()\n    access_token = from_file or from_env\n\n    if not access_token:\n        print(""Not authorized.\\n\\n""\n              ""`hyperdash keys` is an authorized request available only to logged in users.\\n""\n              ""Login with `hyperdash login` to authenticate as a user.\\n\\n"")\n        return\n\n    api_keys = get_api_keys(access_token)\n    if api_keys is None:\n        return\n\n    print(""\\nBelow are the API Keys associated with you account:\\n\\n"")\n\n    for i, api_key in enumerate(api_keys):\n        print(""    {}) {}"".format(i + 1, api_key))\n\n    print(""\\n"")\n\n\ndef tensorboard(args=None, is_test=False):\n    try:\n        from tensorboard.backend.event_processing import event_multiplexer\n    except ImportError:\n        print(""We were unable to import the necessary tensorboard libraries. Please make sure tensorboard is installed in your Python environment and then try again."")\n        return\n\n    # EventMultiplexer is basically a ""manager"" of different accumulators. Accumulators are responsible\n    # for ""accumulating"" all the events for a specific run, so we can use the multiplexer to enumerate all\n    # the available accumulators and find the accumulator for the run that we\'re looking for.\n    multiplexer = event_multiplexer.EventMultiplexer()\n    multiplexer.AddRunsFromDirectory(args.logdir)\n    multiplexer.Reload()\n\n    run_paths = multiplexer.RunPaths()\n\n    # Figure out which run was created most recently\n    # TODO: Allow the user to specify a name\n    latest_run = None\n    latest_run_first_timestamp = None\n    for run in run_paths:\n        first_timestamp = multiplexer.FirstEventTimestamp(run)\n        if latest_run is None:\n            latest_run = run\n            latest_run_first_timestamp = first_timestamp\n        if first_timestamp > latest_run_first_timestamp:\n            latest_run = run\n            latest_run_first_timestamp = first_timestamp\n\n    if latest_run is None:\n        print(""No tensorflow runs detected in {}"".format(args.logdir))\n        return\n            \n    accumulator = multiplexer.GetAccumulator(latest_run)\n    # Synchronously load all events since last call to Relaod\n    accumulator.Reload()\n\n    tags = accumulator.Tags()\n    if \'scalars\' not in tags:\n        print(""Auto-detected most recent run is `{}`, but no metrics were detected"".format(latest_run))\n        return\n  \n    scalars = tags[\'scalars\']\n    scalars_str = \', \'.join(scalars)\n    print(""Auto-detected most recent run is `{}` with the following metrics: {}"".format(latest_run, scalars_str))\n\n    exp = _TensorboardExperiment(args.name, capture_io=False)\n    latest_wall_time_by_scalar = {}\n\n    # If the user doesn\'t want to backfill data, then we set the latest_wall_time for each\n    # scalar to the current time which will prevent us from emitting any metrics that existed\n    # before we started monitoring the folder\n    if not args.backfill:\n        current_time = time.time()\n        for scalar in scalars:\n            latest_wall_time_by_scalar[scalar] = current_time\n\n    # Cleanup on signal so that runs are marked as completed not disconnected\n    def signal_handler(_, __):\n            exp.end()\n            sys.exit(0)\n    signal.signal(signal.SIGINT, signal_handler)\n\n    # TODO: Right now we can\'t detect when the run is done because there is no ""completion""\n    # event in the tensorflow logs so we just run the program inifnitely until the user cancels it.\n    # In theory, we could try and guess when the user\'s program is done by measuring the amount of time\n    # between datapoints and then waiting until we don\'t see any new metrics for some multiple of that\n    # period, but for now we don\'t do that.\n    while True:\n        # Reload will retrieve any new datapoints since the last time we called it\n        accumulator.Reload()\n        tags = accumulator.Tags()\n        # Don\'t rely on the old scalar tags because new ones may have appeared\n        scalars = tags[\'scalars\']\n        for scalar in scalars:\n            for metric in accumulator.Scalars(scalar):\n                # Skip metrics we\'ve already processed before\n                if latest_wall_time_by_scalar.get(scalar) and metric.wall_time <= latest_wall_time_by_scalar[scalar]:\n                    continue\n                # This is gross, but we need to be able to control the actual timestamp that is being set\n                # in case we\'re parsing stale Tensorboard files, otherwise the metric timestamps will be completely\n                # off. Also, note that this will do the same 1s sampling we normally do which is important because\n                # tensorflow emits a LOT of datapoints\n                exp._hd_client._metric(scalar, metric.wall_time, metric.value, log=False)\n                latest_wall_time_by_scalar[scalar] = metric.wall_time\n        # Prevent infinite loop for testing purposes\n        if is_test:\n            break\n        time.sleep(1)\n    # Should never happen for now\n    exp.end()\n\n\ndef run(args):\n    @_monitor(args.name, api_key_getter=None, capture_io=True, api_name=API_NAME_CLI_RUN)\n    def wrapped(exp):\n        # Python detects when its connected to a pipe and buffers output.\n        # Spawn the users program with the PYTHONUNBUFFERED environment\n        # variable set in case they are running a Python program.\n        subprocess_env = os.environ.copy()\n        subprocess_env[""PYTHONUNBUFFERED""] = ""1""\n        # Spawn a subprocess with the user\'s command\n        p = subprocess.Popen(\n            "" "".join(args.args),\n            shell=True,\n            stdout=subprocess.PIPE,\n            stderr=subprocess.PIPE,\n            bufsize=0,\n            env=subprocess_env,\n        )\n\n        # The subprocess\'s output will be written to the associated\n        # pipes. In order for the @monitor decorator to have access\n        # to them, we need to read them out and write them to\n        # stdout/stderr respectively (which have been redirected by\n        # the monitor decorator)\n        def stdout_loop():\n            _connect_streams(p.stdout, sys.stdout)\n\n        def stderr_loop():\n            _connect_streams(p.stderr, sys.stderr)\n\n        stdout_thread = Thread(target=stdout_loop)\n        stderr_thread = Thread(target=stderr_loop)\n        stdout_thread.start()\n        stderr_thread.start()\n        # Wait for the subprocess to finish executing\n        p.wait()\n        # Threads will exit as soon as their associated pipes are closed by the operating system\n        stdout_thread.join()\n        stderr_thread.join()\n    wrapped()\n\n\ndef pipe(args):\n    @_monitor(args.name, api_key_getter=None, capture_io=True, api_name=API_NAME_CLI_PIPE)\n    def wrapped():\n        # Read STDIN and write it to STDOUT so it shows up in the terminal and is\n        # captured by the monitor decorator\n        if PY2:\n            in_stream = sys.stdin\n        else:\n            in_stream = sys.stdin.buffer\n        _connect_streams(in_stream, sys.stdout)\n    wrapped()\n\n\n# Reads a pipe one character at a time, yielding\n# buffers everytime it encounters whitespace. This\n# allows us read the pipe as fast as possible.\n#\n# We yield everytime we encounter whitespace instead\n# of on every byte because yielding every byte individually\n# would break the UTF-8 decoding of multi-byte characters.\n#\n# Before this we were using readline() which works in most\n# cases, but breaks for scripts that use loading bars\n# like tqdm which do not output a \\n everytime they\n# update. Using read() doesn\'t work either because there\n# is no guarantee of when that will be flushed so\n# terminal updates can be delayed.\ndef _gen_tokens_from_stream(stream):\n    buf = []\n    while True:\n        # read one byte        \n        b = stream.read(1)\n        # We\'re done\n        if not b:\n            if buf:\n                # Yield what we have\n                yield b"""".join(buf)\n            return\n        # If its whitespace, yield what we have including the whitespace\n        if b.isspace() and buf:\n            yield b"""".join(buf) + b\n            buf = []\n        # Otherwise grow the buf\n        else:\n            buf.append(b)\n\n\ndef _connect_streams(in_stream, out_stream):\n    """"""Connects two streams and blocks until the input stream is closed.""""""\n    for data in _gen_tokens_from_stream(in_stream):\n        # In PY2 data is str, in PY3 its bytes\n        if PY2:\n            token = data\n        else:\n            token = data.decode(""utf-8"", ""ignore"")\n        out_stream.write(token)\n\n\ndef version(args=None):\n    print(""hyperdash {}"".format(get_hyperdash_version()))\n\n\ndef get_input(prompt, sensitive=False):\n    if sensitive:\n        return getpass(prompt)\n    return input(prompt)\n\n\ndef get_json(path, **kwargs):\n    return requests.get(""{}{}"".format(get_base_url(), path), **kwargs)\n\n\ndef post_json(path, data):\n    return requests.post(\n        ""{}{}"".format(get_base_url(), path),\n        json=data,\n    )\n\n\ndef write_hyperdash_json_file(hyperdash_json):\n    path = get_hyperdash_json_home_path()\n\n    if not os.path.exists(os.path.dirname(path)):\n        try:\n            os.makedirs(os.path.dirname(path))\n        except OSError as exc:\n            if exc.errno != errno.EEXIST:\n                raise\n    try:\n        # Open for read/write, but will not create file\n        with open(path, ""r+"") as f:\n            write_hyperdash_json_helper(f, hyperdash_json)\n    except IOError:\n        # Open for read/write but will truncate if it already exists\n        with open(path, ""w+"") as f:\n            write_hyperdash_json_helper(f, hyperdash_json)\n\n\ndef write_hyperdash_json_helper(file, hyperdash_json):\n    data = file.read()\n\n    existing = {}\n    if len(data) > 0:\n        try:\n            existing = json.loads(data)\n        except ValueError:\n            raise Exception(""{} is not valid JSON!"".format(\n                get_hyperdash_json_home_path()))\n\n    existing.update(hyperdash_json)\n\n    # Seek back to beginning before we write\n    file.seek(0)\n    file.write(json.dumps(existing))\n    file.write(""\\n"")\n    file.truncate()\n\n\ndef get_access_token_from_file():\n    parsed = None\n    for path in get_hyperdash_json_paths():\n        try:\n            with open(path, ""r"") as f:\n                try:\n                    parsed = json.load(f)\n                except ValueError:\n                    print(""hyperdash.json is not valid JSON"")\n                    return None\n        except IOError:\n            continue\n\n    return parsed.get(""access_token"") if parsed else None\n\n\ndef get_access_token_from_env():\n    return os.environ.get(""HYPERDASH_ACCESS_TOKEN"")\n\n\ndef get_api_key_from_file():\n    parsed = None\n    for path in get_hyperdash_json_paths():\n        try:\n            with open(path, ""r"") as f:\n                try:\n                    parsed = json.load(f)\n                except ValueError:\n                    print(""hyperdash.json is not valid JSON"")\n                    return None\n        except IOError:\n            continue\n\n    return parsed.get(""api_key"") if parsed else None\n\n\ndef get_api_key_from_env():\n    return os.environ.get(""HYPERDASH_API_KEY"")\n\n\ndef main():\n    parser = argparse.ArgumentParser(description=""The HyperDash SDK"")\n    subparsers = parser.add_subparsers(\n        title=""subcommands"",\n        description=""valid subcommands"",\n        help=""additional help"",\n        dest=""subcommand""\n    )\n    subparsers.required = True\n\n    signup_parser = subparsers.add_parser(""signup"")\n    signup_parser.add_argument(""--email"", ""-email"", required=False, action=\'store_true\')\n    signup_parser.add_argument(""--github"", ""-github"", required=False, action=\'store_true\')\n    signup_parser.set_defaults(func=signup)\n\n    demo_parser = subparsers.add_parser(""demo"")\n    demo_parser.set_defaults(func=demo)\n\n    login_parser = subparsers.add_parser(""login"")\n    login_parser.add_argument(""--email"", ""-email"", required=False, action=\'store_true\')\n    login_parser.add_argument(""--github"", ""-github"", required=False, action=\'store_true\')\n    login_parser.set_defaults(func=login)\n\n    github_parser = subparsers.add_parser(""github"")\n    github_parser.set_defaults(func=github)\n\n    keys_parser = subparsers.add_parser(""keys"")\n    keys_parser.set_defaults(func=keys)\n\n    run_parser = subparsers.add_parser(""run"")\n    run_parser.add_argument(""--name"", ""-name"", ""--n"", ""-n"", required=True)\n    run_parser.add_argument(""args"", nargs=argparse.REMAINDER)\n    run_parser.set_defaults(func=run)\n\n    pipe_parser = subparsers.add_parser(""pipe"")\n    pipe_parser.add_argument(""--name"", ""-name"", ""--n"", ""-n"", required=True)\n    pipe_parser.set_defaults(func=pipe)\n\n    keys_parser = subparsers.add_parser(""version"")\n    keys_parser.set_defaults(func=version)\n\n    tensorboard_parser = subparsers.add_parser(""tensorboard"")\n    tensorboard_parser.add_argument(""--name"", ""-name"", ""--n"", ""-n"", required=True)\n    tensorboard_parser.add_argument(""--logdir"", ""-logdir"", required=True)\n    tensorboard_parser.add_argument(""--backfill"", ""-backfill"", required=False, action=\'store_true\')\n    tensorboard_parser.set_defaults(func=tensorboard)\n\n    args = parser.parse_args()\n    args.func(args)\n'"
hyperdash_cli/constants.py,0,"b'import os\n\nimport six\n\n\nGITHUB_OAUTH_START = ""oauth/github/start""\n\nTHREADING_TIMEOUT_MAX = 4294967  # Maximum allowed threading.TIMEOUT_MAX in Python 3.6\n\nLOOPBACK = ""127.0.0.1""\n\n\ndef get_base_http_url():\n    return six.text_type(os.environ.get(\n        ""HYPERDASH_SERVER"",\n        ""https://hyperdash.io"",\n    ))\n\n\ndef get_base_url():\n    return six.text_type(\n        ""{}/api/v1"".format(os.environ.get(\n            ""HYPERDASH_SERVER"",\n            ""https://hyperdash.io"",\n        ))\n    )\n'"
tests/mocks.py,0,"b'from six.moves.BaseHTTPServer import BaseHTTPRequestHandler, HTTPServer\nfrom threading import Thread\nimport os\nimport requests\nimport socket\nimport time\n\nhandle_request_cache = dict()\n\n\nclass MockServerRequestHandler(BaseHTTPRequestHandler):\n\n    def handle_request(self, method, path):\n        handle = handle_request_cache.get((method, path))\n        if not handle:\n            raise Exception(\n                ""Mock server called with unknown request {} {}"".format(method, path))\n\n        handle(self)\n\n    def do_GET(self): self.handle_request(""GET"", self.path)\n\n    def do_POST(self): self.handle_request(""POST"", self.path)\n\n    # Turn off HTTP logging so they don\'t interfere with STDOUT for our tests\n    def log_message(*args, **kwargs):\n        pass\n\n\ndef init_mock_server():\n    # Get port\n    s = socket.socket(socket.AF_INET, type=socket.SOCK_STREAM)\n    s.bind((\'localhost\', 0))\n    address, port = s.getsockname()\n    s.close()\n\n    # Start server\n    mock_server = HTTPServer((\'localhost\', port), MockServerRequestHandler)\n    mock_server_thread = Thread(target=mock_server.serve_forever)\n    mock_server_thread.setDaemon(True)\n    mock_server_thread.start()\n\n    # Override production server\n    server = \'http://localhost:{}\'.format(port)\n    os.environ[\'HYPERDASH_SERVER\'] = server\n\n    return handle_request_cache\n'"
tests/test_buffer.py,0,"b'from hyperdash.io_buffer import IOBuffer\n\n\nclass TestBuffer(object):\n    """"""TestBuffer contains tests for the IOBuffer class.""""""\n    def test_buffer_has_atty_method(self):\n        """"""Verify IOBuffer has an atty() method.""""""\n        buf = IOBuffer()\n        assert buf.isatty() is True'"
tests/test_cli.py,0,"b'# -*- coding: utf-8 -*-\nimport argparse\nimport json\nimport os\n\nimport requests\nfrom threading import Thread\n\nfrom six import StringIO\nfrom six import PY2\nfrom mock import patch, Mock\nfrom nose.tools import assert_in\n\nimport hyperdash_cli\nfrom mocks import init_mock_server\nfrom hyperdash.constants import API_KEY_NAME\nfrom hyperdash.constants import API_NAME_CLI_PIPE\nfrom hyperdash.constants import API_NAME_CLI_RUN\nfrom hyperdash.constants import API_NAME_CLI_TENSORBOARD\nfrom hyperdash.constants import get_hyperdash_json_home_path\nfrom hyperdash.constants import get_hyperdash_logs_home_path_for_job\nfrom hyperdash.constants import get_hyperdash_version\nfrom hyperdash.constants import VERSION_KEY_NAME\n\n\nDEFAULT_API_KEY = ""y9bhlYMBivCu8cBj6SQPAbwjxSqnbR1w23TtR9n9yOM=""\nDEFAULT_ACCESS_TOKEN = ""72a84fc0-b272-480a-807d-fd4a40ee2a66""\n\nserver_sdk_headers = []\nserver_sdk_messages = []\n\n\nclass TestCLI(object):\n    def setup(self):\n        global server_sdk_messages\n        global server_sdk_headers\n        server_sdk_messages = []\n        server_sdk_headers = []\n\n    @classmethod\n    def setup_class(_cls):\n        request_handle_dict = init_mock_server()\n\n        def setup(self):\n            try:\n                # Delete hyperdash.json file between tests\n                os.remove(get_hyperdash_json_home_path())\n            except FileNotFoundError:\n                pass\n\n        def user_signup(response):\n            # Add response status code.\n            response.send_response(requests.codes.ok)\n\n            # Add response headers.\n            response.send_header(\n                \'Content-Type\', \'application/json; charset=utf-8\')\n            response.end_headers()\n\n            # Add response content.\n            response_content = json.dumps({\n                ""user_uuid"": ""72a84fc0-b272-480a-807d-fd4a40ee2a66"",\n                ""api_key"": DEFAULT_API_KEY,\n            })\n            response.wfile.write(response_content.encode(\'utf-8\'))\n\n        def user_login(response):\n            # Add response status code.\n            response.send_response(requests.codes.ok)\n\n            # Add response headers.\n            response.send_header(\n                \'Content-Type\', \'application/json; charset=utf-8\')\n            response.end_headers()\n\n            # Add response content.\n            response_content = json.dumps({\n                ""access_token"": ""72a84fc0-b272-480a-807d-fd4a40ee2a66""\n            })\n            response.wfile.write(response_content.encode(\'utf-8\'))\n\n        def user_api_keys(response):\n            # Add response status code.\n            response.send_response(requests.codes.ok)\n\n            # Add response headers.\n            response.send_header(\n                \'Content-Type\', \'application/json; charset=utf-8\')\n            response.end_headers()\n\n            # Add response content.\n            response_content = json.dumps({\n                ""api_keys"": [DEFAULT_API_KEY]\n            })\n            response.wfile.write(response_content.encode(\'utf-8\'))\n\n        def sdk_message(response):\n            global server_sdk_messages\n            global server_sdk_headers\n            message = json.loads(response.rfile.read(\n                int(response.headers[""Content-Length""])).decode(""utf-8""))\n\n            # Store messages / headers so we can assert on them later\n            server_sdk_messages.append(message)\n            if PY2:\n                server_sdk_headers.append(response.headers.dict)\n            else:\n                server_sdk_headers.append(response.headers)\n\n\n            # Add response status code.\n            response.send_response(requests.codes.ok)\n\n            # Add response headers.\n            response.send_header(\n                \'Content-Type\', \'application/json; charset=utf-8\')\n            response.end_headers()\n\n            # Add response content. In this case, we use the exact same response for\n            # all messages from the SDK because the current implementation ignores the\n            # body of the response unless there is an error.\n            response_content = json.dumps({})\n\n            response.wfile.write(response_content.encode(\'utf-8\'))\n\n        request_handle_dict[(""POST"", ""/api/v1/users"")] = user_signup\n        request_handle_dict[(""POST"", ""/api/v1/sessions"")] = user_login\n        request_handle_dict[(""GET"", ""/api/v1/users/api_keys"")] = user_api_keys\n        request_handle_dict[(""POST"", ""/api/v1/sdk/http"")] = sdk_message\n\n    def test_signup(self):\n        vals = {\n            (""Email address: "", ): ""user@email.com"",\n            (""Company (optional): "", ): ""Company"",\n            (""Password (8 characters or more): "", True): ""Password"",\n        }\n\n        def side_effect(*args):\n            return vals[args]\n\n        with patch(\'hyperdash_cli.cli.get_input\', Mock(side_effect=side_effect)), patch(\'sys.stdout\', new=StringIO()) as fake_out:\n            hyperdash_cli.signup(argparse.Namespace(email=True))\n\n        expected_output = [\n            ""Trying to sign you up now..."",\n            ""Congratulations on signing up!"",\n            ""Your API key is: {}"".format(DEFAULT_API_KEY),\n            ""We stored your API key in"",\n            ""If you want to see Hyperdash in action, run `hyperdash demo`"",\n            ""and then install our mobile app to monitor your job in realtime."",\n        ]\n        for expected in expected_output:\n            assert_in(expected, fake_out.getvalue())\n\n    def test_login(self):\n        vals = {\n            (""Email address: "", ): ""user@email.com"",\n            (""Password: "", True): ""Password"",\n        }\n\n        def side_effect(*args):\n            return vals[args]\n\n        with patch(\'hyperdash_cli.cli.get_input\', Mock(side_effect=side_effect)), patch(\'sys.stdout\', new=StringIO()) as fake_out:\n            hyperdash_cli.login(argparse.Namespace(email=True))\n\n        expected_output = [\n            ""Successfully logged in!"",\n            ""We also installed: {} as your default API key"".format(\n                DEFAULT_API_KEY),\n        ]\n        for expected in expected_output:\n            assert_in(expected, fake_out.getvalue())\n\n    # This test is fairly primitive, but its enough to verify python2/3 compatibility\n    def test_github(self):\n        vals = {\n            (""Access token: "", ): DEFAULT_ACCESS_TOKEN,\n        }\n\n        def side_effect(*args):\n            return vals[args]\n\n        with patch(\'hyperdash_cli.cli.get_input\', Mock(side_effect=side_effect)), patch(\'sys.stdout\', new=StringIO()) as fake_out:\n            hyperdash_cli.github()\n\n        expected_output = [\n            ""Successfully logged in!"",\n            ""We also installed: {} as your default API key"".format(\n                DEFAULT_API_KEY),\n        ]\n        for expected in expected_output:\n            assert_in(expected, fake_out.getvalue())\n\n    def test_keys(self):\n        with patch(\'hyperdash_cli.cli.get_access_token_from_file\', Mock(return_value=DEFAULT_ACCESS_TOKEN)), patch(\'sys.stdout\', new=StringIO()) as fake_out:\n            hyperdash_cli.keys()\n\n        expected_output = [\n            ""Below are the API Keys associated with you account:"",\n            ""1) {}"".format(DEFAULT_API_KEY),\n        ]\n        for expected in expected_output:\n            assert_in(expected, fake_out.getvalue())\n\n    def test_run(self):\n        job_name = ""some_job_name""\n        with patch(\'hyperdash_cli.cli.get_access_token_from_file\', Mock(return_value=DEFAULT_ACCESS_TOKEN)), patch(\'sys.stdout\', new=StringIO()) as fake_out:\n            hyperdash_cli.run(\n                argparse.Namespace(\n                    name=job_name,\n                    args=[\n                        ""echo"", ""hello world"", ""&&"",\n                        ""echo"", ""foo bar baz"", ""&&"",\n                        ""python"", ""tests/test_script_for_run_test.py"",\n                    ]\n                )\n            )\n\n        expected_output = [\n            ""hello world"",\n            ""foo bar baz"",\n            ""this is the test script"",\n            ""\xe5\xad\x97"",\n            ""{\'some_obj_key\': \'some_value\'}"",\n        ]\n        for expected in expected_output:\n            if PY2:\n                assert_in(expected, fake_out.getvalue().encode(""utf-8""))\n                continue\n            assert_in(expected, fake_out.getvalue())\n\n        # Make sure correct API name / version headers are sent\n        assert server_sdk_headers[0][API_KEY_NAME] == API_NAME_CLI_RUN\n        assert server_sdk_headers[0][VERSION_KEY_NAME] == get_hyperdash_version()\n\n        # Make sure logs were persisted\n        log_dir = get_hyperdash_logs_home_path_for_job(job_name)\n        latest_log_file = max([\n            os.path.join(log_dir, filename) for\n            filename in\n            os.listdir(log_dir)\n        ], key=os.path.getmtime)\n        with open(latest_log_file, \'r\') as log_file:\n            data = log_file.read()\n            for expected in expected_output:\n                assert_in(expected, data)\n        os.remove(latest_log_file)\n\n\n    def test_tensorboard(self):\n        job_name = ""some_job_name""\n        with patch(\'hyperdash_cli.cli.get_access_token_from_file\', Mock(return_value=DEFAULT_ACCESS_TOKEN)), patch(\'sys.stdout\', new=StringIO()) as fake_out:\n            hyperdash_cli.tensorboard(\n                argparse.Namespace(\n                    name=job_name,\n                    logdir=""tests/test_tensorboard_logs"",\n                    backfill=True,\n                ),\n                is_test=True,\n            )\n\n        # This is not all the datapoints in the tensorboard logs, but only these ones are included\n        # due to the default 1s sampling\n        expected_metrics = [\n            {u\'timestamp\': 1512945127487, u\'sdk_run_uuid\': u\'77972e75-6266-4d2a-94b7-25117b7dcd08\', u\'payload\': {u\'job_name\': u\'some_job_name\'}, u\'type\': u\'run_started\'},\n            {u\'timestamp\': 1512945127489, u\'sdk_run_uuid\': u\'77972e75-6266-4d2a-94b7-25117b7dcd08\', u\'payload\': {u\'timestamp\': 1512944548971, u\'is_internal\': False, u\'name\': u\'loss\', u\'value\': 2.3025853633880615}, u\'type\': u\'metric\'},\n            {u\'timestamp\': 1512945127492, u\'sdk_run_uuid\': u\'77972e75-6266-4d2a-94b7-25117b7dcd08\', u\'payload\': {u\'timestamp\': 1512944549971, u\'is_internal\': False, u\'name\': u\'loss\', u\'value\': 0.7624818682670593}, u\'type\': u\'metric\'},\n            {u\'timestamp\': 1512945127495, u\'sdk_run_uuid\': u\'77972e75-6266-4d2a-94b7-25117b7dcd08\', u\'payload\': {u\'timestamp\': 1512944550972, u\'is_internal\': False, u\'name\': u\'loss\', u\'value\': 0.5105720162391663}, u\'type\': u\'metric\'},\n            {u\'timestamp\': 1512945127497, u\'sdk_run_uuid\': u\'77972e75-6266-4d2a-94b7-25117b7dcd08\', u\'payload\': {u\'timestamp\': 1512944551976, u\'is_internal\': False, u\'name\': u\'loss\', u\'value\': 0.5286356210708618}, u\'type\': u\'metric\'},\n            {u\'timestamp\': 1512945127500, u\'sdk_run_uuid\': u\'77972e75-6266-4d2a-94b7-25117b7dcd08\', u\'payload\': {u\'timestamp\': 1512944553023, u\'is_internal\': False, u\'name\': u\'loss\', u\'value\': 0.455232173204422}, u\'type\': u\'metric\'},\n            {u\'timestamp\': 1512945127503, u\'sdk_run_uuid\': u\'77972e75-6266-4d2a-94b7-25117b7dcd08\', u\'payload\': {u\'timestamp\': 1512944554024, u\'is_internal\': False, u\'name\': u\'loss\', u\'value\': 0.372854620218277}, u\'type\': u\'metric\'},\n            {u\'timestamp\': 1512945127505, u\'sdk_run_uuid\': u\'77972e75-6266-4d2a-94b7-25117b7dcd08\', u\'payload\': {u\'timestamp\': 1512944555025, u\'is_internal\': False, u\'name\': u\'loss\', u\'value\': 0.4262654483318329}, u\'type\': u\'metric\'},\n            {u\'timestamp\': 1512945127508, u\'sdk_run_uuid\': u\'77972e75-6266-4d2a-94b7-25117b7dcd08\', u\'payload\': {u\'timestamp\': 1512944556025, u\'is_internal\': False, u\'name\': u\'loss\', u\'value\': 0.38998115062713623}, u\'type\': u\'metric\'},\n            {u\'timestamp\': 1512945127510, u\'sdk_run_uuid\': u\'77972e75-6266-4d2a-94b7-25117b7dcd08\', u\'payload\': {u\'timestamp\': 1512944557025, u\'is_internal\': False, u\'name\': u\'loss\', u\'value\': 0.38911107182502747}, u\'type\': u\'metric\'},\n            {u\'timestamp\': 1512945127512, u\'sdk_run_uuid\': u\'77972e75-6266-4d2a-94b7-25117b7dcd08\', u\'payload\': {u\'timestamp\': 1512944558026, u\'is_internal\': False, u\'name\': u\'loss\', u\'value\': 0.4348866641521454}, u\'type\': u\'metric\'},\n            {u\'timestamp\': 1512945127516, u\'sdk_run_uuid\': u\'77972e75-6266-4d2a-94b7-25117b7dcd08\', u\'payload\': {u\'timestamp\': 1512944559027, u\'is_internal\': False, u\'name\': u\'loss\', u\'value\': 0.286268413066864}, u\'type\': u\'metric\'},\n            {u\'timestamp\': 1512945127519, u\'sdk_run_uuid\': u\'77972e75-6266-4d2a-94b7-25117b7dcd08\', u\'payload\': {u\'timestamp\': 1512944560028, u\'is_internal\': False, u\'name\': u\'loss\', u\'value\': 0.3003489673137665}, u\'type\': u\'metric\'},\n            {u\'timestamp\': 1512945127523, u\'sdk_run_uuid\': u\'77972e75-6266-4d2a-94b7-25117b7dcd08\', u\'payload\': {u\'timestamp\': 1512944561029, u\'is_internal\': False, u\'name\': u\'loss\', u\'value\': 0.3317962884902954}, u\'type\': u\'metric\'},\n            {u\'timestamp\': 1512945127525, u\'sdk_run_uuid\': u\'77972e75-6266-4d2a-94b7-25117b7dcd08\', u\'payload\': {u\'timestamp\': 1512944562034, u\'is_internal\': False, u\'name\': u\'loss\', u\'value\': 0.27931177616119385}, u\'type\': u\'metric\'},\n            {u\'timestamp\': 1512945127526, u\'sdk_run_uuid\': u\'77972e75-6266-4d2a-94b7-25117b7dcd08\', u\'payload\': {u\'timestamp\': 1512944563042, u\'is_internal\': False, u\'name\': u\'loss\', u\'value\': 0.6486308574676514}, u\'type\': u\'metric\'},\n            {u\'timestamp\': 1512945127528, u\'sdk_run_uuid\': u\'77972e75-6266-4d2a-94b7-25117b7dcd08\', u\'payload\': {u\'timestamp\': 1512944564109, u\'is_internal\': False, u\'name\': u\'loss\', u\'value\': 0.5528809428215027}, u\'type\': u\'metric\'},\n            {u\'timestamp\': 1512945127531, u\'sdk_run_uuid\': u\'77972e75-6266-4d2a-94b7-25117b7dcd08\', u\'payload\': {u\'timestamp\': 1512944565113, u\'is_internal\': False, u\'name\': u\'loss\', u\'value\': 0.3118564486503601}, u\'type\': u\'metric\'},\n            {u\'timestamp\': 1512945127542, u\'sdk_run_uuid\': u\'77972e75-6266-4d2a-94b7-25117b7dcd08\', u\'payload\': {u\'timestamp\': 1512944566113, u\'is_internal\': False, u\'name\': u\'loss\', u\'value\': 0.48642101883888245}, u\'type\': u\'metric\'},\n            {u\'timestamp\': 1512945127544, u\'sdk_run_uuid\': u\'77972e75-6266-4d2a-94b7-25117b7dcd08\', u\'payload\': {u\'timestamp\': 1512944567114, u\'is_internal\': False, u\'name\': u\'loss\', u\'value\': 0.2462320178747177}, u\'type\': u\'metric\'},\n            {u\'timestamp\': 1512945127549, u\'sdk_run_uuid\': u\'77972e75-6266-4d2a-94b7-25117b7dcd08\', u\'payload\': {u\'timestamp\': 1512944568117, u\'is_internal\': False, u\'name\': u\'loss\', u\'value\': 0.2997052073478699}, u\'type\': u\'metric\'},\n            {u\'timestamp\': 1512945127553, u\'sdk_run_uuid\': u\'77972e75-6266-4d2a-94b7-25117b7dcd08\', u\'payload\': {u\'timestamp\': 1512944569119, u\'is_internal\': False, u\'name\': u\'loss\', u\'value\': 0.19338417053222656}, u\'type\': u\'metric\'},\n            {u\'timestamp\': 1512945127557, u\'sdk_run_uuid\': u\'77972e75-6266-4d2a-94b7-25117b7dcd08\', u\'payload\': {u\'timestamp\': 1512944570120, u\'is_internal\': False, u\'name\': u\'loss\', u\'value\': 0.4237367510795593}, u\'type\': u\'metric\'},\n            {u\'timestamp\': 1512945127560, u\'sdk_run_uuid\': u\'77972e75-6266-4d2a-94b7-25117b7dcd08\', u\'payload\': {u\'timestamp\': 1512944571121, u\'is_internal\': False, u\'name\': u\'loss\', u\'value\': 0.2270836979150772}, u\'type\': u\'metric\'},\n            {u\'timestamp\': 1512945127561, u\'sdk_run_uuid\': u\'77972e75-6266-4d2a-94b7-25117b7dcd08\', u\'payload\': {u\'timestamp\': 1512944548971, u\'is_internal\': False, u\'name\': u\'accuracy\', u\'value\': 0.14000000059604645}, u\'type\': u\'metric\'},\n            {u\'timestamp\': 1512945127563, u\'sdk_run_uuid\': u\'77972e75-6266-4d2a-94b7-25117b7dcd08\', u\'payload\': {u\'timestamp\': 1512944549971, u\'is_internal\': False, u\'name\': u\'accuracy\', u\'value\': 0.800000011920929}, u\'type\': u\'metric\'},\n            {u\'timestamp\': 1512945127566, u\'sdk_run_uuid\': u\'77972e75-6266-4d2a-94b7-25117b7dcd08\', u\'payload\': {u\'timestamp\': 1512944550972, u\'is_internal\': False, u\'name\': u\'accuracy\', u\'value\': 0.8999999761581421}, u\'type\': u\'metric\'},\n            {u\'timestamp\': 1512945127568, u\'sdk_run_uuid\': u\'77972e75-6266-4d2a-94b7-25117b7dcd08\', u\'payload\': {u\'timestamp\': 1512944551976, u\'is_internal\': False, u\'name\': u\'accuracy\', u\'value\': 0.8600000143051147}, u\'type\': u\'metric\'},\n            {u\'timestamp\': 1512945127570, u\'sdk_run_uuid\': u\'77972e75-6266-4d2a-94b7-25117b7dcd08\', u\'payload\': {u\'timestamp\': 1512944553023, u\'is_internal\': False, u\'name\': u\'accuracy\', u\'value\': 0.9100000262260437}, u\'type\': u\'metric\'},\n            {u\'timestamp\': 1512945127573, u\'sdk_run_uuid\': u\'77972e75-6266-4d2a-94b7-25117b7dcd08\', u\'payload\': {u\'timestamp\': 1512944554024, u\'is_internal\': False, u\'name\': u\'accuracy\', u\'value\': 0.8999999761581421}, u\'type\': u\'metric\'},\n            {u\'timestamp\': 1512945127575, u\'sdk_run_uuid\': u\'77972e75-6266-4d2a-94b7-25117b7dcd08\', u\'payload\': {u\'timestamp\': 1512944555025, u\'is_internal\': False, u\'name\': u\'accuracy\', u\'value\': 0.8899999856948853}, u\'type\': u\'metric\'},\n            {u\'timestamp\': 1512945127578, u\'sdk_run_uuid\': u\'77972e75-6266-4d2a-94b7-25117b7dcd08\', u\'payload\': {u\'timestamp\': 1512944556025, u\'is_internal\': False, u\'name\': u\'accuracy\', u\'value\': 0.8799999952316284}, u\'type\': u\'metric\'},\n            {u\'timestamp\': 1512945127580, u\'sdk_run_uuid\': u\'77972e75-6266-4d2a-94b7-25117b7dcd08\', u\'payload\': {u\'timestamp\': 1512944557025, u\'is_internal\': False, u\'name\': u\'accuracy\', u\'value\': 0.8899999856948853}, u\'type\': u\'metric\'},\n            {u\'timestamp\': 1512945127582, u\'sdk_run_uuid\': u\'77972e75-6266-4d2a-94b7-25117b7dcd08\', u\'payload\': {u\'timestamp\': 1512944558026, u\'is_internal\': False, u\'name\': u\'accuracy\', u\'value\': 0.8799999952316284}, u\'type\': u\'metric\'},\n            {u\'timestamp\': 1512945127585, u\'sdk_run_uuid\': u\'77972e75-6266-4d2a-94b7-25117b7dcd08\', u\'payload\': {u\'timestamp\': 1512944559027, u\'is_internal\': False, u\'name\': u\'accuracy\', u\'value\': 0.9599999785423279}, u\'type\': u\'metric\'},\n            {u\'timestamp\': 1512945127587, u\'sdk_run_uuid\': u\'77972e75-6266-4d2a-94b7-25117b7dcd08\', u\'payload\': {u\'timestamp\': 1512944560028, u\'is_internal\': False, u\'name\': u\'accuracy\', u\'value\': 0.8999999761581421}, u\'type\': u\'metric\'},\n            {u\'timestamp\': 1512945127591, u\'sdk_run_uuid\': u\'77972e75-6266-4d2a-94b7-25117b7dcd08\', u\'payload\': {u\'timestamp\': 1512944561029, u\'is_internal\': False, u\'name\': u\'accuracy\', u\'value\': 0.9200000166893005}, u\'type\': u\'metric\'},\n            {u\'timestamp\': 1512945127593, u\'sdk_run_uuid\': u\'77972e75-6266-4d2a-94b7-25117b7dcd08\', u\'payload\': {u\'timestamp\': 1512944562034, u\'is_internal\': False, u\'name\': u\'accuracy\', u\'value\': 0.9300000071525574}, u\'type\': u\'metric\'},\n            {u\'timestamp\': 1512945127594, u\'sdk_run_uuid\': u\'77972e75-6266-4d2a-94b7-25117b7dcd08\', u\'payload\': {u\'timestamp\': 1512944563042, u\'is_internal\': False, u\'name\': u\'accuracy\', u\'value\': 0.8500000238418579}, u\'type\': u\'metric\'},\n            {u\'timestamp\': 1512945127596, u\'sdk_run_uuid\': u\'77972e75-6266-4d2a-94b7-25117b7dcd08\', u\'payload\': {u\'timestamp\': 1512944564109, u\'is_internal\': False, u\'name\': u\'accuracy\', u\'value\': 0.8199999928474426}, u\'type\': u\'metric\'},\n            {u\'timestamp\': 1512945127599, u\'sdk_run_uuid\': u\'77972e75-6266-4d2a-94b7-25117b7dcd08\', u\'payload\': {u\'timestamp\': 1512944565113, u\'is_internal\': False, u\'name\': u\'accuracy\', u\'value\': 0.9300000071525574}, u\'type\': u\'metric\'},\n            {u\'timestamp\': 1512945127602, u\'sdk_run_uuid\': u\'77972e75-6266-4d2a-94b7-25117b7dcd08\', u\'payload\': {u\'timestamp\': 1512944566113, u\'is_internal\': False, u\'name\': u\'accuracy\', u\'value\': 0.8600000143051147}, u\'type\': u\'metric\'},\n            {u\'timestamp\': 1512945127603, u\'sdk_run_uuid\': u\'77972e75-6266-4d2a-94b7-25117b7dcd08\', u\'payload\': {u\'timestamp\': 1512944567114, u\'is_internal\': False, u\'name\': u\'accuracy\', u\'value\': 0.949999988079071}, u\'type\': u\'metric\'},\n            {u\'timestamp\': 1512945127605, u\'sdk_run_uuid\': u\'77972e75-6266-4d2a-94b7-25117b7dcd08\', u\'payload\': {u\'timestamp\': 1512944568117, u\'is_internal\': False, u\'name\': u\'accuracy\', u\'value\': 0.9300000071525574}, u\'type\': u\'metric\'},\n            {u\'timestamp\': 1512945127608, u\'sdk_run_uuid\': u\'77972e75-6266-4d2a-94b7-25117b7dcd08\', u\'payload\': {u\'timestamp\': 1512944569119, u\'is_internal\': False, u\'name\': u\'accuracy\', u\'value\': 0.9599999785423279}, u\'type\': u\'metric\'},\n            {u\'timestamp\': 1512945127610, u\'sdk_run_uuid\': u\'77972e75-6266-4d2a-94b7-25117b7dcd08\', u\'payload\': {u\'timestamp\': 1512944570120, u\'is_internal\': False, u\'name\': u\'accuracy\', u\'value\': 0.8799999952316284}, u\'type\': u\'metric\'},\n            {u\'timestamp\': 1512945127612, u\'sdk_run_uuid\': u\'77972e75-6266-4d2a-94b7-25117b7dcd08\', u\'payload\': {u\'timestamp\': 1512944571121, u\'is_internal\': False, u\'name\': u\'accuracy\', u\'value\': 0.9300000071525574}, u\'type\': u\'metric\'},\n            {u\'timestamp\': 1512945128493, u\'sdk_run_uuid\': u\'77972e75-6266-4d2a-94b7-25117b7dcd08\', u\'payload\': {u\'body\': u\'This run of some_job_name ran for 0:00:01 and logs are available locally at: /Users/richie/.hyperdash/logs/some-job-name/some-job-name_2017-12-10t17-32-07-487283.log\\n\', u\'uuid\': u\'2a00ed85-d9a0-4df5-ab2d-fee35fb1dd00\', u\'level\': u\'INFO\'}, u\'type\': u\'log\'},\n            {u\'timestamp\': 1512945128493, u\'sdk_run_uuid\': u\'77972e75-6266-4d2a-94b7-25117b7dcd08\', u\'payload\': {u\'final_status\': u\'success\'}, u\'type\': u\'run_ended\'}\n        ]\n\n        # TODO: Make this part pass in PY3 (for some reason the sampling is slightly different in PY3)\n        if PY2:\n            for i, message in enumerate(server_sdk_messages):\n                if message[\'type\'] == \'metric\':\n                    assert message[\'payload\'] == expected_metrics[i][\'payload\']\n        else:\n            assert len(expected_metrics) > 20\n        \n        # Make sure correct API name / version headers are sent\n        assert server_sdk_headers[0][API_KEY_NAME] == API_NAME_CLI_TENSORBOARD\n        assert server_sdk_headers[0][VERSION_KEY_NAME] == get_hyperdash_version()\n\n\n    def test_pipe(self):\n        job_name = ""some_job_name""\n        inputs = [\n            ""hello world"",\n            ""foo bar baz"",\n            ""this is the test script"",\n            ""\xe5\xad\x97"",\n            ""{\'some_obj_key\': \'some_value\'}"",\n        ]\n        r_d, w_d = os.pipe()\n        r_pipe = os.fdopen(r_d)\n        w_pipe = os.fdopen(w_d, \'w\')\n        with patch(\'hyperdash_cli.cli.get_access_token_from_file\', Mock(return_value=DEFAULT_ACCESS_TOKEN)), patch(\'sys.stdin\', new=r_pipe), patch(\'sys.stdout\', new=StringIO()) as fake_out:\n            def writer():\n                for input_str in inputs:\n                    w_pipe.write(input_str)\n                w_pipe.flush()\n                w_pipe.close()\n            writer_thread = Thread(target=writer)\n            writer_thread.start()\n\n            hyperdash_cli.pipe(\n                argparse.Namespace(\n                    name=job_name\n                )\n            )\n\n        for expected in inputs:\n            if PY2:\n                assert_in(expected, fake_out.getvalue().encode(""utf-8""))\n                continue\n            assert_in(expected, fake_out.getvalue())\n\n        # Make sure correct API name / version headers are sent\n        assert server_sdk_headers[0][API_KEY_NAME] == API_NAME_CLI_PIPE\n        assert server_sdk_headers[0][VERSION_KEY_NAME] == get_hyperdash_version()\n\n        # Make sure logs were persisted\n        log_dir = get_hyperdash_logs_home_path_for_job(job_name)\n        latest_log_file = max([\n            os.path.join(log_dir, filename) for\n            filename in\n            os.listdir(log_dir)\n        ], key=os.path.getmtime)\n        with open(latest_log_file, \'r\') as log_file:\n            data = log_file.read()\n            for expected in inputs:\n                assert_in(expected, data)\n        os.remove(latest_log_file)\n\n    def test_version(self):\n        with patch(\'sys.stdout\', new=StringIO()) as fake_out:\n            hyperdash_cli.version()\n\n        expected_output = [\n            ""hyperdash {}"".format(get_hyperdash_version())\n        ]\n        for expected in expected_output:\n            assert_in(expected, fake_out.getvalue())'"
tests/test_jupyter.py,0,"b'# -*- coding: utf-8 -*-\nimport json\nimport os\n\nfrom six import PY2\nfrom nose.tools import assert_in\nimport requests\nimport nbformat\nfrom nbconvert.preprocessors import ExecutePreprocessor\n\nfrom hyperdash import monitor\nfrom hyperdash.constants import API_KEY_NAME\nfrom hyperdash.constants import API_NAME_JUPYTER\nfrom hyperdash.constants import get_hyperdash_logs_home_path_for_job\nfrom hyperdash.constants import get_hyperdash_version\nfrom hyperdash.constants import VERSION_KEY_NAME\nfrom mocks import init_mock_server\n\n\nserver_sdk_headers = []\n\n\nclass TestJupyter(object):\n    def setup(self):\n        global server_sdk_headers\n        server_sdk_headers = []\n\n    @classmethod\n    def setup_class(_cls):\n        request_handle_dict = init_mock_server()\n\n        def sdk_message(response):\n            # Store headers so we can assert on them later\n            if PY2:\n                server_sdk_headers.append(response.headers.dict)\n            else:\n                server_sdk_headers.append(response.headers)\n\n            # Add response status code.\n            response.send_response(requests.codes.ok)\n\n            # Add response headers.\n            response.send_header(\'Content-Type\', \'application/json; charset=utf-8\')\n            response.end_headers()\n\n            # Add response content. In this case, we use the exact same response for\n            # all messages from the SDK because the current implementation ignores the\n            # body of the response unless there is an error.\n            response_content = json.dumps({})\n\n            response.wfile.write(response_content.encode(\'utf-8\'))\n\n        request_handle_dict[(""POST"", ""/api/v1/sdk/http"")] = sdk_message\n\n    def test_monitor_cell(self):\n        expected_logs = [\n            ""Beginning machine learning..."",\n            ""Still training..."",\n            ""Done!"",\n            # Handle unicode\n            ""\xe5\xad\x97""\n        ]\n\n        with open(""./tests/jupyter_test_file.py.ipynb"") as f:\n            nb = nbformat.read(f, as_version=4.0)\n\n        ep = ExecutePreprocessor(timeout=5000, kernel_name=""python"")\n        result = ep.preprocess(nb, {})\n\n        # Accumulate all output from cell 2\n        all_stdout = """"\n        second_cell_output = result[0][""cells""][1][""outputs""]\n        for output in second_cell_output:\n            if output.name == ""stdout"":\n                all_stdout = all_stdout + output.text\n\n        # Assert on all output from cell2\n        for log in expected_logs:\n            if PY2:\n                assert_in(log, all_stdout.encode(""utf-8""))\n                continue\n            assert_in(log, all_stdout)\n\n        # Verify that variables declared in previous cells can be affected\n        third_cell_output = result[0][""cells""][2][""outputs""]\n        assert third_cell_output[0].text == ""a=1\\n""\n\n        # Make sure correct API name / version headers are sent\n        assert server_sdk_headers[0][API_KEY_NAME] == API_NAME_JUPYTER\n        assert server_sdk_headers[0][VERSION_KEY_NAME] == get_hyperdash_version()\n\n        # Make sure logs were persisted\n        log_dir = get_hyperdash_logs_home_path_for_job(""test_jupyter"")\n        latest_log_file = max([\n            os.path.join(log_dir, filename) for\n            filename in\n            os.listdir(log_dir)\n        ], key=os.path.getmtime)\n        with open(latest_log_file, \'r\') as log_file:\n            data = log_file.read()\n            for log in expected_logs:\n                assert_in(log, data)\n        os.remove(latest_log_file)\n'"
tests/test_script_for_run_test.py,0,"b'# -*- coding: utf-8 -*-\n\nimport random\nimport string\nimport sys\n\n\ndef main():\n    print(""this is the test script"")\n    # Make sure unicode works\n    print(""\xe5\xad\x97"")\n    print(""\xe5\xad\x97"")\n    print(""\xe5\xad\x97"")\n    print(""\xe5\xad\x97"")\n    print(""\xe5\xad\x97"")\n    # Make sure printing non-strings works\n    print({""some_obj_key"": ""some_value""})\n\n\nmain()\n'"
tests/test_sdk.py,0,"b'# -*- coding: utf-8 -*-\n\nimport json\nimport os\nimport random\nimport string\nimport time\n\nimport six\nfrom six import StringIO\nfrom six import PY2\nfrom mock import patch\nfrom nose.tools import assert_in\nimport requests\nimport numpy as np\n\nfrom hyperdash import monitor\nfrom hyperdash import Experiment\nfrom mocks import init_mock_server\nfrom hyperdash.constants import API_KEY_NAME\nfrom hyperdash.constants import API_NAME_EXPERIMENT\nfrom hyperdash.constants import API_NAME_MONITOR\nfrom hyperdash.constants import get_hyperdash_logs_home_path_for_job\nfrom hyperdash.constants import get_hyperdash_version\nfrom hyperdash.constants import VERSION_KEY_NAME\nfrom threading import Thread\nfrom hyperdash.constants import MAX_LOG_SIZE_BYTES\nfrom hyperdash.hyper_dash import HyperDash\n\n\nserver_sdk_messages = []\nserver_sdk_headers = []\n\nif PY2:\n    lowercase_letters = string.lowercase\nelse:\n    lowercase_letters = string.ascii_lowercase\n\n\nclass TestSDK(object):\n    def setup(self):\n        global server_sdk_messages\n        global server_sdk_headers\n        server_sdk_messages = []\n        server_sdk_headers = []\n\n    @classmethod\n    def setup_class(_cls):\n        request_handle_dict = init_mock_server()\n\n        def sdk_message(response):\n            global server_sdk_messages\n            global server_sdk_headers\n            message = json.loads(response.rfile.read(\n                int(response.headers[""Content-Length""])).decode(""utf-8""))\n\n            # Store messages / headers so we can assert on them later\n            server_sdk_messages.append(message)\n            if PY2:\n                server_sdk_headers.append(response.headers.dict)\n            else:\n                server_sdk_headers.append(response.headers)\n\n            # Add response status code.\n            response.send_response(requests.codes.ok)\n\n            # Add response headers.\n            response.send_header(\n                ""Content-Type"", ""application/json; charset=utf-8"")\n            response.end_headers()\n\n            # Add response content. In this case, we use the exact same response for\n            # all messages from the SDK because the current implementation ignores the\n            # body of the response unless there is an error.\n            response_content = json.dumps({})\n\n            response.wfile.write(response_content.encode(""utf-8""))\n\n        request_handle_dict[(""POST"", ""/api/v1/sdk/http"")] = sdk_message\n\n    def test_monitor(self):\n        job_name = ""some:job(name)with unsafe for files ystem chars""\n        logs = [\n            ""Beginning machine learning..."",\n            ""Still training..."",\n            ""Done!"",\n            # Handle unicode\n            ""\xe5\xad\x97"",\n            # Huge log\n            """".join(random.choice(lowercase_letters)\n                    for x in range(10 * MAX_LOG_SIZE_BYTES)),\n        ]\n        test_obj = {""some_obj_key"": ""some_value""}\n        expected_return = ""final_result""\n\n        with patch(""sys.stdout"", new=StringIO()) as fake_out:\n            @monitor(job_name)\n            def test_job():\n                for log in logs:\n                    print(log)\n                    time.sleep(0.1)\n                print(test_obj)\n                time.sleep(0.1)\n                return expected_return\n\n            return_val = test_job()\n\n            assert return_val == expected_return\n            captured_out = fake_out.getvalue()\n            for log in logs:\n                if PY2:\n                    assert log in captured_out.encode(""utf-8"")\n                    continue\n                assert log in captured_out\n            assert str(test_obj) in captured_out\n\n            assert ""error"" not in captured_out\n\n        # Make sure logs were persisted\n        log_dir = get_hyperdash_logs_home_path_for_job(job_name)\n        latest_log_file = max([\n            os.path.join(log_dir, filename) for\n            filename in\n            os.listdir(log_dir)\n        ], key=os.path.getmtime)\n        with open(latest_log_file, ""r"") as log_file:\n            data = log_file.read()\n            for log in logs:\n                assert_in(log, data)\n        os.remove(latest_log_file)\n\n        # Make sure correct API name / version headers are sent\n        assert server_sdk_headers[0][API_KEY_NAME] == API_NAME_MONITOR\n        assert server_sdk_headers[0][VERSION_KEY_NAME] == get_hyperdash_version()\n\n    def test_monitor_raises_exceptions(self):\n        exception_raised = True\n        expected_exception = ""some_exception""\n\n        @monitor(""test_job"")\n        def test_job():\n            time.sleep(0.1)\n            raise Exception(expected_exception)\n\n        try:\n            test_job()\n            exception_raised = False\n        except Exception as e:\n            assert str(e) == expected_exception\n\n        assert exception_raised\n\n    def test_monitor_with_experiment_and_no_capture_io(self):\n        job_name = ""some_job_name""\n\n        with patch(""sys.stdout"", new=StringIO()):\n            def worker(thread_num):\n                @monitor(job_name, capture_io=False)\n                def monitored_func(exp):\n                    print(""this should not be in there"")\n                    exp.logger.info(thread_num)\n                    exp.logger.info(\n                        ""thread {} is doing some work"".format(thread_num))\n                    exp.logger.info(""\xe5\xad\x97"")\n                    time.sleep(0.1)\n                return monitored_func\n\n            t1 = Thread(target=worker(1))\n            t2 = Thread(target=worker(2))\n            t3 = Thread(target=worker(3))\n\n            t1.daemon = True\n            t2.daemon = True\n            t3.daemon = True\n\n            t1.start()\n            t2.start()\n            t3.start()\n\n            t1.join()\n            t2.join()\n            t3.join()\n\n        # Make sure logs were persisted -- We use this as a proxy\n        # to make sure that each of the threads only captured its\n        # own output by checkign that each log file only has 5\n        # lines\n        log_dir = get_hyperdash_logs_home_path_for_job(job_name)\n        latest_log_files = sorted([\n            os.path.join(log_dir, filename) for\n            filename in\n            os.listdir(log_dir)\n        ], key=os.path.getmtime)[-3:]\n        for file_name in latest_log_files:\n            with open(file_name, ""r"") as log_file:\n                data = log_file.read()\n                assert ""this should not be in there"" not in data\n                assert ""is doing some work"" in data\n                assert ""\xe5\xad\x97"" in data\n                assert (len(data.split(""\\n"")) == 5)\n            os.remove(file_name)\n\n    def test_monitor_limits_server_message_size(self):\n        job_name = ""some_job_name""\n        logs = [\n            ""Beginning machine learning..."",\n            ""Still training..."",\n            ""Done!"",\n            # Handle unicode\n            ""\xe5\xad\x97"",\n            # Huge log\n            """".join(random.choice(lowercase_letters)\n                    for x in range(10 * MAX_LOG_SIZE_BYTES)),\n        ]\n        test_obj = {""some_obj_key"": ""some_value""}\n        expected_return = ""final_result""\n\n        @monitor(job_name)\n        def test_job():\n            for log in logs:\n                print(log)\n                time.sleep(0.1)\n            print(test_obj)\n            time.sleep(0.1)\n            return expected_return\n\n        return_val = test_job()\n\n        assert return_val == expected_return\n\n        all_text_sent_to_server = """"\n        for msg in server_sdk_messages:\n            payload = msg[""payload""]\n            if ""body"" in payload:\n                all_text_sent_to_server = all_text_sent_to_server + \\\n                    payload[""body""]\n\n        for log in logs:\n            if PY2:\n                assert log in all_text_sent_to_server.encode(""utf-8"")\n                continue\n            assert log in all_text_sent_to_server\n        assert str(test_obj) in all_text_sent_to_server\n\n    def test_metric(self):\n        job_name = ""metric job name""\n\n        metrics = [\n            (""acc"", 99),\n            (""loss"", 0.00000000041),\n            (""val_loss"", 4324320984309284328743827432),\n            (""mse"", -431.321),\n        ]\n\n        @monitor(job_name)\n        def test_job(exp):\n            for key, val in metrics:\n                exp.metric(key, val)\n            # These ones should not be emitted because we didn\'t\n            # wait long enough\n            for key, val in metrics:\n                exp.metric(key, val-1)\n            time.sleep(1.0)\n            # These one\'s should be emitted\n            for key, val in metrics:\n                exp.metric(key, val-2)\n        test_job()\n\n        sent_vals = []\n        for msg in server_sdk_messages:\n            payload = msg[""payload""]\n            if ""name"" in payload:\n                sent_vals.append(payload)\n\n        assert len(sent_vals) == len(metrics)*2\n        expected_metrics = [\n            {""is_internal"": False, ""name"": ""acc"", ""value"": 99.0},\n            {""is_internal"": False, ""name"": ""loss"", ""value"": 0.00000000041},\n            {""is_internal"": False, ""name"": ""val_loss"", ""value"": 4324320984309284328743827432.0},\n            {""is_internal"": False, ""name"": ""mse"", ""value"": -431.321},\n            {""is_internal"": False, ""name"": ""acc"", ""value"": 97.0},\n            {""is_internal"": False, ""name"": ""loss"", ""value"": -1.99999999959},\n            {""is_internal"": False, ""name"": ""val_loss"", ""value"": 4324320984309284328743827430.0},\n            {""is_internal"": False, ""name"": ""mse"", ""value"": -433.321}\n        ]\n        for i, message in enumerate(sent_vals):\n            assert message[""is_internal""] == expected_metrics[i][""is_internal""]\n            assert message[""name""] == expected_metrics[i][""name""]\n            assert message[""value""] == expected_metrics[i][""value""]\n\n    def test_param(self):\n        params = ((""lr"", 0.5), (""loss_function"", ""MSE""))\n\n        # Run a test job that emits some hyperparameters\n        with patch(""sys.stdout"", new=StringIO()) as fake_out:\n            @monitor(""test params"")\n            def test_job(exp):\n                for param in params:\n                    exp.param(param[0], param[1])\n                    time.sleep(0.1)\n                return\n            test_job()\n\n        # Collect sent SDK messages that had a params payload\n        sent_messages = []\n        for msg in server_sdk_messages:\n            payload = msg[""payload""]\n            if ""params"" in payload:\n                sent_messages.append(payload)\n\n        # Assert the sent SDK messages are what we expect\n        assert len(params) == len(sent_messages)\n        for i, message in enumerate(sent_messages):\n            name = params[i][0]\n            val = params[i][1]\n            assert message[""params""][name] == val\n\n        # Assert that the appropriate messages were printed to STDOUT\n        for param in params:\n            assert param[0] in fake_out.getvalue()\n            assert str(param[1]) in fake_out.getvalue()\n\n    def test_experiment(self):\n        # Run a test job via the Experiment API\n        # Make sure log file is where is supposed to be\n        # look at decorator\n        # verify run start/stop is sent\n        with patch(""sys.stdout"", new=StringIO()) as faked_out:\n            exp = Experiment(""MNIST"")\n            exp.log(""test print"")\n            exp.param(""batch size"", 32)\n            for i in exp.iter(2):\n                time.sleep(1)\n                exp.metric(""accuracy"", i*0.2)\n            time.sleep(0.1)\n            exp.end()\n        \n        # Test params match what is expected\n        params_messages = []\n        for msg in server_sdk_messages:\n            payload = msg[""payload""]\n            if ""params"" in payload:\n                params_messages.append(payload)\n\n        expect_params = [\n            {\n                ""params"": {\n                    ""batch size"": 32,\n                },\n                ""is_internal"": False,\n            },\n            {\n                ""params"": {\n                    ""hd_iter_0_epochs"": 2,\n                },\n                ""is_internal"": True,\n            },\n        ]    \n        assert len(expect_params) == len(params_messages)\n        for i, message in enumerate(params_messages):\n            assert message == expect_params[i]\n\n        # Test metrics match what is expected\n        metrics_messages = []\n        for msg in server_sdk_messages:\n            payload = msg[""payload""]\n            if ""name"" in payload:\n                metrics_messages.append(payload)\n\n        expect_metrics = [\n            {""is_internal"": True, ""name"": ""hd_iter_0"", ""value"": 0},\n            {""is_internal"": False, ""name"": ""accuracy"", ""value"": 0},\n            {""is_internal"": True, ""name"": ""hd_iter_0"", ""value"": 1},\n            {""is_internal"": False, ""name"": ""accuracy"", ""value"": 0.2},\n       ]\n        assert len(expect_metrics) == len(metrics_messages)\n        for i, message in enumerate(metrics_messages):\n            assert message[""is_internal""] == expect_metrics[i][""is_internal""]\n            assert message[""name""] == expect_metrics[i][""name""]\n            assert message[""value""] == expect_metrics[i][""value""]\n        \n        captured_out = faked_out.getvalue()\n        assert ""error"" not in captured_out\n\n        # Make sure correct API name / version headers are sent\n        assert server_sdk_headers[0][API_KEY_NAME] == API_NAME_EXPERIMENT\n        assert server_sdk_headers[0][VERSION_KEY_NAME] == get_hyperdash_version()\n        \n        # Make sure logs were persisted\n        expect_logs = [\n            ""{ batch size: 32 }"",\n            ""test print"",\n            ""| Iteration 0 of 1 |"",\n            ""| accuracy:   0.000000 |"",\n        ]\n\n        log_dir = get_hyperdash_logs_home_path_for_job(""MNIST"")\n        latest_log_file = max([\n            os.path.join(log_dir, filename) for\n            filename in\n            os.listdir(log_dir)\n        ], key=os.path.getmtime)\n        with open(latest_log_file, ""r"") as log_file:\n            data = log_file.read()\n            for log in expect_logs:\n                assert_in(log, data)\n        os.remove(latest_log_file)\n\n    def test_experiment_keras_callback(self):\n        with patch(""sys.stdout"", new=StringIO()) as faked_out:\n            exp = Experiment(""MNIST"")\n            keras_cb = exp.callbacks.keras\n            keras_cb.on_epoch_end(0, {""val_acc"": 1, ""val_loss"": 2})\n            # Sleep 1 second due to client sampling\n            time.sleep(1)\n            keras_cb.on_epoch_end(1, {""val_acc"": 3, ""val_loss"": 4})\n            exp.end()\n\n        # Test metrics match what is expected\n        metrics_messages = []\n        for msg in server_sdk_messages:\n            payload = msg[""payload""]\n            if ""name"" in payload:\n                metrics_messages.append(payload)\n        expect_metrics = [\n            {""is_internal"": False, ""name"": ""val_acc"", ""value"": 1},\n            {""is_internal"": False, ""name"": ""val_loss"", ""value"": 2},\n            {""is_internal"": False, ""name"": ""val_acc"", ""value"": 3},\n            {""is_internal"": False, ""name"": ""val_loss"", ""value"": 4},\n        ]\n        assert len(expect_metrics) == len(metrics_messages)\n        for i, message in enumerate(metrics_messages):\n            assert message[""is_internal""] == expect_metrics[i][""is_internal""]\n            assert message[""name""] == expect_metrics[i][""name""]\n            assert message[""value""] == expect_metrics[i][""value""]\n        \n        captured_out = faked_out.getvalue()\n        assert ""error"" not in captured_out\n\n    def test_experiment_handles_numpy_numbers(self):\n        nums_to_test = [\n            (""int_"", np.int_()),\n            (""intc"", np.intc()),\n            (""intp"", np.intp()),\n            (""int8"", np.int8()),\n            (""int16"", np.int16()),\n            (""int32"", np.int32()),\n            (""int64"", np.int64()),\n            (""uint8"", np.uint8()),\n            (""uint16"", np.uint16()),\n            (""uint32"", np.uint32()),\n            (""uint64"", np.uint64()),\n            (""float16"", np.float16()),\n            (""float32"", np.float32()),\n            (""float64"", np.float64()),\n        ]\n        # Make sure the SDK doesn\'t choke and JSON serialization works\n        exp = Experiment(""MNIST"")\n        for name, num in nums_to_test:\n            exp.metric(""test_metric_{}"".format(name), num)\n            exp.param(""test_param_{}"".format(name), num)\n        exp.end()\n        \n        # Test params match what is expected\n        params_messages = []\n        for msg in server_sdk_messages:\n            payload = msg[""payload""]\n            if ""params"" in payload:\n                params_messages.append(payload)\n\n        expected_params = []\n        for name, num in nums_to_test:\n            obj = {\n                ""params"": {},\n                ""is_internal"": False,\n            }\n            obj[""params""][""test_param_{}"".format(name)] = num\n            obj[""is_internal""] = False\n            expected_params.append(obj)\n\n        assert len(expected_params) == len(params_messages)\n        for i, message in enumerate(params_messages):\n            assert message == expected_params[i]\n\n        # Test metrics match what is expected\n        metrics_messages = []\n        for msg in server_sdk_messages:\n            payload = msg[""payload""]\n            if ""name"" in payload:\n                metrics_messages.append(payload)\n\n        expected_metrics = []\n        for name, num in nums_to_test:\n            expected_metrics.append({\n                ""name"": ""test_metric_{}"".format(name),\n                ""value"": num,\n                ""is_internal"": False,\n            })\n\n        assert len(expected_metrics) == len(metrics_messages)\n        for i, message in enumerate(metrics_messages):\n            assert message[""is_internal""] == expected_metrics[i][""is_internal""]\n            assert message[""name""] == expected_metrics[i][""name""]\n            assert message[""value""] == expected_metrics[i][""value""]\n        \n    def experiment_raises_exceptions(self):\n        exception_raised = True\n        expected_exception = ""some_exception_b""\n\n        def test_job():\n            exp = Experiment(""Exception experiment"")\n            time.sleep(0.1)\n            raise Exception(expected_exception)\n            exp.end()\n        try:\n            test_job()\n            exception_raised = False\n        except Exception as e:\n            assert str(e) == expected_exception\n\n        assert exception_raised\n\n    def test_iter(self):\n        # Run a test job that includes the iterator function\n        with patch(""sys.stdout"", new=StringIO()) as fake_out:\n            @monitor(""test iter"")\n            def test_job(exp):\n                exp.param(""user_param"", ""test"")\n                for i in exp.iter(5):\n                    # Sleep because metrics are sample at 1s\n                    # frequency by default\n                    time.sleep(1.0)\n                    exp.metric(""loss"", i)\n                for i in exp.iter(3):\n                    time.sleep(1.0)\n                    exp.metric(""loss"", i)\n            test_job()\n\n        # Collect sent SDK messages that had a params payload\n        param_messages = []\n        for msg in server_sdk_messages:\n            payload = msg[""payload""]\n            if ""params"" in payload:\n                param_messages.append(payload)\n\n        # Collect sent SDK messages that had a metrics payload\n        metric_messages = []\n        for msg in server_sdk_messages:\n            payload = msg[""payload""]\n            if ""name"" in payload:\n                metric_messages.append(payload)\n\n        # Assert the sent param SDK messages are what we expect\n        expected_params = [\n            {\n                ""params"": {\n                    ""user_param"": ""test"",\n                },\n                ""is_internal"": False,\n            },\n            {\n                ""params"": {\n                    ""hd_iter_0_epochs"": 5,\n                },\n                ""is_internal"": True,\n            },\n            {\n                ""params"": {\n                    ""hd_iter_1_epochs"": 3,\n                },\n                ""is_internal"": True,\n            }\n        ]\n        assert len(expected_params) == len(param_messages)\n        for i, message in enumerate(param_messages):\n            assert message == expected_params[i]\n\n        # Assert the sent metric SDK messages are what we expect\n        expected_metrics = [\n            {""is_internal"": True, ""name"": ""hd_iter_0"", ""value"": 0},\n            {""is_internal"": False, ""name"": ""loss"", ""value"": 0},\n            {""is_internal"": True, ""name"": ""hd_iter_0"", ""value"": 1},\n            {""is_internal"": False, ""name"": ""loss"", ""value"": 1},\n            {""is_internal"": True, ""name"": ""hd_iter_0"", ""value"": 2},\n            {""is_internal"": False, ""name"": ""loss"", ""value"": 2},\n            {""is_internal"": True, ""name"": ""hd_iter_0"", ""value"": 3},\n            {""is_internal"": False, ""name"": ""loss"", ""value"": 3},\n            {""is_internal"": True, ""name"": ""hd_iter_0"", ""value"": 4},\n            {""is_internal"": False, ""name"": ""loss"", ""value"": 4},\n            {""is_internal"": True, ""name"": ""hd_iter_1"", ""value"": 0},\n            {""is_internal"": False, ""name"": ""loss"", ""value"": 0},\n            {""is_internal"": True, ""name"": ""hd_iter_1"", ""value"": 1},\n            {""is_internal"": False, ""name"": ""loss"", ""value"": 1},\n            {""is_internal"": True, ""name"": ""hd_iter_1"", ""value"": 2},\n            {""is_internal"": False, ""name"": ""loss"", ""value"": 2},\n        ]\n        # print(len(expected_metrics), len(metric_messages))\n        assert len(expected_metrics) == len(metric_messages)\n        for i, message in enumerate(metric_messages):\n            assert message[""is_internal""] == expected_metrics[i][""is_internal""]\n            assert message[""name""] == expected_metrics[i][""name""]\n            assert message[""value""] == expected_metrics[i][""value""]\n\n        # Assert that the internal parameters / metrics were not printed to STDOUT\n        assert ""hd_iter_0"" not in fake_out.getvalue()\n        assert ""hd_iter_1"" not in fake_out.getvalue()\n        assert ""hd_iter_0_epochs"" not in fake_out.getvalue()\n        assert ""hd_iter_1_epochs"" not in fake_out.getvalue()\n        for i in range(5):\n            assert ""| Iteration {} of {} |"".format(i, 4) in fake_out.getvalue()\n        for i in range(3):\n            assert ""| Iteration {} of {} |"".format(i, 2) in fake_out.getvalue()\n'"
