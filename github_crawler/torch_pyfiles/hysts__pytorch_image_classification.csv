file_path,api_count,code
evaluate.py,4,"b""#!/usr/bin/env python\n\nimport argparse\nimport pathlib\nimport time\n\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\nimport tqdm\n\nfrom fvcore.common.checkpoint import Checkpointer\n\nfrom pytorch_image_classification import (\n    apply_data_parallel_wrapper,\n    create_dataloader,\n    create_loss,\n    create_model,\n    get_default_config,\n    update_config,\n)\nfrom pytorch_image_classification.utils import (\n    AverageMeter,\n    create_logger,\n    get_rank,\n)\n\n\ndef load_config():\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--config', type=str, required=True)\n    parser.add_argument('options', default=None, nargs=argparse.REMAINDER)\n    args = parser.parse_args()\n\n    config = get_default_config()\n    config.merge_from_file(args.config)\n    config.merge_from_list(args.options)\n    update_config(config)\n    config.freeze()\n    return config\n\n\ndef evaluate(config, model, test_loader, loss_func, logger):\n    device = torch.device(config.device)\n\n    model.eval()\n\n    loss_meter = AverageMeter()\n    correct_meter = AverageMeter()\n    start = time.time()\n\n    pred_raw_all = []\n    pred_prob_all = []\n    pred_label_all = []\n    with torch.no_grad():\n        for data, targets in tqdm.tqdm(test_loader):\n            data = data.to(device)\n            targets = targets.to(device)\n\n            outputs = model(data)\n            loss = loss_func(outputs, targets)\n\n            pred_raw_all.append(outputs.cpu().numpy())\n            pred_prob_all.append(F.softmax(outputs, dim=1).cpu().numpy())\n\n            _, preds = torch.max(outputs, dim=1)\n            pred_label_all.append(preds.cpu().numpy())\n\n            loss_ = loss.item()\n            correct_ = preds.eq(targets).sum().item()\n            num = data.size(0)\n\n            loss_meter.update(loss_, num)\n            correct_meter.update(correct_, 1)\n\n        accuracy = correct_meter.sum / len(test_loader.dataset)\n\n        elapsed = time.time() - start\n        logger.info(f'Elapsed {elapsed:.2f}')\n        logger.info(f'Loss {loss_meter.avg:.4f} Accuracy {accuracy:.4f}')\n\n    preds = np.concatenate(pred_raw_all)\n    probs = np.concatenate(pred_prob_all)\n    labels = np.concatenate(pred_label_all)\n    return preds, probs, labels, loss_meter.avg, accuracy\n\n\ndef main():\n    config = load_config()\n\n    if config.test.output_dir is None:\n        output_dir = pathlib.Path(config.test.checkpoint).parent\n    else:\n        output_dir = pathlib.Path(config.test.output_dir)\n        output_dir.mkdir(exist_ok=True, parents=True)\n\n    logger = create_logger(name=__name__, distributed_rank=get_rank())\n\n    model = create_model(config)\n    model = apply_data_parallel_wrapper(config, model)\n    checkpointer = Checkpointer(model,\n                                checkpoint_dir=output_dir,\n                                logger=logger,\n                                distributed_rank=get_rank())\n    checkpointer.load(config.test.checkpoint)\n\n    test_loader = create_dataloader(config, is_train=False)\n    _, test_loss = create_loss(config)\n\n    preds, probs, labels, loss, acc = evaluate(config, model, test_loader,\n                                               test_loss, logger)\n\n    output_path = output_dir / f'predictions.npz'\n    np.savez(output_path,\n             preds=preds,\n             probs=probs,\n             labels=labels,\n             loss=loss,\n             acc=acc)\n\n\nif __name__ == '__main__':\n    main()\n"""
train.py,13,"b""#!/usr/bin/env python\n\nimport argparse\nimport pathlib\nimport time\n\nimport apex\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.distributed as dist\nimport torchvision\n\nfrom fvcore.common.checkpoint import Checkpointer\n\nfrom pytorch_image_classification import (\n    apply_data_parallel_wrapper,\n    create_dataloader,\n    create_loss,\n    create_model,\n    create_optimizer,\n    create_scheduler,\n    get_default_config,\n    update_config,\n)\nfrom pytorch_image_classification.config.config_node import ConfigNode\nfrom pytorch_image_classification.utils import (\n    AverageMeter,\n    DummyWriter,\n    compute_accuracy,\n    count_op,\n    create_logger,\n    create_tensorboard_writer,\n    find_config_diff,\n    get_env_info,\n    get_rank,\n    save_config,\n    set_seed,\n    setup_cudnn,\n)\n\nglobal_step = 0\n\n\ndef load_config():\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--config', type=str)\n    parser.add_argument('--resume', type=str, default='')\n    parser.add_argument('--local_rank', type=int, default=0)\n    parser.add_argument('options', default=None, nargs=argparse.REMAINDER)\n    args = parser.parse_args()\n\n    config = get_default_config()\n    if args.config is not None:\n        config.merge_from_file(args.config)\n    config.merge_from_list(args.options)\n    if not torch.cuda.is_available():\n        config.device = 'cpu'\n        config.train.dataloader.pin_memory = False\n    if args.resume != '':\n        config_path = pathlib.Path(args.resume) / 'config.yaml'\n        config.merge_from_file(config_path.as_posix())\n        config.merge_from_list(['train.resume', True])\n    config.merge_from_list(['train.dist.local_rank', args.local_rank])\n    config = update_config(config)\n    config.freeze()\n    return config\n\n\ndef subdivide_batch(config, data, targets):\n    subdivision = config.train.subdivision\n\n    if subdivision == 1:\n        return [data], [targets]\n\n    data_chunks = data.chunk(subdivision)\n    if config.augmentation.use_mixup or config.augmentation.use_cutmix:\n        targets1, targets2, lam = targets\n        target_chunks = [(chunk1, chunk2, lam) for chunk1, chunk2 in zip(\n            targets1.chunk(subdivision), targets2.chunk(subdivision))]\n    elif config.augmentation.use_ricap:\n        target_list, weights = targets\n        target_list_chunks = list(\n            zip(*[target.chunk(subdivision) for target in target_list]))\n        target_chunks = [(chunk, weights) for chunk in target_list_chunks]\n    else:\n        target_chunks = targets.chunk(subdivision)\n    return data_chunks, target_chunks\n\n\ndef send_targets_to_device(config, targets, device):\n    if config.augmentation.use_mixup or config.augmentation.use_cutmix:\n        t1, t2, lam = targets\n        targets = (t1.to(device), t2.to(device), lam)\n    elif config.augmentation.use_ricap:\n        labels, weights = targets\n        labels = [label.to(device) for label in labels]\n        targets = (labels, weights)\n    else:\n        targets = targets.to(device)\n    return targets\n\n\ndef train(epoch, config, model, optimizer, scheduler, loss_func, train_loader,\n          logger, tensorboard_writer, tensorboard_writer2):\n    global global_step\n\n    logger.info(f'Train {epoch} {global_step}')\n\n    device = torch.device(config.device)\n\n    model.train()\n\n    loss_meter = AverageMeter()\n    acc1_meter = AverageMeter()\n    acc5_meter = AverageMeter()\n    start = time.time()\n    for step, (data, targets) in enumerate(train_loader):\n        step += 1\n        global_step += 1\n\n        if get_rank() == 0 and step == 1:\n            if config.tensorboard.train_images:\n                image = torchvision.utils.make_grid(data,\n                                                    normalize=True,\n                                                    scale_each=True)\n                tensorboard_writer.add_image('Train/Image', image, epoch)\n\n        data = data.to(device,\n                       non_blocking=config.train.dataloader.non_blocking)\n        targets = send_targets_to_device(config, targets, device)\n\n        data_chunks, target_chunks = subdivide_batch(config, data, targets)\n        optimizer.zero_grad()\n        outputs = []\n        losses = []\n        for data_chunk, target_chunk in zip(data_chunks, target_chunks):\n            if config.augmentation.use_dual_cutout:\n                w = data_chunk.size(3) // 2\n                data1 = data_chunk[:, :, :, :w]\n                data2 = data_chunk[:, :, :, w:]\n                outputs1 = model(data1)\n                outputs2 = model(data2)\n                output_chunk = torch.cat(\n                    (outputs1.unsqueeze(1), outputs2.unsqueeze(1)), dim=1)\n            else:\n                output_chunk = model(data_chunk)\n            outputs.append(output_chunk)\n\n            loss = loss_func(output_chunk, target_chunk)\n            losses.append(loss)\n            with apex.amp.scale_loss(loss, optimizer) as scaled_loss:\n                scaled_loss.backward()\n        outputs = torch.cat(outputs)\n\n        if config.train.gradient_clip > 0:\n            torch.nn.utils.clip_grad_norm_(apex.amp.master_params(optimizer),\n                                           config.train.gradient_clip)\n        if config.train.subdivision > 1:\n            for param in model.parameters():\n                param.grad.data.div_(config.train.subdivision)\n        optimizer.step()\n\n        acc1, acc5 = compute_accuracy(config,\n                                      outputs,\n                                      targets,\n                                      augmentation=True,\n                                      topk=(1, 5))\n\n        loss = sum(losses)\n        if config.train.distributed:\n            loss_all_reduce = dist.all_reduce(loss,\n                                              op=dist.ReduceOp.SUM,\n                                              async_op=True)\n            acc1_all_reduce = dist.all_reduce(acc1,\n                                              op=dist.ReduceOp.SUM,\n                                              async_op=True)\n            acc5_all_reduce = dist.all_reduce(acc5,\n                                              op=dist.ReduceOp.SUM,\n                                              async_op=True)\n            loss_all_reduce.wait()\n            acc1_all_reduce.wait()\n            acc5_all_reduce.wait()\n            loss.div_(dist.get_world_size())\n            acc1.div_(dist.get_world_size())\n            acc5.div_(dist.get_world_size())\n        loss = loss.item()\n        acc1 = acc1.item()\n        acc5 = acc5.item()\n\n        num = data.size(0)\n        loss_meter.update(loss, num)\n        acc1_meter.update(acc1, num)\n        acc5_meter.update(acc5, num)\n\n        torch.cuda.synchronize()\n\n        if get_rank() == 0:\n            if step % config.train.log_period == 0 or step == len(\n                    train_loader):\n                logger.info(\n                    f'Epoch {epoch} '\n                    f'Step {step}/{len(train_loader)} '\n                    f'lr {scheduler.get_last_lr()[0]:.6f} '\n                    f'loss {loss_meter.val:.4f} ({loss_meter.avg:.4f}) '\n                    f'acc@1 {acc1_meter.val:.4f} ({acc1_meter.avg:.4f}) '\n                    f'acc@5 {acc5_meter.val:.4f} ({acc5_meter.avg:.4f})')\n\n                tensorboard_writer2.add_scalar('Train/RunningLoss',\n                                               loss_meter.avg, global_step)\n                tensorboard_writer2.add_scalar('Train/RunningAcc1',\n                                               acc1_meter.avg, global_step)\n                tensorboard_writer2.add_scalar('Train/RunningAcc5',\n                                               acc5_meter.avg, global_step)\n                tensorboard_writer2.add_scalar('Train/RunningLearningRate',\n                                               scheduler.get_last_lr()[0],\n                                               global_step)\n\n        scheduler.step()\n\n    if get_rank() == 0:\n        elapsed = time.time() - start\n        logger.info(f'Elapsed {elapsed:.2f}')\n\n        tensorboard_writer.add_scalar('Train/Loss', loss_meter.avg, epoch)\n        tensorboard_writer.add_scalar('Train/Acc1', acc1_meter.avg, epoch)\n        tensorboard_writer.add_scalar('Train/Acc5', acc5_meter.avg, epoch)\n        tensorboard_writer.add_scalar('Train/Time', elapsed, epoch)\n        tensorboard_writer.add_scalar('Train/LearningRate',\n                                      scheduler.get_last_lr()[0], epoch)\n\n\ndef validate(epoch, config, model, loss_func, val_loader, logger,\n             tensorboard_writer):\n    logger.info(f'Val {epoch}')\n\n    device = torch.device(config.device)\n\n    model.eval()\n\n    loss_meter = AverageMeter()\n    acc1_meter = AverageMeter()\n    acc5_meter = AverageMeter()\n    start = time.time()\n    with torch.no_grad():\n        for step, (data, targets) in enumerate(val_loader):\n            if get_rank() == 0:\n                if config.tensorboard.val_images:\n                    if epoch == 0 and step == 0:\n                        image = torchvision.utils.make_grid(data,\n                                                            normalize=True,\n                                                            scale_each=True)\n                        tensorboard_writer.add_image('Val/Image', image, epoch)\n\n            data = data.to(\n                device, non_blocking=config.validation.dataloader.non_blocking)\n            targets = targets.to(device)\n\n            outputs = model(data)\n            loss = loss_func(outputs, targets)\n\n            acc1, acc5 = compute_accuracy(config,\n                                          outputs,\n                                          targets,\n                                          augmentation=False,\n                                          topk=(1, 5))\n\n            if config.train.distributed:\n                loss_all_reduce = dist.all_reduce(loss,\n                                                  op=dist.ReduceOp.SUM,\n                                                  async_op=True)\n                acc1_all_reduce = dist.all_reduce(acc1,\n                                                  op=dist.ReduceOp.SUM,\n                                                  async_op=True)\n                acc5_all_reduce = dist.all_reduce(acc5,\n                                                  op=dist.ReduceOp.SUM,\n                                                  async_op=True)\n                loss_all_reduce.wait()\n                acc1_all_reduce.wait()\n                acc5_all_reduce.wait()\n                loss.div_(dist.get_world_size())\n                acc1.div_(dist.get_world_size())\n                acc5.div_(dist.get_world_size())\n            loss = loss.item()\n            acc1 = acc1.item()\n            acc5 = acc5.item()\n\n            num = data.size(0)\n            loss_meter.update(loss, num)\n            acc1_meter.update(acc1, num)\n            acc5_meter.update(acc5, num)\n\n            torch.cuda.synchronize()\n\n        logger.info(f'Epoch {epoch} '\n                    f'loss {loss_meter.avg:.4f} '\n                    f'acc@1 {acc1_meter.avg:.4f} '\n                    f'acc@5 {acc5_meter.avg:.4f}')\n\n        elapsed = time.time() - start\n        logger.info(f'Elapsed {elapsed:.2f}')\n\n    if get_rank() == 0:\n        if epoch > 0:\n            tensorboard_writer.add_scalar('Val/Loss', loss_meter.avg, epoch)\n        tensorboard_writer.add_scalar('Val/Acc1', acc1_meter.avg, epoch)\n        tensorboard_writer.add_scalar('Val/Acc5', acc5_meter.avg, epoch)\n        tensorboard_writer.add_scalar('Val/Time', elapsed, epoch)\n        if config.tensorboard.model_params:\n            for name, param in model.named_parameters():\n                tensorboard_writer.add_histogram(name, param, epoch)\n\n\ndef main():\n    global global_step\n\n    config = load_config()\n\n    set_seed(config)\n    setup_cudnn(config)\n\n    epoch_seeds = np.random.randint(np.iinfo(np.int32).max // 2,\n                                    size=config.scheduler.epochs)\n\n    if config.train.distributed:\n        dist.init_process_group(backend=config.train.dist.backend,\n                                init_method=config.train.dist.init_method,\n                                rank=config.train.dist.node_rank,\n                                world_size=config.train.dist.world_size)\n        torch.cuda.set_device(config.train.dist.local_rank)\n\n    output_dir = pathlib.Path(config.train.output_dir)\n    if get_rank() == 0:\n        if not config.train.resume and output_dir.exists():\n            raise RuntimeError(\n                f'Output directory `{output_dir.as_posix()}` already exists')\n        output_dir.mkdir(exist_ok=True, parents=True)\n        if not config.train.resume:\n            save_config(config, output_dir / 'config.yaml')\n            save_config(get_env_info(config), output_dir / 'env.yaml')\n            diff = find_config_diff(config)\n            if diff is not None:\n                save_config(diff, output_dir / 'config_min.yaml')\n\n    logger = create_logger(name=__name__,\n                           distributed_rank=get_rank(),\n                           output_dir=output_dir,\n                           filename='log.txt')\n    logger.info(config)\n    logger.info(get_env_info(config))\n\n    train_loader, val_loader = create_dataloader(config, is_train=True)\n\n    model = create_model(config)\n    macs, n_params = count_op(config, model)\n    logger.info(f'MACs   : {macs}')\n    logger.info(f'#params: {n_params}')\n\n    optimizer = create_optimizer(config, model)\n    model, optimizer = apex.amp.initialize(model,\n                                           optimizer,\n                                           opt_level=config.train.precision)\n    model = apply_data_parallel_wrapper(config, model)\n\n    scheduler = create_scheduler(config,\n                                 optimizer,\n                                 steps_per_epoch=len(train_loader))\n    checkpointer = Checkpointer(model,\n                                optimizer=optimizer,\n                                scheduler=scheduler,\n                                save_dir=output_dir,\n                                save_to_disk=get_rank() == 0)\n\n    start_epoch = config.train.start_epoch\n    scheduler.last_epoch = start_epoch\n    if config.train.resume:\n        checkpoint_config = checkpointer.resume_or_load('', resume=True)\n        global_step = checkpoint_config['global_step']\n        start_epoch = checkpoint_config['epoch']\n        config.defrost()\n        config.merge_from_other_cfg(ConfigNode(checkpoint_config['config']))\n        config.freeze()\n    elif config.train.checkpoint != '':\n        checkpoint = torch.load(config.train.checkpoint, map_location='cpu')\n        if isinstance(model,\n                      (nn.DataParallel, nn.parallel.DistributedDataParallel)):\n            model.module.load_state_dict(checkpoint['model'])\n        else:\n            model.load_state_dict(checkpoint['model'])\n\n    if get_rank() == 0 and config.train.use_tensorboard:\n        tensorboard_writer = create_tensorboard_writer(\n            config, output_dir, purge_step=config.train.start_epoch + 1)\n        tensorboard_writer2 = create_tensorboard_writer(\n            config, output_dir / 'running', purge_step=global_step + 1)\n    else:\n        tensorboard_writer = DummyWriter()\n        tensorboard_writer2 = DummyWriter()\n\n    train_loss, val_loss = create_loss(config)\n\n    if (config.train.val_period > 0 and start_epoch == 0\n            and config.train.val_first):\n        validate(0, config, model, val_loss, val_loader, logger,\n                 tensorboard_writer)\n\n    for epoch, seed in enumerate(epoch_seeds[start_epoch:], start_epoch):\n        epoch += 1\n\n        np.random.seed(seed)\n        train(epoch, config, model, optimizer, scheduler, train_loss,\n              train_loader, logger, tensorboard_writer, tensorboard_writer2)\n\n        if config.train.val_period > 0 and (epoch %\n                                            config.train.val_period == 0):\n            validate(epoch, config, model, val_loss, val_loader, logger,\n                     tensorboard_writer)\n\n        tensorboard_writer.flush()\n        tensorboard_writer2.flush()\n\n        if (epoch % config.train.checkpoint_period == 0) or (\n                epoch == config.scheduler.epochs):\n            checkpoint_config = {\n                'epoch': epoch,\n                'global_step': global_step,\n                'config': config.as_dict(),\n            }\n            checkpointer.save(f'checkpoint_{epoch:05d}', **checkpoint_config)\n\n    tensorboard_writer.close()\n    tensorboard_writer2.close()\n\n\nif __name__ == '__main__':\n    main()\n"""
pytorch_image_classification/__init__.py,0,"b'from .config import get_default_config, update_config\nfrom .collators import create_collator\nfrom .transforms import create_transform\nfrom .datasets import create_dataset, create_dataloader\nfrom .models import apply_data_parallel_wrapper, create_model\nfrom .losses import create_loss\nfrom .optim import create_optimizer\nfrom .scheduler import create_scheduler\n\nimport pytorch_image_classification.utils\n'"
scripts/extract_images.py,0,"b""#!/usr/bin/env python\n\nimport argparse\nimport pathlib\nimport numpy as np\nimport cv2\nfrom tensorboard.backend.event_processing import event_accumulator\n\n\ndef main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--path', type=str, required=True)\n    parser.add_argument('--output-dir', '-o', type=str)\n    args = parser.parse_args()\n\n    event_acc = event_accumulator.EventAccumulator(args.path,\n                                                   size_guidance={'images': 0})\n    event_acc.Reload()\n\n    if args.output_dir is not None:\n        output_dir = pathlib.Path(args.output_dir)\n    else:\n        output_dir = pathlib.Path(args.path).parent / 'images'\n    output_dir.mkdir(exist_ok=True, parents=True)\n\n    for tag in event_acc.Tags()['images']:\n        events = event_acc.Images(tag)\n\n        tag_name = tag.replace('/', '_')\n        dirpath = output_dir / tag_name\n        dirpath.mkdir(exist_ok=True, parents=True)\n\n        for index, event in enumerate(events):\n            s = np.frombuffer(event.encoded_image_string, dtype=np.uint8)\n            image = cv2.imdecode(s, cv2.IMREAD_COLOR)\n            outpath = dirpath / f'{index:04}.jpg'\n            cv2.imwrite(outpath.as_posix(), image)\n\n\nif __name__ == '__main__':\n    main()\n"""
scripts/extract_scalars.py,0,"b""#!/usr/bin/env python\n\nimport argparse\nimport json\nimport pathlib\nfrom tensorboard.backend.event_processing import event_accumulator\n\n\ndef main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--path', type=str, required=True)\n    parser.add_argument('--output-dir', '-o', type=str)\n    args = parser.parse_args()\n\n    event_acc = event_accumulator.EventAccumulator(\n        args.path, size_guidance={'scalars': 0})\n    event_acc.Reload()\n\n    scalars = {}\n    for tag in event_acc.Tags()['scalars']:\n        events = event_acc.Scalars(tag)\n        scalars[tag] = [event.value for event in events]\n\n    if args.output_dir is not None:\n        output_dir = pathlib.Path(args.output_dir)\n    else:\n        output_dir = pathlib.Path(args.path).parent\n    output_dir.mkdir(exist_ok=True, parents=True)\n\n    outpath = output_dir / 'all_scalars.json'\n    with open(outpath, 'w') as fout:\n        json.dump(scalars, fout)\n\n\nif __name__ == '__main__':\n    main()\n"""
pytorch_image_classification/collators/__init__.py,1,b'from typing import Callable\n\nimport torch\nimport yacs.config\n\nfrom .cutmix import CutMixCollator\nfrom .mixup import MixupCollator\nfrom .ricap import RICAPCollator\n\n\ndef create_collator(config: yacs.config.CfgNode) -> Callable:\n    if config.augmentation.use_mixup:\n        return MixupCollator(config)\n    elif config.augmentation.use_ricap:\n        return RICAPCollator(config)\n    elif config.augmentation.use_cutmix:\n        return CutMixCollator(config)\n    else:\n        return torch.utils.data.dataloader.default_collate\n'
pytorch_image_classification/collators/cutmix.py,6,"b'from typing import List, Tuple\n\nimport numpy as np\nimport torch\nimport yacs.config\n\n\ndef cutmix(\n    batch: Tuple[torch.Tensor, torch.Tensor], alpha: float\n) -> Tuple[torch.Tensor, Tuple[torch.Tensor, torch.Tensor, float]]:\n    data, targets = batch\n\n    indices = torch.randperm(data.size(0))\n    shuffled_data = data[indices]\n    shuffled_targets = targets[indices]\n\n    lam = np.random.beta(alpha, alpha)\n\n    image_h, image_w = data.shape[2:]\n    cx = np.random.uniform(0, image_w)\n    cy = np.random.uniform(0, image_h)\n    w = image_w * np.sqrt(1 - lam)\n    h = image_h * np.sqrt(1 - lam)\n    x0 = int(np.round(max(cx - w / 2, 0)))\n    x1 = int(np.round(min(cx + w / 2, image_w)))\n    y0 = int(np.round(max(cy - h / 2, 0)))\n    y1 = int(np.round(min(cy + h / 2, image_h)))\n\n    data[:, :, y0:y1, x0:x1] = shuffled_data[:, :, y0:y1, x0:x1]\n    targets = (targets, shuffled_targets, lam)\n\n    return data, targets\n\n\nclass CutMixCollator:\n    def __init__(self, config: yacs.config.CfgNode):\n        self.alpha = config.augmentation.cutmix.alpha\n\n    def __call__(\n        self, batch: List[Tuple[torch.Tensor, int]]\n    ) -> Tuple[torch.Tensor, Tuple[torch.Tensor, torch.Tensor, float]]:\n        batch = torch.utils.data.dataloader.default_collate(batch)\n        batch = cutmix(batch, self.alpha)\n        return batch\n'"
pytorch_image_classification/collators/mixup.py,6,"b'from typing import List, Tuple\n\nimport numpy as np\nimport torch\nimport yacs.config\n\n\ndef mixup(\n    batch: Tuple[torch.Tensor, torch.Tensor], alpha: float\n) -> Tuple[torch.Tensor, Tuple[torch.Tensor, torch.Tensor, float]]:\n    data, targets = batch\n    indices = torch.randperm(data.size(0))\n    shuffled_data = data[indices]\n    shuffled_targets = targets[indices]\n\n    lam = np.random.beta(alpha, alpha)\n    data = data * lam + shuffled_data * (1 - lam)\n    targets = (targets, shuffled_targets, lam)\n\n    return data, targets\n\n\nclass MixupCollator:\n    def __init__(self, config: yacs.config.CfgNode):\n        self.alpha = config.augmentation.mixup.alpha\n\n    def __call__(\n        self, batch: List[Tuple[torch.Tensor, int]]\n    ) -> Tuple[torch.Tensor, Tuple[torch.Tensor, torch.Tensor, float]]:\n        batch = torch.utils.data.dataloader.default_collate(batch)\n        batch = mixup(batch, self.alpha)\n        return batch\n'"
pytorch_image_classification/collators/ricap.py,9,"b'from typing import List, Tuple\n\nimport numpy as np\nimport torch\nimport yacs.config\n\n\ndef ricap(\n    batch: Tuple[torch.Tensor, torch.Tensor], beta: float\n) -> Tuple[torch.Tensor, Tuple[List[torch.Tensor], List[float]]]:\n    data, targets = batch\n    image_h, image_w = data.shape[2:]\n    ratio = np.random.beta(beta, beta, size=2)\n    w0, h0 = np.round(np.array([image_w, image_h]) * ratio).astype(np.int)\n    w1, h1 = image_w - w0, image_h - h0\n    ws = [w0, w1, w0, w1]\n    hs = [h0, h0, h1, h1]\n\n    patches = []\n    labels = []\n    label_weights = []\n    for w, h in zip(ws, hs):\n        indices = torch.randperm(data.size(0))\n        x0 = np.random.randint(0, image_w - w + 1)\n        y0 = np.random.randint(0, image_h - h + 1)\n        patches.append(data[indices, :, y0:y0 + h, x0:x0 + w])\n        labels.append(targets[indices])\n        label_weights.append(h * w / (image_h * image_w))\n\n    data = torch.cat(\n        [torch.cat(patches[:2], dim=3),\n         torch.cat(patches[2:], dim=3)], dim=2)\n    targets = (labels, label_weights)\n\n    return data, targets\n\n\nclass RICAPCollator:\n    def __init__(self, config: yacs.config.CfgNode):\n        self.beta = config.augmentation.ricap.beta\n\n    def __call__(\n        self, batch: List[Tuple[torch.Tensor, int]]\n    ) -> Tuple[torch.Tensor, Tuple[List[torch.Tensor], List[float]]]:\n        batch = torch.utils.data.dataloader.default_collate(batch)\n        batch = ricap(batch, self.beta)\n        return batch\n'"
pytorch_image_classification/config/__init__.py,1,"b""import torch\n\nfrom .defaults import get_default_config\n\n\ndef update_config(config):\n    if config.dataset.name in ['CIFAR10', 'CIFAR100']:\n        dataset_dir = f'~/.torch/datasets/{config.dataset.name}'\n        config.dataset.dataset_dir = dataset_dir\n        config.dataset.image_size = 32\n        config.dataset.n_channels = 3\n        config.dataset.n_classes = int(config.dataset.name[5:])\n    elif config.dataset.name in ['MNIST', 'FashionMNIST', 'KMNIST']:\n        dataset_dir = '~/.torch/datasets'\n        config.dataset.dataset_dir = dataset_dir\n        config.dataset.image_size = 28\n        config.dataset.n_channels = 1\n        config.dataset.n_classes = 10\n\n    if not torch.cuda.is_available():\n        config.device = 'cpu'\n\n    return config\n"""
pytorch_image_classification/config/config_node.py,0,"b""import yacs.config\n\n\nclass ConfigNode(yacs.config.CfgNode):\n    def __init__(self, init_dict=None, key_list=None, new_allowed=False):\n        super().__init__(init_dict, key_list, new_allowed)\n\n    def __str__(self):\n        def _indent(s_, num_spaces):\n            s = s_.split('\\n')\n            if len(s) == 1:\n                return s_\n            first = s.pop(0)\n            s = [(num_spaces * ' ') + line for line in s]\n            s = '\\n'.join(s)\n            s = first + '\\n' + s\n            return s\n\n        r = ''\n        s = []\n        for k, v in self.items():\n            separator = '\\n' if isinstance(v, ConfigNode) else ' '\n            if isinstance(v, str) and not v:\n                v = '\\'\\''\n            attr_str = f'{str(k)}:{separator}{str(v)}'\n            attr_str = _indent(attr_str, 2)\n            s.append(attr_str)\n        r += '\\n'.join(s)\n        return r\n\n    def as_dict(self):\n        def convert_to_dict(node):\n            if not isinstance(node, ConfigNode):\n                return node\n            else:\n                dic = dict()\n                for k, v in node.items():\n                    dic[k] = convert_to_dict(v)\n                return dic\n\n        return convert_to_dict(self)\n"""
pytorch_image_classification/config/defaults.py,0,"b""from .config_node import ConfigNode\n\nconfig = ConfigNode()\n\nconfig.device = 'cuda'\n# cuDNN\nconfig.cudnn = ConfigNode()\nconfig.cudnn.benchmark = True\nconfig.cudnn.deterministic = False\n\nconfig.dataset = ConfigNode()\nconfig.dataset.name = 'CIFAR10'\nconfig.dataset.dataset_dir = ''\nconfig.dataset.image_size = 32\nconfig.dataset.n_channels = 3\nconfig.dataset.n_classes = 10\n\nconfig.model = ConfigNode()\n# options: 'cifar', 'imagenet'\n# Use 'cifar' for small input images\nconfig.model.type = 'cifar'\nconfig.model.name = 'resnet_preact'\nconfig.model.init_mode = 'kaiming_fan_out'\n\nconfig.model.vgg = ConfigNode()\nconfig.model.vgg.n_channels = [64, 128, 256, 512, 512]\nconfig.model.vgg.n_layers = [2, 2, 3, 3, 3]\nconfig.model.vgg.use_bn = True\n\nconfig.model.resnet = ConfigNode()\nconfig.model.resnet.depth = 110  # for cifar type model\nconfig.model.resnet.n_blocks = [2, 2, 2, 2]  # for imagenet type model\nconfig.model.resnet.block_type = 'basic'\nconfig.model.resnet.initial_channels = 16\n\nconfig.model.resnet_preact = ConfigNode()\nconfig.model.resnet_preact.depth = 110  # for cifar type model\nconfig.model.resnet_preact.n_blocks = [2, 2, 2, 2]  # for imagenet type model\nconfig.model.resnet_preact.block_type = 'basic'\nconfig.model.resnet_preact.initial_channels = 16\nconfig.model.resnet_preact.remove_first_relu = False\nconfig.model.resnet_preact.add_last_bn = False\nconfig.model.resnet_preact.preact_stage = [True, True, True]\n\nconfig.model.wrn = ConfigNode()\nconfig.model.wrn.depth = 28  # for cifar type model\nconfig.model.wrn.initial_channels = 16\nconfig.model.wrn.widening_factor = 10\nconfig.model.wrn.drop_rate = 0.0\n\nconfig.model.densenet = ConfigNode()\nconfig.model.densenet.depth = 100  # for cifar type model\nconfig.model.densenet.n_blocks = [6, 12, 24, 16]  # for imagenet type model\nconfig.model.densenet.block_type = 'bottleneck'\nconfig.model.densenet.growth_rate = 12\nconfig.model.densenet.drop_rate = 0.0\nconfig.model.densenet.compression_rate = 0.5\n\nconfig.model.pyramidnet = ConfigNode()\nconfig.model.pyramidnet.depth = 272  # for cifar type model\nconfig.model.pyramidnet.n_blocks = [3, 24, 36, 3]  # for imagenet type model\nconfig.model.pyramidnet.initial_channels = 16\nconfig.model.pyramidnet.block_type = 'bottleneck'\nconfig.model.pyramidnet.alpha = 200\n\nconfig.model.resnext = ConfigNode()\nconfig.model.resnext.depth = 29  # for cifar type model\nconfig.model.resnext.n_blocks = [3, 4, 6, 3]  # for imagenet type model\nconfig.model.resnext.initial_channels = 64\nconfig.model.resnext.cardinality = 8\nconfig.model.resnext.base_channels = 4\n\nconfig.model.shake_shake = ConfigNode()\nconfig.model.shake_shake.depth = 26  # for cifar type model\nconfig.model.shake_shake.initial_channels = 96\nconfig.model.shake_shake.shake_forward = True\nconfig.model.shake_shake.shake_backward = True\nconfig.model.shake_shake.shake_image = True\n\nconfig.model.se_resnet_preact = ConfigNode()\nconfig.model.se_resnet_preact.depth = 110  # for cifar type model\nconfig.model.se_resnet_preact.initial_channels = 16\nconfig.model.se_resnet_preact.se_reduction = 16\nconfig.model.se_resnet_preact.block_type = 'basic'\nconfig.model.se_resnet_preact.initial_channels = 16\nconfig.model.se_resnet_preact.remove_first_relu = False\nconfig.model.se_resnet_preact.add_last_bn = False\nconfig.model.se_resnet_preact.preact_stage = [True, True, True]\n\nconfig.train = ConfigNode()\nconfig.train.checkpoint = ''\nconfig.train.resume = False\n# optimization level for NVIDIA apex\n# O0 = fp32\n# O1 = mixed precision\n# O2 = almost fp16\n# O3 = fp16\nconfig.train.precision = 'O0'\nconfig.train.batch_size = 128\nconfig.train.subdivision = 1\n# optimizer (options: sgd, adam, lars, adabound, adaboundw)\nconfig.train.optimizer = 'sgd'\nconfig.train.base_lr = 0.1\nconfig.train.momentum = 0.9\nconfig.train.nesterov = True\nconfig.train.weight_decay = 1e-4\nconfig.train.no_weight_decay_on_bn = False\nconfig.train.gradient_clip = 0\nconfig.train.start_epoch = 0\nconfig.train.seed = 0\nconfig.train.val_first = True\nconfig.train.val_period = 1\nconfig.train.val_ratio = 0.0\nconfig.train.use_test_as_val = True\n\nconfig.train.output_dir = 'experiments/exp00'\nconfig.train.log_period = 100\nconfig.train.checkpoint_period = 10\n\nconfig.train.use_tensorboard = True\nconfig.tensorboard = ConfigNode()\nconfig.tensorboard.train_images = False\nconfig.tensorboard.val_images = False\nconfig.tensorboard.model_params = False\n\n# optimizer\nconfig.optim = ConfigNode()\n# Adam\nconfig.optim.adam = ConfigNode()\nconfig.optim.adam.betas = (0.9, 0.999)\n# LARS\nconfig.optim.lars = ConfigNode()\nconfig.optim.lars.eps = 1e-9\nconfig.optim.lars.threshold = 1e-2\n# AdaBound\nconfig.optim.adabound = ConfigNode()\nconfig.optim.adabound.betas = (0.9, 0.999)\nconfig.optim.adabound.final_lr = 0.1\nconfig.optim.adabound.gamma = 1e-3\n\n# scheduler\nconfig.scheduler = ConfigNode()\nconfig.scheduler.epochs = 160\n# warm up (options: none, linear, exponential)\nconfig.scheduler.warmup = ConfigNode()\nconfig.scheduler.warmup.type = 'none'\nconfig.scheduler.warmup.epochs = 0\nconfig.scheduler.warmup.start_factor = 1e-3\nconfig.scheduler.warmup.exponent = 4\n# main scheduler (options: constant, linear, multistep, cosine, sgdr)\nconfig.scheduler.type = 'multistep'\nconfig.scheduler.milestones = [80, 120]\nconfig.scheduler.lr_decay = 0.1\nconfig.scheduler.lr_min_factor = 0.001\nconfig.scheduler.T0 = 10\nconfig.scheduler.T_mul = 1.\n\n# train data loader\nconfig.train.dataloader = ConfigNode()\nconfig.train.dataloader.num_workers = 2\nconfig.train.dataloader.drop_last = True\nconfig.train.dataloader.pin_memory = False\nconfig.train.dataloader.non_blocking = False\n\n# validation data loader\nconfig.validation = ConfigNode()\nconfig.validation.batch_size = 256\nconfig.validation.dataloader = ConfigNode()\nconfig.validation.dataloader.num_workers = 2\nconfig.validation.dataloader.drop_last = False\nconfig.validation.dataloader.pin_memory = False\nconfig.validation.dataloader.non_blocking = False\n\n# distributed\nconfig.train.distributed = False\nconfig.train.dist = ConfigNode()\nconfig.train.dist.backend = 'nccl'\nconfig.train.dist.init_method = 'env://'\nconfig.train.dist.world_size = -1\nconfig.train.dist.node_rank = -1\nconfig.train.dist.local_rank = 0\nconfig.train.dist.use_sync_bn = False\n\nconfig.augmentation = ConfigNode()\nconfig.augmentation.use_random_crop = True\nconfig.augmentation.use_random_horizontal_flip = True\nconfig.augmentation.use_cutout = False\nconfig.augmentation.use_random_erasing = False\nconfig.augmentation.use_dual_cutout = False\nconfig.augmentation.use_mixup = False\nconfig.augmentation.use_ricap = False\nconfig.augmentation.use_cutmix = False\nconfig.augmentation.use_label_smoothing = False\n\nconfig.augmentation.random_crop = ConfigNode()\nconfig.augmentation.random_crop.padding = 4\nconfig.augmentation.random_crop.fill = 0\nconfig.augmentation.random_crop.padding_mode = 'constant'\n\nconfig.augmentation.random_horizontal_flip = ConfigNode()\nconfig.augmentation.random_horizontal_flip.prob = 0.5\n\nconfig.augmentation.cutout = ConfigNode()\nconfig.augmentation.cutout.prob = 1.0\nconfig.augmentation.cutout.mask_size = 16\nconfig.augmentation.cutout.cut_inside = False\nconfig.augmentation.cutout.mask_color = 0\nconfig.augmentation.cutout.dual_cutout_alpha = 0.1\n\nconfig.augmentation.random_erasing = ConfigNode()\nconfig.augmentation.random_erasing.prob = 0.5\nconfig.augmentation.random_erasing.area_ratio_range = [0.02, 0.4]\nconfig.augmentation.random_erasing.min_aspect_ratio = 0.3\nconfig.augmentation.random_erasing.max_attempt = 20\n\nconfig.augmentation.mixup = ConfigNode()\nconfig.augmentation.mixup.alpha = 1.0\n\nconfig.augmentation.ricap = ConfigNode()\nconfig.augmentation.ricap.beta = 0.3\n\nconfig.augmentation.cutmix = ConfigNode()\nconfig.augmentation.cutmix.alpha = 1.0\n\nconfig.augmentation.label_smoothing = ConfigNode()\nconfig.augmentation.label_smoothing.epsilon = 0.1\n\nconfig.tta = ConfigNode()\nconfig.tta.use_resize = False\nconfig.tta.use_center_crop = False\nconfig.tta.resize = 256\n\n# test config\nconfig.test = ConfigNode()\nconfig.test.checkpoint = ''\nconfig.test.output_dir = ''\nconfig.test.batch_size = 256\n# test data loader\nconfig.test.dataloader = ConfigNode()\nconfig.test.dataloader.num_workers = 2\nconfig.test.dataloader.pin_memory = False\n\n\ndef get_default_config():\n    return config.clone()\n"""
pytorch_image_classification/datasets/__init__.py,0,b'from .datasets import create_dataset\nfrom .dataloader import create_dataloader\n'
pytorch_image_classification/datasets/dataloader.py,12,"b'from typing import Tuple, Union\n\nimport numpy as np\nimport torch\nimport torch.distributed as dist\nimport yacs.config\n\nfrom torch.utils.data import DataLoader\n\nfrom pytorch_image_classification import create_collator\nfrom pytorch_image_classification.datasets import create_dataset\n\n\ndef worker_init_fn(worker_id: int) -> None:\n    np.random.seed(np.random.get_state()[1][0] + worker_id)\n\n\ndef create_dataloader(\n        config: yacs.config.CfgNode,\n        is_train: bool) -> Union[Tuple[DataLoader, DataLoader], DataLoader]:\n    if is_train:\n        train_dataset, val_dataset = create_dataset(config, is_train)\n\n        if dist.is_available() and dist.is_initialized():\n            train_sampler = torch.utils.data.distributed.DistributedSampler(\n                train_dataset)\n            val_sampler = torch.utils.data.distributed.DistributedSampler(\n                val_dataset)\n        else:\n            train_sampler = torch.utils.data.sampler.RandomSampler(\n                train_dataset, replacement=False)\n            val_sampler = torch.utils.data.sampler.SequentialSampler(\n                val_dataset)\n\n        train_collator = create_collator(config)\n\n        train_batch_sampler = torch.utils.data.sampler.BatchSampler(\n            train_sampler,\n            batch_size=config.train.batch_size,\n            drop_last=config.train.dataloader.drop_last)\n        train_loader = torch.utils.data.DataLoader(\n            train_dataset,\n            batch_sampler=train_batch_sampler,\n            num_workers=config.train.dataloader.num_workers,\n            collate_fn=train_collator,\n            pin_memory=config.train.dataloader.pin_memory,\n            worker_init_fn=worker_init_fn)\n\n        val_batch_sampler = torch.utils.data.sampler.BatchSampler(\n            val_sampler,\n            batch_size=config.validation.batch_size,\n            drop_last=config.validation.dataloader.drop_last)\n        val_loader = torch.utils.data.DataLoader(\n            val_dataset,\n            batch_sampler=val_batch_sampler,\n            num_workers=config.validation.dataloader.num_workers,\n            pin_memory=config.validation.dataloader.pin_memory,\n            worker_init_fn=worker_init_fn)\n\n        return train_loader, val_loader\n    else:\n        dataset = create_dataset(config, is_train)\n        if dist.is_available() and dist.is_initialized():\n            sampler = torch.utils.data.distributed.DistributedSampler(dataset)\n        else:\n            sampler = None\n        test_loader = torch.utils.data.DataLoader(\n            dataset,\n            batch_size=config.test.batch_size,\n            num_workers=config.test.dataloader.num_workers,\n            sampler=sampler,\n            shuffle=False,\n            drop_last=False,\n            pin_memory=config.test.dataloader.pin_memory)\n        return test_loader\n'"
pytorch_image_classification/datasets/datasets.py,2,"b""from typing import Tuple, Union\n\nimport pathlib\n\nimport torch\nimport torchvision\nimport yacs.config\n\nfrom torch.utils.data import Dataset\n\nfrom pytorch_image_classification import create_transform\n\n\nclass SubsetDataset(Dataset):\n    def __init__(self, subset_dataset, transform=None):\n        self.subset_dataset = subset_dataset\n        self.transform = transform\n\n    def __getitem__(self, index):\n        x, y = self.subset_dataset[index]\n        if self.transform:\n            x = self.transform(x)\n        return x, y\n\n    def __len__(self):\n        return len(self.subset_dataset)\n\n\ndef create_dataset(config: yacs.config.CfgNode,\n                   is_train: bool) -> Union[Tuple[Dataset, Dataset], Dataset]:\n    if config.dataset.name in [\n            'CIFAR10',\n            'CIFAR100',\n            'MNIST',\n            'FashionMNIST',\n            'KMNIST',\n    ]:\n        module = getattr(torchvision.datasets, config.dataset.name)\n        if is_train:\n            if config.train.use_test_as_val:\n                train_transform = create_transform(config, is_train=True)\n                val_transform = create_transform(config, is_train=False)\n                train_dataset = module(config.dataset.dataset_dir,\n                                       train=is_train,\n                                       transform=train_transform,\n                                       download=True)\n                test_dataset = module(config.dataset.dataset_dir,\n                                      train=False,\n                                      transform=val_transform,\n                                      download=True)\n                return train_dataset, test_dataset\n            else:\n                dataset = module(config.dataset.dataset_dir,\n                                 train=is_train,\n                                 transform=None,\n                                 download=True)\n                val_ratio = config.train.val_ratio\n                assert val_ratio < 1\n                val_num = int(len(dataset) * val_ratio)\n                train_num = len(dataset) - val_num\n                lengths = [train_num, val_num]\n                train_subset, val_subset = torch.utils.data.dataset.random_split(\n                    dataset, lengths)\n\n                train_transform = create_transform(config, is_train=True)\n                val_transform = create_transform(config, is_train=False)\n                train_dataset = SubsetDataset(train_subset, train_transform)\n                val_dataset = SubsetDataset(val_subset, val_transform)\n                return train_dataset, val_dataset\n        else:\n            transform = create_transform(config, is_train=False)\n            dataset = module(config.dataset.dataset_dir,\n                             train=is_train,\n                             transform=transform,\n                             download=True)\n            return dataset\n    elif config.dataset.name == 'ImageNet':\n        dataset_dir = pathlib.Path(config.dataset.dataset_dir).expanduser()\n        train_transform = create_transform(config, is_train=True)\n        val_transform = create_transform(config, is_train=False)\n        train_dataset = torchvision.datasets.ImageFolder(\n            dataset_dir / 'train', transform=train_transform)\n        val_dataset = torchvision.datasets.ImageFolder(dataset_dir / 'val',\n                                                       transform=val_transform)\n        return train_dataset, val_dataset\n    else:\n        raise ValueError()\n"""
pytorch_image_classification/losses/__init__.py,1,"b""from typing import Callable, Tuple\n\nimport torch.nn as nn\nimport yacs.config\n\nfrom .cutmix import CutMixLoss\nfrom .mixup import MixupLoss\nfrom .ricap import RICAPLoss\nfrom .dual_cutout import DualCutoutLoss\nfrom .label_smoothing import LabelSmoothingLoss\n\n\ndef create_loss(config: yacs.config.CfgNode) -> Tuple[Callable, Callable]:\n    if config.augmentation.use_mixup:\n        train_loss = MixupLoss(reduction='mean')\n    elif config.augmentation.use_ricap:\n        train_loss = RICAPLoss(reduction='mean')\n    elif config.augmentation.use_cutmix:\n        train_loss = CutMixLoss(reduction='mean')\n    elif config.augmentation.use_label_smoothing:\n        train_loss = LabelSmoothingLoss(config, reduction='mean')\n    elif config.augmentation.use_dual_cutout:\n        train_loss = DualCutoutLoss(config, reduction='mean')\n    else:\n        train_loss = nn.CrossEntropyLoss(reduction='mean')\n    val_loss = nn.CrossEntropyLoss(reduction='mean')\n    return train_loss, val_loss\n"""
pytorch_image_classification/losses/cutmix.py,3,"b'from typing import Tuple\n\nimport torch\nimport torch.nn as nn\n\n\nclass CutMixLoss:\n    def __init__(self, reduction: str):\n        self.criterion = nn.CrossEntropyLoss(reduction=reduction)\n\n    def __call__(\n            self, predictions: torch.Tensor,\n            targets: Tuple[torch.Tensor, torch.Tensor, float]) -> torch.Tensor:\n        targets1, targets2, lam = targets\n        return lam * self.criterion(predictions, targets1) + (\n            1 - lam) * self.criterion(predictions, targets2)\n'"
pytorch_image_classification/losses/dual_cutout.py,4,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport yacs.config\n\n\nclass DualCutoutLoss:\n    def __init__(self, config: yacs.config.CfgNode, reduction: str):\n        self.alpha = config.augmentation.cutout.dual_cutout_alpha\n        self.loss_func = nn.CrossEntropyLoss(reduction=reduction)\n\n    def __call__(self, predictions: torch.Tensor,\n                 targets: torch.Tensor) -> torch.Tensor:\n        predictions1, predictions2 = predictions[:, 0], predictions[:, 1]\n        return (self.loss_func(predictions1, targets) + self.loss_func(\n            predictions2, targets)) * 0.5 + self.alpha * F.mse_loss(\n                predictions1, predictions2)\n'"
pytorch_image_classification/losses/label_smoothing.py,9,"b""import torch\nimport torch.nn.functional as F\nimport yacs.config\n\n\ndef onehot_encoding(label: torch.Tensor, n_classes: int) -> torch.Tensor:\n    return torch.zeros(label.size(0), n_classes).to(label.device).scatter_(\n        1, label.view(-1, 1), 1)\n\n\ndef cross_entropy_loss(data: torch.Tensor, target: torch.Tensor,\n                       reduction: str) -> torch.Tensor:\n    logp = F.log_softmax(data, dim=1)\n    loss = torch.sum(-logp * target, dim=1)\n    if reduction == 'none':\n        return loss\n    elif reduction == 'mean':\n        return loss.mean()\n    elif reduction == 'sum':\n        return loss.sum()\n    else:\n        raise ValueError(\n            '`reduction` must be one of \\'none\\', \\'mean\\', or \\'sum\\'.')\n\n\nclass LabelSmoothingLoss:\n    def __init__(self, config: yacs.config.CfgNode, reduction: str):\n        self.n_classes = config.dataset.n_classes\n        self.epsilon = config.augmentation.label_smoothing.epsilon\n        self.reduction = reduction\n\n    def __call__(self, predictions: torch.Tensor,\n                 targets: torch.Tensor) -> torch.Tensor:\n        device = predictions.device\n\n        onehot = onehot_encoding(\n            targets, self.n_classes).type_as(predictions).to(device)\n        targets = onehot * (1 - self.epsilon) + torch.ones_like(onehot).to(\n            device) * self.epsilon / self.n_classes\n        loss = cross_entropy_loss(predictions, targets, self.reduction)\n        return loss\n"""
pytorch_image_classification/losses/mixup.py,3,"b'from typing import Tuple\n\nimport torch\nimport torch.nn as nn\n\n\nclass MixupLoss:\n    def __init__(self, reduction: str):\n        self.loss_func = nn.CrossEntropyLoss(reduction=reduction)\n\n    def __call__(\n            self, predictions: torch.Tensor,\n            targets: Tuple[torch.Tensor, torch.Tensor, float]) -> torch.Tensor:\n        targets1, targets2, lam = targets\n        return lam * self.loss_func(predictions, targets1) + (\n            1 - lam) * self.loss_func(predictions, targets2)\n'"
pytorch_image_classification/losses/ricap.py,3,"b'from typing import List, Tuple\n\nimport torch\nimport torch.nn as nn\n\n\nclass RICAPLoss:\n    def __init__(self, reduction: str):\n        self.loss_func = nn.CrossEntropyLoss(reduction=reduction)\n\n    def __call__(\n            self, predictions: torch.Tensor,\n            targets: Tuple[List[torch.Tensor], List[float]]) -> torch.Tensor:\n        target_list, weights = targets\n        return sum([\n            weight * self.loss_func(predictions, targets)\n            for targets, weight in zip(target_list, weights)\n        ])\n'"
pytorch_image_classification/models/__init__.py,3,"b""import importlib\n\nimport torch\nimport torch.nn as nn\nimport torch.distributed as dist\nimport yacs.config\n\n\ndef create_model(config: yacs.config.CfgNode) -> nn.Module:\n    module = importlib.import_module(\n        'pytorch_image_classification.models'\n        f'.{config.model.type}.{config.model.name}')\n    model = getattr(module, 'Network')(config)\n    device = torch.device(config.device)\n    model.to(device)\n    return model\n\n\ndef apply_data_parallel_wrapper(config: yacs.config.CfgNode,\n                                model: nn.Module) -> nn.Module:\n    local_rank = config.train.dist.local_rank\n    if dist.is_available() and dist.is_initialized():\n        if config.train.dist.use_sync_bn:\n            model = nn.SyncBatchNorm.convert_sync_batchnorm(model)\n        model = nn.parallel.DistributedDataParallel(model,\n                                                    device_ids=[local_rank],\n                                                    output_device=local_rank)\n    else:\n        if config.device == 'cuda':\n            model = nn.DataParallel(model)\n    return model\n"""
pytorch_image_classification/models/initializer.py,1,"b""from typing import Callable\n\nimport torch.nn as nn\n\n\ndef create_initializer(mode: str) -> Callable:\n    if mode in ['kaiming_fan_out', 'kaiming_fan_in']:\n        mode = mode[8:]\n\n        def initializer(module):\n            if isinstance(module, nn.Conv2d):\n                nn.init.kaiming_normal_(module.weight.data,\n                                        mode=mode,\n                                        nonlinearity='relu')\n            elif isinstance(module, nn.BatchNorm2d):\n                nn.init.ones_(module.weight.data)\n                nn.init.zeros_(module.bias.data)\n            elif isinstance(module, nn.Linear):\n                nn.init.kaiming_normal_(module.weight.data,\n                                        mode=mode,\n                                        nonlinearity='relu')\n                nn.init.zeros_(module.bias.data)\n    else:\n        raise ValueError()\n\n    return initializer\n"""
pytorch_image_classification/optim/__init__.py,3,"b""import torch\n\nfrom .adabound import AdaBound, AdaBoundW\nfrom .lars import LARSOptimizer\n\n\ndef get_param_list(config, model):\n    if config.train.no_weight_decay_on_bn:\n        param_list = []\n        for name, params in model.named_parameters():\n            if 'conv.weight' in name:\n                param_list.append({\n                    'params': params,\n                    'weight_decay': config.train.weight_decay,\n                })\n            else:\n                param_list.append({\n                    'params': params,\n                    'weight_decay': 0,\n                })\n    else:\n        param_list = [{\n            'params': list(model.parameters()),\n            'weight_decay': config.train.weight_decay,\n        }]\n    return param_list\n\n\ndef create_optimizer(config, model):\n    params = get_param_list(config, model)\n\n    if config.train.optimizer == 'sgd':\n        optimizer = torch.optim.SGD(params,\n                                    lr=config.train.base_lr,\n                                    momentum=config.train.momentum,\n                                    nesterov=config.train.nesterov)\n    elif config.train.optimizer == 'adam':\n        optimizer = torch.optim.Adam(params,\n                                     lr=config.train.base_lr,\n                                     betas=config.optim.adam.betas)\n    elif config.train.optimizer == 'amsgrad':\n        optimizer = torch.optim.Adam(params,\n                                     lr=config.train.base_lr,\n                                     betas=config.optim.adam.betas,\n                                     amsgrad=True)\n    elif config.train.optimizer == 'adabound':\n        optimizer = AdaBound(params,\n                             lr=config.train.base_lr,\n                             betas=config.optim.adabound.betas,\n                             final_lr=config.optim.adabound.final_lr,\n                             gamma=config.optim.adabound.gamma)\n    elif config.train.optimizer == 'adaboundw':\n        optimizer = AdaBoundW(params,\n                              lr=config.train.base_lr,\n                              betas=config.optim.adabound.betas,\n                              final_lr=config.optim.adabound.final_lr,\n                              gamma=config.optim.adabound.gamma)\n    elif config.train.optimizer == 'lars':\n        optimizer = LARSOptimizer(params,\n                                  lr=config.train.base_lr,\n                                  momentum=config.train.momentum,\n                                  eps=config.optim.lars.eps,\n                                  thresh=config.optim.lars.threshold)\n    else:\n        raise ValueError()\n    return optimizer\n"""
pytorch_image_classification/optim/adabound.py,13,"b'""""""\nThis code is ported from following implementation:\nhttps://github.com/Luolc/AdaBound\n\nFollowing is the original license of the code:\n\n\n                                 Apache License\n                           Version 2.0, January 2004\n                        http://www.apache.org/licenses/\n\n   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n\n   1. Definitions.\n\n      \'License\' shall mean the terms and conditions for use, reproduction,\n      and distribution as defined by Sections 1 through 9 of this document.\n\n      \'Licensor\' shall mean the copyright owner or entity authorized by\n      the copyright owner that is granting the License.\n\n      \'Legal Entity\' shall mean the union of the acting entity and all\n      other entities that control, are controlled by, or are under common\n      control with that entity. For the purposes of this definition,\n      \'control\' means (i) the power, direct or indirect, to cause the\n      direction or management of such entity, whether by contract or\n      otherwise, or (ii) ownership of fifty percent (50%) or more of the\n      outstanding shares, or (iii) beneficial ownership of such entity.\n\n      \'You\' (or \'Your\') shall mean an individual or Legal Entity\n      exercising permissions granted by this License.\n\n      \'Source\' form shall mean the preferred form for making modifications,\n      including but not limited to software source code, documentation\n      source, and configuration files.\n\n      \'Object\' form shall mean any form resulting from mechanical\n      transformation or translation of a Source form, including but\n      not limited to compiled object code, generated documentation,\n      and conversions to other media types.\n\n      \'Work\' shall mean the work of authorship, whether in Source or\n      Object form, made available under the License, as indicated by a\n      copyright notice that is included in or attached to the work\n      (an example is provided in the Appendix below).\n\n      \'Derivative Works\' shall mean any work, whether in Source or Object\n      form, that is based on (or derived from) the Work and for which the\n      editorial revisions, annotations, elaborations, or other modifications\n      represent, as a whole, an original work of authorship. For the purposes\n      of this License, Derivative Works shall not include works that remain\n      separable from, or merely link (or bind by name) to the interfaces of,\n      the Work and Derivative Works thereof.\n\n      \'Contribution\' shall mean any work of authorship, including\n      the original version of the Work and any modifications or additions\n      to that Work or Derivative Works thereof, that is intentionally\n      submitted to Licensor for inclusion in the Work by the copyright owner\n      or by an individual or Legal Entity authorized to submit on behalf of\n      the copyright owner. For the purposes of this definition, \'submitted\'\n      means any form of electronic, verbal, or written communication sent\n      to the Licensor or its representatives, including but not limited to\n      communication on electronic mailing lists, source code control systems,\n      and issue tracking systems that are managed by, or on behalf of, the\n      Licensor for the purpose of discussing and improving the Work, but\n      excluding communication that is conspicuously marked or otherwise\n      designated in writing by the copyright owner as \'Not a Contribution.\'\n\n      \'Contributor\' shall mean Licensor and any individual or Legal Entity\n      on behalf of whom a Contribution has been received by Licensor and\n      subsequently incorporated within the Work.\n\n   2. Grant of Copyright License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      copyright license to reproduce, prepare Derivative Works of,\n      publicly display, publicly perform, sublicense, and distribute the\n      Work and such Derivative Works in Source or Object form.\n\n   3. Grant of Patent License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      (except as stated in this section) patent license to make, have made,\n      use, offer to sell, sell, import, and otherwise transfer the Work,\n      where such license applies only to those patent claims licensable\n      by such Contributor that are necessarily infringed by their\n      Contribution(s) alone or by combination of their Contribution(s)\n      with the Work to which such Contribution(s) was submitted. If You\n      institute patent litigation against any entity (including a\n      cross-claim or counterclaim in a lawsuit) alleging that the Work\n      or a Contribution incorporated within the Work constitutes direct\n      or contributory patent infringement, then any patent licenses\n      granted to You under this License for that Work shall terminate\n      as of the date such litigation is filed.\n\n   4. Redistribution. You may reproduce and distribute copies of the\n      Work or Derivative Works thereof in any medium, with or without\n      modifications, and in Source or Object form, provided that You\n      meet the following conditions:\n\n      (a) You must give any other recipients of the Work or\n          Derivative Works a copy of this License; and\n\n      (b) You must cause any modified files to carry prominent notices\n          stating that You changed the files; and\n\n      (c) You must retain, in the Source form of any Derivative Works\n          that You distribute, all copyright, patent, trademark, and\n          attribution notices from the Source form of the Work,\n          excluding those notices that do not pertain to any part of\n          the Derivative Works; and\n\n      (d) If the Work includes a \'NOTICE\' text file as part of its\n          distribution, then any Derivative Works that You distribute must\n          include a readable copy of the attribution notices contained\n          within such NOTICE file, excluding those notices that do not\n          pertain to any part of the Derivative Works, in at least one\n          of the following places: within a NOTICE text file distributed\n          as part of the Derivative Works; within the Source form or\n          documentation, if provided along with the Derivative Works; or,\n          within a display generated by the Derivative Works, if and\n          wherever such third-party notices normally appear. The contents\n          of the NOTICE file are for informational purposes only and\n          do not modify the License. You may add Your own attribution\n          notices within Derivative Works that You distribute, alongside\n          or as an addendum to the NOTICE text from the Work, provided\n          that such additional attribution notices cannot be construed\n          as modifying the License.\n\n      You may add Your own copyright statement to Your modifications and\n      may provide additional or different license terms and conditions\n      for use, reproduction, or distribution of Your modifications, or\n      for any such Derivative Works as a whole, provided Your use,\n      reproduction, and distribution of the Work otherwise complies with\n      the conditions stated in this License.\n\n   5. Submission of Contributions. Unless You explicitly state otherwise,\n      any Contribution intentionally submitted for inclusion in the Work\n      by You to the Licensor shall be under the terms and conditions of\n      this License, without any additional terms or conditions.\n      Notwithstanding the above, nothing herein shall supersede or modify\n      the terms of any separate license agreement you may have executed\n      with Licensor regarding such Contributions.\n\n   6. Trademarks. This License does not grant permission to use the trade\n      names, trademarks, service marks, or product names of the Licensor,\n      except as required for reasonable and customary use in describing the\n      origin of the Work and reproducing the content of the NOTICE file.\n\n   7. Disclaimer of Warranty. Unless required by applicable law or\n      agreed to in writing, Licensor provides the Work (and each\n      Contributor provides its Contributions) on an \'AS IS\' BASIS,\n      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n      implied, including, without limitation, any warranties or conditions\n      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\n      PARTICULAR PURPOSE. You are solely responsible for determining the\n      appropriateness of using or redistributing the Work and assume any\n      risks associated with Your exercise of permissions under this License.\n\n   8. Limitation of Liability. In no event and under no legal theory,\n      whether in tort (including negligence), contract, or otherwise,\n      unless required by applicable law (such as deliberate and grossly\n      negligent acts) or agreed to in writing, shall any Contributor be\n      liable to You for damages, including any direct, indirect, special,\n      incidental, or consequential damages of any character arising as a\n      result of this License or out of the use or inability to use the\n      Work (including but not limited to damages for loss of goodwill,\n      work stoppage, computer failure or malfunction, or any and all\n      other commercial damages or losses), even if such Contributor\n      has been advised of the possibility of such damages.\n\n   9. Accepting Warranty or Additional Liability. While redistributing\n      the Work or Derivative Works thereof, You may choose to offer,\n      and charge a fee for, acceptance of support, warranty, indemnity,\n      or other liability obligations and/or rights consistent with this\n      License. However, in accepting such obligations, You may act only\n      on Your own behalf and on Your sole responsibility, not on behalf\n      of any other Contributor, and only if You agree to indemnify,\n      defend, and hold each Contributor harmless for any liability\n      incurred by, or claims asserted against, such Contributor by reason\n      of your accepting any such warranty or additional liability.\n\n   END OF TERMS AND CONDITIONS\n\n   APPENDIX: How to apply the Apache License to your work.\n\n      To apply the Apache License to your work, attach the following\n      boilerplate notice, with the fields enclosed by brackets \'[]\'\n      replaced with your own identifying information. (Don\'t include\n      the brackets!)  The text should be enclosed in the appropriate\n      comment syntax for the file format. We also recommend that a\n      file or class name and description of purpose be included on the\n      same \'printed page\' as the copyright notice for easier\n      identification within third-party archives.\n\n   Copyright [yyyy] [name of copyright owner]\n\n   Licensed under the Apache License, Version 2.0 (the \'License\');\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an \'AS IS\' BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License.\n""""""\n\nimport math\n\nimport torch\n\n\nclass AdaBound(torch.optim.Optimizer):\n    """"""Implements AdaBound algorithm.\n\n    It has been proposed in `Adaptive Gradient Methods with Dynamic Bound\n    of Learning Rate`_.\n\n    Arguments:\n        params (iterable): iterable of parameters to optimize or dicts\n            defining parameter groups\n        lr (float, optional): Adam learning rate (default: 1e-3)\n        betas (Tuple[float, float], optional): coefficients used for\n            computing running averages of gradient and its square\n            (default: (0.9, 0.999))\n        final_lr (float, optional): final (SGD) learning rate (default: 0.1)\n        gamma (float, optional): convergence speed of the bound functions\n            (default: 1e-3)\n        eps (float, optional): term added to the denominator to improve\n            numerical stability (default: 1e-8)\n        weight_decay (float, optional): weight decay (L2 penalty) (default: 0)\n        amsbound (boolean, optional): whether to use the AMSBound variant\n            of this algorithm\n\n    .. Adaptive Gradient Methods with Dynamic Bound of Learning Rate:\n        https://openreview.net/forum?id=Bkg3g2R9FX\n    """"""\n    def __init__(self,\n                 params,\n                 lr=1e-3,\n                 betas=(0.9, 0.999),\n                 final_lr=0.1,\n                 gamma=1e-3,\n                 eps=1e-8,\n                 weight_decay=0,\n                 amsbound=False):\n        if not 0.0 <= lr:\n            raise ValueError(f\'Invalid learning rate: {lr}\')\n        if not 0.0 <= eps:\n            raise ValueError(f\'Invalid epsilon value: {eps}\')\n        if not 0.0 <= betas[0] < 1.0:\n            raise ValueError(f\'Invalid beta parameter at index 0: {betas[0]}\')\n        if not 0.0 <= betas[1] < 1.0:\n            raise ValueError(f\'Invalid beta parameter at index 1: {betas[1]}\')\n        if not 0.0 <= final_lr:\n            raise ValueError(f\'Invalid final learning rate: {final_lr}\')\n        if not 0.0 <= gamma < 1.0:\n            raise ValueError(f\'Invalid gamma parameter: {gamma}\')\n        defaults = dict(lr=lr,\n                        betas=betas,\n                        final_lr=final_lr,\n                        gamma=gamma,\n                        eps=eps,\n                        weight_decay=weight_decay,\n                        amsbound=amsbound)\n        super().__init__(params, defaults)\n\n        self.base_lrs = list(map(lambda group: group[\'lr\'], self.param_groups))\n\n    def __setstate__(self, state):\n        super().__setstate__(state)\n        for group in self.param_groups:\n            group.setdefault(\'amsbound\', False)\n\n    def step(self, closure=None):\n        """"""Performs a single optimization step.\n\n        Arguments:\n            closure (callable, optional): A closure that reevaluates\n                the model and returns the loss.\n        """"""\n        loss = None\n        if closure is not None:\n            loss = closure()\n\n        for group, base_lr in zip(self.param_groups, self.base_lrs):\n            for p in group[\'params\']:\n                if p.grad is None:\n                    continue\n                grad = p.grad.data\n                if grad.is_sparse:\n                    raise RuntimeError(\n                        \'AdaBound does not support sparse gradients\')\n                amsbound = group[\'amsbound\']\n\n                state = self.state[p]\n\n                # State initialization\n                if len(state) == 0:\n                    state[\'step\'] = 0\n                    # Exponential moving average of gradient values\n                    state[\'exp_avg\'] = torch.zeros_like(p.data)\n                    # Exponential moving average of squared gradient values\n                    state[\'exp_avg_sq\'] = torch.zeros_like(p.data)\n                    if amsbound:\n                        # Maintains max of all exp. moving avg. of\n                        # sq. grad. values\n                        state[\'max_exp_avg_sq\'] = torch.zeros_like(p.data)\n\n                exp_avg, exp_avg_sq = state[\'exp_avg\'], state[\'exp_avg_sq\']\n                if amsbound:\n                    max_exp_avg_sq = state[\'max_exp_avg_sq\']\n                beta1, beta2 = group[\'betas\']\n\n                state[\'step\'] += 1\n\n                if group[\'weight_decay\'] != 0:\n                    grad = grad.add(group[\'weight_decay\'], p.data)\n\n                # Decay the first and second moment running average coefficient\n                exp_avg.mul_(beta1).add_(1 - beta1, grad)\n                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n                if amsbound:\n                    # Maintains the maximum of all 2nd moment running avg.\n                    # till now\n                    torch.max(max_exp_avg_sq, exp_avg_sq, out=max_exp_avg_sq)\n                    # Use the max. for normalizing running avg. of gradient\n                    denom = max_exp_avg_sq.sqrt().add_(group[\'eps\'])\n                else:\n                    denom = exp_avg_sq.sqrt().add_(group[\'eps\'])\n\n                bias_correction1 = 1 - beta1**state[\'step\']\n                bias_correction2 = 1 - beta2**state[\'step\']\n                step_size = group[\'lr\'] * math.sqrt(\n                    bias_correction2) / bias_correction1\n\n                # Applies bounds on actual learning rate\n                # lr_scheduler cannot affect final_lr, this is a\n                # workaround to apply lr decay\n                final_lr = group[\'final_lr\'] * group[\'lr\'] / base_lr\n                lower_bound = final_lr * (1 - 1 /\n                                          (group[\'gamma\'] * state[\'step\'] + 1))\n                upper_bound = final_lr * (1 + 1 /\n                                          (group[\'gamma\'] * state[\'step\']))\n                step_size = torch.full_like(denom, step_size)\n                step_size.div_(denom).clamp_(lower_bound,\n                                             upper_bound).mul_(exp_avg)\n\n                p.data.add_(-step_size)\n\n        return loss\n\n\nclass AdaBoundW(torch.optim.Optimizer):\n    """"""Implements AdaBound algorithm with Decoupled Weight Decay\n    (arxiv.org/abs/1711.05101)\n\n    It has been proposed in `Adaptive Gradient Methods with Dynamic Bound\n    of Learning Rate`_.\n\n    Arguments:\n        params (iterable): iterable of parameters to optimize or dicts\n            defining parameter groups\n        lr (float, optional): Adam learning rate (default: 1e-3)\n        betas (Tuple[float, float], optional): coefficients used for\n            computing running averages of gradient and its square\n            (default: (0.9, 0.999))\n        final_lr (float, optional): final (SGD) learning rate (default: 0.1)\n        gamma (float, optional): convergence speed of the bound functions\n            (default: 1e-3)\n        eps (float, optional): term added to the denominator to improve\n            numerical stability (default: 1e-8)\n        weight_decay (float, optional): weight decay (L2 penalty) (default: 0)\n        amsbound (boolean, optional): whether to use the AMSBound variant\n            of this algorithm\n\n    .. Adaptive Gradient Methods with Dynamic Bound of Learning Rate:\n        https://openreview.net/forum?id=Bkg3g2R9FX\n    """"""\n    def __init__(self,\n                 params,\n                 lr=1e-3,\n                 betas=(0.9, 0.999),\n                 final_lr=0.1,\n                 gamma=1e-3,\n                 eps=1e-8,\n                 weight_decay=0,\n                 amsbound=False):\n        if not 0.0 <= lr:\n            raise ValueError(f\'Invalid learning rate: {lr}\')\n        if not 0.0 <= eps:\n            raise ValueError(f\'Invalid epsilon value: {eps}\')\n        if not 0.0 <= betas[0] < 1.0:\n            raise ValueError(f\'Invalid beta parameter at index 0: {betas[0]}\')\n        if not 0.0 <= betas[1] < 1.0:\n            raise ValueError(f\'Invalid beta parameter at index 1: {betas[1]}\')\n        if not 0.0 <= final_lr:\n            raise ValueError(f\'Invalid final learning rate: {final_lr}\')\n        if not 0.0 <= gamma < 1.0:\n            raise ValueError(f\'Invalid gamma parameter: {gamma}\')\n        defaults = dict(lr=lr,\n                        betas=betas,\n                        final_lr=final_lr,\n                        gamma=gamma,\n                        eps=eps,\n                        weight_decay=weight_decay,\n                        amsbound=amsbound)\n        super().__init__(params, defaults)\n\n        self.base_lrs = list(map(lambda group: group[\'lr\'], self.param_groups))\n\n    def __setstate__(self, state):\n        super().__setstate__(state)\n        for group in self.param_groups:\n            group.setdefault(\'amsbound\', False)\n\n    def step(self, closure=None):\n        """"""Performs a single optimization step.\n\n        Arguments:\n            closure (callable, optional): A closure that reevaluates the model\n                and returns the loss.\n        """"""\n        loss = None\n        if closure is not None:\n            loss = closure()\n\n        for group, base_lr in zip(self.param_groups, self.base_lrs):\n            for p in group[\'params\']:\n                if p.grad is None:\n                    continue\n                grad = p.grad.data\n                if grad.is_sparse:\n                    raise RuntimeError(\n                        \'AdaBound does not support sparse gradients\')\n                amsbound = group[\'amsbound\']\n\n                state = self.state[p]\n\n                # State initialization\n                if len(state) == 0:\n                    state[\'step\'] = 0\n                    # Exponential moving average of gradient values\n                    state[\'exp_avg\'] = torch.zeros_like(p.data)\n                    # Exponential moving average of squared gradient values\n                    state[\'exp_avg_sq\'] = torch.zeros_like(p.data)\n                    if amsbound:\n                        # Maintains max of all exp. moving avg. of\n                        # sq. grad. values\n                        state[\'max_exp_avg_sq\'] = torch.zeros_like(p.data)\n\n                exp_avg, exp_avg_sq = state[\'exp_avg\'], state[\'exp_avg_sq\']\n                if amsbound:\n                    max_exp_avg_sq = state[\'max_exp_avg_sq\']\n                beta1, beta2 = group[\'betas\']\n\n                state[\'step\'] += 1\n\n                # Decay the first and second moment running average coefficient\n                exp_avg.mul_(beta1).add_(1 - beta1, grad)\n                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n                if amsbound:\n                    # Maintains the maximum of all 2nd moment running avg.\n                    # till now\n                    torch.max(max_exp_avg_sq, exp_avg_sq, out=max_exp_avg_sq)\n                    # Use the max. for normalizing running avg. of gradient\n                    denom = max_exp_avg_sq.sqrt().add_(group[\'eps\'])\n                else:\n                    denom = exp_avg_sq.sqrt().add_(group[\'eps\'])\n\n                bias_correction1 = 1 - beta1**state[\'step\']\n                bias_correction2 = 1 - beta2**state[\'step\']\n                step_size = group[\'lr\'] * math.sqrt(\n                    bias_correction2) / bias_correction1\n\n                # Applies bounds on actual learning rate\n                # lr_scheduler cannot affect final_lr, this is a\n                # workaround to apply lr decay\n                final_lr = group[\'final_lr\'] * group[\'lr\'] / base_lr\n                lower_bound = final_lr * (1 - 1 /\n                                          (group[\'gamma\'] * state[\'step\'] + 1))\n                upper_bound = final_lr * (1 + 1 /\n                                          (group[\'gamma\'] * state[\'step\']))\n                step_size = torch.full_like(denom, step_size)\n                step_size.div_(denom).clamp_(lower_bound,\n                                             upper_bound).mul_(exp_avg)\n\n                if group[\'weight_decay\'] != 0:\n                    decayed_weights = torch.mul(p.data, group[\'weight_decay\'])\n                    p.data.add_(-step_size)\n                    p.data.sub_(decayed_weights)\n                else:\n                    p.data.add_(-step_size)\n\n        return loss\n'"
pytorch_image_classification/optim/lars.py,6,"b""import torch\n\n\nclass LARSOptimizer(torch.optim.Optimizer):\n    def __init__(self,\n                 params,\n                 lr,\n                 momentum=0,\n                 weight_decay=0,\n                 eps=1e-9,\n                 thresh=1e-2):\n\n        if lr < 0.0:\n            raise ValueError(f'Invalid learning rate: {lr}')\n        if momentum < 0.0:\n            raise ValueError(f'Invalid momentum value: {momentum}')\n        if weight_decay < 0.0:\n            raise ValueError(f'Invalid weight_decay value: {weight_decay}')\n\n        defaults = dict(lr=lr,\n                        momentum=momentum,\n                        weight_decay=weight_decay,\n                        eps=eps,\n                        thresh=thresh)\n        super().__init__(params, defaults)\n\n    def step(self, closure=None):\n        loss = None\n        if closure is not None:\n            loss = closure()\n\n        for group in self.param_groups:\n            weight_decay = group['weight_decay']\n            momentum = group['momentum']\n            lr = group['lr']\n            eps = group['eps']\n            thresh = group['thresh']\n\n            for p in group['params']:\n                if p.grad is None:\n                    continue\n\n                d_p = p.grad.data\n\n                weight_norm = torch.norm(p.data)\n                grad_norm = torch.norm(d_p)\n                local_lr = weight_norm / (eps + grad_norm +\n                                          weight_decay * weight_norm)\n                local_lr = torch.where(weight_norm < thresh,\n                                       torch.ones_like(local_lr), local_lr)\n\n                if weight_decay != 0:\n                    d_p.add_(weight_decay, p.data)\n\n                param_state = self.state[p]\n                if 'momentum_buffer' not in param_state:\n                    buf = param_state['momentum_buffer'] = torch.zeros_like(\n                        p.data)\n                else:\n                    buf = param_state['momentum_buffer']\n                buf.mul_(momentum).add_(lr * local_lr, d_p)\n                p.data.add_(-1.0, buf)\n\n        return loss\n"""
pytorch_image_classification/scheduler/__init__.py,1,"b""import torch\n\nfrom .combined_scheduler import CombinedScheduler\nfrom .components import (\n    ConstantScheduler,\n    CosineScheduler,\n    ExponentialScheduler,\n    LinearScheduler,\n)\nfrom .multistep_scheduler import MultistepScheduler\nfrom .sgdr import SGDRScheduler\n\n\ndef _create_warmup(config, warmup_steps):\n    warmup_type = config.scheduler.warmup.type\n    if warmup_type == 'none' or warmup_steps == 0:\n        return None\n\n    warmup_start_factor = config.scheduler.warmup.start_factor\n\n    if warmup_type == 'linear':\n        lr_end = 1\n        lr_start = warmup_start_factor\n        scheduler = LinearScheduler(warmup_steps, lr_start, lr_end)\n    elif warmup_type == 'exponential':\n        scheduler = ExponentialScheduler(warmup_steps, config.train.base_lr,\n                                         config.scheduler.warmup.exponent,\n                                         warmup_start_factor)\n    else:\n        raise ValueError()\n\n    return scheduler\n\n\ndef _create_main_scheduler(config, main_steps):\n    scheduler_type = config.scheduler.type\n\n    if scheduler_type == 'constant':\n        scheduler = ConstantScheduler(main_steps, 1)\n    elif scheduler_type == 'multistep':\n        lr_decay = config.scheduler.lr_decay\n        scheduler = MultistepScheduler(main_steps, 1, lr_decay,\n                                       config.scheduler.milestones)\n    elif scheduler_type == 'linear':\n        lr_start = 1\n        lr_end = config.scheduler.lr_min_factor\n        scheduler = LinearScheduler(main_steps, lr_start, lr_end)\n    elif scheduler_type == 'cosine':\n        scheduler = CosineScheduler(main_steps, 1,\n                                    config.scheduler.lr_min_factor)\n    elif scheduler_type == 'sgdr':\n        scheduler = SGDRScheduler(main_steps, 1, config.scheduler.T0,\n                                  config.scheduler.T_mul,\n                                  config.scheduler.lr_min_factor)\n    else:\n        raise ValueError()\n\n    return scheduler\n\n\ndef create_scheduler(config, optimizer, steps_per_epoch):\n    warmup_epochs = config.scheduler.warmup.epochs\n    main_epochs = config.scheduler.epochs - warmup_epochs\n\n    warmup_scheduler = _create_warmup(config, warmup_epochs)\n    main_scheduler = _create_main_scheduler(config, main_epochs)\n\n    scheduler_func = CombinedScheduler([warmup_scheduler, main_scheduler])\n    scheduler_func.multiply_steps(steps_per_epoch)\n\n    scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, scheduler_func)\n\n    return scheduler\n"""
pytorch_image_classification/scheduler/combined_scheduler.py,0,"b""import bisect\n\nimport numpy as np\n\n\nclass CombinedScheduler:\n    def __init__(self, schedulers=None):\n        self.schedulers = []\n        if schedulers is not None:\n            for scheduler in schedulers:\n                if scheduler is None:\n                    continue\n                elif hasattr(scheduler, 'schedulers'):\n                    self.schedulers += scheduler.schedulers\n                else:\n                    self.schedulers.append(scheduler)\n\n    def __call__(self, step):\n        index = bisect.bisect_left(self._offsets, step) - 1\n        index = max(0, min(index, len(self.schedulers) - 1))\n        scheduler = self.schedulers[index]\n        offset = self._offsets[index]\n        return scheduler(step - offset)\n\n    @property\n    def _steps(self):\n        return [scheduler.steps for scheduler in self.schedulers]\n\n    @property\n    def steps(self):\n        return sum(self._steps)\n\n    @property\n    def _offsets(self):\n        return np.cumsum(np.concatenate([[0], self._steps]))\n\n    def multiply_steps(self, val):\n        for scheduler in self.schedulers:\n            scheduler.steps *= val\n"""
pytorch_image_classification/scheduler/components.py,0,"b'import numpy as np\n\n\nclass ConstantScheduler:\n    def __init__(self, steps, lr):\n        self.steps = steps\n        self.lr = lr\n\n    def __call__(self, step):\n        return self.lr\n\n\nclass CosineScheduler:\n    def __init__(self, steps, base_lr, lr_min_factor=1e-3):\n        self.steps = steps\n        self.base_lr = base_lr\n        self.lr_min_factor = lr_min_factor\n\n    def __call__(self, step):\n        return self.base_lr * (self.lr_min_factor +\n                               (1 - self.lr_min_factor) * 0.5 *\n                               (1 + np.cos(step / self.steps * np.pi)))\n\n\nclass ExponentialScheduler:\n    def __init__(self, steps, base_lr, exponent, lr_start_factor=1e-3):\n        self.steps = steps\n        self.base_lr = base_lr\n        self.exponent = exponent\n        self.lr_start_factor = lr_start_factor\n\n    def __call__(self, step):\n        return self.base_lr * (self.lr_start_factor +\n                               (1 - self.lr_start_factor) *\n                               (step / self.steps)**self.exponent)\n\n\nclass LinearScheduler:\n    def __init__(self, steps, lr_start, lr_end):\n        self.steps = steps\n        self.lr_start = lr_start\n        self.lr_end = lr_end\n\n    def __call__(self, step):\n        return self.lr_start + (self.lr_end -\n                                self.lr_start) * step / self.steps\n'"
pytorch_image_classification/scheduler/multistep_scheduler.py,0,"b'from pytorch_image_classification.scheduler.combined_scheduler import (\n    CombinedScheduler, )\nfrom pytorch_image_classification.scheduler.components import (\n    ConstantScheduler, )\n\n\nclass MultistepScheduler(CombinedScheduler):\n    def __init__(self, steps, base_lr, gamma, milestones):\n        lrs = [base_lr * gamma**index for index in range(len(milestones) + 1)]\n        step_list = [\n            step1 - step0\n            for step0, step1 in zip([0] + milestones, milestones + [steps])\n        ]\n        schedulers = [\n            ConstantScheduler(step, lr) for step, lr in zip(step_list, lrs)\n        ]\n        super().__init__(schedulers)\n'"
pytorch_image_classification/scheduler/sgdr.py,0,"b'from pytorch_image_classification.scheduler.combined_scheduler import (\n    CombinedScheduler, )\nfrom pytorch_image_classification.scheduler.components import (\n    CosineScheduler, )\n\n\nclass SGDRScheduler(CombinedScheduler):\n    def __init__(self, steps, base_lr, T0, T_mul, lr_min_factor=1e-3):\n        step_list = [T0]\n        while sum(step_list) < steps:\n            step_list.append(int(step_list[-1] * T_mul))\n        assert sum(step_list) == steps\n        schedulers = [\n            CosineScheduler(step, base_lr, lr_min_factor) for step in step_list\n        ]\n        super().__init__(schedulers)\n'"
pytorch_image_classification/transforms/__init__.py,0,"b""from typing import Callable, Tuple\n\nimport numpy as np\nimport torchvision\nimport yacs.config\n\nfrom .transforms import (\n    CenterCrop,\n    Normalize,\n    RandomCrop,\n    RandomHorizontalFlip,\n    RandomResizeCrop,\n    Resize,\n    ToTensor,\n)\n\nfrom .cutout import Cutout, DualCutout\nfrom .random_erasing import RandomErasing\n\n\ndef _get_dataset_stats(\n        config: yacs.config.CfgNode) -> Tuple[np.ndarray, np.ndarray]:\n    name = config.dataset.name\n    if name == 'CIFAR10':\n        # RGB\n        mean = np.array([0.4914, 0.4822, 0.4465])\n        std = np.array([0.2470, 0.2435, 0.2616])\n    elif name == 'CIFAR100':\n        # RGB\n        mean = np.array([0.5071, 0.4865, 0.4409])\n        std = np.array([0.2673, 0.2564, 0.2762])\n    elif name == 'MNIST':\n        mean = np.array([0.1307])\n        std = np.array([0.3081])\n    elif name == 'FashionMNIST':\n        mean = np.array([0.2860])\n        std = np.array([0.3530])\n    elif name == 'KMNIST':\n        mean = np.array([0.1904])\n        std = np.array([0.3475])\n    elif name == 'ImageNet':\n        # RGB\n        mean = [0.485, 0.456, 0.406]\n        std = [0.229, 0.224, 0.225]\n    else:\n        raise ValueError()\n    return mean, std\n\n\ndef create_transform(config: yacs.config.CfgNode, is_train: bool) -> Callable:\n    if config.model.type == 'cifar':\n        return create_cifar_transform(config, is_train)\n    elif config.model.type == 'imagenet':\n        return create_imagenet_transform(config, is_train)\n    else:\n        raise ValueError\n\n\ndef create_cifar_transform(config: yacs.config.CfgNode,\n                           is_train: bool) -> Callable:\n    mean, std = _get_dataset_stats(config)\n    if is_train:\n        transforms = []\n        if config.augmentation.use_random_crop:\n            transforms.append(RandomCrop(config))\n        if config.augmentation.use_random_horizontal_flip:\n            transforms.append(RandomHorizontalFlip(config))\n\n        transforms.append(Normalize(mean, std))\n\n        if config.augmentation.use_cutout:\n            transforms.append(Cutout(config))\n        if config.augmentation.use_random_erasing:\n            transforms.append(RandomErasing(config))\n        if config.augmentation.use_dual_cutout:\n            transforms.append(DualCutout(config))\n\n        transforms.append(ToTensor())\n    else:\n        transforms = [\n            Normalize(mean, std),\n            ToTensor(),\n        ]\n\n    return torchvision.transforms.Compose(transforms)\n\n\ndef create_imagenet_transform(config: yacs.config.CfgNode,\n                              is_train: bool) -> Callable:\n    mean, std = _get_dataset_stats(config)\n    if is_train:\n        transforms = []\n        if config.augmentation.use_random_crop:\n            transforms.append(RandomResizeCrop(config))\n        else:\n            transforms.append(CenterCrop(config))\n        if config.augmentation.use_random_horizontal_flip:\n            transforms.append(RandomHorizontalFlip(config))\n\n        transforms.append(Normalize(mean, std))\n\n        if config.augmentation.use_cutout:\n            transforms.append(Cutout(config))\n        if config.augmentation.use_random_erasing:\n            transforms.append(RandomErasing(config))\n        if config.augmentation.use_dual_cutout:\n            transforms.append(DualCutout(config))\n\n        transforms.append(ToTensor())\n    else:\n        transforms = []\n        if config.tta.use_resize:\n            transforms.append(Resize(config))\n        if config.tta.use_center_crop:\n            transforms.append(CenterCrop(config))\n        transforms += [\n            Normalize(mean, std),\n            ToTensor(),\n        ]\n\n    return torchvision.transforms.Compose(transforms)\n"""
pytorch_image_classification/transforms/cutout.py,0,"b'import numpy as np\nimport yacs.config\n\n\nclass Cutout:\n    def __init__(self, config: yacs.config.CfgNode):\n        aug_config = config.augmentation.cutout\n        self.p = aug_config.prob\n        self.mask_size = aug_config.mask_size\n        self.cutout_inside = aug_config.cut_inside\n        self.mask_color = aug_config.mask_color\n\n        self.mask_size_half = aug_config.mask_size // 2\n        self.offset = 1 if aug_config.mask_size % 2 == 0 else 0\n\n    def __call__(self, image: np.ndarray) -> np.ndarray:\n        image = np.asarray(image).copy()\n\n        if np.random.random() > self.p:\n            return image\n\n        h, w = image.shape[:2]\n\n        if self.cutout_inside:\n            cxmin = self.mask_size_half\n            cxmax = w + self.offset - self.mask_size_half\n            cymin = self.mask_size_half\n            cymax = h + self.offset - self.mask_size_half\n        else:\n            cxmin, cxmax = 0, w + self.offset\n            cymin, cymax = 0, h + self.offset\n\n        cx = np.random.randint(cxmin, cxmax)\n        cy = np.random.randint(cymin, cymax)\n        xmin = cx - self.mask_size_half\n        ymin = cy - self.mask_size_half\n        xmax = xmin + self.mask_size\n        ymax = ymin + self.mask_size\n        xmin = max(0, xmin)\n        ymin = max(0, ymin)\n        xmax = min(w, xmax)\n        ymax = min(h, ymax)\n        image[ymin:ymax, xmin:xmax] = self.mask_color\n        return image\n\n\nclass DualCutout:\n    def __init__(self, config: yacs.config.CfgNode):\n        self.cutout = Cutout(config)\n\n    def __call__(self, image: np.ndarray) -> np.ndarray:\n        return np.hstack([self.cutout(image), self.cutout(image)])\n'"
pytorch_image_classification/transforms/random_erasing.py,0,"b'import numpy as np\nimport yacs.config\n\n\nclass RandomErasing:\n    def __init__(self, config: yacs.config.CfgNode):\n        aug_config = config.augmentation.random_erasing\n        self.p = aug_config.prob\n        self.max_attempt = aug_config.max_attempt\n        self.sl, self.sh = aug_config.area_ratio_range\n        self.rl = aug_config.min_aspect_ratio\n        self.rh = 1. / aug_config.min_aspect_ratio\n\n    def __call__(self, image: np.ndarray) -> np.ndarray:\n        image = np.asarray(image).copy()\n\n        if np.random.random() > self.p:\n            return image\n\n        h, w = image.shape[:2]\n        image_area = h * w\n\n        for _ in range(self.max_attempt):\n            mask_area = np.random.uniform(self.sl, self.sh) * image_area\n            aspect_ratio = np.random.uniform(self.rl, self.rh)\n            mask_h = int(np.sqrt(mask_area * aspect_ratio))\n            mask_w = int(np.sqrt(mask_area / aspect_ratio))\n\n            if mask_w < w and mask_h < h:\n                x0 = np.random.randint(0, w - mask_w)\n                y0 = np.random.randint(0, h - mask_h)\n                x1 = x0 + mask_w\n                y1 = y0 + mask_h\n                image[y0:y1, x0:x1] = np.random.uniform(0, 1)\n                break\n\n        return image\n'"
pytorch_image_classification/transforms/transforms.py,4,"b'from typing import Tuple, Union\n\nimport numpy as np\nimport PIL.Image\nimport torch\nimport torchvision\nimport yacs.config\n\n\nclass CenterCrop:\n    def __init__(self, config: yacs.config.CfgNode):\n        self.transform = torchvision.transforms.CenterCrop(\n            config.dataset.image_size)\n\n    def __call__(self, data: PIL.Image.Image) -> PIL.Image.Image:\n        return self.transform(data)\n\n\nclass Normalize:\n    def __init__(self, mean: np.ndarray, std: np.ndarray):\n        self.mean = np.array(mean)\n        self.std = np.array(std)\n\n    def __call__(self, image: PIL.Image.Image) -> np.ndarray:\n        image = np.asarray(image).astype(np.float32) / 255.\n        image = (image - self.mean) / self.std\n        return image\n\n\nclass RandomCrop:\n    def __init__(self, config: yacs.config.CfgNode):\n        self.transform = torchvision.transforms.RandomCrop(\n            config.dataset.image_size,\n            padding=config.augmentation.random_crop.padding,\n            fill=config.augmentation.random_crop.fill,\n            padding_mode=config.augmentation.random_crop.padding_mode)\n\n    def __call__(self, data: PIL.Image.Image) -> PIL.Image.Image:\n        return self.transform(data)\n\n\nclass RandomResizeCrop:\n    def __init__(self, config: yacs.config.CfgNode):\n        self.transform = torchvision.transforms.RandomResizedCrop(\n            config.dataset.image_size)\n\n    def __call__(self, data: PIL.Image.Image) -> PIL.Image.Image:\n        return self.transform(data)\n\n\nclass RandomHorizontalFlip:\n    def __init__(self, config: yacs.config.CfgNode):\n        self.transform = torchvision.transforms.RandomHorizontalFlip(\n            config.augmentation.random_horizontal_flip.prob)\n\n    def __call__(self, data: PIL.Image.Image) -> PIL.Image.Image:\n        return self.transform(data)\n\n\nclass Resize:\n    def __init__(self, config: yacs.config.CfgNode):\n        self.transform = torchvision.transforms.Resize(config.tta.resize)\n\n    def __call__(self, data: PIL.Image.Image) -> PIL.Image.Image:\n        return self.transform(data)\n\n\nclass ToTensor:\n    def __call__(\n        self, data: Union[np.ndarray, Tuple[np.ndarray, ...]]\n    ) -> Union[torch.Tensor, Tuple[torch.Tensor, ...]]:\n        if isinstance(data, tuple):\n            return tuple([self._to_tensor(image) for image in data])\n        else:\n            return self._to_tensor(data)\n\n    @staticmethod\n    def _to_tensor(data: np.ndarray) -> torch.Tensor:\n        if len(data.shape) == 3:\n            return torch.from_numpy(data.transpose(2, 0, 1).astype(np.float32))\n        else:\n            return torch.from_numpy(data[None, :, :].astype(np.float32))\n'"
pytorch_image_classification/utils/__init__.py,0,"b'from .dist import get_rank\nfrom .diff_config import find_config_diff\nfrom .env_info import get_env_info\nfrom .logger import create_logger\nfrom .metric_logger import AverageMeter\nfrom .metrics import compute_accuracy\nfrom .op_count import count_op\nfrom .tensorboard import DummyWriter, create_tensorboard_writer\nfrom .utils import save_config, set_seed, setup_cudnn\n'"
pytorch_image_classification/utils/diff_config.py,0,"b'from typing import Optional\n\nimport yacs.config\n\nfrom pytorch_image_classification import get_default_config\nfrom pytorch_image_classification.config.config_node import ConfigNode\n\n\ndef find_config_diff(\n        config: yacs.config.CfgNode) -> Optional[yacs.config.CfgNode]:\n    def _find_diff(node: yacs.config.CfgNode,\n                   default_node: yacs.config.CfgNode):\n        root_node = ConfigNode()\n        for key in node:\n            val = node[key]\n            if isinstance(val, yacs.config.CfgNode):\n                new_node = _find_diff(node[key], default_node[key])\n                if new_node is not None:\n                    root_node[key] = new_node\n            else:\n                if node[key] != default_node[key]:\n                    root_node[key] = node[key]\n        return root_node if len(root_node) > 0 else None\n\n    default_config = get_default_config()\n    new_config = _find_diff(config, default_config)\n    return new_config\n'"
pytorch_image_classification/utils/dist.py,1,b'import torch.distributed as dist\n\n\ndef get_rank() -> int:\n    if not (dist.is_available() and dist.is_initialized()):\n        return 0\n    else:\n        return dist.get_rank()\n'
pytorch_image_classification/utils/env_info.py,6,"b""import torch\nimport yacs.config\n\nfrom pytorch_image_classification.config.config_node import ConfigNode\n\n\ndef get_env_info(config: yacs.config.CfgNode) -> yacs.config.CfgNode:\n    info = {\n        'pytorch_version': torch.__version__,\n        'cuda_version': torch.version.cuda,\n        'cudnn_version': torch.backends.cudnn.version(),\n    }\n    if config.device != 'cpu':\n        info['num_gpus'] = torch.cuda.device_count()\n        info['gpu_name'] = torch.cuda.get_device_name(0)\n        capability = torch.cuda.get_device_capability(0)\n        info['gpu_capability'] = f'{capability[0]}.{capability[1]}'\n\n    return ConfigNode({'env_info': info})\n"""
pytorch_image_classification/utils/logger.py,0,"b'from typing import List, Optional\n\nimport logging\nimport pathlib\nimport sys\n\nimport termcolor\n\n\ndef create_logger(name: str,\n                  distributed_rank: int,\n                  output_dir: Optional[pathlib.Path] = None,\n                  filename: str = \'log.txt\') -> logging.Logger:\n    logger = logging.getLogger(name)\n    logger.setLevel(logging.DEBUG)\n    logger.propagate = False\n    if distributed_rank > 0:\n        return logger\n\n    fvcore_logger = logging.getLogger(\'fvcore\')\n    fvcore_logger.setLevel(logging.INFO)\n\n    handlers = _create_handlers(output_dir, filename)\n    for handler in handlers:\n        logger.addHandler(handler)\n        fvcore_logger.addHandler(handler)\n\n    return logger\n\n\ndef _create_handlers(output_dir: Optional[pathlib.Path] = None,\n                     filename: str = \'log.txt\') -> List[logging.Handler]:\n    handlers = []\n    color_formatter = _create_color_formatter()\n    handlers.append(_create_stream_handler(color_formatter))\n    if output_dir is not None:\n        handlers.append(\n            _create_file_handler(output_dir / filename, color_formatter))\n\n        plain_log_name_parts = filename.split(\'.\')\n        plain_log_name_parts[-2] = plain_log_name_parts[-2] + \'_plain\'\n        plain_log_name = \'.\'.join(plain_log_name_parts)\n        plain_formatter = _create_plain_formatter()\n        handlers.append(\n            _create_file_handler(output_dir / plain_log_name, plain_formatter))\n    return handlers\n\n\ndef _create_plain_formatter() -> logging.Formatter:\n    return logging.Formatter(\n        \'[%(asctime)s] %(name)s %(levelname)s: %(message)s\',\n        datefmt=""%Y-%m-%d %H:%M:%S"")\n\n\ndef _create_color_formatter() -> logging.Formatter:\n    return logging.Formatter(\n        termcolor.colored(\'[%(asctime)s] %(name)s %(levelname)s: \', \'green\') +\n        \'%(message)s\',\n        datefmt=""%Y-%m-%d %H:%M:%S"")\n\n\ndef _create_stream_handler(\n        formatter: logging.Formatter) -> logging.StreamHandler:\n    stream_handler = logging.StreamHandler(stream=sys.stdout)\n    stream_handler.setLevel(logging.DEBUG)\n    stream_handler.setFormatter(formatter)\n    return stream_handler\n\n\ndef _create_file_handler(file_path: pathlib.Path,\n                         formatter: logging.Formatter) -> logging.FileHandler:\n    file_handler = logging.FileHandler(file_path.as_posix())\n    file_handler.setLevel(logging.DEBUG)\n    file_handler.setFormatter(formatter)\n    return file_handler\n'"
pytorch_image_classification/utils/metric_logger.py,0,"b'class AverageMeter:\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, num):\n        self.val = val\n        self.sum += val * num\n        self.count += num\n        self.avg = self.sum / self.count\n'"
pytorch_image_classification/utils/metrics.py,1,"b'import torch\n\n\ndef compute_accuracy(config, outputs, targets, augmentation, topk=(1, )):\n    if augmentation:\n        if config.augmentation.use_mixup or config.augmentation.use_cutmix:\n            targets1, targets2, lam = targets\n            accs1 = accuracy(outputs, targets1, topk)\n            accs2 = accuracy(outputs, targets2, topk)\n            accs = tuple([\n                lam * acc1 + (1 - lam) * acc2\n                for acc1, acc2 in zip(accs1, accs2)\n            ])\n        elif config.augmentation.use_ricap:\n            weights = []\n            accs_all = []\n            for labels, weight in zip(*targets):\n                weights.append(weight)\n                accs_all.append(accuracy(outputs, labels, topk))\n            accs = []\n            for i in range(len(accs_all[0])):\n                acc = 0\n                for weight, accs_list in zip(weights, accs_all):\n                    acc += weight * accs_list[i]\n                accs.append(acc)\n            accs = tuple(accs)\n        elif config.augmentation.use_dual_cutout:\n            outputs1, outputs2 = outputs[:, 0], outputs[:, 1]\n            accs = accuracy((outputs1 + outputs2) / 2, targets, topk)\n        else:\n            accs = accuracy(outputs, targets, topk)\n    else:\n        accs = accuracy(outputs, targets, topk)\n    return accs\n\n\ndef accuracy(outputs, targets, topk=(1, )):\n    with torch.no_grad():\n        maxk = max(topk)\n        batch_size = targets.size(0)\n\n        _, pred = outputs.topk(maxk, 1, True, True)\n        pred = pred.t()\n        correct = pred.eq(targets.view(1, -1).expand_as(pred))\n\n        res = []\n        for k in topk:\n            correct_k = correct[:k].view(-1).float().sum(0, keepdim=True)\n            res.append(correct_k.mul_(1 / batch_size))\n    return res\n'"
pytorch_image_classification/utils/op_count.py,4,"b'from typing import Tuple\nimport thop\nimport torch\nimport torch.nn as nn\nimport yacs.config\n\n\ndef count_op(config: yacs.config.CfgNode, model: nn.Module) -> Tuple[str, str]:\n    data = torch.zeros((1, config.dataset.n_channels,\n                        config.dataset.image_size, config.dataset.image_size),\n                       dtype=torch.float32,\n                       device=torch.device(config.device))\n    return thop.clever_format(thop.profile(model, (data, ), verbose=False))\n'"
pytorch_image_classification/utils/tensorboard.py,1,"b""import pathlib\n\nfrom torch.utils.tensorboard import SummaryWriter\nimport yacs.config\n\n\nclass DummyWriter(SummaryWriter):\n    def __init__(self):\n        pass\n\n    def add_scalar(self, tag, scalar_value, global_step=None, walltime=None):\n        pass\n\n    def add_scalars(self,\n                    main_tag,\n                    tag_scalar_dict,\n                    global_step=None,\n                    walltime=None):\n        pass\n\n    def export_scalars_to_json(self, path):\n        pass\n\n    def add_histogram(self,\n                      tag,\n                      values,\n                      global_step=None,\n                      bins='tensorflow',\n                      walltime=None,\n                      max_bins=None):\n        pass\n\n    def add_histogram_raw(self,\n                          tag,\n                          min,\n                          max,\n                          num,\n                          sum,\n                          sum_squares,\n                          bucket_limits,\n                          bucket_counts,\n                          global_step=None,\n                          walltime=None):\n        pass\n\n    def add_image(self,\n                  tag,\n                  img_tensor,\n                  global_step=None,\n                  walltime=None,\n                  dataformats='CHW'):\n        pass\n\n    def add_images(self,\n                   tag,\n                   img_tensor,\n                   global_step=None,\n                   walltime=None,\n                   dataformats='NCHW'):\n        pass\n\n    def add_image_with_boxes(self,\n                             tag,\n                             img_tensor,\n                             box_tensor,\n                             global_step=None,\n                             walltime=None,\n                             dataformats='CHW',\n                             **kwargs):\n        pass\n\n    def add_figure(self,\n                   tag,\n                   figure,\n                   global_step=None,\n                   close=True,\n                   walltime=None):\n        pass\n\n    def add_video(self,\n                  tag,\n                  vid_tensor,\n                  global_step=None,\n                  fps=4,\n                  walltime=None):\n        pass\n\n    def add_audio(self,\n                  tag,\n                  snd_tensor,\n                  global_step=None,\n                  sample_rate=44100,\n                  walltime=None):\n        pass\n\n    def add_text(self, tag, text_string, global_step=None, walltime=None):\n        pass\n\n    def add_onnx_graph(self, prototxt):\n        pass\n\n    def add_graph(self, model, input_to_model=None, verbose=False, **kwargs):\n        pass\n\n    def add_embedding(self,\n                      mat,\n                      metadata=None,\n                      label_img=None,\n                      global_step=None,\n                      tag='default',\n                      metadata_header=None):\n        pass\n\n    def add_pr_curve(self,\n                     tag,\n                     labels,\n                     predictions,\n                     global_step=None,\n                     num_thresholds=127,\n                     weights=None,\n                     walltime=None):\n        pass\n\n    def add_pr_curve_raw(self,\n                         tag,\n                         true_positive_counts,\n                         false_positive_counts,\n                         true_negative_counts,\n                         false_negative_counts,\n                         precision,\n                         recall,\n                         global_step=None,\n                         num_thresholds=127,\n                         weights=None,\n                         walltime=None):\n        pass\n\n    def add_custom_scalars_multilinechart(self,\n                                          tags,\n                                          category='default',\n                                          title='untitled'):\n        pass\n\n    def add_custom_scalars_marginchart(self,\n                                       tags,\n                                       category='default',\n                                       title='untitled'):\n        pass\n\n    def add_custom_scalars(self, layout):\n        pass\n\n    def flush(self):\n        pass\n\n    def close(self):\n        pass\n\n\ndef create_tensorboard_writer(config: yacs.config.CfgNode,\n                              output_dir: pathlib.Path,\n                              purge_step: int) -> SummaryWriter:\n    if config.train.use_tensorboard:\n        if config.train.start_epoch == 0:\n            return SummaryWriter(output_dir.as_posix())\n        else:\n            return SummaryWriter(output_dir.as_posix(), purge_step=purge_step)\n    else:\n        return DummyWriter()\n"""
pytorch_image_classification/utils/utils.py,4,"b""import pathlib\nimport random\n\nimport numpy as np\nimport torch\nimport yacs.config\n\n\ndef set_seed(config: yacs.config.CfgNode) -> None:\n    seed = config.train.seed\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n\n\ndef setup_cudnn(config: yacs.config.CfgNode) -> None:\n    torch.backends.cudnn.benchmark = config.cudnn.benchmark\n    torch.backends.cudnn.deterministic = config.cudnn.deterministic\n\n\ndef save_config(config: yacs.config.CfgNode,\n                output_path: pathlib.Path) -> None:\n    with open(output_path, 'w') as f:\n        f.write(str(config))\n"""
pytorch_image_classification/models/cifar/__init__.py,0,b''
pytorch_image_classification/models/cifar/densenet.py,7,"b""import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom ..initializer import create_initializer\n\n\nclass BasicBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, drop_rate):\n        super().__init__()\n\n        self.drop_rate = drop_rate\n\n        self.bn = nn.BatchNorm2d(in_channels)\n        self.conv = nn.Conv2d(in_channels,\n                              out_channels,\n                              kernel_size=3,\n                              stride=1,\n                              padding=1,\n                              bias=False)\n\n    def forward(self, x):\n        y = self.conv(F.relu(self.bn(x), inplace=True))\n        if self.drop_rate > 0:\n            y = F.dropout(y,\n                          p=self.drop_rate,\n                          training=self.training,\n                          inplace=False)\n        return torch.cat([x, y], dim=1)\n\n\nclass BottleneckBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, drop_rate):\n        super().__init__()\n\n        self.drop_rate = drop_rate\n\n        bottleneck_channels = out_channels * 4\n\n        self.bn1 = nn.BatchNorm2d(in_channels)\n        self.conv1 = nn.Conv2d(in_channels,\n                               bottleneck_channels,\n                               kernel_size=1,\n                               stride=1,\n                               padding=0,\n                               bias=False)\n\n        self.bn2 = nn.BatchNorm2d(bottleneck_channels)\n        self.conv2 = nn.Conv2d(bottleneck_channels,\n                               out_channels,\n                               kernel_size=3,\n                               stride=1,\n                               padding=1,\n                               bias=False)\n\n    def forward(self, x):\n        y = self.conv1(F.relu(self.bn1(x), inplace=True))\n        if self.drop_rate > 0:\n            y = F.dropout(y,\n                          p=self.drop_rate,\n                          training=self.training,\n                          inplace=False)\n        y = self.conv2(F.relu(self.bn2(y), inplace=True))\n        if self.drop_rate > 0:\n            y = F.dropout(y,\n                          p=self.drop_rate,\n                          training=self.training,\n                          inplace=False)\n        return torch.cat([x, y], dim=1)\n\n\nclass TransitionBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, drop_rate):\n        super().__init__()\n\n        self.drop_rate = drop_rate\n\n        self.bn = nn.BatchNorm2d(in_channels)\n        self.conv = nn.Conv2d(in_channels,\n                              out_channels,\n                              kernel_size=1,\n                              stride=1,\n                              padding=0,\n                              bias=False)\n\n    def forward(self, x):\n        x = self.conv(F.relu(self.bn(x), inplace=True))\n        if self.drop_rate > 0:\n            x = F.dropout(x,\n                          p=self.drop_rate,\n                          training=self.training,\n                          inplace=False)\n        x = F.avg_pool2d(x, kernel_size=2, stride=2)\n        return x\n\n\nclass Network(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n\n        model_config = config.model.densenet\n        depth = model_config.depth\n        block_type = model_config.block_type\n        self.growth_rate = model_config.growth_rate\n        self.drop_rate = model_config.drop_rate\n        self.compression_rate = model_config.compression_rate\n\n        assert block_type in ['basic', 'bottleneck']\n        if block_type == 'basic':\n            block = BasicBlock\n            n_blocks_per_stage = (depth - 4) // 3\n            assert n_blocks_per_stage * 3 + 4 == depth\n        else:\n            block = BottleneckBlock\n            n_blocks_per_stage = (depth - 4) // 6\n            assert n_blocks_per_stage * 6 + 4 == depth\n\n        in_channels = [2 * self.growth_rate]\n        for index in range(3):\n            denseblock_out_channels = int(in_channels[-1] +\n                                          n_blocks_per_stage *\n                                          self.growth_rate)\n            if index < 2:\n                transitionblock_out_channels = int(denseblock_out_channels *\n                                                   self.compression_rate)\n            else:\n                transitionblock_out_channels = denseblock_out_channels\n            in_channels.append(transitionblock_out_channels)\n\n        self.conv = nn.Conv2d(config.dataset.n_channels,\n                              in_channels[0],\n                              kernel_size=3,\n                              stride=1,\n                              padding=1,\n                              bias=False)\n        self.stage1 = self._make_stage(in_channels[0], n_blocks_per_stage,\n                                       block, True)\n        self.stage2 = self._make_stage(in_channels[1], n_blocks_per_stage,\n                                       block, True)\n        self.stage3 = self._make_stage(in_channels[2], n_blocks_per_stage,\n                                       block, False)\n        self.bn = nn.BatchNorm2d(in_channels[3])\n\n        # compute conv feature size\n        with torch.no_grad():\n            dummy_data = torch.zeros(\n                (1, config.dataset.n_channels, config.dataset.image_size,\n                 config.dataset.image_size),\n                dtype=torch.float32)\n            self.feature_size = self._forward_conv(dummy_data).view(\n                -1).shape[0]\n\n        self.fc = nn.Linear(self.feature_size, config.dataset.n_classes)\n\n        # initialize weights\n        initializer = create_initializer(config.model.init_mode)\n        self.apply(initializer)\n\n    def _make_stage(self, in_channels, n_blocks, block, add_transition_block):\n        stage = nn.Sequential()\n        for index in range(n_blocks):\n            stage.add_module(\n                f'block{index + 1}',\n                block(in_channels + index * self.growth_rate, self.growth_rate,\n                      self.drop_rate))\n        if add_transition_block:\n            in_channels = int(in_channels + n_blocks * self.growth_rate)\n            out_channels = int(in_channels * self.compression_rate)\n            stage.add_module(\n                'transition',\n                TransitionBlock(in_channels, out_channels, self.drop_rate))\n        return stage\n\n    def _forward_conv(self, x):\n        x = self.conv(x)\n        x = self.stage1(x)\n        x = self.stage2(x)\n        x = self.stage3(x)\n        x = F.relu(self.bn(x), inplace=True)\n        x = F.adaptive_avg_pool2d(x, output_size=1)\n        return x\n\n    def forward(self, x):\n        x = self._forward_conv(x)\n        x = x.view(x.size(0), -1)\n        x = self.fc(x)\n        return x\n"""
pytorch_image_classification/models/cifar/pyramidnet.py,5,"b""import numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom ..initializer import create_initializer\n\n\nclass BasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self, in_channels, out_channels, stride):\n        super().__init__()\n\n        self.bn1 = nn.BatchNorm2d(in_channels)\n        self.conv1 = nn.Conv2d(\n            in_channels,\n            out_channels,\n            kernel_size=3,\n            stride=stride,  # downsample with first conv\n            padding=1,\n            bias=False)\n        self.bn2 = nn.BatchNorm2d(out_channels)\n        self.conv2 = nn.Conv2d(out_channels,\n                               out_channels,\n                               kernel_size=3,\n                               stride=1,\n                               padding=1,\n                               bias=False)\n        self.bn3 = nn.BatchNorm2d(out_channels)\n\n        self.shortcut = nn.Sequential()\n        if stride > 1:\n            self.shortcut = nn.AvgPool2d(kernel_size=2, stride=2)\n\n    def forward(self, x):\n        y = self.bn1(x)\n        y = self.conv1(y)\n\n        y = F.relu(self.bn2(y), inplace=True)\n        y = self.conv2(y)\n\n        y = self.bn3(y)\n\n        if y.size(1) != x.size(1):\n            y += F.pad(self.shortcut(x),\n                       (0, 0, 0, 0, 0, y.size(1) - x.size(1)), 'constant', 0)\n        else:\n            y += self.shortcut(x)\n        return y\n\n\nclass BottleneckBlock(nn.Module):\n    expansion = 4\n\n    def __init__(self, in_channels, out_channels, stride):\n        super().__init__()\n\n        bottleneck_channels = out_channels // self.expansion\n\n        self.bn1 = nn.BatchNorm2d(in_channels)\n        self.conv1 = nn.Conv2d(in_channels,\n                               bottleneck_channels,\n                               kernel_size=1,\n                               stride=1,\n                               padding=0,\n                               bias=False)\n        self.bn2 = nn.BatchNorm2d(bottleneck_channels)\n        self.conv2 = nn.Conv2d(\n            bottleneck_channels,\n            bottleneck_channels,\n            kernel_size=3,\n            stride=stride,  # downsample with 3x3 conv\n            padding=1,\n            bias=False)\n        self.bn3 = nn.BatchNorm2d(bottleneck_channels)\n        self.conv3 = nn.Conv2d(bottleneck_channels,\n                               out_channels,\n                               kernel_size=1,\n                               stride=1,\n                               padding=0,\n                               bias=False)\n\n        self.bn4 = nn.BatchNorm2d(out_channels)\n\n        self.shortcut = nn.Sequential()  # identity\n        if stride > 1:\n            self.shortcut = nn.AvgPool2d(kernel_size=2, stride=2)\n\n    def forward(self, x):\n        y = self.bn1(x)\n        y = self.conv1(y)\n\n        y = F.relu(self.bn2(y), inplace=True)\n        y = self.conv2(y)\n\n        y = F.relu(self.bn3(y), inplace=True)\n        y = self.conv3(y)\n\n        y = self.bn4(y)\n\n        if y.size(1) != x.size(1):\n            y += F.pad(self.shortcut(x),\n                       (0, 0, 0, 0, 0, y.size(1) - x.size(1)), 'constant', 0)\n        else:\n            y += self.shortcut(x)\n        return y\n\n\nclass Network(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n\n        model_config = config.model.pyramidnet\n        depth = model_config.depth\n        initial_channels = model_config.initial_channels\n        block_type = model_config.block_type\n        alpha = model_config.alpha\n\n        assert block_type in ['basic', 'bottleneck']\n        if block_type == 'basic':\n            block = BasicBlock\n            n_blocks_per_stage = (depth - 2) // 6\n            assert n_blocks_per_stage * 6 + 2 == depth\n        else:\n            block = BottleneckBlock\n            n_blocks_per_stage = (depth - 2) // 9\n            assert n_blocks_per_stage * 9 + 2 == depth\n\n        n_channels = [initial_channels]\n        for _ in range(n_blocks_per_stage * 3):\n            num = n_channels[-1] + alpha / (n_blocks_per_stage * 3)\n            n_channels.append(num)\n        n_channels = [int(np.round(c)) * block.expansion for c in n_channels]\n        n_channels[0] //= block.expansion\n\n        self.conv = nn.Conv2d(config.dataset.n_channels,\n                              n_channels[0],\n                              kernel_size=(3, 3),\n                              stride=1,\n                              padding=1,\n                              bias=False)\n        self.bn1 = nn.BatchNorm2d(n_channels[0])\n\n        self.stage1 = self._make_stage(n_channels[:n_blocks_per_stage + 1],\n                                       n_blocks_per_stage,\n                                       block,\n                                       stride=1)\n        self.stage2 = self._make_stage(\n            n_channels[n_blocks_per_stage:n_blocks_per_stage * 2 + 1],\n            n_blocks_per_stage,\n            block,\n            stride=2)\n        self.stage3 = self._make_stage(n_channels[n_blocks_per_stage * 2:],\n                                       n_blocks_per_stage,\n                                       block,\n                                       stride=2)\n\n        self.bn2 = nn.BatchNorm2d(n_channels[-1])\n\n        # compute conv feature size\n        with torch.no_grad():\n            dummy_data = torch.zeros(\n                (1, config.dataset.n_channels, config.dataset.image_size,\n                 config.dataset.image_size),\n                dtype=torch.float32)\n            self.feature_size = self._forward_conv(dummy_data).view(\n                -1).shape[0]\n\n        self.fc = nn.Linear(self.feature_size, config.dataset.n_classes)\n\n        # initialize weights\n        initializer = create_initializer(config.model.init_mode)\n        self.apply(initializer)\n\n    def _make_stage(self, n_channels, n_blocks, block, stride):\n        stage = nn.Sequential()\n        for index in range(n_blocks):\n            block_name = f'block{index + 1}'\n            if index == 0:\n                stage.add_module(\n                    block_name,\n                    block(n_channels[index],\n                          n_channels[index + 1],\n                          stride=stride))\n            else:\n                stage.add_module(\n                    block_name,\n                    block(n_channels[index], n_channels[index + 1], stride=1))\n        return stage\n\n    def _forward_conv(self, x):\n        x = self.conv(x)\n        x = self.bn1(x)\n        x = self.stage1(x)\n        x = self.stage2(x)\n        x = self.stage3(x)\n        x = F.relu(self.bn2(x),\n                   inplace=True)  # apply BN and ReLU before average pooling\n        x = F.adaptive_avg_pool2d(x, output_size=1)\n        return x\n\n    def forward(self, x):\n        x = self._forward_conv(x)\n        x = x.view(x.size(0), -1)\n        x = self.fc(x)\n        return x\n"""
pytorch_image_classification/models/cifar/resnet.py,5,"b""import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom ..initializer import create_initializer\n\n\nclass BasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self, in_channels, out_channels, stride):\n        super().__init__()\n\n        self.conv1 = nn.Conv2d(\n            in_channels,\n            out_channels,\n            kernel_size=3,\n            stride=stride,  # downsample with first conv\n            padding=1,\n            bias=False)\n        self.bn1 = nn.BatchNorm2d(out_channels)\n        self.conv2 = nn.Conv2d(out_channels,\n                               out_channels,\n                               kernel_size=3,\n                               stride=1,\n                               padding=1,\n                               bias=False)\n        self.bn2 = nn.BatchNorm2d(out_channels)\n\n        self.shortcut = nn.Sequential()\n        if in_channels != out_channels:\n            self.shortcut.add_module(\n                'conv',\n                nn.Conv2d(\n                    in_channels,\n                    out_channels,\n                    kernel_size=1,\n                    stride=stride,  # downsample\n                    padding=0,\n                    bias=False))\n            self.shortcut.add_module('bn', nn.BatchNorm2d(out_channels))  # BN\n\n    def forward(self, x):\n        y = F.relu(self.bn1(self.conv1(x)), inplace=True)\n        y = self.bn2(self.conv2(y))\n        y += self.shortcut(x)\n        y = F.relu(y, inplace=True)  # apply ReLU after addition\n        return y\n\n\nclass BottleneckBlock(nn.Module):\n    expansion = 4\n\n    def __init__(self, in_channels, out_channels, stride):\n        super().__init__()\n\n        bottleneck_channels = out_channels // self.expansion\n\n        self.conv1 = nn.Conv2d(in_channels,\n                               bottleneck_channels,\n                               kernel_size=1,\n                               stride=1,\n                               padding=0,\n                               bias=False)\n        self.bn1 = nn.BatchNorm2d(bottleneck_channels)\n\n        self.conv2 = nn.Conv2d(\n            bottleneck_channels,\n            bottleneck_channels,\n            kernel_size=3,\n            stride=stride,  # downsample with 3x3 conv\n            padding=1,\n            bias=False)\n        self.bn2 = nn.BatchNorm2d(bottleneck_channels)\n\n        self.conv3 = nn.Conv2d(bottleneck_channels,\n                               out_channels,\n                               kernel_size=1,\n                               stride=1,\n                               padding=0,\n                               bias=False)\n        self.bn3 = nn.BatchNorm2d(out_channels)\n\n        self.shortcut = nn.Sequential()  # identity\n        if in_channels != out_channels:\n            self.shortcut.add_module(\n                'conv',\n                nn.Conv2d(\n                    in_channels,\n                    out_channels,\n                    kernel_size=1,\n                    stride=stride,  # downsample\n                    padding=0,\n                    bias=False))\n            self.shortcut.add_module('bn', nn.BatchNorm2d(out_channels))  # BN\n\n    def forward(self, x):\n        y = F.relu(self.bn1(self.conv1(x)), inplace=True)\n        y = F.relu(self.bn2(self.conv2(y)), inplace=True)\n        y = self.bn3(self.conv3(y))  # not apply ReLU\n        y += self.shortcut(x)\n        y = F.relu(y, inplace=True)  # apply ReLU after addition\n        return y\n\n\nclass Network(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n\n        model_config = config.model.resnet\n        depth = model_config.depth\n        initial_channels = model_config.initial_channels\n        block_type = model_config.block_type\n\n        assert block_type in ['basic', 'bottleneck']\n        if block_type == 'basic':\n            block = BasicBlock\n            n_blocks_per_stage = (depth - 2) // 6\n            assert n_blocks_per_stage * 6 + 2 == depth\n        else:\n            block = BottleneckBlock\n            n_blocks_per_stage = (depth - 2) // 9\n            assert n_blocks_per_stage * 9 + 2 == depth\n\n        n_channels = [\n            initial_channels,\n            initial_channels * 2 * block.expansion,\n            initial_channels * 4 * block.expansion,\n        ]\n\n        self.conv = nn.Conv2d(config.dataset.n_channels,\n                              n_channels[0],\n                              kernel_size=3,\n                              stride=1,\n                              padding=1,\n                              bias=False)\n        self.bn = nn.BatchNorm2d(initial_channels)\n\n        self.stage1 = self._make_stage(n_channels[0],\n                                       n_channels[0],\n                                       n_blocks_per_stage,\n                                       block,\n                                       stride=1)\n        self.stage2 = self._make_stage(n_channels[0],\n                                       n_channels[1],\n                                       n_blocks_per_stage,\n                                       block,\n                                       stride=2)\n        self.stage3 = self._make_stage(n_channels[1],\n                                       n_channels[2],\n                                       n_blocks_per_stage,\n                                       block,\n                                       stride=2)\n\n        # compute conv feature size\n        with torch.no_grad():\n            dummy_data = torch.zeros(\n                (1, config.dataset.n_channels, config.dataset.image_size,\n                 config.dataset.image_size),\n                dtype=torch.float32)\n            self.feature_size = self._forward_conv(dummy_data).view(\n                -1).shape[0]\n\n        self.fc = nn.Linear(self.feature_size, config.dataset.n_classes)\n\n        # initialize weights\n        initializer = create_initializer(config.model.init_mode)\n        self.apply(initializer)\n\n    def _make_stage(self, in_channels, out_channels, n_blocks, block, stride):\n        stage = nn.Sequential()\n        for index in range(n_blocks):\n            block_name = f'block{index + 1}'\n            if index == 0:\n                stage.add_module(\n                    block_name, block(in_channels, out_channels,\n                                      stride=stride))\n            else:\n                stage.add_module(block_name,\n                                 block(out_channels, out_channels, stride=1))\n        return stage\n\n    def _forward_conv(self, x):\n        x = F.relu(self.bn(self.conv(x)), inplace=True)\n        x = self.stage1(x)\n        x = self.stage2(x)\n        x = self.stage3(x)\n        x = F.adaptive_avg_pool2d(x, output_size=1)\n        return x\n\n    def forward(self, x):\n        x = self._forward_conv(x)\n        x = x.view(x.size(0), -1)\n        x = self.fc(x)\n        return x\n"""
pytorch_image_classification/models/cifar/resnet_preact.py,5,"b""import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom ..initializer import create_initializer\n\n\nclass BasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self,\n                 in_channels,\n                 out_channels,\n                 stride,\n                 remove_first_relu,\n                 add_last_bn,\n                 preact=False):\n        super().__init__()\n\n        self._remove_first_relu = remove_first_relu\n        self._add_last_bn = add_last_bn\n        self._preact = preact\n\n        self.bn1 = nn.BatchNorm2d(in_channels)\n        self.conv1 = nn.Conv2d(\n            in_channels,\n            out_channels,\n            kernel_size=3,\n            stride=stride,  # downsample with first conv\n            padding=1,\n            bias=False)\n        self.bn2 = nn.BatchNorm2d(out_channels)\n        self.conv2 = nn.Conv2d(out_channels,\n                               out_channels,\n                               kernel_size=3,\n                               stride=1,\n                               padding=1,\n                               bias=False)\n\n        if add_last_bn:\n            self.bn3 = nn.BatchNorm2d(out_channels)\n\n        self.shortcut = nn.Sequential()\n        if in_channels != out_channels:\n            self.shortcut.add_module(\n                'conv',\n                nn.Conv2d(\n                    in_channels,\n                    out_channels,\n                    kernel_size=1,\n                    stride=stride,  # downsample\n                    padding=0,\n                    bias=False))\n\n    def forward(self, x):\n        if self._preact:\n            x = F.relu(self.bn1(x),\n                       inplace=True)  # shortcut after preactivation\n            y = self.conv1(x)\n        else:\n            # preactivation only for residual path\n            y = self.bn1(x)\n            if not self._remove_first_relu:\n                y = F.relu(y, inplace=True)\n            y = self.conv1(y)\n\n        y = F.relu(self.bn2(y), inplace=True)\n        y = self.conv2(y)\n\n        if self._add_last_bn:\n            y = self.bn3(y)\n\n        y += self.shortcut(x)\n        return y\n\n\nclass BottleneckBlock(nn.Module):\n    expansion = 4\n\n    def __init__(self,\n                 in_channels,\n                 out_channels,\n                 stride,\n                 remove_first_relu,\n                 add_last_bn,\n                 preact=False):\n        super().__init__()\n\n        self._remove_first_relu = remove_first_relu\n        self._add_last_bn = add_last_bn\n        self._preact = preact\n\n        bottleneck_channels = out_channels // self.expansion\n\n        self.bn1 = nn.BatchNorm2d(in_channels)\n        self.conv1 = nn.Conv2d(in_channels,\n                               bottleneck_channels,\n                               kernel_size=1,\n                               stride=1,\n                               padding=0,\n                               bias=False)\n        self.bn2 = nn.BatchNorm2d(bottleneck_channels)\n        self.conv2 = nn.Conv2d(\n            bottleneck_channels,\n            bottleneck_channels,\n            kernel_size=3,\n            stride=stride,  # downsample with 3x3 conv\n            padding=1,\n            bias=False)\n        self.bn3 = nn.BatchNorm2d(bottleneck_channels)\n        self.conv3 = nn.Conv2d(bottleneck_channels,\n                               out_channels,\n                               kernel_size=1,\n                               stride=1,\n                               padding=0,\n                               bias=False)\n\n        if add_last_bn:\n            self.bn4 = nn.BatchNorm2d(out_channels)\n\n        self.shortcut = nn.Sequential()  # identity\n        if in_channels != out_channels:\n            self.shortcut.add_module(\n                'conv',\n                nn.Conv2d(\n                    in_channels,\n                    out_channels,\n                    kernel_size=1,\n                    stride=stride,  # downsample\n                    padding=0,\n                    bias=False))\n\n    def forward(self, x):\n        if self._preact:\n            x = F.relu(self.bn1(x),\n                       inplace=True)  # shortcut after preactivation\n            y = self.conv1(x)\n        else:\n            # preactivation only for residual path\n            y = self.bn1(x)\n            if not self._remove_first_relu:\n                y = F.relu(y, inplace=True)\n            y = self.conv1(y)\n\n        y = F.relu(self.bn2(y), inplace=True)\n        y = self.conv2(y)\n        y = F.relu(self.bn3(y), inplace=True)\n        y = self.conv3(y)\n\n        if self._add_last_bn:\n            y = self.bn4(y)\n\n        y += self.shortcut(x)\n        return y\n\n\nclass Network(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n\n        model_config = config.model.resnet_preact\n        initial_channels = model_config.initial_channels\n        self._remove_first_relu = model_config.remove_first_relu\n        self._add_last_bn = model_config.add_last_bn\n        block_type = model_config.block_type\n        depth = model_config.depth\n        preact_stage = model_config.preact_stage\n\n        assert block_type in ['basic', 'bottleneck']\n        if block_type == 'basic':\n            block = BasicBlock\n            n_blocks_per_stage = (depth - 2) // 6\n            assert n_blocks_per_stage * 6 + 2 == depth\n        else:\n            block = BottleneckBlock\n            n_blocks_per_stage = (depth - 2) // 9\n            assert n_blocks_per_stage * 9 + 2 == depth\n\n        n_channels = [\n            initial_channels,\n            initial_channels * 2 * block.expansion,\n            initial_channels * 4 * block.expansion,\n        ]\n\n        self.conv = nn.Conv2d(config.dataset.n_channels,\n                              n_channels[0],\n                              kernel_size=(3, 3),\n                              stride=1,\n                              padding=1,\n                              bias=False)\n\n        self.stage1 = self._make_stage(n_channels[0],\n                                       n_channels[0],\n                                       n_blocks_per_stage,\n                                       block,\n                                       stride=1,\n                                       preact=preact_stage[0])\n        self.stage2 = self._make_stage(n_channels[0],\n                                       n_channels[1],\n                                       n_blocks_per_stage,\n                                       block,\n                                       stride=2,\n                                       preact=preact_stage[1])\n        self.stage3 = self._make_stage(n_channels[1],\n                                       n_channels[2],\n                                       n_blocks_per_stage,\n                                       block,\n                                       stride=2,\n                                       preact=preact_stage[2])\n        self.bn = nn.BatchNorm2d(n_channels[2])\n\n        # compute conv feature size\n        with torch.no_grad():\n            dummy_data = torch.zeros(\n                (1, config.dataset.n_channels, config.dataset.image_size,\n                 config.dataset.image_size),\n                dtype=torch.float32)\n            self.feature_size = self._forward_conv(dummy_data).view(\n                -1).shape[0]\n\n        self.fc = nn.Linear(self.feature_size, config.dataset.n_classes)\n\n        # initialize weights\n        initializer = create_initializer(config.model.init_mode)\n        self.apply(initializer)\n\n    def _make_stage(self, in_channels, out_channels, n_blocks, block, stride,\n                    preact):\n        stage = nn.Sequential()\n        for index in range(n_blocks):\n            block_name = f'block{index + 1}'\n            if index == 0:\n                stage.add_module(\n                    block_name,\n                    block(in_channels,\n                          out_channels,\n                          stride=stride,\n                          remove_first_relu=self._remove_first_relu,\n                          add_last_bn=self._add_last_bn,\n                          preact=preact))\n            else:\n                stage.add_module(\n                    block_name,\n                    block(out_channels,\n                          out_channels,\n                          stride=1,\n                          remove_first_relu=self._remove_first_relu,\n                          add_last_bn=self._add_last_bn,\n                          preact=False))\n        return stage\n\n    def _forward_conv(self, x):\n        x = self.conv(x)\n        x = self.stage1(x)\n        x = self.stage2(x)\n        x = self.stage3(x)\n        x = F.relu(self.bn(x),\n                   inplace=True)  # apply BN and ReLU before average pooling\n        x = F.adaptive_avg_pool2d(x, output_size=1)\n        return x\n\n    def forward(self, x):\n        x = self._forward_conv(x)\n        x = x.view(x.size(0), -1)\n        x = self.fc(x)\n        return x\n"""
pytorch_image_classification/models/cifar/resnext.py,5,"b""import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom ..initializer import create_initializer\n\n\nclass BottleneckBlock(nn.Module):\n    expansion = 4\n\n    def __init__(self, in_channels, out_channels, stride, stage_index,\n                 base_channels, cardinality):\n        super().__init__()\n\n        bottleneck_channels = cardinality * base_channels * 2**stage_index\n\n        self.conv1 = nn.Conv2d(in_channels,\n                               bottleneck_channels,\n                               kernel_size=1,\n                               stride=1,\n                               padding=0,\n                               bias=False)\n        self.bn1 = nn.BatchNorm2d(bottleneck_channels)\n\n        self.conv2 = nn.Conv2d(\n            bottleneck_channels,\n            bottleneck_channels,\n            kernel_size=3,\n            stride=stride,  # downsample with 3x3 conv\n            padding=1,\n            groups=cardinality,\n            bias=False)\n        self.bn2 = nn.BatchNorm2d(bottleneck_channels)\n\n        self.conv3 = nn.Conv2d(bottleneck_channels,\n                               out_channels,\n                               kernel_size=1,\n                               stride=1,\n                               padding=0,\n                               bias=False)\n        self.bn3 = nn.BatchNorm2d(out_channels)\n\n        self.shortcut = nn.Sequential()  # identity\n        if in_channels != out_channels:\n            self.shortcut.add_module(\n                'conv',\n                nn.Conv2d(\n                    in_channels,\n                    out_channels,\n                    kernel_size=1,\n                    stride=stride,  # downsample\n                    padding=0,\n                    bias=False))\n            self.shortcut.add_module('bn', nn.BatchNorm2d(out_channels))  # BN\n\n    def forward(self, x):\n        y = F.relu(self.bn1(self.conv1(x)), inplace=True)\n        y = F.relu(self.bn2(self.conv2(y)), inplace=True)\n        y = self.bn3(self.conv3(y))  # not apply ReLU\n        y += self.shortcut(x)\n        y = F.relu(y, inplace=True)  # apply ReLU after addition\n        return y\n\n\nclass Network(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n\n        model_config = config.model.resnext\n        depth = model_config.depth\n        initial_channels = model_config.initial_channels\n        self.base_channels = model_config.base_channels\n        self.cardinality = model_config.cardinality\n\n        n_blocks_per_stage = (depth - 2) // 9\n        assert n_blocks_per_stage * 9 + 2 == depth\n        block = BottleneckBlock\n\n        n_channels = [\n            initial_channels,\n            initial_channels * block.expansion,\n            initial_channels * 2 * block.expansion,\n            initial_channels * 4 * block.expansion,\n        ]\n\n        self.conv = nn.Conv2d(config.dataset.n_channels,\n                              n_channels[0],\n                              kernel_size=3,\n                              stride=1,\n                              padding=1,\n                              bias=False)\n        self.bn = nn.BatchNorm2d(n_channels[0])\n\n        self.stage1 = self._make_stage(n_channels[0],\n                                       n_channels[1],\n                                       n_blocks_per_stage,\n                                       0,\n                                       stride=1)\n        self.stage2 = self._make_stage(n_channels[1],\n                                       n_channels[2],\n                                       n_blocks_per_stage,\n                                       1,\n                                       stride=2)\n        self.stage3 = self._make_stage(n_channels[2],\n                                       n_channels[3],\n                                       n_blocks_per_stage,\n                                       2,\n                                       stride=2)\n\n        # compute conv feature size\n        with torch.no_grad():\n            dummy_data = torch.zeros(\n                (1, config.dataset.n_channels, config.dataset.image_size,\n                 config.dataset.image_size),\n                dtype=torch.float32)\n            self.feature_size = self._forward_conv(dummy_data).view(\n                -1).shape[0]\n\n        self.fc = nn.Linear(self.feature_size, config.dataset.n_classes)\n\n        # initialize weights\n        initializer = create_initializer(config.model.init_mode)\n        self.apply(initializer)\n\n    def _make_stage(self, in_channels, out_channels, n_blocks, stage_index,\n                    stride):\n        stage = nn.Sequential()\n        for index in range(n_blocks):\n            block_name = f'block{index + 1}'\n            if index == 0:\n                stage.add_module(\n                    block_name,\n                    BottleneckBlock(\n                        in_channels,\n                        out_channels,\n                        stride,  # downsample\n                        stage_index,\n                        self.base_channels,\n                        self.cardinality))\n            else:\n                stage.add_module(\n                    block_name,\n                    BottleneckBlock(\n                        out_channels,\n                        out_channels,\n                        1,  # no downsampling\n                        stage_index,\n                        self.base_channels,\n                        self.cardinality))\n        return stage\n\n    def _forward_conv(self, x):\n        x = F.relu(self.bn(self.conv(x)), inplace=True)\n        x = self.stage1(x)\n        x = self.stage2(x)\n        x = self.stage3(x)\n        x = F.adaptive_avg_pool2d(x, output_size=1)\n        return x\n\n    def forward(self, x):\n        x = self._forward_conv(x)\n        x = x.view(x.size(0), -1)\n        x = self.fc(x)\n        return x\n"""
pytorch_image_classification/models/cifar/se_resnet_preact.py,5,"b""import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\ndef initialize_weights(module):\n    if isinstance(module, nn.Conv2d):\n        nn.init.kaiming_normal_(module.weight.data,\n                                mode='fan_out',\n                                nonlinearity='relu')\n    elif isinstance(module, nn.BatchNorm2d):\n        nn.init.ones_(module.weight.data)\n        nn.init.zeros_(module.biasd.data)\n    elif isinstance(module, nn.Linear):\n        nn.init.kaiming_normal_(module.weight.data,\n                                mode='fan_out',\n                                nonlinearity='relu')\n        nn.init.zeros_(module.biasd.data)\n\n\nclass SELayer(nn.Module):\n    def __init__(self, in_channels, reduction):\n        super().__init__()\n        mid_channels = in_channels // reduction\n\n        self.fc1 = nn.Linear(in_channels, mid_channels)\n        self.fc2 = nn.Linear(mid_channels, in_channels)\n\n    def forward(self, x):\n        n_batches, n_channels, _, _ = x.size()\n        y = F.adaptive_avg_pool2d(x, output_size=1).view(n_batches, n_channels)\n        y = F.relu(self.fc1(y), inplace=True)\n        y = F.sigmoid(self.fc2(y)).view(n_batches, n_channels, 1, 1)\n        return x * y\n\n\nclass BasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self,\n                 in_channels,\n                 out_channels,\n                 stride,\n                 remove_first_relu,\n                 add_last_bn,\n                 se_reduction,\n                 preact=False):\n        super().__init__()\n\n        self._remove_first_relu = remove_first_relu\n        self._add_last_bn = add_last_bn\n        self._preact = preact\n\n        self.bn1 = nn.BatchNorm2d(in_channels)\n        self.conv1 = nn.Conv2d(\n            in_channels,\n            out_channels,\n            kernel_size=3,\n            stride=stride,  # downsample with first conv\n            padding=1,\n            bias=False)\n        self.bn2 = nn.BatchNorm2d(out_channels)\n        self.conv2 = nn.Conv2d(out_channels,\n                               out_channels,\n                               kernel_size=3,\n                               stride=1,\n                               padding=1,\n                               bias=False)\n\n        if add_last_bn:\n            self.bn3 = nn.BatchNorm2d(out_channels)\n\n        self.se = SELayer(out_channels, se_reduction)\n\n        self.shortcut = nn.Sequential()\n        if in_channels != out_channels:\n            self.shortcut.add_module(\n                'conv',\n                nn.Conv2d(\n                    in_channels,\n                    out_channels,\n                    kernel_size=1,\n                    stride=stride,  # downsample\n                    padding=0,\n                    bias=False))\n\n    def forward(self, x):\n        if self._preact:\n            x = F.relu(self.bn1(x),\n                       inplace=True)  # shortcut after preactivation\n            y = self.conv1(x)\n        else:\n            # preactivation only for residual path\n            y = self.bn1(x)\n            if not self._remove_first_relu:\n                y = F.relu(y, inplace=True)\n            y = self.conv1(y)\n\n        y = F.relu(self.bn2(y), inplace=True)\n        y = self.conv2(y)\n\n        if self._add_last_bn:\n            y = self.bn3(y)\n\n        y = self.se(y)\n        y += self.shortcut(x)\n        return y\n\n\nclass BottleneckBlock(nn.Module):\n    expansion = 4\n\n    def __init__(self,\n                 in_channels,\n                 out_channels,\n                 stride,\n                 remove_first_relu,\n                 add_last_bn,\n                 se_reduction,\n                 preact=False):\n        super().__init__()\n\n        self._remove_first_relu = remove_first_relu\n        self._add_last_bn = add_last_bn\n        self._preact = preact\n\n        bottleneck_channels = out_channels // self.expansion\n\n        self.bn1 = nn.BatchNorm2d(in_channels)\n        self.conv1 = nn.Conv2d(in_channels,\n                               bottleneck_channels,\n                               kernel_size=1,\n                               stride=1,\n                               padding=0,\n                               bias=False)\n        self.bn2 = nn.BatchNorm2d(bottleneck_channels)\n        self.conv2 = nn.Conv2d(\n            bottleneck_channels,\n            bottleneck_channels,\n            kernel_size=3,\n            stride=stride,  # downsample with 3x3 conv\n            padding=1,\n            bias=False)\n        self.bn3 = nn.BatchNorm2d(bottleneck_channels)\n        self.conv3 = nn.Conv2d(bottleneck_channels,\n                               out_channels,\n                               kernel_size=1,\n                               stride=1,\n                               padding=0,\n                               bias=False)\n\n        if add_last_bn:\n            self.bn4 = nn.BatchNorm2d(out_channels)\n\n        self.se = SELayer(out_channels, se_reduction)\n\n        self.shortcut = nn.Sequential()  # identity\n        if in_channels != out_channels:\n            self.shortcut.add_module(\n                'conv',\n                nn.Conv2d(\n                    in_channels,\n                    out_channels,\n                    kernel_size=1,\n                    stride=stride,  # downsample\n                    padding=0,\n                    bias=False))\n\n    def forward(self, x):\n        if self._preact:\n            x = F.relu(self.bn1(x),\n                       inplace=True)  # shortcut after preactivation\n            y = self.conv1(x)\n        else:\n            # preactivation only for residual path\n            y = self.bn1(x)\n            if not self._remove_first_relu:\n                y = F.relu(y, inplace=True)\n            y = self.conv1(y)\n\n        y = F.relu(self.bn2(y), inplace=True)\n        y = self.conv2(y)\n        y = F.relu(self.bn3(y), inplace=True)\n        y = self.conv3(y)\n\n        if self._add_last_bn:\n            y = self.bn4(y)\n\n        y = self.se(y)\n        y += self.shortcut(x)\n        return y\n\n\nclass Network(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n\n        initial_channels = config.model.se_resnet_preact.initial_channels\n        block_type = config.model.se_resnet_preact.block_type\n        depth = config.model.se_resnet_preact.depth\n        self._remove_first_relu = config.model.se_resnet_preact.remove_first_relu\n        self._add_last_bn = config.model.se_resnet_preact.add_last_bn\n        preact_stage = config.model.se_resnet_preact.preact_stage\n        se_reduction = config.model.se_resnet_preact.se_reduction\n\n        assert block_type in ['basic', 'bottleneck']\n        if block_type == 'basic':\n            block = BasicBlock\n            n_blocks_per_stage = (depth - 2) // 6\n            assert n_blocks_per_stage * 6 + 2 == depth\n        else:\n            block = BottleneckBlock\n            n_blocks_per_stage = (depth - 2) // 9\n            assert n_blocks_per_stage * 9 + 2 == depth\n\n        n_channels = [\n            initial_channels,\n            initial_channels * 2 * block.expansion,\n            initial_channels * 4 * block.expansion,\n        ]\n\n        self.conv = nn.Conv2d(config.dataset.n_channels,\n                              n_channels[0],\n                              kernel_size=(3, 3),\n                              stride=1,\n                              padding=1,\n                              bias=False)\n\n        self.stage1 = self._make_stage(n_channels[0],\n                                       n_channels[0],\n                                       n_blocks_per_stage,\n                                       block,\n                                       stride=1,\n                                       se_reduction=se_reduction,\n                                       preact=preact_stage[0])\n        self.stage2 = self._make_stage(n_channels[0],\n                                       n_channels[1],\n                                       n_blocks_per_stage,\n                                       block,\n                                       stride=2,\n                                       se_reduction=se_reduction,\n                                       preact=preact_stage[1])\n        self.stage3 = self._make_stage(n_channels[1],\n                                       n_channels[2],\n                                       n_blocks_per_stage,\n                                       block,\n                                       stride=2,\n                                       se_reduction=se_reduction,\n                                       preact=preact_stage[2])\n        self.bn = nn.BatchNorm2d(n_channels[2])\n\n        # compute conv feature size\n        with torch.no_grad():\n            dummy_data = torch.zeros(\n                (1, config.dataset.n_channels, config.dataset.image_size,\n                 config.dataset.image_size),\n                dtype=torch.float32)\n            self.feature_size = self._forward_conv(dummy_data).view(\n                -1).shape[0]\n\n        self.fc = nn.Linear(self.feature_size, config.dataset.n_classes)\n\n        # initialize weights\n        self.apply(initialize_weights)\n\n    def _make_stage(self, in_channels, out_channels, n_blocks, block, stride,\n                    se_reduction, preact):\n        stage = nn.Sequential()\n        for index in range(n_blocks):\n            block_name = f'block{index + 1}'\n            if index == 0:\n                stage.add_module(\n                    block_name,\n                    block(in_channels,\n                          out_channels,\n                          stride=stride,\n                          remove_first_relu=self._remove_first_relu,\n                          add_last_bn=self._add_last_bn,\n                          se_reduction=se_reduction,\n                          preact=preact))\n            else:\n                stage.add_module(\n                    block_name,\n                    block(out_channels,\n                          out_channels,\n                          stride=1,\n                          remove_first_relu=self._remove_first_relu,\n                          add_last_bn=self._add_last_bn,\n                          se_reduction=se_reduction,\n                          preact=False))\n        return stage\n\n    def _forward_conv(self, x):\n        x = self.conv(x)\n        x = self.stage1(x)\n        x = self.stage2(x)\n        x = self.stage3(x)\n        x = F.relu(self.bn(x),\n                   inplace=True)  # apply BN and ReLU before average pooling\n        x = F.adaptive_avg_pool2d(x, output_size=1)\n        return x\n\n    def forward(self, x):\n        x = self._forward_conv(x)\n        x = x.view(x.size(0), -1)\n        x = self.fc(x)\n        return x\n"""
pytorch_image_classification/models/cifar/shake_shake.py,6,"b""import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom ..initializer import create_initializer\nfrom ..functions.shake_shake_function import get_alpha_beta, shake_function\n\n\nclass ResidualPath(nn.Module):\n    def __init__(self, in_channels, out_channels, stride):\n        super().__init__()\n\n        self.conv1 = nn.Conv2d(\n            in_channels,\n            out_channels,\n            kernel_size=3,\n            stride=stride,\n            padding=1,\n            bias=False,\n        )\n        self.bn1 = nn.BatchNorm2d(out_channels)\n        self.conv2 = nn.Conv2d(out_channels,\n                               out_channels,\n                               kernel_size=3,\n                               stride=1,\n                               padding=1,\n                               bias=False)\n        self.bn2 = nn.BatchNorm2d(out_channels)\n\n    def forward(self, x):\n        x = F.relu(x, inplace=False)\n        x = F.relu(self.bn1(self.conv1(x)), inplace=False)\n        x = self.bn2(self.conv2(x))\n        return x\n\n\nclass SkipConnection(nn.Module):\n    def __init__(self, in_channels, out_channels, stride):\n        super().__init__()\n        self.conv1 = nn.Conv2d(in_channels,\n                               out_channels // 2,\n                               kernel_size=1,\n                               stride=1,\n                               padding=0,\n                               bias=False)\n        self.conv2 = nn.Conv2d(in_channels,\n                               out_channels // 2,\n                               kernel_size=1,\n                               stride=1,\n                               padding=0,\n                               bias=False)\n        self.bn = nn.BatchNorm2d(out_channels)\n        self.stride = stride\n\n    def forward(self, x):\n        x = F.relu(x, inplace=False)\n        y1 = F.avg_pool2d(x, kernel_size=1, stride=self.stride, padding=0)\n        y1 = self.conv1(y1)\n\n        y2 = F.pad(x[:, :, 1:, 1:], (0, 1, 0, 1))\n        y2 = F.avg_pool2d(y2, kernel_size=1, stride=self.stride, padding=0)\n        y2 = self.conv2(y2)\n\n        z = torch.cat([y1, y2], dim=1)\n        z = self.bn(z)\n\n        return z\n\n\nclass BasicBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, stride, shake_config):\n        super().__init__()\n\n        self.shake_config = shake_config\n\n        self.residual_path1 = ResidualPath(in_channels, out_channels, stride)\n        self.residual_path2 = ResidualPath(in_channels, out_channels, stride)\n\n        self.shortcut = nn.Sequential()\n        if in_channels != out_channels:\n            self.shortcut.add_module(\n                'skip', SkipConnection(in_channels, out_channels, stride))\n\n    def forward(self, x):\n        x1 = self.residual_path1(x)\n        x2 = self.residual_path2(x)\n\n        if self.training:\n            shake_config = self.shake_config\n        else:\n            shake_config = (False, False, False)\n\n        alpha, beta = get_alpha_beta(x.size(0), shake_config, x.device)\n        y = shake_function(x1, x2, alpha, beta)\n\n        return self.shortcut(x) + y\n\n\nclass Network(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n\n        model_config = config.model.shake_shake\n        depth = model_config.depth\n        initial_channels = model_config.initial_channels\n        self.shake_config = [\n            model_config.shake_forward,\n            model_config.shake_backward,\n            model_config.shake_image,\n        ]\n\n        block = BasicBlock\n        n_blocks_per_stage = (depth - 2) // 6\n        assert n_blocks_per_stage * 6 + 2 == depth\n\n        n_channels = [\n            initial_channels,\n            initial_channels * 2,\n            initial_channels * 4,\n        ]\n\n        self.conv = nn.Conv2d(config.dataset.n_channels,\n                              16,\n                              kernel_size=3,\n                              stride=1,\n                              padding=1,\n                              bias=False)\n        self.bn = nn.BatchNorm2d(16)\n\n        self.stage1 = self._make_stage(16,\n                                       n_channels[0],\n                                       n_blocks_per_stage,\n                                       block,\n                                       stride=1)\n        self.stage2 = self._make_stage(n_channels[0],\n                                       n_channels[1],\n                                       n_blocks_per_stage,\n                                       block,\n                                       stride=2)\n        self.stage3 = self._make_stage(n_channels[1],\n                                       n_channels[2],\n                                       n_blocks_per_stage,\n                                       block,\n                                       stride=2)\n\n        # compute conv feature size\n        with torch.no_grad():\n            dummy_data = torch.zeros(\n                (1, config.dataset.n_channels, config.dataset.image_size,\n                 config.dataset.image_size),\n                dtype=torch.float32)\n            self.feature_size = self._forward_conv(dummy_data).view(\n                -1).shape[0]\n\n        self.fc = nn.Linear(self.feature_size, config.dataset.n_classes)\n\n        # initialize weights\n        initializer = create_initializer(config.model.init_mode)\n        self.apply(initializer)\n\n    def _make_stage(self, in_channels, out_channels, n_blocks, block, stride):\n        stage = nn.Sequential()\n        for index in range(n_blocks):\n            block_name = f'block{index + 1}'\n            if index == 0:\n                stage.add_module(\n                    block_name,\n                    block(in_channels,\n                          out_channels,\n                          stride=stride,\n                          shake_config=self.shake_config))\n            else:\n                stage.add_module(\n                    block_name,\n                    block(out_channels,\n                          out_channels,\n                          stride=1,\n                          shake_config=self.shake_config))\n        return stage\n\n    def _forward_conv(self, x):\n        x = self.bn(self.conv(x))\n        x = self.stage1(x)\n        x = self.stage2(x)\n        x = self.stage3(x)\n        x = F.relu(x, inplace=True)\n        x = F.adaptive_avg_pool2d(x, output_size=1)\n        return x\n\n    def forward(self, x):\n        x = self._forward_conv(x)\n        x = x.view(x.size(0), -1)\n        x = self.fc(x)\n        return x\n"""
pytorch_image_classification/models/cifar/vgg.py,4,"b""import torch\nimport torch.nn as nn\n\nfrom ..initializer import create_initializer\n\n\nclass Network(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n\n        model_config = config.model.vgg\n        self.use_bn = model_config.use_bn\n        n_channels = model_config.n_channels\n        n_layers = model_config.n_layers\n\n        self.stage1 = self._make_stage(config.dataset.n_channels,\n                                       n_channels[0], n_layers[0])\n        self.stage2 = self._make_stage(n_channels[0], n_channels[1],\n                                       n_layers[1])\n        self.stage3 = self._make_stage(n_channels[1], n_channels[2],\n                                       n_layers[2])\n        self.stage4 = self._make_stage(n_channels[2], n_channels[3],\n                                       n_layers[3])\n        self.stage5 = self._make_stage(n_channels[3], n_channels[4],\n                                       n_layers[4])\n\n        # compute conv feature size\n        with torch.no_grad():\n            dummy_data = torch.zeros(\n                (1, config.dataset.n_channels, config.dataset.image_size,\n                 config.dataset.image_size),\n                dtype=torch.float32)\n            self.feature_size = self._forward_conv(dummy_data).view(\n                -1).shape[0]\n\n        self.fc = nn.Linear(self.feature_size, config.dataset.n_classes)\n\n        # initialize weights\n        initializer = create_initializer(config.model.init_mode)\n        self.apply(initializer)\n\n    def _make_stage(self, in_channels, out_channels, n_blocks):\n        stage = nn.Sequential()\n        for index in range(n_blocks):\n            if index == 0:\n                conv = nn.Conv2d(\n                    in_channels,\n                    out_channels,\n                    kernel_size=3,\n                    stride=1,\n                    padding=1,\n                )\n            else:\n                conv = nn.Conv2d(\n                    out_channels,\n                    out_channels,\n                    kernel_size=3,\n                    stride=1,\n                    padding=1,\n                )\n            stage.add_module(f'conv{index}', conv)\n            if self.use_bn:\n                stage.add_module(f'bn{index}', nn.BatchNorm2d(out_channels))\n            stage.add_module('relu', nn.ReLU(inplace=True))\n        stage.add_module('pool', nn.MaxPool2d(kernel_size=2, stride=2))\n        return stage\n\n    def _forward_conv(self, x):\n        x = self.stage1(x)\n        x = self.stage2(x)\n        x = self.stage3(x)\n        x = self.stage4(x)\n        x = self.stage5(x)\n        return x\n\n    def forward(self, x):\n        x = self._forward_conv(x)\n        x = x.view(x.size(0), -1)\n        x = self.fc(x)\n        return x\n"""
pytorch_image_classification/models/cifar/wrn.py,5,"b""import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom ..initializer import create_initializer\n\n\nclass BasicBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, stride, drop_rate):\n        super().__init__()\n\n        self.drop_rate = drop_rate\n\n        self._preactivate_both = (in_channels != out_channels)\n\n        self.bn1 = nn.BatchNorm2d(in_channels)\n        self.conv1 = nn.Conv2d(\n            in_channels,\n            out_channels,\n            kernel_size=3,\n            stride=stride,  # downsample with first conv\n            padding=1,\n            bias=False)\n\n        self.bn2 = nn.BatchNorm2d(out_channels)\n        self.conv2 = nn.Conv2d(out_channels,\n                               out_channels,\n                               kernel_size=3,\n                               stride=1,\n                               padding=1,\n                               bias=False)\n\n        self.shortcut = nn.Sequential()\n        if in_channels != out_channels:\n            self.shortcut.add_module(\n                'conv',\n                nn.Conv2d(\n                    in_channels,\n                    out_channels,\n                    kernel_size=1,\n                    stride=stride,  # downsample\n                    padding=0,\n                    bias=False))\n\n    def forward(self, x):\n        if self._preactivate_both:\n            x = F.relu(self.bn1(x),\n                       inplace=True)  # shortcut after preactivation\n            y = self.conv1(x)\n        else:\n            y = F.relu(self.bn1(x),\n                       inplace=True)  # preactivation only for residual path\n            y = self.conv1(y)\n        if self.drop_rate > 0:\n            y = F.dropout(y,\n                          p=self.drop_rate,\n                          training=self.training,\n                          inplace=False)\n\n        y = F.relu(self.bn2(y), inplace=True)\n        y = self.conv2(y)\n        y += self.shortcut(x)\n        return y\n\n\nclass Network(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n\n        model_config = config.model.wrn\n        depth = model_config.depth\n        initial_channels = model_config.initial_channels\n        widening_factor = model_config.widening_factor\n        drop_rate = model_config.drop_rate\n\n        block = BasicBlock\n        n_blocks_per_stage = (depth - 4) // 6\n        assert n_blocks_per_stage * 6 + 4 == depth\n\n        n_channels = [\n            initial_channels,\n            initial_channels * widening_factor,\n            initial_channels * 2 * widening_factor,\n            initial_channels * 4 * widening_factor,\n        ]\n\n        self.conv = nn.Conv2d(config.dataset.n_channels,\n                              n_channels[0],\n                              kernel_size=3,\n                              stride=1,\n                              padding=1,\n                              bias=False)\n\n        self.stage1 = self._make_stage(n_channels[0],\n                                       n_channels[1],\n                                       n_blocks_per_stage,\n                                       block,\n                                       stride=1,\n                                       drop_rate=drop_rate)\n        self.stage2 = self._make_stage(n_channels[1],\n                                       n_channels[2],\n                                       n_blocks_per_stage,\n                                       block,\n                                       stride=2,\n                                       drop_rate=drop_rate)\n        self.stage3 = self._make_stage(n_channels[2],\n                                       n_channels[3],\n                                       n_blocks_per_stage,\n                                       block,\n                                       stride=2,\n                                       drop_rate=drop_rate)\n        self.bn = nn.BatchNorm2d(n_channels[3])\n\n        # compute conv feature size\n        with torch.no_grad():\n            dummy_data = torch.zeros(\n                (1, config.dataset.n_channels, config.dataset.image_size,\n                 config.dataset.image_size),\n                dtype=torch.float32)\n            self.feature_size = self._forward_conv(dummy_data).view(\n                -1).shape[0]\n\n        self.fc = nn.Linear(self.feature_size, config.dataset.n_classes)\n\n        # initialize weights\n        initializer = create_initializer(config.model.init_mode)\n        self.apply(initializer)\n\n    def _make_stage(self, in_channels, out_channels, n_blocks, block, stride,\n                    drop_rate):\n        stage = nn.Sequential()\n        for index in range(n_blocks):\n            block_name = f'block{index + 1}'\n            if index == 0:\n                stage.add_module(\n                    block_name,\n                    block(in_channels,\n                          out_channels,\n                          stride=stride,\n                          drop_rate=drop_rate))\n            else:\n                stage.add_module(\n                    block_name,\n                    block(out_channels,\n                          out_channels,\n                          stride=1,\n                          drop_rate=drop_rate))\n        return stage\n\n    def _forward_conv(self, x):\n        x = self.conv(x)\n        x = self.stage1(x)\n        x = self.stage2(x)\n        x = self.stage3(x)\n        x = F.relu(self.bn(x), inplace=True)\n        x = F.adaptive_avg_pool2d(x, output_size=1)\n        return x\n\n    def forward(self, x):\n        x = self._forward_conv(x)\n        x = x.view(x.size(0), -1)\n        x = self.fc(x)\n        return x\n"""
pytorch_image_classification/models/functions/__init__.py,0,b''
pytorch_image_classification/models/functions/shake_shake_function.py,7,"b'import torch\nfrom torch.autograd import Function\n\n\nclass ShakeFunction(Function):\n    @staticmethod\n    def forward(ctx, x1, x2, alpha, beta):\n        ctx.save_for_backward(x1, x2, alpha, beta)\n\n        y = x1 * alpha + x2 * (1 - alpha)\n        return y\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        x1, x2, alpha, beta = ctx.saved_variables\n        grad_x1 = grad_x2 = grad_alpha = grad_beta = None\n\n        if ctx.needs_input_grad[0]:\n            grad_x1 = grad_output * beta\n        if ctx.needs_input_grad[1]:\n            grad_x2 = grad_output * (1 - beta)\n\n        return grad_x1, grad_x2, grad_alpha, grad_beta\n\n\nshake_function = ShakeFunction.apply\n\n\ndef get_alpha_beta(batch_size, shake_config, device):\n    forward_shake, backward_shake, shake_image = shake_config\n\n    if forward_shake and not shake_image:\n        alpha = torch.rand(1)\n    elif forward_shake and shake_image:\n        alpha = torch.rand(batch_size).view(batch_size, 1, 1, 1)\n    else:\n        alpha = torch.FloatTensor([0.5])\n\n    if backward_shake and not shake_image:\n        beta = torch.rand(1)\n    elif backward_shake and shake_image:\n        beta = torch.rand(batch_size).view(batch_size, 1, 1, 1)\n    else:\n        beta = torch.FloatTensor([0.5])\n\n    alpha = alpha.to(device)\n    beta = beta.to(device)\n\n    return alpha, beta\n'"
pytorch_image_classification/models/imagenet/__init__.py,0,b''
pytorch_image_classification/models/imagenet/densenet.py,7,"b""import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom ..initializer import create_initializer\n\n\nclass BasicBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, drop_rate):\n        super().__init__()\n\n        self.drop_rate = drop_rate\n\n        self.bn = nn.BatchNorm2d(in_channels)\n        self.conv = nn.Conv2d(in_channels,\n                              out_channels,\n                              kernel_size=3,\n                              stride=1,\n                              padding=1,\n                              bias=False)\n\n    def forward(self, x):\n        y = self.conv(F.relu(self.bn(x), inplace=True))\n        if self.drop_rate > 0:\n            y = F.dropout(y,\n                          p=self.drop_rate,\n                          training=self.training,\n                          inplace=False)\n        return torch.cat([x, y], dim=1)\n\n\nclass BottleneckBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, drop_rate):\n        super().__init__()\n\n        self.drop_rate = drop_rate\n\n        bottleneck_channels = out_channels * 4\n\n        self.bn1 = nn.BatchNorm2d(in_channels)\n        self.conv1 = nn.Conv2d(in_channels,\n                               bottleneck_channels,\n                               kernel_size=1,\n                               stride=1,\n                               padding=0,\n                               bias=False)\n\n        self.bn2 = nn.BatchNorm2d(bottleneck_channels)\n        self.conv2 = nn.Conv2d(bottleneck_channels,\n                               out_channels,\n                               kernel_size=3,\n                               stride=1,\n                               padding=1,\n                               bias=False)\n\n    def forward(self, x):\n        y = self.conv1(F.relu(self.bn1(x), inplace=True))\n        if self.drop_rate > 0:\n            y = F.dropout(y,\n                          p=self.drop_rate,\n                          training=self.training,\n                          inplace=False)\n        y = self.conv2(F.relu(self.bn2(y), inplace=True))\n        if self.drop_rate > 0:\n            y = F.dropout(y,\n                          p=self.drop_rate,\n                          training=self.training,\n                          inplace=False)\n        return torch.cat([x, y], dim=1)\n\n\nclass TransitionBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, drop_rate):\n        super().__init__()\n\n        self.drop_rate = drop_rate\n\n        self.bn = nn.BatchNorm2d(in_channels)\n        self.conv = nn.Conv2d(in_channels,\n                              out_channels,\n                              kernel_size=1,\n                              stride=1,\n                              padding=0,\n                              bias=False)\n\n    def forward(self, x):\n        x = self.conv(F.relu(self.bn(x), inplace=True))\n        if self.drop_rate > 0:\n            x = F.dropout(x,\n                          p=self.drop_rate,\n                          training=self.training,\n                          inplace=False)\n        x = F.avg_pool2d(x, kernel_size=2, stride=2)\n        return x\n\n\nclass Network(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n\n        model_config = config.model.densenet\n        block_type = model_config.block_type\n        n_blocks = model_config.n_blocks\n        self.growth_rate = model_config.growth_rate\n        self.drop_rate = model_config.drop_rate\n        self.compression_rate = model_config.compression_rate\n\n        assert block_type in ['basic', 'bottleneck']\n        if block_type == 'basic':\n            block = BasicBlock\n        else:\n            block = BottleneckBlock\n\n        in_channels = [2 * self.growth_rate]\n        for index in range(4):\n            denseblock_out_channels = int(in_channels[-1] +\n                                          n_blocks[index] * self.growth_rate)\n            if index < 3:\n                transitionblock_out_channels = int(denseblock_out_channels *\n                                                   self.compression_rate)\n            else:\n                transitionblock_out_channels = denseblock_out_channels\n            in_channels.append(transitionblock_out_channels)\n\n        self.conv = nn.Conv2d(config.dataset.n_channels,\n                              in_channels[0],\n                              kernel_size=7,\n                              stride=2,\n                              padding=3,\n                              bias=False)\n        self.bn = nn.BatchNorm2d(in_channels[0])\n        self.stage1 = self._make_stage(in_channels[0], n_blocks[0], block,\n                                       True)\n        self.stage2 = self._make_stage(in_channels[1], n_blocks[1], block,\n                                       True)\n        self.stage3 = self._make_stage(in_channels[2], n_blocks[2], block,\n                                       True)\n        self.stage4 = self._make_stage(in_channels[3], n_blocks[3], block,\n                                       False)\n        self.bn_last = nn.BatchNorm2d(in_channels[4])\n\n        # compute conv feature size\n        with torch.no_grad():\n            dummy_data = torch.zeros(\n                (1, config.dataset.n_channels, config.dataset.image_size,\n                 config.dataset.image_size),\n                dtype=torch.float32)\n            self.feature_size = self._forward_conv(dummy_data).view(\n                -1).shape[0]\n\n        self.fc = nn.Linear(self.feature_size, config.dataset.n_classes)\n\n        # initialize weights\n        initializer = create_initializer(config.model.init_mode)\n        self.apply(initializer)\n\n    def _make_stage(self, in_channels, n_blocks, block, add_transition_block):\n        stage = nn.Sequential()\n        for index in range(n_blocks):\n            stage.add_module(\n                f'block{index + 1}',\n                block(in_channels + index * self.growth_rate, self.growth_rate,\n                      self.drop_rate))\n        if add_transition_block:\n            in_channels = int(in_channels + n_blocks * self.growth_rate)\n            out_channels = int(in_channels * self.compression_rate)\n            stage.add_module(\n                'transition',\n                TransitionBlock(in_channels, out_channels, self.drop_rate))\n        return stage\n\n    def _forward_conv(self, x):\n        x = F.relu(self.bn(self.conv(x)), inplace=True)\n        x = F.max_pool2d(x, kernel_size=3, stride=2, padding=1)\n        x = self.stage1(x)\n        x = self.stage2(x)\n        x = self.stage3(x)\n        x = self.stage4(x)\n        x = self.bn_last(x)\n        x = F.adaptive_avg_pool2d(x, output_size=1)\n        return x\n\n    def forward(self, x):\n        x = self._forward_conv(x)\n        x = x.view(x.size(0), -1)\n        x = self.fc(x)\n        return x\n"""
pytorch_image_classification/models/imagenet/pyramidnet.py,5,"b""import numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom ..initializer import create_initializer\n\n\nclass BasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self, in_channels, out_channels, stride):\n        super().__init__()\n\n        self.bn1 = nn.BatchNorm2d(in_channels)\n        self.conv1 = nn.Conv2d(\n            in_channels,\n            out_channels,\n            kernel_size=3,\n            stride=stride,  # downsample with first conv\n            padding=1,\n            bias=False)\n        self.bn2 = nn.BatchNorm2d(out_channels)\n        self.conv2 = nn.Conv2d(out_channels,\n                               out_channels,\n                               kernel_size=3,\n                               stride=1,\n                               padding=1,\n                               bias=False)\n        self.bn3 = nn.BatchNorm2d(out_channels)\n\n        self.shortcut = nn.Sequential()\n        if stride > 1:\n            self.shortcut = nn.AvgPool2d(kernel_size=2, stride=2)\n\n    def forward(self, x):\n        y = self.bn1(x)\n        y = self.conv1(y)\n\n        y = F.relu(self.bn2(y), inplace=True)\n        y = self.conv2(y)\n\n        y = self.bn3(y)\n\n        if y.size(1) != x.size(1):\n            y += F.pad(self.shortcut(x),\n                       (0, 0, 0, 0, 0, y.size(1) - x.size(1)), 'constant', 0)\n        else:\n            y += self.shortcut(x)\n        return y\n\n\nclass BottleneckBlock(nn.Module):\n    expansion = 4\n\n    def __init__(self, in_channels, out_channels, stride):\n        super().__init__()\n\n        bottleneck_channels = out_channels // self.expansion\n\n        self.bn1 = nn.BatchNorm2d(in_channels)\n        self.conv1 = nn.Conv2d(in_channels,\n                               bottleneck_channels,\n                               kernel_size=1,\n                               stride=1,\n                               padding=0,\n                               bias=False)\n        self.bn2 = nn.BatchNorm2d(bottleneck_channels)\n        self.conv2 = nn.Conv2d(\n            bottleneck_channels,\n            bottleneck_channels,\n            kernel_size=3,\n            stride=stride,  # downsample with 3x3 conv\n            padding=1,\n            bias=False)\n        self.bn3 = nn.BatchNorm2d(bottleneck_channels)\n        self.conv3 = nn.Conv2d(bottleneck_channels,\n                               out_channels,\n                               kernel_size=1,\n                               stride=1,\n                               padding=0,\n                               bias=False)\n\n        self.bn4 = nn.BatchNorm2d(out_channels)\n\n        self.shortcut = nn.Sequential()  # identity\n        if stride > 1:\n            self.shortcut = nn.AvgPool2d(kernel_size=2, stride=2)\n\n    def forward(self, x):\n        y = self.bn1(x)\n        y = self.conv1(y)\n\n        y = F.relu(self.bn2(y), inplace=True)\n        y = self.conv2(y)\n\n        y = F.relu(self.bn3(y), inplace=True)\n        y = self.conv3(y)\n\n        y = self.bn4(y)\n\n        if y.size(1) != x.size(1):\n            y += F.pad(self.shortcut(x),\n                       (0, 0, 0, 0, 0, y.size(1) - x.size(1)), 'constant', 0)\n        else:\n            y += self.shortcut(x)\n        return y\n\n\nclass Network(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n\n        model_config = config.model.pyramidnet\n        initial_channels = model_config.initial_channels\n        block_type = model_config.block_type\n        n_blocks = model_config.n_blocks\n        alpha = model_config.alpha\n\n        assert block_type in ['basic', 'bottleneck']\n        if block_type == 'basic':\n            block = BasicBlock\n        else:\n            block = BottleneckBlock\n\n        n_channels = [initial_channels]\n        depth = sum(n_blocks)\n        rate = alpha / depth\n        for _ in range(depth):\n            num = n_channels[-1] + rate\n            n_channels.append(num)\n        n_channels = [int(np.round(c)) * block.expansion for c in n_channels]\n        n_channels[0] //= block.expansion\n\n        self.conv = nn.Conv2d(config.dataset.n_channels,\n                              n_channels[0],\n                              kernel_size=7,\n                              stride=2,\n                              padding=3,\n                              bias=False)\n        self.bn = nn.BatchNorm2d(n_channels[0])\n\n        accs = [n_blocks[0]]\n        for i in range(1, 4):\n            accs.append(accs[-1] + n_blocks[i])\n        self.stage1 = self._make_stage(n_channels[:accs[0] + 1],\n                                       n_blocks[0],\n                                       block,\n                                       stride=1)\n        self.stage2 = self._make_stage(n_channels[accs[0]:accs[1] + 1],\n                                       n_blocks[1],\n                                       block,\n                                       stride=2)\n        self.stage3 = self._make_stage(n_channels[accs[1]:accs[2] + 1],\n                                       n_blocks[2],\n                                       block,\n                                       stride=2)\n        self.stage4 = self._make_stage(n_channels[accs[2]:accs[3] + 1],\n                                       n_blocks[3],\n                                       block,\n                                       stride=2)\n\n        self.bn_last = nn.BatchNorm2d(n_channels[-1])\n\n        # compute conv feature size\n        with torch.no_grad():\n            dummy_data = torch.zeros(\n                (1, config.dataset.n_channels, config.dataset.image_size,\n                 config.dataset.image_size),\n                dtype=torch.float32)\n            self.feature_size = self._forward_conv(dummy_data).view(\n                -1).shape[0]\n\n        self.fc = nn.Linear(self.feature_size, config.dataset.n_classes)\n\n        # initialize weights\n        initializer = create_initializer(config.model.init_mode)\n        self.apply(initializer)\n\n    def _make_stage(self, n_channels, n_blocks, block, stride):\n        stage = nn.Sequential()\n        for index in range(n_blocks):\n            block_name = f'block{index + 1}'\n            if index == 0:\n                stage.add_module(\n                    block_name,\n                    block(n_channels[index],\n                          n_channels[index + 1],\n                          stride=stride))\n            else:\n                stage.add_module(\n                    block_name,\n                    block(n_channels[index], n_channels[index + 1], stride=1))\n        return stage\n\n    def _forward_conv(self, x):\n        x = F.relu(self.bn(self.conv(x)), inplace=True)\n        x = F.max_pool2d(x, kernel_size=3, stride=2, padding=1)\n        x = self.stage1(x)\n        x = self.stage2(x)\n        x = self.stage3(x)\n        x = self.stage4(x)\n        x = F.relu(self.bn_last(x),\n                   inplace=True)  # apply BN and ReLU before average pooling\n        x = F.adaptive_avg_pool2d(x, output_size=1)\n        return x\n\n    def forward(self, x):\n        x = self._forward_conv(x)\n        x = x.view(x.size(0), -1)\n        x = self.fc(x)\n        return x\n"""
pytorch_image_classification/models/imagenet/resnet.py,5,"b""import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom ..initializer import create_initializer\n\n\nclass BasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self, in_channels, out_channels, stride):\n        super().__init__()\n\n        self.conv1 = nn.Conv2d(\n            in_channels,\n            out_channels,\n            kernel_size=3,\n            stride=stride,  # downsample with first conv\n            padding=1,\n            bias=False)\n        self.bn1 = nn.BatchNorm2d(out_channels)\n        self.conv2 = nn.Conv2d(out_channels,\n                               out_channels,\n                               kernel_size=3,\n                               stride=1,\n                               padding=1,\n                               bias=False)\n        self.bn2 = nn.BatchNorm2d(out_channels)\n\n        self.shortcut = nn.Sequential()\n        if in_channels != out_channels:\n            self.shortcut.add_module(\n                'conv',\n                nn.Conv2d(\n                    in_channels,\n                    out_channels,\n                    kernel_size=1,\n                    stride=stride,  # downsample\n                    padding=0,\n                    bias=False))\n            self.shortcut.add_module('bn', nn.BatchNorm2d(out_channels))  # BN\n\n    def forward(self, x):\n        y = F.relu(self.bn1(self.conv1(x)), inplace=True)\n        y = self.bn2(self.conv2(y))\n        y += self.shortcut(x)\n        y = F.relu(y, inplace=True)  # apply ReLU after addition\n        return y\n\n\nclass BottleneckBlock(nn.Module):\n    expansion = 4\n\n    def __init__(self, in_channels, out_channels, stride):\n        super().__init__()\n\n        bottleneck_channels = out_channels // self.expansion\n\n        self.conv1 = nn.Conv2d(in_channels,\n                               bottleneck_channels,\n                               kernel_size=1,\n                               stride=1,\n                               padding=0,\n                               bias=False)\n        self.bn1 = nn.BatchNorm2d(bottleneck_channels)\n\n        self.conv2 = nn.Conv2d(\n            bottleneck_channels,\n            bottleneck_channels,\n            kernel_size=3,\n            stride=stride,  # downsample with 3x3 conv\n            padding=1,\n            bias=False)\n        self.bn2 = nn.BatchNorm2d(bottleneck_channels)\n\n        self.conv3 = nn.Conv2d(bottleneck_channels,\n                               out_channels,\n                               kernel_size=1,\n                               stride=1,\n                               padding=0,\n                               bias=False)\n        self.bn3 = nn.BatchNorm2d(out_channels)\n\n        self.shortcut = nn.Sequential()  # identity\n        if in_channels != out_channels:\n            self.shortcut.add_module(\n                'conv',\n                nn.Conv2d(\n                    in_channels,\n                    out_channels,\n                    kernel_size=1,\n                    stride=stride,  # downsample\n                    padding=0,\n                    bias=False))\n            self.shortcut.add_module('bn', nn.BatchNorm2d(out_channels))  # BN\n\n    def forward(self, x):\n        y = F.relu(self.bn1(self.conv1(x)), inplace=True)\n        y = F.relu(self.bn2(self.conv2(y)), inplace=True)\n        y = self.bn3(self.conv3(y))  # not apply ReLU\n        y += self.shortcut(x)\n        y = F.relu(y, inplace=True)  # apply ReLU after addition\n        return y\n\n\nclass Network(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n\n        model_config = config.model.resnet\n        initial_channels = model_config.initial_channels\n        block_type = model_config.block_type\n        n_blocks = model_config.n_blocks\n\n        assert block_type in ['basic', 'bottleneck']\n        if block_type == 'basic':\n            block = BasicBlock\n        else:\n            block = BottleneckBlock\n\n        n_channels = [\n            initial_channels,\n            initial_channels * 2 * block.expansion,\n            initial_channels * 4 * block.expansion,\n            initial_channels * 8 * block.expansion,\n        ]\n\n        self.conv = nn.Conv2d(config.dataset.n_channels,\n                              n_channels[0],\n                              kernel_size=7,\n                              stride=2,\n                              padding=3,\n                              bias=False)\n        self.bn = nn.BatchNorm2d(initial_channels)\n\n        self.stage1 = self._make_stage(n_channels[0],\n                                       n_channels[0],\n                                       n_blocks[0],\n                                       block,\n                                       stride=1)\n        self.stage2 = self._make_stage(n_channels[0],\n                                       n_channels[1],\n                                       n_blocks[1],\n                                       block,\n                                       stride=2)\n        self.stage3 = self._make_stage(n_channels[1],\n                                       n_channels[2],\n                                       n_blocks[2],\n                                       block,\n                                       stride=2)\n        self.stage4 = self._make_stage(n_channels[2],\n                                       n_channels[3],\n                                       n_blocks[3],\n                                       block,\n                                       stride=2)\n\n        # compute conv feature size\n        with torch.no_grad():\n            dummy_data = torch.zeros(\n                (1, config.dataset.n_channels, config.dataset.image_size,\n                 config.dataset.image_size),\n                dtype=torch.float32)\n            self.feature_size = self._forward_conv(dummy_data).view(\n                -1).shape[0]\n\n        self.fc = nn.Linear(self.feature_size, config.dataset.n_classes)\n\n        # initialize weights\n        initializer = create_initializer(config.model.init_mode)\n        self.apply(initializer)\n\n    def _make_stage(self, in_channels, out_channels, n_blocks, block, stride):\n        stage = nn.Sequential()\n        for index in range(n_blocks):\n            block_name = f'block{index + 1}'\n            if index == 0:\n                stage.add_module(\n                    block_name, block(in_channels, out_channels,\n                                      stride=stride))\n            else:\n                stage.add_module(block_name,\n                                 block(out_channels, out_channels, stride=1))\n        return stage\n\n    def _forward_conv(self, x):\n        x = F.relu(self.bn(self.conv(x)), inplace=True)\n        x = F.max_pool2d(x, kernel_size=3, stride=2, padding=1)\n        x = self.stage1(x)\n        x = self.stage2(x)\n        x = self.stage3(x)\n        x = self.stage4(x)\n        x = F.adaptive_avg_pool2d(x, output_size=1)\n        return x\n\n    def forward(self, x):\n        x = self._forward_conv(x)\n        x = x.view(x.size(0), -1)\n        x = self.fc(x)\n        return x\n"""
pytorch_image_classification/models/imagenet/resnet_preact.py,5,"b""import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom ..initializer import create_initializer\n\n\nclass BasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self,\n                 in_channels,\n                 out_channels,\n                 stride,\n                 remove_first_relu,\n                 add_last_bn,\n                 preact=False):\n        super().__init__()\n\n        self._remove_first_relu = remove_first_relu\n        self._add_last_bn = add_last_bn\n        self._preact = preact\n\n        self.bn1 = nn.BatchNorm2d(in_channels)\n        self.conv1 = nn.Conv2d(\n            in_channels,\n            out_channels,\n            kernel_size=3,\n            stride=stride,  # downsample with first conv\n            padding=1,\n            bias=False)\n        self.bn2 = nn.BatchNorm2d(out_channels)\n        self.conv2 = nn.Conv2d(out_channels,\n                               out_channels,\n                               kernel_size=3,\n                               stride=1,\n                               padding=1,\n                               bias=False)\n\n        if add_last_bn:\n            self.bn3 = nn.BatchNorm2d(out_channels)\n\n        self.shortcut = nn.Sequential()\n        if in_channels != out_channels:\n            self.shortcut.add_module(\n                'conv',\n                nn.Conv2d(\n                    in_channels,\n                    out_channels,\n                    kernel_size=1,\n                    stride=stride,  # downsample\n                    padding=0,\n                    bias=False))\n\n    def forward(self, x):\n        if self._preact:\n            x = F.relu(self.bn1(x),\n                       inplace=True)  # shortcut after preactivation\n            y = self.conv1(x)\n        else:\n            # preactivation only for residual path\n            y = self.bn1(x)\n            if not self._remove_first_relu:\n                y = F.relu(y, inplace=True)\n            y = self.conv1(y)\n\n        y = F.relu(self.bn2(y), inplace=True)\n        y = self.conv2(y)\n\n        if self._add_last_bn:\n            y = self.bn3(y)\n\n        y += self.shortcut(x)\n        return y\n\n\nclass BottleneckBlock(nn.Module):\n    expansion = 4\n\n    def __init__(self,\n                 in_channels,\n                 out_channels,\n                 stride,\n                 remove_first_relu,\n                 add_last_bn,\n                 preact=False):\n        super().__init__()\n\n        self._remove_first_relu = remove_first_relu\n        self._add_last_bn = add_last_bn\n        self._preact = preact\n\n        bottleneck_channels = out_channels // self.expansion\n\n        self.bn1 = nn.BatchNorm2d(in_channels)\n        self.conv1 = nn.Conv2d(in_channels,\n                               bottleneck_channels,\n                               kernel_size=1,\n                               stride=1,\n                               padding=0,\n                               bias=False)\n        self.bn2 = nn.BatchNorm2d(bottleneck_channels)\n        self.conv2 = nn.Conv2d(\n            bottleneck_channels,\n            bottleneck_channels,\n            kernel_size=3,\n            stride=stride,  # downsample with 3x3 conv\n            padding=1,\n            bias=False)\n        self.bn3 = nn.BatchNorm2d(bottleneck_channels)\n        self.conv3 = nn.Conv2d(bottleneck_channels,\n                               out_channels,\n                               kernel_size=1,\n                               stride=1,\n                               padding=0,\n                               bias=False)\n\n        if add_last_bn:\n            self.bn4 = nn.BatchNorm2d(out_channels)\n\n        self.shortcut = nn.Sequential()  # identity\n        if in_channels != out_channels:\n            self.shortcut.add_module(\n                'conv',\n                nn.Conv2d(\n                    in_channels,\n                    out_channels,\n                    kernel_size=1,\n                    stride=stride,  # downsample\n                    padding=0,\n                    bias=False))\n\n    def forward(self, x):\n        if self._preact:\n            x = F.relu(self.bn1(x),\n                       inplace=True)  # shortcut after preactivation\n            y = self.conv1(x)\n        else:\n            # preactivation only for residual path\n            y = self.bn1(x)\n            if not self._remove_first_relu:\n                y = F.relu(y, inplace=True)\n            y = self.conv1(y)\n\n        y = F.relu(self.bn2(y), inplace=True)\n        y = self.conv2(y)\n        y = F.relu(self.bn3(y), inplace=True)\n        y = self.conv3(y)\n\n        if self._add_last_bn:\n            y = self.bn4(y)\n\n        y += self.shortcut(x)\n        return y\n\n\nclass Network(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n\n        model_config = config.model.resnet_preact\n        initial_channels = model_config.initial_channels\n        self._remove_first_relu = model_config.remove_first_relu\n        self._add_last_bn = model_config.add_last_bn\n        block_type = model_config.block_type\n        n_blocks = model_config.n_blocks\n        preact_stage = model_config.preact_stage\n\n        assert block_type in ['basic', 'bottleneck']\n        if block_type == 'basic':\n            block = BasicBlock\n        else:\n            block = BottleneckBlock\n\n        n_channels = [\n            initial_channels,\n            initial_channels * 2 * block.expansion,\n            initial_channels * 4 * block.expansion,\n            initial_channels * 8 * block.expansion,\n        ]\n\n        self.conv = nn.Conv2d(config.dataset.n_channels,\n                              n_channels[0],\n                              kernel_size=7,\n                              stride=2,\n                              padding=3,\n                              bias=False)\n        self.bn = nn.BatchNorm2d(n_channels[0])\n\n        self.stage1 = self._make_stage(n_channels[0],\n                                       n_channels[0],\n                                       n_blocks[0],\n                                       block,\n                                       stride=1,\n                                       preact=preact_stage[0])\n        self.stage2 = self._make_stage(n_channels[0],\n                                       n_channels[1],\n                                       n_blocks[1],\n                                       block,\n                                       stride=2,\n                                       preact=preact_stage[1])\n        self.stage3 = self._make_stage(n_channels[1],\n                                       n_channels[2],\n                                       n_blocks[2],\n                                       block,\n                                       stride=2,\n                                       preact=preact_stage[2])\n        self.stage4 = self._make_stage(n_channels[2],\n                                       n_channels[3],\n                                       n_blocks[3],\n                                       block,\n                                       stride=2,\n                                       preact=preact_stage[3])\n        self.bn_last = nn.BatchNorm2d(n_channels[3])\n\n        # compute conv feature size\n        with torch.no_grad():\n            dummy_data = torch.zeros(\n                (1, config.dataset.n_channels, config.dataset.image_size,\n                 config.dataset.image_size),\n                dtype=torch.float32)\n            self.feature_size = self._forward_conv(dummy_data).view(\n                -1).shape[0]\n\n        self.fc = nn.Linear(self.feature_size, config.dataset.n_classes)\n\n        # initialize weights\n        initializer = create_initializer(config.model.init_mode)\n        self.apply(initializer)\n\n    def _make_stage(self, in_channels, out_channels, n_blocks, block, stride,\n                    preact):\n        stage = nn.Sequential()\n        for index in range(n_blocks):\n            block_name = f'block{index + 1}'\n            if index == 0:\n                stage.add_module(\n                    block_name,\n                    block(in_channels,\n                          out_channels,\n                          stride=stride,\n                          remove_first_relu=self._remove_first_relu,\n                          add_last_bn=self._add_last_bn,\n                          preact=preact))\n            else:\n                stage.add_module(\n                    block_name,\n                    block(out_channels,\n                          out_channels,\n                          stride=1,\n                          remove_first_relu=self._remove_first_relu,\n                          add_last_bn=self._add_last_bn,\n                          preact=False))\n        return stage\n\n    def _forward_conv(self, x):\n        x = F.relu(self.bn(self.conv(x)), inplace=True)\n        x = F.max_pool2d(x, kernel_size=3, stride=2, padding=1)\n        x = self.stage1(x)\n        x = self.stage2(x)\n        x = self.stage3(x)\n        x = self.stage4(x)\n        x = F.relu(self.bn_last(x),\n                   inplace=True)  # apply BN and ReLU before average pooling\n        x = F.adaptive_avg_pool2d(x, output_size=1)\n        return x\n\n    def forward(self, x):\n        x = self._forward_conv(x)\n        x = x.view(x.size(0), -1)\n        x = self.fc(x)\n        return x\n"""
pytorch_image_classification/models/imagenet/resnext.py,5,"b""import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom ..initializer import create_initializer\n\n\nclass BottleneckBlock(nn.Module):\n    expansion = 4\n\n    def __init__(self, in_channels, out_channels, stride, stage_index,\n                 base_channels, cardinality):\n        super().__init__()\n\n        bottleneck_channels = cardinality * base_channels * 2**stage_index\n\n        self.conv1 = nn.Conv2d(in_channels,\n                               bottleneck_channels,\n                               kernel_size=1,\n                               stride=1,\n                               padding=0,\n                               bias=False)\n        self.bn1 = nn.BatchNorm2d(bottleneck_channels)\n\n        self.conv2 = nn.Conv2d(\n            bottleneck_channels,\n            bottleneck_channels,\n            kernel_size=3,\n            stride=stride,  # downsample with 3x3 conv\n            padding=1,\n            groups=cardinality,\n            bias=False)\n        self.bn2 = nn.BatchNorm2d(bottleneck_channels)\n\n        self.conv3 = nn.Conv2d(bottleneck_channels,\n                               out_channels,\n                               kernel_size=1,\n                               stride=1,\n                               padding=0,\n                               bias=False)\n        self.bn3 = nn.BatchNorm2d(out_channels)\n\n        self.shortcut = nn.Sequential()  # identity\n        if in_channels != out_channels:\n            self.shortcut.add_module(\n                'conv',\n                nn.Conv2d(\n                    in_channels,\n                    out_channels,\n                    kernel_size=1,\n                    stride=stride,  # downsample\n                    padding=0,\n                    bias=False))\n            self.shortcut.add_module('bn', nn.BatchNorm2d(out_channels))  # BN\n\n    def forward(self, x):\n        y = F.relu(self.bn1(self.conv1(x)), inplace=True)\n        y = F.relu(self.bn2(self.conv2(y)), inplace=True)\n        y = self.bn3(self.conv3(y))  # not apply ReLU\n        y += self.shortcut(x)\n        y = F.relu(y, inplace=True)  # apply ReLU after addition\n        return y\n\n\nclass Network(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n\n        model_config = config.model.resnext\n        initial_channels = model_config.initial_channels\n        n_blocks = model_config.n_blocks\n        self.base_channels = model_config.base_channels\n        self.cardinality = model_config.cardinality\n\n        block = BottleneckBlock\n\n        n_channels = [\n            initial_channels,\n            initial_channels * block.expansion,\n            initial_channels * 2 * block.expansion,\n            initial_channels * 4 * block.expansion,\n            initial_channels * 8 * block.expansion,\n        ]\n\n        self.conv = nn.Conv2d(config.dataset.n_channels,\n                              n_channels[0],\n                              kernel_size=7,\n                              stride=2,\n                              padding=3,\n                              bias=False)\n        self.bn = nn.BatchNorm2d(n_channels[0])\n\n        self.stage1 = self._make_stage(n_channels[0],\n                                       n_channels[1],\n                                       n_blocks[0],\n                                       0,\n                                       stride=1)\n        self.stage2 = self._make_stage(n_channels[1],\n                                       n_channels[2],\n                                       n_blocks[1],\n                                       1,\n                                       stride=2)\n        self.stage3 = self._make_stage(n_channels[2],\n                                       n_channels[3],\n                                       n_blocks[2],\n                                       2,\n                                       stride=2)\n        self.stage4 = self._make_stage(n_channels[3],\n                                       n_channels[4],\n                                       n_blocks[3],\n                                       3,\n                                       stride=2)\n\n        # compute conv feature size\n        with torch.no_grad():\n            dummy_data = torch.zeros(\n                (1, config.dataset.n_channels, config.dataset.image_size,\n                 config.dataset.image_size),\n                dtype=torch.float32)\n            self.feature_size = self._forward_conv(dummy_data).view(\n                -1).shape[0]\n\n        self.fc = nn.Linear(self.feature_size, config.dataset.n_classes)\n\n        # initialize weights\n        initializer = create_initializer(config.model.init_mode)\n        self.apply(initializer)\n\n    def _make_stage(self, in_channels, out_channels, n_blocks, stage_index,\n                    stride):\n        stage = nn.Sequential()\n        for index in range(n_blocks):\n            block_name = f'block{index + 1}'\n            if index == 0:\n                stage.add_module(\n                    block_name,\n                    BottleneckBlock(\n                        in_channels,\n                        out_channels,\n                        stride,  # downsample\n                        stage_index,\n                        self.base_channels,\n                        self.cardinality))\n            else:\n                stage.add_module(\n                    block_name,\n                    BottleneckBlock(\n                        out_channels,\n                        out_channels,\n                        1,  # no downsampling\n                        stage_index,\n                        self.base_channels,\n                        self.cardinality))\n        return stage\n\n    def _forward_conv(self, x):\n        x = F.relu(self.bn(self.conv(x)), inplace=True)\n        x = F.max_pool2d(x, kernel_size=3, stride=2, padding=1)\n        x = self.stage1(x)\n        x = self.stage2(x)\n        x = self.stage3(x)\n        x = self.stage4(x)\n        x = F.adaptive_avg_pool2d(x, output_size=1)\n        return x\n\n    def forward(self, x):\n        x = self._forward_conv(x)\n        x = x.view(x.size(0), -1)\n        x = self.fc(x)\n        return x\n"""
pytorch_image_classification/models/imagenet/vgg.py,5,"b""import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom ..initializer import create_initializer\n\n\nclass Network(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n\n        model_config = config.model.vgg\n        self.use_bn = model_config.use_bn\n        n_channels = model_config.n_channels\n        n_layers = model_config.n_layers\n\n        self.stage1 = self._make_stage(config.dataset.n_channels,\n                                       n_channels[0], n_layers[0])\n        self.stage2 = self._make_stage(n_channels[0], n_channels[1],\n                                       n_layers[1])\n        self.stage3 = self._make_stage(n_channels[1], n_channels[2],\n                                       n_layers[2])\n        self.stage4 = self._make_stage(n_channels[2], n_channels[3],\n                                       n_layers[3])\n        self.stage5 = self._make_stage(n_channels[3], n_channels[4],\n                                       n_layers[4])\n\n        # compute conv feature size\n        with torch.no_grad():\n            dummy_data = torch.zeros(\n                (1, config.dataset.n_channels, config.dataset.image_size,\n                 config.dataset.image_size),\n                dtype=torch.float32)\n            self.feature_size = self._forward_conv(dummy_data).view(\n                -1).shape[0]\n\n        self.fc1 = nn.Linear(self.feature_size, 4096)\n        self.fc2 = nn.Linear(4096, 4096)\n        self.fc3 = nn.Linear(4096, config.dataset.n_classes)\n\n        # initialize weights\n        initializer = create_initializer(config.model.init_mode)\n        self.apply(initializer)\n\n    def _make_stage(self, in_channels, out_channels, n_blocks):\n        stage = nn.Sequential()\n        for index in range(n_blocks):\n            if index == 0:\n                conv = nn.Conv2d(\n                    in_channels,\n                    out_channels,\n                    kernel_size=3,\n                    stride=1,\n                    padding=1,\n                )\n            else:\n                conv = nn.Conv2d(\n                    out_channels,\n                    out_channels,\n                    kernel_size=3,\n                    stride=1,\n                    padding=1,\n                )\n            stage.add_module(f'conv{index}', conv)\n            if self.use_bn:\n                stage.add_module(f'bn{index}', nn.BatchNorm2d(out_channels))\n            stage.add_module('relu', nn.ReLU(inplace=True))\n        stage.add_module('pool', nn.MaxPool2d(kernel_size=2, stride=2))\n        return stage\n\n    def _forward_conv(self, x):\n        x = self.stage1(x)\n        x = self.stage2(x)\n        x = self.stage3(x)\n        x = self.stage4(x)\n        x = self.stage5(x)\n        return x\n\n    def forward(self, x):\n        x = self._forward_conv(x)\n        x = x.view(x.size(0), -1)\n        x = F.dropout(F.relu(self.fc1(x), inplace=True),\n                      training=self.training)\n        x = F.dropout(F.relu(self.fc2(x), inplace=True),\n                      training=self.training)\n        x = self.fc3(x)\n        return x\n"""
