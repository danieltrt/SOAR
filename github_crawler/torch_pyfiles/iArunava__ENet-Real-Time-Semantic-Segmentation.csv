file_path,api_count,code
init.py,1,"b'import numpy as np\nimport argparse\nfrom train import *\nfrom test import *\n\ncolor_map = {\n    \'unlabeled\'     : (  0,  0,  0),\n    \'dynamic\'       : (111, 74,  0),\n    \'ground\'        : ( 81,  0, 81),\n    \'road\'          : (128, 64,128),\n    \'sidewalk\'      : (244, 35,232),\n    \'parking\'       : (250,170,160),\n    \'rail track\'    : (230,150,140),\n    \'building\'      : ( 70, 70, 70),\n    \'wall\'          : (102,102,156),\n    \'fence\'         : (190,153,153),\n    \'guard rail\'    : (180,165,180),\n    \'bridge\'        : (150,100,100),\n    \'tunnel\'        : (150,120, 90),\n    \'pole\'          : (153,153,153),\n    \'traffic light\' : (250,170, 30),\n    \'traffic sign\'  : (220,220,  0),\n    \'vegetation\'    : (107,142, 35),\n    \'terrain\'       : (152,251,152),\n    \'sky\'           : ( 70,130,180),\n    \'person\'        : (220, 20, 60),\n    \'rider\'         : (255,  0,  0),\n    \'car\'           : (  0,  0,142),\n    \'truck\'         : (  0,  0, 70),\n    \'bus\'           : (  0, 60,100),\n    \'caravan\'       : (  0,  0, 90),\n    \'trailer\'       : (  0,  0,110),\n    \'train\'         : (  0, 80,100),\n    \'motorcycle\'    : (  0,  0,230),\n    \'bicycle\'       : (119, 11, 32)\n}\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser()\n\n    parser.add_argument(\'-m\',\n                        type=str,\n                        default=\'./datasets/CamVid/ckpt-camvid-enet.pth\',\n                        help=\'The path to the pretrained enet model\')\n\n    parser.add_argument(\'-i\', \'--image-path\',\n                        type=str,\n                        help=\'The path to the image to perform semantic segmentation\')\n\n    parser.add_argument(\'-rh\', \'--resize-height\',\n                        type=int,\n                        default=512,\n                        help=\'The height for the resized image\')\n\n    parser.add_argument(\'-rw\', \'--resize-width\',\n                        type=int,\n                        default=512,\n                        help=\'The width for the resized image\')\n\n    parser.add_argument(\'-lr\', \'--learning-rate\',\n                        type=float,\n                        default=5e-4,\n                        help=\'The learning rate\')\n\n    parser.add_argument(\'-bs\', \'--batch-size\',\n                        type=int,\n                        default=10,\n                        help=\'The batch size\')\n\n    parser.add_argument(\'-wd\', \'--weight-decay\',\n                        type=float,\n                        default=2e-4,\n                        help=\'The weight decay\')\n\n    parser.add_argument(\'-c\', \'--constant\',\n                        type=float,\n                        default=1.02,\n                        help=\'The constant used for calculating the class weights\')\n\n    parser.add_argument(\'-e\', \'--epochs\',\n                        type=int,\n                        default=102,\n                        help=\'The number of epochs\')\n\n    parser.add_argument(\'-nc\', \'--num-classes\',\n                        type=int,\n                        default=102,\n                        help=\'The number of epochs\')\n\n    parser.add_argument(\'-se\', \'--save-every\',\n                        type=int,\n                        default=10,\n                        help=\'The number of epochs after which to save a model\')\n\n    parser.add_argument(\'-iptr\', \'--input-path-train\',\n                        type=str,\n                        default=\'./datasets/CamVid/train/\',\n                        help=\'The path to the input dataset\')\n\n    parser.add_argument(\'-lptr\', \'--label-path-train\',\n                        type=str,\n                        default=\'./datasets/CamVid/trainannot/\',\n                        help=\'The path to the label dataset\')\n\n    parser.add_argument(\'-ipv\', \'--input-path-val\',\n                        type=str,\n                        default=\'./datasets/CamVid/val/\',\n                        help=\'The path to the input dataset\')\n\n    parser.add_argument(\'-lpv\', \'--label-path-val\',\n                        type=str,\n                        default=\'./datasets/CamVid/valannot/\',\n                        help=\'The path to the label dataset\')\n\n    parser.add_argument(\'-iptt\', \'--input-path-test\',\n                        type=str,\n                        default=\'./datasets/CamVid/test/\',\n                        help=\'The path to the input dataset\')\n\n    parser.add_argument(\'-lptt\', \'--label-path-test\',\n                        type=str,\n                        default=\'./datasets/CamVid/testannot/\',\n                        help=\'The path to the label dataset\')\n\n    parser.add_argument(\'-pe\', \'--print-every\',\n                        type=int,\n                        default=1,\n                        help=\'The number of epochs after which to print the training loss\')\n\n    parser.add_argument(\'-ee\', \'--eval-every\',\n                        type=int,\n                        default=10,\n                        help=\'The number of epochs after which to print the validation loss\')\n\n    parser.add_argument(\'--cuda\',\n                        type=bool,\n                        default=False,\n                        help=\'Whether to use cuda or not\')\n\n    parser.add_argument(\'--mode\',\n                        choices=[\'train\', \'test\'],\n                        default=\'train\',\n                        help=\'Whether to train or test\')\n    \n    FLAGS, unparsed = parser.parse_known_args()\n\n    FLAGS.cuda = torch.device(\'cuda:0\' if torch.cuda.is_available() and FLAGS.cuda \\\n                               else \'cpu\')\n\n    if FLAGS.mode.lower() == \'train\':\n        train(FLAGS)\n    elif FLAGS.mode.lower() == \'test\':\n        test(FLAGS)\n    else:\n        raise RuntimeError(\'Unknown mode passed. \\n Mode passed should be either \\\n                            of ""train"" or ""test""\')\n'"
test.py,4,"b""import torch\nimport torch.nn as nn\nfrom utils import *\nfrom models.ENet import ENet\nimport sys\nimport os\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\n\ndef test(FLAGS):\n    # Check if the pretrained model is available\n    if not FLAGS.m.endswith('.pth'):\n        raise RuntimeError('Unknown file passed. Must end with .pth')\n    if FLAGS.image_path is None or not os.path.exists(FLAGS.image_path):\n        raise RuntimeError('An image file path must be passed')\n    \n    h = FLAGS.resize_height\n    w = FLAGS.resize_width\n\n    checkpoint = torch.load(FLAGS.m,  map_location=FLAGS.cuda)\n    \n    # Assuming the dataset is camvid\n    enet = ENet(12)\n    enet.load_state_dict(checkpoint['state_dict'])\n\n    tmg_ = plt.imread(FLAGS.image_path)\n    tmg_ = cv2.resize(tmg_, (h, w), cv2.INTER_NEAREST)\n    tmg = torch.tensor(tmg_).unsqueeze(0).float()\n    tmg = tmg.transpose(2, 3).transpose(1, 2)\n\n    with torch.no_grad():\n        out1 = enet(tmg.float()).squeeze(0)\n    \n    #smg_ = Image.open('/content/training/semantic/' + fname)\n    #smg_ = cv2.resize(np.array(smg_), (512, 512), cv2.INTER_NEAREST)\n\n    b_ = out1.data.max(0)[1].cpu().numpy()\n\n    decoded_segmap = decode_segmap(b_)\n\n    images = {\n        0 : ['Input Image', tmg_],\n        1 : ['Predicted Segmentation', b_],\n    }\n\n    show_images(images)\n"""
train.py,5,"b""import torch\nimport torch.nn as nn\nfrom utils import *\nfrom models.ENet import ENet\nimport sys\nfrom tqdm import tqdm\n\ndef train(FLAGS):\n\n    # Defining the hyperparameters\n    device =  FLAGS.cuda\n    batch_size = FLAGS.batch_size\n    epochs = FLAGS.epochs\n    lr = FLAGS.learning_rate\n    print_every = FLAGS.print_every\n    eval_every = FLAGS.eval_every\n    save_every = FLAGS.save_every\n    nc = FLAGS.num_classes\n    wd = FLAGS.weight_decay\n    ip = FLAGS.input_path_train\n    lp = FLAGS.label_path_train\n    ipv = FLAGS.input_path_val\n    lpv = FLAGS.label_path_val\n    print ('[INFO]Defined all the hyperparameters successfully!')\n    \n    # Get the class weights\n    print ('[INFO]Starting to define the class weights...')\n    pipe = loader(ip, lp, batch_size='all')\n    class_weights = get_class_weights(pipe, nc)\n    print ('[INFO]Fetched all class weights successfully!')\n\n    # Get an instance of the model\n    enet = ENet(nc)\n    print ('[INFO]Model Instantiated!')\n    \n    # Move the model to cuda if available\n    enet = enet.to(device)\n\n    # Define the criterion and the optimizer\n    criterion = nn.CrossEntropyLoss(weight=torch.FloatTensor(class_weights).to(device))\n    optimizer = torch.optim.Adam(enet.parameters(),\n                                 lr=lr,\n                                 weight_decay=wd)\n    print ('[INFO]Defined the loss function and the optimizer')\n\n    # Training Loop starts\n    print ('[INFO]Staring Training...')\n    print ()\n\n    train_losses = []\n    eval_losses = []\n    \n    # Assuming we are using the CamVid Dataset\n    bc_train = 367 // batch_size\n    bc_eval = 101 // batch_size\n\n    pipe = loader(ip, lp, batch_size)\n    eval_pipe = loader(ipv, lpv, batch_size)\n\n    epochs = epochs\n            \n    for e in range(1, epochs+1):\n            \n        train_loss = 0\n        print ('-'*15,'Epoch %d' % e, '-'*15)\n        \n        enet.train()\n        \n        for _ in tqdm(range(bc_train)):\n            X_batch, mask_batch = next(pipe)\n            \n            #assert (X_batch >= 0. and X_batch <= 1.0).all()\n            \n            X_batch, mask_batch = X_batch.to(device), mask_batch.to(device)\n\n            optimizer.zero_grad()\n\n            out = enet(X_batch.float())\n\n            loss = criterion(out, mask_batch.long())\n            loss.backward()\n            optimizer.step()\n\n            train_loss += loss.item()\n\n            \n        print ()\n        train_losses.append(train_loss)\n        \n        if (e+1) % print_every == 0:\n            print ('Epoch {}/{}...'.format(e, epochs),\n                    'Loss {:6f}'.format(train_loss))\n        \n        if e % eval_every == 0:\n            with torch.no_grad():\n                enet.eval()\n                \n                eval_loss = 0\n                \n                for _ in tqdm(range(bc_eval)):\n                    inputs, labels = next(eval_pipe)\n\n                    inputs, labels = inputs.to(device), labels.to(device)\n                    out = enet(inputs)\n                    \n                    loss = criterion(out, labels.long())\n\n                    eval_loss += loss.item()\n\n                print ()\n                print ('Loss {:6f}'.format(eval_loss))\n                \n                eval_losses.append(eval_loss)\n            \n        if e % save_every == 0:\n            checkpoint = {\n                'epochs' : e,\n                'state_dict' : enet.state_dict()\n            }\n            torch.save(checkpoint, './ckpt-enet-{}-{}.pth'.format(e, train_loss))\n            print ('Model saved!')\n\n        print ('Epoch {}/{}...'.format(e+1, epochs),\n               'Total Mean Loss: {:6f}'.format(sum(train_losses) / epochs))\n\n    print ('[INFO]Training Process complete!')\n"""
utils.py,2,"b'import numpy as np\nimport cv2\nimport matplotlib.pyplot as plt\nimport os\nfrom PIL import Image\nimport torch\n\ndef create_class_mask(img, color_map, is_normalized_img=True, is_normalized_map=False, show_masks=False):\n    """"""\n    Function to create C matrices from the segmented image, where each of the C matrices is for one class\n    with all ones at the pixel positions where that class is present\n\n    img = The segmented image\n\n    color_map = A list with tuples that contains all the RGB values for each color that represents\n                some class in that image\n\n    is_normalized_img = Boolean - Whether the image is normalized or not\n                        If normalized, then the image is multiplied with 255\n\n    is_normalized_map = Boolean - Represents whether the color map is normalized or not, if so\n                        then the color map values are multiplied with 255\n\n    show_masks = Wherether to show the created masks or not\n    """"""\n\n    if is_normalized_img and (not is_normalized_map):\n        img *= 255\n\n    if is_normalized_map and (not is_normalized_img):\n        img = img / 255\n    \n    mask = []\n    hw_tuple = img.shape[:-1]\n    for color in color_map:\n        color_img = []\n        for idx in range(3):\n            color_img.append(np.ones(hw_tuple) * color[idx])\n\n        color_img = np.array(color_img, dtype=np.uint8).transpose(1, 2, 0)\n\n        mask.append(np.uint8((color_img == img).sum(axis = -1) == 3))\n\n    return np.array(mask)\n\n\ndef loader(training_path, segmented_path, batch_size, h=512, w=512):\n    """"""\n    The Loader to generate inputs and labels from the Image and Segmented Directory\n\n    Arguments:\n\n    training_path - str - Path to the directory that contains the training images\n\n    segmented_path - str - Path to the directory that contains the segmented images\n\n    batch_size - int - the batch size\n\n    yields inputs and labels of the batch size\n    """"""\n\n    filenames_t = os.listdir(training_path)\n    total_files_t = len(filenames_t)\n    \n    filenames_s = os.listdir(segmented_path)\n    total_files_s = len(filenames_s)\n    \n    assert(total_files_t == total_files_s)\n    \n    if str(batch_size).lower() == \'all\':\n        batch_size = total_files_s\n    \n    idx = 0\n    while(1):\n        batch_idxs = np.random.randint(0, total_files_s, batch_size)\n            \n        \n        inputs = []\n        labels = []\n        \n        for jj in batch_idxs:\n            img = plt.imread(training_path + filenames_t[jj])\n            img = cv2.resize(img, (h, w), cv2.INTER_NEAREST)\n            inputs.append(img)\n            \n            img = Image.open(segmented_path + filenames_s[jj])\n            img = np.array(img)\n            img = cv2.resize(img, (h, w), cv2.INTER_NEAREST)\n            labels.append(img)\n         \n        inputs = np.stack(inputs, axis=2)\n        inputs = torch.tensor(inputs).transpose(0, 2).transpose(1, 3)\n        \n        labels = torch.tensor(labels)\n        \n        yield inputs, labels\n\n\ndef decode_segmap(image):\n    Sky = [128, 128, 128]\n    Building = [128, 0, 0]\n    Pole = [192, 192, 128]\n    Road_marking = [255, 69, 0]\n    Road = [128, 64, 128]\n    Pavement = [60, 40, 222]\n    Tree = [128, 128, 0]\n    SignSymbol = [192, 128, 128]\n    Fence = [64, 64, 128]\n    Car = [64, 0, 128]\n    Pedestrian = [64, 64, 0]\n    Bicyclist = [0, 128, 192]\n\n    label_colors = np.array([Sky, Building, Pole, Road_marking, Road, \n                              Pavement, Tree, SignSymbol, Fence, Car, \n                              Pedestrian, Bicyclist]).astype(np.uint8)\n\n    r = np.zeros_like(image).astype(np.uint8)\n    g = np.zeros_like(image).astype(np.uint8)\n    b = np.zeros_like(image).astype(np.uint8)\n\n    for label in range(len(label_colors)):\n            r[image == label] = label_colors[label, 0]\n            g[image == label] = label_colors[label, 1]\n            b[image == label] = label_colors[label, 2]\n\n    rgb = np.zeros((image.shape[0], image.shape[1], 3)).astype(np.uint8)\n    rgb[:, :, 0] = r\n    rgb[:, :, 1] = g\n    rgb[:, :, 2] = b\n\n    return rgb\n\ndef show_images(images, in_row=True):\n    \'\'\'\n    Helper function to show 3 images\n    \'\'\'\n    total_images = len(images)\n\n    rc_tuple = (1, total_images)\n    if not in_row:\n        rc_tuple = (total_images, 1)\n    \n\t#figure = plt.figure(figsize=(20, 10))\n    for ii in range(len(images)):\n        plt.subplot(*rc_tuple, ii+1)\n        plt.title(images[ii][0])\n        plt.axis(\'off\')\n        plt.imshow(images[ii][1])\n    plt.show()\n\ndef get_class_weights(loader, num_classes, c=1.02):\n    \'\'\'\n    This class return the class weights for each class\n    \n    Arguments:\n    - loader : The generator object which return all the labels at one iteration\n               Do Note: That this class expects all the labels to be returned in\n               one iteration\n\n    - num_classes : The number of classes\n\n    Return:\n    - class_weights : An array equal in length to the number of classes\n                      containing the class weights for each class\n    \'\'\'\n\n    _, labels = next(loader)\n    all_labels = labels.flatten()\n    each_class = np.bincount(all_labels, minlength=num_classes)\n    prospensity_score = each_class / len(all_labels)\n    class_weights = 1 / (np.log(c + prospensity_score))\n    return class_weights\n'"
models/ASNeck.py,4,"b'###################################################\n# Copyright (c) 2019                              #\n# Authors: @iArunava <iarunavaofficial@gmail.com> #\n#          @AvivSham <mista2311@gmail.com>        #\n#                                                 #\n# License: BSD License 3.0                        #\n#                                                 #\n# The Code in this file is distributed for free   #\n# usage and modification with proper linkage back #\n# to this repository.                             #\n###################################################\n\nimport torch\nimport torch.nn as nn\n\nclass ASNeck(nn.Module):\n    def __init__(self, in_channels, out_channels, projection_ratio=4):\n        \n        super().__init__()\n        \n        # Define class variables\n        self.in_channels = in_channels\n        self.reduced_depth = int(in_channels / projection_ratio)\n        self.out_channels = out_channels\n        \n        self.dropout = nn.Dropout2d(p=0.1)\n        \n        self.conv1 = nn.Conv2d(in_channels = self.in_channels,\n                               out_channels = self.reduced_depth,\n                               kernel_size = 1,\n                               stride = 1,\n                               padding = 0,\n                               bias = False)\n        \n        self.prelu1 = nn.PReLU()\n        \n        self.conv21 = nn.Conv2d(in_channels = self.reduced_depth,\n                                  out_channels = self.reduced_depth,\n                                  kernel_size = (1, 5),\n                                  stride = 1,\n                                  padding = (0, 2),\n                                  bias = False)\n        \n        self.conv22 = nn.Conv2d(in_channels = self.reduced_depth,\n                                  out_channels = self.reduced_depth,\n                                  kernel_size = (5, 1),\n                                  stride = 1,\n                                  padding = (2, 0),\n                                  bias = False)\n        \n        self.prelu2 = nn.PReLU()\n        \n        self.conv3 = nn.Conv2d(in_channels = self.reduced_depth,\n                                  out_channels = self.out_channels,\n                                  kernel_size = 1,\n                                  stride = 1,\n                                  padding = 0,\n                                  bias = False)\n        \n        self.prelu3 = nn.PReLU()\n        \n        self.batchnorm = nn.BatchNorm2d(self.reduced_depth)\n        self.batchnorm2 = nn.BatchNorm2d(self.out_channels)\n        \n    def forward(self, x):\n        bs = x.size()[0]\n        x_copy = x\n        \n        # Side Branch\n        x = self.conv1(x)\n        x = self.batchnorm(x)\n        x = self.prelu1(x)\n        \n        x = self.conv21(x)\n        x = self.conv22(x)\n        x = self.batchnorm(x)\n        x = self.prelu2(x)\n        \n        x = self.conv3(x)\n                \n        x = self.dropout(x)\n        x = self.batchnorm2(x)\n        \n        # Main Branch\n        \n        if self.in_channels != self.out_channels:\n            out_shape = self.out_channels - self.in_channels\n            extras = torch.zeros((bs, out_shape, x.shape[2], x.shape[3]))\n            if torch.cuda.is_available():\n                extras = extras.cuda()\n            x_copy = torch.cat((x_copy, extras), dim = 1)\n        \n        # Sum of main and side branches\n        x = x + x_copy\n        x = self.prelu3(x)\n        \n        return x\n'"
models/ENet.py,1,"b'##################################################################\n# Reproducing the paper                                          #\n# ENet - Real Time Semantic Segmentation                         #\n# Paper: https://arxiv.org/pdf/1606.02147.pdf                    #\n#                                                                #\n# Copyright (c) 2019                                             #\n# Authors: @iArunava <iarunavaofficial@gmail.com>                #\n#          @AvivSham <mista2311@gmail.com>                       #\n#                                                                #\n# License: BSD License 3.0                                       #\n#                                                                #\n# The Code in this file is distributed for free                  #\n# usage and modification with proper credits                     #\n# directing back to this repository.                             #\n##################################################################\n\nimport torch\nimport torch.nn as nn\nfrom .InitialBlock import InitialBlock\nfrom .RDDNeck import RDDNeck\nfrom .UBNeck import UBNeck\nfrom .ASNeck import ASNeck\n\nclass ENet(nn.Module):\n    def __init__(self, C):\n        super().__init__()\n        \n        # Define class variables\n        self.C = C\n        \n        # The initial block\n        self.init = InitialBlock()\n        \n        \n        # The first bottleneck\n        self.b10 = RDDNeck(dilation=1, \n                           in_channels=16, \n                           out_channels=64, \n                           down_flag=True, \n                           p=0.01)\n        \n        self.b11 = RDDNeck(dilation=1, \n                           in_channels=64, \n                           out_channels=64, \n                           down_flag=False, \n                           p=0.01)\n        \n        self.b12 = RDDNeck(dilation=1, \n                           in_channels=64, \n                           out_channels=64, \n                           down_flag=False, \n                           p=0.01)\n        \n        self.b13 = RDDNeck(dilation=1, \n                           in_channels=64, \n                           out_channels=64, \n                           down_flag=False, \n                           p=0.01)\n        \n        self.b14 = RDDNeck(dilation=1, \n                           in_channels=64, \n                           out_channels=64, \n                           down_flag=False, \n                           p=0.01)\n        \n        \n        # The second bottleneck\n        self.b20 = RDDNeck(dilation=1, \n                           in_channels=64, \n                           out_channels=128, \n                           down_flag=True)\n        \n        self.b21 = RDDNeck(dilation=1, \n                           in_channels=128, \n                           out_channels=128, \n                           down_flag=False)\n        \n        self.b22 = RDDNeck(dilation=2, \n                           in_channels=128, \n                           out_channels=128, \n                           down_flag=False)\n        \n        self.b23 = ASNeck(in_channels=128, \n                          out_channels=128)\n        \n        self.b24 = RDDNeck(dilation=4, \n                           in_channels=128, \n                           out_channels=128, \n                           down_flag=False)\n        \n        self.b25 = RDDNeck(dilation=1, \n                           in_channels=128, \n                           out_channels=128, \n                           down_flag=False)\n        \n        self.b26 = RDDNeck(dilation=8, \n                           in_channels=128, \n                           out_channels=128, \n                           down_flag=False)\n        \n        self.b27 = ASNeck(in_channels=128, \n                          out_channels=128)\n        \n        self.b28 = RDDNeck(dilation=16, \n                           in_channels=128, \n                           out_channels=128, \n                           down_flag=False)\n        \n        \n        # The third bottleneck\n        self.b31 = RDDNeck(dilation=1, \n                           in_channels=128, \n                           out_channels=128, \n                           down_flag=False)\n        \n        self.b32 = RDDNeck(dilation=2, \n                           in_channels=128, \n                           out_channels=128, \n                           down_flag=False)\n        \n        self.b33 = ASNeck(in_channels=128, \n                          out_channels=128)\n        \n        self.b34 = RDDNeck(dilation=4, \n                           in_channels=128, \n                           out_channels=128, \n                           down_flag=False)\n        \n        self.b35 = RDDNeck(dilation=1, \n                           in_channels=128, \n                           out_channels=128, \n                           down_flag=False)\n        \n        self.b36 = RDDNeck(dilation=8, \n                           in_channels=128, \n                           out_channels=128, \n                           down_flag=False)\n        \n        self.b37 = ASNeck(in_channels=128, \n                          out_channels=128)\n        \n        self.b38 = RDDNeck(dilation=16, \n                           in_channels=128, \n                           out_channels=128, \n                           down_flag=False)\n        \n        \n        # The fourth bottleneck\n        self.b40 = UBNeck(in_channels=128, \n                          out_channels=64, \n                          relu=True)\n        \n        self.b41 = RDDNeck(dilation=1, \n                           in_channels=64, \n                           out_channels=64, \n                           down_flag=False, \n                           relu=True)\n        \n        self.b42 = RDDNeck(dilation=1, \n                           in_channels=64, \n                           out_channels=64, \n                           down_flag=False, \n                           relu=True)\n        \n        \n        # The fifth bottleneck\n        self.b50 = UBNeck(in_channels=64, \n                          out_channels=16, \n                          relu=True)\n        \n        self.b51 = RDDNeck(dilation=1, \n                           in_channels=16, \n                           out_channels=16, \n                           down_flag=False, \n                           relu=True)\n        \n        \n        # Final ConvTranspose Layer\n        self.fullconv = nn.ConvTranspose2d(in_channels=16, \n                                           out_channels=self.C, \n                                           kernel_size=3, \n                                           stride=2, \n                                           padding=1, \n                                           output_padding=1,\n                                           bias=False)\n        \n        \n    def forward(self, x):\n        \n        # The initial block\n        x = self.init(x)\n        \n        # The first bottleneck\n        x, i1 = self.b10(x)\n        x = self.b11(x)\n        x = self.b12(x)\n        x = self.b13(x)\n        x = self.b14(x)\n        \n        # The second bottleneck\n        x, i2 = self.b20(x)\n        x = self.b21(x)\n        x = self.b22(x)\n        x = self.b23(x)\n        x = self.b24(x)\n        x = self.b25(x)\n        x = self.b26(x)\n        x = self.b27(x)\n        x = self.b28(x)\n        \n        # The third bottleneck\n        x = self.b31(x)\n        x = self.b32(x)\n        x = self.b33(x)\n        x = self.b34(x)\n        x = self.b35(x)\n        x = self.b36(x)\n        x = self.b37(x)\n        x = self.b38(x)\n        \n        # The fourth bottleneck\n        x = self.b40(x, i2)\n        x = self.b41(x)\n        x = self.b42(x)\n        \n        # The fifth bottleneck\n        x = self.b50(x, i1)\n        x = self.b51(x)\n        \n        # Final ConvTranspose Layer\n        x = self.fullconv(x)\n        \n        return x\n'"
models/InitialBlock.py,2,"b'###################################################\n# Copyright (c) 2019                              #\n# Authors: @iArunava <iarunavaofficial@gmail.com> #\n#          @AvivSham <mista2311@gmail.com>        #\n#                                                 #\n# License: BSD License 3.0                        #\n#                                                 #\n# The Code in this file is distributed for free   #\n# usage and modification with proper linkage back #\n# to this repository.                             #\n###################################################\n\nimport torch\nimport torch.nn as nn\n\nclass InitialBlock(nn.Module):\n    def __init__ (self,in_channels = 3,out_channels = 13):\n        super().__init__()\n\n\n        self.maxpool = nn.MaxPool2d(kernel_size=2, \n                                      stride = 2, \n                                      padding = 0)\n\n        self.conv = nn.Conv2d(in_channels, \n                                out_channels,\n                                kernel_size = 3,\n                                stride = 2, \n                                padding = 1)\n\n        self.prelu = nn.PReLU(16)\n\n        self.batchnorm = nn.BatchNorm2d(out_channels)\n  \n    def forward(self, x):\n        \n        main = self.conv(x)\n        main = self.batchnorm(main)\n        \n        side = self.maxpool(x)\n        \n        x = torch.cat((main, side), dim=1)\n        x = self.prelu(x)\n        \n        return x\n'"
models/RDDNeck.py,4,"b'###################################################\n# Copyright (c) 2019                              #\n# Authors: @iArunava <iarunavaofficial@gmail.com> #\n#          @AvivSham <mista2311@gmail.com>        #\n#                                                 #\n# License: BSD License 3.0                        #\n#                                                 #\n# The Code in this file is distributed for free   #\n# usage and modification with proper linkage back #\n# to this repository.                             #\n###################################################\n\nimport torch\nimport torch.nn as nn\ndevice = torch.device(""cuda:0"" if torch.cuda.is_available() else ""cpu"")\n\nclass RDDNeck(nn.Module):\n    def __init__(self, dilation, in_channels, out_channels, down_flag, relu=False, projection_ratio=4, p=0.1):\n        \n        super().__init__()\n        \n        # Define class variables\n        self.in_channels = in_channels\n        \n        self.out_channels = out_channels\n        self.dilation = dilation\n        self.down_flag = down_flag\n\n        if down_flag:\n            self.stride = 2\n            self.reduced_depth = int(in_channels // projection_ratio)\n        else:\n            self.stride = 1\n            self.reduced_depth = int(out_channels // projection_ratio)\n        \n        if relu:\n            activation = nn.ReLU()\n        else:\n            activation = nn.PReLU()\n        \n        self.maxpool = nn.MaxPool2d(kernel_size = 2,\n                                      stride = 2,\n                                      padding = 0, return_indices=True)\n        \n\n        \n        self.dropout = nn.Dropout2d(p=p)\n\n        self.conv1 = nn.Conv2d(in_channels = self.in_channels,\n                               out_channels = self.reduced_depth,\n                               kernel_size = 1,\n                               stride = 1,\n                               padding = 0,\n                               bias = False,\n                               dilation = 1)\n        \n        self.prelu1 = activation\n        \n        self.conv2 = nn.Conv2d(in_channels = self.reduced_depth,\n                                  out_channels = self.reduced_depth,\n                                  kernel_size = 3,\n                                  stride = self.stride,\n                                  padding = self.dilation,\n                                  bias = True,\n                                  dilation = self.dilation)\n                                  \n        self.prelu2 = activation\n        \n        self.conv3 = nn.Conv2d(in_channels = self.reduced_depth,\n                                  out_channels = self.out_channels,\n                                  kernel_size = 1,\n                                  stride = 1,\n                                  padding = 0,\n                                  bias = False,\n                                  dilation = 1)\n        \n        self.prelu3 = activation\n        \n        self.batchnorm = nn.BatchNorm2d(self.reduced_depth)\n        self.batchnorm2 = nn.BatchNorm2d(self.out_channels)\n        \n        \n    def forward(self, x):\n        \n        bs = x.size()[0]\n        x_copy = x\n        \n        # Side Branch\n        x = self.conv1(x)\n        x = self.batchnorm(x)\n        x = self.prelu1(x)\n        \n        x = self.conv2(x)\n        x = self.batchnorm(x)\n        x = self.prelu2(x)\n        \n        x = self.conv3(x)\n        x = self.batchnorm2(x)\n                \n        x = self.dropout(x)\n        \n        # Main Branch\n        if self.down_flag:\n            x_copy, indices = self.maxpool(x_copy)\n          \n        if self.in_channels != self.out_channels:\n            out_shape = self.out_channels - self.in_channels\n            extras = torch.zeros((bs, out_shape, x.shape[2], x.shape[3]))\n            extras = extras.to(device)\n            x_copy = torch.cat((x_copy, extras), dim = 1)\n\n        # Sum of main and side branches\n        x = x + x_copy\n        x = self.prelu3(x)\n        \n        if self.down_flag:\n            return x, indices\n        else:\n            return x\n'"
models/UBNeck.py,1,"b'import torch\nimport torch.nn as nn\n\nclass UBNeck(nn.Module):\n    def __init__(self, in_channels, out_channels, relu=False, projection_ratio=4):\n        \n        super().__init__()\n        \n        # Define class variables\n        self.in_channels = in_channels\n        self.reduced_depth = int(in_channels / projection_ratio)\n        self.out_channels = out_channels\n        \n        \n        if relu:\n            activation = nn.ReLU()\n        else:\n            activation = nn.PReLU()\n        \n        self.unpool = nn.MaxUnpool2d(kernel_size = 2,\n                                     stride = 2)\n        \n        self.main_conv = nn.Conv2d(in_channels = self.in_channels,\n                                    out_channels = self.out_channels,\n                                    kernel_size = 1)\n        \n        self.dropout = nn.Dropout2d(p=0.1)\n        \n        self.convt1 = nn.ConvTranspose2d(in_channels = self.in_channels,\n                               out_channels = self.reduced_depth,\n                               kernel_size = 1,\n                               padding = 0,\n                               bias = False)\n        \n        \n        self.prelu1 = activation\n        \n        self.convt2 = nn.ConvTranspose2d(in_channels = self.reduced_depth,\n                                  out_channels = self.reduced_depth,\n                                  kernel_size = 3,\n                                  stride = 2,\n                                  padding = 1,\n                                  output_padding = 1,\n                                  bias = False)\n        \n        self.prelu2 = activation\n        \n        self.convt3 = nn.ConvTranspose2d(in_channels = self.reduced_depth,\n                                  out_channels = self.out_channels,\n                                  kernel_size = 1,\n                                  padding = 0,\n                                  bias = False)\n        \n        self.prelu3 = activation\n        \n        self.batchnorm = nn.BatchNorm2d(self.reduced_depth)\n        self.batchnorm2 = nn.BatchNorm2d(self.out_channels)\n        \n    def forward(self, x, indices):\n        x_copy = x\n        \n        # Side Branch\n        x = self.convt1(x)\n        x = self.batchnorm(x)\n        x = self.prelu1(x)\n        \n        x = self.convt2(x)\n        x = self.batchnorm(x)\n        x = self.prelu2(x)\n        \n        x = self.convt3(x)\n        x = self.batchnorm2(x)\n        \n        x = self.dropout(x)\n        \n        # Main Branch\n        \n        x_copy = self.main_conv(x_copy)\n        x_copy = self.unpool(x_copy, indices, output_size=x.size())\n        \n        # Concat\n        x = x + x_copy\n        x = self.prelu3(x)\n        \n        return x\n'"
models/__init__.py,0,b''
