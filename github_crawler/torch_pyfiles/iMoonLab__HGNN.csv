file_path,api_count,code
train.py,11,"b'import os\nimport time\nimport copy\nimport torch\nimport torch.optim as optim\nimport pprint as pp\nimport utils.hypergraph_utils as hgut\nfrom models import HGNN\nfrom config import get_config\nfrom datasets import load_feature_construct_H\n\nos.environ[\'CUDA_VISIBLE_DEVICES\'] = \'0\'\ncfg = get_config(\'config/config.yaml\')\n\n# initialize data\ndata_dir = cfg[\'modelnet40_ft\'] if cfg[\'on_dataset\'] == \'ModelNet40\' \\\n    else cfg[\'ntu2012_ft\']\nfts, lbls, idx_train, idx_test, H = \\\n    load_feature_construct_H(data_dir,\n                             m_prob=cfg[\'m_prob\'],\n                             K_neigs=cfg[\'K_neigs\'],\n                             is_probH=cfg[\'is_probH\'],\n                             use_mvcnn_feature=cfg[\'use_mvcnn_feature\'],\n                             use_gvcnn_feature=cfg[\'use_gvcnn_feature\'],\n                             use_mvcnn_feature_for_structure=cfg[\'use_mvcnn_feature_for_structure\'],\n                             use_gvcnn_feature_for_structure=cfg[\'use_gvcnn_feature_for_structure\'])\nG = hgut.generate_G_from_H(H)\nn_class = int(lbls.max()) + 1\ndevice = torch.device(\'cuda:0\' if torch.cuda.is_available() else \'cpu\')\n\n# transform data to device\nfts = torch.Tensor(fts).to(device)\nlbls = torch.Tensor(lbls).squeeze().long().to(device)\nG = torch.Tensor(G).to(device)\nidx_train = torch.Tensor(idx_train).long().to(device)\nidx_test = torch.Tensor(idx_test).long().to(device)\n\n\ndef train_model(model, criterion, optimizer, scheduler, num_epochs=25, print_freq=500):\n    since = time.time()\n\n    best_model_wts = copy.deepcopy(model.state_dict())\n    best_acc = 0.0\n\n    for epoch in range(num_epochs):\n        if epoch % print_freq == 0:\n            print(\'-\' * 10)\n            print(f\'Epoch {epoch}/{num_epochs - 1}\')\n\n        # Each epoch has a training and validation phase\n        for phase in [\'train\', \'val\']:\n            if phase == \'train\':\n                scheduler.step()\n                model.train()  # Set model to training mode\n            else:\n                model.eval()  # Set model to evaluate mode\n\n            running_loss = 0.0\n            running_corrects = 0\n\n            idx = idx_train if phase == \'train\' else idx_test\n\n            # Iterate over data.\n            optimizer.zero_grad()\n            with torch.set_grad_enabled(phase == \'train\'):\n                outputs = model(fts, G)\n                loss = criterion(outputs[idx], lbls[idx])\n                _, preds = torch.max(outputs, 1)\n\n                # backward + optimize only if in training phase\n                if phase == \'train\':\n                    loss.backward()\n                    optimizer.step()\n\n            # statistics\n            running_loss += loss.item() * fts.size(0)\n            running_corrects += torch.sum(preds[idx] == lbls.data[idx])\n\n            epoch_loss = running_loss / len(idx)\n            epoch_acc = running_corrects.double() / len(idx)\n\n            if epoch % print_freq == 0:\n                print(f\'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}\')\n\n            # deep copy the model\n            if phase == \'val\' and epoch_acc > best_acc:\n                best_acc = epoch_acc\n                best_model_wts = copy.deepcopy(model.state_dict())\n\n        if epoch % print_freq == 0:\n            print(f\'Best val Acc: {best_acc:4f}\')\n            print(\'-\' * 20)\n\n    time_elapsed = time.time() - since\n    print(f\'\\nTraining complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s\')\n    print(f\'Best val Acc: {best_acc:4f}\')\n\n    # load best model weights\n    model.load_state_dict(best_model_wts)\n    return model\n\n\ndef _main():\n    print(f""Classification on {cfg[\'on_dataset\']} dataset!!! class number: {n_class}"")\n    print(f""use MVCNN feature: {cfg[\'use_mvcnn_feature\']}"")\n    print(f""use GVCNN feature: {cfg[\'use_gvcnn_feature\']}"")\n    print(f""use MVCNN feature for structure: {cfg[\'use_mvcnn_feature_for_structure\']}"")\n    print(f""use GVCNN feature for structure: {cfg[\'use_gvcnn_feature_for_structure\']}"")\n    print(\'Configuration -> Start\')\n    pp.pprint(cfg)\n    print(\'Configuration -> End\')\n\n    model_ft = HGNN(in_ch=fts.shape[1],\n                    n_class=n_class,\n                    n_hid=cfg[\'n_hid\'],\n                    dropout=cfg[\'drop_out\'])\n    model_ft = model_ft.to(device)\n\n    optimizer = optim.Adam(model_ft.parameters(), lr=cfg[\'lr\'],\n                           weight_decay=cfg[\'weight_decay\'])\n    # optimizer = optim.SGD(model_ft.parameters(), lr=0.01, weight_decay=cfg[\'weight_decay)\n    schedular = optim.lr_scheduler.MultiStepLR(optimizer,\n                                               milestones=cfg[\'milestones\'],\n                                               gamma=cfg[\'gamma\'])\n    criterion = torch.nn.CrossEntropyLoss()\n\n    model_ft = train_model(model_ft, criterion, optimizer, schedular, cfg[\'max_epoch\'], print_freq=cfg[\'print_freq\'])\n\n\nif __name__ == \'__main__\':\n    _main()\n'"
config/__init__.py,0,b'from .config import get_config\n'
config/config.py,0,"b""import os\nimport yaml\nimport os.path as osp\n\n\ndef get_config(dir='config/config.yaml'):\n    # add direction join function when parse the yaml file\n    def join(loader, node):\n        seq = loader.construct_sequence(node)\n        return os.path.sep.join(seq)\n\n    # add string concatenation function when parse the yaml file\n    def concat(loader, node):\n        seq = loader.construct_sequence(node)\n        seq = [str(tmp) for tmp in seq]\n        return ''.join(seq)\n\n    yaml.add_constructor('!join', join)\n    yaml.add_constructor('!concat', concat)\n    with open(dir, 'r') as f:\n        cfg = yaml.load(f)\n\n    check_dirs(cfg)\n\n    return cfg\n\n\ndef check_dir(folder, mk_dir=True):\n    if not osp.exists(folder):\n        if mk_dir:\n            print(f'making direction {folder}!')\n            os.mkdir(folder)\n        else:\n            raise Exception(f'Not exist direction {folder}')\n\n\ndef check_dirs(cfg):\n    check_dir(cfg['data_root'], mk_dir=False)\n\n    check_dir(cfg['result_root'])\n    check_dir(cfg['ckpt_folder'])\n    check_dir(cfg['result_sub_folder'])\n"""
datasets/__init__.py,0,b'from .data_helper import load_ft\nfrom .visual_data import load_feature_construct_H'
datasets/data_helper.py,0,"b""import scipy.io as scio\nimport numpy as np\n\n\ndef load_ft(data_dir, feature_name='GVCNN'):\n    data = scio.loadmat(data_dir)\n    lbls = data['Y'].astype(np.long)\n    if lbls.min() == 1:\n        lbls = lbls - 1\n    idx = data['indices'].item()\n\n    if feature_name == 'MVCNN':\n        fts = data['X'][0].item().astype(np.float32)\n    elif feature_name == 'GVCNN':\n        fts = data['X'][1].item().astype(np.float32)\n    else:\n        print(f'wrong feature name{feature_name}!')\n        raise IOError\n\n    idx_train = np.where(idx == 1)[0]\n    idx_test = np.where(idx == 0)[0]\n    return fts, lbls, idx_train, idx_test\n\n"""
datasets/visual_data.py,0,"b'from datasets import load_ft\nfrom utils import hypergraph_utils as hgut\n\n\ndef load_feature_construct_H(data_dir,\n                             m_prob=1,\n                             K_neigs=[10],\n                             is_probH=True,\n                             split_diff_scale=False,\n                             use_mvcnn_feature=False,\n                             use_gvcnn_feature=True,\n                             use_mvcnn_feature_for_structure=False,\n                             use_gvcnn_feature_for_structure=True):\n    """"""\n\n    :param data_dir: directory of feature data\n    :param m_prob: parameter in hypergraph incidence matrix construction\n    :param K_neigs: the number of neighbor expansion\n    :param is_probH: probability Vertex-Edge matrix or binary\n    :param use_mvcnn_feature:\n    :param use_gvcnn_feature:\n    :param use_mvcnn_feature_for_structure:\n    :param use_gvcnn_feature_for_structure:\n    :return:\n    """"""\n    # init feature\n    if use_mvcnn_feature or use_mvcnn_feature_for_structure:\n        mvcnn_ft, lbls, idx_train, idx_test = load_ft(data_dir, feature_name=\'MVCNN\')\n    if use_gvcnn_feature or use_gvcnn_feature_for_structure:\n        gvcnn_ft, lbls, idx_train, idx_test = load_ft(data_dir, feature_name=\'GVCNN\')\n    if \'mvcnn_ft\' not in dir() and \'gvcnn_ft\' not in dir():\n        raise Exception(\'None feature initialized\')\n\n    # construct feature matrix\n    fts = None\n    if use_mvcnn_feature:\n        fts = hgut.feature_concat(fts, mvcnn_ft)\n    if use_gvcnn_feature:\n        fts = hgut.feature_concat(fts, gvcnn_ft)\n    if fts is None:\n        raise Exception(f\'None feature used for model!\')\n\n    # construct hypergraph incidence matrix\n    print(\'Constructing hypergraph incidence matrix! \\n(It may take several minutes! Please wait patiently!)\')\n    H = None\n    if use_mvcnn_feature_for_structure:\n        tmp = hgut.construct_H_with_KNN(mvcnn_ft, K_neigs=K_neigs,\n                                        split_diff_scale=split_diff_scale,\n                                        is_probH=is_probH, m_prob=m_prob)\n        H = hgut.hyperedge_concat(H, tmp)\n    if use_gvcnn_feature_for_structure:\n        tmp = hgut.construct_H_with_KNN(gvcnn_ft, K_neigs=K_neigs,\n                                        split_diff_scale=split_diff_scale,\n                                        is_probH=is_probH, m_prob=m_prob)\n        H = hgut.hyperedge_concat(H, tmp)\n    if H is None:\n        raise Exception(\'None feature to construct hypergraph incidence matrix!\')\n\n    return fts, lbls, idx_train, idx_test, H\n'"
models/HGNN.py,1,"b'from torch import nn\nfrom models import HGNN_conv\nimport torch.nn.functional as F\n\n\nclass HGNN(nn.Module):\n    def __init__(self, in_ch, n_class, n_hid, dropout=0.5):\n        super(HGNN, self).__init__()\n        self.dropout = dropout\n        self.hgc1 = HGNN_conv(in_ch, n_hid)\n        self.hgc2 = HGNN_conv(n_hid, n_class)\n\n    def forward(self, x, G):\n        x = F.relu(self.hgc1(x, G))\n        x = F.dropout(x, self.dropout)\n        x = self.hgc2(x, G)\n        return x\n'"
models/__init__.py,0,"b'from .layers import HGNN_conv, HGNN_fc, HGNN_embedding, HGNN_classifier\nfrom .HGNN import HGNN\n'"
models/layers.py,6,"b""import math\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.nn.parameter import Parameter\n\n\nclass HGNN_conv(nn.Module):\n    def __init__(self, in_ft, out_ft, bias=True):\n        super(HGNN_conv, self).__init__()\n\n        self.weight = Parameter(torch.Tensor(in_ft, out_ft))\n        if bias:\n            self.bias = Parameter(torch.Tensor(out_ft))\n        else:\n            self.register_parameter('bias', None)\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        stdv = 1. / math.sqrt(self.weight.size(1))\n        self.weight.data.uniform_(-stdv, stdv)\n        if self.bias is not None:\n            self.bias.data.uniform_(-stdv, stdv)\n\n    def forward(self, x: torch.Tensor, G: torch.Tensor):\n        x = x.matmul(self.weight)\n        if self.bias is not None:\n            x = x + self.bias\n        x = G.matmul(x)\n        return x\n\n\nclass HGNN_fc(nn.Module):\n    def __init__(self, in_ch, out_ch):\n        super(HGNN_fc, self).__init__()\n        self.fc = nn.Linear(in_ch, out_ch)\n\n    def forward(self, x):\n        return self.fc(x)\n\n\nclass HGNN_embedding(nn.Module):\n    def __init__(self, in_ch, n_hid, dropout=0.5):\n        super(HGNN_embedding, self).__init__()\n        self.dropout = dropout\n        self.hgc1 = HGNN_conv(in_ch, n_hid)\n        self.hgc2 = HGNN_conv(n_hid, n_hid)\n\n    def forward(self, x, G):\n        x = F.relu(self.hgc1(x, G))\n        x = F.dropout(x, self.dropout)\n        x = F.relu(self.hgc2(x, G))\n        return x\n\n\nclass HGNN_classifier(nn.Module):\n    def __init__(self, n_hid, n_class):\n        super(HGNN_classifier, self).__init__()\n        self.fc1 = nn.Linear(n_hid, n_class)\n\n    def forward(self, x):\n        x = self.fc1(x)\n        return x"""
utils/hypergraph_utils.py,0,"b'# --------------------------------------------------------\n# Utility functions for Hypergraph\n#\n# Author: Yifan Feng\n# Date: November 2018\n# --------------------------------------------------------\nimport numpy as np\n\n\ndef Eu_dis(x):\n    """"""\n    Calculate the distance among each raw of x\n    :param x: N X D\n                N: the object number\n                D: Dimension of the feature\n    :return: N X N distance matrix\n    """"""\n    x = np.mat(x)\n    aa = np.sum(np.multiply(x, x), 1)\n    ab = x * x.T\n    dist_mat = aa + aa.T - 2 * ab\n    dist_mat[dist_mat < 0] = 0\n    dist_mat = np.sqrt(dist_mat)\n    dist_mat = np.maximum(dist_mat, dist_mat.T)\n    return dist_mat\n\n\ndef feature_concat(*F_list, normal_col=False):\n    """"""\n    Concatenate multiple modality feature. If the dimension of a feature matrix is more than two,\n    the function will reduce it into two dimension(using the last dimension as the feature dimension,\n    the other dimension will be fused as the object dimension)\n    :param F_list: Feature matrix list\n    :param normal_col: normalize each column of the feature\n    :return: Fused feature matrix\n    """"""\n    features = None\n    for f in F_list:\n        if f is not None and f != []:\n            # deal with the dimension that more than two\n            if len(f.shape) > 2:\n                f = f.reshape(-1, f.shape[-1])\n            # normal each column\n            if normal_col:\n                f_max = np.max(np.abs(f), axis=0)\n                f = f / f_max\n            # facing the first feature matrix appended to fused feature matrix\n            if features is None:\n                features = f\n            else:\n                features = np.hstack((features, f))\n    if normal_col:\n        features_max = np.max(np.abs(features), axis=0)\n        features = features / features_max\n    return features\n\n\ndef hyperedge_concat(*H_list):\n    """"""\n    Concatenate hyperedge group in H_list\n    :param H_list: Hyperedge groups which contain two or more hypergraph incidence matrix\n    :return: Fused hypergraph incidence matrix\n    """"""\n    H = None\n    for h in H_list:\n        if h is not None and h != []:\n            # for the first H appended to fused hypergraph incidence matrix\n            if H is None:\n                H = h\n            else:\n                if type(h) != list:\n                    H = np.hstack((H, h))\n                else:\n                    tmp = []\n                    for a, b in zip(H, h):\n                        tmp.append(np.hstack((a, b)))\n                    H = tmp\n    return H\n\n\ndef generate_G_from_H(H, variable_weight=False):\n    """"""\n    calculate G from hypgraph incidence matrix H\n    :param H: hypergraph incidence matrix H\n    :param variable_weight: whether the weight of hyperedge is variable\n    :return: G\n    """"""\n    if type(H) != list:\n        return _generate_G_from_H(H, variable_weight)\n    else:\n        G = []\n        for sub_H in H:\n            G.append(generate_G_from_H(sub_H, variable_weight))\n        return G\n\n\ndef _generate_G_from_H(H, variable_weight=False):\n    """"""\n    calculate G from hypgraph incidence matrix H\n    :param H: hypergraph incidence matrix H\n    :param variable_weight: whether the weight of hyperedge is variable\n    :return: G\n    """"""\n    H = np.array(H)\n    n_edge = H.shape[1]\n    # the weight of the hyperedge\n    W = np.ones(n_edge)\n    # the degree of the node\n    DV = np.sum(H * W, axis=1)\n    # the degree of the hyperedge\n    DE = np.sum(H, axis=0)\n\n    invDE = np.mat(np.diag(np.power(DE, -1)))\n    DV2 = np.mat(np.diag(np.power(DV, -0.5)))\n    W = np.mat(np.diag(W))\n    H = np.mat(H)\n    HT = H.T\n\n    if variable_weight:\n        DV2_H = DV2 * H\n        invDE_HT_DV2 = invDE * HT * DV2\n        return DV2_H, W, invDE_HT_DV2\n    else:\n        G = DV2 * H * W * invDE * HT * DV2\n        return G\n\n\ndef construct_H_with_KNN_from_distance(dis_mat, k_neig, is_probH=True, m_prob=1):\n    """"""\n    construct hypregraph incidence matrix from hypergraph node distance matrix\n    :param dis_mat: node distance matrix\n    :param k_neig: K nearest neighbor\n    :param is_probH: prob Vertex-Edge matrix or binary\n    :param m_prob: prob\n    :return: N_object X N_hyperedge\n    """"""\n    n_obj = dis_mat.shape[0]\n    # construct hyperedge from the central feature space of each node\n    n_edge = n_obj\n    H = np.zeros((n_obj, n_edge))\n    for center_idx in range(n_obj):\n        dis_mat[center_idx, center_idx] = 0\n        dis_vec = dis_mat[center_idx]\n        nearest_idx = np.array(np.argsort(dis_vec)).squeeze()\n        avg_dis = np.average(dis_vec)\n        if not np.any(nearest_idx[:k_neig] == center_idx):\n            nearest_idx[k_neig - 1] = center_idx\n\n        for node_idx in nearest_idx[:k_neig]:\n            if is_probH:\n                H[node_idx, center_idx] = np.exp(-dis_vec[0, node_idx] ** 2 / (m_prob * avg_dis) ** 2)\n            else:\n                H[node_idx, center_idx] = 1.0\n    return H\n\n\ndef construct_H_with_KNN(X, K_neigs=[10], split_diff_scale=False, is_probH=True, m_prob=1):\n    """"""\n    init multi-scale hypergraph Vertex-Edge matrix from original node feature matrix\n    :param X: N_object x feature_number\n    :param K_neigs: the number of neighbor expansion\n    :param split_diff_scale: whether split hyperedge group at different neighbor scale\n    :param is_probH: prob Vertex-Edge matrix or binary\n    :param m_prob: prob\n    :return: N_object x N_hyperedge\n    """"""\n    if len(X.shape) != 2:\n        X = X.reshape(-1, X.shape[-1])\n\n    if type(K_neigs) == int:\n        K_neigs = [K_neigs]\n\n    dis_mat = Eu_dis(X)\n    H = []\n    for k_neig in K_neigs:\n        H_tmp = construct_H_with_KNN_from_distance(dis_mat, k_neig, is_probH, m_prob)\n        if not split_diff_scale:\n            H = hyperedge_concat(H, H_tmp)\n        else:\n            H.append(H_tmp)\n    return H\n'"
