file_path,api_count,code
augment.py,0,"b'from PIL import Image, ImageEnhance, ImageOps\nimport numpy as np\nimport random\n\n# Non-random random\nrandom.seed(0)\n\nclass BasicPolicy(object):\n    def __init__(self, mirror_ratio = 0, flip_ratio = 0, color_change_ratio = 0, is_full_set_colors = False, add_noise_peak = 0.0, erase_ratio = -1.0):\n        # Random color channel order\n        from itertools import product, permutations\n        self.indices = list(product([0,1,2], repeat = 3)) if is_full_set_colors else list(permutations(range(3), 3))\n        self.indices.insert(0, [0,1,2]) # R,G,B\n        self.add_noise_peak = add_noise_peak\n\n        # Mirror and flip\n        self.color_change_ratio = color_change_ratio\n        self.mirror_ratio = mirror_ratio\n        self.flip_ratio = flip_ratio\n\n        # Erase\n        self.erase_ratio = erase_ratio\n\n    def __call__(self, img, depth):\n\n        # 0) Add poisson noise (e.g. choose peak value 20)\n        # https://stackoverflow.com/questions/19289470/adding-poisson-noise-to-an-image\n        if self.add_noise_peak > 0:\n            PEAK = self.add_noise_peak\n            img = np.random.poisson(np.clip(img, 0, 1) * PEAK) / PEAK\n\n        # 1) Color change\n        policy_idx = random.randint(0, len(self.indices) - 1)\n        if random.uniform(0, 1) >= self.color_change_ratio:\n            policy_idx = 0\n\n        img = img[...,list(self.indices[policy_idx])]\n\n        # 2) Mirror image\n        if random.uniform(0, 1) <= self.mirror_ratio:\n            img = img[...,::-1,:]\n            depth = depth[...,::-1,:]\n\n        # 3) Flip image vertically\n        if random.uniform(0, 1) < self.flip_ratio:\n            img = img[...,::-1,:,:]\n            depth = depth[...,::-1,:,:]\n\n        # 4) Erase random box\n        if random.uniform(0, 1) < self.erase_ratio:\n            img = self.eraser(img)\n\n        return img, depth\n\n    def __repr__(self):\n        return ""Basic Policy""\n\n    def eraser(self, input_img, p=0.5, s_l=0.02, s_h=0.4, r_1=0.3, r_2=1/0.3, v_l=0, v_h=255, pixel_level=True):\n        img_h, img_w, img_c = input_img.shape\n        p_1 = np.random.rand()\n\n        if p_1 > p:\n            return input_img\n\n        while True:\n            s = np.random.uniform(s_l, s_h) * img_h * img_w\n            r = np.random.uniform(r_1, r_2)\n            w = int(np.sqrt(s / r))\n            h = int(np.sqrt(s * r))\n            left = np.random.randint(0, img_w)\n            top = np.random.randint(0, img_h)\n\n            if left + w <= img_w and top + h <= img_h:\n                break\n\n        if pixel_level:\n            c = np.random.uniform(v_l, v_h, (h, w, img_c))\n        else:\n            c = np.random.uniform(v_l, v_h)\n\n        input_img[top:top + h, left:left + w, :] = c\n\n        return input_img\n        \n    def debug_img(self, img, depth, idx, i, prefix=\'\'):\n        from PIL import Image\n        aug_img = Image.fromarray(np.clip(np.uint8(img*255), 0, 255))\n        aug_img.save(prefix+str(idx)+""_""+str(i)+\'.jpg\',quality=99)\n        aug_img = Image.fromarray(np.clip(np.uint8(np.tile(depth*255,3)), 0, 255))\n        aug_img.save(prefix+str(idx)+""_""+str(i)+\'.depth.jpg\',quality=99)\n\n#\n# Original code at https://github.com/DeepVoltaire/AutoAugment\n#\nclass ImageNetPolicy(object):\n    """""" Randomly choose one of the best 24 Sub-policies on ImageNet.\n\n        Example:\n        >>> policy = ImageNetPolicy()\n        >>> transformed = policy(image)\n\n        Example as a PyTorch Transform:\n        >>> transform=transforms.Compose([\n        >>>     transforms.Resize(256),\n        >>>     ImageNetPolicy(),\n        >>>     transforms.ToTensor()])\n    """"""\n    def __init__(self, fillcolor=(128, 128, 128)):\n        self.policies = [\n            SubPolicy(0.4, ""posterize"", 8, 0.6, ""rotate"", 9, fillcolor),\n            SubPolicy(0.6, ""solarize"", 5, 0.6, ""autocontrast"", 5, fillcolor),\n            SubPolicy(0.8, ""equalize"", 8, 0.6, ""equalize"", 3, fillcolor),\n            SubPolicy(0.6, ""posterize"", 7, 0.6, ""posterize"", 6, fillcolor),\n            SubPolicy(0.4, ""equalize"", 7, 0.2, ""solarize"", 4, fillcolor),\n\n            SubPolicy(0.4, ""equalize"", 4, 0.8, ""rotate"", 8, fillcolor),\n            SubPolicy(0.6, ""solarize"", 3, 0.6, ""equalize"", 7, fillcolor),\n            SubPolicy(0.8, ""posterize"", 5, 1.0, ""equalize"", 2, fillcolor),\n            SubPolicy(0.2, ""rotate"", 3, 0.6, ""solarize"", 8, fillcolor),\n            SubPolicy(0.6, ""equalize"", 8, 0.4, ""posterize"", 6, fillcolor),\n\n            SubPolicy(0.8, ""rotate"", 8, 0.4, ""color"", 0, fillcolor),\n            SubPolicy(0.4, ""rotate"", 9, 0.6, ""equalize"", 2, fillcolor),\n            SubPolicy(0.0, ""equalize"", 7, 0.8, ""equalize"", 8, fillcolor),\n            SubPolicy(0.6, ""invert"", 4, 1.0, ""equalize"", 8, fillcolor),\n            SubPolicy(0.6, ""color"", 4, 1.0, ""contrast"", 8, fillcolor),\n\n            SubPolicy(0.8, ""rotate"", 8, 1.0, ""color"", 2, fillcolor),\n            SubPolicy(0.8, ""color"", 8, 0.8, ""solarize"", 7, fillcolor),\n            SubPolicy(0.4, ""sharpness"", 7, 0.6, ""invert"", 8, fillcolor),\n            SubPolicy(0.6, ""shearX"", 5, 1.0, ""equalize"", 9, fillcolor),\n            SubPolicy(0.4, ""color"", 0, 0.6, ""equalize"", 3, fillcolor),\n\n            SubPolicy(0.4, ""equalize"", 7, 0.2, ""solarize"", 4, fillcolor),\n            SubPolicy(0.6, ""solarize"", 5, 0.6, ""autocontrast"", 5, fillcolor),\n            SubPolicy(0.6, ""invert"", 4, 1.0, ""equalize"", 8, fillcolor),\n            SubPolicy(0.6, ""color"", 4, 1.0, ""contrast"", 8, fillcolor)\n        ]\n\n\n    def __call__(self, img):\n        policy_idx = random.randint(0, len(self.policies) - 1)\n        return self.policies[policy_idx](img)\n\n    def __repr__(self):\n        return ""AutoAugment ImageNet Policy""\n\n\nclass SubPolicy(object):\n    def __init__(self, p1, operation1, magnitude_idx1, p2, operation2, magnitude_idx2, fillcolor=(128, 128, 128)):\n        ranges = {\n            ""shearX"": np.linspace(0, 0.3, 10),\n            ""shearY"": np.linspace(0, 0.3, 10),\n            ""translateX"": np.linspace(0, 150 / 331, 10),\n            ""translateY"": np.linspace(0, 150 / 331, 10),\n            ""rotate"": np.linspace(0, 30, 10),\n            ""color"": np.linspace(0.0, 0.9, 10),\n            ""posterize"": np.round(np.linspace(8, 4, 10), 0).astype(np.int),\n            ""solarize"": np.linspace(256, 0, 10),\n            ""contrast"": np.linspace(0.0, 0.9, 10),\n            ""sharpness"": np.linspace(0.0, 0.9, 10),\n            ""brightness"": np.linspace(0.0, 0.9, 10),\n            ""autocontrast"": [0] * 10,\n            ""equalize"": [0] * 10,\n            ""invert"": [0] * 10\n        }\n\n        # from https://stackoverflow.com/questions/5252170/specify-image-filling-color-when-rotating-in-python-with-pil-and-setting-expand\n        def rotate_with_fill(img, magnitude):\n            rot = img.convert(""RGBA"").rotate(magnitude)\n            return Image.composite(rot, Image.new(""RGBA"", rot.size, (128,) * 4), rot).convert(img.mode)\n\n        func = {\n            ""shearX"": lambda img, magnitude: img.transform(\n                img.size, Image.AFFINE, (1, magnitude * random.choice([-1, 1]), 0, 0, 1, 0),\n                Image.BICUBIC, fillcolor=fillcolor),\n            ""shearY"": lambda img, magnitude: img.transform(\n                img.size, Image.AFFINE, (1, 0, 0, magnitude * random.choice([-1, 1]), 1, 0),\n                Image.BICUBIC, fillcolor=fillcolor),\n            ""translateX"": lambda img, magnitude: img.transform(\n                img.size, Image.AFFINE, (1, 0, magnitude * img.size[0] * random.choice([-1, 1]), 0, 1, 0),\n                fillcolor=fillcolor),\n            ""translateY"": lambda img, magnitude: img.transform(\n                img.size, Image.AFFINE, (1, 0, 0, 0, 1, magnitude * img.size[1] * random.choice([-1, 1])),\n                fillcolor=fillcolor),\n            #""rotate"": lambda img, magnitude: rotate_with_fill(img, magnitude),\n            # ""rotate"": lambda img, magnitude: img.rotate(magnitude * random.choice([-1, 1])),\n\t\t\t""rotate"": lambda img, magnitude: img,\n            ""color"": lambda img, magnitude: ImageEnhance.Color(img).enhance(1 + magnitude * random.choice([-1, 1])),\n            ""posterize"": lambda img, magnitude: ImageOps.posterize(img, magnitude),\n            ""solarize"": lambda img, magnitude: ImageOps.solarize(img, magnitude),\n            ""contrast"": lambda img, magnitude: ImageEnhance.Contrast(img).enhance(\n                1 + magnitude * random.choice([-1, 1])),\n            ""sharpness"": lambda img, magnitude: ImageEnhance.Sharpness(img).enhance(\n                1 + magnitude * random.choice([-1, 1])),\n            ""brightness"": lambda img, magnitude: ImageEnhance.Brightness(img).enhance(\n                1 + magnitude * random.choice([-1, 1])),\n            ""autocontrast"": lambda img, magnitude: ImageOps.autocontrast(img),\n            ""equalize"": lambda img, magnitude: ImageOps.equalize(img),\n            ""invert"": lambda img, magnitude: ImageOps.invert(img)\n        }\n\n        # self.name = ""{}_{:.2f}_and_{}_{:.2f}"".format(\n        #     operation1, ranges[operation1][magnitude_idx1],\n        #     operation2, ranges[operation2][magnitude_idx2])\n        self.p1 = p1\n        self.operation1 = func[operation1]\n        self.magnitude1 = ranges[operation1][magnitude_idx1]\n        self.p2 = p2\n        self.operation2 = func[operation2]\n        self.magnitude2 = ranges[operation2][magnitude_idx2]\n\n\n    def __call__(self, img):\n        if random.random() < self.p1: img = self.operation1(img, self.magnitude1)\n        if random.random() < self.p2: img = self.operation2(img, self.magnitude2)\n        return img'"
callbacks.py,0,"b""import io\nimport random\nimport numpy as np\nfrom PIL import Image\n\nimport keras\nfrom keras import backend as K\nfrom utils import DepthNorm, predict, evaluate\n\nimport tensorflow as tf\n\ndef make_image(tensor):\n    height, width, channel = tensor.shape\n    image = Image.fromarray(tensor.astype('uint8'))\n    output = io.BytesIO()\n    image.save(output, format='JPEG', quality=90)\n    image_string = output.getvalue()\n    output.close()\n    return tf.Summary.Image(height=height, width=width, colorspace=channel, encoded_image_string=image_string)\n\ndef get_nyu_callbacks(model, basemodel, train_generator, test_generator, test_set, runPath):\n    callbacks = []\n\n    # Callback: Tensorboard\n    class LRTensorBoard(keras.callbacks.TensorBoard):\n        def __init__(self, log_dir):\n            super().__init__(log_dir=log_dir)\n\n            self.num_samples = 6\n            self.train_idx = np.random.randint(low=0, high=len(train_generator), size=10)\n            self.test_idx = np.random.randint(low=0, high=len(test_generator), size=10)\n\n        def on_epoch_end(self, epoch, logs=None):            \n            if not test_set == None:\n                # Samples using current model\n                import matplotlib.pyplot as plt\n                from skimage.transform import resize\n                plasma = plt.get_cmap('plasma')\n\n                minDepth, maxDepth = 10, 1000\n\n                train_samples = []\n                test_samples = []\n\n                for i in range(self.num_samples):\n                    x_train, y_train = train_generator.__getitem__(self.train_idx[i], False)\n                    x_test, y_test = test_generator[self.test_idx[i]]\n\n                    x_train, y_train = x_train[0], np.clip(DepthNorm(y_train[0], maxDepth=1000), minDepth, maxDepth) / maxDepth \n                    x_test, y_test = x_test[0], np.clip(DepthNorm(y_test[0], maxDepth=1000), minDepth, maxDepth) / maxDepth\n\n                    h, w = y_train.shape[0], y_train.shape[1]\n\n                    rgb_train = resize(x_train, (h,w), preserve_range=True, mode='reflect', anti_aliasing=True)\n                    rgb_test = resize(x_test, (h,w), preserve_range=True, mode='reflect', anti_aliasing=True)\n\n                    gt_train = plasma(y_train[:,:,0])[:,:,:3]\n                    gt_test = plasma(y_test[:,:,0])[:,:,:3]\n\n                    predict_train = plasma(predict(model, x_train, minDepth=minDepth, maxDepth=maxDepth)[0,:,:,0])[:,:,:3]\n                    predict_test = plasma(predict(model, x_test, minDepth=minDepth, maxDepth=maxDepth)[0,:,:,0])[:,:,:3]\n\n                    train_samples.append(np.vstack([rgb_train, gt_train, predict_train]))\n                    test_samples.append(np.vstack([rgb_test, gt_test, predict_test]))\n\n                self.writer.add_summary(tf.Summary(value=[tf.Summary.Value(tag='Train', image=make_image(255 * np.hstack(train_samples)))]), epoch)\n                self.writer.add_summary(tf.Summary(value=[tf.Summary.Value(tag='Test', image=make_image(255 * np.hstack(test_samples)))]), epoch)\n                \n                # Metrics\n                e = evaluate(model, test_set['rgb'], test_set['depth'], test_set['crop'], batch_size=6, verbose=True)\n                logs.update({'rel': e[3]})\n                logs.update({'rms': e[4]})\n                logs.update({'log10': e[5]})\n\n            super().on_epoch_end(epoch, logs)\n    callbacks.append( LRTensorBoard(log_dir=runPath) )\n\n    # Callback: Learning Rate Scheduler\n    lr_schedule = keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.7, patience=5, min_lr=0.00009, min_delta=1e-2)\n    callbacks.append( lr_schedule ) # reduce learning rate when stuck\n\n    # Callback: save checkpoints\n    callbacks.append(keras.callbacks.ModelCheckpoint(runPath + '/weights.{epoch:02d}-{val_loss:.2f}.hdf5', monitor='val_loss', \n        verbose=1, save_best_only=False, save_weights_only=False, mode='min', period=5))\n\n    return callbacks"""
data.py,0,"b'import numpy as np\nfrom utils import DepthNorm\nfrom io import BytesIO\nfrom PIL import Image\nfrom zipfile import ZipFile\nfrom keras.utils import Sequence\nfrom augment import BasicPolicy\n\ndef extract_zip(input_zip):\n    input_zip=ZipFile(input_zip)\n    return {name: input_zip.read(name) for name in input_zip.namelist()}\n\ndef nyu_resize(img, resolution=480, padding=6):\n    from skimage.transform import resize\n    return resize(img, (resolution, int(resolution*4/3)), preserve_range=True, mode=\'reflect\', anti_aliasing=True )\n\ndef get_nyu_data(batch_size, nyu_data_zipfile=\'nyu_data.zip\'):\n    data = extract_zip(nyu_data_zipfile)\n\n    nyu2_train = list((row.split(\',\') for row in (data[\'data/nyu2_train.csv\']).decode(""utf-8"").split(\'\\n\') if len(row) > 0))\n    nyu2_test = list((row.split(\',\') for row in (data[\'data/nyu2_test.csv\']).decode(""utf-8"").split(\'\\n\') if len(row) > 0))\n\n    shape_rgb = (batch_size, 480, 640, 3)\n    shape_depth = (batch_size, 240, 320, 1)\n\n    # Helpful for testing...\n    if False:\n        nyu2_train = nyu2_train[:10]\n        nyu2_test = nyu2_test[:10]\n\n    return data, nyu2_train, nyu2_test, shape_rgb, shape_depth\n\ndef get_nyu_train_test_data(batch_size):\n    data, nyu2_train, nyu2_test, shape_rgb, shape_depth = get_nyu_data(batch_size)\n\n    train_generator = NYU_BasicAugmentRGBSequence(data, nyu2_train, batch_size=batch_size, shape_rgb=shape_rgb, shape_depth=shape_depth)\n    test_generator = NYU_BasicRGBSequence(data, nyu2_test, batch_size=batch_size, shape_rgb=shape_rgb, shape_depth=shape_depth)\n\n    return train_generator, test_generator\n\nclass NYU_BasicAugmentRGBSequence(Sequence):\n    def __init__(self, data, dataset, batch_size, shape_rgb, shape_depth, is_flip=False, is_addnoise=False, is_erase=False):\n        self.data = data\n        self.dataset = dataset\n        self.policy = BasicPolicy( color_change_ratio=0.50, mirror_ratio=0.50, flip_ratio=0.0 if not is_flip else 0.2, \n                                    add_noise_peak=0 if not is_addnoise else 20, erase_ratio=-1.0 if not is_erase else 0.5)\n        self.batch_size = batch_size\n        self.shape_rgb = shape_rgb\n        self.shape_depth = shape_depth\n        self.maxDepth = 1000.0\n\n        from sklearn.utils import shuffle\n        self.dataset = shuffle(self.dataset, random_state=0)\n\n        self.N = len(self.dataset)\n\n    def __len__(self):\n        return int(np.ceil(self.N / float(self.batch_size)))\n\n    def __getitem__(self, idx, is_apply_policy=True):\n        batch_x, batch_y = np.zeros( self.shape_rgb ), np.zeros( self.shape_depth )\n\n        # Augmentation of RGB images\n        for i in range(batch_x.shape[0]):\n            index = min((idx * self.batch_size) + i, self.N-1)\n\n            sample = self.dataset[index]\n\n            x = np.clip(np.asarray(Image.open( BytesIO(self.data[sample[0]]) )).reshape(480,640,3)/255,0,1)\n            y = np.clip(np.asarray(Image.open( BytesIO(self.data[sample[1]]) )).reshape(480,640,1)/255*self.maxDepth,0,self.maxDepth)\n            y = DepthNorm(y, maxDepth=self.maxDepth)\n\n            batch_x[i] = nyu_resize(x, 480)\n            batch_y[i] = nyu_resize(y, 240)\n\n            if is_apply_policy: batch_x[i], batch_y[i] = self.policy(batch_x[i], batch_y[i])\n\n            # DEBUG:\n            #self.policy.debug_img(batch_x[i], np.clip(DepthNorm(batch_y[i])/maxDepth,0,1), idx, i)\n        #exit()\n\n        return batch_x, batch_y\n\nclass NYU_BasicRGBSequence(Sequence):\n    def __init__(self, data, dataset, batch_size,shape_rgb, shape_depth):\n        self.data = data\n        self.dataset = dataset\n        self.batch_size = batch_size\n        self.N = len(self.dataset)\n        self.shape_rgb = shape_rgb\n        self.shape_depth = shape_depth\n        self.maxDepth = 1000.0\n\n    def __len__(self):\n        return int(np.ceil(self.N / float(self.batch_size)))\n\n    def __getitem__(self, idx):\n        batch_x, batch_y = np.zeros( self.shape_rgb ), np.zeros( self.shape_depth )\n        for i in range(self.batch_size):            \n            index = min((idx * self.batch_size) + i, self.N-1)\n\n            sample = self.dataset[index]\n\n            x = np.clip(np.asarray(Image.open( BytesIO(self.data[sample[0]]))).reshape(480,640,3)/255,0,1)\n            y = np.asarray(Image.open(BytesIO(self.data[sample[1]])), dtype=np.float32).reshape(480,640,1).copy().astype(float) / 10.0\n            y = DepthNorm(y, maxDepth=self.maxDepth)\n\n            batch_x[i] = nyu_resize(x, 480)\n            batch_y[i] = nyu_resize(y, 240)\n\n            # DEBUG:\n            #self.policy.debug_img(batch_x[i], np.clip(DepthNorm(batch_y[i])/maxDepth,0,1), idx, i)\n        #exit()\n\n        return batch_x, batch_y\n\n#================\n# Unreal dataset\n#================\n\nimport cv2\nfrom skimage.transform import resize\n\ndef get_unreal_data(batch_size, unreal_data_file=\'unreal_data.h5\'):\n    shape_rgb = (batch_size, 480, 640, 3)\n    shape_depth = (batch_size, 240, 320, 1)\n\n    # Open data file\n    import h5py\n    data = h5py.File(unreal_data_file, \'r\')\n\n    # Shuffle\n    from sklearn.utils import shuffle\n    keys = shuffle(list(data[\'x\'].keys()), random_state=0)\n\n    # Split some validation\n    unreal_train = keys[:len(keys)-100]\n    unreal_test = keys[len(keys)-100:]\n\n    # Helpful for testing...\n    if False:\n        unreal_train = unreal_train[:10]\n        unreal_test = unreal_test[:10]\n\n    return data, unreal_train, unreal_test, shape_rgb, shape_depth\n\ndef get_unreal_train_test_data(batch_size):\n    data, unreal_train, unreal_test, shape_rgb, shape_depth = get_unreal_data(batch_size)\n    \n    train_generator = Unreal_BasicAugmentRGBSequence(data, unreal_train, batch_size=batch_size, shape_rgb=shape_rgb, shape_depth=shape_depth)\n    test_generator = Unreal_BasicAugmentRGBSequence(data, unreal_test, batch_size=batch_size, shape_rgb=shape_rgb, shape_depth=shape_depth, is_skip_policy=True)\n\n    return train_generator, test_generator\n\nclass Unreal_BasicAugmentRGBSequence(Sequence):\n    def __init__(self, data, dataset, batch_size, shape_rgb, shape_depth, is_flip=False, is_addnoise=False, is_erase=False, is_skip_policy=False):\n        self.data = data\n        self.dataset = dataset\n        self.policy = BasicPolicy( color_change_ratio=0.50, mirror_ratio=0.50, flip_ratio=0.0 if not is_flip else 0.2, \n                                    add_noise_peak=0 if not is_addnoise else 20, erase_ratio=-1.0 if not is_erase else 0.5)\n        self.batch_size = batch_size\n        self.shape_rgb = shape_rgb\n        self.shape_depth = shape_depth\n        self.maxDepth = 1000.0\n        self.N = len(self.dataset)\n        self.is_skip_policy = is_skip_policy\n\n    def __len__(self):\n        return int(np.ceil(self.N / float(self.batch_size)))\n\n    def __getitem__(self, idx, is_apply_policy=True):\n        batch_x, batch_y = np.zeros( self.shape_rgb ), np.zeros( self.shape_depth )\n        \n        # Useful for validation\n        if self.is_skip_policy: is_apply_policy=False\n\n        # Augmentation of RGB images\n        for i in range(batch_x.shape[0]):\n            index = min((idx * self.batch_size) + i, self.N-1)\n\n            sample = self.dataset[index]\n            \n            rgb_sample = cv2.imdecode(np.asarray(self.data[\'x/{}\'.format(sample)]), 1)\n            depth_sample = self.data[\'y/{}\'.format(sample)] \n            depth_sample = resize(depth_sample, (self.shape_depth[1], self.shape_depth[2]), preserve_range=True, mode=\'reflect\', anti_aliasing=True )\n            \n            x = np.clip(rgb_sample/255, 0, 1)\n            y = np.clip(depth_sample, 10, self.maxDepth)\n            y = DepthNorm(y, maxDepth=self.maxDepth)\n\n            batch_x[i] = x\n            batch_y[i] = y\n\n            if is_apply_policy: batch_x[i], batch_y[i] = self.policy(batch_x[i], batch_y[i])\n                \n            #self.policy.debug_img(batch_x[i], np.clip(DepthNorm(batch_y[i],self.maxDepth)/self.maxDepth,0,1), index, i)\n\n        return batch_x, batch_y'"
demo.py,0,"b'import os\nimport sys\nimport glob\nimport time\nimport math\nimport argparse\nimport numpy as np\n\n# Computer Vision\nimport cv2\nfrom scipy import ndimage\nfrom skimage.transform import resize\n\n# Visualization\nimport matplotlib.pyplot as plt\nplasma = plt.get_cmap(\'plasma\')\n\n# UI and OpenGL\nfrom PySide2 import QtCore, QtGui, QtWidgets, QtOpenGL\nfrom OpenGL import GL, GLU\nfrom OpenGL.arrays import vbo\nfrom OpenGL.GL import shaders\nimport glm\n\n# Argument Parser\nparser = argparse.ArgumentParser(description=\'High Quality Monocular Depth Estimation via Transfer Learning\')\nparser.add_argument(\'--model\', default=\'nyu.h5\', type=str, help=\'Trained Keras model file.\')\nargs = parser.parse_args()\n\n# Image shapes\nheight_rgb, width_rgb = 480, 640\nheight_depth, width_depth = height_rgb // 2, width_rgb // 2\nrgb_width = width_rgb\nrgb_height = height_rgb\n\nimport tensorflow as tf\nglobal graph,model\ngraph = tf.compat.v1.get_default_graph()\n\ndef load_model():\n    # Kerasa / TensorFlow\n    os.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'5\'\n    from keras.models import load_model\n    from layers import BilinearUpSampling2D\n\n    # Custom object needed for inference and training\n    custom_objects = {\'BilinearUpSampling2D\': BilinearUpSampling2D, \'depth_loss_function\': None}\n\n    # Load model into GPU / CPU\n    return load_model(args.model, custom_objects=custom_objects, compile=False)\n\n# Function timing\nticTime = time.time()\ndef tic(): global ticTime; ticTime = time.time()\ndef toc(): print(\'{0} seconds.\'.format(time.time() - ticTime))\n\n# Conversion from Numpy to QImage and back\ndef np_to_qimage(a):\n    im = a.copy()\n    return QtGui.QImage(im.data, im.shape[1], im.shape[0], im.strides[0], QtGui.QImage.Format_RGB888).copy()\n    \ndef qimage_to_np(img):\n    img = img.convertToFormat(QtGui.QImage.Format.Format_ARGB32)\n    return np.array(img.constBits()).reshape(img.height(), img.width(), 4)\n\n# Compute edge magnitudes\ndef edges(d):\n    dx = ndimage.sobel(d, 0)  # horizontal derivative\n    dy = ndimage.sobel(d, 1)  # vertical derivative\n    return np.abs(dx) + np.abs(dy)\n\n# Main window\nclass Window(QtWidgets.QWidget):\n    updateInput = QtCore.Signal()\n\n    def __init__(self, parent=None):\n        QtWidgets.QWidget.__init__(self, parent)\n        self.model = None\n        self.capture = None\n        self.glWidget = GLWidget()\n\n        mainLayout = QtWidgets.QVBoxLayout()\n\n        # Input / output views\n        viewsLayout = QtWidgets.QGridLayout()        \n        self.inputViewer = QtWidgets.QLabel(""[Click to start]"")\n        self.inputViewer.setPixmap(QtGui.QPixmap(rgb_width,rgb_height))\n        self.outputViewer = QtWidgets.QLabel(""[Click to start]"")\n        self.outputViewer.setPixmap(QtGui.QPixmap(rgb_width//2,rgb_height//2))\n\n        imgsFrame = QtWidgets.QFrame()        \n        inputsLayout = QtWidgets.QVBoxLayout()  \n        imgsFrame.setLayout(inputsLayout)\n        inputsLayout.addWidget(self.inputViewer)\n        inputsLayout.addWidget(self.outputViewer)\n\n        viewsLayout.addWidget(imgsFrame,0,0)\n        viewsLayout.addWidget(self.glWidget,0,1)\n        viewsLayout.setColumnStretch(1, 10)\n        mainLayout.addLayout(viewsLayout)\n\n        # Load depth estimation model      \n        toolsLayout = QtWidgets.QHBoxLayout()  \n\n        self.button = QtWidgets.QPushButton(""Load model..."")\n        self.button.clicked.connect(self.loadModel)\n        toolsLayout.addWidget(self.button)\n\n        self.button5 = QtWidgets.QPushButton(""Load image"")\n        self.button5.clicked.connect(self.loadImageFile)\n        toolsLayout.addWidget(self.button5)\n\n        self.button2 = QtWidgets.QPushButton(""Webcam"")\n        self.button2.clicked.connect(self.loadCamera)\n        toolsLayout.addWidget(self.button2)\n\n        self.button3 = QtWidgets.QPushButton(""Video"")\n        self.button3.clicked.connect(self.loadVideoFile)\n        toolsLayout.addWidget(self.button3)\n\n        self.button4 = QtWidgets.QPushButton(""Pause"")\n        self.button4.clicked.connect(self.loadImage)\n        toolsLayout.addWidget(self.button4)\n\n        self.button6 = QtWidgets.QPushButton(""Refresh"")\n        self.button6.clicked.connect(self.updateCloud)\n        toolsLayout.addWidget(self.button6)\n\n        mainLayout.addLayout(toolsLayout)        \n\n        self.setLayout(mainLayout)\n        self.setWindowTitle(self.tr(""RGBD Viewer""))\n\n        # Signals\n        self.updateInput.connect(self.update_input)\n\n        # Default example\n        img = (self.glWidget.rgb * 255).astype(\'uint8\')\n        self.inputViewer.setPixmap(QtGui.QPixmap.fromImage(np_to_qimage(img)))\n        coloredDepth = (plasma(self.glWidget.depth[:,:,0])[:,:,:3] * 255).astype(\'uint8\')\n        self.outputViewer.setPixmap(QtGui.QPixmap.fromImage(np_to_qimage(coloredDepth)))\n        \n    def loadModel(self):        \n        QtGui.QGuiApplication.setOverrideCursor(QtCore.Qt.WaitCursor)\n        tic()  \n        self.model = load_model()\n        print(\'Model loaded.\')\n        toc()\n        self.updateCloud()\n        QtGui.QGuiApplication.restoreOverrideCursor()\n\n    def loadCamera(self):        \n        self.capture = cv2.VideoCapture(0)\n        self.updateInput.emit()\n\n    def loadVideoFile(self):\n        self.capture = cv2.VideoCapture(\'video.mp4\')\n        self.updateInput.emit()\n\n    def loadImage(self):\n        self.capture = None\n        img = (self.glWidget.rgb * 255).astype(\'uint8\')\n        self.inputViewer.setPixmap(QtGui.QPixmap.fromImage(np_to_qimage(img)))\n        self.updateCloud()\n\n    def loadImageFile(self):\n        self.capture = None\n        filename = QtWidgets.QFileDialog.getOpenFileName(None, \'Select image\', \'\', self.tr(\'Image files (*.jpg *.png)\'))[0]\n        img = QtGui.QImage(filename).scaledToHeight(rgb_height)\n        xstart = 0\n        if img.width() > rgb_width: xstart = (img.width() - rgb_width) // 2\n        img = img.copy(xstart, 0, xstart+rgb_width, rgb_height)\n        self.inputViewer.setPixmap(QtGui.QPixmap.fromImage(img))\n        self.updateCloud()\n\n    def update_input(self):\n        # Don\'t update anymore if no capture device is set\n        if self.capture == None: return\n\n        # Capture a frame\n        ret, frame = self.capture.read()\n\n        # Loop video playback if current stream is video file\n        if not ret:\n            self.capture.set(cv2.CAP_PROP_POS_FRAMES, 0)\n            ret, frame = self.capture.read()\n\n        # Prepare image and show in UI\n        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n        image = np_to_qimage(frame)\n        self.inputViewer.setPixmap(QtGui.QPixmap.fromImage(image))\n\n        # Update the point cloud\n        self.updateCloud()\n\n    def updateCloud(self):\n        rgb8 = qimage_to_np(self.inputViewer.pixmap().toImage())\n        self.glWidget.rgb = resize((rgb8[:,:,:3]/255)[:,:,::-1], (rgb_height, rgb_width), order=1, anti_aliasing=True)\n\n        if self.model: \n            with graph.as_default():\n                depth = (1000 / self.model.predict( np.expand_dims(self.glWidget.rgb, axis=0)  )) / 1000\n            coloredDepth = (plasma(depth[0,:,:,0])[:,:,:3] * 255).astype(\'uint8\')\n            self.outputViewer.setPixmap(QtGui.QPixmap.fromImage(np_to_qimage(coloredDepth)))\n            self.glWidget.depth = depth[0,:,:,0]\n        else:\n            self.glWidget.depth = 0.5 + np.zeros((rgb_height//2, rgb_width//2, 1))\n            \n        self.glWidget.updateRGBD()\n        self.glWidget.updateGL()\n\n        # Update to next frame if we are live\n        QtCore.QTimer.singleShot(10, self.updateInput)\n\nclass GLWidget(QtOpenGL.QGLWidget):\n    def __init__(self, parent=None):\n        QtOpenGL.QGLWidget.__init__(self, parent)\n\n        self.object = 0\n        self.xRot = 5040\n        self.yRot = 40\n        self.zRot = 0\n        self.zoomLevel = 9\n\n        self.lastPos = QtCore.QPoint()\n\n        self.trolltechGreen = QtGui.QColor.fromCmykF(0.40, 0.0, 1.0, 0.0)\n        self.trolltechPurple = QtGui.QColor.fromCmykF(0.39, 0.39, 0.0, 0.0)\n\n        # Precompute for world coordinates \n        self.xx, self.yy = self.worldCoords(width=rgb_width//2, height=rgb_height//2)\n\n        # Load test frame from disk\n        self.rgb = np.load(\'demo_rgb.npy\')\n        self.depth = resize(np.load(\'demo_depth.npy\'), (240,320))\n        self.col_vbo = None\n        self.pos_vbo = None\n        self.updateRGBD()\n\n    def xRotation(self):\n        return self.xRot\n\n    def yRotation(self):\n        return self.yRot\n\n    def zRotation(self):\n        return self.zRot\n\n    def minimumSizeHint(self):\n        return QtCore.QSize(640, 480)\n\n    def sizeHint(self):\n        return QtCore.QSize(640, 480)\n\n    def setXRotation(self, angle):\n        if angle != self.xRot:\n            self.xRot = angle\n            self.emit(QtCore.SIGNAL(""xRotationChanged(int)""), angle)\n            self.updateGL()\n\n    def setYRotation(self, angle):\n        if angle != self.yRot:\n            self.yRot = angle\n            self.emit(QtCore.SIGNAL(""yRotationChanged(int)""), angle)\n            self.updateGL()\n\n    def setZRotation(self, angle):\n        if angle != self.zRot:\n            self.zRot = angle\n            self.emit(QtCore.SIGNAL(""zRotationChanged(int)""), angle)\n            self.updateGL()\n\n    def resizeGL(self, width, height):\n        GL.glViewport(0, 0, width, height)            \n\n    def mousePressEvent(self, event):\n        self.lastPos = QtCore.QPoint(event.pos())\n\n    def mouseMoveEvent(self, event):\n        dx = -(event.x() - self.lastPos.x())\n        dy = (event.y() - self.lastPos.y())\n\n        if event.buttons() & QtCore.Qt.LeftButton:\n            self.setXRotation(self.xRot + dy)\n            self.setYRotation(self.yRot + dx)\n        elif event.buttons() & QtCore.Qt.RightButton:\n            self.setXRotation(self.xRot + dy)\n            self.setZRotation(self.zRot + dx)\n\n        self.lastPos = QtCore.QPoint(event.pos())\n\n    def wheelEvent(self, event):\n        numDegrees = event.delta() / 8\n        numSteps = numDegrees / 15\n        self.zoomLevel = self.zoomLevel + numSteps\n        event.accept()\n        self.updateGL()\n\n    def initializeGL(self):\n        self.qglClearColor(self.trolltechPurple.darker())\n        GL.glShadeModel(GL.GL_FLAT)\n        GL.glEnable(GL.GL_DEPTH_TEST)\n        GL.glEnable(GL.GL_CULL_FACE)\n\n        VERTEX_SHADER = shaders.compileShader(""""""#version 330\n        layout(location = 0) in vec3 position;\n        layout(location = 1) in vec3 color;\n        uniform mat4 mvp; out vec4 frag_color;\n        void main() {gl_Position = mvp * vec4(position, 1.0);frag_color = vec4(color, 1.0);}"""""", GL.GL_VERTEX_SHADER)\n\n        FRAGMENT_SHADER = shaders.compileShader(""""""#version 330\n        in vec4 frag_color; out vec4 out_color; \n        void main() {out_color = frag_color;}"""""", GL.GL_FRAGMENT_SHADER)\n\n        self.shaderProgram = shaders.compileProgram(VERTEX_SHADER, FRAGMENT_SHADER)\n        \n        self.UNIFORM_LOCATIONS = {\n            \'position\': GL.glGetAttribLocation( self.shaderProgram, \'position\' ),\n            \'color\': GL.glGetAttribLocation( self.shaderProgram, \'color\' ),\n            \'mvp\': GL.glGetUniformLocation( self.shaderProgram, \'mvp\' ),\n        }\n\n        shaders.glUseProgram(self.shaderProgram)\n\n    def paintGL(self):\n        GL.glClear(GL.GL_COLOR_BUFFER_BIT | GL.GL_DEPTH_BUFFER_BIT)\n        self.drawObject()\n\n    def worldCoords(self, width, height):\n        hfov_degrees, vfov_degrees = 57, 43\n        hFov = math.radians(hfov_degrees)\n        vFov = math.radians(vfov_degrees)\n        cx, cy = width/2, height/2\n        fx = width/(2*math.tan(hFov/2))\n        fy = height/(2*math.tan(vFov/2))\n        xx, yy = np.tile(range(width), height), np.repeat(range(height), width)\n        xx = (xx-cx)/fx\n        yy = (yy-cy)/fy\n        return xx, yy\n\n    def posFromDepth(self, depth):\n        length = depth.shape[0] * depth.shape[1]\n\n        depth[edges(depth) > 0.3] = 1e6  # Hide depth edges       \n        z = depth.reshape(length)\n\n        return np.dstack((self.xx*z, self.yy*z, z)).reshape((length, 3))\n\n    def createPointCloudVBOfromRGBD(self):\n        # Create position and color VBOs\n        self.pos_vbo = vbo.VBO(data=self.pos, usage=GL.GL_DYNAMIC_DRAW, target=GL.GL_ARRAY_BUFFER)\n        self.col_vbo = vbo.VBO(data=self.col, usage=GL.GL_DYNAMIC_DRAW, target=GL.GL_ARRAY_BUFFER)\n\n    def updateRGBD(self):\n        # RGBD dimensions\n        width, height = self.depth.shape[1], self.depth.shape[0]\n\n        # Reshape\n        points = self.posFromDepth(self.depth.copy())\n        colors = resize(self.rgb, (height, width)).reshape((height * width, 3))\n\n        # Flatten and convert to float32\n        self.pos = points.astype(\'float32\')\n        self.col = colors.reshape(height * width, 3).astype(\'float32\')\n\n        # Move center of scene\n        self.pos = self.pos + glm.vec3(0, -0.06, -0.3)\n\n        # Create VBOs\n        if not self.col_vbo:\n            self.createPointCloudVBOfromRGBD()\n\n    def drawObject(self):\n        # Update camera\n        model, view, proj = glm.mat4(1), glm.mat4(1), glm.perspective(45, self.width() / self.height(), 0.01, 100)        \n        center, up, eye = glm.vec3(0,-0.075,0), glm.vec3(0,-1,0), glm.vec3(0,0,-0.4 * (self.zoomLevel/10))\n        view = glm.lookAt(eye, center, up)\n        model = glm.rotate(model, self.xRot / 160.0, glm.vec3(1,0,0))\n        model = glm.rotate(model, self.yRot / 160.0, glm.vec3(0,1,0))\n        model = glm.rotate(model, self.zRot / 160.0, glm.vec3(0,0,1))\n        mvp = proj * view * model\n        GL.glUniformMatrix4fv(self.UNIFORM_LOCATIONS[\'mvp\'], 1, False, glm.value_ptr(mvp))\n\n        # Update data\n        self.pos_vbo.set_array(self.pos)\n        self.col_vbo.set_array(self.col)\n\n        # Point size\n        GL.glPointSize(4)\n\n        # Position\n        self.pos_vbo.bind()\n        GL.glEnableVertexAttribArray(0)\n        GL.glVertexAttribPointer(0, 3, GL.GL_FLOAT, GL.GL_FALSE, 0, None)\n\n        # Color\n        self.col_vbo.bind()\n        GL.glEnableVertexAttribArray(1)\n        GL.glVertexAttribPointer(1, 3, GL.GL_FLOAT, GL.GL_FALSE, 0, None)\n\n        # Draw\n        GL.glDrawArrays(GL.GL_POINTS, 0, self.pos.shape[0])\n\n        # Center debug\n        if False:\n            self.qglColor(QtGui.QColor(255,0,0))\n            GL.glPointSize(20)\n            GL.glBegin(GL.GL_POINTS)\n            GL.glVertex3d(0,0,0)\n            GL.glEnd()\n\nif __name__ == \'__main__\':\n    app = QtWidgets.QApplication(sys.argv)\n    window = Window()\n    window.show()\n    res = app.exec_()'"
evaluate.py,0,"b'import os\nimport glob\nimport time\nimport argparse\n\n# Kerasa / TensorFlow\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'5\'\nfrom keras.models import load_model\nfrom layers import BilinearUpSampling2D\nfrom loss import depth_loss_function\nfrom utils import predict, load_images, display_images, evaluate\nfrom matplotlib import pyplot as plt\n\n# Argument Parser\nparser = argparse.ArgumentParser(description=\'High Quality Monocular Depth Estimation via Transfer Learning\')\nparser.add_argument(\'--model\', default=\'nyu.h5\', type=str, help=\'Trained Keras model file.\')\nargs = parser.parse_args()\n\n# Custom object needed for inference and training\ncustom_objects = {\'BilinearUpSampling2D\': BilinearUpSampling2D, \'depth_loss_function\': depth_loss_function}\n\n# Load model into GPU / CPU\nprint(\'Loading model...\')\nmodel = load_model(args.model, custom_objects=custom_objects, compile=False)\n\n# Load test data\nprint(\'Loading test data...\', end=\'\')\nimport numpy as np\nfrom data import extract_zip\ndata = extract_zip(\'nyu_test.zip\')\nfrom io import BytesIO\nrgb = np.load(BytesIO(data[\'eigen_test_rgb.npy\']))\ndepth = np.load(BytesIO(data[\'eigen_test_depth.npy\']))\ncrop = np.load(BytesIO(data[\'eigen_test_crop.npy\']))\nprint(\'Test data loaded.\\n\')\n\nstart = time.time()\nprint(\'Testing...\')\n\ne = evaluate(model, rgb, depth, crop, batch_size=6)\n\nprint(""{:>10}, {:>10}, {:>10}, {:>10}, {:>10}, {:>10}"".format(\'a1\', \'a2\', \'a3\', \'rel\', \'rms\', \'log_10\'))\nprint(""{:10.4f}, {:10.4f}, {:10.4f}, {:10.4f}, {:10.4f}, {:10.4f}"".format(e[0],e[1],e[2],e[3],e[4],e[5]))\n\nend = time.time()\nprint(\'\\nTest time\', end-start, \'s\')\n'"
layers.py,0,"b""from keras.engine.topology import Layer, InputSpec\nimport keras.utils.conv_utils as conv_utils\nimport tensorflow as tf\nimport keras.backend as K\n\nclass BilinearUpSampling2D(Layer):\n    def __init__(self, size=(2, 2), data_format=None, **kwargs):\n        super(BilinearUpSampling2D, self).__init__(**kwargs)\n        self.data_format = K.normalize_data_format(data_format)\n        self.size = conv_utils.normalize_tuple(size, 2, 'size')\n        self.input_spec = InputSpec(ndim=4)\n\n    def compute_output_shape(self, input_shape):\n        if self.data_format == 'channels_first':\n            height = self.size[0] * input_shape[2] if input_shape[2] is not None else None\n            width = self.size[1] * input_shape[3] if input_shape[3] is not None else None\n            return (input_shape[0],\n                    input_shape[1],\n                    height,\n                    width)\n        elif self.data_format == 'channels_last':\n            height = self.size[0] * input_shape[1] if input_shape[1] is not None else None\n            width = self.size[1] * input_shape[2] if input_shape[2] is not None else None\n            return (input_shape[0],\n                    height,\n                    width,\n                    input_shape[3])\n\n    def call(self, inputs):\n        input_shape = K.shape(inputs)\n        if self.data_format == 'channels_first':\n            height = self.size[0] * input_shape[2] if input_shape[2] is not None else None\n            width = self.size[1] * input_shape[3] if input_shape[3] is not None else None\n        elif self.data_format == 'channels_last':\n            height = self.size[0] * input_shape[1] if input_shape[1] is not None else None\n            width = self.size[1] * input_shape[2] if input_shape[2] is not None else None\n        \n        return tf.image.resize_images(inputs, [height, width], method=tf.image.ResizeMethod.BILINEAR, align_corners=True)\n\n    def get_config(self):\n        config = {'size': self.size, 'data_format': self.data_format}\n        base_config = super(BilinearUpSampling2D, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n"""
loss.py,0,"b'import keras.backend as K\nimport tensorflow as tf\n\ndef depth_loss_function(y_true, y_pred, theta=0.1, maxDepthVal=1000.0/10.0):\n    \n    # Point-wise depth\n    l_depth = K.mean(K.abs(y_pred - y_true), axis=-1)\n\n    # Edges\n    dy_true, dx_true = tf.image.image_gradients(y_true)\n    dy_pred, dx_pred = tf.image.image_gradients(y_pred)\n    l_edges = K.mean(K.abs(dy_pred - dy_true) + K.abs(dx_pred - dx_true), axis=-1)\n\n    # Structural similarity (SSIM) index\n    l_ssim = K.clip((1 - tf.image.ssim(y_true, y_pred, maxDepthVal)) * 0.5, 0, 1)\n\n    # Weights\n    w1 = 1.0\n    w2 = 1.0\n    w3 = theta\n\n    return (w1 * l_ssim) + (w2 * K.mean(l_edges)) + (w3 * K.mean(l_depth))'"
model.py,0,"b""import sys\n\nfrom keras import applications\nfrom keras.models import Model, load_model\nfrom keras.layers import Input, InputLayer, Conv2D, Activation, LeakyReLU, Concatenate\nfrom layers import BilinearUpSampling2D\nfrom loss import depth_loss_function\n\ndef create_model(existing='', is_twohundred=False, is_halffeatures=True):\n        \n    if len(existing) == 0:\n        print('Loading base model (DenseNet)..')\n\n        # Encoder Layers\n        if is_twohundred:\n            base_model = applications.DenseNet201(input_shape=(None, None, 3), include_top=False)\n        else:\n            base_model = applications.DenseNet169(input_shape=(None, None, 3), include_top=False)\n\n        print('Base model loaded.')\n\n        # Starting point for decoder\n        base_model_output_shape = base_model.layers[-1].output.shape\n\n        # Layer freezing?\n        for layer in base_model.layers: layer.trainable = True\n\n        # Starting number of decoder filters\n        if is_halffeatures:\n            decode_filters = int(int(base_model_output_shape[-1])/2)\n        else:\n            decode_filters = int(base_model_output_shape[-1])\n\n        # Define upsampling layer\n        def upproject(tensor, filters, name, concat_with):\n            up_i = BilinearUpSampling2D((2, 2), name=name+'_upsampling2d')(tensor)\n            up_i = Concatenate(name=name+'_concat')([up_i, base_model.get_layer(concat_with).output]) # Skip connection\n            up_i = Conv2D(filters=filters, kernel_size=3, strides=1, padding='same', name=name+'_convA')(up_i)\n            up_i = LeakyReLU(alpha=0.2)(up_i)\n            up_i = Conv2D(filters=filters, kernel_size=3, strides=1, padding='same', name=name+'_convB')(up_i)\n            up_i = LeakyReLU(alpha=0.2)(up_i)\n            return up_i\n\n        # Decoder Layers\n        decoder = Conv2D(filters=decode_filters, kernel_size=1, padding='same', input_shape=base_model_output_shape, name='conv2')(base_model.output)\n\n        decoder = upproject(decoder, int(decode_filters/2), 'up1', concat_with='pool3_pool')\n        decoder = upproject(decoder, int(decode_filters/4), 'up2', concat_with='pool2_pool')\n        decoder = upproject(decoder, int(decode_filters/8), 'up3', concat_with='pool1')\n        decoder = upproject(decoder, int(decode_filters/16), 'up4', concat_with='conv1/relu')\n        if False: decoder = upproject(decoder, int(decode_filters/32), 'up5', concat_with='input_1')\n\n        # Extract depths (final layer)\n        conv3 = Conv2D(filters=1, kernel_size=3, strides=1, padding='same', name='conv3')(decoder)\n\n        # Create the model\n        model = Model(inputs=base_model.input, outputs=conv3)\n    else:\n        # Load model from file\n        if not existing.endswith('.h5'):\n            sys.exit('Please provide a correct model file when using [existing] argument.')\n        custom_objects = {'BilinearUpSampling2D': BilinearUpSampling2D, 'depth_loss_function': depth_loss_function}\n        model = load_model(existing, custom_objects=custom_objects)\n        print('\\nExisting model loaded.\\n')\n\n    print('Model created.')\n    \n    return model"""
test.py,0,"b""import os\nimport glob\nimport argparse\nimport matplotlib\n\n# Keras / TensorFlow\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '5'\nfrom keras.models import load_model\nfrom layers import BilinearUpSampling2D\nfrom utils import predict, load_images, display_images\nfrom matplotlib import pyplot as plt\n\n# Argument Parser\nparser = argparse.ArgumentParser(description='High Quality Monocular Depth Estimation via Transfer Learning')\nparser.add_argument('--model', default='nyu.h5', type=str, help='Trained Keras model file.')\nparser.add_argument('--input', default='examples/*.png', type=str, help='Input filename or folder.')\nargs = parser.parse_args()\n\n# Custom object needed for inference and training\ncustom_objects = {'BilinearUpSampling2D': BilinearUpSampling2D, 'depth_loss_function': None}\n\nprint('Loading model...')\n\n# Load model into GPU / CPU\nmodel = load_model(args.model, custom_objects=custom_objects, compile=False)\n\nprint('\\nModel loaded ({0}).'.format(args.model))\n\n# Input images\ninputs = load_images( glob.glob(args.input) )\nprint('\\nLoaded ({0}) images of size {1}.'.format(inputs.shape[0], inputs.shape[1:]))\n\n# Compute results\noutputs = predict(model, inputs)\n\n#matplotlib problem on ubuntu terminal fix\n#matplotlib.use('TkAgg')   \n\n# Display results\nviz = display_images(outputs.copy(), inputs.copy())\nplt.figure(figsize=(10,5))\nplt.imshow(viz)\nplt.savefig('test.png')\nplt.show()\n"""
train.py,0,"b""import os, sys, glob, time, pathlib, argparse\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '5'\n\n# Kerasa / TensorFlow\nfrom loss import depth_loss_function\nfrom utils import predict, save_images, load_test_data\nfrom model import create_model\nfrom data import get_nyu_train_test_data, get_unreal_train_test_data\nfrom callbacks import get_nyu_callbacks\n\nfrom keras.optimizers import Adam\nfrom keras.utils import multi_gpu_model\nfrom keras.utils.vis_utils import plot_model\n\n# Argument Parser\nparser = argparse.ArgumentParser(description='High Quality Monocular Depth Estimation via Transfer Learning')\nparser.add_argument('--data', default='nyu', type=str, help='Training dataset.')\nparser.add_argument('--lr', type=float, default=0.0001, help='Learning rate')\nparser.add_argument('--bs', type=int, default=4, help='Batch size')\nparser.add_argument('--epochs', type=int, default=20, help='Number of epochs')\nparser.add_argument('--gpus', type=int, default=1, help='The number of GPUs to use')\nparser.add_argument('--gpuids', type=str, default='0', help='IDs of GPUs to use')\nparser.add_argument('--mindepth', type=float, default=10.0, help='Minimum of input depths')\nparser.add_argument('--maxdepth', type=float, default=1000.0, help='Maximum of input depths')\nparser.add_argument('--name', type=str, default='densedepth_nyu', help='A name to attach to the training session')\nparser.add_argument('--checkpoint', type=str, default='', help='Start training from an existing model.')\nparser.add_argument('--full', dest='full', action='store_true', help='Full training with metrics, checkpoints, and image samples.')\n\nargs = parser.parse_args()\n\n# Inform about multi-gpu training\nif args.gpus == 1: \n    os.environ['CUDA_VISIBLE_DEVICES'] = args.gpuids\n    print('Will use GPU ' + args.gpuids)\nelse:\n    print('Will use ' + str(args.gpus) + ' GPUs.')\n\n# Create the model\nmodel = create_model( existing=args.checkpoint )\n\n# Data loaders\nif args.data == 'nyu': train_generator, test_generator = get_nyu_train_test_data( args.bs )\nif args.data == 'unreal': train_generator, test_generator = get_unreal_train_test_data( args.bs )\n\n# Training session details\nrunID = str(int(time.time())) + '-n' + str(len(train_generator)) + '-e' + str(args.epochs) + '-bs' + str(args.bs) + '-lr' + str(args.lr) + '-' + args.name\noutputPath = './models/'\nrunPath = outputPath + runID\npathlib.Path(runPath).mkdir(parents=True, exist_ok=True)\nprint('Output: ' + runPath)\n\n # (optional steps)\nif True:\n    # Keep a copy of this training script and calling arguments\n    with open(__file__, 'r') as training_script: training_script_content = training_script.read()\n    training_script_content = '#' + str(sys.argv) + '\\n' + training_script_content\n    with open(runPath+'/'+__file__, 'w') as training_script: training_script.write(training_script_content)\n\n    # Generate model plot\n    plot_model(model, to_file=runPath+'/model_plot.svg', show_shapes=True, show_layer_names=True)\n\n    # Save model summary to file\n    from contextlib import redirect_stdout\n    with open(runPath+'/model_summary.txt', 'w') as f:\n        with redirect_stdout(f): model.summary()\n\n# Multi-gpu setup:\nbasemodel = model\nif args.gpus > 1: model = multi_gpu_model(model, gpus=args.gpus)\n\n# Optimizer\noptimizer = Adam(lr=args.lr, amsgrad=True)\n\n# Compile the model\nprint('\\n\\n\\n', 'Compiling model..', runID, '\\n\\n\\tGPU ' + (str(args.gpus)+' gpus' if args.gpus > 1 else args.gpuids)\n        + '\\t\\tBatch size [ ' + str(args.bs) + ' ] ' + ' \\n\\n')\nmodel.compile(loss=depth_loss_function, optimizer=optimizer)\n\nprint('Ready for training!\\n')\n\n# Callbacks\ncallbacks = []\nif args.data == 'nyu': callbacks = get_nyu_callbacks(model, basemodel, train_generator, test_generator, load_test_data() if args.full else None , runPath)\nif args.data == 'unreal': callbacks = get_nyu_callbacks(model, basemodel, train_generator, test_generator, load_test_data() if args.full else None , runPath)\n\n# Start training\nmodel.fit_generator(train_generator, callbacks=callbacks, validation_data=test_generator, epochs=args.epochs, shuffle=True)\n\n# Save the final trained model:\nbasemodel.save(runPath + '/model.h5')\n"""
utils.py,0,"b'import numpy as np\nfrom PIL import Image\n\ndef DepthNorm(x, maxDepth):\n    return maxDepth / x\n\ndef predict(model, images, minDepth=10, maxDepth=1000, batch_size=2):\n    # Support multiple RGBs, one RGB image, even grayscale \n    if len(images.shape) < 3: images = np.stack((images,images,images), axis=2)\n    if len(images.shape) < 4: images = images.reshape((1, images.shape[0], images.shape[1], images.shape[2]))\n    # Compute predictions\n    predictions = model.predict(images, batch_size=batch_size)\n    # Put in expected range\n    return np.clip(DepthNorm(predictions, maxDepth=maxDepth), minDepth, maxDepth) / maxDepth\n\ndef scale_up(scale, images):\n    from skimage.transform import resize\n    scaled = []\n    \n    for i in range(len(images)):\n        img = images[i]\n        output_shape = (scale * img.shape[0], scale * img.shape[1])\n        scaled.append( resize(img, output_shape, order=1, preserve_range=True, mode=\'reflect\', anti_aliasing=True ) )\n\n    return np.stack(scaled)\n\ndef load_images(image_files):\n    loaded_images = []\n    for file in image_files:\n        x = np.clip(np.asarray(Image.open( file ), dtype=float) / 255, 0, 1)\n        loaded_images.append(x)\n    return np.stack(loaded_images, axis=0)\n\ndef to_multichannel(i):\n    if i.shape[2] == 3: return i\n    i = i[:,:,0]\n    return np.stack((i,i,i), axis=2)\n        \ndef display_images(outputs, inputs=None, gt=None, is_colormap=True, is_rescale=True):\n    import matplotlib.pyplot as plt\n    import skimage\n    from skimage.transform import resize\n\n    plasma = plt.get_cmap(\'plasma\')\n\n    shape = (outputs[0].shape[0], outputs[0].shape[1], 3)\n    \n    all_images = []\n\n    for i in range(outputs.shape[0]):\n        imgs = []\n        \n        if isinstance(inputs, (list, tuple, np.ndarray)):\n            x = to_multichannel(inputs[i])\n            x = resize(x, shape, preserve_range=True, mode=\'reflect\', anti_aliasing=True )\n            imgs.append(x)\n\n        if isinstance(gt, (list, tuple, np.ndarray)):\n            x = to_multichannel(gt[i])\n            x = resize(x, shape, preserve_range=True, mode=\'reflect\', anti_aliasing=True )\n            imgs.append(x)\n\n        if is_colormap:\n            rescaled = outputs[i][:,:,0]\n            if is_rescale:\n                rescaled = rescaled - np.min(rescaled)\n                rescaled = rescaled / np.max(rescaled)\n            imgs.append(plasma(rescaled)[:,:,:3])\n        else:\n            imgs.append(to_multichannel(outputs[i]))\n\n        img_set = np.hstack(imgs)\n        all_images.append(img_set)\n\n    all_images = np.stack(all_images)\n    \n    return skimage.util.montage(all_images, multichannel=True, fill=(0,0,0))\n\ndef save_images(filename, outputs, inputs=None, gt=None, is_colormap=True, is_rescale=False):\n    montage =  display_images(outputs, inputs, is_colormap, is_rescale)\n    im = Image.fromarray(np.uint8(montage*255))\n    im.save(filename)\n\ndef load_test_data(test_data_zip_file=\'nyu_test.zip\'):\n    print(\'Loading test data...\', end=\'\')\n    import numpy as np\n    from data import extract_zip\n    data = extract_zip(test_data_zip_file)\n    from io import BytesIO\n    rgb = np.load(BytesIO(data[\'eigen_test_rgb.npy\']))\n    depth = np.load(BytesIO(data[\'eigen_test_depth.npy\']))\n    crop = np.load(BytesIO(data[\'eigen_test_crop.npy\']))\n    print(\'Test data loaded.\\n\')\n    return {\'rgb\':rgb, \'depth\':depth, \'crop\':crop}\n\ndef compute_errors(gt, pred):\n    thresh = np.maximum((gt / pred), (pred / gt))\n    a1 = (thresh < 1.25   ).mean()\n    a2 = (thresh < 1.25 ** 2).mean()\n    a3 = (thresh < 1.25 ** 3).mean()\n    abs_rel = np.mean(np.abs(gt - pred) / gt)\n    rmse = (gt - pred) ** 2\n    rmse = np.sqrt(rmse.mean())\n    log_10 = (np.abs(np.log10(gt)-np.log10(pred))).mean()\n    return a1, a2, a3, abs_rel, rmse, log_10\n\ndef evaluate(model, rgb, depth, crop, batch_size=6, verbose=False):\n    N = len(rgb)\n\n    bs = batch_size\n\n    predictions = []\n    testSetDepths = []\n    \n    for i in range(N//bs):    \n        x = rgb[(i)*bs:(i+1)*bs,:,:,:]\n        \n        # Compute results\n        true_y = depth[(i)*bs:(i+1)*bs,:,:]\n        pred_y = scale_up(2, predict(model, x/255, minDepth=10, maxDepth=1000, batch_size=bs)[:,:,:,0]) * 10.0\n        \n        # Test time augmentation: mirror image estimate\n        pred_y_flip = scale_up(2, predict(model, x[...,::-1,:]/255, minDepth=10, maxDepth=1000, batch_size=bs)[:,:,:,0]) * 10.0\n\n        # Crop based on Eigen et al. crop\n        true_y = true_y[:,crop[0]:crop[1]+1, crop[2]:crop[3]+1]\n        pred_y = pred_y[:,crop[0]:crop[1]+1, crop[2]:crop[3]+1]\n        pred_y_flip = pred_y_flip[:,crop[0]:crop[1]+1, crop[2]:crop[3]+1]\n        \n        # Compute errors per image in batch\n        for j in range(len(true_y)):\n            predictions.append(   (0.5 * pred_y[j]) + (0.5 * np.fliplr(pred_y_flip[j]))   )\n            testSetDepths.append(   true_y[j]   )\n\n    predictions = np.stack(predictions, axis=0)\n    testSetDepths = np.stack(testSetDepths, axis=0)\n\n    e = compute_errors(predictions, testSetDepths)\n\n    if verbose:\n        print(""{:>10}, {:>10}, {:>10}, {:>10}, {:>10}, {:>10}"".format(\'a1\', \'a2\', \'a3\', \'rel\', \'rms\', \'log_10\'))\n        print(""{:10.4f}, {:10.4f}, {:10.4f}, {:10.4f}, {:10.4f}, {:10.4f}"".format(e[0],e[1],e[2],e[3],e[4],e[5]))\n\n    return e\n'"
PyTorch/data.py,8,"b'import pandas as pd\nimport numpy as np\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms, utils\nfrom PIL import Image\nfrom io import BytesIO\nimport random\n\ndef _is_pil_image(img):\n    return isinstance(img, Image.Image)\n\ndef _is_numpy_image(img):\n    return isinstance(img, np.ndarray) and (img.ndim in {2, 3})\n\nclass RandomHorizontalFlip(object):\n    def __call__(self, sample):\n        image, depth = sample[\'image\'], sample[\'depth\']\n\n        if not _is_pil_image(image):\n            raise TypeError(\n                \'img should be PIL Image. Got {}\'.format(type(image)))\n        if not _is_pil_image(depth):\n            raise TypeError(\n                \'img should be PIL Image. Got {}\'.format(type(depth)))\n\n        if random.random() < 0.5:\n            image = image.transpose(Image.FLIP_LEFT_RIGHT)\n            depth = depth.transpose(Image.FLIP_LEFT_RIGHT)\n\n        return {\'image\': image, \'depth\': depth}\n\nclass RandomChannelSwap(object):\n    def __init__(self, probability):\n        from itertools import permutations\n        self.probability = probability\n        self.indices = list(permutations(range(3), 3))\n\n    def __call__(self, sample):\n        image, depth = sample[\'image\'], sample[\'depth\']\n        if not _is_pil_image(image): raise TypeError(\'img should be PIL Image. Got {}\'.format(type(image)))\n        if not _is_pil_image(depth): raise TypeError(\'img should be PIL Image. Got {}\'.format(type(depth)))\n        if random.random() < self.probability:\n            image = np.asarray(image)\n            image = Image.fromarray(image[...,list(self.indices[random.randint(0, len(self.indices) - 1)])])\n        return {\'image\': image, \'depth\': depth}\n\ndef loadZipToMem(zip_file):\n    # Load zip file into memory\n    print(\'Loading dataset zip file...\', end=\'\')\n    from zipfile import ZipFile\n    input_zip = ZipFile(zip_file)\n    data = {name: input_zip.read(name) for name in input_zip.namelist()}\n    nyu2_train = list((row.split(\',\') for row in (data[\'data/nyu2_train.csv\']).decode(""utf-8"").split(\'\\n\') if len(row) > 0))\n\n    from sklearn.utils import shuffle\n    nyu2_train = shuffle(nyu2_train, random_state=0)\n\n    #if True: nyu2_train = nyu2_train[:40]\n\n    print(\'Loaded ({0}).\'.format(len(nyu2_train)))\n    return data, nyu2_train\n\nclass depthDatasetMemory(Dataset):\n    def __init__(self, data, nyu2_train, transform=None):\n        self.data, self.nyu_dataset = data, nyu2_train\n        self.transform = transform\n\n    def __getitem__(self, idx):\n        sample = self.nyu_dataset[idx]\n        image = Image.open( BytesIO(self.data[sample[0]]) )\n        depth = Image.open( BytesIO(self.data[sample[1]]) )\n        sample = {\'image\': image, \'depth\': depth}\n        if self.transform: sample = self.transform(sample)\n        return sample\n\n    def __len__(self):\n        return len(self.nyu_dataset)\n\nclass ToTensor(object):\n    def __init__(self,is_test=False):\n        self.is_test = is_test\n\n    def __call__(self, sample):\n        image, depth = sample[\'image\'], sample[\'depth\']\n        \n        image = self.to_tensor(image)\n\n        depth = depth.resize((320, 240))\n\n        if self.is_test:\n            depth = self.to_tensor(depth).float() / 1000\n        else:            \n            depth = self.to_tensor(depth).float() * 1000\n        \n        # put in expected range\n        depth = torch.clamp(depth, 10, 1000)\n\n        return {\'image\': image, \'depth\': depth}\n\n    def to_tensor(self, pic):\n        if not(_is_pil_image(pic) or _is_numpy_image(pic)):\n            raise TypeError(\n                \'pic should be PIL Image or ndarray. Got {}\'.format(type(pic)))\n\n        if isinstance(pic, np.ndarray):\n            img = torch.from_numpy(pic.transpose((2, 0, 1)))\n\n            return img.float().div(255)\n\n        # handle PIL Image\n        if pic.mode == \'I\':\n            img = torch.from_numpy(np.array(pic, np.int32, copy=False))\n        elif pic.mode == \'I;16\':\n            img = torch.from_numpy(np.array(pic, np.int16, copy=False))\n        else:\n            img = torch.ByteTensor(\n                torch.ByteStorage.from_buffer(pic.tobytes()))\n        # PIL image mode: 1, L, P, I, F, RGB, YCbCr, RGBA, CMYK\n        if pic.mode == \'YCbCr\':\n            nchannel = 3\n        elif pic.mode == \'I;16\':\n            nchannel = 1\n        else:\n            nchannel = len(pic.mode)\n        img = img.view(pic.size[1], pic.size[0], nchannel)\n\n        img = img.transpose(0, 1).transpose(0, 2).contiguous()\n        if isinstance(img, torch.ByteTensor):\n            return img.float().div(255)\n        else:\n            return img\n\ndef getNoTransform(is_test=False):\n    return transforms.Compose([\n        ToTensor(is_test=is_test)\n    ])\n\ndef getDefaultTrainTransform():\n    return transforms.Compose([\n        RandomHorizontalFlip(),\n        RandomChannelSwap(0.5),\n        ToTensor()\n    ])\n\ndef getTrainingTestingData(batch_size):\n    data, nyu2_train = loadZipToMem(\'nyu_data.zip\')\n\n    transformed_training = depthDatasetMemory(data, nyu2_train, transform=getDefaultTrainTransform())\n    transformed_testing = depthDatasetMemory(data, nyu2_train, transform=getNoTransform())\n\n    return DataLoader(transformed_training, batch_size, shuffle=True), DataLoader(transformed_testing, batch_size, shuffle=False)\n'"
PyTorch/loss.py,3,"b'import torch\nfrom math import exp\nimport torch.nn.functional as F\n\ndef gaussian(window_size, sigma):\n    gauss = torch.Tensor([exp(-(x - window_size//2)**2/float(2*sigma**2)) for x in range(window_size)])\n    return gauss/gauss.sum()\n\ndef create_window(window_size, channel=1):\n    _1D_window = gaussian(window_size, 1.5).unsqueeze(1)\n    _2D_window = _1D_window.mm(_1D_window.t()).float().unsqueeze(0).unsqueeze(0)\n    window = _2D_window.expand(channel, 1, window_size, window_size).contiguous()\n    return window\n\ndef ssim(img1, img2, val_range, window_size=11, window=None, size_average=True, full=False):\n    L = val_range\n\n    padd = 0\n    (_, channel, height, width) = img1.size()\n    if window is None:\n        real_size = min(window_size, height, width)\n        window = create_window(real_size, channel=channel).to(img1.device)\n\n    mu1 = F.conv2d(img1, window, padding=padd, groups=channel)\n    mu2 = F.conv2d(img2, window, padding=padd, groups=channel)\n\n    mu1_sq = mu1.pow(2)\n    mu2_sq = mu2.pow(2)\n    mu1_mu2 = mu1 * mu2\n\n    sigma1_sq = F.conv2d(img1 * img1, window, padding=padd, groups=channel) - mu1_sq\n    sigma2_sq = F.conv2d(img2 * img2, window, padding=padd, groups=channel) - mu2_sq\n    sigma12 = F.conv2d(img1 * img2, window, padding=padd, groups=channel) - mu1_mu2\n\n    C1 = (0.01 * L) ** 2\n    C2 = (0.03 * L) ** 2\n\n    v1 = 2.0 * sigma12 + C2\n    v2 = sigma1_sq + sigma2_sq + C2\n    cs = torch.mean(v1 / v2)  # contrast sensitivity\n\n    ssim_map = ((2 * mu1_mu2 + C1) * v1) / ((mu1_sq + mu2_sq + C1) * v2)\n\n    if size_average:\n        ret = ssim_map.mean()\n    else:\n        ret = ssim_map.mean(1).mean(1).mean(1)\n\n    if full:\n        return ret, cs\n\n    return ret'"
PyTorch/model.py,3,"b""import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass UpSample(nn.Sequential):\n    def __init__(self, skip_input, output_features):\n        super(UpSample, self).__init__()        \n        self.convA = nn.Conv2d(skip_input, output_features, kernel_size=3, stride=1, padding=1)\n        self.leakyreluA = nn.LeakyReLU(0.2)\n        self.convB = nn.Conv2d(output_features, output_features, kernel_size=3, stride=1, padding=1)\n        self.leakyreluB = nn.LeakyReLU(0.2)\n\n    def forward(self, x, concat_with):\n        up_x = F.interpolate(x, size=[concat_with.size(2), concat_with.size(3)], mode='bilinear', align_corners=True)\n        return self.leakyreluB( self.convB( self.leakyreluA(self.convA( torch.cat([up_x, concat_with], dim=1) ) ) )  )\n\nclass Decoder(nn.Module):\n    def __init__(self, num_features=2208, decoder_width = 0.5):\n        super(Decoder, self).__init__()\n        features = int(num_features * decoder_width)\n\n        self.conv2 = nn.Conv2d(num_features, features, kernel_size=1, stride=1, padding=1)\n\n        self.up1 = UpSample(skip_input=features//1 + 384, output_features=features//2)\n        self.up2 = UpSample(skip_input=features//2 + 192, output_features=features//4)\n        self.up3 = UpSample(skip_input=features//4 +  96, output_features=features//8)\n        self.up4 = UpSample(skip_input=features//8 +  96, output_features=features//16)\n\n        self.conv3 = nn.Conv2d(features//16, 1, kernel_size=3, stride=1, padding=1)\n\n    def forward(self, features):\n        x_block0, x_block1, x_block2, x_block3, x_block4 = features[3], features[4], features[6], features[8], features[11]\n        x_d0 = self.conv2(x_block4)\n        x_d1 = self.up1(x_d0, x_block3)\n        x_d2 = self.up2(x_d1, x_block2)\n        x_d3 = self.up3(x_d2, x_block1)\n        x_d4 = self.up4(x_d3, x_block0)\n        return self.conv3(x_d4)\n\nclass Encoder(nn.Module):\n    def __init__(self):\n        super(Encoder, self).__init__()       \n        import torchvision.models as models\n        self.original_model = models.densenet161( pretrained=True )\n\n    def forward(self, x):\n        features = [x]\n        for k, v in self.original_model.features._modules.items(): features.append( v(features[-1]) )\n        return features\n\nclass Model(nn.Module):\n    def __init__(self):\n        super(Model, self).__init__()\n        self.encoder = Encoder()\n        self.decoder = Decoder()\n\n    def forward(self, x):\n        return self.decoder( self.encoder(x) )\n\n"""
PyTorch/train.py,9,"b""import time\nimport argparse\nimport datetime\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.utils as utils\nimport torchvision.utils as vutils    \nfrom tensorboardX import SummaryWriter\n\nfrom model import Model\nfrom loss import ssim\nfrom data import getTrainingTestingData\nfrom utils import AverageMeter, DepthNorm, colorize\n\ndef main():\n    # Arguments\n    parser = argparse.ArgumentParser(description='High Quality Monocular Depth Estimation via Transfer Learning')\n    parser.add_argument('--epochs', default=20, type=int, help='number of total epochs to run')\n    parser.add_argument('--lr', '--learning-rate', default=0.0001, type=float, help='initial learning rate')\n    parser.add_argument('--bs', default=4, type=int, help='batch size')\n    args = parser.parse_args()\n\n    # Create model\n    model = Model().cuda()\n    print('Model created.')\n\n    # Training parameters\n    optimizer = torch.optim.Adam( model.parameters(), args.lr )\n    batch_size = args.bs\n    prefix = 'densenet_' + str(batch_size)\n\n    # Load data\n    train_loader, test_loader = getTrainingTestingData(batch_size=batch_size)\n\n    # Logging\n    writer = SummaryWriter(comment='{}-lr{}-e{}-bs{}'.format(prefix, args.lr, args.epochs, args.bs), flush_secs=30)\n\n    # Loss\n    l1_criterion = nn.L1Loss()\n\n    # Start training...\n    for epoch in range(args.epochs):\n        batch_time = AverageMeter()\n        losses = AverageMeter()\n        N = len(train_loader)\n\n        # Switch to train mode\n        model.train()\n\n        end = time.time()\n\n        for i, sample_batched in enumerate(train_loader):\n            optimizer.zero_grad()\n\n            # Prepare sample and target\n            image = torch.autograd.Variable(sample_batched['image'].cuda())\n            depth = torch.autograd.Variable(sample_batched['depth'].cuda(non_blocking=True))\n\n            # Normalize depth\n            depth_n = DepthNorm( depth )\n\n            # Predict\n            output = model(image)\n\n            # Compute the loss\n            l_depth = l1_criterion(output, depth_n)\n            l_ssim = torch.clamp((1 - ssim(output, depth_n, val_range = 1000.0 / 10.0)) * 0.5, 0, 1)\n\n            loss = (1.0 * l_ssim) + (0.1 * l_depth)\n\n            # Update step\n            losses.update(loss.data.item(), image.size(0))\n            loss.backward()\n            optimizer.step()\n\n            # Measure elapsed time\n            batch_time.update(time.time() - end)\n            end = time.time()\n            eta = str(datetime.timedelta(seconds=int(batch_time.val*(N - i))))\n        \n            # Log progress\n            niter = epoch*N+i\n            if i % 5 == 0:\n                # Print to console\n                print('Epoch: [{0}][{1}/{2}]\\t'\n                'Time {batch_time.val:.3f} ({batch_time.sum:.3f})\\t'\n                'ETA {eta}\\t'\n                'Loss {loss.val:.4f} ({loss.avg:.4f})'\n                .format(epoch, i, N, batch_time=batch_time, loss=losses, eta=eta))\n\n                # Log to tensorboard\n                writer.add_scalar('Train/Loss', losses.val, niter)\n\n            if i % 300 == 0:\n                LogProgress(model, writer, test_loader, niter)\n\n        # Record epoch's intermediate results\n        LogProgress(model, writer, test_loader, niter)\n        writer.add_scalar('Train/Loss.avg', losses.avg, epoch)\n\ndef LogProgress(model, writer, test_loader, epoch):\n    model.eval()\n    sequential = test_loader\n    sample_batched = next(iter(sequential))\n    image = torch.autograd.Variable(sample_batched['image'].cuda())\n    depth = torch.autograd.Variable(sample_batched['depth'].cuda(non_blocking=True))\n    if epoch == 0: writer.add_image('Train.1.Image', vutils.make_grid(image.data, nrow=6, normalize=True), epoch)\n    if epoch == 0: writer.add_image('Train.2.Depth', colorize(vutils.make_grid(depth.data, nrow=6, normalize=False)), epoch)\n    output = DepthNorm( model(image) )\n    writer.add_image('Train.3.Ours', colorize(vutils.make_grid(output.data, nrow=6, normalize=False)), epoch)\n    writer.add_image('Train.3.Diff', colorize(vutils.make_grid(torch.abs(output-depth).data, nrow=6, normalize=False)), epoch)\n    del image\n    del depth\n    del output\n\nif __name__ == '__main__':\n    main()\n"""
PyTorch/utils.py,0,"b""import matplotlib\nimport matplotlib.cm\nimport numpy as np\n\ndef DepthNorm(depth, maxDepth=1000.0): \n    return maxDepth / depth\n\nclass AverageMeter(object):\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count\n\ndef colorize(value, vmin=10, vmax=1000, cmap='plasma'):\n    value = value.cpu().numpy()[0,:,:]\n\n    # normalize\n    vmin = value.min() if vmin is None else vmin\n    vmax = value.max() if vmax is None else vmax\n    if vmin!=vmax:\n        value = (value - vmin) / (vmax - vmin) # vmin..vmax\n    else:\n        # Avoid 0-division\n        value = value*0.\n    # squeeze last dim if it exists\n    #value = value.squeeze(axis=0)\n\n    cmapper = matplotlib.cm.get_cmap(cmap)\n    value = cmapper(value,bytes=True) # (nxmx4)\n\n    img = value[:,:,:3]\n\n    return img.transpose((2,0,1))"""
Tensorflow/data.py,0,"b""import tensorflow as tf\n\nfrom io import BytesIO\nfrom zipfile import ZipFile\nfrom sklearn.utils import shuffle\n\nclass DataLoader():\n    def __init__(self, csv_file='data/nyu2_train.csv', DEBUG=False):\n        self.shape_rgb = (480, 640, 3)\n        self.shape_depth = (240, 320, 1)\n        self.read_nyu_data(csv_file, DEBUG=DEBUG)\n\n    def nyu_resize(self, img, resolution=480, padding=6):\n        from skimage.transform import resize\n        return resize(img, (resolution, int(resolution*4/3)), preserve_range=True, mode='reflect', anti_aliasing=True )\n\n    def read_nyu_data(self, csv_file, DEBUG=False):\n        csv = open(csv_file, 'r').read()\n        nyu2_train = list((row.split(',') for row in (csv).split('\\n') if len(row) > 0))\n\n        # Dataset shuffling happens here\n        nyu2_train = shuffle(nyu2_train, random_state=0)\n\n        # Test on a smaller dataset\n        if DEBUG: nyu2_train = nyu2_train[:10]\n        \n        # A vector of RGB filenames.\n        self.filenames = [i[0] for i in nyu2_train]\n\n        # A vector of depth filenames.\n        self.labels = [i[1] for i in nyu2_train]\n\n        # Length of dataset\n        self.length = len(self.filenames)\n\n    def _parse_function(self, filename, label): \n        # Read images from disk\n        image_decoded = tf.image.decode_jpeg(tf.io.read_file(filename))\n        depth_resized = tf.image.resize(tf.image.decode_jpeg(tf.io.read_file(label)), [self.shape_depth[0], self.shape_depth[1]])\n\n        # Format\n        rgb = tf.image.convert_image_dtype(image_decoded, dtype=tf.float32)\n        depth = tf.image.convert_image_dtype(depth_resized / 255.0, dtype=tf.float32)\n        \n        # Normalize the depth values (in cm)\n        depth = 1000 / tf.clip_by_value(depth * 1000, 10, 1000)\n\n        return rgb, depth\n\n    def get_batched_dataset(self, batch_size):\n        self.dataset = tf.data.Dataset.from_tensor_slices((self.filenames, self.labels))\n        self.dataset = self.dataset.shuffle(buffer_size=len(self.filenames), reshuffle_each_iteration=True)\n        self.dataset = self.dataset.repeat()\n        self.dataset = self.dataset.map(map_func=self._parse_function, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n        self.dataset = self.dataset.batch(batch_size=batch_size)\n\n        return self.dataset\n\n"""
Tensorflow/evaluate.py,0,"b'import numpy as np\nfrom zipfile import ZipFile\nfrom io import BytesIO\n\n# Load test data\ndef load_test_data():    \n    print(\'Loading test data...\', end=\'\')\n    def extract_zip(input_zip):\n        input_zip=ZipFile(input_zip)\n        return {name: input_zip.read(name) for name in input_zip.namelist()}\n    data = extract_zip(\'nyu_test.zip\')\n    \n    rgb = np.load(BytesIO(data[\'eigen_test_rgb.npy\']))\n    depth = np.load(BytesIO(data[\'eigen_test_depth.npy\']))\n    crop = np.load(BytesIO(data[\'eigen_test_crop.npy\']))\n    print(\'Test data loaded.\\n\')\n    \n    return rgb, depth, crop\n\ndef DepthNorm(x, maxDepth):\n    return maxDepth / x\n\ndef predict(model, images, minDepth=10, maxDepth=1000, batch_size=2):\n    # Support multiple RGBs, one RGB image, even grayscale \n    if len(images.shape) < 3: images = np.stack((images,images,images), axis=2)\n    if len(images.shape) < 4: images = images.reshape((1, images.shape[0], images.shape[1], images.shape[2]))\n    # Compute predictions\n    predictions = model.predict(images, batch_size=batch_size)\n    # Put in expected range\n    return np.clip(DepthNorm(predictions, maxDepth=1000), minDepth, maxDepth) / maxDepth\n\ndef scale_up(scale, images):\n    from skimage.transform import resize\n    scaled = []\n    \n    for i in range(len(images)):\n        img = images[i]\n        output_shape = (scale * img.shape[0], scale * img.shape[1])\n        scaled.append( resize(img, output_shape, order=1, preserve_range=True, mode=\'reflect\', anti_aliasing=True ) )\n\n    return np.stack(scaled)\n\ndef evaluate(model, rgb, depth, crop, batch_size=6):\n    def compute_errors(gt, pred):\n        thresh = np.maximum((gt / pred), (pred / gt))\n        \n        a1 = (thresh < 1.25   ).mean()\n        a2 = (thresh < 1.25 ** 2).mean()\n        a3 = (thresh < 1.25 ** 3).mean()\n\n        abs_rel = np.mean(np.abs(gt - pred) / gt)\n\n        rmse = (gt - pred) ** 2\n        rmse = np.sqrt(rmse.mean())\n\n        log_10 = (np.abs(np.log10(gt)-np.log10(pred))).mean()\n\n        return a1, a2, a3, abs_rel, rmse, log_10\n\n    depth_scores = np.zeros((6, len(rgb))) # six metrics\n\n    bs = batch_size\n\n    for i in range(len(rgb)//bs):    \n        x = rgb[(i)*bs:(i+1)*bs,:,:,:]\n        \n        # Compute results\n        true_y = depth[(i)*bs:(i+1)*bs,:,:]\n        pred_y = scale_up(2, predict(model, x/255, minDepth=10, maxDepth=1000, batch_size=bs)[:,:,:,0]) * 10.0\n        \n        # Test time augmentation: mirror image estimate\n        pred_y_flip = scale_up(2, predict(model, x[...,::-1,:]/255, minDepth=10, maxDepth=1000, batch_size=bs)[:,:,:,0]) * 10.0\n\n        # Crop based on Eigen et al. crop\n        true_y = true_y[:,crop[0]:crop[1]+1, crop[2]:crop[3]+1]\n        pred_y = pred_y[:,crop[0]:crop[1]+1, crop[2]:crop[3]+1]\n        pred_y_flip = pred_y_flip[:,crop[0]:crop[1]+1, crop[2]:crop[3]+1]\n        \n        # Compute errors per image in batch\n        for j in range(len(true_y)):\n            errors = compute_errors(true_y[j], (0.5 * pred_y[j]) + (0.5 * np.fliplr(pred_y_flip[j])))\n            \n            for k in range(len(errors)):\n                depth_scores[k][(i*bs)+j] = errors[k]\n\n    e = depth_scores.mean(axis=1)\n\n    print(""{:>10}, {:>10}, {:>10}, {:>10}, {:>10}, {:>10}"".format(\'a1\', \'a2\', \'a3\', \'rel\', \'rms\', \'log_10\'))\n    print(""{:10.4f}, {:10.4f}, {:10.4f}, {:10.4f}, {:10.4f}, {:10.4f}"".format(e[0],e[1],e[2],e[3],e[4],e[5]))'"
Tensorflow/loss.py,0,"b'import tensorflow as tf\nimport tensorflow.keras.backend as K\n\ndef depth_loss_function(y_true, y_pred, theta=0.1, maxDepthVal=1000.0/10.0):\n    \n    # Point-wise depth\n    l_depth = K.mean(K.abs(y_pred - y_true), axis=-1)\n\n    # Edges\n    dy_true, dx_true = tf.image.image_gradients(y_true)\n    dy_pred, dx_pred = tf.image.image_gradients(y_pred)\n    l_edges = K.mean(K.abs(dy_pred - dy_true) + K.abs(dx_pred - dx_true), axis=-1)\n\n    # Structural similarity (SSIM) index\n    l_ssim = K.clip((1 - tf.image.ssim(y_true, y_pred, maxDepthVal)) * 0.5, 0, 1)\n\n    # Weights\n    w1 = 1.0\n    w2 = 1.0\n    w3 = theta\n\n    return (w1 * l_ssim) + (w2 * K.mean(l_edges)) + (w3 * K.mean(l_depth))'"
Tensorflow/model.py,0,"b""from tensorflow.keras.layers import Conv2D, UpSampling2D, LeakyReLU, Concatenate\nfrom tensorflow.keras import Model\nfrom tensorflow.keras.applications import DenseNet169\n\nclass UpscaleBlock(Model):\n    def __init__(self, filters, name):      \n        super(UpscaleBlock, self).__init__()\n        self.up = UpSampling2D(size=(2, 2), interpolation='bilinear', name=name+'_upsampling2d')\n        self.concat = Concatenate(name=name+'_concat') # Skip connection        \n        self.convA = Conv2D(filters=filters, kernel_size=3, strides=1, padding='same', name=name+'_convA')\n        self.reluA = LeakyReLU(alpha=0.2)\n        self.convB = Conv2D(filters=filters, kernel_size=3, strides=1, padding='same', name=name+'_convB')\n        self.reluB = LeakyReLU(alpha=0.2)\n    \n    def call(self, x):        \n        b = self.reluB( self.convB( self.reluA( self.convA( self.concat( [self.up(x[0]), x[1]] ) ) ) ) )\n        return b \n\nclass Encoder(Model):\n    def __init__(self):\n        super(Encoder, self).__init__()                \n        self.base_model = DenseNet169(input_shape=(None, None, 3), include_top=False, weights='imagenet')   \n        print('Base model loaded {}'.format(DenseNet169.__name__))\n        \n        # Create encoder model that produce final features along with multiple intermediate features\n        outputs = [self.base_model.outputs[-1]]\n        for name in ['pool1', 'pool2_pool', 'pool3_pool', 'conv1/relu'] : outputs.append( self.base_model.get_layer(name).output )        \n        self.encoder = Model(inputs=self.base_model.inputs, outputs=outputs)\n        \n    def call(self, x):\n        return self.encoder(x)\n    \nclass Decoder(Model):\n    def __init__(self, decode_filters):\n        super(Decoder, self).__init__()        \n        self.conv2 =  Conv2D(filters=decode_filters, kernel_size=1, padding='same', name='conv2')        \n        self.up1 = UpscaleBlock(filters=decode_filters//2,  name='up1')\n        self.up2 = UpscaleBlock(filters=decode_filters//4,  name='up2')\n        self.up3 = UpscaleBlock(filters=decode_filters//8,  name='up3')\n        self.up4 = UpscaleBlock(filters=decode_filters//16, name='up4')        \n        self.conv3 = Conv2D(filters=1, kernel_size=3, strides=1, padding='same', name='conv3')       \n\n    def call(self, features):        \n        x, pool1, pool2, pool3, conv1 = features[0], features[1], features[2], features[3], features[4]\n        up0 = self.conv2(x)        \n        up1 = self.up1([up0, pool3])        \n        up2 = self.up2([up1, pool2])        \n        up3 = self.up3([up2, pool1])        \n        up4 = self.up4([up3, conv1])        \n        return self.conv3( up4 )\n    \nclass DepthEstimate(Model):\n    def __init__(self):\n        super(DepthEstimate, self).__init__()\n        self.encoder = Encoder()\n        self.decoder = Decoder( decode_filters = int(self.encoder.layers[-1].output[0].shape[-1] // 2 ) )\n        print('\\nModel created.')\n\n    def call(self, x):\n        return self.decoder( self.encoder(x) )\n"""
