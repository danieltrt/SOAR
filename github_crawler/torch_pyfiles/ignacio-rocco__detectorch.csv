file_path,api_count,code
train_fast.py,10,"b'import argparse\n\nimport torch\nfrom torch.autograd import Variable\nfrom torch.utils.data import DataLoader\n\nimport numpy as np\n\nimport sys\nsys.path.insert(0, ""lib/"")\nfrom data.coco_dataset import CocoDataset\nfrom utils.preprocess_sample import preprocess_sample\nfrom utils.collate_custom import collate_custom\nfrom utils.utils import to_cuda, to_variable, to_cuda_variable\nfrom model.detector import detector\nfrom model.loss import accuracy, smooth_L1\nfrom utils.solver import adjust_learning_rate,get_lr_at_iter\nfrom utils.training_stats import TrainingStats\nfrom torch.nn.utils.clip_grad import clip_grad_norm\nimport torch.nn as nn\nfrom utils.data_parallel import data_parallel\nfrom torch.nn.functional import cross_entropy\n\n\nparser = argparse.ArgumentParser(description=\'PyTorch Fast RCNN Training\')\n# MODEL\nparser.add_argument(\'--cnn-arch\', default=\'resnet50\')\nparser.add_argument(\'--cnn-pkl\', default=\'files/pretrained_base_cnn/R-50.pkl\')\nparser.add_argument(\'--cnn-mapping\', default=\'files/mapping_files/resnet50_mapping.npy\')\n# DATASET\n# parser.add_argument(\'--dset-path\', default=(\'datasets/data/coco/coco_train2014\',\n#           \'datasets/data/coco/coco_val2014/\'))\n# parser.add_argument(\'--dset-rois\', default=(\'files/proposal_files/coco_2014_train/rpn_proposals.pkl\',\n#                 \'files/proposal_files/coco_2014_valminusminival/rpn_proposals.pkl\'))\n# parser.add_argument(\'--dset-ann\', default=(\'datasets/data/coco/annotations/instances_train2014.json\',\n#                 \'datasets/data/coco/annotations/instances_valminusminival2014.json\'))\n# parser.add_argument(\'--dset-path\', default=(\'datasets/data/coco/coco_train2014\',\n#            ))\n# parser.add_argument(\'--dset-rois\', default=(\'files/proposal_files/coco_2014_train/rpn_proposals.pkl\',\n#                  ))\n# parser.add_argument(\'--dset-ann\', default=(\'datasets/data/coco/annotations/instances_train2014.json\',\n#                  ))\n\n# use MINIVAL for debugging as it loads fast\nparser.add_argument(\'--dset-path\', default=(\'datasets/data/coco/coco_val2014\',\n         ))\nparser.add_argument(\'--dset-rois\', default=(\'files/proposal_files/coco_2014_minival/rpn_proposals.pkl\',\n               ))\nparser.add_argument(\'--dset-ann\', default=(\'datasets/data/coco/annotations/instances_minival2014.json\',\n               ))\n# DATALOADER\n\nparser.add_argument(\'-j\', \'--workers\', default=4, type=int, metavar=\'N\',\n                    help=\'number of data loading workers (default: 0)\')\n# SOLVER\nparser.add_argument(\'--base-lr\', default=0.01, type=float)\nparser.add_argument(\'--lr-steps\', default=[0, 240000, 320000])\nparser.add_argument(\'--momentum\', default=0.9, type=float)\nparser.add_argument(\'--wd\', default=1e-4, type=float, help=\'weight decay (default: 1e-4)\')\n# TRAINING\nparser.add_argument(\'--max-iter\', default=360000, type=int)\nparser.add_argument(\'--batch-size\', default=1, type=int)\nparser.add_argument(\'--start-iter\', default=0, type=int, metavar=\'N\',\n                    help=\'manual iter number (useful on restarts)\')\nparser.add_argument(\'--resume\', default=\'\', type=str, metavar=\'PATH\',\n                    help=\'path to latest checkpoint (default: none)\')\nparser.add_argument(\'--checkpoint-period\', default=20000, type=int)\nparser.add_argument(\'--checkpoint-fn\', default=\'files/results/fast.pth.tar\')\n\n\ndef main():\n    args = parser.parse_args()\n    print(args)\n    # for now, batch_size should match number of gpus\n    assert(args.batch_size==torch.cuda.device_count())\n\n    # create model\n    model = detector(arch=args.cnn_arch,\n                 base_cnn_pkl_file=args.cnn_pkl,\n                 mapping_file=args.cnn_mapping,\n                 output_prob=False,\n                 return_rois=False,\n                 return_img_features=False)\n    model = model.cuda()\n\n    # freeze part of the net\n    stop_grad=[\'conv1\',\'bn1\',\'relu\',\'maxpool\',\'layer1\']\n    model_no_grad=torch.nn.Sequential(*[getattr(model.model,l) for l in stop_grad])\n    for param in model_no_grad.parameters():\n        param.requires_grad = False\n\n    # define  optimizer\n    optimizer = torch.optim.SGD(filter(lambda p: p.requires_grad, model.parameters()),\n                                lr=args.base_lr,\n                                momentum=args.momentum,\n                                weight_decay=args.wd)\n\n    # create dataset\n    train_dataset = CocoDataset(ann_file=args.dset_ann,\n                          img_dir=args.dset_path,\n                          proposal_file=args.dset_rois,\n                          mode=\'train\',\n                          sample_transform=preprocess_sample(target_sizes=[800],\n                                                             sample_proposals_for_training=True))\n    train_loader = DataLoader(train_dataset, batch_size=args.batch_size,shuffle=False, num_workers=args.workers, collate_fn=collate_custom)\n\n    training_stats = TrainingStats(losses=[\'loss_cls\',\'loss_bbox\'],\n                                   metrics=[\'accuracy_cls\'],\n                                   solver_max_iters=args.max_iter)\n\n    iter = args.start_iter\n\n    print(\'starting training\')\n\n    while iter<args.max_iter:\n        for i, batch in enumerate(train_loader):\n\n            if args.batch_size==1:\n                batch = to_cuda_variable(batch,volatile=False)\n            else:\n                # when using multiple GPUs convert to cuda later in data_parallel and list_to_tensor\n                batch = to_variable(batch,volatile=False)             \n                \n\n            # update lr\n            lr = get_lr_at_iter(iter)\n            adjust_learning_rate(optimizer, lr)\n\n            # start measuring time\n            training_stats.IterTic()\n\n            # forward pass            \n            if args.batch_size==1:\n                cls_score,bbox_pred=model(batch[\'image\'],batch[\'rois\'])\n                list_to_tensor = lambda x: x                \n            else:\n                cls_score,bbox_pred=data_parallel(model,(batch[\'image\'],batch[\'rois\'])) # run model distributed over gpus and concatenate outputs for all batch\n                # convert gt data from lists to concatenated tensors\n                list_to_tensor = lambda x: torch.cat(tuple([i.cuda() for i in x]),0)\n\n            cls_labels = list_to_tensor(batch[\'labels_int32\']).long()\n            bbox_targets = list_to_tensor(batch[\'bbox_targets\'])\n            bbox_inside_weights = list_to_tensor(batch[\'bbox_inside_weights\'])\n            bbox_outside_weights = list_to_tensor(batch[\'bbox_outside_weights\'])            \n            \n            # compute loss\n            loss_cls=cross_entropy(cls_score,cls_labels)\n            loss_bbox=smooth_L1(bbox_pred,bbox_targets,bbox_inside_weights,bbox_outside_weights)\n                                  \n            # compute classification accuracy (for stats reporting)\n            acc = accuracy(cls_score,cls_labels)\n\n            # get final loss\n            loss = loss_cls + loss_bbox\n\n            # update\n            optimizer.zero_grad()\n            loss.backward()\n            # Without gradient clipping I get inf\'s and NaNs. \n            # it seems that in Caffe the SGD solver performs grad clipping by default. \n            # https://github.com/BVLC/caffe/blob/master/src/caffe/solvers/sgd_solver.cpp\n            # it also seems that Matterport\'s Mask R-CNN required grad clipping as well \n            # (see README in https://github.com/matterport/Mask_RCNN)            \n            # the value max_norm=35 was taken from here https://github.com/BVLC/caffe/blob/master/src/caffe/proto/caffe.proto\n            clip_grad_norm(filter(lambda p: p.requires_grad, model.parameters()), max_norm=35, norm_type=2) \n            optimizer.step()\n\n            # stats\n            training_stats.IterToc()\n            \n            training_stats.UpdateIterStats(losses_dict={\'loss_cls\': loss_cls.data.cpu().numpy().item(),\n                                                        \'loss_bbox\': loss_bbox.data.cpu().numpy().item()},\n                                           metrics_dict={\'accuracy_cls\':acc.data.cpu().numpy().item()})\n\n            training_stats.LogIterStats(iter, lr)\n            # save checkpoint\n            if (iter+1)%args.checkpoint_period == 0:\n                save_checkpoint({\n                    \'iter\': iter,\n                    \'args\': args,\n                    \'state_dict\': model.state_dict(),\n                    \'optimizer\' : optimizer.state_dict(),\n                }, args.checkpoint_fn)\n\n            if iter == args.start_iter + 20: # training_stats.LOG_PERIOD=20\n                # Reset the iteration timer to remove outliers from the first few\n                # SGD iterations\n                training_stats.ResetIterTimer()\n\n            # allow finishing in the middle of an epoch\n            if iter>args.max_iter:\n                break\n            # advance iteration\n            iter+=1\n            #import pdb; pdb.set_trace()\n\ndef save_checkpoint(state, filename=\'checkpoint.pth.tar\'):\n    torch.save(state, filename)    \n\nif __name__ == \'__main__\':\n    main()\n'"
lib/cppcuda_cffi/__init__.py,0,b''
lib/cppcuda_cffi/bind.py,2,"b""import os\nimport torch\nfrom torch.utils.ffi import create_extension\n\n\nsources = ['src/roi_align_forward_cpu.c']\nheaders = ['src/roi_align_forward_cpu.h']\ndefines = []\nwith_cuda = False\n\nif torch.cuda.is_available():\n    print('Including CUDA code.')\n    sources += ['src/roi_align_forward_cuda.c','src/roi_align_backward_cuda.c']\n    headers += ['src/roi_align_forward_cuda.h','src/roi_align_backward_cuda.h']\n    defines += [('WITH_CUDA', None)]\n    with_cuda = True\n\nthis_file = os.path.dirname(os.path.realpath(__file__))\nprint(this_file)\nextra_objects = ['src/cpp/roi_align_cpu_loop.o',\n                 'src/cuda/roi_align_forward_cuda_kernel.cu.o',\n                 'src/cuda/roi_align_backward_cuda_kernel.cu.o']\n\nextra_objects = [os.path.join(this_file, fname) for fname in extra_objects]\n\nffi = create_extension(\n    'roialign',\n    headers=headers,\n    sources=sources,\n    define_macros=defines,\n    relative_to=__file__,\n    with_cuda=with_cuda,\n    extra_objects=extra_objects,\n    extra_compile_args=['-std=c11']\n)\n\nif __name__ == '__main__':\n    ffi.build()\n"""
lib/cppcuda_cffi/get_lib_path.py,1,"b'import os\nimport torch\n\ndef main():\n    libpath=os.path.join(os.path.dirname(torch.__file__),\'lib\',\'include\')\n    print(libpath)\n\n\nif __name__ == ""__main__"":\n    main()\n\n'"
lib/data/coco_dataset.py,3,"b""import os\nimport sys\nimport torch\nfrom torch.utils.data import Dataset\nimport numpy as np\nimport skimage.io as io\n    \nfrom data.json_dataset import JsonDataset\nfrom data.roidb import roidb_for_training\n\nclass CocoDataset(Dataset):\n\n    def __init__(self,\n                ann_file,\n                img_dir,\n                sample_transform=None,\n                proposal_file=None,\n                num_classes=81,\n                proposal_limit=1000,\n                mode='test'):\n        self.img_dir = img_dir\n        if mode=='test':\n            self.coco = JsonDataset(annotation_file=ann_file,image_directory=img_dir) ## needed for evaluation\n        #self.img_ids = sorted(list(self.coco.COCO.imgs.keys()))\n        #self.classes = self.coco.classes   \n        self.num_classes=num_classes\n        self.sample_transform = sample_transform\n        # load proposals        \n        self.proposals=None\n        if mode=='test':\n            self.roidb = self.coco.get_roidb(proposal_file=proposal_file,proposal_limit=proposal_limit)\n            #self.proposals = [entry['boxes'][entry['gt_classes'] == 0] for entry in roidb] # remove gt boxes\n        elif mode=='train':\n            print('creating roidb for training')\n            self.roidb = roidb_for_training(annotation_files=ann_file,\n                                       image_directories=img_dir,\n                                       proposal_files=proposal_file)\n            \n    def __len__(self):\n        return len(self.roidb)\n\n    def __getitem__(self, idx):\n        # get db entry\n        dbentry = self.roidb[idx]\n        # load image\n        image_fn = dbentry['image']        \n        image = io.imread(image_fn)\n        # convert grayscale to RGB\n        if len(image.shape) == 2: \n            image = np.repeat(np.expand_dims(image,2), 3, axis=2)\n        # flip if needed (in these cases proposal coords are already flipped in roidb)\n        if dbentry['flipped']:\n            image = image[:, ::-1, :]\n\n#         # get proposals\n#         proposal_coords = torch.FloatTensor([-1])\n#         if self.proposals is not None:\n#             sample['proposal_coords']=torch.FloatTensor(self.roidb[idx]['boxes'])\n        \n        # initially the sample is just composed of the loaded image and the dbentry\n        sample = {'image': image, 'dbentry': dbentry}\n        \n        # the sample transform will do the preprocessing and convert to the inputs required by the network\n        if self.sample_transform is not None:\n            sample = self.sample_transform(sample)\n\n        return sample\n        """
lib/data/json_dataset.py,0,"b'# Copyright (c) 2017-present, Facebook, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n##############################################################################\n\n\n""""""Representation of the standard COCO json dataset format.\n\nWhen working with a new dataset, we strongly suggest to convert the dataset into\nthe COCO json format and use the existing code; it is not recommended to write\ncode to support new dataset formats.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport copy\nimport pickle\n#import logging\nimport numpy as np\nimport os\nimport scipy.sparse\nimport utils.boxes as box_utils\nfrom pycocotools import mask as COCOmask\nfrom pycocotools.coco import COCO\n\nclass logging():  # overwrite logger with dummy class which prints\n    def info(self,s):\n        print(s)\n\n#logger = logging.getLogger(__name__)\nlogger = logging()\n\ncfg_TRAIN_GT_MIN_AREA = -1\n\nclass JsonDataset(object):\n    """"""A class representing a COCO json dataset.""""""\n\n    def __init__(self, annotation_file, image_directory, image_prefix=\'\',name=\'coco\'):\n        self.name = name      \n        self.COCO = COCO(annotation_file)\n        self.image_directory = image_directory\n        self.image_prefix = image_prefix\n        category_ids = self.COCO.getCatIds()\n        categories = [c[\'name\'] for c in self.COCO.loadCats(category_ids)]\n        self.category_to_id_map = dict(zip(categories, category_ids))\n        self.classes = [\'__background__\'] + categories\n        self.num_classes = len(self.classes)\n        self.json_category_id_to_contiguous_id = {\n            v: i + 1\n            for i, v in enumerate(self.COCO.getCatIds())\n        }\n        self.contiguous_category_id_to_json_id = {\n            v: k\n            for k, v in self.json_category_id_to_contiguous_id.items()\n        }\n        self._init_keypoints()\n\n    def get_roidb(\n        self,\n        gt=False,\n        proposal_file=None,\n        min_proposal_size=2,\n        proposal_limit=-1,\n        crowd_filter_thresh=0\n    ):\n        """"""Return an roidb corresponding to the json dataset. Optionally:\n           - include ground truth boxes in the roidb\n           - add proposals specified in a proposals file\n           - filter proposals based on a minimum side length\n           - filter proposals that intersect with crowd regions\n        """"""\n        assert gt is True or crowd_filter_thresh == 0, \\\n            \'Crowd filter threshold must be 0 if ground-truth annotations \' \\\n            \'are not included.\'\n        image_ids = self.COCO.getImgIds()\n        image_ids.sort()\n        roidb = copy.deepcopy(self.COCO.loadImgs(image_ids))\n        for entry in roidb:\n            self._prep_roidb_entry(entry)\n        if gt:\n            # Include ground-truth object annotations\n            # self.debug_timer.tic()\n            for entry in roidb:\n                self._add_gt_annotations(entry)\n            # logger.debug(\n            #     \'_add_gt_annotations took {:.3f}s\'.\n            #     format(self.debug_timer.toc(average=False))\n            # )\n        if proposal_file is not None:\n            # Include proposals from a file\n            # self.debug_timer.tic()\n            self._add_proposals_from_file(\n                roidb, proposal_file, min_proposal_size, proposal_limit,\n                crowd_filter_thresh\n            )\n            # logger.debug(\n            #     \'_add_proposals_from_file took {:.3f}s\'.\n            #     format(self.debug_timer.toc(average=False))\n            # )\n        _add_class_assignments(roidb)\n        return roidb\n\n    def _prep_roidb_entry(self, entry):\n        """"""Adds empty metadata fields to an roidb entry.""""""\n        # Reference back to the parent dataset\n        entry[\'dataset\'] = self\n        # Make file_name an abs path\n        im_path = os.path.join(\n            self.image_directory, self.image_prefix + entry[\'file_name\']\n        )\n        assert os.path.exists(im_path), \'Image \\\'{}\\\' not found\'.format(im_path)\n        entry[\'image\'] = im_path\n        entry[\'flipped\'] = False\n        entry[\'has_visible_keypoints\'] = False\n        # Empty placeholders\n        entry[\'boxes\'] = np.empty((0, 4), dtype=np.float32)\n        entry[\'segms\'] = []\n        entry[\'gt_classes\'] = np.empty((0), dtype=np.int32)\n        entry[\'seg_areas\'] = np.empty((0), dtype=np.float32)\n        entry[\'gt_overlaps\'] = scipy.sparse.csr_matrix(\n            np.empty((0, self.num_classes), dtype=np.float32)\n        )\n        entry[\'is_crowd\'] = np.empty((0), dtype=np.bool)\n        # \'box_to_gt_ind_map\': Shape is (#rois). Maps from each roi to the index\n        # in the list of rois that satisfy np.where(entry[\'gt_classes\'] > 0)\n        entry[\'box_to_gt_ind_map\'] = np.empty((0), dtype=np.int32)\n        if self.keypoints is not None:\n            entry[\'gt_keypoints\'] = np.empty(\n                (0, 3, self.num_keypoints), dtype=np.int32\n            )\n        # Remove unwanted fields that come from the json file (if they exist)\n        for k in [\'date_captured\', \'url\', \'license\', \'file_name\']:\n            if k in entry:\n                del entry[k]\n\n    def _add_gt_annotations(self, entry):\n        """"""Add ground truth annotation metadata to an roidb entry.""""""\n        ann_ids = self.COCO.getAnnIds(imgIds=entry[\'id\'], iscrowd=None)\n        objs = self.COCO.loadAnns(ann_ids)\n        # Sanitize bboxes -- some are invalid\n        valid_objs = []\n        valid_segms = []\n        width = entry[\'width\']\n        height = entry[\'height\']\n        for obj in objs:\n            # crowd regions are RLE encoded and stored as dicts\n            if isinstance(obj[\'segmentation\'], list):\n                # Valid polygons have >= 3 points, so require >= 6 coordinates\n                obj[\'segmentation\'] = [\n                    p for p in obj[\'segmentation\'] if len(p) >= 6\n                ]\n            if obj[\'area\'] < cfg_TRAIN_GT_MIN_AREA:\n                continue\n            if \'ignore\' in obj and obj[\'ignore\'] == 1:\n                continue\n            # Convert form (x1, y1, w, h) to (x1, y1, x2, y2)\n            x1, y1, x2, y2 = box_utils.xywh_to_xyxy(obj[\'bbox\'])\n            x1, y1, x2, y2 = box_utils.clip_xyxy_to_image(\n                x1, y1, x2, y2, height, width\n            )\n            # Require non-zero seg area and more than 1x1 box size\n            if obj[\'area\'] > 0 and x2 > x1 and y2 > y1:\n                obj[\'clean_bbox\'] = [x1, y1, x2, y2]\n                valid_objs.append(obj)\n                valid_segms.append(obj[\'segmentation\'])\n        num_valid_objs = len(valid_objs)\n\n        boxes = np.zeros((num_valid_objs, 4), dtype=entry[\'boxes\'].dtype)\n        gt_classes = np.zeros((num_valid_objs), dtype=entry[\'gt_classes\'].dtype)\n        gt_overlaps = np.zeros(\n            (num_valid_objs, self.num_classes),\n            dtype=entry[\'gt_overlaps\'].dtype\n        )\n        seg_areas = np.zeros((num_valid_objs), dtype=entry[\'seg_areas\'].dtype)\n        is_crowd = np.zeros((num_valid_objs), dtype=entry[\'is_crowd\'].dtype)\n        box_to_gt_ind_map = np.zeros(\n            (num_valid_objs), dtype=entry[\'box_to_gt_ind_map\'].dtype\n        )\n        if self.keypoints is not None:\n            gt_keypoints = np.zeros(\n                (num_valid_objs, 3, self.num_keypoints),\n                dtype=entry[\'gt_keypoints\'].dtype\n            )\n\n        im_has_visible_keypoints = False\n        for ix, obj in enumerate(valid_objs):\n            cls = self.json_category_id_to_contiguous_id[obj[\'category_id\']]\n            boxes[ix, :] = obj[\'clean_bbox\']\n            gt_classes[ix] = cls\n            seg_areas[ix] = obj[\'area\']\n            is_crowd[ix] = obj[\'iscrowd\']\n            box_to_gt_ind_map[ix] = ix\n            if self.keypoints is not None:\n                gt_keypoints[ix, :, :] = self._get_gt_keypoints(obj)\n                if np.sum(gt_keypoints[ix, 2, :]) > 0:\n                    im_has_visible_keypoints = True\n            if obj[\'iscrowd\']:\n                # Set overlap to -1 for all classes for crowd objects\n                # so they will be excluded during training\n                gt_overlaps[ix, :] = -1.0\n            else:\n                gt_overlaps[ix, cls] = 1.0\n        entry[\'boxes\'] = np.append(entry[\'boxes\'], boxes, axis=0)\n        entry[\'segms\'].extend(valid_segms)\n        # To match the original implementation:\n        # entry[\'boxes\'] = np.append(\n        #     entry[\'boxes\'], boxes.astype(np.int).astype(np.float), axis=0)\n        entry[\'gt_classes\'] = np.append(entry[\'gt_classes\'], gt_classes)\n        entry[\'seg_areas\'] = np.append(entry[\'seg_areas\'], seg_areas)\n        entry[\'gt_overlaps\'] = np.append(\n            entry[\'gt_overlaps\'].toarray(), gt_overlaps, axis=0\n        )\n        entry[\'gt_overlaps\'] = scipy.sparse.csr_matrix(entry[\'gt_overlaps\'])\n        entry[\'is_crowd\'] = np.append(entry[\'is_crowd\'], is_crowd)\n        entry[\'box_to_gt_ind_map\'] = np.append(\n            entry[\'box_to_gt_ind_map\'], box_to_gt_ind_map\n        )\n        if self.keypoints is not None:\n            entry[\'gt_keypoints\'] = np.append(\n                entry[\'gt_keypoints\'], gt_keypoints, axis=0\n            )\n            entry[\'has_visible_keypoints\'] = im_has_visible_keypoints\n\n    def _add_proposals_from_file(\n        self, roidb, proposal_file, min_proposal_size, top_k, crowd_thresh\n    ):\n        """"""Add proposals from a proposals file to an roidb.""""""\n        logger.info(\'Loading proposals from: {}\'.format(proposal_file))\n        with open(proposal_file, \'rb\') as f:\n            proposals = pickle.load(f,encoding=\'latin1\')\n        id_field = \'indexes\' if \'indexes\' in proposals else \'ids\'  # compat fix\n        _sort_proposals(proposals, id_field)\n        box_list = []\n        for i, entry in enumerate(roidb):\n            if i % 2500 == 0:\n                logger.info(\' {:d}/{:d}\'.format(i + 1, len(roidb)))\n            boxes = proposals[\'boxes\'][i]\n            # Sanity check that these boxes are for the correct image id\n            assert entry[\'id\'] == proposals[id_field][i]\n            # Remove duplicate boxes and very small boxes and then take top k\n            boxes = box_utils.clip_boxes_to_image(\n                boxes, entry[\'height\'], entry[\'width\']\n            )\n            keep = box_utils.unique_boxes(boxes)\n            boxes = boxes[keep, :]\n            keep = box_utils.filter_small_boxes(boxes, min_proposal_size)\n            boxes = boxes[keep, :]\n            if top_k > 0:\n                boxes = boxes[:top_k, :]\n            box_list.append(boxes)\n        _merge_proposal_boxes_into_roidb(roidb, box_list)\n        if crowd_thresh > 0:\n            _filter_crowd_proposals(roidb, crowd_thresh)\n\n    def _init_keypoints(self):\n        """"""Initialize COCO keypoint information.""""""\n        self.keypoints = None\n        self.keypoint_flip_map = None\n        self.keypoints_to_id_map = None\n        self.num_keypoints = 0\n        # Thus far only the \'person\' category has keypoints\n        if \'person\' in self.category_to_id_map:\n            cat_info = self.COCO.loadCats([self.category_to_id_map[\'person\']])\n        else:\n            return\n\n        # Check if the annotations contain keypoint data or not\n        if \'keypoints\' in cat_info[0]:\n            keypoints = cat_info[0][\'keypoints\']\n            self.keypoints_to_id_map = dict(\n                zip(keypoints, range(len(keypoints))))\n            self.keypoints = keypoints\n            self.num_keypoints = len(keypoints)\n            self.keypoint_flip_map = {\n                \'left_eye\': \'right_eye\',\n                \'left_ear\': \'right_ear\',\n                \'left_shoulder\': \'right_shoulder\',\n                \'left_elbow\': \'right_elbow\',\n                \'left_wrist\': \'right_wrist\',\n                \'left_hip\': \'right_hip\',\n                \'left_knee\': \'right_knee\',\n                \'left_ankle\': \'right_ankle\'}\n\n    def _get_gt_keypoints(self, obj):\n        """"""Return ground truth keypoints.""""""\n        if \'keypoints\' not in obj:\n            return None\n        kp = np.array(obj[\'keypoints\'])\n        x = kp[0::3]  # 0-indexed x coordinates\n        y = kp[1::3]  # 0-indexed y coordinates\n        # 0: not labeled; 1: labeled, not inside mask;\n        # 2: labeled and inside mask\n        v = kp[2::3]\n        num_keypoints = len(obj[\'keypoints\']) / 3\n        assert num_keypoints == self.num_keypoints\n        gt_kps = np.ones((3, self.num_keypoints), dtype=np.int32)\n        for i in range(self.num_keypoints):\n            gt_kps[0, i] = x[i]\n            gt_kps[1, i] = y[i]\n            gt_kps[2, i] = v[i]\n        return gt_kps\n\n\ndef add_proposals(roidb, rois, scales, crowd_thresh):\n    """"""Add proposal boxes (rois) to an roidb that has ground-truth annotations\n    but no proposals. If the proposals are not at the original image scale,\n    specify the scale factor that separate them in scales.\n    """"""\n    box_list = []\n    for i in range(len(roidb)):\n        inv_im_scale = 1. / scales[i]\n        idx = np.where(rois[:, 0] == i)[0]\n        box_list.append(rois[idx, 1:] * inv_im_scale)\n    _merge_proposal_boxes_into_roidb(roidb, box_list)\n    if crowd_thresh > 0:\n        _filter_crowd_proposals(roidb, crowd_thresh)\n    _add_class_assignments(roidb)\n\n\ndef _merge_proposal_boxes_into_roidb(roidb, box_list):\n    """"""Add proposal boxes to each roidb entry.""""""\n    assert len(box_list) == len(roidb)\n    for i, entry in enumerate(roidb):\n        boxes = box_list[i]\n        num_boxes = boxes.shape[0]\n        gt_overlaps = np.zeros(\n            (num_boxes, entry[\'gt_overlaps\'].shape[1]),\n            dtype=entry[\'gt_overlaps\'].dtype\n        )\n        box_to_gt_ind_map = -np.ones(\n            (num_boxes), dtype=entry[\'box_to_gt_ind_map\'].dtype\n        )\n\n        # Note: unlike in other places, here we intentionally include all gt\n        # rois, even ones marked as crowd. Boxes that overlap with crowds will\n        # be filtered out later (see: _filter_crowd_proposals).\n        gt_inds = np.where(entry[\'gt_classes\'] > 0)[0]\n        if len(gt_inds) > 0:\n            gt_boxes = entry[\'boxes\'][gt_inds, :]\n            gt_classes = entry[\'gt_classes\'][gt_inds]\n            proposal_to_gt_overlaps = box_utils.bbox_overlaps(\n                boxes.astype(dtype=np.float32, copy=False),\n                gt_boxes.astype(dtype=np.float32, copy=False)\n            )\n            # Gt box that overlaps each input box the most\n            # (ties are broken arbitrarily by class order)\n            argmaxes = proposal_to_gt_overlaps.argmax(axis=1)\n            # Amount of that overlap\n            maxes = proposal_to_gt_overlaps.max(axis=1)\n            # Those boxes with non-zero overlap with gt boxes\n            I = np.where(maxes > 0)[0]\n            # Record max overlaps with the class of the appropriate gt box\n            gt_overlaps[I, gt_classes[argmaxes[I]]] = maxes[I]\n            box_to_gt_ind_map[I] = gt_inds[argmaxes[I]]\n        entry[\'boxes\'] = np.append(\n            entry[\'boxes\'],\n            boxes.astype(entry[\'boxes\'].dtype, copy=False),\n            axis=0\n        )\n        entry[\'gt_classes\'] = np.append(\n            entry[\'gt_classes\'],\n            np.zeros((num_boxes), dtype=entry[\'gt_classes\'].dtype)\n        )\n        entry[\'seg_areas\'] = np.append(\n            entry[\'seg_areas\'],\n            np.zeros((num_boxes), dtype=entry[\'seg_areas\'].dtype)\n        )\n        entry[\'gt_overlaps\'] = np.append(\n            entry[\'gt_overlaps\'].toarray(), gt_overlaps, axis=0\n        )\n        entry[\'gt_overlaps\'] = scipy.sparse.csr_matrix(entry[\'gt_overlaps\'])\n        entry[\'is_crowd\'] = np.append(\n            entry[\'is_crowd\'],\n            np.zeros((num_boxes), dtype=entry[\'is_crowd\'].dtype)\n        )\n        entry[\'box_to_gt_ind_map\'] = np.append(\n            entry[\'box_to_gt_ind_map\'],\n            box_to_gt_ind_map.astype(\n                entry[\'box_to_gt_ind_map\'].dtype, copy=False\n            )\n        )\n\n\ndef _filter_crowd_proposals(roidb, crowd_thresh):\n    """"""Finds proposals that are inside crowd regions and marks them as\n    overlap = -1 with each ground-truth rois, which means they will be excluded\n    from training.\n    """"""\n    for entry in roidb:\n        gt_overlaps = entry[\'gt_overlaps\'].toarray()\n        crowd_inds = np.where(entry[\'is_crowd\'] == 1)[0]\n        non_gt_inds = np.where(entry[\'gt_classes\'] == 0)[0]\n        if len(crowd_inds) == 0 or len(non_gt_inds) == 0:\n            continue\n        crowd_boxes = box_utils.xyxy_to_xywh(entry[\'boxes\'][crowd_inds, :])\n        non_gt_boxes = box_utils.xyxy_to_xywh(entry[\'boxes\'][non_gt_inds, :])\n        iscrowd_flags = [int(True)] * len(crowd_inds)\n        ious = COCOmask.iou(non_gt_boxes, crowd_boxes, iscrowd_flags)\n        bad_inds = np.where(ious.max(axis=1) > crowd_thresh)[0]\n        gt_overlaps[non_gt_inds[bad_inds], :] = -1\n        entry[\'gt_overlaps\'] = scipy.sparse.csr_matrix(gt_overlaps)\n\n\ndef _add_class_assignments(roidb):\n    """"""Compute object category assignment for each box associated with each\n    roidb entry.\n    """"""\n    for entry in roidb:\n        gt_overlaps = entry[\'gt_overlaps\'].toarray()\n        # max overlap with gt over classes (columns)\n        max_overlaps = gt_overlaps.max(axis=1)\n        # gt class that had the max overlap\n        max_classes = gt_overlaps.argmax(axis=1)\n        entry[\'max_classes\'] = max_classes\n        entry[\'max_overlaps\'] = max_overlaps\n        # sanity checks\n        # if max overlap is 0, the class must be background (class 0)\n        zero_inds = np.where(max_overlaps == 0)[0]\n        assert all(max_classes[zero_inds] == 0)\n        # if max overlap > 0, the class must be a fg class (not class 0)\n        nonzero_inds = np.where(max_overlaps > 0)[0]\n        assert all(max_classes[nonzero_inds] != 0)\n\n\ndef _sort_proposals(proposals, id_field):\n    """"""Sort proposals by the specified id field.""""""\n    order = np.argsort(proposals[id_field])\n    fields_to_sort = [\'boxes\', id_field, \'scores\']\n    for k in fields_to_sort:\n        proposals[k] = [proposals[k][i] for i in order]'"
lib/data/roidb.py,0,"b'# Copyright (c) 2017-present, Facebook, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n##############################################################################\n\n""""""Functions for common roidb manipulations.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\n# from past.builtins import basestring # in python 3: pip install future\n#import logging\nimport numpy as np\n\nfrom data.json_dataset import JsonDataset\nimport utils.boxes as box_utils\n#import utils.keypoints as keypoint_utils\nimport utils.segms as segm_utils\n\n\nclass logging():  # overwrite logger with dummy class which prints\n    def info(self,s):\n        print(s)\n    def debug(self,s):\n#        print(\'debug: \'+s)\n        return\n\n\n#logger = logging.getLogger(__name__)\nlogger = logging()\n\ndef roidb_for_training(annotation_files,\n                        image_directories,\n                        proposal_files,\n                        train_crowd_filter_thresh=0.7,\n                        use_flipped=True,\n                        train_fg_thresh=0.5,\n                        train_bg_thresh_hi=0.5,\n                        train_bg_thresh_lo=0,\n                        keypoints_on=False,\n                        bbox_thresh=0.5,\n                        cls_agnostic_bbox_reg=False,\n                        bbox_reg_weights=(10.0, 10.0, 5.0, 5.0)):\n    """"""Load and concatenate roidbs for one or more datasets, along with optional\n    object proposals. The roidb entries are then prepared for use in training,\n    which involves caching certain types of metadata for each roidb entry.\n    """"""\n    def get_roidb(annotation_file, image_directory, proposal_file):\n        ds = JsonDataset(annotation_file,image_directory)\n        roidb = ds.get_roidb(\n            gt=True,\n            proposal_file=proposal_file,\n            crowd_filter_thresh=train_crowd_filter_thresh\n        )\n        if use_flipped:\n            logger.info(\'Appending horizontally-flipped training examples...\')\n            extend_with_flipped_entries(roidb, ds)\n        logger.info(\'Loaded dataset: {:s}\'.format(ds.name))\n        return roidb\n\n    if isinstance(annotation_files, str):\n        annotation_files = (annotation_files, )\n    if isinstance(image_directories, str):\n        image_directories = (image_directories, )\n    if isinstance(proposal_files, str):\n        proposal_files = (proposal_files, )\n    if len(proposal_files) == 0:\n        proposal_files = (None, ) * len(annotation_files)\n    assert len(annotation_files) == len(image_directories) and len(annotation_files) == len(proposal_files)\n\n    # if isinstance(annotation_files,(list,tuple)) and isinstance(image_directories,(list,tuple)) and isinstance(proposal_files,(list,tuple)):\n    roidbs = [get_roidb(*args) for args in zip(annotation_files, image_directories, proposal_files)]\n    roidb = roidbs[0]\n    if len(annotation_files)>1:\n        for r in roidbs[1:]:\n            roidb.extend(r)\n    # elif isinstance(annotation_files,str) and isinstance(image_directories,str) and isinstance(proposal_files,str):\n    #     roidb = get_roidb(annotation_files,image_directories,proposal_files)\n\n    roidb = filter_for_training(roidb,train_fg_thresh,train_bg_thresh_hi,train_bg_thresh_lo,keypoints_on)\n\n    logger.info(\'Computing bounding-box regression targets...\')\n    add_bbox_regression_targets(roidb,bbox_thresh,cls_agnostic_bbox_reg,bbox_reg_weights)\n    logger.info(\'done\')\n\n    _compute_and_log_stats(roidb)\n\n    return roidb\n\n\ndef extend_with_flipped_entries(roidb, dataset):\n    """"""Flip each entry in the given roidb and return a new roidb that is the\n    concatenation of the original roidb and the flipped entries.\n\n    ""Flipping"" an entry means that that image and associated metadata (e.g.,\n    ground truth boxes and object proposals) are horizontally flipped.\n    """"""\n    flipped_roidb = []\n    for entry in roidb:\n        width = entry[\'width\']\n        boxes = entry[\'boxes\'].copy()\n        oldx1 = boxes[:, 0].copy()\n        oldx2 = boxes[:, 2].copy()\n        boxes[:, 0] = width - oldx2 - 1\n        boxes[:, 2] = width - oldx1 - 1\n        assert (boxes[:, 2] >= boxes[:, 0]).all()\n        flipped_entry = {}\n        dont_copy = (\'boxes\', \'segms\', \'gt_keypoints\', \'flipped\')\n        for k, v in entry.items():\n            if k not in dont_copy:\n                flipped_entry[k] = v\n        flipped_entry[\'boxes\'] = boxes\n        flipped_entry[\'segms\'] = segm_utils.flip_segms(\n            entry[\'segms\'], entry[\'height\'], entry[\'width\']\n        )\n        # if dataset.keypoints is not None:\n        #     flipped_entry[\'gt_keypoints\'] = keypoint_utils.flip_keypoints(\n        #         dataset.keypoints, dataset.keypoint_flip_map,\n        #         entry[\'gt_keypoints\'], entry[\'width\']\n        #     )\n        flipped_entry[\'flipped\'] = True\n        flipped_roidb.append(flipped_entry)\n    roidb.extend(flipped_roidb)\n\n\ndef filter_for_training(roidb,\n                        train_fg_thresh,\n                        train_bg_thresh_hi,\n                        train_bg_thresh_lo,\n                        keypoints_on):\n    """"""Remove roidb entries that have no usable RoIs based on config settings.\n    """"""\n    def is_valid(entry):\n        # Valid images have:\n        #   (1) At least one foreground RoI OR\n        #   (2) At least one background RoI\n        overlaps = entry[\'max_overlaps\']\n        # find boxes with sufficient overlap\n        fg_inds = np.where(overlaps >= train_fg_thresh)[0]\n        # Select background RoIs as those within [BG_THRESH_LO, BG_THRESH_HI)\n        bg_inds = np.where((overlaps < train_bg_thresh_hi) &\n                           (overlaps >= train_bg_thresh_lo))[0]\n        # image is only valid if such boxes exist\n        valid = len(fg_inds) > 0 or len(bg_inds) > 0\n        if keypoints_on:\n            # If we\'re training for keypoints, exclude images with no keypoints\n            valid = valid and entry[\'has_visible_keypoints\']\n        return valid\n\n    num = len(roidb)\n    filtered_roidb = [entry for entry in roidb if is_valid(entry)]\n    num_after = len(filtered_roidb)\n    logger.info(\'Filtered {} roidb entries: {} -> {}\'.\n                format(num - num_after, num, num_after))\n    return filtered_roidb\n\n\ndef add_bbox_regression_targets(roidb,bbox_thresh,cls_agnostic_bbox_reg,bbox_reg_weights):\n    """"""Add information needed to train bounding-box regressors.""""""\n    for entry in roidb:\n        entry[\'bbox_targets\'] = _compute_targets(entry,bbox_thresh,cls_agnostic_bbox_reg,bbox_reg_weights)\n\n\ndef _compute_targets(entry,bbox_thresh,cls_agnostic_bbox_reg,bbox_reg_weights):\n    """"""Compute bounding-box regression targets for an image.""""""\n    # Indices of ground-truth ROIs\n    rois = entry[\'boxes\']\n    overlaps = entry[\'max_overlaps\']\n    labels = entry[\'max_classes\']\n    gt_inds = np.where((entry[\'gt_classes\'] > 0) & (entry[\'is_crowd\'] == 0))[0]\n    # Targets has format (class, tx, ty, tw, th)\n    targets = np.zeros((rois.shape[0], 5), dtype=np.float32)\n    if len(gt_inds) == 0:\n        # Bail if the image has no ground-truth ROIs\n        return targets\n\n    # Indices of examples for which we try to make predictions\n    ex_inds = np.where(overlaps >= bbox_thresh)[0]\n\n    # Get IoU overlap between each ex ROI and gt ROI\n    ex_gt_overlaps = box_utils.bbox_overlaps(\n        rois[ex_inds, :].astype(dtype=np.float32, copy=False),\n        rois[gt_inds, :].astype(dtype=np.float32, copy=False))\n\n    # Find which gt ROI each ex ROI has max overlap with:\n    # this will be the ex ROI\'s gt target\n    gt_assignment = ex_gt_overlaps.argmax(axis=1)\n    gt_rois = rois[gt_inds[gt_assignment], :]\n    ex_rois = rois[ex_inds, :]\n    # Use class ""1"" for all boxes if using class_agnostic_bbox_reg\n    targets[ex_inds, 0] = (\n        1 if cls_agnostic_bbox_reg else labels[ex_inds])\n    targets[ex_inds, 1:] = box_utils.bbox_transform_inv(ex_rois, gt_rois, bbox_reg_weights)\n    return targets\n\n\ndef _compute_and_log_stats(roidb):\n    classes = roidb[0][\'dataset\'].classes\n    char_len = np.max([len(c) for c in classes])\n    hist_bins = np.arange(len(classes) + 1)\n\n    # Histogram of ground-truth objects\n    gt_hist = np.zeros((len(classes)), dtype=np.int)\n    for entry in roidb:\n        gt_inds = np.where(\n            (entry[\'gt_classes\'] > 0) & (entry[\'is_crowd\'] == 0))[0]\n        gt_classes = entry[\'gt_classes\'][gt_inds]\n        gt_hist += np.histogram(gt_classes, bins=hist_bins)[0]\n    logger.debug(\'Ground-truth class histogram:\')\n    for i, v in enumerate(gt_hist):\n        logger.debug(\n            \'{:d}{:s}: {:d}\'.format(\n                i, classes[i].rjust(char_len), v))\n    logger.debug(\'-\' * char_len)\n    logger.debug(\n        \'{:s}: {:d}\'.format(\n            \'total\'.rjust(char_len), np.sum(gt_hist)))\n'"
lib/model/collect_and_distribute_fpn_rpn_proposals.py,5,"b'# Copyright (c) 2017-present, Facebook, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n##############################################################################\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport torch\nfrom torch.autograd import Variable\n\nimport numpy as np\nfrom utils.multilevel_rois import map_rois_to_fpn_levels\n\nfrom math import log2\n# from core.config import cfg\n# from datasets import json_dataset\n# import modeling.FPN as fpn\n# import roi_data.fast_rcnn\n# import utils.blob as blob_utils\n\n\nclass CollectAndDistributeFpnRpnProposals(torch.nn.Module):\n    def __init__(self, spatial_scales, train=False):\n        super(CollectAndDistributeFpnRpnProposals, self).__init__()\n        self._train = train\n        self.rpn_levels = [int(log2(1/s)) for s in spatial_scales]\n        self.rpn_min_level = self.rpn_levels[0]\n        self.rpn_max_level = self.rpn_levels[-1]\n\n    def forward(self, roi_list, roi_score_list):\n        """"""See modeling.detector.CollectAndDistributeFpnRpnProposals for\n        inputs/outputs documentation.\n        """"""\n        # inputs is\n        # [rpn_rois_fpn2, ..., rpn_rois_fpn6,\n        #  rpn_roi_probs_fpn2, ..., rpn_roi_probs_fpn6]\n        # If training with Faster R-CNN, then inputs will additionally include\n        #  + [roidb, im_info]\n        rois = collect(roi_list, roi_score_list, self._train)\n\n        #           ************** WARNING ***************\n        #      TRAINING CODE BELOW NOT CONVERTED TO PYTORCH\n        #           ************** WARNING ***************\n\n        # if self._train:\n        #     # During training we reuse the data loader code. We populate roidb\n        #     # entries on the fly using the rois generated by RPN.\n        #     # im_info: [[im_height, im_width, im_scale], ...]\n        #     im_info = inputs[-1].data\n        #     im_scales = im_info[:, 2]\n        #     roidb = blob_utils.deserialize(inputs[-2].data)\n        #     # For historical consistency with the original Faster R-CNN\n        #     # implementation we are *not* filtering crowd proposals.\n        #     # This choice should be investigated in the future (it likely does\n        #     # not matter).\n        #     json_dataset.add_proposals(roidb, rois, im_scales, crowd_thresh=0)\n        #     # Compute training labels for the RPN proposals; also handles\n        #     # distributing the proposals over FPN levels\n        #     output_blob_names = roi_data.fast_rcnn.get_fast_rcnn_blob_names()\n        #     blobs = {k: [] for k in output_blob_names}\n        #     roi_data.fast_rcnn.add_fast_rcnn_blobs(blobs, im_scales, roidb)\n        #     for i, k in enumerate(output_blob_names):\n        #         blob_utils.py_op_copy_blob(blobs[k], outputs[i])\n        # else:\n        #     # For inference we have a special code path that avoids some data\n        #     # loader overhead\n        #     distribute(rois, None, outputs, self._train)\n        return distribute(rois, self.rpn_min_level, self.rpn_max_level) #, None, outputs, self._train)\n\n\ndef collect(roi_inputs, score_inputs, train):\n    #cfg_key = \'TRAIN\' if is_training else \'TEST\'\n    post_nms_topN = 2000 if train else 1000 # cfg[cfg_key].RPN_POST_NMS_TOP_N\n    # k_max = 6 #cfg.FPN.RPN_MAX_LEVEL\n    # k_min = 2 #cfg.FPN.RPN_MIN_LEVEL\n    # num_lvls = k_max - k_min + 1\n    # roi_inputs = inputs[:num_lvls]\n    # score_inputs = inputs[num_lvls:]\n    # if is_training:\n    #     score_inputs = score_inputs[:-2]\n\n    # rois are in [[batch_idx, x0, y0, x1, y2], ...] format\n    # Combine predictions across all levels and retain the top scoring\n    #rois = np.concatenate([blob.data for blob in roi_inputs])\n    rois = torch.cat(tuple(roi_inputs),0)\n    #scores = np.concatenate([blob.data for blob in score_inputs]).squeeze()\n    scores = torch.cat(tuple(score_inputs),0).squeeze()\n    #inds = np.argsort(-scores)[:post_nms_topN]\n    vals, inds = torch.sort(-scores)\n    #rois = rois[inds, :]\n    rois = rois[inds[:post_nms_topN], :]\n    return rois\n\n\ndef distribute(rois, lvl_min, lvl_max): #, label_blobs, outputs, train):\n    """"""To understand the output blob order see return value of\n    roi_data.fast_rcnn.get_fast_rcnn_blob_names(is_training=False)\n    """"""\n    # lvl_min = 2 #cfg.FPN.ROI_MIN_LEVEL\n    # lvl_max = 5 #cfg.FPN.ROI_MAX_LEVEL\n    lvls = map_rois_to_fpn_levels(rois.data.cpu().numpy(), lvl_min, lvl_max)\n\n    # outputs[0].reshape(rois.shape)\n    # outputs[0].data[...] = rois\n\n    # Create new roi blobs for each FPN level\n    # (See: modeling.FPN.add_multilevel_roi_blobs which is similar but annoying\n    # to generalize to support this particular case.)\n    rois_idx_order = np.empty((0, ))\n    distr_rois=[]\n    for output_idx, lvl in enumerate(range(lvl_min, lvl_max + 1)):\n        idx_lvl = np.where(lvls == lvl)[0]\n        distr_rois.append(rois[idx_lvl, :])\n        rois_idx_order = np.concatenate((rois_idx_order, idx_lvl))\n    rois_idx_restore = np.argsort(rois_idx_order)\n    return distr_rois, rois_idx_restore'"
lib/model/detector.py,76,"b'import torch\nimport pickle\nimport numpy as np\nimport torchvision.models as models\nfrom model.roi_align import RoIAlignFunction, preprocess_rois\nfrom model.generate_proposals import GenerateProposals\nfrom model.collect_and_distribute_fpn_rpn_proposals import CollectAndDistributeFpnRpnProposals\nfrom utils.utils import parse_th_to_caffe2\nfrom torch.autograd import Variable\nfrom math import log2\n\nclass fpn_body(torch.nn.Module):\n    def __init__(self, conv_body, conv_body_layers, fpn_layers):\n        super(fpn_body, self).__init__()\n        self.conv_body = conv_body\n        # Lateral convolution layers\n        self.fpn_lateral = torch.nn.ModuleList([torch.nn.Conv2d(in_channels=conv_body[conv_body_layers.index(l)][-1].bn3.num_features,\n                                                                 out_channels=256,\n                                                                 kernel_size=1,\n                                                                 stride=1,\n                                                                 padding=0) for l in fpn_layers])\n        # Output convolution layers\n        self.fpn_output = torch.nn.ModuleList([torch.nn.Conv2d(in_channels=256,\n                                                                 out_channels=256,\n                                                                 kernel_size=3,\n                                                                 stride=1,\n                                                                 padding=1) for l in fpn_layers])\n        # upsampling layer\n        self.upsample = torch.nn.Upsample(scale_factor=2, mode=\'nearest\')\n        # store fpn layer indices \n        self.fpn_indices = [conv_body_layers.index(l) for l in fpn_layers]\n        # keep fpn layer names\n        self.fpn_layers = fpn_layers\n\n    def forward(self, x):\n        lateral=[]\n        # do forward pass on the whole conv body, and store tensors for lateral computation\n        for i in range(len(self.conv_body)):\n            x=self.conv_body[i](x)\n            if i in self.fpn_indices:\n                lateral.append(x)\n        # do lateral convolutions\n        for i in range(len(self.fpn_lateral)):\n            lateral[i]=self.fpn_lateral[i](lateral[i])\n        # do top-down pass\n        for i in range(len(self.fpn_lateral)-2, -1, -1): # loop is done backwards, updating from last-1 to first lateral tensors\n            lateral[i]=self.upsample(lateral[i+1])+lateral[i]\n        # do output convolutions\n        for i in range(len(self.fpn_lateral)):\n            lateral[i]=self.fpn_output[i](lateral[i])\n\n        return lateral\n\nclass two_layer_mlp_head(torch.nn.Module):\n    def __init__(self):\n        super(two_layer_mlp_head, self).__init__()\n        self.relu = torch.nn.ReLU(inplace=True)\n        self.fc6 = torch.nn.Linear(256*7*7,1024)\n        self.fc7 = torch.nn.Linear(1024,1024)\n\n    def forward(self, x):\n        x = x.view(x.size(0),-1) \n        x = self.relu(self.fc6(x))\n        x = self.relu(self.fc7(x))\n        return x\n\nclass four_layer_conv(torch.nn.Module):\n    def __init__(self):\n        super(four_layer_conv, self).__init__()\n        self.relu = torch.nn.ReLU(inplace=True)\n        self.fcn1 = torch.nn.Conv2d(256,256,3,stride=1,padding=1)\n        self.fcn2 = torch.nn.Conv2d(256,256,3,stride=1,padding=1)\n        self.fcn3 = torch.nn.Conv2d(256,256,3,stride=1,padding=1)\n        self.fcn4 = torch.nn.Conv2d(256,256,3,stride=1,padding=1)\n      \n\n    def forward(self, x):\n        x = self.relu(self.fcn1(x))\n        x = self.relu(self.fcn2(x))\n        x = self.relu(self.fcn3(x))\n        x = self.relu(self.fcn4(x))\n        return x\n\nclass mask_head(torch.nn.Module):\n    def __init__(self, conv_head, roi_spatial_scale, roi_sampling_ratio, output_prob):\n        super(mask_head, self).__init__()\n        self.output_prob = output_prob\n        self.conv_head = conv_head\n        self.transposed_conv = torch.nn.ConvTranspose2d(256 if isinstance(conv_head,four_layer_conv) else 2048,256,2,stride=2,padding=0)\n        self.classif_logits = torch.nn.Conv2d(256,81,1,stride=1,padding=0)\n        self.relu = torch.nn.ReLU(inplace=True)\n        self.sigmoid=torch.nn.Sigmoid()\n        self.use_fpn = isinstance(roi_spatial_scale,list)\n        self.roi_spatial_scale = roi_spatial_scale\n        self.roi_sampling_ratio = roi_sampling_ratio\n        self.roi_height = 14\n        self.roi_width = 14\n\n    def forward(self, x, rois, roi_original_idx=None):\n        if self.use_fpn==False:\n            x = RoIAlignFunction.apply(x, preprocess_rois(rois), self.roi_height, self.roi_width, self.roi_spatial_scale, self.roi_sampling_ratio) # 14x14 feature per proposal\n        else:\n            x = [RoIAlignFunction.apply(x[i], preprocess_rois(rois[i]), self.roi_height, self.roi_width, \n                                        self.roi_spatial_scale[i], self.roi_sampling_ratio) if rois[i] is not None else None for i in range(len(rois))]\n            x = torch.cat(tuple(filter(lambda z: z is not None, x)),0)\n            x = x[roi_original_idx,:]\n        x = self.conv_head(x)\n        x = self.relu(self.transposed_conv(x))\n        x = self.classif_logits(x)\n        if self.output_prob:\n            x = self.sigmoid(x)\n        return x\n\nclass rpn_head(torch.nn.Module):\n    def __init__(self,in_channels=1024,out_channels=1024,n_anchors=15):\n        super(rpn_head, self).__init__()\n        self.relu = torch.nn.ReLU(inplace=True)\n        self.sigmoid = torch.nn.Sigmoid()\n        self.conv_rpn = torch.nn.Conv2d(in_channels,out_channels,3,stride=1,padding=1)\n        self.rpn_cls_prob = torch.nn.Conv2d(out_channels,n_anchors,1,stride=1,padding=0)\n        self.rpn_bbox_pred = torch.nn.Conv2d(out_channels,4*n_anchors,1,stride=1,padding=0)\n\n    def forward(self, x):\n        conv_rpn = self.relu(self.conv_rpn(x))\n        rpn_cls_prob = self.sigmoid(self.rpn_cls_prob(conv_rpn))\n        rpn_bbox_pred = self.rpn_bbox_pred(conv_rpn)\n        return rpn_cls_prob,rpn_bbox_pred\n\nclass detector(torch.nn.Module):\n    def __init__(self,\n                 train=False,\n                 arch=\'resnet50\',\n                 conv_body_layers=[\'conv1\',\'bn1\',\'relu\',\'maxpool\',\'layer1\',\'layer2\',\'layer3\'],\n                 # conv head can be a list of modules to use from the main model \n                 # or can be the string \'two_layer_mlp\'\n                 conv_head_layers=[\'layer4\',\'avgpool\'], \n                 # fpn layers is a list of the layers of the conv body used to define the levels of the FPN\n                 fpn_layers = [],\n                 fpn_extra_lvl = True, # add additional fpn lvl by 2x subsampling the last level\n                 use_rpn_head = False,\n                 use_mask_head = False,\n                 mask_head_type = \'upshare\',\n                 roi_feature_channels = 2048,\n                 N_classes = 81,\n                 detector_pkl_file = None,\n                 base_cnn_pkl_file = None,\n                 output_prob = True,\n                 roi_height = 14,\n                 roi_width = 14,\n                 roi_spatial_scale = 0.0625,\n                 roi_sampling_ratio = 0):\n        super(detector, self).__init__() \n        # RoI Parameters\n        self.roi_height = int(roi_height)\n        self.roi_width = int(roi_width)\n        self.roi_spatial_scale = [float(i) for i in roi_spatial_scale] if isinstance(roi_spatial_scale, list) else float(roi_spatial_scale) \n        self.roi_sampling_ratio = int(roi_sampling_ratio)\n        # Flags        \n        self.train=train\n        self.mask_head_type = mask_head_type\n        self.use_fpn_body = len(fpn_layers)>0\n        self.fpn_extra_lvl = fpn_extra_lvl\n        self.use_rpn_head = use_rpn_head\n        self.use_mask_head = use_mask_head\n        self.use_two_layer_mlp_head = conv_head_layers==\'two_layer_mlp\'\n        self.output_prob=output_prob        \n\n        ## Create main conv model\n        if arch.startswith(\'resnet\'):\n            self.model = eval(\'models.\'+arch+\'()\') # construct ResNet model (maybe not very safe :) \n\n            # swap stride (2,2) and (1,1) in first layers (PyTorch ResNet is slightly different to caffe2 ResNet)\n            # this is required for compatibility with caffe2 models\n            self.model.layer2[0].conv1.stride=(2,2)\n            self.model.layer2[0].conv2.stride=(1,1)\n            self.model.layer3[0].conv1.stride=(2,2)\n            self.model.layer3[0].conv2.stride=(1,1)\n            self.model.layer4[0].conv1.stride=(2,2)\n            self.model.layer4[0].conv2.stride=(1,1)\n        else:\n            raise(\'Only resnet implemented so far!\')\n        # divide model into conv_body and conv_head        \n        self.conv_body=torch.nn.Sequential(*[getattr(self.model,l) for l in conv_body_layers])\n        # wrap in FPN model if needed\n        if self.use_fpn_body:\n            self.conv_body = fpn_body(self.conv_body, conv_body_layers, fpn_layers)\n        # create conv head\n        if conv_head_layers==\'two_layer_mlp\':\n            self.conv_head=two_layer_mlp_head()\n        else:\n            self.conv_head=torch.nn.Sequential(*[getattr(self.model,l) for l in conv_head_layers])        \n\n        ## Create heads\n        # RPN head\n        if self.use_rpn_head and not self.use_fpn_body:\n            self.rpn = rpn_head()\n            self.proposal_generator = GenerateProposals(train=self.train)\n        if self.use_rpn_head and self.use_fpn_body:\n            self.rpn = rpn_head(in_channels=256,out_channels=256,n_anchors=3)\n            spatial_scales = self.roi_spatial_scale\n            if self.fpn_extra_lvl:\n                spatial_scales = spatial_scales + [spatial_scales[-1]/2.]\n            self.proposal_generator = torch.nn.ModuleList([GenerateProposals(train=self.train,\n                                                         spatial_scale=spatial_scales[i],\n                                                         anchor_sizes=(32*2**i,),\n                                                         rpn_pre_nms_top_n=12000 if self.train else 1000,\n                                                         rpn_post_nms_top_n=2000 if self.train else 1000) for i in range(len(spatial_scales))])\n            # Note, even when using the extra fpn level, proposals are note collected at this level\n            self.collect_and_distr_rois = CollectAndDistributeFpnRpnProposals(spatial_scales=self.roi_spatial_scale,train=self.train)\n\n        # bounding box regression head\n        self.bbox_head=torch.nn.Linear(roi_feature_channels,4*N_classes)\n        # classification head\n        self.classif_head=torch.nn.Linear(roi_feature_channels,N_classes)                    \n        # mask prediction head\n        if self.use_mask_head:\n            if self.mask_head_type == \'upshare\':\n                mask_head_conv = self.conv_head[0]\n            elif self.mask_head_type == \'1up4convs\':\n                mask_head_conv = four_layer_conv()\n            self.mask_head = mask_head(mask_head_conv,\n                                       self.roi_spatial_scale,\n                                       self.roi_sampling_ratio, output_prob)\n        # load pretrained weights\n        if detector_pkl_file is not None:\n            self.load_pretrained_weights(detector_pkl_file, model=\'detector\')\n        elif base_cnn_pkl_file is not None:\n            self.load_pretrained_weights(base_cnn_pkl_file, model=\'base_cnn\')\n\n    \n        self.model.eval() # this is needed as batch norm layers in caffe are only affine layers (no running mean or std)\n        \n    def forward(self, image, rois=None, scaling_factor=None, roi_original_idx=None):\n        h,w = image.size(2), image.size(3)\n\n        # compute dense conv features\n        img_features = self.conv_body(image) # equivalent to gpu_0/res4_5_sum\n\n        # generate rois if equipped with RPN head\n        if self.use_rpn_head and not self.use_fpn_body:\n            # case without FPN, proposal generation in a single step\n            rpn_cls_prob,rpn_bbox_pred = self.rpn(img_features)\n            rois, rpn_roi_probs = self.proposal_generator(rpn_cls_prob,rpn_bbox_pred,h,w,scaling_factor)\n        elif self.use_rpn_head and self.use_fpn_body:\n            # case with FPN, proposal generation for each FPN level\n            assert isinstance(img_features,list) and isinstance(self.roi_spatial_scale,list)\n            img_features_tmp = img_features\n            if self.fpn_extra_lvl:\n                # add extra feature resolution by subsampling the last FPN feature level\n                img_features_tmp = img_features_tmp + [torch.nn.functional.max_pool2d(img_features[-1],1,stride=2)]\n            cls_and_bbox = [self.rpn(img_features_tmp[i]) for i in range(len(img_features_tmp))]\n            rois_and_probs = [self.proposal_generator[i](cls_and_bbox[i][0],cls_and_bbox[i][1],h,w,scaling_factor) for i in range(len(img_features_tmp))]\n            rois = [item[0] for item in rois_and_probs]\n            rpn_roi_probs = [item[1] for item in rois_and_probs]\n            # we now combine rois from all FPN levels and re-assign to correct FPN level for later RoI pooling\n            rois,roi_original_idx = self.collect_and_distr_rois(rois,rpn_roi_probs)\n\n        # compute dense roi features\n        if self.use_fpn_body==False:        \n            roi_features = RoIAlignFunction.apply(img_features, preprocess_rois(rois), self.roi_height, self.roi_width, self.roi_spatial_scale, self.roi_sampling_ratio) # 14x14 feature per proposal\n        else:\n            assert isinstance(img_features,list) and isinstance(rois,list) and isinstance(self.roi_spatial_scale,list)\n            roi_features = [RoIAlignFunction.apply(img_features[i], preprocess_rois(rois[i]),self.roi_height, self.roi_width, \n                                             self.roi_spatial_scale[i], self.roi_sampling_ratio) for i in range(len(self.roi_spatial_scale))]\n            # concatenate roi features from all levels of FPN\n            roi_features = torch.cat(tuple(roi_features),0)\n            rois = torch.cat(tuple(rois),0)\n            # restore original order\n            roi_features = roi_features[roi_original_idx,:]\n            rois = rois[roi_original_idx,:]\n\n        # compute 1x1 roi features\n        roi_features = self.conv_head(roi_features) # 1x1 feature per proposal\n        roi_features = roi_features.view(roi_features.size(0),-1)\n\n        # compute classification scores\n        cls_score = self.classif_head(roi_features)\n\n        # compute classification probabilities\n        if self.output_prob:\n            cls_score = torch.nn.functional.softmax(cls_score,dim=1)\n\n        # compute bounding box parameters \n        bbox_pred = self.bbox_head(roi_features)\n        \n        return (cls_score,bbox_pred,rois,img_features)\n\n    \n    def load_pretrained_weights(self, caffe_pkl_file, model=\'detector\'):\n        ## Load pretrained weights\n        print(\'Loading pretrained weights:\')\n        # load caffe weights\n        with open(caffe_pkl_file, \'rb\') as f:\n            caffe_data = pickle.load(f,encoding=\'latin1\')\n        if model==\'detector\':\n            caffe_data=caffe_data[\'blobs\']\n\n        model_dict = self.model.state_dict()\n        print(\'-> loading conv. body weights\')\n        for k in model_dict.keys():\n            if \'running\' in k or \'fc\' in k: # skip running mean/std and fc weights\n                continue\n            k_caffe = parse_th_to_caffe2(k.split(\'.\'))\n            assert model_dict[k].size()==torch.FloatTensor(caffe_data[k_caffe]).size()\n            if k==\'conv1.weight\': # convert from BGR to RGB                \n                model_dict[k]=torch.FloatTensor(caffe_data[k_caffe][:,(2, 1, 0),:,:])\n            else:\n                model_dict[k]=torch.FloatTensor(caffe_data[k_caffe])\n        # update model\n        self.model.load_state_dict(model_dict)\n        # only if full detector model was loaded, as opposed to loading an ImageNet pretrained base-cnn only\n        if model==\'detector\':\n            print(\'-> loading output head weights\')\n            # load also output head weights\n            self.bbox_head.weight.data=torch.FloatTensor(caffe_data[\'bbox_pred_w\'])\n            self.bbox_head.bias.data=torch.FloatTensor(caffe_data[\'bbox_pred_b\'])\n            self.classif_head.weight.data=torch.FloatTensor(caffe_data[\'cls_score_w\'])\n            self.classif_head.bias.data=torch.FloatTensor(caffe_data[\'cls_score_b\'])\n            # load RPN weights\n            if self.use_rpn_head and not self.use_fpn_body:\n                print(\'-> loading rpn head weights\')\n                self.rpn.conv_rpn.weight.data = torch.FloatTensor(caffe_data[\'conv_rpn_w\'])\n                self.rpn.conv_rpn.bias.data = torch.FloatTensor(caffe_data[\'conv_rpn_b\'])\n                self.rpn.rpn_cls_prob.weight.data = torch.FloatTensor(caffe_data[\'rpn_cls_logits_w\'])\n                self.rpn.rpn_cls_prob.bias.data = torch.FloatTensor(caffe_data[\'rpn_cls_logits_b\'])\n                self.rpn.rpn_bbox_pred.weight.data = torch.FloatTensor(caffe_data[\'rpn_bbox_pred_w\'])\n                self.rpn.rpn_bbox_pred.bias.data = torch.FloatTensor(caffe_data[\'rpn_bbox_pred_b\'])\n            if self.use_rpn_head and self.use_fpn_body:\n                print(\'-> loading rpn head weights\')\n                self.rpn.conv_rpn.weight.data = torch.FloatTensor(caffe_data[\'conv_rpn_fpn2_w\'])\n                self.rpn.conv_rpn.bias.data = torch.FloatTensor(caffe_data[\'conv_rpn_fpn2_b\'])\n                self.rpn.rpn_cls_prob.weight.data = torch.FloatTensor(caffe_data[\'rpn_cls_logits_fpn2_w\'])\n                self.rpn.rpn_cls_prob.bias.data = torch.FloatTensor(caffe_data[\'rpn_cls_logits_fpn2_b\'])\n                self.rpn.rpn_bbox_pred.weight.data = torch.FloatTensor(caffe_data[\'rpn_bbox_pred_fpn2_w\'])\n                self.rpn.rpn_bbox_pred.bias.data = torch.FloatTensor(caffe_data[\'rpn_bbox_pred_fpn2_b\'])\n            if self.use_mask_head:\n                print(\'-> loading mask head weights\')                \n                self.mask_head.transposed_conv.weight.data = torch.FloatTensor(caffe_data[\'conv5_mask_w\'])\n                self.mask_head.transposed_conv.bias.data = torch.FloatTensor(caffe_data[\'conv5_mask_b\'])\n                self.mask_head.classif_logits.weight.data = torch.FloatTensor(caffe_data[\'mask_fcn_logits_w\'])\n                self.mask_head.classif_logits.bias.data = torch.FloatTensor(caffe_data[\'mask_fcn_logits_b\'])  \n                if self.mask_head_type==\'1up4convs\':\n                    print(\'-> loading 1up4convs mask head weights\')                \n                    self.mask_head.conv_head.fcn1.weight.data = torch.FloatTensor(caffe_data[\'_[mask]_fcn1_w\'])\n                    self.mask_head.conv_head.fcn1.bias.data   = torch.FloatTensor(caffe_data[\'_[mask]_fcn1_b\'])\n                    self.mask_head.conv_head.fcn2.weight.data = torch.FloatTensor(caffe_data[\'_[mask]_fcn2_w\'])\n                    self.mask_head.conv_head.fcn2.bias.data   = torch.FloatTensor(caffe_data[\'_[mask]_fcn2_b\'])\n                    self.mask_head.conv_head.fcn3.weight.data = torch.FloatTensor(caffe_data[\'_[mask]_fcn3_w\'])\n                    self.mask_head.conv_head.fcn3.bias.data   = torch.FloatTensor(caffe_data[\'_[mask]_fcn3_b\'])\n                    self.mask_head.conv_head.fcn4.weight.data = torch.FloatTensor(caffe_data[\'_[mask]_fcn4_w\'])\n                    self.mask_head.conv_head.fcn4.bias.data   = torch.FloatTensor(caffe_data[\'_[mask]_fcn4_b\'])\n            # load FPN weights\n            if self.use_fpn_body:\n                print(\'-> loading FPN lateral weights\')\n                for i in range(len(self.conv_body.fpn_layers)):\n                    l=self.conv_body.fpn_layers[i]\n                    # get name of last conv layer of each ResNet block which is used for FPN\n                    k_caffe=parse_th_to_caffe2((l+\'.\'+list(getattr(self.model,l).state_dict().keys())[-1]).split(\'.\'))\n                    k_caffe=k_caffe[:k_caffe.rfind(""_"")]\n                    if i<len(self.conv_body.fpn_layers)-1:\n                        suffix=\'_sum_lateral\'\n                    else:\n                        suffix=\'_sum\'\n                    self.conv_body.fpn_lateral[i].weight.data = torch.FloatTensor(caffe_data[\'fpn_inner_\'+k_caffe+suffix+\'_w\'])\n                    self.conv_body.fpn_lateral[i].bias.data = torch.FloatTensor(caffe_data[\'fpn_inner_\'+k_caffe+suffix+\'_b\'])\n                    self.conv_body.fpn_output[i].weight.data = torch.FloatTensor(caffe_data[\'fpn_\'+k_caffe+\'_sum_w\'])\n                    self.conv_body.fpn_output[i].bias.data = torch.FloatTensor(caffe_data[\'fpn_\'+k_caffe+\'_sum_b\'])\n            # load 2 layer mlp weights\n            if self.use_two_layer_mlp_head:\n                print(\'-> loading two layer mlp conv head...\')\n                self.conv_head.fc6.weight.data = torch.FloatTensor(caffe_data[\'fc6_w\'])\n                self.conv_head.fc6.bias.data = torch.FloatTensor(caffe_data[\'fc6_b\'])\n                self.conv_head.fc7.weight.data = torch.FloatTensor(caffe_data[\'fc7_w\'])\n                self.conv_head.fc7.bias.data = torch.FloatTensor(caffe_data[\'fc7_b\'])\n\n\n\n'"
lib/model/generate_proposals.py,16,"b'import torch\nfrom torch.autograd import Variable\nimport numpy as np\nimport utils.boxes as box_utils\nfrom utils.generate_anchors import generate_anchors\n\n\nclass GenerateProposals(torch.nn.Module):\n    """"""Output object detection proposals by applying estimated bounding-box\n    transformations to a set of regular boxes (called ""anchors"").\n    """"""\n    \n    def __init__(self, spatial_scale=0.0625,\n                 train=False,\n                 rpn_pre_nms_top_n = None,\n                 rpn_post_nms_top_n = None,\n                 rpn_nms_thresh = None,\n                 rpn_min_size = 0,\n                 anchor_sizes=(32, 64, 128, 256, 512), \n                 anchor_aspect_ratios=(0.5, 1, 2)):\n        super(GenerateProposals, self).__init__()\n        self._anchors = generate_anchors(sizes=anchor_sizes, aspect_ratios=anchor_aspect_ratios,stride=1. / spatial_scale)\n        self._num_anchors = self._anchors.shape[0]\n        self._spatial_scale = spatial_scale\n        self._train = train        \n        self.rpn_pre_nms_top_n = rpn_pre_nms_top_n if rpn_pre_nms_top_n is not None else (12000 if train else 6000)\n        self.rpn_post_nms_top_n = rpn_post_nms_top_n if rpn_post_nms_top_n is not None else (2000 if train else 1000)\n        self.rpn_nms_thresh = rpn_nms_thresh if rpn_nms_thresh is not None else 0.7\n        self.rpn_min_size = rpn_min_size if rpn_min_size is not None else 0\n\n    def forward(self, rpn_cls_probs, rpn_bbox_pred, im_height, im_width, scaling_factor, spatial_scale=None):\n        if spatial_scale is None:  \n            spatial_scale = self._spatial_scale\n        """"""See modeling.detector.GenerateProposals for inputs/outputs\n        documentation.\n        """"""\n        # 1. for each location i in a (H, W) grid:\n        #      generate A anchor boxes centered on cell i\n        #      apply predicted bbox deltas to each of the A anchors at cell i\n        # 2. clip predicted boxes to image\n        # 3. remove predicted boxes with either height or width < threshold\n        # 4. sort all (proposal, score) pairs by score from highest to lowest\n        # 5. take the top pre_nms_topN proposals before NMS\n        # 6. apply NMS with a loose threshold (0.7) to the remaining proposals\n        # 7. take after_nms_topN proposals after NMS\n        # 8. return the top proposals\n        \n        # 1. get anchors at all features positions\n        all_anchors_np = self.get_all_anchors(num_images = rpn_cls_probs.shape[0],\n                                      feature_height = rpn_cls_probs.shape[2],\n                                      feature_width = rpn_cls_probs.shape[3],\n                                      spatial_scale = spatial_scale)\n        \n        all_anchors = Variable(torch.FloatTensor(all_anchors_np))\n        if rpn_cls_probs.is_cuda:\n            all_anchors = all_anchors.cuda()\n    \n        # Transpose and reshape predicted bbox transformations to get them\n        # into the same order as the anchors:\n        #   - bbox deltas will be (4 * A, H, W) format from conv output\n        #   - transpose to (H, W, 4 * A)\n        #   - reshape to (H * W * A, 4) where rows are ordered by (H, W, A)\n        #     in slowest to fastest order to match the enumerated anchors\n        bbox_deltas = rpn_bbox_pred.squeeze(0).permute(1, 2, 0).contiguous().view(-1, 4)\n        bbox_deltas_np = bbox_deltas.cpu().data.numpy()\n\n        # Same story for the scores:\n        #   - scores are (A, H, W) format from conv output\n        #   - transpose to (H, W, A)\n        #   - reshape to (H * W * A, 1) where rows are ordered by (H, W, A)\n        #     to match the order of anchors and bbox_deltas\n        scores = rpn_cls_probs.squeeze(0).permute(1, 2, 0).contiguous().view(-1, 1)\n        scores_np = scores.cpu().data.numpy()\n\n        # 4. sort all (proposal, score) pairs by score from highest to lowest\n        # 5. take top pre_nms_topN (e.g. 6000)\n        if self.rpn_pre_nms_top_n <= 0 or self.rpn_pre_nms_top_n >= len(scores_np):\n            order = np.argsort(-scores_np.squeeze())\n        else:\n            # Avoid sorting possibly large arrays; First partition to get top K\n            # unsorted and then sort just those (~20x faster for 200k scores)\n            inds = np.argpartition(\n                -scores_np.squeeze(), self.rpn_pre_nms_top_n\n            )[:self.rpn_pre_nms_top_n]\n            order = np.argsort(-scores_np[inds].squeeze())\n            order = inds[order]\n            \n        bbox_deltas = bbox_deltas[order, :]\n        bbox_deltas_np = bbox_deltas_np[order, :]\n        scores = scores[order,:]        \n        scores_np = scores_np[order,:]\n        all_anchors = all_anchors[order, :]\n        all_anchors_np00 = all_anchors_np[order, :]    \n\n        # Transform anchors into proposals via bbox transformations\n        proposals = self.bbox_transform(all_anchors, bbox_deltas, (1.0, 1.0, 1.0, 1.0))\n\n        # 2. clip proposals to image (may result in proposals with zero area\n        # that will be removed in the next step)\n        proposals = self.clip_tiled_boxes(proposals, im_height, im_width)\n        proposals_np = proposals.cpu().data.numpy()\n\n        # 3. remove predicted boxes with either height or width < min_size\n        keep = self.filter_boxes(proposals_np, self.rpn_min_size, scaling_factor, im_height, im_width)\n\n        proposals = proposals[keep, :]\n        proposals_np = proposals_np[keep, :]\n        scores = scores[keep,:]\n        scores_np = scores_np[keep]\n\n        # 6. apply loose nms (e.g. threshold = 0.7)\n        # 7. take after_nms_topN (e.g. 300)\n        # 8. return the top proposals (-> RoIs top)\n        if self.rpn_nms_thresh > 0:\n            keep = box_utils.nms(np.hstack((proposals_np, scores_np)), self.rpn_nms_thresh)\n            if self.rpn_post_nms_top_n > 0:\n                keep = keep[:self.rpn_post_nms_top_n]\n                \n            proposals = proposals[keep, :]\n            scores = scores[keep,:]\n            \n        return proposals, scores\n        \n    def get_all_anchors(self,num_images,feature_height,feature_width,spatial_scale):\n        # 1. Generate proposals from bbox deltas and shifted anchors\n        # the number of proposals is equal to the number of anchors (eg.15)\n        # times the feature support size (eg=50x50=2500), totaling about 40k\n\n        # Enumerate all shifted positions on the (H, W) grid\n        feat_stride = 1. / spatial_scale\n        shift_x = np.arange(0, feature_width) * feat_stride\n        shift_y = np.arange(0, feature_height) * feat_stride\n        shift_x, shift_y = np.meshgrid(shift_x, shift_y, copy=False)\n        # Convert to (K, 4), K=H*W, where the columns are (dx, dy, dx, dy)\n        # shift pointing to each grid location\n        shifts = np.vstack((shift_x.ravel(), shift_y.ravel(),\n                            shift_x.ravel(), shift_y.ravel())).transpose()\n\n        # Broacast anchors over shifts to enumerate all anchors at all positions\n        # in the (H, W) grid:\n        #   - add A anchors of shape (1, A, 4) to\n        #   - K shifts of shape (K, 1, 4) to get\n        #   - all shifted anchors of shape (K, A, 4)\n        #   - reshape to (K*A, 4) shifted anchors\n        A = self._num_anchors\n        K = shifts.shape[0]\n        all_anchors = self._anchors[np.newaxis, :, :] + shifts[:, np.newaxis, :]\n        all_anchors = all_anchors.reshape((K * A, 4))\n        return all_anchors\n        \n    def filter_boxes(self, boxes, min_size, scale_factor, image_height, image_width):\n        """"""Only keep boxes with both sides >= min_size and center within the image.\n        """"""\n        # Scale min_size to match image scale\n        min_size *= scale_factor\n        ws = boxes[:, 2] - boxes[:, 0] + 1\n        hs = boxes[:, 3] - boxes[:, 1] + 1\n        x_ctr = boxes[:, 0] + ws / 2.\n        y_ctr = boxes[:, 1] + hs / 2.\n        keep = np.where(\n            (ws >= min_size) & (hs >= min_size) &\n            (x_ctr < image_width) & (y_ctr < image_height))[0]\n        return keep\n\n    def bbox_transform(self, boxes, deltas, weights=(1.0, 1.0, 1.0, 1.0), clip_value=4.135166556742356):\n        """"""Forward transform that maps proposal boxes to predicted ground-truth\n        boxes using bounding-box regression deltas. See bbox_transform_inv for a\n        description of the weights argument.\n        """"""\n        if boxes.size(0) == 0:\n            return None\n            #return np.zeros((0, deltas.shape[1]), dtype=deltas.dtype)\n\n        # get boxes dimensions and centers\n        widths = boxes[:, 2] - boxes[:, 0] + 1.0\n        heights = boxes[:, 3] - boxes[:, 1] + 1.0\n        ctr_x = boxes[:, 0] + 0.5 * widths\n        ctr_y = boxes[:, 1] + 0.5 * heights\n\n        wx, wy, ww, wh = weights\n        dx = deltas[:, 0::4] / wx\n        dy = deltas[:, 1::4] / wy\n        dw = deltas[:, 2::4] / ww\n        dh = deltas[:, 3::4] / wh\n        \n        clip_value = Variable(torch.FloatTensor([clip_value]))\n        if boxes.is_cuda:\n            clip_value = clip_value.cuda()\n\n        # Prevent sending too large values into np.exp()\n        dw = torch.min(dw,clip_value)\n        dh = torch.min(dh,clip_value)\n\n        pred_ctr_x = dx * widths.unsqueeze(1) + ctr_x.unsqueeze(1)\n        pred_ctr_y = dy * heights.unsqueeze(1) + ctr_y.unsqueeze(1)\n        pred_w = torch.exp(dw) * widths.unsqueeze(1)\n        pred_h = torch.exp(dh) * heights.unsqueeze(1)\n\n        # pred_boxes = np.zeros(deltas.shape, dtype=deltas.dtype)\n        # x1\n        pred_boxes_x1 = pred_ctr_x - 0.5 * pred_w\n        # y1\n        pred_boxes_y1 = pred_ctr_y - 0.5 * pred_h\n        # x2 (note: ""- 1"" is correct; don\'t be fooled by the asymmetry)\n        pred_boxes_x2 = pred_ctr_x + 0.5 * pred_w - 1\n        # y2 (note: ""- 1"" is correct; don\'t be fooled by the asymmetry)\n        pred_boxes_y2 = pred_ctr_y + 0.5 * pred_h - 1\n\n        pred_boxes = torch.cat((pred_boxes_x1,\n                                pred_boxes_y1,\n                                pred_boxes_x2,\n                                pred_boxes_y2),1)\n\n        return pred_boxes\n\n    def clip_tiled_boxes(self, boxes, image_height, image_width):\n        """"""Clip boxes to image boundaries. im_shape is [height, width] and boxes\n        has shape (N, 4 * num_tiled_boxes).""""""\n\n        im_w = Variable(torch.FloatTensor([float(image_width)]))\n        im_h = Variable(torch.FloatTensor([float(image_height)]))\n        z = Variable(torch.FloatTensor([0]))\n\n        if boxes.is_cuda:\n            im_w = im_w.cuda()\n            im_h = im_h.cuda()\n            z = z.cuda()\n            \n        # x1 >= 0\n        boxes[:, 0::4] = torch.max(torch.min(boxes[:, 0::4], im_w - 1), z)\n        # y1 >= 0\n        boxes[:, 1::4] = torch.max(torch.min(boxes[:, 1::4], im_h - 1), z)\n        # x2 < im_shape[1]\n        boxes[:, 2::4] = torch.max(torch.min(boxes[:, 2::4], im_w - 1), z)\n        # y2 < im_shape[0]\n        boxes[:, 3::4] = torch.max(torch.min(boxes[:, 3::4], im_h - 1), z)\n        \n        return boxes'"
lib/model/loss.py,9,"b""import torch\nimport pickle\nimport numpy as np\n#import copy\nimport torchvision.models as models\nfrom model.roi_align import RoIAlign\nfrom model.generate_proposals import GenerateProposals\nfrom utils.utils import isnan,infbreak,printmax\n\nfrom torch.autograd import Variable\nfrom torch.nn.functional import cross_entropy\n\ndef smooth_L1(pred,targets,alpha_in,alpha_out,beta=1.0):\n    x=(pred-targets)*alpha_in\n    xabs=torch.abs(x)\n    y1=0.5*x**2/beta\n    y2=xabs-0.5*beta\n    case1=torch.le(xabs,beta).float()\n    case2=1-case1\n    return torch.sum((y1*case1+y2*case2)*alpha_out)/pred.size(0)\n\ndef accuracy(cls_score,cls_labels):\n    class_dim = cls_score.dim()-1\n    argmax=torch.max(torch.nn.functional.softmax(cls_score,dim=class_dim),class_dim)[1]\n    accuracy = torch.mean(torch.eq(argmax,cls_labels.long()).float())\n    return accuracy\n\n# class detector_loss(torch.nn.Module):\n#     def __init__(self, do_loss_cls=True, do_loss_bbox=True, do_accuracy_cls=True):\n#         super(detector_loss, self).__init__()\n#         # Flags\n#         self.do_loss_cls = do_loss_cls\n#         self.do_loss_bbox = do_loss_bbox\n#         self.do_accuracy_cls = do_accuracy_cls\n#         # Dicts for losses \n#         # self.losses={}\n#         # if do_loss_cls:\n#         #     self.losses['loss_cls']=0\n#         # if do_loss_bbox:\n#         #     self.losses['loss_bbox']=0\n#         # # Dicts for metrics       \n#         # self.metrics={}\n#         # if do_accuracy_cls:\n#         #     self.metrics['accuracy_cls']=0\n\n#     def forward(self,\n#             cls_score,\n#             cls_labels,\n#             bbox_pred,\n#             bbox_targets,\n#             bbox_inside_weights,\n#             bbox_outside_weights):\n\n#         # compute losses\n#         losses=[]\n#         if self.do_loss_cls:\n#             loss_cls = cross_entropy(cls_score,cls_labels.long())\n#             losses.append(loss_cls)\n#         if self.do_loss_bbox:\n#             loss_bbox = smooth_L1(bbox_pred,bbox_targets,bbox_inside_weights,bbox_outside_weights)\n#             losses.append(loss_bbox)\n\n#         # # compute metrics\n#         # if self.do_accuracy_cls:\n#         #     self.metrics['accuracy_cls'] = accuracy(cls_score,cls_labels.long())\n\n#         # sum total loss\n#         #loss = torch.sum(torch.cat(tuple([v.unsqueeze(0) for v in losses]),0))        \n\n#         # loss.register_hook(printmax)\n\n#         return tuple(losses)\n        """
lib/model/roi_align.py,12,"b'import torch\nfrom torch.autograd import Function\nfrom torch.nn.modules.module import Module\nfrom torch.autograd import Variable \nimport os\nfrom torch.autograd.function import once_differentiable\n\ntorch_ver = torch.__version__[:3]\n\nif torch_ver==""0.4"":\n    from torch.utils.cpp_extension import load   \n    build_path = os.path.realpath(os.path.join(os.path.dirname(os.path.realpath(__file__)),\'../cppcuda/build/\'))\n\n    print(\'compiling/loading roi_align\')\n    roialign = load(name=\'roialign\',sources=[\'lib/cppcuda/roi_align_binding.cpp\',\n                                            \'lib/cppcuda/roi_align_forward_cuda.cu\',\n                                            \'lib/cppcuda/roi_align_backward_cuda.cu\'],\n                    build_directory=build_path,verbose=True)\nelse:\n    import cppcuda_cffi.roialign as roialign\n\n\nclass RoIAlignFunction(Function):\n    # def __init__(ctx, pooled_height, pooled_width, spatial_scale, sampling_ratio):\n    #     ctx.pooled_width = int(pooled_width)\n    #     ctx.pooled_height = int(pooled_height)\n    #     ctx.spatial_scale = float(spatial_scale)\n    #     ctx.sampling_ratio = int(sampling_ratio)\n    #     ctx.features_size = None\n    #     ctx.rois=None\n\n    @staticmethod  \n    def forward(ctx, features, rois, pooled_height, pooled_width, spatial_scale, sampling_ratio):\n        #ctx.save_for_backward(rois)\n        ctx.rois=rois\n        ctx.features_size=features.size()\n        ctx.pooled_height=pooled_height\n        ctx.pooled_width=pooled_width\n        ctx.spatial_scale=spatial_scale\n        ctx.sampling_ratio=sampling_ratio\n\n        # compute\n        if features.is_cuda != rois.is_cuda:\n            raise TypeError(\'features and rois should be on same device (CPU or GPU)\')\n        elif features.is_cuda and rois.is_cuda :\n            if torch_ver==""0.4"":\n                output = roialign.roi_align_forward_cuda(features,\n                                                rois,\n                                                pooled_height,\n                                                pooled_width,\n                                                spatial_scale,\n                                                sampling_ratio)\n            else:\n                num_channels = features.size(1)\n                num_rois = rois.size(0)\n                output = torch.zeros(num_rois, num_channels, pooled_height, pooled_width).cuda()\n                roialign.roi_align_forward_cuda(features,\n                                rois,\n                                output,\n                                pooled_height,\n                                pooled_width,\n                                spatial_scale,\n                                sampling_ratio)\n\n        elif features.is_cuda==False and rois.is_cuda==False:\n            if torch_ver==""0.4"":\n                output = roialign.roi_align_forward_cpu(features,\n                                                rois,\n                                                pooled_height,\n                                                pooled_width,\n                                                spatial_scale,\n                                                sampling_ratio)\n            else:\n                num_channels = features.size(1)\n                num_rois = rois.size(0)\n                output = torch.zeros(num_rois, num_channels, pooled_height, pooled_width)\n                roialign.roi_align_forward_cpu(features,\n                                rois,\n                                output,\n                                pooled_height,\n                                pooled_width,\n                                spatial_scale,\n                                sampling_ratio)\n    \n\n        if torch_ver==""0.4"":\n            return Variable(output,requires_grad=True)\n        else:\n            return output\n\n    @staticmethod\n    @once_differentiable\n    def backward(ctx, grad_output):\n        #rois, = ctx.saved_variables\n        rois = ctx.rois\n        features_size=ctx.features_size\n        pooled_height=ctx.pooled_height\n        pooled_width=ctx.pooled_width\n        spatial_scale=ctx.spatial_scale\n        sampling_ratio=ctx.sampling_ratio\n\n        #rois = ctx.rois\n        if rois.is_cuda:\n            if torch_ver==""0.4"":\n                grad_input = roialign.roi_align_backward_cuda(rois,\n                                    grad_output,\n                                    features_size[0],\n                                    features_size[1],\n                                    features_size[2],\n                                    features_size[3],\n                                    pooled_height,\n                                    pooled_width,\n                                    spatial_scale,\n                                    sampling_ratio)\n            else:\n                #import pdb; pdb.set_trace()\n                grad_input = torch.zeros(features_size).cuda(rois.get_device()) # <- the problem!\n                roialign.roi_align_backward_cuda(rois,\n                                grad_output,\n                                grad_input,\n                                pooled_height,\n                                    pooled_width,\n                                    spatial_scale,\n                                    sampling_ratio)\n            \n        else:\n            if torch_ver==""0.4"":\n                grad_input = roialign.roi_align_backward_cpu(rois,\n                                    grad_output,\n                                    features_size[0],\n                                    features_size[1],\n                                    features_size[2],\n                                    features_size[3],\n                                    pooled_height,\n                                    pooled_width,\n                                    spatial_scale,\n                                    sampling_ratio)\n            else:\n                raise(""backward pass not implemented on cpu in cffi extension"")\n\n        # import pdb; pdb.set_trace()\n        if torch_ver==""0.4"":\n            return Variable(grad_input), None, None, None, None, None\n        else:\n            return grad_input, None, None, None, None, None\n\n\n\n\nclass RoIAlign(Module):\n    def __init__(self, pooled_height, pooled_width, spatial_scale, sampling_ratio=0):\n        super(RoIAlign, self).__init__()\n\n        self.pooled_height=int(pooled_height)\n        self.pooled_width=int(pooled_width)\n        self.spatial_scale=float(spatial_scale)\n        self.sampling_ratio=int(sampling_ratio)\n\n    def forward(self, features, rois):\n        # features is a Variable/FloatTensor of size BxCxHxW\n        # rois is a (optional: list of) Variable/FloatTensor IDX,Xmin,Ymin,Xmax,Ymax (normalized to [0,1])\n        rois = preprocess_rois(rois)\n        output = RoIAlignFunction.apply(features,\n                                        rois,\n                                        self.pooled_height,\n                                        self.pooled_width,\n                                        self.spatial_scale,\n                                        self.sampling_ratio)\n        return output\n       \n\ndef preprocess_rois(rois):\n    # do some verifications on what has been passed as rois\n    if isinstance(rois,list): # if list, convert to single tensor (used for multiscale)\n        rois = torch.cat(tuple(rois),0)\n    if isinstance(rois,Variable):\n        if rois.dim()==3:\n            if rois.size(0)==1:\n                rois = rois.squeeze(0)\n            else:\n                raise(""rois has wrong size"")\n        if rois.size(1)==4:\n            # add zeros\n            zeros = Variable(torch.zeros((rois.size(0),1)))\n            if rois.is_cuda:\n                zeros = zeros.cuda()\n            rois = torch.cat((zeros,rois),1).contiguous()\n    return rois'"
lib/utils/blob.py,0,"b'# Copyright (c) 2017-present, Facebook, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n##############################################################################\n#\n# Based on:\n# --------------------------------------------------------\n# Fast R-CNN\n# Copyright (c) 2015 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ross Girshick\n# --------------------------------------------------------\n\nimport cv2\nimport numpy as np\n\ndef im_list_to_blob(ims,fpn_on=False,fpn_coarsest_stride=32):\n    """"""Convert a list of images into a network input. Assumes images were\n    prepared using prep_im_for_blob or equivalent: i.e.\n      - BGR channel order\n      - pixel means subtracted\n      - resized to the desired input size\n      - float32 numpy ndarray format\n    Output is a 4D HCHW tensor of the images concatenated along axis 0 with\n    shape.\n    """"""\n    max_shape = np.array([im.shape for im in ims]).max(axis=0)\n    # Pad the image so they can be divisible by a stride\n    if fpn_on:\n        stride = float(fpn_coarsest_stride)\n        max_shape[0] = int(np.ceil(max_shape[0] / stride) * stride)\n        max_shape[1] = int(np.ceil(max_shape[1] / stride) * stride)\n\n    num_images = len(ims)\n    blob = np.zeros((num_images, max_shape[0], max_shape[1], 3),\n                    dtype=np.float32)\n    for i in range(num_images):\n        im = ims[i]\n        blob[i, 0:im.shape[0], 0:im.shape[1], :] = im\n    # Move channels (axis 3) to axis 1\n    # Axis order will become: (batch elem, channel, height, width)\n    channel_swap = (0, 3, 1, 2)\n    blob = blob.transpose(channel_swap)\n    return blob\n\n\ndef prep_im_for_blob(im, pixel_means=[122.7717, 115.9465, 102.9801], target_sizes=[800], max_size=1333):\n    """"""Prepare an image for use as a network input blob. Specially:\n      - Subtract per-channel pixel mean\n      - Convert to float32\n      - Rescale to each of the specified target size (capped at max_size)\n    Returns a list of transformed images, one for each target size. Also returns\n    the scale factors that were used to compute each returned image.\n    """"""       \n    im = im.astype(np.float32, copy=False)\n    im -= pixel_means\n    im_shape = im.shape\n    im_size_min = np.min(im_shape[0:2])\n    im_size_max = np.max(im_shape[0:2])\n\n    ims = []\n    im_scales = []\n    for target_size in target_sizes:\n        im_scale = float(target_size) / float(im_size_min)\n        # Prevent the biggest axis from being more than max_size\n        if np.round(im_scale * im_size_max) > max_size:\n            im_scale = float(max_size) / float(im_size_max)\n        # BUGGY im is replaced by scaled im\n        # im = cv2.resize(im, None, None, fx=im_scale, fy=im_scale,\n        #                 interpolation=cv2.INTER_LINEAR)\n        # ims.append(im)\n        im_prime = cv2.resize(im, None, None, fx=im_scale, fy=im_scale,\n                        interpolation=cv2.INTER_LINEAR)\n        ims.append(im_prime)\n        im_scales.append(im_scale)\n\n    return ims, im_scales\n\ndef get_rois_blob(im_rois, im_scale):\n    """"""Converts RoIs into network inputs.\n    Arguments:\n        im_rois (ndarray): R x 4 matrix of RoIs in original image coordinates\n        im_scale_factors (list): scale factors as returned by _get_image_blob\n    Returns:\n        blob (ndarray): R x 5 matrix of RoIs in the image pyramid with columns\n            [level, x1, y1, x2, y2]\n    """"""\n    rois, levels = project_im_rois(im_rois, im_scale)\n    rois_blob = np.hstack((levels, rois))\n    return rois_blob.astype(np.float32, copy=False)\n\n\ndef project_im_rois(im_rois, scales):\n    """"""Project image RoIs into the image pyramid built by _get_image_blob.\n    Arguments:\n        im_rois (ndarray): R x 4 matrix of RoIs in original image coordinates\n        scales (list): scale factors as returned by _get_image_blob\n    Returns:\n        rois (ndarray): R x 4 matrix of projected RoI coordinates\n        levels (ndarray): image pyramid levels used by each projected RoI\n    """"""\n    rois = im_rois.astype(np.float, copy=False) * scales\n    levels = np.zeros((im_rois.shape[0], 1), dtype=np.int)\n    return rois, levels'"
lib/utils/boxes.py,0,"b'# Copyright (c) 2017-present, Facebook, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n##############################################################################\n#\n# Based on:\n# --------------------------------------------------------\n# Fast/er R-CNN\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ross Girshick\n# --------------------------------------------------------\n\n""""""Box manipulation functions. The internal Detectron box format is\n[x1, y1, x2, y2] where (x1, y1) specify the top-left box corner and (x2, y2)\nspecify the bottom-right box corner. Boxes from external sources, e.g.,\ndatasets, may be in other formats (such as [x, y, w, h]) and require conversion.\n\nThis module uses a convention that may seem strange at first: the width of a box\nis computed as x2 - x1 + 1 (likewise for height). The ""+ 1"" dates back to old\nobject detection days when the coordinates were integer pixel indices, rather\nthan floating point coordinates in a subpixel coordinate frame. A box with x2 =\nx1 and y2 = y1 was taken to include a single pixel, having a width of 1, and\nhence requiring the ""+ 1"". Now, most datasets will likely provide boxes with\nfloating point coordinates and the width should be more reasonably computed as\nx2 - x1.\n\nIn practice, as long as a model is trained and tested with a consistent\nconvention either decision seems to be ok (at least in our experience on COCO).\nSince we have a long history of training models with the ""+ 1"" convention, we\nare reluctant to change it even if our modern tastes prefer not to use it.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport numpy as np\nimport os\n\n\n#from core.config import cfg\ntry:\n    import utils_cython.cython_bbox as cython_bbox\n    import utils_cython.cython_nms as cython_nms\nexcept ImportError:\n    print(\'utils_cython modules not compiled\')\n    print(\'--> attempting compilation now\')\n    curr_path=os.getcwd()\n    build_path = os.path.realpath(os.path.join(os.path.dirname(os.path.realpath(__file__)),\'../utils_cython/\'))\n    os.chdir(build_path)\n    res = os.system(""python build_cython.py build_ext --inplace"")\n    if res==0:\n        print(\'--> utils_cython modules compiled successfully\')\n    else:\n        raise(\'--> utils_cython modules compilation failed\')\n    os.chdir(curr_path)\n    import utils_cython.cython_bbox as cython_bbox\n    import utils_cython.cython_nms as cython_nms\n\nbbox_overlaps = cython_bbox.bbox_overlaps\n\ncfg_BBOX_XFORM_CLIP = 4.135166556742356\n\ndef boxes_area(boxes):\n    """"""Compute the area of an array of boxes.""""""\n    w = (boxes[:, 2] - boxes[:, 0] + 1)\n    h = (boxes[:, 3] - boxes[:, 1] + 1)\n    areas = w * h\n    assert np.all(areas >= 0), \'Negative areas founds\'\n    return areas\n\n\ndef unique_boxes(boxes, scale=1.0):\n    """"""Return indices of unique boxes.""""""\n    v = np.array([1, 1e3, 1e6, 1e9])\n    hashes = np.round(boxes * scale).dot(v)\n    _, index = np.unique(hashes, return_index=True)\n    return np.sort(index)\n\n\ndef xywh_to_xyxy(xywh):\n    """"""Convert [x1 y1 w h] box format to [x1 y1 x2 y2] format.""""""\n    if isinstance(xywh, (list, tuple)):\n        # Single box given as a list of coordinates\n        assert len(xywh) == 4\n        x1, y1 = xywh[0], xywh[1]\n        x2 = x1 + np.maximum(0., xywh[2] - 1.)\n        y2 = y1 + np.maximum(0., xywh[3] - 1.)\n        return (x1, y1, x2, y2)\n    elif isinstance(xywh, np.ndarray):\n        # Multiple boxes given as a 2D ndarray\n        return np.hstack(\n            (xywh[:, 0:2], xywh[:, 0:2] + np.maximum(0, xywh[:, 2:4] - 1))\n        )\n    else:\n        raise TypeError(\'Argument xywh must be a list, tuple, or numpy array.\')\n\n\ndef xyxy_to_xywh(xyxy):\n    """"""Convert [x1 y1 x2 y2] box format to [x1 y1 w h] format.""""""\n    if isinstance(xyxy, (list, tuple)):\n        # Single box given as a list of coordinates\n        assert len(xyxy) == 4\n        x1, y1 = xyxy[0], xyxy[1]\n        w = xyxy[2] - x1 + 1\n        h = xyxy[3] - y1 + 1\n        return (x1, y1, w, h)\n    elif isinstance(xyxy, np.ndarray):\n        # Multiple boxes given as a 2D ndarray\n        return np.hstack((xyxy[:, 0:2], xyxy[:, 2:4] - xyxy[:, 0:2] + 1))\n    else:\n        raise TypeError(\'Argument xyxy must be a list, tuple, or numpy array.\')\n\n\ndef filter_small_boxes(boxes, min_size):\n    """"""Keep boxes with width and height both greater than min_size.""""""\n    w = boxes[:, 2] - boxes[:, 0] + 1\n    h = boxes[:, 3] - boxes[:, 1] + 1\n    keep = np.where((w > min_size) & (h > min_size))[0]\n    return keep\n\n\ndef clip_boxes_to_image(boxes, height, width):\n    """"""Clip an array of boxes to an image with the given height and width.""""""\n    boxes[:, [0, 2]] = np.minimum(width - 1., np.maximum(0., boxes[:, [0, 2]]))\n    boxes[:, [1, 3]] = np.minimum(height - 1., np.maximum(0., boxes[:, [1, 3]]))\n    return boxes\n\n\ndef clip_xyxy_to_image(x1, y1, x2, y2, height, width):\n    """"""Clip coordinates to an image with the given height and width.""""""\n    x1 = np.minimum(width - 1., np.maximum(0., x1))\n    y1 = np.minimum(height - 1., np.maximum(0., y1))\n    x2 = np.minimum(width - 1., np.maximum(0., x2))\n    y2 = np.minimum(height - 1., np.maximum(0., y2))\n    return x1, y1, x2, y2\n\n\ndef clip_tiled_boxes(boxes, im_shape):\n    """"""Clip boxes to image boundaries. im_shape is [height, width] and boxes\n    has shape (N, 4 * num_tiled_boxes).""""""\n    assert boxes.shape[1] % 4 == 0, \\\n        \'boxes.shape[1] is {:d}, but must be divisible by 4.\'.format(\n        boxes.shape[1]\n    )\n    # x1 >= 0\n    boxes[:, 0::4] = np.maximum(np.minimum(boxes[:, 0::4], im_shape[1] - 1), 0)\n    # y1 >= 0\n    boxes[:, 1::4] = np.maximum(np.minimum(boxes[:, 1::4], im_shape[0] - 1), 0)\n    # x2 < im_shape[1]\n    boxes[:, 2::4] = np.maximum(np.minimum(boxes[:, 2::4], im_shape[1] - 1), 0)\n    # y2 < im_shape[0]\n    boxes[:, 3::4] = np.maximum(np.minimum(boxes[:, 3::4], im_shape[0] - 1), 0)\n    return boxes\n\n\ndef bbox_transform(boxes, deltas, weights=(1.0, 1.0, 1.0, 1.0)):\n    """"""Forward transform that maps proposal boxes to predicted ground-truth\n    boxes using bounding-box regression deltas. See bbox_transform_inv for a\n    description of the weights argument.\n    """"""\n    if boxes.shape[0] == 0:\n        return np.zeros((0, deltas.shape[1]), dtype=deltas.dtype)\n\n    boxes = boxes.astype(deltas.dtype, copy=False)\n\n    widths = boxes[:, 2] - boxes[:, 0] + 1.0\n    heights = boxes[:, 3] - boxes[:, 1] + 1.0\n    ctr_x = boxes[:, 0] + 0.5 * widths\n    ctr_y = boxes[:, 1] + 0.5 * heights\n\n    wx, wy, ww, wh = weights\n    dx = deltas[:, 0::4] / wx\n    dy = deltas[:, 1::4] / wy\n    dw = deltas[:, 2::4] / ww\n    dh = deltas[:, 3::4] / wh\n\n    # Prevent sending too large values into np.exp()\n    dw = np.minimum(dw, cfg_BBOX_XFORM_CLIP)\n    dh = np.minimum(dh, cfg_BBOX_XFORM_CLIP)\n\n    pred_ctr_x = dx * widths[:, np.newaxis] + ctr_x[:, np.newaxis]\n    pred_ctr_y = dy * heights[:, np.newaxis] + ctr_y[:, np.newaxis]\n    pred_w = np.exp(dw) * widths[:, np.newaxis]\n    pred_h = np.exp(dh) * heights[:, np.newaxis]\n\n    pred_boxes = np.zeros(deltas.shape, dtype=deltas.dtype)\n    # x1\n    pred_boxes[:, 0::4] = pred_ctr_x - 0.5 * pred_w\n    # y1\n    pred_boxes[:, 1::4] = pred_ctr_y - 0.5 * pred_h\n    # x2 (note: ""- 1"" is correct; don\'t be fooled by the asymmetry)\n    pred_boxes[:, 2::4] = pred_ctr_x + 0.5 * pred_w - 1\n    # y2 (note: ""- 1"" is correct; don\'t be fooled by the asymmetry)\n    pred_boxes[:, 3::4] = pred_ctr_y + 0.5 * pred_h - 1\n\n    return pred_boxes\n\n\ndef bbox_transform_inv(boxes, gt_boxes, weights=(1.0, 1.0, 1.0, 1.0)):\n    """"""Inverse transform that computes target bounding-box regression deltas\n    given proposal boxes and ground-truth boxes. The weights argument should be\n    a 4-tuple of multiplicative weights that are applied to the regression\n    target.\n\n    In older versions of this code (and in py-faster-rcnn), the weights were set\n    such that the regression deltas would have unit standard deviation on the\n    training dataset. Presently, rather than computing these statistics exactly,\n    we use a fixed set of weights (10., 10., 5., 5.) by default. These are\n    approximately the weights one would get from COCO using the previous unit\n    stdev heuristic.\n    """"""\n    ex_widths = boxes[:, 2] - boxes[:, 0] + 1.0\n    ex_heights = boxes[:, 3] - boxes[:, 1] + 1.0\n    ex_ctr_x = boxes[:, 0] + 0.5 * ex_widths\n    ex_ctr_y = boxes[:, 1] + 0.5 * ex_heights\n\n    gt_widths = gt_boxes[:, 2] - gt_boxes[:, 0] + 1.0\n    gt_heights = gt_boxes[:, 3] - gt_boxes[:, 1] + 1.0\n    gt_ctr_x = gt_boxes[:, 0] + 0.5 * gt_widths\n    gt_ctr_y = gt_boxes[:, 1] + 0.5 * gt_heights\n\n    wx, wy, ww, wh = weights\n    targets_dx = wx * (gt_ctr_x - ex_ctr_x) / ex_widths\n    targets_dy = wy * (gt_ctr_y - ex_ctr_y) / ex_heights\n    targets_dw = ww * np.log(gt_widths / ex_widths)\n    targets_dh = wh * np.log(gt_heights / ex_heights)\n\n    targets = np.vstack((targets_dx, targets_dy, targets_dw,\n                         targets_dh)).transpose()\n    return targets\n\n\ndef expand_boxes(boxes, scale):\n    """"""Expand an array of boxes by a given scale.""""""\n    w_half = (boxes[:, 2] - boxes[:, 0]) * .5\n    h_half = (boxes[:, 3] - boxes[:, 1]) * .5\n    x_c = (boxes[:, 2] + boxes[:, 0]) * .5\n    y_c = (boxes[:, 3] + boxes[:, 1]) * .5\n\n    w_half *= scale\n    h_half *= scale\n\n    boxes_exp = np.zeros(boxes.shape)\n    boxes_exp[:, 0] = x_c - w_half\n    boxes_exp[:, 2] = x_c + w_half\n    boxes_exp[:, 1] = y_c - h_half\n    boxes_exp[:, 3] = y_c + h_half\n\n    return boxes_exp\n\n\ndef flip_boxes(boxes, im_width):\n    """"""Flip boxes horizontally.""""""\n    boxes_flipped = boxes.copy()\n    boxes_flipped[:, 0::4] = im_width - boxes[:, 2::4] - 1\n    boxes_flipped[:, 2::4] = im_width - boxes[:, 0::4] - 1\n    return boxes_flipped\n\n\ndef aspect_ratio(boxes, aspect_ratio):\n    """"""Perform width-relative aspect ratio transformation.""""""\n    boxes_ar = boxes.copy()\n    boxes_ar[:, 0::4] = aspect_ratio * boxes[:, 0::4]\n    boxes_ar[:, 2::4] = aspect_ratio * boxes[:, 2::4]\n    return boxes_ar\n\n\ndef box_voting(top_dets, all_dets, thresh, scoring_method=\'ID\', beta=1.0):\n    """"""Apply bounding-box voting to refine `top_dets` by voting with `all_dets`.\n    See: https://arxiv.org/abs/1505.01749. Optional score averaging (not in the\n    referenced  paper) can be applied by setting `scoring_method` appropriately.\n    """"""\n    # top_dets is [N, 5] each row is [x1 y1 x2 y2, sore]\n    # all_dets is [N, 5] each row is [x1 y1 x2 y2, sore]\n    top_dets_out = top_dets.copy()\n    top_boxes = top_dets[:, :4]\n    all_boxes = all_dets[:, :4]\n    all_scores = all_dets[:, 4]\n    top_to_all_overlaps = bbox_overlaps(top_boxes, all_boxes)\n    for k in range(top_dets_out.shape[0]):\n        inds_to_vote = np.where(top_to_all_overlaps[k] >= thresh)[0]\n        boxes_to_vote = all_boxes[inds_to_vote, :]\n        ws = all_scores[inds_to_vote]\n        top_dets_out[k, :4] = np.average(boxes_to_vote, axis=0, weights=ws)\n        if scoring_method == \'ID\':\n            # Identity, nothing to do\n            pass\n        elif scoring_method == \'TEMP_AVG\':\n            # Average probabilities (considered as P(detected class) vs.\n            # P(not the detected class)) after smoothing with a temperature\n            # hyperparameter.\n            P = np.vstack((ws, 1.0 - ws))\n            P_max = np.max(P, axis=0)\n            X = np.log(P / P_max)\n            X_exp = np.exp(X / beta)\n            P_temp = X_exp / np.sum(X_exp, axis=0)\n            P_avg = P_temp[0].mean()\n            top_dets_out[k, 4] = P_avg\n        elif scoring_method == \'AVG\':\n            # Combine new probs from overlapping boxes\n            top_dets_out[k, 4] = ws.mean()\n        elif scoring_method == \'IOU_AVG\':\n            P = ws\n            ws = top_to_all_overlaps[k, inds_to_vote]\n            P_avg = np.average(P, weights=ws)\n            top_dets_out[k, 4] = P_avg\n        elif scoring_method == \'GENERALIZED_AVG\':\n            P_avg = np.mean(ws**beta)**(1.0 / beta)\n            top_dets_out[k, 4] = P_avg\n        elif scoring_method == \'QUASI_SUM\':\n            top_dets_out[k, 4] = ws.sum() / float(len(ws))**beta\n        else:\n            raise NotImplementedError(\n                \'Unknown scoring method {}\'.format(scoring_method)\n            )\n\n    return top_dets_out\n\n\ndef nms(dets, thresh):\n    """"""Apply classic DPM-style greedy NMS.""""""\n    if dets.shape[0] == 0:\n        return []\n    return cython_nms.nms(dets, thresh)\n\n\ndef soft_nms(\n    dets, sigma=0.5, overlap_thresh=0.3, score_thresh=0.001, method=\'linear\'\n):\n    """"""Apply the soft NMS algorithm from https://arxiv.org/abs/1704.04503.""""""\n    if dets.shape[0] == 0:\n        return dets, []\n\n    methods = {\'hard\': 0, \'linear\': 1, \'gaussian\': 2}\n    assert method in methods, \'Unknown soft_nms method: {}\'.format(method)\n\n    dets, keep = cython_nms.soft_nms(\n        np.ascontiguousarray(dets, dtype=np.float32),\n        np.float32(sigma),\n        np.float32(overlap_thresh),\n        np.float32(score_thresh),\n        np.uint8(methods[method])\n    )\n    return dets, keep'"
lib/utils/collate_custom.py,2,"b'import torch\nimport collections\n#from torch.utils.data.dataloader import default_collate\nimport itertools\n\ndef collate_custom(batch,key=None):\n    """""" Custom collate function for the Dataset class\n     * It doesn\'t convert numpy arrays to stacked-tensors, but rather combines them in a list\n     * This is useful for processing annotations of different sizes\n    """"""    \n    \n    # this case will occur in first pass, and will convert a\n    # list of dictionaries (returned by the threads by sampling dataset[idx])\n    # to a unified dictionary of collated values    \n    if isinstance(batch[0], collections.Mapping):\n        return {key: collate_custom([d[key] for d in batch],key) for key in batch[0]}\n    # these cases will occur in recursion\n    #elif torch.is_tensor(batch[0]): # for tensors, use standrard collating function\n        #return default_collate(batch)\n    elif isinstance(batch,list) and isinstance(batch[0],list): # flatten lists of lists\n        flattened_list  = list(itertools.chain(*batch))\n        return flattened_list\n    elif isinstance(batch,list) and len(batch)==1: # lists of length 1, remove list wrap\n        return batch[0]\n    else: # for other types (i.e. lists of len!=1), return as is\n        return batch\n\n'"
lib/utils/collections.py,0,"b'# Copyright (c) 2017-present, Facebook, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n##############################################################################\n\n""""""A simple attribute dictionary used for representing configuration options.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\n\nclass AttrDict(dict):\n\n    def __getattr__(self, name):\n        if name in self.__dict__:\n            return self.__dict__[name]\n        elif name in self:\n            return self[name]\n        else:\n            raise AttributeError(name)\n\n    def __setattr__(self, name, value):\n        if name in self.__dict__:\n            self.__dict__[name] = value\n        else:\n            self[name] = value\n'"
lib/utils/colormap.py,0,"b'# Copyright (c) 2017-present, Facebook, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n##############################################################################\n\n""""""An awesome colormap for really neat visualizations.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport numpy as np\n\n\ndef colormap(rgb=False):\n    color_list = np.array(\n        [\n            0.000, 0.447, 0.741,\n            0.850, 0.325, 0.098,\n            0.929, 0.694, 0.125,\n            0.494, 0.184, 0.556,\n            0.466, 0.674, 0.188,\n            0.301, 0.745, 0.933,\n            0.635, 0.078, 0.184,\n            0.300, 0.300, 0.300,\n            0.600, 0.600, 0.600,\n            1.000, 0.000, 0.000,\n            1.000, 0.500, 0.000,\n            0.749, 0.749, 0.000,\n            0.000, 1.000, 0.000,\n            0.000, 0.000, 1.000,\n            0.667, 0.000, 1.000,\n            0.333, 0.333, 0.000,\n            0.333, 0.667, 0.000,\n            0.333, 1.000, 0.000,\n            0.667, 0.333, 0.000,\n            0.667, 0.667, 0.000,\n            0.667, 1.000, 0.000,\n            1.000, 0.333, 0.000,\n            1.000, 0.667, 0.000,\n            1.000, 1.000, 0.000,\n            0.000, 0.333, 0.500,\n            0.000, 0.667, 0.500,\n            0.000, 1.000, 0.500,\n            0.333, 0.000, 0.500,\n            0.333, 0.333, 0.500,\n            0.333, 0.667, 0.500,\n            0.333, 1.000, 0.500,\n            0.667, 0.000, 0.500,\n            0.667, 0.333, 0.500,\n            0.667, 0.667, 0.500,\n            0.667, 1.000, 0.500,\n            1.000, 0.000, 0.500,\n            1.000, 0.333, 0.500,\n            1.000, 0.667, 0.500,\n            1.000, 1.000, 0.500,\n            0.000, 0.333, 1.000,\n            0.000, 0.667, 1.000,\n            0.000, 1.000, 1.000,\n            0.333, 0.000, 1.000,\n            0.333, 0.333, 1.000,\n            0.333, 0.667, 1.000,\n            0.333, 1.000, 1.000,\n            0.667, 0.000, 1.000,\n            0.667, 0.333, 1.000,\n            0.667, 0.667, 1.000,\n            0.667, 1.000, 1.000,\n            1.000, 0.000, 1.000,\n            1.000, 0.333, 1.000,\n            1.000, 0.667, 1.000,\n            0.167, 0.000, 0.000,\n            0.333, 0.000, 0.000,\n            0.500, 0.000, 0.000,\n            0.667, 0.000, 0.000,\n            0.833, 0.000, 0.000,\n            1.000, 0.000, 0.000,\n            0.000, 0.167, 0.000,\n            0.000, 0.333, 0.000,\n            0.000, 0.500, 0.000,\n            0.000, 0.667, 0.000,\n            0.000, 0.833, 0.000,\n            0.000, 1.000, 0.000,\n            0.000, 0.000, 0.167,\n            0.000, 0.000, 0.333,\n            0.000, 0.000, 0.500,\n            0.000, 0.000, 0.667,\n            0.000, 0.000, 0.833,\n            0.000, 0.000, 1.000,\n            0.000, 0.000, 0.000,\n            0.143, 0.143, 0.143,\n            0.286, 0.286, 0.286,\n            0.429, 0.429, 0.429,\n            0.571, 0.571, 0.571,\n            0.714, 0.714, 0.714,\n            0.857, 0.857, 0.857,\n            1.000, 1.000, 1.000\n        ]\n    ).astype(np.float32)\n    color_list = color_list.reshape((-1, 3)) * 255\n    if not rgb:\n        color_list = color_list[:, ::-1]\n    return color_list\n'"
lib/utils/data_parallel.py,6,"b'import operator\nimport torch\nimport warnings\nfrom torch.nn import Module\nfrom torch.nn.parallel.scatter_gather import scatter_kwargs, gather\nfrom torch.nn.parallel.replicate import replicate\nfrom torch.nn.parallel.parallel_apply import parallel_apply\n\nclass DataParallel(torch.nn.DataParallel):\n    def __init__(self, *args, **kwargs):\n            super(MyDataParallel, self).__init__(*args, **kwargs)\n\n    def scatter(self, inputs, kwargs, device_ids): # scatter a list of len N into N gpus\n        return scatter_lists(inputs, kwargs, device_ids)\n\ndef scatter_lists(inputs, kwargs,device_ids):\n        n_inputs = len(inputs)\n        n_devices = len(device_ids)\n        for i in range(n_inputs):\n            assert(len(inputs[i])==n_devices)\n        inputs=tuple([tuple([inputs[i][j].cuda(device_ids[j]) for i in range(n_inputs)]) for j in range(n_devices)])\n        return inputs,kwargs\n\n\ndef data_parallel(module, inputs, device_ids=None, output_device=None, dim=0, module_kwargs=None, dont_scatter=False, dont_gather=False):\n    r""""""Evaluates module(input) in parallel across the GPUs given in device_ids.\n\n    This is the functional version of the DataParallel module.\n\n    Args:\n        module: the module to evaluate in parallel\n        inputs: inputs to the module\n        device_ids: GPU ids on which to replicate module\n        output_device: GPU location of the output  Use -1 to indicate the CPU.\n            (default: device_ids[0])\n    Returns:\n        a Variable containing the result of module(input) located on\n        output_device\n    """"""\n    if not isinstance(inputs, tuple):\n        inputs = (inputs,)\n    #print(\'getting device_ids\')\n    if device_ids is None:\n        device_ids = list(range(torch.cuda.device_count()))\n    #print(device_ids)\n    if output_device is None:\n        output_device = device_ids[0]\n\n    if dont_scatter==False:\n        do_scatter_lists=isinstance(inputs[0],list)\n        if do_scatter_lists:\n            inputs, module_kwargs = scatter_lists(inputs, module_kwargs, device_ids)\n        else:\n            inputs, module_kwargs = scatter_kwargs(inputs, module_kwargs, device_ids, dim)\n\n    if len(device_ids) == 1:\n        return module(*inputs[0], **module_kwargs[0])\n    #print(\'getting used device_ids\')\n    used_device_ids = device_ids[:len(inputs)]\n    #print(used_device_ids)\n    #print(\'making model replicas\')\n    replicas = replicate(module, used_device_ids)\n    #print(\'applying model\')\n    outputs = parallel_apply(replicas, inputs, module_kwargs, used_device_ids)\n    if dont_gather:\n        return tuple([[out[i] for out in outputs] for i in range(len(outputs[0]))])\n    #print(\'gathering result\')\n    return gather(outputs, output_device, dim)'"
lib/utils/dummy_datasets.py,0,"b'# Copyright (c) 2017-present, Facebook, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n##############################################################################\n""""""Provide stub objects that can act as stand-in ""dummy"" datasets for simple use\ncases, like getting all classes in a dataset. This exists so that demos can be\nrun without requiring users to download/install datasets first.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nfrom utils.collections import AttrDict\n\n\ndef get_coco_dataset():\n    """"""A dummy COCO dataset that includes only the \'classes\' field.""""""\n    ds = AttrDict()\n    classes = [\n        \'__background__\', \'person\', \'bicycle\', \'car\', \'motorcycle\', \'airplane\',\n        \'bus\', \'train\', \'truck\', \'boat\', \'traffic light\', \'fire hydrant\',\n        \'stop sign\', \'parking meter\', \'bench\', \'bird\', \'cat\', \'dog\', \'horse\',\n        \'sheep\', \'cow\', \'elephant\', \'bear\', \'zebra\', \'giraffe\', \'backpack\',\n        \'umbrella\', \'handbag\', \'tie\', \'suitcase\', \'frisbee\', \'skis\',\n        \'snowboard\', \'sports ball\', \'kite\', \'baseball bat\', \'baseball glove\',\n        \'skateboard\', \'surfboard\', \'tennis racket\', \'bottle\', \'wine glass\',\n        \'cup\', \'fork\', \'knife\', \'spoon\', \'bowl\', \'banana\', \'apple\', \'sandwich\',\n        \'orange\', \'broccoli\', \'carrot\', \'hot dog\', \'pizza\', \'donut\', \'cake\',\n        \'chair\', \'couch\', \'potted plant\', \'bed\', \'dining table\', \'toilet\', \'tv\',\n        \'laptop\', \'mouse\', \'remote\', \'keyboard\', \'cell phone\', \'microwave\',\n        \'oven\', \'toaster\', \'sink\', \'refrigerator\', \'book\', \'clock\', \'vase\',\n        \'scissors\', \'teddy bear\', \'hair drier\', \'toothbrush\'\n    ]\n    ds.classes = {i: name for i, name in enumerate(classes)}\n    return ds\n'"
lib/utils/fast_rcnn_sample_rois.py,0,"b'# Copyright (c) 2017-present, Facebook, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n##############################################################################\n#\n# Based on:\n# --------------------------------------------------------\n# Fast R-CNN\n# Copyright (c) 2015 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ross Girshick\n# --------------------------------------------------------\n\nimport numpy as np\nimport numpy.random as npr\n\n\n\ndef ones(shape, int32=False):\n    """"""Return a blob of all ones of the given shape with the correct float or\n    int data type.\n    """"""\n    return np.ones(shape, dtype=np.int32 if int32 else np.float32)\n\ndef zeros(shape, int32=False):\n    """"""Return a blob of all zeros of the given shape with the correct float or\n    int data type.\n    """"""\n    return np.zeros(shape, dtype=np.int32 if int32 else np.float32)\n\ndef fast_rcnn_sample_rois(roidb,\n                        im_scale,\n                        batch_idx,\n                        train_batch_size_per_image=512,  # rois per im\n                        train_fg_roi_fraction=0.25,\n                        train_fg_thresh=0.5,\n                        train_bg_thresh_hi=0.5,\n                        train_bg_thresh_lo=0,\n                        mask_on=False,\n                        keypoints_on=False\n                        ):\n    #print(\'debug: setting random seed 1234 in fast_rcnn.py: _sample_rois()\')\n    # npr.seed(1234) # DEBUG\n    """"""Generate a random sample of RoIs comprising foreground and background\n    examples.\n    """"""\n    rois_per_image = int(train_batch_size_per_image)\n    fg_rois_per_image = int(np.round(train_fg_roi_fraction * rois_per_image))\n    max_overlaps = roidb[\'max_overlaps\']\n\n    # Select foreground RoIs as those with >= FG_THRESH overlap\n    fg_inds = np.where(max_overlaps >= train_fg_thresh)[0]\n    # Guard against the case when an image has fewer than fg_rois_per_image\n    # foreground RoIs\n    fg_rois_per_this_image = np.minimum(fg_rois_per_image, fg_inds.size)\n    # Sample foreground regions without replacement\n    if fg_inds.size > 0:\n        fg_inds = npr.choice(\n            fg_inds, size=fg_rois_per_this_image, replace=False\n        )\n\n    # Select background RoIs as those within [BG_THRESH_LO, BG_THRESH_HI)\n    bg_inds = np.where(\n        (max_overlaps < train_bg_thresh_hi) &\n        (max_overlaps >= train_bg_thresh_lo)\n    )[0]\n    # Compute number of background RoIs to take from this image (guarding\n    # against there being fewer than desired)\n    bg_rois_per_this_image = rois_per_image - fg_rois_per_this_image\n    bg_rois_per_this_image = np.minimum(bg_rois_per_this_image, bg_inds.size)\n    # Sample foreground regions without replacement\n    if bg_inds.size > 0:\n        bg_inds = npr.choice(\n            bg_inds, size=bg_rois_per_this_image, replace=False\n        )\n\n    # The indices that we\'re selecting (both fg and bg)\n    keep_inds = np.append(fg_inds, bg_inds)\n    # Label is the class each RoI has max overlap with\n    sampled_labels = roidb[\'max_classes\'][keep_inds]\n    sampled_labels[fg_rois_per_this_image:] = 0  # Label bg RoIs with class 0\n    sampled_boxes = roidb[\'boxes\'][keep_inds]\n\n    if \'bbox_targets\' not in roidb:\n        gt_inds = np.where(roidb[\'gt_classes\'] > 0)[0]\n        gt_boxes = roidb[\'boxes\'][gt_inds, :]\n        gt_assignments = gt_inds[roidb[\'box_to_gt_ind_map\'][keep_inds]]\n        bbox_targets = _compute_targets(\n            sampled_boxes, gt_boxes[gt_assignments, :], sampled_labels\n        )\n        bbox_targets, bbox_inside_weights = _expand_bbox_targets(bbox_targets)\n    else:\n        bbox_targets, bbox_inside_weights = _expand_bbox_targets(\n            roidb[\'bbox_targets\'][keep_inds, :]\n        )\n\n    bbox_outside_weights = np.array(\n        bbox_inside_weights > 0, dtype=bbox_inside_weights.dtype\n    )\n\n    # Scale rois and format as (batch_idx, x1, y1, x2, y2)\n    sampled_rois = sampled_boxes * im_scale\n    repeated_batch_idx = batch_idx * ones((sampled_rois.shape[0], 1))\n    sampled_rois = np.hstack((repeated_batch_idx, sampled_rois))\n\n    # Base Fast R-CNN blobs\n    blob_dict = dict(\n        labels_int32=sampled_labels.astype(np.int32, copy=False),\n        rois=sampled_rois,\n        bbox_targets=bbox_targets,\n        bbox_inside_weights=bbox_inside_weights,\n        bbox_outside_weights=bbox_outside_weights\n    )\n\n    # # Optionally add Mask R-CNN blobs\n    # if mask_on:\n    #     roi_data.mask_rcnn.add_mask_rcnn_blobs(\n    #         blob_dict, sampled_boxes, roidb, im_scale, batch_idx\n    #     )\n\n    # # Optionally add Keypoint R-CNN blobs\n    # if keypoints_on:\n    #     roi_data.keypoint_rcnn.add_keypoint_rcnn_blobs(\n    #         blob_dict, roidb, fg_rois_per_image, fg_inds, im_scale, batch_idx\n    #     )\n\n    return blob_dict\n\ndef _expand_bbox_targets(bbox_target_data, num_classes=81, cls_agnostic_bbox_reg=False):\n    """"""Bounding-box regression targets are stored in a compact form in the\n    roidb.\n    This function expands those targets into the 4-of-4*K representation used\n    by the network (i.e. only one class has non-zero targets). The loss weights\n    are similarly expanded.\n    Returns:\n        bbox_target_data (ndarray): N x 4K blob of regression targets\n        bbox_inside_weights (ndarray): N x 4K blob of loss weights\n    """"""\n    num_bbox_reg_classes = num_classes\n    if cls_agnostic_bbox_reg:\n        num_bbox_reg_classes = 2  # bg and fg\n\n    clss = bbox_target_data[:, 0]\n    bbox_targets = zeros((clss.size, 4 * num_bbox_reg_classes))\n    bbox_inside_weights = zeros(bbox_targets.shape)\n    inds = np.where(clss > 0)[0]\n    for ind in inds:\n        cls = int(clss[ind])\n        start = 4 * cls\n        end = start + 4\n        bbox_targets[ind, start:end] = bbox_target_data[ind, 1:]\n        bbox_inside_weights[ind, start:end] = (1.0, 1.0, 1.0, 1.0)\n    return bbox_targets, bbox_inside_weights'"
lib/utils/generate_anchors.py,0,"b'# Copyright (c) 2017-present, Facebook, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n##############################################################################\n#\n# Based on:\n# --------------------------------------------------------\n# Faster R-CNN\n# Copyright (c) 2015 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ross Girshick and Sean Bell\n# --------------------------------------------------------\n\nimport numpy as np\n\n# Verify that we compute the same anchors as Shaoqing\'s matlab implementation:\n#\n#    >> load output/rpn_cachedir/faster_rcnn_VOC2007_ZF_stage1_rpn/anchors.mat\n#    >> anchors\n#\n#    anchors =\n#\n#       -83   -39   100    56\n#      -175   -87   192   104\n#      -359  -183   376   200\n#       -55   -55    72    72\n#      -119  -119   136   136\n#      -247  -247   264   264\n#       -35   -79    52    96\n#       -79  -167    96   184\n#      -167  -343   184   360\n\n# array([[ -83.,  -39.,  100.,   56.],\n#        [-175.,  -87.,  192.,  104.],\n#        [-359., -183.,  376.,  200.],\n#        [ -55.,  -55.,   72.,   72.],\n#        [-119., -119.,  136.,  136.],\n#        [-247., -247.,  264.,  264.],\n#        [ -35.,  -79.,   52.,   96.],\n#        [ -79., -167.,   96.,  184.],\n#        [-167., -343.,  184.,  360.]])\n\n\ndef generate_anchors(\n    stride=16, sizes=(32, 64, 128, 256, 512), aspect_ratios=(0.5, 1, 2)\n):\n    """"""Generates a matrix of anchor boxes in (x1, y1, x2, y2) format. Anchors\n    are centered on stride / 2, have (approximate) sqrt areas of the specified\n    sizes, and aspect ratios as given.\n    """"""\n    return _generate_anchors(\n        stride,\n        np.array(sizes, dtype=np.float) / stride,\n        np.array(aspect_ratios, dtype=np.float)\n    )\n\n\ndef _generate_anchors(base_size, scales, aspect_ratios):\n    """"""Generate anchor (reference) windows by enumerating aspect ratios X\n    scales wrt a reference (0, 0, base_size - 1, base_size - 1) window.\n    """"""\n    anchor = np.array([1, 1, base_size, base_size], dtype=np.float) - 1\n    anchors = _ratio_enum(anchor, aspect_ratios)\n    anchors = np.vstack(\n        [_scale_enum(anchors[i, :], scales) for i in range(anchors.shape[0])]\n    )\n    return anchors\n\n\ndef _whctrs(anchor):\n    """"""Return width, height, x center, and y center for an anchor (window).""""""\n    w = anchor[2] - anchor[0] + 1\n    h = anchor[3] - anchor[1] + 1\n    x_ctr = anchor[0] + 0.5 * (w - 1)\n    y_ctr = anchor[1] + 0.5 * (h - 1)\n    return w, h, x_ctr, y_ctr\n\n\ndef _mkanchors(ws, hs, x_ctr, y_ctr):\n    """"""Given a vector of widths (ws) and heights (hs) around a center\n    (x_ctr, y_ctr), output a set of anchors (windows).\n    """"""\n    ws = ws[:, np.newaxis]\n    hs = hs[:, np.newaxis]\n    anchors = np.hstack(\n        (\n            x_ctr - 0.5 * (ws - 1),\n            y_ctr - 0.5 * (hs - 1),\n            x_ctr + 0.5 * (ws - 1),\n            y_ctr + 0.5 * (hs - 1)\n        )\n    )\n    return anchors\n\n\ndef _ratio_enum(anchor, ratios):\n    """"""Enumerate a set of anchors for each aspect ratio wrt an anchor.""""""\n    w, h, x_ctr, y_ctr = _whctrs(anchor)\n    size = w * h\n    size_ratios = size / ratios\n    ws = np.round(np.sqrt(size_ratios))\n    hs = np.round(ws * ratios)\n    anchors = _mkanchors(ws, hs, x_ctr, y_ctr)\n    return anchors\n\n\ndef _scale_enum(anchor, scales):\n    """"""Enumerate a set of anchors for each scale wrt an anchor.""""""\n    w, h, x_ctr, y_ctr = _whctrs(anchor)\n    ws = w * scales\n    hs = h * scales\n    anchors = _mkanchors(ws, hs, x_ctr, y_ctr)\n    return anchors'"
lib/utils/io.py,0,"b'# Copyright (c) 2017-present, Facebook, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n##############################################################################\n\n""""""IO utilities.""""""\n\nimport pickle\nimport os\n\ndef save_object(obj, file_name):\n    """"""Save a Python object by pickling it.""""""\n    file_name = os.path.abspath(file_name)\n    with open(file_name, \'wb\') as f:\n        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n'"
lib/utils/json_dataset_evaluator.py,0,"b'# Copyright (c) 2017-present, Facebook, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n##############################################################################\n\n""""""Functions for evaluating results computed for a json dataset.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport json\nimport logging\nimport numpy as np\nimport os\nimport uuid\n\nfrom pycocotools.cocoeval import COCOeval\n\n# from core.config import cfg\nfrom utils.io import save_object\nimport utils.boxes as box_utils\n\nlogger = logging.getLogger(__name__)\n\n\ncfg_KRCNN_KEYPOINT_CONFIDENCE=\'bbox\'\n\ndef evaluate_masks(\n    json_dataset,\n    all_boxes,\n    all_segms,\n    output_dir,\n    use_salt=True,\n    cleanup=False\n):\n    res_file = os.path.join(\n        output_dir, \'segmentations_\' + json_dataset.name + \'_results\'\n    )\n    if use_salt:\n        res_file += \'_{}\'.format(str(uuid.uuid4()))\n    res_file += \'.json\'\n    _write_coco_segms_results_file(\n        json_dataset, all_boxes, all_segms, res_file)\n    # Only do evaluation on non-test sets (annotations are undisclosed on test)\n    if json_dataset.name.find(\'test\') == -1:\n        coco_eval = _do_segmentation_eval(json_dataset, res_file, output_dir)\n    else:\n        coco_eval = None\n    # Optionally cleanup results json file\n    if cleanup:\n        os.remove(res_file)\n    return coco_eval\n\n\ndef _write_coco_segms_results_file(\n    json_dataset, all_boxes, all_segms, res_file\n):\n    # [{""image_id"": 42,\n    #   ""category_id"": 18,\n    #   ""segmentation"": [...],\n    #   ""score"": 0.236}, ...]\n    results = []\n    for cls_ind, cls in enumerate(json_dataset.classes):\n        if cls == \'__background__\':\n            continue\n        if cls_ind >= len(all_boxes):\n            break\n        cat_id = json_dataset.category_to_id_map[cls]\n        results.extend(_coco_segms_results_one_category(\n            json_dataset, all_boxes[cls_ind], all_segms[cls_ind], cat_id))\n    logger.info(\n        \'Writing segmentation results json to: {}\'.format(\n            os.path.abspath(res_file)))\n    with open(res_file, \'w\') as fid:\n        json.dump(results, fid)\n\n\ndef _coco_segms_results_one_category(json_dataset, boxes, segms, cat_id):\n    results = []\n    image_ids = json_dataset.COCO.getImgIds()\n    image_ids.sort()\n    assert len(boxes) == len(image_ids)\n    assert len(segms) == len(image_ids)\n    for i, image_id in enumerate(image_ids):\n        dets = boxes[i]\n        rles = segms[i]\n\n        if isinstance(dets, list) and len(dets) == 0:\n            continue\n\n        dets = dets.astype(np.float)\n        scores = dets[:, -1]\n\n        results.extend(\n            [{\'image_id\': image_id,\n              \'category_id\': cat_id,\n              \'segmentation\': rles[k],\n              \'score\': scores[k]}\n              for k in range(dets.shape[0])])\n\n    return results\n\n\ndef _do_segmentation_eval(json_dataset, res_file, output_dir):\n    coco_dt = json_dataset.COCO.loadRes(str(res_file))\n    coco_eval = COCOeval(json_dataset.COCO, coco_dt, \'segm\')\n    coco_eval.evaluate()\n    coco_eval.accumulate()\n    _log_detection_eval_metrics(json_dataset, coco_eval)\n    eval_file = os.path.join(output_dir, \'segmentation_results.pkl\')\n    save_object(coco_eval, eval_file)\n    logger.info(\'Wrote json eval results to: {}\'.format(eval_file))\n    return coco_eval\n\n\ndef evaluate_boxes(\n    json_dataset, all_boxes, output_dir, use_salt=True, cleanup=False\n):\n    res_file = os.path.join(\n        output_dir, \'bbox_\' + json_dataset.name + \'_results\'\n    )\n    if use_salt:\n        res_file += \'_{}\'.format(str(uuid.uuid4()))\n    res_file += \'.json\'\n    _write_coco_bbox_results_file(json_dataset, all_boxes, res_file)\n    # Only do evaluation on non-test sets (annotations are undisclosed on test)\n    if json_dataset.name.find(\'test\') == -1:\n        coco_eval = _do_detection_eval(json_dataset, res_file, output_dir)\n    else:\n        coco_eval = None\n    # Optionally cleanup results json file\n    if cleanup:\n        os.remove(res_file)\n    return coco_eval\n\n\ndef _write_coco_bbox_results_file(json_dataset, all_boxes, res_file):\n    # [{""image_id"": 42,\n    #   ""category_id"": 18,\n    #   ""bbox"": [258.15,41.29,348.26,243.78],\n    #   ""score"": 0.236}, ...]\n    results = []\n    for cls_ind, cls in enumerate(json_dataset.classes):\n        if cls == \'__background__\':\n            continue\n        if cls_ind >= len(all_boxes):\n            break\n        cat_id = json_dataset.category_to_id_map[cls]\n        results.extend(_coco_bbox_results_one_category(\n            json_dataset, all_boxes[cls_ind], cat_id))\n    logger.info(\n        \'Writing bbox results json to: {}\'.format(os.path.abspath(res_file)))\n    with open(res_file, \'w\') as fid:\n        json.dump(results, fid)\n\n\ndef _coco_bbox_results_one_category(json_dataset, boxes, cat_id):\n    results = []\n    image_ids = json_dataset.COCO.getImgIds()\n    image_ids.sort()\n    assert len(boxes) == len(image_ids)\n    for i, image_id in enumerate(image_ids):\n        dets = boxes[i]\n        if isinstance(dets, list) and len(dets) == 0:\n            continue\n        dets = dets.astype(np.float)\n        scores = dets[:, -1]\n        xywh_dets = box_utils.xyxy_to_xywh(dets[:, 0:4])\n        xs = xywh_dets[:, 0]\n        ys = xywh_dets[:, 1]\n        ws = xywh_dets[:, 2]\n        hs = xywh_dets[:, 3]\n        results.extend(\n            [{\'image_id\': image_id,\n              \'category_id\': cat_id,\n              \'bbox\': [xs[k], ys[k], ws[k], hs[k]],\n              \'score\': scores[k]} for k in range(dets.shape[0])])\n    return results\n\n\ndef _do_detection_eval(json_dataset, res_file, output_dir):\n    coco_dt = json_dataset.COCO.loadRes(str(res_file))\n    coco_eval = COCOeval(json_dataset.COCO, coco_dt, \'bbox\')\n    coco_eval.evaluate()\n    coco_eval.accumulate()\n    _log_detection_eval_metrics(json_dataset, coco_eval)\n    eval_file = os.path.join(output_dir, \'detection_results.pkl\')\n    save_object(coco_eval, eval_file)\n    logger.info(\'Wrote json eval results to: {}\'.format(eval_file))\n    return coco_eval\n\n\ndef _log_detection_eval_metrics(json_dataset, coco_eval):\n    def _get_thr_ind(coco_eval, thr):\n        ind = np.where((coco_eval.params.iouThrs > thr - 1e-5) &\n                       (coco_eval.params.iouThrs < thr + 1e-5))[0][0]\n        iou_thr = coco_eval.params.iouThrs[ind]\n        assert np.isclose(iou_thr, thr)\n        return ind\n\n    IoU_lo_thresh = 0.5\n    IoU_hi_thresh = 0.95\n    ind_lo = _get_thr_ind(coco_eval, IoU_lo_thresh)\n    ind_hi = _get_thr_ind(coco_eval, IoU_hi_thresh)\n    # precision has dims (iou, recall, cls, area range, max dets)\n    # area range index 0: all area ranges\n    # max dets index 2: 100 per image\n    precision = coco_eval.eval[\'precision\'][ind_lo:(ind_hi + 1), :, :, 0, 2]\n    ap_default = np.mean(precision[precision > -1])\n    logger.info(\n        \'~~~~ Mean and per-category AP @ IoU=[{:.2f},{:.2f}] ~~~~\'.format(\n            IoU_lo_thresh, IoU_hi_thresh))\n    logger.info(\'{:.1f}\'.format(100 * ap_default))\n    for cls_ind, cls in enumerate(json_dataset.classes):\n        if cls == \'__background__\':\n            continue\n        # minus 1 because of __background__\n        precision = coco_eval.eval[\'precision\'][\n            ind_lo:(ind_hi + 1), :, cls_ind - 1, 0, 2]\n        ap = np.mean(precision[precision > -1])\n        logger.info(\'{:.1f}\'.format(100 * ap))\n    logger.info(\'~~~~ Summary metrics ~~~~\')\n    coco_eval.summarize()\n\n\ndef evaluate_box_proposals(\n    json_dataset, roidb, thresholds=None, area=\'all\', limit=None\n):\n    """"""Evaluate detection proposal recall metrics. This function is a much\n    faster alternative to the official COCO API recall evaluation code. However,\n    it produces slightly different results.\n    """"""\n    # Record max overlap value for each gt box\n    # Return vector of overlap values\n    areas = {\n        \'all\': 0,\n        \'small\': 1,\n        \'medium\': 2,\n        \'large\': 3,\n        \'96-128\': 4,\n        \'128-256\': 5,\n        \'256-512\': 6,\n        \'512-inf\': 7}\n    area_ranges = [\n        [0**2, 1e5**2],    # all\n        [0**2, 32**2],     # small\n        [32**2, 96**2],    # medium\n        [96**2, 1e5**2],   # large\n        [96**2, 128**2],   # 96-128\n        [128**2, 256**2],  # 128-256\n        [256**2, 512**2],  # 256-512\n        [512**2, 1e5**2]]  # 512-inf\n    assert area in areas, \'Unknown area range: {}\'.format(area)\n    area_range = area_ranges[areas[area]]\n    gt_overlaps = np.zeros(0)\n    num_pos = 0\n    for entry in roidb:\n        gt_inds = np.where(\n            (entry[\'gt_classes\'] > 0) & (entry[\'is_crowd\'] == 0))[0]\n        gt_boxes = entry[\'boxes\'][gt_inds, :]\n        gt_areas = entry[\'seg_areas\'][gt_inds]\n        valid_gt_inds = np.where(\n            (gt_areas >= area_range[0]) & (gt_areas <= area_range[1]))[0]\n        gt_boxes = gt_boxes[valid_gt_inds, :]\n        num_pos += len(valid_gt_inds)\n        non_gt_inds = np.where(entry[\'gt_classes\'] == 0)[0]\n        boxes = entry[\'boxes\'][non_gt_inds, :]\n        if boxes.shape[0] == 0:\n            continue\n        if limit is not None and boxes.shape[0] > limit:\n            boxes = boxes[:limit, :]\n        overlaps = box_utils.bbox_overlaps(\n            boxes.astype(dtype=np.float32, copy=False),\n            gt_boxes.astype(dtype=np.float32, copy=False))\n        _gt_overlaps = np.zeros((gt_boxes.shape[0]))\n        for j in range(min(boxes.shape[0], gt_boxes.shape[0])):\n            # find which proposal box maximally covers each gt box\n            argmax_overlaps = overlaps.argmax(axis=0)\n            # and get the iou amount of coverage for each gt box\n            max_overlaps = overlaps.max(axis=0)\n            # find which gt box is \'best\' covered (i.e. \'best\' = most iou)\n            gt_ind = max_overlaps.argmax()\n            gt_ovr = max_overlaps.max()\n            assert gt_ovr >= 0\n            # find the proposal box that covers the best covered gt box\n            box_ind = argmax_overlaps[gt_ind]\n            # record the iou coverage of this gt box\n            _gt_overlaps[j] = overlaps[box_ind, gt_ind]\n            assert _gt_overlaps[j] == gt_ovr\n            # mark the proposal box and the gt box as used\n            overlaps[box_ind, :] = -1\n            overlaps[:, gt_ind] = -1\n        # append recorded iou coverage level\n        gt_overlaps = np.hstack((gt_overlaps, _gt_overlaps))\n\n    gt_overlaps = np.sort(gt_overlaps)\n    if thresholds is None:\n        step = 0.05\n        thresholds = np.arange(0.5, 0.95 + 1e-5, step)\n    recalls = np.zeros_like(thresholds)\n    # compute recall for each iou threshold\n    for i, t in enumerate(thresholds):\n        recalls[i] = (gt_overlaps >= t).sum() / float(num_pos)\n    # ar = 2 * np.trapz(recalls, thresholds)\n    ar = recalls.mean()\n    return {\'ar\': ar, \'recalls\': recalls, \'thresholds\': thresholds,\n            \'gt_overlaps\': gt_overlaps, \'num_pos\': num_pos}\n\n\ndef evaluate_keypoints(\n    json_dataset,\n    all_boxes,\n    all_keypoints,\n    output_dir,\n    use_salt=True,\n    cleanup=False\n):\n    res_file = os.path.join(\n        output_dir, \'keypoints_\' + json_dataset.name + \'_results\'\n    )\n    if use_salt:\n        res_file += \'_{}\'.format(str(uuid.uuid4()))\n    res_file += \'.json\'\n    _write_coco_keypoint_results_file(\n        json_dataset, all_boxes, all_keypoints, res_file)\n    # Only do evaluation on non-test sets (annotations are undisclosed on test)\n    if json_dataset.name.find(\'test\') == -1:\n        coco_eval = _do_keypoint_eval(json_dataset, res_file, output_dir)\n    else:\n        coco_eval = None\n    # Optionally cleanup results json file\n    if cleanup:\n        os.remove(res_file)\n    return coco_eval\n\n\ndef _write_coco_keypoint_results_file(\n    json_dataset, all_boxes, all_keypoints, res_file\n):\n    results = []\n    for cls_ind, cls in enumerate(json_dataset.classes):\n        if cls == \'__background__\':\n            continue\n        if cls_ind >= len(all_keypoints):\n            break\n        logger.info(\n            \'Collecting {} results ({:d}/{:d})\'.format(\n                cls, cls_ind, len(all_keypoints) - 1))\n        cat_id = json_dataset.category_to_id_map[cls]\n        results.extend(_coco_kp_results_one_category(\n            json_dataset, all_boxes[cls_ind], all_keypoints[cls_ind], cat_id))\n    logger.info(\n        \'Writing keypoint results json to: {}\'.format(\n            os.path.abspath(res_file)))\n    with open(res_file, \'w\') as fid:\n        json.dump(results, fid)\n\n\ndef _coco_kp_results_one_category(json_dataset, boxes, kps, cat_id):\n    results = []\n    image_ids = json_dataset.COCO.getImgIds()\n    image_ids.sort()\n    assert len(kps) == len(image_ids)\n    assert len(boxes) == len(image_ids)\n    use_box_score = False\n    if cfg_KRCNN_KEYPOINT_CONFIDENCE == \'logit\':\n        # This is ugly; see utils.keypoints.heatmap_to_keypoints for the magic\n        # indexes\n        score_index = 2\n    elif cfg_KRCNN_KEYPOINT_CONFIDENCE == \'prob\':\n        score_index = 3\n    elif cfg_KRCNN_KEYPOINT_CONFIDENCE == \'bbox\':\n        use_box_score = True\n    else:\n        raise ValueError(\n            \'KRCNN.KEYPOINT_CONFIDENCE must be ""logit"", ""prob"", or ""bbox""\')\n    for i, image_id in enumerate(image_ids):\n        if len(boxes[i]) == 0:\n            continue\n        kps_dets = kps[i]\n        scores = boxes[i][:, -1].astype(np.float)\n        if len(kps_dets) == 0:\n            continue\n        for j in range(len(kps_dets)):\n            xy = []\n\n            kps_score = 0\n            for k in range(kps_dets[j].shape[1]):\n                xy.append(float(kps_dets[j][0, k]))\n                xy.append(float(kps_dets[j][1, k]))\n                xy.append(1)\n                if not use_box_score:\n                    kps_score += kps_dets[j][score_index, k]\n\n            if use_box_score:\n                kps_score = scores[j]\n            else:\n                kps_score /= kps_dets[j].shape[1]\n\n            results.extend([{\'image_id\': image_id,\n                             \'category_id\': cat_id,\n                             \'keypoints\': xy,\n                             \'score\': kps_score}])\n    return results\n\n\ndef _do_keypoint_eval(json_dataset, res_file, output_dir):\n    ann_type = \'keypoints\'\n    imgIds = json_dataset.COCO.getImgIds()\n    imgIds.sort()\n    coco_dt = json_dataset.COCO.loadRes(res_file)\n    coco_eval = COCOeval(json_dataset.COCO, coco_dt, ann_type)\n    coco_eval.params.imgIds = imgIds\n    coco_eval.evaluate()\n    coco_eval.accumulate()\n    eval_file = os.path.join(output_dir, \'keypoint_results.pkl\')\n    save_object(coco_eval, eval_file)\n    logger.info(\'Wrote json eval results to: {}\'.format(eval_file))\n    coco_eval.summarize()\n    return coco_eval\n'"
lib/utils/logging.py,0,"b'# Copyright (c) 2017-present, Facebook, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n##############################################################################\n\n""""""Utilities for logging.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nfrom collections import deque\nfrom email.mime.text import MIMEText\nimport json\nimport logging\nimport numpy as np\nimport smtplib\nimport sys\n\n# Print lower precision floating point values than default FLOAT_REPR\njson.encoder.FLOAT_REPR = lambda o: format(o, \'.6f\')\n\n\ndef log_json_stats(stats, sort_keys=True):\n    print(\'json_stats: {:s}\'.format(json.dumps(stats, sort_keys=sort_keys)))\n\n\nclass SmoothedValue(object):\n    """"""Track a series of values and provide access to smoothed values over a\n    window or the global series average.\n    """"""\n\n    def __init__(self, window_size):\n        self.deque = deque(maxlen=window_size)\n        self.series = []\n        self.total = 0.0\n        self.count = 0\n\n    def AddValue(self, value):\n        self.deque.append(value)\n        self.series.append(value)\n        self.count += 1\n        self.total += value\n\n    def GetMedianValue(self):\n        return np.median(self.deque)\n\n    def GetAverageValue(self):\n        return np.mean(self.deque)\n\n    def GetGlobalAverageValue(self):\n        return self.total / self.count\n\n\ndef send_email(subject, body, to):\n    s = smtplib.SMTP(\'localhost\')\n    mime = MIMEText(body)\n    mime[\'Subject\'] = subject\n    mime[\'To\'] = to\n    s.sendmail(\'detectron\', to, mime.as_string())\n\n\ndef setup_logging(name):\n    FORMAT = \'%(levelname)s %(filename)s:%(lineno)4d: %(message)s\'\n    # Manually clear root loggers to prevent any module that may have called\n    # logging.basicConfig() from blocking our logging setup\n    logging.root.handlers = []\n    logging.basicConfig(level=logging.INFO, format=FORMAT, stream=sys.stdout)\n    logger = logging.getLogger(name)\n    return logger'"
lib/utils/multilevel_rois.py,0,"b'# Copyright (c) 2017-present, Facebook, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n##############################################################################\n\nimport utils.boxes as box_utils\nimport numpy as np\n\ndef add_multilevel_rois_for_test(blobs, name, roi_min_level=2,roi_max_level=5):\n    """"""Distributes a set of RoIs across FPN pyramid levels by creating new level\n    specific RoI blobs.\n\n    Arguments:\n        blobs (dict): dictionary of blobs\n        name (str): a key in \'blobs\' identifying the source RoI blob\n\n    Returns:\n        [by ref] blobs (dict): new keys named by `name + \'fpn\' + level`\n            are added to dict each with a value that\'s an R_level x 5 ndarray of\n            RoIs (see _get_rois_blob for format)\n    """"""\n    lvl_min = roi_min_level\n    lvl_max = roi_max_level\n    #lvls = map_rois_to_fpn_levels(blobs[name][:, 1:5], lvl_min, lvl_max)\n    lvls = map_rois_to_fpn_levels(blobs[name], lvl_min, lvl_max)\n    blobs = add_multilevel_roi_blobs(\n       blobs, name, blobs[name], lvls, lvl_min, lvl_max\n    )\n    return blobs\n    \ndef map_rois_to_fpn_levels(rois, k_min, k_max, roi_canonical_scale=224, roi_canonical_level=4):\n    """"""Determine which FPN level each RoI in a set of RoIs should map to based\n    on the heuristic in the FPN paper.\n    """"""\n    # Compute level ids\n    s = np.sqrt(box_utils.boxes_area(rois))\n    s0 = roi_canonical_scale  # default: 224\n    lvl0 = roi_canonical_level  # default: 4\n\n    # Eqn.(1) in FPN paper\n    target_lvls = np.floor(lvl0 + np.log2(s / s0 + 1e-6))\n    target_lvls = np.clip(target_lvls, k_min, k_max)\n    return target_lvls\n\n\ndef add_multilevel_roi_blobs(\n    #rois,target_lvls, lvl_min, lvl_max):\n    blobs, blob_prefix, rois, target_lvls, lvl_min, lvl_max):\n    """"""Add RoI blobs for multiple FPN levels to the blobs dict.\n\n    blobs: a dict mapping from blob name to numpy ndarray\n    blob_prefix: name prefix to use for the FPN blobs\n    rois: the source rois as a 2D numpy array of shape (N, 5) where each row is\n      an roi and the columns encode (batch_idx, x1, y1, x2, y2)\n    target_lvls: numpy array of shape (N, ) indicating which FPN level each roi\n      in rois should be assigned to\n    lvl_min: the finest (highest resolution) FPN level (e.g., 2)\n    lvl_max: the coarest (lowest resolution) FPN level (e.g., 6)\n    """"""\n    rois_idx_order = np.empty((0, ))\n    rois_stacked = np.zeros((0, 4), dtype=np.float32)  # for assert    \n    for lvl in range(lvl_min, lvl_max + 1):\n        idx_lvl = np.where(target_lvls == lvl)[0]\n        blobs[blob_prefix + \'_fpn\' + str(lvl)] = rois[idx_lvl, :]\n        rois_idx_order = np.concatenate((rois_idx_order, idx_lvl))\n        rois_stacked = np.vstack(\n            [rois_stacked, blobs[blob_prefix + \'_fpn\' + str(lvl)]]\n        )\n    rois_idx_restore = np.argsort(rois_idx_order).astype(np.int32, copy=False)\n    blobs[blob_prefix + \'_idx_restore_int32\'] = rois_idx_restore\n    # Sanity check that restore order is correct\n    assert (rois_stacked[rois_idx_restore] == rois).all()\n    return blobs'"
lib/utils/preprocess_sample.py,5,"b""import numpy as np\nimport torch\nfrom utils.blob import prep_im_for_blob,im_list_to_blob\nfrom utils.fast_rcnn_sample_rois import fast_rcnn_sample_rois\nfrom utils.multilevel_rois import add_multilevel_rois_for_test\n\nclass preprocess_sample(object):\n    # performs the preprocessing (including building image pyramids and scaling the coordinates)\n    def __init__(self,\n                 target_sizes=800,\n                 max_size=1333,\n                 mean=[122.7717, 115.9465, 102.9801],\n                 remove_dup_proposals=True,\n                 fpn_on=False,\n                 spatial_scale=0.0625,\n                 sample_proposals_for_training=False):\n        self.mean=mean\n        self.target_sizes=target_sizes if isinstance(target_sizes,list) else [target_sizes]\n        self.max_size=max_size\n        self.remove_dup_proposals=remove_dup_proposals\n        self.fpn_on=fpn_on\n        self.spatial_scale=spatial_scale\n        self.sample_proposals_for_training = sample_proposals_for_training\n        \n    def __call__(self, sample):\n        # resizes image and returns scale factors\n        original_im_size=sample['image'].shape\n        im_list,im_scales = prep_im_for_blob(sample['image'],\n                                             pixel_means=self.mean,\n                                             target_sizes=self.target_sizes,\n                                             max_size=self.max_size)\n        sample['image'] = torch.FloatTensor(im_list_to_blob(im_list,self.fpn_on)) # im_list_to blob swaps channels and adds stride in case of fpn\n        sample['scaling_factors'] = im_scales[0] \n        sample['original_im_size'] = torch.FloatTensor(original_im_size)\n        if len(sample['dbentry']['boxes'])!=0 and not self.sample_proposals_for_training: # Fast RCNN test\n            proposals = sample['dbentry']['boxes']*im_scales[0]  \n            if self.remove_dup_proposals:\n                proposals,_ = self.remove_dup_prop(proposals) \n            \n            if self.fpn_on==False:\n                sample['rois'] = torch.FloatTensor(proposals)\n            else:\n                multiscale_proposals = add_multilevel_rois_for_test({'rois': proposals},'rois')\n                for k in multiscale_proposals.keys():\n                    sample[k] = torch.FloatTensor(multiscale_proposals[k])\n\n        elif self.sample_proposals_for_training: # Fast RCNN training\n            sampled_rois_labels_and_targets = fast_rcnn_sample_rois(roidb=sample['dbentry'],\n                                                                    im_scale=im_scales[0],\n                                                                    batch_idx=0) # ok as long as we keep batch_size=1\n            sampled_rois_labels_and_targets = {key: torch.FloatTensor(value) for key,value in sampled_rois_labels_and_targets.items()}\n            # add to sample\n            sample = {**sample, **sampled_rois_labels_and_targets} \n        # remove dbentry from sample\n        del sample['dbentry']\n        return sample\n\n    # from Detectron test.py\n    # When mapping from image ROIs to feature map ROIs, there's some aliasing\n    # (some distinct image ROIs get mapped to the same feature ROI).\n    # Here, we identify duplicate feature ROIs, so we only compute features\n    # on the unique subset.\n    def remove_dup_prop(self,proposals): \n        v = np.array([1e3, 1e6, 1e9, 1e12])\n\n        hashes = np.round(proposals * self.spatial_scale).dot(v)\n        _, index, inv_index = np.unique(hashes, return_index=True, return_inverse=True)\n        proposals = proposals[index, :]\n\n        return (proposals,inv_index)"""
lib/utils/result_utils.py,2,"b'# Copyright (c) 2017-present, Facebook, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n##############################################################################\n\n# some functions are from Detectron\n\nimport numpy as np\nfrom torch.autograd import Variable\nimport utils.boxes as box_utils\nimport cv2\nimport pycocotools.mask as mask_util\n\n\ndef to_np(x):\n    if isinstance(x,np.ndarray):\n        return x    \n    if isinstance(x,Variable):\n        x=x.data\n    return x.cpu().numpy()\n\ndef empty_results(num_classes, num_images):\n    """"""Return empty results lists for boxes, masks, and keypoints.\n    Box detections are collected into:\n      all_boxes[cls][image] = N x 5 array with columns (x1, y1, x2, y2, score)\n    Instance mask predictions are collected into:\n      all_segms[cls][image] = [...] list of COCO RLE encoded masks that are in\n      1:1 correspondence with the boxes in all_boxes[cls][image]\n    Keypoint predictions are collected into:\n      all_keyps[cls][image] = [...] list of keypoints results, each encoded as\n      a 3D array (#rois, 4, #keypoints) with the 4 rows corresponding to\n      [x, y, logit, prob] (See: utils.keypoints.heatmaps_to_keypoints).\n      Keypoints are recorded for person (cls = 1); they are in 1:1\n      correspondence with the boxes in all_boxes[cls][image].\n    """"""\n    # Note: do not be tempted to use [[] * N], which gives N references to the\n    # *same* empty list.\n    all_boxes = [[[] for _ in range(num_images)] for _ in range(num_classes)]\n    all_segms = [[[] for _ in range(num_images)] for _ in range(num_classes)]\n    all_keyps = [[[] for _ in range(num_images)] for _ in range(num_classes)]\n    return all_boxes, all_segms, all_keyps\n\n\ndef extend_results(index, all_res, im_res):\n    """"""Add results for an image to the set of all results at the specified\n    index.\n    """"""\n    # Skip cls_idx 0 (__background__)\n    for cls_idx in range(1, len(im_res)):\n        all_res[cls_idx][index] = im_res[cls_idx]\n        \n# When mapping from image ROIs to feature map ROIs, there\'s some aliasing\n# (some distinct image ROIs get mapped to the same feature ROI).\n# Here, we identify duplicate feature ROIs, so we only compute features\n# on the unique subset.\ndef remove_dup_prop(self,proposals): \n    proposals=proposals.data.numpy()\n    v = np.array([1e3, 1e6, 1e9, 1e12])\n\n    hashes = np.round(proposals * self.spatial_scale).dot(v)\n    _, index, inv_index = np.unique(hashes, return_index=True, return_inverse=True)\n    proposals = proposals[index, :]\n    return torch.FloatTensor(proposals)\n\n\ndef postprocess_output(rois,scaling_factor,im_size,class_scores,bbox_deltas,bbox_reg_weights = (10.0,10.0,5.0,5.0)):\n    boxes = to_np(rois.div(scaling_factor).squeeze(0))\n    bbox_deltas = to_np(bbox_deltas)    \n    orig_im_size = to_np(im_size).squeeze()    \n    # apply deltas\n    pred_boxes = box_utils.bbox_transform(boxes, bbox_deltas, bbox_reg_weights)\n    # clip on boundaries\n    pred_boxes = box_utils.clip_tiled_boxes(pred_boxes,orig_im_size)    \n    scores = to_np(class_scores)\n    # Map scores and predictions back to the original set of boxes\n    # This re-duplicates the previously removed boxes\n    # Is there any use for this?\n#    inv_index = to_np(batch[\'proposal_inv_index\']).squeeze().astype(np.int64)\n#    scores = scores[inv_index, :]\n#    pred_boxes = pred_boxes[inv_index, :]\n    # threshold on score and run nms to remove duplicates\n    scores_final, boxes_final, boxes_per_class = box_results_with_nms_and_limit(scores, pred_boxes)\n    \n    return (scores_final, boxes_final, boxes_per_class)\n\ndef box_results_with_nms_and_limit(scores, boxes,\n                                   num_classes=81,\n                                   score_thresh=0.05,\n                                   overlap_thresh=0.5,\n                                   do_soft_nms=False,\n                                   soft_nms_sigma=0.5,\n                                   soft_nms_method=\'linear\',\n                                   do_bbox_vote=False,\n                                   bbox_vote_thresh=0.8,\n                                   bbox_vote_method=\'ID\',\n                                   max_detections_per_img=100, ### over all classes ###\n                                   ):\n    """"""Returns bounding-box detection results by thresholding on scores and\n    applying non-maximum suppression (NMS).\n    \n    A number of #detections presist after this and are returned, sorted by class\n\n    `boxes` has shape (#detections, 4 * #classes), where each row represents\n    a list of predicted bounding boxes for each of the object classes in the\n    dataset (including the background class). The detections in each row\n    originate from the same object proposal.\n\n    `scores` has shape (#detection, #classes), where each row represents a list\n    of object detection confidence scores for each of the object classes in the\n    dataset (including the background class). `scores[i, j]`` corresponds to the\n    box at `boxes[i, j * 4:(j + 1) * 4]`.\n    """"""\n    cls_boxes = [[] for _ in range(num_classes)]\n    # Apply threshold on detection probabilities and apply NMS\n    # Skip j = 0, because it\'s the background class\n    for j in range(1, num_classes):\n        inds = np.where(scores[:, j] > score_thresh)[0]\n        scores_j = scores[inds, j]\n        boxes_j = boxes[inds, j * 4:(j + 1) * 4]\n        dets_j = np.hstack((boxes_j, scores_j[:, np.newaxis])).astype(\n            np.float32, copy=False\n        )\n        if do_soft_nms:\n            nms_dets, _ = box_utils.soft_nms(\n                dets_j,\n                sigma=soft_nms_sigma,\n                overlap_thresh=overlap_thresh,\n                score_thresh=0.0001,\n                method=soft_nms_method\n            )\n        else:\n            keep = box_utils.nms(dets_j, overlap_thresh)\n            nms_dets = dets_j[keep, :]\n        # Refine the post-NMS boxes using bounding-box voting\n        if do_bbox_vote:\n            nms_dets = box_utils.box_voting(\n                nms_dets,\n                dets_j,\n                bbox_vote_thresh,\n                scoring_method=bbox_vote_method\n            )\n        cls_boxes[j] = nms_dets\n\n    # Limit to max_per_image detections **over all classes**\n    if max_detections_per_img > 0:\n        image_scores = np.hstack(\n            [cls_boxes[j][:, -1] for j in range(1, num_classes)]\n        )\n        if len(image_scores) > max_detections_per_img:\n            image_thresh = np.sort(image_scores)[-max_detections_per_img]\n            for j in range(1, num_classes):\n                keep = np.where(cls_boxes[j][:, -1] >= image_thresh)[0]\n                cls_boxes[j] = cls_boxes[j][keep, :]\n\n    im_results = np.vstack([cls_boxes[j] for j in range(1, num_classes)])\n    boxes = im_results[:, :-1]\n    scores = im_results[:, -1]\n    return scores, boxes, cls_boxes\n\ndef segm_results(cls_boxes, masks, ref_boxes, im_h, im_w,\n                 num_classes=81,\n                 M=14, #  cfg.MRCNN.RESOLUTION\n                 cls_specific_mask=True,\n                 thresh_binarize=0.5):\n    cls_segms = [[] for _ in range(num_classes)]\n    mask_ind = 0\n    # To work around an issue with cv2.resize (it seems to automatically pad\n    # with repeated border values), we manually zero-pad the masks by 1 pixel\n    # prior to resizing back to the original image resolution. This prevents\n    # ""top hat"" artifacts. We therefore need to expand the reference boxes by an\n    # appropriate factor.\n    scale = (M + 2.0) / M\n    ref_boxes = box_utils.expand_boxes(ref_boxes, scale)\n    ref_boxes = ref_boxes.astype(np.int32)\n    padded_mask = np.zeros((M + 2, M + 2), dtype=np.float32)\n\n    # skip j = 0, because it\'s the background class\n    for j in range(1, num_classes):\n        segms = []\n        for _ in range(cls_boxes[j].shape[0]):\n            if cls_specific_mask:\n                padded_mask[1:-1, 1:-1] = masks[mask_ind, j, :, :]\n            else:\n                padded_mask[1:-1, 1:-1] = masks[mask_ind, 0, :, :]\n\n            ref_box = ref_boxes[mask_ind, :]\n            w = ref_box[2] - ref_box[0] + 1\n            h = ref_box[3] - ref_box[1] + 1\n            w = np.maximum(w, 1)\n            h = np.maximum(h, 1)\n\n            mask = cv2.resize(padded_mask, (w, h))\n            mask = np.array(mask > thresh_binarize, dtype=np.uint8)\n            im_mask = np.zeros((im_h, im_w), dtype=np.uint8)\n\n            x_0 = max(ref_box[0], 0)\n            x_1 = min(ref_box[2] + 1, im_w)\n            y_0 = max(ref_box[1], 0)\n            y_1 = min(ref_box[3] + 1, im_h)\n\n            im_mask[y_0:y_1, x_0:x_1] = mask[\n                (y_0 - ref_box[1]):(y_1 - ref_box[1]),\n                (x_0 - ref_box[0]):(x_1 - ref_box[0])\n            ]\n\n            # Get RLE encoding used by the COCO evaluation API\n            rle = mask_util.encode(\n                np.array(im_mask[:, :, np.newaxis], order=\'F\')\n            )[0]\n            rle[\'counts\'] = rle[\'counts\'].decode() # convert back to str so that it can be later saved to json\n            segms.append(rle)\n\n            mask_ind += 1\n\n        cls_segms[j] = segms\n\n    assert mask_ind == masks.shape[0]\n    return cls_segms'"
lib/utils/segms.py,0,"b'# Copyright (c) 2017-present, Facebook, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n##############################################################################\n\n""""""Functions for interacting with segmentation masks in the COCO format.\n\nThe following terms are used in this module\n    mask: a binary mask encoded as a 2D numpy array\n    segm: a segmentation mask in one of the two COCO formats (polygon or RLE)\n    polygon: COCO\'s polygon format\n    RLE: COCO\'s run length encoding format\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport numpy as np\n\nimport pycocotools.mask as mask_util\n\n\ndef flip_segms(segms, height, width):\n    """"""Left/right flip each mask in a list of masks.""""""\n    def _flip_poly(poly, width):\n        flipped_poly = np.array(poly)\n        flipped_poly[0::2] = width - np.array(poly[0::2]) - 1\n        return flipped_poly.tolist()\n\n    def _flip_rle(rle, height, width):\n        if \'counts\' in rle and type(rle[\'counts\']) == list:\n            # Magic RLE format handling painfully discovered by looking at the\n            # COCO API showAnns function.\n            rle = mask_util.frPyObjects([rle], height, width)\n        mask = mask_util.decode(rle)\n        mask = mask[:, ::-1, :]\n        rle = mask_util.encode(np.array(mask, order=\'F\', dtype=np.uint8))\n        return rle\n\n    flipped_segms = []\n    for segm in segms:\n        if type(segm) == list:\n            # Polygon format\n            flipped_segms.append([_flip_poly(poly, width) for poly in segm])\n        else:\n            # RLE format\n            assert type(segm) == dict\n            flipped_segms.append(_flip_rle(segm, height, width))\n    return flipped_segms\n\n\ndef polys_to_mask(polygons, height, width):\n    """"""Convert from the COCO polygon segmentation format to a binary mask\n    encoded as a 2D array of data type numpy.float32. The polygon segmentation\n    is understood to be enclosed inside a height x width image. The resulting\n    mask is therefore of shape (height, width).\n    """"""\n    rle = mask_util.frPyObjects(polygons, height, width)\n    mask = np.array(mask_util.decode(rle), dtype=np.float32)\n    # Flatten in case polygons was a list\n    mask = np.sum(mask, axis=2)\n    mask = np.array(mask > 0, dtype=np.float32)\n    return mask\n\n\ndef mask_to_bbox(mask):\n    """"""Compute the tight bounding box of a binary mask.""""""\n    xs = np.where(np.sum(mask, axis=0) > 0)[0]\n    ys = np.where(np.sum(mask, axis=1) > 0)[0]\n\n    if len(xs) == 0 or len(ys) == 0:\n        return None\n\n    x0 = xs[0]\n    x1 = xs[-1]\n    y0 = ys[0]\n    y1 = ys[-1]\n    return np.array((x0, y0, x1, y1), dtype=np.float32)\n\n\ndef polys_to_mask_wrt_box(polygons, box, M):\n    """"""Convert from the COCO polygon segmentation format to a binary mask\n    encoded as a 2D array of data type numpy.float32. The polygon segmentation\n    is understood to be enclosed in the given box and rasterized to an M x M\n    mask. The resulting mask is therefore of shape (M, M).\n    """"""\n    w = box[2] - box[0]\n    h = box[3] - box[1]\n\n    w = np.maximum(w, 1)\n    h = np.maximum(h, 1)\n\n    polygons_norm = []\n    for poly in polygons:\n        p = np.array(poly, dtype=np.float32)\n        p[0::2] = (p[0::2] - box[0]) * M / w\n        p[1::2] = (p[1::2] - box[1]) * M / h\n        polygons_norm.append(p)\n\n    rle = mask_util.frPyObjects(polygons_norm, M, M)\n    mask = np.array(mask_util.decode(rle), dtype=np.float32)\n    # Flatten in case polygons was a list\n    mask = np.sum(mask, axis=2)\n    mask = np.array(mask > 0, dtype=np.float32)\n    return mask\n\n\ndef polys_to_boxes(polys):\n    """"""Convert a list of polygons into an array of tight bounding boxes.""""""\n    boxes_from_polys = np.zeros((len(polys), 4), dtype=np.float32)\n    for i in range(len(polys)):\n        poly = polys[i]\n        x0 = min(min(p[::2]) for p in poly)\n        x1 = max(max(p[::2]) for p in poly)\n        y0 = min(min(p[1::2]) for p in poly)\n        y1 = max(max(p[1::2]) for p in poly)\n        boxes_from_polys[i, :] = [x0, y0, x1, y1]\n\n    return boxes_from_polys\n\n\ndef rle_mask_voting(\n    top_masks, all_masks, all_dets, iou_thresh, binarize_thresh, method=\'AVG\'\n):\n    """"""Returns new masks (in correspondence with `top_masks`) by combining\n    multiple overlapping masks coming from the pool of `all_masks`. Two methods\n    for combining masks are supported: \'AVG\' uses a weighted average of\n    overlapping mask pixels; \'UNION\' takes the union of all mask pixels.\n    """"""\n    if len(top_masks) == 0:\n        return\n\n    all_not_crowd = [False] * len(all_masks)\n    top_to_all_overlaps = mask_util.iou(top_masks, all_masks, all_not_crowd)\n    decoded_all_masks = [\n        np.array(mask_util.decode(rle), dtype=np.float32) for rle in all_masks\n    ]\n    decoded_top_masks = [\n        np.array(mask_util.decode(rle), dtype=np.float32) for rle in top_masks\n    ]\n    all_boxes = all_dets[:, :4].astype(np.int32)\n    all_scores = all_dets[:, 4]\n\n    # Fill box support with weights\n    mask_shape = decoded_all_masks[0].shape\n    mask_weights = np.zeros((len(all_masks), mask_shape[0], mask_shape[1]))\n    for k in range(len(all_masks)):\n        ref_box = all_boxes[k]\n        x_0 = max(ref_box[0], 0)\n        x_1 = min(ref_box[2] + 1, mask_shape[1])\n        y_0 = max(ref_box[1], 0)\n        y_1 = min(ref_box[3] + 1, mask_shape[0])\n        mask_weights[k, y_0:y_1, x_0:x_1] = all_scores[k]\n    mask_weights = np.maximum(mask_weights, 1e-5)\n\n    top_segms_out = []\n    for k in range(len(top_masks)):\n        # Corner case of empty mask\n        if decoded_top_masks[k].sum() == 0:\n            top_segms_out.append(top_masks[k])\n            continue\n\n        inds_to_vote = np.where(top_to_all_overlaps[k] >= iou_thresh)[0]\n        # Only matches itself\n        if len(inds_to_vote) == 1:\n            top_segms_out.append(top_masks[k])\n            continue\n\n        masks_to_vote = [decoded_all_masks[i] for i in inds_to_vote]\n        if method == \'AVG\':\n            ws = mask_weights[inds_to_vote]\n            soft_mask = np.average(masks_to_vote, axis=0, weights=ws)\n            mask = np.array(soft_mask > binarize_thresh, dtype=np.uint8)\n        elif method == \'UNION\':\n            # Any pixel that\'s on joins the mask\n            soft_mask = np.sum(masks_to_vote, axis=0)\n            mask = np.array(soft_mask > 1e-5, dtype=np.uint8)\n        else:\n            raise NotImplementedError(\'Method {} is unknown\'.format(method))\n        rle = mask_util.encode(np.array(mask[:, :, np.newaxis], order=\'F\'))[0]\n        top_segms_out.append(rle)\n\n    return top_segms_out\n\n\ndef rle_mask_nms(masks, dets, thresh, mode=\'IOU\'):\n    """"""Performs greedy non-maximum suppression based on an overlap measurement\n    between masks. The type of measurement is determined by `mode` and can be\n    either \'IOU\' (standard intersection over union) or \'IOMA\' (intersection over\n    mininum area).\n    """"""\n    if len(masks) == 0:\n        return []\n    if len(masks) == 1:\n        return [0]\n\n    if mode == \'IOU\':\n        # Computes ious[m1, m2] = area(intersect(m1, m2)) / area(union(m1, m2))\n        all_not_crowds = [False] * len(masks)\n        ious = mask_util.iou(masks, masks, all_not_crowds)\n    elif mode == \'IOMA\':\n        # Computes ious[m1, m2] = area(intersect(m1, m2)) / min(area(m1), area(m2))\n        all_crowds = [True] * len(masks)\n        # ious[m1, m2] = area(intersect(m1, m2)) / area(m2)\n        ious = mask_util.iou(masks, masks, all_crowds)\n        # ... = max(area(intersect(m1, m2)) / area(m2),\n        #           area(intersect(m2, m1)) / area(m1))\n        ious = np.maximum(ious, ious.transpose())\n    elif mode == \'CONTAINMENT\':\n        # Computes ious[m1, m2] = area(intersect(m1, m2)) / area(m2)\n        # Which measures how much m2 is contained inside m1\n        all_crowds = [True] * len(masks)\n        ious = mask_util.iou(masks, masks, all_crowds)\n    else:\n        raise NotImplementedError(\'Mode {} is unknown\'.format(mode))\n\n    scores = dets[:, 4]\n    order = np.argsort(-scores)\n\n    keep = []\n    while order.size > 0:\n        i = order[0]\n        keep.append(i)\n        ovr = ious[i, order[1:]]\n        inds_to_keep = np.where(ovr <= thresh)[0]\n        order = order[inds_to_keep + 1]\n\n    return keep\n\n\ndef rle_masks_to_boxes(masks):\n    """"""Computes the bounding box of each mask in a list of RLE encoded masks.""""""\n    if len(masks) == 0:\n        return []\n\n    decoded_masks = [\n        np.array(mask_util.decode(rle), dtype=np.float32) for rle in masks\n    ]\n\n    def get_bounds(flat_mask):\n        inds = np.where(flat_mask > 0)[0]\n        return inds.min(), inds.max()\n\n    boxes = np.zeros((len(decoded_masks), 4))\n    keep = [True] * len(decoded_masks)\n    for i, mask in enumerate(decoded_masks):\n        if mask.sum() == 0:\n            keep[i] = False\n            continue\n        flat_mask = mask.sum(axis=0)\n        x0, x1 = get_bounds(flat_mask)\n        flat_mask = mask.sum(axis=1)\n        y0, y1 = get_bounds(flat_mask)\n        boxes[i, :] = (x0, y0, x1, y1)\n\n    return boxes, np.where(keep)[0]\n'"
lib/utils/selective_search.py,0,"b""import numpy as np\nimport cv2\n \ndef selective_search(pil_image=None,quality='f',size=800):\n    # speed-up using multithreads\n    cv2.setUseOptimized(True);\n    cv2.setNumThreads(4);\n\n    # resize image to limit number of proposals and to bypass a bug in OpenCV with non-square images\n    w,h = pil_image.size\n    h_factor,w_factor=h/size,w/size\n    pil_image=pil_image.resize((size,size))\n\n    im = cv2.cvtColor(np.array(pil_image), cv2.COLOR_RGB2BGR)        \n \n    # create Selective Search Segmentation Object using default parameters\n    ss = cv2.ximgproc.segmentation.createSelectiveSearchSegmentation()\n \n    # set input image on which we will run segmentation\n    ss.setBaseImage(im)\n \n    # Switch to fast but low recall Selective Search method\n    if (quality == 'f'):\n        ss.switchToSelectiveSearchFast()\n     # Switch to high recall but slow Selective Search method\n    elif (quality == 'q'):\n        ss.switchToSelectiveSearchQuality()\n\n    # run selective search segmentation on input image\n    rects = ss.process()\n\n    # rect is in x,y,w,h format\n    # convert to xmin,ymin,xmax,ymax format\n    rects = np.vstack((rects[:,0]*w_factor, rects[:,1]*h_factor, (rects[:,0]+rects[:,2])*w_factor, (rects[:,1]+rects[:,3])*h_factor)).transpose()\n    \n    return rects"""
lib/utils/solver.py,0,"b'def adjust_learning_rate(optimizer, lr):\n    for param_group in optimizer.param_groups:\n        param_group[\'lr\'] = lr\n\n\ndef get_step_index(cur_iter,lr_steps=[0, 240000, 320000],max_iter=360000):\n    """"""Given an iteration, find which learning rate step we\'re at.""""""\n    assert lr_steps[0] == 0, \'The first step should always start at 0.\'\n    steps = lr_steps + [max_iter]\n    for ind, step in enumerate(steps):  # NoQA\n        if cur_iter < step:\n            break\n    return ind - 1\n\n\ndef lr_func_steps_with_decay(cur_iter,base_lr=0.01,gamma=0.1):\n    """"""For cfg.SOLVER.LR_POLICY = \'steps_with_decay\'\n    Change the learning rate specified iterations based on the formula\n    lr = base_lr * gamma ** lr_step_count.\n    Example:\n    cfg.SOLVER.MAX_ITER: 90\n    cfg.SOLVER.STEPS:    [0,    60,    80]\n    cfg.SOLVER.BASE_LR:  0.02\n    cfg.SOLVER.GAMMA:    0.1\n    for cur_iter in [0, 59]   use 0.02 = 0.02 * 0.1 ** 0\n                 in [60, 79]  use 0.002 = 0.02 * 0.1 ** 1\n                 in [80, inf] use 0.0002 = 0.02 * 0.1 ** 2\n    """"""\n    ind = get_step_index(cur_iter)\n    return base_lr * gamma ** ind\n\ndef get_lr_at_iter(it,warm_up_iters=500,warm_up_factor=0.3333333333333333,warm_up_method=\'linear\'):\n    """"""Get the learning rate at iteration it according to the cfg.SOLVER\n    settings.\n    """"""\n    lr = lr_func_steps_with_decay(it)\n    if it < warm_up_iters:\n        if warm_up_method == \'linear\':\n            alpha = it / warm_up_iters\n            warm_up_factor = warm_up_factor * (1 - alpha) + alpha\n        elif warm_up_method != \'constant\':\n            raise KeyError(\'Unknown WARM_UP_METHOD: {}\'.format(warm_up_method))\n        lr *= warm_up_factor\n    return lr'"
lib/utils/timer.py,0,"b'# Copyright (c) 2017-present, Facebook, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n##############################################################################\n#\n# Based on:\n# --------------------------------------------------------\n# Fast R-CNN\n# Copyright (c) 2015 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ross Girshick\n# --------------------------------------------------------\n\n""""""Timing related functions.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport time\n\n\nclass Timer(object):\n    """"""A simple timer.""""""\n\n    def __init__(self):\n        self.reset()\n\n    def tic(self):\n        # using time.time instead of time.clock because time time.clock\n        # does not normalize for multithreading\n        self.start_time = time.time()\n\n    def toc(self, average=True):\n        self.diff = time.time() - self.start_time\n        self.total_time += self.diff\n        self.calls += 1\n        self.average_time = self.total_time / self.calls\n        if average:\n            return self.average_time\n        else:\n            return self.diff\n\n    def reset(self):\n        self.total_time = 0.\n        self.calls = 0\n        self.start_time = 0.\n        self.diff = 0.\n        self.average_time = 0.\n'"
lib/utils/training_stats.py,0,"b'#!/usr/bin/env python2\n\n# Copyright (c) 2017-present, Facebook, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n##############################################################################\n\n""""""Utilities for training.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport datetime\nimport numpy as np\n\n#from caffe2.python import utils as c2_py_utils\n#from core.config import cfg\nfrom utils.logging import log_json_stats\nfrom utils.logging import SmoothedValue\nfrom utils.timer import Timer\n\n\nclass TrainingStats(object):\n    """"""Track vital training statistics.""""""\n\n    def __init__(self, metrics, losses,\n                 solver_max_iters):\n        self.solver_max_iters = solver_max_iters\n        # Window size for smoothing tracked values (with median filtering)\n        self.win_sz = 20\n        # Output logging period in SGD iterations\n        self.log_period = 20\n        self.smoothed_losses_and_metrics = {\n            key: SmoothedValue(self.win_sz)\n            for key in losses + metrics\n        }\n        self.losses_and_metrics = {\n            key: 0\n            for key in losses + metrics\n        }\n        self.smoothed_total_loss = SmoothedValue(self.win_sz)\n        self.smoothed_mb_qsize = SmoothedValue(self.win_sz)\n        self.iter_total_loss = np.nan\n        self.iter_timer = Timer()\n        self.metrics = metrics\n        self.losses = losses\n\n    def IterTic(self):\n        self.iter_timer.tic()\n\n    def IterToc(self):\n        return self.iter_timer.toc(average=False)\n\n    def ResetIterTimer(self):\n        self.iter_timer.reset()\n\n    def UpdateIterStats(self,losses_dict, metrics_dict):\n        """"""Update tracked iteration statistics.""""""\n        for k in self.losses_and_metrics.keys():\n            if k in self.losses: # if loss\n                self.losses_and_metrics[k] = losses_dict[k]\n            else: # if metric\n                self.losses_and_metrics[k] = metrics_dict[k]\n\n        for k, v in self.smoothed_losses_and_metrics.items():\n            v.AddValue(self.losses_and_metrics[k])\n        #import pdb; pdb.set_trace()\n        self.iter_total_loss = np.sum(\n            np.array([self.losses_and_metrics[k] for k in self.losses])\n        )\n        self.smoothed_total_loss.AddValue(self.iter_total_loss)\n        self.smoothed_mb_qsize.AddValue(\n            #self.model.roi_data_loader._minibatch_queue.qsize()\n            64\n        )\n\n    def LogIterStats(self, cur_iter, lr):\n        """"""Log the tracked statistics.""""""\n        if (cur_iter % self.log_period == 0 or\n                cur_iter == self.solver_max_iters - 1):\n            stats = self.GetStats(cur_iter, lr)\n            log_json_stats(stats)\n\n    def GetStats(self, cur_iter, lr):\n        eta_seconds = self.iter_timer.average_time * (\n            self.solver_max_iters - cur_iter\n        )\n        eta = str(datetime.timedelta(seconds=int(eta_seconds)))\n        #mem_stats = c2_py_utils.GetGPUMemoryUsageStats()\n        #mem_usage = np.max(mem_stats[\'max_by_gpu\'][:cfg.NUM_GPUS])\n        stats = dict(\n            iter=cur_iter,\n            lr=""{:.6f}"".format(float(lr)),\n            time=""{:.6f}"".format(self.iter_timer.average_time),\n            loss=""{:.6f}"".format(self.smoothed_total_loss.GetMedianValue()),\n            eta=eta,\n            #mb_qsize=int(np.round(self.smoothed_mb_qsize.GetMedianValue())),\n            #mem=int(np.ceil(mem_usage / 1024 / 1024))\n        )\n        for k, v in self.smoothed_losses_and_metrics.items():\n            stats[k] = ""{:.6f}"".format(v.GetMedianValue())\n        return stats'"
lib/utils/utils.py,3,"b'import torch\nfrom torch.autograd import Variable\n\ndef normalize_axis(x,L):\n    return (x-1-(L-1)/2)*2/(L-1)\n\ndef unnormalize_axis(x,L):\n    return x*(L-1)/2+1+(L-1)/2\n\ndef expand_dim(tensor,dim,desired_dim_len):\n    sz = list(tensor.size())\n    sz[dim]=desired_dim_len\n    return tensor.expand(tuple(sz))\n\ndef create_file_path(filename):\n    if not os.path.exists(os.path.dirname(filename)):\n        try:\n            os.makedirs(os.path.dirname(filename))\n        except OSError as exc: # Guard against race condition\n            if exc.errno != errno.EEXIST:\n                raise\n\ndef to_cuda(x):\n    if isinstance(x,dict):\n        return {key: to_cuda(x[key]) for key in x.keys()}\n    if isinstance(x,list):\n        return [y.cuda() for y in x]\n    return x.cuda()\n\ndef to_cuda_variable(x,volatile=True):\n    if isinstance(x,dict):\n        return {key: to_cuda_variable(x[key],volatile=volatile) for key in x.keys()}\n    if isinstance(x,list):\n        return [to_cuda_variable(y) for y in x]\n    if isinstance(x, (int, float)):\n        return x\n    if isinstance(x, torch.Tensor):\n        if torch.__version__[:3]==""0.4"" or volatile==False:\n            return Variable(x.cuda())\n        else:\n            return Variable(x.cuda(),volatile=True)\n\n\ndef parse_th_to_caffe2(terms,i=0,parsed=\'\'):\n    # Convert PyTorch ResNet weight names to caffe2 weight names\n    if i==0:\n        if terms[i]==\'conv1\':\n            parsed=\'conv1\'\n        elif terms[i]==\'bn1\':\n            parsed=\'res_conv1\'\n        elif terms[i].startswith(\'layer\'):\n            parsed=\'res\'+str(int(terms[i][-1])+1)\n    else:\n        if terms[i]==\'weight\' and (terms[i-1].startswith(\'conv\') or terms[i-1]==\'0\'):\n            parsed+=\'_w\'\n        elif terms[i]==\'weight\' and (terms[i-1].startswith(\'bn\') or terms[i-1]==\'1\'):\n            parsed+=\'_bn_s\'\n        elif terms[i]==\'bias\' and (terms[i-1].startswith(\'bn\') or terms[i-1]==\'1\'):\n            parsed+=\'_bn_b\'\n        elif terms[i-1].startswith(\'layer\'):\n            parsed+=\'_\'+terms[i]\n        elif terms[i].startswith(\'conv\') or terms[i].startswith(\'bn\'):\n            parsed+=\'_branch2\'+chr(96+int(terms[i][-1]))\n        elif terms[i]==\'downsample\':\n            parsed+=\'_branch1\'\n    # increase counter\n    i+=1\n    # do recursion\n    if i==len(terms):\n        return parsed\n    return parse_th_to_caffe2(terms,i,parsed)\n'"
lib/utils/vis.py,0,"b'# Copyright (c) 2017-present, Facebook, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n##############################################################################\n\n""""""Detection output visualization module.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport cv2\nimport numpy as np\nimport os\n\nimport pycocotools.mask as mask_util\n\nfrom utils.colormap import colormap\n# import utils.keypoints as keypoint_utils\n\n# Matplotlib requires certain adjustments in some environments\n# Must happen before importing matplotlib\nimport matplotlib\n# matplotlib.use(\'Agg\') # Use a non-interactive backend\nimport matplotlib.pyplot as plt\nfrom matplotlib.patches import Polygon\n\nplt.rcParams[\'pdf.fonttype\'] = 42  # For editing in Adobe Illustrator\n\n\n_GRAY = (218, 227, 218)\n_GREEN = (18, 127, 15)\n_WHITE = (255, 255, 255)\n\n\n# def kp_connections(keypoints):\n#     kp_lines = [\n#         [keypoints.index(\'left_eye\'), keypoints.index(\'right_eye\')],\n#         [keypoints.index(\'left_eye\'), keypoints.index(\'nose\')],\n#         [keypoints.index(\'right_eye\'), keypoints.index(\'nose\')],\n#         [keypoints.index(\'right_eye\'), keypoints.index(\'right_ear\')],\n#         [keypoints.index(\'left_eye\'), keypoints.index(\'left_ear\')],\n#         [keypoints.index(\'right_shoulder\'), keypoints.index(\'right_elbow\')],\n#         [keypoints.index(\'right_elbow\'), keypoints.index(\'right_wrist\')],\n#         [keypoints.index(\'left_shoulder\'), keypoints.index(\'left_elbow\')],\n#         [keypoints.index(\'left_elbow\'), keypoints.index(\'left_wrist\')],\n#         [keypoints.index(\'right_hip\'), keypoints.index(\'right_knee\')],\n#         [keypoints.index(\'right_knee\'), keypoints.index(\'right_ankle\')],\n#         [keypoints.index(\'left_hip\'), keypoints.index(\'left_knee\')],\n#         [keypoints.index(\'left_knee\'), keypoints.index(\'left_ankle\')],\n#         [keypoints.index(\'right_shoulder\'), keypoints.index(\'left_shoulder\')],\n#         [keypoints.index(\'right_hip\'), keypoints.index(\'left_hip\')],\n#     ]\n#     return kp_lines\n\n\ndef convert_from_cls_format(cls_boxes, cls_segms, cls_keyps):\n    """"""Convert from the class boxes/segms/keyps format generated by the testing\n    code.\n    """"""\n    box_list = [b for b in cls_boxes if len(b) > 0]\n    if len(box_list) > 0:\n        boxes = np.concatenate(box_list)\n    else:\n        boxes = None\n    if cls_segms is not None:\n        segms = [s for slist in cls_segms for s in slist]\n    else:\n        segms = None\n    if cls_keyps is not None:\n        keyps = [k for klist in cls_keyps for k in klist]\n    else:\n        keyps = None\n    classes = []\n    for j in range(len(cls_boxes)):\n        classes += [j] * len(cls_boxes[j])\n    return boxes, segms, keyps, classes\n\n\ndef get_class_string(class_index, score, dataset):\n    class_text = dataset.classes[class_index] if dataset is not None else \\\n        \'id{:d}\'.format(class_index)\n    return class_text + \' {:0.2f}\'.format(score).lstrip(\'0\')\n\n\ndef vis_mask(img, mask, col, alpha=0.4, show_border=True, border_thick=1):\n    """"""Visualizes a single binary mask.""""""\n\n    img = img.astype(np.float32)\n    idx = np.nonzero(mask)\n\n    img[idx[0], idx[1], :] *= 1.0 - alpha\n    img[idx[0], idx[1], :] += alpha * col\n\n    if show_border:\n        _, contours, _ = cv2.findContours(\n            mask.copy(), cv2.RETR_CCOMP, cv2.CHAIN_APPROX_NONE)\n        cv2.drawContours(img, contours, -1, _WHITE, border_thick, cv2.LINE_AA)\n\n    return img.astype(np.uint8)\n\n\ndef vis_class(img, pos, class_str, font_scale=0.35):\n    """"""Visualizes the class.""""""\n    x0, y0 = int(pos[0]), int(pos[1])\n    # Compute text size.\n    txt = class_str\n    font = cv2.FONT_HERSHEY_SIMPLEX\n    ((txt_w, txt_h), _) = cv2.getTextSize(txt, font, font_scale, 1)\n    # Place text background.\n    back_tl = x0, y0 - int(1.3 * txt_h)\n    back_br = x0 + txt_w, y0\n    cv2.rectangle(img, back_tl, back_br, _GREEN, -1)\n    # Show text.\n    txt_tl = x0, y0 - int(0.3 * txt_h)\n    cv2.putText(img, txt, txt_tl, font, font_scale, _GRAY, lineType=cv2.LINE_AA)\n    return img\n\n\ndef vis_bbox(img, bbox, thick=1):\n    """"""Visualizes a bounding box.""""""\n    (x0, y0, w, h) = bbox\n    x1, y1 = int(x0 + w), int(y0 + h)\n    x0, y0 = int(x0), int(y0)\n    cv2.rectangle(img, (x0, y0), (x1, y1), _GREEN, thickness=thick)\n    return img\n\n\n# def vis_keypoints(img, kps, kp_thresh=2, alpha=0.7):\n#     """"""Visualizes keypoints (adapted from vis_one_image).\n#     kps has shape (4, #keypoints) where 4 rows are (x, y, logit, prob).\n#     """"""\n#     dataset_keypoints, _ = keypoint_utils.get_keypoints()\n#     kp_lines = kp_connections(dataset_keypoints)\n\n#     # Convert from plt 0-1 RGBA colors to 0-255 BGR colors for opencv.\n#     cmap = plt.get_cmap(\'rainbow\')\n#     colors = [cmap(i) for i in np.linspace(0, 1, len(kp_lines) + 2)]\n#     colors = [(c[2] * 255, c[1] * 255, c[0] * 255) for c in colors]\n\n#     # Perform the drawing on a copy of the image, to allow for blending.\n#     kp_mask = np.copy(img)\n\n#     # Draw mid shoulder / mid hip first for better visualization.\n#     mid_shoulder = (\n#         kps[:2, dataset_keypoints.index(\'right_shoulder\')] +\n#         kps[:2, dataset_keypoints.index(\'left_shoulder\')]) / 2.0\n#     sc_mid_shoulder = np.minimum(\n#         kps[2, dataset_keypoints.index(\'right_shoulder\')],\n#         kps[2, dataset_keypoints.index(\'left_shoulder\')])\n#     mid_hip = (\n#         kps[:2, dataset_keypoints.index(\'right_hip\')] +\n#         kps[:2, dataset_keypoints.index(\'left_hip\')]) / 2.0\n#     sc_mid_hip = np.minimum(\n#         kps[2, dataset_keypoints.index(\'right_hip\')],\n#         kps[2, dataset_keypoints.index(\'left_hip\')])\n#     nose_idx = dataset_keypoints.index(\'nose\')\n#     if sc_mid_shoulder > kp_thresh and kps[2, nose_idx] > kp_thresh:\n#         cv2.line(\n#             kp_mask, tuple(mid_shoulder), tuple(kps[:2, nose_idx]),\n#             color=colors[len(kp_lines)], thickness=2, lineType=cv2.LINE_AA)\n#     if sc_mid_shoulder > kp_thresh and sc_mid_hip > kp_thresh:\n#         cv2.line(\n#             kp_mask, tuple(mid_shoulder), tuple(mid_hip),\n#             color=colors[len(kp_lines) + 1], thickness=2, lineType=cv2.LINE_AA)\n\n#     # Draw the keypoints.\n#     for l in range(len(kp_lines)):\n#         i1 = kp_lines[l][0]\n#         i2 = kp_lines[l][1]\n#         p1 = kps[0, i1], kps[1, i1]\n#         p2 = kps[0, i2], kps[1, i2]\n#         if kps[2, i1] > kp_thresh and kps[2, i2] > kp_thresh:\n#             cv2.line(\n#                 kp_mask, p1, p2,\n#                 color=colors[l], thickness=2, lineType=cv2.LINE_AA)\n#         if kps[2, i1] > kp_thresh:\n#             cv2.circle(\n#                 kp_mask, p1,\n#                 radius=3, color=colors[l], thickness=-1, lineType=cv2.LINE_AA)\n#         if kps[2, i2] > kp_thresh:\n#             cv2.circle(\n#                 kp_mask, p2,\n#                 radius=3, color=colors[l], thickness=-1, lineType=cv2.LINE_AA)\n\n#     # Blend the keypoints.\n#     return cv2.addWeighted(img, 1.0 - alpha, kp_mask, alpha, 0)\n\n\ndef vis_one_image_opencv(\n        im, boxes, segms=None, keypoints=None, thresh=0.9, kp_thresh=2,\n        show_box=False, dataset=None, show_class=False):\n    """"""Constructs a numpy array with the detections visualized.""""""\n\n    if isinstance(boxes, list):\n        boxes, segms, keypoints, classes = convert_from_cls_format(\n            boxes, segms, keypoints)\n\n    if boxes is None or boxes.shape[0] == 0 or max(boxes[:, 4]) < thresh:\n        return im\n\n    if segms is not None:\n        masks = mask_util.decode(segms)\n        color_list = colormap()\n        mask_color_id = 0\n\n    # Display in largest to smallest order to reduce occlusion\n    areas = (boxes[:, 2] - boxes[:, 0]) * (boxes[:, 3] - boxes[:, 1])\n    sorted_inds = np.argsort(-areas)\n\n    for i in sorted_inds:\n        bbox = boxes[i, :4]\n        score = boxes[i, -1]\n        if score < thresh:\n            continue\n\n        # show box (off by default)\n        if show_box:\n            im = vis_bbox(\n                im, (bbox[0], bbox[1], bbox[2] - bbox[0], bbox[3] - bbox[1]))\n\n        # show class (off by default)\n        if show_class:\n            class_str = get_class_string(classes[i], score, dataset)\n            im = vis_class(im, (bbox[0], bbox[1] - 2), class_str)\n\n        # show mask\n        if segms is not None and len(segms) > i:\n            color_mask = color_list[mask_color_id % len(color_list), 0:3]\n            mask_color_id += 1\n            im = vis_mask(im, masks[..., i], color_mask)\n\n        # # show keypoints\n        # if keypoints is not None and len(keypoints) > i:\n        #     im = vis_keypoints(im, keypoints[i], kp_thresh)\n\n    return im\n\n\ndef vis_one_image(\n        im, im_name, output_dir, boxes, segms=None, keypoints=None, thresh=0.9,\n        kp_thresh=2, dpi=200, box_alpha=0.0, dataset=None, show_class=False,\n        ext=\'pdf\', show=False):\n    """"""Visual debugging of detections.""""""\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n\n    if isinstance(boxes, list):\n        boxes, segms, keypoints, classes = convert_from_cls_format(\n            boxes, segms, keypoints)\n\n    if boxes is None or boxes.shape[0] == 0 or max(boxes[:, 4]) < thresh:\n        return\n\n    # dataset_keypoints, _ = keypoint_utils.get_keypoints()\n\n    if segms is not None:\n        masks = mask_util.decode(segms)\n\n    color_list = colormap(rgb=True) / 255\n\n    # kp_lines = kp_connections(dataset_keypoints)\n    # cmap = plt.get_cmap(\'rainbow\')\n    # colors = [cmap(i) for i in np.linspace(0, 1, len(kp_lines) + 2)]\n\n    fig = plt.figure(frameon=False)\n    fig.set_size_inches(im.shape[1] / dpi, im.shape[0] / dpi)\n    ax = plt.Axes(fig, [0., 0., 1., 1.])\n    ax.axis(\'off\')\n    fig.add_axes(ax)\n    ax.imshow(im)\n\n    # Display in largest to smallest order to reduce occlusion\n    areas = (boxes[:, 2] - boxes[:, 0]) * (boxes[:, 3] - boxes[:, 1])\n    sorted_inds = np.argsort(-areas)\n\n    mask_color_id = 0\n    for i in sorted_inds:\n        bbox = boxes[i, :4]\n        score = boxes[i, -1]\n        if score < thresh:\n            continue\n\n        # show box (off by default)\n        ax.add_patch(\n            plt.Rectangle((bbox[0], bbox[1]),\n                          bbox[2] - bbox[0],\n                          bbox[3] - bbox[1],\n                          fill=False, edgecolor=\'g\',\n                          linewidth=0.5, alpha=box_alpha))\n\n        if show_class:\n            ax.text(\n                bbox[0], bbox[1] - 2,\n                get_class_string(classes[i], score, dataset),\n                fontsize=3,\n                family=\'serif\',\n                bbox=dict(\n                    facecolor=\'g\', alpha=0.4, pad=0, edgecolor=\'none\'),\n                color=\'white\')\n\n        # show mask\n        if segms is not None and len(segms) > i:\n            img = np.ones(im.shape)\n            color_mask = color_list[mask_color_id % len(color_list), 0:3]\n            mask_color_id += 1\n\n            w_ratio = .4\n            for c in range(3):\n                color_mask[c] = color_mask[c] * (1 - w_ratio) + w_ratio\n            for c in range(3):\n                img[:, :, c] = color_mask[c]\n            e = masks[:, :, i]\n\n            _, contour, hier = cv2.findContours(\n                e.copy(), cv2.RETR_CCOMP, cv2.CHAIN_APPROX_NONE)\n\n            for c in contour:\n                polygon = Polygon(\n                    c.reshape((-1, 2)),\n                    fill=True, facecolor=color_mask,\n                    edgecolor=\'w\', linewidth=1.2,\n                    alpha=0.5)\n                ax.add_patch(polygon)\n\n        # # show keypoints\n        # if keypoints is not None and len(keypoints) > i:\n        #     kps = keypoints[i]\n        #     plt.autoscale(False)\n        #     for l in range(len(kp_lines)):\n        #         i1 = kp_lines[l][0]\n        #         i2 = kp_lines[l][1]\n        #         if kps[2, i1] > kp_thresh and kps[2, i2] > kp_thresh:\n        #             x = [kps[0, i1], kps[0, i2]]\n        #             y = [kps[1, i1], kps[1, i2]]\n        #             line = plt.plot(x, y)\n        #             plt.setp(line, color=colors[l], linewidth=1.0, alpha=0.7)\n        #         if kps[2, i1] > kp_thresh:\n        #             plt.plot(\n        #                 kps[0, i1], kps[1, i1], \'.\', color=colors[l],\n        #                 markersize=3.0, alpha=0.7)\n\n        #         if kps[2, i2] > kp_thresh:\n        #             plt.plot(\n        #                 kps[0, i2], kps[1, i2], \'.\', color=colors[l],\n        #                 markersize=3.0, alpha=0.7)\n\n        #     # add mid shoulder / mid hip for better visualization\n        #     mid_shoulder = (\n        #         kps[:2, dataset_keypoints.index(\'right_shoulder\')] +\n        #         kps[:2, dataset_keypoints.index(\'left_shoulder\')]) / 2.0\n        #     sc_mid_shoulder = np.minimum(\n        #         kps[2, dataset_keypoints.index(\'right_shoulder\')],\n        #         kps[2, dataset_keypoints.index(\'left_shoulder\')])\n        #     mid_hip = (\n        #         kps[:2, dataset_keypoints.index(\'right_hip\')] +\n        #         kps[:2, dataset_keypoints.index(\'left_hip\')]) / 2.0\n        #     sc_mid_hip = np.minimum(\n        #         kps[2, dataset_keypoints.index(\'right_hip\')],\n        #         kps[2, dataset_keypoints.index(\'left_hip\')])\n        #     if (sc_mid_shoulder > kp_thresh and\n        #             kps[2, dataset_keypoints.index(\'nose\')] > kp_thresh):\n        #         x = [mid_shoulder[0], kps[0, dataset_keypoints.index(\'nose\')]]\n        #         y = [mid_shoulder[1], kps[1, dataset_keypoints.index(\'nose\')]]\n        #         line = plt.plot(x, y)\n        #         plt.setp(\n        #             line, color=colors[len(kp_lines)], linewidth=1.0, alpha=0.7)\n        #     if sc_mid_shoulder > kp_thresh and sc_mid_hip > kp_thresh:\n        #         x = [mid_shoulder[0], mid_hip[0]]\n        #         y = [mid_shoulder[1], mid_hip[1]]\n        #         line = plt.plot(x, y)\n        #         plt.setp(\n        #             line, color=colors[len(kp_lines) + 1], linewidth=1.0,\n        #             alpha=0.7)\n\n    output_name = os.path.basename(im_name) + \'.\' + ext\n    fig.savefig(os.path.join(output_dir, \'{}\'.format(output_name)), dpi=dpi)\n    print(\'result saved to {}\'.format(os.path.join(output_dir, \'{}\'.format(output_name))))\n    if show:\n        plt.show()\n    plt.close(\'all\')\n'"
lib/utils_cython/build_cython.py,0,"b'# Copyright (c) 2017-present, Facebook, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n##############################################################################\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom Cython.Build import cythonize\nfrom setuptools import Extension\nfrom setuptools import setup\n\nimport numpy as np\n\n_NP_INCLUDE_DIRS = np.get_include()\n\n\n# Extension modules\next_modules = [\n    Extension(\n        name=\'cython_bbox\',\n        sources=[\n            \'cython_bbox.pyx\'\n        ],\n        extra_compile_args=[\n            \'-Wno-cpp\'\n        ],\n        include_dirs=[\n            _NP_INCLUDE_DIRS\n        ]\n    ),\n    Extension(\n        name=\'cython_nms\',\n        sources=[\n            \'cython_nms.pyx\'\n        ],\n        extra_compile_args=[\n            \'-Wno-cpp\'\n        ],\n        include_dirs=[\n            _NP_INCLUDE_DIRS\n        ]\n    )\n]\n\nsetup(\n    name=\'Detectron\',\n    ext_modules=cythonize(ext_modules)\n)'"
