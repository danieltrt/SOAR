file_path,api_count,code
__init__.py,0,b''
config.py,0,"b'# -*- coding: utf-8 -*-\n\nGPU_ID = 0\nTRAIN_BATCH_SIZE = 32\nTEST_BATCH_SIZE = 32\nTRIPLET_BATCH_SIZE = 32\nEXTRACT_BATCH_SIZE = 128\nTEST_BATCH_COUNT = 30\nNUM_WORKERS = 4\nLR = 0.001\nMOMENTUM = 0.5\nEPOCH = 10\nDUMPED_MODEL = ""model_10_final.pth.tar""\n\nLOG_INTERVAL = 10\nDUMP_INTERVAL = 500\nTEST_INTERVAL = 100\n\nDATASET_BASE = r\'/DATACETNER/1/ch/deepfashion_data\'\nENABLE_INSHOP_DATASET = True\nINSHOP_DATASET_PRECENT = 0.8\nIMG_SIZE = 256\nCROP_SIZE = 224\nINTER_DIM = 512\nCATEGORIES = 20\nN_CLUSTERS = 50\nCOLOR_TOP_N = 10\nTRIPLET_WEIGHT = 2.0\nENABLE_TRIPLET_WITH_COSINE = False  # Buggy when backward...\nCOLOR_WEIGHT = 0.1\nDISTANCE_METRIC = (\'euclidean\', \'euclidean\')\nFREEZE_PARAM = False\n'"
data.py,1,"b'# -*- coding: utf-8 -*-\n\nimport torch.utils.data as data\nimport torch\nfrom config import *\nimport os\nfrom PIL import Image\nimport random\n\n\nclass Fashion_attr_prediction(data.Dataset):\n    def __init__(self, type=""train"", transform=None, target_transform=None, crop=False, img_path=None):\n        self.transform = transform\n        self.target_transform = target_transform\n        self.crop = crop\n        # type_all = [""train"", ""test"", ""all"", ""triplet"", ""single""]\n        self.type = type\n        if type == ""single"":\n            self.img_path = img_path\n            return\n        self.train_list = []\n        self.train_dict = {i: [] for i in range(CATEGORIES)}\n        self.test_list = []\n        self.all_list = []\n        self.bbox = dict()\n        self.anno = dict()\n        self.read_partition_category()\n        self.read_bbox()\n\n    def __len__(self):\n        if self.type == ""all"":\n            return len(self.all_list)\n        elif self.type == ""train"":\n            return len(self.train_list)\n        elif self.type == ""test"":\n            return len(self.test_list)\n        else:\n            return 1\n\n    def read_partition_category(self):\n        list_eval_partition = os.path.join(DATASET_BASE, r\'Eval\', r\'list_eval_partition.txt\')\n        list_category_img = os.path.join(DATASET_BASE, r\'Anno\', r\'list_category_img.txt\')\n        partition_pairs = self.read_lines(list_eval_partition)\n        category_img_pairs = self.read_lines(list_category_img)\n        for k, v in category_img_pairs:\n            v = int(v)\n            if v <= 20:\n                self.anno[k] = v - 1\n        for k, v in partition_pairs:\n            if k in self.anno:\n                if v == ""train"":\n                    self.train_list.append(k)\n                    self.train_dict[self.anno[k]].append(k)\n                else:\n                    # Test and Val\n                    self.test_list.append(k)\n        self.all_list = self.test_list + self.train_list\n        random.shuffle(self.train_list)\n        random.shuffle(self.test_list)\n        random.shuffle(self.all_list)\n\n    def read_bbox(self):\n        list_bbox = os.path.join(DATASET_BASE, r\'Anno\', r\'list_bbox.txt\')\n        pairs = self.read_lines(list_bbox)\n        for k, x1, y1, x2, y2 in pairs:\n            self.bbox[k] = [x1, y1, x2, y2]\n\n    def read_lines(self, path):\n        with open(path) as fin:\n            lines = fin.readlines()[2:]\n            lines = list(filter(lambda x: len(x) > 0, lines))\n            pairs = list(map(lambda x: x.strip().split(), lines))\n        return pairs\n\n    def read_crop(self, img_path):\n        img_full_path = os.path.join(DATASET_BASE, img_path)\n        with open(img_full_path, \'rb\') as f:\n            with Image.open(f) as img:\n                img = img.convert(\'RGB\')\n        if self.crop:\n            x1, y1, x2, y2 = self.bbox[img_path]\n            if x1 < x2 <= img.size[0] and y1 < y2 <= img.size[1]:\n                img = img.crop((x1, y1, x2, y2))\n        return img\n\n    def __getitem__(self, index):\n        if self.type == ""triplet"":\n            img_path = self.train_list[index]\n            target = self.anno[img_path]\n            img_p = random.choice(self.train_dict[target])\n            img_n = random.choice(self.train_dict[random.choice(list(filter(lambda x: x != target, range(20))))])\n            img = self.read_crop(img_path)\n            img_p = self.read_crop(img_p)\n            img_n = self.read_crop(img_n)\n            if self.transform is not None:\n                img = self.transform(img)\n                img_p = self.transform(img_p)\n                img_n = self.transform(img_n)\n            return img, img_p, img_n\n\n        if self.type == ""single"":\n            img_path = self.img_path\n            img = self.read_crop(img_path)\n            if self.transform is not None:\n                img = self.transform(img)\n            return img\n\n        if self.type == ""all"":\n            img_path = self.all_list[index]\n        elif self.type == ""train"":\n            img_path = self.train_list[index]\n        else:\n            img_path = self.test_list[index]\n        target = self.anno[img_path]\n        img = self.read_crop(img_path)\n\n        if self.transform is not None:\n            img = self.transform(img)\n        if self.target_transform is not None:\n            target = self.target_transform(target)\n\n        return img, img_path if self.type == ""all"" else target\n\n\nclass Fashion_inshop(data.Dataset):\n    def __init__(self, type=""train"", transform=None):\n        self.transform = transform\n        self.type = type\n        self.train_dict = {}\n        self.test_dict = {}\n        self.train_list = []\n        self.test_list = []\n        self.all_path = []\n        self.cloth = self.readcloth()\n        self.read_train_test()\n\n    def read_lines(self, path):\n        with open(path) as fin:\n            lines = fin.readlines()[2:]\n            lines = list(filter(lambda x: len(x) > 0, lines))\n            pairs = list(map(lambda x: x.strip().split(), lines))\n        return pairs\n\n    def readcloth(self):\n        lines = self.read_lines(os.path.join(DATASET_BASE, \'in_shop\', \'list_bbox_inshop.txt\'))\n        valid_lines = list(filter(lambda x: x[1] == \'1\', lines))\n        names = set(list(map(lambda x: x[0], valid_lines)))\n        return names\n\n    def read_train_test(self):\n        lines = self.read_lines(os.path.join(DATASET_BASE, \'in_shop\', \'list_eval_partition.txt\'))\n        valid_lines = list(filter(lambda x: x[0] in self.cloth, lines))\n        for line in valid_lines:\n            s = self.train_dict if line[2] == \'train\' else self.test_dict\n            if line[1] not in s:\n                s[line[1]] = [line[0]]\n            else:\n                s[line[1]].append(line[0])\n\n        def clear_single(d):\n            keys_to_delete = []\n            for k, v in d.items():\n                if len(v) < 2:\n                    keys_to_delete.append(k)\n            for k in keys_to_delete:\n                d.pop(k, None)\n        clear_single(self.train_dict)\n        clear_single(self.test_dict)\n        self.train_list, self.test_list = list(self.train_dict.keys()), list(self.test_dict.keys())\n        for v in list(self.train_dict.values()):\n            self.all_path += v\n        self.train_len = len(self.all_path)\n        for v in list(self.test_dict.values()):\n            self.all_path += v\n        self.test_len = len(self.all_path) - self.train_len\n\n    def process_img(self, img_path):\n        img_full_path = os.path.join(DATASET_BASE, \'in_shop\', img_path)\n        with open(img_full_path, \'rb\') as f:\n            with Image.open(f) as img:\n                img = img.convert(\'RGB\')\n        if self.transform is not None:\n            img = self.transform(img)\n        return img\n\n    def __len__(self):\n        if self.type == \'train\':\n            return len(self.train_list)\n        elif self.type == \'test\':\n            return len(self.test_list)\n        else:\n            return len(self.all_path)\n\n    def __getitem__(self, item):\n        if self.type == \'all\':\n            img_path = self.all_path[item]\n            img = self.process_img(img_path)\n            return img, img_path\n        s_d = self.train_dict if self.type == \'train\' else self.test_dict\n        s_l = self.train_list if self.type == \'train\' else self.test_list\n        imgs = s_d[s_l[item]]\n        img_triplet = random.sample(imgs, 2)\n        img_other_id = random.choice(list(range(0, item)) + list(range(item + 1, len(s_l))))\n        img_other = random.choice(s_d[s_l[img_other_id]])\n        img_triplet.append(img_other)\n        return list(map(self.process_img, img_triplet))\n'"
debug.py,4,"b""# -*- coding: utf-8 -*-\n# Debug script, useless...\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torchvision import transforms\nfrom torch.autograd import Variable\nimport torchvision\nfrom config import *\nimport numpy as np\n\n\nfrom scipy import spatial\n\n# dataSetI = [3, 45, 7, 2]\n# dataSetII = [2, 54, 13, 15]\n# result = 1 - spatial.distance.cosine(dataSetI, dataSetII)\n# print(result)\n#\n# from sklearn.metrics.pairwise import cosine_similarity\n# from scipy import sparse\n# import numpy as np\n# import scipy\n#\n# A = np.array([[0, 1, 0, 0, 1]])\n# B = np.array([[0, 2, 0, 0, 2], [0, 4, 0, 0, 2]])\n# result = 1-scipy.spatial.distance.cdist(A, B, metric='cosine')\n# print(result)\n\nx = np.random.random((1, 3, 224, 224))\nx = torch.from_numpy(x).float()\nvar = Variable(x)\nbackbone = torchvision.models.resnet50(pretrained=True)\nbackbone(var)\n"""
feaure_extractor.py,3,"b'# -*- coding: utf-8 -*-\n\nimport os\nfrom config import *\nfrom utils import *\nfrom torch.autograd import Variable\nfrom data import Fashion_attr_prediction, Fashion_inshop\nfrom net import f_model, c_model, p_model\n\n\nmain_model = f_model(model_path=DUMPED_MODEL).cuda(GPU_ID)\ncolor_model = c_model().cuda(GPU_ID)\npooling_model = p_model().cuda(GPU_ID)\nextractor = FeatureExtractor(main_model, color_model, pooling_model)\n\n\ndef dump_dataset(loader, deep_feats, color_feats, labels):\n    for batch_idx, (data, data_path) in enumerate(loader):\n        data = Variable(data).cuda(GPU_ID)\n        deep_feat, color_feat = extractor(data)\n        for i in range(len(data_path)):\n            path = data_path[i]\n            feature_n = deep_feat[i].squeeze()\n            color_feature_n = color_feat[i]\n            # dump_feature(feature, path)\n\n            deep_feats.append(feature_n)\n            color_feats.append(color_feature_n)\n            labels.append(path)\n\n        if batch_idx % LOG_INTERVAL == 0:\n            print(""{} / {}"".format(batch_idx * EXTRACT_BATCH_SIZE, len(loader.dataset)))\n\n\ndef dump():\n    all_loader = torch.utils.data.DataLoader(\n        Fashion_attr_prediction(type=""all"", transform=data_transform_test),\n        batch_size=EXTRACT_BATCH_SIZE, num_workers=NUM_WORKERS, pin_memory=True\n    )\n    deep_feats = []\n    color_feats = []\n    labels = []\n    dump_dataset(all_loader, deep_feats, color_feats, labels)\n\n    if ENABLE_INSHOP_DATASET:\n        inshop_loader = torch.utils.data.DataLoader(\n            Fashion_inshop(type=""all"", transform=data_transform_test),\n            batch_size=EXTRACT_BATCH_SIZE, num_workers=NUM_WORKERS, pin_memory=True\n        )\n        dump_dataset(inshop_loader, deep_feats, color_feats, labels)\n\n    feat_all = os.path.join(DATASET_BASE, \'all_feat.npy\')\n    color_feat_all = os.path.join(DATASET_BASE, \'all_color_feat.npy\')\n    feat_list = os.path.join(DATASET_BASE, \'all_feat.list\')\n    with open(feat_list, ""w"") as fw:\n        fw.write(""\\n"".join(labels))\n    np.save(feat_all, np.vstack(deep_feats))\n    np.save(color_feat_all, np.vstack(color_feats))\n    print(""Dumped to all_feat.npy, all_color_feat.npy and all_feat.list."")\n\n\nif __name__ == ""__main__"":\n    dump()\n\n\n'"
in_shop_eval.py,0,"b'# -*- coding: utf-8 -*-\n\nfrom data import Fashion_inshop\nfrom retrieval import load_feat_db, get_deep_color_top_n\nimport random\n\n\ndef eval(retrieval_top_n=10):\n    dataset = Fashion_inshop()\n    length = dataset.test_len\n    deep_feats, color_feats, labels = load_feat_db()\n    deep_feats, color_feats, labels = deep_feats[-length:], color_feats[-length:], labels[-length:]\n    feat_dict = {labels[i]: (deep_feats[i], color_feats[i]) for i in range(len(labels))}\n\n    include_once = 0\n    include_zero = 0\n    include_times = 0\n    should_include_times = 0\n    for iter_id, item_id in enumerate(dataset.test_list):\n        item_imgs = dataset.test_dict[item_id]\n        item_img = random.choice(item_imgs)\n        result = get_deep_color_top_n(feat_dict[item_img], deep_feats, color_feats, labels, retrieval_top_n)\n        keys = list(map(lambda x: x[0], result))\n        included = list(map(lambda x: x in item_imgs, keys))\n\n        should_include_times += (len(item_imgs) - 1)\n        include_once += (1 if included.count(True) >= 2 else 0)\n        include_zero += (1 if included.count(True) <= 1 else 0)\n        include_times += (included.count(True) - 1)\n\n        if iter_id % 10 == 0:\n            print(""{}/{}, is included: {}/{}, included times: {}/{}"".format(iter_id, len(dataset.test_list),\n                  include_once, include_once + include_zero,\n                  include_times, should_include_times))\n\n    return include_times, should_include_times, include_once, include_zero\n\n\nif __name__ == \'__main__\':\n    print(eval())\n'"
kmeans.py,0,"b""# -*- coding:utf-8 -*-\n\n\nfrom sklearn.cluster import KMeans\nfrom retrieval import load_feat_db\nfrom sklearn.externals import joblib\nfrom config import DATASET_BASE, N_CLUSTERS\nimport os\n\n\nif __name__ == '__main__':\n    feats, labels = load_feat_db()\n    model = KMeans(n_clusters=N_CLUSTERS, random_state=0, n_jobs=-1).fit(feats)\n    model_path = os.path.join(DATASET_BASE, r'models', r'kmeans.m')\n    joblib.dump(model, model_path)\n"""
net.py,3,"b'# -*- coding: utf-8 -*-\n\nimport torchvision\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom config import *\nfrom utils import *\nfrom torch.autograd import Variable\n\n\nclass f_model(nn.Module):\n    \'\'\'\n    input: N * 3 * 224 * 224\n    output: N * num_classes, N * inter_dim, N * C\' * 7 * 7\n    \'\'\'\n    def __init__(self, freeze_param=False, inter_dim=INTER_DIM, num_classes=CATEGORIES, model_path=None):\n        super(f_model, self).__init__()\n        self.backbone = torchvision.models.resnet50(pretrained=True)\n        state_dict = self.backbone.state_dict()\n        num_features = self.backbone.fc.in_features\n        self.backbone = nn.Sequential(*list(self.backbone.children())[:-2])\n        model_dict = self.backbone.state_dict()\n        model_dict.update({k: v for k, v in state_dict.items() if k in model_dict})\n        self.backbone.load_state_dict(model_dict)\n        if freeze_param:\n            for param in self.backbone.parameters():\n                param.requires_grad = False\n\n        self.avg_pooling = nn.AvgPool2d(7, stride=1)\n        self.fc = nn.Linear(num_features, inter_dim)\n        self.fc2 = nn.Linear(inter_dim, num_classes)\n        state = load_model(model_path)\n        if state:\n            new_state = self.state_dict()\n            new_state.update({k: v for k, v in state.items() if k in new_state})\n            self.load_state_dict(new_state)\n\n    def forward(self, x):\n        x = self.backbone(x)\n        pooled = self.avg_pooling(x)\n        inter_out = self.fc(pooled.view(pooled.size(0), -1))\n        out = self.fc2(inter_out)\n        return out, inter_out, x\n\n\nclass c_model(nn.Module):\n    \'\'\'\n    input: N * C * 224 * 224\n    output: N * C * 7 * 7\n    \'\'\'\n    def __init__(self, pooling_size=32):\n        super(c_model, self).__init__()\n        self.pooling = nn.AvgPool2d(pooling_size)\n\n    def forward(self, x):\n        return self.pooling(x)\n\n\nclass p_model(nn.Module):\n    \'\'\'\n    input: N * C * W * H\n    output: N * 1 * W * H\n    \'\'\'\n    def __init__(self):\n        super(p_model, self).__init__()\n\n    def forward(self, x):\n        n, c, w, h = x.size()\n        x = x.view(n, c, w * h).permute(0, 2, 1)\n        pooled = F.avg_pool1d(x, c)\n        return pooled.view(n, 1, w, h)\n\n\nif __name__ == ""__main__"":\n    model = f_model()\n    # model = c_model()\n    print(1)\n'"
retrieval.py,2,"b'# -*- coding: utf-8 -*-\n\nimport os\nimport sys\nimport numpy as np\nfrom scipy.spatial.distance import cdist\nfrom torch.autograd import Variable\nfrom config import *\nfrom utils import *\nfrom data import Fashion_attr_prediction\nfrom net import f_model, c_model, p_model\nfrom sklearn.externals import joblib\n\n\n@timer_with_task(""Loading model"")\ndef load_test_model():\n    if not os.path.isfile(DUMPED_MODEL) and not os.path.isfile(os.path.join(DATASET_BASE, ""models"", DUMPED_MODEL)):\n        print(""No trained model file!"")\n        return\n    main_model = f_model(model_path=DUMPED_MODEL).cuda(GPU_ID)\n    color_model = c_model().cuda(GPU_ID)\n    pooling_model = p_model().cuda(GPU_ID)\n    extractor = FeatureExtractor(main_model, color_model, pooling_model)\n    return extractor\n\n\n@timer_with_task(""Loading feature database"")\ndef load_feat_db():\n    feat_all = os.path.join(DATASET_BASE, \'all_feat.npy\')\n    feat_list = os.path.join(DATASET_BASE, \'all_feat.list\')\n    color_feat = os.path.join(DATASET_BASE, \'all_color_feat.npy\')\n    if not os.path.isfile(feat_list) or not os.path.isfile(feat_all) or not os.path.isfile(color_feat):\n        print(""No feature db file! Please run feature_extractor.py first."")\n        return\n    deep_feats = np.load(feat_all)\n    color_feats = np.load(color_feat)\n    with open(feat_list) as f:\n        labels = list(map(lambda x: x.strip(), f.readlines()))\n    return deep_feats, color_feats, labels\n\n\n@timer_with_task(""Loading feature K-means model"")\ndef load_kmeans_model():\n    clf_model_path = os.path.join(DATASET_BASE, r\'models\', r\'kmeans.m\')\n    clf = joblib.load(clf_model_path)\n    return clf\n\n\ndef read_lines(path):\n    with open(path) as fin:\n        lines = fin.readlines()[2:]\n        lines = list(filter(lambda x: len(x) > 0, lines))\n        names = list(map(lambda x: x.strip().split()[0], lines))\n    return names\n\n\ndef get_top_n(dist, labels, retrieval_top_n):\n    ind = np.argpartition(dist, -retrieval_top_n)[-retrieval_top_n:][::-1]\n    ret = list(zip([labels[i] for i in ind], dist[ind]))\n    ret = sorted(ret, key=lambda x: x[1], reverse=True)\n    return ret\n\n\ndef get_similarity(feature, feats, metric=\'cosine\'):\n    dist = -cdist(np.expand_dims(feature, axis=0), feats, metric)[0]\n    return dist\n\n\ndef get_deep_color_top_n(features, deep_feats, color_feats, labels, retrieval_top_n=5):\n    deep_scores = get_similarity(features[0], deep_feats, DISTANCE_METRIC[0])\n    color_scores = get_similarity(features[1], color_feats, DISTANCE_METRIC[1])\n    results = get_top_n(deep_scores + color_scores * COLOR_WEIGHT, labels, retrieval_top_n)\n    return results\n\n\n@timer_with_task(""Doing naive query"")\ndef naive_query(features, deep_feats, color_feats, labels, retrieval_top_n=5):\n    results = get_deep_color_top_n(features, deep_feats, color_feats, labels, retrieval_top_n)\n    return results\n\n\n@timer_with_task(""Doing query with k-Means"")\ndef kmeans_query(clf, features, deep_feats, color_feats, labels, retrieval_top_n=5):\n    label = clf.predict(features[0].reshape(1, features[0].shape[0]))\n    ind = np.where(clf.labels_ == label)\n    d_feats = deep_feats[ind]\n    c_feats = color_feats[ind]\n    n_labels = list(np.array(labels)[ind])\n    results = get_deep_color_top_n(features, d_feats, c_feats, n_labels, retrieval_top_n)\n    return results\n\n\n@timer_with_task(""Extracting image feature"")\ndef dump_single_feature(img_path, extractor):\n    paths = [img_path, os.path.join(DATASET_BASE, img_path), os.path.join(DATASET_BASE, \'in_shop\', img_path)]\n    for i in paths:\n        if not os.path.isfile(i):\n            continue\n        single_loader = torch.utils.data.DataLoader(\n            Fashion_attr_prediction(type=""single"", img_path=i, transform=data_transform_test),\n            batch_size=1, num_workers=NUM_WORKERS, pin_memory=True\n        )\n        data = list(single_loader)[0]\n        data = Variable(data).cuda(GPU_ID)\n        deep_feat, color_feat = extractor(data)\n        deep_feat = deep_feat[0].squeeze()\n        color_feat = color_feat[0]\n        return deep_feat, color_feat\n    return None\n\n\ndef visualize(original, result, cols=1):\n    import matplotlib.pyplot as plt\n    import cv2\n    n_images = len(result) + 1\n    titles = [""Original""] + [""Score: {:.4f}"".format(v) for k, v in result]\n    images = [original] + [k for k, v in result]\n    mod_full_path = lambda x: os.path.join(DATASET_BASE, x) \\\n        if os.path.isfile(os.path.join(DATASET_BASE, x)) \\\n        else os.path.join(DATASET_BASE, \'in_shop\', x,)\n    images = list(map(mod_full_path, images))\n    images = list(map(lambda x: cv2.cvtColor(cv2.imread(x), cv2.COLOR_BGR2RGB), images))\n    fig = plt.figure()\n    for n, (image, title) in enumerate(zip(images, titles)):\n        a = fig.add_subplot(cols, np.ceil(n_images / float(cols)), n + 1)\n        plt.imshow(image)\n        a.set_title(title)\n    fig.set_size_inches(np.array(fig.get_size_inches()) * n_images * 0.25)\n    plt.show()\n\n\nif __name__ == ""__main__"":\n    example = ""img/Sheer_Pleated-Front_Blouse/img_00000005.jpg""\n    if len(sys.argv) > 1 and sys.argv[1].endswith(""jpg""):\n        example = sys.argv[1]\n    else:\n        print(""Usage: python {} img_path\\nNo input image, use default."".format(sys.argv[0]))\n\n    extractor = load_test_model()\n    deep_feats, color_feats, labels = load_feat_db()\n    f = dump_single_feature(example, extractor)\n\n    if any(list(map(lambda x: x is None, f))):\n        print(""Input feature is None"")\n        exit()\n\n    clf = load_kmeans_model()\n\n    result = naive_query(f, deep_feats, color_feats, labels, 5)\n    result_kmeans = kmeans_query(clf, f, deep_feats, color_feats, labels, 5)\n\n    print(""Naive query result:"", result)\n    print(""K-Means query result:"", result_kmeans)\n    visualize(example, result)\n    visualize(example, result_kmeans)\n'"
train.py,8,"b'# -*- coding: utf-8 -*-\nfrom __future__ import print_function\n\nimport torch\nimport random\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torchvision import transforms\nfrom torch.autograd import Variable\nfrom config import *\nfrom utils import *\nfrom data import Fashion_attr_prediction, Fashion_inshop\nfrom net import f_model\n\n\ndata_transform_train = transforms.Compose([\n    transforms.Scale(IMG_SIZE),\n    transforms.RandomSizedCrop(CROP_SIZE),\n    transforms.RandomHorizontalFlip(),\n    transforms.ToTensor(),\n    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n    ])\n\ndata_transform_test = transforms.Compose([\n    transforms.Scale(CROP_SIZE),\n    transforms.CenterCrop(CROP_SIZE),\n    transforms.ToTensor(),\n    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n    ])\n\n\ntrain_loader = torch.utils.data.DataLoader(\n    Fashion_attr_prediction(type=""train"", transform=data_transform_train),\n    batch_size=TRAIN_BATCH_SIZE, num_workers=NUM_WORKERS, pin_memory=True\n)\n\ntest_loader = torch.utils.data.DataLoader(\n    Fashion_attr_prediction(type=""test"", transform=data_transform_test),\n    batch_size=TEST_BATCH_SIZE, num_workers=NUM_WORKERS, pin_memory=True\n)\n\ntriplet_loader = torch.utils.data.DataLoader(\n    Fashion_attr_prediction(type=""triplet"", transform=data_transform_train),\n    batch_size=TRIPLET_BATCH_SIZE, num_workers=NUM_WORKERS, pin_memory=True\n)\n\nif ENABLE_INSHOP_DATASET:\n    triplet_in_shop_loader = torch.utils.data.DataLoader(\n        Fashion_inshop(type=""train"", transform=data_transform_train),\n        batch_size=TRIPLET_BATCH_SIZE, num_workers=NUM_WORKERS, pin_memory=True\n    )\n\nmodel = f_model(freeze_param=FREEZE_PARAM, model_path=DUMPED_MODEL).cuda(GPU_ID)\noptimizer = optim.SGD(filter(lambda p: p.requires_grad, model.parameters()), lr=LR, momentum=MOMENTUM)\n\n\ndef train(epoch):\n    model.train()\n    criterion_c = nn.CrossEntropyLoss()\n    if ENABLE_TRIPLET_WITH_COSINE:\n        criterion_t = TripletMarginLossCosine()\n    else:\n        criterion_t = nn.TripletMarginLoss()\n    triplet_loader_iter = iter(triplet_loader)\n    triplet_type = 0\n    if ENABLE_INSHOP_DATASET:\n        triplet_in_shop_loader_iter = iter(triplet_in_shop_loader)\n    for batch_idx, (data, target) in enumerate(train_loader):\n        if batch_idx % TEST_INTERVAL == 0:\n            test()\n        data, target = data.cuda(GPU_ID), target.cuda(GPU_ID)\n        data, target = Variable(data), Variable(target)\n        optimizer.zero_grad()\n        outputs = model(data)[0]\n        classification_loss = criterion_c(outputs, target)\n        if TRIPLET_WEIGHT:\n            if ENABLE_INSHOP_DATASET and random.random() < INSHOP_DATASET_PRECENT:\n                triplet_type = 1\n                try:\n                    data_tri_list = next(triplet_in_shop_loader_iter)\n                except StopIteration:\n                    triplet_in_shop_loader_iter = iter(triplet_in_shop_loader)\n                    data_tri_list = next(triplet_in_shop_loader_iter)\n            else:\n                triplet_type = 0\n                try:\n                    data_tri_list = next(triplet_loader_iter)\n                except StopIteration:\n                    triplet_loader_iter = iter(triplet_loader)\n                    data_tri_list = next(triplet_loader_iter)\n            triplet_batch_size = data_tri_list[0].shape[0]\n            data_tri = torch.cat(data_tri_list, 0)\n            data_tri = data_tri.cuda(GPU_ID)\n            data_tri = Variable(data_tri, requires_grad=True)\n            feats = model(data_tri)[1]\n            triplet_loss = criterion_t(\n                feats[:triplet_batch_size],\n                feats[triplet_batch_size:2 * triplet_batch_size],\n                feats[2 * triplet_batch_size:]\n            )\n            loss = classification_loss + triplet_loss * TRIPLET_WEIGHT\n        else:\n            loss = classification_loss\n        loss.backward()\n        optimizer.step()\n        if batch_idx % LOG_INTERVAL == 0:\n            if TRIPLET_WEIGHT:\n                print(\'Train Epoch: {} [{}/{} ({:.0f}%)]\\tAll Loss: {:.4f}\\t\'\n                      \'Triple Loss({}): {:.4f}\\tClassification Loss: {:.4f}\'.format(\n                    epoch, batch_idx * len(data), len(train_loader.dataset),\n                    100. * batch_idx / len(train_loader), loss.data[0], triplet_type,\n                    triplet_loss.data[0], classification_loss.data[0]))\n            else:\n                print(\'Train Epoch: {} [{}/{} ({:.0f}%)]\\tClassification Loss: {:.4f}\'.format(\n                    epoch, batch_idx * len(data), len(train_loader.dataset),\n                           100. * batch_idx / len(train_loader), loss.data[0]))\n        if batch_idx and batch_idx % DUMP_INTERVAL == 0:\n            print(\'Model saved to {}\'.format(dump_model(model, epoch, batch_idx)))\n\n    print(\'Model saved to {}\'.format(dump_model(model, epoch)))\n\n\ndef test():\n    model.eval()\n    criterion = nn.CrossEntropyLoss(size_average=False)\n    test_loss = 0\n    correct = 0\n    for batch_idx, (data, target) in enumerate(test_loader):\n        data, target = data.cuda(GPU_ID), target.cuda(GPU_ID)\n        data, target = Variable(data, volatile=True), Variable(target)\n        output = model(data)[0]\n        test_loss += criterion(output, target).data[0]\n        pred = output.data.max(1, keepdim=True)[1]\n        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n        if batch_idx > TEST_BATCH_COUNT:\n            break\n    test_loss /= (TEST_BATCH_COUNT * TEST_BATCH_SIZE)\n    print(\'\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n\'.format(\n        float(test_loss), correct, (TEST_BATCH_COUNT * TEST_BATCH_SIZE),\n        float(100. * correct / (TEST_BATCH_COUNT * TEST_BATCH_SIZE))))\n\n\nif __name__ == ""__main__"":\n    for epoch in range(1, EPOCH + 1):\n        train(epoch)\n'"
utils.py,6,"b'# -*- coding: utf-8 -*-\n\nfrom config import *\nimport os\nimport time\nimport torch\nimport numpy as np\nimport torch.nn as nn\nfrom torchvision import transforms\nimport torch.nn.functional as F\n\n\ndef dump_model(model, epoch, batch_idx=""final""):\n    dump_folder = os.path.join(DATASET_BASE, \'models\')\n    if not os.path.isdir(dump_folder):\n        os.mkdir(dump_folder)\n    save_path = os.path.join(dump_folder, ""model_{}_{}.pth.tar"".format(epoch, batch_idx))\n    torch.save(model.state_dict(), save_path)\n    return save_path\n\n\ndef load_model(path=None):\n    if not path:\n        return None\n    full = os.path.join(DATASET_BASE, \'models\', path)\n    for i in [path, full]:\n        if os.path.isfile(i):\n            return torch.load(i)\n    return None\n\n\ndef dump_feature(feat, img_path):\n    feat_folder = os.path.join(DATASET_BASE, \'features\')\n    if not os.path.isdir(feat_folder):\n        os.mkdir(feat_folder)\n    np_path = img_path.replace(""/"", ""+"")\n    np_path = os.path.join(feat_folder, np_path)\n    np.save(np_path, feat)\n\n\ndef load_feature(img_path):\n    feat_folder = os.path.join(DATASET_BASE, \'features\')\n    np_path = img_path.replace(""/"", ""+"")\n    np_path = os.path.join(feat_folder, np_path + \'.npy\')\n    if os.path.isfile(np_path):\n        feat = np.load(np_path)\n        return feat\n    else:\n        return None\n\n\ndata_transform_test = transforms.Compose([\n    transforms.Scale(CROP_SIZE),\n    transforms.CenterCrop(CROP_SIZE),\n    transforms.ToTensor(),\n    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n    ])\n\n\nclass FeatureExtractor(nn.Module):\n    def __init__(self, deep_module, color_module, pooling_module):\n        super(FeatureExtractor, self).__init__()\n        self.deep_module = deep_module\n        self.color_module = color_module\n        self.pooling_module = pooling_module\n        self.deep_module.eval()\n        self.color_module.eval()\n        self.pooling_module.eval()\n\n    def forward(self, x):\n        # for name, module in list(self.deep_module._modules.items())[:-1]:\n        #     if name == \'fc\':\n        #         x = x.view(x.size(0), -1)\n        #     x = module(x)\n        cls, feat, conv_out = self.deep_module(x)\n        color = self.color_module(x).cpu().data.numpy()  # N * C * 7 * 7\n        weight = self.pooling_module(conv_out).cpu().data.numpy()  # N * 1 * 7 * 7\n        result = []\n        for i in range(cls.size(0)):\n            weight_n = weight[i].reshape(-1)\n            idx = np.argpartition(weight_n, -COLOR_TOP_N)[-COLOR_TOP_N:][::-1]\n            color_n = color[i].reshape(color.shape[1], -1)\n            color_selected = color_n[:, idx].reshape(-1)\n            result.append(color_selected)\n        return feat.cpu().data.numpy(), result\n\n\nclass TripletMarginLossCosine(nn.Module):\n    def __init__(self, margin=1.0):\n        super(TripletMarginLossCosine, self).__init__()\n        self.margin = margin\n\n    def forward(self, anchor, positive, negative):\n        d_p = 1 - F.cosine_similarity(anchor, positive).view(-1, 1)\n        d_n = 1 - F.cosine_similarity(anchor, negative).view(-1, 1)\n        # p = 2\n        # eps = 1e-6\n        # d_p = F.pairwise_distance(anchor, positive, p, eps)\n        # d_n = F.pairwise_distance(anchor, negative, p, eps)\n        dist_hinge = torch.clamp(self.margin + d_p - d_n, min=0.0)\n        loss = torch.mean(dist_hinge)\n        return loss\n\n\ndef timer_with_task(job=""""):\n    def timer(fn):\n        def wrapped(*args, **kw):\n            print(""{}"".format(job + ""...""))\n            tic = time.time()\n            ret = fn(*args, **kw)\n            toc = time.time()\n            print(""{} Done. Time: {:.3f} sec"".format(job, (toc - tic)))\n            return ret\n        return wrapped\n    return timer\n'"
scripts/__init__.py,0,b''
scripts/category_count.py,0,"b'# -*- coding: utf-8 -*-\n\nfrom config import *\nimport os\n\nlist_category_img = os.path.join(DATASET_BASE, r\'Anno\', r\'list_category_img.txt\')\noutput = os.path.join(DATASET_BASE, r\'Anno\', r\'analytic.txt\')\nwith open(list_category_img) as fin:\n    lines = fin.readlines()[2:]\n    lines = list(filter(lambda x: len(x) > 0, lines))\n    numbers = list(map(lambda x: int(x.strip().split()[1]), lines))\n\nwith open(output, ""w"") as fw:\n    for i in range(1, 51):\n        fw.write(""%d %d\\n"" % (i, numbers.count(i)))\n'"
scripts/model_convertor.py,2,"b'# -*- coding: utf-8 -*-\n\nimport torch\nimport os\nfrom config import DATASET_BASE, DUMPED_MODEL\nfrom collections import OrderedDict\n\nNEW_MODEL_NAME = ""model_5_final_converted.pth.tar""\n\nif __name__ == \'__main__\':\n    d = torch.load(os.path.join(DATASET_BASE, DUMPED_MODEL))\n    d = OrderedDict([(k, v) if k != \'backbone.fc.weight\' else (\'fc.weight\', v) for k, v in d.items()])\n    d = OrderedDict([(k, v) if k != \'backbone.fc.bias\' else (\'fc.bias\', v) for k, v in d.items()])\n    torch.save(d, os.path.join(DATASET_BASE, NEW_MODEL_NAME))\n'"
