file_path,api_count,code
DataLoader.py,0,"b'from numpy import ndarray\nfrom utils import load_and_shuffle_datasets\nfrom typing import Sized, Iterable, List, Tuple\nimport glob\nimport os\nimport threading\nimport time\nimport random\nimport numpy as np\n\nglobal data\nglobal file_chunks\nglobal next_file\n\n\ndef chunks(iterable: Sized, n: int = 1) -> Iterable:\n    """"""\n    Given a iterable generate chunks of size n\n    Input:\n     - Sized that will be chunked\n     - n: Integer batch size\n    Output:\n    - Iterable\n    """"""\n    l: int = len(iterable)\n    for ndx in range(0, l, n):\n        yield iterable[ndx : min(ndx + n, l)]\n\n\ndef dataLoaderThread(\n    hide_map_prob: float,\n    dropout_images_prob: List[float],\n    fp: int,\n    load_next_event: threading.Event,\n    end_thread_event: threading.Event,\n):\n    """"""\n    When load_next_event is set the thread will load file_chunks[next_file] and store it in the data global variable\n    Input:\n     - hide_map_prob: Probability for removing the minimap (put a black square)\n       from a training example (0<=hide_map_prob<=1)\n     - dropout_images_prob List of 5 floats or None, probability for removing each input image during training\n     (black image) from a training example (0<=dropout_images_prob<=1)\n     - fp: floating-point precision: Available values: 16, 32, 64\n     - load_next_event: Event to load the next chunk of data\n     - end_thread_event: Event to end the thread that loads the data\n    Output:\n    """"""\n    global data\n    global file_chunks\n    global next_file\n    while not end_thread_event.isSet():\n        load_next_event.wait(timeout=10)\n        if load_next_event.isSet():\n            load_next_event.clear()\n\n            data = load_and_shuffle_datasets(\n                paths=file_chunks[next_file],\n                hide_map_prob=hide_map_prob,\n                dropout_images_prob=dropout_images_prob,\n                fp=fp,\n                force_cpu=True,\n            )\n\n            next_file += 1\n\n\nclass DataLoaderTEDD:\n    """"""\n    Class for loading the dataset during training. The class will spawn a thread that will buffer the next\n    chunk of data so the next chunk will be loaded using the CPU while the GPU is being used for training\n    the model. Once all the data has been used the DataLoader will return ""None"".\n    """"""\n\n    def __init__(\n        self,\n        dataset_dir: str,\n        nfiles2load: int,\n        hide_map_prob: float,\n        dropout_images_prob: List[float],\n        fp: int,\n    ):\n        """"""\n        Dataloader initialization\n        Input:\n         - dataset_dir: Directory where the training files (training_file*.npz are stored)\n         - nfiles2load: Number of files to load each chunk. The examples in the files will be shuffled\n         - hide_map_prob: Probability for removing the minimap (put a black square)\n           from a training example (0<=hide_map_prob<=1)\n         - dropout_images_prob List of 5 floats or None, probability for removing each input image during training\n           (black image) from a training example (0<=dropout_images_prob<=1)\n         - fp: floating-point precision: Available values: 16, 32, 64\n         Output:\n        """"""\n        global data\n        global file_chunks\n        global next_file\n\n        files: List[str] = glob.glob(os.path.join(dataset_dir, ""*.npz""))\n        random.seed()\n        random.shuffle(files)\n        self.total_files: int = len(files)\n        file_chunks = list(chunks(files, n=nfiles2load))\n        if len(file_chunks) > 0:\n            self.load_next_event = threading.Event()\n            self.end_thread_event = threading.Event()\n\n            th_data: threading.Thread = threading.Thread(\n                target=dataLoaderThread,\n                args=[\n                    hide_map_prob,\n                    dropout_images_prob,\n                    fp,\n                    self.load_next_event,\n                    self.end_thread_event,\n                ],\n            )\n            th_data.setDaemon(True)\n            th_data.start()\n            self.load_next_event.set()\n\n            next_file = 0\n            self.actual_file = 0\n        else:\n            raise ValueError(f""No .npz file found in {dataset_dir}"")\n\n    def get_next(self) -> (np.ndarray, np.ndarray):\n        """"""\n        Return the next chunk of data and sets the event to buffering the next chunk. Note: The last chunk\n        will load num_files%nfiles2load files (the remaining ones).\n        Input:\n        Output:\n         If there are files remaining\n          - X: input examples [nfiles2load * num_examples_per_file, 5, 3, H, W]\n          - y: golds for the input examples [nfiles2load * num_examples_per_file]\n         else (if all files have been already loaded):\n          - None\n        """"""\n        global next_file\n        global data\n\n        if self.actual_file >= len(file_chunks):\n            print(f""All files in the DataLoader used"")\n            self.end_thread_event.set()\n            return None\n\n        while self.actual_file == next_file:\n            time.sleep(0.1)\n\n        rdata: Tuple[ndarray, ndarray] = data\n        self.actual_file += 1\n        if not next_file >= len(file_chunks):\n            self.load_next_event.set()\n        else:\n            self.end_thread_event.set()\n        return rdata\n\n    def __len__(self) -> int:\n        """"""\n        Return the number of files in the dataset\n        Input\n        Output:\n        -int: Number of files in the dataset\n        """"""\n        return self.total_files\n\n    def close(self):\n        """"""\n        Set the event to stop the thread that buffers the data\n        Input\n        Output:\n        -int: 0\n        """"""\n        self.end_thread_event.set()\n        return 0\n'"
generate_data.py,0,"b'import os\r\nimport time\r\n\r\nimport numpy as np\r\nimport argparse\r\nimport threading\r\nimport screen.record_screen as screen_recorder\r\nfrom keyboard.getkeys import key_check\r\n\r\n\r\ndef save_data(dir_path: str, data: np.ndarray, number: int):\r\n    """"""\r\n    Save a numpy ndarray to a directory and delete it from RAM\r\n    Input:\r\n     - dir_path path of the directory where the files are going to be stored\r\n     - data umpy ndarray\r\n     - number integer used to name the file\r\n    Ouput:\r\n\r\n    """"""\r\n    file_name = os.path.join(dir_path, f""training_data{number}.npz"")\r\n    np.savez_compressed(file_name, data)\r\n    del data\r\n\r\n\r\ndef get_last_file_num(dir_path: str) -> int:\r\n    """"""\r\n    Given a directory with files in the format training_data[number].npz return the higher number\r\n    Input:\r\n     - dir_path path of the directory where the files are stored\r\n    Ouput:\r\n     - int max number in the directory. -1 if no file exits\r\n     """"""\r\n\r\n    files = [\r\n        int(f.split(""."")[0][13:])\r\n        for f in os.listdir(dir_path)\r\n        if os.path.isfile(os.path.join(dir_path, f)) and f.startswith(""training_data"")\r\n    ]\r\n\r\n    return -1 if len(files) == 0 else max(files)\r\n\r\n\r\ndef counter_keys(key: np.ndarray) -> int:\r\n    """"""\r\n    Multi-hot vector to one hot vector (represented as an integer)\r\n    Input:\r\n     - key numpy array of integers (1,0) of size 4\r\n    Ouput:\r\n    - One hot vector encoding represented as an index (int). If the vector does not represent any valid key\r\n    input the returned value will be -1\r\n\r\n    """"""\r\n    if np.array_equal(key, [0, 0, 0, 0]):\r\n        return 0\r\n    elif np.array_equal(key, [1, 0, 0, 0]):\r\n        return 1\r\n    elif np.array_equal(key, [0, 1, 0, 0]):\r\n        return 2\r\n    elif np.array_equal(key, [0, 0, 1, 0]):\r\n        return 3\r\n    elif np.array_equal(key, [0, 0, 0, 1]):\r\n        return 4\r\n    elif np.array_equal(key, [1, 0, 1, 0]):\r\n        return 5\r\n    elif np.array_equal(key, [1, 0, 0, 1]):\r\n        return 6\r\n    elif np.array_equal(key, [0, 1, 1, 0]):\r\n        return 7\r\n    elif np.array_equal(key, [0, 1, 0, 1]):\r\n        return 8\r\n    else:\r\n        return -1\r\n\r\n\r\ndef generate_dataset(\r\n    output_dir: str, num_training_examples_per_file: int, use_probability: bool = True\r\n) -> None:\r\n    """"""\r\n    Generate dataset exampled from a human playing a videogame\r\n    HOWTO:\r\n        Set your game in windowed mode\r\n        Set your game to 1600x900 resolution\r\n        Move the game window to the top left corner, there should be a blue line of 1 pixel in the left bezel of your\r\n         screen and the window top bar should start in the top bezel of your screen.\r\n        Play the game! The program will capture your screen and generate the training examples. There will be saved\r\n         as files named ""training_dataX.npz"" (numpy compressed array). Don\'t worry if you re-launch this script,\r\n          the program will search for already existing dataset files in the directory and it won\'t overwrite them.\r\n\r\n    Input:\r\n    - output_dir: Directory where the training files will be saved\r\n    - num_training_examples_per_file: Number of training examples per output file\r\n    - use_probability: Use probability to generate a balanced dataset. Each example will have a probability that\r\n      depends on the number of instances with the same key combination in the dataset.\r\n\r\n    Output:\r\n\r\n    """"""\r\n\r\n    training_data: list = []\r\n    stop_recording: threading.Event = threading.Event()\r\n\r\n    th_img: threading.Thread = threading.Thread(\r\n        target=screen_recorder.img_thread, args=[stop_recording]\r\n    )\r\n\r\n    th_seq: threading.Thread = threading.Thread(\r\n        target=screen_recorder.image_sequencer_thread, args=[stop_recording]\r\n    )\r\n    th_img.setDaemon(True)\r\n    th_seq.setDaemon(True)\r\n    th_img.start()\r\n    # Wait to launch the image_sequencer_thread, it needs the img_thread to be running\r\n    time.sleep(1)\r\n    th_seq.start()\r\n    number_of_files: int = get_last_file_num(output_dir) + 1\r\n    total_examples_in_dataset: int = (\r\n        number_of_files * num_training_examples_per_file\r\n    ) + number_of_files\r\n    time.sleep(4)\r\n    last_num: int = 5  # The image sequence starts with images containing zeros, wait until it is filled with real images\r\n\r\n    number_of_keys = np.asarray([0, 0, 0, 0, 0, 0, 0, 0, 0])\r\n\r\n    while True:\r\n\r\n        while last_num == screen_recorder.num:\r\n            time.sleep(0.01)\r\n\r\n        last_num = screen_recorder.num\r\n        img_seq, output = screen_recorder.seq.copy(), screen_recorder.key_out.copy()\r\n\r\n        print(\r\n            f""Recording at {screen_recorder.fps} FPS\\n""\r\n            f""Images in sequence {len(img_seq)}\\n""\r\n            f""Training data len {total_examples_in_dataset - number_of_files} sequences\\n""\r\n            f""Number of archives {number_of_files}\\n""\r\n            f""Keys pressed: {output}\\n""\r\n            f""Keys samples recorded: ""\r\n            f""None: {str(number_of_keys[0])} ""\r\n            f""A: {str(number_of_keys[1])} ""\r\n            f""D {str(number_of_keys[2])} ""\r\n            f""W {str(number_of_keys[3])} ""\r\n            f""S {str(number_of_keys[4])} ""\r\n            f""AW {str(number_of_keys[5])} ""\r\n            f""AS {str(number_of_keys[6])} ""\r\n            f""WD {str(number_of_keys[7])} ""\r\n            f""SD {str(number_of_keys[8])}\\n""\r\n            f""Push QE to exit\\n"",\r\n            end=""\\r"",\r\n        )\r\n\r\n        key = counter_keys(output)\r\n\r\n        if key != -1:\r\n            if use_probability:\r\n                total = np.sum(number_of_keys)\r\n                key_num = number_of_keys[key]\r\n                if total != 0:\r\n                    prop = ((total - key_num) / total) ** 2\r\n                    if prop < 0.5:\r\n                        prop = 0.1\r\n\r\n                else:\r\n                    prop = 1.0\r\n                if np.random.rand() <= prop:\r\n                    number_of_keys[key] += 1\r\n                    total_examples_in_dataset += 1\r\n                    training_data.append(\r\n                        [\r\n                            img_seq[0],\r\n                            img_seq[1],\r\n                            img_seq[2],\r\n                            img_seq[3],\r\n                            img_seq[4],\r\n                            output,\r\n                        ]\r\n                    )\r\n\r\n            else:\r\n                number_of_keys[key] += 1\r\n                total_examples_in_dataset += 1\r\n                training_data.append(\r\n                    [img_seq[0], img_seq[1], img_seq[2], img_seq[3], img_seq[4], output]\r\n                )\r\n\r\n        keys = key_check()\r\n        if ""Q"" in keys and ""E"" in keys:\r\n            print(""\\nStopping..."")\r\n            stop_recording.set()\r\n            save_thread = threading.Thread(\r\n                target=save_data,\r\n                args=(output_dir, training_data.copy(), number_of_files,),\r\n            )\r\n            save_thread.start()\r\n            th_seq.join()\r\n            th_img.join()\r\n            save_thread.join()\r\n            break\r\n\r\n        if total_examples_in_dataset % num_training_examples_per_file == 0:\r\n            threading.Thread(\r\n                target=save_data,\r\n                args=(output_dir, training_data.copy(), number_of_files,),\r\n            ).start()\r\n            number_of_files += 1\r\n            training_data = []\r\n            total_examples_in_dataset += 1\r\n\r\n\r\nif __name__ == ""__main__"":\r\n\r\n    parser = argparse.ArgumentParser()\r\n\r\n    parser.add_argument(\r\n        ""--save_dir"",\r\n        type=str,\r\n        default=os.getcwd(),\r\n        help=""Directory where the training data will be saved"",\r\n    )\r\n\r\n    parser.add_argument(\r\n        ""--num_training_examples_per_file"",\r\n        type=int,\r\n        default=500,\r\n        help=""Number of sequences per file"",\r\n    )\r\n\r\n    parser.add_argument(\r\n        ""--save_everything"",\r\n        action=""store_true"",\r\n        help=""If this flag is added we will save every recorded sequence,""\r\n        "" it will result in a very unbalanced dataset. If this flag ""\r\n        ""is not added we will use probability to try to generate a balanced ""\r\n        ""dataset"",\r\n    )\r\n\r\n    args = parser.parse_args()\r\n\r\n    screen_recorder.initialize_global_variables()\r\n\r\n    generate_dataset(\r\n        output_dir=args.save_dir,\r\n        num_training_examples_per_file=args.num_training_examples_per_file,\r\n        use_probability=not args.save_everything,\r\n    )\r\n'"
model.py,34,"b'import os\nimport json\nfrom typing import List, Union\n\nimport torch\nimport torch.nn as nn\nimport torchvision.models as models\nimport torchvision.models.resnet\n\n\ndef get_resnet(model: int, pretrained: bool) -> torchvision.models.resnet.ResNet:\n    """"""\n    Get resnet model\n\n    Output:\n     torchvision.models.resnet[18,34,50,101,152]\n\n    Hyperparameters:\n    - model: Resnet model from torchvision.models (number of layers): [18,34,50,101,152]\n    - pretrained: Load model pretrained weights\n    """"""\n    if model == 18:\n        return models.resnet18(pretrained=pretrained)\n    elif model == 34:\n        return models.resnet34(pretrained=pretrained)\n    elif model == 50:\n        return models.resnet50(pretrained=pretrained)\n    elif model == 101:\n        return models.resnet101(pretrained=pretrained)\n    elif model == 152:\n        return models.resnet152(pretrained=pretrained)\n\n    raise ValueError(f""Resnet_{model} not found in torchvision.models"")\n\n\nclass EncoderCNN(nn.Module):\n    """"""\n    Extract feature vectors from input images (CNN)\n\n    Input:\n     torch.tensor [batch_size, num_channels, H, W]\n\n    Output:\n     torch.tensor [batch_size, embedded_size]\n\n    Hyperparameters:\n    - embedded_size: Size of the feature vectors\n    - dropout_cnn: dropout probability for the CNN layers\n    - dropout_cnn_out: dropout probability for the CNN representations (output layer)\n    - resnet: resnet module to use [18,34,50,101,152]\n    - pretrained_resnet: Load pretrained resnet weights\n    """"""\n\n    def __init__(\n        self,\n        embedded_size: int,\n        dropout_cnn: float,\n        dropout_cnn_out: float,\n        resnet: int,\n        pretrained_resnet: bool,\n    ):\n        super(EncoderCNN, self).__init__()\n        resnet: models.resnet.ResNet = get_resnet(resnet, pretrained_resnet)\n        modules: List[nn.Module] = list(resnet.children())[\n            :-1\n        ]  # delete the last fc layer.\n        modules_dropout: List[Union[nn.Module, nn.Dropout]] = []\n        for layer in modules:\n            modules_dropout.append(layer)\n            modules_dropout.append(nn.Dropout(dropout_cnn))\n\n        self.resnet: nn.Module = nn.Sequential(*modules_dropout)\n        self.fc: nn.Linear = nn.Linear(resnet.fc.in_features, embedded_size)\n        self.dropout: nn.Dropout = nn.Dropout(p=dropout_cnn_out)\n        self.bn: nn.BatchNorm1d = nn.BatchNorm1d(embedded_size, momentum=0.01)\n\n    def forward(self, images):\n        features = self.resnet(images)\n        features = features.reshape(features.size(0), -1)\n        features = self.bn(self.fc(features))\n        features = self.dropout(features)\n        return features\n\n    def predict(self, images):\n        with torch.no_grad():\n            features = self.resnet(images)\n            features = features.reshape(features.size(0), -1)\n            features = self.bn(self.fc(features))\n            return features\n\n\nclass PackFeatureVectors(nn.Module):\n    """"""\n    Reshape a list of features into a time distributed list of features. CNN ->  PackFeatureVectors -> RNN\n\n    Input:\n     torch.tensor [batch_size, embedded_size]\n\n    Output:\n     torch.tensor [batch_size/sequence_size, sequence_size, embedded_size]\n\n    Hyperparameters:\n    - sequence_size: Length of each series of features\n    """"""\n\n    def __init__(self, sequence_size: int):\n        super(PackFeatureVectors, self).__init__()\n        self.sequence_size: int = sequence_size\n\n    def forward(self, images):\n        return images.view(\n            int(images.size(0) / self.sequence_size), self.sequence_size, images.size(1)\n        )\n\n    def predict(self, images):\n        with torch.no_grad():\n            return images.view(\n                int(images.size(0) / self.sequence_size),\n                self.sequence_size,\n                images.size(1),\n            )\n\n\nclass EncoderRNN(nn.Module):\n    """"""\n    Extract feature vectors from input images (CNN)\n\n    Input:\n     torch.tensor [batch_size, sequence_size, embedded_size]\n\n    Output:\n     torch.tensor if bidirectional [batch_size, hidden_size*2]\n                 else [batch_size, hidden_size]\n\n     Hyperparameters:\n    - embedded_size: Size of the input feature vectors\n    - hidden_size: LSTM hidden size\n    - num_layers: number of layers in the LSTM\n    - dropout_lstm: dropout probability for the LSTM\n    - dropout_lstm_out: dropout probability for the LSTM representations (output layer)\n    - bidirectional: forward or bidirectional LSTM\n\n    """"""\n\n    def __init__(\n        self,\n        embedded_size: int,\n        hidden_size: int,\n        num_layers: int,\n        bidirectional_lstm: bool,\n        dropout_lstm: float,\n        dropout_lstm_out: float,\n    ):\n        super(EncoderRNN, self).__init__()\n\n        self.lstm: nn.LSTM = nn.LSTM(\n            embedded_size,\n            hidden_size,\n            num_layers,\n            dropout=dropout_lstm,\n            bidirectional=bidirectional_lstm,\n            batch_first=True,\n        )\n\n        self.bidirectional_lstm = bidirectional_lstm\n\n        self.dropout: nn.Dropout = nn.Dropout(p=dropout_lstm_out)\n\n    def forward(self, features: torch.tensor):\n        output, (h_n, c_n) = self.lstm(features)\n        if self.bidirectional_lstm:\n            x = torch.cat((h_n[-2], h_n[-1]), 1)\n        else:\n            x = h_n[-1]\n        return self.dropout(x)\n\n    def predict(self, features):\n        with torch.no_grad():\n            output, (h_n, c_n) = self.lstm(features)\n            if self.bidirectional_lstm:\n                x = torch.cat((h_n[-2], h_n[-1]), 1)\n            else:\n                x = h_n[-1]\n            return x\n\n\nclass OutputLayer(nn.Module):\n    """"""\n    Output linear layer that produces the predictions\n\n    Input:\n     torch.tensor [batch_size, hidden_size]\n\n    Output:\n     Forward: torch.tensor [batch_size, 12] (output values without softmax)\n     Predict: torch.tensor [batch_size, 1] (index of the max value after softmax)\n\n    Hyperparameters:\n    - hidden_size: Size of the input feature vectors\n    - layers: list of integer, for each integer i a linear layer with i neurons will be added.\n    """"""\n\n    def __init__(self, hidden_size: int, layers: List[int] = None):\n        super(OutputLayer, self).__init__()\n\n        linear_layers: List[Union[nn.Linear, nn.ReLU]] = []\n        if layers:\n            linear_layers.append(nn.Linear(hidden_size, layers[0]))\n            linear_layers.append(nn.ReLU())\n            for i in range(1, len(layers)):\n                linear_layers.append(nn.Linear(layers[i - 1], layers[i]))\n                linear_layers.append(nn.ReLU())\n            linear_layers.append(nn.Linear(layers[-1], 9))\n\n        else:\n            linear_layers.append(nn.Linear(hidden_size, 9))\n\n        self.linear = nn.Sequential(*linear_layers)\n        self.softmax = nn.Softmax(dim=1)\n\n    def forward(self, inputs):\n        return self.linear(inputs)\n\n    def predict(self, inputs):\n        with torch.no_grad():\n            value, index = torch.max(self.softmax(self.linear(inputs)), 1)\n            return index\n\n\nclass TEDD1104(nn.Module):\n    """"""\n    T.E.D.D. 1104 (https://nazizombiesplus.fandom.com/wiki/T.E.D.D.) is the neural network that learns\n    how to drive in videogames. It has been develop with Grand Theft Auto V (GTAV) in mind. However\n    it can learn how to drive in any videogame and if the model and controls are modified accordingly\n    it can play any game. The model receive as input 5 consecutive images that have been captured\n    with a fixed time interval between then (by default 1/10 seconds) and learn the correct\n    key to push in the keyboard (None,W,A,S,D,WA,WD,SA,SD).\n\n    T.E.D.D 1104 consists of 3 modules:\n        [*] A CNN (Resnet) that extract features from the images\n        [*] A RNN (LSTM) that generates a representation of the sequence of features from the CNN\n        [*] A linear output layer that predicts the key to push.\n\n    Input:\n     torch.tensor [batch_size, num_channels, H, W]\n     For efficiency the input input is not packed as sequence of 5 images, all the images in the batch will be\n     encoded in the CNN and the features vectors will be packed as sequences of 5 vectors before feeding them to the\n     RNN.\n\n    Output:\n     Forward: torch.tensor [batch_size, 12] (output values without softmax)\n     Predict: torch.tensor [batch_size, 1] (index of the max value after softmax)\n\n    Hyperparameters:\n    - resnet: resnet module to use [18,34,50,101,152]\n    - pretrained_resnet: Load pretrained resnet weights\n    - sequence_size: Length of each series of features\n    - embedded_size: Size of the feature vectors\n    - hidden_size: LSTM hidden size\n    - num_layers_lstm: number of layers in the LSTM\n    - bidirectional_lstm: forward or bidirectional LSTM\n    - layers_out: list of integer, for each integer i a linear layer with i neurons will be added.\n    - dropout_cnn: dropout probability for the CNN layers\n    - dropout_cnn_out: dropout probability for the cnn features (output layer)\n    - dropout_lstm: dropout probability for the LSTM\n    - dropout_lstm_out: dropout probability for the LSTM features (output layer)\n\n    """"""\n\n    def __init__(\n        self,\n        resnet: int,\n        pretrained_resnet: bool,\n        sequence_size: int,\n        embedded_size: int,\n        hidden_size: int,\n        num_layers_lstm: int,\n        bidirectional_lstm: bool,\n        layers_out: List[int],\n        dropout_cnn: float,\n        dropout_cnn_out: float,\n        dropout_lstm: float,\n        dropout_lstm_out: float,\n    ):\n        super(TEDD1104, self).__init__()\n\n        # Remember hyperparameters.\n        self.resnet: int = resnet\n        self.pretrained_resnet: bool = pretrained_resnet\n        self.sequence_size: int = sequence_size\n        self.embedded_size: int = embedded_size\n        self.hidden_size: int = hidden_size\n        self.num_layers_lstm: int = num_layers_lstm\n        self.bidirectional_lstm: bool = bidirectional_lstm\n        self.layers_out: List[int] = layers_out\n        self.dropout_cnn: float = dropout_cnn\n        self.dropout_cnn_out: float = dropout_cnn_out\n        self.dropout_lstm: float = dropout_lstm\n        self.dropout_lstm_out: float = dropout_lstm_out\n\n        self.EncoderCNN = EncoderCNN(\n            embedded_size=embedded_size,\n            dropout_cnn=dropout_cnn,\n            dropout_cnn_out=dropout_cnn_out,\n            resnet=resnet,\n            pretrained_resnet=pretrained_resnet,\n        )\n\n        self.PackFeatureVectors = PackFeatureVectors(sequence_size=sequence_size)\n\n        self.EncoderRNN = EncoderRNN(\n            embedded_size=embedded_size,\n            hidden_size=hidden_size,\n            num_layers=num_layers_lstm,\n            bidirectional_lstm=bidirectional_lstm,\n            dropout_lstm=dropout_lstm,\n            dropout_lstm_out=dropout_lstm_out,\n        )\n\n        self.OutputLayer = OutputLayer(\n            hidden_size=int(hidden_size * 2) if bidirectional_lstm else hidden_size,\n            layers=layers_out,\n        )\n\n    def forward(self, x):\n        x = self.EncoderCNN(x)\n        x = self.PackFeatureVectors(x)\n        x = self.EncoderRNN(x)\n        return self.OutputLayer(x)\n\n    def predict(self, x):\n        with torch.no_grad():\n            x = self.EncoderCNN.predict(x)\n            x = self.PackFeatureVectors.predict(x)\n            x = self.EncoderRNN.predict(x)\n            return self.OutputLayer.predict(x)\n\n\ndef save_model(model: TEDD1104, save_dir: str, fp16, amp_opt_level: str = None) -> None:\n    """"""\n    Save model to a directory. This function stores two files, the hyperparameters and the weights.\n\n    Input:\n     - model: TEDD1104 model to save\n     - save_dir: directory where the model will be saved, if it doesn\'t exists we create it\n     - amp: If the model uses FP16, Nvidia Apex AMP\n     - amp_opt_level: If the model uses FP16, the AMP opt_level\n\n    Output:\n\n    """"""\n\n    if fp16:\n        try:\n            from apex import amp\n        except ImportError:\n            raise ImportError(\n                ""Please install apex from https://www.github.com/nvidia/apex to use fp16 training.""\n            )\n\n    if not os.path.exists(save_dir):\n        os.makedirs(save_dir)\n\n    dict_hyperparams: dict = {\n        ""resnet"": model.resnet,\n        ""pretrained_resnet"": model.pretrained_resnet,\n        ""sequence_size"": model.sequence_size,\n        ""embedded_size"": model.embedded_size,\n        ""hidden_size"": model.hidden_size,\n        ""num_layers_lstm"": model.num_layers_lstm,\n        ""bidirectional_lstm"": model.bidirectional_lstm,\n        ""layers_out"": model.layers_out,\n        ""dropout_cnn"": model.dropout_cnn,\n        ""dropout_cnn_out"": model.dropout_cnn_out,\n        ""dropout_lstm"": model.dropout_lstm,\n        ""dropout_lstm_out"": model.dropout_lstm_out,\n        ""fp16"": fp16,\n        ""amp_opt_level"": amp_opt_level,\n    }\n\n    model_weights: dict = {\n        ""model"": model.state_dict(),\n        ""amp"": None if not fp16 else amp.state_dict(),\n    }\n\n    with open(os.path.join(save_dir, ""model_hyperparameters.json""), ""w+"") as file:\n        json.dump(dict_hyperparams, file)\n\n    torch.save(obj=model_weights, f=os.path.join(save_dir, ""model.bin""))\n\n\ndef load_model(save_dir: str, device: torch.device, fp16: bool) -> TEDD1104:\n    """"""\n    Load a model from directory. The directory should contain a json with the model hyperparameters and a bin file\n    with the model weights.\n\n    Input:\n     - save_dir: Directory where the model is stored\n\n    Output:\n    - TEDD1104 model\n    - fp16: True if the model uses FP16 else False\n\n    """"""\n\n    with open(os.path.join(save_dir, ""model_hyperparameters.json""), ""r"") as file:\n        dict_hyperparams = json.load(file)\n\n    model: TEDD1104 = TEDD1104(\n        resnet=dict_hyperparams[""resnet""],\n        pretrained_resnet=dict_hyperparams[""pretrained_resnet""],\n        sequence_size=dict_hyperparams[""sequence_size""],\n        embedded_size=dict_hyperparams[""embedded_size""],\n        hidden_size=dict_hyperparams[""hidden_size""],\n        num_layers_lstm=dict_hyperparams[""num_layers_lstm""],\n        bidirectional_lstm=dict_hyperparams[""bidirectional_lstm""],\n        layers_out=dict_hyperparams[""layers_out""],\n        dropout_cnn=dict_hyperparams[""dropout_cnn""],\n        dropout_cnn_out=dict_hyperparams[""dropout_cnn_out""],\n        dropout_lstm=dict_hyperparams[""dropout_lstm""],\n        dropout_lstm_out=dict_hyperparams[""dropout_lstm_out""],\n    ).to(device=device)\n\n    model_weights = torch.load(f=os.path.join(save_dir, ""model.bin""))\n\n    if fp16:\n        try:\n            from apex import amp\n        except ImportError:\n            raise ImportError(\n                ""You used the fp16 training flag but you don\'t seem have Nvidia Apex installed. ""\n                ""For using FP16, please install apex from https://www.github.com/nvidia/apex""\n            )\n\n        model = amp.initialize(model, opt_level=dict_hyperparams[""amp_opt_level""])\n        amp.load_state_dict(model_weights[""amp""])\n\n    model.load_state_dict(model_weights[""model""])\n\n    return model\n\n\ndef save_checkpoint(\n    path: str,\n    model: TEDD1104,\n    optimizer_name: str,\n    optimizer: torch.optim,\n    scheduler: torch.optim.lr_scheduler,\n    acc_dev: float,\n    epoch: int,\n    fp16: bool,\n    opt_level: str = None,\n) -> None:\n\n    """"""\n    Save a checkpoint that allows to continue training the model in the future\n\n    Input:\n     - path: path where the model is going to be saved\n     - model: TEDD1104 model to save\n     - optimizer_name: Name of the optimizer used for training: SGD or Adam\n     - optimizer: Optimizer used for training\n     - acc_dev: Accuracy of the model in the development set\n     - epoch: Num of epoch used to train the model\n     - amp: If the model uses FP16, Nvidia Apex AMP\n     - amp_opt_level: If the model uses FP16, the AMP opt_level\n\n    Output:\n    """"""\n\n    if fp16:\n        try:\n            from apex import amp\n        except ImportError:\n            raise ImportError(\n                ""Please install apex from https://www.github.com/nvidia/apex to use fp16 training.""\n            )\n\n    dict_hyperparams: dict = {\n        ""sequence_size"": model.sequence_size,\n        ""resnet"": model.resnet,\n        ""pretrained_resnet"": model.pretrained_resnet,\n        ""embedded_size"": model.embedded_size,\n        ""hidden_size"": model.hidden_size,\n        ""num_layers_lstm"": model.num_layers_lstm,\n        ""bidirectional_lstm"": model.bidirectional_lstm,\n        ""layers_out"": model.layers_out,\n        ""dropout_cnn"": model.dropout_cnn,\n        ""dropout_cnn_out"": model.dropout_cnn_out,\n        ""dropout_lstm"": model.dropout_lstm,\n        ""dropout_lstm_out"": model.dropout_lstm_out,\n        ""fp16"": fp16,\n        ""amp_opt_level"": opt_level,\n    }\n\n    checkpoint = {\n        ""hyper_params"": dict_hyperparams,\n        ""model"": model.state_dict(),\n        ""optimizer_name"": optimizer_name,\n        ""optimizer"": optimizer.state_dict(),\n        ""scheduler"": scheduler.state_dict(),\n        ""acc_dev"": acc_dev,\n        ""epoch"": epoch,\n        ""amp"": None if not fp16 else amp.state_dict(),\n        ""opt_level"": opt_level,\n    }\n\n    torch.save(checkpoint, path)\n\n\ndef load_checkpoint(\n    path: str, device: torch.device\n) -> (TEDD1104, str, torch.optim, torch.optim.lr_scheduler, float, int, bool, str):\n\n    """"""\n    Restore checkpoint\n\n    Input:\n    -path: path of the checkpoint to restore\n\n    Output:\n     - model: restored TEDD1104 model\n     - optimizer_name: Name of the optimizer used for training: SGD or Adam\n     - optimizer: Optimizer used for training\n     - acc_dev: Accuracy of the model in the development set\n     - epoch: Num of epoch used to train the model\n     - fp16: true if the model uses fp16 else false\n     - amp_opt_level: If the model uses FP16, the AMP opt_level\n    """"""\n\n    checkpoint = torch.load(path)\n    dict_hyperparams = checkpoint[""hyper_params""]\n    model_weights = checkpoint[""model""]\n    optimizer_name = checkpoint[""optimizer_name""]\n    optimizer_state = checkpoint[""optimizer""]\n    acc_dev = checkpoint[""acc_dev""]\n    epoch = checkpoint[""acc_dev""]\n    amp_state = checkpoint[""amp""]\n    opt_level = checkpoint[""opt_level""]\n    fp16 = dict_hyperparams[""fp16""]\n\n    model: TEDD1104 = TEDD1104(\n        resnet=dict_hyperparams[""resnet""],\n        pretrained_resnet=dict_hyperparams[""pretrained_resnet""],\n        sequence_size=dict_hyperparams[""sequence_size""],\n        embedded_size=dict_hyperparams[""embedded_size""],\n        hidden_size=dict_hyperparams[""hidden_size""],\n        num_layers_lstm=dict_hyperparams[""num_layers_lstm""],\n        bidirectional_lstm=dict_hyperparams[""bidirectional_lstm""],\n        layers_out=dict_hyperparams[""layers_out""],\n        dropout_cnn=dict_hyperparams[""dropout_cnn""],\n        dropout_cnn_out=dict_hyperparams[""dropout_cnn_out""],\n        dropout_lstm=dict_hyperparams[""dropout_lstm""],\n        dropout_lstm_out=dict_hyperparams[""dropout_lstm_out""],\n    ).to(device=device)\n\n    if optimizer_name == ""SGD"":\n        optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n    elif optimizer_name == ""Adam"":\n        optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n    else:\n        raise ValueError(\n            f""The optimizer you are trying to load is unknown: ""\n            f""Optimizer name {optimizer_name}. Available optimizers: SGD, Adam""\n        )\n\n    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, verbose=True)\n\n    model.load_state_dict(model_weights)\n    optimizer.load_state_dict(optimizer_state)\n    try:\n        scheduler_state = checkpoint[""scheduler""]\n        scheduler.load_state_dict(scheduler_state)\n    except KeyError:\n        print(f""Legacy checkpoint, a new scheduler will be created"")\n\n    if fp16:\n        try:\n            from apex import amp\n        except ImportError:\n            raise ImportError(\n                ""The model you are trying to load uses FP16 training.""\n                ""Please install Nvidia Apex from https://www.github.com/nvidia/apex""\n            )\n\n        model, optimizer = amp.initialize(model, optimizer, opt_level=opt_level)\n        amp.load_state_dict(amp_state)\n\n\n\n    return (\n        model,\n        optimizer_name,\n        optimizer,\n        scheduler,\n        acc_dev,\n        epoch,\n        fp16,\n        opt_level,\n    )\n'"
run_TEDD1104.py,4,"b'from model import load_model, TEDD1104\r\nfrom keyboard.inputsHandler import select_key\r\nfrom keyboard.getkeys import key_check, key_press\r\nimport argparse\r\nimport threading\r\nimport screen.record_screen as screen_recorder\r\nimport torch\r\nimport logging\r\nimport time\r\nfrom tkinter import *\r\nfrom utils import reshape_x, mse\r\nimport numpy as np\r\nimport cv2\r\nfrom segmentation.segmentation_coco import ImageSegmentation\r\n\r\nif torch.cuda.is_available():\r\n    device = torch.device(""cuda:0"")\r\nelse:\r\n    device = torch.device(""cpu"")\r\n    logging.warning(\r\n        ""GPU not found, using CPU, training will be very slow. CPU NOT COMPATIBLE WITH FP16""\r\n    )\r\n\r\n\r\ndef run_TED1104(\r\n    model_dir,\r\n    fp16: bool,\r\n    enable_evasion: bool,\r\n    show_current_control: bool,\r\n    num_parallel_sequences: int = 1,\r\n    evasion_score=1000,\r\n    enable_segmentation: bool = False,\r\n) -> None:\r\n    """"""\r\n    Generate dataset exampled from a human playing a videogame\r\n    HOWTO:\r\n        Set your game in windowed mode\r\n        Set your game to 1600x900 resolution\r\n        Move the game window to the top left corner, there should be a blue line of 1 pixel in the left bezel of your\r\n         screen and the window top bar should start in the top bezel of your screen.\r\n        Let the AI play the game!\r\n    Controls:\r\n        Push QE to exit\r\n        Push L to see the input images\r\n        Push and hold J to use to use manual control\r\n\r\n    Input:\r\n    - model_dir: Directory where the model to use is stored (model.bin and model_hyperparameters.json files)\r\n    - enable_evasion: automatic evasion maneuvers when the car gets stuck somewhere. Note: It adds computation time\r\n    - show_current_control: Show a window with text that indicates if the car is currently being driven by\r\n      the AI or a human\r\n    - num_parallel_sequences: num_parallel_sequences to record, is the number is larger the recorded sequence of images\r\n      will be updated faster and the model  will use more recent images as well as being able to do more iterations\r\n      per second. However if num_parallel_sequences is too high it wont be able to update the sequences with 1/10 secs\r\n      between images (default capturate to generate training examples).\r\n    -evasion_score: Mean squared error value between images to activate the evasion maneuvers\r\n    -enable_segmentation: Image segmentation will be performed using a pretrained model. Cars, persons, bikes.. will be\r\n     highlighted to help the model to identify them.\r\n\r\n    Output:\r\n\r\n    """"""\r\n    if enable_segmentation:\r\n        image_segmentation = ImageSegmentation(\r\n            model_name=""fcn_resnet101"", device=device, fp16=fp16\r\n        )\r\n    else:\r\n        image_segmentation = None\r\n\r\n    show_what_ai_sees: bool = False\r\n    fp16: bool\r\n    model: TEDD1104\r\n    model = load_model(save_dir=model_dir, device=device, fp16=fp16)\r\n    model.eval()\r\n    stop_recording: threading.Event = threading.Event()\r\n\r\n    th_img: threading.Thread = threading.Thread(\r\n        target=screen_recorder.img_thread, args=[stop_recording]\r\n    )\r\n    th_seq: threading.Thread = threading.Thread(\r\n        target=screen_recorder.multi_image_sequencer_thread,\r\n        args=[stop_recording, num_parallel_sequences],\r\n    )\r\n    th_img.setDaemon(True)\r\n    th_seq.setDaemon(True)\r\n    th_img.start()\r\n    # Wait to launch the image_sequencer_thread, it needs the img_thread to be running\r\n    time.sleep(5)\r\n    th_seq.start()\r\n\r\n    if show_current_control:\r\n        root = Tk()\r\n        var = StringVar()\r\n        var.set(""T.E.D.D. 1104 Driving"")\r\n        l = Label(root, textvariable=var, fg=""green"", font=(""Courier"", 44))\r\n        l.pack()\r\n\r\n    last_time: float = time.time()\r\n    model_prediction: np.ndarray = np.asarray([0])\r\n    score: np.float = np.float(0)\r\n    last_num: int = 0\r\n    while True:\r\n        while (\r\n            last_num == screen_recorder.num\r\n        ):  # Don\'t run the same sequence again, the resulted key will be the same\r\n            time.sleep(0.0001)\r\n        last_num = screen_recorder.num\r\n        if enable_segmentation:\r\n            img_seq: np.ndarray = image_segmentation.add_segmentation(\r\n                np.copy(screen_recorder.seq)\r\n            )\r\n        else:\r\n            img_seq: np.ndarray = np.copy(screen_recorder.seq)\r\n\r\n        keys = key_check()\r\n        if not ""J"" in keys:\r\n            X: torch.Tensor = torch.from_numpy(\r\n                reshape_x(np.array([img_seq]), fp=16 if fp16 else 32)\r\n            )\r\n            model_prediction = model.predict(X.to(device)).cpu().numpy()\r\n            select_key(int(model_prediction[0]))\r\n\r\n            if show_current_control:\r\n                var.set(""T.E.D.D. 1104 Driving"")\r\n                l.config(fg=""green"")\r\n                root.update()\r\n\r\n            if enable_evasion:\r\n                score = mse(img_seq[0], img_seq[4])\r\n                if score < evasion_score:\r\n                    if show_current_control:\r\n                        var.set(""Evasion maneuver"")\r\n                        l.config(fg=""blue"")\r\n                        root.update()\r\n                    select_key(4)\r\n                    time.sleep(1)\r\n                    if np.random.rand() > 0.5:\r\n                        select_key(6)\r\n                    else:\r\n                        select_key(8)\r\n                    time.sleep(0.2)\r\n                    if show_current_control:\r\n                        var.set(""T.E.D.D. 1104 Driving"")\r\n                        l.config(fg=""green"")\r\n                        root.update()\r\n\r\n        else:\r\n            if show_current_control:\r\n                var.set(""Manual Control"")\r\n                l.config(fg=""red"")\r\n                root.update()\r\n\r\n        if show_what_ai_sees:\r\n            cv2.imshow(""window1"", img_seq[0])\r\n            cv2.waitKey(1)\r\n            cv2.imshow(""window2"", img_seq[1])\r\n            cv2.waitKey(1)\r\n            cv2.imshow(""window3"", img_seq[2])\r\n            cv2.waitKey(1)\r\n            cv2.imshow(""window4"", img_seq[3])\r\n            cv2.waitKey(1)\r\n            cv2.imshow(""window5"", img_seq[4])\r\n            cv2.waitKey(1)\r\n\r\n        if ""Q"" in keys and ""E"" in keys:\r\n            print(""\\nStopping..."")\r\n            stop_recording.set()\r\n            th_seq.join()\r\n            th_img.join()\r\n            if show_what_ai_sees:\r\n                cv2.destroyAllWindows()\r\n\r\n            break\r\n\r\n        if ""L"" in keys:\r\n            time.sleep(0.1)  # Wait for key release\r\n            if show_what_ai_sees:\r\n                cv2.destroyAllWindows()\r\n                show_what_ai_sees = False\r\n            else:\r\n                show_what_ai_sees = True\r\n\r\n        time_it: float = time.time() - last_time\r\n        print(\r\n            f""Recording at {screen_recorder.fps} FPS\\n""\r\n            f""Actions per second {None if time_it==0 else 1/time_it}\\n""\r\n            f""Key predicted by nn: {key_press(model_prediction[0])}\\n""\r\n            f""Difference from img 1 to img 5 {None if not enable_evasion else score}\\n""\r\n            f""Push QE to exit\\n""\r\n            f""Push L to see the input images\\n""\r\n            f""Push J to use to use manual control\\n"",\r\n            end=""\\r"",\r\n        )\r\n\r\n        last_time = time.time()\r\n\r\n\r\nif __name__ == ""__main__"":\r\n\r\n    parser = argparse.ArgumentParser()\r\n\r\n    parser.add_argument(\r\n        ""--model_dir"",\r\n        type=str,\r\n        required=True,\r\n        help=""Directory where the model to use is stored"",\r\n    )\r\n\r\n    parser.add_argument(\r\n        ""--fp16"",\r\n        action=""store_true"",\r\n        help=""Use FP16 floating point precision: ""\r\n        ""Requires Nvidia Apex: https://www.github.com/nvidia/apex ""\r\n        ""and a modern Nvidia GPU FP16 capable (Volta, Turing and future architectures)."",\r\n    )\r\n\r\n    parser.add_argument(\r\n        ""--enable_evasion"",\r\n        action=""store_true"",\r\n        help=""Enable automatic evasion maneuvers when the car gets stuck somewhere. Note: It adds computation time"",\r\n    )\r\n\r\n    parser.add_argument(\r\n        ""--show_current_control"",\r\n        action=""store_true"",\r\n        help=""Show a window with text that indicates if the car is currently being driven by the AI or a human"",\r\n    )\r\n\r\n    parser.add_argument(\r\n        ""--num_parallel_sequences"",\r\n        type=int,\r\n        default=1,\r\n        help=""num_parallel_sequences to record, is the number is larger the recorded sequence of images will be ""\r\n        ""updated faster and the model  will use more recent images as well as being able to do more iterations ""\r\n        ""per second. However if num_parallel_sequences is too high it wont be able to update the sequences with ""\r\n        ""1/10 secs between images (default capturate to generate training examples). "",\r\n    )\r\n\r\n    parser.add_argument(\r\n        ""--evasion_score"",\r\n        type=float,\r\n        default=200,\r\n        help=""Mean squared error value between images to activate the evasion maneuvers"",\r\n    )\r\n\r\n    parser.add_argument(\r\n        ""--enable_segmentation"",\r\n        action=""store_true"",\r\n        help=""Image segmentation will be performed using a pretrained model. ""\r\n        ""Cars, persons, bikes.. will be highlighted to help the model to identify them. ""\r\n        ""Note: Segmentation will very significantly increase compuation time"",\r\n    )\r\n\r\n    args = parser.parse_args()\r\n\r\n    screen_recorder.initialize_global_variables()\r\n\r\n    run_TED1104(\r\n        model_dir=args.model_dir,\r\n        fp16=args.fp16,\r\n        enable_evasion=args.enable_evasion,\r\n        show_current_control=args.show_current_control,\r\n        num_parallel_sequences=args.num_parallel_sequences,\r\n        evasion_score=args.evasion_score,\r\n        enable_segmentation=args.enable_segmentation,\r\n    )\r\n'"
train.py,18,"b'from torch.nn import CrossEntropyLoss\nfrom utils import *\nfrom model import TEDD1104, save_model, load_checkpoint, save_checkpoint\nimport torch\nimport torch.optim as optim\nfrom typing import List\nimport time\nimport argparse\nimport random\nimport math\nfrom torch.utils.tensorboard import SummaryWriter\nfrom DataLoader import DataLoaderTEDD\n\nif torch.cuda.is_available():\n    device: torch.device = torch.device(""cuda:0"")\nelse:\n    device: torch.device = torch.device(""cpu"")\n    logging.warning(\n        ""GPU not found, using CPU, training will be very slow. CPU NOT COMPATIBLE WITH FP16""\n    )\n\n\ndef train(\n    model: TEDD1104,\n    optimizer_name: str,\n    optimizer: torch.optim,\n    scheduler: torch.optim.lr_scheduler,\n    train_dir: str,\n    dev_dir: str,\n    test_dir: str,\n    output_dir: str,\n    batch_size: int,\n    accumulation_steps: int,\n    initial_epoch: int,\n    num_epoch: int,\n    max_acc: float,\n    hide_map_prob: float,\n    dropout_images_prob: List[float],\n    num_load_files_training: int,\n    fp16: bool = True,\n    amp_opt_level=None,\n    save_checkpoints: bool = True,\n    eval_every: int = 5,\n    save_every: int = 20,\n    save_best: bool = True,\n):\n\n    """"""\n    Train a model\n\n    Input:\n    - model: TEDD1104 model to train\n    - optimizer_name: Name of the optimizer to use [SGD, Adam]\n    - optimizer: Optimizer (torch.optim)\n    - train_dir: Directory where the train files are stored\n    - dev_dir: Directory where the development files are stored\n    - test_dir: Directory where the test files are stored\n    - output_dir: Directory where the model and the checkpoints are going to be saved\n    - batch_size: Batch size (Around 10 for 8GB GPU)\n    - initial_epoch: Number of previous epochs used to train the model (0 unless the model has been\n      restored from checkpoint)\n    - num_epochs: Number of epochs to do\n    - max_acc: Accuracy in the development set (0 unless the model has been\n      restored from checkpoint)\n    - hide_map_prob: Probability for removing the minimap (put a black square)\n       from a training example (0<=hide_map_prob<=1)\n    - dropout_images_prob List of 5 floats or None, probability for removing each input image during training\n     (black image) from a training example (0<=dropout_images_prob<=1)\n    - fp16: Use FP16 for training\n    - amp_opt_level: If FP16 training Nvidia apex opt level\n    - save_checkpoints: save a checkpoint each epoch (Each checkpoint will rewrite the previous one)\n    - save_best: save the model that achieves the higher accuracy in the development set\n\n    Output:\n     - float: Accuracy in the development test of the best model\n    """"""\n    writer: SummaryWriter = SummaryWriter()\n\n    if fp16:\n        try:\n            from apex import amp\n        except ImportError:\n            raise ImportError(\n                ""Please install apex from https://www.github.com/nvidia/apex to use fp16 training.""\n            )\n\n    criterion: CrossEntropyLoss = torch.nn.CrossEntropyLoss()\n    print(""Loading dev set"")\n    X_dev, y_dev = load_dataset(dev_dir, fp=16 if fp16 else 32)\n    X_dev = torch.from_numpy(X_dev)\n    print(""Loading test set"")\n    X_test, y_test = load_dataset(test_dir, fp=16 if fp16 else 32)\n    X_test = torch.from_numpy(X_test)\n    total_training_exampels: int = 0\n    model.zero_grad()\n\n    printTrace(""Training..."")\n    for epoch in range(num_epoch):\n        step_no: int = 0\n        iteration_no: int = 0\n        num_used_files: int = 0\n        data_loader = DataLoaderTEDD(\n            dataset_dir=train_dir,\n            nfiles2load=num_load_files_training,\n            hide_map_prob=hide_map_prob,\n            dropout_images_prob=dropout_images_prob,\n            fp=16 if fp16 else 32,\n        )\n\n        data = data_loader.get_next()\n        # Get files in batches, all files will be loaded and data will be shuffled\n        while data:\n            X, y = data\n            model.train()\n            start_time: float = time.time()\n            total_training_exampels += len(y)\n            running_loss: float = 0.0\n            num_batchs: int = 0\n            acc_dev: float = 0.0\n\n            for X_bacth, y_batch in nn_batchs(X, y, batch_size):\n                X_bacth, y_batch = (\n                    torch.from_numpy(X_bacth).to(device),\n                    torch.from_numpy(y_batch).long().to(device),\n                )\n\n                outputs = model.forward(X_bacth)\n                loss = criterion(outputs, y_batch) / accumulation_steps\n                running_loss += loss.item()\n\n                if fp16:\n                    with amp.scale_loss(loss, optimizer) as scaled_loss:\n                        scaled_loss.backward()\n                else:\n                    loss.backward()\n\n                if fp16:\n                    torch.nn.utils.clip_grad_norm_(amp.master_params(optimizer), 1.0)\n                else:\n                    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n\n                if (step_no + 1) % accumulation_steps or (\n                    num_used_files + 1 > len(data_loader) - num_load_files_training\n                    and num_batchs == math.ceil(len(y) / batch_size) - 1\n                ):  # If we are in the last bach of the epoch we also want to perform gradient descent\n                    optimizer.step()\n                    model.zero_grad()\n\n                num_batchs += 1\n                step_no += 1\n\n            num_used_files += num_load_files_training\n\n            # Print Statistics\n            printTrace(\n                f""EPOCH: {initial_epoch+epoch}. Iteration {iteration_no}. ""\n                f""{num_used_files} of {len(data_loader)} files. ""\n                f""Total examples used for training {total_training_exampels}. ""\n                f""Iteration time: {round(time.time() - start_time,2)} secs.""\n            )\n            printTrace(\n                f""Loss: {-1 if num_batchs == 0 else running_loss / num_batchs}. ""\n                f""Learning rate {optimizer.state_dict()[\'param_groups\'][0][\'lr\']}""\n            )\n            writer.add_scalar(""Loss/train"", running_loss / num_batchs, iteration_no)\n\n            scheduler.step(running_loss / num_batchs)\n\n            if (iteration_no + 1) % eval_every == 0:\n                start_time_eval: float = time.time()\n                if len(X) > 0 and len(y) > 0:\n                    acc_train: float = evaluate(\n                        model=model,\n                        X=torch.from_numpy(X),\n                        golds=y,\n                        device=device,\n                        batch_size=batch_size,\n                    )\n                else:\n                    acc_train = -1.0\n\n                acc_dev: float = evaluate(\n                    model=model,\n                    X=X_dev,\n                    golds=y_dev,\n                    device=device,\n                    batch_size=batch_size,\n                )\n\n                acc_test: float = evaluate(\n                    model=model,\n                    X=X_test,\n                    golds=y_test,\n                    device=device,\n                    batch_size=batch_size,\n                )\n\n                printTrace(\n                    f""Acc training set: {round(acc_train,2)}. ""\n                    f""Acc dev set: {round(acc_dev,2)}. ""\n                    f""Acc test set: {round(acc_test,2)}.  ""\n                    f""Eval time: {round(time.time() - start_time_eval,2)} secs.""\n                )\n\n                if 0.0 < acc_dev > max_acc and save_best:\n                    max_acc = acc_dev\n                    printTrace(\n                        f""New max acc in dev set {round(max_acc,2)}. Saving model...""\n                    )\n                    save_model(\n                        model=model,\n                        save_dir=output_dir,\n                        fp16=fp16,\n                        amp_opt_level=amp_opt_level,\n                    )\n                if acc_train > -1:\n                    writer.add_scalar(""Accuracy/train"", acc_train, iteration_no)\n                writer.add_scalar(""Accuracy/dev"", acc_dev, iteration_no)\n                writer.add_scalar(""Accuracy/test"", acc_test, iteration_no)\n\n            if save_checkpoints and (iteration_no + 1) % save_every == 0:\n                printTrace(""Saving checkpoint..."")\n                save_checkpoint(\n                    path=os.path.join(output_dir, ""checkpoint.pt""),\n                    model=model,\n                    optimizer_name=optimizer_name,\n                    optimizer=optimizer,\n                    scheduler=scheduler,\n                    acc_dev=acc_dev,\n                    epoch=initial_epoch + epoch,\n                    fp16=fp16,\n                    opt_level=amp_opt_level,\n                )\n\n            iteration_no += 1\n            data = data_loader.get_next()\n\n        data_loader.close()\n\n    return max_acc\n\n\ndef train_new_model(\n    train_dir=""Data\\\\GTAV-AI\\\\data-v2\\\\train\\\\"",\n    dev_dir=""Data\\\\GTAV-AI\\\\data-v2\\\\dev\\\\"",\n    test_dir=""Data\\\\GTAV-AI\\\\data-v2\\\\test\\\\"",\n    output_dir=""Data\\\\models\\\\"",\n    batch_size=10,\n    accumulation_steps: int = 1,\n    num_epoch=20,\n    optimizer_name=""SGD"",\n    learning_rate: float = 0.01,\n    scheduler_patience: int = 100,\n    resnet: int = 18,\n    pretrained_resnet: bool = True,\n    sequence_size: int = 5,\n    embedded_size: int = 256,\n    hidden_size: int = 128,\n    num_layers_lstm: int = 1,\n    bidirectional_lstm: bool = False,\n    layers_out: List[int] = None,\n    dropout_cnn: float = 0.1,\n    dropout_cnn_out: float = 0.1,\n    dropout_lstm: float = 0.1,\n    dropout_lstm_out: float = 0.1,\n    hide_map_prob: float = 0.0,\n    dropout_images_prob=None,\n    num_load_files_training: int = 5,\n    fp16=True,\n    apex_opt_level=""O2"",\n    save_checkpoints=True,\n    eval_every: int = 5,\n    save_every: int = 20,\n    save_best=True,\n):\n\n    """"""\n    Train a new model\n\n    Input:\n    - train_dir: Directory where the train files are stored\n    - dev_dir: Directory where the development files are stored\n    - test_dir: Directory where the test files are stored\n    - output_dir: Directory where the model and the checkpoints are going to be saved\n    - batch_size: Batch size (Around 10 for 8GB GPU)\n    - num_epochs: Number of epochs to do\n    - optimizer_name: Name of the optimizer to use [SGD, Adam]\n    - optimizer: Optimizer (torch.optim)\n    - resnet: resnet module to use [18,34,50,101,152]\n    - pretrained_resnet: Load pretrained resnet weights\n    - sequence_size: Length of each series of features\n    - embedded_size: Size of the feature vectors\n    - hidden_size: LSTM hidden size\n    - num_layers_lstm: number of layers in the LSTM\n    - bidirectional_lstm: forward or bidirectional LSTM\n    - layers_out: list of integer, for each integer i a linear layer with i neurons will be added.\n    - dropout_cnn: dropout probability for the CNN layers\n    - dropout_cnn_out: dropout probability for the cnn features (output layer)\n    - dropout_lstm: dropout probability for the LSTM\n    - dropout_lstm_out: dropout probability for the LSTM features (output layer)\n    - hide_map_prob: Probability for removing the minimap (put a black square)\n      from a training example (0<=hide_map_prob<=1)\n    - dropout_images_prob List of 5 floats or None, probability for removing each input image during training\n     (black image) from a training example (0<=dropout_images_prob<=1)\n    - fp16: Use FP16 for training\n    - amp_opt_level: If FP16 training Nvidia apex opt level\n    - save_checkpoints: save a checkpoint each epoch (Each checkpoint will rewrite the previous one)\n    - save_best: save the model that achieves the higher accuracy in the development set\n\n    Output:\n\n    """"""\n\n    print(""Loading new model"")\n    model: TEDD1104 = TEDD1104(\n        resnet=resnet,\n        pretrained_resnet=pretrained_resnet,\n        sequence_size=sequence_size,\n        embedded_size=embedded_size,\n        hidden_size=hidden_size,\n        num_layers_lstm=num_layers_lstm,\n        bidirectional_lstm=bidirectional_lstm,\n        layers_out=layers_out,\n        dropout_cnn=dropout_cnn,\n        dropout_cnn_out=dropout_cnn_out,\n        dropout_lstm=dropout_lstm,\n        dropout_lstm_out=dropout_lstm_out,\n    ).to(device)\n\n    if optimizer_name == ""SGD"":\n        optimizer = optim.SGD(\n            model.parameters(), lr=learning_rate, momentum=0.9, nesterov=True\n        )\n    elif optimizer_name == ""Adam"":\n        optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=0.01)\n    else:\n        raise ValueError(\n            f""Optimizer {optimizer_name} not implemented. Available optimizers: SGD, Adam""\n        )\n\n    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n        optimizer, verbose=True, patience=scheduler_patience, factor=0.5\n    )\n\n    if fp16:\n        try:\n            from apex import amp\n        except ImportError:\n            raise ImportError(\n                ""Please install apex from https://www.github.com/nvidia/apex to use fp16 training.""\n            )\n\n        model, optimizer = amp.initialize(\n            model,\n            optimizer,\n            opt_level=apex_opt_level,\n            keep_batchnorm_fp32=True,\n            loss_scale=""dynamic"",\n        )\n\n    max_acc = train(\n        model=model,\n        optimizer_name=optimizer_name,\n        optimizer=optimizer,\n        scheduler=scheduler,\n        train_dir=train_dir,\n        dev_dir=dev_dir,\n        test_dir=test_dir,\n        output_dir=output_dir,\n        batch_size=batch_size,\n        accumulation_steps=accumulation_steps,\n        initial_epoch=0,\n        num_epoch=num_epoch,\n        max_acc=0.0,\n        hide_map_prob=hide_map_prob,\n        dropout_images_prob=dropout_images_prob,\n        num_load_files_training=num_load_files_training,\n        fp16=fp16,\n        amp_opt_level=apex_opt_level if fp16 else None,\n        save_checkpoints=save_checkpoints,\n        eval_every=eval_every,\n        save_every=save_every,\n        save_best=save_best,\n    )\n\n    print(f""Training finished, max accuracy in the development set {max_acc}"")\n\n\ndef continue_training(\n    checkpoint_path: str,\n    train_dir: str = ""Data\\\\GTAV-AI\\\\data-v2\\\\train\\\\"",\n    dev_dir: str = ""Data\\\\GTAV-AI\\\\data-v2\\\\dev\\\\"",\n    test_dir: str = ""Data\\\\GTAV-AI\\\\data-v2\\\\test\\\\"",\n    output_dir: str = ""Data\\\\models\\\\"",\n    batch_size: int = 10,\n    accumulation_steps: int = 1,\n    num_epoch: int = 20,\n    hide_map_prob: float = 0.0,\n    dropout_images_prob: List[float] = None,\n    num_load_files_training: int = 5,\n    save_checkpoints=True,\n    eval_every: int = 5,\n    save_every: int = 100,\n    save_best=True,\n):\n\n    """"""\n    Load a checkpoint and continue training, we will restore the model, the optimizer and the nvidia apex data if\n    the model was trained using fp16. Note: If the model was trained using fp16 it cannot be restored as an fp32\n    model and vice versa. The floating point precision used for training the model will be restored automatically\n    from the checkpoint.\n\n    Input:\n    - checkpoint_path: Path of the checkpoint to restore\n    - train_dir: Directory where the train files are stored\n    - dev_dir: Directory where the development files are stored\n    - test_dir: Directory where the test files are stored\n    - output_dir: Directory where the model and the checkpoints are going to be saved\n    - batch_size: Batch size (Around 10 for 8GB GPU)\n    - num_epochs: Number of epochs to do\n    - optimizer_name: Name of the optimizer to use [SGD, Adam]\n    - hide_map_prob: Probability for removing the minimap (put a black square)\n      from a training example (0<=hide_map_prob<=1)\n    -Probability for removing each input image during training (black image)\n     from a training example (0<=dropout_images_prob<=1)\n    - save_checkpoints: save a checkpoint each epoch (Each checkpoint will rewrite the previous one)\n    - save_best: save the model that achieves the higher accuracy in the development set\n\n    Output:\n\n    """"""\n\n    (\n        model,\n        optimizer_name,\n        optimizer,\n        scheduler,\n        acc_dev,\n        epoch,\n        fp16,\n        opt_level,\n    ) = load_checkpoint(checkpoint_path, device)\n    model = model.to(device)\n\n    max_acc = train(\n        model=model,\n        optimizer_name=optimizer_name,\n        optimizer=optimizer,\n        scheduler=scheduler,\n        train_dir=train_dir,\n        dev_dir=dev_dir,\n        test_dir=test_dir,\n        output_dir=output_dir,\n        batch_size=batch_size,\n        accumulation_steps=accumulation_steps,\n        initial_epoch=epoch,\n        num_epoch=num_epoch,\n        max_acc=acc_dev,\n        hide_map_prob=hide_map_prob,\n        dropout_images_prob=dropout_images_prob,\n        num_load_files_training=num_load_files_training,\n        fp16=fp16,\n        amp_opt_level=opt_level if fp16 else None,\n        save_checkpoints=save_checkpoints,\n        eval_every=eval_every,\n        save_every=save_every,\n        save_best=save_best,\n    )\n\n    print(f""Training finished, max accuracy in the development set {max_acc}"")\n\n\nif __name__ == ""__main__"":\n    parser = argparse.ArgumentParser()\n\n    group = parser.add_mutually_exclusive_group(required=True)\n    group.add_argument(\n        ""--train_new"", action=""store_true"", help=""Train a new model"",\n    )\n\n    group.add_argument(\n        ""--continue_training"",\n        action=""store_true"",\n        help=""Restore a checkpoint and continue training"",\n    )\n\n    parser.add_argument(\n        ""--train_dir"",\n        type=str,\n        required=True,\n        help=""Directory containing the train files"",\n    )\n\n    parser.add_argument(\n        ""--dev_dir"",\n        type=str,\n        required=True,\n        help=""Directory containing the development files"",\n    )\n\n    parser.add_argument(\n        ""--test_dir"",\n        type=str,\n        required=True,\n        help=""Directory containing the test files"",\n    )\n\n    parser.add_argument(\n        ""--output_dir"",\n        type=str,\n        required=True,\n        help=""Directory where the model and checkpoints are going to be saved"",\n    )\n\n    parser.add_argument(\n        ""--batch_size"",\n        type=int,\n        required=True,\n        help=""batch size for training (10 for a 8GB GPU seems fine)"",\n    )\n\n    parser.add_argument(\n        ""--gradient_accumulation_steps"",\n        type=int,\n        default=1,\n        help=""Number of gradient steps to accumulate. True batch size =  --batch_size * --accumulation_steps"",\n    )\n\n    parser.add_argument(\n        ""--num_epochs"", type=int, required=True, help=""Number of epochs to perform"",\n    )\n\n    parser.add_argument(\n        ""--hide_map_prob"",\n        type=float,\n        default=0.0,\n        help=""Probability for removing the minimap (put a black square) from a training example (0<=hide_map_prob<=1)"",\n    )\n\n    parser.add_argument(\n        ""--dropout_images_prob"",\n        type=float,\n        nargs=5,\n        default=[0.0, 0.0, 0.0, 0.0, 0.0],\n        help=""List of 5 floats. Probability for removing each input image during training (black image) ""\n        ""from a training example (0<=dropout_images_prob<=1) "",\n    )\n\n    parser.add_argument(\n        ""--num_load_files_training"",\n        type=int,\n        default=5,\n        help=""Number of dataset files to load each iteration for training. Files will be merged and shuffled. Loading""\n        ""more may be helpful for training, but it will use more RAM"",\n    )\n\n    parser.add_argument(\n        ""--not_save_checkpoints"",\n        action=""store_false"",\n        help=""Do NOT save a checkpoint each epoch (Each checkpoint will rewrite the previous one)"",\n    )\n\n    parser.add_argument(\n        ""--eval_every"",\n        type=int,\n        default=5,\n        help=""Evaluate the model every --eval_every iterations (1 iteration = --num_load_files_training files used) "",\n    )\n\n    parser.add_argument(\n        ""--save_every"",\n        type=int,\n        default=20,\n        help=""Save the model every --save_every iterations (1 iteration = --num_load_files_training files used) "",\n    )\n\n    parser.add_argument(\n        ""--not_save_best"",\n        action=""store_false"",\n        help=""Dot NOT save the best model in the development set"",\n    )\n\n    parser.add_argument(\n        ""--fp16"",\n        action=""store_true"",\n        help=""[new_model] Use FP16 floating point precision: ""\n        ""Requires Nvidia Apex: https://www.github.com/nvidia/apex ""\n        ""and a modern Nvidia GPU FP16 capable (Volta, Turing and future architectures).""\n        ""If you restore a checkpoint the original FP configuration of the model will be restored."",\n    )\n\n    parser.add_argument(\n        ""--amp_opt_level"",\n        type=str,\n        default=""O2"",\n        help=""[new_model] If FP16 training, the Apex OPT level"",\n    )\n\n    parser.add_argument(\n        ""--optimizer_name"",\n        type=str,\n        default=""SGD"",\n        choices=[""SGD"", ""Adam""],\n        help=""[new_model] Optimizer to use for training a new model: SGD or Adam"",\n    )\n\n    parser.add_argument(\n        ""--learning_rate"",\n        type=float,\n        default=0.01,\n        help=""[new_model] Optimizer learning rate"",\n    )\n\n    parser.add_argument(\n        ""--scheduler_patience"",\n        type=int,\n        default=100,\n        help=""[new_model] Number of steps where the loss does not decrease until decrease the learning rate"",\n    )\n\n    parser.add_argument(\n        ""--resnet"",\n        type=int,\n        default=18,\n        choices=[18, 34, 50, 101, 152],\n        help=""[new_model] Which of the resnet model availabel in torchvision.models use. Availabel model:""\n        ""18, 34, 50, 101 and 152."",\n    )\n\n    parser.add_argument(\n        ""--do_not_load_pretrained_resnet"",\n        action=""store_false"",\n        help=""[new_model] Do not load the pretrained weights for the resnet model"",\n    )\n\n    parser.add_argument(\n        ""--sequence_size"",\n        type=int,\n        default=5,\n        help=""[new_model] Number of images to use to decide witch key press. Note: Only 5 supported for right now"",\n    )\n\n    parser.add_argument(\n        ""--embedded_size"",\n        type=int,\n        default=256,\n        help=""[new_model] Size of the feature vectors (CNN encoder output size)"",\n    )\n\n    parser.add_argument(""--hidden_size"", type=int, default=128, help=""LSTM hidden size"")\n\n    parser.add_argument(\n        ""--num_layers_lstm"",\n        type=int,\n        default=1,\n        help=""[new_model] number of layers in the LSTM"",\n    )\n\n    parser.add_argument(\n        ""--bidirectional_lstm"",\n        action=""store_true"",\n        help=""[new_model] Use a bidirectional LSTM instead of a forward LSTM"",\n    )\n\n    parser.add_argument(\n        ""--layers_out"",\n        nargs=""+"",\n        type=int,\n        required=False,\n        help=""[new_model] list of integer, for each integer i a linear layer with i neurons will be added to the ""\n        "" output, if none layers are provided the ouput layer will be just a linear layer with input size hidden_size ""\n        ""and output size 9. Note: The input size of the first layer and last layer will automatically be added ""\n        ""regardless of the user input, so you don\'t need to care about the size of these layers. "",\n    )\n\n    parser.add_argument(\n        ""--dropout_cnn"",\n        type=float,\n        default=0.1,\n        help=""[new_model] Dropout of the CNN layers between 0.0 and 1.0"",\n    )\n\n    parser.add_argument(\n        ""--dropout_cnn_out"",\n        type=float,\n        default=0.1,\n        help=""[new_model] Dropout of the CNN representations (output layer) between 0.0 and 1.0"",\n    )\n\n    parser.add_argument(\n        ""--dropout_lstm"",\n        type=float,\n        default=0.1,\n        help=""[new_model] Dropout of the LSTM layer between 0.0 and 1.0"",\n    )\n\n    parser.add_argument(\n        ""--dropout_lstm_out"",\n        type=float,\n        default=0.1,\n        help=""[new_model] Dropout of the LSTM representations (output layer) between 0.0 and 1.0"",\n    )\n\n    parser.add_argument(\n        ""--checkpoint_path"",\n        type=str,\n        help=""[continue_training] Path of the checkpoint to load for continue training it"",\n    )\n\n    args = parser.parse_args()\n\n    if args.train_new:\n        train_new_model(\n            train_dir=args.train_dir,\n            dev_dir=args.dev_dir,\n            test_dir=args.test_dir,\n            output_dir=args.output_dir,\n            batch_size=args.batch_size,\n            accumulation_steps=args.gradient_accumulation_steps,\n            num_epoch=args.num_epochs,\n            hide_map_prob=args.hide_map_prob,\n            dropout_images_prob=args.dropout_images_prob,\n            num_load_files_training=args.num_load_files_training,\n            optimizer_name=args.optimizer_name,\n            learning_rate=args.learning_rate,\n            scheduler_patience=args.scheduler_patience,\n            resnet=args.resnet,\n            pretrained_resnet=args.do_not_load_pretrained_resnet,\n            sequence_size=args.sequence_size,\n            embedded_size=args.embedded_size,\n            hidden_size=args.hidden_size,\n            num_layers_lstm=args.num_layers_lstm,\n            bidirectional_lstm=args.bidirectional_lstm,\n            layers_out=args.layers_out,\n            dropout_cnn=args.dropout_cnn,\n            dropout_cnn_out=args.dropout_cnn_out,\n            dropout_lstm=args.dropout_lstm,\n            dropout_lstm_out=args.dropout_lstm_out,\n            fp16=args.fp16,\n            apex_opt_level=args.amp_opt_level,\n            save_checkpoints=args.not_save_checkpoints,\n            eval_every=args.eval_every,\n            save_every=args.save_every,\n            save_best=args.not_save_best,\n        )\n\n    else:\n        continue_training(\n            checkpoint_path=args.checkpoint_path,\n            train_dir=args.train_dir,\n            dev_dir=args.dev_dir,\n            test_dir=args.test_dir,\n            output_dir=args.output_dir,\n            batch_size=args.batch_size,\n            accumulation_steps=args.gradient_accumulation_steps,\n            hide_map_prob=args.hide_map_prob,\n            dropout_images_prob=args.dropout_images_prob,\n            num_load_files_training=args.num_load_files_training,\n            save_checkpoints=args.not_save_checkpoints,\n            eval_every=args.eval_every,\n            save_every=args.save_every,\n            save_best=args.not_save_best,\n        )\n'"
utils.py,3,"b'import numpy as np\r\nimport logging\r\nfrom typing import Iterable, Sized, List, Set\r\n\r\nfrom model import TEDD1104\r\nimport glob\r\nimport datetime\r\nimport torch\r\nimport os\r\nimport random\r\n\r\n\r\ntry:\r\n    import cupy as cp\r\n\r\n    cupy = True\r\nexcept ModuleNotFoundError:\r\n    cupy = False\r\n    logging.warning(\r\n        ""Cupy not found, dataset preprocessing is going to be slow. ""\r\n        ""Installing copy is highly recommended (x10 speedup): ""\r\n        ""https://docs-cupy.chainer.org/en/latest/install.html?highlight=cuda90#install-cupy""\r\n    )\r\n\r\n\r\ndef check_valid_y(data: np.ndarray) -> bool:\r\n    """"""\r\n    Check if any key has been pressed in the datased. Some files may not have any key recorded due to windows\r\n    permission errors on some computers, people not using WASD or other problems, we want to discard these files.\r\n    Input:\r\n     - data: ndarray [num_examples x 6]\r\n    Output:\r\n    - Bool: True if the file is valid, False is there no key recorded\r\n    """"""\r\n    seen_keys: Set[int] = set()\r\n    for i in range(0, data.shape[0]):\r\n        if np.array_equal(data[i][5], [0, 0, 0, 0]):\r\n            seen_keys.add(0)\r\n        elif np.array_equal(data[i][5], [1, 0, 0, 0]):\r\n            seen_keys.add(1)\r\n        elif np.array_equal(data[i][5], [0, 1, 0, 0]):\r\n            seen_keys.add(2)\r\n        elif np.array_equal(data[i][5], [0, 0, 1, 0]):\r\n            seen_keys.add(3)\r\n        elif np.array_equal(data[i][5], [0, 0, 0, 1]):\r\n            seen_keys.add(4)\r\n        elif np.array_equal(data[i][5], [1, 0, 1, 0]):\r\n            seen_keys.add(5)\r\n        elif np.array_equal(data[i][5], [1, 0, 0, 1]):\r\n            seen_keys.add(6)\r\n        elif np.array_equal(data[i][5], [0, 1, 1, 0]):\r\n            seen_keys.add(7)\r\n        elif np.array_equal(data[i][5], [0, 1, 0, 1]):\r\n            seen_keys.add(8)\r\n\r\n        if len(seen_keys) >= 3:\r\n            return True\r\n\r\n    else:\r\n        return False\r\n\r\n\r\ndef reshape_y(data: np.ndarray) -> np.ndarray:\r\n    """"""\r\n    Get gold values from data. multi-hot vector to one-hot vector\r\n    Input:\r\n     - data: ndarray [num_examples x 6]\r\n    Output:\r\n    - ndarray [num_examples]\r\n\r\n    """"""\r\n    reshaped = np.zeros(data.shape[0], dtype=np.int16)\r\n    for i in range(0, data.shape[0]):\r\n        if np.array_equal(data[i][5], [0, 0, 0, 0]):\r\n            reshaped[i] = 0\r\n        elif np.array_equal(data[i][5], [1, 0, 0, 0]):\r\n            reshaped[i] = 1\r\n        elif np.array_equal(data[i][5], [0, 1, 0, 0]):\r\n            reshaped[i] = 2\r\n        elif np.array_equal(data[i][5], [0, 0, 1, 0]):\r\n            reshaped[i] = 3\r\n        elif np.array_equal(data[i][5], [0, 0, 0, 1]):\r\n            reshaped[i] = 4\r\n        elif np.array_equal(data[i][5], [1, 0, 1, 0]):\r\n            reshaped[i] = 5\r\n        elif np.array_equal(data[i][5], [1, 0, 0, 1]):\r\n            reshaped[i] = 6\r\n        elif np.array_equal(data[i][5], [0, 1, 1, 0]):\r\n            reshaped[i] = 7\r\n        elif np.array_equal(data[i][5], [0, 1, 0, 1]):\r\n            reshaped[i] = 8\r\n    return reshaped\r\n\r\n\r\ndef reshape_x_numpy(\r\n    data: np.ndarray,\r\n    dtype=np.float16,\r\n    hide_map_prob: float = 0.0,\r\n    dropout_images_prob: List[float] = None,\r\n) -> np.ndarray:\r\n    """"""\r\n    Get images from data as a list and preprocess them.\r\n    Input:\r\n     - data: ndarray [num_examples x 6]\r\n     -dtype: numpy dtype for the output array\r\n     -hide_map_prob: Probability for removing the minimap (black square)\r\n      from the sequence of images (0<=hide_map_prob<=1)\r\n    - dropout_images_prob List of 5 floats or None, probability for removing each input image during training\r\n     (black image) from a training example (0<=dropout_images_prob<=1)\r\n    Output:\r\n    - ndarray [num_examples * 5, num_channels, H, W]\r\n    """"""\r\n    mean = np.array([0.485, 0.456, 0.406], dtype)\r\n    std = np.array([0.229, 0.224, 0.225], dtype)\r\n    reshaped = np.zeros((len(data) * 5, 3, 270, 480), dtype=dtype)\r\n    for i in range(0, len(data)):\r\n        black_minimap: bool = (random.random() <= hide_map_prob)\r\n        for j in range(0, 5):\r\n            black_image: bool = False\r\n            if dropout_images_prob is not None:\r\n                black_image = random.random() <= dropout_images_prob[j]\r\n            if black_image:\r\n                reshaped[i * 5 + j] = np.zeros(\r\n                    (3, data[i][j].shape[0], data[i][j].shape[1]), dtype=dtype\r\n                )\r\n            else:\r\n                img = np.array(data[i][j], dtype=dtype)\r\n                if black_minimap:  # Put a black square over the minimap\r\n                    img[215:, :80] = np.zeros((55, 80, 3), dtype=dtype)\r\n\r\n                reshaped[i * 5 + j] = np.rollaxis(\r\n                    (img / dtype(255.0)) - mean / std, 2, 0\r\n                )\r\n\r\n    return reshaped\r\n\r\n\r\ndef reshape_x_cupy(\r\n    data: np.ndarray,\r\n    dtype=cp.float16,\r\n    hide_map_prob: float = 0.0,\r\n    dropout_images_prob: List[float] = None,\r\n) -> np.ndarray:\r\n    """"""\r\n    Get images from data as a list and preprocess them (using GPU).\r\n    Input:\r\n     - data: ndarray [num_examples x 6]\r\n     -dtype: numpy dtype for the output array\r\n     -hide_map_prob: Probability for removing the minimap (black square)\r\n      from the sequence of images (0<=hide_map_prob<=1)\r\n    Output:\r\n    - ndarray [num_examples * 5, num_channels, H, W]\r\n\r\n    """"""\r\n\r\n    mean = cp.array([0.485, 0.456, 0.406], dtype=dtype)\r\n    std = cp.array([0.229, 0.224, 0.225], dtype=dtype)\r\n    reshaped = np.zeros((len(data) * 5, 3, 270, 480), dtype=dtype)\r\n    for i in range(0, len(data)):\r\n        black_minimap: bool = (random.random() <= hide_map_prob)\r\n        for j in range(0, 5):\r\n            black_image: bool = False\r\n            if dropout_images_prob is not None:\r\n                black_image = random.random() <= dropout_images_prob[j]\r\n            if black_image:\r\n                reshaped[i * 5 + j] = np.zeros(\r\n                    (3, data[i][j].shape[0], data[i][j].shape[1]), dtype=dtype\r\n                )\r\n            else:\r\n                img = cp.array(data[i][j], dtype=dtype)\r\n                if black_minimap:  # Put a black square over the minimap\r\n                    img[215:, :80] = cp.zeros((55, 80, 3), dtype=dtype)\r\n\r\n                reshaped[i * 5 + j] = cp.asnumpy(\r\n                    cp.rollaxis((img / dtype(255.0)) - mean / std, 2, 0,)\r\n                )\r\n\r\n    return reshaped\r\n\r\n\r\ndef reshape_x(\r\n    data: np.ndarray,\r\n    fp=16,\r\n    hide_map_prob: float = 0.0,\r\n    dropout_images_prob: List[float] = None,\r\n    force_cpu: bool = False,\r\n) -> np.ndarray:\r\n    """"""\r\n    Get images from data as a list and preprocess them, if cupy is available it uses the GPU,\r\n    else it uses the CPU (numpy)\r\n    Input:\r\n     - data: ndarray [num_examples x 6]\r\n     - fp: floating-point precision: Available values: 16, 32, 64\r\n     - hide_map_prob: Probability for removing the minimap (black square) from the image (0<=hide_map_prob<=1)\r\n     - dropout_images_prob List of 5 floats or None, probability for removing each input image during training\r\n     (black image) from a training example (0<=dropout_images_prob<=1)\r\n     - force_cpu: Use numpy version even if cupy is available (In case the GPU is being used to train the model)\r\n    Output:\r\n    - ndarray [num_examples * 5, num_channels, H, W]\r\n    """"""\r\n    assert (\r\n        0 <= hide_map_prob <= 1\r\n    ), f""Hide map prob must be between 0.0 and 1.0. Hide map prob: {hide_map_prob}""\r\n    if cupy and not force_cpu:\r\n        if fp == 16:\r\n            return reshape_x_cupy(\r\n                data,\r\n                dtype=cp.float16,\r\n                hide_map_prob=hide_map_prob,\r\n                dropout_images_prob=dropout_images_prob,\r\n            )\r\n        elif fp == 32:\r\n            return reshape_x_cupy(\r\n                data,\r\n                dtype=cp.float32,\r\n                hide_map_prob=hide_map_prob,\r\n                dropout_images_prob=dropout_images_prob,\r\n            )\r\n        elif fp == 64:\r\n            return reshape_x_cupy(\r\n                data,\r\n                dtype=cp.float64,\r\n                hide_map_prob=hide_map_prob,\r\n                dropout_images_prob=dropout_images_prob,\r\n            )\r\n        else:\r\n            raise ValueError(\r\n                f""Invalid floating-point precision: {fp}: Available values: 16, 32, 64""\r\n            )\r\n    else:\r\n        if fp == 16:\r\n            return reshape_x_numpy(\r\n                data,\r\n                dtype=np.float16,\r\n                hide_map_prob=hide_map_prob,\r\n                dropout_images_prob=dropout_images_prob,\r\n            )\r\n        elif fp == 32:\r\n            return reshape_x_numpy(\r\n                data,\r\n                dtype=np.float32,\r\n                hide_map_prob=hide_map_prob,\r\n                dropout_images_prob=dropout_images_prob,\r\n            )\r\n        elif fp == 64:\r\n            return reshape_x_numpy(\r\n                data,\r\n                dtype=np.float64,\r\n                hide_map_prob=hide_map_prob,\r\n                dropout_images_prob=dropout_images_prob,\r\n            )\r\n        else:\r\n            raise ValueError(\r\n                f""Invalid floating-point precision: {fp}: Available values: 16, 32, 64""\r\n            )\r\n\r\n\r\ndef batch(iterable: Sized, n: int = 1) -> Iterable:\r\n    """"""\r\n    Given a iterable generate batches of size n\r\n    Input:\r\n     - Sized that will be batched\r\n     - n: Integer batch size\r\n    Output:\r\n    - Iterable\r\n    """"""\r\n    l: int = len(iterable)\r\n    for ndx in range(0, l, n):\r\n        yield iterable[ndx : min(ndx + n, l)]\r\n\r\n\r\ndef nn_batchs(X: Sized, y: Sized, n: int = 1, sequence_size: int = 5) -> Iterable:\r\n    """"""\r\n    Given the input examples and the golds generate batches of sequence_size\r\n    Input:\r\n     - X: Sized input examples\r\n     - y: Sized golds\r\n     - n: Integer batch size\r\n     -sequence_size: Number of images in a training example. len(x) = len(y) * sequence_size\r\n    Output:\r\n    - Iterable\r\n    """"""\r\n\r\n    assert len(X) == len(y) * sequence_size, (\r\n        f""Inconsistent data, len(X) must equal len(y)*sequence_size.""\r\n        f"" len(X)={len(X)}, len(y)={len(y)}, sequence_size={sequence_size}""\r\n    )\r\n    bg_X: Iterable = batch(X, n * sequence_size)\r\n    bg_y: Iterable = batch(y, n)\r\n\r\n    for b_X, bg_y in zip(bg_X, bg_y):\r\n        yield b_X, bg_y\r\n\r\n\r\ndef evaluate(\r\n    model: TEDD1104,\r\n    X: torch.tensor,\r\n    golds: torch.tensor,\r\n    device: torch.device,\r\n    batch_size: int,\r\n) -> float:\r\n    """"""\r\n    Given a set of input examples and the golds for these examples evaluates the model accuracy\r\n    Input:\r\n     - model: TEDD1104 model to evaluate\r\n     - X: input examples [num_examples, sequence_size, 3, H, W]\r\n     - golds: golds for the input examples [num_examples]\r\n     - device: string, use cuda or cpu\r\n     -batch_size: integer batch size\r\n    Output:\r\n    - Accuracy: float\r\n    """"""\r\n    model.eval()\r\n    correct = 0\r\n    for X_batch, y_batch in nn_batchs(X, golds, batch_size):\r\n        predictions: np.ndarray = model.predict(X_batch.to(device)).cpu().numpy()\r\n        correct += np.sum(predictions == y_batch)\r\n\r\n    return correct / len(golds)\r\n\r\n\r\ndef load_file(\r\n    path: str,\r\n    fp: int = 16,\r\n    hide_map_prob: float = 0.0,\r\n    dropout_images_prob: List[float] = None,\r\n) -> (np.ndarray, np.ndarray):\r\n    """"""\r\n    Load dataset from file: Load, reshape and preprocess data.\r\n    Input:\r\n     - path: Path of the dataset\r\n     - fp: floating-point precision: Available values: 16, 32, 64\r\n     - hide_map_prob: Probability for removing the minimap (put a black square)\r\n       from a training example (0<=hide_map_prob<=1)\r\n    - dropout_images_prob List of 5 floats or None, probability for removing each input image during training\r\n     (black image) from a training example (0<=dropout_images_prob<=1)\r\n    Output:\r\n    - X: input examples [num_examples, 5, 3, H, W]\r\n    - y: golds for the input examples [num_examples]\r\n    """"""\r\n    try:\r\n        data = np.load(path, allow_pickle=True)[""arr_0""]\r\n    except (IOError, ValueError) as err:\r\n        logging.warning(f""[{err}] Error in file: {path}, ignoring the file."")\r\n        return np.array([]), np.array([])\r\n    except:\r\n        logging.warning(\r\n            f""[Unknown exception, probably corrupted file] Error in file: {path}, ignoring the file.""\r\n        )\r\n        return np.array([]), np.array([])\r\n\r\n    if check_valid_y(data):\r\n        X = reshape_x(\r\n            data,\r\n            fp=fp,\r\n            hide_map_prob=hide_map_prob,\r\n            dropout_images_prob=dropout_images_prob,\r\n        )\r\n        y = reshape_y(data)\r\n        return X, y\r\n\r\n    else:\r\n        logging.warning(f""Invalid file, no keys recorded: {path}, ignoring the file."")\r\n        return np.array([]), np.array([])\r\n\r\n\r\ndef load_dataset(path: str, fp: int = 16) -> (np.ndarray, np.ndarray):\r\n    """"""\r\n    Load dataset from directory: Load, reshape and preprocess data for all the files in a directory.\r\n    Input:\r\n     - path: Path of the directory\r\n     - fp: floating-point precision: Available values: 16, 32, 64\r\n    Output:\r\n    - X: input examples [num_examples_per_file * num_files, 5, 3, H, W]\r\n    - y: golds for the input examples [num_examples_per_file * num_files]\r\n    """"""\r\n    X: np.ndarray = np.array([])\r\n    y: np.ndarray = np.array([])\r\n\r\n    files = glob.glob(os.path.join(path, ""*.npz""))\r\n    for file_n, file in enumerate(files):\r\n        print(f""Loading file {file_n+1} of {len(files)}..."")\r\n        X_batch, y_batch = load_file(file, fp)\r\n        if len(X_batch) > 0 and len(y_batch) > 0:\r\n            if len(X) == 0:\r\n                X = X_batch\r\n                y = y_batch\r\n            else:\r\n                X = np.concatenate((X, X_batch), axis=0)\r\n                y = np.concatenate((y, y_batch), axis=0)\r\n\r\n    if len(X) == 0 or len(y) == 0:\r\n        # Since this function is used for loading the dev and test set, we want to stop the execution if we don\'t\r\n        # have a valid test of dev set.\r\n        raise ValueError(f""Empty dataset, all files invalid. Path: {path}"")\r\n\r\n    return X, y\r\n\r\n\r\ndef load_and_shuffle_datasets(\r\n    paths: List[str],\r\n    hide_map_prob: float,\r\n    dropout_images_prob: List[float] = None,\r\n    fp: int = 16,\r\n    force_cpu: bool = False,\r\n) -> (np.ndarray, np.ndarray):\r\n    """"""\r\n    Load multiple dataset files and shuffle the data, useful for training\r\n    Input:\r\n     - paths: List of paths to dataset files\r\n     - hide_map_prob: Probability for removing the minimap (put a black square)\r\n       from a training example (0<=hide_map_prob<=1)\r\n     - dropout_images_prob List of 5 floats or None, probability for removing each input image during training\r\n     (black image) from a training example (0<=dropout_images_prob<=1)\r\n     - fp: floating-point precision: Available values: 16, 32, 64\r\n     - force_cpu: Use numpy version even if cupy is available (In case the GPU is being used to train the model)\r\n    Output:\r\n    - X: input examples [num_examples_per_file * len(paths), 5, 3, H, W]\r\n    - y: golds for the input examples [num_examples_per_file * len(paths)]\r\n    """"""\r\n    data_array: np.ndarray = np.array([])\r\n\r\n    for file_no, file in enumerate(paths):\r\n        # print(f""Loading file {file_no+1} of {len(paths)}..."")\r\n        try:\r\n            data: np.ndarray = np.load(file, allow_pickle=True)[""arr_0""]\r\n        except (IOError, ValueError) as err:\r\n            logging.warning(f""[{err}] Error in file: {file}, ignoring the file."")\r\n            continue\r\n        except:\r\n            logging.warning(\r\n                f""[Unknown exception, probably corrupted file] Error in file: {file}, ignoring the file.""\r\n            )\r\n            continue\r\n\r\n        if check_valid_y(data):\r\n            if len(data_array) == 0:\r\n                data_array = data\r\n            else:\r\n                data_array = np.concatenate((data_array, data), axis=0)\r\n        else:\r\n            logging.warning(\r\n                f""Invalid file, no keys recorded: {file}, ignoring the file.""\r\n            )\r\n\r\n    if len(data_array) > 0:\r\n        np.random.seed(None)\r\n        np.random.shuffle(data_array)\r\n    else:\r\n        # Since this function is used for training, we want to continue training with the next files,\r\n        # so we return two empty arrays\r\n        logging.warning(f""Empty dataset, all files invalid. Path: {paths}"")\r\n        return np.array([]), np.array([])\r\n\r\n    X: np.ndarray = reshape_x(\r\n        data_array,\r\n        fp=fp,\r\n        hide_map_prob=hide_map_prob,\r\n        dropout_images_prob=dropout_images_prob,\r\n        force_cpu=force_cpu,\r\n    )\r\n    y: np.ndarray = reshape_y(data_array)\r\n\r\n    return X, y\r\n\r\n\r\ndef printTrace(message: str) -> None:\r\n    """"""\r\n    Print a message in the <date> : message format\r\n    Input:\r\n     - message: string to print\r\n    Output:\r\n    """"""\r\n    print(""<"" + str(datetime.datetime.now()) + "">  "" + str(message))\r\n\r\n\r\ndef mse_numpy(image1: np.ndarray, image2: np.ndarray) -> np.float:\r\n    """"""\r\n    Mean squared error between two numpy ndarrays\r\n    Input:\r\n     - image1: fist array\r\n     - image2: second numpy ndarray\r\n    Ouput:\r\n     - Mean squared error numpy.float\r\n    """"""\r\n    err = np.float(np.sum((image1 - image2) ** 2))\r\n    err /= np.float(image1.shape[0] * image1.shape[1])\r\n    return err\r\n\r\n\r\ndef mse_cupy(image1: cp.ndarray, image2: cp.ndarray) -> np.float:\r\n    """"""\r\n    Mean squared error between two cupy ndarrays\r\n    Input:\r\n     - image1: fist array\r\n     - image2: second numpy ndarray\r\n    Ouput:\r\n     - Mean squared error numpy.float\r\n     """"""\r\n    err = np.float(cp.sum((image1 - image2) ** 2))\r\n    err /= np.float(image1.shape[0] * image1.shape[1])\r\n    return err\r\n\r\n\r\ndef mse(image1: np.ndarray, image2: np.ndarray) -> np.float:\r\n    """"""\r\n    Mean squared error between two numpy ndarrays.\r\n    If available we will use the GPU (cupy) else we will use the CPU (numpy)\r\n    Input:\r\n     - image1: fist numpy ndarray\r\n     - image2: second numpy ndarray\r\n    Ouput:\r\n     - Mean squared error numpy.float\r\n     """"""\r\n    if cupy:\r\n        return mse_cupy(cp.asarray(image1), cp.asarray(image2))\r\n    else:\r\n        return mse_numpy(image1, image2)\r\n'"
keyboard/game_control.py,0,"b'# direct inputs\r\n\r\n# Author: hodka (https://stackoverflow.com/users/3550306/hodka)\r\n# source to this solution and code:\r\n# http://stackoverflow.com/questions/14489013/simulate-python-keypresses-for-controlling-a-game\r\n# http://www.gamespp.com/directx/directInputKeyboardScanCodes.html\r\n\r\nimport ctypes\r\nimport time\r\n\r\nSendInput = ctypes.windll.user32.SendInput\r\n\r\n\r\nW = 0x11\r\nA = 0x1E\r\nS = 0x1F\r\nD = 0x20\r\n\r\n# C struct redefinitions\r\nPUL = ctypes.POINTER(ctypes.c_ulong)\r\n\r\n\r\nclass KeyBdInput(ctypes.Structure):\r\n    _fields_ = [\r\n        (""wVk"", ctypes.c_ushort),\r\n        (""wScan"", ctypes.c_ushort),\r\n        (""dwFlags"", ctypes.c_ulong),\r\n        (""time"", ctypes.c_ulong),\r\n        (""dwExtraInfo"", PUL),\r\n    ]\r\n\r\n\r\nclass HardwareInput(ctypes.Structure):\r\n    _fields_ = [\r\n        (""uMsg"", ctypes.c_ulong),\r\n        (""wParamL"", ctypes.c_short),\r\n        (""wParamH"", ctypes.c_ushort),\r\n    ]\r\n\r\n\r\nclass MouseInput(ctypes.Structure):\r\n    _fields_ = [\r\n        (""dx"", ctypes.c_long),\r\n        (""dy"", ctypes.c_long),\r\n        (""mouseData"", ctypes.c_ulong),\r\n        (""dwFlags"", ctypes.c_ulong),\r\n        (""time"", ctypes.c_ulong),\r\n        (""dwExtraInfo"", PUL),\r\n    ]\r\n\r\n\r\nclass Input_I(ctypes.Union):\r\n    _fields_ = [(""ki"", KeyBdInput), (""mi"", MouseInput), (""hi"", HardwareInput)]\r\n\r\n\r\nclass Input(ctypes.Structure):\r\n    _fields_ = [(""type"", ctypes.c_ulong), (""ii"", Input_I)]\r\n\r\n\r\n# Actuals Functions\r\n\r\n\r\ndef PressKey(hexKeyCode):\r\n    extra = ctypes.c_ulong(0)\r\n    ii_ = Input_I()\r\n    ii_.ki = KeyBdInput(0, hexKeyCode, 0x0008, 0, ctypes.pointer(extra))\r\n    x = Input(ctypes.c_ulong(1), ii_)\r\n    ctypes.windll.user32.SendInput(1, ctypes.pointer(x), ctypes.sizeof(x))\r\n\r\n\r\ndef ReleaseKey(hexKeyCode):\r\n    extra = ctypes.c_ulong(0)\r\n    ii_ = Input_I()\r\n    ii_.ki = KeyBdInput(0, hexKeyCode, 0x0008 | 0x0002, 0, ctypes.pointer(extra))\r\n    x = Input(ctypes.c_ulong(1), ii_)\r\n    ctypes.windll.user32.SendInput(1, ctypes.pointer(x), ctypes.sizeof(x))\r\n\r\n\r\nif __name__ == ""__main__"":\r\n    PressKey(0x11)\r\n    time.sleep(1)\r\n    ReleaseKey(0x11)\r\n    time.sleep(1)\r\n'"
keyboard/getkeys.py,0,"b'# getkeys.py\r\n# Citation: Box Of Hats (https://github.com/Box-Of-Hats )\r\n\r\nimport win32api as wapi\r\n\r\nkeyList = [""\\b""]\r\nfor char in ""ABCDEFGHIJKLMNOPQRSTUVWXYZ 123456789,.\'APS$/\\\\"":\r\n    keyList.append(char)\r\n\r\n\r\ndef key_check():\r\n    keys = []\r\n    for key in keyList:\r\n        if wapi.GetAsyncKeyState(ord(key)):\r\n            keys.append(key)\r\n    return keys\r\n\r\n\r\ndef key_press(key):\r\n    if key == 1:\r\n        return ""A""\r\n    if key == 2:\r\n        return ""D""\r\n    if key == 3:\r\n        return ""W""\r\n    if key == 4:\r\n        return ""S""\r\n    if key == 5:\r\n        return ""AW""\r\n    if key == 6:\r\n        return ""AS""\r\n    if key == 7:\r\n        return ""DW""\r\n    if key == 8:\r\n        return ""DS""\r\n    return ""none""\r\n'"
keyboard/inputsHandler.py,0,"b'# inputsHandler.py\r\n\r\n# Authors: Iker Garc\xc3\xada Ferrero and Eritz Yerga\r\n\r\nfrom keyboard.game_control import ReleaseKey, PressKey\r\n\r\n\r\ndef noKey() -> None:\r\n    """"""\r\n    Release all keys\r\n    """"""\r\n    ReleaseKey(0x11)\r\n    ReleaseKey(0x1E)\r\n    ReleaseKey(0x1F)\r\n    ReleaseKey(0x20)\r\n\r\n\r\ndef W() -> None:\r\n    """"""\r\n    Release all keys and push W\r\n    """"""\r\n    PressKey(0x11)\r\n    ReleaseKey(0x1E)\r\n    ReleaseKey(0x1F)\r\n    ReleaseKey(0x20)\r\n\r\n\r\ndef A() -> None:\r\n    """"""\r\n    Release all keys and push A\r\n    """"""\r\n    ReleaseKey(0x11)\r\n    PressKey(0x1E)\r\n    ReleaseKey(0x1F)\r\n    ReleaseKey(0x20)\r\n\r\n\r\ndef S() -> None:\r\n    """"""\r\n    Release all keys and push S\r\n    """"""\r\n    ReleaseKey(0x11)\r\n    ReleaseKey(0x1E)\r\n    PressKey(0x1F)\r\n    ReleaseKey(0x20)\r\n\r\n\r\ndef D() -> None:\r\n    """"""\r\n    Release all keys and push D\r\n    """"""\r\n    ReleaseKey(0x11)\r\n    ReleaseKey(0x1E)\r\n    ReleaseKey(0x1F)\r\n    PressKey(0x20)\r\n\r\n\r\ndef WA() -> None:\r\n    """"""\r\n    Release all keys and push W and A\r\n    """"""\r\n    PressKey(0x11)\r\n    PressKey(0x1E)\r\n    ReleaseKey(0x1F)\r\n    ReleaseKey(0x20)\r\n\r\n\r\ndef WD() -> None:\r\n    """"""\r\n    Release all keys and push W and D\r\n    """"""\r\n    PressKey(0x11)\r\n    ReleaseKey(0x1E)\r\n    ReleaseKey(0x1F)\r\n    PressKey(0x20)\r\n\r\n\r\ndef SA() -> None:\r\n    """"""\r\n    Release all keys and push S and A\r\n    """"""\r\n    ReleaseKey(0x11)\r\n    PressKey(0x1E)\r\n    PressKey(0x1F)\r\n    ReleaseKey(0x20)\r\n\r\n\r\ndef SD() -> None:\r\n    """"""\r\n    Release all keys and push S and D\r\n    """"""\r\n    ReleaseKey(0x11)\r\n    ReleaseKey(0x1E)\r\n    PressKey(0x1F)\r\n    PressKey(0x20)\r\n\r\n\r\ndef select_key(key: int) -> None:\r\n    """"""\r\n    Given a ket in integer format, send to windows the virtual ket push\r\n    """"""\r\n    if key == 0:\r\n        noKey()\r\n    elif key == 1:\r\n        A()\r\n    elif key == 2:\r\n        D()\r\n    elif key == 3:\r\n        W()\r\n    elif key == 4:\r\n        S()\r\n    elif key == 5:\r\n        WA()\r\n    elif key == 6:\r\n        SA()\r\n    elif key == 7:\r\n        WD()\r\n    elif key == 8:\r\n        SD()\r\n'"
screen/grabber.py,0,"b'# Grabber.py https://gist.github.com/tzickel/5c2c51ddde7a8f5d87be730046612cd0\r\n# Author: tzickel (https://gist.github.com/tzickel)\r\n# A port of https://github.com/phoboslab/jsmpeg-vnc/blob/master/source/grabber.c to python\r\n# License information (GPLv3) is here https://github.com/phoboslab/jsmpeg-vnc/blob/master/README.md\r\n\r\nfrom ctypes import Structure, c_int, POINTER, WINFUNCTYPE, windll, WinError, sizeof\r\nfrom ctypes.wintypes import (\r\n    BOOL,\r\n    HWND,\r\n    RECT,\r\n    HDC,\r\n    HBITMAP,\r\n    HGDIOBJ,\r\n    DWORD,\r\n    LONG,\r\n    WORD,\r\n    UINT,\r\n    LPVOID,\r\n)\r\nimport numpy as np\r\n\r\nSRCCOPY = 0x00CC0020\r\nDIB_RGB_COLORS = 0\r\nBI_RGB = 0\r\n\r\n\r\nclass BITMAPINFOHEADER(Structure):\r\n    _fields_ = [\r\n        (""biSize"", DWORD),\r\n        (""biWidth"", LONG),\r\n        (""biHeight"", LONG),\r\n        (""biPlanes"", WORD),\r\n        (""biBitCount"", WORD),\r\n        (""biCompression"", DWORD),\r\n        (""biSizeImage"", DWORD),\r\n        (""biXPelsPerMeter"", LONG),\r\n        (""biYPelsPerMeter"", LONG),\r\n        (""biClrUsed"", DWORD),\r\n        (""biClrImportant"", DWORD),\r\n    ]\r\n\r\n\r\ndef err_on_zero_or_null_check(result, func, args):\r\n    if not result:\r\n        raise WinError()\r\n    return args\r\n\r\n\r\ndef quick_win_define(name, output, *args, **kwargs):\r\n    dllname, fname = name.split(""."")\r\n    params = kwargs.get(""params"", None)\r\n    if params:\r\n        params = tuple([(x,) for x in params])\r\n    func = (WINFUNCTYPE(output, *args))((fname, getattr(windll, dllname)), params)\r\n    err = kwargs.get(""err"", err_on_zero_or_null_check)\r\n    if err:\r\n        func.errcheck = err\r\n    return func\r\n\r\n\r\nGetClientRect = quick_win_define(\r\n    ""user32.GetClientRect"", BOOL, HWND, POINTER(RECT), params=(1, 2)\r\n)\r\nGetDC = quick_win_define(""user32.GetDC"", HDC, HWND)\r\nCreateCompatibleDC = quick_win_define(""gdi32.CreateCompatibleDC"", HDC, HDC)\r\nCreateCompatibleBitmap = quick_win_define(\r\n    ""gdi32.CreateCompatibleBitmap"", HBITMAP, HDC, c_int, c_int\r\n)\r\nReleaseDC = quick_win_define(""user32.ReleaseDC"", c_int, HWND, HDC)\r\nDeleteDC = quick_win_define(""gdi32.DeleteDC"", BOOL, HDC)\r\nDeleteObject = quick_win_define(""gdi32.DeleteObject"", BOOL, HGDIOBJ)\r\nSelectObject = quick_win_define(""gdi32.SelectObject"", HGDIOBJ, HDC, HGDIOBJ)\r\nBitBlt = quick_win_define(\r\n    ""gdi32.BitBlt"", BOOL, HDC, c_int, c_int, c_int, c_int, HDC, c_int, c_int, DWORD\r\n)\r\nGetDIBits = quick_win_define(\r\n    ""gdi32.GetDIBits"",\r\n    c_int,\r\n    HDC,\r\n    HBITMAP,\r\n    UINT,\r\n    UINT,\r\n    LPVOID,\r\n    POINTER(BITMAPINFOHEADER),\r\n    UINT,\r\n)\r\nGetDesktopWindow = quick_win_define(""user32.GetDesktopWindow"", HWND)\r\n\r\n\r\nclass Grabber(object):\r\n    def __init__(self, window=None, with_alpha=False, bbox=None):\r\n        window = window or GetDesktopWindow()\r\n        self.window = window\r\n        rect = GetClientRect(window)\r\n        self.width = rect.right - rect.left\r\n        self.height = rect.bottom - rect.top\r\n        if bbox:\r\n            bbox = [bbox[0], bbox[1], bbox[2] - bbox[0], bbox[3] - bbox[1]]\r\n            if not bbox[2] or not bbox[3]:\r\n                bbox[2] = self.width - bbox[0]\r\n                bbox[3] = self.height - bbox[1]\r\n            self.x, self.y, self.width, self.height = bbox\r\n        else:\r\n            self.x = 0\r\n            self.y = 0\r\n        self.windowDC = GetDC(window)\r\n        self.memoryDC = CreateCompatibleDC(self.windowDC)\r\n        self.bitmap = CreateCompatibleBitmap(self.windowDC, self.width, self.height)\r\n        self.bitmapInfo = BITMAPINFOHEADER()\r\n        self.bitmapInfo.biSize = sizeof(BITMAPINFOHEADER)\r\n        self.bitmapInfo.biPlanes = 1\r\n        self.bitmapInfo.biBitCount = 32 if with_alpha else 24\r\n        self.bitmapInfo.biWidth = self.width\r\n        self.bitmapInfo.biHeight = -self.height\r\n        self.bitmapInfo.biCompression = BI_RGB\r\n        self.bitmapInfo.biSizeImage = 0\r\n        self.channels = 4 if with_alpha else 3\r\n        self.closed = False\r\n\r\n    def __del__(self):\r\n        try:\r\n            self.close()\r\n        except:\r\n            pass\r\n\r\n    def close(self):\r\n        if self.closed:\r\n            return\r\n        ReleaseDC(self.window, self.windowDC)\r\n        DeleteDC(self.memoryDC)\r\n        DeleteObject(self.bitmap)\r\n        self.closed = True\r\n\r\n    def grab(self, output=None):\r\n        if self.closed:\r\n            raise ValueError(""Grabber already closed"")\r\n        if output is None:\r\n            output = np.empty((self.height, self.width, self.channels), dtype=""uint8"")\r\n        else:\r\n            if output.shape != (self.height, self.width, self.channels):\r\n                raise ValueError(""Invalid output dimentions"")\r\n        SelectObject(self.memoryDC, self.bitmap)\r\n        BitBlt(\r\n            self.memoryDC,\r\n            0,\r\n            0,\r\n            self.width,\r\n            self.height,\r\n            self.windowDC,\r\n            self.x,\r\n            self.y,\r\n            SRCCOPY,\r\n        )\r\n        GetDIBits(\r\n            self.memoryDC,\r\n            self.bitmap,\r\n            0,\r\n            self.height,\r\n            output.ctypes.data,\r\n            self.bitmapInfo,\r\n            DIB_RGB_COLORS,\r\n        )\r\n        return output\r\n'"
screen/record_screen.py,0,"b'from screen.grabber import Grabber\r\nimport numpy as np\r\nimport time\r\nimport cv2\r\nfrom keyboard.getkeys import key_check\r\nimport threading\r\nimport logging\r\nimport math\r\n\r\nglobal fps\r\nglobal grb\r\nglobal front_buffer\r\nglobal back_buffer\r\nglobal num\r\nglobal seq\r\nglobal key_out\r\n\r\n\r\ndef keys_to_output(keys: np.ndarray) -> np.ndarray:\r\n    """"""\r\n    Convert keys to a ...multi-hot... array\r\n    Input:\r\n     - np.ndarray of strings [""A"",""W""]\r\n    Ouput:\r\n     - multi-hot array of integers (0,1) representing which keys are pressed (1). Array: [A,D,W,S]\r\n    """"""\r\n    output = np.asarray([0, 0, 0, 0])\r\n\r\n    if ""A"" in keys:\r\n        output[0] = 1\r\n    if ""D"" in keys:\r\n        output[1] = 1\r\n    if ""W"" in keys:\r\n        output[2] = 1\r\n    if ""S"" in keys:\r\n        output[3] = 1\r\n\r\n    return output\r\n\r\n\r\ndef screen_record():\r\n    """"""\r\n    Do a screenshot using ImageGRAB from PIL\r\n    Input:\r\n\r\n    Output:\r\n    - Screenshot in the (1, 26, 1601, 926) coordinates of the screen: size [1600,900,3]\r\n    """"""\r\n    global grb\r\n    general_img = grb.grab(None)\r\n    return general_img\r\n\r\n\r\ndef img_thread(stop_event: threading.Event):\r\n    """"""\r\n    A thread that will continuously do screenshots and will store the last one in the global back_back_buffer variable\r\n    Input:\r\n    - stop_event: threading.Event that will stop the thread\r\n    Output:\r\n\r\n    """"""\r\n    global front_buffer\r\n    global back_buffer\r\n    global fps\r\n\r\n    last_time = time.time()\r\n    while not stop_event.is_set():\r\n        front_buffer = screen_record()\r\n        # Swap buffers\r\n        front_buffer, back_buffer = back_buffer, front_buffer\r\n        fps = int(1.0 / (time.time() - last_time))\r\n        last_time = time.time()\r\n\r\n\r\ndef preprocess_image(image):\r\n    """"""\r\n    Given an image resize it and convert it to a numpy array\r\n    Input:\r\n    - image: PIL image\r\n    Output:\r\n    - numpy ndarray: [480,270,3]\r\n    """"""\r\n    processed_image = cv2.resize(image, (480, 270))\r\n    return np.asarray(processed_image, dtype=np.uint8,)\r\n\r\n\r\ndef image_sequencer_thread(stop_event: threading.Event) -> None:\r\n    """"""\r\n    Get the images from img_thread and maintain an updated array seq of the last 5 captured images with a 1/10 secs\r\n    span between them.\r\n    Input:\r\n    - stop_event: threading.Event that will stop the thread\r\n    Output:\r\n\r\n    """"""\r\n    global back_buffer\r\n    global seq\r\n    global key_out\r\n    global num\r\n\r\n    # Frames per second capture rate\r\n    capturerate = 10.0\r\n    while not stop_event.is_set():\r\n        last_time = time.time()\r\n        seq, num, key_out = (\r\n            np.concatenate(\r\n                (seq[1:], [preprocess_image(np.copy(back_buffer))]), axis=0,\r\n            ),\r\n            num + 1,\r\n            keys_to_output(key_check()),\r\n        )\r\n        waittime = (1.0 / capturerate) - (time.time() - last_time)\r\n        if waittime > 0.0:\r\n            time.sleep(waittime)\r\n\r\n\r\ndef multi_image_sequencer_thread(\r\n    stop_event: threading.Event, num_sequences: int\r\n) -> None:\r\n    """"""\r\n    Get the images from img_thread and maintain an updated array seq of the last 5 captured images with a 1/10 secs\r\n    span between them.\r\n    Input:\r\n    - stop_event: threading.Event that will stop the thread\r\n    Output:\r\n\r\n    """"""\r\n    global back_buffer\r\n    global seq\r\n    global key_out\r\n    global num\r\n\r\n    # Frames per second capture rate\r\n    capturerate: float = 10.0\r\n    sequence_delay: float = 1.0 / capturerate / num_sequences\r\n    sequences: np.ndarray = np.repeat(\r\n        np.expand_dims(\r\n            np.asarray(\r\n                [\r\n                    np.zeros((270, 480, 3)),\r\n                    np.zeros((270, 480, 3)),\r\n                    np.zeros((270, 480, 3)),\r\n                    np.zeros((270, 480, 3)),\r\n                    np.zeros((270, 480, 3)),\r\n                ],\r\n                dtype=np.uint8,\r\n            ),\r\n            0,\r\n        ),\r\n        num_sequences,\r\n        axis=0,\r\n    )\r\n\r\n    while not stop_event.is_set():\r\n        for i in range(num_sequences):\r\n            start_time: float = time.time()\r\n            sequences[i][0] = preprocess_image(np.copy(back_buffer))\r\n            sequences[i] = sequences[i][[1, 2, 3, 4, 0]]\r\n            seq, num, key_out = sequences[i], num + 1, keys_to_output(key_check())\r\n            waittime: float = sequence_delay - (time.time() - start_time)\r\n            if waittime > 0:\r\n                time.sleep(waittime)\r\n            else:\r\n                logging.warning(\r\n                    f""{math.fabs(waittime)} delay in the sequence capture, consider reducing num_sequences""\r\n                )\r\n\r\n\r\ndef initialize_global_variables() -> None:\r\n    """"""\r\n    Initialize global variables\r\n    Input:\r\n    Output:\r\n    """"""\r\n    global fps\r\n    global grb\r\n    global front_buffer\r\n    global back_buffer\r\n    global num\r\n    global seq\r\n    global key_out\r\n\r\n    fps = 10\r\n    grb = Grabber(bbox=(1, 26, 1601, 926))\r\n    front_buffer = np.zeros((1600, 900), dtype=np.int8)\r\n    back_buffer = np.zeros((1600, 900), dtype=np.int8)\r\n    num = 0\r\n    seq = np.asarray(\r\n        [\r\n            np.zeros((270, 480, 3)),\r\n            np.zeros((270, 480, 3)),\r\n            np.zeros((270, 480, 3)),\r\n            np.zeros((270, 480, 3)),\r\n            np.zeros((270, 480, 3)),\r\n        ],\r\n        dtype=np.uint8,\r\n    )\r\n    key_out = np.array([0, 0, 0, 0])\r\n'"
segmentation/segmentation_coco.py,9,"b'""""""\nSemantic segmentation using pytorch pretrained models.\nClasses: [\'__background__\', \'aeroplane\', \'bicycle\', \'bird\', \'boat\', \'bottle\', \'bus\',\n \'car\', \'cat\', \'chair\', \'cow\', \'diningtable\', \'dog\', \'horse\', \'motorbike\',\n \'person\', \'pottedplant\', \'sheep\', \'sofa\', \'train\', \'tvmonitor\']\nDocumentation: https://pytorch.org/docs/stable/torchvision/models.html#object-detection-instance-segmentation-and-person-keypoint-detection\n""""""\nimport torch\nimport torch.hub\nfrom PIL import Image\nfrom torchvision import transforms\nimport numpy as np\n\n\nclass ImageSegmentation:\n    def __init__(\n        self,\n        model_name: str,\n        device: torch.device,\n        fp16: bool,\n        apex_opt_level: str = ""O2"",\n    ):\n        """"""\n        Image segmentation\n\n        Input:\n        -model_name: Name of the pretrained model to use from pytorch/vision\n        -device: Torch.device where the model will be loaded (GPU recommended)\n        -fp16: Use FP16 (only available if device = cuda:x)\n        -apex_opt_level: Apex opt level for FP16\n        Output:\n        """"""\n        if model_name == ""fcn_resnet101"":\n            self.model = torch.hub.load(\n                ""pytorch/vision"", ""fcn_resnet101"", pretrained=True\n            ).to(device)\n\n        elif model_name == ""deeplabv3_resnet101"":\n            self.model = torch.hub.load(\n                ""pytorch/vision"", ""deeplabv3_resnet101"", pretrained=True\n            ).to(device)\n        else:\n            raise ValueError(\n                f""model: {model_name} not supported. Choose between [fcn_resnet101, deeplabv3_resnet101]""\n            )\n\n        if fp16:\n            try:\n                from apex import amp\n\n                self.model = amp.initialize(\n                    self.model, opt_level=apex_opt_level, keep_batchnorm_fp32=True,\n                )\n\n            except ImportError:\n                raise ImportError(\n                    ""Please install apex from https://www.github.com/nvidia/apex to use fp16 training.""\n                )\n\n        self.model.eval()\n\n        self.preprocess = transforms.Compose(\n            [\n                transforms.ToTensor(),\n                transforms.Normalize(\n                    mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n                ),\n            ]\n        )\n\n        palette = torch.tensor([2 ** 25 - 1, 2 ** 15 - 1, 2 ** 21 - 1])\n        colors = torch.as_tensor([i for i in range(21)])[:, None] * palette\n        self.colors = (colors % 255).numpy().astype(""uint8"")\n\n        self.device = device\n\n    def add_segmentation(self, images: np.ndarray):\n        """"""\n        Given a list of images, we will perform image segmentation and the detected entities will be\n        printed over the original image to highlight them.\n\n        Input:\n        -images: Array of images (num_images x height x width x num_channels)\n        Output:\n        -images modified with segmented entities printed over them: (num_images x height x width x num_channels)\n        """"""\n\n        img_size = (images.shape[2], images.shape[1])\n        input_batch = torch.stack(\n            [self.preprocess(Image.fromarray(image).convert(""RGB"")) for image in images]\n        )\n\n        input_batch.to(self.device)\n\n        with torch.no_grad():\n            output = self.model(input_batch.to(self.device))[""out""]\n\n        output_predictions = output.argmax(1).byte().cpu().numpy()\n\n        segmented_images = [\n            Image.fromarray(output_prediction).resize(img_size)\n            for output_prediction in output_predictions\n        ]\n\n        [image.putpalette(self.colors) for image in segmented_images]\n\n        masks = [np.array(image) != 0 for image in segmented_images]\n\n        segmented_images = [\n            np.array(image.convert(""RGB"")) for image in segmented_images\n        ]\n\n        for image, segmented_image, mask in zip(images, segmented_images, masks):\n            image[mask] = segmented_image[mask]\n\n        return images\n'"
