file_path,api_count,code
enjoy.py,4,"b'import argparse\nimport os\n# workaround to unpickle olf model files\nimport sys\n\nimport numpy as np\nimport torch\n\nfrom a2c_ppo_acktr.envs import VecPyTorch, make_vec_envs\nfrom a2c_ppo_acktr.utils import get_render_func, get_vec_normalize\n\nsys.path.append(\'a2c_ppo_acktr\')\n\nparser = argparse.ArgumentParser(description=\'RL\')\nparser.add_argument(\n    \'--seed\', type=int, default=1, help=\'random seed (default: 1)\')\nparser.add_argument(\n    \'--log-interval\',\n    type=int,\n    default=10,\n    help=\'log interval, one log per n updates (default: 10)\')\nparser.add_argument(\n    \'--env-name\',\n    default=\'PongNoFrameskip-v4\',\n    help=\'environment to train on (default: PongNoFrameskip-v4)\')\nparser.add_argument(\n    \'--load-dir\',\n    default=\'./trained_models/\',\n    help=\'directory to save agent logs (default: ./trained_models/)\')\nparser.add_argument(\n    \'--non-det\',\n    action=\'store_true\',\n    default=False,\n    help=\'whether to use a non-deterministic policy\')\nargs = parser.parse_args()\n\nargs.det = not args.non_det\n\nenv = make_vec_envs(\n    args.env_name,\n    args.seed + 1000,\n    1,\n    None,\n    None,\n    device=\'cpu\',\n    allow_early_resets=False)\n\n# Get a render function\nrender_func = get_render_func(env)\n\n# We need to use the same statistics for normalization as used in training\nactor_critic, ob_rms = \\\n            torch.load(os.path.join(args.load_dir, args.env_name + "".pt""))\n\nvec_norm = get_vec_normalize(env)\nif vec_norm is not None:\n    vec_norm.eval()\n    vec_norm.ob_rms = ob_rms\n\nrecurrent_hidden_states = torch.zeros(1,\n                                      actor_critic.recurrent_hidden_state_size)\nmasks = torch.zeros(1, 1)\n\nobs = env.reset()\n\nif render_func is not None:\n    render_func(\'human\')\n\nif args.env_name.find(\'Bullet\') > -1:\n    import pybullet as p\n\n    torsoId = -1\n    for i in range(p.getNumBodies()):\n        if (p.getBodyInfo(i)[0].decode() == ""torso""):\n            torsoId = i\n\nwhile True:\n    with torch.no_grad():\n        value, action, _, recurrent_hidden_states = actor_critic.act(\n            obs, recurrent_hidden_states, masks, deterministic=args.det)\n\n    # Obser reward and next obs\n    obs, reward, done, _ = env.step(action)\n\n    masks.fill_(0.0 if done else 1.0)\n\n    if args.env_name.find(\'Bullet\') > -1:\n        if torsoId > -1:\n            distance = 5\n            yaw = 0\n            humanPos, humanOrn = p.getBasePositionAndOrientation(torsoId)\n            p.resetDebugVisualizerCamera(distance, yaw, -20, humanPos)\n\n    if render_func is not None:\n        render_func(\'human\')\n'"
evaluation.py,5,"b'import numpy as np\nimport torch\n\nfrom a2c_ppo_acktr import utils\nfrom a2c_ppo_acktr.envs import make_vec_envs\n\n\ndef evaluate(actor_critic, ob_rms, env_name, seed, num_processes, eval_log_dir,\n             device):\n    eval_envs = make_vec_envs(env_name, seed + num_processes, num_processes,\n                              None, eval_log_dir, device, True)\n\n    vec_norm = utils.get_vec_normalize(eval_envs)\n    if vec_norm is not None:\n        vec_norm.eval()\n        vec_norm.ob_rms = ob_rms\n\n    eval_episode_rewards = []\n\n    obs = eval_envs.reset()\n    eval_recurrent_hidden_states = torch.zeros(\n        num_processes, actor_critic.recurrent_hidden_state_size, device=device)\n    eval_masks = torch.zeros(num_processes, 1, device=device)\n\n    while len(eval_episode_rewards) < 10:\n        with torch.no_grad():\n            _, action, _, eval_recurrent_hidden_states = actor_critic.act(\n                obs,\n                eval_recurrent_hidden_states,\n                eval_masks,\n                deterministic=True)\n\n        # Obser reward and next obs\n        obs, _, done, infos = eval_envs.step(action)\n\n        eval_masks = torch.tensor(\n            [[0.0] if done_ else [1.0] for done_ in done],\n            dtype=torch.float32,\n            device=device)\n\n        for info in infos:\n            if \'episode\' in info.keys():\n                eval_episode_rewards.append(info[\'episode\'][\'r\'])\n\n    eval_envs.close()\n\n    print("" Evaluation using {} episodes: mean reward {:.5f}\\n"".format(\n        len(eval_episode_rewards), np.mean(eval_episode_rewards)))\n'"
generate_tmux_yaml.py,0,"b'import argparse\n\nimport yaml\n\nparser = argparse.ArgumentParser(description=\'Process some integers.\')\nparser.add_argument(\n    \'--num-seeds\',\n    type=int,\n    default=4,\n    help=\'number of random seeds to generate\')\nparser.add_argument(\n    \'--env-names\',\n    default=""PongNoFrameskip-v4"",\n    help=\'environment name separated by semicolons\')\nargs = parser.parse_args()\n\nppo_mujoco_template = ""python main.py --env-name {0} --algo ppo --use-gae --log-interval 1 --num-steps 2048 --num-processes 1 --lr 3e-4 --entropy-coef 0 --value-loss-coef 0.5 --ppo-epoch 10 --num-mini-batch 32 --gamma 0.99 --tau 0.95 --num-env-steps 1000000 --use-linear-lr-decay --no-cuda --log-dir /tmp/gym/{1}/{1}-{2} --seed {2} --use-proper-time-limits""\n\nppo_atari_template = ""env CUDA_VISIBLE_DEVICES={2} python main.py --env-name {0} --algo ppo --use-gae --lr 2.5e-4 --clip-param 0.1 --value-loss-coef 0.5 --num-processes 8 --num-steps 128 --num-mini-batch 4 --log-interval 1 --use-linear-lr-decay --entropy-coef 0.01 --log-dir /tmp/gym/{1}/{1}-{2} --seed {2}""\n\ntemplate = ppo_atari_template\n\nconfig = {""session_name"": ""run-all"", ""windows"": []}\n\nfor i in range(args.num_seeds):\n    panes_list = []\n    for env_name in args.env_names.split(\';\'):\n        panes_list.append(\n            template.format(env_name,\n                            env_name.split(\'-\')[0].lower(), i))\n\n    config[""windows""].append({\n        ""window_name"": ""seed-{}"".format(i),\n        ""panes"": panes_list\n    })\n\nyaml.dump(config, open(""run_all.yaml"", ""w""), default_flow_style=False)\n'"
main.py,16,"b'import copy\nimport glob\nimport os\nimport time\nfrom collections import deque\n\nimport gym\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\n\nfrom a2c_ppo_acktr import algo, utils\nfrom a2c_ppo_acktr.algo import gail\nfrom a2c_ppo_acktr.arguments import get_args\nfrom a2c_ppo_acktr.envs import make_vec_envs\nfrom a2c_ppo_acktr.model import Policy\nfrom a2c_ppo_acktr.storage import RolloutStorage\nfrom evaluation import evaluate\n\n\ndef main():\n    args = get_args()\n\n    torch.manual_seed(args.seed)\n    torch.cuda.manual_seed_all(args.seed)\n\n    if args.cuda and torch.cuda.is_available() and args.cuda_deterministic:\n        torch.backends.cudnn.benchmark = False\n        torch.backends.cudnn.deterministic = True\n\n    log_dir = os.path.expanduser(args.log_dir)\n    eval_log_dir = log_dir + ""_eval""\n    utils.cleanup_log_dir(log_dir)\n    utils.cleanup_log_dir(eval_log_dir)\n\n    torch.set_num_threads(1)\n    device = torch.device(""cuda:0"" if args.cuda else ""cpu"")\n\n    envs = make_vec_envs(args.env_name, args.seed, args.num_processes,\n                         args.gamma, args.log_dir, device, False)\n\n    actor_critic = Policy(\n        envs.observation_space.shape,\n        envs.action_space,\n        base_kwargs={\'recurrent\': args.recurrent_policy})\n    actor_critic.to(device)\n\n    if args.algo == \'a2c\':\n        agent = algo.A2C_ACKTR(\n            actor_critic,\n            args.value_loss_coef,\n            args.entropy_coef,\n            lr=args.lr,\n            eps=args.eps,\n            alpha=args.alpha,\n            max_grad_norm=args.max_grad_norm)\n    elif args.algo == \'ppo\':\n        agent = algo.PPO(\n            actor_critic,\n            args.clip_param,\n            args.ppo_epoch,\n            args.num_mini_batch,\n            args.value_loss_coef,\n            args.entropy_coef,\n            lr=args.lr,\n            eps=args.eps,\n            max_grad_norm=args.max_grad_norm)\n    elif args.algo == \'acktr\':\n        agent = algo.A2C_ACKTR(\n            actor_critic, args.value_loss_coef, args.entropy_coef, acktr=True)\n\n    if args.gail:\n        assert len(envs.observation_space.shape) == 1\n        discr = gail.Discriminator(\n            envs.observation_space.shape[0] + envs.action_space.shape[0], 100,\n            device)\n        file_name = os.path.join(\n            args.gail_experts_dir, ""trajs_{}.pt"".format(\n                args.env_name.split(\'-\')[0].lower()))\n        \n        expert_dataset = gail.ExpertDataset(\n            file_name, num_trajectories=4, subsample_frequency=20)\n        drop_last = len(expert_dataset) > args.gail_batch_size\n        gail_train_loader = torch.utils.data.DataLoader(\n            dataset=expert_dataset,\n            batch_size=args.gail_batch_size,\n            shuffle=True,\n            drop_last=drop_last)\n\n    rollouts = RolloutStorage(args.num_steps, args.num_processes,\n                              envs.observation_space.shape, envs.action_space,\n                              actor_critic.recurrent_hidden_state_size)\n\n    obs = envs.reset()\n    rollouts.obs[0].copy_(obs)\n    rollouts.to(device)\n\n    episode_rewards = deque(maxlen=10)\n\n    start = time.time()\n    num_updates = int(\n        args.num_env_steps) // args.num_steps // args.num_processes\n    for j in range(num_updates):\n\n        if args.use_linear_lr_decay:\n            # decrease learning rate linearly\n            utils.update_linear_schedule(\n                agent.optimizer, j, num_updates,\n                agent.optimizer.lr if args.algo == ""acktr"" else args.lr)\n\n        for step in range(args.num_steps):\n            # Sample actions\n            with torch.no_grad():\n                value, action, action_log_prob, recurrent_hidden_states = actor_critic.act(\n                    rollouts.obs[step], rollouts.recurrent_hidden_states[step],\n                    rollouts.masks[step])\n\n            # Obser reward and next obs\n            obs, reward, done, infos = envs.step(action)\n\n            for info in infos:\n                if \'episode\' in info.keys():\n                    episode_rewards.append(info[\'episode\'][\'r\'])\n\n            # If done then clean the history of observations.\n            masks = torch.FloatTensor(\n                [[0.0] if done_ else [1.0] for done_ in done])\n            bad_masks = torch.FloatTensor(\n                [[0.0] if \'bad_transition\' in info.keys() else [1.0]\n                 for info in infos])\n            rollouts.insert(obs, recurrent_hidden_states, action,\n                            action_log_prob, value, reward, masks, bad_masks)\n\n        with torch.no_grad():\n            next_value = actor_critic.get_value(\n                rollouts.obs[-1], rollouts.recurrent_hidden_states[-1],\n                rollouts.masks[-1]).detach()\n\n        if args.gail:\n            if j >= 10:\n                envs.venv.eval()\n\n            gail_epoch = args.gail_epoch\n            if j < 10:\n                gail_epoch = 100  # Warm up\n            for _ in range(gail_epoch):\n                discr.update(gail_train_loader, rollouts,\n                             utils.get_vec_normalize(envs)._obfilt)\n\n            for step in range(args.num_steps):\n                rollouts.rewards[step] = discr.predict_reward(\n                    rollouts.obs[step], rollouts.actions[step], args.gamma,\n                    rollouts.masks[step])\n\n        rollouts.compute_returns(next_value, args.use_gae, args.gamma,\n                                 args.gae_lambda, args.use_proper_time_limits)\n\n        value_loss, action_loss, dist_entropy = agent.update(rollouts)\n\n        rollouts.after_update()\n\n        # save for every interval-th episode or for the last epoch\n        if (j % args.save_interval == 0\n                or j == num_updates - 1) and args.save_dir != """":\n            save_path = os.path.join(args.save_dir, args.algo)\n            try:\n                os.makedirs(save_path)\n            except OSError:\n                pass\n\n            torch.save([\n                actor_critic,\n                getattr(utils.get_vec_normalize(envs), \'ob_rms\', None)\n            ], os.path.join(save_path, args.env_name + "".pt""))\n\n        if j % args.log_interval == 0 and len(episode_rewards) > 1:\n            total_num_steps = (j + 1) * args.num_processes * args.num_steps\n            end = time.time()\n            print(\n                ""Updates {}, num timesteps {}, FPS {} \\n Last {} training episodes: mean/median reward {:.1f}/{:.1f}, min/max reward {:.1f}/{:.1f}\\n""\n                .format(j, total_num_steps,\n                        int(total_num_steps / (end - start)),\n                        len(episode_rewards), np.mean(episode_rewards),\n                        np.median(episode_rewards), np.min(episode_rewards),\n                        np.max(episode_rewards), dist_entropy, value_loss,\n                        action_loss))\n\n        if (args.eval_interval is not None and len(episode_rewards) > 1\n                and j % args.eval_interval == 0):\n            ob_rms = utils.get_vec_normalize(envs).ob_rms\n            evaluate(actor_critic, ob_rms, args.env_name, args.seed,\n                     args.num_processes, eval_log_dir, device)\n\n\nif __name__ == ""__main__"":\n    main()\n'"
setup.py,0,"b""from setuptools import find_packages, setup\n\nsetup(\n    name='a2c-ppo-acktr',\n    packages=find_packages(),\n    version='0.0.1',\n    install_requires=['gym', 'matplotlib', 'pybullet'])\n"""
a2c_ppo_acktr/__init__.py,0,b''
a2c_ppo_acktr/arguments.py,1,"b'import argparse\n\nimport torch\n\n\ndef get_args():\n    parser = argparse.ArgumentParser(description=\'RL\')\n    parser.add_argument(\n        \'--algo\', default=\'a2c\', help=\'algorithm to use: a2c | ppo | acktr\')\n    parser.add_argument(\n        \'--gail\',\n        action=\'store_true\',\n        default=False,\n        help=\'do imitation learning with gail\')\n    parser.add_argument(\n        \'--gail-experts-dir\',\n        default=\'./gail_experts\',\n        help=\'directory that contains expert demonstrations for gail\')\n    parser.add_argument(\n        \'--gail-batch-size\',\n        type=int,\n        default=128,\n        help=\'gail batch size (default: 128)\')\n    parser.add_argument(\n        \'--gail-epoch\', type=int, default=5, help=\'gail epochs (default: 5)\')\n    parser.add_argument(\n        \'--lr\', type=float, default=7e-4, help=\'learning rate (default: 7e-4)\')\n    parser.add_argument(\n        \'--eps\',\n        type=float,\n        default=1e-5,\n        help=\'RMSprop optimizer epsilon (default: 1e-5)\')\n    parser.add_argument(\n        \'--alpha\',\n        type=float,\n        default=0.99,\n        help=\'RMSprop optimizer apha (default: 0.99)\')\n    parser.add_argument(\n        \'--gamma\',\n        type=float,\n        default=0.99,\n        help=\'discount factor for rewards (default: 0.99)\')\n    parser.add_argument(\n        \'--use-gae\',\n        action=\'store_true\',\n        default=False,\n        help=\'use generalized advantage estimation\')\n    parser.add_argument(\n        \'--gae-lambda\',\n        type=float,\n        default=0.95,\n        help=\'gae lambda parameter (default: 0.95)\')\n    parser.add_argument(\n        \'--entropy-coef\',\n        type=float,\n        default=0.01,\n        help=\'entropy term coefficient (default: 0.01)\')\n    parser.add_argument(\n        \'--value-loss-coef\',\n        type=float,\n        default=0.5,\n        help=\'value loss coefficient (default: 0.5)\')\n    parser.add_argument(\n        \'--max-grad-norm\',\n        type=float,\n        default=0.5,\n        help=\'max norm of gradients (default: 0.5)\')\n    parser.add_argument(\n        \'--seed\', type=int, default=1, help=\'random seed (default: 1)\')\n    parser.add_argument(\n        \'--cuda-deterministic\',\n        action=\'store_true\',\n        default=False,\n        help=""sets flags for determinism when using CUDA (potentially slow!)"")\n    parser.add_argument(\n        \'--num-processes\',\n        type=int,\n        default=16,\n        help=\'how many training CPU processes to use (default: 16)\')\n    parser.add_argument(\n        \'--num-steps\',\n        type=int,\n        default=5,\n        help=\'number of forward steps in A2C (default: 5)\')\n    parser.add_argument(\n        \'--ppo-epoch\',\n        type=int,\n        default=4,\n        help=\'number of ppo epochs (default: 4)\')\n    parser.add_argument(\n        \'--num-mini-batch\',\n        type=int,\n        default=32,\n        help=\'number of batches for ppo (default: 32)\')\n    parser.add_argument(\n        \'--clip-param\',\n        type=float,\n        default=0.2,\n        help=\'ppo clip parameter (default: 0.2)\')\n    parser.add_argument(\n        \'--log-interval\',\n        type=int,\n        default=10,\n        help=\'log interval, one log per n updates (default: 10)\')\n    parser.add_argument(\n        \'--save-interval\',\n        type=int,\n        default=100,\n        help=\'save interval, one save per n updates (default: 100)\')\n    parser.add_argument(\n        \'--eval-interval\',\n        type=int,\n        default=None,\n        help=\'eval interval, one eval per n updates (default: None)\')\n    parser.add_argument(\n        \'--num-env-steps\',\n        type=int,\n        default=10e6,\n        help=\'number of environment steps to train (default: 10e6)\')\n    parser.add_argument(\n        \'--env-name\',\n        default=\'PongNoFrameskip-v4\',\n        help=\'environment to train on (default: PongNoFrameskip-v4)\')\n    parser.add_argument(\n        \'--log-dir\',\n        default=\'/tmp/gym/\',\n        help=\'directory to save agent logs (default: /tmp/gym)\')\n    parser.add_argument(\n        \'--save-dir\',\n        default=\'./trained_models/\',\n        help=\'directory to save agent logs (default: ./trained_models/)\')\n    parser.add_argument(\n        \'--no-cuda\',\n        action=\'store_true\',\n        default=False,\n        help=\'disables CUDA training\')\n    parser.add_argument(\n        \'--use-proper-time-limits\',\n        action=\'store_true\',\n        default=False,\n        help=\'compute returns taking into account time limits\')\n    parser.add_argument(\n        \'--recurrent-policy\',\n        action=\'store_true\',\n        default=False,\n        help=\'use a recurrent policy\')\n    parser.add_argument(\n        \'--use-linear-lr-decay\',\n        action=\'store_true\',\n        default=False,\n        help=\'use a linear schedule on the learning rate\')\n    args = parser.parse_args()\n\n    args.cuda = not args.no_cuda and torch.cuda.is_available()\n\n    assert args.algo in [\'a2c\', \'ppo\', \'acktr\']\n    if args.recurrent_policy:\n        assert args.algo in [\'a2c\', \'ppo\'], \\\n            \'Recurrent policy is not implemented for ACKTR\'\n\n    return args\n'"
a2c_ppo_acktr/distributions.py,8,"b'import math\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom a2c_ppo_acktr.utils import AddBias, init\n\n""""""\nModify standard PyTorch distributions so they are compatible with this code.\n""""""\n\n#\n# Standardize distribution interfaces\n#\n\n# Categorical\nclass FixedCategorical(torch.distributions.Categorical):\n    def sample(self):\n        return super().sample().unsqueeze(-1)\n\n    def log_probs(self, actions):\n        return (\n            super()\n            .log_prob(actions.squeeze(-1))\n            .view(actions.size(0), -1)\n            .sum(-1)\n            .unsqueeze(-1)\n        )\n\n    def mode(self):\n        return self.probs.argmax(dim=-1, keepdim=True)\n\n\n# Normal\nclass FixedNormal(torch.distributions.Normal):\n    def log_probs(self, actions):\n        return super().log_prob(actions).sum(-1, keepdim=True)\n\n    def entrop(self):\n        return super.entropy().sum(-1)\n\n    def mode(self):\n        return self.mean\n\n\n# Bernoulli\nclass FixedBernoulli(torch.distributions.Bernoulli):\n    def log_probs(self, actions):\n        return super.log_prob(actions).view(actions.size(0), -1).sum(-1).unsqueeze(-1)\n\n    def entropy(self):\n        return super().entropy().sum(-1)\n\n    def mode(self):\n        return torch.gt(self.probs, 0.5).float()\n\n\nclass Categorical(nn.Module):\n    def __init__(self, num_inputs, num_outputs):\n        super(Categorical, self).__init__()\n\n        init_ = lambda m: init(\n            m,\n            nn.init.orthogonal_,\n            lambda x: nn.init.constant_(x, 0),\n            gain=0.01)\n\n        self.linear = init_(nn.Linear(num_inputs, num_outputs))\n\n    def forward(self, x):\n        x = self.linear(x)\n        return FixedCategorical(logits=x)\n\n\nclass DiagGaussian(nn.Module):\n    def __init__(self, num_inputs, num_outputs):\n        super(DiagGaussian, self).__init__()\n\n        init_ = lambda m: init(m, nn.init.orthogonal_, lambda x: nn.init.\n                               constant_(x, 0))\n\n        self.fc_mean = init_(nn.Linear(num_inputs, num_outputs))\n        self.logstd = AddBias(torch.zeros(num_outputs))\n\n    def forward(self, x):\n        action_mean = self.fc_mean(x)\n\n        #  An ugly hack for my KFAC implementation.\n        zeros = torch.zeros(action_mean.size())\n        if x.is_cuda:\n            zeros = zeros.cuda()\n\n        action_logstd = self.logstd(zeros)\n        return FixedNormal(action_mean, action_logstd.exp())\n\n\nclass Bernoulli(nn.Module):\n    def __init__(self, num_inputs, num_outputs):\n        super(Bernoulli, self).__init__()\n\n        init_ = lambda m: init(m, nn.init.orthogonal_, lambda x: nn.init.\n                               constant_(x, 0))\n\n        self.linear = init_(nn.Linear(num_inputs, num_outputs))\n\n    def forward(self, x):\n        x = self.linear(x)\n        return FixedBernoulli(logits=x)\n'"
a2c_ppo_acktr/envs.py,8,"b'import os\n\nimport gym\nimport numpy as np\nimport torch\nfrom gym.spaces.box import Box\n\nfrom baselines import bench\nfrom baselines.common.atari_wrappers import make_atari, wrap_deepmind\nfrom baselines.common.vec_env import VecEnvWrapper\nfrom baselines.common.vec_env.dummy_vec_env import DummyVecEnv\nfrom baselines.common.vec_env.shmem_vec_env import ShmemVecEnv\nfrom baselines.common.vec_env.vec_normalize import \\\n    VecNormalize as VecNormalize_\n\ntry:\n    import dm_control2gym\nexcept ImportError:\n    pass\n\ntry:\n    import roboschool\nexcept ImportError:\n    pass\n\ntry:\n    import pybullet_envs\nexcept ImportError:\n    pass\n\n\ndef make_env(env_id, seed, rank, log_dir, allow_early_resets):\n    def _thunk():\n        if env_id.startswith(""dm""):\n            _, domain, task = env_id.split(\'.\')\n            env = dm_control2gym.make(domain_name=domain, task_name=task)\n        else:\n            env = gym.make(env_id)\n\n        is_atari = hasattr(gym.envs, \'atari\') and isinstance(\n            env.unwrapped, gym.envs.atari.atari_env.AtariEnv)\n        if is_atari:\n            env = make_atari(env_id)\n\n        env.seed(seed + rank)\n\n        if str(env.__class__.__name__).find(\'TimeLimit\') >= 0:\n            env = TimeLimitMask(env)\n\n        if log_dir is not None:\n            env = bench.Monitor(\n                env,\n                os.path.join(log_dir, str(rank)),\n                allow_early_resets=allow_early_resets)\n\n        if is_atari:\n            if len(env.observation_space.shape) == 3:\n                env = wrap_deepmind(env)\n        elif len(env.observation_space.shape) == 3:\n            raise NotImplementedError(\n                ""CNN models work only for atari,\\n""\n                ""please use a custom wrapper for a custom pixel input env.\\n""\n                ""See wrap_deepmind for an example."")\n\n        # If the input has shape (W,H,3), wrap for PyTorch convolutions\n        obs_shape = env.observation_space.shape\n        if len(obs_shape) == 3 and obs_shape[2] in [1, 3]:\n            env = TransposeImage(env, op=[2, 0, 1])\n\n        return env\n\n    return _thunk\n\n\ndef make_vec_envs(env_name,\n                  seed,\n                  num_processes,\n                  gamma,\n                  log_dir,\n                  device,\n                  allow_early_resets,\n                  num_frame_stack=None):\n    envs = [\n        make_env(env_name, seed, i, log_dir, allow_early_resets)\n        for i in range(num_processes)\n    ]\n\n    if len(envs) > 1:\n        envs = ShmemVecEnv(envs, context=\'fork\')\n    else:\n        envs = DummyVecEnv(envs)\n\n    if len(envs.observation_space.shape) == 1:\n        if gamma is None:\n            envs = VecNormalize(envs, ret=False)\n        else:\n            envs = VecNormalize(envs, gamma=gamma)\n\n    envs = VecPyTorch(envs, device)\n\n    if num_frame_stack is not None:\n        envs = VecPyTorchFrameStack(envs, num_frame_stack, device)\n    elif len(envs.observation_space.shape) == 3:\n        envs = VecPyTorchFrameStack(envs, 4, device)\n\n    return envs\n\n\n# Checks whether done was caused my timit limits or not\nclass TimeLimitMask(gym.Wrapper):\n    def step(self, action):\n        obs, rew, done, info = self.env.step(action)\n        if done and self.env._max_episode_steps == self.env._elapsed_steps:\n            info[\'bad_transition\'] = True\n\n        return obs, rew, done, info\n\n    def reset(self, **kwargs):\n        return self.env.reset(**kwargs)\n\n\n# Can be used to test recurrent policies for Reacher-v2\nclass MaskGoal(gym.ObservationWrapper):\n    def observation(self, observation):\n        if self.env._elapsed_steps > 0:\n            observation[-2:] = 0\n        return observation\n\n\nclass TransposeObs(gym.ObservationWrapper):\n    def __init__(self, env=None):\n        """"""\n        Transpose observation space (base class)\n        """"""\n        super(TransposeObs, self).__init__(env)\n\n\nclass TransposeImage(TransposeObs):\n    def __init__(self, env=None, op=[2, 0, 1]):\n        """"""\n        Transpose observation space for images\n        """"""\n        super(TransposeImage, self).__init__(env)\n        assert len(op) == 3, ""Error: Operation, "" + str(op) + "", must be dim3""\n        self.op = op\n        obs_shape = self.observation_space.shape\n        self.observation_space = Box(\n            self.observation_space.low[0, 0, 0],\n            self.observation_space.high[0, 0, 0], [\n                obs_shape[self.op[0]], obs_shape[self.op[1]],\n                obs_shape[self.op[2]]\n            ],\n            dtype=self.observation_space.dtype)\n\n    def observation(self, ob):\n        return ob.transpose(self.op[0], self.op[1], self.op[2])\n\n\nclass VecPyTorch(VecEnvWrapper):\n    def __init__(self, venv, device):\n        """"""Return only every `skip`-th frame""""""\n        super(VecPyTorch, self).__init__(venv)\n        self.device = device\n        # TODO: Fix data types\n\n    def reset(self):\n        obs = self.venv.reset()\n        obs = torch.from_numpy(obs).float().to(self.device)\n        return obs\n\n    def step_async(self, actions):\n        if isinstance(actions, torch.LongTensor):\n            # Squeeze the dimension for discrete actions\n            actions = actions.squeeze(1)\n        actions = actions.cpu().numpy()\n        self.venv.step_async(actions)\n\n    def step_wait(self):\n        obs, reward, done, info = self.venv.step_wait()\n        obs = torch.from_numpy(obs).float().to(self.device)\n        reward = torch.from_numpy(reward).unsqueeze(dim=1).float()\n        return obs, reward, done, info\n\n\nclass VecNormalize(VecNormalize_):\n    def __init__(self, *args, **kwargs):\n        super(VecNormalize, self).__init__(*args, **kwargs)\n        self.training = True\n\n    def _obfilt(self, obs, update=True):\n        if self.ob_rms:\n            if self.training and update:\n                self.ob_rms.update(obs)\n            obs = np.clip((obs - self.ob_rms.mean) /\n                          np.sqrt(self.ob_rms.var + self.epsilon),\n                          -self.clipob, self.clipob)\n            return obs\n        else:\n            return obs\n\n    def train(self):\n        self.training = True\n\n    def eval(self):\n        self.training = False\n\n\n# Derived from\n# https://github.com/openai/baselines/blob/master/baselines/common/vec_env/vec_frame_stack.py\nclass VecPyTorchFrameStack(VecEnvWrapper):\n    def __init__(self, venv, nstack, device=None):\n        self.venv = venv\n        self.nstack = nstack\n\n        wos = venv.observation_space  # wrapped ob space\n        self.shape_dim0 = wos.shape[0]\n\n        low = np.repeat(wos.low, self.nstack, axis=0)\n        high = np.repeat(wos.high, self.nstack, axis=0)\n\n        if device is None:\n            device = torch.device(\'cpu\')\n        self.stacked_obs = torch.zeros((venv.num_envs, ) +\n                                       low.shape).to(device)\n\n        observation_space = gym.spaces.Box(\n            low=low, high=high, dtype=venv.observation_space.dtype)\n        VecEnvWrapper.__init__(self, venv, observation_space=observation_space)\n\n    def step_wait(self):\n        obs, rews, news, infos = self.venv.step_wait()\n        self.stacked_obs[:, :-self.shape_dim0] = \\\n            self.stacked_obs[:, self.shape_dim0:].clone()\n        for (i, new) in enumerate(news):\n            if new:\n                self.stacked_obs[i] = 0\n        self.stacked_obs[:, -self.shape_dim0:] = obs\n        return self.stacked_obs, rews, news, infos\n\n    def reset(self):\n        obs = self.venv.reset()\n        if torch.backends.cudnn.deterministic:\n            self.stacked_obs = torch.zeros(self.stacked_obs.shape)\n        else:\n            self.stacked_obs.zero_()\n        self.stacked_obs[:, -self.shape_dim0:] = obs\n        return self.stacked_obs\n\n    def close(self):\n        self.venv.close()\n'"
a2c_ppo_acktr/model.py,3,"b'import numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom a2c_ppo_acktr.distributions import Bernoulli, Categorical, DiagGaussian\nfrom a2c_ppo_acktr.utils import init\n\n\nclass Flatten(nn.Module):\n    def forward(self, x):\n        return x.view(x.size(0), -1)\n\n\nclass Policy(nn.Module):\n    def __init__(self, obs_shape, action_space, base=None, base_kwargs=None):\n        super(Policy, self).__init__()\n        if base_kwargs is None:\n            base_kwargs = {}\n        if base is None:\n            if len(obs_shape) == 3:\n                base = CNNBase\n            elif len(obs_shape) == 1:\n                base = MLPBase\n            else:\n                raise NotImplementedError\n\n        self.base = base(obs_shape[0], **base_kwargs)\n\n        if action_space.__class__.__name__ == ""Discrete"":\n            num_outputs = action_space.n\n            self.dist = Categorical(self.base.output_size, num_outputs)\n        elif action_space.__class__.__name__ == ""Box"":\n            num_outputs = action_space.shape[0]\n            self.dist = DiagGaussian(self.base.output_size, num_outputs)\n        elif action_space.__class__.__name__ == ""MultiBinary"":\n            num_outputs = action_space.shape[0]\n            self.dist = Bernoulli(self.base.output_size, num_outputs)\n        else:\n            raise NotImplementedError\n\n    @property\n    def is_recurrent(self):\n        return self.base.is_recurrent\n\n    @property\n    def recurrent_hidden_state_size(self):\n        """"""Size of rnn_hx.""""""\n        return self.base.recurrent_hidden_state_size\n\n    def forward(self, inputs, rnn_hxs, masks):\n        raise NotImplementedError\n\n    def act(self, inputs, rnn_hxs, masks, deterministic=False):\n        value, actor_features, rnn_hxs = self.base(inputs, rnn_hxs, masks)\n        dist = self.dist(actor_features)\n\n        if deterministic:\n            action = dist.mode()\n        else:\n            action = dist.sample()\n\n        action_log_probs = dist.log_probs(action)\n        dist_entropy = dist.entropy().mean()\n\n        return value, action, action_log_probs, rnn_hxs\n\n    def get_value(self, inputs, rnn_hxs, masks):\n        value, _, _ = self.base(inputs, rnn_hxs, masks)\n        return value\n\n    def evaluate_actions(self, inputs, rnn_hxs, masks, action):\n        value, actor_features, rnn_hxs = self.base(inputs, rnn_hxs, masks)\n        dist = self.dist(actor_features)\n\n        action_log_probs = dist.log_probs(action)\n        dist_entropy = dist.entropy().mean()\n\n        return value, action_log_probs, dist_entropy, rnn_hxs\n\n\nclass NNBase(nn.Module):\n    def __init__(self, recurrent, recurrent_input_size, hidden_size):\n        super(NNBase, self).__init__()\n\n        self._hidden_size = hidden_size\n        self._recurrent = recurrent\n\n        if recurrent:\n            self.gru = nn.GRU(recurrent_input_size, hidden_size)\n            for name, param in self.gru.named_parameters():\n                if \'bias\' in name:\n                    nn.init.constant_(param, 0)\n                elif \'weight\' in name:\n                    nn.init.orthogonal_(param)\n\n    @property\n    def is_recurrent(self):\n        return self._recurrent\n\n    @property\n    def recurrent_hidden_state_size(self):\n        if self._recurrent:\n            return self._hidden_size\n        return 1\n\n    @property\n    def output_size(self):\n        return self._hidden_size\n\n    def _forward_gru(self, x, hxs, masks):\n        if x.size(0) == hxs.size(0):\n            x, hxs = self.gru(x.unsqueeze(0), (hxs * masks).unsqueeze(0))\n            x = x.squeeze(0)\n            hxs = hxs.squeeze(0)\n        else:\n            # x is a (T, N, -1) tensor that has been flatten to (T * N, -1)\n            N = hxs.size(0)\n            T = int(x.size(0) / N)\n\n            # unflatten\n            x = x.view(T, N, x.size(1))\n\n            # Same deal with masks\n            masks = masks.view(T, N)\n\n            # Let\'s figure out which steps in the sequence have a zero for any agent\n            # We will always assume t=0 has a zero in it as that makes the logic cleaner\n            has_zeros = ((masks[1:] == 0.0) \\\n                            .any(dim=-1)\n                            .nonzero()\n                            .squeeze()\n                            .cpu())\n\n            # +1 to correct the masks[1:]\n            if has_zeros.dim() == 0:\n                # Deal with scalar\n                has_zeros = [has_zeros.item() + 1]\n            else:\n                has_zeros = (has_zeros + 1).numpy().tolist()\n\n            # add t=0 and t=T to the list\n            has_zeros = [0] + has_zeros + [T]\n\n            hxs = hxs.unsqueeze(0)\n            outputs = []\n            for i in range(len(has_zeros) - 1):\n                # We can now process steps that don\'t have any zeros in masks together!\n                # This is much faster\n                start_idx = has_zeros[i]\n                end_idx = has_zeros[i + 1]\n\n                rnn_scores, hxs = self.gru(\n                    x[start_idx:end_idx],\n                    hxs * masks[start_idx].view(1, -1, 1))\n\n                outputs.append(rnn_scores)\n\n            # assert len(outputs) == T\n            # x is a (T, N, -1) tensor\n            x = torch.cat(outputs, dim=0)\n            # flatten\n            x = x.view(T * N, -1)\n            hxs = hxs.squeeze(0)\n\n        return x, hxs\n\n\nclass CNNBase(NNBase):\n    def __init__(self, num_inputs, recurrent=False, hidden_size=512):\n        super(CNNBase, self).__init__(recurrent, hidden_size, hidden_size)\n\n        init_ = lambda m: init(m, nn.init.orthogonal_, lambda x: nn.init.\n                               constant_(x, 0), nn.init.calculate_gain(\'relu\'))\n\n        self.main = nn.Sequential(\n            init_(nn.Conv2d(num_inputs, 32, 8, stride=4)), nn.ReLU(),\n            init_(nn.Conv2d(32, 64, 4, stride=2)), nn.ReLU(),\n            init_(nn.Conv2d(64, 32, 3, stride=1)), nn.ReLU(), Flatten(),\n            init_(nn.Linear(32 * 7 * 7, hidden_size)), nn.ReLU())\n\n        init_ = lambda m: init(m, nn.init.orthogonal_, lambda x: nn.init.\n                               constant_(x, 0))\n\n        self.critic_linear = init_(nn.Linear(hidden_size, 1))\n\n        self.train()\n\n    def forward(self, inputs, rnn_hxs, masks):\n        x = self.main(inputs / 255.0)\n\n        if self.is_recurrent:\n            x, rnn_hxs = self._forward_gru(x, rnn_hxs, masks)\n\n        return self.critic_linear(x), x, rnn_hxs\n\n\nclass MLPBase(NNBase):\n    def __init__(self, num_inputs, recurrent=False, hidden_size=64):\n        super(MLPBase, self).__init__(recurrent, num_inputs, hidden_size)\n\n        if recurrent:\n            num_inputs = hidden_size\n\n        init_ = lambda m: init(m, nn.init.orthogonal_, lambda x: nn.init.\n                               constant_(x, 0), np.sqrt(2))\n\n        self.actor = nn.Sequential(\n            init_(nn.Linear(num_inputs, hidden_size)), nn.Tanh(),\n            init_(nn.Linear(hidden_size, hidden_size)), nn.Tanh())\n\n        self.critic = nn.Sequential(\n            init_(nn.Linear(num_inputs, hidden_size)), nn.Tanh(),\n            init_(nn.Linear(hidden_size, hidden_size)), nn.Tanh())\n\n        self.critic_linear = init_(nn.Linear(hidden_size, 1))\n\n        self.train()\n\n    def forward(self, inputs, rnn_hxs, masks):\n        x = inputs\n\n        if self.is_recurrent:\n            x, rnn_hxs = self._forward_gru(x, rnn_hxs, masks)\n\n        hidden_critic = self.critic(x)\n        hidden_actor = self.actor(x)\n\n        return self.critic_linear(hidden_critic), hidden_actor, rnn_hxs\n'"
a2c_ppo_acktr/storage.py,19,"b'import torch\nfrom torch.utils.data.sampler import BatchSampler, SubsetRandomSampler\n\n\ndef _flatten_helper(T, N, _tensor):\n    return _tensor.view(T * N, *_tensor.size()[2:])\n\n\nclass RolloutStorage(object):\n    def __init__(self, num_steps, num_processes, obs_shape, action_space,\n                 recurrent_hidden_state_size):\n        self.obs = torch.zeros(num_steps + 1, num_processes, *obs_shape)\n        self.recurrent_hidden_states = torch.zeros(\n            num_steps + 1, num_processes, recurrent_hidden_state_size)\n        self.rewards = torch.zeros(num_steps, num_processes, 1)\n        self.value_preds = torch.zeros(num_steps + 1, num_processes, 1)\n        self.returns = torch.zeros(num_steps + 1, num_processes, 1)\n        self.action_log_probs = torch.zeros(num_steps, num_processes, 1)\n        if action_space.__class__.__name__ == \'Discrete\':\n            action_shape = 1\n        else:\n            action_shape = action_space.shape[0]\n        self.actions = torch.zeros(num_steps, num_processes, action_shape)\n        if action_space.__class__.__name__ == \'Discrete\':\n            self.actions = self.actions.long()\n        self.masks = torch.ones(num_steps + 1, num_processes, 1)\n\n        # Masks that indicate whether it\'s a true terminal state\n        # or time limit end state\n        self.bad_masks = torch.ones(num_steps + 1, num_processes, 1)\n\n        self.num_steps = num_steps\n        self.step = 0\n\n    def to(self, device):\n        self.obs = self.obs.to(device)\n        self.recurrent_hidden_states = self.recurrent_hidden_states.to(device)\n        self.rewards = self.rewards.to(device)\n        self.value_preds = self.value_preds.to(device)\n        self.returns = self.returns.to(device)\n        self.action_log_probs = self.action_log_probs.to(device)\n        self.actions = self.actions.to(device)\n        self.masks = self.masks.to(device)\n        self.bad_masks = self.bad_masks.to(device)\n\n    def insert(self, obs, recurrent_hidden_states, actions, action_log_probs,\n               value_preds, rewards, masks, bad_masks):\n        self.obs[self.step + 1].copy_(obs)\n        self.recurrent_hidden_states[self.step +\n                                     1].copy_(recurrent_hidden_states)\n        self.actions[self.step].copy_(actions)\n        self.action_log_probs[self.step].copy_(action_log_probs)\n        self.value_preds[self.step].copy_(value_preds)\n        self.rewards[self.step].copy_(rewards)\n        self.masks[self.step + 1].copy_(masks)\n        self.bad_masks[self.step + 1].copy_(bad_masks)\n\n        self.step = (self.step + 1) % self.num_steps\n\n    def after_update(self):\n        self.obs[0].copy_(self.obs[-1])\n        self.recurrent_hidden_states[0].copy_(self.recurrent_hidden_states[-1])\n        self.masks[0].copy_(self.masks[-1])\n        self.bad_masks[0].copy_(self.bad_masks[-1])\n\n    def compute_returns(self,\n                        next_value,\n                        use_gae,\n                        gamma,\n                        gae_lambda,\n                        use_proper_time_limits=True):\n        if use_proper_time_limits:\n            if use_gae:\n                self.value_preds[-1] = next_value\n                gae = 0\n                for step in reversed(range(self.rewards.size(0))):\n                    delta = self.rewards[step] + gamma * self.value_preds[\n                        step + 1] * self.masks[step +\n                                               1] - self.value_preds[step]\n                    gae = delta + gamma * gae_lambda * self.masks[step +\n                                                                  1] * gae\n                    gae = gae * self.bad_masks[step + 1]\n                    self.returns[step] = gae + self.value_preds[step]\n            else:\n                self.returns[-1] = next_value\n                for step in reversed(range(self.rewards.size(0))):\n                    self.returns[step] = (self.returns[step + 1] * \\\n                        gamma * self.masks[step + 1] + self.rewards[step]) * self.bad_masks[step + 1] \\\n                        + (1 - self.bad_masks[step + 1]) * self.value_preds[step]\n        else:\n            if use_gae:\n                self.value_preds[-1] = next_value\n                gae = 0\n                for step in reversed(range(self.rewards.size(0))):\n                    delta = self.rewards[step] + gamma * self.value_preds[\n                        step + 1] * self.masks[step +\n                                               1] - self.value_preds[step]\n                    gae = delta + gamma * gae_lambda * self.masks[step +\n                                                                  1] * gae\n                    self.returns[step] = gae + self.value_preds[step]\n            else:\n                self.returns[-1] = next_value\n                for step in reversed(range(self.rewards.size(0))):\n                    self.returns[step] = self.returns[step + 1] * \\\n                        gamma * self.masks[step + 1] + self.rewards[step]\n\n    def feed_forward_generator(self,\n                               advantages,\n                               num_mini_batch=None,\n                               mini_batch_size=None):\n        num_steps, num_processes = self.rewards.size()[0:2]\n        batch_size = num_processes * num_steps\n\n        if mini_batch_size is None:\n            assert batch_size >= num_mini_batch, (\n                ""PPO requires the number of processes ({}) ""\n                ""* number of steps ({}) = {} ""\n                ""to be greater than or equal to the number of PPO mini batches ({}).""\n                """".format(num_processes, num_steps, num_processes * num_steps,\n                          num_mini_batch))\n            mini_batch_size = batch_size // num_mini_batch\n        sampler = BatchSampler(\n            SubsetRandomSampler(range(batch_size)),\n            mini_batch_size,\n            drop_last=True)\n        for indices in sampler:\n            obs_batch = self.obs[:-1].view(-1, *self.obs.size()[2:])[indices]\n            recurrent_hidden_states_batch = self.recurrent_hidden_states[:-1].view(\n                -1, self.recurrent_hidden_states.size(-1))[indices]\n            actions_batch = self.actions.view(-1,\n                                              self.actions.size(-1))[indices]\n            value_preds_batch = self.value_preds[:-1].view(-1, 1)[indices]\n            return_batch = self.returns[:-1].view(-1, 1)[indices]\n            masks_batch = self.masks[:-1].view(-1, 1)[indices]\n            old_action_log_probs_batch = self.action_log_probs.view(-1,\n                                                                    1)[indices]\n            if advantages is None:\n                adv_targ = None\n            else:\n                adv_targ = advantages.view(-1, 1)[indices]\n\n            yield obs_batch, recurrent_hidden_states_batch, actions_batch, \\\n                value_preds_batch, return_batch, masks_batch, old_action_log_probs_batch, adv_targ\n\n    def recurrent_generator(self, advantages, num_mini_batch):\n        num_processes = self.rewards.size(1)\n        assert num_processes >= num_mini_batch, (\n            ""PPO requires the number of processes ({}) ""\n            ""to be greater than or equal to the number of ""\n            ""PPO mini batches ({})."".format(num_processes, num_mini_batch))\n        num_envs_per_batch = num_processes // num_mini_batch\n        perm = torch.randperm(num_processes)\n        for start_ind in range(0, num_processes, num_envs_per_batch):\n            obs_batch = []\n            recurrent_hidden_states_batch = []\n            actions_batch = []\n            value_preds_batch = []\n            return_batch = []\n            masks_batch = []\n            old_action_log_probs_batch = []\n            adv_targ = []\n\n            for offset in range(num_envs_per_batch):\n                ind = perm[start_ind + offset]\n                obs_batch.append(self.obs[:-1, ind])\n                recurrent_hidden_states_batch.append(\n                    self.recurrent_hidden_states[0:1, ind])\n                actions_batch.append(self.actions[:, ind])\n                value_preds_batch.append(self.value_preds[:-1, ind])\n                return_batch.append(self.returns[:-1, ind])\n                masks_batch.append(self.masks[:-1, ind])\n                old_action_log_probs_batch.append(\n                    self.action_log_probs[:, ind])\n                adv_targ.append(advantages[:, ind])\n\n            T, N = self.num_steps, num_envs_per_batch\n            # These are all tensors of size (T, N, -1)\n            obs_batch = torch.stack(obs_batch, 1)\n            actions_batch = torch.stack(actions_batch, 1)\n            value_preds_batch = torch.stack(value_preds_batch, 1)\n            return_batch = torch.stack(return_batch, 1)\n            masks_batch = torch.stack(masks_batch, 1)\n            old_action_log_probs_batch = torch.stack(\n                old_action_log_probs_batch, 1)\n            adv_targ = torch.stack(adv_targ, 1)\n\n            # States is just a (N, -1) tensor\n            recurrent_hidden_states_batch = torch.stack(\n                recurrent_hidden_states_batch, 1).view(N, -1)\n\n            # Flatten the (T, N, ...) tensors to (T * N, ...)\n            obs_batch = _flatten_helper(T, N, obs_batch)\n            actions_batch = _flatten_helper(T, N, actions_batch)\n            value_preds_batch = _flatten_helper(T, N, value_preds_batch)\n            return_batch = _flatten_helper(T, N, return_batch)\n            masks_batch = _flatten_helper(T, N, masks_batch)\n            old_action_log_probs_batch = _flatten_helper(T, N, \\\n                    old_action_log_probs_batch)\n            adv_targ = _flatten_helper(T, N, adv_targ)\n\n            yield obs_batch, recurrent_hidden_states_batch, actions_batch, \\\n                value_preds_batch, return_batch, masks_batch, old_action_log_probs_batch, adv_targ\n'"
a2c_ppo_acktr/utils.py,1,"b'import glob\nimport os\n\nimport torch\nimport torch.nn as nn\n\nfrom a2c_ppo_acktr.envs import VecNormalize\n\n\n# Get a render function\ndef get_render_func(venv):\n    if hasattr(venv, \'envs\'):\n        return venv.envs[0].render\n    elif hasattr(venv, \'venv\'):\n        return get_render_func(venv.venv)\n    elif hasattr(venv, \'env\'):\n        return get_render_func(venv.env)\n\n    return None\n\n\ndef get_vec_normalize(venv):\n    if isinstance(venv, VecNormalize):\n        return venv\n    elif hasattr(venv, \'venv\'):\n        return get_vec_normalize(venv.venv)\n\n    return None\n\n\n# Necessary for my KFAC implementation.\nclass AddBias(nn.Module):\n    def __init__(self, bias):\n        super(AddBias, self).__init__()\n        self._bias = nn.Parameter(bias.unsqueeze(1))\n\n    def forward(self, x):\n        if x.dim() == 2:\n            bias = self._bias.t().view(1, -1)\n        else:\n            bias = self._bias.t().view(1, -1, 1, 1)\n\n        return x + bias\n\n\ndef update_linear_schedule(optimizer, epoch, total_num_epochs, initial_lr):\n    """"""Decreases the learning rate linearly""""""\n    lr = initial_lr - (initial_lr * (epoch / float(total_num_epochs)))\n    for param_group in optimizer.param_groups:\n        param_group[\'lr\'] = lr\n\n\ndef init(module, weight_init, bias_init, gain=1):\n    weight_init(module.weight.data, gain=gain)\n    bias_init(module.bias.data)\n    return module\n\n\ndef cleanup_log_dir(log_dir):\n    try:\n        os.makedirs(log_dir)\n    except OSError:\n        files = glob.glob(os.path.join(log_dir, \'*.monitor.csv\'))\n        for f in files:\n            os.remove(f)\n'"
gail_experts/convert_to_pytorch.py,5,"b""import argparse\nimport os\nimport sys\n\nimport h5py\nimport numpy as np\nimport torch\n\n\ndef main():\n    parser = argparse.ArgumentParser(\n        'Converts expert trajectories from h5 to pt format.')\n    parser.add_argument(\n        '--h5-file',\n        default='trajs_halfcheetah.h5',\n        help='input h5 file',\n        type=str)\n    parser.add_argument(\n        '--pt-file',\n        default=None,\n        help='output pt file, by default replaces file extension with pt',\n        type=str)\n    args = parser.parse_args()\n\n    if args.pt_file is None:\n        args.pt_file = os.path.splitext(args.h5_file)[0] + '.pt'\n\n    with h5py.File(args.h5_file, 'r') as f:\n        dataset_size = f['obs_B_T_Do'].shape[0]  # full dataset size\n\n        states = f['obs_B_T_Do'][:dataset_size, ...][...]\n        actions = f['a_B_T_Da'][:dataset_size, ...][...]\n        rewards = f['r_B_T'][:dataset_size, ...][...]\n        lens = f['len_B'][:dataset_size, ...][...]\n\n        states = torch.from_numpy(states).float()\n        actions = torch.from_numpy(actions).float()\n        rewards = torch.from_numpy(rewards).float()\n        lens = torch.from_numpy(lens).long()\n\n    data = {\n        'states': states,\n        'actions': actions,\n        'rewards': rewards,\n        'lengths': lens\n    }\n\n    torch.save(data, args.pt_file)\n\n\nif __name__ == '__main__':\n    main()\n"""
a2c_ppo_acktr/algo/__init__.py,0,b'from .a2c_acktr import A2C_ACKTR\nfrom .ppo import PPO'
a2c_ppo_acktr/algo/a2c_acktr.py,3,"b'import torch\nimport torch.nn as nn\nimport torch.optim as optim\n\nfrom a2c_ppo_acktr.algo.kfac import KFACOptimizer\n\n\nclass A2C_ACKTR():\n    def __init__(self,\n                 actor_critic,\n                 value_loss_coef,\n                 entropy_coef,\n                 lr=None,\n                 eps=None,\n                 alpha=None,\n                 max_grad_norm=None,\n                 acktr=False):\n\n        self.actor_critic = actor_critic\n        self.acktr = acktr\n\n        self.value_loss_coef = value_loss_coef\n        self.entropy_coef = entropy_coef\n\n        self.max_grad_norm = max_grad_norm\n\n        if acktr:\n            self.optimizer = KFACOptimizer(actor_critic)\n        else:\n            self.optimizer = optim.RMSprop(\n                actor_critic.parameters(), lr, eps=eps, alpha=alpha)\n\n    def update(self, rollouts):\n        obs_shape = rollouts.obs.size()[2:]\n        action_shape = rollouts.actions.size()[-1]\n        num_steps, num_processes, _ = rollouts.rewards.size()\n\n        values, action_log_probs, dist_entropy, _ = self.actor_critic.evaluate_actions(\n            rollouts.obs[:-1].view(-1, *obs_shape),\n            rollouts.recurrent_hidden_states[0].view(\n                -1, self.actor_critic.recurrent_hidden_state_size),\n            rollouts.masks[:-1].view(-1, 1),\n            rollouts.actions.view(-1, action_shape))\n\n        values = values.view(num_steps, num_processes, 1)\n        action_log_probs = action_log_probs.view(num_steps, num_processes, 1)\n\n        advantages = rollouts.returns[:-1] - values\n        value_loss = advantages.pow(2).mean()\n\n        action_loss = -(advantages.detach() * action_log_probs).mean()\n\n        if self.acktr and self.optimizer.steps % self.optimizer.Ts == 0:\n            # Compute fisher, see Martens 2014\n            self.actor_critic.zero_grad()\n            pg_fisher_loss = -action_log_probs.mean()\n\n            value_noise = torch.randn(values.size())\n            if values.is_cuda:\n                value_noise = value_noise.cuda()\n\n            sample_values = values + value_noise\n            vf_fisher_loss = -(values - sample_values.detach()).pow(2).mean()\n\n            fisher_loss = pg_fisher_loss + vf_fisher_loss\n            self.optimizer.acc_stats = True\n            fisher_loss.backward(retain_graph=True)\n            self.optimizer.acc_stats = False\n\n        self.optimizer.zero_grad()\n        (value_loss * self.value_loss_coef + action_loss -\n         dist_entropy * self.entropy_coef).backward()\n\n        if self.acktr == False:\n            nn.utils.clip_grad_norm_(self.actor_critic.parameters(),\n                                     self.max_grad_norm)\n\n        self.optimizer.step()\n\n        return value_loss.item(), action_loss.item(), dist_entropy.item()\n'"
a2c_ppo_acktr/algo/gail.py,21,"b""import h5py\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.utils.data\nfrom torch import autograd\n\nfrom baselines.common.running_mean_std import RunningMeanStd\n\n\nclass Discriminator(nn.Module):\n    def __init__(self, input_dim, hidden_dim, device):\n        super(Discriminator, self).__init__()\n\n        self.device = device\n\n        self.trunk = nn.Sequential(\n            nn.Linear(input_dim, hidden_dim), nn.Tanh(),\n            nn.Linear(hidden_dim, hidden_dim), nn.Tanh(),\n            nn.Linear(hidden_dim, 1)).to(device)\n\n        self.trunk.train()\n\n        self.optimizer = torch.optim.Adam(self.trunk.parameters())\n\n        self.returns = None\n        self.ret_rms = RunningMeanStd(shape=())\n\n    def compute_grad_pen(self,\n                         expert_state,\n                         expert_action,\n                         policy_state,\n                         policy_action,\n                         lambda_=10):\n        alpha = torch.rand(expert_state.size(0), 1)\n        expert_data = torch.cat([expert_state, expert_action], dim=1)\n        policy_data = torch.cat([policy_state, policy_action], dim=1)\n\n        alpha = alpha.expand_as(expert_data).to(expert_data.device)\n\n        mixup_data = alpha * expert_data + (1 - alpha) * policy_data\n        mixup_data.requires_grad = True\n\n        disc = self.trunk(mixup_data)\n        ones = torch.ones(disc.size()).to(disc.device)\n        grad = autograd.grad(\n            outputs=disc,\n            inputs=mixup_data,\n            grad_outputs=ones,\n            create_graph=True,\n            retain_graph=True,\n            only_inputs=True)[0]\n\n        grad_pen = lambda_ * (grad.norm(2, dim=1) - 1).pow(2).mean()\n        return grad_pen\n\n    def update(self, expert_loader, rollouts, obsfilt=None):\n        self.train()\n\n        policy_data_generator = rollouts.feed_forward_generator(\n            None, mini_batch_size=expert_loader.batch_size)\n\n        loss = 0\n        n = 0\n        for expert_batch, policy_batch in zip(expert_loader,\n                                              policy_data_generator):\n            policy_state, policy_action = policy_batch[0], policy_batch[2]\n            policy_d = self.trunk(\n                torch.cat([policy_state, policy_action], dim=1))\n\n            expert_state, expert_action = expert_batch\n            expert_state = obsfilt(expert_state.numpy(), update=False)\n            expert_state = torch.FloatTensor(expert_state).to(self.device)\n            expert_action = expert_action.to(self.device)\n            expert_d = self.trunk(\n                torch.cat([expert_state, expert_action], dim=1))\n\n            expert_loss = F.binary_cross_entropy_with_logits(\n                expert_d,\n                torch.ones(expert_d.size()).to(self.device))\n            policy_loss = F.binary_cross_entropy_with_logits(\n                policy_d,\n                torch.zeros(policy_d.size()).to(self.device))\n\n            gail_loss = expert_loss + policy_loss\n            grad_pen = self.compute_grad_pen(expert_state, expert_action,\n                                             policy_state, policy_action)\n\n            loss += (gail_loss + grad_pen).item()\n            n += 1\n\n            self.optimizer.zero_grad()\n            (gail_loss + grad_pen).backward()\n            self.optimizer.step()\n        return loss / n\n\n    def predict_reward(self, state, action, gamma, masks, update_rms=True):\n        with torch.no_grad():\n            self.eval()\n            d = self.trunk(torch.cat([state, action], dim=1))\n            s = torch.sigmoid(d)\n            reward = s.log() - (1 - s).log()\n            if self.returns is None:\n                self.returns = reward.clone()\n\n            if update_rms:\n                self.returns = self.returns * masks * gamma + reward\n                self.ret_rms.update(self.returns.cpu().numpy())\n\n            return reward / np.sqrt(self.ret_rms.var[0] + 1e-8)\n\n\nclass ExpertDataset(torch.utils.data.Dataset):\n    def __init__(self, file_name, num_trajectories=4, subsample_frequency=20):\n        all_trajectories = torch.load(file_name)\n        \n        perm = torch.randperm(all_trajectories['states'].size(0))\n        idx = perm[:num_trajectories]\n\n        self.trajectories = {}\n        \n        # See https://github.com/pytorch/pytorch/issues/14886\n        # .long() for fixing bug in torch v0.4.1\n        start_idx = torch.randint(\n            0, subsample_frequency, size=(num_trajectories, )).long()\n\n        for k, v in all_trajectories.items():\n            data = v[idx]\n\n            if k != 'lengths':\n                samples = []\n                for i in range(num_trajectories):\n                    samples.append(data[i, start_idx[i]::subsample_frequency])\n                self.trajectories[k] = torch.stack(samples)\n            else:\n                self.trajectories[k] = data // subsample_frequency\n\n        self.i2traj_idx = {}\n        self.i2i = {}\n        \n        self.length = self.trajectories['lengths'].sum().item()\n\n        traj_idx = 0\n        i = 0\n\n        self.get_idx = []\n        \n        for j in range(self.length):\n            \n            while self.trajectories['lengths'][traj_idx].item() <= i:\n                i -= self.trajectories['lengths'][traj_idx].item()\n                traj_idx += 1\n\n            self.get_idx.append((traj_idx, i))\n\n            i += 1\n            \n            \n    def __len__(self):\n        return self.length\n\n    def __getitem__(self, i):\n        traj_idx, i = self.get_idx[i]\n\n        return self.trajectories['states'][traj_idx][i], self.trajectories[\n            'actions'][traj_idx][i]\n"""
a2c_ppo_acktr/algo/kfac.py,7,"b'import math\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\n\nfrom a2c_ppo_acktr.utils import AddBias\n\n# TODO: In order to make this code faster:\n# 1) Implement _extract_patches as a single cuda kernel\n# 2) Compute QR decomposition in a separate process\n# 3) Actually make a general KFAC optimizer so it fits PyTorch\n\n\ndef _extract_patches(x, kernel_size, stride, padding):\n    if padding[0] + padding[1] > 0:\n        x = F.pad(x, (padding[1], padding[1], padding[0],\n                      padding[0])).data  # Actually check dims\n    x = x.unfold(2, kernel_size[0], stride[0])\n    x = x.unfold(3, kernel_size[1], stride[1])\n    x = x.transpose_(1, 2).transpose_(2, 3).contiguous()\n    x = x.view(\n        x.size(0), x.size(1), x.size(2),\n        x.size(3) * x.size(4) * x.size(5))\n    return x\n\n\ndef compute_cov_a(a, classname, layer_info, fast_cnn):\n    batch_size = a.size(0)\n\n    if classname == \'Conv2d\':\n        if fast_cnn:\n            a = _extract_patches(a, *layer_info)\n            a = a.view(a.size(0), -1, a.size(-1))\n            a = a.mean(1)\n        else:\n            a = _extract_patches(a, *layer_info)\n            a = a.view(-1, a.size(-1)).div_(a.size(1)).div_(a.size(2))\n    elif classname == \'AddBias\':\n        is_cuda = a.is_cuda\n        a = torch.ones(a.size(0), 1)\n        if is_cuda:\n            a = a.cuda()\n\n    return a.t() @ (a / batch_size)\n\n\ndef compute_cov_g(g, classname, layer_info, fast_cnn):\n    batch_size = g.size(0)\n\n    if classname == \'Conv2d\':\n        if fast_cnn:\n            g = g.view(g.size(0), g.size(1), -1)\n            g = g.sum(-1)\n        else:\n            g = g.transpose(1, 2).transpose(2, 3).contiguous()\n            g = g.view(-1, g.size(-1)).mul_(g.size(1)).mul_(g.size(2))\n    elif classname == \'AddBias\':\n        g = g.view(g.size(0), g.size(1), -1)\n        g = g.sum(-1)\n\n    g_ = g * batch_size\n    return g_.t() @ (g_ / g.size(0))\n\n\ndef update_running_stat(aa, m_aa, momentum):\n    # Do the trick to keep aa unchanged and not create any additional tensors\n    m_aa *= momentum / (1 - momentum)\n    m_aa += aa\n    m_aa *= (1 - momentum)\n\n\nclass SplitBias(nn.Module):\n    def __init__(self, module):\n        super(SplitBias, self).__init__()\n        self.module = module\n        self.add_bias = AddBias(module.bias.data)\n        self.module.bias = None\n\n    def forward(self, input):\n        x = self.module(input)\n        x = self.add_bias(x)\n        return x\n\n\nclass KFACOptimizer(optim.Optimizer):\n    def __init__(self,\n                 model,\n                 lr=0.25,\n                 momentum=0.9,\n                 stat_decay=0.99,\n                 kl_clip=0.001,\n                 damping=1e-2,\n                 weight_decay=0,\n                 fast_cnn=False,\n                 Ts=1,\n                 Tf=10):\n        defaults = dict()\n\n        def split_bias(module):\n            for mname, child in module.named_children():\n                if hasattr(child, \'bias\') and child.bias is not None:\n                    module._modules[mname] = SplitBias(child)\n                else:\n                    split_bias(child)\n\n        split_bias(model)\n\n        super(KFACOptimizer, self).__init__(model.parameters(), defaults)\n\n        self.known_modules = {\'Linear\', \'Conv2d\', \'AddBias\'}\n\n        self.modules = []\n        self.grad_outputs = {}\n\n        self.model = model\n        self._prepare_model()\n\n        self.steps = 0\n\n        self.m_aa, self.m_gg = {}, {}\n        self.Q_a, self.Q_g = {}, {}\n        self.d_a, self.d_g = {}, {}\n\n        self.momentum = momentum\n        self.stat_decay = stat_decay\n\n        self.lr = lr\n        self.kl_clip = kl_clip\n        self.damping = damping\n        self.weight_decay = weight_decay\n\n        self.fast_cnn = fast_cnn\n\n        self.Ts = Ts\n        self.Tf = Tf\n\n        self.optim = optim.SGD(\n            model.parameters(),\n            lr=self.lr * (1 - self.momentum),\n            momentum=self.momentum)\n\n    def _save_input(self, module, input):\n        if torch.is_grad_enabled() and self.steps % self.Ts == 0:\n            classname = module.__class__.__name__\n            layer_info = None\n            if classname == \'Conv2d\':\n                layer_info = (module.kernel_size, module.stride,\n                              module.padding)\n\n            aa = compute_cov_a(input[0].data, classname, layer_info,\n                               self.fast_cnn)\n\n            # Initialize buffers\n            if self.steps == 0:\n                self.m_aa[module] = aa.clone()\n\n            update_running_stat(aa, self.m_aa[module], self.stat_decay)\n\n    def _save_grad_output(self, module, grad_input, grad_output):\n        # Accumulate statistics for Fisher matrices\n        if self.acc_stats:\n            classname = module.__class__.__name__\n            layer_info = None\n            if classname == \'Conv2d\':\n                layer_info = (module.kernel_size, module.stride,\n                              module.padding)\n\n            gg = compute_cov_g(grad_output[0].data, classname, layer_info,\n                               self.fast_cnn)\n\n            # Initialize buffers\n            if self.steps == 0:\n                self.m_gg[module] = gg.clone()\n\n            update_running_stat(gg, self.m_gg[module], self.stat_decay)\n\n    def _prepare_model(self):\n        for module in self.model.modules():\n            classname = module.__class__.__name__\n            if classname in self.known_modules:\n                assert not ((classname in [\'Linear\', \'Conv2d\']) and module.bias is not None), \\\n                                    ""You must have a bias as a separate layer""\n\n                self.modules.append(module)\n                module.register_forward_pre_hook(self._save_input)\n                module.register_backward_hook(self._save_grad_output)\n\n    def step(self):\n        # Add weight decay\n        if self.weight_decay > 0:\n            for p in self.model.parameters():\n                p.grad.data.add_(self.weight_decay, p.data)\n\n        updates = {}\n        for i, m in enumerate(self.modules):\n            assert len(list(m.parameters())\n                       ) == 1, ""Can handle only one parameter at the moment""\n            classname = m.__class__.__name__\n            p = next(m.parameters())\n\n            la = self.damping + self.weight_decay\n\n            if self.steps % self.Tf == 0:\n                # My asynchronous implementation exists, I will add it later.\n                # Experimenting with different ways to this in PyTorch.\n                self.d_a[m], self.Q_a[m] = torch.symeig(\n                    self.m_aa[m], eigenvectors=True)\n                self.d_g[m], self.Q_g[m] = torch.symeig(\n                    self.m_gg[m], eigenvectors=True)\n\n                self.d_a[m].mul_((self.d_a[m] > 1e-6).float())\n                self.d_g[m].mul_((self.d_g[m] > 1e-6).float())\n\n            if classname == \'Conv2d\':\n                p_grad_mat = p.grad.data.view(p.grad.data.size(0), -1)\n            else:\n                p_grad_mat = p.grad.data\n\n            v1 = self.Q_g[m].t() @ p_grad_mat @ self.Q_a[m]\n            v2 = v1 / (\n                self.d_g[m].unsqueeze(1) * self.d_a[m].unsqueeze(0) + la)\n            v = self.Q_g[m] @ v2 @ self.Q_a[m].t()\n\n            v = v.view(p.grad.data.size())\n            updates[p] = v\n\n        vg_sum = 0\n        for p in self.model.parameters():\n            v = updates[p]\n            vg_sum += (v * p.grad.data * self.lr * self.lr).sum()\n\n        nu = min(1, math.sqrt(self.kl_clip / vg_sum))\n\n        for p in self.model.parameters():\n            v = updates[p]\n            p.grad.data.copy_(v)\n            p.grad.data.mul_(nu)\n\n        self.optim.step()\n        self.steps += 1\n'"
a2c_ppo_acktr/algo/ppo.py,7,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\n\n\nclass PPO():\n    def __init__(self,\n                 actor_critic,\n                 clip_param,\n                 ppo_epoch,\n                 num_mini_batch,\n                 value_loss_coef,\n                 entropy_coef,\n                 lr=None,\n                 eps=None,\n                 max_grad_norm=None,\n                 use_clipped_value_loss=True):\n\n        self.actor_critic = actor_critic\n\n        self.clip_param = clip_param\n        self.ppo_epoch = ppo_epoch\n        self.num_mini_batch = num_mini_batch\n\n        self.value_loss_coef = value_loss_coef\n        self.entropy_coef = entropy_coef\n\n        self.max_grad_norm = max_grad_norm\n        self.use_clipped_value_loss = use_clipped_value_loss\n\n        self.optimizer = optim.Adam(actor_critic.parameters(), lr=lr, eps=eps)\n\n    def update(self, rollouts):\n        advantages = rollouts.returns[:-1] - rollouts.value_preds[:-1]\n        advantages = (advantages - advantages.mean()) / (\n            advantages.std() + 1e-5)\n\n        value_loss_epoch = 0\n        action_loss_epoch = 0\n        dist_entropy_epoch = 0\n\n        for e in range(self.ppo_epoch):\n            if self.actor_critic.is_recurrent:\n                data_generator = rollouts.recurrent_generator(\n                    advantages, self.num_mini_batch)\n            else:\n                data_generator = rollouts.feed_forward_generator(\n                    advantages, self.num_mini_batch)\n\n            for sample in data_generator:\n                obs_batch, recurrent_hidden_states_batch, actions_batch, \\\n                   value_preds_batch, return_batch, masks_batch, old_action_log_probs_batch, \\\n                        adv_targ = sample\n\n                # Reshape to do in a single forward pass for all steps\n                values, action_log_probs, dist_entropy, _ = self.actor_critic.evaluate_actions(\n                    obs_batch, recurrent_hidden_states_batch, masks_batch,\n                    actions_batch)\n\n                ratio = torch.exp(action_log_probs -\n                                  old_action_log_probs_batch)\n                surr1 = ratio * adv_targ\n                surr2 = torch.clamp(ratio, 1.0 - self.clip_param,\n                                    1.0 + self.clip_param) * adv_targ\n                action_loss = -torch.min(surr1, surr2).mean()\n\n                if self.use_clipped_value_loss:\n                    value_pred_clipped = value_preds_batch + \\\n                        (values - value_preds_batch).clamp(-self.clip_param, self.clip_param)\n                    value_losses = (values - return_batch).pow(2)\n                    value_losses_clipped = (\n                        value_pred_clipped - return_batch).pow(2)\n                    value_loss = 0.5 * torch.max(value_losses,\n                                                 value_losses_clipped).mean()\n                else:\n                    value_loss = 0.5 * (return_batch - values).pow(2).mean()\n\n                self.optimizer.zero_grad()\n                (value_loss * self.value_loss_coef + action_loss -\n                 dist_entropy * self.entropy_coef).backward()\n                nn.utils.clip_grad_norm_(self.actor_critic.parameters(),\n                                         self.max_grad_norm)\n                self.optimizer.step()\n\n                value_loss_epoch += value_loss.item()\n                action_loss_epoch += action_loss.item()\n                dist_entropy_epoch += dist_entropy.item()\n\n        num_updates = self.ppo_epoch * self.num_mini_batch\n\n        value_loss_epoch /= num_updates\n        action_loss_epoch /= num_updates\n        dist_entropy_epoch /= num_updates\n\n        return value_loss_epoch, action_loss_epoch, dist_entropy_epoch\n'"
