file_path,api_count,code
envs.py,0,"b""import cv2\nimport gym\nimport numpy as np\nfrom gym.spaces.box import Box\n\n\n# Taken from https://github.com/openai/universe-starter-agent\ndef create_atari_env(env_id):\n    env = gym.make(env_id)\n    env = AtariRescale42x42(env)\n    env = NormalizedEnv(env)\n    return env\n\n\ndef _process_frame42(frame):\n    frame = frame[34:34 + 160, :160]\n    # Resize by half, then down to 42x42 (essentially mipmapping). If\n    # we resize directly we lose pixels that, when mapped to 42x42,\n    # aren't close enough to the pixel boundary.\n    frame = cv2.resize(frame, (80, 80))\n    frame = cv2.resize(frame, (42, 42))\n    frame = frame.mean(2, keepdims=True)\n    frame = frame.astype(np.float32)\n    frame *= (1.0 / 255.0)\n    frame = np.moveaxis(frame, -1, 0)\n    return frame\n\n\nclass AtariRescale42x42(gym.ObservationWrapper):\n    def __init__(self, env=None):\n        super(AtariRescale42x42, self).__init__(env)\n        self.observation_space = Box(0.0, 1.0, [1, 42, 42])\n\n    def _observation(self, observation):\n        return _process_frame42(observation)\n\n\nclass NormalizedEnv(gym.ObservationWrapper):\n    def __init__(self, env=None):\n        super(NormalizedEnv, self).__init__(env)\n        self.state_mean = 0\n        self.state_std = 0\n        self.alpha = 0.9999\n        self.num_steps = 0\n\n    def _observation(self, observation):\n        self.num_steps += 1\n        self.state_mean = self.state_mean * self.alpha + \\\n            observation.mean() * (1 - self.alpha)\n        self.state_std = self.state_std * self.alpha + \\\n            observation.std() * (1 - self.alpha)\n\n        unbiased_mean = self.state_mean / (1 - pow(self.alpha, self.num_steps))\n        unbiased_std = self.state_std / (1 - pow(self.alpha, self.num_steps))\n\n        return (observation - unbiased_mean) / (unbiased_std + 1e-8)\n"""
main.py,2,"b'from __future__ import print_function\n\nimport argparse\nimport os\n\nimport torch\nimport torch.multiprocessing as mp\n\nimport my_optim\nfrom envs import create_atari_env\nfrom model import ActorCritic\nfrom test import test\nfrom train import train\n\n# Based on\n# https://github.com/pytorch/examples/tree/master/mnist_hogwild\n# Training settings\nparser = argparse.ArgumentParser(description=\'A3C\')\nparser.add_argument(\'--lr\', type=float, default=0.0001,\n                    help=\'learning rate (default: 0.0001)\')\nparser.add_argument(\'--gamma\', type=float, default=0.99,\n                    help=\'discount factor for rewards (default: 0.99)\')\nparser.add_argument(\'--gae-lambda\', type=float, default=1.00,\n                    help=\'lambda parameter for GAE (default: 1.00)\')\nparser.add_argument(\'--entropy-coef\', type=float, default=0.01,\n                    help=\'entropy term coefficient (default: 0.01)\')\nparser.add_argument(\'--value-loss-coef\', type=float, default=0.5,\n                    help=\'value loss coefficient (default: 0.5)\')\nparser.add_argument(\'--max-grad-norm\', type=float, default=50,\n                    help=\'value loss coefficient (default: 50)\')\nparser.add_argument(\'--seed\', type=int, default=1,\n                    help=\'random seed (default: 1)\')\nparser.add_argument(\'--num-processes\', type=int, default=4,\n                    help=\'how many training processes to use (default: 4)\')\nparser.add_argument(\'--num-steps\', type=int, default=20,\n                    help=\'number of forward steps in A3C (default: 20)\')\nparser.add_argument(\'--max-episode-length\', type=int, default=1000000,\n                    help=\'maximum length of an episode (default: 1000000)\')\nparser.add_argument(\'--env-name\', default=\'PongDeterministic-v4\',\n                    help=\'environment to train on (default: PongDeterministic-v4)\')\nparser.add_argument(\'--no-shared\', default=False,\n                    help=\'use an optimizer without shared momentum.\')\n\n\nif __name__ == \'__main__\':\n    os.environ[\'OMP_NUM_THREADS\'] = \'1\'\n    os.environ[\'CUDA_VISIBLE_DEVICES\'] = """"\n\n    args = parser.parse_args()\n\n    torch.manual_seed(args.seed)\n    env = create_atari_env(args.env_name)\n    shared_model = ActorCritic(\n        env.observation_space.shape[0], env.action_space)\n    shared_model.share_memory()\n\n    if args.no_shared:\n        optimizer = None\n    else:\n        optimizer = my_optim.SharedAdam(shared_model.parameters(), lr=args.lr)\n        optimizer.share_memory()\n\n    processes = []\n\n    counter = mp.Value(\'i\', 0)\n    lock = mp.Lock()\n\n    p = mp.Process(target=test, args=(args.num_processes, args, shared_model, counter))\n    p.start()\n    processes.append(p)\n\n    for rank in range(0, args.num_processes):\n        p = mp.Process(target=train, args=(rank, args, shared_model, counter, lock, optimizer))\n        p.start()\n        processes.append(p)\n    for p in processes:\n        p.join()\n'"
model.py,5,"b""import numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\ndef normalized_columns_initializer(weights, std=1.0):\n    out = torch.randn(weights.size())\n    out *= std / torch.sqrt(out.pow(2).sum(1, keepdim=True))\n    return out\n\n\ndef weights_init(m):\n    classname = m.__class__.__name__\n    if classname.find('Conv') != -1:\n        weight_shape = list(m.weight.data.size())\n        fan_in = np.prod(weight_shape[1:4])\n        fan_out = np.prod(weight_shape[2:4]) * weight_shape[0]\n        w_bound = np.sqrt(6. / (fan_in + fan_out))\n        m.weight.data.uniform_(-w_bound, w_bound)\n        m.bias.data.fill_(0)\n    elif classname.find('Linear') != -1:\n        weight_shape = list(m.weight.data.size())\n        fan_in = weight_shape[1]\n        fan_out = weight_shape[0]\n        w_bound = np.sqrt(6. / (fan_in + fan_out))\n        m.weight.data.uniform_(-w_bound, w_bound)\n        m.bias.data.fill_(0)\n\n\nclass ActorCritic(torch.nn.Module):\n    def __init__(self, num_inputs, action_space):\n        super(ActorCritic, self).__init__()\n        self.conv1 = nn.Conv2d(num_inputs, 32, 3, stride=2, padding=1)\n        self.conv2 = nn.Conv2d(32, 32, 3, stride=2, padding=1)\n        self.conv3 = nn.Conv2d(32, 32, 3, stride=2, padding=1)\n        self.conv4 = nn.Conv2d(32, 32, 3, stride=2, padding=1)\n\n        self.lstm = nn.LSTMCell(32 * 3 * 3, 256)\n\n        num_outputs = action_space.n\n        self.critic_linear = nn.Linear(256, 1)\n        self.actor_linear = nn.Linear(256, num_outputs)\n\n        self.apply(weights_init)\n        self.actor_linear.weight.data = normalized_columns_initializer(\n            self.actor_linear.weight.data, 0.01)\n        self.actor_linear.bias.data.fill_(0)\n        self.critic_linear.weight.data = normalized_columns_initializer(\n            self.critic_linear.weight.data, 1.0)\n        self.critic_linear.bias.data.fill_(0)\n\n        self.lstm.bias_ih.data.fill_(0)\n        self.lstm.bias_hh.data.fill_(0)\n\n        self.train()\n\n    def forward(self, inputs):\n        inputs, (hx, cx) = inputs\n        x = F.elu(self.conv1(inputs))\n        x = F.elu(self.conv2(x))\n        x = F.elu(self.conv3(x))\n        x = F.elu(self.conv4(x))\n\n        x = x.view(-1, 32 * 3 * 3)\n        hx, cx = self.lstm(x, (hx, cx))\n        x = hx\n\n        return self.critic_linear(x), self.actor_linear(x), (hx, cx)\n"""
my_optim.py,2,"b'import math\n\nimport torch\nimport torch.optim as optim\n\n\nclass SharedAdam(optim.Adam):\n    """"""Implements Adam algorithm with shared states.\n    """"""\n\n    def __init__(self,\n                 params,\n                 lr=1e-3,\n                 betas=(0.9, 0.999),\n                 eps=1e-8,\n                 weight_decay=0):\n        super(SharedAdam, self).__init__(params, lr, betas, eps, weight_decay)\n\n        for group in self.param_groups:\n            for p in group[\'params\']:\n                state = self.state[p]\n                state[\'step\'] = torch.zeros(1)\n                state[\'exp_avg\'] = p.data.new().resize_as_(p.data).zero_()\n                state[\'exp_avg_sq\'] = p.data.new().resize_as_(p.data).zero_()\n\n    def share_memory(self):\n        for group in self.param_groups:\n            for p in group[\'params\']:\n                state = self.state[p]\n                state[\'step\'].share_memory_()\n                state[\'exp_avg\'].share_memory_()\n                state[\'exp_avg_sq\'].share_memory_()\n\n    def step(self, closure=None):\n        """"""Performs a single optimization step.\n        Arguments:\n            closure (callable, optional): A closure that reevaluates the model\n                and returns the loss.\n        """"""\n        loss = None\n        if closure is not None:\n            loss = closure()\n\n        for group in self.param_groups:\n            for p in group[\'params\']:\n                if p.grad is None:\n                    continue\n                grad = p.grad.data\n                state = self.state[p]\n\n                exp_avg, exp_avg_sq = state[\'exp_avg\'], state[\'exp_avg_sq\']\n                beta1, beta2 = group[\'betas\']\n\n                state[\'step\'] += 1\n\n                if group[\'weight_decay\'] != 0:\n                    grad = grad.add(group[\'weight_decay\'], p.data)\n\n                # Decay the first and second moment running average coefficient\n                exp_avg.mul_(beta1).add_(1 - beta1, grad)\n                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n\n                denom = exp_avg_sq.sqrt().add_(group[\'eps\'])\n\n                bias_correction1 = 1 - beta1 ** state[\'step\'].item()\n                bias_correction2 = 1 - beta2 ** state[\'step\'].item()\n                step_size = group[\'lr\'] * math.sqrt(\n                    bias_correction2) / bias_correction1\n\n                p.data.addcdiv_(-step_size, exp_avg, denom)\n\n        return loss\n'"
test.py,7,"b'import time\nfrom collections import deque\n\nimport torch\nimport torch.nn.functional as F\n\nfrom envs import create_atari_env\nfrom model import ActorCritic\n\n\ndef test(rank, args, shared_model, counter):\n    torch.manual_seed(args.seed + rank)\n\n    env = create_atari_env(args.env_name)\n    env.seed(args.seed + rank)\n\n    model = ActorCritic(env.observation_space.shape[0], env.action_space)\n\n    model.eval()\n\n    state = env.reset()\n    state = torch.from_numpy(state)\n    reward_sum = 0\n    done = True\n\n    start_time = time.time()\n\n    # a quick hack to prevent the agent from stucking\n    actions = deque(maxlen=100)\n    episode_length = 0\n    while True:\n        episode_length += 1\n        # Sync with the shared model\n        if done:\n            model.load_state_dict(shared_model.state_dict())\n            cx = torch.zeros(1, 256)\n            hx = torch.zeros(1, 256)\n        else:\n            cx = cx.detach()\n            hx = hx.detach()\n\n        with torch.no_grad():\n            value, logit, (hx, cx) = model((state.unsqueeze(0), (hx, cx)))\n        prob = F.softmax(logit, dim=-1)\n        action = prob.max(1, keepdim=True)[1].numpy()\n\n        state, reward, done, _ = env.step(action[0, 0])\n        done = done or episode_length >= args.max_episode_length\n        reward_sum += reward\n\n        # a quick hack to prevent the agent from stucking\n        actions.append(action[0, 0])\n        if actions.count(actions[0]) == actions.maxlen:\n            done = True\n\n        if done:\n            print(""Time {}, num steps {}, FPS {:.0f}, episode reward {}, episode length {}"".format(\n                time.strftime(""%Hh %Mm %Ss"",\n                              time.gmtime(time.time() - start_time)),\n                counter.value, counter.value / (time.time() - start_time),\n                reward_sum, episode_length))\n            reward_sum = 0\n            episode_length = 0\n            actions.clear()\n            state = env.reset()\n            time.sleep(60)\n\n        state = torch.from_numpy(state)\n'"
train.py,10,"b'import torch\nimport torch.nn.functional as F\nimport torch.optim as optim\n\nfrom envs import create_atari_env\nfrom model import ActorCritic\n\n\ndef ensure_shared_grads(model, shared_model):\n    for param, shared_param in zip(model.parameters(),\n                                   shared_model.parameters()):\n        if shared_param.grad is not None:\n            return\n        shared_param._grad = param.grad\n\n\ndef train(rank, args, shared_model, counter, lock, optimizer=None):\n    torch.manual_seed(args.seed + rank)\n\n    env = create_atari_env(args.env_name)\n    env.seed(args.seed + rank)\n\n    model = ActorCritic(env.observation_space.shape[0], env.action_space)\n\n    if optimizer is None:\n        optimizer = optim.Adam(shared_model.parameters(), lr=args.lr)\n\n    model.train()\n\n    state = env.reset()\n    state = torch.from_numpy(state)\n    done = True\n\n    episode_length = 0\n    while True:\n        # Sync with the shared model\n        model.load_state_dict(shared_model.state_dict())\n        if done:\n            cx = torch.zeros(1, 256)\n            hx = torch.zeros(1, 256)\n        else:\n            cx = cx.detach()\n            hx = hx.detach()\n\n        values = []\n        log_probs = []\n        rewards = []\n        entropies = []\n\n        for step in range(args.num_steps):\n            episode_length += 1\n            value, logit, (hx, cx) = model((state.unsqueeze(0),\n                                            (hx, cx)))\n            prob = F.softmax(logit, dim=-1)\n            log_prob = F.log_softmax(logit, dim=-1)\n            entropy = -(log_prob * prob).sum(1, keepdim=True)\n            entropies.append(entropy)\n\n            action = prob.multinomial(num_samples=1).detach()\n            log_prob = log_prob.gather(1, action)\n\n            state, reward, done, _ = env.step(action.numpy())\n            done = done or episode_length >= args.max_episode_length\n            reward = max(min(reward, 1), -1)\n\n            with lock:\n                counter.value += 1\n\n            if done:\n                episode_length = 0\n                state = env.reset()\n\n            state = torch.from_numpy(state)\n            values.append(value)\n            log_probs.append(log_prob)\n            rewards.append(reward)\n\n            if done:\n                break\n\n        R = torch.zeros(1, 1)\n        if not done:\n            value, _, _ = model((state.unsqueeze(0), (hx, cx)))\n            R = value.detach()\n\n        values.append(R)\n        policy_loss = 0\n        value_loss = 0\n        gae = torch.zeros(1, 1)\n        for i in reversed(range(len(rewards))):\n            R = args.gamma * R + rewards[i]\n            advantage = R - values[i]\n            value_loss = value_loss + 0.5 * advantage.pow(2)\n\n            # Generalized Advantage Estimation\n            delta_t = rewards[i] + args.gamma * \\\n                values[i + 1] - values[i]\n            gae = gae * args.gamma * args.gae_lambda + delta_t\n\n            policy_loss = policy_loss - \\\n                log_probs[i] * gae.detach() - args.entropy_coef * entropies[i]\n\n        optimizer.zero_grad()\n\n        (policy_loss + args.value_loss_coef * value_loss).backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n\n        ensure_shared_grads(model, shared_model)\n        optimizer.step()\n'"
