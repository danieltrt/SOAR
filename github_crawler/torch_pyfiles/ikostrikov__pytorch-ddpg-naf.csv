file_path,api_count,code
ddpg.py,18,"b'import sys\n\nimport torch\nimport torch.nn as nn\nfrom torch.optim import Adam\nfrom torch.autograd import Variable\nimport torch.nn.functional as F\n\ndef soft_update(target, source, tau):\n    for target_param, param in zip(target.parameters(), source.parameters()):\n        target_param.data.copy_(target_param.data * (1.0 - tau) + param.data * tau)\n\ndef hard_update(target, source):\n    for target_param, param in zip(target.parameters(), source.parameters()):\n        target_param.data.copy_(param.data)\n\n""""""\nFrom: https://github.com/pytorch/pytorch/issues/1959\nThere\'s an official LayerNorm implementation in pytorch now, but it hasn\'t been included in \npip version yet. This is a temporary version\nThis slows down training by a bit\n""""""\nclass LayerNorm(nn.Module):\n    def __init__(self, num_features, eps=1e-5, affine=True):\n        super(LayerNorm, self).__init__()\n        self.num_features = num_features\n        self.affine = affine\n        self.eps = eps\n\n        if self.affine:\n            self.gamma = nn.Parameter(torch.Tensor(num_features).uniform_())\n            self.beta = nn.Parameter(torch.zeros(num_features))\n\n    def forward(self, x):\n        shape = [-1] + [1] * (x.dim() - 1)\n        mean = x.view(x.size(0), -1).mean(1).view(*shape)\n        std = x.view(x.size(0), -1).std(1).view(*shape)\n\n        y = (x - mean) / (std + self.eps)\n        if self.affine:\n            shape = [1, -1] + [1] * (x.dim() - 2)\n            y = self.gamma.view(*shape) * y + self.beta.view(*shape)\n        return y\n\nnn.LayerNorm = LayerNorm\n\n\nclass Actor(nn.Module):\n    def __init__(self, hidden_size, num_inputs, action_space):\n        super(Actor, self).__init__()\n        self.action_space = action_space\n        num_outputs = action_space.shape[0]\n\n        self.linear1 = nn.Linear(num_inputs, hidden_size)\n        self.ln1 = nn.LayerNorm(hidden_size)\n\n        self.linear2 = nn.Linear(hidden_size, hidden_size)\n        self.ln2 = nn.LayerNorm(hidden_size)\n\n        self.mu = nn.Linear(hidden_size, num_outputs)\n        self.mu.weight.data.mul_(0.1)\n        self.mu.bias.data.mul_(0.1)\n\n    def forward(self, inputs):\n        x = inputs\n        x = self.linear1(x)\n        x = self.ln1(x)\n        x = F.relu(x)\n        x = self.linear2(x)\n        x = self.ln2(x)\n        x = F.relu(x)\n        mu = F.tanh(self.mu(x))\n        return mu\n\nclass Critic(nn.Module):\n    def __init__(self, hidden_size, num_inputs, action_space):\n        super(Critic, self).__init__()\n        self.action_space = action_space\n        num_outputs = action_space.shape[0]\n\n        self.linear1 = nn.Linear(num_inputs, hidden_size)\n        self.ln1 = nn.LayerNorm(hidden_size)\n\n        self.linear2 = nn.Linear(hidden_size+num_outputs, hidden_size)\n        self.ln2 = nn.LayerNorm(hidden_size)\n\n        self.V = nn.Linear(hidden_size, 1)\n        self.V.weight.data.mul_(0.1)\n        self.V.bias.data.mul_(0.1)\n\n    def forward(self, inputs, actions):\n        x = inputs\n        x = self.linear1(x)\n        x = self.ln1(x)\n        x = F.relu(x)\n\n        x = torch.cat((x, actions), 1)\n        x = self.linear2(x)\n        x = self.ln2(x)\n        x = F.relu(x)\n        V = self.V(x)\n        return V\n\nclass DDPG(object):\n    def __init__(self, gamma, tau, hidden_size, num_inputs, action_space):\n\n        self.num_inputs = num_inputs\n        self.action_space = action_space\n\n        self.actor = Actor(hidden_size, self.num_inputs, self.action_space)\n        self.actor_target = Actor(hidden_size, self.num_inputs, self.action_space)\n        self.actor_perturbed = Actor(hidden_size, self.num_inputs, self.action_space)\n        self.actor_optim = Adam(self.actor.parameters(), lr=1e-4)\n\n        self.critic = Critic(hidden_size, self.num_inputs, self.action_space)\n        self.critic_target = Critic(hidden_size, self.num_inputs, self.action_space)\n        self.critic_optim = Adam(self.critic.parameters(), lr=1e-3)\n\n        self.gamma = gamma\n        self.tau = tau\n\n        hard_update(self.actor_target, self.actor)  # Make sure target is with the same weight\n        hard_update(self.critic_target, self.critic)\n\n\n    def select_action(self, state, action_noise=None, param_noise=None):\n        self.actor.eval()\n        if param_noise is not None: \n            mu = self.actor_perturbed((Variable(state)))\n        else:\n            mu = self.actor((Variable(state)))\n\n        self.actor.train()\n        mu = mu.data\n\n        if action_noise is not None:\n            mu += torch.Tensor(action_noise.noise())\n\n        return mu.clamp(-1, 1)\n\n\n    def update_parameters(self, batch):\n        state_batch = Variable(torch.cat(batch.state))\n        action_batch = Variable(torch.cat(batch.action))\n        reward_batch = Variable(torch.cat(batch.reward))\n        mask_batch = Variable(torch.cat(batch.mask))\n        next_state_batch = Variable(torch.cat(batch.next_state))\n        \n        next_action_batch = self.actor_target(next_state_batch)\n        next_state_action_values = self.critic_target(next_state_batch, next_action_batch)\n\n        reward_batch = reward_batch.unsqueeze(1)\n        mask_batch = mask_batch.unsqueeze(1)\n        expected_state_action_batch = reward_batch + (self.gamma * mask_batch * next_state_action_values)\n\n        self.critic_optim.zero_grad()\n\n        state_action_batch = self.critic((state_batch), (action_batch))\n\n        value_loss = F.mse_loss(state_action_batch, expected_state_action_batch)\n        value_loss.backward()\n        self.critic_optim.step()\n\n        self.actor_optim.zero_grad()\n\n        policy_loss = -self.critic((state_batch),self.actor((state_batch)))\n\n        policy_loss = policy_loss.mean()\n        policy_loss.backward()\n        self.actor_optim.step()\n\n        soft_update(self.actor_target, self.actor, self.tau)\n        soft_update(self.critic_target, self.critic, self.tau)\n\n        return value_loss.item(), policy_loss.item()\n\n    def perturb_actor_parameters(self, param_noise):\n        """"""Apply parameter noise to actor model, for exploration""""""\n        hard_update(self.actor_perturbed, self.actor)\n        params = self.actor_perturbed.state_dict()\n        for name in params:\n            if \'ln\' in name: \n                pass \n            param = params[name]\n            param += torch.randn(param.shape) * param_noise.current_stddev\n\n    def save_model(self, env_name, suffix="""", actor_path=None, critic_path=None):\n        if not os.path.exists(\'models/\'):\n            os.makedirs(\'models/\')\n\n        if actor_path is None:\n            actor_path = ""models/ddpg_actor_{}_{}"".format(env_name, suffix) \n        if critic_path is None:\n            critic_path = ""models/ddpg_critic_{}_{}"".format(env_name, suffix) \n        print(\'Saving models to {} and {}\'.format(actor_path, critic_path))\n        torch.save(self.actor.state_dict(), actor_path)\n        torch.save(self.critic.state_dict(), critic_path)\n\n    def load_model(self, actor_path, critic_path):\n        print(\'Loading models from {} and {}\'.format(actor_path, critic_path))\n        if actor_path is not None:\n            self.actor.load_state_dict(torch.load(actor_path))\n        if critic_path is not None: \n            self.critic.load_state_dict(torch.load(critic_path))'"
main.py,10,"b'import argparse\nimport math\nfrom collections import namedtuple\nfrom itertools import count\nfrom tqdm import tqdm\nfrom tensorboardX import SummaryWriter\n\nimport gym\nimport numpy as np\nfrom gym import wrappers\n\nimport torch\nfrom ddpg import DDPG\nfrom naf import NAF\nfrom normalized_actions import NormalizedActions\nfrom ounoise import OUNoise\nfrom param_noise import AdaptiveParamNoiseSpec, ddpg_distance_metric\nfrom replay_memory import ReplayMemory, Transition\n\nparser = argparse.ArgumentParser(description=\'PyTorch REINFORCE example\')\nparser.add_argument(\'--algo\', default=\'NAF\',\n                    help=\'algorithm to use: DDPG | NAF\')\nparser.add_argument(\'--env-name\', default=""HalfCheetah-v2"",\n                    help=\'name of the environment to run\')\nparser.add_argument(\'--gamma\', type=float, default=0.99, metavar=\'G\',\n                    help=\'discount factor for reward (default: 0.99)\')\nparser.add_argument(\'--tau\', type=float, default=0.001, metavar=\'G\',\n                    help=\'discount factor for model (default: 0.001)\')\nparser.add_argument(\'--ou_noise\', type=bool, default=True)\nparser.add_argument(\'--param_noise\', type=bool, default=False)\nparser.add_argument(\'--noise_scale\', type=float, default=0.3, metavar=\'G\',\n                    help=\'initial noise scale (default: 0.3)\')\nparser.add_argument(\'--final_noise_scale\', type=float, default=0.3, metavar=\'G\',\n                    help=\'final noise scale (default: 0.3)\')\nparser.add_argument(\'--exploration_end\', type=int, default=100, metavar=\'N\',\n                    help=\'number of episodes with noise (default: 100)\')\nparser.add_argument(\'--seed\', type=int, default=4, metavar=\'N\',\n                    help=\'random seed (default: 4)\')\nparser.add_argument(\'--batch_size\', type=int, default=128, metavar=\'N\',\n                    help=\'batch size (default: 128)\')\nparser.add_argument(\'--num_steps\', type=int, default=1000, metavar=\'N\',\n                    help=\'max episode length (default: 1000)\')\nparser.add_argument(\'--num_episodes\', type=int, default=1000, metavar=\'N\',\n                    help=\'number of episodes (default: 1000)\')\nparser.add_argument(\'--hidden_size\', type=int, default=128, metavar=\'N\',\n                    help=\'number of episodes (default: 128)\')\nparser.add_argument(\'--updates_per_step\', type=int, default=5, metavar=\'N\',\n                    help=\'model updates per simulator step (default: 5)\')\nparser.add_argument(\'--replay_size\', type=int, default=1000000, metavar=\'N\',\n                    help=\'size of replay buffer (default: 1000000)\')\nargs = parser.parse_args()\n\nenv = NormalizedActions(gym.make(args.env_name))\n\nwriter = SummaryWriter()\n\nenv.seed(args.seed)\ntorch.manual_seed(args.seed)\nnp.random.seed(args.seed)\nif args.algo == ""NAF"":\n    agent = NAF(args.gamma, args.tau, args.hidden_size,\n                      env.observation_space.shape[0], env.action_space)\nelse:\n    agent = DDPG(args.gamma, args.tau, args.hidden_size,\n                      env.observation_space.shape[0], env.action_space)\n\nmemory = ReplayMemory(args.replay_size)\n\nounoise = OUNoise(env.action_space.shape[0]) if args.ou_noise else None\nparam_noise = AdaptiveParamNoiseSpec(initial_stddev=0.05, \n    desired_action_stddev=args.noise_scale, adaptation_coefficient=1.05) if args.param_noise else None\n\nrewards = []\ntotal_numsteps = 0\nupdates = 0\n\nfor i_episode in range(args.num_episodes):\n    state = torch.Tensor([env.reset()])\n\n    if args.ou_noise: \n        ounoise.scale = (args.noise_scale - args.final_noise_scale) * max(0, args.exploration_end -\n                                                                      i_episode) / args.exploration_end + args.final_noise_scale\n        ounoise.reset()\n\n    if args.param_noise and args.algo == ""DDPG"":\n        agent.perturb_actor_parameters(param_noise)\n\n    episode_reward = 0\n    while True:\n        action = agent.select_action(state, ounoise, param_noise)\n        next_state, reward, done, _ = env.step(action.numpy()[0])\n        total_numsteps += 1\n        episode_reward += reward\n\n        action = torch.Tensor(action)\n        mask = torch.Tensor([not done])\n        next_state = torch.Tensor([next_state])\n        reward = torch.Tensor([reward])\n\n        memory.push(state, action, mask, next_state, reward)\n\n        state = next_state\n\n        if len(memory) > args.batch_size:\n            for _ in range(args.updates_per_step):\n                transitions = memory.sample(args.batch_size)\n                batch = Transition(*zip(*transitions))\n\n                value_loss, policy_loss = agent.update_parameters(batch)\n\n                writer.add_scalar(\'loss/value\', value_loss, updates)\n                writer.add_scalar(\'loss/policy\', policy_loss, updates)\n\n                updates += 1\n        if done:\n            break\n\n    writer.add_scalar(\'reward/train\', episode_reward, i_episode)\n\n    # Update param_noise based on distance metric\n    if args.param_noise:\n        episode_transitions = memory.memory[memory.position-t:memory.position]\n        states = torch.cat([transition[0] for transition in episode_transitions], 0)\n        unperturbed_actions = agent.select_action(states, None, None)\n        perturbed_actions = torch.cat([transition[1] for transition in episode_transitions], 0)\n\n        ddpg_dist = ddpg_distance_metric(perturbed_actions.numpy(), unperturbed_actions.numpy())\n        param_noise.adapt(ddpg_dist)\n\n    rewards.append(episode_reward)\n    if i_episode % 10 == 0:\n        state = torch.Tensor([env.reset()])\n        episode_reward = 0\n        while True:\n            action = agent.select_action(state)\n\n            next_state, reward, done, _ = env.step(action.numpy()[0])\n            episode_reward += reward\n\n            next_state = torch.Tensor([next_state])\n\n            state = next_state\n            if done:\n                break\n\n        writer.add_scalar(\'reward/test\', episode_reward, i_episode)\n\n        rewards.append(episode_reward)\n        print(""Episode: {}, total numsteps: {}, reward: {}, average reward: {}"".format(i_episode, total_numsteps, rewards[-1], np.mean(rewards[-10:])))\n    \nenv.close()\n'"
naf.py,20,"b'import sys\n\nimport torch\nimport torch.nn as nn\nfrom torch.optim import Adam\nfrom torch.autograd import Variable\nimport torch.nn.functional as F\n\ndef MSELoss(input, target):\n    return torch.sum((input - target)**2) / input.data.nelement()\n\ndef soft_update(target, source, tau):\n    for target_param, param in zip(target.parameters(), source.parameters()):\n        target_param.data.copy_(target_param.data * (1.0 - tau) + param.data * tau)\n\ndef hard_update(target, source):\n    for target_param, param in zip(target.parameters(), source.parameters()):\n        target_param.data.copy_(param.data)\n\nclass Policy(nn.Module):\n\n    def __init__(self, hidden_size, num_inputs, action_space):\n        super(Policy, self).__init__()\n        self.action_space = action_space\n        num_outputs = action_space.shape[0]\n\n        self.bn0 = nn.BatchNorm1d(num_inputs)\n        self.bn0.weight.data.fill_(1)\n        self.bn0.bias.data.fill_(0)\n\n        self.linear1 = nn.Linear(num_inputs, hidden_size)\n        self.bn1 = nn.BatchNorm1d(hidden_size)\n        self.bn1.weight.data.fill_(1)\n        self.bn1.bias.data.fill_(0)\n\n        self.linear2 = nn.Linear(hidden_size, hidden_size)\n        self.bn2 = nn.BatchNorm1d(hidden_size)\n        self.bn2.weight.data.fill_(1)\n        self.bn2.bias.data.fill_(0)\n\n        self.V = nn.Linear(hidden_size, 1)\n        self.V.weight.data.mul_(0.1)\n        self.V.bias.data.mul_(0.1)\n\n        self.mu = nn.Linear(hidden_size, num_outputs)\n        self.mu.weight.data.mul_(0.1)\n        self.mu.bias.data.mul_(0.1)\n\n        self.L = nn.Linear(hidden_size, num_outputs ** 2)\n        self.L.weight.data.mul_(0.1)\n        self.L.bias.data.mul_(0.1)\n\n        self.tril_mask = Variable(torch.tril(torch.ones(\n            num_outputs, num_outputs), diagonal=-1).unsqueeze(0))\n        self.diag_mask = Variable(torch.diag(torch.diag(\n            torch.ones(num_outputs, num_outputs))).unsqueeze(0))\n\n    def forward(self, inputs):\n        x, u = inputs\n        x = self.bn0(x)\n        x = F.tanh(self.linear1(x))\n        x = F.tanh(self.linear2(x))\n\n        V = self.V(x)\n        mu = F.tanh(self.mu(x))\n\n        Q = None\n        if u is not None:\n            num_outputs = mu.size(1)\n            L = self.L(x).view(-1, num_outputs, num_outputs)\n            L = L * \\\n                self.tril_mask.expand_as(\n                    L) + torch.exp(L) * self.diag_mask.expand_as(L)\n            P = torch.bmm(L, L.transpose(2, 1))\n\n            u_mu = (u - mu).unsqueeze(2)\n            A = -0.5 * \\\n                torch.bmm(torch.bmm(u_mu.transpose(2, 1), P), u_mu)[:, :, 0]\n\n            Q = A + V\n\n        return mu, Q, V\n\n\nclass NAF:\n\n    def __init__(self, gamma, tau, hidden_size, num_inputs, action_space):\n        self.action_space = action_space\n        self.num_inputs = num_inputs\n        \n        self.model = Policy(hidden_size, num_inputs, action_space)\n        self.target_model = Policy(hidden_size, num_inputs, action_space)\n        self.optimizer = Adam(self.model.parameters(), lr=1e-3)\n\n        self.gamma = gamma\n        self.tau = tau\n\n        hard_update(self.target_model, self.model)\n\n    def select_action(self, state, action_noise=None, param_noise=None):\n        self.model.eval()\n        mu, _, _ = self.model((Variable(state), None))\n        self.model.train()\n        mu = mu.data\n        if action_noise is not None:\n            mu += torch.Tensor(action_noise.noise())\n\n        return mu.clamp(-1, 1)\n\n    def update_parameters(self, batch):\n        state_batch = Variable(torch.cat(batch.state))\n        action_batch = Variable(torch.cat(batch.action))\n        reward_batch = Variable(torch.cat(batch.reward))\n        mask_batch = Variable(torch.cat(batch.mask))\n        next_state_batch = Variable(torch.cat(batch.next_state))\n\n        _, _, next_state_values = self.target_model((next_state_batch, None))\n\n        reward_batch = reward_batch.unsqueeze(1)\n        mask_batch = mask_batch.unsqueeze(1)\n        expected_state_action_values = reward_batch + (self.gamma * mask_batch + next_state_values)\n\n        _, state_action_values, _ = self.model((state_batch, action_batch))\n\n        loss = MSELoss(state_action_values, expected_state_action_values)\n\n        self.optimizer.zero_grad()\n        loss.backward()\n        torch.nn.utils.clip_grad_norm(self.model.parameters(), 1)\n        self.optimizer.step()\n\n        soft_update(self.target_model, self.model, self.tau)\n\n        return loss.item(), 0\n\n    def save_model(self, env_name, suffix="""", model_path=None):\n        if not os.path.exists(\'models/\'):\n            os.makedirs(\'models/\')\n\n        if model_path is None:\n            model_path = ""models/naf_{}_{}"".format(env_name, suffix) \n        print(\'Saving model to {}\'.format(model_path))\n        torch.save(self.model.state_dict(), model_path)\n\n    def load_model(self, model_path):\n        print(\'Loading model from {}\'.format(model_path))\n        self.model.load_state_dict(torch.load(model_path))\n'"
normalized_actions.py,0,"b'import gym\n\n\nclass NormalizedActions(gym.ActionWrapper):\n\n    def _action(self, action):\n        action = (action + 1) / 2  # [-1, 1] => [0, 1]\n        action *= (self.action_space.high - self.action_space.low)\n        action += self.action_space.low\n        return action\n\n    def _reverse_action(self, action):\n        action -= self.action_space.low\n        action /= (self.action_space.high - self.action_space.low)\n        action = action * 2 - 1\n        return action\n'"
ounoise.py,0,"b'import numpy as np\n\n\n# from https://github.com/songrotek/DDPG/blob/master/ou_noise.py\nclass OUNoise:\n\n    def __init__(self, action_dimension, scale=0.1, mu=0, theta=0.15, sigma=0.2):\n        self.action_dimension = action_dimension\n        self.scale = scale\n        self.mu = mu\n        self.theta = theta\n        self.sigma = sigma\n        self.state = np.ones(self.action_dimension) * self.mu\n        self.reset()\n\n    def reset(self):\n        self.state = np.ones(self.action_dimension) * self.mu\n\n    def noise(self):\n        x = self.state\n        dx = self.theta * (self.mu - x) + self.sigma * np.random.randn(len(x))\n        self.state = x + dx\n        return self.state * self.scale\n'"
param_noise.py,0,"b'import numpy as np\nimport torch\nfrom math import sqrt\n\n""""""\nFrom OpenAI Baselines:\nhttps://github.com/openai/baselines/blob/master/baselines/ddpg/noise.py\n""""""\nclass AdaptiveParamNoiseSpec(object):\n    def __init__(self, initial_stddev=0.1, desired_action_stddev=0.2, adaptation_coefficient=1.01):\n        """"""\n        Note that initial_stddev and current_stddev refer to std of parameter noise, \n        but desired_action_stddev refers to (as name notes) desired std in action space\n        """"""\n        self.initial_stddev = initial_stddev\n        self.desired_action_stddev = desired_action_stddev\n        self.adaptation_coefficient = adaptation_coefficient\n\n        self.current_stddev = initial_stddev\n\n    def adapt(self, distance):\n        if distance > self.desired_action_stddev:\n            # Decrease stddev.\n            self.current_stddev /= self.adaptation_coefficient\n        else:\n            # Increase stddev.\n            self.current_stddev *= self.adaptation_coefficient\n\n    def get_stats(self):\n        stats = {\n            \'param_noise_stddev\': self.current_stddev,\n        }\n        return stats\n\n    def __repr__(self):\n        fmt = \'AdaptiveParamNoiseSpec(initial_stddev={}, desired_action_stddev={}, adaptation_coefficient={})\'\n        return fmt.format(self.initial_stddev, self.desired_action_stddev, self.adaptation_coefficient)\n\ndef ddpg_distance_metric(actions1, actions2):\n    """"""\n    Compute ""distance"" between actions taken by two policies at the same states\n    Expects numpy arrays\n    """"""\n    diff = actions1-actions2\n    mean_diff = np.mean(np.square(diff), axis=0)\n    dist = sqrt(np.mean(mean_diff))\n    return dist\n\n\n\n\n'"
replay_memory.py,0,"b'import random\nfrom collections import namedtuple\n\n# Taken from\n# https://github.com/pytorch/tutorials/blob/master/Reinforcement%20(Q-)Learning%20with%20PyTorch.ipynb\n\nTransition = namedtuple(\n    \'Transition\', (\'state\', \'action\', \'mask\', \'next_state\', \'reward\'))\n\n\nclass ReplayMemory(object):\n\n    def __init__(self, capacity):\n        self.capacity = capacity\n        self.memory = []\n        self.position = 0\n\n    def push(self, *args):\n        """"""Saves a transition.""""""\n        if len(self.memory) < self.capacity:\n            self.memory.append(None)\n        self.memory[self.position] = Transition(*args)\n        self.position = (self.position + 1) % self.capacity\n\n    def sample(self, batch_size):\n        return random.sample(self.memory, batch_size)\n\n    def __len__(self):\n        return len(self.memory)\n'"
