file_path,api_count,code
flow_test.py,14,"b'import unittest\n\nimport torch\n\nimport flows as fnn\n\nEPS = 1e-5\nBATCH_SIZE = 32\nNUM_INPUTS = 11\nNUM_HIDDEN = 64\nmask = torch.arange(0, NUM_INPUTS) % 2\nmask = mask.unsqueeze(0)\n\n\nclass TestFlow(unittest.TestCase):\n    def testCoupling(self):\n        m1 = fnn.FlowSequential(\n            fnn.CouplingLayer(NUM_INPUTS, NUM_HIDDEN, mask))\n\n        x = torch.randn(BATCH_SIZE, NUM_INPUTS)\n\n        y, logdets = m1(x)\n        z, inv_logdets = m1(y, mode=\'inverse\')\n\n        self.assertTrue((logdets + inv_logdets).abs().max() < EPS,\n                        \'CouplingLayer Det is not zero.\')\n        self.assertTrue((x - z).abs().max() < EPS, \'CouplingLayer is wrong\')\n\n    def testInv(self):\n        m1 = fnn.FlowSequential(fnn.InvertibleMM(NUM_INPUTS))\n\n        x = torch.randn(BATCH_SIZE, NUM_INPUTS)\n\n        y, logdets = m1(x)\n        z, inv_logdets = m1(y, mode=\'inverse\')\n\n        self.assertTrue((logdets + inv_logdets).abs().max() < EPS,\n                        \'InvMM Det is not zero.\')\n        self.assertTrue((x - z).abs().max() < EPS, \'InvMM is wrong.\')\n\n    def testSigmoid(self):\n        m1 = fnn.FlowSequential(fnn.Sigmoid())\n\n        x = torch.randn(BATCH_SIZE, NUM_INPUTS)\n\n        y, logdets = m1(x)\n        z, inv_logdets = m1(y, mode=\'inverse\')\n\n        self.assertTrue((logdets + inv_logdets).abs().max() < EPS,\n                        \'Sigmoid Det is not zero.\')\n        self.assertTrue((x - z).abs().max() < EPS, \'Sigmoid is wrong.\')\n\n    def testActNorm(self):\n        m1 = fnn.FlowSequential(fnn.ActNorm(NUM_INPUTS))\n\n        x = torch.randn(BATCH_SIZE, NUM_INPUTS)\n\n        y, logdets = m1(x)\n        z, inv_logdets = m1(y, mode=\'inverse\')\n\n        self.assertTrue((logdets + inv_logdets).abs().max() < EPS,\n                        \'ActNorm Det is not zero.\')\n        self.assertTrue((x - z).abs().max() < EPS, \'ActNorm is wrong.\')\n\n        # Second run.\n        x = torch.randn(BATCH_SIZE, NUM_INPUTS)\n\n        y, logdets = m1(x)\n        z, inv_logdets = m1(y, mode=\'inverse\')\n\n        self.assertTrue((logdets + inv_logdets).abs().max() < EPS,\n                        \'ActNorm Det is not zero for the second run.\')\n        self.assertTrue((x - z).abs().max() < EPS,\n                        \'ActNorm is wrong for the second run.\')\n\n    def testBatchNorm(self):\n        m1 = fnn.FlowSequential(fnn.BatchNormFlow(NUM_INPUTS))\n        m1.train()\n\n        x = torch.randn(BATCH_SIZE, NUM_INPUTS)\n\n        y, logdets = m1(x)\n        z, inv_logdets = m1(y, mode=\'inverse\')\n\n        self.assertTrue((logdets + inv_logdets).abs().max() < EPS,\n                        \'BatchNorm Det is not zero.\')\n        self.assertTrue((x - z).abs().max() < EPS, \'BatchNorm is wrong.\')\n\n        # Second run.\n        x = torch.randn(BATCH_SIZE, NUM_INPUTS)\n\n        y, logdets = m1(x)\n        z, inv_logdets = m1(y, mode=\'inverse\')\n\n        self.assertTrue((logdets + inv_logdets).abs().max() < EPS,\n                        \'BatchNorm Det is not zero for the second run.\')\n        self.assertTrue((x - z).abs().max() < EPS,\n                        \'BatchNorm is wrong for the second run.\')\n\n        m1.eval()\n        m1 = fnn.FlowSequential(fnn.BatchNormFlow(NUM_INPUTS))\n\n        x = torch.randn(BATCH_SIZE, NUM_INPUTS)\n\n        y, logdets = m1(x)\n        z, inv_logdets = m1(y, mode=\'inverse\')\n\n        self.assertTrue((logdets + inv_logdets).abs().max() < EPS,\n                        \'BatchNorm Det is not zero in eval.\')\n        self.assertTrue((x - z).abs().max() < EPS,\n                        \'BatchNorm is wrong in eval.\')\n\n    def testSequential(self):\n        m1 = fnn.FlowSequential(\n            fnn.ActNorm(NUM_INPUTS), fnn.InvertibleMM(NUM_INPUTS),\n            fnn.CouplingLayer(NUM_INPUTS, NUM_HIDDEN, mask))\n\n        x = torch.randn(BATCH_SIZE, NUM_INPUTS)\n\n        y, logdets = m1(x)\n        z, inv_logdets = m1(y, mode=\'inverse\')\n\n        self.assertTrue((logdets + inv_logdets).abs().max() < EPS,\n                        \'Sequential Det is not zero.\')\n        self.assertTrue((x - z).abs().max() < EPS, \'Sequential is wrong.\')\n\n        # Second run.\n        x = torch.randn(BATCH_SIZE, NUM_INPUTS)\n\n        y, logdets = m1(x)\n        z, inv_logdets = m1(y, mode=\'inverse\')\n\n        self.assertTrue((logdets + inv_logdets).abs().max() < EPS,\n                        \'Sequential Det is not zero for the second run.\')\n        self.assertTrue((x - z).abs().max() < EPS,\n                        \'Sequential is wrong for the second run.\')\n\n    def testSequentialBN(self):\n        m1 = fnn.FlowSequential(\n            fnn.BatchNormFlow(NUM_INPUTS), fnn.InvertibleMM(NUM_INPUTS),\n            fnn.CouplingLayer(NUM_INPUTS, NUM_HIDDEN, mask))\n\n        m1.train()\n        x = torch.randn(BATCH_SIZE, NUM_INPUTS)\n\n        y, logdets = m1(x)\n        z, inv_logdets = m1(y, mode=\'inverse\')\n\n        self.assertTrue((logdets + inv_logdets).abs().max() < EPS,\n                        \'Sequential BN Det is not zero.\')\n        self.assertTrue((x - z).abs().max() < EPS, \'Sequential BN is wrong.\')\n\n        # Second run.\n        x = torch.randn(BATCH_SIZE, NUM_INPUTS)\n\n        y, logdets = m1(x)\n        z, inv_logdets = m1(y, mode=\'inverse\')\n\n        self.assertTrue((logdets + inv_logdets).abs().max() < EPS,\n                        \'Sequential BN Det is not zero for the second run.\')\n        self.assertTrue((x - z).abs().max() < EPS,\n                        \'Sequential BN is wrong for the second run.\')\n\n        m1.eval()\n        # Eval run.\n        x = torch.randn(BATCH_SIZE, NUM_INPUTS)\n\n        y, logdets = m1(x)\n        z, inv_logdets = m1(y, mode=\'inverse\')\n\n        self.assertTrue((logdets + inv_logdets).abs().max() < EPS,\n                        \'Sequential BN Det is not zero for the eval run.\')\n        self.assertTrue((x - z).abs().max() < EPS,\n                        \'Sequential BN is wrong for the eval run.\')\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
flows.py,54,"b'import math\nimport types\n\nimport numpy as np\nimport scipy as sp\nimport scipy.linalg\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\ndef get_mask(in_features, out_features, in_flow_features, mask_type=None):\n    """"""\n    mask_type: input | None | output\n    \n    See Figure 1 for a better illustration:\n    https://arxiv.org/pdf/1502.03509.pdf\n    """"""\n    if mask_type == \'input\':\n        in_degrees = torch.arange(in_features) % in_flow_features\n    else:\n        in_degrees = torch.arange(in_features) % (in_flow_features - 1)\n\n    if mask_type == \'output\':\n        out_degrees = torch.arange(out_features) % in_flow_features - 1\n    else:\n        out_degrees = torch.arange(out_features) % (in_flow_features - 1)\n\n    return (out_degrees.unsqueeze(-1) >= in_degrees.unsqueeze(0)).float()\n\n\nclass MaskedLinear(nn.Module):\n    def __init__(self,\n                 in_features,\n                 out_features,\n                 mask,\n                 cond_in_features=None,\n                 bias=True):\n        super(MaskedLinear, self).__init__()\n        self.linear = nn.Linear(in_features, out_features)\n        if cond_in_features is not None:\n            self.cond_linear = nn.Linear(\n                cond_in_features, out_features, bias=False)\n\n        self.register_buffer(\'mask\', mask)\n\n    def forward(self, inputs, cond_inputs=None):\n        output = F.linear(inputs, self.linear.weight * self.mask,\n                          self.linear.bias)\n        if cond_inputs is not None:\n            output += self.cond_linear(cond_inputs)\n        return output\n\n\nnn.MaskedLinear = MaskedLinear\n\n\nclass MADESplit(nn.Module):\n    """""" An implementation of MADE\n    (https://arxiv.org/abs/1502.03509).\n    """"""\n\n    def __init__(self,\n                 num_inputs,\n                 num_hidden,\n                 num_cond_inputs=None,\n                 s_act=\'tanh\',\n                 t_act=\'relu\',\n                 pre_exp_tanh=False):\n        super(MADESplit, self).__init__()\n\n        self.pre_exp_tanh = pre_exp_tanh\n\n        activations = {\'relu\': nn.ReLU, \'sigmoid\': nn.Sigmoid, \'tanh\': nn.Tanh}\n\n        input_mask = get_mask(num_inputs, num_hidden, num_inputs,\n                              mask_type=\'input\')\n        hidden_mask = get_mask(num_hidden, num_hidden, num_inputs)\n        output_mask = get_mask(num_hidden, num_inputs, num_inputs,\n                               mask_type=\'output\')\n\n        act_func = activations[s_act]\n        self.s_joiner = nn.MaskedLinear(num_inputs, num_hidden, input_mask,\n                                      num_cond_inputs)\n\n        self.s_trunk = nn.Sequential(act_func(),\n                                   nn.MaskedLinear(num_hidden, num_hidden,\n                                                   hidden_mask), act_func(),\n                                   nn.MaskedLinear(num_hidden, num_inputs,\n                                                   output_mask))\n\n        act_func = activations[t_act]\n        self.t_joiner = nn.MaskedLinear(num_inputs, num_hidden, input_mask,\n                                      num_cond_inputs)\n\n        self.t_trunk = nn.Sequential(act_func(),\n                                   nn.MaskedLinear(num_hidden, num_hidden,\n                                                   hidden_mask), act_func(),\n                                   nn.MaskedLinear(num_hidden, num_inputs,\n                                                   output_mask))\n        \n    def forward(self, inputs, cond_inputs=None, mode=\'direct\'):\n        if mode == \'direct\':\n            h = self.s_joiner(inputs, cond_inputs)\n            m = self.s_trunk(h)\n            \n            h = self.t_joiner(inputs, cond_inputs)\n            a = self.t_trunk(h)\n\n            if self.pre_exp_tanh:\n                a = torch.tanh(a)\n            \n            u = (inputs - m) * torch.exp(-a)\n            return u, -a.sum(-1, keepdim=True)\n\n        else:\n            x = torch.zeros_like(inputs)\n            for i_col in range(inputs.shape[1]):\n                h = self.s_joiner(x, cond_inputs)\n                m = self.s_trunk(h)\n\n                h = self.t_joiner(x, cond_inputs)\n                a = self.t_trunk(h)\n\n                if self.pre_exp_tanh:\n                    a = torch.tanh(a)\n\n                x[:, i_col] = inputs[:, i_col] * torch.exp(\n                    a[:, i_col]) + m[:, i_col]\n            return x, -a.sum(-1, keepdim=True)\n\nclass MADE(nn.Module):\n    """""" An implementation of MADE\n    (https://arxiv.org/abs/1502.03509).\n    """"""\n\n    def __init__(self,\n                 num_inputs,\n                 num_hidden,\n                 num_cond_inputs=None,\n                 act=\'relu\',\n                 pre_exp_tanh=False):\n        super(MADE, self).__init__()\n\n        activations = {\'relu\': nn.ReLU, \'sigmoid\': nn.Sigmoid, \'tanh\': nn.Tanh}\n        act_func = activations[act]\n\n        input_mask = get_mask(\n            num_inputs, num_hidden, num_inputs, mask_type=\'input\')\n        hidden_mask = get_mask(num_hidden, num_hidden, num_inputs)\n        output_mask = get_mask(\n            num_hidden, num_inputs * 2, num_inputs, mask_type=\'output\')\n\n        self.joiner = nn.MaskedLinear(num_inputs, num_hidden, input_mask,\n                                      num_cond_inputs)\n\n        self.trunk = nn.Sequential(act_func(),\n                                   nn.MaskedLinear(num_hidden, num_hidden,\n                                                   hidden_mask), act_func(),\n                                   nn.MaskedLinear(num_hidden, num_inputs * 2,\n                                                   output_mask))\n\n    def forward(self, inputs, cond_inputs=None, mode=\'direct\'):\n        if mode == \'direct\':\n            h = self.joiner(inputs, cond_inputs)\n            m, a = self.trunk(h).chunk(2, 1)\n            u = (inputs - m) * torch.exp(-a)\n            return u, -a.sum(-1, keepdim=True)\n\n        else:\n            x = torch.zeros_like(inputs)\n            for i_col in range(inputs.shape[1]):\n                h = self.joiner(x, cond_inputs)\n                m, a = self.trunk(h).chunk(2, 1)\n                x[:, i_col] = inputs[:, i_col] * torch.exp(\n                    a[:, i_col]) + m[:, i_col]\n            return x, -a.sum(-1, keepdim=True)\n\n\nclass Sigmoid(nn.Module):\n    def __init__(self):\n        super(Sigmoid, self).__init__()\n\n    def forward(self, inputs, cond_inputs=None, mode=\'direct\'):\n        if mode == \'direct\':\n            s = torch.sigmoid\n            return s(inputs), torch.log(s(inputs) * (1 - s(inputs))).sum(\n                -1, keepdim=True)\n        else:\n            return torch.log(inputs /\n                             (1 - inputs)), -torch.log(inputs - inputs**2).sum(\n                                 -1, keepdim=True)\n\n\nclass Logit(Sigmoid):\n    def __init__(self):\n        super(Logit, self).__init__()\n\n    def forward(self, inputs, cond_inputs=None, mode=\'direct\'):\n        if mode == \'direct\':\n            return super(Logit, self).forward(inputs, \'inverse\')\n        else:\n            return super(Logit, self).forward(inputs, \'direct\')\n\n\nclass BatchNormFlow(nn.Module):\n    """""" An implementation of a batch normalization layer from\n    Density estimation using Real NVP\n    (https://arxiv.org/abs/1605.08803).\n    """"""\n\n    def __init__(self, num_inputs, momentum=0.0, eps=1e-5):\n        super(BatchNormFlow, self).__init__()\n\n        self.log_gamma = nn.Parameter(torch.zeros(num_inputs))\n        self.beta = nn.Parameter(torch.zeros(num_inputs))\n        self.momentum = momentum\n        self.eps = eps\n\n        self.register_buffer(\'running_mean\', torch.zeros(num_inputs))\n        self.register_buffer(\'running_var\', torch.ones(num_inputs))\n\n    def forward(self, inputs, cond_inputs=None, mode=\'direct\'):\n        if mode == \'direct\':\n            if self.training:\n                self.batch_mean = inputs.mean(0)\n                self.batch_var = (\n                    inputs - self.batch_mean).pow(2).mean(0) + self.eps\n\n                self.running_mean.mul_(self.momentum)\n                self.running_var.mul_(self.momentum)\n\n                self.running_mean.add_(self.batch_mean.data *\n                                       (1 - self.momentum))\n                self.running_var.add_(self.batch_var.data *\n                                      (1 - self.momentum))\n\n                mean = self.batch_mean\n                var = self.batch_var\n            else:\n                mean = self.running_mean\n                var = self.running_var\n\n            x_hat = (inputs - mean) / var.sqrt()\n            y = torch.exp(self.log_gamma) * x_hat + self.beta\n            return y, (self.log_gamma - 0.5 * torch.log(var)).sum(\n                -1, keepdim=True)\n        else:\n            if self.training:\n                mean = self.batch_mean\n                var = self.batch_var\n            else:\n                mean = self.running_mean\n                var = self.running_var\n\n            x_hat = (inputs - self.beta) / torch.exp(self.log_gamma)\n\n            y = x_hat * var.sqrt() + mean\n\n            return y, (-self.log_gamma + 0.5 * torch.log(var)).sum(\n                -1, keepdim=True)\n\n\nclass ActNorm(nn.Module):\n    """""" An implementation of a activation normalization layer\n    from Glow: Generative Flow with Invertible 1x1 Convolutions\n    (https://arxiv.org/abs/1807.03039).\n    """"""\n\n    def __init__(self, num_inputs):\n        super(ActNorm, self).__init__()\n        self.weight = nn.Parameter(torch.ones(num_inputs))\n        self.bias = nn.Parameter(torch.zeros(num_inputs))\n        self.initialized = False\n\n    def forward(self, inputs, cond_inputs=None, mode=\'direct\'):\n        if self.initialized == False:\n            self.weight.data.copy_(torch.log(1.0 / (inputs.std(0) + 1e-12)))\n            self.bias.data.copy_(inputs.mean(0))\n            self.initialized = True\n\n        if mode == \'direct\':\n            return (\n                inputs - self.bias) * torch.exp(self.weight), self.weight.sum(\n                    -1, keepdim=True).unsqueeze(0).repeat(inputs.size(0), 1)\n        else:\n            return inputs * torch.exp(\n                -self.weight) + self.bias, -self.weight.sum(\n                    -1, keepdim=True).unsqueeze(0).repeat(inputs.size(0), 1)\n\n\nclass InvertibleMM(nn.Module):\n    """""" An implementation of a invertible matrix multiplication\n    layer from Glow: Generative Flow with Invertible 1x1 Convolutions\n    (https://arxiv.org/abs/1807.03039).\n    """"""\n\n    def __init__(self, num_inputs):\n        super(InvertibleMM, self).__init__()\n        self.W = nn.Parameter(torch.Tensor(num_inputs, num_inputs))\n        nn.init.orthogonal_(self.W)\n\n    def forward(self, inputs, cond_inputs=None, mode=\'direct\'):\n        if mode == \'direct\':\n            return inputs @ self.W, torch.slogdet(\n                self.W)[-1].unsqueeze(0).unsqueeze(0).repeat(\n                    inputs.size(0), 1)\n        else:\n            return inputs @ torch.inverse(self.W), -torch.slogdet(\n                self.W)[-1].unsqueeze(0).unsqueeze(0).repeat(\n                    inputs.size(0), 1)\n\n\nclass LUInvertibleMM(nn.Module):\n    """""" An implementation of a invertible matrix multiplication\n    layer from Glow: Generative Flow with Invertible 1x1 Convolutions\n    (https://arxiv.org/abs/1807.03039).\n    """"""\n\n    def __init__(self, num_inputs):\n        super(LUInvertibleMM, self).__init__()\n        self.W = torch.Tensor(num_inputs, num_inputs)\n        nn.init.orthogonal_(self.W)\n        self.L_mask = torch.tril(torch.ones(self.W.size()), -1)\n        self.U_mask = self.L_mask.t().clone()\n\n        P, L, U = sp.linalg.lu(self.W.numpy())\n        self.P = torch.from_numpy(P)\n        self.L = nn.Parameter(torch.from_numpy(L))\n        self.U = nn.Parameter(torch.from_numpy(U))\n\n        S = np.diag(U)\n        sign_S = np.sign(S)\n        log_S = np.log(abs(S))\n        self.sign_S = torch.from_numpy(sign_S)\n        self.log_S = nn.Parameter(torch.from_numpy(log_S))\n\n        self.I = torch.eye(self.L.size(0))\n\n    def forward(self, inputs, cond_inputs=None, mode=\'direct\'):\n        if str(self.L_mask.device) != str(self.L.device):\n            self.L_mask = self.L_mask.to(self.L.device)\n            self.U_mask = self.U_mask.to(self.L.device)\n            self.I = self.I.to(self.L.device)\n            self.P = self.P.to(self.L.device)\n            self.sign_S = self.sign_S.to(self.L.device)\n\n        L = self.L * self.L_mask + self.I\n        U = self.U * self.U_mask + torch.diag(\n            self.sign_S * torch.exp(self.log_S))\n        W = self.P @ L @ U\n\n        if mode == \'direct\':\n            return inputs @ W, self.log_S.sum().unsqueeze(0).unsqueeze(\n                0).repeat(inputs.size(0), 1)\n        else:\n            return inputs @ torch.inverse(\n                W), -self.log_S.sum().unsqueeze(0).unsqueeze(0).repeat(\n                    inputs.size(0), 1)\n\n\nclass Shuffle(nn.Module):\n    """""" An implementation of a shuffling layer from\n    Density estimation using Real NVP\n    (https://arxiv.org/abs/1605.08803).\n    """"""\n\n    def __init__(self, num_inputs):\n        super(Shuffle, self).__init__()\n        self.perm = np.random.permutation(num_inputs)\n        self.inv_perm = np.argsort(self.perm)\n\n    def forward(self, inputs, cond_inputs=None, mode=\'direct\'):\n        if mode == \'direct\':\n            return inputs[:, self.perm], torch.zeros(\n                inputs.size(0), 1, device=inputs.device)\n        else:\n            return inputs[:, self.inv_perm], torch.zeros(\n                inputs.size(0), 1, device=inputs.device)\n\n\nclass Reverse(nn.Module):\n    """""" An implementation of a reversing layer from\n    Density estimation using Real NVP\n    (https://arxiv.org/abs/1605.08803).\n    """"""\n\n    def __init__(self, num_inputs):\n        super(Reverse, self).__init__()\n        self.perm = np.array(np.arange(0, num_inputs)[::-1])\n        self.inv_perm = np.argsort(self.perm)\n\n    def forward(self, inputs, cond_inputs=None, mode=\'direct\'):\n        if mode == \'direct\':\n            return inputs[:, self.perm], torch.zeros(\n                inputs.size(0), 1, device=inputs.device)\n        else:\n            return inputs[:, self.inv_perm], torch.zeros(\n                inputs.size(0), 1, device=inputs.device)\n\n\nclass CouplingLayer(nn.Module):\n    """""" An implementation of a coupling layer\n    from RealNVP (https://arxiv.org/abs/1605.08803).\n    """"""\n\n    def __init__(self,\n                 num_inputs,\n                 num_hidden,\n                 mask,\n                 num_cond_inputs=None,\n                 s_act=\'tanh\',\n                 t_act=\'relu\'):\n        super(CouplingLayer, self).__init__()\n\n        self.num_inputs = num_inputs\n        self.mask = mask\n\n        activations = {\'relu\': nn.ReLU, \'sigmoid\': nn.Sigmoid, \'tanh\': nn.Tanh}\n        s_act_func = activations[s_act]\n        t_act_func = activations[t_act]\n\n        if num_cond_inputs is not None:\n            total_inputs = num_inputs + num_cond_inputs\n        else:\n            total_inputs = num_inputs\n            \n        self.scale_net = nn.Sequential(\n            nn.Linear(total_inputs, num_hidden), s_act_func(),\n            nn.Linear(num_hidden, num_hidden), s_act_func(),\n            nn.Linear(num_hidden, num_inputs))\n        self.translate_net = nn.Sequential(\n            nn.Linear(total_inputs, num_hidden), t_act_func(),\n            nn.Linear(num_hidden, num_hidden), t_act_func(),\n            nn.Linear(num_hidden, num_inputs))\n\n        def init(m):\n            if isinstance(m, nn.Linear):\n                m.bias.data.fill_(0)\n                nn.init.orthogonal_(m.weight.data)\n\n    def forward(self, inputs, cond_inputs=None, mode=\'direct\'):\n        mask = self.mask\n        \n        masked_inputs = inputs * mask\n        if cond_inputs is not None:\n            masked_inputs = torch.cat([masked_inputs, cond_inputs], -1)\n        \n        if mode == \'direct\':\n            log_s = self.scale_net(masked_inputs) * (1 - mask)\n            t = self.translate_net(masked_inputs) * (1 - mask)\n            s = torch.exp(log_s)\n            return inputs * s + t, log_s.sum(-1, keepdim=True)\n        else:\n            log_s = self.scale_net(masked_inputs) * (1 - mask)\n            t = self.translate_net(masked_inputs) * (1 - mask)\n            s = torch.exp(-log_s)\n            return (inputs - t) * s, -log_s.sum(-1, keepdim=True)\n\n\nclass FlowSequential(nn.Sequential):\n    """""" A sequential container for flows.\n    In addition to a forward pass it implements a backward pass and\n    computes log jacobians.\n    """"""\n\n    def forward(self, inputs, cond_inputs=None, mode=\'direct\', logdets=None):\n        """""" Performs a forward or backward pass for flow modules.\n        Args:\n            inputs: a tuple of inputs and logdets\n            mode: to run direct computation or inverse\n        """"""\n        self.num_inputs = inputs.size(-1)\n\n        if logdets is None:\n            logdets = torch.zeros(inputs.size(0), 1, device=inputs.device)\n\n        assert mode in [\'direct\', \'inverse\']\n        if mode == \'direct\':\n            for module in self._modules.values():\n                inputs, logdet = module(inputs, cond_inputs, mode)\n                logdets += logdet\n        else:\n            for module in reversed(self._modules.values()):\n                inputs, logdet = module(inputs, cond_inputs, mode)\n                logdets += logdet\n\n        return inputs, logdets\n\n    def log_probs(self, inputs, cond_inputs = None):\n        u, log_jacob = self(inputs, cond_inputs)\n        log_probs = (-0.5 * u.pow(2) - 0.5 * math.log(2 * math.pi)).sum(\n            -1, keepdim=True)\n        return (log_probs + log_jacob).sum(-1, keepdim=True)\n\n    def sample(self, num_samples=None, noise=None, cond_inputs=None):\n        if noise is None:\n            noise = torch.Tensor(num_samples, self.num_inputs).normal_()\n        device = next(self.parameters()).device\n        noise = noise.to(device)\n        if cond_inputs is not None:\n            cond_inputs = cond_inputs.to(device)\n        samples = self.forward(noise, cond_inputs, mode=\'inverse\')[0]\n        return samples\n'"
main.py,31,"b'import argparse\nimport copy\nimport math\nimport sys\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport torch.utils.data\nfrom tqdm import tqdm\nfrom tensorboardX import SummaryWriter\n\nimport datasets\nimport flows as fnn\nimport utils\n\nif sys.version_info < (3, 6):\n    print(\'Sorry, this code might need Python 3.6 or higher\')\n\n# Training settings\nparser = argparse.ArgumentParser(description=\'PyTorch Flows\')\nparser.add_argument(\n    \'--batch-size\',\n    type=int,\n    default=100,\n    help=\'input batch size for training (default: 100)\')\nparser.add_argument(\n    \'--test-batch-size\',\n    type=int,\n    default=1000,\n    help=\'input batch size for testing (default: 1000)\')\nparser.add_argument(\n    \'--epochs\',\n    type=int,\n    default=1000,\n    help=\'number of epochs to train (default: 1000)\')\nparser.add_argument(\n    \'--lr\', type=float, default=0.0001, help=\'learning rate (default: 0.0001)\')\nparser.add_argument(\n    \'--dataset\',\n    default=\'POWER\',\n    help=\'POWER | GAS | HEPMASS | MINIBONE | BSDS300 | MOONS\')\nparser.add_argument(\n    \'--flow\', default=\'maf\', help=\'flow to use: maf | realnvp | glow\')\nparser.add_argument(\n    \'--no-cuda\',\n    action=\'store_true\',\n    default=False,\n    help=\'disables CUDA training\')\nparser.add_argument(\n    \'--cond\',\n    action=\'store_true\',\n    default=False,\n    help=\'train class conditional flow (only for MNIST)\')\nparser.add_argument(\n    \'--num-blocks\',\n    type=int,\n    default=5,\n    help=\'number of invertible blocks (default: 5)\')\nparser.add_argument(\n    \'--seed\', type=int, default=1, help=\'random seed (default: 1)\')\nparser.add_argument(\n    \'--log-interval\',\n    type=int,\n    default=1000,\n    help=\'how many batches to wait before logging training status\')\n\nargs = parser.parse_args()\nargs.cuda = not args.no_cuda and torch.cuda.is_available()\ndevice = torch.device(""cuda:0"" if args.cuda else ""cpu"")\n\ntorch.manual_seed(args.seed)\nif args.cuda:\n    torch.cuda.manual_seed(args.seed)\n    \nkwargs = {\'num_workers\': 4, \'pin_memory\': True} if args.cuda else {}\n\nassert args.dataset in [\n    \'POWER\', \'GAS\', \'HEPMASS\', \'MINIBONE\', \'BSDS300\', \'MOONS\', \'MNIST\'\n]\ndataset = getattr(datasets, args.dataset)()\n\nif args.cond:\n    assert args.flow in [\'maf\', \'realnvp\'] and args.dataset == \'MNIST\', \\\n        \'Conditional flows are implemented only for maf and MNIST\'\n    \n    train_tensor = torch.from_numpy(dataset.trn.x)\n    train_labels = torch.from_numpy(dataset.trn.y)\n    train_dataset = torch.utils.data.TensorDataset(train_tensor, train_labels)\n\n    valid_tensor = torch.from_numpy(dataset.val.x)\n    valid_labels = torch.from_numpy(dataset.val.y)\n    valid_dataset = torch.utils.data.TensorDataset(valid_tensor, valid_labels)\n\n    test_tensor = torch.from_numpy(dataset.tst.x)\n    test_labels = torch.from_numpy(dataset.tst.y)\n    test_dataset = torch.utils.data.TensorDataset(test_tensor, test_labels)\n    num_cond_inputs = 10\nelse:\n    train_tensor = torch.from_numpy(dataset.trn.x)\n    train_dataset = torch.utils.data.TensorDataset(train_tensor)\n\n    valid_tensor = torch.from_numpy(dataset.val.x)\n    valid_dataset = torch.utils.data.TensorDataset(valid_tensor)\n\n    test_tensor = torch.from_numpy(dataset.tst.x)\n    test_dataset = torch.utils.data.TensorDataset(test_tensor)\n    num_cond_inputs = None\n    \ntrain_loader = torch.utils.data.DataLoader(\n    train_dataset, batch_size=args.batch_size, shuffle=True, **kwargs)\n\nvalid_loader = torch.utils.data.DataLoader(\n    valid_dataset,\n    batch_size=args.test_batch_size,\n    shuffle=False,\n    drop_last=False,\n    **kwargs)\n\ntest_loader = torch.utils.data.DataLoader(\n    test_dataset,\n    batch_size=args.test_batch_size,\n    shuffle=False,\n    drop_last=False,\n    **kwargs)\n\nnum_inputs = dataset.n_dims\nnum_hidden = {\n    \'POWER\': 100,\n    \'GAS\': 100,\n    \'HEPMASS\': 512,\n    \'MINIBOONE\': 512,\n    \'BSDS300\': 512,\n    \'MOONS\': 64,\n    \'MNIST\': 1024\n}[args.dataset]\n\nact = \'tanh\' if args.dataset is \'GAS\' else \'relu\'\n\nmodules = []\n\nassert args.flow in [\'maf\', \'maf-split\', \'maf-split-glow\', \'realnvp\', \'glow\']\nif args.flow == \'glow\':\n    mask = torch.arange(0, num_inputs) % 2\n    mask = mask.to(device).float()\n\n    print(""Warning: Results for GLOW are not as good as for MAF yet."")\n    for _ in range(args.num_blocks):\n        modules += [\n            fnn.BatchNormFlow(num_inputs),\n            fnn.LUInvertibleMM(num_inputs),\n            fnn.CouplingLayer(\n                num_inputs, num_hidden, mask, num_cond_inputs,\n                s_act=\'tanh\', t_act=\'relu\')\n        ]\n        mask = 1 - mask\nelif args.flow == \'realnvp\':\n    mask = torch.arange(0, num_inputs) % 2\n    mask = mask.to(device).float()\n\n    for _ in range(args.num_blocks):\n        modules += [\n            fnn.CouplingLayer(\n                num_inputs, num_hidden, mask, num_cond_inputs,\n                s_act=\'tanh\', t_act=\'relu\'),\n            fnn.BatchNormFlow(num_inputs)\n        ]\n        mask = 1 - mask\nelif args.flow == \'maf\':\n    for _ in range(args.num_blocks):\n        modules += [\n            fnn.MADE(num_inputs, num_hidden, num_cond_inputs, act=act),\n            fnn.BatchNormFlow(num_inputs),\n            fnn.Reverse(num_inputs)\n        ]\nelif args.flow == \'maf-split\':\n    for _ in range(args.num_blocks):\n        modules += [\n            fnn.MADESplit(num_inputs, num_hidden, num_cond_inputs,\n                         s_act=\'tanh\', t_act=\'relu\'),\n            fnn.BatchNormFlow(num_inputs),\n            fnn.Reverse(num_inputs)\n        ]\nelif args.flow == \'maf-split-glow\':\n    for _ in range(args.num_blocks):\n        modules += [\n            fnn.MADESplit(num_inputs, num_hidden, num_cond_inputs,\n                         s_act=\'tanh\', t_act=\'relu\'),\n            fnn.BatchNormFlow(num_inputs),\n            fnn.InvertibleMM(num_inputs)\n        ]\n\nmodel = fnn.FlowSequential(*modules)\n\nfor module in model.modules():\n    if isinstance(module, nn.Linear):\n        nn.init.orthogonal_(module.weight)\n        if hasattr(module, \'bias\') and module.bias is not None:\n            module.bias.data.fill_(0)\n\nmodel.to(device)\n\noptimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=1e-6)\n\nwriter = SummaryWriter(comment=args.flow + ""_"" + args.dataset)\nglobal_step = 0\n\ndef train(epoch):\n    global global_step, writer\n    model.train()\n    train_loss = 0\n\n    pbar = tqdm(total=len(train_loader.dataset))\n    for batch_idx, data in enumerate(train_loader):\n        if isinstance(data, list):\n            if len(data) > 1:\n                cond_data = data[1].float()\n                cond_data = cond_data.to(device)\n            else:\n                cond_data = None\n\n            data = data[0]\n        data = data.to(device)\n        optimizer.zero_grad()\n        loss = -model.log_probs(data, cond_data).mean()\n        train_loss += loss.item()\n        loss.backward()\n        optimizer.step()\n\n        pbar.update(data.size(0))\n        pbar.set_description(\'Train, Log likelihood in nats: {:.6f}\'.format(\n            -train_loss / (batch_idx + 1)))\n        \n        writer.add_scalar(\'training/loss\', loss.item(), global_step)\n        global_step += 1\n        \n    pbar.close()\n        \n    for module in model.modules():\n        if isinstance(module, fnn.BatchNormFlow):\n            module.momentum = 0\n\n    if args.cond:\n        with torch.no_grad():\n            model(train_loader.dataset.tensors[0].to(data.device),\n                train_loader.dataset.tensors[1].to(data.device).float())\n    else:\n        with torch.no_grad():\n            model(train_loader.dataset.tensors[0].to(data.device))\n\n\n    for module in model.modules():\n        if isinstance(module, fnn.BatchNormFlow):\n            module.momentum = 1\n\n\ndef validate(epoch, model, loader, prefix=\'Validation\'):\n    global global_step, writer\n\n    model.eval()\n    val_loss = 0\n\n    pbar = tqdm(total=len(loader.dataset))\n    pbar.set_description(\'Eval\')\n    for batch_idx, data in enumerate(loader):\n        if isinstance(data, list):\n            if len(data) > 1:\n                cond_data = data[1].float()\n                cond_data = cond_data.to(device)\n            else:\n                cond_data = None\n\n            data = data[0]\n        data = data.to(device)\n        with torch.no_grad():\n            val_loss += -model.log_probs(data, cond_data).sum().item()  # sum up batch loss\n        pbar.update(data.size(0))\n        pbar.set_description(\'Val, Log likelihood in nats: {:.6f}\'.format(\n            -val_loss / pbar.n))\n\n    writer.add_scalar(\'validation/LL\', val_loss / len(loader.dataset), epoch)\n\n    pbar.close()\n    return val_loss / len(loader.dataset)\n\n\nbest_validation_loss = float(\'inf\')\nbest_validation_epoch = 0\nbest_model = model\n\nfor epoch in range(args.epochs):\n    print(\'\\nEpoch: {}\'.format(epoch))\n\n    train(epoch)\n    validation_loss = validate(epoch, model, valid_loader)\n\n    if epoch - best_validation_epoch >= 30:\n        break\n\n    if validation_loss < best_validation_loss:\n        best_validation_epoch = epoch\n        best_validation_loss = validation_loss\n        best_model = copy.deepcopy(model)\n\n    print(\n        \'Best validation at epoch {}: Average Log Likelihood in nats: {:.4f}\'.\n        format(best_validation_epoch, -best_validation_loss))\n\n    if args.dataset == \'MOONS\' and epoch % 10 == 0:\n        utils.save_moons_plot(epoch, model, dataset)\n    elif args.dataset == \'MNIST\' and epoch % 1 == 0:\n        utils.save_images(epoch, model, args.cond)\n\n\nvalidate(best_validation_epoch, best_model, test_loader, prefix=\'Test\')\n'"
utils.py,6,"b""import os\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport torch\nimport torchvision\n\n\ndef save_moons_plot(epoch, best_model, dataset):\n    # generate some examples\n    best_model.eval()\n    with torch.no_grad():\n        x_synth = best_model.sample(500).detach().cpu().numpy()\n\n    fig = plt.figure()\n\n    ax = fig.add_subplot(121)\n    ax.plot(dataset.val.x[:, 0], dataset.val.x[:, 1], '.')\n    ax.set_title('Real data')\n\n    ax = fig.add_subplot(122)\n    ax.plot(x_synth[:, 0], x_synth[:, 1], '.')\n    ax.set_title('Synth data')\n\n    try:\n        os.makedirs('plots')\n    except OSError:\n        pass\n\n    plt.savefig('plots/plot_{:03d}.png'.format(epoch))\n    plt.close()\n\n\nbatch_size = 100\nfixed_noise = torch.Tensor(batch_size, 28 * 28).normal_()\ny = torch.arange(batch_size).unsqueeze(-1) % 10\ny_onehot = torch.FloatTensor(batch_size, 10)\ny_onehot.zero_()\ny_onehot.scatter_(1, y, 1)\n\n\ndef save_images(epoch, best_model, cond):\n    best_model.eval()\n    with torch.no_grad():\n        if cond:\n            imgs = best_model.sample(batch_size, noise=fixed_noise, cond_inputs=y_onehot).detach().cpu()\n        else:\n            imgs = best_model.sample(batch_size, noise=fixed_noise).detach().cpu()\n\n        imgs = torch.sigmoid(imgs.view(batch_size, 1, 28, 28))\n    \n    try:\n        os.makedirs('images')\n    except OSError:\n        pass\n\n    torchvision.utils.save_image(imgs, 'images/img_{:03d}.png'.format(epoch), nrow=10)\n\n"""
datasets/__init__.py,0,"b""root = 'data/'\n\nfrom .power import POWER\nfrom .gas import GAS\nfrom .hepmass import HEPMASS\nfrom .miniboone import MINIBOONE\nfrom .bsds300 import BSDS300\nfrom .moons import MOONS\nfrom .mnist import MNIST"""
datasets/bsds300.py,0,"b'import h5py\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nimport datasets\n\nfrom . import util\n\n\nclass BSDS300:\n    """"""\n    A dataset of patches from BSDS300.\n    """"""\n\n    class Data:\n        """"""\n        Constructs the dataset.\n        """"""\n\n        def __init__(self, data):\n\n            self.x = data[:]\n            self.N = self.x.shape[0]\n\n    def __init__(self):\n\n        # load dataset\n        f = h5py.File(datasets.root + \'BSDS300/BSDS300.hdf5\', \'r\')\n\n        self.trn = self.Data(f[\'train\'])\n        self.val = self.Data(f[\'validation\'])\n        self.tst = self.Data(f[\'test\'])\n\n        self.n_dims = self.trn.x.shape[1]\n        self.image_size = [int(np.sqrt(self.n_dims + 1))] * 2\n\n        f.close()\n\n    def show_pixel_histograms(self, split, pixel=None):\n        """"""\n        Shows the histogram of pixel values, or of a specific pixel if given.\n        """"""\n\n        data_split = getattr(self, split, None)\n        if data_split is None:\n            raise ValueError(\'Invalid data split\')\n\n        if pixel is None:\n            data = data_split.x.flatten()\n\n        else:\n            row, col = pixel\n            idx = row * self.image_size[0] + col\n            data = data_split.x[:, idx]\n\n        n_bins = int(np.sqrt(data_split.N))\n        fig, ax = plt.subplots(1, 1)\n        ax.hist(data, n_bins, normed=True)\n        plt.show()\n\n    def show_images(self, split):\n        """"""\n        Displays the images in a given split.\n        :param split: string\n        """"""\n\n        # get split\n        data_split = getattr(self, split, None)\n        if data_split is None:\n            raise ValueError(\'Invalid data split\')\n\n        # add a pixel at the bottom right\n        last_pixel = -np.sum(data_split.x, axis=1)\n        images = np.hstack([data_split.x, last_pixel[:, np.newaxis]])\n\n        # display images\n        util.disp_imdata(images, self.image_size, [6, 10])\n\n        plt.show()\n'"
datasets/gas.py,0,"b'import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\nimport datasets\n\nfrom . import util\n\n\nclass GAS:\n    class Data:\n        def __init__(self, data):\n\n            self.x = data.astype(np.float32)\n            self.N = self.x.shape[0]\n\n    def __init__(self):\n\n        file = datasets.root + \'gas/ethylene_CO.pickle\'\n        trn, val, tst = load_data_and_clean_and_split(file)\n\n        self.trn = self.Data(trn)\n        self.val = self.Data(val)\n        self.tst = self.Data(tst)\n\n        self.n_dims = self.trn.x.shape[1]\n\n    def show_histograms(self, split):\n\n        data_split = getattr(self, split, None)\n        if data_split is None:\n            raise ValueError(\'Invalid data split\')\n\n        util.plot_hist_marginals(data_split.x)\n        plt.show()\n\n\ndef load_data(file):\n\n    data = pd.read_pickle(file)\n    # data = pd.read_pickle(file).sample(frac=0.25)\n    # data.to_pickle(file)\n    data.drop(""Meth"", axis=1, inplace=True)\n    data.drop(""Eth"", axis=1, inplace=True)\n    data.drop(""Time"", axis=1, inplace=True)\n    return data\n\n\ndef get_correlation_numbers(data):\n    C = data.corr()\n    A = C > 0.98\n    B = A.as_matrix().sum(axis=1)\n    return B\n\n\ndef load_data_and_clean(file):\n\n    data = load_data(file)\n    B = get_correlation_numbers(data)\n\n    while np.any(B > 1):\n        col_to_remove = np.where(B > 1)[0][0]\n        col_name = data.columns[col_to_remove]\n        data.drop(col_name, axis=1, inplace=True)\n        B = get_correlation_numbers(data)\n    # print(data.corr())\n    data = (data - data.mean()) / data.std()\n\n    return data\n\n\ndef load_data_and_clean_and_split(file):\n\n    data = load_data_and_clean(file).as_matrix()\n    N_test = int(0.1 * data.shape[0])\n    data_test = data[-N_test:]\n    data_train = data[0:-N_test]\n    N_validate = int(0.1 * data_train.shape[0])\n    data_validate = data_train[-N_validate:]\n    data_train = data_train[0:-N_validate]\n\n    return data_train, data_validate, data_test\n'"
datasets/hepmass.py,0,"b'from collections import Counter\nfrom os.path import join\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\nimport datasets\n\nfrom . import util\n\n\nclass HEPMASS:\n    """"""\n    The HEPMASS data set.\n    http://archive.ics.uci.edu/ml/datasets/HEPMASS\n    """"""\n\n    class Data:\n        def __init__(self, data):\n\n            self.x = data.astype(np.float32)\n            self.N = self.x.shape[0]\n\n    def __init__(self):\n\n        path = datasets.root + \'hepmass/\'\n        trn, val, tst = load_data_no_discrete_normalised_as_array(path)\n\n        self.trn = self.Data(trn)\n        self.val = self.Data(val)\n        self.tst = self.Data(tst)\n\n        self.n_dims = self.trn.x.shape[1]\n\n    def show_histograms(self, split, vars):\n\n        data_split = getattr(self, split, None)\n        if data_split is None:\n            raise ValueError(\'Invalid data split\')\n\n        util.plot_hist_marginals(data_split.x[:, vars])\n        plt.show()\n\n\ndef load_data(path):\n\n    data_train = pd.read_csv(\n        filepath_or_buffer=join(path, ""1000_train.csv""), index_col=False)\n    data_test = pd.read_csv(\n        filepath_or_buffer=join(path, ""1000_test.csv""), index_col=False)\n\n    return data_train, data_test\n\n\ndef load_data_no_discrete(path):\n    """"""\n    Loads the positive class examples from the first 10 percent of the dataset.\n    """"""\n    data_train, data_test = load_data(path)\n\n    # Gets rid of any background noise examples i.e. class label 0.\n    data_train = data_train[data_train[data_train.columns[0]] == 1]\n    data_train = data_train.drop(data_train.columns[0], axis=1)\n    data_test = data_test[data_test[data_test.columns[0]] == 1]\n    data_test = data_test.drop(data_test.columns[0], axis=1)\n    # Because the data set is messed up!\n    data_test = data_test.drop(data_test.columns[-1], axis=1)\n\n    return data_train, data_test\n\n\ndef load_data_no_discrete_normalised(path):\n\n    data_train, data_test = load_data_no_discrete(path)\n    mu = data_train.mean()\n    s = data_train.std()\n    data_train = (data_train - mu) / s\n    data_test = (data_test - mu) / s\n\n    return data_train, data_test\n\n\ndef load_data_no_discrete_normalised_as_array(path):\n\n    data_train, data_test = load_data_no_discrete_normalised(path)\n    data_train, data_test = data_train.as_matrix(), data_test.as_matrix()\n\n    i = 0\n    # Remove any features that have too many re-occurring real values.\n    features_to_remove = []\n    for feature in data_train.T:\n        c = Counter(feature)\n        max_count = np.array([v for k, v in sorted(c.items())])[0]\n        if max_count > 5:\n            features_to_remove.append(i)\n        i += 1\n    data_train = data_train[:,\n                            np.array([\n                                i for i in range(data_train.shape[1])\n                                if i not in features_to_remove\n                            ])]\n    data_test = data_test[:,\n                          np.array([\n                              i for i in range(data_test.shape[1])\n                              if i not in features_to_remove\n                          ])]\n\n    N = data_train.shape[0]\n    N_validate = int(N * 0.1)\n    data_validate = data_train[-N_validate:]\n    data_train = data_train[0:-N_validate]\n\n    return data_train, data_validate, data_test\n'"
datasets/miniboone.py,0,"b'import matplotlib.pyplot as plt\nimport numpy as np\n\nimport datasets\n\nfrom . import util\n\n\nclass MINIBOONE:\n    class Data:\n        def __init__(self, data):\n\n            self.x = data.astype(np.float32)\n            self.N = self.x.shape[0]\n\n    def __init__(self):\n\n        file = datasets.root + \'miniboone/data.npy\'\n        trn, val, tst = load_data_normalised(file)\n\n        self.trn = self.Data(trn)\n        self.val = self.Data(val)\n        self.tst = self.Data(tst)\n\n        self.n_dims = self.trn.x.shape[1]\n\n    def show_histograms(self, split, vars):\n\n        data_split = getattr(self, split, None)\n        if data_split is None:\n            raise ValueError(\'Invalid data split\')\n\n        util.plot_hist_marginals(data_split.x[:, vars])\n        plt.show()\n\n\ndef load_data(root_path):\n    # NOTE: To remember how the pre-processing was done.\n    # data = pd.read_csv(root_path, names=[str(x) for x in range(50)], delim_whitespace=True)\n    # print data.head()\n    # data = data.as_matrix()\n    # # Remove some random outliers\n    # indices = (data[:, 0] < -100)\n    # data = data[~indices]\n    #\n    # i = 0\n    # # Remove any features that have too many re-occuring real values.\n    # features_to_remove = []\n    # for feature in data.T:\n    #     c = Counter(feature)\n    #     max_count = np.array([v for k, v in sorted(c.iteritems())])[0]\n    #     if max_count > 5:\n    #         features_to_remove.append(i)\n    #     i += 1\n    # data = data[:, np.array([i for i in range(data.shape[1]) if i not in features_to_remove])]\n    # np.save(""~/data/miniboone/data.npy"", data)\n\n    data = np.load(root_path)\n    N_test = int(0.1 * data.shape[0])\n    data_test = data[-N_test:]\n    data = data[0:-N_test]\n    N_validate = int(0.1 * data.shape[0])\n    data_validate = data[-N_validate:]\n    data_train = data[0:-N_validate]\n\n    return data_train, data_validate, data_test\n\n\ndef load_data_normalised(root_path):\n\n    data_train, data_validate, data_test = load_data(root_path)\n    data = np.vstack((data_train, data_validate))\n    mu = data.mean(axis=0)\n    s = data.std(axis=0)\n    data_train = (data_train - mu) / s\n    data_validate = (data_validate - mu) / s\n    data_test = (data_test - mu) / s\n\n    return data_train, data_validate, data_test\n'"
datasets/mnist.py,0,"b'import gzip\nimport pickle\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nimport datasets\n\nfrom . import util\n\n\nclass MNIST:\n    """"""\n    The MNIST dataset of handwritten digits.\n    """"""\n\n    alpha = 1.0e-6\n\n    class Data:\n        """"""\n        Constructs the dataset.\n        """"""\n\n        def __init__(self, data, logit, dequantize, rng):\n\n            x = self._dequantize(\n                data[0], rng) if dequantize else data[0]  # dequantize pixels\n            self.x = self._logit_transform(x) if logit else x  # logit\n            self.labels = data[1]  # numeric labels\n            self.y = util.one_hot_encode(self.labels,\n                                         10)  # 1-hot encoded labels\n            self.N = self.x.shape[0]  # number of datapoints\n            self.x = self.x.astype(\'float32\')\n            self.y = self.y.astype(\'int\')\n\n        @staticmethod\n        def _dequantize(x, rng):\n            """"""\n            Adds noise to pixels to dequantize them.\n            """"""\n            return x + rng.rand(*x.shape) / 256.0\n\n        @staticmethod\n        def _logit_transform(x):\n            """"""\n            Transforms pixel values with logit to be unconstrained.\n            """"""\n            return util.logit(MNIST.alpha + (1 - 2 * MNIST.alpha) * x)\n\n    def __init__(self, logit=True, dequantize=True):\n\n        # load dataset\n        f = gzip.open(datasets.root + \'mnist/mnist.pkl.gz\', \'rb\')\n        trn, val, tst = pickle.load(f, encoding=\'latin1\')\n        f.close()\n\n        rng = np.random.RandomState(42)\n        self.trn = self.Data(trn, logit, dequantize, rng)\n        self.val = self.Data(val, logit, dequantize, rng)\n        self.tst = self.Data(tst, logit, dequantize, rng)\n\n        self.n_dims = self.trn.x.shape[1]\n        self.n_labels = self.trn.y.shape[1]\n        self.image_size = [int(np.sqrt(self.n_dims))] * 2\n\n    def show_pixel_histograms(self, split, pixel=None):\n        """"""\n        Shows the histogram of pixel values, or of a specific pixel if given.\n        """"""\n\n        data_split = getattr(self, split, None)\n        if data_split is None:\n            raise ValueError(\'Invalid data split\')\n\n        if pixel is None:\n            data = data_split.x.flatten()\n\n        else:\n            row, col = pixel\n            idx = row * self.image_size[0] + col\n            data = data_split.x[:, idx]\n\n        n_bins = int(np.sqrt(data_split.N))\n        fig, ax = plt.subplots(1, 1)\n        ax.hist(data, n_bins, normed=True)\n        plt.show()\n\n    def show_images(self, split):\n        """"""\n        Displays the images in a given split.\n        :param split: string\n        """"""\n\n        # get split\n        data_split = getattr(self, split, None)\n        if data_split is None:\n            raise ValueError(\'Invalid data split\')\n\n        # display images\n        util.disp_imdata(data_split.x, self.image_size, [6, 10])\n\n        plt.show()\n'"
datasets/moons.py,0,"b""import matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn import datasets as ds\n\nimport datasets\nimport datasets.util\n\n\nclass MOONS:\n    class Data:\n        def __init__(self, data):\n\n            self.x = data.astype(np.float32)\n            self.N = self.x.shape[0]\n\n    def __init__(self):\n\n        trn, val, tst = load_data()\n\n        self.trn = self.Data(trn)\n        self.val = self.Data(val)\n        self.tst = self.Data(tst)\n\n        self.n_dims = self.trn.x.shape[1]\n\n    def show_histograms(self, split):\n\n        data_split = getattr(self, split, None)\n        if data_split is None:\n            raise ValueError('Invalid data split')\n\n        datasets.util.plot_hist_marginals(data_split.x)\n        plt.show()\n\n\ndef load_data():\n    x = ds.make_moons(n_samples=30000, shuffle=True, noise=0.05)[0]\n    return x[:24000], x[24000:27000], x[27000:]\n"""
datasets/power.py,0,"b""import matplotlib.pyplot as plt\nimport numpy as np\n\nimport datasets\nimport datasets.util\n\n\nclass POWER:\n    class Data:\n        def __init__(self, data):\n\n            self.x = data.astype(np.float32)\n            self.N = self.x.shape[0]\n\n    def __init__(self):\n\n        trn, val, tst = load_data_normalised()\n\n        self.trn = self.Data(trn)\n        self.val = self.Data(val)\n        self.tst = self.Data(tst)\n\n        self.n_dims = self.trn.x.shape[1]\n\n    def show_histograms(self, split):\n\n        data_split = getattr(self, split, None)\n        if data_split is None:\n            raise ValueError('Invalid data split')\n\n        datasets.util.plot_hist_marginals(data_split.x)\n        plt.show()\n\n\ndef load_data():\n    return np.load(datasets.root + 'power/data.npy')\n\n\ndef load_data_split_with_noise():\n\n    rng = np.random.RandomState(42)\n\n    data = load_data()\n    rng.shuffle(data)\n    N = data.shape[0]\n\n    data = np.delete(data, 3, axis=1)\n    data = np.delete(data, 1, axis=1)\n    ############################\n    # Add noise\n    ############################\n    # global_intensity_noise = 0.1*rng.rand(N, 1)\n    voltage_noise = 0.01 * rng.rand(N, 1)\n    # grp_noise = 0.001*rng.rand(N, 1)\n    gap_noise = 0.001 * rng.rand(N, 1)\n    sm_noise = rng.rand(N, 3)\n    time_noise = np.zeros((N, 1))\n    # noise = np.hstack((gap_noise, grp_noise, voltage_noise, global_intensity_noise, sm_noise, time_noise))\n    # noise = np.hstack((gap_noise, grp_noise, voltage_noise, sm_noise, time_noise))\n    noise = np.hstack((gap_noise, voltage_noise, sm_noise, time_noise))\n    data = data + noise\n\n    N_test = int(0.1 * data.shape[0])\n    data_test = data[-N_test:]\n    data = data[0:-N_test]\n    N_validate = int(0.1 * data.shape[0])\n    data_validate = data[-N_validate:]\n    data_train = data[0:-N_validate]\n\n    return data_train, data_validate, data_test\n\n\ndef load_data_normalised():\n\n    data_train, data_validate, data_test = load_data_split_with_noise()\n    data = np.vstack((data_train, data_validate))\n    mu = data.mean(axis=0)\n    s = data.std(axis=0)\n    data_train = (data_train - mu) / s\n    data_validate = (data_validate - mu) / s\n    data_test = (data_test - mu) / s\n\n    return data_train, data_validate, data_test\n"""
datasets/util.py,0,"b'import os\nimport pickle as pickle\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport numpy.random as rng\n\n\ndef isposint(n):\n    """"""\n    Determines whether number n is a positive integer.\n    :param n: number\n    :return: bool\n    """"""\n    return isinstance(n, int) and n > 0\n\n\ndef logistic(x):\n    """"""\n    Elementwise logistic sigmoid.\n    :param x: numpy array\n    :return: numpy array\n    """"""\n    return 1.0 / (1.0 + np.exp(-x))\n\n\ndef logit(x):\n    """"""\n    Elementwise logit (inverse logistic sigmoid).\n    :param x: numpy array\n    :return: numpy array\n    """"""\n    return np.log(x / (1.0 - x))\n\n\ndef disp_imdata(xs, imsize, layout=(1, 1)):\n    """"""\n    Displays an array of images, a page at a time. The user can navigate pages with\n    left and right arrows, start over by pressing space, or close the figure by esc.\n    :param xs: an numpy array with images as rows\n    :param imsize: size of the images\n    :param layout: layout of images in a page\n    :return: none\n    """"""\n\n    num_plots = np.prod(layout)\n    num_xs = xs.shape[0]\n    idx = [0]\n\n    # create a figure with suplots\n    fig, axs = plt.subplots(layout[0], layout[1])\n\n    if isinstance(axs, np.ndarray):\n        axs = axs.flatten()\n    else:\n        axs = [axs]\n\n    for ax in axs:\n        ax.axes.get_xaxis().set_visible(False)\n        ax.axes.get_yaxis().set_visible(False)\n\n    def plot_page():\n        """"""Plots the next page.""""""\n\n        ii = np.arange(idx[0], idx[0] + num_plots) % num_xs\n\n        for ax, i in zip(axs, ii):\n            ax.imshow(xs[i].reshape(imsize), cmap=\'gray\', interpolation=\'none\')\n            ax.set_title(str(i))\n\n        fig.canvas.draw()\n\n    def on_key_event(event):\n        """"""Event handler after key press.""""""\n\n        key = event.key\n\n        if key == \'right\':\n            # show next page\n            idx[0] = (idx[0] + num_plots) % num_xs\n            plot_page()\n\n        elif key == \'left\':\n            # show previous page\n            idx[0] = (idx[0] - num_plots) % num_xs\n            plot_page()\n\n        elif key == \' \':\n            # show first page\n            idx[0] = 0\n            plot_page()\n\n        elif key == \'escape\':\n            # close figure\n            plt.close(fig)\n\n    fig.canvas.mpl_connect(\'key_press_event\', on_key_event)\n    plot_page()\n\n\ndef isdistribution(p):\n    """"""\n    :param p: a vector representing a discrete probability distribution\n    :return: True if p is a valid probability distribution\n    """"""\n    return np.all(p >= 0.0) and np.isclose(np.sum(p), 1.0)\n\n\ndef discrete_sample(p, n_samples=1):\n    """"""\n    Samples from a discrete distribution.\n    :param p: a distribution with N elements\n    :param n_samples: number of samples\n    :return: vector of samples\n    """"""\n\n    # check distribution\n    #assert isdistribution(p), \'Probabilities must be non-negative and sum to one.\'\n\n    # cumulative distribution\n    c = np.cumsum(p[:-1])[np.newaxis, :]\n\n    # get the samples\n    r = rng.rand(n_samples, 1)\n    return np.sum((r > c).astype(int), axis=1)\n\n\ndef ess_importance(ws):\n    """"""\n    Calculates the effective sample size of a set of weighted independent samples (e.g. as given by importance\n    sampling or sequential monte carlo). Takes as input the normalized sample weights.\n    """"""\n\n    ess = 1.0 / np.sum(ws**2)\n    return ess\n\n\ndef ess_mcmc(xs):\n    """"""\n    Calculates the effective sample size of a correlated sequence of samples, e.g. as given by markov chain monte\n    carlo.\n    """"""\n\n    n_samples, n_dim = xs.shape\n\n    mean = np.mean(xs, axis=0)\n    xms = xs - mean\n\n    acors = np.zeros_like(xms)\n    for i in range(n_dim):\n        for lag in range(n_samples):\n            acor = np.sum(xms[:n_samples - lag, i] * xms[lag:, i]) / (\n                n_samples - lag)\n            if acor <= 0.0: break\n            acors[lag, i] = acor\n\n    act = 1.0 + 2.0 * np.sum(acors[1:], axis=0) / acors[0]\n    ess = n_samples / act\n\n    return np.min(ess)\n\n\ndef probs2contours(probs, levels):\n    """"""\n    Takes an array of probabilities and produces an array of contours at specified percentile levels\n    :param probs: probability array. doesn\'t have to sum to 1, but it is assumed it contains all the mass\n    :param levels: percentile levels. have to be in [0.0, 1.0]\n    :return: array of same shape as probs with percentile labels\n    """"""\n\n    # make sure all contour levels are in [0.0, 1.0]\n    levels = np.asarray(levels)\n    assert np.all(levels <= 1.0) and np.all(levels >= 0.0)\n\n    # flatten probability array\n    shape = probs.shape\n    probs = probs.flatten()\n\n    # sort probabilities in descending order\n    idx_sort = probs.argsort()[::-1]\n    idx_unsort = idx_sort.argsort()\n    probs = probs[idx_sort]\n\n    # cumulative probabilities\n    cum_probs = probs.cumsum()\n    cum_probs /= cum_probs[-1]\n\n    # create contours at levels\n    contours = np.ones_like(cum_probs)\n    levels = np.sort(levels)[::-1]\n    for level in levels:\n        contours[cum_probs <= level] = level\n\n    # make sure contours have the order and the shape of the original probability array\n    contours = np.reshape(contours[idx_unsort], shape)\n\n    return contours\n\n\ndef plot_pdf_marginals(pdf, lims, gt=None, levels=(0.68, 0.95)):\n    """"""\n    Plots marginals of a pdf, for each variable and pair of variables.\n    """"""\n\n    if pdf.ndim == 1:\n\n        fig, ax = plt.subplots(1, 1)\n        xx = np.linspace(lims[0], lims[1], 200)\n\n        pp = pdf.eval(xx[:, np.newaxis], log=False)\n        ax.plot(xx, pp)\n        ax.set_xlim(lims)\n        ax.set_ylim([0, ax.get_ylim()[1]])\n        if gt is not None: ax.vlines(gt, 0, ax.get_ylim()[1], color=\'r\')\n\n    else:\n\n        fig, ax = plt.subplots(pdf.ndim, pdf.ndim)\n\n        lims = np.asarray(lims)\n        lims = np.tile(lims, [pdf.ndim, 1]) if lims.ndim == 1 else lims\n\n        for i in range(pdf.ndim):\n            for j in range(pdf.ndim):\n\n                if i == j:\n                    xx = np.linspace(lims[i, 0], lims[i, 1], 500)\n                    pp = pdf.eval(xx, ii=[i], log=False)\n                    ax[i, j].plot(xx, pp)\n                    ax[i, j].set_xlim(lims[i])\n                    ax[i, j].set_ylim([0, ax[i, j].get_ylim()[1]])\n                    if gt is not None:\n                        ax[i, j].vlines(\n                            gt[i], 0, ax[i, j].get_ylim()[1], color=\'r\')\n\n                else:\n                    xx = np.linspace(lims[i, 0], lims[i, 1], 200)\n                    yy = np.linspace(lims[j, 0], lims[j, 1], 200)\n                    X, Y = np.meshgrid(xx, yy)\n                    xy = np.concatenate(\n                        [X.reshape([-1, 1]),\n                         Y.reshape([-1, 1])], axis=1)\n                    pp = pdf.eval(xy, ii=[i, j], log=False)\n                    pp = pp.reshape(list(X.shape))\n                    ax[i, j].contour(X, Y, probs2contours(pp, levels), levels)\n                    ax[i, j].set_xlim(lims[i])\n                    ax[i, j].set_ylim(lims[j])\n                    if gt is not None: ax[i, j].plot(gt[i], gt[j], \'r.\', ms=8)\n\n    plt.show(block=False)\n\n    return fig, ax\n\n\ndef plot_hist_marginals(data, lims=None, gt=None):\n    """"""\n    Plots marginal histograms and pairwise scatter plots of a dataset.\n    """"""\n\n    n_bins = int(np.sqrt(data.shape[0]))\n\n    if data.ndim == 1:\n\n        fig, ax = plt.subplots(1, 1)\n        ax.hist(data, n_bins, normed=True)\n        ax.set_ylim([0, ax.get_ylim()[1]])\n        if lims is not None: ax.set_xlim(lims)\n        if gt is not None: ax.vlines(gt, 0, ax.get_ylim()[1], color=\'r\')\n\n    else:\n\n        n_dim = data.shape[1]\n        fig, ax = plt.subplots(n_dim, n_dim)\n        ax = np.array([[ax]]) if n_dim == 1 else ax\n\n        if lims is not None:\n            lims = np.asarray(lims)\n            lims = np.tile(lims, [n_dim, 1]) if lims.ndim == 1 else lims\n\n        for i in range(n_dim):\n            for j in range(n_dim):\n\n                if i == j:\n                    ax[i, j].hist(data[:, i], n_bins, normed=True)\n                    ax[i, j].set_ylim([0, ax[i, j].get_ylim()[1]])\n                    if lims is not None: ax[i, j].set_xlim(lims[i])\n                    if gt is not None:\n                        ax[i, j].vlines(\n                            gt[i], 0, ax[i, j].get_ylim()[1], color=\'r\')\n\n                else:\n                    ax[i, j].plot(data[:, i], data[:, j], \'k.\', ms=2)\n                    if lims is not None:\n                        ax[i, j].set_xlim(lims[i])\n                        ax[i, j].set_ylim(lims[j])\n                    if gt is not None: ax[i, j].plot(gt[i], gt[j], \'r.\', ms=8)\n\n    plt.show(block=False)\n\n    return fig, ax\n\n\ndef save(data, file):\n    """"""\n    Saves data to a file.\n    """"""\n\n    f = open(file, \'w\')\n    pickle.dump(data, f)\n    f.close()\n\n\ndef load(file):\n    """"""\n    Loads data from file.\n    """"""\n\n    f = open(file, \'r\')\n    data = pickle.load(f)\n    f.close()\n    return data\n\n\ndef calc_whitening_transform(xs):\n    """"""\n    Calculates the parameters that whiten a dataset.\n    """"""\n\n    assert xs.ndim == 2, \'Data must be a matrix\'\n    N = xs.shape[0]\n\n    means = np.mean(xs, axis=0)\n    ys = xs - means\n\n    cov = np.dot(ys.T, ys) / N\n    vars, U = np.linalg.eig(cov)\n    istds = np.sqrt(1.0 / vars)\n\n    return means, U, istds\n\n\ndef whiten(xs, params):\n    """"""\n    Whitens a given dataset using the whitening transform provided.\n    """"""\n\n    means, U, istds = params\n\n    ys = xs.copy()\n    ys -= means\n    ys = np.dot(ys, U)\n    ys *= istds\n\n    return ys\n\n\ndef copy_model_parms(source_model, target_model):\n    """"""\n    Copies the parameters of source_model to target_model.\n    """"""\n\n    for sp, tp in zip(source_model.parms, target_model.parms):\n        tp.set_value(sp.get_value())\n\n\ndef one_hot_encode(labels, n_labels):\n    """"""\n    Transforms numeric labels to 1-hot encoded labels. Assumes numeric labels are in the range 0, 1, ..., n_labels-1.\n    """"""\n\n    assert np.min(labels) >= 0 and np.max(labels) < n_labels\n\n    y = np.zeros([labels.size, n_labels])\n    y[range(labels.size), labels] = 1\n\n    return y\n\n\ndef make_folder(folder):\n    """"""\n    Creates given folder (or path) if it doesn\'t exist.\n    """"""\n\n    if not os.path.exists(folder):\n        os.makedirs(folder)\n'"
