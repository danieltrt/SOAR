file_path,api_count,code
data.py,1,"b'import torch\n\n\ndef get_batch(batch_size):\n    x = torch.randn(batch_size, 10)\n    x = x - 2 * x.pow(2)\n    y = x.sum(1)\n    return x, y\n'"
layer_norm.py,5,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\n\n\nclass LayerNorm1D(nn.Module):\n\n    def __init__(self, num_outputs, eps=1e-5, affine=True):\n        super(LayerNorm1D, self).__init__()\n        self.eps = eps\n        self.weight = nn.Parameter(torch.ones(1, num_outputs))\n        self.bias = nn.Parameter(torch.zeros(1, num_outputs))\n\n    def forward(self, inputs):\n        input_mean = inputs.mean(1,keepdim=True).expand_as(inputs)\n        input_std = inputs.std(1,keepdim=True).expand_as(inputs)\n        x = (inputs - input_mean) / (input_std + self.eps)\n        return x * self.weight.expand_as(x) + self.bias.expand_as(x)\n'"
layer_norm_lstm.py,3,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nfrom layer_norm import LayerNorm1D\n\n\nclass LayerNormLSTMCell(nn.Module):\n\n    def __init__(self, num_inputs, num_hidden, forget_gate_bias=-1):\n        super(LayerNormLSTMCell, self).__init__()\n\n        self.forget_gate_bias = forget_gate_bias\n        self.num_hidden = num_hidden\n        self.fc_i2h = nn.Linear(num_inputs, 4 * num_hidden)\n        self.fc_h2h = nn.Linear(num_hidden, 4 * num_hidden)\n\n        self.ln_i2h = LayerNorm1D(4 * num_hidden)\n        self.ln_h2h = LayerNorm1D(4 * num_hidden)\n\n        self.ln_h2o = LayerNorm1D(num_hidden)\n\n    def forward(self, inputs, state):\n        hx, cx = state\n        i2h = self.fc_i2h(inputs)\n        h2h = self.fc_h2h(hx)\n        x = self.ln_i2h(i2h) + self.ln_h2h(h2h)\n        gates = x.split(self.num_hidden, 1)\n\n        in_gate = F.sigmoid(gates[0])\n        forget_gate = F.sigmoid(gates[1] + self.forget_gate_bias)\n        out_gate = F.sigmoid(gates[2])\n        in_transform = F.tanh(gates[3])\n\n        cx = forget_gate * cx + in_gate * in_transform\n        hx = out_gate * F.tanh(self.ln_h2o(cx))\n        return hx, cx\n'"
main.py,8,"b'import argparse\nimport operator\nimport sys\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom data import get_batch\nfrom meta_optimizer import MetaModel, MetaOptimizer, FastMetaOptimizer\nfrom model import Model\nfrom torch.autograd import Variable\nfrom torchvision import datasets, transforms\n\nparser = argparse.ArgumentParser(description=\'PyTorch REINFORCE example\')\nparser.add_argument(\'--batch_size\', type=int, default=32, metavar=\'N\',\n                    help=\'batch size (default: 32)\')\nparser.add_argument(\'--optimizer_steps\', type=int, default=100, metavar=\'N\',\n                    help=\'number of meta optimizer steps (default: 100)\')\nparser.add_argument(\'--truncated_bptt_step\', type=int, default=20, metavar=\'N\',\n                    help=\'step at which it truncates bptt (default: 20)\')\nparser.add_argument(\'--updates_per_epoch\', type=int, default=10, metavar=\'N\',\n                    help=\'updates per epoch (default: 100)\')\nparser.add_argument(\'--max_epoch\', type=int, default=10000, metavar=\'N\',\n                    help=\'number of epoch (default: 10000)\')\nparser.add_argument(\'--hidden_size\', type=int, default=10, metavar=\'N\',\n                    help=\'hidden size of the meta optimizer (default: 10)\')\nparser.add_argument(\'--num_layers\', type=int, default=2, metavar=\'N\',\n                    help=\'number of LSTM layers (default: 2)\')\nparser.add_argument(\'--no-cuda\', action=\'store_true\', default=False,\n                    help=\'enables CUDA training\')\nargs = parser.parse_args()\nargs.cuda = not args.no_cuda and torch.cuda.is_available()\n\nassert args.optimizer_steps % args.truncated_bptt_step == 0\n\nkwargs = {\'num_workers\': 1, \'pin_memory\': True} if args.cuda else {}\ntrain_loader = torch.utils.data.DataLoader(\n    datasets.MNIST(\'../data\', train=True, download=True,\n                   transform=transforms.Compose([\n                       transforms.ToTensor(),\n                       transforms.Normalize((0.1307,), (0.3081,))\n                   ])),\n    batch_size=args.batch_size, shuffle=True, **kwargs)\ntest_loader = torch.utils.data.DataLoader(\n    datasets.MNIST(\'../data\', train=False, transform=transforms.Compose([\n                       transforms.ToTensor(),\n                       transforms.Normalize((0.1307,), (0.3081,))\n                   ])),\n    batch_size=args.batch_size, shuffle=True, **kwargs)\n\ndef main():\n    # Create a meta optimizer that wraps a model into a meta model\n    # to keep track of the meta updates.\n    meta_model = Model()\n    if args.cuda:\n        meta_model.cuda()\n\n    meta_optimizer = FastMetaOptimizer(MetaModel(meta_model), args.num_layers, args.hidden_size)\n    if args.cuda:\n        meta_optimizer.cuda()\n\n    optimizer = optim.Adam(meta_optimizer.parameters(), lr=1e-3)\n\n    for epoch in range(args.max_epoch):\n        decrease_in_loss = 0.0\n        final_loss = 0.0\n        train_iter = iter(train_loader)\n        for i in range(args.updates_per_epoch):\n\n            # Sample a new model\n            model = Model()\n            if args.cuda:\n                model.cuda()\n\n            x, y = next(train_iter)\n            if args.cuda:\n                x, y = x.cuda(), y.cuda()\n            x, y = Variable(x), Variable(y)\n\n            # Compute initial loss of the model\n            f_x = model(x)\n            initial_loss = F.nll_loss(f_x, y)\n\n            for k in range(args.optimizer_steps // args.truncated_bptt_step):\n                # Keep states for truncated BPTT\n                meta_optimizer.reset_lstm(\n                    keep_states=k > 0, model=model, use_cuda=args.cuda)\n\n                loss_sum = 0\n                prev_loss = torch.zeros(1)\n                if args.cuda:\n                    prev_loss = prev_loss.cuda()\n                for j in range(args.truncated_bptt_step):\n                    x, y = next(train_iter)\n                    if args.cuda:\n                        x, y = x.cuda(), y.cuda()\n                    x, y = Variable(x), Variable(y)\n\n                    # First we need to compute the gradients of the model\n                    f_x = model(x)\n                    loss = F.nll_loss(f_x, y)\n                    model.zero_grad()\n                    loss.backward()\n\n                    # Perfom a meta update using gradients from model\n                    # and return the current meta model saved in the optimizer\n                    meta_model = meta_optimizer.meta_update(model, loss.data)\n\n                    # Compute a loss for a step the meta optimizer\n                    f_x = meta_model(x)\n                    loss = F.nll_loss(f_x, y)\n\n                    loss_sum += (loss - Variable(prev_loss))\n\n                    prev_loss = loss.data\n\n                # Update the parameters of the meta optimizer\n                meta_optimizer.zero_grad()\n                loss_sum.backward()\n                for param in meta_optimizer.parameters():\n                    param.grad.data.clamp_(-1, 1)\n                optimizer.step()\n\n            # Compute relative decrease in the loss function w.r.t initial\n            # value\n            decrease_in_loss += loss.data[0] / initial_loss.data[0]\n            final_loss += loss.data[0]\n\n        print(""Epoch: {}, final loss {}, average final/initial loss ratio: {}"".format(epoch, final_loss / args.updates_per_epoch,\n                                                                       decrease_in_loss / args.updates_per_epoch))\n\nif __name__ == ""__main__"":\n    main()\n'"
meta_optimizer.py,14,"b""from functools import reduce\nfrom operator import mul\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.autograd import Variable\nimport math\nfrom utils import preprocess_gradients\nfrom layer_norm_lstm import LayerNormLSTMCell\nfrom layer_norm import LayerNorm1D\n\nclass MetaOptimizer(nn.Module):\n\n    def __init__(self, model, num_layers, hidden_size):\n        super(MetaOptimizer, self).__init__()\n        self.meta_model = model\n\n        self.hidden_size = hidden_size\n\n        self.linear1 = nn.Linear(3, hidden_size)\n        self.ln1 = LayerNorm1D(hidden_size)\n\n        self.lstms = []\n        for i in range(num_layers):\n            self.lstms.append(LayerNormLSTMCell(hidden_size, hidden_size))\n\n        self.linear2 = nn.Linear(hidden_size, 1)\n        self.linear2.weight.data.mul_(0.1)\n        self.linear2.bias.data.fill_(0.0)\n\n    def cuda(self):\n        super(MetaOptimizer, self).cuda()\n        for i in range(len(self.lstms)):\n            self.lstms[i].cuda()\n\n    def reset_lstm(self, keep_states=False, model=None, use_cuda=False):\n        self.meta_model.reset()\n        self.meta_model.copy_params_from(model)\n\n        if keep_states:\n            for i in range(len(self.lstms)):\n                self.hx[i] = Variable(self.hx[i].data)\n                self.cx[i] = Variable(self.cx[i].data)\n        else:\n            self.hx = []\n            self.cx = []\n            for i in range(len(self.lstms)):\n                self.hx.append(Variable(torch.zeros(1, self.hidden_size)))\n                self.cx.append(Variable(torch.zeros(1, self.hidden_size)))\n                if use_cuda:\n                    self.hx[i], self.cx[i] = self.hx[i].cuda(), self.cx[i].cuda()\n\n    def forward(self, x):\n        # Gradients preprocessing\n        x = F.tanh(self.ln1(self.linear1(x)))\n\n        for i in range(len(self.lstms)):\n            if x.size(0) != self.hx[i].size(0):\n                self.hx[i] = self.hx[i].expand(x.size(0), self.hx[i].size(1))\n                self.cx[i] = self.cx[i].expand(x.size(0), self.cx[i].size(1))\n\n            self.hx[i], self.cx[i] = self.lstms[i](x, (self.hx[i], self.cx[i]))\n            x = self.hx[i]\n\n        x = self.linear2(x)\n        return x.squeeze()\n\n    def meta_update(self, model_with_grads, loss):\n        # First we need to create a flat version of parameters and gradients\n        grads = []\n\n        for module in model_with_grads.children():\n            grads.append(module._parameters['weight'].grad.data.view(-1))\n            grads.append(module._parameters['bias'].grad.data.view(-1))\n\n        flat_params = self.meta_model.get_flat_params()\n        flat_grads = preprocess_gradients(torch.cat(grads))\n\n        inputs = Variable(torch.cat((flat_grads, flat_params.data), 1))\n\n        # Meta update itself\n        flat_params = flat_params + self(inputs)\n\n        self.meta_model.set_flat_params(flat_params)\n\n        # Finally, copy values from the meta model to the normal one.\n        self.meta_model.copy_params_to(model_with_grads)\n        return self.meta_model.model\n\nclass FastMetaOptimizer(nn.Module):\n\n    def __init__(self, model, num_layers, hidden_size):\n        super(FastMetaOptimizer, self).__init__()\n        self.meta_model = model\n\n        self.linear1 = nn.Linear(6, 2)\n        self.linear1.bias.data[0] = 1\n\n    def forward(self, x):\n        # Gradients preprocessing\n        x = F.sigmoid(self.linear1(x))\n        return x.split(1, 1)\n\n    def reset_lstm(self, keep_states=False, model=None, use_cuda=False):\n        self.meta_model.reset()\n        self.meta_model.copy_params_from(model)\n\n        if keep_states:\n            self.f = Variable(self.f.data)\n            self.i = Variable(self.i.data)\n        else:\n            self.f = Variable(torch.zeros(1, 1))\n            self.i = Variable(torch.zeros(1, 1))\n            if use_cuda:\n                self.f = self.f.cuda()\n                self.i = self.i.cuda()\n\n    def meta_update(self, model_with_grads, loss):\n        # First we need to create a flat version of parameters and gradients\n        grads = []\n\n        for module in model_with_grads.children():\n            grads.append(module._parameters['weight'].grad.data.view(-1).unsqueeze(-1))\n            grads.append(module._parameters['bias'].grad.data.view(-1).unsqueeze(-1))\n\n        flat_params = self.meta_model.get_flat_params().unsqueeze(-1)\n        flat_grads = torch.cat(grads)\n\n        self.i = self.i.expand(flat_params.size(0), 1)\n        self.f = self.f.expand(flat_params.size(0), 1)\n\n        loss = loss.expand_as(flat_grads)\n        inputs = Variable(torch.cat((preprocess_gradients(flat_grads), flat_params.data, loss), 1))\n        inputs = torch.cat((inputs, self.f, self.i), 1)\n        self.f, self.i = self(inputs)\n\n        # Meta update itself\n        flat_params = self.f * flat_params - self.i * Variable(flat_grads)\n        flat_params = flat_params.view(-1)\n\n        self.meta_model.set_flat_params(flat_params)\n\n        # Finally, copy values from the meta model to the normal one.\n        self.meta_model.copy_params_to(model_with_grads)\n        return self.meta_model.model\n\n# A helper class that keeps track of meta updates\n# It's done by replacing parameters with variables and applying updates to\n# them.\n\n\nclass MetaModel:\n\n    def __init__(self, model):\n        self.model = model\n\n    def reset(self):\n        for module in self.model.children():\n            module._parameters['weight'] = Variable(\n                module._parameters['weight'].data)\n            module._parameters['bias'] = Variable(\n                module._parameters['bias'].data)\n\n    def get_flat_params(self):\n        params = []\n\n        for module in self.model.children():\n            params.append(module._parameters['weight'].view(-1))\n            params.append(module._parameters['bias'].view(-1))\n\n        return torch.cat(params)\n\n    def set_flat_params(self, flat_params):\n        # Restore original shapes\n        offset = 0\n        for i, module in enumerate(self.model.children()):\n            weight_shape = module._parameters['weight'].size()\n            bias_shape = module._parameters['bias'].size()\n\n            weight_flat_size = reduce(mul, weight_shape, 1)\n            bias_flat_size = reduce(mul, bias_shape, 1)\n\n            module._parameters['weight'] = flat_params[\n                offset:offset + weight_flat_size].view(*weight_shape)\n            module._parameters['bias'] = flat_params[\n                offset + weight_flat_size:offset + weight_flat_size + bias_flat_size].view(*bias_shape)\n\n            offset += weight_flat_size + bias_flat_size\n\n    def copy_params_from(self, model):\n        for modelA, modelB in zip(self.model.parameters(), model.parameters()):\n            modelA.data.copy_(modelB.data)\n\n    def copy_params_to(self, model):\n        for modelA, modelB in zip(self.model.parameters(), model.parameters()):\n            modelB.data.copy_(modelA.data)\n"""
model.py,4,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.autograd import Variable\n\n\nclass Model(nn.Module):\n\n    def __init__(self):\n        super(Model, self).__init__()\n        self.linear1 = nn.Linear(28 * 28, 32)\n        self.linear2 = nn.Linear(32, 10)\n\n    def forward(self, inputs):\n        x = inputs.view(-1, 28 * 28)\n        x = F.relu(self.linear1(x))\n        x = self.linear2(x)\n        return F.log_softmax(x)\n'"
utils.py,1,"b'import math\nimport torch\n\ndef preprocess_gradients(x):\n    p = 10\n    eps = 1e-6\n    indicator = (x.abs() > math.exp(-p)).float()\n    x1 = (x.abs() + eps).log() / p * indicator - (1 - indicator)\n    x2 = x.sign() * indicator + math.exp(p) * x * (1 - indicator)\n\n    return torch.cat((x1, x2), 1)\n'"
