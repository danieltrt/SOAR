file_path,api_count,code
conjugate_gradients.py,5,"b'import torch\n\n\ndef conjugate_gradients(Avp, b, nsteps, residual_tol=1e-10):\n    x = torch.zeros(b.size())\n    r = b - Avp(x)\n    p = r\n    rdotr = torch.dot(r, r)\n\n    for i in range(nsteps):\n        _Avp = Avp(p)\n        alpha = rdotr / torch.dot(p, _Avp)\n        x += alpha * p\n        r -= alpha * _Avp\n        new_rdotr = torch.dot(r, r)\n        betta = new_rdotr / rdotr\n        p = r + betta * p\n        rdotr = new_rdotr\n        if rdotr < residual_tol:\n            break\n    return x\n\n\ndef flat_grad_from(net, grad_grad=False):\n    grads = []\n    for param in net.parameters():\n        if grad_grad:\n            grads.append(param.grad.grad.view(-1))\n        else:\n            grads.append(param.grad.view(-1))\n\n    flat_grad = torch.cat(grads)\n    return flat_grad\n'"
main.py,18,"b'import argparse\nfrom itertools import count\n\nimport gym\nimport scipy.optimize\n\nimport torch\nfrom models import *\nfrom replay_memory import Memory\nfrom running_state import ZFilter\nfrom torch.autograd import Variable\nfrom trpo import trpo_step\nfrom utils import *\n\ntorch.utils.backcompat.broadcast_warning.enabled = True\ntorch.utils.backcompat.keepdim_warning.enabled = True\n\ntorch.set_default_tensor_type(\'torch.DoubleTensor\')\n\nparser = argparse.ArgumentParser(description=\'PyTorch actor-critic example\')\nparser.add_argument(\'--gamma\', type=float, default=0.995, metavar=\'G\',\n                    help=\'discount factor (default: 0.995)\')\nparser.add_argument(\'--env-name\', default=""Reacher-v1"", metavar=\'G\',\n                    help=\'name of the environment to run\')\nparser.add_argument(\'--tau\', type=float, default=0.97, metavar=\'G\',\n                    help=\'gae (default: 0.97)\')\nparser.add_argument(\'--l2-reg\', type=float, default=1e-3, metavar=\'G\',\n                    help=\'l2 regularization regression (default: 1e-3)\')\nparser.add_argument(\'--max-kl\', type=float, default=1e-2, metavar=\'G\',\n                    help=\'max kl value (default: 1e-2)\')\nparser.add_argument(\'--damping\', type=float, default=1e-1, metavar=\'G\',\n                    help=\'damping (default: 1e-1)\')\nparser.add_argument(\'--seed\', type=int, default=543, metavar=\'N\',\n                    help=\'random seed (default: 1)\')\nparser.add_argument(\'--batch-size\', type=int, default=15000, metavar=\'N\',\n                    help=\'random seed (default: 1)\')\nparser.add_argument(\'--render\', action=\'store_true\',\n                    help=\'render the environment\')\nparser.add_argument(\'--log-interval\', type=int, default=1, metavar=\'N\',\n                    help=\'interval between training status logs (default: 10)\')\nargs = parser.parse_args()\n\nenv = gym.make(args.env_name)\n\nnum_inputs = env.observation_space.shape[0]\nnum_actions = env.action_space.shape[0]\n\nenv.seed(args.seed)\ntorch.manual_seed(args.seed)\n\npolicy_net = Policy(num_inputs, num_actions)\nvalue_net = Value(num_inputs)\n\ndef select_action(state):\n    state = torch.from_numpy(state).unsqueeze(0)\n    action_mean, _, action_std = policy_net(Variable(state))\n    action = torch.normal(action_mean, action_std)\n    return action\n\ndef update_params(batch):\n    rewards = torch.Tensor(batch.reward)\n    masks = torch.Tensor(batch.mask)\n    actions = torch.Tensor(np.concatenate(batch.action, 0))\n    states = torch.Tensor(batch.state)\n    values = value_net(Variable(states))\n\n    returns = torch.Tensor(actions.size(0),1)\n    deltas = torch.Tensor(actions.size(0),1)\n    advantages = torch.Tensor(actions.size(0),1)\n\n    prev_return = 0\n    prev_value = 0\n    prev_advantage = 0\n    for i in reversed(range(rewards.size(0))):\n        returns[i] = rewards[i] + args.gamma * prev_return * masks[i]\n        deltas[i] = rewards[i] + args.gamma * prev_value * masks[i] - values.data[i]\n        advantages[i] = deltas[i] + args.gamma * args.tau * prev_advantage * masks[i]\n\n        prev_return = returns[i, 0]\n        prev_value = values.data[i, 0]\n        prev_advantage = advantages[i, 0]\n\n    targets = Variable(returns)\n\n    # Original code uses the same LBFGS to optimize the value loss\n    def get_value_loss(flat_params):\n        set_flat_params_to(value_net, torch.Tensor(flat_params))\n        for param in value_net.parameters():\n            if param.grad is not None:\n                param.grad.data.fill_(0)\n\n        values_ = value_net(Variable(states))\n\n        value_loss = (values_ - targets).pow(2).mean()\n\n        # weight decay\n        for param in value_net.parameters():\n            value_loss += param.pow(2).sum() * args.l2_reg\n        value_loss.backward()\n        return (value_loss.data.double().numpy(), get_flat_grad_from(value_net).data.double().numpy())\n\n    flat_params, _, opt_info = scipy.optimize.fmin_l_bfgs_b(get_value_loss, get_flat_params_from(value_net).double().numpy(), maxiter=25)\n    set_flat_params_to(value_net, torch.Tensor(flat_params))\n\n    advantages = (advantages - advantages.mean()) / advantages.std()\n\n    action_means, action_log_stds, action_stds = policy_net(Variable(states))\n    fixed_log_prob = normal_log_density(Variable(actions), action_means, action_log_stds, action_stds).data.clone()\n\n    def get_loss(volatile=False):\n        if volatile:\n            with torch.no_grad():\n                action_means, action_log_stds, action_stds = policy_net(Variable(states))\n        else:\n            action_means, action_log_stds, action_stds = policy_net(Variable(states))\n                \n        log_prob = normal_log_density(Variable(actions), action_means, action_log_stds, action_stds)\n        action_loss = -Variable(advantages) * torch.exp(log_prob - Variable(fixed_log_prob))\n        return action_loss.mean()\n\n\n    def get_kl():\n        mean1, log_std1, std1 = policy_net(Variable(states))\n\n        mean0 = Variable(mean1.data)\n        log_std0 = Variable(log_std1.data)\n        std0 = Variable(std1.data)\n        kl = log_std1 - log_std0 + (std0.pow(2) + (mean0 - mean1).pow(2)) / (2.0 * std1.pow(2)) - 0.5\n        return kl.sum(1, keepdim=True)\n\n    trpo_step(policy_net, get_loss, get_kl, args.max_kl, args.damping)\n\nrunning_state = ZFilter((num_inputs,), clip=5)\nrunning_reward = ZFilter((1,), demean=False, clip=10)\n\nfor i_episode in count(1):\n    memory = Memory()\n\n    num_steps = 0\n    reward_batch = 0\n    num_episodes = 0\n    while num_steps < args.batch_size:\n        state = env.reset()\n        state = running_state(state)\n\n        reward_sum = 0\n        for t in range(10000): # Don\'t infinite loop while learning\n            action = select_action(state)\n            action = action.data[0].numpy()\n            next_state, reward, done, _ = env.step(action)\n            reward_sum += reward\n\n            next_state = running_state(next_state)\n\n            mask = 1\n            if done:\n                mask = 0\n\n            memory.push(state, np.array([action]), mask, next_state, reward)\n\n            if args.render:\n                env.render()\n            if done:\n                break\n\n            state = next_state\n        num_steps += (t-1)\n        num_episodes += 1\n        reward_batch += reward_sum\n\n    reward_batch /= num_episodes\n    batch = memory.sample()\n    update_params(batch)\n\n    if i_episode % args.log_interval == 0:\n        print(\'Episode {}\\tLast reward: {}\\tAverage reward {:.2f}\'.format(\n            i_episode, reward_sum, reward_batch))\n'"
models.py,8,"b'import torch\nimport torch.autograd as autograd\nimport torch.nn as nn\n\n\nclass Policy(nn.Module):\n    def __init__(self, num_inputs, num_outputs):\n        super(Policy, self).__init__()\n        self.affine1 = nn.Linear(num_inputs, 64)\n        self.affine2 = nn.Linear(64, 64)\n\n        self.action_mean = nn.Linear(64, num_outputs)\n        self.action_mean.weight.data.mul_(0.1)\n        self.action_mean.bias.data.mul_(0.0)\n\n        self.action_log_std = nn.Parameter(torch.zeros(1, num_outputs))\n\n        self.saved_actions = []\n        self.rewards = []\n        self.final_value = 0\n\n    def forward(self, x):\n        x = torch.tanh(self.affine1(x))\n        x = torch.tanh(self.affine2(x))\n\n        action_mean = self.action_mean(x)\n        action_log_std = self.action_log_std.expand_as(action_mean)\n        action_std = torch.exp(action_log_std)\n\n        return action_mean, action_log_std, action_std\n\n\nclass Value(nn.Module):\n    def __init__(self, num_inputs):\n        super(Value, self).__init__()\n        self.affine1 = nn.Linear(num_inputs, 64)\n        self.affine2 = nn.Linear(64, 64)\n        self.value_head = nn.Linear(64, 1)\n        self.value_head.weight.data.mul_(0.1)\n        self.value_head.bias.data.mul_(0.0)\n\n    def forward(self, x):\n        x = torch.tanh(self.affine1(x))\n        x = torch.tanh(self.affine2(x))\n\n        state_values = self.value_head(x)\n        return state_values\n'"
replay_memory.py,0,"b'import random\nfrom collections import namedtuple\n\n# Taken from\n# https://github.com/pytorch/tutorials/blob/master/Reinforcement%20(Q-)Learning%20with%20PyTorch.ipynb\n\nTransition = namedtuple(\'Transition\', (\'state\', \'action\', \'mask\', \'next_state\',\n                                       \'reward\'))\n\n\nclass Memory(object):\n    def __init__(self):\n        self.memory = []\n\n    def push(self, *args):\n        """"""Saves a transition.""""""\n        self.memory.append(Transition(*args))\n\n    def sample(self):\n        return Transition(*zip(*self.memory))\n\n    def __len__(self):\n        return len(self.memory)\n'"
running_state.py,0,"b'from collections import deque\n\nimport numpy as np\n\n\n# from https://github.com/joschu/modular_rl\n# http://www.johndcook.com/blog/standard_deviation/\nclass RunningStat(object):\n    def __init__(self, shape):\n        self._n = 0\n        self._M = np.zeros(shape)\n        self._S = np.zeros(shape)\n\n    def push(self, x):\n        x = np.asarray(x)\n        assert x.shape == self._M.shape\n        self._n += 1\n        if self._n == 1:\n            self._M[...] = x\n        else:\n            oldM = self._M.copy()\n            self._M[...] = oldM + (x - oldM) / self._n\n            self._S[...] = self._S + (x - oldM) * (x - self._M)\n\n    @property\n    def n(self):\n        return self._n\n\n    @property\n    def mean(self):\n        return self._M\n\n    @property\n    def var(self):\n        return self._S / (self._n - 1) if self._n > 1 else np.square(self._M)\n\n    @property\n    def std(self):\n        return np.sqrt(self.var)\n\n    @property\n    def shape(self):\n        return self._M.shape\n\n\nclass ZFilter:\n    """"""\n    y = (x-mean)/std\n    using running estimates of mean,std\n    """"""\n\n    def __init__(self, shape, demean=True, destd=True, clip=10.0):\n        self.demean = demean\n        self.destd = destd\n        self.clip = clip\n\n        self.rs = RunningStat(shape)\n\n    def __call__(self, x, update=True):\n        if update: self.rs.push(x)\n        if self.demean:\n            x = x - self.rs.mean\n        if self.destd:\n            x = x / (self.rs.std + 1e-8)\n        if self.clip:\n            x = np.clip(x, -self.clip, self.clip)\n        return x\n\n    def output_shape(self, input_space):\n        return input_space.shape\n'"
trpo.py,12,"b'import numpy as np\n\nimport torch\nfrom torch.autograd import Variable\nfrom utils import *\n\n\ndef conjugate_gradients(Avp, b, nsteps, residual_tol=1e-10):\n    x = torch.zeros(b.size())\n    r = b.clone()\n    p = b.clone()\n    rdotr = torch.dot(r, r)\n    for i in range(nsteps):\n        _Avp = Avp(p)\n        alpha = rdotr / torch.dot(p, _Avp)\n        x += alpha * p\n        r -= alpha * _Avp\n        new_rdotr = torch.dot(r, r)\n        betta = new_rdotr / rdotr\n        p = r + betta * p\n        rdotr = new_rdotr\n        if rdotr < residual_tol:\n            break\n    return x\n\n\ndef linesearch(model,\n               f,\n               x,\n               fullstep,\n               expected_improve_rate,\n               max_backtracks=10,\n               accept_ratio=.1):\n    fval = f(True).data\n    print(""fval before"", fval.item())\n    for (_n_backtracks, stepfrac) in enumerate(.5**np.arange(max_backtracks)):\n        xnew = x + stepfrac * fullstep\n        set_flat_params_to(model, xnew)\n        newfval = f(True).data\n        actual_improve = fval - newfval\n        expected_improve = expected_improve_rate * stepfrac\n        ratio = actual_improve / expected_improve\n        print(""a/e/r"", actual_improve.item(), expected_improve.item(), ratio.item())\n\n        if ratio.item() > accept_ratio and actual_improve.item() > 0:\n            print(""fval after"", newfval.item())\n            return True, xnew\n    return False, x\n\n\ndef trpo_step(model, get_loss, get_kl, max_kl, damping):\n    loss = get_loss()\n    grads = torch.autograd.grad(loss, model.parameters())\n    loss_grad = torch.cat([grad.view(-1) for grad in grads]).data\n\n    def Fvp(v):\n        kl = get_kl()\n        kl = kl.mean()\n\n        grads = torch.autograd.grad(kl, model.parameters(), create_graph=True)\n        flat_grad_kl = torch.cat([grad.view(-1) for grad in grads])\n\n        kl_v = (flat_grad_kl * Variable(v)).sum()\n        grads = torch.autograd.grad(kl_v, model.parameters())\n        flat_grad_grad_kl = torch.cat([grad.contiguous().view(-1) for grad in grads]).data\n\n        return flat_grad_grad_kl + v * damping\n\n    stepdir = conjugate_gradients(Fvp, -loss_grad, 10)\n\n    shs = 0.5 * (stepdir * Fvp(stepdir)).sum(0, keepdim=True)\n\n    lm = torch.sqrt(shs / max_kl)\n    fullstep = stepdir / lm[0]\n\n    neggdotstepdir = (-loss_grad * stepdir).sum(0, keepdim=True)\n    print((""lagrange multiplier:"", lm[0], ""grad_norm:"", loss_grad.norm()))\n\n    prev_params = get_flat_params_from(model)\n    success, new_params = linesearch(model, get_loss, prev_params, fullstep,\n                                     neggdotstepdir / lm[0])\n    set_flat_params_to(model, new_params)\n\n    return loss\n'"
utils.py,3,"b'import math\n\nimport numpy as np\n\nimport torch\n\n\ndef normal_entropy(std):\n    var = std.pow(2)\n    entropy = 0.5 + 0.5 * torch.log(2 * var * math.pi)\n    return entropy.sum(1, keepdim=True)\n\n\ndef normal_log_density(x, mean, log_std, std):\n    var = std.pow(2)\n    log_density = -(x - mean).pow(2) / (\n        2 * var) - 0.5 * math.log(2 * math.pi) - log_std\n    return log_density.sum(1, keepdim=True)\n\n\ndef get_flat_params_from(model):\n    params = []\n    for param in model.parameters():\n        params.append(param.data.view(-1))\n\n    flat_params = torch.cat(params)\n    return flat_params\n\n\ndef set_flat_params_to(model, flat_params):\n    prev_ind = 0\n    for param in model.parameters():\n        flat_size = int(np.prod(list(param.size())))\n        param.data.copy_(\n            flat_params[prev_ind:prev_ind + flat_size].view(param.size()))\n        prev_ind += flat_size\n\n\ndef get_flat_grad_from(net, grad_grad=False):\n    grads = []\n    for param in net.parameters():\n        if grad_grad:\n            grads.append(param.grad.grad.view(-1))\n        else:\n            grads.append(param.grad.view(-1))\n\n    flat_grad = torch.cat(grads)\n    return flat_grad\n'"
