file_path,api_count,code
setup.py,0,"b""from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport setuptools\n\n_VERSION = '0.3.2'\n\nREQUIRED_PACKAGES = [\n]\n\nDEPENDENCY_LINKS = [\n]\n\nsetuptools.setup(\n    name='warmup_scheduler',\n    version=_VERSION,\n    description='Gradually Warm-up LR Scheduler for Pytorch',\n    install_requires=REQUIRED_PACKAGES,\n    dependency_links=DEPENDENCY_LINKS,\n    url='https://github.com/ildoonet/pytorch-gradual-warmup-lr',\n    license='MIT License',\n    package_dir={},\n    packages=setuptools.find_packages(exclude=['tests']),\n)\n"""
warmup_scheduler/__init__.py,0,"b""\nfrom warmup_scheduler.scheduler import GradualWarmupScheduler\n\n\n__version__ = '0.3.2'\n"""
warmup_scheduler/run.py,3,"b""import torch\nfrom torch.optim.lr_scheduler import StepLR, ExponentialLR\nfrom torch.optim.sgd import SGD\n\nfrom warmup_scheduler import GradualWarmupScheduler\n\n\nif __name__ == '__main__':\n    model = [torch.nn.Parameter(torch.randn(2, 2, requires_grad=True))]\n    optim = SGD(model, 0.1)\n\n    # scheduler_warmup is chained with schduler_steplr\n    scheduler_steplr = StepLR(optim, step_size=10, gamma=0.1)\n    scheduler_warmup = GradualWarmupScheduler(optim, multiplier=1, total_epoch=5, after_scheduler=scheduler_steplr)\n\n    # this zero gradient update is needed to avoid a warning message, issue #8.\n    optim.zero_grad()\n    optim.step()\n\n    for epoch in range(1, 20):\n        scheduler_warmup.step(epoch)\n        print(epoch, optim.param_groups[0]['lr'])\n\n        optim.step()    # backward pass (update network)\n"""
warmup_scheduler/scheduler.py,2,"b'from torch.optim.lr_scheduler import _LRScheduler\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\n\n\nclass GradualWarmupScheduler(_LRScheduler):\n    """""" Gradually warm-up(increasing) learning rate in optimizer.\n    Proposed in \'Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour\'.\n\n    Args:\n        optimizer (Optimizer): Wrapped optimizer.\n        multiplier: target learning rate = base lr * multiplier if multiplier > 1.0. if multiplier = 1.0, lr starts from 0 and ends up with the base_lr.\n        total_epoch: target learning rate is reached at total_epoch, gradually\n        after_scheduler: after target_epoch, use this scheduler(eg. ReduceLROnPlateau)\n    """"""\n\n    def __init__(self, optimizer, multiplier, total_epoch, after_scheduler=None):\n        self.multiplier = multiplier\n        if self.multiplier < 1.:\n            raise ValueError(\'multiplier should be greater thant or equal to 1.\')\n        self.total_epoch = total_epoch\n        self.after_scheduler = after_scheduler\n        self.finished = False\n        super(GradualWarmupScheduler, self).__init__(optimizer)\n\n    def get_lr(self):\n        if self.last_epoch > self.total_epoch:\n            if self.after_scheduler:\n                if not self.finished:\n                    self.after_scheduler.base_lrs = [base_lr * self.multiplier for base_lr in self.base_lrs]\n                    self.finished = True\n                return self.after_scheduler.get_last_lr()\n            return [base_lr * self.multiplier for base_lr in self.base_lrs]\n\n        if self.multiplier == 1.0:\n            return [base_lr * (float(self.last_epoch) / self.total_epoch) for base_lr in self.base_lrs]\n        else:\n            return [base_lr * ((self.multiplier - 1.) * self.last_epoch / self.total_epoch + 1.) for base_lr in self.base_lrs]\n\n    def step_ReduceLROnPlateau(self, metrics, epoch=None):\n        if epoch is None:\n            epoch = self.last_epoch + 1\n        self.last_epoch = epoch if epoch != 0 else 1  # ReduceLROnPlateau is called at the end of epoch, whereas others are called at beginning\n        if self.last_epoch <= self.total_epoch:\n            warmup_lr = [base_lr * ((self.multiplier - 1.) * self.last_epoch / self.total_epoch + 1.) for base_lr in self.base_lrs]\n            for param_group, lr in zip(self.optimizer.param_groups, warmup_lr):\n                param_group[\'lr\'] = lr\n        else:\n            if epoch is None:\n                self.after_scheduler.step(metrics, None)\n            else:\n                self.after_scheduler.step(metrics, epoch - self.total_epoch)\n\n    def step(self, epoch=None, metrics=None):\n        if type(self.after_scheduler) != ReduceLROnPlateau:\n            if self.finished and self.after_scheduler:\n                if epoch is None:\n                    self.after_scheduler.step(None)\n                else:\n                    self.after_scheduler.step(epoch - self.total_epoch)\n                self._last_lr = self.after_scheduler.get_last_lr()\n            else:\n                return super(GradualWarmupScheduler, self).step(epoch)\n        else:\n            self.step_ReduceLROnPlateau(metrics, epoch)\n'"
