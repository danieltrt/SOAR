file_path,api_count,code
keras_code/A_Dist_Feat_CAMs_Extraction.py,0,"b'from scipy.misc import imread\nimport time\nimport sys\nimport h5py\nimport numpy as np\nfrom cam_utils import extract_feat_cam, extract_feat_cam_all\nfrom vgg_cam import vggcam\nfrom utils import create_folders, save_data, preprocess_images\nfrom pooling_functions import weighted_cam_pooling\n\n\n# Dataset Selection\ndataset = \'distractors100k\'\n\n# Extract Offline\naggregation_type = \'Offline\'\n\n# Image Pre-processing (Size W x H)\n\n# Horizontal Images\nsize_h = [1024, 720]\n# Vertical Images\nsize_v = [720, 1024]\n\ndim = \'1024x720\'\n\n# Mean to substract\nmean_data = \'Imagenet\'\n\n# Model Selection\nmodel_name = \'Vgg_16_CAM\'\n\nif mean_data == \'Places\':\n    mean_value = [104, 166.66, 122.67]\n    folder = \'places/\'\nelif mean_data == \'Imagenet\':\n    mean_value = [123.68, 116.779, 103.939]\n    folder = \'imagenet/\'\nelse:\n    mean_value = [0, 0, 0]\n\n# Model Selection: VGG_CAM\nif model_name == \'Vgg_16_CAM\':\n    nb_classes = 1000\n    VGGCAM_weight_path = \'../models/vgg_cam_weights.h5\'\n    layer = \'relu5_1\'\n    dim_descriptor = 512\n    model = vggcam(nb_classes)\n    model.load_weights(VGGCAM_weight_path)\n    model.summary()\n\n# CAM Extraction\n\nif aggregation_type == \'Offline\':\n    num_classes = 64\nelif aggregation_type == \'Online\':\n    num_classes = 1000\n\n# Images to load into the net (+ images, + memory, + fast)\nbatch_size = 6\n# Images to pre-load (+ images, + memory, + fast) (also saves feats & CAMs for this number when saving-CAMs)\nimage_batch_size = 1000\n# Dimension of h5 files (+ images, + memory)\ndescriptors_batch_size = 10000\nchunk_index = 0\n\n# For saving also features & CAMs\nsaving_CAMs = False\nind = 0\n\nif dataset == \'distractors100k\':\n    n_img_dataset = 100070\n    train_list_path_h = ""../lists/list_oxford105k_horizontal.txt""\n    train_list_path_v = ""../lists/list_oxford105k_vertical.txt""\n    path_descriptors = \'/data/jim011/distractors100k/descriptors/\' + model_name + \'/\' + layer + \'/\' + dim + \'/\'\n    descriptors_cams_path_wp = path_descriptors + \'distractor_all_\' + str(num_classes) + \'_wp\'\n    descriptors_cams_path_mp = path_descriptors + \'distractor_all_\' + str(num_classes) + \'_mp\'\n    create_folders(path_descriptors)\n\n    # If you want to save features & CAMs\n    # feature_path = \'/imatge/ajimenez/work/ITR/distractors100k/features/\' + model_name + \'/\' + layer + \'/\' + dim + \'/\'\n    # cam_path = \'/imatge/ajimenez/work/ITR/distractors100k/cams/\' + model_name + \'/\' + layer + \'/\' + dim + \'/\'\n    # create_folders(feature_path)\n    # create_folders(cam_path)\n\n\ndef extract_cam_descriptors(model, batch_size, num_classes, size, mean_value, image_train_list_path, desc_wp, chunk_index, ind=0):\n    images = [0] * image_batch_size\n    image_names = [0] * image_batch_size\n    counter = 0\n    desc_count = 0\n    num_images = 0\n    t0 = time.time()\n\n    print \'Horizontal size: \', size[0]\n    print \'Vertical size: \', size[1]\n\n    for line in open(image_train_list_path):\n        if counter >= image_batch_size:\n            print \'Processing image batch: \', ind\n            t1 = time.time()\n            data = preprocess_images(images, size[0], size[1], mean_value)\n\n            if aggregation_type == \'Offline\':\n                features, cams, cl = \\\n                    extract_feat_cam(model, layer, batch_size, data, num_classes)\n                if saving_CAMs:\n                    save_data(cams, cam_path, \'cams_\' + str(ind) + \'.h5\')\n                    save_data(features, feature_path, \'features_\' + str(ind) + \'.h5\')\n                d_wp = weighted_cam_pooling(features, cams)\n                desc_wp = np.concatenate((desc_wp, d_wp))\n\n            print \'Image batch processed, CAMs descriptors obtained!\'\n            print \'Time elapsed: \', time.time()-t1\n            sys.stdout.flush()\n            counter = 0\n            desc_count += image_batch_size\n            if descriptors_batch_size == desc_count and aggregation_type == \'Offline\':\n                print \'Saving ...\' + descriptors_cams_path_wp + \'_\' + str(chunk_index)+\'.h5\'\n                save_data(desc_wp, descriptors_cams_path_wp + \'_\' + str(chunk_index)+\'.h5\', \'\')\n                desc_count = 0\n                chunk_index += 1\n                desc_wp = np.zeros((0, dim_descriptor), dtype=np.float32)\n            ind += 1\n\n        line = line.rstrip(\'\\n\')\n        images[counter] = imread(line)\n        image_names[counter] = line\n        counter += 1\n        num_images += 1\n\n    #Last batch\n    print \'Last Batch:\'\n    data = np.zeros((counter, 3, size[1], size[0]), dtype=np.float32)\n    data[0:] = preprocess_images(images[0:counter], size[0], size[1], mean_value)\n\n    if aggregation_type == \'Offline\':\n        features, cams, cl = extract_feat_cam(model, layer, batch_size, data, num_classes)\n        if saving_CAMs:\n            save_data(cams, cam_path, \'cams_\' + str(ind) + \'.h5\')\n            save_data(features, feature_path, \'features_\' + str(ind) + \'.h5\')\n        d_wp = weighted_cam_pooling(features, cams)\n        desc_wp = np.concatenate((desc_wp, d_wp))\n        save_data(desc_wp, descriptors_cams_path_wp + \'_\' + str(chunk_index) + \'.h5\', \'\')\n        chunk_index += 1\n        desc_wp = np.zeros((0, dim_descriptor), dtype=np.float32)\n\n    ind += 1\n    print desc_wp.shape\n    print \'Batch processed, CAMs descriptors obtained!\'\n    print \'Total time elapsed: \', time.time() - t0\n    sys.stdout.flush()\n\n    return desc_wp, chunk_index\n\n\n########################################################################################################################\n# Main Script\n\nprint \'Num classes: \', num_classes\nprint \'Mean: \', mean_value\n\nt_0 = time.time()\ndesc_wp = np.zeros((0, dim_descriptor), dtype=np.float32)\n\n# Horizontal Images\ndesc_wp, c_ind = \\\n    extract_cam_descriptors(model, batch_size, num_classes, size_h, mean_value, train_list_path_h, desc_wp, chunk_index)\n\n# Vertical Images\ndesc_wp, c_ind = \\\n    extract_cam_descriptors(model, batch_size, num_classes, size_v, mean_value, train_list_path_v, desc_wp, c_ind, ind)\n\n\nprint \'Data Saved\'\nprint \'Total time elapsed: \', time.time() - t_0\n'"
keras_code/A_Oxf_Par_Feat_CAMs_Extraction.py,0,"b'from scipy.misc import imread\nimport sys\nimport getopt\nimport time\nimport h5py\nimport numpy as np\nfrom cam_utils import extract_feat_cam, extract_feat_cam_all\nfrom vgg_cam import vggcam\nfrom utils import create_folders, save_data, preprocess_images\nfrom pooling_functions import weighted_cam_pooling, sum_pooling\n\n\n# Instructions Arguments: python script.py -d \'Oxford/Paris\' -a \'Offline/Online\'\ntry:\n    opts, args = getopt.getopt(sys.argv[1:], ""a:d:"")\n    flag_d = False\n    flag_a = False\nexcept getopt.GetoptError:\n    print \'script.py -d <dataset> -a <aggregation>\'\n    sys.exit(2)\nfor opt, arg in opts:\n    if opt == \'-d\':\n        if arg == \'Oxford\' or arg == \'Paris\':\n            dataset = arg\n            flag_d = True\n    elif opt == ""-a"":\n        if arg == \'Offline\' or arg == \'Online\':\n            aggregation_type = arg\n            flag_a = True\n\n# Dataset Selection (Oxford/Paris) - Default\nif not flag_d:\n    dataset = \'Oxford\'\n    print \'Default dataset: \', dataset\n\n# Extract Online or Offline (Online saves 1 file/image) - Default\nif not flag_a:\n    aggregation_type = \'Offline\'\n    print \'Default aggregation: \', aggregation_type\n\n# Image Pre-processing (Size W x H)\n\n# Horizontal Images\nsize_h = [1024, 720]\n# Vertical Images\nsize_v = [720, 1024]\n\ndim = \'1024x720\'\n\n# Mean to substract\nmean_data = \'Imagenet\'\n\n# Model Selection\nmodel_name = \'Vgg_16_CAM\'\n\nif mean_data == \'Places\':\n    mean_value = [104, 166.66, 122.67]\n    folder = \'places/\'\nelif mean_data == \'Imagenet\':\n    mean_value = [123.68, 116.779, 103.939]\n    folder = \'imagenet/\'\nelse:\n    mean_value = [0, 0, 0]\n\n# Model Selection: VGG_CAM\nif model_name == \'Vgg_16_CAM\':\n    nb_classes = 1000\n    VGGCAM_weight_path = \'../models/vgg_cam_weights.h5\'\n    layer = \'relu5_1\'\n    dim_descriptor = 512\n    model = vggcam(nb_classes)\n    model.load_weights(VGGCAM_weight_path)\n    model.summary()\n\n# CAM Extraction (# CAMs)\n\nif aggregation_type == \'Offline\':\n    num_classes = 1\nelif aggregation_type == \'Online\':\n    num_classes = 1000\n\n# Images to load into the net (+ images, + memory, + fast)\nbatch_size = 6\n# Images to pre-load (+ images, + memory, + fast) (also saves feats & CAMs for this number when saving-CAMs)\nimage_batch_size = 500\n\n# For saving also features & CAMs\nsaving_CAMs = False\n# Index for saving chunks\nind = 0\n\n\nif dataset == \'Oxford\':\n    n_img_dataset = 5063\n    train_list_path_h = ""../lists/list_oxford_horizontal_no_queries.txt""\n    train_list_path_v = ""../lists/list_oxford_vertical_no_queries.txt""\n    path_descriptors = \'/data/jim011/oxford/descriptors/\' + model_name + \'/\' + layer + \'/\' + dim + \'/\'\n    descriptors_cams_path_wp = path_descriptors + \'oxford_all_\' + str(num_classes) + \'_wp.h5\'\n    descriptors_cams_path_sp = path_descriptors + \'oxford_all_\' + \'_sp.h5\'\n    descriptors_cams_path_sp_c = path_descriptors + \'oxford_all_\' + \'_sp_c.h5\'\n    path_images_oxford = \'/data/jim011/datasets_retrieval/Oxford5k/images/\'\n\n    if aggregation_type ==\'Online\':\n        path_descriptors += \'online/\'\n    # If you want to save features & CAMs\n    #feature_path = \'/imatge/ajimenez/work/ITR/oxford/features/\' + model_name + \'/\' + layer + \'/\' + dim + \'/\'\n    #cam_path = \'/imatge/ajimenez/work/ITR/oxford/cams/\' + model_name + \'/\' + layer + \'/\' + dim + \'/\'\n\n    create_folders(path_descriptors)\n    #create_folders(feature_path)\n    #create_folders(cam_path)\n\nif dataset == \'Paris\':\n    n_img_dataset = 6392\n    train_list_path_h = ""../lists/list_paris_horizontal_no_queries.txt""\n    train_list_path_v = ""../lists/list_paris_vertical_no_queries.txt""\n    path_descriptors = \'/data/jim011/paris/descriptors/\' + model_name + \'/\' + layer + \'/\' + dim + \'/\'\n    descriptors_cams_path_wp = path_descriptors + \'paris_all_\' + str(num_classes) + \'_wp.h5\'\n    descriptors_cams_path_sp = path_descriptors + \'paris_all_\' + \'_sp.h5\'\n    descriptors_cams_path_sp_c = path_descriptors + \'paris_all_\' + \'_sp_c.h5\'\n    path_images_paris = \'/data/jim011/datasets_retrieval/Paris6k/images/\'\n\n    if aggregation_type ==\'Online\':\n        path_descriptors += \'online/\'\n\n    create_folders(path_descriptors)\n\n    # If you want to save features & CAMs\n    #feature_path = \'/imatge/ajimenez/work/ITR/paris/features/\' + model_name + \'/\' + layer + \'/\' + dim + \'/\'\n    #cam_path = \'/imatge/ajimenez/work/ITR/paris/cams/\' + model_name + \'/\' + layer + \'/\' + dim + \'/\'\n    #create_folders(feature_path)\n    #create_folders(cam_path)\n\n\ndef extract_cam_descriptors(model, batch_size, num_classes, size, mean_value, image_train_list_path,\n                            desc_wp, desc_sp, desc_sp_c, ind=0):\n    images = [0] * image_batch_size\n    image_names = [0] * image_batch_size\n    counter = 0\n    num_images = 0\n    t0 = time.time()\n\n    print \'Horizontal size: \', size[0]\n    print \'Vertical size: \', size[1]\n\n    for line in open(image_train_list_path):\n        if counter >= image_batch_size:\n            print \'Processing image batch: \', ind\n            t1 = time.time()\n            data = preprocess_images(images, size[0], size[1], mean_value)\n\n            if aggregation_type == \'Offline\':\n                features, cams, cl = \\\n                    extract_feat_cam(model, layer, batch_size, data, num_classes)\n                if saving_CAMs:\n                    save_data(cams, cam_path, \'cams_\' + str(ind) + \'.h5\')\n                    save_data(features, feature_path, \'features_\' + str(ind) + \'.h5\')\n                d_wp = weighted_cam_pooling(features, cams)\n                desc_wp = np.concatenate((desc_wp, d_wp))\n                #d_sp = sum_pooling(features)\n                #desc_sp = np.concatenate((desc_sp, d_sp))\n                #d_sp_c = sum_pooling(features, CroW=True)\n                #desc_sp_c = np.concatenate((desc_sp_c, d_sp_c))\n\n            elif aggregation_type == \'Online\':\n                features, cams = extract_feat_cam_all(model, layer, batch_size, data)\n                d_wp = weighted_cam_pooling(features, cams)\n                for img_ind in range(0, image_batch_size):\n                    print \'Saved \' + image_names[img_ind] + \'.h5\'\n                    save_data(d_wp[img_ind*nb_classes:(img_ind+1)*nb_classes], path_descriptors,\n                              image_names[img_ind]+\'.h5\')\n\n            print \'Image batch processed, CAMs descriptors obtained!\'\n            print \'Time elapsed: \', time.time()-t1\n            #print desc_wp.shape\n            sys.stdout.flush()\n            counter = 0\n            ind += 1\n\n        line = line.rstrip(\'\\n\')\n        images[counter] = imread(line)\n        if dataset == \'Oxford\':\n            line = line.replace(\'/data/jim011/datasets_retrieval/Oxford5k/images/\', \'\')\n        elif dataset == \'Paris\':\n            line = line.replace(\'/data/jim011/datasets_retrieval/Paris6k/images/\', \'\')\n        image_names[counter] = (line.replace(\'.jpg\', \'\'))\n        counter += 1\n        num_images += 1\n\n    #Last batch\n    print \'Last Batch:\'\n    data = np.zeros((counter, 3, size[1], size[0]), dtype=np.float32)\n    data[0:] = preprocess_images(images[0:counter], size[0], size[1], mean_value)\n\n    if aggregation_type == \'Offline\':\n        features, cams, cl = extract_feat_cam(model, layer, batch_size, data, num_classes)\n        if saving_CAMs:\n            save_data(cams, cam_path, \'cams_\' + str(ind) + \'.h5\')\n            save_data(features, feature_path, \'features_\' + str(ind) + \'.h5\')\n        d_wp = weighted_cam_pooling(features, cams)\n        desc_wp = np.concatenate((desc_wp, d_wp))\n        #d_sp = sum_pooling(features)\n        #desc_sp = np.concatenate((desc_sp, d_sp))\n        #d_sp_c = sum_pooling(features, CroW=True)\n        #desc_sp_c = np.concatenate((desc_sp_c, d_sp_c))\n\n    elif aggregation_type == \'Online\':\n        features, cams = extract_feat_cam_all(model, layer, batch_size, data)\n        d_wp = weighted_cam_pooling(features, cams)\n        for img_ind in range(0, counter):\n            save_data(d_wp[img_ind * nb_classes:(img_ind + 1) * nb_classes], path_descriptors,\n                      image_names[img_ind] + \'.h5\')\n    ind += 1\n    print desc_sp.shape\n    print \'Batch processed, CAMs descriptors obtained!\'\n    print \'Total time elapsed: \', time.time() - t0\n    sys.stdout.flush()\n\n    return desc_wp, desc_sp, desc_sp_c\n\n\n########################################################################################################################\n\n# Main Script\n\n# Uncomment some parts to get Descriptors for Sum-pooling or Sum-Pooling + CroW Channel Weights\n\nprint \'Dataset: \', dataset\nprint \'Aggregation type \', aggregation_type\nprint \'Num classes: \', num_classes\nprint \'Mean: \', mean_value\n\nt_0 = time.time()\ndesc_wp = np.zeros((0, dim_descriptor), dtype=np.float32)\ndesc_sp_c = np.zeros((0, dim_descriptor), dtype=np.float32)\ndesc_sp = np.zeros((0, dim_descriptor), dtype=np.float32)\n\n\n# Horizontal Images\ndesc_wp, desc_sp, desc_sp_c = \\\n    extract_cam_descriptors(model, batch_size, num_classes, size_h, mean_value, train_list_path_h,\n                            desc_wp, desc_sp, desc_sp_c)\n\n# Vertical Images\ndesc_wp, desc_sp, desc_sp_c = \\\n    extract_cam_descriptors(model, batch_size, num_classes, size_v, mean_value, train_list_path_v,\n                            desc_wp, desc_sp, desc_sp_c, ind)\n\n\n# For the queries\nif dataset == \'Oxford\':\n    i = 0\n    with open(\'../lists/list_queries_oxford.txt\', ""r"") as f:\n        for line in f:\n            print i\n            line = line.replace(\'\\n\', \'\')\n            img = np.array(\n                (imread(path_images_oxford + line + \'.jpg\')),\n                dtype=np.float32)\n\n            #img = np.transpose(img, (2, 0, 1))\n            print img.shape\n            if img.shape[0] > img.shape[1]:\n                size = size_v\n            else:\n                size = size_h\n\n            img_p = preprocess_images(img, size[0], size[1], mean_value)\n\n            if aggregation_type == \'Offline\':\n                features, cams, cl = extract_feat_cam(model, layer, batch_size, img_p, num_classes)\n                if saving_CAMs:\n                    save_data(cams, cam_path, \'cams_\' + str(ind) + \'.h5\')\n                    save_data(features, feature_path, \'features_\' + str(ind) + \'.h5\')\n\n                d_wp = weighted_cam_pooling(features, cams)\n                desc_wp = np.concatenate((desc_wp, d_wp))\n                #d_sp = sum_pooling(features)\n                #desc_sp = np.concatenate((desc_sp, d_sp))\n                #d_sp_c = sum_pooling(features, CroW=True)\n                #desc_sp_c = np.concatenate((desc_sp_c, d_sp_c))\n\n            elif aggregation_type == \'Online\':\n                features, cams = extract_feat_cam_all(model, layer, batch_size, img_p)\n                d_wp = weighted_cam_pooling(features, cams)\n                print \'Saved \' + line + \'.h5\'\n                save_data(d_wp, path_descriptors, line+\'.h5\')\n            print desc_sp.shape\n            i += 1\n            ind += 1\n\nelif dataset == \'Paris\':\n    i = 0\n    with open(\'../lists/list_queries_paris.txt\', ""r"") as f:\n        for line in f:\n            print i\n            line = line.replace(\'\\n\', \'\')\n            img = np.array(\n                (imread(path_images_paris + line + \'.jpg\')),\n                dtype=np.float32)\n\n            # img = np.transpose(img, (2, 0, 1))\n            print img.shape\n            if img.shape[0] > img.shape[1]:\n                size = size_v\n            else:\n                size = size_h\n\n            img_p = preprocess_images(img, size[0], size[1], mean_value)\n\n            if aggregation_type == \'Offline\':\n                features, cams, cl = extract_feat_cam(model, layer, batch_size, img_p, num_classes)\n                if saving_CAMs:\n                    save_data(cams, cam_path, \'cams_\' + str(ind) + \'.h5\')\n                    save_data(features, feature_path, \'features_\' + str(ind) + \'.h5\')\n                d_wp = weighted_cam_pooling(features, cams)\n                desc_wp = np.concatenate((desc_wp, d_wp))\n                #d_sp = sum_pooling(features)\n                #desc_sp = np.concatenate((desc_sp, d_sp))\n                #d_sp_c = sum_pooling(features, CroW=True)\n                #desc_sp_c = np.concatenate((desc_sp_c, d_sp_c))\n\n            elif aggregation_type == \'Online\':\n                features, cams = extract_feat_cam_all(model, layer, batch_size, img_p)\n                d_wp = weighted_cam_pooling(features, cams)\n                print \'Saved \' + line + \'.h5\'\n                save_data(d_wp, path_descriptors, line+\'.h5\')\n\n            i += 1\n            ind += 1\n\n\nif aggregation_type ==\'Offline\':\n    save_data(desc_wp, descriptors_cams_path_wp, \'\')\n    #save_data(desc_sp, descriptors_cams_path_sp, \'\')\n    #save_data(desc_sp_c, descriptors_cams_path_sp_c, \'\')\nprint \'Data Saved\'\nprint \'Total time elapsed: \', time.time() - t_0\n'"
keras_code/B_Offline_Eval.py,0,"b'import numpy as np\nimport os\nimport h5py\nimport sys\nimport getopt\nimport evaluate_oxford_paris as eval\nimport utils as utils\nimport time\nfrom vgg_cam import vggcam\nfrom cam_utils import extract_feat_cam_fast, get_output_layer\nfrom utils import create_folders, save_data, preprocess_images, load_data, print_classes\nfrom pooling_functions import descriptor_aggregation, retrieve_n_descriptors, compute_pca, sum_pooling, weighted_cam_pooling\nfrom scipy.misc import imread\nimport math\nfrom reranking import re_ranking\nimport pickle\nimport cv2\nfrom keras.models import *\nfrom keras.callbacks import *\nimport keras.backend as K\n\n\nimagenet_dictionary = pickle.load(open(""../imagenet1000_clsid_to_human.pkl"", ""rb""))\n\n# Instructions Arguments: python script.py -d \'Oxford/Paris\' --nc_q 32 --pca 1 --qe 10 --re 100 --nc_re 6\n\ntry:\n    opts, args = getopt.getopt(sys.argv[1:], \'d:\', [\'nc_q=\', \'pca=\', \'qe=\', \'re=\', \'nc_re=\'])\n    flag_nc_q = False\n    flag_pca = False\n    flag_d = False\n    flag_nc_re = False\n    flag_qe = False\n    flag_re = False\nexcept getopt.GetoptError:\n    print \'script.py -d <dataset> --nc_q <nclasses_query> --pca <n_classes_pca> --qe <n_query_exp> --re <n_re_ranking> \' \\\n          \'--nc_re <n_classes_re_ranking>\'\n    sys.exit(2)\nfor opt, arg in opts:\n    if opt == \'-d\':\n        if arg == \'Oxford\' or arg == \'Paris\' or arg == \'Oxford105k\' or arg == \'Paris106k\':\n            dataset = arg\n            flag_d = True\n\n    elif opt == \'--nc_q\':\n            num_cams = int(arg)\n            flag_nc_q = True\n\n    elif opt == \'--pca\':\n            num_classes_pca = int(arg)\n            flag_pca = True\n\n    elif opt == \'--qe\':\n            n_expand = int(arg)\n            query_expansion = True\n            flag_qe = True\n\n    elif opt == \'--re\':\n            do_re_ranking = True\n            top_n_ranking = int(arg)\n            flag_re = True\n\n    elif opt == \'--nc_re\':\n            num_cams2 = int(arg)\n            flag_nc_re = True\n\n# SET FOR RE-RANKING\nbatch_size_re = 6   # More large = Faster, depends on GPU RAM\n\n# Global params\nn_images_distractors = 100070\nn_images_oxford = 5063\nn_images_paris = 6392\nn_queries = 55\n\n# Descriptors for Re-ranking  (Size W x H)\ndim = \'1024x720\'\nsize_v = [720, 1024]\nsize_h = [1024, 720]\nmean_value = [123.68, 116.779, 103.939]\n\n# Parameters to set\n\n# Dataset\nif not flag_d:\n    dataset = \'Oxford\'\n    print \'Default dataset: \', dataset\n\n# Network Parameters\nnb_classes = 1000\nVGGCAM_weight_path = \'../models/vgg_cam_weights.h5\'\nmodel_name = \'vgg_16_CAM\'\nlayer = \'relu5_1\'\n\nmodel = vggcam(nb_classes)\nmodel.load_weights(VGGCAM_weight_path)\n\n# For faster processing of individual queries:\n# Set convolutional layer to extract the CAMs (CAM_relu layer)\nfinal_conv_layer = get_output_layer(model, ""CAM_relu"")\n\n# Set layer to extract the features\nconv_layer_features = get_output_layer(model, layer)\n\n# Function to get scores, conv_maps --> Could be implemented outside, bottleneck\nget_output = K.function([model.layers[0].input, K.learning_phase()],\n                        [final_conv_layer.output, model.layers[-1].output, conv_layer_features.output])\n\ncount = 0\n\n# PCA Parameters\napply_pca = True\npca_dim = 512\n\nif not flag_pca:\n    num_classes_pca = 1\n    print \'Default pca_classes: \', num_classes_pca\n\n# N Class Activation Maps\nif not flag_nc_q:\n    num_cams = 64\n    print \'Default classes: \', num_cams\n\n# Num_cams2 --> Used to compute the descriptors when re-ranking\nif not flag_nc_re:\n    num_cams_re = 6\n    print \'Default classes for re-ranking: \', num_cams_re\n\n# Re-ranking\nif not flag_re:\n    do_re_ranking = False\n    top_n_ranking = 0\n    print \'Not doing Re-ranking\'\n\n# Query Expansion\nif not flag_qe:\n    # Re-ranking\n    query_expansion = False\n    n_expand = 0\n    print \'Not doing Query Expansion\'\n\nnum_prec_classes = 64\n\nprint \'Dataset: \', dataset\nprint \'Num_cams \', num_cams\nprint \'PCA with \', num_classes_pca\nif do_re_ranking:\n    print \'Re-ranking with first \', top_n_ranking\nif query_expansion:\n    print \'Applying query expansion using the first \', n_expand\n\n\nif dataset == \'Oxford\':\n    image_path = \'/data/jim011/datasets_retrieval/Oxford5k/images/\'\n    ranking_path = \'../results/oxford/\' + model_name + \'/\' + layer + \'/\' + dim \\\n                   + \'/R\' + str(top_n_ranking) + \'QE\' + str(n_expand)+\'/off/\'\n    ranking_image_names_list = \'../lists/list_oxford_rank.txt\'\n    utils.create_folders(ranking_path)\n\n    pca_descriptors_path = \'/data/jim011/paris/descriptors/Vgg_16_CAM/relu5_1/1024x720/\' \\\n                           \'paris_all_64_wp.h5\'\n\n    cam_descriptors_path = \'/data/jim011/oxford/descriptors/Vgg_16_CAM/relu5_1/1024x720/oxford_all_64_wp.h5\'\n\n    n_images_pca = n_images_paris\n\n    num_images = n_images_oxford\n\n    t = time.time()\n\n    image_names = list()\n\n    with open(ranking_image_names_list, ""r"") as f:\n        for line in f:\n            image_names.append(line)\n\n    image_names = np.array(image_names)\n\n    print image_names\n\n    sys.stdout.flush()\n\n    path_gt = ""/data/jim011/datasets_retrieval/Oxford5k/ground_truth/""\n    query_names = [""all_souls"", ""ashmolean"", ""balliol"", ""bodleian"", ""christ_church"", ""cornmarket"", ""hertford"", ""keble"",\n                   ""magdalen"", ""pitt_rivers"", ""radcliffe_camera""]\n\n\nelif dataset == \'Paris\':\n    image_path = \'/data/jim011/datasets_retrieval/Paris6k/images/\'\n    ranking_path = \'../results/paris/\' + model_name + \'/\' + layer + \'/\' + dim \\\n                   + \'/R\' + str(top_n_ranking) + \'QE\' + str(n_expand) + \'/off/\'\n    ranking_image_names_list = \'../lists/list_paris_rank.txt\'\n    utils.create_folders(ranking_path)\n\n    descriptors_path = \'/data/jim011/paris/descriptors/Vgg_16_CAM/relu5_1/1024x720/\'\n\n    pca_descriptors_path = \'/data/jim011/oxford/descriptors/Vgg_16_CAM/relu5_1/1024x720/\' \\\n                           \'oxford_all_64_wp.h5\'\n\n    cam_descriptors_path = \'/data/jim011/paris/descriptors/Vgg_16_CAM/relu5_1/1024x720/paris_all_64_wp.h5\'\n    num_images = n_images_paris\n    n_images_pca = n_images_oxford\n\n    t = time.time()\n\n    image_names = list()\n\n    with open(ranking_image_names_list, ""r"") as f:\n        for line in f:\n            image_names.append(line)\n\n    image_names = np.array(image_names)\n\n    path_gt = ""/data/jim011/datasets_retrieval/Paris6k/ground_truth/""\n    query_names = [""defense"", ""eiffel"", ""invalides"", ""louvre"", ""moulinrouge"", ""museedorsay"", ""notredame"", ""pantheon"",\n                   ""pompidou"", ""sacrecoeur"", ""triomphe""]\n\n\nelif dataset == \'Oxford105k\':\n    image_path = \'/data/jim011/datasets_retrieval/Oxford5k/images/\'\n    ranking_path = \'../results/oxford105k/\' + model_name + \'/\' + layer + \'/\' \\\n                   + dim + \'/\' + \'/R\' + str(top_n_ranking) + \'QE\' + str(n_expand)+\'/off/\'\n    ranking_image_names_list = \'../lists/list_oxford_rank.txt\'\n    ranking_distractors_list = \'../lists/list_oxford_105k_rank.txt\'\n    utils.create_folders(ranking_path)\n\n    cam_distractors_path = \'/data/jim011/descriptors100k/descriptors/Vgg_16_CAM/relu5_1/1024x720/distractor_all_64_wp_\'\n\n    pca_descriptors_path = \'/data/jim011/paris/descriptors/Vgg_16_CAM/relu5_1/1024x720/\' \\\n                           \'paris_all_64_wp.h5\'\n\n    cam_descriptors_path = \'/data/jim011/oxford/descriptors/Vgg_16_CAM/relu5_1/1024x720/oxford_all_64_wp.h5\'\n\n    n_images_pca = n_images_paris\n\n    num_images = n_images_oxford\n    sys.stdout.flush()\n\n    t = time.time()\n\n    image_names = list()\n\n    with open(ranking_image_names_list, ""r"") as f:\n        for line in f:\n            image_names.append(line)\n\n    with open(ranking_distractors_list, ""r"") as f:\n        for line in f:\n            image_names.append(line)\n\n    image_names = np.array(image_names)\n\n    sys.stdout.flush()\n\n    path_gt = ""/data/jim011/datasets_retrieval/Oxford5k/ground_truth/""\n    query_names = [""all_souls"", ""ashmolean"", ""balliol"", ""bodleian"", ""christ_church"", ""cornmarket"", ""hertford"", ""keble"",\n                   ""magdalen"", ""pitt_rivers"", ""radcliffe_camera""]\n\n\nelif dataset == \'Paris106k\':\n    image_path = \'/data/jim011/datasets_retrieval/Paris6k/images/\'\n    ranking_path = \'../results/paris106k/\' + model_name + \'/\' + layer + \'/\' \\\n                   + dim + \'/\' + \'/R\' + str(top_n_ranking) + \'QE\' + str(n_expand)+\'/off/\'\n    ranking_image_names_list = \'../lists/list_paris_rank.txt\'\n    ranking_distractors_list = \'../lists/list_oxford_105k_rank.txt\'\n    utils.create_folders(ranking_path)\n\n    cam_distractors_path = \'/data/jim011/descriptors100k/descriptors/Vgg_16_CAM/relu5_1/1024x720/distractor_all_64_wp_\'\n\n    pca_descriptors_path = \'/data/jim011/oxford/descriptors/Vgg_16_CAM/relu5_1/1024x720/\' \\\n                           \'oxford_all_64_wp.h5\'\n\n    cam_descriptors_path = \'/data/jim011/paris/descriptors/Vgg_16_CAM/relu5_1/1024x720/paris_all_64_wp.h5\'\n\n    n_images_pca = n_images_oxford\n\n    sys.stdout.flush()\n    num_images = n_images_paris\n\n    image_names = list()\n    with open(ranking_image_names_list, ""r"") as f:\n        for line in f:\n            image_names.append(line)\n\n    with open(ranking_distractors_list, ""r"") as f:\n        for line in f:\n            image_names.append(line)\n\n    image_names = np.array(image_names)\n\n    path_gt = ""/data/jim011/datasets_retrieval/Paris6k/ground_truth/""\n    query_names = [""defense"", ""eiffel"", ""invalides"", ""louvre"", ""moulinrouge"", ""museedorsay"", ""notredame"", ""pantheon"",\n                   ""pompidou"", ""sacrecoeur"", ""triomphe""]\n\n\nmaps = list()\n\n# PCA and Aggregation --> This could be precomputed, for ablation purposes it has been included here\n\nif apply_pca:\n    pca_desc = retrieve_n_descriptors(num_classes_pca, n_images_pca, load_data(pca_descriptors_path))\n    pca_matrix = compute_pca(pca_desc, pca_dim=pca_dim, whiten=True)\n    print \'PCA matrix shape:\', pca_matrix.components_.shape\nelse:\n    pca_matrix = None\n\nif dataset == \'Oxford105k\' or dataset == \'Paris106k\':\n    n_chunks = 10\n    distractors = np.zeros((0, 512), dtype=np.float32)\n    for n_in in range(0, n_chunks + 1):\n        desc = load_data(cam_distractors_path + str(n_in) + \'.h5\')\n        print desc.shape\n        distractors = np.concatenate((distractors, descriptor_aggregation(desc, desc.shape[0] / num_prec_classes,\n                                                                          num_cams, pca_matrix)))\n        print distractors.shape\n        t = time.time()\n        cam_descriptors = load_data(cam_descriptors_path)\n        print \'Time elapsed loading: \', time.time() - t\n        sys.stdout.flush()\n    data = descriptor_aggregation(cam_descriptors, num_images, num_cams, pca_matrix)\n    data = np.concatenate((data, distractors))\n\nelif dataset == \'Oxford\' or dataset == \'Paris\':\n    t = time.time()\n    cam_descriptors = load_data(cam_descriptors_path)\n    print \'Time elapsed loading: \', time.time() - t\n    data = descriptor_aggregation(cam_descriptors, num_images, num_cams, pca_matrix)\n    #data = pca_matrix.transform(cam_descriptors)\n    #data /= np.linalg.norm(data)\n    #data = sum_pooling(cam_descriptors, CroW = True)\n    #data = cam_descriptors\n\n    sys.stdout.flush()\nt_vector = list()\n\n# TESTING\n\nfor query_name in query_names:\n    print count\n    for i in range(1, 6):\n        f = open(path_gt + query_name + \'_\' + str(i) + \'_query.txt\').readline()\n        if dataset == \'Oxford\' or dataset == \'Oxford105k\':\n            f = f.replace(""oxc1_"", """")\n        f_list = f.split("" "")\n        for k in range(1, 5):\n            f_list[k] = (int(math.floor(float(f_list[k]))))\n\n        query_img_name = f_list[0]\n        img = imread(image_path + query_img_name + \'.jpg\')\n\n        # Query bounding box\n        x, y, dx, dy = f_list[1], f_list[2], f_list[3], f_list[4]\n\n        # Feature map of the query bounding box (the dimensions of the last layer are 16 times smaller)\n\n        f_x, f_y, f_dx, f_dy = int((x - (x % 16)) / 16), int((y - (y % 16)) / 16), \\\n                               int((dx - (dx % 16)) / 16), int((dy - (dy % 16)) / 16)\n\n        img_cropped = img[y:dy, x:dx]\n\n        print \'Name of the query: \', query_img_name\n\n        # Make multiple of 16\n\n        h = img_cropped.shape[0] - (img_cropped.shape[0] % 16)\n        w = img_cropped.shape[1] - (img_cropped.shape[1] % 16)\n        img_cropped = preprocess_images(img_cropped, w, h, mean_value)\n\n        # Obtain the classes from the cropped query (To-do implement it directly on model --> Now we do 2 forward)\n        features_c, cams_c, class_list = extract_feat_cam_fast(model, get_output, conv_layer_features,\n                                                               1, img_cropped, num_cams)\n\n        if img.shape[0] > img.shape[1]:\n            size = size_v\n        else:\n            size = size_h\n\n        # Query resized to be 1024x720 or 720x1024\n        img_p = preprocess_images(img, size[0], size[1], mean_value)\n\n        # Obtain features for all the image\n        time_search = time.time()\n        t_extract = time.time()\n        features, cams, _ = extract_feat_cam_fast(model, get_output, conv_layer_features, 1, img_p,\n                                               num_cams, class_list[0, 0:num_cams])\n\n        print \'Time extraction features \', time.time() - t_extract\n\n        # Build the descriptor with the query features\n        cam_pool_t = time.time()\n\n        # Cropped Features (Query Bounding Box)\n        d_wp = weighted_cam_pooling(features[:, :, f_y:f_dy, f_x:f_dx], cams[:, :, f_y:f_dy, f_x:f_dx])\n\n        # Cropped Query (Bounding Box)\n        #d_wp = weighted_cam_pooling(features_c, cams_c)\n\n        print \'time campool \', time.time() - cam_pool_t\n        # Compute Query Descriptor\n        time_agg = time.time()\n\n        desc = descriptor_aggregation(d_wp, 1, num_cams, pca_matrix)\n\n        # For Sum-Pooling (Raw)\n\n        #desc = pca_matrix.transform(sum_pooling(features[:, :, f_y:f_dy, f_x:f_dx], CroW=False))\n        #desc = pca_matrix.transform(sum_pooling(features[:, :, f_y:f_dy, f_x:f_dx], CroW=True))\n        #desc = sum_pooling(features_c)\n        #desc /= np.linalg.norm(desc)\n\n        print \'time agg \', time.time() - time_agg\n\n        time_ind = time.time()\n        indices_local, data_local = eval.save_ranking_one_query(data, desc, image_names, ranking_path, query_img_name)\n        print \'Time ind = \', time.time() - time_ind\n        print \'Time search = \', time.time() - time_search\n        t_vector.append(time.time() - time_search)\n\n        if do_re_ranking:\n            # When re-ranking descriptor for the query computed with less CAMs, as we know the relevant objects\n            desc = descriptor_aggregation(d_wp, 1, num_cams_re, pca_matrix)\n            t_rerank = time.time()\n            indices_re_ranking, data_re_ranking = re_ranking(desc, class_list[0, 0:num_cams_re], batch_size_re, image_names,\n                                                             indices_local, dataset, top_n_ranking, pca_matrix,\n                                                             model, conv_layer_features, get_output)\n            print \'Time reranking: \', time.time() - t_rerank\n            eval.save_ranking_indices(indices_re_ranking, image_names, query_img_name, ranking_path)\n\n        if query_expansion:\n            if do_re_ranking:\n                data_local[indices_re_ranking[0:top_n_ranking]] = data_re_ranking\n                desc_expanded = eval.expand_query(n_expand, data_local, indices_re_ranking)\n            else:\n                desc_expanded = eval.expand_query(n_expand, data_local, indices_local)\n            eval.save_ranking_one_query(data, desc_expanded, image_names, ranking_path, query_img_name)\n\nprint \'Time elapsed computing distances: \', time.time() - t\nprint np.array(t_vector).sum()/55\nif dataset == \'Oxford\' or dataset == \'Oxford105k\':\n    maps.append(eval.evaluate_oxford(ranking_path))\nelif dataset == \'Paris\' or dataset == \'Paris106k\':\n    maps.append(eval.evaluate_paris(ranking_path))\n\nmaps_file = open(ranking_path + \'maps\' + dataset + \'_Npca_\' + str(num_classes_pca) + \'_Nc_\' + str(num_cams) + \'.txt\', \'w\')\n\nprint maps\n\nfor res in maps:\n    maps_file.write(str(res) + \'\\n\')\n\nmaps_file.close()\n'"
keras_code/B_Online_Aggregation_Eval.py,0,"b'import numpy as np\nimport os\nimport h5py\nimport sys\nimport evaluate_oxford_paris as eval\nimport utils as utils\nimport time\nimport getopt\nfrom vgg_cam import vggcam\nfrom utils import create_folders, save_data, preprocess_images, load_data, print_classes\nfrom pooling_functions import weighted_cam_pooling, descriptor_aggregation, retrieve_n_descriptors, \\\n    descriptor_aggregation_cl, compute_pca\nfrom cam_utils import extract_feat_cam_fast, get_output_layer\nfrom scipy.misc import imread\nimport math\nimport pickle\nfrom reranking import re_ranking\nfrom keras.models import *\nfrom keras.callbacks import *\nimport keras.backend as K\n\nimagenet_dictionary = pickle.load(open(""../imagenet1000_clsid_to_human.pkl"", ""rb""))\n\n# Instructions Arguments: python script.py -d \'Oxford/Paris\' --nc_q 32 --pca 1 --qe 10 --re 100 --nc_re 6\ntry:\n    opts, args = getopt.getopt(sys.argv[1:], \'d:\', [\'nc_q=\', \'pca=\', \'qe=\', \'re=\', \'nc_re=\'])\n    flag_nc_q = False\n    flag_pca = False\n    flag_d = False\n    flag_qe = False\n    flag_re = False\n    flag_nc_re = False\n\nexcept getopt.GetoptError:\n    print \'script.py -d <dataset> --nc_q <nclasses_query> --pca <n_classes_pca> --qe <n_query_exp> --re <n_re_ranking>\' \\\n          \'--nc_re <n_classes_re_ranking>\'\n    sys.exit(2)\nfor opt, arg in opts:\n    if opt == \'-d\':\n        if arg == \'Oxford\' or arg == \'Paris\':\n            dataset = arg\n            flag_d = True\n\n    elif opt == \'--nc_q\':\n        num_cams = int(arg)\n        flag_nc_q = True\n\n    elif opt == \'--pca\':\n        num_classes_pca = int(arg)\n        flag_pca = True\n\n    elif opt == \'--qe\':\n        n_expand = int(arg)\n        query_expansion = True\n        flag_qe = True\n\n    elif opt == \'--re\':\n        do_re_ranking = True\n        top_n_ranking = int(arg)\n        flag_re = True\n\n    elif opt == \'--nc_re\':\n        num_cams2 = int(arg)\n        flag_nc_re = True\n\n\nn_images_oxford = 5063\nn_images_paris = 6392\nn_queries = 55\n\n# Descriptors for Re-ranking / Local Search\n\n# Image Pre-Processing\ndim = \'1024x720\'\nsize_v = [720, 1024]\nsize_h = [1024, 720]\nmean_value = [123.68, 116.779, 103.939]\n\ndescriptors_dim = 512\n\n# Parameters to set\n\n# Dataset\nif not flag_d:\n    dataset = \'Oxford\'\n    print \'Default dataset: \', dataset\n\n# Network Parameters\nnb_classes = 1000\nVGGCAM_weight_path = \'../models/vgg_cam_weights.h5\'\nmodel_name = \'vgg_16_CAM\'\nlayer = \'relu5_1\'\n\nmodel = vggcam(nb_classes)\nmodel.load_weights(VGGCAM_weight_path)\n\n# For faster processing of individual queries:\n# Set convolutional layer to extract the CAMs (CAM_relu layer)\nfinal_conv_layer = get_output_layer(model, ""CAM_relu"")\n\n# Set layer to extract the features\nconv_layer_features = get_output_layer(model, layer)\n\n# Function to get scores, conv_maps --> Could be implemented outside, bottleneck\nget_output = K.function([model.layers[0].input, K.learning_phase()],\n                        [final_conv_layer.output, model.layers[-1].output, conv_layer_features.output])\n\n# Re-Ranking CAMs\nbatch_size_re = 6  # More large = Faster, depends on GPU RAM\nnum_cams_re = 6\n\n# PCA Application\napply_pca = True\npca_dim = 512\nif not flag_pca:\n    num_classes_pca = 1\n    print \'Default pca_classes: \', num_classes_pca\n\n# N Class Activation Maps\nif not flag_nc_q:\n    num_cams = 64\n    print \'Default classes: \', num_cams\n\n# Re-ranking\nif not flag_re:\n    do_re_ranking = False\n    top_n_ranking = 0\n    print \'Not doing Re-ranking\'\n\n# Query Expansion\nif not flag_qe:\n    # Re-ranking\n    query_expansion = False\n    n_expand = 0\n    print \'Not doing Query Expansion\'\n\n\nmodel = vggcam(nb_classes)\nmodel.load_weights(VGGCAM_weight_path)\n\ncount = 0\n\nprint \'Dataset: \', dataset\nprint \'Num_cams \', num_cams\nprint \'PCA with \', num_classes_pca\nif do_re_ranking:\n    print \'Re-Ranking the top  \', top_n_ranking\nif query_expansion:\n    print \'Query Expansion = \', n_expand\n\n\nif dataset == \'Oxford\':\n    # Set Paths\n    images_path = \'/data/jim011/datasets_retrieval/Oxford5k/images/\'\n    ranking_path = \'../results/oxford/\' + model_name + \'/\' + layer + \'/\' + dim + \'/R\' +\\\n                   str(top_n_ranking) + \'QE\' + str(n_expand)+\'/online/\'\n    ranking_image_names_list = \'../lists/list_oxford_rank.txt\'\n    utils.create_folders(ranking_path)\n    n_images_pca = n_images_paris\n\n    descriptors_path = \'/data/jim011/oxford/descriptors/Vgg_16_CAM/relu5_1/1024x720/online/\'\n\n    pca_descriptors_path = \'/data/jim011/paris/descriptors/Vgg_16_CAM/relu5_1/1024x720/\' \\\n                           \'paris_all_64_wp.h5\'\n    n_images = n_images_oxford\n\n    path_gt = \'/data/jim011/datasets_retrieval/Oxford5k/ground_truth/\'\n    query_names = [""all_souls"", ""ashmolean"", ""balliol"", ""bodleian"", ""christ_church"", ""cornmarket"", ""hertford"", ""keble"",\n                   ""magdalen"", ""pitt_rivers"", ""radcliffe_camera""]\n\n    # Load Image names\n    image_names = list()\n    with open(ranking_image_names_list, ""r"") as f:\n        for line in f:\n            image_names.append(line)\n    image_names = np.array(image_names)\n\n    # Load Class Vectors\n    print \'Loading descriptors...\'\n\n    descriptors = np.zeros((nb_classes*n_images_oxford, 512), dtype=np.float32)\n    for index, img_n in enumerate(image_names):\n        descriptors[index*nb_classes:(index+1)*nb_classes] = load_data(descriptors_path+img_n.replace(\'\\n\', \'\')+\'.h5\')\n\n    print \'Descriptors loaded\'\n    sys.stdout.flush()\n\n\nelif dataset == \'Paris\':\n    # Set Paths\n    images_path = \'/data/jim011/datasets_retrieval/Paris6k/images/\'\n\n    ranking_path = \'../results/paris/\' + model_name + \'/\' + layer + \'/\' + dim + \'/R\' +\\\n                   str(top_n_ranking) + \'QE\' + str(n_expand)+\'/online/\'\n    ranking_image_names_list = \'../lists/list_paris_rank.txt\'\n    utils.create_folders(ranking_path)\n\n    descriptors_path = \'/data/jim011/paris/descriptors/Vgg_16_CAM/relu5_1/1024x720/online/\'\n\n    pca_descriptors_path = \'/data/jim011/oxford/descriptors/Vgg_16_CAM/relu5_1/1024x720/\' \\\n                           \'oxford_all_64_wp.h5\'\n    n_images = n_images_paris\n    n_images_pca = n_images_oxford\n\n    path_gt = ""/data/jim011/datasets_retrieval/Paris6k/ground_truth/""\n    query_names = [""defense"", ""eiffel"", ""invalides"", ""louvre"", ""moulinrouge"", ""museedorsay"", ""notredame"", ""pantheon"",\n                   ""pompidou"", ""sacrecoeur"", ""triomphe""]\n\n    # Load Image Names\n    image_names = list()\n    with open(ranking_image_names_list, ""r"") as f:\n        for line in f:\n            image_names.append(line)\n\n    image_names = np.array(image_names)\n\n    # Load Class Vectors\n    print \'Loading descriptors...\'\n\n    descriptors = np.zeros((nb_classes * n_images_paris, 512))\n    for index, img_n in enumerate(image_names):\n        descriptors[index * nb_classes:(index + 1) * nb_classes] = load_data(\n            descriptors_path + img_n.replace(\'\\n\', \'\') + \'.h5\')\n\n    print \'Descriptors loaded\'\n    sys.stdout.flush()\n\n\nmaps = list()\n\nif apply_pca:\n    pca_desc = retrieve_n_descriptors(num_classes_pca, n_images_pca, load_data(pca_descriptors_path))\n    print pca_desc.shape\n    pca_matrix = compute_pca(pca_desc, pca_dim=pca_dim)\nelse:\n    pca_matrix = None\n\nfor query_name in query_names:\n    for i in range(1, 6):\n        f = open(path_gt + query_name + \'_\' + str(i) + \'_query.txt\').readline()\n        if dataset == \'Oxford\':\n            f = f.replace(""oxc1_"", """")\n        f_list = f.split("" "")\n        for k in range(1, 5):\n            f_list[k] = (int(math.floor(float(f_list[k]))))\n\n        img = imread(images_path + f_list[0] + \'.jpg\')\n        #print \'Image Shape: \' + str(img.shape[0]) + \'x\' + str(img.shape[1])\n\n        # Queries bounding box\n        x, y, dx, dy = f_list[1], f_list[2], f_list[3], f_list[4]\n\n        # Feature Maps bounding box\n        f_x, f_y, f_dx, f_dy = int((x - (x % 16)) / 16), int((y - (y % 16)) / 16), \\\n                               int((dx - (dx % 16)) / 16), int((dy - (dy % 16)) / 16)\n\n        img_cropped = img[y:dy, x:dx]\n        query_img_name = f_list[0]\n\n        print \'Name of the query: \', f_list[0]\n\n        h = img_cropped.shape[0] - (img_cropped.shape[0] % 16)\n        w = img_cropped.shape[1] - (img_cropped.shape[1] % 16)\n        img_cropped = preprocess_images(img_cropped, w, h, mean_value)\n\n        features_c, cams_c, class_list = extract_feat_cam_fast(model, get_output, conv_layer_features, 1, img_cropped, num_cams)\n\n        # Show Classes of Query (uncomment to see)\n        # print_classes(imagenet_dictionary, class_list[0])\n\n        if img.shape[0] > img.shape[1]:\n            size = size_v\n        else:\n            size = size_h\n\n        img_p = preprocess_images(img, size[0], size[1], mean_value)\n\n        features, cams, roi = extract_feat_cam_fast(model, get_output, conv_layer_features, 1, img_p,\n                                    num_cams, class_list[0, 0:num_cams])\n\n        d_wp = weighted_cam_pooling(features[:, :, f_y:f_dy, f_x:f_dx],\n                                    cams[:, :, f_y:f_dy, f_x:f_dx])\n\n        desc = descriptor_aggregation(d_wp, 1, num_cams, pca_matrix)\n\n        tagregation = time.time()\n        data = descriptor_aggregation_cl(descriptors, n_images, pca_matrix, class_list[0])\n        print \'Time elapsed agregation: \', time.time() - tagregation\n\n        indices_local, data_local = eval.save_ranking_one_query(data, desc, image_names, ranking_path, query_img_name)\n\n        if do_re_ranking:\n            desc = descriptor_aggregation(d_wp, 1, num_cams_re, pca_matrix)\n            t_rerank = time.time()\n            indices_re_ranking, data_re_ranking = re_ranking(desc, class_list[0,0:num_cams_re], batch_size_re, image_names,\n                                                             indices_local, dataset, top_n_ranking, pca_matrix,\n                                                             model, conv_layer_features, get_output)\n            print \'Time reranking: \', time.time() - t_rerank\n            eval.save_ranking_indices(indices_re_ranking, image_names, query_img_name, ranking_path)\n\n        if query_expansion:\n            if do_re_ranking:\n                data_local[indices_re_ranking[0:top_n_ranking]] = data_re_ranking\n                desc_expanded = eval.expand_query(n_expand, data_local, indices_re_ranking)\n            else:\n                desc_expanded = eval.expand_query(n_expand, data_local, indices_local)\n            eval.save_ranking_one_query(data, desc_expanded, image_names, ranking_path, query_img_name)\n\nif dataset == \'Oxford\':\n    maps.append(eval.evaluate_oxford(ranking_path))\nelif dataset == \'Paris\':\n    maps.append(eval.evaluate_paris(ranking_path))\n\nmaps_file = open(ranking_path + \'maps\' + dataset + \'_pca_\' + str(num_classes_pca) + \'_Nc_\' + str(num_cams) + \'.txt\', \'w\')\n\nprint maps\n\nfor res in maps:\n    maps_file.write(str(res) + \'\\n\')\n\nmaps_file.close()\n'"
keras_code/cam_utils.py,0,"b'from keras.models import *\nfrom keras.callbacks import *\nimport keras.backend as K\nimport cv2\nimport os\nimport sys\nimport h5py\nimport numpy as np\nimport time\nimport math\nfrom scipy.misc import imread, imresize, imsave\n\nclasses_places = 205\nclasses_imagenet = 1000\n\n\ndef get_output_layer(model, layer_name):\n    # get the symbolic outputs of each ""key"" layer (we gave them unique names).\n    layer_dict = dict([(layer.name, layer) for layer in model.layers])\n    layer = layer_dict[layer_name]\n    return layer\n\n\n# Extract region of interest from CAMs\ndef extract_ROI(heatmap, threshold):\n    th = threshold * np.max(heatmap)\n    heatmap = heatmap > th\n    # Find the largest connected component\n\n    contours, hierarchy = cv2.findContours(heatmap.astype(\'uint8\'), mode=cv2.RETR_EXTERNAL, method=cv2.CHAIN_APPROX_SIMPLE)\n\n    areas = [cv2.contourArea(ctr) for ctr in contours]\n\n    max_contour = contours[areas.index(max(areas))]\n\n    x, y, w, h = cv2.boundingRect(max_contour)\n    if w == 0:\n        w = heatmap.shape[1]\n    if h == 0:\n        h = heatmap.shape[0]\n    return x, y, w, h\n\n\n# Visualization Purposes, Draw bounding box around object of interest\ndef draw_bounding_box(img, full_heatmap, label, color=(0, 0, 255), threshold=0.3):\n    # Apply the thresholding\n    full_heatmap = cv2.resize(full_heatmap, (img.shape[1], img.shape[0]))  # , interpolation=cv2.INTER_NEAREST)\n    th = threshold * np.max(full_heatmap)\n    full_heatmap = full_heatmap > th\n    # Find the largest connected component\n    ima2, contours, hierarchy = cv2.findContours(full_heatmap.astype(\'uint8\'), mode=cv2.RETR_EXTERNAL,\n                                                  method=cv2.CHAIN_APPROX_SIMPLE)\n\n    cv2.imwrite(\'contours_\' + str(threshold) + \'.jpg\', ima2)\n    areas = [cv2.contourArea(ctr) for ctr in contours]\n    max_contour = contours[areas.index(max(areas))]\n\n    x, y, w, h = cv2.boundingRect(max_contour)\n    # Draw bounding box and label\n    cv2.rectangle(img, (x, y), (x + w, y + h), color, 2)\n    #cv2.putText(img, label[:], (x + 3, y + 15), cv2.FONT_HERSHEY_SIMPLEX, 0.01, color, 2)\n    cv2.imwrite(\'bounded_\' + str(threshold) + \'.jpg\', img)\n    return x, y, w, h\n\n\n# Method for Online aggregation\ndef extract_feat_cam_all(model, layer, batch_size, images, top_nclass=1000):\n    \'\'\'\n    Extract CAM masks for all classes, for each image in the dataset. Also extract  features\n    from layer\n    :param model: The network\n    :param batch_size: batch_size\n    :param images: images in format [num_total,3,height, width]\n    :return:\n    \'\'\'\n\n    num_samples = images.shape[0]\n\n    print \'Num of total samples: \', num_samples\n    print \'Batch size: \', batch_size\n    sys.stdout.flush()\n\n    num_it = int(math.floor(num_samples / batch_size))\n    last_batch = num_samples % batch_size\n    batch_size_loop = batch_size\n\n    # Set convolutional layer to extract the CAMs (CAM_relu layer)\n    final_conv_layer = get_output_layer(model, ""CAM_relu"")\n\n    # Set layer to extract the features\n    conv_layer_features = get_output_layer(model, layer)\n    f_shape = conv_layer_features.output_shape\n\n    # Initialize Arrays\n    features_conv = np.zeros((num_samples, f_shape[1], images.shape[2]//16, images.shape[3]//16))\n    cams = np.zeros((images.shape[0], top_nclass, images.shape[2]//16, images.shape[3]//16), dtype=np.float32)\n\n    # Function to get conv_maps\n    get_output = K.function([model.layers[0].input, K.learning_phase()],\n                            [final_conv_layer.output, conv_layer_features.output])\n    # Extract weights from Dense\n    weights_fc = model.layers[-1].get_weights()[0]\n\n    for i in range(0, num_it+1):\n        t0 = time.time()\n        if i == num_it:\n            if last_batch != 0:\n                x = images[i*batch_size:batch_size*i+last_batch, :, :, :]\n                batch_size_loop = last_batch\n            else:\n                break\n        else:\n            x = images[i*batch_size:batch_size*(i+1), :, :, :]\n\n        print \'Batch number: \', i\n\n        [conv_outputs, features] = get_output([x, 0])\n        features_conv[i*batch_size:i*batch_size+features.shape[0], :, :, :] = features\n\n        print (\'Time elapsed to forward the batch: \', time.time()-t0)\n\n        for ii in range(0, batch_size_loop):\n            for k in range(0, top_nclass):\n                w_class = weights_fc[:, k]\n                cam = np.zeros(dtype=np.float32, shape=conv_outputs.shape[2:4])\n                for ind, w in enumerate(w_class):\n                    cam += w * conv_outputs[ii, ind, :, :]\n                cam /= np.max(cam)\n                cam[np.where(cam < 0)] = 0\n\n                cams[i*batch_size+ii, k, :, :] = cam\n\n        print \'Time elapsed to compute CAMs: \', time.time()-t0\n\n    return features_conv, cams\n\n\ndef extract_feat_cam(model, layer, batch_size, images, top_nclass, specify_class=None, roi=False):\n    \'\'\'\n    :param model: Network\n    :param layer: Layer to extract features\n    :param batch_size: Batch size\n    :param images: data [n_samples,3,H,W]\n    :param top_nclass: number of CAMs to extract (Top predicted N classes)\n    :param specify_class: (If we know the classes) --> Class Array\n    :param roi: Region of Interest given list of classes\n    :return: features, cams, class_list , roi\n    \'\'\'\n    # width, height of conv5_1 layer\n    # 14x14 for 224x224 input image\n    # H/16 x W/16 for H x W input image with VGG-16\n\n    num_samples = images.shape[0]\n\n    class_list = np.zeros((num_samples, top_nclass), dtype=np.int32)\n    print \'Num of total samples: \', num_samples\n    print \'Batch size: \', batch_size\n    sys.stdout.flush()\n\n    num_it = int(math.floor(num_samples / batch_size))\n    last_batch = num_samples % batch_size\n    batch_size_loop = batch_size\n\n    # Set convolutional layer to extract the CAMs (CAM_relu layer)\n    final_conv_layer = get_output_layer(model, ""CAM_relu"")\n\n    # Set layer to extract the features\n    conv_layer_features = get_output_layer(model, layer)\n    f_shape = conv_layer_features.output_shape\n\n    # Initialize Arrays\n    features_conv = np.zeros((num_samples, f_shape[1], images.shape[2] // 16, images.shape[3] // 16))\n    cams = np.zeros((images.shape[0], top_nclass, images.shape[2] // 16, images.shape[3] // 16), dtype=np.float32)\n    all_scores = np.zeros((num_samples, classes_imagenet))\n\n    # Function to get scores, conv_maps --> Could be implemented outside, bottleneck (fast version have it outside)\n    get_output = K.function([model.layers[0].input, K.learning_phase()],\n                            [final_conv_layer.output, model.layers[-1].output, conv_layer_features.output])\n\n    # Extract weights from Dense\n    weights_fc = model.layers[-1].get_weights()[0]\n\n    # Region of interest for re-ranking (bounding box coordinates --> (num samples, num_thresholds, x,y,dx,dy)\n    if roi:\n        bbox_coord = np.zeros((num_samples, 5, 4), dtype=np.int16)\n\n    for i in range(0, num_it+1):\n        t0 = time.time()\n        if i == num_it:\n            if last_batch != 0:\n                x = images[i*batch_size:batch_size*i+last_batch, :, :, :]\n                batch_size_loop = last_batch\n            else:\n                break\n        else:\n            x = images[i*batch_size:batch_size*(i+1), :, :, :]\n\n        [conv_outputs, scores, features] = get_output([x, 0])\n        features_conv[i*batch_size:i*batch_size+features.shape[0], :, :, :] = features\n\n        if specify_class is None:\n\n            for ii in range(0, batch_size_loop):\n                indexed_scores = scores[ii].argsort()[::-1]\n                for k in range(0, top_nclass):\n                    w_class = weights_fc[:, indexed_scores[k]]\n                    all_scores[i * batch_size + ii, k] = scores[ii, indexed_scores[k]]\n                    cam = np.zeros(dtype=np.float32, shape=conv_outputs.shape[2:4])\n                    for ind, w in enumerate(w_class):\n                        cam += w * conv_outputs[ii, ind, :, :]\n                    cam /= np.max(cam)\n                    cam[np.where(cam < 0)] = 0\n\n                    cams[i*batch_size+ii, k, :, :] = cam\n\n                    class_list[i*batch_size+ii, k] = indexed_scores[k]\n\n        else:\n            for ii in range(0, batch_size_loop):\n                # print \'Image number: \', ii\n                for k in range(0, top_nclass):\n                    w_class = weights_fc[:, specify_class[k]]\n                    all_scores[i * batch_size + ii, k] = scores[ii, specify_class[k]]\n                    cam = np.zeros(dtype=np.float32, shape=conv_outputs.shape[2:4])\n                    for ind, w in enumerate(w_class):\n                        cam += w * conv_outputs[ii, ind, :, :]\n                    cam /= np.max(cam)\n                    cam[np.where(cam < 0)] = 0\n\n                    cams[i * batch_size + ii, k, :, :] = cam\n\n                # How to compute the ROI of the image,\n                # in the paper results we average 2 most probable classes (little difference)\n                if roi:\n                    average = True\n\n                    if average:\n                        average_cam = np.zeros((cams.shape[2], cams.shape[3]))\n                        for cam in cams[i*batch_size+ii, 0:2]:\n                            average_cam += cam\n                        heatmap = average_cam / 2\n                    else:\n                        heatmap = cams[i*batch_size+ii, 0]\n\n                    bbox_coord[i*batch_size+ii, 0, :] = extract_ROI(heatmap=heatmap, threshold=0.01)# Full Image\n                    bbox_coord[i*batch_size+ii, 1, :] = extract_ROI(heatmap=heatmap, threshold=0.1)\n                    bbox_coord[i*batch_size+ii, 2, :] = extract_ROI(heatmap=heatmap, threshold=0.2)\n                    bbox_coord[i*batch_size+ii, 3, :] = extract_ROI(heatmap=heatmap, threshold=0.3)\n                    bbox_coord[i*batch_size+ii, 4, :] = extract_ROI(heatmap=heatmap, threshold=0.4)\n                else:\n                    bbox_coord = None\n\n        print \'Time elapsed to compute CAMs & Features: \', time.time()-t0\n    sys.stdout.flush()\n\n    if specify_class is None:\n        return features_conv, cams, class_list\n\n    else:\n        return features_conv, cams, bbox_coord\n\n\ndef extract_feat_cam_fast(model, get_output_function, layer_feat, batch_size, images, top_nclass, specify_class=None, roi=False):\n    \'\'\'\n    :param model: network\n    :param get_output_function: function to extract features\n    :param layer_feat: layer features\n    :param batch_size: batch size\n    :param images: images [num_images, 3, h, w]\n    :param top_nclass: top_predicted CAMs\n    :param specify_class: Give class array\n    :param roi: Region of interest (True / False)\n    :return:\n    \'\'\'\n\n    # width, height of conv5_1 layer\n    # 14x14 for 224x224 input image\n    # H/16 x W/16 for H x W input image with VGG-16\n\n    num_samples = images.shape[0]\n\n    class_list = np.zeros((num_samples, top_nclass), dtype=np.int32)\n    print \'Num of total samples: \', num_samples\n    print \'Batch size: \', batch_size\n    sys.stdout.flush()\n\n    num_it = int(math.floor(num_samples / batch_size))\n    last_batch = num_samples % batch_size\n    batch_size_loop = batch_size\n\n    f_shape = layer_feat.output_shape\n\n    # Initialize Arrays\n    features_conv = np.zeros((num_samples, f_shape[1], images.shape[2] // 16, images.shape[3] // 16))\n    cams = np.zeros((images.shape[0], top_nclass, images.shape[2] // 16, images.shape[3] // 16), dtype=np.float32)\n    all_scores = np.zeros((num_samples, classes_imagenet))\n\n    # Extract weights from Dense\n    weights_fc = model.layers[-1].get_weights()[0]\n\n    # Region of interest for re-ranking (bounding box coordinates --> (num samples, num_thresholds, x,y,dx,dy)\n    if roi:\n        bbox_coord = np.zeros((num_samples, 5, 4), dtype=np.int16)\n\n    for i in range(0, num_it+1):\n        t0 = time.time()\n        if i == num_it:\n            if last_batch != 0:\n                x = images[i*batch_size:batch_size*i+last_batch, :, :, :]\n                batch_size_loop = last_batch\n            else:\n                break\n        else:\n            x = images[i*batch_size:batch_size*(i+1), :, :, :]\n\n        #print \'Batch number: \', i\n\n        [conv_outputs, scores, features] = get_output_function([x, 0])\n        features_conv[i*batch_size:i*batch_size+features.shape[0], :, :, :] = features\n        #print \'Time elapsed to compute Features: \', time.time() - t0\n        #print (\'Time elapsed to forward the batch: \', time.time()-t0)\n\n        if specify_class is None:\n\n            for ii in range(0, batch_size_loop):\n                indexed_scores = scores[ii].argsort()[::-1]\n                for k in range(0, top_nclass):\n                    w_class = weights_fc[:, indexed_scores[k]]\n                    all_scores[i * batch_size + ii, k] = scores[ii, indexed_scores[k]]\n                    cam = np.zeros(dtype=np.float32, shape=conv_outputs.shape[2:4])\n                    for ind, w in enumerate(w_class):\n                        cam += w * conv_outputs[ii, ind, :, :]\n                    cam /= np.max(cam)\n                    cam[np.where(cam < 0)] = 0\n\n                    cams[i*batch_size+ii, k, :, :] = cam\n\n                    class_list[i*batch_size+ii, k] = indexed_scores[k]\n\n        else:\n            for ii in range(0, batch_size_loop):\n                # print \'Image number: \', ii\n                for k in range(0, top_nclass):\n                    w_class = weights_fc[:, specify_class[k]]\n                    all_scores[i * batch_size + ii, k] = scores[ii, specify_class[k]]\n                    cam = np.zeros(dtype=np.float32, shape=conv_outputs.shape[2:4])\n                    for ind, w in enumerate(w_class):\n                        cam += w * conv_outputs[ii, ind, :, :]\n                    cam /= np.max(cam)\n                    cam[np.where(cam < 0)] = 0\n\n                    cams[i * batch_size + ii, k, :, :] = cam\n\n                # How to compute the ROI of the image, in the paper results we average 2 most probable classes\n                if roi:\n                    average = True\n\n                    if average:\n                        average_cam = np.zeros((cams.shape[2], cams.shape[3]))\n                        for cam in cams[i*batch_size+ii, 0:2]:\n                            average_cam += cam\n                        heatmap = average_cam / 2\n                    else:\n                        heatmap = cams[i*batch_size+ii, 0]\n\n                    bbox_coord[i*batch_size+ii, 0, :] = extract_ROI(heatmap=heatmap, threshold=0.01)# Almost Full Image\n                    bbox_coord[i*batch_size+ii, 1, :] = extract_ROI(heatmap=heatmap, threshold=0.1)\n                    bbox_coord[i*batch_size+ii, 2, :] = extract_ROI(heatmap=heatmap, threshold=0.2)\n                    bbox_coord[i*batch_size+ii, 3, :] = extract_ROI(heatmap=heatmap, threshold=0.3)\n                    bbox_coord[i*batch_size+ii, 4, :] = extract_ROI(heatmap=heatmap, threshold=0.4)\n\n                else:\n                    bbox_coord = None\n\n        print \'Time elapsed to compute CAMs & Features: \', time.time()-t0\n    sys.stdout.flush()\n\n    if specify_class is None:\n        return features_conv, cams, class_list\n\n    else:\n        return features_conv, cams, bbox_coord\n\n\ndef extract_feat_cam_all_fast(model, function, layer, batch_size, images, top_nclass=1000):\n    \'\'\'\n    :param model: Network\n    :param function: Function to extract CAMs / Features\n    :param layer: Layer to extract features\n    :param batch_size:\n    :param images: Images\n    :param top_nclass:  1000 by default (ALL Imagenet)\n    :return:\n    \'\'\'\n\n    num_samples = images.shape[0]\n\n    print \'Num of total samples: \', num_samples\n    print \'Batch size: \', batch_size\n    sys.stdout.flush()\n\n    num_it = int(math.floor(num_samples / batch_size))\n    last_batch = num_samples % batch_size\n    batch_size_loop = batch_size\n\n    f_shape = layer.output_shape\n\n    # Initialize Arrays\n    features_conv = np.zeros((num_samples, f_shape[1], images.shape[2] // 16, images.shape[3] // 16))\n    cams = np.zeros((images.shape[0], top_nclass, images.shape[2] // 16, images.shape[3] // 16), dtype=np.float32)\n\n    # Extract weights from Dense\n    weights_fc = model.layers[-1].get_weights()[0]\n\n    for i in range(0, num_it+1):\n        t0 = time.time()\n        if i == num_it:\n            if last_batch != 0:\n                x = images[i*batch_size:batch_size*i+last_batch, :, :, :]\n                batch_size_loop = last_batch\n            else:\n                break\n        else:\n            x = images[i*batch_size:batch_size*(i+1), :, :, :]\n\n        print \'Batch number: \', i\n\n        [conv_outputs, features] = function([x, 0])\n        features_conv[i*batch_size:i*batch_size+features.shape[0], :, :, :] = features\n\n        print (\'Time elapsed to forward the batch: \', time.time()-t0)\n\n        for ii in range(0, batch_size_loop):\n            for k in range(0, top_nclass):\n                w_class = weights_fc[:, k]\n                cam = np.zeros(dtype=np.float32, shape=conv_outputs.shape[2:4])\n                for ind, w in enumerate(w_class):\n                    cam += w * conv_outputs[ii, ind, :, :]\n                cam /= np.max(cam)\n                cam[np.where(cam < 0)] = 0\n\n                cams[i*batch_size+ii, k, :, :] = cam\n\n        print \'Time elapsed to compute CAMs: \', time.time()-t0\n\n    return features_conv, cams\n'"
keras_code/crow.py,0,"b'# Copyright 2015, Yahoo Inc.\n# Licensed under the terms of the Apache License, Version 2.0. See the LICENSE file associated with the project for terms.\nimport numpy as np\nimport scipy\nfrom sklearn.preprocessing import normalize as sknormalize\nfrom sklearn.decomposition import PCA\n\n\ndef compute_crow_spatial_weight(X, a=2, b=2):\n    """"""\n    Given a tensor of features, compute spatial weights as normalized total activation.\n    Normalization parameters default to values determined experimentally to be most effective.\n    :param ndarray X:\n        3d tensor of activations with dimensions (channels, height, width)\n    :param int a:\n        the p-norm\n    :param int b:\n        power normalization\n    :returns ndarray:\n        a spatial weight matrix of size (height, width)\n    """"""\n    S = X.sum(axis=0)\n    z = (S**a).sum()**(1./a)\n    return (S / z)**(1./b) if b != 1 else (S / z)\n\n\ndef compute_crow_channel_weight(X):\n    """"""\n    Given a tensor of features, compute channel weights as the\n    log of inverse channel sparsity.\n    :param ndarray X:\n        3d tensor of activations with dimensions (channels, height, width)\n    :returns ndarray:\n        a channel weight vector\n    """"""\n    K, w, h = X.shape\n    area = float(w * h)\n    nonzeros = np.zeros(K, dtype=np.float32)\n    for i, x in enumerate(X):\n        nonzeros[i] = np.count_nonzero(x) / area\n\n    nzsum = nonzeros.sum()\n    for i, d in enumerate(nonzeros):\n        nonzeros[i] = np.log(nzsum / d) if d > 0. else 0.\n\n    return nonzeros\n\n\ndef apply_crow_aggregation(X):\n    """"""\n    Given a tensor of activations, compute the aggregate CroW feature, weighted\n    spatially and channel-wise.\n    :param ndarray X:\n        3d tensor of activations with dimensions (channels, height, width)\n    :returns ndarray:\n        CroW aggregated global image feature\n    """"""\n    S = compute_crow_spatial_weight(X)\n    C = compute_crow_channel_weight(X)\n    X = X * S\n    X = X.sum(axis=(1, 2))\n    return X * C\n\n\ndef apply_ucrow_aggregation(X):\n    """"""\n    Given a tensor of activations, aggregate by sum-pooling without weighting.\n    :param ndarray X:\n        3d tensor of activations with dimensions (channels, height, width)\n    :returns ndarray:\n        unweighted global image feature\n    """"""\n    return X.sum(axis=(1, 2))\n\n\ndef normalize(x, copy=False):\n    """"""\n    A helper function that wraps the function of the same name in sklearn.\n    This helper handles the case of a single column vector.\n    """"""\n    if type(x) == np.ndarray and len(x.shape) == 1:\n        return np.squeeze(sknormalize(x.reshape(1,-1), copy=copy))\n    else:\n        return sknormalize(x, copy=copy)\n\n\ndef run_feature_processing_pipeline(features, d=128, whiten=True, copy=False, params=None):\n    """"""\n    Given a set of feature vectors, process them with PCA/whitening and return the transformed features.\n    If the params argument is not provided, the transformation is fitted to the data.\n    :param ndarray features:\n        image features for transformation with samples on the rows and features on the columns\n    :param int d:\n        dimension of final features\n    :param bool whiten:\n        flag to indicate whether features should be whitened\n    :param bool copy:\n        flag to indicate whether features should be copied for transformed in place\n    :param dict params:\n        a dict of transformation parameters; if present they will be used to transform the features\n    :returns ndarray: transformed features\n    :returns dict: transform parameters\n    """"""\n    # Normalize\n    features = normalize(features, copy=copy)\n\n    # Whiten and reduce dimension\n    if params:\n        pca = params[\'pca\']\n        features = pca.transform(features)\n    else:\n        pca = PCA(n_components=d, whiten=whiten, copy=copy)\n        features = pca.fit_transform(features)\n        params = { \'pca\': pca }\n\n    # Normalize\n    features = normalize(features, copy=copy)\n\n    return features, params\n\n\ndef save_spatial_weights_as_jpg(S, path=\'.\', filename=\'crow_sw\', size=None):\n    """"""\n    Save an image for visualizing a spatial weighting. Optionally provide path, filename,\n    and size. If size is not provided, the size of the spatial map is used. For instance,\n    if the spatial map was computed with VGG, setting size=(S.shape[0] * 32, S.shape[1] * 32)\n    will scale the spatial weight map back to the size of the image.\n    :param ndarray S:\n        spatial weight matrix\n    :param str path:\n    :param str filename:\n    :param tuple size:\n    """"""\n    img = scipy.misc.toimage(S)\n    if size is not None:\n        img = img.resize(size)\n\n    img.save(os.path.join(path, \'%s.jpg\' % str(filename)))'"
keras_code/evaluate_oxford_paris.py,0,"b'import numpy as np\nimport os\nimport math\nimport h5py\nimport sys\nfrom sklearn.neighbors import NearestNeighbors\nimport sklearn\nfrom scipy.misc import imread, imresize, imsave\nimport matplotlib.pyplot as plt\nimport time\n\n\n# Query Expansion\ndef expand_query(n_expand, data, indices):\n    ind_data = indices[0:n_expand]\n    print ind_data.shape\n    desc = np.zeros(data.shape[1])\n    for ind in ind_data:\n        desc += data[ind]\n    #desc = desc / n_expand\n    desc /= np.linalg.norm(desc)\n    desc = desc.reshape((1, desc.shape[0]))\n    return desc\n\n\n# Save Ranking after Re-ranking\ndef save_ranking_indices(indices, image_names, image_name, path):\n    for i in range(0, image_names.shape[0]):\n        if image_names[i].replace(\'\\n\', \'\') == image_name:\n            print \'Saving ranking for ... \', image_name\n            print i\n            file = open(path + image_names[i].replace(\'\\n\', \'\') + \'.txt\', \'w\')\n            for ind in indices:\n                file.write(image_names[ind])\n            file.close()\n\n\n# Save Ranking for one query\ndef save_ranking_one_query(data, query_desc, image_names, path, image_name):\n    for i in range(0, image_names.shape[0]):\n        if image_names[i].replace(\'\\n\', \'\') == image_name:\n            print \'Saving ranking for ... \', image_name\n            data_aux = data[i].copy()\n            data[i] = query_desc\n            data_local = data\n            t_o = time.time()\n            distances, indices = compute_distances_optim(query_desc, data)\n            print \'Time computing distances: \', time.time() - t_o\n            file = open(path + image_names[i].replace(\'\\n\', \'\') + \'.txt\', \'w\')\n            for ind in indices:\n                file.write(image_names[ind])\n            file.close()\n            data[i] = data_aux\n            return indices, data_local\n\n\n# Save Rankings for all the queries (GS)\ndef save_rankings(indices, image_names, path, dataset):\n    if dataset == \'Oxford\':\n        f = open(\'/home/jim011/workspace/retrieval-2017-icmr/lists//queries_list_oxford.txt\')\n    if dataset == \'Paris\':\n        f = open(\'/home/jim011/workspace/retrieval-2017-icmr/lists//queries_list_paris.txt\')\n\n    for line in f:\n        for i in range(0, image_names.shape[0]):\n            if image_names[i] == line:\n                print line\n                file = open(path + image_names[i].replace(\'\\n\', \'\') + \'.txt\', \'w\')\n                for ind in indices[i]:\n                    file.write(image_names[ind])\n                file.close()\n                break\n    f.close()\n\n\n# Compute distances and get list of indices\ndef compute_distances(data, neighbors):\n    print(\'Computing distances...\')\n    nbrs = NearestNeighbors(n_neighbors=neighbors, metric=\'cosine\', algorithm=\'brute\').fit(data)\n    distances, indices = nbrs.kneighbors(data)\n    print distances.shape\n    print indices.shape\n    sys.stdout.flush()\n    return distances, indices\n\n\n# Compute distances and get list of indices (dot product --> Faster)\ndef compute_distances_optim(desc, data):\n    print(\'Computing distances...\')\n    print desc.shape\n    print data.shape\n    dist = np.dot(desc, np.transpose(data))\n    ind = dist[0].argsort()[::-1]\n    return dist[0], ind\n\n\n# Compute mAP\ndef evaluate_oxford(ranking_path):\n    print(\'Ranking and Evaluating Oxford...\')\n    #  queries\n    path_gt = ""/data/jim011/datasets_retrieval/Oxford5k/ground_truth/""\n    query_names = [""all_souls"", ""ashmolean"", ""balliol"", ""bodleian"", ""christ_church"", ""cornmarket"",""hertford"",""keble"",""magdalen"",""pitt_rivers"",""radcliffe_camera""]\n    r_n = int(np.random.rand() * 100)\n    file_temp = str(r_n) + \'_ox_tmp.txt\'\n    ap_list = list()\n    for query_name in query_names:\n        for i in range(1,6):\n            f = open(path_gt + query_name + \'_\' + str(i)+\'_query.txt\').readline()\n            f = f.replace(""oxc1_"", """")\n            f_list = f.split("" "")\n            f = f_list[0]\n            print f\n            cmd = ""./compute_ap {} {}{}.txt > {}"".format(path_gt+query_name + \'_\' + str(i), ranking_path, f,file_temp)\n            # print cmd\n            # execute command\n            os.system(cmd)\n            # retrieve result\n            ap = np.loadtxt(file_temp, dtype=\'float32\')\n            print (\'AP: \', ap)\n            ap_list.append(ap)\n\n    ap_file = open(ranking_path+\'all_scores_map.txt\', \'w\')\n    for res in ap_list:\n        ap_file.write(str(res)+\'\\n\')\n    mean_ap = sum(ap_list) / len(ap_list)\n    print (""The mean_ap is: "", mean_ap)\n    ap_file.write(\'\\n\\n Mean AP: \')\n    ap_file.write(str(mean_ap))\n    ap_file.close()\n    return mean_ap\n\n\ndef evaluate_paris(ranking_path):\n    print(\'Ranking and Evaluating Paris...\')\n    #  queries\n    path_gt = ""/data/jim011/datasets_retrieval/Paris6k/ground_truth/""\n    query_names = [""defense"", ""eiffel"", ""invalides"", ""louvre"", ""moulinrouge"", ""museedorsay"", ""notredame"", ""pantheon"",\n                   ""pompidou"", ""sacrecoeur"", ""triomphe""]\n    ap_list = list()\n    r_n = int(np.random.rand() * 100)\n    file_temp = str(r_n) + \'_par_tmp.txt\'\n    for query_name in query_names:\n        for i in range(1, 6):\n            f = open(path_gt + query_name + \'_\' + str(i)+\'_query.txt\').readline()\n            #f = f.replace("""", """")\n            f_list = f.split("" "")\n            f = f_list[0]\n            print f\n            cmd = ""./compute_ap {} {}{}.txt > {}"".format(path_gt+query_name + \'_\' + str(i), ranking_path, f,file_temp)\n            # print cmd\n            # execute command\n            os.system(cmd)\n            # retrieve result\n            ap = np.loadtxt(file_temp, dtype=\'float32\')\n            print (\'AP: \', ap)\n            ap_list.append(ap)\n\n    ap_file = open(ranking_path + \'all_scores_map.txt\', \'w\')\n    for res in ap_list:\n        ap_file.write(str(res) + \'\\n\')\n    mean_ap = sum(ap_list) / len(ap_list)\n    print (""The mean_ap is: "", mean_ap)\n    ap_file.write(\'\\n\\n Mean AP: \')\n    ap_file.write(str(mean_ap))\n    ap_file.close()\n    return mean_ap\n\n\n# Best Images Plot\ndef show_images_top(n_images, dataset):\n    if dataset == \'Oxford\':\n        query_list = open(\'/home/jim011/workspace/retrieval-2017-icmr/lists/queries_list_oxford.txt\')\n        images_path = \'/data/jim011/datasets_retrieval/Oxford5k/\'\n        ranking_path = \'/imatge/ajimenez/workspace/ITR/results/ranked_oxford_RMAC_PCA/\'\n\n    elif dataset == \'Paris\':\n        query_list = open(\'/home/jim011/workspace/retrieval-2017-icmr/lists//queries_list_paris.txt\')\n\n    for query in query_list:\n        count = 1\n        fig = plt.figure()\n        f = open(ranking_path+query+\'.txt\')\n        for line in f:\n            if count <= nimages:\n                line = line.rstrip(\'\\n\')\n                fig.add_subplot(1, n_images, count)\n                plt.imshow(imresize(imread(images_path+line+\'.jpg\'), (128, 128)))\n                count += 1\n            else:\n                plt.savefig(\'/imatge/ajimenez/work/\'+query + \'.png\')\n                print \'saving images\'\n                break\n\n\n# Statistics about ranking\ndef show_stats(dataset, results_path):\n\n    if dataset == \'Oxford\':\n        path_gt = ""/data/jim011/datasets_retrieval/Oxford5k/ground_truth/""\n        query_names = [""all_souls"", ""ashmolean"", ""balliol"", ""bodleian"", ""christ_church"", ""cornmarket"", ""hertford"",\n                       ""keble"",\n                       ""magdalen"", ""pitt_rivers"", ""radcliffe_camera""]\n    elif dataset == \'Paris\':\n        path_gt = ""/data/jim011/datasets_retrieval/Paris6k/ground_truth/""\n        query_names = [""defense"", ""eiffel"", ""invalides"", ""louvre"", ""moulinrouge"", ""museedorsay"", ""notredame"",\n                       ""pantheon"",\n                       ""pompidou"", ""sacrecoeur"", ""triomphe""]\n\n    for query in query_names:\n        for i in range(1, 6):\n            f = open(path_gt + query + \'_\' + str(i)+\'_query.txt\').readline()\n            if dataset == \'Oxford\':\n                q = f.replace(""oxc1_"", """")\n                f_list = q.split("" "")\n            else:\n                f_list = f.split("" "")\n            q = f_list[0]\n            print q + \':\'\n            print\n            f_good = open(path_gt+query+\'_\'+str(i)+\'_good.txt\')\n            f_ok = open(path_gt+query+\'_\'+str(i)+\'_ok.txt\')\n            f_junk = open(path_gt+query+\'_\'+str(i)+\'_junk.txt\')\n            print \'Good Images\'\n            counter_total_img = 0\n            count_100 = 0\n            count_1000 = 0\n            count_more = 0\n            count_25 = 0\n            for line_good in f_good:\n                counter_total_img += 1\n                line_good = line_good.replace(\'\\n\',\'\')\n                f_res = open(results_path + q + \'.txt\')\n                for i, line_res in enumerate(f_res):\n                    if line_good == line_res.replace(\'\\n\',\'\'):\n                        if i >= 1000:\n                            #print line_good + \'in position \' + str(i) + \'<<<<<<<<<\'\n                            count_more += 1\n                        elif i >= 100:\n                            count_1000 +=1\n                            #print line_good + \'in position \' + str(i) + \'<<<<<<\'\n                        elif i >= 25:\n                            count_100 += 1\n                            #print line_good + \'in position \' + str(i) + \'<<<\'\n                        else:\n                            count_25 += 1\n                            #print line_good + \'in position \' + str(i)\n            print \'Stats: \'\n            print \'Total images good: \', counter_total_img\n            print \'Images in top 25: \', count_25\n            print \'Images between 25-100: \', count_100\n            print \'Images between 100-1000: \', count_1000\n            print \'Images above rank 1000: \', count_more\n            print \'Ok images\'\n            counter_total_img = 0\n            count_100 = 0\n            count_1000 = 0\n            count_more = 0\n            count_25 = 0\n            for line_ok in f_ok:\n                counter_total_img += 1\n                line_ok = line_ok.replace(\'\\n\', \'\')\n                f_res = open(results_path + q + \'.txt\')\n                for i, line_res in enumerate(f_res):\n                    if line_ok == line_res.replace(\'\\n\',\'\'):\n                        if i >= 1000:\n                            #print line_ok + \'in position \' + str(i) + \'<<<<<<<<<\'\n                            count_more += 1\n                        elif i >= 100:\n                            count_1000 += 1\n                            #print line_ok + \'in position \' + str(i) + \'<<<<<<\'\n                        elif i >= 25:\n                            count_100 += 1\n                            #print line_ok + \'in position \' + str(i) + \'<<<\'\n                        else:\n                            count_25 += 1\n                            #print line_ok + \'in position \' + str(i)\n            print \'Stats: \'\n            print \'Total images ok: \', counter_total_img\n            print \'Images in top 25: \', count_25\n            print \'Images between 25-100: \', count_100\n            print \'Images between 100-1000: \', count_1000\n            print \'Images above rank 1000: \', count_more\n\n            print \'##########################################################################################\'\n\n\ndef compare_scores(list_1_path, list_2_path):\n    f_1 = open(list_1_path, \'r\')\n    f_2 = open(list_2_path, \'r\')\n    for i in range(0, 55):\n        dif = float(f_1.readline()) - float(f_2.readline())\n        print dif\n\n\n\n\n\n\n\n\n'"
keras_code/pooling_functions.py,0,"b""import cam_utils as cu\nimport numpy as np\nimport time\nfrom sklearn.decomposition import PCA\nimport sys\nimport utils as ud\nimport time\nfrom crow import compute_crow_channel_weight\n\n\ndef compute_pca(descriptors, pca_dim=512, whiten=True):\n    print descriptors.shape\n    t1 = time.time()\n    print 'Computing PCA with dimension reduction to: ', pca_dim\n    sys.stdout.flush()\n    pca = PCA(n_components=pca_dim, whiten=whiten)\n    pca.fit(descriptors)\n    print pca.components_.shape\n    print 'PCA finished!'\n    print 'Time elapsed computing PCA: ', time.time() - t1\n    return pca\n\n\ndef sum_pooling(features, CroW=False):\n    num_samples = features.shape[0]\n    num_features = features.shape[1]\n    sys.stdout.flush()\n    descriptors = np.zeros((num_samples, num_features), dtype=np.float32)\n    for i in range(0, num_samples):\n        if CroW:\n            C = np.array(compute_crow_channel_weight(features[i]))\n        for f in range(0, num_features):\n            descriptors[i, f] = features[i, f].sum()\n        if CroW:\n            descriptors[i] *= C\n    descriptors /= np.linalg.norm(descriptors, axis=1)[:, None]\n    return descriptors\n\n\ndef max_pooling(features):\n    num_samples = features.shape[0]\n    num_features = features.shape[1]\n    sys.stdout.flush()\n    descriptors = np.zeros((num_samples, num_features), dtype=np.float32)\n    for i in range(0, num_samples):\n        for f in range(0, num_features):\n            descriptors[i, f] = np.amax(features[i, f])\n    descriptors /= np.linalg.norm(descriptors, axis=1)[:, None]\n    return descriptors\n\n\n# Do complete aggregation, class vectors + aggregation\ndef weighted_pooling(features, cams, max_pool=False, region_descriptors=False, pca=None, q=False):\n    t = time.time()\n    num_samples = features.shape[0]\n    num_features = features.shape[1]\n    num_features_des = features.shape[1]\n    num_classes = cams.shape[1]\n\n    if pca != '':\n        print 'Applying PCA...'\n        sys.stdout.flush()\n        num_features_des = int(pca.components_.shape[0])\n        print pca.components_.shape[0]\n\n    wp_batch_representations = np.zeros((num_samples, num_features_des), dtype=np.float32)\n    wp_regions = np.zeros((num_features, num_classes), dtype=np.float32)\n    wsp_descriptors_reg = np.zeros((num_samples * num_classes, num_features), dtype=np.float32)\n    wmp_descriptors_reg = np.zeros((num_samples * num_classes, num_features), dtype=np.float32)\n\n    if max_pool:\n        mp_regions = np.zeros((num_features, num_classes), dtype=np.float32)\n        mp_batch_representations = np.zeros((num_samples, num_features), dtype=np.float32)\n\n    for i in range(0, num_samples):\n        #CROW\n        C = np.array(compute_crow_channel_weight(features[i]))\n\n        for f in range(0, num_features):\n            for k in range(0, num_classes):\n                # For each region compute avg weighted sum of activations and l2 normalize\n                if not q:\n                    if max_pool:\n                        mp_regions[f, k] = np.amax(np.multiply(features[i, f], cams[i, k]))\n\n                    wp_regions[f, k] = np.multiply(features[i, f], cams[i, k]).sum()\n                else:\n                    if max_pool:\n                        mp_regions[f, k] = np.amax(features[i, f])\n\n                    wp_regions[f, k] = features[i, f].sum()\n\n        wp_regions = wp_regions * C[:, None]\n        wp_regions /= np.linalg.norm(wp_regions, axis=0)\n\n        if max_pool:\n            mp_regions = mp_regions * C[:, None]\n            mp_regions /= np.linalg.norm(mp_regions, axis=0)\n\n        if region_descriptors:\n            wsp_descriptors_reg[num_classes*i:num_classes*(i+1)] = np.transpose(wp_regions)\n            if max_pool:\n                wmp_descriptors_reg[num_classes*i:num_classes*(i+1)] = np.transpose(mp_regions)\n\n        if pca is not None:\n            wp_regions = np.transpose(pca.transform(np.transpose(wp_regions)))\n            wp_regions /= np.linalg.norm(wp_regions, axis=0)\n            mp_regions = np.transpose(pca.transform(np.transpose(mp_regions)))\n            mp_regions /= np.linalg.norm(mp_regions, axis=0)\n\n\n        wp_batch_representations[i] = wp_regions.sum(axis=1)\n        wp_batch_representations[i] /= np.linalg.norm(wp_batch_representations[i])\n\n        #wp_batch_representations[i][np.where(wp_batch_representations[i] < 0.001)] = 0\n\n        if max_pool:\n            mp_batch_representations[i] = mp_regions.sum(axis=1)\n            mp_batch_representations[i] /= np.linalg.norm(mp_batch_representations[i])\n\n    print 'Time elapsed computing image representations for the batch: ', time.time() - t\n\n    if region_descriptors and max_pool:\n        print wp_batch_representations.shape\n        print wsp_descriptors_reg.shape\n        return wp_batch_representations, mp_batch_representations, wsp_descriptors_reg, wmp_descriptors_reg\n    elif region_descriptors:\n        return wp_batch_representations, wsp_descriptors_reg\n    elif max_pool:\n        return wp_batch_representations, mp_batch_representations\n    else:\n        return wp_batch_representations\n\n# Return class vectors (1 per class)\ndef weighted_cam_pooling(features, cams, channel_weights=True):\n    '''\n    :param features: Feature Maps\n    :param cams: Class Activation Maps\n    :param channel_weights: Channel Weighting as in Crow\n    :return: A descriptor for each CAM.\n    '''\n    num_samples = features.shape[0]\n    num_features = features.shape[1]\n    num_classes = cams.shape[1]\n\n    wp_regions = np.zeros((num_features, num_classes), dtype=np.float32)\n    wsp_descriptors_reg = np.zeros((num_samples * num_classes, num_features), dtype=np.float32)\n\n    for i in range(0, num_samples):\n        # CROW\n        if channel_weights:\n            C = np.array(compute_crow_channel_weight(features[i]))\n\n        for f in range(0, num_features):\n            for k in range(0, num_classes):\n                # For each region compute avg weighted sum of activations and l2 normalize\n                wp_regions[f, k] = np.multiply(features[i, f], cams[i, k]).sum()\n\n        if channel_weights:\n            wp_regions = wp_regions * C[:, None]\n        wp_regions /= np.linalg.norm(wp_regions, axis=0)\n\n        wsp_descriptors_reg[num_classes * i:num_classes * (i + 1)] = np.transpose(wp_regions)\n\n    # print 'Time elapsed computing image representations for the batch: ', time.time() - t\n    else:\n        return wsp_descriptors_reg\n\n\n# General Descriptor Aggregation : PCA + Aggregation\ndef descriptor_aggregation(descriptors_cams, num_images, num_classes, pca=None):\n\n    num_classes_ori = descriptors_cams.shape[0] / num_images\n    descriptors = np.zeros((num_images, descriptors_cams.shape[1]), dtype=np.float32)\n\n    if pca is not None:\n        # Sometimes we may have errors during re-ranking due to bounding box generation on places where CAM=0\n        try:\n            descriptors_pca = pca.transform(descriptors_cams)\n        except:\n            print '---------------------------->Exception'\n            desc_err = np.zeros((descriptors_cams.shape[0], descriptors_cams.shape[1]), dtype=np.float32)\n            for j in range(0, descriptors_cams.shape[0]):\n                try:\n                    desc_err[j] = pca.transform(descriptors_cams[j].reshape(1,-1))\n                except:\n                    print '------------------> Exception'\n                    print j\n                    desc_err[j] = desc_err[j-1]\n            descriptors_pca = desc_err\n\n        descriptors = np.zeros((num_images, descriptors_pca.shape[1]), dtype=np.float32)\n        #print descriptors_pca.shape\n\n    index = 0\n    for i in range(0, num_images):\n        index = num_classes_ori + index\n        if i == 0:\n            index = 0\n        if pca is not None:\n            for k in range(index, index+num_classes):\n                descriptors_pca[k] /= np.linalg.norm(descriptors_pca[k])\n                descriptors[i] += descriptors_pca[k]\n\n            descriptors[i] /= np.linalg.norm(descriptors[i])\n        else:\n            for k in range(index, index+num_classes):\n                descriptors[i] += descriptors_cams[k]\n            descriptors[i] /= np.linalg.norm(descriptors[i])\n\n    return descriptors\n\n\n# Descriptor aggregation from list of classes : PCA + Aggregation\ndef descriptor_aggregation_cl(descriptors_cams, num_images, pca, class_list):\n    num_classes_ori = descriptors_cams.shape[0] / num_images\n    descriptors = np.zeros((num_images, 512), dtype=np.float32)\n    for i in range(0, num_images):\n        descriptors_good = descriptors_cams[i*num_classes_ori:(i+1)*num_classes_ori]\n        descriptors_good = descriptors_good[class_list]\n        # Sometimes we may have errors during re-ranking due to bounding box generation on places where CAM=0\n        if pca is not None:\n            try:\n                descriptors_pca = pca.transform(descriptors_good)\n            except:\n                print '---------------------------->Exception'\n                desc_err = np.zeros((descriptors_good.shape[0], descriptors_good.shape[1]), dtype=np.float32)\n                for j in range(0, descriptors_good.shape[0]):\n                    try:\n                        desc_err[j] = pca.transform(descriptors_good[j].reshape(1,-1))\n                    except:\n                        print '------------------> Exception'\n                        print j\n                        print desc_err[j].shape\n                        desc_err[j] = desc_err[j - 1]\n                descriptors_pca = desc_err\n        else:\n            descriptors_pca = descriptors_good\n        for k in range(0, descriptors_pca.shape[0]):\n            descriptors_pca[k] /= np.linalg.norm(descriptors_pca[k])\n            descriptors[i] += descriptors_pca[k]\n\n        descriptors[i] /= np.linalg.norm(descriptors[i])\n\n    return descriptors\n\n\n# Retrieve n most probable class vectors (For PCA mainly)\ndef retrieve_n_descriptors(num_classes, num_images, all_descriptors):\n    num_classes_ori = all_descriptors.shape[0] / num_images\n    descriptors = np.zeros((num_images * num_classes, all_descriptors.shape[1]), dtype=np.float32)\n\n    index = 0\n    for i in range(0, num_images):\n        index = num_classes_ori + index\n        if i == 0:\n            index = 0\n        descriptors[i*num_classes:num_classes*(i+1)] = all_descriptors[index:index+num_classes]\n    return descriptors\n\n\n# Retrieve n class vectors (For PCA mainly)\ndef retrieve_n_descriptors_2(class_vector, num_images, all_descriptors):\n    num_classes_ori = all_descriptors.shape[0] / num_images\n    descriptors = np.zeros((num_images * class_vector.shape[0], all_descriptors.shape[1]), dtype=np.float32)\n    for i in range(0, num_images):\n        descriptors[i*class_vector.shape[0]:class_vector.shape[0]*(i+1)] = \\\n            all_descriptors[i * num_classes_ori:(i + 1)*num_classes_ori][class_vector]\n    return descriptors\n\n# # Return class vectors (1 per class)\n# def weighted_cam_pooling(features, cams, max_pool=False, channel_weights=True):\n#     '''\n#     :param features: Feature Maps\n#     :param cams: Class Activation Maps\n#     :param max_pool: Perform also Max pooling\n#     :param channel_weights: Channel Weighting as in Crow\n#     :return: A descriptor for each CAM.\n#     '''\n#     t = time.time()\n#     num_samples = features.shape[0]\n#     num_features = features.shape[1]\n#     num_classes = cams.shape[1]\n#\n#     wp_regions = np.zeros((num_features, num_classes), dtype=np.float32)\n#     wsp_descriptors_reg = np.zeros((num_samples * num_classes, num_features), dtype=np.float32)\n#     wmp_descriptors_reg = np.zeros((num_samples * num_classes, num_features), dtype=np.float32)\n#\n#     if max_pool:\n#         mp_regions = np.zeros((num_features, num_classes), dtype=np.float32)\n#\n#     for i in range(0, num_samples):\n#         #CROW\n#         if channel_weights:\n#             C = np.array(compute_crow_channel_weight(features[i]))\n#\n#         for f in range(0, num_features):\n#             for k in range(0, num_classes):\n#                 # For each region compute avg weighted sum of activations and l2 normalize\n#                 if max_pool:\n#                         mp_regions[f, k] = np.amax(np.multiply(features[i, f], cams[i, k]))\n#                 wp_regions[f, k] = np.multiply(features[i, f], cams[i, k]).sum()\n#\n#         if channel_weights:\n#             wp_regions = wp_regions * C[:, None]\n#         wp_regions /= np.linalg.norm(wp_regions, axis=0)\n#\n#         if max_pool:\n#             if channel_weights:\n#                 mp_regions = mp_regions * C[:, None]\n#             mp_regions /= np.linalg.norm(mp_regions, axis=0)\n#\n#         wsp_descriptors_reg[num_classes*i:num_classes*(i+1)] = np.transpose(wp_regions)\n#\n#         if max_pool:\n#             wmp_descriptors_reg[num_classes*i:num_classes*(i+1)] = np.transpose(mp_regions)\n#\n#     #print 'Time elapsed computing image representations for the batch: ', time.time() - t\n#\n#     if max_pool:\n#         return wsp_descriptors_reg, wmp_descriptors_reg\n#     else:\n#         return wsp_descriptors_reg\n"""
keras_code/reranking.py,0,"b""import numpy as np\r\nimport os\r\nimport h5py\r\nimport sys\r\nimport utils as utils\r\nimport time\r\nfrom vgg_cam import vggcam\r\nfrom utils import create_folders, save_data, preprocess_images, load_data\r\nfrom pooling_functions import weighted_cam_pooling, descriptor_aggregation\r\nfrom cam_utils import extract_feat_cam_fast\r\nfrom scipy.misc import imread\r\nimport math\r\n\r\n\r\n# Image Preprocessing\r\nsize_v = [720, 1024]\r\nsize_h = [1024, 720]\r\n\r\nmean_value = [123.68, 116.779, 103.939]\r\n\r\ndim_descriptors = 512\r\n\r\nn_images_oxford = 5063\r\nn_images_paris = 6392\r\n\r\n# Compute score using CAMs, PCA , Region of interest\r\ndef compute_scores_cams(desc_query, features_img, cams, roi, pca_matrix):\r\n    #print 'Feat shape:', features_img.shape\r\n    #print 'Cams shape', cams.shape\r\n    nthres = 4\r\n    scores = np.zeros(features_img.shape[0])\r\n    feats = np.zeros((1, features_img.shape[1], features_img.shape[2], features_img.shape[3]), dtype=np.float32)\r\n    cams_ = np.zeros((1, cams.shape[1], cams.shape[2], cams.shape[3]), dtype=np.float32)\r\n    final_descriptors = np.zeros((features_img.shape[0], features_img.shape[1]), dtype=np.float32)\r\n    for img_ind in range(features_img.shape[0]):\r\n        #print features_img[img_ind].shape\r\n        feats[0] = features_img[img_ind]\r\n        cams_[0] = cams[img_ind]\r\n        scores[img_ind] = -10\r\n        #print 'Img: ', img_ind\r\n        x, y, w, h = roi[img_ind, :, 0], roi[img_ind, :, 1], roi[img_ind, :, 2], roi[img_ind, :, 3]\r\n        for th in range(0, nthres):\r\n            #print y[th], y[th] + h[th]\r\n            #print x[th], x[th] + w[th]\r\n            sys.stdout.flush()\r\n            if h[th] >= 5 and w[th] >= 5:\r\n                d_wp = weighted_cam_pooling(feats[:, :,y[th]:y[th]+h[th], x[th]:x[th]+w[th]],\r\n                                            cams_[:, :, y[th]:y[th]+h[th], x[th]:x[th]+w[th]])\r\n                descriptor = descriptor_aggregation(d_wp, 1, cams.shape[1], pca_matrix)\r\n\r\n                score_aux = np.dot(desc_query, np.transpose(descriptor))\r\n\r\n                #print 'Thresh: ', th\r\n                #print 'Score:', score_aux\r\n                if score_aux > scores[img_ind]:\r\n                    #print 'Max in th:', th\r\n                    scores[img_ind] = np.copy(score_aux)\r\n                    final_descriptors[img_ind] = descriptor\r\n            else:\r\n                pass\r\n    return scores, final_descriptors\r\n\r\n\r\ndef re_order(order, vector_h, vector_v):\r\n    vector = list()\r\n    count_h = 0\r\n    count_v = 0\r\n    for pos in order:\r\n        if pos == 0:\r\n            vector.append(vector_h[count_h])\r\n            count_h += 1\r\n        elif pos == 1:\r\n            vector.append(vector_v[count_v])\r\n            count_v += 1\r\n    return vector\r\n\r\n\r\ndef re_ranking(desc_query, class_list, batch_size, image_names, indices, dataset, top_n_ranking, pca_matrix, model, layer, fnc):\r\n    if dataset == 'Oxford' or dataset == 'Oxford105k':\r\n        images_path = '/data/jim011/datasets_retrieval/Oxford5k/images/'\r\n\r\n    if dataset == 'Paris' or dataset == 'Paris106k':\r\n        images_path = '/data/jim011/datasets_retrieval/Paris6k/images/'\r\n\r\n    index_q = indices[0:top_n_ranking]\r\n    tt = time.time()\r\n    indexed_names = list()\r\n    i = 0\r\n    if top_n_ranking >= 1000:\r\n        image_batch = 300\r\n    else:\r\n        image_batch = top_n_ranking\r\n\r\n    n_iterations = int(math.floor(top_n_ranking / image_batch))\r\n    last_batch = top_n_ranking % image_batch\r\n    scores = np.zeros(top_n_ranking, dtype=np.float32)\r\n    scores_h = np.zeros(top_n_ranking, dtype=np.float32)\r\n    scores_v = np.zeros(top_n_ranking, dtype=np.float32)\r\n    final_desc_h = np.zeros(top_n_ranking, dtype=np.float32)\r\n    final_desc_v = np.zeros(top_n_ranking, dtype=np.float32)\r\n    #print desc_query.shape\r\n    final_descriptors_all = np.zeros((top_n_ranking, desc_query.shape[1]), dtype=np.float32)\r\n    image_ranked_names = image_names[index_q]\r\n\r\n    num_cams = class_list.shape[0]\r\n\r\n    for k in range(0, n_iterations+1):\r\n        images_h = list()\r\n        images_v = list()\r\n        images_ver = False\r\n        images_hor = False\r\n        t1 = time.time()\r\n        if k == n_iterations:\r\n            #Last Batch\r\n            if last_batch != 0:\r\n                last_ind = image_batch * k + last_batch\r\n            else:\r\n                break\r\n        else:\r\n            last_ind = image_batch * (k+1)\r\n\r\n        print image_names[index_q[k*image_batch:last_ind]]\r\n\r\n        # Separate the images in Horizontal/Vertical for faster processing\r\n        image_order = list()\r\n        for ind_im, name in enumerate(image_names[index_q[k*image_batch:last_ind]]):\r\n            if name[0] == '/':\r\n                im = imread(name.replace('\\n',''))\r\n            else:\r\n                im = imread(images_path + name.replace('\\n', '') + '.jpg')\r\n            if im.shape[0] >= im.shape[1]:\r\n                images_v.append(im)\r\n                images_ver = True\r\n                image_order.append(1)\r\n            else:\r\n                images_h.append(im)\r\n                images_hor = True\r\n                image_order.append(0)\r\n\r\n        # Extract Features/CAMs\r\n        print 'Time loading images: ', time.time() - t1\r\n\r\n        if images_hor:\r\n            size = size_h\r\n            t2 = time.time()\r\n            images_h_prep = preprocess_images(images_h, size[0], size[1], mean_value)\r\n            features_h, cams_h, roi_h = extract_feat_cam_fast(model, fnc,  layer, batch_size, images_h_prep, num_cams,\r\n                                                         class_list, roi=True)\r\n\r\n\r\n            print 'Time extracting features: ', time.time() - t2\r\n            t3 = time.time()\r\n            scores_h, final_desc_h = compute_scores_cams(desc_query, features_h, cams_h, roi_h, pca_matrix)\r\n            print 'Time computing scores: ', time.time() - t3\r\n            print scores_h\r\n\r\n        if images_ver:\r\n            size = size_v\r\n            t2 = time.time()\r\n            images_v_prep = preprocess_images(images_v, size[0], size[1], mean_value)\r\n            features_v, cams_v, roi_v = extract_feat_cam_fast(model, fnc, layer, batch_size, images_v_prep, num_cams,\r\n                                                         class_list, roi=True)\r\n\r\n            print 'Time extracting features: ', time.time() - t2\r\n\r\n            t3 = time.time()\r\n            scores_v, final_desc_v = compute_scores_cams(desc_query, features_v, cams_v, roi_v, pca_matrix)\r\n            print 'Time computing scores: ', time.time() - t3\r\n            print scores_v\r\n\r\n        # Compute Scores\r\n        print image_order\r\n        # Re-order\r\n        scores[k*image_batch:last_ind] = re_order(image_order, scores_h, scores_v)\r\n        final_descriptors_all[k*image_batch:last_ind] = re_order(image_order, final_desc_h, final_desc_v)\r\n        #print final_descriptors_all.shape\r\n\r\n        print scores[k*image_batch:image_batch*(k+1)]\r\n        print 'Time loading computing scores: ', time.time() - t2\r\n        print 'Time elapsed x image:', time.time() - t1\r\n\r\n    print scores\r\n    ordered_sc = scores.argsort()[::-1]\r\n    print ordered_sc\r\n    print image_names[index_q]\r\n    print image_ranked_names[ordered_sc]\r\n    # Index of the in order of relevance\r\n    ordered_ind = index_q[ordered_sc]\r\n    indexed_names.append(np.copy(image_ranked_names[ordered_sc]))\r\n    indices[0:top_n_ranking] = ordered_ind\r\n    i += 1\r\n    print 'Time elapsed:', time.time()-tt\r\n    # Return indices and data ordered by similarity with the query\r\n    return indices, final_descriptors_all[ordered_sc]"""
keras_code/utils.py,0,"b'from scipy.misc import imread, imresize, imsave\nimport numpy as np\nimport os\nimport math\nimport h5py\nimport matplotlib.pyplot as plt\nimport sys\n\n\ndef create_folders(path):\n    if not os.path.exists(path):\n        print \'Creating path: \', path\n        os.makedirs(path)\n    else:\n        print \'Path already exists\'\n\n\ndef load_data(filepath):\n    with h5py.File(filepath, \'r\') as hf:\n        data = np.array(hf.get(\'data\'))\n        #print \'Shape of the array features: \', data.shape\n        return data\n\n\ndef save_data(data, path, name):\n    with h5py.File(path + name, \'w\') as hf:\n        hf.create_dataset(\'data\', data=data)\n\n\ndef preprocess_images(images, img_width, img_height, mean_value):\n    if isinstance(images, list):\n        num_images = len(images)\n        print ""Preprocessing "" + str(num_images) + \' Images...\'\n        x = np.zeros((num_images, 3, img_height, img_width), dtype=np.float32)\n\n        for i in range(0, num_images):\n            images[i] = imresize(images[i], [img_height, img_width]).astype(dtype=np.float32)\n\n            # RGB -> BGR\n            R = np.copy(images[i][:, :, 0])\n            B = np.copy(images[i][:, :, 2])\n            images[i][:, :, 0] = B\n            images[i][:, :, 2] = R\n\n            # Subtract mean\n            images[i][:, :, 0] -= mean_value[0]\n            images[i][:, :, 1] -= mean_value[1]\n            images[i][:, :, 2] -= mean_value[2]\n\n            x[i, :, :, :] = np.transpose(images[i], (2, 0, 1))\n        return x\n\n    else:\n        print \'Preprocessing Image...\'\n        images = imresize(images, [img_height, img_width]).astype(dtype=np.float32)\n        x = np.zeros((1, 3, img_height, img_width), dtype=np.float32)\n\n        # RGB -> BGR\n        R = np.copy(images[:, :, 0])\n        B = np.copy(images[:, :, 2])\n        images[:, :, 0] = B\n        images[:, :, 2] = R\n\n        # Subtract mean\n        images[:, :, 0] -= mean_value[0]\n        images[:, :, 1] -= mean_value[1]\n        images[:, :, 2] -= mean_value[2]\n\n        x[0] = np.transpose(images, (2, 0, 1))\n        return x\n\n\ndef print_classes(dictionary_labels, vector_classes):\n    class_list = list()\n    for vc in vector_classes:\n        print dictionary_labels[vc]\n        class_list.append(dictionary_labels[vc])\n    return class_list\n'"
keras_code/vgg_cam.py,0,"b'import keras.backend as K\nfrom keras.models import Sequential\nfrom keras.layers.core import Flatten, Dense\nfrom keras.layers.convolutional import Convolution2D, Cropping2D\nfrom keras.layers.pooling import MaxPooling2D\nfrom keras.layers.pooling import AveragePooling2D, GlobalAveragePooling2D\nfrom keras.layers.convolutional import ZeroPadding2D\nimport matplotlib.pylab as plt\nimport numpy as np\nimport theano.tensor.nnet.abstract_conv as absconv\nimport cv2\nimport h5py\nimport os\n\n\ndef vggcam(nb_classes, input_shape=(3, None, None), num_input_channels=1024):\n    \'\'\'\n    :param nb_classes: # classes (IMAGENET = 1000)\n    :param input_shape: image shape\n    :param num_input_channels: channels CAM layer\n    :param bounding_box:  Query processing (Oxford/Paris)\n    :return: instance of the model VGG-16 CAM\n    \'\'\'\n\n    model = Sequential()\n    model.add(ZeroPadding2D((1, 1), input_shape=input_shape))\n    model.add(Convolution2D(64, 3, 3, activation=\'relu\'))\n    model.add(ZeroPadding2D((1, 1)))\n    model.add(Convolution2D(64, 3, 3, activation=\'relu\'))\n    model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n\n    model.add(ZeroPadding2D((1, 1)))\n    model.add(Convolution2D(128, 3, 3, activation=\'relu\'))\n    model.add(ZeroPadding2D((1, 1)))\n    model.add(Convolution2D(128, 3, 3, activation=\'relu\'))\n    model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n\n    model.add(ZeroPadding2D((1, 1)))\n    model.add(Convolution2D(256, 3, 3, activation=\'relu\'))\n    model.add(ZeroPadding2D((1, 1)))\n    model.add(Convolution2D(256, 3, 3, activation=\'relu\'))\n    model.add(ZeroPadding2D((1, 1)))\n    model.add(Convolution2D(256, 3, 3, activation=\'relu\'))\n    model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n\n    model.add(ZeroPadding2D((1, 1)))\n    model.add(Convolution2D(512, 3, 3, activation=\'relu\'))\n    model.add(ZeroPadding2D((1, 1)))\n    model.add(Convolution2D(512, 3, 3, activation=\'relu\'))\n    model.add(ZeroPadding2D((1, 1)))\n    model.add(Convolution2D(512, 3, 3, activation=\'relu\'))\n    model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n\n    model.add(ZeroPadding2D((1, 1)))\n    model.add(Convolution2D(512, 3, 3, activation=\'relu\', name=\'relu5_1\'))\n    model.add(ZeroPadding2D((1, 1)))\n    model.add(Convolution2D(512, 3, 3, activation=\'relu\'))\n    model.add(ZeroPadding2D((1, 1)))\n    model.add(Convolution2D(512, 3, 3, activation=\'relu\', name=\'relu5_3\'))\n\n    # Add another conv layer with ReLU + GAP\n    model.add(Convolution2D(num_input_channels, 3, 3, activation=\'relu\', border_mode=""same"", name=\'CAM_relu\'))\n\n    # Global Average Pooling\n    model.add(GlobalAveragePooling2D(name=\'CAM_pool\'))\n\n    # Add the W layer\n    model.add(Dense(nb_classes, activation=\'softmax\'))\n\n    model.name = ""vgg_cam""\n\n    return model\n\n'"
pytorch_code/A_Feature_Extraction.py,8,"b'from scipy.misc import imread\nimport sys\nimport getopt\nimport time\nimport h5py\nimport numpy as np\nfrom cam_functions import extract_feat_cam_all, extract_feat_cam\nfrom utils import create_folders, save_data, preprocess_images\nfrom pooling_functions import weighted_cam_pooling, sum_pooling\nimport torch\nimport torchvision\nimport torch.nn.parallel\nimport torch.backends.cudnn as cudnn\nimport resnet\nimport densenet\n\n# Instructions Arguments: python script.py -d \'Oxford/Paris\' -a \'Offline/Online - \'DenseNet161/ResNet50\'\n\ntry:\n    opts, args = getopt.getopt(sys.argv[1:], ""a:d:m:"")\n    flag_d = False\n    flag_a = False\n    flag_m = False\nexcept getopt.GetoptError:\n    print \'script.py -d <dataset> -a <aggregation> -m <model_name>\'\n    sys.exit(2)\nfor opt, arg in opts:\n    if opt == \'-d\':\n        if arg == \'Oxford\' or arg == \'Paris\':\n            dataset = arg\n            flag_d = True\n    elif opt == \'-m\':\n        if arg == \'ResNet50\' or arg == \'DenseNet161\':\n            model_name = arg\n            flag_m = True\n    elif opt == ""-a"":\n        if arg == \'Offline\' or arg == \'Online\':\n            aggregation_type = arg\n            flag_a = True\n\n\n# Dataset Selection (Oxford/Paris) - Default\nif not flag_d:\n    dataset = \'Oxford\'\n    print \'Default dataset: \', dataset\n\n# Extract Online or Offline (Online saves 1 file/image) - Default\nif not flag_a:\n    aggregation_type = \'Offline\'\n    print \'Default aggregation: \', aggregation_type\n\nif not flag_m:\n    model_name = \'ResNet50\'\n    print \'Default model: \', model_name\n\n# Image Pre-processing (Size W x H)\n\n# Horizontal Images\nsize_h = [1024, 720]\n# Vertical Images\nsize_v = [720, 1024]\n\ndim = \'1024x720\'\n\n# Mean to substract\nmean_data = \'Imagenet\'\n\nif mean_data == \'Imagenet\':\n    stats = list()\n    mean = [0.485, 0.456, 0.406]\n    std = [0.229, 0.224, 0.225]\n    stats.append(mean)\n    stats.append(std)\n\ncudnn.benchmark = True\n\nif model_name == \'ResNet50\':\n    # ResNet50\n    model = resnet.resnet50(pretrained=True)\n    model = torch.nn.DataParallel(model)\n    dim_descriptor = 2048\n\nelif model_name == \'DenseNet161\':\n    # DenseNet161\n    model = densenet.densenet161(pretrained=True)\n    model.features = torch.nn.DataParallel(model.features)\n    dim_descriptor = 2208\n\nmodel.cuda()\n\nprint model\n\n# CAM Extraction (# CAMs)\n\nif aggregation_type == \'Offline\':\n    num_classes = 64\nelif aggregation_type == \'Online\':\n    num_classes = 1000\n\n# Images to load into the net (+ images, + memory, + fast)\nbatch_size = 50\n# Images to pre-load (+ images, + memory, + fast) (also saves feats & CAMs for this number when saving-CAMs)\nimage_batch_size = 500\n\n# For saving also features & CAMs\nsaving_CAMs = False\n# Index for saving chunks\nind = 0\n\nnb_classes = 1000\n\nif dataset == \'Oxford\':\n    n_img_dataset = 5063\n    train_list_path_h = ""../lists/list_oxford_horizontal_no_queries.txt""\n    train_list_path_v = ""../lists/list_oxford_vertical_no_queries.txt""\n    path_descriptors = \'/flush2/jim011/oxford/descriptors/\' + model_name + \'/\' + dim + \'/\'\n    descriptors_cams_path_wp = path_descriptors + \'oxford_all_\' + str(num_classes) + \'_wp.h5\'\n    descriptors_cams_path_sp = path_descriptors + \'oxford_all\' + \'_sp.h5\'\n    path_images_oxford = \'/data/jim011/datasets_retrieval/Oxford5k/images/\'\n\n    create_folders(path_descriptors)\n\n    if aggregation_type ==\'Online\':\n        path_descriptors += \'online/\'\n\n    # If you want to save features & CAMs\n    #feature_path = \'/imatge/ajimenez/work/ITR/oxford/features/\' + model_name + \'/\' + layer + \'/\' + dim + \'/\'\n    #cam_path = \'/imatge/ajimenez/work/ITR/oxford/cams/\' + model_name + \'/\' + layer + \'/\' + dim + \'/\'\n    # create_folders(feature_path)\n    # create_folders(cam_path)\n\n\nif dataset == \'Paris\':\n    n_img_dataset = 6392\n    train_list_path_h = ""../lists/list_paris_horizontal_no_queries.txt""\n    train_list_path_v = ""../lists/list_paris_vertical_no_queries.txt""\n    path_descriptors = \'/flush2/jim011/paris/descriptors/\' + model_name + \'/\' + dim + \'/\'\n    descriptors_cams_path_wp = path_descriptors + \'paris_all_\' + str(num_classes) + \'_wp.h5\'\n    descriptors_cams_path_sp = path_descriptors + \'paris_all\' + \'_sp.h5\'\n    path_images_paris = \'/data/jim011/datasets_retrieval/Paris6k/images/\'\n\n    if aggregation_type ==\'Online\':\n        path_descriptors += \'online/\'\n\n    create_folders(path_descriptors)\n\n    # If you want to save features & CAMs\n    #feature_path = \'/imatge/ajimenez/work/ITR/paris/features/\' + model_name + \'/\' + layer + \'/\' + dim + \'/\'\n    #cam_path = \'/imatge/ajimenez/work/ITR/paris/cams/\' + model_name + \'/\' + layer + \'/\' + dim + \'/\'\n    #create_folders(feature_path)\n    #create_folders(cam_path)\n\n\ndef extract_cam_descriptors(model, model_name, batch_size, num_classes, size, stats, image_train_list_path,\n                            desc_wp, desc_sp, ind=0):\n    images = [0] * image_batch_size\n    image_names = [0] * image_batch_size\n    counter = 0\n    num_images = 0\n    t0 = time.time()\n\n    print \'Horizontal size: \', size[0]\n    print \'Vertical size: \', size[1]\n\n    for line in open(image_train_list_path):\n        if counter >= image_batch_size:\n            print \'Processing image batch: \', ind\n            t1 = time.time()\n            data = preprocess_images(images, size[0], size[1], stats[0], stats[1])\n            print data.size()\n            data = torch.autograd.Variable(data, volatile=True)\n            if aggregation_type == \'Offline\':\n                features, cams, cl, _ = extract_feat_cam(model, model_name, batch_size, data, num_classes)\n                if saving_CAMs:\n                    save_data(cams, cam_path, \'cams_\' + str(ind) + \'.h5\')\n                    save_data(features, feature_path, \'features_\' + str(ind) + \'.h5\')\n                d_wp = weighted_cam_pooling(features, cams)\n                desc_wp = np.concatenate((desc_wp, d_wp))\n                d_sp = sum_pooling(features)\n                desc_sp = np.concatenate((desc_sp, d_sp))\n\n            elif aggregation_type == \'Online\':\n                features, cams = extract_feat_cam_all(model, model_name, batch_size, data, num_classes)\n                d_wp = weighted_cam_pooling(features, cams)\n                for img_ind in range(0, image_batch_size):\n                    print \'Saved \' + image_names[img_ind] + \'.h5\'\n                    save_data(d_wp[img_ind*nb_classes:(img_ind+1)*nb_classes], path_descriptors,\n                              image_names[img_ind]+\'.h5\')\n\n            print \'Image batch processed, CAMs descriptors obtained!\'\n            print \'Time elapsed: \', time.time()-t1\n            sys.stdout.flush()\n            counter = 0\n            ind += 1\n\n        line = line.rstrip(\'\\n\')\n        images[counter] = imread(line)\n        if dataset == \'Oxford\':\n            line = line.replace(\'/data/jim011/datasets_retrieval/Oxford5k/images/\', \'\')\n        elif dataset == \'Paris\':\n            line = line.replace(\'/data/jim011/datasets_retrieval/Paris6k/images/\', \'\')\n        image_names[counter] = (line.replace(\'.jpg\', \'\'))\n        counter += 1\n        num_images += 1\n\n    #Last batch\n    print \'Last Batch:\'\n    data = preprocess_images(images[0:counter], size[0], size[1], stats[0], stats[1])\n    data = torch.autograd.Variable(data, volatile=True)\n    if aggregation_type == \'Offline\':\n        features, cams, cl, _ = extract_feat_cam(model, model_name, batch_size, data, num_classes)\n        if saving_CAMs:\n            save_data(cams, cam_path, \'cams_\' + str(ind) + \'.h5\')\n            save_data(features, feature_path, \'features_\' + str(ind) + \'.h5\')\n        d_wp = weighted_cam_pooling(features, cams)\n        desc_wp = np.concatenate((desc_wp, d_wp))\n        d_sp = sum_pooling(features)\n        desc_sp = np.concatenate((desc_sp, d_sp))\n\n    elif aggregation_type == \'Online\':\n        features, cams = extract_feat_cam_all(model, model_name, batch_size, data, num_classes)\n        d_wp = weighted_cam_pooling(features, cams)\n        for img_ind in range(0, counter):\n            save_data(d_wp[img_ind * nb_classes:(img_ind + 1) * nb_classes], path_descriptors,\n                      image_names[img_ind] + \'.h5\')\n    ind += 1\n    print desc_wp.shape\n    print \'Batch processed, CAMs descriptors obtained!\'\n    print \'Total time elapsed: \', time.time() - t0\n    sys.stdout.flush()\n\n    return desc_wp, desc_sp\n\n\n########################################################################################################################\n# Main Script\n\nprint \'Dataset: \', dataset\nprint \'Aggregation type \', aggregation_type\nprint \'Num classes: \', num_classes\n\nt_0 = time.time()\ndesc_wp = np.zeros((0, dim_descriptor), dtype=np.float32)\ndesc_sp = np.zeros((0, dim_descriptor), dtype=np.float32)\n\n\n# Horizontal Images\ndesc_wp, desc_sp = \\\n    extract_cam_descriptors(model,model_name, batch_size, num_classes, size_h, stats, train_list_path_h,\n                            desc_wp, desc_sp)\n\n# Vertical Images\ndesc_wp, desc_sp = \\\n    extract_cam_descriptors(model, model_name, batch_size, num_classes, size_v, stats, train_list_path_v,\n                            desc_wp, desc_sp, ind)\n\nprint desc_wp.shape\nprint desc_sp.shape\n\n# Queries\nif dataset == \'Oxford\':\n    i = 0\n    with open(\'../lists/list_queries_oxford.txt\', ""r"") as f:\n        for line in f:\n            print i\n            line = line.replace(\'\\n\', \'\')\n            img = np.array(imread(path_images_oxford + line + \'.jpg\'), dtype=np.float32)\n\n            #img = np.transpose(img, (2, 0, 1))\n            print img.shape\n            if img.shape[0] > img.shape[1]:\n                size = size_v\n            else:\n                size = size_h\n\n            img_tensor = preprocess_images(img, size[0], size[1], stats[0], stats[1])\n\n            img_tensor = torch.autograd.Variable(img_tensor, volatile=True)\n\n            if aggregation_type == \'Offline\':\n                features, cams, cl, _ = extract_feat_cam(model, model_name, 1, img_tensor, num_classes)\n                if saving_CAMs:\n                    save_data(cams, cam_path, \'cams_\' + str(ind) + \'.h5\')\n                    save_data(features, feature_path, \'features_\' + str(ind) + \'.h5\')\n\n                d_wp = weighted_cam_pooling(features, cams)\n                desc_wp = np.concatenate((desc_wp, d_wp))\n                d_sp = sum_pooling(features)\n                desc_sp = np.concatenate((desc_sp, d_sp))\n\n            elif aggregation_type == \'Online\':\n                features, cams = extract_feat_cam_all(model, model_name, feature_layer, 1, img_tensor)\n                d_wp = weighted_cam_pooling(features, cams)\n                print \'Saved \' + line + \'.h5\'\n                save_data(d_wp, path_descriptors, line+\'.h5\')\n            print desc_wp.shape\n            i += 1\n            ind += 1\n\nelif dataset == \'Paris\':\n    i = 0\n    with open(\'../lists/list_queries_paris.txt\', ""r"") as f:\n        for line in f:\n            print i\n            line = line.replace(\'\\n\', \'\')\n            img = np.array(imread(path_images_paris + line + \'.jpg\'), dtype=np.float32)\n\n            # img = np.transpose(img, (2, 0, 1))\n            print img.shape\n            if img.shape[0] > img.shape[1]:\n                size = size_v\n            else:\n                size = size_h\n\n            img_tensor = preprocess_images(img, size[0], size[1], stats[0], stats[1])\n            img_tensor = torch.autograd.Variable(img_tensor, volatile=True)\n\n            if aggregation_type == \'Offline\':\n                features, cams, cl, _ = extract_feat_cam(model, model_name, 1, img_tensor, num_classes)\n                if saving_CAMs:\n                    save_data(cams, cam_path, \'cams_\' + str(ind) + \'.h5\')\n                    save_data(features, feature_path, \'features_\' + str(ind) + \'.h5\')\n                d_wp = weighted_cam_pooling(features, cams)\n                desc_wp = np.concatenate((desc_wp, d_wp))\n                d_sp = sum_pooling(features)\n                desc_sp = np.concatenate((desc_sp, d_sp))\n\n            elif aggregation_type == \'Online\':\n                features, cams = extract_feat_cam_all(model, model_name, feature_layer, 1, img_tensor)\n                d_wp = weighted_cam_pooling(features, cams)\n                print \'Saved \' + line + \'.h5\'\n                save_data(d_wp, path_descriptors, line+\'.h5\')\n\n            i += 1\n            ind += 1\n\nprint \'Saving Data...\'\nprint desc_wp.shape\nprint desc_sp.shape\n# Shape = [num_images * num_classes, dim_descriptor]\nif aggregation_type ==\'Offline\':\n    save_data(desc_wp, descriptors_cams_path_wp, \'\')\n    save_data(desc_sp, descriptors_cams_path_sp, \'\')\nprint \'Data Saved\'\nprint \'Total time elapsed: \', time.time() - t_0'"
pytorch_code/B_Offline_Aggregation.py,6,"b'from cam_functions import extract_feat_cam\nfrom utils import create_folders, save_data, preprocess_images, load_data, print_classes\nfrom pooling_functions import weighted_cam_pooling, descriptor_aggregation, retrieve_n_descriptors, compute_pca, sum_pooling\nfrom scipy.misc import imread\nimport math\nfrom reranking import re_ranking\nimport pickle\nimport resnet\nimport densenet\nimport numpy as np\nimport os\nimport h5py\nimport sys\nimport getopt\nimport evaluate_oxford_paris as eval\nimport time\nimport torch\nimport torchvision\nimport torch.nn.parallel\nimport torch.backends.cudnn as cudnn\n\n\nimagenet_dictionary = pickle.load(open(""../imagenet1000_clsid_to_human.pkl"", ""rb""))\n\n# Instructions Arguments: python script.py -d \'Oxford/Paris\' -nc_q 32 -pca 1 -qe 10 -re 100 -nc_re 6\n\ntry:\n    opts, args = getopt.getopt(sys.argv[1:], \'d:m:\', [\'nc_q=\', \'pca=\', \'qe=\', \'re=\', \'nc_re=\'])\n    flag_nc_q = False\n    flag_pca = False\n    flag_d = False\n    flag_nc_re = False\n    flag_qe = False\n    flag_re = False\n    flag_m = False\nexcept getopt.GetoptError:\n    print \'script.py -d <dataset> --nc_q <nclasses_query> --pca <n_classes_pca> --qe <n_query_exp> --re <n_re_ranking> \' \\\n          \'--nc_re <n_classes_re_ranking> -m <model_name>\'\n    sys.exit(2)\nfor opt, arg in opts:\n    if opt == \'-d\':\n        if arg == \'Oxford\' or arg == \'Paris\' or arg == \'Oxford105k\' or arg == \'Paris106k\':\n            dataset = arg\n            flag_d = True\n\n    elif opt == \'--nc_q\':\n            num_cams = int(arg)\n            flag_nc_q = True\n\n    elif opt == \'--pca\':\n            num_classes_pca = int(arg)\n            flag_pca = True\n\n    elif opt == \'-m\':\n        if arg == \'ResNet50\' or arg == \'DenseNet161\':\n            model_name = arg\n            flag_m = True\n\n    elif opt == \'--qe\':\n            n_expand = int(arg)\n            query_expansion = True\n            flag_qe = True\n\n    elif opt == \'--re\':\n            do_re_ranking = True\n            top_n_ranking = int(arg)\n            flag_re = True\n\n    elif opt == \'--nc_re\':\n            num_cams2 = int(arg)\n            flag_nc_re = True\n\nif not flag_pca:\n    num_classes_pca = 1\n    print \'Default pca_classes: \', num_classes_pca\n\n# N Class Activation Maps\nif not flag_nc_q:\n    num_cams = 64\n    print \'Default classes: \', num_cams\n\n# Num_cams2 --> Used to compute the descriptors when re-ranking\nif not flag_nc_re:\n    num_cams_re = 6\n    print \'Default classes for re-ranking: \', num_cams_re\n\n# Re-ranking\nif not flag_re:\n    do_re_ranking = False\n    top_n_ranking = 0\n    print \'Not doing Re-ranking\'\n\n# Query Expansion\nif not flag_qe:\n    # Re-ranking\n    query_expansion = False\n    n_expand = 0\n    print \'Not doing Query Expansion\'\n\nif not flag_m:\n    model_name = \'ResNet50\'\n    print \'Default model: \', model_name\n\n\n# Num classes stored in the precomputed --> Have to be set up\nnum_prec_classes = 64\n\n# SET FOR RE-RANKING\nbatch_size_re = 6\n\n# Global params\nn_images_distractors = 100070\nn_images_oxford = 5063\nn_images_paris = 6392\nn_queries = 55\n\n# Descriptors for Re-ranking  (Size W x H)\ndim = \'1024x720\'\nsize_v = [720, 1024]\nsize_h = [1024, 720]\n\nstats = list()\nmean = [0.485, 0.456, 0.406]\nstd = [0.229, 0.224, 0.225]\nstats.append(mean)\nstats.append(std)\n\n# Parameters to set\n\n# Dataset\nif not flag_d:\n    dataset = \'Oxford\'\n    print \'Default dataset: \', dataset\n\ncudnn.benchmark = True\n\nif model_name == \'ResNet50\':\n    # ResNet50\n    model = resnet.resnet50(pretrained=True)\n    model = torch.nn.DataParallel(model)\n    dim_descriptor = 2048\n    pca_dim = 2048\nelif model_name == \'DenseNet161\':\n    # DenseNet161\n    model = densenet.densenet161(pretrained=True)\n    model.features = torch.nn.DataParallel(model.features)\n    dim_descriptor = 2208\n    pca_dim = 2208\n\nmodel.cuda()\n\n# PCA Parameters\napply_pca = True\n\nprint \'Dataset: \', dataset\nprint \'Num_cams \', num_cams\nprint \'PCA with \', num_classes_pca\nprint \'Model: \', model_name\n\nif do_re_ranking:\n    print \'Re-ranking with first \', top_n_ranking\nif query_expansion:\n    print \'Applying query expansion using the first \', n_expand\n\n\nif dataset == \'Oxford\':\n    image_path = \'/data/jim011/datasets_retrieval/Oxford5k/images/\'\n    ranking_path = \'/flush2/jim011/results/oxford/\' + model_name + \'/\' + dim + \'/\'\n    ranking_image_names_list = \'../lists/list_oxford_rank.txt\'\n    create_folders(ranking_path)\n\n    cam_descriptors_path = \'/data/jim011/oxford/descriptors/\' + model_name + \'/\' + dim + \'/\' + \'oxford_all_64_wp.h5\'\n\n    pca_descriptors_path = \'/data/jim011/paris/descriptors/\' + model_name + \'/1024x720/\' + \'paris_all_64_wp.h5\'\n\n    t = time.time()\n\n    image_names = list()\n\n    with open(ranking_image_names_list, ""r"") as f:\n        for line in f:\n            image_names.append(line)\n\n    num_images = n_images_oxford\n    num_img_pca = n_images_paris\n\n    image_names = np.array(image_names)\n\n    path_gt = ""/data/jim011/datasets_retrieval/Oxford5k/ground_truth/""\n    query_names = [""all_souls"", ""ashmolean"", ""balliol"", ""bodleian"", ""christ_church"", ""cornmarket"", ""hertford"", ""keble"",\n                   ""magdalen"", ""pitt_rivers"", ""radcliffe_camera""]\n\n\nelif dataset == \'Paris\':\n    ranking_path = \'/flush2/jim011/results/paris/\' + model_name + \'/\' + dim + \'/\'\n    ranking_image_names_list = \'../lists/list_paris_rank.txt\'\n    create_folders(ranking_path)\n\n    descriptors_path = \'/flush2/jim011/paris/descriptors/\' + model_name + \'/1024x720/\'\n    descriptors_name = \'paris_32_pca_2208_oxford_1.h5\'\n\n    cam_descriptors_path = \'/flush2/jim011/paris/descriptors/\' + model_name + \'/\' + dim + \'/\' \\\n                           \'paris_all2_sp.h5\'\n\n    pca_descriptors_path = \'/flush2/jim011/oxford/descriptors/\' + model_name + \'/1024x720/\' \\\n                           \'oxford_all2_sp.h5\'\n\n    image_path = \'/data/jim011/datasets_retrieval/Paris6k/images/\'\n\n    num_images = n_images_paris\n    num_img_pca = n_images_oxford\n\n    path_gt = ""/data/jim011/datasets_retrieval/Paris6k/ground_truth/""\n    query_names = [""defense"", ""eiffel"", ""invalides"", ""louvre"", ""moulinrouge"", ""museedorsay"", ""notredame"", ""pantheon"",\n                   ""pompidou"", ""sacrecoeur"", ""triomphe""]\n\n    t = time.time()\n\n    image_names = list()\n    with open(ranking_image_names_list, ""r"") as f:\n        for line in f:\n            image_names.append(line)\n\n    image_names = np.array(image_names)\n\n\nelif dataset == \'Oxford105k\':\n    image_path = \'/data/jim011/datasets_retrieval/Oxford5k/images/\'\n    ranking_path = \'/home/jim011/workspace/retrieval-2017-icmr/results/oxford105k/\' + model_name +  \'/\' \\\n                   + dim + \'/\' + \'/R\' + str(top_n_ranking) + \'QE\' + str(n_expand)+\'/\'\n    ranking_image_names_list = \'/home/jim011/workspace/retrieval-2017-icmr/lists/list_oxford_rank.txt\'\n    ranking_distractors_list = \'/home/jim011/workspace/retrieval-2017-icmr/lists/list_oxford_105k_rank.txt\'\n    create_folders(ranking_path)\n\n    descriptors_path = \'/data/jim011/oxford/descriptors/Vgg_16_CAM/relu5_1/1024x720/\'\n    distractors_path = \'/data/jim011/oxford/descriptors/Vgg_16_CAM/relu5_1/1024x720/\' \\\n                       \'oxford_105k_32_pca_512_paris_1.h5\'\n\n    # descriptors_name = \'oxford_fusion_8_th_0_pca_paris_8_wp_wp.h5\'\n    descriptors_name = \'oxford_32_pca_512_paris_1.h5\'\n    pca_descriptors_path = \'/data/jim011/paris/descriptors/Vgg_16_CAM/relu5_1/1024x720/\' \\\n                           \'paris_all_64_wp.h5\'\n\n    cam_distractors_path = \'/data/jim011/descriptors100k/descriptors/\' + model_name + \'/\' + \'/\' + dim + \'/\' \\\n                           \'distractor_all_64_wp_\'\n    num_images = n_images_distractors\n    num_img_pca = n_images_paris\n\n    t = time.time()\n\n    image_names = list()\n\n    with open(ranking_image_names_list, ""r"") as f:\n        for line in f:\n            image_names.append(line)\n\n    with open(ranking_distractors_list, ""r"") as f:\n        for line in f:\n            image_names.append(line)\n\n    image_names = np.array(image_names)\n\n\n    path_gt = ""/data/jim011/datasets_retrieval/Oxford5k/ground_truth/""\n    query_names = [""all_souls"", ""ashmolean"", ""balliol"", ""bodleian"", ""christ_church"", ""cornmarket"", ""hertford"", ""keble"",\n                   ""magdalen"", ""pitt_rivers"", ""radcliffe_camera""]\n\nelif dataset == \'Paris106k\':\n    ranking_path = \'/home/jim011/workspace/retrieval-2017-icmr/results/paris106k/\' + model_name + \'/\' + layer + \'/\' \\\n                   + dim + \'/\' + \'/R\' + str(top_n_ranking) + \'QE\' + str(n_expand)+\'/\'\n    ranking_image_names_list = \'/home/jim011/workspace/retrieval-2017-icmr/lists/list_paris_rank.txt\'\n    ranking_distractors_list = \'/home/jim011/workspace/retrieval-2017-icmr/lists/list_oxford_105k_rank.txt\'\n    create_folders(ranking_path)\n\n    descriptors_path = \'/data/jim011/oxford/descriptors/Vgg_16_CAM/relu5_1/1024x720/\'\n    descriptors_name = \'paris_32_pca_512_oxford_1.h5\'\n    distractors_path = \'/data/jim011/\'\n\n    pca_descriptors_path = \'/data/jim011/oxford/descriptors/Vgg_16_CAM/relu5_1/1024x720/\' \\\n                           \'oxford_all_32_wp.h5\'\n    image_path = \'/data/jim011/datasets_retrieval/Paris6k/images/\'\n\n    num_images = n_images_distractors\n    num_img_pca = n_images_oxford\n    cam_distractors_path = \'/data/jim011/descriptors100k/descriptors/\' + model_name + \'/\' + \'/\' + dim + \'/\' \\\n                            \'distractor_all_64_wp_\'\n\n    t = time.time()\n\n    image_names = list()\n    with open(ranking_image_names_list, ""r"") as f:\n        for line in f:\n            image_names.append(line)\n\n    with open(ranking_distractors_list, ""r"") as f:\n        for line in f:\n            image_names.append(line)\n\n    image_names = np.array(image_names)\n\n    path_gt = ""/data/jim011/datasets_retrieval/Paris6k/ground_truth/""\n    query_names = [""defense"", ""eiffel"", ""invalides"", ""louvre"", ""moulinrouge"", ""museedorsay"", ""notredame"", ""pantheon"",\n                   ""pompidou"", ""sacrecoeur"", ""triomphe""]\n\nmaps = list()\n\n# Compute PCA\nif apply_pca:\n    tpca = time.time()\n    pca_desc = retrieve_n_descriptors(num_classes_pca, num_img_pca, load_data(pca_descriptors_path))\n    pca_matrix = compute_pca(pca_desc, pca_dim=pca_dim, whiten=True)\n    print \'PCA matrix shape:\', pca_matrix.components_.shape\n    print \'Time elapsed PCA: \', time.time() - tpca\nelse:\n    pca_matrix = None\n\nif dataset == \'Oxford105k\' or dataset == \'Paris106k\':\n    n_chunks = 10\n    distractors = np.zeros((0, 512), dtype=np.float32)\n    for n_in in range(0, n_chunks+1):\n        desc = load_data(cam_distractors_path + str(n_in) + \'.h5\')\n        print desc.shape\n        distractors = np.concatenate((distractors, descriptor_aggregation(desc, desc.shape[0]/num_prec_classes,\n                                                                          num_cams, pca_matrix)))\n        print distractors.shape\n        t = time.time()\n        cam_descriptors = load_data(cam_descriptors_path)\n        print \'Time elapsed loading: \', time.time() - t\n        data = descriptor_aggregation(cam_descriptors, num_images, num_cams, pca_matrix)\n        data = np.concatenate((data, distractors))\n\nelif dataset == \'Oxford\' or dataset == \'Paris\':\n    t = time.time()\n    cam_descriptors = load_data(cam_descriptors_path)\n    print \'Time elapsed loading: \', time.time() - t\n    #data = cam_descriptors\n    data = descriptor_aggregation(cam_descriptors, num_images, num_cams, pca_matrix)\n\nfor query_name in query_names:\n    for i in range(1, 6):\n        f = open(path_gt + query_name + \'_\' + str(i) + \'_query.txt\').readline()\n        if dataset == \'Oxford\' or dataset == \'Oxford105k\':\n            f = f.replace(""oxc1_"", """")\n        f_list = f.split("" "")\n        for k in range(1, 5):\n            f_list[k] = (int(math.floor(float(f_list[k]))))\n\n        query_img_name = f_list[0]\n        img = imread(image_path + query_img_name + \'.jpg\')\n        print \'Image Shape: \' + str(img.shape[0]) + \'x\' + str(img.shape[1])\n\n        # Query bounding box\n        x, y, dx, dy = f_list[1], f_list[2], f_list[3], f_list[4]\n\n        # Feature map query bounding box\n        f_x, f_y, f_dx, f_dy = int((x - (x % 32)) / 32), int((y - (y % 32)) / 32), \\\n                               int((dx - (dx % 32)) / 32), int((dy - (dy % 32)) / 32)\n\n        img_cropped = img[y:dy, x:dx]\n\n        print \'Name of the query: \', query_img_name\n\n        h = img_cropped.shape[0] - (img_cropped.shape[0] % 32)\n        w = img_cropped.shape[1] - (img_cropped.shape[1] % 32)\n\n        img_tensor = preprocess_images(img_cropped, w, h, stats[0], stats[1])\n        img_tensor = torch.autograd.Variable(img_tensor, volatile=True)\n\n        # Cropped Query\n        features_c, cams_c, class_list, _ = extract_feat_cam(model, model_name, 1, img_tensor, num_cams)\n\n        if img.shape[0] > img.shape[1]:\n            size = size_v\n        else:\n            size = size_h\n\n        img_tensor = preprocess_images(img, size[0], size[1], stats[0], stats[1])\n        img_tensor = torch.autograd.Variable(img_tensor, volatile=True)\n\n        # All image query (With bounding box classes, to be implemented in one step...)\n        features, cams, _ = extract_feat_cam(model, model_name, 1, img_tensor, num_cams, class_list[0, 0:num_cams], roi=False)\n\n        # Features that fall inside Bounding box query\n        d_wp = weighted_cam_pooling(features[:, :, f_y:f_dy, f_x:f_dx], cams[:, :, f_y:f_dy, f_x:f_dx])\n\n        #d_wp = weighted_cam_pooling(features_c, cams_c, max_pool=False)\n\n        # Compute Query Descriptor\n        desc = descriptor_aggregation(d_wp, 1, num_cams, pca_matrix)\n\n        # desc = sum_pooling(features_c)\n\n        # desc = sum_pooling(features[:, :, f_y:f_dy, f_x:f_dx])\n\n        indices_local, data_local = eval.save_ranking_one_query(data, desc, image_names, ranking_path, query_img_name)\n\n        if do_re_ranking:\n            # When re-ranking descriptor for the query computed with less CAMs, as we know the relevant objects\n            desc = descriptor_aggregation(d_wp, 1, num_cams_re, pca_matrix)\n            t_rerank = time.time()\n            indices_re_ranking, data_re_ranking = re_ranking(desc, class_list[0, 0:num_cams_re], batch_size_re, image_names,\n                                                             indices_local, dataset, top_n_ranking, pca_matrix,\n                                                             model, model_name)\n            print \'Time reranking: \', time.time() - t_rerank\n            eval.save_ranking_indices(indices_re_ranking, image_names, query_img_name, ranking_path)\n\n        if query_expansion:\n            if do_re_ranking:\n                data_local[indices_re_ranking[0:top_n_ranking]] = data_re_ranking\n                desc_expanded = eval.expand_query(n_expand, data_local, indices_re_ranking)\n            else:\n                desc_expanded = eval.expand_query(n_expand, data_local, indices_local)\n            eval.save_ranking_one_query(data, desc_expanded, image_names, ranking_path, query_img_name)\n\n        sys.stdout.flush()\n\nprint \'Time elapsed computing distances: \', time.time() - t\n\nif dataset == \'Oxford\' or dataset == \'Oxford105k\':\n    maps.append(eval.evaluate_oxford(ranking_path))\nelif dataset == \'Paris\' or dataset == \'Paris106k\':\n    maps.append(eval.evaluate_paris(ranking_path))\n\nmaps_file = open(ranking_path + \'maps\' + dataset + \'_pca_\' + str(num_classes_pca) + \'.txt\', \'w\')\n\nprint maps\n\nfor res in maps:\n    maps_file.write(str(res) + \'\\n\')\n\nmaps_file.close()\n'"
pytorch_code/cam_functions.py,0,"b""import cv2\nimport torch\nimport sys\nimport time\nimport h5py\nimport numpy as np\nimport torchvision\nimport pickle\nimport math\n\n\n# Extract region of interest from CAMs\ndef extract_ROI(heatmap, threshold):\n    th = threshold * np.max(heatmap)\n    heatmap = heatmap > th\n    # Find the largest connected component\n\n    contours, hierarchy = cv2.findContours(heatmap.astype('uint8'), mode=cv2.RETR_EXTERNAL, method=cv2.CHAIN_APPROX_SIMPLE)\n\n    areas = [cv2.contourArea(ctr) for ctr in contours]\n\n    max_contour = contours[areas.index(max(areas))]\n\n    x, y, w, h = cv2.boundingRect(max_contour)\n    if w == 0:\n        w = heatmap.shape[1]\n    if h == 0:\n        h = heatmap.shape[0]\n    return x, y, w, h\n\n\ndef extract_feat_cam(model, model_name, batch_size, data, n_classes, specify_class=None, roi=False):\n\n    num_samples = data.size()[0]\n    class_list = np.zeros((num_samples, n_classes), dtype=np.int32)\n    scores_vec = list()\n    print 'Num of total samples: ', num_samples\n    print 'Batch size: ', batch_size\n    sys.stdout.flush()\n\n    num_it = int(math.floor(num_samples / batch_size))\n    last_batch = num_samples % batch_size\n    batch_size_loop = batch_size\n\n    model.eval()\n\n    # Extract weights from Dense\n    if model_name == 'DenseNet161':\n        weights_fc = model.classifier.state_dict()['weight']\n    elif model_name == 'ResNet50':\n        weights_fc = model.module.fc.state_dict()['weight']\n\n    weights_fc = np.transpose(weights_fc.cpu().numpy(), (1, 0))\n\n\n    # Region of interest for re-ranking (bounding box coordinates --> (num samples, num_thresholds, x,y,dx,dy)\n    if roi:\n        bbox_coord = np.zeros((num_samples, 5, 4), dtype=np.int16)\n    else:\n        bbox_coord = None\n\n    for i in range(0, num_it+1):\n        t0 = time.time()\n        if i == num_it:\n            if last_batch != 0:\n                conv_outputs_cam, scores = model.forward(data[i*batch_size:batch_size*i+last_batch, :, :, :])\n                batch_size_loop = last_batch\n                conv_outputs_cam = conv_outputs_cam.data.cpu().numpy()\n                scores = scores.data.cpu().numpy()\n                if i == 0:\n                    features_conv = np.zeros((num_samples, conv_outputs_cam.shape[1], conv_outputs_cam.shape[2], conv_outputs_cam.shape[3]))\n                    cams = np.zeros((num_samples, n_classes, conv_outputs_cam.shape[2], conv_outputs_cam.shape[3]))\n                    features_conv[i * batch_size:batch_size * i + last_batch, :, :, :] = conv_outputs_cam\n                else:\n                    features_conv[i * batch_size:batch_size * i + last_batch, :, :, :] = conv_outputs_cam\n            else:\n                break\n        else:\n            conv_outputs_cam, scores = model.forward(data[i*batch_size:batch_size*(i+1), :, :, :])\n            conv_outputs_cam = conv_outputs_cam.data.cpu().numpy()\n            scores = scores.data.cpu().numpy()\n            if i == 0:\n                features_conv = np.zeros((num_samples, conv_outputs_cam.shape[1], conv_outputs_cam.shape[2], conv_outputs_cam.shape[3]))\n                cams = np.zeros((num_samples, n_classes, conv_outputs_cam.shape[2], conv_outputs_cam.shape[3]))\n                features_conv[i * batch_size:batch_size * (i + 1), :, :, :] = conv_outputs_cam\n            else:\n                features_conv[i*batch_size:batch_size*(i+1), :, :, :] = conv_outputs_cam\n        print 'Batch number: ', i\n        print ('Time elapsed to forward the batch: ', time.time()-t0)\n        sys.stdout.flush()\n        if specify_class is None:\n\n            for ii in range(0, batch_size_loop):\n                #print 'Image number: ', ii\n                indexed_scores = scores[ii].argsort()[::-1]\n                scores_vec.append(scores[ii])\n                for k in range(0, n_classes):\n                    w_class = weights_fc[:, indexed_scores[k]]\n                    cam = np.zeros(dtype=np.float32, shape=conv_outputs_cam.shape[2:4])\n                    for ind, w in enumerate(w_class):\n                        cam += w * conv_outputs_cam[ii, ind, :, :]\n                    cam /= np.max(cam)\n                    cam[np.where(cam < 0)] = 0\n\n                    cams[i*batch_size+ii, k, :, :] = cam\n\n                    class_list[i*batch_size+ii, k] = indexed_scores[k]\n\n        else:\n            for ii in range(0, batch_size_loop):\n                # print 'Image number: ', ii\n                for k in range(0, n_classes):\n                    w_class = weights_fc[:, specify_class[k]]\n                    cam = np.zeros(dtype=np.float32, shape=conv_outputs_cam.shape[2:4])\n                    for ind, w in enumerate(w_class):\n                        cam += w * conv_outputs_cam[ii, ind, :, :]\n                    cam /= np.max(cam)\n                    cam[np.where(cam < 0)] = 0\n\n                    cams[i * batch_size + ii, k, :, :] = cam\n\n                # How to compute the ROI of the image, in the paper results we average 2 most probable classes\n                if roi:\n                    average = True\n\n                    if average:\n                        average_cam = np.zeros((cams.shape[2], cams.shape[3]))\n                        for cam in cams[i*batch_size+ii, 0:2]:\n                            average_cam += cam\n                        heatmap = average_cam / 2\n                    else:\n                        heatmap = cams[i*batch_size+ii, 0]\n\n                    bbox_coord[i*batch_size+ii, 0, :] = extract_ROI(heatmap=heatmap, threshold=0.01)# Full Image\n                    bbox_coord[i*batch_size+ii, 1, :] = extract_ROI(heatmap=heatmap, threshold=0.1)\n                    bbox_coord[i*batch_size+ii, 2, :] = extract_ROI(heatmap=heatmap, threshold=0.2)\n                    bbox_coord[i*batch_size+ii, 3, :] = extract_ROI(heatmap=heatmap, threshold=0.3)\n                    bbox_coord[i*batch_size+ii, 4, :] = extract_ROI(heatmap=heatmap, threshold=0.4)\n\n        print 'Time elapsed to compute CAMs & Features: ', time.time()-t0\n        sys.stdout.flush()\n    if specify_class is None:\n        return features_conv, cams, class_list, scores_vec\n    else:\n        return features_conv, cams, bbox_coord\n\n\ndef extract_feat_cam_all(model, model_name, batch_size, data, n_classes=1000):\n    '''\n    Extract CAM masks for all classes, for each image in the dataset. Also extract  features\n    from layer\n    :param model: The network\n    :param batch_size: batch_size\n    :param images: images in format [num_total,3,height, width]\n    :return:\n    '''\n\n    num_samples = data.size()[0]\n    print 'Num of total samples: ', num_samples\n    print 'Batch size: ', batch_size\n    sys.stdout.flush()\n\n    num_it = int(math.floor(num_samples / batch_size))\n    last_batch = num_samples % batch_size\n    batch_size_loop = batch_size\n\n    model.eval()\n\n    # Extract weights from Dense\n    if model_name == 'DenseNet161':\n        weights_fc = model.classifier.state_dict()['weight']\n    elif model_name == 'ResNet50':\n        weights_fc = model.module.fc.state_dict()['weight']\n\n    for i in range(0, num_it+1):\n        t0 = time.time()\n        if i == num_it:\n            if last_batch != 0:\n                conv_outputs_cam, _ = model.forward(data[i*batch_size:batch_size*i+last_batch, :, :, :])\n                batch_size_loop = last_batch\n                conv_outputs_cam = conv_outputs_cam.data.cpu().numpy()\n                if i == 0:\n                    features_conv = np.zeros((num_samples, conv_outputs_cam.shape[1], conv_outputs_cam.shape[2], conv_outputs_cam.shape[3]))\n                    cams = np.zeros((num_samples, n_classes, conv_outputs_cam.shape[2], conv_outputs_cam.shape[3]))\n                    features_conv[i * batch_size:batch_size * i + last_batch, :, :, :] = conv_outputs_cam\n                else:\n                    features_conv[i * batch_size:batch_size * i + last_batch, :, :, :] = conv_outputs_cam\n            else:\n                break\n        else:\n            conv_outputs_cam, _ = model.forward(data[i*batch_size:batch_size*(i+1), :, :, :])\n            conv_outputs_cam = conv_outputs_cam.data.cpu().numpy()\n            if i == 0:\n                features_conv = np.zeros((num_samples, conv_outputs_cam.shape[1], conv_outputs_cam.shape[2], conv_outputs_cam.shape[3]))\n                cams = np.zeros((num_samples, n_classes, conv_outputs_cam.shape[2], conv_outputs_cam.shape[3]))\n                features_conv[i * batch_size:batch_size * (i + 1), :, :, :] = conv_outputs_cam\n            else:\n                features_conv[i*batch_size:batch_size*(i+1), :, :, :] = conv_outputs_cam\n\n        print ('Time elapsed to forward the batch: ', time.time()-t0)\n        sys.stdout.flush()\n\n        for ii in range(0, batch_size_loop):\n            for k in range(0, n_classes):\n                w_class = weights_fc[:, k]\n                cam = np.zeros(dtype=np.float32, shape=conv_outputs_cam.shape[2:4])\n                for ind, w in enumerate(w_class):\n                    cam += w * conv_outputs_cam[ii, ind, :, :]\n                cam /= np.max(cam)\n                cam[np.where(cam < 0)] = 0\n\n                cams[i*batch_size+ii, k, :, :] = cam\n\n        print 'Time elapsed to compute CAMs: ', time.time()-t0\n\n    return features_conv, cams\n\n\n# def extract_feat_cam_resnet(model, batch_size, data, n_classes, specify_class=None, roi=False):\n\n# '''\n#     :param model:\n#     :param layer_features:\n#     :param batch_size:\n#     :param data:\n#     :param n_classes:\n#     :param specify_class:\n#     :param roi:\n#     :return:\n#     '''\n#\n#     num_samples = data.size()[0]\n#     class_list = np.zeros((num_samples, n_classes), dtype=np.int32)\n#     scores_vec = list()\n#     print 'Num of total samples: ', num_samples\n#     print 'Batch size: ', batch_size\n#     sys.stdout.flush()\n#\n#     num_it = int(math.floor(num_samples / batch_size))\n#     last_batch = num_samples % batch_size\n#     batch_size_loop = batch_size\n#\n#     model.eval()\n#\n#     # Extract weights from Dense\n#     weights_fc = model.module.fc.state_dict()['weight']\n#     weights_fc = np.transpose(weights_fc.cpu().numpy(), (1, 0))\n#\n#     # Region of interest for re-ranking (bounding box coordinates --> (num samples, num_thresholds, x,y,dx,dy)\n#     if roi:\n#         bbox_coord = np.zeros((num_samples, 5, 4), dtype=np.int16)\n#     else:\n#         bbox_coord = None\n#\n#     for i in range(0, num_it+1):\n#         t0 = time.time()\n#         if i == num_it:\n#             if last_batch != 0:\n#                 conv_outputs_cam, scores = model.forward(data[i*batch_size:batch_size*i+last_batch, :, :, :])\n#                 batch_size_loop = last_batch\n#                 conv_outputs_cam = conv_outputs_cam.data.cpu().numpy()\n#                 scores = scores.data.cpu().numpy()\n#                 if i == 0:\n#                     features_conv = np.zeros((num_samples, conv_outputs_cam.shape[1], conv_outputs_cam.shape[2], conv_outputs_cam.shape[3]))\n#                     cams = np.zeros((num_samples, n_classes, conv_outputs_cam.shape[2], conv_outputs_cam.shape[3]))\n#                     features_conv[i * batch_size:batch_size * i + last_batch, :, :, :] = conv_outputs_cam\n#                 else:\n#                     features_conv[i * batch_size:batch_size * i + last_batch, :, :, :] = conv_outputs_cam\n#             else:\n#                 break\n#         else:\n#             conv_outputs_cam, scores = model.forward(data[i*batch_size:batch_size*(i+1), :, :, :])\n#             conv_outputs_cam = conv_outputs_cam.data.cpu().numpy()\n#             scores = scores.data.cpu().numpy()\n#             if i == 0:\n#                 features_conv = np.zeros((num_samples, conv_outputs_cam.shape[1], conv_outputs_cam.shape[2], conv_outputs_cam.shape[3]))\n#                 cams = np.zeros((num_samples, n_classes, conv_outputs_cam.shape[2], conv_outputs_cam.shape[3]))\n#                 features_conv[i * batch_size:batch_size * (i + 1), :, :, :] = conv_outputs_cam\n#             else:\n#                 features_conv[i*batch_size:batch_size*(i+1), :, :, :] = conv_outputs_cam\n#         print 'Batch number: ', i\n#         print ('Time elapsed to forward the batch: ', time.time()-t0)\n#         sys.stdout.flush()\n#         if specify_class is None:\n#\n#             for ii in range(0, batch_size_loop):\n#                 #print 'Image number: ', ii\n#                 indexed_scores = scores[ii].argsort()[::-1]\n#                 scores_vec.append(scores[ii])\n#                 for k in range(0, n_classes):\n#                     w_class = weights_fc[:, indexed_scores[k]]\n#                     cam = np.zeros(dtype=np.float32, shape=conv_outputs_cam.shape[2:4])\n#                     for ind, w in enumerate(w_class):\n#                         cam += w * conv_outputs_cam[ii, ind, :, :]\n#                     cam /= np.max(cam)\n#                     cam[np.where(cam < 0)] = 0\n#\n#                     cams[i*batch_size+ii, k, :, :] = cam\n#\n#                     class_list[i*batch_size+ii, k] = indexed_scores[k]\n#\n#         else:\n#             for ii in range(0, batch_size_loop):\n#                 # print 'Image number: ', ii\n#                 for k in range(0, n_classes):\n#                     w_class = weights_fc[:, specify_class[k]]\n#                     cam = np.zeros(dtype=np.float32, shape=conv_outputs_cam.shape[2:4])\n#                     for ind, w in enumerate(w_class):\n#                         cam += w * conv_outputs_cam[ii, ind, :, :]\n#                     cam /= np.max(cam)\n#                     cam[np.where(cam < 0)] = 0\n#\n#                     cams[i * batch_size + ii, k, :, :] = cam\n#\n#                 # How to compute the ROI of the image, in the paper results we average 2 most probable classes\n#                 if roi:\n#                     average = True\n#\n#                     if average:\n#                         average_cam = np.zeros((cams.shape[2], cams.shape[3]))\n#                         for cam in cams[i*batch_size+ii, 0:2]:\n#                             average_cam += cam\n#                         heatmap = average_cam / 2\n#                     else:\n#                         heatmap = cams[i*batch_size+ii, 0]\n#\n#                     bbox_coord[i*batch_size+ii, 0, :] = extract_ROI(heatmap=heatmap, threshold=0.01)# Full Image\n#                     bbox_coord[i*batch_size+ii, 1, :] = extract_ROI(heatmap=heatmap, threshold=0.1)\n#                     bbox_coord[i*batch_size+ii, 2, :] = extract_ROI(heatmap=heatmap, threshold=0.2)\n#                     bbox_coord[i*batch_size+ii, 3, :] = extract_ROI(heatmap=heatmap, threshold=0.3)\n#                     bbox_coord[i*batch_size+ii, 4, :] = extract_ROI(heatmap=heatmap, threshold=0.4)\n#\n#         print 'Time elapsed to compute CAMs & Features: ', time.time()-t0\n#         sys.stdout.flush()\n#     if specify_class is None:\n#         return features_conv, cams, class_list, scores_vec\n#     else:\n#         return features_conv, cams, bbox_coord\n\n\n# def extract_feat_cam(model, layer_features, batch_size, data, n_classes, specify_class=None, roi=False):\n#     '''\n#     :param model:\n#     :param layer_features:\n#     :param batch_size:\n#     :param data:\n#     :param n_classes:\n#     :param specify_class:\n#     :param roi:\n#     :return:\n#     '''\n#\n#     num_samples = data.size()[0]\n#     class_list = np.zeros((num_samples, n_classes), dtype=np.int32)\n#     print 'Num of total samples: ', num_samples\n#     print 'Batch size: ', batch_size\n#     sys.stdout.flush()\n#     scores_vec = list()\n#     num_it = int(math.floor(num_samples / batch_size))\n#     last_batch = num_samples % batch_size\n#     batch_size_loop = batch_size\n#\n#     model.eval()\n#\n#     # Define Hook to Extract Convolutional Features\n#     feats = list()\n#\n#     def hook_features(m, input, output):\n#         print type(feats)\n#         feats.append(output.data.cpu().numpy())\n#         print 'features Extracted'\n#\n#     hookie = layer_features.register_forward_hook(hook_features)\n#\n#     # Extract weights from Dense\n#     weights_fc = model.classifier[0].state_dict()['weight']\n#     weights_fc = np.transpose(weights_fc.cpu().numpy(), (1, 0))\n#\n#     # Region of interest for re-ranking (bounding box coordinates --> (num samples, num_thresholds, x,y,dx,dy)\n#     if roi:\n#         bbox_coord = np.zeros((num_samples, 5, 4), dtype=np.int16)\n#\n#     for i in range(0, num_it+1):\n#         print 'Batch number: ', i\n#         feats = list()\n#         t0 = time.time()\n#         if i == num_it:\n#             if last_batch != 0:\n#                 conv_outputs_cam, scores = model.forward(data[i*batch_size:batch_size*i+last_batch, :, :, :])\n#                 batch_size_loop = last_batch\n#                 feats = np.array(feats)\n#                 print feats.shape\n#                 if i == 0:\n#                     features_conv = np.zeros((num_samples, feats.shape[2], feats.shape[3], feats.shape[4]))\n#                     cams = np.zeros((num_samples, n_classes, feats.shape[3], feats.shape[4]))\n#                 if n_gpu == 3 and num_samples >= 2: # Not working yet\n#                     features_conv[i*batch_size:batch_size*i+last_batch, :, :, :] = \\\n#                         np.concatenate((feats[0], feats[1], feats[2]))\n#                 else:\n#                     features_conv[i * batch_size:batch_size * i + last_batch, :, :, :] = feats[0]\n#             else:\n#                 break\n#         else:\n#             conv_outputs_cam, scores = model.forward(data[i*batch_size:batch_size*(i+1), :, :, :])\n#             feats = np.array(feats)\n#             print feats.shape\n#             if i == 0:\n#                 features_conv = np.zeros((num_samples, feats.shape[2], feats.shape[3], feats.shape[4]))\n#                 cams = np.zeros((num_samples, n_classes, feats.shape[3], feats.shape[4]))\n#             if n_gpu == 3 and num_samples >= 2: # Not working yet\n#                 features_conv[i*batch_size:batch_size*(i+1), :, :, :] = \\\n#                     np.concatenate((feats[0], feats[1], feats[2]))\n#             else:\n#                 features_conv[i*batch_size:batch_size*(i+1), :, :, :] = feats[0]\n#\n#         conv_outputs_cam = conv_outputs_cam.data.cpu().numpy()\n#         scores = scores.data.cpu().numpy()\n#         print ('Time elapsed to forward the batch: ', time.time()-t0)\n#         sys.stdout.flush()\n#         if specify_class is None:\n#\n#             for ii in range(0, batch_size_loop):\n#                 #print 'Image number: ', ii\n#                 indexed_scores = scores[ii].argsort()[::-1]\n#                 scores_vec.append(scores[ii])\n#                 for k in range(0, n_classes):\n#                     w_class = weights_fc[:, indexed_scores[k]]\n#                     cam = np.zeros(dtype=np.float32, shape=conv_outputs_cam.shape[2:4])\n#                     for ind, w in enumerate(w_class):\n#                         cam += w * conv_outputs_cam[ii, ind, :, :]\n#                     cam /= np.max(cam)\n#                     cam[np.where(cam < 0)] = 0\n#\n#                     cams[i*batch_size+ii, k, :, :] = cam\n#\n#                     class_list[i*batch_size+ii, k] = indexed_scores[k]\n#\n#         else:\n#             for ii in range(0, batch_size_loop):\n#                 # print 'Image number: ', ii\n#                 for k in range(0, n_classes):\n#                     w_class = weights_fc[:, specify_class[k]]\n#                     cam = np.zeros(dtype=np.float32, shape=conv_outputs_cam.shape[2:4])\n#                     for ind, w in enumerate(w_class):\n#                         cam += w * conv_outputs_cam[ii, ind, :, :]\n#                     cam /= np.max(cam)\n#                     cam[np.where(cam < 0)] = 0\n#\n#                     cams[i * batch_size + ii, k, :, :] = cam\n#\n#                 # How to compute the ROI of the image, in the paper results we average 2 most probable classes\n#                 if roi:\n#                     average = True\n#\n#                     if average:\n#                         average_cam = np.zeros((cams.shape[2], cams.shape[3]))\n#                         for cam in cams[i*batch_size+ii, 0:2]:\n#                             average_cam += cam\n#                         heatmap = average_cam / 2\n#                     else:\n#                         heatmap = cams[i*batch_size+ii, 0]\n#\n#                     bbox_coord[i*batch_size+ii, 0, :] = extract_ROI(heatmap=heatmap, threshold=0.01)# Full Image\n#                     bbox_coord[i*batch_size+ii, 1, :] = extract_ROI(heatmap=heatmap, threshold=0.1)\n#                     bbox_coord[i*batch_size+ii, 2, :] = extract_ROI(heatmap=heatmap, threshold=0.2)\n#                     bbox_coord[i*batch_size+ii, 3, :] = extract_ROI(heatmap=heatmap, threshold=0.3)\n#                     bbox_coord[i*batch_size+ii, 4, :] = extract_ROI(heatmap=heatmap, threshold=0.4)\n#\n#         print 'Time elapsed to compute CAMs & Features: ', time.time()-t0\n#         sys.stdout.flush()\n#     if specify_class is None:\n#         hookie.remove()\n#         return features_conv, cams, class_list, scores_vec\n#     else:\n#         hookie.remove()\n#         return features_conv, cams, bbox_coord\n#\n#\n# # Method for Online aggregation\n# def extract_feat_cam_all(model, layer_features, batch_size, data, n_classes=1000):\n#     '''\n#     Extract CAM masks for all classes, for each image in the dataset. Also extract  features\n#     from layer\n#     :param model: The network\n#     :param batch_size: batch_size\n#     :param images: images in format [num_total,3,height, width]\n#     :return:\n#     '''\n#\n#     num_samples = data.size()[0]\n#     print 'Num of total samples: ', num_samples\n#     print 'Batch size: ', batch_size\n#     sys.stdout.flush()\n#\n#     num_it = int(math.floor(num_samples / batch_size))\n#     last_batch = num_samples % batch_size\n#     batch_size_loop = batch_size\n#\n#     model.eval()\n#\n#     # Define Hook to Extract Convolutional Features\n#     features = list()\n#\n#     def hook_features(m, input, output):\n#         print type(feats)\n#         feats.append(output.data.cpu().numpy())\n#         print 'features Extracted'\n#\n#     hookie = layer_features.register_forward_hook(hook_features)\n#\n#     # Extract weights from Dense\n#     weights_fc = model.classifier[0].state_dict()['weight']\n#     weights_fc = np.transpose(weights_fc.cpu().numpy(), (1, 0))\n#\n#     for i in range(0, num_it+1):\n#         print 'Batch number: ', i\n#         feats = list()\n#         t0 = time.time()\n#         if i == num_it:\n#             if last_batch != 0:\n#                 conv_outputs_cam, scores = model.forward(data[i*batch_size:batch_size*i+last_batch, :, :, :])\n#                 batch_size_loop = last_batch\n#                 feats = np.array(feats)\n#                 if i == 0:\n#                     features_conv = np.zeros((num_samples, feats.shape[2], feats.shape[3], feats.shape[4]))\n#                     cams = np.zeros((num_samples, n_classes, feats.shape[3], feats.shape[4]))\n#\n#                 features_conv[i * batch_size:batch_size * i + last_batch, :, :, :] = feats[0]\n#             else:\n#                 break\n#         else:\n#             conv_outputs_cam, scores = model.forward(data[i*batch_size:batch_size*(i+1), :, :, :])\n#             feats = np.array(feats)\n#             if i == 0:\n#                 features_conv = np.zeros((num_samples, feats.shape[2], feats.shape[3], feats.shape[4]))\n#                 cams = np.zeros((num_samples, n_classes, feats.shape[3], feats.shape[4]))\n#\n#             features_conv[i*batch_size:batch_size*(i+1), :, :, :] = feats[0]\n#\n#         conv_outputs_cam = conv_outputs_cam.data.cpu().numpy()\n#\n#         print ('Time elapsed to forward the batch: ', time.time()-t0)\n#         sys.stdout.flush()\n#\n#         for ii in range(0, batch_size_loop):\n#             for k in range(0, n_classes):\n#                 w_class = weights_fc[:, k]\n#                 cam = np.zeros(dtype=np.float32, shape=conv_outputs_cam.shape[2:4])\n#                 for ind, w in enumerate(w_class):\n#                     cam += w * conv_outputs_cam[ii, ind, :, :]\n#                 cam /= np.max(cam)\n#                 cam[np.where(cam < 0)] = 0\n#\n#                 cams[i*batch_size+ii, k, :, :] = cam\n#\n#         print 'Time elapsed to compute CAMs: ', time.time()-t0\n#\n#     hookie.remove()\n#     return features_conv, cams"""
pytorch_code/crow.py,0,"b'# Copyright 2015, Yahoo Inc.\n# Licensed under the terms of the Apache License, Version 2.0. See the LICENSE file associated with the project for terms.\nimport numpy as np\nimport scipy\nfrom sklearn.preprocessing import normalize as sknormalize\nfrom sklearn.decomposition import PCA\n\n\ndef compute_crow_spatial_weight(X, a=2, b=2):\n    """"""\n    Given a tensor of features, compute spatial weights as normalized total activation.\n    Normalization parameters default to values determined experimentally to be most effective.\n    :param ndarray X:\n        3d tensor of activations with dimensions (channels, height, width)\n    :param int a:\n        the p-norm\n    :param int b:\n        power normalization\n    :returns ndarray:\n        a spatial weight matrix of size (height, width)\n    """"""\n    S = X.sum(axis=0)\n    z = (S**a).sum()**(1./a)\n    return (S / z)**(1./b) if b != 1 else (S / z)\n\n\ndef compute_crow_channel_weight(X):\n    """"""\n    Given a tensor of features, compute channel weights as the\n    log of inverse channel sparsity.\n    :param ndarray X:\n        3d tensor of activations with dimensions (channels, height, width)\n    :returns ndarray:\n        a channel weight vector\n    """"""\n    K, w, h = X.shape\n    area = float(w * h)\n    nonzeros = np.zeros(K, dtype=np.float32)\n    for i, x in enumerate(X):\n        nonzeros[i] = np.count_nonzero(x) / area\n\n    nzsum = nonzeros.sum()\n    for i, d in enumerate(nonzeros):\n        nonzeros[i] = np.log(nzsum / d) if d > 0. else 0.\n\n    return nonzeros\n\n\ndef apply_crow_aggregation(X):\n    """"""\n    Given a tensor of activations, compute the aggregate CroW feature, weighted\n    spatially and channel-wise.\n    :param ndarray X:\n        3d tensor of activations with dimensions (channels, height, width)\n    :returns ndarray:\n        CroW aggregated global image feature\n    """"""\n    S = compute_crow_spatial_weight(X)\n    C = compute_crow_channel_weight(X)\n    X = X * S\n    X = X.sum(axis=(1, 2))\n    return X * C\n\n\ndef apply_ucrow_aggregation(X):\n    """"""\n    Given a tensor of activations, aggregate by sum-pooling without weighting.\n    :param ndarray X:\n        3d tensor of activations with dimensions (channels, height, width)\n    :returns ndarray:\n        unweighted global image feature\n    """"""\n    return X.sum(axis=(1, 2))\n\n\ndef normalize(x, copy=False):\n    """"""\n    A helper function that wraps the function of the same name in sklearn.\n    This helper handles the case of a single column vector.\n    """"""\n    if type(x) == np.ndarray and len(x.shape) == 1:\n        return np.squeeze(sknormalize(x.reshape(1,-1), copy=copy))\n    else:\n        return sknormalize(x, copy=copy)\n\n\ndef run_feature_processing_pipeline(features, d=128, whiten=True, copy=False, params=None):\n    """"""\n    Given a set of feature vectors, process them with PCA/whitening and return the transformed features.\n    If the params argument is not provided, the transformation is fitted to the data.\n    :param ndarray features:\n        image features for transformation with samples on the rows and features on the columns\n    :param int d:\n        dimension of final features\n    :param bool whiten:\n        flag to indicate whether features should be whitened\n    :param bool copy:\n        flag to indicate whether features should be copied for transformed in place\n    :param dict params:\n        a dict of transformation parameters; if present they will be used to transform the features\n    :returns ndarray: transformed features\n    :returns dict: transform parameters\n    """"""\n    # Normalize\n    features = normalize(features, copy=copy)\n\n    # Whiten and reduce dimension\n    if params:\n        pca = params[\'pca\']\n        features = pca.transform(features)\n    else:\n        pca = PCA(n_components=d, whiten=whiten, copy=copy)\n        features = pca.fit_transform(features)\n        params = { \'pca\': pca }\n\n    # Normalize\n    features = normalize(features, copy=copy)\n\n    return features, params\n\n\ndef save_spatial_weights_as_jpg(S, path=\'.\', filename=\'crow_sw\', size=None):\n    """"""\n    Save an image for visualizing a spatial weighting. Optionally provide path, filename,\n    and size. If size is not provided, the size of the spatial map is used. For instance,\n    if the spatial map was computed with VGG, setting size=(S.shape[0] * 32, S.shape[1] * 32)\n    will scale the spatial weight map back to the size of the image.\n    :param ndarray S:\n        spatial weight matrix\n    :param str path:\n    :param str filename:\n    :param tuple size:\n    """"""\n    img = scipy.misc.toimage(S)\n    if size is not None:\n        img = img.resize(size)\n\n    img.save(os.path.join(path, \'%s.jpg\' % str(filename)))'"
pytorch_code/densenet.py,9,"b'import torch.nn as nn\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.utils.model_zoo as model_zoo\nfrom collections import OrderedDict\n\nmodel_urls = {\n    \'densenet121\': \'https://download.pytorch.org/models/densenet121-241335ed.pth\',\n    \'densenet169\': \'https://download.pytorch.org/models/densenet169-6f0f7f60.pth\',\n    \'densenet201\': \'https://download.pytorch.org/models/densenet201-4c113574.pth\',\n    \'densenet161\': \'https://download.pytorch.org/models/densenet161-17b70270.pth\',\n}\n\n\ndef densenet121(pretrained=False, **kwargs):\n    r""""""Densenet-121 model from\n    `""Densely Connected Convolutional Networks"" <https://arxiv.org/pdf/1608.06993.pdf>`\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = DenseNet(num_init_features=64, growth_rate=32, block_config=(6, 12, 24, 16))\n    if pretrained:\n        model.load_state_dict(model_zoo.load_url(model_urls[\'densenet121\']))\n    return model\n\n\ndef densenet169(pretrained=False, **kwargs):\n    r""""""Densenet-169 model from\n    `""Densely Connected Convolutional Networks"" <https://arxiv.org/pdf/1608.06993.pdf>`\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = DenseNet(num_init_features=64, growth_rate=32, block_config=(6, 12, 32, 32))\n    if pretrained:\n        model.load_state_dict(model_zoo.load_url(model_urls[\'densenet169\']))\n    return model\n\n\ndef densenet201(pretrained=False, **kwargs):\n    r""""""Densenet-201 model from\n    `""Densely Connected Convolutional Networks"" <https://arxiv.org/pdf/1608.06993.pdf>`\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = DenseNet(num_init_features=64, growth_rate=32, block_config=(6, 12, 48, 32))\n    if pretrained:\n        model.load_state_dict(model_zoo.load_url(model_urls[\'densenet201\']))\n    return model\n\n\ndef densenet161(pretrained=False, **kwargs):\n    r""""""Densenet-161 model from\n    `""Densely Connected Convolutional Networks"" <https://arxiv.org/pdf/1608.06993.pdf>`\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = DenseNet(num_init_features=96, growth_rate=48, block_config=(6, 12, 36, 24))\n    if pretrained:\n        model.load_state_dict(model_zoo.load_url(model_urls[\'densenet161\']))\n    return model\n\n\nclass _DenseLayer(nn.Sequential):\n    def __init__(self, num_input_features, growth_rate, bn_size, drop_rate):\n        super(_DenseLayer, self).__init__()\n        self.add_module(\'norm.1\', nn.BatchNorm2d(num_input_features)),\n        self.add_module(\'relu.1\', nn.ReLU(inplace=True)),\n        self.add_module(\'conv.1\', nn.Conv2d(num_input_features, bn_size *\n                        growth_rate, kernel_size=1, stride=1, bias=False)),\n        self.add_module(\'norm.2\', nn.BatchNorm2d(bn_size * growth_rate)),\n        self.add_module(\'relu.2\', nn.ReLU(inplace=True)),\n        self.add_module(\'conv.2\', nn.Conv2d(bn_size * growth_rate, growth_rate,\n                        kernel_size=3, stride=1, padding=1, bias=False)),\n        self.drop_rate = drop_rate\n\n    def forward(self, x):\n        new_features = super(_DenseLayer, self).forward(x)\n        if self.drop_rate > 0:\n            new_features = F.dropout(new_features, p=self.drop_rate, training=self.training)\n        return torch.cat([x, new_features], 1)\n\n\nclass _DenseBlock(nn.Sequential):\n    def __init__(self, num_layers, num_input_features, bn_size, growth_rate, drop_rate):\n        super(_DenseBlock, self).__init__()\n        for i in range(num_layers):\n            layer = _DenseLayer(num_input_features + i * growth_rate, growth_rate, bn_size, drop_rate)\n            self.add_module(\'denselayer%d\' % (i + 1), layer)\n\n\nclass _Transition(nn.Sequential):\n    def __init__(self, num_input_features, num_output_features):\n        super(_Transition, self).__init__()\n        self.add_module(\'norm\', nn.BatchNorm2d(num_input_features))\n        self.add_module(\'relu\', nn.ReLU(inplace=True))\n        self.add_module(\'conv\', nn.Conv2d(num_input_features, num_output_features,\n                                          kernel_size=1, stride=1, bias=False))\n        self.add_module(\'pool\', nn.AvgPool2d(kernel_size=2, stride=2))\n\n\nclass DenseNet(nn.Module):\n    r""""""Densenet-BC model class, based on\n    `""Densely Connected Convolutional Networks"" <https://arxiv.org/pdf/1608.06993.pdf>`\n    Args:\n        growth_rate (int) - how many filters to add each layer (`k` in paper)\n        block_config (list of 4 ints) - how many layers in each pooling block\n        num_init_features (int) - the number of filters to learn in the first convolution layer\n        bn_size (int) - multiplicative factor for number of bottle neck layers\n          (i.e. bn_size * k features in the bottleneck layer)\n        drop_rate (float) - dropout rate after each dense layer\n        num_classes (int) - number of classification classes\n    """"""\n    def __init__(self, growth_rate=32, block_config=(6, 12, 24, 16),\n                 num_init_features=64, bn_size=4, drop_rate=0, num_classes=1000):\n\n        super(DenseNet, self).__init__()\n\n        # First convolution\n        self.features = nn.Sequential(OrderedDict([\n            (\'conv0\', nn.Conv2d(3, num_init_features, kernel_size=7, stride=2, padding=3, bias=False)),\n            (\'norm0\', nn.BatchNorm2d(num_init_features)),\n            (\'relu0\', nn.ReLU(inplace=True)),\n            (\'pool0\', nn.MaxPool2d(kernel_size=3, stride=2, padding=1)),\n        ]))\n\n        # Each denseblock\n        num_features = num_init_features\n        for i, num_layers in enumerate(block_config):\n            block = _DenseBlock(num_layers=num_layers, num_input_features=num_features,\n                                bn_size=bn_size, growth_rate=growth_rate, drop_rate=drop_rate)\n            self.features.add_module(\'denseblock%d\' % (i + 1), block)\n            num_features = num_features + num_layers * growth_rate\n            if i != len(block_config) - 1:\n                trans = _Transition(num_input_features=num_features, num_output_features=num_features // 2)\n                self.features.add_module(\'transition%d\' % (i + 1), trans)\n                num_features = num_features // 2\n\n        # Final batch norm\n        self.features.add_module(\'norm5\', nn.BatchNorm2d(num_features))\n\n        # Linear layer\n        self.classifier = nn.Linear(num_features, num_classes)\n\n    def forward(self, x):\n        features = self.features(x)\n        cams = F.relu(features, inplace=True)\n        out = F.avg_pool2d(cams, kernel_size=(features.size(2), features.size(3))).view(features.size(0), -1)\n        out = self.classifier(out)\n        return cams, out\n'"
pytorch_code/pooling_functions.py,0,"b""import numpy as np\nimport time\nfrom sklearn.decomposition import PCA\nimport sys\nimport utils as ud\nimport time\nfrom crow import compute_crow_channel_weight\n\n\ndef compute_pca(descriptors, pca_dim=512, whiten=True):\n    print descriptors.shape\n    t1 = time.time()\n    print 'Computing PCA with dimension reduction to: ', pca_dim\n    sys.stdout.flush()\n    pca = PCA(n_components=pca_dim, whiten=whiten)\n    pca.fit(descriptors)\n    print pca.components_.shape\n    print 'PCA finished!'\n    print 'Time elapsed computing PCA: ', time.time() - t1\n    return pca\n\n\ndef sum_pooling(features):\n    num_samples = features.shape[0]\n    num_features = features.shape[1]\n    sys.stdout.flush()\n    descriptors = np.zeros((num_samples, num_features), dtype=np.float32)\n    for i in range(0, num_samples):\n        #print 'Image: ', i\n        #sys.stdout.flush()\n        for f in range(0, num_features):\n            descriptors[i, f] = features[i, f].sum()\n    descriptors /= np.linalg.norm(descriptors, axis=1)[:, None]\n    return descriptors\n\n\ndef max_pooling(features):\n    num_samples = features.shape[0]\n    num_features = features.shape[1]\n    sys.stdout.flush()\n    descriptors = np.zeros((num_samples, num_features), dtype=np.float32)\n    for i in range(0, num_samples):\n        #print 'Image: ', i\n        #sys.stdout.flush()\n        for f in range(0, num_features):\n            descriptors[i, f] = np.amax(features[i, f])\n    descriptors /= np.linalg.norm(descriptors, axis=1)[:, None]\n    return descriptors\n\n\n# Do complete aggregation, class vectors + aggregation\ndef weighted_pooling(features, cams, max_pool=False, region_descriptors=False, pca=None, q=False):\n    t = time.time()\n    num_samples = features.shape[0]\n    num_features = features.shape[1]\n    num_features_des = features.shape[1]\n    num_classes = cams.shape[1]\n\n    if pca != '':\n        print 'Applying PCA...'\n        sys.stdout.flush()\n        num_features_des = int(pca.components_.shape[0])\n        print pca.components_.shape[0]\n\n    wp_batch_representations = np.zeros((num_samples, num_features_des), dtype=np.float32)\n    wp_regions = np.zeros((num_features, num_classes), dtype=np.float32)\n    wsp_descriptors_reg = np.zeros((num_samples * num_classes, num_features), dtype=np.float32)\n    wmp_descriptors_reg = np.zeros((num_samples * num_classes, num_features), dtype=np.float32)\n\n    if max_pool:\n        mp_regions = np.zeros((num_features, num_classes), dtype=np.float32)\n        mp_batch_representations = np.zeros((num_samples, num_features), dtype=np.float32)\n\n    for i in range(0, num_samples):\n        #CROW\n        C = np.array(compute_crow_channel_weight(features[i]))\n\n        for f in range(0, num_features):\n            for k in range(0, num_classes):\n                # For each region compute avg weighted sum of activations and l2 normalize\n                if not q:\n                    if max_pool:\n                        mp_regions[f, k] = np.amax(np.multiply(features[i, f], cams[i, k]))\n\n                    wp_regions[f, k] = np.multiply(features[i, f], cams[i, k]).sum()\n                else:\n                    if max_pool:\n                        mp_regions[f, k] = np.amax(features[i, f])\n\n                    wp_regions[f, k] = features[i, f].sum()\n\n        wp_regions = wp_regions * C[:, None]\n        wp_regions /= np.linalg.norm(wp_regions, axis=0)\n\n        if max_pool:\n            mp_regions = mp_regions * C[:, None]\n            mp_regions /= np.linalg.norm(mp_regions, axis=0)\n\n        if region_descriptors:\n            wsp_descriptors_reg[num_classes*i:num_classes*(i+1)] = np.transpose(wp_regions)\n            if max_pool:\n                wmp_descriptors_reg[num_classes*i:num_classes*(i+1)] = np.transpose(mp_regions)\n\n        if pca is not None:\n            wp_regions = np.transpose(pca.transform(np.transpose(wp_regions)))\n            wp_regions /= np.linalg.norm(wp_regions, axis=0)\n            mp_regions = np.transpose(pca.transform(np.transpose(mp_regions)))\n            mp_regions /= np.linalg.norm(mp_regions, axis=0)\n\n\n        wp_batch_representations[i] = wp_regions.sum(axis=1)\n        wp_batch_representations[i] /= np.linalg.norm(wp_batch_representations[i])\n\n        #wp_batch_representations[i][np.where(wp_batch_representations[i] < 0.001)] = 0\n\n        if max_pool:\n            mp_batch_representations[i] = mp_regions.sum(axis=1)\n            mp_batch_representations[i] /= np.linalg.norm(mp_batch_representations[i])\n\n    print 'Time elapsed computing image representations for the batch: ', time.time() - t\n\n    if region_descriptors and max_pool:\n        print wp_batch_representations.shape\n        print wsp_descriptors_reg.shape\n        return wp_batch_representations, mp_batch_representations, wsp_descriptors_reg, wmp_descriptors_reg\n    elif region_descriptors:\n        return wp_batch_representations, wsp_descriptors_reg\n    elif max_pool:\n        return wp_batch_representations, mp_batch_representations\n    else:\n        return wp_batch_representations\n\n\n# Return class vectors (1 per class)\ndef weighted_cam_pooling(features, cams, max_pool=False, channel_weights=True):\n    '''\n    :param features: Feature Maps\n    :param cams: Class Activation Maps\n    :param max_pool: Perform also Max pooling\n    :param channel_weights: Channel Weighting as in Crow\n    :return: A descriptor for each CAM.\n    '''\n    t = time.time()\n    num_samples = features.shape[0]\n    num_features = features.shape[1]\n    num_classes = cams.shape[1]\n\n    wp_regions = np.zeros((num_features, num_classes), dtype=np.float32)\n    wsp_descriptors_reg = np.zeros((num_samples * num_classes, num_features), dtype=np.float32)\n    wmp_descriptors_reg = np.zeros((num_samples * num_classes, num_features), dtype=np.float32)\n\n    if max_pool:\n        mp_regions = np.zeros((num_features, num_classes), dtype=np.float32)\n\n    for i in range(0, num_samples):\n        #CROW\n        if channel_weights:\n            C = np.array(compute_crow_channel_weight(features[i]))\n\n        for f in range(0, num_features):\n            for k in range(0, num_classes):\n                # For each region compute avg weighted sum of activations and l2 normalize\n                if max_pool:\n                        mp_regions[f, k] = np.amax(np.multiply(features[i, f], cams[i, k]))\n                wp_regions[f, k] = np.multiply(features[i, f], cams[i, k]).sum()\n\n        if channel_weights:\n            wp_regions = wp_regions * C[:, None]\n        wp_regions /= np.linalg.norm(wp_regions, axis=0)\n\n        if max_pool:\n            if channel_weights:\n                mp_regions = mp_regions * C[:, None]\n            mp_regions /= np.linalg.norm(mp_regions, axis=0)\n\n        wsp_descriptors_reg[num_classes*i:num_classes*(i+1)] = np.transpose(wp_regions)\n\n        if max_pool:\n            wmp_descriptors_reg[num_classes*i:num_classes*(i+1)] = np.transpose(mp_regions)\n\n    #print 'Time elapsed computing image representations for the batch: ', time.time() - t\n\n    if max_pool:\n        return wsp_descriptors_reg, wmp_descriptors_reg\n    else:\n        return wsp_descriptors_reg\n\n\n# General Descriptor Aggregation : PCA + Aggregation\ndef descriptor_aggregation(descriptors_cams, num_images, num_classes, pca=None):\n\n    num_classes_ori = descriptors_cams.shape[0] / num_images\n    descriptors = np.zeros((num_images, descriptors_cams.shape[1]), dtype=np.float32)\n\n    if pca is not None:\n        # Sometimes we may have errors during re-ranking due to bounding box generation on places where CAM=0\n        try:\n            descriptors_pca = pca.transform(descriptors_cams)\n        except:\n            print '---------------------------->Exception'\n            desc_err = np.zeros((descriptors_cams.shape[0], descriptors_cams.shape[1]), dtype=np.float32)\n            for j in range(0, descriptors_cams.shape[0]):\n                try:\n                    desc_err[j] = pca.transform(descriptors_cams[j])\n                except:\n                    print '------------------> Exception'\n                    print j\n                    desc_err[j] = desc_err[j-1]\n            descriptors_pca = desc_err\n\n        descriptors = np.zeros((num_images, descriptors_pca.shape[1]), dtype=np.float32)\n        #print descriptors_pca.shape\n\n    index = 0\n    for i in range(0, num_images):\n        index = num_classes_ori + index\n        if i == 0:\n            index = 0\n        if pca is not None:\n            for k in range(index, index+num_classes):\n                descriptors_pca[k] /= np.linalg.norm(descriptors_pca[k])\n                descriptors[i] += descriptors_pca[k]\n\n            descriptors[i] /= np.linalg.norm(descriptors[i])\n        else:\n            for k in range(index, index+num_classes):\n                descriptors[i] += descriptors_cams[k]\n            descriptors[i] /= np.linalg.norm(descriptors[i])\n\n    return descriptors\n\n\n# Descriptor aggregation from list of classes : PCA + Aggregation\ndef descriptor_aggregation_cl(descriptors_cams, num_images, pca, class_list):\n    num_classes_ori = descriptors_cams.shape[0] / num_images\n    descriptors = np.zeros((num_images, 512), dtype=np.float32)\n    for i in range(0, num_images):\n        descriptors_good = descriptors_cams[i*num_classes_ori:(i+1)*num_classes_ori]\n        descriptors_good = descriptors_good[class_list]\n        # Sometimes we may have errors during re-ranking due to bounding box generation on places where CAM=0\n        if pca is not None:\n            try:\n                descriptors_pca = pca.transform(descriptors_good)\n            except:\n                print '---------------------------->Exception'\n                desc_err = np.zeros((descriptors_good.shape[0], descriptors_good.shape[1]), dtype=np.float32)\n                for j in range(0, descriptors_good.shape[0]):\n                    try:\n                        desc_err[j] = pca.transform(descriptors_good[j])\n                    except:\n                        print '------------------> Exception'\n                        print j\n                        desc_err[j] = desc_err[j - 1]\n                descriptors_pca = desc_err\n        else:\n            descriptors_pca = descriptors_good\n        for k in range(0, descriptors_pca.shape[0]):\n            descriptors_pca[k] /= np.linalg.norm(descriptors_pca[k])\n            descriptors[i] += descriptors_pca[k]\n\n        descriptors[i] /= np.linalg.norm(descriptors[i])\n\n    return descriptors\n\n\n# Retrieve n most probable class vectors (For PCA mainly)\ndef retrieve_n_descriptors(num_classes, num_images, all_descriptors):\n    num_classes_ori = all_descriptors.shape[0] / num_images\n    descriptors = np.zeros((num_images * num_classes, all_descriptors.shape[1]), dtype=np.float32)\n\n    index = 0\n    for i in range(0, num_images):\n        index = num_classes_ori + index\n        if i == 0:\n            index = 0\n        descriptors[i*num_classes:num_classes*(i+1)] = all_descriptors[index:index+num_classes]\n    return descriptors\n\n\n# Retrieve n most probable class vectors (For PCA mainly)\ndef retrieve_n_descriptors_concat(num_classes, num_images, all_descriptors):\n    num_classes_ori = all_descriptors.shape[0] / num_images\n    descriptors = np.zeros((num_images, all_descriptors.shape[1]*num_classes), dtype=np.float32)\n\n    index = 0\n    for i in range(0, num_images):\n        index = num_classes_ori + index\n        if i == 0:\n            index = 0\n        for k in range(index, index + num_classes):\n            if k == index:\n                d = all_descriptors[k]\n            else:\n                d = np.concatenate((d, all_descriptors[k]), axis=0)\n        descriptors[i] = d\n    return descriptors\n\n\n# General Descriptor Aggregation : PCA + Aggregation\ndef descriptor_aggregation_concat(descriptors_cams, num_images, num_classes, pca=None):\n\n    num_classes_ori = descriptors_cams.shape[0] / num_images\n    descriptors = np.zeros((num_images, descriptors_cams.shape[1]*num_classes), dtype=np.float32)\n    print descriptors_cams.shape\n    index = 0\n    for i in range(0, num_images):\n        index = num_classes_ori + index\n        if i == 0:\n            index = 0\n        if pca is not None:\n            for k in range(index, index+num_classes):\n                if k == index:\n                    d = descriptors_cams[k]\n                else:\n                    d = np.concatenate((d, descriptors_cams[k]), axis=0)\n            descriptors[i] = pca.transform(d)\n            descriptors[i] /= np.linalg.norm(descriptors[i])\n\n    return descriptors\n"""
pytorch_code/reranking.py,2,"b""import numpy as np\nimport os\nimport h5py\nimport sys\nimport time\nfrom utils import preprocess_images\nfrom pooling_functions import weighted_cam_pooling, descriptor_aggregation\nfrom cam_functions import extract_feat_cam\nfrom scipy.misc import imread\nimport torch\n\n\n\n\n# Image Preprocessing\nsize_v = [720, 1024]\nsize_h = [1024, 720]\n\nstats = list()\nmean = [0.485, 0.456, 0.406]\nstd = [0.229, 0.224, 0.225]\nstats.append(mean)\nstats.append(std)\n\ndim_descriptors = 512\n\nn_images_oxford = 5063\nn_images_paris = 6392\n\n\n# Compute score using CAMs, PCA , Region of interest\ndef compute_scores_cams(desc_query, features_img, cams, roi, pca_matrix):\n    #print 'Feat shape:', features_img.shape\n    #print 'Cams shape', cams.shape\n    nthres = 4\n    scores = np.zeros(features_img.shape[0])\n    feats = np.zeros((1, features_img.shape[1], features_img.shape[2], features_img.shape[3]), dtype=np.float32)\n    cams_ = np.zeros((1, cams.shape[1], cams.shape[2], cams.shape[3]), dtype=np.float32)\n    final_descriptors = np.zeros((features_img.shape[0], features_img.shape[1]), dtype=np.float32)\n    for img_ind in range(features_img.shape[0]):\n        #print features_img[img_ind].shape\n        feats[0] = features_img[img_ind]\n        cams_[0] = cams[img_ind]\n        scores[img_ind] = -10\n        #print 'Img: ', img_ind\n        x, y, w, h = roi[img_ind, :, 0], roi[img_ind, :, 1], roi[img_ind, :, 2], roi[img_ind, :, 3]\n        for th in range(0, nthres):\n            #print y[th], y[th] + h[th]\n            #print x[th], x[th] + w[th]\n            sys.stdout.flush()\n            if h[th] >= 5 and w[th] >= 5:\n                d_wp = weighted_cam_pooling(feats[:, :,y[th]:y[th]+h[th], x[th]:x[th]+w[th]],\n                                            cams_[:, :, y[th]:y[th]+h[th], x[th]:x[th]+w[th]], max_pool=False)\n                descriptor = descriptor_aggregation(d_wp, 1, cams.shape[1], pca_matrix)\n\n                score_aux = np.dot(desc_query, np.transpose(descriptor))\n\n                #print 'Thresh: ', th\n                #print 'Score:', score_aux\n                if score_aux > scores[img_ind]:\n                    #print 'Max in th:', th\n                    scores[img_ind] = np.copy(score_aux)\n                    final_descriptors[img_ind] = descriptor\n            else:\n                pass\n    return scores, final_descriptors\n\n\ndef re_order(order, vector_h, vector_v):\n    vector = list()\n    count_h = 0\n    count_v = 0\n    for pos in order:\n        if pos == 0:\n            vector.append(vector_h[count_h])\n            count_h += 1\n        elif pos == 1:\n            vector.append(vector_v[count_v])\n            count_v += 1\n    return vector\n\n\ndef re_ranking(desc_query, class_list, batch_size, image_names, indices, dataset, top_n_ranking, pca_matrix, model, model_name):\n    if dataset == 'Oxford' or dataset == 'Oxford105k':\n        images_path = '/data/jim011/datasets_retrieval/Oxford5k/images/'\n\n    if dataset == 'Paris' or dataset == 'Paris106k':\n        images_path = '/data/jim011/datasets_retrieval/Paris6k/images/'\n\n    index_q = indices[0:top_n_ranking]\n    tt = time.time()\n    indexed_names = list()\n    i = 0\n    if top_n_ranking >= 1000:\n        image_batch = 300\n    else:\n        image_batch = top_n_ranking\n\n    n_iterations = int(math.floor(top_n_ranking / image_batch))\n    last_batch = top_n_ranking % image_batch\n    scores = np.zeros(top_n_ranking, dtype=np.float32)\n    scores_h = np.zeros(top_n_ranking, dtype=np.float32)\n    scores_v = np.zeros(top_n_ranking, dtype=np.float32)\n    final_desc_h = np.zeros(top_n_ranking, dtype=np.float32)\n    final_desc_v = np.zeros(top_n_ranking, dtype=np.float32)\n    print desc_query.shape\n    final_descriptors_all = np.zeros((top_n_ranking, desc_query.shape[1]), dtype=np.float32)\n    image_ranked_names = image_names[index_q]\n\n    num_cams = class_list.shape[0]\n\n    for k in range(0, n_iterations+1):\n        images_h = list()\n        images_v = list()\n        images_ver = False\n        images_hor = False\n        t1 = time.time()\n        if k == n_iterations:\n            #Last Batch\n            if last_batch != 0:\n                last_ind = image_batch * k + last_batch\n            else:\n                break\n        else:\n            last_ind = image_batch * (k+1)\n\n        print image_names[index_q[k*image_batch:last_ind]]\n\n        # Separate the images in Horizontal/Vertical for faster processing\n        image_order = list()\n        for ind_im, name in enumerate(image_names[index_q[k*image_batch:last_ind]]):\n            if name[0] == '/':\n                im = imread(name.replace('\\n',''))\n            else:\n                im = imread(images_path + name.replace('\\n', '') + '.jpg')\n            if im.shape[0] >= im.shape[1]:\n                images_v.append(im)\n                images_ver = True\n                image_order.append(1)\n            else:\n                images_h.append(im)\n                images_hor = True\n                image_order.append(0)\n\n        # Extract Features/CAMs\n        print 'Time loading images: ', time.time() - t1\n\n        if images_hor:\n            size = size_h\n            t2 = time.time()\n            images_tensor = preprocess_images(images_h, size[0], size[1], stats[0], stats[1])\n            images_tensor = torch.autograd.Variable(images_tensor, volatile=True)\n            features_h, cams_h, roi_h = extract_feat_cam(model, model_name, layer, batch_size, images_tensor, num_cams,\n                                                         class_list, roi=True)\n            print 'Time extracting features hor: ', time.time() - t2\n            t3 = time.time()\n            scores_h, final_desc_h = compute_scores_cams(desc_query, features_h, cams_h, roi_h, pca_matrix)\n            print 'Time computing scores: ', time.time() - t3\n            print scores_h\n\n        if images_ver:\n            size = size_v\n            t2 = time.time()\n            images_tensor = preprocess_images(images_v, size[0], size[1], stats[0], stats[1])\n            images_tensor = torch.autograd.Variable(images_tensor, volatile=True)\n            features_v, cams_v, roi_v = extract_feat_cam(model, model_name, layer, batch_size, images_tensor, num_cams,\n                                                         class_list, roi=True)\n            print 'Time extracting features ver: ', time.time() - t2\n            t3 = time.time()\n            scores_v, final_desc_v = compute_scores_cams(desc_query, features_v, cams_v, roi_v, pca_matrix)\n            print 'Time computing scores: ', time.time() - t3\n            print scores_v\n\n        # Compute Scores\n        print image_order\n        # Re-order\n        scores[k*image_batch:last_ind] = re_order(image_order, scores_h, scores_v)\n        final_descriptors_all[k*image_batch:last_ind] = re_order(image_order, final_desc_h, final_desc_v)\n        print final_descriptors_all.shape\n\n        print scores[k*image_batch:image_batch*(k+1)]\n        print 'Time loading computing scores: ', time.time() - t2\n        print 'Time elapsed x image:', time.time() - t1\n\n    print scores\n    ordered_sc = scores.argsort()[::-1]\n    print ordered_sc\n    print image_names[index_q]\n    print image_ranked_names[ordered_sc]\n    # Index of the in order of relevance\n    ordered_ind = index_q[ordered_sc]\n    indexed_names.append(np.copy(image_ranked_names[ordered_sc]))\n    indices[0:top_n_ranking] = ordered_ind\n    i += 1\n    print 'Time elapsed:', time.time()-tt\n    # Return indices and data ordered by similarity with the query\n    return indices, final_descriptors_all[ordered_sc]\n"""
pytorch_code/resnet.py,7,"b'import torch.nn as nn\nimport math\nimport torch.utils.model_zoo as model_zoo\n\n\n__all__ = [\'ResNet\', \'resnet18\', \'resnet34\', \'resnet50\', \'resnet101\',\n           \'resnet152\']\n\n\nmodel_urls = {\n    \'resnet18\': \'https://download.pytorch.org/models/resnet18-5c106cde.pth\',\n    \'resnet34\': \'https://download.pytorch.org/models/resnet34-333f7ec4.pth\',\n    \'resnet50\': \'https://download.pytorch.org/models/resnet50-19c8e357.pth\',\n    \'resnet101\': \'https://download.pytorch.org/models/resnet101-5d3b4d8f.pth\',\n    \'resnet152\': \'https://download.pytorch.org/models/resnet152-b121ed2d.pth\',\n}\n\n\ndef conv3x3(in_planes, out_planes, stride=1):\n    ""3x3 convolution with padding""\n    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n                     padding=1, bias=False)\n\n\nclass BasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None):\n        super(BasicBlock, self).__init__()\n        self.conv1 = conv3x3(inplanes, planes, stride)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = conv3x3(planes, planes)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\n\nclass Bottleneck(nn.Module):\n    expansion = 4\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None):\n        super(Bottleneck, self).__init__()\n        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,\n                               padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)\n        self.bn3 = nn.BatchNorm2d(planes * 4)\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\n\nclass ResNet(nn.Module):\n\n    def __init__(self, block, layers, num_classes=1000):\n        self.inplanes = 64\n        super(ResNet, self).__init__()\n        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3,\n                               bias=False)\n        self.bn1 = nn.BatchNorm2d(64)\n        self.relu = nn.ReLU(inplace=True)\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n        self.layer1 = self._make_layer(block, 64, layers[0])\n        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n        self.avgpool = nn.AdaptiveAvgPool2d(1)\n        self.fc = nn.Linear(512 * block.expansion, num_classes)\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n                m.weight.data.normal_(0, math.sqrt(2. / n))\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n\n    def _make_layer(self, block, planes, blocks, stride=1):\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                nn.Conv2d(self.inplanes, planes * block.expansion,\n                          kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(planes * block.expansion),\n            )\n\n        layers = []\n        layers.append(block(self.inplanes, planes, stride, downsample))\n        self.inplanes = planes * block.expansion\n        for i in range(1, blocks):\n            layers.append(block(self.inplanes, planes))\n\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        feat = self.layer4(x)\n\n        x = self.avgpool(feat)\n        x = x.view(x.size(0), -1)\n        x = self.fc(x)\n\n        return feat, x\n\n\ndef resnet18(pretrained=False, **kwargs):\n    """"""Constructs a ResNet-18 model.\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = ResNet(BasicBlock, [2, 2, 2, 2], **kwargs)\n    if pretrained:\n        model.load_state_dict(model_zoo.load_url(model_urls[\'resnet18\']))\n    return model\n\n\ndef resnet34(pretrained=False, **kwargs):\n    """"""Constructs a ResNet-34 model.\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = ResNet(BasicBlock, [3, 4, 6, 3], **kwargs)\n    if pretrained:\n        model.load_state_dict(model_zoo.load_url(model_urls[\'resnet34\']))\n    return model\n\n\ndef resnet50(pretrained=False, **kwargs):\n    """"""Constructs a ResNet-50 model.\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = ResNet(Bottleneck, [3, 4, 6, 3], **kwargs)\n    if pretrained:\n        model.load_state_dict(model_zoo.load_url(model_urls[\'resnet50\']))\n    return model\n\n\ndef resnet101(pretrained=False, **kwargs):\n    """"""Constructs a ResNet-101 model.\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = ResNet(Bottleneck, [3, 4, 23, 3], **kwargs)\n    if pretrained:\n        model.load_state_dict(model_zoo.load_url(model_urls[\'resnet101\']))\n    return model\n\n\ndef resnet152(pretrained=False, **kwargs):\n    """"""Constructs a ResNet-152 model.\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = ResNet(Bottleneck, [3, 8, 36, 3], **kwargs)\n    if pretrained:\n        model.load_state_dict(model_zoo.load_url(model_urls[\'resnet152\']))\n    return model'"
pytorch_code/utils.py,2,"b'import torchvision.transforms as transforms\nimport torch\nimport numpy as np\nimport os\nimport math\nimport h5py\nimport matplotlib.pyplot as plt\nimport sys\nfrom scipy.misc import imread, imresize, imsave\nimport time\n\n\ndef create_folders(path):\n    if not os.path.exists(path):\n        print \'Creating path: \', path\n        os.makedirs(path)\n    else:\n        print \'Path already exists\'\n\n\ndef load_data(filepath):\n    with h5py.File(filepath, \'r\') as hf:\n        data = np.array(hf.get(\'data\'))\n        #print \'Shape of the array features: \', data.shape\n        return data\n\n\ndef save_data(data, path, name):\n    with h5py.File(path + name, \'w\') as hf:\n        hf.create_dataset(\'data\', data=data)\n\n\ndef preprocess_images(images, img_width, img_height, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]):\n    time_p = time.time()\n    normalize = transforms.Normalize(mean, std)\n    preprocess = transforms.Compose([transforms.ToTensor(), normalize])\n\n    if isinstance(images, list):\n        print (""Preprocessing Images... "")\n        num_images = len(images)\n        x = torch.FloatTensor(num_images, 3, img_height, img_width)\n        for i in range(0, num_images):\n            images[i] = imresize(images[i], [img_height, img_width]).astype(dtype=np.float32)\n            x[i, :, :, :] = preprocess(images[i])\n    else:\n        print (""Preprocessing Image... "")\n        x = torch.FloatTensor(1, 3, img_height, img_width)\n        images = imresize(images, [img_height, img_width]).astype(dtype=np.float32)\n        x[0, :, :, :] = preprocess(images)\n    print x.size()\n    print \'Time elapsed preprocessing: \', time.time()-time_p\n    return x\n\n\ndef print_classes(dictionary_labels, vector_classes):\n    for vc in vector_classes:\n        print dictionary_labels[vc]'"
