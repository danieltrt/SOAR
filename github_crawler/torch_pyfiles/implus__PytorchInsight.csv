file_path,api_count,code
classification/flops_counter.py,14,"b'import torch.nn as nn\nimport torch\nimport numpy as np\n\ndef get_model_complexity_info(model, input_res, print_per_layer_stat=True, as_strings=True, channel=3):\n    assert type(input_res) is tuple\n    assert len(input_res) == 2\n    batch = torch.FloatTensor(1, channel, *input_res)\n    flops_model = add_flops_counting_methods(model)\n    flops_model.eval().start_flops_count()\n    out = flops_model(batch)\n\n    if print_per_layer_stat:\n        print_model_with_flops(flops_model)\n    flops_count = flops_model.compute_average_flops_cost()\n    params_count = get_model_parameters_number(flops_model)\n    flops_model.stop_flops_count()\n\n    if as_strings:\n        return flops_to_string(flops_count), params_to_string(params_count)\n\n    return flops_count, params_count\n\ndef flops_to_string(flops, units=\'GMac\', precision=2):\n    if units is None:\n        if flops // 10**9 > 0:\n            return str(round(flops / 10.**9, precision)) + \' GMac\'\n        elif flops // 10**6 > 0:\n            return str(round(flops / 10.**6, precision)) + \' MMac\'\n        elif flops // 10**3 > 0:\n            return str(round(flops / 10.**3, precision)) + \' KMac\'\n        else:\n            return str(flops) + \' Mac\'\n    else:\n        if units == \'GMac\':\n            return str(round(flops / 10.**9, precision)) + \' \' + units\n        elif units == \'MMac\':\n            return str(round(flops / 10.**6, precision)) + \' \' + units\n        elif units == \'KMac\':\n            return str(round(flops / 10.**3, precision)) + \' \' + units\n        else:\n            return str(flops) + \' Mac\'\n\ndef params_to_string(params_num):\n    if params_num // 10 ** 6 > 0:\n        return str(round(params_num / 10 ** 6, 2)) + \' M\'\n    elif params_num // 10 ** 3:\n        return str(round(params_num / 10 ** 3, 2)) + \' k\'\n\ndef print_model_with_flops(model, units=\'GMac\', precision=3):\n    total_flops = model.compute_average_flops_cost()\n\n    def accumulate_flops(self):\n        if is_supported_instance(self):\n            return self.__flops__ / model.__batch_counter__\n        else:\n            sum = 0\n            for m in self.children():\n                sum += m.accumulate_flops()\n            return sum\n\n    def flops_repr(self):\n        accumulated_flops_cost = self.accumulate_flops()\n        return \', \'.join([flops_to_string(accumulated_flops_cost, units=units, precision=precision),\n                          \'{:.3%} MACs\'.format(accumulated_flops_cost / total_flops),\n                          self.original_extra_repr()])\n\n    def add_extra_repr(m):\n        m.accumulate_flops = accumulate_flops.__get__(m)\n        flops_extra_repr = flops_repr.__get__(m)\n        if m.extra_repr != flops_extra_repr:\n            m.original_extra_repr = m.extra_repr\n            m.extra_repr = flops_extra_repr\n            assert m.extra_repr != m.original_extra_repr\n\n    def del_extra_repr(m):\n        if hasattr(m, \'original_extra_repr\'):\n            m.extra_repr = m.original_extra_repr\n            del m.original_extra_repr\n        if hasattr(m, \'accumulate_flops\'):\n            del m.accumulate_flops\n\n    model.apply(add_extra_repr)\n    print(model)\n    model.apply(del_extra_repr)\n\ndef get_model_parameters_number(model):\n    params_num = sum(p.numel() for p in model.parameters() if p.requires_grad)\n    return params_num\n\ndef add_flops_counting_methods(net_main_module):\n    # adding additional methods to the existing module object,\n    # this is done this way so that each function has access to self object\n    net_main_module.start_flops_count = start_flops_count.__get__(net_main_module)\n    net_main_module.stop_flops_count = stop_flops_count.__get__(net_main_module)\n    net_main_module.reset_flops_count = reset_flops_count.__get__(net_main_module)\n    net_main_module.compute_average_flops_cost = compute_average_flops_cost.__get__(net_main_module)\n\n    net_main_module.reset_flops_count()\n\n    # Adding variables necessary for masked flops computation\n    net_main_module.apply(add_flops_mask_variable_or_reset)\n\n    return net_main_module\n\n\ndef compute_average_flops_cost(self):\n    """"""\n    A method that will be available after add_flops_counting_methods() is called\n    on a desired net object.\n\n    Returns current mean flops consumption per image.\n\n    """"""\n\n    batches_count = self.__batch_counter__\n    flops_sum = 0\n    for module in self.modules():\n        if is_supported_instance(module):\n            flops_sum += module.__flops__\n\n    return flops_sum / batches_count\n\n\ndef start_flops_count(self):\n    """"""\n    A method that will be available after add_flops_counting_methods() is called\n    on a desired net object.\n\n    Activates the computation of mean flops consumption per image.\n    Call it before you run the network.\n\n    """"""\n    add_batch_counter_hook_function(self)\n    self.apply(add_flops_counter_hook_function)\n\n\ndef stop_flops_count(self):\n    """"""\n    A method that will be available after add_flops_counting_methods() is called\n    on a desired net object.\n\n    Stops computing the mean flops consumption per image.\n    Call whenever you want to pause the computation.\n\n    """"""\n    remove_batch_counter_hook_function(self)\n    self.apply(remove_flops_counter_hook_function)\n\n\ndef reset_flops_count(self):\n    """"""\n    A method that will be available after add_flops_counting_methods() is called\n    on a desired net object.\n\n    Resets statistics computed so far.\n\n    """"""\n    add_batch_counter_variables_or_reset(self)\n    self.apply(add_flops_counter_variable_or_reset)\n\n\ndef add_flops_mask(module, mask):\n    def add_flops_mask_func(module):\n        if isinstance(module, torch.nn.Conv2d):\n            module.__mask__ = mask\n    module.apply(add_flops_mask_func)\n\n\ndef remove_flops_mask(module):\n    module.apply(add_flops_mask_variable_or_reset)\n\n\n# ---- Internal functions\ndef is_supported_instance(module):\n    if isinstance(module, (torch.nn.Conv2d, torch.nn.ReLU, torch.nn.PReLU, torch.nn.ELU, \\\n                           torch.nn.LeakyReLU, torch.nn.ReLU6, torch.nn.Linear, \\\n                           torch.nn.MaxPool2d, torch.nn.AvgPool2d, torch.nn.BatchNorm2d, \\\n                           torch.nn.Upsample, nn.AdaptiveMaxPool2d, nn.AdaptiveAvgPool2d)):\n        return True\n\n    return False\n\n\ndef empty_flops_counter_hook(module, input, output):\n    module.__flops__ += 0\n\n\ndef upsample_flops_counter_hook(module, input, output):\n    output_size = output[0]\n    batch_size = output_size.shape[0]\n    output_elements_count = batch_size\n    for val in output_size.shape[1:]:\n        output_elements_count *= val\n    module.__flops__ += output_elements_count\n\n\ndef relu_flops_counter_hook(module, input, output):\n    active_elements_count = output.numel()\n    module.__flops__ += active_elements_count\n\n\ndef linear_flops_counter_hook(module, input, output):\n    input = input[0]\n    batch_size = input.shape[0]\n    module.__flops__ += batch_size * input.shape[1] * output.shape[1]\n\n\ndef pool_flops_counter_hook(module, input, output):\n    input = input[0]\n    module.__flops__ += np.prod(input.shape)\n\ndef bn_flops_counter_hook(module, input, output):\n    module.affine\n    input = input[0]\n\n    batch_flops = np.prod(input.shape)\n    if module.affine:\n        batch_flops *= 2\n    module.__flops__ += batch_flops\n\ndef conv_flops_counter_hook(conv_module, input, output):\n    # Can have multiple inputs, getting the first one\n    input = input[0]\n\n    batch_size = input.shape[0]\n    output_height, output_width = output.shape[2:]\n\n    kernel_height, kernel_width = conv_module.kernel_size\n    in_channels = conv_module.in_channels\n    out_channels = conv_module.out_channels\n    groups = conv_module.groups\n\n    filters_per_channel = out_channels // groups\n    conv_per_position_flops = kernel_height * kernel_width * in_channels * filters_per_channel\n\n    active_elements_count = batch_size * output_height * output_width\n\n    if conv_module.__mask__ is not None:\n        # (b, 1, h, w)\n        flops_mask = conv_module.__mask__.expand(batch_size, 1, output_height, output_width)\n        active_elements_count = flops_mask.sum()\n\n    overall_conv_flops = conv_per_position_flops * active_elements_count\n\n    bias_flops = 0\n\n    if conv_module.bias is not None:\n\n        bias_flops = out_channels * active_elements_count\n\n    overall_flops = overall_conv_flops + bias_flops\n\n    conv_module.__flops__ += overall_flops\n\n\ndef batch_counter_hook(module, input, output):\n    # Can have multiple inputs, getting the first one\n    input = input[0]\n    batch_size = input.shape[0]\n    module.__batch_counter__ += batch_size\n\n\ndef add_batch_counter_variables_or_reset(module):\n\n    module.__batch_counter__ = 0\n\n\ndef add_batch_counter_hook_function(module):\n    if hasattr(module, \'__batch_counter_handle__\'):\n        return\n\n    handle = module.register_forward_hook(batch_counter_hook)\n    module.__batch_counter_handle__ = handle\n\n\ndef remove_batch_counter_hook_function(module):\n    if hasattr(module, \'__batch_counter_handle__\'):\n        module.__batch_counter_handle__.remove()\n        del module.__batch_counter_handle__\n\n\ndef add_flops_counter_variable_or_reset(module):\n    if is_supported_instance(module):\n        module.__flops__ = 0\n\n\ndef add_flops_counter_hook_function(module):\n    if is_supported_instance(module):\n        if hasattr(module, \'__flops_handle__\'):\n            return\n\n        if isinstance(module, torch.nn.Conv2d):\n            handle = module.register_forward_hook(conv_flops_counter_hook)\n        elif isinstance(module, (torch.nn.ReLU, torch.nn.PReLU, torch.nn.ELU, \\\n                                 torch.nn.LeakyReLU, torch.nn.ReLU6)):\n            handle = module.register_forward_hook(relu_flops_counter_hook)\n        elif isinstance(module, torch.nn.Linear):\n            handle = module.register_forward_hook(linear_flops_counter_hook)\n        elif isinstance(module, (torch.nn.AvgPool2d, torch.nn.MaxPool2d, nn.AdaptiveMaxPool2d, \\\n                                 nn.AdaptiveAvgPool2d)):\n            handle = module.register_forward_hook(pool_flops_counter_hook)\n        elif isinstance(module, torch.nn.BatchNorm2d):\n            handle = module.register_forward_hook(bn_flops_counter_hook)\n        elif isinstance(module, torch.nn.Upsample):\n            handle = module.register_forward_hook(upsample_flops_counter_hook)\n        else:\n            handle = module.register_forward_hook(empty_flops_counter_hook)\n        module.__flops_handle__ = handle\n\n\ndef remove_flops_counter_hook_function(module):\n    if is_supported_instance(module):\n        if hasattr(module, \'__flops_handle__\'):\n            module.__flops_handle__.remove()\n            del module.__flops_handle__\n# --- Masked flops counting\n\n\n# Also being run in the initialization\ndef add_flops_mask_variable_or_reset(module):\n    if is_supported_instance(module):\n        module.__mask__ = None\n'"
classification/imagenet.py,18,"b'from __future__ import print_function\nimport sys\n\nimport argparse\nimport os\nimport shutil\nimport time\nimport random\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.parallel\nimport torch.backends.cudnn as cudnn\nimport torch.optim as optim\nimport torch.utils.data as data\nimport torchvision.transforms as transforms\nimport torchvision.datasets as datasets\nimport torchvision.models as models\nimport models.imagenet as customized_models\nfrom flops_counter import get_model_complexity_info\nfrom PIL import ImageFile\nImageFile.LOAD_TRUNCATED_IMAGES = True\n\nfrom utils import Bar, Logger, AverageMeter, accuracy, mkdir_p\n\n# for servers to immediately record the logs\ndef flush_print(func):\n    def new_print(*args, **kwargs):\n        func(*args, **kwargs)\n        sys.stdout.flush()\n    return new_print\nprint = flush_print(print)\n\n\n# Models\ndefault_model_names = sorted(name for name in models.__dict__\n    if name.islower() and not name.startswith(""__"")\n    and callable(models.__dict__[name]))\n\ncustomized_models_names = sorted(name for name in customized_models.__dict__\n    if name.islower() and not name.startswith(""__"")\n    and callable(customized_models.__dict__[name]))\n\nfor name in customized_models.__dict__:\n    if name.islower() and not name.startswith(""__"") and callable(customized_models.__dict__[name]):\n        models.__dict__[name] = customized_models.__dict__[name]\n\nmodel_names = default_model_names + customized_models_names\n\n# Parse arguments\nparser = argparse.ArgumentParser(description=\'PyTorch ImageNet Training\')\n\n# Datasets\nparser.add_argument(\'-d\', \'--data\', default=\'path to dataset\', type=str)\nparser.add_argument(\'-j\', \'--workers\', default=32, type=int, metavar=\'N\',\n                    help=\'number of data loading workers (default: 4)\')\n# Optimization options\nparser.add_argument(\'--epochs\', default=90, type=int, metavar=\'N\',\n                    help=\'number of total epochs to run\')\nparser.add_argument(\'--start-epoch\', default=0, type=int, metavar=\'N\',\n                    help=\'manual epoch number (useful on restarts)\')\nparser.add_argument(\'--train-batch\', default=256, type=int, metavar=\'N\',\n                    help=\'train batchsize (default: 256)\')\nparser.add_argument(\'--test-batch\', default=200, type=int, metavar=\'N\',\n                    help=\'test batchsize (default: 200)\')\nparser.add_argument(\'--lr\', \'--learning-rate\', default=0.1, type=float,\n                    metavar=\'LR\', help=\'initial learning rate\')\nparser.add_argument(\'--drop\', \'--dropout\', default=0, type=float,\n                    metavar=\'Dropout\', help=\'Dropout ratio\')\nparser.add_argument(\'--schedule\', type=int, nargs=\'+\', default=[150, 225],\n                        help=\'Decrease learning rate at these epochs.\')\nparser.add_argument(\'--gamma\', type=float, default=0.1, help=\'LR is multiplied by gamma on schedule.\')\nparser.add_argument(\'--momentum\', default=0.9, type=float, metavar=\'M\',\n                    help=\'momentum\')\nparser.add_argument(\'--weight-decay\', \'--wd\', default=1e-4, type=float,\n                    metavar=\'W\', help=\'weight decay (default: 1e-4)\')\n# Checkpoints\nparser.add_argument(\'-c\', \'--checkpoint\', default=\'checkpoint\', type=str, metavar=\'PATH\',\n                    help=\'path to save checkpoint (default: checkpoint)\')\nparser.add_argument(\'--resume\', default=\'\', type=str, metavar=\'PATH\',\n                    help=\'path to latest checkpoint (default: none)\')\n# Architecture\nparser.add_argument(\'--modelsize\', \'-ms\', metavar=\'large\', default=\'large\', \\\n                    choices=[\'large\', \'small\'], \\\n                    help = \'model_size affects the data augmentation, please choose:\' + \\\n                           \' large or small \')\nparser.add_argument(\'--arch\', \'-a\', metavar=\'ARCH\', default=\'resnet18\',\n                    choices=model_names,\n                    help=\'model architecture: \' +\n                        \' | \'.join(model_names) +\n                        \' (default: resnet18)\')\nparser.add_argument(\'--depth\', type=int, default=29, help=\'Model depth.\')\nparser.add_argument(\'--cardinality\', type=int, default=32, help=\'ResNet cardinality (group).\')\nparser.add_argument(\'--base-width\', type=int, default=4, help=\'ResNet base width.\')\nparser.add_argument(\'--widen-factor\', type=int, default=4, help=\'Widen factor. 4 -> 64, 8 -> 128, ...\')\n# Miscs\nparser.add_argument(\'--manualSeed\', type=int, help=\'manual seed\')\nparser.add_argument(\'-e\', \'--evaluate\', dest=\'evaluate\', action=\'store_true\',\n                    help=\'evaluate model on validation set\')\nparser.add_argument(\'--pretrained\', dest=\'pretrained\', action=\'store_true\',\n                    help=\'use pre-trained model\')\n#Device options\nparser.add_argument(\'--gpu-id\', default=\'0\', type=str,\n                    help=\'id(s) for CUDA_VISIBLE_DEVICES\')\n\nargs = parser.parse_args()\nstate = {k: v for k, v in args._get_kwargs()}\n\n# Use CUDA\nos.environ[\'CUDA_VISIBLE_DEVICES\'] = args.gpu_id\nuse_cuda = torch.cuda.is_available()\n\n# Random seed\nif args.manualSeed is None:\n    args.manualSeed = random.randint(1, 10000)\nrandom.seed(args.manualSeed)\ntorch.manual_seed(args.manualSeed)\nif use_cuda:\n    torch.cuda.manual_seed_all(args.manualSeed)\n\nbest_acc = 0  # best test accuracy\n\ndef main():\n    global best_acc\n    start_epoch = args.start_epoch  # start from epoch 0 or last checkpoint epoch\n\n    if not os.path.isdir(args.checkpoint):\n        mkdir_p(args.checkpoint)\n\n    # Data loading code\n    traindir = os.path.join(args.data, \'train\')\n    valdir = os.path.join(args.data, \'valf\')\n    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                                     std=[0.229, 0.224, 0.225])\n\n    data_aug_scale = (0.08, 1.0) if args.modelsize == \'large\' else (0.2, 1.0)\n\n    train_loader = torch.utils.data.DataLoader(\n        datasets.ImageFolder(traindir, transforms.Compose([\n            transforms.RandomResizedCrop(224, scale = data_aug_scale),\n            transforms.RandomHorizontalFlip(),\n            transforms.ToTensor(),\n            normalize,\n        ])),\n        batch_size=args.train_batch, shuffle=True,\n        num_workers=args.workers, pin_memory=True)\n\n    val_loader = torch.utils.data.DataLoader(\n        datasets.ImageFolder(valdir, transforms.Compose([\n            transforms.Scale(256),\n            transforms.CenterCrop(224),\n            transforms.ToTensor(),\n            normalize,\n        ])),\n        batch_size=args.test_batch, shuffle=True,\n        num_workers=args.workers, pin_memory=True)\n\n    # create model\n    if args.pretrained:\n        print(""=> using pre-trained model \'{}\'"".format(args.arch))\n        model = models.__dict__[args.arch](pretrained=True)\n    elif \'resnext\' in args.arch:\n        model = models.__dict__[args.arch](\n                    baseWidth=args.base_width,\n                    cardinality=args.cardinality,\n                )\n    else:\n        print(""=> creating model \'{}\'"".format(args.arch))\n        model = models.__dict__[args.arch]()\n\n    flops, params = get_model_complexity_info(model, (224, 224), as_strings=False, print_per_layer_stat=False)\n    print(\'Flops:  %.3f\' % (flops / 1e9))\n    print(\'Params: %.2fM\' % (params / 1e6))\n\n\n    if args.arch.startswith(\'alexnet\') or args.arch.startswith(\'vgg\'):\n        model.features = torch.nn.DataParallel(model.features)\n        model.cuda()\n    else:\n        model = torch.nn.DataParallel(model).cuda()\n\n    cudnn.benchmark = True\n\n    # define loss function (criterion) and optimizer\n    criterion = nn.CrossEntropyLoss().cuda()\n    optimizer = optim.SGD(model.parameters(), lr=args.lr, momentum=args.momentum, weight_decay=args.weight_decay)\n\n    # Resume\n    title = \'ImageNet-\' + args.arch\n    if args.resume:\n        # Load checkpoint.\n        print(\'==> Resuming from checkpoint..\', args.resume)\n        assert os.path.isfile(args.resume), \'Error: no checkpoint directory found!\'\n        args.checkpoint = os.path.dirname(args.resume)\n        checkpoint = torch.load(args.resume)\n        best_acc = checkpoint[\'best_acc\']\n        start_epoch = checkpoint[\'epoch\']\n        # model may have more keys\n        t = model.state_dict()\n        c = checkpoint[\'state_dict\']\n        flag = True \n        for k in t:\n            if k not in c:\n                print(\'not in loading dict! fill it\', k, t[k])\n                c[k] = t[k]\n                flag = False\n        model.load_state_dict(c)\n        if flag:\n            print(\'optimizer load old state\')\n            optimizer.load_state_dict(checkpoint[\'optimizer\'])\n        else:\n            print(\'new optimizer !\')\n        logger = Logger(os.path.join(args.checkpoint, \'log.txt\'), title=title, resume=True)\n    else:\n        logger = Logger(os.path.join(args.checkpoint, \'log.txt\'), title=title)\n        logger.set_names([\'Learning Rate\', \'Train Loss\', \'Valid Loss\', \'Train Acc.\', \'Valid Acc.\'])\n\n\n    if args.evaluate:\n        print(\'\\nEvaluation only\')\n        test_loss, test_acc = test(val_loader, model, criterion, start_epoch, use_cuda)\n        print(\' Test Loss:  %.8f, Test Acc:  %.2f\' % (test_loss, test_acc))\n        return\n\n    # Train and val\n    for epoch in range(start_epoch, args.epochs):\n        adjust_learning_rate(optimizer, epoch)\n\n        print(\'\\nEpoch: [%d | %d] LR: %f\' % (epoch + 1, args.epochs, state[\'lr\']))\n\n        train_loss, train_acc = train(train_loader, model, criterion, optimizer, epoch, use_cuda)\n        test_loss, test_acc = test(val_loader, model, criterion, epoch, use_cuda)\n\n        # append logger file\n        logger.append([state[\'lr\'], train_loss, test_loss, train_acc, test_acc])\n\n        # save model\n        is_best = test_acc > best_acc\n        best_acc = max(test_acc, best_acc)\n        save_checkpoint({\n                \'epoch\': epoch + 1,\n                \'state_dict\': model.state_dict(),\n                \'acc\': test_acc,\n                \'best_acc\': best_acc,\n                \'optimizer\' : optimizer.state_dict(),\n            }, is_best, checkpoint=args.checkpoint)\n\n    logger.close()\n\n    print(\'Best acc:\')\n    print(best_acc)\n\ndef train(train_loader, model, criterion, optimizer, epoch, use_cuda):\n    # switch to train mode\n    model.train()\n    torch.set_grad_enabled(True)\n\n    batch_time = AverageMeter()\n    data_time = AverageMeter()\n    losses = AverageMeter()\n    top1 = AverageMeter()\n    top5 = AverageMeter()\n    end = time.time()\n\n    bar = Bar(\'Processing\', max=len(train_loader))\n    show_step = len(train_loader) // 10\n    for batch_idx, (inputs, targets) in enumerate(train_loader):\n        batch_size = inputs.size(0)\n        if batch_size < args.train_batch:\n            continue\n        # measure data loading time\n        data_time.update(time.time() - end)\n\n        if use_cuda:\n            inputs, targets = inputs.cuda(), targets.cuda(async=True)\n        inputs, targets = torch.autograd.Variable(inputs), torch.autograd.Variable(targets)\n\n        # compute output\n        outputs = model(inputs)\n        loss = criterion(outputs, targets)\n\n        # measure accuracy and record loss\n        prec1, prec5 = accuracy(outputs.data, targets.data, topk=(1, 5))\n        losses.update(loss.data, inputs.size(0))\n        top1.update(prec1, inputs.size(0))\n        top5.update(prec5, inputs.size(0))\n\n        # compute gradient and do SGD step\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        # measure elapsed time\n        batch_time.update(time.time() - end)\n        end = time.time()\n\n        # plot progress\n        bar.suffix  = \'({batch}/{size}) Data: {data:.3f}s | Batch: {bt:.3f}s | Total: {total:} | ETA: {eta:} | Loss: {loss:.4f} | top1: {top1: .4f} | top5: {top5: .4f}\'.format(\n                    batch=batch_idx + 1,\n                    size=len(train_loader),\n                    data=data_time.val,\n                    bt=batch_time.val,\n                    total=bar.elapsed_td,\n                    eta=bar.eta_td,\n                    loss=losses.avg,\n                    top1=top1.avg,\n                    top5=top5.avg,\n                    )\n        if (batch_idx) % show_step == 0:\n            print(bar.suffix)\n        bar.next()\n    bar.finish()\n    return (losses.avg, top1.avg)\n\ndef test(val_loader, model, criterion, epoch, use_cuda):\n    global best_acc\n\n    batch_time = AverageMeter()\n    data_time = AverageMeter()\n    losses = AverageMeter()\n    top1 = AverageMeter()\n    top5 = AverageMeter()\n\n    # switch to evaluate mode\n    model.eval()\n    torch.set_grad_enabled(False)\n\n    end = time.time()\n    bar = Bar(\'Processing\', max=len(val_loader))\n    for batch_idx, (inputs, targets) in enumerate(val_loader):\n        # measure data loading time\n        data_time.update(time.time() - end)\n\n        if use_cuda:\n            inputs, targets = inputs.cuda(), targets.cuda()\n        inputs, targets = torch.autograd.Variable(inputs, volatile=True), torch.autograd.Variable(targets)\n\n        # compute output\n        outputs = model(inputs)\n        loss = criterion(outputs, targets)\n\n        # measure accuracy and record loss\n        prec1, prec5 = accuracy(outputs.data, targets.data, topk=(1, 5))\n        # losses.update(loss.data[0], inputs.size(0))\n        losses.update(loss.data, inputs.size(0))\n        #top1.update(prec1[0], inputs.size(0))\n        top1.update(prec1, inputs.size(0))\n        #top5.update(prec5[0], inputs.size(0))\n        top5.update(prec5, inputs.size(0))\n\n        # measure elapsed time\n        batch_time.update(time.time() - end)\n        end = time.time()\n\n        # plot progress\n        bar.suffix  = \'({batch}/{size}) Data: {data:.3f}s | Batch: {bt:.3f}s | Total: {total:} | ETA: {eta:} | Loss: {loss:.4f} | top1: {top1: .4f} | top5: {top5: .4f}\'.format(\n                    batch=batch_idx + 1,\n                    size=len(val_loader),\n                    data=data_time.avg,\n                    bt=batch_time.avg,\n                    total=bar.elapsed_td,\n                    eta=bar.eta_td,\n                    loss=losses.avg,\n                    top1=top1.avg,\n                    top5=top5.avg,\n                    )\n        bar.next()\n    print(bar.suffix)\n    bar.finish()\n    return (losses.avg, top1.avg)\n\ndef save_checkpoint(state, is_best, checkpoint=\'checkpoint\', filename=\'checkpoint.pth.tar\'):\n    filepath = os.path.join(checkpoint, filename)\n    torch.save(state, filepath)\n    if is_best:\n        shutil.copyfile(filepath, os.path.join(checkpoint, \'model_best.pth.tar\'))\n\ndef adjust_learning_rate(optimizer, epoch):\n    global state\n    if epoch in args.schedule:\n        state[\'lr\'] *= args.gamma\n        for param_group in optimizer.param_groups:\n            param_group[\'lr\'] = state[\'lr\']\n\nif __name__ == \'__main__\':\n    main()\n'"
classification/imagenet_fast.py,45,"b'from __future__ import print_function\nimport sys\n\nimport argparse\nimport os\nimport shutil\nimport time\nimport random\nimport numpy as np\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.parallel\nimport torch.nn.functional as F\nimport torch.backends.cudnn as cudnn\nimport torch.distributed as dist\nimport torch.optim as optim\nimport torch.utils.data as data\nimport torch.utils.data.distributed\nimport torchvision.transforms as transforms\nimport torchvision.datasets as datasets\nimport torchvision.models as models\nimport models.imagenet as customized_models\nfrom flops_counter import get_model_complexity_info\nfrom PIL import ImageFile\nImageFile.LOAD_TRUNCATED_IMAGES = True\n\nfrom utils import Bar, Logger, AverageMeter, accuracy, mkdir_p\n\nimport warnings\nwarnings.filterwarnings(\'ignore\')\n\ntry:\n    from apex.parallel import DistributedDataParallel as DDP\n    from apex.fp16_utils import *\n    from apex import amp, optimizers\nexcept ImportError:\n    raise ImportError(""Please install apex from https://www.github.com/nvidia/apex to run this example."")\n\n# for servers to immediately record the logs\ndef flush_print(func):\n    def new_print(*args, **kwargs):\n        func(*args, **kwargs)\n        sys.stdout.flush()\n    return new_print\nprint = flush_print(print)\n\n\nfrom torch.optim.optimizer import Optimizer, required\n\nclass LSGD(Optimizer):\n    def __init__(self, params, lr=required, momentum=0, dampening=0,\n                 weight_decay=0, nesterov=False):\n        if lr is not required and lr < 0.0:\n            raise ValueError(""Invalid learning rate: {}"".format(lr))\n        if momentum < 0.0:\n            raise ValueError(""Invalid momentum value: {}"".format(momentum))\n        if weight_decay < 0.0:\n            raise ValueError(""Invalid weight_decay value: {}"".format(weight_decay))\n\n        defaults = dict(lr=lr, momentum=momentum, dampening=dampening,\n                        weight_decay=weight_decay, nesterov=nesterov)\n        if nesterov and (momentum <= 0 or dampening != 0):\n            raise ValueError(""Nesterov momentum requires a momentum and zero dampening"")\n        super(LSGD, self).__init__(params, defaults)\n\n    def __setstate__(self, state):\n        super(LSGD, self).__setstate__(state)\n        for group in self.param_groups:\n            group.setdefault(\'nesterov\', False)\n\n    def step(self, closure=None, print_flag=False):\n        """"""Performs a single optimization step.\n\n        Arguments:\n            closure (callable, optional): A closure that reevaluates the model\n                and returns the loss.\n        """"""\n        loss = None\n        if closure is not None:\n            loss = closure()\n\n        for group in self.param_groups:\n            weight_decay = group[\'weight_decay\']\n            momentum = group[\'momentum\']\n            dampening = group[\'dampening\']\n            nesterov = group[\'nesterov\']\n\n\n            for p in group[\'params\']:\n                if p.grad is None:\n                    continue\n                d_p = p.grad.data\n\n\n                if weight_decay != 0:\n                    d_p.add_(weight_decay, p.data)\n\n                sz = p.data.size()\n                if d_p.dim() == 4 and sz[1] != 1: # we do not consider dw conv\n                    assert(weight_decay == 0)\n                    sz = p.data.size()\n                    w  = p.data.view(sz[0], -1)\n                    wstd = w.std(dim=1).view(sz[0], 1, 1, 1)\n                    wmean = w.mean(dim=1).view(sz[0], 1, 1, 1)\n\n                    if args.local_rank == 0 and print_flag:\n                        wm = wstd.view(-1).mean().item()\n                        wmm = wmean.view(-1).mean().item()\n                        print(\'lam = %.6f\' % args.lam, \'mineps = %.6f\' % args.mineps, \n                                \'1 - eps/std = %.10f\' % (1 - args.mineps / wm), \n                                \'std = %.10f\' % wm, \'mean = %.10f\' % wmm,  \'sz = \', sz)\n                    \n                    d_p.add_(args.lam, (1 - args.mineps / wstd) * (p.data - wmean) + wmean)\n\n\n                if momentum != 0:\n                    param_state = self.state[p]\n                    if \'momentum_buffer\' not in param_state:\n                        buf = param_state[\'momentum_buffer\'] = torch.zeros_like(p.data)\n                        buf.mul_(momentum).add_(d_p)\n                    else:\n                        buf = param_state[\'momentum_buffer\']\n                        buf.mul_(momentum).add_(1 - dampening, d_p)\n                    if nesterov:\n                        d_p = d_p.add(momentum, buf)\n                    else:\n                        d_p = buf\n\n                p.data.add_(-group[\'lr\'], d_p)\n\n        return loss\n\n\n# Models\ndefault_model_names = sorted(name for name in models.__dict__\n    if name.islower() and not name.startswith(""__"")\n    and callable(models.__dict__[name]))\n\ncustomized_models_names = sorted(name for name in customized_models.__dict__\n    if name.islower() and not name.startswith(""__"")\n    and callable(customized_models.__dict__[name]))\n\nfor name in customized_models.__dict__:\n    if name.islower() and not name.startswith(""__"") and callable(customized_models.__dict__[name]):\n        models.__dict__[name] = customized_models.__dict__[name]\n\nmodel_names = default_model_names + customized_models_names\n\n# Parse arguments\nparser = argparse.ArgumentParser(description=\'PyTorch ImageNet Training\')\n\nparser.add_argument(\'--cutmix\', dest=\'cutmix\', action=\'store_true\')\nparser.add_argument(\'--cutmix_prob\', default=1., type=float)\n\nparser.add_argument(\'--cutout\', dest=\'cutout\', action=\'store_true\')\nparser.add_argument(\'--cutout_size\', default=112, type=float)\n\nparser.add_argument(\'--el2\', dest=\'el2\', action=\'store_true\', help=\'whether to use e-shifted L2 regularizer\')\nparser.add_argument(\'--mineps\', dest=\'mineps\', default=1e-3, type=float, help=\'min of weights std, typically 1e-3, 1e-8, 1e-2\')\nparser.add_argument(\'--lam\', dest=\'lam\', default=1e-4, type=float, help=\'lam of weights for e-shifted L2 regularizer\')\n\n\nparser.add_argument(\'--nowd-bn\', dest=\'nowd_bn\', action=\'store_true\',\n                    help=\'no weight decay on bn weights\')\nparser.add_argument(\'--nowd-fc\', dest=\'nowd_fc\', action=\'store_true\',\n                    help=\'no weight decay on fc weights\')\nparser.add_argument(\'--nowd-conv\', dest=\'nowd_conv\', action=\'store_true\',\n                    help=\'no weight decay on conv weights\')\n\n# Datasets\nparser.add_argument(\'-d\', \'--data\', default=\'path to dataset\', type=str)\nparser.add_argument(\'-j\', \'--workers\', default=32, type=int, metavar=\'N\',\n                    help=\'number of data loading workers (default: 4)\')\n# Optimization options\nparser.add_argument(\'--opt-level\', default=\'O2\', type=str, \n        help=\'O2 is mixed FP16/32 training, see more in https://github.com/NVIDIA/apex/tree/f5cd5ae937f168c763985f627bbf850648ea5f3f/examples/imagenet\')\nparser.add_argument(\'--keep-batchnorm-fp32\', default=True, action=\'store_true\',\n                    help=\'keeping cudnn bn leads to fast training\')\nparser.add_argument(\'--loss-scale\', type=float, default=None)\n\nparser.add_argument(\'--label-smoothing\', \'--ls\', default=0.1, type=float)\n\nparser.add_argument(\'--mixup\', dest=\'mixup\', action=\'store_true\',\n                    help=\'whether to use mixup\')\nparser.add_argument(\'--alpha\', default=0.2, type=float,\n                    metavar=\'mixup alpha\', help=\'alpha value for mixup B(alpha, alpha) distribution\')\nparser.add_argument(\'--cos\', dest=\'cos\', action=\'store_true\', \n                    help=\'using cosine decay lr schedule\')\nparser.add_argument(\'--warmup\', \'--wp\', default=5, type=int,\n                    help=\'number of epochs to warmup\')\nparser.add_argument(\'--epochs\', default=100, type=int, metavar=\'N\',\n                    help=\'number of total epochs to run\')\nparser.add_argument(\'--start-epoch\', default=0, type=int, metavar=\'N\',\n                    help=\'manual epoch number (useful on restarts)\')\nparser.add_argument(\'--train-batch\', default=256, type=int, metavar=\'N\',\n                    help=\'train batchsize (default: 256)\')\nparser.add_argument(\'--test-batch\', default=125, type=int, metavar=\'N\',\n                    help=\'test batchsize (default: 200)\')\nparser.add_argument(\'--lr\', \'--learning-rate\', default=0.1, type=float,\n                    metavar=\'LR\', help=\'initial learning rate\')\nparser.add_argument(\'--drop\', \'--dropout\', default=0, type=float,\n                    metavar=\'Dropout\', help=\'Dropout ratio\')\nparser.add_argument(\'--schedule\', type=int, nargs=\'+\', default=[30, 60, 90],\n                        help=\'Decrease learning rate at these epochs.\')\nparser.add_argument(\'--gamma\', type=float, default=0.1, help=\'LR is multiplied by gamma on schedule.\')\nparser.add_argument(\'--momentum\', default=0.9, type=float, metavar=\'M\',\n                    help=\'momentum\')\nparser.add_argument(\'--weight-decay\', \'--wd\', default=1e-4, type=float,\n                    metavar=\'W\', help=\'weight decay (default: 1e-4)\')\nparser.add_argument(\'--wd-all\', dest = \'wdall\', action=\'store_true\', \n                    help=\'weight decay on all parameters\')\n\n# Checkpoints\nparser.add_argument(\'--print-freq\', \'-p\', default=10, type=int,\n                    metavar=\'N\', help=\'print frequency (default: 10)\')\nparser.add_argument(\'-c\', \'--checkpoint\', default=\'checkpoint\', type=str, metavar=\'PATH\',\n                    help=\'path to save checkpoint (default: checkpoint)\')\nparser.add_argument(\'--resume\', default=\'\', type=str, metavar=\'PATH\',\n                    help=\'path to latest checkpoint (default: none)\')\n\n# Architecture\nparser.add_argument(\'--arch\', \'-a\', metavar=\'ARCH\', default=\'resnet18\',\n                    choices=model_names,\n                    help=\'model architecture: \' +\n                        \' | \'.join(model_names) +\n                        \' (default: resnet18)\')\nparser.add_argument(\'--depth\', type=int, default=29, help=\'Model depth.\')\nparser.add_argument(\'--cardinality\', type=int, default=32, help=\'ResNet cardinality (group).\')\nparser.add_argument(\'--base-width\', type=int, default=4, help=\'ResNet base width.\')\nparser.add_argument(\'--widen-factor\', type=int, default=4, help=\'Widen factor. 4 -> 64, 8 -> 128, ...\')\n# Miscs\nparser.add_argument(\'--manualSeed\', type=int, help=\'manual seed\')\nparser.add_argument(\'-e\', \'--evaluate\', dest=\'evaluate\', action=\'store_true\',\n                    help=\'evaluate model on validation set\')\nparser.add_argument(\'--pretrained\', dest=\'pretrained\', action=\'store_true\',\n                    help=\'use pre-trained model\')\n#Device options\n#parser.add_argument(\'--gpu-id\', default=\'0\', type=str, help=\'id(s) for CUDA_VISIBLE_DEVICES\')\nparser.add_argument(\'--local_rank\', default=0, type=int)\n\nargs = parser.parse_args()\nstate = {k: v for k, v in args._get_kwargs()}\n\nprint(""opt_level = {}"".format(args.opt_level))\nprint(""keep_batchnorm_fp32 = {}"".format(args.keep_batchnorm_fp32), type(args.keep_batchnorm_fp32))\nprint(""loss_scale = {}"".format(args.loss_scale), type(args.loss_scale))\n\n# Use CUDA\n# os.environ[\'CUDA_VISIBLE_DEVICES\'] = args.gpu_id\nuse_cuda = torch.cuda.is_available()\n\n# Random seed\nif args.manualSeed is None:\n    args.manualSeed = random.randint(1, 10000)\nrandom.seed(args.manualSeed)\ntorch.manual_seed(args.manualSeed)\nif use_cuda:\n    torch.cuda.manual_seed_all(args.manualSeed)\n\nbest_acc = 0  # best test accuracy\n\ndef fast_collate(batch):\n    imgs = [img[0] for img in batch]\n    targets = torch.tensor([target[1] for target in batch], dtype=torch.int64)\n    w = imgs[0].size[0]\n    h = imgs[0].size[1]\n    tensor = torch.zeros((len(imgs), 3, h, w), dtype=torch.uint8)\n    for i, img in enumerate(imgs):\n        nump_array = np.asarray(img, dtype=np.uint8)\n        # tens = torch.from_numpy(nump_array)\n        if nump_array.ndim < 3:\n            nump_array = np.expand_dims(nump_array, axis=-1)\n        nump_array = np.rollaxis(nump_array, 2)\n\n        tensor[i] += torch.from_numpy(nump_array)\n\n    return tensor, targets\n\nclass data_prefetcher():\n    def __init__(self, loader):\n        self.loader = iter(loader)\n        self.stream = torch.cuda.Stream()\n        self.mean   = torch.tensor([0.485 * 255, 0.456 * 255, 0.406 * 255]) \\\n                .cuda().view(1, 3, 1, 1)\n        self.std    = torch.tensor([0.229 * 255, 0.224 * 255, 0.225 * 255]) \\\n                .cuda().view(1, 3, 1, 1)\n        self.preload()\n\n    def preload(self):\n        try:\n            self.next_input, self.next_target = next(self.loader)\n        except StopIteration:\n            self.next_input = None\n            self.next_target = None\n            return\n        \n        with torch.cuda.stream(self.stream):\n            self.next_input = self.next_input.cuda(non_blocking=True)\n            self.next_target = self.next_target.cuda(non_blocking=True)\n            self.next_input = self.next_input.float()\n            self.next_input = self.next_input.sub_(self.mean).div_(self.std)\n\n    def next(self):\n        torch.cuda.current_stream().wait_stream(self.stream)\n        input = self.next_input\n        target = self.next_target\n        if input is not None:\n            self.preload()\n        return input, target\n\n\ndef main():\n    global best_acc\n    start_epoch = args.start_epoch  # start from epoch 0 or last checkpoint epoch\n\n    if not os.path.isdir(args.checkpoint) and args.local_rank == 0:\n        mkdir_p(args.checkpoint)\n\n    args.distributed = True\n    args.gpu = args.local_rank\n    torch.cuda.set_device(args.gpu)\n    torch.distributed.init_process_group(backend=\'nccl\', init_method=\'env://\')\n    args.world_size = torch.distributed.get_world_size()\n    print(\'world_size = \', args.world_size)\n\n    assert torch.backends.cudnn.enabled, ""Amp requires cudnn backend to be enabled.""\n\n    # create model\n    if args.pretrained:\n        print(""=> using pre-trained model \'{}\'"".format(args.arch))\n        model = models.__dict__[args.arch](pretrained=True)\n    elif \'resnext\' in args.arch:\n        model = models.__dict__[args.arch](\n                    baseWidth=args.base_width,\n                    cardinality=args.cardinality,\n                )\n    else:\n        print(""=> creating model \'{}\'"".format(args.arch))\n        model = models.__dict__[args.arch]()\n\n    flops, params = get_model_complexity_info(model, (224, 224), as_strings=False, print_per_layer_stat=False)\n    print(\'Flops:  %.3f\' % (flops / 1e9))\n    print(\'Params: %.2fM\' % (params / 1e6))\n\n    cudnn.benchmark = True\n    # define loss function (criterion) and optimizer\n    # criterion = nn.CrossEntropyLoss().cuda()\n    criterion = SoftCrossEntropyLoss(label_smoothing=args.label_smoothing).cuda()\n    model = model.cuda()\n\n    args.lr = float(0.1 * float(args.train_batch*args.world_size)/256.)\n    state[\'lr\'] = args.lr\n    optimizer = set_optimizer(model)\n    #optim.SGD(model.parameters(), lr=args.lr, momentum=args.momentum, weight_decay=args.weight_decay)\n\n    model, optimizer = amp.initialize(model, optimizer,\n                                      opt_level=args.opt_level,\n                                      keep_batchnorm_fp32=args.keep_batchnorm_fp32,\n                                      loss_scale=args.loss_scale)\n\n    #model = torch.nn.DataParallel(model).cuda()\n    #model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[args.local_rank], output_device=args.local_rank)\n    model = DDP(model, delay_allreduce=True)\n\n    # Data loading code\n    traindir = os.path.join(args.data, \'train\')\n    valdir = os.path.join(args.data, \'valf\')\n    #normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n\n    data_aug_scale = (0.08, 1.0) \n\n    train_dataset = datasets.ImageFolder(traindir, transforms.Compose([\n            transforms.RandomResizedCrop(224, scale = data_aug_scale),\n            transforms.RandomHorizontalFlip(),\n            # transforms.ToTensor(),\n            # normalize,\n        ]))\n    val_dataset   = datasets.ImageFolder(valdir, transforms.Compose([\n            transforms.Resize(256),\n            transforms.CenterCrop(224),\n            # transforms.ToTensor(),\n            # normalize,\n        ]))\n\n    train_sampler = torch.utils.data.distributed.DistributedSampler(train_dataset)\n    val_sampler = torch.utils.data.distributed.DistributedSampler(val_dataset)\n\n    train_loader = torch.utils.data.DataLoader(\n        train_dataset, batch_size=args.train_batch, shuffle=False,\n        num_workers=args.workers, pin_memory=True, sampler=train_sampler, collate_fn=fast_collate)\n    val_loader = torch.utils.data.DataLoader(\n        val_dataset, batch_size=args.test_batch, shuffle=False,\n        num_workers=args.workers, pin_memory=True, sampler=val_sampler, collate_fn=fast_collate)\n\n\n    # Resume\n    title = \'ImageNet-\' + args.arch\n    if args.resume:\n        # Load checkpoint.\n        print(\'==> Resuming from checkpoint..\', args.resume)\n        assert os.path.isfile(args.resume), \'Error: no checkpoint directory found!\'\n        args.checkpoint = os.path.dirname(args.resume)\n        #checkpoint = torch.load(args.resume, map_location=torch.device(\'cpu\'))\n        checkpoint = torch.load(args.resume, map_location = lambda storage, loc: storage.cuda(args.gpu))\n        best_acc = checkpoint[\'best_acc\']\n        start_epoch = checkpoint[\'epoch\']\n        # model may have more keys\n        t = model.state_dict()\n        c = checkpoint[\'state_dict\']\n        for k in t:\n            if k not in c:\n                print(\'not in loading dict! fill it\', k, t[k])\n                c[k] = t[k]\n        model.load_state_dict(c)\n        print(\'optimizer load old state\')\n        optimizer.load_state_dict(checkpoint[\'optimizer\'])\n        if args.local_rank == 0:\n            logger = Logger(os.path.join(args.checkpoint, \'log.txt\'), title=title, resume=True)\n    else:\n        if args.local_rank == 0:\n            logger = Logger(os.path.join(args.checkpoint, \'log.txt\'), title=title)\n            logger.set_names([\'Learning Rate\', \'Train Loss\', \'Valid Loss\', \'Train Acc.\', \'Valid Acc.\'])\n\n\n    if args.evaluate:\n        print(\'\\nEvaluation only\')\n        test_loss, test_acc = test(val_loader, model, criterion, start_epoch, use_cuda)\n        print(\' Test Loss:  %.8f, Test Acc:  %.2f\' % (test_loss, test_acc))\n        return\n\n    # Train and val\n    for epoch in range(start_epoch, args.epochs):\n        train_sampler.set_epoch(epoch)\n\n        adjust_learning_rate(optimizer, epoch)\n\n        if args.local_rank == 0:\n            print(\'\\nEpoch: [%d | %d] LR: %f\' % (epoch + 1, args.epochs, state[\'lr\']))\n\n        train_loss, train_acc = train(train_loader, model, criterion, optimizer, epoch, use_cuda)\n        test_loss, test_acc = test(val_loader, model, criterion, epoch, use_cuda)\n\n        # save model\n        if args.local_rank == 0:\n            # append logger file\n            logger.append([state[\'lr\'], train_loss, test_loss, train_acc, test_acc])\n\n            is_best = test_acc > best_acc\n            best_acc = max(test_acc, best_acc)\n            save_checkpoint({\n                    \'epoch\': epoch + 1,\n                    \'state_dict\': model.state_dict(),\n                    \'acc\': test_acc,\n                    \'best_acc\': best_acc,\n                    \'optimizer\' : optimizer.state_dict(),\n                }, is_best, checkpoint=args.checkpoint)\n\n    if args.local_rank == 0:\n        logger.close()\n\n    print(\'Best acc:\')\n    print(best_acc)\n\ndef train(train_loader, model, criterion, optimizer, epoch, use_cuda):\n    printflag = False\n    # switch to train mode\n    model.train()\n    torch.set_grad_enabled(True)\n\n    batch_time = AverageMeter()\n    losses = AverageMeter()\n    top1 = AverageMeter()\n    top5 = AverageMeter()\n    end = time.time()\n\n    if args.local_rank == 0:\n        bar = Bar(\'Processing\', max=len(train_loader))\n    show_step = len(train_loader) // 10\n    \n    prefetcher = data_prefetcher(train_loader)\n    inputs, targets = prefetcher.next()\n\n    batch_idx = -1\n    while inputs is not None:\n    # for batch_idx, (inputs, targets) in enumerate(train_loader):\n        batch_idx += 1\n        batch_size = inputs.size(0)\n        if batch_size < args.train_batch:\n            break\n        # measure data loading time\n\n        #if use_cuda:\n        #    inputs, targets = inputs.cuda(), targets.cuda(async=True)\n        #inputs, targets = torch.autograd.Variable(inputs), torch.autograd.Variable(targets)\n        if (batch_idx) % show_step == 0 and args.local_rank == 0:\n            print_flag = True\n        else:\n            print_flag = False\n\n        if args.cutmix:\n            if printflag==False:\n                print(\'using cutmix !\')\n                printflag=True\n            inputs, targets_a, targets_b, lam = cutmix_data(inputs, targets, args.cutmix_prob, use_cuda)\n            outputs = model(inputs)\n            loss_func = mixup_criterion(targets_a, targets_b, lam)\n            old_loss = loss_func(criterion, outputs)\n        elif args.mixup:\n            if printflag==False:\n                print(\'using mixup !\')\n                printflag=True\n            inputs, targets_a, targets_b, lam = mixup_data(inputs, targets, args.alpha, use_cuda)\n            outputs = model(inputs)\n            loss_func = mixup_criterion(targets_a, targets_b, lam)\n            old_loss = loss_func(criterion, outputs)\n        elif args.cutout:\n            if printflag==False:\n                print(\'using cutout !\')\n                printflag=True\n            inputs = cutout_data(inputs, args.cutout_size, use_cuda)\n            outputs = model(inputs)\n            old_loss = criterion(outputs, targets)\n        else:\n            outputs = model(inputs)\n            old_loss = criterion(outputs, targets)\n\n        # compute gradient and do SGD step\n        optimizer.zero_grad()\n        # loss.backward()\n        with amp.scale_loss(old_loss, optimizer) as loss:\n            loss.backward()\n\n        if args.el2:\n            optimizer.step(print_flag=print_flag)\n        else:\n            optimizer.step()\n\n\n        if batch_idx % args.print_freq == 0:\n            # measure accuracy and record loss\n            prec1, prec5 = accuracy(outputs.data, targets.data, topk=(1, 5))\n            reduced_loss = reduce_tensor(loss.data)\n            prec1        = reduce_tensor(prec1)\n            prec5        = reduce_tensor(prec5)\n\n            # to_python_float incurs a host<->device sync\n            losses.update(to_python_float(reduced_loss), inputs.size(0))\n            top1.update(to_python_float(prec1), inputs.size(0))\n            top5.update(to_python_float(prec5), inputs.size(0))\n\n            torch.cuda.synchronize()\n            # measure elapsed time\n            batch_time.update((time.time() - end) / args.print_freq)\n            end = time.time()\n\n            if args.local_rank == 0: # plot progress\n                bar.suffix  = \'({batch}/{size}) | Batch: {bt:.3f}s | Total: {total:} | Loss: {loss:.4f} | top1: {top1: .4f} | top5: {top5: .4f}\'.format(\n                            batch=batch_idx + 1,\n                            size=len(train_loader),\n                            bt=batch_time.val,\n                            total=bar.elapsed_td,\n                            loss=losses.avg,\n                            top1=top1.avg,\n                            top5=top5.avg,\n                            )\n                bar.next()\n        if (batch_idx) % show_step == 0 and args.local_rank == 0:\n            print(\'E%d\' % (epoch) + bar.suffix)\n\n        inputs, targets = prefetcher.next()\n\n    if args.local_rank == 0:\n        bar.finish()\n    return (losses.avg, top1.avg)\n\ndef test(val_loader, model, criterion, epoch, use_cuda):\n    global best_acc\n\n    batch_time = AverageMeter()\n    losses = AverageMeter()\n    top1 = AverageMeter()\n    top5 = AverageMeter()\n\n    # switch to evaluate mode\n    model.eval()\n    # torch.set_grad_enabled(False)\n\n    end = time.time()\n    if args.local_rank == 0:\n        bar = Bar(\'Processing\', max=len(val_loader))\n\n    prefetcher = data_prefetcher(val_loader)\n    inputs, targets = prefetcher.next()\n\n    batch_idx = -1\n    while inputs is not None:\n    # for batch_idx, (inputs, targets) in enumerate(val_loader):\n        batch_idx += 1\n\n        #if use_cuda:\n        #    inputs, targets = inputs.cuda(), targets.cuda()\n        #inputs, targets = torch.autograd.Variable(inputs, volatile=True), torch.autograd.Variable(targets)\n\n        # compute output\n        with torch.no_grad():\n            outputs = model(inputs)\n            loss = criterion(outputs, targets)\n\n        # measure accuracy and record loss\n        prec1, prec5 = accuracy(outputs.data, targets.data, topk=(1, 5))\n\n        reduced_loss = reduce_tensor(loss.data)\n        prec1        = reduce_tensor(prec1)\n        prec5        = reduce_tensor(prec5)\n\n        # to_python_float incurs a host<->device sync\n        losses.update(to_python_float(reduced_loss), inputs.size(0))\n        top1.update(to_python_float(prec1), inputs.size(0))\n        top5.update(to_python_float(prec5), inputs.size(0))\n\n        # measure elapsed time\n        batch_time.update(time.time() - end)\n        end = time.time()\n\n        # plot progress\n        if args.local_rank == 0:\n            bar.suffix  = \'Valid({batch}/{size}) | Batch: {bt:.3f}s | Total: {total:} | Loss: {loss:.4f} | top1: {top1: .4f} | top5: {top5: .4f}\'.format(\n                        batch=batch_idx + 1,\n                        size=len(val_loader),\n                        bt=batch_time.avg,\n                        total=bar.elapsed_td,\n                        loss=losses.avg,\n                        top1=top1.avg,\n                        top5=top5.avg,\n                        )\n            bar.next()\n\n        inputs, targets = prefetcher.next()\n\n    if args.local_rank == 0:\n        print(bar.suffix)\n        bar.finish()\n    return (losses.avg, top1.avg)\n\ndef save_checkpoint(state, is_best, checkpoint=\'checkpoint\', filename=\'checkpoint.pth.tar\'):\n    filepath = os.path.join(checkpoint, filename)\n    torch.save(state, filepath)\n    if is_best:\n        shutil.copyfile(filepath, os.path.join(checkpoint, \'model_best.pth.tar\'))\n\n\ndef set_optimizer(model):\n    \n    optim_use = optim.SGD\n    if args.el2:\n        optim_use = LSGD\n        if args.local_rank == 0:\n            print(\'use e-shifted L2 regularizer based SGD optimizer!\')\n    else:\n        if args.local_rank == 0:\n            print(\'use SGD optimizer!\')\n\n\n    if args.wdall:\n        optimizer = optim_use(model.parameters(), lr=args.lr, momentum=args.momentum, weight_decay=args.weight_decay)\n        print(\'weight decay on all parameters\')\n    else:\n        decay_list = []\n        no_decay_list = []\n        dns = []\n        ndns = []\n\n        for name, p in model.named_parameters():\n            no_decay_flag = False\n            dim = p.dim()\n\n            if \'bias\' in name:\n                no_decay_flag = True\n            elif dim == 1:\n                if args.nowd_bn: # bn weights\n                    no_decay_flag = True\n            elif dim == 2:\n                if args.nowd_fc:  # fc weights\n                    no_decay_flag = True\n            elif dim == 4:\n                if args.nowd_conv: # conv weights\n                    no_decay_flag = True\n            else:\n                print(\'no valid dim!!!, dim = \', dim)\n                exit(-1)\n\n            if no_decay_flag:\n                no_decay_list.append(p)\n                ndns.append(name)\n            else:\n                decay_list.append(p)\n                dns.append(name)\n\n        if args.local_rank == 0:\n            print(\'------------\' * 6)\n            print(\'no decay list = \', ndns)\n            print(\'------------\' * 6)\n            print(\'decay list = \', dns)\n            print(\'------summary------\')\n            if args.nowd_bn:\n                print(\'no decay on bn weights!\')\n            else:\n                print(\'decay on bn weights!\')\n            if args.nowd_conv:\n                print(\'no decay on conv weights!\')\n            else:\n                print(\'decay on conv weights!\')\n            if args.nowd_fc:\n                print(\'no decay on fc weights!\')\n            else:\n                print(\'decay on fc weights!\')\n            print(\'------------\' * 6)\n\n        params = [{\'params\': no_decay_list, \'weight_decay\': 0},\n                  {\'params\': decay_list}]\n        optimizer = optim_use(params, lr=args.lr, momentum=args.momentum, weight_decay=args.weight_decay)\n        if args.local_rank == 0:\n            print(\'optimizer = \', optimizer)\n\n    return optimizer\n\ndef adjust_learning_rate(optimizer, epoch):\n    global state\n\n    def adjust_optimizer():\n        for param_group in optimizer.param_groups:\n            param_group[\'lr\'] = state[\'lr\']\n\n    if epoch < args.warmup:\n        state[\'lr\'] = args.lr * (epoch + 1) / args.warmup\n        adjust_optimizer()\n\n    elif args.cos: # cosine decay lr schedule (Note: epoch-wise, not batch-wise)\n        state[\'lr\'] = args.lr * 0.5 * (1 + np.cos(np.pi * epoch / args.epochs))\n        adjust_optimizer()\n\n    elif epoch in args.schedule: # step lr schedule\n        state[\'lr\'] *= args.gamma\n        adjust_optimizer()\n\nclass SoftCrossEntropyLoss(nn.NLLLoss):\n    def __init__(self, label_smoothing=0, num_classes=1000, **kwargs):\n        assert label_smoothing >= 0 and label_smoothing <= 1\n        super(SoftCrossEntropyLoss, self).__init__(**kwargs)\n        self.confidence = 1 - label_smoothing\n        self.other      = label_smoothing * 1.0 / (num_classes - 1)\n        self.criterion  = nn.KLDivLoss(reduction=\'batchmean\')\n        print(\'using soft celoss!!!, label_smoothing = \', label_smoothing)\n\n    def forward(self, input, target):\n        one_hot = torch.zeros_like(input)\n        one_hot.fill_(self.other)\n        one_hot.scatter_(1, target.unsqueeze(1).long(), self.confidence)\n        input   = F.log_softmax(input, 1)\n        return self.criterion(input, one_hot)\n\ndef mixup_data(x, y, alpha=1.0, use_cuda=True):\n    if alpha > 0.:\n        lam = np.random.beta(alpha, alpha)\n    else:\n        lam = 1.\n\n    batch_size = x.size(0)\n    if use_cuda:\n        index = torch.randperm(batch_size).cuda()\n    else:\n        index = torch.randperm(batch_size)\n\n    mixed_x = lam * x + (1 - lam) * x[index, ...]\n    y_a, y_b = y, y[index]\n    return mixed_x, y_a, y_b, lam\n\ndef rand_bbox(size, lam):\n    W = size[2]\n    H = size[3]\n    cut_rat = np.sqrt(1. - lam)\n    cut_w = np.int(W * cut_rat)\n    cut_h = np.int(H * cut_rat)\n\n    # uniform\n    cx = np.random.randint(W)\n    cy = np.random.randint(H)\n\n    bbx1 = np.clip(cx - cut_w // 2, 0, W)\n    bby1 = np.clip(cy - cut_h // 2, 0, H)\n    bbx2 = np.clip(cx + cut_w // 2, 0, W)\n    bby2 = np.clip(cy + cut_h // 2, 0, H)\n\n    return bbx1, bby1, bbx2, bby2\n\ndef cutmix_data(x, y, cutmix_prob=1.0, use_cuda=True):\n    lam = np.random.beta(1, 1)\n\n    batch_size = x.size(0)\n    index = torch.randperm(batch_size).cuda()\n    y_a, y_b = y, y[index]\n\n    bbx1, bby1, bbx2, bby2 = rand_bbox(x.size(), lam)\n    x[:, :, bbx1:bbx2, bby1:bby2] = x[index, :, bbx1:bbx2, bby1:bby2]\n    lam = 1 - ((bbx2 - bbx1) * (bby2 - bby1) / (x.size()[-1] * x.size()[-2]))\n    return x, y_a, y_b, lam\n\ndef cutout_data(x, cutout_size=112, use_cuda=True):\n    W = x.size(2)\n    H = x.size(3)\n    cx = np.random.randint(W)\n    cy = np.random.randint(H)\n\n    bbx1 = np.clip(cx - cutout_size // 2, 0, W)\n    bby1 = np.clip(cy - cutout_size // 2, 0, H)\n    bbx2 = np.clip(cx + cutout_size // 2, 0, W)\n    bby2 = np.clip(cy + cutout_size // 2, 0, H)\n\n    x[:, :, bbx1:bbx2, bby1:bby2] = 0\n\n    return x\n\ndef mixup_criterion(y_a, y_b, lam):\n    return lambda criterion, pred: lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)\n\ndef reduce_tensor(tensor):\n    rt = tensor.clone()\n    dist.all_reduce(rt, op=dist.ReduceOp.SUM)\n    rt /= args.world_size\n    return rt\n\nif __name__ == \'__main__\':\n    main()\n'"
classification/imagenet_mobile.py,43,"b'from __future__ import print_function\nimport sys\n\nimport argparse\nimport os\nimport shutil\nimport time\nimport random\nimport numpy as np\nimport math\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.parallel\nimport torch.nn.functional as F\nimport torch.backends.cudnn as cudnn\nimport torch.distributed as dist\nimport torch.optim as optim\nimport torch.utils.data as data\nimport torch.utils.data.distributed\nimport torchvision.transforms as transforms\nimport torchvision.datasets as datasets\nimport torchvision.models as models\nimport models.imagenet as customized_models\nfrom flops_counter import get_model_complexity_info\nfrom PIL import ImageFile\nImageFile.LOAD_TRUNCATED_IMAGES = True\n\nfrom utils import Bar, Logger, AverageMeter, accuracy, mkdir_p\n\nimport warnings\nwarnings.filterwarnings(\'ignore\')\n\ntry:\n    from apex.parallel import DistributedDataParallel as DDP\n    from apex.fp16_utils import *\n    from apex import amp, optimizers\nexcept ImportError:\n    raise ImportError(""Please install apex from https://www.github.com/nvidia/apex to run this example."")\n\n# for servers to immediately record the logs\ndef flush_print(func):\n    def new_print(*args, **kwargs):\n        func(*args, **kwargs)\n        sys.stdout.flush()\n    return new_print\nprint = flush_print(print)\n\n\nfrom torch.optim.optimizer import Optimizer, required\n\nclass LSGD(Optimizer):\n    def __init__(self, params, lr=required, momentum=0, dampening=0,\n                 weight_decay=0, nesterov=False):\n        if lr is not required and lr < 0.0:\n            raise ValueError(""Invalid learning rate: {}"".format(lr))\n        if momentum < 0.0:\n            raise ValueError(""Invalid momentum value: {}"".format(momentum))\n        if weight_decay < 0.0:\n            raise ValueError(""Invalid weight_decay value: {}"".format(weight_decay))\n\n        defaults = dict(lr=lr, momentum=momentum, dampening=dampening,\n                        weight_decay=weight_decay, nesterov=nesterov)\n        if nesterov and (momentum <= 0 or dampening != 0):\n            raise ValueError(""Nesterov momentum requires a momentum and zero dampening"")\n        super(LSGD, self).__init__(params, defaults)\n\n    def __setstate__(self, state):\n        super(LSGD, self).__setstate__(state)\n        for group in self.param_groups:\n            group.setdefault(\'nesterov\', False)\n\n\n    def preprint_weight(self, print_flag=False):\n        max_norm = 0\n        min_norm = 11111111111111111\n        change_num = 0\n        total_num = 0\n        for group in self.param_groups:\n            weight_decay = group[\'weight_decay\']\n            lr = group[\'lr\']\n\n            for p in group[\'params\']:\n                if p.dim() == 4:\n                    assert(weight_decay == 0)\n                    sz = p.size()\n                    w  = p.data.view(sz[0], -1)\n\n                    wn   = w.std(dim=1)\n                    wmax = wn.max().item()\n                    wmin = wn.min().item()\n                    max_norm = max(max_norm, wmax)\n                    min_norm = min(min_norm, wmin)\n\n        if args.print_always:\n            print_flag = True\n\n        if args.local_rank == 0 and print_flag:\n            print(\'max_std = \', max_norm, \'min_std = \', min_norm)\n\n\n\n    def step(self, closure=None, print_flag=False):\n        """"""Performs a single optimization step.\n\n        Arguments:\n            closure (callable, optional): A closure that reevaluates the model\n                and returns the loss.\n        """"""\n        loss = None\n        if closure is not None:\n            loss = closure()\n\n        eps = 1e-5\n\n        for group in self.param_groups:\n            weight_decay = group[\'weight_decay\']\n            momentum = group[\'momentum\']\n            dampening = group[\'dampening\']\n            nesterov = group[\'nesterov\']\n\n\n            for p in group[\'params\']:\n                if p.grad is None:\n                    continue\n                d_p = p.grad.data\n\n\n                if weight_decay != 0:\n                    d_p.add_(weight_decay, p.data)\n\n                sz = p.data.size()\n                if d_p.dim() == 4 and sz[1] != 1: # we do not consider dw conv\n                    assert(weight_decay == 0)\n                    sz = p.data.size()\n                    w  = p.data.view(sz[0], -1)\n                    wstd = w.std(dim=1).view(sz[0], 1, 1, 1)\n                    wmean = w.mean(dim=1).view(sz[0], 1, 1, 1)\n\n                    if args.local_rank == 0 and (args.print_always or print_flag):\n                        wm = wstd.view(-1).mean().item()\n                        wmm = wmean.view(-1).mean().item()\n                        print(\'lam = %.6f\' % args.lam, \'mineps = %.6f\' % args.mineps, \'1 - eps/std = %.10f\' % (1 - args.mineps / wm), \'std = %.10f\' % wm, \'mean = %.10f\' % wmm,  \'sz = \', sz)\n                    \n                    d_p.add_(args.lam, (1 - args.mineps / wstd) * (p.data - wmean) + wmean)\n\n\n                if momentum != 0:\n                    param_state = self.state[p]\n                    if \'momentum_buffer\' not in param_state:\n                        buf = param_state[\'momentum_buffer\'] = torch.zeros_like(p.data)\n                        buf.mul_(momentum).add_(d_p)\n                    else:\n                        buf = param_state[\'momentum_buffer\']\n                        buf.mul_(momentum).add_(1 - dampening, d_p)\n                    if nesterov:\n                        d_p = d_p.add(momentum, buf)\n                    else:\n                        d_p = buf\n\n                p.data.add_(-group[\'lr\'], d_p)\n\n        return loss\n\n\n# Models\ndefault_model_names = sorted(name for name in models.__dict__\n    if name.islower() and not name.startswith(""__"")\n    and callable(models.__dict__[name]))\n\ncustomized_models_names = sorted(name for name in customized_models.__dict__\n    if name.islower() and not name.startswith(""__"")\n    and callable(customized_models.__dict__[name]))\n\nfor name in customized_models.__dict__:\n    if name.islower() and not name.startswith(""__"") and callable(customized_models.__dict__[name]):\n        models.__dict__[name] = customized_models.__dict__[name]\n\nmodel_names = default_model_names + customized_models_names\n\n# Parse arguments\nparser = argparse.ArgumentParser(description=\'PyTorch ImageNet Training\')\n\nparser.add_argument(\'--cutmix\', dest=\'cutmix\', action=\'store_true\')\nparser.add_argument(\'--cutmix_prob\', default=1., type=float)\n\nparser.add_argument(\'--cutout\', dest=\'cutout\', action=\'store_true\')\nparser.add_argument(\'--cutout_size\', default=112, type=float)\n\nparser.add_argument(\'--el2\', dest=\'el2\', action=\'store_true\', help=\'whether to use e-shifted L2 regularizer\')\nparser.add_argument(\'--mineps\', dest=\'mineps\', default=1e-8, type=float, help=\'min of weights std\')\nparser.add_argument(\'--lam\', dest=\'lam\', default=4e-5, type=float, help=\'lam of weights\')\nparser.add_argument(\'--print-always\', dest=\'print_always\', action=\'store_true\')\n\nparser.add_argument(\'--nowd-bn\', dest=\'nowd_bn\', action=\'store_true\',\n                    help=\'no weight decay on bn weights\')\nparser.add_argument(\'--nowd-fc\', dest=\'nowd_fc\', action=\'store_true\',\n                    help=\'no weight decay on fc weights\')\nparser.add_argument(\'--nowd-conv\', dest=\'nowd_conv\', action=\'store_true\',\n                    help=\'no weight decay on conv weights\')\n\n# Datasets\nparser.add_argument(\'-d\', \'--data\', default=\'path to dataset\', type=str)\nparser.add_argument(\'-j\', \'--workers\', default=32, type=int, metavar=\'N\',\n                    help=\'number of data loading workers (default: 4)\')\n# Optimization options\nparser.add_argument(\'--opt-level\', default=\'O2\', type=str, \n        help=\'O2 is mixed FP16/32 training, see more in https://github.com/NVIDIA/apex/tree/f5cd5ae937f168c763985f627bbf850648ea5f3f/examples/imagenet\')\nparser.add_argument(\'--keep-batchnorm-fp32\', default=True, action=\'store_true\',\n                    help=\'keeping cudnn bn leads to fast training\')\nparser.add_argument(\'--loss-scale\', type=float, default=None)\n\nparser.add_argument(\'--mixup\', dest=\'mixup\', action=\'store_true\',\n                    help=\'whether to use mixup\')\nparser.add_argument(\'--alpha\', default=0.2, type=float,\n                    metavar=\'mixup alpha\', help=\'alpha value for mixup B(alpha, alpha) distribution\')\nparser.add_argument(\'--cos\', dest=\'cos\', action=\'store_true\', \n                    help=\'using cosine decay lr schedule\')\nparser.add_argument(\'--warmup\', \'--wp\', default=5, type=int,\n                    help=\'number of epochs to warmup\')\nparser.add_argument(\'--epochs\', default=120, type=int, metavar=\'N\',\n                    help=\'number of total epochs to run\')\nparser.add_argument(\'--start-epoch\', default=0, type=int, metavar=\'N\',\n                    help=\'manual epoch number (useful on restarts)\')\nparser.add_argument(\'--train-batch\', default=256, type=int, metavar=\'N\',\n                    help=\'train batchsize (default: 256)\')\nparser.add_argument(\'--test-batch\', default=125, type=int, metavar=\'N\',\n                    help=\'test batchsize (default: 200)\')\nparser.add_argument(\'--lr\', \'--learning-rate\', default=0.1, type=float,\n                    metavar=\'LR\', help=\'initial learning rate\')\nparser.add_argument(\'--drop\', \'--dropout\', default=0, type=float,\n                    metavar=\'Dropout\', help=\'Dropout ratio\')\nparser.add_argument(\'--schedule\', type=int, nargs=\'+\', default=[30, 60, 90],\n                        help=\'Decrease learning rate at these epochs.\')\nparser.add_argument(\'--gamma\', type=float, default=0.1, help=\'LR is multiplied by gamma on schedule.\')\nparser.add_argument(\'--momentum\', default=0.9, type=float, metavar=\'M\',\n                    help=\'momentum\')\nparser.add_argument(\'--weight-decay\', \'--wd\', default=1e-4, type=float,\n                    metavar=\'W\', help=\'weight decay (default: 1e-4)\')\nparser.add_argument(\'--wd-all\', dest = \'wdall\', action=\'store_true\', \n                    help=\'weight decay on all parameters\')\n\n# Checkpoints\nparser.add_argument(\'--print-freq\', \'-p\', default=10, type=int,\n                    metavar=\'N\', help=\'print frequency (default: 10)\')\nparser.add_argument(\'-c\', \'--checkpoint\', default=\'checkpoint\', type=str, metavar=\'PATH\',\n                    help=\'path to save checkpoint (default: checkpoint)\')\nparser.add_argument(\'--resume\', default=\'\', type=str, metavar=\'PATH\',\n                    help=\'path to latest checkpoint (default: none)\')\n# Architecture\nparser.add_argument(\'--arch\', \'-a\', metavar=\'ARCH\', default=\'resnet18\',\n                    choices=model_names,\n                    help=\'model architecture: \' +\n                        \' | \'.join(model_names) +\n                        \' (default: resnet18)\')\nparser.add_argument(\'--depth\', type=int, default=29, help=\'Model depth.\')\nparser.add_argument(\'--cardinality\', type=int, default=32, help=\'ResNet cardinality (group).\')\nparser.add_argument(\'--base-width\', type=int, default=4, help=\'ResNet base width.\')\nparser.add_argument(\'--widen-factor\', type=int, default=4, help=\'Widen factor. 4 -> 64, 8 -> 128, ...\')\n# Miscs\nparser.add_argument(\'--manualSeed\', type=int, help=\'manual seed\')\nparser.add_argument(\'-e\', \'--evaluate\', dest=\'evaluate\', action=\'store_true\',\n                    help=\'evaluate model on validation set\')\nparser.add_argument(\'--pretrained\', dest=\'pretrained\', action=\'store_true\',\n                    help=\'use pre-trained model\')\n#Device options\n#parser.add_argument(\'--gpu-id\', default=\'0\', type=str, help=\'id(s) for CUDA_VISIBLE_DEVICES\')\nparser.add_argument(\'--local_rank\', default=0, type=int)\n\nargs = parser.parse_args()\nstate = {k: v for k, v in args._get_kwargs()}\n\nprint(""opt_level = {}"".format(args.opt_level))\nprint(""keep_batchnorm_fp32 = {}"".format(args.keep_batchnorm_fp32), type(args.keep_batchnorm_fp32))\nprint(""loss_scale = {}"".format(args.loss_scale), type(args.loss_scale))\n\n# Use CUDA\n# os.environ[\'CUDA_VISIBLE_DEVICES\'] = args.gpu_id\nuse_cuda = torch.cuda.is_available()\n\n# Random seed\nif args.manualSeed is None:\n    args.manualSeed = random.randint(1, 10000)\nrandom.seed(args.manualSeed)\ntorch.manual_seed(args.manualSeed)\nif use_cuda:\n    torch.cuda.manual_seed_all(args.manualSeed)\n\nbest_acc = 0  # best test accuracy\n\ndef fast_collate(batch):\n    imgs = [img[0] for img in batch]\n    targets = torch.tensor([target[1] for target in batch], dtype=torch.int64)\n    w = imgs[0].size[0]\n    h = imgs[0].size[1]\n    tensor = torch.zeros((len(imgs), 3, h, w), dtype=torch.uint8)\n    for i, img in enumerate(imgs):\n        nump_array = np.asarray(img, dtype=np.uint8)\n        # tens = torch.from_numpy(nump_array)\n        if nump_array.ndim < 3:\n            nump_array = np.expand_dims(nump_array, axis=-1)\n        nump_array = np.rollaxis(nump_array, 2)\n\n        tensor[i] += torch.from_numpy(nump_array)\n\n    return tensor, targets\n\nclass data_prefetcher():\n    def __init__(self, loader):\n        self.loader = iter(loader)\n        self.stream = torch.cuda.Stream()\n        self.mean   = torch.tensor([0.485 * 255, 0.456 * 255, 0.406 * 255]) \\\n                .cuda().view(1, 3, 1, 1)\n        self.std    = torch.tensor([0.229 * 255, 0.224 * 255, 0.225 * 255]) \\\n                .cuda().view(1, 3, 1, 1)\n        self.preload()\n\n    def preload(self):\n        try:\n            self.next_input, self.next_target = next(self.loader)\n        except StopIteration:\n            self.next_input = None\n            self.next_target = None\n            return\n        \n        with torch.cuda.stream(self.stream):\n            self.next_input = self.next_input.cuda(non_blocking=True)\n            self.next_target = self.next_target.cuda(non_blocking=True)\n            self.next_input = self.next_input.float()\n            self.next_input = self.next_input.sub_(self.mean).div_(self.std)\n\n    def next(self):\n        torch.cuda.current_stream().wait_stream(self.stream)\n        input = self.next_input\n        target = self.next_target\n        if input is not None:\n            self.preload()\n        return input, target\n\n\ndef main():\n    global best_acc\n    start_epoch = args.start_epoch  # start from epoch 0 or last checkpoint epoch\n\n    if not os.path.isdir(args.checkpoint) and args.local_rank == 0:\n        mkdir_p(args.checkpoint)\n\n    args.distributed = True\n    args.gpu = args.local_rank\n    torch.cuda.set_device(args.gpu)\n    torch.distributed.init_process_group(backend=\'nccl\', init_method=\'env://\')\n    args.world_size = torch.distributed.get_world_size()\n    print(\'world_size = \', args.world_size)\n\n    assert torch.backends.cudnn.enabled, ""Amp requires cudnn backend to be enabled.""\n\n    # create model\n    if args.pretrained:\n        print(""=> using pre-trained model \'{}\'"".format(args.arch))\n        model = models.__dict__[args.arch](pretrained=True)\n    elif \'resnext\' in args.arch:\n        model = models.__dict__[args.arch](\n                    baseWidth=args.base_width,\n                    cardinality=args.cardinality,\n                )\n    else:\n        print(""=> creating model \'{}\'"".format(args.arch))\n        model = models.__dict__[args.arch]()\n\n    flops, params = get_model_complexity_info(model, (224, 224), as_strings=False, print_per_layer_stat=False)\n    print(\'Flops:  %.3f\' % (flops / 1e9))\n    print(\'Params: %.2fM\' % (params / 1e6))\n\n    cudnn.benchmark = True\n    # define loss function (criterion) and optimizer\n    # criterion = nn.CrossEntropyLoss().cuda()\n    criterion = SoftCrossEntropyLoss(label_smoothing=0.1).cuda()\n    model = model.cuda()\n\n    args.lr = float(args.lr * float(args.train_batch*args.world_size)/256.) # default args.lr = 0.1 -> 256\n    optimizer = set_optimizer(model)\n\n    #optim.SGD(model.parameters(), lr=args.lr, momentum=args.momentum, weight_decay=args.weight_decay)\n\n    model, optimizer = amp.initialize(model, optimizer,\n                                      opt_level=args.opt_level,\n                                      keep_batchnorm_fp32=args.keep_batchnorm_fp32,\n                                      loss_scale=args.loss_scale)\n\n    #model = torch.nn.DataParallel(model).cuda()\n    #model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[args.local_rank], output_device=args.local_rank)\n    model = DDP(model, delay_allreduce=True)\n\n    # Data loading code\n    traindir  = os.path.join(args.data, \'train\')\n    #trainmeta = os.path.join(args.data, \'meta/train.txt\')\n    valdir  = os.path.join(args.data, \'valf\')\n    #valmeta = os.path.join(args.data, \'meta/val.txt\')\n    #normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n\n    data_aug_scale = (0.08, 1.0)\n\n    train_dataset = datasets.ImageFolder(traindir, transforms.Compose([\n            transforms.RandomResizedCrop(224, scale = data_aug_scale),\n            transforms.RandomHorizontalFlip(),\n            # transforms.ToTensor(),\n            # normalize,\n        ]))\n    val_dataset   = datasets.ImageFolder(valdir, transforms.Compose([\n            transforms.Resize(256),\n            transforms.CenterCrop(224),\n            # transforms.ToTensor(),\n            # normalize,\n        ]))\n\n    train_sampler = torch.utils.data.distributed.DistributedSampler(train_dataset)\n    val_sampler = torch.utils.data.distributed.DistributedSampler(val_dataset)\n\n    train_loader = torch.utils.data.DataLoader(\n        train_dataset, batch_size=args.train_batch, shuffle=False,\n        num_workers=args.workers, pin_memory=True, sampler=train_sampler, collate_fn=fast_collate)\n    val_loader = torch.utils.data.DataLoader(\n        val_dataset, batch_size=args.test_batch, shuffle=False,\n        num_workers=args.workers, pin_memory=True, sampler=val_sampler, collate_fn=fast_collate)\n\n\n    # Resume\n    title = \'ImageNet-\' + args.arch\n    if args.resume:\n        # Load checkpoint.\n        print(\'==> Resuming from checkpoint..\', args.resume)\n        assert os.path.isfile(args.resume), \'Error: no checkpoint directory found!\'\n        args.checkpoint = os.path.dirname(args.resume)\n        checkpoint = torch.load(args.resume, map_location = lambda storage, loc: storage.cuda(args.gpu))\n        best_acc = checkpoint[\'best_acc\']\n        start_epoch = checkpoint[\'epoch\']\n        # model may have more keys\n        t = model.state_dict()\n        c = checkpoint[\'state_dict\']\n        for k in t:\n            if k not in c:\n                print(\'not in loading dict! fill it\', k, t[k])\n                c[k] = t[k]\n        model.load_state_dict(c)\n        print(\'optimizer load old state\')\n        optimizer.load_state_dict(checkpoint[\'optimizer\'])\n        if args.local_rank == 0:\n            logger = Logger(os.path.join(args.checkpoint, \'log.txt\'), title=title, resume=True)\n    else:\n        if args.local_rank == 0:\n            logger = Logger(os.path.join(args.checkpoint, \'log.txt\'), title=title)\n            logger.set_names([\'Learning Rate\', \'Train Loss\', \'Valid Loss\', \'Train Acc.\', \'Valid Acc.\'])\n\n\n    if args.evaluate:\n        print(\'\\nEvaluation only\')\n        test_loss, test_acc = test(val_loader, model, criterion, start_epoch, use_cuda)\n        print(\' Test Loss:  %.8f, Test Acc:  %.2f\' % (test_loss, test_acc))\n        return\n\n    scheduler = CosineAnnealingLR(optimizer,\n        args.epochs, len(train_loader), eta_min=0., warmup=args.warmup) \n\n    # Train and val\n    for epoch in range(start_epoch, args.epochs):\n        train_sampler.set_epoch(epoch)\n\n\n        if args.local_rank == 0:\n            print(\'\\nEpoch: [%d | %d]\' % (epoch + 1, args.epochs))\n\n        train_loss, train_acc = train(train_loader, model, criterion, optimizer, epoch, scheduler, use_cuda)\n        test_loss, test_acc = test(val_loader, model, criterion, epoch, use_cuda)\n\n        # save model\n        if args.local_rank == 0:\n            # append logger file\n            logger.append([state[\'lr\'], train_loss, test_loss, train_acc, test_acc])\n\n            is_best = test_acc > best_acc\n            best_acc = max(test_acc, best_acc)\n            save_checkpoint({\n                    \'epoch\': epoch + 1,\n                    \'state_dict\': model.state_dict(),\n                    \'acc\': test_acc,\n                    \'best_acc\': best_acc,\n                    \'optimizer\' : optimizer.state_dict(),\n                }, is_best, checkpoint=args.checkpoint)\n\n    if args.local_rank == 0:\n        logger.close()\n\n    print(\'Best acc:\')\n    print(best_acc)\n\ndef train(train_loader, model, criterion, optimizer, epoch, scheduler, use_cuda):\n    printflag = False\n    # switch to train mode\n    model.train()\n    torch.set_grad_enabled(True)\n\n    batch_time = AverageMeter()\n    losses = AverageMeter()\n    top1 = AverageMeter()\n    top5 = AverageMeter()\n    end = time.time()\n\n    if args.local_rank == 0:\n        bar = Bar(\'Processing\', max=len(train_loader))\n    show_step = len(train_loader) // 10\n    \n    prefetcher = data_prefetcher(train_loader)\n    inputs, targets = prefetcher.next()\n\n    batch_idx = -1\n    while inputs is not None:\n        batch_idx += 1\n        lr = scheduler.update(epoch, batch_idx)\n\n        batch_size = inputs.size(0)\n        if batch_size < args.train_batch:\n            break\n\n        if (batch_idx) % show_step == 0 and args.local_rank == 0:\n            print_flag = True\n        else:\n            print_flag = False\n\n        #optimizer.preprint_weight(print_flag)\n\n        if args.cutmix:\n            if printflag==False:\n                print(\'using cutmix !\')\n                printflag=True\n            inputs, targets_a, targets_b, lam = cutmix_data(inputs, targets, args.cutmix_prob, use_cuda)\n            outputs = model(inputs)\n            loss_func = mixup_criterion(targets_a, targets_b, lam)\n            old_loss = loss_func(criterion, outputs)\n        elif args.mixup:\n            if printflag==False:\n                print(\'using mixup !\')\n                printflag=True\n            inputs, targets_a, targets_b, lam = mixup_data(inputs, targets, args.alpha, use_cuda)\n            outputs = model(inputs)\n            loss_func = mixup_criterion(targets_a, targets_b, lam)\n            old_loss = loss_func(criterion, outputs)\n        elif args.cutout:\n            if printflag==False:\n                print(\'using cutout !\')\n                printflag=True\n            inputs = cutout_data(inputs, args.cutout_size, use_cuda)\n            outputs = model(inputs)\n            old_loss = criterion(outputs, targets)\n        else:\n            outputs = model(inputs)\n            old_loss = criterion(outputs, targets)\n\n        # compute gradient and do SGD step\n        optimizer.zero_grad()\n        # loss.backward()\n        with amp.scale_loss(old_loss, optimizer) as loss:\n            loss.backward()\n        #optimizer.step()\n        if args.el2:\n            optimizer.step(print_flag=print_flag)\n        else:\n            optimizer.step()\n\n\n        if batch_idx % args.print_freq == 0:\n            # measure accuracy and record loss\n            prec1, prec5 = accuracy(outputs.data, targets.data, topk=(1, 5))\n            reduced_loss = reduce_tensor(loss.data)\n            prec1        = reduce_tensor(prec1)\n            prec5        = reduce_tensor(prec5)\n\n            # to_python_float incurs a host<->device sync\n            losses.update(to_python_float(reduced_loss), inputs.size(0))\n            top1.update(to_python_float(prec1), inputs.size(0))\n            top5.update(to_python_float(prec5), inputs.size(0))\n\n            torch.cuda.synchronize()\n            # measure elapsed time\n            batch_time.update((time.time() - end) / args.print_freq)\n            end = time.time()\n\n            if args.local_rank == 0: # plot progress\n                bar.suffix  = \'({batch}/{size}) lr({lr:.6f}) | Batch: {bt:.3f}s | Total: {total:} | Loss: {loss:.4f} | top1: {top1: .4f} | top5: {top5: .4f}\'.format(\n                            lr=lr[0],\n                            batch=batch_idx + 1,\n                            size=len(train_loader),\n                            bt=batch_time.val,\n                            total=bar.elapsed_td,\n                            loss=losses.avg,\n                            top1=top1.avg,\n                            top5=top5.avg,\n                            )\n                bar.next()\n        if (batch_idx) % show_step == 0 and args.local_rank == 0:\n            print(\'E%d\' % (epoch) + bar.suffix)\n\n        inputs, targets = prefetcher.next()\n\n    if args.local_rank == 0:\n        bar.finish()\n    return (losses.avg, top1.avg)\n\ndef test(val_loader, model, criterion, epoch, use_cuda):\n    global best_acc\n\n    batch_time = AverageMeter()\n    losses = AverageMeter()\n    top1 = AverageMeter()\n    top5 = AverageMeter()\n\n    # switch to evaluate mode\n    model.eval()\n    # torch.set_grad_enabled(False)\n\n    end = time.time()\n    if args.local_rank == 0:\n        bar = Bar(\'Processing\', max=len(val_loader))\n\n    prefetcher = data_prefetcher(val_loader)\n    inputs, targets = prefetcher.next()\n\n    batch_idx = -1\n    while inputs is not None:\n        batch_idx += 1\n\n        # compute output\n        with torch.no_grad():\n            outputs = model(inputs)\n            loss = criterion(outputs, targets)\n\n        # measure accuracy and record loss\n        prec1, prec5 = accuracy(outputs.data, targets.data, topk=(1, 5))\n\n        reduced_loss = reduce_tensor(loss.data)\n        prec1        = reduce_tensor(prec1)\n        prec5        = reduce_tensor(prec5)\n\n        # to_python_float incurs a host<->device sync\n        losses.update(to_python_float(reduced_loss), inputs.size(0))\n        top1.update(to_python_float(prec1), inputs.size(0))\n        top5.update(to_python_float(prec5), inputs.size(0))\n\n        # measure elapsed time\n        batch_time.update(time.time() - end)\n        end = time.time()\n\n        # plot progress\n        if args.local_rank == 0:\n            bar.suffix  = \'Valid({batch}/{size}) | Batch: {bt:.3f}s | Total: {total:} | Loss: {loss:.4f} | top1: {top1: .4f} | top5: {top5: .4f}\'.format(\n                        batch=batch_idx + 1,\n                        size=len(val_loader),\n                        bt=batch_time.avg,\n                        total=bar.elapsed_td,\n                        loss=losses.avg,\n                        top1=top1.avg,\n                        top5=top5.avg,\n                        )\n            bar.next()\n\n        inputs, targets = prefetcher.next()\n\n    if args.local_rank == 0:\n        print(bar.suffix)\n        bar.finish()\n    return (losses.avg, top1.avg)\n\ndef save_checkpoint(state, is_best, checkpoint=\'checkpoint\', filename=\'checkpoint.pth.tar\'):\n    filepath = os.path.join(checkpoint, filename)\n    torch.save(state, filepath)\n    if is_best:\n        shutil.copyfile(filepath, os.path.join(checkpoint, \'model_best.pth.tar\'))\n\ndef set_optimizer(model):\n    optim_use = optim.SGD\n    if args.el2:\n        optim_use = LSGD\n        if args.local_rank == 0:\n            print(\'use e-shifted L2 regularizer based SGD optimizer!\')\n    else:\n        if args.local_rank == 0:\n            print(\'use SGD optimizer!\')\n\n\n    if args.wdall:\n        optimizer = optim_use(model.parameters(), lr=args.lr, momentum=args.momentum, weight_decay=args.weight_decay)\n        print(\'weight decay on all parameters\')\n    else:\n        decay_list = []\n        no_decay_list = []\n        dns = []\n        ndns = []\n\n        for name, p in model.named_parameters():\n            no_decay_flag = False\n            dim = p.dim()\n\n            if \'bias\' in name:\n                no_decay_flag = True\n            elif dim == 1:\n                if args.nowd_bn: # bn weights\n                    no_decay_flag = True\n            elif dim == 2:\n                if args.nowd_fc:  # fc weights\n                    no_decay_flag = True\n            elif dim == 4:\n                if args.nowd_conv: # conv weights\n                    no_decay_flag = True\n            else:\n                print(\'no valid dim!!!, dim = \', dim)\n                exit(-1)\n\n            if no_decay_flag:\n                no_decay_list.append(p)\n                ndns.append(name)\n            else:\n                decay_list.append(p)\n                dns.append(name)\n        if args.local_rank == 0:\n            print(\'------------\' * 6)\n            print(\'no decay list = \', ndns)\n            print(\'------------\' * 6)\n            print(\'decay list = \', dns)\n            print(\'------summary------\')\n            if args.nowd_bn:\n                print(\'no decay on bn weights!\')\n            else:\n                print(\'decay on bn weights!\')\n            if args.nowd_conv:\n                print(\'no decay on conv weights!\')\n            else:\n                print(\'decay on conv weights!\')\n            if args.nowd_fc:\n                print(\'no decay on fc weights!\')\n            else:\n                print(\'decay on fc weights!\')\n            print(\'------------\' * 6)\n\n        params = [{\'params\': no_decay_list, \'weight_decay\': 0},\n                  {\'params\': decay_list}]\n        optimizer = optim_use(params, lr=args.lr, momentum=args.momentum, weight_decay=args.weight_decay)\n        if args.local_rank == 0:\n            print(\'optimizer = \', optimizer)\n    return optimizer\n\n\nclass SoftCrossEntropyLoss(nn.NLLLoss):\n    def __init__(self, label_smoothing=0, num_classes=1000, **kwargs):\n        assert label_smoothing >= 0 and label_smoothing <= 1\n        super(SoftCrossEntropyLoss, self).__init__(**kwargs)\n        self.confidence = 1 - label_smoothing\n        self.other      = label_smoothing * 1.0 / (num_classes - 1)\n        self.criterion  = nn.KLDivLoss(reduction=\'batchmean\')\n        print(\'using soft celoss!!!, label_smoothing = \', label_smoothing)\n\n    def forward(self, input, target):\n        one_hot = torch.zeros_like(input)\n        one_hot.fill_(self.other)\n        one_hot.scatter_(1, target.unsqueeze(1).long(), self.confidence)\n        input   = F.log_softmax(input, 1)\n        return self.criterion(input, one_hot)\n\ndef mixup_data(x, y, alpha=1.0, use_cuda=True):\n    if alpha > 0.:\n        lam = np.random.beta(alpha, alpha)\n    else:\n        lam = 1.\n\n    batch_size = x.size(0)\n    if use_cuda:\n        index = torch.randperm(batch_size).cuda()\n    else:\n        index = torch.randperm(batch_size)\n\n    mixed_x = lam * x + (1 - lam) * x[index, ...]\n    y_a, y_b = y, y[index]\n    return mixed_x, y_a, y_b, lam\n\n\ndef rand_bbox(size, lam):\n    W = size[2]\n    H = size[3]\n    cut_rat = np.sqrt(1. - lam)\n    cut_w = np.int(W * cut_rat)\n    cut_h = np.int(H * cut_rat)\n\n    # uniform\n    cx = np.random.randint(W)\n    cy = np.random.randint(H)\n\n    bbx1 = np.clip(cx - cut_w // 2, 0, W)\n    bby1 = np.clip(cy - cut_h // 2, 0, H)\n    bbx2 = np.clip(cx + cut_w // 2, 0, W)\n    bby2 = np.clip(cy + cut_h // 2, 0, H)\n\n    return bbx1, bby1, bbx2, bby2\n\ndef cutmix_data(x, y, cutmix_prob=1.0, use_cuda=True):\n    lam = np.random.beta(1, 1)\n\n    batch_size = x.size(0)\n    index = torch.randperm(batch_size).cuda()\n    y_a, y_b = y, y[index]\n\n    bbx1, bby1, bbx2, bby2 = rand_bbox(x.size(), lam)\n    x[:, :, bbx1:bbx2, bby1:bby2] = x[index, :, bbx1:bbx2, bby1:bby2]\n    lam = 1 - ((bbx2 - bbx1) * (bby2 - bby1) / (x.size()[-1] * x.size()[-2]))\n    return x, y_a, y_b, lam\n\ndef cutout_data(x, cutout_size=112, use_cuda=True):\n    W = x.size(2)\n    H = x.size(3)\n    cx = np.random.randint(W)\n    cy = np.random.randint(H)\n\n    bbx1 = np.clip(cx - cutout_size // 2, 0, W)\n    bby1 = np.clip(cy - cutout_size // 2, 0, H)\n    bbx2 = np.clip(cx + cutout_size // 2, 0, W)\n    bby2 = np.clip(cy + cutout_size // 2, 0, H)\n\n    x[:, :, bbx1:bbx2, bby1:bby2] = 0\n\n    return x\n\n\ndef mixup_criterion(y_a, y_b, lam):\n    return lambda criterion, pred: lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)\n\ndef reduce_tensor(tensor):\n    rt = tensor.clone()\n    dist.all_reduce(rt, op=dist.ReduceOp.SUM)\n    rt /= args.world_size\n    return rt\n\nclass CosineAnnealingLR(object):\n    def __init__(self, optimizer, T_max, N_batch, eta_min=0, last_epoch=-1, warmup=0):\n        if not isinstance(optimizer, torch.optim.Optimizer):\n            raise TypeError(\'{} is not an Optimizer\'.format(\n                type(optimizer).__name__))\n        self.optimizer = optimizer\n        self.T_max = T_max\n        self.N_batch = N_batch\n        self.eta_min = eta_min\n        self.warmup = warmup\n\n        if last_epoch == -1:\n            for group in optimizer.param_groups:\n                group.setdefault(\'initial_lr\', group[\'lr\'])\n        else:\n            for i, group in enumerate(optimizer.param_groups):\n                if \'initial_lr\' not in group:\n                    raise KeyError(""param \'initial_lr\' is not specified ""\n                                   ""in param_groups[{}] when resuming an optimizer"".format(i))\n        self.base_lrs = list(map(lambda group: group[\'initial_lr\'], optimizer.param_groups))\n        self.update(last_epoch+1)\n        self.last_epoch = last_epoch\n        self.iter = 0\n\n    def state_dict(self):\n        return {key: value for key, value in self.__dict__.items() if key != \'optimizer\'}\n\n    def load_state_dict(self, state_dict):\n        self.__dict__.update(state_dict)\n\n    def get_lr(self):\n        if self.last_epoch < self.warmup:\n            lrs = [base_lr * (self.last_epoch + self.iter / self.N_batch) / self.warmup for base_lr in self.base_lrs]\n        else:\n            lrs = [self.eta_min + (base_lr - self.eta_min) *\n                    (1 + math.cos(math.pi * (self.last_epoch - self.warmup + self.iter / self.N_batch) / (self.T_max - self.warmup))) / 2\n                    for base_lr in self.base_lrs]\n        return lrs\n\n    def update(self, epoch, batch=0):\n        self.last_epoch = epoch\n        self.iter = batch + 1\n        lrs = self.get_lr()\n        for param_group, lr in zip(self.optimizer.param_groups, lrs):\n            param_group[\'lr\'] = lr\n\n        return lrs\n\n\nif __name__ == \'__main__\':\n    main()\n'"
detection/setup.py,0,"b'import os\nimport subprocess\nimport time\nfrom setuptools import find_packages, setup\n\n\ndef readme():\n    with open(\'README.md\', encoding=\'utf-8\') as f:\n        content = f.read()\n    return content\n\n\nMAJOR = 0\nMINOR = 6\nPATCH = \'rc0\'\nSUFFIX = \'\'\nSHORT_VERSION = \'{}.{}.{}{}\'.format(MAJOR, MINOR, PATCH, SUFFIX)\n\nversion_file = \'mmdet/version.py\'\n\n\ndef get_git_hash():\n\n    def _minimal_ext_cmd(cmd):\n        # construct minimal environment\n        env = {}\n        for k in [\'SYSTEMROOT\', \'PATH\', \'HOME\']:\n            v = os.environ.get(k)\n            if v is not None:\n                env[k] = v\n        # LANGUAGE is used on win32\n        env[\'LANGUAGE\'] = \'C\'\n        env[\'LANG\'] = \'C\'\n        env[\'LC_ALL\'] = \'C\'\n        out = subprocess.Popen(\n            cmd, stdout=subprocess.PIPE, env=env).communicate()[0]\n        return out\n\n    try:\n        out = _minimal_ext_cmd([\'git\', \'rev-parse\', \'HEAD\'])\n        sha = out.strip().decode(\'ascii\')\n    except OSError:\n        sha = \'unknown\'\n\n    return sha\n\n\ndef get_hash():\n    if os.path.exists(\'.git\'):\n        sha = get_git_hash()[:7]\n    elif os.path.exists(version_file):\n        try:\n            from mmdet.version import __version__\n            sha = __version__.split(\'+\')[-1]\n        except ImportError:\n            raise ImportError(\'Unable to get git version\')\n    else:\n        sha = \'unknown\'\n\n    return sha\n\n\ndef write_version_py():\n    content = """"""# GENERATED VERSION FILE\n# TIME: {}\n\n__version__ = \'{}\'\nshort_version = \'{}\'\n""""""\n    sha = get_hash()\n    VERSION = SHORT_VERSION + \'+\' + sha\n\n    with open(version_file, \'w\') as f:\n        f.write(content.format(time.asctime(), VERSION, SHORT_VERSION))\n\n\ndef get_version():\n    with open(version_file, \'r\') as f:\n        exec(compile(f.read(), version_file, \'exec\'))\n    return locals()[\'__version__\']\n\n\nif __name__ == \'__main__\':\n    write_version_py()\n    setup(\n        name=\'mmdet\',\n        version=get_version(),\n        description=\'Open MMLab Detection Toolbox\',\n        long_description=readme(),\n        keywords=\'computer vision, object detection\',\n        url=\'https://github.com/open-mmlab/mmdetection\',\n        packages=find_packages(exclude=(\'configs\', \'tools\', \'demo\')),\n        package_data={\'mmdet.ops\': [\'*/*.so\']},\n        classifiers=[\n            \'Development Status :: 4 - Beta\',\n            \'License :: OSI Approved :: Apache Software License\',\n            \'Operating System :: OS Independent\',\n            \'Programming Language :: Python :: 2\',\n            \'Programming Language :: Python :: 2.7\',\n            \'Programming Language :: Python :: 3\',\n            \'Programming Language :: Python :: 3.4\',\n            \'Programming Language :: Python :: 3.5\',\n            \'Programming Language :: Python :: 3.6\',\n        ],\n        license=\'GPLv3\',\n        setup_requires=[\'pytest-runner\'],\n        tests_require=[\'pytest\'],\n        install_requires=[\n            \'mmcv\', \'numpy\', \'matplotlib\', \'six\', \'terminaltables\',\n            \'pycocotools\'\n        ],\n        zip_safe=False)\n'"
classification/models/layers.py,4,"b""import torch\nimport torch.nn as nn\nfrom torch.nn.parameter import Parameter\nfrom torch.nn import functional as F\nfrom torch.nn import init\nimport math\n\nclass Conv2d(nn.Conv2d):\n    def __init__(self, in_channels, out_channels, kernel_size, stride=1,\n                 padding=0, dilation=1, groups=1, bias=True):\n        super(Conv2d, self).__init__(in_channels, out_channels, kernel_size, stride,\n                 padding, dilation, groups, bias)\n\n    def forward(self, x):\n        # return super(Conv2d, self).forward(x)\n        weight = self.weight\n        weight_mean = weight.mean(dim=1, keepdim=True).mean(dim=2,\n                                  keepdim=True).mean(dim=3, keepdim=True)\n        weight = weight - weight_mean\n        std = weight.view(weight.size(0), -1).std(dim=1).view(-1, 1, 1, 1) + 1e-5\n        weight = weight / std.expand_as(weight)\n        return F.conv2d(x, weight, self.bias, self.stride,\n                        self.padding, self.dilation, self.groups)\n\n\nclass A1Conv2d(nn.Conv2d):\n    def __init__(self, in_channels, out_channels, kernel_size, stride=1,\n                 padding=0, dilation=1, groups=1, bias=True):\n        super(A1Conv2d, self).__init__(in_channels, out_channels, kernel_size, stride,\n                 padding, dilation, groups, bias)\n\n        sz = self.weight.size()\n        d  = 1.0\n        for v in sz:\n            d *= v\n        print('self.d = ', d, '->', math.sqrt(d))\n        self.d  = math.sqrt(d)\n\n    def forward(self, x):\n        # return super(Conv2d, self).forward(x)\n        weight = self.weight\n\n        weight_mean = weight.mean(dim=1, keepdim=True).mean(dim=2,\n                                  keepdim=True).mean(dim=3, keepdim=True)\n        weight = weight - weight_mean\n        std = weight.view(weight.size(0), -1).std(dim=1).view(-1, 1, 1, 1) * self.d  + 1e-5\n        weight = weight / std.expand_as(weight)\n        return F.conv2d(x, weight, self.bias, self.stride,\n                        self.padding, self.dilation, self.groups)\n\n\ndef BatchNorm2d(num_features):\n    return nn.GroupNorm(num_channels=num_features, num_groups=32)\n\n"""
classification/utils/__init__.py,0,"b'""""""Useful utils\n""""""\nfrom .misc import *\nfrom .logger import *\nfrom .visualize import *\nfrom .eval import *\n\n# progress bar\nimport os, sys\nsys.path.append(os.path.join(os.path.dirname(__file__), ""progress""))\nfrom progress.bar import Bar as Bar'"
classification/utils/eval.py,0,"b'from __future__ import print_function, absolute_import\n\n__all__ = [\'accuracy\']\n\ndef accuracy(output, target, topk=(1,)):\n    """"""Computes the precision@k for the specified values of k""""""\n    maxk = max(topk)\n    batch_size = target.size(0)\n\n    _, pred = output.topk(maxk, 1, True, True)\n    pred = pred.t()\n    correct = pred.eq(target.view(1, -1).expand_as(pred))\n\n    res = []\n    for k in topk:\n        correct_k = correct[:k].view(-1).float().sum(0)\n        res.append(correct_k.mul_(100.0 / batch_size))\n    return res'"
classification/utils/logger.py,0,"b'# A simple torch style logger\n# (C) Wei YANG 2017\nfrom __future__ import absolute_import\nimport matplotlib.pyplot as plt\nimport os\nimport sys\nimport numpy as np\n\n__all__ = [\'Logger\', \'LoggerMonitor\', \'savefig\']\n\ndef savefig(fname, dpi=None):\n    dpi = 150 if dpi == None else dpi\n    plt.savefig(fname, dpi=dpi)\n    \ndef plot_overlap(logger, names=None):\n    names = logger.names if names == None else names\n    numbers = logger.numbers\n    for _, name in enumerate(names):\n        x = np.arange(len(numbers[name]))\n        plt.plot(x, np.asarray(numbers[name]))\n    return [logger.title + \'(\' + name + \')\' for name in names]\n\nclass Logger(object):\n    \'\'\'Save training process to log file with simple plot function.\'\'\'\n    def __init__(self, fpath, title=None, resume=False): \n        self.file = None\n        self.resume = resume\n        self.title = \'\' if title == None else title\n        if fpath is not None:\n            if resume: \n                self.file = open(fpath, \'r\') \n                name = self.file.readline()\n                self.names = name.rstrip().split(\'\\t\')\n                self.numbers = {}\n                for _, name in enumerate(self.names):\n                    self.numbers[name] = []\n\n                for numbers in self.file:\n                    numbers = numbers.rstrip().split(\'\\t\')\n                    for i in range(0, len(numbers)):\n                        self.numbers[self.names[i]].append(numbers[i])\n                self.file.close()\n                self.file = open(fpath, \'a\')  \n            else:\n                self.file = open(fpath, \'w\')\n\n    def set_names(self, names):\n        if self.resume: \n            pass\n        # initialize numbers as empty list\n        self.numbers = {}\n        self.names = names\n        for _, name in enumerate(self.names):\n            self.file.write(name)\n            self.file.write(\'\\t\')\n            self.numbers[name] = []\n        self.file.write(\'\\n\')\n        self.file.flush()\n\n\n    def append(self, numbers):\n        assert len(self.names) == len(numbers), \'Numbers do not match names\'\n        for index, num in enumerate(numbers):\n            self.file.write(""{0:.6f}"".format(num))\n            self.file.write(\'\\t\')\n            self.numbers[self.names[index]].append(num)\n        self.file.write(\'\\n\')\n        self.file.flush()\n\n    def plot(self, names=None):   \n        names = self.names if names == None else names\n        numbers = self.numbers\n        for _, name in enumerate(names):\n            x = np.arange(len(numbers[name]))\n            plt.plot(x, np.asarray(numbers[name]))\n        plt.legend([self.title + \'(\' + name + \')\' for name in names])\n        plt.grid(True)\n\n    def close(self):\n        if self.file is not None:\n            self.file.close()\n\nclass LoggerMonitor(object):\n    \'\'\'Load and visualize multiple logs.\'\'\'\n    def __init__ (self, paths):\n        \'\'\'paths is a distionary with {name:filepath} pair\'\'\'\n        self.loggers = []\n        for title, path in paths.items():\n            logger = Logger(path, title=title, resume=True)\n            self.loggers.append(logger)\n\n    def plot(self, names=None):\n        plt.figure()\n        plt.subplot(121)\n        legend_text = []\n        for logger in self.loggers:\n            legend_text += plot_overlap(logger, names)\n        plt.legend(legend_text, bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n        plt.grid(True)\n                    \nif __name__ == \'__main__\':\n    # # Example\n    # logger = Logger(\'test.txt\')\n    # logger.set_names([\'Train loss\', \'Valid loss\',\'Test loss\'])\n\n    # length = 100\n    # t = np.arange(length)\n    # train_loss = np.exp(-t / 10.0) + np.random.rand(length) * 0.1\n    # valid_loss = np.exp(-t / 10.0) + np.random.rand(length) * 0.1\n    # test_loss = np.exp(-t / 10.0) + np.random.rand(length) * 0.1\n\n    # for i in range(0, length):\n    #     logger.append([train_loss[i], valid_loss[i], test_loss[i]])\n    # logger.plot()\n\n    # Example: logger monitor\n    paths = {\n    \'resadvnet20\':\'/home/wyang/code/pytorch-classification/checkpoint/cifar10/resadvnet20/log.txt\', \n    \'resadvnet32\':\'/home/wyang/code/pytorch-classification/checkpoint/cifar10/resadvnet32/log.txt\',\n    \'resadvnet44\':\'/home/wyang/code/pytorch-classification/checkpoint/cifar10/resadvnet44/log.txt\',\n    }\n\n    field = [\'Valid Acc.\']\n\n    monitor = LoggerMonitor(paths)\n    monitor.plot(names=field)\n    savefig(\'test.eps\')'"
classification/utils/misc.py,6,"b'\'\'\'Some helper functions for PyTorch, including:\n    - get_mean_and_std: calculate the mean and std value of dataset.\n    - msr_init: net parameter initialization.\n    - progress_bar: progress bar mimic xlua.progress.\n\'\'\'\nimport errno\nimport os\nimport sys\nimport time\nimport math\n\nimport torch.nn as nn\nimport torch.nn.init as init\nfrom torch.autograd import Variable\n\n__all__ = [\'get_mean_and_std\', \'init_params\', \'mkdir_p\', \'AverageMeter\']\n\n\ndef get_mean_and_std(dataset):\n    \'\'\'Compute the mean and std value of dataset.\'\'\'\n    dataloader = trainloader = torch.utils.data.DataLoader(dataset, batch_size=1, shuffle=True, num_workers=2)\n\n    mean = torch.zeros(3)\n    std = torch.zeros(3)\n    print(\'==> Computing mean and std..\')\n    for inputs, targets in dataloader:\n        for i in range(3):\n            mean[i] += inputs[:,i,:,:].mean()\n            std[i] += inputs[:,i,:,:].std()\n    mean.div_(len(dataset))\n    std.div_(len(dataset))\n    return mean, std\n\ndef init_params(net):\n    \'\'\'Init layer parameters.\'\'\'\n    for m in net.modules():\n        if isinstance(m, nn.Conv2d):\n            init.kaiming_normal(m.weight, mode=\'fan_out\')\n            if m.bias:\n                init.constant(m.bias, 0)\n        elif isinstance(m, nn.BatchNorm2d):\n            init.constant(m.weight, 1)\n            init.constant(m.bias, 0)\n        elif isinstance(m, nn.Linear):\n            init.normal(m.weight, std=1e-3)\n            if m.bias:\n                init.constant(m.bias, 0)\n\ndef mkdir_p(path):\n    \'\'\'make dir if not exist\'\'\'\n    try:\n        os.makedirs(path)\n    except OSError as exc:  # Python >2.5\n        if exc.errno == errno.EEXIST and os.path.isdir(path):\n            pass\n        else:\n            raise\n\nclass AverageMeter(object):\n    """"""Computes and stores the average and current value\n       Imported from https://github.com/pytorch/examples/blob/master/imagenet/main.py#L247-L262\n    """"""\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count'"
classification/utils/visualize.py,6,"b""import matplotlib.pyplot as plt\nimport torch\nimport torch.nn as nn\nimport torchvision\nimport torchvision.transforms as transforms\nimport numpy as np\nfrom .misc import *   \n\n__all__ = ['make_image', 'show_batch', 'show_mask', 'show_mask_single']\n\n# functions to show an image\ndef make_image(img, mean=(0,0,0), std=(1,1,1)):\n    for i in range(0, 3):\n        img[i] = img[i] * std[i] + mean[i]    # unnormalize\n    npimg = img.numpy()\n    return np.transpose(npimg, (1, 2, 0))\n\ndef gauss(x,a,b,c):\n    return torch.exp(-torch.pow(torch.add(x,-b),2).div(2*c*c)).mul(a)\n\ndef colorize(x):\n    ''' Converts a one-channel grayscale image to a color heatmap image '''\n    if x.dim() == 2:\n        torch.unsqueeze(x, 0, out=x)\n    if x.dim() == 3:\n        cl = torch.zeros([3, x.size(1), x.size(2)])\n        cl[0] = gauss(x,.5,.6,.2) + gauss(x,1,.8,.3)\n        cl[1] = gauss(x,1,.5,.3)\n        cl[2] = gauss(x,1,.2,.3)\n        cl[cl.gt(1)] = 1\n    elif x.dim() == 4:\n        cl = torch.zeros([x.size(0), 3, x.size(2), x.size(3)])\n        cl[:,0,:,:] = gauss(x,.5,.6,.2) + gauss(x,1,.8,.3)\n        cl[:,1,:,:] = gauss(x,1,.5,.3)\n        cl[:,2,:,:] = gauss(x,1,.2,.3)\n    return cl\n\ndef show_batch(images, Mean=(2, 2, 2), Std=(0.5,0.5,0.5)):\n    images = make_image(torchvision.utils.make_grid(images), Mean, Std)\n    plt.imshow(images)\n    plt.show()\n\n\ndef show_mask_single(images, mask, Mean=(2, 2, 2), Std=(0.5,0.5,0.5)):\n    im_size = images.size(2)\n\n    # save for adding mask\n    im_data = images.clone()\n    for i in range(0, 3):\n        im_data[:,i,:,:] = im_data[:,i,:,:] * Std[i] + Mean[i]    # unnormalize\n\n    images = make_image(torchvision.utils.make_grid(images), Mean, Std)\n    plt.subplot(2, 1, 1)\n    plt.imshow(images)\n    plt.axis('off')\n\n    # for b in range(mask.size(0)):\n    #     mask[b] = (mask[b] - mask[b].min())/(mask[b].max() - mask[b].min())\n    mask_size = mask.size(2)\n    # print('Max %f Min %f' % (mask.max(), mask.min()))\n    mask = (upsampling(mask, scale_factor=im_size/mask_size))\n    # mask = colorize(upsampling(mask, scale_factor=im_size/mask_size))\n    # for c in range(3):\n    #     mask[:,c,:,:] = (mask[:,c,:,:] - Mean[c])/Std[c]\n\n    # print(mask.size())\n    mask = make_image(torchvision.utils.make_grid(0.3*im_data+0.7*mask.expand_as(im_data)))\n    # mask = make_image(torchvision.utils.make_grid(0.3*im_data+0.7*mask), Mean, Std)\n    plt.subplot(2, 1, 2)\n    plt.imshow(mask)\n    plt.axis('off')\n\ndef show_mask(images, masklist, Mean=(2, 2, 2), Std=(0.5,0.5,0.5)):\n    im_size = images.size(2)\n\n    # save for adding mask\n    im_data = images.clone()\n    for i in range(0, 3):\n        im_data[:,i,:,:] = im_data[:,i,:,:] * Std[i] + Mean[i]    # unnormalize\n\n    images = make_image(torchvision.utils.make_grid(images), Mean, Std)\n    plt.subplot(1+len(masklist), 1, 1)\n    plt.imshow(images)\n    plt.axis('off')\n\n    for i in range(len(masklist)):\n        mask = masklist[i].data.cpu()\n        # for b in range(mask.size(0)):\n        #     mask[b] = (mask[b] - mask[b].min())/(mask[b].max() - mask[b].min())\n        mask_size = mask.size(2)\n        # print('Max %f Min %f' % (mask.max(), mask.min()))\n        mask = (upsampling(mask, scale_factor=im_size/mask_size))\n        # mask = colorize(upsampling(mask, scale_factor=im_size/mask_size))\n        # for c in range(3):\n        #     mask[:,c,:,:] = (mask[:,c,:,:] - Mean[c])/Std[c]\n\n        # print(mask.size())\n        mask = make_image(torchvision.utils.make_grid(0.3*im_data+0.7*mask.expand_as(im_data)))\n        # mask = make_image(torchvision.utils.make_grid(0.3*im_data+0.7*mask), Mean, Std)\n        plt.subplot(1+len(masklist), 1, i+2)\n        plt.imshow(mask)\n        plt.axis('off')\n\n\n\n# x = torch.zeros(1, 3, 3)\n# out = colorize(x)\n# out_im = make_image(out)\n# plt.imshow(out_im)\n# plt.show()"""
detection/local_configs/cascade_rcnn_r101_fpn_20e.py,0,"b""# model settings\nmodel = dict(\n    type='CascadeRCNN',\n    num_stages=3,\n    pretrained='modelzoo://resnet101',\n    backbone=dict(\n        type='ResNet',\n        depth=101,\n        num_stages=4,\n        out_indices=(0, 1, 2, 3),\n        frozen_stages=1,\n        style='pytorch'),\n    neck=dict(\n        type='FPN',\n        in_channels=[256, 512, 1024, 2048],\n        out_channels=256,\n        num_outs=5),\n    rpn_head=dict(\n        type='RPNHead',\n        in_channels=256,\n        feat_channels=256,\n        anchor_scales=[8],\n        anchor_ratios=[0.5, 1.0, 2.0],\n        anchor_strides=[4, 8, 16, 32, 64],\n        target_means=[.0, .0, .0, .0],\n        target_stds=[1.0, 1.0, 1.0, 1.0],\n        use_sigmoid_cls=True),\n    bbox_roi_extractor=dict(\n        type='SingleRoIExtractor',\n        roi_layer=dict(type='RoIAlign', out_size=7, sample_num=2),\n        out_channels=256,\n        featmap_strides=[4, 8, 16, 32]),\n    bbox_head=[\n        dict(\n            type='SharedFCBBoxHead',\n            num_fcs=2,\n            in_channels=256,\n            fc_out_channels=1024,\n            roi_feat_size=7,\n            num_classes=81,\n            target_means=[0., 0., 0., 0.],\n            target_stds=[0.1, 0.1, 0.2, 0.2],\n            reg_class_agnostic=True),\n        dict(\n            type='SharedFCBBoxHead',\n            num_fcs=2,\n            in_channels=256,\n            fc_out_channels=1024,\n            roi_feat_size=7,\n            num_classes=81,\n            target_means=[0., 0., 0., 0.],\n            target_stds=[0.05, 0.05, 0.1, 0.1],\n            reg_class_agnostic=True),\n        dict(\n            type='SharedFCBBoxHead',\n            num_fcs=2,\n            in_channels=256,\n            fc_out_channels=1024,\n            roi_feat_size=7,\n            num_classes=81,\n            target_means=[0., 0., 0., 0.],\n            target_stds=[0.033, 0.033, 0.067, 0.067],\n            reg_class_agnostic=True)\n    ])\n# model training and testing settings\ntrain_cfg = dict(\n    rpn=dict(\n        assigner=dict(\n            type='MaxIoUAssigner',\n            pos_iou_thr=0.7,\n            neg_iou_thr=0.3,\n            min_pos_iou=0.3,\n            ignore_iof_thr=-1),\n        sampler=dict(\n            type='RandomSampler',\n            num=256,\n            pos_fraction=0.5,\n            neg_pos_ub=-1,\n            add_gt_as_proposals=False),\n        allowed_border=0,\n        pos_weight=-1,\n        smoothl1_beta=1 / 9.0,\n        debug=False),\n    rcnn=[\n        dict(\n            assigner=dict(\n                type='MaxIoUAssigner',\n                pos_iou_thr=0.5,\n                neg_iou_thr=0.5,\n                min_pos_iou=0.5,\n                ignore_iof_thr=-1),\n            sampler=dict(\n                type='RandomSampler',\n                num=512,\n                pos_fraction=0.25,\n                neg_pos_ub=-1,\n                add_gt_as_proposals=True),\n            pos_weight=-1,\n            debug=False),\n        dict(\n            assigner=dict(\n                type='MaxIoUAssigner',\n                pos_iou_thr=0.6,\n                neg_iou_thr=0.6,\n                min_pos_iou=0.6,\n                ignore_iof_thr=-1),\n            sampler=dict(\n                type='RandomSampler',\n                num=512,\n                pos_fraction=0.25,\n                neg_pos_ub=-1,\n                add_gt_as_proposals=True),\n            pos_weight=-1,\n            debug=False),\n        dict(\n            assigner=dict(\n                type='MaxIoUAssigner',\n                pos_iou_thr=0.7,\n                neg_iou_thr=0.7,\n                min_pos_iou=0.7,\n                ignore_iof_thr=-1),\n            sampler=dict(\n                type='RandomSampler',\n                num=512,\n                pos_fraction=0.25,\n                neg_pos_ub=-1,\n                add_gt_as_proposals=True),\n            pos_weight=-1,\n            debug=False)\n    ],\n    stage_loss_weights=[1, 0.5, 0.25])\ntest_cfg = dict(\n    rpn=dict(\n        nms_across_levels=False,\n        nms_pre=2000,\n        nms_post=2000,\n        max_num=2000,\n        nms_thr=0.7,\n        min_bbox_size=0),\n    rcnn=dict(\n        score_thr=0.05, nms=dict(type='nms', iou_thr=0.5), max_per_img=100),\n    keep_all_stages=False)\n# dataset settings\ndataset_type = 'CocoDataset'\ndata_root = 'data/coco/'\nimg_norm_cfg = dict(\n    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)\ndata = dict(\n    imgs_per_gpu=2,\n    workers_per_gpu=2,\n    train=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_train2017.json',\n        img_prefix=data_root + 'train2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0.5,\n        with_mask=False,\n        with_crowd=True,\n        with_label=True),\n    val=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_val2017.json',\n        img_prefix=data_root + 'val2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0,\n        with_mask=False,\n        with_crowd=True,\n        with_label=True),\n    test=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_val2017.json',\n        img_prefix=data_root + 'val2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0,\n        with_mask=False,\n        with_label=False,\n        test_mode=True))\n# optimizer\noptimizer = dict(type='SGD', lr=0.02, momentum=0.9, weight_decay=0.0001)\noptimizer_config = dict(grad_clip=dict(max_norm=35, norm_type=2))\n# learning policy\nlr_config = dict(\n    policy='step',\n    warmup='linear',\n    warmup_iters=500,\n    warmup_ratio=1.0 / 3,\n    step=[16, 19])\ncheckpoint_config = dict(interval=1)\n# yapf:disable\nlog_config = dict(\n    interval=50,\n    hooks=[\n        dict(type='TextLoggerHook'),\n        # dict(type='TensorboardLoggerHook')\n    ])\n# yapf:enable\n# runtime settings\ntotal_epochs = 20\ndist_params = dict(backend='nccl')\nlog_level = 'INFO'\nwork_dir = './work_dirs/cascade_rcnn_r101_fpn_20e'\nload_from = None\nresume_from = None\nworkflow = [('train', 1)]\n"""
detection/local_configs/cascade_rcnn_r101_fpn_20e_pretrain_sge_resnet101.py,0,"b""# model settings\nmodel = dict(\n    type='CascadeRCNN',\n    num_stages=3,\n    pretrained='/share1/xiangli/pytorch-classification/checkpoints/imagenet/sge_resnet101/model_best.pth.tar',\n    backbone=dict(\n        type='ResNetSGE',\n        depth=101,\n        num_stages=4,\n        out_indices=(0, 1, 2, 3),\n        frozen_stages=1,\n        style='pytorch'),\n    neck=dict(\n        type='FPN',\n        in_channels=[256, 512, 1024, 2048],\n        out_channels=256,\n        num_outs=5),\n    rpn_head=dict(\n        type='RPNHead',\n        in_channels=256,\n        feat_channels=256,\n        anchor_scales=[8],\n        anchor_ratios=[0.5, 1.0, 2.0],\n        anchor_strides=[4, 8, 16, 32, 64],\n        target_means=[.0, .0, .0, .0],\n        target_stds=[1.0, 1.0, 1.0, 1.0],\n        use_sigmoid_cls=True),\n    bbox_roi_extractor=dict(\n        type='SingleRoIExtractor',\n        roi_layer=dict(type='RoIAlign', out_size=7, sample_num=2),\n        out_channels=256,\n        featmap_strides=[4, 8, 16, 32]),\n    bbox_head=[\n        dict(\n            type='SharedFCBBoxHead',\n            num_fcs=2,\n            in_channels=256,\n            fc_out_channels=1024,\n            roi_feat_size=7,\n            num_classes=81,\n            target_means=[0., 0., 0., 0.],\n            target_stds=[0.1, 0.1, 0.2, 0.2],\n            reg_class_agnostic=True),\n        dict(\n            type='SharedFCBBoxHead',\n            num_fcs=2,\n            in_channels=256,\n            fc_out_channels=1024,\n            roi_feat_size=7,\n            num_classes=81,\n            target_means=[0., 0., 0., 0.],\n            target_stds=[0.05, 0.05, 0.1, 0.1],\n            reg_class_agnostic=True),\n        dict(\n            type='SharedFCBBoxHead',\n            num_fcs=2,\n            in_channels=256,\n            fc_out_channels=1024,\n            roi_feat_size=7,\n            num_classes=81,\n            target_means=[0., 0., 0., 0.],\n            target_stds=[0.033, 0.033, 0.067, 0.067],\n            reg_class_agnostic=True)\n    ])\n# model training and testing settings\ntrain_cfg = dict(\n    rpn=dict(\n        assigner=dict(\n            type='MaxIoUAssigner',\n            pos_iou_thr=0.7,\n            neg_iou_thr=0.3,\n            min_pos_iou=0.3,\n            ignore_iof_thr=-1),\n        sampler=dict(\n            type='RandomSampler',\n            num=256,\n            pos_fraction=0.5,\n            neg_pos_ub=-1,\n            add_gt_as_proposals=False),\n        allowed_border=0,\n        pos_weight=-1,\n        smoothl1_beta=1 / 9.0,\n        debug=False),\n    rcnn=[\n        dict(\n            assigner=dict(\n                type='MaxIoUAssigner',\n                pos_iou_thr=0.5,\n                neg_iou_thr=0.5,\n                min_pos_iou=0.5,\n                ignore_iof_thr=-1),\n            sampler=dict(\n                type='RandomSampler',\n                num=512,\n                pos_fraction=0.25,\n                neg_pos_ub=-1,\n                add_gt_as_proposals=True),\n            pos_weight=-1,\n            debug=False),\n        dict(\n            assigner=dict(\n                type='MaxIoUAssigner',\n                pos_iou_thr=0.6,\n                neg_iou_thr=0.6,\n                min_pos_iou=0.6,\n                ignore_iof_thr=-1),\n            sampler=dict(\n                type='RandomSampler',\n                num=512,\n                pos_fraction=0.25,\n                neg_pos_ub=-1,\n                add_gt_as_proposals=True),\n            pos_weight=-1,\n            debug=False),\n        dict(\n            assigner=dict(\n                type='MaxIoUAssigner',\n                pos_iou_thr=0.7,\n                neg_iou_thr=0.7,\n                min_pos_iou=0.7,\n                ignore_iof_thr=-1),\n            sampler=dict(\n                type='RandomSampler',\n                num=512,\n                pos_fraction=0.25,\n                neg_pos_ub=-1,\n                add_gt_as_proposals=True),\n            pos_weight=-1,\n            debug=False)\n    ],\n    stage_loss_weights=[1, 0.5, 0.25])\ntest_cfg = dict(\n    rpn=dict(\n        nms_across_levels=False,\n        nms_pre=2000,\n        nms_post=2000,\n        max_num=2000,\n        nms_thr=0.7,\n        min_bbox_size=0),\n    rcnn=dict(\n        score_thr=0.05, nms=dict(type='nms', iou_thr=0.5), max_per_img=100),\n    keep_all_stages=False)\n# dataset settings\ndataset_type = 'CocoDataset'\ndata_root = 'data/coco/'\nimg_norm_cfg = dict(\n    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)\ndata = dict(\n    imgs_per_gpu=1,\n    workers_per_gpu=2,\n    train=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_train2017.json',\n        img_prefix=data_root + 'train2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0.5,\n        with_mask=False,\n        with_crowd=True,\n        with_label=True),\n    val=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_val2017.json',\n        img_prefix=data_root + 'val2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0,\n        with_mask=False,\n        with_crowd=True,\n        with_label=True),\n    test=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_val2017.json',\n        img_prefix=data_root + 'val2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0,\n        with_mask=False,\n        with_label=False,\n        test_mode=True))\n# optimizer\noptimizer = dict(type='SGD', lr=0.01, momentum=0.9, weight_decay=0.0001)\noptimizer_config = dict(grad_clip=dict(max_norm=35, norm_type=2))\n# learning policy\nlr_config = dict(\n    policy='step',\n    warmup='linear',\n    warmup_iters=500,\n    warmup_ratio=1.0 / 3,\n    step=[16, 19])\ncheckpoint_config = dict(interval=1)\n# yapf:disable\nlog_config = dict(\n    interval=50,\n    hooks=[\n        dict(type='TextLoggerHook'),\n        # dict(type='TensorboardLoggerHook')\n    ])\n# yapf:enable\n# runtime settings\ntotal_epochs = 20\ndist_params = dict(backend='nccl')\nlog_level = 'INFO'\nwork_dir = './work_dirs/cascade_rcnn_r101_fpn_20e_pretrain_sge_resnet101'\nload_from = None\nresume_from = None\nworkflow = [('train', 1)]\n"""
detection/local_configs/cascade_rcnn_r50_fpn_20e.py,0,"b""# model settings\nmodel = dict(\n    type='CascadeRCNN',\n    num_stages=3,\n    pretrained='modelzoo://resnet50',\n    backbone=dict(\n        type='ResNet',\n        depth=50,\n        num_stages=4,\n        out_indices=(0, 1, 2, 3),\n        frozen_stages=1,\n        style='pytorch'),\n    neck=dict(\n        type='FPN',\n        in_channels=[256, 512, 1024, 2048],\n        out_channels=256,\n        num_outs=5),\n    rpn_head=dict(\n        type='RPNHead',\n        in_channels=256,\n        feat_channels=256,\n        anchor_scales=[8],\n        anchor_ratios=[0.5, 1.0, 2.0],\n        anchor_strides=[4, 8, 16, 32, 64],\n        target_means=[.0, .0, .0, .0],\n        target_stds=[1.0, 1.0, 1.0, 1.0],\n        use_sigmoid_cls=True),\n    bbox_roi_extractor=dict(\n        type='SingleRoIExtractor',\n        roi_layer=dict(type='RoIAlign', out_size=7, sample_num=2),\n        out_channels=256,\n        featmap_strides=[4, 8, 16, 32]),\n    bbox_head=[\n        dict(\n            type='SharedFCBBoxHead',\n            num_fcs=2,\n            in_channels=256,\n            fc_out_channels=1024,\n            roi_feat_size=7,\n            num_classes=81,\n            target_means=[0., 0., 0., 0.],\n            target_stds=[0.1, 0.1, 0.2, 0.2],\n            reg_class_agnostic=True),\n        dict(\n            type='SharedFCBBoxHead',\n            num_fcs=2,\n            in_channels=256,\n            fc_out_channels=1024,\n            roi_feat_size=7,\n            num_classes=81,\n            target_means=[0., 0., 0., 0.],\n            target_stds=[0.05, 0.05, 0.1, 0.1],\n            reg_class_agnostic=True),\n        dict(\n            type='SharedFCBBoxHead',\n            num_fcs=2,\n            in_channels=256,\n            fc_out_channels=1024,\n            roi_feat_size=7,\n            num_classes=81,\n            target_means=[0., 0., 0., 0.],\n            target_stds=[0.033, 0.033, 0.067, 0.067],\n            reg_class_agnostic=True)\n    ])\n# model training and testing settings\ntrain_cfg = dict(\n    rpn=dict(\n        assigner=dict(\n            type='MaxIoUAssigner',\n            pos_iou_thr=0.7,\n            neg_iou_thr=0.3,\n            min_pos_iou=0.3,\n            ignore_iof_thr=-1),\n        sampler=dict(\n            type='RandomSampler',\n            num=256,\n            pos_fraction=0.5,\n            neg_pos_ub=-1,\n            add_gt_as_proposals=False),\n        allowed_border=0,\n        pos_weight=-1,\n        smoothl1_beta=1 / 9.0,\n        debug=False),\n    rcnn=[\n        dict(\n            assigner=dict(\n                type='MaxIoUAssigner',\n                pos_iou_thr=0.5,\n                neg_iou_thr=0.5,\n                min_pos_iou=0.5,\n                ignore_iof_thr=-1),\n            sampler=dict(\n                type='RandomSampler',\n                num=512,\n                pos_fraction=0.25,\n                neg_pos_ub=-1,\n                add_gt_as_proposals=True),\n            pos_weight=-1,\n            debug=False),\n        dict(\n            assigner=dict(\n                type='MaxIoUAssigner',\n                pos_iou_thr=0.6,\n                neg_iou_thr=0.6,\n                min_pos_iou=0.6,\n                ignore_iof_thr=-1),\n            sampler=dict(\n                type='RandomSampler',\n                num=512,\n                pos_fraction=0.25,\n                neg_pos_ub=-1,\n                add_gt_as_proposals=True),\n            pos_weight=-1,\n            debug=False),\n        dict(\n            assigner=dict(\n                type='MaxIoUAssigner',\n                pos_iou_thr=0.7,\n                neg_iou_thr=0.7,\n                min_pos_iou=0.7,\n                ignore_iof_thr=-1),\n            sampler=dict(\n                type='RandomSampler',\n                num=512,\n                pos_fraction=0.25,\n                neg_pos_ub=-1,\n                add_gt_as_proposals=True),\n            pos_weight=-1,\n            debug=False)\n    ],\n    stage_loss_weights=[1, 0.5, 0.25])\ntest_cfg = dict(\n    rpn=dict(\n        nms_across_levels=False,\n        nms_pre=2000,\n        nms_post=2000,\n        max_num=2000,\n        nms_thr=0.7,\n        min_bbox_size=0),\n    rcnn=dict(\n        score_thr=0.05, nms=dict(type='nms', iou_thr=0.5), max_per_img=100),\n    keep_all_stages=False)\n# dataset settings\ndataset_type = 'CocoDataset'\ndata_root = 'data/coco/'\nimg_norm_cfg = dict(\n    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)\ndata = dict(\n    imgs_per_gpu=2,\n    workers_per_gpu=2,\n    train=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_train2017.json',\n        img_prefix=data_root + 'train2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0.5,\n        with_mask=False,\n        with_crowd=True,\n        with_label=True),\n    val=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_val2017.json',\n        img_prefix=data_root + 'val2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0,\n        with_mask=False,\n        with_crowd=True,\n        with_label=True),\n    test=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_val2017.json',\n        img_prefix=data_root + 'val2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0,\n        with_mask=False,\n        with_label=False,\n        test_mode=True))\n# optimizer\noptimizer = dict(type='SGD', lr=0.02, momentum=0.9, weight_decay=0.0001)\noptimizer_config = dict(grad_clip=dict(max_norm=35, norm_type=2))\n# learning policy\nlr_config = dict(\n    policy='step',\n    warmup='linear',\n    warmup_iters=500,\n    warmup_ratio=1.0 / 3,\n    step=[16, 19])\ncheckpoint_config = dict(interval=1)\n# yapf:disable\nlog_config = dict(\n    interval=50,\n    hooks=[\n        dict(type='TextLoggerHook'),\n        # dict(type='TensorboardLoggerHook')\n    ])\n# yapf:enable\n# runtime settings\ntotal_epochs = 20\ndist_params = dict(backend='nccl')\nlog_level = 'INFO'\nwork_dir = './work_dirs/cascade_rcnn_r50_fpn_20e'\nload_from = None\nresume_from = None\nworkflow = [('train', 1)]\n"""
detection/local_configs/cascade_rcnn_r50_fpn_20e_pretrain_sge_resnet50.py,0,"b""# model settings\nmodel = dict(\n    type='CascadeRCNN',\n    num_stages=3,\n    #pretrained='modelzoo://resnet50',\n    pretrained='/home/xiangli/share1/pytorch-classification/checkpoints/imagenet/sge_resnet50/model_best.pth.tar',\n    backbone=dict(\n        type='ResNetSGE',\n        depth=50,\n        num_stages=4,\n        out_indices=(0, 1, 2, 3),\n        frozen_stages=1,\n        style='pytorch'),\n    neck=dict(\n        type='FPN',\n        in_channels=[256, 512, 1024, 2048],\n        out_channels=256,\n        num_outs=5),\n    rpn_head=dict(\n        type='RPNHead',\n        in_channels=256,\n        feat_channels=256,\n        anchor_scales=[8],\n        anchor_ratios=[0.5, 1.0, 2.0],\n        anchor_strides=[4, 8, 16, 32, 64],\n        target_means=[.0, .0, .0, .0],\n        target_stds=[1.0, 1.0, 1.0, 1.0],\n        use_sigmoid_cls=True),\n    bbox_roi_extractor=dict(\n        type='SingleRoIExtractor',\n        roi_layer=dict(type='RoIAlign', out_size=7, sample_num=2),\n        out_channels=256,\n        featmap_strides=[4, 8, 16, 32]),\n    bbox_head=[\n        dict(\n            type='SharedFCBBoxHead',\n            num_fcs=2,\n            in_channels=256,\n            fc_out_channels=1024,\n            roi_feat_size=7,\n            num_classes=81,\n            target_means=[0., 0., 0., 0.],\n            target_stds=[0.1, 0.1, 0.2, 0.2],\n            reg_class_agnostic=True),\n        dict(\n            type='SharedFCBBoxHead',\n            num_fcs=2,\n            in_channels=256,\n            fc_out_channels=1024,\n            roi_feat_size=7,\n            num_classes=81,\n            target_means=[0., 0., 0., 0.],\n            target_stds=[0.05, 0.05, 0.1, 0.1],\n            reg_class_agnostic=True),\n        dict(\n            type='SharedFCBBoxHead',\n            num_fcs=2,\n            in_channels=256,\n            fc_out_channels=1024,\n            roi_feat_size=7,\n            num_classes=81,\n            target_means=[0., 0., 0., 0.],\n            target_stds=[0.033, 0.033, 0.067, 0.067],\n            reg_class_agnostic=True)\n    ])\n# model training and testing settings\ntrain_cfg = dict(\n    rpn=dict(\n        assigner=dict(\n            type='MaxIoUAssigner',\n            pos_iou_thr=0.7,\n            neg_iou_thr=0.3,\n            min_pos_iou=0.3,\n            ignore_iof_thr=-1),\n        sampler=dict(\n            type='RandomSampler',\n            num=256,\n            pos_fraction=0.5,\n            neg_pos_ub=-1,\n            add_gt_as_proposals=False),\n        allowed_border=0,\n        pos_weight=-1,\n        smoothl1_beta=1 / 9.0,\n        debug=False),\n    rcnn=[\n        dict(\n            assigner=dict(\n                type='MaxIoUAssigner',\n                pos_iou_thr=0.5,\n                neg_iou_thr=0.5,\n                min_pos_iou=0.5,\n                ignore_iof_thr=-1),\n            sampler=dict(\n                type='RandomSampler',\n                num=512,\n                pos_fraction=0.25,\n                neg_pos_ub=-1,\n                add_gt_as_proposals=True),\n            pos_weight=-1,\n            debug=False),\n        dict(\n            assigner=dict(\n                type='MaxIoUAssigner',\n                pos_iou_thr=0.6,\n                neg_iou_thr=0.6,\n                min_pos_iou=0.6,\n                ignore_iof_thr=-1),\n            sampler=dict(\n                type='RandomSampler',\n                num=512,\n                pos_fraction=0.25,\n                neg_pos_ub=-1,\n                add_gt_as_proposals=True),\n            pos_weight=-1,\n            debug=False),\n        dict(\n            assigner=dict(\n                type='MaxIoUAssigner',\n                pos_iou_thr=0.7,\n                neg_iou_thr=0.7,\n                min_pos_iou=0.7,\n                ignore_iof_thr=-1),\n            sampler=dict(\n                type='RandomSampler',\n                num=512,\n                pos_fraction=0.25,\n                neg_pos_ub=-1,\n                add_gt_as_proposals=True),\n            pos_weight=-1,\n            debug=False)\n    ],\n    stage_loss_weights=[1, 0.5, 0.25])\ntest_cfg = dict(\n    rpn=dict(\n        nms_across_levels=False,\n        nms_pre=2000,\n        nms_post=2000,\n        max_num=2000,\n        nms_thr=0.7,\n        min_bbox_size=0),\n    rcnn=dict(\n        score_thr=0.05, nms=dict(type='nms', iou_thr=0.5), max_per_img=100),\n    keep_all_stages=False)\n# dataset settings\ndataset_type = 'CocoDataset'\ndata_root = 'data/coco/'\nimg_norm_cfg = dict(\n    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)\ndata = dict(\n    imgs_per_gpu=2,\n    workers_per_gpu=2,\n    train=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_train2017.json',\n        img_prefix=data_root + 'train2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0.5,\n        with_mask=False,\n        with_crowd=True,\n        with_label=True),\n    val=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_val2017.json',\n        img_prefix=data_root + 'val2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0,\n        with_mask=False,\n        with_crowd=True,\n        with_label=True),\n    test=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_val2017.json',\n        img_prefix=data_root + 'val2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0,\n        with_mask=False,\n        with_label=False,\n        test_mode=True))\n# optimizer\noptimizer = dict(type='SGD', lr=0.02, momentum=0.9, weight_decay=0.0001)\noptimizer_config = dict(grad_clip=dict(max_norm=35, norm_type=2))\n# learning policy\nlr_config = dict(\n    policy='step',\n    warmup='linear',\n    warmup_iters=500,\n    warmup_ratio=1.0 / 3,\n    step=[16, 19])\ncheckpoint_config = dict(interval=1)\n# yapf:disable\nlog_config = dict(\n    interval=50,\n    hooks=[\n        dict(type='TextLoggerHook'),\n        # dict(type='TensorboardLoggerHook')\n    ])\n# yapf:enable\n# runtime settings\ntotal_epochs = 20\ndist_params = dict(backend='nccl')\nlog_level = 'INFO'\nwork_dir = './work_dirs/cascade_rcnn_r50_fpn_20e_pretrain_sge_resnet50'\nload_from = None\nresume_from = None\nworkflow = [('train', 1)]\n"""
detection/local_configs/faster_rcnn_r101_fpn_2x.py,0,"b""# model settings\nmodel = dict(\n    type='FasterRCNN',\n    pretrained='modelzoo://resnet101',\n    backbone=dict(\n        type='ResNet',\n        depth=101,\n        num_stages=4,\n        out_indices=(0, 1, 2, 3),\n        frozen_stages=1,\n        style='pytorch'),\n    neck=dict(\n        type='FPN',\n        in_channels=[256, 512, 1024, 2048],\n        out_channels=256,\n        num_outs=5),\n    rpn_head=dict(\n        type='RPNHead',\n        in_channels=256,\n        feat_channels=256,\n        anchor_scales=[8],\n        anchor_ratios=[0.5, 1.0, 2.0],\n        anchor_strides=[4, 8, 16, 32, 64],\n        target_means=[.0, .0, .0, .0],\n        target_stds=[1.0, 1.0, 1.0, 1.0],\n        use_sigmoid_cls=True),\n    bbox_roi_extractor=dict(\n        type='SingleRoIExtractor',\n        roi_layer=dict(type='RoIAlign', out_size=7, sample_num=2),\n        out_channels=256,\n        featmap_strides=[4, 8, 16, 32]),\n    bbox_head=dict(\n        type='SharedFCBBoxHead',\n        num_fcs=2,\n        in_channels=256,\n        fc_out_channels=1024,\n        roi_feat_size=7,\n        num_classes=81,\n        target_means=[0., 0., 0., 0.],\n        target_stds=[0.1, 0.1, 0.2, 0.2],\n        reg_class_agnostic=False))\n# model training and testing settings\ntrain_cfg = dict(\n    rpn=dict(\n        assigner=dict(\n            type='MaxIoUAssigner',\n            pos_iou_thr=0.7,\n            neg_iou_thr=0.3,\n            min_pos_iou=0.3,\n            ignore_iof_thr=-1),\n        sampler=dict(\n            type='RandomSampler',\n            num=256,\n            pos_fraction=0.5,\n            neg_pos_ub=-1,\n            add_gt_as_proposals=False),\n        allowed_border=0,\n        pos_weight=-1,\n        smoothl1_beta=1 / 9.0,\n        debug=False),\n    rcnn=dict(\n        assigner=dict(\n            type='MaxIoUAssigner',\n            pos_iou_thr=0.5,\n            neg_iou_thr=0.5,\n            min_pos_iou=0.5,\n            ignore_iof_thr=-1),\n        sampler=dict(\n            type='RandomSampler',\n            num=512,\n            pos_fraction=0.25,\n            neg_pos_ub=-1,\n            add_gt_as_proposals=True),\n        pos_weight=-1,\n        debug=False))\ntest_cfg = dict(\n    rpn=dict(\n        nms_across_levels=False,\n        nms_pre=2000,\n        nms_post=2000,\n        max_num=2000,\n        nms_thr=0.7,\n        min_bbox_size=0),\n    rcnn=dict(\n        score_thr=0.05, nms=dict(type='nms', iou_thr=0.5), max_per_img=100)\n    # soft-nms is also supported for rcnn testing\n    # e.g., nms=dict(type='soft_nms', iou_thr=0.5, min_score=0.05)\n)\n# dataset settings\ndataset_type = 'CocoDataset'\ndata_root = 'data/coco/'\nimg_norm_cfg = dict(\n    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)\ndata = dict(\n    imgs_per_gpu=2,\n    workers_per_gpu=2,\n    train=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_train2017.json',\n        img_prefix=data_root + 'train2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0.5,\n        with_mask=False,\n        with_crowd=True,\n        with_label=True),\n    val=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_val2017.json',\n        img_prefix=data_root + 'val2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0,\n        with_mask=False,\n        with_crowd=True,\n        with_label=True),\n    test=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_val2017.json',\n        img_prefix=data_root + 'val2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0,\n        with_mask=False,\n        with_label=False,\n        test_mode=True))\n# optimizer\noptimizer = dict(type='SGD', lr=0.02, momentum=0.9, weight_decay=0.0001)\noptimizer_config = dict(grad_clip=dict(max_norm=35, norm_type=2))\n# learning policy\nlr_config = dict(\n    policy='step',\n    warmup='linear',\n    warmup_iters=500,\n    warmup_ratio=1.0 / 3,\n    step=[16, 22])\ncheckpoint_config = dict(interval=1)\n# yapf:disable\nlog_config = dict(\n    interval=50,\n    hooks=[\n        dict(type='TextLoggerHook'),\n        # dict(type='TensorboardLoggerHook')\n    ])\n# yapf:enable\n# runtime settings\ntotal_epochs = 24\ndist_params = dict(backend='nccl')\nlog_level = 'INFO'\nwork_dir = './work_dirs/faster_rcnn_r101_fpn_2x'\nload_from = None\nresume_from = None\nworkflow = [('train', 1)]\n"""
detection/local_configs/faster_rcnn_r101_fpn_2x_pretrain_sge_resnet101.py,0,"b""# model settings\nmodel = dict(\n    type='FasterRCNN',\n    #pretrained='modelzoo://resnet101',\n    pretrained='/share1/xiangli/pytorch-classification/checkpoints/imagenet/sge_resnet101/model_best.pth.tar',\n    backbone=dict(\n        type='ResNetSGE',\n        depth=101,\n        num_stages=4,\n        out_indices=(0, 1, 2, 3),\n        frozen_stages=1,\n        style='pytorch'),\n    neck=dict(\n        type='FPN',\n        in_channels=[256, 512, 1024, 2048],\n        out_channels=256,\n        num_outs=5),\n    rpn_head=dict(\n        type='RPNHead',\n        in_channels=256,\n        feat_channels=256,\n        anchor_scales=[8],\n        anchor_ratios=[0.5, 1.0, 2.0],\n        anchor_strides=[4, 8, 16, 32, 64],\n        target_means=[.0, .0, .0, .0],\n        target_stds=[1.0, 1.0, 1.0, 1.0],\n        use_sigmoid_cls=True),\n    bbox_roi_extractor=dict(\n        type='SingleRoIExtractor',\n        roi_layer=dict(type='RoIAlign', out_size=7, sample_num=2),\n        out_channels=256,\n        featmap_strides=[4, 8, 16, 32]),\n    bbox_head=dict(\n        type='SharedFCBBoxHead',\n        num_fcs=2,\n        in_channels=256,\n        fc_out_channels=1024,\n        roi_feat_size=7,\n        num_classes=81,\n        target_means=[0., 0., 0., 0.],\n        target_stds=[0.1, 0.1, 0.2, 0.2],\n        reg_class_agnostic=False))\n# model training and testing settings\ntrain_cfg = dict(\n    rpn=dict(\n        assigner=dict(\n            type='MaxIoUAssigner',\n            pos_iou_thr=0.7,\n            neg_iou_thr=0.3,\n            min_pos_iou=0.3,\n            ignore_iof_thr=-1),\n        sampler=dict(\n            type='RandomSampler',\n            num=256,\n            pos_fraction=0.5,\n            neg_pos_ub=-1,\n            add_gt_as_proposals=False),\n        allowed_border=0,\n        pos_weight=-1,\n        smoothl1_beta=1 / 9.0,\n        debug=False),\n    rcnn=dict(\n        assigner=dict(\n            type='MaxIoUAssigner',\n            pos_iou_thr=0.5,\n            neg_iou_thr=0.5,\n            min_pos_iou=0.5,\n            ignore_iof_thr=-1),\n        sampler=dict(\n            type='RandomSampler',\n            num=512,\n            pos_fraction=0.25,\n            neg_pos_ub=-1,\n            add_gt_as_proposals=True),\n        pos_weight=-1,\n        debug=False))\ntest_cfg = dict(\n    rpn=dict(\n        nms_across_levels=False,\n        nms_pre=2000,\n        nms_post=2000,\n        max_num=2000,\n        nms_thr=0.7,\n        min_bbox_size=0),\n    rcnn=dict(\n        score_thr=0.05, nms=dict(type='nms', iou_thr=0.5), max_per_img=100)\n    # soft-nms is also supported for rcnn testing\n    # e.g., nms=dict(type='soft_nms', iou_thr=0.5, min_score=0.05)\n)\n# dataset settings\ndataset_type = 'CocoDataset'\ndata_root = 'data/coco/'\nimg_norm_cfg = dict(\n    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)\ndata = dict(\n    imgs_per_gpu=1,\n    workers_per_gpu=2,\n    train=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_train2017.json',\n        img_prefix=data_root + 'train2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0.5,\n        with_mask=False,\n        with_crowd=True,\n        with_label=True),\n    val=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_val2017.json',\n        img_prefix=data_root + 'val2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0,\n        with_mask=False,\n        with_crowd=True,\n        with_label=True),\n    test=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_val2017.json',\n        img_prefix=data_root + 'val2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0,\n        with_mask=False,\n        with_label=False,\n        test_mode=True))\n# optimizer\noptimizer = dict(type='SGD', lr=0.01, momentum=0.9, weight_decay=0.0001)\noptimizer_config = dict(grad_clip=dict(max_norm=35, norm_type=2))\n# learning policy\nlr_config = dict(\n    policy='step',\n    warmup='linear',\n    warmup_iters=500,\n    warmup_ratio=1.0 / 3,\n    step=[16, 22])\ncheckpoint_config = dict(interval=1)\n# yapf:disable\nlog_config = dict(\n    interval=50,\n    hooks=[\n        dict(type='TextLoggerHook'),\n        # dict(type='TensorboardLoggerHook')\n    ])\n# yapf:enable\n# runtime settings\ntotal_epochs = 24\ndist_params = dict(backend='nccl')\nlog_level = 'INFO'\nwork_dir = './work_dirs/faster_rcnn_r101_fpn_2x_pretrain_sge_resnet101'\nload_from = None\nresume_from = None\nworkflow = [('train', 1)]\n"""
detection/local_configs/faster_rcnn_r50_fpn_2x.py,0,"b""# model settings\nmodel = dict(\n    type='FasterRCNN',\n    pretrained='modelzoo://resnet50',\n    backbone=dict(\n        type='ResNet',\n        depth=50,\n        num_stages=4,\n        out_indices=(0, 1, 2, 3),\n        frozen_stages=1,\n        style='pytorch'),\n    neck=dict(\n        type='FPN',\n        in_channels=[256, 512, 1024, 2048],\n        out_channels=256,\n        num_outs=5),\n    rpn_head=dict(\n        type='RPNHead',\n        in_channels=256,\n        feat_channels=256,\n        anchor_scales=[8],\n        anchor_ratios=[0.5, 1.0, 2.0],\n        anchor_strides=[4, 8, 16, 32, 64],\n        target_means=[.0, .0, .0, .0],\n        target_stds=[1.0, 1.0, 1.0, 1.0],\n        use_sigmoid_cls=True),\n    bbox_roi_extractor=dict(\n        type='SingleRoIExtractor',\n        roi_layer=dict(type='RoIAlign', out_size=7, sample_num=2),\n        out_channels=256,\n        featmap_strides=[4, 8, 16, 32]),\n    bbox_head=dict(\n        type='SharedFCBBoxHead',\n        num_fcs=2,\n        in_channels=256,\n        fc_out_channels=1024,\n        roi_feat_size=7,\n        num_classes=81,\n        target_means=[0., 0., 0., 0.],\n        target_stds=[0.1, 0.1, 0.2, 0.2],\n        reg_class_agnostic=False))\n# model training and testing settings\ntrain_cfg = dict(\n    rpn=dict(\n        assigner=dict(\n            type='MaxIoUAssigner',\n            pos_iou_thr=0.7,\n            neg_iou_thr=0.3,\n            min_pos_iou=0.3,\n            ignore_iof_thr=-1),\n        sampler=dict(\n            type='RandomSampler',\n            num=256,\n            pos_fraction=0.5,\n            neg_pos_ub=-1,\n            add_gt_as_proposals=False),\n        allowed_border=0,\n        pos_weight=-1,\n        smoothl1_beta=1 / 9.0,\n        debug=False),\n    rcnn=dict(\n        assigner=dict(\n            type='MaxIoUAssigner',\n            pos_iou_thr=0.5,\n            neg_iou_thr=0.5,\n            min_pos_iou=0.5,\n            ignore_iof_thr=-1),\n        sampler=dict(\n            type='RandomSampler',\n            num=512,\n            pos_fraction=0.25,\n            neg_pos_ub=-1,\n            add_gt_as_proposals=True),\n        pos_weight=-1,\n        debug=False))\ntest_cfg = dict(\n    rpn=dict(\n        nms_across_levels=False,\n        nms_pre=2000,\n        nms_post=2000,\n        max_num=2000,\n        nms_thr=0.7,\n        min_bbox_size=0),\n    rcnn=dict(\n        score_thr=0.05, nms=dict(type='nms', iou_thr=0.5), max_per_img=100)\n    # soft-nms is also supported for rcnn testing\n    # e.g., nms=dict(type='soft_nms', iou_thr=0.5, min_score=0.05)\n)\n# dataset settings\ndataset_type = 'CocoDataset'\ndata_root = 'data/coco/'\nimg_norm_cfg = dict(\n    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)\ndata = dict(\n    imgs_per_gpu=2,\n    workers_per_gpu=2,\n    train=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_train2017.json',\n        img_prefix=data_root + 'train2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0.5,\n        with_mask=False,\n        with_crowd=True,\n        with_label=True),\n    val=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_val2017.json',\n        img_prefix=data_root + 'val2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0,\n        with_mask=False,\n        with_crowd=True,\n        with_label=True),\n    test=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_val2017.json',\n        img_prefix=data_root + 'val2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0,\n        with_mask=False,\n        with_label=False,\n        test_mode=True))\n# optimizer\noptimizer = dict(type='SGD', lr=0.02, momentum=0.9, weight_decay=0.0001)\noptimizer_config = dict(grad_clip=dict(max_norm=35, norm_type=2))\n# learning policy\nlr_config = dict(\n    policy='step',\n    warmup='linear',\n    warmup_iters=500,\n    warmup_ratio=1.0 / 3,\n    step=[16, 22])\ncheckpoint_config = dict(interval=1)\n# yapf:disable\nlog_config = dict(\n    interval=50,\n    hooks=[\n        dict(type='TextLoggerHook'),\n        # dict(type='TensorboardLoggerHook')\n    ])\n# yapf:enable\n# runtime settings\ntotal_epochs = 24\ndist_params = dict(backend='nccl')\nlog_level = 'INFO'\nwork_dir = './work_dirs/faster_rcnn_r50_fpn_2x'\nload_from = None\nresume_from = None\nworkflow = [('train', 1)]\n"""
detection/local_configs/faster_rcnn_r50_fpn_2x_pretrain_sge_resnet50.py,0,"b""# model settings\nmodel = dict(\n    type='FasterRCNN',\n    pretrained='/home/xiangli/share1/pytorch-classification/checkpoints/imagenet/sge_resnet50/model_best.pth.tar',\n    backbone=dict(\n        type='ResNetSGE',\n        depth=50,\n        num_stages=4,\n        out_indices=(0, 1, 2, 3),\n        frozen_stages=1,\n        style='pytorch'),\n    neck=dict(\n        type='FPN',\n        in_channels=[256, 512, 1024, 2048],\n        out_channels=256,\n        num_outs=5),\n    rpn_head=dict(\n        type='RPNHead',\n        in_channels=256,\n        feat_channels=256,\n        anchor_scales=[8],\n        anchor_ratios=[0.5, 1.0, 2.0],\n        anchor_strides=[4, 8, 16, 32, 64],\n        target_means=[.0, .0, .0, .0],\n        target_stds=[1.0, 1.0, 1.0, 1.0],\n        use_sigmoid_cls=True),\n    bbox_roi_extractor=dict(\n        type='SingleRoIExtractor',\n        roi_layer=dict(type='RoIAlign', out_size=7, sample_num=2),\n        out_channels=256,\n        featmap_strides=[4, 8, 16, 32]),\n    bbox_head=dict(\n        type='SharedFCBBoxHead',\n        num_fcs=2,\n        in_channels=256,\n        fc_out_channels=1024,\n        roi_feat_size=7,\n        num_classes=81,\n        target_means=[0., 0., 0., 0.],\n        target_stds=[0.1, 0.1, 0.2, 0.2],\n        reg_class_agnostic=False))\n# model training and testing settings\ntrain_cfg = dict(\n    rpn=dict(\n        assigner=dict(\n            type='MaxIoUAssigner',\n            pos_iou_thr=0.7,\n            neg_iou_thr=0.3,\n            min_pos_iou=0.3,\n            ignore_iof_thr=-1),\n        sampler=dict(\n            type='RandomSampler',\n            num=256,\n            pos_fraction=0.5,\n            neg_pos_ub=-1,\n            add_gt_as_proposals=False),\n        allowed_border=0,\n        pos_weight=-1,\n        smoothl1_beta=1 / 9.0,\n        debug=False),\n    rcnn=dict(\n        assigner=dict(\n            type='MaxIoUAssigner',\n            pos_iou_thr=0.5,\n            neg_iou_thr=0.5,\n            min_pos_iou=0.5,\n            ignore_iof_thr=-1),\n        sampler=dict(\n            type='RandomSampler',\n            num=512,\n            pos_fraction=0.25,\n            neg_pos_ub=-1,\n            add_gt_as_proposals=True),\n        pos_weight=-1,\n        debug=False))\ntest_cfg = dict(\n    rpn=dict(\n        nms_across_levels=False,\n        nms_pre=2000,\n        nms_post=2000,\n        max_num=2000,\n        nms_thr=0.7,\n        min_bbox_size=0),\n    rcnn=dict(\n        score_thr=0.05, nms=dict(type='nms', iou_thr=0.5), max_per_img=100)\n    # soft-nms is also supported for rcnn testing\n    # e.g., nms=dict(type='soft_nms', iou_thr=0.5, min_score=0.05)\n)\n# dataset settings\ndataset_type = 'CocoDataset'\ndata_root = 'data/coco/'\nimg_norm_cfg = dict(\n    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)\ndata = dict(\n    imgs_per_gpu=2,\n    workers_per_gpu=2,\n    train=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_train2017.json',\n        img_prefix=data_root + 'train2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0.5,\n        with_mask=False,\n        with_crowd=True,\n        with_label=True),\n    val=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_val2017.json',\n        img_prefix=data_root + 'val2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0,\n        with_mask=False,\n        with_crowd=True,\n        with_label=True),\n    test=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_val2017.json',\n        img_prefix=data_root + 'val2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0,\n        with_mask=False,\n        with_label=False,\n        test_mode=True))\n# optimizer\noptimizer = dict(type='SGD', lr=0.02, momentum=0.9, weight_decay=0.0001)\noptimizer_config = dict(grad_clip=dict(max_norm=35, norm_type=2))\n# learning policy\nlr_config = dict(\n    policy='step',\n    warmup='linear',\n    warmup_iters=500,\n    warmup_ratio=1.0 / 3,\n    step=[16, 22])\ncheckpoint_config = dict(interval=1)\n# yapf:disable\nlog_config = dict(\n    interval=50,\n    hooks=[\n        dict(type='TextLoggerHook'),\n        # dict(type='TensorboardLoggerHook')\n    ])\n# yapf:enable\n# runtime settings\ntotal_epochs = 24\ndist_params = dict(backend='nccl')\nlog_level = 'INFO'\nwork_dir = './work_dirs/faster_rcnn_r50_fpn_2x_pretrain_sge_resnet50'\nload_from = None\nresume_from = None\nworkflow = [('train', 1)]\n"""
detection/local_configs/mask_rcnn_r101_fpn_2x.py,0,"b""# model settings\nmodel = dict(\n    type='MaskRCNN',\n    pretrained='modelzoo://resnet101',\n    backbone=dict(\n        type='ResNet',\n        depth=101,\n        num_stages=4,\n        out_indices=(0, 1, 2, 3),\n        frozen_stages=1,\n        style='pytorch'),\n    neck=dict(\n        type='FPN',\n        in_channels=[256, 512, 1024, 2048],\n        out_channels=256,\n        num_outs=5),\n    rpn_head=dict(\n        type='RPNHead',\n        in_channels=256,\n        feat_channels=256,\n        anchor_scales=[8],\n        anchor_ratios=[0.5, 1.0, 2.0],\n        anchor_strides=[4, 8, 16, 32, 64],\n        target_means=[.0, .0, .0, .0],\n        target_stds=[1.0, 1.0, 1.0, 1.0],\n        use_sigmoid_cls=True),\n    bbox_roi_extractor=dict(\n        type='SingleRoIExtractor',\n        roi_layer=dict(type='RoIAlign', out_size=7, sample_num=2),\n        out_channels=256,\n        featmap_strides=[4, 8, 16, 32]),\n    bbox_head=dict(\n        type='SharedFCBBoxHead',\n        num_fcs=2,\n        in_channels=256,\n        fc_out_channels=1024,\n        roi_feat_size=7,\n        num_classes=81,\n        target_means=[0., 0., 0., 0.],\n        target_stds=[0.1, 0.1, 0.2, 0.2],\n        reg_class_agnostic=False),\n    mask_roi_extractor=dict(\n        type='SingleRoIExtractor',\n        roi_layer=dict(type='RoIAlign', out_size=14, sample_num=2),\n        out_channels=256,\n        featmap_strides=[4, 8, 16, 32]),\n    mask_head=dict(\n        type='FCNMaskHead',\n        num_convs=4,\n        in_channels=256,\n        conv_out_channels=256,\n        num_classes=81))\n# model training and testing settings\ntrain_cfg = dict(\n    rpn=dict(\n        assigner=dict(\n            type='MaxIoUAssigner',\n            pos_iou_thr=0.7,\n            neg_iou_thr=0.3,\n            min_pos_iou=0.3,\n            ignore_iof_thr=-1),\n        sampler=dict(\n            type='RandomSampler',\n            num=256,\n            pos_fraction=0.5,\n            neg_pos_ub=-1,\n            add_gt_as_proposals=False),\n        allowed_border=0,\n        pos_weight=-1,\n        smoothl1_beta=1 / 9.0,\n        debug=False),\n    rcnn=dict(\n        assigner=dict(\n            type='MaxIoUAssigner',\n            pos_iou_thr=0.5,\n            neg_iou_thr=0.5,\n            min_pos_iou=0.5,\n            ignore_iof_thr=-1),\n        sampler=dict(\n            type='RandomSampler',\n            num=512,\n            pos_fraction=0.25,\n            neg_pos_ub=-1,\n            add_gt_as_proposals=True),\n        mask_size=28,\n        pos_weight=-1,\n        debug=False))\ntest_cfg = dict(\n    rpn=dict(\n        nms_across_levels=False,\n        nms_pre=2000,\n        nms_post=2000,\n        max_num=2000,\n        nms_thr=0.7,\n        min_bbox_size=0),\n    rcnn=dict(\n        score_thr=0.05,\n        nms=dict(type='nms', iou_thr=0.5),\n        max_per_img=100,\n        mask_thr_binary=0.5))\n# dataset settings\ndataset_type = 'CocoDataset'\ndata_root = 'data/coco/'\nimg_norm_cfg = dict(\n    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)\ndata = dict(\n    imgs_per_gpu=2,\n    workers_per_gpu=2,\n    train=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_train2017.json',\n        img_prefix=data_root + 'train2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0.5,\n        with_mask=True,\n        with_crowd=True,\n        with_label=True),\n    val=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_val2017.json',\n        img_prefix=data_root + 'val2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0,\n        with_mask=True,\n        with_crowd=True,\n        with_label=True),\n    test=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_val2017.json',\n        img_prefix=data_root + 'val2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0,\n        with_mask=False,\n        with_label=False,\n        test_mode=True))\n# optimizer\noptimizer = dict(type='SGD', lr=0.02, momentum=0.9, weight_decay=0.0001)\noptimizer_config = dict(grad_clip=dict(max_norm=35, norm_type=2))\n# learning policy\nlr_config = dict(\n    policy='step',\n    warmup='linear',\n    warmup_iters=500,\n    warmup_ratio=1.0 / 3,\n    step=[16, 22])\ncheckpoint_config = dict(interval=1)\n# yapf:disable\nlog_config = dict(\n    interval=50,\n    hooks=[\n        dict(type='TextLoggerHook'),\n        # dict(type='TensorboardLoggerHook')\n    ])\n# yapf:enable\n# runtime settings\ntotal_epochs = 24\ndist_params = dict(backend='nccl')\nlog_level = 'INFO'\nwork_dir = './work_dirs/mask_rcnn_r101_fpn_2x'\nload_from = None\nresume_from = None\nworkflow = [('train', 1)]\n"""
detection/local_configs/mask_rcnn_r101_fpn_2x_pretrain_sge_resnet101.py,0,"b""# model settings\nmodel = dict(\n    type='MaskRCNN',\n    #pretrained='modelzoo://resnet101',\n    pretrained='/share1/xiangli/pytorch-classification/checkpoints/imagenet/sge_resnet101/model_best.pth.tar',\n    backbone=dict(\n        type='ResNetSGE',\n        depth=101,\n        num_stages=4,\n        out_indices=(0, 1, 2, 3),\n        frozen_stages=1,\n        style='pytorch'),\n    neck=dict(\n        type='FPN',\n        in_channels=[256, 512, 1024, 2048],\n        out_channels=256,\n        num_outs=5),\n    rpn_head=dict(\n        type='RPNHead',\n        in_channels=256,\n        feat_channels=256,\n        anchor_scales=[8],\n        anchor_ratios=[0.5, 1.0, 2.0],\n        anchor_strides=[4, 8, 16, 32, 64],\n        target_means=[.0, .0, .0, .0],\n        target_stds=[1.0, 1.0, 1.0, 1.0],\n        use_sigmoid_cls=True),\n    bbox_roi_extractor=dict(\n        type='SingleRoIExtractor',\n        roi_layer=dict(type='RoIAlign', out_size=7, sample_num=2),\n        out_channels=256,\n        featmap_strides=[4, 8, 16, 32]),\n    bbox_head=dict(\n        type='SharedFCBBoxHead',\n        num_fcs=2,\n        in_channels=256,\n        fc_out_channels=1024,\n        roi_feat_size=7,\n        num_classes=81,\n        target_means=[0., 0., 0., 0.],\n        target_stds=[0.1, 0.1, 0.2, 0.2],\n        reg_class_agnostic=False),\n    mask_roi_extractor=dict(\n        type='SingleRoIExtractor',\n        roi_layer=dict(type='RoIAlign', out_size=14, sample_num=2),\n        out_channels=256,\n        featmap_strides=[4, 8, 16, 32]),\n    mask_head=dict(\n        type='FCNMaskHead',\n        num_convs=4,\n        in_channels=256,\n        conv_out_channels=256,\n        num_classes=81))\n# model training and testing settings\ntrain_cfg = dict(\n    rpn=dict(\n        assigner=dict(\n            type='MaxIoUAssigner',\n            pos_iou_thr=0.7,\n            neg_iou_thr=0.3,\n            min_pos_iou=0.3,\n            ignore_iof_thr=-1),\n        sampler=dict(\n            type='RandomSampler',\n            num=256,\n            pos_fraction=0.5,\n            neg_pos_ub=-1,\n            add_gt_as_proposals=False),\n        allowed_border=0,\n        pos_weight=-1,\n        smoothl1_beta=1 / 9.0,\n        debug=False),\n    rcnn=dict(\n        assigner=dict(\n            type='MaxIoUAssigner',\n            pos_iou_thr=0.5,\n            neg_iou_thr=0.5,\n            min_pos_iou=0.5,\n            ignore_iof_thr=-1),\n        sampler=dict(\n            type='RandomSampler',\n            num=512,\n            pos_fraction=0.25,\n            neg_pos_ub=-1,\n            add_gt_as_proposals=True),\n        mask_size=28,\n        pos_weight=-1,\n        debug=False))\ntest_cfg = dict(\n    rpn=dict(\n        nms_across_levels=False,\n        nms_pre=2000,\n        nms_post=2000,\n        max_num=2000,\n        nms_thr=0.7,\n        min_bbox_size=0),\n    rcnn=dict(\n        score_thr=0.05,\n        nms=dict(type='nms', iou_thr=0.5),\n        max_per_img=100,\n        mask_thr_binary=0.5))\n# dataset settings\ndataset_type = 'CocoDataset'\ndata_root = 'data/coco/'\nimg_norm_cfg = dict(\n    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)\ndata = dict(\n    imgs_per_gpu=1,\n    workers_per_gpu=2,\n    train=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_train2017.json',\n        img_prefix=data_root + 'train2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0.5,\n        with_mask=True,\n        with_crowd=True,\n        with_label=True),\n    val=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_val2017.json',\n        img_prefix=data_root + 'val2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0,\n        with_mask=True,\n        with_crowd=True,\n        with_label=True),\n    test=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_val2017.json',\n        img_prefix=data_root + 'val2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0,\n        with_mask=False,\n        with_label=False,\n        test_mode=True))\n# optimizer\noptimizer = dict(type='SGD', lr=0.01, momentum=0.9, weight_decay=0.0001)\noptimizer_config = dict(grad_clip=dict(max_norm=35, norm_type=2))\n# learning policy\nlr_config = dict(\n    policy='step',\n    warmup='linear',\n    warmup_iters=500,\n    warmup_ratio=1.0 / 3,\n    step=[16, 22])\ncheckpoint_config = dict(interval=1)\n# yapf:disable\nlog_config = dict(\n    interval=50,\n    hooks=[\n        dict(type='TextLoggerHook'),\n        # dict(type='TensorboardLoggerHook')\n    ])\n# yapf:enable\n# runtime settings\ntotal_epochs = 24\ndist_params = dict(backend='nccl')\nlog_level = 'INFO'\nwork_dir = './work_dirs/mask_rcnn_r101_fpn_2x_pretrain_sge_resnet101'\nload_from = None\nresume_from = None\nworkflow = [('train', 1)]\n"""
detection/local_configs/mask_rcnn_r50_fpn_2x.py,0,"b""# model settings\nmodel = dict(\n    type='MaskRCNN',\n    pretrained='modelzoo://resnet50',\n    backbone=dict(\n        type='ResNet',\n        depth=50,\n        num_stages=4,\n        out_indices=(0, 1, 2, 3),\n        frozen_stages=1,\n        style='pytorch'),\n    neck=dict(\n        type='FPN',\n        in_channels=[256, 512, 1024, 2048],\n        out_channels=256,\n        num_outs=5),\n    rpn_head=dict(\n        type='RPNHead',\n        in_channels=256,\n        feat_channels=256,\n        anchor_scales=[8],\n        anchor_ratios=[0.5, 1.0, 2.0],\n        anchor_strides=[4, 8, 16, 32, 64],\n        target_means=[.0, .0, .0, .0],\n        target_stds=[1.0, 1.0, 1.0, 1.0],\n        use_sigmoid_cls=True),\n    bbox_roi_extractor=dict(\n        type='SingleRoIExtractor',\n        roi_layer=dict(type='RoIAlign', out_size=7, sample_num=2),\n        out_channels=256,\n        featmap_strides=[4, 8, 16, 32]),\n    bbox_head=dict(\n        type='SharedFCBBoxHead',\n        num_fcs=2,\n        in_channels=256,\n        fc_out_channels=1024,\n        roi_feat_size=7,\n        num_classes=81,\n        target_means=[0., 0., 0., 0.],\n        target_stds=[0.1, 0.1, 0.2, 0.2],\n        reg_class_agnostic=False),\n    mask_roi_extractor=dict(\n        type='SingleRoIExtractor',\n        roi_layer=dict(type='RoIAlign', out_size=14, sample_num=2),\n        out_channels=256,\n        featmap_strides=[4, 8, 16, 32]),\n    mask_head=dict(\n        type='FCNMaskHead',\n        num_convs=4,\n        in_channels=256,\n        conv_out_channels=256,\n        num_classes=81))\n# model training and testing settings\ntrain_cfg = dict(\n    rpn=dict(\n        assigner=dict(\n            type='MaxIoUAssigner',\n            pos_iou_thr=0.7,\n            neg_iou_thr=0.3,\n            min_pos_iou=0.3,\n            ignore_iof_thr=-1),\n        sampler=dict(\n            type='RandomSampler',\n            num=256,\n            pos_fraction=0.5,\n            neg_pos_ub=-1,\n            add_gt_as_proposals=False),\n        allowed_border=0,\n        pos_weight=-1,\n        smoothl1_beta=1 / 9.0,\n        debug=False),\n    rcnn=dict(\n        assigner=dict(\n            type='MaxIoUAssigner',\n            pos_iou_thr=0.5,\n            neg_iou_thr=0.5,\n            min_pos_iou=0.5,\n            ignore_iof_thr=-1),\n        sampler=dict(\n            type='RandomSampler',\n            num=512,\n            pos_fraction=0.25,\n            neg_pos_ub=-1,\n            add_gt_as_proposals=True),\n        mask_size=28,\n        pos_weight=-1,\n        debug=False))\ntest_cfg = dict(\n    rpn=dict(\n        nms_across_levels=False,\n        nms_pre=2000,\n        nms_post=2000,\n        max_num=2000,\n        nms_thr=0.7,\n        min_bbox_size=0),\n    rcnn=dict(\n        score_thr=0.05,\n        nms=dict(type='nms', iou_thr=0.5),\n        max_per_img=100,\n        mask_thr_binary=0.5))\n# dataset settings\ndataset_type = 'CocoDataset'\ndata_root = 'data/coco/'\nimg_norm_cfg = dict(\n    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)\ndata = dict(\n    imgs_per_gpu=2,\n    workers_per_gpu=2,\n    train=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_train2017.json',\n        img_prefix=data_root + 'train2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0.5,\n        with_mask=True,\n        with_crowd=True,\n        with_label=True),\n    val=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_val2017.json',\n        img_prefix=data_root + 'val2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0,\n        with_mask=True,\n        with_crowd=True,\n        with_label=True),\n    test=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_val2017.json',\n        img_prefix=data_root + 'val2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0,\n        with_mask=False,\n        with_label=False,\n        test_mode=True))\n# optimizer\noptimizer = dict(type='SGD', lr=0.02, momentum=0.9, weight_decay=0.0001)\noptimizer_config = dict(grad_clip=dict(max_norm=35, norm_type=2))\n# learning policy\nlr_config = dict(\n    policy='step',\n    warmup='linear',\n    warmup_iters=500,\n    warmup_ratio=1.0 / 3,\n    step=[16, 22])\ncheckpoint_config = dict(interval=1)\n# yapf:disable\nlog_config = dict(\n    interval=50,\n    hooks=[\n        dict(type='TextLoggerHook'),\n        # dict(type='TensorboardLoggerHook')\n    ])\n# yapf:enable\n# runtime settings\ntotal_epochs = 24\ndist_params = dict(backend='nccl')\nlog_level = 'INFO'\nwork_dir = './work_dirs/mask_rcnn_r50_fpn_2x'\nload_from = None\nresume_from = None\nworkflow = [('train', 1)]\n"""
detection/local_configs/mask_rcnn_r50_fpn_2x_pretrain_sge_resnet50.py,0,"b""# model settings\nmodel = dict(\n    type='MaskRCNN',\n    # pretrained='modelzoo://resnet50',\n    pretrained='/home/xiangli/share1/pytorch-classification/checkpoints/imagenet/sge_resnet50/model_best.pth.tar',\n    backbone=dict(\n        type='ResNetSGE',\n        depth=50,\n        num_stages=4,\n        out_indices=(0, 1, 2, 3),\n        frozen_stages=1,\n        style='pytorch'),\n    neck=dict(\n        type='FPN',\n        in_channels=[256, 512, 1024, 2048],\n        out_channels=256,\n        num_outs=5),\n    rpn_head=dict(\n        type='RPNHead',\n        in_channels=256,\n        feat_channels=256,\n        anchor_scales=[8],\n        anchor_ratios=[0.5, 1.0, 2.0],\n        anchor_strides=[4, 8, 16, 32, 64],\n        target_means=[.0, .0, .0, .0],\n        target_stds=[1.0, 1.0, 1.0, 1.0],\n        use_sigmoid_cls=True),\n    bbox_roi_extractor=dict(\n        type='SingleRoIExtractor',\n        roi_layer=dict(type='RoIAlign', out_size=7, sample_num=2),\n        out_channels=256,\n        featmap_strides=[4, 8, 16, 32]),\n    bbox_head=dict(\n        type='SharedFCBBoxHead',\n        num_fcs=2,\n        in_channels=256,\n        fc_out_channels=1024,\n        roi_feat_size=7,\n        num_classes=81,\n        target_means=[0., 0., 0., 0.],\n        target_stds=[0.1, 0.1, 0.2, 0.2],\n        reg_class_agnostic=False),\n    mask_roi_extractor=dict(\n        type='SingleRoIExtractor',\n        roi_layer=dict(type='RoIAlign', out_size=14, sample_num=2),\n        out_channels=256,\n        featmap_strides=[4, 8, 16, 32]),\n    mask_head=dict(\n        type='FCNMaskHead',\n        num_convs=4,\n        in_channels=256,\n        conv_out_channels=256,\n        num_classes=81))\n# model training and testing settings\ntrain_cfg = dict(\n    rpn=dict(\n        assigner=dict(\n            type='MaxIoUAssigner',\n            pos_iou_thr=0.7,\n            neg_iou_thr=0.3,\n            min_pos_iou=0.3,\n            ignore_iof_thr=-1),\n        sampler=dict(\n            type='RandomSampler',\n            num=256,\n            pos_fraction=0.5,\n            neg_pos_ub=-1,\n            add_gt_as_proposals=False),\n        allowed_border=0,\n        pos_weight=-1,\n        smoothl1_beta=1 / 9.0,\n        debug=False),\n    rcnn=dict(\n        assigner=dict(\n            type='MaxIoUAssigner',\n            pos_iou_thr=0.5,\n            neg_iou_thr=0.5,\n            min_pos_iou=0.5,\n            ignore_iof_thr=-1),\n        sampler=dict(\n            type='RandomSampler',\n            num=512,\n            pos_fraction=0.25,\n            neg_pos_ub=-1,\n            add_gt_as_proposals=True),\n        mask_size=28,\n        pos_weight=-1,\n        debug=False))\ntest_cfg = dict(\n    rpn=dict(\n        nms_across_levels=False,\n        nms_pre=2000,\n        nms_post=2000,\n        max_num=2000,\n        nms_thr=0.7,\n        min_bbox_size=0),\n    rcnn=dict(\n        score_thr=0.05,\n        nms=dict(type='nms', iou_thr=0.5),\n        max_per_img=100,\n        mask_thr_binary=0.5))\n# dataset settings\ndataset_type = 'CocoDataset'\ndata_root = 'data/coco/'\nimg_norm_cfg = dict(\n    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)\ndata = dict(\n    imgs_per_gpu=2,\n    workers_per_gpu=2,\n    train=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_train2017.json',\n        img_prefix=data_root + 'train2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0.5,\n        with_mask=True,\n        with_crowd=True,\n        with_label=True),\n    val=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_val2017.json',\n        img_prefix=data_root + 'val2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0,\n        with_mask=True,\n        with_crowd=True,\n        with_label=True),\n    test=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_val2017.json',\n        img_prefix=data_root + 'val2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0,\n        with_mask=False,\n        with_label=False,\n        test_mode=True))\n# optimizer\noptimizer = dict(type='SGD', lr=0.02, momentum=0.9, weight_decay=0.0001)\noptimizer_config = dict(grad_clip=dict(max_norm=35, norm_type=2))\n# learning policy\nlr_config = dict(\n    policy='step',\n    warmup='linear',\n    warmup_iters=500,\n    warmup_ratio=1.0 / 3,\n    step=[16, 22])\ncheckpoint_config = dict(interval=1)\n# yapf:disable\nlog_config = dict(\n    interval=50,\n    hooks=[\n        dict(type='TextLoggerHook'),\n        # dict(type='TensorboardLoggerHook')\n    ])\n# yapf:enable\n# runtime settings\ntotal_epochs = 24\ndist_params = dict(backend='nccl')\nlog_level = 'INFO'\nwork_dir = './work_dirs/mask_rcnn_r50_fpn_2x_pretrain_sge_resnet50'\nload_from = None\nresume_from = None\nworkflow = [('train', 1)]\n"""
detection/local_configs/retinanet_r101_fpn_2x.py,0,"b""# model settings\nmodel = dict(\n    type='RetinaNet',\n    pretrained='modelzoo://resnet101',\n    backbone=dict(\n        type='ResNet',\n        depth=101,\n        num_stages=4,\n        out_indices=(0, 1, 2, 3),\n        frozen_stages=1,\n        style='pytorch'),\n    neck=dict(\n        type='FPN',\n        in_channels=[256, 512, 1024, 2048],\n        out_channels=256,\n        start_level=1,\n        add_extra_convs=True,\n        num_outs=5),\n    bbox_head=dict(\n        type='RetinaHead',\n        num_classes=81,\n        in_channels=256,\n        stacked_convs=4,\n        feat_channels=256,\n        octave_base_scale=4,\n        scales_per_octave=3,\n        anchor_ratios=[0.5, 1.0, 2.0],\n        anchor_strides=[8, 16, 32, 64, 128],\n        target_means=[.0, .0, .0, .0],\n        target_stds=[1.0, 1.0, 1.0, 1.0]))\n# training and testing settings\ntrain_cfg = dict(\n    assigner=dict(\n        type='MaxIoUAssigner',\n        pos_iou_thr=0.5,\n        neg_iou_thr=0.4,\n        min_pos_iou=0,\n        ignore_iof_thr=-1),\n    smoothl1_beta=0.11,\n    gamma=2.0,\n    alpha=0.25,\n    allowed_border=-1,\n    pos_weight=-1,\n    debug=False)\ntest_cfg = dict(\n    nms_pre=1000,\n    min_bbox_size=0,\n    score_thr=0.05,\n    nms=dict(type='nms', iou_thr=0.5),\n    max_per_img=100)\n# dataset settings\ndataset_type = 'CocoDataset'\ndata_root = 'data/coco/'\nimg_norm_cfg = dict(\n    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)\ndata = dict(\n    imgs_per_gpu=1,\n    workers_per_gpu=2,\n    train=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_train2017.json',\n        img_prefix=data_root + 'train2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0.5,\n        with_mask=False,\n        with_crowd=False,\n        with_label=True),\n    val=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_val2017.json',\n        img_prefix=data_root + 'val2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0,\n        with_mask=False,\n        with_crowd=False,\n        with_label=True),\n    test=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_val2017.json',\n        img_prefix=data_root + 'val2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0,\n        with_mask=False,\n        with_crowd=False,\n        with_label=False,\n        test_mode=True))\n# optimizer\noptimizer = dict(type='SGD', lr=0.01, momentum=0.9, weight_decay=0.0001)\noptimizer_config = dict(grad_clip=dict(max_norm=35, norm_type=2))\n# learning policy\nlr_config = dict(\n    policy='step',\n    warmup='linear',\n    warmup_iters=500,\n    warmup_ratio=1.0 / 3,\n    step=[16, 22])\ncheckpoint_config = dict(interval=1)\n# yapf:disable\nlog_config = dict(\n    interval=50,\n    hooks=[\n        dict(type='TextLoggerHook'),\n        # dict(type='TensorboardLoggerHook')\n    ])\n# yapf:enable\n# runtime settings\ntotal_epochs = 24\ndevice_ids = range(8)\ndist_params = dict(backend='nccl')\nlog_level = 'INFO'\nwork_dir = './work_dirs/retinanet_r101_fpn_2x'\nload_from = None\nresume_from = None\nworkflow = [('train', 1)]\n"""
detection/local_configs/retinanet_r101_fpn_2x_pretrain_cbam_resnet101.py,0,"b""# model settings\nmodel = dict(\n    type='RetinaNet',\n    #pretrained='modelzoo://resnet101',\n    pretrained='/home/xiangli/share1/pytorch-classification/checkpoints/imagenet/cbam_resnet101/model_best.pth.tar',\n    backbone=dict(\n        type='ResNetCBAM',\n        depth=101,\n        num_stages=4,\n        out_indices=(0, 1, 2, 3),\n        frozen_stages=1,\n        style='pytorch'),\n    neck=dict(\n        type='FPN',\n        in_channels=[256, 512, 1024, 2048],\n        out_channels=256,\n        start_level=1,\n        add_extra_convs=True,\n        num_outs=5),\n    bbox_head=dict(\n        type='RetinaHead',\n        num_classes=81,\n        in_channels=256,\n        stacked_convs=4,\n        feat_channels=256,\n        octave_base_scale=4,\n        scales_per_octave=3,\n        anchor_ratios=[0.5, 1.0, 2.0],\n        anchor_strides=[8, 16, 32, 64, 128],\n        target_means=[.0, .0, .0, .0],\n        target_stds=[1.0, 1.0, 1.0, 1.0]))\n# training and testing settings\ntrain_cfg = dict(\n    assigner=dict(\n        type='MaxIoUAssigner',\n        pos_iou_thr=0.5,\n        neg_iou_thr=0.4,\n        min_pos_iou=0,\n        ignore_iof_thr=-1),\n    smoothl1_beta=0.11,\n    gamma=2.0,\n    alpha=0.25,\n    allowed_border=-1,\n    pos_weight=-1,\n    debug=False)\ntest_cfg = dict(\n    nms_pre=1000,\n    min_bbox_size=0,\n    score_thr=0.05,\n    nms=dict(type='nms', iou_thr=0.5),\n    max_per_img=100)\n# dataset settings\ndataset_type = 'CocoDataset'\ndata_root = 'data/coco/'\nimg_norm_cfg = dict(\n    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)\ndata = dict(\n    imgs_per_gpu=1,\n    workers_per_gpu=2,\n    train=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_train2017.json',\n        img_prefix=data_root + 'train2017/',\n        img_scale=(1233, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0.5,\n        with_mask=False,\n        with_crowd=False,\n        with_label=True),\n    val=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_val2017.json',\n        img_prefix=data_root + 'val2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0,\n        with_mask=False,\n        with_crowd=False,\n        with_label=True),\n    test=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_val2017.json',\n        img_prefix=data_root + 'val2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0,\n        with_mask=False,\n        with_crowd=False,\n        with_label=False,\n        test_mode=True))\n# optimizer\noptimizer = dict(type='SGD', lr=0.01, momentum=0.9, weight_decay=0.0001)\noptimizer_config = dict(grad_clip=dict(max_norm=35, norm_type=2))\n# learning policy\nlr_config = dict(\n    policy='step',\n    warmup='linear',\n    warmup_iters=500,\n    warmup_ratio=1.0 / 3,\n    step=[16, 22])\ncheckpoint_config = dict(interval=1)\n# yapf:disable\nlog_config = dict(\n    interval=50,\n    hooks=[\n        dict(type='TextLoggerHook'),\n        # dict(type='TensorboardLoggerHook')\n    ])\n# yapf:enable\n# runtime settings\ntotal_epochs = 24\ndevice_ids = range(8)\ndist_params = dict(backend='nccl')\nlog_level = 'INFO'\nwork_dir = './work_dirs/retinanet_r101_fpn_2x_pretrain_cbam_resnet101'\nload_from = None\nresume_from = None\nworkflow = [('train', 1)]\n"""
detection/local_configs/retinanet_r101_fpn_2x_pretrain_gc_resnet101.py,0,"b""# model settings\nmodel = dict(\n    type='RetinaNet',\n    #pretrained='modelzoo://resnet101',\n    pretrained='/home/xiangli/share1/pytorch-classification/checkpoints/imagenet/gcnet_resnet101/model_best.pth.tar',\n    backbone=dict(\n        type='ResNetGC',\n        depth=101,\n        num_stages=4,\n        out_indices=(0, 1, 2, 3),\n        frozen_stages=1,\n        style='pytorch'),\n    neck=dict(\n        type='FPN',\n        in_channels=[256, 512, 1024, 2048],\n        out_channels=256,\n        start_level=1,\n        add_extra_convs=True,\n        num_outs=5),\n    bbox_head=dict(\n        type='RetinaHead',\n        num_classes=81,\n        in_channels=256,\n        stacked_convs=4,\n        feat_channels=256,\n        octave_base_scale=4,\n        scales_per_octave=3,\n        anchor_ratios=[0.5, 1.0, 2.0],\n        anchor_strides=[8, 16, 32, 64, 128],\n        target_means=[.0, .0, .0, .0],\n        target_stds=[1.0, 1.0, 1.0, 1.0]))\n# training and testing settings\ntrain_cfg = dict(\n    assigner=dict(\n        type='MaxIoUAssigner',\n        pos_iou_thr=0.5,\n        neg_iou_thr=0.4,\n        min_pos_iou=0,\n        ignore_iof_thr=-1),\n    smoothl1_beta=0.11,\n    gamma=2.0,\n    alpha=0.25,\n    allowed_border=-1,\n    pos_weight=-1,\n    debug=False)\ntest_cfg = dict(\n    nms_pre=1000,\n    min_bbox_size=0,\n    score_thr=0.05,\n    nms=dict(type='nms', iou_thr=0.5),\n    max_per_img=100)\n# dataset settings\ndataset_type = 'CocoDataset'\ndata_root = 'data/coco/'\nimg_norm_cfg = dict(\n    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)\ndata = dict(\n    imgs_per_gpu=1,\n    workers_per_gpu=2,\n    train=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_train2017.json',\n        img_prefix=data_root + 'train2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0.5,\n        with_mask=False,\n        with_crowd=False,\n        with_label=True),\n    val=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_val2017.json',\n        img_prefix=data_root + 'val2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0,\n        with_mask=False,\n        with_crowd=False,\n        with_label=True),\n    test=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_val2017.json',\n        img_prefix=data_root + 'val2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0,\n        with_mask=False,\n        with_crowd=False,\n        with_label=False,\n        test_mode=True))\n# optimizer\noptimizer = dict(type='SGD', lr=0.01, momentum=0.9, weight_decay=0.0001)\noptimizer_config = dict(grad_clip=dict(max_norm=35, norm_type=2))\n# learning policy\nlr_config = dict(\n    policy='step',\n    warmup='linear',\n    warmup_iters=500,\n    warmup_ratio=1.0 / 3,\n    step=[16, 22])\ncheckpoint_config = dict(interval=1)\n# yapf:disable\nlog_config = dict(\n    interval=50,\n    hooks=[\n        dict(type='TextLoggerHook'),\n        # dict(type='TensorboardLoggerHook')\n    ])\n# yapf:enable\n# runtime settings\ntotal_epochs = 24\ndevice_ids = range(8)\ndist_params = dict(backend='nccl')\nlog_level = 'INFO'\nwork_dir = './work_dirs/retinanet_r101_fpn_2x_pretrain_gc_resnet101'\nload_from = None\nresume_from = None\nworkflow = [('train', 1)]\n"""
detection/local_configs/retinanet_r101_fpn_2x_pretrain_se_resnet101.py,0,"b""# model settings\nmodel = dict(\n    type='RetinaNet',\n    #pretrained='modelzoo://resnet101',\n    pretrained='/share1/xiangli/pytorch-classification/checkpoints/imagenet/se_resnet101/model_best.pth.tar',\n    backbone=dict(\n        type='ResNetSE',\n        depth=101,\n        num_stages=4,\n        out_indices=(0, 1, 2, 3),\n        frozen_stages=1,\n        style='pytorch'),\n    neck=dict(\n        type='FPN',\n        in_channels=[256, 512, 1024, 2048],\n        out_channels=256,\n        start_level=1,\n        add_extra_convs=True,\n        num_outs=5),\n    bbox_head=dict(\n        type='RetinaHead',\n        num_classes=81,\n        in_channels=256,\n        stacked_convs=4,\n        feat_channels=256,\n        octave_base_scale=4,\n        scales_per_octave=3,\n        anchor_ratios=[0.5, 1.0, 2.0],\n        anchor_strides=[8, 16, 32, 64, 128],\n        target_means=[.0, .0, .0, .0],\n        target_stds=[1.0, 1.0, 1.0, 1.0]))\n# training and testing settings\ntrain_cfg = dict(\n    assigner=dict(\n        type='MaxIoUAssigner',\n        pos_iou_thr=0.5,\n        neg_iou_thr=0.4,\n        min_pos_iou=0,\n        ignore_iof_thr=-1),\n    smoothl1_beta=0.11,\n    gamma=2.0,\n    alpha=0.25,\n    allowed_border=-1,\n    pos_weight=-1,\n    debug=False)\ntest_cfg = dict(\n    nms_pre=1000,\n    min_bbox_size=0,\n    score_thr=0.05,\n    nms=dict(type='nms', iou_thr=0.5),\n    max_per_img=100)\n# dataset settings\ndataset_type = 'CocoDataset'\ndata_root = 'data/coco/'\nimg_norm_cfg = dict(\n    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)\ndata = dict(\n    imgs_per_gpu=1,\n    workers_per_gpu=2,\n    train=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_train2017.json',\n        img_prefix=data_root + 'train2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0.5,\n        with_mask=False,\n        with_crowd=False,\n        with_label=True),\n    val=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_val2017.json',\n        img_prefix=data_root + 'val2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0,\n        with_mask=False,\n        with_crowd=False,\n        with_label=True),\n    test=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_val2017.json',\n        img_prefix=data_root + 'val2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0,\n        with_mask=False,\n        with_crowd=False,\n        with_label=False,\n        test_mode=True))\n# optimizer\noptimizer = dict(type='SGD', lr=0.01, momentum=0.9, weight_decay=0.0001)\noptimizer_config = dict(grad_clip=dict(max_norm=35, norm_type=2))\n# learning policy\nlr_config = dict(\n    policy='step',\n    warmup='linear',\n    warmup_iters=500,\n    warmup_ratio=1.0 / 3,\n    step=[16, 22])\ncheckpoint_config = dict(interval=1)\n# yapf:disable\nlog_config = dict(\n    interval=50,\n    hooks=[\n        dict(type='TextLoggerHook'),\n        # dict(type='TensorboardLoggerHook')\n    ])\n# yapf:enable\n# runtime settings\ntotal_epochs = 24\ndevice_ids = range(8)\ndist_params = dict(backend='nccl')\nlog_level = 'INFO'\nwork_dir = './work_dirs/retinanet_r101_fpn_2x_pretrain_se_resnet101'\nload_from = None\nresume_from = None\nworkflow = [('train', 1)]\n"""
detection/local_configs/retinanet_r101_fpn_2x_pretrain_sge_resnet101.py,0,"b""# model settings\nmodel = dict(\n    type='RetinaNet',\n    #pretrained='modelzoo://resnet101',\n    pretrained='/share1/xiangli/pytorch-classification/checkpoints/imagenet/sge_resnet101/model_best.pth.tar',\n    backbone=dict(\n        type='ResNetSGE',\n        depth=101,\n        num_stages=4,\n        out_indices=(0, 1, 2, 3),\n        frozen_stages=1,\n        style='pytorch'),\n    neck=dict(\n        type='FPN',\n        in_channels=[256, 512, 1024, 2048],\n        out_channels=256,\n        start_level=1,\n        add_extra_convs=True,\n        num_outs=5),\n    bbox_head=dict(\n        type='RetinaHead',\n        num_classes=81,\n        in_channels=256,\n        stacked_convs=4,\n        feat_channels=256,\n        octave_base_scale=4,\n        scales_per_octave=3,\n        anchor_ratios=[0.5, 1.0, 2.0],\n        anchor_strides=[8, 16, 32, 64, 128],\n        target_means=[.0, .0, .0, .0],\n        target_stds=[1.0, 1.0, 1.0, 1.0]))\n# training and testing settings\ntrain_cfg = dict(\n    assigner=dict(\n        type='MaxIoUAssigner',\n        pos_iou_thr=0.5,\n        neg_iou_thr=0.4,\n        min_pos_iou=0,\n        ignore_iof_thr=-1),\n    smoothl1_beta=0.11,\n    gamma=2.0,\n    alpha=0.25,\n    allowed_border=-1,\n    pos_weight=-1,\n    debug=False)\ntest_cfg = dict(\n    nms_pre=1000,\n    min_bbox_size=0,\n    score_thr=0.05,\n    nms=dict(type='nms', iou_thr=0.5),\n    max_per_img=100)\n# dataset settings\ndataset_type = 'CocoDataset'\ndata_root = 'data/coco/'\nimg_norm_cfg = dict(\n    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)\ndata = dict(\n    imgs_per_gpu=1,\n    workers_per_gpu=2,\n    train=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_train2017.json',\n        img_prefix=data_root + 'train2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0.5,\n        with_mask=False,\n        with_crowd=False,\n        with_label=True),\n    val=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_val2017.json',\n        img_prefix=data_root + 'val2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0,\n        with_mask=False,\n        with_crowd=False,\n        with_label=True),\n    test=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_val2017.json',\n        img_prefix=data_root + 'val2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0,\n        with_mask=False,\n        with_crowd=False,\n        with_label=False,\n        test_mode=True))\n# optimizer\noptimizer = dict(type='SGD', lr=0.01, momentum=0.9, weight_decay=0.0001)\noptimizer_config = dict(grad_clip=dict(max_norm=35, norm_type=2))\n# learning policy\nlr_config = dict(\n    policy='step',\n    warmup='linear',\n    warmup_iters=500,\n    warmup_ratio=1.0 / 3,\n    step=[16, 22])\ncheckpoint_config = dict(interval=1)\n# yapf:disable\nlog_config = dict(\n    interval=50,\n    hooks=[\n        dict(type='TextLoggerHook'),\n        # dict(type='TensorboardLoggerHook')\n    ])\n# yapf:enable\n# runtime settings\ntotal_epochs = 24\ndevice_ids = range(8)\ndist_params = dict(backend='nccl')\nlog_level = 'INFO'\nwork_dir = './work_dirs/retinanet_r101_fpn_2x_pretrain_sge_resnet101'\nload_from = None\nresume_from = None\nworkflow = [('train', 1)]\n"""
detection/local_configs/retinanet_r50_fpn_2x.py,0,"b""# model settings\nmodel = dict(\n    type='RetinaNet',\n    pretrained='modelzoo://resnet50',\n    # pretrained='/data/nfs_share/public/AS/pytorch-classification/checkpoints/imagenet/se_resnet50/model_best.pth.tar',\n    backbone=dict(\n        type='ResNet',\n        depth=50,\n        num_stages=4,\n        out_indices=(0, 1, 2, 3),\n        frozen_stages=1,\n        style='pytorch'),\n    neck=dict(\n        type='FPN',\n        in_channels=[256, 512, 1024, 2048],\n        out_channels=256,\n        start_level=1,\n        add_extra_convs=True,\n        num_outs=5),\n    bbox_head=dict(\n        type='RetinaHead',\n        num_classes=81,\n        in_channels=256,\n        stacked_convs=4,\n        feat_channels=256,\n        octave_base_scale=4,\n        scales_per_octave=3,\n        anchor_ratios=[0.5, 1.0, 2.0],\n        anchor_strides=[8, 16, 32, 64, 128],\n        target_means=[.0, .0, .0, .0],\n        target_stds=[1.0, 1.0, 1.0, 1.0]))\n# training and testing settings\ntrain_cfg = dict(\n    assigner=dict(\n        type='MaxIoUAssigner',\n        pos_iou_thr=0.5,\n        neg_iou_thr=0.4,\n        min_pos_iou=0,\n        ignore_iof_thr=-1),\n    smoothl1_beta=0.11,\n    gamma=2.0,\n    alpha=0.25,\n    allowed_border=-1,\n    pos_weight=-1,\n    debug=False)\ntest_cfg = dict(\n    nms_pre=1000,\n    min_bbox_size=0,\n    score_thr=0.05,\n    nms=dict(type='nms', iou_thr=0.5),\n    max_per_img=100)\n# dataset settings\ndataset_type = 'CocoDataset'\ndata_root = 'data/coco/'\nimg_norm_cfg = dict(\n    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)\ndata = dict(\n    imgs_per_gpu=2,\n    workers_per_gpu=2,\n    train=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_train2017.json',\n        img_prefix=data_root + 'train2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0.5,\n        with_mask=False,\n        with_crowd=False,\n        with_label=True),\n    val=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_val2017.json',\n        img_prefix=data_root + 'val2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0,\n        with_mask=False,\n        with_crowd=False,\n        with_label=True),\n    test=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_val2017.json',\n        img_prefix=data_root + 'val2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0,\n        with_mask=False,\n        with_crowd=False,\n        with_label=False,\n        test_mode=True))\n# optimizer\noptimizer = dict(type='SGD', lr=0.01, momentum=0.9, weight_decay=0.0001)\noptimizer_config = dict(grad_clip=dict(max_norm=35, norm_type=2))\n# learning policy\nlr_config = dict(\n    policy='step',\n    warmup='linear',\n    warmup_iters=500,\n    warmup_ratio=1.0 / 3,\n    step=[16, 22])\ncheckpoint_config = dict(interval=1)\n# yapf:disable\nlog_config = dict(\n    interval=50,\n    hooks=[\n        dict(type='TextLoggerHook'),\n        # dict(type='TensorboardLoggerHook')\n    ])\n# yapf:enable\n# runtime settings\ntotal_epochs = 24\ndevice_ids = range(8)\ndist_params = dict(backend='nccl')\nlog_level = 'INFO'\nwork_dir = './work_dirs/retinanet_r50_fpn_2x'\nload_from = None\nresume_from = None\nworkflow = [('train', 1)]\n"""
detection/local_configs/retinanet_r50_fpn_2x_pretrain_bam_resnet50.py,0,"b""# model settings\nmodel = dict(\n    type='RetinaNet',\n    # pretrained='modelzoo://resnet50',\n    pretrained='/data/nfs_share/public/AS/pytorch-classification/checkpoints/imagenet/bam_resnet50/model_best.pth.tar',\n    backbone=dict(\n        type='ResNetBAM',\n        depth=50,\n        num_stages=4,\n        out_indices=(0, 1, 2, 3),\n        frozen_stages=1,\n        style='pytorch'),\n    neck=dict(\n        type='FPN',\n        in_channels=[256, 512, 1024, 2048],\n        out_channels=256,\n        start_level=1,\n        add_extra_convs=True,\n        num_outs=5),\n    bbox_head=dict(\n        type='RetinaHead',\n        num_classes=81,\n        in_channels=256,\n        stacked_convs=4,\n        feat_channels=256,\n        octave_base_scale=4,\n        scales_per_octave=3,\n        anchor_ratios=[0.5, 1.0, 2.0],\n        anchor_strides=[8, 16, 32, 64, 128],\n        target_means=[.0, .0, .0, .0],\n        target_stds=[1.0, 1.0, 1.0, 1.0]))\n# training and testing settings\ntrain_cfg = dict(\n    assigner=dict(\n        type='MaxIoUAssigner',\n        pos_iou_thr=0.5,\n        neg_iou_thr=0.4,\n        min_pos_iou=0,\n        ignore_iof_thr=-1),\n    smoothl1_beta=0.11,\n    gamma=2.0,\n    alpha=0.25,\n    allowed_border=-1,\n    pos_weight=-1,\n    debug=False)\ntest_cfg = dict(\n    nms_pre=1000,\n    min_bbox_size=0,\n    score_thr=0.05,\n    nms=dict(type='nms', iou_thr=0.5),\n    max_per_img=100)\n# dataset settings\ndataset_type = 'CocoDataset'\ndata_root = 'data/coco/'\nimg_norm_cfg = dict(\n    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)\ndata = dict(\n    imgs_per_gpu=1,\n    workers_per_gpu=2,\n    train=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_train2017.json',\n        img_prefix=data_root + 'train2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0.5,\n        with_mask=False,\n        with_crowd=False,\n        with_label=True),\n    val=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_val2017.json',\n        img_prefix=data_root + 'val2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0,\n        with_mask=False,\n        with_crowd=False,\n        with_label=True),\n    test=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_val2017.json',\n        img_prefix=data_root + 'val2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0,\n        with_mask=False,\n        with_crowd=False,\n        with_label=False,\n        test_mode=True))\n# optimizer\noptimizer = dict(type='SGD', lr=0.01, momentum=0.9, weight_decay=0.0001)\noptimizer_config = dict(grad_clip=dict(max_norm=35, norm_type=2))\n# learning policy\nlr_config = dict(\n    policy='step',\n    warmup='linear',\n    warmup_iters=500,\n    warmup_ratio=1.0 / 3,\n    step=[16, 22])\ncheckpoint_config = dict(interval=1)\n# yapf:disable\nlog_config = dict(\n    interval=50,\n    hooks=[\n        dict(type='TextLoggerHook'),\n        # dict(type='TensorboardLoggerHook')\n    ])\n# yapf:enable\n# runtime settings\ntotal_epochs = 24\ndevice_ids = range(8)\ndist_params = dict(backend='nccl')\nlog_level = 'INFO'\nwork_dir = './work_dirs/retinanet_r50_fpn_2x_pretrain_bam_resnet50'\nload_from = None\nresume_from = None\nworkflow = [('train', 1)]\n"""
detection/local_configs/retinanet_r50_fpn_2x_pretrain_cbam_resnet50.py,0,"b""# model settings\nmodel = dict(\n    type='RetinaNet',\n    # pretrained='modelzoo://resnet50',\n    pretrained='/home/xiangli/share1/pytorch-classification/checkpoints/imagenet/cbam_resnet50/model_best.pth.tar',\n    backbone=dict(\n        type='ResNetCBAM',\n        depth=50,\n        num_stages=4,\n        out_indices=(0, 1, 2, 3),\n        frozen_stages=1,\n        style='pytorch'),\n    neck=dict(\n        type='FPN',\n        in_channels=[256, 512, 1024, 2048],\n        out_channels=256,\n        start_level=1,\n        add_extra_convs=True,\n        num_outs=5),\n    bbox_head=dict(\n        type='RetinaHead',\n        num_classes=81,\n        in_channels=256,\n        stacked_convs=4,\n        feat_channels=256,\n        octave_base_scale=4,\n        scales_per_octave=3,\n        anchor_ratios=[0.5, 1.0, 2.0],\n        anchor_strides=[8, 16, 32, 64, 128],\n        target_means=[.0, .0, .0, .0],\n        target_stds=[1.0, 1.0, 1.0, 1.0]))\n# training and testing settings\ntrain_cfg = dict(\n    assigner=dict(\n        type='MaxIoUAssigner',\n        pos_iou_thr=0.5,\n        neg_iou_thr=0.4,\n        min_pos_iou=0,\n        ignore_iof_thr=-1),\n    smoothl1_beta=0.11,\n    gamma=2.0,\n    alpha=0.25,\n    allowed_border=-1,\n    pos_weight=-1,\n    debug=False)\ntest_cfg = dict(\n    nms_pre=1000,\n    min_bbox_size=0,\n    score_thr=0.05,\n    nms=dict(type='nms', iou_thr=0.5),\n    max_per_img=100)\n# dataset settings\ndataset_type = 'CocoDataset'\ndata_root = 'data/coco/'\nimg_norm_cfg = dict(\n    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)\ndata = dict(\n    imgs_per_gpu=1,\n    workers_per_gpu=2,\n    train=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_train2017.json',\n        img_prefix=data_root + 'train2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0.5,\n        with_mask=False,\n        with_crowd=False,\n        with_label=True),\n    val=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_val2017.json',\n        img_prefix=data_root + 'val2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0,\n        with_mask=False,\n        with_crowd=False,\n        with_label=True),\n    test=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_val2017.json',\n        img_prefix=data_root + 'val2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0,\n        with_mask=False,\n        with_crowd=False,\n        with_label=False,\n        test_mode=True))\n# optimizer\noptimizer = dict(type='SGD', lr=0.01, momentum=0.9, weight_decay=0.0001)\noptimizer_config = dict(grad_clip=dict(max_norm=35, norm_type=2))\n# learning policy\nlr_config = dict(\n    policy='step',\n    warmup='linear',\n    warmup_iters=500,\n    warmup_ratio=1.0 / 3,\n    step=[16, 22])\ncheckpoint_config = dict(interval=1)\n# yapf:disable\nlog_config = dict(\n    interval=50,\n    hooks=[\n        dict(type='TextLoggerHook'),\n        # dict(type='TensorboardLoggerHook')\n    ])\n# yapf:enable\n# runtime settings\ntotal_epochs = 24\ndevice_ids = range(8)\ndist_params = dict(backend='nccl')\nlog_level = 'INFO'\nwork_dir = './work_dirs/retinanet_r50_fpn_2x_pretrain_cbam_resnet50'\nload_from = None\nresume_from = None\nworkflow = [('train', 1)]\n"""
detection/local_configs/retinanet_r50_fpn_2x_pretrain_gc_resnet50.py,0,"b""# model settings\nmodel = dict(\n    type='RetinaNet',\n    # pretrained='modelzoo://resnet50',\n    pretrained='/home/xiangli/share1/pytorch-classification/checkpoints/imagenet/gcnet_resnet50/model_best.pth.tar',\n    backbone=dict(\n        type='ResNetGC',\n        depth=50,\n        num_stages=4,\n        out_indices=(0, 1, 2, 3),\n        frozen_stages=1,\n        style='pytorch'),\n    neck=dict(\n        type='FPN',\n        in_channels=[256, 512, 1024, 2048],\n        out_channels=256,\n        start_level=1,\n        add_extra_convs=True,\n        num_outs=5),\n    bbox_head=dict(\n        type='RetinaHead',\n        num_classes=81,\n        in_channels=256,\n        stacked_convs=4,\n        feat_channels=256,\n        octave_base_scale=4,\n        scales_per_octave=3,\n        anchor_ratios=[0.5, 1.0, 2.0],\n        anchor_strides=[8, 16, 32, 64, 128],\n        target_means=[.0, .0, .0, .0],\n        target_stds=[1.0, 1.0, 1.0, 1.0]))\n# training and testing settings\ntrain_cfg = dict(\n    assigner=dict(\n        type='MaxIoUAssigner',\n        pos_iou_thr=0.5,\n        neg_iou_thr=0.4,\n        min_pos_iou=0,\n        ignore_iof_thr=-1),\n    smoothl1_beta=0.11,\n    gamma=2.0,\n    alpha=0.25,\n    allowed_border=-1,\n    pos_weight=-1,\n    debug=False)\ntest_cfg = dict(\n    nms_pre=1000,\n    min_bbox_size=0,\n    score_thr=0.05,\n    nms=dict(type='nms', iou_thr=0.5),\n    max_per_img=100)\n# dataset settings\ndataset_type = 'CocoDataset'\ndata_root = 'data/coco/'\nimg_norm_cfg = dict(\n    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)\ndata = dict(\n    imgs_per_gpu=2,\n    workers_per_gpu=2,\n    train=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_train2017.json',\n        img_prefix=data_root + 'train2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0.5,\n        with_mask=False,\n        with_crowd=False,\n        with_label=True),\n    val=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_val2017.json',\n        img_prefix=data_root + 'val2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0,\n        with_mask=False,\n        with_crowd=False,\n        with_label=True),\n    test=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_val2017.json',\n        img_prefix=data_root + 'val2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0,\n        with_mask=False,\n        with_crowd=False,\n        with_label=False,\n        test_mode=True))\n# optimizer\noptimizer = dict(type='SGD', lr=0.01, momentum=0.9, weight_decay=0.0001)\noptimizer_config = dict(grad_clip=dict(max_norm=35, norm_type=2))\n# learning policy\nlr_config = dict(\n    policy='step',\n    warmup='linear',\n    warmup_iters=500,\n    warmup_ratio=1.0 / 3,\n    step=[16, 22])\ncheckpoint_config = dict(interval=1)\n# yapf:disable\nlog_config = dict(\n    interval=50,\n    hooks=[\n        dict(type='TextLoggerHook'),\n        # dict(type='TensorboardLoggerHook')\n    ])\n# yapf:enable\n# runtime settings\ntotal_epochs = 24\ndevice_ids = range(8)\ndist_params = dict(backend='nccl')\nlog_level = 'INFO'\nwork_dir = './work_dirs/retinanet_r50_fpn_2x_pretrain_gc_resnet50'\nload_from = None\nresume_from = None\nworkflow = [('train', 1)]\n"""
detection/local_configs/retinanet_r50_fpn_2x_pretrain_se_resnet50.py,0,"b""# model settings\nmodel = dict(\n    type='RetinaNet',\n    # pretrained='modelzoo://resnet50',\n    pretrained='/data/nfs_share/public/AS/pytorch-classification/checkpoints/imagenet/se_resnet50/model_best.pth.tar',\n    backbone=dict(\n        type='ResNetSE',\n        depth=50,\n        num_stages=4,\n        out_indices=(0, 1, 2, 3),\n        frozen_stages=1,\n        style='pytorch'),\n    neck=dict(\n        type='FPN',\n        in_channels=[256, 512, 1024, 2048],\n        out_channels=256,\n        start_level=1,\n        add_extra_convs=True,\n        num_outs=5),\n    bbox_head=dict(\n        type='RetinaHead',\n        num_classes=81,\n        in_channels=256,\n        stacked_convs=4,\n        feat_channels=256,\n        octave_base_scale=4,\n        scales_per_octave=3,\n        anchor_ratios=[0.5, 1.0, 2.0],\n        anchor_strides=[8, 16, 32, 64, 128],\n        target_means=[.0, .0, .0, .0],\n        target_stds=[1.0, 1.0, 1.0, 1.0]))\n# training and testing settings\ntrain_cfg = dict(\n    assigner=dict(\n        type='MaxIoUAssigner',\n        pos_iou_thr=0.5,\n        neg_iou_thr=0.4,\n        min_pos_iou=0,\n        ignore_iof_thr=-1),\n    smoothl1_beta=0.11,\n    gamma=2.0,\n    alpha=0.25,\n    allowed_border=-1,\n    pos_weight=-1,\n    debug=False)\ntest_cfg = dict(\n    nms_pre=1000,\n    min_bbox_size=0,\n    score_thr=0.05,\n    nms=dict(type='nms', iou_thr=0.5),\n    max_per_img=100)\n# dataset settings\ndataset_type = 'CocoDataset'\ndata_root = 'data/coco/'\nimg_norm_cfg = dict(\n    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)\ndata = dict(\n    imgs_per_gpu=2,\n    workers_per_gpu=2,\n    train=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_train2017.json',\n        img_prefix=data_root + 'train2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0.5,\n        with_mask=False,\n        with_crowd=False,\n        with_label=True),\n    val=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_val2017.json',\n        img_prefix=data_root + 'val2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0,\n        with_mask=False,\n        with_crowd=False,\n        with_label=True),\n    test=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_val2017.json',\n        img_prefix=data_root + 'val2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0,\n        with_mask=False,\n        with_crowd=False,\n        with_label=False,\n        test_mode=True))\n# optimizer\noptimizer = dict(type='SGD', lr=0.01, momentum=0.9, weight_decay=0.0001)\noptimizer_config = dict(grad_clip=dict(max_norm=35, norm_type=2))\n# learning policy\nlr_config = dict(\n    policy='step',\n    warmup='linear',\n    warmup_iters=500,\n    warmup_ratio=1.0 / 3,\n    step=[16, 22])\ncheckpoint_config = dict(interval=1)\n# yapf:disable\nlog_config = dict(\n    interval=50,\n    hooks=[\n        dict(type='TextLoggerHook'),\n        # dict(type='TensorboardLoggerHook')\n    ])\n# yapf:enable\n# runtime settings\ntotal_epochs = 24\ndevice_ids = range(8)\ndist_params = dict(backend='nccl')\nlog_level = 'INFO'\nwork_dir = './work_dirs/retinanet_r50_fpn_2x_pretrain_se_resnet50'\nload_from = None\nresume_from = None\nworkflow = [('train', 1)]\n"""
detection/local_configs/retinanet_r50_fpn_2x_pretrain_sge_resnet50.py,0,"b""# model settings\nmodel = dict(\n    type='RetinaNet',\n    # pretrained='modelzoo://resnet50',\n    pretrained='/home/xiangli/share1/pytorch-classification/checkpoints/imagenet/sge_resnet50/model_best.pth.tar',\n    backbone=dict(\n        type='ResNetSGE',\n        depth=50,\n        num_stages=4,\n        out_indices=(0, 1, 2, 3),\n        frozen_stages=1,\n        style='pytorch'),\n    neck=dict(\n        type='FPN',\n        in_channels=[256, 512, 1024, 2048],\n        out_channels=256,\n        start_level=1,\n        add_extra_convs=True,\n        num_outs=5),\n    bbox_head=dict(\n        type='RetinaHead',\n        num_classes=81,\n        in_channels=256,\n        stacked_convs=4,\n        feat_channels=256,\n        octave_base_scale=4,\n        scales_per_octave=3,\n        anchor_ratios=[0.5, 1.0, 2.0],\n        anchor_strides=[8, 16, 32, 64, 128],\n        target_means=[.0, .0, .0, .0],\n        target_stds=[1.0, 1.0, 1.0, 1.0]))\n# training and testing settings\ntrain_cfg = dict(\n    assigner=dict(\n        type='MaxIoUAssigner',\n        pos_iou_thr=0.5,\n        neg_iou_thr=0.4,\n        min_pos_iou=0,\n        ignore_iof_thr=-1),\n    smoothl1_beta=0.11,\n    gamma=2.0,\n    alpha=0.25,\n    allowed_border=-1,\n    pos_weight=-1,\n    debug=False)\ntest_cfg = dict(\n    nms_pre=1000,\n    min_bbox_size=0,\n    score_thr=0.05,\n    nms=dict(type='nms', iou_thr=0.5),\n    max_per_img=100)\n# dataset settings\ndataset_type = 'CocoDataset'\ndata_root = 'data/coco/'\nimg_norm_cfg = dict(\n    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)\ndata = dict(\n    imgs_per_gpu=2,\n    workers_per_gpu=2,\n    train=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_train2017.json',\n        img_prefix=data_root + 'train2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0.5,\n        with_mask=False,\n        with_crowd=False,\n        with_label=True),\n    val=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_val2017.json',\n        img_prefix=data_root + 'val2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0,\n        with_mask=False,\n        with_crowd=False,\n        with_label=True),\n    test=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_val2017.json',\n        img_prefix=data_root + 'val2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0,\n        with_mask=False,\n        with_crowd=False,\n        with_label=False,\n        test_mode=True))\n# optimizer\noptimizer = dict(type='SGD', lr=0.01, momentum=0.9, weight_decay=0.0001)\noptimizer_config = dict(grad_clip=dict(max_norm=35, norm_type=2))\n# learning policy\nlr_config = dict(\n    policy='step',\n    warmup='linear',\n    warmup_iters=500,\n    warmup_ratio=1.0 / 3,\n    step=[16, 22])\ncheckpoint_config = dict(interval=1)\n# yapf:disable\nlog_config = dict(\n    interval=50,\n    hooks=[\n        dict(type='TextLoggerHook'),\n        # dict(type='TensorboardLoggerHook')\n    ])\n# yapf:enable\n# runtime settings\ntotal_epochs = 24\ndevice_ids = range(8)\ndist_params = dict(backend='nccl')\nlog_level = 'INFO'\nwork_dir = './work_dirs/retinanet_r50_fpn_2x_pretrain_sge_resnet50'\nload_from = None\nresume_from = None\nworkflow = [('train', 1)]\n"""
detection/local_configs/retinanet_r50_fpn_2x_pretrain_sk_resnet50.py,0,"b""# model settings\nmodel = dict(\n    type='RetinaNet',\n    # pretrained='modelzoo://resnet50',\n    pretrained='/home/xiangli/share1/pytorch-classification/checkpoints/imagenet/sk_resnet50/model_best.pth.tar',\n    backbone=dict(\n        type='ResNetSK',\n        depth=50,\n        num_stages=4,\n        out_indices=(0, 1, 2, 3),\n        frozen_stages=1,\n        style='pytorch'),\n    neck=dict(\n        type='FPN',\n        in_channels=[256, 512, 1024, 2048],\n        out_channels=256,\n        start_level=1,\n        add_extra_convs=True,\n        num_outs=5),\n    bbox_head=dict(\n        type='RetinaHead',\n        num_classes=81,\n        in_channels=256,\n        stacked_convs=4,\n        feat_channels=256,\n        octave_base_scale=4,\n        scales_per_octave=3,\n        anchor_ratios=[0.5, 1.0, 2.0],\n        anchor_strides=[8, 16, 32, 64, 128],\n        target_means=[.0, .0, .0, .0],\n        target_stds=[1.0, 1.0, 1.0, 1.0]))\n# training and testing settings\ntrain_cfg = dict(\n    assigner=dict(\n        type='MaxIoUAssigner',\n        pos_iou_thr=0.5,\n        neg_iou_thr=0.4,\n        min_pos_iou=0,\n        ignore_iof_thr=-1),\n    smoothl1_beta=0.11,\n    gamma=2.0,\n    alpha=0.25,\n    allowed_border=-1,\n    pos_weight=-1,\n    debug=False)\ntest_cfg = dict(\n    nms_pre=1000,\n    min_bbox_size=0,\n    score_thr=0.05,\n    nms=dict(type='nms', iou_thr=0.5),\n    max_per_img=100)\n# dataset settings\ndataset_type = 'CocoDataset'\ndata_root = 'data/coco/'\nimg_norm_cfg = dict(\n    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)\ndata = dict(\n    imgs_per_gpu=1,\n    workers_per_gpu=2,\n    train=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_train2017.json',\n        img_prefix=data_root + 'train2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0.5,\n        with_mask=False,\n        with_crowd=False,\n        with_label=True),\n    val=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_val2017.json',\n        img_prefix=data_root + 'val2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0,\n        with_mask=False,\n        with_crowd=False,\n        with_label=True),\n    test=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_val2017.json',\n        img_prefix=data_root + 'val2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0,\n        with_mask=False,\n        with_crowd=False,\n        with_label=False,\n        test_mode=True))\n# optimizer\noptimizer = dict(type='SGD', lr=0.01, momentum=0.9, weight_decay=0.0001)\noptimizer_config = dict(grad_clip=dict(max_norm=35, norm_type=2))\n# learning policy\nlr_config = dict(\n    policy='step',\n    warmup='linear',\n    warmup_iters=500,\n    warmup_ratio=1.0 / 3,\n    step=[16, 22])\ncheckpoint_config = dict(interval=1)\n# yapf:disable\nlog_config = dict(\n    interval=50,\n    hooks=[\n        dict(type='TextLoggerHook'),\n        # dict(type='TensorboardLoggerHook')\n    ])\n# yapf:enable\n# runtime settings\ntotal_epochs = 24\ndevice_ids = range(8)\ndist_params = dict(backend='nccl')\nlog_level = 'INFO'\nwork_dir = './work_dirs/retinanet_r50_fpn_2x_pretrain_sk_resnet50'\nload_from = None\nresume_from = None\nworkflow = [('train', 1)]\n"""
detection/mmdet/__init__.py,0,"b""from .version import __version__, short_version\n\n__all__ = ['__version__', 'short_version']\n"""
detection/mmdet/version.py,0,"b""# GENERATED VERSION FILE\n# TIME: Wed Jun  5 06:01:46 2019\n\n__version__ = '0.6.rc0+edb0393'\nshort_version = '0.6.rc0'\n"""
detection/tools/__init__.py,0,b''
detection/tools/coco_eval.py,0,"b""from argparse import ArgumentParser\n\nfrom mmdet.core import coco_eval\n\n\ndef main():\n    parser = ArgumentParser(description='COCO Evaluation')\n    parser.add_argument('result', help='result file path')\n    parser.add_argument('--ann', help='annotation file path')\n    parser.add_argument(\n        '--types',\n        type=str,\n        nargs='+',\n        choices=['proposal_fast', 'proposal', 'bbox', 'segm', 'keypoint'],\n        default=['bbox'],\n        help='result types')\n    parser.add_argument(\n        '--max-dets',\n        type=int,\n        nargs='+',\n        default=[100, 300, 1000],\n        help='proposal numbers, only used for recall evaluation')\n    args = parser.parse_args()\n    coco_eval(args.result, args.types, args.ann, args.max_dets)\n\n\nif __name__ == '__main__':\n    main()\n"""
detection/tools/test.py,2,"b""import argparse\n\nimport torch\nimport mmcv\nfrom mmcv.runner import load_checkpoint, parallel_test, obj_from_dict\nfrom mmcv.parallel import scatter, collate, MMDataParallel\n\nfrom mmdet import datasets\nfrom mmdet.core import results2json, coco_eval\nfrom mmdet.datasets import build_dataloader\nfrom mmdet.models import build_detector, detectors\n\n\ndef single_test(model, data_loader, show=False):\n    model.eval()\n    results = []\n    dataset = data_loader.dataset\n    prog_bar = mmcv.ProgressBar(len(dataset))\n    for i, data in enumerate(data_loader):\n        with torch.no_grad():\n            result = model(return_loss=False, rescale=not show, **data)\n        results.append(result)\n\n        if show:\n            model.module.show_result(data, result, dataset.img_norm_cfg,\n                                     dataset=dataset.CLASSES)\n\n        batch_size = data['img'][0].size(0)\n        for _ in range(batch_size):\n            prog_bar.update()\n    return results\n\n\ndef _data_func(data, device_id):\n    data = scatter(collate([data], samples_per_gpu=1), [device_id])[0]\n    return dict(return_loss=False, rescale=True, **data)\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser(description='MMDet test detector')\n    parser.add_argument('config', help='test config file path')\n    parser.add_argument('checkpoint', help='checkpoint file')\n    parser.add_argument(\n        '--gpus', default=1, type=int, help='GPU number used for testing')\n    parser.add_argument(\n        '--proc_per_gpu',\n        default=1,\n        type=int,\n        help='Number of processes per GPU')\n    parser.add_argument('--out', help='output result file')\n    parser.add_argument(\n        '--eval',\n        type=str,\n        nargs='+',\n        choices=['proposal', 'proposal_fast', 'bbox', 'segm', 'keypoints'],\n        help='eval types')\n    parser.add_argument('--show', action='store_true', help='show results')\n    args = parser.parse_args()\n    return args\n\n\ndef main():\n    args = parse_args()\n\n    if args.out is not None and not args.out.endswith(('.pkl', '.pickle')):\n        raise ValueError('The output file must be a pkl file.')\n\n    cfg = mmcv.Config.fromfile(args.config)\n    # set cudnn_benchmark\n    if cfg.get('cudnn_benchmark', False):\n        torch.backends.cudnn.benchmark = True\n    cfg.model.pretrained = None\n    cfg.data.test.test_mode = True\n\n    dataset = obj_from_dict(cfg.data.test, datasets, dict(test_mode=True))\n    if args.gpus == 1:\n        model = build_detector(\n            cfg.model, train_cfg=None, test_cfg=cfg.test_cfg)\n        load_checkpoint(model, args.checkpoint)\n        model = MMDataParallel(model, device_ids=[0])\n\n        data_loader = build_dataloader(\n            dataset,\n            imgs_per_gpu=1,\n            workers_per_gpu=cfg.data.workers_per_gpu,\n            num_gpus=1,\n            dist=False,\n            shuffle=False)\n        outputs = single_test(model, data_loader, args.show)\n    else:\n        model_args = cfg.model.copy()\n        model_args.update(train_cfg=None, test_cfg=cfg.test_cfg)\n        model_type = getattr(detectors, model_args.pop('type'))\n        outputs = parallel_test(\n            model_type,\n            model_args,\n            args.checkpoint,\n            dataset,\n            _data_func,\n            range(args.gpus),\n            workers_per_gpu=args.proc_per_gpu)\n\n    if args.out:\n        print('writing results to {}'.format(args.out))\n        mmcv.dump(outputs, args.out)\n        eval_types = args.eval\n        if eval_types:\n            print('Starting evaluate {}'.format(' and '.join(eval_types)))\n            if eval_types == ['proposal_fast']:\n                result_file = args.out\n                coco_eval(result_file, eval_types, dataset.coco)\n            else:\n                if not isinstance(outputs[0], dict):\n                    result_file = args.out + '.json'\n                    results2json(dataset, outputs, result_file)\n                    coco_eval(result_file, eval_types, dataset.coco)\n                else:\n                    for name in outputs[0]:\n                        print('\\nEvaluating {}'.format(name))\n                        outputs_ = [out[name] for out in outputs]\n                        result_file = args.out + '.{}.json'.format(name)\n                        results2json(dataset, outputs_, result_file)\n                        coco_eval(result_file, eval_types, dataset.coco)\n\n\nif __name__ == '__main__':\n    main()\n"""
detection/tools/train.py,1,"b""from __future__ import division\n\nimport argparse\nfrom mmcv import Config\n\nfrom mmdet import __version__\nfrom mmdet.datasets import get_dataset\nfrom mmdet.apis import (train_detector, init_dist, get_root_logger,\n                        set_random_seed)\nfrom mmdet.models import build_detector\nimport torch\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser(description='Train a detector')\n    parser.add_argument('config', help='train config file path')\n    parser.add_argument('--work_dir', help='the dir to save logs and models')\n    parser.add_argument(\n        '--resume_from', help='the checkpoint file to resume from')\n    parser.add_argument(\n        '--validate',\n        action='store_true',\n        #default=True,\n        help='whether to evaluate the checkpoint during training')\n    parser.add_argument(\n        '--gpus',\n        type=int,\n        default=1,\n        help='number of gpus to use '\n        '(only applicable to non-distributed training)')\n    parser.add_argument('--seed', type=int, default=None, help='random seed')\n    parser.add_argument(\n        '--launcher',\n        choices=['none', 'pytorch', 'slurm', 'mpi'],\n        default='none',\n        help='job launcher')\n    parser.add_argument('--local_rank', type=int, default=0)\n    args = parser.parse_args()\n\n    return args\n\n\ndef main():\n    args = parse_args()\n\n    cfg = Config.fromfile(args.config)\n    # set cudnn_benchmark\n    if cfg.get('cudnn_benchmark', False):\n        torch.backends.cudnn.benchmark = True\n    # update configs according to CLI args\n    if args.work_dir is not None:\n        cfg.work_dir = args.work_dir\n    if args.resume_from is not None:\n        cfg.resume_from = args.resume_from\n\n    #cfg.resume_from = 'work_dirs/retinanet_r101_fpn_2x_pretrain_cbam_resnet101/epoch_9.pth'\n    #print('resume from', cfg.resume_from)\n\n    cfg.gpus = args.gpus\n    if cfg.checkpoint_config is not None:\n        # save mmdet version in checkpoints as meta data\n        cfg.checkpoint_config.meta = dict(\n            mmdet_version=__version__, config=cfg.text)\n\n    # init distributed env first, since logger depends on the dist info.\n    if args.launcher == 'none':\n        distributed = False\n    else:\n        distributed = True\n        init_dist(args.launcher, **cfg.dist_params)\n\n    # init logger before other steps\n    logger = get_root_logger(cfg.log_level)\n    logger.info('Distributed training: {}'.format(distributed))\n\n    # set random seeds\n    if args.seed is not None:\n        logger.info('Set random seed to {}'.format(args.seed))\n        set_random_seed(args.seed)\n\n    model = build_detector(\n        cfg.model, train_cfg=cfg.train_cfg, test_cfg=cfg.test_cfg)\n\n    train_dataset = get_dataset(cfg.data.train)\n    train_detector(model, train_dataset, cfg, distributed=distributed, validate=args.validate, logger=logger)\n\n\nif __name__ == '__main__':\n    main()\n"""
detection/tools/voc_eval.py,0,"b""from argparse import ArgumentParser\n\nimport mmcv\nimport numpy as np\n\nfrom mmdet import datasets\nfrom mmdet.core import eval_map\n\n\ndef voc_eval(result_file, dataset, iou_thr=0.5):\n    det_results = mmcv.load(result_file)\n    gt_bboxes = []\n    gt_labels = []\n    gt_ignore = []\n    for i in range(len(dataset)):\n        ann = dataset.get_ann_info(i)\n        bboxes = ann['bboxes']\n        labels = ann['labels']\n        if 'bboxes_ignore' in ann:\n            ignore = np.concatenate([\n                np.zeros(bboxes.shape[0], dtype=np.bool),\n                np.ones(ann['bboxes_ignore'].shape[0], dtype=np.bool)\n            ])\n            gt_ignore.append(ignore)\n            bboxes = np.vstack([bboxes, ann['bboxes_ignore']])\n            labels = np.concatenate([labels, ann['labels_ignore']])\n        gt_bboxes.append(bboxes)\n        gt_labels.append(labels)\n    if not gt_ignore:\n        gt_ignore = gt_ignore\n    if hasattr(dataset, 'year') and dataset.year == 2007:\n        dataset_name = 'voc07'\n    else:\n        dataset_name = dataset.CLASSES\n    eval_map(\n        det_results,\n        gt_bboxes,\n        gt_labels,\n        gt_ignore=gt_ignore,\n        scale_ranges=None,\n        iou_thr=iou_thr,\n        dataset=dataset_name,\n        print_summary=True)\n\n\ndef main():\n    parser = ArgumentParser(description='VOC Evaluation')\n    parser.add_argument('result', help='result file path')\n    parser.add_argument('config', help='config file path')\n    parser.add_argument(\n        '--iou-thr',\n        type=float,\n        default=0.5,\n        help='IoU threshold for evaluation')\n    args = parser.parse_args()\n    cfg = mmcv.Config.fromfile(args.config)\n    test_dataset = mmcv.runner.obj_from_dict(cfg.data.test, datasets)\n    voc_eval(args.result, test_dataset, args.iou_thr)\n\n\nif __name__ == '__main__':\n    main()\n"""
classification/models/imagenet/__init__.py,0,b'from __future__ import absolute_import\n\nfrom .resnet_old import *\nfrom .resnet_sge import *\nfrom .resnet_se  import *\nfrom .resnet_cbam  import *\nfrom .resnet_bam  import *\nfrom .resnet_sk  import *\n\nfrom .resnet_ws import *\n\nfrom .shufflenetv2 import *\n'
classification/models/imagenet/common_head.py,6,b'import torch.nn as nn\nimport torch.utils.model_zoo as model_zoo\nfrom torch.nn.parameter import Parameter\nimport torch\nimport torch.nn.functional as F\nfrom torch.nn import init\nfrom torch.autograd import Variable\nfrom collections import OrderedDict\nimport math\n\n'
classification/models/imagenet/resnet_bam.py,0,"b'from .common_head import *\n\n__all__  = [\'bam_resnet18\', \'bam_resnet34\', \'bam_resnet50\', \'bam_resnet101\']\n\nclass Flatten(nn.Module):\n    def forward(self, x):\n        return x.view(x.size(0), -1)\n\nclass ChannelGate(nn.Module):\n    def __init__(self, gate_channel, reduction_ratio=16, num_layers=1):\n        super(ChannelGate, self).__init__()\n        # self.gate_activation = gate_activation\n        self.gate_c = nn.Sequential()\n        self.gate_c.add_module( \'flatten\', Flatten() )\n        gate_channels = [gate_channel]\n        gate_channels += [gate_channel // reduction_ratio] * num_layers\n        gate_channels += [gate_channel]\n        for i in range( len(gate_channels) - 2 ):\n            self.gate_c.add_module( \'gate_c_fc_%d\'%i, nn.Linear(gate_channels[i], gate_channels[i+1]) )\n            self.gate_c.add_module( \'gate_c_bn_%d\'%(i+1), nn.BatchNorm1d(gate_channels[i+1]) )\n            self.gate_c.add_module( \'gate_c_relu_%d\'%(i+1), nn.ReLU() )\n        self.gate_c.add_module( \'gate_c_fc_final\', nn.Linear(gate_channels[-2], gate_channels[-1]) )\n    def forward(self, in_tensor):\n        avg_pool = F.avg_pool2d( in_tensor, in_tensor.size(2), stride=in_tensor.size(2) )\n        return self.gate_c( avg_pool ).unsqueeze(2).unsqueeze(3).expand_as(in_tensor)\n\nclass SpatialGate(nn.Module):\n    def __init__(self, gate_channel, reduction_ratio=16, dilation_conv_num=2, dilation_val=4):\n        super(SpatialGate, self).__init__()\n        self.gate_s = nn.Sequential()\n        self.gate_s.add_module( \'gate_s_conv_reduce0\', nn.Conv2d(gate_channel, gate_channel//reduction_ratio, kernel_size=1))\n        self.gate_s.add_module( \'gate_s_bn_reduce0\',\tnn.BatchNorm2d(gate_channel//reduction_ratio) )\n        self.gate_s.add_module( \'gate_s_relu_reduce0\',nn.ReLU() )\n        for i in range( dilation_conv_num ):\n            self.gate_s.add_module( \'gate_s_conv_di_%d\'%i, nn.Conv2d(gate_channel//reduction_ratio, gate_channel//reduction_ratio, kernel_size=3, \\\n\t\t\t\t\t\tpadding=dilation_val, dilation=dilation_val) )\n            self.gate_s.add_module( \'gate_s_bn_di_%d\'%i, nn.BatchNorm2d(gate_channel//reduction_ratio) )\n            self.gate_s.add_module( \'gate_s_relu_di_%d\'%i, nn.ReLU() )\n        self.gate_s.add_module( \'gate_s_conv_final\', nn.Conv2d(gate_channel//reduction_ratio, 1, kernel_size=1) )\n    def forward(self, in_tensor):\n        return self.gate_s( in_tensor ).expand_as(in_tensor)\n\nclass BAM(nn.Module):\n    def __init__(self, gate_channel):\n        super(BAM, self).__init__()\n        self.channel_att = ChannelGate(gate_channel)\n        self.spatial_att = SpatialGate(gate_channel)\n    def forward(self,in_tensor):\n        att = 1 + F.sigmoid( self.channel_att(in_tensor) * self.spatial_att(in_tensor) )\n        return att * in_tensor\n\ndef conv3x3(in_planes, out_planes, stride=1):\n    ""3x3 convolution with padding""\n    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n                     padding=1, bias=False)\n\nclass BasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None, use_cbam=False):\n        super(BasicBlock, self).__init__()\n        self.conv1 = conv3x3(inplanes, planes, stride)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = conv3x3(planes, planes)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.downsample = downsample\n        self.stride = stride\n\n        if use_cbam:\n            self.cbam = CBAM( planes, 16 )\n        else:\n            self.cbam = None\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        if not self.cbam is None:\n            out = self.cbam(out)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\nclass Bottleneck(nn.Module):\n    expansion = 4\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None, use_cbam=False):\n        super(Bottleneck, self).__init__()\n        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,\n                               padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)\n        self.bn3 = nn.BatchNorm2d(planes * 4)\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = downsample\n        self.stride = stride\n\n        if use_cbam:\n            self.cbam = CBAM( planes * 4, 16 )\n        else:\n            self.cbam = None\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        if not self.cbam is None:\n            out = self.cbam(out)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\nclass ResNet(nn.Module):\n    def __init__(self, block, layers,  network_type, num_classes, att_type=None):\n        self.inplanes = 64\n        super(ResNet, self).__init__()\n        self.network_type = network_type\n        # different model config between ImageNet and CIFAR \n        if network_type == ""ImageNet"":\n            self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n            self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n            self.avgpool = nn.AvgPool2d(7)\n        else:\n            self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n\n        self.bn1 = nn.BatchNorm2d(64)\n        self.relu = nn.ReLU(inplace=True)\n\n        if att_type==\'BAM\':\n            self.bam1 = BAM(64*block.expansion)\n            self.bam2 = BAM(128*block.expansion)\n            self.bam3 = BAM(256*block.expansion)\n        else:\n            self.bam1, self.bam2, self.bam3 = None, None, None\n\n        self.layer1 = self._make_layer(block, 64,  layers[0], att_type=att_type)\n        self.layer2 = self._make_layer(block, 128, layers[1], stride=2, att_type=att_type)\n        self.layer3 = self._make_layer(block, 256, layers[2], stride=2, att_type=att_type)\n        self.layer4 = self._make_layer(block, 512, layers[3], stride=2, att_type=att_type)\n\n        self.fc = nn.Linear(512 * block.expansion, num_classes)\n\n        init.kaiming_normal(self.fc.weight)\n        for key in self.state_dict():\n            if key.split(\'.\')[-1]==""weight"":\n                if ""conv"" in key:\n                    init.kaiming_normal(self.state_dict()[key], mode=\'fan_out\')\n                if ""bn"" in key:\n                    if ""SpatialGate"" in key:\n                        self.state_dict()[key][...] = 0\n                    else:\n                        self.state_dict()[key][...] = 1\n            elif key.split(""."")[-1]==\'bias\':\n                self.state_dict()[key][...] = 0\n\n    def _make_layer(self, block, planes, blocks, stride=1, att_type=None):\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                nn.Conv2d(self.inplanes, planes * block.expansion,\n                          kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(planes * block.expansion),\n            )\n\n        layers = []\n        layers.append(block(self.inplanes, planes, stride, downsample, use_cbam=att_type==\'CBAM\'))\n        self.inplanes = planes * block.expansion\n        for i in range(1, blocks):\n            layers.append(block(self.inplanes, planes, use_cbam=att_type==\'CBAM\'))\n\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        if self.network_type == ""ImageNet"":\n            x = self.maxpool(x)\n\n        x = self.layer1(x)\n        if not self.bam1 is None:\n            x = self.bam1(x)\n\n        x = self.layer2(x)\n        if not self.bam2 is None:\n            x = self.bam2(x)\n\n        x = self.layer3(x)\n        if not self.bam3 is None:\n            x = self.bam3(x)\n\n        x = self.layer4(x)\n\n        if self.network_type == ""ImageNet"":\n            x = self.avgpool(x)\n        else:\n            x = F.avg_pool2d(x, 4)\n        x = x.view(x.size(0), -1)\n        x = self.fc(x)\n        return x\n\ndef ResidualNet(network_type, depth, num_classes, att_type):\n    assert network_type in [""ImageNet"", ""CIFAR10"", ""CIFAR100""], ""network type should be ImageNet or CIFAR10 / CIFAR100""\n    assert depth in [18, 34, 50, 101], \'network depth should be 18, 34, 50 or 101\'\n\n    if depth == 18:\n        model = ResNet(BasicBlock, [2, 2, 2, 2], network_type, num_classes, att_type)\n\n    elif depth == 34:\n        model = ResNet(BasicBlock, [3, 4, 6, 3], network_type, num_classes, att_type)\n\n    elif depth == 50:\n        model = ResNet(Bottleneck, [3, 4, 6, 3], network_type, num_classes, att_type)\n\n    elif depth == 101:\n        model = ResNet(Bottleneck, [3, 4, 23, 3], network_type, num_classes, att_type)\n\n    return model\n\n\ndef bam_resnet18(pretrained=False, **kwargs):\n    model = ResidualNet(\'ImageNet\', 18, 1000, \'BAM\')\n    return model\n\ndef bam_resnet34(pretrained=False, **kwargs):\n    model = ResidualNet(\'ImageNet\', 34, 1000, \'BAM\')\n    return model\n\ndef bam_resnet50(pretrained=False, **kwargs):\n    model = ResidualNet(\'ImageNet\', 50, 1000, \'BAM\')\n    return model\n\ndef bam_resnet101(pretrained=False, **kwargs):\n    model = ResidualNet(\'ImageNet\', 101, 1000, \'BAM\')\n    return model\n'"
classification/models/imagenet/resnet_cbam.py,1,"b'from .common_head import *\n\n__all__  = [\'cbam_resnet18\', \'cbam_resnet34\', \'cbam_resnet50\', \'cbam_resnet101\']\n\nclass BasicConv(nn.Module):\n    def __init__(self, in_planes, out_planes, kernel_size, stride=1, padding=0 \\\n            , dilation=1, groups=1, relu=True, bn=True, bias=False):\n        super(BasicConv, self).__init__()\n        self.out_channels = out_planes\n        self.conv = nn.Conv2d(in_planes, out_planes, kernel_size=kernel_size \\\n                , stride=stride, padding=padding, dilation=dilation \\\n                , groups=groups, bias=bias)\n        self.bn = nn.BatchNorm2d(out_planes,eps=1e-5, momentum=0.01 \\\n                , affine=True) if bn else None\n        self.relu = nn.ReLU() if relu else None\n\n    def forward(self, x):\n        x = self.conv(x)\n        if self.bn is not None:\n            x = self.bn(x)\n        if self.relu is not None:\n            x = self.relu(x)\n        return x\n\nclass Flatten(nn.Module):\n    def forward(self, x):\n        return x.view(x.size(0), -1)\n\nclass ChannelGate(nn.Module):\n    def __init__(self, gate_channels, reduction_ratio=16 \\\n            , pool_types=[\'avg\', \'max\']):\n        super(ChannelGate, self).__init__()\n        self.gate_channels = gate_channels\n        self.mlp = nn.Sequential(\n            Flatten(),\n            nn.Linear(gate_channels, gate_channels // reduction_ratio),\n            nn.ReLU(),\n            nn.Linear(gate_channels // reduction_ratio, gate_channels)\n            )\n        self.pool_types = pool_types\n        self.avgpool = nn.AdaptiveAvgPool2d(1)\n        self.maxpool = nn.AdaptiveMaxPool2d(1)\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n        channel_att_sum = None\n        for pool_type in self.pool_types:\n            if pool_type==\'avg\':\n                avg_pool = self.avgpool(x)\n                channel_att_raw = self.mlp( avg_pool )\n            elif pool_type==\'max\':\n                max_pool = self.maxpool(x)\n                channel_att_raw = self.mlp( max_pool )\n\n            if channel_att_sum is None:\n                channel_att_sum = channel_att_raw\n            else:\n                channel_att_sum = channel_att_sum + channel_att_raw\n\n        scale = self.sigmoid(channel_att_sum).unsqueeze(2) \\\n                .unsqueeze(3).expand_as(x)\n        return x * scale\n\n\nclass ChannelPool(nn.Module):\n    def forward(self, x):\n        return torch.cat( (torch.max(x,1)[0].unsqueeze(1), torch.mean(x,1) \\\n                .unsqueeze(1)), dim=1 )\n\nclass SpatialGate(nn.Module):\n    def __init__(self):\n        super(SpatialGate, self).__init__()\n        kernel_size = 7\n        self.compress = ChannelPool()\n        self.spatial = BasicConv(2, 1, kernel_size, stride=1 \\\n                , padding=(kernel_size-1) // 2, relu=False)\n        self.sigmoid = nn.Sigmoid()\n    def forward(self, x):\n        x_compress = self.compress(x)\n        x_out = self.spatial(x_compress)\n        scale = self.sigmoid(x_out) # broadcasting\n        return x * scale\n\nclass CBAM(nn.Module):\n    def __init__(self, gate_channels, reduction_ratio=16 \\\n            , pool_types=[\'avg\', \'max\'], no_spatial=False):\n        super(CBAM, self).__init__()\n        self.ChannelGate = ChannelGate(gate_channels, reduction_ratio, pool_types)\n        self.no_spatial=no_spatial\n        if not no_spatial:\n            self.SpatialGate = SpatialGate()\n    def forward(self, x):\n        x_out = self.ChannelGate(x)\n        if not self.no_spatial:\n            x_out = self.SpatialGate(x_out)\n        return x_out\n\n\n\n\ndef conv3x3(in_planes, out_planes, stride=1):\n    ""3x3 convolution with padding""\n    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n                     padding=1, bias=False)\n\nclass BasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None, use_cbam=False):\n        super(BasicBlock, self).__init__()\n        self.conv1 = conv3x3(inplanes, planes, stride)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = conv3x3(planes, planes)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.downsample = downsample\n        self.stride = stride\n\n        if use_cbam:\n            self.cbam = CBAM( planes, 16 )\n        else:\n            self.cbam = None\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        if not self.cbam is None:\n            out = self.cbam(out)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\nclass Bottleneck(nn.Module):\n    expansion = 4\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None, use_cbam=False):\n        super(Bottleneck, self).__init__()\n        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,\n                               padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)\n        self.bn3 = nn.BatchNorm2d(planes * 4)\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = downsample\n        self.stride = stride\n\n        if use_cbam:\n            self.cbam = CBAM( planes * 4, 16 )\n        else:\n            self.cbam = None\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        if not self.cbam is None:\n            out = self.cbam(out)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\nclass ResNet(nn.Module):\n    def __init__(self, block, layers,  network_type, num_classes, att_type=None):\n        self.inplanes = 64\n        super(ResNet, self).__init__()\n        self.network_type = network_type\n        # different model config between ImageNet and CIFAR \n        if network_type == ""ImageNet"":\n            self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n            self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n            self.avgpool = nn.AdaptiveAvgPool2d(1)\n        else:\n            self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n\n        self.bn1 = nn.BatchNorm2d(64)\n        self.relu = nn.ReLU(inplace=True)\n\n        if att_type==\'BAM\':\n            self.bam1 = BAM(64*block.expansion)\n            self.bam2 = BAM(128*block.expansion)\n            self.bam3 = BAM(256*block.expansion)\n        else:\n            self.bam1, self.bam2, self.bam3 = None, None, None\n\n        self.layer1 = self._make_layer(block, 64,  layers[0], att_type=att_type)\n        self.layer2 = self._make_layer(block, 128, layers[1], stride=2, att_type=att_type)\n        self.layer3 = self._make_layer(block, 256, layers[2], stride=2, att_type=att_type)\n        self.layer4 = self._make_layer(block, 512, layers[3], stride=2, att_type=att_type)\n\n        self.fc = nn.Linear(512 * block.expansion, num_classes)\n\n        init.kaiming_normal(self.fc.weight)\n        for key in self.state_dict():\n            if key.split(\'.\')[-1]==""weight"":\n                if ""conv"" in key:\n                    init.kaiming_normal(self.state_dict()[key], mode=\'fan_out\')\n                if ""bn"" in key:\n                    if ""SpatialGate"" in key:\n                        self.state_dict()[key][...] = 0\n                    else:\n                        self.state_dict()[key][...] = 1\n            elif key.split(""."")[-1]==\'bias\':\n                self.state_dict()[key][...] = 0\n\n    def _make_layer(self, block, planes, blocks, stride=1, att_type=None):\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                nn.Conv2d(self.inplanes, planes * block.expansion,\n                          kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(planes * block.expansion),\n            )\n\n        layers = []\n        layers.append(block(self.inplanes, planes, stride, downsample, use_cbam=att_type==\'CBAM\'))\n        self.inplanes = planes * block.expansion\n        for i in range(1, blocks):\n            layers.append(block(self.inplanes, planes, use_cbam=att_type==\'CBAM\'))\n\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        if self.network_type == ""ImageNet"":\n            x = self.maxpool(x)\n\n        x = self.layer1(x)\n        if not self.bam1 is None:\n            x = self.bam1(x)\n\n        x = self.layer2(x)\n        if not self.bam2 is None:\n            x = self.bam2(x)\n\n        x = self.layer3(x)\n        if not self.bam3 is None:\n            x = self.bam3(x)\n\n        x = self.layer4(x)\n\n        if self.network_type == ""ImageNet"":\n            x = self.avgpool(x)\n        else:\n            x = F.avg_pool2d(x, 4)\n        x = x.view(x.size(0), -1)\n        x = self.fc(x)\n        return x\n\ndef ResidualNet(network_type, depth, num_classes, att_type):\n    assert network_type in [""ImageNet"", ""CIFAR10"", ""CIFAR100""], ""network type should be ImageNet or CIFAR10 / CIFAR100""\n    assert depth in [18, 34, 50, 101], \'network depth should be 18, 34, 50 or 101\'\n\n    if depth == 18:\n        model = ResNet(BasicBlock, [2, 2, 2, 2], network_type, num_classes, att_type)\n\n    elif depth == 34:\n        model = ResNet(BasicBlock, [3, 4, 6, 3], network_type, num_classes, att_type)\n\n    elif depth == 50:\n        model = ResNet(Bottleneck, [3, 4, 6, 3], network_type, num_classes, att_type)\n\n    elif depth == 101:\n        model = ResNet(Bottleneck, [3, 4, 23, 3], network_type, num_classes, att_type)\n\n    return model\n\n\ndef cbam_resnet18(pretrained=False, **kwargs):\n    model = ResidualNet(\'ImageNet\', 18, 1000, \'CBAM\')\n    return model\n\ndef cbam_resnet34(pretrained=False, **kwargs):\n    model = ResidualNet(\'ImageNet\', 34, 1000, \'CBAM\')\n    return model\n\ndef cbam_resnet50(pretrained=False, **kwargs):\n    model = ResidualNet(\'ImageNet\', 50, 1000, \'CBAM\')\n    return model\n\ndef cbam_resnet101(pretrained=False, **kwargs):\n    model = ResidualNet(\'ImageNet\', 101, 1000, \'CBAM\')\n    return model\n'"
classification/models/imagenet/resnet_old.py,0,"b'from .common_head import *\n\n__all__ = [\'old_resnet18\', \'old_resnet34\', \'old_resnet50\', \'old_resnet101\',\n           \'old_resnet152\']\n\n\ndef conv3x3(in_planes, out_planes, stride=1):\n    """"""3x3 convolution with padding""""""\n    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n                     padding=1, bias=False)\n\n\ndef conv1x1(in_planes, out_planes, stride=1):\n    """"""1x1 convolution""""""\n    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n\n\nclass BasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None):\n        super(BasicBlock, self).__init__()\n        self.conv1 = conv3x3(inplanes, planes, stride)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = conv3x3(planes, planes)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        identity = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n\n        if self.downsample is not None:\n            identity = self.downsample(x)\n\n        out += identity\n        out = self.relu(out)\n\n        return out\n\n\nclass Bottleneck(nn.Module):\n    expansion = 4\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None):\n        super(Bottleneck, self).__init__()\n        self.conv1 = conv1x1(inplanes, planes)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.conv2 = conv3x3(planes, planes, stride)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.conv3 = conv1x1(planes, planes * self.expansion)\n        self.bn3 = nn.BatchNorm2d(planes * self.expansion)\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        identity = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        if self.downsample is not None:\n            identity = self.downsample(x)\n\n        out += identity\n        out = self.relu(out)\n\n        return out\n\n\nclass ResNet(nn.Module):\n\n    def __init__(self, block, layers, num_classes=1000, zero_init_residual=False):\n        super(ResNet, self).__init__()\n        self.inplanes = 64\n        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3,\n                               bias=False)\n        self.bn1 = nn.BatchNorm2d(64)\n        self.relu = nn.ReLU(inplace=True)\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n        self.layer1 = self._make_layer(block, 64, layers[0])\n        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n        self.fc = nn.Linear(512 * block.expansion, num_classes)\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight, mode=\'fan_out\', nonlinearity=\'relu\')\n            elif isinstance(m, nn.BatchNorm2d):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n\n        # Zero-initialize the last BN in each residual branch,\n        # so that the residual branch starts with zeros, and each residual block behaves like an identity.\n        # This improves the model by 0.2~0.3% according to https://arxiv.org/abs/1706.02677\n        if zero_init_residual:\n            for m in self.modules():\n                if isinstance(m, Bottleneck):\n                    nn.init.constant_(m.bn3.weight, 0)\n                elif isinstance(m, BasicBlock):\n                    nn.init.constant_(m.bn2.weight, 0)\n\n    def _make_layer(self, block, planes, blocks, stride=1):\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                conv1x1(self.inplanes, planes * block.expansion, stride),\n                nn.BatchNorm2d(planes * block.expansion),\n            )\n\n        layers = []\n        layers.append(block(self.inplanes, planes, stride, downsample))\n        self.inplanes = planes * block.expansion\n        for _ in range(1, blocks):\n            layers.append(block(self.inplanes, planes))\n\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n\n        x = self.avgpool(x)\n        x = x.view(x.size(0), -1)\n        x = self.fc(x)\n\n        return x\n\n\ndef old_resnet18(pretrained=False, **kwargs):\n    """"""Constructs a ResNet-18 model.\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = ResNet(BasicBlock, [2, 2, 2, 2], **kwargs)\n    return model\n\n\ndef old_resnet34(pretrained=False, **kwargs):\n    """"""Constructs a ResNet-34 model.\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = ResNet(BasicBlock, [3, 4, 6, 3], **kwargs)\n    return model\n\n\ndef old_resnet50(pretrained=False, **kwargs):\n    """"""Constructs a ResNet-50 model.\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = ResNet(Bottleneck, [3, 4, 6, 3], **kwargs)\n    return model\n\n\ndef old_resnet101(pretrained=False, **kwargs):\n    """"""Constructs a ResNet-101 model.\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = ResNet(Bottleneck, [3, 4, 23, 3], **kwargs)\n    return model\n\n\ndef old_resnet152(pretrained=False, **kwargs):\n    """"""Constructs a ResNet-152 model.\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = ResNet(Bottleneck, [3, 8, 36, 3], **kwargs)\n    return model\n'"
classification/models/imagenet/resnet_se.py,0,"b'from .common_head import *\n\n__all__ = [\'se_resnet18\', \'se_resnet34\', \'se_resnet50\', \'se_resnet101\', \'se_resnet152\']\n\nclass SELayer(nn.Module):\n    def __init__(self, channel, reduction = 16):\n        super(SELayer, self).__init__()\n        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n        self.fc       = nn.Sequential(\n                        nn.Linear(channel, channel // reduction),\n                        nn.ReLU(inplace = True),\n                        nn.Linear(channel // reduction, channel),\n                        nn.Sigmoid()\n                )\n\n    def forward(self, x):\n        b, c, _, _ = x.size()\n        y = self.avg_pool(x).view(b, c)\n        y = self.fc(y).view(b, c, 1, 1)\n        return x * y\n\ndef conv3x3(in_planes, out_planes, stride=1):\n    """"""3x3 convolution with padding""""""\n    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n                     padding=1, bias=False)\n\n\ndef conv1x1(in_planes, out_planes, stride=1):\n    """"""1x1 convolution""""""\n    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n\n\nclass BasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None):\n        super(BasicBlock, self).__init__()\n        self.conv1 = conv3x3(inplanes, planes, stride)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = conv3x3(planes, planes)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.downsample = downsample\n        self.stride = stride\n        self.se  = SELayer(planes)\n\n    def forward(self, x):\n        identity = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.se(out)\n\n        if self.downsample is not None:\n            identity = self.downsample(x)\n\n        out += identity\n        out = self.relu(out)\n\n        return out\n\n\nclass Bottleneck(nn.Module):\n    expansion = 4\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None):\n        super(Bottleneck, self).__init__()\n        self.conv1 = conv1x1(inplanes, planes)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.conv2 = conv3x3(planes, planes, stride)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.conv3 = conv1x1(planes, planes * self.expansion)\n        self.bn3 = nn.BatchNorm2d(planes * self.expansion)\n        self.se  = SELayer(planes * self.expansion)\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        identity = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n        out = self.se(out)\n\n        if self.downsample is not None:\n            identity = self.downsample(x)\n\n        out += identity\n        out = self.relu(out)\n\n        return out\n\n\nclass ResNet(nn.Module):\n\n    def __init__(self, block, layers, num_classes=1000, zero_init_residual=False):\n        super(ResNet, self).__init__()\n        self.inplanes = 64\n        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3,\n                               bias=False)\n        self.bn1 = nn.BatchNorm2d(64)\n        self.relu = nn.ReLU(inplace=True)\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n        self.layer1 = self._make_layer(block, 64, layers[0])\n        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n        self.fc = nn.Linear(512 * block.expansion, num_classes)\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight, mode=\'fan_out\', nonlinearity=\'relu\')\n            elif isinstance(m, nn.BatchNorm2d):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n\n        # Zero-initialize the last BN in each residual branch,\n        # so that the residual branch starts with zeros, and each residual block behaves like an identity.\n        # This improves the model by 0.2~0.3% according to https://arxiv.org/abs/1706.02677\n        if zero_init_residual:\n            for m in self.modules():\n                if isinstance(m, Bottleneck):\n                    nn.init.constant_(m.bn3.weight, 0)\n                elif isinstance(m, BasicBlock):\n                    nn.init.constant_(m.bn2.weight, 0)\n\n    def _make_layer(self, block, planes, blocks, stride=1):\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                conv1x1(self.inplanes, planes * block.expansion, stride),\n                nn.BatchNorm2d(planes * block.expansion),\n            )\n\n        layers = []\n        layers.append(block(self.inplanes, planes, stride, downsample))\n        self.inplanes = planes * block.expansion\n        for _ in range(1, blocks):\n            layers.append(block(self.inplanes, planes))\n\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n\n        x = self.avgpool(x)\n        x = x.view(x.size(0), -1)\n        x = self.fc(x)\n\n        return x\n\n\ndef se_resnet18(pretrained=False, **kwargs):\n    """"""Constructs a ResNet-18 model.\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = ResNet(BasicBlock, [2, 2, 2, 2], **kwargs)\n    return model\n\n\ndef se_resnet34(pretrained=False, **kwargs):\n    """"""Constructs a ResNet-34 model.\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = ResNet(BasicBlock, [3, 4, 6, 3], **kwargs)\n    return model\n\n\ndef se_resnet50(pretrained=False, **kwargs):\n    """"""Constructs a ResNet-50 model.\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = ResNet(Bottleneck, [3, 4, 6, 3], **kwargs)\n    return model\n\n\ndef se_resnet101(pretrained=False, **kwargs):\n    """"""Constructs a ResNet-101 model.\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = ResNet(Bottleneck, [3, 4, 23, 3], **kwargs)\n    return model\n\n\ndef se_resnet152(pretrained=False, **kwargs):\n    """"""Constructs a ResNet-152 model.\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = ResNet(Bottleneck, [3, 8, 36, 3], **kwargs)\n    return model\n\n'"
classification/models/imagenet/resnet_sge.py,2,"b'from .common_head import *\n\n__all__ = [\'sge_resnet18\', \'sge_resnet34\', \'sge_resnet50\', \'sge_resnet101\',\n           \'sge_resnet152\']\n\nclass SpatialGroupEnhance(nn.Module):\n    def __init__(self, groups = 64):\n        super(SpatialGroupEnhance, self).__init__()\n        self.groups   = groups\n        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n        self.weight   = Parameter(torch.zeros(1, groups, 1, 1))\n        self.bias     = Parameter(torch.ones(1, groups, 1, 1))\n        self.sig      = nn.Sigmoid()\n\n    def forward(self, x): # (b, c, h, w)\n        b, c, h, w = x.size()\n        x = x.view(b * self.groups, -1, h, w) \n        xn = x * self.avg_pool(x)\n        xn = xn.sum(dim=1, keepdim=True)\n        t = xn.view(b * self.groups, -1)\n        t = t - t.mean(dim=1, keepdim=True)\n        std = t.std(dim=1, keepdim=True) + 1e-5\n        t = t / std\n        t = t.view(b, self.groups, h, w)\n        t = t * self.weight + self.bias\n        t = t.view(b * self.groups, 1, h, w)\n        x = x * self.sig(t)\n        x = x.view(b, c, h, w)\n        return x\n\ndef conv3x3(in_planes, out_planes, stride=1):\n    """"""3x3 convolution with padding""""""\n    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n                     padding=1, bias=False)\n\n\ndef conv1x1(in_planes, out_planes, stride=1):\n    """"""1x1 convolution""""""\n    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n\n\nclass BasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None):\n        super(BasicBlock, self).__init__()\n        self.conv1 = conv3x3(inplanes, planes, stride)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = conv3x3(planes, planes)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.downsample = downsample\n        self.stride = stride\n        self.sge    = SpatialGroupEnhance(64)\n\n    def forward(self, x):\n        identity = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.sge(out)\n\n        if self.downsample is not None:\n            identity = self.downsample(x)\n\n        out += identity\n        out = self.relu(out)\n\n        return out\n\nclass Bottleneck(nn.Module):\n    expansion = 4\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None):\n        super(Bottleneck, self).__init__()\n        self.conv1 = conv1x1(inplanes, planes)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.conv2 = conv3x3(planes, planes, stride)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.conv3 = conv1x1(planes, planes * self.expansion)\n        self.bn3 = nn.BatchNorm2d(planes * self.expansion)\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = downsample\n        self.stride = stride\n        self.sge    = SpatialGroupEnhance(64)\n\n    def forward(self, x):\n        identity = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n        out = self.sge(out)\n\n        if self.downsample is not None:\n            identity = self.downsample(x)\n\n        out += identity\n        out = self.relu(out)\n\n        return out\n\n\n\n\nclass ResNet(nn.Module):\n\n    def __init__(self, block, layers, num_classes=1000, zero_init_residual=False):\n        super(ResNet, self).__init__()\n        self.inplanes = 64\n        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3,\n                               bias=False)\n        self.bn1 = nn.BatchNorm2d(64)\n        self.relu = nn.ReLU(inplace=True)\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n        self.layer1 = self._make_layer(block, 64, layers[0])\n        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n        self.fc = nn.Linear(512 * block.expansion, num_classes)\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight, mode=\'fan_out\', nonlinearity=\'relu\')\n            elif isinstance(m, nn.BatchNorm2d):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n\n        # Zero-initialize the last BN in each residual branch,\n        # so that the residual branch starts with zeros, and each residual block behaves like an identity.\n        # This improves the model by 0.2~0.3% according to https://arxiv.org/abs/1706.02677\n        if zero_init_residual:\n            for m in self.modules():\n                if isinstance(m, Bottleneck):\n                    nn.init.constant_(m.bn3.weight, 0)\n                elif isinstance(m, BasicBlock):\n                    nn.init.constant_(m.bn2.weight, 0)\n\n    def _make_layer(self, block, planes, blocks, stride=1):\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                conv1x1(self.inplanes, planes * block.expansion, stride),\n                nn.BatchNorm2d(planes * block.expansion),\n            )\n\n        layers = []\n        layers.append(block(self.inplanes, planes, stride, downsample))\n        self.inplanes = planes * block.expansion\n        for _ in range(1, blocks):\n            layers.append(block(self.inplanes, planes))\n\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n\n        x = self.avgpool(x)\n        x = x.view(x.size(0), -1)\n        x = self.fc(x)\n\n        return x\n\n\n\n\ndef sge_resnet18(pretrained=False, **kwargs):\n    """"""Constructs a ResNet-18 model.\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = ResNet(BasicBlock, [2, 2, 2, 2], **kwargs)\n    return model\n\n\ndef sge_resnet34(pretrained=False, **kwargs):\n    """"""Constructs a ResNet-34 model.\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = ResNet(BasicBlock, [3, 4, 6, 3], **kwargs)\n    return model\n\n\ndef sge_resnet50(pretrained=False, **kwargs):\n    """"""Constructs a ResNet-50 model.\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = ResNet(Bottleneck, [3, 4, 6, 3], **kwargs)\n    return model\n\n\ndef sge_resnet101(pretrained=False, **kwargs):\n    """"""Constructs a ResNet-101 model.\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = ResNet(Bottleneck, [3, 4, 23, 3], **kwargs)\n    return model\n\n\ndef sge_resnet152(pretrained=False, **kwargs):\n    """"""Constructs a ResNet-152 model.\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = ResNet(Bottleneck, [3, 8, 36, 3], **kwargs)\n    return model\n\n'"
classification/models/imagenet/resnet_sk.py,1,"b'from .common_head import *\n\n#TODO sknet for basicblocks\n\n__all__ = [\'sk_resnet18\', \'sk_resnet34\', \'sk_resnet50\', \'sk_resnet101\',\n           \'sk_resnet152\']\n\n\ndef conv3x3(in_planes, out_planes, stride=1, groups=1):\n    """"""3x3 convolution with padding""""""\n    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n                     padding=1, bias=False, groups=groups)\n\n\ndef conv1x1(in_planes, out_planes, stride=1):\n    """"""1x1 convolution""""""\n    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n\n\nclass BasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None):\n        super(BasicBlock, self).__init__()\n        self.conv1 = conv3x3(inplanes, planes, stride)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = conv3x3(planes, planes)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        identity = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n\n        if self.downsample is not None:\n            identity = self.downsample(x)\n\n        out += identity\n        out = self.relu(out)\n\n        return out\n\n\nclass Bottleneck(nn.Module):\n    expansion = 4\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None):\n        super(Bottleneck, self).__init__()\n        self.conv1 = conv1x1(inplanes, planes)\n        self.bn1 = nn.BatchNorm2d(planes)\n\n        self.conv2 = conv3x3(planes, planes, stride)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.conv2g = conv3x3(planes, planes, stride, groups = 32)\n        self.bn2g   = nn.BatchNorm2d(planes)\n\n        self.conv3 = conv1x1(planes, planes * self.expansion)\n        self.bn3 = nn.BatchNorm2d(planes * self.expansion)\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = downsample\n        self.stride = stride\n\n        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n        self.conv_fc1 = nn.Conv2d(planes, planes//16, 1, bias=False)\n        self.bn_fc1   = nn.BatchNorm2d(planes//16)\n        self.conv_fc2 = nn.Conv2d(planes//16, 2 * planes, 1, bias=False)\n\n        self.D = planes\n\n    def forward(self, x):\n        identity = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        d1 = self.conv2(out)\n        d1 = self.bn2(d1)\n        d1 = self.relu(d1)\n\n        d2 = self.conv2g(out)\n        d2 = self.bn2g(d2)\n        d2 = self.relu(d2)\n\n        d  = self.avg_pool(d1) + self.avg_pool(d2)\n        d = F.relu(self.bn_fc1(self.conv_fc1(d)))\n        d = self.conv_fc2(d)\n        d = torch.unsqueeze(d, 1).view(-1, 2, self.D, 1, 1)\n        d = F.softmax(d, 1)\n        d1 = d1 * d[:, 0, :, :, :].squeeze(1)\n        d2 = d2 * d[:, 1, :, :, :].squeeze(1)\n        d  = d1 + d2\n\n        out = self.conv3(d)\n        out = self.bn3(out)\n\n        if self.downsample is not None:\n            identity = self.downsample(x)\n\n        out += identity\n        out = self.relu(out)\n\n        return out\n\n\nclass ResNet(nn.Module):\n\n    def __init__(self, block, layers, num_classes=1000, zero_init_residual=False):\n        super(ResNet, self).__init__()\n        self.inplanes = 64\n        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3,\n                               bias=False)\n        self.bn1 = nn.BatchNorm2d(64)\n        self.relu = nn.ReLU(inplace=True)\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n        self.layer1 = self._make_layer(block, 64, layers[0])\n        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n        self.fc = nn.Linear(512 * block.expansion, num_classes)\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight, mode=\'fan_out\', nonlinearity=\'relu\')\n            elif isinstance(m, nn.BatchNorm2d):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n\n        # Zero-initialize the last BN in each residual branch,\n        # so that the residual branch starts with zeros, and each residual block behaves like an identity.\n        # This improves the model by 0.2~0.3% according to https://arxiv.org/abs/1706.02677\n        if zero_init_residual:\n            for m in self.modules():\n                if isinstance(m, Bottleneck):\n                    nn.init.constant_(m.bn3.weight, 0)\n                elif isinstance(m, BasicBlock):\n                    nn.init.constant_(m.bn2.weight, 0)\n\n    def _make_layer(self, block, planes, blocks, stride=1):\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                conv1x1(self.inplanes, planes * block.expansion, stride),\n                nn.BatchNorm2d(planes * block.expansion),\n            )\n\n        layers = []\n        layers.append(block(self.inplanes, planes, stride, downsample))\n        self.inplanes = planes * block.expansion\n        for _ in range(1, blocks):\n            layers.append(block(self.inplanes, planes))\n\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n\n        x = self.avgpool(x)\n        x = x.view(x.size(0), -1)\n        x = self.fc(x)\n\n        return x\n\n\ndef sk_resnet18(pretrained=False, **kwargs):\n    """"""Constructs a ResNet-18 model.\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = ResNet(BasicBlock, [2, 2, 2, 2], **kwargs)\n    return model\n\n\ndef sk_resnet34(pretrained=False, **kwargs):\n    """"""Constructs a ResNet-34 model.\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = ResNet(BasicBlock, [3, 4, 6, 3], **kwargs)\n    return model\n\n\ndef sk_resnet50(pretrained=False, **kwargs):\n    """"""Constructs a ResNet-50 model.\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = ResNet(Bottleneck, [3, 4, 6, 3], **kwargs)\n    return model\n\n\ndef sk_resnet101(pretrained=False, **kwargs):\n    """"""Constructs a ResNet-101 model.\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = ResNet(Bottleneck, [3, 4, 23, 3], **kwargs)\n    return model\n\ndef sk_resnet152(pretrained=False, **kwargs):\n    """"""Constructs a ResNet-152 model.\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = ResNet(Bottleneck, [3, 8, 36, 3], **kwargs)\n    return model\n\n'"
classification/models/imagenet/resnet_ws.py,0,"b'from .common_head import *\nfrom .. import layers as L\n\n\n__all__ = [\'ws_resnet18\', \'ws_resnet34\', \'ws_resnet50\', \'ws_resnet101\',\n           \'ws_resnet152\']\n\n\ndef conv3x3(in_planes, out_planes, stride=1):\n    """"""3x3 convolution with padding""""""\n    return L.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n                     padding=1, bias=False)\n\n\ndef conv1x1(in_planes, out_planes, stride=1):\n    """"""1x1 convolution""""""\n    return L.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n\n\nclass BasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None):\n        super(BasicBlock, self).__init__()\n        self.conv1 = conv3x3(inplanes, planes, stride)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = conv3x3(planes, planes)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        identity = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n\n        if self.downsample is not None:\n            identity = self.downsample(x)\n\n        out += identity\n        out = self.relu(out)\n\n        return out\n\n\nclass Bottleneck(nn.Module):\n    expansion = 4\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None):\n        super(Bottleneck, self).__init__()\n        self.conv1 = conv1x1(inplanes, planes)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.conv2 = conv3x3(planes, planes, stride)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.conv3 = conv1x1(planes, planes * self.expansion)\n        self.bn3 = nn.BatchNorm2d(planes * self.expansion)\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        identity = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        if self.downsample is not None:\n            identity = self.downsample(x)\n\n        out += identity\n        out = self.relu(out)\n\n        return out\n\n\nclass ResNet(nn.Module):\n\n    def __init__(self, block, layers, num_classes=1000, zero_init_residual=False):\n        super(ResNet, self).__init__()\n        self.inplanes = 64\n        self.conv1 = L.Conv2d(3, 64, kernel_size=7, stride=2, padding=3,\n                               bias=False)\n        self.bn1 = nn.BatchNorm2d(64)\n        self.relu = nn.ReLU(inplace=True)\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n        self.layer1 = self._make_layer(block, 64, layers[0])\n        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n        self.fc = nn.Linear(512 * block.expansion, num_classes)\n        #self.fc     = L.LConv2d(512 * block.expansion, num_classes, 1, bias=False)\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight, mode=\'fan_out\', nonlinearity=\'relu\')\n            elif isinstance(m, nn.BatchNorm2d):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n\n        # Zero-initialize the last BN in each residual branch,\n        # so that the residual branch starts with zeros, and each residual block behaves like an identity.\n        # This improves the model by 0.2~0.3% according to https://arxiv.org/abs/1706.02677\n        if zero_init_residual:\n            for m in self.modules():\n                if isinstance(m, Bottleneck):\n                    nn.init.constant_(m.bn3.weight, 0)\n                elif isinstance(m, BasicBlock):\n                    nn.init.constant_(m.bn2.weight, 0)\n\n    def _make_layer(self, block, planes, blocks, stride=1):\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                conv1x1(self.inplanes, planes * block.expansion, stride),\n                nn.BatchNorm2d(planes * block.expansion),\n            )\n\n        layers = []\n        layers.append(block(self.inplanes, planes, stride, downsample))\n        self.inplanes = planes * block.expansion\n        for _ in range(1, blocks):\n            layers.append(block(self.inplanes, planes))\n\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n\n        x = self.avgpool(x)\n        x = x.view(x.size(0), -1)\n        x = self.fc(x)\n\n        return x\n\n\ndef ws_resnet18(pretrained=False, **kwargs):\n    """"""Constructs a ResNet-18 model.\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = ResNet(BasicBlock, [2, 2, 2, 2], **kwargs)\n    return model\n\n\ndef ws_resnet34(pretrained=False, **kwargs):\n    """"""Constructs a ResNet-34 model.\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = ResNet(BasicBlock, [3, 4, 6, 3], **kwargs)\n    return model\n\n\ndef ws_resnet50(pretrained=False, **kwargs):\n    """"""Constructs a ResNet-50 model.\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = ResNet(Bottleneck, [3, 4, 6, 3], **kwargs)\n    return model\n\n\ndef ws_resnet101(pretrained=False, **kwargs):\n    """"""Constructs a ResNet-101 model.\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = ResNet(Bottleneck, [3, 4, 23, 3], **kwargs)\n    return model\n\n\ndef ws_resnet152(pretrained=False, **kwargs):\n    """"""Constructs a ResNet-152 model.\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = ResNet(Bottleneck, [3, 8, 36, 3], **kwargs)\n    return model\n\n'"
classification/models/imagenet/shufflenetv2.py,6,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nfrom collections import OrderedDict\nfrom torch.nn import init\nimport math\n\n__all__ = [\'shufflenetv2_1x\', \'shufflenetv2_05x\']\n\ndef conv_bn(inp, oup, stride):\n    return nn.Sequential(\n        nn.Conv2d(inp, oup, 3, stride, 1, bias=False),\n        nn.BatchNorm2d(oup),\n        nn.ReLU(inplace=True)\n    )\n\n\ndef conv_1x1_bn(inp, oup):\n    return nn.Sequential(\n        nn.Conv2d(inp, oup, 1, 1, 0, bias=False),\n        nn.BatchNorm2d(oup),\n        nn.ReLU(inplace=True)\n    )\n\ndef channel_shuffle(x, groups):\n    batchsize, num_channels, height, width = x.data.size()\n\n    channels_per_group = num_channels // groups\n    \n    # reshape\n    x = x.view(batchsize, groups, \n        channels_per_group, height, width)\n\n    x = torch.transpose(x, 1, 2).contiguous()\n\n    # flatten\n    x = x.view(batchsize, -1, height, width)\n\n    return x\n    \nclass InvertedResidual(nn.Module):\n    def __init__(self, inp, oup, stride, benchmodel):\n        super(InvertedResidual, self).__init__()\n        self.benchmodel = benchmodel\n        self.stride = stride\n        assert stride in [1, 2]\n\n        oup_inc = oup//2\n        \n        if self.benchmodel == 1:\n            #assert inp == oup_inc\n        \tself.banch2 = nn.Sequential(\n                # pw\n                nn.Conv2d(oup_inc, oup_inc, 1, 1, 0, bias=False),\n                nn.BatchNorm2d(oup_inc),\n                nn.ReLU(inplace=True),\n                # dw\n                nn.Conv2d(oup_inc, oup_inc, 3, stride, 1, groups=oup_inc, bias=False),\n                nn.BatchNorm2d(oup_inc),\n                # pw-linear\n                nn.Conv2d(oup_inc, oup_inc, 1, 1, 0, bias=False),\n                nn.BatchNorm2d(oup_inc),\n                nn.ReLU(inplace=True),\n            )                \n        else:                  \n            self.banch1 = nn.Sequential(\n                # dw\n                nn.Conv2d(inp, inp, 3, stride, 1, groups=inp, bias=False),\n                nn.BatchNorm2d(inp),\n                # pw-linear\n                nn.Conv2d(inp, oup_inc, 1, 1, 0, bias=False),\n                nn.BatchNorm2d(oup_inc),\n                nn.ReLU(inplace=True),\n            )        \n    \n            self.banch2 = nn.Sequential(\n                # pw\n                nn.Conv2d(inp, oup_inc, 1, 1, 0, bias=False),\n                nn.BatchNorm2d(oup_inc),\n                nn.ReLU(inplace=True),\n                # dw\n                nn.Conv2d(oup_inc, oup_inc, 3, stride, 1, groups=oup_inc, bias=False),\n                nn.BatchNorm2d(oup_inc),\n                # pw-linear\n                nn.Conv2d(oup_inc, oup_inc, 1, 1, 0, bias=False),\n                nn.BatchNorm2d(oup_inc),\n                nn.ReLU(inplace=True),\n            )\n          \n    @staticmethod\n    def _concat(x, out):\n        # concatenate along channel axis\n        return torch.cat((x, out), 1)        \n\n    def forward(self, x):\n        if 1==self.benchmodel:\n            x1 = x[:, :(x.shape[1]//2), :, :]\n            x2 = x[:, (x.shape[1]//2):, :, :]\n            out = self._concat(x1, self.banch2(x2))\n        elif 2==self.benchmodel:\n            out = self._concat(self.banch1(x), self.banch2(x))\n\n        return channel_shuffle(out, 2)\n\n\nclass ShuffleNetV2(nn.Module):\n    def __init__(self, n_class=1000, input_size=224, width_mult=1.):\n        super(ShuffleNetV2, self).__init__()\n        \n        assert input_size % 32 == 0\n        \n        self.stage_repeats = [4, 8, 4]\n        # index 0 is invalid and should never be called.\n        # only used for indexing convenience.\n        if width_mult == 0.5:\n            self.stage_out_channels = [-1, 24,  48,  96, 192, 1024]\n        elif width_mult == 1.0:\n            self.stage_out_channels = [-1, 24, 116, 232, 464, 1024]\n        elif width_mult == 1.5:\n            self.stage_out_channels = [-1, 24, 176, 352, 704, 1024]\n        elif width_mult == 2.0:\n            self.stage_out_channels = [-1, 24, 224, 488, 976, 2048]\n        else:\n            raise ValueError(\n                """"""{} groups is not supported for\n                       1x1 Grouped Convolutions"""""".format(num_groups))\n\n        # building first layer\n        input_channel = self.stage_out_channels[1]\n        self.conv1 = conv_bn(3, input_channel, 2)    \n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n        \n        self.features = []\n        # building inverted residual blocks\n        for idxstage in range(len(self.stage_repeats)):\n            numrepeat = self.stage_repeats[idxstage]\n            output_channel = self.stage_out_channels[idxstage+2]\n            for i in range(numrepeat):\n                if i == 0:\n\t            #inp, oup, stride, benchmodel):\n                    self.features.append(InvertedResidual(input_channel, output_channel, 2, 2))\n                else:\n                    self.features.append(InvertedResidual(input_channel, output_channel, 1, 1))\n                input_channel = output_channel\n                \n                \n        # make it nn.Sequential\n        self.features = nn.Sequential(*self.features)\n\n        # building last several layers\n        self.conv_last      = conv_1x1_bn(input_channel, self.stage_out_channels[-1])\n        self.globalpool = nn.Sequential(nn.AvgPool2d(int(input_size/32)))              \n    \n        # building classifier\n        self.classifier = nn.Sequential(nn.Linear(self.stage_out_channels[-1], n_class))\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.maxpool(x)\n        x = self.features(x)\n        x = self.conv_last(x)\n        x = self.globalpool(x)\n        x = x.view(-1, self.stage_out_channels[-1])\n        x = self.classifier(x)\n        return x\n\n\ndef shufflenetv2_1x(pretrained=False, **kwargs):\n    model = ShuffleNetV2(width_mult=1., **kwargs)\n    return model\n\n\ndef shufflenetv2_05x(pretrained=False, **kwargs):\n    model = ShuffleNetV2(width_mult=.5, **kwargs)\n    return model\n    \n'"
classification/models/imagenet/shufflenetv2_bng2.py,9,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nfrom collections import OrderedDict\nfrom torch.nn import init\nimport math\nfrom torch.nn.parameter import Parameter\n\n__all__ = [\'bng2_shufflenetv2_1x\', \'bng2_shufflenetv2_05x\']\n\n\nclass GBatchNorm2d(nn.BatchNorm2d):\n    def __init__(self, num_features, g=2):\n        super(GBatchNorm2d, self).__init__(num_features, affine=False)\n        self.weight = Parameter(torch.ones(num_features // g, 1))\n        self.bias   = Parameter(torch.zeros(num_features // g, 1))\n        self.d      = num_features // g\n        assert num_features % g == 0, \'%d / %d = %d error\' % (num_features, g, self.d)\n        self.g      = g\n\n    def expand(self, p):\n        p = p.expand(self.d, self.g).reshape(-1)\n        return p\n\n    def forward(self, input):\n        exponential_average_factor = 0.0\n\n        weight = self.expand(self.weight)\n        bias   = self.expand(self.bias)\n\n        if self.training and self.track_running_stats:\n            if self.num_batches_tracked is not None:\n                self.num_batches_tracked += 1\n                if self.momentum is None:  # use cumulative moving average\n                    exponential_average_factor = 1.0 / float(self.num_batches_tracked)\n                else:  # use exponential moving average\n                    exponential_average_factor = self.momentum\n\n        return F.batch_norm(\n                input, self.running_mean, self.running_var, weight, bias,\n                self.training or not self.track_running_stats,\n                exponential_average_factor, self.eps\n                )\n\ndef conv_bn(inp, oup, stride):\n    return nn.Sequential(\n        nn.Conv2d(inp, oup, 3, stride, 1, bias=False),\n        GBatchNorm2d(oup),\n        nn.ReLU(inplace=True)\n    )\n\n\ndef conv_1x1_bn(inp, oup):\n    return nn.Sequential(\n        nn.Conv2d(inp, oup, 1, 1, 0, bias=False),\n        GBatchNorm2d(oup),\n        nn.ReLU(inplace=True)\n    )\n\ndef channel_shuffle(x, groups):\n    batchsize, num_channels, height, width = x.data.size()\n\n    channels_per_group = num_channels // groups\n    \n    # reshape\n    x = x.view(batchsize, groups, \n        channels_per_group, height, width)\n\n    x = torch.transpose(x, 1, 2).contiguous()\n\n    # flatten\n    x = x.view(batchsize, -1, height, width)\n\n    return x\n    \nclass InvertedResidual(nn.Module):\n    def __init__(self, inp, oup, stride, benchmodel):\n        super(InvertedResidual, self).__init__()\n        self.benchmodel = benchmodel\n        self.stride = stride\n        assert stride in [1, 2]\n\n        oup_inc = oup//2\n        \n        if self.benchmodel == 1:\n            #assert inp == oup_inc\n        \tself.banch2 = nn.Sequential(\n                # pw\n                nn.Conv2d(oup_inc, oup_inc, 1, 1, 0, bias=False),\n                GBatchNorm2d(oup_inc),\n                nn.ReLU(inplace=True),\n                # dw\n                nn.Conv2d(oup_inc, oup_inc, 3, stride, 1, groups=oup_inc, bias=False),\n                GBatchNorm2d(oup_inc),\n                # pw-linear\n                nn.Conv2d(oup_inc, oup_inc, 1, 1, 0, bias=False),\n                GBatchNorm2d(oup_inc),\n                nn.ReLU(inplace=True),\n            )                \n        else:                  \n            self.banch1 = nn.Sequential(\n                # dw\n                nn.Conv2d(inp, inp, 3, stride, 1, groups=inp, bias=False),\n                GBatchNorm2d(inp),\n                # pw-linear\n                nn.Conv2d(inp, oup_inc, 1, 1, 0, bias=False),\n                GBatchNorm2d(oup_inc),\n                nn.ReLU(inplace=True),\n            )        \n    \n            self.banch2 = nn.Sequential(\n                # pw\n                nn.Conv2d(inp, oup_inc, 1, 1, 0, bias=False),\n                GBatchNorm2d(oup_inc),\n                nn.ReLU(inplace=True),\n                # dw\n                nn.Conv2d(oup_inc, oup_inc, 3, stride, 1, groups=oup_inc, bias=False),\n                GBatchNorm2d(oup_inc),\n                # pw-linear\n                nn.Conv2d(oup_inc, oup_inc, 1, 1, 0, bias=False),\n                GBatchNorm2d(oup_inc),\n                nn.ReLU(inplace=True),\n            )\n          \n    @staticmethod\n    def _concat(x, out):\n        # concatenate along channel axis\n        return torch.cat((x, out), 1)        \n\n    def forward(self, x):\n        if 1==self.benchmodel:\n            x1 = x[:, :(x.shape[1]//2), :, :]\n            x2 = x[:, (x.shape[1]//2):, :, :]\n            out = self._concat(x1, self.banch2(x2))\n        elif 2==self.benchmodel:\n            out = self._concat(self.banch1(x), self.banch2(x))\n\n        return channel_shuffle(out, 2)\n\n\nclass ShuffleNetV2(nn.Module):\n    def __init__(self, n_class=1000, input_size=224, width_mult=1.):\n        super(ShuffleNetV2, self).__init__()\n        \n        assert input_size % 32 == 0\n        \n        self.stage_repeats = [4, 8, 4]\n        # index 0 is invalid and should never be called.\n        # only used for indexing convenience.\n        if width_mult == 0.5:\n            self.stage_out_channels = [-1, 24,  48,  96, 192, 1024]\n        elif width_mult == 1.0:\n            self.stage_out_channels = [-1, 24, 116, 232, 464, 1024]\n        elif width_mult == 1.5:\n            self.stage_out_channels = [-1, 24, 176, 352, 704, 1024]\n        elif width_mult == 2.0:\n            self.stage_out_channels = [-1, 24, 224, 488, 976, 2048]\n        else:\n            raise ValueError(\n                """"""{} groups is not supported for\n                       1x1 Grouped Convolutions"""""".format(num_groups))\n\n        # building first layer\n        input_channel = self.stage_out_channels[1]\n        self.conv1 = conv_bn(3, input_channel, 2)    \n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n        \n        self.features = []\n        # building inverted residual blocks\n        for idxstage in range(len(self.stage_repeats)):\n            numrepeat = self.stage_repeats[idxstage]\n            output_channel = self.stage_out_channels[idxstage+2]\n            for i in range(numrepeat):\n                if i == 0:\n\t            #inp, oup, stride, benchmodel):\n                    self.features.append(InvertedResidual(input_channel, output_channel, 2, 2))\n                else:\n                    self.features.append(InvertedResidual(input_channel, output_channel, 1, 1))\n                input_channel = output_channel\n                \n                \n        # make it nn.Sequential\n        self.features = nn.Sequential(*self.features)\n\n        # building last several layers\n        self.conv_last      = conv_1x1_bn(input_channel, self.stage_out_channels[-1])\n        self.globalpool = nn.Sequential(nn.AvgPool2d(int(input_size/32)))              \n    \n        # building classifier\n        self.classifier = nn.Sequential(nn.Linear(self.stage_out_channels[-1], n_class))\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.maxpool(x)\n        x = self.features(x)\n        x = self.conv_last(x)\n        x = self.globalpool(x)\n        x = x.view(-1, self.stage_out_channels[-1])\n        x = self.classifier(x)\n        return x\n\n\ndef bng2_shufflenetv2_1x(pretrained=False, **kwargs):\n    model = ShuffleNetV2(width_mult=1., **kwargs)\n    return model\n\n\ndef bng2_shufflenetv2_05x(pretrained=False, **kwargs):\n    model = ShuffleNetV2(width_mult=.5, **kwargs)\n    return model\n    \n'"
classification/models/imagenet/shufflenetv2_gl4gbn.py,8,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nfrom collections import OrderedDict\nfrom torch.nn import init\nimport math\nfrom .common_head import *\n\n__all__ = [\'gl4gbn_shufflenetv2_1x\', \'gl4gbn_shufflenetv2_05x\']\n\nclass GL(nn.Module):\n    def __init__(self, c, g = 4):\n        super(GL, self).__init__()\n        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n        self.weight   = Parameter(torch.zeros(1, c//g)) #, groups, 1, 1))\n        self.bias     = Parameter(torch.ones(1, c//g)) #, groups, 1, 1))\n        self.sig      = nn.Sigmoid()\n        self.c        = c\n        self.g        = g\n        self.d        = c // g\n        self.bn       = nn.BatchNorm2d(1, affine=False)\n\n    def forward(self, x): # (b, c, h, w)\n        gx = self.avg_pool(x) # b, c, 1, 1\n        bx = self.bn(gx.view(-1, 1, self.c, 1)).view(-1, self.c, 1, 1)\n        weight = self.weight.expand(self.g, self.d).reshape(1, self.c, 1, 1)\n        bias   = self.bias.expand(self.g, self.d).reshape(1, self.c, 1, 1)\n        bx = bx * weight + bias\n        x  = x * self.sig(bx)\n        return x\n\ndef conv_bn(inp, oup, stride):\n    return nn.Sequential(\n        nn.Conv2d(inp, oup, 3, stride, 1, bias=False),\n        nn.BatchNorm2d(oup),\n        nn.ReLU(inplace=True)\n    )\n\n\ndef conv_1x1_bn(inp, oup):\n    return nn.Sequential(\n        nn.Conv2d(inp, oup, 1, 1, 0, bias=False),\n        nn.BatchNorm2d(oup),\n        nn.ReLU(inplace=True)\n    )\n\ndef channel_shuffle(x, groups):\n    batchsize, num_channels, height, width = x.data.size()\n\n    channels_per_group = num_channels // groups\n    \n    # reshape\n    x = x.view(batchsize, groups, \n        channels_per_group, height, width)\n\n    x = torch.transpose(x, 1, 2).contiguous()\n\n    # flatten\n    x = x.view(batchsize, -1, height, width)\n\n    return x\n    \nclass InvertedResidual(nn.Module):\n    def __init__(self, inp, oup, stride, benchmodel):\n        super(InvertedResidual, self).__init__()\n        self.benchmodel = benchmodel\n        self.stride = stride\n        assert stride in [1, 2]\n\n        oup_inc = oup//2\n        \n        if self.benchmodel == 1:\n            #assert inp == oup_inc\n        \tself.banch2 = nn.Sequential(\n                # pw\n                nn.Conv2d(oup_inc, oup_inc, 1, 1, 0, bias=False),\n                nn.BatchNorm2d(oup_inc),\n                nn.ReLU(inplace=True),\n                # dw\n                nn.Conv2d(oup_inc, oup_inc, 3, stride, 1, groups=oup_inc, bias=False),\n                nn.BatchNorm2d(oup_inc),\n                # pw-linear\n                nn.Conv2d(oup_inc, oup_inc, 1, 1, 0, bias=False),\n                nn.BatchNorm2d(oup_inc),\n                nn.ReLU(inplace=True),\n            )                \n        else:                  \n            self.banch1 = nn.Sequential(\n                # dw\n                nn.Conv2d(inp, inp, 3, stride, 1, groups=inp, bias=False),\n                nn.BatchNorm2d(inp),\n                # pw-linear\n                nn.Conv2d(inp, oup_inc, 1, 1, 0, bias=False),\n                nn.BatchNorm2d(oup_inc),\n                nn.ReLU(inplace=True),\n            )        \n    \n            self.banch2 = nn.Sequential(\n                # pw\n                nn.Conv2d(inp, oup_inc, 1, 1, 0, bias=False),\n                nn.BatchNorm2d(oup_inc),\n                nn.ReLU(inplace=True),\n                # dw\n                nn.Conv2d(oup_inc, oup_inc, 3, stride, 1, groups=oup_inc, bias=False),\n                nn.BatchNorm2d(oup_inc),\n                # pw-linear\n                nn.Conv2d(oup_inc, oup_inc, 1, 1, 0, bias=False),\n                nn.BatchNorm2d(oup_inc),\n                nn.ReLU(inplace=True),\n            )\n\n        self.gl = GL(oup)\n          \n    @staticmethod\n    def _concat(x, out):\n        # concatenate along channel axis\n        return torch.cat((x, out), 1)        \n\n    def forward(self, x):\n        if 1==self.benchmodel:\n            x1 = x[:, :(x.shape[1]//2), :, :]\n            x2 = x[:, (x.shape[1]//2):, :, :]\n            out = self._concat(x1, self.banch2(x2))\n        elif 2==self.benchmodel:\n            out = self._concat(self.banch1(x), self.banch2(x))\n        \n        out = self.gl(out)\n        return channel_shuffle(out, 2)\n\n\nclass ShuffleNetV2(nn.Module):\n    def __init__(self, n_class=1000, input_size=224, width_mult=1.):\n        super(ShuffleNetV2, self).__init__()\n        \n        assert input_size % 32 == 0\n        \n        self.stage_repeats = [4, 8, 4]\n        # index 0 is invalid and should never be called.\n        # only used for indexing convenience.\n        if width_mult == 0.5:\n            self.stage_out_channels = [-1, 24,  48,  96, 192, 1024]\n        elif width_mult == 1.0:\n            self.stage_out_channels = [-1, 24, 116, 232, 464, 1024]\n        elif width_mult == 1.5:\n            self.stage_out_channels = [-1, 24, 176, 352, 704, 1024]\n        elif width_mult == 2.0:\n            self.stage_out_channels = [-1, 24, 224, 488, 976, 2048]\n        else:\n            raise ValueError(\n                """"""{} groups is not supported for\n                       1x1 Grouped Convolutions"""""".format(num_groups))\n\n        # building first layer\n        input_channel = self.stage_out_channels[1]\n        self.conv1 = conv_bn(3, input_channel, 2)    \n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n        \n        self.features = []\n        # building inverted residual blocks\n        for idxstage in range(len(self.stage_repeats)):\n            numrepeat = self.stage_repeats[idxstage]\n            output_channel = self.stage_out_channels[idxstage+2]\n            for i in range(numrepeat):\n                if i == 0:\n\t            #inp, oup, stride, benchmodel):\n                    self.features.append(InvertedResidual(input_channel, output_channel, 2, 2))\n                else:\n                    self.features.append(InvertedResidual(input_channel, output_channel, 1, 1))\n                input_channel = output_channel\n                \n                \n        # make it nn.Sequential\n        self.features = nn.Sequential(*self.features)\n\n        # building last several layers\n        self.conv_last      = conv_1x1_bn(input_channel, self.stage_out_channels[-1])\n        self.globalpool = nn.Sequential(nn.AvgPool2d(int(input_size/32)))              \n    \n        # building classifier\n        self.classifier = nn.Sequential(nn.Linear(self.stage_out_channels[-1], n_class))\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.maxpool(x)\n        x = self.features(x)\n        x = self.conv_last(x)\n        x = self.globalpool(x)\n        x = x.view(-1, self.stage_out_channels[-1])\n        x = self.classifier(x)\n        return x\n\n\ndef gl4gbn_shufflenetv2_1x(pretrained=False, **kwargs):\n    model = ShuffleNetV2(width_mult=1., **kwargs)\n    return model\n\n\ndef gl4gbn_shufflenetv2_05x(pretrained=False, **kwargs):\n    model = ShuffleNetV2(width_mult=.5, **kwargs)\n    return model\n    \n'"
classification/models/imagenet/shufflenetv2_se.py,6,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nfrom collections import OrderedDict\nfrom torch.nn import init\nimport math\nfrom .common_head import *\n\n__all__ = [\'se_shufflenetv2_1x\', \'se_shufflenetv2_05x\']\n\n\nclass SELayer(nn.Module):\n    def __init__(self, channel, reduction = 16):\n        super(SELayer, self).__init__()\n        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n        self.fc       = nn.Sequential(\n                        nn.Linear(channel, channel // reduction),\n                        nn.ReLU(inplace = True),\n                        nn.Linear(channel // reduction, channel),\n                        nn.Sigmoid()\n                )\n\n    def forward(self, x):\n        b, c, _, _ = x.size()\n        y = self.avg_pool(x).view(b, c)\n        y = self.fc(y).view(b, c, 1, 1)\n        return x * y\n\n\ndef conv_bn(inp, oup, stride):\n    return nn.Sequential(\n        nn.Conv2d(inp, oup, 3, stride, 1, bias=False),\n        nn.BatchNorm2d(oup),\n        nn.ReLU(inplace=True)\n    )\n\n\ndef conv_1x1_bn(inp, oup):\n    return nn.Sequential(\n        nn.Conv2d(inp, oup, 1, 1, 0, bias=False),\n        nn.BatchNorm2d(oup),\n        nn.ReLU(inplace=True)\n    )\n\ndef channel_shuffle(x, groups):\n    batchsize, num_channels, height, width = x.data.size()\n\n    channels_per_group = num_channels // groups\n    \n    # reshape\n    x = x.view(batchsize, groups, \n        channels_per_group, height, width)\n\n    x = torch.transpose(x, 1, 2).contiguous()\n\n    # flatten\n    x = x.view(batchsize, -1, height, width)\n\n    return x\n    \nclass InvertedResidual(nn.Module):\n    def __init__(self, inp, oup, stride, benchmodel):\n        super(InvertedResidual, self).__init__()\n        self.benchmodel = benchmodel\n        self.stride = stride\n        assert stride in [1, 2]\n\n        oup_inc = oup//2\n        \n        if self.benchmodel == 1:\n            #assert inp == oup_inc\n        \tself.banch2 = nn.Sequential(\n                # pw\n                nn.Conv2d(oup_inc, oup_inc, 1, 1, 0, bias=False),\n                nn.BatchNorm2d(oup_inc),\n                nn.ReLU(inplace=True),\n                # dw\n                nn.Conv2d(oup_inc, oup_inc, 3, stride, 1, groups=oup_inc, bias=False),\n                nn.BatchNorm2d(oup_inc),\n                # pw-linear\n                nn.Conv2d(oup_inc, oup_inc, 1, 1, 0, bias=False),\n                nn.BatchNorm2d(oup_inc),\n                nn.ReLU(inplace=True),\n            )                \n        else:                  \n            self.banch1 = nn.Sequential(\n                # dw\n                nn.Conv2d(inp, inp, 3, stride, 1, groups=inp, bias=False),\n                nn.BatchNorm2d(inp),\n                # pw-linear\n                nn.Conv2d(inp, oup_inc, 1, 1, 0, bias=False),\n                nn.BatchNorm2d(oup_inc),\n                nn.ReLU(inplace=True),\n            )        \n    \n            self.banch2 = nn.Sequential(\n                # pw\n                nn.Conv2d(inp, oup_inc, 1, 1, 0, bias=False),\n                nn.BatchNorm2d(oup_inc),\n                nn.ReLU(inplace=True),\n                # dw\n                nn.Conv2d(oup_inc, oup_inc, 3, stride, 1, groups=oup_inc, bias=False),\n                nn.BatchNorm2d(oup_inc),\n                # pw-linear\n                nn.Conv2d(oup_inc, oup_inc, 1, 1, 0, bias=False),\n                nn.BatchNorm2d(oup_inc),\n                nn.ReLU(inplace=True),\n            )\n\n        self.se = SELayer(oup)\n          \n    @staticmethod\n    def _concat(x, out):\n        # concatenate along channel axis\n        return torch.cat((x, out), 1)        \n\n    def forward(self, x):\n        if 1==self.benchmodel:\n            x1 = x[:, :(x.shape[1]//2), :, :]\n            x2 = x[:, (x.shape[1]//2):, :, :]\n            out = self._concat(x1, self.banch2(x2))\n        elif 2==self.benchmodel:\n            out = self._concat(self.banch1(x), self.banch2(x))\n        \n        out = self.se(out)\n        return channel_shuffle(out, 2)\n\n\nclass ShuffleNetV2(nn.Module):\n    def __init__(self, n_class=1000, input_size=224, width_mult=1.):\n        super(ShuffleNetV2, self).__init__()\n        \n        assert input_size % 32 == 0\n        \n        self.stage_repeats = [4, 8, 4]\n        # index 0 is invalid and should never be called.\n        # only used for indexing convenience.\n        if width_mult == 0.5:\n            self.stage_out_channels = [-1, 24,  48,  96, 192, 1024]\n        elif width_mult == 1.0:\n            self.stage_out_channels = [-1, 24, 116, 232, 464, 1024]\n        elif width_mult == 1.5:\n            self.stage_out_channels = [-1, 24, 176, 352, 704, 1024]\n        elif width_mult == 2.0:\n            self.stage_out_channels = [-1, 24, 224, 488, 976, 2048]\n        else:\n            raise ValueError(\n                """"""{} groups is not supported for\n                       1x1 Grouped Convolutions"""""".format(num_groups))\n\n        # building first layer\n        input_channel = self.stage_out_channels[1]\n        self.conv1 = conv_bn(3, input_channel, 2)    \n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n        \n        self.features = []\n        # building inverted residual blocks\n        for idxstage in range(len(self.stage_repeats)):\n            numrepeat = self.stage_repeats[idxstage]\n            output_channel = self.stage_out_channels[idxstage+2]\n            for i in range(numrepeat):\n                if i == 0:\n\t            #inp, oup, stride, benchmodel):\n                    self.features.append(InvertedResidual(input_channel, output_channel, 2, 2))\n                else:\n                    self.features.append(InvertedResidual(input_channel, output_channel, 1, 1))\n                input_channel = output_channel\n                \n                \n        # make it nn.Sequential\n        self.features = nn.Sequential(*self.features)\n\n        # building last several layers\n        self.conv_last      = conv_1x1_bn(input_channel, self.stage_out_channels[-1])\n        self.globalpool = nn.Sequential(nn.AvgPool2d(int(input_size/32)))              \n    \n        # building classifier\n        self.classifier = nn.Sequential(nn.Linear(self.stage_out_channels[-1], n_class))\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.maxpool(x)\n        x = self.features(x)\n        x = self.conv_last(x)\n        x = self.globalpool(x)\n        x = x.view(-1, self.stage_out_channels[-1])\n        x = self.classifier(x)\n        return x\n\n\ndef se_shufflenetv2_1x(pretrained=False, **kwargs):\n    model = ShuffleNetV2(width_mult=1., **kwargs)\n    return model\n\n\ndef se_shufflenetv2_05x(pretrained=False, **kwargs):\n    model = ShuffleNetV2(width_mult=.5, **kwargs)\n    return model\n    \n'"
classification/utils/progress/setup.py,0,"b""#!/usr/bin/env python\n\nfrom setuptools import setup\n\nimport progress\n\n\nsetup(\n    name='progress',\n    version=progress.__version__,\n    description='Easy to use progress bars',\n    long_description=open('README.rst').read(),\n    author='Giorgos Verigakis',\n    author_email='verigak@gmail.com',\n    url='http://github.com/verigak/progress/',\n    license='ISC',\n    packages=['progress'],\n    classifiers=[\n        'Environment :: Console',\n        'Intended Audience :: Developers',\n        'License :: OSI Approved :: ISC License (ISCL)',\n        'Programming Language :: Python :: 2.6',\n        'Programming Language :: Python :: 2.7',\n        'Programming Language :: Python :: 3.3',\n        'Programming Language :: Python :: 3.4',\n        'Programming Language :: Python :: 3.5',\n        'Programming Language :: Python :: 3.6',\n    ]\n)\n"""
classification/utils/progress/test_progress.py,0,"b""#!/usr/bin/env python\n\nfrom __future__ import print_function\n\nimport random\nimport time\n\nfrom progress.bar import (Bar, ChargingBar, FillingSquaresBar,\n                          FillingCirclesBar, IncrementalBar, PixelBar,\n                          ShadyBar)\nfrom progress.spinner import (Spinner, PieSpinner, MoonSpinner, LineSpinner,\n                              PixelSpinner)\nfrom progress.counter import Counter, Countdown, Stack, Pie\n\n\ndef sleep():\n    t = 0.01\n    t += t * random.uniform(-0.1, 0.1)  # Add some variance\n    time.sleep(t)\n\n\nfor bar_cls in (Bar, ChargingBar, FillingSquaresBar, FillingCirclesBar):\n    suffix = '%(index)d/%(max)d [%(elapsed)d / %(eta)d / %(eta_td)s]'\n    bar = bar_cls(bar_cls.__name__, suffix=suffix)\n    for i in bar.iter(range(200)):\n        sleep()\n\nfor bar_cls in (IncrementalBar, PixelBar, ShadyBar):\n    suffix = '%(percent)d%% [%(elapsed_td)s / %(eta)d / %(eta_td)s]'\n    bar = bar_cls(bar_cls.__name__, suffix=suffix)\n    for i in bar.iter(range(200)):\n        sleep()\n\nfor spin in (Spinner, PieSpinner, MoonSpinner, LineSpinner, PixelSpinner):\n    for i in spin(spin.__name__ + ' ').iter(range(100)):\n        sleep()\n    print()\n\nfor singleton in (Counter, Countdown, Stack, Pie):\n    for i in singleton(singleton.__name__ + ' ').iter(range(100)):\n        sleep()\n    print()\n\nbar = IncrementalBar('Random', suffix='%(index)d')\nfor i in range(100):\n    bar.goto(random.randint(0, 100))\n    sleep()\nbar.finish()\n"""
detection/.eggs/pytest_runner-5.1-py3.6.egg/ptr.py,0,"b'""""""\nImplementation\n""""""\n\nimport os as _os\nimport shlex as _shlex\nimport contextlib as _contextlib\nimport sys as _sys\nimport operator as _operator\nimport itertools as _itertools\nimport warnings as _warnings\n\ntry:\n    # ensure that map has the same meaning on Python 2\n    from future_builtins import map\nexcept ImportError:\n    pass\n\nimport pkg_resources\nimport setuptools.command.test as orig\nfrom setuptools import Distribution\n\n\n@_contextlib.contextmanager\ndef _save_argv(repl=None):\n    saved = _sys.argv[:]\n    if repl is not None:\n        _sys.argv[:] = repl\n    try:\n        yield saved\n    finally:\n        _sys.argv[:] = saved\n\n\nclass CustomizedDist(Distribution):\n\n    allow_hosts = None\n    index_url = None\n\n    def fetch_build_egg(self, req):\n        """""" Specialized version of Distribution.fetch_build_egg\n        that respects respects allow_hosts and index_url. """"""\n        from setuptools.command.easy_install import easy_install\n\n        dist = Distribution({\'script_args\': [\'easy_install\']})\n        dist.parse_config_files()\n        opts = dist.get_option_dict(\'easy_install\')\n        keep = (\n            \'find_links\',\n            \'site_dirs\',\n            \'index_url\',\n            \'optimize\',\n            \'site_dirs\',\n            \'allow_hosts\',\n        )\n        for key in list(opts):\n            if key not in keep:\n                del opts[key]  # don\'t use any other settings\n        if self.dependency_links:\n            links = self.dependency_links[:]\n            if \'find_links\' in opts:\n                links = opts[\'find_links\'][1].split() + links\n            opts[\'find_links\'] = (\'setup\', links)\n        if self.allow_hosts:\n            opts[\'allow_hosts\'] = (\'test\', self.allow_hosts)\n        if self.index_url:\n            opts[\'index_url\'] = (\'test\', self.index_url)\n        install_dir_func = getattr(self, \'get_egg_cache_dir\', _os.getcwd)\n        install_dir = install_dir_func()\n        cmd = easy_install(\n            dist,\n            args=[""x""],\n            install_dir=install_dir,\n            exclude_scripts=True,\n            always_copy=False,\n            build_directory=None,\n            editable=False,\n            upgrade=False,\n            multi_version=True,\n            no_report=True,\n            user=False,\n        )\n        cmd.ensure_finalized()\n        return cmd.easy_install(req)\n\n\nclass PyTest(orig.test):\n    """"""\n    >>> import setuptools\n    >>> dist = setuptools.Distribution()\n    >>> cmd = PyTest(dist)\n    """"""\n\n    user_options = [\n        (\'extras\', None, ""Install (all) setuptools extras when running tests""),\n        (\n            \'index-url=\',\n            None,\n            ""Specify an index url from which to retrieve "" ""dependencies"",\n        ),\n        (\n            \'allow-hosts=\',\n            None,\n            ""Whitelist of comma-separated hosts to allow ""\n            ""when retrieving dependencies"",\n        ),\n        (\n            \'addopts=\',\n            None,\n            ""Additional options to be passed verbatim to the "" ""pytest runner"",\n        ),\n    ]\n\n    def initialize_options(self):\n        self.extras = False\n        self.index_url = None\n        self.allow_hosts = None\n        self.addopts = []\n        self.ensure_setuptools_version()\n\n    @staticmethod\n    def ensure_setuptools_version():\n        """"""\n        Due to the fact that pytest-runner is often required (via\n        setup-requires directive) by toolchains that never invoke\n        it (i.e. they\'re only installing the package, not testing it),\n        instead of declaring the dependency in the package\n        metadata, assert the requirement at run time.\n        """"""\n        pkg_resources.require(\'setuptools>=27.3\')\n\n    def finalize_options(self):\n        if self.addopts:\n            self.addopts = _shlex.split(self.addopts)\n\n    @staticmethod\n    def marker_passes(marker):\n        """"""\n        Given an environment marker, return True if the marker is valid\n        and matches this environment.\n        """"""\n        return (\n            not marker\n            or not pkg_resources.invalid_marker(marker)\n            and pkg_resources.evaluate_marker(marker)\n        )\n\n    def install_dists(self, dist):\n        """"""\n        Extend install_dists to include extras support\n        """"""\n        return _itertools.chain(\n            orig.test.install_dists(dist), self.install_extra_dists(dist)\n        )\n\n    def install_extra_dists(self, dist):\n        """"""\n        Install extras that are indicated by markers or\n        install all extras if \'--extras\' is indicated.\n        """"""\n        extras_require = dist.extras_require or {}\n\n        spec_extras = (\n            (spec.partition(\':\'), reqs) for spec, reqs in extras_require.items()\n        )\n        matching_extras = (\n            reqs\n            for (name, sep, marker), reqs in spec_extras\n            # include unnamed extras or all if self.extras indicated\n            if (not name or self.extras)\n            # never include extras that fail to pass marker eval\n            and self.marker_passes(marker)\n        )\n        results = list(map(dist.fetch_build_eggs, matching_extras))\n        return _itertools.chain.from_iterable(results)\n\n    @staticmethod\n    def _warn_old_setuptools():\n        msg = (\n            ""pytest-runner will stop working on this version of setuptools; ""\n            ""please upgrade to setuptools 30.4 or later or pin to ""\n            ""pytest-runner < 5.""\n        )\n        ver_str = pkg_resources.get_distribution(\'setuptools\').version\n        ver = pkg_resources.parse_version(ver_str)\n        if ver < pkg_resources.parse_version(\'30.4\'):\n            _warnings.warn(msg)\n\n    def run(self):\n        """"""\n        Override run to ensure requirements are available in this session (but\n        don\'t install them anywhere).\n        """"""\n        self._warn_old_setuptools()\n        dist = CustomizedDist()\n        for attr in \'allow_hosts index_url\'.split():\n            setattr(dist, attr, getattr(self, attr))\n        for attr in (\n            \'dependency_links install_requires \' \'tests_require extras_require \'\n        ).split():\n            setattr(dist, attr, getattr(self.distribution, attr))\n        installed_dists = self.install_dists(dist)\n        if self.dry_run:\n            self.announce(\'skipping tests (dry run)\')\n            return\n        paths = map(_operator.attrgetter(\'location\'), installed_dists)\n        with self.paths_on_pythonpath(paths):\n            with self.project_on_sys_path():\n                return self.run_tests()\n\n    @property\n    def _argv(self):\n        return [\'pytest\'] + self.addopts\n\n    def run_tests(self):\n        """"""\n        Invoke pytest, replacing argv. Return result code.\n        """"""\n        with _save_argv(_sys.argv[:1] + self.addopts):\n            result_code = __import__(\'pytest\').main()\n            if result_code:\n                raise SystemExit(result_code)\n'"
detection/mmdet/apis/__init__.py,0,"b""from .env import init_dist, get_root_logger, set_random_seed\nfrom .train import train_detector\nfrom .inference import inference_detector, show_result\n\n__all__ = [\n    'init_dist', 'get_root_logger', 'set_random_seed', 'train_detector',\n    'inference_detector', 'show_result'\n]\n"""
detection/mmdet/apis/env.py,6,"b""import logging\nimport os\nimport random\n\nimport numpy as np\nimport torch\nimport torch.distributed as dist\nimport torch.multiprocessing as mp\nfrom mmcv.runner import get_dist_info\n\n\ndef init_dist(launcher, backend='nccl', **kwargs):\n    if mp.get_start_method(allow_none=True) is None:\n        mp.set_start_method('spawn')\n    if launcher == 'pytorch':\n        _init_dist_pytorch(backend, **kwargs)\n    elif launcher == 'mpi':\n        _init_dist_mpi(backend, **kwargs)\n    elif launcher == 'slurm':\n        _init_dist_slurm(backend, **kwargs)\n    else:\n        raise ValueError('Invalid launcher type: {}'.format(launcher))\n\n\ndef _init_dist_pytorch(backend, **kwargs):\n    # TODO: use local_rank instead of rank % num_gpus\n    rank = int(os.environ['RANK'])\n    num_gpus = torch.cuda.device_count()\n    torch.cuda.set_device(rank % num_gpus)\n    dist.init_process_group(backend=backend, **kwargs)\n\n\ndef _init_dist_mpi(backend, **kwargs):\n    raise NotImplementedError\n\n\ndef _init_dist_slurm(backend, **kwargs):\n    raise NotImplementedError\n\n\ndef set_random_seed(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n\n\ndef get_root_logger(log_level=logging.INFO):\n    logger = logging.getLogger()\n    if not logger.hasHandlers():\n        logging.basicConfig(\n            format='%(asctime)s - %(levelname)s - %(message)s',\n            level=log_level)\n    rank, _ = get_dist_info()\n    if rank != 0:\n        logger.setLevel('ERROR')\n    return logger\n"""
detection/mmdet/apis/inference.py,1,"b""import mmcv\nimport numpy as np\nimport pycocotools.mask as maskUtils\nimport torch\n\nfrom mmdet.core import get_classes\nfrom mmdet.datasets import to_tensor\nfrom mmdet.datasets.transforms import ImageTransform\n\n\ndef _prepare_data(img, img_transform, cfg, device):\n    ori_shape = img.shape\n    img, img_shape, pad_shape, scale_factor = img_transform(\n        img,\n        scale=cfg.data.test.img_scale,\n        keep_ratio=cfg.data.test.get('resize_keep_ratio', True))\n    img = to_tensor(img).to(device).unsqueeze(0)\n    img_meta = [\n        dict(\n            ori_shape=ori_shape,\n            img_shape=img_shape,\n            pad_shape=pad_shape,\n            scale_factor=scale_factor,\n            flip=False)\n    ]\n    return dict(img=[img], img_meta=[img_meta])\n\n\ndef _inference_single(model, img, img_transform, cfg, device):\n    img = mmcv.imread(img)\n    data = _prepare_data(img, img_transform, cfg, device)\n    with torch.no_grad():\n        result = model(return_loss=False, rescale=True, **data)\n    return result\n\n\ndef _inference_generator(model, imgs, img_transform, cfg, device):\n    for img in imgs:\n        yield _inference_single(model, img, img_transform, cfg, device)\n\n\ndef inference_detector(model, imgs, cfg, device='cuda:0'):\n    img_transform = ImageTransform(\n        size_divisor=cfg.data.test.size_divisor, **cfg.img_norm_cfg)\n    model = model.to(device)\n    model.eval()\n\n    if not isinstance(imgs, list):\n        return _inference_single(model, imgs, img_transform, cfg, device)\n    else:\n        return _inference_generator(model, imgs, img_transform, cfg, device)\n\n\ndef show_result(img, result, dataset='coco', score_thr=0.3, out_file=None):\n    img = mmcv.imread(img)\n    class_names = get_classes(dataset)\n    if isinstance(result, tuple):\n        bbox_result, segm_result = result\n    else:\n        bbox_result, segm_result = result, None\n    bboxes = np.vstack(bbox_result)\n    # draw segmentation masks\n    if segm_result is not None:\n        segms = mmcv.concat_list(segm_result)\n        inds = np.where(bboxes[:, -1] > score_thr)[0]\n        for i in inds:\n            color_mask = np.random.randint(\n                0, 256, (1, 3), dtype=np.uint8)\n            mask = maskUtils.decode(segms[i]).astype(np.bool)\n            img[mask] = img[mask] * 0.5 + color_mask * 0.5\n    # draw bounding boxes\n    labels = [\n        np.full(bbox.shape[0], i, dtype=np.int32)\n        for i, bbox in enumerate(bbox_result)\n    ]\n    labels = np.concatenate(labels)\n    mmcv.imshow_det_bboxes(\n        img.copy(),\n        bboxes,\n        labels,\n        class_names=class_names,\n        score_thr=score_thr,\n        show=out_file is None)\n"""
detection/mmdet/apis/train.py,1,"b""from __future__ import division\n\nfrom collections import OrderedDict\n\nimport torch\nfrom mmcv.runner import Runner, DistSamplerSeedHook\nfrom mmcv.parallel import MMDataParallel, MMDistributedDataParallel\n\nfrom mmdet.core import (DistOptimizerHook, DistEvalmAPHook,\n                        CocoDistEvalRecallHook, CocoDistEvalmAPHook)\nfrom mmdet.datasets import build_dataloader\nfrom mmdet.models import RPN\nfrom .env import get_root_logger\n\n\ndef parse_losses(losses):\n    log_vars = OrderedDict()\n    for loss_name, loss_value in losses.items():\n        if isinstance(loss_value, torch.Tensor):\n            log_vars[loss_name] = loss_value.mean()\n        elif isinstance(loss_value, list):\n            log_vars[loss_name] = sum(_loss.mean() for _loss in loss_value)\n        else:\n            raise TypeError(\n                '{} is not a tensor or list of tensors'.format(loss_name))\n\n    loss = sum(_value for _key, _value in log_vars.items() if 'loss' in _key)\n\n    log_vars['loss'] = loss\n    for name in log_vars:\n        log_vars[name] = log_vars[name].item()\n\n    return loss, log_vars\n\n\ndef batch_processor(model, data, train_mode):\n    losses = model(**data)\n    loss, log_vars = parse_losses(losses)\n\n    outputs = dict(\n        loss=loss, log_vars=log_vars, num_samples=len(data['img'].data))\n\n    return outputs\n\n\ndef train_detector(model,\n                   dataset,\n                   cfg,\n                   distributed=False,\n                   validate=False,\n                   logger=None):\n    if logger is None:\n        logger = get_root_logger(cfg.log_level)\n\n    # start training\n    if distributed:\n        _dist_train(model, dataset, cfg, validate=validate)\n    else:\n        _non_dist_train(model, dataset, cfg, validate=validate)\n\n\ndef _dist_train(model, dataset, cfg, validate=False):\n    # prepare data loaders\n    data_loaders = [\n        build_dataloader(\n            dataset,\n            cfg.data.imgs_per_gpu,\n            cfg.data.workers_per_gpu,\n            dist=True)\n    ]\n    # put model on gpus\n    model = MMDistributedDataParallel(model.cuda())\n    # build runner\n    runner = Runner(model, batch_processor, cfg.optimizer, cfg.work_dir,\n                    cfg.log_level)\n    # register hooks\n    optimizer_config = DistOptimizerHook(**cfg.optimizer_config)\n    runner.register_training_hooks(cfg.lr_config, optimizer_config,\n                                   cfg.checkpoint_config, cfg.log_config)\n    runner.register_hook(DistSamplerSeedHook())\n    # register eval hooks\n    if validate:\n        if isinstance(model.module, RPN):\n            # TODO: implement recall hooks for other datasets\n            runner.register_hook(CocoDistEvalRecallHook(cfg.data.val))\n        else:\n            if cfg.data.val.type == 'CocoDataset':\n                runner.register_hook(CocoDistEvalmAPHook(cfg.data.val))\n            else:\n                runner.register_hook(DistEvalmAPHook(cfg.data.val))\n\n    if cfg.resume_from:\n        runner.resume(cfg.resume_from)\n    elif cfg.load_from:\n        runner.load_checkpoint(cfg.load_from)\n    runner.run(data_loaders, cfg.workflow, cfg.total_epochs)\n\n\ndef _non_dist_train(model, dataset, cfg, validate=False):\n    # prepare data loaders\n    data_loaders = [\n        build_dataloader(\n            dataset,\n            cfg.data.imgs_per_gpu,\n            cfg.data.workers_per_gpu,\n            cfg.gpus,\n            dist=False)\n    ]\n    # put model on gpus\n    model = MMDataParallel(model, device_ids=range(cfg.gpus)).cuda()\n    # build runner\n    runner = Runner(model, batch_processor, cfg.optimizer, cfg.work_dir,\n                    cfg.log_level)\n    runner.register_training_hooks(cfg.lr_config, cfg.optimizer_config,\n                                   cfg.checkpoint_config, cfg.log_config)\n\n    if cfg.resume_from:\n        runner.resume(cfg.resume_from)\n    elif cfg.load_from:\n        runner.load_checkpoint(cfg.load_from)\n    runner.run(data_loaders, cfg.workflow, cfg.total_epochs)\n"""
detection/mmdet/core/__init__.py,0,"b'from .anchor import *  # noqa: F401, F403\nfrom .bbox import *  # noqa: F401, F403\nfrom .mask import *  # noqa: F401, F403\nfrom .loss import *  # noqa: F401, F403\nfrom .evaluation import *  # noqa: F401, F403\nfrom .post_processing import *  # noqa: F401, F403\nfrom .utils import *  # noqa: F401, F403\n'"
detection/mmdet/datasets/__init__.py,0,"b""from .custom import CustomDataset\nfrom .xml_style import XMLDataset\nfrom .coco import CocoDataset\nfrom .voc import VOCDataset\nfrom .loader import GroupSampler, DistributedGroupSampler, build_dataloader\nfrom .utils import to_tensor, random_scale, show_ann, get_dataset\nfrom .concat_dataset import ConcatDataset\nfrom .repeat_dataset import RepeatDataset\nfrom .extra_aug import ExtraAugmentation\n\n__all__ = [\n    'CustomDataset', 'XMLDataset', 'CocoDataset', 'VOCDataset', 'GroupSampler',\n    'DistributedGroupSampler', 'build_dataloader', 'to_tensor', 'random_scale',\n    'show_ann', 'get_dataset', 'ConcatDataset', 'RepeatDataset',\n    'ExtraAugmentation'\n]\n"""
detection/mmdet/datasets/coco.py,0,"b'import numpy as np\nfrom pycocotools.coco import COCO\n\nfrom .custom import CustomDataset\n\n\nclass CocoDataset(CustomDataset):\n\n    CLASSES = (\'person\', \'bicycle\', \'car\', \'motorcycle\', \'airplane\', \'bus\',\n               \'train\', \'truck\', \'boat\', \'traffic_light\', \'fire_hydrant\',\n               \'stop_sign\', \'parking_meter\', \'bench\', \'bird\', \'cat\', \'dog\',\n               \'horse\', \'sheep\', \'cow\', \'elephant\', \'bear\', \'zebra\', \'giraffe\',\n               \'backpack\', \'umbrella\', \'handbag\', \'tie\', \'suitcase\', \'frisbee\',\n               \'skis\', \'snowboard\', \'sports_ball\', \'kite\', \'baseball_bat\',\n               \'baseball_glove\', \'skateboard\', \'surfboard\', \'tennis_racket\',\n               \'bottle\', \'wine_glass\', \'cup\', \'fork\', \'knife\', \'spoon\', \'bowl\',\n               \'banana\', \'apple\', \'sandwich\', \'orange\', \'broccoli\', \'carrot\',\n               \'hot_dog\', \'pizza\', \'donut\', \'cake\', \'chair\', \'couch\',\n               \'potted_plant\', \'bed\', \'dining_table\', \'toilet\', \'tv\', \'laptop\',\n               \'mouse\', \'remote\', \'keyboard\', \'cell_phone\', \'microwave\',\n               \'oven\', \'toaster\', \'sink\', \'refrigerator\', \'book\', \'clock\',\n               \'vase\', \'scissors\', \'teddy_bear\', \'hair_drier\', \'toothbrush\')\n\n    def load_annotations(self, ann_file):\n        self.coco = COCO(ann_file)\n        self.cat_ids = self.coco.getCatIds()\n        self.cat2label = {\n            cat_id: i + 1\n            for i, cat_id in enumerate(self.cat_ids)\n        }\n        self.img_ids = self.coco.getImgIds()\n        img_infos = []\n        for i in self.img_ids:\n            info = self.coco.loadImgs([i])[0]\n            info[\'filename\'] = info[\'file_name\']\n            img_infos.append(info)\n        return img_infos\n\n    def get_ann_info(self, idx):\n        img_id = self.img_infos[idx][\'id\']\n        ann_ids = self.coco.getAnnIds(imgIds=[img_id])\n        ann_info = self.coco.loadAnns(ann_ids)\n        return self._parse_ann_info(ann_info, self.with_mask)\n\n    def _filter_imgs(self, min_size=32):\n        """"""Filter images too small or without ground truths.""""""\n        valid_inds = []\n        ids_with_ann = set(_[\'image_id\'] for _ in self.coco.anns.values())\n        for i, img_info in enumerate(self.img_infos):\n            if self.img_ids[i] not in ids_with_ann:\n                continue\n            if min(img_info[\'width\'], img_info[\'height\']) >= min_size:\n                valid_inds.append(i)\n        return valid_inds\n\n    def _parse_ann_info(self, ann_info, with_mask=True):\n        """"""Parse bbox and mask annotation.\n\n        Args:\n            ann_info (list[dict]): Annotation info of an image.\n            with_mask (bool): Whether to parse mask annotations.\n\n        Returns:\n            dict: A dict containing the following keys: bboxes, bboxes_ignore,\n                labels, masks, mask_polys, poly_lens.\n        """"""\n        gt_bboxes = []\n        gt_labels = []\n        gt_bboxes_ignore = []\n        # Two formats are provided.\n        # 1. mask: a binary map of the same size of the image.\n        # 2. polys: each mask consists of one or several polys, each poly is a\n        # list of float.\n        if with_mask:\n            gt_masks = []\n            gt_mask_polys = []\n            gt_poly_lens = []\n        for i, ann in enumerate(ann_info):\n            if ann.get(\'ignore\', False):\n                continue\n            x1, y1, w, h = ann[\'bbox\']\n            if ann[\'area\'] <= 0 or w < 1 or h < 1:\n                continue\n            bbox = [x1, y1, x1 + w - 1, y1 + h - 1]\n            if ann[\'iscrowd\']:\n                gt_bboxes_ignore.append(bbox)\n            else:\n                gt_bboxes.append(bbox)\n                gt_labels.append(self.cat2label[ann[\'category_id\']])\n            if with_mask:\n                gt_masks.append(self.coco.annToMask(ann))\n                mask_polys = [\n                    p for p in ann[\'segmentation\'] if len(p) >= 6\n                ]  # valid polygons have >= 3 points (6 coordinates)\n                poly_lens = [len(p) for p in mask_polys]\n                gt_mask_polys.append(mask_polys)\n                gt_poly_lens.extend(poly_lens)\n        if gt_bboxes:\n            gt_bboxes = np.array(gt_bboxes, dtype=np.float32)\n            gt_labels = np.array(gt_labels, dtype=np.int64)\n        else:\n            gt_bboxes = np.zeros((0, 4), dtype=np.float32)\n            gt_labels = np.array([], dtype=np.int64)\n\n        if gt_bboxes_ignore:\n            gt_bboxes_ignore = np.array(gt_bboxes_ignore, dtype=np.float32)\n        else:\n            gt_bboxes_ignore = np.zeros((0, 4), dtype=np.float32)\n\n        ann = dict(\n            bboxes=gt_bboxes, labels=gt_labels, bboxes_ignore=gt_bboxes_ignore)\n\n        if with_mask:\n            ann[\'masks\'] = gt_masks\n            # poly format is not used in the current implementation\n            ann[\'mask_polys\'] = gt_mask_polys\n            ann[\'poly_lens\'] = gt_poly_lens\n        return ann\n'"
detection/mmdet/datasets/concat_dataset.py,2,"b'import numpy as np\nfrom torch.utils.data.dataset import ConcatDataset as _ConcatDataset\n\n\nclass ConcatDataset(_ConcatDataset):\n    """"""A wrapper of concatenated dataset.\n\n    Same as :obj:`torch.utils.data.dataset.ConcatDataset`, but\n    concat the group flag for image aspect ratio.\n\n    Args:\n        datasets (list[:obj:`Dataset`]): A list of datasets.\n    """"""\n\n    def __init__(self, datasets):\n        super(ConcatDataset, self).__init__(datasets)\n        self.CLASSES = datasets[0].CLASSES\n        if hasattr(datasets[0], \'flag\'):\n            flags = []\n            for i in range(0, len(datasets)):\n                flags.append(datasets[i].flag)\n            self.flag = np.concatenate(flags)\n'"
detection/mmdet/datasets/custom.py,1,"b'import os.path as osp\n\nimport mmcv\nimport numpy as np\nfrom mmcv.parallel import DataContainer as DC\nfrom torch.utils.data import Dataset\n\nfrom .transforms import (ImageTransform, BboxTransform, MaskTransform,\n                         Numpy2Tensor)\nfrom .utils import to_tensor, random_scale\nfrom .extra_aug import ExtraAugmentation\n\n\nclass CustomDataset(Dataset):\n    """"""Custom dataset for detection.\n\n    Annotation format:\n    [\n        {\n            \'filename\': \'a.jpg\',\n            \'width\': 1280,\n            \'height\': 720,\n            \'ann\': {\n                \'bboxes\': <np.ndarray> (n, 4),\n                \'labels\': <np.ndarray> (n, ),\n                \'bboxes_ignore\': <np.ndarray> (k, 4),\n                \'labels_ignore\': <np.ndarray> (k, 4) (optional field)\n            }\n        },\n        ...\n    ]\n\n    The `ann` field is optional for testing.\n    """"""\n\n    CLASSES = None\n\n    def __init__(self,\n                 ann_file,\n                 img_prefix,\n                 img_scale,\n                 img_norm_cfg,\n                 multiscale_mode=\'value\',\n                 size_divisor=None,\n                 proposal_file=None,\n                 num_max_proposals=1000,\n                 flip_ratio=0,\n                 with_mask=True,\n                 with_crowd=True,\n                 with_label=True,\n                 extra_aug=None,\n                 resize_keep_ratio=True,\n                 test_mode=False):\n        # prefix of images path\n        self.img_prefix = img_prefix\n\n        # load annotations (and proposals)\n        self.img_infos = self.load_annotations(ann_file)\n        if proposal_file is not None:\n            self.proposals = self.load_proposals(proposal_file)\n        else:\n            self.proposals = None\n        # filter images with no annotation during training\n        if not test_mode:\n            valid_inds = self._filter_imgs()\n            self.img_infos = [self.img_infos[i] for i in valid_inds]\n            if self.proposals is not None:\n                self.proposals = [self.proposals[i] for i in valid_inds]\n\n        # (long_edge, short_edge) or [(long1, short1), (long2, short2), ...]\n        self.img_scales = img_scale if isinstance(img_scale,\n                                                  list) else [img_scale]\n        assert mmcv.is_list_of(self.img_scales, tuple)\n        # normalization configs\n        self.img_norm_cfg = img_norm_cfg\n\n        # multi-scale mode (only applicable for multi-scale training)\n        self.multiscale_mode = multiscale_mode\n        assert multiscale_mode in [\'value\', \'range\']\n\n        # max proposals per image\n        self.num_max_proposals = num_max_proposals\n        # flip ratio\n        self.flip_ratio = flip_ratio\n        assert flip_ratio >= 0 and flip_ratio <= 1\n        # padding border to ensure the image size can be divided by\n        # size_divisor (used for FPN)\n        self.size_divisor = size_divisor\n\n        # with mask or not (reserved field, takes no effect)\n        self.with_mask = with_mask\n        # some datasets provide bbox annotations as ignore/crowd/difficult,\n        # if `with_crowd` is True, then these info is returned.\n        self.with_crowd = with_crowd\n        # with label is False for RPN\n        self.with_label = with_label\n        # in test mode or not\n        self.test_mode = test_mode\n\n        # set group flag for the sampler\n        if not self.test_mode:\n            self._set_group_flag()\n        # transforms\n        self.img_transform = ImageTransform(\n            size_divisor=self.size_divisor, **self.img_norm_cfg)\n        self.bbox_transform = BboxTransform()\n        self.mask_transform = MaskTransform()\n        self.numpy2tensor = Numpy2Tensor()\n\n        # if use extra augmentation\n        if extra_aug is not None:\n            self.extra_aug = ExtraAugmentation(**extra_aug)\n        else:\n            self.extra_aug = None\n\n        # image rescale if keep ratio\n        self.resize_keep_ratio = resize_keep_ratio\n\n    def __len__(self):\n        return len(self.img_infos)\n\n    def load_annotations(self, ann_file):\n        return mmcv.load(ann_file)\n\n    def load_proposals(self, proposal_file):\n        return mmcv.load(proposal_file)\n\n    def get_ann_info(self, idx):\n        return self.img_infos[idx][\'ann\']\n\n    def _filter_imgs(self, min_size=32):\n        """"""Filter images too small.""""""\n        valid_inds = []\n        for i, img_info in enumerate(self.img_infos):\n            if min(img_info[\'width\'], img_info[\'height\']) >= min_size:\n                valid_inds.append(i)\n        return valid_inds\n\n    def _set_group_flag(self):\n        """"""Set flag according to image aspect ratio.\n\n        Images with aspect ratio greater than 1 will be set as group 1,\n        otherwise group 0.\n        """"""\n        self.flag = np.zeros(len(self), dtype=np.uint8)\n        for i in range(len(self)):\n            img_info = self.img_infos[i]\n            if img_info[\'width\'] / img_info[\'height\'] > 1:\n                self.flag[i] = 1\n\n    def _rand_another(self, idx):\n        pool = np.where(self.flag == self.flag[idx])[0]\n        return np.random.choice(pool)\n\n    def __getitem__(self, idx):\n        if self.test_mode:\n            return self.prepare_test_img(idx)\n        while True:\n            data = self.prepare_train_img(idx)\n            if data is None:\n                idx = self._rand_another(idx)\n                continue\n            return data\n\n    def prepare_train_img(self, idx):\n        img_info = self.img_infos[idx]\n        # load image\n        img = mmcv.imread(osp.join(self.img_prefix, img_info[\'filename\']))\n        # load proposals if necessary\n        if self.proposals is not None:\n            proposals = self.proposals[idx][:self.num_max_proposals]\n            # TODO: Handle empty proposals properly. Currently images with\n            # no proposals are just ignored, but they can be used for\n            # training in concept.\n            if len(proposals) == 0:\n                return None\n            if not (proposals.shape[1] == 4 or proposals.shape[1] == 5):\n                raise AssertionError(\n                    \'proposals should have shapes (n, 4) or (n, 5), \'\n                    \'but found {}\'.format(proposals.shape))\n            if proposals.shape[1] == 5:\n                scores = proposals[:, 4, None]\n                proposals = proposals[:, :4]\n            else:\n                scores = None\n\n        ann = self.get_ann_info(idx)\n        gt_bboxes = ann[\'bboxes\']\n        gt_labels = ann[\'labels\']\n        if self.with_crowd:\n            gt_bboxes_ignore = ann[\'bboxes_ignore\']\n\n        # skip the image if there is no valid gt bbox\n        if len(gt_bboxes) == 0:\n            return None\n\n        # extra augmentation\n        if self.extra_aug is not None:\n            img, gt_bboxes, gt_labels = self.extra_aug(img, gt_bboxes,\n                                                       gt_labels)\n\n        # apply transforms\n        flip = True if np.random.rand() < self.flip_ratio else False\n        # randomly sample a scale\n        img_scale = random_scale(self.img_scales, self.multiscale_mode)\n        img, img_shape, pad_shape, scale_factor = self.img_transform(\n            img, img_scale, flip, keep_ratio=self.resize_keep_ratio)\n        img = img.copy()\n        if self.proposals is not None:\n            proposals = self.bbox_transform(proposals, img_shape, scale_factor,\n                                            flip)\n            proposals = np.hstack(\n                [proposals, scores]) if scores is not None else proposals\n        gt_bboxes = self.bbox_transform(gt_bboxes, img_shape, scale_factor,\n                                        flip)\n        if self.with_crowd:\n            gt_bboxes_ignore = self.bbox_transform(gt_bboxes_ignore, img_shape,\n                                                   scale_factor, flip)\n        if self.with_mask:\n            gt_masks = self.mask_transform(ann[\'masks\'], pad_shape,\n                                           scale_factor, flip)\n\n        ori_shape = (img_info[\'height\'], img_info[\'width\'], 3)\n        img_meta = dict(\n            ori_shape=ori_shape,\n            img_shape=img_shape,\n            pad_shape=pad_shape,\n            scale_factor=scale_factor,\n            flip=flip)\n\n        data = dict(\n            img=DC(to_tensor(img), stack=True),\n            img_meta=DC(img_meta, cpu_only=True),\n            gt_bboxes=DC(to_tensor(gt_bboxes)))\n        if self.proposals is not None:\n            data[\'proposals\'] = DC(to_tensor(proposals))\n        if self.with_label:\n            data[\'gt_labels\'] = DC(to_tensor(gt_labels))\n        if self.with_crowd:\n            data[\'gt_bboxes_ignore\'] = DC(to_tensor(gt_bboxes_ignore))\n        if self.with_mask:\n            data[\'gt_masks\'] = DC(gt_masks, cpu_only=True)\n        return data\n\n    def prepare_test_img(self, idx):\n        """"""Prepare an image for testing (multi-scale and flipping)""""""\n        img_info = self.img_infos[idx]\n        img = mmcv.imread(osp.join(self.img_prefix, img_info[\'filename\']))\n        if self.proposals is not None:\n            proposal = self.proposals[idx][:self.num_max_proposals]\n            if not (proposal.shape[1] == 4 or proposal.shape[1] == 5):\n                raise AssertionError(\n                    \'proposals should have shapes (n, 4) or (n, 5), \'\n                    \'but found {}\'.format(proposal.shape))\n        else:\n            proposal = None\n\n        def prepare_single(img, scale, flip, proposal=None):\n            _img, img_shape, pad_shape, scale_factor = self.img_transform(\n                img, scale, flip, keep_ratio=self.resize_keep_ratio)\n            _img = to_tensor(_img)\n            _img_meta = dict(\n                ori_shape=(img_info[\'height\'], img_info[\'width\'], 3),\n                img_shape=img_shape,\n                pad_shape=pad_shape,\n                scale_factor=scale_factor,\n                flip=flip)\n            if proposal is not None:\n                if proposal.shape[1] == 5:\n                    score = proposal[:, 4, None]\n                    proposal = proposal[:, :4]\n                else:\n                    score = None\n                _proposal = self.bbox_transform(proposal, img_shape,\n                                                scale_factor, flip)\n                _proposal = np.hstack(\n                    [_proposal, score]) if score is not None else _proposal\n                _proposal = to_tensor(_proposal)\n            else:\n                _proposal = None\n            return _img, _img_meta, _proposal\n\n        imgs = []\n        img_metas = []\n        proposals = []\n        for scale in self.img_scales:\n            _img, _img_meta, _proposal = prepare_single(\n                img, scale, False, proposal)\n            imgs.append(_img)\n            img_metas.append(DC(_img_meta, cpu_only=True))\n            proposals.append(_proposal)\n            if self.flip_ratio > 0:\n                _img, _img_meta, _proposal = prepare_single(\n                    img, scale, True, proposal)\n                imgs.append(_img)\n                img_metas.append(DC(_img_meta, cpu_only=True))\n                proposals.append(_proposal)\n        data = dict(img=imgs, img_meta=img_metas)\n        if self.proposals is not None:\n            data[\'proposals\'] = proposals\n        return data\n'"
detection/mmdet/datasets/extra_aug.py,0,"b'import mmcv\nimport numpy as np\nfrom numpy import random\n\nfrom mmdet.core.evaluation.bbox_overlaps import bbox_overlaps\n\n\nclass PhotoMetricDistortion(object):\n\n    def __init__(self,\n                 brightness_delta=32,\n                 contrast_range=(0.5, 1.5),\n                 saturation_range=(0.5, 1.5),\n                 hue_delta=18):\n        self.brightness_delta = brightness_delta\n        self.contrast_lower, self.contrast_upper = contrast_range\n        self.saturation_lower, self.saturation_upper = saturation_range\n        self.hue_delta = hue_delta\n\n    def __call__(self, img, boxes, labels):\n        # random brightness\n        if random.randint(2):\n            delta = random.uniform(-self.brightness_delta,\n                                   self.brightness_delta)\n            img += delta\n\n        # mode == 0 --> do random contrast first\n        # mode == 1 --> do random contrast last\n        mode = random.randint(2)\n        if mode == 1:\n            if random.randint(2):\n                alpha = random.uniform(self.contrast_lower,\n                                       self.contrast_upper)\n                img *= alpha\n\n        # convert color from BGR to HSV\n        img = mmcv.bgr2hsv(img)\n\n        # random saturation\n        if random.randint(2):\n            img[..., 1] *= random.uniform(self.saturation_lower,\n                                          self.saturation_upper)\n\n        # random hue\n        if random.randint(2):\n            img[..., 0] += random.uniform(-self.hue_delta, self.hue_delta)\n            img[..., 0][img[..., 0] > 360] -= 360\n            img[..., 0][img[..., 0] < 0] += 360\n\n        # convert color from HSV to BGR\n        img = mmcv.hsv2bgr(img)\n\n        # random contrast\n        if mode == 0:\n            if random.randint(2):\n                alpha = random.uniform(self.contrast_lower,\n                                       self.contrast_upper)\n                img *= alpha\n\n        # randomly swap channels\n        if random.randint(2):\n            img = img[..., random.permutation(3)]\n\n        return img, boxes, labels\n\n\nclass Expand(object):\n\n    def __init__(self, mean=(0, 0, 0), to_rgb=True, ratio_range=(1, 4)):\n        if to_rgb:\n            self.mean = mean[::-1]\n        else:\n            self.mean = mean\n        self.min_ratio, self.max_ratio = ratio_range\n\n    def __call__(self, img, boxes, labels):\n        if random.randint(2):\n            return img, boxes, labels\n\n        h, w, c = img.shape\n        ratio = random.uniform(self.min_ratio, self.max_ratio)\n        expand_img = np.full((int(h * ratio), int(w * ratio), c),\n                             self.mean).astype(img.dtype)\n        left = int(random.uniform(0, w * ratio - w))\n        top = int(random.uniform(0, h * ratio - h))\n        expand_img[top:top + h, left:left + w] = img\n        img = expand_img\n        boxes += np.tile((left, top), 2)\n        return img, boxes, labels\n\n\nclass RandomCrop(object):\n\n    def __init__(self,\n                 min_ious=(0.1, 0.3, 0.5, 0.7, 0.9),\n                 min_crop_size=0.3):\n        # 1: return ori img\n        self.sample_mode = (1, *min_ious, 0)\n        self.min_crop_size = min_crop_size\n\n    def __call__(self, img, boxes, labels):\n        h, w, c = img.shape\n        while True:\n            mode = random.choice(self.sample_mode)\n            if mode == 1:\n                return img, boxes, labels\n\n            min_iou = mode\n            for i in range(50):\n                new_w = random.uniform(self.min_crop_size * w, w)\n                new_h = random.uniform(self.min_crop_size * h, h)\n\n                # h / w in [0.5, 2]\n                if new_h / new_w < 0.5 or new_h / new_w > 2:\n                    continue\n\n                left = random.uniform(w - new_w)\n                top = random.uniform(h - new_h)\n\n                patch = np.array((int(left), int(top), int(left + new_w),\n                                  int(top + new_h)))\n                overlaps = bbox_overlaps(\n                    patch.reshape(-1, 4), boxes.reshape(-1, 4)).reshape(-1)\n                if overlaps.min() < min_iou:\n                    continue\n\n                # center of boxes should inside the crop img\n                center = (boxes[:, :2] + boxes[:, 2:]) / 2\n                mask = (center[:, 0] > patch[0]) * (\n                    center[:, 1] > patch[1]) * (center[:, 0] < patch[2]) * (\n                        center[:, 1] < patch[3])\n                if not mask.any():\n                    continue\n                boxes = boxes[mask]\n                labels = labels[mask]\n\n                # adjust boxes\n                img = img[patch[1]:patch[3], patch[0]:patch[2]]\n                boxes[:, 2:] = boxes[:, 2:].clip(max=patch[2:])\n                boxes[:, :2] = boxes[:, :2].clip(min=patch[:2])\n                boxes -= np.tile(patch[:2], 2)\n\n                return img, boxes, labels\n\n\nclass ExtraAugmentation(object):\n\n    def __init__(self,\n                 photo_metric_distortion=None,\n                 expand=None,\n                 random_crop=None):\n        self.transforms = []\n        if photo_metric_distortion is not None:\n            self.transforms.append(\n                PhotoMetricDistortion(**photo_metric_distortion))\n        if expand is not None:\n            self.transforms.append(Expand(**expand))\n        if random_crop is not None:\n            self.transforms.append(RandomCrop(**random_crop))\n\n    def __call__(self, img, boxes, labels):\n        img = img.astype(np.float32)\n        for transform in self.transforms:\n            img, boxes, labels = transform(img, boxes, labels)\n        return img, boxes, labels\n'"
detection/mmdet/datasets/repeat_dataset.py,0,"b""import numpy as np\n\n\nclass RepeatDataset(object):\n\n    def __init__(self, dataset, times):\n        self.dataset = dataset\n        self.times = times\n        self.CLASSES = dataset.CLASSES\n        if hasattr(self.dataset, 'flag'):\n            self.flag = np.tile(self.dataset.flag, times)\n\n        self._ori_len = len(self.dataset)\n\n    def __getitem__(self, idx):\n        return self.dataset[idx % self._ori_len]\n\n    def __len__(self):\n        return self.times * self._ori_len\n"""
detection/mmdet/datasets/transforms.py,2,"b'import mmcv\nimport numpy as np\nimport torch\n\n__all__ = [\'ImageTransform\', \'BboxTransform\', \'MaskTransform\', \'Numpy2Tensor\']\n\n\nclass ImageTransform(object):\n    """"""Preprocess an image.\n\n    1. rescale the image to expected size\n    2. normalize the image\n    3. flip the image (if needed)\n    4. pad the image (if needed)\n    5. transpose to (c, h, w)\n    """"""\n\n    def __init__(self,\n                 mean=(0, 0, 0),\n                 std=(1, 1, 1),\n                 to_rgb=True,\n                 size_divisor=None):\n        self.mean = np.array(mean, dtype=np.float32)\n        self.std = np.array(std, dtype=np.float32)\n        self.to_rgb = to_rgb\n        self.size_divisor = size_divisor\n\n    def __call__(self, img, scale, flip=False, keep_ratio=True):\n        if keep_ratio:\n            img, scale_factor = mmcv.imrescale(img, scale, return_scale=True)\n        else:\n            img, w_scale, h_scale = mmcv.imresize(\n                img, scale, return_scale=True)\n            scale_factor = np.array([w_scale, h_scale, w_scale, h_scale],\n                                    dtype=np.float32)\n        img_shape = img.shape\n        img = mmcv.imnormalize(img, self.mean, self.std, self.to_rgb)\n        if flip:\n            img = mmcv.imflip(img)\n        if self.size_divisor is not None:\n            img = mmcv.impad_to_multiple(img, self.size_divisor)\n            pad_shape = img.shape\n        else:\n            pad_shape = img_shape\n        img = img.transpose(2, 0, 1)\n        return img, img_shape, pad_shape, scale_factor\n\n\ndef bbox_flip(bboxes, img_shape):\n    """"""Flip bboxes horizontally.\n\n    Args:\n        bboxes(ndarray): shape (..., 4*k)\n        img_shape(tuple): (height, width)\n    """"""\n    assert bboxes.shape[-1] % 4 == 0\n    w = img_shape[1]\n    flipped = bboxes.copy()\n    flipped[..., 0::4] = w - bboxes[..., 2::4] - 1\n    flipped[..., 2::4] = w - bboxes[..., 0::4] - 1\n    return flipped\n\n\nclass BboxTransform(object):\n    """"""Preprocess gt bboxes.\n\n    1. rescale bboxes according to image size\n    2. flip bboxes (if needed)\n    3. pad the first dimension to `max_num_gts`\n    """"""\n\n    def __init__(self, max_num_gts=None):\n        self.max_num_gts = max_num_gts\n\n    def __call__(self, bboxes, img_shape, scale_factor, flip=False):\n        gt_bboxes = bboxes * scale_factor\n        if flip:\n            gt_bboxes = bbox_flip(gt_bboxes, img_shape)\n        gt_bboxes[:, 0::2] = np.clip(gt_bboxes[:, 0::2], 0, img_shape[1] - 1)\n        gt_bboxes[:, 1::2] = np.clip(gt_bboxes[:, 1::2], 0, img_shape[0] - 1)\n        if self.max_num_gts is None:\n            return gt_bboxes\n        else:\n            num_gts = gt_bboxes.shape[0]\n            padded_bboxes = np.zeros((self.max_num_gts, 4), dtype=np.float32)\n            padded_bboxes[:num_gts, :] = gt_bboxes\n            return padded_bboxes\n\n\nclass MaskTransform(object):\n    """"""Preprocess masks.\n\n    1. resize masks to expected size and stack to a single array\n    2. flip the masks (if needed)\n    3. pad the masks (if needed)\n    """"""\n\n    def __call__(self, masks, pad_shape, scale_factor, flip=False):\n        masks = [\n            mmcv.imrescale(mask, scale_factor, interpolation=\'nearest\')\n            for mask in masks\n        ]\n        if flip:\n            masks = [mask[:, ::-1] for mask in masks]\n        padded_masks = [\n            mmcv.impad(mask, pad_shape[:2], pad_val=0) for mask in masks\n        ]\n        padded_masks = np.stack(padded_masks, axis=0)\n        return padded_masks\n\n\nclass Numpy2Tensor(object):\n\n    def __init__(self):\n        pass\n\n    def __call__(self, *args):\n        if len(args) == 1:\n            return torch.from_numpy(args[0])\n        else:\n            return tuple([torch.from_numpy(np.array(array)) for array in args])\n'"
detection/mmdet/datasets/utils.py,7,"b'import copy\nfrom collections import Sequence\n\nimport mmcv\nfrom mmcv.runner import obj_from_dict\nimport torch\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom .concat_dataset import ConcatDataset\nfrom .repeat_dataset import RepeatDataset\nfrom .. import datasets\n\n\ndef to_tensor(data):\n    """"""Convert objects of various python types to :obj:`torch.Tensor`.\n\n    Supported types are: :class:`numpy.ndarray`, :class:`torch.Tensor`,\n    :class:`Sequence`, :class:`int` and :class:`float`.\n    """"""\n    if isinstance(data, torch.Tensor):\n        return data\n    elif isinstance(data, np.ndarray):\n        return torch.from_numpy(data)\n    elif isinstance(data, Sequence) and not mmcv.is_str(data):\n        return torch.tensor(data)\n    elif isinstance(data, int):\n        return torch.LongTensor([data])\n    elif isinstance(data, float):\n        return torch.FloatTensor([data])\n    else:\n        raise TypeError(\'type {} cannot be converted to tensor.\'.format(\n            type(data)))\n\n\ndef random_scale(img_scales, mode=\'range\'):\n    """"""Randomly select a scale from a list of scales or scale ranges.\n\n    Args:\n        img_scales (list[tuple]): Image scale or scale range.\n        mode (str): ""range"" or ""value"".\n\n    Returns:\n        tuple: Sampled image scale.\n    """"""\n    num_scales = len(img_scales)\n    if num_scales == 1:  # fixed scale is specified\n        img_scale = img_scales[0]\n    elif num_scales == 2:  # randomly sample a scale\n        if mode == \'range\':\n            img_scale_long = [max(s) for s in img_scales]\n            img_scale_short = [min(s) for s in img_scales]\n            long_edge = np.random.randint(\n                min(img_scale_long),\n                max(img_scale_long) + 1)\n            short_edge = np.random.randint(\n                min(img_scale_short),\n                max(img_scale_short) + 1)\n            img_scale = (long_edge, short_edge)\n        elif mode == \'value\':\n            img_scale = img_scales[np.random.randint(num_scales)]\n    else:\n        if mode != \'value\':\n            raise ValueError(\n                \'Only ""value"" mode supports more than 2 image scales\')\n        img_scale = img_scales[np.random.randint(num_scales)]\n    return img_scale\n\n\ndef show_ann(coco, img, ann_info):\n    plt.imshow(mmcv.bgr2rgb(img))\n    plt.axis(\'off\')\n    coco.showAnns(ann_info)\n    plt.show()\n\n\ndef get_dataset(data_cfg):\n    if data_cfg[\'type\'] == \'RepeatDataset\':\n        return RepeatDataset(\n            get_dataset(data_cfg[\'dataset\']), data_cfg[\'times\'])\n\n    if isinstance(data_cfg[\'ann_file\'], (list, tuple)):\n        ann_files = data_cfg[\'ann_file\']\n        num_dset = len(ann_files)\n    else:\n        ann_files = [data_cfg[\'ann_file\']]\n        num_dset = 1\n\n    if \'proposal_file\' in data_cfg.keys():\n        if isinstance(data_cfg[\'proposal_file\'], (list, tuple)):\n            proposal_files = data_cfg[\'proposal_file\']\n        else:\n            proposal_files = [data_cfg[\'proposal_file\']]\n    else:\n        proposal_files = [None] * num_dset\n    assert len(proposal_files) == num_dset\n\n    if isinstance(data_cfg[\'img_prefix\'], (list, tuple)):\n        img_prefixes = data_cfg[\'img_prefix\']\n    else:\n        img_prefixes = [data_cfg[\'img_prefix\']] * num_dset\n    assert len(img_prefixes) == num_dset\n\n    dsets = []\n    for i in range(num_dset):\n        data_info = copy.deepcopy(data_cfg)\n        data_info[\'ann_file\'] = ann_files[i]\n        data_info[\'proposal_file\'] = proposal_files[i]\n        data_info[\'img_prefix\'] = img_prefixes[i]\n        dset = obj_from_dict(data_info, datasets)\n        dsets.append(dset)\n    if len(dsets) > 1:\n        dset = ConcatDataset(dsets)\n    else:\n        dset = dsets[0]\n    return dset\n'"
detection/mmdet/datasets/voc.py,0,"b""from .xml_style import XMLDataset\n\n\nclass VOCDataset(XMLDataset):\n\n    CLASSES = ('aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus', 'car',\n               'cat', 'chair', 'cow', 'diningtable', 'dog', 'horse',\n               'motorbike', 'person', 'pottedplant', 'sheep', 'sofa', 'train',\n               'tvmonitor')\n\n    def __init__(self, **kwargs):\n        super(VOCDataset, self).__init__(**kwargs)\n        if 'VOC2007' in self.img_prefix:\n            self.year = 2007\n        elif 'VOC2012' in self.img_prefix:\n            self.year = 2012\n        else:\n            raise ValueError('Cannot infer dataset year from img_prefix')\n"""
detection/mmdet/datasets/xml_style.py,0,"b""import os.path as osp\nimport xml.etree.ElementTree as ET\n\nimport mmcv\nimport numpy as np\n\nfrom .custom import CustomDataset\n\n\nclass XMLDataset(CustomDataset):\n\n    def __init__(self, **kwargs):\n        super(XMLDataset, self).__init__(**kwargs)\n        self.cat2label = {cat: i + 1 for i, cat in enumerate(self.CLASSES)}\n\n    def load_annotations(self, ann_file):\n        img_infos = []\n        img_ids = mmcv.list_from_file(ann_file)\n        for img_id in img_ids:\n            filename = 'JPEGImages/{}.jpg'.format(img_id)\n            xml_path = osp.join(self.img_prefix, 'Annotations',\n                                '{}.xml'.format(img_id))\n            tree = ET.parse(xml_path)\n            root = tree.getroot()\n            size = root.find('size')\n            width = int(size.find('width').text)\n            height = int(size.find('height').text)\n            img_infos.append(\n                dict(id=img_id, filename=filename, width=width, height=height))\n        return img_infos\n\n    def get_ann_info(self, idx):\n        img_id = self.img_infos[idx]['id']\n        xml_path = osp.join(self.img_prefix, 'Annotations',\n                            '{}.xml'.format(img_id))\n        tree = ET.parse(xml_path)\n        root = tree.getroot()\n        bboxes = []\n        labels = []\n        bboxes_ignore = []\n        labels_ignore = []\n        for obj in root.findall('object'):\n            name = obj.find('name').text\n            label = self.cat2label[name]\n            difficult = int(obj.find('difficult').text)\n            bnd_box = obj.find('bndbox')\n            bbox = [\n                int(bnd_box.find('xmin').text),\n                int(bnd_box.find('ymin').text),\n                int(bnd_box.find('xmax').text),\n                int(bnd_box.find('ymax').text)\n            ]\n            if difficult:\n                bboxes_ignore.append(bbox)\n                labels_ignore.append(label)\n            else:\n                bboxes.append(bbox)\n                labels.append(label)\n        if not bboxes:\n            bboxes = np.zeros((0, 4))\n            labels = np.zeros((0, ))\n        else:\n            bboxes = np.array(bboxes, ndmin=2) - 1\n            labels = np.array(labels)\n        if not bboxes_ignore:\n            bboxes_ignore = np.zeros((0, 4))\n            labels_ignore = np.zeros((0, ))\n        else:\n            bboxes_ignore = np.array(bboxes_ignore, ndmin=2) - 1\n            labels_ignore = np.array(labels_ignore)\n        ann = dict(\n            bboxes=bboxes.astype(np.float32),\n            labels=labels.astype(np.int64),\n            bboxes_ignore=bboxes_ignore.astype(np.float32),\n            labels_ignore=labels_ignore.astype(np.int64))\n        return ann\n"""
detection/mmdet/models/__init__.py,0,"b""from .backbones import *  # noqa: F401,F403\nfrom .necks import *  # noqa: F401,F403\nfrom .roi_extractors import *  # noqa: F401,F403\nfrom .anchor_heads import *  # noqa: F401,F403\nfrom .bbox_heads import *  # noqa: F401,F403\nfrom .mask_heads import *  # noqa: F401,F403\nfrom .detectors import *  # noqa: F401,F403\nfrom .registry import BACKBONES, NECKS, ROI_EXTRACTORS, HEADS, DETECTORS\nfrom .builder import (build_backbone, build_neck, build_roi_extractor,\n                      build_head, build_detector)\n\n__all__ = [\n    'BACKBONES', 'NECKS', 'ROI_EXTRACTORS', 'HEADS', 'DETECTORS',\n    'build_backbone', 'build_neck', 'build_roi_extractor', 'build_head',\n    'build_detector'\n]\n"""
detection/mmdet/models/builder.py,0,"b""import mmcv\nfrom torch import nn\n\nfrom .registry import BACKBONES, NECKS, ROI_EXTRACTORS, HEADS, DETECTORS\n\n\ndef _build_module(cfg, registry, default_args):\n    assert isinstance(cfg, dict) and 'type' in cfg\n    assert isinstance(default_args, dict) or default_args is None\n    args = cfg.copy()\n    obj_type = args.pop('type')\n    if mmcv.is_str(obj_type):\n        if obj_type not in registry.module_dict:\n            raise KeyError('{} is not in the {} registry'.format(\n                obj_type, registry.name))\n        obj_type = registry.module_dict[obj_type]\n    elif not isinstance(obj_type, type):\n        raise TypeError('type must be a str or valid type, but got {}'.format(\n            type(obj_type)))\n    if default_args is not None:\n        for name, value in default_args.items():\n            args.setdefault(name, value)\n    return obj_type(**args)\n\n\ndef build(cfg, registry, default_args=None):\n    if isinstance(cfg, list):\n        modules = [_build_module(cfg_, registry, default_args) for cfg_ in cfg]\n        return nn.Sequential(*modules)\n    else:\n        return _build_module(cfg, registry, default_args)\n\n\ndef build_backbone(cfg):\n    return build(cfg, BACKBONES)\n\n\ndef build_neck(cfg):\n    return build(cfg, NECKS)\n\n\ndef build_roi_extractor(cfg):\n    return build(cfg, ROI_EXTRACTORS)\n\n\ndef build_head(cfg):\n    return build(cfg, HEADS)\n\n\ndef build_detector(cfg, train_cfg=None, test_cfg=None):\n    return build(cfg, DETECTORS, dict(train_cfg=train_cfg, test_cfg=test_cfg))\n\n"""
detection/mmdet/models/registry.py,1,"b'import torch.nn as nn\n\n\nclass Registry(object):\n\n    def __init__(self, name):\n        self._name = name\n        self._module_dict = dict()\n\n    @property\n    def name(self):\n        return self._name\n\n    @property\n    def module_dict(self):\n        return self._module_dict\n\n    def _register_module(self, module_class):\n        """"""Register a module.\n\n        Args:\n            module (:obj:`nn.Module`): Module to be registered.\n        """"""\n        if not issubclass(module_class, nn.Module):\n            raise TypeError(\n                \'module must be a child of nn.Module, but got {}\'.format(\n                    module_class))\n        module_name = module_class.__name__\n        if module_name in self._module_dict:\n            raise KeyError(\'{} is already registered in {}\'.format(\n                module_name, self.name))\n        self._module_dict[module_name] = module_class\n\n    def register_module(self, cls):\n        self._register_module(cls)\n        return cls\n\n\nBACKBONES = Registry(\'backbone\')\nNECKS = Registry(\'neck\')\nROI_EXTRACTORS = Registry(\'roi_extractor\')\nHEADS = Registry(\'head\')\nDETECTORS = Registry(\'detector\')\n'"
detection/mmdet/ops/__init__.py,0,"b""from .dcn import (DeformConv, DeformRoIPooling, DeformRoIPoolingPack,\n                  ModulatedDeformRoIPoolingPack, ModulatedDeformConv,\n                  ModulatedDeformConvPack, deform_conv, modulated_deform_conv,\n                  deform_roi_pooling)\nfrom .nms import nms, soft_nms\nfrom .roi_align import RoIAlign, roi_align\nfrom .roi_pool import RoIPool, roi_pool\n\n__all__ = [\n    'nms', 'soft_nms', 'RoIAlign', 'roi_align', 'RoIPool', 'roi_pool',\n    'DeformConv', 'DeformRoIPooling', 'DeformRoIPoolingPack',\n    'ModulatedDeformRoIPoolingPack', 'ModulatedDeformConv',\n    'ModulatedDeformConvPack', 'deform_conv', 'modulated_deform_conv',\n    'deform_roi_pooling'\n]\n"""
detection/tools/convert_datasets/pascal_voc.py,0,"b'import argparse\nimport os.path as osp\nimport xml.etree.ElementTree as ET\n\nimport mmcv\nimport numpy as np\n\nfrom mmdet.core import voc_classes\n\nlabel_ids = {name: i + 1 for i, name in enumerate(voc_classes())}\n\n\ndef parse_xml(args):\n    xml_path, img_path = args\n    tree = ET.parse(xml_path)\n    root = tree.getroot()\n    size = root.find(\'size\')\n    w = int(size.find(\'width\').text)\n    h = int(size.find(\'height\').text)\n    bboxes = []\n    labels = []\n    bboxes_ignore = []\n    labels_ignore = []\n    for obj in root.findall(\'object\'):\n        name = obj.find(\'name\').text\n        label = label_ids[name]\n        difficult = int(obj.find(\'difficult\').text)\n        bnd_box = obj.find(\'bndbox\')\n        bbox = [\n            int(bnd_box.find(\'xmin\').text),\n            int(bnd_box.find(\'ymin\').text),\n            int(bnd_box.find(\'xmax\').text),\n            int(bnd_box.find(\'ymax\').text)\n        ]\n        if difficult:\n            bboxes_ignore.append(bbox)\n            labels_ignore.append(label)\n        else:\n            bboxes.append(bbox)\n            labels.append(label)\n    if not bboxes:\n        bboxes = np.zeros((0, 4))\n        labels = np.zeros((0, ))\n    else:\n        bboxes = np.array(bboxes, ndmin=2) - 1\n        labels = np.array(labels)\n    if not bboxes_ignore:\n        bboxes_ignore = np.zeros((0, 4))\n        labels_ignore = np.zeros((0, ))\n    else:\n        bboxes_ignore = np.array(bboxes_ignore, ndmin=2) - 1\n        labels_ignore = np.array(labels_ignore)\n    annotation = {\n        \'filename\': img_path,\n        \'width\': w,\n        \'height\': h,\n        \'ann\': {\n            \'bboxes\': bboxes.astype(np.float32),\n            \'labels\': labels.astype(np.int64),\n            \'bboxes_ignore\': bboxes_ignore.astype(np.float32),\n            \'labels_ignore\': labels_ignore.astype(np.int64)\n        }\n    }\n    return annotation\n\n\ndef cvt_annotations(devkit_path, years, split, out_file):\n    if not isinstance(years, list):\n        years = [years]\n    annotations = []\n    for year in years:\n        filelist = osp.join(devkit_path, \'VOC{}/ImageSets/Main/{}.txt\'.format(\n            year, split))\n        if not osp.isfile(filelist):\n            print(\'filelist does not exist: {}, skip voc{} {}\'.format(\n                filelist, year, split))\n            return\n        img_names = mmcv.list_from_file(filelist)\n        xml_paths = [\n            osp.join(devkit_path, \'VOC{}/Annotations/{}.xml\'.format(\n                year, img_name)) for img_name in img_names\n        ]\n        img_paths = [\n            \'VOC{}/JPEGImages/{}.jpg\'.format(year, img_name)\n            for img_name in img_names\n        ]\n        part_annotations = mmcv.track_progress(parse_xml,\n                                               list(zip(xml_paths, img_paths)))\n        annotations.extend(part_annotations)\n    mmcv.dump(annotations, out_file)\n    return annotations\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser(\n        description=\'Convert PASCAL VOC annotations to mmdetection format\')\n    parser.add_argument(\'devkit_path\', help=\'pascal voc devkit path\')\n    parser.add_argument(\'-o\', \'--out-dir\', help=\'output path\')\n    args = parser.parse_args()\n    return args\n\n\ndef main():\n    args = parse_args()\n    devkit_path = args.devkit_path\n    out_dir = args.out_dir if args.out_dir else devkit_path\n    mmcv.mkdir_or_exist(out_dir)\n\n    years = []\n    if osp.isdir(osp.join(devkit_path, \'VOC2007\')):\n        years.append(\'2007\')\n    if osp.isdir(osp.join(devkit_path, \'VOC2012\')):\n        years.append(\'2012\')\n    if \'2007\' in years and \'2012\' in years:\n        years.append([\'2007\', \'2012\'])\n    if not years:\n        raise IOError(\'The devkit path {} contains neither ""VOC2007"" nor \'\n                      \'""VOC2012"" subfolder\'.format(devkit_path))\n    for year in years:\n        if year == \'2007\':\n            prefix = \'voc07\'\n        elif year == \'2012\':\n            prefix = \'voc12\'\n        elif year == [\'2007\', \'2012\']:\n            prefix = \'voc0712\'\n        for split in [\'train\', \'val\', \'trainval\']:\n            dataset_name = prefix + \'_\' + split\n            print(\'processing {} ...\'.format(dataset_name))\n            cvt_annotations(devkit_path, year, split,\n                            osp.join(out_dir, dataset_name + \'.pkl\'))\n        if not isinstance(year, list):\n            dataset_name = prefix + \'_test\'\n            print(\'processing {} ...\'.format(dataset_name))\n            cvt_annotations(devkit_path, year, \'test\',\n                            osp.join(out_dir, dataset_name + \'.pkl\'))\n    print(\'Done!\')\n\n\nif __name__ == \'__main__\':\n    main()\n'"
classification/utils/progress/progress/__init__.py,0,"b'# Copyright (c) 2012 Giorgos Verigakis <verigak@gmail.com>\n#\n# Permission to use, copy, modify, and distribute this software for any\n# purpose with or without fee is hereby granted, provided that the above\n# copyright notice and this permission notice appear in all copies.\n#\n# THE SOFTWARE IS PROVIDED ""AS IS"" AND THE AUTHOR DISCLAIMS ALL WARRANTIES\n# WITH REGARD TO THIS SOFTWARE INCLUDING ALL IMPLIED WARRANTIES OF\n# MERCHANTABILITY AND FITNESS. IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR\n# ANY SPECIAL, DIRECT, INDIRECT, OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES\n# WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN AN\n# ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION, ARISING OUT OF\n# OR IN CONNECTION WITH THE USE OR PERFORMANCE OF THIS SOFTWARE.\n\nfrom __future__ import division\n\nfrom collections import deque\nfrom datetime import timedelta\nfrom math import ceil\nfrom sys import stderr\nfrom time import time\n\n\n__version__ = \'1.3\'\n\n\nclass Infinite(object):\n    file = stderr\n    sma_window = 10         # Simple Moving Average window\n\n    def __init__(self, *args, **kwargs):\n        self.index = 0\n        self.start_ts = time()\n        self.avg = 0\n        self._ts = self.start_ts\n        self._xput = deque(maxlen=self.sma_window)\n        for key, val in kwargs.items():\n            setattr(self, key, val)\n\n    def __getitem__(self, key):\n        if key.startswith(\'_\'):\n            return None\n        return getattr(self, key, None)\n\n    @property\n    def elapsed(self):\n        return int(time() - self.start_ts)\n\n    @property\n    def elapsed_td(self):\n        return timedelta(seconds=self.elapsed)\n\n    def update_avg(self, n, dt):\n        if n > 0:\n            self._xput.append(dt / n)\n            self.avg = sum(self._xput) / len(self._xput)\n\n    def update(self):\n        pass\n\n    def start(self):\n        pass\n\n    def finish(self):\n        pass\n\n    def next(self, n=1):\n        now = time()\n        dt = now - self._ts\n        self.update_avg(n, dt)\n        self._ts = now\n        self.index = self.index + n\n        self.update()\n\n    def iter(self, it):\n        try:\n            for x in it:\n                yield x\n                self.next()\n        finally:\n            self.finish()\n\n\nclass Progress(Infinite):\n    def __init__(self, *args, **kwargs):\n        super(Progress, self).__init__(*args, **kwargs)\n        self.max = kwargs.get(\'max\', 100)\n\n    @property\n    def eta(self):\n        return int(ceil(self.avg * self.remaining))\n\n    @property\n    def eta_td(self):\n        return timedelta(seconds=self.eta)\n\n    @property\n    def percent(self):\n        return self.progress * 100\n\n    @property\n    def progress(self):\n        return min(1, self.index / self.max)\n\n    @property\n    def remaining(self):\n        return max(self.max - self.index, 0)\n\n    def start(self):\n        self.update()\n\n    def goto(self, index):\n        incr = index - self.index\n        self.next(incr)\n\n    def iter(self, it):\n        try:\n            self.max = len(it)\n        except TypeError:\n            pass\n\n        try:\n            for x in it:\n                yield x\n                self.next()\n        finally:\n            self.finish()\n'"
classification/utils/progress/progress/bar.py,0,"b'# -*- coding: utf-8 -*-\n\n# Copyright (c) 2012 Giorgos Verigakis <verigak@gmail.com>\n#\n# Permission to use, copy, modify, and distribute this software for any\n# purpose with or without fee is hereby granted, provided that the above\n# copyright notice and this permission notice appear in all copies.\n#\n# THE SOFTWARE IS PROVIDED ""AS IS"" AND THE AUTHOR DISCLAIMS ALL WARRANTIES\n# WITH REGARD TO THIS SOFTWARE INCLUDING ALL IMPLIED WARRANTIES OF\n# MERCHANTABILITY AND FITNESS. IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR\n# ANY SPECIAL, DIRECT, INDIRECT, OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES\n# WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN AN\n# ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION, ARISING OUT OF\n# OR IN CONNECTION WITH THE USE OR PERFORMANCE OF THIS SOFTWARE.\n\nfrom __future__ import unicode_literals\nfrom . import Progress\nfrom .helpers import WritelnMixin\n\n\nclass Bar(WritelnMixin, Progress):\n    width = 32\n    message = \'\'\n    suffix = \'%(index)d/%(max)d\'\n    bar_prefix = \' |\'\n    bar_suffix = \'| \'\n    empty_fill = \' \'\n    fill = \'#\'\n    hide_cursor = True\n\n    def update(self):\n        filled_length = int(self.width * self.progress)\n        empty_length = self.width - filled_length\n\n        message = self.message % self\n        bar = self.fill * filled_length\n        empty = self.empty_fill * empty_length\n        suffix = self.suffix % self\n        line = \'\'.join([message, self.bar_prefix, bar, empty, self.bar_suffix,\n                        suffix])\n        self.writeln(line)\n\n\nclass ChargingBar(Bar):\n    suffix = \'%(percent)d%%\'\n    bar_prefix = \' \'\n    bar_suffix = \' \'\n    empty_fill = \'\xe2\x88\x99\'\n    fill = \'\xe2\x96\x88\'\n\n\nclass FillingSquaresBar(ChargingBar):\n    empty_fill = \'\xe2\x96\xa2\'\n    fill = \'\xe2\x96\xa3\'\n\n\nclass FillingCirclesBar(ChargingBar):\n    empty_fill = \'\xe2\x97\xaf\'\n    fill = \'\xe2\x97\x89\'\n\n\nclass IncrementalBar(Bar):\n    phases = (\' \', \'\xe2\x96\x8f\', \'\xe2\x96\x8e\', \'\xe2\x96\x8d\', \'\xe2\x96\x8c\', \'\xe2\x96\x8b\', \'\xe2\x96\x8a\', \'\xe2\x96\x89\', \'\xe2\x96\x88\')\n\n    def update(self):\n        nphases = len(self.phases)\n        filled_len = self.width * self.progress\n        nfull = int(filled_len)                      # Number of full chars\n        phase = int((filled_len - nfull) * nphases)  # Phase of last char\n        nempty = self.width - nfull                  # Number of empty chars\n\n        message = self.message % self\n        bar = self.phases[-1] * nfull\n        current = self.phases[phase] if phase > 0 else \'\'\n        empty = self.empty_fill * max(0, nempty - len(current))\n        suffix = self.suffix % self\n        line = \'\'.join([message, self.bar_prefix, bar, current, empty,\n                        self.bar_suffix, suffix])\n        self.writeln(line)\n\n\nclass PixelBar(IncrementalBar):\n    phases = (\'\xe2\xa1\x80\', \'\xe2\xa1\x84\', \'\xe2\xa1\x86\', \'\xe2\xa1\x87\', \'\xe2\xa3\x87\', \'\xe2\xa3\xa7\', \'\xe2\xa3\xb7\', \'\xe2\xa3\xbf\')\n\n\nclass ShadyBar(IncrementalBar):\n    phases = (\' \', \'\xe2\x96\x91\', \'\xe2\x96\x92\', \'\xe2\x96\x93\', \'\xe2\x96\x88\')\n'"
classification/utils/progress/progress/counter.py,0,"b'# -*- coding: utf-8 -*-\n\n# Copyright (c) 2012 Giorgos Verigakis <verigak@gmail.com>\n#\n# Permission to use, copy, modify, and distribute this software for any\n# purpose with or without fee is hereby granted, provided that the above\n# copyright notice and this permission notice appear in all copies.\n#\n# THE SOFTWARE IS PROVIDED ""AS IS"" AND THE AUTHOR DISCLAIMS ALL WARRANTIES\n# WITH REGARD TO THIS SOFTWARE INCLUDING ALL IMPLIED WARRANTIES OF\n# MERCHANTABILITY AND FITNESS. IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR\n# ANY SPECIAL, DIRECT, INDIRECT, OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES\n# WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN AN\n# ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION, ARISING OUT OF\n# OR IN CONNECTION WITH THE USE OR PERFORMANCE OF THIS SOFTWARE.\n\nfrom __future__ import unicode_literals\nfrom . import Infinite, Progress\nfrom .helpers import WriteMixin\n\n\nclass Counter(WriteMixin, Infinite):\n    message = \'\'\n    hide_cursor = True\n\n    def update(self):\n        self.write(str(self.index))\n\n\nclass Countdown(WriteMixin, Progress):\n    hide_cursor = True\n\n    def update(self):\n        self.write(str(self.remaining))\n\n\nclass Stack(WriteMixin, Progress):\n    phases = (\' \', \'\xe2\x96\x81\', \'\xe2\x96\x82\', \'\xe2\x96\x83\', \'\xe2\x96\x84\', \'\xe2\x96\x85\', \'\xe2\x96\x86\', \'\xe2\x96\x87\', \'\xe2\x96\x88\')\n    hide_cursor = True\n\n    def update(self):\n        nphases = len(self.phases)\n        i = min(nphases - 1, int(self.progress * nphases))\n        self.write(self.phases[i])\n\n\nclass Pie(Stack):\n    phases = (\'\xe2\x97\x8b\', \'\xe2\x97\x94\', \'\xe2\x97\x91\', \'\xe2\x97\x95\', \'\xe2\x97\x8f\')\n'"
classification/utils/progress/progress/helpers.py,0,"b'# Copyright (c) 2012 Giorgos Verigakis <verigak@gmail.com>\n#\n# Permission to use, copy, modify, and distribute this software for any\n# purpose with or without fee is hereby granted, provided that the above\n# copyright notice and this permission notice appear in all copies.\n#\n# THE SOFTWARE IS PROVIDED ""AS IS"" AND THE AUTHOR DISCLAIMS ALL WARRANTIES\n# WITH REGARD TO THIS SOFTWARE INCLUDING ALL IMPLIED WARRANTIES OF\n# MERCHANTABILITY AND FITNESS. IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR\n# ANY SPECIAL, DIRECT, INDIRECT, OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES\n# WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN AN\n# ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION, ARISING OUT OF\n# OR IN CONNECTION WITH THE USE OR PERFORMANCE OF THIS SOFTWARE.\n\nfrom __future__ import print_function\n\n\nHIDE_CURSOR = \'\\x1b[?25l\'\nSHOW_CURSOR = \'\\x1b[?25h\'\n\n\nclass WriteMixin(object):\n    hide_cursor = False\n\n    def __init__(self, message=None, **kwargs):\n        super(WriteMixin, self).__init__(**kwargs)\n        self._width = 0\n        if message:\n            self.message = message\n\n        if self.file.isatty():\n            if self.hide_cursor:\n                print(HIDE_CURSOR, end=\'\', file=self.file)\n            print(self.message, end=\'\', file=self.file)\n            self.file.flush()\n\n    def write(self, s):\n        if self.file.isatty():\n            b = \'\\b\' * self._width\n            c = s.ljust(self._width)\n            print(b + c, end=\'\', file=self.file)\n            self._width = max(self._width, len(s))\n            self.file.flush()\n\n    def finish(self):\n        if self.file.isatty() and self.hide_cursor:\n            print(SHOW_CURSOR, end=\'\', file=self.file)\n\n\nclass WritelnMixin(object):\n    hide_cursor = False\n\n    def __init__(self, message=None, **kwargs):\n        super(WritelnMixin, self).__init__(**kwargs)\n        if message:\n            self.message = message\n\n        if self.file.isatty() and self.hide_cursor:\n            print(HIDE_CURSOR, end=\'\', file=self.file)\n\n    def clearln(self):\n        if self.file.isatty():\n            print(\'\\r\\x1b[K\', end=\'\', file=self.file)\n\n    def writeln(self, line):\n        if self.file.isatty():\n            self.clearln()\n            print(line, end=\'\', file=self.file)\n            self.file.flush()\n\n    def finish(self):\n        if self.file.isatty():\n            print(file=self.file)\n            if self.hide_cursor:\n                print(SHOW_CURSOR, end=\'\', file=self.file)\n\n\nfrom signal import signal, SIGINT\nfrom sys import exit\n\n\nclass SigIntMixin(object):\n    """"""Registers a signal handler that calls finish on SIGINT""""""\n\n    def __init__(self, *args, **kwargs):\n        super(SigIntMixin, self).__init__(*args, **kwargs)\n        signal(SIGINT, self._sigint_handler)\n\n    def _sigint_handler(self, signum, frame):\n        self.finish()\n        exit(0)\n'"
classification/utils/progress/progress/spinner.py,0,"b'# -*- coding: utf-8 -*-\n\n# Copyright (c) 2012 Giorgos Verigakis <verigak@gmail.com>\n#\n# Permission to use, copy, modify, and distribute this software for any\n# purpose with or without fee is hereby granted, provided that the above\n# copyright notice and this permission notice appear in all copies.\n#\n# THE SOFTWARE IS PROVIDED ""AS IS"" AND THE AUTHOR DISCLAIMS ALL WARRANTIES\n# WITH REGARD TO THIS SOFTWARE INCLUDING ALL IMPLIED WARRANTIES OF\n# MERCHANTABILITY AND FITNESS. IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR\n# ANY SPECIAL, DIRECT, INDIRECT, OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES\n# WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN AN\n# ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION, ARISING OUT OF\n# OR IN CONNECTION WITH THE USE OR PERFORMANCE OF THIS SOFTWARE.\n\nfrom __future__ import unicode_literals\nfrom . import Infinite\nfrom .helpers import WriteMixin\n\n\nclass Spinner(WriteMixin, Infinite):\n    message = \'\'\n    phases = (\'-\', \'\\\\\', \'|\', \'/\')\n    hide_cursor = True\n\n    def update(self):\n        i = self.index % len(self.phases)\n        self.write(self.phases[i])\n\n\nclass PieSpinner(Spinner):\n    phases = [\'\xe2\x97\xb7\', \'\xe2\x97\xb6\', \'\xe2\x97\xb5\', \'\xe2\x97\xb4\']\n\n\nclass MoonSpinner(Spinner):\n    phases = [\'\xe2\x97\x91\', \'\xe2\x97\x92\', \'\xe2\x97\x90\', \'\xe2\x97\x93\']\n\n\nclass LineSpinner(Spinner):\n    phases = [\'\xe2\x8e\xba\', \'\xe2\x8e\xbb\', \'\xe2\x8e\xbc\', \'\xe2\x8e\xbd\', \'\xe2\x8e\xbc\', \'\xe2\x8e\xbb\']\n\nclass PixelSpinner(Spinner):\n    phases = [\'\xe2\xa3\xbe\',\'\xe2\xa3\xb7\', \'\xe2\xa3\xaf\', \'\xe2\xa3\x9f\', \'\xe2\xa1\xbf\', \'\xe2\xa2\xbf\', \'\xe2\xa3\xbb\', \'\xe2\xa3\xbd\']\n'"
detection/mmdet/core/anchor/__init__.py,0,"b""from .anchor_generator import AnchorGenerator\nfrom .anchor_target import anchor_target\n\n__all__ = ['AnchorGenerator', 'anchor_target']\n"""
detection/mmdet/core/anchor/anchor_generator.py,9,"b""import torch\n\n\nclass AnchorGenerator(object):\n\n    def __init__(self, base_size, scales, ratios, scale_major=True, ctr=None):\n        self.base_size = base_size\n        self.scales = torch.Tensor(scales)\n        self.ratios = torch.Tensor(ratios)\n        self.scale_major = scale_major\n        self.ctr = ctr\n        self.base_anchors = self.gen_base_anchors()\n\n    @property\n    def num_base_anchors(self):\n        return self.base_anchors.size(0)\n\n    def gen_base_anchors(self):\n        w = self.base_size\n        h = self.base_size\n        if self.ctr is None:\n            x_ctr = 0.5 * (w - 1)\n            y_ctr = 0.5 * (h - 1)\n        else:\n            x_ctr, y_ctr = self.ctr\n\n        h_ratios = torch.sqrt(self.ratios)\n        w_ratios = 1 / h_ratios\n        if self.scale_major:\n            ws = (w * w_ratios[:, None] * self.scales[None, :]).view(-1)\n            hs = (h * h_ratios[:, None] * self.scales[None, :]).view(-1)\n        else:\n            ws = (w * self.scales[:, None] * w_ratios[None, :]).view(-1)\n            hs = (h * self.scales[:, None] * h_ratios[None, :]).view(-1)\n\n        base_anchors = torch.stack(\n            [\n                x_ctr - 0.5 * (ws - 1), y_ctr - 0.5 * (hs - 1),\n                x_ctr + 0.5 * (ws - 1), y_ctr + 0.5 * (hs - 1)\n            ],\n            dim=-1).round()\n\n        return base_anchors\n\n    def _meshgrid(self, x, y, row_major=True):\n        xx = x.repeat(len(y))\n        yy = y.view(-1, 1).repeat(1, len(x)).view(-1)\n        if row_major:\n            return xx, yy\n        else:\n            return yy, xx\n\n    def grid_anchors(self, featmap_size, stride=16, device='cuda'):\n        base_anchors = self.base_anchors.to(device)\n\n        feat_h, feat_w = featmap_size\n        shift_x = torch.arange(0, feat_w, device=device) * stride\n        shift_y = torch.arange(0, feat_h, device=device) * stride\n        shift_xx, shift_yy = self._meshgrid(shift_x, shift_y)\n        shifts = torch.stack([shift_xx, shift_yy, shift_xx, shift_yy], dim=-1)\n        shifts = shifts.type_as(base_anchors)\n        # first feat_w elements correspond to the first row of shifts\n        # add A anchors (1, A, 4) to K shifts (K, 1, 4) to get\n        # shifted anchors (K, A, 4), reshape to (K*A, 4)\n\n        all_anchors = base_anchors[None, :, :] + shifts[:, None, :]\n        all_anchors = all_anchors.view(-1, 4)\n        # first A rows correspond to A anchors of (0, 0) in feature map,\n        # then (0, 1), (0, 2), ...\n        return all_anchors\n\n    def valid_flags(self, featmap_size, valid_size, device='cuda'):\n        feat_h, feat_w = featmap_size\n        valid_h, valid_w = valid_size\n        assert valid_h <= feat_h and valid_w <= feat_w\n        valid_x = torch.zeros(feat_w, dtype=torch.uint8, device=device)\n        valid_y = torch.zeros(feat_h, dtype=torch.uint8, device=device)\n        valid_x[:valid_w] = 1\n        valid_y[:valid_h] = 1\n        valid_xx, valid_yy = self._meshgrid(valid_x, valid_y)\n        valid = valid_xx & valid_yy\n        valid = valid[:, None].expand(\n            valid.size(0), self.num_base_anchors).contiguous().view(-1)\n        return valid\n"""
detection/mmdet/core/anchor/anchor_target.py,8,"b'import torch\n\nfrom ..bbox import assign_and_sample, build_assigner, PseudoSampler, bbox2delta\nfrom ..utils import multi_apply\n\n\ndef anchor_target(anchor_list,\n                  valid_flag_list,\n                  gt_bboxes_list,\n                  img_metas,\n                  target_means,\n                  target_stds,\n                  cfg,\n                  gt_bboxes_ignore_list=None,\n                  gt_labels_list=None,\n                  label_channels=1,\n                  sampling=True,\n                  unmap_outputs=True):\n    """"""Compute regression and classification targets for anchors.\n\n    Args:\n        anchor_list (list[list]): Multi level anchors of each image.\n        valid_flag_list (list[list]): Multi level valid flags of each image.\n        gt_bboxes_list (list[Tensor]): Ground truth bboxes of each image.\n        img_metas (list[dict]): Meta info of each image.\n        target_means (Iterable): Mean value of regression targets.\n        target_stds (Iterable): Std value of regression targets.\n        cfg (dict): RPN train configs.\n\n    Returns:\n        tuple\n    """"""\n    num_imgs = len(img_metas)\n    assert len(anchor_list) == len(valid_flag_list) == num_imgs\n\n    # anchor number of multi levels\n    num_level_anchors = [anchors.size(0) for anchors in anchor_list[0]]\n    # concat all level anchors and flags to a single tensor\n    for i in range(num_imgs):\n        assert len(anchor_list[i]) == len(valid_flag_list[i])\n        anchor_list[i] = torch.cat(anchor_list[i])\n        valid_flag_list[i] = torch.cat(valid_flag_list[i])\n\n    # compute targets for each image\n    if gt_bboxes_ignore_list is None:\n        gt_bboxes_ignore_list = [None for _ in range(num_imgs)]\n    if gt_labels_list is None:\n        gt_labels_list = [None for _ in range(num_imgs)]\n    (all_labels, all_label_weights, all_bbox_targets, all_bbox_weights,\n     pos_inds_list, neg_inds_list) = multi_apply(\n         anchor_target_single,\n         anchor_list,\n         valid_flag_list,\n         gt_bboxes_list,\n         gt_bboxes_ignore_list,\n         gt_labels_list,\n         img_metas,\n         target_means=target_means,\n         target_stds=target_stds,\n         cfg=cfg,\n         label_channels=label_channels,\n         sampling=sampling,\n         unmap_outputs=unmap_outputs)\n    # no valid anchors\n    if any([labels is None for labels in all_labels]):\n        return None\n    # sampled anchors of all images\n    num_total_pos = sum([max(inds.numel(), 1) for inds in pos_inds_list])\n    num_total_neg = sum([max(inds.numel(), 1) for inds in neg_inds_list])\n    # split targets to a list w.r.t. multiple levels\n    labels_list = images_to_levels(all_labels, num_level_anchors)\n    label_weights_list = images_to_levels(all_label_weights, num_level_anchors)\n    bbox_targets_list = images_to_levels(all_bbox_targets, num_level_anchors)\n    bbox_weights_list = images_to_levels(all_bbox_weights, num_level_anchors)\n    return (labels_list, label_weights_list, bbox_targets_list,\n            bbox_weights_list, num_total_pos, num_total_neg)\n\n\ndef images_to_levels(target, num_level_anchors):\n    """"""Convert targets by image to targets by feature level.\n\n    [target_img0, target_img1] -> [target_level0, target_level1, ...]\n    """"""\n    target = torch.stack(target, 0)\n    level_targets = []\n    start = 0\n    for n in num_level_anchors:\n        end = start + n\n        level_targets.append(target[:, start:end].squeeze(0))\n        start = end\n    return level_targets\n\n\ndef anchor_target_single(flat_anchors,\n                         valid_flags,\n                         gt_bboxes,\n                         gt_bboxes_ignore,\n                         gt_labels,\n                         img_meta,\n                         target_means,\n                         target_stds,\n                         cfg,\n                         label_channels=1,\n                         sampling=True,\n                         unmap_outputs=True):\n    inside_flags = anchor_inside_flags(flat_anchors, valid_flags,\n                                       img_meta[\'img_shape\'][:2],\n                                       cfg.allowed_border)\n    if not inside_flags.any():\n        return (None, ) * 6\n    # assign gt and sample anchors\n    anchors = flat_anchors[inside_flags, :]\n\n    if sampling:\n        assign_result, sampling_result = assign_and_sample(\n            anchors, gt_bboxes, gt_bboxes_ignore, None, cfg)\n    else:\n        bbox_assigner = build_assigner(cfg.assigner)\n        assign_result = bbox_assigner.assign(anchors, gt_bboxes,\n                                             gt_bboxes_ignore, gt_labels)\n        bbox_sampler = PseudoSampler()\n        sampling_result = bbox_sampler.sample(assign_result, anchors,\n                                              gt_bboxes)\n\n    num_valid_anchors = anchors.shape[0]\n    bbox_targets = torch.zeros_like(anchors)\n    bbox_weights = torch.zeros_like(anchors)\n    labels = anchors.new_zeros(num_valid_anchors, dtype=torch.long)\n    label_weights = anchors.new_zeros(num_valid_anchors, dtype=torch.float)\n\n    pos_inds = sampling_result.pos_inds\n    neg_inds = sampling_result.neg_inds\n    if len(pos_inds) > 0:\n        pos_bbox_targets = bbox2delta(sampling_result.pos_bboxes,\n                                      sampling_result.pos_gt_bboxes,\n                                      target_means, target_stds)\n        bbox_targets[pos_inds, :] = pos_bbox_targets\n        bbox_weights[pos_inds, :] = 1.0\n        if gt_labels is None:\n            labels[pos_inds] = 1\n        else:\n            labels[pos_inds] = gt_labels[sampling_result.pos_assigned_gt_inds]\n        if cfg.pos_weight <= 0:\n            label_weights[pos_inds] = 1.0\n        else:\n            label_weights[pos_inds] = cfg.pos_weight\n    if len(neg_inds) > 0:\n        label_weights[neg_inds] = 1.0\n\n    # map up to original set of anchors\n    if unmap_outputs:\n        num_total_anchors = flat_anchors.size(0)\n        labels = unmap(labels, num_total_anchors, inside_flags)\n        label_weights = unmap(label_weights, num_total_anchors, inside_flags)\n        if label_channels > 1:\n            labels, label_weights = expand_binary_labels(\n                labels, label_weights, label_channels)\n        bbox_targets = unmap(bbox_targets, num_total_anchors, inside_flags)\n        bbox_weights = unmap(bbox_weights, num_total_anchors, inside_flags)\n\n    return (labels, label_weights, bbox_targets, bbox_weights, pos_inds,\n            neg_inds)\n\n\ndef expand_binary_labels(labels, label_weights, label_channels):\n    bin_labels = labels.new_full((labels.size(0), label_channels), 0)\n    inds = torch.nonzero(labels >= 1).squeeze()\n    if inds.numel() > 0:\n        bin_labels[inds, labels[inds] - 1] = 1\n    bin_label_weights = label_weights.view(-1, 1).expand(\n        label_weights.size(0), label_channels)\n    return bin_labels, bin_label_weights\n\n\ndef anchor_inside_flags(flat_anchors, valid_flags, img_shape,\n                        allowed_border=0):\n    img_h, img_w = img_shape[:2]\n    if allowed_border >= 0:\n        inside_flags = valid_flags & \\\n            (flat_anchors[:, 0] >= -allowed_border) & \\\n            (flat_anchors[:, 1] >= -allowed_border) & \\\n            (flat_anchors[:, 2] < img_w + allowed_border) & \\\n            (flat_anchors[:, 3] < img_h + allowed_border)\n    else:\n        inside_flags = valid_flags\n    return inside_flags\n\n\ndef unmap(data, count, inds, fill=0):\n    """""" Unmap a subset of item (data) back to the original set of items (of\n    size count) """"""\n    if data.dim() == 1:\n        ret = data.new_full((count, ), fill)\n        ret[inds] = data\n    else:\n        new_size = (count, ) + data.size()[1:]\n        ret = data.new_full(new_size, fill)\n        ret[inds, :] = data\n    return ret\n'"
detection/mmdet/core/bbox/__init__.py,0,"b""from .geometry import bbox_overlaps\nfrom .assigners import BaseAssigner, MaxIoUAssigner, AssignResult\nfrom .samplers import (BaseSampler, PseudoSampler, RandomSampler,\n                       InstanceBalancedPosSampler, IoUBalancedNegSampler,\n                       CombinedSampler, SamplingResult)\nfrom .assign_sampling import build_assigner, build_sampler, assign_and_sample\nfrom .transforms import (bbox2delta, delta2bbox, bbox_flip, bbox_mapping,\n                         bbox_mapping_back, bbox2roi, roi2bbox, bbox2result)\nfrom .bbox_target import bbox_target\n\n__all__ = [\n    'bbox_overlaps', 'BaseAssigner', 'MaxIoUAssigner', 'AssignResult',\n    'BaseSampler', 'PseudoSampler', 'RandomSampler',\n    'InstanceBalancedPosSampler', 'IoUBalancedNegSampler', 'CombinedSampler',\n    'SamplingResult', 'build_assigner', 'build_sampler', 'assign_and_sample',\n    'bbox2delta', 'delta2bbox', 'bbox_flip', 'bbox_mapping',\n    'bbox_mapping_back', 'bbox2roi', 'roi2bbox', 'bbox2result', 'bbox_target'\n]\n"""
detection/mmdet/core/bbox/assign_sampling.py,0,"b""import mmcv\n\nfrom . import assigners, samplers\n\n\ndef build_assigner(cfg, **kwargs):\n    if isinstance(cfg, assigners.BaseAssigner):\n        return cfg\n    elif isinstance(cfg, dict):\n        return mmcv.runner.obj_from_dict(\n            cfg, assigners, default_args=kwargs)\n    else:\n        raise TypeError('Invalid type {} for building a sampler'.format(\n            type(cfg)))\n\n\ndef build_sampler(cfg, **kwargs):\n    if isinstance(cfg, samplers.BaseSampler):\n        return cfg\n    elif isinstance(cfg, dict):\n        return mmcv.runner.obj_from_dict(\n            cfg, samplers, default_args=kwargs)\n    else:\n        raise TypeError('Invalid type {} for building a sampler'.format(\n            type(cfg)))\n\n\ndef assign_and_sample(bboxes, gt_bboxes, gt_bboxes_ignore, gt_labels, cfg):\n    bbox_assigner = build_assigner(cfg.assigner)\n    bbox_sampler = build_sampler(cfg.sampler)\n    assign_result = bbox_assigner.assign(bboxes, gt_bboxes, gt_bboxes_ignore,\n                                         gt_labels)\n    sampling_result = bbox_sampler.sample(assign_result, bboxes, gt_bboxes,\n                                          gt_labels)\n    return assign_result, sampling_result\n"""
detection/mmdet/core/bbox/bbox_target.py,6,"b'import torch\n\nfrom .transforms import bbox2delta\nfrom ..utils import multi_apply\n\n\ndef bbox_target(pos_bboxes_list,\n                neg_bboxes_list,\n                pos_gt_bboxes_list,\n                pos_gt_labels_list,\n                cfg,\n                reg_classes=1,\n                target_means=[.0, .0, .0, .0],\n                target_stds=[1.0, 1.0, 1.0, 1.0],\n                concat=True):\n    labels, label_weights, bbox_targets, bbox_weights = multi_apply(\n        bbox_target_single,\n        pos_bboxes_list,\n        neg_bboxes_list,\n        pos_gt_bboxes_list,\n        pos_gt_labels_list,\n        cfg=cfg,\n        reg_classes=reg_classes,\n        target_means=target_means,\n        target_stds=target_stds)\n\n    if concat:\n        labels = torch.cat(labels, 0)\n        label_weights = torch.cat(label_weights, 0)\n        bbox_targets = torch.cat(bbox_targets, 0)\n        bbox_weights = torch.cat(bbox_weights, 0)\n    return labels, label_weights, bbox_targets, bbox_weights\n\n\ndef bbox_target_single(pos_bboxes,\n                       neg_bboxes,\n                       pos_gt_bboxes,\n                       pos_gt_labels,\n                       cfg,\n                       reg_classes=1,\n                       target_means=[.0, .0, .0, .0],\n                       target_stds=[1.0, 1.0, 1.0, 1.0]):\n    num_pos = pos_bboxes.size(0)\n    num_neg = neg_bboxes.size(0)\n    num_samples = num_pos + num_neg\n    labels = pos_bboxes.new_zeros(num_samples, dtype=torch.long)\n    label_weights = pos_bboxes.new_zeros(num_samples)\n    bbox_targets = pos_bboxes.new_zeros(num_samples, 4)\n    bbox_weights = pos_bboxes.new_zeros(num_samples, 4)\n    if num_pos > 0:\n        labels[:num_pos] = pos_gt_labels\n        pos_weight = 1.0 if cfg.pos_weight <= 0 else cfg.pos_weight\n        label_weights[:num_pos] = pos_weight\n        pos_bbox_targets = bbox2delta(pos_bboxes, pos_gt_bboxes, target_means,\n                                      target_stds)\n        bbox_targets[:num_pos, :] = pos_bbox_targets\n        bbox_weights[:num_pos, :] = 1\n    if num_neg > 0:\n        label_weights[-num_neg:] = 1.0\n    if reg_classes > 1:\n        bbox_targets, bbox_weights = expand_target(bbox_targets, bbox_weights,\n                                                   labels, reg_classes)\n\n    return labels, label_weights, bbox_targets, bbox_weights\n\n\ndef expand_target(bbox_targets, bbox_weights, labels, num_classes):\n    bbox_targets_expand = bbox_targets.new_zeros((bbox_targets.size(0),\n                                                  4 * num_classes))\n    bbox_weights_expand = bbox_weights.new_zeros((bbox_weights.size(0),\n                                                  4 * num_classes))\n    for i in torch.nonzero(labels > 0).squeeze(-1):\n        start, end = labels[i] * 4, (labels[i] + 1) * 4\n        bbox_targets_expand[i, start:end] = bbox_targets[i, :]\n        bbox_weights_expand[i, start:end] = bbox_weights[i, :]\n    return bbox_targets_expand, bbox_weights_expand\n'"
detection/mmdet/core/bbox/geometry.py,4,"b'import torch\n\n\ndef bbox_overlaps(bboxes1, bboxes2, mode=\'iou\', is_aligned=False):\n    """"""Calculate overlap between two set of bboxes.\n\n    If ``is_aligned`` is ``False``, then calculate the ious between each bbox\n    of bboxes1 and bboxes2, otherwise the ious between each aligned pair of\n    bboxes1 and bboxes2.\n\n    Args:\n        bboxes1 (Tensor): shape (m, 4)\n        bboxes2 (Tensor): shape (n, 4), if is_aligned is ``True``, then m and n\n            must be equal.\n        mode (str): ""iou"" (intersection over union) or iof (intersection over\n            foreground).\n\n    Returns:\n        ious(Tensor): shape (m, n) if is_aligned == False else shape (m, 1)\n    """"""\n\n    assert mode in [\'iou\', \'iof\']\n\n    rows = bboxes1.size(0)\n    cols = bboxes2.size(0)\n    if is_aligned:\n        assert rows == cols\n\n    if rows * cols == 0:\n        return bboxes1.new(rows, 1) if is_aligned else bboxes1.new(rows, cols)\n\n    if is_aligned:\n        lt = torch.max(bboxes1[:, :2], bboxes2[:, :2])  # [rows, 2]\n        rb = torch.min(bboxes1[:, 2:], bboxes2[:, 2:])  # [rows, 2]\n\n        wh = (rb - lt + 1).clamp(min=0)  # [rows, 2]\n        overlap = wh[:, 0] * wh[:, 1]\n        area1 = (bboxes1[:, 2] - bboxes1[:, 0] + 1) * (\n            bboxes1[:, 3] - bboxes1[:, 1] + 1)\n\n        if mode == \'iou\':\n            area2 = (bboxes2[:, 2] - bboxes2[:, 0] + 1) * (\n                bboxes2[:, 3] - bboxes2[:, 1] + 1)\n            ious = overlap / (area1 + area2 - overlap)\n        else:\n            ious = overlap / area1\n    else:\n        lt = torch.max(bboxes1[:, None, :2], bboxes2[:, :2])  # [rows, cols, 2]\n        rb = torch.min(bboxes1[:, None, 2:], bboxes2[:, 2:])  # [rows, cols, 2]\n\n        wh = (rb - lt + 1).clamp(min=0)  # [rows, cols, 2]\n        overlap = wh[:, :, 0] * wh[:, :, 1]\n        area1 = (bboxes1[:, 2] - bboxes1[:, 0] + 1) * (\n            bboxes1[:, 3] - bboxes1[:, 1] + 1)\n\n        if mode == \'iou\':\n            area2 = (bboxes2[:, 2] - bboxes2[:, 0] + 1) * (\n                bboxes2[:, 3] - bboxes2[:, 1] + 1)\n            ious = overlap / (area1[:, None] + area2 - overlap)\n        else:\n            ious = overlap / (area1[:, None])\n\n    return ious\n'"
detection/mmdet/core/bbox/transforms.py,10,"b'import mmcv\nimport numpy as np\nimport torch\n\n\ndef bbox2delta(proposals, gt, means=[0, 0, 0, 0], stds=[1, 1, 1, 1]):\n    assert proposals.size() == gt.size()\n\n    proposals = proposals.float()\n    gt = gt.float()\n    px = (proposals[..., 0] + proposals[..., 2]) * 0.5\n    py = (proposals[..., 1] + proposals[..., 3]) * 0.5\n    pw = proposals[..., 2] - proposals[..., 0] + 1.0\n    ph = proposals[..., 3] - proposals[..., 1] + 1.0\n\n    gx = (gt[..., 0] + gt[..., 2]) * 0.5\n    gy = (gt[..., 1] + gt[..., 3]) * 0.5\n    gw = gt[..., 2] - gt[..., 0] + 1.0\n    gh = gt[..., 3] - gt[..., 1] + 1.0\n\n    dx = (gx - px) / pw\n    dy = (gy - py) / ph\n    dw = torch.log(gw / pw)\n    dh = torch.log(gh / ph)\n    deltas = torch.stack([dx, dy, dw, dh], dim=-1)\n\n    means = deltas.new_tensor(means).unsqueeze(0)\n    stds = deltas.new_tensor(stds).unsqueeze(0)\n    deltas = deltas.sub_(means).div_(stds)\n\n    return deltas\n\n\ndef delta2bbox(rois,\n               deltas,\n               means=[0, 0, 0, 0],\n               stds=[1, 1, 1, 1],\n               max_shape=None,\n               wh_ratio_clip=16 / 1000):\n    means = deltas.new_tensor(means).repeat(1, deltas.size(1) // 4)\n    stds = deltas.new_tensor(stds).repeat(1, deltas.size(1) // 4)\n    denorm_deltas = deltas * stds + means\n    dx = denorm_deltas[:, 0::4]\n    dy = denorm_deltas[:, 1::4]\n    dw = denorm_deltas[:, 2::4]\n    dh = denorm_deltas[:, 3::4]\n    max_ratio = np.abs(np.log(wh_ratio_clip))\n    dw = dw.clamp(min=-max_ratio, max=max_ratio)\n    dh = dh.clamp(min=-max_ratio, max=max_ratio)\n    px = ((rois[:, 0] + rois[:, 2]) * 0.5).unsqueeze(1).expand_as(dx)\n    py = ((rois[:, 1] + rois[:, 3]) * 0.5).unsqueeze(1).expand_as(dy)\n    pw = (rois[:, 2] - rois[:, 0] + 1.0).unsqueeze(1).expand_as(dw)\n    ph = (rois[:, 3] - rois[:, 1] + 1.0).unsqueeze(1).expand_as(dh)\n    gw = pw * dw.exp()\n    gh = ph * dh.exp()\n    gx = torch.addcmul(px, 1, pw, dx)  # gx = px + pw * dx\n    gy = torch.addcmul(py, 1, ph, dy)  # gy = py + ph * dy\n    x1 = gx - gw * 0.5 + 0.5\n    y1 = gy - gh * 0.5 + 0.5\n    x2 = gx + gw * 0.5 - 0.5\n    y2 = gy + gh * 0.5 - 0.5\n    if max_shape is not None:\n        x1 = x1.clamp(min=0, max=max_shape[1] - 1)\n        y1 = y1.clamp(min=0, max=max_shape[0] - 1)\n        x2 = x2.clamp(min=0, max=max_shape[1] - 1)\n        y2 = y2.clamp(min=0, max=max_shape[0] - 1)\n    bboxes = torch.stack([x1, y1, x2, y2], dim=-1).view_as(deltas)\n    return bboxes\n\n\ndef bbox_flip(bboxes, img_shape):\n    """"""Flip bboxes horizontally.\n\n    Args:\n        bboxes(Tensor or ndarray): Shape (..., 4*k)\n        img_shape(tuple): Image shape.\n\n    Returns:\n        Same type as `bboxes`: Flipped bboxes.\n    """"""\n    if isinstance(bboxes, torch.Tensor):\n        assert bboxes.shape[-1] % 4 == 0\n        flipped = bboxes.clone()\n        flipped[:, 0::4] = img_shape[1] - bboxes[:, 2::4] - 1\n        flipped[:, 2::4] = img_shape[1] - bboxes[:, 0::4] - 1\n        return flipped\n    elif isinstance(bboxes, np.ndarray):\n        return mmcv.bbox_flip(bboxes, img_shape)\n\n\ndef bbox_mapping(bboxes, img_shape, scale_factor, flip):\n    """"""Map bboxes from the original image scale to testing scale""""""\n    new_bboxes = bboxes * scale_factor\n    if flip:\n        new_bboxes = bbox_flip(new_bboxes, img_shape)\n    return new_bboxes\n\n\ndef bbox_mapping_back(bboxes, img_shape, scale_factor, flip):\n    """"""Map bboxes from testing scale to original image scale""""""\n    new_bboxes = bbox_flip(bboxes, img_shape) if flip else bboxes\n    new_bboxes = new_bboxes / scale_factor\n    return new_bboxes\n\n\ndef bbox2roi(bbox_list):\n    """"""Convert a list of bboxes to roi format.\n\n    Args:\n        bbox_list (list[Tensor]): a list of bboxes corresponding to a batch\n            of images.\n\n    Returns:\n        Tensor: shape (n, 5), [batch_ind, x1, y1, x2, y2]\n    """"""\n    rois_list = []\n    for img_id, bboxes in enumerate(bbox_list):\n        if bboxes.size(0) > 0:\n            img_inds = bboxes.new_full((bboxes.size(0), 1), img_id)\n            rois = torch.cat([img_inds, bboxes[:, :4]], dim=-1)\n        else:\n            rois = bboxes.new_zeros((0, 5))\n        rois_list.append(rois)\n    rois = torch.cat(rois_list, 0)\n    return rois\n\n\ndef roi2bbox(rois):\n    bbox_list = []\n    img_ids = torch.unique(rois[:, 0].cpu(), sorted=True)\n    for img_id in img_ids:\n        inds = (rois[:, 0] == img_id.item())\n        bbox = rois[inds, 1:]\n        bbox_list.append(bbox)\n    return bbox_list\n\n\ndef bbox2result(bboxes, labels, num_classes):\n    """"""Convert detection results to a list of numpy arrays.\n\n    Args:\n        bboxes (Tensor): shape (n, 5)\n        labels (Tensor): shape (n, )\n        num_classes (int): class number, including background class\n\n    Returns:\n        list(ndarray): bbox results of each class\n    """"""\n    if bboxes.shape[0] == 0:\n        return [\n            np.zeros((0, 5), dtype=np.float32) for i in range(num_classes - 1)\n        ]\n    else:\n        bboxes = bboxes.cpu().numpy()\n        labels = labels.cpu().numpy()\n        return [bboxes[labels == i, :] for i in range(num_classes - 1)]\n'"
detection/mmdet/core/evaluation/__init__.py,0,"b""from .class_names import (voc_classes, imagenet_det_classes,\n                          imagenet_vid_classes, coco_classes, dataset_aliases,\n                          get_classes)\nfrom .coco_utils import coco_eval, fast_eval_recall, results2json\nfrom .eval_hooks import (DistEvalHook, DistEvalmAPHook, CocoDistEvalRecallHook,\n                         CocoDistEvalmAPHook)\nfrom .mean_ap import average_precision, eval_map, print_map_summary\nfrom .recall import (eval_recalls, print_recall_summary, plot_num_recall,\n                     plot_iou_recall)\n\n__all__ = [\n    'voc_classes', 'imagenet_det_classes', 'imagenet_vid_classes',\n    'coco_classes', 'dataset_aliases', 'get_classes', 'coco_eval',\n    'fast_eval_recall', 'results2json', 'DistEvalHook', 'DistEvalmAPHook',\n    'CocoDistEvalRecallHook', 'CocoDistEvalmAPHook', 'average_precision',\n    'eval_map', 'print_map_summary', 'eval_recalls', 'print_recall_summary',\n    'plot_num_recall', 'plot_iou_recall'\n]\n"""
detection/mmdet/core/evaluation/bbox_overlaps.py,0,"b'import numpy as np\n\n\ndef bbox_overlaps(bboxes1, bboxes2, mode=\'iou\'):\n    """"""Calculate the ious between each bbox of bboxes1 and bboxes2.\n\n    Args:\n        bboxes1(ndarray): shape (n, 4)\n        bboxes2(ndarray): shape (k, 4)\n        mode(str): iou (intersection over union) or iof (intersection\n            over foreground)\n\n    Returns:\n        ious(ndarray): shape (n, k)\n    """"""\n\n    assert mode in [\'iou\', \'iof\']\n\n    bboxes1 = bboxes1.astype(np.float32)\n    bboxes2 = bboxes2.astype(np.float32)\n    rows = bboxes1.shape[0]\n    cols = bboxes2.shape[0]\n    ious = np.zeros((rows, cols), dtype=np.float32)\n    if rows * cols == 0:\n        return ious\n    exchange = False\n    if bboxes1.shape[0] > bboxes2.shape[0]:\n        bboxes1, bboxes2 = bboxes2, bboxes1\n        ious = np.zeros((cols, rows), dtype=np.float32)\n        exchange = True\n    area1 = (bboxes1[:, 2] - bboxes1[:, 0] + 1) * (\n        bboxes1[:, 3] - bboxes1[:, 1] + 1)\n    area2 = (bboxes2[:, 2] - bboxes2[:, 0] + 1) * (\n        bboxes2[:, 3] - bboxes2[:, 1] + 1)\n    for i in range(bboxes1.shape[0]):\n        x_start = np.maximum(bboxes1[i, 0], bboxes2[:, 0])\n        y_start = np.maximum(bboxes1[i, 1], bboxes2[:, 1])\n        x_end = np.minimum(bboxes1[i, 2], bboxes2[:, 2])\n        y_end = np.minimum(bboxes1[i, 3], bboxes2[:, 3])\n        overlap = np.maximum(x_end - x_start + 1, 0) * np.maximum(\n            y_end - y_start + 1, 0)\n        if mode == \'iou\':\n            union = area1[i] + area2 - overlap\n        else:\n            union = area1[i] if not exchange else area2\n        ious[i, :] = overlap / union\n    if exchange:\n        ious = ious.T\n    return ious\n'"
detection/mmdet/core/evaluation/class_names.py,0,"b'import mmcv\n\n\ndef voc_classes():\n    return [\n        \'aeroplane\', \'bicycle\', \'bird\', \'boat\', \'bottle\', \'bus\', \'car\', \'cat\',\n        \'chair\', \'cow\', \'diningtable\', \'dog\', \'horse\', \'motorbike\', \'person\',\n        \'pottedplant\', \'sheep\', \'sofa\', \'train\', \'tvmonitor\'\n    ]\n\n\ndef imagenet_det_classes():\n    return [\n        \'accordion\', \'airplane\', \'ant\', \'antelope\', \'apple\', \'armadillo\',\n        \'artichoke\', \'axe\', \'baby_bed\', \'backpack\', \'bagel\', \'balance_beam\',\n        \'banana\', \'band_aid\', \'banjo\', \'baseball\', \'basketball\', \'bathing_cap\',\n        \'beaker\', \'bear\', \'bee\', \'bell_pepper\', \'bench\', \'bicycle\', \'binder\',\n        \'bird\', \'bookshelf\', \'bow_tie\', \'bow\', \'bowl\', \'brassiere\', \'burrito\',\n        \'bus\', \'butterfly\', \'camel\', \'can_opener\', \'car\', \'cart\', \'cattle\',\n        \'cello\', \'centipede\', \'chain_saw\', \'chair\', \'chime\', \'cocktail_shaker\',\n        \'coffee_maker\', \'computer_keyboard\', \'computer_mouse\', \'corkscrew\',\n        \'cream\', \'croquet_ball\', \'crutch\', \'cucumber\', \'cup_or_mug\', \'diaper\',\n        \'digital_clock\', \'dishwasher\', \'dog\', \'domestic_cat\', \'dragonfly\',\n        \'drum\', \'dumbbell\', \'electric_fan\', \'elephant\', \'face_powder\', \'fig\',\n        \'filing_cabinet\', \'flower_pot\', \'flute\', \'fox\', \'french_horn\', \'frog\',\n        \'frying_pan\', \'giant_panda\', \'goldfish\', \'golf_ball\', \'golfcart\',\n        \'guacamole\', \'guitar\', \'hair_dryer\', \'hair_spray\', \'hamburger\',\n        \'hammer\', \'hamster\', \'harmonica\', \'harp\', \'hat_with_a_wide_brim\',\n        \'head_cabbage\', \'helmet\', \'hippopotamus\', \'horizontal_bar\', \'horse\',\n        \'hotdog\', \'iPod\', \'isopod\', \'jellyfish\', \'koala_bear\', \'ladle\',\n        \'ladybug\', \'lamp\', \'laptop\', \'lemon\', \'lion\', \'lipstick\', \'lizard\',\n        \'lobster\', \'maillot\', \'maraca\', \'microphone\', \'microwave\', \'milk_can\',\n        \'miniskirt\', \'monkey\', \'motorcycle\', \'mushroom\', \'nail\', \'neck_brace\',\n        \'oboe\', \'orange\', \'otter\', \'pencil_box\', \'pencil_sharpener\', \'perfume\',\n        \'person\', \'piano\', \'pineapple\', \'ping-pong_ball\', \'pitcher\', \'pizza\',\n        \'plastic_bag\', \'plate_rack\', \'pomegranate\', \'popsicle\', \'porcupine\',\n        \'power_drill\', \'pretzel\', \'printer\', \'puck\', \'punching_bag\', \'purse\',\n        \'rabbit\', \'racket\', \'ray\', \'red_panda\', \'refrigerator\',\n        \'remote_control\', \'rubber_eraser\', \'rugby_ball\', \'ruler\',\n        \'salt_or_pepper_shaker\', \'saxophone\', \'scorpion\', \'screwdriver\',\n        \'seal\', \'sheep\', \'ski\', \'skunk\', \'snail\', \'snake\', \'snowmobile\',\n        \'snowplow\', \'soap_dispenser\', \'soccer_ball\', \'sofa\', \'spatula\',\n        \'squirrel\', \'starfish\', \'stethoscope\', \'stove\', \'strainer\',\n        \'strawberry\', \'stretcher\', \'sunglasses\', \'swimming_trunks\', \'swine\',\n        \'syringe\', \'table\', \'tape_player\', \'tennis_ball\', \'tick\', \'tie\',\n        \'tiger\', \'toaster\', \'traffic_light\', \'train\', \'trombone\', \'trumpet\',\n        \'turtle\', \'tv_or_monitor\', \'unicycle\', \'vacuum\', \'violin\',\n        \'volleyball\', \'waffle_iron\', \'washer\', \'water_bottle\', \'watercraft\',\n        \'whale\', \'wine_bottle\', \'zebra\'\n    ]\n\n\ndef imagenet_vid_classes():\n    return [\n        \'airplane\', \'antelope\', \'bear\', \'bicycle\', \'bird\', \'bus\', \'car\',\n        \'cattle\', \'dog\', \'domestic_cat\', \'elephant\', \'fox\', \'giant_panda\',\n        \'hamster\', \'horse\', \'lion\', \'lizard\', \'monkey\', \'motorcycle\', \'rabbit\',\n        \'red_panda\', \'sheep\', \'snake\', \'squirrel\', \'tiger\', \'train\', \'turtle\',\n        \'watercraft\', \'whale\', \'zebra\'\n    ]\n\n\ndef coco_classes():\n    return [\n        \'person\', \'bicycle\', \'car\', \'motorcycle\', \'airplane\', \'bus\', \'train\',\n        \'truck\', \'boat\', \'traffic_light\', \'fire_hydrant\', \'stop_sign\',\n        \'parking_meter\', \'bench\', \'bird\', \'cat\', \'dog\', \'horse\', \'sheep\',\n        \'cow\', \'elephant\', \'bear\', \'zebra\', \'giraffe\', \'backpack\', \'umbrella\',\n        \'handbag\', \'tie\', \'suitcase\', \'frisbee\', \'skis\', \'snowboard\',\n        \'sports_ball\', \'kite\', \'baseball_bat\', \'baseball_glove\', \'skateboard\',\n        \'surfboard\', \'tennis_racket\', \'bottle\', \'wine_glass\', \'cup\', \'fork\',\n        \'knife\', \'spoon\', \'bowl\', \'banana\', \'apple\', \'sandwich\', \'orange\',\n        \'broccoli\', \'carrot\', \'hot_dog\', \'pizza\', \'donut\', \'cake\', \'chair\',\n        \'couch\', \'potted_plant\', \'bed\', \'dining_table\', \'toilet\', \'tv\',\n        \'laptop\', \'mouse\', \'remote\', \'keyboard\', \'cell_phone\', \'microwave\',\n        \'oven\', \'toaster\', \'sink\', \'refrigerator\', \'book\', \'clock\', \'vase\',\n        \'scissors\', \'teddy_bear\', \'hair_drier\', \'toothbrush\'\n    ]\n\n\ndataset_aliases = {\n    \'voc\': [\'voc\', \'pascal_voc\', \'voc07\', \'voc12\'],\n    \'imagenet_det\': [\'det\', \'imagenet_det\', \'ilsvrc_det\'],\n    \'imagenet_vid\': [\'vid\', \'imagenet_vid\', \'ilsvrc_vid\'],\n    \'coco\': [\'coco\', \'mscoco\', \'ms_coco\']\n}\n\n\ndef get_classes(dataset):\n    """"""Get class names of a dataset.""""""\n    alias2name = {}\n    for name, aliases in dataset_aliases.items():\n        for alias in aliases:\n            alias2name[alias] = name\n\n    if mmcv.is_str(dataset):\n        if dataset in alias2name:\n            labels = eval(alias2name[dataset] + \'_classes()\')\n        else:\n            raise ValueError(\'Unrecognized dataset: {}\'.format(dataset))\n    else:\n        raise TypeError(\'dataset must a str, but got {}\'.format(type(dataset)))\n    return labels\n'"
detection/mmdet/core/evaluation/coco_utils.py,0,"b""import mmcv\nimport numpy as np\nfrom pycocotools.coco import COCO\nfrom pycocotools.cocoeval import COCOeval\n\nfrom .recall import eval_recalls\n\n\ndef coco_eval(result_file, result_types, coco, max_dets=(100, 300, 1000)):\n    for res_type in result_types:\n        assert res_type in [\n            'proposal', 'proposal_fast', 'bbox', 'segm', 'keypoints'\n        ]\n\n    if mmcv.is_str(coco):\n        coco = COCO(coco)\n    assert isinstance(coco, COCO)\n\n    if result_types == ['proposal_fast']:\n        ar = fast_eval_recall(result_file, coco, np.array(max_dets))\n        for i, num in enumerate(max_dets):\n            print('AR@{}\\t= {:.4f}'.format(num, ar[i]))\n        return\n\n    assert result_file.endswith('.json')\n    coco_dets = coco.loadRes(result_file)\n\n    img_ids = coco.getImgIds()\n    for res_type in result_types:\n        iou_type = 'bbox' if res_type == 'proposal' else res_type\n        cocoEval = COCOeval(coco, coco_dets, iou_type)\n        cocoEval.params.imgIds = img_ids\n        if res_type == 'proposal':\n            cocoEval.params.useCats = 0\n            cocoEval.params.maxDets = list(max_dets)\n        cocoEval.evaluate()\n        cocoEval.accumulate()\n        cocoEval.summarize()\n\n\ndef fast_eval_recall(results,\n                     coco,\n                     max_dets,\n                     iou_thrs=np.arange(0.5, 0.96, 0.05)):\n    if mmcv.is_str(results):\n        assert results.endswith('.pkl')\n        results = mmcv.load(results)\n    elif not isinstance(results, list):\n        raise TypeError(\n            'results must be a list of numpy arrays or a filename, not {}'.\n            format(type(results)))\n\n    gt_bboxes = []\n    img_ids = coco.getImgIds()\n    for i in range(len(img_ids)):\n        ann_ids = coco.getAnnIds(imgIds=img_ids[i])\n        ann_info = coco.loadAnns(ann_ids)\n        if len(ann_info) == 0:\n            gt_bboxes.append(np.zeros((0, 4)))\n            continue\n        bboxes = []\n        for ann in ann_info:\n            if ann.get('ignore', False) or ann['iscrowd']:\n                continue\n            x1, y1, w, h = ann['bbox']\n            bboxes.append([x1, y1, x1 + w - 1, y1 + h - 1])\n        bboxes = np.array(bboxes, dtype=np.float32)\n        if bboxes.shape[0] == 0:\n            bboxes = np.zeros((0, 4))\n        gt_bboxes.append(bboxes)\n\n    recalls = eval_recalls(\n        gt_bboxes, results, max_dets, iou_thrs, print_summary=False)\n    ar = recalls.mean(axis=1)\n    return ar\n\n\ndef xyxy2xywh(bbox):\n    _bbox = bbox.tolist()\n    return [\n        _bbox[0],\n        _bbox[1],\n        _bbox[2] - _bbox[0] + 1,\n        _bbox[3] - _bbox[1] + 1,\n    ]\n\n\ndef proposal2json(dataset, results):\n    json_results = []\n    for idx in range(len(dataset)):\n        img_id = dataset.img_ids[idx]\n        bboxes = results[idx]\n        for i in range(bboxes.shape[0]):\n            data = dict()\n            data['image_id'] = img_id\n            data['bbox'] = xyxy2xywh(bboxes[i])\n            data['score'] = float(bboxes[i][4])\n            data['category_id'] = 1\n            json_results.append(data)\n    return json_results\n\n\ndef det2json(dataset, results):\n    json_results = []\n    for idx in range(len(dataset)):\n        img_id = dataset.img_ids[idx]\n        result = results[idx]\n        for label in range(len(result)):\n            bboxes = result[label]\n            for i in range(bboxes.shape[0]):\n                data = dict()\n                data['image_id'] = img_id\n                data['bbox'] = xyxy2xywh(bboxes[i])\n                data['score'] = float(bboxes[i][4])\n                data['category_id'] = dataset.cat_ids[label]\n                json_results.append(data)\n    return json_results\n\n\ndef segm2json(dataset, results):\n    json_results = []\n    for idx in range(len(dataset)):\n        img_id = dataset.img_ids[idx]\n        det, seg = results[idx]\n        for label in range(len(det)):\n            bboxes = det[label]\n            segms = seg[label]\n            for i in range(bboxes.shape[0]):\n                data = dict()\n                data['image_id'] = img_id\n                data['bbox'] = xyxy2xywh(bboxes[i])\n                data['score'] = float(bboxes[i][4])\n                data['category_id'] = dataset.cat_ids[label]\n                segms[i]['counts'] = segms[i]['counts'].decode()\n                data['segmentation'] = segms[i]\n                json_results.append(data)\n    return json_results\n\n\ndef results2json(dataset, results, out_file):\n    if isinstance(results[0], list):\n        json_results = det2json(dataset, results)\n    elif isinstance(results[0], tuple):\n        json_results = segm2json(dataset, results)\n    elif isinstance(results[0], np.ndarray):\n        json_results = proposal2json(dataset, results)\n    else:\n        raise TypeError('invalid type of results')\n    mmcv.dump(json_results, out_file)\n"""
detection/mmdet/core/evaluation/eval_hooks.py,4,"b'import os\nimport os.path as osp\nimport shutil\nimport time\n\nimport mmcv\nimport numpy as np\nimport torch\nfrom mmcv.runner import Hook, obj_from_dict\nfrom mmcv.parallel import scatter, collate\nfrom pycocotools.cocoeval import COCOeval\nfrom torch.utils.data import Dataset\n\nfrom .coco_utils import results2json, fast_eval_recall\nfrom .mean_ap import eval_map\nfrom mmdet import datasets\n\n\nclass DistEvalHook(Hook):\n\n    def __init__(self, dataset, interval=1):\n        if isinstance(dataset, Dataset):\n            self.dataset = dataset\n        elif isinstance(dataset, dict):\n            self.dataset = obj_from_dict(dataset, datasets,\n                                         {\'test_mode\': True})\n        else:\n            raise TypeError(\n                \'dataset must be a Dataset object or a dict, not {}\'.format(\n                    type(dataset)))\n        self.interval = interval\n        self.lock_dir = None\n\n    def _barrier(self, rank, world_size):\n        """"""Due to some issues with `torch.distributed.barrier()`, we have to\n        implement this ugly barrier function.\n        """"""\n        if rank == 0:\n            for i in range(1, world_size):\n                tmp = osp.join(self.lock_dir, \'{}.pkl\'.format(i))\n                while not (osp.exists(tmp)):\n                    time.sleep(1)\n            for i in range(1, world_size):\n                tmp = osp.join(self.lock_dir, \'{}.pkl\'.format(i))\n                os.remove(tmp)\n        else:\n            tmp = osp.join(self.lock_dir, \'{}.pkl\'.format(rank))\n            mmcv.dump([], tmp)\n            while osp.exists(tmp):\n                time.sleep(1)\n\n    def before_run(self, runner):\n        self.lock_dir = osp.join(runner.work_dir, \'.lock_map_hook\')\n        if runner.rank == 0:\n            if osp.exists(self.lock_dir):\n                shutil.rmtree(self.lock_dir)\n            mmcv.mkdir_or_exist(self.lock_dir)\n\n    def after_run(self, runner):\n        if runner.rank == 0:\n            shutil.rmtree(self.lock_dir)\n\n    def after_train_epoch(self, runner):\n        if not self.every_n_epochs(runner, self.interval):\n            return\n        runner.model.eval()\n        results = [None for _ in range(len(self.dataset))]\n        prog_bar = mmcv.ProgressBar(len(self.dataset))\n        for idx in range(runner.rank, len(self.dataset), runner.world_size):\n            data = self.dataset[idx]\n            data_gpu = scatter(\n                collate([data], samples_per_gpu=1),\n                [torch.cuda.current_device()])[0]\n\n            # compute output\n            with torch.no_grad():\n                result = runner.model(\n                    return_loss=False, rescale=True, **data_gpu)\n            results[idx] = result\n\n            batch_size = runner.world_size\n            for _ in range(batch_size):\n                prog_bar.update()\n\n        if runner.rank == 0:\n            print(\'\\n\')\n            self._barrier(runner.rank, runner.world_size)\n            for i in range(1, runner.world_size):\n                tmp_file = osp.join(runner.work_dir, \'temp_{}.pkl\'.format(i))\n                tmp_results = mmcv.load(tmp_file)\n                for idx in range(i, len(results), runner.world_size):\n                    results[idx] = tmp_results[idx]\n                os.remove(tmp_file)\n            self.evaluate(runner, results)\n        else:\n            tmp_file = osp.join(runner.work_dir,\n                                \'temp_{}.pkl\'.format(runner.rank))\n            mmcv.dump(results, tmp_file)\n            self._barrier(runner.rank, runner.world_size)\n        self._barrier(runner.rank, runner.world_size)\n\n    def evaluate(self):\n        raise NotImplementedError\n\n\nclass DistEvalmAPHook(DistEvalHook):\n\n    def evaluate(self, runner, results):\n        gt_bboxes = []\n        gt_labels = []\n        gt_ignore = [] if self.dataset.with_crowd else None\n        for i in range(len(self.dataset)):\n            ann = self.dataset.get_ann_info(i)\n            bboxes = ann[\'bboxes\']\n            labels = ann[\'labels\']\n            if gt_ignore is not None:\n                ignore = np.concatenate([\n                    np.zeros(bboxes.shape[0], dtype=np.bool),\n                    np.ones(ann[\'bboxes_ignore\'].shape[0], dtype=np.bool)\n                ])\n                gt_ignore.append(ignore)\n                bboxes = np.vstack([bboxes, ann[\'bboxes_ignore\']])\n                labels = np.concatenate([labels, ann[\'labels_ignore\']])\n            gt_bboxes.append(bboxes)\n            gt_labels.append(labels)\n        # If the dataset is VOC2007, then use 11 points mAP evaluation.\n        if hasattr(self.dataset, \'year\') and self.dataset.year == 2007:\n            ds_name = \'voc07\'\n        else:\n            ds_name = self.dataset.CLASSES\n        mean_ap, eval_results = eval_map(\n            results,\n            gt_bboxes,\n            gt_labels,\n            gt_ignore=gt_ignore,\n            scale_ranges=None,\n            iou_thr=0.5,\n            dataset=ds_name,\n            print_summary=True)\n        runner.log_buffer.output[\'mAP\'] = mean_ap\n        runner.log_buffer.ready = True\n\n\nclass CocoDistEvalRecallHook(DistEvalHook):\n\n    def __init__(self,\n                 dataset,\n                 proposal_nums=(100, 300, 1000),\n                 iou_thrs=np.arange(0.5, 0.96, 0.05)):\n        super(CocoDistEvalRecallHook, self).__init__(dataset)\n        self.proposal_nums = np.array(proposal_nums, dtype=np.int32)\n        self.iou_thrs = np.array(iou_thrs, dtype=np.float32)\n\n    def evaluate(self, runner, results):\n        # the official coco evaluation is too slow, here we use our own\n        # implementation instead, which may get slightly different results\n        ar = fast_eval_recall(results, self.dataset.coco, self.proposal_nums,\n                              self.iou_thrs)\n        for i, num in enumerate(self.proposal_nums):\n            runner.log_buffer.output[\'AR@{}\'.format(num)] = ar[i]\n        runner.log_buffer.ready = True\n\n\nclass CocoDistEvalmAPHook(DistEvalHook):\n\n    def evaluate(self, runner, results):\n        tmp_file = osp.join(runner.work_dir, \'temp_0.json\')\n        results2json(self.dataset, results, tmp_file)\n\n        res_types = [\'bbox\',\n                     \'segm\'] if runner.model.module.with_mask else [\'bbox\']\n        cocoGt = self.dataset.coco\n        cocoDt = cocoGt.loadRes(tmp_file)\n        imgIds = cocoGt.getImgIds()\n        for res_type in res_types:\n            iou_type = res_type\n            cocoEval = COCOeval(cocoGt, cocoDt, iou_type)\n            cocoEval.params.imgIds = imgIds\n            cocoEval.evaluate()\n            cocoEval.accumulate()\n            cocoEval.summarize()\n            field = \'{}_mAP\'.format(res_type)\n            runner.log_buffer.output[field] = cocoEval.stats[0]\n        runner.log_buffer.ready = True\n        os.remove(tmp_file)\n'"
detection/mmdet/core/evaluation/mean_ap.py,0,"b'import mmcv\nimport numpy as np\nfrom terminaltables import AsciiTable\n\nfrom .bbox_overlaps import bbox_overlaps\nfrom .class_names import get_classes\n\n\ndef average_precision(recalls, precisions, mode=\'area\'):\n    """"""Calculate average precision (for single or multiple scales).\n\n    Args:\n        recalls (ndarray): shape (num_scales, num_dets) or (num_dets, )\n        precisions (ndarray): shape (num_scales, num_dets) or (num_dets, )\n        mode (str): \'area\' or \'11points\', \'area\' means calculating the area\n            under precision-recall curve, \'11points\' means calculating\n            the average precision of recalls at [0, 0.1, ..., 1]\n\n    Returns:\n        float or ndarray: calculated average precision\n    """"""\n    no_scale = False\n    if recalls.ndim == 1:\n        no_scale = True\n        recalls = recalls[np.newaxis, :]\n        precisions = precisions[np.newaxis, :]\n    assert recalls.shape == precisions.shape and recalls.ndim == 2\n    num_scales = recalls.shape[0]\n    ap = np.zeros(num_scales, dtype=np.float32)\n    if mode == \'area\':\n        zeros = np.zeros((num_scales, 1), dtype=recalls.dtype)\n        ones = np.ones((num_scales, 1), dtype=recalls.dtype)\n        mrec = np.hstack((zeros, recalls, ones))\n        mpre = np.hstack((zeros, precisions, zeros))\n        for i in range(mpre.shape[1] - 1, 0, -1):\n            mpre[:, i - 1] = np.maximum(mpre[:, i - 1], mpre[:, i])\n        for i in range(num_scales):\n            ind = np.where(mrec[i, 1:] != mrec[i, :-1])[0]\n            ap[i] = np.sum(\n                (mrec[i, ind + 1] - mrec[i, ind]) * mpre[i, ind + 1])\n    elif mode == \'11points\':\n        for i in range(num_scales):\n            for thr in np.arange(0, 1 + 1e-3, 0.1):\n                precs = precisions[i, recalls[i, :] >= thr]\n                prec = precs.max() if precs.size > 0 else 0\n                ap[i] += prec\n            ap /= 11\n    else:\n        raise ValueError(\n            \'Unrecognized mode, only ""area"" and ""11points"" are supported\')\n    if no_scale:\n        ap = ap[0]\n    return ap\n\n\ndef tpfp_imagenet(det_bboxes,\n                  gt_bboxes,\n                  gt_ignore,\n                  default_iou_thr,\n                  area_ranges=None):\n    """"""Check if detected bboxes are true positive or false positive.\n\n    Args:\n        det_bbox (ndarray): the detected bbox\n        gt_bboxes (ndarray): ground truth bboxes of this image\n        gt_ignore (ndarray): indicate if gts are ignored for evaluation or not\n        default_iou_thr (float): the iou thresholds for medium and large bboxes\n        area_ranges (list or None): gt bbox area ranges\n\n    Returns:\n        tuple: two arrays (tp, fp) whose elements are 0 and 1\n    """"""\n    num_dets = det_bboxes.shape[0]\n    num_gts = gt_bboxes.shape[0]\n    if area_ranges is None:\n        area_ranges = [(None, None)]\n    num_scales = len(area_ranges)\n    # tp and fp are of shape (num_scales, num_gts), each row is tp or fp\n    # of a certain scale.\n    tp = np.zeros((num_scales, num_dets), dtype=np.float32)\n    fp = np.zeros((num_scales, num_dets), dtype=np.float32)\n    if gt_bboxes.shape[0] == 0:\n        if area_ranges == [(None, None)]:\n            fp[...] = 1\n        else:\n            det_areas = (det_bboxes[:, 2] - det_bboxes[:, 0] + 1) * (\n                det_bboxes[:, 3] - det_bboxes[:, 1] + 1)\n            for i, (min_area, max_area) in enumerate(area_ranges):\n                fp[i, (det_areas >= min_area) & (det_areas < max_area)] = 1\n        return tp, fp\n    ious = bbox_overlaps(det_bboxes, gt_bboxes - 1)\n    gt_w = gt_bboxes[:, 2] - gt_bboxes[:, 0] + 1\n    gt_h = gt_bboxes[:, 3] - gt_bboxes[:, 1] + 1\n    iou_thrs = np.minimum((gt_w * gt_h) / ((gt_w + 10.0) * (gt_h + 10.0)),\n                          default_iou_thr)\n    # sort all detections by scores in descending order\n    sort_inds = np.argsort(-det_bboxes[:, -1])\n    for k, (min_area, max_area) in enumerate(area_ranges):\n        gt_covered = np.zeros(num_gts, dtype=bool)\n        # if no area range is specified, gt_area_ignore is all False\n        if min_area is None:\n            gt_area_ignore = np.zeros_like(gt_ignore, dtype=bool)\n        else:\n            gt_areas = gt_w * gt_h\n            gt_area_ignore = (gt_areas < min_area) | (gt_areas >= max_area)\n        for i in sort_inds:\n            max_iou = -1\n            matched_gt = -1\n            # find best overlapped available gt\n            for j in range(num_gts):\n                # different from PASCAL VOC: allow finding other gts if the\n                # best overlaped ones are already matched by other det bboxes\n                if gt_covered[j]:\n                    continue\n                elif ious[i, j] >= iou_thrs[j] and ious[i, j] > max_iou:\n                    max_iou = ious[i, j]\n                    matched_gt = j\n            # there are 4 cases for a det bbox:\n            # 1. it matches a gt, tp = 1, fp = 0\n            # 2. it matches an ignored gt, tp = 0, fp = 0\n            # 3. it matches no gt and within area range, tp = 0, fp = 1\n            # 4. it matches no gt but is beyond area range, tp = 0, fp = 0\n            if matched_gt >= 0:\n                gt_covered[matched_gt] = 1\n                if not (gt_ignore[matched_gt] or gt_area_ignore[matched_gt]):\n                    tp[k, i] = 1\n            elif min_area is None:\n                fp[k, i] = 1\n            else:\n                bbox = det_bboxes[i, :4]\n                area = (bbox[2] - bbox[0] + 1) * (bbox[3] - bbox[1] + 1)\n                if area >= min_area and area < max_area:\n                    fp[k, i] = 1\n    return tp, fp\n\n\ndef tpfp_default(det_bboxes, gt_bboxes, gt_ignore, iou_thr, area_ranges=None):\n    """"""Check if detected bboxes are true positive or false positive.\n\n    Args:\n        det_bbox (ndarray): the detected bbox\n        gt_bboxes (ndarray): ground truth bboxes of this image\n        gt_ignore (ndarray): indicate if gts are ignored for evaluation or not\n        iou_thr (float): the iou thresholds\n\n    Returns:\n        tuple: (tp, fp), two arrays whose elements are 0 and 1\n    """"""\n    num_dets = det_bboxes.shape[0]\n    num_gts = gt_bboxes.shape[0]\n    if area_ranges is None:\n        area_ranges = [(None, None)]\n    num_scales = len(area_ranges)\n    # tp and fp are of shape (num_scales, num_gts), each row is tp or fp of\n    # a certain scale\n    tp = np.zeros((num_scales, num_dets), dtype=np.float32)\n    fp = np.zeros((num_scales, num_dets), dtype=np.float32)\n    # if there is no gt bboxes in this image, then all det bboxes\n    # within area range are false positives\n    if gt_bboxes.shape[0] == 0:\n        if area_ranges == [(None, None)]:\n            fp[...] = 1\n        else:\n            det_areas = (det_bboxes[:, 2] - det_bboxes[:, 0] + 1) * (\n                det_bboxes[:, 3] - det_bboxes[:, 1] + 1)\n            for i, (min_area, max_area) in enumerate(area_ranges):\n                fp[i, (det_areas >= min_area) & (det_areas < max_area)] = 1\n        return tp, fp\n    ious = bbox_overlaps(det_bboxes, gt_bboxes)\n    ious_max = ious.max(axis=1)\n    ious_argmax = ious.argmax(axis=1)\n    sort_inds = np.argsort(-det_bboxes[:, -1])\n    for k, (min_area, max_area) in enumerate(area_ranges):\n        gt_covered = np.zeros(num_gts, dtype=bool)\n        # if no area range is specified, gt_area_ignore is all False\n        if min_area is None:\n            gt_area_ignore = np.zeros_like(gt_ignore, dtype=bool)\n        else:\n            gt_areas = (gt_bboxes[:, 2] - gt_bboxes[:, 0] + 1) * (\n                gt_bboxes[:, 3] - gt_bboxes[:, 1] + 1)\n            gt_area_ignore = (gt_areas < min_area) | (gt_areas >= max_area)\n        for i in sort_inds:\n            if ious_max[i] >= iou_thr:\n                matched_gt = ious_argmax[i]\n                if not (gt_ignore[matched_gt] or gt_area_ignore[matched_gt]):\n                    if not gt_covered[matched_gt]:\n                        gt_covered[matched_gt] = True\n                        tp[k, i] = 1\n                    else:\n                        fp[k, i] = 1\n                # otherwise ignore this detected bbox, tp = 0, fp = 0\n            elif min_area is None:\n                fp[k, i] = 1\n            else:\n                bbox = det_bboxes[i, :4]\n                area = (bbox[2] - bbox[0] + 1) * (bbox[3] - bbox[1] + 1)\n                if area >= min_area and area < max_area:\n                    fp[k, i] = 1\n    return tp, fp\n\n\ndef get_cls_results(det_results, gt_bboxes, gt_labels, gt_ignore, class_id):\n    """"""Get det results and gt information of a certain class.""""""\n    cls_dets = [det[class_id]\n                for det in det_results]  # det bboxes of this class\n    cls_gts = []  # gt bboxes of this class\n    cls_gt_ignore = []\n    for j in range(len(gt_bboxes)):\n        gt_bbox = gt_bboxes[j]\n        cls_inds = (gt_labels[j] == class_id + 1)\n        cls_gt = gt_bbox[cls_inds, :] if gt_bbox.shape[0] > 0 else gt_bbox\n        cls_gts.append(cls_gt)\n        if gt_ignore is None:\n            cls_gt_ignore.append(np.zeros(cls_gt.shape[0], dtype=np.int32))\n        else:\n            cls_gt_ignore.append(gt_ignore[j][cls_inds])\n    return cls_dets, cls_gts, cls_gt_ignore\n\n\ndef eval_map(det_results,\n             gt_bboxes,\n             gt_labels,\n             gt_ignore=None,\n             scale_ranges=None,\n             iou_thr=0.5,\n             dataset=None,\n             print_summary=True):\n    """"""Evaluate mAP of a dataset.\n\n    Args:\n        det_results (list): a list of list, [[cls1_det, cls2_det, ...], ...]\n        gt_bboxes (list): ground truth bboxes of each image, a list of K*4\n            array.\n        gt_labels (list): ground truth labels of each image, a list of K array\n        gt_ignore (list): gt ignore indicators of each image, a list of K array\n        scale_ranges (list, optional): [(min1, max1), (min2, max2), ...]\n        iou_thr (float): IoU threshold\n        dataset (None or str or list): dataset name or dataset classes, there\n            are minor differences in metrics for different datsets, e.g.\n            ""voc07"", ""imagenet_det"", etc.\n        print_summary (bool): whether to print the mAP summary\n\n    Returns:\n        tuple: (mAP, [dict, dict, ...])\n    """"""\n    assert len(det_results) == len(gt_bboxes) == len(gt_labels)\n    if gt_ignore is not None:\n        assert len(gt_ignore) == len(gt_labels)\n        for i in range(len(gt_ignore)):\n            assert len(gt_labels[i]) == len(gt_ignore[i])\n    area_ranges = ([(rg[0]**2, rg[1]**2) for rg in scale_ranges]\n                   if scale_ranges is not None else None)\n    num_scales = len(scale_ranges) if scale_ranges is not None else 1\n    eval_results = []\n    num_classes = len(det_results[0])  # positive class num\n    gt_labels = [\n        label if label.ndim == 1 else label[:, 0] for label in gt_labels\n    ]\n    for i in range(num_classes):\n        # get gt and det bboxes of this class\n        cls_dets, cls_gts, cls_gt_ignore = get_cls_results(\n            det_results, gt_bboxes, gt_labels, gt_ignore, i)\n        # calculate tp and fp for each image\n        tpfp_func = (tpfp_imagenet\n                     if dataset in [\'det\', \'vid\'] else tpfp_default)\n        tpfp = [\n            tpfp_func(cls_dets[j], cls_gts[j], cls_gt_ignore[j], iou_thr,\n                      area_ranges) for j in range(len(cls_dets))\n        ]\n        tp, fp = tuple(zip(*tpfp))\n        # calculate gt number of each scale, gts ignored or beyond scale\n        # are not counted\n        num_gts = np.zeros(num_scales, dtype=int)\n        for j, bbox in enumerate(cls_gts):\n            if area_ranges is None:\n                num_gts[0] += np.sum(np.logical_not(cls_gt_ignore[j]))\n            else:\n                gt_areas = (bbox[:, 2] - bbox[:, 0] + 1) * (\n                    bbox[:, 3] - bbox[:, 1] + 1)\n                for k, (min_area, max_area) in enumerate(area_ranges):\n                    num_gts[k] += np.sum(\n                        np.logical_not(cls_gt_ignore[j]) &\n                        (gt_areas >= min_area) & (gt_areas < max_area))\n        # sort all det bboxes by score, also sort tp and fp\n        cls_dets = np.vstack(cls_dets)\n        num_dets = cls_dets.shape[0]\n        sort_inds = np.argsort(-cls_dets[:, -1])\n        tp = np.hstack(tp)[:, sort_inds]\n        fp = np.hstack(fp)[:, sort_inds]\n        # calculate recall and precision with tp and fp\n        tp = np.cumsum(tp, axis=1)\n        fp = np.cumsum(fp, axis=1)\n        eps = np.finfo(np.float32).eps\n        recalls = tp / np.maximum(num_gts[:, np.newaxis], eps)\n        precisions = tp / np.maximum((tp + fp), eps)\n        # calculate AP\n        if scale_ranges is None:\n            recalls = recalls[0, :]\n            precisions = precisions[0, :]\n            num_gts = num_gts.item()\n        mode = \'area\' if dataset != \'voc07\' else \'11points\'\n        ap = average_precision(recalls, precisions, mode)\n        eval_results.append({\n            \'num_gts\': num_gts,\n            \'num_dets\': num_dets,\n            \'recall\': recalls,\n            \'precision\': precisions,\n            \'ap\': ap\n        })\n    if scale_ranges is not None:\n        # shape (num_classes, num_scales)\n        all_ap = np.vstack([cls_result[\'ap\'] for cls_result in eval_results])\n        all_num_gts = np.vstack(\n            [cls_result[\'num_gts\'] for cls_result in eval_results])\n        mean_ap = [\n            all_ap[all_num_gts[:, i] > 0, i].mean()\n            if np.any(all_num_gts[:, i] > 0) else 0.0\n            for i in range(num_scales)\n        ]\n    else:\n        aps = []\n        for cls_result in eval_results:\n            if cls_result[\'num_gts\'] > 0:\n                aps.append(cls_result[\'ap\'])\n        mean_ap = np.array(aps).mean().item() if aps else 0.0\n    if print_summary:\n        print_map_summary(mean_ap, eval_results, dataset)\n\n    return mean_ap, eval_results\n\n\ndef print_map_summary(mean_ap, results, dataset=None):\n    """"""Print mAP and results of each class.\n\n    Args:\n        mean_ap(float): calculated from `eval_map`\n        results(list): calculated from `eval_map`\n        dataset(None or str or list): dataset name or dataset classes.\n    """"""\n    num_scales = len(results[0][\'ap\']) if isinstance(results[0][\'ap\'],\n                                                     np.ndarray) else 1\n    num_classes = len(results)\n\n    recalls = np.zeros((num_scales, num_classes), dtype=np.float32)\n    precisions = np.zeros((num_scales, num_classes), dtype=np.float32)\n    aps = np.zeros((num_scales, num_classes), dtype=np.float32)\n    num_gts = np.zeros((num_scales, num_classes), dtype=int)\n    for i, cls_result in enumerate(results):\n        if cls_result[\'recall\'].size > 0:\n            recalls[:, i] = np.array(cls_result[\'recall\'], ndmin=2)[:, -1]\n            precisions[:, i] = np.array(\n                cls_result[\'precision\'], ndmin=2)[:, -1]\n        aps[:, i] = cls_result[\'ap\']\n        num_gts[:, i] = cls_result[\'num_gts\']\n\n    if dataset is None:\n        label_names = [str(i) for i in range(1, num_classes + 1)]\n    elif mmcv.is_str(dataset):\n        label_names = get_classes(dataset)\n    else:\n        label_names = dataset\n\n    if not isinstance(mean_ap, list):\n        mean_ap = [mean_ap]\n    header = [\'class\', \'gts\', \'dets\', \'recall\', \'precision\', \'ap\']\n    for i in range(num_scales):\n        table_data = [header]\n        for j in range(num_classes):\n            row_data = [\n                label_names[j], num_gts[i, j], results[j][\'num_dets\'],\n                \'{:.3f}\'.format(recalls[i, j]), \'{:.3f}\'.format(\n                    precisions[i, j]), \'{:.3f}\'.format(aps[i, j])\n            ]\n            table_data.append(row_data)\n        table_data.append([\'mAP\', \'\', \'\', \'\', \'\', \'{:.3f}\'.format(mean_ap[i])])\n        table = AsciiTable(table_data)\n        table.inner_footing_row_border = True\n        print(table.table)\n'"
detection/mmdet/core/evaluation/recall.py,0,"b'import numpy as np\nfrom terminaltables import AsciiTable\n\nfrom .bbox_overlaps import bbox_overlaps\n\n\ndef _recalls(all_ious, proposal_nums, thrs):\n\n    img_num = all_ious.shape[0]\n    total_gt_num = sum([ious.shape[0] for ious in all_ious])\n\n    _ious = np.zeros((proposal_nums.size, total_gt_num), dtype=np.float32)\n    for k, proposal_num in enumerate(proposal_nums):\n        tmp_ious = np.zeros(0)\n        for i in range(img_num):\n            ious = all_ious[i][:, :proposal_num].copy()\n            gt_ious = np.zeros((ious.shape[0]))\n            if ious.size == 0:\n                tmp_ious = np.hstack((tmp_ious, gt_ious))\n                continue\n            for j in range(ious.shape[0]):\n                gt_max_overlaps = ious.argmax(axis=1)\n                max_ious = ious[np.arange(0, ious.shape[0]), gt_max_overlaps]\n                gt_idx = max_ious.argmax()\n                gt_ious[j] = max_ious[gt_idx]\n                box_idx = gt_max_overlaps[gt_idx]\n                ious[gt_idx, :] = -1\n                ious[:, box_idx] = -1\n            tmp_ious = np.hstack((tmp_ious, gt_ious))\n        _ious[k, :] = tmp_ious\n\n    _ious = np.fliplr(np.sort(_ious, axis=1))\n    recalls = np.zeros((proposal_nums.size, thrs.size))\n    for i, thr in enumerate(thrs):\n        recalls[:, i] = (_ious >= thr).sum(axis=1) / float(total_gt_num)\n\n    return recalls\n\n\ndef set_recall_param(proposal_nums, iou_thrs):\n    """"""Check proposal_nums and iou_thrs and set correct format.\n    """"""\n    if isinstance(proposal_nums, list):\n        _proposal_nums = np.array(proposal_nums)\n    elif isinstance(proposal_nums, int):\n        _proposal_nums = np.array([proposal_nums])\n    else:\n        _proposal_nums = proposal_nums\n\n    if iou_thrs is None:\n        _iou_thrs = np.array([0.5])\n    elif isinstance(iou_thrs, list):\n        _iou_thrs = np.array(iou_thrs)\n    elif isinstance(iou_thrs, float):\n        _iou_thrs = np.array([iou_thrs])\n    else:\n        _iou_thrs = iou_thrs\n\n    return _proposal_nums, _iou_thrs\n\n\ndef eval_recalls(gts,\n                 proposals,\n                 proposal_nums=None,\n                 iou_thrs=None,\n                 print_summary=True):\n    """"""Calculate recalls.\n\n    Args:\n        gts(list or ndarray): a list of arrays of shape (n, 4)\n        proposals(list or ndarray): a list of arrays of shape (k, 4) or (k, 5)\n        proposal_nums(int or list of int or ndarray): top N proposals\n        thrs(float or list or ndarray): iou thresholds\n\n    Returns:\n        ndarray: recalls of different ious and proposal nums\n    """"""\n\n    img_num = len(gts)\n    assert img_num == len(proposals)\n\n    proposal_nums, iou_thrs = set_recall_param(proposal_nums, iou_thrs)\n\n    all_ious = []\n    for i in range(img_num):\n        if proposals[i].ndim == 2 and proposals[i].shape[1] == 5:\n            scores = proposals[i][:, 4]\n            sort_idx = np.argsort(scores)[::-1]\n            img_proposal = proposals[i][sort_idx, :]\n        else:\n            img_proposal = proposals[i]\n        prop_num = min(img_proposal.shape[0], proposal_nums[-1])\n        if gts[i] is None or gts[i].shape[0] == 0:\n            ious = np.zeros((0, img_proposal.shape[0]), dtype=np.float32)\n        else:\n            ious = bbox_overlaps(gts[i], img_proposal[:prop_num, :4])\n        all_ious.append(ious)\n    all_ious = np.array(all_ious)\n    recalls = _recalls(all_ious, proposal_nums, iou_thrs)\n    if print_summary:\n        print_recall_summary(recalls, proposal_nums, iou_thrs)\n    return recalls\n\n\ndef print_recall_summary(recalls,\n                         proposal_nums,\n                         iou_thrs,\n                         row_idxs=None,\n                         col_idxs=None):\n    """"""Print recalls in a table.\n\n    Args:\n        recalls(ndarray): calculated from `bbox_recalls`\n        proposal_nums(ndarray or list): top N proposals\n        iou_thrs(ndarray or list): iou thresholds\n        row_idxs(ndarray): which rows(proposal nums) to print\n        col_idxs(ndarray): which cols(iou thresholds) to print\n    """"""\n    proposal_nums = np.array(proposal_nums, dtype=np.int32)\n    iou_thrs = np.array(iou_thrs)\n    if row_idxs is None:\n        row_idxs = np.arange(proposal_nums.size)\n    if col_idxs is None:\n        col_idxs = np.arange(iou_thrs.size)\n    row_header = [\'\'] + iou_thrs[col_idxs].tolist()\n    table_data = [row_header]\n    for i, num in enumerate(proposal_nums[row_idxs]):\n        row = [\n            \'{:.3f}\'.format(val)\n            for val in recalls[row_idxs[i], col_idxs].tolist()\n        ]\n        row.insert(0, num)\n        table_data.append(row)\n    table = AsciiTable(table_data)\n    print(table.table)\n\n\ndef plot_num_recall(recalls, proposal_nums):\n    """"""Plot Proposal_num-Recalls curve.\n\n    Args:\n        recalls(ndarray or list): shape (k,)\n        proposal_nums(ndarray or list): same shape as `recalls`\n    """"""\n    if isinstance(proposal_nums, np.ndarray):\n        _proposal_nums = proposal_nums.tolist()\n    else:\n        _proposal_nums = proposal_nums\n    if isinstance(recalls, np.ndarray):\n        _recalls = recalls.tolist()\n    else:\n        _recalls = recalls\n\n    import matplotlib.pyplot as plt\n    f = plt.figure()\n    plt.plot([0] + _proposal_nums, [0] + _recalls)\n    plt.xlabel(\'Proposal num\')\n    plt.ylabel(\'Recall\')\n    plt.axis([0, proposal_nums.max(), 0, 1])\n    f.show()\n\n\ndef plot_iou_recall(recalls, iou_thrs):\n    """"""Plot IoU-Recalls curve.\n\n    Args:\n        recalls(ndarray or list): shape (k,)\n        iou_thrs(ndarray or list): same shape as `recalls`\n    """"""\n    if isinstance(iou_thrs, np.ndarray):\n        _iou_thrs = iou_thrs.tolist()\n    else:\n        _iou_thrs = iou_thrs\n    if isinstance(recalls, np.ndarray):\n        _recalls = recalls.tolist()\n    else:\n        _recalls = recalls\n\n    import matplotlib.pyplot as plt\n    f = plt.figure()\n    plt.plot(_iou_thrs + [1.0], _recalls + [0.])\n    plt.xlabel(\'IoU\')\n    plt.ylabel(\'Recall\')\n    plt.axis([iou_thrs.min(), 1, 0, 1])\n    f.show()\n'"
detection/mmdet/core/loss/__init__.py,0,"b""from .losses import (weighted_nll_loss, weighted_cross_entropy,\n                     weighted_binary_cross_entropy, sigmoid_focal_loss,\n                     weighted_sigmoid_focal_loss, mask_cross_entropy,\n                     smooth_l1_loss, weighted_smoothl1, accuracy)\n\n__all__ = [\n    'weighted_nll_loss', 'weighted_cross_entropy',\n    'weighted_binary_cross_entropy', 'sigmoid_focal_loss',\n    'weighted_sigmoid_focal_loss', 'mask_cross_entropy', 'smooth_l1_loss',\n    'weighted_smoothl1', 'accuracy'\n]\n"""
detection/mmdet/core/loss/losses.py,12,"b""# TODO merge naive and weighted loss.\nimport torch\nimport torch.nn.functional as F\n\n\ndef weighted_nll_loss(pred, label, weight, avg_factor=None):\n    if avg_factor is None:\n        avg_factor = max(torch.sum(weight > 0).float().item(), 1.)\n    raw = F.nll_loss(pred, label, reduction='none')\n    return torch.sum(raw * weight)[None] / avg_factor\n\n\ndef weighted_cross_entropy(pred, label, weight, avg_factor=None, reduce=True):\n    if avg_factor is None:\n        avg_factor = max(torch.sum(weight > 0).float().item(), 1.)\n    raw = F.cross_entropy(pred, label, reduction='none')\n    if reduce:\n        return torch.sum(raw * weight)[None] / avg_factor\n    else:\n        return raw * weight / avg_factor\n\n\ndef weighted_binary_cross_entropy(pred, label, weight, avg_factor=None):\n    if avg_factor is None:\n        avg_factor = max(torch.sum(weight > 0).float().item(), 1.)\n    return F.binary_cross_entropy_with_logits(\n        pred, label.float(), weight.float(),\n        reduction='sum')[None] / avg_factor\n\n\ndef sigmoid_focal_loss(pred,\n                       target,\n                       weight,\n                       gamma=2.0,\n                       alpha=0.25,\n                       reduction='mean'):\n    pred_sigmoid = pred.sigmoid()\n    target = target.type_as(pred)\n    pt = (1 - pred_sigmoid) * target + pred_sigmoid * (1 - target)\n    weight = (alpha * target + (1 - alpha) * (1 - target)) * weight\n    weight = weight * pt.pow(gamma)\n    loss = F.binary_cross_entropy_with_logits(\n        pred, target, reduction='none') * weight\n    reduction_enum = F._Reduction.get_enum(reduction)\n    # none: 0, mean:1, sum: 2\n    if reduction_enum == 0:\n        return loss\n    elif reduction_enum == 1:\n        return loss.mean()\n    elif reduction_enum == 2:\n        return loss.sum()\n\n\ndef weighted_sigmoid_focal_loss(pred,\n                                target,\n                                weight,\n                                gamma=2.0,\n                                alpha=0.25,\n                                avg_factor=None,\n                                num_classes=80):\n    if avg_factor is None:\n        avg_factor = torch.sum(weight > 0).float().item() / num_classes + 1e-6\n    return sigmoid_focal_loss(\n        pred, target, weight, gamma=gamma, alpha=alpha,\n        reduction='sum')[None] / avg_factor\n\n\ndef mask_cross_entropy(pred, target, label):\n    num_rois = pred.size()[0]\n    inds = torch.arange(0, num_rois, dtype=torch.long, device=pred.device)\n    pred_slice = pred[inds, label].squeeze(1)\n    return F.binary_cross_entropy_with_logits(\n        pred_slice, target, reduction='mean')[None]\n\n\ndef smooth_l1_loss(pred, target, beta=1.0, reduction='mean'):\n    assert beta > 0\n    assert pred.size() == target.size() and target.numel() > 0\n    diff = torch.abs(pred - target)\n    loss = torch.where(diff < beta, 0.5 * diff * diff / beta,\n                       diff - 0.5 * beta)\n    reduction_enum = F._Reduction.get_enum(reduction)\n    # none: 0, mean:1, sum: 2\n    if reduction_enum == 0:\n        return loss\n    elif reduction_enum == 1:\n        return loss.sum() / pred.numel()\n    elif reduction_enum == 2:\n        return loss.sum()\n\n\ndef weighted_smoothl1(pred, target, weight, beta=1.0, avg_factor=None):\n    if avg_factor is None:\n        avg_factor = torch.sum(weight > 0).float().item() / 4 + 1e-6\n    loss = smooth_l1_loss(pred, target, beta, reduction='none')\n    return torch.sum(loss * weight)[None] / avg_factor\n\n\ndef accuracy(pred, target, topk=1):\n    if isinstance(topk, int):\n        topk = (topk, )\n        return_single = True\n    else:\n        return_single = False\n\n    maxk = max(topk)\n    _, pred_label = pred.topk(maxk, 1, True, True)\n    pred_label = pred_label.t()\n    correct = pred_label.eq(target.view(1, -1).expand_as(pred_label))\n\n    res = []\n    for k in topk:\n        correct_k = correct[:k].view(-1).float().sum(0, keepdim=True)\n        res.append(correct_k.mul_(100.0 / pred.size(0)))\n    return res[0] if return_single else res\n"""
detection/mmdet/core/mask/__init__.py,0,"b""from .utils import split_combined_polys\nfrom .mask_target import mask_target\n\n__all__ = ['split_combined_polys', 'mask_target']\n"""
detection/mmdet/core/mask/mask_target.py,2,"b'import torch\nimport numpy as np\nimport mmcv\n\n\ndef mask_target(pos_proposals_list, pos_assigned_gt_inds_list, gt_masks_list,\n                cfg):\n    cfg_list = [cfg for _ in range(len(pos_proposals_list))]\n    mask_targets = map(mask_target_single, pos_proposals_list,\n                       pos_assigned_gt_inds_list, gt_masks_list, cfg_list)\n    mask_targets = torch.cat(list(mask_targets))\n    return mask_targets\n\n\ndef mask_target_single(pos_proposals, pos_assigned_gt_inds, gt_masks, cfg):\n    mask_size = cfg.mask_size\n    num_pos = pos_proposals.size(0)\n    mask_targets = []\n    if num_pos > 0:\n        proposals_np = pos_proposals.cpu().numpy()\n        pos_assigned_gt_inds = pos_assigned_gt_inds.cpu().numpy()\n        for i in range(num_pos):\n            gt_mask = gt_masks[pos_assigned_gt_inds[i]]\n            bbox = proposals_np[i, :].astype(np.int32)\n            x1, y1, x2, y2 = bbox\n            w = np.maximum(x2 - x1 + 1, 1)\n            h = np.maximum(y2 - y1 + 1, 1)\n            # mask is uint8 both before and after resizing\n            target = mmcv.imresize(gt_mask[y1:y1 + h, x1:x1 + w],\n                                   (mask_size, mask_size))\n            mask_targets.append(target)\n        mask_targets = torch.from_numpy(np.stack(mask_targets)).float().to(\n            pos_proposals.device)\n    else:\n        mask_targets = pos_proposals.new_zeros((0, mask_size, mask_size))\n    return mask_targets\n'"
detection/mmdet/core/mask/utils.py,0,"b'import mmcv\n\n\ndef split_combined_polys(polys, poly_lens, polys_per_mask):\n    """"""Split the combined 1-D polys into masks.\n\n    A mask is represented as a list of polys, and a poly is represented as\n    a 1-D array. In dataset, all masks are concatenated into a single 1-D\n    tensor. Here we need to split the tensor into original representations.\n\n    Args:\n        polys (list): a list (length = image num) of 1-D tensors\n        poly_lens (list): a list (length = image num) of poly length\n        polys_per_mask (list): a list (length = image num) of poly number\n            of each mask\n\n    Returns:\n        list: a list (length = image num) of list (length = mask num) of\n            list (length = poly num) of numpy array\n    """"""\n    mask_polys_list = []\n    for img_id in range(len(polys)):\n        polys_single = polys[img_id]\n        polys_lens_single = poly_lens[img_id].tolist()\n        polys_per_mask_single = polys_per_mask[img_id].tolist()\n\n        split_polys = mmcv.slice_list(polys_single, polys_lens_single)\n        mask_polys = mmcv.slice_list(split_polys, polys_per_mask_single)\n        mask_polys_list.append(mask_polys)\n    return mask_polys_list\n'"
detection/mmdet/core/post_processing/__init__.py,0,"b""from .bbox_nms import multiclass_nms\nfrom .merge_augs import (merge_aug_proposals, merge_aug_bboxes,\n                         merge_aug_scores, merge_aug_masks)\n\n__all__ = [\n    'multiclass_nms', 'merge_aug_proposals', 'merge_aug_bboxes',\n    'merge_aug_scores', 'merge_aug_masks'\n]\n"""
detection/mmdet/core/post_processing/bbox_nms.py,5,"b'import torch\n\nfrom mmdet.ops.nms import nms_wrapper\n\n\ndef multiclass_nms(multi_bboxes, multi_scores, score_thr, nms_cfg, max_num=-1):\n    """"""NMS for multi-class bboxes.\n\n    Args:\n        multi_bboxes (Tensor): shape (n, #class*4) or (n, 4)\n        multi_scores (Tensor): shape (n, #class)\n        score_thr (float): bbox threshold, bboxes with scores lower than it\n            will not be considered.\n        nms_thr (float): NMS IoU threshold\n        max_num (int): if there are more than max_num bboxes after NMS,\n            only top max_num will be kept.\n\n    Returns:\n        tuple: (bboxes, labels), tensors of shape (k, 5) and (k, 1). Labels\n            are 0-based.\n    """"""\n    num_classes = multi_scores.shape[1]\n    bboxes, labels = [], []\n    nms_cfg_ = nms_cfg.copy()\n    nms_type = nms_cfg_.pop(\'type\', \'nms\')\n    nms_op = getattr(nms_wrapper, nms_type)\n    for i in range(1, num_classes):\n        cls_inds = multi_scores[:, i] > score_thr\n        if not cls_inds.any():\n            continue\n        # get bboxes and scores of this class\n        if multi_bboxes.shape[1] == 4:\n            _bboxes = multi_bboxes[cls_inds, :]\n        else:\n            _bboxes = multi_bboxes[cls_inds, i * 4:(i + 1) * 4]\n        _scores = multi_scores[cls_inds, i]\n        cls_dets = torch.cat([_bboxes, _scores[:, None]], dim=1)\n        cls_dets, _ = nms_op(cls_dets, **nms_cfg_)\n        cls_labels = multi_bboxes.new_full(\n            (cls_dets.shape[0], ), i - 1, dtype=torch.long)\n        bboxes.append(cls_dets)\n        labels.append(cls_labels)\n    if bboxes:\n        bboxes = torch.cat(bboxes)\n        labels = torch.cat(labels)\n        if bboxes.shape[0] > max_num:\n            _, inds = bboxes[:, -1].sort(descending=True)\n            inds = inds[:max_num]\n            bboxes = bboxes[inds]\n            labels = labels[inds]\n    else:\n        bboxes = multi_bboxes.new_zeros((0, 5))\n        labels = multi_bboxes.new_zeros((0, ), dtype=torch.long)\n\n    return bboxes, labels\n'"
detection/mmdet/core/post_processing/merge_augs.py,5,"b'import torch\n\nimport numpy as np\n\nfrom mmdet.ops import nms\nfrom ..bbox import bbox_mapping_back\n\n\ndef merge_aug_proposals(aug_proposals, img_metas, rpn_test_cfg):\n    """"""Merge augmented proposals (multiscale, flip, etc.)\n\n    Args:\n        aug_proposals (list[Tensor]): proposals from different testing\n            schemes, shape (n, 5). Note that they are not rescaled to the\n            original image size.\n        img_metas (list[dict]): image info including ""shape_scale"" and ""flip"".\n        rpn_test_cfg (dict): rpn test config.\n\n    Returns:\n        Tensor: shape (n, 4), proposals corresponding to original image scale.\n    """"""\n    recovered_proposals = []\n    for proposals, img_info in zip(aug_proposals, img_metas):\n        img_shape = img_info[\'img_shape\']\n        scale_factor = img_info[\'scale_factor\']\n        flip = img_info[\'flip\']\n        _proposals = proposals.clone()\n        _proposals[:, :4] = bbox_mapping_back(_proposals[:, :4], img_shape,\n                                              scale_factor, flip)\n        recovered_proposals.append(_proposals)\n    aug_proposals = torch.cat(recovered_proposals, dim=0)\n    merged_proposals, _ = nms(aug_proposals, rpn_test_cfg.nms_thr)\n    scores = merged_proposals[:, 4]\n    _, order = scores.sort(0, descending=True)\n    num = min(rpn_test_cfg.max_num, merged_proposals.shape[0])\n    order = order[:num]\n    merged_proposals = merged_proposals[order, :]\n    return merged_proposals\n\n\ndef merge_aug_bboxes(aug_bboxes, aug_scores, img_metas, rcnn_test_cfg):\n    """"""Merge augmented detection bboxes and scores.\n\n    Args:\n        aug_bboxes (list[Tensor]): shape (n, 4*#class)\n        aug_scores (list[Tensor] or None): shape (n, #class)\n        img_shapes (list[Tensor]): shape (3, ).\n        rcnn_test_cfg (dict): rcnn test config.\n\n    Returns:\n        tuple: (bboxes, scores)\n    """"""\n    recovered_bboxes = []\n    for bboxes, img_info in zip(aug_bboxes, img_metas):\n        img_shape = img_info[0][\'img_shape\']\n        scale_factor = img_info[0][\'scale_factor\']\n        flip = img_info[0][\'flip\']\n        bboxes = bbox_mapping_back(bboxes, img_shape, scale_factor, flip)\n        recovered_bboxes.append(bboxes)\n    bboxes = torch.stack(recovered_bboxes).mean(dim=0)\n    if aug_scores is None:\n        return bboxes\n    else:\n        scores = torch.stack(aug_scores).mean(dim=0)\n        return bboxes, scores\n\n\ndef merge_aug_scores(aug_scores):\n    """"""Merge augmented bbox scores.""""""\n    if isinstance(aug_scores[0], torch.Tensor):\n        return torch.mean(torch.stack(aug_scores), dim=0)\n    else:\n        return np.mean(aug_scores, axis=0)\n\n\ndef merge_aug_masks(aug_masks, img_metas, rcnn_test_cfg, weights=None):\n    """"""Merge augmented mask prediction.\n\n    Args:\n        aug_masks (list[ndarray]): shape (n, #class, h, w)\n        img_shapes (list[ndarray]): shape (3, ).\n        rcnn_test_cfg (dict): rcnn test config.\n\n    Returns:\n        tuple: (bboxes, scores)\n    """"""\n    recovered_masks = [\n        mask if not img_info[0][\'flip\'] else mask[..., ::-1]\n        for mask, img_info in zip(aug_masks, img_metas)\n    ]\n    if weights is None:\n        merged_masks = np.mean(recovered_masks, axis=0)\n    else:\n        merged_masks = np.average(\n            np.array(recovered_masks), axis=0, weights=np.array(weights))\n    return merged_masks\n'"
detection/mmdet/core/utils/__init__.py,0,"b""from .dist_utils import allreduce_grads, DistOptimizerHook\nfrom .misc import tensor2imgs, unmap, multi_apply\n\n__all__ = [\n    'allreduce_grads', 'DistOptimizerHook', 'tensor2imgs', 'unmap',\n    'multi_apply'\n]\n"""
detection/mmdet/core/utils/dist_utils.py,2,"b""from collections import OrderedDict\n\nimport torch.distributed as dist\nfrom torch._utils import (_flatten_dense_tensors, _unflatten_dense_tensors,\n                          _take_tensors)\nfrom mmcv.runner import OptimizerHook\n\n\ndef _allreduce_coalesced(tensors, world_size, bucket_size_mb=-1):\n    if bucket_size_mb > 0:\n        bucket_size_bytes = bucket_size_mb * 1024 * 1024\n        buckets = _take_tensors(tensors, bucket_size_bytes)\n    else:\n        buckets = OrderedDict()\n        for tensor in tensors:\n            tp = tensor.type()\n            if tp not in buckets:\n                buckets[tp] = []\n            buckets[tp].append(tensor)\n        buckets = buckets.values()\n\n    for bucket in buckets:\n        flat_tensors = _flatten_dense_tensors(bucket)\n        dist.all_reduce(flat_tensors)\n        flat_tensors.div_(world_size)\n        for tensor, synced in zip(\n                bucket, _unflatten_dense_tensors(flat_tensors, bucket)):\n            tensor.copy_(synced)\n\n\ndef allreduce_grads(model, coalesce=True, bucket_size_mb=-1):\n    grads = [\n        param.grad.data for param in model.parameters()\n        if param.requires_grad and param.grad is not None\n    ]\n    world_size = dist.get_world_size()\n    if coalesce:\n        _allreduce_coalesced(grads, world_size, bucket_size_mb)\n    else:\n        for tensor in grads:\n            dist.all_reduce(tensor.div_(world_size))\n\n\nclass DistOptimizerHook(OptimizerHook):\n\n    def __init__(self, grad_clip=None, coalesce=True, bucket_size_mb=-1):\n        self.grad_clip = grad_clip\n        self.coalesce = coalesce\n        self.bucket_size_mb = bucket_size_mb\n\n    def after_train_iter(self, runner):\n        runner.optimizer.zero_grad()\n        runner.outputs['loss'].backward()\n        allreduce_grads(runner.model, self.coalesce, self.bucket_size_mb)\n        if self.grad_clip is not None:\n            self.clip_grads(runner.model.parameters())\n        runner.optimizer.step()\n"""
detection/mmdet/core/utils/misc.py,0,"b'from functools import partial\n\nimport mmcv\nimport numpy as np\nfrom six.moves import map, zip\n\n\ndef tensor2imgs(tensor, mean=(0, 0, 0), std=(1, 1, 1), to_rgb=True):\n    num_imgs = tensor.size(0)\n    mean = np.array(mean, dtype=np.float32)\n    std = np.array(std, dtype=np.float32)\n    imgs = []\n    for img_id in range(num_imgs):\n        img = tensor[img_id, ...].cpu().numpy().transpose(1, 2, 0)\n        img = mmcv.imdenormalize(\n            img, mean, std, to_bgr=to_rgb).astype(np.uint8)\n        imgs.append(np.ascontiguousarray(img))\n    return imgs\n\n\ndef multi_apply(func, *args, **kwargs):\n    pfunc = partial(func, **kwargs) if kwargs else func\n    map_results = map(pfunc, *args)\n    return tuple(map(list, zip(*map_results)))\n\n\ndef unmap(data, count, inds, fill=0):\n    """""" Unmap a subset of item (data) back to the original set of items (of\n    size count) """"""\n    if data.dim() == 1:\n        ret = data.new_full((count, ), fill)\n        ret[inds] = data\n    else:\n        new_size = (count, ) + data.size()[1:]\n        ret = data.new_full(new_size, fill)\n        ret[inds, :] = data\n    return ret\n'"
detection/mmdet/datasets/loader/__init__.py,0,"b""from .build_loader import build_dataloader\nfrom .sampler import GroupSampler, DistributedGroupSampler\n\n__all__ = [\n    'GroupSampler', 'DistributedGroupSampler', 'build_dataloader'\n]\n"""
detection/mmdet/datasets/loader/build_loader.py,1,"b""from functools import partial\n\nfrom mmcv.runner import get_dist_info\nfrom mmcv.parallel import collate\nfrom torch.utils.data import DataLoader\n\nfrom .sampler import GroupSampler, DistributedGroupSampler\n\n# https://github.com/pytorch/pytorch/issues/973\nimport resource\nrlimit = resource.getrlimit(resource.RLIMIT_NOFILE)\nresource.setrlimit(resource.RLIMIT_NOFILE, (4096, rlimit[1]))\n\n\ndef build_dataloader(dataset,\n                     imgs_per_gpu,\n                     workers_per_gpu,\n                     num_gpus=1,\n                     dist=True,\n                     **kwargs):\n    if dist:\n        rank, world_size = get_dist_info()\n        sampler = DistributedGroupSampler(dataset, imgs_per_gpu, world_size,\n                                          rank)\n        batch_size = imgs_per_gpu\n        num_workers = workers_per_gpu\n    else:\n        if not kwargs.get('shuffle', True):\n            sampler = None\n        else:\n            sampler = GroupSampler(dataset, imgs_per_gpu)\n        batch_size = num_gpus * imgs_per_gpu\n        num_workers = num_gpus * workers_per_gpu\n\n    data_loader = DataLoader(\n        dataset,\n        batch_size=batch_size,\n        sampler=sampler,\n        num_workers=num_workers,\n        collate_fn=partial(collate, samples_per_gpu=imgs_per_gpu),\n        pin_memory=False,\n        **kwargs)\n\n    return data_loader\n"""
detection/mmdet/datasets/loader/sampler.py,7,"b'from __future__ import division\n\nimport math\nimport torch\nimport numpy as np\n\nfrom torch.distributed import get_world_size, get_rank\nfrom torch.utils.data.sampler import Sampler\n\n\nclass GroupSampler(Sampler):\n\n    def __init__(self, dataset, samples_per_gpu=1):\n        assert hasattr(dataset, \'flag\')\n        self.dataset = dataset\n        self.samples_per_gpu = samples_per_gpu\n        self.flag = dataset.flag.astype(np.int64)\n        self.group_sizes = np.bincount(self.flag)\n        self.num_samples = 0\n        for i, size in enumerate(self.group_sizes):\n            self.num_samples += int(np.ceil(\n                size / self.samples_per_gpu)) * self.samples_per_gpu\n\n    def __iter__(self):\n        indices = []\n        for i, size in enumerate(self.group_sizes):\n            if size == 0:\n                continue\n            indice = np.where(self.flag == i)[0]\n            assert len(indice) == size\n            np.random.shuffle(indice)\n            num_extra = int(np.ceil(size / self.samples_per_gpu)\n                            ) * self.samples_per_gpu - len(indice)\n            indice = np.concatenate([indice, indice[:num_extra]])\n            indices.append(indice)\n        indices = np.concatenate(indices)\n        indices = [\n            indices[i * self.samples_per_gpu:(i + 1) * self.samples_per_gpu]\n            for i in np.random.permutation(\n                range(len(indices) // self.samples_per_gpu))\n        ]\n        indices = np.concatenate(indices)\n        indices = torch.from_numpy(indices).long()\n        assert len(indices) == self.num_samples\n        return iter(indices)\n\n    def __len__(self):\n        return self.num_samples\n\n\nclass DistributedGroupSampler(Sampler):\n    """"""Sampler that restricts data loading to a subset of the dataset.\n    It is especially useful in conjunction with\n    :class:`torch.nn.parallel.DistributedDataParallel`. In such case, each\n    process can pass a DistributedSampler instance as a DataLoader sampler,\n    and load a subset of the original dataset that is exclusive to it.\n    .. note::\n        Dataset is assumed to be of constant size.\n    Arguments:\n        dataset: Dataset used for sampling.\n        num_replicas (optional): Number of processes participating in\n            distributed training.\n        rank (optional): Rank of the current process within num_replicas.\n    """"""\n\n    def __init__(self,\n                 dataset,\n                 samples_per_gpu=1,\n                 num_replicas=None,\n                 rank=None):\n        if num_replicas is None:\n            num_replicas = get_world_size()\n        if rank is None:\n            rank = get_rank()\n        self.dataset = dataset\n        self.samples_per_gpu = samples_per_gpu\n        self.num_replicas = num_replicas\n        self.rank = rank\n        self.epoch = 0\n\n        assert hasattr(self.dataset, \'flag\')\n        self.flag = self.dataset.flag\n        self.group_sizes = np.bincount(self.flag)\n\n        self.num_samples = 0\n        for i, j in enumerate(self.group_sizes):\n            self.num_samples += int(\n                math.ceil(self.group_sizes[i] * 1.0 / self.samples_per_gpu /\n                          self.num_replicas)) * self.samples_per_gpu\n        self.total_size = self.num_samples * self.num_replicas\n\n    def __iter__(self):\n        # deterministically shuffle based on epoch\n        g = torch.Generator()\n        g.manual_seed(self.epoch)\n\n        indices = []\n        for i, size in enumerate(self.group_sizes):\n            if size > 0:\n                indice = np.where(self.flag == i)[0]\n                assert len(indice) == size\n                indice = indice[list(torch.randperm(int(size),\n                                                    generator=g))].tolist()\n                extra = int(\n                    math.ceil(\n                        size * 1.0 / self.samples_per_gpu / self.num_replicas)\n                ) * self.samples_per_gpu * self.num_replicas - len(indice)\n                indice += indice[:extra]\n                indices += indice\n\n        assert len(indices) == self.total_size\n\n        indices = [\n            indices[j] for i in list(\n                torch.randperm(\n                    len(indices) // self.samples_per_gpu, generator=g))\n            for j in range(i * self.samples_per_gpu, (i + 1) *\n                           self.samples_per_gpu)\n        ]\n\n        # subsample\n        offset = self.num_samples * self.rank\n        indices = indices[offset:offset + self.num_samples]\n        assert len(indices) == self.num_samples\n\n        return iter(indices)\n\n    def __len__(self):\n        return self.num_samples\n\n    def set_epoch(self, epoch):\n        self.epoch = epoch\n'"
detection/mmdet/models/anchor_heads/__init__.py,0,"b""from .anchor_head import AnchorHead\nfrom .rpn_head import RPNHead\nfrom .retina_head import RetinaHead\nfrom .ssd_head import SSDHead\n\n__all__ = ['AnchorHead', 'RPNHead', 'RetinaHead', 'SSDHead']\n"""
detection/mmdet/models/anchor_heads/anchor_head.py,4,"b'from __future__ import division\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom mmcv.cnn import normal_init\n\nfrom mmdet.core import (AnchorGenerator, anchor_target, delta2bbox,\n                        multi_apply, weighted_cross_entropy, weighted_smoothl1,\n                        weighted_binary_cross_entropy,\n                        weighted_sigmoid_focal_loss, multiclass_nms)\nfrom ..registry import HEADS\n\n\n@HEADS.register_module\nclass AnchorHead(nn.Module):\n    """"""Anchor-based head (RPN, RetinaNet, SSD, etc.).\n\n    Args:\n        in_channels (int): Number of channels in the input feature map.\n        feat_channels (int): Number of channels of the feature map.\n        anchor_scales (Iterable): Anchor scales.\n        anchor_ratios (Iterable): Anchor aspect ratios.\n        anchor_strides (Iterable): Anchor strides.\n        anchor_base_sizes (Iterable): Anchor base sizes.\n        target_means (Iterable): Mean values of regression targets.\n        target_stds (Iterable): Std values of regression targets.\n        use_sigmoid_cls (bool): Whether to use sigmoid loss for classification.\n            (softmax by default)\n        use_focal_loss (bool): Whether to use focal loss for classification.\n    """"""  # noqa: W605\n\n    def __init__(self,\n                 num_classes,\n                 in_channels,\n                 feat_channels=256,\n                 anchor_scales=[8, 16, 32],\n                 anchor_ratios=[0.5, 1.0, 2.0],\n                 anchor_strides=[4, 8, 16, 32, 64],\n                 anchor_base_sizes=None,\n                 target_means=(.0, .0, .0, .0),\n                 target_stds=(1.0, 1.0, 1.0, 1.0),\n                 use_sigmoid_cls=False,\n                 use_focal_loss=False):\n        super(AnchorHead, self).__init__()\n        self.in_channels = in_channels\n        self.num_classes = num_classes\n        self.feat_channels = feat_channels\n        self.anchor_scales = anchor_scales\n        self.anchor_ratios = anchor_ratios\n        self.anchor_strides = anchor_strides\n        self.anchor_base_sizes = list(\n            anchor_strides) if anchor_base_sizes is None else anchor_base_sizes\n        self.target_means = target_means\n        self.target_stds = target_stds\n        self.use_sigmoid_cls = use_sigmoid_cls\n        self.use_focal_loss = use_focal_loss\n\n        self.anchor_generators = []\n        for anchor_base in self.anchor_base_sizes:\n            self.anchor_generators.append(\n                AnchorGenerator(anchor_base, anchor_scales, anchor_ratios))\n\n        self.num_anchors = len(self.anchor_ratios) * len(self.anchor_scales)\n        if self.use_sigmoid_cls:\n            self.cls_out_channels = self.num_classes - 1\n        else:\n            self.cls_out_channels = self.num_classes\n\n        self._init_layers()\n\n    def _init_layers(self):\n        self.conv_cls = nn.Conv2d(self.feat_channels,\n                                  self.num_anchors * self.cls_out_channels, 1)\n        self.conv_reg = nn.Conv2d(self.feat_channels, self.num_anchors * 4, 1)\n\n    def init_weights(self):\n        normal_init(self.conv_cls, std=0.01)\n        normal_init(self.conv_reg, std=0.01)\n\n    def forward_single(self, x):\n        cls_score = self.conv_cls(x)\n        bbox_pred = self.conv_reg(x)\n        return cls_score, bbox_pred\n\n    def forward(self, feats):\n        return multi_apply(self.forward_single, feats)\n\n    def get_anchors(self, featmap_sizes, img_metas):\n        """"""Get anchors according to feature map sizes.\n\n        Args:\n            featmap_sizes (list[tuple]): Multi-level feature map sizes.\n            img_metas (list[dict]): Image meta info.\n\n        Returns:\n            tuple: anchors of each image, valid flags of each image\n        """"""\n        num_imgs = len(img_metas)\n        num_levels = len(featmap_sizes)\n\n        # since feature map sizes of all images are the same, we only compute\n        # anchors for one time\n        multi_level_anchors = []\n        for i in range(num_levels):\n            anchors = self.anchor_generators[i].grid_anchors(\n                featmap_sizes[i], self.anchor_strides[i])\n            multi_level_anchors.append(anchors)\n        anchor_list = [multi_level_anchors for _ in range(num_imgs)]\n\n        # for each image, we compute valid flags of multi level anchors\n        valid_flag_list = []\n        for img_id, img_meta in enumerate(img_metas):\n            multi_level_flags = []\n            for i in range(num_levels):\n                anchor_stride = self.anchor_strides[i]\n                feat_h, feat_w = featmap_sizes[i]\n                h, w, _ = img_meta[\'pad_shape\']\n                valid_feat_h = min(int(np.ceil(h / anchor_stride)), feat_h)\n                valid_feat_w = min(int(np.ceil(w / anchor_stride)), feat_w)\n                flags = self.anchor_generators[i].valid_flags(\n                    (feat_h, feat_w), (valid_feat_h, valid_feat_w))\n                multi_level_flags.append(flags)\n            valid_flag_list.append(multi_level_flags)\n\n        return anchor_list, valid_flag_list\n\n    def loss_single(self, cls_score, bbox_pred, labels, label_weights,\n                    bbox_targets, bbox_weights, num_total_samples, cfg):\n        # classification loss\n        if self.use_sigmoid_cls:\n            labels = labels.reshape(-1, self.cls_out_channels)\n            label_weights = label_weights.reshape(-1, self.cls_out_channels)\n        else:\n            labels = labels.reshape(-1)\n            label_weights = label_weights.reshape(-1)\n        cls_score = cls_score.permute(0, 2, 3, 1).reshape(\n            -1, self.cls_out_channels)\n        if self.use_sigmoid_cls:\n            if self.use_focal_loss:\n                cls_criterion = weighted_sigmoid_focal_loss\n            else:\n                cls_criterion = weighted_binary_cross_entropy\n        else:\n            if self.use_focal_loss:\n                raise NotImplementedError\n            else:\n                cls_criterion = weighted_cross_entropy\n        if self.use_focal_loss:\n            loss_cls = cls_criterion(\n                cls_score,\n                labels,\n                label_weights,\n                gamma=cfg.gamma,\n                alpha=cfg.alpha,\n                avg_factor=num_total_samples)\n        else:\n            loss_cls = cls_criterion(\n                cls_score, labels, label_weights, avg_factor=num_total_samples)\n        # regression loss\n        bbox_targets = bbox_targets.reshape(-1, 4)\n        bbox_weights = bbox_weights.reshape(-1, 4)\n        bbox_pred = bbox_pred.permute(0, 2, 3, 1).reshape(-1, 4)\n        loss_reg = weighted_smoothl1(\n            bbox_pred,\n            bbox_targets,\n            bbox_weights,\n            beta=cfg.smoothl1_beta,\n            avg_factor=num_total_samples)\n        return loss_cls, loss_reg\n\n    def loss(self,\n             cls_scores,\n             bbox_preds,\n             gt_bboxes,\n             gt_labels,\n             img_metas,\n             cfg,\n             gt_bboxes_ignore=None):\n        featmap_sizes = [featmap.size()[-2:] for featmap in cls_scores]\n        assert len(featmap_sizes) == len(self.anchor_generators)\n\n        anchor_list, valid_flag_list = self.get_anchors(\n            featmap_sizes, img_metas)\n        sampling = False if self.use_focal_loss else True\n        label_channels = self.cls_out_channels if self.use_sigmoid_cls else 1\n        cls_reg_targets = anchor_target(\n            anchor_list,\n            valid_flag_list,\n            gt_bboxes,\n            img_metas,\n            self.target_means,\n            self.target_stds,\n            cfg,\n            gt_bboxes_ignore_list=gt_bboxes_ignore,\n            gt_labels_list=gt_labels,\n            label_channels=label_channels,\n            sampling=sampling)\n        if cls_reg_targets is None:\n            return None\n        (labels_list, label_weights_list, bbox_targets_list, bbox_weights_list,\n         num_total_pos, num_total_neg) = cls_reg_targets\n        num_total_samples = (num_total_pos if self.use_focal_loss else\n                             num_total_pos + num_total_neg)\n        losses_cls, losses_reg = multi_apply(\n            self.loss_single,\n            cls_scores,\n            bbox_preds,\n            labels_list,\n            label_weights_list,\n            bbox_targets_list,\n            bbox_weights_list,\n            num_total_samples=num_total_samples,\n            cfg=cfg)\n        return dict(loss_cls=losses_cls, loss_reg=losses_reg)\n\n    def get_bboxes(self, cls_scores, bbox_preds, img_metas, cfg,\n                   rescale=False):\n        assert len(cls_scores) == len(bbox_preds)\n        num_levels = len(cls_scores)\n\n        mlvl_anchors = [\n            self.anchor_generators[i].grid_anchors(cls_scores[i].size()[-2:],\n                                                   self.anchor_strides[i])\n            for i in range(num_levels)\n        ]\n        result_list = []\n        for img_id in range(len(img_metas)):\n            cls_score_list = [\n                cls_scores[i][img_id].detach() for i in range(num_levels)\n            ]\n            bbox_pred_list = [\n                bbox_preds[i][img_id].detach() for i in range(num_levels)\n            ]\n            img_shape = img_metas[img_id][\'img_shape\']\n            scale_factor = img_metas[img_id][\'scale_factor\']\n            proposals = self.get_bboxes_single(cls_score_list, bbox_pred_list,\n                                               mlvl_anchors, img_shape,\n                                               scale_factor, cfg, rescale)\n            result_list.append(proposals)\n        return result_list\n\n    def get_bboxes_single(self,\n                          cls_scores,\n                          bbox_preds,\n                          mlvl_anchors,\n                          img_shape,\n                          scale_factor,\n                          cfg,\n                          rescale=False):\n        assert len(cls_scores) == len(bbox_preds) == len(mlvl_anchors)\n        mlvl_bboxes = []\n        mlvl_scores = []\n        for cls_score, bbox_pred, anchors in zip(cls_scores, bbox_preds,\n                                                 mlvl_anchors):\n            assert cls_score.size()[-2:] == bbox_pred.size()[-2:]\n            cls_score = cls_score.permute(1, 2, 0).reshape(\n                -1, self.cls_out_channels)\n            if self.use_sigmoid_cls:\n                scores = cls_score.sigmoid()\n            else:\n                scores = cls_score.softmax(-1)\n            bbox_pred = bbox_pred.permute(1, 2, 0).reshape(-1, 4)\n            nms_pre = cfg.get(\'nms_pre\', -1)\n            if nms_pre > 0 and scores.shape[0] > nms_pre:\n                if self.use_sigmoid_cls:\n                    max_scores, _ = scores.max(dim=1)\n                else:\n                    max_scores, _ = scores[:, 1:].max(dim=1)\n                _, topk_inds = max_scores.topk(nms_pre)\n                anchors = anchors[topk_inds, :]\n                bbox_pred = bbox_pred[topk_inds, :]\n                scores = scores[topk_inds, :]\n            bboxes = delta2bbox(anchors, bbox_pred, self.target_means,\n                                self.target_stds, img_shape)\n            mlvl_bboxes.append(bboxes)\n            mlvl_scores.append(scores)\n        mlvl_bboxes = torch.cat(mlvl_bboxes)\n        if rescale:\n            mlvl_bboxes /= mlvl_bboxes.new_tensor(scale_factor)\n        mlvl_scores = torch.cat(mlvl_scores)\n        if self.use_sigmoid_cls:\n            padding = mlvl_scores.new_zeros(mlvl_scores.shape[0], 1)\n            mlvl_scores = torch.cat([padding, mlvl_scores], dim=1)\n        det_bboxes, det_labels = multiclass_nms(\n            mlvl_bboxes, mlvl_scores, cfg.score_thr, cfg.nms, cfg.max_per_img)\n        return det_bboxes, det_labels\n'"
detection/mmdet/models/anchor_heads/retina_head.py,1,"b'import numpy as np\nimport torch.nn as nn\nfrom mmcv.cnn import normal_init\n\nfrom .anchor_head import AnchorHead\nfrom ..registry import HEADS\nfrom ..utils import bias_init_with_prob\n\n\n@HEADS.register_module\nclass RetinaHead(AnchorHead):\n\n    def __init__(self,\n                 num_classes,\n                 in_channels,\n                 stacked_convs=4,\n                 octave_base_scale=4,\n                 scales_per_octave=3,\n                 **kwargs):\n        self.stacked_convs = stacked_convs\n        self.octave_base_scale = octave_base_scale\n        self.scales_per_octave = scales_per_octave\n        octave_scales = np.array(\n            [2**(i / scales_per_octave) for i in range(scales_per_octave)])\n        anchor_scales = octave_scales * octave_base_scale\n        super(RetinaHead, self).__init__(\n            num_classes,\n            in_channels,\n            anchor_scales=anchor_scales,\n            use_sigmoid_cls=True,\n            use_focal_loss=True,\n            **kwargs)\n\n    def _init_layers(self):\n        self.relu = nn.ReLU(inplace=True)\n        self.cls_convs = nn.ModuleList()\n        self.reg_convs = nn.ModuleList()\n        for i in range(self.stacked_convs):\n            chn = self.in_channels if i == 0 else self.feat_channels\n            self.cls_convs.append(\n                nn.Conv2d(chn, self.feat_channels, 3, stride=1, padding=1))\n            self.reg_convs.append(\n                nn.Conv2d(chn, self.feat_channels, 3, stride=1, padding=1))\n        self.retina_cls = nn.Conv2d(\n            self.feat_channels,\n            self.num_anchors * self.cls_out_channels,\n            3,\n            padding=1)\n        self.retina_reg = nn.Conv2d(\n            self.feat_channels, self.num_anchors * 4, 3, padding=1)\n\n    def init_weights(self):\n        for m in self.cls_convs:\n            normal_init(m, std=0.01)\n        for m in self.reg_convs:\n            normal_init(m, std=0.01)\n        bias_cls = bias_init_with_prob(0.01)\n        normal_init(self.retina_cls, std=0.01, bias=bias_cls)\n        normal_init(self.retina_reg, std=0.01)\n\n    def forward_single(self, x):\n        cls_feat = x\n        reg_feat = x\n        for cls_conv in self.cls_convs:\n            cls_feat = self.relu(cls_conv(cls_feat))\n        for reg_conv in self.reg_convs:\n            reg_feat = self.relu(reg_conv(reg_feat))\n        cls_score = self.retina_cls(cls_feat)\n        bbox_pred = self.retina_reg(reg_feat)\n        return cls_score, bbox_pred\n'"
detection/mmdet/models/anchor_heads/rpn_head.py,5,"b""import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom mmcv.cnn import normal_init\n\nfrom mmdet.core import delta2bbox\nfrom mmdet.ops import nms\nfrom .anchor_head import AnchorHead\nfrom ..registry import HEADS\n\n\n@HEADS.register_module\nclass RPNHead(AnchorHead):\n\n    def __init__(self, in_channels, **kwargs):\n        super(RPNHead, self).__init__(2, in_channels, **kwargs)\n\n    def _init_layers(self):\n        self.rpn_conv = nn.Conv2d(\n            self.in_channels, self.feat_channels, 3, padding=1)\n        self.rpn_cls = nn.Conv2d(self.feat_channels,\n                                 self.num_anchors * self.cls_out_channels, 1)\n        self.rpn_reg = nn.Conv2d(self.feat_channels, self.num_anchors * 4, 1)\n\n    def init_weights(self):\n        normal_init(self.rpn_conv, std=0.01)\n        normal_init(self.rpn_cls, std=0.01)\n        normal_init(self.rpn_reg, std=0.01)\n\n    def forward_single(self, x):\n        x = self.rpn_conv(x)\n        x = F.relu(x, inplace=True)\n        rpn_cls_score = self.rpn_cls(x)\n        rpn_bbox_pred = self.rpn_reg(x)\n        return rpn_cls_score, rpn_bbox_pred\n\n    def loss(self,\n             cls_scores,\n             bbox_preds,\n             gt_bboxes,\n             img_metas,\n             cfg,\n             gt_bboxes_ignore=None):\n        losses = super(RPNHead, self).loss(\n            cls_scores,\n            bbox_preds,\n            gt_bboxes,\n            None,\n            img_metas,\n            cfg,\n            gt_bboxes_ignore=gt_bboxes_ignore)\n        return dict(\n            loss_rpn_cls=losses['loss_cls'], loss_rpn_reg=losses['loss_reg'])\n\n    def get_bboxes_single(self,\n                          cls_scores,\n                          bbox_preds,\n                          mlvl_anchors,\n                          img_shape,\n                          scale_factor,\n                          cfg,\n                          rescale=False):\n        mlvl_proposals = []\n        for idx in range(len(cls_scores)):\n            rpn_cls_score = cls_scores[idx]\n            rpn_bbox_pred = bbox_preds[idx]\n            assert rpn_cls_score.size()[-2:] == rpn_bbox_pred.size()[-2:]\n            anchors = mlvl_anchors[idx]\n            rpn_cls_score = rpn_cls_score.permute(1, 2, 0)\n            if self.use_sigmoid_cls:\n                rpn_cls_score = rpn_cls_score.reshape(-1)\n                scores = rpn_cls_score.sigmoid()\n            else:\n                rpn_cls_score = rpn_cls_score.reshape(-1, 2)\n                scores = rpn_cls_score.softmax(dim=1)[:, 1]\n            rpn_bbox_pred = rpn_bbox_pred.permute(1, 2, 0).reshape(-1, 4)\n            if cfg.nms_pre > 0 and scores.shape[0] > cfg.nms_pre:\n                _, topk_inds = scores.topk(cfg.nms_pre)\n                rpn_bbox_pred = rpn_bbox_pred[topk_inds, :]\n                anchors = anchors[topk_inds, :]\n                scores = scores[topk_inds]\n            proposals = delta2bbox(anchors, rpn_bbox_pred, self.target_means,\n                                   self.target_stds, img_shape)\n            if cfg.min_bbox_size > 0:\n                w = proposals[:, 2] - proposals[:, 0] + 1\n                h = proposals[:, 3] - proposals[:, 1] + 1\n                valid_inds = torch.nonzero((w >= cfg.min_bbox_size) &\n                                           (h >= cfg.min_bbox_size)).squeeze()\n                proposals = proposals[valid_inds, :]\n                scores = scores[valid_inds]\n            proposals = torch.cat([proposals, scores.unsqueeze(-1)], dim=-1)\n            proposals, _ = nms(proposals, cfg.nms_thr)\n            proposals = proposals[:cfg.nms_post, :]\n            mlvl_proposals.append(proposals)\n        proposals = torch.cat(mlvl_proposals, 0)\n        if cfg.nms_across_levels:\n            proposals, _ = nms(proposals, cfg.nms_thr)\n            proposals = proposals[:cfg.max_num, :]\n        else:\n            scores = proposals[:, 4]\n            num = min(cfg.max_num, proposals.shape[0])\n            _, topk_inds = scores.topk(num)\n            proposals = proposals[topk_inds, :]\n        return proposals\n"""
detection/mmdet/models/anchor_heads/ssd_head.py,10,"b""import numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom mmcv.cnn import xavier_init\n\nfrom mmdet.core import (AnchorGenerator, anchor_target, weighted_smoothl1,\n                        multi_apply)\nfrom .anchor_head import AnchorHead\nfrom ..registry import HEADS\n\n\n@HEADS.register_module\nclass SSDHead(AnchorHead):\n\n    def __init__(self,\n                 input_size=300,\n                 num_classes=81,\n                 in_channels=(512, 1024, 512, 256, 256, 256),\n                 anchor_strides=(8, 16, 32, 64, 100, 300),\n                 basesize_ratio_range=(0.1, 0.9),\n                 anchor_ratios=([2], [2, 3], [2, 3], [2, 3], [2], [2]),\n                 target_means=(.0, .0, .0, .0),\n                 target_stds=(1.0, 1.0, 1.0, 1.0)):\n        super(AnchorHead, self).__init__()\n        self.input_size = input_size\n        self.num_classes = num_classes\n        self.in_channels = in_channels\n        self.cls_out_channels = num_classes\n        num_anchors = [len(ratios) * 2 + 2 for ratios in anchor_ratios]\n        reg_convs = []\n        cls_convs = []\n        for i in range(len(in_channels)):\n            reg_convs.append(\n                nn.Conv2d(\n                    in_channels[i],\n                    num_anchors[i] * 4,\n                    kernel_size=3,\n                    padding=1))\n            cls_convs.append(\n                nn.Conv2d(\n                    in_channels[i],\n                    num_anchors[i] * num_classes,\n                    kernel_size=3,\n                    padding=1))\n        self.reg_convs = nn.ModuleList(reg_convs)\n        self.cls_convs = nn.ModuleList(cls_convs)\n\n        min_ratio, max_ratio = basesize_ratio_range\n        min_ratio = int(min_ratio * 100)\n        max_ratio = int(max_ratio * 100)\n        step = int(np.floor(max_ratio - min_ratio) / (len(in_channels) - 2))\n        min_sizes = []\n        max_sizes = []\n        for r in range(int(min_ratio), int(max_ratio) + 1, step):\n            min_sizes.append(int(input_size * r / 100))\n            max_sizes.append(int(input_size * (r + step) / 100))\n        if input_size == 300:\n            if basesize_ratio_range[0] == 0.15:  # SSD300 COCO\n                min_sizes.insert(0, int(input_size * 7 / 100))\n                max_sizes.insert(0, int(input_size * 15 / 100))\n            elif basesize_ratio_range[0] == 0.2:  # SSD300 VOC\n                min_sizes.insert(0, int(input_size * 10 / 100))\n                max_sizes.insert(0, int(input_size * 20 / 100))\n        elif input_size == 512:\n            if basesize_ratio_range[0] == 0.1:  # SSD512 COCO\n                min_sizes.insert(0, int(input_size * 4 / 100))\n                max_sizes.insert(0, int(input_size * 10 / 100))\n            elif basesize_ratio_range[0] == 0.15:  # SSD512 VOC\n                min_sizes.insert(0, int(input_size * 7 / 100))\n                max_sizes.insert(0, int(input_size * 15 / 100))\n        self.anchor_generators = []\n        self.anchor_strides = anchor_strides\n        for k in range(len(anchor_strides)):\n            base_size = min_sizes[k]\n            stride = anchor_strides[k]\n            ctr = ((stride - 1) / 2., (stride - 1) / 2.)\n            scales = [1., np.sqrt(max_sizes[k] / min_sizes[k])]\n            ratios = [1.]\n            for r in anchor_ratios[k]:\n                ratios += [1 / r, r]  # 4 or 6 ratio\n            anchor_generator = AnchorGenerator(\n                base_size, scales, ratios, scale_major=False, ctr=ctr)\n            indices = list(range(len(ratios)))\n            indices.insert(1, len(indices))\n            anchor_generator.base_anchors = torch.index_select(\n                anchor_generator.base_anchors, 0, torch.LongTensor(indices))\n            self.anchor_generators.append(anchor_generator)\n\n        self.target_means = target_means\n        self.target_stds = target_stds\n        self.use_sigmoid_cls = False\n        self.use_focal_loss = False\n\n    def init_weights(self):\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                xavier_init(m, distribution='uniform', bias=0)\n\n    def forward(self, feats):\n        cls_scores = []\n        bbox_preds = []\n        for feat, reg_conv, cls_conv in zip(feats, self.reg_convs,\n                                            self.cls_convs):\n            cls_scores.append(cls_conv(feat))\n            bbox_preds.append(reg_conv(feat))\n        return cls_scores, bbox_preds\n\n    def loss_single(self, cls_score, bbox_pred, labels, label_weights,\n                    bbox_targets, bbox_weights, num_total_samples, cfg):\n        loss_cls_all = F.cross_entropy(\n            cls_score, labels, reduction='none') * label_weights\n        pos_inds = (labels > 0).nonzero().view(-1)\n        neg_inds = (labels == 0).nonzero().view(-1)\n\n        num_pos_samples = pos_inds.size(0)\n        num_neg_samples = cfg.neg_pos_ratio * num_pos_samples\n        if num_neg_samples > neg_inds.size(0):\n            num_neg_samples = neg_inds.size(0)\n        topk_loss_cls_neg, _ = loss_cls_all[neg_inds].topk(num_neg_samples)\n        loss_cls_pos = loss_cls_all[pos_inds].sum()\n        loss_cls_neg = topk_loss_cls_neg.sum()\n        loss_cls = (loss_cls_pos + loss_cls_neg) / num_total_samples\n\n        loss_reg = weighted_smoothl1(\n            bbox_pred,\n            bbox_targets,\n            bbox_weights,\n            beta=cfg.smoothl1_beta,\n            avg_factor=num_total_samples)\n        return loss_cls[None], loss_reg\n\n    def loss(self,\n             cls_scores,\n             bbox_preds,\n             gt_bboxes,\n             gt_labels,\n             img_metas,\n             cfg,\n             gt_bboxes_ignore=None):\n        featmap_sizes = [featmap.size()[-2:] for featmap in cls_scores]\n        assert len(featmap_sizes) == len(self.anchor_generators)\n\n        anchor_list, valid_flag_list = self.get_anchors(\n            featmap_sizes, img_metas)\n        cls_reg_targets = anchor_target(\n            anchor_list,\n            valid_flag_list,\n            gt_bboxes,\n            img_metas,\n            self.target_means,\n            self.target_stds,\n            cfg,\n            gt_bboxes_ignore_list=gt_bboxes_ignore,\n            gt_labels_list=gt_labels,\n            label_channels=1,\n            sampling=False,\n            unmap_outputs=False)\n        if cls_reg_targets is None:\n            return None\n        (labels_list, label_weights_list, bbox_targets_list, bbox_weights_list,\n         num_total_pos, num_total_neg) = cls_reg_targets\n\n        num_images = len(img_metas)\n        all_cls_scores = torch.cat([\n            s.permute(0, 2, 3, 1).reshape(\n                num_images, -1, self.cls_out_channels) for s in cls_scores\n        ], 1)\n        all_labels = torch.cat(labels_list, -1).view(num_images, -1)\n        all_label_weights = torch.cat(label_weights_list, -1).view(\n            num_images, -1)\n        all_bbox_preds = torch.cat([\n            b.permute(0, 2, 3, 1).reshape(num_images, -1, 4)\n            for b in bbox_preds\n        ], -2)\n        all_bbox_targets = torch.cat(bbox_targets_list, -2).view(\n            num_images, -1, 4)\n        all_bbox_weights = torch.cat(bbox_weights_list, -2).view(\n            num_images, -1, 4)\n\n        losses_cls, losses_reg = multi_apply(\n            self.loss_single,\n            all_cls_scores,\n            all_bbox_preds,\n            all_labels,\n            all_label_weights,\n            all_bbox_targets,\n            all_bbox_weights,\n            num_total_samples=num_total_pos,\n            cfg=cfg)\n        return dict(loss_cls=losses_cls, loss_reg=losses_reg)\n"""
detection/mmdet/models/backbones/__init__.py,0,"b""from .resnet import ResNet\nfrom .resnext import ResNeXt\nfrom .ssd_vgg import SSDVGG\nfrom .resnet_sge import ResNetSGE\nfrom .resnet_se import ResNetSE\nfrom .resnet_cbam import ResNetCBAM\nfrom .resnet_bam import ResNetBAM\nfrom .resnet_gc import ResNetGC\nfrom .resnet_sk import ResNetSK\n\n__all__ = ['ResNet', 'ResNeXt', 'SSDVGG', 'ResNetSGE' \\\n        , 'ResNetSE', 'ResNetCBAM', 'ResNetGC' \\\n        , 'ResNetBAM', 'ResNetSK']\n"""
detection/mmdet/models/backbones/bam.py,2,"b""import torch\nimport math\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Flatten(nn.Module):\n    def forward(self, x):\n        return x.view(x.size(0), -1)\nclass ChannelGate(nn.Module):\n    def __init__(self, gate_channel, reduction_ratio=16, num_layers=1):\n        super(ChannelGate, self).__init__()\n        # self.gate_activation = gate_activation\n        self.gate_c = nn.Sequential()\n        self.gate_c.add_module( 'flatten', Flatten() )\n        gate_channels = [gate_channel]\n        gate_channels += [gate_channel // reduction_ratio] * num_layers\n        gate_channels += [gate_channel]\n        for i in range( len(gate_channels) - 2 ):\n            self.gate_c.add_module( 'gate_c_fc_%d'%i, nn.Linear(gate_channels[i], gate_channels[i+1]) )\n            self.gate_c.add_module( 'gate_c_bn_%d'%(i+1), nn.BatchNorm1d(gate_channels[i+1]) )\n            self.gate_c.add_module( 'gate_c_relu_%d'%(i+1), nn.ReLU() )\n        self.gate_c.add_module( 'gate_c_fc_final', nn.Linear(gate_channels[-2], gate_channels[-1]) )\n        self.pool = nn.AdaptiveAvgPool2d(1)\n    def forward(self, in_tensor):\n        #avg_pool = F.avg_pool2d( in_tensor, in_tensor.size(2), stride=in_tensor.size(2) )\n        avg_pool = self.pool(in_tensor)\n        return self.gate_c( avg_pool ).unsqueeze(2).unsqueeze(3).expand_as(in_tensor)\n\nclass SpatialGate(nn.Module):\n    def __init__(self, gate_channel, reduction_ratio=16, dilation_conv_num=2, dilation_val=4):\n        super(SpatialGate, self).__init__()\n        self.gate_s = nn.Sequential()\n        self.gate_s.add_module( 'gate_s_conv_reduce0', nn.Conv2d(gate_channel, gate_channel//reduction_ratio, kernel_size=1))\n        self.gate_s.add_module( 'gate_s_bn_reduce0',\tnn.BatchNorm2d(gate_channel//reduction_ratio) )\n        self.gate_s.add_module( 'gate_s_relu_reduce0',nn.ReLU() )\n        for i in range( dilation_conv_num ):\n            self.gate_s.add_module( 'gate_s_conv_di_%d'%i, nn.Conv2d(gate_channel//reduction_ratio, gate_channel//reduction_ratio, kernel_size=3, \\\n\t\t\t\t\t\tpadding=dilation_val, dilation=dilation_val) )\n            self.gate_s.add_module( 'gate_s_bn_di_%d'%i, nn.BatchNorm2d(gate_channel//reduction_ratio) )\n            self.gate_s.add_module( 'gate_s_relu_di_%d'%i, nn.ReLU() )\n        self.gate_s.add_module( 'gate_s_conv_final', nn.Conv2d(gate_channel//reduction_ratio, 1, kernel_size=1) )\n    def forward(self, in_tensor):\n        return self.gate_s( in_tensor ).expand_as(in_tensor)\nclass BAM(nn.Module):\n    def __init__(self, gate_channel):\n        super(BAM, self).__init__()\n        self.channel_att = ChannelGate(gate_channel)\n        self.spatial_att = SpatialGate(gate_channel)\n    def forward(self,in_tensor):\n        ca = self.channel_att(in_tensor)\n        sa = self.spatial_att(in_tensor)\n        #print('ca size = ', ca.size(), 'sa size = ', sa.size())\n        att = 1 + F.sigmoid( ca * sa ) #self.channel_att(in_tensor) * self.spatial_att(in_tensor) )\n        return att * in_tensor\n"""
detection/mmdet/models/backbones/cbam.py,4,"b""import torch\nimport math\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass BasicConv(nn.Module):\n    def __init__(self, in_planes, out_planes, kernel_size, stride=1, padding=0, dilation=1, groups=1, relu=True, bn=True, bias=False):\n        super(BasicConv, self).__init__()\n        self.out_channels = out_planes\n        self.conv = nn.Conv2d(in_planes, out_planes, kernel_size=kernel_size, stride=stride, padding=padding, dilation=dilation, groups=groups, bias=bias)\n        self.bn = nn.BatchNorm2d(out_planes,eps=1e-5, momentum=0.01, affine=True) if bn else None\n        self.relu = nn.ReLU() if relu else None\n\n    def forward(self, x):\n        x = self.conv(x)\n        if self.bn is not None:\n            x = self.bn(x)\n        if self.relu is not None:\n            x = self.relu(x)\n        return x\n\nclass Flatten(nn.Module):\n    def forward(self, x):\n        return x.view(x.size(0), -1)\n\nclass ChannelGate(nn.Module):\n    def __init__(self, gate_channels, reduction_ratio=16, pool_types=['avg', 'max']):\n        super(ChannelGate, self).__init__()\n        self.gate_channels = gate_channels\n        self.mlp = nn.Sequential(\n            Flatten(),\n            nn.Linear(gate_channels, gate_channels // reduction_ratio),\n            nn.ReLU(),\n            nn.Linear(gate_channels // reduction_ratio, gate_channels)\n            )\n        self.pool_types = pool_types\n    def forward(self, x):\n        channel_att_sum = None\n        for pool_type in self.pool_types:\n            if pool_type=='avg':\n                avg_pool = F.avg_pool2d( x, (x.size(2), x.size(3)), stride=(x.size(2), x.size(3)))\n                channel_att_raw = self.mlp( avg_pool )\n            elif pool_type=='max':\n                max_pool = F.max_pool2d( x, (x.size(2), x.size(3)), stride=(x.size(2), x.size(3)))\n                channel_att_raw = self.mlp( max_pool )\n            elif pool_type=='lp':\n                lp_pool = F.lp_pool2d( x, 2, (x.size(2), x.size(3)), stride=(x.size(2), x.size(3)))\n                channel_att_raw = self.mlp( lp_pool )\n            elif pool_type=='lse':\n                # LSE pool only\n                lse_pool = logsumexp_2d(x)\n                channel_att_raw = self.mlp( lse_pool )\n\n            if channel_att_sum is None:\n                channel_att_sum = channel_att_raw\n            else:\n                channel_att_sum = channel_att_sum + channel_att_raw\n\n        scale = F.sigmoid( channel_att_sum ).unsqueeze(2).unsqueeze(3).expand_as(x)\n        return x * scale\n\ndef logsumexp_2d(tensor):\n    tensor_flatten = tensor.view(tensor.size(0), tensor.size(1), -1)\n    s, _ = torch.max(tensor_flatten, dim=2, keepdim=True)\n    outputs = s + (tensor_flatten - s).exp().sum(dim=2, keepdim=True).log()\n    return outputs\n\nclass ChannelPool(nn.Module):\n    def forward(self, x):\n        return torch.cat( (torch.max(x,1)[0].unsqueeze(1), torch.mean(x,1).unsqueeze(1)), dim=1 )\n\nclass SpatialGate(nn.Module):\n    def __init__(self):\n        super(SpatialGate, self).__init__()\n        kernel_size = 7\n        self.compress = ChannelPool()\n        self.spatial = BasicConv(2, 1, kernel_size, stride=1, padding=(kernel_size-1) // 2, relu=False)\n    def forward(self, x):\n        x_compress = self.compress(x)\n        x_out = self.spatial(x_compress)\n        scale = F.sigmoid(x_out) # broadcasting\n        return x * scale\n\nclass CBAM(nn.Module):\n    def __init__(self, gate_channels, reduction_ratio=16, pool_types=['avg', 'max'], no_spatial=False):\n        super(CBAM, self).__init__()\n        self.ChannelGate = ChannelGate(gate_channels, reduction_ratio, pool_types)\n        self.no_spatial=no_spatial\n        if not no_spatial:\n            self.SpatialGate = SpatialGate()\n    def forward(self, x):\n        x_out = self.ChannelGate(x)\n        if not self.no_spatial:\n            x_out = self.SpatialGate(x_out)\n        return x_out\n"""
detection/mmdet/models/backbones/global_context.py,2,"b""import torch\r\nfrom torch import nn\r\n\r\n__all__ = ['ContextBlock2d']\r\n#nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\r\n#nn.init.constant_(m.weight, 1)\r\n\r\ndef last_zero_init(m):\r\n    if isinstance(m, nn.Sequential):\r\n        #constant_init(m[-1], val=0)\r\n        nn.init.constant_(m[-1].weight, 0)\r\n        m[-1].inited = True\r\n    else:\r\n        #constant_init(m, val=0)\r\n        nn.init.constant_(m.weight, 0)\r\n        m.inited = True\r\n\r\n\r\nclass ContextBlock2d(nn.Module):\r\n    # pool = 'att', fusions = ['channel_add']\r\n    def __init__(self, inplanes, planes, pool, fusions):\r\n        super(ContextBlock2d, self).__init__()\r\n        assert pool in ['avg', 'att']\r\n        assert all([f in ['channel_add', 'channel_mul'] for f in fusions])\r\n        assert len(fusions) > 0, 'at least one fusion should be used'\r\n        self.inplanes = inplanes\r\n        self.planes = planes\r\n        self.pool = pool\r\n        self.fusions = fusions\r\n        if 'att' in pool:\r\n            self.conv_mask = nn.Conv2d(inplanes, 1, kernel_size=1)\r\n            self.softmax = nn.Softmax(dim=2)\r\n        else:\r\n            self.avg_pool = nn.AdaptiveAvgPool2d(1)\r\n        if 'channel_add' in fusions:\r\n            self.channel_add_conv = nn.Sequential(\r\n                nn.Conv2d(self.inplanes, self.planes, kernel_size=1),\r\n                nn.LayerNorm([self.planes, 1, 1]),\r\n                nn.ReLU(inplace=True),\r\n                nn.Conv2d(self.planes, self.inplanes, kernel_size=1)\r\n            )\r\n        else:\r\n            self.channel_add_conv = None\r\n        if 'channel_mul' in fusions:\r\n            self.channel_mul_conv = nn.Sequential(\r\n                nn.Conv2d(self.inplanes, self.planes, kernel_size=1),\r\n                nn.LayerNorm([self.planes, 1, 1]),\r\n                nn.ReLU(inplace=True),\r\n                nn.Conv2d(self.planes, self.inplanes, kernel_size=1)\r\n            )\r\n        else:\r\n            self.channel_mul_conv = None\r\n        self.reset_parameters()\r\n\r\n    def reset_parameters(self):\r\n        if self.pool == 'att':\r\n            #kaiming_init(self.conv_mask, mode='fan_in')\r\n            nn.init.kaiming_normal_(self.conv_mask.weight, mode='fan_in', nonlinearity='relu')\r\n            self.conv_mask.inited = True\r\n\r\n        if self.channel_add_conv is not None:\r\n            last_zero_init(self.channel_add_conv)\r\n        if self.channel_mul_conv is not None:\r\n            last_zero_init(self.channel_mul_conv)\r\n\r\n    def spatial_pool(self, x):\r\n        batch, channel, height, width = x.size()\r\n        if self.pool == 'att':\r\n            input_x = x\r\n            # [N, C, H * W]\r\n            input_x = input_x.view(batch, channel, height * width)\r\n            # [N, 1, C, H * W]\r\n            input_x = input_x.unsqueeze(1)\r\n            # [N, 1, H, W]\r\n            context_mask = self.conv_mask(x)\r\n            # [N, 1, H * W]\r\n            context_mask = context_mask.view(batch, 1, height * width)\r\n            # [N, 1, H * W]\r\n            context_mask = self.softmax(context_mask)\r\n            # [N, 1, H * W, 1]\r\n            context_mask = context_mask.unsqueeze(3)\r\n            # [N, 1, C, 1]\r\n            context = torch.matmul(input_x, context_mask)\r\n            # [N, C, 1, 1]\r\n            context = context.view(batch, channel, 1, 1)\r\n        else:\r\n            # [N, C, 1, 1]\r\n            context = self.avg_pool(x)\r\n\r\n        return context\r\n\r\n    def forward(self, x):\r\n        # [N, C, 1, 1]\r\n        context = self.spatial_pool(x)\r\n\r\n        if self.channel_mul_conv is not None:\r\n            # [N, C, 1, 1]\r\n            channel_mul_term = torch.sigmoid(self.channel_mul_conv(context))\r\n            out = x * channel_mul_term\r\n        else:\r\n            out = x\r\n        if self.channel_add_conv is not None:\r\n            # [N, C, 1, 1]\r\n            channel_add_term = self.channel_add_conv(context)\r\n            out = out + channel_add_term\r\n\r\n        return out\r\n"""
detection/mmdet/models/backbones/resnet.py,3,"b'import logging\n\nimport torch.nn as nn\nimport torch.utils.checkpoint as cp\n\nfrom mmcv.cnn import constant_init, kaiming_init\nfrom mmcv.runner import load_checkpoint\n\nfrom mmdet.ops import DeformConv, ModulatedDeformConv\nfrom ..registry import BACKBONES\nfrom ..utils import build_norm_layer\n\n\ndef conv3x3(in_planes, out_planes, stride=1, dilation=1):\n    ""3x3 convolution with padding""\n    return nn.Conv2d(\n        in_planes,\n        out_planes,\n        kernel_size=3,\n        stride=stride,\n        padding=dilation,\n        dilation=dilation,\n        bias=False)\n\n\nclass BasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self,\n                 inplanes,\n                 planes,\n                 stride=1,\n                 dilation=1,\n                 downsample=None,\n                 style=\'pytorch\',\n                 with_cp=False,\n                 normalize=dict(type=\'BN\'),\n                 dcn=None):\n        super(BasicBlock, self).__init__()\n        assert dcn is None, ""Not implemented yet.""\n\n        self.norm1_name, norm1 = build_norm_layer(normalize, planes, postfix=1)\n        self.norm2_name, norm2 = build_norm_layer(normalize, planes, postfix=2)\n\n        self.conv1 = conv3x3(inplanes, planes, stride, dilation)\n        self.add_module(self.norm1_name, norm1)\n        self.conv2 = conv3x3(planes, planes)\n        self.add_module(self.norm2_name, norm2)\n\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = downsample\n        self.stride = stride\n        self.dilation = dilation\n        assert not with_cp\n\n    @property\n    def norm1(self):\n        return getattr(self, self.norm1_name)\n\n    @property\n    def norm2(self):\n        return getattr(self, self.norm2_name)\n\n    def forward(self, x):\n        identity = x\n\n        out = self.conv1(x)\n        out = self.norm1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.norm2(out)\n\n        if self.downsample is not None:\n            identity = self.downsample(x)\n\n        out += identity\n        out = self.relu(out)\n\n        return out\n\n\nclass Bottleneck(nn.Module):\n    expansion = 4\n\n    def __init__(self,\n                 inplanes,\n                 planes,\n                 stride=1,\n                 dilation=1,\n                 downsample=None,\n                 style=\'pytorch\',\n                 with_cp=False,\n                 normalize=dict(type=\'BN\'),\n                 dcn=None):\n        """"""Bottleneck block for ResNet.\n        If style is ""pytorch"", the stride-two layer is the 3x3 conv layer,\n        if it is ""caffe"", the stride-two layer is the first 1x1 conv layer.\n        """"""\n        super(Bottleneck, self).__init__()\n        assert style in [\'pytorch\', \'caffe\']\n        assert dcn is None or isinstance(dcn, dict)\n        self.inplanes = inplanes\n        self.planes = planes\n        self.normalize = normalize\n        self.dcn = dcn\n        self.with_dcn = dcn is not None\n        if style == \'pytorch\':\n            self.conv1_stride = 1\n            self.conv2_stride = stride\n        else:\n            self.conv1_stride = stride\n            self.conv2_stride = 1\n\n        self.norm1_name, norm1 = build_norm_layer(normalize, planes, postfix=1)\n        self.norm2_name, norm2 = build_norm_layer(normalize, planes, postfix=2)\n        self.norm3_name, norm3 = build_norm_layer(\n            normalize, planes * self.expansion, postfix=3)\n\n        self.conv1 = nn.Conv2d(\n            inplanes,\n            planes,\n            kernel_size=1,\n            stride=self.conv1_stride,\n            bias=False)\n        self.add_module(self.norm1_name, norm1)\n        fallback_on_stride = False\n        self.with_modulated_dcn = False\n        if self.with_dcn:\n            fallback_on_stride = dcn.get(\'fallback_on_stride\', False)\n            self.with_modulated_dcn = dcn.get(\'modulated\', False)\n        if not self.with_dcn or fallback_on_stride:\n            self.conv2 = nn.Conv2d(\n                planes,\n                planes,\n                kernel_size=3,\n                stride=self.conv2_stride,\n                padding=dilation,\n                dilation=dilation,\n                bias=False)\n        else:\n            deformable_groups = dcn.get(\'deformable_groups\', 1)\n            if not self.with_modulated_dcn:\n                conv_op = DeformConv\n                offset_channels = 18\n            else:\n                conv_op = ModulatedDeformConv\n                offset_channels = 27\n            self.conv2_offset = nn.Conv2d(\n                planes,\n                deformable_groups * offset_channels,\n                kernel_size=3,\n                stride=self.conv2_stride,\n                padding=dilation,\n                dilation=dilation)\n            self.conv2 = conv_op(\n                planes,\n                planes,\n                kernel_size=3,\n                stride=self.conv2_stride,\n                padding=dilation,\n                dilation=dilation,\n                deformable_groups=deformable_groups,\n                bias=False)\n        self.add_module(self.norm2_name, norm2)\n        self.conv3 = nn.Conv2d(\n            planes, planes * self.expansion, kernel_size=1, bias=False)\n        self.add_module(self.norm3_name, norm3)\n\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = downsample\n        self.stride = stride\n        self.dilation = dilation\n        self.with_cp = with_cp\n        self.normalize = normalize\n\n    @property\n    def norm1(self):\n        return getattr(self, self.norm1_name)\n\n    @property\n    def norm2(self):\n        return getattr(self, self.norm2_name)\n\n    @property\n    def norm3(self):\n        return getattr(self, self.norm3_name)\n\n    def forward(self, x):\n\n        def _inner_forward(x):\n            identity = x\n\n            out = self.conv1(x)\n            out = self.norm1(out)\n            out = self.relu(out)\n\n            if not self.with_dcn:\n                out = self.conv2(out)\n            elif self.with_modulated_dcn:\n                offset_mask = self.conv2_offset(out)\n                offset = offset_mask[:, :18, :, :]\n                mask = offset_mask[:, -9:, :, :].sigmoid()\n                out = self.conv2(out, offset, mask)\n            else:\n                offset = self.conv2_offset(out)\n                out = self.conv2(out, offset)\n            out = self.norm2(out)\n            out = self.relu(out)\n\n            out = self.conv3(out)\n            out = self.norm3(out)\n\n            if self.downsample is not None:\n                identity = self.downsample(x)\n\n            out += identity\n\n            return out\n\n        if self.with_cp and x.requires_grad:\n            out = cp.checkpoint(_inner_forward, x)\n        else:\n            out = _inner_forward(x)\n\n        out = self.relu(out)\n\n        return out\n\n\ndef make_res_layer(block,\n                   inplanes,\n                   planes,\n                   blocks,\n                   stride=1,\n                   dilation=1,\n                   style=\'pytorch\',\n                   with_cp=False,\n                   normalize=dict(type=\'BN\'),\n                   dcn=None):\n    downsample = None\n    if stride != 1 or inplanes != planes * block.expansion:\n        downsample = nn.Sequential(\n            nn.Conv2d(\n                inplanes,\n                planes * block.expansion,\n                kernel_size=1,\n                stride=stride,\n                bias=False),\n            build_norm_layer(normalize, planes * block.expansion)[1],\n        )\n\n    layers = []\n    layers.append(\n        block(\n            inplanes,\n            planes,\n            stride,\n            dilation,\n            downsample,\n            style=style,\n            with_cp=with_cp,\n            normalize=normalize,\n            dcn=dcn))\n    inplanes = planes * block.expansion\n    for i in range(1, blocks):\n        layers.append(\n            block(\n                inplanes,\n                planes,\n                1,\n                dilation,\n                style=style,\n                with_cp=with_cp,\n                normalize=normalize,\n                dcn=dcn))\n\n    return nn.Sequential(*layers)\n\n\n@BACKBONES.register_module\nclass ResNet(nn.Module):\n    """"""ResNet backbone.\n\n    Args:\n        depth (int): Depth of resnet, from {18, 34, 50, 101, 152}.\n        num_stages (int): Resnet stages, normally 4.\n        strides (Sequence[int]): Strides of the first block of each stage.\n        dilations (Sequence[int]): Dilation of each stage.\n        out_indices (Sequence[int]): Output from which stages.\n        style (str): `pytorch` or `caffe`. If set to ""pytorch"", the stride-two\n            layer is the 3x3 conv layer, otherwise the stride-two layer is\n            the first 1x1 conv layer.\n        frozen_stages (int): Stages to be frozen (all param fixed). -1 means\n            not freezing any parameters.\n        normalize (dict): dictionary to construct and config norm layer.\n        norm_eval (bool): Whether to set norm layers to eval mode, namely,\n            freeze running stats (mean and var). Note: Effect on Batch Norm\n            and its variants only.\n        with_cp (bool): Use checkpoint or not. Using checkpoint will save some\n            memory while slowing down the training speed.\n        zero_init_residual (bool): whether to use zero init for last norm layer\n            in resblocks to let them behave as identity.\n    """"""\n\n    arch_settings = {\n        18: (BasicBlock, (2, 2, 2, 2)),\n        34: (BasicBlock, (3, 4, 6, 3)),\n        50: (Bottleneck, (3, 4, 6, 3)),\n        101: (Bottleneck, (3, 4, 23, 3)),\n        152: (Bottleneck, (3, 8, 36, 3))\n    }\n\n    def __init__(self,\n                 depth,\n                 num_stages=4,\n                 strides=(1, 2, 2, 2),\n                 dilations=(1, 1, 1, 1),\n                 out_indices=(0, 1, 2, 3),\n                 style=\'pytorch\',\n                 frozen_stages=-1,\n                 normalize=dict(type=\'BN\', frozen=False),\n                 norm_eval=True,\n                 dcn=None,\n                 stage_with_dcn=(False, False, False, False),\n                 with_cp=False,\n                 zero_init_residual=True):\n        super(ResNet, self).__init__()\n        if depth not in self.arch_settings:\n            raise KeyError(\'invalid depth {} for resnet\'.format(depth))\n        self.depth = depth\n        self.num_stages = num_stages\n        assert num_stages >= 1 and num_stages <= 4\n        self.strides = strides\n        self.dilations = dilations\n        assert len(strides) == len(dilations) == num_stages\n        self.out_indices = out_indices\n        assert max(out_indices) < num_stages\n        self.style = style\n        self.frozen_stages = frozen_stages\n        self.normalize = normalize\n        self.with_cp = with_cp\n        self.norm_eval = norm_eval\n        self.dcn = dcn\n        self.stage_with_dcn = stage_with_dcn\n        if dcn is not None:\n            assert len(stage_with_dcn) == num_stages\n        self.zero_init_residual = zero_init_residual\n        self.block, stage_blocks = self.arch_settings[depth]\n        self.stage_blocks = stage_blocks[:num_stages]\n        self.inplanes = 64\n\n        self._make_stem_layer()\n\n        self.res_layers = []\n        for i, num_blocks in enumerate(self.stage_blocks):\n            stride = strides[i]\n            dilation = dilations[i]\n            dcn = self.dcn if self.stage_with_dcn[i] else None\n            planes = 64 * 2**i\n            res_layer = make_res_layer(\n                self.block,\n                self.inplanes,\n                planes,\n                num_blocks,\n                stride=stride,\n                dilation=dilation,\n                style=self.style,\n                with_cp=with_cp,\n                normalize=normalize,\n                dcn=dcn)\n            self.inplanes = planes * self.block.expansion\n            layer_name = \'layer{}\'.format(i + 1)\n            self.add_module(layer_name, res_layer)\n            self.res_layers.append(layer_name)\n\n        self._freeze_stages()\n\n        self.feat_dim = self.block.expansion * 64 * 2**(\n            len(self.stage_blocks) - 1)\n\n    @property\n    def norm1(self):\n        return getattr(self, self.norm1_name)\n\n    def _make_stem_layer(self):\n        self.conv1 = nn.Conv2d(\n            3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n        self.norm1_name, norm1 = build_norm_layer(\n            self.normalize, 64, postfix=1)\n        self.add_module(self.norm1_name, norm1)\n        self.relu = nn.ReLU(inplace=True)\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n\n    def _freeze_stages(self):\n        if self.frozen_stages >= 0:\n            for m in [self.conv1, self.norm1]:\n                for param in m.parameters():\n                    param.requires_grad = False\n\n        for i in range(1, self.frozen_stages + 1):\n            m = getattr(self, \'layer{}\'.format(i))\n            for param in m.parameters():\n                param.requires_grad = False\n\n    def init_weights(self, pretrained=None):\n        if isinstance(pretrained, str):\n            logger = logging.getLogger()\n            load_checkpoint(self, pretrained, strict=False, logger=logger, map_location=torch.device(\'cpu\'))\n        elif pretrained is None:\n            for m in self.modules():\n                if isinstance(m, nn.Conv2d):\n                    kaiming_init(m)\n                elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n                    constant_init(m, 1)\n\n            if self.dcn is not None:\n                for m in self.modules():\n                    if isinstance(m, Bottleneck) and hasattr(\n                            m, \'conv2_offset\'):\n                        constant_init(m.conv2_offset, 0)\n\n            if self.zero_init_residual:\n                for m in self.modules():\n                    if isinstance(m, Bottleneck):\n                        constant_init(m.norm3, 0)\n                    elif isinstance(m, BasicBlock):\n                        constant_init(m.norm2, 0)\n        else:\n            raise TypeError(\'pretrained must be a str or None\')\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.norm1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n        outs = []\n        for i, layer_name in enumerate(self.res_layers):\n            res_layer = getattr(self, layer_name)\n            x = res_layer(x)\n            if i in self.out_indices:\n                outs.append(x)\n        if len(outs) == 1:\n            return outs[0]\n        else:\n            return tuple(outs)\n\n    def train(self, mode=True):\n        super(ResNet, self).train(mode)\n        if mode and self.norm_eval:\n            for m in self.modules():\n                # trick: eval have effect on BatchNorm only\n                if isinstance(m, nn.BatchNorm2d):\n                    m.eval()\n'"
detection/mmdet/models/backbones/resnet_bam.py,3,"b'import logging\n\nimport torch.nn as nn\nimport torch.utils.checkpoint as cp\n\nfrom mmcv.cnn import constant_init, kaiming_init\nfrom mmcv.runner import load_checkpoint\n\nfrom mmdet.ops import DeformConv, ModulatedDeformConv\nfrom ..registry import BACKBONES\nfrom ..utils import build_norm_layer\nfrom .bam import *\n\n\ndef conv3x3(in_planes, out_planes, stride=1, dilation=1):\n    ""3x3 convolution with padding""\n    return nn.Conv2d(\n        in_planes,\n        out_planes,\n        kernel_size=3,\n        stride=stride,\n        padding=dilation,\n        dilation=dilation,\n        bias=False)\n\n\nclass BasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self,\n                 inplanes,\n                 planes,\n                 stride=1,\n                 dilation=1,\n                 downsample=None,\n                 style=\'pytorch\',\n                 with_cp=False,\n                 normalize=dict(type=\'BN\'),\n                 dcn=None):\n        super(BasicBlock, self).__init__()\n        assert dcn is None, ""Not implemented yet.""\n\n        self.norm1_name, norm1 = build_norm_layer(normalize, planes, postfix=1)\n        self.norm2_name, norm2 = build_norm_layer(normalize, planes, postfix=2)\n\n        self.conv1 = conv3x3(inplanes, planes, stride, dilation)\n        self.add_module(self.norm1_name, norm1)\n        self.conv2 = conv3x3(planes, planes)\n        self.add_module(self.norm2_name, norm2)\n\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = downsample\n        self.stride = stride\n        self.dilation = dilation\n        assert not with_cp\n\n    @property\n    def norm1(self):\n        return getattr(self, self.norm1_name)\n\n    @property\n    def norm2(self):\n        return getattr(self, self.norm2_name)\n\n    def forward(self, x):\n        identity = x\n\n        out = self.conv1(x)\n        out = self.norm1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.norm2(out)\n\n        if self.downsample is not None:\n            identity = self.downsample(x)\n\n        out += identity\n        out = self.relu(out)\n\n        return out\n\n\nclass Bottleneck(nn.Module):\n    expansion = 4\n\n    def __init__(self,\n                 inplanes,\n                 planes,\n                 stride=1,\n                 dilation=1,\n                 downsample=None,\n                 style=\'pytorch\',\n                 with_cp=False,\n                 normalize=dict(type=\'BN\'),\n                 dcn=None):\n        """"""Bottleneck block for ResNetBAM.\n        If style is ""pytorch"", the stride-two layer is the 3x3 conv layer,\n        if it is ""caffe"", the stride-two layer is the first 1x1 conv layer.\n        """"""\n        super(Bottleneck, self).__init__()\n        assert style in [\'pytorch\', \'caffe\']\n        assert dcn is None or isinstance(dcn, dict)\n        self.inplanes = inplanes\n        self.planes = planes\n        self.normalize = normalize\n        self.dcn = dcn\n        self.with_dcn = dcn is not None\n        if style == \'pytorch\':\n            self.conv1_stride = 1\n            self.conv2_stride = stride\n        else:\n            self.conv1_stride = stride\n            self.conv2_stride = 1\n\n        self.norm1_name, norm1 = build_norm_layer(normalize, planes, postfix=1)\n        self.norm2_name, norm2 = build_norm_layer(normalize, planes, postfix=2)\n        self.norm3_name, norm3 = build_norm_layer(\n            normalize, planes * self.expansion, postfix=3)\n\n        self.conv1 = nn.Conv2d(\n            inplanes,\n            planes,\n            kernel_size=1,\n            stride=self.conv1_stride,\n            bias=False)\n        self.add_module(self.norm1_name, norm1)\n        fallback_on_stride = False\n        self.with_modulated_dcn = False\n        if self.with_dcn:\n            fallback_on_stride = dcn.get(\'fallback_on_stride\', False)\n            self.with_modulated_dcn = dcn.get(\'modulated\', False)\n        if not self.with_dcn or fallback_on_stride:\n            self.conv2 = nn.Conv2d(\n                planes,\n                planes,\n                kernel_size=3,\n                stride=self.conv2_stride,\n                padding=dilation,\n                dilation=dilation,\n                bias=False)\n        else:\n            deformable_groups = dcn.get(\'deformable_groups\', 1)\n            if not self.with_modulated_dcn:\n                conv_op = DeformConv\n                offset_channels = 18\n            else:\n                conv_op = ModulatedDeformConv\n                offset_channels = 27\n            self.conv2_offset = nn.Conv2d(\n                planes,\n                deformable_groups * offset_channels,\n                kernel_size=3,\n                stride=self.conv2_stride,\n                padding=dilation,\n                dilation=dilation)\n            self.conv2 = conv_op(\n                planes,\n                planes,\n                kernel_size=3,\n                stride=self.conv2_stride,\n                padding=dilation,\n                dilation=dilation,\n                deformable_groups=deformable_groups,\n                bias=False)\n        self.add_module(self.norm2_name, norm2)\n        self.conv3 = nn.Conv2d(\n            planes, planes * self.expansion, kernel_size=1, bias=False)\n        self.add_module(self.norm3_name, norm3)\n\n        #self.se  = SELayer(planes * self.expansion)\n        #self.cbam = CBAM( planes * 4, 16 )\n\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = downsample\n        self.stride = stride\n        self.dilation = dilation\n        self.with_cp = with_cp\n        self.normalize = normalize\n\n    @property\n    def norm1(self):\n        return getattr(self, self.norm1_name)\n\n    @property\n    def norm2(self):\n        return getattr(self, self.norm2_name)\n\n    @property\n    def norm3(self):\n        return getattr(self, self.norm3_name)\n\n    def forward(self, x):\n\n        def _inner_forward(x):\n            identity = x\n\n            out = self.conv1(x)\n            out = self.norm1(out)\n            out = self.relu(out)\n\n            if not self.with_dcn:\n                out = self.conv2(out)\n            elif self.with_modulated_dcn:\n                offset_mask = self.conv2_offset(out)\n                offset = offset_mask[:, :18, :, :]\n                mask = offset_mask[:, -9:, :, :].sigmoid()\n                out = self.conv2(out, offset, mask)\n            else:\n                offset = self.conv2_offset(out)\n                out = self.conv2(out, offset)\n            out = self.norm2(out)\n            out = self.relu(out)\n\n            out = self.conv3(out)\n            out = self.norm3(out)\n\n            #out = self.cbam(out)\n\n            if self.downsample is not None:\n                identity = self.downsample(x)\n\n            out += identity\n\n            return out\n\n        if self.with_cp and x.requires_grad:\n            out = cp.checkpoint(_inner_forward, x)\n        else:\n            out = _inner_forward(x)\n\n        out = self.relu(out)\n\n        return out\n\n\ndef make_res_layer(block,\n                   inplanes,\n                   planes,\n                   blocks,\n                   stride=1,\n                   dilation=1,\n                   style=\'pytorch\',\n                   with_cp=False,\n                   normalize=dict(type=\'BN\'),\n                   dcn=None):\n    downsample = None\n    if stride != 1 or inplanes != planes * block.expansion:\n        downsample = nn.Sequential(\n            nn.Conv2d(\n                inplanes,\n                planes * block.expansion,\n                kernel_size=1,\n                stride=stride,\n                bias=False),\n            build_norm_layer(normalize, planes * block.expansion)[1],\n        )\n\n    layers = []\n    layers.append(\n        block(\n            inplanes,\n            planes,\n            stride,\n            dilation,\n            downsample,\n            style=style,\n            with_cp=with_cp,\n            normalize=normalize,\n            dcn=dcn))\n    inplanes = planes * block.expansion\n    for i in range(1, blocks):\n        layers.append(\n            block(\n                inplanes,\n                planes,\n                1,\n                dilation,\n                style=style,\n                with_cp=with_cp,\n                normalize=normalize,\n                dcn=dcn))\n\n    return nn.Sequential(*layers)\n\n\n@BACKBONES.register_module\nclass ResNetBAM(nn.Module):\n    """"""ResNetBAM backbone.\n\n    Args:\n        depth (int): Depth of resnet, from {18, 34, 50, 101, 152}.\n        num_stages (int): Resnet stages, normally 4.\n        strides (Sequence[int]): Strides of the first block of each stage.\n        dilations (Sequence[int]): Dilation of each stage.\n        out_indices (Sequence[int]): Output from which stages.\n        style (str): `pytorch` or `caffe`. If set to ""pytorch"", the stride-two\n            layer is the 3x3 conv layer, otherwise the stride-two layer is\n            the first 1x1 conv layer.\n        frozen_stages (int): Stages to be frozen (all param fixed). -1 means\n            not freezing any parameters.\n        normalize (dict): dictionary to construct and config norm layer.\n        norm_eval (bool): Whether to set norm layers to eval mode, namely,\n            freeze running stats (mean and var). Note: Effect on Batch Norm\n            and its variants only.\n        with_cp (bool): Use checkpoint or not. Using checkpoint will save some\n            memory while slowing down the training speed.\n        zero_init_residual (bool): whether to use zero init for last norm layer\n            in resblocks to let them behave as identity.\n    """"""\n\n    arch_settings = {\n        18: (BasicBlock, (2, 2, 2, 2)),\n        34: (BasicBlock, (3, 4, 6, 3)),\n        50: (Bottleneck, (3, 4, 6, 3)),\n        101: (Bottleneck, (3, 4, 23, 3)),\n        152: (Bottleneck, (3, 8, 36, 3))\n    }\n\n    def __init__(self,\n                 depth,\n                 num_stages=4,\n                 strides=(1, 2, 2, 2),\n                 dilations=(1, 1, 1, 1),\n                 out_indices=(0, 1, 2, 3),\n                 style=\'pytorch\',\n                 frozen_stages=-1,\n                 normalize=dict(type=\'BN\', frozen=False),\n                 norm_eval=True,\n                 dcn=None,\n                 stage_with_dcn=(False, False, False, False),\n                 with_cp=False,\n                 zero_init_residual=True):\n        super(ResNetBAM, self).__init__()\n        if depth not in self.arch_settings:\n            raise KeyError(\'invalid depth {} for resnet\'.format(depth))\n        self.depth = depth\n        self.num_stages = num_stages\n        assert num_stages >= 1 and num_stages <= 4\n        self.strides = strides\n        self.dilations = dilations\n        assert len(strides) == len(dilations) == num_stages\n        self.out_indices = out_indices\n        assert max(out_indices) < num_stages\n        self.style = style\n        self.frozen_stages = frozen_stages\n        self.normalize = normalize\n        self.with_cp = with_cp\n        self.norm_eval = norm_eval\n        self.dcn = dcn\n        self.stage_with_dcn = stage_with_dcn\n        if dcn is not None:\n            assert len(stage_with_dcn) == num_stages\n        self.zero_init_residual = zero_init_residual\n        self.block, stage_blocks = self.arch_settings[depth]\n        self.stage_blocks = stage_blocks[:num_stages]\n        self.inplanes = 64\n\n        self._make_stem_layer()\n\n        self.res_layers = []\n        for i, num_blocks in enumerate(self.stage_blocks):\n            stride = strides[i]\n            dilation = dilations[i]\n            dcn = self.dcn if self.stage_with_dcn[i] else None\n            planes = 64 * 2**i\n            res_layer = make_res_layer(\n                self.block,\n                self.inplanes,\n                planes,\n                num_blocks,\n                stride=stride,\n                dilation=dilation,\n                style=self.style,\n                with_cp=with_cp,\n                normalize=normalize,\n                dcn=dcn)\n            self.inplanes = planes * self.block.expansion\n            layer_name = \'layer{}\'.format(i + 1)\n            self.add_module(layer_name, res_layer)\n            self.res_layers.append(layer_name)\n\n        self.bam1 = BAM(64*self.block.expansion)\n        self.bam2 = BAM(128*self.block.expansion)\n        self.bam3 = BAM(256*self.block.expansion)\n\n        self._freeze_stages()\n\n        self.feat_dim = self.block.expansion * 64 * 2**(\n            len(self.stage_blocks) - 1)\n\n    @property\n    def norm1(self):\n        return getattr(self, self.norm1_name)\n\n    def _make_stem_layer(self):\n        self.conv1 = nn.Conv2d(\n            3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n        self.norm1_name, norm1 = build_norm_layer(\n            self.normalize, 64, postfix=1)\n        self.add_module(self.norm1_name, norm1)\n        self.relu = nn.ReLU(inplace=True)\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n\n    def _freeze_stages(self):\n        if self.frozen_stages >= 0:\n            for m in [self.conv1, self.norm1]:\n                for param in m.parameters():\n                    param.requires_grad = False\n\n        for i in range(1, self.frozen_stages + 1):\n            m = getattr(self, \'layer{}\'.format(i))\n            for param in m.parameters():\n                param.requires_grad = False\n\n    def init_weights(self, pretrained=None):\n        if isinstance(pretrained, str):\n            logger = logging.getLogger()\n            load_checkpoint(self, pretrained, strict=False, logger=logger, map_location=torch.device(\'cpu\'))\n        elif pretrained is None:\n            for m in self.modules():\n                if isinstance(m, nn.Conv2d):\n                    kaiming_init(m)\n                elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n                    constant_init(m, 1)\n\n            if self.dcn is not None:\n                for m in self.modules():\n                    if isinstance(m, Bottleneck) and hasattr(\n                            m, \'conv2_offset\'):\n                        constant_init(m.conv2_offset, 0)\n\n            if self.zero_init_residual:\n                for m in self.modules():\n                    if isinstance(m, Bottleneck):\n                        constant_init(m.norm3, 0)\n                    elif isinstance(m, BasicBlock):\n                        constant_init(m.norm2, 0)\n        else:\n            raise TypeError(\'pretrained must be a str or None\')\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.norm1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n        outs = []\n        for i, layer_name in enumerate(self.res_layers):\n            res_layer = getattr(self, layer_name)\n            x = res_layer(x)\n            #print(\'layer\', layer_name, \'x size = \', x.size())\n            if hasattr(self, \'bam%d\' % (i + 1)):\n                x = getattr(self, \'bam%d\' % (i + 1))(x) \n                #print(\'after bam, x size = \', x.size())\n            if i in self.out_indices:\n                outs.append(x)\n        if len(outs) == 1:\n            return outs[0]\n        else:\n            return tuple(outs)\n\n    def train(self, mode=True):\n        super(ResNetBAM, self).train(mode)\n        if mode and self.norm_eval:\n            for m in self.modules():\n                # trick: eval have effect on BatchNorm only\n                if isinstance(m, nn.BatchNorm2d) or isinstance(m, nn.BatchNorm1d):\n                    m.eval()\n'"
detection/mmdet/models/backbones/resnet_cbam.py,3,"b'import logging\n\nimport torch.nn as nn\nimport torch.utils.checkpoint as cp\n\nfrom mmcv.cnn import constant_init, kaiming_init\nfrom mmcv.runner import load_checkpoint\n\nfrom mmdet.ops import DeformConv, ModulatedDeformConv\nfrom ..registry import BACKBONES\nfrom ..utils import build_norm_layer\nfrom .cbam import *\n\n\ndef conv3x3(in_planes, out_planes, stride=1, dilation=1):\n    ""3x3 convolution with padding""\n    return nn.Conv2d(\n        in_planes,\n        out_planes,\n        kernel_size=3,\n        stride=stride,\n        padding=dilation,\n        dilation=dilation,\n        bias=False)\n\n\nclass BasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self,\n                 inplanes,\n                 planes,\n                 stride=1,\n                 dilation=1,\n                 downsample=None,\n                 style=\'pytorch\',\n                 with_cp=False,\n                 normalize=dict(type=\'BN\'),\n                 dcn=None):\n        super(BasicBlock, self).__init__()\n        assert dcn is None, ""Not implemented yet.""\n\n        self.norm1_name, norm1 = build_norm_layer(normalize, planes, postfix=1)\n        self.norm2_name, norm2 = build_norm_layer(normalize, planes, postfix=2)\n\n        self.conv1 = conv3x3(inplanes, planes, stride, dilation)\n        self.add_module(self.norm1_name, norm1)\n        self.conv2 = conv3x3(planes, planes)\n        self.add_module(self.norm2_name, norm2)\n\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = downsample\n        self.stride = stride\n        self.dilation = dilation\n        assert not with_cp\n\n    @property\n    def norm1(self):\n        return getattr(self, self.norm1_name)\n\n    @property\n    def norm2(self):\n        return getattr(self, self.norm2_name)\n\n    def forward(self, x):\n        identity = x\n\n        out = self.conv1(x)\n        out = self.norm1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.norm2(out)\n\n        if self.downsample is not None:\n            identity = self.downsample(x)\n\n        out += identity\n        out = self.relu(out)\n\n        return out\n\n\nclass Bottleneck(nn.Module):\n    expansion = 4\n\n    def __init__(self,\n                 inplanes,\n                 planes,\n                 stride=1,\n                 dilation=1,\n                 downsample=None,\n                 style=\'pytorch\',\n                 with_cp=False,\n                 normalize=dict(type=\'BN\'),\n                 dcn=None):\n        """"""Bottleneck block for ResNetCBAM.\n        If style is ""pytorch"", the stride-two layer is the 3x3 conv layer,\n        if it is ""caffe"", the stride-two layer is the first 1x1 conv layer.\n        """"""\n        super(Bottleneck, self).__init__()\n        assert style in [\'pytorch\', \'caffe\']\n        assert dcn is None or isinstance(dcn, dict)\n        self.inplanes = inplanes\n        self.planes = planes\n        self.normalize = normalize\n        self.dcn = dcn\n        self.with_dcn = dcn is not None\n        if style == \'pytorch\':\n            self.conv1_stride = 1\n            self.conv2_stride = stride\n        else:\n            self.conv1_stride = stride\n            self.conv2_stride = 1\n\n        self.norm1_name, norm1 = build_norm_layer(normalize, planes, postfix=1)\n        self.norm2_name, norm2 = build_norm_layer(normalize, planes, postfix=2)\n        self.norm3_name, norm3 = build_norm_layer(\n            normalize, planes * self.expansion, postfix=3)\n\n        self.conv1 = nn.Conv2d(\n            inplanes,\n            planes,\n            kernel_size=1,\n            stride=self.conv1_stride,\n            bias=False)\n        self.add_module(self.norm1_name, norm1)\n        fallback_on_stride = False\n        self.with_modulated_dcn = False\n        if self.with_dcn:\n            fallback_on_stride = dcn.get(\'fallback_on_stride\', False)\n            self.with_modulated_dcn = dcn.get(\'modulated\', False)\n        if not self.with_dcn or fallback_on_stride:\n            self.conv2 = nn.Conv2d(\n                planes,\n                planes,\n                kernel_size=3,\n                stride=self.conv2_stride,\n                padding=dilation,\n                dilation=dilation,\n                bias=False)\n        else:\n            deformable_groups = dcn.get(\'deformable_groups\', 1)\n            if not self.with_modulated_dcn:\n                conv_op = DeformConv\n                offset_channels = 18\n            else:\n                conv_op = ModulatedDeformConv\n                offset_channels = 27\n            self.conv2_offset = nn.Conv2d(\n                planes,\n                deformable_groups * offset_channels,\n                kernel_size=3,\n                stride=self.conv2_stride,\n                padding=dilation,\n                dilation=dilation)\n            self.conv2 = conv_op(\n                planes,\n                planes,\n                kernel_size=3,\n                stride=self.conv2_stride,\n                padding=dilation,\n                dilation=dilation,\n                deformable_groups=deformable_groups,\n                bias=False)\n        self.add_module(self.norm2_name, norm2)\n        self.conv3 = nn.Conv2d(\n            planes, planes * self.expansion, kernel_size=1, bias=False)\n        self.add_module(self.norm3_name, norm3)\n\n        #self.se  = SELayer(planes * self.expansion)\n        self.cbam = CBAM( planes * 4, 16 )\n\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = downsample\n        self.stride = stride\n        self.dilation = dilation\n        self.with_cp = with_cp\n        self.normalize = normalize\n\n    @property\n    def norm1(self):\n        return getattr(self, self.norm1_name)\n\n    @property\n    def norm2(self):\n        return getattr(self, self.norm2_name)\n\n    @property\n    def norm3(self):\n        return getattr(self, self.norm3_name)\n\n    def forward(self, x):\n\n        def _inner_forward(x):\n            identity = x\n\n            out = self.conv1(x)\n            out = self.norm1(out)\n            out = self.relu(out)\n\n            if not self.with_dcn:\n                out = self.conv2(out)\n            elif self.with_modulated_dcn:\n                offset_mask = self.conv2_offset(out)\n                offset = offset_mask[:, :18, :, :]\n                mask = offset_mask[:, -9:, :, :].sigmoid()\n                out = self.conv2(out, offset, mask)\n            else:\n                offset = self.conv2_offset(out)\n                out = self.conv2(out, offset)\n            out = self.norm2(out)\n            out = self.relu(out)\n\n            out = self.conv3(out)\n            out = self.norm3(out)\n\n            out = self.cbam(out)\n\n            if self.downsample is not None:\n                identity = self.downsample(x)\n\n            out += identity\n\n            return out\n\n        if self.with_cp and x.requires_grad:\n            out = cp.checkpoint(_inner_forward, x)\n        else:\n            out = _inner_forward(x)\n\n        out = self.relu(out)\n\n        return out\n\n\ndef make_res_layer(block,\n                   inplanes,\n                   planes,\n                   blocks,\n                   stride=1,\n                   dilation=1,\n                   style=\'pytorch\',\n                   with_cp=False,\n                   normalize=dict(type=\'BN\'),\n                   dcn=None):\n    downsample = None\n    if stride != 1 or inplanes != planes * block.expansion:\n        downsample = nn.Sequential(\n            nn.Conv2d(\n                inplanes,\n                planes * block.expansion,\n                kernel_size=1,\n                stride=stride,\n                bias=False),\n            build_norm_layer(normalize, planes * block.expansion)[1],\n        )\n\n    layers = []\n    layers.append(\n        block(\n            inplanes,\n            planes,\n            stride,\n            dilation,\n            downsample,\n            style=style,\n            with_cp=with_cp,\n            normalize=normalize,\n            dcn=dcn))\n    inplanes = planes * block.expansion\n    for i in range(1, blocks):\n        layers.append(\n            block(\n                inplanes,\n                planes,\n                1,\n                dilation,\n                style=style,\n                with_cp=with_cp,\n                normalize=normalize,\n                dcn=dcn))\n\n    return nn.Sequential(*layers)\n\n\n@BACKBONES.register_module\nclass ResNetCBAM(nn.Module):\n    """"""ResNetCBAM backbone.\n\n    Args:\n        depth (int): Depth of resnet, from {18, 34, 50, 101, 152}.\n        num_stages (int): Resnet stages, normally 4.\n        strides (Sequence[int]): Strides of the first block of each stage.\n        dilations (Sequence[int]): Dilation of each stage.\n        out_indices (Sequence[int]): Output from which stages.\n        style (str): `pytorch` or `caffe`. If set to ""pytorch"", the stride-two\n            layer is the 3x3 conv layer, otherwise the stride-two layer is\n            the first 1x1 conv layer.\n        frozen_stages (int): Stages to be frozen (all param fixed). -1 means\n            not freezing any parameters.\n        normalize (dict): dictionary to construct and config norm layer.\n        norm_eval (bool): Whether to set norm layers to eval mode, namely,\n            freeze running stats (mean and var). Note: Effect on Batch Norm\n            and its variants only.\n        with_cp (bool): Use checkpoint or not. Using checkpoint will save some\n            memory while slowing down the training speed.\n        zero_init_residual (bool): whether to use zero init for last norm layer\n            in resblocks to let them behave as identity.\n    """"""\n\n    arch_settings = {\n        18: (BasicBlock, (2, 2, 2, 2)),\n        34: (BasicBlock, (3, 4, 6, 3)),\n        50: (Bottleneck, (3, 4, 6, 3)),\n        101: (Bottleneck, (3, 4, 23, 3)),\n        152: (Bottleneck, (3, 8, 36, 3))\n    }\n\n    def __init__(self,\n                 depth,\n                 num_stages=4,\n                 strides=(1, 2, 2, 2),\n                 dilations=(1, 1, 1, 1),\n                 out_indices=(0, 1, 2, 3),\n                 style=\'pytorch\',\n                 frozen_stages=-1,\n                 normalize=dict(type=\'BN\', frozen=False),\n                 norm_eval=True,\n                 dcn=None,\n                 stage_with_dcn=(False, False, False, False),\n                 with_cp=False,\n                 zero_init_residual=True):\n        super(ResNetCBAM, self).__init__()\n        if depth not in self.arch_settings:\n            raise KeyError(\'invalid depth {} for resnet\'.format(depth))\n        self.depth = depth\n        self.num_stages = num_stages\n        assert num_stages >= 1 and num_stages <= 4\n        self.strides = strides\n        self.dilations = dilations\n        assert len(strides) == len(dilations) == num_stages\n        self.out_indices = out_indices\n        assert max(out_indices) < num_stages\n        self.style = style\n        self.frozen_stages = frozen_stages\n        self.normalize = normalize\n        self.with_cp = with_cp\n        self.norm_eval = norm_eval\n        self.dcn = dcn\n        self.stage_with_dcn = stage_with_dcn\n        if dcn is not None:\n            assert len(stage_with_dcn) == num_stages\n        self.zero_init_residual = zero_init_residual\n        self.block, stage_blocks = self.arch_settings[depth]\n        self.stage_blocks = stage_blocks[:num_stages]\n        self.inplanes = 64\n\n        self._make_stem_layer()\n\n        self.res_layers = []\n        for i, num_blocks in enumerate(self.stage_blocks):\n            stride = strides[i]\n            dilation = dilations[i]\n            dcn = self.dcn if self.stage_with_dcn[i] else None\n            planes = 64 * 2**i\n            res_layer = make_res_layer(\n                self.block,\n                self.inplanes,\n                planes,\n                num_blocks,\n                stride=stride,\n                dilation=dilation,\n                style=self.style,\n                with_cp=with_cp,\n                normalize=normalize,\n                dcn=dcn)\n            self.inplanes = planes * self.block.expansion\n            layer_name = \'layer{}\'.format(i + 1)\n            self.add_module(layer_name, res_layer)\n            self.res_layers.append(layer_name)\n\n        self._freeze_stages()\n\n        self.feat_dim = self.block.expansion * 64 * 2**(\n            len(self.stage_blocks) - 1)\n\n    @property\n    def norm1(self):\n        return getattr(self, self.norm1_name)\n\n    def _make_stem_layer(self):\n        self.conv1 = nn.Conv2d(\n            3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n        self.norm1_name, norm1 = build_norm_layer(\n            self.normalize, 64, postfix=1)\n        self.add_module(self.norm1_name, norm1)\n        self.relu = nn.ReLU(inplace=True)\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n\n    def _freeze_stages(self):\n        if self.frozen_stages >= 0:\n            for m in [self.conv1, self.norm1]:\n                for param in m.parameters():\n                    param.requires_grad = False\n\n        for i in range(1, self.frozen_stages + 1):\n            m = getattr(self, \'layer{}\'.format(i))\n            for param in m.parameters():\n                param.requires_grad = False\n\n    def init_weights(self, pretrained=None):\n        if isinstance(pretrained, str):\n            logger = logging.getLogger()\n            load_checkpoint(self, pretrained, strict=False, logger=logger, map_location=torch.device(\'cpu\'))\n        elif pretrained is None:\n            for m in self.modules():\n                if isinstance(m, nn.Conv2d):\n                    kaiming_init(m)\n                elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n                    constant_init(m, 1)\n\n            if self.dcn is not None:\n                for m in self.modules():\n                    if isinstance(m, Bottleneck) and hasattr(\n                            m, \'conv2_offset\'):\n                        constant_init(m.conv2_offset, 0)\n\n            if self.zero_init_residual:\n                for m in self.modules():\n                    if isinstance(m, Bottleneck):\n                        constant_init(m.norm3, 0)\n                    elif isinstance(m, BasicBlock):\n                        constant_init(m.norm2, 0)\n        else:\n            raise TypeError(\'pretrained must be a str or None\')\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.norm1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n        outs = []\n        for i, layer_name in enumerate(self.res_layers):\n            res_layer = getattr(self, layer_name)\n            x = res_layer(x)\n            if i in self.out_indices:\n                outs.append(x)\n        if len(outs) == 1:\n            return outs[0]\n        else:\n            return tuple(outs)\n\n    def train(self, mode=True):\n        super(ResNetCBAM, self).train(mode)\n        if mode and self.norm_eval:\n            for m in self.modules():\n                # trick: eval have effect on BatchNorm only\n                if isinstance(m, nn.BatchNorm2d):\n                    m.eval()\n'"
detection/mmdet/models/backbones/resnet_gc.py,3,"b'import logging\n\nimport torch.nn as nn\nimport torch.utils.checkpoint as cp\n\nfrom mmcv.cnn import constant_init, kaiming_init\nfrom mmcv.runner import load_checkpoint\n\nfrom mmdet.ops import DeformConv, ModulatedDeformConv\nfrom ..registry import BACKBONES\nfrom ..utils import build_norm_layer\nfrom .global_context import *\n\ndef conv3x3(in_planes, out_planes, stride=1, dilation=1):\n    ""3x3 convolution with padding""\n    return nn.Conv2d(\n        in_planes,\n        out_planes,\n        kernel_size=3,\n        stride=stride,\n        padding=dilation,\n        dilation=dilation,\n        bias=False)\n\n\nclass BasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self,\n                 inplanes,\n                 planes,\n                 stride=1,\n                 dilation=1,\n                 downsample=None,\n                 style=\'pytorch\',\n                 with_cp=False,\n                 normalize=dict(type=\'BN\'),\n                 dcn=None):\n        super(BasicBlock, self).__init__()\n        assert dcn is None, ""Not implemented yet.""\n\n        self.norm1_name, norm1 = build_norm_layer(normalize, planes, postfix=1)\n        self.norm2_name, norm2 = build_norm_layer(normalize, planes, postfix=2)\n\n        self.conv1 = conv3x3(inplanes, planes, stride, dilation)\n        self.add_module(self.norm1_name, norm1)\n        self.conv2 = conv3x3(planes, planes)\n        self.add_module(self.norm2_name, norm2)\n\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = downsample\n        self.stride = stride\n        self.dilation = dilation\n        assert not with_cp\n\n    @property\n    def norm1(self):\n        return getattr(self, self.norm1_name)\n\n    @property\n    def norm2(self):\n        return getattr(self, self.norm2_name)\n\n    def forward(self, x):\n        identity = x\n\n        out = self.conv1(x)\n        out = self.norm1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.norm2(out)\n\n        if self.downsample is not None:\n            identity = self.downsample(x)\n\n        out += identity\n        out = self.relu(out)\n\n        return out\n\n\nclass Bottleneck(nn.Module):\n    expansion = 4\n\n    def __init__(self,\n                 inplanes,\n                 planes,\n                 stride=1,\n                 dilation=1,\n                 downsample=None,\n                 style=\'pytorch\',\n                 with_cp=False,\n                 normalize=dict(type=\'BN\'),\n                 dcn=None):\n        """"""Bottleneck block for ResNetGC.\n        If style is ""pytorch"", the stride-two layer is the 3x3 conv layer,\n        if it is ""caffe"", the stride-two layer is the first 1x1 conv layer.\n        """"""\n        super(Bottleneck, self).__init__()\n        assert style in [\'pytorch\', \'caffe\']\n        assert dcn is None or isinstance(dcn, dict)\n        self.inplanes = inplanes\n        self.planes = planes\n        self.normalize = normalize\n        self.dcn = dcn\n        self.with_dcn = dcn is not None\n        if style == \'pytorch\':\n            self.conv1_stride = 1\n            self.conv2_stride = stride\n        else:\n            self.conv1_stride = stride\n            self.conv2_stride = 1\n\n        self.norm1_name, norm1 = build_norm_layer(normalize, planes, postfix=1)\n        self.norm2_name, norm2 = build_norm_layer(normalize, planes, postfix=2)\n        self.norm3_name, norm3 = build_norm_layer(\n            normalize, planes * self.expansion, postfix=3)\n\n        self.conv1 = nn.Conv2d(\n            inplanes,\n            planes,\n            kernel_size=1,\n            stride=self.conv1_stride,\n            bias=False)\n        self.add_module(self.norm1_name, norm1)\n        fallback_on_stride = False\n        self.with_modulated_dcn = False\n        if self.with_dcn:\n            fallback_on_stride = dcn.get(\'fallback_on_stride\', False)\n            self.with_modulated_dcn = dcn.get(\'modulated\', False)\n        if not self.with_dcn or fallback_on_stride:\n            self.conv2 = nn.Conv2d(\n                planes,\n                planes,\n                kernel_size=3,\n                stride=self.conv2_stride,\n                padding=dilation,\n                dilation=dilation,\n                bias=False)\n        else:\n            deformable_groups = dcn.get(\'deformable_groups\', 1)\n            if not self.with_modulated_dcn:\n                conv_op = DeformConv\n                offset_channels = 18\n            else:\n                conv_op = ModulatedDeformConv\n                offset_channels = 27\n            self.conv2_offset = nn.Conv2d(\n                planes,\n                deformable_groups * offset_channels,\n                kernel_size=3,\n                stride=self.conv2_stride,\n                padding=dilation,\n                dilation=dilation)\n            self.conv2 = conv_op(\n                planes,\n                planes,\n                kernel_size=3,\n                stride=self.conv2_stride,\n                padding=dilation,\n                dilation=dilation,\n                deformable_groups=deformable_groups,\n                bias=False)\n        self.add_module(self.norm2_name, norm2)\n        self.conv3 = nn.Conv2d(\n            planes, planes * self.expansion, kernel_size=1, bias=False)\n        self.add_module(self.norm3_name, norm3)\n\n        #self.se  = SELayer(planes * self.expansion)\n        #self.cbam = CBAM( planes * 4, 16 )\n        self.gc = ContextBlock2d(planes * self.expansion, planes // 4, \'att\',  [\'channel_add\'])\n\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = downsample\n        self.stride = stride\n        self.dilation = dilation\n        self.with_cp = with_cp\n        self.normalize = normalize\n\n    @property\n    def norm1(self):\n        return getattr(self, self.norm1_name)\n\n    @property\n    def norm2(self):\n        return getattr(self, self.norm2_name)\n\n    @property\n    def norm3(self):\n        return getattr(self, self.norm3_name)\n\n    def forward(self, x):\n\n        def _inner_forward(x):\n            identity = x\n\n            out = self.conv1(x)\n            out = self.norm1(out)\n            out = self.relu(out)\n\n            if not self.with_dcn:\n                out = self.conv2(out)\n            elif self.with_modulated_dcn:\n                offset_mask = self.conv2_offset(out)\n                offset = offset_mask[:, :18, :, :]\n                mask = offset_mask[:, -9:, :, :].sigmoid()\n                out = self.conv2(out, offset, mask)\n            else:\n                offset = self.conv2_offset(out)\n                out = self.conv2(out, offset)\n            out = self.norm2(out)\n            out = self.relu(out)\n\n            out = self.conv3(out)\n            out = self.norm3(out)\n\n            out = self.gc(out)\n\n            if self.downsample is not None:\n                identity = self.downsample(x)\n\n            out += identity\n\n            return out\n\n        if self.with_cp and x.requires_grad:\n            out = cp.checkpoint(_inner_forward, x)\n        else:\n            out = _inner_forward(x)\n\n        out = self.relu(out)\n\n        return out\n\n\ndef make_res_layer(block,\n                   inplanes,\n                   planes,\n                   blocks,\n                   stride=1,\n                   dilation=1,\n                   style=\'pytorch\',\n                   with_cp=False,\n                   normalize=dict(type=\'BN\'),\n                   dcn=None):\n    downsample = None\n    if stride != 1 or inplanes != planes * block.expansion:\n        downsample = nn.Sequential(\n            nn.Conv2d(\n                inplanes,\n                planes * block.expansion,\n                kernel_size=1,\n                stride=stride,\n                bias=False),\n            build_norm_layer(normalize, planes * block.expansion)[1],\n        )\n\n    layers = []\n    layers.append(\n        block(\n            inplanes,\n            planes,\n            stride,\n            dilation,\n            downsample,\n            style=style,\n            with_cp=with_cp,\n            normalize=normalize,\n            dcn=dcn))\n    inplanes = planes * block.expansion\n    for i in range(1, blocks):\n        layers.append(\n            block(\n                inplanes,\n                planes,\n                1,\n                dilation,\n                style=style,\n                with_cp=with_cp,\n                normalize=normalize,\n                dcn=dcn))\n\n    return nn.Sequential(*layers)\n\n\n@BACKBONES.register_module\nclass ResNetGC(nn.Module):\n    """"""ResNetGC backbone.\n\n    Args:\n        depth (int): Depth of resnet, from {18, 34, 50, 101, 152}.\n        num_stages (int): Resnet stages, normally 4.\n        strides (Sequence[int]): Strides of the first block of each stage.\n        dilations (Sequence[int]): Dilation of each stage.\n        out_indices (Sequence[int]): Output from which stages.\n        style (str): `pytorch` or `caffe`. If set to ""pytorch"", the stride-two\n            layer is the 3x3 conv layer, otherwise the stride-two layer is\n            the first 1x1 conv layer.\n        frozen_stages (int): Stages to be frozen (all param fixed). -1 means\n            not freezing any parameters.\n        normalize (dict): dictionary to construct and config norm layer.\n        norm_eval (bool): Whether to set norm layers to eval mode, namely,\n            freeze running stats (mean and var). Note: Effect on Batch Norm\n            and its variants only.\n        with_cp (bool): Use checkpoint or not. Using checkpoint will save some\n            memory while slowing down the training speed.\n        zero_init_residual (bool): whether to use zero init for last norm layer\n            in resblocks to let them behave as identity.\n    """"""\n\n    arch_settings = {\n        18: (BasicBlock, (2, 2, 2, 2)),\n        34: (BasicBlock, (3, 4, 6, 3)),\n        50: (Bottleneck, (3, 4, 6, 3)),\n        101: (Bottleneck, (3, 4, 23, 3)),\n        152: (Bottleneck, (3, 8, 36, 3))\n    }\n\n    def __init__(self,\n                 depth,\n                 num_stages=4,\n                 strides=(1, 2, 2, 2),\n                 dilations=(1, 1, 1, 1),\n                 out_indices=(0, 1, 2, 3),\n                 style=\'pytorch\',\n                 frozen_stages=-1,\n                 normalize=dict(type=\'BN\', frozen=False),\n                 norm_eval=True,\n                 dcn=None,\n                 stage_with_dcn=(False, False, False, False),\n                 with_cp=False,\n                 zero_init_residual=True):\n        super(ResNetGC, self).__init__()\n        if depth not in self.arch_settings:\n            raise KeyError(\'invalid depth {} for resnet\'.format(depth))\n        self.depth = depth\n        self.num_stages = num_stages\n        assert num_stages >= 1 and num_stages <= 4\n        self.strides = strides\n        self.dilations = dilations\n        assert len(strides) == len(dilations) == num_stages\n        self.out_indices = out_indices\n        assert max(out_indices) < num_stages\n        self.style = style\n        self.frozen_stages = frozen_stages\n        self.normalize = normalize\n        self.with_cp = with_cp\n        self.norm_eval = norm_eval\n        self.dcn = dcn\n        self.stage_with_dcn = stage_with_dcn\n        if dcn is not None:\n            assert len(stage_with_dcn) == num_stages\n        self.zero_init_residual = zero_init_residual\n        self.block, stage_blocks = self.arch_settings[depth]\n        self.stage_blocks = stage_blocks[:num_stages]\n        self.inplanes = 64\n\n        self._make_stem_layer()\n\n        self.res_layers = []\n        for i, num_blocks in enumerate(self.stage_blocks):\n            stride = strides[i]\n            dilation = dilations[i]\n            dcn = self.dcn if self.stage_with_dcn[i] else None\n            planes = 64 * 2**i\n            res_layer = make_res_layer(\n                self.block,\n                self.inplanes,\n                planes,\n                num_blocks,\n                stride=stride,\n                dilation=dilation,\n                style=self.style,\n                with_cp=with_cp,\n                normalize=normalize,\n                dcn=dcn)\n            self.inplanes = planes * self.block.expansion\n            layer_name = \'layer{}\'.format(i + 1)\n            self.add_module(layer_name, res_layer)\n            self.res_layers.append(layer_name)\n\n        self._freeze_stages()\n\n        self.feat_dim = self.block.expansion * 64 * 2**(\n            len(self.stage_blocks) - 1)\n\n    @property\n    def norm1(self):\n        return getattr(self, self.norm1_name)\n\n    def _make_stem_layer(self):\n        self.conv1 = nn.Conv2d(\n            3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n        self.norm1_name, norm1 = build_norm_layer(\n            self.normalize, 64, postfix=1)\n        self.add_module(self.norm1_name, norm1)\n        self.relu = nn.ReLU(inplace=True)\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n\n    def _freeze_stages(self):\n        if self.frozen_stages >= 0:\n            for m in [self.conv1, self.norm1]:\n                for param in m.parameters():\n                    param.requires_grad = False\n\n        for i in range(1, self.frozen_stages + 1):\n            m = getattr(self, \'layer{}\'.format(i))\n            for param in m.parameters():\n                param.requires_grad = False\n\n    def init_weights(self, pretrained=None):\n        if isinstance(pretrained, str):\n            logger = logging.getLogger()\n            load_checkpoint(self, pretrained, strict=False, logger=logger, map_location=torch.device(\'cpu\'))\n        elif pretrained is None:\n            for m in self.modules():\n                if isinstance(m, nn.Conv2d):\n                    kaiming_init(m)\n                elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n                    constant_init(m, 1)\n\n            if self.dcn is not None:\n                for m in self.modules():\n                    if isinstance(m, Bottleneck) and hasattr(\n                            m, \'conv2_offset\'):\n                        constant_init(m.conv2_offset, 0)\n\n            if self.zero_init_residual:\n                for m in self.modules():\n                    if isinstance(m, Bottleneck):\n                        constant_init(m.norm3, 0)\n                    elif isinstance(m, BasicBlock):\n                        constant_init(m.norm2, 0)\n        else:\n            raise TypeError(\'pretrained must be a str or None\')\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.norm1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n        outs = []\n        for i, layer_name in enumerate(self.res_layers):\n            res_layer = getattr(self, layer_name)\n            x = res_layer(x)\n            if i in self.out_indices:\n                outs.append(x)\n        if len(outs) == 1:\n            return outs[0]\n        else:\n            return tuple(outs)\n\n    def train(self, mode=True):\n        super(ResNetGC, self).train(mode)\n        if mode and self.norm_eval:\n            for m in self.modules():\n                # trick: eval have effect on BatchNorm only\n                if isinstance(m, nn.BatchNorm2d):\n                    m.eval()\n'"
detection/mmdet/models/backbones/resnet_se.py,3,"b'import logging\n\nimport torch.nn as nn\nimport torch.utils.checkpoint as cp\n\nfrom mmcv.cnn import constant_init, kaiming_init\nfrom mmcv.runner import load_checkpoint\n\nfrom mmdet.ops import DeformConv, ModulatedDeformConv\nfrom ..registry import BACKBONES\nfrom ..utils import build_norm_layer\n\n\nclass SELayer(nn.Module):\n    def __init__(self, channel, reduction = 16):\n        super(SELayer, self).__init__()\n        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n        self.fc       = nn.Sequential(\n                        nn.Linear(channel, channel // reduction),\n                        nn.ReLU(inplace = True),\n                        nn.Linear(channel // reduction, channel),\n                        nn.Sigmoid()\n                )\n        print(\'add one SELayer!\')\n\n    def forward(self, x):\n        b, c, _, _ = x.size()\n        y = self.avg_pool(x).view(b, c)\n        y = self.fc(y).view(b, c, 1, 1)\n        return x * y\n\n\ndef conv3x3(in_planes, out_planes, stride=1, dilation=1):\n    ""3x3 convolution with padding""\n    return nn.Conv2d(\n        in_planes,\n        out_planes,\n        kernel_size=3,\n        stride=stride,\n        padding=dilation,\n        dilation=dilation,\n        bias=False)\n\n\nclass BasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self,\n                 inplanes,\n                 planes,\n                 stride=1,\n                 dilation=1,\n                 downsample=None,\n                 style=\'pytorch\',\n                 with_cp=False,\n                 normalize=dict(type=\'BN\'),\n                 dcn=None):\n        super(BasicBlock, self).__init__()\n        assert dcn is None, ""Not implemented yet.""\n\n        self.norm1_name, norm1 = build_norm_layer(normalize, planes, postfix=1)\n        self.norm2_name, norm2 = build_norm_layer(normalize, planes, postfix=2)\n\n        self.conv1 = conv3x3(inplanes, planes, stride, dilation)\n        self.add_module(self.norm1_name, norm1)\n        self.conv2 = conv3x3(planes, planes)\n        self.add_module(self.norm2_name, norm2)\n\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = downsample\n        self.stride = stride\n        self.dilation = dilation\n        assert not with_cp\n\n    @property\n    def norm1(self):\n        return getattr(self, self.norm1_name)\n\n    @property\n    def norm2(self):\n        return getattr(self, self.norm2_name)\n\n    def forward(self, x):\n        identity = x\n\n        out = self.conv1(x)\n        out = self.norm1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.norm2(out)\n\n        if self.downsample is not None:\n            identity = self.downsample(x)\n\n        out += identity\n        out = self.relu(out)\n\n        return out\n\n\nclass Bottleneck(nn.Module):\n    expansion = 4\n\n    def __init__(self,\n                 inplanes,\n                 planes,\n                 stride=1,\n                 dilation=1,\n                 downsample=None,\n                 style=\'pytorch\',\n                 with_cp=False,\n                 normalize=dict(type=\'BN\'),\n                 dcn=None):\n        """"""Bottleneck block for ResNetSE.\n        If style is ""pytorch"", the stride-two layer is the 3x3 conv layer,\n        if it is ""caffe"", the stride-two layer is the first 1x1 conv layer.\n        """"""\n        super(Bottleneck, self).__init__()\n        assert style in [\'pytorch\', \'caffe\']\n        assert dcn is None or isinstance(dcn, dict)\n        self.inplanes = inplanes\n        self.planes = planes\n        self.normalize = normalize\n        self.dcn = dcn\n        self.with_dcn = dcn is not None\n        if style == \'pytorch\':\n            self.conv1_stride = 1\n            self.conv2_stride = stride\n        else:\n            self.conv1_stride = stride\n            self.conv2_stride = 1\n\n        self.norm1_name, norm1 = build_norm_layer(normalize, planes, postfix=1)\n        self.norm2_name, norm2 = build_norm_layer(normalize, planes, postfix=2)\n        self.norm3_name, norm3 = build_norm_layer(\n            normalize, planes * self.expansion, postfix=3)\n\n        self.conv1 = nn.Conv2d(\n            inplanes,\n            planes,\n            kernel_size=1,\n            stride=self.conv1_stride,\n            bias=False)\n        self.add_module(self.norm1_name, norm1)\n        fallback_on_stride = False\n        self.with_modulated_dcn = False\n        if self.with_dcn:\n            fallback_on_stride = dcn.get(\'fallback_on_stride\', False)\n            self.with_modulated_dcn = dcn.get(\'modulated\', False)\n        if not self.with_dcn or fallback_on_stride:\n            self.conv2 = nn.Conv2d(\n                planes,\n                planes,\n                kernel_size=3,\n                stride=self.conv2_stride,\n                padding=dilation,\n                dilation=dilation,\n                bias=False)\n        else:\n            deformable_groups = dcn.get(\'deformable_groups\', 1)\n            if not self.with_modulated_dcn:\n                conv_op = DeformConv\n                offset_channels = 18\n            else:\n                conv_op = ModulatedDeformConv\n                offset_channels = 27\n            self.conv2_offset = nn.Conv2d(\n                planes,\n                deformable_groups * offset_channels,\n                kernel_size=3,\n                stride=self.conv2_stride,\n                padding=dilation,\n                dilation=dilation)\n            self.conv2 = conv_op(\n                planes,\n                planes,\n                kernel_size=3,\n                stride=self.conv2_stride,\n                padding=dilation,\n                dilation=dilation,\n                deformable_groups=deformable_groups,\n                bias=False)\n        self.add_module(self.norm2_name, norm2)\n        self.conv3 = nn.Conv2d(\n            planes, planes * self.expansion, kernel_size=1, bias=False)\n        self.add_module(self.norm3_name, norm3)\n\n        self.se  = SELayer(planes * self.expansion)\n\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = downsample\n        self.stride = stride\n        self.dilation = dilation\n        self.with_cp = with_cp\n        self.normalize = normalize\n\n    @property\n    def norm1(self):\n        return getattr(self, self.norm1_name)\n\n    @property\n    def norm2(self):\n        return getattr(self, self.norm2_name)\n\n    @property\n    def norm3(self):\n        return getattr(self, self.norm3_name)\n\n    def forward(self, x):\n\n        def _inner_forward(x):\n            identity = x\n\n            out = self.conv1(x)\n            out = self.norm1(out)\n            out = self.relu(out)\n\n            if not self.with_dcn:\n                out = self.conv2(out)\n            elif self.with_modulated_dcn:\n                offset_mask = self.conv2_offset(out)\n                offset = offset_mask[:, :18, :, :]\n                mask = offset_mask[:, -9:, :, :].sigmoid()\n                out = self.conv2(out, offset, mask)\n            else:\n                offset = self.conv2_offset(out)\n                out = self.conv2(out, offset)\n            out = self.norm2(out)\n            out = self.relu(out)\n\n            out = self.conv3(out)\n            out = self.norm3(out)\n\n            out = self.se(out)\n\n            if self.downsample is not None:\n                identity = self.downsample(x)\n\n            out += identity\n\n            return out\n\n        if self.with_cp and x.requires_grad:\n            out = cp.checkpoint(_inner_forward, x)\n        else:\n            out = _inner_forward(x)\n\n        out = self.relu(out)\n\n        return out\n\n\ndef make_res_layer(block,\n                   inplanes,\n                   planes,\n                   blocks,\n                   stride=1,\n                   dilation=1,\n                   style=\'pytorch\',\n                   with_cp=False,\n                   normalize=dict(type=\'BN\'),\n                   dcn=None):\n    downsample = None\n    if stride != 1 or inplanes != planes * block.expansion:\n        downsample = nn.Sequential(\n            nn.Conv2d(\n                inplanes,\n                planes * block.expansion,\n                kernel_size=1,\n                stride=stride,\n                bias=False),\n            build_norm_layer(normalize, planes * block.expansion)[1],\n        )\n\n    layers = []\n    layers.append(\n        block(\n            inplanes,\n            planes,\n            stride,\n            dilation,\n            downsample,\n            style=style,\n            with_cp=with_cp,\n            normalize=normalize,\n            dcn=dcn))\n    inplanes = planes * block.expansion\n    for i in range(1, blocks):\n        layers.append(\n            block(\n                inplanes,\n                planes,\n                1,\n                dilation,\n                style=style,\n                with_cp=with_cp,\n                normalize=normalize,\n                dcn=dcn))\n\n    return nn.Sequential(*layers)\n\n\n@BACKBONES.register_module\nclass ResNetSE(nn.Module):\n    """"""ResNetSE backbone.\n\n    Args:\n        depth (int): Depth of resnet, from {18, 34, 50, 101, 152}.\n        num_stages (int): Resnet stages, normally 4.\n        strides (Sequence[int]): Strides of the first block of each stage.\n        dilations (Sequence[int]): Dilation of each stage.\n        out_indices (Sequence[int]): Output from which stages.\n        style (str): `pytorch` or `caffe`. If set to ""pytorch"", the stride-two\n            layer is the 3x3 conv layer, otherwise the stride-two layer is\n            the first 1x1 conv layer.\n        frozen_stages (int): Stages to be frozen (all param fixed). -1 means\n            not freezing any parameters.\n        normalize (dict): dictionary to construct and config norm layer.\n        norm_eval (bool): Whether to set norm layers to eval mode, namely,\n            freeze running stats (mean and var). Note: Effect on Batch Norm\n            and its variants only.\n        with_cp (bool): Use checkpoint or not. Using checkpoint will save some\n            memory while slowing down the training speed.\n        zero_init_residual (bool): whether to use zero init for last norm layer\n            in resblocks to let them behave as identity.\n    """"""\n\n    arch_settings = {\n        18: (BasicBlock, (2, 2, 2, 2)),\n        34: (BasicBlock, (3, 4, 6, 3)),\n        50: (Bottleneck, (3, 4, 6, 3)),\n        101: (Bottleneck, (3, 4, 23, 3)),\n        152: (Bottleneck, (3, 8, 36, 3))\n    }\n\n    def __init__(self,\n                 depth,\n                 num_stages=4,\n                 strides=(1, 2, 2, 2),\n                 dilations=(1, 1, 1, 1),\n                 out_indices=(0, 1, 2, 3),\n                 style=\'pytorch\',\n                 frozen_stages=-1,\n                 normalize=dict(type=\'BN\', frozen=False),\n                 norm_eval=True,\n                 dcn=None,\n                 stage_with_dcn=(False, False, False, False),\n                 with_cp=False,\n                 zero_init_residual=True):\n        super(ResNetSE, self).__init__()\n        if depth not in self.arch_settings:\n            raise KeyError(\'invalid depth {} for resnet\'.format(depth))\n        self.depth = depth\n        self.num_stages = num_stages\n        assert num_stages >= 1 and num_stages <= 4\n        self.strides = strides\n        self.dilations = dilations\n        assert len(strides) == len(dilations) == num_stages\n        self.out_indices = out_indices\n        assert max(out_indices) < num_stages\n        self.style = style\n        self.frozen_stages = frozen_stages\n        self.normalize = normalize\n        self.with_cp = with_cp\n        self.norm_eval = norm_eval\n        self.dcn = dcn\n        self.stage_with_dcn = stage_with_dcn\n        if dcn is not None:\n            assert len(stage_with_dcn) == num_stages\n        self.zero_init_residual = zero_init_residual\n        self.block, stage_blocks = self.arch_settings[depth]\n        self.stage_blocks = stage_blocks[:num_stages]\n        self.inplanes = 64\n\n        self._make_stem_layer()\n\n        self.res_layers = []\n        for i, num_blocks in enumerate(self.stage_blocks):\n            stride = strides[i]\n            dilation = dilations[i]\n            dcn = self.dcn if self.stage_with_dcn[i] else None\n            planes = 64 * 2**i\n            res_layer = make_res_layer(\n                self.block,\n                self.inplanes,\n                planes,\n                num_blocks,\n                stride=stride,\n                dilation=dilation,\n                style=self.style,\n                with_cp=with_cp,\n                normalize=normalize,\n                dcn=dcn)\n            self.inplanes = planes * self.block.expansion\n            layer_name = \'layer{}\'.format(i + 1)\n            self.add_module(layer_name, res_layer)\n            self.res_layers.append(layer_name)\n\n        self._freeze_stages()\n\n        self.feat_dim = self.block.expansion * 64 * 2**(\n            len(self.stage_blocks) - 1)\n\n    @property\n    def norm1(self):\n        return getattr(self, self.norm1_name)\n\n    def _make_stem_layer(self):\n        self.conv1 = nn.Conv2d(\n            3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n        self.norm1_name, norm1 = build_norm_layer(\n            self.normalize, 64, postfix=1)\n        self.add_module(self.norm1_name, norm1)\n        self.relu = nn.ReLU(inplace=True)\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n\n    def _freeze_stages(self):\n        if self.frozen_stages >= 0:\n            for m in [self.conv1, self.norm1]:\n                for param in m.parameters():\n                    param.requires_grad = False\n\n        for i in range(1, self.frozen_stages + 1):\n            m = getattr(self, \'layer{}\'.format(i))\n            for param in m.parameters():\n                param.requires_grad = False\n\n    def init_weights(self, pretrained=None):\n        if isinstance(pretrained, str):\n            logger = logging.getLogger()\n            load_checkpoint(self, pretrained, strict=False, logger=logger, map_location=torch.device(\'cpu\'))\n        elif pretrained is None:\n            for m in self.modules():\n                if isinstance(m, nn.Conv2d):\n                    kaiming_init(m)\n                elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n                    constant_init(m, 1)\n\n            if self.dcn is not None:\n                for m in self.modules():\n                    if isinstance(m, Bottleneck) and hasattr(\n                            m, \'conv2_offset\'):\n                        constant_init(m.conv2_offset, 0)\n\n            if self.zero_init_residual:\n                for m in self.modules():\n                    if isinstance(m, Bottleneck):\n                        constant_init(m.norm3, 0)\n                    elif isinstance(m, BasicBlock):\n                        constant_init(m.norm2, 0)\n        else:\n            raise TypeError(\'pretrained must be a str or None\')\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.norm1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n        outs = []\n        for i, layer_name in enumerate(self.res_layers):\n            res_layer = getattr(self, layer_name)\n            x = res_layer(x)\n            if i in self.out_indices:\n                outs.append(x)\n        if len(outs) == 1:\n            return outs[0]\n        else:\n            return tuple(outs)\n\n    def train(self, mode=True):\n        super(ResNetSE, self).train(mode)\n        if mode and self.norm_eval:\n            for m in self.modules():\n                # trick: eval have effect on BatchNorm only\n                if isinstance(m, nn.BatchNorm2d):\n                    m.eval()\n'"
detection/mmdet/models/backbones/resnet_sge.py,6,"b'import logging\n\nimport torch.nn as nn\nimport torch.utils.checkpoint as cp\n\nfrom mmcv.cnn import constant_init, kaiming_init\nfrom mmcv.runner import load_checkpoint\n\nfrom mmdet.ops import DeformConv, ModulatedDeformConv\nfrom ..registry import BACKBONES\nfrom ..utils import build_norm_layer\n\nimport torch\nfrom torch.nn.parameter import Parameter\n\n\nclass SpatialGroupEnhance(nn.Module):\n    def __init__(self, groups):\n        super(SpatialGroupEnhance, self).__init__()\n        self.groups   = groups\n        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n        self.weight   = Parameter(torch.zeros(1, groups, 1, 1))\n        self.bias     = Parameter(torch.ones(1, groups, 1, 1))\n        self.sig      = nn.Sigmoid()\n        print(\'add one SGE!\')\n\n    def forward(self, x): # (b, c, h, w)\n        b, c, h, w = x.size()\n        x = x.view(b * self.groups, -1, h, w) # (b*32, c\', h, w)\n        xn = x * self.avg_pool(x)\n        xn = xn.sum(dim=1, keepdim=True) # (b*32, 1, h, w)\n        t = xn.view(b * self.groups, -1)\n        t = t - t.mean(dim=1, keepdim=True)\n        std = t.std(dim=1, keepdim=True) + 1e-5\n        t = t / std\n        t = t.view(b, self.groups, h, w)\n        t = t * self.weight + self.bias\n        t = t.view(b * self.groups, 1, h, w)\n        x = x * self.sig(t)\n        x = x.view(b, c, h, w)\n\n        return x\n\n\ndef conv3x3(in_planes, out_planes, stride=1, dilation=1):\n    ""3x3 convolution with padding""\n    return nn.Conv2d(\n        in_planes,\n        out_planes,\n        kernel_size=3,\n        stride=stride,\n        padding=dilation,\n        dilation=dilation,\n        bias=False)\n\n\nclass BasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self,\n                 inplanes,\n                 planes,\n                 stride=1,\n                 dilation=1,\n                 downsample=None,\n                 style=\'pytorch\',\n                 with_cp=False,\n                 normalize=dict(type=\'BN\'),\n                 dcn=None):\n        super(BasicBlock, self).__init__()\n        assert dcn is None, ""Not implemented yet.""\n\n        self.norm1_name, norm1 = build_norm_layer(normalize, planes, postfix=1)\n        self.norm2_name, norm2 = build_norm_layer(normalize, planes, postfix=2)\n\n        self.conv1 = conv3x3(inplanes, planes, stride, dilation)\n        self.add_module(self.norm1_name, norm1)\n        self.conv2 = conv3x3(planes, planes)\n        self.add_module(self.norm2_name, norm2)\n\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = downsample\n        self.stride = stride\n        self.dilation = dilation\n        assert not with_cp\n\n    @property\n    def norm1(self):\n        return getattr(self, self.norm1_name)\n\n    @property\n    def norm2(self):\n        return getattr(self, self.norm2_name)\n\n    def forward(self, x):\n        identity = x\n\n        out = self.conv1(x)\n        out = self.norm1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.norm2(out)\n\n        if self.downsample is not None:\n            identity = self.downsample(x)\n\n        out += identity\n        out = self.relu(out)\n\n        return out\n\n\nclass Bottleneck(nn.Module):\n    expansion = 4\n\n    def __init__(self,\n                 inplanes,\n                 planes,\n                 stride=1,\n                 dilation=1,\n                 downsample=None,\n                 style=\'pytorch\',\n                 with_cp=False,\n                 normalize=dict(type=\'BN\'),\n                 dcn=None):\n        """"""Bottleneck block for ResNet.\n        If style is ""pytorch"", the stride-two layer is the 3x3 conv layer,\n        if it is ""caffe"", the stride-two layer is the first 1x1 conv layer.\n        """"""\n        super(Bottleneck, self).__init__()\n        assert style in [\'pytorch\', \'caffe\']\n        assert dcn is None or isinstance(dcn, dict)\n        self.inplanes = inplanes\n        self.planes = planes\n        self.normalize = normalize\n        self.dcn = dcn\n        self.with_dcn = dcn is not None\n        if style == \'pytorch\':\n            self.conv1_stride = 1\n            self.conv2_stride = stride\n        else:\n            self.conv1_stride = stride\n            self.conv2_stride = 1\n\n        self.norm1_name, norm1 = build_norm_layer(normalize, planes, postfix=1)\n        self.norm2_name, norm2 = build_norm_layer(normalize, planes, postfix=2)\n        self.norm3_name, norm3 = build_norm_layer(\n            normalize, planes * self.expansion, postfix=3)\n\n        self.conv1 = nn.Conv2d(\n            inplanes,\n            planes,\n            kernel_size=1,\n            stride=self.conv1_stride,\n            bias=False)\n        self.add_module(self.norm1_name, norm1)\n        fallback_on_stride = False\n        self.with_modulated_dcn = False\n        if self.with_dcn:\n            fallback_on_stride = dcn.get(\'fallback_on_stride\', False)\n            self.with_modulated_dcn = dcn.get(\'modulated\', False)\n        if not self.with_dcn or fallback_on_stride:\n            self.conv2 = nn.Conv2d(\n                planes,\n                planes,\n                kernel_size=3,\n                stride=self.conv2_stride,\n                padding=dilation,\n                dilation=dilation,\n                bias=False)\n        else:\n            deformable_groups = dcn.get(\'deformable_groups\', 1)\n            if not self.with_modulated_dcn:\n                conv_op = DeformConv\n                offset_channels = 18\n            else:\n                conv_op = ModulatedDeformConv\n                offset_channels = 27\n            self.conv2_offset = nn.Conv2d(\n                planes,\n                deformable_groups * offset_channels,\n                kernel_size=3,\n                stride=self.conv2_stride,\n                padding=dilation,\n                dilation=dilation)\n            self.conv2 = conv_op(\n                planes,\n                planes,\n                kernel_size=3,\n                stride=self.conv2_stride,\n                padding=dilation,\n                dilation=dilation,\n                deformable_groups=deformable_groups,\n                bias=False)\n        self.add_module(self.norm2_name, norm2)\n        self.conv3 = nn.Conv2d(\n            planes, planes * self.expansion, kernel_size=1, bias=False)\n        self.add_module(self.norm3_name, norm3)\n\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = downsample\n        self.stride = stride\n        self.dilation = dilation\n        self.with_cp = with_cp\n        self.normalize = normalize\n\n        self.sge = SpatialGroupEnhance(64)\n\n    @property\n    def norm1(self):\n        return getattr(self, self.norm1_name)\n\n    @property\n    def norm2(self):\n        return getattr(self, self.norm2_name)\n\n    @property\n    def norm3(self):\n        return getattr(self, self.norm3_name)\n\n    def forward(self, x):\n\n        def _inner_forward(x):\n            identity = x\n\n            out = self.conv1(x)\n            out = self.norm1(out)\n            out = self.relu(out)\n\n            if not self.with_dcn:\n                out = self.conv2(out)\n            elif self.with_modulated_dcn:\n                offset_mask = self.conv2_offset(out)\n                offset = offset_mask[:, :18, :, :]\n                mask = offset_mask[:, -9:, :, :].sigmoid()\n                out = self.conv2(out, offset, mask)\n            else:\n                offset = self.conv2_offset(out)\n                out = self.conv2(out, offset)\n            out = self.norm2(out)\n            out = self.relu(out)\n\n            out = self.conv3(out)\n            out = self.norm3(out)\n            out = self.sge(out)\n\n            if self.downsample is not None:\n                identity = self.downsample(x)\n\n            out += identity\n\n            return out\n\n        if self.with_cp and x.requires_grad:\n            out = cp.checkpoint(_inner_forward, x)\n        else:\n            out = _inner_forward(x)\n\n        out = self.relu(out)\n\n        return out\n\n\ndef make_res_layer(block,\n                   inplanes,\n                   planes,\n                   blocks,\n                   stride=1,\n                   dilation=1,\n                   style=\'pytorch\',\n                   with_cp=False,\n                   normalize=dict(type=\'BN\'),\n                   dcn=None):\n    downsample = None\n    if stride != 1 or inplanes != planes * block.expansion:\n        downsample = nn.Sequential(\n            nn.Conv2d(\n                inplanes,\n                planes * block.expansion,\n                kernel_size=1,\n                stride=stride,\n                bias=False),\n            build_norm_layer(normalize, planes * block.expansion)[1],\n        )\n\n    layers = []\n    layers.append(\n        block(\n            inplanes,\n            planes,\n            stride,\n            dilation,\n            downsample,\n            style=style,\n            with_cp=with_cp,\n            normalize=normalize,\n            dcn=dcn))\n    inplanes = planes * block.expansion\n    for i in range(1, blocks):\n        layers.append(\n            block(\n                inplanes,\n                planes,\n                1,\n                dilation,\n                style=style,\n                with_cp=with_cp,\n                normalize=normalize,\n                dcn=dcn))\n    # layers.append(SpatialGroupEnhance(64))\n\n    return nn.Sequential(*layers)\n\n\n@BACKBONES.register_module\nclass ResNetSGE(nn.Module):\n    """"""ResNetSGE backbone.\n\n    Args:\n        depth (int): Depth of resnet, from {18, 34, 50, 101, 152}.\n        num_stages (int): Resnet stages, normally 4.\n        strides (Sequence[int]): Strides of the first block of each stage.\n        dilations (Sequence[int]): Dilation of each stage.\n        out_indices (Sequence[int]): Output from which stages.\n        style (str): `pytorch` or `caffe`. If set to ""pytorch"", the stride-two\n            layer is the 3x3 conv layer, otherwise the stride-two layer is\n            the first 1x1 conv layer.\n        frozen_stages (int): Stages to be frozen (all param fixed). -1 means\n            not freezing any parameters.\n        normalize (dict): dictionary to construct and config norm layer.\n        norm_eval (bool): Whether to set norm layers to eval mode, namely,\n            freeze running stats (mean and var). Note: Effect on Batch Norm\n            and its variants only.\n        with_cp (bool): Use checkpoint or not. Using checkpoint will save some\n            memory while slowing down the training speed.\n        zero_init_residual (bool): whether to use zero init for last norm layer\n            in resblocks to let them behave as identity.\n    """"""\n\n    arch_settings = {\n        18: (BasicBlock, (2, 2, 2, 2)),\n        34: (BasicBlock, (3, 4, 6, 3)),\n        50: (Bottleneck, (3, 4, 6, 3)),\n        101: (Bottleneck, (3, 4, 23, 3)),\n        152: (Bottleneck, (3, 8, 36, 3))\n    }\n\n    def __init__(self,\n                 depth,\n                 num_stages=4,\n                 strides=(1, 2, 2, 2),\n                 dilations=(1, 1, 1, 1),\n                 out_indices=(0, 1, 2, 3),\n                 style=\'pytorch\',\n                 frozen_stages=-1,\n                 normalize=dict(type=\'BN\', frozen=False),\n                 norm_eval=True,\n                 dcn=None,\n                 stage_with_dcn=(False, False, False, False),\n                 with_cp=False,\n                 zero_init_residual=True):\n        super(ResNetSGE, self).__init__()\n        if depth not in self.arch_settings:\n            raise KeyError(\'invalid depth {} for resnet\'.format(depth))\n        self.depth = depth\n        self.num_stages = num_stages\n        assert num_stages >= 1 and num_stages <= 4\n        self.strides = strides\n        self.dilations = dilations\n        assert len(strides) == len(dilations) == num_stages\n        self.out_indices = out_indices\n        assert max(out_indices) < num_stages\n        self.style = style\n        self.frozen_stages = frozen_stages\n        self.normalize = normalize\n        self.with_cp = with_cp\n        self.norm_eval = norm_eval\n        self.dcn = dcn\n        self.stage_with_dcn = stage_with_dcn\n        if dcn is not None:\n            assert len(stage_with_dcn) == num_stages\n        self.zero_init_residual = zero_init_residual\n        self.block, stage_blocks = self.arch_settings[depth]\n        self.stage_blocks = stage_blocks[:num_stages]\n        self.inplanes = 64\n\n        self._make_stem_layer()\n\n        self.res_layers = []\n        for i, num_blocks in enumerate(self.stage_blocks):\n            stride = strides[i]\n            dilation = dilations[i]\n            dcn = self.dcn if self.stage_with_dcn[i] else None\n            planes = 64 * 2**i\n            res_layer = make_res_layer(\n                self.block,\n                self.inplanes,\n                planes,\n                num_blocks,\n                stride=stride,\n                dilation=dilation,\n                style=self.style,\n                with_cp=with_cp,\n                normalize=normalize,\n                dcn=dcn)\n            self.inplanes = planes * self.block.expansion\n            layer_name = \'layer{}\'.format(i + 1)\n            self.add_module(layer_name, res_layer)\n            self.res_layers.append(layer_name)\n\n        self._freeze_stages()\n\n        self.feat_dim = self.block.expansion * 64 * 2**(\n            len(self.stage_blocks) - 1)\n\n    @property\n    def norm1(self):\n        return getattr(self, self.norm1_name)\n\n    def _make_stem_layer(self):\n        self.conv1 = nn.Conv2d(\n            3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n        self.norm1_name, norm1 = build_norm_layer(\n            self.normalize, 64, postfix=1)\n        self.add_module(self.norm1_name, norm1)\n        self.relu = nn.ReLU(inplace=True)\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n\n    def _freeze_stages(self):\n        if self.frozen_stages >= 0:\n            for m in [self.conv1, self.norm1]:\n                for param in m.parameters():\n                    param.requires_grad = False\n\n        for i in range(1, self.frozen_stages + 1):\n            m = getattr(self, \'layer{}\'.format(i))\n            for param in m.parameters():\n                param.requires_grad = False\n\n    def init_weights(self, pretrained=None):\n        if isinstance(pretrained, str):\n            logger = logging.getLogger()\n            load_checkpoint(self, pretrained, strict=False, logger=logger, map_location=torch.device(\'cpu\'))\n        elif pretrained is None:\n            for m in self.modules():\n                if isinstance(m, nn.Conv2d):\n                    kaiming_init(m)\n                elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n                    constant_init(m, 1)\n\n            if self.dcn is not None:\n                for m in self.modules():\n                    if isinstance(m, Bottleneck) and hasattr(\n                            m, \'conv2_offset\'):\n                        constant_init(m.conv2_offset, 0)\n\n            if self.zero_init_residual:\n                for m in self.modules():\n                    if isinstance(m, Bottleneck):\n                        constant_init(m.norm3, 0)\n                    elif isinstance(m, BasicBlock):\n                        constant_init(m.norm2, 0)\n        else:\n            raise TypeError(\'pretrained must be a str or None\')\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.norm1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n        outs = []\n        for i, layer_name in enumerate(self.res_layers):\n            res_layer = getattr(self, layer_name)\n            x = res_layer(x)\n            if i in self.out_indices:\n                outs.append(x)\n        if len(outs) == 1:\n            return outs[0]\n        else:\n            return tuple(outs)\n\n    def train(self, mode=True):\n        super(ResNetSGE, self).train(mode)\n        if mode and self.norm_eval:\n            for m in self.modules():\n                # trick: eval have effect on BatchNorm only\n                if isinstance(m, nn.BatchNorm2d):\n                    m.eval()\n'"
detection/mmdet/models/backbones/resnet_sk.py,6,"b'import logging\n\nimport torch.nn as nn\nimport torch.utils.checkpoint as cp\n\nfrom mmcv.cnn import constant_init, kaiming_init\nfrom mmcv.runner import load_checkpoint\n\nfrom mmdet.ops import DeformConv, ModulatedDeformConv\nfrom ..registry import BACKBONES\nfrom ..utils import build_norm_layer\n\nimport torch\nfrom torch.nn.parameter import Parameter\nimport torch.nn.functional as F\n\n\ndef conv3x3(in_planes, out_planes, stride=1, dilation=1, groups=1):\n    ""3x3 convolution with padding""\n    return nn.Conv2d(\n        in_planes,\n        out_planes,\n        kernel_size=3,\n        stride=stride,\n        padding=dilation,\n        dilation=dilation,\n        groups=groups,\n        bias=False)\n\n\nclass BasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self,\n                 inplanes,\n                 planes,\n                 stride=1,\n                 dilation=1,\n                 downsample=None,\n                 style=\'pytorch\',\n                 with_cp=False,\n                 normalize=dict(type=\'BN\'),\n                 dcn=None):\n        super(BasicBlock, self).__init__()\n        assert dcn is None, ""Not implemented yet.""\n\n        self.norm1_name, norm1 = build_norm_layer(normalize, planes, postfix=1)\n        self.norm2_name, norm2 = build_norm_layer(normalize, planes, postfix=2)\n\n        self.conv1 = conv3x3(inplanes, planes, stride, dilation)\n        self.add_module(self.norm1_name, norm1)\n        self.conv2 = conv3x3(planes, planes)\n        self.add_module(self.norm2_name, norm2)\n\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = downsample\n        self.stride = stride\n        self.dilation = dilation\n        assert not with_cp\n\n    @property\n    def norm1(self):\n        return getattr(self, self.norm1_name)\n\n    @property\n    def norm2(self):\n        return getattr(self, self.norm2_name)\n\n    def forward(self, x):\n        identity = x\n\n        out = self.conv1(x)\n        out = self.norm1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.norm2(out)\n\n        if self.downsample is not None:\n            identity = self.downsample(x)\n\n        out += identity\n        out = self.relu(out)\n\n        return out\n\n\nclass Bottleneck(nn.Module):\n    expansion = 4\n\n    def __init__(self,\n                 inplanes,\n                 planes,\n                 stride=1,\n                 dilation=1,\n                 downsample=None,\n                 style=\'pytorch\',\n                 with_cp=False,\n                 normalize=dict(type=\'BN\'),\n                 dcn=None):\n        """"""Bottleneck block for ResNet.\n        If style is ""pytorch"", the stride-two layer is the 3x3 conv layer,\n        if it is ""caffe"", the stride-two layer is the first 1x1 conv layer.\n        """"""\n        super(Bottleneck, self).__init__()\n        assert style in [\'pytorch\', \'caffe\']\n        assert dcn is None or isinstance(dcn, dict)\n        self.inplanes = inplanes\n        self.planes = planes\n        self.normalize = normalize\n        self.dcn = dcn\n        self.with_dcn = dcn is not None\n        if style == \'pytorch\':\n            self.conv1_stride = 1\n            self.conv2_stride = stride\n        else:\n            self.conv1_stride = stride\n            self.conv2_stride = 1\n\n        self.norm1_name, norm1 = build_norm_layer(normalize, planes, postfix=1)\n        self.norm2_name, norm2 = build_norm_layer(normalize, planes, postfix=2)\n        self.norm3_name, norm3 = build_norm_layer(\n            normalize, planes * self.expansion, postfix=3)\n\n        self.conv1 = nn.Conv2d(\n            inplanes,\n            planes,\n            kernel_size=1,\n            stride=self.conv1_stride,\n            bias=False)\n        self.add_module(self.norm1_name, norm1)\n        fallback_on_stride = False\n        self.with_modulated_dcn = False\n        if self.with_dcn:\n            fallback_on_stride = dcn.get(\'fallback_on_stride\', False)\n            self.with_modulated_dcn = dcn.get(\'modulated\', False)\n        if not self.with_dcn or fallback_on_stride:\n            self.conv2 = nn.Conv2d(\n                planes,\n                planes,\n                kernel_size=3,\n                stride=self.conv2_stride,\n                padding=dilation,\n                dilation=dilation,\n                bias=False)\n        else:\n            deformable_groups = dcn.get(\'deformable_groups\', 1)\n            if not self.with_modulated_dcn:\n                conv_op = DeformConv\n                offset_channels = 18\n            else:\n                conv_op = ModulatedDeformConv\n                offset_channels = 27\n            self.conv2_offset = nn.Conv2d(\n                planes,\n                deformable_groups * offset_channels,\n                kernel_size=3,\n                stride=self.conv2_stride,\n                padding=dilation,\n                dilation=dilation)\n            self.conv2 = conv_op(\n                planes,\n                planes,\n                kernel_size=3,\n                stride=self.conv2_stride,\n                padding=dilation,\n                dilation=dilation,\n                deformable_groups=deformable_groups,\n                bias=False)\n        self.add_module(self.norm2_name, norm2)\n        self.conv3 = nn.Conv2d(\n            planes, planes * self.expansion, kernel_size=1, bias=False)\n        self.add_module(self.norm3_name, norm3)\n\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = downsample\n        self.stride = stride\n        self.dilation = dilation\n        self.with_cp = with_cp\n        self.normalize = normalize\n\n        #self.sge = SpatialGroupEnhance(64)\n        self.conv2g = conv3x3(planes, planes, stride, groups = 32)\n        self.bn2g   = nn.BatchNorm2d(planes)\n\n        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n        self.conv_fc1 = nn.Conv2d(planes, planes//16, 1, bias=False)\n        self.bn_fc1   = nn.BatchNorm2d(planes//16)\n        self.conv_fc2 = nn.Conv2d(planes//16, 2 * planes, 1, bias=False)\n\n        self.D = planes\n\n    @property\n    def norm1(self):\n        return getattr(self, self.norm1_name)\n\n    @property\n    def norm2(self):\n        return getattr(self, self.norm2_name)\n\n    @property\n    def norm3(self):\n        return getattr(self, self.norm3_name)\n\n    def forward(self, x):\n\n        def _inner_forward(x):\n            identity = x\n\n            out = self.conv1(x)\n            out = self.norm1(out)\n            out = self.relu(out)\n\n            #da = self.conv2(out)\n            #out = self.norm2(out)\n            #out = self.relu(out)\n\n            d1 = self.conv2(out)\n            d1 = self.norm2(d1)\n            d1 = self.relu(d1)\n\n            d2 = self.conv2g(out)\n            d2 = self.bn2g(d2)\n            d2 = self.relu(d2)\n\n            d  = self.avg_pool(d1) + self.avg_pool(d2)\n            d = F.relu(self.bn_fc1(self.conv_fc1(d)))\n            d = self.conv_fc2(d)\n            d = torch.unsqueeze(d, 1).view(-1, 2, self.D, 1, 1)\n            d = F.softmax(d, 1)\n            d1 = d1 * d[:, 0, :, :, :].squeeze(1)\n            d2 = d2 * d[:, 1, :, :, :].squeeze(1)\n            out  = d1 + d2\n\n            out = self.conv3(out)\n            out = self.norm3(out)\n            #out = self.sge(out)\n\n            if self.downsample is not None:\n                identity = self.downsample(x)\n\n            out += identity\n\n            return out\n\n        if self.with_cp and x.requires_grad:\n            out = cp.checkpoint(_inner_forward, x)\n        else:\n            out = _inner_forward(x)\n\n        out = self.relu(out)\n\n        return out\n\n\ndef make_res_layer(block,\n                   inplanes,\n                   planes,\n                   blocks,\n                   stride=1,\n                   dilation=1,\n                   style=\'pytorch\',\n                   with_cp=False,\n                   normalize=dict(type=\'BN\'),\n                   dcn=None):\n    downsample = None\n    if stride != 1 or inplanes != planes * block.expansion:\n        downsample = nn.Sequential(\n            nn.Conv2d(\n                inplanes,\n                planes * block.expansion,\n                kernel_size=1,\n                stride=stride,\n                bias=False),\n            build_norm_layer(normalize, planes * block.expansion)[1],\n        )\n\n    layers = []\n    layers.append(\n        block(\n            inplanes,\n            planes,\n            stride,\n            dilation,\n            downsample,\n            style=style,\n            with_cp=with_cp,\n            normalize=normalize,\n            dcn=dcn))\n    inplanes = planes * block.expansion\n    for i in range(1, blocks):\n        layers.append(\n            block(\n                inplanes,\n                planes,\n                1,\n                dilation,\n                style=style,\n                with_cp=with_cp,\n                normalize=normalize,\n                dcn=dcn))\n    # layers.append(SpatialGroupEnhance(64))\n\n    return nn.Sequential(*layers)\n\n\n@BACKBONES.register_module\nclass ResNetSK(nn.Module):\n    """"""ResNetSK backbone.\n\n    Args:\n        depth (int): Depth of resnet, from {18, 34, 50, 101, 152}.\n        num_stages (int): Resnet stages, normally 4.\n        strides (Sequence[int]): Strides of the first block of each stage.\n        dilations (Sequence[int]): Dilation of each stage.\n        out_indices (Sequence[int]): Output from which stages.\n        style (str): `pytorch` or `caffe`. If set to ""pytorch"", the stride-two\n            layer is the 3x3 conv layer, otherwise the stride-two layer is\n            the first 1x1 conv layer.\n        frozen_stages (int): Stages to be frozen (all param fixed). -1 means\n            not freezing any parameters.\n        normalize (dict): dictionary to construct and config norm layer.\n        norm_eval (bool): Whether to set norm layers to eval mode, namely,\n            freeze running stats (mean and var). Note: Effect on Batch Norm\n            and its variants only.\n        with_cp (bool): Use checkpoint or not. Using checkpoint will save some\n            memory while slowing down the training speed.\n        zero_init_residual (bool): whether to use zero init for last norm layer\n            in resblocks to let them behave as identity.\n    """"""\n\n    arch_settings = {\n        18: (BasicBlock, (2, 2, 2, 2)),\n        34: (BasicBlock, (3, 4, 6, 3)),\n        50: (Bottleneck, (3, 4, 6, 3)),\n        101: (Bottleneck, (3, 4, 23, 3)),\n        152: (Bottleneck, (3, 8, 36, 3))\n    }\n\n    def __init__(self,\n                 depth,\n                 num_stages=4,\n                 strides=(1, 2, 2, 2),\n                 dilations=(1, 1, 1, 1),\n                 out_indices=(0, 1, 2, 3),\n                 style=\'pytorch\',\n                 frozen_stages=-1,\n                 normalize=dict(type=\'BN\', frozen=False),\n                 norm_eval=True,\n                 dcn=None,\n                 stage_with_dcn=(False, False, False, False),\n                 with_cp=False,\n                 zero_init_residual=True):\n        super(ResNetSK, self).__init__()\n        if depth not in self.arch_settings:\n            raise KeyError(\'invalid depth {} for resnet\'.format(depth))\n        self.depth = depth\n        self.num_stages = num_stages\n        assert num_stages >= 1 and num_stages <= 4\n        self.strides = strides\n        self.dilations = dilations\n        assert len(strides) == len(dilations) == num_stages\n        self.out_indices = out_indices\n        assert max(out_indices) < num_stages\n        self.style = style\n        self.frozen_stages = frozen_stages\n        self.normalize = normalize\n        self.with_cp = with_cp\n        self.norm_eval = norm_eval\n        self.dcn = dcn\n        self.stage_with_dcn = stage_with_dcn\n        if dcn is not None:\n            assert len(stage_with_dcn) == num_stages\n        self.zero_init_residual = zero_init_residual\n        self.block, stage_blocks = self.arch_settings[depth]\n        self.stage_blocks = stage_blocks[:num_stages]\n        self.inplanes = 64\n\n        self._make_stem_layer()\n\n        self.res_layers = []\n        for i, num_blocks in enumerate(self.stage_blocks):\n            stride = strides[i]\n            dilation = dilations[i]\n            dcn = self.dcn if self.stage_with_dcn[i] else None\n            planes = 64 * 2**i\n            res_layer = make_res_layer(\n                self.block,\n                self.inplanes,\n                planes,\n                num_blocks,\n                stride=stride,\n                dilation=dilation,\n                style=self.style,\n                with_cp=with_cp,\n                normalize=normalize,\n                dcn=dcn)\n            self.inplanes = planes * self.block.expansion\n            layer_name = \'layer{}\'.format(i + 1)\n            self.add_module(layer_name, res_layer)\n            self.res_layers.append(layer_name)\n\n        self._freeze_stages()\n\n        self.feat_dim = self.block.expansion * 64 * 2**(\n            len(self.stage_blocks) - 1)\n\n    @property\n    def norm1(self):\n        return getattr(self, self.norm1_name)\n\n    def _make_stem_layer(self):\n        self.conv1 = nn.Conv2d(\n            3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n        self.norm1_name, norm1 = build_norm_layer(\n            self.normalize, 64, postfix=1)\n        self.add_module(self.norm1_name, norm1)\n        self.relu = nn.ReLU(inplace=True)\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n\n    def _freeze_stages(self):\n        if self.frozen_stages >= 0:\n            for m in [self.conv1, self.norm1]:\n                for param in m.parameters():\n                    param.requires_grad = False\n\n        for i in range(1, self.frozen_stages + 1):\n            m = getattr(self, \'layer{}\'.format(i))\n            for param in m.parameters():\n                param.requires_grad = False\n\n    def init_weights(self, pretrained=None):\n        if isinstance(pretrained, str):\n            logger = logging.getLogger()\n            load_checkpoint(self, pretrained, strict=False, logger=logger, map_location=torch.device(\'cpu\'))\n        elif pretrained is None:\n            for m in self.modules():\n                if isinstance(m, nn.Conv2d):\n                    kaiming_init(m)\n                elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n                    constant_init(m, 1)\n\n            if self.dcn is not None:\n                for m in self.modules():\n                    if isinstance(m, Bottleneck) and hasattr(\n                            m, \'conv2_offset\'):\n                        constant_init(m.conv2_offset, 0)\n\n            if self.zero_init_residual:\n                for m in self.modules():\n                    if isinstance(m, Bottleneck):\n                        constant_init(m.norm3, 0)\n                    elif isinstance(m, BasicBlock):\n                        constant_init(m.norm2, 0)\n        else:\n            raise TypeError(\'pretrained must be a str or None\')\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.norm1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n        outs = []\n        for i, layer_name in enumerate(self.res_layers):\n            res_layer = getattr(self, layer_name)\n            x = res_layer(x)\n            if i in self.out_indices:\n                outs.append(x)\n        if len(outs) == 1:\n            return outs[0]\n        else:\n            return tuple(outs)\n\n    def train(self, mode=True):\n        super(ResNetSK, self).train(mode)\n        if mode and self.norm_eval:\n            for m in self.modules():\n                # trick: eval have effect on BatchNorm only\n                if isinstance(m, nn.BatchNorm2d):\n                    m.eval()\n'"
detection/mmdet/models/backbones/resnext.py,1,"b'import math\n\nimport torch.nn as nn\n\nfrom mmdet.ops import DeformConv, ModulatedDeformConv\nfrom .resnet import Bottleneck as _Bottleneck\nfrom .resnet import ResNet\nfrom ..registry import BACKBONES\nfrom ..utils import build_norm_layer\n\n\nclass Bottleneck(_Bottleneck):\n\n    def __init__(self, *args, groups=1, base_width=4, **kwargs):\n        """"""Bottleneck block for ResNeXt.\n        If style is ""pytorch"", the stride-two layer is the 3x3 conv layer,\n        if it is ""caffe"", the stride-two layer is the first 1x1 conv layer.\n        """"""\n        super(Bottleneck, self).__init__(*args, **kwargs)\n\n        if groups == 1:\n            width = self.planes\n        else:\n            width = math.floor(self.planes * (base_width / 64)) * groups\n\n        self.norm1_name, norm1 = build_norm_layer(\n            self.normalize, width, postfix=1)\n        self.norm2_name, norm2 = build_norm_layer(\n            self.normalize, width, postfix=2)\n        self.norm3_name, norm3 = build_norm_layer(\n            self.normalize, self.planes * self.expansion, postfix=3)\n\n        self.conv1 = nn.Conv2d(\n            self.inplanes,\n            width,\n            kernel_size=1,\n            stride=self.conv1_stride,\n            bias=False)\n        self.add_module(self.norm1_name, norm1)\n        fallback_on_stride = False\n        self.with_modulated_dcn = False\n        if self.with_dcn:\n            fallback_on_stride = self.dcn.get(\'fallback_on_stride\', False)\n            self.with_modulated_dcn = self.dcn.get(\'modulated\', False)\n        if not self.with_dcn or fallback_on_stride:\n            self.conv2 = nn.Conv2d(\n                width,\n                width,\n                kernel_size=3,\n                stride=self.conv2_stride,\n                padding=self.dilation,\n                dilation=self.dilation,\n                groups=groups,\n                bias=False)\n        else:\n            groups = self.dcn.get(\'groups\', 1)\n            deformable_groups = self.dcn.get(\'deformable_groups\', 1)\n            if not self.with_modulated_dcn:\n                conv_op = DeformConv\n                offset_channels = 18\n            else:\n                conv_op = ModulatedDeformConv\n                offset_channels = 27\n            self.conv2_offset = nn.Conv2d(\n                width,\n                deformable_groups * offset_channels,\n                kernel_size=3,\n                stride=self.conv2_stride,\n                padding=self.dilation,\n                dilation=self.dilation)\n            self.conv2 = conv_op(\n                width,\n                width,\n                kernel_size=3,\n                stride=self.conv2_stride,\n                padding=self.dilation,\n                dilation=self.dilation,\n                groups=groups,\n                deformable_groups=deformable_groups,\n                bias=False)\n        self.add_module(self.norm2_name, norm2)\n        self.conv3 = nn.Conv2d(\n            width, self.planes * self.expansion, kernel_size=1, bias=False)\n        self.add_module(self.norm3_name, norm3)\n\n\ndef make_res_layer(block,\n                   inplanes,\n                   planes,\n                   blocks,\n                   stride=1,\n                   dilation=1,\n                   groups=1,\n                   base_width=4,\n                   style=\'pytorch\',\n                   with_cp=False,\n                   normalize=dict(type=\'BN\'),\n                   dcn=None):\n    downsample = None\n    if stride != 1 or inplanes != planes * block.expansion:\n        downsample = nn.Sequential(\n            nn.Conv2d(\n                inplanes,\n                planes * block.expansion,\n                kernel_size=1,\n                stride=stride,\n                bias=False),\n            build_norm_layer(normalize, planes * block.expansion)[1],\n        )\n\n    layers = []\n    layers.append(\n        block(\n            inplanes,\n            planes,\n            stride=stride,\n            dilation=dilation,\n            downsample=downsample,\n            groups=groups,\n            base_width=base_width,\n            style=style,\n            with_cp=with_cp,\n            normalize=normalize,\n            dcn=dcn))\n    inplanes = planes * block.expansion\n    for i in range(1, blocks):\n        layers.append(\n            block(\n                inplanes,\n                planes,\n                stride=1,\n                dilation=dilation,\n                groups=groups,\n                base_width=base_width,\n                style=style,\n                with_cp=with_cp,\n                normalize=normalize,\n                dcn=dcn))\n\n    return nn.Sequential(*layers)\n\n\n@BACKBONES.register_module\nclass ResNeXt(ResNet):\n    """"""ResNeXt backbone.\n\n    Args:\n        depth (int): Depth of resnet, from {18, 34, 50, 101, 152}.\n        num_stages (int): Resnet stages, normally 4.\n        groups (int): Group of resnext.\n        base_width (int): Base width of resnext.\n        strides (Sequence[int]): Strides of the first block of each stage.\n        dilations (Sequence[int]): Dilation of each stage.\n        out_indices (Sequence[int]): Output from which stages.\n        style (str): `pytorch` or `caffe`. If set to ""pytorch"", the stride-two\n            layer is the 3x3 conv layer, otherwise the stride-two layer is\n            the first 1x1 conv layer.\n        frozen_stages (int): Stages to be frozen (all param fixed). -1 means\n            not freezing any parameters.\n        normalize (dict): dictionary to construct and config norm layer.\n        norm_eval (bool): Whether to set norm layers to eval mode, namely,\n            freeze running stats (mean and var). Note: Effect on Batch Norm\n            and its variants only.\n        with_cp (bool): Use checkpoint or not. Using checkpoint will save some\n            memory while slowing down the training speed.\n        zero_init_residual (bool): whether to use zero init for last norm layer\n            in resblocks to let them behave as identity.\n    """"""\n\n    arch_settings = {\n        50: (Bottleneck, (3, 4, 6, 3)),\n        101: (Bottleneck, (3, 4, 23, 3)),\n        152: (Bottleneck, (3, 8, 36, 3))\n    }\n\n    def __init__(self, groups=1, base_width=4, **kwargs):\n        super(ResNeXt, self).__init__(**kwargs)\n        self.groups = groups\n        self.base_width = base_width\n\n        self.inplanes = 64\n        self.res_layers = []\n        for i, num_blocks in enumerate(self.stage_blocks):\n            stride = self.strides[i]\n            dilation = self.dilations[i]\n            dcn = self.dcn if self.stage_with_dcn[i] else None\n            planes = 64 * 2**i\n            res_layer = make_res_layer(\n                self.block,\n                self.inplanes,\n                planes,\n                num_blocks,\n                stride=stride,\n                dilation=dilation,\n                groups=self.groups,\n                base_width=self.base_width,\n                style=self.style,\n                with_cp=self.with_cp,\n                normalize=self.normalize,\n                dcn=dcn)\n            self.inplanes = planes * self.block.expansion\n            layer_name = \'layer{}\'.format(i + 1)\n            self.add_module(layer_name, res_layer)\n            self.res_layers.append(layer_name)\n\n        self._freeze_stages()\n'"
detection/mmdet/models/backbones/ssd_vgg.py,3,"b""import logging\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom mmcv.cnn import (VGG, xavier_init, constant_init, kaiming_init,\n                      normal_init)\nfrom mmcv.runner import load_checkpoint\nfrom ..registry import BACKBONES\n\n\n@BACKBONES.register_module\nclass SSDVGG(VGG):\n    extra_setting = {\n        300: (256, 'S', 512, 128, 'S', 256, 128, 256, 128, 256),\n        512: (256, 'S', 512, 128, 'S', 256, 128, 'S', 256, 128, 'S', 256, 128),\n    }\n\n    def __init__(self,\n                 input_size,\n                 depth,\n                 with_last_pool=False,\n                 ceil_mode=True,\n                 out_indices=(3, 4),\n                 out_feature_indices=(22, 34),\n                 l2_norm_scale=20.):\n        super(SSDVGG, self).__init__(\n            depth,\n            with_last_pool=with_last_pool,\n            ceil_mode=ceil_mode,\n            out_indices=out_indices)\n        assert input_size in (300, 512)\n        self.input_size = input_size\n\n        self.features.add_module(\n            str(len(self.features)),\n            nn.MaxPool2d(kernel_size=3, stride=1, padding=1))\n        self.features.add_module(\n            str(len(self.features)),\n            nn.Conv2d(512, 1024, kernel_size=3, padding=6, dilation=6))\n        self.features.add_module(\n            str(len(self.features)), nn.ReLU(inplace=True))\n        self.features.add_module(\n            str(len(self.features)), nn.Conv2d(1024, 1024, kernel_size=1))\n        self.features.add_module(\n            str(len(self.features)), nn.ReLU(inplace=True))\n        self.out_feature_indices = out_feature_indices\n\n        self.inplanes = 1024\n        self.extra = self._make_extra_layers(self.extra_setting[input_size])\n        self.l2_norm = L2Norm(\n            self.features[out_feature_indices[0] - 1].out_channels,\n            l2_norm_scale)\n\n    def init_weights(self, pretrained=None):\n        if isinstance(pretrained, str):\n            logger = logging.getLogger()\n            load_checkpoint(self, pretrained, strict=False, logger=logger)\n        elif pretrained is None:\n            for m in self.features.modules():\n                if isinstance(m, nn.Conv2d):\n                    kaiming_init(m)\n                elif isinstance(m, nn.BatchNorm2d):\n                    constant_init(m, 1)\n                elif isinstance(m, nn.Linear):\n                    normal_init(m, std=0.01)\n        else:\n            raise TypeError('pretrained must be a str or None')\n\n        for m in self.extra.modules():\n            if isinstance(m, nn.Conv2d):\n                xavier_init(m, distribution='uniform')\n\n        constant_init(self.l2_norm, self.l2_norm.scale)\n\n    def forward(self, x):\n        outs = []\n        for i, layer in enumerate(self.features):\n            x = layer(x)\n            if i in self.out_feature_indices:\n                outs.append(x)\n        for i, layer in enumerate(self.extra):\n            x = F.relu(layer(x), inplace=True)\n            if i % 2 == 1:\n                outs.append(x)\n        outs[0] = self.l2_norm(outs[0])\n        if len(outs) == 1:\n            return outs[0]\n        else:\n            return tuple(outs)\n\n    def _make_extra_layers(self, outplanes):\n        layers = []\n        kernel_sizes = (1, 3)\n        num_layers = 0\n        outplane = None\n        for i in range(len(outplanes)):\n            if self.inplanes == 'S':\n                self.inplanes = outplane\n                continue\n            k = kernel_sizes[num_layers % 2]\n            if outplanes[i] == 'S':\n                outplane = outplanes[i + 1]\n                conv = nn.Conv2d(\n                    self.inplanes, outplane, k, stride=2, padding=1)\n            else:\n                outplane = outplanes[i]\n                conv = nn.Conv2d(\n                    self.inplanes, outplane, k, stride=1, padding=0)\n            layers.append(conv)\n            self.inplanes = outplanes[i]\n            num_layers += 1\n        if self.input_size == 512:\n            layers.append(nn.Conv2d(self.inplanes, 256, 4, padding=1))\n\n        return nn.Sequential(*layers)\n\n\nclass L2Norm(nn.Module):\n\n    def __init__(self, n_dims, scale=20., eps=1e-10):\n        super(L2Norm, self).__init__()\n        self.n_dims = n_dims\n        self.weight = nn.Parameter(torch.Tensor(self.n_dims))\n        self.eps = eps\n        self.scale = scale\n\n    def forward(self, x):\n        norm = x.pow(2).sum(1, keepdim=True).sqrt() + self.eps\n        return self.weight[None, :, None, None].expand_as(x) * x / norm\n"""
detection/mmdet/models/bbox_heads/__init__.py,0,"b""from .bbox_head import BBoxHead\nfrom .convfc_bbox_head import ConvFCBBoxHead, SharedFCBBoxHead\n\n__all__ = ['BBoxHead', 'ConvFCBBoxHead', 'SharedFCBBoxHead']\n"""
detection/mmdet/models/bbox_heads/bbox_head.py,6,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom mmdet.core import (delta2bbox, multiclass_nms, bbox_target,\n                        weighted_cross_entropy, weighted_smoothl1, accuracy)\nfrom ..registry import HEADS\n\n\n@HEADS.register_module\nclass BBoxHead(nn.Module):\n    """"""Simplest RoI head, with only two fc layers for classification and\n    regression respectively""""""\n\n    def __init__(self,\n                 with_avg_pool=False,\n                 with_cls=True,\n                 with_reg=True,\n                 roi_feat_size=7,\n                 in_channels=256,\n                 num_classes=81,\n                 target_means=[0., 0., 0., 0.],\n                 target_stds=[0.1, 0.1, 0.2, 0.2],\n                 reg_class_agnostic=False):\n        super(BBoxHead, self).__init__()\n        assert with_cls or with_reg\n        self.with_avg_pool = with_avg_pool\n        self.with_cls = with_cls\n        self.with_reg = with_reg\n        self.roi_feat_size = roi_feat_size\n        self.in_channels = in_channels\n        self.num_classes = num_classes\n        self.target_means = target_means\n        self.target_stds = target_stds\n        self.reg_class_agnostic = reg_class_agnostic\n\n        in_channels = self.in_channels\n        if self.with_avg_pool:\n            self.avg_pool = nn.AvgPool2d(roi_feat_size)\n        else:\n            in_channels *= (self.roi_feat_size * self.roi_feat_size)\n        if self.with_cls:\n            self.fc_cls = nn.Linear(in_channels, num_classes)\n        if self.with_reg:\n            out_dim_reg = 4 if reg_class_agnostic else 4 * num_classes\n            self.fc_reg = nn.Linear(in_channels, out_dim_reg)\n        self.debug_imgs = None\n\n    def init_weights(self):\n        if self.with_cls:\n            nn.init.normal_(self.fc_cls.weight, 0, 0.01)\n            nn.init.constant_(self.fc_cls.bias, 0)\n        if self.with_reg:\n            nn.init.normal_(self.fc_reg.weight, 0, 0.001)\n            nn.init.constant_(self.fc_reg.bias, 0)\n\n    def forward(self, x):\n        if self.with_avg_pool:\n            x = self.avg_pool(x)\n        x = x.view(x.size(0), -1)\n        cls_score = self.fc_cls(x) if self.with_cls else None\n        bbox_pred = self.fc_reg(x) if self.with_reg else None\n        return cls_score, bbox_pred\n\n    def get_target(self, sampling_results, gt_bboxes, gt_labels,\n                   rcnn_train_cfg):\n        pos_proposals = [res.pos_bboxes for res in sampling_results]\n        neg_proposals = [res.neg_bboxes for res in sampling_results]\n        pos_gt_bboxes = [res.pos_gt_bboxes for res in sampling_results]\n        pos_gt_labels = [res.pos_gt_labels for res in sampling_results]\n        reg_classes = 1 if self.reg_class_agnostic else self.num_classes\n        cls_reg_targets = bbox_target(\n            pos_proposals,\n            neg_proposals,\n            pos_gt_bboxes,\n            pos_gt_labels,\n            rcnn_train_cfg,\n            reg_classes,\n            target_means=self.target_means,\n            target_stds=self.target_stds)\n        return cls_reg_targets\n\n    def loss(self,\n             cls_score,\n             bbox_pred,\n             labels,\n             label_weights,\n             bbox_targets,\n             bbox_weights,\n             reduce=True):\n        losses = dict()\n        if cls_score is not None:\n            losses[\'loss_cls\'] = weighted_cross_entropy(\n                cls_score, labels, label_weights, reduce=reduce)\n            losses[\'acc\'] = accuracy(cls_score, labels)\n        if bbox_pred is not None:\n            losses[\'loss_reg\'] = weighted_smoothl1(\n                bbox_pred,\n                bbox_targets,\n                bbox_weights,\n                avg_factor=bbox_targets.size(0))\n        return losses\n\n    def get_det_bboxes(self,\n                       rois,\n                       cls_score,\n                       bbox_pred,\n                       img_shape,\n                       scale_factor,\n                       rescale=False,\n                       cfg=None):\n        if isinstance(cls_score, list):\n            cls_score = sum(cls_score) / float(len(cls_score))\n        scores = F.softmax(cls_score, dim=1) if cls_score is not None else None\n\n        if bbox_pred is not None:\n            bboxes = delta2bbox(rois[:, 1:], bbox_pred, self.target_means,\n                                self.target_stds, img_shape)\n        else:\n            bboxes = rois[:, 1:]\n            # TODO: add clip here\n\n        if rescale:\n            bboxes /= scale_factor\n\n        if cfg is None:\n            return bboxes, scores\n        else:\n            det_bboxes, det_labels = multiclass_nms(\n                bboxes, scores, cfg.score_thr, cfg.nms, cfg.max_per_img)\n\n            return det_bboxes, det_labels\n\n    def refine_bboxes(self, rois, labels, bbox_preds, pos_is_gts, img_metas):\n        """"""Refine bboxes during training.\n\n        Args:\n            rois (Tensor): Shape (n*bs, 5), where n is image number per GPU,\n                and bs is the sampled RoIs per image.\n            labels (Tensor): Shape (n*bs, ).\n            bbox_preds (Tensor): Shape (n*bs, 4) or (n*bs, 4*#class).\n            pos_is_gts (list[Tensor]): Flags indicating if each positive bbox\n                is a gt bbox.\n            img_metas (list[dict]): Meta info of each image.\n\n        Returns:\n            list[Tensor]: Refined bboxes of each image in a mini-batch.\n        """"""\n        img_ids = rois[:, 0].long().unique(sorted=True)\n        assert img_ids.numel() == len(img_metas)\n\n        bboxes_list = []\n        for i in range(len(img_metas)):\n            inds = torch.nonzero(rois[:, 0] == i).squeeze()\n            num_rois = inds.numel()\n\n            bboxes_ = rois[inds, 1:]\n            label_ = labels[inds]\n            bbox_pred_ = bbox_preds[inds]\n            img_meta_ = img_metas[i]\n            pos_is_gts_ = pos_is_gts[i]\n\n            bboxes = self.regress_by_class(bboxes_, label_, bbox_pred_,\n                                           img_meta_)\n            # filter gt bboxes\n            pos_keep = 1 - pos_is_gts_\n            keep_inds = pos_is_gts_.new_ones(num_rois)\n            keep_inds[:len(pos_is_gts_)] = pos_keep\n\n            bboxes_list.append(bboxes[keep_inds])\n\n        return bboxes_list\n\n    def regress_by_class(self, rois, label, bbox_pred, img_meta):\n        """"""Regress the bbox for the predicted class. Used in Cascade R-CNN.\n\n        Args:\n            rois (Tensor): shape (n, 4) or (n, 5)\n            label (Tensor): shape (n, )\n            bbox_pred (Tensor): shape (n, 4*(#class+1)) or (n, 4)\n            img_meta (dict): Image meta info.\n\n        Returns:\n            Tensor: Regressed bboxes, the same shape as input rois.\n        """"""\n        assert rois.size(1) == 4 or rois.size(1) == 5\n\n        if not self.reg_class_agnostic:\n            label = label * 4\n            inds = torch.stack((label, label + 1, label + 2, label + 3), 1)\n            bbox_pred = torch.gather(bbox_pred, 1, inds)\n        assert bbox_pred.size(1) == 4\n\n        if rois.size(1) == 4:\n            new_rois = delta2bbox(rois, bbox_pred, self.target_means,\n                                  self.target_stds, img_meta[\'img_shape\'])\n        else:\n            bboxes = delta2bbox(rois[:, 1:], bbox_pred, self.target_means,\n                                self.target_stds, img_meta[\'img_shape\'])\n            new_rois = torch.cat((rois[:, [0]], bboxes), dim=1)\n\n        return new_rois\n'"
detection/mmdet/models/bbox_heads/convfc_bbox_head.py,1,"b'import torch.nn as nn\n\nfrom .bbox_head import BBoxHead\nfrom ..registry import HEADS\nfrom ..utils import ConvModule\n\n\n@HEADS.register_module\nclass ConvFCBBoxHead(BBoxHead):\n    """"""More general bbox head, with shared conv and fc layers and two optional\n    separated branches.\n\n                                /-> cls convs -> cls fcs -> cls\n    shared convs -> shared fcs\n                                \\-> reg convs -> reg fcs -> reg\n    """"""  # noqa: W605\n\n    def __init__(self,\n                 num_shared_convs=0,\n                 num_shared_fcs=0,\n                 num_cls_convs=0,\n                 num_cls_fcs=0,\n                 num_reg_convs=0,\n                 num_reg_fcs=0,\n                 conv_out_channels=256,\n                 fc_out_channels=1024,\n                 normalize=None,\n                 *args,\n                 **kwargs):\n        super(ConvFCBBoxHead, self).__init__(*args, **kwargs)\n        assert (num_shared_convs + num_shared_fcs + num_cls_convs + num_cls_fcs\n                + num_reg_convs + num_reg_fcs > 0)\n        if num_cls_convs > 0 or num_reg_convs > 0:\n            assert num_shared_fcs == 0\n        if not self.with_cls:\n            assert num_cls_convs == 0 and num_cls_fcs == 0\n        if not self.with_reg:\n            assert num_reg_convs == 0 and num_reg_fcs == 0\n        self.num_shared_convs = num_shared_convs\n        self.num_shared_fcs = num_shared_fcs\n        self.num_cls_convs = num_cls_convs\n        self.num_cls_fcs = num_cls_fcs\n        self.num_reg_convs = num_reg_convs\n        self.num_reg_fcs = num_reg_fcs\n        self.conv_out_channels = conv_out_channels\n        self.fc_out_channels = fc_out_channels\n        self.normalize = normalize\n        self.with_bias = normalize is None\n\n        # add shared convs and fcs\n        self.shared_convs, self.shared_fcs, last_layer_dim = \\\n            self._add_conv_fc_branch(\n                self.num_shared_convs, self.num_shared_fcs, self.in_channels,\n                True)\n        self.shared_out_channels = last_layer_dim\n\n        # add cls specific branch\n        self.cls_convs, self.cls_fcs, self.cls_last_dim = \\\n            self._add_conv_fc_branch(\n                self.num_cls_convs, self.num_cls_fcs, self.shared_out_channels)\n\n        # add reg specific branch\n        self.reg_convs, self.reg_fcs, self.reg_last_dim = \\\n            self._add_conv_fc_branch(\n                self.num_reg_convs, self.num_reg_fcs, self.shared_out_channels)\n\n        if self.num_shared_fcs == 0 and not self.with_avg_pool:\n            if self.num_cls_fcs == 0:\n                self.cls_last_dim *= (self.roi_feat_size * self.roi_feat_size)\n            if self.num_reg_fcs == 0:\n                self.reg_last_dim *= (self.roi_feat_size * self.roi_feat_size)\n\n        self.relu = nn.ReLU(inplace=True)\n        # reconstruct fc_cls and fc_reg since input channels are changed\n        if self.with_cls:\n            self.fc_cls = nn.Linear(self.cls_last_dim, self.num_classes)\n        if self.with_reg:\n            out_dim_reg = (4 if self.reg_class_agnostic else\n                           4 * self.num_classes)\n            self.fc_reg = nn.Linear(self.reg_last_dim, out_dim_reg)\n\n    def _add_conv_fc_branch(self,\n                            num_branch_convs,\n                            num_branch_fcs,\n                            in_channels,\n                            is_shared=False):\n        """"""Add shared or separable branch\n\n        convs -> avg pool (optional) -> fcs\n        """"""\n        last_layer_dim = in_channels\n        # add branch specific conv layers\n        branch_convs = nn.ModuleList()\n        if num_branch_convs > 0:\n            for i in range(num_branch_convs):\n                conv_in_channels = (last_layer_dim\n                                    if i == 0 else self.conv_out_channels)\n                branch_convs.append(\n                    ConvModule(\n                        conv_in_channels,\n                        self.conv_out_channels,\n                        3,\n                        padding=1,\n                        normalize=self.normalize,\n                        bias=self.with_bias))\n            last_layer_dim = self.conv_out_channels\n        # add branch specific fc layers\n        branch_fcs = nn.ModuleList()\n        if num_branch_fcs > 0:\n            # for shared branch, only consider self.with_avg_pool\n            # for separated branches, also consider self.num_shared_fcs\n            if (is_shared\n                    or self.num_shared_fcs == 0) and not self.with_avg_pool:\n                last_layer_dim *= (self.roi_feat_size * self.roi_feat_size)\n            for i in range(num_branch_fcs):\n                fc_in_channels = (last_layer_dim\n                                  if i == 0 else self.fc_out_channels)\n                branch_fcs.append(\n                    nn.Linear(fc_in_channels, self.fc_out_channels))\n            last_layer_dim = self.fc_out_channels\n        return branch_convs, branch_fcs, last_layer_dim\n\n    def init_weights(self):\n        super(ConvFCBBoxHead, self).init_weights()\n        for module_list in [self.shared_fcs, self.cls_fcs, self.reg_fcs]:\n            for m in module_list.modules():\n                if isinstance(m, nn.Linear):\n                    nn.init.xavier_uniform_(m.weight)\n                    nn.init.constant_(m.bias, 0)\n\n    def forward(self, x):\n        # shared part\n        if self.num_shared_convs > 0:\n            for conv in self.shared_convs:\n                x = conv(x)\n\n        if self.num_shared_fcs > 0:\n            if self.with_avg_pool:\n                x = self.avg_pool(x)\n            x = x.view(x.size(0), -1)\n            for fc in self.shared_fcs:\n                x = self.relu(fc(x))\n        # separate branches\n        x_cls = x\n        x_reg = x\n\n        for conv in self.cls_convs:\n            x_cls = conv(x_cls)\n        if x_cls.dim() > 2:\n            if self.with_avg_pool:\n                x_cls = self.avg_pool(x_cls)\n            x_cls = x_cls.view(x_cls.size(0), -1)\n        for fc in self.cls_fcs:\n            x_cls = self.relu(fc(x_cls))\n\n        for conv in self.reg_convs:\n            x_reg = conv(x_reg)\n        if x_reg.dim() > 2:\n            if self.with_avg_pool:\n                x_reg = self.avg_pool(x_reg)\n            x_reg = x_reg.view(x_reg.size(0), -1)\n        for fc in self.reg_fcs:\n            x_reg = self.relu(fc(x_reg))\n\n        cls_score = self.fc_cls(x_cls) if self.with_cls else None\n        bbox_pred = self.fc_reg(x_reg) if self.with_reg else None\n        return cls_score, bbox_pred\n\n\n@HEADS.register_module\nclass SharedFCBBoxHead(ConvFCBBoxHead):\n\n    def __init__(self, num_fcs=2, fc_out_channels=1024, *args, **kwargs):\n        assert num_fcs >= 1\n        super(SharedFCBBoxHead, self).__init__(\n            num_shared_convs=0,\n            num_shared_fcs=num_fcs,\n            num_cls_convs=0,\n            num_cls_fcs=0,\n            num_reg_convs=0,\n            num_reg_fcs=0,\n            fc_out_channels=fc_out_channels,\n            *args,\n            **kwargs)\n'"
detection/mmdet/models/detectors/__init__.py,0,"b""from .base import BaseDetector\nfrom .single_stage import SingleStageDetector\nfrom .two_stage import TwoStageDetector\nfrom .rpn import RPN\nfrom .fast_rcnn import FastRCNN\nfrom .faster_rcnn import FasterRCNN\nfrom .mask_rcnn import MaskRCNN\nfrom .cascade_rcnn import CascadeRCNN\nfrom .retinanet import RetinaNet\n\n__all__ = [\n    'BaseDetector', 'SingleStageDetector', 'TwoStageDetector', 'RPN',\n    'FastRCNN', 'FasterRCNN', 'MaskRCNN', 'CascadeRCNN', 'RetinaNet'\n]\n"""
detection/mmdet/models/detectors/base.py,1,"b'import logging\nfrom abc import ABCMeta, abstractmethod\n\nimport mmcv\nimport numpy as np\nimport torch.nn as nn\nimport pycocotools.mask as maskUtils\n\nfrom mmdet.core import tensor2imgs, get_classes\n\n\nclass BaseDetector(nn.Module):\n    """"""Base class for detectors""""""\n\n    __metaclass__ = ABCMeta\n\n    def __init__(self):\n        super(BaseDetector, self).__init__()\n\n    @property\n    def with_neck(self):\n        return hasattr(self, \'neck\') and self.neck is not None\n\n    @property\n    def with_bbox(self):\n        return hasattr(self, \'bbox_head\') and self.bbox_head is not None\n\n    @property\n    def with_mask(self):\n        return hasattr(self, \'mask_head\') and self.mask_head is not None\n\n    @abstractmethod\n    def extract_feat(self, imgs):\n        pass\n\n    def extract_feats(self, imgs):\n        assert isinstance(imgs, list)\n        for img in imgs:\n            yield self.extract_feat(img)\n\n    @abstractmethod\n    def forward_train(self, imgs, img_metas, **kwargs):\n        pass\n\n    @abstractmethod\n    def simple_test(self, img, img_meta, **kwargs):\n        pass\n\n    @abstractmethod\n    def aug_test(self, imgs, img_metas, **kwargs):\n        pass\n\n    def init_weights(self, pretrained=None):\n        if pretrained is not None:\n            logger = logging.getLogger()\n            logger.info(\'load model from: {}\'.format(pretrained))\n\n    def forward_test(self, imgs, img_metas, **kwargs):\n        for var, name in [(imgs, \'imgs\'), (img_metas, \'img_metas\')]:\n            if not isinstance(var, list):\n                raise TypeError(\'{} must be a list, but got {}\'.format(\n                    name, type(var)))\n\n        num_augs = len(imgs)\n        if num_augs != len(img_metas):\n            raise ValueError(\n                \'num of augmentations ({}) != num of image meta ({})\'.format(\n                    len(imgs), len(img_metas)))\n        # TODO: remove the restriction of imgs_per_gpu == 1 when prepared\n        imgs_per_gpu = imgs[0].size(0)\n        assert imgs_per_gpu == 1\n\n        if num_augs == 1:\n            return self.simple_test(imgs[0], img_metas[0], **kwargs)\n        else:\n            return self.aug_test(imgs, img_metas, **kwargs)\n\n    def forward(self, img, img_meta, return_loss=True, **kwargs):\n        if return_loss:\n            return self.forward_train(img, img_meta, **kwargs)\n        else:\n            return self.forward_test(img, img_meta, **kwargs)\n\n    def show_result(self,\n                    data,\n                    result,\n                    img_norm_cfg,\n                    dataset=\'coco\',\n                    score_thr=0.3):\n        if isinstance(result, tuple):\n            bbox_result, segm_result = result\n        else:\n            bbox_result, segm_result = result, None\n\n        img_tensor = data[\'img\'][0]\n        img_metas = data[\'img_meta\'][0].data[0]\n        imgs = tensor2imgs(img_tensor, **img_norm_cfg)\n        assert len(imgs) == len(img_metas)\n\n        if isinstance(dataset, str):\n            class_names = get_classes(dataset)\n        elif isinstance(dataset, (list, tuple)) or dataset is None:\n            class_names = dataset\n        else:\n            raise TypeError(\n                \'dataset must be a valid dataset name or a sequence\'\n                \' of class names, not {}\'.format(type(dataset)))\n\n        for img, img_meta in zip(imgs, img_metas):\n            h, w, _ = img_meta[\'img_shape\']\n            img_show = img[:h, :w, :]\n\n            bboxes = np.vstack(bbox_result)\n            # draw segmentation masks\n            if segm_result is not None:\n                segms = mmcv.concat_list(segm_result)\n                inds = np.where(bboxes[:, -1] > score_thr)[0]\n                for i in inds:\n                    color_mask = np.random.randint(\n                        0, 256, (1, 3), dtype=np.uint8)\n                    mask = maskUtils.decode(segms[i]).astype(np.bool)\n                    img_show[mask] = img_show[mask] * 0.5 + color_mask * 0.5\n            # draw bounding boxes\n            labels = [\n                np.full(bbox.shape[0], i, dtype=np.int32)\n                for i, bbox in enumerate(bbox_result)\n            ]\n            labels = np.concatenate(labels)\n            mmcv.imshow_det_bboxes(\n                img_show,\n                bboxes,\n                labels,\n                class_names=class_names,\n                score_thr=score_thr)\n'"
detection/mmdet/models/detectors/cascade_rcnn.py,3,"b'from __future__ import division\n\nimport torch\nimport torch.nn as nn\n\nfrom .base import BaseDetector\nfrom .test_mixins import RPNTestMixin\nfrom .. import builder\nfrom ..registry import DETECTORS\nfrom mmdet.core import (assign_and_sample, bbox2roi, bbox2result, multi_apply,\n                        merge_aug_masks)\n\n\n@DETECTORS.register_module\nclass CascadeRCNN(BaseDetector, RPNTestMixin):\n\n    def __init__(self,\n                 num_stages,\n                 backbone,\n                 neck=None,\n                 rpn_head=None,\n                 bbox_roi_extractor=None,\n                 bbox_head=None,\n                 mask_roi_extractor=None,\n                 mask_head=None,\n                 train_cfg=None,\n                 test_cfg=None,\n                 pretrained=None):\n        assert bbox_roi_extractor is not None\n        assert bbox_head is not None\n        super(CascadeRCNN, self).__init__()\n\n        self.num_stages = num_stages\n        self.backbone = builder.build_backbone(backbone)\n\n        if neck is not None:\n            self.neck = builder.build_neck(neck)\n        else:\n            raise NotImplementedError\n\n        if rpn_head is not None:\n            self.rpn_head = builder.build_head(rpn_head)\n\n        if bbox_head is not None:\n            self.bbox_roi_extractor = nn.ModuleList()\n            self.bbox_head = nn.ModuleList()\n            if not isinstance(bbox_roi_extractor, list):\n                bbox_roi_extractor = [\n                    bbox_roi_extractor for _ in range(num_stages)\n                ]\n            if not isinstance(bbox_head, list):\n                bbox_head = [bbox_head for _ in range(num_stages)]\n            assert len(bbox_roi_extractor) == len(bbox_head) == self.num_stages\n            for roi_extractor, head in zip(bbox_roi_extractor, bbox_head):\n                self.bbox_roi_extractor.append(\n                    builder.build_roi_extractor(roi_extractor))\n                self.bbox_head.append(builder.build_head(head))\n\n        if mask_head is not None:\n            self.mask_roi_extractor = nn.ModuleList()\n            self.mask_head = nn.ModuleList()\n            if not isinstance(mask_roi_extractor, list):\n                mask_roi_extractor = [\n                    mask_roi_extractor for _ in range(num_stages)\n                ]\n            if not isinstance(mask_head, list):\n                mask_head = [mask_head for _ in range(num_stages)]\n            assert len(mask_roi_extractor) == len(mask_head) == self.num_stages\n            for roi_extractor, head in zip(mask_roi_extractor, mask_head):\n                self.mask_roi_extractor.append(\n                    builder.build_roi_extractor(roi_extractor))\n                self.mask_head.append(builder.build_head(head))\n\n        self.train_cfg = train_cfg\n        self.test_cfg = test_cfg\n\n        self.init_weights(pretrained=pretrained)\n\n    @property\n    def with_rpn(self):\n        return hasattr(self, \'rpn_head\') and self.rpn_head is not None\n\n    def init_weights(self, pretrained=None):\n        super(CascadeRCNN, self).init_weights(pretrained)\n        self.backbone.init_weights(pretrained=pretrained)\n        if self.with_neck:\n            if isinstance(self.neck, nn.Sequential):\n                for m in self.neck:\n                    m.init_weights()\n            else:\n                self.neck.init_weights()\n        if self.with_rpn:\n            self.rpn_head.init_weights()\n        for i in range(self.num_stages):\n            if self.with_bbox:\n                self.bbox_roi_extractor[i].init_weights()\n                self.bbox_head[i].init_weights()\n            if self.with_mask:\n                self.mask_roi_extractor[i].init_weights()\n                self.mask_head[i].init_weights()\n\n    def extract_feat(self, img):\n        x = self.backbone(img)\n        if self.with_neck:\n            x = self.neck(x)\n        return x\n\n    def forward_train(self,\n                      img,\n                      img_meta,\n                      gt_bboxes,\n                      gt_labels,\n                      gt_bboxes_ignore=None,\n                      gt_masks=None,\n                      proposals=None):\n        x = self.extract_feat(img)\n\n        losses = dict()\n\n        if self.with_rpn:\n            rpn_outs = self.rpn_head(x)\n            rpn_loss_inputs = rpn_outs + (gt_bboxes, img_meta,\n                                          self.train_cfg.rpn)\n            rpn_losses = self.rpn_head.loss(\n                *rpn_loss_inputs, gt_bboxes_ignore=gt_bboxes_ignore)\n            losses.update(rpn_losses)\n\n            proposal_inputs = rpn_outs + (img_meta, self.test_cfg.rpn)\n            proposal_list = self.rpn_head.get_bboxes(*proposal_inputs)\n        else:\n            proposal_list = proposals\n\n        for i in range(self.num_stages):\n            rcnn_train_cfg = self.train_cfg.rcnn[i]\n            lw = self.train_cfg.stage_loss_weights[i]\n\n            # assign gts and sample proposals\n            assign_results, sampling_results = multi_apply(\n                assign_and_sample,\n                proposal_list,\n                gt_bboxes,\n                gt_bboxes_ignore,\n                gt_labels,\n                cfg=rcnn_train_cfg)\n\n            # bbox head forward and loss\n            bbox_roi_extractor = self.bbox_roi_extractor[i]\n            bbox_head = self.bbox_head[i]\n\n            rois = bbox2roi([res.bboxes for res in sampling_results])\n            bbox_feats = bbox_roi_extractor(x[:bbox_roi_extractor.num_inputs],\n                                            rois)\n            cls_score, bbox_pred = bbox_head(bbox_feats)\n\n            bbox_targets = bbox_head.get_target(sampling_results, gt_bboxes,\n                                                gt_labels, rcnn_train_cfg)\n            loss_bbox = bbox_head.loss(cls_score, bbox_pred, *bbox_targets)\n            for name, value in loss_bbox.items():\n                losses[\'s{}.{}\'.format(i, name)] = (value * lw if\n                                                    \'loss\' in name else value)\n\n            # mask head forward and loss\n            if self.with_mask:\n                mask_roi_extractor = self.mask_roi_extractor[i]\n                mask_head = self.mask_head[i]\n                pos_rois = bbox2roi(\n                    [res.pos_bboxes for res in sampling_results])\n                mask_feats = mask_roi_extractor(\n                    x[:mask_roi_extractor.num_inputs], pos_rois)\n                mask_pred = mask_head(mask_feats)\n                mask_targets = mask_head.get_target(sampling_results, gt_masks,\n                                                    rcnn_train_cfg)\n                pos_labels = torch.cat(\n                    [res.pos_gt_labels for res in sampling_results])\n                loss_mask = mask_head.loss(mask_pred, mask_targets, pos_labels)\n                for name, value in loss_mask.items():\n                    losses[\'s{}.{}\'.format(i, name)] = (value * lw\n                                                        if \'loss\' in name else\n                                                        value)\n\n            # refine bboxes\n            if i < self.num_stages - 1:\n                pos_is_gts = [res.pos_is_gt for res in sampling_results]\n                roi_labels = bbox_targets[0]  # bbox_targets is a tuple\n                with torch.no_grad():\n                    proposal_list = bbox_head.refine_bboxes(\n                        rois, roi_labels, bbox_pred, pos_is_gts, img_meta)\n\n        return losses\n\n    def simple_test(self, img, img_meta, proposals=None, rescale=False):\n        x = self.extract_feat(img)\n        proposal_list = self.simple_test_rpn(\n            x, img_meta, self.test_cfg.rpn) if proposals is None else proposals\n\n        img_shape = img_meta[0][\'img_shape\']\n        ori_shape = img_meta[0][\'ori_shape\']\n        scale_factor = img_meta[0][\'scale_factor\']\n\n        # ""ms"" in variable names means multi-stage\n        ms_bbox_result = {}\n        ms_segm_result = {}\n        ms_scores = []\n        rcnn_test_cfg = self.test_cfg.rcnn\n\n        rois = bbox2roi(proposal_list)\n        for i in range(self.num_stages):\n            bbox_roi_extractor = self.bbox_roi_extractor[i]\n            bbox_head = self.bbox_head[i]\n\n            bbox_feats = bbox_roi_extractor(\n                x[:len(bbox_roi_extractor.featmap_strides)], rois)\n            cls_score, bbox_pred = bbox_head(bbox_feats)\n            ms_scores.append(cls_score)\n\n            if self.test_cfg.keep_all_stages:\n                det_bboxes, det_labels = bbox_head.get_det_bboxes(\n                    rois,\n                    cls_score,\n                    bbox_pred,\n                    img_shape,\n                    scale_factor,\n                    rescale=rescale,\n                    cfg=rcnn_test_cfg)\n                bbox_result = bbox2result(det_bboxes, det_labels,\n                                          bbox_head.num_classes)\n                ms_bbox_result[\'stage{}\'.format(i)] = bbox_result\n\n                if self.with_mask:\n                    mask_roi_extractor = self.mask_roi_extractor[i]\n                    mask_head = self.mask_head[i]\n                    if det_bboxes.shape[0] == 0:\n                        segm_result = [\n                            [] for _ in range(mask_head.num_classes - 1)\n                        ]\n                    else:\n                        _bboxes = (det_bboxes[:, :4] * scale_factor\n                                   if rescale else det_bboxes)\n                        mask_rois = bbox2roi([_bboxes])\n                        mask_feats = mask_roi_extractor(\n                            x[:len(mask_roi_extractor.featmap_strides)],\n                            mask_rois)\n                        mask_pred = mask_head(mask_feats)\n                        segm_result = mask_head.get_seg_masks(\n                            mask_pred, _bboxes, det_labels, rcnn_test_cfg,\n                            ori_shape, scale_factor, rescale)\n                    ms_segm_result[\'stage{}\'.format(i)] = segm_result\n\n            if i < self.num_stages - 1:\n                bbox_label = cls_score.argmax(dim=1)\n                rois = bbox_head.regress_by_class(rois, bbox_label, bbox_pred,\n                                                  img_meta[0])\n\n        cls_score = sum(ms_scores) / self.num_stages\n        det_bboxes, det_labels = self.bbox_head[-1].get_det_bboxes(\n            rois,\n            cls_score,\n            bbox_pred,\n            img_shape,\n            scale_factor,\n            rescale=rescale,\n            cfg=rcnn_test_cfg)\n        bbox_result = bbox2result(det_bboxes, det_labels,\n                                  self.bbox_head[-1].num_classes)\n        ms_bbox_result[\'ensemble\'] = bbox_result\n\n        if self.with_mask:\n            if det_bboxes.shape[0] == 0:\n                segm_result = [\n                    [] for _ in range(self.mask_head[-1].num_classes - 1)\n                ]\n            else:\n                _bboxes = (det_bboxes[:, :4] * scale_factor\n                           if rescale else det_bboxes)\n                mask_rois = bbox2roi([_bboxes])\n                aug_masks = []\n                for i in range(self.num_stages):\n                    mask_roi_extractor = self.mask_roi_extractor[i]\n                    mask_feats = mask_roi_extractor(\n                        x[:len(mask_roi_extractor.featmap_strides)], mask_rois)\n                    mask_pred = self.mask_head[i](mask_feats)\n                    aug_masks.append(mask_pred.sigmoid().cpu().numpy())\n                merged_masks = merge_aug_masks(aug_masks,\n                                               [img_meta] * self.num_stages,\n                                               self.test_cfg.rcnn)\n                segm_result = self.mask_head[-1].get_seg_masks(\n                    merged_masks, _bboxes, det_labels, rcnn_test_cfg,\n                    ori_shape, scale_factor, rescale)\n            ms_segm_result[\'ensemble\'] = segm_result\n\n        if not self.test_cfg.keep_all_stages:\n            if self.with_mask:\n                results = (ms_bbox_result[\'ensemble\'],\n                           ms_segm_result[\'ensemble\'])\n            else:\n                results = ms_bbox_result[\'ensemble\']\n        else:\n            if self.with_mask:\n                results = {\n                    stage: (ms_bbox_result[stage], ms_segm_result[stage])\n                    for stage in ms_bbox_result\n                }\n            else:\n                results = ms_bbox_result\n\n        return results\n\n    def aug_test(self, img, img_meta, proposals=None, rescale=False):\n        raise NotImplementedError\n\n    def show_result(self, data, result, img_norm_cfg, **kwargs):\n        if self.with_mask:\n            ms_bbox_result, ms_segm_result = result\n            if isinstance(ms_bbox_result, dict):\n                result = (ms_bbox_result[\'ensemble\'],\n                          ms_segm_result[\'ensemble\'])\n        else:\n            if isinstance(result, dict):\n                result = result[\'ensemble\']\n        super(CascadeRCNN, self).show_result(data, result, img_norm_cfg,\n                                             **kwargs)\n'"
detection/mmdet/models/detectors/fast_rcnn.py,0,"b""from .two_stage import TwoStageDetector\nfrom ..registry import DETECTORS\n\n\n@DETECTORS.register_module\nclass FastRCNN(TwoStageDetector):\n\n    def __init__(self,\n                 backbone,\n                 neck,\n                 bbox_roi_extractor,\n                 bbox_head,\n                 train_cfg,\n                 test_cfg,\n                 mask_roi_extractor=None,\n                 mask_head=None,\n                 pretrained=None):\n        super(FastRCNN, self).__init__(\n            backbone=backbone,\n            neck=neck,\n            bbox_roi_extractor=bbox_roi_extractor,\n            bbox_head=bbox_head,\n            train_cfg=train_cfg,\n            test_cfg=test_cfg,\n            mask_roi_extractor=mask_roi_extractor,\n            mask_head=mask_head,\n            pretrained=pretrained)\n\n    def forward_test(self, imgs, img_metas, proposals, **kwargs):\n        for var, name in [(imgs, 'imgs'), (img_metas, 'img_metas')]:\n            if not isinstance(var, list):\n                raise TypeError('{} must be a list, but got {}'.format(\n                    name, type(var)))\n\n        num_augs = len(imgs)\n        if num_augs != len(img_metas):\n            raise ValueError(\n                'num of augmentations ({}) != num of image meta ({})'.format(\n                    len(imgs), len(img_metas)))\n        # TODO: remove the restriction of imgs_per_gpu == 1 when prepared\n        imgs_per_gpu = imgs[0].size(0)\n        assert imgs_per_gpu == 1\n\n        if num_augs == 1:\n            return self.simple_test(imgs[0], img_metas[0], proposals[0],\n                                    **kwargs)\n        else:\n            return self.aug_test(imgs, img_metas, proposals, **kwargs)\n"""
detection/mmdet/models/detectors/faster_rcnn.py,0,"b'from .two_stage import TwoStageDetector\nfrom ..registry import DETECTORS\n\n\n@DETECTORS.register_module\nclass FasterRCNN(TwoStageDetector):\n\n    def __init__(self,\n                 backbone,\n                 neck,\n                 rpn_head,\n                 bbox_roi_extractor,\n                 bbox_head,\n                 train_cfg,\n                 test_cfg,\n                 pretrained=None):\n        super(FasterRCNN, self).__init__(\n            backbone=backbone,\n            neck=neck,\n            rpn_head=rpn_head,\n            bbox_roi_extractor=bbox_roi_extractor,\n            bbox_head=bbox_head,\n            train_cfg=train_cfg,\n            test_cfg=test_cfg,\n            pretrained=pretrained)\n'"
detection/mmdet/models/detectors/mask_rcnn.py,0,"b'from .two_stage import TwoStageDetector\nfrom ..registry import DETECTORS\n\n\n@DETECTORS.register_module\nclass MaskRCNN(TwoStageDetector):\n\n    def __init__(self,\n                 backbone,\n                 neck,\n                 rpn_head,\n                 bbox_roi_extractor,\n                 bbox_head,\n                 mask_roi_extractor,\n                 mask_head,\n                 train_cfg,\n                 test_cfg,\n                 pretrained=None):\n        super(MaskRCNN, self).__init__(\n            backbone=backbone,\n            neck=neck,\n            rpn_head=rpn_head,\n            bbox_roi_extractor=bbox_roi_extractor,\n            bbox_head=bbox_head,\n            mask_roi_extractor=mask_roi_extractor,\n            mask_head=mask_head,\n            train_cfg=train_cfg,\n            test_cfg=test_cfg,\n            pretrained=pretrained)\n'"
detection/mmdet/models/detectors/retinanet.py,0,"b'from .single_stage import SingleStageDetector\nfrom ..registry import DETECTORS\n\n\n@DETECTORS.register_module\nclass RetinaNet(SingleStageDetector):\n\n    def __init__(self,\n                 backbone,\n                 neck,\n                 bbox_head,\n                 train_cfg=None,\n                 test_cfg=None,\n                 pretrained=None):\n        super(RetinaNet, self).__init__(backbone, neck, bbox_head, train_cfg,\n                                        test_cfg, pretrained)\n'"
detection/mmdet/models/detectors/rpn.py,0,"b'import mmcv\n\nfrom mmdet.core import tensor2imgs, bbox_mapping\nfrom .base import BaseDetector\nfrom .test_mixins import RPNTestMixin\nfrom .. import builder\nfrom ..registry import DETECTORS\n\n\n@DETECTORS.register_module\nclass RPN(BaseDetector, RPNTestMixin):\n\n    def __init__(self,\n                 backbone,\n                 neck,\n                 rpn_head,\n                 train_cfg,\n                 test_cfg,\n                 pretrained=None):\n        super(RPN, self).__init__()\n        self.backbone = builder.build_backbone(backbone)\n        self.neck = builder.build_neck(neck) if neck is not None else None\n        self.rpn_head = builder.build_head(rpn_head)\n        self.train_cfg = train_cfg\n        self.test_cfg = test_cfg\n        self.init_weights(pretrained=pretrained)\n\n    def init_weights(self, pretrained=None):\n        super(RPN, self).init_weights(pretrained)\n        self.backbone.init_weights(pretrained=pretrained)\n        if self.with_neck:\n            self.neck.init_weights()\n        self.rpn_head.init_weights()\n\n    def extract_feat(self, img):\n        x = self.backbone(img)\n        if self.with_neck:\n            x = self.neck(x)\n        return x\n\n    def forward_train(self,\n                      img,\n                      img_meta,\n                      gt_bboxes=None,\n                      gt_bboxes_ignore=None):\n        if self.train_cfg.rpn.get(\'debug\', False):\n            self.rpn_head.debug_imgs = tensor2imgs(img)\n\n        x = self.extract_feat(img)\n        rpn_outs = self.rpn_head(x)\n\n        rpn_loss_inputs = rpn_outs + (gt_bboxes, img_meta, self.train_cfg.rpn)\n        losses = self.rpn_head.loss(\n            *rpn_loss_inputs, gt_bboxes_ignore=gt_bboxes_ignore)\n        return losses\n\n    def simple_test(self, img, img_meta, rescale=False):\n        x = self.extract_feat(img)\n        proposal_list = self.simple_test_rpn(x, img_meta, self.test_cfg.rpn)\n        if rescale:\n            for proposals, meta in zip(proposal_list, img_meta):\n                proposals[:, :4] /= meta[\'scale_factor\']\n        # TODO: remove this restriction\n        return proposal_list[0].cpu().numpy()\n\n    def aug_test(self, imgs, img_metas, rescale=False):\n        proposal_list = self.aug_test_rpn(\n            self.extract_feats(imgs), img_metas, self.test_cfg.rpn)\n        if not rescale:\n            for proposals, img_meta in zip(proposal_list, img_metas[0]):\n                img_shape = img_meta[\'img_shape\']\n                scale_factor = img_meta[\'scale_factor\']\n                flip = img_meta[\'flip\']\n                proposals[:, :4] = bbox_mapping(proposals[:, :4], img_shape,\n                                                scale_factor, flip)\n        # TODO: remove this restriction\n        return proposal_list[0].cpu().numpy()\n\n    def show_result(self, data, result, img_norm_cfg, dataset=None, top_k=20):\n        """"""Show RPN proposals on the image.\n\n        Although we assume batch size is 1, this method supports arbitrary\n        batch size.\n        """"""\n        img_tensor = data[\'img\'][0]\n        img_metas = data[\'img_meta\'][0].data[0]\n        imgs = tensor2imgs(img_tensor, **img_norm_cfg)\n        assert len(imgs) == len(img_metas)\n        for img, img_meta in zip(imgs, img_metas):\n            h, w, _ = img_meta[\'img_shape\']\n            img_show = img[:h, :w, :]\n            mmcv.imshow_bboxes(img_show, result, top_k=top_k)\n'"
detection/mmdet/models/detectors/single_stage.py,1,"b'import torch.nn as nn\n\nfrom .base import BaseDetector\nfrom .. import builder\nfrom ..registry import DETECTORS\nfrom mmdet.core import bbox2result\n\n\n@DETECTORS.register_module\nclass SingleStageDetector(BaseDetector):\n\n    def __init__(self,\n                 backbone,\n                 neck=None,\n                 bbox_head=None,\n                 train_cfg=None,\n                 test_cfg=None,\n                 pretrained=None):\n        super(SingleStageDetector, self).__init__()\n        self.backbone = builder.build_backbone(backbone)\n        if neck is not None:\n            self.neck = builder.build_neck(neck)\n        self.bbox_head = builder.build_head(bbox_head)\n        self.train_cfg = train_cfg\n        self.test_cfg = test_cfg\n        self.init_weights(pretrained=pretrained)\n\n    def init_weights(self, pretrained=None):\n        super(SingleStageDetector, self).init_weights(pretrained)\n        self.backbone.init_weights(pretrained=pretrained)\n        if self.with_neck:\n            if isinstance(self.neck, nn.Sequential):\n                for m in self.neck:\n                    m.init_weights()\n            else:\n                self.neck.init_weights()\n        self.bbox_head.init_weights()\n\n    def extract_feat(self, img):\n        x = self.backbone(img)\n        if self.with_neck:\n            x = self.neck(x)\n        return x\n\n    def forward_train(self,\n                      img,\n                      img_metas,\n                      gt_bboxes,\n                      gt_labels,\n                      gt_bboxes_ignore=None):\n        x = self.extract_feat(img)\n        outs = self.bbox_head(x)\n        loss_inputs = outs + (gt_bboxes, gt_labels, img_metas, self.train_cfg)\n        losses = self.bbox_head.loss(\n            *loss_inputs, gt_bboxes_ignore=gt_bboxes_ignore)\n        return losses\n\n    def simple_test(self, img, img_meta, rescale=False):\n        x = self.extract_feat(img)\n        outs = self.bbox_head(x)\n        bbox_inputs = outs + (img_meta, self.test_cfg, rescale)\n        bbox_list = self.bbox_head.get_bboxes(*bbox_inputs)\n        bbox_results = [\n            bbox2result(det_bboxes, det_labels, self.bbox_head.num_classes)\n            for det_bboxes, det_labels in bbox_list\n        ]\n        return bbox_results[0]\n\n    def aug_test(self, imgs, img_metas, rescale=False):\n        raise NotImplementedError\n'"
detection/mmdet/models/detectors/test_mixins.py,0,"b'from mmdet.core import (bbox2roi, bbox_mapping, merge_aug_proposals,\n                        merge_aug_bboxes, merge_aug_masks, multiclass_nms)\n\n\nclass RPNTestMixin(object):\n\n    def simple_test_rpn(self, x, img_meta, rpn_test_cfg):\n        rpn_outs = self.rpn_head(x)\n        proposal_inputs = rpn_outs + (img_meta, rpn_test_cfg)\n        proposal_list = self.rpn_head.get_bboxes(*proposal_inputs)\n        return proposal_list\n\n    def aug_test_rpn(self, feats, img_metas, rpn_test_cfg):\n        imgs_per_gpu = len(img_metas[0])\n        aug_proposals = [[] for _ in range(imgs_per_gpu)]\n        for x, img_meta in zip(feats, img_metas):\n            proposal_list = self.simple_test_rpn(x, img_meta, rpn_test_cfg)\n            for i, proposals in enumerate(proposal_list):\n                aug_proposals[i].append(proposals)\n        # after merging, proposals will be rescaled to the original image size\n        merged_proposals = [\n            merge_aug_proposals(proposals, img_meta, rpn_test_cfg)\n            for proposals, img_meta in zip(aug_proposals, img_metas)\n        ]\n        return merged_proposals\n\n\nclass BBoxTestMixin(object):\n\n    def simple_test_bboxes(self,\n                           x,\n                           img_meta,\n                           proposals,\n                           rcnn_test_cfg,\n                           rescale=False):\n        """"""Test only det bboxes without augmentation.""""""\n        rois = bbox2roi(proposals)\n        roi_feats = self.bbox_roi_extractor(\n            x[:len(self.bbox_roi_extractor.featmap_strides)], rois)\n        cls_score, bbox_pred = self.bbox_head(roi_feats)\n        img_shape = img_meta[0][\'img_shape\']\n        scale_factor = img_meta[0][\'scale_factor\']\n        det_bboxes, det_labels = self.bbox_head.get_det_bboxes(\n            rois,\n            cls_score,\n            bbox_pred,\n            img_shape,\n            scale_factor,\n            rescale=rescale,\n            cfg=rcnn_test_cfg)\n        return det_bboxes, det_labels\n\n    def aug_test_bboxes(self, feats, img_metas, proposal_list, rcnn_test_cfg):\n        aug_bboxes = []\n        aug_scores = []\n        for x, img_meta in zip(feats, img_metas):\n            # only one image in the batch\n            img_shape = img_meta[0][\'img_shape\']\n            scale_factor = img_meta[0][\'scale_factor\']\n            flip = img_meta[0][\'flip\']\n            # TODO more flexible\n            proposals = bbox_mapping(proposal_list[0][:, :4], img_shape,\n                                     scale_factor, flip)\n            rois = bbox2roi([proposals])\n            # recompute feature maps to save GPU memory\n            roi_feats = self.bbox_roi_extractor(\n                x[:len(self.bbox_roi_extractor.featmap_strides)], rois)\n            cls_score, bbox_pred = self.bbox_head(roi_feats)\n            bboxes, scores = self.bbox_head.get_det_bboxes(\n                rois,\n                cls_score,\n                bbox_pred,\n                img_shape,\n                scale_factor,\n                rescale=False,\n                cfg=None)\n            aug_bboxes.append(bboxes)\n            aug_scores.append(scores)\n        # after merging, bboxes will be rescaled to the original image size\n        merged_bboxes, merged_scores = merge_aug_bboxes(\n            aug_bboxes, aug_scores, img_metas, rcnn_test_cfg)\n        det_bboxes, det_labels = multiclass_nms(\n            merged_bboxes, merged_scores, rcnn_test_cfg.score_thr,\n            rcnn_test_cfg.nms, rcnn_test_cfg.max_per_img)\n        return det_bboxes, det_labels\n\n\nclass MaskTestMixin(object):\n\n    def simple_test_mask(self,\n                         x,\n                         img_meta,\n                         det_bboxes,\n                         det_labels,\n                         rescale=False):\n        # image shape of the first image in the batch (only one)\n        ori_shape = img_meta[0][\'ori_shape\']\n        scale_factor = img_meta[0][\'scale_factor\']\n        if det_bboxes.shape[0] == 0:\n            segm_result = [[] for _ in range(self.mask_head.num_classes - 1)]\n        else:\n            # if det_bboxes is rescaled to the original image size, we need to\n            # rescale it back to the testing scale to obtain RoIs.\n            _bboxes = (det_bboxes[:, :4] * scale_factor\n                       if rescale else det_bboxes)\n            mask_rois = bbox2roi([_bboxes])\n            mask_feats = self.mask_roi_extractor(\n                x[:len(self.mask_roi_extractor.featmap_strides)], mask_rois)\n            mask_pred = self.mask_head(mask_feats)\n            segm_result = self.mask_head.get_seg_masks(\n                mask_pred, _bboxes, det_labels, self.test_cfg.rcnn, ori_shape,\n                scale_factor, rescale)\n        return segm_result\n\n    def aug_test_mask(self, feats, img_metas, det_bboxes, det_labels):\n        if det_bboxes.shape[0] == 0:\n            segm_result = [[] for _ in range(self.mask_head.num_classes - 1)]\n        else:\n            aug_masks = []\n            for x, img_meta in zip(feats, img_metas):\n                img_shape = img_meta[0][\'img_shape\']\n                scale_factor = img_meta[0][\'scale_factor\']\n                flip = img_meta[0][\'flip\']\n                _bboxes = bbox_mapping(det_bboxes[:, :4], img_shape,\n                                       scale_factor, flip)\n                mask_rois = bbox2roi([_bboxes])\n                mask_feats = self.mask_roi_extractor(\n                    x[:len(self.mask_roi_extractor.featmap_strides)],\n                    mask_rois)\n                mask_pred = self.mask_head(mask_feats)\n                # convert to numpy array to save memory\n                aug_masks.append(mask_pred.sigmoid().cpu().numpy())\n            merged_masks = merge_aug_masks(aug_masks, img_metas,\n                                           self.test_cfg.rcnn)\n\n            ori_shape = img_metas[0][0][\'ori_shape\']\n            segm_result = self.mask_head.get_seg_masks(\n                merged_masks,\n                det_bboxes,\n                det_labels,\n                self.test_cfg.rcnn,\n                ori_shape,\n                scale_factor=1.0,\n                rescale=False)\n        return segm_result\n'"
detection/mmdet/models/detectors/two_stage.py,2,"b'import torch\nimport torch.nn as nn\n\nfrom .base import BaseDetector\nfrom .test_mixins import RPNTestMixin, BBoxTestMixin, MaskTestMixin\nfrom .. import builder\nfrom ..registry import DETECTORS\nfrom mmdet.core import bbox2roi, bbox2result, build_assigner, build_sampler\n\n\n@DETECTORS.register_module\nclass TwoStageDetector(BaseDetector, RPNTestMixin, BBoxTestMixin,\n                       MaskTestMixin):\n\n    def __init__(self,\n                 backbone,\n                 neck=None,\n                 rpn_head=None,\n                 bbox_roi_extractor=None,\n                 bbox_head=None,\n                 mask_roi_extractor=None,\n                 mask_head=None,\n                 train_cfg=None,\n                 test_cfg=None,\n                 pretrained=None):\n        super(TwoStageDetector, self).__init__()\n        self.backbone = builder.build_backbone(backbone)\n\n        if neck is not None:\n            self.neck = builder.build_neck(neck)\n        else:\n            raise NotImplementedError\n\n        if rpn_head is not None:\n            self.rpn_head = builder.build_head(rpn_head)\n\n        if bbox_head is not None:\n            self.bbox_roi_extractor = builder.build_roi_extractor(\n                bbox_roi_extractor)\n            self.bbox_head = builder.build_head(bbox_head)\n\n        if mask_head is not None:\n            self.mask_roi_extractor = builder.build_roi_extractor(\n                mask_roi_extractor)\n            self.mask_head = builder.build_head(mask_head)\n\n        self.train_cfg = train_cfg\n        self.test_cfg = test_cfg\n\n        self.init_weights(pretrained=pretrained)\n\n    @property\n    def with_rpn(self):\n        return hasattr(self, \'rpn_head\') and self.rpn_head is not None\n\n    def init_weights(self, pretrained=None):\n        super(TwoStageDetector, self).init_weights(pretrained)\n        self.backbone.init_weights(pretrained=pretrained)\n        if self.with_neck:\n            if isinstance(self.neck, nn.Sequential):\n                for m in self.neck:\n                    m.init_weights()\n            else:\n                self.neck.init_weights()\n        if self.with_rpn:\n            self.rpn_head.init_weights()\n        if self.with_bbox:\n            self.bbox_roi_extractor.init_weights()\n            self.bbox_head.init_weights()\n        if self.with_mask:\n            self.mask_roi_extractor.init_weights()\n            self.mask_head.init_weights()\n\n    def extract_feat(self, img):\n        x = self.backbone(img)\n        if self.with_neck:\n            x = self.neck(x)\n        return x\n\n    def forward_train(self,\n                      img,\n                      img_meta,\n                      gt_bboxes,\n                      gt_labels,\n                      gt_bboxes_ignore=None,\n                      gt_masks=None,\n                      proposals=None):\n        x = self.extract_feat(img)\n\n        losses = dict()\n\n        # RPN forward and loss\n        if self.with_rpn:\n            rpn_outs = self.rpn_head(x)\n            rpn_loss_inputs = rpn_outs + (gt_bboxes, img_meta,\n                                          self.train_cfg.rpn)\n            rpn_losses = self.rpn_head.loss(\n                *rpn_loss_inputs, gt_bboxes_ignore=gt_bboxes_ignore)\n            losses.update(rpn_losses)\n\n            proposal_inputs = rpn_outs + (img_meta, self.test_cfg.rpn)\n            proposal_list = self.rpn_head.get_bboxes(*proposal_inputs)\n        else:\n            proposal_list = proposals\n\n        # assign gts and sample proposals\n        if self.with_bbox or self.with_mask:\n            bbox_assigner = build_assigner(self.train_cfg.rcnn.assigner)\n            bbox_sampler = build_sampler(\n                self.train_cfg.rcnn.sampler, context=self)\n            num_imgs = img.size(0)\n            if gt_bboxes_ignore is None:\n                gt_bboxes_ignore = [None for _ in range(num_imgs)]\n            sampling_results = []\n            for i in range(num_imgs):\n                assign_result = bbox_assigner.assign(\n                    proposal_list[i], gt_bboxes[i], gt_bboxes_ignore[i],\n                    gt_labels[i])\n                sampling_result = bbox_sampler.sample(\n                    assign_result,\n                    proposal_list[i],\n                    gt_bboxes[i],\n                    gt_labels[i],\n                    feats=[lvl_feat[i][None] for lvl_feat in x])\n                sampling_results.append(sampling_result)\n\n        # bbox head forward and loss\n        if self.with_bbox:\n            rois = bbox2roi([res.bboxes for res in sampling_results])\n            # TODO: a more flexible way to decide which feature maps to use\n            bbox_feats = self.bbox_roi_extractor(\n                x[:self.bbox_roi_extractor.num_inputs], rois)\n            cls_score, bbox_pred = self.bbox_head(bbox_feats)\n\n            bbox_targets = self.bbox_head.get_target(\n                sampling_results, gt_bboxes, gt_labels, self.train_cfg.rcnn)\n            loss_bbox = self.bbox_head.loss(cls_score, bbox_pred,\n                                            *bbox_targets)\n            losses.update(loss_bbox)\n\n        # mask head forward and loss\n        if self.with_mask:\n            pos_rois = bbox2roi([res.pos_bboxes for res in sampling_results])\n            mask_feats = self.mask_roi_extractor(\n                x[:self.mask_roi_extractor.num_inputs], pos_rois)\n            mask_pred = self.mask_head(mask_feats)\n\n            mask_targets = self.mask_head.get_target(\n                sampling_results, gt_masks, self.train_cfg.rcnn)\n            pos_labels = torch.cat(\n                [res.pos_gt_labels for res in sampling_results])\n            loss_mask = self.mask_head.loss(mask_pred, mask_targets,\n                                            pos_labels)\n            losses.update(loss_mask)\n\n        return losses\n\n    def simple_test(self, img, img_meta, proposals=None, rescale=False):\n        """"""Test without augmentation.""""""\n        assert self.with_bbox, ""Bbox head must be implemented.""\n\n        x = self.extract_feat(img)\n\n        proposal_list = self.simple_test_rpn(\n            x, img_meta, self.test_cfg.rpn) if proposals is None else proposals\n\n        det_bboxes, det_labels = self.simple_test_bboxes(\n            x, img_meta, proposal_list, self.test_cfg.rcnn, rescale=rescale)\n        bbox_results = bbox2result(det_bboxes, det_labels,\n                                   self.bbox_head.num_classes)\n\n        if not self.with_mask:\n            return bbox_results\n        else:\n            segm_results = self.simple_test_mask(\n                x, img_meta, det_bboxes, det_labels, rescale=rescale)\n            return bbox_results, segm_results\n\n    def aug_test(self, imgs, img_metas, rescale=False):\n        """"""Test with augmentations.\n\n        If rescale is False, then returned bboxes and masks will fit the scale\n        of imgs[0].\n        """"""\n        # recompute feats to save memory\n        proposal_list = self.aug_test_rpn(\n            self.extract_feats(imgs), img_metas, self.test_cfg.rpn)\n        det_bboxes, det_labels = self.aug_test_bboxes(\n            self.extract_feats(imgs), img_metas, proposal_list,\n            self.test_cfg.rcnn)\n\n        if rescale:\n            _det_bboxes = det_bboxes\n        else:\n            _det_bboxes = det_bboxes.clone()\n            _det_bboxes[:, :4] *= img_metas[0][0][\'scale_factor\']\n        bbox_results = bbox2result(_det_bboxes, det_labels,\n                                   self.bbox_head.num_classes)\n\n        # det_bboxes always keep the original scale\n        if self.with_mask:\n            segm_results = self.aug_test_mask(\n                self.extract_feats(imgs), img_metas, det_bboxes, det_labels)\n            return bbox_results, segm_results\n        else:\n            return bbox_results\n'"
detection/mmdet/models/mask_heads/__init__.py,0,"b""from .fcn_mask_head import FCNMaskHead\n\n__all__ = ['FCNMaskHead']\n"""
detection/mmdet/models/mask_heads/fcn_mask_head.py,3,"b'import mmcv\nimport numpy as np\nimport pycocotools.mask as mask_util\nimport torch\nimport torch.nn as nn\n\nfrom ..registry import HEADS\nfrom ..utils import ConvModule\nfrom mmdet.core import mask_cross_entropy, mask_target\n\n\n@HEADS.register_module\nclass FCNMaskHead(nn.Module):\n\n    def __init__(self,\n                 num_convs=4,\n                 roi_feat_size=14,\n                 in_channels=256,\n                 conv_kernel_size=3,\n                 conv_out_channels=256,\n                 upsample_method=\'deconv\',\n                 upsample_ratio=2,\n                 num_classes=81,\n                 class_agnostic=False,\n                 normalize=None):\n        super(FCNMaskHead, self).__init__()\n        if upsample_method not in [None, \'deconv\', \'nearest\', \'bilinear\']:\n            raise ValueError(\n                \'Invalid upsample method {}, accepted methods \'\n                \'are ""deconv"", ""nearest"", ""bilinear""\'.format(upsample_method))\n        self.num_convs = num_convs\n        self.roi_feat_size = roi_feat_size  # WARN: not used and reserved\n        self.in_channels = in_channels\n        self.conv_kernel_size = conv_kernel_size\n        self.conv_out_channels = conv_out_channels\n        self.upsample_method = upsample_method\n        self.upsample_ratio = upsample_ratio\n        self.num_classes = num_classes\n        self.class_agnostic = class_agnostic\n        self.normalize = normalize\n        self.with_bias = normalize is None\n\n        self.convs = nn.ModuleList()\n        for i in range(self.num_convs):\n            in_channels = (self.in_channels\n                           if i == 0 else self.conv_out_channels)\n            padding = (self.conv_kernel_size - 1) // 2\n            self.convs.append(\n                ConvModule(\n                    in_channels,\n                    self.conv_out_channels,\n                    self.conv_kernel_size,\n                    padding=padding,\n                    normalize=normalize,\n                    bias=self.with_bias))\n        if self.upsample_method is None:\n            self.upsample = None\n        elif self.upsample_method == \'deconv\':\n            self.upsample = nn.ConvTranspose2d(\n                self.conv_out_channels,\n                self.conv_out_channels,\n                self.upsample_ratio,\n                stride=self.upsample_ratio)\n        else:\n            self.upsample = nn.Upsample(\n                scale_factor=self.upsample_ratio, mode=self.upsample_method)\n\n        out_channels = 1 if self.class_agnostic else self.num_classes\n        self.conv_logits = nn.Conv2d(self.conv_out_channels, out_channels, 1)\n        self.relu = nn.ReLU(inplace=True)\n        self.debug_imgs = None\n\n    def init_weights(self):\n        for m in [self.upsample, self.conv_logits]:\n            if m is None:\n                continue\n            nn.init.kaiming_normal_(\n                m.weight, mode=\'fan_out\', nonlinearity=\'relu\')\n            nn.init.constant_(m.bias, 0)\n\n    def forward(self, x):\n        for conv in self.convs:\n            x = conv(x)\n        if self.upsample is not None:\n            x = self.upsample(x)\n            if self.upsample_method == \'deconv\':\n                x = self.relu(x)\n        mask_pred = self.conv_logits(x)\n        return mask_pred\n\n    def get_target(self, sampling_results, gt_masks, rcnn_train_cfg):\n        pos_proposals = [res.pos_bboxes for res in sampling_results]\n        pos_assigned_gt_inds = [\n            res.pos_assigned_gt_inds for res in sampling_results\n        ]\n        mask_targets = mask_target(pos_proposals, pos_assigned_gt_inds,\n                                   gt_masks, rcnn_train_cfg)\n        return mask_targets\n\n    def loss(self, mask_pred, mask_targets, labels):\n        loss = dict()\n        if self.class_agnostic:\n            loss_mask = mask_cross_entropy(mask_pred, mask_targets,\n                                           torch.zeros_like(labels))\n        else:\n            loss_mask = mask_cross_entropy(mask_pred, mask_targets, labels)\n        loss[\'loss_mask\'] = loss_mask\n        return loss\n\n    def get_seg_masks(self, mask_pred, det_bboxes, det_labels, rcnn_test_cfg,\n                      ori_shape, scale_factor, rescale):\n        """"""Get segmentation masks from mask_pred and bboxes.\n\n        Args:\n            mask_pred (Tensor or ndarray): shape (n, #class+1, h, w).\n                For single-scale testing, mask_pred is the direct output of\n                model, whose type is Tensor, while for multi-scale testing,\n                it will be converted to numpy array outside of this method.\n            det_bboxes (Tensor): shape (n, 4/5)\n            det_labels (Tensor): shape (n, )\n            img_shape (Tensor): shape (3, )\n            rcnn_test_cfg (dict): rcnn testing config\n            ori_shape: original image size\n\n        Returns:\n            list[list]: encoded masks\n        """"""\n        if isinstance(mask_pred, torch.Tensor):\n            mask_pred = mask_pred.sigmoid().cpu().numpy()\n        assert isinstance(mask_pred, np.ndarray)\n\n        cls_segms = [[] for _ in range(self.num_classes - 1)]\n        bboxes = det_bboxes.cpu().numpy()[:, :4]\n        labels = det_labels.cpu().numpy() + 1\n\n        if rescale:\n            img_h, img_w = ori_shape[:2]\n        else:\n            img_h = np.round(ori_shape[0] * scale_factor).astype(np.int32)\n            img_w = np.round(ori_shape[1] * scale_factor).astype(np.int32)\n            scale_factor = 1.0\n\n        for i in range(bboxes.shape[0]):\n            bbox = (bboxes[i, :] / scale_factor).astype(np.int32)\n            label = labels[i]\n            w = max(bbox[2] - bbox[0] + 1, 1)\n            h = max(bbox[3] - bbox[1] + 1, 1)\n\n            if not self.class_agnostic:\n                mask_pred_ = mask_pred[i, label, :, :]\n            else:\n                mask_pred_ = mask_pred[i, 0, :, :]\n            im_mask = np.zeros((img_h, img_w), dtype=np.uint8)\n\n            bbox_mask = mmcv.imresize(mask_pred_, (w, h))\n            bbox_mask = (bbox_mask > rcnn_test_cfg.mask_thr_binary).astype(\n                np.uint8)\n            im_mask[bbox[1]:bbox[1] + h, bbox[0]:bbox[0] + w] = bbox_mask\n            rle = mask_util.encode(\n                np.array(im_mask[:, :, np.newaxis], order=\'F\'))[0]\n            cls_segms[label - 1].append(rle)\n\n        return cls_segms\n'"
detection/mmdet/models/necks/__init__.py,0,"b""from .fpn import FPN\nfrom .fpn_sge import FPNSGE\n\n__all__ = ['FPN', 'FPNSGE']\n"""
detection/mmdet/models/necks/fpn.py,2,"b""import torch.nn as nn\nimport torch.nn.functional as F\nfrom mmcv.cnn import xavier_init\n\nfrom ..utils import ConvModule\nfrom ..registry import NECKS\n\n\n@NECKS.register_module\nclass FPN(nn.Module):\n\n    def __init__(self,\n                 in_channels,\n                 out_channels,\n                 num_outs,\n                 start_level=0,\n                 end_level=-1,\n                 add_extra_convs=False,\n                 normalize=None,\n                 activation=None):\n        super(FPN, self).__init__()\n        assert isinstance(in_channels, list)\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        self.num_ins = len(in_channels)\n        self.num_outs = num_outs\n        self.activation = activation\n        self.with_bias = normalize is None\n\n        if end_level == -1:\n            self.backbone_end_level = self.num_ins\n            assert num_outs >= self.num_ins - start_level\n        else:\n            # if end_level < inputs, no extra level is allowed\n            self.backbone_end_level = end_level\n            assert end_level <= len(in_channels)\n            assert num_outs == end_level - start_level\n        self.start_level = start_level\n        self.end_level = end_level\n        self.add_extra_convs = add_extra_convs\n\n        self.lateral_convs = nn.ModuleList()\n        self.fpn_convs = nn.ModuleList()\n\n        for i in range(self.start_level, self.backbone_end_level):\n            l_conv = ConvModule(\n                in_channels[i],\n                out_channels,\n                1,\n                normalize=normalize,\n                bias=self.with_bias,\n                activation=self.activation,\n                inplace=False)\n            fpn_conv = ConvModule(\n                out_channels,\n                out_channels,\n                3,\n                padding=1,\n                normalize=normalize,\n                bias=self.with_bias,\n                activation=self.activation,\n                inplace=False)\n\n            self.lateral_convs.append(l_conv)\n            self.fpn_convs.append(fpn_conv)\n\n            # lvl_id = i - self.start_level\n            # setattr(self, 'lateral_conv{}'.format(lvl_id), l_conv)\n            # setattr(self, 'fpn_conv{}'.format(lvl_id), fpn_conv)\n\n        # add extra conv layers (e.g., RetinaNet)\n        extra_levels = num_outs - self.backbone_end_level + self.start_level\n        if add_extra_convs and extra_levels >= 1:\n            for i in range(extra_levels):\n                in_channels = (self.in_channels[self.backbone_end_level - 1]\n                               if i == 0 else out_channels)\n                extra_fpn_conv = ConvModule(\n                    in_channels,\n                    out_channels,\n                    3,\n                    stride=2,\n                    padding=1,\n                    normalize=normalize,\n                    bias=self.with_bias,\n                    activation=self.activation,\n                    inplace=False)\n                self.fpn_convs.append(extra_fpn_conv)\n\n    # default init_weights for conv(msra) and norm in ConvModule\n    def init_weights(self):\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                xavier_init(m, distribution='uniform')\n\n    def forward(self, inputs):\n        assert len(inputs) == len(self.in_channels)\n\n        # build laterals\n        laterals = [\n            lateral_conv(inputs[i + self.start_level])\n            for i, lateral_conv in enumerate(self.lateral_convs)\n        ]\n\n        # build top-down path\n        used_backbone_levels = len(laterals)\n        for i in range(used_backbone_levels - 1, 0, -1):\n            laterals[i - 1] += F.interpolate(\n                laterals[i], scale_factor=2, mode='nearest')\n\n        # build outputs\n        # part 1: from original levels\n        outs = [\n            self.fpn_convs[i](laterals[i]) for i in range(used_backbone_levels)\n        ]\n        # part 2: add extra levels\n        if self.num_outs > len(outs):\n            # use max pool to get more levels on top of outputs\n            # (e.g., Faster R-CNN, Mask R-CNN)\n            if not self.add_extra_convs:\n                for i in range(self.num_outs - used_backbone_levels):\n                    outs.append(F.max_pool2d(outs[-1], 1, stride=2))\n            # add conv layers on top of original feature maps (RetinaNet)\n            else:\n                orig = inputs[self.backbone_end_level - 1]\n                outs.append(self.fpn_convs[used_backbone_levels](orig))\n                for i in range(used_backbone_levels + 1, self.num_outs):\n                    # BUG: we should add relu before each extra conv\n                    outs.append(self.fpn_convs[i](outs[-1]))\n        return tuple(outs)\n"""
detection/mmdet/models/necks/fpn_sge.py,5,"b""import torch.nn as nn\nimport torch.nn.functional as F\nfrom mmcv.cnn import xavier_init\n\nfrom ..utils import ConvModule\nfrom ..registry import NECKS\n\nimport torch\nfrom torch.nn.parameter import Parameter\n\nclass SpatialGroupEnhance(nn.Module):\n    def __init__(self, groups):\n        super(SpatialGroupEnhance, self).__init__()\n        self.groups   = groups\n        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n        self.weight   = Parameter(torch.zeros(1, groups, 1, 1))\n        self.bias     = Parameter(torch.ones(1, groups, 1, 1))\n        self.sig      = nn.Sigmoid()\n        print('add one SGE!')\n\n    def forward(self, x): # (b, c, h, w)\n        b, c, h, w = x.size()\n        x = x.view(b * self.groups, -1, h, w) # (b*32, c', h, w)\n        xn = x * self.avg_pool(x)\n        xn = xn.sum(dim=1, keepdim=True) # (b*32, 1, h, w)\n        t = xn.view(b * self.groups, -1)\n        t = t - t.mean(dim=1, keepdim=True)\n        std = t.std(dim=1, keepdim=True) + 1e-5\n        t = t / std\n        t = t.view(b, self.groups, h, w)\n        t = t * self.weight + self.bias\n        t = t.view(b * self.groups, 1, h, w)\n        x = x * self.sig(t)\n        x = x.view(b, c, h, w)\n        return x\n\n\n@NECKS.register_module\nclass FPNSGE(nn.Module):\n\n    def __init__(self,\n                 in_channels,\n                 out_channels,\n                 num_outs,\n                 start_level=0,\n                 end_level=-1,\n                 add_extra_convs=False,\n                 normalize=None,\n                 activation=None):\n        super(FPNSGE, self).__init__()\n        assert isinstance(in_channels, list)\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        self.num_ins = len(in_channels)\n        self.num_outs = num_outs\n        self.activation = activation\n        self.with_bias = normalize is None\n\n        if end_level == -1:\n            self.backbone_end_level = self.num_ins\n            assert num_outs >= self.num_ins - start_level\n        else:\n            # if end_level < inputs, no extra level is allowed\n            self.backbone_end_level = end_level\n            assert end_level <= len(in_channels)\n            assert num_outs == end_level - start_level\n        self.start_level = start_level\n        self.end_level = end_level\n        self.add_extra_convs = add_extra_convs\n\n        self.lateral_convs = nn.ModuleList()\n        self.fpn_convs = nn.ModuleList()\n        self.sges = nn.ModuleList()\n\n        for i in range(self.start_level, self.backbone_end_level):\n            l_conv = ConvModule(\n                in_channels[i],\n                out_channels,\n                1,\n                normalize=normalize,\n                bias=self.with_bias,\n                activation=self.activation,\n                inplace=False)\n            fpn_conv = ConvModule(\n                out_channels,\n                out_channels,\n                3,\n                padding=1,\n                normalize=normalize,\n                bias=self.with_bias,\n                activation=self.activation,\n                inplace=False)\n\n            sge = SpatialGroupEnhance(64)\n\n            self.lateral_convs.append(l_conv)\n            self.fpn_convs.append(fpn_conv)\n            self.sges.append(sge)\n\n            # lvl_id = i - self.start_level\n            # setattr(self, 'lateral_conv{}'.format(lvl_id), l_conv)\n            # setattr(self, 'fpn_conv{}'.format(lvl_id), fpn_conv)\n\n        # add extra conv layers (e.g., RetinaNet)\n        extra_levels = num_outs - self.backbone_end_level + self.start_level\n        if add_extra_convs and extra_levels >= 1:\n            for i in range(extra_levels):\n                in_channels = (self.in_channels[self.backbone_end_level - 1]\n                               if i == 0 else out_channels)\n                extra_fpn_conv = ConvModule(\n                    in_channels,\n                    out_channels,\n                    3,\n                    stride=2,\n                    padding=1,\n                    normalize=normalize,\n                    bias=self.with_bias,\n                    activation=self.activation,\n                    inplace=False)\n                self.fpn_convs.append(extra_fpn_conv)\n                \n\n    # default init_weights for conv(msra) and norm in ConvModule\n    def init_weights(self):\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                xavier_init(m, distribution='uniform')\n\n    def forward(self, inputs):\n        assert len(inputs) == len(self.in_channels)\n\n        # build laterals\n        laterals = [\n            lateral_conv(inputs[i + self.start_level])\n            for i, lateral_conv in enumerate(self.lateral_convs)\n        ]\n\n        # build top-down path\n        used_backbone_levels = len(laterals)\n        for i in range(used_backbone_levels - 1, 0, -1):\n            laterals[i - 1] += F.interpolate(\n                laterals[i], scale_factor=2, mode='nearest')\n\n        # build outputs\n        # part 1: from original levels\n        outs = [\n            self.sges[i](self.fpn_convs[i](laterals[i])) for i in range(used_backbone_levels)\n        ]\n\n        # part 2: add extra levels\n        if self.num_outs > len(outs):\n            # use max pool to get more levels on top of outputs\n            # (e.g., Faster R-CNN, Mask R-CNN)\n            if not self.add_extra_convs:\n                for i in range(self.num_outs - used_backbone_levels):\n                    outs.append(F.max_pool2d(outs[-1], 1, stride=2))\n            # add conv layers on top of original feature maps (RetinaNet)\n            else:\n                assert False, 'we are not here'\n                orig = inputs[self.backbone_end_level - 1]\n                outs.append(self.fpn_convs[used_backbone_levels](orig))\n                for i in range(used_backbone_levels + 1, self.num_outs):\n                    # BUG: we should add relu before each extra conv\n                    outs.append(self.fpn_convs[i](outs[-1]))\n        return tuple(outs)\n"""
detection/mmdet/models/roi_extractors/__init__.py,0,"b""from .single_level import SingleRoIExtractor\n\n__all__ = ['SingleRoIExtractor']\n"""
detection/mmdet/models/roi_extractors/single_level.py,4,"b'from __future__ import division\n\nimport torch\nimport torch.nn as nn\n\nfrom mmdet import ops\nfrom ..registry import ROI_EXTRACTORS\n\n\n@ROI_EXTRACTORS.register_module\nclass SingleRoIExtractor(nn.Module):\n    """"""Extract RoI features from a single level feature map.\n\n    If there are mulitple input feature levels, each RoI is mapped to a level\n    according to its scale.\n\n    Args:\n        roi_layer (dict): Specify RoI layer type and arguments.\n        out_channels (int): Output channels of RoI layers.\n        featmap_strides (int): Strides of input feature maps.\n        finest_scale (int): Scale threshold of mapping to level 0.\n    """"""\n\n    def __init__(self,\n                 roi_layer,\n                 out_channels,\n                 featmap_strides,\n                 finest_scale=56):\n        super(SingleRoIExtractor, self).__init__()\n        self.roi_layers = self.build_roi_layers(roi_layer, featmap_strides)\n        self.out_channels = out_channels\n        self.featmap_strides = featmap_strides\n        self.finest_scale = finest_scale\n\n    @property\n    def num_inputs(self):\n        """"""int: Input feature map levels.""""""\n        return len(self.featmap_strides)\n\n    def init_weights(self):\n        pass\n\n    def build_roi_layers(self, layer_cfg, featmap_strides):\n        cfg = layer_cfg.copy()\n        layer_type = cfg.pop(\'type\')\n        assert hasattr(ops, layer_type)\n        layer_cls = getattr(ops, layer_type)\n        roi_layers = nn.ModuleList(\n            [layer_cls(spatial_scale=1 / s, **cfg) for s in featmap_strides])\n        return roi_layers\n\n    def map_roi_levels(self, rois, num_levels):\n        """"""Map rois to corresponding feature levels by scales.\n\n        - scale < finest_scale: level 0\n        - finest_scale <= scale < finest_scale * 2: level 1\n        - finest_scale * 2 <= scale < finest_scale * 4: level 2\n        - scale >= finest_scale * 4: level 3\n\n        Args:\n            rois (Tensor): Input RoIs, shape (k, 5).\n            num_levels (int): Total level number.\n\n        Returns:\n            Tensor: Level index (0-based) of each RoI, shape (k, )\n        """"""\n        scale = torch.sqrt(\n            (rois[:, 3] - rois[:, 1] + 1) * (rois[:, 4] - rois[:, 2] + 1))\n        target_lvls = torch.floor(torch.log2(scale / self.finest_scale + 1e-6))\n        target_lvls = target_lvls.clamp(min=0, max=num_levels - 1).long()\n        return target_lvls\n\n    def forward(self, feats, rois):\n        if len(feats) == 1:\n            return self.roi_layers[0](feats[0], rois)\n\n        out_size = self.roi_layers[0].out_size\n        num_levels = len(feats)\n        target_lvls = self.map_roi_levels(rois, num_levels)\n        roi_feats = torch.cuda.FloatTensor(rois.size()[0], self.out_channels,\n                                           out_size, out_size).fill_(0)\n        for i in range(num_levels):\n            inds = target_lvls == i\n            if inds.any():\n                rois_ = rois[inds, :]\n                roi_feats_t = self.roi_layers[i](feats[i], rois_)\n                roi_feats[inds] += roi_feats_t\n        return roi_feats\n'"
detection/mmdet/models/utils/__init__.py,0,"b""from .conv_module import ConvModule\nfrom .norm import build_norm_layer\nfrom .weight_init import (xavier_init, normal_init, uniform_init, kaiming_init,\n                          bias_init_with_prob)\n\n__all__ = [\n    'ConvModule', 'build_norm_layer', 'xavier_init', 'normal_init',\n    'uniform_init', 'kaiming_init', 'bias_init_with_prob'\n]\n"""
detection/mmdet/models/utils/conv_module.py,1,"b""import warnings\n\nimport torch.nn as nn\nfrom mmcv.cnn import kaiming_init, constant_init\n\nfrom .norm import build_norm_layer\n\n\nclass ConvModule(nn.Module):\n\n    def __init__(self,\n                 in_channels,\n                 out_channels,\n                 kernel_size,\n                 stride=1,\n                 padding=0,\n                 dilation=1,\n                 groups=1,\n                 bias=True,\n                 normalize=None,\n                 activation='relu',\n                 inplace=True,\n                 activate_last=True):\n        super(ConvModule, self).__init__()\n        self.with_norm = normalize is not None\n        self.with_activatation = activation is not None\n        self.with_bias = bias\n        self.activation = activation\n        self.activate_last = activate_last\n\n        if self.with_norm and self.with_bias:\n            warnings.warn('ConvModule has norm and bias at the same time')\n\n        self.conv = nn.Conv2d(\n            in_channels,\n            out_channels,\n            kernel_size,\n            stride,\n            padding,\n            dilation,\n            groups,\n            bias=bias)\n\n        self.in_channels = self.conv.in_channels\n        self.out_channels = self.conv.out_channels\n        self.kernel_size = self.conv.kernel_size\n        self.stride = self.conv.stride\n        self.padding = self.conv.padding\n        self.dilation = self.conv.dilation\n        self.transposed = self.conv.transposed\n        self.output_padding = self.conv.output_padding\n        self.groups = self.conv.groups\n\n        if self.with_norm:\n            norm_channels = out_channels if self.activate_last else in_channels\n            self.norm_name, norm = build_norm_layer(normalize, norm_channels)\n            self.add_module(self.norm_name, norm)\n\n        if self.with_activatation:\n            assert activation in ['relu'], 'Only ReLU supported.'\n            if self.activation == 'relu':\n                self.activate = nn.ReLU(inplace=inplace)\n\n        # Default using msra init\n        self.init_weights()\n\n    @property\n    def norm(self):\n        return getattr(self, self.norm_name)\n\n    def init_weights(self):\n        nonlinearity = 'relu' if self.activation is None else self.activation\n        kaiming_init(self.conv, nonlinearity=nonlinearity)\n        if self.with_norm:\n            constant_init(self.norm, 1, bias=0)\n\n    def forward(self, x, activate=True, norm=True):\n        if self.activate_last:\n            x = self.conv(x)\n            if norm and self.with_norm:\n                x = self.norm(x)\n            if activate and self.with_activatation:\n                x = self.activate(x)\n        else:\n            if norm and self.with_norm:\n                x = self.norm(x)\n            if activate and self.with_activatation:\n                x = self.activate(x)\n            x = self.conv(x)\n        return x\n"""
detection/mmdet/models/utils/norm.py,1,"b'import torch.nn as nn\n\n\nnorm_cfg = {\n    # format: layer_type: (abbreviation, module)\n    \'BN\': (\'bn\', nn.BatchNorm2d),\n    \'SyncBN\': (\'bn\', None),\n    \'GN\': (\'gn\', nn.GroupNorm),\n    # and potentially \'SN\'\n}\n\n\ndef build_norm_layer(cfg, num_features, postfix=\'\'):\n    """""" Build normalization layer\n\n    Args:\n        cfg (dict): cfg should contain:\n            type (str): identify norm layer type.\n            layer args: args needed to instantiate a norm layer.\n            frozen (bool): [optional] whether stop gradient updates\n                of norm layer, it is helpful to set frozen mode\n                in backbone\'s norms.\n        num_features (int): number of channels from input\n        postfix (int, str): appended into norm abbreation to\n            create named layer.\n\n    Returns:\n        name (str): abbreation + postfix\n        layer (nn.Module): created norm layer\n    """"""\n    assert isinstance(cfg, dict) and \'type\' in cfg\n    cfg_ = cfg.copy()\n\n    layer_type = cfg_.pop(\'type\')\n    if layer_type not in norm_cfg:\n        raise KeyError(\'Unrecognized norm type {}\'.format(layer_type))\n    else:\n        abbr, norm_layer = norm_cfg[layer_type]\n        if norm_layer is None:\n            raise NotImplementedError\n\n    assert isinstance(postfix, (int, str))\n    name = abbr + str(postfix)\n\n    frozen = cfg_.pop(\'frozen\', False)\n    cfg_.setdefault(\'eps\', 1e-5)\n    if layer_type != \'GN\':\n        layer = norm_layer(num_features, **cfg_)\n    else:\n        assert \'num_groups\' in cfg_\n        layer = norm_layer(num_channels=num_features, **cfg_)\n\n    if frozen:\n        for param in layer.parameters():\n            param.requires_grad = False\n\n    return name, layer\n'"
detection/mmdet/models/utils/weight_init.py,1,"b'import numpy as np\nimport torch.nn as nn\n\n\ndef xavier_init(module, gain=1, bias=0, distribution=\'normal\'):\n    assert distribution in [\'uniform\', \'normal\']\n    if distribution == \'uniform\':\n        nn.init.xavier_uniform_(module.weight, gain=gain)\n    else:\n        nn.init.xavier_normal_(module.weight, gain=gain)\n    if hasattr(module, \'bias\'):\n        nn.init.constant_(module.bias, bias)\n\n\ndef normal_init(module, mean=0, std=1, bias=0):\n    nn.init.normal_(module.weight, mean, std)\n    if hasattr(module, \'bias\'):\n        nn.init.constant_(module.bias, bias)\n\n\ndef uniform_init(module, a=0, b=1, bias=0):\n    nn.init.uniform_(module.weight, a, b)\n    if hasattr(module, \'bias\'):\n        nn.init.constant_(module.bias, bias)\n\n\ndef kaiming_init(module,\n                 mode=\'fan_out\',\n                 nonlinearity=\'relu\',\n                 bias=0,\n                 distribution=\'normal\'):\n    assert distribution in [\'uniform\', \'normal\']\n    if distribution == \'uniform\':\n        nn.init.kaiming_uniform_(\n            module.weight, mode=mode, nonlinearity=nonlinearity)\n    else:\n        nn.init.kaiming_normal_(\n            module.weight, mode=mode, nonlinearity=nonlinearity)\n    if hasattr(module, \'bias\'):\n        nn.init.constant_(module.bias, bias)\n\n\ndef bias_init_with_prob(prior_prob):\n    """""" initialize conv/fc bias value according to giving probablity""""""\n    bias_init = float(-np.log((1 - prior_prob) / prior_prob))\n    return bias_init\n'"
detection/mmdet/ops/dcn/__init__.py,0,"b""from .functions.deform_conv import deform_conv, modulated_deform_conv\nfrom .functions.deform_pool import deform_roi_pooling\nfrom .modules.deform_conv import (DeformConv, ModulatedDeformConv,\n                                  ModulatedDeformConvPack)\nfrom .modules.deform_pool import (DeformRoIPooling, DeformRoIPoolingPack,\n                                  ModulatedDeformRoIPoolingPack)\n\n__all__ = [\n    'DeformConv', 'DeformRoIPooling', 'DeformRoIPoolingPack',\n    'ModulatedDeformRoIPoolingPack', 'ModulatedDeformConv',\n    'ModulatedDeformConvPack', 'deform_conv',\n    'modulated_deform_conv', 'deform_roi_pooling'\n]\n"""
detection/mmdet/ops/dcn/setup.py,1,"b""from setuptools import setup\nfrom torch.utils.cpp_extension import BuildExtension, CUDAExtension\n\nsetup(\n    name='deform_conv',\n    ext_modules=[\n        CUDAExtension('deform_conv_cuda', [\n            'src/deform_conv_cuda.cpp',\n            'src/deform_conv_cuda_kernel.cu',\n        ]),\n        CUDAExtension('deform_pool_cuda', [\n            'src/deform_pool_cuda.cpp', 'src/deform_pool_cuda_kernel.cu'\n        ]),\n    ],\n    cmdclass={'build_ext': BuildExtension})\n"""
detection/mmdet/ops/nms/__init__.py,0,"b""from .nms_wrapper import nms, soft_nms\n\n__all__ = ['nms', 'soft_nms']\n"""
detection/mmdet/ops/nms/nms_wrapper.py,4,"b'import numpy as np\nimport torch\n\nfrom .gpu_nms import gpu_nms\nfrom .cpu_nms import cpu_nms\nfrom .cpu_soft_nms import cpu_soft_nms\n\n\ndef nms(dets, iou_thr, device_id=None):\n    """"""Dispatch to either CPU or GPU NMS implementations.""""""\n    if isinstance(dets, torch.Tensor):\n        is_tensor = True\n        if dets.is_cuda:\n            device_id = dets.get_device()\n        dets_np = dets.detach().cpu().numpy()\n    elif isinstance(dets, np.ndarray):\n        is_tensor = False\n        dets_np = dets\n    else:\n        raise TypeError(\n            \'dets must be either a Tensor or numpy array, but got {}\'.format(\n                type(dets)))\n\n    if dets_np.shape[0] == 0:\n        inds = []\n    else:\n        inds = (gpu_nms(dets_np, iou_thr, device_id=device_id)\n                if device_id is not None else cpu_nms(dets_np, iou_thr))\n\n    if is_tensor:\n        inds = dets.new_tensor(inds, dtype=torch.long)\n    else:\n        inds = np.array(inds, dtype=np.int64)\n    return dets[inds, :], inds\n\n\ndef soft_nms(dets, iou_thr, method=\'linear\', sigma=0.5, min_score=1e-3):\n    if isinstance(dets, torch.Tensor):\n        is_tensor = True\n        dets_np = dets.detach().cpu().numpy()\n    elif isinstance(dets, np.ndarray):\n        is_tensor = False\n        dets_np = dets\n    else:\n        raise TypeError(\n            \'dets must be either a Tensor or numpy array, but got {}\'.format(\n                type(dets)))\n\n    method_codes = {\'linear\': 1, \'gaussian\': 2}\n    if method not in method_codes:\n        raise ValueError(\'Invalid method for SoftNMS: {}\'.format(method))\n    new_dets, inds = cpu_soft_nms(\n        dets_np,\n        iou_thr,\n        method=method_codes[method],\n        sigma=sigma,\n        min_score=min_score)\n\n    if is_tensor:\n        return dets.new_tensor(new_dets), dets.new_tensor(\n            inds, dtype=torch.long)\n    else:\n        return new_dets.astype(np.float32), inds.astype(np.int64)\n'"
detection/mmdet/ops/nms/setup.py,0,"b'import os.path as osp\nfrom distutils.core import setup, Extension\n\nimport numpy as np\nfrom Cython.Build import cythonize\nfrom Cython.Distutils import build_ext\n\n# extensions\next_args = dict(\n    include_dirs=[np.get_include()],\n    language=\'c++\',\n    extra_compile_args={\n        \'cc\': [\'-Wno-unused-function\', \'-Wno-write-strings\'],\n        \'nvcc\': [\'-c\', \'--compiler-options\', \'-fPIC\'],\n    },\n)\n\nextensions = [\n    Extension(\'cpu_nms\', [\'cpu_nms.pyx\'], **ext_args),\n    Extension(\'cpu_soft_nms\', [\'cpu_soft_nms.pyx\'], **ext_args),\n    Extension(\'gpu_nms\', [\'gpu_nms.pyx\', \'nms_kernel.cu\'], **ext_args),\n]\n\n\ndef customize_compiler_for_nvcc(self):\n    """"""inject deep into distutils to customize how the dispatch\n    to cc/nvcc works.\n    If you subclass UnixCCompiler, it\'s not trivial to get your subclass\n    injected in, and still have the right customizations (i.e.\n    distutils.sysconfig.customize_compiler) run on it. So instead of going\n    the OO route, I have this. Note, it\'s kindof like a wierd functional\n    subclassing going on.""""""\n\n    # tell the compiler it can processes .cu\n    self.src_extensions.append(\'.cu\')\n\n    # save references to the default compiler_so and _comple methods\n    default_compiler_so = self.compiler_so\n    super = self._compile\n\n    # now redefine the _compile method. This gets executed for each\n    # object but distutils doesn\'t have the ability to change compilers\n    # based on source extension: we add it.\n    def _compile(obj, src, ext, cc_args, extra_postargs, pp_opts):\n        if osp.splitext(src)[1] == \'.cu\':\n            # use the cuda for .cu files\n            self.set_executable(\'compiler_so\', \'nvcc\')\n            # use only a subset of the extra_postargs, which are 1-1 translated\n            # from the extra_compile_args in the Extension class\n            postargs = extra_postargs[\'nvcc\']\n        else:\n            postargs = extra_postargs[\'cc\']\n\n        super(obj, src, ext, cc_args, postargs, pp_opts)\n        # reset the default compiler_so, which we might have changed for cuda\n        self.compiler_so = default_compiler_so\n\n    # inject our redefined _compile method into the class\n    self._compile = _compile\n\n\n# run the customize_compiler\nclass custom_build_ext(build_ext):\n\n    def build_extensions(self):\n        customize_compiler_for_nvcc(self.compiler)\n        build_ext.build_extensions(self)\n\n\nsetup(\n    name=\'nms\',\n    cmdclass={\'build_ext\': custom_build_ext},\n    ext_modules=cythonize(extensions),\n)\n'"
detection/mmdet/ops/roi_align/__init__.py,0,"b""from .functions.roi_align import roi_align\nfrom .modules.roi_align import RoIAlign\n\n__all__ = ['roi_align', 'RoIAlign']\n"""
detection/mmdet/ops/roi_align/gradcheck.py,3,"b""import numpy as np\nimport torch\nfrom torch.autograd import gradcheck\n\nimport os.path as osp\nimport sys\nsys.path.append(osp.abspath(osp.join(__file__, '../../')))\nfrom roi_align import RoIAlign  # noqa: E402\n\nfeat_size = 15\nspatial_scale = 1.0 / 8\nimg_size = feat_size / spatial_scale\nnum_imgs = 2\nnum_rois = 20\n\nbatch_ind = np.random.randint(num_imgs, size=(num_rois, 1))\nrois = np.random.rand(num_rois, 4) * img_size * 0.5\nrois[:, 2:] += img_size * 0.5\nrois = np.hstack((batch_ind, rois))\n\nfeat = torch.randn(\n    num_imgs, 16, feat_size, feat_size, requires_grad=True, device='cuda:0')\nrois = torch.from_numpy(rois).float().cuda()\ninputs = (feat, rois)\nprint('Gradcheck for roi align...')\ntest = gradcheck(RoIAlign(3, spatial_scale), inputs, atol=1e-3, eps=1e-3)\nprint(test)\ntest = gradcheck(RoIAlign(3, spatial_scale, 2), inputs, atol=1e-3, eps=1e-3)\nprint(test)\n"""
detection/mmdet/ops/roi_align/setup.py,1,"b""from setuptools import setup\nfrom torch.utils.cpp_extension import BuildExtension, CUDAExtension\n\nsetup(\n    name='roi_align_cuda',\n    ext_modules=[\n        CUDAExtension('roi_align_cuda', [\n            'src/roi_align_cuda.cpp',\n            'src/roi_align_kernel.cu',\n        ]),\n    ],\n    cmdclass={'build_ext': BuildExtension})\n"""
detection/mmdet/ops/roi_pool/__init__.py,0,"b""from .functions.roi_pool import roi_pool\nfrom .modules.roi_pool import RoIPool\n\n__all__ = ['roi_pool', 'RoIPool']\n"""
detection/mmdet/ops/roi_pool/gradcheck.py,3,"b""import torch\nfrom torch.autograd import gradcheck\n\nimport os.path as osp\nimport sys\nsys.path.append(osp.abspath(osp.join(__file__, '../../')))\nfrom roi_pool import RoIPool  # noqa: E402\n\nfeat = torch.randn(4, 16, 15, 15, requires_grad=True).cuda()\nrois = torch.Tensor([[0, 0, 0, 50, 50], [0, 10, 30, 43, 55],\n                     [1, 67, 40, 110, 120]]).cuda()\ninputs = (feat, rois)\nprint('Gradcheck for roi pooling...')\ntest = gradcheck(RoIPool(4, 1.0 / 8), inputs, eps=1e-5, atol=1e-3)\nprint(test)\n"""
detection/mmdet/ops/roi_pool/setup.py,1,"b""from setuptools import setup\nfrom torch.utils.cpp_extension import BuildExtension, CUDAExtension\n\nsetup(\n    name='roi_pool',\n    ext_modules=[\n        CUDAExtension('roi_pool_cuda', [\n            'src/roi_pool_cuda.cpp',\n            'src/roi_pool_kernel.cu',\n        ])\n    ],\n    cmdclass={'build_ext': BuildExtension})\n"""
detection/mmdet/core/bbox/assigners/__init__.py,0,"b""from .base_assigner import BaseAssigner\nfrom .max_iou_assigner import MaxIoUAssigner\nfrom .assign_result import AssignResult\n\n__all__ = ['BaseAssigner', 'MaxIoUAssigner', 'AssignResult']\n"""
detection/mmdet/core/bbox/assigners/assign_result.py,5,"b'import torch\n\n\nclass AssignResult(object):\n\n    def __init__(self, num_gts, gt_inds, max_overlaps, labels=None):\n        self.num_gts = num_gts\n        self.gt_inds = gt_inds\n        self.max_overlaps = max_overlaps\n        self.labels = labels\n\n    def add_gt_(self, gt_labels):\n        self_inds = torch.arange(\n            1, len(gt_labels) + 1, dtype=torch.long, device=gt_labels.device)\n        self.gt_inds = torch.cat([self_inds, self.gt_inds])\n        self.max_overlaps = torch.cat(\n            [self.max_overlaps.new_ones(self.num_gts), self.max_overlaps])\n        if self.labels is not None:\n            self.labels = torch.cat([gt_labels, self.labels])\n'"
detection/mmdet/core/bbox/assigners/base_assigner.py,0,"b'from abc import ABCMeta, abstractmethod\n\n\nclass BaseAssigner(metaclass=ABCMeta):\n\n    @abstractmethod\n    def assign(self, bboxes, gt_bboxes, gt_bboxes_ignore=None, gt_labels=None):\n        pass\n'"
detection/mmdet/core/bbox/assigners/max_iou_assigner.py,2,"b'import torch\n\nfrom .base_assigner import BaseAssigner\nfrom .assign_result import AssignResult\nfrom ..geometry import bbox_overlaps\n\n\nclass MaxIoUAssigner(BaseAssigner):\n    """"""Assign a corresponding gt bbox or background to each bbox.\n\n    Each proposals will be assigned with `-1`, `0`, or a positive integer\n    indicating the ground truth index.\n\n    - -1: don\'t care\n    - 0: negative sample, no assigned gt\n    - positive integer: positive sample, index (1-based) of assigned gt\n\n    Args:\n        pos_iou_thr (float): IoU threshold for positive bboxes.\n        neg_iou_thr (float or tuple): IoU threshold for negative bboxes.\n        min_pos_iou (float): Minimum iou for a bbox to be considered as a\n            positive bbox. Positive samples can have smaller IoU than\n            pos_iou_thr due to the 4th step (assign max IoU sample to each gt).\n        gt_max_assign_all (bool): Whether to assign all bboxes with the same\n            highest overlap with some gt to that gt.\n        ignore_iof_thr (float): IoF threshold for ignoring bboxes (if\n            `gt_bboxes_ignore` is specified). Negative values mean not\n            ignoring any bboxes.\n        ignore_wrt_candidates (bool): Whether to compute the iof between\n            `bboxes` and `gt_bboxes_ignore`, or the contrary.\n    """"""\n\n    def __init__(self,\n                 pos_iou_thr,\n                 neg_iou_thr,\n                 min_pos_iou=.0,\n                 gt_max_assign_all=True,\n                 ignore_iof_thr=-1,\n                 ignore_wrt_candidates=True):\n        self.pos_iou_thr = pos_iou_thr\n        self.neg_iou_thr = neg_iou_thr\n        self.min_pos_iou = min_pos_iou\n        self.gt_max_assign_all = gt_max_assign_all\n        self.ignore_iof_thr = ignore_iof_thr\n        self.ignore_wrt_candidates = ignore_wrt_candidates\n\n    def assign(self, bboxes, gt_bboxes, gt_bboxes_ignore=None, gt_labels=None):\n        """"""Assign gt to bboxes.\n\n        This method assign a gt bbox to every bbox (proposal/anchor), each bbox\n        will be assigned with -1, 0, or a positive number. -1 means don\'t care,\n        0 means negative sample, positive number is the index (1-based) of\n        assigned gt.\n        The assignment is done in following steps, the order matters.\n\n        1. assign every bbox to -1\n        2. assign proposals whose iou with all gts < neg_iou_thr to 0\n        3. for each bbox, if the iou with its nearest gt >= pos_iou_thr,\n           assign it to that bbox\n        4. for each gt bbox, assign its nearest proposals (may be more than\n           one) to itself\n\n        Args:\n            bboxes (Tensor): Bounding boxes to be assigned, shape(n, 4).\n            gt_bboxes (Tensor): Groundtruth boxes, shape (k, 4).\n            gt_bboxes_ignore (Tensor, optional): Ground truth bboxes that are\n                labelled as `ignored`, e.g., crowd boxes in COCO.\n            gt_labels (Tensor, optional): Label of gt_bboxes, shape (k, ).\n\n        Returns:\n            :obj:`AssignResult`: The assign result.\n        """"""\n        if bboxes.shape[0] == 0 or gt_bboxes.shape[0] == 0:\n            raise ValueError(\'No gt or bboxes\')\n        bboxes = bboxes[:, :4]\n        overlaps = bbox_overlaps(gt_bboxes, bboxes)\n\n        if (self.ignore_iof_thr > 0) and (gt_bboxes_ignore is not None) and (\n                gt_bboxes_ignore.numel() > 0):\n            if self.ignore_wrt_candidates:\n                ignore_overlaps = bbox_overlaps(\n                    bboxes, gt_bboxes_ignore, mode=\'iof\')\n                ignore_max_overlaps, _ = ignore_overlaps.max(dim=1)\n            else:\n                ignore_overlaps = bbox_overlaps(\n                    gt_bboxes_ignore, bboxes, mode=\'iof\')\n                ignore_max_overlaps, _ = ignore_overlaps.max(dim=0)\n            overlaps[:, ignore_max_overlaps > self.ignore_iof_thr] = -1\n\n        assign_result = self.assign_wrt_overlaps(overlaps, gt_labels)\n        return assign_result\n\n    def assign_wrt_overlaps(self, overlaps, gt_labels=None):\n        """"""Assign w.r.t. the overlaps of bboxes with gts.\n\n        Args:\n            overlaps (Tensor): Overlaps between k gt_bboxes and n bboxes,\n                shape(k, n).\n            gt_labels (Tensor, optional): Labels of k gt_bboxes, shape (k, ).\n\n        Returns:\n            :obj:`AssignResult`: The assign result.\n        """"""\n        if overlaps.numel() == 0:\n            raise ValueError(\'No gt or proposals\')\n\n        num_gts, num_bboxes = overlaps.size(0), overlaps.size(1)\n\n        # 1. assign -1 by default\n        assigned_gt_inds = overlaps.new_full(\n            (num_bboxes, ), -1, dtype=torch.long)\n\n        # for each anchor, which gt best overlaps with it\n        # for each anchor, the max iou of all gts\n        max_overlaps, argmax_overlaps = overlaps.max(dim=0)\n        # for each gt, which anchor best overlaps with it\n        # for each gt, the max iou of all proposals\n        gt_max_overlaps, gt_argmax_overlaps = overlaps.max(dim=1)\n\n        # 2. assign negative: below\n        if isinstance(self.neg_iou_thr, float):\n            assigned_gt_inds[(max_overlaps >= 0)\n                             & (max_overlaps < self.neg_iou_thr)] = 0\n        elif isinstance(self.neg_iou_thr, tuple):\n            assert len(self.neg_iou_thr) == 2\n            assigned_gt_inds[(max_overlaps >= self.neg_iou_thr[0])\n                             & (max_overlaps < self.neg_iou_thr[1])] = 0\n\n        # 3. assign positive: above positive IoU threshold\n        pos_inds = max_overlaps >= self.pos_iou_thr\n        assigned_gt_inds[pos_inds] = argmax_overlaps[pos_inds] + 1\n\n        # 4. assign fg: for each gt, proposals with highest IoU\n        for i in range(num_gts):\n            if gt_max_overlaps[i] >= self.min_pos_iou:\n                if self.gt_max_assign_all:\n                    max_iou_inds = overlaps[i, :] == gt_max_overlaps[i]\n                    assigned_gt_inds[max_iou_inds] = i + 1\n                else:\n                    assigned_gt_inds[gt_argmax_overlaps[i]] = i + 1\n\n        if gt_labels is not None:\n            assigned_labels = assigned_gt_inds.new_zeros((num_bboxes, ))\n            pos_inds = torch.nonzero(assigned_gt_inds > 0).squeeze()\n            if pos_inds.numel() > 0:\n                assigned_labels[pos_inds] = gt_labels[\n                    assigned_gt_inds[pos_inds] - 1]\n        else:\n            assigned_labels = None\n\n        return AssignResult(\n            num_gts, assigned_gt_inds, max_overlaps, labels=assigned_labels)\n'"
detection/mmdet/core/bbox/samplers/__init__.py,0,"b""from .base_sampler import BaseSampler\nfrom .pseudo_sampler import PseudoSampler\nfrom .random_sampler import RandomSampler\nfrom .instance_balanced_pos_sampler import InstanceBalancedPosSampler\nfrom .iou_balanced_neg_sampler import IoUBalancedNegSampler\nfrom .combined_sampler import CombinedSampler\nfrom .ohem_sampler import OHEMSampler\nfrom .sampling_result import SamplingResult\n\n__all__ = [\n    'BaseSampler', 'PseudoSampler', 'RandomSampler',\n    'InstanceBalancedPosSampler', 'IoUBalancedNegSampler', 'CombinedSampler',\n    'OHEMSampler', 'SamplingResult'\n]\n"""
detection/mmdet/core/bbox/samplers/base_sampler.py,4,"b'from abc import ABCMeta, abstractmethod\n\nimport torch\n\nfrom .sampling_result import SamplingResult\n\n\nclass BaseSampler(metaclass=ABCMeta):\n\n    def __init__(self,\n                 num,\n                 pos_fraction,\n                 neg_pos_ub=-1,\n                 add_gt_as_proposals=True,\n                 **kwargs):\n        self.num = num\n        self.pos_fraction = pos_fraction\n        self.neg_pos_ub = neg_pos_ub\n        self.add_gt_as_proposals = add_gt_as_proposals\n        self.pos_sampler = self\n        self.neg_sampler = self\n\n    @abstractmethod\n    def _sample_pos(self, assign_result, num_expected, **kwargs):\n        pass\n\n    @abstractmethod\n    def _sample_neg(self, assign_result, num_expected, **kwargs):\n        pass\n\n    def sample(self,\n               assign_result,\n               bboxes,\n               gt_bboxes,\n               gt_labels=None,\n               **kwargs):\n        """"""Sample positive and negative bboxes.\n\n        This is a simple implementation of bbox sampling given candidates,\n        assigning results and ground truth bboxes.\n\n        Args:\n            assign_result (:obj:`AssignResult`): Bbox assigning results.\n            bboxes (Tensor): Boxes to be sampled from.\n            gt_bboxes (Tensor): Ground truth bboxes.\n            gt_labels (Tensor, optional): Class labels of ground truth bboxes.\n\n        Returns:\n            :obj:`SamplingResult`: Sampling result.\n        """"""\n        bboxes = bboxes[:, :4]\n\n        gt_flags = bboxes.new_zeros((bboxes.shape[0], ), dtype=torch.uint8)\n        if self.add_gt_as_proposals:\n            bboxes = torch.cat([gt_bboxes, bboxes], dim=0)\n            assign_result.add_gt_(gt_labels)\n            gt_ones = bboxes.new_ones(gt_bboxes.shape[0], dtype=torch.uint8)\n            gt_flags = torch.cat([gt_ones, gt_flags])\n\n        num_expected_pos = int(self.num * self.pos_fraction)\n        pos_inds = self.pos_sampler._sample_pos(\n            assign_result, num_expected_pos, bboxes=bboxes, **kwargs)\n        # We found that sampled indices have duplicated items occasionally.\n        # (may be a bug of PyTorch)\n        pos_inds = pos_inds.unique()\n        num_sampled_pos = pos_inds.numel()\n        num_expected_neg = self.num - num_sampled_pos\n        if self.neg_pos_ub >= 0:\n            _pos = max(1, num_sampled_pos)\n            neg_upper_bound = int(self.neg_pos_ub * _pos)\n            if num_expected_neg > neg_upper_bound:\n                num_expected_neg = neg_upper_bound\n        neg_inds = self.neg_sampler._sample_neg(\n            assign_result, num_expected_neg, bboxes=bboxes, **kwargs)\n        neg_inds = neg_inds.unique()\n\n        return SamplingResult(pos_inds, neg_inds, bboxes, gt_bboxes,\n                              assign_result, gt_flags)\n'"
detection/mmdet/core/bbox/samplers/combined_sampler.py,0,"b'from .base_sampler import BaseSampler\nfrom ..assign_sampling import build_sampler\n\n\nclass CombinedSampler(BaseSampler):\n\n    def __init__(self, pos_sampler, neg_sampler, **kwargs):\n        super(CombinedSampler, self).__init__(**kwargs)\n        self.pos_sampler = build_sampler(pos_sampler, **kwargs)\n        self.neg_sampler = build_sampler(neg_sampler, **kwargs)\n\n    def _sample_pos(self, **kwargs):\n        raise NotImplementedError\n\n    def _sample_neg(self, **kwargs):\n        raise NotImplementedError\n'"
detection/mmdet/core/bbox/samplers/instance_balanced_pos_sampler.py,5,"b'import numpy as np\nimport torch\n\nfrom .random_sampler import RandomSampler\n\n\nclass InstanceBalancedPosSampler(RandomSampler):\n\n    def _sample_pos(self, assign_result, num_expected, **kwargs):\n        pos_inds = torch.nonzero(assign_result.gt_inds > 0)\n        if pos_inds.numel() != 0:\n            pos_inds = pos_inds.squeeze(1)\n        if pos_inds.numel() <= num_expected:\n            return pos_inds\n        else:\n            unique_gt_inds = assign_result.gt_inds[pos_inds].unique()\n            num_gts = len(unique_gt_inds)\n            num_per_gt = int(round(num_expected / float(num_gts)) + 1)\n            sampled_inds = []\n            for i in unique_gt_inds:\n                inds = torch.nonzero(assign_result.gt_inds == i.item())\n                if inds.numel() != 0:\n                    inds = inds.squeeze(1)\n                else:\n                    continue\n                if len(inds) > num_per_gt:\n                    inds = self.random_choice(inds, num_per_gt)\n                sampled_inds.append(inds)\n            sampled_inds = torch.cat(sampled_inds)\n            if len(sampled_inds) < num_expected:\n                num_extra = num_expected - len(sampled_inds)\n                extra_inds = np.array(\n                    list(set(pos_inds.cpu()) - set(sampled_inds.cpu())))\n                if len(extra_inds) > num_extra:\n                    extra_inds = self.random_choice(extra_inds, num_extra)\n                extra_inds = torch.from_numpy(extra_inds).to(\n                    assign_result.gt_inds.device).long()\n                sampled_inds = torch.cat([sampled_inds, extra_inds])\n            elif len(sampled_inds) > num_expected:\n                sampled_inds = self.random_choice(sampled_inds, num_expected)\n            return sampled_inds\n'"
detection/mmdet/core/bbox/samplers/iou_balanced_neg_sampler.py,2,"b'import numpy as np\nimport torch\n\nfrom .random_sampler import RandomSampler\n\n\nclass IoUBalancedNegSampler(RandomSampler):\n\n    def __init__(self,\n                 num,\n                 pos_fraction,\n                 hard_thr=0.1,\n                 hard_fraction=0.5,\n                 **kwargs):\n        super(IoUBalancedNegSampler, self).__init__(num, pos_fraction,\n                                                    **kwargs)\n        assert hard_thr > 0\n        assert 0 < hard_fraction < 1\n        self.hard_thr = hard_thr\n        self.hard_fraction = hard_fraction\n\n    def _sample_neg(self, assign_result, num_expected, **kwargs):\n        neg_inds = torch.nonzero(assign_result.gt_inds == 0)\n        if neg_inds.numel() != 0:\n            neg_inds = neg_inds.squeeze(1)\n        if len(neg_inds) <= num_expected:\n            return neg_inds\n        else:\n            max_overlaps = assign_result.max_overlaps.cpu().numpy()\n            # balance sampling for negative samples\n            neg_set = set(neg_inds.cpu().numpy())\n            easy_set = set(\n                np.where(\n                    np.logical_and(max_overlaps >= 0,\n                                   max_overlaps < self.hard_thr))[0])\n            hard_set = set(np.where(max_overlaps >= self.hard_thr)[0])\n            easy_neg_inds = list(easy_set & neg_set)\n            hard_neg_inds = list(hard_set & neg_set)\n\n            num_expected_hard = int(num_expected * self.hard_fraction)\n            if len(hard_neg_inds) > num_expected_hard:\n                sampled_hard_inds = self.random_choice(hard_neg_inds,\n                                                       num_expected_hard)\n            else:\n                sampled_hard_inds = np.array(hard_neg_inds, dtype=np.int)\n            num_expected_easy = num_expected - len(sampled_hard_inds)\n            if len(easy_neg_inds) > num_expected_easy:\n                sampled_easy_inds = self.random_choice(easy_neg_inds,\n                                                       num_expected_easy)\n            else:\n                sampled_easy_inds = np.array(easy_neg_inds, dtype=np.int)\n            sampled_inds = np.concatenate((sampled_easy_inds,\n                                           sampled_hard_inds))\n            if len(sampled_inds) < num_expected:\n                num_extra = num_expected - len(sampled_inds)\n                extra_inds = np.array(list(neg_set - set(sampled_inds)))\n                if len(extra_inds) > num_extra:\n                    extra_inds = self.random_choice(extra_inds, num_extra)\n                sampled_inds = np.concatenate((sampled_inds, extra_inds))\n            sampled_inds = torch.from_numpy(sampled_inds).long().to(\n                assign_result.gt_inds.device)\n            return sampled_inds\n'"
detection/mmdet/core/bbox/samplers/ohem_sampler.py,3,"b""import torch\n\nfrom .base_sampler import BaseSampler\nfrom ..transforms import bbox2roi\n\n\nclass OHEMSampler(BaseSampler):\n\n    def __init__(self,\n                 num,\n                 pos_fraction,\n                 context,\n                 neg_pos_ub=-1,\n                 add_gt_as_proposals=True,\n                 **kwargs):\n        super(OHEMSampler, self).__init__(num, pos_fraction, neg_pos_ub,\n                                          add_gt_as_proposals)\n        self.bbox_roi_extractor = context.bbox_roi_extractor\n        self.bbox_head = context.bbox_head\n\n    def hard_mining(self, inds, num_expected, bboxes, labels, feats):\n        with torch.no_grad():\n            rois = bbox2roi([bboxes])\n            bbox_feats = self.bbox_roi_extractor(\n                feats[:self.bbox_roi_extractor.num_inputs], rois)\n            cls_score, _ = self.bbox_head(bbox_feats)\n            loss = self.bbox_head.loss(\n                cls_score=cls_score,\n                bbox_pred=None,\n                labels=labels,\n                label_weights=cls_score.new_ones(cls_score.size(0)),\n                bbox_targets=None,\n                bbox_weights=None,\n                reduce=False)['loss_cls']\n            _, topk_loss_inds = loss.topk(num_expected)\n        return inds[topk_loss_inds]\n\n    def _sample_pos(self,\n                    assign_result,\n                    num_expected,\n                    bboxes=None,\n                    feats=None,\n                    **kwargs):\n        # Sample some hard positive samples\n        pos_inds = torch.nonzero(assign_result.gt_inds > 0)\n        if pos_inds.numel() != 0:\n            pos_inds = pos_inds.squeeze(1)\n        if pos_inds.numel() <= num_expected:\n            return pos_inds\n        else:\n            return self.hard_mining(pos_inds, num_expected, bboxes[pos_inds],\n                                    assign_result.labels[pos_inds], feats)\n\n    def _sample_neg(self,\n                    assign_result,\n                    num_expected,\n                    bboxes=None,\n                    feats=None,\n                    **kwargs):\n        # Sample some hard negative samples\n        neg_inds = torch.nonzero(assign_result.gt_inds == 0)\n        if neg_inds.numel() != 0:\n            neg_inds = neg_inds.squeeze(1)\n        if len(neg_inds) <= num_expected:\n            return neg_inds\n        else:\n            return self.hard_mining(neg_inds, num_expected, bboxes[neg_inds],\n                                    assign_result.labels[neg_inds], feats)\n"""
detection/mmdet/core/bbox/samplers/pseudo_sampler.py,3,"b'import torch\n\nfrom .base_sampler import BaseSampler\nfrom .sampling_result import SamplingResult\n\n\nclass PseudoSampler(BaseSampler):\n\n    def __init__(self, **kwargs):\n        pass\n\n    def _sample_pos(self, **kwargs):\n        raise NotImplementedError\n\n    def _sample_neg(self, **kwargs):\n        raise NotImplementedError\n\n    def sample(self, assign_result, bboxes, gt_bboxes, **kwargs):\n        pos_inds = torch.nonzero(\n            assign_result.gt_inds > 0).squeeze(-1).unique()\n        neg_inds = torch.nonzero(\n            assign_result.gt_inds == 0).squeeze(-1).unique()\n        gt_flags = bboxes.new_zeros(bboxes.shape[0], dtype=torch.uint8)\n        sampling_result = SamplingResult(pos_inds, neg_inds, bboxes, gt_bboxes,\n                                         assign_result, gt_flags)\n        return sampling_result\n'"
detection/mmdet/core/bbox/samplers/random_sampler.py,3,"b'import numpy as np\nimport torch\n\nfrom .base_sampler import BaseSampler\n\n\nclass RandomSampler(BaseSampler):\n\n    def __init__(self,\n                 num,\n                 pos_fraction,\n                 neg_pos_ub=-1,\n                 add_gt_as_proposals=True,\n                 **kwargs):\n        super(RandomSampler, self).__init__(num, pos_fraction, neg_pos_ub,\n                                            add_gt_as_proposals)\n\n    @staticmethod\n    def random_choice(gallery, num):\n        """"""Random select some elements from the gallery.\n\n        It seems that Pytorch\'s implementation is slower than numpy so we use\n        numpy to randperm the indices.\n        """"""\n        assert len(gallery) >= num\n        if isinstance(gallery, list):\n            gallery = np.array(gallery)\n        cands = np.arange(len(gallery))\n        np.random.shuffle(cands)\n        rand_inds = cands[:num]\n        if not isinstance(gallery, np.ndarray):\n            rand_inds = torch.from_numpy(rand_inds).long().to(gallery.device)\n        return gallery[rand_inds]\n\n    def _sample_pos(self, assign_result, num_expected, **kwargs):\n        """"""Randomly sample some positive samples.""""""\n        pos_inds = torch.nonzero(assign_result.gt_inds > 0)\n        if pos_inds.numel() != 0:\n            pos_inds = pos_inds.squeeze(1)\n        if pos_inds.numel() <= num_expected:\n            return pos_inds\n        else:\n            return self.random_choice(pos_inds, num_expected)\n\n    def _sample_neg(self, assign_result, num_expected, **kwargs):\n        """"""Randomly sample some negative samples.""""""\n        neg_inds = torch.nonzero(assign_result.gt_inds == 0)\n        if neg_inds.numel() != 0:\n            neg_inds = neg_inds.squeeze(1)\n        if len(neg_inds) <= num_expected:\n            return neg_inds\n        else:\n            return self.random_choice(neg_inds, num_expected)\n'"
detection/mmdet/core/bbox/samplers/sampling_result.py,1,"b'import torch\n\n\nclass SamplingResult(object):\n\n    def __init__(self, pos_inds, neg_inds, bboxes, gt_bboxes, assign_result,\n                 gt_flags):\n        self.pos_inds = pos_inds\n        self.neg_inds = neg_inds\n        self.pos_bboxes = bboxes[pos_inds]\n        self.neg_bboxes = bboxes[neg_inds]\n        self.pos_is_gt = gt_flags[pos_inds]\n\n        self.num_gts = gt_bboxes.shape[0]\n        self.pos_assigned_gt_inds = assign_result.gt_inds[pos_inds] - 1\n        self.pos_gt_bboxes = gt_bboxes[self.pos_assigned_gt_inds, :]\n        if assign_result.labels is not None:\n            self.pos_gt_labels = assign_result.labels[pos_inds]\n        else:\n            self.pos_gt_labels = None\n\n    @property\n    def bboxes(self):\n        return torch.cat([self.pos_bboxes, self.neg_bboxes])\n'"
detection/mmdet/ops/dcn/functions/__init__.py,0,b''
detection/mmdet/ops/dcn/functions/deform_conv.py,10,"b'import torch\nfrom torch.autograd import Function\nfrom torch.nn.modules.utils import _pair\n\nfrom .. import deform_conv_cuda\n\n\nclass DeformConvFunction(Function):\n\n    @staticmethod\n    def forward(ctx,\n                input,\n                offset,\n                weight,\n                stride=1,\n                padding=0,\n                dilation=1,\n                groups=1,\n                deformable_groups=1,\n                im2col_step=64):\n        if input is not None and input.dim() != 4:\n            raise ValueError(\n                ""Expected 4D tensor as input, got {}D tensor instead."".format(\n                    input.dim()))\n        ctx.stride = _pair(stride)\n        ctx.padding = _pair(padding)\n        ctx.dilation = _pair(dilation)\n        ctx.groups = groups\n        ctx.deformable_groups = deformable_groups\n        ctx.im2col_step = im2col_step\n\n        ctx.save_for_backward(input, offset, weight)\n\n        output = input.new_empty(\n            DeformConvFunction._output_size(input, weight, ctx.padding,\n                                            ctx.dilation, ctx.stride))\n\n        ctx.bufs_ = [input.new_empty(0), input.new_empty(0)]  # columns, ones\n\n        if not input.is_cuda:\n            raise NotImplementedError\n        else:\n            cur_im2col_step = min(ctx.im2col_step, input.shape[0])\n            assert (input.shape[0] %\n                    cur_im2col_step) == 0, \'im2col step must divide batchsize\'\n            deform_conv_cuda.deform_conv_forward_cuda(\n                input, weight, offset, output, ctx.bufs_[0], ctx.bufs_[1],\n                weight.size(3), weight.size(2), ctx.stride[1], ctx.stride[0],\n                ctx.padding[1], ctx.padding[0], ctx.dilation[1],\n                ctx.dilation[0], ctx.groups, ctx.deformable_groups,\n                cur_im2col_step)\n        return output\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        input, offset, weight = ctx.saved_tensors\n\n        grad_input = grad_offset = grad_weight = None\n\n        if not grad_output.is_cuda:\n            raise NotImplementedError\n        else:\n            cur_im2col_step = min(ctx.im2col_step, input.shape[0])\n            assert (input.shape[0] %\n                    cur_im2col_step) == 0, \'im2col step must divide batchsize\'\n\n            if ctx.needs_input_grad[0] or ctx.needs_input_grad[1]:\n                grad_input = torch.zeros_like(input)\n                grad_offset = torch.zeros_like(offset)\n                deform_conv_cuda.deform_conv_backward_input_cuda(\n                    input, offset, grad_output, grad_input,\n                    grad_offset, weight, ctx.bufs_[0], weight.size(3),\n                    weight.size(2), ctx.stride[1], ctx.stride[0],\n                    ctx.padding[1], ctx.padding[0], ctx.dilation[1],\n                    ctx.dilation[0], ctx.groups, ctx.deformable_groups,\n                    cur_im2col_step)\n\n            if ctx.needs_input_grad[2]:\n                grad_weight = torch.zeros_like(weight)\n                deform_conv_cuda.deform_conv_backward_parameters_cuda(\n                    input, offset, grad_output,\n                    grad_weight, ctx.bufs_[0], ctx.bufs_[1], weight.size(3),\n                    weight.size(2), ctx.stride[1], ctx.stride[0],\n                    ctx.padding[1], ctx.padding[0], ctx.dilation[1],\n                    ctx.dilation[0], ctx.groups, ctx.deformable_groups, 1,\n                    cur_im2col_step)\n\n        return (grad_input, grad_offset, grad_weight, None, None, None, None,\n                None)\n\n    @staticmethod\n    def _output_size(input, weight, padding, dilation, stride):\n        channels = weight.size(0)\n        output_size = (input.size(0), channels)\n        for d in range(input.dim() - 2):\n            in_size = input.size(d + 2)\n            pad = padding[d]\n            kernel = dilation[d] * (weight.size(d + 2) - 1) + 1\n            stride_ = stride[d]\n            output_size += ((in_size + (2 * pad) - kernel) // stride_ + 1, )\n        if not all(map(lambda s: s > 0, output_size)):\n            raise ValueError(\n                ""convolution input is too small (output would be {})"".format(\n                    \'x\'.join(map(str, output_size))))\n        return output_size\n\n\nclass ModulatedDeformConvFunction(Function):\n\n    @staticmethod\n    def forward(ctx,\n                input,\n                offset,\n                mask,\n                weight,\n                bias=None,\n                stride=1,\n                padding=0,\n                dilation=1,\n                groups=1,\n                deformable_groups=1):\n        ctx.stride = stride\n        ctx.padding = padding\n        ctx.dilation = dilation\n        ctx.groups = groups\n        ctx.deformable_groups = deformable_groups\n        ctx.with_bias = bias is not None\n        if not ctx.with_bias:\n            bias = input.new_empty(1)  # fake tensor\n        if not input.is_cuda:\n            raise NotImplementedError\n        if weight.requires_grad or mask.requires_grad or offset.requires_grad \\\n                or input.requires_grad:\n            ctx.save_for_backward(input, offset, mask, weight, bias)\n        output = input.new_empty(\n            ModulatedDeformConvFunction._infer_shape(ctx, input, weight))\n        ctx._bufs = [input.new_empty(0), input.new_empty(0)]\n        deform_conv_cuda.modulated_deform_conv_cuda_forward(\n            input, weight, bias, ctx._bufs[0], offset, mask, output,\n            ctx._bufs[1], weight.shape[2], weight.shape[3], ctx.stride,\n            ctx.stride, ctx.padding, ctx.padding, ctx.dilation, ctx.dilation,\n            ctx.groups, ctx.deformable_groups, ctx.with_bias)\n        return output\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        if not grad_output.is_cuda:\n            raise NotImplementedError\n        input, offset, mask, weight, bias = ctx.saved_tensors\n        grad_input = torch.zeros_like(input)\n        grad_offset = torch.zeros_like(offset)\n        grad_mask = torch.zeros_like(mask)\n        grad_weight = torch.zeros_like(weight)\n        grad_bias = torch.zeros_like(bias)\n        deform_conv_cuda.modulated_deform_conv_cuda_backward(\n            input, weight, bias, ctx._bufs[0], offset, mask, ctx._bufs[1],\n            grad_input, grad_weight, grad_bias, grad_offset, grad_mask,\n            grad_output, weight.shape[2], weight.shape[3], ctx.stride,\n            ctx.stride, ctx.padding, ctx.padding, ctx.dilation, ctx.dilation,\n            ctx.groups, ctx.deformable_groups, ctx.with_bias)\n        if not ctx.with_bias:\n            grad_bias = None\n\n        return (grad_input, grad_offset, grad_mask, grad_weight, grad_bias,\n                None, None, None, None, None)\n\n    @staticmethod\n    def _infer_shape(ctx, input, weight):\n        n = input.size(0)\n        channels_out = weight.size(0)\n        height, width = input.shape[2:4]\n        kernel_h, kernel_w = weight.shape[2:4]\n        height_out = (height + 2 * ctx.padding -\n                      (ctx.dilation * (kernel_h - 1) + 1)) // ctx.stride + 1\n        width_out = (width + 2 * ctx.padding -\n                     (ctx.dilation * (kernel_w - 1) + 1)) // ctx.stride + 1\n        return n, channels_out, height_out, width_out\n\n\ndeform_conv = DeformConvFunction.apply\nmodulated_deform_conv = ModulatedDeformConvFunction.apply\n'"
detection/mmdet/ops/dcn/functions/deform_pool.py,3,"b'import torch\nfrom torch.autograd import Function\n\nfrom .. import deform_pool_cuda\n\n\nclass DeformRoIPoolingFunction(Function):\n\n    @staticmethod\n    def forward(ctx,\n                data,\n                rois,\n                offset,\n                spatial_scale,\n                out_size,\n                out_channels,\n                no_trans,\n                group_size=1,\n                part_size=None,\n                sample_per_part=4,\n                trans_std=.0):\n        ctx.spatial_scale = spatial_scale\n        ctx.out_size = out_size\n        ctx.out_channels = out_channels\n        ctx.no_trans = no_trans\n        ctx.group_size = group_size\n        ctx.part_size = out_size if part_size is None else part_size\n        ctx.sample_per_part = sample_per_part\n        ctx.trans_std = trans_std\n\n        assert 0.0 <= ctx.trans_std <= 1.0\n        if not data.is_cuda:\n            raise NotImplementedError\n\n        n = rois.shape[0]\n        output = data.new_empty(n, out_channels, out_size, out_size)\n        output_count = data.new_empty(n, out_channels, out_size, out_size)\n        deform_pool_cuda.deform_psroi_pooling_cuda_forward(\n            data, rois, offset, output, output_count, ctx.no_trans,\n            ctx.spatial_scale, ctx.out_channels, ctx.group_size, ctx.out_size,\n            ctx.part_size, ctx.sample_per_part, ctx.trans_std)\n\n        if data.requires_grad or rois.requires_grad or offset.requires_grad:\n            ctx.save_for_backward(data, rois, offset)\n        ctx.output_count = output_count\n\n        return output\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        if not grad_output.is_cuda:\n            raise NotImplementedError\n\n        data, rois, offset = ctx.saved_tensors\n        output_count = ctx.output_count\n        grad_input = torch.zeros_like(data)\n        grad_rois = None\n        grad_offset = torch.zeros_like(offset)\n\n        deform_pool_cuda.deform_psroi_pooling_cuda_backward(\n            grad_output, data, rois, offset, output_count, grad_input,\n            grad_offset, ctx.no_trans, ctx.spatial_scale, ctx.out_channels,\n            ctx.group_size, ctx.out_size, ctx.part_size, ctx.sample_per_part,\n            ctx.trans_std)\n        return (grad_input, grad_rois, grad_offset, None, None, None, None,\n                None, None, None, None)\n\n\ndeform_roi_pooling = DeformRoIPoolingFunction.apply\n'"
detection/mmdet/ops/dcn/modules/__init__.py,0,b''
detection/mmdet/ops/dcn/modules/deform_conv.py,8,"b""import math\n\nimport torch\nimport torch.nn as nn\nfrom torch.nn.modules.utils import _pair\n\nfrom ..functions.deform_conv import deform_conv, modulated_deform_conv\n\n\nclass DeformConv(nn.Module):\n\n    def __init__(self,\n                 in_channels,\n                 out_channels,\n                 kernel_size,\n                 stride=1,\n                 padding=0,\n                 dilation=1,\n                 groups=1,\n                 deformable_groups=1,\n                 bias=False):\n        assert not bias\n        super(DeformConv, self).__init__()\n\n        assert in_channels % groups == 0, \\\n            'in_channels {} cannot be divisible by groups {}'.format(\n                in_channels, groups)\n        assert out_channels % groups == 0, \\\n            'out_channels {} cannot be divisible by groups {}'.format(\n                out_channels, groups)\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        self.kernel_size = _pair(kernel_size)\n        self.stride = _pair(stride)\n        self.padding = _pair(padding)\n        self.dilation = _pair(dilation)\n        self.groups = groups\n        self.deformable_groups = deformable_groups\n\n        self.weight = nn.Parameter(\n            torch.Tensor(out_channels, in_channels // self.groups,\n                         *self.kernel_size))\n\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        n = self.in_channels\n        for k in self.kernel_size:\n            n *= k\n        stdv = 1. / math.sqrt(n)\n        self.weight.data.uniform_(-stdv, stdv)\n\n    def forward(self, input, offset):\n        return deform_conv(input, offset, self.weight, self.stride,\n                           self.padding, self.dilation, self.groups,\n                           self.deformable_groups)\n\n\nclass ModulatedDeformConv(nn.Module):\n\n    def __init__(self,\n                 in_channels,\n                 out_channels,\n                 kernel_size,\n                 stride=1,\n                 padding=0,\n                 dilation=1,\n                 groups=1,\n                 deformable_groups=1,\n                 bias=True):\n        super(ModulatedDeformConv, self).__init__()\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        self.kernel_size = _pair(kernel_size)\n        self.stride = stride\n        self.padding = padding\n        self.dilation = dilation\n        self.groups = groups\n        self.deformable_groups = deformable_groups\n        self.with_bias = bias\n\n        self.weight = nn.Parameter(\n            torch.Tensor(out_channels, in_channels // groups,\n                         *self.kernel_size))\n        if bias:\n            self.bias = nn.Parameter(torch.Tensor(out_channels))\n        else:\n            self.register_parameter('bias', None)\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        n = self.in_channels\n        for k in self.kernel_size:\n            n *= k\n        stdv = 1. / math.sqrt(n)\n        self.weight.data.uniform_(-stdv, stdv)\n        if self.bias is not None:\n            self.bias.data.zero_()\n\n    def forward(self, input, offset, mask):\n        return modulated_deform_conv(\n            input, offset, mask, self.weight, self.bias, self.stride,\n            self.padding, self.dilation, self.groups, self.deformable_groups)\n\n\nclass ModulatedDeformConvPack(ModulatedDeformConv):\n\n    def __init__(self,\n                 in_channels,\n                 out_channels,\n                 kernel_size,\n                 stride=1,\n                 padding=0,\n                 dilation=1,\n                 groups=1,\n                 deformable_groups=1,\n                 bias=True):\n        super(ModulatedDeformConvPack, self).__init__(\n            in_channels, out_channels, kernel_size, stride, padding, dilation,\n            groups, deformable_groups, bias)\n\n        self.conv_offset_mask = nn.Conv2d(\n            self.in_channels // self.groups,\n            self.deformable_groups * 3 * self.kernel_size[0] *\n            self.kernel_size[1],\n            kernel_size=self.kernel_size,\n            stride=_pair(self.stride),\n            padding=_pair(self.padding),\n            bias=True)\n        self.init_offset()\n\n    def init_offset(self):\n        self.conv_offset_mask.weight.data.zero_()\n        self.conv_offset_mask.bias.data.zero_()\n\n    def forward(self, input):\n        out = self.conv_offset_mask(input)\n        o1, o2, mask = torch.chunk(out, 3, dim=1)\n        offset = torch.cat((o1, o2), dim=1)\n        mask = torch.sigmoid(mask)\n        return modulated_deform_conv(\n            input, offset, mask, self.weight, self.bias, self.stride,\n            self.padding, self.dilation, self.groups, self.deformable_groups)\n"""
detection/mmdet/ops/dcn/modules/deform_pool.py,0,"b'from torch import nn\n\nfrom ..functions.deform_pool import deform_roi_pooling\n\n\nclass DeformRoIPooling(nn.Module):\n\n    def __init__(self,\n                 spatial_scale,\n                 out_size,\n                 out_channels,\n                 no_trans,\n                 group_size=1,\n                 part_size=None,\n                 sample_per_part=4,\n                 trans_std=.0):\n        super(DeformRoIPooling, self).__init__()\n        self.spatial_scale = spatial_scale\n        self.out_size = out_size\n        self.out_channels = out_channels\n        self.no_trans = no_trans\n        self.group_size = group_size\n        self.part_size = out_size if part_size is None else part_size\n        self.sample_per_part = sample_per_part\n        self.trans_std = trans_std\n\n    def forward(self, data, rois, offset):\n        if self.no_trans:\n            offset = data.new_empty(0)\n        return deform_roi_pooling(\n            data, rois, offset, self.spatial_scale, self.out_size,\n            self.out_channels, self.no_trans, self.group_size, self.part_size,\n            self.sample_per_part, self.trans_std)\n\n\nclass DeformRoIPoolingPack(DeformRoIPooling):\n\n    def __init__(self,\n                 spatial_scale,\n                 out_size,\n                 out_channels,\n                 no_trans,\n                 group_size=1,\n                 part_size=None,\n                 sample_per_part=4,\n                 trans_std=.0,\n                 deform_fc_channels=1024):\n        super(DeformRoIPoolingPack,\n              self).__init__(spatial_scale, out_size, out_channels, no_trans,\n                             group_size, part_size, sample_per_part, trans_std)\n\n        self.deform_fc_channels = deform_fc_channels\n\n        if not no_trans:\n            self.offset_fc = nn.Sequential(\n                nn.Linear(self.out_size * self.out_size * self.out_channels,\n                          self.deform_fc_channels),\n                nn.ReLU(inplace=True),\n                nn.Linear(self.deform_fc_channels, self.deform_fc_channels),\n                nn.ReLU(inplace=True),\n                nn.Linear(self.deform_fc_channels,\n                          self.out_size * self.out_size * 2))\n            self.offset_fc[-1].weight.data.zero_()\n            self.offset_fc[-1].bias.data.zero_()\n\n    def forward(self, data, rois):\n        assert data.size(1) == self.out_channels\n        if self.no_trans:\n            offset = data.new_empty(0)\n            return deform_roi_pooling(\n                data, rois, offset, self.spatial_scale, self.out_size,\n                self.out_channels, self.no_trans, self.group_size,\n                self.part_size, self.sample_per_part, self.trans_std)\n        else:\n            n = rois.shape[0]\n            offset = data.new_empty(0)\n            x = deform_roi_pooling(data, rois, offset, self.spatial_scale,\n                                   self.out_size, self.out_channels, True,\n                                   self.group_size, self.part_size,\n                                   self.sample_per_part, self.trans_std)\n            offset = self.offset_fc(x.view(n, -1))\n            offset = offset.view(n, 2, self.out_size, self.out_size)\n            return deform_roi_pooling(\n                data, rois, offset, self.spatial_scale, self.out_size,\n                self.out_channels, self.no_trans, self.group_size,\n                self.part_size, self.sample_per_part, self.trans_std)\n\n\nclass ModulatedDeformRoIPoolingPack(DeformRoIPooling):\n\n    def __init__(self,\n                 spatial_scale,\n                 out_size,\n                 out_channels,\n                 no_trans,\n                 group_size=1,\n                 part_size=None,\n                 sample_per_part=4,\n                 trans_std=.0,\n                 deform_fc_channels=1024):\n        super(ModulatedDeformRoIPoolingPack, self).__init__(\n            spatial_scale, out_size, out_channels, no_trans, group_size,\n            part_size, sample_per_part, trans_std)\n\n        self.deform_fc_channels = deform_fc_channels\n\n        if not no_trans:\n            self.offset_fc = nn.Sequential(\n                nn.Linear(self.out_size * self.out_size * self.out_channels,\n                          self.deform_fc_channels),\n                nn.ReLU(inplace=True),\n                nn.Linear(self.deform_fc_channels, self.deform_fc_channels),\n                nn.ReLU(inplace=True),\n                nn.Linear(self.deform_fc_channels,\n                          self.out_size * self.out_size * 2))\n            self.offset_fc[-1].weight.data.zero_()\n            self.offset_fc[-1].bias.data.zero_()\n            self.mask_fc = nn.Sequential(\n                nn.Linear(self.out_size * self.out_size * self.out_channels,\n                          self.deform_fc_channels),\n                nn.ReLU(inplace=True),\n                nn.Linear(self.deform_fc_channels,\n                          self.out_size * self.out_size * 1),\n                nn.Sigmoid())\n            self.mask_fc[2].weight.data.zero_()\n            self.mask_fc[2].bias.data.zero_()\n\n    def forward(self, data, rois):\n        assert data.size(1) == self.out_channels\n        if self.no_trans:\n            offset = data.new_empty(0)\n            return deform_roi_pooling(\n                data, rois, offset, self.spatial_scale, self.out_size,\n                self.out_channels, self.no_trans, self.group_size,\n                self.part_size, self.sample_per_part, self.trans_std)\n        else:\n            n = rois.shape[0]\n            offset = data.new_empty(0)\n            x = deform_roi_pooling(data, rois, offset, self.spatial_scale,\n                                   self.out_size, self.out_channels, True,\n                                   self.group_size, self.part_size,\n                                   self.sample_per_part, self.trans_std)\n            offset = self.offset_fc(x.view(n, -1))\n            offset = offset.view(n, 2, self.out_size, self.out_size)\n            mask = self.mask_fc(x.view(n, -1))\n            mask = mask.view(n, 1, self.out_size, self.out_size)\n            return deform_roi_pooling(\n                data, rois, offset, self.spatial_scale, self.out_size,\n                self.out_channels, self.no_trans, self.group_size,\n                self.part_size, self.sample_per_part, self.trans_std) * mask\n'"
detection/mmdet/ops/roi_align/functions/__init__.py,0,b''
detection/mmdet/ops/roi_align/functions/roi_align.py,1,"b'from torch.autograd import Function\n\nfrom .. import roi_align_cuda\n\n\nclass RoIAlignFunction(Function):\n\n    @staticmethod\n    def forward(ctx, features, rois, out_size, spatial_scale, sample_num=0):\n        if isinstance(out_size, int):\n            out_h = out_size\n            out_w = out_size\n        elif isinstance(out_size, tuple):\n            assert len(out_size) == 2\n            assert isinstance(out_size[0], int)\n            assert isinstance(out_size[1], int)\n            out_h, out_w = out_size\n        else:\n            raise TypeError(\n                \'""out_size"" must be an integer or tuple of integers\')\n        ctx.spatial_scale = spatial_scale\n        ctx.sample_num = sample_num\n        ctx.save_for_backward(rois)\n        ctx.feature_size = features.size()\n\n        batch_size, num_channels, data_height, data_width = features.size()\n        num_rois = rois.size(0)\n\n        output = features.new_zeros(num_rois, num_channels, out_h, out_w)\n        if features.is_cuda:\n            roi_align_cuda.forward(features, rois, out_h, out_w, spatial_scale,\n                                   sample_num, output)\n        else:\n            raise NotImplementedError\n\n        return output\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        feature_size = ctx.feature_size\n        spatial_scale = ctx.spatial_scale\n        sample_num = ctx.sample_num\n        rois = ctx.saved_tensors[0]\n        assert (feature_size is not None and grad_output.is_cuda)\n\n        batch_size, num_channels, data_height, data_width = feature_size\n        out_w = grad_output.size(3)\n        out_h = grad_output.size(2)\n\n        grad_input = grad_rois = None\n        if ctx.needs_input_grad[0]:\n            grad_input = rois.new_zeros(batch_size, num_channels, data_height,\n                                        data_width)\n            roi_align_cuda.backward(grad_output.contiguous(), rois, out_h,\n                                    out_w, spatial_scale, sample_num,\n                                    grad_input)\n\n        return grad_input, grad_rois, None, None, None\n\n\nroi_align = RoIAlignFunction.apply\n'"
detection/mmdet/ops/roi_align/modules/__init__.py,0,b''
detection/mmdet/ops/roi_align/modules/roi_align.py,1,"b'from torch.nn.modules.module import Module\nfrom ..functions.roi_align import RoIAlignFunction\n\n\nclass RoIAlign(Module):\n\n    def __init__(self, out_size, spatial_scale, sample_num=0):\n        super(RoIAlign, self).__init__()\n\n        self.out_size = out_size\n        self.spatial_scale = float(spatial_scale)\n        self.sample_num = int(sample_num)\n\n    def forward(self, features, rois):\n        return RoIAlignFunction.apply(features, rois, self.out_size,\n                                      self.spatial_scale, self.sample_num)\n'"
detection/mmdet/ops/roi_pool/functions/__init__.py,0,b''
detection/mmdet/ops/roi_pool/functions/roi_pool.py,2,"b'import torch\nfrom torch.autograd import Function\n\nfrom .. import roi_pool_cuda\n\n\nclass RoIPoolFunction(Function):\n\n    @staticmethod\n    def forward(ctx, features, rois, out_size, spatial_scale):\n        if isinstance(out_size, int):\n            out_h = out_size\n            out_w = out_size\n        elif isinstance(out_size, tuple):\n            assert len(out_size) == 2\n            assert isinstance(out_size[0], int)\n            assert isinstance(out_size[1], int)\n            out_h, out_w = out_size\n        else:\n            raise TypeError(\n                \'""out_size"" must be an integer or tuple of integers\')\n        assert features.is_cuda\n        ctx.save_for_backward(rois)\n        num_channels = features.size(1)\n        num_rois = rois.size(0)\n        out_size = (num_rois, num_channels, out_h, out_w)\n        output = features.new_zeros(out_size)\n        argmax = features.new_zeros(out_size, dtype=torch.int)\n        roi_pool_cuda.forward(features, rois, out_h, out_w, spatial_scale,\n                              output, argmax)\n        ctx.spatial_scale = spatial_scale\n        ctx.feature_size = features.size()\n        ctx.argmax = argmax\n\n        return output\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        assert grad_output.is_cuda\n        spatial_scale = ctx.spatial_scale\n        feature_size = ctx.feature_size\n        argmax = ctx.argmax\n        rois = ctx.saved_tensors[0]\n        assert feature_size is not None\n\n        grad_input = grad_rois = None\n        if ctx.needs_input_grad[0]:\n            grad_input = grad_output.new_zeros(feature_size)\n            roi_pool_cuda.backward(grad_output.contiguous(), rois, argmax,\n                                   spatial_scale, grad_input)\n\n        return grad_input, grad_rois, None, None\n\n\nroi_pool = RoIPoolFunction.apply\n'"
detection/mmdet/ops/roi_pool/modules/__init__.py,0,b''
detection/mmdet/ops/roi_pool/modules/roi_pool.py,1,"b'from torch.nn.modules.module import Module\nfrom ..functions.roi_pool import roi_pool\n\n\nclass RoIPool(Module):\n\n    def __init__(self, out_size, spatial_scale):\n        super(RoIPool, self).__init__()\n\n        self.out_size = out_size\n        self.spatial_scale = float(spatial_scale)\n\n    def forward(self, features, rois):\n        return roi_pool(features, rois, self.out_size, self.spatial_scale)\n'"
