file_path,api_count,code
setup.py,0,"b""from setuptools import setup, find_packages\n\ndescription = 'A PyTorch implementation of Paragraph Vectors (doc2vec).'\n\nwith open('README.md') as f:\n    long_description = f.read()\n\nwith open('requirements.txt') as f:\n    requires = f.read().splitlines()\n\nsetup(\n    name='paragraph-vectors',\n    version='0.0.1',\n    author='Nejc Ilenic',\n    description=description,\n    long_description=long_description,\n    license='MIT',\n    keywords='nlp documents embedding machine-learning',\n    install_requires=requires,\n    packages=find_packages(),\n    test_suite='tests',\n    classifiers=[\n        'Topic :: Scientific/Engineering :: Artificial Intelligence',\n        'License :: OSI Approved :: MIT License',\n        'Natural Language :: English',\n        'Operating System :: OS Independent',\n        'Programming Language :: Python :: 3.5',\n    ],\n)\n"""
paragraphvec/__init__.py,0,b''
paragraphvec/data.py,3,"b'import multiprocessing\nimport os\nimport re\nimport signal\nfrom math import ceil\nfrom os.path import join\n\nimport numpy as np\nimport torch\nfrom numpy.random import choice\nfrom torchtext.data import Field, TabularDataset\n\nfrom paragraphvec.utils import DATA_DIR\n\n\ndef load_dataset(file_name):\n    """"""Loads contents from a file in the *data* directory into a\n    torchtext.data.TabularDataset instance.\n    """"""\n    file_path = join(DATA_DIR, file_name)\n    text_field = Field(pad_token=None, tokenize=_tokenize_str)\n\n    dataset = TabularDataset(\n        path=file_path,\n        format=\'csv\',\n        fields=[(\'text\', text_field)],\n        skip_header=True)\n\n    text_field.build_vocab(dataset)\n    return dataset\n\n\ndef _tokenize_str(str_):\n    # keep only alphanumeric and punctations\n    str_ = re.sub(r\'[^A-Za-z0-9(),.!?\\\'`]\', \' \', str_)\n    # remove multiple whitespace characters\n    str_ = re.sub(r\'\\s{2,}\', \' \', str_)\n    # punctations to tokens\n    str_ = re.sub(r\'\\(\', \' ( \', str_)\n    str_ = re.sub(r\'\\)\', \' ) \', str_)\n    str_ = re.sub(r\',\', \' , \', str_)\n    str_ = re.sub(r\'\\.\', \' . \', str_)\n    str_ = re.sub(r\'!\', \' ! \', str_)\n    str_ = re.sub(r\'\\?\', \' ? \', str_)\n    # split contractions into multiple tokens\n    str_ = re.sub(r\'\\\'s\', \' \\\'s\', str_)\n    str_ = re.sub(r\'\\\'ve\', \' \\\'ve\', str_)\n    str_ = re.sub(r\'n\\\'t\', \' n\\\'t\', str_)\n    str_ = re.sub(r\'\\\'re\', \' \\\'re\', str_)\n    str_ = re.sub(r\'\\\'d\', \' \\\'d\', str_)\n    str_ = re.sub(r\'\\\'ll\', \' \\\'ll\', str_)\n    # lower case\n    return str_.strip().lower().split()\n\n\nclass NCEData(object):\n    """"""An infinite, parallel (multiprocess) batch generator for\n    noise-contrastive estimation of word vector models.\n\n    Parameters\n    ----------\n    dataset: torchtext.data.TabularDataset\n        Dataset from which examples are generated. A column labeled *text*\n        is expected and should be comprised of a list of tokens. Each row\n        should represent a single document.\n\n    batch_size: int\n        Number of examples per single gradient update.\n\n    context_size: int\n        Half the size of a neighbourhood of target words (i.e. how many\n        words left and right are regarded as context).\n\n    num_noise_words: int\n        Number of noise words to sample from the noise distribution.\n\n    max_size: int\n        Maximum number of pre-generated batches.\n\n    num_workers: int\n        Number of jobs to run in parallel. If value is set to -1, total number\n        of machine CPUs is used.\n    """"""\n    # code inspired by parallel generators in https://github.com/fchollet/keras\n    def __init__(self, dataset, batch_size, context_size,\n                 num_noise_words, max_size, num_workers):\n        self.max_size = max_size\n\n        self.num_workers = num_workers if num_workers != -1 else os.cpu_count()\n        if self.num_workers is None:\n            self.num_workers = 1\n\n        self._generator = _NCEGenerator(\n            dataset,\n            batch_size,\n            context_size,\n            num_noise_words,\n            _NCEGeneratorState(context_size))\n\n        self._queue = None\n        self._stop_event = None\n        self._processes = []\n\n    def __len__(self):\n        return len(self._generator)\n\n    def vocabulary_size(self):\n        return self._generator.vocabulary_size()\n\n    def start(self):\n        """"""Starts num_worker processes that generate batches of data.""""""\n        self._queue = multiprocessing.Queue(maxsize=self.max_size)\n        self._stop_event = multiprocessing.Event()\n\n        for _ in range(self.num_workers):\n            process = multiprocessing.Process(target=self._parallel_task)\n            process.daemon = True\n            self._processes.append(process)\n            process.start()\n\n    def _parallel_task(self):\n        while not self._stop_event.is_set():\n            try:\n                batch = self._generator.next()\n                # queue blocks a call to put() until a free slot is available\n                self._queue.put(batch)\n            except KeyboardInterrupt:\n                self._stop_event.set()\n\n    def get_generator(self):\n        """"""Returns a generator that yields batches of data.""""""\n        while self._is_running():\n            yield self._queue.get()\n\n    def stop(self):\n        """"""Terminates all processes that were created with start().""""""\n        if self._is_running():\n            self._stop_event.set()\n\n        for process in self._processes:\n            if process.is_alive():\n                os.kill(process.pid, signal.SIGINT)\n                process.join()\n\n        if self._queue is not None:\n            self._queue.close()\n\n        self._queue = None\n        self._stop_event = None\n        self._processes = []\n\n    def _is_running(self):\n        return self._stop_event is not None and not self._stop_event.is_set()\n\n\nclass _NCEGenerator(object):\n    """"""An infinite, process-safe batch generator for noise-contrastive\n    estimation of word vector models.\n\n    Parameters\n    ----------\n    state: paragraphvec.data._NCEGeneratorState\n        Initial (indexing) state of the generator.\n\n    For other parameters see the NCEData class.\n    """"""\n    def __init__(self, dataset, batch_size, context_size,\n                 num_noise_words, state):\n        self.dataset = dataset\n        self.batch_size = batch_size\n        self.context_size = context_size\n        self.num_noise_words = num_noise_words\n\n        self._vocabulary = self.dataset.fields[\'text\'].vocab\n        self._sample_noise = None\n        self._init_noise_distribution()\n        self._state = state\n\n    def _init_noise_distribution(self):\n        # we use a unigram distribution raised to the 3/4rd power,\n        # as proposed by T. Mikolov et al. in Distributed Representations\n        # of Words and Phrases and their Compositionality\n        probs = np.zeros(len(self._vocabulary) - 1)\n\n        for word, freq in self._vocabulary.freqs.items():\n            probs[self._word_to_index(word)] = freq\n\n        probs = np.power(probs, 0.75)\n        probs /= np.sum(probs)\n\n        self._sample_noise = lambda: choice(\n            probs.shape[0], self.num_noise_words, p=probs).tolist()\n\n    def __len__(self):\n        num_examples = sum(self._num_examples_in_doc(d) for d in self.dataset)\n        return ceil(num_examples / self.batch_size)\n\n    def vocabulary_size(self):\n        return len(self._vocabulary) - 1\n\n    def next(self):\n        """"""Updates state for the next process in a process-safe manner\n        and generates the current batch.""""""\n        prev_doc_id, prev_in_doc_pos = self._state.update_state(\n            self.dataset,\n            self.batch_size,\n            self.context_size,\n            self._num_examples_in_doc)\n\n        # generate the actual batch\n        batch = _NCEBatch(self.context_size)\n\n        while len(batch) < self.batch_size:\n            if prev_doc_id == len(self.dataset):\n                # last document exhausted\n                batch.torch_()\n                return batch\n            if prev_in_doc_pos <= (len(self.dataset[prev_doc_id].text) - 1\n                                   - self.context_size):\n                # more examples in the current document\n                self._add_example_to_batch(prev_doc_id, prev_in_doc_pos, batch)\n                prev_in_doc_pos += 1\n            else:\n                # go to the next document\n                prev_doc_id += 1\n                prev_in_doc_pos = self.context_size\n\n        batch.torch_()\n        return batch\n\n    def _num_examples_in_doc(self, doc, in_doc_pos=None):\n        if in_doc_pos is not None:\n            # number of remaining\n            if len(doc.text) - in_doc_pos >= self.context_size + 1:\n                return len(doc.text) - in_doc_pos - self.context_size\n            return 0\n\n        if len(doc.text) >= 2 * self.context_size + 1:\n            # total number\n            return len(doc.text) - 2 * self.context_size\n        return 0\n\n    def _add_example_to_batch(self, doc_id, in_doc_pos, batch):\n        doc = self.dataset[doc_id].text\n        batch.doc_ids.append(doc_id)\n\n        # sample from the noise distribution\n        current_noise = self._sample_noise()\n        current_noise.insert(0, self._word_to_index(doc[in_doc_pos]))\n        batch.target_noise_ids.append(current_noise)\n\n        if self.context_size == 0:\n            return\n\n        current_context = []\n        context_indices = (in_doc_pos + diff for diff in\n                           range(-self.context_size, self.context_size + 1)\n                           if diff != 0)\n\n        for i in context_indices:\n            context_id = self._word_to_index(doc[i])\n            current_context.append(context_id)\n        batch.context_ids.append(current_context)\n\n    def _word_to_index(self, word):\n        return self._vocabulary.stoi[word] - 1\n\n\nclass _NCEGeneratorState(object):\n    """"""Batch generator state that is represented with a document id and\n    in-document position. It abstracts a process-safe indexing mechanism.""""""\n    def __init__(self, context_size):\n        # use raw values because both indices have\n        # to manually be locked together\n        self._doc_id = multiprocessing.RawValue(\'i\', 0)\n        self._in_doc_pos = multiprocessing.RawValue(\'i\', context_size)\n        self._lock = multiprocessing.Lock()\n\n    def update_state(self, dataset, batch_size,\n                     context_size, num_examples_in_doc):\n        """"""Returns current indices and computes new indices for the\n        next process.""""""\n        with self._lock:\n            doc_id = self._doc_id.value\n            in_doc_pos = self._in_doc_pos.value\n            self._advance_indices(\n                dataset, batch_size, context_size, num_examples_in_doc)\n            return doc_id, in_doc_pos\n\n    def _advance_indices(self, dataset, batch_size,\n                         context_size, num_examples_in_doc):\n        num_examples = num_examples_in_doc(\n            dataset[self._doc_id.value], self._in_doc_pos.value)\n\n        if num_examples > batch_size:\n            # more examples in the current document\n            self._in_doc_pos.value += batch_size\n            return\n\n        if num_examples == batch_size:\n            # just enough examples in the current document\n            if self._doc_id.value < len(dataset) - 1:\n                self._doc_id.value += 1\n            else:\n                self._doc_id.value = 0\n            self._in_doc_pos.value = context_size\n            return\n\n        while num_examples < batch_size:\n            if self._doc_id.value == len(dataset) - 1:\n                # last document: reset indices\n                self._doc_id.value = 0\n                self._in_doc_pos.value = context_size\n                return\n\n            self._doc_id.value += 1\n            num_examples += num_examples_in_doc(\n                dataset[self._doc_id.value])\n\n        self._in_doc_pos.value = (len(dataset[self._doc_id.value].text)\n                                  - context_size\n                                  - (num_examples - batch_size))\n\n\nclass _NCEBatch(object):\n    def __init__(self, context_size):\n        self.context_ids = [] if context_size > 0 else None\n        self.doc_ids = []\n        self.target_noise_ids = []\n\n    def __len__(self):\n        return len(self.doc_ids)\n\n    def torch_(self):\n        if self.context_ids is not None:\n            self.context_ids = torch.LongTensor(self.context_ids)\n        self.doc_ids = torch.LongTensor(self.doc_ids)\n        self.target_noise_ids = torch.LongTensor(self.target_noise_ids)\n\n    def cuda_(self):\n        if self.context_ids is not None:\n            self.context_ids = self.context_ids.cuda()\n        self.doc_ids = self.doc_ids.cuda()\n        self.target_noise_ids = self.target_noise_ids.cuda()\n'"
paragraphvec/export_vectors.py,2,"b'import csv\nimport re\nfrom os.path import join\n\nimport fire\nimport torch\n\nfrom paragraphvec.data import load_dataset\nfrom paragraphvec.models import DM, DBOW\nfrom paragraphvec.utils import DATA_DIR, MODELS_DIR\n\n\ndef start(data_file_name, model_file_name):\n    """"""Saves trained paragraph vectors to a csv file in the *data* directory.\n\n    Parameters\n    ----------\n    data_file_name: str\n        Name of a file in the *data* directory that was used during training.\n\n    model_file_name: str\n        Name of a file in the *models* directory (a model trained on\n        the *data_file_name* dataset).\n    """"""\n    dataset = load_dataset(data_file_name)\n\n    vec_dim = int(re.search(\'_vecdim\\.(\\d+)_\', model_file_name).group(1))\n\n    model = _load_model(\n        model_file_name,\n        vec_dim,\n        num_docs=len(dataset),\n        num_words=len(dataset.fields[\'text\'].vocab) - 1)\n\n    _write_to_file(data_file_name, model_file_name, model, vec_dim)\n\n\ndef _load_model(model_file_name, vec_dim, num_docs, num_words):\n    model_ver = re.search(\'_model\\.(dm|dbow)\', model_file_name).group(1)\n    if model_ver is None:\n        raise ValueError(""Model file name contains an invalid""\n                         ""version of the model"")\n\n    model_file_path = join(MODELS_DIR, model_file_name)\n\n    try:\n        checkpoint = torch.load(model_file_path)\n    except AssertionError:\n        checkpoint = torch.load(\n            model_file_path,\n            map_location=lambda storage, location: storage)\n\n    if model_ver == \'dbow\':\n        model = DBOW(vec_dim, num_docs, num_words)\n    else:\n        model = DM(vec_dim, num_docs, num_words)\n\n    model.load_state_dict(checkpoint[\'model_state_dict\'])\n    return model\n\n\ndef _write_to_file(data_file_name, model_file_name, model, vec_dim):\n    result_lines = []\n\n    with open(join(DATA_DIR, data_file_name)) as f:\n        reader = csv.reader(f)\n\n        for i, line in enumerate(reader):\n            # skip text\n            result_line = line[1:]\n            if i == 0:\n                # header line\n                result_line += [""d{:d}"".format(x) for x in range(vec_dim)]\n            else:\n                vector = model.get_paragraph_vector(i - 1)\n                result_line += [str(x) for x in vector]\n\n            result_lines.append(result_line)\n\n    result_file_name = model_file_name[:-7] + \'csv\'\n\n    with open(join(DATA_DIR, result_file_name), \'w\') as f:\n        writer = csv.writer(f)\n        writer.writerows(result_lines)\n\n\nif __name__ == \'__main__\':\n    fire.Fire()\n'"
paragraphvec/loss.py,3,"b'import torch\nimport torch.nn as nn\n\n\nclass NegativeSampling(nn.Module):\n    """"""Negative sampling loss as proposed by T. Mikolov et al. in Distributed\n    Representations of Words and Phrases and their Compositionality.\n    """"""\n    def __init__(self):\n        super(NegativeSampling, self).__init__()\n        self._log_sigmoid = nn.LogSigmoid()\n\n    def forward(self, scores):\n        """"""Computes the value of the loss function.\n\n        Parameters\n        ----------\n        scores: autograd.Variable of size (batch_size, num_noise_words + 1)\n            Sparse unnormalized log probabilities. The first element in each\n            row is the ground truth score (i.e. the target), other elements\n            are scores of samples from the noise distribution.\n        """"""\n        k = scores.size()[1] - 1\n        return -torch.sum(\n            self._log_sigmoid(scores[:, 0])\n            + torch.sum(self._log_sigmoid(-scores[:, 1:]), dim=1) / k\n        ) / scores.size()[0]\n'"
paragraphvec/models.py,15,"b'import torch\nimport torch.nn as nn\n\n\nclass DM(nn.Module):\n    """"""Distributed Memory version of Paragraph Vectors.\n\n    Parameters\n    ----------\n    vec_dim: int\n        Dimensionality of vectors to be learned (for paragraphs and words).\n\n    num_docs: int\n        Number of documents in a dataset.\n\n    num_words: int\n        Number of distinct words in a daset (i.e. vocabulary size).\n    """"""\n    def __init__(self, vec_dim, num_docs, num_words):\n        super(DM, self).__init__()\n        # paragraph matrix\n        self._D = nn.Parameter(\n            torch.randn(num_docs, vec_dim), requires_grad=True)\n        # word matrix\n        self._W = nn.Parameter(\n            torch.randn(num_words, vec_dim), requires_grad=True)\n        # output layer parameters\n        self._O = nn.Parameter(\n            torch.FloatTensor(vec_dim, num_words).zero_(), requires_grad=True)\n\n    def forward(self, context_ids, doc_ids, target_noise_ids):\n        """"""Sparse computation of scores (unnormalized log probabilities)\n        that should be passed to the negative sampling loss.\n\n        Parameters\n        ----------\n        context_ids: torch.Tensor of size (batch_size, num_context_words)\n            Vocabulary indices of context words.\n\n        doc_ids: torch.Tensor of size (batch_size,)\n            Document indices of paragraphs.\n\n        target_noise_ids: torch.Tensor of size (batch_size, num_noise_words + 1)\n            Vocabulary indices of target and noise words. The first element in\n            each row is the ground truth index (i.e. the target), other\n            elements are indices of samples from the noise distribution.\n\n        Returns\n        -------\n            autograd.Variable of size (batch_size, num_noise_words + 1)\n        """"""\n        # combine a paragraph vector with word vectors of\n        # input (context) words\n        x = torch.add(\n            self._D[doc_ids, :], torch.sum(self._W[context_ids, :], dim=1))\n\n        # sparse computation of scores (unnormalized log probabilities)\n        # for negative sampling\n        return torch.bmm(\n            x.unsqueeze(1),\n            self._O[:, target_noise_ids].permute(1, 0, 2)).squeeze()\n\n    def get_paragraph_vector(self, index):\n        return self._D[index, :].data.tolist()\n\n\nclass DBOW(nn.Module):\n    """"""Distributed Bag of Words version of Paragraph Vectors.\n\n    Parameters\n    ----------\n    vec_dim: int\n        Dimensionality of vectors to be learned (for paragraphs and words).\n\n    num_docs: int\n        Number of documents in a dataset.\n\n    num_words: int\n        Number of distinct words in a daset (i.e. vocabulary size).\n    """"""\n    def __init__(self, vec_dim, num_docs, num_words):\n        super(DBOW, self).__init__()\n        # paragraph matrix\n        self._D = nn.Parameter(\n            torch.randn(num_docs, vec_dim), requires_grad=True)\n        # output layer parameters\n        self._O = nn.Parameter(\n            torch.FloatTensor(vec_dim, num_words).zero_(), requires_grad=True)\n\n    def forward(self, doc_ids, target_noise_ids):\n        """"""Sparse computation of scores (unnormalized log probabilities)\n        that should be passed to the negative sampling loss.\n\n        Parameters\n        ----------\n        doc_ids: torch.Tensor of size (batch_size,)\n            Document indices of paragraphs.\n\n        target_noise_ids: torch.Tensor of size (batch_size, num_noise_words + 1)\n            Vocabulary indices of target and noise words. The first element in\n            each row is the ground truth index (i.e. the target), other\n            elements are indices of samples from the noise distribution.\n\n        Returns\n        -------\n            autograd.Variable of size (batch_size, num_noise_words + 1)\n        """"""\n        # sparse computation of scores (unnormalized log probabilities)\n        # for negative sampling\n        return torch.bmm(\n            self._D[doc_ids, :].unsqueeze(1),\n            self._O[:, target_noise_ids].permute(1, 0, 2)).squeeze()\n\n    def get_paragraph_vector(self, index):\n        return self._D[index, :].data.tolist()\n'"
paragraphvec/train.py,4,"b'import time\nfrom sys import float_info, stdout\n\nimport fire\nimport torch\nfrom torch.optim import Adam\n\nfrom paragraphvec.data import load_dataset, NCEData\nfrom paragraphvec.loss import NegativeSampling\nfrom paragraphvec.models import DM, DBOW\nfrom paragraphvec.utils import save_training_state\n\n\ndef start(data_file_name,\n          num_noise_words,\n          vec_dim,\n          num_epochs,\n          batch_size,\n          lr,\n          model_ver=\'dbow\',\n          context_size=0,\n          vec_combine_method=\'sum\',\n          save_all=False,\n          generate_plot=True,\n          max_generated_batches=5,\n          num_workers=1):\n    """"""Trains a new model. The latest checkpoint and the best performing\n    model are saved in the *models* directory.\n\n    Parameters\n    ----------\n    data_file_name: str\n        Name of a file in the *data* directory.\n\n    model_ver: str, one of (\'dm\', \'dbow\'), default=\'dbow\'\n        Version of the model as proposed by Q. V. Le et al., Distributed\n        Representations of Sentences and Documents. \'dbow\' stands for\n        Distributed Bag Of Words, \'dm\' stands for Distributed Memory.\n\n    vec_combine_method: str, one of (\'sum\', \'concat\'), default=\'sum\'\n        Method for combining paragraph and word vectors when model_ver=\'dm\'.\n        Currently only the \'sum\' operation is implemented.\n\n    context_size: int, default=0\n        Half the size of a neighbourhood of target words when model_ver=\'dm\'\n        (i.e. how many words left and right are regarded as context). When\n        model_ver=\'dm\' context_size has to greater than 0, when\n        model_ver=\'dbow\' context_size has to be 0.\n\n    num_noise_words: int\n        Number of noise words to sample from the noise distribution.\n\n    vec_dim: int\n        Dimensionality of vectors to be learned (for paragraphs and words).\n\n    num_epochs: int\n        Number of iterations to train the model (i.e. number\n        of times every example is seen during training).\n\n    batch_size: int\n        Number of examples per single gradient update.\n\n    lr: float\n        Learning rate of the Adam optimizer.\n\n    save_all: bool, default=False\n        Indicates whether a checkpoint is saved after each epoch.\n        If false, only the best performing model is saved.\n\n    generate_plot: bool, default=True\n        Indicates whether a diagnostic plot displaying loss value over\n        epochs is generated after each epoch.\n\n    max_generated_batches: int, default=5\n        Maximum number of pre-generated batches.\n\n    num_workers: int, default=1\n        Number of batch generator jobs to run in parallel. If value is set\n        to -1 number of machine cores are used.\n    """"""\n    if model_ver not in (\'dm\', \'dbow\'):\n        raise ValueError(""Invalid version of the model"")\n\n    model_ver_is_dbow = model_ver == \'dbow\'\n\n    if model_ver_is_dbow and context_size != 0:\n        raise ValueError(""Context size has to be zero when using dbow"")\n    if not model_ver_is_dbow:\n        if vec_combine_method not in (\'sum\', \'concat\'):\n            raise ValueError(""Invalid method for combining paragraph and word ""\n                             ""vectors when using dm"")\n        if context_size <= 0:\n            raise ValueError(""Context size must be positive when using dm"")\n\n    dataset = load_dataset(data_file_name)\n    nce_data = NCEData(\n        dataset,\n        batch_size,\n        context_size,\n        num_noise_words,\n        max_generated_batches,\n        num_workers)\n    nce_data.start()\n\n    try:\n        _run(data_file_name, dataset, nce_data.get_generator(), len(nce_data),\n             nce_data.vocabulary_size(), context_size, num_noise_words, vec_dim,\n             num_epochs, batch_size, lr, model_ver, vec_combine_method,\n             save_all, generate_plot, model_ver_is_dbow)\n    except KeyboardInterrupt:\n        nce_data.stop()\n\n\ndef _run(data_file_name,\n         dataset,\n         data_generator,\n         num_batches,\n         vocabulary_size,\n         context_size,\n         num_noise_words,\n         vec_dim,\n         num_epochs,\n         batch_size,\n         lr,\n         model_ver,\n         vec_combine_method,\n         save_all,\n         generate_plot,\n         model_ver_is_dbow):\n\n    if model_ver_is_dbow:\n        model = DBOW(vec_dim, num_docs=len(dataset), num_words=vocabulary_size)\n    else:\n        model = DM(vec_dim, num_docs=len(dataset), num_words=vocabulary_size)\n\n    cost_func = NegativeSampling()\n    optimizer = Adam(params=model.parameters(), lr=lr)\n\n    if torch.cuda.is_available():\n        model.cuda()\n\n    print(""Dataset comprised of {:d} documents."".format(len(dataset)))\n    print(""Vocabulary size is {:d}.\\n"".format(vocabulary_size))\n    print(""Training started."")\n\n    best_loss = float(""inf"")\n    prev_model_file_path = None\n\n    for epoch_i in range(num_epochs):\n        epoch_start_time = time.time()\n        loss = []\n\n        for batch_i in range(num_batches):\n            batch = next(data_generator)\n            if torch.cuda.is_available():\n                batch.cuda_()\n\n            if model_ver_is_dbow:\n                x = model.forward(batch.doc_ids, batch.target_noise_ids)\n            else:\n                x = model.forward(\n                    batch.context_ids,\n                    batch.doc_ids,\n                    batch.target_noise_ids)\n\n            x = cost_func.forward(x)\n\n            loss.append(x.item())\n            model.zero_grad()\n            x.backward()\n            optimizer.step()\n            _print_progress(epoch_i, batch_i, num_batches)\n\n        # end of epoch\n        loss = torch.mean(torch.FloatTensor(loss))\n        is_best_loss = loss < best_loss\n        best_loss = min(loss, best_loss)\n\n        state = {\n            \'epoch\': epoch_i + 1,\n            \'model_state_dict\': model.state_dict(),\n            \'best_loss\': best_loss,\n            \'optimizer_state_dict\': optimizer.state_dict()\n        }\n\n        prev_model_file_path = save_training_state(\n            data_file_name,\n            model_ver,\n            vec_combine_method,\n            context_size,\n            num_noise_words,\n            vec_dim,\n            batch_size,\n            lr,\n            epoch_i,\n            loss,\n            state,\n            save_all,\n            generate_plot,\n            is_best_loss,\n            prev_model_file_path,\n            model_ver_is_dbow)\n\n        epoch_total_time = round(time.time() - epoch_start_time)\n        print("" ({:d}s) - loss: {:.4f}"".format(epoch_total_time, loss))\n\n\ndef _print_progress(epoch_i, batch_i, num_batches):\n    progress = round((batch_i + 1) / num_batches * 100)\n    print(""\\rEpoch {:d}"".format(epoch_i + 1), end=\'\')\n    stdout.write("" - {:d}%"".format(progress))\n    stdout.flush()\n\n\nif __name__ == \'__main__\':\n    fire.Fire()\n'"
paragraphvec/utils.py,2,"b'from os import remove\nfrom os.path import join, dirname, isfile\n\nimport matplotlib.pyplot as plt\nimport torch\n\n_root_dir = dirname(dirname(__file__))\n\nDATA_DIR = join(_root_dir, \'data\')\nMODELS_DIR = join(_root_dir, \'models\')\n_DIAGNOSTICS_DIR = join(_root_dir, \'diagnostics\')\n\n_DM_MODEL_NAME = (""{:s}_model.{:s}.{:s}_contextsize.{:d}_numnoisewords.{:d}""\n                  ""_vecdim.{:d}_batchsize.{:d}_lr.{:f}_epoch.{:d}_loss.{:f}""\n                  "".pth.tar"")\n_DM_DIAGNOSTIC_FILE_NAME = (""{:s}_model.{:s}.{:s}_contextsize.{:d}""\n                            ""_numnoisewords.{:d}_vecdim.{:d}_batchsize.{:d}""\n                            ""_lr.{:f}.csv"")\n_DBOW_MODEL_NAME = (""{:s}_model.{:s}_numnoisewords.{:d}_vecdim.{:d}""\n                    ""_batchsize.{:d}_lr.{:f}_epoch.{:d}_loss.{:f}.pth.tar"")\n_DBOW_DIAGNOSTIC_FILE_NAME = (""{:s}_model.{:s}_numnoisewords.{:d}_vecdim.{:d}""\n                              ""_batchsize.{:d}_lr.{:f}.csv"")\n\n\ndef save_training_state(data_file_name,\n                        model_ver,\n                        vec_combine_method,\n                        context_size,\n                        num_noise_words,\n                        vec_dim,\n                        batch_size,\n                        lr,\n                        epoch_i,\n                        loss,\n                        model_state,\n                        save_all,\n                        generate_plot,\n                        is_best_loss,\n                        prev_model_file_path,\n                        model_ver_is_dbow):\n    """"""Saves the state of the model. If generate_plot is True, it also\n    saves current epoch\'s loss value and generates a plot of all loss\n    values up to this epoch.\n\n    Returns\n    -------\n        str representing a model file path from the previous epoch\n    """"""\n    if generate_plot:\n        # save the loss value for a diagnostic plot\n        if model_ver_is_dbow:\n            diagnostic_file_name = _DBOW_DIAGNOSTIC_FILE_NAME.format(\n                data_file_name[:-4],\n                model_ver,\n                num_noise_words,\n                vec_dim,\n                batch_size,\n                lr)\n        else:\n            diagnostic_file_name = _DM_DIAGNOSTIC_FILE_NAME.format(\n                data_file_name[:-4],\n                model_ver,\n                vec_combine_method,\n                context_size,\n                num_noise_words,\n                vec_dim,\n                batch_size,\n                lr)\n\n        diagnostic_file_path = join(_DIAGNOSTICS_DIR, diagnostic_file_name)\n\n        if epoch_i == 0 and isfile(diagnostic_file_path):\n            remove(diagnostic_file_path)\n\n        with open(diagnostic_file_path, \'a\') as f:\n            f.write(\'{:f}\\n\'.format(loss))\n\n        # generate a diagnostic loss plot\n        with open(diagnostic_file_path) as f:\n            loss_values = [float(l.rstrip()) for l in f.readlines()]\n\n        diagnostic_plot_file_path = diagnostic_file_path[:-3] + \'png\'\n        fig = plt.figure()\n        plt.plot(range(1, epoch_i + 2), loss_values, color=\'r\')\n        plt.xlabel(\'epoch\')\n        plt.ylabel(\'training loss\')\n        fig.savefig(diagnostic_plot_file_path, bbox_inches=\'tight\')\n        plt.close()\n\n    # save the model\n    if model_ver_is_dbow:\n        model_file_name = _DBOW_MODEL_NAME.format(\n            data_file_name[:-4],\n            model_ver,\n            num_noise_words,\n            vec_dim,\n            batch_size,\n            lr,\n            epoch_i + 1,\n            loss)\n    else:\n        model_file_name = _DM_MODEL_NAME.format(\n            data_file_name[:-4],\n            model_ver,\n            vec_combine_method,\n            context_size,\n            num_noise_words,\n            vec_dim,\n            batch_size,\n            lr,\n            epoch_i + 1,\n            loss)\n\n    model_file_path = join(MODELS_DIR, model_file_name)\n\n    if save_all:\n        torch.save(model_state, model_file_path)\n        return None\n    elif is_best_loss:\n        if prev_model_file_path is not None:\n            remove(prev_model_file_path)\n\n        torch.save(model_state, model_file_path)\n        return model_file_path\n    else:\n        return prev_model_file_path\n'"
tests/__init__.py,0,b''
tests/test_data.py,0,"b""import time\nfrom unittest import TestCase\n\nfrom paragraphvec.data import load_dataset, NCEData\n\n\nclass NCEDataTest(TestCase):\n\n    def setUp(self):\n        self.dataset = load_dataset('example.csv')\n\n    def test_num_examples_for_different_batch_sizes(self):\n        len_1 = self._num_examples_with_batch_size(1)\n\n        for batch_size in range(2, 100):\n            len_x = self._num_examples_with_batch_size(batch_size)\n            self.assertEqual(len_x, len_1)\n\n    def _num_examples_with_batch_size(self, batch_size):\n        nce_data = NCEData(\n            self.dataset,\n            batch_size=batch_size,\n            context_size=2,\n            num_noise_words=3,\n            max_size=1,\n            num_workers=1)\n        num_batches = len(nce_data)\n        nce_data.start()\n        nce_generator = nce_data.get_generator()\n\n        total = 0\n        for _ in range(num_batches):\n            batch = next(nce_generator)\n            total += len(batch)\n        nce_data.stop()\n        return total\n\n    def test_multiple_iterations(self):\n        nce_data = NCEData(\n            self.dataset,\n            batch_size=16,\n            context_size=3,\n            num_noise_words=3,\n            max_size=1,\n            num_workers=1)\n        num_batches = len(nce_data)\n        nce_data.start()\n        nce_generator = nce_data.get_generator()\n\n        iter0_targets = []\n        for _ in range(num_batches):\n            batch = next(nce_generator)\n            iter0_targets.append([x[0] for x in batch.target_noise_ids])\n\n        iter1_targets = []\n        for _ in range(num_batches):\n            batch = next(nce_generator)\n            iter1_targets.append([x[0] for x in batch.target_noise_ids])\n\n        for ts0, ts1 in zip(iter0_targets, iter1_targets):\n            for t0, t1 in zip(ts0, ts0):\n                self.assertEqual(t0, t1)\n        nce_data.stop()\n\n    def test_different_batch_sizes(self):\n        nce_data = NCEData(\n            self.dataset,\n            batch_size=16,\n            context_size=1,\n            num_noise_words=3,\n            max_size=1,\n            num_workers=1)\n        num_batches = len(nce_data)\n        nce_data.start()\n        nce_generator = nce_data.get_generator()\n\n        targets0 = []\n        for _ in range(num_batches):\n            batch = next(nce_generator)\n            for ts in batch.target_noise_ids:\n                targets0.append(ts[0])\n        nce_data.stop()\n\n        nce_data = NCEData(\n            self.dataset,\n            batch_size=19,\n            context_size=1,\n            num_noise_words=3,\n            max_size=1,\n            num_workers=1)\n        num_batches = len(nce_data)\n        nce_data.start()\n        nce_generator = nce_data.get_generator()\n\n        targets1 = []\n        for _ in range(num_batches):\n            batch = next(nce_generator)\n            for ts in batch.target_noise_ids:\n                targets1.append(ts[0])\n        nce_data.stop()\n\n        for t0, t1 in zip(targets0, targets1):\n            self.assertEqual(t0, t1)\n\n    def test_tensor_sizes(self):\n        nce_data = NCEData(\n            self.dataset,\n            batch_size=32,\n            context_size=5,\n            num_noise_words=3,\n            max_size=1,\n            num_workers=1)\n        nce_data.start()\n        nce_generator = nce_data.get_generator()\n        batch = next(nce_generator)\n        nce_data.stop()\n\n        self.assertEqual(batch.context_ids.size()[0], 32)\n        self.assertEqual(batch.context_ids.size()[1], 10)\n        self.assertEqual(batch.doc_ids.size()[0], 32)\n        self.assertEqual(batch.target_noise_ids.size()[0], 32)\n        self.assertEqual(batch.target_noise_ids.size()[1], 4)\n\n    def test_parallel(self):\n        # serial version has max_size=3, because in the parallel version two\n        # processes advance the state before they are blocked by the queue.put()\n        nce_data = NCEData(\n            self.dataset,\n            batch_size=32,\n            context_size=5,\n            num_noise_words=1,\n            max_size=3,\n            num_workers=1)\n        nce_data.start()\n        time.sleep(1)\n        nce_data.stop()\n        state_serial = nce_data._generator._state\n\n        nce_data = NCEData(\n            self.dataset,\n            batch_size=32,\n            context_size=5,\n            num_noise_words=1,\n            max_size=2,\n            num_workers=2)\n        nce_data.start()\n        time.sleep(1)\n        nce_data.stop()\n        state_parallel = nce_data._generator._state\n\n        self.assertEqual(\n            state_parallel._doc_id.value,\n            state_serial._doc_id.value)\n        self.assertEqual(\n            state_parallel._in_doc_pos.value,\n            state_serial._in_doc_pos.value)\n\n    def test_no_context(self):\n        nce_data = NCEData(\n            self.dataset,\n            batch_size=16,\n            context_size=0,\n            num_noise_words=3,\n            max_size=1,\n            num_workers=1)\n        nce_data.start()\n        nce_generator = nce_data.get_generator()\n        batch = next(nce_generator)\n        nce_data.stop()\n\n        self.assertEqual(batch.context_ids, None)\n\n\nclass DataUtilsTest(TestCase):\n\n    def setUp(self):\n        self.dataset = load_dataset('example.csv')\n\n    def test_load_dataset(self):\n        self.assertEqual(len(self.dataset), 4)\n\n    def test_vocab(self):\n        self.assertTrue(self.dataset.fields['text'].use_vocab)\n        self.assertTrue(len(self.dataset.fields['text'].vocab) > 0)\n"""
tests/test_loss.py,1,"b'from unittest import TestCase\n\nimport torch\n\nfrom paragraphvec.loss import NegativeSampling\n\n\nclass NegativeSamplingTest(TestCase):\n\n    def setUp(self):\n        self.loss_f = NegativeSampling()\n\n    def test_forward(self):\n        # todo: test actual value\n        scores = torch.FloatTensor([[12.1, 1.3, 6.5], [18.9, 2.1, 9.4]])\n        loss = self.loss_f.forward(scores)\n        self.assertTrue(loss.data[0] >= 0)\n'"
tests/test_models.py,19,"b'from unittest import TestCase\n\nimport torch\n\nfrom paragraphvec.loss import NegativeSampling\nfrom paragraphvec.models import DM, DBOW\n\n\nclass DMTest(TestCase):\n\n    def setUp(self):\n        self.batch_size = 2\n        self.num_noise_words = 2\n        self.num_docs = 3\n        self.num_words = 15\n        self.vec_dim = 10\n\n        self.context_ids = torch.LongTensor([[0, 2, 5, 6], [3, 4, 1, 6]])\n        self.doc_ids = torch.LongTensor([1, 2])\n        self.target_noise_ids = torch.LongTensor([[1, 3, 4], [2, 4, 7]])\n        self.model = DM(\n            self.vec_dim, self.num_docs, self.num_words)\n\n    def test_num_parameters(self):\n        self.assertEqual(\n            sum([x.size()[0] * x.size()[1] for x in self.model.parameters()]),\n            self.num_docs * self.vec_dim + 2 * self.num_words * self.vec_dim)\n\n    def test_forward(self):\n        x = self.model.forward(\n            self.context_ids, self.doc_ids, self.target_noise_ids)\n\n        self.assertEqual(x.size()[0], self.batch_size)\n        self.assertEqual(x.size()[1], self.num_noise_words + 1)\n\n    def test_backward(self):\n        cost_func = NegativeSampling()\n        optimizer = torch.optim.SGD(self.model.parameters(), lr=0.001)\n        for _ in range(2):\n            x = self.model.forward(\n                self.context_ids, self.doc_ids, self.target_noise_ids)\n            x = cost_func.forward(x)\n            self.model.zero_grad()\n            x.backward()\n            optimizer.step()\n\n        self.assertEqual(torch.sum(self.model._D.grad[0, :].data), 0)\n        self.assertNotEqual(torch.sum(self.model._D.grad[1, :].data), 0)\n        self.assertNotEqual(torch.sum(self.model._D.grad[2, :].data), 0)\n\n        context_ids = self.context_ids.numpy().flatten()\n        target_noise_ids = self.target_noise_ids.numpy().flatten()\n\n        for word_id in range(15):\n            if word_id in context_ids:\n                self.assertNotEqual(\n                    torch.sum(self.model._W.grad[word_id, :].data), 0)\n            else:\n                self.assertEqual(\n                    torch.sum(self.model._W.grad[word_id, :].data), 0)\n\n            if word_id in target_noise_ids:\n                self.assertNotEqual(\n                    torch.sum(self.model._O.grad[:, word_id].data), 0)\n            else:\n                self.assertEqual(\n                    torch.sum(self.model._O.grad[:, word_id].data), 0)\n\n\nclass DBOWTest(TestCase):\n\n    def setUp(self):\n        self.batch_size = 2\n        self.num_noise_words = 2\n        self.num_docs = 3\n        self.num_words = 15\n        self.vec_dim = 10\n\n        self.doc_ids = torch.LongTensor([1, 2])\n        self.target_noise_ids = torch.LongTensor([[1, 3, 4], [2, 4, 7]])\n        self.model = DBOW(\n            self.vec_dim, self.num_docs, self.num_words)\n\n    def test_num_parameters(self):\n        self.assertEqual(\n            sum([x.size()[0] * x.size()[1] for x in self.model.parameters()]),\n            self.num_docs * self.vec_dim + self.num_words * self.vec_dim)\n\n    def test_forward(self):\n        x = self.model.forward(self.doc_ids, self.target_noise_ids)\n\n        self.assertEqual(x.size()[0], self.batch_size)\n        self.assertEqual(x.size()[1], self.num_noise_words + 1)\n\n    def test_backward(self):\n        cost_func = NegativeSampling()\n        optimizer = torch.optim.SGD(self.model.parameters(), lr=0.001)\n        for _ in range(2):\n            x = self.model.forward(self.doc_ids, self.target_noise_ids)\n            x = cost_func.forward(x)\n            self.model.zero_grad()\n            x.backward()\n            optimizer.step()\n\n        self.assertEqual(torch.sum(self.model._D.grad[0, :].data), 0)\n        self.assertNotEqual(torch.sum(self.model._D.grad[1, :].data), 0)\n        self.assertNotEqual(torch.sum(self.model._D.grad[2, :].data), 0)\n\n        target_noise_ids = self.target_noise_ids.numpy().flatten()\n\n        for word_id in range(15):\n            if word_id in target_noise_ids:\n                self.assertNotEqual(\n                    torch.sum(self.model._O.grad[:, word_id].data), 0)\n            else:\n                self.assertEqual(\n                    torch.sum(self.model._O.grad[:, word_id].data), 0)\n'"
